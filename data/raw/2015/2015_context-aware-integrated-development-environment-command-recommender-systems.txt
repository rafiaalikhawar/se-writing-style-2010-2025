Context-Aware Integrated Development
Environment Command Recommender Systems
Marko Gasparic, Tural Gurbanov, Francesco Ricci
Free University of Bozen-Bolzano
Piazza Domenicani, 3, 39100 Bolzano, Italy
Abstract —Integrated development environments (IDEs) are
complex applications that integrate multiple tools for creating
and manipulating software project artifacts. T o improve users’knowledge and the effectiveness of usage of the available function-
ality, the inclusion of recommender systems into IDEs has been
proposed. We present a novel IDE command recommendation
algorithm that, by taking into account the contexts in which adeveloper works and in which different commands are usually
executed, is able to provide relevant recommendations. We
performed an empirical comparison of the proposed algorithmwith state-of-the-art IDE command recommenders on a real-
world data set. The algorithms were evaluated in terms of
precision, recall, F1, k-tail, and with a new evaluation met-ric that is speciﬁcally measuring the usefulness of contextual
recommendations. The experiments revealed that in terms of
the contextual relevance and usefulness of recommendations theproposed algorithm outperforms existing algorithms.
I. I NTRODUCTION
Software development tools can affect the efﬁciency and the
quality of software construction [1]. Integrated development
environments (IDEs) are popular applications that serve theneeds of a large and diverse user population, by bringingtogether multiple tools to create and manipulate softwareproject artifacts. A generic IDE user is not supposed to
access the full provided functionality if that is not useful to
accomplish the task at hand. Nevertheless, it is important totry to increase the breadth of the used functionality, especiallyof the less skilled users, since the lack of knowledge mayprevent the exploitation of useful functions [2]. In fact, in our
previous work, we conducted a user study and performed a
number of interviews which conﬁrmed that even professionalsoftware developers are willing to learn new IDE functionalityand would like to use an application that could help them toachieve this goal [3][4].
To improve IDE users’ knowledge and usage of the available
functionality, command recommender systems (RSs) havebeen proposed [5]. Commands are shortcuts or menu buttonsthat execute a certain function, and they can be recommendedas:opportunistic suggestions, which refer to recommendations
that are relevant in the speciﬁc situation when the recommen-dation is presented; and global suggestions, which refer to
recommendations that are based on the long-term commandusage history and are supposed to be relevant in general for thefuture activity of the user. In our work, we focus on global IDE
command recommendations, which should take into account
the typical contexts in which a particular developer works andare useful in these contexts. We note that the actual deliveryof the recommendations, i.e., presentation and timing, are outof scope of this paper, but we refer an interested reader to
some of our previous publications, in particular: [4] and [6].
We model the context with contextual factors that describe
the environment in which an IDE user executed a command,the temporal perspective of the execution, what was the user’sactivity before and during the execution, and what projectartifact she interacted with [7]. The exploitation of contextualinformation to generate global command recommendations
is novel, since existing algorithms, which are either basedon command popularity, collaborative ﬁltering, or commanddiscovery patterns [5], do not take into account any contextualinformation. Even CoDis [8], which is a recent algorithmthat combines the information about the co-occurrences ofcommands in the same session with the command discoverypatterns, cannot be considered as context-aware, when itgenerates global recommendations. We conjecture that without
taking into account contextual information, the usefulness ofthe recommendations to the recipient’s work is coincidental.
Imagine an illustrative scenario, which we also observed in
the data set used in the evaluation. By the end of the ﬁrstweek of usage of Eclipse IDE (see http://www.eclipse.org), anovice programmer executed 13 distinct commands, such asPaste, Build All, and Navigate Back. CoDis recommends to
this user: Save, Save As, Run, Quick Fix, and Open Browser.
Conversely, recommending the most popular commands, theuser would learn: Save, Run, Undo, Copy, and Content Assist(also called “autocomplete”). Naturally, these commands arevery useful for any IDE user, and especially for that novice,who is using Eclipse for editing simple Java code. However,these are the most basic functions provided by any IDE andthe user will surely learn them quickly, also by herself. Onthe other hand, the context-aware RS, based on the algorithmproposed in this paper, which we named CNTX, would suggestsimple, but less obvious commands, namely: Save All Files,Run Last, Reset Perspective, Go To Line Start, and ShowMarketplace Wizard. Moreover, after some more time, whenthe novice user has already executed 41 distinct commands,our algorithm would also recommend two simple commands:Collapse All and Go To Text Start, and three advanced com-mands: Execute, Open Run Conﬁgurations, and IncrementalFind. But, the other two algorithms would still be focused onsimple commands, such as Redo, Go To Previous Word, SelectNext Word, Select Previous Word, and Show File Properties.
Hence, as this example illustrates, we believe that there
978-1-5386-2684-9/17/$31.00 c/circlecopyrt2017 IEEEASE 2017, Urbana-Champaign, IL, USA
T echnical Research - New Ideas688
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:19:44 UTC from IEEE Xplore.  Restrictions apply. is a great potential in context-aware command recommender
systems, and this paper discusses their main advantages andalso their limitations.
In the rest of the paper, we ﬁrst present the basic char-
acteristics of existing algorithms for generating global IDE
command recommendations (Sec. II). Then we present the
proposed CNTX algorithm (Sec. III) and the ofﬂine evaluationmethod (Sec. IV). The results (Sec. V) show that by usingcontext, the recommender is able to identify and suggestcommands that are less likely to be discovered without aRS and are more useful for the working context than the
commands recommended by existing algorithms. At the end,we draw the conclusions of our research results and outlinethe plans for the future work (Sec. VI).
The main contributions of this paper are: a novel context-
based algorithm for recommending IDE commands, a detailed
discussion of the algorithm implementation and problemsresolution, a novel context-based metric for estimating thepotential usefulness of the recommended commands, and anofﬂine evaluation of the existing algorithms.
II. R
ELATED WORK
Murphy-Hill et al. [5] studied eight command recommen-
dation algorithms, namely: Most Popular, which recommendsmost frequently executed commands; Most Widely Used,which recommends commands that are used by the largestnumber of users; Item-based Collaborative Filtering (CF)and User-based CF; which recommend commands that aremost similar to those already used by the user, or usedby similar users; and Advanced Discovery, Most PopularDiscovery, Item-based CF with Discovery, and User-basedCF with Discovery, which are based on sequential patternmining that is combined with basic collaborative ﬁlteringalgorithms or popularity-based algorithms. To evaluate theseeight algorithms, Murphy-Hill et al. performed ofﬂine andonline evaluations.
In the ofﬂine evaluation, they applied the k-tail evaluation
method suggested by Li et al. [9], which measures the ca-pability of an algorithm to predict commands that the usereventually used. Advanced Discovery algorithm achieved thehighest score, which was noticeably higher than the scoresof Most Popular, User-based CF, and Item-based CF, andrelatively similar to the scores of other algorithms. In the
special k-tail evaluation, where a command was treated as
being discovered only if it was used multiple times or in
multiple sessions, the highest score was achieved by User-
based CF with Discovery.
In the online evaluation, Murphy-Hill et al. recruited four
experts and nine novices. The participants were asked to ratethe usefulness and novelty of recommendations. The resultsshow that for the novices, User-based CF, Most Popular, andItem-based CF with Discovery generated the largest propor-
tion of useful and novel recommendations. For the experts,the Item-based CF with Discovery algorithm was the onlyalgorithm that performed relatively well.Recently, Zolaktaf and Murphy [8] proposed CoDis, which
is an algorithm based on the analysis of command discovery
patterns and co-occurrence of command executions in work-sessions. In an ofﬂine study, CoDis outperformed all otheralgorithms, in terms of the k-tail metric, when the number of
observed top-N recommendations is larger than 2; for N∈
{1,2}, User-based CF with Discovery outperformed CoDis.
It is worth noting that the aforementioned algorithms were
designed to accurately predict which commands an IDE user
will discover and start using autonomously. Nevertheless, weargue in this paper that it is also important for a recommenda-tion algorithm to identify commands which a recommendationrecipient can and will use during the work, but are not likely tobe discovered without the help of a RS. We think that in sucha case an IDE command RS is especially valuable. Hence, weconjecture that by employing information about the contextsin which a developer works and in which different commandsare usually executed, it is possible to provide such a type ofrecommendations.
III. CNTX: C
ONTEXT -BASED IDE C OMMAND
RECOMMENDATION GENERATION ALGORITHM
We apply a probabilistic model to generate personalized,
context-aware, novel, and useful command recommendations.The recommendation score of a command afor a user uis
deﬁned to be P(a∣u), which is the probability of observing
the usage of a, assuming that uknows a. Commands with the
highest recommendation score should be recommended ﬁrst
(top-N ). We assume, as in [10], that the RS users are only
interested to receive recommendations of commands that they
are not aware of, i.e., have never been executed by the user.
The input data for the recommendation algorithm was
collected by logging the IDE interactions of ﬁrst year bachelorstudents at the Free University of Bozen-Bolzano, during theﬁrst ten weeks of the Introduction to Programming course.
The data collection was completely anonymous. The data setcontains 199,220 command execution records. Each record
is a tuple<u, a, t, c>, where tis the timestamp and cis
the context in which uexecuted a. Overall, we detected 113
different user identiﬁers and 219 different commands. Each
context is a set of values for different contextual factors. Weused the context model proposed by Gasparic et al. [7], whichconsists of eleven contextual factors, namely: type, length,
and complexity of the artifact under development ,current
and previous activity, time of the day, day of the week, IDE
instance, active perspective, opened user interface elements,
and user interface element with focus. An example of the
interaction history log is presented in Tab. I; the columnsfollowing “timestamp” refer to the contextual factors.
A user ucan be described by a set of contexts C
uthat
were detected when she executed commands. The probabilityP(a∣u), that acan be executed by u, if she knows a,i s
estimated as P(a∣C
u), which is the probability to observe the
execution of ain the population of users that know and use
a, given a set of contexts in which uworked. Assuming that
the contexts are alternative, we deﬁne P(a∣u)as the average
689
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:19:44 UTC from IEEE Xplore.  Restrictions apply. TABLE I
USER-IDE INTERACTION DATA LOG EXAMPLE .
command iduser idtimestamp artifact type ...uiwith focus
undo A 1493241234 .html ... editor
collapseAll A 1493590321 .java ... editor
lineStart B 1493628126 .java ... console
... ... ... ... ... ...
undo X 1493708129 .java ... expressions
value of the probabilities P(a∣c)to observe the execution of a,
given the u’s context c. Hence, the scoring function is deﬁned
as follows:
score(u, a)= P(a∣u)= P(a∣Cu)=1
∣Cu∣∑
c∈CuP(a∣c)
To estimate P(a∣c), for each command a, we train a regres-
sion model [11] (for each command a) on a set of contexts
extracted from the IDE-interaction history logs of the users
who are using a. The regression model is trained to predict
a boolean variable indicating whether awill be executed
when a speciﬁc context cis detected. The score calculated
by the regression model for the context cis considered as the
probability P(a∣c)to observe agiven c.
To train a regression model, categorical variables should
be transformed into continuous [12]. Since the values of ourcontextual factors are nominal, i.e., cannot be ordered, wetransform each value of a contextual factor into a binaryvariable. For example, if the contextual factor type of the
artifact under development can have “.html” and “.java” values
(see “artifact
type” column in Tab. I), two binary variables are
created: “artifact type::.html” and “artifact type::.java”; and
the values of “artifact type::.html” are equal to 1for the rows
where “artifact type” contains “.html” and 0otherwise (see
“artifact type::.html” column in Tab. II).
Moreover, since regression models trained with few obser-
vations are unreliable, we limited the set of recommendablecommands to those for which we have at least mobservations.
We tested different parameter values and ﬁnally set mto 5
since this value yields the best results in the ofﬂine evaluationreported in this paper.
Furthermore, in order to improve the accuracy of the regres-
sion model and to compress the transformed data set, whichcontains a large number of potentially correlated columns,we performed “dimensionality reduction” [11]. We appliedLatent Semantic Indexing (LSI) [13], which uses SingularValue Decomposition to identify a linear subspace of the fullfeatures’ space that captures the largest part of the variance in
the collection. If we consider the generated binary variablesasfeatures, we could have applied LSI directly, however,
since the number of rows, i.e., observations of the command
executions, is large, we trained LSI with a random sampleof observations from the interaction history log. This, without
losing the quality, ensures that the calculations can be per-formed in minutes, and not in hours or days. By setting the
number of features to 35 and the maximum size of the sample
to 45,000 command execution records, LSI explains 99% ofthe training data variance, even though the initial data setcontains more than 1,000 features. We used this conﬁguration
also in the ofﬂine evaluation.
The class imbalance problem was the last issue to solve,
before we could train the regression model. The number of
executions of any command ais much lower than the number
of executions of non-a commands, i.e., commands other than
a. Consequently, any trained model learned on the full data set
would be biased to predict that command ais never executed,
regardless of the context. To solve this problem, we adopted“random undersampling” [14]: for each command regressionmodel, we included in the training data all the observations of
the command aexecution and an equal number of randomly
picked observations of executions of the other commands.
Finally, for each command, the training data, represented
with the selected latent features, was used to train a regression
model that predicts P(a∣c). In our experiments, we consid-
ered three linear regression models: Ridge, AdaBoost with alinear loss function, and AdaBoost with an exponential lossfunction [15]. The Ridge regressor is based on the linear leastsquares loss function and uses L
2-norm regularization to avoid
model’s overﬁtting. An AdaBoost regressor is a meta-estimator
that begins by ﬁtting a regressor on the original data set andthen ﬁts additional copies of the regressor on the same dataset, with adjusted weights assigned to instances, which arelarger for instances where the regressor is wrong. In that way,subsequent regressors focus more on difﬁcult cases. The Ridgemodel has been used as a component of the AdaBoost models.
To identify the most suitable regression model for a com-
mand, we conducted a ﬁve-fold random cross-validation test,over a set of candidate models. The model with the highestaverage score, according to the R
2metric [11] was selected.
The R2metric, also called coefﬁcient of determination, is
a regression score function that measures how well future
samples are likely to be predicted by the model.
IV . O FFLINE EV ALUATION METHOD
We conducted an ofﬂine experiment to evaluate RS algo-
rithms. With an ofﬂine experiment, one can efﬁciently compare
a wide range of algorithms and estimate their quality, byusing some proxy metrics, before the RS is deployed to realusers. Typically, these types of experiments are simple and
inexpensive, if compared to user studies and online evalua-
tions, as they require no interaction with real users. However,ofﬂine experiments can only answer a narrow set of researchquestions, since the data sets collected before the deploymentof the RS cannot be used for measuring the RS’s effect onuser behavior in a real-world setting [16].
In our empirical evaluation, we have compared CNTX with
the algorithms listed in Sec. II. We used the data set describedin Sec. III. Since the participating students were enrolled in anintroductory course, they were expected to have none or verybasic programming knowledge. Consequently, the assigned
tasks were focused on Java syntax. To allow a fair comparisonand meaningful analysis of the evolution of the metrics overthe weeks, we limited the testing set to 43 students who used
690
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:19:44 UTC from IEEE Xplore.  Restrictions apply. TABLE II
BINARIZED INTERACTION DATA LOG EXAMPLE .
command iduser idtimestamp artifact type::.html artifact type::.java ...uiwith focus::editor uiwith focus::console ...uiwith focus::expressions
undo A 1493241234 1 0 ... 1 0 ... 0
collapseAll A 1493590321 0 1 ... 1 0 ... 0
lineStart B 1493628126 0 1 ... 0 1 ... 0
... ... ... ... ... ... ... ... ... ...
undo X 1493708129 0 1 ... 0 0 ... 1
the IDE for at least ﬁve weeks. For each user and with each
algorithm, we generated the top-5 recommendations, for eachdifferent week of usage, by using the data observed in thepast. Previous analyses benchmarked the proposed algorithmson a larger number of recommendations. However, in a real-world setting only the very ﬁrst recommended commands areusually browsed by the users.
We must observe that IDE command usage history logs
show only the actions that have been performed by the users,and it is unclear why certain actions—in particular, executionsof speciﬁc commands—have not been performed. For instance,
when a command is never observed, is this because the user isnot aware of its existence or has she deliberately decided not
to use it? Secondly, IDE command usage history logs containcommands that have been discovered by the users withoutthe help of a RS, and traditional ofﬂine evaluation metricsshow how well the recommender prediction model identiﬁes
commands that the user eventually used, but they do not
fully show whether the recommender identiﬁes new and usefulcommands. For instance, the k-tail evaluation method [9]
is measuring the accuracy of the recommendation algorithmin suggesting commands autonomously discovered by the
user. That is why we also introduce here a novel evaluation
metric that measures the usefulness of the recommendations byconsidering the contexts in which users will actually executethe suggested commands.
A. Usage Prediction
In our experiments, to measure the accuracy of the al-
gorithms’ predictions, we considered precision, recall , and
F1metrics [16]. These are popular RSs evaluation metrics.
Moreover, we also adopted the k-tail evaluation method, to
better relate our analysis to previous studies.
To perform the k-tail evaluation and to compute precision,
recall , and F1, the data must be split into training and
testing sets. The recommendations are generated by training
the predictive model on the observations from the trainingset. Standard evaluation is based on the assumption that thecommands included in the testing set, which have been used
by the users, are good recommendations: higher similaritybetween the recommendations and the commands in the testingset indicates higher quality of the algorithm.
P recision measures how many items in the list of recom-
mendations are relevant, while recall shows how many of all
the relevant items are included in the list of recommendations.
Since there is usually a trade-off between precision andrecall , it is also common to measure F1, which averages
(harmonically) the two metrics:
precision=∣TP∣
∣TP∣+∣FP∣recall=∣TP∣
∣TP∣+∣FN∣
F1=2⋅precision⋅recall
precision+recall
where TP represents the set of correct recommendations, often
called “true positives”, FP represents the set of incorrect
recommendations, often called “false positives”, and FN
represents the set of correct items that were not recommended,often called “false negatives”. The highest possible score ofF1is 1. It is obtained if the algorithm recommends exactly
all the relevant items.
To calculate precision, recall , and F1and to observe their
evolution over time, we split the command usage logs in tenweeks. We generated a set of recommendations every week,for the ﬁrst 9 weeks. The data collected until Wednesday 6am,which is between the weekly assignment submission deadlinesand laboratory exercises, was used as the training set. The datacollected in the following one week was used as the testingset, where the set of relevant recommendations is composedof the commands not already present in the training set.
Conversely, in k-tail testing procedure, the training set
consists of the logs of the command executions that occurredbefore the user started using the last kcommands, while the
testing set consists of the commands that were executed forthe ﬁrst time after that moment. The testing set is used tocalculate hit-ratio (HR ):
HR=1
∣U∣∑
u∈Uhitu
where Uis the set of all users uandhituis a binary variable
equal to 1 when the recommended commands include at leastone of the last kcommands and equal to 0 otherwise. In our
experiment, kis equal to 1, as in [5] and [8], which means
that the testing set contains only the last discovered command.
B. Contextual Relevance
In this paper, we propose to relate the relevance of a com-
mand recommendation to the contexts in which the command
has been executed. In fact, we know that the contexts inwhich the command is executed are the contexts in which the
command is useful. In other words, in ideal conditions, themost relevant command for a given context is the one that hasbeen observed most often in this context. But, since contexts
are presented in a high dimensional space, it is very unlikely
691
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:19:44 UTC from IEEE Xplore.  Restrictions apply. to observe for a target user uexactly the same contexts that
are observed in the training set. Hence, we use a notion of
similarity between the target contexts of the user and thecontexts in which the command has been executed, to identifythe most relevant recommendations. In particular, the relevanceof a command ato a user ucan be deﬁned as the average
similarity between the contexts in which the command was
used—by the users that know and use it—and the contexts
in which the recommendation recipient worked during theperiod that is included in the testing set. The more similar thetwo contexts are, the more relevant ais to u. The contextual
relevance is calculated as follows:
rel(a, u)=1
∣C(u)∣∑
cu∈C(u)∑ca∈C(a)sim(cu,ca)
∣C(a)∣
where C(a)is a set of contexts in which ahas been executed,
C(u)is a set of contexts in which uwas executing commands,
sim(cu,ca)is the similarity between two contexts, and ca
andcuare a context where the command awas executed
and the target context of user u, respectively. We use “cosine
similarity” since we are interested in the correlation betweenfeatures of the different contexts [17].
The context-aware usefulness metric AR@Ncalculates the
average relevance of the top-N recommendations for each
user u∈U.I fRec @N
uis the set of top-N recommended
commands, then AR@Ncan be deﬁned as follows:
AR@N=1
∣U∣∑
u∈U∑ar∈Rec@N urel(ar,u)
N
Higher values indicate that more useful commands are
recommended by the algorithm. Moreover, AR@Npenalizes
the algorithms that generate less than Nrecommendations.
To calculate the weekly contextual relevance of the top-5
commands for each user, i.e., AR@5, we used the interac-
tion history logs in the same way as for the calculation of
precision, recall , and F1. But, instead of matching recom-
mended commands to commands in the testing set, we match
the observed contexts of the users, which were detected duringthe week that followed the recommendation generation mile-stone, with the contexts in which the recommended commandscan be effectively used.
We note that the source code of AR@Nand CNTX is
available at https://gitlab.inf.unibz.it/tural-gurbanov/ide
rs.
V. R ESULTS
The algorithms evaluated in our study generate very differ-
ent recommendations, as can be seen in Fig. 1. Here, as anexample, the word cloud created from the recommendationsproduced by the CNTX and the Most Widely Used algorithmsare shown. It can be seen that the commands recommendedand their frequencies are dissimilar.
The results of the k-tail evaluation (Tab. III) show that
Advanced Discovery has the highest hit-ratio , as in [5]. Apart
from that, our results are in a disagreement with the previousstudies. For instance, Most Popular has higher hit-ratio than
Most Widely Used, and User-based CF with Discovery and
Fig. 1. Recommendation cloud of CNTX (left) and Most Widely Used (right)
algorithms in the ﬁfth week of the study.
TABLE III
RESULTS OF k-TAIL EV ALUATION .
Algorithm Hit-ratio Algorithm Hit-ratio
Most Popular 9.3% Most Widely Used 4.7%
Item CF 11.6% User CF 4.7%
Advanced Discovery 18.6% Most Popular Discovery 14%
Item CF + Discovery 7% User CF + Discovery 4.7%
CoDis 4.7% CNTX 2.3%
CoDis have a very low hit-ratio . Interestingly, the lowest hit-
ratio was achieved by our algorithm: 2.3% means that only
1 out of 43 students autonomously discovered and executed a
command that would have been recommended to them by thecontext-aware algoirthm. Hence, according to the results of thek-tail evaluation, we expect that the proposed algorithm will
recommend commands that the users would ﬁnd really novel.
The average recall ,precision, F1, and AR@5,o ft h e
considered algorithms during nine weeks, are shown in Fig. 2.These results show that recall grows over time. The main
reason for this is that there are less and less commands in thetesting set every week. For the same reason, precision tends
to decrease over time. The only exceptions are offered by thealgorithms based on discovery patterns, which do not provideany recommendations in the ﬁrst weeks, thus, their precision
is initially 0 and later starts increasing. In Fig. 2, it can beseen that the ranking of the algorithms according to F1and
precision is basically identical. Similarly to what we observed
and discussed in the k-tail evaluation, we can conclude that
CNTX algorithm would recommend only a small set of thecommands that the students would discover autonomouslyduring the next week. The only algorithm that would providemore novel recommendations is User-based CF.
As the number of the commands that can be recommended
decreases, also the number of the recommendable commandswith the contexts similar to the users’ contexts decreasesover time. Consequently, the AR@5scores tend to decrease
with time. Nevertheless, considering this metric, the proposed
CNTX algorithm outperforms other algorithms in every week.
Furthermore, we can see that the popularity-based algorithmsperform worse than how they performed with respect to F1.
But the ranking of the other algorithms remains almost thesame as for F1. Still, it seems that the popularity-based algo-
rithms perform very well at the beginning, which means thatrecommending popular commands to novices is reasonable.This result is also in agreement with the results of the online
692
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:19:44 UTC from IEEE Xplore.  Restrictions apply. 246800.20.40.6RecallMost Popular Most Widely Used
Item CF User CF
Advanced Discovery Most Popular Discovery
Item CF + Discovery User CF + Discovery
CoDis CNTX
246800.20.40.6Precision
246800.20.4F1
246800.10.2
WeekAR@5
Fig. 2. Recall, precision, F1, and AR@5 values of the algorithms per week.
evaluation performed by Murphy-Hill et al. [5]. However,
after the number of known commands by the users increases,personalized algorithms tend to outperform popularity-basedalgorithms, while context-aware algorithms have an evengreater potential.
VI. C
ONCLUSIONS AND FUTURE WORK
In this paper, we have presented a novel IDE command
recommendation algorithm that provides recommendations by
taking into account the contexts in which a particular softwaredeveloper works and the contexts in which different commandsare usually executed. The algorithm aims at suggesting thecommands that a recommendation recipient can and will useduring the work, but which are not likely to be discoveredwithout the help of the RS. Since traditional metrics, such as
precision, recall ,F1, and hit-ratio , only measure the accu-
racy of predicting the commands autonomously discovered bythe users, we also introduced a novel evaluation metric thatmeasures the relevance of the recommendations in the contextsof the users. We have compared the proposed algorithm with aset of state-of-the-art algorithms, on a real-world data set. Theexperiments revealed that in terms of contextual relevance andrecommendation usefulness, the proposed algorithm outper-forms existing algorithms, while the traditional metrics scoreit lower. The explanation of this is that the proposed algorithmrecommends more novel commands to the user, which are notlikely to be discovered without the help of the recommender.
In the future, we plan to conduct an online experiment,
on a smaller set of algorithms, to evaluate the RS’s effect
on the user behavior in a real-world setting. Moreover, wewill consider designing and testing hybrid algorithms, withthe aim to obtain the beneﬁts of the popularity, collaborativeﬁltering, and context-based algorithms, while decreasing theirdrawbacks.
R
EFERENCES
[1] IEEE Comp. Soc., P. Bourque, and R. E. Fairley, Guide to the Software
Engineering Body of Knowledge. IEEE Computer Society Press, 2014.
[2] T. Grossman, G. Fitzmaurice, and R. Attar, “A survey of software
learnability: Metrics, methodologies and guidelines,” in Conference on
Human Factors in Computing Systems, 2009.
[3] M. Gasparic, A. Janes, F. Ricci, and M. Zanellati, “GUI design for IDE
command recommendations,” in International Conference on Intelligent
User Interfaces, 2017.
[4] M. Gasparic, A. Janes, F. Ricci, G. C. Murphy, and T. Gurbanov, “A
graphical user interface for presenting integrated development environ-
ment command recommendations: Design, evaluation, and implementa-
tion,” Inf. Softw. Tech., 2017.
[5] E. Murphy-Hill, R. Jiresal, and G. C. Murphy, “Improving software
developers’ ﬂuency by recommending development environment com-mands,” in International Symposium on the F oundations of Software
Engineering, 2012.
[6] M. Gasparic and F. Ricci, “Should context-aware IDE command rec-
ommendations always be presented in-context or not?” in Workshop on
Awareness Interfaces and Interactions, 2017.
[7] M. Gasparic, G. C. Murphy, and F. Ricci, “A context model for IDE-
based recommendation systems,” J. Syst. Softw., vol. 128, pp. 200–219,
2017.
[8] S. Zolaktaf and G. C. Murphy, “What to learn next: Recommending
commands in a feature-rich environment,” in International Conference
on Machine Learning and Applications, 2015.
[9] W. Li, J. Matejka, T. Grossman, J. A. Konstan, and G. Fitzmaurice,
“Design and evaluation of a command recommendation system forsoftware applications,” ACM Trans. Comput.-Hum. Interaction, vol. 18,
pp. 6:1–6:35, 2011.
[10] Y . Hu, Y . Koren, and C. V olinsky, “Collaborative ﬁltering for implicit
feedback datasets,” in International Conference on Data Mining, 2008.
[11] G. James, D. Witten, T. Hastie, and R. Tibshirani, An Introduction to
Statistical Learning: With Applications in R. Springer, 2014.
[12] S. Koranne, Handbook of Open Source Tools. Springer, 2010.
[13] S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Landauer, and
R. Harshman, “Indexing by latent semantic analysis,” J. Amer . Soc. Inf.
Sci., vol. 41, pp. 391–407, 1990.
[14] H. He and E. A. Garcia, “Learning from imbalanced data,” IEEE Trans.
on Knowl. and Data Eng., vol. 21, pp. 1263–1284, 2009.
[15] C. M. Bishop, Pattern Recognition and Machine Learning. Springer,
2006.
[16] A. Gunawardana and G. Shani, “Evaluating recommender systems,” in
Recommender Systems Handbook. Springer, 2015.
[17] X. Amatriain and J. M. Pujol, “Data mining methods for recommender
systems,” in Recommender Systems Handbook. Springer, 2015.
693
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:19:44 UTC from IEEE Xplore.  Restrictions apply. 