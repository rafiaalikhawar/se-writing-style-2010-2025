Property Inference for Deep Neural Networks
Divya Gopinath∗, Hayes Converse†, Corina S. P ˘as˘areanu∗and Ankur Taly‡
∗Carnegie Mellon University and NASA Ames
Email: divgml@gmail.com,corina.pasareanu@west.cmu.edu
†University of Texas at Austin
Email: hayesconverse@gmail.com
‡Google AI (now at Fiddler labs)
Email: ankur@ﬁddler .ai
Abstract —We present techniques for automatically inferring
formal properties of feed-forward neural networks. We observe
that a signiﬁcant part (if not all) of the logic of feed forward
networks is captured in the activation status ( on oroﬀ)
of its neurons. We propose to extract patterns based on
neuron decisions as preconditions that imply certain desirable
output property e.g., the prediction being a certain class.
We present techniques to extract input properties , encoding
convex predicates on the input space that imply given output
properties and layer properties , representing network properties
captured in the hidden layers that imply the desired output
behavior . We apply our techniques on networks for the MNIST
and ACASXU applications. Our experiments highlight the
use of the inferred properties in a variety of tasks, such
as explaining predictions, providing robustness guarantees,
simplifying proofs, and network distillation.
I. I NTRODUCTION
Deep Neural Networks (DNNs) have emerged as a pow-
erful mechanism for solving complex computational tasks,
achieving impressive results that equal and sometimes even
surpass human ability in performing these tasks. However,
the increased use of DNNs also brings along several safety
and security concerns. These are due to many factors, among
them lack of robustness . For instance, it is well known that
DNNs, including highly trained and smooth networks, are
vulnerable to adversarial perturbations. Small (impercepti-
ble) changes to an input lead to misclassiﬁcations. If such a
classiﬁer is used in the perception module of an autonomous
car, the network’s decision on an adversarial image can
have disastrous consequences. DNNs also suffer from a lack
of explainability : it is not well understood why a network
makes a certain prediction, which impedes on applications
of DNNs in safety-critical domains such as autonomous
driving, banking, or medicine. Finally, rigorous reasoning
is obstructed by a lack of intent when designing neural
networks, which only learn from examples, often without
a high-level requirements speciﬁcation. Such speciﬁcations
are commonly used when designing more traditional safety-
critical software systems.
In this paper, we present techniques for automatically
inferring formal properties of feed-forward neural networks.
These properties are of the form Pre⇒Post .Post isa postcondition stating the desired output behaviour, for
instance, the network’s prediction being a certain class. Pre
is a precondition that we automatically infer and can serve
as a formal explanation for why the output property holds.
We study input properties which encode predicates in the
input space that imply a given output propertyWe further
study layer properties which group inputs that have common
characteristics observed at an intermediate layer and that
together imply the desired output behaviorThe intention is
to capture properties based on the features extracted by the
network.
There are many choices for deﬁning network properties
that are appropriate preconditions for network behavior. In
this work, we infer properties corresponding to decision
patterns of neurons in the DNN. Such patterns prescribe
which neurons are onoroﬀin various layers. For neurons
implementing the ReLU activation function, this amounts to
whether the neuron output is greater than zero ( on) or equal
to zero (oﬀ). We focus on these simple patterns because
they are easy to compute and have simple mathematical rep-
resentations. Furthermore, they deﬁne natural partitions on
the input space, grouping together inputs that are processed
the same by the network and that yield the same output.
Other obvious, more complex properties (e.g. use a positive
threshold rather than zero for the activation functions, use
linear combinations on neuron values) are left for study in
future work.
We deﬁne input properties based on patterns that constrain
the activation status ( onoroﬀ) of all neurons up to an inter-
mediate layer. Such patterns form convex predicates in the
input space. Convexity is attractive as it makes the inferred
properties easy to visualize and interpret. Furthermore,
convex predicates can be solved efﬁciently with existing
linear programming solvers. Analogously, we deﬁne layer
properties based on patterns that constrain the activation
status at an intermediate layer. Layer patterns deﬁne convex
regions over the values at an intermediate layer and can be
expressed as unions of convex regions in the input space.
Another motivation for studying decision patterns is that
they are analogous to path constrains in program analysis.
Different program paths capture different input-output be-
7972019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE)
978-1-7281-2508-4/19/$31.00 ©2019 IEEE
DOI 10.1109/ASE.2019.00079
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:39 UTC from IEEE Xplore.  Restrictions apply. haviour of the program. Similarly, different neuron decision
patterns capture different behaviours of a DNN. It is our
proposition that we should be able to extract succinct input-
output properties based on decision patterns that together
explain the behavior of the network, and can act as formal
speciﬁcations of networks. We present two techniques to
extract network properties. Our ﬁrst technique is based on
iteratively reﬁning decision patterns while leveraging an off-
the-shelf decision procedure. We make use of the decision
procedure Reluplex [20], designed to prove properties of
feed-forward ReLU networks, but other decision procedures
can be used as well. Our second technique uses decision
tree learning to directly learn layer patterns from data. The
learned patterns can be formally checked using a decision
procedure. In lieu of a formal check, which is typically
expensive, one could empirically validate the learned pat-
terns over a held-out dataset to obtain conﬁdence in their
precision.
We consider this work as a ﬁrst step in the study of formal
properties of DNNs. As a proof of concept, we present
several different applications. We learn input and layer
properties for an MNIST network, and demonstrate their use
in providing robustness guarantees, explaining the network’s
decisions and debugging misclassiﬁcations made by the
network. We also study the use of patterns at intermediate
layers as interpolants in the proof of given input-output
properties for a network modeling a safety-critical system
for unmanned aircraft control (ACAS XU) [19]. The learned
patterns help decompose the proofs thereby making them
computationally efﬁcient. Finally, we discuss a somewhat
radical application of the learned patterns in distilling [16]
the behavior of DNNs. The key idea is to use the patterns
that have high support as distillation rules that directly
determine the network’s prediction without evaluating the
entire network. This results in a signiﬁcant speedup without
much loss of accuracy.
We provide an extended version of this paper, containing
more details about the applications and all the proofs,
in [14].
II. B ACKGROUND
A neural network deﬁnes a function F:I Rn→IRm
mapping an input vector of real values X∈IRnto an output
vectorY∈IRm. For a classiﬁcation network, the output
deﬁnes a score (or probability) across mclasses, and the
class with the highest score is typically the predicted class.
Afeed forward network is organized as a sequence of layers
with the ﬁrst layer being the input. Each intermediate layer
consists of computation units called neurons . Each neuron
consumes a linear combination of the outputs of neurons in
the previous layer, applies a non-linear activation function
to it, and propagates the output to the next layer. The output
vectorYis a linear combination of the outputs of neurons
in the ﬁnal layer. For instance, in a Rectiﬁed Linear Unit(ReLU) network, each neuron applies the activation function
ReLU (x)=max (0,x). Thus, the output of each neuron
is of the form ReLU (w1·v1+...+wp·vp+b)where
v1,...vpare the outputs of the neurons from the previous
layer,w1,...,w pare the weight parameters, and bis the
bias parameter of the neuron.1
Example. We use a simple feed forward ReLU network,
shown in Figure 1a, as a running example throughout this pa-
per. The network has four layers: one input layer, two hidden
layers and one output layer. It takes as input a vector of size
2. The output vector is also of size 2, indicating classiﬁcation
scores for 2 classes. All neurons in the hidden layers use
the ReLU activation function. The ﬁnal output is a linear
combination of the outputs of the neurons in the last hidden
layer. Weights are written on the edges. For simplicity, all
biases are zero. Consider the input [1.0,−1.0]. The output
on this input is F([1.0,−1.0]) = [y1,y2]=[ 1.0,−1.0].T o
see this, notice that the output of the ﬁrst hidden layer is
[v1,1,v1,2]=[ReLU (1.0·1.0−1.0·−1.0),ReLU (1.0·1.0+
1.0·−1.0)] = [2.0,0.0]. This feeds into the second hidden
layer whose output then is [v2,1,v2,2]=[ ReLU (0.5·2.0−
0.2·0.0),ReLU (−0.5·2.0+0.1·0.0)] = [1.0,0.0]. This in
turn feeds into the output layer which computes [y1,y2]=
[1.0·1.0−1.0·0.0,−1.0·1.0+1.0·0.0 ]=[ 1.0,−1.0].
A feed forward network is called fully connected if all
neurons in a hidden layer feed into all neurons in the
next layer; the network in Figure 1a is such a network.
Convolutional Neural Networks (CNNs) are similar to ReLU
networks, but in addition to (fully connected) layers, they
may also contain convolutional layers which compute mul-
tiple convolutions of the input with different ﬁlters and then
apply the ReLU activation function. For simplicity, we focus
our discussion on ReLU networks, but our work applies to
all piece-wise linear networks, including ReLUs and CNNs
(and in experiments we describe an analysis for a CNN).
Notations and Deﬁnitions. All subsequent notations and
deﬁnitions are for a feed forward ReLU network F, often
referred to implicitly. We use uppercase letters to denote
vectors and functions, and lowercase letters for scalars. We
useN,N/prime,N1,... to range over neurons, and Nfor the set
of all neurons in the network. For any two neurons N1,N2,
the relation N1≺N2holds if and only if the output of
neuronN1feeds into neuron N2, either directly or via in-
termediate layers. We deﬁne feeds (N)::={N/prime|N/prime≺N},
and extend it to sets of neurons in the natural way.
The output of each neuron Ncan be expressed as a
function of the input X. We abuse notation and use N(X)
to denote this function. It is deﬁned recursively via neurons
in the preceding layer. That is, if N1,...,N pare neurons
1Most classiﬁcation networks based on ReLUs typically apply a softmax
function at the output layer to convert the output to a probability distri-
bution. We express such networks as F::== softmax (G), whereGis a
pure ReLU network, and then focus our analysis on the network G.A n y
property of the output of Fis translated to a corresponding property of G.
798
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:39 UTC from IEEE Xplore.  Restrictions apply. (a) Example
(b) Input property for prediction “1”
Figure 1: Example neural network and input contract
from the preceding layer that directly feed into N, then
N(X)=ReLU(w 1·N1(X)+...+w2·N2(X)+b).F o r
ReLU networks, N(X)is always greater than or equal to
0. We say that the neuron is oﬀifN(X)=0 andon
ifN(X)>0. This essentially splits the cases when the
ReLU ﬁres and does not ﬁre. As we will see in Section III,
theon/oﬀ activation status of neurons is our key building
block for deﬁning network properties.
III. N ETWORK PROPERTIES
Our goal is to extract succinct input-output characteri-
zations of the network behaviour, that can act as formalspeciﬁcations for the network. The network itself providesan input-output mapping but of course this is uninteresting.Ideally we should group together inputs that lead to the sameoutput and express that in concise mathematical form. Tothis end we propose to infer input properties wrt a given
output property P. An input property is a predicate over the
input space, such that, all inputs satisfying it evaluate to anoutput satisfying the property P. In other words, an input
property is a precondition for postcondition P. Together, the
input property and the post condition form a formal contract
for the network. An example of an output property for aclassiﬁcation network is that the top predicted class is c,
i.e.,P(Y)::=argmax (Y)=c . Such properties are called
prediction postconditions.
In this work, we infer input properties that characterize
inputs that are processed in the same way by the network,i.e. they follow the same on/off activation pattern up tosome layer and deﬁne convex regions in the input space.There may be many such convex regions for a particularoutput property (say a particular prediction). The union ofthese regions fully captures the behavior of the network wrtthe output property. In practice it may be too expensiveto compute precisely this union but we show that evencomputing a subset of these regions can be useful for manyapplications.
We further study layer properties which encode common
properties at an intermediate layer that imply the desiredoutput behavior. Neural networks work by applying layerafter layer of transformations over the inputs, to extractimportant features of the data, and then make decisions
based on these features. Thus layer properties can potentiallycapture common characteristics over the extracted features,allowing us to get insights into the inner workings of thenetwork. Similar to input properties, we seek to infer layerproperties by studying the activation patterns of the network.Unlike input properties, layer properties do not map toconvex regions in the input space, but rather to unions ofconvex input regions.
Decision Patterns. We infer network properties based on
decision patterns of neurons in the network. A decision
patternσspeciﬁes an activation status (on oroﬀ) for some
subset of neurons. All other neurons are don’t care. We
formalize decision patterns σas partial functions N/arrowrighttophalf
{on,oﬀ}, and write on(σ)for the set of neurons marked
on, andoﬀ(σ)be the set of neurons marked oﬀin the
patternσ. Each decision pattern σdeﬁnes a predicate σ(X)
that is satisﬁed by all inputs whose evaluation achieves thesame activation status for all neurons as prescribed by the
pattern.
σ(X)::=/logicalanddisplay
N∈on(σ)N(X)>0∧/logicalanddisplay
N∈oﬀ(σ)N(X)=0 (1)
A decision pattern σis a network property wrt a postcondi-
tionPif:
∀X:σ(X)=⇒P(F(X)). (2)
We seek minimal patternsσwhich have the property
that dropping (which amounts to unconstraining) any neuron
from the pattern invalidates it. Minimality helps in gettingrid of unnecessary constraints, and ensuring that more inputscan satisfy the property.
The support of a pattern, denoted by supp (σ),i sa
measure of the number of inputs that follow the pattern.Formally, it is the total probability mass of inputs satisfyingσ, under a given input distribution. In the absence of an
explicit input distribution, support can be measured empiri-cally based on a training or test dataset. For large networksa formal proof for ∀X:σ(X)=⇒P(F(X))may not
be feasible. In such cases, one could aim for a probabilisticguarantee that the conditional expectation (denoted E)o f
P(F(X))givenσ(X)is above a certain threshold, i.e.,
E(P(F(X))|σ(X))≥τ.
2
A. Input Properties
To build input properties we infer input properties that
are convex predicates in the input space implying a given
2This is similar to the probabilistic guarantee associated with “An-
chors” [29], which we discuss further in Section VI.
799
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:39 UTC from IEEE Xplore.  Restrictions apply. postcondition. Given that feed forward ReLU networks
encode highly non-convex functions, the existence of input
properties is itself interesting. To identify input properties,
we consider decision patterns wherein for each neuron Nin
the pattern, all neurons that feed into Nare also included in
the pattern. We call such patterns ≺-closed. We show that ≺-
closed patterns capture convex predicates in the input space.
Theorem 1: For all≺-closed patterns σ,σ(X)is convex,
and has the form:
/logicalanddisplay
iin1..|on(σ)|Wi·X+bi>0∧/logicalanddisplay
ji n 1..|oﬀ(σ)|Wj·X+bj≤0
HereWi,bi,Wj,bjare some constants derived from the
weight and bias parameters of the network.
The proof is provided in the Appendix in [14]. It is based on
induction over the depth of neurons in the pattern σ. It shows
that the value of any neuron in the pattern can be expressed
as a linear combination of the inputs and that each on/off
activation adds a linear constraint to the input predicate.3
Thus, an input property can be obtained by identifying a ≺-
closed pattern σsuch that ∀X:σ(X)=⇒P(F(X)).F o r
convex postconditions P, we show that an input property can
be identiﬁed using any input Xwhose output satisﬁes P.
For this, we consider the activation signature ofX, which
is a decision pattern σXthat constrains the activation status
ofallneurons to that obtained during the evaluation of X.
Deﬁnition 1: Given an input X, the activation signature
ofXis a decision pattern σXsuch that for each neuron
N∈N ,σX(N)isonifN(X)>0, andoﬀotherwise.
It is easy to see that σXis a≺-closed pattern. Thus,
following Theorem 1, σXcan be used to obtain an input
property, i.e. a property that implies a desired output behav-
ior. We state this result as a proposition, which will be used
in Section IV.
Proposition 1: Given a convex postcondition Pand an
inputXwhose output satisﬁes P(i.e.,P(F(X)holds), the
following holds. There exist parameters W,b such that:
(A)∀X/prime:σX(X/prime)=⇒F(X/prime)=W·X/prime+b
(B) The predicate σX(X/prime)∧P(W·X/prime+b)is an input
property.
Example. We illustrate input properties on the network
shown in Figure 1a (introduced in Section II). Consider
the postcondition that the top prediction is class 1, i.e.,
P([y1,y2])::=y1>y 2. LetN1,1,N1,2be the neurons in
the ﬁrst hidden layer, and N2,1,N2,2be the neurons in the
second hidden layer. Consider the pattern σ={N1,1→
on,N1,2→oﬀ}. We argue that this pattern is an input
property wrt P. SinceN1,1isonit must be the case that the
values that feed into N1,1(which have the form x1−x2) are
positive, hence the inputs satisfy x1−x2>0. Furthermore,
sinceN1,2isoﬀit must be the case that the values that
3The theorem can also be proven by representing the network as a
conditional afﬁne transformation as shown in [12].feed into N1,2(which have the form x1+x2) are negative,
hence the inputs satisfy x1+x2≤0. Now notice that all the
inputs that satisfy these two constraints also satisfy neuron
N2,1is always onand neuron N2,2is always oﬀ. This is
because the value that feeds into N2,1is0.5·(x1−x2)
which must be positive (since x1−x2>0). Similarly
the value that feeds into N2,2is−0.5·(x1−x2)which
must be negative. Consequently the output [y1,y2]=[ 1.0·
N2,1(X)−1.0·N2,2(X),−1.0·N2,1(X)+1.0·N2,2(X)] =
[0.5·(x1−x2),−0.5·(x1−x2)]always satisﬁes y1>y 2
(whenx1−x2>0), making the pattern a precondition for
the property P. The pattern is ≺-closed, and therefore by
Theorem 1, the predicate σ(X)is convex. The predicate
σ(X)=N1,1(X)>0∧N1,2(X)=0 (see Equation 1)
amounts to the convex region x1−x2>0∧x1+x2≤0
(shown in blue in Figure 1b) and is minimal.
B. Layer Properties
While inferred input properties may be easy to interpret,
they often have tiny support. For instance, a property deﬁned
based on the activation signature of an input Xmay only
be satisﬁed by X, and possibly a few other inputs that are
syntactically close to X. Ideally, we’d like properties to
group together inputs that are semantically similar in the eye
of the network. To this end, we focus on decision patterns
at an intermediate layer that capture high-level features.
A layer property for a postcondition Pencodes a decision
patternσlover neurons in a speciﬁc layer lthat satisﬁes
∀X:σl(X)=⇒P(F(X)).4
Note that a layer property is convex in the space of values
at that layer, but not in the input space. However, it is
simple to express a layer property as a disjunction of input
preconditions. This is achieved by extending a layer pattern
with all possible patterns over neurons that feed into the
layer (directly or indirectly). Each such extended pattern
is≺-closed, and therefore convex (by Theorem 1). We
formulate this connection between layer and input properties
in the following proposition.
Proposition 2: Letσlbe a layer property for an output
property P. LetNlbe the set of neurons constrained by
σl, and letσ1,...,σ pbe all possible decision patterns over
neurons in feeds (Nl).5Then the following statements hold:
(A) For each i,σl(X)∧σi(X)is an input property.
(B)σl(X)⇔/logicalortext
i(σl(X)∧σi(X)).
Thus, layer properties can be seen as a grouping of several
input properties as dictated by an internal layer. We note
that identifying the right layer is key here. For instance,
if one picks a layer too close to the output then the layer
property may span all possible input properties, which is
uninteresting. In general, the choice of layer would depend
on the application. We discuss it further in Section V.
4For simplicity, we restrict ourselves to computing properties with respect
to a single internal layer but the approach extends to multiple layers.
5There are two 2|feeds (Nl)|such patterns.
800
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:39 UTC from IEEE Xplore.  Restrictions apply. Example. Let us revisit the example in Figure 1a for
the postcondition that the top prediction is class 1, i.e.,
P([y1,y2])::=y1>y 2. A layer pattern for this property is
{N2,1→on,N2,2→oﬀ}. It is easy to see that for all inputs
satisfying this pattern, the output [y1,y2]=[ 1.0·N2,1(X)−
1.0·N2,2(X),−1.0·N2,1(X)+1.0·N2,2(X)]will satisfy
y1>y 2, making the pattern a layer property wrt P. The pat-
tern is satisﬁed by the input [1.0,−1.0]. The execution of this
input involves neuron N1,1beingonand neuron N1,2being
oﬀ. Consequently, by proposition 2 (part (A)), the extended
pattern{N1,1→on,N1,2→oﬀ,N2,1→on,N2,2→oﬀ}
is an input property wrt P.
C. Interpreting and Using Inferred Network Properties
Robustness guarantees and adversarial examples. We
ﬁrst remark that provably-correct input and layer properties
deﬁned wrt prediction postconditions characterize regions
in the input space in which the network is guaranteed
to give the same label, i.e. the network is robust. Inputs
generated from counter-examples of pattern candidates that
fail to prove represent potential adversarial examples, as
they are close (in the Euclidean space) to (regions of)
inputs that are classiﬁed differently. Furthermore, they are
semantically similar to benign ones (since they follow the
same decision pattern) yet are classiﬁed differently. We show
such examples in Section V.
Explaining network predictions. Neural networks are
infamous for being complex black-boxes [22], [4]. An
important problem in interpreting them is to understand
why the network makes a certain prediction on an input.
Predictions properties (that ensure that the prediction is
a certain class) can be used to obtain such explanations.
But, such properties are useful explanations only if they
are themselves understandable. Inferred input properties are
useful in this respect as they trace convex regions in the
input space. Such regions are easy to interpret when the
input space is low dimensional.
For networks with high-dimensional inputs (e.g., image
classiﬁcation networks) input properties may be hard to
interpret or visualize. The conventional approach here is
to explain a prediction by assigning an importance score,
called attribution , to each input feature [32], [33]. The
attributions can be visualized as a heatmap overlayed on the
visualization of the input. In light of this, we propose two
different methods to obtain similar visualizations from input
properties. We note that in contrast to attributions, which
help explain predictions for individual inputs, our proposed
input properties help explain the predictions for regions of
the input space. Furthermore, and in contrast to existing
attribution methods, they provide formal guarantees as the
computed explanations are themselves network properties
that imply the given postcondition.
Under-approximation Boxes. As stated in Theorem 1, aninput property consists of a conjunction of linear inequa-
tions, which can be solved efﬁciently with existing Linear
Programming (LP) solvers. We propose computing under-
approximation boxes (i.e. bounds on each dimension) as a
way to interpret input properties. Speciﬁcally, we use LP
solving (after a suitable re-writing of the constraints)6to
ﬁnd solution intervals [loi,hii]for each input dimension i
such that/summationtext
i(hii−loi)is maximized. As there are many
such boxes, we constrain each box to include as many inputs
from the support as possible. These boxes provide simple
mathematical representations of the properties, and are easy
to visualize and interpret. Note that the under-approximating
boxes are themselves network properties that formally imply
the input properties and hence the given postcondition.
Minimal Assignments. We also propose another natural way
to interpret both input and layer properties through the lens
of a particular input. Analogous to attribution methods, we
aim to determine which input dimensions (or features) are
most relevant for the satisfaction of the property. Every
concrete input deﬁnes an assignment to the input variables
x1=v1∧x2=v2∧..∧xn=vnthat satisﬁes σ(X). The
problem now is to ﬁnd a minimal assignment that still leads
to the satisfaction of the property, i.e., a minimal subset of
the assignments such that xk1=vk1∧xk2=vk2∧..∧xkn=
vkn=⇒σ(X). The problem has been studied in the con-
straint solving literature, and is known to be computationally
expensive [3]. We adopt a greedy approach that eliminates
constraints iteratively and stops when σ(X)is no longer
implied; the checks are performed with a decision procedure.
The resulting constraints are also network properties that
formally guarantee the corresponding postcondition.
Layer Patterns as Interpolants. For deep networks de-
ployed in safety-critical contexts, one often wishes to a prove
a contract of the form A=⇒B, which says that for
all inputs Xsatisfying A(X), the corresponding output Y
(=F(X)) satisﬁes B(Y). For the ACASXU application,
there are several desirable properties of this form, wherein,
Ais a set of constraints deﬁning a single or disjoint convex
regions in the input space, and Bis an expected output
advisory. Formally, proving such properties for multi-layer
feed forward networks is computationally expensive [20].
We show that the inferred network patterns, in particular
layer patterns, help decompose proofs of such properties by
serving as useful interpolants [24]. Given a layer pattern σl,
we propose the following rule to decompose a proof.
(A=⇒σl),(σl=⇒B)
(A=⇒B)(3)
Thus, to prove A=⇒B, we must ﬁrst identify a layer
patternσlthat implies output property B, and then attempt
6We replace each occurrence of variable xiwithloiorhiibased on
the sign of the coefﬁcient in the inequalities. See [14] for details on the
computation of under-approximation boxes.
801
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:39 UTC from IEEE Xplore.  Restrictions apply. the proof A=⇒σlon the smaller network up to layer
l. Additionally, once a layer pattern σlis identiﬁed for
a property B, it can be reused to prove other properties
involving B. In Section V, we show that this decomposition
leads to signiﬁcant savings in veriﬁcation time for properties
of the ACASXU network.
Distilling rules from networks. Distillation is the process
of approximating the behavior of a large, complex deep
network with a smaller network [16]. The smaller network
is meant to be favorable to deployment under latency and
compute constraints while having comparable accuracy. We
show that layer patterns with high support provide a novel
way to perform such distillation. Suppose σlis a pattern
at an intermediate layer lthat implies that the prediction
is a certain class c. For any input X, we can execute the
network up to layer l, and check if the activation statuses of
the neurons in layer lsatisfy the pattern σl. If they do then
we can directly return the prediction class c. Otherwise we
continue executing the network. Thus for all inputs where
the pattern is satisﬁed, we replace the cost of executing
the network from layer lonward (possibly involving several
matrix multiplications) with simply checking the pattern σl.
The savings could be substantial if layer lis sufﬁciently
far from the output, and the layer pattern has high support.
Notice that if the patterns are formally veriﬁed then this hy-
brid setup is guaranteed to have no degradation in accuracy.
Having said this, we also note that most distillation methods
typically tolerate a small degradation in accuracy. Conse-
quently, instead of the expensive formal veriﬁcation step one
could perform an empirical validation of the patterns, and
select ones that hold with high probability. This makes the
approach practically attractive. As a proof of concept, we
evaluate this approach on an eight layer MNIST network in
Section V. Interestingly, we note that a network simpliﬁed in
this manner satisﬁes the inferred properties by construction ,
without any proof needed.
IV . C OMPUTING NETWORK PROPERTIES
We now describe two techniques to build input and layer
properties from a feed-forward network wrt convex output
propertyP.
A. Iterative relaxation of decision patterns
This is a technique for extracting input properties. It
makes use of an off-the-shelf decision procedure for neural
networks. In this work, we use Reluplex [20] but other
decision procedures can be used too (see Section VI).7
Recall from Section III that an input property is a ≺-
closed pattern σthat satisﬁes ∀X:σ(X)=⇒P(F(X)).
Ideally we would like to identify the weakest such pattern,
7As discussed, in the absence of a decision procedure, empirical valida-
tion of properties can also used. While we would lose the formal guarantee
that the computed decision patterns imply the postcondition, they may still
be useful in practice.i.e., one that constraints the fewest neurons. Computing
such a property would involve enumerating all ≺-closed
patterns ( O(2|N|)), and using a decision procedure to val-
idate whether Equation 2 holds. This is computationally
prohibitive.
Instead, we apply a greedy approach to identify a minimal
≺-closed pattern σ, meaning that there is no ≺-closed sub-
pattern of σthat also satisﬁes Equation 2. We start with
an inputXwhose output satisﬁes the postcondition P, i.e.,
P(F(X))holds. Let σXbe the activation signature (see
Deﬁnition 1) of the input X. By Proposition 1 (Part (B)),
we have that σX(X/prime)∧P(F(X/prime))is an input property; recall
thatPis assumed to be convex. But this property may not be
minimal. Therefore, we iteratively drop constraints from it
till we obtain a minimal property. The algorithm is formally
described in the Appendix in [14] (see Algorithm 1). It is
easy to see that the resulting pattern is ≺−closed , minimal,
and it implies the output property ( F(X/prime)=y).
Proposition 3: Algorithm 1 (see Appendix in [14]) al-
ways returns a minimal input property, and involves at most
n+mcalls to the decision procedure, where nis the number
of layers, and mis the maximum number of neurons in a
layer.
Example. Consider the example network from Figure 1a,
and the input X=[ 1.0,−1.0]for which the network
predicts class 1. We apply Algorithm 1 to identify an input
property for class 1. The algorithm starts with the activation
signature ofX, which is the pattern σX={N1,1→
on,N1,2→oﬀ,N2,1→on,N2,2→oﬀ}. Notice that σX
is already an input property for class 1. The algorithm begins
to unconstrain all neurons in each layer, starting from the
last layer, and identiﬁes layer 1as the critical layer (i.e.,
unconstraining neurons in layer 1violates the postcondition).
The algorithm then identiﬁes {N1,1→on,N1,2→oﬀ}as
a minimal pattern that implies the postcondition.
B. Mining layer properties using decision tree learning
The greedy algorithm described in the previous section
is computationally expensive as it invokes a decision pro-
cedure at each step. We now present a relatively inexpen-
sive technique that relies on data, and avoids invoking a
decision procedure multiple times. The idea is to observe
the activation signatures of a large number of inputs, and
learn decision patterns that imply various output properties.
In this work, we use decision tree learning (see Appendix in
[14] for background) to extract compact rules based on the
activation statuses ( onoroﬀ) of neurons in a layer. Decision
trees are attractive as they yield decision patterns that are
compact (and therefore have high support) based on various
information-theoretic measures. The resulting patterns are
empirically validated layer properties, which can be formally
checked with a single call to a decision procedure.
Our algorithm works as follows. Suppose we have a
dataset of inputs D. Consider a layer lwhere we would
802
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:39 UTC from IEEE Xplore.  Restrictions apply. like to learn a layer property wrt postcondition P.W e
evaluate the network on each input X∈D, and note: (1) the
activation status of all neurons in layer l, denoted by σl
X,
and (2) the boolean P(F(X))indicating whether the output
F(X)satisﬁes property P. Thus, we have a labeled dataset
of feature vectors σl
Xmapped to labels P(F(X)); see for
example Figure 2a. We now learn a decision tree from this
dataset. The nodes of the tree are neurons from layer l, and
branches are based on whether the neuron is onoroﬀ.
Each path from root to a leaf labeled Tr u e forms a decision
pattern for predicting the output property; see Figure 2b.
We ﬁlter out patterns σthat are impure , meaning that there
exists an input X∈D that satisﬁes σ(X)butP(F(X))
does not hold. The remaining patterns are “likely” layer
properties wrt the postcondition. We sort them in decreasing
order of their support and invoke the decision procedure
(DP(σ(X),P(F(X)))) to formally verify them. This last
step can be skipped for applications such as distillation (see
Section V) where empirically validated patterns may sufﬁce.
We can reﬁne the method for the case where the output
property is a prediction postcondition i.e., of the form
P(Y)::=argmax (Y)=c. In this case, rather than pre-
dicting a boolean as to whether the predicted class is c,w e
train a decision tree to directly predict the class label. This
lets us harvest layer patterns for prediction postconditions
corresponding to all classes. Speciﬁcally, the path from the
root to a leaf labeled class cis a likely layer property for
the postcondition that the top predicted class is c.
Counter-example guided reﬁnement. In verifying Equa-
tion 2 for a decision pattern σusing a decision procedure,
if a counter-example is found, we strengthen the pattern
by additionally constraining the activation status of those
neurons from layer lthat have the same activation status for
all inputs satisfying the pattern σ. If veriﬁcation fails on this
stronger pattern then we do a ﬁnal step of constraining all
neurons from layer lbased on the activation signature of a
single input satisfying the pattern. If veriﬁcation still fails,
we discard the pattern. One can also consider a different
strategy for reﬁnement, were the counter-examples are added
back to the data set and the decision tree learning is re-run,
obtaining new layer patterns that will no longer lead to those
counter-examples. The drawback is that it may require too
many calls to the decision procedure, if many reﬁnement
steps are needed.
V. A PPLICATIONS
In this section, we discuss case studies on computing
input and layer properties, and using them for different
applications. We implemented all our algorithms in Python
3.0 and Tensorﬂow. The Python notebook is connected
to Python2 Google Compute Engine backend with 12Gb
RAM allotted. Our implementation supports analysis of both
ReLU and CNN networks. However, for the proofs we
use Reluplex [20], which is limited to ReLU networks. Toenforce a decision pattern we modiﬁed Reluplex to constrain
intermediate neuron values. As more decision procedures for
neural networks become available, we plan to incorporate
them in our tool, thus extending its applicability. The Re-
luplex runs were done on a server with Ubuntu v16.04 (8
core, 64 GB RAM). We use the linear programming solver
pulp 2.3.1 to solve for under-approximation boxes. We
plan to make the implementation and the networks available
with a ﬁnal paper version.
A. Analysis of ACASXU
We ﬁrst discuss the analysis of ACASXU , a safety-
critical collision avoidance system for unmanned aircraft
control [19]. ACASX is a family of collision avoidance
systems for aircraft, under development by the Federal
Aviation Administration (FAA). ACASXU is the version for
unmanned aircraft. It receives sensor information regarding
the drone (the ownship ) and any nearby intruder drones, and
then issues horizontal turning advisories aimed at preventing
collisions. The input sensor data includes: (1) Range: dis-
tance between ownship and intruder; (2) θ: angle of intruder
relative to ownship heading direction; (3) ψ: heading angle
of intruder relative to ownship heading direction; (4) vown:
speed of ownship; (5) vint: speed of intruder; (6) τ: time until
loss of vertical separation; and (7) aprev: previous advisory.
The FAA is exploring an implementation of ACASXU
that uses an array of 45 deep neural networks, from which
we selected one network for discussion here. The ﬁve
possible output actions are as follows: (0) Clear-of-Conﬂict
(COC), (1) Weak Left, (2) Weak Right, (3) Strong Left, and
(4) Strong Right. Each advisory is assigned a score, with the
lowest score corresponding to the best action. The network
that we analyzed consists of 6 hidden layers, and 50 ReLU
activation nodes per layer. We used 384221 inputs with
known labels. ACASXU networks were analyzed before
with Reluplex [20]. Veriﬁcation for ACASXU is challenging,
taking many hours (may even time out after 12h); we give
more details below.
1) Property Inference: We infer network properties wrt
prediction postconditions that require that the output of a
network classiﬁer is a certain class. We used decision tree
learning to extract layer patterns; we list them all (total 25)
in Table III in the Appendix in [14]. The learning took
45 seconds on average per layer (4.5 minutes in total). We
discuss here the veriﬁcation of one speciﬁc layer pattern.
This pattern was for label COC (clear-of-conﬂict) at layer
5, and was subsequently used to decompose proofs of
ACASXU properties, as discussed below. The pattern has a
support of 109417 inputs. We were able to prove a property
computed based on this pattern after two reﬁnement steps
(Section IV), within 5 minutes. We also extracted candidate
input properties corresponding to the decision pattern of the
layer property following proposition 2. From the 109417
inputs that satisﬁed the decision pattern at layer 5, we
803
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:39 UTC from IEEE Xplore.  Restrictions apply. /angbracketleftx1,x2/angbracketright /angbracketleftN1,1,N1,2/angbracketright P(F(X))
/angbracketleft0,−1/angbracketright /angbracketlefton,oﬀ/angbracketright True
/angbracketleft1,0/angbracketright /angbracketlefton,on/angbracketright True
/angbracketleft0,1/angbracketright /angbracketleftoﬀ,on/angbracketright False
/angbracketleft4,3/angbracketright /angbracketlefton,on/angbracketright False
/angbracketleft1,−1/angbracketright /angbracketlefton,oﬀ/angbracketright True
(a) Training dataset for decision tree.
(b) Resultant decision tree. The pattern harvested for
True is{N1,1→on,N1,2→oﬀ}.
Figure 2: Illustration of decision tree learning for mining properties for the network in Figure 1a. The output property is
that the top predicted class is “1”.
extracted distinct decision preﬁxes corresponding to 5532
inputs. We were able to prove all of them (3600 properties)
within an average time of 1 minute per property.
These experiments show that it is feasible to extract input
and layer properties in terms of the on/off patterns of the
ReLU nodes of real networks. The experiments also show
that the patterns constraining lesser number of neurons have
higher support and layer properties have higher support than
input properties, as expected, since they cover a union of
regions in the input space.
2) Explaining Network Predictions: The input-output
properties derived for ACASXU can explain the network
behavior. We further used LP solving to calculate under-
approximation boxes corresponding to input properties. We
calculated such a box for each of the 3600 input properties
that we had proved. We also generated under-approximation
boxes for input decision patterns that could not be proved
within a time limit of 12 hours but had high support. This
helped elicit novel properties of the network, which were
validated by the domain experts. We give some examples
below.
– All the inputs within: 31900≤range≤37976, 1.684
≤θ≤2.5133,ψ=-2.83, 414.3 ≤vown≤506.86,vint=
300, should have the turning advisory as COC.
– All the inputs within: range =499, -0.314 ≤θ≤-3.14,
-3.14≤ψ≤0, 100≤vown≤571, 0≤vint≤150, should
have the turning advisory as Strong Left.
Please refer the Appendix in [14] for more results.
We further experimented with computing minimal assign-
ments that satisfy the inferred properies. For instance, we
analyzed a layer 2property for the label COC, with a support
of 51704 inputs. By computing the minimal assignment over
an input that satisﬁed this property, we determined that the
last two input attributes, namely, vown(speed of ownship)
andvint(speed of intruder) were not relevant when the other
attributes are constrained as follows: range =48608,θ=-
3.14 andψ=-2.83. This represents an input-output property
of the network elicited by our technique. The domain experts
conﬁrmed that this was indeed a valid and novel propertyof the ACASXU network.
3) Layer Patterns as Interpolants: To evaluate the use of
layer patterns in simplifying difﬁcult proofs, we selected 3
properties from the ACASXU application. These properties
have previously been considered for veriﬁcation directly
using Reluplex [20]. We list here the three properties.
•Property 1: All the inputs within the following region:
55947.691 ≤range≤679848, -3.14 ≤θ≤3.14, -3.14
≤ψ≤3.14, 1145 ≤vown≤1200, 0≤vint≤60, should
have the turning advisory as Clear-of-Conﬂict (COC).
This property takes approx. 31 minutes to check with
Reluplex.
•Property 2: All the inputs within the following region:
12000≤range≤62000, (0.7 ≤θ≤3.14) or (-3.14
≤θ≤-0.7), -3.14 ≤ψ≤-3.14 + 0.005, 100 ≤vown
≤1200, 0 ≤vint≤1200 , should have the turning
advisory as COC. This property has a huge input region
and direct veriﬁcation with Reluplex times out after 12
hours.
•Property 3: All inputs within the following region,
36000≤range≤60760, 0.7 ≤θ≤3.14, -3.14 ≤
ψ≤-3.14 + 0.01, 900 ≤vown≤1200, 600 ≤vint
≤1200 , should have the turning advisory as COC.
This corresponds takes approx. 5 hours to check with
Reluplex.
All three properties have the form A=⇒B, whereA
speciﬁes constraints on the input attributes, and Bspeciﬁes
that the output turning advisory is COC. For each property,
we used decision tree learning to extract multiple layer
patterns for label COC at every layer, and selected the one
that covers maximum number of inputs within the input
regionA. Incidentally, for all three properties the same
pattern at layer 5(denoted by σ5) was selected.
Property 1: We found 195 inputs in the training set that fall
withinAand classify as COC. All of these inputs are also
covered by σ5. We therefore proceeded to prove A=⇒σ5
andσ5=⇒Busing Reluplex. For proving σ5=⇒B,
we had to strengthen the pattern by constraining 48 nodes
at layer 5. This made the proof go through, and ﬁnish in 5
804
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:39 UTC from IEEE Xplore.  Restrictions apply. minutes.
We then attempted to prove A =⇒σ5for the
strengthened version. This process ﬁnished in 2 minutes.
Thus, we were able to prove this property in 7 minutes. In
contrast, direct veriﬁcation of the property using Reluplex
takes 31 minutes.
Properties 2 and 3: We could not identify a single layer
pattern that covered the inputs within Acompletely. The
patternσ5had maximum coverage with respect to the
training inputs within A(5276/7618 inputs for property
2,256/441 for property 3). We split the proof into two
parts. First, we extracted the activation signature preﬁxes
up to layer 5for each of the training inputs that satisfy
σ5. Letcov be the set of these preﬁxes. We then checked
(A∧/logicalortext
σi∈covσi(X)) =⇒B8Checks of the form
(A∧σi(X)) =⇒Bwere spawned in parallel for every σi.
This completed in an hour for property 2 and within 6 mins
for property 3. The remaining obligation in completing the
proof for the property was (A∧¬(/logicalortext
i∈covσi(X))) =⇒
B. To check this efﬁciently, we determined the under-
approximation boxes for each σi, and spawned parallel
checks on the partitions within Anot covered by the boxes.
The longest time taken by any job was 2 hours 10 minutes
for property 2 and 1 hour 30 minutes for property 3. This
is a promising result as a direct proof of property 2 using
Reluplex times out after 12 hours. For property 3, a direct
proof takes 5 hours.
B. Analysis of MNIST
We also analyzed MNIST , an image classiﬁcation net-
work based on a large collection of handwritten digits [25].
It has 60,000 training input images, each characterized by
784 attributes and belonging to one of 10 labels. We ﬁrst
analyzed a simple network from the Reluplex distribution
(containing 10 layers with 10 ReLU nodes per layer). The
simplicity of the network makes it amenable to proofs
using Reluplex. For the distillation experiments (described
in the following subsection) we use a more complex MNIST
network that is close to state-of-the art.
1) Property Inference: We extracted input properties us-
ing iterative relaxation and layer properties using decision
tree learning, showing the feasibility of our approach in
the context of image classiﬁcation, which involves a much
larger input space compared to ACASXU. Details about the
computed properties (total 30) are given in Tables I and II
in the Appendix in [14].
The Reluplex checks for some of the network properties
generated counter-examples which show potential vulnera-
bilities of the network, since they are close (in the Euclidean
space) to other inputs that are classiﬁed differently (Figure
3).
8Sinceσ5implies the property Bonly after strengthening, showing
that (A∧/logicalortext
i∈covσi(X)) =⇒σ5is not enough to ensure that
(A∧/logicalortext
i∈covσi(X)) =⇒B.
Figure 3: Original images from the data set (left). Counter-
examples to failed proofs for patterns containing the original
images (right).
Figure 4: Visualization of MNIST input properties using
under-approximation boxes.
2) Explaining Network Predictions: We further computed
and visualized under-approximation boxes for the inferred
properties. As an example, in Figure 4, we show a visual-
ization of input properties corresponding to three different
images from the training set. The ﬁrst column shows original
images. Columns 2 and 3 show images with all pixels set to
their minimum and maximum values in the computed under-
approximating box, respectively. Columns 4, 5 and 6 have
each pixel set to the mean value of its range in the box,
a randomly chosen value below the mean, and a randomly
chosen value above the mean, respectively.
In Figure 5, we visualize layer properties via under-
approximation boxes corresponding to 5 input properties,
based on 5 randomly chosen images from the support of
the property. Each box is represented by 2 images, setting
all the pixels to their respective minimum and maximum
values in the box. Note that the images drawn from the
under-approximation boxes represent new inputs (not in the
training set) that satisfy the same property and hence are
labelled the same. While input properties capture visually (or
syntactically) similar images, layer properties cluster images
of the same digit written in different ways, indicating that
layer properties can potentially capture common features
across inputs. The developer can examine the generated
images to get a sense of the image characteristics that
contributed to the network decisions.
3) Misclassiﬁcations: Under-approximation boxes can
also be used to reason about misclassiﬁcations . Misclassiﬁed
inputs are typically “rare” and spread across the input space,
and it is very difﬁcult for developers to understand their
cause and ﬁx the underlying problem. Figure 6 shows an
image of digit 1 misclassiﬁed to digit 2 (Figure 6, ﬁrst
column). We used this input to extract an input decision
pattern and compute an under-approximation box for it
805
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:39 UTC from IEEE Xplore.  Restrictions apply. Figure 5: Visualization of MNIST layer properties using
under-approximation boxes.
Figure 6: Digit 1 misclassiﬁed to 2 and im-
ages with min and max values from under-
approximation box of original image.
(Figure 6, 2nd and 3rd columns). We can thus draw many
more inputs from the box that are similarly misclassiﬁed.
These inputs can help developers understand the cause of
misclassiﬁcation and re-train the network on them.
C. Distillation
Our ﬁnal experiment is to evaluate the use of layer prop-
erties in distilling a network. As discussed in Section III-C,
the key idea is to use prediction properties at an intermediate
layer as distillation rules. For inputs satisfying the property,
we save the inference cost of evaluating the network from
the intermediate layer onwards. We present a preliminary
evaluation of this idea using a more complex MNIST net-
work [1] with 8 hidden layers; two convolutional, one max
pooling, two convolutional, one max pooling, and two fully
connected layers. The network has a superior accuracy of
0.9943 but it is computationally expensive during inference.
We use the decision tree algorithm to obtain layer patterns.
We then empirically validate them (using a validation set
of5000 images), and select ones with accuracy above a
threshold τ(see Section III). The selected properties are
used as distillation rules for inputs satisfying them. Using a
held-out test dataset, we measure the overall accuracy and
inference time of this hybrid setup for different values of τ.
Figure 7 shows the results of distillation from the ﬁrst
max pooling layer9, which consists of 4608 neurons. The
x-axis shows the empirical validation threshold used for
selecting properties. The extreme right point (threshold >
1) corresponds to one where no properties are selected, and
therefore distillation is not triggered. The reported inference
times are based on an average of 10 runs of the test dataset
on a single core Intel(R) Xeon(R) CPU @ 2.30GHz. The
ﬁgure shows the trend of overall accuracy and inference time
as the threshold τis varied from 0.9to1.0. Observe that at
a threshold of τ=0.98, one can achieve a 22% saving in
9While max pooling neurons are different from ReLU neurons, we could
still consider activation patterns on them based on whether the neuron
output is greater than 0or equal to 0. A decision tree can then be learned
over these patterns to ﬁt the prediction labels.
Figure 7: Distillation of an eight layer MNIST network
(from [1]) using properties at the ﬁrst max pooling layer.
Figure 8: Distillation of an eight layer MNIST network
(from [1]) using layer patterns at the second max pooling
layers.
inference time while only degrading accuracy from 0.9943 to
0.9903 . This is quite promising. As expected, lowering the
threshold further considers more properties, and therefore
reduces both inference time and accuracy. The results from
the second max pooling layer (shown in Figure 8) are similar
except that both the degradation in accuracy and the saving
in inference time are smaller. This is expected as the second
max pooling layer is closer to the output, and therefore the
properties that we infer approximate a smaller part of the
network.
VI. R ELATED WORK
We survey the works that are the most closely related
to ours. In [34] it has been shown that neural networks
are highly vulnerable to small adversarial perturbations.
Since then, many works have focused on methods for
ﬁnding adversarial examples. They range from heuristic and
optimization-based methods [13], [7], [27], [1], [26] to anal-
ysis techniques which can also provide formal guarantees. In
the latter category, tools based on constraint solving, interval
analysis or abstract interpretation, such as DLV [17], Relu-
plex [20], AI2[12] ReluVal [36], Neurify [35] and others [5],
[8], are gaining prominence. Our work is complementary
as it focuses on inferring input-output properties of neural
networks. In principle, we can leverage the previous analysis
techniques to verify the inferred properties.
806
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:39 UTC from IEEE Xplore.  Restrictions apply. There are several papers on explaining predictions made
by neural networks, see [15] for a survey. One line of work
is on explaining individual predictions by attributing them
to input features [32], [28], [31], [33], [23], [18]. They are
either based on computing gradients of the prediction with
respect to input features [32], [33], back-propagating the
prediction score to input features using a set of rules [31],
[18], using attribution techniques from cooperative game
theory [23], or computing local linear approximations of the
behavior of the network [28].
The closest to ours is the work on Anchors [29], which
aims to explain the network behaviour by means of rules
(called anchors ), which represent sufﬁcient conditions for
network predictions. These anchors are computed solely
based on the black-box behaviour of the neural network.
Input properties from our work can be viewed as anchors
for various output properties. The key difference is that our
input properties are obtained via a white-box analysis of
the neurons in the network, and are backed with a formal
guarantee.
Also relevant, there is work on computing the inﬂuence of
individual neurons on predictions made by the network [2],
[21]. In a sense, our layer properties can be seen as a
means for identifying inﬂuential neurons for a prediction,
the key difference being that layer properties also guarantee
that decisions of the inﬂuential neurons indeed imply the
prediction. These previous approaches evaluate neuron inﬂu-
ence by measuring how accurately the top k most inﬂuential
neurons alone can predict the class. Interestingly, we believe
these works also lend themselves to distillation. We leave a
thorough comparison of different distillation mechanisms to
future work.
There is a large body of work on property inference,
including [9], [6], [10], [11] to name just a few, although
none of the previous works have addressed neural networks.
The programs considered in this literature tend to be small
but have complex constructs such as loops, arrays, pointers.
In contrast, neural networks have simpler structure but can
be massive in scale.
A recent paper [30] uses properties over neuron activation
distributions to determine whether a given input is benign
(i.e., non adversarial). The ‘invariants’ in [30] are meant to
capture properties of a given set of inputs (benign inputs),
while our input and layer properties are meant to capture
properties of the network. Furthermore, our properties par-
tition the input space into prediction-based regions, and are
justiﬁed with a formal proof. We do note that our properties
can be seen as invariance properties of the network, that
have the special form ‘precondition implies postcondition’.
Our distillation approach is related to teacher-student
learning in neural networks [16]. Note that we do not
perform transfer learning (from a teacher to a student) but
instead use the inferred properties to simplify the network.
Thus, unlike teacher/student learning, our distillation ap-proach is adaptive , allowing to process some inputs (that sat-
isfy the layer properties) using the simpliﬁed computation;
the other inputs (that may need more complex processing)
go through the original network. Furthermore, we provide
formal guarantees as by construction, our ‘distilled’ network
satisﬁes the properties used in the distillation.
VII. C ONCLUSION
We presented techniques to extract neural network input-
output properties and we discussed their application to
explaining neural networks, providing robustness guarantees,
simplifying proofs and distilling the networks. As more
decision procedures for neural networks become available,
we plan to incorporate them in our tool, thus extending its
applicability and scalability. We also plan to leverage the
decision patterns to obtain parallel veriﬁcation techniques
for neural networks and to investigate other applications
of the inferred properties, such as conﬁdence modeling,
adversarial detection and guarding monitors for safety and
security critical systems.
REFERENCES
[1] N. Carlini and D. Wagner, “Towards evaluating the robustness
of neural networks,” in IEEE S&P , 2017.
[2] K. Dhamdhere, M. Sundararajan, and Q. Yan, “How
important is a neuron,” in International Conference
on Learning Representations , 2019. [Online]. Available:
https://openreview.net/forum?id=SylKoo0cKm
[3] I. Dillig, T. Dillig, K. L. McMillan, and A. Aiken, “Minimum
satisfying assignments for smt,” in Proceedings of the 24th
International Conference on Computer Aided V eriﬁcation ,
ser. CA V’12. Berlin, Heidelberg: Springer-Verlag, 2012, pp.
394–409. [Online]. Available: http://dx.doi.org/10.1007/978-
3-642-31424-7 30
[4] B. Doshi-Velez, Finale; Kim, “Towards a rigorous science of
interpretable machine learning,” in eprint arXiv:1702.08608 ,
2017.
[5] S. Dutta, S. Jha, S. Sankaranarayanan, and A. Tiwari, “Out-
put range analysis for deep feedforward neural networks,”
inNASA F ormal Methods - 10th International Symposium,
NFM 2018, Newport News, VA, USA, April 17-19, 2018,
Proceedings , 2018.
[6] M. D. Ernst, J. H. Perkins, P. J. Guo, S. McCamant,
C. Pacheco, M. S. Tschantz, and C. Xiao, “The Daikon
system for dynamic detection of likely invariants,” Science
of Computer Programming , vol. 69, no. 1–3, pp. 35–45, Dec.
2007.
[7] R. Feinman, R. R. Curtin, S. Shintre, and A. B. Gardner, “Ad-
versarial machine learning at scale,” 2016, technical Report.
http://arxiv.org/abs/1611.01236.
[8] M. Fischetti and J. Jo, “Deep neural networks as 0-1 mixed
integer linear programs: A feasibility study,” CoRR , vol.
abs/1712.06174, 2017.
807
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:39 UTC from IEEE Xplore.  Restrictions apply. [9] C. Flanagan and K. R. M. Leino, “Houdini, an annotation
assistant for esc/java,” in Proceedings of the International
Symposium of F ormal Methods Europe on F ormal Methods
for Increasing Software Productivity , ser. FME ’01. Berlin,
Heidelberg: Springer-Verlag, 2001, pp. 500–517. [Online].
Available: http://dl.acm.org/citation.cfm?id=647540.730008
[10] P. Garg, C. L ¨oding, P. Madhusudan, and D. Neider, “ICE:
A robust framework for learning invariants,” in Computer
Aided V eriﬁcation - 26th International Conference, CA V
2014, Held as Part of the Vienna Summer of Logic, VSL
2014, Vienna, Austria, July 18-22, 2014. Proceedings , 2014,
pp. 69–87. [Online]. Available: https://doi.org/10.1007/978-
3-319-08867-9 5
[11] P. Garg, D. Neider, P. Madhusudan, and D. Roth,
“Learning invariants using decision trees and implication
counterexamples,” in Proceedings of the 43rd Annual
ACM SIGPLAN-SIGACT Symposium on Principles of
Programming Languages , ser. POPL ’16. New York,
NY , USA: ACM, 2016, pp. 499–512. [Online]. Available:
http://doi.acm.org/10.1145/2837614.2837664
[12] T. Gehr, M. Mirman, D. Drachsler-Cohen, P. Tsankov,
S. Chaudhuri, and M. T. Vechev, “AI2: safety and
robustness certiﬁcation of neural networks with abstract
interpretation,” in 2018 IEEE Symposium on Security and
Privacy, SP 2018, Proceedings, 21-23 May 2018, San
Francisco, California, USA , 2018, pp. 3–18. [Online].
Available: https://doi.org/10.1109/SP.2018.00058
[13] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining
and harnessing adversarial examples,” 2014, technical Report.
http://arxiv.org/abs/1412.6572.
[14] D. Gopinath, H. Converse, C. S. Pasareanu, and
A. Taly, “Property inference for deep neural networks,”
CoRR , vol. abs/1904.13215, 2019. [Online]. Available:
http://arxiv.org/abs/1904.13215
[15] R. Guidotti, A. Monreale, S. Ruggieri, F. Turini, F. Giannotti,
and D. Pedreschi, “A survey of methods for explaining black
box models,” ACM Comput. Surv. , vol. 51, no. 5, pp. 93:1–
93:42, Aug. 2018.
[16] G. Hinton, O. Vinyals, and J. Dean, “Distilling
the knowledge in a neural network,” CoRR ,
vol. arXiv:1503.02531, 2015. [Online]. Available:
https://arxiv.org/abs/1503.02531?context=cs
[17] X. Huang, M. Kwiatkowska, S. Wang, and M. Wu, “Safety
veriﬁcation of deep neural networks,” in CA V, 2017.
[18] P. jan Kindermans, K. T. Sch ¨utt, M. Alber, K.-R. M ¨uller,
D. Erhan, B. Kim, and S. D ¨ahne, “Learning how to ex-
plain neural networks: Patternnet and patternattribution,” in
International Conference on Learning Representation (ICLR) ,
2018.
[19] K. Julian, J. Lopez, J. Brush, M. Owen, and M. Kochenderfer,
“Policy compression for aircraft collision avoidance systems,”
inProc. 35th Digital Avionics System Conf. (DASC) , 2016,
pp. 1–10.[20] G. Katz, C. Barrett, D. Dill, K. Julian, and M. Kochenderfer,
“Reluplex: An efﬁcient SMT solver for verifying deep neural
networks,” in CA V, 2017.
[21] K. Leino, S. Sen, A. Datta, M. Fredrikson, and L. Li,
“Inﬂuence-directed explanations for deep convolutional net-
works,” in IEEE International Test Conference, ITC 2018,
Phoenix, AZ, USA, October 29 - Nov. 1, 2018 , 2018.
[22] Z. C. Lipton, “The mythos of model interpretability,” Queue ,
vol. 16, pp. 30:31–30:57, 2018.
[23] S. M. Lundberg and S.-I. Lee, “A uniﬁed approach to inter-
preting model predictions,” in Advances in Neural Informa-
tion Processing Systems (NIPS) , 2017, pp. 4765–4774.
[24] K. L. McMillan, “Interpolation and model checking,” in
Handbook of Model Checking. , 2018, pp. 421–446. [Online].
Available: https://doi.org/10.1007/978-3-319-10575-8 14
[25] “The MNIST database of handwritten digits Home Page,”
http://yann.lecun.com/exdb/mnist/.
[26] S. Moosavi-Dezfooli, A. Fawzi, and P. Frossard, “DeepFool:
A simple and accurate method to fool deep neural networks,”
inCVPR , 2016.
[27] N. Papernot, P. D. McDaniel, S. Jha, M. Fredrikson, Z. B.
Celik, and A. Swami, “The limitations of deep learning in
adversarial settings,” in EuroS&P , 2016.
[28] M. T. Ribeiro, S. Singh, and C. Guestrin, “”Why Should I
Trust You?”: Explaining the Predictions of Any Classiﬁer,”
inProceedings of the 22nd ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, San
Francisco, CA, USA, August 13-17, 2016 , 2016, pp. 1135–
1144.
[29] ——, “Anchors: High-precision model-agnostic explana-
tions,” in Proceedings of the Thirty-Second AAAI Conference
on Artiﬁcial Intelligence, New Orleans, Louisiana, USA,
February 2-7, 2018 , 2018, pp. 1527–1535.
[30] G. T. W.-C. L. X. Z. Shiqing Ma, Yingqi Liu, “Nic: Detecting
adversarial samples with neural network invariant checking,”
inNDSS , 2019.
[31] A. Shrikumar, P. Greenside, A. Shcherbina, and A. Kundaje,
“Not just a black box: Learning important features through
propagating activation differences,” CoRR , 2016.
[32] K. Simonyan, A. Vedaldi, and A. Zisserman, “Deep inside
convolutional networks: Visualising image classiﬁcation mod-
els and saliency maps,” CoRR , 2013.
[33] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution
for deep networks,” in ICML , 2017.
[34] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna,
D. Erhan, I. Goodfellow, and R. Fergus, “Intriguing
properties of neural networks,” 2013, technical Report.
http://arxiv.org/abs/1312.6199.
808
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:39 UTC from IEEE Xplore.  Restrictions apply. [35] S. Wang, K. Pei, J. Whitehouse, J. Yang, and S. Jana,
“Efﬁcient formal safety analysis of neural networks,” in Ad-
vances in Neural Information Processing Systems 31: Annual
Conference on Neural Information Processing Systems 2018,
NeurIPS 2018, 3-8 December 2018, Montr ´eal, Canada. ,
2018.
[36] ——, “Formal security analysis of neural networks using
symbolic intervals,” in 27th USENIX Security Symposium,
USENIX Security 2018, Baltimore, MD, USA, August 15-17,
2018. , 2018.
809
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:39 UTC from IEEE Xplore.  Restrictions apply. 