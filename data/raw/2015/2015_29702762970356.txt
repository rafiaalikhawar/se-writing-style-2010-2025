How Good Are the Specs? A Study of the Bug-Finding
Effectiveness of Existing Java API Speciﬁcations
Owolabi Legunsen, Wajih Ul Hassan, Xinyue Xu, Grigore Ro¸ su , and Darko Marinov
Department of Computer Science
University of Illinois at Urbana-Champaign, USA
{legunse2,whassan3,xxu52,rosu,marinov}@illinois.edu
ABSTRACT
Runtime veriﬁcation can be used to ﬁnd bugs early, during
software development, bymonitoring test executions again st
formal speciﬁcations (specs). The quality of runtime veri-
ﬁcation depends on the quality of the specs. While previ-
ous research has produced many specs for the Java API,
manually or through automatic mining, there has been no
large-scale study of their bug-ﬁnding eﬀectiveness .
We present the ﬁrst in-depth study of the bug-ﬁnding
eﬀectiveness of previously proposed specs. We used Java-
MOP to monitor 182 manually written and 17 automati-
cally mined specs against more than 18K manually written
and 2.1M automatically generated tests in 200 open-source
projects. The average runtime overhead was under 4 .3×.
We inspected 652 violations of manually written specs and
(randomly sampled) 200 violations of automatically mined
specs. We reported 95 bugs, out of which developers al-
ready ﬁxed 74. However, most violations, 82.81% of 652
and 97.89% of 200, were false alarms.
Our empirical results show that (1) runtime veriﬁcation
technology has matured enough to incur tolerable runtime
overhead during testing, and (2) the existing API speciﬁca-
tions can ﬁnd many bugs that developers are willing to ﬁx;
however, (3) the false alarm rates are worrisome and sug-
gest that substantial eﬀort needs to be spent on engineering
better specs and properly evaluating their eﬀectiveness.
CCS Concepts
•Software and its engineering →Software testing
and debugging;
Keywords
runtime veriﬁcation, speciﬁcation quality, empirical stu dy
1. INTRODUCTION
In runtime veriﬁcation, the execution of a software system
is dynamically checked against formal speciﬁcations (spec sfor short) [ 6,8,10,15,19,35–37]. At a high level, the pro-
gram being monitored is instrumented to capture, as events,
method calls and ﬁeld updates that are related to the specs
being checked. Then, at runtime, the instrumented program
creates listener objects, commonly referred to as monitors ,
which check that the events conform to the specs and re-
portviolations when some spec is violated. In this paper, a
“spec”refers to a behavioral speciﬁcation, deﬁned by Robil -
lard et al. [ 55] as“a way to use an API as asserted by the
developer or analyst, and which encodes information about
the behavior of a program when an API is used” . A spec
violation indicates that some API is used in a way that is
not consistent with its usage guideline, but such violation
may or may not be a real bug in the code.
The potential for using runtime veriﬁcation during soft-
ware testing was previously recognized [ 24,27,31,35], but
combining testing with runtime veriﬁcation of multi-objec t
parametric specs, required by object-oriented API specs,
only recently became practically feasible, thanks to resea rch
and development progress on (i) making parametric spec
runtimeveriﬁcation more eﬃcient [ 8,16,23,35,38,61], (ii) be-
ing able to monitor many specs simultaneously [ 2,52], and
(iii) better-engineered runtime veriﬁcation tools [ 5,24]. We
recently proposed to combine runtime veriﬁcation with re-
gression testing, wheretest executionsaremonitored agai nst
formal specs to ﬁnd bugs during software evolution [ 32].
The quality of specs has generally been taken for granted
in the runtime veriﬁcation research community, where the
major research direction over the last decade has been to
improve the eﬃciency and scalability of algorithms, tech-
niques, and tools. The specs used in previous research were
manually written [ 1,6,23,35] or automatically mined [ 3,9,13,
17,28,30,33,34,39,40,45–49,54,58,62,63]. These specs were
monitored to measure their runtime overhead. However, for
ﬁndingbugsbycombiningruntimeveriﬁcationwithsoftware
testing, the eﬀectiveness of these specs becomes critical.
In this paper, we present the ﬁrst in-depthinvestigation of
the eﬀectiveness of existing specs for ﬁnding bugs. We con-
sider a spec eﬀective for bug ﬁnding if it can catch true bugs
but does not generate too many false alarms. We consider
specs of the standard Java API because such specs can po-
tentially ﬁnd bugs in many projects across various domains,
require no domain knowledge, and the runtime veriﬁcation
tool that we evaluate, JavaMOP [ 21,24,35], works for Java.
We evaluate 199 existing manually written and automati-
cally mined specs. Speciﬁcally, we use 182 manually writ-
ten specs that were formalized directly from the Java API
documentation [ 31] and used in previous studies on the eﬃ-
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a
fee. Request permissions from Permissions@acm.org.
ASE’16 , September 3–7, 2016, Singapore, Singapore
c2016 ACM. 978-1-4503-3845-5/16/09...$15.00
http://dx.doi.org/10.1145/2970276.2970356
602
ciency and scalability of runtime veriﬁcation [ 32,35,52]. We
also use 17 specs that were mined automatically from large
traces [47] and were used in spec mining studies [ 48,49].
Our work diﬀers from previous evaluations of specs in the
runtime veriﬁcation and spec mining literature in three ma-
jor ways. First, previous runtime veriﬁcation studies most ly
focused on the eﬃciency of monitoring, but we focus on
theeﬀectiveness question: “How good are the specs?” Sec-
ond, most previous evaluations were conducted on the Da-
Capo benchmarks [ 4] (with at most 14 projects) or with a
smaller number of open-source projects; in contrast, we use
200 open-source projects. Our results thus provide fresh in -
sights to researchers in both runtime veriﬁcation and spec
mining communities, because our evaluation is based on a
substantially larger set of more diverse projects. We belie ve
that evaluating specs on (current) open-source projects in -
stead of (old) benchmarks can be more representative for
assessing the eﬀectiveness of specs from developers’ point of
view and should be strongly considered in future evaluation s
of specs. Third, in many previous studies, researchers as-
sumed that any spec violations were bugs, or decided them-
selves what was a bugor not, butwe submit bugreports and
ﬁxes (i.e., pull requests ) to let the developers be the judges
of the bugs we discovered by inspecting spec violations.
In our experiments, we monitored the 182 manually writ-
ten and 17 automatically mined specs while running 18065
manually written and 2135081 automatically generated test s
in 200 open-source projects, manually inspected a subset of
the spec violations, and sent pull requests for the violatio ns
that we believed to be bugs. On the positive, the average
runtime overhead of monitoring was under 4 .3×, and devel-
opers already ﬁxed 74 of 95 bugs that we reported. On the
negative, we found a large rate of false alarms among the
inspected violations. We inspected 652 of 5263 violations
of manually written and 200 of 1141 violations of automat-
ically mined specs, and observed overall false alarm rates
of 82.81% and 97.89%, respectively. Further, only a small
fraction of the specs led to the discovery of bugs—11 of 182
manually written and 3 of 17 automatically mined specs—
and even among these, the average false alarm rates were
high, 45.51% and 96.69%, respectively. We reported several
issues about the existing specs, and JavaMOP maintainers
already corrected some, but in many cases, the specs appear
completely ineﬀective and should not be used at all.
Inspecting spec violations and submitting pull requests to
developers took an estimated 1,200 hours and was challeng-
ing for three reasons. First, understanding the root cause
of a violation is non-trivial. Although JavaMOP reports
the line number for each spec violation, reasoning about a
change that could correct the violation often requires deep er
understanding of the code (and we were not developers on
any of the 200 open-source projects); moreover, some of the
violations were in third-partylibraries, so we neededto co m-
prehend parts of those libraries as well. Second, it is chal-
lenging to decide what constitutes an actual bug that should
be submitted to the developers. At one extreme, we could
only submit violations that can lead to program crashes.
At the other extreme, we could simply submit every vio-
lation to the developers and see what they say, but this
could unnecessarily burden the developers (who may then
blacklist us or start to“desk-reject”our pull requests if t hey
feel those are mostly useless to them). Even between these
two extremes, it is debatable how to classify so-called“cod esmells”[17,39,49]which mayindicate APImisunderstanding
by developers but are harmless in the current version of the
code, e.g., calling close()on anOutputStream instance for
whichclose()is a no op. Third, preparing a pull request in
a way that developers would ﬁnd useful requires substantial
eﬀort (another reason to not even attempt to submit every
violation), and sometimes involved multiple internal iter a-
tions before submission. For all these reasons, we chose to
report to the developers those cases where at least one of the
authors believed that a violation indicated some problem in
the current version of the code.
The results from our study show that the eﬀort spent by
the runtime veriﬁcation community over the last decade on
improving the performance of simultaneous monitoring of
parametric specs has paid oﬀ. Indeed, the technology has
matured enough to incur acceptable runtime overhead when
monitoring test executions in open-source projects agains t
dozens of specs. Also, the existing API specs from prior
runtime veriﬁcation and spec mining research can ﬁnd many
bugs that developers are willing to ﬁx. However, the false
alarm rates are worrisome and suggest that there is a need
for the research community to fundamentally re-think spec
ﬁnding and“spec engineering”approaches, towards making
runtime veriﬁcation a more eﬀective early-stage, bug-ﬁndi ng
aid that developers can use.
This paper makes the following contributions:
⋆Large-Scale Evaluation. Wepresenttheﬁrstlarge-scale
evaluation of runtime veriﬁcation during software testing ,
with 199 specs and 200 open-source projects. The results
show that monitoring has acceptable overhead duringtest-
ing and can ﬁnd important bugs, but the specs are largely
ineﬀective and generate way too many false alarms.
⋆Analysis of Eﬀectiveness. We analyze reasons for bug-
ﬁnding eﬀectiveness of existing specs, in particular, the
high rates of false alarms, and discuss developers’ feedbac k
on our pull requests.
⋆Recommendations and Data. We provide a set of rec-
ommendations that can help the research community engi-
neer more eﬀective specs and better evaluate these specs.
We also make data from our study publicly available [ 57].
2. BACKGROUND
We brieﬂy describe runtime veriﬁcation of specs in Java-
MOP [10,21,24,35,36].Collection_SynchronizedCollection
(CSC), shown in Figure 1, is one of the specs in our study. CSC
was proposed by Bodden et al. [ 7] (called ASyncIteration ) to
check for cases where a synchronized Collection ’sIterator
is accessed from non-synchronized code. Figure 1shows the
three parts of a JavaMOP spec: lines 3–10deﬁneeventsrel-
evant at runtime, line 11is the formal property to monitor
over the events, and line 12shows user-deﬁned handlercode
that JavaMOP invokeswhen the monitored program reaches
a certain state, i.e., when the spec is violated.
Each spec is parameterized by the types of objects whose
instances may generate the events. Speciﬁcally, CSCis pa-
rameterized (line 1) byCollection c andIterator i , which
means that one monitor object will be created at runtime for
everypairofrelated candi. Thecreation keywordindicates
that a monitor will be created after the syncevent occurs
(i.e., when one of the synchronized* methods on line 4is
invoked on a Collection ). The monitor subsequently listens
for the events syncMk(line5),asyncMk(line7), andaccess
6031Collections SynchronizedCollection( Collection c , Iterator i ) {
2Collection c ;
3creation event syncafter()returning ( Collection c) :
4call(∗Collections . synchronizedCollection( Collection)) ||. . ./∗more calls ∗/){this. c = c ; }
5event syncMk after( Collection c) returning ( Iterator i ) :
6call(∗Collection+.iterator () ) && target (c) && condition(Thread. holdsLock(c)) {}
7event asyncMk after( Collection c) returning ( Iterator i ) :
8call(∗Collection+.iterator () ) && target (c) && condition (! Thread. holdsLock(c)) {}
9event access before ( Iterator i ) :
10call(∗Iterator . ∗(..) ) && target ( i ) && condition (! Thread . holdsLock( this. c)){}
11ere: (sync asyncMk) |(sync syncMk access )
12@match {RVMLogging. out . println ( Level .CRITICAL, DEFAULT MESSAGE) ; . . . /∗more printing ∗/}
13}
Figure 1: Example spec, Collections SynchronizedCollection (CSC), with its events and propert y
1im = Collections . synchronizedList ( .. . ) ;
2+synchronized (im){
3for(IInvokedMethod iim : im) {
4ITestNGMethod tm = iim . getTestMethod() ;
5 . . .}
6+}
Figure 2: Buggy code in TestNG
1Speciﬁcation Collections SynchronizedCollection has been violated on line
org.testng.reporters.SuiteHTMLReporter.generateMeth odsChronologically
(SuiteHTMLReporter.java:365). Documentation for this prop erty can be
found at https://runtimeveriﬁcation.com/monitor/
annotated-java/ properties/html/java/util/Collections
SynchronizedCollection.html
2A synchronized collection was accessed in a thread −unsafe manner.
Figure 3: A sample violation
(line9). ThesyncMkevents occur after iterator() is invoked
on aCollection instance, c, to create an Iterator,i, and the
thread did synchronize on c(lines5–6). TheasyncMkevents
occur after iterator() is invoked on c, but the thread did
not synchronize on c(lines7–8). Finally, the accessevents
occur before any invocation of Iterator methods on ifrom
any thread that did not synchronize on c(lines9–10).
If the monitored program reaches a state where the ex-
tendedregularexpression( ere)propertyonline 11ismatched,
thenthe handlercode on line 12is invoked. The erematches
when non-synchronizedcode creates an Iterator from a syn-
chronized Collection (sync asyncMk ) orwhenaccessingasyn-
chronized Collection ’sIterator from non-synchronized code
(sync syncMk access ). In our experiments, we used the de-
fault handler in JavaMOP: print a violation containing the
spec name, the program line number where the spec viola-
tion occurred, a URL for the spec, and an explanation.
As an example, consider the buggy code in Figure 2, sim-
pliﬁed from one of the six bugs we found in TestNG. The
lines not starting with“+”( 1and3–5) represent part of the
original code that iterates over the synchronized Collection
im. Note that the forloop is not synchronized, leading to a
violation of the CSCspec. The violation that JavaMOP re-
ports is shown in Figure 3; our inspection starting from this
reported line of code led us to ﬁnd the bug. The developers
accepted our pull request that added the synchronization
code, in the lines starting with“+”( 2and6).
3. EXPERIMENTAL SETUP
We describe the open-source projects used in our study,
the specs that were monitored while running tests in theTable 1: Statistics of 200 projects used in our study
PID Project SHA LOCManTests AutoTests
P1 Altoros.YCSB bfcfe23a 7290 1 –
P2 LogBlock.LogBlock-2 40548aad 875 1 –
P3 edanuﬀ.CassandraCompositeType 6d09cceb 1234 1 5427
P4 jriecken.gae-java-mini-proﬁler 80f3a59e 908 8 92058
P5 mqtt f4384253 11478 18 –
P6 plista.kornakapi 178061c3 3088 2 21594
P7 threerings.playn c969160c 38388 139 –
P8 tbuktu.ntru 8126929e 7715 70 –
P9 OpenGamma.ElSql db6c6d07 2581 160 11034
P10 sematext.ActionGenerator 10f4a3e6 1864 7 –
P11 vivin.GenericTree 15c59c99 677 49 7787
P12 hoverruan.weiboclient4j 6ca0c73f 8748 34 1229
P13 joda-time cc35fb2e 85000 4157 12123
P14 IvanTrendaﬁlov.Confucius 2c302878 1203 84 23196
P15 mikebrock.jboss-websockets fd03a4ef 1736 1 6668
P16 b3log.b3log-latke afb48c40 24399 76 –
P17 Thomas-S-B.visualee 410a80f0 3574 76 8164
P18 asterisk-java b07617fe 39498 220 33632
P19 Cue.lucene-interval-ﬁelds 8f8bﬀ6d 736 9 13162
P20 JSqlParser 001d665d 10517 341 14837
P21 Ovea.jetty-session-redis afb2b25b 6358 7 15414
P22 bcel 24014e5e 35827 87 –
P23 zookeeper-utils a2b80474 455 4 633
P24 bucchi.OAuth2.0ProviderForJava db5e1d06 2654 47 –
P25 htrace c32ec0b1 2521 11 –
P26 ptgoetz.storm-jms d152d72f 1085 2 –
P27 UrbanCode.terraform d67ac40c 12108 4 3069
P28 pignlproc 1a609980 2296 19 53693
P29 jmxtrans.embedded-jmxtrans 4f1ce2cc 5806 56 –
P30 apache.gora bb09d891 24185 56 –
F69 69 projects with 100% FAR various 349029 3834 561031
N101 101 projects without violations various 520472 8484 1250330
TOTAL 1214305 18065 2135081
AVG 6071.52 90.33 17500.66
MIN 24 1 1
MAX 93260 4157 219404
projects, and how we automatically generated tests using
Randoop [ 42,44,53]. We also explain our procedure for run-
ning JavaMOP and for inspecting resulting violations.
3.1 Experimental Subjects
We selected the projects for our studyfrom GitHub, start-
ing from a list of the most popular Java projects. From
these, we selected 200 projects that (i) used Maven (for ease
of automation), (ii) had at least one test (so we can mon-
itor test runs), (iii) had all tests pass without monitoring ,
and (iv) had all tests pass when monitoring with JavaMOP.
Requirements(iii) and (iv) are important to havea fair mea-
surement of runtime overhead of JavaMOP—if tests were to
fail between the two runs, with and without monitoring,
they may fail at diﬀerent points in the execution, leading
to rather diﬀerent time measurements. Furthermore, tests
could fail due to problems in the project or due to inte-
gration of JavaMOP. For example, we observed some fail-
ures of time-sensitive tests that have some timeouts result -
ing from the overhead of JavaMOP. We also observed test
failures that happened because JavaMOP instrumentation
interacted unexpectedly with some other instrumentation
frameworks, e.g., test-mocking frameworks. We already re-
ported some of these issues to the JavaMOP project [ 57].
Table1lists some basic statistics about the 200 projects
used in our study. PID either starts with“P”to provide the
604short ID of a project in which we found some real bug, or
summarizes multiple projects with similar characteristic s—
“F69” summarizes 69 projects in which all inspected viola-
tions were false alarms, and“N101”summarizes 101 projects
in which no violations were generated for the specs that
we inspected. Project is the project name, SHA is the
project revision we used, LOC is the number of Java lines
in the project, ManTests is the number of manually written
tests, and AutoTests is the number of automatically gener-
ated tests. “–” marks that we did not have Randoop tests,
which happened for 49 projects with multiple Maven mod-
ules, 16 projects where generated tests did not compile, and
13 projects where Randoop did not generate any test within
the time limit. For F69 and N101, ManTests and AutoTests
show the sums for all respective projects. The rows TOTAL,
AVG, MIN, and MAX are the sum, average, minimum, and
maximum across all projects in each column.
3.2 Specs Used in this Study
All Java API specs that we used in our study were ob-
tained from the literature, 182 manually written specs [ 31,
35] and 17 automatically mined specs [ 49,50]. We describe
our rationale and procedure for selecting each set of specs.
3.2.1 Manually Written Specs
We used 182 manually written JavaMOP specs [ 32,35],
which are publicly available [ 51]. The specs were written by
Lee et al. [ 31], who read Javadoc comments in four widely-
used packages ( java.lang ,java.net,java.io, andjava.util )
and formalized sentences describing “must”, “should” or “i s
better to”conditions. The specs are formalized using ﬁnite -
state machines, extended regular expressions, linear temp o-
ral logic, andcontext-freegrammars. JavaMOP canmonitor
specs in any formalism for which a suitable plugin exists.
To illustrate manual formalization of specs, consider agai n
theCSCspec [12] from Section 2. It was formalized from text
inCollections.synchronizedCollection() method’s Javadoc:
“It is imperative that the user manually synchronize on the
returned collection when iterating over it ... Failure to fo llow
this advice may result in non-deterministic behavior” [22].
Section2explained CSCin detail. As mentioned, this spec
had been also used earlier [ 7]; by analyzing Javadoc com-
ments, Lee et al. [ 31] ended up with some of the same specs
that others had formalized before. Monitoring CSCin our
experiments revealed bugs in several widely used projects,
including TestNG,ActiveMQ, andXStream. However, our ex-
periments also revealed issues and opportunities for impro v-
ing the manually written specs, discussed in Section 5.2.
3.2.2 Automatically Mined Specs
To compare the eﬀectiveness of manually written specs
and automatically mined specs, we monitored 17 of the 223
specs automatically mined by Pradel et al. [ 45,47,49,50].
Before settlingonthese specs, weperformed amini-surveyo f
the spec mining literature to search for specs and to identif y
how spec mining was evaluated.
Paper Search: We searched for spec mining papers on
DBLP [14] using this query: specification|propert|contrac
t|invariant|precondition mining|monitor|enforce|infe r|mi
ne venue:ICSE|venue:ASE|venue:RV|venue:PLDI|venue:PO PL|v
enue:ISSTA|venue:ieee_trans_software_eng_tse_|venue :sigs
oft_fse|venue:autom_softw_eng_ase_|venue:esec_sigso ft_fs
e|venue:tacas|venue:icsm|venue:icsme|venue:sas|venu e:sacTable 2: Mini-Survey. Ref: references; Subjects:
kind of subjects; OSS: open-source projects; Sel-
Classes: selected classes; #Sub: number of subjects;
FAR[%]: false alarm rate reported; #Bugs: number
of bugs found; Rep?: bugs reported to developers?
Ref Subjects #Sub FAR[%] #Bugs Rep?
[46]DaCapo+OSS 12 n/a n/a n/a
[54]n/a n/a n/a n/a n/a
[28]n/a 8 n/a n/a n/a
[39]DaCapo 743.00 20no
[13]OSS+JDK 7 n/a n/a n/a
[63]OSS 573.90 100 yes
[62]OSS 8 n/a 1no
[48]DaCapo 10 0.00 54no
[33,34]Sel-Classes 3 n/a n/a n/a
[60]OSS 658.00 9yes
[9]OSS 4 n/a n/a n/a
[40]OSS 3559 n/a n/a n/a
[17]DaCapo 1170.00 11no
[3]DaCapo 1 n/a n/a n/a
[29]OSS 75.00 265 no
[49]DaCapo 1249.00 26no
[58]Sel-Classes 15 n/a n/a n/a
|venue:paste|venue:icfem|venue:issre|venue:compsac| venue
:formats|venue:sttt|venue:ecoop|venue:fase|venue:oo psla_
companion|venue:kdd|venue:vmcai|venue:seke|venue:ca v|ven
ue:oopsla|venue:electr_notes_theor_comput_sci_entcs _
We obtained 163 potentially related papers, of which we
considered only the 100 papers published in 2009–2015.
Paper Filtering: Wesplitthese100papersinhalf, andtwo
of the authors read abstracts from each half independently
to ﬁnd papers that mined Java API specs that we could
use. We omitted related papers, e.g., a survey [ 55], which
did not report ﬁnding new specs. The result was 26 papers
that we then read in more detail to answer these questions:
(i) in what formalism are the mined specs (and can they be
monitored with JavaMOP)? (ii) how many specs did they
mine? (iii) did they ﬁnd any bugs? (iv) do they report false
alarms from evaluating the bug-ﬁnding eﬀectiveness of the
specs? (v) what is the reported false alarm rate, if any?
Email to Authors: After ﬁltering, we settled on 17 papers
and emailed authors who are not at our institution to ask
for their mined specs. We received responses from authors
of 7 papers, with 5 providing their specs. Of these 5, the
specs from Pradel et al. [ 49] had the largest number that we
could easily use—their specs were provided in the DOT for-
mat, which was straightforward to automatically translate
to ﬁnite-state machines in the JavaMOP syntax.
Prior Evaluations: Table2lists the 17 papers. Although
7 papers report ﬁnding bugs while evaluating mined specs,
only 2 report conﬁrming the bugs with the developers. Fur-
ther, evaluations were mostly performed on DaCapo, the
benchmarkinitiallycuratedtoevaluateperformance andno t
bug-ﬁnding eﬀectiveness, and on a small number of open-
source projects, with the exception of Nguyen et al. [ 40] who
used thousands of projects but only toapply statistical tec h-
niquestominespecsandnottoevaluatetheirbug-ﬁndingef-
fectiveness. Finally, among the 7 papers that reported fals e
alarm rates, the rates varied widely, from 0.0% to 73.9%.
Our experiments are therefore complementary to those in
the papers we surveyed. In fact, we ﬁnd even higher false
alarms rates. Our experiments also revealed issues in the
automatically mined specs, as discussed in Section 5.
6053.3 Runtime Veriﬁcation with JavaMOP
Using JavaMOP to monitor test runs is quite simple: in-
tegrate JavaMOP in the project and invoke mvn test . Java-
MOP integration in Maven-based projects is described on-
line [20]. First, the JavaMOP compiler generates a Java
agent [41] from the specs to be monitored, enabling dynamic
instrumentation of code running in the Java Virtual Ma-
chine. Next, the build conﬁguration ﬁle, pom.xml, is modi-
ﬁedtomaketheMavenSureﬁreplugin(whichrunsthetests)
aware of the JavaMOP agent. Subsequentinvocations of mvn
testattach the JavaMOP agent to the test-running process
for monitoring the runs against all the specs simultaneousl y.
We automated JavaMOP agent creation, changing pom.xml,
monitoring each project, and post-processing results.
In each set of experiments, we ran the tests in each project
twice. First, we ran without JavaMOP to measure the base
test-running time and check that the tests pass by them-
selves. We then integrated JavaMOP and reran the tests
to measure test-running time with monitoring and to record
violations. We conﬁgured JavaMOP to log all output to a
ﬁle. We excluded from monitoring standard Java libraries
(that are less likely to have bugs) and some third-party li-
braries, such as Maven Sureﬁre (to reduce overhead) and
test-mocking frameworks (which we found to have unex-
pected interactions with JavaMOP, as mentioned in Sec-
tion3.1). All JavaMOP experiments were run on a 64-bit
computer with IntelR/circlecopyrtCoreTMi7-3770K CPU @ 3.50GHz
processor and 32GB of RAM running Ubuntu 14.04.4 LTS
and Java 7 or 8 (as required by the project).
3.4 Automatically Generating Tests
To evaluate whether the type of tests impacts the bug-
ﬁnding eﬀectiveness of the specs, we used Randoop [ 42,44,
53] toautomatically generate additional tests. We generated
tests on a CoreTMi7-4700MQ 2.40GHz Quad-Core proces-
sor PC with 8GB of RAM, Ubuntu 15.04, Java 7 or 8, and
Randoop heap usage limited to 4GB. We ran Randoop on all
151 single-module Maven projects (out of total 200), which
were easier to automate than multi-module Maven projects.
We limited test-generation time to 1min and 5min. We had
a separate run to monitor the generated tests (using Java-
MOP) against the same set of manually written specs. The
number of newviolations, i.e., those which were not already
reported while monitoring manually written tests, showed
little diﬀerence between the tests automatically generate d in
1min and 5min. Therefore, we decided to use the tests gen-
erated in 5min and did not increase the time limit further.
Other researchers who used Randoop also found tests gen-
erated in diﬀerent intervals to behave similarly [ 43,56,59].
3.5 Inspecting Violations
We describe our procedure for selecting and inspecting vi-
olations that JavaMOP reported while monitoring test runs.
We refer to the source-code line number at which JavaMOP
reports a spec violation as the violation site. JavaMOP re-
ports a violation every time a spec is violated at runtime,
so it can report many violations of the same spec at the
same site (e.g., if the site is in a loop or invoked from mul-
tiple tests). We refer to all violations that are reported by
JavaMOP during test execution as dynamic violations (DV)
and we refer to unique violations—those that happen in the
same project, for the same spec, and at the same site—as
static violations (SV) .We inspected some static violations from both manually
written and automatically generated specs. For manually
written specs, we inspected all violations from 42 specs and
ignoredallviolationsfrom21specs. Forautomaticallymin ed
specs, we sampledtoinspect200outof1141violations ofthe
17 automatically mined specs that we monitored. To sample
200 violations, we used stratiﬁed sampling [ 11]: we divided
all violations into strata based on the spec, and from each
stratumrandomlyselectedanumberofviolations, inpropor -
tion to the ratio of the stratum’s size to the total number of
violations. We excluded 21 manually written specs from in-
spection and did not monitor 206 automatically mined specs
because of issues with these specs, discussed in Section 5.2.
Our inspection goal was to ﬁnd as many bugs as possi-
ble while increasing the chance that the developers accept
the resulting pull requests. Therefore, multiple authors i n-
spected most violations. For manually written tests and
manually written specs, ﬁrst, two reviewers independently
inspected each violation and classiﬁed it as one of:
TrueBug: A potential bug to be conﬁrmed by reporting to
the developers or by checking if it was already ﬁxed;
FalseAlarm: The violation does not indicate a bug in the
code but eﬀectively a bug/imprecision in the spec; or
HardToInspect: The violation is hard to classify as a
TrueBug or a FalseAlarm, because source code is missing
or is particularly hard to reason about.
Next, the independent reviewers met to discuss and agree
ontheclassiﬁcations theyhadindependentlyassignedandt o
resolve cases in which one reviewer had classiﬁed a violatio n
as a TrueBug but the other had given another classiﬁca-
tion. Cases where they still could not agree were classiﬁed
as TrueBug if any one of the reviewers had classiﬁed as a
TrueBug. A third reviewer then met with the two initial re-
viewers to conﬁrm all violations that were classiﬁed as True -
Bugs. For automatically mined specs, we followed a similar
procedure: two reviewers inspected each violation reporte d
from monitoring automatically mined specs while running
manually written tests. For automatically generated tests ,
only one reviewer inspected each violation because we had
built enough experience from inspecting the violations fro m
manually written tests.
For each violation that we classiﬁed as a TrueBug, we
submitted a bug report and/or a ﬁx (pull request) to the
developers of the respective project to check whether they
agree that a code change can be beneﬁcial. As discussed in
Section1, inspecting violations and submitting pull requests
to developers is challenging. For inspections alone, each o f
the two initial reviewers spent between 4min and 54min per
violation. Summing up all the time to meet for resolving
disagreements, to prepare pull requests, to iterate over th em
internally, to communicate with developers, and to record
and process the status of each pull request, we estimate that
it took over 1,200 hours just for this process of inspecting
and creating pull requests.
We carefully prepared pull requests, trying to obtain an
“upper bound” on the eﬀectiveness of the specs. That is,
some violations that we classiﬁed as TrueBugs may have
been ignored by developers runninga tool on their own or in
the absence of our pull requests. We did not simply submit
bug reports indicating the violation of a spec in a codebase;
we were concerned that developers may not understand the
spec or care to change the code. Instead, we submitted pull
requests that included a proposed code change.
606Table 3: Dynamic (DV) and static (SV) violations,
and overhead for 42 manually written Specs. Man-
Tests: manually written tests; AutoTests: automat-
ically generated tests; PID, TOTAL, AVG, MIN,
MAX, “–”: same as in Table 1
PIDManTests AutoTests
DV SVOverhead[%] DV SV
P1 13 4 187.93 ––
P2 11 50.96 ––
P3 20 2 110.72 00
P4 00 157.72 20 1
P5 412 2 -28.37 ––
P6 24 2 155.36 00
P7 11 201.39 ––
P8 27 7 27.88 ––
P9 384 9 239.98 00
P10 961 3 128.17 ––
P11 26 5 128.86 71
P12 00 248.97 558 16
P13 236 95 245.26 00
P14 74 1 123.09 75242 9
P15 00 2.30 287 3
P16 167 13 72.25 ––
P17 3713 126.38 18 1
P18 21 104.53 6717 6
P19 746 5 284.76 12520 3
P20 27977 1 105.98 1493 3
P21 21 4 324.51 7241 4
P22 181430 4 338.72 ––
P23 1038 16 67.46 00
P24 88 5 228.58 ––
P25 3110 69.88 ––
P26 75 51.50 ––
P27 00 84.23 1322 8
P28 414 13 14.57 00
P29 2913 4.30 ––
P30 467 29 616.59 ––
F69 85120 269 13297.49 96111 64
N101 00 8106.04 00
TOTAL 299753 533 25877.95 201536 119
AVG 1498.77 2.67 129.39 1651.93 0.98
MIN 00 -28.37 00
MAX 181430 95 1036.57 75242 16
4. RESULTS
We aim to evaluate the eﬀectiveness of existing specs for
ﬁnding bugs when monitoring tests in open-source projects.
We investigated the following research questions (RQs):
RQ1What is the runtime overhead of monitoring?
RQ2How many bugs are found from violations?
RQ3What are the false alarm rates among violations?
4.1 RQ1: Runtime Overhead of Monitoring
Table3shows the runtime overhead (Overhead[%]) from
monitoring 42 manually written specs. We measured over-
head only for manually written (and not automatically gen-
erated) tests, because they pass in all 200 projects (while
some automatically generated tests fail, making it hard to
reliably measure overhead). Runtime overhead is computed
as (mop−base)/base∗100%, where mopis the time to run
testswithmonitoring, and baseis time to run the tests with-
outmonitoring. As in previous JavaMOP studies, we ob-
served some negative runtime overheads, e.g., in P5. These
can be due to noise in the time measurements or due to the
instrumentation changing the garbage-collection behavio r of
the program, causing it to run faster [ 23,25,36].
The average runtime overhead was 129.39% when mon-
itoring only the 42 inspected specs (as shown in the ta-
ble) and 330.14% when monitoring all 182 manually written
specs(elidedforlackofspace). Therefore, theoverheadof si-multaneously monitoring all specs is under 4 .3×on average.
We believe this runtime overhead is acceptable duringdevel -
opment time (not production time), considering the number
of bugs we found and the fact that the tests in these projects
run relatively fast—the average additional time incurred b y
JavaMOP was 4.08sfor 42 specs and 12.48sfor 182 specs.
The relatively small average overhead reﬂects the tremen-
dous progress made in the research community over the last
decade to make runtime veriﬁcation more eﬃcient.
Table3also shows the number of dynamic (DV) and
static (SV) violations from monitoring 42 manually writ-
ten specs on both manually written tests (ManTests) for all
200 projects and automatically generated tests (AutoTests )
for 122 projects (of the 200 projects, 151 were single-modul e
Maven projects, but the tests generated by Randoop did not
compile in 16 projects, and Randoop did not generate any
test in 5min for 13 projects). Even when DV is relatively
high, the overhead remains reasonable.
4.2 RQ2: Bugs Found
We found a total of 114 SV that were TrueBugs, 110 for
manually written specs and 4 for automatically mined specs.
Recall thatwemapdynamictostatic violations basedonthe
project being monitored, the spec being violated, and the
violation site. When multiple projects use the same library
(even if not the exact same version), then multiple static
violations can actually map to the same bug. Our 114 True-
Bugs map to 97 unique bugs. Because most projects evolved
since we started our experiments (with then latest revision s
of the projects), 2 of the unique bugs we found were already
ﬁxed in the current latest revisions. For the remaining 95
bugs, we submitted pull requests, with 74 already accepted
and only 3 rejected; the remaining 18 are still pending.
4.3 RQ3: False Alarm Rates
A key metric to evaluate the eﬀectiveness of specs is the
false alarm rate (FAR), i.e., the ratio FA/(TB+FA), where
FAandTBare the number of FalseAlarms and TrueBugs
among inspected violations. For manually written specs, we
inspected 652 violations—533 from manually written tests
and 119 from automatically generated tests. Table 4shows,
for each project in which we inspected violations, the proje ct
ID (PID), number of inspected static violations (SV), num-
ber of violations in each classiﬁcation (HTI, TB, and FA),
and false alarm rate (FAR[%]). All 69 projects in F69 have
100% FAR (no TrueBugs) and had slightly more violations
than all those with TrueBugs. 19 of 30 projects with some
TrueBughadgreaterthan50%FAR.TheTOTALrowshows
the overall FAR: for manually written specs, it is 82.81%
(110 TrueBugs and 530 FalseAlarms). For automatically
mined specs, we inspected 200 violations. We elide the
breakdown per project, but the overall FAR for automati-
callyminedspecsis97.89% (4TrueBugsand186FalseAlarms) .
We furtheranalyzed FAR along several dimensions, trying
to identify where it may be lower. Table 5(top part) shows
the FAR breakdown for manually written specs. Violations
in third-party libraries had 86.55% FAR, while violations i n
the project code had 80.82% FAR. Violations in single- vs.
multi-module Maven projects had 81.87% vs. 86.23% FAR,
and violations for manually written tests vs. automaticall y
generated tests had 82.51% vs. 84.21% FAR. The similar
FARs across all these dimensions suggests that the FARs are
mostly due to inherent (in)eﬀectiveness of the specs and les s
607Table 4: Per-project inspection summary for 42
manually written specs. SV: static violations; HTI:
hard to inspect; TB: true bugs; FA: false alarms;
FAR[%]: false alarm rate
PID SVHTITBFAFAR[%]
P1 40400.00
P2 10100.00
P3 20200.00
P4 10100.00
P5 20200.00
P6 20200.00
P7 10100.00
P8 706114.29
P9 906333.33
P10 302133.33
P11 603350.00
P12 16 07956.25
P13 95 0405557.89
P14 10 04660.00
P15 301266.67
P16 13 04969.23
P17 14 041071.43
P18 702571.43
P19 802675.00
P20 401375.00
P21 802675.00
P22 401375.00
P23 16 041275.00
P24 501480.00
P25 10 02880.00
P26 501480.00
P27 801787.50
P28 13 111191.67
P29 13 011292.31
P30 29 112796.43
F69 333 100323100.00
TOTAL 652 12110530 82.81
due to speciﬁc code-related factors. An interesting ﬁnding
is that violations in libraries are somewhat more likely to b e
false alarms, as one would expect that libraries are indeed
better tested and have fewer bugs than the project code.
Table5(bottom part) shows the breakdown for automat-
ically mined specs. Compared to manually written specs,
the FAR values are higher along all dimensions. The over-
all FAR was 97.89% (186 of 190 non-HTI violations). Com-
pared within diﬀerent dimensions, the FAR values were sim-
ilar, e.g., 100.00% for violations in libraries vs. somewha t
lower 94.87% for violations in the project code, showing a
consistent relationship with violations of manually writt en
specs. In brief, all these FARs appear rather high.
Table6shows the FAR values for the 42 manually writ-
ten specs that we inspected (we did not inspect violations
of 21 specs, as explained in Section 5.2.1). First, only 11
specs (i.e., 26.19% of 42 inspected specs and 6.04% of all 182
specs) helped ﬁnd a TrueBug and could have provided some
value to developers of some project(s). Second, 119 specs
were never violated, so theyonly increased the runtimeover -
head. These specs may get violated if monitored on other
projects. Third, all but one of the specs that we inspected
caused at least one FalseAlarm, and the only spec without
false alarms, URLDecoder_DecodeUTF8 , was violated only once.
Interestingly, the spec that was violated the most and is the
least eﬀective at bug ﬁnding, Iterator_HasNext with 97.40%
FAR, is the de facto example spec in research papers on spec
mining and runtime veriﬁcation. Section 5.2.2discusses why
this spec and others generate so many FalseAlarms.Table 5: Split of inspection results along various di-
mensions. Column headers are same as in Table 4
Type of specs SVHTITBFAFAR[%]
Manually written 652 12110530 82.81
Libraries 232 930193 86.55
Project code 420 380337 80.82
Single-module 513 1191411 81.87
Multi-module 139 119119 86.23
ManTests 533 792434 82.51
AutoTests 119 5189684.21
Automatically mined 200 104186 97.89
Libraries 122 100112100.00
Project code 78 047494.87
Single-module 148 93136 97.84
Multi-module 52 115098.04
Table 6: Per-spec inspection summary. Column
headers are same as in Table 4
Spec SVHTITBFAFAR[%]
URLDecoder DecodeUTF8 10100.00
Collections SynchronizedColl... 22 019313.64
Collections SynchronizedMap 504120.00
ByteBadParsingArgs 302133.33
LongBadParsingArgs 22 014836.36
InetSocketAddress Port 201150.00
ByteArrayOutputStream Flu... 123 0556855.28
StringTokenizer HasMoreEle... 11 04763.64
MathContendedRandom 14 05964.29
ShortBadParsingArgs 301266.67
Iterator HasNext 157 34150 97.40
31 Specs with 100% FAR 289 90280100.00
TOTAL 652 12110530 82.81
For automatically mined specs, only 3 (i.e., 17.65% of
the 17) led to at least one TrueBug in the 200 inspected
violations— FSM162,FSM33, andFSM3731, withFARsof87.50%,
90.00% and 98.06%, respectively. FSM373is very similar to
the manually written Iterator_HasNext spec and has similar
FAR as well. Based on the very high FARs among violations
of both manually written and automatically mined specs,
we conclude that the existing specs are rather ineﬀective fo r
ﬁnding bugs, because they raise too many false alarms.
5. ANALYSIS OF RESULTS
Wediscusssomebugswefound, someissues withthespecs
(and opportunities to improve them), and some developers’
responses to our pull requests (bug reports and ﬁxes).
5.1 Analysis of Bugs Found
We describe some of our pull requests that the develop-
ers accepted and all three pull requests that the developers
rejected so far.
5.1.1 Accepted Pull Requests
The project with the largest number of accepted pull re-
quests in our study was joda-time ,“thede facto standard
date and time library for Java prior to Java SE 8” [26].
Thejoda-time developers accepted all our 40 pull requests,
37 of which based on the violations of the manually writ-
ten spec ByteArrayOutputStream_FlushBeforeRetrieve (BAOS).
BAOScatches cases where an underlying ByteArrayOutput-
Streamis not closed or ﬂushed before retrieving the contents
of the enclosing stream. The ﬁx in our pull requests was
1These specs are publicly available [ 50].
608simply to invoke flush()beforetoByteArray() ,toString() ,
orwrite*() on aByteArrayOutputStream . In all projects, 49
outof55 BAOSpullrequeststhatwesubmittedwereaccepted,
1 was rejected, and the others are pending.
Another big set of bugs was found from the violations of
CSC(discussed in sections 2and3.2.1) and a closely related
spec,Collections_SynchronizedMap . These specs are violated
if theIterator of a synchronized Collection is accessed from
code that is not synchronized. Our ﬁx was to put the calling
code in a synchronized block. Our pull requests for these
specs were mostly accepted, or were already ﬁxed between
the start of our experiments and when we wanted to report
them in widely usedapplications— Spring-Beans ,TestNG, and
XStream. We also have a pending pull request in ActiveMQ.
All the 18 bugs that we found while monitoring automat-
ically generated tests were related to missing checks for in -
valid input. 17 were of the form Type_BadParsingArgs , where
TypeisLong,Short, orByte. These specs check that calls to
the respective Type.parseType(String s, int r) methods do
not have sempty or null. 12 pull requests were accepted, 1
has been rejected, and 4 are pending. The remaining (and
still pending)invalid-input-relatedpullrequestwas for avio-
lation of InetSocketAddress_Port spec which checks that the
intport number used to create new java.net.InetSocketAd
dressobjects is between 0 and 65535, inclusive.
Finally, we found 4 bugs from monitoring the specs that
Pradel et al. [ 49] mined automatically. Of these, 3 were
duplicates of bugs found from monitoring manually written
specs, so we did not report them again. The additional bug
(with a pending pull request) was from a violation of FSM33,
whereremoveFirst() was invoked on a java.util.LinkedList
object without ﬁrst checking that it was not empty.
5.1.2 Rejected Pull Requests
Three of our pull requests were rejected, mostly because
we had limited domain knowledge. In XStream, we submitted
a pull request for a Collections_SynchronizedMap violation,
but the developer rejected it and responded: “...there’s no
need to synchronize it... As explicitly stated in the docu-
mentation, XStream is not thread-safe during setup... this
is documented behavior.” InJSqlParser , we reported a miss-
ing check for the validity of sinLong.parseLong(String s,
int i), and the developer responded: “...The parser itself
ensures that only long values are passed to LongValue. So
do you have a problematic SQL, that produces a NumberFor-
matException?” Indeed, the violation was from monitoring
an automatically generated test, but since the violation is in
a public class, it could lead to unhandled exceptions in ap-
plications that depend on JSqlParser but which do not thor-
oughlysanitizetheirowninput SQLqueries; weplantorevisit
this in the future. In threerings.playn , we submitted a ﬁx
for aBAOSviolation, and the developer responded: “JsonAp-
pendableWriter automatically ﬂushes the target stream whe n
done()is called, as is documented in the Javadoc for done.
So an additional ﬂush is unnecessary.” Indeed, BAOSdid not
detect the ﬂush because the spec is buggy. The violation
occurred in a method which casts a java.io.OutputStream to
java.io.Flushable before invoking flush(). However, BAOS
was written to only track calls of flush() onjava.io.Out
putStream and its subtypes, whereas Flushable is a super-
type ofOutputStream . JavaMOP, therefore, correctly ﬁnds a
violation of the spec, but the spec is incorrect. We submit-
ted a bug report for BAOSto the JavaMOP repository andconﬁrmed that it did not aﬀect any other BAOS-related pull
request that we sent.
5.2 Issues with Monitored Specs
We next discuss why we did not monitor some specs or
inspect some violations, and give examples to show why the
specs reported a lot of false alarms.
5.2.1 Ignored Specs
Manually Written Specs: We inspected all (652) static
violations (SV) from 42 manually written specs. 21 other
manually written specs had violations, but we did not in-
spect them: (i) 8 *StaticFactory specs may, at best, ﬁnd
performance bugs not functional bugs (459 SV); (ii) 2 *_Ob-
soletespecs get violated for every call to Dictionary() or
Enumeration() , and were written as “suggestion” specs that
should not lead to bugs (518 SV); (iii) 4 *_StandardConstruc-
torsspecs were marked as potentially reporting false alarms
(430 SV); (iv) 2 Enum_*specs were buggy and get violated
on every invocation of Enummethods (874 SV); (v) 1 Seri-
alizable_UID spec gets violated when a Serializable class
does not declare a serialVersionUID , which can be trivially
checked statically (2348 SV); and (vi) 4 more specs were ig-
nored because they did not report violation sites (93 SV).
We reported 16 of these spec issues, together with 7 bugs
that we found in other specs during our inspections—a to-
tal of 23 bug reports—to the JavaMOP repository, and the
process of improving the specs is ongoing.
Automatically Mined Specs: Although we originally ob-
tained 223 mined specs from Pradel et al., we monitored
only 17, because a brief manual inspection of specs found
that 206 had one or more of the following issues: (i) the
spec (FSM) was very large, sometimes having tens of tran-
sitions and/or states, making it hard to understand and to
inspect its violations; (ii) the spec relates only methods i n
thejavax.swing.* orjava.awt.* libraries; (iii) the spec im-
poses unnecessary temporal order on methods of multiple
unrelated object types; and (iv) the spec imposes unnec-
essary temporal order on unrelated methods of the same
object type. We did not report or attempt to improve the
automatically mined specs. In fact, Pradel et al. [ 49] ac-
knowledge that some of these specs are of low quality and
develop a system that to prune some violations of mined
specs. However, it would be better to additionally evaluate
the spec mining techniques on larger, more diverse projects
and conﬁrm detected (potential) bugs with developers.
5.2.2 Analysis of False Alarms
The monitored specs reported many false alarms mainly
because the specs (i) did not encode all correctness condi-
tions, or encoded wrong conditions, and thus need to be
improved; or (ii) captured harmless misuse of APIs which
would rarely or never lead to actual bugs.
For example, consider the Iterator_HasNext spec which
statesthateachinvocationof next()onajava.util.Iterator
object must be preceded by an invocation of hasNext() that
returns trueon the same object. Iterator_HasNext viola-
tions led us to discover 4 accepted bugs in the Thomas-S-
B.visualee project, and other researchers had previously
usedIterator_HasNext toﬁndsome real bugs in AspectJ(bug
IDs#218167 and#218171 [ 60]). However, Iterator_HasNext
also reports a huge number of false alarms—150 of 154 non-
HardToInspect violations were false alarms—with FAR of
6091ArrayList <Integer >list = newArrayList <>();
2list .add(1) ;
3Iterator <Integer >it = list . iterator () ;
4if( it . hasNext () ) {inta = it . next () ; }
5if( list . size () >0){
6intb = list . iterator () . next () ;
7}
8if(! list . isEmpty() ) {
9intc = list . iterator () . next () ;
10}
11HashMap <String , Integer >map = newHashMap <>();
12map. put(”one”, 1) ;
13if(map. containsKey(”one”)) {
14intd = map. values () . iterator () . next () ;
15}
16inte = list . iterator () . next () ;
17intf = map. values () . iterator () . next () ;
Figure 4: False alarms from Iterator_HasNext spec
1Map<String , String >map = newHashMap <>();
2map. put(”1”, ”1”) ; map. put(”2”, ”2”) ;
3for( String key : map. keySet() ) {
4String value = map. get(key) ;
5map. put(key , value + ”x”) ;
6//map. put(key + ”x”, value + ”x”) ;
7}
Figure 5: False alarms from Map_UnsafeIterator spec
97.40%. Figure 4illustrates several valid invocations of It-
erator.next() —lines4,6,9,14,16, and17—with no bugs in
the shown code. However, Iterator_HasNext will be violated
for all those invocations except the one on line 4. The ex-
amplenext()invocations in Figure 4illustrate only a few of
the valid uses of the next()method that were violations of
theIterator_HasNext spec during our experiments. To make
theIterator_HasNext spec more precise, one would need to
ensure that it encodes more valid ways of checking that an
Iterator has enough elements before invoking next(), taking
into consideration various possible Collection types.
Map_UnsafeIterator is another spec with false alarms; it
checks whether code is modifying a java.util.Map instance
while iterating over it. All 9 Map_UnsafeIterator violations
that we inspected were false alarms. To illustrate, conside r
the code snippet in Figure 5. On each iteration, line 5mod-
iﬁes the values in the Map—a valid operation. Neverthe-
less,Map_UnsafeIterator is violated, because it is too restric-
tive, and reports a violation for any modiﬁcation to the Map.
If line5is replaced with the commented-out statement on
line6, the standard Java library would throw a Concurrent-
ModificationException . We therefore asked other JavaMOP
developers (not involved in this project) why anyone would
want to monitor this spec. The response reﬂects one chal-
lenge in coming up with eﬀective specs: “Invoking the put
method on a map object may or may not change its key set...
there is a trade-oﬀ between accuracy and simplicity... it is
up to the user; one can put more eﬀort into writing more
ﬁne-grained specs... so that there will be fewer false alarm s
reported; or write a simple spec easily and [then] manually
eliminate the false alarms.”
Roughly 20% of all the false alarms among manually writ-
ten specs were from two Closeable_* specs, with 113 vio-
lations between them. Both had 100% FAR. One of them
catches calls of close()on subtypes of java.io.OutputStream
for which close()is a no op. The other catches situations
where calling close()on anOutputStream object that is al-ready closed has no eﬀect. Although both of these specs can
help ﬁnd developers’ likely misunderstanding of the API, we
classiﬁed them as FalseAlarms because they are harmless in
the current version of the code. It is debatable whether we
should have classiﬁed these as“code smells”as done in some
priorwork[ 17,39,49], andwhetherthesewereserious enough
to submit to the developers. We could not easily change the
code to avoid these problems, and it is highly unlikely that
the developers would have accepted our changes.
Automatically mined specs have similar reasons for false
alarms as manually written specs. For example, FSM373is
similar to Iterator_HasNext , so its false alarms were simi-
lar as well. However, one additional cause of false alarms
among violations of FSM373was that it did not permit to
callhasNext() multiple times successively (a self transition
is missing from a state in the FSM). FSM162also contains
transitions that are similar to Iterator_HasNext , but also
adds in a single transition on the Iterator.remove() method
such that the spec is violated if remove() is called multiple
times successively.
5.3 Developers’ Responses
We discuss some example responses and comments that
developers made regarding our pull requests, which gives a
valuable insight into developers’ perception.
Developers Asked for More: After we submitted a pull
requestfora BAOSviolation, the apache.gora developersasked
us to help check other portions of their code: “...Are there
any other instances of this behavior throughout the codebas e?
...I just undertook a quick scan of the codebase for ByteAr-
rayOutputStream, I found the following instances. Can you
please check these out as well?” Even after we ﬁxed these
other instances that they pointed out, the developers asked
whether we would be interested to help with similar prob-
lems in their other codebase. In another project, hover-
ruan.weiboclient4j , we sent a pull request that ﬁxed one
of seven Long_BadParsingArgs violations and simply reported
the other six. The developers ﬁxed the remaining six within
a day of accepting our pull request.
DevelopersViewedPull Requests Liberally: Thejoda-
timedevelopers accepted one of our BAOSpull requests al-
though they found it unnecessary: “While I’m not convinced
it is necessary, this will cause no harm.” Wegot similar com-
ments for two pending pull requests. In Apache Zookeeper ,
for theBAOSspec, the developer wrote “Makes sense. I don’t
see why we shouldn’t do what you suggest (add the ﬂush).
You see why it’s a no-op currently though, right? (and why
we haven’t seen issues with this code)” . InTestNG, the devel-
oper tagged one of our synchronization-related pull reques ts
as aperf/enhancement and said, “I’m not sure if it is relevant
here: the lists of results should be already computed...no o ne
is supposed to add something new at the report phase.”
DevelopersAccepted Better Exception Messages: For
pull requests pertaining to missing checks for invalid inpu ts,
developers responded well to the better error messages that
we provided. In IvanTrendafilov.Confucius , the developer
responded “Looks good, I’ll be happy to add that more help-
ful error message to the lib. Yes, please also add this check
forparseShort andparseByte ...”. Similarly, in jriecken.gae-
java-mini-profiler , the developer commented on our sug-
gested error message “Not sure that this is much better than
the previous behavior - the exception message is a little mor e
helpful, but it still throws a NumberFormatException” , and
610requested that we further modify our pull request before
they accepted it.
6. SUGGESTIONS FOR THE FUTURE
Based on the experience from this study, we give several
suggestions to help the spec mining and runtime veriﬁca-
tion research communities with spec engineering , i.e., writ-
ing/discovering and evaluating more eﬀective specs.
(1)Increased Focus on Bug-Finding Eﬀectiveness:
More focus should be on the bug-ﬁnding eﬀectiveness of
specs, which is more important to developers than the per-
formance of monitoring. For example, the most widely used
Iterator_HasNext specwas highlyineﬀectivefor ﬁndingbugs.
(2)Better Spec Categorization: It is crucial toﬁndgood
ways to designate the severity levels of specs. All specs are
not equal in their bug-ﬁnding eﬀectiveness. Some specs,
when violated, indicate a bug with a very high probabil-
ity. Other specs indicate issues that may be bugs in some
projects but not in others. Finally, some specs are less se-
vere, indicating potentially poor coding practices and may
not lead to the detection of actual bugs.
(3)ComplementingBenchmarks: Continueduseofbench-
marks like DaCapo is good for comparison with older results
and evaluating performance of new techniques, but bench-
marks should be complemented with evaluations on a larger
numberof open-sourceprojects, toassess the techniquesan d
specs in more realistic scenarios.
(4)Conﬁrming Detected Bugs with Developers: Eval-
uating on recent project versions and reporting detected
bugs to developers of open-source projects should be encour -
aged more. Admittedly, the process is challenging and time
consuming, requiring to understand the application domain
and communicate with the developers. We have publicly re-
leased a list of all our pull requests, to serve as a starting
point for collecting true bugs: [ 57]. We found interesting re-
sults from submitted pull requests, e.g., even“buggy”spec s
likeBAOScan lead to accepted pull requests
(5)Automated Filtering of Specs and False Alarms:
It is necessary to better automatically ﬁlter out likely fal se
alarms to improve ineﬀective specs. We found that specs
with too many violations were almost always ineﬀective.
Pradel et al. [ 49] deﬁned some heuristics-based automated
techniques for ﬁltering out violations while statically ch eck-
ing mined specs, while Gabel and Su [ 18] as well as Nguyen
and Khoo [ 39] proposed techniques for checking that mined
specs are indeed true specs. Much more work in this direc-
tion is needed, especially because manual inspection, whic h
we did in this paper, is rather tedious.
(6)Open Spec Repositories: It would be beneﬁcial to
have community-driven spec repositories and standardized
ways of representing specs to facilitate spec sharing—we
could have evaluated more specs if it were easier to ﬁnd and
use them. We started such a repository using all the specs
monitored in this paper [ 51]; we plan to continue adding
more specs to this repository, and invite the research com-
munity to contribute their specs there as well to facilitate
research on engineering better specs.
7. THREATS TO V ALIDITY
External: The results of our study may not generalize be-
yond the projects, tests, or specs that we evaluated. To
mitigate this threat, we used a larger number of open-sourceprojects than had been evaluated in previous runtime veri-
ﬁcation and spec mining studies. Further, the 200 projects
that we used were quite diverse in size, number of tests, and
GitHub activity. Concerning the bug-ﬁnding eﬀectiveness
of specs, we used the largest sets of manually written and
automatically mined specs that we could ﬁnd with our mini-
surveyofthespecminingandruntimeveriﬁcationliteratur e,
and that could easily work with JavaMOP. JavaMOP is rep-
resentative of the performance of runtime veriﬁcation tool s
and allows to simultaneously monitor specs written in dif-
ferent formalisms, making it well suited for our large-scal e
evaluation of existing specs. Our study is focused on Java,
and the results may diﬀer for other programming languages.
Internal: We wrote scripts to automate the monitoring of
tests against the specs. Our scripts that run the tests, mea-
sure overhead, and post-process results were reviewed by at
least two authors. During inspection and classiﬁcation of v i-
olations into TrueBug, FalseAlarm, and HardToInspect, we
initially had two reviewers inspect independently to preve nt
them from inﬂuencing each other. It is possible that some
violations we labeled as FalseAlarms are actually TrueBugs .
For violations that we labeled as TrueBugs, we submitted
95 pull requests, and developers make the ﬁnal judgment
whether to accept (74 so far) or reject (3 so far).
8. CONCLUSIONS
Runtime veriﬁcation has been receiving increased atten-
tion in the research community, with substantial contribu-
tions to reducing the overhead of monitoring. However, in-
suﬃcient work has been done on evaluating and improving
specs. Ourstudyshows thatexistingtools suchas JavaMOP
have an acceptable overhead for development-time monitor-
ing of test runs, and the existing specs can ﬁnd some true
bugs. Unfortunately,avastmajority ofviolations from the se
specs are false alarms. We believe that this greatly hinders
the adoption of these techniques by practitioners.
Based on the experience from our study, we provided a
set of recommendations for future work. We also made pub-
licly available the data from our study [ 57] to aid future
research. The runtime veriﬁcation and spec mining research
communities need toputmuchmore emphasis on better spec
engineering to develop more eﬀective specs. It is possible
that improving existing specs or mining new eﬀective specs
will need more expressive formalisms, which may slow down
monitoring and require further eﬃciency improvements to
runtime veriﬁcation. But only when eﬀective specs are avail -
able, will it be truly worthwhile to consider how to further
make monitoring faster and to have some chance of practical
adoption. We hope this paper presents a call to action for
the researchers to develop better specs and evaluate them
more thoroughly.
9. ACKNOWLEDGMENTS
We thank Alex Gyori, Farah Hariri, Cosmin Radoi, and
August Shi for feedback on early drafts of this paper, Rahul
Gopinath for discussions and help with Randoop, and He
Xiao and Yi Zhang for help with JavaMOP. We also thank
all authors of papers who replied to our emails concern-
ing their mined specs. This research was partially sup-
ported by the NSF Grants CCF-1421503, CCF-1421575,
CCF-1438982, andCCF-1439957. Wajih UlHassanwas par-
tially supported by the Sohaib and Sara Abassi Fellowship.
61110. REFERENCES
[1] C. Allan, P. Avgustinov, A. S. Christensen,
L. Hendren, S. Kuzins, O. Lhot´ ak, O. de Moor,
D. Sereni, G. Sittampalam, and J. Tibble. Adding
trace matching with free variables to AspectJ. In
OOPSLA , pages 345–364, 2005.
[2] M. Arnold, M. Vechev, and E. Yahav. QVM: An
eﬃcient runtime for detecting defects in deployed
systems. In OOPSLA , pages 143–162, 2008.
[3] N. E. Beckman and A. V. Nori. Probabilistic, modular
and scalable inference of typestate speciﬁcations. In
PLDI, pages 211–221, 2011.
[4] S. M. Blackburn, R. Garner, C. Hoﬀmann, A. M.
Khang, K. S. McKinley, R. Bentzur, A. Diwan,
D. Feinberg, D. Frampton, S. Z. Guyer, M. Hirzel,
A. Hosking, M. Jump, H. Lee, J. E. B. Moss,
A. Phansalkar, D. Stefanovi´ c, T. VanDrunen, D. von
Dincklage, and B. Wiedermann. The DaCapo
benchmarks: Java benchmarking development and
analysis. In OOPSLA , pages 169–190, 2006.
[5] E. Bodden. MOPBox: A library approach to runtime
veriﬁcation. In RV Tool Demo , pages 365–369, 2011.
[6] E. Bodden, L. Hendren, P. Lam, O. Lhot´ ak, and N. A.
Naeem. Collaborative runtime veriﬁcation with
tracematches. In RV, pages 22–37, 2007.
[7] E. Bodden, L. J. Hendren, and O. Lhot´ ak. A staged
static program analysis to improve the performance of
runtime monitoring. In ECOOP, pages 525–549, 2007.
[8] E. Bodden, P. Lam, and L. Hendren. Finding
programming errors earlier by evaluating runtime
monitors ahead-of-time. In FSE, pages 36–47, 2008.
[9] D. Chen, Y. Zhang, R. Wang, X. Li, L. Peng, and
W. Wei. Mining universal speciﬁcation based on
probabilistic model. In SEKE, pages 471–476, 2015.
[10] F. Chen and G. Ro¸ su. Towards monitoring-oriented
programming: A paradigm combining speciﬁcation
and implementation. In RV, pages 108–127, 2003.
[11] W. G. Cochran. Sampling techniques. John Wiley &
Sons, 1977.
[12] Collections SynchronizedCollection. http://fsl.cs.
illinois.edu/annotated-java/ properties/html/java/
util/Collections SynchronizedCollection.html .
[13] V. Dallmeier, N. Knopp, C. Mallon, S. Hack, and
A. Zeller. Generating test cases for speciﬁcation
mining. In ISSTA, pages 85–96, 2010.
[14] CompleteSearch DBLP. http://www.dblp.org/search/
index.php .
[15] M. B. Dwyer, R. Purandare, and S. Person. Runtime
veriﬁcation in context: Can optimizing error detection
improve fault diagnosis? In RV, pages 36–50, 2010.
[16] V. Forejt, M. Kwiatkowska, D. Parker, H. Qu, and
M. Ujma. Incremental runtime veriﬁcation of
probabilistic systems. In RV, pages 314–319, 2012.
[17] M. Gabel and Z. Su. Online inference and enforcement
of temporal properties. In ICSE, pages 15–24, 2010.
[18] M. Gabel and Z. Su. Testing mined speciﬁcations. In
FSE, pages 1–11, 2012.
[19] S. Hussein, P. Meredith, and G. Ro¸ su. Security-policy
monitoring and enforcement with JavaMOP. In PLAS,
pages 1–11, 2012.
[20] JavaMOPAgent Documentation. https://github.com/runtimeveriﬁcation/javamop/blob/master/docs/
JavaMOPAgentUsage.md .
[21] JavaMOP. http://fsl.cs.illinois.edu/index.php/
JavaMOP .
[22] java.util.collections. https://docs.oracle.com/javase/7/
docs/api/java/util/Collections.html .
[23] D. Jin, P. O. Meredith, D. Griﬃth, and G. Ro¸ su.
Garbage collection for monitoring parametric
properties. In PLDI, pages 415–424, 2011.
[24] D. Jin, P. O. Meredith, C. Lee, and G. Ro¸ su.
JavaMOP: Eﬃcient parametric runtime monitoring
framework. In ICSE Demo , pages 1427–1430, 2012.
[25] D. Jin, P. O. Meredith, and G. Ro¸ su. Scalable
parametric runtime monitoring. Technical report,
Computer Science Dept., UIUC, 2012.
[26] Joda-Time. http://www.joda.org/joda-time/ .
[27] M. Karaorman and J. Freeman. jMonitor: Java
runtime event speciﬁcation and monitoring library. In
RV, pages 181 – 200, 2004.
[28] I. Krka, Y. Brun, and N. Medvidovic. Automatic
mining of speciﬁcations from invocation traces and
method invariants. In FSE, pages 178–189, 2014.
[29] C. Le Goues and W. Weimer. Speciﬁcation mining
with few false positives. In TACAS, pages 292–306,
2009.
[30] C. Lee, F. Chen, and G. Ro¸ su. Mining parametric
speciﬁcations. In ICSE, pages 591–600, 2011.
[31] C. Lee, D. Jin, P. O. Meredith, and G. Ro¸ su. Towards
categorizing and formalizing the JDK API. Technical
report, Computer Science Dept., UIUC, 2012.
[32] O. Legunsen, D. Marinov, and G. Ro¸ su.
Evolution-aware monitoring-oriented programming. In
ICSE NIER , pages 615–618, 2015.
[33] C. Lemieux. Mining temporal properties of data
invariants. In ICSE SRC , pages 751–753, 2015.
[34] C. Lemieux, D. Park, and I. Beschastnikh. General
LTL speciﬁcation mining. In ASE, pages 81–92, 2015.
[35] Q. Luo, Y. Zhang, C. Lee, D. Jin, P. O. Meredith,
T. F. ¸ Serb˘ anu¸ t˘ a, and G. Ro¸ su. RV-Monitor: Eﬃcient
parametric runtime veriﬁcation with simultaneous
properties. In RV, pages 285–300, 2014.
[36] P. Meredith, D. Jin, F. Chen, and G. Ro¸ su. Eﬃcient
monitoring of parametric context-free patterns. In
ASE, pages 148–157, 2008.
[37] P. Meredith and G. Ro¸ su. Eﬃcient parametric runtime
veriﬁcation with deterministic string rewriting. In
ASE, pages 70–80, 2013.
[38] S. Navabpour, C. W. W. Wu, B. Bonakdarpour, and
S. Fischmeister. Eﬃcient techniques for near-optimal
instrumentation in time-triggered runtime veriﬁcation.
InRV, pages 208–222, 2011.
[39] A. C. Nguyen and S.-C. Khoo. Extracting signiﬁcant
speciﬁcations from mining through mutation testing.
InICFEM, pages 472–488, 2011.
[40] H. A. Nguyen, R. Dyer, T. N. Nguyen, and H. Rajan.
Mining preconditions of APIs in large-scale code
corpus. In FSE, pages 166–177, 2014.
[41] java.lang.instrument. http://docs.oracle.com/javase/
7/docs/api/java/lang/instrument/package-summary.
html.
[42] C. Pacheco and M. D. Ernst. Randoop:
612Feedback-directed random testing for Java. In
OOPSLA Companion , pages 815–816, 2007.
[43] C. Pacheco, S. K. Lahiri, and T. Ball. Finding errors
in .NET with feedback-directed random testing. In
ISSTA, pages 87–96, 2008.
[44] C. Pacheco, S. K. Lahiri, M. D. Ernst, and T. Ball.
Feedback-directed random test generation. In ICSE,
pages 75–84, 2007.
[45] M. Pradel. Dynamically inferring, reﬁning, and
checking API usage protocols. In OOPSLA
Companion , pages 773–774, 2009.
[46] M. Pradel, P. Bichsel, and T. R. Gross. A framework
for the evaluation of speciﬁcation miners based on
ﬁnite state machines. In ICSM, pages 1–10, 2010.
[47] M. Pradel and T. R. Gross. Automatic generation of
object usage speciﬁcations from large method traces.
InASE, pages 371–382, 2009.
[48] M. Pradel and T. R. Gross. Leveraging test generation
and speciﬁcation mining for automated bug detection
without false positives. In ICSE, pages 288–298, 2012.
[49] M. Pradel, C. Jaspan, J. Aldrich, and T. R. Gross.
Statically checking API protocol conformance with
mined multi-object speciﬁcations. In ICSE, pages
925–935, 2012.
[50] Statically checking API protocol conformance with
mined multi-object speciﬁcations (supplementary
material). http://mp.binaervarianz.de/
icse2012-statically/ .
[51] FSL Speciﬁcation Database. https://
runtimeveriﬁcation.com/monitor/propertydb .
[52] R. Purandare, M. B. Dwyer, and S. Elbaum.
Optimizing monitoring of ﬁnite state properties
through monitor compaction. In ISSTA, pages
280–290, 2013.[53] Randoop. https://randoop.github.io/randoop/ .
[54] G. Reger, H. Barringer, and D. Rydeheard. A
pattern-based approach to parametric speciﬁcation
mining. In ASE, pages 658–663, 2013.
[55] M. P. Robillard, E. Bodden, D. Kawrykow, M. Mezini,
and T. Ratchford. Automated API property inference
techniques. TSE, 39(5):613–637, 2013.
[56] S. Shamshiri, R. Just, J. Rojas, G. Fraser, P. McMinn,
and A. Arcuri. Do automatically generated unit tests
ﬁnd real faults? An empirical study of eﬀectiveness
and challenges. In ASE, pages 201–211, 2015.
[57] Supplementary material for this paper. http://fsl.cs.
illinois.edu/spec-eval .
[58] J. Sun, H. Xiao, Y. Liu, S.-W. Lin, and S. Qin. TLV:
Abstraction through testing, learning, and validation.
InESEC/FSE , pages 698–709, 2015.
[59] S. H. Tan, D. Marinov, L. Tan, and G. T. Leavens.
@tComment: Testing Javadoc comments to detect
comment-code inconsistencies. In ICST, pages
260–269, 2012.
[60] A. Wasylkowski and A. Zeller. Mining temporal
speciﬁcations from object usage. In ASE, pages
295–306, 2009.
[61] C. W. W. Wu, D. Kumar, B. Bonakdarpour, and
S. Fischmeister. Reducing monitoring overhead by
integrating event- and time-triggered techniques. In
RV, pages 304–321, 2013.
[62] Q. Wu, G. Liang, Q. Wang, T. Xie, and H. Mei.
Iterative mining of resource-releasing speciﬁcations. In
ASE, pages 233–242, 2011.
[63] H. Zhong, L. Zhang, T. Xie, and H. Mei. Inferring
resource speciﬁcations from natural language API
documentation. In ASE, pages 307–318, 2009.
613