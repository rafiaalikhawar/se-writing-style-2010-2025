OCELOT: A Search-Based Test-Data Generation Tool for C
Simone Scalabrino
University of Molise
Pesche, ItalyGiovanni Grano
University of Zurich
Zurich, SwitzerlandDario Di Nucci
Vrije Universiteit Brussel
Brussels, Belgium
Michele Guerra
University of Molise
Pesche, ItalyAndrea De Lucia
University of Salerno
Salerno, ItalyHarald C. Gall
University of Zurich
Zurich, Switzerland
Rocco Oliveto
University of Molise
Pesche, Italy
ABSTRACT
Automaticallygeneratingtestcasesplaysanimportantroletore-
duce the time spent by developers during the testing phase. In last
years,severalapproacheshavebeenproposedtotacklesuchaprob-
lem: amongst others, search-based techniques have been shown
to be particularly promising. In this paper we describe Ocelot,a search-based tool for the automatic generation of test cases in
C.Ocelotallowspractitioners towriteskeletonsof testcasesfor
theirprogramsandresearcherstoeasilyimplementandexperiment
new approaches for automatic test-data generation. We show that
Ocelotachievesahighercoveragecomparedtoacompetitivetool
in 81% of the cases. Ocelot is publicly available to support both
researchers and practitioners.
CCS CONCEPTS
•Software and its engineering →Maintaining software ;
Search-based software engineering;
KEYWORDS
Automated Testing, Test Case Generation, Search-Based Software
Engineering
ACM Reference Format:
SimoneScalabrino,GiovanniGrano,DarioDiNucci,MicheleGuerra,An-
drea De Lucia, Harald C. Gall, and Rocco Oliveto. 2018. OCELOT: A Search-
Based Test-Data Generation Tool for C. In Proceedings of the 2018 33rd
ACM/IEEEInternationalConferenceonAutomatedSoftwareEngineering(ASE
’18), September 3–7, 2018, Montpellier, France. ACM, New York, NY, USA,
4pages.https://doi.org/10.1145/3238147.3240477
1 INTRODUCTION
Software testing —and, in particular, test case writing— is one ofthemosthuman-intensiveandtime-consumingactivitiesinsoft-
ware development life-cycle [
1]. Therefore, noticeable effort has
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ASE ’18, September 3–7, 2018, Montpellier, France
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5937-5/18/09...$15.00
https://doi.org/10.1145/3238147.3240477beendevotedtowardautomatedtest-datageneration,applyingand
possibly combining approaches like random testing, symbolic, and
concolic execution [ 3,7,10,16,20]. Search-based approaches have
also been proved to be particularly well-suited [8, 13].
While many search-based tools are nowadays available for Java,
some of which very mature and used in practice (e.g., EvoSuite
[6]), the same is not true for C. Indeed, the tools developed in
the past, like TESTGEN [ 5], QUEST [ 2], and GADGET [ 15], are
quite dated and not available (neither as executable binaries). To
thebestofourknowledge,themostrecentsearch-basedtoolsfor
test-data generation in C are: i) IGUANA, proposed by McMinn[
14], developed in Java and relying on JNI to interface with C;
ii) Austin, introduced by Lakhotia et al.[12], developed in OCaml
andbasedontheCILframework1;iii)CAVM,recentlyproposedby
Kimetal.[9],developedinC,C++,andPythonandbasedonCLang
and gcc for source code instrumentation. Considering that both
IGUANAandCAVMarenotpubliclyreleased,theonlyavailable
toolfortest-datagenerationinCisAustin.However,despitebeing
open source2, this project is not active anymore. In summary, C
remainsawidelyusedlanguage(ranked2ndintheTIOBEIndexfor
May20183)and,forthisreason,webelievethatthesearch-based
researchcommunityshouldputsomeeffortintoactivelydeveloping
test-data generation tool for this programming language.
In thispaper, we takea step insuch adirection and wepresent
Ocelot,asearch-basedtest-datagenerationtoolforCthatprovides
anextensibleframeworktoquicklyimplementdifferenttypesof
search-basedapproaches.Ocelotisalsoabletoautomaticallywrite
test suites, relying on the LibCheck framework4. We empirically
evaluate Ocelot with respect to Austin on 26 functions from 3open-source C programs, and we show that Ocelot achieves ahigher branch coverage in 81% of the cases. Ocelot is publicly
available5along with its documentation6.
2 BACKGROUND
In the context of white-box coverage-driven testing, the automatic
test-datagenerationconsistsofthegenerationofa(possiblymin-
imal) set of inputs ( TD) for a given target program Pto reach a
desiredamountofcodecoverage(e.g.,branchcoverage).Toachieve
thisgoal,asearch-basedtest-datagenerator( TDG)usesasearch
1http://www.cs.berkeley.edu/~necula/cil/2https://github.com/kiranlak/austin-sbst
3https://www.tiobe.com/tiobe-index/4https://libcheck.github.io/check
5https://ocelot.science/6https://ocelot.gitbook.io/manual/
868
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:48:24 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Scalabrino, Grano, Di Nucci, Guerra, De Lucia, Gall, and Oliveto
 
Minimize
WriteInstrument
Wrap
Makele
CompileGenerate input
Execute
Simulate
Evaluate
Source
Test suite
JNI
library
Conguration
Figure 1: The Workflow of Ocelot
algorithm, calling Pwith many combinations of inputs (test data),
with the aim of maximizing thetotal achieved coverage. This pro-
cess continues until the specified search-budget (e.g., number of
iterations)isover.Therefore,generatingtestdatarequiresacon-
tinuousinteractionbetweenthetargetprogramandthetest-data
generator. When TDGcallsPwith a given input, Phas to provide
feedback about the branches covered with that input to TDG.
Since Cis aprocedural programminglanguage, it ispossible to
considerfunctions asthe unitsto test( P). Thismakestest genera-
tion conceptually different compared to object-oriented languages
(e.g., Java), where the units to test are classes with a state [ 21]. Fur-
thermore, automatically generating test data for C presents unique
technical challenges. The main technical challenge ( C1) is thein-
strumentation ofP,necessaryto registerwhathappensin Pwhen
TDGtests a specific input. Indeed, the object language of C de-
pendsonthetargetarchitectureanditisnecessarytofindaway
to instrument P, without tying to a specific architecture/operating
system, to grant a certain level of portability. A second technical
challenge( C2)regardsthe integration betweenTDGandP∗(i.e.,the
instrumented P). It is necessary todevise a mechanism to i) make
the input generated by TDGsuitable for P∗and ii) to make P∗able
to communicate with TDG. A final major research and technical
challenge ( C3) regards one of the main peculiarities of C, i.e., point-
ers. Consider a function that takes as input ( int* a, int b ). At
somepointinthefunction,thereisacondition if (*a == b) .A
test-datageneratorshouldbeabletogeneratetestdatathatevaluate
such a condition as true. While Java has a more rigid and implicit
typingrule(i.e.,objectsarestoredasreferences,primitivetypesare
storesbyvalues),inCitispossibletohavemixedsituations,that
makes handling data types in TDGmore difficult.
3 OCELOT
The architecture of Ocelot is inspired by IGUANA [ 14]: it is de-
velopedinJavaandusesJNIasinterfacewiththetargetprogram
to try different combinations of test data. Running Ocelot con-
sistsoftwodistinctmacro-phases: buildandrun.Figure 1shows
the phases of test-data generation in Ocelot. In the buildphase,
thetargetprogramisinstrumented,wrappedinaJNIlibraryand
thencompiled.Theoutputofsuchaphaseisastatic library.This
library is linked by Ocelot and used in the runphase, where a
search-based algorithm is exploited to identify a set of inputs that
maximizethecodecoverage.Theoutputofthisphaseisasetoftest
datathatcanbeusedtotestthetargetprogram.Ocelotalsopro-
videstwoadditionaloptionalphases: minimize andwrite.During
theminimize phase, test data that do not contribute to improve thecodecoverageareremoved,whileduringthe writephase,skeletons
of actual test cases are written. Such skeletons lack the assertions,
thatshouldbemanuallyadded,andtheyarebasedontheLibCheck
framework.
3.1 Build Phase
Thebuildphaseconsistsof:i) instrumenting thetargetprogram(i.e.,
transforming PtoP∗), ii)wrapping it in the JNI library, iii) generat-
ingthemakefile,andiv) compiling thestaticlibrary.Ocelotdirectly
instruments the C source code, making it able to potentially work
regardless of the target architecture. Specifically, it adds probes
nearthecontrolstructurestotracktheexactstepsexecutedin P.
Existent tools (e.g., gcovorlcov) are not used, since they do not
report the runtime values of the variables, needed to compute part
of thefitness functionfor someapproaches. Duringthe wrapping
sub-phase, Ocelot adds to the target function a set of pre-defined
C functions to integrate P∗in Ocelot through JNI. Moreover, it
adapts some of those files to handle the specific inputs requiredby
P. Indeed, while the static Java interface to call Premains the
same,eachtargetfunctionhasitsowninputtypes.The wrapping
sub-phasehandlestheinformationthatwillbepassedbyOcelot
during the runphase and translate it into actual C variables that
will be passed to P∗for the execution. Finally, Ocelot generates
themakefile(dependingontheoperatingsystemandonthespecial
requirements of the target program) and it compilesthe JNI library.
3.2 Run Phase
Therunphase consists of: i) generating inputs for P, ii) calling the
JNI interface to executethe code, iii) grabbing the execution events
andsimulating themontheControlFlowGraphof P,andiv)evalu-
atingthe inputs and giving feedback to generate other inputs. The
input generation and the feedback mechanism strongly depend
onthespecifictest-datagenerationstrategyused.Forexample,a
strategy based on random search would randomly generate inputs
and ignore the evaluation, adding all the generated test data to the
resultingtestsuite.Ontheotherhand,the execution andsimulation
sub-phases are common to all the strategies. After the execution
ofP∗, alist ofevents (i.e.,the actual evaluationsof conditionsand
fitness function) is generated. Such a list is used to simulatethe
eventsontheControlFlowGraphof Ptoeasilycomputethefitness
function necessary to give feedback to the strategy.
3.3 Features
Ocelotprovidessomeadditionalfeaturescomparedtothestate-
of-the-art tools. First of all, it implements different search-based
869
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:48:24 UTC from IEEE Xplore.  Restrictions apply. OCELOT: A Search-Based Test-Data Generation Tool for C ASE ’18, September 3–7, 2018, Montpellier, France
Figure 2: The CLion plugin configuration for Ocelot
approaches and it is easily extensible. Indeed, Ocelot is not tied to
anyspecificapproach.ItimplementsLIPS[ 19],MOSA[ 18],AVM
[11], and the random approach [ 17] while, for instance, CAVM [ 9]
is only based on AVM [ 11], while Austin implements AVM and
AVM+[12].Furthermore,itsupportsdifferentbudget-handlingand
minimization strategies that can be combined together with the
aforementioned algorithms. Thus, Ocelot is not designed to work
only with a specific type of algorithm and, therefore, it is easily
extensiblewithnewtest-datagenerationstrategies.Withrespectto
othertools(e.g.,Austin),OcelotisbasedonEclipseCDT7.Hence,
itisabletoautomaticallydetectthedatatypesoftheparameters
and to fully instrument code.
Ocelotstronglyreliesonaconfigurationfile,inwhichallthe
parametersforallthephaseshavetobespecified.Forexample,thisfilecontainsthemaximumnumberofevaluations,thedesiredtarget
coverage, the name of the target function, the path to the file with
the target function, and so on. Since there are many parametersthat can be set, we also developed a user interface for Ocelot,
implemented in a CLion8plugin. This plugin allows developers to
easily use Ocelot for their projects. While, as previously stated,
thecommandlineallowstomakeexperiments,theCLionplugin
is focused on practitioners and, therefore, it supports by defaultthewritemode able to generate skeletons of test cases. It adds
a configuration page for Ocelot in the general configuration ofCLion. Moreover, it introduces an item in the “Code ” menu (i.e.,
“Generate TC ”). When clicking on this item, the plugin shows a
window with all the functions of the current files for which the
developercouldgeneratetest-data.Finally,thepluginautomatically
recognizes the signature of each function, allowing the users to
specify, for each parameter, the ranges of possible values. Figure 2
shows the Ocelot configuration window in CLion.
4 EVALUATION
InthissectionwereportacomparisonbetweenOcelotandAustin,
theonlyavailablesearch-basedtoolforautomaticallygenerating
test-datainC,tothebestofourknowledge.Wewerenotableto
compareOcelottoCAVMorIGUANA,sincethesetoolsarenot
publicly available. Note that our goal was to compare Ocelot with
7https://www.eclipse.org/cdt/8https://www.jetbrains.com/clion/competitive search-based tools, so we did not consider test datagenerationtoolsforCbasedonotherapproaches,suchasCUTE
[20] and DART [7]. This comparison will be part of future work.
As previously shown in Section 3, Ocelot provides a broader
number of features compared to other state-of-the-art tools . More-
over, itis more flexibleand easilyextensible. Amongstthe others,
Ocelotallowstogeneratetestsrelyingondifferentmeta-heuristics.Beingthe goaloftheevaluationtocheckwhetherOcelotisagood
alternative to Austin, we set Ocelot to run with AVM, i.e., the
search-algorithm employed by Austin.
With our study we aim to compare out-of-the-box the perfor-
mance of these tools. It is worth noting that Austin requires an
explicitpointerconstraintsinthesourcecodeofthetargetfunctiontoinstantiateanypointer.Thus,apointerwillnotbeinstantiatedif
the code does not compare it to NULL. We argue that this an impor-
tant limitation for the usability of the tool. Therefore, as done in
previous work [ 9], we decide to not manually modify the subjects
to evaluate the tools in a real-world scenario.
To compare the tools, we run both of them on a subset of 26
functions coming from 3 different C open source projects: GIMP,
GLS, and SGLIB.GIMPis the open source GNU image manipulation
software; GLSstands for GNU Scientific Library; SGLIBis a library
offering generic utilities. It is worth noting that this set of func-tions has been used both in our previous work and in the paper
thatintroducedAustin[ 12,19].Table1liststhesetoffunctions,
representingthe contextofthecomparison.Intotal,wetakeinto
account 400 branches for 26 functions.
Asdoneinpreviouswork[ 9],weset1000evaluationsforeach
branchofthefunctionundertestassearchbudget.Moreover,for
the same reason, we do not impose any time limit constraints.
Werunthetools20timesforeachfunction;wethencomparethe
achieved branch coverage relying on the non-parametric Wilcoxon
Rank Sum Test [ 4] with significance level α=0.05. Significant
p-valuesallows us to reject the null hypothesis, i.e., that the two
tools achieve the same coverage. Moreover, we rely on the Vargha-
Delaney ( ˆA12) statistic [ 22] to estimate the effect size, i.e., the mag-
nitude of the difference between the measured metrics. ˆA12<=0.5
means that Ocelot reaches a higher coverage than Austin, while
ˆA12>0.5hastheoppositemeaning.Vargha-Delaneystatisticsalso
classifies such effect size into four different levels: negligible ;small;
medium;large[22]. For the experiments, we execute both the tools
on a virtual machine with 8 cores and 32GB RAM, running Ubuntu
16.04 LTS. We make available two docker images on DockerHub
—one for each tool— to foster the replicability of the results9,10.
4.1 Results
Table1summarizes thebranchcoverage resultswith thecompar-
ison between Austin and Ocelot. The table shows the average
coverageachievedover20runsalongwiththe p-valuesobtained
with the Wilcoxon test and the Vargha-Delaney statistic. We type-
set the statistically significantly results (i.e., p-value ≤0.05) in bold,
highlighting the tool reaching the highest coverage. From Table
1we notice that Ocelot performs better than Austin in 21 out
26cases( ≈81%),whiletheoppositehappensinonlyonecase.In
detail,thecoverageimprovementrangesfromaminimumof43%
9https://hub.docker.com/r/giograno/austin/
10https://hub.docker.com/r/giograno/ocelot/
870
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:48:24 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Scalabrino, Grano, Di Nucci, Guerra, De Lucia, Gall, and Oliveto
Table 1: Comparison between Austin and Ocelot
Program Function Branches Austin Ocelot p-value Â 12
SGLIB sglib_int_array_quick_sort 48 0.17 0.93 0.00 0.00 (L)
SGLIB sglib_int_array_heap_sort 44 0.19 1.00 0.00 0.00 (L)
SGLIB sglib_int_array_binary_search 14 0.32 0.75 0.00 0.00 (L)
GLS gsl_poly_eval_derivs 12 0.58 1.00 0.00 0.00 (L)
GLS gsl_poly_solve_cubic 20 0.55 0.55 0.15 0.55 (N)
GLS gsl_poly_solve_quadratic 14 0.64 0.63 0.07 0.65 (S)
GLS gsl_poly_complex_solve_quadratic 14 0.64 0.64 0.56 0.55 (N)
GLS gsl_poly_complex_solve_cubic 220.73 0.42 0.00 1.00 (L)
GIMP gimp_cmyk_to_rgb 2 0.00 1.00 0.00 0.00 (L)
GIMP gimp_cmyk_to_rgb_int 2 0.00 0.68 0.00 0.00 (L)
GIMP gimp_hsl_to_rgb 4 0.00 0.75 0.00 0.00 (L)
GIMP gimp_hsl_to_rgb_int 4 0.00 0.76 0.00 0.00 (L)
GIMP gimp_hsv_to_rgb 10 0.00 0.80 0.00 0.00 (L)
GIMP gimp_rgb_to_cmyk 8 1.00 1.00 NaN 0.50 (N)
GIMP gimp_rgb_to_hsl 14 0.00 0.17 0.00 0.00 (L)
GIMP gimp_rgb_to_hsl_int 28 0.00 0.85 0.00 0.00 (L)
GIMP gimp_rgb_to_hsv4 26 0.00 0.78 0.00 0.00 (L)
GIMP gimp_rgb_to_hsv_int 30 0.00 0.80 0.00 0.00 (L)
GIMP gimp_rgb_to_hwb 10 0.00 0.50 0.00 0.00 (L)
GIMP gradient_calc_square_factor 10 0.50 0.93 0.00 0.00 (L)
GIMP gradient_calc_radial_factor 8 0.50 0.93 0.00 0.03 (L)
GIMP gradient_calc_linear_factor 12 0.09 0.99 0.00 0.00 (L)
GIMP gradient_calc_bilinear_factor 8 0.12 0.97 0.00 0.00 (L)
GIMP gradient_calc_spiral_factor 10 0.30 0.89 0.00 0.00 (L)
GIMP gradient_calc_conical_sym_factor 14 0.21 0.89 0.00 0.00 (L)
GIMP gradient_calc_conical_asym_factor 12 0.25 0.87 0.00 0.00 (L)
Total Average 0.26 0.79
toamaximumof100%forthefunction(e.g., gimp_cmyk_to_rgb ).
Indeed, in each of the aforementioned cases, the effect size is large.
Qualitatively looking at the results, we can see that Austin
stops at 0% of coverage for the subjects having only pointers as
arguments(e.g., gimp_hsl_to_rgb ).Furthermore,evenincaseof
functions with only primitive values as parameters, Ocelot is
able to reach a higher coverage than Austin. For example, on
gradient_calc_radial_factor Austin constantly achieves 50%
of coverage, while Ocelot hits ≈93% on average with a 0.13 stan-
dard deviation.
Even if we do not impose any time limit constraints as search
budget, it is still worth to discuss this dimension when it comes to
compare the two tools. In particular, during our experiment we no-
ticed that, given the same maximum amount of iterations, Ocelot
is way faster than Austin. Just to give an idea, the evolutionarysearchforthefunction
sglib_int_array_quick_sort takes ≈6
seconds for Ocelot, while it takes ≈4 minutes for Austin.
5 CONCLUSION AND FUTURE WORK
In this paper we presented Ocelot, a search-based tool for au-
tomaticallygeneratingtest-dataforCprograms.Wereleasedthe
executableof Ocelot,describingtheconfigurationparametersthat
can be set, to allow both researchers and practitioners to use the
tool.Ourpreliminarystudyshows thatOcelotisabletoachieve
higher coverage than Austin. Moreover, Ocelot presents some
additional advantages over Austin: i) it is easier to use : on the one
hand Austin requires manual code-changes to instantiate pointers
or to specify input preconditions; on the other hand Ocelot auto-
matically handles those cases; ii) it is written in Java, a more com-
mon programming language compared to OCaml, and it is actively
maintained; iii) it offers a CLion plugin to help the practitioners toconfigure the tool; iv) it supports different search algorithms, gen-
erationandminimizationstrategiesthatcanbecombinedtogether;
v) it is more effective and efficient.
Future work will be aimed at improving Ocelot in several as-
pects. We plan to implement a different representation for the test
input,tosupportrecursivedatastructures.Wealsoplantoimple-
mentanewinstrumentationprocesswithofaimofreducingthe
timeneededfortheinstrumentationofmultipletargetfunctions:
this step should improve the scalability of the tool. Finally, we plan
to release the code of tool as open source in the near future.
REFERENCES
[1]A.Bertolino. Softwaretestingresearch:Achievements,challenges,dreams. In
2007 Future of Software Engineering, pages 85–103. IEEE Computer Society, 2007.
[2]K.H.Chang,J.H.CROSSII,W.H.Carlisle,andS.-S.Liao. Aperformanceeval-
uation of heuristics-based test case generation methods for software branch
coverage. International Journal of Software Engineering and Knowledge Engineer-
ing, 6(04):585–608, 1996.
[3]K. Claessen and J. Hughes. Quickcheck: a lightweight tool for random testing of
haskell programs. Acm sigplan notices, 46(4):53–64, 2011.
[4] W. J. Conover and W. J. Conover. Practical nonparametric statistics. 1980.
[5]R.FergusonandB.Korel.Thechainingapproachforsoftwaretestdatageneration.
ACM Transactions on Software Engineering and Methodology, 5(1):63–86, 1996.
[6]G. Fraser and A. Arcuri. Evosuite: automatic test suite generation for object-
oriented software. In Joint meeting of the European Software Engineering Confer-
enceandtheACMSIGSOFTSymposiumontheFoundationsofSoftwareEngineering,
pages 416–419. ACM, 2011.
[7]P. Godefroid, N. Klarlund, and K. Sen. Dart: directed automated random testing.
InACM Sigplan Notices, volume 40, pages 213–223. ACM, 2005.
[8]M. Harman, Y. Jia, and Y. Zhang. Achievements, open problems and challenges
for search based software testing. In Software Testing, Verification and Validation
(ICST), 2015 IEEE 8th International Conference on, pages 1–12. IEEE, 2015.
[9]J. Kim, B.You, M. Kwon, P.McMinn, and S. Yoo. Evaluating cavm: Anew search-
basedtestdatagenerationtoolforc. In InternationalSymposiumonSearchBased
Software Engineering, pages 143–149. Springer, 2017.
[10]J.C.King. Symbolicexecutionandprogramtesting. CommunicationsoftheACM,
19(7):385–394, 1976.
[11]B.Korel. Automatedsoftwaretestdatageneration. IEEETransactionsonsoftware
engineering, 16(8):870–879, 1990.
[12]K.Lakhotia,M.Harman,andH.Gross. Austin:Anopensourcetoolforsearch
based software testing of c programs. Information and Software Technology,
55(1):112–125, 2013.
[13]P. McMinn. Search-based software test data generation: a survey. Software
Testing, Verification and Reliability, 14(2):105–156, 2004.
[14]P. McMinn. Iguana: Input generation using automated novel algorithms. a plug
andplayresearchtool. DepartmentofComputerScience,UniversityofSheffield,
Tech. Rep. CS-07-14, 2007.
[15]C.C.Michael,G.E.McGraw,M.A.Schatz,andC.C.Walton. Geneticalgorithms
fordynamictestdatageneration. In InternationalConferenceAutomatedSoftware
Engineering, pages 307–308. IEEE, 1997.
[16]C. Pacheco and M. D. Ernst. Randoop: feedback-directed random testing for
java. InCompanion to the 22nd ACM SIGPLAN conference on Object-oriented
programming systems and applications companion, pages 815–816. ACM, 2007.
[17]C. Pacheco, S. K. Lahiri, M. D. Ernst, and T. Ball. Feedback-directed random
test generation. In Proceedings of the 29th international conference on Software
Engineering, pages 75–84. IEEE Computer Society, 2007.
[18]A.Panichella,F.M.Kifetew,andP.Tonella. Automatedtestcasegenerationas
a many-objective optimisation problem with dynamic selection of the targets.
IEEE Transactions on Software Engineering, 44(2):122–158, 2018.
[19]S.Scalabrino,G.Grano,D.DiNucci,R.Oliveto,andA.DeLucia. Search-based
testing of procedural programs: Iterative single-target or multi-target approach?
InInternationalSymposiumonSearchBasedSoftwareEngineering,pages64–79.
Springer, 2016.
[20]K.Sen,D.Marinov,andG.Agha. Cute:aconcolicunittestingengineforc. In
ACMSIGSOFTSoftwareEngineeringNotes,volume30,pages263–272.ACM,2005.
[21]P.Tonella. Evolutionarytestingofclasses. In ACMSIGSOFTSoftwareEngineering
Notes, volume 29, pages 119–128. ACM, 2004.
[22]A. Vargha and H. D. Delaney. A critique and improvement of the cl common
languageeffectsizestatisticsofmcgrawandwong. JournalofEducationaland
Behavioral Statistics, 25(2):101–132, 2000.
871
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:48:24 UTC from IEEE Xplore.  Restrictions apply. 