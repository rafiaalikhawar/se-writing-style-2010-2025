Privacy Preserving via Interval Covering Based Subclass 
Division and Manifold Learning Based Bi-directional 
Obfuscation for Effort Estimation  
Fumin Qi1, Xiao-Yuan Jing1,2,*, Xiaoke Zhu1,3, Fei Wu1,2, Li Cheng1 
1State Key Laboratory of Software Engineering, School of Compute r, Wuhan University, China 
2School of Automation, Nanjing University of Posts and Telecommu nications, China 
3School of Computer and Information Engineering, Henan Universit y, China 
*Corresponding author: jingxy_2000@126.com 
 
 
ABSTRACT  
When a company lacks local data in hand, engineers can build an  
effort model for the effort estimation of a new project by util izing 
the training data shared by other  companies. However, one of th e 
most important obstacles for dat a sharing is the privacy concer ns 
of software development organizat ions. In software engineering,   
most of existing privacy-preserving works mainly focus on the 
defect prediction, or debugging and testing, yet the privacy-
preserving data sharing problem  has not been well studied in 
effort estimation. In this paper, we aim to provide data owners  
with an effective approach of privatizing their data before rel ease. 
We firstly design an Interval Covering based Subclass Division 
(ICSD) strategy. ICSD can divide the target data into several 
subclasses by digging a new attribute (i.e., class label) from the 
effort data. And the obtained class label is beneficial to 
maintaining the distribution of t he target data after obfuscati on. 
Then, we propose a manifold learning based bi-directional data 
obfuscation (MLBDO) algorithm , which uses two nearest 
neighbors, which are selected respectively from the previous an d 
next subclasses by utilizing the  manifold learning based neares t 
neighbor selector, as the distur bances to obfuscate the target 
sample. We call the entire approach as ICSD&MLBDO. 
Experimental results on seven p ublic effort datasets show that:  1) 
ICSD&MLBDO can guarantee the privacy and maintain the 
utility of obfuscated data. 2) I CSD&MLBDO can achieve better 
privacy and utility than the comp ared privacy-preserving method s. 
CCS Concepts  
•Software and its engineering → E m p i r i c a l  s o f t w a r e  
validation. •Security and privacy → Privacy-Preserving 
protocols.  
 
Keywords  
Effort estimation; Privacy-pre serving; Locality preserving 
projection; Subclass division. 1. INTRODUCTION 
Engineers usually try to build  a model for estimating/predictin g a 
new project by seeking training samples from other companies, when their own company lacks enough local data. However, 
except the limited publicly avail able datasets, e.g., Promise
1, it is 
very hard to obtain useful data from other companies. The main 
reason for this phenomenon can be summarized as follows: there 
usually exist some sensitive attributes in the data to be share d, 
which may bring privacy disclosure for the data owners (e.g., t he 
leakage of commercial secret), leading to that the data owners are 
unwilling to share their data. Therefore, privacy preserving ha s 
been one of the most important research topics in software 
engineering, and has attracted much attention from both academi c 
and industrial communities [1-19]. 
To avoid privacy disclosure, data owners usually remove the 
sensitive attributes and use privacy-preserving methods to 
obfuscate the data before publishing their dataset. The commonl y 
used general privacy-preserving methods include generalization 
and suppression based methods [1-8], clustering-based methods 
[9-14] and swapping-based methods [15-17, 20]. Recently, a few 
PPDS methods have been presented for software engineering [21-
26]. Most of these methods mainly focus on the applications suc h 
as defect prediction or debugging and testing. Specifically, Pe ters 
et al. [21-23] studied on the PPDS for defect prediction. Claus e 
[24], Taneja et al. [25] and Lo D [26] investigated the privacy -
preserving problem in debugging and testing. 
Privacy preserving is also needed in effort estimation. In prac tice, 
since many indicators (i.e., att ributes) contained in the effor t data 
are related to the properties of the product or the image of a team, 
e.g., Function points count (FP) , Line of code (LOC) and number  
of function point (size), data holders don’t hope these attribu tes to 
be made public. In addition, Peter s et al. [21] mentioned that:  “In 
a personal communication, Barry Boehm stated that he was able 
to publish less than 200 cost estimation records even after 30 
years of COCOMO effort”. All of these mean that the privacy 
threats have hindered people to share their effort data. Theref ore, 
it’s urgent to investigate how t o preserve the data privacy in effort 
data sharing.  
1.1 Background 
In order to facilitate understand ing privacy preserving in effo rt 
estimation and the protected attributes researched in this pape r, we 
offer the following definitions. 
                                                                 
1http://openscience. us/repo/effort/ Permission to make digital or hard copies of all or part of thi s work for 
personal or classroom use is gran ted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that 
copies bear this notice and the full citation on the first page . To copy 
otherwise, or republish, to post on servers or to redistribute to lists, 
requires prior specific permission and/or a fee. Request permis sions 
from Permission@acm.org. 
ASE’16 , September 3–7, 2016, Singapore, Singapore 
Copyright 2016 ACM 1-58113-000-0/00/0010…$15.00 
DOI: http://dx.doi.org/10.1145/2970276.2970302 
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a
fee. Request permissions from Permissions@acm.org.
ASE’16 , September 3–7, 2016, Singapore, Singapore
c2016 ACM. 978-1-4503-3845-5/16/09...$15.00
http://dx.doi.org/10.1145/2970276.2970302
75
Let 11{( , ), ,( , ), ( , )}ii nn Ss y s y s y   denote a data set with n 
samples, where (, )iisyis the thi sample of S, isrepresents 
independent attributes, and iyrepresents dependent attributes. All 
the attributes in S can be classified into the following one or 
more categories [1-17, 28]: 
Sensitive attributes (SA): Attributes we do not want adversaries 
to associate with a target. 
Explicit identifiers:  Attributes that clearly identify individuals. 
Quasi-identifiers (QIDs):  Attributes whose values when taken 
together can potentially identify an individual. 
Privacy threat  refers to the unwanted disclosures of SA, explicit 
identifiers and QIDs. Privacy threats can be categorized into t hree 
types [23]: 1) sensitive attribute values disclosure, 2) identi ty 
disclosure or re-identification and 3) membership disclosure. 
Sensitive attribute occurs when a target is associated with 
information about their sensitive  attributes. Re-identification , 
which occurs when an attacker with external information, can re -
identify an individual from data that has been removed of 
personally identifiable informa tion. Membership disclosure is 
another privacy threat that focuses on protecting a person’s mi cro 
data. Adversaries can identify the relationship between sample 
and target classes according to public data. Like in [23], we 
mainly evaluate ICSD&MLBDO against the first privacy threat, 
i.e., sensitive attribute disclo sure, in this paper. The evalua tion of 
ICSD&MLBDO against the other two types of privacy threats will 
be our future work. 
1.2 Motivation  
Sensitive attribute disclosure problem is one of the main obsta cles 
to effort data sharing. The QID s values are principal component s 
for building an effort model, which are usually contained in th e 
published data.  If the data is published without any 
anonymization processing, the a dversaries can deduce the values  
of related sensitive attributes by using the QIDs values in the se 
data, and obtain relative advant ages in bidding. For example, L OC 
is linked closely with QIDs in effort data and can be obtained 
according to the QIDs values with specific background knowledge , 
and then the adversaries can get ranges of hourly productivity by 
dividing the LOC by effort. To the best of our knowledge, the 
problem of sensitive attributes values disclosure in effort dat a 
sharing has not been well studied. 
Although there exist a number of general privacy-preserving 
methods [1-14], these methods cannot be directly employed to 
solve the privacy-preserving problem in effort data sharing 
effectively. Specifically, gene ralization and suppression based  
anonymization methods [1-8] use  generalization or suppression 
with some rules to replace QIDs values. However, the utility of  
the data obfuscated by these m ethods will decline when most of 
the values tend to be consistent . Clustering-based methods [9-1 4] 
use the mean/center values of some attributes in a cluster, to 
replace the value of the corresponding attribute in the target 
sample from the same cluster. Thus these methods will be 
severely influenced by the clustering parameter k. Swapping-
based methods [15-17, 20] replace the value of an attribute by 
using the other values in the value list of this attribute acco rding 
to certain percentage and rules. These methods will be influenc ed 
by the swapping percentages, and thus their performances are unstable. In recent years, a few privacy-preserving works have been 
presented in software engineering [21-26]. However, these methods cannot be utilized to t ackle the privacy-preserving 
problem in effort data sharing.  Specifically, methods [21-23] 
utilize the nearest unlike neighbors to obfuscate the target da ta 
and remain the utility and privacy of obfuscated data. Methods 
[21-23] are designed to solve the sensitive attributes values 
disclosure problem for defect da ta, and it cannot directly be u sed 
for effort data, since the effort data is different with defect  d a t a  
(the defect data has the class labels while the effort data doe sn’t 
have this attribute). Methods [24-26] are designed for software  
debugging and testing. These met hods cannot be directly used fo r 
effort data due to the follo wing reason: There exists an 
assumption in debugging and testi ng data, that is, the data own s 
detailed connection knowledge betw een parts of a system, yet th is 
assumption doesn’t exist in effort data. 
Researches in [28-29] show that the performance of the estimato r 
and classifier are influenced by data distribution. If the 
distribution of original data can be maintained in obfuscated d ata, 
the capability of estimator or classifier will be kept. Researc hes in 
[21-23] indicate that the class labels of data are helpful to 
maintain the boundaries of subclasses after obfuscation. This 
boundary information is beneficial to preserve the distribution  of 
data after obfuscation. However, there are no class labels in t he 
effort data. Intuitively, there may exist some relationships 
between samples having similar effort. Following this intuition , 
we conducted an experiment: we fir st categorize the samples wit h 
similar effort into the same subclass, and then investigate the  data 
distribution of each subclass. We divide the observed samples i nto 
three equal subclasses according to the effort values, perform 
principal component analysis (PCA) [31] on these samples to 
obtain the principle components, and then use two major PCA 
features of samples to illustrate the distributions. Figure 1 
illustrates the distribution of each subclass in the Kitchenham  and 
Coc81 dataset [32-33]. We can see that the samples from the sam e 
subclass have a roughly similar di stribution. This inspires us to 
design an efficient method to dig class labels from effort valu es 
(i.e., divide effort data into several subclasses according to the 
effort values). Furthermore, the  effort value ranges of differe nt 
subclasses are ordered and di fferent subclasses have clear 
boundary information, which motivates us to use the subclass 
order for protecting the boundary of obfuscated data and 
maintaining the distribution of the data inherited from origina l 
data. 
In this paper, we aim to answer the following research question s: 
RQ 1: In privacy preserving of effort data, how to maintain the 
utility of the privatized data? 
RQ 2: How to effectively preser ve the privacy contained in the 
original effort data? 
 -12000 -10000 -8000 -6000 -4000 -2000 0-1000-500050010001500
second principal componentfirst principle componentKitchenham
  
subclass 1 effort range:[121-1148]
subclass 2 effort range:[1160-2350]
subclass 3 effort range:[2436-14226]
-500 -450 -400 -350 -300 -250 -200 -150 -100 -50 0-6-5-4-3-2-1012
second principle componentfirst principle componentCoc81
  
subclass 1 effort range:[5.90-50]
subclass 2 effort range:[55-237]
subclass 3 effort range:[240-11400]
 
Figure 1. An example of sub-cl ass division for effort data 
761.3 Contribution 
The contributions of our study are summarized as following thre e 
points: 
1. We are among the first to investigate the problem of privacy  
preserving in software effort e stimation data sharing, and prov ide 
an effective solution for this problem. 
2. We design an Interval Covering based Subclass Division (ICSD ) 
strategy for effort data. ICSD digs a new attribute (i.e., clas s label) 
for effort data, which is helpful to maintain the distribution of 
original data after obfuscation.  In addition, for other tasks i n 
which the data has no the class label, ICSD can also be applied  to 
dig class labels for their data, such that the quantitative pro blem 
can be transformed into qualitative problem. 
3. We propose a manifold learning based bi-directional 
obfuscation algorithm (MLBDO) for effort data obfuscation. 
MLBDO uses two nearest neighbors, which are selected from the 
previous subclass and the next s ubclass of target sample by usi ng 
manifold learning based selector, as the disturbance to obfusca te 
the target sample.  
We conduct experiments on the public datasets from Promise 
repositories including NASA93 [34] , Maxwell [35], Kitchenham 
[32], Kemerer [36], Coc81 [33], China [20, 34], and Albrecht [3 7]. 
The experimental results demonstrate that our approach, i.e., 
ICSD&MLBDO, is an effective pr ivacy-preserving approach for 
effort data, and it can protect the privacy of original data an d 
maintain the utility of obfus cated data simultaneously. 
2. RELATED WORK 
2.1 General Privacy-Preserving Methods 
To solve the privacy-preserving problem, a number of general 
privacy-preserving methods have been presented [38], including 
generalization and suppression ba sed methods [1-8], clustering-
based methods [9-14] and swapping- based methods [15-17], etc.  
Generalization and suppression ba sed methods use a less specifi c 
and more general value, which is  faithful to the original value , to 
replace the target value. In the process of generalization, pro per 
suppression strategies are usual ly incorporated to avoid all of  the 
values are generalized into the maximal element (e.g., 8533 and  
9633 are generalized to the same value ‘****’). For example, k-
anonymity [3] requires that each sample is indistinguishable wi th 
at least 1k other samples with respect to the QIDs. However, k-
anonymity cannot ensure the privacy if the attacker has 
background knowledge of the domain [5, 7-8]. l-diversity [7] aims 
to solve the disadvantages in k- anonymity, which requires that the 
distribution of a sensitive attr ibute in each equivalence class  has at 
least l “well-represented” values. However, researches in [6, 8 , 22] 
demonstrate that l-diversity is insufficient to prevent the attribute 
disclosure. t-closeness [8] aims to keep the distance between the 
distributions of a sensitive attribute in a QIDs group and that  of  
the whole table no more than a threshold t apart. Yet, t-closeness 
has the following shortcoming: i t limits the relationship betwe en 
Quasi-identifiers and sensitive attributes, and lacks computati onal 
procedures which allow to reach its goal with minimum data 
utility loss [22,38-40]. 
Clustering-based methods firstly divide the original data into 
several clusters, where the samples within the same cluster are  
related and those from different clusters are unrelated, and th en 
replace the QIDs value of a target sample by using the 
mean/center QIDs value of its c orresponding cluster. For instan ce, 
Aggarwal et al. [14] partitioned the records into several clust ers with each cluster containing at least  k data points (i.e., records), 
and then published the final cluster centers along with some 
cluster size and radius information. Method in [10] uses the 
clustering idea to implement the k-anonymity and wants to find a 
set of clusters (i.e., equivalence classes), each of which cont ains at 
least k records.  The clustering-based methods are sensitive to 
the setting of parameters k and improper selection of parameter k 
will influence the privacy and utility of obfuscated data. 
Swapping-based anonymization me thods select a part of QIDs 
values to replace the other partial QIDs values, which belong t o 
permutation approach that dissoci ate the relationship between a n 
insensitive QIDs and a numerical s ensitive attribute [22]. Esti vill-
Castro and Brankovic [15] propo sed a method, which randomly 
swaps the class labels for priva tizing data. In the Census Bure au’s 
version [42], records are swapped between census blocks for 
individuals or households that have been matched on a 
predetermined set of k variables. These methods are affected by 
the percentage of swapping, and their performances are instable . 
2.2 Privacy-Preserving Methods in Software 
Engineering 
In recent years, a few privacy-preserving methods have been 
presented in software engineering [21-26]. These works mainly 
focus on defect predicti on, testing and debugging.  
Representative privacy-preserving methods for defect prediction  
include LACE1 [21-22] and LACE2 with LeaF [23]. LACE1 is a 
collective name, including tw o methods MOPRH [21] and 
CLIFF&MORPH [22]. MOPRH uses the nearest unlike neighbors 
to obfuscate the target sample, and achieves promising results.  
Considering that the uninformative samples may affect the 
performance of the privacy result and the speed of processing, 
CLIFF&MORPH was presented, w hich employs an instance 
pruner to delete uninformative sa mples. Recently, LACE2 [23] is  
presented to tackle the privac y-preserving data sharing problem  
where data owners can incrementally share their owned data into  
the same data pool.  
Due to the limited resource, co mpanies usually outsource part o f 
software to other companies. Whe n the outsourced software enter s 
the phase of testing, the subcontractors need the relevant data  to 
test the software. However, the data to be provided to 
subcontractors may contain sensitive attribute values, which 
makes the data owners do not wan t to provide real samples for t he 
testing. In order to solve this issue, Taneja et al. [25] propo sed 
PRIvacy Equalizer for Softwar e Testing (PRIEST), which 
combines a new data privacy framework with program analysis 
enabling business analysts to determine the output testing data . 
Budi et al. [26] proposed a ekb anonymity method, which 
creates the testing coverage data based on the concept of “ subpath 
equivalence ”. Clause and Orso [24] designed the Camouflage 
method, which introduces the path condition relaxation and 
breakable input conditions to genera te several anonymized versi on 
sample of original failu re-inducing input sample. 
2.3 Manifold Learning 
Manifold learning is an efficien t dimension reduction technique , 
which can reduce the dimension and preserve the non-linear 
structure of the data [43]. Manifold learning has been applied in 
many practical problems, such as  human action recognition [45],  
voice recognition [46] and face recognition [47].  
Locality Preserving Projections (LPP) [48] is a representative 
Manifold learning method. LPP fir stly uses the information of 
77data points to establish a connection adjacency graph, and then  
computes a transformation matrix  which maps the data points int o 
a subspace, and gets the represe ntation of the data in a lower 
dimensional space. In the low di mensional space, the intrinsic 
dimensionality of the data can b e obtained and the distribution  of 
the original data can be maintained. 
3. OUR APPROACH 
3.1 Overview of Our Approach 
The privacy and utility are two important aspects that need to be 
considered when designing a privacy-preserving algorithm. The 
basic idea of our approach is as follows: we firstly divide the  
effort data into several subclasses with the designed ICSD stra tegy. 
Then, target samples in each subclass are obfuscated by utilizi ng 
the proposed MLBDO algorithm. Figure 2 illustrates an overview 
of our approach for privacy preserving problem in effort data 
sharing. More technical details  will be introduced in subsectio n 
3.2 and 3.3. 

*
11 1 *( ) *( )jj j l jk
ii i i i i obfuscated x x x h x h      
1l
ih 1k
ih
ix (1 )thi ix (1 )thi
 
Figure 2. Illustration of th e framework of our privacy-
preserving approach 
3.2 Interval Covering Based Subclass Division 
Algorithm 
Researches in [21-23] indicate tha t the class labels of data ar e 
helpful to maintain the boundaries of subclasses after obfuscat ion. 
C o n s i d e r i n g  t h a t  t h e r e  a r e  n o  c l a s s  l a b e l s  i n  e f f o r t  d a t a ,  i t  i s 
necessary to dig a new attribute, i.e., class label, for effort  d a t a .  
Figure 1 indicates that the samples with similar effort have 
roughly similar distributions, wh ich motivates us to design a 
subclass division strategy according to the effort values.  
In practice, since the final effort of a new project may be aff ected 
by some uncertainty factors, e.g., changes in funds and 
requirement, the estimation error is inevitable in effort estim ation. 
Denoted by  the estimation error, if the actual value is 500, the 
estimated value should fall i n the interval of [500*(1- ), 
500*(1+)]. Therefore, it is reasonable to consider the estimation 
error in the process of designi ng the subclass di vision strateg y. 
Based on the above analysis, we design the following basic 
dividing criterion: given two samples whose efforts are 1y and 
2y, respectively, if 12 1*( 1 ) *( 1 )yy y    , these two 
samples can be classified into the same subclass. Obviously, th e 
problem of subclass division based on this dividing criterion i s 
actually an interval covering problem [49]. Therefore, we 
formulate our subclass division problem as the following interv al 
covering problem: Given an effort dataset 11{( , ), ,( , ), ,( , )}ii nn X xy xy x y   with 
n samples, (, )iixy denotes the thisample,ix represents the 
independent attributes (QIDs) of the thi sample,iy represents 
effort of the thi samples, then the effort labels of Xcan be 
denoted by 12(, , )n Yy y y . We use {[ , ] | 1 }jj Is f j m   to 
denote mintervals, at the anytime,jjsf, and then we aim to get 
the following output: 
(1)  A numberiy in Y,which is not covered by any interval in  I, 
or 
(2)  A minimum cardi nality subset C of intervals I, which 
collectively covers all points in Y. 
The division process of  ICSD is as follows: 
Step 1: Calculate the tolerance error range of eachiy, i.e., upper 
boundary and lower boundary ofiy, according to Formula (1). 
The obtained ranges of all sa mples are represented by 
11{ [ ,] ,, [ ,] ,, [ ,] }lu lu lu
ii nn Y R yy yy yy   
*
*l
ii i
u
ii iyy y
yy y
                                       (1) 
whereu
iy is the upper bound and l
iy is the lower bound. 
Step 2: Calculate the coverage number of each iy, i.e., how many 
ranges iy is covered by, according to Formula (2). Denote by 
1{,,,, }in CC C C  the coverage numbers  of all samples, 
where iCis the coverage number of iy. 
1n
ii j
jCC
                                          (2), 
where 1, [ , ]
0,lu
ij j
ijif y y yC
otherwise
. 
Step 3: Label samples according to the ascending order of the C. 
For thethisample that has not been categorized into any subclass, 
we classify all the samples covered by the tolerance error rang e of 
iy into a new subclass if these samples have not been labeled. 
The reason of using the ascending order  is that: If a sample has 
higher covering number, then mo re samples may be covered by 
the range of the sample with higher possibility. Therefore, 
labeling samples according to the descending order of C will 
generate subclasses suffering fr om the class imbalance problem 
[50]. 
Step 4: For the sample whose tolerance error range only covers 
itself, we offer two ways to process it: (1) discarding it, in our 
opinion, this kind of sample can be regarded as noise sample th at 
may impair the effort estimation of a new project; (2) Classify ing 
it into the nearest subclasses. 
Researches in [51-56] show that the error between the estimated  
effort and the actual effort is acceptable within 25%. In this paper, 
we set the error tolerance 0.25  (in reality, the researchers can 
modify the tolerance according to their own needs). To help 
understand our ICSD, we provide an example as follows:  
We randomly select a part of samples from dataset NASA93 [34]. 
The effort values of the selecte d samples are shown in Figure 3  (a). 
The tolerance error used in this paper is 0.25 . First , we 
calculate the tolerance error ranges of each effort value accor ding 
78to Formula (1) and store the results in Figure 2 (b). Then  w e  
calculate the range num bers that each effort value is covered b y 
according to Formula (2). For example, the effort 352.8 is cove red 
by the tolerance error ranges of 444 and itself (i.e., 
352.8 [264.6, 441] and352.8 [333, 555] ), the coverage number 
of 352.8 is set as 2 (i.e., 12 C). The effort 72 is covered by 
tolerance error ranges of 72, 90 and itself, the coverage numbe r of 
7 2  i s  s e t  a s  3  ( i . e . ,  23 C). Repeat this process for each effort 
value, and the results is reported in Figure 2 (c). Finally , we label 
each sample according to the ascending order of the coverage 
numbers. For example, from Figure 2 (c), we can see that the 
samples with lowest coverage number (i.e., 1iC) are 24, 48, 
444, 2400, 973, 8211, and from Figure 2 (b), we can see that th e 
tolerance error ranges of 24, 48, 2400, 8211 only cover 
themselves, so we discard these samples; The tolerance error 
range of 444 covers the effort 352.8 and itself 
(i.e., 352.8 [333, 555]  ), we put the efforts 352.8 and 444 into a 
new subclass, whose cl ass label is set as 444. In a similar way , the 
efforts 973 and 750 can be classified into another subclass wit h 
the class label 973. Next, we label the samples with 2iC.We 
firstly check whether these samples have been classified into a  
subclass, and find that 352.8 and 750 have been classified into  
subclasses 444 and 973, respectiv ely, and then efforts 352.8 an d 
750 are skipped. Repeating this process until all samples have 
been labeled or discarded. The division results of ICSD on 
example data are reported in Figure 3 (d). 
 (d) the results of ICSD on (a)effor t352.8 72 72 24 90 48 444 2400 973 750 8211
upper bound 441 90 90 30 113 60 555 300 1216 938 112064
effort 352.8 72 72 24 90 48 444 2400 973 750 8211low er bound 264.6 54 54 18 68 36 333 1800 730 563 6158.3
effort 352.8 72 72 24 90 48 444 2400 973 750 8211
2331311 112 1
sub-classes 72 444 973
effort 72,72,90 444,352.8 973,750C(a) effort labels of partial samples in NASA93 dataset
(b) tolerance error sub-ranges of effort labels in (a)
(c) coverage number of each effort label for (b)
 
Figure 3. An example of effort d ata subclass division via ICSD 
3.3 Manifold Learning-Based Bidirectional 
Data Obfuscation Algorithm 
B y  u s i n g  t h e  d i v i s i o n  s t r a t e g y  I CSD, the dataset is divided int o 
several sub-classes, with each sample in the dataset being 
classified into one subclass or  discarded. In this subsection, we 
describe how we protect the pr ivacy of the samples in each 
subclass and keep their data utility simultaneously.   
Nearest unlike neighbor (NUN) bas ed obfuscation algorithms 
have achieved interesting results in software engineering 
applications [21-23]. However, w hen these algorithms are used 
for obfuscating the effort sampl es in each subclass, they selec t 
NUN samples as disturbance from the whole dataset except the 
subclass to which the target sa mple belongs, and thus the selec ted 
disturbance sample may be a noise  sample (sample that is close to 
t h e  t a r g e t  s a m p l e ,  b u t  h a s  a  s i g n i f i c a n t l y  d i f f e r e n t  e f f o r t  v a l ue). 
This will result in that the target sample is close to the nois e 
s a m p l e  a f t e r  o b f u s c a t i o n ,  w h i c h  i s  h a r m f u l  t o  t h e  u t i l i t y  o f  
privatized data. To solve this p roblem, we design a bidirection al 
obfuscation algorithm. Specifically, it utilizes the order of l abels 
of subclasses, and selects NUN sam ples (disturbance samples) from the previous and next subclasses, such that the influence of 
noise samples can be avoid. The definition of NUN can be found in 
Definition  1. Assume that after ICSD, the original dataset is 
divided into m sub-classes, 1m.The obfuscation strategy is 
shown in Formula (3): 
*
1
*
1
*
11() * , 1
() * ,() * () * , 1jjj l
iii i
jjj k
iii i
jjj l j k
i i ii iixxx h i f i
xxx h i f i m
x xx h x h i fi m


  
  
     (3) 
wherej
ixis the thjsample of the thi subclass in original data, 
*j
ixdenotes the privatized sample ofj
ix, 1l
ihand 1k
ihare two 
disturbance samples, which are t he nearest neighbor samples fro m 
the (1 )thi  and ( 1)thi subclasses of original data respectively, 
for obfuscatingj
ix.andare random values to control the 
obfuscation degree for target sample. The values of  
andrange from 0.05 to 0.20 . 
Definition  1: Nearest unlike neighbor (NUN).  G iven a da tas e t 
1{, ,, , }im SS S S with m sub-classes, iS denotes the thi 
subclass of S, j
iisS denotes the thjsample in iS. We call the 
sample ,l
jjsSi j  as the nearest unlike neighbor (NUN) 
sample of j
is, if the l
js is the nearest neighbor sample of j
is in 
jS. 
In the designed obfuscation strategy, NUN samples are used as 
disturbances, therefore, we should select a proper NUN selector  
for our obfuscation algorithm. Euclidean distance based nearest  
neighbor selector [57] is one of the most popular nearest neigh bor 
selectors in both academic and industrial communities. However,  
o n e  w e a k n e s s  o f  t h e  b a s i c  E u c l i d e an distance selector is that i f 
one of the input attributes has a relatively large range, then it can 
overpower the other attributes, leading to that the “true” NUN 
sample may be missed [58]. He nce, we should filter those 
attributes that may overpower the other ones when selecting the  
NUN samples. 
As described in subsection 2.3, L PP is a representative manifol d 
learning method, which is similar with some spectral graph theo ry 
[59] technologies (e.g., spectra l clustering) used for software  
engineering. LPP can obtain th e intrinsic dimensionality and 
preserves the structure of data.  This property is beneficial fo r 
selecting a more precise sample.  Therefore, we design a manifol d 
learning based nearest unlike neighbor selector . Specifically, we 
use LPP as the basic method to m ap the original data into an 
“intrinsic dimensionality” space, and then select the NUN sampl e 
in the projection space.  
Given a dataset 1[, ,, , ]in X xxx  experiencing the ICSD 
process, LPP aims to find out a transform matrix A and an equal 
class 1[, ,, , ]in Z zzz , whereT
iizA x . The procedure of LPP 
[48] is formally stated as follows: 
1) Constructing the adjacency graph: Let G denote a graph 
with m nodes. Nodes i and j are connected by an edge, if ix 
and jx are “close”. “Close” is measured by following two 
methods:  
 -neighborhoods, R, if 2
ijxx  , then ix and jx 
are “close”. 
79 k-nearest neighbor. kN , ix and jx is “close”, if 
(, )ijxnearest x k  or (,)jixnearest x k , *(, ) nearest x k  
denotes the k nearest neighbors of *x. 
2) Choosing the weights: the weight ijwbetween ix and jx has 
two forms: 
 Heat Kernel, 2
ijxx
t
ijwe
 , tR , 
 Simple minded, 1ijw if and only if vertices i a n d  j a r e  
connected by an edge.  
The justification for the choice of weights can be traced back to 
[44].  
3) Eigenmaps ： Compute the eigenvectors and eigenvalues for 
the generalized eig envector problem: 
TTXLX a XDX a                                    (4) 
where D is a diagonal matrix whose entries are column (or row, 
since D is symmetric) sums of W, ii ji jDW . LD W  is 
the Laplacian matrix. Thethi column of matrix Xis ix. 
Let the column vectors 01,,l aa  be the solutions of Equation (4), 
ordered according to their eigenvalues 01 l  . Thus, the 
embedding is as follows: 
01 ,( , , )T
ii i lx zA x Aa a    , 
whereizis a l-dimensional vector, and A is a nl matrix. 
After obtaining t he equal class Z, then we can obfuscate each 
sample j
ix of the thisub-class iX with it. We firstly get two 
NUN samples’ indexes of j
ix from ( 1)thi  and ( 1)thi  sub-
classes of Z respectively, and then we select the two NUN 
samples 1l
ih and 1l
ih from S according to this two indexes. 
Finally, we can obfuscate the j
ixaccording to Equation (3). We 
call the bidirectional obfuscatio n algorithm with the manifold 
learning based nearest unlike neighbor selector as Manifold 
learning based Bidirectional Data Obfuscation (MLBDO) 
algorithm. 
3.4 An Example of ICSD&MLBDO  
In this subsection, we provide a complete example to explain ou r 
ICSD&MLBDO approach. 
We randomly select a part of samples from NASA93 [34], and set 
the tolerance error as 0.25 . The selected samples are shown 
in Figure 4 (a). We firstly use ICSD to divide the data into se veral 
subclasses (the detail process of subclasses division is shown in 
Figure 3). The original data is divided into 3 sub-classes and the 
division result is shown in Figure 4 (c). Next, we use the MLBD O 
to obfuscate the samples in Figure 4(c), and the result of 
obfuscation is reporte d in Figure 4 (d). 
Next, we test whether the privacy of the effort data has been 
protected successfully. We suppose that the KLOC attribute has 
been removed after obfuscation, and the adversary has the relat ed 
background knowledge about NASA da taset. The test strategy is 
as follows: The adversary sends a query to original dataset and  
privatized dataset respectively, then both datasets will return  a  
group of answers for this query; If the result returned from th e 
original dataset is equal to that returned from the privatized 
dataset, the protection for this sample for this attack is rega rded as 
a failure, and vice versa. In the testing process, for the orig inal and privatized data, we first divide the range of possible values o f 
each attribute into n bins/sub-ranges by employing the Equal 
frequency binning (EFB) method [21]. Here, n is set as 4. Figure 
4 (b) shows the equal frequency binned version of Figure 4 (a).  
Figure 4 (e) shows an equal frequency binned version of Figure 4 
(d). Assume that the adversary sends the same query cplx=[1.15-
1.3] to Figure 4 (b) and Figure 4 (e) respectively, where 
cplx=[1.15-1.3] means “Please return the sensitive attribute va lues 
of the samples with the cplx attribute value of [1.15-1.3]”. Th e 
Figure 4 (b) will return the sensitive values of 10# and 11# 
samples, i.e., (66.6-233]  and  (66.6-233] . The Figure 4 (e) will return 
the sensitive values o f 2# samples, i.e., (6-10] . From the results, we 
can see that the returned results of Figure 4 (b) and (e) are 
different, which means that the ICSD&MLBDO successfully 
protects the privacy of the data  under the requirement that que ry 
cplx=[1.15-1.3] and query size=1. 
(e)  EFB version of (d)(d) the obfuscated data after ICSD&MLBDO(a) the remained samples of (a) after ICSD(b) EFB version of (a)(a) randomly selected partial samples from NASA93# cplx acap Pcap kloc effort
1 1.15 1 1 66.6 352.8
2 1.15 0.86 0.86 7.5 72
3 1.15 0.86 0.7 20 72
4 1.15 0.86 0.86 6 24
5 1 1 0.86 15 90
6 1 1 0.86 10 48
7 1.15 1 0.86 90 444
8 1.15 1 0.86 302 2400
9 0.85 0.86 1 284.7 973
10 1.3 0.86 1 101 750
11 1.3 0.86 0.86 233 8211
# cplx acap pcap kloc effort
1 (1-1.15] (0.86-1] (0.86-1] (10-66.6] 352.8
2 (1-1.15] [0.86-0.86] [0.7-0.86] [6-10] 72
3 (1-1.15] [0.86-0.86] [0.7-0.86] (10-66.6] 72
4 (1-1.15] [0.86-0.86] [0.7-0.86] [6-10] 24
5 [0.85-1] (0.86-1] [0.7-0.86] (10-66.6] 90
6 [0.85-1] (0.86-1] [0.7-0.86] [6-10] 48
7 (1-1.15] (0.86-1] [0.7-0.86] (66.6-233] 444
8 (1-1.15] (0.86-1] [0.7-0.86] (233-302] 2400
9 [0.85-1] [0.86-0.86] (0.86-1] (233-302] 973
10 (1.15-1.3] [0.86-0.86] (0.86-1] (66.6-233] 750
11 (1.15-1.3] [0.86-0.86] [0.7-0.86] (66.6-233] 8211
# cplx acap pcap kloc effort subclasses
2 1.15 0.86 0.86 7.5 72
3 1.15 0.86 0.7 20 72
5 1 1 0.86 15 90
7 1.15 1 0.86 90 444
1 1.15 1 1 66.6 352.8
9 0.85 0.86 1 284.7 973
10 1.3 0.86 1 101 75072
444
973
# cplx acap pcap Kloc effort
2 1.16 0.87 0.87 7.5 72
3 0.93 0.64 0.51 20.0 72
5 1.46 1.35 1.24 15.0 90
7 1.58 1.29 1.19 90.0 444
1 0.92 0.77 0.80 66.6 352.8
9 1.06 1.07 1.18 284.7 973
10 1.15 0.71 0.87 101.0 750
# cplx acap pcap kloc effort
2 (1.15-1.3] (0.86-1] (0.86-1] [6-10] 72
3 [0.85-1] (-*-0.86] (-*-0.7] (10-66.6] 72
5 (1.3-*) (1-*) (1-*) (10-66.6] 90
7 (1.3-*) (1-*) (1-*) (66.6-233] 444
1 [0.85-1] (-*-0.86] [0.7-0.86] (10-66.6] 352.8
9 (1-1.15] (1-*) (1-*) (233-302] 973
10 (1-1.15] (-*-0.86] (0.86-1] (66.6-233] 750
Figure 4. An example of ICSD&MLBDO 
3.5 Answers for Two Research Questions  
A n s w e r s  f o r  R Q 1 :  In privacy preserving of effort data, how to 
maintain the utility of the obfuscated data? 
In order to achieve this purpose, we design the ICSD to classif y 
the samples with similar effort into the same class, and the 
obtained ordered class labels p rovide a guideline for obfuscati ng 
the target sample. In the process of obfuscation, instead of 
selecting NUN samples from all of other subclasses, we select t he 
NUN samples from the previous and the next subclasses 
80according to the class labels, and use the obtained NUN samples  
as disturbances to obfuscate the target sample. In this way, th e 
obfuscated target sample will not invade the boundaries of the 
other subclasses that have significantly different effort value s, 
which means that the data distribution can be maintained after 
obfuscation. 
Answering for RQ2: How to effectively preserve the privacy 
contained in the ori ginal effort data? 
In order to achieve this purpose, we design the MLBDO algorithm  
to obfuscate the target sample. MLBDO uses two NUN samples, 
which are selected by using manifold learning based nearest 
unlike neighbor selector, as the disturbance samples to obfusca te 
the target sample bi-directionally. The bi-directional strategy  can 
not only avoid the influence of noise sample but also increase the 
privacy of obfuscation. 
3.6 Comparison with Related Works 
In this subsection, we provide a discussion about the differenc es 
between our privacy-preserving method and related privacy 
preserving methods [3,19-24]. 
Comparison with general pr ivacy preserving methods:  T h e  
main differences between our approach and general privacy 
preserving methods [3, 20] are two-folds: (1) These methods are  
not designed for tasks in software engineering, while our metho d 
is designed for the PPDS of effort data. (2) These methods don’ t 
consider the influence of outli er samples, while our approach c an 
cut these samples automatically, such that the remaining sample s 
are more suitable for building a precise estimation/prediction 
model. 
Comparison with privacy pres erving methods in software 
engineering: The main differences between our approach and 
these methods [21-26] are two-folds: (1) Different with these 
methods, which use one NUN samp le as the disturbance to 
obfuscate the target sample, our  method uses two NUN samples as  
the disturbance to obfuscate the target sample bi-directionally . By 
using the bi-directional strategy, our method can avoid using t he 
noise sample as the disturbance, whose effort values are 
significantly different from thos e of target sample. The utiliz ation 
of noise samples will affect the maintaining of data distributi on. (2) 
When maintaining the utility of obfuscated data, we not only 
consider the influence of obfuscation ranges but also consider 
maintaining the data distr ibution after obfuscation.  
Table 1. Brief properties of used data sets with different quer y 
size 
Dataset Number of 
samples Number of 
attributes Minimum 
effort value Maximum 
effort value 
Nasa93 93 18 8.4 8211 
Maxwell 62 26 583 63694 
Kitchenham 145 5 219 113930 
Kemerer 15 15 23.2 1107.31 
Coc81 63 17 5.9 11400 
China 499 18 26 54620 
Albrecht 24 8 0.5 105.2 
4. EXPERIMENTS 
4.1 Data Sets 
To evaluate the performance of our ICSD&MLBDO approach, we 
conduct extensive experiments on  seven effort datasets, includi ng 
NASA93 [34], Kitchenham [32], Kemerer [36], Coc81 [33], 
China [34, 60],Albrecht [37] and Maxwell [35]. The metrics of 
these datasets are based on COCOMO [54, 60-61] or Function Points [32, 36, 60,62-63]. The brief properties of these datase ts 
used in this paper are shown in Table 1. The metrics of these datasets are show in Figure 5. 
rely Required software reliability durationTotal elapsed time for the
project NlanNumber of different developmentlanguages used
dataData base size RAWFP Raw function points count T01 Customer participation
turn Turnaround tim e AdjFP Adjusted function points T02Development environm entadequacy
time Time constraint for cpu FPAdj Transformation of RAWFP T03 Staff availability
storMain m em ory constraint enquiryFunction points(ufp) of ex ternalenquiryT04 Standards use
virt Machine volatility fileUfp of internal logical files orentity referencesT05 Methods use
toolUse of  software tools Pdr_ufpNormalized level 1 productivitydelivery rateT06 Tools use
dcedSchedule constraint Npdr_afpNormalized productivitydelivery rateT07 Software’s logical complexity
aexp Application ex perience outputFunction points(ufp)of ex ternaloutputT08 Requirements volatility
pcap Program mers capability T09 Quality requirements
vexpVirtual machine ex perience Npdu_ufp Productivity delivery rate T10 Efficiency requirements
lexpLanguage ex perience input Function points(ufp) of input T11 Installation requirements
modp Modern programing practices afp Adjusted function points T12 Staff analysis skills
cplxProcess com pl ex ity KSLOC Thousands of lines of code T13 Staff application knowledge
acapAnalysts capability App Application type T14 Staff tool skills
LOCLine of code har Hardware platform T15 Staff team skills
dba Databas eduratio
nDuration
interfaceFunction points(ufp) ofex ternal interface addedifc User interface time Time
changedFunction points(ufp) ofchanged functionssource Where developed sizeApplication size(num ber offunction points)
deletedFunction points(cfp) ofdeleted functionstelonuse Telon use Effort
resource Team typeCOCOMO
Function pointsMaxwell Function points
Maxwell
 
Figure 5. The metrics of the dat a sets used in this work. The 
red rectangle box outlines the sensitive attributes used in thi s 
paper. 
4.2 Evaluation Measures 
The privacy-preserving methods s hould ensure that the privatize d 
data has both favorable privacy  and utility. To evaluate the 
privacy and utility of the privatized data, we employ the follo wing 
measures. 
4.2.1 Measure of Privacy 
We use the Increased Privacy Ratio (IPR) [23] to evaluate the 
privacy ability of a method. Given 12{, , , }N Qq q q  denotes N 
queries. Informally, the IPR can be defined as follows 
01100 * ( ) 1Q
i
iPI P R T KQ                     (5), 
where T represents the dataset to be evaluated, and 
'
max max 1 , () ()
0, .iiTT
iif S R S RK
otherwise  
. 
Here, *
max()Ti SR  is the highest frequency value of *
TiR. *
TiRis the 
results of thiquery, which is a group of value from any dataset 
matches the thi query. For example, '{(1 2],(1 2],[3 6]}TiR    
denotes the results of thi query returned from privatized data, and 
then '
max() { ( 1 2 ] }Ti SR . The higher IPR the better a privacy-
preserving method. The query generator used in this paper 
detailed in subsection 4.3.   
4.2.2 Measure of Utility  
Median Magnitude of Relative E rror (MdMRE) and Pred(25) [51-
53] are two commonly used measures for evaluating the effort 
81estimation accuracy of estimators. In the experiments, we emplo y 
both measures to evaluate the utility of the privatized data. T he 
definitions of MdMRE and Pred(25) are as follows: 
Given a sample ix with the actual effort being iy, if the predicted 
effort is iy, the Magnitude of Relative Error (MRE) of ix can be 
calculated by ii
i
iyyMREy . Then, MdMRE of N samples 
can be computed as 
1 (, ,, , )iN MdMRE median MRE MRE MRE  .  
Pred(25) is defined as the percentage of estimated values 
falling within 25 percent of the actual values: 
11, 0.25 100(25)0,N
i
iif MREPREDotherwise N 
 . 
For these two measures, the lower of MdMRE represents the 
better performance of estimator, and the higher Pred(25) 
represents the estimator s being more precise. 
4.3 Query Generator 
In this subsection, we introduce t he query generator used in th is 
paper. Assume the query size is 1, the process of the generator  to 
create a query is as follows: 
(1) Randomly select an attribute from QIDs attributes (except 
sensitive attributes). For example, randomly select an attribut e 
from Figure 4 (b), here, assume that the cplx attribute has bee n 
selected by the generator. Then, the generator will find out th e 
distinct ranges from cplx, i.e., 
{[0.85 1],(1 1.15],(1.15 1.3]}rangescplx   . 
(2) Randomly select a range from rangescplx  as a query. For 
example, the generator randomly selects the [0.85-1] then the n ew 
created query is [0.85 1]cplx, which means “Please return the 
sensitive attribute values of the samples with the cplx attribu te 
value of [0.85-1]”. 
Table 2 shows the examples of ge nerated queries with different 
query sizes(1, 2) on the data of Figure 4 (b). 
Table 2. Examples of generated queries  
Query size QIDs Return 
1 [0.85 1]cplx (10 66.6] ,[6 10] ,(233 302]  
2 [0.85 1] [0.86 1], cplx acap (10 66.6] ,[6 10]  
4.4 Experimental Settings 
Assume that a dataset has im QIDs attributes, with the thj 
attribute has jn distinct sub-ranges. If the query size is p, we can 
get ()
ip p
mjCn quires. It is unrealistic and unnecessary to 
enumerate all possible quires of all possible query sizes, 
especially when the number of QID s attributes is very large. Fo r 
example, the dataset NASA93 ha s 16 QIDs attributes, if each 
QIDs attribute has 10 distinct sub-ranges and query size is 
respectively 1, 2, and 4, the n the generator will create 
11 22 44
16 16 16 1,821,26 (10) (10) * (1 ) 0 0 CCC   queries. Thus, in 
our experiments, the selected query sizes include 1, 2, and 4, and 
up to 1000 queries are generated  for each query size. The numbe r 
of sensitive attributes and the si ze of EFB used in this paper are 
listed in Table 3. The sensitive attributes used in this paper 
include LOC (KLOC), AFP and size, because these attributes are 
very important influence factors for bidding, and the data owne rs 
may not want these data to be obt ained by adversaries. In addit ion, KLOC, AFP and size can be converted to each other [37][64]. We 
choose the k-nearest neighbor as the “close” measure, and the 
“Heat Kernel” as the weight measure in LPP, respectively. 
In the experiment of utility, w e randomly select 70% modules in  
each of effort dataset for training and the remained modules ar e 
used for testing. The random selection process for training and  
testing data may be biased and may affect the evaluation 
performance. Therefore, we repe at random selection 20 times and  
report the average estimation results. 
Table 3. Experimental settings used in this work 
Dataset Number of QIDs Sensitive attributes  EFB size 
Nasa93 16 KLOC 10 
Maxwell 24 SIZE 10 
Kitchenham 3 AFP 10 
Kemerer 13 KSLOC 10 
Coc81 15 LOC 10 
China 16 AFP 10 
Albrecht 6 AFP 10 
4.5 Evaluation of Privacy and Utility for 
ICSD&MLBDO 
For this part, we perform experiments to evaluate the privacy a nd 
utility for our approach and the applicability of our privacy-
preserving approach for different effort estimators, respective ly. 
Compared methods . To benchmark our method, we compare our 
approach with four methods, including k-anonymity [3], swapping  
[22], clustering [30] combined with MLBDO 
(Clustering&MLBDO) and ICSD combined with MORPH [21] 
(ICSD&MLBDO). We implement th e k-anonymity by following 
the Datafly algorithm [3], and create two versions of k-
anonymity, namely 2-anonymity and 4-anonymity. In swapping 
[22], for each QIDs attributes, a certain percent of values nee d to 
be replaced by any other distinguishable values in that QIDs. I n 
the experiments, the used percen tages include 10%, 20% and 40%.  
The Clustering&MLBDO method is used to evaluate the 
performance of the designed ICSD  strategy. In experiments, K-
Means [29] is employed as the clustering algorithm with the 
statistical significance level 0.0001 . The ICSD&MORPH is 
used to evaluate the performance  of the proposed bi-directional  
obfuscation algorithm. 
Results of privacy and utility testing . In the experiments, the 
classic classification and re gression trees (CART) [63] is 
employed as the baseline estim ator. We conduct our approach and  
the compared methods on 7 datase ts, and then compute the IPR, 
Pred(25) and MdMRE of each method. Due to the limited space, 
Figure 6 only provides the experi mental results with query size =1. 
F r o m  F i g u r e  6 ,  w e  c a n  s e e  t h a t  t h e  p r i v a c y  a n d  u t i l i t y  o f  
ICSD&MLBDO are better than those of the competing methods. 
Compared with k-anonymity, our approach achieves significantly 
better utility performance. The r eason is that k-anonymity uses  the 
“generalization” strategy to protect the sensitive values, lead ing to 
that the information used for m odeling becomes less. Compared 
with swapping [22], our approach can achieve more stable 
performance. The main reason is  that swapping method replaces 
the values of sensitive attributes with the random strategy. 
Compared with ICSD&MORPH, ICSD&MLBDO achieves 
higher privacy performance. The main reasons, in our opinion, a re 
two-folds: (1) MORPH uses the global NUN sample selected from 
all other subclasses as the disturbance to obfuscate the target  
sample, while MLBDO selects the NUN samples from previous 
and next subclasses, and thus the disturbance ability of the NU N 
82samples selected by MORPH ma y be lower than that of NUN 
samples selected by MLBDO. (2) The designed bi-directional obfuscation algorithm of MLBDO can provide more powerful 
obfuscating ability. Compared with Clustering&MLBDO, 
ICSD&MLBDO achieves higher utility. The reason is that ICSD 
considers the estimation error in the process of subclass divis ion, 
and removes the outlier sample automatically. ICSD&MLBDO 
combines the advantages of ICSD and MLBDO, and therefore 
better performances in privacy and utility can be achieved. 
The boxplot figures in Figure 6 s how the variability of utility  
measures of different privatized data obtained by using differe nt 
privacy methods, we can see that the variability of 
ICSD&MLBDO is better than that  of compared methods. Figure 7 
illustrates the experiment results under different query sizes (1, 2 
and 4) on different datasets. We can see that our approach obta ins 
higher privacy performances than the compared methods under all  
three query sizes. 
Results of applicability testing . To investigate whether the 
privatized data with our approach can be applied to multiple 
estimators, we design another expe riment. We select another two  
estimators including automatically transformed linear model 
(ATLM) [54] and radial basis function networks (RBFN) [51, 64].  
Tables 4 and 5 report the average effort estimation results of 
ICSD&MLBDO and the competing methods with these two 
estimators on seven datasets for the MdMRE and Pred(25) 
measures, respectively. In Tables 4 and 5, we also report the 
estimation results using original data, and the results are cal led as 
“normal”. From both tables, we can see that the performances of  
our approach are still better tha n those of the competing metho ds, 
which indicates that the privatized data by using our approach can 
well apply to different estimators. To statistically analyze th e 
results given in Tables 4 and 5,  we conduct a sta tistical test,  i.e., 
Wilcoxon test [55-56], with 95 per cent confidence to get the so -
called win-tie-loss (w/t/l) results. “Win” means the results of  our 
approach is significantly diffe rent with compared methods, and “tie” means “equal”, otherwise “lose”. We can see that the 
proposed approach makes a signifi cant difference in comparison 
with other compared methods. 
NASA93 Maxwell Kitchenham Kemerer Coc81 China Albrecht0  20 30405060708090100
DatasetsIPR(%)Query Size=1
NASA93 Maxwell Kitchenham Kemerer Coc81 China Albrecht2030405060
DatasetsIPR(%)Query Size=2
NASA93 Maxwell Kitchenham Kemerer Coc81 China Albrecht50556065
DatasetsIPR(%)Query Size=4
  ICSD&MLDBO
2-anonymity
4-anonymity
swapping-10%
swapping-20%
swapping-40%
ICSD&MORPH
Clustering&MLBDO 
Figure 7. Privacy comparision o f  a l l  m e t h o d s  o n  7  d a t a s e t s  
versus different query sizes. 
 
5. THREATS AND VALIDITY 
Followings are several potential threats to the validity with r espect 
to experiments: 
(1) Bias of estimators. A bias i n this study is the estimators we 
u s e d  f o r  e f f o r t  e s t i m a t i o n .  I n  experiments, we select three 
commonly used estimators to eva luate our approach. As to more 
other estimators, experiments mi ght need to be done to evaluate  
our approach.  
(2) Bias of evaluation measures. Another bias is the MdMRE and 
Pred(25) measures used to repor t the imputation or estimation 
performances. Other measures, such as Mean Balanced Relative 
Error (MBRE) [34], the Mean Inve rted Balanced Relative Error 
(MIBRE) [34], CLUSTER [55] and Standardized Accuracy [68] 
are not used. In this work, we employ the widely used MdMRE 
and Pred(25) measures to show the empirical evaluation for the 
application of softwar e effort estimation. 
0 20 40 60 80 100011.823.635.447.259MdMRE
IPR(%)NASA93
0 20 40 60 80 100011.222.433.644.8MdMRE
IPR(%)Maxwell
0 20 40 60 80 100014.428.843.257.6MdMRE
IPR(%)Kitc henham
0 20 40 60 80 10007.915.823.731.6MdMRE
IPR(%)Kemerer
0 20 40 60 80 100013.52740.55467.5MdMRE
IPR(%)Coc81
0 20 40 60 80 100017.935.853.771.6MdMRE
IPR(%)China
0 20 40 60 80 10009.218.427.636.8MdMRE
IPR(%)Albrecht
  0 50 100020406080100
Pred(25)(%)
1020304050
n o r m a l o u r s 1s 2s 4k 2k 4i m c mPred(25) of NASA93Pred(25)(%)
202530354045
normalour s1 s2 s4 k2 k4 im cmMdMRE of NASA93MdMRE
0 50 100020406080100
Pred(25)(%)
010203040
normalour s1 s2 s4 k2 k4 im cmPred(25) of MaxwellPred(25)(%)
2030405060
n o r m a l o u r s 1s 2s 4k 2k 4i m c mMdMRE of MaxwellMdMRE
0 50 100020406080100
Pred(25)(%)
1020304050
normalour s1 s2 s4 k2 k4 im cmPred(25) of KitchenhamPred(25)(%)
20406080100
normalour s1 s2 s4 k2 k4 im cmMdMRE of KitchenhamMdMRE
0 50 100020406080100
Pred(25)(%)
01020304050
n o r m a l o u r s 1s 2s 4k 2k 4i mc mPred(25) of KemererPred(25)(%)
102030405060
normalour s1 s2 s4 k2 k4 im cmMdMRE of KemererMdMRE
0 50 100020406080100
Pred(25)(%)
0102030
n o r m a l o u r s 1s 2s 4k 2k 4i mc mPred(25) of Coc81Pred(25)(%)
40506070
normalour s1 s2 s4 k2 k4 im cmMdMRE of Coc81MdMRE
0 50 100020406080100
Pred(25)(%) 204060
n o r m a l o u r s 1s 2s 4k 2k 4i mc mPred(25) of ChinaPred(25)(%)
20406080100120140
normalour s1 s2 s4 k2 k4 im cmMdMRE of ChinaMdMRE
0 50 100020406080100
Pred(25)(%)Pred(25) of swapping 10% (s1)
MdMRE of swapping 10% (s1)
Pred(25) of swapping 20% (s2)
MdMRE of swapping 20% (s2)
Pred(25) of swapping 40% (s4)
MdMRE of swapping 40% (s4)  
Pred(25) of original data (normal)
MdMRE of original data (normal)
Pred(25) of ICSD&MLBDO (our)
MdMRE of ICSD&MLBDO (our)
Pred(25) of 2-anonymity (k2)
MdMRE of 2-anonymity (k2)
Pred(25) of 4-anonymity (k4)
MdMRE of 4-anonymity (k4)  
Pred(25) of Clustering&MLB DO (cm)
MdMRE of Clustering&MLBDO (cm)
Pred(25) of ICSD&MORPH (im)
MdMRE of ICSD&MORPH (im)
Privacy baseline with IPR=80%
Pred(25) line of original data
MdMRE line of original data
0204060
n o r m a l o u r s 1s 2s 4k 2k 4i mc mPred(25) of AlbrechtPred(25)(%)
0204060
normalour s1 s2 s4 k2 k4 im cmMdMRE of AlbrechtMdMRE
 
Figure 6. Comparison of privacy- preserving methods using the CA RT estimator on MdMRE and Pred(25) measures with query 
size=1 
The horizontal dashed thin line represents  the Pred(25) measure s, the dashed thick line represen ts the MdMRE measures, and the  vertical 
line denotes the privacy baseline with IPR=80%. 
83Table 4. Comparison of utility r esults using two estimators wit h seven data sets on MdMRE  
Estimator methods params NASA93 Maxwell Kitchenham Kemerer Coc8 1 China Albrecht 
ATLM k-anonymity k=2 35.1±0.89 34.4±5.08 33.3±3.6 10.5±6.72 41± 10.81 43.8±5.47 13.8±11.22 
k=4 39.7±1.76 38.4±2.74 33.9±1. 94 11.3±5.95 52.3±12.56 45.3±4.2 1 18.8±14.61 
Swapping(N%) N=10 35.7±0.95 34.3± 2.62 33.2±2.06 10.3±5.23 42.1± 6.1 43.2±1.49 13.3±9.91 
N=20 37.7±1.16 38.2±3.48 34.8±2.16  13.1±6.33 45.7±6.57 44.1±1.5 2 14.5±12.57 
N=40 39±1.28 39.4±3.02 39.2±2.09  12.1±5 37.1±13.34 49.5±1.88 16 .7±8.69 
ICSD&MORPH 35.6±1 34.9±2.38 33. 2±1.86 12±6.22 42.2±11.49 45.2±6 .45 12.4±6.82 
Clustering&MLBDO 37±0.92 36.5±2. 82 34.1±1.95 11±4.22 42±14.3 47 .4±4.92 13.6±7.52 
ICSD&MLBDO 33.8±0.97 33.1±3.55 32.3±2.06 9.7±5.53 41.6±9.24 42.6±6.71 13.3 ±9.76 
Normal 31.6±1.16 29.7±2.76 30±1.88 8.8±5.46 39.3±6.49 40±1.47 11.3±10.01 
statistical  test w/t/l 7/0/1 7/0/1 7/0/1 7/0/1 4/2/2 7/0/1 5/2/1 
RBFN k-anonymity k=2 40.2±3.73 33.6±1.35 25.6±5.41 15.5±16.9 50 .7±25.65 22.2±4.15 18.1±1.47 
k=4 45.7±2.25 39.6±1.32 35.1±4. 97 19.8±30.2 58.8±19.72 27.3±5.0 5 23.6±6.09 
Swapping(N%) N=10 42.3±2.52 31.2± 1.02 27.4±4.46 14.5±10.8 50.4± 22.77 21.4±8.24 20.6±1.91 
N=20 48.1±4.25 37±1.1 35.3±4.5 16.1±8.73 55.6±16.5 24.6±10.1 25 .5±1.82 
N=40 53±3.41 43.2±1.52 41.3±4. 43 17.7±10.3 61.4±10.88 33.6±9.25  31.1±1.56 
ICSD&MORPH 42.2±13.1 33.3±3.68 26.3±5.25 12.7±7.36 49.2±19.44 2 3.1±13.6 14.4±1.7 
Clustering&MLBDO 46.7±12.9 35.4±3. 61 28.3±5.29 13.7±7.31 51.9±1 5.48 25.9±13.4 16.9±1.4 
ICSD&MLBDO 38.6±6.1 29.3±3.99 23.6±5.36 12.3±8.55 48.2±22.09 19.1±10.7 15. 5±1.99 
Normal 35±3.17 26.7±1.7 20.4±4.46 11.1±9.56 46.8±21.65 17.1±10.3 10.5±1.7 
statistical test w/t/l 7/0/1 7/0/1 7/0/1 5/0/3 7/0/1 7/0/1 6/0/ 2 
Table 5. Comparison of utility r esults using two estimators wit h seven data sets on Pred(25)  
Estimator  methods params NASA93 Maxwell Kitchenham Kemerer Coc81 China Albrecht 
ATLM k-anonymity k=2 56.7±0.08 38.4±0.09 29.8±0.07 28±0.21 49.9 ±0.07 70.3±0.03 32.5±0.16 
k=4 48.5±0.06 36.3±0.11 27.6±0.08  16±0.12 37.4±0.06 64.4±0.03 1 8.5±0.12 
Swapping(N%) N=10 55.8±0.08 40.1± 0.08 29±0.09 27±0.25 49.1±0.07  69.8±0.04 33.1±0.13 
N=20 49.9±0.07 35.5±0.09 27.4±0.09  22±0.13 42.6±0.05 65.5±0.03 21.5±0.17 
N=40 50±0.06 28±0.08 27.6±0.09 21± 0.17 39.9±0.07 56±0.04 14.9±0 .13 
ICSD&MORPH 55.1±0.08 37.4±0.11 29.5±0.1 27±0.21 47.2±0.03 69.2± 0.03 31.8±0.14 
Clustering&MLBDO 51±0.09 38.3±0.12  29±0.12 25±0.21 45.4±0.04 67 .3±0.02 28±0.18 
ICSD&MLBDO 57.1±0.08 40.1±0.07 30±0.09 30±0. 2 50.2±0.04 72.2±0.04 30.6±0.1 7 
normal 58.1±0.07 42.3±0.08 30.6±0.09 32±0.24 52.3±0.05 73.4±0.04 33.1±0.16 
statistical  test w/t/l 5/0/3 7/0/1 6/1/1 4/0/4 6/1/1 7/0/1 8/0/0 
RBFN k-anonymity k=2 20.9±0.12 15.9±0.1 27±0.06 33.9±0.15 16.9± 0.13 20.9±0.02 37.8±0.11 
k=4 19.5±0.03 11.2±0.14 19±0.05 28.4±0.13 13.3±0.11 17.5±0.02 3 6.2±0.16 
Swapping(N%) N=10 20.1±0.14 14.6± 0.11 26.4±0.05 33.9±0.12 16.6± 0.12 20.4±0.02 38±0.15 
N=20 18.6±0.14 9.8±0.13 21.8±0.05 27.3±0.14 15.9±0.13 16±0.03 3 0.4±0.15 
N=40 14±0.14 8.5±0.11 17.4±0.05 24.1±0.12 10.8±0.14 14.7±0.02 2 6.5±0.12 
ICSD&MORPH 28.8±0.16 16.4±0.14 24.9±0.06 30.5±0.14 17.4±0.12 20 .2±0.09 27.5±0.13 
Clustering&MLBDO 27.3±0.13 13.1±0. 08 25.6±0.06 29.3±0.12 16.6±0 .15 19.7±0.01 39±0.09 
ICSD&MLBDO 22±0.17 16.6±0.13 26.3±0.05 34.8± 0.14 17.6±0.09 21.6±0.02 39.9± 0.12 
normal 23.2±0.08 18±0.15 28±0.05 35.3±0.12 18.2±0.15 24.7±0.02 40.5±0.13 
statistical test w/t/l 6/1/1 6/0/2 6/0/2 7/0/1 8/0/0 7/0/1 7/0/ 1 
 
 (3) Bias of compared methods. In the family of anonymization, 
there exist many anonymization methods, and it is hard to 
compare our ICSD&MLBDO with all of other anonymization 
methods. In this paper, we compare our method with four 
different privacy methods from both software engineering domain  
and general field. As to more ot her privacy methods, experiment s 
might need to be done to compare our approach. 
6. CONCLUSION AND FUTURE WORK  
In this paper, we study the privacy-preserving problem on effor t 
data. When a company lacks history data to build a model for 
estimating the effort of a new project, the engineers may try t o 
request relevant data from other companies. However, considerin g 
the risk of sensitive attribute values disclosure, most compani es 
would refuse these requests. In order to solve this problem, we  
propose the ICSD&MLBDO approach. ICSD&MLBDO uses the designed ICSD strategy to divide the original data into several  
subclasses, whose labels are orde red, and then it utilizes the 
designed MLBDO algorit hm to obfuscate the target sample. 
Experimental results on seve n benchmark effort datasets 
demonstrate that our approach can make the effort data obtain 
favorable privacy and keep its utility simultaneously. For the 
future work, we would like to utilize more effort datasets to 
validate the effectiveness of our approach. 
7. ACKNOWLEDGEMENTS 
The authors want to thank the anonymous reviewers for their 
constructive comments and sugge stions. The work described in 
this paper was supported by the National Nature Science 
Foundation of China under Projects No. 61272273, No. 61572375, 
No. 61233011, No. 91418202, No. 61472178, the Chinese 973 
Program under Proj ect No. 2014CB340702. 
848. REFERENCES 
[1] K. LeFevre, D. J. DeWitt, R. Ramakrishnan. Mondrian 
multidimensional k-anonymity. In IEEE International 
Conference on Data Engineering (ICDE) , pages 25-25, 2006. 
[2] K .  W a n g ,  P .  S .  Y u ,  S .  Chakraborty. Bottom-up 
generalization: A data mining solution to privacy protection. 
In IEEE International Confer ence on Data Mining (ICDM) , 
pages 249-256, 2004. 
[3] L. Sweeney. Achieving k-anonymity privacy protection 
using generalization and suppression. International Journal 
of Uncertainty, Fuzziness and Knowledge-Based Systems , 
10(05): 571-588,2002. 
[4] L .  S w e e n e y.  K - a n o n ym i t y:  A  m odel for protecting privacy. 
International Journal of Uncertainty, Fuzziness and 
Knowledge-Based Systems , 10(05): 557-570, 2002. 
[5] B. Fung, K. Wang, P. S. Yu. T op-down specialization for 
information and privacy preservation. In IEEE International 
Conference on Data Engineering (ICDE) , pages 205-216, 
2005. 
[6] R. Chen, B. C. M. Fung, N. Mohammed, et al. Privacy-
preserving trajectory data publishing by local suppression. 
Information Sciences , 231: 83-97,2013. 
[7] A. Machanavajjhala, D. Kifer, J. Gehrke, et al. l-diversity: 
Privacy beyond k-anonymity. ACM Transactions on 
Knowledge Discovery from Data , 1(1): 1-52,2007. 
[8] N .  L i ,  T .  L i ,  S .  V e n k a t a s u b r a m a n i a n .  T - c l o s e n e s s :  P r i v a c y  
beyond k-anonymity and l-diversity. In IEEE International 
Conference on Data Engineering (ICDE) , pages 106-115, 
2007. 
[9] K. Honda, A. Kawano, A. Notsu,  et al. A fuzzy variant of k-
member clustering for collaborative filtering with data 
anonymization. In IEEE International Conference on Fuzzy 
Systems (FUZZ) , pages 1-6, 2012. 
[10] J. W. Byun, A. Kamra, E. Bertino, et al. Efficient k-
anonymization using clustering techniques. Springer Berlin 
Heidelberg , 2007 
[11] H. Kasugai, A. Kawano, K. Honda, et al. A study on 
applicability of fuzzy k-mem ber clustering to privacy-
preserving pattern recognition. In IEEE International 
Conference on Fuzzy Systems (FUZZ) , pages 1-6, 2013. 
[12] J. Casas-Roma, J. Herrera-Joancomartí, V. Torra. 
Anonymizing graphs: measur ing quality for clustering. 
Knowledge and Information Systems , 44(3): 507-528, 2015. 
[13] J. Vaidya, C. Clifton. Privacy-p reserving k-means clustering 
over vertically partitioned data. In ACM International 
Conference on Knowledge Discovery and Data Mining 
(TKDDM) , pages 206-215, 2003. 
[14] G. Aggarwal, R. Panigrahy, T. Feder, et al. Achieving 
anonymity via clustering.  ACM Transactions on Algorithms , 
6(3): 49, 2010. 
[15] X .  X i a o ,  Y .  T a o .  A n a t o m y :  S i m p l e  a n d  e f f e c t i v e  p r i v a c y  
preservation.  International Conference  on Very Large Data 
Bases (VLDB) , pages 139-150, 2006. 
[16] R. C. W. Wong, J. Li, A. W. C. Fu, et al. (α, k)-anonymity: 
an enhanced k-anonymity model for privacy-preserving data 
publishing. In ACM International Conference on Knowledge 
Discovery and Data Mining (KDD) , pages 754-759, 2006. [17] V. S. Verykios, E. Bertino, I. N. Fovino, et al. State-of-the-
art in privacy-preserving data mining.  ACM Sigmod Record , 
33(1): 50-57, 2004. 
[18] M. Grechanik, C. Csallner, C. Fu, et al. Is data privacy 
always good for software testing?. In IEEE International 
Symposium on Software Reliability Engineering (ISSRE) , 
pages 368-377, 2010. 
[19] T .  L i ,  N .  L i ,  J .  Z h a n g ,  e t  a l .  S l i c i n g :  A  n e w  a p p r o a c h  f o r  
privacy-preserving data publishing. IEEE Transactions on 
Knowledge and Data Engineering , 24(3): 561-574, 2012. 
[20] B. Fung, K. Wang, R. Chen, et a l. Privacy-preserving data 
publishing: A survey of recent developments. In ACM 
Computing Surveys , 42(4): 14, 2010. 
[21] F. Peters, T. Menzies. Pri vacy and utility for defect 
Prediction: Experiments with morph.  In ACM International 
Conference on Software Engineering (ICSE) , pages 189-199, 
2012. 
[22] F. Peters, T. Menzies, L. Gong,  H. Zhang. Balancing privacy 
and utility in cross-company defect Prediction. IEEE 
Transactions on Software Engineering , 39(8): 1054-1068, 
2013. 
[23] F. Peters, T. Menzies, L. Layman. LACE2: better privacy-
preserving data sharing for cro ss project defect Prediction. In 
ACM International Conferen ce on Software Engineering 
(ICSE) , pages 801-811, 2015. 
[24] J. Clause, A. Orso. Camouflage : automated anonymization of 
field data. In ACM International Conference on Software 
Engineering (ICSE) , pages 21-30, 2011. 
[25] K. Taneja, M. Grechanik, R. Ghani, et al. Testing software in 
age of data privacy: a balancing act. In ACM SIGSOFT 
Symposium and European Conference on Foundations of 
Software Engineering (ESEC/FSE) , pages 201-211, 2011. 
[26] D. Lo, L. Jiang, A. Budi. eKb-anonymity: test data 
anonymization for evolving programs. In IEEE/ACM 
International Conference on Automated Software 
Engineering (ASE) , pages 262-265, 2012. 
[27] A. Budi, D. Lo, L. Jiang. Kb-anonymity: a model for 
anonymized behavior-preserving test and debugging data. 
ACM SIGPLAN Notices , 46(6):447-457, 2011. 
[28] J. Brickell, V. Shmatikov. The cost of privacy: destruction of 
data-mining utility in anonymized data publishing. In ACM 
International Conference on Knowledge Discovery and Data 
Mining (ICKDDM) , pages 70-78, 2008. 
[29] S. J. Pan, Q. Yang. A survey on transfer learning. IEEE 
Transactions on Knowledge and Data Engineering , 22(10): 
1345-1359, 2010. 
[30] G. Hamerly, C. Elkan. Learning the K inK-means. Technical 
Report CS2002-0716, University of California San Diego, 
2002. 
[31] I. Jolliffe. Principal component analysis. John Wiley & Sons , 
2002. 
[32] B .  K i t c h e n h a m ,  S .  L .  P f l e e g e r ,  B .  M c C o l l ,  S .  E a g a n .  A n  
empirical study of maintenan ce and developm ent estimation 
accuracy, Journal of Systems and Software , 64(1):57-77, 
2002. 
85[33] A .  P .  D e m p s t e r ,  N .  M .  L a i r d ,  D .  B .  R u b i n .  M a x i m u m  
likelihood from incomplete data via the EM algorithm. 
Journal of the Royal Statistical Society , 1(39):1-38, 1977. 
[34] E. Kocaguneli, T. Menzies, J . W. Keung. On the value of 
ensemble effort estimation. IEEE Transactions on Software 
Engineering , 38(6):1403-1416, 2012. 
[35] G. Boetticher, T. Me nzies, T. Ostrand. PROMISE Repository 
of empirical softwar e engineering data. West Virginia 
University, Department of Computer Science , 2007. 
[36] C. F. Kemerer. An empirical validation of software cost 
estimation models. Communications of the ACM , 30(5):416-
429,1987. 
[37] J. E. Matson, B. E. Barrett, J. M. Mellichamp. Software 
development cost estimati on using function points. IEEE 
Transactions on Software Engineering , 20(4): 275-287, 1994. 
[38] C. Dwork. Differential priv acy: A survey of results.  Springer 
Berlin Heidelberg , 2008. 
[39] J. Li, G. Ruhe. Decision support analysis for software effort 
estimation by analogy. In IEEE International Workshop on 
Predictor Models in Softw are Engineering (PROMISE) , 
pages 6-6, 2007. 
[40] D. Rebollo-Monedero, J. Forne , J. Domingo-Ferrer. From t-
closeness-like privacy to postr andomization vi a information 
theory. IEEE Transactions on Knowledge and Data 
Engineering , 22(11): 1623-1636, 2010. 
[41] J. Li, Y. Tao, X. Xiao. Preservation of proximity privacy in 
publishing numerical sensitive data. In ACM International 
Conference on Management of Data (ICMD) , pages 473-486, 
2008. 
[42] S. L. Parker, T. Tong, S. Bolden, et al. Cancer statistics, 199 6. 
CA: A cancer journal for clinicians , 46(1): 5-27, 1996. 
[43] J .  B .  T e n e n b a u m ,  V .  D e .  S i l v a ,  J .  C .  L a n g f o r d .  A  g l o b a l  
geometric framework for nonlinea r dimensionality reduction. 
Science , 290(5500): 2319-2323, 2000. 
[44] M. Belkin, P. Niyogi. Laplac ian eigenmaps and spectral 
techniques for embedding and clustering, Advance in Neural 
Information Processing System . 14: 585-591, 2001. 
[45] J. Cheng, H. Liu, F. Wang, et al. Silhouette Analysis for 
Human Action Recognition Base d on Supervised Temporal 
t-SNE and Incremental Learning. IEEE Transactions on 
Image Processing , 24(10): 3203-3217, 2015. 
[46] Y. Tang, R. Rose. A study of using locality preserving 
projections for feature extraction in speech recognition. In 
IEEE International Conference on Acoustics, Speech and 
Signal Processing (ICASSP) , pages 1569-1572, 2008. 
[47] J .  G u i ,  Z .  S u n ,  W .  J i a ,  R .  H u ,  Y .  L e i ,  S .  J i .  D i s c r i m i n a n t  
sparse neighborhood preserving embedding for face 
recognition. Pattern Recognition , 45(8):2884-2893, 2012 . 
[48] X. Niyogi. Locality pres erving projections. MIT Press , 2004. 
[49] M. Dubinko, R. Kumar, J. Magnan i, J. Novak, P. Raghavan, 
A. Tomkins. Visualiz ing tags over time. In ACM 
International Conference on World Wide Web , pages193-202, 
2006. 
[50] H. He, E. A. Garcia. Learning from imbalanced data. IEEE 
Transactions on Knowledge and  Data Engineering, 21(9): 
1263-1284, 2009. [51] E. Kocaguneli, T. Menzies, A. B. Bener, J. W. Keung. 
Exploiting the essen tial assumptions of analogy-based effort 
estimation. IEEE Transactions on Software Engineering , 
38(2):425-438, 2012. 
[52] K. Dejaeger, W. Verbeke, D. Martens, B. Baesens. Data 
mining techniques for softw are effort estimation: a 
comparative study. IEEE Transactions on Software 
Engineering , 38(2):375-397, 2012. 
[53] K. Liu, L. Xu, J. Zhao. Co-Extracting Opinion Targets and 
Opinion Words from Online Reviews Based on the Word 
Alignment Model. IEEE Transactions on Knowledge and 
Data Engineering , 27(3):636-650, 2015. 
[54] T. Menzies, D. Port, Z. Chen, J. Hihn, S. Sstukes. Validation 
Methods for Calibrating Software Effort Models, In ACM 
International Conference on Software Engineering (ICSE) , 
pages 587-595, 2005.  
[55] X. Jing, F. Qi, F. Wu, B. Xu. Missing data imputation based 
on low-rank recovery and sem i-supervised regression for 
software effort estimation.  In ACM International Conference 
on Software Engineering (ICSE) , pages 607-618, 2016. 
[56] J. Keung, E. Kocaguneli, T. Me nzies. Finding conclusion 
stability for selecting the best  effort predictor in software 
effort estimation. Automated Software Engineering , 
20(4):543-567, 2013. 
[57] P. E. Danielsson. Eucli dean distance mapping. Computer 
Graphics and image processing , 14(3): 227-248, 1980. 
[58] D. R. Wilson, T. R. Martinez. Improved heterogeneous 
distance functions. Journal of Artificial Intelligence 
Research , (6): 1-34, 1997. 
[59] F. Zhang, Q. Zheng, Y. Zou, Ahmed E. Hassan. Cross-
project defect prediction using a connectivity-based 
unsupervised classifier. In ACM International Conference on 
Software Engineering (ICSE) , pages 309-320, 2016. 
[60] T. Menzies, A. Butcher, A. Marcus, T. Zimmermann, D. Cok. 
Local vs. global models for e ffort estimation and defect 
Prediction. In IEEE/ACM International Conference on 
Automated Software Engineering (ASE) , pages 343-351, 
2011. 
[61] B. W. Boehm, R. Madachy, B. Steece. Software cost 
estimation with Cocomo II with Cdrom. Prentice Hall , 2000. 
[62] B. Twala, M. Cartwright. Ense mble missing data techniques 
for software effort Prediction. Intelligent Data Analysis , 
14(3):299-331, 2010. 
[63] T. Menzies, A. Butcher, D. Cok, A. Marcus, L. Layman, F. 
Shull, B. Turhan, T. Zimme rmann. Local versus global 
lessons for defect predicti on and effort estimation.,  IEEE 
Transactions on Software Engineering , 39(6): 822-834, 2013. 
[64] A. J. Albrecht, J. E GaffneyJr . Software function, source 
lines of code, and development effort Prediction: a software 
science validation. IEEE Transactions on Software 
Engineering , SE-9(6): 639-648, 1983. 
[65] A. Heiat. Comparison of artificial neural network and 
regression models for estimati ng software development effort. 
Information and Software Technology , 44(15):911-922, 2002. 
[66]  F. Sarro, A. Petrozziello, M. Harman. Multi-objective 
software effort estimation. In IEEE International Conference 
on Software Engineering (ICSE) , pages 619-630, 2016. 
86