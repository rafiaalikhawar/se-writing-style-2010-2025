CodeExchange 
Supporting Reformulation of Internet-Scale Code Queries in Context 
 
Lee Martie, Thomas D. LaToza, and André van der Hoek 
University of California, Irvine 
Department of Informatics 
Irvine, CA  92697-3440  U.S.A. 
{lmartie,tlatoza,andre}@ics.uci.edu
 
 
Abstract — Programming today regularly involves searching 
for source code online, whether th rough a general se arch engine 
such as Google or a specialized code search engine such as 
SearchCode, Ohloh, or GitHub. Searching typically is an itera-
tive process, with developers adjusting the keywords they use 
based on the results of the previous query. However, searching in 
this manner is not ideal, b ecause just using keywords places lim-
its on what developers can express as well as the overall interac-tion that is required. Based on the observation that the results 
from one query create a context  in which a next is formulated, we 
present CodeExchange, a new code search engine that we devel-
oped to explicitly leverage this co ntext to support fluid, expres-
sive reformulation of queries. We motivate the need for 
CodeExchange, highlight its key design decisions and overall architecture, and evaluate its use  in both a field deployment and 
a laboratory study.  
Keywords—Code search; query reformulation; context; inter-
face; internet-scale 
I. INTRODUCTION  
Today, with the proliferation of code on the web [4,5], 
searching for code has become an integral  part of the job [30]. 
Indeed, it can be argued that the modern programmer has to be 
as versed in finding and interp reting relevant code on the web 
as they are in writing it. 
Programmers search for a variety of reasons. Some look for 
sample implementations of algorithms or data structures, others 
are curious about a particular API and how they can best use it, and yet others may search for specialized, domain-specific code that performs certain desired functionality [30,39,44] While question and answer sites such as StackOverflow [15] can be helpful in many cases, oft en developers want more than 
just a brief answer with some illustrat ive code – they want to 
examine and perhaps copy and paste real working code [42]. 
Typically, developers use a general-purpose search engine 
like Google or a specialized code search engine such as Ohloh [11], GitHub [2], or SearchCode [12]. The advantage of using a specialized code search engine is that all of the results are code as opposed to manuals, tutori als, code, and other kinds of 
results intermixed. Regardless, th e way in which to search is 
the same: the developer in puts keywords into the search 
engine, receives and examines results, and issues an adjusted 
set of keywords if they do not find what they are looking for, repeating these steps until they find what they are looking for, 
start over, or just give up.  
Previous work has characterized this process as one of 
query reformulation: an initi al query is issued, after which 
subsequent queries adjust the set of keywords of the previous 
query in order to attempt to ‘steer’ the search engine into 
delivering the desired results [26,37, 46,51,60]. It is important 
to realize that such reformula tion is not random; rather, it is 
based on examining the search results, deciding what is desired 
and what is not desired in those results, and mapping this back to a modified set of keywords that is then issued to the search engine. 
It is here that two problem s arise. First, developers can only 
express a limited amount of information in keywords. It is, for instance, not possible to express that they want some code that is shorter (and presumably simp ler) than the results shown. As 
another example, if they see a result that provides them with the desired functionality, but no t in the righ t way (e.g., they 
want a method with Java Generics instead of a fully specified 
method), just adding Generics or Java Generics  to the current 
query does not necessarily improve the search results, and may well lead to very different results. 
The second problem lies in the overall user interaction that 
is required: each time a set of keywords is reissued, the previous query is lost (except for using the back button), and it is difficult to combine keyw ords from different queries, since 
the previous queries are no longer visible. The result is that developers rarely go back or recombi ne parts of old queries, 
instead continuing a linear se quence of modifying keywords. 
This is less than ideal. 
The underlying issue is that current code search engines are 
designed to support query fo rmulation, and fail to recognize 
that query reformulation is perhaps much more important to achieve successful search experiences. We are not alone in making this observation (e.g., [50,53]) and some preliminary 
work exists that seeks to support reformulation (e.g., [34,53]). 
The work to date, however, h as limited itself to making 
enhancements that remain clos ely tied to the traditional search 
paradigm. A variety of extensions are proposed, but the focus remains on issuing  sets of keywords as queries. To better 
support reformulation in code search, we pose that it is necessary to rethink the search engine, from the information 
2015 30th IEEE/ACM International Conference on Automated Software Engineering
978-1-5090-0025-8/15 $31.00 © 2015 IEEE
DOI 10.1109/ASE.2015.5124
that is indexed, to the kinds of  queries that can be issued, to the 
way those queries are issued. 
The key insight underlying our  work is that the results 
being returned by a code search engi ne provide a rich context 
that can be leveraged in many different ways to enable 
reformulation of the current query. In this paper, we experiment with three such ways: (1) making keyword 
recommendations that are based on the words that appear most 
frequently across the variable names in the results that the 
search returns, (2) providing criti ques that allow developers to 
dial up or dial down propertie s (e.g., length, complexity) of the 
returned results, and (3) enabling developers to select language 
concepts in a search result and add them to the query to, for 
instance, specify that the search results should contain a particular method call or import a particular interface (which is different from adding that method call or interface as a keyword). 
Simply providing all these features, however, is 
insufficient, as it still leaves unsol ved the interaction problem. 
We therefore designed a code search user interface that breaks down queries into query parts . Each reformulation of the 
current query is then performed either by adding a new query part through one of the three mechanisms above , by typing an 
additional keyword or set of keywo rds to add as a new query 
part, or by deactivating an existing query part (it will still 
remain visible, so it can easily be reactivated). In this way, the 
current query is a composition that  can be fluidly manipulated 
in many different ways.  
We implemented these ideas in a prototype, 
CodeExchange, and evaluated its use in a field deployment as 
well as a laboratory experiment. Combined, this gave us a broad look at how people search with CodeExchange ‘in the wild’ as well as a detailed look at how searching with CodeExchange compares to searching with GitHub. In the field deployment, 78% of the successful  searches (those involving 
copying or downloading of c ode) involved use of the advanced 
features of CodeExchange. In the laboratory experiment, we 
drew on successful searches from our fi eld deployment as input 
and compared task performance of six participants in issuing 
queries on CodeExchange or GitHub. We found that 
participants with CodeExchange on average were about 2 1/2 
minutes faster in finding results. While this overall is strongly 
indicative of the benefits of our approach, the evaluation also 
identified several opportunities fo r improving CodeExchange. 
The remainder of this paper is organized as follows.  Sec-
tion 2 presents relevant backgro und material in code search. 
Section 3 motivates our work with three problems developers commonly encounter when they search. Section 4 introduces our high-level approach and Section 5 details CodeExchange. 
In Section 6, we present the system architecture of CodeExchange. In Section 7, we present the results of our evaluation. Section 8 discusses threats to validity. Finally, we 
conclude in Section 9 with an  outlook at future work. 
II. B
ACKGROUND  
This section introduces relevant  background material in 
how developers search, query form ulation, manual query re-
formulation, and automat ic query reformulation. A. How developers search 
That programmers search for code is not a new phenome-
non. Even decades ago, they wo uld search for code locally 
with tools such as grep [32]. Sometimes, they  just want to find 
some code to look at, but other times they search for code to 
actually copy, paste, and perhaps adapt somewhere else. With the proliferation of source code available on the Internet, pro-grammers today can oft be found searching online as part of their daily routine [30].  
Among other things, it is known that they look for source 
code that illustrates how to use some API, helps them write their own (often domain-specific) code, enables them to recall how to use and integrate with a particular piece of code, helps them gain a deeper understanding of some programming lan-
guage feature, and allows them to compare their approach to 
others [30,39,44].  
While, early on, general search engines such as Google 
were the only available tool to search for code online, today a variety of specialized code search engines such as Ohloh [11], GitHub [2], Sourcegraph [14], and SearchCode [12] are availa-
ble. The research community, too, has produced a number of code search engines (see Section II.B).  
Of importance to the design of these code search engines is 
the question of how developers search. Surprisingly, not much 
is known yet, with few studies having taken place. The existing studies, though, do shed light on some important behaviors that we build on in this paper.  
First and foremost, search is an iterative process [53,60]. 
This is not a surprise, and well-know n in information retrieval 
[51]. It is simply too difficult to, on a first try, formulate a que-ry that is sufficiently precise to yield the desired result. Devel-opers, thus, need to reissue modi fied queries. Typically, this 
means that they refine the query with additional keywords to 
attempt to further constrain th e results that are returned [46]. At 
the same time, it has been found that developers sometimes also need to move in the opposite direction, having used a que-ry that was too precise and thus needing to adjust the query to be more general [51]. 
The second behavior pertains to the keywords developers 
use to construct their queries.  When they do not exactly know 
what they are looking for [33], or are more broadly looking to learn about a technology or language [30], they tend to use 
relatively generic keywords in th eir queries. When they refine 
the queries, though, the keywor ds tend to become less generic 
and contain more language-specific terms [54]. 
Work has begun to try to theoretically cast these search be-
haviors. Information Foraging Theory  [50] is perhaps the most 
explored theory in this regard, dis tinguishing prey (a goal to be 
reached), predator (developer), and approximations of the prey (queries). Its authors observe that “accuracy of goal approxi-mation suffers when developers have dif/g191culty coming up with effective query terms—a common problem.” 
B. Support for query formulation 
A traditional search engine takes as its input keywords that 
it attempts to match. Using keywords as such is still the pre-
25vailing mechanism in the code search engines that are used in 
practice today.  
It has been recognized, however, that alternative mecha-
nisms exist and may be desired. It is, for instance, possible to 
perform a search in S6 [55], C ode Conjurer [43], or Code Ge-
nie [41] by specifying test cases, with the results that are re-turned code that passes the test  cases. Recognizing that perfect 
matches between the test cases and the code in the search data-
base do not always exist, some  approaches first attempt to 
transform (e.g., rename methods, reor der parameters) the inter-
faces to expand the results that may be returned [43,55]. Simi-
lar to test cases, others use input -output specifications to form 
the query [35]. Yet other appr oaches use the code that the pro-
grammer is currently writing to create the query. Strathcona [47] and CodeBroker [59], for instance, take keywords from 
the code being written to automatically issue queries and 
GraPACC [16] and the approach of Bruch et al. [36] use pat-
tern matching to search for code that auto-completes what the developer is working on. Similarly, in local code search [17], an approach for utilizing the workspace of the developer was shown to help developers  find artifacts. 
Other approaches, like SNIFF [4 9] and Assieme [45], use 
the API documentation of the code in their datab ase to enrich 
their search indexes so there is a better chance for more generic (non-programmatic) queries to pro vide useful results. Explain-
er [26] and CodeTrail [38] ev en augment how the results are 
presented with relevant document ation to support code com-
prehension. 
A final class of code sear ch engines supports queries 
through more programming language oriented input mecha-nisms. As an example, Sourcerer [23] allows developers to 
search for method signatures or specific interface relationships. A number of other code search t ools (e.g, JSearch [48], Pro-
spector [21], ParseWeb [56]) offer various other such search 
capabilities. 
Common to all of these approaches is a heavy focus on ini-
tial specification of the ‘right’  query. While this is certainly 
helpful, from the perspective of reformulation, they perhaps 
worsen the problem, since now the presence of undesirable results must be translated to up dates to complex input struc-
tures. 
C. Manual query reformulation 
Reformulation, as a separate step from formulation of the 
initial query, has received much less attention. In most code 
search engines, it is only supported through inputting new keywords. A few search engines (e.g., [2,11] support the use of 
filters to narrow down results, for in stance by project or author, 
but do not go beyond. 
A few projects do explicitly seek to support reformulation. 
CodeFinder [53] uses spreading activation networks to match words appearing in th e results to words th at it draws from 1800 
example source code files; a match leads to the matching word being recommended to the devel oper. Mica [34] also generates 
keyword recommendations, although it does so somewhat dif-
ferently. It matches keywords in th e results to method and in-
terface names of the Java SDK libraries. A match again results in the matching word to be suggested. Both of the approaches are useful, though limited in general applicability. 
An interesting approach is presented in [58]. Though aimed 
at local code search only, the authors report success in match-ing prefixes and suffixes of id entifiers (drawn from its reposi-
tory of code) to keywords in failed searches, with the longest matching prefix or suffix rec ommended for reformulating the 
query. 
D. Automatic query reformulation 
In contrast to the suggestive approaches above, which still 
require the human to actually choose  to a dapt the suggestion 
(or parts of it), automatic que ry reformulation modifies a query 
automatically in an attempt to provide better results.  
BluePrint [29], B.A.R.T. [24], and the work by Lemos et al. 
[40] are examples of automatic query reformulation.  All pre-
process a query before it is issued to the code search engine. BluePrint augments the query with words from the code cur-
rently being written, in an attempt  to find code that fits better 
with the ongoing effort. Th e other two approaches actually 
issue not just a single query, but multiple, each query replacing some keywords with semantically related keywords (e.g. ‘box’ 
with ‘rectangle’) in order to broaden the candidate set of search results. 
Areas outside Internet-scale code search also use automatic 
query reformulation.  Sonia Hai duc et al. [52], apply it for find-
ing methods to change in a project. The authors experimented with many techniques, and  even built a ma chine learning tech-
nique to choose the best depending on the kind of query used. Roccio [18], an information retri eval technique that has not yet 
been applied to software engineering, lets users mark docu-ments as relevant or not  and then itself constr ucts a new query.   
 While these approaches have show n promise, they at the 
same time might make things worse. That is, if the automati-cally reformulated query does not deliver the right result, then 
how is the user to adjust the query? 
III. M
OTIVATING EXAMPLES  
This section illustrates the kinds of problems one 
encounters when query reformulation is limited to reissuing 
keywords only. We show t wo examples, both executed on 
GitHub’s code search engine with the filters code and Java 
enabled.  
A. Finding traveling salesman code Our first example is taken from one of the searches that was 
performed during our field de ployment. Someone was looking 
for code implementing the trave ling salesman problem. In 
effect, they needed an algorithm implementation, something 
that is very common in code search [39,54]. While the person succeeded in the search by using CodeExchange’s features, re-creating the search with GitHub reveals problems. 
A reasonable starting query is, of course, traveling 
salesman .  The top 10 results from  this query, however, all 
have zero logic in them as they are abstract classes, test cases, and empty classes. A second query could be traveling 
26salesman implementation  to indicate we prefer actual code. 
This does not lead to the d esired result either, as the first result 
is a Java file containing a package statement and nothing else, 
and the other nine results are copies of the same Junit test file 
in different GitHub repositories. It  is unclear what to do next, 
though the GitHub advanced search is a possibility: it has the option to specify a file size. This leads to the query traveling 
salesman implementation size:">100" . This query also does 
not lead to any code that actually implements the algorithm. Rather, the top 10 results are test cases and other code containing library calls. 
While a brief, non-exhaustive example, it is representative 
of many of the searches we saw in our evaluation, when developers were restricted to usi ng GitHub’s search engine, 
and had trouble formulating the ‘right’ queries to obtain the results they needed. Without kn owledge of (even just one of) 
the traveling salesman implementations on GitHub, it is difficult to guess the keywords that might lead to them. 
B. Finding the Observer pattern Our second example mimics what happens when one has to 
look for how to use a particular API. In this case, we search for 
how to use the Java Observer interface.  
A reasonable starting point is to issue the query implements 
Observer , as we are looking for an example of how the 
interface is used. The top 10 results do not contain classes that implement the Java Observer  class, but instead contain 
implementations of other Obse rver classes. Clearly the query is 
too generic; to avoid this proble m, a refined query might tag on 
Java to the previous query: implements Observer Java . Some 
of the results returned do implement the Java Observer 
interface, but they are abstract classes and do not show any method invocation of the Observer  interface. Refining the 
query further, for instance as implements Observer Java import 
java.util.Observer , still leads to the t op 4 results being abstract 
classes with no implementations an d other  results that do not 
implement the Java Observer  interface at all, or, if they do, that 
do not show actual method invocations but simply comments 
that say “// Write your code here” . Using quotes to group the 
import statement ( implements Observer Java “import 
java.util.Observer” ) finally leads to results that implement the 
Java Observer  interface. Still, the results are not helpful as they 
are abstract classes, empty implementations of Observable  
methods, or statements such as System.out.println("Changed") . 
None of the top 10 results show real examples of how to use 
the interface.  
Again, the example is brief and non-exhaustive. Certainly it 
is possible to issue queries that lead  to the desired result quick-
ly. On the other hand, it is representative of the kind of mean-dering we witnessed in our la boratory experiment and shows 
how it is all-too-easy to be walking down a non-productive path. 
IV. A
PPROACH  
Our work distinguishes itself by focusing on reformulation 
from the start. We sought to design a code search environment in which reformulation is the m ain driver for the features that it 
provides. To do so, we stud ied the literature (per Section II.A), talked with local developers abou t their code search habits, and 
prototyped a number of different approaches.  
The key insight that emerged is that a significant opportuni-
ty exists when one considers the search results from one query as a context in which developers specify the next query. That 
is, it is typical for developers to think of the next query in re-sponse to the results that the previous query gives them. This response can be of two forms: 
1. It is a response that borrows fr om the results to augment the 
query.  A developer might see a new keyword, or they may 
find a programmatic construct th at they think could lead 
them to the desired result if it is incorporated in the new 
query. 
2. It is a response that is relativ e to some of the results.  A de-
veloper might want method im plementations that are not 
empty, but have actual code in them, or might want results that use more methods than just a few of the API that they are seeking to understand.  
One can imagine a rich variety of ways in which the con-
text of code search results can be leveraged in these ways. It may be interesting, for instance, to explore whether it helps 
developers to know the number of test cases that each result has, how many open bugs each result had fixed over the past 
months (and how many open bugs are left), or how many other 
developers already looked at a result and copied or downloaded 
it. Indeed, one can easily imagine a rich context consisting of 
many different kinds of social and technical information that is shown to a developer who is searching.  
Before we create such a fully-featured environment, 
though, it is necessary to first estab lish the feasibility of the 
overall idea and examine whether providing developers access to context is actually beneficial in aiding the search process. To do so, then, we experiment with three novel ways of leveraging 
context:  
/g120 Recommendations . Because a developer may not know the 
exact keywords to use in narrowing down a query, it is 
useful to suggest possibilities  to them. We particularly 
make keyword recommendations that are based on those 
words that appear most frequently across variable names 
in the results that are return ed. Variable names regularly 
encode domain information [19], a fact that we leverage 
here to make borrowing from the results easier. 
/g120 Critiques . To enable developers to  create responses that are 
relative to the current search  results, we use critiques 
through which developers can dial up, or down, certain 
properties desired of the results. We particularly explore three such properties: length of th e code, complexity of the 
code, and number of imports. 
/g120 Language concepts.  To enable developers to leverage the 
code results directly in n arrowing down their search, we 
allow them to select instances of language concepts shown in the results to add to the query. In this manner, develop-ers can specify that search results should contain a particu-lar method call or that they should be restricted to code 
that imports a given interface. 
27Simply providing these features, however, is insufficient. It 
enables reformulation, but the overall interaction mechanism of 
the search engine has to be designed to support it also. That is, 
while the above features make it easier for the developer to add 
important considerations to their queries, it has to be as simple to remove  parts from a current query that may not have had the 
intended effect, or to recombine  parts of previous queries.  
To do so, our queries consist of query parts . A reformula-
tion of a  current query is perform ed either by adding a new 
query part through one of th e three mechanisms above, or by 
typing additional keywords to add as a query part, or by deac-tivating an existing query part. The key is that deactivated que-ry parts do not disappear, but remain visible, and can be reac-
tivated. The current query, then, is a composition of query parts 
that can be manipulated fluidly. 
V. C
ODEEXCHANGE  
In this section, we present how we implemented these ideas 
in CodeExchange, our pro totype code search engine. Before 
we detail its support for reformulation, we briefly discuss first how a developer can use CodeEx change to form  an initial 
query. We also note that CodeExchange, currently, is implemented to deal with Java code only. 
A. Query formulation 
CodeExchange starts with its splash screen (Figure 1) 
where the programmer inputs an  initial query as a set of 
keywords that are submitted via the Submit  button or simply by 
hitting Enter . The input box provi des autocomplete functionality that uses the word s occurring in identifier names 
in the indexed code, similar to [58].  
 CodeExchange also offers an adv anced form for inputting 
initial searches that are more language specific. Pressing the 
advanced search button in Figure 1 brings up the window 
shown in Figure 2. The adva nced search form allows users to 
specify queries that involve imports of certain classes, interface properties (extends, implements), location (package or project), method calls (class, method, parameters), and method declarations. Additionally, it all ows developers to specify, 
where appropriate, whether a class or method should be 
generic or have a variable arguments. 
Once a developer has submitt ed their initial search, they are 
presented with the main search interface of CodeExchange (see Figure 3) in which they can reformulate their search. Note that 
the search interface immediately shows results (no clicking on Figure 1. CodeExchange splash screen. 
Figure 2. Advanced search. 
Figure 3. CodeExchange main interface. 
28links is required, as is typical in most code search engines), and 
that it is possible to paginate to further results (bottom right, 
A). All results are ranked using Lucene’s TF-IDF [13].   
B. Query parts 
After issuing the first query, the keyword input box remains 
at the top of CodeExchange interface (B). It, though, is emptied 
out, with the keywords that  were typed now added to the 
current query on the left of the interface (C).  The current query is listed as a set of query parts, with all parts that are selected 
composing the current query. By  default, any query part added 
is stacked at the top and each query part displays both its kind (e.g., keyword, complexity, import count, language construct) 
and its value. In the figu re, three query parts have been 
provided: (1) keywords traveling salesman , (2) complexity  
more than 0 , and (3) import count less than 6 . 
Each query part can be deactivated or reactivated simply by 
clicking it. Activated query part s are highlighted in yellow, 
with deactivated query parts show n in white. The current query 
is the logical AND  of all activated query parts. 
Note that, compared to the traditi onal back and forth button 
of a browser, composing queries through query parts is flexible: any query part can be deactivated or reactivated at any 
moment in time, as there are no restrictions on their order. Programmers can generalize the query by deactivating any part, specialize by reactivating an y inactive part, or try entirely 
new combinations by both deactivating and reactivating.  
C. Recommendations 
Below the current query are the keyword recommendations 
that CodeExchange makes once it has obtained search results 
in response to the current query  (note that CodeExchange 
provides additional recomme ndations based on language 
concepts; see Section V.E for a discussion). In this case, it lists city, distance , citycount , index , name , and nextIndex  (D). If a 
recommendation is clicked, it is added to the current query and the current query is re-issued. Once the results return, the 
recommendations also are updated to offer further assistance in scoping the next query. 
The keywords that CodeExchange recommends are the 
ones that are most frequent across variable names in the results. This relies on the following  three observations regarding code 
search and the structure of code. First, search results have co-occurring words that can be relevant to a programmer’s keyword query [28]; this is an essential assumption in the 
entire field of informati on retrieval, and applies here as well. 
Second, variable names usually have domain words in them, and consist often of concatenati ons of such words [19]. Finally, 
we observe that variable names are the most abundant source of names in the code, as they can make up even 70% of its content [25].  
Based on these observations, we implemented 
CodeExchange to extract all variable names, split them (it does this at index time), and use the resulting co-occurring relationships of “name parts within files” to make its recommendations. Words, thus, do not need to be co-occurring 
in the variable names themselv es, but be part of co-occurring 
variable names in the same file or files. 
A particular striking example of this approach is represent-
ed by the keyword query LDA (LDA is a popular means to 
generate topics describing a la rge corpus [20]).  After LDA is 
submitted to CodeExchange, the first result is empty and it is 
unclear if the second and thir d result generate topic models and 
how they set the model’s para meters. Using the automated 
recommendations made by CodeExchange, however, it is possible to select model , alpha, beta, and topic  to obtain search 
results that generate topic models using the Mallet library [10]. Similar recommendation sequences would achieve the same results.  
D. Critiques 
At the top of each of the results are the critiques (E, 
enlarged version shown in Figure 4). Each critique allows the 
developer to specify, relative to the given result, whether they 
prefer to get code that is l onger/shorter, has more/less branch 
complexity, or has more/fewer imports. 
In Figure 3, the initial query was traveling salesman , which 
led to results that had no logic in it. This was indicated by the 
red zero in the complexity critique for these results (as is 
shown in Figure 4 for one of the results). Programmers know 
that most algorithms typically exhibit some branch complexity, caused by for loops and if statements. By clicking the arrow up  
above the complexity critique, a ne w query part is added that 
specifies that the complexity of the returned results must be greater than 0. In the example of Figure 3, this led to results that involved a large number of ex ternal libraries (as evidenced 
by a large number of import statements), so the imports criticism was invoked to ask for fewer than 6 import 
statements. The result is the code shown in Figure 3, which actually represent complete implementations of the algorithm. 
E. Language concepts  
The third and final use of context is via language concepts 
in the results that are shown. Each result is presented in an 
editor with the various language concepts highlighted (e.g., 
package, imports, method names, types, method invocations). 
Rather than ha ving to encode these as keywords, a developer 
can click on a highlighted language construct to refine the current query to look for code that has instances of this construct.  
For example, in Figure 5,  the programmer hovers the 
mouse over the method compareTo , which presents them with 
a popup with additional information about this method. If they determine they want to add this method invocation as a query part, a click on compareTo  does so and re-issues the query; all 
results that are now retu rned by CodeExchange will include 
Figure 4. Critiques. 
29compareTo  method calls that take type E arguments (E is a 
generic type).   
For convenience, CodeExchange automatically 
recommends the top three most occurring instances (across all 
of the returned results) of im plements, extends, and imports. 
This not only saves the user time, but because they are based on co-occurrences with the other keywords in the current 
query, these recommendations (Fi gure 3, F) help the developer 
drill down their search quickly. 
Returning to our second motivating example, as discussed 
in Section III.B, submitting the query implements Observer  
suffers the same problem in CodeExchange as in GitHub in that none of the top results implement the Java Observer interface.  Contrary to a GitHub search, however, CodeExchange offers a developer ad vice. In this case, it 
suggests the query to be refi ned by classes that implement 
interface java.util.observer . Simply clicking this 
recommendation adds it as a query part to the current query 
and the next set of results all now implement the interface. 
F. Additional functionality 
CodeExchange has several additional features that are 
worth mentioning briefly. First, th e current query can also be 
restricted to a given result’s project. Each code result lists its 
project in its respective upper right corner (G). When clicked, a 
query part is added to the current query that specifies that all 
code results must be within this project. The motivation for 
providing this support is to allow the programmer to scope the 
results down to a project when th ey think it will have other 
relevant code.  
Second, CodeExchange offers devel opers the opportunity 
to download the search result, or its encompassing project, by using the download icons listed below the code results (H). 
Finally, programmers often reuse historical queries to form 
their future queries [60]. CodeExchange provides a query his-
tory (not shown) that contains all final queries that were issued. 
Each of the listed queries can be reissued as a starting point for a new search simply by clicking it. Once clicked, all query parts that were activated at that time are restored (assuming past queries were successful, we do not store the deactivated 
query parts as part of th e history). The history is presented as a 
grid, where each cell contains a past query. The cells are chronologically ordered. A similar approach is taken by SearchBar [22], a tool for resum ing search for web pages. VI. S
YSTEM ARCHITECTURE  
CodeExchange consists of four pri mary components: a List 
Server, a Code Miner, a Search Engine, and a Web Search Interface. Figure 6 presents its architecture as a data flow diagram of the primary informati on that flows among the four 
components.   
The List Server contains a list of repository URLs (i.e., 
different projects) on GitHub. We obtained this list by using GitHub’s API to identify the URLs of 602,244 repositories that included only Java code, a list we obtained between February 4, 2014 and February 12, 2014. The List Server simply passes a single, not-yet-mined repository URL to the Code Miner each 
time the Code Miner has completed mining a previous repository. 
Once the Code Miner has received a repository URL, it 
clones the repository from GitHub and analyzes it to create a summary of each of the source code files in that repository. To do so, it extracts a variety of in formation, including method 
and variable names, import statem ents, method invocations, 
code length, code complexity, and other items necessary to support the CodeExchange user interface. This summary is stored in a database that the Search Engine maintains. 
The Search Engine uses the Apache SOLR framework [1] 
to index the source code summaries. When it receives a query from the Web Search Interface, it uses SOLR to identify the 
source files that match the search criteria. It ranks the results using Lucene’s TF-IDF [13] and returns them to the Web Search Interface, together with the original source files that it 
obtains from GitHub (we do not store any source files locally, 
since cloning GitHub is prohibitively expensive in disk space).  
The Web Search Interface was built in Ajax using the 
JQuery library [9]. It implements the user interface discussed in the previous section, and is responsible for sending queries 
to the Search Engine and rendering the results. 
From the 602,244 repositories, we thus far have mined over 
300,000 projects, with the resu lting database consisting of 10M 
classes, 150M methods, and 253M  method calls that are 
searchable through CodeExchange at codeexchange.ics.uci.edu.  
VII. E
VALUATION  
We evaluated CodeExchange in two complementary ways. 
First, we wanted to deploy it to real programmers to understand if they would actually use it and, if so, how they would use it to reformulate queries. We posted brief announcements on several Java and programming forums [6,7,8], and collected 
Figure 5. Language concepts. Figure 6. System architecture. 
30logs of the resulting usage. Second, we wanted to directly 
compare how searching with CodeExchange differs from 
searching with GitHub keywords. To do this, we performed a lab experiment for which we recruited six participants. Each of them performed a number of searches inspired from successful field deployment searches.  
We chose GitHub as our comparison for two reasons. First, 
because CodeExchange indexed Java source code from 
GitHub, it puts the two on equ al footing with respect to the 
collection of projects against which queries are evaluated. This 
enabled us to perform the second experiment. Second, we pre-ferred to use GitHub over a ge neral purpose search engine such 
as, for instance, Google, to filter out non-code related search results. 
In the below, we present the details of each of the evalua-
tions and discuss our findings. 
A. Field deployment 
On July 31
th 2014, we posted a pre-announcement to a 
small group of developers that our research group interacts 
with on a regular basis. We made this pre-announcement to vet 
possible problems before we made the more public announce-
ments. As we did not experience any serious issues, we moved on quickly and posted  several brief announcements of 
CodeExchange’s availability on p ublic forums such as Reddit 
[8], JavaRanch [7], and Hacker News [6]. Our post on Reddit, 
for instance, read as follows: 
CodeExchange – a new Java code search engine (codeexchange.ics.uci.edu) 
We received the most activity in the week we posted on 
Hacker News (August 4
th) and Reddit (August 6th). Other fo-
rums were less effective in drawing attention.  CodeExchange logged all visito r behavior by assigning 
each first-time visitor a unique anonymous id that is stored as a 
cookie in their browser. This is especially important to track 
their return behavior. Every time a CodeExchange feature was 
used, an entry was made in the logs, detailing which feature 
was invoked, any input the user provided, and the date and 
time of feature use. 
From July 31st to August 19th 2014, we observed more than 
4,000 users of CodeExchange. Approximately 2,000 of the users were robots, which we di scarded immediately. About 
1,000 users just typed in keywords on the splash screen, and subsequently never did anythi ng else. We discarded these from 
our analysis as well, leaving roughly 1,000 users who collec-tively performed 1,242 searches. 
Figure 7 plots all of these users with all of their searches on 
a graph where the y-axis is the visitor identifier and the x-axis represents the date and time of the search. As the visitors came from many different time zones, we treated all their local search times as PST time. (This is why there is a search on August 20
th, even though we stopped co llecting data on August 
19th). Black dots denote users who searched once. Blue squares 
are searches by a visitor who, after at least one hour has elapsed, returned for another search (blue lines connect back to the first search by the visitor). Red circles around a blue square or a black dot indicate that a copy or a download occurred as 
part of that search. Visitor 2747, for example, searched on Au-
gust 6
th, 11th, and 12th, with the latter two searches involving 
copies or downloads of actual code. Overall, 242 visitors re-turned for additional searches over th e fourteen days, and 895 
searches used the advanced features of CodeExchange. 
For purposes of studying how ref ormulation helps with the 
search process, we focus on just those searches that involved a copy or download, since we can reasonably assume that a search involving copy or download meant that the user actually 
found what they are looking fo r (this is conservative and opti-
mistic at the same time, see Section VIII). Out of the 1,242 searches, this left 51, covering a broad range of searches (e.g., travel agency code, Twitter API usage, convex hull algo-rithms).  
Figure 8 presents an encoding of some of these 51 searches, 
with each sequence of columns representing a set of queries that constitute a search ( K for keywords, R for recommenda-
tions, Q for critiques, C for language concepts, P for project 
filter, empty  for times when all query parts are deactivated). 
We can see how users reformulated their queries in a variety of ways, using all of the Co deExchange features in the process. 
Use of keywords still dominated, as did the use of project re-Figure 7. Searches from our field deployment.  
Figure 8. Sample query sequences. 
31finements, which is not a s urprise, since it is how developers 
are used to search. Of the CodeExchange features, language 
concepts were used in 35 searches (86 invocations in total), recommendations in 14 (33), and critiques in 3 (10). While in 
the absolute these numbers are small (especiall y for critiques), 
78% of the successful searches involved use of the new fea-tures. We further note that among the successful searches, 31 (61%) involved users turning off one or more query parts as 
part of their explorations. 
On the whole, the field deploy ment did not lead to as much 
usage as we had hoped. While users showed a lot of interest 
and explored the features, only 51 searches qualified as ‘suc-cessful’ in that we could re asonably assume the developers 
found what they needed. In some ways this is not surprising, given that we posted merely bri ef announcements, performed 
no training, and many users likel y just visited to satisfy their 
curiosity. On the other hand, we had hoped for more. 
Nonetheless, we are encouraged by the results. Some users 
returned days or weeks later to search again, with a number of those searches leading to copies or do wnloads. Additionally, 
the fact that, in the successful searches, the advanced features were used extensively provides in itial evidence that there is 
value in the overall approach. 
B. Laboratory experiment: comparative use 
To delve into th e differences between GitHub search and 
CodeExchange search, we performed a laboratory study. It 
involved six Java programmers. Two were working as devel-oper interns, one was working a full time development job, and three were graduate students with 2 or more years working as a 
developer.  
We constructed six independent search tasks, each pat-
terned after a successful search taken from the field deploy-
ment. We read the code that was copied or downloaded, looked 
up documentation to further dete rmine what the code does, and 
developed a general task descripti on. Figure 9 presents one of 
the resulting tasks, with others  concerning code related to Ora-
cle thin jdbc drivers, Minecraft  YAML files, Android wake 
locks, calculating convex hulls, and Raspberry Pi GPIO. 
Before the experiment began, CodeExchange and GitHub’s 
search engine were explained. For CodeExchange, we walked through all of the featur es that are discussed in Section 5.  For 
GitHub, we showed them how to use keyword search, ad-vanced search, sorting functions, how to navigate to a project 
page of a code result, as well as how to filter results by code 
and the Java language.  
Each participant worked on the six tasks in  the same order, 
switching search engines after each task. Half starting with 
CodeExchange and half with Git Hub. Participants were given a 
description of the task and up to 20 minutes to complete each task. Each task asked participants to find th e code that they felt 
best matched the instructions. After completing the task, partic-
ipants answered several survey questions. At the end, the ex-perimenter conducted a semi-struct ured interview focusing on 
their relative perceptions and their use of features during the study. We recorded the screen and logged participants’ actions. From this, we calculated task time from the moment they be-gan interacting with the search engi ne by typing  search terms 
until they pasted the found code into the task completion sur-vey. 
Table 1 presents our main results, listing times for each 
task. Grey cells indicate GitHub times and white cells CodeExchange times. On averag e, participants completed tasks 
3 1/2 minutes faster with CodeExhan ge than with GitHub (7.23 
versus 10.44 minutes). However, there were several outliers in 
the data. For example, in two cases using GitHub, participants 
were still looking for code at the end of 20 minutes. To remove these and other outliers, we perfo rmed symmetrical truncation 
[57], an accepted technique for trimming outliers, and removed the two slowest and two fastest times from each search engine (double underlined in Table  2).  After removing outliers, we 
found participants were about 2 1/2 minutes faster with CodeExchange.  Performing a one tailed t-test (CodeExchange and GitHub times are separate groups and have equal variance) on the truncated results, we found  that this difference is signifi-
cant (p = 0.0268).  Further, with a power level of .99, we found 
the effect size to be large (d = 1.67).  This means that CodeExchange had a large effect on the user’s task times (Co-hen’s d greater than .8 is considered large [31]). 
In the 18 tasks with CodeExchange, its advanced features 
often helped participan ts in identifying the code they copied. In 
6 of the tasks, they did not need them; in the other 12, they used 2 recommendations, 12 language constructs, 6 critiques, 
as well as 13 deactivations and 6 reactivations – for 2/3rds of 
all tasks being helped with CodeExchange. One participant 
reported that complexity cri tiques helped when he found code 
that was too “dumb” and did not kn ow how else to continue, 
enabling him to increase the desi red complexity and find his 
desired search result. Another participant said she never used CodeExchange’s advanced search because searching through language concepts and automated keyword recommendations let her do similar searches. Several reported that the layout of the results made them easier to read. 
When searching, participants of ten wished to reformulate 
their queries by backing up a ‘level’, removing a portion of their query. To back up in GitHub, participants were forced to start from scratch, needing to remem ber the keywords they had 
used before. Participants reported that CodeExchange made it easier to back up, as they could simply activate and deactivate query parts. Others reported that query parts helped them try 
different combinations of queries.  
All of the participants had positive reactions to 
CodeExchange, such as "CodeExchange is better than GitHub, no doubt.” But participants al so had several suggestions for 
Background: 
JBoss is an open source Java application server maintained by 
Red Hat. There is a JBoss API for creating different kinds of references to Java objects.  References point to objects in Java 
and based on their “strength” the Java garbage collector treats 
them differently. The types of strengths are: Phantom, Weak, Soft, and Strong. 
Task: 
Find code to create a Phantom reference using the JBoss API. 
Figure 9. Sam ple search task for ex periment. 
32improvement. Participants wished to be able to rapidly scroll 
through results and to click on a method call in a search result 
to go to its definition. Participan ts also wished to search for 
results that do not cont ain keywords or imports and to see addi-
tional trustability metrics, su ch as author reputation. 
VIII. THREATS TO VALIDITY  
Several possible threats to validit y exist in how we con-
ducted our evaluation. First, for the fiel d deployment, using 
copies and downloads as an  indicator of whether a search was 
successful is an approximation. It may be lower than the real number, as it is possible that a developer had a successful search, looked at the results, but did not copy or download the code. It also might be higher th an the real number, since some 
developers might have been experimenting with the features 
and simply downloaded a random result. We note, however, 
that selection and downloads as an indicator of successful searches has been used in previous work [54], and that copies and downloads is more conservative. We also note that, for most of the searches involving copies and downloads, reformu-lation took place. Overall, then, we believe our reported results are more of a lower bound. 
Second, the comparative study stopped short of taking a 
look at the code results that partic ipants selected; that is, we did 
not examine the quality of the selected code in terms of how well it matched the search query. It mi ght be that all of the re-
sults from GitHub searches are qualitativ ely better than those 
found using CodeExcha nge. Selection criteri a, though, are in-
herently tied to the human performing the search. Since we counterbalanced the experiment (each participant used GitHub 
and CodeExchange), the same person got  to apply their criteria 
across both conditions, which  we believe this threat mitigates 
to a degree. Further, the fact that, twice, participants could not 
find the code they needed with GitHub is evidence that they put their best foot forward. 
Third, the comparative experiment used graduate students 
as some of the participants. Wh ile they had industrial experi-
ence, it is possible that their exhibited behaviors are not repre-sentative of the broader developer population and how they search. Because our overall evaluation also includes a field deployment, and we saw similar behaviors in how the profes-
sionals searched and the students searched, we believe this risk is significantly reduced. IX. C
ONCLUSION  
Searching for code remains difficu lt today, in no small part 
because manual query reformulation is still poorly supported. Just using keywords places limits on what developers can express as well as the overall interaction that is required.  
This paper has introduced a new approach to code search in 
which results from one query create a context in which a next is formulated; presented CodeExchange, a new search engine that leverages this context to  support fl uid, expressive 
reformulation of queries; and evaluated the use in a field deployment and laboratory experi ment. Our findings show 
significant promise, with reformulation extensively used in successful queries from the field and leading to searches that on average take 2 ½ minutes less time. 
Substantial work remains to be done. At this time, the kinds 
of information we leverage from the context of previous results is relatively small (keywords reco mmendations based on words 
that appear most frequently across variable names, critiques that allow developers to dial up  or down properties, and selec-
tion of semantic constructs from the source code). Many op-portunities exist for augmenti ng this, for instance with the 
number of past bugs reported, the number of authors, the num-ber of past clones made, and so on – all information that can influence the choice a developer makes in which result to study, copy, or download. We pl an on exploring how to inte-
grate such additional information, without overloading the en-
vironment. Additionally, we plan to perform furt her studies on 
how developers use CodeExchange and how it helps them in their day-to-day work, especially as compared to a broader 
range of search engines, including Google.  
A
CKNOWLEDGMENT  
This work was sponsored by NSF grant CCF-1321112. 
REFERENCES  
[1] “Apache Lucene - Apache Solr.” [Online]. Available: 
http://lucene.apache.org/solr/. [Accessed: 05-Sep-2014]. 
[2] “Code Search · GitHub.” [Online]. Available: https://github.com/search. [Accessed: 05-Sep-2014]. 
[3] “CodeExchange.” [Online]. Available: http://codeexchange.ics.uci.edu/. [Accessed: 05-Sep-2014]. 
[4] “GitHub · Build software better, together.” [Online]. Available: https://github.com/. [Accessed: 05-Sep-2014].  
Subject/g3
ID/g3Minecraft/g3/g3 JBoss/g3/g3 Android/g3/g3Raspberry/g3
Pi/g3/g3Convex/g3Hull/g3/g3 Oracle/g3/g3/g3CodeE/g3
AVG/g3GitHub/g3
AVG/g3
1/g3 7.62/g3 10.87/g3 4.88/g3 9.83/g3 7.58/g3 7.72/g3 6.69/g3 9.47/g3
2/g3 12.23/g3 1.42/g3 2.80/g3 17.42/g3 9.87/g3 10.27/g3 9.70/g3 8.30/g3
3/g3 11.00/g3 8.60/g3 11.27/g3 20.00/g3 2.92/g3 9.75/g3 8.39/g3 12.78/g3
4/g3 12.55/g3 3.33/g3 4.38/g3 2.10/g3 3.08/g3 1.20/g3 2.21/g3 6.67/g3
5/g3 3.03/g3 2.00/g3 1.95/g3 6.60/g3 3.33/g3 1.60/g3 2.77/g3 3.40/g3
6/g3 20.00/g3 11.02/g3 10.60/g3 5.25/g3 4.07/g3 2.85/g3 6.37/g3 11.56/g3Table 1. Search Task Time Results. 
33[5] “Google Code.” [Online]. Available: https://code.google.com/. 
[Accessed: 05-Sep-2014]. 
[6] “Hacker News.” [Online]. Available: https://news.ycombinator.com/. [Accessed: 05-Sep-2014].  
[7] “Java Forums at the Big Moose Saloon.” [Online]. Available: http://www.coderanch.com/forums. [Accessed: 05-Sep-2014]. 
[8] “Java.” [Online]. Available: http://www.reddit.com/r/java/. [Accessed: 05-Sep-2014]. 
[9] “jQuery.” [Online]. Available: http://jquery.com/. [Accessed: 05-Sep-2014]. 
[10] “MALLET homepage.” [Online]. Available: http://mallet.cs.umass.edu/. [Accessed: 05-Sep-2014]. 
[11] “Ohloh Code Search.” [Online]. Available: https://code.ohloh.net/. [Accessed: 05-Sep-2014]. 
[12] “searchcode | source code search engine.” [Online]. Available: https://searchcode.com/. [Accessed: 05-Sep-2014]. 
[13] “Similarity (Lucene 2.9.4 API).” [Online]. Available: 
https://lucene.apache.org/core/2_9_4/api/all/org/apache/lucene/search/Si
milarity.html. [Accessed: 05-Sep-2014]. 
[14] “Sourcegraph.” [Online]. Available: https://sourcegraph.com/. [Accessed: 05-Sep-2014].  
[15] “Stack Overflow.” [Online]. Available: http://stackoverflow.com/. [Accessed: 05-Sep-2014]. 
[16] A. T. Nguyen, H. A. Nguyen, T. T. Nguyen, and T. N. Nguyen, 
“GraPacc: A Graph-based Pattern-oriented, Context-sensitive Code Completion Tool,” Proceedings of the 34th International Conference on 
Software Engineering, Piscataway, NJ, USA, 2012, pp. 1407–1410. 
[17] B. Antunes, J. Cordeiro, and P. Gomes, “Context-Based Search in 
Software Development” in European Conference of Artificial Intelligence (ECAI), 2012, vol. 242,  pp. 937-942. 
[18] C. D. Manning, P. Raghavan, and H. Schütze, “Introduction to 
Information Retrieval”. New York, NY, USA: Cambridge University 
Press, 2008. 
[19] D. Lawrie, C. Morrell, H. Feild, and D. Binkley, “What’s in a Name? A 
Study of Identifiers,” 14th IEEE International Conference on Program 
Comprehension, 2006. ICPC 2006,  2006, pp. 3–12. 
[20] D. M. Blei, A. Y. Ng, and M. I. Jordan, “Latent dirichlet allocation,” J. Mach. Learn. Res., vol. 3, pp. 993–1022, Mar. 2003.  
[21] D. Mandelin, L. Xu, R. Bodík, and D. Kimelman, “Jungloid Mining: 
Helping to Navigate the API Jungle,” in Proceedings of the 2005 ACM SIGPLAN Conference on Programming Language Design and 
Implementation, New York, NY, USA, 2005, pp. 48–61. 
[22] D. Morris, M. Ringel Morris, and G. Venolia, “SearchBar: A Search-
centric Web History for Task Resumption and Information Re-finding,”  
Proceedings of the SIGCHI Conference on Human Factors in 
Computing Systems, New York, NY,  USA, 2008, pp. 1207–1216. 
[23] E. Linstead, S. Bajracharya, T. Ngo, P. Rigor, C. Lopes, and P. Baldi, 
“Sourcerer: Mining and Searching Internet-scale Software 
Repositories,” Data Min. Knowl. Discov., vol.  18, no. 2, pp. 300–336, 
Apr. 2009. 
[24] F. A. Durão, T. A. Vanderlei, E. S. Almeida, and S. R. de L. Meira, “Applying a Semantic Layer in a Source Code Search Tool,” in Proceedings of the 2008 ACM Symposium on Applied Computing, New 
York, NY, USA, 2008, pp. 1151–1157. 
[25] F. Deissenbock and M. Pizka, “Concise and consistent naming [software 
system identifier naming],” in 13th In ternational Workshop on Program 
Comprehension, 2005. IWPC 2005. Proceedings, 2005, pp. 97–106. 
[26] G. Fischer, S. Henninger, and D. Redmiles, “Cognitive Tools for Locating and Comprehending Software Objects for Reuse,” in Proceedings of the 13th International Conference on Software 
Engineering, Los Alamitos, CA, USA, 1991, pp. 318–328. 
[27] G. Salton and C. Buckley, “Term-weighting approaches in automatic 
text retrieval,” Information Processing & Management, vol. 24, no. 5, 
pp. 513–523, 1988. 
[28] H. Rubenstein and J. B. Goodenough, “Contextual Correlates of Synonymy,” Commun. ACM, vol. 8, no.  10, pp. 627–633, Oct. 1965. [29] J. Brandt, M. Dontcheva, M. Weskamp, and S. R. Klemmer, “Example-
centric Programming: Integrating Web Search into the Development Environment,” Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, New Yor k, NY, USA, 2010, pp. 513–
522. 
[30] J. Brandt, P. J. Guo, J. Lewenstein, M. Dontcheva, and S. R. Klemmer, “Two Studies of Opportunistic Programming: Interleaving Web Foraging, Learning, and Writing Code,”  Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, New York, NY, 
USA, 2009, pp. 1589–1598. 
[31] J. Cohen, “Statistical Power Analysis for the Behavioral Sciences”, 2 edition. Hillsdale, N.J: Routledge, 1988. 
[32] J. Singer, T. C. Lethbridge, C. Gauthier, W. M. Gentleman, H. Johnson, 
and J. Sayyad, “What’s so great about `grep’? Implications for program 
comprehension tools,” Tech. rep., National Research Council, Canada, 
1997. 
[33] J. Starke, C. Luce, and J. Sillito, “Working with search results,” in ICSE Workshop on Search-Driven Development- Users, Infrastructure, Tools 
and Evaluation, 2009. SUITE ’ 09, 2009, pp. 53 –56 
[34] J. Stylos and B. A. Myers, “Mica: A Web-Search T ool for Finding API 
Components and Examples,” IEEE Symposium on Visual Languages and Human-Centric Computing, 2006. VL/HCC 2006, 2006, pp. 195–
202. 
[35] K. T. Stolee, S. Elbaum, and D. Dobos, “Solving the Search for Source 
Code,” ACM Trans. Softw. Eng. Methodol., vol. 23,  no. 3, pp. 26:1–
26:45, Jun. 2014. 
[36] M. Bruch, M. Monperrus, and M. Mezini, “Learning from Examples to 
Improve Code Completion Systems,” Proceedings of the 7th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on The Foundations of Software 
Engineering, New York, NY, USA,  2009, pp. 213–222. 
[37] M. D . Williams, “What Makes RABBIT Run?,” Int. J. Man-Mach. 
Stud., vol. 21, no. 4, pp. 333–352, Oct. 1984. 
[38] M. Goldman and R. C. Miller, “Codetrail: Connecting source code and 
web resources,” in IEEE Symposium on Visual Languages and Human-
Centric Computing, 2008. VL/HCC 2008, 2008, pp. 65–72. 
[39] M. Umarji, S. E. Sim, and C. Lopes, “Archetypal Internet-Scale Source 
Code Searching,” in Open Source Development, Communities and Quality, B. Russo, E. Damiani, S. Hissam, B. Lundell, and G. Succi, 
Eds. Springer US, 2008, pp. 257–263. 
[40] O. A. L. Lemos, A. C. de Paula,  F. C. Zanichelli, and C. V. Lopes, 
“Thesaurus-based Automatic Query Expansion for Interface-driven 
Code Search,” in Proceedings of the 11th Working Conference on Mining Software Repositories, New Yor k, NY, USA, 2014, pp. 212–
221. 
[41] O. A. L. Lemos, S. K. Bajracharya, J. Ossher, R. S. Morla, P. C. Masiero, P. Baldi, and C. V. Lope s, “CodeGenie: Using Test-cases to 
Search and Reuse Source Code,” in Proceedings of the Twenty-second IEEE/ACM International Conference on Automated Software 
Engineering, New York, NY, USA,  2007, pp. 525–526. 
[42] O. Barzilay, O. Hazzan, and A. Yehudai, “Characterizing Example 
Embedding as a software activity,” in ICSE Workshop on Search-Driven Development-Users,  Infrastructure, Tools and Evaluation, 2009. 
SUITE ’09, 2009, pp. 5 –8. 
[43] O. Hummel, W. Janjic, and C. Atkinson, “Code Conjurer: Pulling 
Reusable Software out of Thin Air,” IEEE Software, vol. 25, no. 5, pp. 
45–52, Sep. 2008. 
[44] R. E. Gallardo-Valencia and S. E. Sim, “What Kinds of Development 
Problems Can Be Solved by Searching the Web?: A Field Study,” in Proceedings of the 3rd International Workshop on Search-Driven Development: Users, Infrastructure, Tools, and Evaluation, New York, 
NY, USA, 2011, pp. 41–44. 
[45] R. Hoffmann, J. Fogarty, and D. S. Weld, “Assieme: Finding and 
Leveraging Implicit References in a Web Search Interface for Programmers,” Proceedings of the 20th Annual ACM Symposium on User Interface Software and Technology, New York, NY, USA, 2007, 
pp. 13–22. 
[46] R. Holmes, “Do developers search  for source code examples using 
multiple facts?,” in ICSE Workshop on Search-Driven Development-
34Users, Infrastructure, Tools and Evaluation, 2009. SUI TE ’09, 2009, pp. 
13–16. 
[47] R. Holmes, R. J. Walker, and G. C. Murphy, “Approximate Structural 
Context Matching: An Approach to Recommend Relevant Examples,” 
IEEE Trans. Softw. Eng., vol. 32,  no. 12, pp. 952–970, Dec. 2006. 
[48] R. Sindhgatta, “Using an Information Retrieval System to Retrieve 
Source Code Samples,” in Proceedings of the 28th International Conference on Software Engineering, New York, NY, USA, 2006, pp. 
905–908. 
[49] S. Chatterjee, S. Juvekar , and K. Sen, “SNIFF: A Search Engine for Java 
Using Free-Form Queries,” in Proceedings of the 12th International 
Conference on Fundamental Approaches to Software Engineering: Held As Part of the Joint European Conferences on Theory and Practice of 
Software, ETAPS 2009, Berlin, Heidelberg, 2009, pp. 385–400. 
[50] S. D. Fleming, C. Scaffidi, D. Piorkowski, M. Burnett, R. Bellamy, J. 
Lawrance, and I. Kwan, “An Infor mation Foraging Theory Perspective 
on Tools for Debugging, Refactoring, and Reuse Tasks,” ACM Trans. 
Softw. Eng. Methodol., vol. 22, no. 2, pp. 14:1–14:41, Mar. 2013. 
[51] S. E. Sim, M. Agarwala, and M. Um arji, “A Controlled Experiment on 
the Process Used by Developers During Internet-Scale Code Search,” 
Finding Source Code on the Web for Remix and Reuse, S. E. Sim and 
R. E. Gallardo-Valencia, Eds. Spri nger New York, 2013, pp. 53–77. 
[52] S. Haiduc, G. Bavota, A. Marcus, R. Oliveto, A. De Lucia, and T. 
Menzies, “Automatic Query Reformulations for Text Retrieval in Software Engineering,” Proceedings of the 2013 International Conference on Software Engineering,  Piscataway, NJ, USA, 2013, pp. 
842–851. [53] S. Henninger, “Using iterative refinem ent to find reusable software,” 
IEEE Software, vol. 11, no. 5, pp. 48–59, Sep. 1994. 
[54] S. K. Bajracharya and C. V. L opes, “Analyzing and mining a code 
search engine usage log,” Empir Software Eng, vol. 17, no. 4–5, pp. 
424–466, Aug. 2012. 
[55] S. P. Reiss, “Semantics-based Code Search,” Proceedings of the 31st International Conference on Software Engineering, Washington, DC, 
USA, 2009, pp. 243–253. 
[56] S. Thummalapenta and T. Xie, “Parseweb: A Programmer Assistant for Reusing Open Source Code on the Web,” in Proceedings of the Twenty-second IEEE/ACM International Conference on Automated Software 
Engineering, New York, NY, USA,  2007, pp. 204–213. 
[57] W. Kruskal, T. S. Ferguson, J. W.  Tukey, E. J. Gumbel, and F. J. 
Anscombe, “Discussion of the Papers of Messrs. Anscombe and Daniel,” Technometrics, vol. 2, no.  2, p. 157, May 1960. 
[58] X. Ge, D. Shepherd, K. Damcvski, and E. Murphy-Hill, “How 
developers use multi-recommendation system in local code search,” in 2014 IEEE Symposium on Visual Languages and Human-Centric 
Computing (VL/HCC), 2014, pp. 69–76. 
[59] Y. Ye and G. Fischer, “Supporting Reuse by Delivering Task-relevant 
and Personalized Information,” Proceedings of the 24th International Conference on Software Engineering, New York, NY, USA, 2002, pp. 
513–523. 
[60] Z. Yue, S. Han, D. He, and J. Jiang, “Influences on Query 
Reformulation in Collaborative Web Search,” Computer, vol. 47, no. 3, pp. 46–53, Mar. 2014. 
 
35