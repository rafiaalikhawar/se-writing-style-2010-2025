An Automated Approach to Estimating Code Coverage Measures
via Execution Logs
Boyuan Chen
York University
Toronto, Canada
chenfsd@cse.yorku.caJian Song
Baidu Inc.
Beijing, China
songjian02@baidu.comPeng Xu
Baidu Inc.
Beijing, China
xupeng@baidu.com
Xing Hu
Baidu Inc.
Beijing, China
huxing@baidu.comZhen Ming (Jack) Jiang
York University
Toronto, Canada
zmjiang@cse.yorku.ca
ABSTRACT
Software testing is a widely used technique to ensure the qual-
ity of software systems. Code coverage measures are commonly
used to evaluate and improve the existing test suites. Based on our
industrial and open source studies, existing state-of-the-art codecoverage tools are only used during unit and integration testing
duetoissueslikeengineeringchallenges,performanceoverhead,
and incomplete results. To resolve these issues, in this paper we
have proposed an automated approach, called LogCoCo, to estimat-
ingcodecoveragemeasuresusingthereadilyavailableexecution
logs. Using program analysis techniques, LogCoCo matches the
execution logs with their corresponding code paths and estimates
three different code coverage criteria: method coverage, statement
coverage,andbranchcoverage.Casestudiesononeopensource
system(HBase)andfivecommercialsystemsfromBaiduandsys-
tems show that: (1) the results of LogCoCo are highly accurate
(>96% in seven out of nine experiments) under a variety of testing
activities(unittesting,integrationtesting,andbenchmarking);and
(2) the results of LogCoCo can be used to evaluate and improve
the existing test suites. Our collaborators at Baidu are currently
considering adopting LogCoCo and use it on a daily basis.
CCS CONCEPTS
•Software and its engineering →Software testing and de-
bugging;
KEYWORDS
software testing, logging code, test coverage, empirical studies,
software maintenance;
ACM Reference Format:
BoyuanChen,JianSong,PengXu,XingHu,andZhenMing(Jack)Jiang.
2018. An Automated Approach to Estimating Code Coverage Measures
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACM
mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ASE ’18, September 3–7, 2018, Montpellier, France
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5937-5/18/09...$15.00
https://doi.org/10.1145/3238147.3238214via Execution Logs. In Proceedings of the 2018 33rd ACM/IEEE International
Conference on Automated Software Engineering (ASE ’18), September 3–7,
2018,Montpellier,France. ACM,NewYork,NY,USA, 12pages.https://doi.
org/10.1145/3238147.3238214
1 INTRODUCTION
A recent report by Tricentis shows that software failure caused
$1.7 trillion in financial losses in 2017 [ 48]. Therefore, software
testing, which verifies a system’s behavior under a set of inputs, is
arequiredprocesstoensurethesystemquality.Unfortunately,as
softwaretestingcanonlyshowthepresenceofbugsbutnottheir
absence[15],completetesting(a.k.a.,revealingallthefaults)isoften
not feasible [ 3,36]. Thus, it is important to develop high quality
test suites, which systematically examine a system’s behavior.
Code coverage measures the amount of executed source code,
whenthesystemisrunningundervariousscenarios[ 18].Thereare
various code coverage criteria (e.g., statement coverage, condition
coverage, and decision coverage) proposed to measure how well
the testsuite exercises thesystem’s source code.For example, the
statementcoveragemeasurestheamountofexecutedstatements,
whereas the condition coverage measures the amount of true/false
decisionstakenfromeachconditionalstatement.Althoughthere
are mixed results between the relationship of code coverage and
testsuiteeffectiveness[ 30,42],codecoverageisstillwidelyused
in research [ 53,57,59] and industry [ 1,54,58] to evaluate and
improve the quality of existing test suites.
Therearequiteafewcommercial(e.g.,[ 14,49])andopensource
(e.g., [11,32]) tools already available to automatically measure
thecodecoverage. Allthesetoolsrelyon instrumentationatvari-
ouscode locations(e.g., methodentry/exit pointsandconditional
branches)eitheratsourcecode[ 6]oratbinary/bytecodelevels[ 11,
31]. There are three main issues associated with these tools when
usedinpractice:(1) Engineeringchallenges :forreal-worldlarge-
scale distributed systems, it is not straight-forward to configure
and deploy such tools, and collect the resulting data [ 47,71]. (2)
Performanceoverhead :theheavyinstrumentationprocesscan
introduce performance overhead and slow down the system exe-cution [
21,29,64]. (3)Incomplete results : due to various issues
associated with code instrumentation, the coverage results from
these tools do not agree with each other and are sometimes incom-
plete[27].Hence,theapplicationcontextofthesetoolsaregenerally
very limited (e.g., during unit and integration testing). It is very
305
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:29 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Boyuan Chen, Jian Song, Peng Xu, Xing Hu, and Zhen Ming (Jack) Jiang
challengingtomeasurethecoverageforthesystemundertest(SUT)
in a field-like environment to answer questions like evaluating the
representativeness of in-house test suites [ 66]. Such problem is
going to be increasingly important, as more and more systems are
adopting the rapid deployment process like DevOps [40].
Execution logs are generated by the output statements (e.g.,
Log.info(‘‘User’’ + user + ‘‘checked out’’) ) that devel-
opersinsertintothesourcecode.Studieshaveshownthatexecution
logshavebeenactivelymaintainedformanyopensource[ 8,74]
andcommercialsoftwaresystems[ 19,55]andhavebeenusedexten-
sivelyinpracticeforavarietyoftasks(e.g.,systemmonitoring[ 60],
problem debugging [ 52,73], and business decision making [ 4]). In
this paper, we have proposed an approach, called LogCoCo (Log-
basedCodeCoverage), which automatically estimates the code
coverage criteria by analyzing the readily available execution logs.
We first leverage program analysis techniques to extract a set of
possible code paths from the SUT. Then we traverse through these
code paths to derive the list of corresponding log sequences, repre-
sentedusingregularexpressions.Wematchthereadilyavailable
executionlogs,eitherfromtestingorinthefield,withtheseregular
expressions. Based on the matched results, we label the code re-
gionsas Must(definitelycovered), May(maybecovered,maybenot),
andMust-not (definitely not covered) and use these labels to infer
threetypesofcodecoveragecriteria:methodcoverage,statement
coverage,andbranchcoverage.Thecontributionsofthispaperare:
(1)This work systematically assesses the use of the code cov-
eragetoolsinapracticalsetting.Itisthefirstwork,tothe
authors’knowledge,toautomaticallyestimatecodecoverage
measures from execution logs.
(2)Casestudiesononeopensourceandfivecommercialsystems
(fromBaidu)showthatthecodecoveragemeasuresinferred
byLogCoCoishighlyaccurate:achievinghigherthan96%
accuracy in seven out of nine experiments. Using LogCoCo,
we can evaluate and improve the quality of various test
suites(unittesting,integrationtesting,andbenchmarking)
by comparing and studying their code coverage measures.
(3)ThisprojectisdoneincollaborationwithBaidu,alargescale
software company whose services are used by hundreds of
millions of users. Our industrial collaborators are currently
considering adopting and using LogCoCo on a daily basis.
This clearlydemonstrates the usefulness and thepractical
impact of our approach.
PaperOrganization : the restof this paper isstructured as fol-
lows. Section 2explains issues when applying code coverage tools
inpractice.Section 3explainsLogCoCobyusingarunningexample.
Section4describesourexperimentsetup.Section 5and6studytwo
research questions, respectively. Section 7introduces the related
work.Section 8discussesthethreatstovalidity.Section 9concludes
the paper.
2 APPLYING CODE COVERAGE TOOLS IN
PRACTICE
We interviewed a few QA engineers at Baidu regarding their expe-
rienceontheuseofthecodecoveragetools.Theyregularlyused
code coverage tools like JaCoCo [ 32] and Cobertura [ 11]. How-
ever, they apply these tools only during the unit and integrationtesting. It turned out that there are some general issues associated
with these state-of-the-art code coverage tools, which limit their
application contexts (e.g., during performance testing and in the
field).Wesummarizedthemintothefollowingthreemainissues,
whicharealsoproblematicforothercompanies[ 47,71]:(1)Engi-
neeringchallenges: dependingontheinstrumentationtechniques,
configuringanddeployingthesetoolsalongwiththeSUTcanbe
tedious (e.g., involving recompilation of source code) and error-
prone(e.g.,changingruntimeoptions).(2) Performanceoverhead :
althoughthese toolscanprovide variouscodecoverage measures
(e.g.,statement,branch,andmethodcoverage),theyintroduceaddi-
tional performance overhead. Such overhead can be very apparent,
when the SUT is processing hundreds or thousands of concurrent
requests.Therefore,theyarenotsuitabletoberunningduringnon-
functional testing (e.g., performance or user acceptance testing) or
in the field (e.g., to evaluate and improve the representativeness of
thein-housetestsuites).(3) Incompleteresults :thecodecoverage
results from these tools are sometimes incomplete.
In this section, we will illustrate the three issues mentioned
above through our experience in applying the state-of-the-art code
coverage tools on HBase [23] in a field-like environment.
2.1 The HBase Experiment
HBase,whichisanopensourcedistributedNoSQLdatabase,has
been used by many companies (e.g., Facebook [ 50], Twitter, and
Yahoo! [24]) serving millions of users everyday. It is important
toassessitsbehaviorunderload(a.k.a.,collectingcodecoverage
measures)andensuretherepresentativenessofthein-housetest
suites (a.k.a., covering the behavior in the field).
YCSB[72]isapopularbenchmarksuite,originallydevelopedby
Yahoo!,toevaluatetheperformanceofvariouscloud-basedsystems
(e.g., Cassandra, Hadoop, HBase, and MongoDB). YCSB contains
six core benchmark workloads ( A,B,C,D,E, and F) which are
derived by examining a wide range of workload characteristics
fromreal-worldapplications[ 12].Hence,weusethisbenchmark
suite to simulate the field behavior of HBase.
OurHBaseexperimentwasconductedonathree-machine-cluster
with one master node and two region server nodes. These three
machineshavethesamehardwarespecifications:Inteli7-4790CPU,
16GBmemory,and2TBhard-drive.WepickedHBaseversion1 .2.6
for this experiment, since it was the most current stable release by
thetimeofourstudy.Weconfiguredthenumberofoperationsto
be 10 million foreach benchmark workload.Each benchmark test
exercised all the benchmark workloads under one of the following
threeYCSBthreadnumberconfigurations:5,10,and15.Different
number of YCSB threads indicates different load levels: the higher
the number of threads, the higher the benchmark load.
2.2 Engineering Challenges
Since HBase is implemented in Java, we experimented with two
Java-based state-of-the-art code coverage tools: JaCoCo [ 32] and
Clover [10]. Both tools have been used widely in research (e.g., [ 27,
28,41]) and practice (e.g., [ 47,71]). These two tools use different
instrumentationapproachestocollectingthecodecoveragemea-
sures.Clover[ 10]instrumentstheSUTandinjectsitsmonitoring
306
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:29 UTC from IEEE Xplore.  Restrictions apply. An Automated Approach to Estimating Code Coverage Measures via Execution Logs ASE ’18, September 3–7, 2018, Montpellier, France
0102030405060708090100110120
ABCDEF
WorkloadOverhead (%)
Figure 1: The JaCoCo overhead for the HBase experiment.
probesatthesourcecodelevel,whileJaCoCo[ 31]usesthebytecode
instrumentation technique and injects its probes during runtime.
Overall, we found the configuration and the deployment pro-
cesses for both tools to be quite tedious and error-prone. For ex-
ample, to enable the code coverage measurement by JaCoCo, we
hadtoexaminevariousHBasescriptstofigureoutthecommand
lineoptionstostartupHBaseanditsrequiredjarfiles.Thisprocess
wasnon-trivialandrequiredmanualeffort,asthecommandline
options could differ from systems to systems and even different
versions of the same systems. The process for Clover was even
morecomplicated,aswehadtoreconfiguretheMavenbuildsys-
tem to produce a new set of instrumented jar files. In addition, we
could not simply copy and replace the newly instrumented jar filesintothetestenvironmentduetodependencychanges.Itrequireda
thoroughcleanupofthetestenvironmentbeforere-deployingSUT
andrunninganytests.Weconsideredsucheffortstobenon-trivial,
as we had to repeat this process on all three target machines. This
effortcouldbemuchhigheriftheexperimentsweredoneontensorhundredsofmachines,whichisconsideredasanormaldeployment
size for HBase [65].
We decided to proceed with JaCoCo. Its instrumentation and
deploymentprocesswerelessintrusive,asthebehaviorofHBase
needed to be assessed in a field like environment.
2.3 Performance Overhead
We ran each benchmark test twice: once with JaCoCo enabled, and
once without. We gathered the response time statistics for each
benchmark run and estimated the performance overhead intro-
ducedbyJaCoCo.Figure 1showstheperformanceoverheadforthe
sixdifferentworkloads(workload A,...,F).Withineachworkload,
the figure shows the average performance overhead (in percent-
ages) as well as the confidence intervals across different YCSB
threadnumbers.Forexample,theaverageperformanceoverhead
for workload Ais 16%, but can vary from 10% to 22% depending on
the number of threads.
Depending on the workload, the performance impact of JaCoCo
varies. Workload Bhas the highest impact (79% to 106%) with Ja-
CoCo enabled, whereas workload Ehas the smallest impact (4%to13%).Overall,JaCoCodoeshaveanegativeimpactontheSUT
with a noticeable performance overhead ( >8% on average) across
all benchmark tests. Hence, it is not feasible to deploy JaCoCo in a
field-likeenvironmentwiththeSUT,asitcansignificantlydegrade
the user experience.
2.4 Incomplete Results
WesampledsomeofthecodecoveragedataproducedbyJaCoCo
for manual verification. We found that JaCoCo did not reportthe code coverage measures for some modules. JaCoCo only in-strumented the HBase modules (a.k.a., the
hbase-server mod-
ule) in which the YCSB benchmark suite directly invoked. If the
hbase-server moduleinvokesanothermodule(e.g., client)not
specified during the HBase startup, the clientmodule will not be
instrumentedbyJaCoCoandwillnothaveanycoveragedatare-
ported. For example, during our experiment, the logging statement
from the method setTableState inZKTableStateManager.java
was outputted. Hence, setTableState should be covered. Since
setTableState calls setTableStateInZK ,whichcalls joinZNode
inZKUtil.java , the method joinZNode should also be covered.
However, the joinZNode method was marked as not covered by
JaCoCo. A similar problem was also reported in [27].
To resolve the three issues mentioned above, we have proposed
a new approach to automatically estimating the code coverage
measuresbyleveragingthereadilyavailableexecutionlogs.Our
approachestimatesthecodecoveragebycorrelatinginformation
in the source code and the log files, once the tests are completed. It
imposeslittleperformanceoverheadtotheSUT,andrequiresno
additional setup or configuration actions from the QA engineers.
3 LOGCOCO
Source Code
Log FilesCode Path & 
LogRE Pairs 
Log 
SequencesLabeled
Source CodeCoverage 
ResultsProgram
Analysis
Log
AnalysisPath
AnalysisCode Coverage
Estimation(1)
(2)(3) (4)
Figure 2: An overview of LogCoCo.
Inthissection,wewilldescribeLogCoCo,whichisanautomated
approach to estimating code coverage measures using execution
logs. As illustrated in Figure 2, our approach consists of the fol-
lowing four phases: (1) during the program analysis phase, we
analyze the SUT’s source code and derive a list of possible code
pathsandtheir corresponding logsequencesexpressedinregular
expressions (LogRE). (2) During the log analysis phase, we analyze
theexecutionlogfilesandrecoverthelogsequencesbasedontheir
execution context. (3) During the path analysis phase, we match
eachlogsequencewithoneofthederivedLogREandhighlightthe
correspondingcodepathswiththreekindsoflabels: May,Must,and
Must-not . (4) Based on the labels, we estimate the values for the
followingthreecodecoveragecriteria:methodcoverage,statement
coverage,andbranchcoverage.Intherestofthissection,wewill
explaintheaforementionedfourphasesindetailswitharunning
example shown in Figure 3.
307
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:29 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Boyuan Chen, Jian Song, Peng Xu, Xing Hu, and Zhen Ming (Jack) Jiang
Log Sequences  Seq 1  Seq 2   
Final  Code 
Coverage  Branch selection set  [if@4: true,for@12: true,if@16: false] [if@4: false,for@12: true,if@16: false] 
LogRE  (Log@3)(Log@51)(Log@14)+  (Log@3)(Log@14)+  
Code Snippets  Intermediate Code Coverage  
1     void computation(int a, int b) { Must Must  Must  
2       int a = randomInt(); Must Must  Must  
3       log.info("Random No: " + a); Must Must  Must  
4       if (a < 10) { Must Must  Must  
5         a = process(a); Must Must-not  Must  
6       } else { Must-not  Must  Must  
7         a = a + 10; Must-not  Must  Must  
8       } - - - 
9       if (a % 2 = 0) { Must Must  Must  
10         a ++; May May May 
11       } - - - 
12       for (;b < 3; b++) { Must Must  Must  
13         a ++; Must Must  Must  
14         log.info("Loop: " + a); Must Must  Must  
15       } - - - 
16       if (a > 20) { Must Must  Must  
17         log.info("Check: " + a); Must-not  Must-not  Must-not  
18       } - - - 
19     } - - - 
ՔՔ ՔՔ ՔՔ ՔՔ ՔՔ 
50     int process(int num) { Must Must  Must  
51       log.info("Process: " + (++num)); Must Must  Must  
52       return num; Must Must  Must  
53 } - - - 
Figure 3: The code snippet of our running example.
3 . 1 Ph a s e1-P r ogram Analysis
DifferentsequencesofloglineswillbegeneratediftheSUTexe-
cutes different scenarios. Hence, the goal of this phase is to derive
thematchingpairsbetweenthelistofpossiblecodepathsandtheir
corresponding log sequences. This phase is further divided into
three steps:
Step 1 - Deriving AST for Each Method. wederivethepermethod
Abstract Syntax Tree (AST) using a static analysis tool called Java
Development Tools (JDT) [ 17] from the Eclipse Foundation. JDT is
a very robust and accurate program analysis tool, which has been
usedin manysoftware engineeringresearch papers(e.g., bugpre-
diction[78],loggingcodeanalysis[ 9],andsoftwareevolution[ 68]).
ConsiderourrunningexampleshownontheleftpartofFigure 3.
ThisstepwillgeneratetwoASTsforthetwomethods: computation
andprocess.EachnodeintheresultingASTismarkedwiththe
correspondinglinenumberandthestatementtype.Forexample,
at line 3, there is a logging statement and at line 4 there is an if
statement. There are alsoedges connecting two nodes if one node
is the parent of the other node.
Step 2 - Deriving Call Graphs. the resulting ASTs from the previ-
oussteponlycontainthecontrolflowinformationatthemethod
level. In order to derive a list of possible code paths, we need toform call graphs by chaining the ASTs of different methods. Wehave developed a script, which automatically detects method invo-
cationsintheASTs,andlinksthemwiththecorrespondingmethod
body.Intherunningexample,ourscriptwillconnectthemethod
invocationofthe process methodatline5withthecorresponding
method body starting at line 50.
Step 3 - Deriving Code Paths and LogRE Pairs. based on the re-
sulting call graphs, we will derive a list of possible code paths. The
number of resulting code paths depends on the number and the
type of control flow nodes (e.g., if, else, for, and while), which may
contain multiple branching choices. Consider the if statement at
line4inourrunningexample:dependingonthetrue/falsevalues
fortheconditionalvariable a,therecanbetwocallpathsgenerated:
[4, 5, 50, 51, 52] and[4, 6, 7].
WeleveragetheBreadth-First-Search(BFS)algorithmtotraverse
throughthecallgraphsinorderto derivethelistofpossiblecode
paths and their corresponding LogREs. When visiting each control
flow node, we pick one of the decision outcomes for that node and
gotothecorrespondingbranches.Duringthisprocess,wealsokeep
track of the resulting LogREs. Each time when a logging statement
is visited, we add itto our resulting LogRE. If a logging statement
isinsidealoop,a“+”signwillbeappendedtoitindicatingthatthis
loggingstatementcouldbeprintedmorethanonce.Forthelogging
statement, which is inside a conditional branch within a loop, it
willbeappendedwitha“?”followedbya“+”.Intheend,wewill
308
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:29 UTC from IEEE Xplore.  Restrictions apply. An Automated Approach to Estimating Code Coverage Measures via Execution Logs ASE ’18, September 3–7, 2018, Montpellier, France
1  2018-01-18 15:42:18,158 INFO  [Thread-1] [Test.java:3] Random No: 2  
2  2018-01-18 15:42:18,159 INFO  [Thread-2] [Test.java:3] Random No:  4 
3  2018-01-18 15:42:18,159 INFO  [Thread-1] [Test.java:51] Process: 3  
4  2018-01-18 15:42:18,162 INFO  [Thread-2] [Test.java:14] Loop: 6  
5  2018-01-18 15:42:18,163 INFO  [Thread-1] [Test.java:14] Loop: 4  
6  2018-01-18 15:42:18,163 INFO  [Thread-1] [Test.java:14] Loop: 5  
Figure 4: Log file snippets for our running example.
generate a branch selection set for this particular code path and
its corresponding LogRE. There can be some control flow nodes,underwhichthereisnologgingstatementnode.Inthiscase,we
cannotbecertainifanycodeunderthesecontrolflownodeswill
be executed.For scalabilityconcerns, we donot visit thesubtrees
under these control flow nodes.
Inourrunningexample,therearefourcontrolflownodes:line4
(if),9(if),12( for),and16( if).Iftheconditionsaretrueforline4
and 12, and false for line 16, the branch selection set is represented
as[if@4:true, for@12:true, if@16: false] . The value for
theifconditionnode atline 9isirrelevant, asthere isno logging
statement under it. We will not visit its subtree, as no changes will
be made to the resulting LogRE. The resulting code path for this
branch selection is 1,2,3,4,5,50,51,52,9,(10),12,13,14,16 .
ThecorrespondingLogREis (Log@3)(Log@51)(Log@14)+ .Line10
in the resulting code path is shown in brackets, because there is no
logging statement under the ifcondition node at line 9. Thus, we
cannot tell if line 10 is executed based on the generated log lines.
3.2 Phase 2 - Log Analysis
Execution logs are generated when logging statements are exe-
cuted. However, since there can be multiple scenarios executed
concurrently, logs related to the different scenario executions may
be inter-mixed. Hence, in this phase, we will recover the relatedlogs into sequences by analyzing the log files. Suppose after exe-
cutingsometestcasesforourrunningexample,asetofloglines,
shown in Figure 4, is generated. This phase is further divided into
the following three steps:
Step 1 - Abstracting Log Lines. each log line contains static texts,
which describe the particular logging context, and dynamic con-
tents, which reveal the SUT’s runtime states. Logs generated by
modernloggingframeworkslikeLog4j[ 45]canbeconfiguredto
contain information such as file name and line number. Hence, we
caneasilymapthegeneratedloglinestothecorrespondinglogging
statements. In our example, each log line contains the file name
Test.java and the line number. In other cases, if the file name
and the line number is not printed, we can leverage existing log
abstractiontechniques(e.g.,[ 25,34]),whichautomaticallyrecog-
nizethedynamicallygeneratedcontentsandmaploglinesintothe
corresponding logging statements.
Step 2 - Grouping Related Log Lines. each log line contains some
execution contexts (e.g., thread or session or user IDs). In this
step,wegrouptherelatedloglinesintosequencesbyleveraging
these execution contexts. In our running example, we group therelated log lines by their thread IDs. There are in total two log
sequences in our running example, which correspond to Thread-1
andThread-2.Step 3 - Forming Log Sequences. in this step, we replace the
groupedloglinesequencesintosequencesofloggingstatements.
For example, the log line sequence of line 1,3,5,6 grouped under
Thread-1 becomes Log@3, Log@51, Log@14, Log@14.
3.3 Phase 3 - Path Analysis
Based on the obtained log line sequences from the previous phase,
we intend to estimate the covered code paths in this phase using a
four-step process.
Step 1 - Matching Log Sequences with LogREs. wematch these-
quencesofloggingstatementsobtainedinPhase2withtheLogREs
obtained in Phase 1. The two recovered log sequences are matched
with the two LogREs, which are shown on the third row in Fig-
ure3. Thesequenceof loggingstatementin ourrunningexample
Log@3, Log@51, Log@14, Log@14 (a.k.a.,Seq1)willbematched
with LogRE (Log@3)(Log@51)(Log@14)+.
Step 2 - Labeling Statements. inthesecondstep,basedoneachof
thematchedLogRE,weapplythreetypesoflabelstothecorrespond-
ing source code based on their estimated coverage: Must,May, and
Must-Not .ThelowerpartofFigure 3showstheresultsforourwork-
ingexample.For Seq1,welabellines 1,2,3,4,5,9,12,13,14,16,
50,51,52 asMust,astheselinesaredefinitelycoverediftheabove
logsequenceisgenerated.Line10ismarkedas May,becauseweare
uncertainiftheconditionofthe ifstatementatline9issatisfied.
Lines 6,7,17are marked as Must-Not , because the branch choice
istrueatline4andfalseatline17.Duetothepagelimit,wedonot
explain the source code labeling process for Seq 2.
Step 3 - Reconciling Statement-level Labels. as one line of source
code may be assigned with multiple different labels from different
log sequences, in the third step, we reconcile the labels obtainedfrom different log sequences and assign one final resulting label
to each line of source code. We use the following criteria for our
assignment:
•At least one Mustlabel: since a particular line of source
codeisconsideredas“covered”,whenithasbeenexecutedat
least once. Hence, if there is at least one Mustlabel assigned
to that line of source code, regardless of other scenarios,it is considered as covered (a.k.a., assigning
Mustlabels as
the final resulting label). In our running example, line 5 is
markedas MustinSeq1andMust-not inSeq2.Therefore,
it will be marked as Mustin the final label.
•NoMustlabels,andatleastone Maylabel:inthisparticu-
larcase,wemayhavea specificlineof source codeassigned
withall Maylabels,oramixtureof MayandMust-not labels.
As there is a possibility that this particular line of source
code can be covered by some test cases, we assigned it to be
Mayin the final resulting label. In our running example, line
10 is marked May.
•AllMust-not labels: in this particular case, since there are
noexistingtestcasescoveringthislineofsourcecode,we
assignedittobe Must-not inthefinalresultinglabel.Inour
running example, line17 is marked Must-not in bothSeq 1
andSeq 2. It will be marked as Must-not in the final label.
309
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:29 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Boyuan Chen, Jian Song, Peng Xu, Xing Hu, and Zhen Ming (Jack) Jiang
Step 4 - Inferring Labels at the Method Levels. basedontheline-
level labels, we assign one final label to each method using the
following criteria:
•for a particular method, if there is at least one line of source
code labeled as Must, this method will be assigned with a
Mustlabel.Inourrunningexamples,bothmethodswillbe
labeled as Must.
•for methods without Mustlabeled statements, we apply the
following process:
– InitialLabeling :alloftheloggingstatementsundersuch
methods should already be labeled as Must-not , since
none of these logging statements are executed. If there
isatleastoneloggingstatementwhichisnotunderany
controlflowstatementnodesinthecallgraph,thismethod
will be labeled as Must-not.
– CalleeLabeling :startingfromtheinitialsetofthe Must-not
labeled methods, we search for methods that will only
be called by these methods, and assign them with the
Must-not labels.Weiterativelyrepeatthisprocessuntil
no more methods can be added to the set.
– Remaining Labeling : we assign the Maylabels to the
remaining set of unlabeled methods.
Similarly,eachbranchwillbeassignedwithonefinalresulting
label based on the statement-level labels. Due to space constraints,
we will not explain the process here.
Ph a s e4-C od eC o v erage Estimation
In this phase, we estimate the method/branch/statement-level code
coverage measures using the labels obtained from the previous
phase. As logging statements are not instrumented everywhere,
there are code regions labeled as May, which indicates uncertainty
of coverage. Hence, when estimating thecode coverage measures,
weprovidetwovalues:aminimumandamaximumvalueforthe
above three coverage criteria. The minimum value of statement
coverage is calculated as#ofMust labels
Total#of labels, and the maximum value
iscalculatedas#ofMust labels+#ofMaylabels
Total#of labels.Inourrunningex-
ample, the numbers of Must,May, and Must-not statements are
15, 1, and 1, respectively. Therefore, the range of the statement
coverage is from 88% (15
15+1+1×100%) to 94% (15+1
15+1+1×100%).
Similarly,sincethenumberof Must,May,and Must-not branches
is 5, 1, and 2 in our running example, the range of branch coverage
is from 62 .5% (5
5+1+2×100%) to 87 .5% (5+2
5+1+2×100%).
Thenumberof Must,May,and Must-not methodsare2,0,and
0. Hence, the method level coverage is 100%.
4 CASE SETUP
Toevaluatetheeffectivenessofourapproach,wehaveselectedfive
commercialprojectsfromBaiduandonelarge-scaleopen-source
project in our case study. Table 1shows the general information
about these projects in terms of their project name, project de-
scriptions, and their sizes. All six projects are implemented in Java
andtheirdomainsspanwidelyfromwebservices,toapplication
platformsandNoSQLdatabases.Themainreasonwhywepicked
commercialprojectstostudyisthatwecaneasilygetholdofQA
engineersforquestionsandfeedback.Thefivecommercialprojects(C1,C2,C3,C4,and C5)werecarefullyselectedbasedonconsulta-
tions with Baidu’s QA engineers. We also picked one large-scalepopular open source project, HBase [
23], because we can freely
discussaboutthedetails.WefocusonHBaseversion1 .2.6inthis
study,since itisthemostrecentstable releasebythetime ofthestudy. All six studied projects are actively maintained and being
usedbymillionsorhundredsofmillionsofusersworldwide.We
proposedthefollowingtworesearchquestions(RQs),whichwill
be discussed in the next two sections:
Table 1: Information about the six studied projects.
Projet Descriptions LOC
C1Internal API library 24K
C2Platform 80K
C3Cloud service 12K
C4Video streaming service 35K
C5Distributed file system 228K
HBase Distributed NoSQL Database 453K
•RQ1: (Accuracy) How accurate is LogCoCo compared to the
state-of-the-art code coverage tools? The goal of this RQ is to
evaluate the quality of the code coverage measures derived
from LogCoCo against the state-of-the-art code coverage
tools. We intend to conduct this study using data from vari-
ous testing activities.
•RQ2:(Usefulness) Canweevaluateandimprovetheexisting
test suites by comparing the LogCoCo results derived from
various execution contexts? The goal of this RQ is to check if
the existing test suites can be improved by comparing the
estimatedcoverage measuresusingLogCoCo fromvarious
system execution contexts.
5 RQ1: ACCURACY
On one hand, existing state-of-the-art code coverage tools (e.g.,
Jacoco [32], Cobertura [ 11]) collect the code coverage measures by
excessively instrumenting the SUT either at the source code [ 6]o r
atthebinary/bytecodelevels[ 11,31].Theexcessiveinstrumenta-
tion (e.g., for every method entry/exit, and for every conditional
and loop branch) ensures accurate measurements of code coverage,butimposesproblemslikedeploymentchallengesandperformance
overhead (Section 2), which limit their application context. On the
other hand, LogCoCo is easy to setup and imposes little perfor-
manceoverheadbyanalyzingthereadilyavailableexecutionlogs.
However,theestimatedcodecoveragemeasuresmaybeinaccurate
or incomplete, as developers only selectively instrument certain
partsofthesourcecodebyaddingloggingstatements.Hence,inthis
RQ,wewanttoassessthequalityoftheestimatedcodecoverage
measures produced by LogCoCo.
5.1 Experiment
We ran nine test suites for the six studied projects as shown in
Table2. The nine test suites contained unit and integration tests.
Sincetheunittestsuiteswerenotconfiguredtogeneratelogsfor
C1,C4,andC5,wedidnotincludetheminourstudy. C5andHBase
310
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:29 UTC from IEEE Xplore.  Restrictions apply. An Automated Approach to Estimating Code Coverage Measures via Execution Logs ASE ’18, September 3–7, 2018, Montpellier, France
are distributedsystems, so weconducted their integrationtests in
a field-like deployment setting.
Each test suite was run twice: once with the JaCoCo configured,
and once without. We used the code coverage data from JaCoCo
as our oracle and compared it against the estimated results from
LogCoCo. For all the experiments, we collected data like generated
log files and code coverage measures from JaCoCo. JaCoCo is a
widely used state-of-the-art code coverage tool, which is used in
both research [ 27,28,41] and practice [ 47,71]. We picked JaCoCo
to ensure that the experiments could be done in a field-like envi-
ronment. Code coverage tools, which leverage source code-level
instrumentationtechniques,requirerecompilationandredeploy-
ment of the SUT. Such requirements would make the SUT’s testing
behaviornolongercloselyresemblethefieldbehavior.JaCoCo,a
bytecodeinstrumentationbasedcodecoveragetool,islessinvasiveandinstrumentstheSUTduringruntime.Foreachtest,wegathered
the JaCoCo results and the log files. Depending on the tests, the
sizes of the log files range from 8 MB to 1.1 GB.
5.2 Data Analysis
Wecomparedthethreetypesofcodecoveragemeasures(method,
statement, and branch coverage) derived from LogCoCo and Ja-
CoCo. Since LogCoCo marks the source code for each type of cov-
erage using the following three labels: Must,May, and Must-not ,
we calculated the percentage of correctly labeled entities for three
types of labels.
Forthe Mustlabeledmethods,wecalculatedtheportionofmeth-
odswhicharemarkedas coveredintheJaCoCoresults.Forexample,
if LogCoCo marked five methods as Mustamong which four were
reportedas coveredinJaCoCo,theaccuracyoftheLogCoComethod-
levelcoveragemeasurewouldbe4
5×100%=80%.Similarly,forthe
Must-not labeledentities,wecalculatedthepercentageofmethods
which were marked as not covered by JaCoCo.
For the Maylabeled methods, we calculated the portion of meth-
ods which are reported as coveredby JaCoCo. Note that this calcu-
lationis nottoassesstheaccuracyofthe Maycoveredmethods,but
toassesstheactualamountofmethodswhichareindeedcovered
during testing.
Whencalculatingtheaccuracyofthestatementandbranchlevel
coveragemeasuresfromLogCoCo,weonlyfocusedoncodeblocks
fromthe Mustcoveredmethods.Thisisbecauseallthestatement
and branch level coverage measures will be MayorMust-not for
theMayorMust-not labeledmethods,respectively.Itwouldnotbe
meaningfultoevaluatethesetwocasesagainatthestatementor
branch level.
Theevaluationresultsforthethreecoveragemeasuresareshown
inTable2.Ifacellismarkedas“-”,itmeansthereisnosourcecode
assigned with the label. We will discuss the results in details below.
5.3 Discussion on Method-Level Coverage
As shown in Table 2, all methods labeled with Mustare 100% ac-
curate. It means that LogCoCo can achieve 100% accuracy when
detectingcoveredmethods.Ratherthaninstrumentingallthemeth-
ods like the existing code coverage tools do, LogCoCo uses pro-gram analysis techniques to infer the system execution contexts.
For example, only 13% of the methods in C1have logs printed. The
remaining 87% of the Mustcovered methods are inferred indirectly.The methods labeled with Must-not are not always accurate.
Threecommercialprojects( C3,C4,andC5),andHBasehavesome
methods which are actually covered in the tests but are falsely
flaggedas Must-not coveredmethods.Exceptforthreecases,the
accuracyofthe Must-not labeledmethodsareallabove90%.We
manuallyexaminedthemisclassifiedinstancesandfoundthefol-
lowingtwomainreasons:(1)wehavelimitedthesizeofourcall
graphsto20levelsdeeporamaximumof100 ,000pathsperAST
tree due to memory constraints of our machine. Therefore, we
missedsomemethods,whichhaddeepercallchains.(2)Ourcur-
rent technique cannot handle recursive functions properly.
Theamountof Maycoveredmethodsthatareactuallycovered
is highly dependent on the type of projects and can range from 6%
to 83%. In addition, this number seems to be irrelevant of the types
oftestingconducted.Inordertoobtainamoreaccurateestimate
of the code coverage measures using LogCoCo, additional logging
statements need to be added into the SUT to reduce the amount
ofMaylabeled methods. However, the logging locations should be
decided strategically (e.g., leveraging techniques like [ 16,76]) in
order to minimize performance overhead.
5.4 Discussion on the Statement and Branch
Coverage
For statement and branch coverage, the accuracy of the Must-not
labelsis100%foralltheexperiments.However,theaccuracyofthe
Mustcoveredlabelsrangesfrom83%to100%forstatementcoverage
and 50% to 100% for branch coverage. In seven out of the nine total
experiments,theaccuracyofthe Mustcoveredstatementsis97%
or higher. We manually examined the cases where the LogCoCo
results are different from JaCoCo. We summarized them as follows:
(1)limitations on static analysis (LogCoCo issue) : Java supports
polymorphism. The actual type of certain objects are un-
knownuntiltheyarebeingexecuted.LogCoCoinfersthecall
graphsstaticallyandmistakenlyflagssomeofthemethod
invocations.
(2)newprogrammingconstructs(JaCoCoissue) :thelambdaex-
pressionisoneofthenewprogramminglanguageconstructs
introduced in Java 8. JaCoCo mistakenly tags some state-
ments containing lambda expressions as not covered.
Theaccuracyofthe Mustcoveredbranchesisgenerallyabove
95%, except one case: during the integration testing of C3, Log-
CoCo detectedtwo Mustcovered branchesbeing executed, oneof
which was falsely labeled. The rationales for the differences of the
branch coverage measures are the same as the statement coverage
measures.
Theamountof Mayactuallycoveredstatementsandbranchesare
generally higher than the amount of actually covered Maymethods.
However,similartothemethod-levelcoverage,wecannoteasily
guess the actual coverage information for a Maylabeled statement
or branch.
5.5 Feedback from the QA Engineers
We demonstrated LogCoCo to the QA engineers at Baidu. They
agreedthatLogCoCocanbeusedfortheirdailytestingactivities,
due to its ease of setup, wider application context, and accurate
results. In particular, instead of treating all the source code equally,
311
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:29 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Boyuan Chen, Jian Song, Peng Xu, Xing Hu, and Zhen Ming (Jack) Jiang
Table 2: Comparing the performance of LogCoCo against JaCoCo under various testing activities. The numbers above shows
the amount of overlap between LogCoCo and JaCoCo.
ProjectType of Size of Method Coverage Statement Coverage Branch Coverage
Testing the Logs (Must Must-not May) (Must Must-not May) (Must Must-not May)
C1Integration 20 MB 100% 100% 16% 99% 100% 62% 100% 100% 67%
C2Unit 600 MB 100% 100% 78% 100% 100% 75% 100% 100% 76%
Integration 550 MB 100% 100% 30% 100% 100% 90% 100% 100% 83%
C3Unit 8 MB 100% 88% 15% 84% 100% 77% 100% 100% 50%
Integration 26 MB 100% - 33% 83% 100% 89% 50% 100% 60%
C4Integration 29 MB 100% 99% 18% 97% 100% 49% 100% 100% 55%
C5Integration 1.1 GB 100% 90% 6% 97% 100% 45% 96% 100% 39%
HBaseUnit 527 MB 100% 83% 83% 99% 100% 83% 100% 100% 76%
Integration 193 MB 100% 89% 50% 99% 100% 71% 100% 100% 63%
theywouldpayparticularattentiontothecoverageofthemethods,
which have logging statements instrumented. This was because
manyoftheloggingstatementswereinsertedintoriskymethods
or methods which suffered from past field failures. Having test
casescoverthesemethodsisconsideredahigherpriority.LogCoCo
addressedthistasknicely.Inaddition,theyagreedthatLogCoCo
can also be used to speed up problem diagnosis in the field by
automaticallypin-pointingtheproblematiccoderegions.Finally,
theywerealsoveryinterestedintheamountof Maylabeledentities
(a.k.a.,methods,statements,andbranches),astheyknewlittleabout
theruntimebehavioroftheseentities.Theyconsideredreducingthe
amount of Maylabeled entities as one approach to improving their
existingloggingpracticesandwereveryinterestedtocollaborate
further with us on this topic.
Findings: The accuracy of MustandMust-not labeled entities
from LogCoCo is very high for all three types of code coverage
measures.However,onecannoteasilyinferwhethera Maylabeled
entity is actually covered in a test.
Implications: To further improve the accuracy, one must reduce
the amount of Maylabeled entities through additional instrumen-
tation. Researchers and practitioners can look into existing works
(e.g., [16,76]), which improve the SUT’s logging behavior with
minimal performance overhead.
6 RQ2: USEFULNESS
ExistingcodecoveragetoolsareusuallyappliedonlyduringunitorintegrationtestingduetovariouschallengesexplainedinSection 2.
LogCoCo, which analyzes the readily available execution logs, can
workonamuchwiderapplicationcontext.InthisRQ,weintend
to check if we can leverage the LogCoCo results from various
executioncontextstoimprovetheexistingtestsuites.Totacklethis
problem, we further split this RQ into the following two sub-RQs:
RQ2.1: Can we improve the in-house functional
test suites by the comparison among each other?
Inthissub-RQ,wewillfocusonunitandintegrationtesting,asthey
have different testing purposes. Unit testing examines the SUT’s
behavior with respect to the implementation of individual classes,
whereas integration testing examines whether individual units canworkcorrectlywhentheyareconnectedtoeachother.Weintend
tocheckifonecanleveragethecoveragedifferencestoimprovethe
existing unit or integration test suites using data from LogCoCo.
Experiment. To study this sub-RQ, we reused the data obtained
fromRQ1’sexperiments.Inparticular,weselectedthedatafrom
twocommercialprojects: C2andC3,astheycontaindatafromboth
unit and integration test suites. The main reason we focused on
the commercial projects in this sub-RQ was because we can easily
get hold of the QA engineers of Baidu for feedback or surveys (e.g.,
whethertheycanevaluateandimprovetheunitorintegrationtests
by comparing the coverage data).
Data Analysis and Discussion. Itwouldbeimpracticaltostudy
all the coverage differences from the two types of tests due to their
large size. We randomly sampled a subset of methods, where both
typesoftestingcoveredbuttheirstatementandbranchlevelcover-
agemeasuresdiffered.WepresentedthisdatasettoQAengineers
fromthetwocommercialprojectsforfeedback.Aftermanualex-
aminations, the QA engineers agreed to add additional unit testing
cases for all the cases where unit testing did not cover. However,
addingadditional integrationtestsisharderthan addingadditional
unit tests. The QA engineers rejected about 85% of the cases where
unit testing covered but integration testing missed, as they wereconsidered as hard or lower priority. We summarized their ratio-
nales as follows:
•Defensive Programming : defensive programming is a pro-
gramming style to guard against unexpected conditions. Al-
thoughitgenerallyimprovestherobustnessoftheSUT,there
can be unnecessary code introduced to guard against errors
thatwouldbeimpossibletohappen.Developersinsertmuch
errorcheckingcodeintothesystems.Someoftheseissues
are rare or impossible to happen. It is very hard to come up
withanintegrationtestcasewhoseinputvaluescanexercise
certain branches.
•LowRiskCode :someofthemodulesareconsideredaslow
riskbasedontheexperienceoftheQAengineers.Sincetheyarealreadycoveredbytheunittestsuites,addingadditional
integration test cases is considered as low priority.
312
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:29 UTC from IEEE Xplore.  Restrictions apply. An Automated Approach to Estimating Code Coverage Measures via Execution Logs ASE ’18, September 3–7, 2018, Montpellier, France
RQ2.2: Can we evaluate the representativeness
of in-house test suites by comparing them
against field behavior?
One of the common concerns associated with QA engineers is
whether the existing in-house test suites can properly represent
fieldbehavior.Weintendtocheckifonecanevaluatethequality
oftheexistingin-housetestsuitesbycomparingthemagainstfield
coverage using data from LogCoCo.
Experiment. Duetoconfidentialityreasons,wecannotdisclose
the details about the field behavior for the commercial projects.
Therefore, we studied the open source system, HBase, for RQ2.2.
The integration test suite from HBase is considered as a compre-
hensive test suite and is intended for “elaborate proofing of a re-
lease candidate beyond what unit tests can do” [ 22]. Hence, we
consideritasHBase’sin-housetestsuite.Therearetwosetupap-
proachestorunningtheHBase’sintegrationtests:amini-cluster,
or a distributed cluster. The mini-cluster setup is usually run on
onemachineandcanbeintegratedwiththeMavenbuildprocess.
The distributed cluster setup needs a real HBase cluster setup and
is invoked using a separated command. In this experiment, we ran
under both setups and collected their logs.
TheworkloadsdefinedinYCSBarederivedbyexaminingawide
rangeof workloadcharacteristicsfrom realwebapplications[ 12].
We thus used the YCSB benchmark test suite to mimic the field be-
havior of HBase. However, as we discovered under default settings,
HBasedidnotoutputanylogsduringthebenchmarkingprocess.
We followed the instructions from [ 33] to change the log levels
forHBasefromINFOtoDEBUGonthefly(a.k.a.,withoutserver
reboot). The resulting log file size is around 270 MB when running
the YCSB benchmark tests for one hour.
Data Analysis and Discussion. Based on the LogCoCo results,
there are 12 methods which were covered by the YCSB test and
not by the integration test under the mini-cluster setup. Most of
thesemethodswererelatedtothefunctionalitiesassociatedwith
cluster setup and communications. Under the distributed cluster
setup, which is more realistic, all the covered methods in the YSCB
test were covered by the integration test.
Thelogverbositylevelfortheunitandtheintegrationtestsof
HBaseinRQ1waskeptasthedefaultINFOlevel.Bothtestsgen-
erated hundreds of megabytes of logs. However, under the default
verbosity level, the YCSB benchmarking test generated no logs ex-
cept a few lines at the beginning of the test. This is mainly because
HBasedoesnotperformloggingfortheirnormalread,write,and
scan operations for performance concerns. The integration tests
output more INFO level logs is because: (1) many of the testing
methodsareinstrumentedwithINFOorhigherlevellogs.Suchlogsarenotprintedinpractice;and(2)inadditiontothefunctionalities
covered in the YCSB benchmark, integration tests also verify other
usecases,whichcangeneratemanylogs.Forexample,oneintegra-
tion test case is about region replications. In this test, one of the
HBase component, ZooKeeper, which is responsible for distributed
configurationandnamingservice,generatedmanyINFOlevellogs.
Wefurtherassessedtheperformanceimpactofturningonthe
DEBUGlevellogsforHBase.Wecomparedtheresponsetimeun-
der the DEBUG and the INFO level logging with YCSB threadsconfiguredat5,10,and15,respectively.Theperformanceimpact
wasverysmall( <1%)underallthreeYCSBsettings(a.k.a.,three
different YCSB benchmark runs). Thus, for HBase the impact of
DEBUGlevelloggingismuchsmallerthanJaCoCo.Furthermore,
comparedtoJaCoCo,whichrequiresserverrestarttoenable/disable
its process, the DEBUG level logging can easily be turned on/off
during runtime.
Findings: LogCoCoresultscanbeusedtoevaluateandimprove
the existing test suites. Multiple rationales are considered when
adding a test case besides coverage.Implications:
Therearematuretechniques(e.g.,EvoSuite[ 2]
andPex[69])toautomaticallygenerateunittestcaseswithhigh
coverage.However,therearenosuchtechniquesforothertypesoftests,whichstillrequirehighmanualefforttounderstandthe
context and to decide on a case-by-case basis. Further research
is needed in this area.
The coverage information from LogCoCo highly depends on
amountofgeneratedlogs.Researchersorpractitionersshould
lookintosystemmonitoringtechniques(e.g.,sampling[ 62]or
adaptive instrumentation [ 38]), which maximize the obtained
information with minimal logging overhead.
7 RELATED WORK
In this section, we will discuss two areas of related research: (1)
code coverage, and (2) software logging.
7.1 Code Coverage
Codecoveragemeasurestheamountofsourcecodeexecutedwhile
runningSUTsundervariousscenarios[ 18].Theyhavebeenused
widely in both academia and industry to assess and improve the
effectivenessofexistingtestsuites[ 3,5,43,46,51,53].Thereare
quiteafewopensource(e.g.,[ 11,32])andcommercial(e.g.,[ 14,49])
code coverage tools available. All these tools leverage additional
codeinstrumentation,eitheratthesourcecodelevel(e.g.,[ 6,10,13])
oratthebinary/bytecode(e.g.,[ 11,31,49])level,toautomatically
collecttheruntimesystembehaviorinordertomeasurethecodecoveragemeasures.In[
21],Häubletal.derivedcodecoveragein-
formationfromtheprofilingdatarecordedbyanthejust-in-time
(JIT) compiler. They also compared their coverage information
against JaCoCo. They showed their results are more accurate than
JaCoCoandyieldsmalleroverhead.Häubletal.’sapproachisdif-
ferentfromours,astheyreliedondatafromtheunderlyingvirtual
machines, whereas we focus on the logging statements from the
SUT’ssourcecode.Recently,Horváthetal.[ 27]comparedthere-
sultsfromvariousJavacodecoveragetoolsandassessedtheimpact
of their differences to test prioritization and test suite reduction.In this paper, we have evaluated the state-of-the-art Java-based
codecoveragetoolsinafieldlikesettingandproposedanewap-
proach, which leverages the readily available execution logs, to
automatically estimating the code coverage measures.
Inadditiontothetraditionalcodecoveragemetrics(e.g.,method,
branch, decision, and MC/DC coverage), new metrics have been
proposed to better assess the oracle quality [ 59], to detect untested
code regions [ 28], and to compose test cases with better abilities to
detect faults [ 67]. There are various empirical studies conducted to
examinetherelationshipbetweenthetesteffectivenessandvarious
code coverage metrics. For example, Inozemtseva and Holmes [ 30]
313
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:29 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Boyuan Chen, Jian Song, Peng Xu, Xing Hu, and Zhen Ming (Jack) Jiang
leveraged mutation testing to evaluate the fault detection effective-
nessofvariouscodecoveragemeasuresandfoundthatthereisa
lowtomoderatecorrelationbetweenthetwo.Kochharetal.[ 41,42]
performedasimilarstudy,excepttheyusedrealbugsinstead.Their
studyreportedastatisticallysignificantcorrelation(moderateto
strong) between fault detection and code coverage measures. Glig-
oric et al. [ 20] compared the effectiveness of various code coverage
metrics in the context of test adequacy. The work by Wang et
al.[66],whichistheclosesttoourwork,comparedthecoverage
betweenin-housetestsuitesandfieldexecutionsusinginvariant-
basedmodels.Ourworkdiffersfrom[ 66]inthefollowingtwomain
areas: (1) they used a record-replay tool, which instruments the
SUTtocollectcoveragemeasures.Ourworkestimatesthecoveragemeasures based on the existing logs without extra instrumentation.
(2) While they mainly focused on the client and desktop-based sys-
tems,ourfocusisontheserver-baseddistributedsystemsdeployed
in a field-like environment processing large volumes of concurrent
requests. In our context, extra instrumentation would not be ideal,
as it will have a negative impact on the user experience.
7.2 Software Logging
Software logging is a cross-cutting concern that scatters across
theentiresystemandinter-mixeswiththefeaturecode[ 39].Un-
fortunately,recentempiricalstudiesshowthattherearenowell-
established logging practices for commercial [ 19,55] and open
sourcesystems[ 8,74].Recentlyresearchershavefocusedonpro-
viding automated logging suggestions based on learning from past
loggingpractices[ 9,19,37,77]orprogramanalysis[ 76].Execution
logs are widely available inside large-scale software systems for
anomaly detection [ 26,61], system monitoring [ 52,60], problem
debugging[ 7,44,73,75],testanalysis[ 35,63],andbusinessdeci-
sionmaking[ 4].Ourworkwasinspiredby[ 75],whichleveraged
logs to infer executed code paths for problem diagnosis. However,
thisisthefirstwork,totheauthors’knowledge,whichuseslogsto
automatically estimate code coverage measures.
The limitation is that we rely on that the study systems contain
sufficientlogging.Itisgenerallythecaseamongserver-sideprojects.Forclient-sideorotherprojectswithlimitedlogging,ourapproach
should be complementary by other code coverage tools.
8 THREATS TO VALIDITY
In this section, we will discuss the threats to validity.
8.1 Internal Validity
Inthispaper, weproposedanautomatedapproach toestimating
code coverage by analyzing the readily available execution logs.
Theperformanceofourapproachhighlydependsontheamountof
theloggingandtheverbositylevels.Theamountofloggingisnota
majorissueasexistingempiricalstudiesshowthatsoftwarelogging
is pervasive in both open source [ 8,74] and commercial [ 19,55]
systems.Theloggingoverheadduetolowerverbositylevelsissmall
fortheHBasestudy.Forothersystems,onecanchoosetoenable
lower verbosity level for a short period of time or use advanced
logging techniques like sampling and adaptive instrumentation.8.2 External Validity
Inthispaper,wefocusedontheserver-sidesystemsmainlybecause
these systems use logs extensively for a variety of tasks. All these
systems are under active development and used by millions ofusers worldwide. To ensure our approach is generic, we studied
both commercial and open source systems. Although our approach
was evaluated on Java systems, to support other programming
languages, we just need to replace the parser for another language
(e.g., Saturn [ 70] for C, and AST [ 56] for Python) in LogCoCo. The
remaining process stays the same. Our findings in the case studies
may not be generalizable to systems and tools which have no or
veryfewloggingstatements(sometimesseeninmobileapplications
and client/desktop-based systems).
8.3 Construct Validity
When comparing the results between LogCoCo and the state-of-
the-art code coverage tools, we focused on JaCoCo, which collects
codecoverageinformationviabytecodeinstrumentation.Thisis
because:(1)JaCoCoiswidelyusedinBaidu,sowecaneasilycollect
the code coverage for the systems and gather feedback from the
QAengineers;and(2)weintendtoassessthecoveragemeasures
in a field-like environment, in which the system is deployed in a
distributedenvironmentandusedbymillionsofusers.Source-code-
instrumentation-based code coverage tools (e.g., [ 10]) are not ideal,
as they require recompilation and redeployment of the SUT.
9 CONCLUSIONS AND FUTURE WORK
Existingcodecoveragetoolssufferfromvariousproblems,which
limittheirapplicationcontext.Toovercomewiththeseproblems,
this paper presents a novel approach, LogCoCo, which automat-ically estimates the code coverage measures by using the readily
available execution logs. We have evaluated LogCoCo on a variety
of testing activities conducted on open source and commercial sys-
tems.OurresultsshowthatLogCoCoyieldshighaccuracyandcan
be used to evaluate and improve existing test suites.
Inthefuture,weplantoextendLogCoCoforotherprogramming
languages. In particular, we are interested in applying LogCoCo to
systemsimplementedinmultipleprogramminglanguages.Further-more,wealsointendtoextendLogCoCotosupportothercoverage
criteria(e.g.,data-flowcoverageandconcurrencycoverage).Finally,
since the quality of the LogCoCo results highly depends on the
qualityoflogging,wewillresearchintocost-effectivetechniques
to improve the existing logging code.
ACKNOWLEDGEMENT
This work is done during the first author’s internship at Baidu.
The findings and opinions expressed in this paper are those of the
authors anddo not necessarily represent orreflect those ofBaidu
and/oritssubsidiariesandaffiliates.Moreover,ourresultsdonot
in any way reflect the quality of Baidu’s products.
REFERENCES
[1]Yoram Adler, Noam Behar, Orna Raz, Onn Shehory, Nadav Steindler, Shmuel Ur,
andAviadZlotnick.2011. CodeCoverageAnalysisinPracticeforLargeSystems.
InProceedingsofthe33rdInternationalConferenceonSoftwareEngineering(ICSE).
[2]M. Moein Almasi, Hadi Hemmati, Gordon Fraser, Andrea Arcuri, and Janis Bene-felds.2017. AnIndustrialEvaluationofUnitTestGeneration:FindingRealFaults
314
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:29 UTC from IEEE Xplore.  Restrictions apply. An Automated Approach to Estimating Code Coverage Measures via Execution Logs ASE ’18, September 3–7, 2018, Montpellier, France
inaFinancialApplication.In IEEE/ACM39thInternationalConferenceonSoftware
Engineering: Software Engineering in Practice Track (ICSE SEIP).
[3]Paul Ammann and Jeff Offutt. 2017. Introduction to Software Testing (2nd ed.).
Cambridge University Press, New York, NY, USA.
[4]Titus Barik, Robert DeLine, Steven Drucker, and Danyel Fisher. 2016. The Bones
oftheSystem:ACaseStudyofLoggingandTelemetryatMicrosoft.In Companion
Proceedings of the 38th International Conference on Software Engineering).
[5]Earl T. Barr, Mark Harman, Phil McMinn, Muzammil Shahbaz, and Shin Yoo.
2015. TheOracleProbleminSoftwareTesting:ASurvey. IEEETransactionson
Software Engineering (2015).
[6]Ira Baxter. 2002. Branch Coverage for Arbitrary Languages Made Easy. http:
//www.semdesigns.com/Company/Publications/TestCoverage.pdf . Lastaccessed:
01/29/2018.
[7]Ivan Beschastnikh, Yuriy Brun, Sigurd Schneider, Michael Sloan, and Michael D.
Ernst.2011.LeveragingExistingInstrumentationtoAutomaticallyInferInvariant-
constrained Models. In Proceedings of the 19th ACM SIGSOFT Symposium and the
13th European Conference on Foundations of Software Engineering (FSE).
[8]BoyuanChenandZhenMing(Jack)Jiang.2016. Characterizingloggingpractices
in Java-based open source software projects – a replication study in Apache
Software Foundation. Empirical Software Engineering (2016).
[9]Boyuan Chen and Zhen Ming (Jack) Jiang. 2017. Characterizing and Detect-
ingAnti-patternsintheLoggingCode.In Proceedingsofthe39thInternational
Conference on Software Engineering.
[10]Clover. 2018. Java and Groovy code coverage. https://www.atlassian.com/
software/clover. Last accessed: 03/04/2018.
[11]Cobertuna.2018. Cobertura. http://cobertura.github.io/cobertura/. Lastaccessed:
01/29/2018.
[12]BrianF.Cooper,AdamSilberstein,ErwinTam,RaghuRamakrishnan,andRussell
Sears. 2010. Benchmarking Cloud Serving Systems with YCSB. In Proceedings of
the 1st ACM Symposium on Cloud Computing (SoCC).
[13]Coverage.py. 2018. A tool for measuring code coverage of Python programs.
https://coverage.readthedocs.io/en/coverage-4.5.1/. Last accessed: 01/29/2018.
[14]Semantic Designs. 2016. Test Coverage tools. http://www.semdesigns.com/
Products/TestCoverage/. Last accessed: 01/29/2018.
[15] Edsger W. Dijkstra. 1970. Notes on Structured Programming. (April 1970).
[16]Rui Ding, Hucheng Zhou, Jian-Guang Lou, Hongyu Zhang, Qingwei Lin, Qiang
Fu,DongmeiZhang,andTaoXie.2015. Log2:ACost-awareLoggingMechanism
for Performance Diagnosis. In Proceedings of the 2015 USENIX Conference on
Usenix Annual Technical Conference (ATC).
[17]Eclipse.2018. JDTJavaDevelopmentTools. https://eclipse.org/jdt/. Lastaccessed:
08/26/2016.
[18]MartinFowler.2012. TestCoverage. https://martinfowler.com/bliki/TestCoverage.
html. Last accessed: 01/29/2018.
[19]Qiang Fu, Jieming Zhu, Wenlu Hu, Jian-Guang Lou, Rui Ding, Qingwei Lin,
DongmeiZhang,andTaoXie.2014. WhereDoDevelopersLog?AnEmpirical
Study on Logging Practices in Industry. In Companion Proceedings of the 36th
International Conference on Software Engineering.
[20]MilosGligoric,AlexGroce,ChaoqiangZhang,RohanSharma,MohammadAmin
Alipour, and Darko Marinov. 2013. Comparing Non-adequate Test Suites Using
Coverage Criteria. In In Proceedings of the 2013 International Symposium on
Software Testing and Analysis (ISSTA).
[21]Christian Häubl, Christian Wimmer, and Hanspeter Mössenböck. 2013. Deriving
Code Coverage Information from Profiling Data Recorded for a Trace-based
Just-in-timeCompiler.In InProceedingsofthe2013InternationalConferenceon
PrinciplesandPracticesofProgrammingontheJavaPlatform:VirtualMachines,
Languages, and Tools (PPPJ).
[22]HBase. 2012. Integration Tests for HBase.
http://hbase.apache.org/0.94/book/hbase.tests.html. Last accessed: 01/29/2018.
[23]HBase.2018. ApacheHBase. https://hbase.apache.org. Lastaccessed:01/29/2018.
[24]HBase. 2018. Powered By Apache HBase. http://hbase.apache.org/
poweredbyhbase.html. Last accessed: 03/04/2018.
[25]Pinjia He, Jieming Zhu, Shilin He, Jian Li, and Michael R. Lyu. 2017. Towards
Automated Log Parsing for Large-Scale Log Data Analysis. IEEE Transactions on
Dependable and Secure Computing (2017).
[26]Shilin He, Jieming Zhu, Pinjia He, and Michael R. Lyu. 2016. Experience Report:
SystemLogAnalysisforAnomalyDetection.In InProceedingsoftheIEEE27th
International Symposium on Software Reliability Engineering (ISSRE).
[27]Ferenc Horváth, Tamás Gergely, Árpád Beszédes, Dávid Tengeri, Gergő Balogh,
and Tibor Gyimóthy. 2017. Code coverage differences of Java bytecode and
source code instrumentation tools. Software Quality Journal (Dec 2017).
[28]Chen Huo and James Clause. 2016. Interpreting Coverage Information Using
DirectandIndirectCoverage.In Proceedingsofthe2016IEEEInternationalCon-
ference on Software Testing, Verification and Validation (ICST).
[29]IBM. 2017. IBM Rational Test Realtie - Estimating Instrumentation Over-
head.https://www.ibm.com/support/knowledgecenter/en/SSSHUF_8.0.0/com.
ibm.rational.testrt.studio.doc/topics/tsciestimate.htm. Last accessed: 01/29/2018.[30]Laura Inozemtseva and Reid Holmes. 2014. Coverage is Not Strongly Correlated
with Test Suite Effectiveness. In Proceedings of the 36th International Conference
on Software Engineering (ICSE).
[31]Jacoco. 2018. JaCoCo Implementation Design. http://www.jacoco.org/jacoco/
trunk/doc/implementation.html. Last accessed: 01/29/2018.
[32]JaCoCo. 2018. JaCoCo JavaCode Coveragelibrary. http://www.eclemma.org/
jacoco/. Last accessed: 01/29/2018.
[33]Vincent Jiang. 2016. How to change HBase log level on the
fly? https://community.hortonworks.com/content/supportkb/49499/
how-to-change-hbase-log-level-on-the-fly.html. Last accessed: 01/29/2018.
[34]ZhenMingJiang,AhmedE.Hassan,GilbertHamann,andParminderFlora.2008.
AnAutomatedApproachforAbstractingExecutionLogstoExecutionEvents.
Journal of Software Maintaince and Evolution 20, 4 (July 2008).
[35]ZhenMingJiang,AhmedE.Hassan,GilbertHamann,andParminderFlora.2009.
Automated Performance Analysis of LoadTests. In Proceedings of the 25thIEEE
International Conference on Software Maintenance (ICSM).
[36]Paul C. Jorgensen. 2008. Software Testing: A Craftsman’s Approach (3rd ed.).
Auerbach Publications, Boston, MA, USA.
[37]Suhas Kabinna, Cor-Paul Bezemer, Weiyi Shang, and Ahmed E. Hassan. 2016.
Logging Library Migrations: A Case Study for the Apache Software Foundation
Projects.In Proceedingsofthe13thInternationalConferenceonMiningSoftware
Repositories (MSR).
[38]Emre Kiciman and Helen J. Wang. 2007. Live Monitoring: Using Adaptive Instru-
mentationandAnalysistoDebugandMaintainWebApplications.In Proceedings
of the 11th USENIX Workshop on Hot Topics in Operating Systems.
[39]GregorKiczales,JohnLamping,AnuragMendhekar,ChrisMaeda,CristinaLopes,
Jean-Marc Loingtier, and John Irwin. 1997. Aspect-oriented programming.
[40]Gene Kim, Patrick Debois, John Willis, and Jez Humble. 2016. The DevOps Hand-
book:HowtoCreateWorld-ClassAgility,Reliability,andSecurityinTechnology
Organizations.
[41]PavneetSinghKochhar,DavidLo,JuliaLawall,andNachiappanNagappan.[n.
d.]. PCode Coverage and Postrelease Defects: A Large-Scale Study on Open
Source Projects. IEEE Transactions on Reliability ([n. d.]).
[42]Pavneet Singh Kochhar, Ferdian Thung, and David Lo. 2015. Code coverage
and test suite effectiveness: Empirical study with real bugs in large systems.
InProceedings of the 22nd IEEE International Conference on Software Analysis,
Evolution, and Reengineering (SANER).
[43]Pavneet Singh Kochhar, Ferdian Thungand, David Lo, and Julia Lawall. 2014. An
Empirical Study on the Adequacy of Testing in Open Source Projects. In 2014
21st Asia-Pacific Software Engineering Conference (APSEC).
[44]Qingwei Lin, Jian-Guang Lou, Hongyu Zhang, and Dongmei Zhang. 2016. iDice:
ProblemIdentificationforEmergingIssues.In Proceedingsofthe38thInternational
Conference on Software Engineering (ICSE).
[45]Log4J. 2012. LOG4J A logging library for Java. http://logging.apache.org/log4j/1.
2/. Last accessed: 07/17/2018.
[46]Leonardo Mariani, Dan Hao, Rajesh Subramanyan, and Hong Zhu. 2017. The
centralroleoftestautomationinsoftwarequalityassurance. SoftwareQuality
Journal(2017).
[47]Google Zúrich Marko Ivanković. 2014. Measuring Coverage at Google. https:
//testing.googleblog.com/2014/07/measuring-coverage-at-google.html . Last
accessed: 01/29/2018.
[48]Scott Matteson. 2018. https://www.techrepublic.com/article/
report-software-failure-caused-1-7-trillion-in-financial-losses-in-2017/.
[49]Microsoft.2016. MicrosoftVisualStudio-UsingCodeCoveragetoDetermine
How Much Code is being Tested. https://docs.microsoft.com/en-us/visualstudio/
test/using-code-coverage-to-determine-how-much-code-is-being-tested . Last
accessed: 01/29/2018.
[50]KannanMuthukkaruppan.2010.TheUnderlyingTechnologyofMessages. https://
www.facebook.com/note.php?note_id=454991608919. Last accessed: 03/04/2018.
[51]Hoan Anh Nguyen, Tung Thanh Nguyen, Tung Thanh Nguyen, and Hung Viet
Nguyen. 2017. Interaction-Based Tracking of Program Entities for Test CaseEvolution. In 2017 IEEE International Conference on Software Maintenance and
Evolution (ICSME).
[52]Adam Oliner, Archana Ganapathi, and Wei Xu. 2012. Advances and Challenges
in Log Analysis. Communications of ACM (2012).
[53]Alessandro Orso and Gregg Rothermel. 2014. Software Testing: A Research
Travelogue(2000–2014).In ProceedingsoftheonFutureofSoftwareEngineering
(FOSE).
[54]Alan Page and Ken Johnston. 2008. How We Test Software at Microsoft. Microsoft
Press.
[55]Antonio Pecchia, Marcello Cinque, Gabriella Carrozza, and Domenico Cotroneo.
2015. IndustryPracticesandEventLogging:AssessmentofaCriticalSoftwareDe-
velopmentProcess.In CompanionProceedingsofthe37thInternationalConference
on Software Engineering.
[56]Python. 2018. The AST module in Python standard library. https://docs.python.
org/2/library/ast.html. Last accessed: 04/02/2018.
[57]Ajitha Rajan, Michael Whalen, and Mats Heimdahl. 2008. The effect of program
andmodelstructureonmc/dctestadequacycoverage.In InProceedingsofthe
315
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:29 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Boyuan Chen, Jian Song, Peng Xu, Xing Hu, and Zhen Ming (Jack) Jiang
ACM/IEEE 30th International Conference on Software Engineering (ICSE).
[58]Alberto Savoia. 2010. Code coverage goal: 80% and no less! https://testing.
googleblog.com/2010/07/code-coverage-goal-80-and-no-less.html. Last ac-
cessed: 01/29/2018.
[59]David Schuler and Andreas Zeller. 2011. Assessing Oracle Quality with Checked
Coverage.In InProceedingsoftheFourthIEEEInternationalConferenceonSoftware
Testing, Verification and Validation (ICST).
[60]Weiyi Shang, Zhen Ming Jiang, Bram Adams, Ahmed E. Hassan, Michael W.
Godfrey, Mohamed Nasser, and Parminder Flora. 2014. An exploratory study of
theevolutionofcommunicatedinformationabouttheexecutionoflargesoftware
systems. Journal of Software: Evolution and Process (2014).
[61]WeiyiShang,ZhenMing(Jack)Jiang,HadiHemmati,BramAdams,AhmedE.
Hassan,andPatrickMartin.2013. AssistingDevelopersofBigDataAnalytics
Applications when Deploying on Hadoop Clouds. In Proceedings of the 2013
International Conference on Software Engineering (ICSE).
[62]Benjamin H. Sigelman, Luiz Andre Barroso, Mike Burrows, Pat Stephenson,
ManojPlakal,DonaldBeaver,SaulJaspan,andChandanShanbhag.2010. Dapper,
aLarge-ScaleDistributedSystemsTracingInfrastructure.TechnicalReport.Google,
Inc.https://research.google.com/archive/papers/dapper-2010-1.pdf
[63]MarkD.Syer,WeiyiShang,ZhenMingJiang,andAhmedE.Hassan.2017. Contin-
uous validation of performance test workloads. Automated Software Engineering
(2017).
[64]MustafaM.TikirandJeffreyK.Hollingsworth.2002. EfficientInstrumentation
forCodeCoverageTesting.In Proceedingsofthe2002ACMSIGSOFTInternational
Symposium on Software Testing and Analysis (ISSTA).
[65]Tumblr. 2018. Tumblr Architecture - 15 Billion Page Views A Month And
Harder To Scale Than Twitter. http://highscalability.com/blog/2012/2/13/
tumblr-architecture-15-billion-page-views-a-month-and-harder.html. Last
accessed: 03/04/2018.
[66]QianqianWang,YuriyBrun,andAlessandroOrso.2017. BehavioralExecutionComparison:AreTestsRepresentativeofFieldBehavior?.In 2017IEEEInterna-
tional Conference on Software Testing, Verification and Validation (ICST).[67]MichaelWhalen,GregoryGay,DongjiangYou,MatsP.E.Heimdahl,andMatt
Staats. 2013. Observable Modified Condition/Decision Coverage. In Proceedings
of the 2013 International Conference on Software Engineering (ICSE).
[68]MichaelWürsch,EmanuelGiger,andHaraldC.Gall.2013. EvaluatingaQuery
Framework for Software Evolution Data. ACM Transactions on Software Engi-
neering and Methodology (TOSEM) 22, 4 (October 2013).
[69]TaoXie,NikolaiTillmann,andPratapLakshman.2016. AdvancesinUnitTest-
ing:TheoryandPractice.In Proceedingsofthe38thInternationalConferenceon
Software Engineering Companion.
[70]Yichen Xie and Alex Aiken. 2007. Saturn: A Scalable Framework for Error
Detection Using Boolean Satisfiability. ACM Trans. Program. Lang. Syst. (May
2007).
[71]XWiki.2018. XWikiDevelopmentZone-Testing. http://dev.xwiki.org/xwiki/
bin/view/Community/Testing. Last accessed: 03/04/2018.
[72]Yahoo. 2018. Yahoo! Cloud Serving Benchmark. https://github.com/
brianfrankcooper/YCSB/. Last accessed: 01/29/2018.
[73]DingYuan,SoyeonPark,PengHuang,YangLiu,MichaelM.Lee,XiaomingTang,
YuanyuanZhou,andStefanSavage.2012. BeConservative:EnhancingFailure
Diagnosis with Proactive Logging. In Proceedings of the 10th USENIX Conference
on Operating Systems Design and Implementation (OSDI).
[74]DingYuan,SoyeonPark,andYuanyuanZhou.2012. CharacterizingLoggingPrac-
ticesinOpen-sourceSoftware.In Proceedingsofthe34thInternationalConference
on Software Engineering (ICSE).
[75]Ding Yuan,Jing Zheng, SoyeonPark, YuanyuanZhou, and StefanSavage. 2011.
Improving Software Diagnosability via Log Enhancement. In Proceedings of
theSixteenth InternationalConferenceonArchitectural SupportforProgramming
Languages and Operating Systems (ASPLOS).
[76]Xu Zhao, Kirk Rodrigues, Yu Luo, Michael Stumm, Ding Yuan, and Yuanyuan
Zhou.2017. Log20:FullyAutomatedOptimalPlacementofLogPrintingState-
mentsUnderSpecifiedOverheadThreshold.In Proceedingsofthe26thSymposium
on Operating Systems Principles.
[77]Jieming Zhu, Pinjia He, Qiang Fu, Hongyu Zhang, Michael R. Lyu, and Dongmei
Zhang.2015. LearningtoLog:HelpingDevelopersMakeInformedLoggingDeci-
sions.InProceedingsofthe37thInternationalConferenceonSoftwareEngineering.
[78]Thomas Zimmermann, Rahul Premraj, and Andreas Zeller. 2007. Predicting
DefectsforEclipse.In ProceedingsoftheThirdInternationalWorkshoponPredictor
Models in Software Engineering.
316
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:29 UTC from IEEE Xplore.  Restrictions apply. 