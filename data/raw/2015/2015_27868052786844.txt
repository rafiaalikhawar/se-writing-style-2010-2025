Turning Programs against Each Other: High Coverage
Fuzz-Testing using Binary-Code Mutation and Dynamic
Slicing
Ulf Kargén Nahid Shahmehri
Department of Computer and Information Science
Linköping University
SE-58183 Linköping, Sweden
{ulf.kargen, nahid.shahmehri}@liu.se
ABSTRACT
Mutation-based fuzzing is a popular and widely employed
black-box testing technique for ﬁnding security and robust-
ness bugs in software. It owes much of its success to its
simplicity; a well-formed seed input is mutated, e.g. through
random bit-ﬂipping, to produce test inputs. While reduc-
ing the need for human eﬀort, and enabling security testing
even of closed-source programs with undocumented input
formats, the simplicity of mutation-based fuzzing comes at
the cost of poor code coverage. Often millions of iterations
are needed, and the results are highly dependent on conﬁg-
uration parameters and the choice of seed inputs.
In this paper we propose a novel method for automated
generation of high-coverage test cases for robustness test-
ing. Our method is based on the observation that, even for
closed-source programs with proprietary input formats, an
implementation that can generate well-formed inputs to the
program is typically available. By systematically mutating
theprogram code of suchgenerating programs , we leverage
information about the input format encoded in the generat-
ingprogramtoproducehigh-coveragetestinputs, capableof
reaching deep states in the program under test. Our method
works entirely at the machine-code level, enabling use-cases
similar to traditional black-box fuzzing.
We have implemented the method in our tool MutaGen ,
and evaluated it on 7 popular Linux programs. We found
that, formostprograms, ourmethodimprovescodecoverage
by one order of magnitude or more, compared to two well-
known mutation-based fuzzers. We also found a total of 8
unique bugs.
Categories and Subject Descriptors
D.2.5 [Software Engineering ]: Testing and debugging—
Testing toolsGeneral Terms
Security
Keywords
Fuzz testing, fuzzing, black-box, dynamic slicing, program
mutation
1. INTRODUCTION
Memory safety errors, such as buﬀer overﬂows, are often
the result of subtle programming mistakes, but can expose
users to highly critical security risks. By supplying vulner-
able software with speciﬁcally crafted inputs, attackers can
often leverage these kinds of bugs to corrupt important data
structures and execute arbitrary code with the privileges of
the vulnerable program. Such attacks may lead to a com-
plete compromise of the system on which the program is
running.
Security bugs commonly stem from programs failing to
appropriately handle rare corner cases during the process-
ing of inputs. Traditional testing methods using manually
written test cases, such as unit testing, are typically aimed
at testing functional correctness of software under “normal”
usage situations, and have therefore often proven insuﬃcient
to uncover critical security bugs. Intuitively, a security bug
is often introduced as a consequence of a particularly un-
clear or complex part of the program speciﬁcation. If test
cases are based on the same speciﬁcation, chances are high
that the test writer has also failed to account for all ob-
scure corner cases. In that perspective, automated test case
generation, not biased by a speciﬁcation or the developers
interpretation thereof, is an attractive alternative for secu-
rity testing. In this paper we consider fuzz testing , a practi-
cal and popular black-box technique for automatic test case
generation.
The term fuzz testing, or simply fuzzing, was ﬁrst coined
byB.P.Millerin1990afternoticingthat, whileworkingover
a dial-up connection to a mainframe, random bit-errors in
program inputs, introduced by interference from a thunder-
storm, would often cause common UNIX utilities to crash.
The subsequent study [20] showed that supplying random
inputs to programs was a surprisingly eﬀective means to dis-
cover robustness errors. More recently, fuzzing has gained
widespread popularity as a security testing method. Many
large software vendors, such as Adobe [7], Google [12], and
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a
fee. Request permissions from Permissions@acm.org.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ESEC/FSE’15 , August 30 – September 4, 2015, Bergamo, Italy
ACM. 978-1-4503-3675-8/15/08...$15.00
http://dx.doi.org/10.1145/2786805.2786844
782
Microsoft [5] employ fuzzing as part of their quality assur-
ance processes.
Most fuzz testing tools can be broadly divided into us-
ing either a mutation-based or ageneration-based input-
generationstrategy[30]. Theformerentailsperformingsome
random transformations to an initial well-formed input, cal-
ledaseed,whilethegeneration-basedapproachusesaformal
speciﬁcation of the input format, e.g. a grammar, to gener-
ate inputs. While simple and easy to get started, mutation-
based methods almost invariably yield lower code coverage
than generation-based approaches. Mutated inputs often
deviate too much from the format expected by the program,
causing them to be rejected early in processing. However, as
many input formats are highly complex, creating a formal
speciﬁcation for a generation-based fuzzer is often a very
time-consuming and error-prone task. Furthermore, it is
diﬃcult to create tools and input-speciﬁcation formalisms
that can cater to all possible kinds of input formats. For
that reason, security analysts are often forced to write their
own target-speciﬁc fuzzers.
Another case where a security analyst may be conﬁned to
usingamutation-basedapproachiswhentestingthird-party
closed-source programs, as it is not uncommon for such pro-
grams to use proprietary or undocumented input formats.
Analyzingclosed-sourceprogramsforsecurityﬂawsisacom-
mon (and proﬁtable) endeavor in practice, as third-party
independent security testing has become an important part
in many software vendors’ strategy to rid their programs of
security bugs [13]. Several large companies, such as Google
[1] and Microsoft [4], are e.g. oﬀering substantial monetary
awards for security bugs found in their products.
In this paper, we propose a novel method for completely
automaticgenerationofhigh-coveragetestinputsforrobust-
ness testing. Our method works directly at the machine-
code level, and thus allows use cases where the analyst does
nothaveaccesstosourcecodeordocumentationoftheinput
format. The method is based on the observation that, even
in cases of proprietary input formats, the tester often has
access to an implementation that can generate well-formed
input to the program under test. Consider, for example, a
hypothetical spreadsheet editor. Even if the editor is closed-
source, and uses a proprietary format for spreadsheets, it
can most likely store spreadsheets in a format that it can
read itself. In a sense, the speciﬁcation of the input format
isimplicitly encoded in the machine code of the program’s
output module.
On a high level, our method works by systematically in-
troducingmutations(essentiallybugs)intoa generating pro-
gram, and using the outputs from that mutated program as
inputs to the program under test. Instead of simply mu-
tating every instruction in the generating program, which
would be impossible due to the undecidability of static dis-
assembly, or using the naive approach of mutating every
instruction observed to execute during one run, we use dy-
namic slicing to decide which instructions to mutate. This
approach signiﬁcantly reduces the number of necessary mu-
tations (by 90%–95% in our experiments), with a corre-
sponding reduction in time spent on executing mutants.
Our main hypothesis is:
By performing mutations on the generating program’s ma-
chine code instead of directly on a well-formed input, the
resulting test inputs are closer to the format expected by the
program under test, and thus yield better code coverage.
Dispatcher &  
Assessor  Fuzzer  Found bugs,  
Coverage data  
0101
1101
1011 
Target program  Well-formed 
input  Mutated inputs  Figure 1: Typical mutation-based fuzzing pipeline.
We have implemented the proposed method in our pro-
totype tool called MutaGen . We evaluated the method on
several popular Linux programs, and found that, for most
programs, it improved code coverage by one order of magni-
tude or more, compared to two well-known mutation-based
fuzzers. In the 7 programs used in our tests, we found a
total of 8 unique bugs.
2. BACKGROUND AND MOTIVATION
Typically, a testing campaign using mutation-based fuz-
zing proceeds as follows. First, a set of seed inputs of the ap-
propriatetypeiscollected. Forexample, iftheobjectiveisto
testaPDFreader,asetofwell-formedPDFﬁlesiscompiled,
eithermanuallyore.g.usinganautomatedwebsearch. Each
seed is then passed to the fuzzer, which performs mutations
on well-formed inputs. Mutations can range from simply
ﬂipping bits at random, to more complex transformations,
such as deleting, moving, or duplicating data elements. The
fuzzingdispatcher is responsible for launching the program
under test and supplying it with mutated inputs from the
fuzzer. An assessor component is responsible for determin-
ingiftheprogramhandledthemalformedinputinagraceful
way, or if it suﬀered a fault. When ﬁnding security bugs is
the objective, a simple but eﬀective approach is to simply
observe whether the program under test crashes, as danger-
ous memory-corruption bugs typically manifest themselves
as e.g. segmentation faults during fuzz testing. Since several
diﬀerent mutated inputs often trigger the same bug during a
fuzzing campaign, the assessor commonly also has a means
of estimating the uniqueness of found bugs. The industry-
standard approach, used by most existing fuzzing tools, is to
calculate a hash digest of the stack trace at the point of fail-
ure. The outcome of the fuzzing campaign is a set of crash-
ers, mutated inputs that were observed to trigger unique
faults. Figure 1 shows the testing pipeline for a mutation-
based fuzzer.
As mutation-based fuzzing usually yields poor code cov-
erage, the general view among security experts is that sev-
eral million iterations are typically needed to ﬁnd bugs in
practice [24, 22]. Further, while seemingly simple to set
up and get started, mutation-based fuzzing is highly depen-
dent on the quality of the seed inputs [28], as well as various
conﬁguration parameters, such as the fraction of input bits
that are mutated [16]. The traditional wisdom has therefore
been that a great deal of tweaking, based on human intu-
ition, is needed to achieve good results with fuzz-testing.
Our method, by contrast, strives to eliminate the need for
human eﬀort as much as possible by leveraging information
about the input format encoded in the generating program.
Toillustratethepotentialmeritsofourapproach, consider
for example a highly structured format, e.g. one based on
XML. Blindly mutating a well-formed input is extremely
783likely to break the structure of the input, causing it to be
rejected early in processing by the program under test. By
contrast, our method would cause the input mutation to
be applied already on the internal, binary representation of
the data, used by the generating program. By performing
mutations before the ﬁnal output encoding is applied, we
hypothesize that our method is much better at maintaining
the structure of the inputs, and thus reaching deeper states
in the program under test.
3. DESIGN AND IMPLEMENTATION
Figure 2 shows a high-level overview of the MutaGen pro-
cess. The dynamic slicer ﬁrst executes the generating pro-
gram on a given input, and computes a cumulative dynamic
slice– a set containing instructions that contributed to com-
puting at least one byte of the generator’s output. That set
is then fed to the mutator, which again executes the generat-
ing program, this time introducing mutations to the instruc-
tions contained in the slice. For each execution, one speciﬁc
mutation (from a ﬁnite set of mutation operators) is intro-
duced at one instruction, until all applicable mutations have
been applied to each instruction in the slice. The resulting
set of outputs from the generator, one per execution, is used
as the set of test inputs to the program under test. The ﬁnal
stage is similar to that of a classical mutation-based fuzzer,
using a dispatcher and assessor to run the program under
test and detect failures. While our current implementation
only considers input/output from ﬁles, it could also be ex-
tended to e.g. handle communication over network sockets.
Rather than ﬁnding all instructions that somehow inﬂu-
enced output, the purpose of our dynamic slicing is to com-
puteaconservativesubset ofoutput-inﬂuencinginstructions.
More speciﬁcally, we only consider instructions that the out-
put is directly data-dependent upon, and not implicit de-
pendencies from indirect memory accesses or control depen-
dence. The reasons for this are twofold. The ﬁrst reason is
that, in order to maintain output structure, we want to limit
mutations to the output generation of the program, while
keeping the program semantics largely the same. There-
fore, we want to avoid performing mutations that e.g. sig-
niﬁcantly alter the control ﬂow of generating programs. The
second reason is that considering implicit dependencies al-
most invariably leads to overly-large slices. Previous work
[29] has e.g. shown that considering implicit dependencies
from indirect memory accesses in binary programs leads to
many spurious transitive dependencies. Overly-large slices
not only lead to more time spent on input-generation, but
can also be detrimental to the overall stability of the input-
generation process. If the selection of target instructions
is too wide, chances are high that the generating program
will at some point corrupt its environment, e.g. by overwrit-
ing an important conﬁguration ﬁle, thus preventing future
invocations of the program from running properly. Using a
narrowerselectionoftarget instructions formutation largely
eliminates the need for isolation between mutant executions
(e.g. by snapshotting and restoring the system), and is thus
a key factor in reducing the runtime cost of our method.
The following subsections describe the MutaGen compo-
nents in more detail.
3.1 Dynamic Slicing
We base our dynamic slicer on a system we developed
in previous work [18]. In order to make the paper self-
Dispatcher &  
Assessor  Mutator  Dynamic slicer  Found 
bugs,  
Coverage 
data 
0101
1101
1011 Generating 
program  0101
1101
1011 
Target program  Generator 
input  
Well-formed 
input  Slice  
Mutated inputs  Figure 2: The MutaGen pipeline for test generation
and execution.
contained, we brieﬂy describe the relevant details of that
system here.
The dynamic slicer is implemented using the Pin [19]
dynamic binary instrumentation framework, and works di-
rectly on Linux x86 executable programs. First, the system
records a dynamic data dependence graph (dyDDG), rep-
resenting the entire data ﬂow during one execution. That
graph is then traversed backwards from a point of interest
to compute a slice.
During execution, every instruction instance is assigned a
uniqueID,derivedfromitsaddressandaper-basic-blockex-
ecution count. These instruction instances eﬀectively form
nodes in the dyDDG. In order to record the data-ﬂow of
programs, a shadow state is maintained during execution,
mirroring every register and memory location with a cor-
responding shadow register and shadow-memory location.
The shadows are dynamically updated during runtime to al-
ways contain the ID of the instruction instance (i.e. dyDDG
node) that last deﬁned the corresponding real register or
memory location. Each time an instruction is executed, the
shadows of its input operands are inspected, so that edges
to the preceding data-ﬂow nodes can be recorded. Note
that our system considers all executed instructions, includ-
ing those in e.g. shared libraries.
In order to scale to realistic executions, the system uses
secondary storage to record dyDDGs. Since dyDDGs are of-
ten too large to ﬁt in RAM, this approach is necessary, but
requirescarefuldesignofbothalgorithmsandthegraphrep-
resentation, in order to maintain good performance despite
high access-time delays.
In addition to its core function, we have modiﬁed the sys-
tem to monitor all ﬁle-related system calls during execution
of the generating program. Whenever the program writes a
byte from memory to a ﬁle, the system makes a record of
the node in the dyDDG that the written data corresponds
to. This way, after the generating program has ﬁnished, we
can compute the cumulative dynamic slice by simply ﬁnd-
ing all nodes that are reachable from any of the nodes cor-
responding to an output-byte. We perform a simple depth-
ﬁrst search, using the spanning-tree algorithm, to traverse
the dyDDG and compute the slice. Thus, we don’t need to
re-visit already traversed nodes, ensuring good performance
even for large output ﬁles.
One of the challenges with dynamic data-ﬂow analysis of
binaries is the lack of type information. When implement-
ing similar analyses for high-level programs, one can use the
program symbol table to allocate one shadow-variable for
each variable in the program, ensuring accurate and pre-
cise data-ﬂow tracking. In a binary program, however, the
analysis engine only sees memory and register accesses. The
most conservative approach would be to track dependencies
784on the byte-level, i.e. allocating one shadow-byte for every
byte of memory. However, this approach would be highly
wasteful in terms of time and space overhead, since the vast
majority of memory accesses use the native word size of the
CPU(e.g.4bytesfortheIA32/x86architecture). Therefore,
the slicer uses shadow memory with a minimum granular-
ity of the native word size, a common approximation used
by e.g. the popular dynamic-analysis tool Memcheck [25].
This approach may lead to lost data dependencies in case
of sub-word writes, since an entry in shadow memory will
always point to the instruction that last wrote any byte of
the corresponding memory word. While such problems are
rare in general, due to the scarcity of sub-word writes, the
approximation can sometimes be problematic in the output-
tracking stage of our analysis, since many programs emit
single bytes as part of the ﬁnal output encoding. To avoid
having to track dependencies on the byte level, which would
eﬀectively quadruple the time and space overhead of slic-
ing, we instead use the following method: When a sub-word
write is observed during runtime, a virtualdependence edge
to the previous deﬁnition of the full word is recorded, along
with edges for the regular input operands. This way, no
dependencies are ever lost, at the cost of many false de-
pendencies. To compensate for false dependencies, we also
modify the dyDDG traversal algorithm. If a virtual depen-
dence edge is encountered during traversal, we only follow
that edge if the corresponding deﬁning instruction (i.e. the
target of the edge) performs a sub-word memory write. This
way, we prune the vast majority of spurious dependencies
arising from re-used memory on e.g. the stack, while still
accurately accounting for e.g. writes into byte-arrays. The
remaining spurious dependencies may lead to slightly over-
approximated slices, but does not seem to negatively aﬀect
our test-generation in practice.
3.2 Program Mutation
Programmutationhasbeenstudiedextensivelywithinthe
context of mutation testing [17]. The objective of mutation
testing is often to determine the adequacy of test suites.
To assess the test suite of a program, the tests are run on
mutated variants of the program, referred to as mutants.
By counting the number of mutants that were killed by the
test suite (i.e. failed at least one test), the overall quality
of the test suite can be quantiﬁed, and missing test cases
can be identiﬁed. A mutant is created by applying a mu-
tation operator to one or more statements in a program. A
mutation operator applies a small syntactic change to the
aﬀected statement.
We have implemented our mutation component using Val-
grind [26]. Valgrind performs dynamic binary instrumenta-
tionbyﬁrstconvertingnativeinstructionsintotheRISC-like
VEX intermediate representation (IR). All instrumentation
and analysis is performed on the VEX IR, which is transpar-
ently re-compiled to native code and executed by Valgrind,
using a just-in-time compiler. We apply our mutations on
the VEX IR, which avoids having to deal with the highly
complex x86 instruction set, comprising more than 1000 dif-
ferent opcodes.
The code translation units used by Valgrind are single-
entry-multiple-exit superblocks of VEX code. Complex in-
structionsarebrokendownintoseveralprimitiveVEXstate-
ments, and intermediate results are stored in single-static-
assignment virtual registers called temporaries , which are lo-1: t0 = GET :I32 (4)
2: t1 = GET :I32 (8)
3: t2 = Shl32 (t1 ,0 x2:I8)
4: t3 = Add32 (t0 ,t2)
5: t4 = LDle :I32 (t3)
6: PUT (0) = t4
(a) Original VEX IR
1: t0 = GET :I32 (4)
2: t1 = GET :I32 (8)
3:t5= Shl32 (t1 ,0 x2:I8)
4: t2 = Sub32(t5,0x1:I32)
5: t3 = Add32 (t0 ,t2)
6: t4 = LDle :I32 (t3)
7: PUT (0) = t4
(b) VEX IR after applying mutation 2 to statement 3
Figure 3: Example of VEX IR for the x86 instruc-
tion mov eax,[ecx+edx*4]
cal to one superblock. As an example, we consider the x86
instruction mov eax,[ecx+edx*4] , which calculates an ad-
dress from registers ecxandedx, fetches a 32-bit word from
that address, and writes the word to register eax. The cor-
responding translation into VEX is shown in ﬁgure 3a. The
ﬁrst two VEX statements fetch the contents of ecxand edx
intotemporaries t0andt1,respectively. Thenexttwostate-
ments perform the address calculation, statement 5 reads a
word from memory, and the ﬁnal statement puts that value
into eax.
Since mutation testing often aims to assess a test suite’s
ability to catch errors in the program logic, it is common to
use mutation operators that replace arithmetic, relational,
or logic operators, or the absolute values of constants. Re-
call, however, that in our setting the objective is to mutate
the output generation of programs, while keeping program
semantics mostly unchanged. Hence, most of our muta-
tion operators perform small arithmetic changes to compu-
tations. We only mutate VEX statements that produce an
intermediate result stored in a temporary. Mutating other
statements, such as statement 6 in ﬁgure 3a, is redundant,
since such statements only serve to propagate the result of
a computation to a register or memory. Our 17 diﬀerent
mutation operators are summarized in table 1.
Mutations 1–4 perform simple arithmetic changes to the
result of a computation, while mutations 7–10 and 11–14
perform the same changes to respectively the ﬁrst and sec-
ond operands of a statement, where applicable. Note that,
since temporaries are single-static-assignment, we allocate
a new temporary for the mutated input operand in these
cases, ensuring that the mutation only aﬀects the target
statement. Figure 3b shows an example of applying muta-
tion 2 (subtract 1 from destination) to statement 3 in the
original VEX code.
Mutations 5 and 6 are special cases that are used instead
of mutations 1–4 when the destination temporary is later
directly used as an address. Statement 4 in ﬁgure 3a is an
example of such a case. The rationale for treating these
statements diﬀerently is that a multiplicative change of an
785Table 1: Mutation operators used by MutaGen.
Mutation Description
1 Add 1 to destination
2 Subtract 1 from destination
3 Multiply destination by 2
4 Divide destination by 2
5 Add 4 to destination
6 Subtract 4 from destination
7 Add 1 to ﬁrst operand
8 Subtract 1 from ﬁrst operand
9 Multiply ﬁrst operand by 2
10 Divide ﬁrst operand by 2
11 Add 1 to second operand
12 Subtract 1 from second operand
13 Multiply second operand by 2
14 Divide second operand by 2
15 Switch and/or
16 Switch add/sub
17 Switch signedness
addressisextremelyunlikelytoresultinanewvalidaddress;
the program would most likely only crash as a result of such
amutation. Further, itismuchmorelikelythatthemodiﬁed
address will point to a valid data object, e.g. an adjacent
element of an array, if the native word size (4 bytes in x86)
is used in the addition/subtraction, instead of 1.
Mutation 15 switches the bitwise And/Or operators for
one another, where applicable, and mutation 16 equivalently
switches the Add/Subtract operators. Finally, mutation 17
attempts to emulate a common programmer error where a
signeddatatypeisusedinsteadofanunsigned, orviceversa.
For operations that exist in both signed and unsigned vari-
ants, mutation 17 changes the signedness of the operation.
We systematically apply all applicable mutation opera-
tors to each VEX statement that is part of a translated x86
instruction in the dynamic slice. Using the terminology of
mutation testing, we only perform ﬁrst-order mutations, i.e.
we only apply one mutation operator to one VEX state-
ment per execution. Special-purpose instructions, such as
ﬂoating-point or SIMD instructions, are ignored in our pro-
totype implementation.
One of the challenges of mutation testing is to avoid equiv-
alent mutants , i.e. mutants that are syntactically diﬀerent
but semantically equivalent. In this work, we redeﬁne the
term to mean mutants that produce identical output given
a speciﬁc input , albeit being possibly semantically diﬀer-
ent. Equivalent mutants does not adversely aﬀect the out-
come of testing, but introduce unnecessary time overhead
in the test generation. We perform basic data-ﬂow analysis
within superblocks to prune obvious cases that would lead
to equivalent mutants. Performing mutation 3 on statement
1 in ﬁgure 3a is e.g. redundant if we know that t0is only
used in statement 4, where mutation 9 is going to be applied
anyway. Similarly, applying mutation 7 to statement 4 is re-
dundant if mutation 1 has already been applied in an earlier
execution. These optimizations reduce the number of equiv-
alent mutants by roughly 30%. During test generation, we
compute a hash of each output and keep only unique ﬁles.3.3 Test Dispatching
The test dispatcher runs the program under test on each
mutated ﬁle. To avoid missing crashes in cases where the
program has registered handlers for certain signals, we per-
form simple instrumentation of the program to catch inter-
esting signals (segmentation fault, illegal instruction, etc.)
before they are sent to a signal handler. When a crash is de-
tected, we make note of the input ﬁle that caused the crash,
and, similarly to other fuzzing platforms, compute a hash
digest of the stack trace at the time of failure, to distinguish
between unique crashes.
4. EVALUATION
Similar to mutation-based fuzzers, our approach relies on
simple syntactic mutations, without explicitly incorporating
semantic information as in generation-based fuzzing. There-
fore, an important question is what beneﬁt our approach of-
fersovermutation-basedfuzzing. Iftherearenobeneﬁts, we
would have a hard time justifying the increased complexity
of our method to developers and security analysts.
Another important question is how eﬀective our mutation
operators are. Since, to the best of our knowledge, program
mutation has not been applied in this context before, in-
vestigating the eﬀectiveness and adequacy of the mutation
operators is important, as it will help identify directions for
future improvement of our method.
In the following section, we address these two questions,
and present a preliminary evaluation of the overall perfor-
mance and eﬀectiveness of MutaGen.
4.1 Methodology
In our evaluation we used 6 diﬀerent generating programs
and 7 diﬀerent ﬁle types. For each ﬁle type we generated
two sets of mutated inputs, which were used to test a to-
tal of 7 popular Linux programs. All experiments were run
on VirtualBox virtual machines with one virtual CPU-core
per machine. All virtual machines were running Linux Mint
17.1. We used two Intel Xeon E3-1245 workstations at 3.3
GHz as hosts, each host running four concurrent virtual ma-
chines (one per host core).
All programs (generators, programs under test, or both)
used in our experiments are listed in table 2. For the open-
source programs, we used the latest versions supplied by
the Linux Mint package manager. The latest version of the
closed-source program nconvert was downloaded from the
vendor’s website. For our fuzzing targets, we have focused
on programs that operate on popular document, image, or
media types. Since such programs often take inputs origi-
nating from untrusted sources, such as the internet, they are
popular targets for attackers. Fuzzing is therefore often ap-
plied as part of the quality-assurance process for these kinds
of programs.
Table 3 shows the selection of generating programs. With
the exception of avconv, we used two distinct inputs for
each test generation run. For avconv, we used one input to
generate mp3 and aac ﬁles using both constant and variable
bit-rate encodings. All test sets were created by mutating
the conversion of a sample ﬁle (column 2), using the con-
ﬁgurations shown in column 3. The names of the resulting
sets of test inputs are shown in column 4.
For each generator conﬁguration we also performed one
run without mutations, to produce a well-formed input. We
786Table 2: Programs used in the experiments.
Program Version Description
avconv 9.16-6Audio and video encoder/decoder.
Open source.
convert 6.7.7-10Commandline interface to the
ImageMagick image editor/converter.
Open source.
nconvert 6.17Commandline interface to the XnView
image editor/converter. Closed source.
pdftocairo 0.24.5PDF conversion utility using poppler.
Open source.
mudraw 1.3-2PDF conversion utility based on
mupdf. Open source.
pdftops 0.24.5PDF to postscript conversion utility
using poppler. Open source.
ps2pdf 9.10Postscript to PDF conversion utility
using ghostscript. Open source.
inkscape 0.48.4SVG vector graphics editor. Open
source.
refer to that input here as the base case of each generated
test set.
Our selection of programs is limited in part by the fact
that many popular Linux programs share the same code
base. For example, there exist a plethora of PDF utilities
and viewers for Linux, but the vast majority are based on
oneof xpdf,poppler (aforkof xpdf),mupdf,orghostscript .
Two programs that use the same library or code base are
likely to crash on the same malformed input. To avoid po-
tentially skewing the results of our testing, we have striven
to choose only one testing target from each “family” of pro-
grams.
Table 4 shows the combinations of programs under test
and input data sets used in the test runs. These combina-
tions are henceforth referred to as test conﬁgurations . We
measured instruction coverage for each test execution us-
ing a simple tool implemented with Pin. Since the coverage
tool incurs a signiﬁcant overhead, we also performed all runs
without any instrumentation to get accurate time measure-
ments.
Toevaluatehowourapproachcomparestoexistingfuzzing
tools, wecomparecodecoveragewithtwopopularmutation-
based fuzzers; zzuf[6] and radamsa [2].zzufperforms sim-
plerandombit-ﬂippingoninputs,while radamsa usesseveral
inputtransformationsknowntohavefoundbugsinthepast,
such as duplicating or moving data elements, incrementing
bytes, removing bytes, etc. As seeds for the mutation-based
fuzzers we used the base cases of each test set. Default
parameters are used for both fuzzers. For each test con-
ﬁguration we ran the mutation-based fuzzers for as many
iterations as there were ﬁles in the corresponding MutaGen
test set. Note that this approach favors MutaGen, since
the time taken for executing mutants is not accounted for.
Therefore, we also ran zzufand radamsa on all test conﬁg-
urations without coverage instrumentation for 24 hours, to
allow a more fair comparison of the ability to ﬁnd bugs.
4.2 Results
Program mutation . The results of our test-generation
runs are shown in table 3. Column 7 and 8 show the total
number of runs and ﬁles generated, respectively. Roughly
40%–80% of mutant executions results in an output. Col-umn 9 shows the number of unique ﬁles produced for each
conﬁguration. With the exception of inkscape , about 10%
of all mutant executions produce unique outputs. The num-
berofmutationexecutionsfor inkscape issigniﬁcantlylarger
than for the other programs, yet the number of unique ﬁles
is much lower. The reason for this anomaly is unknown.
One possibility is that the slicing step fails to identify “in-
teresting” instructions to mutate for inkscape , e.g. because
the output generation code makes heavy use of implicit de-
pendencies (section 3).
Comparing the number of runs with the dynamic slice size
(column 6) we see that each instruction results in roughly
10 mutations on average.
Beneﬁt of dynamic slicing . Column 5 shows the total
number of instructions executed at least once, for each gen-
erator conﬁguration. The beneﬁt of dynamic slicing is evi-
dent when comparing these ﬁgures with the slice sizes. Per-
forming dynamic slicing reduces the number of instructions
to mutate by between 90% and 95%, yielding a correspond-
ing reduction in test generation time. The time required for
computing slices was largely negligible in comparison to the
time taken for executing mutants. The longest slice compu-
tation ( inkscape ,sample-1.svg ), required 497.8 seconds,
and the shortest ( nconvert ,sample-1.tiff ) 7.6 seconds.
Coverage and found bugs . The outcome of our testing
experiments is summarized in table 4. We report all cover-
age metrics as the relative cumulative increase in instruction
coverage over the base coverage , i.e. the coverage when run-
ning the program under test on the base case of the test set.
Note that we present coverage relative to the baseline only,
as there is no clear deﬁnition of an absolute coverage ratio
when testing binary programs that are dynamically linked.
With the exception of nconvert andinkscape , the cover-
age increase of MutaGen is typically one order of magnitude
ormoreoverthatof zzufandradamsa.pdftops particularly
stands out with a coverage increase of over 150%. inkscape
showsamoremoderate2.5xincreaseoverthebestmutation-
based fuzzer, probably largely due to the low number of test
inputs. The results also show that the coverage increase of
the mutation-based fuzzers was much larger for nconvert
than for the other programs. Upon closer examination, we
found that the ﬁrst iteration achieved a coverage increase
of about 45%, and that for the remaining iterations the in-
crease was negligible. We speculate that this may be due
to some particularly complex error-management code being
run when certain malformed inputs are encountered.
Column 6 shows the time required to run tests without
coverage instrumentation. Due to the small number of test
cases, no test conﬁguration requires more than 30 minutes
to complete.
As mentioned in section 4.1, measuring coverage of the
mutation-based fuzzers for only as many iterations as there
areﬁlesinthecorrespondingMutaGentestsetmaynotoﬀer
a fair comparison. Figure 4 shows the coverage of zzufand
radamsa versus iteration for 10 test conﬁgurations. (Due to
space constraints we only show one plot per program and
ﬁle type, but all conﬁgurations show similar results.) The
“X” in the plots marks the last iteration when any coverage
improvement was observed. With the possible exception of
inkscape , all cases appear saturated already after a small
number of iterations. It is therefore unlikely that running
the fuzzers for a longer time would produce signiﬁcantly
787Table 3: Mutation conﬁgurations and results.
Generating
programInput ConﬁgurationTest set
nameTotal
ins.Slice
size# runs # ﬁlesUniq.
ﬁlesTime
(hours)
avconv sample-1.mp3mp3→mp3, CBR mp3-cbr 108,279 5,358 51,692 34,892 3,841 24.46
mp3→mp3, VBR mp3-vbr 108,783 5,339 51,411 35,254 4,383 25.08
mp3→aac, CBR aac-cbr 103,567 5,337 51,125 34,401 5,507 25.78
mp3→aac, VBR aac-vbr 103,580 5,371 51,689 34,918 5,716 27.71
convertsample-1.pngpng→tiﬀtiﬀ-1 64,957 5,312 46,328 23,108 5,008 15.51
sample-2.png tiﬀ-2 64,627 4,164 39,440 20,530 3,037 11.55
nconvertsample-1.tiﬀtiﬀ→pngpng-1 32,385 3,055 33,446 20,815 5,275 6.50
sample-2.tiﬀ png-2 30,104 2,017 22,152 14,355 3,041 3.63
pdftocairosample-1.pdfpdf→pdfpdf-1 104,328 8,353 81,262 66,868 9,714 33.99
sample-2.pdf pdf-2 129,349 10,320 97,759 80,277 10,715 44.74
pdftopssample-1.pdfpdf→psps-1 56,120 4,823 44,964 32,934 5,124 13.17
sample-2.pdf ps-2 76,870 5,883 53,001 40,835 4,890 26.32
inkscapesample-1.svgsvg→svgsvg-1 235,372 13,996 121,719 44,478 978 102.08
sample-2.svg svg-2 236,187 14,299 123,864 44,798 1,067 105.94
Table 4: Testing results.
Program Test setBasecov.
(# ins.)MutaGen zzuf radamsa
CrashesUnique
bugsTime (s)Rel. cov.
increaseRel. cov.
increaseRel. cov.
increase
ps2pdfps-1 179,819 111319.2 48.19% 0.85% 0.92%
ps-2 181,282 0 1734.8 42.87% 0.85% 0.89%
pdftopspdf-1 54,775 531783.3 154.23% 2.38% 1.82%
pdf-2 52,890 3 1177.1 165.10% 2.44% 1.83%
mudrawpdf-1 63,103 00987.8 58.86% 2.06% 3.07%
pdf-2 69,066 0 736.0 57.95% 2.26% 2.94%
inkscapesvg-1 238,259 00280.9 6.36% 2.61% 0.74%
svg-2 239,250 0 312.4 6.72% 2.64% 2.60%
avconvmp3-cbr 108,568 0
0459.9 25.46% 3.68% 0.68%
mp3-vbr 109,124 0 496.8 12.98% 1.25% 0.64%
aac-cbr 110,587 0 518.6 22.03% 0.25% 0.40%
aac-vbr 110,677 0 545.5 20.29% 0.30% 0.53%
convertpng-1 64,542 0
1195.9 50.80% 3.79% 4.52%
png-2 64,043 0 108.0 21.72% 3.85% 3.85%
tiﬀ-1 65,918 0 338.6 44.58% 4.00% 4.21%
tiﬀ-2 64,020 1 154.1 45.73% 4.20% 4.40%
nconvertpng-1 29,332 0
3161.1 90.52% 45.57% 2.03%
png-2 29,134 0 55.6 70.35% 45.23% 45.28%
tiﬀ-1 32,491 2 292.3 96.93% 41.51% 41.67%
tiﬀ-2 31,979 4 135.8 93.91% 42.40% 42.41%
higher coverage. Again, inkscape stands out somewhat. In
order to be consistent with the methodology used for the
other programs, only about 1000 iterations were used for
inkscape , which is likely too few to achieve full saturation.
Some of the plots show conspicuous similarities between
the results for zzufandradamsa. We suspect that this may
be due to the naming scheme used for generated ﬁles. We
add the iteration number as a suﬃx to each ﬁle name, which
causes the length of the ﬁle name to change deterministi-
cally. For avconc-aac-cbr we e.g. see small jumps in cover-
age at the 10th, 100th, and 1000th iteration.
MutaGen produces 16 inputs that crash programs, out
of which 8 were unique bugs according to the stack trace.
Some of the bugs are simple correctness errors, such as di-
visions by zero. Other bugs involve trying to free memory
at invalid addresses, which could have security implications.
Neither the short instrumented runs, nor the 24-hour unin-
strumented runs of zzuforradamsa yielded any bugs.
Eﬀectiveness of mutation operators . Table 5 summa-
rizes the fractions of mutants and test inputs contributedby each mutation operator, for all test sets. Some muta-
tion operators are signiﬁcantly more common than others.
For example, every VEX statement that is candidate for
mutation has an output operand, which means that muta-
tions 1–4 are always applied, while for example mutation 15
(switch And/Or) is very uncommon. Note that the number
of mutants for mutations 1–4 are not exactly equal. Since
we identify the candidate VEX statements online during the
test generation phase, we may in some rare cases fail to ap-
ply a mutation when the preceding mutant crashed very
early. This results in a very small variation in the number
of mutants for each “family” of mutation operators. We do
not expect this to aﬀect our results in a signiﬁcant way.
Due to our pruning of equivalent mutants the “add” and
“subtract” mutations 7 and 8, as well as 11 and 12, are
rare in comparison to their “multiply” and “divide” coun-
terparts(section3.2). Wealsonotethatmutation16(switch
Add/Subtract) is very common, probably much due to the
commonality of address-generation expressions like the one
in ﬁgure 3.
788 0 450 900 1350 1800
 0  1500  3000  4500ps2pdf-ps-1
 0 350 700 1050 1400
 0  2500  5000  7500pdftops-pdf-1
 0 500 1000 1500 2000
 0  2500  5000  7500mudraw-pdf-1
 0 1750 3500 5250 7000
 0  300  600  900inkscape-svg-1
 0 1000 2000 3000 4000
 0  1000  2000  3000avconv-mp3-cbr
 0 112.5 225 337.5 450
 0  1500  3000  4500avconv-aac-cbr
 0 750 1500 2250 3000
 0  1500  3000  4500convert-png-1
 0 750 1500 2250 3000
 0  1500  3000  4500convert-tiﬀ-1
 0 3500 7000 10500 14000
 0  1500  3000  4500nconvert-png-1
 0 3500 7000 10500 14000
 0  1500  3000  4500nconvert-tiﬀ-1Figure 4: Instruction coverage versus iteration for the mutation-based fuzzers. The solid red line represents
zzuf, while the dashed blue line represents radamsa.
Within the ﬁeld of mutation testing, it has previously [27]
been shown that, out of a larger set of mutation operators, a
small number of operators can achieve results very close to
the full set. We wanted to investigate if the same was also
true in our context. To this end, we investigated the instruc-
tion coverage achieved by the optimal set Onof mutation
operators of size n, for each nbetween 1 and 17. Since this
is an instance of the minimum set-cover problem, which is
known to be NP-complete, we simply computed our results
bymeansofexhaustiveenumerationofallcombinations. For
eachcombinationof nmutationoperators, wecalculatedthe
averageoftherelativecoverageincreaseoveralltestconﬁgu-
rations, and selected the combination with the highest aver-
age coverage. We also counted how many of the bugs would
be found with the optimal reduced operator set of each size.
At ﬁrst, it may seem appropriate to weight the cover-
age of each mutation operator by e.g. the number of test
cases, or the number of mutants, since table 5 shows that
some mutations are much more common than others. How-
ever, applying such linear weighting would unduly favor the
rare mutations, due to the very pronounced saturation ef-
fect observed in random testing. Consider for example two
mutation operators miandmj, yielding the respective sets
of test inputs TiandTj,|Ti|>|Tj|. These test sets in
turn cover instructions IiandIjwhen used as inputs to
some program. If we randomly sample |Tj|inputs from Ti
to produce a smaller test set Tr, that test set will still yield a
coveragethatissigniﬁcantlyhigherthan/parenleftbig
|Tj|/|Ti|/parenrightbig
·|Ii|, due
to said saturation eﬀect. For this reason, we do not apply
any weighting of coverage scores when computing optimal
set combinations.
The optimal operator sets of each size are presented in
table 6. Using only 6 mutation operators achieves 97.6% of
the mean coverage of all operators, and ﬁnds 7 of the 8 bugs
in our experiments. This corresponds to a 53% reduction
of mutant executions (cf. table 5). With 10 operators, we
achieve 99.6% of the mean coverage and ﬁnd all bugs, whileachieving a 26% reduction of mutant executions. The most
eﬀective operator is number 2, which alone achieves 73% of
the total mean coverage. Operator 15 is the least eﬀective,
which is perhaps not surprising, since it is also the least
common one.
Several interesting observations regarding the eﬀective-
ness of diﬀerent types of mutation operators can also be
made from the results:
•Subtraction is better than addition. All the “subtract
1” operators (2,8,12) perform better than the “add 1”
operators (1,7,11). This is likely because subtraction
can cause larger semantic modiﬁcations to inputs by
changing a small number to a very large number by
means of a wrap-around. As Boolean variables are
commonly represented by the integers 0 and 1, sub-
tractioncanalsochangetruthvaluesinbothdirections
(0→232,1→0), while addition can only change false
to true ( 0→1).
•Operators 1–4 largely subsume operators 7–10. Muta-
tions on destination operands appear to largely sub-
sume mutations on the ﬁrst input operand. This is
not an unexpected result, since many VEX statements
that have only one input operand simply serve to re-
ceive some intermediate result produced by an earlier
statement. Note that we prune the trivial cases where
the result of a computation is only used once within a
superblock (section 3.2).
•Mutating the second operand is eﬀective. Mutations
13,14 are signiﬁcantly more eﬀective than 9,10. We
speculate that this is because, in expressions with con-
stants, the second operand often holds the constant
value. As constants are not ﬁrst loaded into tempo-
raries in the VEX IR, they are not subjected to muta-
tion by operators 1–4. Mutating the second operand
789Table 5: Contributions from each mutation opera-
tor.
Mutation Mutants Test inputs Crashes Bugs
1 7.38% 11.30% 3 1
2 7.35% 10.87% 2 2
3 7.34% 8.26% 0 0
4 7.34% 8.89% 0 0
5 8.65% 9.98% 3 3
6 8.63% 7.31% 0 0
7 1.62% 2.70% 0 0
8 1.60% 3.04% 1 1
9 9.51% 4.05% 0 0
10 9.36% 4.91% 0 0
11 1.28% 0.82% 0 0
12 1.28% 1.34% 0 0
13 9.03% 9.09% 2 1
14 9.03% 7.91% 3 1
15 0.44% 0.32% 0 0
16 8.07% 8.41% 2 1
17 0.36% 0.78% 0 0
Table 6: Eﬀectiveness of reduced mutation operator
sets.
Optimal mutation operator set Cover Bugs
2 73.116% 2/8
2 5 84.132% 5/8
2 5 13 90.139% 6/8
2 4 5 13 93.744% 6/8
2 3 4 5 13 96.118% 6/8
1 2 3 4 5 13 97.610% 7/8
1 2 3 4 5 13 16 98.561% 7/8
1 2 3 4 5 6 13 16 99.193% 7/8
1 2 3 4 5 6 13 14 16 99.450% 7/8
1 2 3 4 5 6 8 13 14 16 99.585% 8/8
1 2 3 4 5 6 8 10 13 14 16 99.702% 8/8
1 2 3 4 5 6 8 10 12 13 14 16 99.805% 8/8
1 2 3 4 5 6 8 9 10 12 13 14 16 99.881% 8/8
1 2 3 4 5 6 8 9 10 12 13 14 16 17 99.936% 8/8
1 2 3 4 5 6 7 8 9 10 12 13 14 16 17 99.970% 8/8
1 2 3 4 5 6 7 8 9 10 11 12 13 14 16 17 99.993% 8/8
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 100.000% 8/8
could therefore be important for instructions that use
constants.
•Mutating address-generators is worthwhile. Since mu-
tations 5 and 6 are only applied on statements that
compute addresses, we can implicitly draw the conclu-
sion that mutating address-generating instructions is
eﬀective, both for improving coverage and for ﬁnding
bugs. One reason that addition beats subtraction here
could be that pointers to arrays are common in pro-
grams. Adding 4 to such a pointer will shift it to the
second element of the array, while subtracting from it
will cause it to point outside the array.
5. DISCUSSION AND FUTURE WORK
The results in section 4.2 clearly suggest that our main
hypothesis holds; mutating a correct implementation of a
program that can generate well-formed inputs achieves a
considerably better coverage and ﬁnds more bugs, comparedto traditional mutation-based fuzzing. In this section, we
further analyze the results and suggest directions for future
work.
Comparison to mutation-based fuzzing. While it is
diﬃcult to ﬁt MutaGen into one of the traditional fuzzing
paradigms, we believe its typical use cases to be similar to
those of mutation-based fuzzing. For example, MutaGen
could potentially serve as a “drop-in” replacement for a
mutation-basedfuzzerwhene.g.craftingaformalinputspec-
iﬁcation for a generation-based fuzzer is not feasible. Com-
pared to previous results from mutation-based fuzzing [22,
24, 28], our method has a relatively high crash density, i.e.
ratioofcrashestotestcases, andhasaveryhighbugdensity
per crash (50%). That suggests that MutaGen can produce
a small set of high-quality test inputs, which can ﬁnd bugs in
a small number of test runs. While test generation is fairly
time-consuming with our current implementation, MutaGen
can generate a set of inputs that can be used to quickly test
several programs that accept the same ﬁle type. This is not
possible with traditional mutation-based fuzzing, where the
main cost lies in executing the program under test on a very
large number of test inputs. Further, while the results of our
limited evaluation are not statistically signiﬁcant, the fact
that MutaGen found several bugs, while zzufand radamsa
found none, may suggest that our approach is better at ﬁnd-
ing complex bugs that mutation-based fuzzing fails to catch.
Comparisontogeneration-basedfuzzing. Ourmethod
also bears some resemblance to generation-based fuzzing,
most notably the ability to exploit information about input
formats to generate semi-valid inputs. There are, however,
several important diﬀerences. First, our method is poten-
tially more versatile, as it does not require a formal spec-
iﬁcation of an input format. For example, the well-known
generation-based fuzzer SPIKE [3] uses a block-based ap-
proach to deﬁne input formats. While SPIKE has proven
very successful at testing e.g. implementations of binary
network protocols, it would not work for inputs that are
not easily ﬁtted into a block-based formalism. Secondly,
a key diﬀerence between mutation-based and generation-
basedmethodsisthelatter’sabilitytotestlong-runningpro-
grams with state, such as implementations of stateful net-
work protocols, by modeling the protocol state machine. As
our method relies entirely on implicit information about the
structure of input, such models cannot be incorporated into
our approach. An interesting direction for future work is to
investigate if MutaGen can suﬃciently retain the semantics
of the generating program, so that its state machine is kept
“synchronized” with the server when testing stateful proto-
cols. Finally, apart from a formal speciﬁcation of the input
format, generation-based fuzzers typically also need a set of
heuristics for generating semi-valid inputs. Such heuristics
are often compiled from inputs or input-patterns that have
been known to trigger bugs in the past [30, 15]. Our method
does not currently allow incorporating such heuristics when
applying mutations.
Thegeneralconsensusamongsecurityexpertsisthatmeth-
ods based on generation typically achieve better coverage
than those based on mutation. Miller and Peterson [23] e.g.
report 76% higher coverage for generation compared to mu-
tation when fuzzing a PNG parsing library. For this reason,
a comparison between our approach and generation-based
fuzzing would be interesting. There are several practical
complications, however, that make such a comparison diﬃ-
790cult. For example, to the best of our knowledge, there are
no publicly available robust generation-based fuzzers avail-
able for many of the popular input formats that we use in
our evaluation. Should we repeat our evaluation, comparing
against generation-based fuzzers instead of mutation-based,
we would be forced to write our own fuzzers for many of
the input formats. Aside from the signiﬁcant extra work
eﬀort, this would introduce an obvious risk of bias. A di-
rect side-by-side comparison may also be diﬃcult due to the
aforementioned diﬀerences in use cases and application ar-
eas. Analogous to mutation-based fuzzing, the results of a
comparisonaree.g.likelytobeverydependentonthechoice
of seed-inputs for the generating programs.
Input space compression. A traditional mutation-based
fuzzer, such as zzuf, has a search space of size 2kfor a
seed of size kbits. The obvious beneﬁt of a generation-
based fuzzer, as well as our approach, is that the search
space can be reduced by applying information about the
input format. In order to cover a suﬃcient part of the input
space, amutation-basedfuzzeralsoneedsalargeanddiverse
selection of seed inputs. The optimal selection of seeds is
still an active area of research [28]. With our approach,
the equivalent problem is to identify appropriate generating
programs, as well asinputs and conﬁgurationparameters for
these programs. Investigating the importance of the choice
ofgeneratingprogramsisanimportanttopicforfuturework.
Another interesting question is whether the code coverage
of thegenerating program is a good estimate of the covered
input space.
Performance improvement. One obvious direction of
future work is to improve the performance of MutaGen.
Currently, the most time consuming part of the MutaGen
pipeline is the execution of mutants to produce test cases.
The current mutation tool is implemented with Valgrind,
which facilitates rapid development, since it is not necessary
todealdirectlywiththeintricaciesofthex86instructionset.
However, Valgrind does incur an overhead of at least 10x.
A more performance-conscious choice would be to perform
mutations by means of e.g. binary-patching, which would
allow mutants to execute at near-native speed.
Mutant reduction. Another way to reduce the cost of test
generation is to reduce the number of mutants. Our investi-
gation of the eﬀectiveness of mutation operators shows that,
in our particular experiments, we could reduce the number
of mutants by about 50% while still ﬁnding most bugs and
retaining high coverage. We intend to further study the po-
tential for mutant reduction in future work. We also intend
to verify if the reason that mutations of the second operand
perform so well is indeed due to arithmetic with constants.
If that is the case, an alternative mutation strategy could
be to only perform mutations 1–5, along with direct manip-
ulation of constants.
Intermittent mutation. Finally, another strategy for in-
put generation is to only intermittently inject errors into
the generating program, rather than statically mutating in-
structions. This could potentially retain the semantics of
the generating program to a larger degree, possibly enabling
even higher coverage of the program under test. Similar to
mutation-based fuzzing, however, the number of generated
inputs would be unbounded.6. RELATED WORK
Following the pioneering work by Miller et al. [20, 21],
many of the early advances in fuzzing were made outside
of academia. Recently, Woo et al. [32] studied scheduling
algorithmsformutation-basedfuzzing, andRebertetal.[28]
investigated the importance of seed selection. While both
of these works focus on traditional mutation-based fuzzing,
MutaGen could still beneﬁt from their ﬁndings. Eﬃcient
scheduling algorithms could help MutaGen prioritize which
parts of a generating program to mutate ﬁrst, if there is
a ﬁxed time budget for executing mutants. Our approach
could also beneﬁt from the ﬁndings of Rebert et al. when
selecting inputs to generating programs.
Generation-based fuzzers have proven particularly eﬀec-
tive at ﬁnding bugs in compilers and interpreters. Yang et
al. use generation-based fuzzing to ﬁnd bugs in C compilers
[33], and Holler et al. use grammars, in combination with
code fragments known to have previously triggered bugs, to
test interpreters [15]. Due to the highly stringent require-
ments on input structure, and the general scarcity of input-
generating programs, we believe generation-based fuzzing to
be favorable over our method in this speciﬁc domain.
Symbolic execution is an alternative to black-box testing
methods. The SAGE [14] and Mayhem [9] systems can for
example generate high-coverage test cases using concolic ex-
ecution of program binaries. While such methods have seen
tremendous development during recent years, their scalabil-
ity is still limited.
Wang et al. propose a hybrid approach [31] to improve
the coverage of programs that perform integrity checks on
their inputs. A constraint solver is used to repair check-
sum ﬁelds in fuzzed ﬁles, allowing them to pass integrity
checks. The authors note that the checksum-repair stage is
time consuming, and that their method would fail to han-
dle input formats that use cryptographically strong integrity
checks. Our approach, by contrast, sidesteps that problem
altogether by allowing mutations to be applied before check-
sum computations.
Several previous works describe methods for automati-
cally recovering input formats by dynamic analysis of pro-
gram binaries [8, 11, 10]. The recovered input formats could
later be used to drive a generation-based fuzzer. While sim-
ilar in spirit to our work, these methods are limited by the
fact that input formats can only be recovered with respect
to a pre-determined formalism, e.g. context-free grammars.
For that reason, they face a similar problem as generation-
based fuzzing systems; there is no one formalism suitable for
describing all input formats. Our method tackles the prob-
lem of input generation at a lower level, and does not need
to make assumptions about properties of input formats.
7. CONCLUSION
In this paper we have presented a novel method for au-
tomatic test case generation using dynamic slicing and pro-
gram mutation. We empirically evaluated the method on 7
Linux programs and found that it improved code coverage
withatleastoneorderofmagnitudeinmostcases,compared
to two well-known fuzzers. We found a total of 16 crashing
inputs, 8 of which represent unique bugs. We evaluated
the eﬀectiveness of our mutation operators, and suggested
ways to further improve the coverage and performance of
our method.
7918. REFERENCES
[1] Chrome reward program rules. http://www.google.
com/about/appsecurity/chrome-rewards/ .
[2] A crash course to radamsa.
https://code.google.com/p/ouspg/wiki/Radamsa .
[3] Immunity inc - resources. http:
//www.immunityinc.com/resources/index.html .
[4] Microsoft bounty programs. https://technet.
microsoft.com/en-us/library/dn425036.aspx .
[5] Sdl process: Veriﬁcation. http://www.microsoft.
com/security/sdl/process/verification.aspx .
[6] zzuf - multi-purpose fuzzer.
http://caca.zoy.org/wiki/zzuf .
[7] B. Arkin. Adobe reader and acrobat security
initiative. http://blogs.adobe.com/security/2009/
05/adobe_reader_and_acrobat_secur.html , 2009.
[8] J. Caballero, H. Yin, Z. Liang, and D. Song. Polyglot:
Automatic extraction of protocol message format
using dynamic binary analysis. In Proceedings of the
14th ACM Conference on Computer and
Communications Security , pages 317–329, 2007.
[9] S. K. Cha, T. Avgerinos, A. Rebert, and D. Brumley.
Unleashing mayhem on binary code. In Proceedings of
the 2012 IEEE Symposium on Security and Privacy ,
pages 380–394, 2012.
[10] P. Comparetti, G. Wondracek, C. Kruegel, and
E. Kirda. Prospex: Protocol speciﬁcation extraction.
InProceedings of the 2009 30th IEEE Symposium on
Security and Privacy , pages 110–125, 2009.
[11] W. Cui, M. Peinado, K. Chen, H. J. Wang, and
L. Irun-Briz. Tupni: Automatic reverse engineering of
input formats. In Proceedings of the 15th ACM
Conference on Computer and Communications
Security, pages 391–402, 2008.
[12] C. Evans, M. Moore, and T. Ormandy. Google online
security blog – fuzzing at scale.
http://googleonlinesecurity.blogspot.se/2011/
08/fuzzing-at-scale.html , 2011.
[13] M. Finifter, D. Akhawe, and D. Wagner. An empirical
study of vulnerability rewards programs. In
Proceedings of the 22nd USENIX Security Symposium ,
pages 273–288, 2013.
[14] P. Godefroid, M. Y. Levin, and D. A. Molnar.
Automated whitebox fuzz testing. In Proceedings of
the Network and Distributed System Security
Symposium , 2008.
[15] C. Holler, K. Herzig, and A. Zeller. Fuzzing with code
fragments. In Proceedings of the 21st USENIX
Security Symposium , 2012.
[16] A. D. Householder and J. M. Foote. Probability-based
parameter selection for black-box fuzz testing.
Technical report, CERT, 2012.
[17] Y. Jia and M. Harman. An analysis and survey of the
development of mutation testing. IEEE Transactions
on Software Engineering , 37(5):649–678, 2011.
[18] U. Kargén and N. Shahmehri. Eﬃcient utilization of
secondary storage for scalable dynamic slicing. In
Proceedings of the 2014 IEEE 14th International
Working Conference on Source Code Analysis and
Manipulation , pages 155–164, 2014.[19] C.-K. Luk, R. Cohn, R. Muth, H. Patil, A. Klauser,
G. Lowney, S. Wallace, V. J. Reddi, and
K. Hazelwood. Pin: Building customized program
analysis tools with dynamic instrumentation. In
Proceedings of the 2005 ACM SIGPLAN Conference
on Programming Language Design and
Implementation , pages 190–200, 2005.
[20] B. P. Miller, L. Fredriksen, and B. So. An empirical
study of the reliability of unix utilities. Commun.
ACM, 33(12):32–44, Dec. 1990.
[21] B. P. Miller, D. Koski, C. P. Lee, V. Maganty,
R. Murthy, A. Natarajan, and J. Steidl. Fuzz
revisited: A re-examination of the reliability of unix
utilities and services. Technical report, University of
Wisconsin-Madison, Computer Sciences Department,
1995.
[22] C. Miller. Babysitting an army of monkeys.
CanSecWest, 2010.
[23] C. Miller and Z. N. Peterson. Analysis of mutation
and generation-based fuzzing. Technical report,
Independent Security Evaluators, 2007.
[24] D. Molnar and L. Opstad. Eﬀective fuzzing strategies.
CERT vulnerability discovery workshop, 2010.
[25] N. Nethercote and J. Seward. How to shadow every
byte of memory used by a program. In Proceedings of
the 3rd International Conference on Virtual Execution
Environments , pages 65–74, 2007.
[26] N. Nethercote and J. Seward. Valgrind: A framework
for heavyweight dynamic binary instrumentation. In
Proceedings of the 2007 ACM SIGPLAN Conference
on Programming Language Design and
Implementation , pages 89–100, 2007.
[27] A. J. Oﬀutt, A. Lee, G. Rothermel, R. H. Untch, and
C. Zapf. An experimental determination of suﬃcient
mutant operators. ACM Trans. Softw. Eng. Methodol. ,
5(2):99–118, 1996.
[28] A. Rebert, S. K. Cha, T. Avgerinos, J. Foote,
D. Warren, G. Grieco, and D. Brumley. Optimizing
seed selection for fuzzing. In Proceedings of the 23rd
USENIX Security Symposium , pages 861–875, 2014.
[29] A. Slowinska and H. Bos. Pointless tainting?:
Evaluating the practicality of pointer tainting. In
Proceedings of the 4th ACM European Conference on
Computer Systems , pages 61–74, 2009.
[30] M. Sutton, A. Greene, and P. Amini. Fuzzing: brute
force vulnerability discovery . Pearson Education, 2007.
[31] T. Wang, T. Wei, G. Gu, and W. Zou. Taintscope: A
checksum-aware directed fuzzing tool for automatic
software vulnerability detection. In Proceedings of the
2010 IEEE Symposium on Security and Privacy , pages
497–512, 2010.
[32] M. Woo, S. K. Cha, S. Gottlieb, and D. Brumley.
Scheduling black-box mutational fuzzing. In
Proceedings of the 2013 ACM SIGSAC Conference on
Computer and Communications Security , pages
511–522, 2013.
[33] X. Yang, Y. Chen, E. Eide, and J. Regehr. Finding
and understanding bugs in c compilers. In Proceedings
of the 32nd ACM SIGPLAN Conference on
Programming Language Design and Implementation ,
pages 283–294, 2011.
792