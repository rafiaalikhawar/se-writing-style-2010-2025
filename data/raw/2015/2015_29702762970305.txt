Inferring Annotations for Device Drivers from VeriÔ¨Åcation
Histories
Zvonimir Pavlinovic
New Y ork University, USA
zvonimir@cs.nyu.eduAkash Lal
Microsoft Research, India
akashl@microsoft.comRahul Sharma
Stanford University, USA
sharmar@cs.stanford.edu
ABSTRACT
This paper studies and optimizes automated program veri-
cation. Detailed reasoning about software behavior is often
facilitated by program invariants that hold across all pro-
gram executions. Finding program invariants is in fact an
essential step in automated program verication. Automatic
discovery of precise invariants, however, can be very dicult
in practice. The problem can be simplied if one has ac-
cess to a candidate set of assertions (or annotations ) and the
search for invariants is limited over the space dened by these
annotations. Then, the main challenge is to automatically
generate quality program annotations.
We present an approach that infers program annotations
automatically by leveraging the history of verifying related
programs. Our algorithm extracts high-quality annotations
from previous verication attempts, and then applies them
for verifying new programs. We present a case study where
we applied our algorithm to Microsoft's Static Driver Verier
(SDV). SDV is an industrial-strength tool for verication of
Windows device drivers that uses manually-tuned heuristics
for obtaining a set of annotations. Our technique inferred
program annotations comparable in performance to the ex-
isting annotations used in SDV that were devised manually
by human experts over years. Additionally, the inferred
annotations together with the existing ones improved the
performance of SDV overall, proving correct 47% of drivers
more while running 22% faster in our experiments.
CCS Concepts
Theory of computation !Invariants;Software and
its engineering !Software verication; Formal soft-
ware verication;
Author did part of the work as a research intern at Microsoft
Research Bangalore, IndiaKeywords
Program verication; Invariant generation; Learning invari-
ants; Verication history; Big Code
1. INTRODUCTION
The performance of program veriers depends on the dis-
covery of precise assertions that hold during every program
run, called program invariants. Examples of such assertions
are procedure pre/post conditions and loop invariants. The
task of nding invariants is often broken down into nd-
ing a set of candidate facts, or annotations, and then using
these facts to establish invariants. For example, predicate-
abstraction-based tools such as SLAM [2] or BLAST [13]
rely on discovery of useful predicates to construct program
invariants. Tools such as UFO [1] and Duality [20] rely
on interpolation to generate candidates for procedure sum-
maries. Each of these techniques, however, infer annotations
by analyzing only the program given to be veried.
We propose a novel and complementary approach of infer-
ring program annotations automatically by exploiting infor-
mation available from prior verication runs. We build on
the insight that annotations useful for verifying a particular
program are often already observed earlier during the ver-
ication of related programs. For instance, programs that
use the same API probably require similar annotations for
verifying contracts of that API. We keep track of the veri-
cation history by accumulating a set of programs and the
annotations required to construct their respective proofs. We
leverage this history to generate a small set of annotations
that are useful for subsequent (unseen) programs.
There are two key challenges in making this approach
work. First, annotations are logical formulas over program
variables, thus, tied to program-specic variable names. We
abstract away from program-specic names by working with
abstract annotations , which are arbitrary formulas with holes .
Abstract annotations are concretized to a program by lling
the holes with the program's variables.
The set of all (abstract) annotations in the verication
history has the nice property that it is sucient to establish
the correctness of all programs observed in the history. How-
ever, this set is likely to be very large, making the verier
spend a signicant amount of time just discarding invalid an-
notations. Our second challenge is to keep the set of inferred
annotations small. We design a minimization algorithm that
computes a set of abstract annotations such that: (1) it is
enough to establish correctness proofs of all programs in the
history, and (2) no smaller subset (or syntactically simpler
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proÔ¨Åt or commercial advantage and that copies bear this notice and the full citation
on the Ô¨Årst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speciÔ¨Åc permission and/or a
fee. Request permissions from Permissions@acm.org.
ASE‚Äô16 , September 3‚Äì7, 2016, Singapore, Singapore
c2016 ACM. 978-1-4503-3845-5/16/09...$15.00
http://dx.doi.org/10.1145/2970276.2970305
450
set, in a sense that we formalize later) is enough to establish
all correctness proofs.
Our primary motivation behind these ideas is to im-
prove the performance of Microsoft's Static Driver Verier
(SDV) [21]. SDV is an industrial-strength tool for formal
verication of Windows device drivers. SDV checks that
drivers conform to certain properties (called rules ) that es-
tablish correct usage of the Windows kernel APIs. SDV
currently uses manually-tuned heuristics for obtaining a set
of annotations that are passed to a program verier. Using
a repository of small in-house drivers, our techniques can
not only replace the need for this manual eort, they even
out-perform these heuristics and improve the performance of
SDV overall.
We summarize our contributions as follows:
Given a verication history, we formally dene the
notion of a minimal set of annotations, and present an
algorithm for computing it.
We apply the algorithm to SDV and experimentally
show that inferring annotations from past verication
eorts can potentially generate better annotations than
ones provided by human experts. The set of abstract
annotations inferred by our algorithm improved the
verication times by 22% on average and reduced in-
conclusive results by 47% in our experiments.
2. OVERVIEW
This section motivates the need for automatic inference
of program annotations and provides an overview of our
techniques.
2.1 Static Driver VeriÔ¨Åer
The Static Driver Verier (SDV) has been an important
success story for verication technology [2]. Over a decade,
it has helped Windows developers to statically nd bugs in
Windows device drivers. The main verication engine of SDV
was SLAM, with several upgrades along the way [2]. SDV
switched to using an SMT-based verier called Corral [15]
for superior performance.
An overview of SDV is shown in Figure 1. SDV accepts
the source code of a Windows device driver as input and
links it against a model of the kernel (called \OS Model" in
the gure). It then checks multiple rules that the driver
must satisfy. These rules and kernel contracts (encoded
in the OS Model) are made known to driver developers via
Microsoft Developer Network (MSDN).1Each driver and rule
results in (possibly multiple) programs with assertions, called
verication instances that are fed to the verier (Corral).
Corral's job is to nd an execution that leads to an assertion
violation in the verication instance. Such executions are
reported as defects to the user.
Corral operates by lazily inlining procedures and utiliz-
ing an SMT solver to search through the partially-inlined
program. To help Corral, SDV uses annotation-based invari-
ant generation. SDV generates annotations based on the
rule being checked and runs Houdini [9] on the verication
instance to compute invariants constructed from these anno-
tations. These invariants are injected back to the verication
1https://msdn.microsoft.com/en-us/library/windows/hardware/ff552840(v=vs.
85).aspxinstance as assumptions , which help Corral prune search
without compromising soundness.
Corral has four possible outcomes, as mentioned in Fig-
ure 1. Corral uses over-approximations (rened by invariants
inferred by Houdini), hence it can prove correctness and
return \proof". Corral can also nd a bug and report the
failing execution. Corral stops search when it hits an internal
coverage bound [18] and returns \bound". Although this is an
inconclusive verdict, it is still considered more useful than a
Timeout outcome because the latter does not guarantee any
coverage. SDV has experienced several upgrades, including
major improvements to Corral [16, 17]. Showing further
improvements would truly build on the state-of-the-art in
the area.
This paper focuses on improving annotation generation
in SDV. The current process of generating annotations is
guided by a set of heuristics that have been manually tuned
and maintained over several years. These heuristics can
be found in our technical report [28]. What makes this
approach feasible is that the heuristics only look at the rule
being checked (not the driver) for generating the annotations.
While driver code is not known ahead of time, the set of rules
is xed and known to SDV developers before SDV's release.
The goal of this paper is two-fold. First, we want to auto-
matically generate annotations instead of requiring manual
eort. Second, we wish to show that automated inference can
out-perform the expert-driven heuristics. We achieve these
goals by learning useful information from past verication
eorts.
2.2 Examples of SDV Rules
We give two examples of rules (properties) that SDV checks.
The rules are described in an abstract manner, without using
the actual tool notation, for clarity.
Acquire-release rules. The OS kernel provides multiple
resources to help the driver accomplish its task. The kernel
expects that the driver (specically, the dispatch routines of
the drivers) must release all acquired resources when it exits.
SDV checks this property by instrumenting the Acquire -
Release API as shown in Figure 2(a). The actual code of
the API is immaterial while checking this property, thus, is
not shown. The rule introduces the model variable depth
to keep track of the number of resources held; the variable is
asserted to be zero when the driver's dispatch routine exits.
This rule abstractly captures the SpinLockRelease rule
of SDV.2
Figure 2(b) shows a family of (fake) drivers that exercise
this API correctly. Entry points of the drivers are the proce-
dures PnandQ.Pnis parameterized by the value of n. It calls
the routine dispatchPnwhere we use the notation [st]nto
denotenoccurrences of the statement st. These programs
model typical usage of the Acquire -Release API that we
have seen in drivers (while abstracting away irrelevant de-
tails). SDV links the driver code (Figure 2(b)) together with
the instrumented API code (Figure 2(a)) to produce a veri-
cation instance that is fed to Corral. The construction of a
proof of correctness and the role of annotations is discussed
in the next section.
Irql-based rules. AnInterrupt Request Level (IRQL)
captures the priority associated with a task. Tasks with
higher IRQL cannot be interrupted by tasks with a lower
2https://msdn.microsoft.com/en-us/library/windows/hardware/ff552780(v=vs.
85).aspx
451Figure 1: An overview of SDV's implementation.
IRQL. Drivers often raise the IRQL level to perform critical
activity uninterrupted, but are required not to spend too
much time at the high IRQL for the sake of responsiveness
of the system. SDV checks that drivers do not call certain
time consuming kernel APIs when at a high IRQL3.
Figure 3 shows IRQL modeling and usage. The OS
model variable sdv_irql_current records the current
IRQL value of the processor. Kernel APIs KeRaiseIrql4
and KeLowerIrql can be used to change the IRQL
value. The rule simply asserts that the procedure
\do_work_at_low_irql " is only called when IRQL is 0
(lowest possible value). The program with entry point main
is a (fake) driver that correctly exercises the kernel API.
2.3 Annotations and Proofs
We now set up some notation to describe program proofs.
Iffis a procedure and is a formula, we use []@fto
denote that is a valid postcondition of f. Iffhas a loop
starting at location Lthen the notation []@f@Ldenotes
thatis a valid loop invariant for L. We model assertion
failures as setting of a special okbit to false. For a variable
x, let old(x) refer to the value of xat the beginning of
the procedure or loop, depending on the context in which
it is used. For instance, [(x==old(x) + 1)_:ok]@f
means that the execution of feither increments the value
ofxor it fails an assertion. In other words, if fdoesn't
fail then it increments x.[x==old(x) ]@f@Lmeans
that the loop at location Lof procedure fpreserves the
value of xacross an arbitrary number of loop iterations.
Aproof of correctness of a program is simply a sequence
of mutually-inductive postconditions of procedures or loops
in the program that imply ok == true at the end of the
program. For simplicity (and without loss of generality) we
do not talk about procedure preconditions in this paper.
Annotations are simply formulas that serve as candidates for
postconditions.
Figure 2(c) shows possible proofs for the previously intro-
duced programs of Figure 2. The gure contains two possible
proofs for Pn(marked as \A" and \B") and a single proof
forQ. Note that all postconditions in proof AofPndo not
depend on the value of n, whereas proof Bis specic to the
value ofn.
Generation of invariants from a given set of annotations
(which we call annotation-based invariant generation ) is much
3See, for example, https://msdn.microsoft.com/en-us/
library/windows/hardware/547747(v=vs.85).aspx
4https://msdn.microsoft.com/en-us/library/windows/
hardware/553079(v=vs.85).aspxsimpler than full-blown verication, often even decidable.
One may use, for example, predicate abstraction [3] to con-
struct invariants that are Boolean combinations of the given
annotations. In our work, we use the Houdini algorithm [9]
to nd conjunctive invariants: ones that are conjunctions of
some subset of the given annotations. This problem has a
lower complexity than predicate abstraction and is very fast
in practice for small to medium number of annotations. For
example, given annotations fdepth == 0 ;  1; 0; 1;g
(not knowing if they are valid postconditions or loop invari-
ants) it is very ecient to construct a proof for Pn(of type
A) using Houdini.
2.4 Minimal Repositories
Our technique requires a repository of programs and their
proof of correctness. The proofs may be constructed manually
or by using proof-generating veriers. We do not expect
to control the proof-generation process. Suppose we have
programs QandPnfor eachn2N, for some large set N.
Further, suppose we have proof of type B(Figure 2(c)) for
Pnfor all values of n, exceptn0, and Pn0has a proof of type
A. TheseNprograms together with their corresponding
proofs constitute a repository.
We extract all annotations present in the proofs, which
produces a large set A=fdepth == 0 ;  1; 0; 1;; 0^
g[fnjn2N fn0gg. Retaining a large set of annotations
is inecient, even for annotation-based invariant generation
techniques. Moreover, some of the annotations are very
specic to a program, e.g., nis only useful for proving
correctness of Pn.
Our technique minimizes Awhile retaining its invariant-
generation power. The \power" is captured using a cost
metric based on the ability of a set of annotations to prove a
set of programs correct, given a xed verier. The cost is 1
if some program cannot be proved, otherwise, it reects the
running time of the verier. Our algorithm simplies Aby
dropping annotations or making them syntactically simpler
as long as the cost does not increase (or only increases by
atolerable amount; the exact formulation can be found in
Section 3).
For illustration, assume that the cost becomes 1as soon
as the annotations cannot establish some loop invariant or
postcondition of a recursive procedure (intuitively, because
these are the critical parts of a proof), and is unit cost oth-
erwise. Starting with A, our algorithm drops depth == 0
andfrom this set because these are not important for the
inductive argument; i.e., cost remains unit after dropping
them. Next, if it tries to drop  0, the cost becomes 1be-
452var depth: int;
procedure init() {
depth := 0;
}
procedure Acquire() {
depth := depth + 1;
}
procedure Release() {
depth := depth - 1;
}
procedure d_exit()
{
assert depth == 0;
}procedure Pn()
{call init(); call dispatchP n(); }
procedure dispatchP n() {
[call Acquire()]n;
L1: while (*)
{call Acquire(); call Release(); }
[call Release()]n;call d_exit();
}
procedure Q()
{call init(); call dispatchQ(); }
procedure dispatchQ() {
L2: while (*)
{call Acquire(); call Release();
call dispatchQ(); }
call d_exit();
}
(a) (b)
//Denitions
 idepth old(depth) ==i
nold(depth) ==n)depth ==n
(old(depth) == 0)ok)
//Proof A of Pn
[depth == 0 ]@init , [ 1]@Acquire , [  1]@Release , []@d_exit , [ 0]@dispatchPn@L1, []@dispatchPn
//Proof B of Pn
[depth == 0 ]@init , [ 1]@Acquire , [  1]@Release , []@d_exit , [n]@dispatchPn@L1, []@dispatchPn
//Proof ofQ
[depth == 0 ]@init , [ 1]@Acquire , [  1]@Release , []@d_exit , [ 0^]@dispatchQ @L2, [ 0^]@dispatchQ
(c)
Figure 2: (a) An Acquire-Release API, (b) a family of programs exercising the API, and (c) possible proofs
of correctness of the programs.
cause the loop invariant of Pn0is lost. Thus,  0is retained
inA. Next, each of the nannotations get dropped. Even
though these annotations were loop invariants in the original
proofs, they can be replaced by the more general annotation
 0that is present in A. In this way, learning from a large set
of proofs increases the chances of nding annotations useful
for many programs.
Finally, while  0^cannot be dropped, our algorithm tries
to simplify its Boolean structure. The algorithm simplies
it tobecause having annotations f 0;gis enough for
annotation-based invariant generation to establish  0^as
an invariant. At this point, the algorithm reaches a xpoint
where no annotation can be dropped or simplied and it
returns the set:f  1; 0; 1;g.
The algorithm is non-deterministic; it could have chosen to
drop 0^in its rst iteration because was still present in A.
In general, our algorithm only guarantees a locally optimal
solution with respect to a given cost metric. Globally-optimal
solutions are also possible to compute, but at a higher cost,
which was not justied in our experiments.Although the programs considered here are simple, they
are derived from real-world code. They reect common us-
age patterns of acquire-release-kind of APIs that we have
observed in drivers. The program Qillustrates an uncom-
mon (but not rare) scenario where recursion happens via
kernel callbacks (driver code itself typically does not exhibit
recursion).
2.5 Abstract Annotations
Annotations are formulas over program variables. In gen-
eral, dierent programs have dierent variables. To abstract
away from program-specic variables, we introduce the con-
cept of an abstract annotation that is a formula over only
generic and shared variables.
We call the set of global variables common to all programs
in the repository as the shared vocabulary . We assume these
variables serve a similar role in all programs, e.g., the depth
variable of Figure 2 will be present in all programs that
exercise the acquire-release API, and sdv_irql_current
will be present in all programs that exercise the IRQL API
of Figure 3. Shared variables, i.e., variables in the shared
453// DRIVER
procedure main() {
var loc: int;
call init();
call loc := KeRaiseIrql(2);
while (*) {
call KeLowerIrql(loc);
call do_work_at_low_irql();
call loc := KeRaiseIrql(2);
}
call KeLowerIrql(loc);
}
// RULE
procedure do_work_at_low_irql()
{assert sdv_irql_current == 0; }
// OS MODEL and RULE
var sdv_irql_current: int;
procedure init()
{ sdv_irql_current := 0; }
procedure KeRaiseIrql(new_irql: int)
returns (old_irql: int) {
old_irql := sdv_irql_current;
sdv_irql_current := new_irql;
}
procedure KeLowerIrql(new_irql: int)
{ sdv_irql_current := new_irql; }
Figure 3: IRQL modeling and example of usage.
vocabulary, can be freely used in annotations because they
are present in all programs that exercise the same rule.
Generic variables are not specic to any program. There
are fours kinds of generic variables:
fLocal;Global;FormalIn;FormalOutg:
The kind of a generic variable is determined statically by
inspecting the declaration of a program variable. An anno-
tation is converted to an abstract annotation by replacing
variables by generic variables of the corresponding type. For
example, consider the postcondition [ x == y ]@fon a pro-
cedure fwith formal input argument xand formal output
argument y. This will get converted to the abstract annota-
tion($fin == $fout )where $fin and$fout are generic
variables of kind FormalIn andFormalOut , respectively.
Abstract annotations are concretized when applied to a
program. Let pbe a program and proc a procedure in p.
We dene a concretization function p;proc as follows. For
an abstract annotation a,p;proc (a) returns all annotations
such that a generic variable of type Global is substituted
with some global variable of p, a generic variable of type
FormalIn is substituted with some formal-in parameter of
proc, and similarly for FormalOut andLocal .p;proc must
return all such annotations. p;proc leaves shared variables
unchanged.
Consider the driver in Figure 3. Proving correct-
ness of main requires a loop invariant that the value ofsdv_irql_current is unchanged across loop iterations,
which in turn requires that the value of locis maintained
across iterations. That is, it requires the loop invariant
loc == old(loc) . Our technique, once it observes a proof
with this loop invariant will generate and keep the abstract an-
notation $floc == old($floc) , where $floc is a generic
variable of type Local . With this abstract annotation, when
SDV is executed on a driver that exercises the IRQL rule,
the annotation will get instantiated with all local variables
manipulated by loops in the driver, and the annotation-based
invariant generation algorithm will be able to establish cor-
rectness of drivers that require preservation of local variable
values, similar to main of Figure 3.
3. ALGORITHM
We now formally describe the annotation inference algo-
rithm. Our presentation of the algorithm is general, ab-
stracting away from SDV specics for ease of presentation
and to emphasize the use of verication histories. However,
we evaluate the algorithm only on SDV in this work. We
leave generalization to other verication domains as future
work. We start by introducing the necessary notation and
denitions.
Language. We assume an imperative programming lan-
guage with standard features such as global variables, pro-
cedures, assume andassert statements, assignments, etc.
We also assume that programs in this language do not have
loops. Loops can be encoded using recursion. This allows our
framework to only concentrate on procedure postconditions
for establishing proofs of correctness.
Given a program p, we denote the set of procedures in p
withprocs (p). Each procedure can be annotated with any
number of rst order logic (FOL) formulas. These formulas
are dened over procedure parameters and global variables
and they do not take part in program execution; they are
used by program veriers as candidate postconditions for
establishing program correctness.
Abstract annotations. LetVbe a set of variables called
the shared vocabulary. All programs must contain Vas global
variables. Let Gbe a set of generic variables. None of the
programs contain a variable from G. An abstract annotation
ais a formula over variables in V[G. Further, for every
programpand procedure proc2procs (p), we assume a
functionp;proc that maps an abstract annotation to a set
of concrete annotations. As dened in the previous section,
p;proc substitutes generic variables with the variables in
scope ofproc.
We call a nite set of abstract annotations t2Tatemplate .
Given a program pand a template t,annotate (p;t) returns
pwhere each procedure proc2procs (p)is annotated with
the set of program annotationsS
a2tp;proc (a).
Verication. Given a xed verier, for a program pand
a template t, we say that proves (p;t)holds if the verier can
prove the correctness of annotate (p;t). The verier can use
the procedure annotations as potential postconditions during
the verication. Also, we require that if proves (p;t)and
tt0, thenproves (p;t0) must hold as well. In other words,
if a set of abstract annotations is sucient for proving some
program correct then all of its supersets are also sucient.
3.1 Problem
Our annotation inference problem is dened using the
notion of an objective relation. Such relations are used to
454encode what templates (and hence annotations) are more
desirable for the current application in mind.
Definition 1 (Objective relation for a program).
We say!p:TTis an objective relation for a program p
i (1) it is well-founded and (2) for each t2dom(!p), the
setft0jt!pt0gis nite.
The objective relation is hence nite branching . One example
of such a relation is the proper subset relation, i.e., t!pt0
it0t. Another example would be the relation where t0is
a copy oftexcept that an annotation in t0is a sub-formula
of the corresponding annotation in t. This relation roughly
corresponds to the syntactically simpler concept mentioned
in Section 1. Section 4 presents the objective relation used
in our experiments with SDV.
We extend the objective relation to a set Pof programs
p1;:::;p n:
Definition 2 (Objective relation for programs).
We dene!P:TT, an objective relation for a set of
programsP=fp1;:::;pngas a well founded and nite
branching relation t!Pt0,V
it!pit0.
We proceed by dening the notion of a minimal template
that intuitively stands for a locally optimal template. The
locality is dened as a branching set of a template induced
by a given objective relation.
Definition 3 (Minimal program template).
Given a program p, a template tsuch thatproves (p;t), and
an objective relation !p, we sayt0is a minimal template i:
1.proves (p;t0)
2. there exists no t00such thatt0!pt00andproves (p;t00)
The denition states that a minimal template must prove a
given program and none of its immediate ( !p) successors do.
We point out that, in the above denition, tonly ensures that
pis correct and this denition establishes no relationship
between a minimal template and t. However, the results com-
puted by the implementations for nding minimal templates
can be dependent on t.
Our inference algorithm is built around the notion of a
minimal template. We hence dene the problem of nding a
minimal template for a given program.
Problem 1 (Computing a minimal template).
Given a program p, a template tsuch thatproves (p;t),
and an objective relation !p, the problem of computing a
minimal template is nding a formula t0that is minimal
subject topand the ordering !p.
We dene a program repository asR= [(p1;t1);:::;(pn;tn)]
whereproves (pi;ti)for1in. Repositories capture
verication histories. The set of programs in the repository R
is denoted by PR. The actual technique used for proving the
correctness of pican be arbitrary. However, we envision that
in practice, a verier will be xed for the whole repository.
We now dene a locally optimal template for a verication
history.
Definition 4 (Minimal repository template).
Given a program repository R= [(p1;t1);:::;(pn;tn)]where
proves (pi;ti)for all 1in, and an objective relation
!PR, we say that Tis a minimal repository template
(subject to!PR) i the following holdsAlgorithm 1 Computing a minimal program template
Require:proves (p;t) and!is an objective relation
1:procedure MinTemplate (p;t;!)
2:mint t
3:loop
4: for allt02ft0jt!t0gdo
5: ifproves (p;t0)then
6:mint t0
7: goto 3
8: returnmint
1.proves (pi;T)for all 1in
2.there exists no T0such that T!PRT0and
proves (pi;T0)for all 1in
We are now ready to formally state the problem of inferring
program annotations from past verication runs.
Problem 2 (Inferring program annotations).
Given a program repository R= [(p1;t1);:::;(pn;tn)]where
proves (pi;ti)for all 1in, and an objective relation
!PR, the problem of inferring program annotations is to
nd a template Tthat is a minimal repository template
subject to!PR.
In the sequel, we suppress the subscripts of the !relations
for brevity. We now show algorithms for solving the problems
of computing minimal templates.
3.2 Solution
We start with the solution for Problem 1 shown in Al-
gorithm 1. The MinTemplate algorithm assumes that a
given template tis sucient to establish correctness of p
and that!is an objective relation. We start by considering
tas a minimal template candidate (line 2). We continue
by enumerating all immediate successors of tby!(line 4).
Then, the algorithm checks if any of the successors can prove
p(line 5). If so, then the algorithm sets such a successor as
a candidate for the minimal template and repeats the whole
process (lines 6 and 7). Otherwise, the minimal candidate is
returned as the solution (line 8).
Theorem 1.Letpbe a program, ta template, and!an
objective relation. If proves (p;t), then Algorithm 1 computes
a minimal template for p,t, and!.
Proof. Since!is nite branching, we have that inner
loop at line 4 terminates. From the fact that !is well-
founded, it follows that the outer loop at line 3 also termi-
nates. Since lines 4 and 5 simply follow the denition of a
minimal template, we have that the returned template is
indeed minimal.
The complexity of the algorithm depends on !relation and
implementation of proves . Assuming proves has unit com-
plexity, the running time of Algorithm 1 is O(lm), wherelis
the longest well-founded chain of !andmis the maximum
size of the branching sets maxfjft0jt!t0gjjt2dom(!)g.
However, proving a program correct is undecidable in general
and expensive in practice. Further, annotating a program p
given a template tcan also be expensive if for proc2procs (p),
455Algorithm 2 Computing a minimal repository template
Require:proves (pi;ti) for all (pi;ti)2R
Require:!is an objective relation
1:procedure MinRepoTemplate (R;!)
2:mints [ ]
3:for alli2[1;:::;jRj]do
4: (p;t) =R[i]
5:mints [i] =MinTemplate (p;t;!)
6:C=?
7:for alli2[1;:::;jRj]do
8:C=C[mints [i]
9: for allt2ft0jC!fp1;:::;pigt0gdo
10:b=true
11: for allj2[1;:::;i ]do
12: b=b^proves (pj;t)
13: ifbthen
14: C=t
15: goto 9
16: returnC
the concretization function p;proc has a large image; tcan
then potentially be instantiated with a large number of con-
cretizations. This high complexity of the algorithm can be
remedied in practice by choosing p;proc with smaller images
and exploiting the structure of !if the relation is known
beforehand. We note that a result computed by the algo-
rithm is not necessarily minimum. As we show in Section 5,
minimal templates suced for all practical purposes in our
experiments.
Algorithm 2 computes a minimal repository template by
building on Algorithm 1. First, we nd the minimum tem-
plate for each program in the repository and store it in
mints (lines 3-5). Next, the minimal repository template
is set to the empty set of clauses (line 6). The outer loop
(lines 7-15) has the invariant that after the ithiteration
Cis a minimal repository template for the sub-repository
[(p1;t1);:::;(pi;ti)]. The inner loop checks if an immediate
successor of Ccan prove the correctness of the programs
p1;:::;pi. If so, then Cis updated to that successor tem-
plate.
One possible optimization is to cache the clauses un-
der which a program can/cannot be proved. Therefore,
ifproves (p;t)is in the cache and we later make a query
proves (p;t0)wherett0, then, we can return true. Simi-
larly, if:proves (p;t)is in the cache and we make a query
proves (p;t0) wheret0t, then, we can return false.
Theorem 2.LetRbe a program repository and !an
objective relation. If proves (p;t)for all (p;t)2R, then
Algorithm 2 computes a minimal repository template for R
and!.
Proof. For every subrepository Ri= [(p1;t1);:::;(pi;ti)]
the relation!Riis well-founded and nite branching for any
chosen reduction operator. Since the body of the loop at
line 9 follows the denition of a minimal repository template,
then at the end of each iteration of the loop at line 7, Cis a
minimal template for Ri, as pointed out earlier. The result
then follows from the case when i=jRj.We also point out that the loop starting at line 7 is in fact
not necessary for optimality. The algorithm can immediately
start withCas the union of all minimal program templates
stored inmints . However, such a Ccould become impracti-
cally large. In Algorithm 2, the size of Cis kept moderate.
Observe that the minimal templates computed using these
two versions of the algorithm might not be the same. This
is because minimal templates are not unique.
4. APPLICATION TO SDV
We use a specic objective relation to infer useful program
annotations for SDV on Windows device drivers.
4.1 Repository
For internal testing, SDV uses a set of toy drivers, col-
lectively called the Rule Test Suite (RTS) for quick \smoke
testing" of SDV. These drivers are small, often a few hun-
dred lines of code (with relevant part in tens of lines of code
only). We use RTS as the program repository for inferring
a minimal repository template for each rule. We will refer
to such templates as \inferred templates" and to manually
crafted templates simply as \manual templates". The use of
RTS as the training set is quite natural as it allows us to
leverage existing test cases to learn and improve performance
on real drivers. RTS consists of 304 drivers totaling around
100KLOC.
Once we infer a template, we measure the performance of
SDV using the template on a set of real device drivers. We use
66device drivers for this task, totaling around 700KLOC. We
note that the device drivers were functionally very dierent:
they included storage drivers, modem drivers, bus drivers,
etc.
SDV has hundreds of rules. For this paper, we concentrate
on a collection of 28 rules. Of these, 14 rules were selected
because they were known to cause performance issues for
SDV. We added, randomly chosen, 14 other rules. These 28
rules on the 66 drivers produced a total of 1420 verication
instances (not all rules apply to all drivers).
Obtaining Annotations. The RTS suite consists of
fairly small drivers that are easy to prove manually. We,
however, automated the entire process by running a proof-
generating verier. Our implementation uses Duality [20]
but conceptually we could have used tools such as SLAM
and Yogi [4] as well.
Our objective is to learn a template per rule. For each of
the28rules, we (1) form a repository of programs that assert
the rule and their corresponding correctness proofs and (2)
dene the shared vocabulary Vto consist of model variables
of the rule as well as OS model variables. Every program
in a repository is guaranteed to include the corresponding
shared vocabulary as global variables.
4.2 Objective Relation
We restrict abstract annotations to be clauses , i.e., a dis-
junction of formulas. Conjunctions at the top level are broken
down into multiple annotations, one for each conjunct. For
convenience, we think of a clause as a set of formulas where
disjunction is implied between the elements of the set. The
empty set corresponds to true . Given two annotations a1
anda2, bya1a2we therefore designate that the formulas
ofa1are a subset of the formulas of a2. For a template t,
we dene tto betconsistently indexed by the set f1;:::;jtjg.
In other words, t=fa1;:::;ajtjgwhereai2t()ai2t.
456For convenience, we dene t[i] =ai. Given two templates t1
andt2, we sayt2issimplied (or simpler) than t1, written
t2t1, i (1)jt1j=jt2jand (2) t2[i]t1[i]for all 1in.
Ift2t1and there exists isuch that t2[i]t1[i], we sayt2
isstrictly simpler thant1, writtent2t1. Finally,t21t1
holds it2t1andjt1=t2j= 1. In other words, t2is strictly
simpler than t1but only at one abstract annotation; we then
sayt2is 1-simpler than t1. For example, suppose a template
t0istexcept that literal lis in t[1]but not in t0[1]. Then we
havet0t,t0t, andt01t.
Given a program pand a template t, leth(p;t)be the result
of running Houdini on annotates (p;t). Further, let cperf(p;t)
denote the number of procedures that Corral inlined if it
was able to prove correctness of h(p;t), and 1otherwise.
cperf(p;t)measures the performance of Corral. We use it
as a proxy for the running time that is independent of the
machine conguration. We now dene the corral objective
relation. Given a program pand two templates t1andt2,
t1!pt2holds i:
t21t1
0cperf(p;t2)2cperf(p;t1)
The above denitions encode our intention to nd those tem-
plates that are structurally simpler and smaller, if one views
making a clause empty as removing it. The reason why we
chose 1-simpler relation instead of the general simplication
is that we want to keep branching sets of the objective re-
lation tractable. This way, we are sacricing optimality for
better inference times. Also note that our objective relation
allows the performance of Corral to get worse when using t2.
But Corral must still be able to prove correctness. We allow
the degradation in performance to allow more opportunities
for the templates to get simplied. However, we still restrict
the performance to not get out of hand (not more than a
factor of 2). This allows us to kill the execution of Corral on
h(p;t2) as soon as it inlines twice as many procedures as on
h(p;t1), without waiting for a verdict.
Lastly, we dene a repository objective relation. For a
set of programs P=fp1;:::;p ngwe denet1!Pt2()V
it1!pit2. In other words, we want to infer templates that
are consistently optimal for all repository programs, subject
to the objective relation !p.
4.3 Implementation
We use a slightly modied version of Algorithm 2 for
computing a minimal repository template. We describe
the algorithm by only explaining the modications that we
introduced.
We rst note that the corral objective relation has to
be computed by actually running Corral. While perform-
ing a greedy descent, we simply enumerate all 1-simplied
templates and run Corral on each of them. This run tells
us whether the program can be proved and the number of
inlined procedures, if any. Also, we enumerate rst those an-
notations where the corresponding simplied clause is empty.
Then, we enumerate annotations where the simplied clause
has one literal, and then when it has two, and so forth, until
we have enumerated all 1-simplied templates. This way, we
are heuristically choosing the well-founded chains of smaller
lengths while searching for a minimal template.
The second modication we introduce has the purpose of
keepingCsmall in Algorithm 2. While computing a minimaltemplate for a single program, we rst check whether any
of the previously computed minimal templates is perhaps
minimal for the new program. If so then we continue by ana-
lyzing the next program. As a result, the number of distinct
minimal templates in mints reduces, and so does the size of
C. Further, due to the denition of proc;p , our algorithm
can produce the same annotated program for dierent tem-
plates. In that case, the result of running Houdini+Corral
for the rst template is same as the result of running them
on the other template. We therefore use the concretized
annotations to cache Corral's outcome and reuse the results
when possible.
We set a timeout of 8hours for the algorithm. If the
execution reaches a timeout, we simply return the current
value ofC. In practice, for SDV, the template inference
needs to be performed just once in a release cycle; hence,
one can devote much more time for inference.
5. EV ALUATION
We implemented our algorithms in a tool called ProofMin-
imization5. It accepts a list of annotated programs written
in Boogie [19] as input and computes the minimal repository
template. The shared vocabulary is automatically dened to
be the set of global variables common to all input programs.
The implementation is a simple wrapper, of less than 1000
lines of C# code, around the implementation of Corral [18].
We ran our experiments on a cluster of 6 identical server-
class machines. Each of the servers had Intel Xeon CPUs
1.8 GHz, 64 GB RAM and 16 logical processors. The total
CPU time of our experiments exceeded well over a month.
We relied on parallelism extensively to produce results in a
reasonable amount of time.
Training Modes. Our experiments evaluate two dier-
ent ways of using the inferred templates. In the rst mode,
called MT, the inferred templates only augment the manual
templates. This is achieved by adding the manual templates
to each (p;t)pair in our program repository and requiring
our algorithm to never throw out a manually-generated an-
notation. This mode of operation is more controlled: it does
not seek to replace existing manual eort, but rather to just
augment it.
The second mode of operation, called NT, does not use
manual templates at all. This simulates the scenario when
no manually-generated templates had been added to SDV.
It answers how much of the manual eort behind the design
of manual templates can be automated using our techniques.
Clearly, the quality of inferred templates will depend on
the quality of the training set, i.e., the repository used for
inferring a minimal repository template. Because it was never
intended to use RTS for inferring templates, the training sets
are sometimes inadequate for our approach. For instance,
for a few rules, RTS only contains buggy drivers. (Inferring
annotations from buggy programs is an interesting problem,
but outside the scope of this paper.) Thus, we also evaluate
expanding our training set by sampling a randomly-chosen
fraction of the 66real drivers and including them in the
training set. The test set, i.e., the set of unseen drivers over
which we evaluate the inferred templates then shrinks to the
remaining drivers.
5Implementation is available at https://github.com/
boogie-org/corral/tree/master/AddOns/pminbench along
with supplemental material and examples that the reader
can use to experiment.
457Figure 4: Annotation inference times for the 28
rules.
LetTrain (f%;m), where 0f100andmis either
MT or NT, refer to the experiment where the training set
consisted of all RTS drivers and fpercent of the real drivers,
and the inference was done in mode m. An exception is
the special case of Train (100%;MT ) which denotes that all
drivers were used in the training set as well as the test set.
We use Train (100%;MT ) as a limit study on the quality of
annotations that we can infer.
Results. For the experiment Train (0%, NT), the running
time of ProofMinimization is shown in Figure 4. It takes
just a couple of minutes for some rules while it timeouts
after 8hours for ve rules. Not including the rules for which
ProofMinimization times out, it takes roughly 2 :5 hours
on average to compute the result.
The results with various dierent training modes are re-
ported in Tables 1 and 2. Each of the tables compares three
versions of SDV. The version called \None" does not use any
templates and captures the performance of Corral without
any annotations. \Manual" refers to using manual templates,
which is the currently-shipping production system. \Inferred"
refers to using the set of annotations inferred by our tool.
Each of the tables measures performance in terms of the num-
ber of timeouts (#TO), number of times the coverage bound
was hit (#Bnd), the number of bugs reported (#Bugs), the
average running time of Houdini+Corral (Avg), and the
average running time of just Houdini (Houd).
Table 1(left) compares performance for Train (0%, MT).
In this mode, our inferred set of annotations was empty for 12
of the 28rules. The inferred set can be empty when the RTS
wasn't rich enough, or because the manual templates were
sucient. In this case, the performance of \Inferred" matches
with that of \Manual". Table 1 compares performance on the
remaining 16rules where we did infer annotations. These
results demonstrate that our extra set of annotations are
useful. The number of timeouts come down a fraction and
the number of times the coverage bound was hit comes down
signicantly. All of these previously inconclusive cases, 54
in number, convert to a proved verdict. In summary, the
total number of inconclusive answers drops down by 47%.
Moreover, even though Houdini ran slower because of the
extra annotations, the performance improvement in Corral
made the overall system much faster (22%).
ForTrain (30%, MT), we did three runs, each time sam-
pling a dierent fraction of the drivers. (The variance acrossthe three runs was very small, thus, we stopped with three
runs.) The results for \Inferred" were averaged across the
three runs and are shown in Table 1 (right). It is interesting
that performance is similar to Train (0%, MT). Using a
fraction of drivers did not provide much new information.
However, the results of Train (100%, MT) shown in Table 2
(left) do indicate that drivers potentially carry information
not present in RTS. Learning over all drivers increased the
quality of inferred annotations signicantly (even though the
running time of Houdini is highest in this setting).
Table 2 (right) shows results of Train (0%, NT). With no
help from manual templates, Corral's performance depends
even more signicantly on the inferred set of annotations.
We were able to achieve a similar quality of results compared
to using manual templates. There was a near-equal split
between rules on which inferred annotations do better and
ones on which manual templates do better. In consultation
with the SDV team, we realized that the exercise of setting up
manual templates often borrowed annotations across rules,
where annotations useful for one rule would be generated
for similar rules as well. In our setting, we did not explore
sharing information between rules.
The number of abstract annotations per template, on
average, was 12:78for the manual templates. The Train (0%,
MT) experiment produced 13 :63 annotations per template,
on average. The Train (100%, MT) experiment produced
16:6 annotations per template, on average. The Train (0%,
NT) experiment produced 3:3annotations per template,
on average, which is signicantly smaller than the manual
templates.
We supply the list of all rules and templates in the sup-
plemental material. Here we summarize examples of useful
annotations that our technique was able to infer, but were
missed by experts. Firstly, for IRQL-based rules, the annota-
tion$floc == old($floc) was necessary for establishing
important loop invariants such as \the IRQL value is un-
changed across loop iterations" (see example in Figure 3 and
discussion in Sections 2.2 and 2.5). While the experts an-
ticipated the latter (i.e., IRQL is unchanged) to be a useful
annotation, they did not expect that an invariant over local
variables would be required to prove it inductively.
A second instance of useful annotations found by our
technique is for SDV rules with multiple model variables.
Manual templates mostly have annotations that capture the
eect of a procedure's execution on a single model variable
[28]. However, these are insucient to capture inter-variable
relationships. We infer several annotations over multiple
variables which helped signicantly for such rules.
To summarize, Train (0%, MT) results show that we can
signicantly improve the performance of a production sys-
tem by augmenting existing manual eort. The results for
Train (0%, NT) show that as new rules are developed and
test cases are added to RTS, we can automatically generate
useful annotations avoiding the need for further manual eort
in coming up with new templates.
6. RELATED WORK
This work falls into the category of predicting program
properties from codebases. For example, JSNice learns from
Javascript repositories on GitHub and predicts more legible
identier names and (unveried) type annotations [24]. In
contrast, this work is the rst attempt at inferring annota-
tions from verication histories and demonstrating their use
458Table 1: Left: Results for Train(0%, MT) on a total of 16rules with 873verication instances. Right: Results
for Train(30%, MT), averaged across three runs, on a total of 25rules with 1002verication instances.
Time (sec)
Cong #TO #Bnd #Bugs Avg Houd
None 77 228 46 94.3 0
Manual 27 88 46 71.9 10.0
Inferred 24 37 46 51.6 12.1Time (sec)
Cong #TO #Bnd #Bugs Avg Houd
None 112 265 64 104.3 0
Manual 50 91 64 70.5 10.1
Inferred 46.6 41 64 59.4 14.6
Table 2: Left: Results for Train(100%, MT) on a total of 28rules with 1420 verication instances. Right:
Results for Train(0%, NT) on a total of 28rules with 1420verication instances.
Time (sec)
Cong #TO #Bnd #Bugs Avg Houd
None 143 342 104 98.1 0
Manual 55 123 108 58.4 9.3
Inferred 45 25 108 46.1 16.2Time (sec)
Cong #TO #Bnd #Bugs Avg Houd
None 143 342 104 98.1 0
Manual 55 123 108 58.44 9.3
Inferred 59 92 108 56.5 9.1
in an industrial-scale verication setting. Other approaches
use codebases to predict dierent program properties rather
than annotations [22, 23, 25]. For instance, work presented
in [23] applies Bayesian optimization on existing codebases
to learn a strategy for deciding for which part of an unseen
program a static analyzer should sacrice time for precision
while performing the analysis.
There are verication and testing approaches that lever-
age previous versions of a program under analysis. For
example, [11] improves performance of test generation for a
program by leveraging existing tests belonging to a previous
version. Regression verication veries the equivalence of
two successive versions of a program [12]. For similar pro-
grams, [12] argues that this verication task is easier than
our goal here, i.e., formal verication of a stand alone pro-
gram. A recent generalization of regression verication is
dierential assertion checking where a verier checks that a
bug is not introduced in going from one program version to
another [14]. Techniques in [7] extrapolate from predicates
used for verifying a program under sequential consistency
to verify the same program under a relaxed memory model.
Incremental verication attempts to reuse annotations cor-
responding to a program in the verication of its updated
version [26, 8, 5]. In these works, the history typically con-
sists of revisions of the program under analysis. We consider
histories with programs that use the same kernel API but
can be very dierent otherwise. For example, we can use
annotations inferred from a storage and a modem driver to
help verify an IEEE-1394 bus driver. Also, our technique
uses the minimization mechanism to reduce the size of the
accumulated annotations in order to retain practicality.
The techniques described in this paper can be used to infer
invariants using verication histories. Previous approaches
to invariant inference perform analysis only over the program
under consideration. These include invariant inference using
static analysis [6] or learning from concrete executions [10,
27]. Our work is complementary to these.
7. CONCLUSION
We present an algorithm for computing a small set of use-
fulprogram annotations from a repository of past verication
runs. We used this algorithm to improve the performance
of SDV by generating better quality annotations than those
produced by human experts. By utilizing the inferred an-notations, we reduce the number of inconclusive answers
by47% while running 22% faster on average, even for a
heavily-optimized system.
Our approach beneted from SDV's separation of verifying
driver correctness into checking multiple rules. By concen-
trating on one rule at a time, our inferred annotations were
tailored to a specic property of drivers. In future work,
we wish to study the generality of our algorithm in other
verication domains that rely on annotations for constructing
program proofs.
8. ACKNOWLEDGMENTS
The authors would like to thank Zilong Wang and Subhajit
Roy for their help during initial development of the ideas
presented in this paper, and Kenneth McMillan for his help
with using the Duality verication engine. This work was in
part supported by the National Science Foundation under
grant CCF-1350574.
9. REFERENCES
[1] A. Albarghouthi, A. Gurnkel, Y. Li, S. Chaki, and
M. Chechik. Ufo: Verication with interpolants and
abstract interpretation. In Tools and Algorithms for the
Construction and Analysis of Systems , pages 637{640.
Springer, 2013.
[2] T. Ball, V. Levin, and S. K. Rajamani. A decade of
software model checking with SLAM. Communications
of the ACM , 54(7):68{76, 2011.
[3] T. Ball, R. Majumdar, T. D. Millstein, and S. K.
Rajamani. Automatic predicate abstraction of C
programs. In Proceedings of the 2001 ACM SIGPLAN
Conference on Programming Language Design and
Implementation (PLDI), Snowbird, Utah, USA, June
20-22, 2001 , pages 203{213, 2001.
[4] N. E. Beckman, A. V. Nori, S. K. Rajamani, R. J.
Simmons, S. Tetali, and A. V. Thakur. Proofs from
tests. IEEE Trans. Software Eng. , 36(4):495{508, 2010.
[5] D. Beyer, S. L owe, E. Novikov, A. Stahlbauer, and
P. Wendler. Precision reuse for ecient regression
verication. In Joint Meeting of the European Software
Engineering Conference and the ACM SIGSOFT
Symposium on the Foundations of Software
459Engineering, ESEC/FSE'13, Saint Petersburg, Russian
Federation, August 18-26, 2013 , pages 389{399, 2013.
[6] P. Cousot and R. Cousot. Abstract interpretation: A
unied lattice model for static analysis of programs by
construction or approximation of xpoints. In
Conference Record of the Fourth ACM Symposium on
Principles of Programming Languages, Los Angeles,
California, USA, January 1977 , pages 238{252, 1977.
[7]A. M. Dan, Y. Meshman, M. T. Vechev, and E. Yahav.
Predicate abstraction for relaxed memory models. In
Static Analysis - 20th International Symposium, SAS
2013, Seattle, WA, USA, June 20-22, 2013.
Proceedings , pages 84{104, 2013.
[8] G. Fedyukovich, A. Gurnkel, and N. Sharygina.
Incremental verication of compiler optimizations. In
NASA Formal Methods - 6th International Symposium,
NFM 2014, Houston, TX, USA, April 29 - May 1,
2014. Proceedings , pages 300{306, 2014.
[9] C. Flanagan and K. R. M. Leino. Houdini, an
annotation assistant for esc/java. In FME 2001:
Formal Methods for Increasing Software Productivity,
International Symposium of Formal Methods Europe,
Berlin, Germany, March 12-16, 2001, Proceedings ,
pages 500{517, 2001.
[10] P. Garg, C. L oding, P. Madhusudan, and D. Neider.
ICE: A robust framework for learning invariants. In
Computer Aided Verication - 26th International
Conference, CAV 2014, Held as Part of the Vienna
Summer of Logic, VSL 2014, Vienna, Austria, July
18-22, 2014. Proceedings , pages 69{87, 2014.
[11] P. Godefroid, S. K. Lahiri, and C. Rubio-Gonz alez.
Statically validating must summaries for incremental
compositional dynamic test generation. In Static
Analysis - 18th International Symposium, SAS 2011,
Venice, Italy, September 14-16, 2011. Proceedings ,
pages 112{128, 2011.
[12] B. Godlin and O. Strichman. Regression verication:
proving the equivalence of similar programs. Softw.
Test., Verif. Reliab. , 23(3):241{258, 2013.
[13] T. A. Henzinger, R. Jhala, R. Majumdar, and K. L.
McMillan. Abstractions from proofs. In Proceedings of
the 31st ACM SIGPLAN-SIGACT Symposium on
Principles of Programming Languages, POPL 2004,
Venice, Italy, January 14-16, 2004 , pages 232{244,
2004.
[14] S. K. Lahiri, K. L. McMillan, R. Sharma, and
C. Hawblitzel. Dierential assertion checking. In Joint
Meeting of the European Software Engineering
Conference and the ACM SIGSOFT Symposium on the
Foundations of Software Engineering, ESEC/FSE'13,
Saint Petersburg, Russian Federation, August 18-26,
2013, pages 345{355, 2013.
[15] A. Lal and S. Qadeer. Powering the static driver
verier using corral. In Proceedings of the 22nd ACM
SIGSOFT International Symposium on Foundations of
Software Engineering, (FSE-22), Hong Kong, China,
November 16 - 22, 2014 , pages 202{212, 2014.
[16] A. Lal and S. Qadeer. A program transformation for
faster goal-directed search. In Formal Methods in
Computer-Aided Design, FMCAD 2014, Lausanne,
Switzerland, October 21-24, 2014 , pages 147{154, 2014.[17] A. Lal and S. Qadeer. DAG inlining: a decision
procedure for reachability-modulo-theories in
hierarchical programs. In Proceedings of the 36th ACM
SIGPLAN Conference on Programming Language
Design and Implementation, Portland, OR, USA, June
15-17, 2015 , pages 280{290, 2015.
[18] A. Lal, S. Qadeer, and S. K. Lahiri. A solver for
reachability modulo theories. In Computer Aided
Verication - 24th International Conference, CAV 2012,
Berkeley, CA, USA, July 7-13, 2012 Proceedings , pages
427{443, 2012.
[19] K. R. M. Leino. This is boogie 2. Manuscript KRML ,
178:131, 2008.
http://https://github.com/boogie-org/boogie.
[20] K. L. McMillan and A. Rybalchenko. Computing
relational xed points using interpolation. Technical
report, Technical report, 2012. available from authors,
2013.
[21] Microsoft. The Static Driver Verier.
http://msdn.microsoft.com/en-us/library/windows/
hardware/552808(v=vs.85).aspx.
[22]A. Mishne, S. Shoham, and E. Yahav. Typestate-based
semantic code search over partial programs. In
Proceedings of the 27th Annual ACM SIGPLAN
Conference on Object-Oriented Programming, Systems,
Languages, and Applications, OOPSLA 2012, part of
SPLASH 2012, Tucson, AZ, USA, October 21-25, 2012 ,
pages 997{1016, 2012.
[23] H. Oh, H. Yang, and K. Yi. Learning a strategy for
adapting a program analysis via bayesian optimisation.
InProceedings of the 2015 ACM SIGPLAN
International Conference on Object-Oriented
Programming, Systems, Languages, and Applications,
OOPSLA 2015, part of SLASH 2015, Pittsburgh, PA,
USA, October 25-30, 2015 , pages 572{588, 2015.
[24] V. Raychev, M. T. Vechev, and A. Krause. Predicting
program properties from big code . InProceedings of
the 42nd Annual ACM SIGPLAN-SIGACT Symposium
on Principles of Programming Languages, POPL 2015,
Mumbai, India, January 15-17, 2015 , pages 111{124,
2015.
[25] V. Raychev, M. T. Vechev, and E. Yahav. Code
completion with statistical language models. In ACM
SIGPLAN Conference on Programming Language
Design and Implementation, PLDI '14, Edinburgh,
United Kingdom - June 09 - 11, 2014 , page 44, 2014.
[26] O. Sery, G. Fedyukovich, and N. Sharygina.
Incremental upgrade checking by means of
interpolation-based function summaries. In Formal
Methods in Computer-Aided Design, FMCAD 2012,
Cambridge, UK, October 22-25, 2012 , pages 114{121,
2012.
[27] R. Sharma and A. Aiken. From invariant checking to
invariant inference using randomized search. In
Computer Aided Verication - 26th International
Conference, CAV 2014, Held as Part of the Vienna
Summer of Logic, VSL 2014, Vienna, Austria, July
18-22, 2014. Proceedings , pages 88{105, 2014.
[28] R. S. Zvonimir Pavlinovic, Akash Lal. Inferring
annotations for device drivers from verication histories.
Technical report, Microsoft Research, April 2016.
460