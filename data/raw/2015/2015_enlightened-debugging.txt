Enlightened Debugging∗
Xiangyu Li
 Shaowei Zhu
 Marcelo d’Amorim
 Alessandro Orso
Georgia Institute of Technology
 Federal Univesity of Pernambuco
Atlanta, GA 30332-0765, USA Recife, PE 50740-560, Brazil
{xiangyu.li, swzhu, orso}@cc.gatech.edu damorim@cin.ufpe.br
ABSTRACT
Numerous automated techniques have been proposed to reduce
the cost of software debugging, a notoriously time-consuming and
human-intensiveactivity.Amongthesetechniques,StatisticalFault
Localization (SFL) is particularly popular. One issue with SFL is
that it is based on strong, often unrealistic assumptions on how
developersbehavewhendebugging.Toaddressthisproblem,we
propose Enlighten, an interactive, feedback-driven fault localiza-
tiontechnique.Givenafailingtest,Enlighten(1)leveragesSFLand dynamic dependence analysis to identify suspicious method
invocationsandcorrespondingdatavalues,(2)presentsthedevel-
oper with a query about the most suspicious invocation expressed
in terms of inputs and outputs, (3) encodes the developer feedback
on the correctness of individual data values as extra program spec-
ifications, and (4) repeats these steps until the fault is found. Weevaluated Enlighten in two ways. First, we applied Enlighten
to 1,807 real and seeded faults in 3 open source programs using an
automatedoracleasasimulateduser;forover96%ofthesefaults,
Enlightenrequiredlessthan10interactionswiththesimulated
user to localize the fault, and a sensitivity analysis showed that the
results were robust to erroneous responses. Second, we performed
anactualuserstudyon4faultswith24participantsandfoundthat
participantswhousedEnlightenperformedsignificantlybetter
than those not using our tool, in terms of both number of faults
localized and time needed to localize the faults.
CCS CONCEPTS
•Software and its engineering →Software testing and de-
bugging;
KEYWORDS
debugging, fault localization, dynamic analysis
∗In the title, we use the term “enlightened” with its physical, rather than spiritual,
meaning of “well informed.” The technique we propose is well informed because it
incorporates user feedback in its otherwise automated debugging process.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthe firstpage.Copyrights forcomponentsof thisworkowned byothersthan the
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/or a fee. Request permissions from permissions@acm.org.
ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
©2018 Copyright held by the owner/author(s). Publication rights licensed to Associa-
tion for Computing Machinery.
ACM ISBN 978-1-4503-5638-1/18/05...$15.00
https://doi.org/10.1145/3180155.31802421 INTRODUCTION
Software debugging is a notoriously difficult, time consuming, and
human-intensive activity. The task of locating the faulty code (i.e.,
fault localization ), specifically, is one of the most challenging parts
of debugging. For this reason, countless techniques have been pro-
posedovertheyearstohelpdevelopersdecreasethecostoffault
localization(anddebuggingingeneral).Amongtheseapproaches,
one that has been particularly successful is statistical fault localiza-
tion (SFL) (e.g., [6–9, 15, 16, 23, 27, 33, 34]).
The intuition behind SFL is that dynamic data collected while
running passing and failing test cases can be used to perform a
statistical analysis and to compute a suspiciousness value for each
entity (e.g., statement, basic block, or predicate) in the program.
Basically, the suspiciousness of a code entity should be directly
(resp., inversely) proportional to the number of failing (resp., pass-
ing) test cases that traverse that entity. Although SFL has been adisruptive change in the area of debugging, and has generated a
tremendousamountoffollowupresearch,ithassomesignificant
limitations.MostSFLtechniquestendtomakestrong,oftenunreal-
isticassumptionsonhowdevelopersbehavewhendebugging.In
particular,previousworkhasshownthatitisunrealistictoassume
that developers provided with a possibly long list of suspiciousstatements would go through this list in order and immediately
spotthefaultwhentheyseeit,withoutanyadditionalcontext[ 25].
To address these limitations of SFL, while still taking advantage
ofitsstrengths,weproposeEnlighten,aninteractive,feedback-
driven fault localization technique. We defined Enlighten so as
to follow the way in which debugging is typically performed, with
thegoal ofhelping—rather thancompletely replacing—thehumans
intheloop.Typically,developerswouldstudythefailureathand,
makehypothesesonwhatthecause(s)ofthefailuremaybe,and
examineasubsetoftheexe cutionthatcanconfirmordisprovetheir
hypothesis.Theywouldthenleveragetheadditionalunderstanding
ofthefailureacquiredinthisprocesstomakenewhypothesesor
refinetheexistingones,examineadditionalsubsetsoftheexecution,
and so on. This process would continue until either the developers
give up or they find the fault.
Enlightenaimstomimicandsupportthisprocessasfollows.
First, it uses traditional SFL to formulate an initial hypothesis of
where the fault may be. Second, it identifies a relevant subset of
theexecutionthatcanhelpsupportornegatetheformulatedhy-
pothesis. Intuitively, this execution subset is identified in the form
of amethod invocationthat resultsin the executionof highlysus-
piciousentities.Third,Enlightenpresentsthedeveloperwitha
queryabouttheidentifiedmethodinvocation,expressedintermsof
822018 ACM/IEEE 40th International Conference on Software Engineering
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:50:07 UTC from IEEE Xplore.  Restrictions apply. Dynamic 
Dependency 
Graph (DDG)
Feedback
Analyzer
Incorrect
Data Values
Test Results
and Coverage
✔✔✘ SFL 
Calculator
Ranked List
of Statements1. ---------
2. ----------
3. -------
4. --------
5. ----------
6. …
Query
GeneratorDebugging Data
Debugging
Query
✘✔✔✔✔✔
FeedbackTest
Runner
Dependency
Analyzer
Test SuiteProgram
ENLIGHTEN
qn op
Figure 1: Overview of the approach.
theinputandresultingoutputoftheinvocation.Fourth,Enlighten
collects the developer feedback on the correctness of individual
data values in the provided inputs and outputs and encodes this
feedbackasextraprogramspecifications(i.e.,extrateststhatcan
improve the SFL results). Finally, Enlighten repeats these steps
until the fault is found or the developer decides to stop.
Ourapproachcanovercometheimportantlimitationsoftradi-
tionalSFLthatwehighlightedearlier.Specifically,Enlightendoes
notrequiredeveloperstodecidewhetherastatementinisolation
iscorrect,butrathertocheckhigh-levelinput-outputrelationships
at the method level. We believe that operating at the level of ab-straction of a method, whose semantics is often well understood
by developers, can make the technique considerably more effec-
tive and usable. Moreover, developers can skip queries that theycannot answer and, as shown in our evaluation, the technique isresilient to occasional erroneous responses. Basically, when suc-cessful, Enlighten can nicely guide the developers towards the
faultbyfollowinganiterativeprocessthatgetstheirinputatalevel
of abstraction they can typically understand.
To evaluate Enlighten, we implemented the approach and per-
formedtwoempiricalstudies.Inourfirststudy,weusedanauto-
matedoracletosimulatethepresenceofadeveloperandapplied
Enlighten to a large number of faults in 3 open source programs.
Thefaultsweconsideredincluded27realfaultsand1,780mutation
faults,whichwegeneratedtoincreasethenumberofdatapoints.As
our results show, for over 96% of the faults considered, Enlighten
required less than 10 interactions with the simulated user to local-
ize the fault. In the second part of our evaluation, we performed
anactualuserstudy.Weselected4realfaultsand24participants
and assigned to each participant two debugging tasks: one to beperformed using Enlighten, and one to be performed using the
debuggingtechnique(s)oftheirchoice.WhenusingEnlighten,the
participantsperformedconsiderablybetterthanwhendebugging
withoutthehelpofourtool.Theimprovementwassignificantin
terms ofboth number offaults localized andtime needed tolocal-
ize the faults. Overall, we believe that our results provide a clear
indication that Enlighten is a promising approach for debugging
and fault localization.The main contributions of this paper are:
•Enlighten,anewdebuggingtechniquethatimprovestra-
ditional fault localization approaches by incorporating user
feedback.
•AtoolthatimplementsEnlightenandthatispubliclyavail-
able,togetherwithourexperimentdataandinfrastructure
(http://www.cc.gatech.edu/~orso/software/enlighten/).
•Twocomplementaryevaluationsof Enlighten:anextensive
analytical study with simulated users, in which we evalu-ate Enlighten on a large number of faults; and an actualuser study, in which we evaluate Enlighten in a realistic
debugging scenario involving real users.
2 APPROACH
Figure1providesahigh-levelviewof Enlightenandshowsinput
(left side), output (right side), and main components of the tech-
nique (inside the box). As the figure shows, Enlighten takes as
inputaprogramanditstestsuiteandproducesasoutputthelikely
locationofthefault.Thefaultlocalizationprocessof Enlighten
is iterative and user-driven, as indicated by the loop and the de-
veloper’s avatar in the figure. Intuitively, at the beginning of the
process, Enlighten has limited knowledge about what may be
causing a failure. Each iteration, however, adds relevant debugging
information to Enlighten’s knowledge base, which helps eventu-
ally locating the bug. In the following, we first briefly describe the
main components of the technique and then discuss them in detail.
1)TheTestRunnerandDependencyAnalyzer componenttakesas
input the faulty program and a test suite for the program and
computes,foreachtest,adynamicdependencegraph,testresults,
coverage information, and a set of incorrect data values.
2)TheSFLCalculator usesthetestresultsandthecoverageinfor-
mationtoproducearankedlistofsuspiciousstatements,usinga
traditional SFL approach.
3)TheQuery Generator takes as input the program, its test suite,
and the artifacts produced by the Test Runner and Dependency
Analyzer, and generates debugging queries using the SFL results.
Each query consists of a method invocation, together with itsinputs (parameters plus relevant state) and outputs (including
sideeffects),whichthedevelopercanmarkascorrectorincorrect.
83
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:50:07 UTC from IEEE Xplore.  Restrictions apply. 4)TheFeedbackAnalyzer takesasinputtheresponsetoadebugging
query. If the developer has found the bug, the process stops.
Otherwise,theFeedbackAnalyzerupdatesthedebuggingdata
based on the developer feedback and performs another iteration.
Conceptually, Enlighten can operate with test suites that con-
taintestcasestriggeringdifferentfaults.Multiplefaultscannega-
tivelyaffecttheinitialSFLresults.However,becauseEnlighten
generates queries for a specific test case, the feedback provided
by the user should overcome the noise introduced by the multiple
faults. Moreover, there are several techniquesthat can cluster test
cases that fail for similar reasons (e.g., [ 5,14]) and that could be
used to “specialize” the test suite before applying Enlighten.
It is also worth noting that debugging queries do notneedtobe
formulatedatthegranularityofmethodinvocations,andalternative
partial program executions could be used instead. We choose to
usemethodcallsbecausemethodsareafundamentalabstraction
developersusetoreasonaboutprogramsemantics,andthebehavior
of many methods should be well understood by the developers.
2.1 Test Runner & Dependency Analyzer
The Test Runner is a traditional driver that takes a program and its
testsuiteasinputandproducesasoutputthetestresults(passor
fail),coveragedata,andasetofincorrectvalues.Thislatterisasetof
incorrectvaluesthatisinitialized,foreachtestcase,withthevalue
associated with the corresponding failure. (The Feedback Analyzer
thenaddstothesetvaluesmarkedaserroneousbythedevelopersin
response to queries.) In our implementation of Enlighten, which
is for Java programs, a test failure can result in either an uncaught
exception or a failing assertion. In these cases, the value associated
with the failure is the reference to the object corresponding to the
uncaught exception or the failed assertion, respectively.
The Dependency Analyzer, conversely, produces a Dynamic
Dependence Graph (DDG) for every failing test in the test suite. In
theDDG,nodesrepresentoccurrencesofstatementsintheprogram,
whereas edges represent dynamic (data or control) dependences
between these statements. As it is traditionally done, statements
that containmorethan one definitionare split so thateach node
contains at most one definition [11].
2.2 SFL Calculator
EnlightenusesamodifiedversionofOchiai[ 3]toperformSFL.
WeselectedOchiaibecauseithasbeenshowntoperformwellin
practice. The specific formula we use to compute the suspicious-
nessofastatement sissusp(s)=aef//radicalBig/parenleftbigaep+aef/parenrightbig×/parenleftbigaef+anf/parenrightbig.Inthe
formula,aef(resp.,anf) denotes the number of failing tests that
covered (resp., did not cover) s. The term aepdenotes the sum of
theweightsofthepassingteststhatcovered s(asopposedtothe
numberofpassingteststhatcovered sintheoriginalformula).The
approach assigns weight 0 .1 to the tests in the initial test suite and
weight1tothevirtual teststhatencodethefeedbackprovided by
the user (see Section 2.4). The rationale for this decision is that we
want the human feedback to have a high impact on the SFL results,
as it relates to very focused partial executions. We picked these
specific numbersso thatthere isan order ofmagnitude difference
betweenthetwo.Infuturework,weplantoexperimentwithdif-
ferent weights and better understand their effect. In computing theformula, statements that are not executed in any test are assigned
a suspiciousness score of 0.
2.3 Query Generator
Enlighteninteractswiththedeveloperthroughdebuggingqueries,
which are expressed in terms of inputs and outputs of a suspicious
methodinvocationandareaboutthecorrectnessofthatinvocation.
The query generation process consists of (1) selecting a failing test,
(2)selectingasuspiciousmethodinvocation,and(3)producinga
debugging query. We now describe each of these steps.
2.3.1 Selecting a Failing Test. Enlighten generates debugging
queries for a failing test. When multiple failing tests exist, and de-
velopersdonotspecifytheirtestofchoice,thetechniqueselectsthe
test that makes the smallest number of method calls. The rationale
is that shorter traces should be easier to debug.
2.3.2 Selecting a Suspicious Method Invocation. Before describ-
ing how Enlighten selects suspicious method invocations, wemust define the concept of value suspiciousness. Traditional SFL
techniques (e.g., [
3,15]) associate suspiciousness scores to pro-
gram statements. Enlighten uses these scores to compute thesuspiciousness of values defined within a dynamic method invo-
cation (i.e., a specific runtime instance of a method execution).
Inthefollowing, slice(v,invoc )denotesthedynamicbackward
slice associated withvalue definition v, limited todynamic method
invocation invoc,v.instrdenotes the instruction associated with
definition v(i.e., the instruction that defines v), andsusp(instr)
denotes the suspiciousness score of instruction instr, as computed
by the SFL calculator. Enlighten computes the suspiciousness
of a value definition vfor a dynamic method invocation invoc
in two steps. First, it computes the base suspiciousness of vas
base_susp(v,invoc)=max{susp(v/prime.instr)|v/prime∈slice(v,invoc)}.I t
thencomputestheactualvaluesuspiciousnessof v(val_susp(v))
basedonwhether vaffects,throughdirectorindirectdependencies,
valuesalreadyknowntobeincorrect.Specifically,Enlightencom-
putesval_susp(v)by multiplying base_susp(v)by anamplifying
factorthat is equal to either 1, if no previously labeled incorrect
valuedependson v,or1plusthenumberofincorrectvaluesthat
dependon v,otherwise.Intuitively,valuesthataffectothersthat
developers previously labeled as incorrect are more suspicious.
To select the method invocation for the next query, Enlighten
considers the output of all the invocations within the (failing) test
execution being considered, where the output includes the state
of the objects passed as parameters, the values of the modified
global variables and objects, and the return value (or exceptionthrown).
1For each such output item, Enlighten computes the
corresponding valuesuspiciousness (i.e.,the value suspiciousness
of the corresponding definition). It then identifies the outputs with
the highest value suspiciousness and selects the corresponding
method invocation. In case of ties, Enlighten prioritizes methods
higher in the call chain and chooses randomly when all conditions
are equal.
2.3.3 Producing a Debugging Query. Conceptually, a query is a
setofquestionsintheform“Isthisvaluecorrect?”.Specifically,En-lightenreportstothedevelopertheinputsandoutputsofamethod
1EnlightencurrentlyignoresdatawrittenthroughI/Ooperations,whichcouldbe
added through additional engineering.
84
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:50:07 UTC from IEEE Xplore.  Restrictions apply. 1Set<Test> passingTests = ...
2Set<Value> incorrectValues = ...
3
4incorporateFeedback(Feedback feedback) {
5 if(feedback.isIncorrectInput()) {
6 incorrectValues.addAll(
7 feedback.getIncorrectInputValueDefs());
8 return;
9 }
10 for (Value v : feedback.getCorrectOutputValueDefs()) {
11 Test virtualTest = new Test();
12 virtualTest.setCoverage(slice(v, feedback.invoc));
13 passingTests.add(virtualTest);
14 }
15 if(feedback.hasIncorrectOutput()) {
16 response = askIfFaultFound();
17 if(response == ``yes'') return;
18 Set<Instr> directCov = feedback.invoc.getDirectCoverage();
19 if(response == ``no'') removeCoverage(directCov);
20 else {// ``I don't know''
21 Test virtualTest = new Test();
22 virtualTest.setCoverage(directCov);
23 }
24 incorrectValues.addAll(
25 feedback.getIncorrectOutputValueDefs());
26 Set<Instr> transitiveCov =
27 feedback.invoc.getTransitiveCoverage();
28 restrictSflTo(transitiveCov);
29 }}
Figure 2: Algorithm for incorporating feedback.
callthatproducesthemostsuspiciousoutput(seeSection2.3.2)and
highlightsthedatavalueswithvariouscolors(andtransparency)
toindicatetheirrelativesuspiciousness.Figure4showsanexam-
ple of a debuggingquery where the field numElems is classified as
highly suspicious. Developers can answer positively or negatively
to any number of questions in a debugging query. A positive (resp.,
negative) answer indicates that the developer believes the value
is correct (resp., incorrect) for that specific invocation. Intuitively,
labelinganoutputasincorrectindicatesthatthebugiseitherinthe
methoditselforinoneofthemethodsitcalls.Notethatdevelopers
can also label an input as incorrect (e.g., an unexpected nullvalue);
labelinganinputasincorrecttellsEnlightentoignorethecurrentinvocationandfocusonmethodsthatledtothisinvocationinstead.
2.4 Feedback Analyzer
Figure2showsthealgorithmforincorporatingthefeedbackpro-
vided by developers through their answers to debugging queries
(seeFigure1).Globalvariables passingTests andincorrectValues ,
declared at lines 1 and 2, store coverage information for passingtests and a set of known incorrect values observed during a de-
buggingsession.Enlightenincorporatesfeedbackbymodifying
these sets. At lines 5–9, the algorithm handles the case of the de-
veloper marking some values on the input as incorrect; method
getIncorrectInputValueDefs returnsthesetofvaluedefinitions
specified as incorrect by the developer, and the algorithm adds
those values to the set incorrectValues and returns.
Lines 10–14 handle the case in which the developer has labeled
some output values as correct. In this case, Enlighten creates a
passing virtual test for each value vlabeled as correct and updates
the debugging information accordingly: function slicecomputes
the dynamic backward slice from v, and function setCoverage
marks thestatements in theslice ascovered by thenewly created
virtual test. Intuitively, adding passing virtual tests reduces the
suspiciousness of statements related to the computation of v.
Lines 15–29 handle the case in which the developer has labeled
some output values as incorrect, which indicates that there maybe faults in the current method or in one of the methods it calls.Enlightenthereforeasksthedevelopertocheckwhetherthefault
is in the code of the current method and to provide one of threepossible answers:
yes,no,Idon/primetknow(line 16). If the developer’s
answer is yes, the fault localization process ends (line 17). If the
answeris no,Enlightenmarksallthestatementsinthemethod
as not covered (by any test), which has the effect of setting to
zero thesuspiciousness of all instructionsin this method (line 19).
(NotethatthisdoesnotpreventEnlightenfromlookingforthe
fault in methods called by this method.) Finally, if the answer is
Idon/primetknow, Enlighten slightly decreases the suspiciousness of
thecurrentmethodbyaddingapassingvirtualtestwhosecoverageconsistsofthestatementsdirectlycoveredbythecurrentinvocation
(lines 21–22).
Inthesetwolattercases(i.e., noandIdon/primetknowanswers),the
faultlocalizationprocessthencontinues.Asinthecaseofincorrect
inputvalues,Enlightenaddsoutputvaluesmarkedasincorrect
to the set of known incorrect values. Lines 26–28 then restrict
the computation of SFL suspiciousness to the instructions covered,
directly or indirectly, by the current invocation only.
2.5 Illustrative Example
Tohelpillustratethedetailsofourapproach,weintroduceanexam-
ple consisting of a simple faulty program. Figure 3 shows the code
and corresponding test suite for class BoundedStack , which imple-
mentsastackofboundedsizeandwhichweadaptedfromprevious
work[30].Forbrevity,weomittedthecodethatchecksthecapacity
of the stackin method push. The faultis located at line 8:method
popshould have no effect on an empty stack, but it does not check
whetherthestackisempty.Consequently,whenthestackisempty,
themethod popincorrectlydecrementsthefield numElems denot-
ingthestacksize,whichbecomesnegative.Thisclasshasthreeunittests,andtest
t3failswithan ArrayIndexOutOfBoundsException
when calling bs.peek() at line 35, after calling bs.pop() on an
emptystack.Atthatpoint,thefield numElems is-1,andtheexpres-
sion size()-1 at line 12 evaluates to -2.
We now describe how Enlighten would support a developer in
localizing the fault in this code.
First Iteration. The left table in Figure 5 shows the initial SFL
results: line 13 is the most suspicious statement, while the actually
faultylineisranked,intheworstcase,atfourthplaceamongthe
eightexecutablestatementsoftheprogram.TheimprecisionofSFL
is caused by the fact that line 13 happens to be invoked only in
the failing test case, and it thus has a stronger correlation with test
failures than the actual faulty statement.
Thevaluestoredinfield numElems ,definedduringtheinvocation
ofclear,getsitsbasesuspiciousnessscorefromthesuspiciousness
ofthedefinitionatline13,whichis1 .0.Thisscoreisthenmultiplied
by its amplifying factor, which is computed based on the set of
incorrect data values. This set initially only contains the exception
object thrown when accessing the array elemsat line 12 in t3.
Because this exception object has a dynamic dependence on the
valuestoredinfield numElems ,theamplifyingfactorassociatedwith
that valuewould be 2, andthe value suspiciousnessfor numElems
would therefore be 2 .0 (see Section 2.3.2).
In this case, the value suspiciousness of numElems would be the
highestamongstallvaluesobserved.Enlightenwouldtherefore
85
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:50:07 UTC from IEEE Xplore.  Restrictions apply. 1public class BoundedStack {
2
3 Integer[] elems; int numElems;
4 BoundedStack(int max) { elems = new Integer[max]; }
5
6 void push(Integer k) {// check size against capacity
7 elems[numElems++] = k; }
8 void pop() { --numElems; }
9 Integer peek() {
10 if (size() = 0)
11 return null;
12 else return elems[size() - 1]; }
13 void clear() { numElems = 0; }
14 int size() { return numElems; } ... }
15
16 @Test
17 t1() {
18 BoundedStack bs = new BoundedStack(3);
19 bs.push(3);
20 assertEquals(1, bs.size()); }
21
22 @Test
23 t2() {
24 BoundedStack bs = new BoundedStack(3);
25 bs.push(4); bs.push(5);
26 bs.pop();
27 assertEquals(4, bs.peek()); }
28
29 @Test
30 t3() {
31 BoundedStack bs = new BoundedStack(3);
32 bs.push(6);
33 bs.clear();
34 bs.pop();
35 assertEquals(null, bs.peek()); }
Figure 3: Stack and corresponding test suite.
generatea debuggingquery for clear,shown inFigure4, withthe
value of field numElems , marked as highly suspicious (i.e., red).
After inspecting the inputs and outputs, the developer would
findthatthemethodcorrectlyset numElems to0andrespondtothe
query accordingly. As a result, Enlighten would add a virtual test
to the test suite reflecting the positive feedback from the developer
on that output value. The coverage matrix on the right side of
Figure 5 shows the updated rankings after this first iteration. Note
thatfailingtestcasesandpassingvirtualtestcaseshaveweight1,
as described in Section 2.2.
Second Iteration. The statements at lines 8, 10, and 12 appear at
the top of the ranking after the first iteration, with suspiciousness
0.95.Line8computesthevalueof bs.numElems inbs.pop() ,while
the execution of line 10 and 12 result in an array-out-of-bound ex-
ceptioninbs.peek().Thevalueof bs.numElems attheexitof pop()
andthereferenceoftheexceptionthrownby bs.peek() havethus
a base suspiciousness of 0.95. Because the observed failure dynami-
cally depends on both these values, their value suspiciousness is
1.90 (0 .95×2). Since there are two invocations that result in the
same (highest) suspiciousness value, let us assume that Enlighten
randomly picks the call to function peekfor the next query. In this
case,theexceptionobject(alongwiththe“this”reference)would
be the output of the call.
Given this query, the developer would realize that the exception
isexpected,asitiscausedbyastacksizethatwasalreadynegative
attheentryofthecall.Thedeveloperwouldthereforemarkfield
numElems intheinputasincorrect.Enlightenwouldthusaddthe
value(−1)infield numElems tothesetofknown-incorrectvalues,
which has the effect of increasing the amplifying factor associated
withalldefinitionsthataffectthatvalue,andreturn(seeSection2.4).
Third Iteration. Due to the increasein its amplifying factor dur-
ing the last iteration, the data value bs.numElems inbs.pop()
Figure 4: Debugging query on the 1stiteration.
stmt.t1 t2 t3 susp.
4 1 1 1 0.91
7 1 1 1 0.918 0 1 1 0.95
10 0 1 1 0.9511 0 0 0 0.0012 0 1 1 0.95
13 0 0 1 1.0014 1 1 1 0.91
result  -
weight 0.1 0.1 1 -stmt.t1 t2 t3 t4 susp.
4 1 1 1 0 0.917 1 1 1 0 0.91
8 0 1 1 0 0.95
10 0 1 1 0 0.9511 0 0 0 0 0.00
12 0 1 1 0 0.9513 0 0 1
10.71
14 1 1 1 0 0.91
result  -
weight 0.1 0.1 1 1 -
Figure 5: Coverage matrices before / after the 1stiteration.
becomes the single most suspicious value definition, with a sus-
piciousness score of 2 .85 (0 .95×3). Enlighten would therefore
select the invocation of pop()for the third query to the developer.
The developer would likely and quickly understand the failure, by
observing that the value of this.numElems is 0 at the entry of the
call and−1 at its exit, and end the fault localization process.
3 EMPIRICAL EVALUATION
We conducted two complementary studies to evaluate Enlighten:
an analytical study with simulated users (Section 3.1) and a user
studywithrealusers(Section3.2).Theformerletusevaluateour
techniqueon alarge numberof datapoints andunder variousset-
tings, which is typically challenging in studies involving real users.
Thestudywithrealusers,conversely,letusassesstheusefulnessof
Enlightenwhenconsideringactual developers’behavior,which
can only be approximated in a simulation.
3.1 Study with Simulated Users
In this study, we investigated different aspects of Enlighten us-
ingsimulatedusersandalargenumberoffaults.Specifically,we
investigated the following research questions:
RQ1.HowmanyiterationsarenecessaryforEnlightentolocalize
a fault?
RQ2.WhatistheimpactofthecustomizedSFLformulaandthe
amplifying factor on the effectiveness of Enlighten?
RQ3.HowsensitiveisEnlightentoincorrectuserresponsesto
debugging queries?
The first question evaluates the performance of Enlighten in a
scenarioinwhichtheuseralwaysanswersqueriescorrectly.The
second question assesses the usefulness of some key features of
Enlighten. Finally, the third question evaluates how the perfor-
manceof Enlightendegradeswhentheaccuracyofthedevelopers’
responses degrades.
3.1.1 Experiment Setup.
Benchmark Programs and Faults. As benchmarks, we used three
open-source programs widely used in fault localization research:
Math,Lang,andJsoup.MathandJsoupareavailableintheirpublic
86
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:50:07 UTC from IEEE Xplore.  Restrictions apply. Table 1: Benchmarks and faults considered.
Benchmark # Classes # Methods kLOC# Faults
Real Mutation
Math 236-447 1,723-3,899 43-83 11 1,174
Lang 123-170 1,835-2,281 45-57 8 490
Jsoup 75-206 611-1,032 8-14 8 116
repositories[ 1],whereasLangisavailableintheDefects4Jreposi-
tory[18].(MathisalsopartofDefects4J,butwithdifferentversions
fromtheonesweconsidered.)Weselectedthesebenchmarksbe-
causetheydonotusefeaturesunsupportedbyJavaPathFinder[ 31],
which Enlighten currently leverages to build DDGs. Table 1
presents these programs and faults. Since each program has multi-
ple versions, we report the number of classes, number of methods,
and code size as numeric ranges. We considered two sets of faults:
(1)27realfaults,availabletogetherwiththebenchmarks,and(2)
1,780 mutation faults [ 4,19], which we created using the mutation
tool Major [ 17]. We discarded faults that traditional SFL ranked in
a top position, as we wanted to evaluate Enlighten in the more
challenging (and more common) cases in which vanilla SFL would
not be useful. This led to discarding 3 of the 30 real faults available.
Forthemutationfaults,weranMajorinitsdefaultconfiguration
and only considered mutants killed by at least one failing test case.
SimulatedUsers(AutomatedOracles). Weusedautomatedoracles,
inlieuofrealusers,toanswerthequeriesthatEnlightengenerated.
Consider a query involving a specific invocation iof a method. To
suitably classify an output value as correct or incorrect, the oracle
re-runsiusing the correct version of the program and compares
this expected output with that of i’s actual execution. To ensure
thatiis executed with the same input as the faulty program, the
oraclestartsthetestexecutiononthefaultyversionandreplaces
thedefinitionofthefaultyclasswiththecorrectonerightbefore
invoking i, using runtime class re-definition [2].
We assume deterministic executions, so that any difference in
program state between the two runs on the faulty and correct
versions can only be caused by the fault. Also, for each query, our
oracle only provides feedback on the most suspicious of the output
values, rather than on multiple ones. Note that providing feedback
onmultiplevalueswouldhelplocatethefaultinfeweriterations,
andthuslikelyimprovetheperformanceofourtechnique.However,
webelievethattheapproachwechosemirrorswellthebehavior
ofarealuser,whoismorelikelytofocusononeoratmostafew
output values than on all of them. We confirmed this in our user
study (see Section 3.2).
In the simulated study, Enlighten terminates when (1) the cur-
rentmostsuspiciousdatavalueisactuallyfaulty(i.e.,ithasbeen
produced by a faulty statement), and (2) this value is computed
directlyinthecurrentqueriedinvocation.Ifthesetwoconditions
are not met within 100 iterations, Enlighten terminates the fault
localization process and considers it failed.
Metrics.We used two metrics for evaluating the effectiveness of
Enlighten:(1)thenumberofqueriesansweredbythesimulated
user before finding the fault and (2) the number of distinct method
invocations involved in such queries (the same invocation canbecome the most suspicious more than once). We consider these
metrics reasonable approximations of developer effort: the formerTable 2: Summary of results for real faults.
Benchmark Fault ID IRoF #Invocs#Queries
default w/o Wt w/o AF
MathC_AK_1 5 2 2 2 4
EDI_AK_1 37 2 2 2 6F_AK_1 36 3 3 4 3
M_AK_1 112 8 10 22 13
VS_AK_1 16 1 1 2 3CDI_AK_1 26 3 28 32 38CRVG_AK_1 62 6 23 19 19F_AK_2 9 1 1 1 5MU_AK_1 29 1 1 1 10MU_AK_4 36 3 3 4 8URSU_AK_1 13 1 1 1 1
Langb10 63 10 16 39 17
b16 53 1 1 1 7
b24 65 1 1 1 64
b26 114 - - - -
b28 5 1 2 1 1
b39 53 2 2 2 4
b5 7 1 1 1 1
b6 17 3 3 4 6
Jsoup1_3_4-1 3 1 1 1 111_3_4-3 73 4 4 4 -1_4_2-1 16 1 1 1 11_5_2-2 21 2 2 2 2
1_5_2-5 20 1 1 1 1
1_6_1-1CR1 56 2 9 16 81_6_1-1CR2 3 1 1 1 141_6_3-3 36 5 5 3 6
Average - 2.58 4.81 6.46 10.12
measures the number of interactions between the developer and
the tool; the latter measures the number of times the developer
needs to understand a new invocation (i.e., partial execution).
3.1.2 RQ1: How many iterations are necessary for Enlighten to
localize a fault? To answer RQ1, we ran Enlighten on our bench-
marksandfaults.Wediscusstheresultsfortherealfaultsandthose
forthemutationfaultsseparately.Table2showsthesummaryof
our results for the real faults. Column “Fault ID ” shows the iden-
tifierofthefaultsdocumentedintherepositoriesfromwhichwe
obtained them. Column “IRoF ” (Initial Rank of Fault) shows the
statement-level rank of the fault produced by SFL on the first itera-
tion of adebugging session. Column“# Invocs ” showsthe number
ofdistinctmethodinvocationsinthequeriesproducedtolocatethe
fault.Column“#Queries ”showsthenumberofqueriesanswered
by the simulated user before finding the fault. Column “default ”
shows the results obtained with the default configuration of En-lighten, whereas the remaining columns show results obtained
using alternative configurations (see Section 3.1.3).
Enlightensuccessfullylocalized23ofthe27(85%)faultswithin
10iterationsorless,and26ofthe27(96%)faultswithin28iterations
orless.In11casesEnlightenrequiredonly1querytolocalizethefault,eventhoughSFLdidnotrankthefaultylinefirst.Consideringallthefaultsinthestudy,theaveragenumberofiterationsnecessary
for localization was 4 .81 (min=1,max=28), and the average
number of invocations involved was 2 .58 (min=1,max=10).
We manually inspected the case of Lang b26, for which En-
lighten fails to locate the fault with less than 100 queries. The
faulty invocation is selected for the first time on the 15thdebug-
ging query. The suspicious output of this invocation is a string
thatispartiallyincorrect.However,duetotheparticularwaythe
87
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:50:07 UTC from IEEE Xplore.  Restrictions apply. Table 3: Summary of results for mutation faults.
Benchmark #Mut.# QueriesNot Found
[1, 1] [2, 10] [11, 100]
Math 1,174 915 77.94% 215 18.31% 29 2.47% 15 1.28%
Lang 490 438 89.39% 47 9.59% 5 1.02% 0 0.00%
Jsoup 116 77 66.38% 34 29.31% 2 1.72% 3 2.59%
Total 1,780 1,430 80.34% 296 16.63% 36 2.02% 18 1.01%
assertionof thefailingtest iswritten, theamplifyingfactor forall
characters in the string is the same, and the oracle fails to identify
the character that is actually incorrect. In subsequent debugging
queries,thesamefaultyinvocationisselectedseveraltimes,butthe
oraclekeepsmissingtheincorrectcharacterforthesamereason.
We conjecture that in this case a real developer would be more
likelytospottheerrorintheoutputstringandprovidetheright
feedback, as humans tend to view strings as a whole instead ofas individual characters. (Our oracle is purposely weak to avoid
unfairlyfavoringourtechniqueandconsidersthestringasmultiple
values, as discussed above.)
Wealsoanalyzedthecorrelationbetween IRoFand#Queries and
betweenIRoFand#Invocs.ThePearson’scorrelationcoefficient[ 26]
betweenthenumberofqueriesandtheinitialrankofthefaultis
0.38,whichsuggestsaweakpositivecorrelation.Thecorrelation
coefficientbetweenthenumberofdistinctinvocationsinthequeries
and the initial rank of the fault is 0.67, suggesting a moderateto strong positive correlation. Overall, the results suggest some
correlationbetweentheproblemdifficulty,asmeasuredby IRoF,and
the performance of Enlighten. However, the data also suggests
that, even in cases where IRoFhas a considerably high value, #
Queriescan be fairly low (e.g., Math.M_AK_1 and Lang.b10).
Table3showsthesummaryofourresultsforthemutationfaults.
Column“#Queries[min,max] ”showsthenumberofmutantsfor
whichthenumberofqueriesneededtolocatethecorresponding
fault was between the indicated min and max values. For example,
only one query was necessary to locate 915 faults (i.e., mutants)
inMath,whereasbetweentwoandtenquerieswerenecessaryto
localize47faultsinLang.Overall,Enlightensuccessfullylocalized
99% of the 1,780 mutation faults, and on average, over 96% of all
mutation faults were localized with at most 10 queries. Enlighten
failed to localize the fault in only 1.01% of the cases. The results
suggestthatEnlightenworksslightlybetterformutationfaults
thanforrealfaults,atleastforthecasesweconsidered.Thereason
may be that many of the real faults are inherently more difficult to
debug—aconjecturethatispotentiallysupportedbytheobservation
thatsomeofthesefaultswerepresentinthereleasedversionsof
popular libraries.
Answering RQ1: Our results show that Enlighten can identify
alargenumberoffaultswithinafewiterations.With10orless
singleanswerstoqueries,Enlightenlocated85%oftherealfaults
and 96% of the mutation faults.
3.1.3 RQ2:WhatistheimpactofthecustomizedSFLformulaand
theamplifyingfactorontheeffectivenessofEnlighten? Theweights
usedtocomputestatementsuspiciousnessandtheamplifyingfactorusedtocomputevaluesuspiciousnessaretwoimportantaspectsof
the design of Enlighten. This research question evaluates theireffectiveness. To answer RQ2, we ran Enlighten disabling each
ofthesefeatures separatelyandcomparedtheresultssoobtained
with those obtained using both features.
Table2showstheresultsforthisstudyinthecolumnslabeled
“# Queries ”. Column “ w/o Wt” shows the number of answers to
queriesthatEnlightenneededtolocatethefaultwhenweights
werenottaken into account (i.e., we simply set to 1 the weights
of all tests, which are used to compute the term aepof the SFL
formula in Section 2.2). Results show that, on average, Enlighten
needed 6.46 queries in this setting, compared to 4.81 queries in the
defaultconfiguration,whichcorrespondtoa34%increase.Column
“w/o AF” shows the number of queries when the amplifying factor
(AF)wasignored(seeSection2.3.2).Whenusingthisconfiguration,
Enlighten failed to locate the fault Jsoup.1_3_4-3 and needed, on
average, 10.12 queries to locate the remaining faults. This corre-
spondstoa110%increaseoverthedefaultconfiguration.Notethat,
due to the statistical nature of Enlighten, it is possible for the
configurations “w/o Wt ” and “w/o AF ” to perform slightly better in
some cases (e.g., Math CRVG_AK_1), but these cases are rare.
Weobservedsimilarresultsonmutationfaults,whichwedonot
report for space reasons. For “w/o Wt ”, the success rate of locating
the fault decreased by 0.5%, and the average number of queries
increasedby3.7%.For“w/oAF ”,thesuccessratedecreasedby1.8%,
and the average number of queries increased by 139%.
AnsweringRQ2:ResultsindicatethatthecustomizedSFLformulaandtheamplifyingfactorbothcontributetoimprove Enlighten’s
performance.ThecontributionofthecustomizedSFLformulais
lower compared to the contribution of the amplifying factor.
3.1.4 RQ3:HowsensitiveisEnlightentoincorrectuserresponses
to debugging queries? So far, we haveassumed that developers do
notmakemistakes.Inpractice,however,theycanerrbylabeling
correct values as incorrect or vice versa. This research questioninvestigates how sensitive is the performance of Enlighten toincorrect values labeled as correct. (We leave to future work the
investigation of the opposite case, which we consider less likely
to happen.) To conduct this study, we modified our automatedoracle so that it produced this type of erroneous answers with
aconfigurableprobability.Specifically,weconsiderederrorrates
ranging from 5% to 30%, with 5 percentage points increments, and
measured the number of queries and the number of cases in which
Enlighten fails. As before, we configured the oracle to provide
only one answer per query.
Table 4 shows, for each benchmark and for the different error
rates considered, the average increase in the number of queries
necessarytolocalizeafaultoverthecaseofanidealoracle(i.e.,a
userthatdoesnotmakemistakes).Forexample,whentheerroneous
answer rate is 30%, Enlighten needs, on average, 42.67% more
queries to locate a fault. The results in the table show, as expected,
apositivecorrelationbetweentherateoferroneousanswersand
the increase in the number of answers required to locate a fault.However, the results also show that Enlighten is still able tolocalize the fault in almost all cases. Even with 30% erroneous
answers, the average success rate was higher than 99.8%.
88
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:50:07 UTC from IEEE Xplore.  Restrictions apply. Table 4: Sensitivity of Enlighten to human errors. Values
indicate the percentual increase in the number of queries
overanidealoracle(i.e.,auserthatdoesnotmakemistakes).
BenchmarkError rate
5% 10% 15% 20% 25% 30%
Math 5.92% 12.26% 18.82% 25.46% 32.08% 38.58%
Lang 6.08% 14.36% 21.93% 29.44% 36.74% 43.75%
Jsoup 7.63% 15.88% 24.37% 32.82% 41.02% 48.85%
Average 6.13% 13.89% 21.24% 28.58% 35.75% 42.67%
AnsweringRQ3:Althoughthenumberofqueriesneededtolocalize
a fault increases with the ratio of erroneous answers, Enlighten
cansuccessfullylocatethefaultinmostcaseseveninthepresence
of (considerable amounts of) erroneous feedback.
3.2 User Study
Inadditiontoourstudywithsimulatedusers,weconductedtwo
actual user studies to evaluate Enlighten in a realistic scenario.
Ouruserstudiesinvolvetwodebuggingtaskseach,whereeachtask
consists of localizing and proposing a fix for a fault in a program.
3.2.1 Study Setup.
Benchmarks, Faults, and Participants. The software benchmarks
and faults we selected are non-trivial, real faults that existed in
releasedversionsofpopularsoftwarelibrarieswritteninJava.To
simulateascenarioinwhichparticipantsdebugcodewithwhich
they are familiar, we wanted software whose semantics should be
well understood by a person with a computer science background.
To this end, we chose code that involves either basic mathematical
concepts or XML parsing. In addition, as we did for our simulated
study,weselectedfaultsforwhichtraditionalSFLtechniquesdo
not perform well (i.e., the faulty statements are not ranked among
the most suspicious statements). We do so to avoid trivial cases in
which SFL by itself would be enough to localize the fault.
Table 5 summarizes the information about the two studies we
performed. Foreach studyandeach taskin thatstudy,it showsthe
benchmark used in the task and a concise description of the corre-
spondingfaultconsidered.Asthetableshows,thefaultsweused
forTasks1,2,and4wereselectedfromabenchmarkusedinthe
simulated study, whereas the fault we selected for Task 3 was used
in a previous user study on SFL techniques [ 25]. It is worth noting
that, although we used the same benchmark for Tasks 1 and 2, the
partsoftheprograminvolvedinthetwotasksaredifferent.Inother
words,completing Task 1 should notaffectthe participants’ perfor-
mance in Task 2. (Even if it had an effect, it should benefit equally
participants performing Task 2 with and without Enlighten.)
Basedonourassessmentandobservationsduringpilotstudies,
thepairofdebuggingtasksineachstudyareofsimilardifficulty,but
thetasksin Study2aresignificantlyharder thanthoseinStudy1.
This let us evaluate how Enlighten performs on faults at different
difficulty levels.
For each of the studies, we recruited 12 participants (different
for each study). The participants are graduate students enrolled
inthecomputerscienceprogrameitheratGeorgiaTechoratthe
FederalUnivesityofPernambuco.WealsorequiredtheparticipantsTable 5: Debugging tasks for the user study.
User Study Task ID Benchmark Fault Description
1Task 1 Math Complex number multiplication error
Task 2 Math Least common divisor computation error
2Task 3 Nanoxml XML qualified name parsing error
Task 4 Jsoup Absolute address construction error
to (1) have at least three years of programming experience and (2)
be familiar with the Java language and the Eclipse IDE.
For each study, we randomly assigned the participants to one
oftwogroups:GroupAorGroupB.ParticipantsinGroupAper-
formed Task 1 (Study 1) or Task 3 (Study 2) without Enlighten
andTask2(Study1)orTask4(Study2)withEnlighten.Partici-
pants in Group B performed Task 2 (Study 1) or Task 4 (Study 2)
withoutEnlightenandTask1(Study1)orTask3(Study2)with
Enlighten.TheparticipantsnotusingEnlightenwereallowed
to use their preferred traditional debugging approach(es) (e.g., the
Eclipse IDE debugger, print statements, step-by-step execution).
We used traditional debugging approaches instead of SFL as
ourbaselinefortworeasons.First,existingstudiesshowthatSFL
tends to produce no measurable advantages over traditional debug-
ging [25,32], so we do not expect user performance to improve
using SFL instead of traditional debugging. Second, we believe that
traditional debugging is a more objective baseline, as it relies on
mature/well-accepted tools known to our participants.
We implemented Enlighten as a plugin for the Eclipse IDE and
distributed the materials for the user study as a virtual machine
image, so as to ensure a uniform experience across all participants.
Weinformedthe participantsthatwewould measuretheirperfor-
mance while debugging using two debugging approaches, without
mentioningthatEnlightenwasourtechnique.Beforethestudy
began, the participants read a tutorial on the Enlighten plugin.
When done with the tutorial, they performed their assigned debug-
ging task. The time limit for each debugging task in Study 1 and
Study 2 was 20 and 30 minutes, respectively.
In pilot studies, we found that the participants gave up on their
tasksduetothecomplexityofthecodeinvolvedandtheirlackof
understanding of (some of) that code. Therefore, when perform-
ing the actual study, we allowed participants in all groups to ask
questions about the semantics of a piece of code during the debug-
ging process. This is akin to the common scenario in which the
person who is performing the debugging task asks questions about
the code to a developer with deeper knowledge of the softwareinvolved. We made sure to answer only general questions about
whatthemethodsweresupposedtodo,andwedidnotanswerany
questions about the faults being diagnosed.
3.2.2 Results. Tables6and7showtheresultsofthetwostudies.
In both tables, the first two columns show the ID and the corre-sponding group for each participant. Columns labeled “Success”
indicatewhethertheparticipantcorrectlyidentifiedthefaultinthe
debuggingtasks(“Y”)ornot(“N”).Columnslabeled“Time(min)”
report the time spent in localizing the fault (in case of success). For
both groups, the results for the task performed using traditional
debuggingareshowninthe3rdand4thcolumns,andtheresults
for the task performed using Enlighten are shown in the 5thand
89
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:50:07 UTC from IEEE Xplore.  Restrictions apply. Table 6: Results for User Study 1.
Participant GroupTask 1 (Traditional) Task 2 (Enlighten)
Success Time (min) Success Time (min)
1 A Y 17.5 Y 5.4
3 A Y 8.9 Y 11.0
5 A Y 8.0 Y 10.8
7 A Y 5.5 Y 8.5
9 A Y 18.3 Y 16.0
11 A Y 25.1 Y 16.4
Task 2 (Traditional) Task 1 (Enlighten)
Success Time (min) Success Time (min)
2 B Y 19.7 Y 8.6
4 B Y 20.1 Y 9.0
6 B Y 12.2 Y 4.0
8 B Y 20.8 Y 9.5
10 B Y 18.9 Y 8.4
12 B Y 11.0 Y 5.4
Average 100% 15.5 100% 9.4
6thcolumns. The last row in each table shows the average success
rate and debugging time for each task and technique.
In Study1, all participantssuccessfully completed bothof their
debugging tasks. On average, participants took 15.5 minutes to
complete the tasks using traditional debugging, and 9.4 minutesto complete the tasks using Enlighten. Therefore, for the tasksconsidered, Enlighten reduced the debugging time by 39% onaverage. This difference is statistically significant with a p-value
less than 0.005 using a one-tailed t test.
InStudy2,participantssuccessfullycompleted58.3%ofthede-
bugging tasks using traditional debugging, and the average debug-
ging time for these successful cases was 23.2 minutes. Conversely,
the participants successfully completed all their tasks when us-
ingEnlighten,andtheaveragetimespentoneachtaskwas9.5
minutes. In these cases, therefore, the use of Enlighten increased
the success rate by 71.5% and reduced the debugging time by 59%.
Also in this case, the differences for both metrics are statistically
significant. The p-value of the one-tailed t-test of the success rates
is 0.009, and that of the debugging time is less than 0.001.
Onaverage,forthetaskscompletedusingEnlighten,partici-
pants needed67% more queriesthan the perfect oracle tolocalize
thefaults,whichindicatesthathumansdomakemistakesinanswer-
ingqueries.However,itisworthnotingthat71%oftheparticipants
needed exactly the same number of queries as the perfect oracle.
Comparing the reduction in debugging time in the two studies,
the resultsseem to indicatethat Enlighten improves developers’
efficiency in debugging tasks more significantly for faults that are
more difficult to diagnose, which we consider a positive result.
Attheendoftheuserstudy,weaskedtheparticipantstocom-
plete a questionnaire about whether/how Enlighten helped them,
as well as what other information could have been provided by
the tool to make it easier to localize and understand the fault. The
two advantages of Enlighten most frequently mentioned werethat (1) it points developers to the likely faulty invocation in the
execution,and(2)itprovidesdetailedprogramstateinformation
forinspection.Thesetwoaspectsroughlycorrespondtowhatwe
considertobethemain improvementswemadeinEnlightenover
traditional debugging and traditional SFL techniques. The most
wanted feature that Enlighten does not currently provide, accord-
ing to the questionnaires, is the ability to get the context of theTable 7: Results for User Study 2.
Participant GroupTask 3 (Traditional) Task 4 (Enlighten)
Success Time (min) Success Time (min)
1 A N - Y 9.3
3 A Y 21.9 Y 18.0
5 A N - Y 9.6
7 A Y 30.0 Y 5.9
9 A Y 24.0 Y 6.0
11 A Y 21.8 Y 9.9
Task 4 (Traditional) Task 3 (Enlighten)
Success Time (min) Success Time (min)
2 B N - Y 11.1
4 B N - Y 7.4
6 B Y 16.9 Y 7.3
8 B Y 25.4 Y 11.0
10 B Y 22.4 Y 4.1
12 B N - Y 14.9
Average 58.3% 23.2 100% 9.5
method invocation in the debugging query, including the call stack
andthepositionofthecurrentinvocationintheentireexecution.
Severalparticipantsthoughtthatthisinformationwouldgivethem
a better understanding of the entire debugging process and help
them give feedback to debugging queries more efficiently. It would
bestraightforwardtoprovidethisadditionalinformation,andwe
are planning to do it in future work.
We also interviewed the participants about their general feeling
on the debugging experience using Enlighten. Multiple partici-
pants mentioned that learning to use the Enlighten plugin in the
time we allocated for the training was challenging. One partici-pant specifically pointed out that it was difficult to change their
debuggingmindsetfromatraditionalcode-centricparadigmtoa
more data-centric one. Finally, several participants reported thatthey spent a long time inspecting the code of the method in the
queryonlytolaterdiscoverthatitwasnotnecessary.Wespeculate
that these feedback may indicate that people’s performance using
Enlightencouldfurtherimproveafter theygetmorefamiliarwith
the technique.
3.3 Limitations and Threats to Validity
The main limitation of our current implementation of Enlighten
comes from the computation of the dynamic dependence informa-
tion.Duetotheenormousengineeringeffortrequiredtodevelop
atoolthatimplementsourapproach,thecurrentdynamicdepen-
dencyanalyzerdoesnotsupportsomefeaturesoftheJavastandard
library(e.g.,certainencryptionalgorithmsandSwing).Thisisan
implementation-specificlimitationandcanbeaddressedwithad-
ditionalengineering.Anotherlimitation,sharedwithmanyother
debugging techniques that rely on dynamic slicing, is that the per-
formanceoverheadof Enlightenduringprogramexecutioncanbe
significant.Intheuserstudy,however,noparticipantcomplained
about the running time of Enlighten.
Themajorinternalthreattovalidityforourevaluationhastodo
with possible faults in our implementation of Enlighten that may
invalidate our results. To address this threat, we carefully checked
and unit tested our code during development. Furthermore, for the
realfaultsinthebenchmark,wemanuallyinspectedtheinteractions
between the automated oracle and Enlighten to confirm that the
sequences of debugging queries and feedback were correct.
90
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:50:07 UTC from IEEE Xplore.  Restrictions apply. Themainexternalthreattovalidityisthatthebenchmarkswe
usedmightnotberepresentativeoffaultsinreal-worldscenarios
and/orourresultsmaynotgeneralize.Tomitigatethisthreat,we
selected benchmarks that perform different types of tasks: manipu-
lating complex data structures, performing numeric computations,
and processing XML files. In addition, in the study with simulated
users we evaluated Enlighten with both real faults and a large set
ofmutationfaults,andinthestudywithrealdevelopersweused
four different real-world faults. Another possible external threat is
that the population of participants we recruited for the user study
might not be representative of real developers. To mitigate this
threat,werequiredtheparticipantstohaveatleastthreeyearsof
programming experience.
4 RELATED WORK
Countlesspapershavebeenpublishedondebuggingandfaultlo-
calization.Forthesakeofspace,thissectionfocusesonthework
that is most closely related to Enlighten.
AlgorithmicDebugging(AD)isaninteractivedebuggingtech-
niquethatwasproposedinthefunctionalprogrammingcommunity
by Shapiro in the early eighties [ 28]. Similar to our technique, AD
asksdevelopersquestionsonthecorrectnessofspecificfunction
invocationsintheexecutiontreeforagivenfailingtest.Thetree
isthensystematicallyprunedbasedontheanswerstotheseques-
tions until the fault can be isolated. Enlighten differs from AD
intwoimportantways.First,ADusesbasicheuristicstoidentify
whichfunctioninvocationstotarget,whereasEnlightenleverages
SFL and dynamic dependences. Second, AD requires developers
todeterminewhetherafunctioninvocationiscompletelycorrect,
whichisdifficulttodointhecommoncaseoffunctionsthatinvolve
large portions of the program state. (This problem is common tomost techniques based on AD [
29], including our own previous
work [22], and tends to make these approaches error-prone and
impractical.) Conversely, Enlighten asks developers for feedback
onindividualinputandoutputvalues,whichwe believe(andour
initial results show) is a more realistic approach.
Ko and Myers proposed Whyline [ 20,21], an interactive debug-
ger that lets a developer trace incorrect variable values backwards
byaskingquestionsabouthowthesevaluescametobe.Whyline
issimilarinspirittodynamicbackwardslicing—theuserfollows
a sequence of incorrect variable values through program depen-
dence chainsto get tothe fault. Morerecently, Lin andcolleagues
proposed Microbat [ 24], a feedback-driven debugging technique
thatimprovesonWhylinebyinferringpatternsinexecutiontraces
andusingdeveloperfeedbacktoskippartialprogramexecutions,
expediting the backward tracing process. Enlighten, Whyline,and Microbat all leverage lightweight user feedback to improve
faultlocalization.However,incontrasttotheseothertechniques,
the queries Enlighten produces are contextualized by methodinvocations as opposed to focused on arbitrary execution points.
This feature not only lets the developer obtain relevant contextual
information when answering specific queries, but also enables the
techniquetojumpacrosscallingcontextsguidedbythesuspicious-
ness of program statements.
The debugging technique proposed by Hao and colleagues [ 12]
sets breakpoints in the faulty program using suspiciousness of pro-
gramstatementsgivenbySFL.Ateachbreakpoint,thetechniqueasksthedevelopertoinspecttheprogramusingadebuggertode-
termine whether the program statehas been infected by the fault.
The suspiciousness of related statements is then increased or de-
creasedbyafixedratiobasedontheprovidedfeedback.Incontrast
to theirapproach, Enlightenselectsfor inspectiona small setof
suspiciousdataitemswithinselectedmethodinvocations;itdoes
notrequirethedeveloperstofindfaultymemorylocationsinthe
entire program state. In addition, Enlighten incorporates develop-
ers’feedbackintotheSFLalgorithm,soastodynamicallyupdate
suspiciousnessinformation.Infollow-upwork,HaoandcolleaguesproposedVIDA[
13],whichleveragesprogramdependencestofind
statementswhosesuspiciousnessmustbeupdated.Comparedto
VIDA, Enlighten asks for feedback onthe input-output relations
of methods, whose intended behavior tends to be well understood,
rather than on individual program statements.
Gong and colleagues [ 10] proposed an interactive fault localiza-
tiontechniquethatcontinuouslyupdatestherankedlistofsuspi-
ciousstatementsastheusermarksstatementsasfaultyandnon-
faulty. The intuition behind the technique is that, once a statement
is labeled as non-faulty, the other statements executed in the same
failing test case should be considered more suspicious. Like tra-ditional SFL approaches, and unlike Enlighten, their technique
requires developers to determine the correctness of individual pro-
gram statementswithout contextual information,which has been
shown to be problematic [25].
5 CONCLUSIONS
We presented Enlighten, aninteractive feedback-driven faultlo-
calization technique. Enlighten combines SFL, algorithmic de-bugging, and dynamic dependence analysis by leveraging their
strengthswhilemitigatingtheirweaknesses.Inparticular,unlike
traditionalSFL,Enlightenaskdeveloperscontextualizedquestionsthatconsistofqueriesabouttheinputsandoutputsassociatedwith
concreteinstancesofsuspiciousmethodinvocations.Also,unlike
algorithmic debugging, Enlighten lets developers reason in terms
of individual input/output data items, which is important in order
to be able to handle large program states.
Our empirical results show that Enlighten is effective when
applied to bothreal-world and mutation faults in thebenchmarks
weconsidered.Specifically,ourstudywithsimulatedusersshows
that Enlighten can localize a majority of the faults with less than
10debuggingqueries;andouruserstudyshowsthatEnlighten
canprovidesignificantimprov ementsovertraditionaldebuggingin
termsofbothnumberoffaultslocalizedandtimeneededtolocalize
the faults.
Infuturework,wewillconductadditionaluserstudiestofurther
investigateourcoreassumptionthatmethodsareasuitablelevelof
abstraction for developers to understand program behavior during
debugging. We will also perform a direct comparison between our
approach and traditional SFL techniques in real-world scenarios.
Finally,wewillextendourimplementationtoincorporateitinto
additional IDEs and to remove some of its practical limitations.
ACKNOWLEDGMENTS
ThisworkwaspartiallysupportedbytheNSFundergrantsCCF-1161821
and 1548856, and by CNPq under grants 457756/2014-4 and 203981/2014-6.
91
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:50:07 UTC from IEEE Xplore.  Restrictions apply. REFERENCES
[1]2015. SAEG-SoftwareAnalysisandExperimentationGroup(atUniversidade
deSãoPaulo(USP),Brazil). (2015). https://github.com/saeg/experiments/tree/
master/jaguar-2015.
[2]2017. JavaInstrumentationAPI (ClassRedefinition). (2017). https://docs.oracle.
com/javase/8/docs/api/java/lang/instrument/package-summary.html.
[3]RuiAbreu,PeterZoeteweij,andArjanJ.C.vanGemund.2006. AnEvaluation
ofSimilarityCoefficientsforSoftwareFaultLocalization.In Proceedingsofthe
12th Pacific Rim International Symposium on Dependable Computing (PRDC 2006).
39–46.
[4]J. H. Andrews, L. C. Briand, and Y. Labiche. 2005. Is Mutation an Appropriate
ToolforTestingExperiments?.In Proceedingsofthe27thInternationalConference
on Software Engineering (ICSE 2005). 402–411.
[5]B.Ashok,JosephJoy,HongkangLiang,SriramK.Rajamani,GopalSrinivasa,and
VipindeepVangala.2009. DebugAdvisor:ARecommenderSystemforDebugging.
InProceedings of the 7th Joint Meeting of the European Software Engineering
Conference and the ACM SIGSOFT Symposium on The Foundations of Software
Engineering (ESEC/FSE 2009). 373–382.
[6]Thomas Ball, Mayur Naik, and Sriram K. Rajamani. 2003. From Symptom to
Cause: Localizing Errors in Counterexample Traces. In Proceedings of the 30th
ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages
(POPL 2003). 97–105.
[7]Satish Chandra, Emina Torlak, Shaon Barman, and Rastislav Bodik. 2011. An-
gelicDebugging.In Proceedingsofthe33rdInternationalConferenceonSoftware
Engineering (ICSE 2011). 121–130.
[8]Holger Cleve and Andreas Zeller. 2005. Locating Causes of Program Failures. In
Proceedings of the 27th International Conference on Software Engineering (ICSE
2005). 342–351.
[9]BrianDemsky,MichaelD.Ernst,PhilipJ.Guo,StephenMcCamant,JeffH.Perkins,
and Martin Rinard. 2006. Inference and Enforcement of Data Structure Con-
sistency Specifications. In Proceedings of the 2006 International Symposium on
Software Testing and Analysis (ISSTA 2006). 233–244.
[10]Liang Gong, David Lo, Lingxiao Jiang, and Hongyu Zhang. 2012. Interactive
fault localization leveraging simple user feedback. In Proceedings of the 28th
International Conference on Software Maintenance (ICSM 2012). 67–76.
[11]Tibor Gyimóthy, Árpád Beszédes, and Istán Forgács. 1999. An Efficient Rele-
vant Slicing Method for Debugging. In Proceedings of the 7th European Software
Engineering Conference Held Jointly with the 7th ACM SIGSOFT International
Symposium on Foundations of Software Engineering (ESEC/FSE 1999). 303–321.
[12] Dan Hao, Lu Zhang, Tao Xie, Hong Mei, and Jia-Su Sun. 2009. Interactive Fault
LocalizationUsingTestInformation. JournalofComputerScienceandTechnology
24, 5 (2009), 962–974.
[13]D. Hao, L. Zhang, L. Zhang, J. Sun, and H. Mei. 2009. VIDA: Visual interac-tive debugging. In Proceedings of the 31st International Conference on Software
Engineering (ICSE 2009). 583–586.
[14]JamesA.Jones,JamesF.Bowring,andMaryJeanHarrold.2007. Debuggingin
Parallel. In Proceedings ofthe 2007International Symposium onSoftware Testing
and Analysis (ISSTA 2007). 16–26.
[15]James A. Jones, Mary Jean Harrold, and John Stasko. 2002. Visualization of Test
Information to AssistFault Localization. In Proceedings ofthe 24th International
Conference on Software Engineering (ICSE 2002). 467–477.
[16]ManuJoseandRupakMajumdar.2011. CauseClueClauses:ErrorLocalizationUs-
ing Maximum Satisfiability. In Proceedings of the 32nd ACM SIGPLAN Conference
on Programming Language Design and Implementation (PLDI 2011). 437–446.[17]René Just. 2014. The Major mutation framework: Efficient and scalable mutation
analysis for Java. In Proceedings of the 2014 International Symposium on Software
Testing and Analysis (ISSTA 2014). 433–436.
[18]René Just, Darioush Jalali, and Michael D. Ernst. 2014. Defects4J: A Databaseof Existing Faults to Enable Controlled Testing Studies for Java Programs. In
Proceedings of the 2014 International Symposium on Software Testing and Analysis
(ISSTA 2014). 437–440.
[19]RenéJust,DarioushJalali,LauraInozemtseva,MichaelD.Ernst,ReidHolmes,and
Gordon Fraser. 2014. Are Mutants a Valid Substitute for Real Faults in Software
Testing?.In Proceedingsofthe22ndACMSIGSOFTInternationalSymposiumon
Foundations of Software Engineering (FSE 2014). 654–665.
[20]AndrewJ.KoandBradA.Myers.2004. DesigningtheWhyline:ADebuggingInterfaceforAskingQuestionsAboutProgramBehavior.In Proceedingsofthe
SIGCHIConferenceonHumanFactorsinComputingSystems(CHI2004).151–158.
[21]Andrew J. Ko and Brad A. Myers. 2008. Debugging Reinvented: Asking and
AnsweringWhyandWhyNotQuestionsAboutProgramBehavior.In Proceedings
ofthe30thInternationalConferenceonSoftwareEngineering(ICSE2008).301–310.
[22]XiangyuLi,Marcelod’Amorim,andAlessandroOrso.2016. IterativeUser-Driven
Fault Localization. In Proceedings of the 12th International Haifa Verification
Conference (HVC 2016). 82–98.
[23]Ben Liblit, Mayur Naik, Alice X. Zheng, Alex Aiken, and Michael I. Jordan.
2005. Scalable Statistical Bug Isolation. In Proceedings of the 2005 ACM SIGPLAN
Conference on Programming Language Design and Implementation (PLDI 2005).
15–26.
[24]Yun Lin, Jun Sun, Yinxing Xue, Yang Liu, and Jinsong Dong. 2017. Feedback-
based Debugging. In Proceedings of the 39th International Conference on Software
Engineering (ICSE 2017). 393–403.
[25]Chris Parnin and Alessandro Orso. 2011. Are Automated Debugging Techniques
Actually Helping Programmers?. In Proceedings of the 2011 International Sympo-
sium on Software Testing and Analysis (ISSTA 2011). 199–209.
[26]Karl Pearson.1895. Notes onregressionand inheritancein caseof two parents.
InRoyal Society of London. 246–263.
[27]Manos Renieris and Steven P. Reiss. 2003. Fault Localization With NearestNeighbor Queries. In Proceedings of the 18th IEEE International Conference on
Automated Software Engineering (ASE 2003). 30–39.
[28] Ehud Y. Shapiro. 1983. Algorithmic Program DeBugging. MIT Press,Cambridge,
MA, USA.
[29]JosepSilva.2011. ASurveyonAlgorithmicDebuggingStrategies. Advancesin
Enginnering Software 42, 11 (Nov. 2011), 976–991.
[30]P. David Stotts, Mark Lindsey, and Angus Antley. 2002. An Informal Formal
Method for Systematic JUnit Test Case Generation. In Proceedings of the Second
XP Universe and First Agile Universe Conference on Extreme Programming and
Agile Methods (XP/Agile Universe 2002). 131–143.
[31]Willem Visser, Klaus Havelund, Guillaume Brat, Seungjoon Park, and Flavio
Lerda.2003. ModelCheckingPrograms. AutomatedSoftwareEngineering 10,2
(April 2003), 203–232.
[32]QianqianWang,ChrisParnin,andAlessandroOrso.2015. EvaluatingtheUse-
fulness of IR-based Fault Localization Techniques. In Proceedings of the 2015
International Symposium on Software Testing and Analysis (ISSTA 2015). 1–11.
[33]Xiangyu Zhang, Neelam Gupta, and Rajiv Gupta. 2006. Locating Faults Through
AutomatedPredicateSwitching.In Proceedingsofthe28thInternationalConference
on Software Engineering (ICSE 2006). 272–281.
[34]Alice X. Zheng, Michael I. Jordan, Ben Liblit, Mayur Naik, and Alex Aiken.2006. Statistical Debugging: Simultaneous Identification of Multiple Bugs. In
Proceedingsofthe23rdInternationalConferenceonMachineLearning(ICML2006).
1105–1112.
92
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:50:07 UTC from IEEE Xplore.  Restrictions apply. 