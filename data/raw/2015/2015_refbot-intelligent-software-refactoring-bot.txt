RefBot: Intelligent Software Refactoring Bot
Vahid Alizadeh, Mohamed Amine Ouali, Marouane Kessentini and Meriem Chater
Software Engineering Intelligence Lab, CIS Department, University of Michigan, USA
alizadeh,mouali,marouane,meriemchater@umich.edu
Abstract ‚ÄîThe adoption of refactoring techniques for contin-
uous integration received much less attention from the research
community comparing to root-canal refactoring to Ô¨Åx the quality
issues in the whole system. Several recent empirical studiesshow that developers, in practice, are applying refactoringincrementally when they are Ô¨Åxing bugs or adding new features.There is an urgent need for refactoring tools that can support
continuous integration and some recent development processes
such as DevOps that are based on rapid releases. Furthermore,
several studies show that manual refactoring is expensive and
existing automated refactoring tools are challenging to conÔ¨Ågure
and integrate into the development pipelines with signiÔ¨Åcantdisruption cost.
In this paper, we propose, for the Ô¨Årst time, an intelligent soft-
ware refactoring bot, called RefBot. Integrated into the version
control system (e.g. GitHub), our bot continuously monitors thesoftware repository, and it is triggered by any ‚Äùopen‚Äù or ‚Äùmerge‚Äù
action on pull requests. The bot analyzes the Ô¨Åles changed during
that pull request to identify refactoring opportunities using a
set of quality attributes then it will Ô¨Ånd the best sequence ofrefactorings to Ô¨Åx the quality issues if any. The bot recommends
all these refactorings through an automatically generated pull-
request. The developer can review the recommendations and their
impacts in a detailed report and select the code changes that he
wants to keep or ignore. After this review, the developer can closeand approve the merge of the bot‚Äôs pull request. We quantitatively
and qualitatively evaluated the performance and effectiveness of
RefBot by a survey conducted with experienced developers who
used the bot on both open source and industry projects.
Index Terms ‚ÄîSoftware bot, refactoring, quality
I. I NTRODUCTION
Refactoring, deÔ¨Åned as a set of program transformations
intended to improve the system design while preserving thedesired behaviour, is becoming a critical software maintenanceactivity, especially with the growing complexity of software
systems [1]. A recent study by the US Air Force Software
Technology Support Center (STSC) shows that restructuringthe code of a large project reduced developers‚Äô time by over
60% when introducing new features. However, refactoring is
expensive. Developers take an average of 6 weeks to refactorthe design of medium-size projects (around 30K LOC) [2].There has been much work done on various techniques andtools for software refactoring [3]‚Äì[7] and these approaches
can be classiÔ¨Åed into three main categories: manual ,semi-
automated and fully-automated approaches.
In manual refactoring, the developers refactor with no tool
support except the execution part, identifying the parts of theprogram that require attention and performing all aspects of thecode transformation by hand. It may seem surprising that a de-veloper would eschew the use of tools in this way, but Murphy-
Hill et al. [8] found in their empirical study of the developers‚Äôusage of the Eclipse refactoring tooling that in almost 90% ofcases the developers performed refactorings manually and did
not use automated refactoring tools. Kim et al. [9] conÔ¨Årmed
this observation, Ô¨Ånding that the interviewed developers fromMicrosoft preferred to perform refactoring manually in 86%of cases. Despite its apparent popularity, manual refactoring is
very limited. However, several studies have shown that manual
refactoring is error-prone, time-consuming, not scalable and
not practical for extensive application of refactorings to Ô¨Åx
major quality issues [4], [10], [11]. Although developers are
doing refactorings manually, the surveys conÔ¨Årmed that they
are not frequently refactoring their code because of the above
limitations.
In fully-automated refactoring, developers provide their
code as input, and the tool will provide refactoring recommen-
dations automatically [12]. The majority of existing automated
refactoring tools assume that developers want to Ô¨Åx code
smells [13]‚Äì[15]. This approach is appealing, in that it is
a complete solution and requires little developer effort, but
it suffers from several serious drawbacks as well. First, therecommended refactoring sequence may change the program
design radically, and this is likely to cause the developer to
struggle to understand the refactored program, and they loseany control of the introduced code changes. Second, it lacksÔ¨Çexibility since the developer has to either accept or reject
the entire refactoring solution. In fact, developers intentions
may not be, most of the time, Ô¨Åxing code smells or themajority of them. Third, it fails to consider the developer
perspective, as the developer has no opportunity to provide
feedback on the refactoring solution as it is being created.
Furthermore, as development must halt while the refactoring
process executes, fully-automated refactoring methods are not
useful for Ô¨Çoss refactoring where the goal is to maintain
good design quality while modifying existing functionality.The developers have to accept the entire refactoring solutioneven though they prefer, in general, step-wise approacheswhere the process is interactive and they have control of therefactorings being applied [16]. Finally, one of the signiÔ¨Åcantlimitations of existing automated refactoring tools is the highconÔ¨Åguration effort required to integrate them into the current
development pipeline of the team/company. In fact, several
companies are now using continuous integration and DevOps,
which make the adoption of current automated refactoring
tools very challenging.
UI*&&&"$.*OUFSOBUJPOBM$POGFSFODFPO"VUPNBUFE4PGUXB SF&OHJOFFSJOH	"4&
¬•*&&&
%0*"4&
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:37:13 UTC from IEEE Xplore.  Restrictions apply. Recently, few interactive refactoring techniques were pro-
posed [17]‚Äì[20]. They provide to the developers the Ô¨Çexibilityto approve or reject the recommended refactoring that canimprove the quality. However, this interaction process is time-consuming, and developers get frustrated from providing feed-back on Ô¨Åles that are out of their interests/ownership or nav-igating through many refactoring recommendations/strategiesto improve several quality metrics.
To address all the above challenges, we propose the Ô¨Årst
attempt to design and build an intelligent refactoring bot asa GitHub app that can be easily integrated into any project
repository on GitHub. The bot can be customized to monitor
the quality in the repository after some pull-requests repeatedly
or automatically executed when the quality analysis showsa signiÔ¨Åcant decrease. The bot analyzes the Ô¨Åles changed
during that pull request(s) to identify refactoring opportunities
using a set of quality attributes then it will Ô¨Ånd the bestsequence of refactorings to Ô¨Åx the quality issues if any. The botrecommends all these refactorings through an automaticallygenerated pull-request. The developer, whenever availablewithout interrupting the development pipeline, can review therecommendations, and their impacts in a detailed report and
select the code changes that he wants to keep or ignore. After
this review, the developer can close and approve the mergeof the bot‚Äôs pull request. We quantitatively and qualitativelyevaluated the performance and effectiveness of RefBot by asurvey conducted with experienced developers who used thebot on both open source and industry projects.
The primary contributions of this paper can be summarized
as follows:
1) The paper introduces a novel way to refactor software
systems using autonomous intelligent software bots butstill considering developers interaction to review thegenerated pull-request.
2) We propose an implementation of the refactoring bot as
a Git app that can be quickly adopted in a continuous
integration environment or DevOps process.
3) The paper reports the results of an empirical study on
an implementation of our approach. The obtained resultsprovide evidence to support the claim that, on average,our bot is more efÔ¨Åcient than existing automated refac-toring techniques based on a benchmark of six open
source systems and one industrial project. The paper also
evaluates the relevance and usefulness of the suggestedrefactorings for software developers in improving thequality of the modiÔ¨Åed Ô¨Åles in several pull-request.
The remainder of this paper is structured as follows. Section
2 presents the relevant related work. Section 3 describes our
intelligent refactoring bot, while the results obtained from our
experiments are presented and discussed in Section 4. Threatsto validity are discussed in Section 5. Finally, in Section 6, wesummarize our conclusions and present some ideas for futurework.II. R
ELATED WORK
Our work is mainly related to 1) refactoring recommen-
dations; 2) empirical studies on refactoring, mostly the onesinvestigating its relationship with fault-proneness; and 3) soft-ware bots.
A. Refactoring Recommendation
Much effort has been devoted to the deÔ¨Ånition of ap-
proaches supporting refactoring. One representative example is
JDeodorant, the tool proposed by Tsantalis and Chatzigeorgiou
[16].Our paper is mostly related to approaches exploitingsearch-based techniques to identify refactoring opportunities,and our discussion focuses on them since the bot is basedon multi-objective refactoring. We point the interested readerto the survey by Bavota [21] for an overview of approachessupporting code refactoring.
O‚ÄôKeeffe and Cinn ¬¥eide [22] presented the idea of formulat-
ing the refactoring task as a search problem in the space ofalternative designs, generated by applying a set of refactoring
operations. Such a search is guided by a quality evaluation
function based on eleven object-oriented design metrics thatreÔ¨Çect refactoring goals. Harman and Tratt [23] were the Ô¨Årstto introduce the concept of Pareto optimality to search-based
refactoring. They used it to combine two metrics, namely CBO
(Coupling Between Objects) and SDMPC (Standard Deviationof Methods Per Class), into a Ô¨Åtness function and showed
its superior performance as compared to a mono-objective
technique [23].
The two aforementioned works [22], [23] paved the way
to several search-based approaches aimed at recommending
refactoring operations [17], [18], [24]‚Äì[27]. Several other
studies proposed refactorings at the model level as well
[28]‚Äì[35]. A representative example of these techniques isthe recent work by Alizadeh et al. [20], who proposed aninteractive multi-criteria code refactoring approach to improve
the QMOOD quality metrics while minimizing the number ofrefactorings. In our approach, we decided to rely on a simpler
optimization algorithm by only considering the refactoring ofrecently changed Ô¨Åles in other pull requests rather than theroot-canal refactoring approach of Alizadeh et al. [20].
B. Empirical Studies on Refactoring
Empirical studies on software refactoring mainly aim at
investigating the refactoring habits of software developers
and the relationship between refactoring and code quality.We only discuss studies reporting Ô¨Åndings relevant to ourwork. Murphy-Hill [8] investigated how developers performrefactorings. Examples of the exploited datasets are usagedata from 41 developers using the Eclipse environment andinformation extracted from versioning systems. Among their
several Ô¨Åndings, they show that developers often perform Ô¨Çoss
refactoring ; namely, they interleave refactoring with other
programming activities, conÔ¨Årming that refactoring is rarely
performed in isolation. Kim [9] present a survey of software
refactoring with 328 Microsoft engineers. They show that
the major obstacle of adopting many existing refactoring

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:37:13 UTC from IEEE Xplore.  Restrictions apply. tools is their conÔ¨Åguration and painful integration within
their pipelines without disturbing developers with their currentfocus in terms of meeting deadlines and making regular codechanges. Those Ô¨Åndings stress out the need for refactoringbots that can be adopted for continuous integration withoutconsiderable conÔ¨Åguration effort.
C. Software Bots
The design and implementation of software bots are still in
its infancy with a signiÔ¨Åcant focus on chatbots. For instance,Lebeuf et al. [36], [37] discussed the potential of using chat
bots in software engineering and how they can be helpful
to increase collaborations between programmers. The authorsalso proposed a possible classiÔ¨Åcation of potential beneÔ¨Åts ofusing software bots in various domains, especially to improvethe productivity of developers.
An extensive empirical study of over 90 software bots was
performed by Wessel et al. [38] to provide a classiÔ¨Åcation
and taxonomy for them. They found that around 21 bots
were actually tried on GitHub repositories and the dominant
majority are around testing but without providing any code
actions or recommendations to developers. The authors foundthat none of these bots provides explanations of their analysiswhich reduced the adoption by developers.
Some examples of regression testing bots include Travis
CI and the bot designed by Urli et al. [39] to repair bugs.These tools did not open a new pull-request, but they are
executed manually by the developers where they can check
the recommended patches. Another bot related to qualityassessment but not refactoring is Fix-it [40]. It is mainly
limited to a few types of code changes, mainly targeting
dynamic analysis metrics.
Finally, Wyrich et al. [41] proposed a vision paper to
emphasize the importance of refactoring bots and motivatestheir potential use in practice. They proposed a prototype, nota complete bot, by running SonarQube to detect code smells.However, the work is still in its initial stage where refactoringsare not recommended yet.
III. A
PPROACH
We developed the ‚ÄùRefactoring Bot‚Äù (RefBot) as a GitHub
App using which the workÔ¨Çow can be automated, and thedevelopers can integrate the bot easily to any repository of
their interest. The overview of the Refactoring bot is shown
in Figure 1.
A. RefBot Installation
The Ô¨Årst step of utilizing the Refactoring bot is to install its
GitHub application on organizations or user accounts and toset up the appropriate permissions. As the installation page in
Figure 2 shows, the user can select the repositories. Therefore,RefBot is granted access to the speciÔ¨Åc repositories via the
GitHub API. RefBot has read and write permissions to ‚ÄùPull
Requests‚Äù and ‚ÄùWebHook‚Äù, and also is subscribed to ‚ÄùPull
Requests‚Äù and its related ‚Äùreviews and comments‚Äù events.After this step, RefBot automatically sets up a web-hook for
the developer‚Äôs proÔ¨Åle which means the permitted activitieson the selected repositories will be posted as JSON-formattedpayloads to the designated external server.
B. Processing a Pull Request
RefBot continuously monitors the actions performed on the
repository by checking the subscribed payloads delivered toits server. In our current conÔ¨Åguration, opening a new pull
request action triggers the RefBot‚Äôs workÔ¨Çow.
First, the commits in the pull request are compared to the
commit at the point where the branch is created to extract the
list of all Ô¨Åles changed by the pull request. Then, two versions
of the Ô¨Åles, before and after the pull request, are downloadedto the external server for further processing and modiÔ¨Åcations.
By processing only the changed Ô¨Åles by the pull request, we
ensure that the developers are provided with the reports and
refactorings limited to the codes they recently modiÔ¨Åed. This
feature facilitates the evaluation of recommended refactorings
and is aligned with the idea of maintaining/improving thecode quality in the continuous development process.
1)Calculating Quality Changes :The RefBot analyses
the code quality of the extracted Ô¨Åles. For this purpose, weadopted QMOOD quality assessment methodology, which isa hierarchical model for object-oriented designs [42].
QMOOD model comprises of four levels from which we
utilized the Ô¨Årst level, Design Quality Attributes, to mea-sure code quality changes of the pull request. This quality
attributes set is deÔ¨Åned based on ISO 9126 and consists of
‚ÄùReusability‚Äù, ‚ÄùFlexibility‚Äù, ‚ÄùUnderstandability‚Äù, ‚ÄùFunctional-ity‚Äù, ‚ÄùExtendibility‚Äù, and ‚ÄùEffectiveness‚Äù. Table I describes the
QMOOD metrics deÔ¨Ånitions.
It is shown that QMOOD metrics model is highly effective
in predicting software defects in both traditional and iterative(like agile) software development processes [43].
Since the QMOOD metrics are not limited to a speciÔ¨Åc
range, it is difÔ¨Åcult for the user to interpret their values.Therefore, we built a software quality benchmark datasetconsisting of the quality metrics calculated for over 100 open-
source and industrial software projects. Then, to summarize all
six quality attributes, we deÔ¨Åned a super metric called Total
Quality Index (TQI) as the linear summation of the metrics.
Finally, we compared the quality metrics and TQI of a new
project/Ô¨Åle with the range of the benchmark and assigned aquality label (A, B, C, and D) based on the quartile of a value.
This method facilitates the analysis of quality reports and
gives meaning to the metrics in terms of the quality level
(low/high) of software compared to other standard projects.
2)Optimization using Refactoring :Finding a refactoring
solution can be a challenging task since a huge search space
requires to be explored. This search space is the outcomeof the number of refactoring operations and the importanceof their order and combination. To search this space, weemployed an adaptation of the non-dominated sorting genetic

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:37:13 UTC from IEEE Xplore.  Restrictions apply. Fig. 1. The overview of RefBot Pipeline
Fig. 2. Installing RefBot on a repository
algorithm (NSGA-II) [44] to discover a trade-off between
multiple quality attributes.
NSGA-II is a multi-objective evolutionary algorithm oper-
ating on a population of candidate solutions that are evolved
toward the Pareto-optimal solution set. NSGA-II uses an
explicit diversity-preserving strategy together with an elite-preservation strategy. [44].
A refactoring solution is designed as a vector that consists
of an ordered sequence of multiple refactoring operations.Each refactoring operation includes a refactoring action andits speciÔ¨Åc controlling parameters. The refactoring operationsTABLE I
QUALITY ATTRIBUTES AND THEIR COMPUTATION EQUATIONS .
Quality attributesDeÔ¨Ånition
Computation
ReusabilityA design with low coupling and high cohesion is
easily reused by other designs.
0.25‚àóCoupling +0.25‚àóCohesion +0.5‚àó
Messaging +0.5‚àóDesignSize
FlexibilityThe degree of allowance of changes in the design.
0.25‚àóEncapsulation ‚àí0.25‚àóCoupling +0.5‚àó
Composition +0.5‚àóP olymorphism
UnderstandabilityThe degree of understanding and the easiness of
learning the design implementation details.
0.33‚àóAbstraction +0.33‚àóEncapsulation ‚àí
0.33‚àóCoupling +0.33‚àóCohesion ‚àí0.33‚àó
P olymorphism ‚àí0.33‚àóComplexity ‚àí0.33‚àó
DesignSize
FunctionalityClasses with given functions that are publicly
stated in interfaces to be used by others.
0.12‚àóCohesion +0.22‚àóP olymorphism +
0.22‚àóMessaging +0.22‚àóDesignSize +0.22‚àó
Hierarchies
ExtendibilityMeasurement of design‚Äôs allowance to incorporate
new functional requirements.
0.5‚àóAbstraction ‚àí0.5‚àóCoupling +0.5‚àó
Inheritance +0.5‚àóP olymorphism
EffectivenessDesign efÔ¨Åciency in fulÔ¨Ålling the required func-
tionality.
0.2‚àóAbstraction +0.2‚àóEncapsulation +
0.2‚àóComposition +0.2‚àóInheritance +0.2‚àó
P olymorphism

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:37:13 UTC from IEEE Xplore.  Restrictions apply. TABLE II
LIST OF REFACTORING OPERATIONS INCLUDED IN REFBOT.
Refactoring Controlling Parameter
Moving Features Between Objects
Move Method Source, Target, Method
Move Field Source, Target, Attribute
Extract Class Source, Target, Attributes, Methods
Organizing Data
Encapsulate Field Source, Attribute
Simplifying Method Calls
Decrease Field Security Source, Attribute
Decrease Method Security Source, Method
Increase Field Security Source, Attribute
Increase Method Security Source, Method
Dealing with Generalization
Pull Up Field Source, Target, Attribute
Pull Up Method Source, Target, Method
Push Down Field Source, Target, Attribute
Push Down Method Source, Target, Method
Extract SubClass Source, Target, Attributes, Methods
Extract SuperClass Source, Target, Attributes, Methods
considered in RefBot cover the most used operations se-
lected from different categories: ‚ÄùMoving features‚Äù, ‚ÄùData
organizers‚Äù, ‚ÄùMethod calls simpliÔ¨Åers‚Äù, and ‚ÄùGeneralizationmodiÔ¨Åers‚Äù. These refactorings are listed in Table II. Refac-toring operations are created or modiÔ¨Åed randomly during
the population initialization or mutation. Also, the size of a
solution vector which is the number of included refactoring
operation is randomly selected between lower and upper bound
values. Therefore, it is crucial to examine the feasibility of asolution using related pre-conditions and post-conditions [45].These conditions ensure that the program will not break whilethe behaviour is preserved by the refactoring.
To evaluate a candidate refactoring solution, a Ô¨Åtness func-
tion is deÔ¨Åned to estimate its goodness. In order to measure
the impact of a refactoring solution on the software project, we
utilized six QMOOD quality attributes. The relative change ofeach quality attribute after applying the refactoring solution to
the software system is considered as the Ô¨Åtness function and
is expressed as:
F itnessF unction
i=AQMafter
i (CC)‚àíAQMbefore
i (CC)
AQMbefore
i (CC)
(1)
where AQMbefore
i andAQMafter
i are the averages of the
quality metric ibefore and after applying a refactoring solution
over all changed classes CC, respectively.
By deÔ¨Åning the Ô¨Åtness function in this way, we aim to Ô¨Ånd
the solutions capable of improving the quality attributes of thepull request.
Additionally, we constraint the search process to the solu-
tions in which at least a ‚Äùclass‚Äù controlling parameter is in theset of changed Ô¨Åles in the pull request. For this purpose, we
modiÔ¨Åed a variation operator of the search algorithm called‚ÄùSelection Operator‚Äù. Variation operators help to navigatethrough the search space and to maintain a good diversityin the population. Parent selection is a crucial step thatdirectly affects the convergence rate. We added the controllingparameter constraint to the selection process.
After the execution of the refactoring search algorithm is
Ô¨Ånished, the instruction of applying each refactoring operationis added to the related Ô¨Åles as a distinctive marker formatsimilar to the Git conÔ¨Çict marker. Finally, RefBot creates a
new pull request to introduce the changes to the repository.
C. Developer‚Äôs Interaction
One of the main advantages of RefBot is to include the
developer in the refactoring process loop. When the internalworkÔ¨Çow of RefBot on a pull request is completed, the
developer is notiÔ¨Åed by email and also via GitHub checks
API in the same page of the pull request. These notiÔ¨Åcationscontain a link to the report page of the pull request wherethe users can analyze the results and give feedback to the
recommended refactorings.
There are three levels of reports generated for each pull
request and provided for the user:
‚Ä¢Solution Report: contains the quality history of the pull
request and the impact of the recommended solution onthe changed Ô¨Åles.
‚Ä¢File Report: includes the list of refactorings applied to the
selected Ô¨Åle and the detailed quality history and impactof refactoring.
‚Ä¢Refactoring Report: represents the instruction of a single
refactoring and the high-level code abstraction of sourceand target classes which are transformed by the operation.
Analyzing these simple yet effective reports give the ability
of swift detection of required improvements based on individ-ual preferences.
The developer can interact with the refactoring results of
RefBot with three actions. Each refactoring can be ‚Äùrejected‚Äù,‚Äùapplied with a code marker‚Äù, or ‚Äùapplied automatically‚Äù.
By rejecting a refactoring, it is not considered in the pull
request. Applying with a code marker adds the refactoringinstruction as a marker inside the related Ô¨Åles. Therefore, thedeveloper can manually implement the required changes. Last,applying automatically, gives permission to RefBot to changeand apply the refactorings to the source code itself.
The reason we have both manual and automated refactoring
is that sometimes the developers prefer to take control of the
refactoring process and the changes in the structure of their
code either for the whole software or a speciÔ¨Åc set of importantclasses/Ô¨Åles.
When the developer is satisÔ¨Åed with the feedback, he/she
can update the previously created RefBot‚Äôs pull request.
RefBot can be combined with continuous integration tools
like TravisCI, Jenkins, or CircleCI to identify the problems

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:37:13 UTC from IEEE Xplore.  Restrictions apply. that may occur during the automated refactoring by running
integration tests.
D. ConÔ¨Åguration and Customization
RefBot is highly customizable in terms of setting its internal
workÔ¨Çow parameters and execution management.
Sometimes a developer is not willing to be disturbed for
every new pull request. Therefore, RefBot can be conÔ¨Ågured
to monitor the repository at a speciÔ¨Åc time interval or evencan be triggered manually for a speciÔ¨Åc pull request.
Furthermore, users can enable/disable different refactoring
types and quality attributes. In this way, they can control theoptimization process and limit the search to the refactoringoperations they are willing to apply and to the quality attributesthey prefer to improve.
Additional materials such as the default parameter settings
for NSGA-II and video demo of RefBot can be found at thispublication‚Äôs web page
1.
E. Running Example
In this section, to illustrate the process of RefBot and its
performance in refactoring a pull request, we provide a running
example on a real open-source software system.
We considered a pull request from ‚Äùatomix‚Äù software repos-
itory and manually triggered RefBot to process it. Figure
3 represents part of the Ô¨Åle quality table in the solution
report page, which is generated for the selected pull request.It shows the TQI grade for the changed Ô¨Åles before andafter creating the pull request alongside with the impact of
the recommended refactoring solution on the quality. As an
example, the quality of the second Ô¨Åle is degraded from 4.05(B) to 1.18 (C). The solution which RefBot found for the pull
request contains seven refactoring operations applied to this
Ô¨Åle. These refactorings could improve the Ô¨Åle quality to 5.72(B).
The user can view the detailed report page for each Ô¨Åle.
The bar charts in the Ô¨Åle report page are provided in Figure
4. It shows the quality changes after the pull request and
the refactoring solution impact for each of the six quality
attributes, individually. We can observe that the recommended
refactoring solution improves 5 out of 6 quality attributes for
the Ô¨Åle compared to the pull request quality.
Another section in the Ô¨Åle report page is shown in Figure
5. It lists the refactoring operations from the recommendedsolution which have a controlling parameter applied to the
selected Ô¨Åle. The developer can interact with this list and rejector apply (code mark/auto options are as a popup window) eachof the refactorings.
Additionally, the developer can further investigate each of
the refactorings by viewing the refactoring report page. Figure
6 represents the abstract code changes after applying the
selected refactoring on the source and target classes. Thisreport can facilitate the decision making of users and help
them to understand the changes in the structure introduced by
a speciÔ¨Åc refactoring.
1https://sites.google.com/view/ase2019refbotWhen a developer completes the interaction and analysis,
the pull request is updated in the software repository, includingthe feedbacks on the refactorings. For any refactoring thatapplied as a code marker, the instructions are added to thetop of the related Ô¨Åles. Figure 7 depicts an example of theformat of these markers.
IV . V
ALIDATION
We deÔ¨Åne three categories of research questions to evalu-
ate RefBot and compare it to state-of-the-art techniques forautomated refactoring:
‚Ä¢RQ1: Quality improvement. To what extent can our
refactoring bot improve the quality of software systems ascompared to existing automated refactoring techniques?
In RQ1, we use the internal quality attributes [42] andcode smells as proxies to assess the quality improvementbrought by the refactoring operations generated by theRefBot for a set of selected pull-requests on different sys-tems. We compare the performance of our approach (MO-
MFO) with two, state-of-the-art, refactoring techniques:Ouni [27] and JDeodorant [46]. Ouni [27] proposed an
automated multi-objective refactoring formulation based
on NSGA-II using an aggregation of quality metrics
while reducing the number of refactorings. JDeodorant
[46] is an Eclipse plugin able to detect code smellsand automatically recommend refactorings to Ô¨Åx them.JDeodorant is not based on the use of heuristics search.
As JDeodorant supports a lower number of refactoring
types with respect to the ones we considered, we restrictour comparison with it to these refactorings. We have
also limited the comparison to the changed Ô¨Åles in the
pull-requests.
‚Ä¢RQ2: Refactoring meaningfulness. Are the refactoring
recommendations produced by the RefBot meaningful
from a developer‚Äôs point of view? How do they compare
with those generated by existing automated refactoringtechniques? Using antipatterns or internal quality indica-
tors as proxies for code quality (as we do in RQ1) hassubstantial limitations. For this reason, in RQ1, we survey25 developers asking for their opinion about the meaning-fulness of the refactorings recommended by our technique
and by the automated refactoring competitive technique[27]. In RQ2, we do not compare with JDeodorant since
we preferred to focus on the most similar competitive
technique in the literature to better study the advantagesbrought by the refactoring bot. The main substantial
difference between RefBot and the approach by Ouni [27]
is indeed the interactive and incremental approach of the
refactoring bot to focus on pull-requests.
‚Ä¢RQ3: Industrial validation. To what extent can RefBot
support of refactoring in a real-world continuous inte-
gration setting? We integrated a beta version of Refbot
into a previously licensed refactoring tool and asked oneof our industrial partners to use it for a limited periodof 3 business days (with six developers involved) on
their regular pull-request after installing the bot on their

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:37:13 UTC from IEEE Xplore.  Restrictions apply. Fig. 3. The quality table in solution report page
Fig. 4. The quality bar charts in Ô¨Åle report page for all six quality attributes.
Fig. 5. The list of refactoring operations recommended for a single Ô¨Åle.
repository. During this period, we checked the ability of
RefBot to select relevant refactorings for the recent pull-requests introduced by the programmers during their dailyactivities.
The context of our study is represented by the seven systems
in Table III. We selected these seven systems for our validationbecause they range from medium to large-size projects and
Fig. 6. The code abstraction of source and target classes after applying a
speciÔ¨Åc refactoring.
Fig. 7. The refactoring instructions related to a single Ô¨Åle are added to thesource code as a marker style.
TABLE III
S
TATISTICS OF THE STUDIED SYSTEMS .
System Release #classes #smells KLOC
Xerces-J v2.7.0 991 91 240
JHotDraw v7.5.1 585 25 21JFreeChart v1.0.18 521 72 170GanttProject v1.11.1 245 49 41JDI v5.8 638 88 247Apache Ant v1.8.2 1191 112 255
Rhino v1.7.5 305 69 42
have been actively developed over the past 10 years. JDI2is
an industrial project for which 6 of the developers involved inthe JDI maintenance agreed to take part in our experiments.
Table III provides information about the size of the subject
2Company anonymized for double-blind.

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:37:13 UTC from IEEE Xplore.  Restrictions apply. systems (in terms of the number of classes and KLOC), and
the number of code smells affecting them as detected with therules deÔ¨Åned in [47].
A. Data Collection
We present the data collection and analysis process grouped
by research question category.
To address RQ1 , we calculated NFas the percentage of
code smells Ô¨Åxed by the refactoring solutions generated bythe three considered approaches, over the total number of code
smells which are affecting recent pull-requests of the subjectsystems. We selected the latest ten pull-requests for each of
the open-source systems while a total of 8 pull-requests wereopened during the three business days of the RefBot trial byour industrial partner. The detection of code smells before/afterapplying a refactoring solution was performed with the rulesdeÔ¨Åned in [47]. The considered code smells are Blob, Feature
Envy (FE), Data Class (DC), Spaghetti Code (SC), Functional
Decomposition (FD), and Shotgun Surgery (SS) .
Since the concept of code smell is very subjective (different
developers may have different opinions on whether a codecomponent is smelly or not) [48], we also use more objectivemetrics to assess the quality of the refactorings generated by
the experimental approaches. We adopted the Gmetric based
onQMOOD [42] that estimates the quality improvement of
the system by comparing the quality before and after refac-
toring independently from the number of Ô¨Åxed design defects.
Six quality factors are considered by QMOOD : reusability,
Ô¨Çexibility, extendibility, functionality, understandability and
effectiveness. All of them are formalized using a set of quality
metrics. Hence, the total gain in quality Gfor each of the
considered QMOOD quality attributes q
ibefore and after
refactoring can be estimated as:
G=/summationtext6
i=1Gqi
6where Gqi=q/prime
i‚àíqi (2)
where q/prime
iandqirepresent the value of the quality attribute i
respectively after and before refactoring.
To answer RQ2 we asked 25 developers to evaluate the
meaningfulness of the refactorings recommended by RefBot
and by the approach of Ouni [27] for pull-requests on theseven subject systems. Before explaining the study design for
RQ2, it is important to remember that both the experimental
techniques generate output sequences of refactoring operationsthat make sense when considered together rather than when
looking at them in isolation. However, it is not an option to aska developer to assess the meaningfulness of all the refactoring
operations generated for a given system. For this reason, westarted by Ô¨Åltering for each system the sequences of refactoring
operations impacting the Ô¨Åles of a set of pull-requests to make
a fair comparison between both tools. Then, the developersmanually evaluated the outcomes of both tools for each pull-
request.
Each participant was then asked to assess the meaningful-
ness of the sequences of refactoring operations. Since on sixof the seven systems (all but JDI) we involved external de-
velopers (professional developers who did not take part in thedevelopment of the subject system), we made sure that eachparticipant only evaluated refactoring sequences recommendedby the two competitive techniques on one speciÔ¨Åc system(JHotDraw). The rationale for such a choice is that an externaldeveloper would need time to acquire a system‚Äôs knowledgeby inspecting its code, and we did not want participants tocomprehend the code from four different systems since thiswould introduce a strong tiring effect in our study.
To answer RQ3 , the six developers of the JDI project
evaluated the refactoring sequences generated for that system,since here we wanted to exploit their experience as originaldevelopers of the system. They used RefBot, as a beta versiontool, during a period of 3 days instead of a refactoring tool thatwe licensed to their company in the past. Our industrial partnerwas motivated to try out RefBot since they are interestedin upgrading their current quality assessment tool to anotherone that can support DevOps like our RefBot. They alsoexpressed a concern about the lack of customization and highconÔ¨Åguration effort/training required by existing automatedrefactoring tools.
To support such a complex experimental design, we built
a Java Web-app that automatically assigns the refactoredpull-requests to be evaluated to the developers. The Web-app showed each participant one sequence of refactoringoperations on a single page, providing the developer with (i)the list of refactorings (move method m
ito class Cj, then
push down Ô¨Åeld fkto subclass Cj, ), (ii) the code of the
classes impacted by the sequence of refactorings, and (iii) thecomplete code of the system subject of the refactoring with
the description of the opened pull-request and the generated
refactoring pull-request by the refactoring bot. The web pageshowing the refactoring sequence asked participants the ques-
tion Would you apply the proposed refactorings? with a choice
between no(the refactoring sequence is not meaningful),
maybe (the refactoring sequence is meaningful, but the quality
improvement it brings does not justify changing the code),
oryes(the refactoring sequence is meaningful and should be
implemented). Moreover, participants were allowed to leave a
comment justifying their assessment (this was optional). The
Web-app was also in charge of:
Balancing the evaluations per system. We made sure that
each system received roughly the same number of participantsevaluating the different refactored pull-requests (Ô¨Åles associ-ated/modiÔ¨Åed by these pull-requests) by the two approaches.
Keeping track of the time spent by participants in the eval-
uation of each refactoring sequence/refactoring pull-request.The time spent by participants was counted in seconds since
the moment the Web-app showed the refactoring on the
screen to the moment in which the participant submitted theirassessment. This feature was done to remove participantsfrom our data set who did not spend a reasonable amount of
time in evaluating the refactorings. We consider less than 60

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:37:13 UTC from IEEE Xplore.  Restrictions apply. TABLE IV
PARTICIPANTS INVOLVED IN RQ2.
System #Partic. Avg. Prog. Avg. Java Avg. Refact.
Experience Experience Exp.(1-5)
Xerces-J 4 11 9 4.0 (high)
JHotDraw 4 10 7 3.0 (medium)JFreeChart 4 10 7 3.3 (medium)GanttProject 4 9 8 3.5 (high)
JDI 6 14 12 4.5 (very high)
Apache Ant 3 9 7 3.7 (high)
seconds a reasonable threshold to remove noise (we removed
all evaluation sessions in which the participant spent less than60 seconds in analyzing a single refactoring sequence).
Collecting demographic information about the participants.
We asked their programming experience (in years) overall and
in Java, and a self-assessment of their refactoring experience(from very low to very high).
Table IV shows the participants involved in our study and
how they were distributed in the evaluation of the refactoringsequences generated on the seven systems.
For the three days industrial validation, we integrated a rou-
tine in our RefBot to record all the actions of the 6 developers
including the number of applied and rejected refactorings,number of selected test cases, the introduced code changes
and commit messages.
B. Experimental Setting and Data Analysis
For each algorithm and each system, we performed a set of
experiments using several population sizes: 50, 100, 200, and300. Then, we speciÔ¨Åed the maximum chromosome length(maximum number of operations/test cases per solution).The resulting vector length is proportional to the number ofrefactorings that are considered, and the size of the program
to refactor. Based on those considerations, the upper andlower bounds on the chromosome length were set to 10 and
350, respectively. The stopping criterion was set to 10,000
Ô¨Åtness evaluations for all algorithms to ensure fairness. In
order to have signiÔ¨Åcant results, for each couple (algorithm,system), we use the trial and error method [49] for parameterconÔ¨Åguration.
Concerning RQ2, we report the percentage of refactoring
sequences assessed with a no,maybe ,o r yesby developers
for each treatment (RefBot and Ouni system [27]). Then,
we discuss interesting comments left by developers whenjustifying their assessment.
C. Results
RQ1: Quality improvement. Figures 8 and 9 provide the
percentage of Ô¨Åxed code smells (NF) and the quality gain ( G)
based on the QMOOD model, respectively. The average NFon
the seven systems is 91% with peaks of ‚àº96% for JHotDraw
and GanttProject.
The recommended refactorings also improved the Gmetric
values (Figure 9) of the seven systems. The average quality
gain for the Rhino system was the highest among the seven
systems with 0.43. The improvement in the quality gain shows
that the recommended refactorings help to optimize different
Fig. 8. Median percentage of Ô¨Åxed code smells (NF) on the different pull-
requests of the seven systems.
Fig. 9. Median quality gain (G) on the different pull-requests of the seven
systems.
TABLE V
RQ2: W OULD YOU APPLY THE PROPOSED REFACTORINGS OF THE
GENERATED REFACTORING PULL -REQUEST ?
Approach no maybe yes
RefBot 4/68 (5%) 11/68 (16%) 53/68 (77%)
Ouni [27] 29/83 (34%) 41/83 (49%) 13/83 (15%)
quality metrics. Besides, the performance of RefBot is superior
as compared to the competitive refactoring techniques [27],[46], even though the difference in terms of Ô¨Åxed code smellsis not that marked (Figure 8). This latter result is also dueto the fact that RefBot does not only recommend refactoringoperations aimed at removing code smells it also focuseson refactoring classes not affected by code smells but werechanged during recent pull-requests. For example, in a manual
investigation of the refactorings recommended by RefBot for
JFreeChart, we found that 17 of the impacted classes do notexhibit any criticality as indicated by code smells and theywere still improved in terms of quality attributes.
RQ2: Refactoring meaningfulness. Table V summarizes
the manual refactoring evaluation results obtained from the 25participants. Note that there is a slight deviation between thetotal number of refactorings evaluated by the two approaches(68 vs83) since, as explained in Section IV, we did not con-
sider for the data analysis the evaluations in which participants
spent less than 60 seconds to assess the meaningfulness of the
refactoring sequence under analysis and also the approach of
Ouni et al. tends to generate much more refactorings on the

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:37:13 UTC from IEEE Xplore.  Restrictions apply. analyzed Ô¨Åles from the pull-requests.
The analysis of the quality by the Refactoring Bot improved
the relevance of the recommended refactorings compared to
the fully automated multi-objective approach. Indeed, thepercentage of meaningful recommendations (the sum of the
maybe and yesanswers) is much better for RefBot comparing
to Ouni et al. (94% for RefBot and 66% for Ouni ). Thepercentage of refactorings that participants believe must be
applied ( yesanswers) is signiÔ¨Åcantly higher for Refbot as well
(77% vs15%).
By looking at the comments left by participants when justi-
fying their assessment, four out of the six original developersof the JDI system highlighted in their comments for three
refactoring sequences that they found the refactorings relevant
because it is improving the modularity of a class that theyfrequently modify in all the most recent pull-requests. Forexample, one of the developers wrote in a comment: ‚ÄúThat
is a very good recommendation, I spent days working on thisclass recently there, so I like this move method very muchand extract sub-class. It will improve the reusability a lot ashighlighted by the explanations of the bot‚Äù . We found this
comment as important qualitative evidence of the value of ourrefactoring bot in terms of analyzing the recently closed pull-
requests to identify changed Ô¨Åles and Ô¨Åx the identiÔ¨Åed quality
issues in these Ô¨Åles.
RQ3: Industry validation. Figures 8 and 9 summarize the
results of deploying our RefBot during 3 business days to our
industrial partner on the JDI repository. The six developers
used the bot as part of their daily programming activitiesinstead of a previously licensed refactoring tool. The tool
was deployed as a Git app that connects automatically to aprivate GitHub repository whenever some code changes areintroduced by the developers to check for refactorings andgenerate a new pull-request for the review of developers.
Overall, the achieved results conÔ¨Årm the effectiveness of our
bot to generate efÔ¨Åcient refactoring pull-requests. We foundthat the developers approved 9 out of 11 refactoring pull-
requests generated by the bot during the three days. For the
two remaining pull-requests, we found that a total of 7 out of11 refactorings were approved. The achieved results conÔ¨Årmthe basic intuition behind this work, showing that developersare more motivated to apply refactorings when the tool iseasy to integrate within their development pipeline. The six
developers also conÔ¨Årmed that they feel more comfortable in
applying refactorings due to the high level of control proposed
by the bot to review the generated pull-request which gives
them more conÔ¨Ådence and trust to the tool. This may explain
the reason why a good number of recommended refactorings
were applied.
V. T
HREATS TO VALIDITY
Our refactoring bot mainly focuses on the recent pull-
requests, but developers may have different priorities based
on their current context. However, the developers can modifythe conÔ¨Åguration of our bot to focus on commits, branches,
speciÔ¨Åc Ô¨Åles or developers‚Äô contributions. Another internalthreat is related to the used quality attributes since developers
may want to express different preferences than QMOOD, orthey want to tune them based on their needs or how criticalis the code.
Construct validity is concerned with the relationship be-
tween theory and what is observed. To evaluate the resultsof our approach, we selected a set of pull-requests whencomparing with other techniques, but may perform better onother pull-requests where the quality of them are different.
External validity refers to the generalize-ability of our Ô¨Ånd-
ings. We performed our experiments on open-source systems
belonging to different domains, and one industrial project,by involving participants in the evaluations of the refactoringoperations. However, we cannot assert that our results canbe generalized to other applications, and other developers.
Future replications of this study are necessary to conÔ¨Årm our
Ô¨Åndings.
VI. C
ONCLUSION
We presented a Ô¨Årst attempt to propose an intelligent
software refactoring bot, as GitHub app, that can submit a pull-request to refactor recent code changes. The salient feature ofthe proposed bot is that it incorporates interaction support,
via our Web app, hence allowing developers to approve ormodify or reject the applied code refactoring. The refactoring
bot also provides support to explain why the refactorings are
applied by quantifying the quality improvements. To evaluatethe effectiveness of our technique, we applied it to four open-source and one industrial projects comparing it with state-of-
the-art approaches. Our results show promising evidence onthe usefulness of the proposed interactive refactoring bot. The
participants highlighted the high usability of the bot in terms
of easy integration with their development environments with
the least conÔ¨Åguration effort.
Future work will involve validating our technique with
additional refactoring types, programming languages, quality
issues and participation from practitioners to investigate the
general applicability of the proposed methodology.
R
EFERENCES
[1] S. A. Bohner and R. S. Arnold, Software change impact analysis . IEEE
Computer Society Press Los Alamitos, 1996, vol. 6.
[2] Y . Lin, X. Peng, Y . Cai, D. Dig, D. Zheng, and W. Zhao, ‚ÄúInteractive and
guided architectural refactoring with search-based recommendation,‚Äù in
ACM SIGSOFT International Symposium on F oundations of Software
Engineering . ACM, 2016, pp. 535‚Äì546.
[3] T. Mens and T. Tourw ¬¥e, ‚ÄúA survey of software refactoring,‚Äù IEEE
Transactions on software engineering , vol. 30, no. 2, pp. 126‚Äì139, 2004.
[4] E. Mealy, D. Carrington, P. Strooper, and P. Wyeth, ‚ÄúImproving usability
of software refactoring tools,‚Äù in 2007 Australian Software Engineering
Conference (ASWEC‚Äô07) . IEEE, 2007, pp. 307‚Äì318.
[5] M. W. Mkaouer, M. Kessentini, S. Bechikh, K. Deb, and M. ¬¥O Cinn ¬¥eide,
‚ÄúHigh dimensional search-based software engineering: Ô¨Ånding tradeoffs
among 15 objectives for automating software refactoring using nsga-iii,‚Äù in Proceedings of the 2014 Annual Conference on Genetic and
Evolutionary Computation . ACM, 2014, pp. 1263‚Äì1270.
[6] M. OKeeffe and M. O. Cinn ¬¥eide, ‚ÄúSearch-based refactoring for software
maintenance,‚Äù Journal of Systems and Software , vol. 81, no. 4, pp. 502‚Äì
516, 2008.
[7] J. Simmonds and T. Mens, ‚ÄúA comparison of software refactoring tools,‚Äù
Programming Technology Lab , 2002.

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:37:13 UTC from IEEE Xplore.  Restrictions apply. [8] E. Murphy-Hill, C. Parnin, and A. P. Black, ‚ÄúHow we refactor, and how
we know it,‚Äù IEEE Transactions on Software Engineering (TSE) , vol. 38,
no. 1, pp. 5‚Äì18, 2011.
[9] M. Kim, T. Zimmermann, and N. Nagappan, ‚ÄúAn empirical study of
refactoringchallenges and beneÔ¨Åts at microsoft,‚Äù Software Engineering,
IEEE Transactions on , vol. 40, no. 7, pp. 633‚Äì649, July 2014.
[10] S. Negara, N. Chen, M. Vakilian, R. E. Johnson, and D. Dig, ‚ÄúA
comparative study of manual and automated refactorings,‚Äù in European
Conference on Object-Oriented Programming . Springer, 2013, pp. 552‚Äì
576.
[11] X. Ge, Q. L. DuBose, and E. Murphy-Hill, ‚ÄúReconciling manual
and automatic refactoring,‚Äù in Proceedings of the 34th International
Conference on Software Engineering . IEEE Press, 2012, pp. 211‚Äì221.
[12] T. Mens and T. Tourw ¬¥e, ‚ÄúA survey of software refactoring,‚Äù IEEE Trans.
Software Eng. , vol. 30, no. 2, pp. 126‚Äì139, 2004.
[13] M. Vakilian, N. Chen, S. Negara, B. A. Rajkumar, B. P. Bailey, and R. E.
Johnson, ‚ÄúUse, disuse, and misuse of automated refactorings,‚Äù in Pro-
ceedings of the 34th International Conference on Software Engineering .
IEEE Press, 2012, pp. 233‚Äì243.
[14] G. Sz Àùoke, C. Nagy, L. J. F ¬®ul¬®op, R. Ferenc, and T. Gyim ¬¥othy, ‚ÄúFault-
buster: An automatic code smell refactoring toolset,‚Äù in 2015 IEEE
15th International Working Conference on Source Code Analysis and
Manipulation (SCAM) . IEEE, 2015, pp. 253‚Äì258.
[15] F. Palomba, G. Bavota, M. D. Penta, F. Fasano, R. Oliveto, and
A. De Lucia, ‚ÄúOn the diffuseness and the impact on maintainability of
code smells: a large scale empirical investigation,‚Äù Empirical Software
Engineering , 2017.
[16] N. Tsantalis and A. Chatzigeorgiou, ‚ÄúIdentiÔ¨Åcation of move method
refactoring opportunities,‚Äù IEEE Transactions on Software Engineering ,
vol. 35, no. 3, pp. 347‚Äì367, 2009.
[17] M. W. Mkaouer, M. Kessentini, S. Bechikh, K. Deb, and M. ¬¥O Cinn ¬¥eide,
‚ÄúRecommendation system for software refactoring using innovization
and interactive dynamic optimization,‚Äù in Proceedings of the 29th
ACM/IEEE International Conference on Automated Software Engineer-
ing (ASE 2014) , pp. 331‚Äì336.
[18] W. Mkaouer, M. Kessentini, A. Shaout, P. Koligheu, S. Bechikh, K. Deb,
and A. Ouni, ‚ÄúMany-objective software remodularization using nsga-iii,‚Äù
ACM Transactions on Software Engineering and Methodology (TOSEM) ,
vol. 24, no. 3, pp. 17:1‚Äì17:45, 2015.
[19] V . Alizadeh and M. Kessentini, ‚ÄúReducing interactive refactoring effort
via clustering-based multi-objective search,‚Äù in Proceedings of the 33rd
ACM/IEEE International Conference on Automated Software Engineer-
ing, ser. ASE 2018. New York, NY , USA: ACM, 2018, pp. 464‚Äì474.
[Online]. Available: http://doi.acm.org/10.1145/3238147.3238217
[20] V . Alizadeh, M. Kessentini, W. Mkaouer, M. Ocinneide, A. Ouni,
and Y . Cai, ‚ÄúAn interactive and dynamic search-based approach to
software refactoring recommendations,‚Äù IEEE Transactions on Software
Engineering , 2018.
[21] G. Bavota, A. De Lucia, A. Marcus, and R. Oliveto, ‚ÄúRecommending
refactoring operations in large software systems,‚Äù in Recommendation
Systems in Software Engineering , M. P. Robillard, W. Maalej, R. J.
Walker, and T. Zimmermann, Eds. Springer Berlin Heidelberg, 2014,pp. 387‚Äì419.
[22] M. O‚ÄôKeeffe and M. ¬¥O Cinn ¬¥eide, ‚ÄúA stochastic approach to automated
design improvement,‚Äù in International Conference on Principles and
practice of programming in Java
. Computer Science Press, Inc., 2003,
pp. 59‚Äì62.
[23] M. Harman and L. Tratt, ‚ÄúPareto optimal search based refactoring at
the design level,‚Äù in 9th annual conference on Genetic and evolutionary
computation , 2007, pp. 1106‚Äì1113.
[24] O. Seng, J. Stammel, and D. Burkhart, ‚ÄúSearch-based determination
of refactorings for improving the class structure of object-orientedsystems,‚Äù in International conference on Genetic and evolutionary
computation . ACM, 2006, pp. 1909‚Äì1916.
[25] M. Kessentini, W. Kessentini, H. Sahraoui, M. Boukadoum, and A. Ouni,
‚ÄúDesign defects detection and correction by example,‚Äù in International
Conference on Program Comprehension (ICPC) . IEEE, 2011, pp. 81‚Äì
90.
[26] A. Ouni, M. Kessentini, and H. Sahraoui, ‚ÄúSearch-based refactoring
using recorded code changes,‚Äù in Proceedings of the 17th European
Conference on Software Maintenance and Reengineering (CSMR 2013) ,
pp. 221‚Äì230.
[27] A. Ouni, M. Kessentini, H. Sahraoui, K. Inoue, and K. Deb, ‚ÄúMulti-
criteria code refactoring using search-based software engineering: anindustrial case study,‚Äù ACM Transactions on Software Engineering and
Methodology (TOSEM) , vol. 25, no. 3, p. 23, 2016.
[28] M. Fleck, J. Troya, M. Kessentini, M. Wimmer, and B. Alkhazi,
‚ÄúModel transformation modularization as a many-objective optimization
problem,‚Äù IEEE Transactions on Software Engineering , vol. 43, no. 11,
pp. 1009‚Äì1032, 2017.
[29] A. Ouni, R. G. Kula, M. Kessentini, T. Ishio, D. M. German, and
K. Inoue, ‚ÄúSearch-based software library recommendation using multi-
objective optimization,‚Äù Information and Software Technology , vol. 83,
pp. 55‚Äì75, 2017.
[30] A. Ouni, R. Gaikovina Kula, M. Kessentini, and K. Inoue, ‚ÄúWeb service
antipatterns detection using genetic programming,‚Äù in Proceedings of the
2015 Annual Conference on Genetic and Evolutionary Computation .
ACM, 2015, pp. 1351‚Äì1358.
[31] A. Ouni, M. Kessentini, S. Bechikh, and H. Sahraoui, ‚ÄúPrioritizing code-
smells correction tasks using chemical reaction optimization,‚Äù Software
Quality Journal , vol. 23, no. 2, pp. 323‚Äì361, 2015.
[32] M. Kessentini, M. Wimmer, H. Sahraoui, and M. Boukadoum, ‚ÄúGen-
erating transformation rules from examples for behavioral models,‚Äù
inProceedings of the Second International Workshop on Behaviour
Modelling: F oundation and Applications . ACM, 2010, p. 2.
[33] A. ben Fadhel, M. Kessentini, P. Langer, and M. Wimmer, ‚ÄúSearch-based
detection of high-level model changes,‚Äù in 2012 28th IEEE International
Conference on Software Maintenance (ICSM) . IEEE, 2012, pp. 212‚Äì
221.
[34] M. Kessentini, H. Sahraoui, M. Boukadoum, and M. Wimmer, ‚ÄúSearch-
based design defects detection by example,‚Äù in International Conference
on Fundamental Approaches to Software Engineering . Springer, Berlin,
Heidelberg, 2011, pp. 401‚Äì415.
[35] M. Kessentini, A. Bouchoucha, H. Sahraoui, and M. Boukadoum,
‚ÄúExample-based sequence diagrams to colored petri nets transformation
using heuristic search,‚Äù in European Conference on Modelling F ounda-
tions and Applications . Springer, Berlin, Heidelberg, 2010, pp. 156‚Äì
172.
[36] C. Lebeuf, M.-A. Storey, and A. Zagalsky, ‚ÄúSoftware bots,‚Äù IEEE
Software , vol. 35, no. 1, pp. 18‚Äì23, 2018.
[37] ‚Äî‚Äî, ‚ÄúHow software developers mitigate collaboration friction with
chatbots,‚Äù arXiv preprint arXiv:1702.07011 , 2017.
[38] M. WESSEL, B. M. DE SOUZA, I. STEINMACHER, I. S. WIESE,
I. POLATO, A. P. CHA VES, and M. A. GEROSA, ‚ÄúThe power of bots:
Understanding bots in oss projects,‚Äù Proceedings of the ACM on Human-
Computer Interaction , vol. 2, pp. 1‚Äì19, 2018.
[39] S. Urli, Z. Yu, L. Seinturier, and M. Monperrus, ‚ÄúHow to design a pro-
gram repair bot?: insights from the repairnator project,‚Äù in Proceedings
of the 40th International Conference on Software Engineering: Software
Engineering in Practice . ACM, 2018, pp. 95‚Äì104.
[40] V . Balachandran, ‚ÄúFix-it: An extensible code auto-Ô¨Åx component in
review bot,‚Äù in 2013 IEEE 13th International Working Conference on
Source Code Analysis and Manipulation (SCAM) . IEEE, 2013, pp.
167‚Äì172.
[41] M. Wyrich and J. Bogner, ‚ÄúTowards an autonomous bot for automatic
source code refactoring.‚Äù
[42] J. Bansiya and C. G. Davis, ‚ÄúA hierarchical model for object-oriented
design quality assessment,‚Äù IEEE Transactions on software engineering ,
vol. 28, no. 1, pp. 4‚Äì17, 2002.
[43] H. M. Olague, L. H. Etzkorn, S. Gholston, and S. Quattlebaum,
‚ÄúEmpirical validation of three software metrics suites to predict fault-proneness of object-oriented classes developed using highly iterative or
agile software development processes,‚Äù IEEE Transactions on software
Engineering , vol. 33, no. 6, pp. 402‚Äì419, 2007.
[44] K. Deb, A. Pratap, S. Agarwal, and T. Meyarivan, ‚ÄúA fast and elitist
multiobjective genetic algorithm: NSGA-II,‚Äù IEEE Transactions on
Evolutionary Computation , vol. 6, no. 2, pp. 182‚Äì197, 2002.
[45] W. F. Opdyke, ‚ÄúRefactoring object-oriented frameworks,‚Äù Ph.D. disser-
tation, University of Illinois at Urbana-Champaign, 1992.
[46] M. Fokaefs, N. Tsantalis, E. Stroulia, and A. Chatzigeorgiou, ‚ÄúJdeodor-
ant: identiÔ¨Åcation and application of extract class refactorings,‚Äù in 33rd
International Conference on Software Engineering (ICSE) , 2011, pp.
1037‚Äì1039.
[47] W. Kessentini, M. Kessentini, H. Sahraoui, S. Bechikh, and A. Ouni,
‚ÄúA cooperative parallel search-based software engineering approach for
code-smells detection,‚Äù IEEE Transactions on Software Engineering ,
vol. 40, no. 9, pp. 841‚Äì861, 2014.

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:37:13 UTC from IEEE Xplore.  Restrictions apply. [48] F. Palomba, G. Bavota, M. D. Penta, R. Oliveto, and A. D. Lucia,
‚ÄúDo they really smell bad? A study on developers‚Äô perception of
bad code smells,‚Äù in 30th IEEE International Conference on Software
Maintenance and Evolution , 2014, pp. 101‚Äì110.
[49] A. Arcuri and L. Briand, ‚ÄúA practical guide for using statistical tests
to assess randomized algorithms in software engineering,‚Äù in 33rd
International Conference on Software Engineering (ICSE) . IEEE, 2011,
pp. 1‚Äì10.

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:37:13 UTC from IEEE Xplore.  Restrictions apply. 