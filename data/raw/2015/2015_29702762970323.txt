Identifying Domain Elements from Textual SpeciÔ¨Åcations
Jitendra Singh Thakur
Indian Institute of Information Technology
Design and Manufacturing, Jabalpur, India
jsthakur@iiitdmj.ac.in,
Jabalpur Engineering College, Jabalpur, India
jsthakur@jecjabalpur.ac.inAtul Gupta
Indian Institute of Information Technology
Design and Manufacturing
Jabalpur, India
atul@iiitdmj.ac.in
ABSTRACT
Analysis modeling refers to the task of identifying domain
objects, their attributes and operations, and the relation-
ships between these objects from software requirements spec-
ications which are usually written in some natural lan-
guage. There have been a few eorts to automate this task,
but they seem to be largely constrained by the language
related issues as well as the lack of a systematic transfor-
mation process. In this paper, we propose a systematic, au-
tomated transformation approach which rst interprets the
specication sentences based on the Hornby's verb patterns,
and then uses semantic relationships between the words in
the sentences, obtained from Type Dependencies using Stan-
ford NL Parser, to identify the domain elements from them.
With the help of a controlled experiment, we show that the
analysis class diagrams generated by the proposed approach
are far more correct, far more complete and less redundant
than those generated by the exiting automated approaches.
CCS Concepts
Software and its engineering !Model-driven soft-
ware engineering; Requirements analysis; Object ori-
ented development;
Keywords
Model transformation; Analysis modeling; Analysis class di-
agram; Automated approach; Natural Language Processing
1. INTRODUCTION
Analysis modeling refers to the task of identifying do-
main objects, their attributes and operations, and the rela-
tionships between these objects from software requirements
specications which are usually written in some natural lan-
guage [4]. The analysis models represent dierent view-
points of a problem domain, so as to allow the developers
to talk and reason about the domain objects in order toenhance their understanding of the problem and avoid po-
tential pitfalls [27].
The analysis modeling requires a group of analysts to read
and analyse typically several hundred pages of software re-
quirements specications which involves signicant eorts
and time. A large semantic gap between a less formal tex-
tual specications and a more precise analysis models like
the UML class diagrams along with human slip-ups makes
the transformation error prone. Moreover, the varying skills
of human analysts, and their understanding of the domain
knowledge and of the mapping process make the transfor-
mation non-repeatable.
Sensing the problem, there have been some eorts in the
direction of formulating semi-automated [28, 13, 34] and au-
tomated [13, 21, 17, 29, 43] approaches that can help to gen-
erate the analysis models from textual specications. The
proposed semi-automated approaches assist the user in de-
riving the analysis models but most of them are highly de-
pendent on the user skills for identifying various elements of
the analysis models.
The existing automated approaches though do not require
human interventions, but suer from serious issues such as
the approaches [13, 17] are unable to identify major ele-
ments of the analysis class diagrams, the approach of Liu et
al. [21] requires a glossary of domain specic terms to iden-
tify the classes and their attributes; building such glossaries
are again dependent on the skills of the domain experts.
The automated approaches [29, 43] identify all the major
elements of the analysis class diagrams but they too suers
from problems. The Popescu et al. approach [29] is unable
to extract the relevant domain elements from a wide variety
of simple sentences. The approach of Yue et al. [43] is un-
able to identify the domain entities (objects and attributes)
consisting of more than one words. As the approach does
not use semantic relationships between the words of the sen-
tences, therefore, it makes errors in extracting the relevant
domain elements from dierent places in the sentences.
The natural languages like English have varieties of sen-
tence types and structures which facilitate us to express a
similar concept in various ways. Therefore, an automated
approach must be accomplished with i) a rich language model
that can correctly interpret the variety of sentences, ii) the
domain elements extraction rules or algorithm that can nd
the semantic relationships between the words of the sen-
tences to correctly extract the relevant domain elements
from dierent positions in the sentences. To improve the
quality of the analysis models, the proposed approach ad-
heres to above observations.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proÔ¨Åt or commercial advantage and that copies bear this notice and the full citation
on the Ô¨Årst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speciÔ¨Åc permission and/or a
fee. Request permissions from Permissions@acm.org.
ASE‚Äô16 , September 3‚Äì7, 2016, Singapore, Singapore
c2016 ACM. 978-1-4503-3845-5/16/09...$15.00
http://dx.doi.org/10.1145/2970276.2970323
566
Table 1: TDs for sentence: \The bank sends the customer an sms." and use of TDs to extract the relevant
domain elements from dierent places in the sentence.
Type dependencies (TDs) Semantic relationship between
the words depicted by TDsUse of TDs to extract relevant domain elements
TDName Head Dependent
The  bank  sends  the customer  an sms.  
(Verb) (Direct Object) (Subject)  (Indirect Object)  nsubj  ( sends , bank ) iobj ( sends , customer ) dobj  ( sends , sms ) 
sms 
 sends ( customer  ) bank  sends 
customer  
 sends ( sms ) bank  sends 
det Bank-2 The-1 \The" is determiner of \Bank"
nsubj sends-3 Bank-2 \Bank" is subject of \sends"
root ROOT-0 sends-3 \sends" is root of the sentence
det customer-5 the-4 \the" is determiner of \customer"
iobj sends-3 customer-5 \customer" is indirect object of \sends"
det sms-7 an-6 \an" is determiner of \sms"
dobj sends-3 sms-7 \sms" is direct object of \sends"
In this paper we propose a systematic, automated ap-
proach to identify the domain elements (domain classes,
their attributes and operations, and the relationships be-
tween the classes). The approach is prototyped in a tool
named AnModeler [36]. The approach takes as input the
functional specications of a problem documented in En-
glish as a Use Case Specication (UCS) [18, 37]. It in-
terprets the specication sentences using a richer language
model (Section 2). As the relevant domain elements are em-
bedded at dierent places in dierent sentences, the domain
elements extraction rules extract them based on the iden-
tied structures of the sentences and the semantic relation-
ships between the words obtained from Type Dependencies
(TDs) [8] (Section 2). With the help of a controlled ex-
periment we compare the quality of analysis class diagrams
generated by the proposed approach with those generated
by existing automated approaches; the results showed that
the analysis class diagrams generated by the proposed ap-
proach are more correct, more complete and less redundant
than those generated by the exiting automated approaches.
The rest of the paper is organised as follows. Section 2
presents the Natural Language Processing (NLP) constructs
(TDs and POS-tags) used by the approach to analyse the
textual specications and to extract the domain elements
from the sentences. It also presents the language model
used by the approach along with restriction rules to be used
for documenting the UCS. Section 3 presents the working
of the approach. Section 4 presents the prototype tool sup-
port developed. Section 5 compare the proposed approach
with the existing approaches on the basis of the quality (cor-
rectness, completeness and redundancy) of the analysis class
diagrams generated by the approaches. Section 6 discusses
the strengths and limitations of the proposed approach. Sec-
tion 7 presents the related work in the literature. Section 8
presents the conclusion and the future directions of the pre-
sented work.
2. NLP CONCEPTS & LANGUAGE MODEL
To identify the semantic relationships between the words
of the sentences, the approach generates two NLP constructs
i) Type dependencies (TDs) and ii) Parts of speech tags
(POS-tags) from the sentences in the UCS using the Stan-
ford NL parser API. The approach then uses these NLP
constructs rst to identify the sentence structure of the sen-
tences, then to extract the class diagram elements from
them.
i)Type Dependencies (TDs) : A type dependency (TD)
represents grammatical dependency relationship (bi-lexical
asymmetrical relationship) between the words of a sentence.The TDs can be used to obtain the semantic relationships
between the words in the sentence [8], so they are used in
the proposed approach to disambiguate the extraction of
relevant domain elements from the sentences. The proposed
approach uses Stanford Parser APIs version 2.0.4 to gen-
erate TDs from the sentences. The present Stanford typed
dependencies1set can identify 53 such grammatical relation-
ships. The parser generates a TD as a triplet structure td-
Name(head, dependent) , where tdName represents the name
of the dependency, head represents the head word and de-
pendent represents the dependent word. More formally td-
Name depict that the dependent word is related to the head
word by the dependency tdName .
Example 1.For the sentence \The bank sends the cus-
tomer an sms.", the TDs generated by the parser are given
below.
TDs = [det(bank-2, The-1), nsubj(sends-3, bank-2), root(
ROOT-0, sends-3), det(customer-5, the-4), iobj( sends-3,
customer-5), det(sms-7, an-6), dobj(sends-3, sms-7)]
The description of the TDs are shown in Table 1. The
last column in this table presents the graphical view of some
important dependencies in the sentence, and how these TDs
help to disambiguate the extraction of relevant domain ele-
ments from the sentence.
ii)Parts-of-Speech tags (POS-tags) : The POS-tags2re-
fer to the words in a sentence tagged (or annotated) with
parts of speech, such as noun, pronoun, verb, adjective, ad-
verb, etc. When a sentence is given as input to the parser, it
analyses the sentence, and assigns each word in the sentence
with a POS-tag taken from a set of 36 such POS-tags3.
Example 2.If a sentence: \The bank sends the customer
an sms." is given as input to the parser then the POS tagged
sentence generated by the parser as output is:
\[The/DT, bank/NNP, sends/VBZ, the/DT, customer/NN,
an/DT, sms/NN, ./.]"
Where, the tag /DT after a word denote that the word is
determiner, /NNP denote that the word is a singular proper
noun, /NN denote that the word is a singular noun, and
/VBZ denote that the word is a 3rd person singular present
tense verb.
1http://nlp.stanford.edu/software/dependencies manual.
pdf(accessedApril29,2016)
2http://nlp.stanford.edu/software/tagger.
html(accessedFebruary29,2016)
3http://www.comp.leeds.ac.uk/amalgam/tagsets/upenn.
html(accessedFebruary29,2016)
567Table 2: Sentence structure rules
Rule
#Antecedent (If the
sentence contains
TDs:)Consequent
(then the iden-
tied sentence
structure is:)Example
sentenceType dependencies of Example sentence
SSR1 nsubj (A,B),
iobj(A,C), dobj(A,D)SVIODO
(Subject-Verb-
IndirectObject-
DirectObject)The system
sends the user
an email.[det(system-2, The-1), nsubj (sends-3, system-2), root(ROOT-0,
sends-3), det(user-5, the-4), iobj(sends-3, user-5), det(email-7,
an-6), dobj(sends-3, email-7)]
SSR2 nsubj (A,B),
dobj(A,C),
complm (D,E),
nsubj (D,F)SVDOThatClause
(Subject-Verb-
DirectObject-
ThatClause)The system
informs the
user that the
battery is full[det(system-2, The-1), nsubj (informs-3, system-2), root(ROOT-
0, informs-3), det(user-5, the-4), dobj(informs-3, user-5),
complm (full-10, that-6), det(battery-8, the-7), nsubj (full-10,
battery-8), cop(full-10, is-9), ccomp(informs-3, full-10)]
SSR3 nsubj (A,B),
complm (C,D),
nsubj (C,E)SVThatClause
(Subject-Verb-
ThatClause)The system
validates that
the password
is correct[det(system-2, The-1), nsubj (validates-3, system-2), root(ROOT-
0, validates-3), complm (correct-8, that-4), det(password-6,
the-5), nsubj (correct-8, password-6), cop(correct-8, is-7),
ccomp(validates-3, correct-8)]
SSR4 nsubj (A,B),
dobj(A,C),
neg(D,E), aux(D,F),
infmod (C,D)SVDONotToInf
(Subject-Verb-
DirectObject-
Not-To-
Innitive)The system
warns the user
not to restart
the system.[det(system-2, The-1), nsubj (warns-3, system-2), root(ROOT-0,
warns-3), det(user-5, the-4), dobj(warns-3, user-5), neg(restart-8,
not-6), aux(restart-8, to-7), infmod (user-5, restart-8), det(system-
10, the-9), dobj(restart-8, system-10)]
SSR5 nsubj (A,B),
neg(C,D), aux(C,E),
xcomp (A,C),
dobj(C,F)SVNotToInf
(Subject-
Verb-Not-To-
Innitive)The customer
selects not to
ll the tank[det(customer-2, The-1), nsubj (selects-3, customer-2),
root(ROOT-0, selects-3), neg(ll-6, not-4), aux(ll-6, to-5),
xcomp (selects-3, ll-6), det(tank-8, the-7), dobj(ll-6, tank-8)]
Table 3: Restriction rules for documenting use case specications
# Rule description Rationale
1Use simple sentences to write the steps, except for a few
complex sentences such as sentences expressing validation or
check (example: \The system validates that the password
is correct.") and sentences specifying conditions (example:\If
the ATM card is invalid, the system ejects the card.")In order to reduce ambiguity [19, 39], various authors in literature rec-
ommend the use of simple sentences for documenting the UCSs. The
sentence structure rules used by the approach, presently, are able to
process the simple sentences and a few complex sentences.
2Do not use the pronouns. As the state of the art approaches for pronoun resolution problem have
precisions between 80-90% [20], the approach avoids the use of pronouns
for documenting the UCS to avoid the errors in pronoun resolution.
3Use consistent names for things, concepts etc. (i.e. use of
dierent names to represent same thing or concept at dierent
places in UCS must be avoided)The approach recommends using consistent names for things and con-
cepts in order to avoid the identication of multiple classes representing
same thing or concept.
4Use \system" or use case name to refer to the system under
development.To avoid the identication of multiple classes representing the system.
5Use keywords:
INCLUDE to specify include relationship with other UCS The approach creates an association relationship named INCLUDE be-
tween the control classes of the two UCSs.
EXTEND to specify extend relationship with other UCS The approach creates an association relationship named EXTEND be-
tween the control classes of the two UCSs.
RESUME to specify resume or return of control to a specic
step in the UCSThe approach creates a resumeStep() operation in the control class of
the UCS. This can further help in the generation of template code which
could be one of the future directions of the proposed approach.
REPEAT to specify the repeated execution of some steps in
the UCS.The approach creates a repeatSteps() operation in the control class of
the UCS. This can further help in the generation of template code which
could be one of the future directions of the proposed approach.
Language Model : We dene the language model of an ap-
proach as the set of sentence patterns that the approach can
interpret. The language model of the proposed approach
consists of a comprehensive set of sentence patterns that in-
clude all the simple sentences (including sentences contain-
ing participles, innitives and gerunds) and a few complex
sentences (sentences specifying conditions and the sentences
containing that clause and conjunctive clause) written in En-
glish (For details on types of sentences please refer [37, 10]).
The proposed language model is constructed using the 25
verb patterns proposed by Hornby et al. in [15, 38]. These
verb patterns dene the 25 possible ways in which a verb
phrase in a sentence can be written. Using all these verb
patterns, we framed the comprehensive set of sentence struc-
ture rules to recognize the structures of sentences. These
rules use TDs of the sentences to identify their structures.
Table 2 contains a few sentence structure rules for the il-
lustrations (for the complete rule set, please refer to [37]).
These rules are presented in Antecedent-Consequent formatand they are ordered. To identify the sentence structure of
a given sentence, the approach sequentially searches for all
TDs in the Antecedent part of each rule against the TDs
of the given sentence. In case of a match, the Consequent
part of the rule is used to determine the structure of that
sentence.
The language model along with the need to avoid ambigu-
ities [19, 39] in the sentences, enforce the UCSs to be writ-
ten in English language using a few restriction rules. The
restriction rules along with their rationale are presented in
Table 3.
The following example shows how the approach identies
the sentence structures.
Example 3.Here we show how the approach identies
the sentence structure of a given sentence using the rules
presented in Table 2. For sentence say S=\The system sends
the user an email.", the TDs generated by the parser are:
568[det(system-2 The-1), nsubj (sends-3,system-2), root(ROOT-
0, sends-3), det( user-5, the-4), iobj( sends-3, user-5), det(
email-7, an-6), dobj ( sends-3, email-7)]
Here, for each rule shown in Table 2 the approach one by
one checks whether the TDs present in the Antecedent part
of the rule are found in the TDs of the sentence. All the TDs
in the Antecedent part of the rule SSR1 are found in the
TDs of the sentence, hence the approach gets the sentence
structure as SVIODO from Consequent part of the rule.
3. PROPOSED APPROACH
The approach works in ve steps. Its schematic diagram,
along with the Example Input and Example Ouput of each
setp is shown in Figure 1. The key steps are Step 3 and Step
4. In Step 3, the approach uses the semantic relationships
between the words obtained using TDs to identify the struc-
tures of the sentences based on Hornby's verb patterns. In
Step 4, the approach processes the sentences based on their
identied structures, and uses the semantic relationships be-
tween the words to extract the relevant domain elements em-
bedded in the sentences. The domain elements extraction
rules (aka transformation rules) also take care of the object
oriented analysis principals to map the extracted domain el-
ements to the analysis class diagram.
Step 1 and 2: Read and Parse UCS
The Step 1 reads the sentences from each section of the input
UCS. Then Step 2 parses each sentence using Stanford NL
Parser API to obtain TDs depicting the semantic relation-
ships between the words of the sentence, and also obtains
the POS-tags.
Step 3: Identify Sentence Structures
The proposed approach uses a comprehensive set of sentence
structure rules (SSR1-SSR33) to identify the structures of
the sentences as described in Section 2. The identied struc-
tures of the sentences along with their TDs and POS-tags
are then used in Step 4.4 to identify class operations and/or
attributes. If sentence structures of some sentences can-
not be identied then those sentences are marked and pre-
sented to the user for modication. The user can modify
sentences or may choose to continue without modifying. If
user chooses to continue without modifying the unidentied
sentences then the unidentied sentences are skipped by the
transformation process.
Step 4: Identify Domain Elements
The following steps present how the proposed approach iden-
ties elements of the analysis class diagram.
Step 4.1: Identify Control and Boundary Classes (Rules
TR1-TR2)
i) The approach creates a control class4using the name of
the UCS (transformation rule-TR1). For one UCS, only one
control class is created, and from now all the references to
the word \System" and to the name of UCS in the sentences
refer to this control class.
ii) The approach creates a boundary class5for each actor
of the UCS (transformation rule-TR2).
4A class that represents control tasks performed by a system
5A class that models interactions between an actor and the
systemStep 4.2: Identify Entity Terms (Rule TR3)
The noun phrases in the sentences may contain single nouns
(Example: Transaction, Withdrawal) or a group of two or
more consecutive nouns (group of nouns representing a sin-
gle term, Example: Transaction number, ATM Card, ATM
Card PIN number). These single nouns or group of two or
more consecutive nouns are called entity terms. The entity
terms are the prospects of potential classes or attributes.
This step combines two or more consecutive nouns in the
sentences to identify the entity terms (transformation rule-
TR3). This is done by concatenating two or more consecu-
tive words in the sentences whose POS-tag starts with\NN".
The TDs and POS-tags of the sentence are updated to reect
the changes (concatenation) done if any. A similar method
for entity term extraction is used in an approach for entity
disambiguation proposed by [24].
Step 4.3: Identify Entity Classes and their Attributes
(Rules TR4-10)
The step applies heuristics (transformation rules TR4-TR10)
on the identied entity terms and TDs of the sentences to
identify the entity classes and attributes; the identied en-
tity classes are stored in setOfClasses6. Rule TR4 is given
below, for rules TR5-TR10 please refer [37]
Rule-TR4 : For every two entity terms t1 and t2 in setOfEn-
tityTerms
If (t2 startsWith t1) AND (t2 6=t1) then
class = createClass (t1,\<<entity class >>");
class.addAttribute(t2);
EndIf
EndFor
Step 4.4: Identify Class Operations and More Class
Attributes (Rules TR11-TR43)
As the ow sentences (sentences in the main ow, sub ow
and alternate ow sections of UCS) species the sequence
of actions preformed by the system and the actor(s) of the
system, the verbs representing these actions can be used to
identify class operations. For each operation, three things
are to be identied: i) the operation, ii) source entity term
that calls this operation and iii) destination entity term that
hosts this operation. As these essential elements are embed-
ded at dierent places in dierent sentences, the sentences
with dierent sentence structures are dealt dierently, and
the dependency relationships between the main verb and the
other words in the sentences are used to correctly extract
these elements.
With the help of various drills on more than 40 sen-
tences of each sentence pattern, we carefully constructed
a comprehensive set of domain elements extraction rules
(or transformation rules TR11-TR43) to identify the oper-
ations, the source entity terms and the destination entity
terms from the TDs. The transformation rules TR11-TR15
are presented in Table 4 (for full rule set please refer [37]).
These rules are presented in Antecedent-Consequent for-
mat. The Antecedent part or If part (Column A) contains
the sentence structure of the sentences to be matched for
the rules to be red, the Consequent part or then part has
two columns (Column B and column C) where, column B
presents the TDs of the sentence to be used for identify-
6A set which is used to store the classes identied by the
approach
569Step 1: Read the sentences from UCS  
Step 2: Parse the sentences to obtain 
TDs and POS tags  
Step 3: Identify sentence structure of 
each sentence (rules SSR1 -SSR33)  
Step 5: Generate & Visualize analysis 
class diagram  
(Output ‚Äì Analysis class diagram of the UCS)  UCName :  WithdrawFund 
Actor : Customer  
PreCondition : ATM card validation done  
Main Flow:  
1. Customer selects Withdrawal.  
2. Customer enters the withdrawal 
amount. ‚Ä¶‚Ä¶‚Ä¶‚Ä¶..  
 
   Step 4.2: Identify entity terms  
                         (Rule TR3)  
  Step 4.1: Identify control and boundary 
classes   (Rules TR1- TR2)  
Step 4.4: Identify entity classes and their 
attributes (Rules TR4- TR10)  
Step 4.5: Identify class operations, class 
attributes (Rules TR11- TR43)  
Step 4.6: Identify more entity classes 
(Rule TR44- TR45)  
Step 4.7: Identify association 
relationships (Rule TR46)  
Step 4.8: Identify generalization 
relationships (Rule TR47- TR50)  
Step 4.10: Eliminate extra classes  
(Rule TR54)0 
Step 4.9: Identify aggregation 
relationships (Rules TR51- TR53)  Sentence=‚ÄúATM customer enters the withdrawal amount.‚Äù  TDs=[ nn(customer -2, ATM -1), nsubj (enters- 3, customer -2), 
root(ROOT -0, enters- 3), det(amount -6, the- 4), nn(amount -
6, withdrawal -5), dobj (enters- 3, amount -6)] 
POS-tags=[ATM/NNP, customer/NN, enters/VBZ, the/DT, 
withdrawal/NN, amount/NN, ./.]  
Sentence=‚ÄúATM customer enters the ATM Card Pin Number.‚Äù  
POS=tags=[ATM/NNP, customer/NN, enters/VBZ, the/DT, 
ATM/NNP, Card/NNP, Pin/NNP, Number/NNP, ./.]  entityTerms= {ATMcustomer, ATMCardPinNumber}  
entityTerms ={`ATMcustomer , ATMCardPinNumber , 
Withdrawal, withdrawalAmount } Withdrawal  
withdrawalAmount  
TDs= [ nsubj (enters- 2, ATMCustomer -1),   dobj (enters- 2,  
withdrawalAmount -4)] Sentence Structure = SVDO  Example Input  Example Output  Steps of approach  
Sentence=‚ÄúATM customer enters the withdrawal amount.‚Äù  
TDs=[ nn(customer -2, ATM -1), nsubj (enters- 3, customer -2), 
root(ROOT -0, enters- 3), det(amount -6, the- 4), nn(amount -6, 
withdrawal -5), dobj (enters- 3, amount -6)] 
UCName :  WithdrawFund 
Actor : ATM Customer  
 WithdrawFund 
<<Control Class>> ATMCustomer  
<<Boundary Class>> 
operationName=‚Äúenters‚Äù sourceEntityTerm=ATMCustomer destinationEntityTerm=withdrawalAmount  
operationName=‚Äúenters‚Äù 
sourceEntityTerm =ATMCustomer  
destinationEntityTerm =withdrawalAmount  Withdrawal  
withdrawalAmount  
enters ATMCustomer  
<<Boundary Class>> enters Sentence Structure = SVDO  Sentence=‚Äú ATMCustomer  enters the withdrawalAmount .‚Äù 
Withdrawal  
withdrawalAmount  Transaction  
Deposit  Transfer  Sentence=‚ÄúThe withdrawal, deposit and transfer are types 
of transaction.‚Äù  
POS-tags=[The/DT, withdrawal/NN, ,/,, deposit/NN, ,/,, 
transfer/NN, and/CC, query/NN, are/VBP, types/NNS, of/IN, transaction/NNS, ./.]  
Book  
bookDetail BookDetail 
Book  BookDetail Step 4: Identify domain elements 
(rules  TR1- TR54)  Use Case Specification (UCS)  Figure 1: Schematic diagram of the proposed approach - with example input and output of each step
570Table 4: Transformation rules to identify operations and attributes of classes
Antecedent (If A) Consequent ( then use B to identify C )
Rule # A (Sentence
structure)B (TDs of the sentence:) C (operations/attributes:)
TR11 SVIODO nsubj (A,B), iobj(A,C), dobj(A,D) op.SourceEntityTerm=B, op.DestEntityTerm=D, op.name=A
TR12 SVDOThatClause nsubj (A,B), dobj(A,C), complm (D,E),
nsubj (D,F)op.SourceEntityTerm=B, op.DestEntityTerm=C, op.name=A
TR13 SVThatClause nsubj (A,B), complm (C,D),
nsubj (C,E)op.SourceEntityTerm=B, op.DestEntityTerm=E, op.name=A
TR14 SVDONotToInf nsubj (A,B), dobj(A,C), neg(D,E),
aux(D,F), infmod (C,D)op.SourceEntityTerm=B, op.DestEntityTerm=C, op.name=A
TR15 SVNotToInf nsubj (A,B), neg(C,D), aux(C,E),
xcomp (A,C), dobj(C,F)op.SourceEntityTerm=B, op.DestEntityTerm=F, op.name=A
Note: \op" is a meta object used to store the identied operation
ing operations/attributes and column C denes how the ap-
proach identies class operations and attributes from these
TDs when the rule is red. To identify the class operations
and attributes from a given sentence, the approach one by
one compares the sentence structure of the given sentence
with that present in column A, if a match is found then the
approach uses the TDs present in column B to identify the
operations/attributes as shown in column C. These identi-
ed operations are further used in Step 4.5 for identifying
more entity classes and in Step 4.6 for identifying associa-
tion relationships between the classes.
Step 4.5: Identify More Entity Classes (Rule TR44-
TR45)
The source entity terms and destination entity terms of the
operations identied in previous step are used in this step
to identify more entity classes. A source entity term is the
caller of the identied operation hence it is the prospect of
a class (if a class for the source entity term does not exist in
setOfClasses , a new entity class is created for it (transfor-
mation rule TR44)). Whereas, a destination entity term has
two possibilities: it may either be an attribute of an existing
class in which the identied operation is to be hosted or may
be a prospect of a class (if a class for the destination entity
term exists in setOfClasses then the identied operation is
hosted in that class, otherwise a new entity class is created
with the identied operation hosted in it, and the class is
added to setOfClasses ) (transformation rule TR45).
Step 4.6: Identify Association Relationships (Rule
TR46)
From the operations identied in step 4.5, the approach iden-
ties association relationship between the class representing
source entity term and the class representing destination en-
tity term (if the association relationship between them does
not already exist in setOfRelationships7). The navigabil-
ity of the relationship is set from the class representing the
source entity term to the class representing the destination
entity term (transformation rule TR46).
Step 4.7: Identify Generalization Relationships (Rules
TR47-TR50)
The sentences in ows and description sections of UCS which
contains sub strings of types \is a", \kind of" and all their
synonyms are used to identify generalization relationships
(transformation rules TR47-TR50 [37]).
7A set which is used to store the identied relationshipsStep 4.8: Identify Aggregation Relationships (Rules
TR51-TR53)
The approach identies aggregation relationships using two
tactics: 1) When a class say C1 is present as an attribute
in another class say C2 in the setOfClasses , then class C1
is recognized as part class and C2 is recognized as whole
class, and aggregation relationship is created between the
part class and whole class (transformation rule TR51). 2)
The sentences in ows and description sections of UCS con-
taining sub strings such as \part of", \consists of", \contains"
and all their synonyms are used to identify aggregation re-
lationships (transformation rules TR52-TR53 [37]).
Step 4.9: Eliminate Extra Classes (Rule TR54)
Every noun or entity term may not be a class, if a noun or
an entity term is identied as a class by the approach, then
it must participate in relationship with some other class (or
classes) in the class diagram. Hence, if some classes are
not participating in any relationships then those classes are
extra classes and are removed from setOfClasses (transfor-
mation rule TR54).
Step 5: Generate and Visualize the Class Diagram
This step uses the identied classes and relationships to gen-
erate DOT language commands (commands that are used
to create and visualize various graph elements such as lines,
rectangles, triangles etc. in GraphViz) which are then visu-
alized using GraphViz API as a class diagram.
4. TOOL SUPPORT DEVELOPED
We have implemented the proposed approach in a pro-
totype tool named Analysis Modeler (AnModeler) in Java
1.6 using Eclipse Indigo IDE release 3.7.0. AnModeler uses
Stanford NLP parser APIs version 2.0.4 for parsing the sen-
tences. It uses Apache POI API 3.9 for reading the UCSs
written in MS Excel les. For in-place visualizing of the
generated class diagrams it uses Graphviz API. It also uses
a Java API to call dot (GraphViz) from Java program. The
interested readers can nd the details of AnModeler in [36].
5. EVALUATION
This section reports a controlled experiment that we
conducted for comparing the analysis class diagrams gen-
erated by the proposed approach with those generated by
the two existing approaches, one proposed by Popescu et
al. [29] and the other proposed by Yue et al. [43]. We se-
lected these approaches [29, 43] for comparison with the pro-
posed approach because these are the recent approaches, and
571they can identify the all class diagram elements which are
identied by the proposed approach also. Whereas, the ap-
proaches [13, 17] do not identify the major elements of the
analysis class diagrams and the Liu et al. approach [21] re-
quires glossary of terms dening domain entities to identify
the classes.
In the experiment the analysis class diagrams gener-
ated by the three approaches for 40 UCSs were evaluated
by 40 subjects on the basis of correctness, completeness and
redundancy. The procedure followed for conducting the ex-
periment is based the guidelines for experimental studies
presented in [40, 35, 41].
5.1 Metrics Used for Evaluation
We used the quality metrics for computing Class Di-
agram Correctness ( CD cr), Class Diagram Completeness
(CD cm) and Class Diagram Redundancy ( CD rd) whose de-
tails can be found in [37].
5.2 Variables
Following are the independent and the dependent vari-
ables of the experimental study: i) Independent variables:
The analysis class diagrams generated using the three ap-
proaches. ii) Dependent variables: Class Diagram Correct-
ness (CD cr),Class Diagram Completeness ( CD cm)andClass
Diagram Redundancy ( CD rd)of the analysis class diagrams
generated by the three approaches.
5.3 Subject and Object Selection
The subjects (or participants) in the experiment were
40 students of Computer Science and Engineering discipline,
a mix of senior undergraduate (B.Tech) and junior post-
graduate students (M.Tech and Ph.D). All the subjects had
done a basic course in Software Engineering as well as ei-
ther Object-Oriented Software Engineering course or Object-
Oriented Analysis and Design course, both of which re-
quired a course project which needed them to develop suit-
able object-oriented analysis and design models for some
specic problems.
The objects were the analysis class diagrams generated
by the three approaches viz. Popescu et al. [29], Yue et
al. [43] and the proposed approach for 40 UCSs. We took
these 40 UCSs from various software engineering books [4,
27, 12, 33, 32, 5] and research works [21, 29, 43, 45, 9]. The
sentences in these UCSs were modied as per the restrictions
given in Table 3 except for the Restriction No. 1 (since the
restrictions No. 2-4 are common for all the 3 approaches).
In case of the Restriction No. 5 which persuade the use
of some keywords, the input UCSs for each approach were
modied to include the keywords (if any) recognized by that
approach.
5.4 Experimental Design
The experiment design was a complete block design in
which each subject evaluated the analysis class diagrams
of all the three approaches for a dierent UCS provided to
him/her.
5.5 Instrumentation/Experimental-material
The experiment material provided to each subject was:
i) A use case specication (UCS). ii) The analysis class dia-
grams generated by the three approaches for the given UCS.
iii) Questionnaires for evaluating correctness, completenessand redundancy, based on the quality metrics presented
in [37], of the analysis class diagrams of each approach.
5.6 Execution of the Experiment
The experimental materials were randomly distributed
to the subjects. The subjects were clearly explained the
steps for reading the UCS, examining the analysis class dia-
grams and lling the answers to the questionnaires. We pro-
vided the subjects maximum of three hours time for com-
pleting the experiment (In a prior mock experiment with
four UCSs of dierent sizes taken from the 40 UCSs used
in this experimental study, we found that the time needed
to answer the questionnaires was 1 hour 20 minutes for the
UCS of smallest size and 2 hours 45 minutes for the UCS
of largest size. So we decided the maximum time of 3 hours
for the experiment.). Each subject had to answer the ques-
tionnaires for all the three analysis class diagrams provided
to him/her. In the experiment, each subject, rst, read the
UCS provided to him/her, then examined the analysis class
diagrams and lled the answers to the given questionnaires.
The subjects were not allowed to talk and share the an-
swers with each other during the experiment. As soon as a
subject had completed his/her experiment, the answers to
questionnaire were collected from him/her, and the subject
was allowed to move out.
5.7 Results
Table 5 shows the data set for the correctness, com-
pleteness and redundancy of the analysis class diagrams of
the three approaches for each UCS. This data set is obtained
from the answers to the questionnaires collected from the
subjects. The variability of the data for the correctness,
completeness and redundancy of the analysis class diagrams
generated by the approaches for the 40 UCSs are shown us-
ing box plots in Figure 2.
5.8 Validity Consideration
Here we present the important threats to validity in our
experimental study and discuss the strategies used to deal
with these threats [40, 35, 41]
Internal Validity : One of the main threats to internal va-
lidity was the knowledge, understanding and experience of
the subjects in object oriented concepts and analysis mod-
eling. All the subjects have studied at least two courses in
Object Oriented Software Engineering , one of which is Object
Oriented Analysis and Design , they have also done a course
project involving object oriented analysis . Additionally, we
organised training sessions to brush up their analysis mod-
eling concepts; and tested their concepts and understanding
through some exercises.
The three approaches were compared on the basis of
the analysis class diagrams generated by them for the same
set of 40 UCSs and using the same quality measures; hence
there was no comparison bias threat.
Construct Validity : The threat to construct validity was:
Whether the measures used for the evaluation of the class
diagrams generated by the tools are able to depict the qual-
ity of the class diagrams? We minimized this threat by using
the quality metrics presented in [37] for estimating the qual-
ity of analysis class diagrams generated by the tools. These
quality metrics are based on a systematic literature review
on model quality [25] and on the quality measures which
572Table 5: The correctness, completeness and redundancy of the analysis class diagrams generated by the three
approaches for the 40 UCSs.
S# UseCaseNameCDcr CDcm CDrd
Popescu
et al.Yue
et al.Our Popescu
et al.Yue
et al.Our Popescu
et al.Yue
et al.Our
1 AGV Move to Station 0.55 0.34 0.90 0.48 0.50 1.00 0.35 0.50 0.00
2 Arena Announce Tournament 0.57 0.51 0.98 0.27 0.87 0.96 0.00 0.31 0.00
3 ATM QueryAccount 0.70 0.83 0.90 0.50 0.68 1.00 0.00 0.14 0.00
4 ATM TransferFund 0.60 0.62 0.93 0.44 0.82 1.00 0.20 0.25 0.00
5 ATM ValidatePIN 0.52 0.52 0.93 0.52 0.70 1.00 0.25 0.49 0.00
6 ATM Withdraw Funds 0.20 0.65 0.95 0.17 0.72 1.00 0.50 0.31 0.00
7 ATM Withdrawl Transaction UCDA 0.55 0.73 0.96 0.60 0.83 1.00 0.37 0.15 0.00
8 Elevator Dowser 0.59 0.75 0.95 0.50 0.44 0.82 0.44 0.29 0.00
9 Elevator Request Elevator 0.41 0.60 0.93 0.51 0.92 1.00 0.17 0.30 0.00
10 Elevator Select Destination 0.42 0.58 0.91 0.41 1.00 1.00 0.20 0.19 0.00
11 EMS Generate Alarm 0.19 0.68 0.95 0.63 0.75 1.00 0.64 0.32 0.00
12 EMS Generate Monitoring Data 0.38 0.58 0.90 0.46 0.63 1.00 0.25 0.25 0.00
13 EMS View Alarms 0.38 0.66 0.93 0.63 0.64 1.00 0.42 0.31 0.00
14 EMS View Monitoring Data 0.39 0.20 0.74 0.50 0.50 1.00 0.35 0.50 0.20
15 iCoot Browse Index 0.48 0.75 0.90 0.50 0.50 0.88 0.29 0.25 0.00
16 iCoot Cancel Reservation 0.43 0.47 0.93 0.41 0.73 1.00 0.42 0.33 0.00
17 iCoot Change Password 0.70 0.65 0.93 0.48 0.80 1.00 0.13 0.38 0.00
18 iCoot Log O 0.30 0.85 0.93 0.33 0.50 1.00 0.50 0.25 0.00
19 iCoot LogOn 0.59 0.85 0.85 0.56 0.39 1.00 0.17 0.25 0.00
20 iCoot Make Reservations 0.40 0.65 0.93 0.53 0.72 1.00 0.31 0.46 0.00
21 iCoot Search 0.70 0.66 0.96 0.90 0.90 1.00 0.00 0.08 0.00
22 iCoot View Car Model Details 0.40 0.41 0.88 0.20 0.65 1.00 0.38 0.29 0.00
23 iCoot View Member Details 0.30 0.64 0.82 0.25 0.67 1.00 0.00 0.29 0.00
24 iCoot View Rentals. 0.38 0.16 0.87 0.50 0.25 1.00 0.42 0.50 0.00
25 iCoot View Results 0.30 0.64 0.83 0.25 0.38 1.00 0.50 0.30 0.00
26 Internet Book Store Show Book Details 0.33 0.33 0.88 0.33 0.50 0.82 0.33 0.50 0.00
27 Internet Book Store Write Review 0.67 0.61 0.97 0.38 0.65 0.96 0.17 0.40 0.00
28 JEWEL Zoom Map 0.19 0.84 0.97 0.28 0.81 0.95 0.90 0.18 0.00
29 MyTrip ExecuteTrip 0.34 0.52 0.74 0.44 0.67 1.00 0.50 0.29 0.10
30 MyTrip PlanTrip 0.30 0.82 0.92 0.32 0.61 1.00 0.48 0.35 0.00
31 OSS Browse Catalog 0.30 0.42 0.98 0.10 0.90 1.00 0.50 0.17 0.00
32 OSS Process Delivery Order 0.69 0.57 0.80 0.50 0.69 0.94 0.13 0.20 0.00
33 Print Pack Types 0.39 0.63 0.91 0.50 0.75 1.00 0.25 0.23 0.00
34 QVS Perform Verication 0.43 0.44 0.95 0.36 0.68 1.00 0.39 0.48 0.00
35 Ticket Distributor PurchaseTicket 0.52 0.80 0.82 0.37 0.93 0.86 0.25 0.15 0.12
36 TTMS Monitor Train Systems 0.46 0.46 0.94 0.33 0.64 1.00 0.29 0.38 0.00
37 TTMS Route Train 0.32 0.23 0.94 0.48 0.65 0.95 0.64 0.76 0.00
38 VTS Cancel Approved Request 0.48 0.43 0.96 0.45 0.61 0.96 0.31 0.25 0.00
39 VTS Edit Pending Request 0.48 0.44 0.92 0.46 0.63 1.00 0.21 0.29 0.00
40 VTS Withdraw Request 0.48 0.48 0.86 0.51 0.64 1.00 0.29 0.50 0.00
Average 0.48 0.58 0.91 0.43 0.67 0.98 0.32 0.32 0.01
CDcr=Class Diagram Correctness, CDcm=Class Diagram Completeness, CDrd=Class Diagram Redundancy
were previously dened and used for estimating the correct-
ness, completeness and redundancy of the class diagrams
in [44].
Conclusion Validity : We minimized the threats to statis-
tical conclusion validity by applying the suitable statistical
tests on the data. First, we checked the normality of the
data obtained from the experiment by applying Kolmogorov-
Smirnov test, and found that the data was not normal. Then
we applied Friedman test (a non parametric test) for hypoth-
esis testing as our experimental design, procedure applied for
the execution of experiment and the data collection methods
fullled all the conditions for applying Friedman test on the
data. We tested all hypotheses considering the signicance
level of 0.01 (p <0.01). Hence, we are condent that the
correct statistical tests were applied, as the assumptions of
the statistical tests were not violated. (Due to limit in the
length of the paper, the details of all these tests cannot be
included, the interested readers can nd them in [37])
External Validity : One of the threats to external validity
is whether the results obtained through the answers from
the students generalize to software professionals. The sub-
jects of the experiment were well trained in object orientedanalysis and modeling through the courses in object oriented
software engineering and the course projects done involving
analysis and design. Additionally, we conducted training
sessions to brush up their analysis modeling concepts. Their
concepts and understanding were then tested through dier-
ent exercises involving analysis modeling. Studies like [16,
14, 3] reported no signicant dierence between the students
and software professionals used as subjects in the experi-
ments. Porter et al. in their work [30] reported identical
outcomes for the two experiments conducted by them, one
involving students as subjects and the other replicated ex-
periment that involved software professionals as subjects.
Moreover, studies that involves object oriented design and
modeling with UML like [2] suggested that the students
are better trained representatives than most professionals,
specically those who have not been taught OO modeling
with UML in detail. A survey of controlled experiment in
software engineering [35] reported that 81% percent of the
subjects were students in a total of 113 experiments investi-
gated in the study.
In a replicated experiment reported in [36] conducted
with two industry experts (having more than three years of
industry experience in analysis modeling) as subjects and
5730 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 
Popescu et al.  Yue et al.  Our approach  0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 
Popescu et al.  Yue et al.  Our approach  
Correctness  Completeness  Redundancy  0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 
Popescu et al.  Yue et al.  Our approach  Figure 2: Box plot for the correctness, completeness and redundancy of the analysis class diagrams - showing
the IQR, whiskers (1.5*IQR) , and max/min outliers.
the analysis class diagrams generated by AnModeler for ten
UCSs (selected randomly from the 40 UCSs) as objects An-
Modeler achieved results very close to that achieved by the
proposed approach in the experimental study reported in
this paper.
Other threats to external validity in the experiments
like ours' are the size of the study and the number of the
representative of the population. In our study we used the
40 UCSs from various domains such as control system, real
time system, web applications etc. All these UCS were from
the standard software engineering books such as [4], [5], [12],
Rosenberg et al. [33, 32] etc. and research works [21, 29, 43,
45, 9]. Although, these UCSs were not the true representa-
tive of the industry level UCS, but are fairly good. Moreover,
the same UCSs were used in all the three treatments so their
eect if any would had been be same in the evaluation of all
the approaches. Regarding the number of representative of
the population, in the same survey [35] as reported above
in the total of 113 experiments investigated the number of
subjects per experiment ranged from 04 to 266, with a mean
value of 48.6. Hence we think the 40 subjects in our study
were fair in number.
6. DISCUSSION
The proposed approach for generating analysis class di-
agrams from UCSs is a fully automated approach which is
supported by a GUI based tool support named AnModeler .
The approach provides in-place visualization of the gener-
ated analysis class diagrams.
The results of the experimental study conducted for the
evaluation of the approach showed that the analysis class
diagrams generated by the proposed approach are far more
correct, far more complete and less redundant than those
generated by Popescu et al. approach [29] as well as Yue
et al. [43] approach. The proposed approach was able to
achieve high scores because: i) the comprehensive language
model (or the comprehensive sentence structure rules) used
in the approach is able to correctly interpret the wider va-
riety of sentences as compared to the existing approaches,
ii) the transformation rules use the semantic relationships
between the words obtained from TDs to correctly extract
the relevant domain elements from dierent positions in the
sentences, and map them to the relevant classes, their at-
tributes, operations, and relationships between the classes.
A replicated experiment reported in [36] conducted with two
industry experts validates the scores achieved by the pro-
posed approach.As the Popescu et al. approach [29] can interpret only
the simple sentences with ve patterns (SV, SVDO, SVPO,
SVtobePO and SVDOPO), thereofore, it is unable to cor-
rectly extract the relevant domain elements from the other
types of sentences in the input specications resulting in the
lowest correctness and completeness scores. The Yue et al.
approach [43] is unable to identify the domain entities (ob-
jects and attributes) that are more than one word in length,
and it uses parse tree (which represents only semantic re-
lationships between the words in the sentence) of the input
sentence to identify the domain elements therefore, it makes
errors in extracting the relevant elements from dierent po-
sitions in the sentence resulting in very low correctness and
completeness scores as compared to the proposed approach.
Following are some limitations of the proposed approach.
1. The approach requires the UCSs to be written using a
few restrictions shown in Table 3. The restrictions are re-
quired for handling the issues that are associated with the
natural language such as ambiguity, variety of sentence
types, anaphora (or pronoun) resolution problem and in-
consistency [19, 26, 11, 42]. At present the approach can
process and interpret all the simple sentences (including
sentences containing participles, innitives and gerunds)
as well as a few complex sentences viz. sentences spec-
ifying conditions, the sentences containing that clause
and conjunctive clause. In a grammatical analysis of 152
UCSs, [7] identied 27 most common grammar structures
that are used in documenting the UCSs. These grammar
structures are the subset of the sentence structures that
our approach can process and interpret.
2. The proposed approach does not uses entity disambigua-
tion techniques such as misspelling identication, abbre-
viation identication, alias identication etc. [24]
3. The current implementation of AutoAMG supports the
input of one UCS at a time, and the generation of analy-
sis class diagram for one UCS at a time. But, the inter-
actions of the given UCS with other UCSs, as specied
in the UCS using INCLUDE, EXTEND keywords and
Parent Use Case Name eld of the UCS, are shown in
the generated analysis class diagram with the help of IN-
CLUDE, EXTEND and generalization relationships be-
tween the control class representing given UCS and the
control classes representing the other UCSs respectively.
In future we will extend our tool to take a set of UCS for
a given problem as input, and to generate the analysis
class diagram for the problem.
5744. As the approach uses NL parser to process the sentences
in UCS, hence the accuracy of the approach is inherently
bounded by the accuracy of the NL parser. The Stanford
NL parser used in our approach generates POS tags with
accuracy of about 97% [22] and TDs with accuracy of
about 84.2% [6], these gures are for all the type of sen-
tences, the accuracy may be more for the simple sentences
and a few complex sentences that our approach handles at
present. However, the parser accuracy in generating TDs
can further be improved from 84.2% to 89.1% by using
Charniak-Johnson re-ranking parser [6, 23] for generating
the dependencies and converting them to Stanford TDs.
7. RELATED WORK
This section describes the existing automated approaches
in the literature.
Harmain et al. [13] proposed an automated approach
supported by a tool named CM-Builder2 for the generation
of analysis class diagrams. The approach obtains a rst
cut domain models from the NL requirements automatically.
The tool considers all the nouns as candidate classes, all the
non copular verbs as candidate relationships; it obtains at-
tributes from possessive relationships and adjectives. The
candidate classes having low frequency in the requirements
and also do not participating in any relationships are dis-
carded by the tool. The approach cannot identify class op-
erations; it cannot identify association relationships between
many classes (i.e. the class diagrams contain many uncon-
nected components), and it is unable to identify aggregation
and generalization relationships.
Liu et al. [21] proposed an automated approach named
UCDA for generating analysis class diagrams from UCS writ-
ten using restricted grammar. Their approach processes the
sentences based on the classication of sentences as tran-
sitive, intransitive, ditransitive, intensive, complex transi-
tive, prepositional and non-nite given in [31]. The ap-
proach needs a glossary dening domain specic terms to
identify domain classes. The approach is unable to identify
attributes of the classes, and aggregation and generalization
relationships between the entity classes.
Ilieva et al. [17] proposed an approach that obtains do-
main models from unrestricted NL requirements. The ap-
proach uses a POS tagger to mark the words in the NL
requirements with parts of speech. Then it identies three
roles from the sentences: subject, predicate and object. Us-
ing these roles, a semantic network of the words is created.
The semantic network is then transformed into a hybrid ac-
tivity model and a domain model. The domain model repre-
sents only the identied classes and the relationship between
them; it lacks the attributes, operations, relationship names
and relationship types. Any tool supporting their approach
was not presented by the authors.
Popescu et al. [29] proposed an approach supported
by a prototype tool named Dowser to identify inconsisten-
cies in requirement specications with the help of automat-
ically generated domain models. The tool rst parses the
requirements based on the constraining grammar. Then us-
ing a NL parser it generates a link grammar parse of the
sentences. It then uses link types to identify the classes,
methods, variables and associations, and generates a textual
object-oriented analysis model of the specied system. The
textual model is then visualized using GraphViz (this needs
human involvement). The approach can interpret the simplesentences with only ve patterns (SV, SVDO, SVPO, SV-
tobePO and SVDOPO) and ve keyword specic sentences.
The approach is unable to assign operations to most of the
classes in the model, and is unable to identify relationships
between many classes (i.e. the class diagrams contain many
unconnected components) in the class diagram.
Yue et al. [43] proposed an automated approach sup-
ported by a framework [45] to derive analysis models from
use case models. The approach can interpret a large variety
of sentences as compared to other approaches. The approach
uses parse trees (that represent only syntactic relationships
between the words) to extract the domain elements from the
sentences therefore, it makes errors in extracting the rele-
vant domain elements from dierent places in the sentences.
It can identify the domain entities (objects and attributes)
that are only one word in length. Most of the classes can
be termed as UML smells [1] as the approach assigns al-
most all the operations to a single control class, whereas the
other classes in the class diagram may not have assigned any
operations.
The proposed approach in contrary to the approaches [13,
17] but alike approaches [29, 43] identies all the major el-
ements (classes, attributes, operations and relationships) of
analysis class diagrams without the need of any glossary as
needed in [21]. In contrast to the approach of Popescu et
al. [29] it uses a very stronger language model and in con-
trast to the approach of Yue et al. [43] it uses semantic re-
lationships between the words depicted by TDs to extracts
relevant domain elements from the sentences in UCSs.
8. CONCLUSION AND FUTURE WORK
In this paper, we proposed a systematic, automated ap-
proach to identify the domain elements from textual speci-
cations. The approach uses a richer language model, which
is based on the Hornby's verb patterns, to interpret the sen-
tences, and identies the domain elements using the seman-
tic relationships between the words in the sentences obtained
from Type Dependencies (TDs). The approach is supported
by a GUI based tool named AnModeler .
We presented the control experiment conducted for the
validation of the proposed approach with the help of 40 sub-
jects and 40 UCSs. The results of the experimental study
conducted for the evaluation of the approach showed that
the analysis class diagrams generated by the proposed ap-
proach are far more correct, far more complete and less re-
dundant than those generated by Popescu et al. approach [29]
as well as Yue et al. approach [43]. The experimental study
is to the best of our knowledge the rst one to be conducted
for comparing dierent automated approaches used to gen-
erate analysis class diagrams. We made the following exper-
imental material along with other details of the experiment
available at8for other researchers of this domain to replicate
such experiments in future.
The presented work can be extended to interpret the
other complex and compound sentences and to extract the
domain elements from them. Anaphora (or pronouns) reso-
lution can be incorporated. It can be extended to generate
other artifacts such as state charts and activity diagrams.
Further traceability information can be incorporated with
the approach and studies can be done accordingly.
8https://sites.google.com/site/anmodeler (accessed July 31,
2016)
575References
[1] T. Arendt and G. Taentzer. UML model smells
and model refactorings in early software development
phases. Universitat Marburg , 2010.
[2] E. Arisholm, L. C. Briand, S. E. Hove, and Y. Labiche.
The impact of UML documentation on software main-
tenance: An experimental evaluation. Software Engi-
neering, IEEE Transactions on , 32(6):365{381, 2006.
[3] E. Arisholm and D. I. Sjoberg. Evaluating the eect of
a delegated versus centralized control style on the main-
tainability of object-oriented software. Software Engi-
neering, IEEE Transactions on , 30(8):521{534, 2004.
[4] G. Booch, R. A. Maksimchuk, M. W. Engle, B. Young,
J. Conallen, and K. Houston. Object Oriented Anal-
ysis & Design with Application 3rd Edition . Pearson
Education India, 2010.
[5] B. Bruegge and A. A. Dutoit. Object-oriented software
engineering; conquering complex and changing systems .
Prentice Hall PTR, 1999.
[6] D. M. Cer, M.-C. De Marnee, D. Jurafsky, and C. D.
Manning. Parsing to stanford dependencies: Trade-os
between speed and accuracy. In LREC , 2010.
[7] K. Cox. Heuristics for use case descriptions. PhD the-
sis, Bournemouth University, 2002.
[8] M.-C. De Marnee and C. D. Manning. The stanford
typed dependencies representation. In Coling 2008:
Proceedings of the workshop on Cross-Framework and
Cross-Domain Parser Evaluation , pages 1{8. Associa-
tion for Computational Linguistics, 2008.
[9] D. K. Deeptimahanti and R. Sanyal. Semi-automatic
generation of UML models from natural language re-
quirements. In Proceedings of the 4th India Software
Engineering Conference , pages 165{174. ACM, 2011.
[10] J. Eastwood. Oxford guide to English grammar . Oxford
University Press, 1994.
[11] F. Fabbrini, M. Fusani, S. Gnesi, and G. Lami. The lin-
guistic approach to the natural language requirements
quality: benet of the use of an automatic tool. In Soft-
ware Engineering Workshop, 2001. Proceedings. 26th
Annual NASA Goddard , pages 97{105. IEEE, 2001.
[12] H. Gomaa. Software modeling and design: UML, use
cases, patterns, and software architectures . Cambridge
University Press, 2011.
[13] H. Harmain and R. Gaizauskas. Cm-builder: A natural
language-based case tool for object-oriented analysis.
Automated Software Engineering , 10(2):157{181, 2003.
[14] R. W. Holt, D. A. Boehm-Davis, and A. C. Shultz. Men-
tal representations of programs for student and profes-
sional programmers. In Empirical studies of program-
mers: Second workshop , pages 33{46. Ablex Publishing
Corp., 1987.
[15] A. S. Hornby, A. P. Cowie, and J. W. Lewis. Oxford
advanced learner's dictionary of current English , vol-
ume 4. Oxford University Press, 1974.[16] M. Host, B. Regnell, and C. Wohlin. Using students
as subjects - a comparative study of students and pro-
fessionals in lead-time impact assessment. Empirical
Software Engineering , 5(3):201{214, 2000.
[17] M. Ilieva and O. Ormandjieva. Models derived from
automatically analyzed textual user requirements. In
Software Engineering Research, Management and Ap-
plications, 2006. Fourth International Conference on ,
pages 13{21. IEEE, 2006.
[18] I. Jacobson. Object-oriented software engineering: a use
case driven approach . Pearson Education India, 1992.
[19] E. Kamsties and B. Peach. Taming ambiguity in natural
language requirements. In Proceedings of the Thirteenth
International Conference on Software and Systems En-
gineering and Applications , 2000.
[20] H. Lee, Y. Peirsman, A. Chang, N. Chambers, M. Sur-
deanu, and D. Jurafsky. Stanford's multi-pass sieve
coreference resolution system at the conll-2011 shared
task. In Proceedings of the Fifteenth Conference on
Computational Natural Language Learning: Shared
Task, pages 28{34. Association for Computational Lin-
guistics, 2011.
[21] D. Liu, K. Subramaniam, A. Eberlein, and B. H. Far.
Natural language requirements analysis and class model
generation using ucda. In Innovations in Applied Arti-
cial Intelligence , pages 295{304. Springer, 2004.
[22] C. D. Manning. Part-of-speech tagging from 97% to
100%: is it time for some linguistics? In Computational
Linguistics and Intelligent Text Processing , pages 171{
189. Springer, 2011.
[23] D. McClosky, E. Charniak, and M. Johnson. Eective
self-training for parsing. In Proceedings of the main con-
ference on human language technology conference of the
North American Chapter of the Association of Com-
putational Linguistics , pages 152{159. Association for
Computational Linguistics, 2006.
[24] J. Misra and S. Das. Entity disambiguation in natural
language text requirements. In Software Engineering
Conference (APSEC, 2013 20th Asia-Pacic , volume 1,
pages 239{246. IEEE, 2013.
[25] P. Mohagheghi, V. Dehlen, and T. Neple. Denitions
and approaches to model quality in model-based soft-
ware development{a review of literature. Information
and Software Technology , 51(12):1646{1669, 2009.
[26] B. Nuseibeh, S. Easterbrook, and A. Russo. Making in-
consistency respectable in software development. Jour-
nal of Systems and Software , 58(2):171{180, 2001.
[27] M. O'Docherty. Object-Oriented Analysis and Design
Understanding System Development with UML 2.0 .
John Wiley & Sons Ltd, 2005.
[28] S. P. Overmyer, B. Lavoie, and O. Rambow. Concep-
tual modeling through linguistic analysis using LIDA.
InProceedings of the 23rd international conference on
Software engineering , pages 401{410. IEEE Computer
Society, 2001.
576[29] D. Popescu, S. Rugaber, N. Medvidovic, and D. M.
Berry. Reducing ambiguities in requirements speci-
cations via automatically created object-oriented mod-
els. In Innovations for Requirement Analysis. From
Stakeholders' Needs to Formal Designs , pages 103{124.
Springer, 2008.
[30] A. Porter and L. Votta. Comparing detection methods
for software requirements inspections: A replication us-
ing professional subjects. Empirical software engineer-
ing, 3(4):355{379, 1998.
[31] P. Roberts. Patterns of English . Harcourt, Brace, 1956.
[32] D. Rosenberg and M. Stephens. Use case driven object
modeling with UML. APress, Berkeley, USA , 2007.
[33] D. Rosenberg and M. Stephens. Use Case Driven Ob-
ject Modeling with UML: Theory and Practice . Springer
Science & Business, 2008.
[34] N. Samarasinghe and S. S. Som e. Generating a domain
model from a use case model. In IASSE , page 278, 2005.
[35] D. I. Sjberg, J. E. Hannay, O. Hansen, V. B. Kamp-
enes, A. Karahasanovic, N.-K. Liborg, and A. C. Rek-
dal. A survey of controlled experiments in software engi-
neering. Software Engineering, IEEE Transactions on ,
31(9):733{753, 2005.
[36] J. S. Thakur and A. Gupta. Anmodeler - a tool
for generating domain models from textual specica-
tions. In Proceedings of the 31st IEEE/ACM Interna-
tional Conference on Automated Software Engineering .
IEEE/ACM, 2016.
[37] J. S. Thakur and A. Gupta. Automatic genera-
tion of analysis class diagrams from textual specica-
tions. Technical report, Software Engineering Research
Group, IIITDM Jabalpur, India, February 2016.[38] S. Wehmeier and A. S. Hornby. Oxford advanced
learner's dictionary of current English . Cornelsen &
Oxford, 2000.
[39] K. Wiegers and J. Beatty. Software requirements . Pear-
son Education, 2013.
[40] C. Wohlin, M. H ost, and K. Henningsson. Empirical
research methods in software engineering. In Empirical
Methods and Studies in Software Engineering , pages 7{
23. Springer, 2003.
[41] C. Wohlin, P. Runeson, M. Hst, M. C. Ohlsson, B. Reg-
nell, and A. Wessln. Experimentation in software engi-
neering . Springer Publishing Company, Incorporated,
2012.
[42] H. Yang, A. De Roeck, V. Gervasi, A. Willis, and
B. Nuseibeh. Extending nocuous ambiguity analysis for
anaphora in natural language requirements. In Require-
ments Engineering Conference (RE), 2010 18th IEEE
International , pages 25{34. IEEE, 2010.
[43] T. Yue, L. Briand, and Y. Labiche. Automatically de-
riving a UML analysis model from a use case model.
Technical Report 2010-15 (Version 2), Simula Research
Laboratory, January 2013.
[44] T. Yue, L. C. Briand, and Y. Labiche. Facilitating the
transition from use case models to analysis models: Ap-
proach and experiments. ACM Transactions on Soft-
ware Engineering and Methodology (TOSEM) , 22(1):5,
2013.
[45] T. Yue, L. C. Briand, and Y. Labiche. atoucan: An
automated framework to derive UML analysis mod-
els from use case models. ACM Transactions on Soft-
ware Engineering and Methodology (TOSEM) , 24(3):13,
2015.
577