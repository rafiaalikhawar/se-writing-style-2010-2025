Automated Modularization of GUI Test Cases
Rahulkrishna Yandrapally, Giriprasad Sridhara, Saurabh Sinha
IBM Research, India
Abstract —Test cases that drive an application under test via its
graphical user interface (GUI) consist of sequences of steps that
perform actions on, or verify the state of, the application userinterface. Such tests can be hard to maintain, especially if they arenot properly modularized—that is, common steps occur in manytest cases, which can make test maintenance cumbersome andexpensive. Performing modularization manually can take up con-siderable human effort. To address this, we present an automatedapproach for modularizing GUI test cases. Our approach consistsof multiple phases. In the ﬁrst phase, it analyzes individual testcases to partition test steps into candidate subroutines, based onhow user-interface elements are accessed in the steps. This phasecan analyze the test cases only or also leverage execution traces ofthe tests, which involves a cost-accuracy tradeoff. In the secondphase, the technique compares candidate subroutines across testcases, and reﬁnes them to compute the ﬁnal set of subroutines. Inthe last phase, it creates callable subroutines, with parameterizeddata and control ﬂow, and refactors the original tests to call thesubroutines with context-speciﬁc data and control parameters.Our empirical results, collected using open-source applications,illustrate the effectiveness of the approach.
I. I NTRODUCTION
Test cases that drive an application under test via its
graphical user interface (GUI) are frequently used in functional
testing of enterprise applications. Such a test case consists of asequence of steps that perform actions on the application userinterface and check the expected behavior of the application,in response to those actions, as exhibited in the state of itsuser interface. GUI tests are created using a test-automationtool such as Selenium [1].
Like any software system, GUI test cases, after creation,
require maintenance. As the application under test evolves,the test cases need to be adapted accordingly. Some testscould become obsolete and, therefore, are removed from thetest suite, whereas other tests may need to be repaired—involving addition, deletion, or modiﬁcation of test steps—toreﬂect the updated application behavior. In particular, contentof the application screens might change, requiring the teststhat navigate to those screens to be updated.
To ease test maintenance, it is essential that the tests cases
are properly modularized. The beneﬁts of modularization arewell-known in software development, and these beneﬁts arejust as applicable to GUI test cases. To illustrate this, considerthe Registration page of a
Bookstore web application shown
in Figure 1. There are several testing scenarios for this page.After providing the required data in the ﬁelds: (1) clicking theRegister button results in successful registration, (2) clickingthe Cancel button causes the registration process to terminateand the previous page to be displayed, and (3) clicking theClear button erases all entered data and control stays on the
t1 t2
Action Target Data Action Target Data
1. click Registration 1. click Registration
2. enter Login L1 2. enter Login L2
3. enter Password P1 3. enter Password P2
4. enter Conﬁrm Password P1 4. enter First Name John
5. enter First Name John 5. enter Last Name Smith
6. enter Last Name Smith 6. click Register
7. click Register 7. exists/angbracketlefterror message/angbracketright
8. click Sign in
9. enter Login L1
10. enter Password P1
11. click Login
12. exists/angbracketleftsuccessful login/angbracketright
Fig. 1. The Registration page in the Bookstore web application (top). Two
test cases that navigate to the Registration page (bottom).
Registration page. In addition to these (valid) scenarios, thereare error scenarios to be exercised as well—attempting toregister with an already existing login ID or without providingall required information—in which case, registration fails withan appropriate error message.
Figure 1 also shows two GUI test cases that navigate to the
Registration page. Test t
1exercises the valid scenario of
successful registration, whereas t2covers the error scenario
where registration is attempted without providing data for therequired “Conﬁrm Password” ﬁeld. A test case is a sequenceof steps where each step consists of an action, a target user-interface (UI) element on which the action is performed,and an optional data value. Step 12 in t
1and step 7 in t2
contain assertions that verify successful and failed registration,respectively (the actual messages are elided for brevity).
During application evolution, different structural and logical
changes could be made that break GUI test cases [2]. Asexample of a structural change, the label “Login” could bechanged to “User ID” which would affect whether the targetUI element can be located in step 2 of t
1andt2. Partly, the
problem of maintaining GUI tests pertains to the resilienceof UI element locators to such structural page changes. AUI element can be located using different “selectors” such asXPaths, CSS paths, or HTML attributes (e.g.,
idandname ) [1]
or, alternatively, via neighboring text labels [3] or contextual
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
978-1-4799-1934-5/15 $31.00 © 2015 IEEE
DOI 10.1109/ICSE.2015.2744
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
978-1-4799-1934-5/15 $31.00 © 2015 IEEE
DOI 10.1109/ICSE.2015.2744
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
978-1-4799-1934-5/15 $31.00 © 2015 IEEE
DOI 10.1109/ICSE.2015.2744
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
978-1-4799-1934-5/15 $31.00 © 2015 IEEE
DOI 10.1109/ICSE.2015.2744
ICSE 2015, Florence, Italy
labels [4]. Depending on what locators are used and the types
of changes made on a page, test steps may break and need tobe patched. Apart from structural changes, logical changes thataffect application ﬂows can break test cases too: in such cases,steps have to be added or deleted to ﬁx the test. For instance,the Registration page could be split into two pages where onlythe login text box appears on the ﬁrst page, and control reachesto the second page (containing the remaining UI elements)only after the login value passes validation checks (e.g., that
the ID is not already existing).
The problem of test breakage caused by such structural
and logical changes can be addressed partially by robust UI-element locators (e.g., [4]) and test-repair techniques ( e.g., [5]–
[8]), but these techniques nonetheless have limitations and,often, test repair requires manual intervention. In this situation,the manual maintenance effort would be much lower if testswere modularized. For our example, if the steps in t
1andt2
that are performed on the Registration page were extractedinto a reusable module (which was invoked from those tests),changes on the Registration page would require updates inonly the common module instead of each test case (and theRegistration page, in fact, has more scenarios to be coveredthan just the two illustrated by t
1andt2).
These maintenance issues with GUI tests has led to devel-
opment of the notion of page objects for creating modular
and easy-to-maintain GUI tests [9], [10]. The idea is toencapsulate in a page object the UI elements and functionalityon an application page, and then create test cases by invokingmethods on page objects. The problem is that creating pageobjects is a manual process, requiring considerable humaneffort. Test scenarios have to be broken down into modularizedsequences of steps based on the structure of the navigatedpages, page objects with appropriate parameterized methodshave to be created, and tests have to be coded by invoking thepage-object methods. Leotta and colleagues [2] report that thedevelopment of GUI test cases via manual coding of modularpage objects can be much more expensive than—in somecases, taking more than twice as much effort as—creating thesame tests using capture-replay techniques (the latter set oftests, as expected, have a greater maintenance cost).
Therefore, an approach for automatically modularizing tests
created via recording would be very useful: it would providethe beneﬁts of low maintenance costs and avoid the upfrontcosts of crafting modular tests by hand. In this paper, wepresent such a technique. Our technique can be applied to GUItests created via manual recording or automated test generation(e.g., [11]–[15]), which also produce non-modular test cases.
Intuitively, modularization of GUI tests requires the struc-
ture of the navigated pages to be analyzed, so that elementson a “page” or, at a more granular level, in a “form” can beaggregated into a module. Thus, at a high level, our techniqueattempts to recover this structure by analyzing the UI elementsaccessed in the test steps. The notions of “page” and “form”are well-deﬁned in conventional web applications: pages areassociated with URLs and forms can be related to HTML
form
elements. But, in modern AJAX-style applications, whichcontain signiﬁcant client-side processing, there is often noclear distinction between pages (e.g., all application pages can
have the same URL). Similarly, forms need not be associatedwith HTML
form elements. To overcome these issues and
to be more generally applicable, our approach analyzes theunderlying Document Object Model (DOM) of the pagesnavigated by test cases. Our approach operates in three phases.
In the ﬁrst phase, it analyzes each test case independently to
partition the test steps into candidate subroutines. This phaseanalyzes the occurrences of the accessed UI elements in theDOM. It uses distance metrics on the DOM to detect theloading of a new page and, consequently, the beginning ofa new module. This phase can be performed dynamically,by leveraging execution traces consisting of the DOM snap-shot after each test step, or statically, by analyzing the testcases only. If DOM snapshots are available, the candidatesubroutines can be computed more accurately (at the additionalcost of trace collection). The output of this phase is a set ofcandidate subroutines, which feeds into the subsequent phases.
In the second phase, the technique compares candidate
subroutines across test cases to compute the ﬁnal set of subrou-tines. It creates initial groups of candidate subroutines and thenreﬁnes the groups based on the degree of commonality in theaccessed UI elements and whether the candidate subroutinesin a group contain conﬂicting orderings of actions.
The last phase performs test refactoring to transform the
original test suite into a suite consisting of reusable subroutinesand modular test cases. Speciﬁcally, Phase 3 creates callablesubroutines, with parameterized data and control ﬂow, andreplaces steps in the original test cases with calls to the subrou-tines along with context-speciﬁc data and control parameters.
Although many approaches have been developed for gener-
ating GUI tests (e.g., [12]–[21]), there is not much existing
research on modularizing GUI tests to make them easierto maintain, which is a problem with signiﬁcant practicalimplications. Mahmud and Lau [22] present a supervisedmachine-learning technique for identifying subroutines in GUItest cases written in the ClearScript language [23]. Unlike thattechnique, our approach analyzes the structure of the pagesnavigated by a test case and does not require a training sampleof subroutines. Section IV further discusses related work.
We implemented the technique and conducted empirical
studies using four open-source web applications, with 179 GUItests in total, to evaluate various aspects of the technique. Theresults show that our technique can extract highly reusablesubroutines and reduce the sizes of test cases: for our subjects,it extracted 50 subroutines with an average of about sixsteps and eight call sites per subroutine; the total test stepsdecreased from over 1869 in the original test suites to 721in the refactored suites (a reduction of 61%). Moreover, theextracted subroutines correlate closely to application pages,which typically is the goal when modular test cases are de-signed by hand, for example, using the page-object abstraction.This indicates that our approach could be effectively used toautomate the task of ﬁnding plausible subroutines in recordedor automatically generated GUI tests.
45
45
45
45
ICSE 2015, Florence, ItalyThe contributions of this work are
•The description of an automated technique for extracting
subroutines from, and refactoring, GUI tests that is basedon the structure of the pages navigated by the tests andis generally applicable.
•The implementation of the technique for modularizingGUI tests for web applications.
•The presentation of empirical results demonstrating theeffectiveness and efﬁciency of the technique.
The rest of the paper is organized as follows. The next
section describes our technique for modularizing GUI testcases. Section III presents the results of the empirical studies.Section IV discusses related work. Finally, Section V summa-rizes the paper and lists directions for future research.
II. O
URTECHNIQUE
Given a suite of GUI test cases, our technique refactors the
tests by extracting reusable subroutines and replacing steps inthe original tests with calls to the extracted subroutines.
Figure 2 shows the refactored
Bookstore test cases and
the extracted subroutine for entering registration information.To simplify the presentation, we use pseudo-code notation inFigure 2 to illustrate the subroutine and the tests; the imple-mented tool creates the refactored test suite as Selenium [1]test cases, written in Java, that can be executed using JUnit (seeSection III). The extracted subroutine,
EnterRegDetails ,
contains test steps that are executed on the Registration pageand takes two input parameters: an array
data of context-
speciﬁc data values for test steps, and an array control
of boolean ﬂags controlling the execution of test steps.1
The values of data are used in those test steps that re-
quired data: steps 1, 2, 4, 5, and 6. The boolean ﬂags in
control determine whether steps 4 and 9 are executed; the
remaining steps execute unconditionally. Tests t1andt2are
refactored to call EnterRegDetails (line 4 in t1andt2),
and set up the context-speciﬁc data and control parameters(lines 1 and 2 in t
1andt2). The data setup in t1creates
ﬁve data values, whereas the data setup in t2creates four
values—the third parameter, which is used for the “ConﬁrmPassword” text box, is empty because the correspondingtest step (step 4 in
EnterRegDetails ) does not execute
when EnterRegDetails is called from t2. The control setup
(line 2) ensures that when EnterRegDetails is invoked from
t1, step 4 executes and step 9 does not execute; the converse
is true when EnterRegDetails is called from t2.
Our technique performs this transformation in a mechanized
way, and ensures that the behavior of the original tests ispreserved in the transformed tests. Before presenting thealgorithm, we provide some preliminary deﬁnitions.
1Although Figure 2 illustrates the extracted subroutine’s data and control
parameters as arrays, alternatively, a separate formal parameter with a mean-
ingful name (e.g., derived from attributes of the target UI element to which the
parameter applies) could be generated for each data and/or control parameter.Sub EnterRegDetails(Str[] data, Bool[] control)
1. enter, Login, data[1]
2. enter, Password, data[2]3. if (control[1])4. enter, Confirm Password, data[3]5. enter, First Name, data[4]6. enter, Last Name, data[5]7. click, Register8. if (control[2])9. exists, error message
End subTest t1
1. Str[] data = {"L1", "P1", "P1", "John", "Smith"}2. Bool[] control = {true, false}3. click, Registration4. call EnterRegDetails(data, control)5. click, Sign in6. enter, Login, L17. enter, Password, P18. click, Login9. exists, successful login
End testTest t2
1. Str[] data = {"L2", "P2", "", "John", "Smith"}2. Bool[] control = {false, true}3. click, Registration4. call EnterRegDetails(data, control)
End test
Fig. 2. The transformed Bookstore test suite with the extracted subroutine
for entering registration information, and the refactored test cases that call the
extracted subroutine.
A. Deﬁnitions
We consider GUI test cases of the form shown in Fig-
ure 1(b). A test case is a sequence of test steps /angbracketlefts1,...,s n/angbracketright,
where each siis an action performed on the application
user interface (an action step) or a veriﬁcation of the user-
interface state (a veriﬁcation step). A test step is a triple
(a,e,d) consisting of a command a, a target UI element e,
and an optional data value d. The command acan be a
predeﬁned veriﬁcation command (e.g., exists ,selected ,
enabled ) that is a predicate on the state of a UI element, a
predeﬁned action command (e.g., right-click ), or a generic
action command (e.g., enter ,select ). The set of predeﬁned
commands depends on the test-automation tool being used.
The target UI element for a test step is identiﬁed using
element locators. Web testing tools, such as Selenium [1],
provide different types of locators that are based on XPaths,CSS paths, or HTML attributes. Our approach requires, inparticular, the XPaths of UI elements. For example, forstep 1 of tests t
1andt2, where the target is the “Regis-
tration” image on the Bookstore home page, the XPath
/html/body/table/tbody/tr/td/table/tbody/tr/td[3]/a/image is
recorded in those tests. An XPath encodes both the hierarchyof elements and the relative position of an element within itsparent; for example,
td[3] refers to the third tdelement within
the parent trelement. Figure 3 shows a partial DOM for the
Registration page in Bookstore . The UI elements, labels, and
other HTML elements are nodes in the DOM.
To match test steps effectively across test cases, we use
a notion of equivalence, rather than strict equality, which
abstracts out unnecessary details. We deﬁne a canonical rep-
resentation of a test step sasσ(s)= /llbracketσ(a),σ(e),σ(d)/rrbracket.σ(a)
maps command ato the constant Aifais a generic action
command; otherwise, σ(a)=a .σ(e)maps element eto its
XPath.σ(d)mapsdto the boolean value true ifdis non-
46
46
46
46
ICSE 2015, Florence, Italy/g1005
/g1013
/g1005/g1008/g1005/g1008/g1006
/g1012
/g1005/g1007/g1005/g1007/g171
/g1005/g1005/g1005/g1005 /g1005/g1006/g1005/g1006 /g1005/g1004/g1005/g1004
/g1005/g1010/g1005/g1010/g1005/g1009/g1005/g1009 /g1006/g1004/g1006/g1004 /g1005/g1013/g1005/g1013 /g1006/g1005/g1006/g1005/g1007/g1007
/g1011/g1011/g1008/g1008/g1009/g1009/g1010/g1010
/g171
/g171
/g1005/g1012/g1005/g1012 /g1005/g1011/g1005/g1011
Fig. 3. Partial DOM for the Bookstore Registration page.
empty, and to false otherwise. The canonical representation
abstracts out generic action commands and data values so
that test steps in different contexts can be matched. Thus, thecanonical representations of step 2 of t
1andt2are the same:
/llbracketA,/html/body/table/.../tr/td/input ,true /rrbracket.
Next, we deﬁne path equivalence on the DOM. A DOM
node has a type (e.g., div ,table ,button ,image ) and a set
of HTML attributes (e.g., id,name ,class ,src ) associated
with it. Node nisequivalent to nodemif and only if the
nodes have the same type, the same set of attributes, and thesame value for each attribute. Let idx(n)denote the index of
n. Letp=(n
1,...,n k)andq=(m1,...,m k)be two paths
in the DOM. Then, p≡qif and only if, for all 1≤i≤k,niis
equivalent to miand for all 1≤i≤k−1,idx(ni)=idx(mi).
B. The Test-Modularization Algorithm
The algorithm has three phases, which we explain next.
1) Computation of Candidate Subroutines: Algorithm 1
presents the Phase 1 analysis; it takes as input a test suite
Tand produces as output the set of candidate subroutines
for the tests in T. The algorithm partitions the steps of each
test case based on the DOM locations of the referenced UIelements. The intuition behind this is that the proximity of thereferenced UI elements indicates logical groupings of steps:test steps that access closely located elements in the DOMpertain to actions performed on a “page” or “form,” whereasa step that accesses a signiﬁcantly different part of the DOMthan its preceding step potentially begins a new subroutine.More concretely, the analysis determines this by computing theleast common ancestor (LCA) of a set Eof UI element nodes.
The farther away that the LCA of Eis from the DOM root
node, the more likely that the UI elements in Eare related.
Conversely, if the LCA is close to the root, Econtains widely
dispersed, and potentially unrelated, UI elements.
Acandidate subroutine S
c=(D,ψ,E,p ), whereDis a
DOM,ψis a sequence of test steps, Eis the set of UI elements
referenced in the steps in ψ, andp, referred to as path preﬁx,
is the path in Dfrom the root node to the LCA of all nodes
inE. Consider Figure 3. Suppose that Econsists of nodes 14
(the Login text box), 16 (the Password text box), and 19 (theRegister button); these nodes are highlighted in Figure 3. Then,the LCA of these nodes is node 8 and pis the path (1,3,8),
shown as the dashed path in Figure 3.
As mentioned earlier, Phase 1 can be performed statically
or dynamically; Algorithm 1 presents the dynamic variant ofAlgorithm 1: Identiﬁcation of candidate subroutines.
Input: Test suite T
Output: Set Scof candidate subroutines
1foreacht∈Tdo
2 initialize Sc=(D,ψ,E,p ),lprev
3 foreachs=(a,e,d)∈tdo
4 letDcurr be the DOM snapshot before step s
5 letlcurr be the LCA of eandlprev inDcurr
6 ifstartNewSubroutine(l prev,lcurr,D curr,E)then
7 letlbe the LCA in Dcurr of the elements in E
8 setDto the subtree in Dcurr rooted at l
9 setpto the path from the root node to linDcurr
10 addSctoSc
11 reinitialize Sc,lprev
12 else
13 setlprev tolcurr
14 addetoE, addstoψ
15 returnSc
Phase 1, which assumes that an execution trace containingthe DOM snapshot before each test step is available. Afterexplaining the dynamic variant of the algorithm, we discussthe modiﬁcations required for the static variant.
The algorithm iterates over each test case t(line 1) and each
step int(line 3). Line 4 reads the DOM snapshot before step s
from the execution trace, and line 5 computes the LCA of e
(the UI element referenced at s) andl
prev (the LCA from the
previous iteration, which is undeﬁned for the ﬁrst test step).The LCA computation locates the nodes for eandl
prev in
Dcurr . Botheandlprev have XPaths associated with them, so
the algorithm uses path equivalence to locate them in Dcurr .
If stepsresults in the loading of a new page, lprev would not
appear in Dcurr ; in this case, eis used as the LCA in line 5.
Next, the algorithm calls the startNewSubroutine()
function (line 6), which uses two metrics to determine whethera new subroutine should be started at s.
First,
startNewSubroutine() checks whether all ele-
ments in E(i.e., the elements referenced by steps in the
candidate subroutine under consideration) appear in Dcurr .I f
this is the case, the function proceeds with the second check;otherwise, it returns true, indicating that a new subroutineshould be started. Typically, the latter condition would indicatea server-side communication that re-renders the DOM, causingpreviously referenced elements to no longer be available. (Thischeck is performed only in the dynamic variant of Phase 1.)
Second,
startNewSubroutine() checks whether the nor-
malized difference between the distances of lprev andlcurr
from the root node of Dcurr exceeds a threshold value, and
returns true if it does. The check is performed as
δ(lprev,Dcurr)−δ(lcurr,Dcurr)
δ(lprev,Dcurr)>λ d
whereδ(n,D)returns the distance of nfrom the root node
ofD. Intuitively, this formula captures whether saccesses a UI
element in a substantially different part of the DOM than thepart accessed in the preceding steps, thus causing a big shift inthe LCA node toward the DOM root. To accommodate caseswherel
prev is close to the root, before applying the distance
metric, startNewSubroutine() checks whether lcurr is the
HTML body element and, if it is, returns true.
47
47
47
47
ICSE 2015, Florence, ItalyTABLE I
CANDIDA TE SUBROUTINES COMPUTED FOR t1ANDt2INPHASE 1.
Sub-
routine D ψ E p
S1
c1DOM rooted at node 5 /angbracketleft1/angbracketright {5} (1,2,5)
S1
c2DOM rooted at node 8 /angbracketleft2,3,4,5,6,7/angbracketright{14,16,18,19,...}(1,3,8)
S1
c3DOM rooted at node 6 /angbracketleft8/angbracketright {6} (1,2,6)
S1
c4... /angbracketleft9,10,11,12/angbracketright{...} (...)
S2
c1DOM rooted at node 5 /angbracketleft1/angbracketright {5} (1,2,5)
S2
c2DOM rooted at node 8 /angbracketleft2,3,4,5,6,7/angbracketright{14,16,19,...}(1,3,8)
If a new subroutine is to be started, the algorithm updates the
information for the current subroutine Sc(lines 7–9), adds Sc
to the set of candidate subroutines (line 10), and re-initializes
Scandlprev (line 11). However, if startNewSubroutine()
returns false, the algorithm simply updates lprev for the next
iteration. In either case, it also adds eandstoSc.
Consider the partitioning of the steps of t1(Figure 1) using
the DOM shown in Figure 3. The ﬁrst test step accessesthe Registration image, which corresponds to node 5 in theDOM. Line 5 of Algorithm 1 sets l
curr to node 5 (l prev is
undeﬁned). In the next iteration, test step 2, which accessesthe Login text box (node 14), is processed. The LCA ofnodes 5 and 14 is the root node. The distance metric on theold LCA (node 5) and the new LCA (node 1) evaluates to 1(
δ(lcurr,Dcurr)=0 ), which exceeds the threshold λd(setting
λd=0.75, the value we used in our experiments). Thus, the
processing of the current candidate subroutine is complete—itcontains step 1 only—and step 2 and node 14 are added to anew candidate subroutine. Next, test step 3 is processed forwhich the accessed UI element is node 16. The new LCA isnode 8, and the distance metric is calculated as 0.5, whichis less than λ
d. Therefore, step 3 and node 16 are added to
the current subroutine. Continuing in this way, the algorithmadds steps 4–7, for which the referenced UI elements occur inthe subtree rooted at node 8, to the current subroutine. Afterstep 7, a new page is loaded on which step 8 is performed;thus, a new subroutine is initialized.
In the end, the algorithm computes four candidate sub-
routines for t
1, shown in rows 1–4 of Table I. Two of
the subroutines, S1
c1andS1
c3, have only one test step each,
whereas the remaining two, S1
c2andS1
c4, have six and four
steps, respectively. Similarly, two candidate subroutines arecomputed for t
2, shown in the last two rows of Table I.
The static variant of Phase 1 differs from the dynamic
variant in two ways. First, at line 4, Dcurr is not set to
the DOM snapshot; instead, Dcurr is built incrementally by
adding XPaths to it for the referenced UI elements, one at atime. Thus, D
curr corresponds to the DOM induced by the
test steps in the current candidate subroutine. Second, in thestatic variant,
startNewSubroutine() does not perform the
ﬁrst check—that check is redundant because all elements in E
always occur in the induced DOM. The dynamic variant can bemore accurate in computing subroutines than the static variant,but it comes with the cost of collecting execution traces. Inone of the empirical studies, we compared the effectivenessof the dynamic and static variants of Phase 1 (Section III-C).2) Computation of Final Subroutines: Phase 2 starts by
grouping the candidate subroutines into subroutine groups. Itthen reﬁnes each subgroup, possibly splitting a subgroup intosmaller subgroups, to compute the ﬁnal set of subroutines.
Asubroutine group S
G={Sc1,...,S ck},k≥1,is a set
of candidate subroutines such that for each pair Sci,Scj∈
SG,i/negationslash=j,pi≡pj. In other words, SGis the set of
candidate subroutines that have equivalent path preﬁxes. Forthe candidate subgroups for our running example (Table I), thetechnique computes four subroutine groups: S
G1={S1
c1,S2
c1}
groups two of the subroutines based on the equivalent pathpreﬁx(1,2,5);S
G2={S1
c2,S2
c2}groups two subroutines
based on the equivalent path preﬁx (1,3,8); the remaining
two groups contain one candidate subroutine each.
These subroutine groups are a conservative initial grouping;
the rest of Phase 2 reﬁnes the initial groups. First, it analyzesthe overlap of UI elements among the subroutines in a groupto reﬁne the group into smaller groups. Second, it computes atotal order of the test steps for a group, splitting a group whennecessary to resolve sequencing conﬂicts.
Algorithm 2 presents this analysis; it takes as input a set
S
Gof subroutine groups and returns the set Sfof ﬁnal
subroutines. Line 1–11 reﬁne the groups based on elementoverlap. Intuitively, the analysis determines the “cohesion” ofcandidate subroutines in a group based on commonality of thereferenced UI elements. In doing this, it serves to separate outpotentially incorrect groupings: for example, two subroutinesthat perform actions on different pages, but coincidentally havethe same path preﬁx (because of which they were groupedtogether) can be separated based on low element overlap. Theanalysis thus attempts to follow the reasoning that would bedone in identifying page-object abstractions manually.
The algorithm iterates over each S
Giin the input set of
subroutine groups (line 2); it removes subroutines from SGi
untilSGibecomes empty (lines 3–10), creating one or more
reﬁned groups SGoin the process. It ﬁrst sorts the subroutines
inSGibased on the sizes of their element sets and initializes
the output group SGowith the ﬁrst subroutine (lines 4–5).
Then, it incrementally builds SGoby moving those elements
that have a high overlap with the set of DOMs in SGo. The
parameter λedetermines whether a candidate subroutine is
moved to SGo. When the loop in line 6 terminates, some or
all of the subroutines in SGihave been moved to SGo. If there
are remaining subroutines in SGi, this process is repeated.
Note that the element intersection in line 8 is computed
with respect to the DOM objects in the candidate subroutines.In the dynamic variant of Phase 1, even elements that arenot referenced in a test case, but that occur in the DOMrooted at the LCA, appear in the subroutine DOM object. Thishas the nice property of accommodating the scenario wheretwo tests navigate to the same form but access different setsof UI elements in the form, with little or no overlap. Theelement intersection of these tests, as computed in line 8,would still be high because the respective DOMs contain thenon-referenced elements also. Thus, the initial grouping ofthese tests is maintained. However, for the static variant of
48
48
48
48
ICSE 2015, Florence, ItalyAlgorithm 2: Computation of ﬁnal subroutines.
Input: Set SG={SG1,...,S Gm}of subroutine groups
Output: Set Sf={Sf1,...,S fn}of ﬁnal subroutines
// Reﬁne SGbased on element overlap to create SG1
1initialize SG1to the empty set
2foreachSGi∈SGdo
3 whileSGi/negationslash=∅do
4 sortSGiin decreasing size of element set E, initialize SGo
5 remove the ﬁrst subroutine from SGiand add it to SGo
6 foreachSc∈SGido
7 letEbe the element set of Sc
8 letEsbe elements of Ethat occur in a DOM in SGo
9 if(|E s|/|E|)>λ ethen
10 removeScfromSGiand add it to SGo
11 addSGotoSG1
// Compute ﬁnal subroutines from SG1, further reﬁning the subroutine groups in
case of conﬂicts in step sequences
12initialize Sfto the empty set
13 foreachSGi∈SG1do
14 sortSGiin decreasing length of step sequence
15 whileSGi/negationslash=∅do
16 remove the ﬁrst sub. Sc1fromSGiand initialize Sfwith it
17 letψ1be the step sequence of Sc1
18 initialize graph Gwith canonical representation of steps in ψ1
19 foreachSc∈SGido
20 letψcbe the sequence of canonical rep. of the steps in Sc
21 ifaddingψctoGdoes not create a cycle in Gthen
22 removeScfromSGi
23 update test mapping information γforG
24 letΨbe the step sequence in topological sort order of G
25 add(Ψ,γ)toSf
26 returnSf
Phase 1—in which case the subroutine DOM is the induced
DOM—Algorithm 2 would separate these tests into different
subroutine groups because of low element overlap.
After reﬁning SGintoSG1, Algorithm 2 computes the ﬁnal
subroutines (lines 12–25), which involves creating, for eachsubroutine group S
GinSG1, a total order of step sequences.
LetSG={Sc1,...,S ck}be a subroutine group. Let
ψ1,...,ψ kbe the step sequences in the subroutines in SG,
with each test step in its canonical representation. A ﬁnal
subroutine Sf=( Ψ,γ)is constructed from SG, whereΨis
an interleaving of ψ1,...,ψ ksuch that, for any ψi, the order
of steps in ψiis preserved in Ψ;γis a function that maps a
step to the test cases in which the step occurs.
The total order in a ﬁnal subroutine must ensure that the or-
der of execution of steps in the original tests is preserved whenthe subroutine is called from those tests. This is necessary toguarantee behavior-preserving subroutine extraction. Considerthe subroutine group S
G2for the Bookstore example, which
consists of two candidate subroutines (rows 2 and 6 of Table I)with six steps each, ﬁve of which are common: steps 2, 3,5–7 ofS
1
c2which come from test t1and steps 2–6 of S2
c2
which come from t2. The total ordering shown in the extracted
subroutine in Figure 2 preserves the execution order of thesteps from t
1andt2.
In general, some “precedes” relations between test steps
must be preserved (e.g., steps 2–5 must execute before step 6
int2), whereas other relations can be relaxed (e.g., the order
of execution of steps 4 and 5 does not matter). But, withoutknowing the semantics of each step, it is not possible todetermine which precedes relations can be relaxed. Thus,although steps could be reordered in a semantics-preservingmanner, we take the conservative approach of ensuring thatall precedes relations are preserved in the ﬁnal subroutine; ifconﬂicts occur, the subroutine group is reﬁned via splitting.
The problem of computing a precedence-preserving total
order, and detecting conﬂicts, can be solved by constructinga directed graph in which a cycle indicates conﬂicts, and atopological sort on the ﬁnal (acyclic) graph gives a precedence-preserving total order. At a high level, this is what Algorithm 2does in lines 13–25. It iterates over each subroutine group(line 13), and processes each candidate subroutine, startingwith the one with the longest step sequence (lines 14–15).Line 18 initializes a graph Gin which nodes represent
canonical representations of test steps and edges represent theorder of execution among steps. (The graph can have multipleinitial and ﬁnal nodes.) Then, for each subroutine S
cin the
group, the algorithm attempts to add the steps of Sc, in order,
toG. If the addition of a step and its outedge results in a
cycle inG, the algorithm has detected a conﬂicting sequence.
In that case, it omits Scand proceeds to the next subroutine.
If the steps in Sccause no cycles, it is removed from SGi
and the step-to-tests map γis updated (lines 22–23) with the
newly added steps from Sc.
After each subroutine has been processed, a topological sort
ofGgives the total step sequence Ψ, which along with the
mapγ, is added to the set of ﬁnal subroutines (lines 24–25).
3) Generation of Refactored Test Suite: Phase 3 uses the
set of ﬁnal subroutine to create the transformed test suite. Thisphase creates a callable subroutine for each ﬁnal subroutineS
f=( Ψ,γ)inSf. To do this, it ﬁrst determines which
statements in Ψexecute conditionally and which ones execute
unconditionally (in other words, in all calling contexts). Themapping information γmaps each test step in Ψto the set
of tests containing that step. Thus, the union of the tests inthe range of γgives the set of tests T
cthat forms all calling
contexts for Sf. Then, for any step s∈Ψ,i fγ(s)=Tc,s
executes unconditionally; otherwise, it executes conditionally.The algorithm creates a formal control parameter and enclosesthe conditionally-executing test steps of the subroutine in
if
statements, as illustrated for EnterRegDetails in Figure 2.
Also, for each test step that takes a data value, the datareference is parameterized.
After creating the subroutines, the algorithm refactors each
test case by replacing tests steps with calls to the subroutines,and adding statements that set up the actual parameters for thecalls with context-speciﬁc data and control values.
III. E
MPIRICAL EV ALUA TION
We implemented our technique and conducted three em-
pirical studies using open-source web applications to evaluatevarious aspects of the technique. After describing the experi-mental setup, we present the results of the studies.
A. Experimental Setup
We implemented the static and dynamic variants of our
technique as an extension to
ATA [24], which is a tool for
49
49
49
49
ICSE 2015, Florence, ItalyTABLE II
SUBJECTS USED IN THE EMPIRICAL STUDIES .
Test Steps
Subject Description Tests Total Avg Min Max
Bookstore Shopping portal for books 57 643 11 7 28
Classiﬁeds Portal for posting and checking 30 332 11 6 21
advertisements
jBilling Enterprise billing application 73 748 10 5 18
Tudulist Portal for managing personal TODOs 19 146 7 6 10
Total 179 1869
creating GUI tests for web applications. The implementation
analyzes the tests created in ATA : given a suite of test cases,
it creates a modularized test suite, consisting of Selenium [1]test cases coded in Java, that can be executed using JUnit. Themodularized test suite consists of parameterized subroutines,each of which is a static Java method, deﬁned in a
Modules
class, that takes an array of strings (data values) and an array ofbooleans (control predicates) as formal parameters. A methodis parameterized only if it contains at least one test step thatrequires a data value or that needs to execute conditionally.Each transformed
ATA test case is emitted as a JUnit test case.
We also implemented a trace-collection feature, which
records the complete browser DOM before the execution ofeach test step; this is done using the Selenium API.
We used four open-source web applications as experimen-
tal subjects; Table II lists the applications. Among these,
jBilling andTudulist are AJAX applications, whereas the
others are JSP applications.
Table II also presents information about the test cases
for the subjects. We created test suites consisting of func-tional test cases. We designed the test cases to cover vari-ous application features. For example, the 57 test cases for
Bookstore exercise application functionality such as adding,
modifying, or deleting members, books, orders, etc. The testscover normal scenarios and error scenarios (e.g., attempting
to register with an existing user ID). Over all subjects, wecreated 179 functional tests cases, consisting of 1869 teststeps. The average number of test steps ranges from 7 to11; the largest test, consisting of 28 steps, occurs in the testsuite for
Bookstore . (The evaluation dataset is available at:
sites.google.com/site/irlexternal/test-modularization.)
We automated the test cases using ATA [24], and then
executed the dynamic and static variants of the implementationto generate modularized Selenium test cases. As a sanity checkfor correctness, we executed each refactored test to ensure thatit passed. At the minimum, this guarantees that subroutineextraction does not alter the (passing) outcome of test cases.
In the ﬁrst study, we used the dynamic variant of the
technique (which, in general, is more effective than the staticvariant), and the parameter values λ
d=0.75andλe=0.8.I n
the second study, we compared the static and dynamic variants(using the default parameter values above). In the ﬁnal study,we evaluated the effects of varying λ
dandλe.
B. Study 1: Effectiveness
Goals and Method: In the ﬁrst study, we evaluated the
effectiveness of our technique. We collected data about theTABLE III
SIZES OF THE REFACTORED TESTS AND EXTRACTED SUBROUTINES .
Refactored Test Steps Sub- Subroutine Steps Call
Subject Tests Tot Avg Min Max routines Tot Avg Min Max Sites
Bookstore 57 223 3 1 13 19 104 5 2 13 130
Classiﬁeds 30 103 3 0 9 12 67 5 2 21 69
jBilling 73 107 1 0 3 16 79 4 2 11 158
Tudulist 19 5 0 0 1 3 33 11 3 22 33
Total 179 438 50 283 390
reduced lengths of the refactored tests and data about the ex-tracted subroutines, such as their lengths and number of callingcontexts. Moreover, to quantify the simpliﬁcation attained inthe refactored test suite (T
r) over the original test suite (T ),
we used three metrics.
The ﬁrst metric, reduction index (RI ), quantiﬁes the de-
crease in the number of test steps in Tr, whereas the second
metric, call index (CI), quantiﬁes the reuse via calls to
subroutines, for a refactored test suite:
RI=1−/summationtext
t∈Trst(t)+/summationtext
S∈Sfst(S)
/summationtext
t∈Tst(t)CI=1−|Sf|/summationtext
S∈Sfcs(S)
where st(t)is the number of steps, excluding calls, in test
t,st(S)is the number of steps in subroutine S, and cs(S)is
the number of calls to S. Both RI andCI range from zero
(inclusive) to one (exclusive), with higher values indicatinggreater reduction in test steps and greater reuse, respectively.
Although high reduction via extraction of highly reusable
subroutines is desirable, the complexity of the extractedsubroutines—in terms of the distinct ﬂows that occur throughthem—can make the refactored tests harder to understand,thereby negatively impacting the simpliﬁcation achieved. Toinvestigate this aspect of modularization, we deﬁne the numberofdistinct behaviors of a subroutine as the distinct sequences
of steps, abstracting out the data values, that execute overall calling contexts of the subroutine.
2Our third metric,
subroutine behavior index (BI), quantiﬁes this for a refactored
test suite:
BI=|Sf|/summationtext
Sf∈Sfdb(Sf)
where db(Sf)is the number of distinct behaviors of sub-
routineSf.BIvaries from zero (exclusive) to one (inclusive),
with higher values indicating fewer distinct behaviors, which isdesirable from the perspective of comprehension complexity.
Results and Analysis: Table III presents the basic data about
the sizes of the refactored tests and the extracted subroutines.Columns 3–6 show data about the refactored test cases. Arefactored test case can contain UI actions and calls to the ex-tracted subroutines: in measuring the size of a refactored test,we count only the UI actions. As the data illustrate, the testsbecome smaller after subroutine extraction: the total numberof test steps decreases from 1869 to 438 (77% reduction),whereas the average test size over all subjects, decreases from
2Note that, by our deﬁnition, distinct behaviors are a subset of the linearly
independent paths in the control-ﬂow graph of a subroutine [25]: they
represent only those paths that execute in at least one calling context of thesubroutine.
50
50
50
50
ICSE 2015, Florence, Italy/g17/g381/g381/g364/g400/g410/g381/g396/g286 /g18/g367/g258/g400/g400/g349/g296/g349/g286/g282/g400 /g361/g17/g349/g367/g367/g349/g374/g336 /g100/g437/g282/g437/g367/g349/g400/g410/g18/g94 /g24/g17 /g18/g94 /g24/g17 /g18/g94 /g24/g17 /g18/g94 /g24/g17/g951/g3/g18/g258/g367/g367/g400/g3/g400/g349/g410/g286/g400/g3/g876/g3/g282/g349/g400/g410/g349/g374/g272/g410/g3/g271/g286/g346/g258/g448/g349/g381/g396/g400
Fig. 4. Distribution of call sites and distinct behaviors for extracted subrou-
tines; “CS” represents call sites, whereas “DB” represents distinct behaviors.
9.75 to 1.75. In three of the subjects, the smallest test case
ends up containing only subroutine calls (and no UI actions),indicated by the “Min” values of zero.
Columns 7–11 show data about the extracted subroutines:
50 subroutines are extracted over the four subjects, containinga total of 283 steps (or UI actions). On average, the subroutinescontain about six steps; the largest subroutine, which occursin
Tudulist , has 22 steps. Thus, the technique is able to
extract reasonably sized subroutines, which are, on average,more than half as long as the original tests.
The next question about the extracted subroutines is about
their reuse: how many times are the subroutines called (andthus reused) by test cases? The last column of Table III liststhe total number of call sites created in the refactored tests.For example, for the 19 subroutines extracted for
Bookstore ,
130 call sites are created—on average, over six call sitesper subroutine. For
jBilling , the average is over nine call
sites per subroutine. To provide further details on call-sitedistribution, Figure 4 presents the data for all subroutines usinga boxplot (plots labeled “CS”), which shows median value, therange of the quartiles, and the outliers.
3
Each subroutine has at least two call sites, which is ensured
by our algorithm: it does not extract subroutines that have onlyone caller. For
Bookstore ,jBilling, andTudulist 50%
or more of the subroutines have four or more calls to them.Excluding the data points not plotted,
2the maximum numbers
of call sites for any subroutine in Bookstore ,Classifieds ,
jBilling andTudulist are 10, 21, 16 and 22 respectively.
Listing 1 shows an example extracted subroutine for
jBilling .F o r jBilling , we have test cases that exercise the
functionality of adding a new partner. This requires the tests tonavigate to a registration page, in which various details, suchas login name, password, email ID, etc., have to be entered.The test cases either save the changes on the page or discardthe changes by clicking Cancel. These steps are extractedinto a subroutine, as shown in the top part of Listing 1; thebottom part shows call sites from two test cases that invokethe subroutine with appropriate data and control parameters.The illustration in Listing 1 is a simpliﬁed version of this
3The data is plotted after eliminating three data points: one subroutine each
inBookstore, Classifieds, and jBilling, with 54, 21, 73 call sites,
respectively. In each case, these are “login” subroutines that are called from
many test cases—in some cases at the beginning of test cases and, in othercases, in the middle of test ﬂows.public static void enter login name password verify password
( String [] data , boolean [] control ) {
execute (”enter” ,”Login N a m e” , data [0] , lo cator , false );
execute (”enter” ,”Password” , data [1] , loc ator , false );
execute (”enter” ,”Verify Password” , data [2] , lo cator , false );
execute (”enter” ,”Next payout date” , data [3] , lo cator , false );
execute (” select” ,”Rel ated clerk” , data [4] , lo cator , false );
execute (”enter” ,”email” , data [5] , loc ator , false );
if ( control [0])
execute (” click” , ”save changes” , ”” , lo cator , false );
if ( control [1])
execute (” click” , ”cancel” , ”” , loc ator , false );
}
− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − −
/ / Test Case 1: Save changes after entering details
enter login name password verify password ( new String []
{”partner1” , ”Pass123$” , ”Pass123$” , ” 06/06/2014” , ”clerk1” ,
”partner1@xyz .com” },new boolean []{true ,false});
/ / Test Case 2: Cancel after entering details
enter login name password verify password ( new String []
{”partner1” , ”Pass123$” , ”Pass123$” , ” 06/06/2014” , ”clerk1” ,
”partner1@xyz .com” },new boolean []{false ,true});
Listing 1. An extracted subroutine and two related call sites for jBilling.
TABLE IV
SIMPLIFICA TION ACHIEVED IN THE REFACTORED TEST SUITES .
Subject Reduction Index (RI )Call Index (CI )Behavior Index (BI )
Bookstore 0.49 0.85 0.42
Classiﬁeds 0.49 0.83 0.36
jBilling 0.75 0.90 0.46
Tudulist 0.74 0.91 0.27
subroutine; the actual subroutine incorporates more steps and
is invoked from eight call sites.
We manually examined the subroutines for Bookstore ,
Classifieds , and Tudulist to see whether they correspond
to pages or forms in those applications. For Classifieds ,
11 of the 12 subroutines roughly correspond to an applicationpage, whereas one subroutine combines elements from twopages—Member Record and Registration—that have similarforms pertaining to member information. We found threesimilar instances in
Bookstore ; the remaining 16 subroutines
correspond to a form. For Tudulist , two of the subroutines
correspond to Login and Registration features, each of whichis implemented on one page; the third subroutine correspondsto the main page, which supports different features, such asadding, deleting, or editing lists, all of which cause the samepage (with the list of available items) to be displayed.
Table IV shows the data about the reduction, call, and
behavior indexes. As the data show, a signiﬁcant amount ofreduction, or removal of duplication, occurs for
jBilling and
Tudulist , where the total number of UI action steps in the
refactored suites is less than 30% of what they were in theoriginal test suites. For
Bookstore andClassifieds , the
reduction is less, but still close to 50%. The CI values are
consistently high for all subjects, indicating that the extractedsubroutines have a very high degree of reuse via calls frommultiple call sites. The behavior index ranges from 0.27 to0.46. On average, the subroutines for
Tudulist exhibit about
four distinct behaviors, whereas the subroutines for jBilling
exhibit only two distinct behaviors.
Figure 4 also shows the distribution of distinct behaviors
of subroutines (plots labeled “DB”). In all cases, the numberof distinct behaviors is signiﬁcantly lower than the number ofcallers, indicating that multiple call sites invoke a subroutine
51
51
51
51
ICSE 2015, Florence, ItalyTABLE V
COMPARISON OF THE DYNAMIC AND STA TIC V ARIANTS .
Dynamic V ariant Static V ariant
# Avg # Avg
Test # Sub Test # Sub
Subject Steps Sub Size RI CI BI Steps Sub Size RI CI BI
Bookstore 223 19 50.49 0.85 0.42 279 15 50.44 0.84 0.58
Classiﬁeds 103 12 50.49 0.83 0.36 161 7 12 0.26 0.65 0.41
jBilling 107 16 40.75 0.90 0.46 47 13 11 0.73 0.81 0.37
Tudulist 5 3 11 0.74 0.91 0.27 19 5 80.71 0.74 0.50
T o t/A v g 438 50 6.25 0.62 0.87 0.38 506 40 90.54 0.76 0.47
●
●●
●●
B/g237D B/g237S C/g237D C/g237S J/g237D J/g237S T/g237D T/g237S5 1 01 52 0# Sub/g237Routine Steps
Fig. 5. Distribution of subroutine sizes for the dynamic and static variants.
The subject names are abbreviated to their initials; “D” indicates dynamic,whereas “S” represents the static variant.
with the same control parameters. For Classifieds and
jBilling , none of the subroutines have more than four
distinct behaviors; for Bookstore , none of the subroutines
have more than ﬁve distinct behaviors, with 75% of the
subroutines having at most three distinct behaviors. Even in thecase of
Tudulist , for which the subroutines exhibit relatively
more behaviors, most of the subroutines have ﬁve or lessdistinct behaviors; for the remaining, the number of behaviorsranges between six and eight.
C. Study 2: Comparison of Dynamic and Static V ariants
Goals and Method: In the second study, we compared the
dynamic and static variants. In general, we expect the dynamic
variant to be more effective in extracting subroutines that havehigh reduction and reuse, with low distinct behaviors. Wecollected data to compare the variants in terms of the sizesof the refactored tests, the number of subroutines, the averagesubroutine size, the three indexes, and execution times.
Results and Analysis: Table V presents the data comparing
the variants on these metrics. The dynamic variant identiﬁesmore subroutines (50 versus 40) that have smaller lengths onaverage (6.25 steps versus 9 steps) than the static variant.The dynamic variant also outperforms the static variant on thereduction index and the call index for all subjects. On the BI
metric, however, the results are mixed and more favorable tothe static variant. While the dynamic variant is more effectivefor
jBilling , it is less effective for the other subjects. This is
particularly notable for Tudulist , where the subroutines for
the dynamic variants can have twice as many distinct behaviors(four versus two) as the subroutines computed by the staticvariant. For
Tudulist , the dynamic variant computes a large
subroutine (mentioned earlier), which contains 22 steps andhas 33 callers and 11 distinct behaviors.
Figure 5 presents the distribution of the subroutine sizes
for the dynamic and static variants. With the exception of/g19/g19/g17/g21/g19/g17/g23/g19/g17/g25/g19/g17/g27
/g19/g17/g20 /g19/g17/g21 /g19/g17/g22 /g19/g17/g23 /g19/g17/g24 /g19/g17/g25 /g19/g17/g26 /g19/g17/g27 /g19/g17/g28 /g20/g17/g381/g381/g364/g400/g410/g381/g396/g286 /g18/g367/g258/g400/g400/g349/g296/g349/g286/g282/g400 /g361/g17/g349/g367/g367/g349/g374/g336 /g100/g437/g282/g437/g367/g349/g400/g410
/g19/g19/g17/g21/g19/g17/g23/g19/g17/g25/g19/g17/g27
/g19/g17/g20 /g19/g17/g21 /g19/g17/g22 /g19/g17/g23 /g19/g17/g24 /g19/g17/g25 /g19/g17/g26 /g19/g17/g27 /g19/g17/g28 /g20
Fig. 6. Effects of varying λe(horizontal axis) on the harmonic mean of RI,
CI, and BI for the dynamic variant (top) and the static variant (bottom).
Tudulist , the dynamic variant produces smaller subroutines
than the static variant, and this difference is quite signiﬁcant
forClassifieds andjBilling .
In terms of execution performance, the static variant exe-
cuted in about a second or less for each subject. The dynamicvariant has a large overhead for loading execution traces; afterthe traces are loaded, the time for modularaization is roughlythe same as that for the static variant. For
jBilling , the
execution time was a little over a minute (most of which wasspent in loading a 60MB trace ﬁle), whereas for the othersubjects, the dynamic variant took about 10 seconds or less.
D. Study 3: Effects of the Conﬁguration Parameters
Goals and Method: In the ﬁnal study, we investigated the
effects of varying the LCA distance parameter λ
d(Algo-
rithm 1) and the element-overlap parameter λe(Algorithm 2).
We varied λdandλefrom 0.1 to 1 in increments of 0.1 for both
the dynamic and static variants. As the dependent variable, we
used the harmonic mean of RI,CI, and BI, which we refer
to as the modularization index (MI ). The harmonic mean is
used as it tends to privilege balanced systems compared toother means such as the arithmetic mean [26].
Results and Analysis: Forλ
d, with the dynamic algorithm,
we did not see any difference in MI, whereas with the static
variant, MI decreased slightly with increase in λd, before
stabilizing. We next discuss in more detail the results for λe
for which we observed greater variations in MI.
Figure 6 presents the plot of the modularization index
against different values of λefor the dynamic and static
variants. A general trend that can be observed is that theMI values increase monotonically and plateau out around
0.7, with only minor or no increases after that. There are acouple of notable exceptions in the static case: MI continues
to increase for
jBilling even for λe>0.7and decreases
sharply for Bookstore forλe>0.8. Based on this data, the
range 0.7–0.8 for λeseems to offer the best trade-off between
accommodating candidate subroutines with some diversity tobe merged, while also attaining high scores for the reduction,call, and behavior indexes.
52
52
52
52
ICSE 2015, Florence, ItalyMore generally, the static variant displays a greater sensi-
tivity toλe, and therefore greater instability, than the dynamic
variant, which also suggests that the dynamic algorithm is
superior to the static algorithm for subroutine extraction.
E. Discussion
Although our results are promising, like any empirical study,
there are threats to the validity of our results. The most
signiﬁcant of these are threats to external validity, which arisewhen the observed results cannot be generalized to otherexperimental setups. In our study, we used four subjects withmanually created functional test suites. The performance of thetechnique may vary for other subjects and types of test suites(e.g., tests generated via automated techniques, with different
coverage goals).
IV . R
ELA TED WORK
There is a large body of work on automated techniques for
generating GUI tests, based on different notions of coverage,in the context of web applications (e.g., [12]–[16], [19], [20])
and other forms of GUIs (e.g., [17], [18], [21], [27]). The
tests generated by such techniques are essentially sequencesof GUI events (e.g., as illustrated in Figure 1(b)); thus, being
non-modular, they can be hard to maintain when intended foruse in regression testing. This has motivated the developmentof techniques for automatically repairing test cases (e.g., [5]–
[8], [28]), or generating less-brittle tests upfront or tests withreliable oracles (e.g., [4], [29]) that require less maintenance.
An orthogonal approach toward simplifying maintenance is tomodularize the tests, but this has not received much attentionin the testing research community.
The work that comes closest to ours in terms of the objective
is the test-modularization approach presented by Mahmud andLau [22]. Their technique uses supervised machine learning(requiring a training sample of subroutines) and analyzes testswritten in ClearScript [23], [30]. Given a training set ofsubroutines, it computes similarity of test-case instruction se-quences with instructions in the labeled subroutines, and usesheuristics to accommodate partial matches and generalizations.The effectiveness of that techniques, in general, depends onthe diversity of the training set. Moreover, it does not analyzethe referenced UI elements and the structure of the navigatedwebpages, which can affect its accuracy in two ways. First, twotest steps that read the same in ClearScript may, in fact, referto UI elements on different pages. Therefore, similar-lookingactions on different pages could be erroneously marked asbelonging to the same subroutine. Second, two sequences ofinstructions that reach the same page, but perform differentactions on the page would have low similarity, but nonetheless,could be considered as part of the same subroutine (fromthe perspective of maintaining test scripts). In such cases,the technique could fail to identify reusable subroutines. Ourtechnique addresses these issues by analyzing the UI elementsand the structure of the pages accessed in the tests, and notrelying on the availability of training samples.
In previous work, we presented a technique for merging
similar GUI tests in a semantics-preserving manner [31]. Thegoal was to improve the execution efﬁciency of GUI tests
by eliminating repeated executions of the same sequence ofactions in the context of different test cases (while preservingthe fault-detection effectiveness). Given a suite of GUI tests,the technique identiﬁes the tests that can be combined andcreates a merged test, which covers all the application statesexercised individually by the tests, but with the redundantcommon steps executed only once. In contrast, the goal of thiswork is to improve the maintenance efﬁciency by extracting
out reusable subroutines and making GUI tests modular.Although modularization can simplify maintenance, it doesnot improve execution efﬁciency—e.g., the common steps of
t
1andt2(Figure 1) execute twice, in the contexts of those
tests, after modularization.
In programming, ExtractMethod code refactoring refers to
the extraction of code statements from a given method into an-other newly created method. The end goals of ExtractMethod
code refactoring and our test-case modularization are thesame—increased modularization and better maintainability.However, ExtractMethod code refactoring is distinct from test-
case modularization in the techniques used to achieve theend goal. Method extraction is primarily based on programslicing [32], [33] to make the dependence-related statementscontiguous for extraction. Our technique is applicable toGUI tests cases that consist of sequences of events on theapplication user interface; it extracts contiguous events into asubroutine with parameterized data and control ﬂow.
V. S
UMMARY AND FUTURE WORK
We presented a new technique for automated modularization
of GUI test cases. Unlike learning-based GUI test modulariza-tion [22], our technique analyzes the references to UI elementsin test steps and the induced DOM states, and requires notraining samples of labeled subroutines. Our approach can beapplied to GUI test cases created via manual recording orautomated test-generation techniques (e.g., [12], [13], [15],
[19]), both of which result in non-modular tests. Efﬁcientlymodularizing such tests using an automated technique, suchas ours, can reduce the maintenance cost of the tests and,thus, make them more suitable for use in regression testing.Our empirical results demonstrated the effectiveness of thetechnique in extracting highly reusable subroutines and reduc-ing the sizes of test cases, while keeping the comprehensionoverhead of the subroutines low.
More experimentation with varied subjects and test suites
(e.g., considering tests generated by automated techniques)
would help conﬁrm the generality of our results. Our techniquecould be extended by iterative reﬁnement of subroutines so thatmulti-level subroutine decomposition can be performed, in-stead of one-level decomposition which our current algorithmdoes. Such reﬁnement could be guided, for example, by an up-per bound on the number of distinct behaviors. Finally, futureresearch could investigate how modular and non-modular testscompare in terms of maintenance and comprehension effort asthe application under test evolves.
53
53
53
53
ICSE 2015, Florence, ItalyREFERENCES
[1] “Selenium,” http://seleniumhq.org/.
[2] M. Leotta, D. Clerissi, F. Ricca, and P . Tonella, “Capture-replay vs.
programmable web testing: An empirical assessment during test caseevolution,” in WCRE, 2013, pp. 272–281.
[3] S. Thummalapenta, P . Devaki, S. Sinha, S. Chandra, S. Gnanasundaram,
D. D. Nagaraj, and S. Sathishkumar, “Efﬁcient and change-resilient testautomation: An industrial case study,” in ICSE Software Engineering in
Practice Track, 2013, pp. 1002–1011.
[4] R. Yandrapally, S. Thummalapenta, S. Sinha, and S. Chandra, “Robust
test automation using contextual clues,” in ISSTA, 2014, pp. 304–314.
[5] A. M. Memon, “Automatically repairing event sequence-based GUI test
suites for regression testing,” ACM Transaction Software Engineering
and Methodology, vol. 18, pp. 1–36, November 2008.
[6] M. Grechanik, Q. Xie, and C. Fu, “Maintaining and evolving GUI-
directed test scripts,” in ICSE, 2009, pp. 408–418.
[7] S. R. Choudhary, D. Zhao, H. V ersee, and A. Orso, “W A TER: Web
application test repair,” in ETSE, 2011, pp. 24–29.
[8] S. Zhang, H. L ¨u, and M. D. Ernst, “Automatically repairing broken
workﬂows for evolving GUI applications,” in ISSTA, 2013, pp. 45–55.
[9] M. Leotta, D. Clerissi, F. Ricca, and C. Spadaro, “Improving test suites
maintainability with the page object pattern: An industrial case study,”inICST Workshops, 2013, pp. 108–113.
[10] “Page Objects,” https://code.google.com/p/selenium/wiki/PageObjects.[11] A. M. Memon, “An event-ﬂow model of GUI-based applications for
testing,” Softw. Test., V erif. Reliab., vol. 17, no. 3, pp. 137–157, 2007.
[12] A. Marchetto, P . Tonella, and F. Ricca, “State-based testing of Ajax web
applications,” in ICST, 2008, pp. 121–130.
[13] A. Mesbah, A. van Deursen, and D. Roest, “Invariant-based automatic
testing of modern web applications,” IEEE Trans. Software Eng., vol. 38,
no. 1, pp. 35–53, Jan/Feb 2012.
[14] D. Amalﬁtano, A. R. Fasolino, and P . Tramontana, “Rich internet
application testing using execution trace data,” in ICST Workshops, 2010,
pp. 274–283.
[15] S. Thummalapenta, K. V . Lakshmi, S. Sinha, N. Sinha, and S. Chandra,
“Guided test generation for web applications,” in ICSE, 2013, pp. 162–
171.
[16] A. Mesbah, A. van Deursen, and S. Lenselink, “Crawling Ajax-based
web applications through dynamic analysis of user interface statechanges,” ACM Trans. on the Web, vol. 6, no. 1, pp. 1–30, Mar. 2012.[17] B. N. Nguyen, B. Robbins, I. Banerjee, and A. Memon, “GUITAR:
An innovative tool for automated testing of GUI-driven software,”Automated Software Engg., vol. 21, no. 1, pp. 65–105, Mar. 2014.
[18] F. Gross, G. Fraser, and A. Zeller, “EXSYST: Search-based GUI testing,”
inICSE, 2012, pp. 1423–1426.
[19] A. M. Fard and A. Mesbah, “Feedback-directed exploration of web
applications to derive test models,” in ISSRE, 2013, pp. 278–287.
[20] A. Mesbah and M. R. Prasad, “Automated cross-browser compatibility
testing,” in ICSE, 2011, pp. 561–570.
[21] S. Ganov, C. Killmar, S. Khurshid, and D. E. Perry, “Event listener anal-
ysis and symbolic execution for testing GUI applications,” in ICFEM,
2009, pp. 69–87.
[22] J. Mahmud and T. Lau, “Lowering the barriers to website testing with
CoTester,” in IUI, 2010, pp. 169–178.
[23] G. Leshed, E. M. Haber, T. Matthews, and T. Lau, “Coscripter: Automat-
ing and sharing how-to knowledge in the enterprise,” in CHI, 2009, pp.
1719–1728.
[24] S. Thummalapenta, N. Singhania, P . Devaki, S. Sinha, S. Chandra, A. K.
Das, and S. Mangipudi, “Efﬁciently scripting change-resilient tests,” in
FSE Tool Demonstrations Track, 2012, pp. 41:1–41:2.
[25] T. J. McCabe, “A complexity measure,” IEEE Transactions on Software
Engineering, vol. SE-2, no. 4, pp. 308–320, Dec. 1976.
[26] D. Nadeau and S. Sekine, “A survey of named entity recognition and
classiﬁcation,” Lingvisticae Investigationes, vol. 30, no. 1, pp. 3–26,
2007.
[27] L. Mariani, M. Pezze, O. Riganelli, and M. Santoro, “AutoBlackTest:
Automatic black-box testing of interactive applications,” in ICST, 2012,
pp. 81–90.
[28] S. Huang, M. B. Cohen, and A. M. Memon, “Repairing gui test suites
using a genetic algorithm,” in ICST, 2010, pp. 245–254.
[29] D. Roest, A. Mesbah, and A. van Deursen, “Regression testing Ajax
applications: Coping with dynamism,” in ICST, 2010, pp. 127–136.
[30] G. Little, T. A. Lau, A. Cypher, J. Lin, E. M. Haber, and E. Kandogan,
“Koala: capture, share, automate, personalize business processes on the
web,” in CHI, 2007, pp. 943–946.
[31] P . Devaki, S. Thummalapenta, N. Singhania, and S. Sinha, “Efﬁcient
and ﬂexible GUI test execution via test merging,” in ISSTA, 2013, pp.
34–44.
[32] N. Tsantalis and A. Chatzigeorgiou, “Identiﬁcation of extract method
refactoring opportunities,” in CSMR, 2009, pp. 119–128.
[33] R. Komondoor and S. Horwitz, “Effective, automatic procedure extrac-
tion,” in IWPC, 2003, pp. 33–42.
54
54
54
54
ICSE 2015, Florence, Italy