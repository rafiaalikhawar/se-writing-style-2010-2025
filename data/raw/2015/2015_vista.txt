Visual Web Test Repair
Andrea Stocco
University of British Columbia
Vancouver, BC, Canada
astocco@ece.ubc.caRahulkrishna Yandrapally
University of British Columbia
Vancouver, BC, Canada
rahulky@ece.ubc.caAli Mesbah
University of British Columbia
Vancouver, BC, Canada
amesbah@ece.ubc.ca
ABSTRACT
Web tests are prone to break frequently as the application under
test evolves, causing much maintenance e￿ort in practice. To detect
the root causes of a test breakage, developers typically inspect the
test’s interactions with the application through the GUI. Existing
automated test repair techniques focus instead on the code and
entirely ignore visual aspects of the application. We propose a test
repair technique that is informed by a visual analysis of the appli-
cation. Our approach captures relevant visual information from
tests execution and analyzes them through a fast image processing
pipeline to visually validate test cases as they re-executed for re-
gression purposes. Then, it reports the occurrences of breakages
and potential ￿xes to the testers. Our approach is also equipped
with a local crawling mechanism to handle non-trivial breakage
scenarios such as the ones that require to repair the test’s work￿ow.
We implemented our approach in a tool called V￿￿￿￿ . Our empirical
evaluation on 2,672 test cases spanning 86 releases of four web
applications shows that V￿￿￿￿ is able to repair, on average, 81% of
the breakages, a 41% increment with respect to existing techniques.
CCS CONCEPTS
•Software and its engineering →Software testing and de-
bugging ;
KEYWORDS
web testing, test repair, computer vision, image analysis
ACM Reference Format:
Andrea Stocco, Rahulkrishna Yandrapally, and Ali Mesbah. 2018. Visual
Web Test Repair. In Proceedings of the 26th ACM Joint European Software
Engineering Conference and Symposium on the Foundations of Software Engi-
neering (ESEC/FSE ’18), November 4–9, 2018, Lake Buena Vista, FL, USA. ACM,
New York, NY, USA, 12pages. https://doi.org/10.1145/3236024.3236063
1 INTRODUCTION
Test automation techniques are used to enable end-to-end (E2E)
functional testing of web applications [ 57]. In this context, the
tester veri￿es the correct functioning of the application under test
(AUT) by means of automated test scripts. Such scripts automate
the set of manual operations that the end user would perform on the
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for pro￿t or commercial advantage and that copies bear this notice and the full citation
on the ￿rst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speci￿c permission and/or a
fee. Request permissions from permissions@acm.org.
ESEC/FSE ’18, November 4–9, 2018, Lake Buena Vista, FL, USA
©2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5573-5/18/11. . . $15.00
https://doi.org/10.1145/3236024.3236063web application’s graphical user interface (GUI), such as delivering
events with clicks, or ￿lling in forms, and they are typically used
for regression testing [ 6,19,21,43,45].
Despite their wide adoption, E2E tests are known to be fragile.
Changes as simple as repositioning GUI elements on the page or
altering the selections in a drop-down list can cause the test to
break. In literature, instances of these problems are known as test
breakages [15–17,24]. A test breakage is de￿ned as the event that
occurs when the test raises exceptions or errors that do not pertain
to the presence of a bug or a malfunction of the application under
test. This is di￿erent from cases in which tests expose failures ,
meaning they raise exceptions which signal the presence of one or
more bugs in the production code. In the latter case, the developer
is required to correct the application, whereas in the former case,
the tester must ￿nd a ￿x for the broken test.
Researchers have categorized breakages happening as test suites
for web applications are maintained [ 24]. Web element locators
have emerged as the main cause of fragility of tests, con￿rming
previous anecdotal ￿ndings [ 17,30].
Existing automated web test repair techniques [ 11,23] essen-
tially consider only the DOM of the web page as a source where
to ￿nd possible repairs. Thus, these techniques are, in many cases,
either unable to correct breakages, or produce many false positives.
Moreover, breakages do not always occur at the precise point in
which the test execution stops, which renders automated repair
even more challenging. To detect their root causes, testers man-
ually inspect the tests’ interaction with the application, carefully
verifying the action performed by each test step on the GUI, which
is tedious and time-consuming.
In this paper, we propose a novel test repair algorithm, imple-
mented in a tool named V￿￿￿￿ , that leverages the visual information
extracted through the execution of test cases, along with image
processing and crawling techniques, to support automated repair of
web test breakages. The key insight behind our approach is that the
manual actions and reasoning that testers perform while searching
for repairs can be automated to a large extent by leveraging and
combining di￿erential testing, computer vision, and local crawling.
V￿￿￿￿ performs visual online monitoring capturing visual snap-
shots of the correct execution of the test cases. When the tests
are replayed on a new version of the application, it uses the web
elements’ visual appearance to validate each test statement prior to
their execution. On the occurrence of a breakage, V￿￿￿￿ triggers a
series of repair heuristics on the ￿y, to ￿nd potential ￿xes to report
to the user. Upon inspection, the tester can con￿rm or discard the
suggestions. In addition, using an automated local visual-based
crawl exploration mechanism, V￿￿￿￿ is able to handle complex
breakage scenarios, such as those that break the test’s execution
work￿ow, e.g., when elements are moved to another page, or a new
web page is added in between test steps.ESEC/FSE ’18, November 4–9, 2018, Lake Buena Vista, FL, USA A. Stocco et al.
We evaluated V￿￿￿￿ on a benchmark of 733 individual breakages
from 2,672 test cases spanning 82 releases of four open source web
applications. For each of them, we documented the type of breakage,
and the position of the associated repair, resulting in a taxonomy of
test breakages in web applications from a repair-oriented viewpoint.
Our paper makes the following contributions:
•The ￿rst repair-oriented taxonomy of test breakages in web
applications.
•An algorithm for visually monitoring and validating web
test cases, based on a fast image processing pipeline.
•A novel approach for repairing broken test cases using visual
analysis and crawling.
•An implementation of our algorithm in a tool named V￿￿￿￿ ,
which is publicly available [ 58].
•An empirical evaluation of V￿￿￿￿ in repairing the breakages
found in our study. V￿￿￿￿ was able to provide correct repairs
for 81% of breakages, with a 41% increment over an existing
DOM-based state-of-the-art approach.
2 A WEB TEST BREAKAGE TRAVELOGUE
E2E web tests are known for being fragile in the face of software
evolution [ 24,30]. Even a minor change in the DOM or GUI might
break a previously developed test case. The test would then need
to be repaired manually to match the updated version of the appli-
cation, even if conceptually the functionality is unaltered, and no
errors are present in the application code.
Characterization of a Breakage. In order to clarify the scope of
our work, it is important to emphasize the di￿erence between test
breakages andtest failures . We consider a test breakage as the event
that occurs when a test Tnthat was used to work and pass on a
certain version Vn, fails to be applicable to a version Vn+k(k 1)
due to changes in the application code that interrupt its execution
unpremeditatedly. This is di￿erent from cases when tests desirably
expose a program failure and hence do something for which they
have been designed (i.e., exposing regression faults). In this paper,
we focus our analysis on test breakages.
Study of Breakages. In a recent study, researchers have catego-
rized breakages as test suites for web applications were evolved [ 24].
The study shows that web element locators are the main cause of
fragility (74% of the totality of breakages).
Indeed, the mapping between locators and DOM elements is
massively a￿ected by structural changes of the web page, that may
render tests inapplicable, just because the locators (and not the
tests themselves) become ine￿ective [ 11,24,30]. The addition or
repositioning of elements within the DOM can in fact cause locators
to “non-select” or “mis-select” elements across di￿erent versions of
the same web page. In the former case, a locator is unable to retrieve
the target element, whereas in the latter case a locator selects a
di￿erent DOM element from the one that was used to target.
Concerning the temporal characterization of test breakages, re-
searchers distinguish between direct, propagated, and silent break-
ages [ 24]. A breakage is called direct when the test stops at a state-
ment sti, and stihas to be repaired in order to let the test pass or
continue its execution. With propagated breakages, on the other
hand, the test stops at a statement sti, but another statement stj,
preceding sti(i.e., i>j), must be repaired in order to make thetest pass or continue its execution. Finally, silent breakages do not
manifest explicitly because the test neither stops nor fails, but yet
diverges from its original intent, and only by manually checking
its execution (for example by looking at the actions performed on
the GUI), the tester can detect the mis-behaviour.
Existing Web Test Repair Approaches. Test repair techniques
have been proposed in recent years [ 11,15–17,20]. In the web
domain, the state-of-the-art test repair algorithm is W￿￿￿￿ [11], a
di￿erential technique that compares the execution of a test over two
di￿erent releases, one where the test runs correctly and another
where it breaks. By gathering data about these executions, W￿￿￿￿
examines the DOM-level di￿erences between the two versions and
uses heuristics to ￿nd and suggest potential repairs. While the repair
procedure of W￿￿￿￿ has a straightforward design and can manage
a considerable number of cases related to locators or assertions,
it has a number of limitations that derive from its DOM-related
narrowness: First, considering only on the DOM as a source where
to ￿nd possible repairs may be insu￿cient to ￿nd candidate ￿xes.
Second, this can lead to a large number of false positives [ 11]. Third,
the algorithm attempts repairs always at the point in which the test
stops, which makes it impossible to handle propagated breakages.
3 A STUDY ON TEST BREAKAGES & REPAIRS
3.1 Breakages Root Cause Analysis
Given the predominance of locator-related test breakages [ 17,24],
we focus our analysis on locator problems. Existing taxonomies
related to web and GUI breakages [ 24,26,28] lack a temporal quan-
ti￿cation of breakages, even less they propose approaches to repair
them. Indeed, knowing the exact position in which a test fails is ar-
guably a prerequisite for developing e￿ective automated test repair
solutions. Although repairing locators seems mostly a mechanical
and straightforward activity, to conduct an e￿ective root cause anal-
ysis, testers must consider all aspects behind breakages (e.g., their
causes and positions in the tests) and link them together to devise
possible repairs that do not change the intended test scenario.
3.2 Study Design
To gain an understanding of the variety of web test repairs, we con-
ducted a study to categorize the breakages from a repair-oriented
perspective. The ￿ndings of our study highlight the complexity and
the variety of breakage scenarios that can occur in web tests, which
automated test repair techniques should aim to handle.
With the intention of studying realistic test regression scenarios,
we selected open source web applications that were used in the
context of previous research on web testing, for which multiple
versions and Selenium test cases were available.
Table 1 (Web Applications) shows information about the selected
applications, including their names, the numbers of releases con-
sidered, and the average number of lines of code, counted using
cloc [13].Table 1 (Test Suites) provides data on the test suites,
including the total number of test cases counted across all releases
along with the average number of test cases per release, and the to-
tal number of statements in all test cases counted across all releases
along with the average number of statements per test case.
Procedure. To collect breakage information for each web applica-
tion, we followed a systematic and iterative procedure, similar toVisual Web Test Repair ESEC/FSE ’18, November 4–9, 2018, Lake Buena Vista, FL, USA1  driver.findElement(By.name(“user”)).sendKeys(“user”); 2  driver.findElement(By.name(“password”)).sendKeys(“pass”); 3  driver.findElement(By.xpath(“//input[@value=“Login”]”)).click(); 4  driver.findElement(By.linkText(“add new”)).click(); 5  driver.findElement(By.xpath(“//input[3]“)).sendKeys(“Name”); 6  driver.findElement(By.xpath(“//input[4]“)).sendKeys(“LastName”); 7  driver.findElement(By.xpath(“//input[5]“)).sendKeys(“Company”);
1
2
3
4
5
6
7
a
1
2
3
4
5
6
7(a)Version 6.2.121  driver.findElement(By.name(“user”)).sendKeys(“user”); 2  driver.findElement(By.name(“password”)).sendKeys(“pass”);  3  driver.findElement(By.xpath(“//button[text()=’Login’]”)).click(); 4  driver.findElement(By.linkText(“add new”)).click(); 5  driver.findElement(By.name(“Next”)).click(); 6  driver.findElement(By.name(“//input[3]“)).sendKeys(“Name”); 7  driver.findElement(By.xpath(“//input[4]“)).sendKeys(“LastName”); 8  driver.findElement(By.xpath(“//input[7]“)).sendKeys(“Company”);
1
2
3
4
6
7
8
5
1
2
3
4
5
6
7
8
b
(b)Version 7.0.0
Figure 1: AddressBook web application, version 6.2.12 ( 1a) and version 7.0.0 ( 1b), along with Selenium WebDriver tests.
an analogous data collection study [ 24]. For each subject, and for
each version Vnand its accompanying test suite Tn, we executed
Tnon the subsequent version Vn+1. As locator breakages occurred,
we noted information about the type of each breakage, the position
of the associated repair, and assigned descriptive labels. Reviewing
these descriptions allowed equivalence classes to be identi￿ed, to
which descriptive labels were also assigned.
To continue the experimental work, however, we had to repair
the breakages. To minimize bias and subjectivity, we utilized the
same construct used by the locator (i.e., if an idattribute is changed,
we used the new idattribute). In the cases where this was not
possible, we favoured locators that we believe were the most robust
and reliable [ 34]. We repeated this process until all the versions
were taken into account.
Our benchmark comprises 733 individual test breakages, dis-
tributed as follows: 50 for AddressBook, 165 for Claroline, 218 for
Collabtive, and 300 for PPMA.
Table 1: Subject systems and their test suites
W￿￿ A￿￿￿￿￿￿￿￿￿￿￿ T￿￿￿ S￿￿￿￿￿
Releases (#) LOCs (K) Tot/Avg (#) LOCs (K)
AddressBook 48 8,757 1,344/28 61,826/43
Claroline 12 338,129 492/41 20,287/43
Collabtive 14 150,696 560/40 22,710/38
PPMA 12 556,164 276/23 16,732/47
Total/Average 86 263,436 2,672/33 121,555/43
3.3 Test Breakages and How To Repair Them
We ￿rst describe the breakage scenarios our study revealed.
Basic Terminology. At a high level, each web test statement is a
tuple <locator, action, value> . The locator component speci-
￿es the web element the test statement is interacting with. A locator
lis a function on a DOM state D. Notationally, l:D!{ e}where
eis the web element returned by the locator lwhen applied to D.
Non-Selection •Same Page. A non-selection occurs when a lo-
cator lapplied to a DOM state Dreturns no elements—formally,
l:D!;, but the target element eis still present on the page
(e2D ). Then, possible repairs require to ￿nd another locator
l0|l0:D!e.Let us consider the login page of AddressBook in Figure 1a ,
and the accompanying WebDriver test a . In the new subsequent
version 7.0.0, as a result of the application evolution, the login but-
ton gets modi￿ed as follows: <input value=‘‘Login’’></input>
becomes <button>Login</button> .
When executed on version 7.0.0, the test b will stop at Line 4
while attempting to locate the login button. At a visual inspection
of the two GUIs, a tester would expect the test to work, because
her perception is immaterial as far as changes at DOM-level are
concerned. It is indeed evident that the target element is visually still
present on the page, and its position on the GUI has not changed.
Non-Selection •Neighbouring Page. Notationally this can be
expressed as l:D!;^e<D^9D02nei h(D) | l:D0!{ e}.
As a concrete example consider Figure 1a , speci￿cally the pages
in which the user can insert a new entry. The test a clicks on the
“add new” link on the home page (Line 4), and ￿lls in the “First name”,
“Last name” and “Company” text ￿elds (Lines 5–7). When executed
on the successive version 7.0.0 b , the test will raise an exception
of kind NoSuchElementException at Line 5, when attempting to
locate the “First name” text ￿eld. Indeed, a new intermediate con￿r-
mation page has been added, and the navigational work￿ow of the
test must be corrected to re￿ect that of the new web application.
From a testing perspective, the “First name” text ￿eld can no
longer be found on the web page (test state) following the execution
of the statement at Line 5. However, conceptually, the repair action
that needs to be triggered in order to correct the test has nothing
to do with the locator at Line 6. In fact, by only looking at the
exception, it is arduous for the tester to understand what the actual
problem is, unless the visual execution of the test is taken into
consideration. A possible solution would require to (1) detect that
the web element eno longer exists as part of the test state stiin
version V, (2) try to match the ein the neighbouring states of stiin
the new version V0, which in turn requires to (3) ￿nd a web element
e02stisuch that (e0,sti)!stj(the “Next” button in Figure 1b ).
Non-Selection •Removed. The third and last non-selection sce-
nario concerns a web element being removed from a web page.
Formally, this can be expressed as l:D!;^@D02nei h(D) |
l:D0!{ e}. Let us consider the application being evolved in
the reverse order as depicted in Figure 1 (i.e., from version 7.0.0
to version 6.2.12). The test b would stop at Line 6, when trying
to select the “Next” button, which was on a page that is no longer
present. In this case, the only possible ￿x is to delete the statement
at Line 6.ESEC/FSE ’18, November 4–9, 2018, Lake Buena Vista, FL, USA A. Stocco et al.
Removed2.8%Same Page88.5%Direct
2.6% Neighbouring Page
4.5% Propagated
1.6%Mis-Selection
Non-Selection
Figure 2: Causal (inner pie) and Temporal (outer pie) Repair-
Oriented Characterization of Locator Breakages.
Mis-Selection •Direct and Propagated. The addition or repo-
sitioning of elements within the DOM can cause locators to “mis-
select” elements across di￿erent versions of the same web page.
Speci￿cally, a mis-selection occurs when a locator selects a di￿erent
DOM element from the one that was used to target. Notationally,
l:D!{ e}inV, and l:D0!{ e0}inV0where e,e0.
Consider Figure 1 again. Suppose that the test a is repaired so
as to reach the edit page on version 7.0.0 (for instance, as in b ).
On the new version 7.0.0, the statements at Lines 6–7 will execute
correctly, whereas at Line 8 (in the new version) the test will ￿ll in
the ￿eld “Nickname”, instead of the ￿eld “Company”.
The mis-selection problem can lead to unpredictable test execu-
tions, that diverge from the test’s intended behaviour. The test may
continue its execution until it reaches a point in which an action
cannot be performed or an element cannot be found, but the actual
repair has to be triggered in a previous test statement .
In fact, the point in which the repair must be applied varies
depending on where the mis-selection ￿rst originated. Repairing
mis-selections requires ￿nding another locator l0|l0:D0!{ e}.
3.4 Findings
Figure 2 shows the distribution of the di￿erent classes of locator
breakages and relative repairs. Our study revealed two major cate-
gories, each of which has speci￿c subtypes.
The most prevalent category concerns Non-Selection of web ele-
ments (695/733), all raising direct breakages in the tests. Concerning
the associated repairs, the vast majority were found on the Same
Page , whereas other scenarios refer to web elements that were
moved to a Neighbouring Page , or being Removed from any web
page. The second main category consists of Mis-Selection of web
elements (38/733), whose repairs were always found within the
same page, and of which 2/3led to Direct breakages, and 1/3toProp-
agated breakages. We have not observed silent breakages in our
benchmark. Additionally, we collected 94 test failures —meaning the
tests exposing actual bugs—and 22 failures due to obsolete assertion
values. In this work, we only focus on repairing locator breakages.
Overall, 329/2,672 tests (12%) exhibited at least one breakage,
and⇡80% of those tests su￿ered from multiple breakages. Figure 3
shows box-plots of the distribution of such breakages per test in0 2 4 6 8 10 12All AppsPPMACollabtiveClarolineAddressBookFigure 3: Distribution of test breakages per broken test case.each subject system. We observe that, on average, between 1–4breakages are present per test.To summarize, (1) test suites break frequently as the web appli-cation evolves, (2) breakages may occur multiple times within thesame test, (3) breakages fall into multiple recurrent patterns whichwe call breakage scenarios in this paper.4 APPROACHThe goal of our approach is to detect the occurrence of locatorbreakages as tests execute, and ￿nd potential ￿xes to report to theuser. Our focus is on the classes of breakages described inSection 3.3.In a nutshell, our approach captures the visual execution trace ofeach test statement for a versionVnand uses it to validate theircorrectness (or to ￿nd potential repairs) when they are executedon a subsequent versionVn+k(k 1).Figure 4illustrates the usage scenario of our approach, whichrequires a correct version of the web applicationValong with itsworking test suiteTS(i.e., in which all tests pass)∂. A tester wouldrunTSby means of the ￿rst module of the presented approach, theVisual Execution Tracer∑. Such a module collects, for each test,a variety of visual information (e.g., screenshots)∏. Then, as theapplicationVevolves into a new versionV0, a tester may wish touseTSto check if regressions have occurredπ. To this aim, thetester would use the second module of our approach, the VisualTest Repair∫, which runs each test case ofTSagainst the newversionV0, and makes use of the information about the previousexecution traces tovalidateeach test statement. On the detectionof breakages, our approach attempts to ￿nd candidate repairs thatcan ￿x them. At the end of the process, the approach outputs thevalidated (and eventually repaired) test suiteTS0, together withreport informationª. The tester can then manually analyze thereport along with the repaired test cases.TS0now represents aworking test suite for the versionV0which can be used as a baselinefor our approach whenV0will evolve further. The manual e￿ortrequired is potentially signi￿cantly reduced in comparison to a usercarefully verifying each executing test and manually searching for￿xes as breakages occur. We now detail each step of our approach.4.1 Visual Execution Trace CollectionIn the ￿rst part, the approach captures information about thedy-namic visual execution traceof the test cases, associating each teststatement with its visual information.More in detail, for each test statement, our approach capturesthe screenshot of the web page and the coordinates and size of theweb element’s bounding box in order to get a visual locator [35].Visual Web Test Repair ESEC/FSE ’18, November 4–9, 2018, Lake Buena Vista, FL, USA
App V
1Visual Execution TracerVisual Test Repair
Test Suite TS’(repaired)
Dynamic Visual Execution Trace2536
Test Suite TS4
ReportApp V’Test Suite TS
testscreenshotvisual locator
App V’
Figure 4: Overview of our approach
4.2 Visual Test Repair
Algorithm 1 illustrates the main procedure of our algorithm for
the visual-augmented execution of test cases. The procedure takes
as input a test case T, the dynamic visual execution trace EXof
Ton a version Vof the web application, and the URL Uof the
subsequent version V0of the web application. The outputs are a
testT0, validated or corrected to work on V0, and the map of repairs.
Initialization. The initial part involves loading the dynamic visual
execution trace EX, and opening a WebDriver session with the new
version V0(Lines 1–2).
Visual-Augmented Execution. The information contained in EX
is used to visually validate each statement stiofT, when executed
onV0(main loop Lines 4–30). The validation proceeds as follows.
First, the DOM-based locator utilized by stiis extracted, along with
its visual locator (i.e., an image) in V(Lines 5–6). Then, the driver
instance is used to query the DOM of V0to observe if the original
DOM locator returns a web element (Line 7).
Detecting and Repairing Non-Selection breakages. If no ele-
ment is returned, we treat it as a non-selection happening on the
Same Page . Given that sti, if executed, will break in V0, we attempt
to ￿nd a candidate repair through a series of countermeasures.
The ￿rst heuristic tries to search for the web element visually
on the same page. The visualSearch function (Line 10) uses ad-
vanced computer vision algorithms to retrieve the target web el-
ement by matching the visual locator captured in Von the cur-
rent GUI of V0(see details in Section 4.3 ). If a result is found, the
corresponding DOM element’s XPath is retrieved, and saved in
the map of candidate repairs (Line 19). Before proceeding to the
next statement, the approach outputs the outcome of the veri-
￿cation phase to the user, who can inspect, con￿rm, or enter a
manual ￿x before executing sti(for brevity, such details are encap-
sulated within the confirmEnterRepair andexecuteStatement
functions at Lines 28 and 29).
If the visual search on the same page returned no elements, then
our approach assumes that such an element no longer exists on
the current page and considers it as a broken work￿ow. A localAlgorithm 1: Visual Test Repair
Input: T: A test case developed for web application V,EX: execution trace of Twhen
executed on V,U: the URL of subsequent version V0
Output: T0: A veri￿ed/repaired Tworking on V0,repairMap : repairs for each test step
1trace  loadExecutionTrace( EX)
2driver  loadApp( U)
3statements  getStatements(T), repairMap  Maphstatement , <repair, locator >i
4foreach test statement sti2statements do
5 locator  getLocator( sti)
6 visLocator  getVisualLocator( trace ,sti)
7 webElement  driver .￿ndElement( locator )
8 /* Manage non-selection of web elements. */
9 ifwebElement == null then
10 webElemVisual  ￿￿￿￿￿￿S￿￿￿￿￿ (driver ,sti,visLocator )
11 ifwebElemVisual == null then
12 webElemVisual  ￿￿￿￿￿C￿￿￿￿￿￿￿ (driver ,sti,visLocator )
13 ifwebElemVisual == null then
14 repairMap .add(sti, <remove, null>)
15 else
16 repairMap .add(sti, <addBefore, webElemVisual >)
17 end
18 else
19 repairMap .add(sti, <update, webElemVisual >)
20 end
21 /* Manage mis-selection of web elements. */
22 else if webElement ,null then
23 webElemVisual  ￿￿￿￿￿￿S￿￿￿￿￿ (driver ,sti,visLocator )
24 if￿￿￿￿￿￿￿￿￿￿ (webElement, webElemVisual) == false then
25 repairMap .add(sti, <update, webElemVisual >)
26 end
27 end
28 ￿￿￿￿￿￿￿E￿￿￿￿R￿￿￿￿￿ (repairMap ,sti)
29 driver  ￿￿￿￿￿￿￿S￿￿￿￿￿￿￿￿ (sti,V0)
30end
31T0 ￿￿￿￿￿￿￿￿￿￿T￿￿￿ (repairMap )
32return T0, repairMap
exploration of the application state space is hence triggered (proce-
dure localCrawling of Line 12) in order to ￿nd the element in a
Neighbouring Page (see details in Section 4.3 ). In each new state dis-
covered by the exploration, the crawler executes the visualSearch
procedure to locate the target element. If a match is found in at
least one of those states, the XPath of the element to reach that
page is saved in the map of repairs (Line 16), and marked as a test
statement that needs to be added before stiin the repaired test case
(thus creating the missing transition).
On the contrary, if a match is not found, i.e., no elements were
found through local crawl, our approach considers the web element
asRemoved from the application, thus it suggests the deletion of
sti(Line 14).
Detecting and Repairing Mis-Selection breakages. If a web
element was returned by the original DOM locator (Line 7), our
approach attempts to validate the correctness of the selection by
using the previously collected visual information (Lines 22–27). The
equivalent function checks the equivalence of the web elements
found by the DOM locator and the visual locator. If they do di￿er,
our approach considers it a possible case of Mis-Selection that could
lead to a direct or propagated breakage. The procedure stores the
alternative web element’s XPath (Line 25), it reports the mismatch
to the user and asks for her input.
Outputs. At last, a repaired test T0is automatically created upon
the given suggestions/manual repairs (Line 31), and the algorithm
terminates. In the following section, we describe the visualSearch
andlocalCrawling procedures that underlie at the functioning of
our approach.ESEC/FSE ’18, November 4–9, 2018, Lake Buena Vista, FL, USA A. Stocco et al.
a
b
c
d
Feature extraction
Feature detection
Template matching & ﬁlteringDOM Queryhtmlheadtitlebodyformlabelinputlabelinputinput
s1
s2e1e3
s3
s4
s1
s0
e2next
evisual-based local crawlingFigure 5: Computer vision pipeline for robust web element detection (left) and visual-based local crawling for work￿ow repair.
4.3 Visual Search of Web Elements
Computer vision (CV) provides techniques for analyzing web pages
the way end users perceive them. Thereby, we provide a short
background of the CV solutions that are relevant in this work.
Challenges of Template Matching. Identifying web elements
visually across di￿erent versions (i.e., pages) of a web application
can be tackled with an image analysis technique known as template
matching . Template matching aims to detect the occurrence of a
speci￿c object image (template) in another reference image [ 8].
The template is slid over the image, and a similarity measure is
computed at each pixel location. The top-left corner point of the
area where the maximum similarity has been found is then returned.
In our context, the matching technique must handle translation
(i.e., the captured template can be shifted with respect to the refer-
ence image) and scaling (i.e., the aspect ratio of the template is not
preserved in the reference image, or a di￿erent font is used) prob-
lems. Indeed, web applications are often modi￿ed to accommodate
cosmetic or stylistic changes to align the GUI with the latest trends.
These changes may render our technique fragile, because the visual
locators captured on the oldGUI might be ine￿ective on the new
GUI of the application. Additionally, standard template matching
algorithms are not e￿ective in detecting the presence/absence of
a template, which is instead of paramount importance for the ac-
curacy of our algorithm. Even though stricter similarity threshold
values might be used, in our exploratory experiments this strategy
failed to provide robust or generalizable results. Thus, we explored
a more advanced CV solution, namely feature detection.
Feature Detection. The philosophy of this method is to ￿nd cer-
tain “important” points ( key-points ) in the template image and
store information about the neighbourhood of those key-points
(key-point descriptors ) as a description of the template. Then, af-
ter ￿nding the descriptors of key-points in the reference image,
the algorithm tries to match the two descriptor sets (one from the
template and one from the reference image) using some notion of
similarity, and see how many descriptors match.
Thus, the visualSearch procedure adopts a pipeline of CV al-
gorithms, which is graphically illustrated in Figure 5 .
Feature Detection for Template Absence & Presence. We im-
plemented a detector based upon two extensively-used and accurate
feature detection algorithms from the CV literature, SIFT [ 36,37]
and FAST [ 47,48]. The detector uses these algorithms to detect
the key-points from the template image using SIFT descriptors a ,
and then adopts a Flann-based descriptor matcher with a distance
threshold ratio of  =0.8, as per Lowe’s paper [ 37]. The detector
returns a positive result on the presence of the template if at least
70% of the key-points are matched. We used a combination of twofeature detection algorithms because we found, through experimen-
tation, that SIFT and FAST complement each other. As such, SIFT
performs well mostly for textual-based templates whereas FAST
can handle the cases where the template represents an “empty” text
￿eld, as it is speci￿cally designed for corner detection. In our picto-
rial example a , most of the key-points detected by SIFT are in fact
nearby the “Login” label (orange circles), whereas FAST detected
key-points also in proximity of the corners (blue circles). In b we
can see how the algorithms matched the key-points onto the new
GUI of the application. For further details, the interested reader is
referred to the relevant literature [ 36,37,47,48].
Template Matching with Visual Filtering and NMS. In the
next step, if the feature detection returned a positive result about
the presence of the template image, a template matching technique
is executed c . We experimented with di￿erent algorithms avail-
able in the open-source computer vision library OpenCV [ 44]. We
found the Fast Normalized Cross Correlation algorithm [ 7] with a
similarity threshold  =0.99to be optimal in our setting.
Yet, any template matching technique might return (1) false
visual matches, as well as (2) multiple overlapping bounding boxes
around the area of interest. Since our procedure ought to return
exactly one result, a post-processing algorithm is responsible for
discarding the false matches and merging all redundant detections.
In brief, (1) the matches that do not fall in the region where the
key-points have been found are discarded, and (2) a non-maxima
suppression operation (NMS) is also applied [ 39] (basically NMS
assumes highly overlapping detections as belonging to the same
object [ 9]). Thus, only the closest match is returned (see the green
thick rectangle over the “Login” button).
To summarize, the three CV algorithms (SIFT, FAST, and Fast
Normalized Cross Correlation) operate synergistically to ￿nd a
consensus on the location of the best match.
From GUI to DOM. At last, in order to retrieve the DOM ele-
ment corresponding to a speci￿c point of coordinates (x, ), the
visualSearch function queries the browser through JavaScript to
retrieve the DOM element whose bounding box centre contains x
and d . (otherwise, a DOM ancestor of the searched web element—
as aform ordivcontainer—will be erroneously returned).
Local Crawling for Work￿ow Repair. Manually repairing every
broken work￿ow is tedious and frustrating, since even a medium-
size web application may contain tens of GUI screens and hundreds
of GUI actions. It is hence likely infeasible for a tester to quickly
explore this state space to choose replacement actions from. For-
tunately, a web crawler can do this automatically. To this aim, the
localCrawling function explores the state space of V0looking for
a visual match in the immediate neighbouring pages of the currentVisual Web Test Repair ESEC/FSE ’18, November 4–9, 2018, Lake Buena Vista, FL, USA
web page. If a match is found, the work￿ow is repaired by adding a
transition to that page through the matched element ( Figure 5 d 
shows how the local crawling approach works for the example of
Figure 1b b ). Otherwise, our approach considers the element as
removed. Thus, to repair the work￿ow, it suggests the deletion of
the corresponding test step.
4.4 Implementation
We implemented our approach in a tool called V￿￿￿￿ (VisualTest
Repair), which is publicly available [ 58]. The tool is written in Java,
and supports Selenium test suites written in Java. However, our
overall approach is more general and applicable to test suites devel-
oped using other programming languages or testing frameworks.
V￿￿￿￿ gets as input the path to the test suites, collects the visual exe-
cution traces by means of PESTO [35]. Our visual detection pipeline
adopts algorithms available in the open-source computer vision
library OpenCV [ 44].V￿￿￿￿ makes use of the traces to retrieve
potential repairs and generates a list of repaired test cases. For the
local crawling exploration, V￿￿￿￿ features a Crawljax [ 41,42] plugin
that incorporates the visualSearch function. In our evaluation,
we limited the crawler’s exploration depth to one (1), its running
time to 2 minutes, and con￿gured it to return the ￿rst match found.
This was a conservative choice, since the number of DOM states
and events can be numerous. While this limits the search capability,
this design choice kept the running time acceptable.
For the interested reader, more technical details can be found in
our accompanying demo paper [ 56].
5 EMPIRICAL EVALUATION
We consider the following research questions:
RQ1(e￿ectiveness): How e￿ective is the proposed visual ap-
proach at repairing test breakages compared to a state-of-the-art
DOM-based approach?
RQ2(performance): What is the overhead and runtime of execut-
ing the visual approach compared to the DOM-based one?
RQ1aims to evaluate the e￿ectiveness of V￿￿￿￿ at detecting and
repairing test breakages, and how this varies across the breakage
classes. RQ 2aims to evaluate the overhead and running time of the
two main modules of V￿￿￿￿ : the Visual Execution Tracer and the
Visual Test Repair.
Object of Analysis. We used the same subjects and test suites
discussed in Section 3.2 , for which 733 breakages were identi￿ed.
Independent Variables. In our study, the visual-aided approach is
represented by V￿￿￿￿ . As a DOM-based approach we chose W￿￿￿￿ .
However, we have not adopted the implementation described in the
original paper [ 11]. In fact, there are fundamental design di￿erences
between the algorithms: W￿￿￿￿ is an o￿ine technique that runs a
test, collects information about the breakages, and runs the repair
as apost-processing procedure. Our algorithm, instead, is designed
to analyze the test suite at runtime, and to attempt repairs in an
online mode. Given that the scope of this paper is to compare the
e￿ectiveness at repairing breakages, we implemented a DOM-based
version of our approach by (i) customizing the Visual Execution
Tracer to collect DOM information and (ii) invoking W￿￿￿￿ ’s re-
pair heuristics (referred to as RepairLocators in [11]) within ourmain Algorithm 1 . For simplicity, however, in the evaluation section
we refer to the two competing tools as V￿￿￿￿ andW￿￿￿￿ .
Dependent Variables. To measure e￿ectiveness (RQ 1), we counted
the number of correct repairs as well as the amount of manual re-
pairs triggered by the two techniques on our benchmark. Regarding
e￿ciency (RQ 2), we utilize two metrics. First, we measured the over-
head imposed by the Visual Execution Tracer on the test suite to
create the visual traces. As a second metric, we counted the time
spent by each competing technique at repairing breakages.
Procedure. For each subject application, we applied W￿￿￿￿ and
V￿￿￿￿ to each test Tnin which a breakage was observed on the
successive release Vn+1. For each tool, and for each breakage, we
manually examined the ￿rst proposed repair repto determined its
correctness. If repwas found correct upon manual inspection, we
incremented the number of correct repairs. When both tools were
unable to repair, we incremented the number of manual repairs.
5.1 Results
E￿ectiveness (RQ 1).Table 2 presents the e￿ectiveness results.
For each subject, the table reports the number of breakages, and
the amount of correct repairs triggered by W￿￿￿￿ andV￿￿￿￿ both
numerically and percentage-wise. The results are further divided
into the various breakage classes. Totals across all applications are
also provided.
Overall, we can notice that W￿￿￿￿ was able to repair 420/733
breakages (57%), whereas V￿￿￿￿ found correct repairs for 592/733
breakages (81%). V￿￿￿￿ was hence able to correct 172 breakages
more than W￿￿￿￿ , a 41% increment.
Looking at the speci￿c breakage classes, each tool performed
well with respect to Same Page :W￿￿￿￿ repaired correctly 63%
of the times, whereas V￿￿￿￿ 84%. Concerning Neighbouring Page ,
no correct repairs were found by W￿￿￿￿ , while V￿￿￿￿ was 33%
successful. About elements being Removed , both tools performed
equally, detecting 14/21 cases (67%).
The main di￿erence between the two approaches emerges when
considering Mis-Selection cases: W￿￿￿￿ was never able to detect
any mis-selection of elements, whereas V￿￿￿￿ detected and repaired
correctly on average 80% of them (avoiding 94% of Direct breakages
and preventing 67% of Propagated ones).
Overall, the visual approach implemented by V￿￿￿￿ was con-
stantly superior to W￿￿￿￿ , over all the considered applications.
Individual improvements range from +28 (45-17) for AddressBook
to +69 (119-50) for Claroline.
Table 3 compares the two approaches further. The ￿rst row shows
the number of cases in which both tools were able to correct the
breakages. The second row, instead, illustrates the cases in which
V￿￿￿￿ prevailed, whereas the third row reports the cases that W￿￿￿￿
repaired correctly whereas V￿￿￿￿ did not. Lastly, the last fourth
row indicates the number of breakages that neither tool was able
to correct, and that were hence ￿xed manually.
In more than half of the cases (56%), both tools found the correct
repair. However, in 175 cases V￿￿￿￿ repaired a breakage that W￿￿￿￿
could not ￿x. Conversely, only in three cases W￿￿￿￿ prevailed
over the visual approach. Finally, 19% of breakages were repaired
manually because neither technique was able to ￿x them. We will
discuss those cases in Section 6 .ESEC/FSE ’18, November 4–9, 2018, Lake Buena Vista, FL, USA A. Stocco et al.
Table 2: Repair results across all applications and across all breakage classes
AddressBook Claroline Collabtive PPMA All AppsBreakages (#)
Rep. W￿￿￿￿ (#)
%
Rep. V￿￿￿￿ (#)
%
Breakages (#)
Rep. W￿￿￿￿ (#)
%
Rep. V￿￿￿￿ (#)
%
Breakages (#)
Rep. W￿￿￿￿ (#)
%
Rep. V￿￿￿￿ (#)
%
Breakages (#)
Rep. W￿￿￿￿ (#)
%
Rep. V￿￿￿￿ (#)
%
Breakages (#)
Rep. W￿￿￿￿ (#)
%
Rep. V￿￿￿￿ (#)
%
Non-Selection
Same Page 23 3 13 22 96 162 50 31 117 72 189 166 88 184 97 275 187 68 219 80 649 406 63 542 84
Neighbouring Page 1 0 0 1 100 0 0 0 0 0 11 0 0 10 91 21 0 0 0 0 33 0 0 11 33
Removed 14 14 100 14 100 0 0 0 0 0 3 0 0 0 0 4 0 0 0 0 21 14 67 14 67
Mis-Selection
Direct 8 0 0 8 100 3 0 0 2 67 7 0 0 7 100 0 0 0 0 0 18 0 0 17 94
Propagated 4 0 0 0 0 0 0 0 0 0 8 0 0 8 100 0 0 0 0 0 12 0 0 8 67
Total 50 17 344590 165 50 3011972 218 166 76209 96 300 187 6221973 733 420 5759281
Table 3: Comparison between DOM and Visual Repair Strate-
gies for all breakages, and amount of Manual Repairs.
AddressBook Claroline Collabtive PPMA All Apps
V￿￿￿￿ 4W￿￿￿￿ 4 17 47 166 187 417
V￿￿￿￿ 4W￿￿￿￿ 8 28 72 43 32 175
V￿￿￿￿ 8W￿￿￿￿ 4 03 0 0 3
V￿￿￿￿ 8W￿￿￿￿ 8 5 43 9 81 138
Performance (RQ 2).To assess the performance of running our
approach, we measured the execution time on a macOS machine,
equipped with a 2.3GHz Intel Core i7 and 16 GB of memory.
Table 4 (Trace Generation) shows the average time required to
run the test suites without (Column 2) and with (Column 3) the
Visual Execution Tracer (VET) module, and Column 4 reports the
relative slowdown. Overall, our technique imposed a 3.6 ⇥slow-
down compared to normal test suites execution (+38 minutes). The
biggest overhead occured for Collabtive (+19 minutes), whereas the
lowest occured for AddressBook (+3 minutes).
Concerning the execution time of our repair technique, Table 4
(Repair Time) shows the total running time, in seconds, for each
tool. To allow a fair comparison, we split the results between the
cases in which the tools were successful (Columns 5 and 6) and
unsuccessful (Columns 7 and 8).
In the former case, W￿￿￿￿ was overall 7% faster than V￿￿￿￿ ,
which employed nearly 4 minutes more. In the latter case, W￿￿￿￿
was 600% slower as compared to V￿￿￿￿ . Application-wise, W￿￿￿￿
was faster on AddressBook and Claroline (about -4 and -3 minutes,
respectively), but slower on Collabtive and PPMA (+17 s and +2.6
minutes, respectively). We will comment those results in Section 6 .
6 DISCUSSION
In this section, we discuss some of our ￿ndings, tool design decisions
and limitations, as well as threats to validity of our study.
Our empirical study con￿rmed the fragility of E2E web tests
when used for regression testing scenarios ( Section 3.2 ). Although
such tests did fail 94 times due to actual bugs, breakages (733) are
still a largely predominant issue a￿ecting test suite maintenance.Table 4: Performance results
Trace Generation Repair Time
seconds % correct (s) incorrect (s)
Original
With VET
Slowdown
W￿￿￿￿
V￿￿￿￿
W￿￿￿￿
V￿￿￿￿
AddressBook 50 207 4.1 ⇥ 127 388 594 59
Claroline 155 861 5.6 ⇥ 600 767 5,571 592
Collabtive 622 1,780 2.9 ⇥ 1,930 1,913 621 82
PPMA 228 458 2.0 ⇥ 802 646 2,972 659
Total 1,055 3,306 3.6 ⇥ 3,459 3,714 9,758 1,392
This is why repairing web tests is often considered a tedious activity
that leads the test suites to be abandoned [ 12]. This is certainly due
to the high frequency at which those tests break, but also due to
the dearth of solid tooling solutions in this area.
6.1 Explaining Results across Approaches
E￿ectiveness. Our evaluation revealed that our visual-based ap-
proach can successfully repair a large number of breakages cor-
rectly, outperforming an existing DOM-based solution.
We investigated why allbreakages were not repaired. We enu-
merate some of the main reasons next, which pertain both to the
inherent nature of our subjects and tests and some of the design
choices and limitations of the two competing approaches.
W￿￿￿￿ could not detect any Mis-Selection case because the mis-
selected element is among those retrieved by its heuristics. V￿￿￿￿ ,
on the contrary, does not trust a priori DOM-based locators and
validates them by means of their visual appearance.
For analogous reasons, the DOM-based repair strategy was inef-
fective when searching elements with local crawling ( Neighbouring
Page cases), whereas V￿￿￿￿ could detect nearly one third of them.
The failing cases were due to the retrieving of no elements (or wrong
ones). In PPMA, a challenging case occurred: the desired elementVisual Web Test Repair ESEC/FSE ’18, November 4–9, 2018, Lake Buena Vista, FL, USA
was hidden in a hoverable drop-down menu, whereas the crawler
does not detect such interaction patterns as it was instructed to
search in the neighbouring pages.
Concerning the Removed cases, both techniques failed seven
times because the crawler retrieved a wrong element. For the Same
Page category, both techniques performed quite well, however,
V￿￿￿￿ achieved a 33% improvement in the correct repair rate.
For the Claroline application, W￿￿￿￿ outperformed V￿￿￿￿ in
three cases. These breakages concerned buttons whose DOM at-
tributes were stable across the two considered releases whereas
their visual appearance did change substantially making our visual
technique ine￿ective.
Looking at the results on a per-application basis, AddressBook
and Collabtive tests experienced breakages pertaining to all classes.
On such applications, V￿￿￿￿ performed better than W￿￿￿￿ , with
165% and 11% improvements on the number of correct repairs,
respectively. For AddressBook, the DOM evolved quite frequently
across the releases. Thus, for a tester it would be challenging to ￿nd
reliable attributes upon which to create stable web element locators
(and hence robust test suites). Also the GUI of the web application
evolved, e.g., with changes in background colour and font size to
make it more appealing and user-friendly. However, our approach
demonstrated to be robust to both shifting and scaling (invariant
to translation and scale transformation).
Five mis-selection breakages could not be corrected by either
of the two approaches. Those cases refer again to buttons whose
tag changed (from <img> to<i>) as well as their visual appearance
(from buttons to icons).
In Collabtive, the high number of breakages is explained by the
numerous XPath locators used in the test suite, which are typi-
cally impacted by minor DOM changes. However, such DOM-level
modi￿cations did not jeopardize the e￿ectiveness of W￿￿￿￿ . Also
V￿￿￿￿ had its best best repair results because the GUI remained
quite stable across the analyzed releases.
For Claroline and PPMA, almost all repairs were found on the
Same Page , with V￿￿￿￿ being 134% and 17% more e￿ective than W￿￿
￿￿￿, respectively. Claroline and PPMA were also the applications
in which more manual repairs were needed. Claroline experienced
signi￿cant DOM and GUI evolutions, whereas for PPMA our tech-
nique failed due to timing issues (e.g., delays or page reloads that
were needed to display the web elements on the GUI).
Performance and Overhead. When the tools are successful, their
running time is comparable, with W￿￿￿￿ being on average slightly
faster than V￿￿￿￿ , across all breakages. However, looking at the
results in conjunction with the repair rate, suggests a di￿erent
picture. W￿￿￿￿ repaired 420 breakages correctly in 58 minutes
(8.2 s/repair), whereas V￿￿￿￿ repaired 592 breakages correctly in 62
minutes (6.3 s/repair). On the contrary, when the tools were unable
to repair, W￿￿￿￿ employed 139 minutes more than V￿￿￿￿ (31.2 vs
9.9 s/repair).
These low results are explained by two reasons: (1) if the DOM-
based heuristic fails to ￿nd the target element on the same state, the
local crawling is unnecessarily executed, and (2) one of W￿￿￿￿ ’s
heuristics searches for web elements having similar XPaths to the
target element’s. Being real world applications, our subjects have
web pages with signi￿cant DOM sizes. Thus, W￿￿￿￿ was often
forced to compute the similarity score on dozens of web elements,whereas V￿￿￿￿ required only two feature detection and one template
matching operations on images of reasonable sizes. This gives us
con￿dence in the applicability of our technique in common web
testing scenarios.
6.2 Estimated Manual E￿ort Saving
Our study revealed that 329/2,672 tests of our benchmark (12%)
experienced at least one breakage for which the test engineer must
￿nd an appropriate repair for. Manually inspecting the application
to ￿nd a correct ￿x and creating a locator for each broken statement
can be however a time-consuming task.
In a recent test repair work [ 23], authors report an average
manual repair time of 15 minutes/breakage. On our dataset, hypo-
thetically speaking, the estimated manual e￿ort reduction thanks
to our technique would be 148 hours (592·15
60).
Our own experience in repairing tests, gained during the em-
pirical study, corroborates the costliness of the task. For example,
in AddressBook, one test for the search functionality broke 11
times when applied from version 4.02 to version 4.1.1, due to three
non-selections ( Removed ) and seven mis-selections ( Direct ).V￿￿￿￿
created the dynamic visual execution trace of this test in 22 s, and
then found correct repairs for all 11 breakages in 57 s. Thus, in
this speci￿c case, our technique is advantageous only if the manual
detection and repair performed by a human tester takes more than
80 s (7 s/breakage), which seems likely infeasible in practice.
However, comparative experiments with human testers are nec-
essary to measure the costs associated with repairs. Before un-
dertaking such a study, we evaluated whether our technique has
prospect for success against a state-of-the-art repair algorithm.
6.3 Applications
V￿￿￿￿ can be used by test engineers to validate and repair their
E2E web test cases. Each statement of the tests is associated with
its visual trace. Hence, this information can also aid testers to
monitor the behaviour of the test suite across time and perform
more e￿ective root cause analysis .
Our technique can also play a role in automating software oracles .
A similar approach is implemented in tools such as Applitools [ 2],
where testers can manually inject visual checks at speci￿c places
of the test’s execution. This has two drawbacks: (1) the insertion of
the check-points must be performed manually, (2) this extra-code
clutters the initial test code, with statements that do not pertain
to the test scenario itself. On the contrary, our technique can be
introduced smoothly in existing testing infrastructures, as it neither
impacts nor modi￿es the source code of the test cases.
V￿￿￿￿ can be utilized as a runtime monitoring technique for the
detection of tests misbehaviours. Our study shows that V￿￿￿￿ goes
beyond simple detection and can e￿ciently suggest a high number
of correct repairs at runtime, while requiring a low overhead. Hence,
our approach can be utilized as an automated self-repair testing
technique, and can also be integrated with robust locator generation
algorithms [ 31,33,34].
Finally, our taxonomy can drive the design of automated test
repair techniques such as the one presented in this work.ESEC/FSE ’18, November 4–9, 2018, Lake Buena Vista, FL, USA A. Stocco et al.
6.4 Limitations
V￿￿￿￿ depends on the uniqueness of the templates used to identify
web elements. When feature detection fails, template matching is
not executed, thus undermining the applicability of our approach.
Concerning the local exploration, since crawling depth is limited
to one, we manage work￿ow repairs that pertain to the addition/re-
moval of one statement only. However, if two subsequent state-
ments needs to be removed, our technique does so by running the
crawler twice. Moreover, our implementation does not currently
support the creation of general purpose statements, such as the
ones that need input data, or the repair of assertion values.
6.5 Threats to Validity
We limited the bias of having produced test suites ourselves by
choosing existing test suites. This also ensures, to some extent,
that the chosen objects of analysis are non-trivial, therefore rep-
resentative of test suites that a real web tester would implement.
The manual evolution task poses a threat to validity that we tried
to mitigate by following a systematic approach in repairing the
breakages.
Concerning the generalizability of our results, we ran our ap-
proach with a limited number of subjects and test suites. However,
we believe the approach to be applicable in a general web testing
scenario (unless strict timing constraints apply), even though the
magnitude of the results might change. To limit biases in the manual
selection of the versions, we considered allthe available releases
after those for which the test suites were developed for.
Lastly, we constructed our taxonomy based on the analysis of
several hundreds of tests on real-world applications. However, we
do not claim that our taxonomy represents all possible breakage
scenarios. However, taxonomies typically evolve as additional ob-
servations are made.
7 RELATED WORK
Web and GUI Test Repair. We already discussed and evaluated
W￿￿￿￿ [11]. A recent work [ 23] adopted W￿￿￿￿ to repair the break-
ages happening to the intermediate minor versions between two
major releases of a web application.
Grechanik et al. [22] analyze an initial and modi￿ed GUI for dif-
ferences and generate a report for engineers documenting ways
in which test scripts may have broken. Zhang et al. [60] describe
F￿￿￿F￿￿￿￿ , a tool that repairs broken GUI work￿ows in desktop
applications. Memon [40] presents an event-based model of a GUI
which is used to determine the modi￿cations during software evo-
lutions. In similar fashion, Gao et al. [20] present SITAR , a model-
based repair algorithm of unusable GUI test scripts.
R￿A￿￿￿￿￿ is a tool to repair mainly assertions in unit tests for
Java applications [ 14–17].Huang et al. [25] describe a genetic al-
gorithm to automatically repair GUI test suites by evolving new
test cases that increase the test suite’s coverage while avoiding
infeasible sequences.
Di￿erently from the aforementioned works, our test repair tech-
nique uses a computer vision-based approach to ￿x classes of break-
ages speci￿c to the web testing domain that have not been reported
in general GUI desktop applications.Breakage Prevention. Recent papers have considered increasing
the robustness and maintainability of web test suites. In order to
make test scripts robust, several tools producing smart web element
locators have been proposed [ 3,31,33,34,59]. A study by Leotta et
al. [29] discusses the development and maintenance cost associated
with both DOM and visual test suites for web applications. Addi-
tionally, Stocco et al. [52–55] investigate the automated generation
of page objects that con￿ne causes of test breakages to a single
class, a form of breakage prevention.
Computer Vision to assist SE tasks. Recently, software engi-
neering community has witnessed an increasing adoption of CV
techniques to assist or solve common software engineering tasks.
One of the foundational works on computer vision applied to
testing is by Chang and colleagues. Their tool S￿￿￿￿￿ [10] allows
testers to write a visual test script that uses images to specify which
GUI components to interact with and what visual feedback to be ob-
served. Their work shows how this approach can facilitate a number
of testing activites such as unit testing, regression testing, and test-
driven development. On the same line, JA￿￿￿￿￿￿￿ [1] is a visual
record-and-replay tool for the testing of GUI-based applications.
CV techniques have been employed to detect cross-browser in-
compatibilities (XBIs). W￿￿S￿￿ [38] compares whole images with
a perceptual di￿erence technique, whereas W￿￿D￿￿￿ [49] and X￿
PERT [50] utilize an image similarity technique based on image
colour histogram matching.
The tool PESTO [32,35,51] migrates DOM-based web tests
to visual tests. It does so by using a standard template matching
algorithm for the automatic construction of visual locators, which
we used for the development of the Visual Execution Tracer.
Feng et al. [ 18] use a combination of visual analysis and NLP
techniques to assist the inspection of crowdsourced test reports.
Ramler and Ziebermayr [46] use visual test automation joint with
computer vision techniques to test physical properties of a mecha-
tronic system, whereas Kıraç and colleagues [ 27] used an image
processing pipeline for test oracle automation of visual output sys-
tems. Bajammal et al. [ 4] use visual methods generate reusable web
components from a mockup in order to facilitate UI web develop-
ment. In another work, Bajammal et al. [ 5] use visual analysis to
infer a DOM-like state of HTML canvas elements, thus making
them testable by commonly used testing approaches.
8 CONCLUSIONS AND FUTURE WORK
In this paper, we proposed V￿￿￿￿ , a novel web test repair technique
based on a fast image-processing pipeline. First, we used 2,672 tests
spanning 86 releases of four subject systems to collect a dataset of
733 individual test breakages, for which a repair-oriented taxonomy
has been presented. Then, we empirically evaluated our technique
at repairing these breakages. Overall, V￿￿￿￿ was able to repair
correctly on average 81% of such breakages, outperforming the
state-of-the-art solution, while incurring an acceptable overhead.
For future work, we plan to investigate the potential for hy-
bridization, i.e., joining DOM- and visual- heuristics in a single
solution. We also intend to run a controlled experiment with hu-
man subjects to measure the accuracy of the suggested repairs.
Lastly, we plan to experiment our technique with more subjects,
and study its capabilities to repair mobile test suites.Visual Web Test Repair ESEC/FSE ’18, November 4–9, 2018, Lake Buena Vista, FL, USA
REFERENCES
[1]E. Alégroth, M. Nass, and H. H. Olsson. 2013. JAutomate: A Tool for System- and
Acceptance-test Automation. In Proceedings of IEEE 6th International Conference
on Software Testing, Veri￿cation and Validation (ICST ’13) . 439–446.
[2]Applitools 2018. Applitools. Visual app testing and monitoring. https://applitools.
com/ . (2018). Accessed: 2017-08-01.
[3]Kartik Bajaj, Karthik Pattabiraman, and Ali Mesbah. 2015. Synthesizing Web
Element Locators. In Proceedings of 30th IEEE/ACM International Conference on
Automated Software Engineering (ASE ’15) . IEEE Computer Society, 331–341.
[4]Mohammad Bajammal, Davood Mazinanian, and Ali Mesbah. 2018. Generating
Reusable Web Components from Mockups. In Proceedings of 33rd IEEE/ACM
International Conference on Automated Software Engineering (ASE ’18) . IEEE
Computer Society.
[5]Mohammad Bajammal and Ali Mesbah. 2018. Web Canvas Testing through
Visual Inference. In Proceedings of 11th International Conference on Software
Testing, Veri￿cation and Validation (ICST ’18) . IEEE Computer Society, 193–203.
[6]Robert V. Binder. 1996. Testing object-oriented software: a survey. Software
Testing, Veri￿cation and Reliability 6, 3-4 (1996), 125–252.
[7]Kai Briechle and Uwe D Hanebeck. 2001. Template matching using fast normal-
ized cross correlation. In Optical Pattern Recognition XII , Vol. 4387. International
Society for Optics and Photonics, 95–103.
[8]Roberto Brunelli. 2009. Template Matching Techniques in Computer Vision: Theory
and Practice . Wiley Publishing.
[9]Gilles Burel and Dominique Carel. 1994. Detection and localization of faces on
digital images. Pattern Recognition Letters 15, 10 (1994), 963 – 967.
[10] Tsung-Hsiang Chang, Tom Yeh, and Robert C. Miller. 2010. GUI testing using
computer vision. In Proceedings of 28th ACM Conference on Human Factors in
Computing Systems (CHI ’10) . ACM, 1535–1544.
[11] Shauvik Roy Choudhary, Dan Zhao, Husayn Versee, and Alessandro Orso. 2011.
WATER: Web Application TEst Repair. In Proceedings of 1st International Workshop
on End-to-End Test Script Engineering (ETSE ’11) . ACM, 24–29.
[12] Laurent Christophe, Reinout Stevens, Coen De Roover, and Wolfgang De Meuter.
2014. Prevalence and Maintenance of Automated Functional Tests for Web Appli-
cations. In Proceedings of 30th International Conference on Software Maintenance
and Evolution (ICSME ’14) . IEEE, 141–150.
[13] Cloc 2018. Counts blank lines, comment lines, and physical lines of source code
in many programming languages. https://github.com/AlDanial/cloc . (2018).
[14] Brett Daniel, Danny Dig, Tihomir Gvero, Vilas Jagannath, Johnston Jiaa, Damion
Mitchell, Jurand Nogiec, Shin Hwei Tan, and Darko Marinov. 2011. ReAssert:
A Tool for Repairing Broken Unit Tests. In Proceedings of 33rd International
Conference on Software Engineering (ICSE ’11) . ACM, 1010–1012.
[15] Brett Daniel, Tihomir Gvero, and Darko Marinov. 2010. On Test Repair Using
Symbolic Execution. In Proceedings of 19th International Symposium on Software
Testing and Analysis (ISSTA ’10) . ACM, 207–218.
[16] Brett Daniel, Vilas Jagannath, Danny Dig, and Darko Marinov. 2009. ReAssert:
Suggesting Repairs for Broken Unit Tests. In Proceedings of 2009 IEEE/ACM
International Conference on Automated Software Engineering (ASE ’09) . IEEE
Computer Society, 433–444.
[17] Brett Daniel, Qingzhou Luo, Mehdi Mirzaaghaei, Danny Dig, Darko Marinov,
and Mauro Pezzè. 2011. Automated GUI Refactoring and Test Script Repair. In
Proceedings of First International Workshop on End-to-End Test Script Engineering
(ETSE ’11) . ACM, 38–41.
[18] Yang Feng, James A. Jones, Zhenyu Chen, and Chunrong Fang. 2016. Multi-
objective Test Report Prioritization Using Image Understanding. In Proceedings
of 31st IEEE/ACM International Conference on Automated Software Engineering
(ASE ’16) . ACM, 202–213.
[19] Mark Fewster and Dorothy Graham. 1999. Software Test Automation: E￿ective
Use of Test Execution Tools . Addison-Wesley Longman Publishing Co., Inc.
[20] Zebao Gao, Zhenyu Chen, Yunxiao Zou, and Atif M. Memon. 2016. SITAR: GUI
Test Script Repair. IEEE Transactions on Software Engineering 42, 2 (feb 2016),
170–186.
[21] Z. Gao, C. Fang, and A. M. Memon. 2015. Pushing the limits on automation in
GUI regression testing. In Proceedings of IEEE 26th International Symposium on
Software Reliability Engineering (ISSRE ’15) . 565–575.
[22] Mark Grechanik, Qing Xie, and Chen Fu. 2009. Maintaining and evolving GUI-
directed test scripts. In Proceedings of 31st International Conference on Software
Engineering (ICSE ’09) . IEEE Computer Society, 408–418.
[23] Mouna Hammoudi, Gregg Rothermel, and Andrea Stocco. 2016. WATERFALL:
An Incremental Approach for Repairing Record-replay Tests of Web Applications.
InProceedings of 24th ACM SIGSOFT International Symposium on Foundations of
Software Engineering (FSE ’16) . ACM, 751–762.
[24] Mouna Hammoudi, Gregg Rothermel, and Paolo Tonella. 2016. Why do Record/Re-
play Tests of Web Applications Break?. In Proceedings of 9th International Confer-
ence on Software Testing, Veri￿cation and Validation (ICST ’16) . IEEE, 180–190.
[25] Si Huang, Myra B. Cohen, and Atif M. Memon. 2010. Repairing GUI Test Suites
Using a Genetic Algorithm. In Proceedings of 3rd International Conference on
Software Testing, Veri￿cation and Validation (ICST ’10) . IEEE Computer Society,
245–254.[26] Ayman Issa, Jonathan Sillito, and Vahid Garousi. 2012. Visual Testing of Graphi-
cal User Interfaces: An Exploratory Study Towards Systematic De￿nitions and
Approaches. In Proceedings of IEEE 14th International Symposium on Web Systems
Evolution (WSE ’12) . IEEE Computer Society, 11–15.
[27] M. Furkan Kıraç, Barış Aktemur, and Hasan Sözer. 2018. VISOR: A fast image
processing pipeline with scaling and translation invariance for test oracle au-
tomation of visual output systems. Journal of Systems and Software 136 (2018),
266 – 277.
[28] V. Lelli, A. Blouin, and B. Baudry. 2015. Classifying and Qualifying GUI Defects.
InProceedings of 8th IEEE International Conference on Software Testing, Veri￿cation
and Validation (ICST ’15) . 1–10.
[29] Maurizio Leotta, Diego Clerissi, Filippo Ricca, and Paolo Tonella. 2014. Visual vs.
DOM-based Web Locators: An Empirical Study. In Proceedings of 14th Interna-
tional Conference on Web Engineering (ICWE ’14) , Vol. 8541. Springer, 322–340.
[30] Maurizio Leotta, Diego Clerissi, Filippo Ricca, and Paolo Tonella. 2016. Ap-
proaches and Tools for Automated End-to-End Web Testing. Advances in Com-
puters 101 (2016), 193–237.
[31] Maurizio Leotta, Andrea Stocco, Filippo Ricca, and Paolo Tonella. 2014. Reducing
Web Test Cases Aging by means of Robust XPath Locators. In Proceedings of 25th
International Symposium on Software Reliability Engineering Workshops (ISSREW
’14). IEEE Computer Society, 449–454.
[32] Maurizio Leotta, Andrea Stocco, Filippo Ricca, and Paolo Tonella. 2015. Au-
tomated Migration of DOM-based to Visual Web Tests. In Proceedings of 30th
Symposium on Applied Computing (SAC ’15) . ACM, 775–782.
[33] Maurizio Leotta, Andrea Stocco, Filippo Ricca, and Paolo Tonella. 2015. Using
Multi-Locators to Increase the Robustness of Web Test Cases. In Proceedings of
8th IEEE International Conference on Software Testing, Veri￿cation and Validation
(ICST ’15) . IEEE, 1–10.
[34] Maurizio Leotta, Andrea Stocco, Filippo Ricca, and Paolo Tonella. 2016. ROBULA+:
An Algorithm for Generating Robust XPath Locators for Web Testing. Journal of
Software: Evolution and Process (2016), 28:177–204.
[35] Maurizio Leotta, Andrea Stocco, Filippo Ricca, and Paolo Tonella. 2018. PESTO:
Automated migration of DOM-based Web tests towards the visual approach.
Software Testing, Veri￿cation And Reliability 28, 4 (2018).
[36] D. G. Lowe. 1999. Object recognition from local scale-invariant features. In
Proceedings of 7th IEEE International Conference on Computer Vision , Vol. 2. 1150–
1157.
[37] David G. Lowe. 2004. Distinctive Image Features from Scale-Invariant Keypoints.
International Journal of Computer Vision 60, 2 (01 Nov 2004), 91–110.
[38] S. Mahajan and W. G. J. Halfond. 2015. Detection and Localization of HTML
Presentation Failures Using Computer Vision-Based Techniques. In Proceedings of
8th IEEE International Conference on Software Testing, Veri￿cation and Validation
(ICST ’15) . 1–10.
[39] Tomasz Malisiewicz, Abhinav Gupta, and Alexei A. Efros. 2011. Ensemble of
exemplar-SVMs for Object Detection and Beyond. In Proceedings of 2011 In-
ternational Conference on Computer Vision (ICCV ’11) . IEEE Computer Society,
89–96.
[40] Atif M. Memon. 2008. Automatically Repairing Event Sequence-based GUI Test
Suites for Regression Testing. ACM Transactions on Software Engineering and
Methodologies 18, 2, Article 4 (nov 2008), 36 pages.
[41] Ali Mesbah, Arie van Deursen, and Stefan Lenselink. 2012. Crawling Ajax-based
Web Applications through Dynamic Analysis of User Interface State Changes.
ACM Transactions on the Web 6, 1 (2012), 3:1–3:30.
[42] Ali Mesbah, Arie van Deursen, and Danny Roest. 2012. Invariant-based Automatic
Testing of Modern Web Applications. IEEE Transactions on Software Engineering
38, 1 (2012), 35–53.
[43] Bao N. Nguyen, Bryan Robbins, Ishan Banerjee, and Atif Memon. 2014. GUITAR:
an innovative tool for automated testing of GUI-driven software. Automated
Software Engineering 21, 1 (2014), 65–105.
[44] OpenCV 2018. Open Source Computer Vision Library. https://opencv.org . (2018).
[45] Rudolf Ramler and Klaus Wolfmaier. 2006. Economic Perspectives in Test Au-
tomation: Balancing Automated and Manual Testing with Opportunity Cost. In
Proceedings of 1st International Workshop on Automation of Software Test (AST
’06). ACM, 85–91.
[46] R. Ramler and T. Ziebermayr. 2017. What You See Is What You Test - Augmenting
Software Testing with Computer Vision. In Proceedings of 10th IEEE International
Conference on Software Testing, Veri￿cation and Validation Workshops (ICSTW
2017) . 398–400.
[47] Edward Rosten and Tom Drummond. 2005. Fusing points and lines for high
performance tracking. In IEEE International Conference on Computer Vision , Vol. 2.
1508–1511.
[48] Edward Rosten, Reid Porter, and Tom Drummond. 2010. FASTER and better:
A machine learning approach to corner detection. IEEE Transaction on Pattern
Analysis and Machine Intelligence 32 (2010), 105–119.
[49] Shauvik Roy Choudhary, Mukul R. Prasad, and Alessandro Orso. 2013. X-PERT:
Accurate Identi￿cation of Cross-browser Issues in Web Applications. In Proceed-
ings of 2013 International Conference on Software Engineering (ICSE ’13) . IEEE
Press, 702–711.ESEC/FSE ’18, November 4–9, 2018, Lake Buena Vista, FL, USA A. Stocco et al.
[50] Shauvik Roy Choudhary, Husayn Versee, and Alessandro Orso. 2010. WEBDIFF:
Automated Identi￿cation of Cross-browser Issues in Web Applications. In Pro-
ceedings of 2010 IEEE International Conference on Software Maintenance (ICSM
’10). IEEE Computer Society, 1–10.
[51] Andrea Stocco, Maurizio Leotta, Filippo Ricca, and Paolo Tonella. 2014. PESTO:
A Tool for Migrating DOM-based to Visual Web Tests. In Proceedings of 14th
International Working Conference on Source Code Analysis and Manipulation
(SCAM ’14) . IEEE Computer Society, 65–70.
[52] Andrea Stocco, Maurizio Leotta, Filippo Ricca, and Paolo Tonella. 2015. Why
Creating Web Page Objects Manually If It Can Be Done Automatically?. In Pro-
ceedings of 10th IEEE/ACM International Workshop on Automation of Software Test
(AST ’15) . IEEE/ACM, 70–74.
[53] Andrea Stocco, Maurizio Leotta, Filippo Ricca, and Paolo Tonella. 2016. Auto-
matic Page Object Generation with APOGEN. In Proceedings of 16th International
Conference on Web Engineering (ICWE ’16 - Demo Track) . Springer, 533–537.
[54] Andrea Stocco, Maurizio Leotta, Filippo Ricca, and Paolo Tonella. 2016. Clustering-
Aided Page Object Generation for Web Testing. In Proceedings of 16th International
Conference on Web Engineering (ICWE ’16) . Springer, 132–151.[55] Andrea Stocco, Maurizio Leotta, Filippo Ricca, and Paolo Tonella. 2017. APOGEN:
Automatic Page Object Generator for Web Testing. Software Quality Journal 25,
3 (Sept. 2017), 1007–1039.
[56] Andrea Stocco, Rahulkrishna Yandrapally, and Ali Mesbah. 2018. Web Test Repair
Using Computer Vision. In Proceedings of 26th ACM Joint European Software
Engineering Conference and Symposium on the Foundations of Software Engineering
(ESEC/FSE 2018 - Demonstration Track) . ACM.
[57] Paolo Tonella, Filippo Ricca, and Alessandro Marchetto. 2014. Recent Advances
in Web Testing. Advances in Computers 93 (2014), 1–51.
[58] V￿￿￿￿ 2018. Web Test Repair using Computer Vision. https://github.com/saltlab/
vista . (2018).
[59] Rahulkrishna Yandrapally, Suresh Thummalapenta, Saurabh Sinha, and Satish
Chandra. 2014. Robust Test Automation Using Contextual Clues. In Proceedings
of 2014 International Symposium on Software Testing and Analysis (ISSTA ’14) .
ACM, 304–314.
[60] Sai Zhang, Hao Lü, and Michael D. Ernst. 2013. Automatically Repairing Broken
Work￿ows for Evolving GUI Applications. In Proceedings of 2013 International
Symposium on Software Testing and Analysis (ISSTA ’13) . ACM, 45–55.