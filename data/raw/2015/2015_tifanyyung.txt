Comparing and Combining Test-Suite Reduction and
Regression Test Selection
August Shi, Tifany Yung, Alex Gyori, Darko Marinov
Department of Computer Science
University of Illinois at Urbana-Champaign, Urbana, IL 618 01, USA
{awshi2,yung4,gyori,marinov}@illinois.edu
ABSTRACT
Regression testingis widelyused tocheckthatchanges made
to software do not break existing functionality, but regres -
sion test suites grow, and running them fully can become
costly. Researchers have proposed test-suite reduction an d
regression test selection as two approaches to reduce this
cost by not running some of the tests from the test suite.
However, previous research has not empirically evaluated
how the two approaches compare to each other, and how
well a combination of these approaches performs.
We present the ﬁrst extensive study that compares test-
suite reduction and regression test selection approaches i n-
dividually, and also evaluates a combination of the two ap-
proaches. We also propose a new criterion to measure the
quality of tests with respect to software changes. Our exper -
iments on 4,793 commits from 17 open-source projects show
that regression test selection runs on average fewer tests ( by
40.15pp) than test-suite reduction. However, test-suite r e-
duction can have a high loss in fault-detection capability
with respect to the changes, whereas a (safe) regression tes t
selection has noloss. The experiments also show thata com-
bination of the two approaches runs even fewer tests (on av-
erage 5.34pp) than regression test selection, but these tes ts
still have a loss in fault-detection capability with respec t to
the changes.
Categories and Subject Descriptors: D.2.5 [Software
Engineering ]: Testing and Debugging
General Terms: Experimentation, Measurement
Keywords: Regression testing, Test-suite reduction, Re-
gression test selection
1. INTRODUCTION
Regression testing is important but time consuming. Re-
gression testing is widely used to check that changes to a
software system do not break existing functionality [23,33 ].
Modern software evolves fast, with changes pushed to a
repository even several times per minute [6,7,9], trigger-ing regression-test runs. Additionally, regression test s uites
grow and often become costly to run, with previous research
reporting test suites taking even weeks to run [26]. Even
Google, a corporation with a lot of computing resources, re-
ported that running all tests is prohibitive, with a quadrat ic
growth in the test execution over time [29].
Previous research has proposed two approaches to speed
upregression testing: test-suite reduction andregression test
selection . Both approaches focus on runningonly some tests
from the full test suite; we call these tests chosen-to-run
tests. Test-suite reduction [33] aims to (permanently) re-
move from the full test suite those tests that are redundant
with respect to some testing requirements, commonly with
respect to statement or branch coverage. The analysis is
performed once on a single revision of software, and the re-
duced test suite is used in subsequent revisions. Although
running the reduced test suite can miss some faults that the
full test suite detects, it may still be tolerable in some con -
texts to get faster test results; the full test suite could be run
more rarely [13,18,34]. Regression test selection [33] aim s
to select from the full test suite those tests that are releva nt
to the changes made to the software. The analysis is per-
formed on every revision of software to determine how the
changes between revisions aﬀect the tests. Regression test
selection is safeif it selects all tests that maybe aﬀected
by the changes, guaranteeing that the outcome of the tests
that are not selected will not be aﬀected by the changes.
When test engineers wish to speed up regression test-
ing, it is unclear which approach is better to use: test-
suite reduction or regression test selection. To the best
of our knowledge, there is no prior empirical comparison
of test-suite reduction and regression test selection. Eac h
approach has some pros and cons. Regression test selec-
tion, when safe, selects all tests that could detect faults d ue
to the changes, while test-suite reduction can remove some
tests that could detect such faults, because test-suite red uc-
tion is not evolution-aware (i.e., it removes tests that are
deemed redundant for the current revision irrespective of
future changes [27]). Test-suite reduction has a low (amor-
tized) analysis cost because it is performed on one revision ,
while regression test selection incurs an additional analy sis
cost for every revision. However, test engineers care about
the total testing time that includes not only the analysis
time but also the time to execute the chosen-to-run tests; it
is unclear a priori which approach has lower total time.
To compare test-suite reduction and regression test se-
lection, we would ideally measure how much time each ap-
proach takes and how many real faults it ﬁnds in the con-
This is the author’s version of the work. It is posted here for your personal use. Not for
redistribution. The deﬁnitive version was published in the following publication:
ESEC/FSE’15 , August 30 – September 4, 2015, Bergamo, Italy
c2015 ACM. 978-1-4503-3675-8/15/08...
http://dx.doi.org/10.1145/2786805.2786878
237text of software evolution. Because measuring total time
is challenging, most research on regression testing measur es
the number of chosen-to-run tests as a proxy for time [33].
We also measure the number of chosen-to-run tests to deter-
mine whethertest-suite reductionor regression test selec tion
yields a smaller number. Because using real faults is chal-
lenging, mostresearch uses mutantsas aproxy[17,22,27,37 ].
We also use mutants, but we have an additional challenge to
measure the quality of the chosen-to-run tests with respect
to the changes .
We introduce a new criterion to measure the loss of fault-
detection capability of test suites with respect to changes .
We refer to the faults that are related to the changes as
change-related faults. Intuitively, these faults can be (1) di-
rectly in the code parts that changed between two revisions
or (2)latentfaults that are in the unchanged code parts but
exposed by the changes. Simply reusing a criterion for eval-
uating a test suite on an entire given program revision (i.e. ,
measuring the fault-detection capability of the test suite on
the code revision obtained after the changes) could be mis-
leading because it can ﬁnd faults unrelated to the changes.
For example, consider two test suites, T1that broadly cov-
ers the entire code and T2that covers less code overall but
focuses on one module M. If the faults are randomly seeded
throughout the code, T1can be better as it detects a higher
percentage of faults than T2. However, if the change-related
faults aﬀect the module Mon which T2focuses, then T2can
be better than T1at detecting these change-related faults.
We propose a new test criterion, called Change-Related Re-
quirements (CRR) , that approximates the change-related
faults a test suite needs to ﬁnd. We use this criterion to
measure the quality of a test suite and to compare test-suite
reduction and regression test selection.
To further speed up testing, we can combine the two ap-
proaches: ﬁrst perform test-suite reduction on the full tes t
suite to create a smaller, reduced test suite, and then per-
form regression test selection on this reduced test suite af ter
the changes, yielding an even smaller number of tests than
selecting from the full test suite. Combining the two ap-
proaches provides the greatest reduction in the number of
tests. We call this combination selection of reduction . We
empirically evaluate how much it reduces the size of the test
suite to run. Considering the quality of the tests selected b y
regression test selection from the reduced test suite, assu m-
ing safe selection, the change-related fault-detection ca pa-
bility of the tests selected from the reduced test suite is as
good as the change-related fault-detection capability of t he
reduced test suite.
We perform an empirical evaluation on 17 open-source
Java projects over a total of 4,793 revisions to answer the
following three questions:
RQ1:Does test-suite reduction or regression test selec-
tion yield a smaller number of tests to run on average?
RQ2:What is the change-related loss in quality of the
reduced test suite, based on our CRR criterion?
RQ3:How does selection of reduction compare with the
other two approaches in terms of number of tests to run?
2. BACKGROUND
We review some background on test-suite reduction and
regression test selection, and including how their eﬀectiv e-
ness is traditionally evaluated.2.1 Test-Suite Reduction
Test-suite reduction aims to remove redundant tests from
a test suite, creating a reduced test suite that is a subset
of the full test suite; ideally, a developer could run fewer
tests yet still detect almost the same faults as if running th e
full test suite. Test-suite reduction techniques ﬁnd redun -
dant tests with respect to some testing requirements, com-
monlydeﬁnedbycodecoveragecriteria, suchasstatementor
branch coverage. A test is redundant with respect to a test
suite if the test satisﬁes only the requirements already sat -
isﬁed by some other tests in the test suite. For a set of tests
T, we write req(T) for the set of requirements satisﬁed by T.
Given a full test suite O, test-suite reduction constructs a
reduced test suite R ⊆ Osuch that req(O) =req(R). After
obtaining this reduced test suite R, a developer would use
it in lieu of the full test suite Ofor testing the software as
it evolves.
Previous research on test-suite reduction [24,25,28, 30,
31,33,37] measured the eﬀectiveness of a reduced test suite
by two metrics, the reduction of size and the loss in fault-
detectioncapability, bothincomparisontothefulltestsu ite.
For a reduced test suite Rconstructed from the full test
suiteO, the size reduction is measured by 100 × |R|/|O|;
a smaller value is better as it means fewer tests need to
be run. The loss in fault-detection capability is measured
using a proxy for the number of faults detected, often the
number of mutants killed, but sometimes also the number
of statements covered or other requirements satisﬁed [27].
Withreq(T) denoting the set of requirements satisﬁed by
the set of tests T, the loss in fault-detection capability is
measured by 100 × |req(O)\req(R)|/|req(O)|; a smaller
value is better as the number of requirements satisﬁed by
the full test suite but not satisﬁed by the reduced test suite
is smaller relative to the set of all requirements satisﬁed.
Note that diﬀerent kinds of requirements should be used for
thereductionandevaluation; otherwise, usingthesame kin d
would trivially produce a loss of 0.
2.2 Regression Test Selection
Unliketest-suitereductiontechniquesthatoperateononl y
onesoftware revision, regression testselectiontechniqu esop-
erate on two (or more [16]) software revisions. The aim of
regression test selection is to run only the tests that may
be aﬀected by the changes made between the two revisions.
Regression test selection techniques are often designed to be
safe with respect to the changes, i.e., a test is not selected
to be run only if the software changes cannot aﬀect the out-
come of the test [23].
Given a full test suite Oiat revision i, regression test se-
lection selects torunasubset of thesetests Si,∆⊆ Oi, where
∆ represents all the other inputs that regression test selec -
tion takes, including most importantly the changes between
revision iand the previous revision (typically revision i−1,
except for branching version histories [16]) but also the co v-
erage matrix from the previous revision. The eﬀectiveness
of regression test selection is measured by the ratio of num-
ber of tests selected to run over the total number of tests,
100×|Si,∆|/|Oi|; a smaller value is better.
Developersuseregression testselection tospeeduptestin g
with a focus on detecting change-related faults. Commonly,
a developer starts from a software revision where all tests
in the test suite pass, and after making some changes, the
developer wants regression test selection to select (all) t he
238tests that could fail, because those tests ﬁnd new faults. In
the general case, however, a developer can start from a soft-
ware revision where some tests in the test suite fail, and
after making some changes, the developer can use regres-
sion test selection in two scenarios: (1) select all previou sly
failing tests even if they are not aﬀected by the change, re-
minding the developer that these failing tests are due to
faults not ﬁxed by the changes, or (2) select only the tests
that are aﬀected by the changes, and if a previously pass-
ing test now fails, it is due to a change-related fault. (Note
that a change-related fault is not necessarily introduced i n
the changed code; instead, the change can be correct by
itself but could lead the execution to some previously exist -
ing, latent fault that was not executed before.) Our CRR
criterion (Section 3.2) captures the second scenario, wher e
a developer is only concerned with the new faults related
to the developer’s own changes and ignores other, existing
faults in the code (possibly introduced by other developers )
that are unrelated to whatever change the developer made.
3. METHODOLOGY
This section describes our methodology for comparing the
eﬀectiveness of test-suite reduction, regression test sel ection,
and their combination. We ﬁrst describe how we evolve re-
duced test suites across multiple software revisions. We th en
deﬁne our CRR criterion that evaluates the change-related
quality of a test suite for a change at a given revision. We
ﬁnally describe our proposed combination of the two ap-
proaches that applies regression test selection after havi ng
performed test-suite reductionon an earlier revision.
3.1 Evolving Reduced Test Suite
Test-suite reduction constructs a reduced test suite on a
singlerevision, so evaluating the eﬀectiveness of a reduced
test suite as software evolves requires evolving the reduce d
test suite into each subsequent revision. Ideally, we would
evaluate test-suite reductionbyapplyingit on some softwa re
revision and then consulting with developers on how the re-
duced test suite would actually evolve into the future. How-
ever, performing such an experiment with real developers is
rather hard, so we instead simulate, from the actual project
history, how the reduced test suite could have evolved had
test-suite reduction been applied at some revision.
In general, we need a procedure that takes a reduced test
suite from one revision and outputs the“evolved”test suite
at a later revision. In our previous work [27], we considered
the evolution of the reduced test suite into a future revisio n
by (1) tracking tests based on name and (2) only considering
the tests that existed across all revisions, which ignores t he
tests added (or removed) after performing reduction. This
procedure captures how the tests from the reduced test suite
behave in the future, but it ignores new (andremoved) tests.
We modify our previous procedure to also consider the
new (and removed) tests. New tests are often introduced to
cover new functionality; if developers were to use test-sui te
reduction, we assume they would augment the reduced test
suite with these new tests as their software evolves with new
functionality. Removed tests, on the other hand, may cover
functionality that has been removed, or may even be re-
dundant tests manually identiﬁed, so we assume developers
would remove these tests from the reduced test suite as well
(if they still exist in the reduced test suite). In other word s,
the reduced test suite, used to replace the full test suite,should go through similar changes as the full test suite went
through while the software evolved. We denote by Rithe
reduced test suite constructed at revision ifrom the full test
suiteOi. To evolve Rito a later revision i′, we consider
any new and removed tests between OiandOi′, where the
tests are identiﬁedbyname. (Amore precise approach could
track tests not only by name but by the semantics, inferring
likely renames [11].)
Definition 1.The reduced test suite computed at revi-
sioniand evolved to subsequent revision i′is the evolved
reduced test suite: Ei,i′= (Ri∩Oi′)∪(Oi′\Oi)
3.2 Change-Related Requirements (CRR)
Our goal is to compare the quality of the subsets of tests
chosen-to-run by test-suite reduction and regression test se-
lection, which requires an appropriate metric. Because tes ts
are run to detect faults, the ideal measure would be by the
faults that the tests detect. In regression testing, develo pers
focus on code changes and care most about change-related
faults(eithernewlyintroducedfaultsorlatentfaultsthatare
exposed by tests only after the change). The tests chosen-
to-run by test-suite reduction and regression test selecti on
should ideally detect all the change-related faults that th e
full test suite detects.
Ideally,wewouldliketomeasureonlychange-relatedfault s
and not all faults that a test suite detects. Given a set of
testsT ⊆ O i′from revision i′that are chosen-to-run after
a change ∆, let faults(T) represent the set of faults de-
tected by T. Intuitively, change-related faults are the faults
fromfaults(T) that are not detected before the change ∆
is applied. While the intuition can be clear, it is challeng-
ing to give an automatic procedure to precisely determine
change-related faults, because it is challenging to (1) map
each test failure to fault(s), and (2) map faults across soft -
ware revisions (e.g., consider a case where a function with
a fault is inlined in several callers, and a test fails in one
of those callers). For this reason, we approximate the set
of change-related faults with the set of faults detected onl y
by the tests aﬀected by the change ∆, as found using a
safe regression test selection technique, and not detected by
tests not aﬀected, i.e., the set of change-related faults is
faults(Si′,∆∩T)\faults(T \Si′,∆), where Si′,∆is the set
of tests selected from Oi′. This formula can overapproxi-
mate change-related faults when it includes faults that are
only detected by the selected tests but unrelated to ∆.
It is also challenging to build a dataset of real test suites
that can detect real regression faults, because we need not
only one test written to detect each regression fault after i t
had been already ﬁxed [8,20] but an entire test suite that
could have detected the fault. For this reason, we measure,
for each test, what test requirements—statements covered
and killed mutants—it satisﬁes instead of measuring what
real faults it detects.
We deﬁne a new criterion, which we call Change-Related
Requirements (CRR), that speciﬁes the change-related re-
quirements that a test suite satisﬁes:
Definition 2.The set of change-related requirements sat-
isﬁed by Tis:CRRSi′,∆(T)=req(Si′,∆∩T)\req(T \Si′,∆)
Here,req(T) is used in the broadest sense to denote any
requirements, e.g., a set of statements covered or a set of
(seeded or real) faults detected.
239R1R2R3R4R5
t1 ✓ ✓
t2 ✓ ✓
[t3]✓ ✓
[t4] ✓ ✓ ✓
Figure 1: Matrix for requirements (R)satisﬁed by
tests(t). Red color marks change-related require-
ments and tests selected by regression test selec-
tion. Brackets mark tests in the evolved reduced
test suite.
As a simple example to demonstrate CRR, consider the
matrix in Figure 1 that shows what requirements each test
satisﬁes. Inthisexample, onlytherequirement R3ischange-
related, Si′,∆={t1,t4}andEi,i′={t3,t4}. For test-suite
reduction, the loss in quality of a reduced test suite is typi -
cally measured by 100 ×|req(O)\req(R)|/|req(O)|, where
req(T)denotesallrequirementsthatatestsuitesatisﬁes, in-
cludingthose thatare notchange-related. Inthisexample, if
we use all requirements that all four tests satisfy, both Si′,∆
andEi,i′would need to satisfy all ﬁve requirements, and
we see that both Si′,∆andEi,i′fail to satisfy some require-
ments. However, R3is the only change-related requirement,
and CRR should consider only such requirements.
If regression test selection is safe, then the requirements
satisﬁed by the tests chosen-to-run by regression test sele c-
tion,Si′,∆, satisfy all change-related requirements (and po-
tentially some more requirements). We see in our example
thatSi′,∆does indeed satisfy R3. We want to measure how
many change-related requirements a reduced test suite sat-
isﬁes. If we required it to satisfy all requirements satisﬁe d
bySi′,∆, those would include R2,R4andR5as well, which
are not change-related, hence including these extra requir e-
ments could lead to inaccurate results. Indeed, Ei,i′does not
satisfyR4despite satisfying the change-related requirement
R3, so using req(Si′,∆) would inaccurately report a quality
loss forEi,i′. However, usingourproposedcriterion, CRR,in
this example, ﬁlters out the requirements not change-relat ed
by removing the requirements satisﬁed by the non-selected
testst2andt3. We see that CRRSi′,∆(Oi′) does not include
R2,R4, andR5, but only includes R3. Therefore, in this
example, a set of tests only needs to satisfy R3to have no
quality loss, which is the exact change-related requiremen t.
3.3 Quality of Reduced Test Suites
We useCRRSi′,∆(Ei,i′) as the set of requirements that an
evolved reduced test suite Ei,i′(initially reduced at revision
iand evolved to revision i′) satisﬁes with respect to the
changes ∆. We apply the traditional metric for measuring
loss in quality by substituting CRR in lieu of requirements:
Definition 3.Given the full test suite Oi′, the selected
testsSi′,∆, and the evolved reduced test suite Ei,i′, the change-
related loss in quality of Ei,i′is:
CRRLoss i,i′,∆=100×|CRRSi′,∆(Oi′)\CRRSi′,∆(Ei,i′)|
|CRRSi′,∆(Oi′)|
In the example from Figure 1, we determine the change-
related requirements for the full test suite to be just R3, i.e.,
CRRSi′,∆(Oi′) ={R3}. Because CRRSi′,∆(Ei,i′) ={R3,R5}
has all the change-related requirements, CRRLoss i,i′,∆is 0.3.4 Selection of Reduction
We compare the eﬀectiveness of test-suite reduction and
regression test selection with each other because they both
run a subset of tests from the full test suite. However, the
two approaches are orthogonal: test-suite reduction remov es
redundant tests from a single revision, while regression te st
selection considers changes between two revisions to selec t
tests to run. The two approaches can be combined. After
test-suite reduction constructs Rifor revision i,Rican be
used as a replacement for Oi, and its evolved form Ei,i′can
be used for all future revisions i′in lieu of Oi′. Regression
test selection can then select tests, aﬀected by the changes
between revision i′and its predecessor, from Ei,i′instead of
fromOi′. Basically, to get selection of reduction, regression
test selection would select tests from a reduced test suite.
For a revision i′, givenSi′,∆andEi,i′, the tests selected
from the evolved reduced test suite are Si′,∆∩Ei,i′.
4. EV ALUATION
We next describe our evaluation of test-suite reduction,
regression test selection, and their combination on 17 open -
source projects from GitHub [2]. We ﬁrst summarize the
projects used in our study and then describe how we set
up our experiments to answer the following three research
questions:
RQ1:Does test-suite reduction or regression test selec-
tion yield a smaller number of tests to run on average?
RQ2:What is the change-related loss in quality of the
reduced test suite, based on our CRR criterion?
RQ3:How does selection of reduction compare with the
other two approaches in terms of number of tests to run?
4.1 Projects
Figure 2lists the17projects thatwe use in ourevaluation.
We cloned from GitHub Java projects that use Maven [3] to
build and JUnit [19] to run tests. We use 15 projects from
our prior study of test-suite reduction [27]; we do not use
three projects from the prior study: one does not build any
more, one has issues with running tests on many commits,
and one does not work with the regression test selection
tool Ekstazi [15]. We also include two new projects: Square
Retroﬁtand ApachePDFBox. For each project, we tabulate
the number of commits used in our evaluation; the starting
SHA (the commit ID in Git); the minimum, median, and
maximum number of lines of code among the commits used
in theevaluation (measured usingSLOCCount [5]); themin-
imum, median, andmaximumnumberoftests inthefull test
suite among the commits used in the evaluation; and the
time it takes to run the test suite (using mvn test ) on the
latest revision we evaluated on. We conducted the experi-
ments on a 2.33GHz Intel Core2 Quad machine with 16GB
of RAM, running Ubuntu 14.04.
4.2 Experimental Setup
For each project, we perform regression test selection for
the range of commits speciﬁed in Figure 2. We use the state-
of-the-art regression test selection tool Ekstazi [1,14] t o ﬁnd
selected tests for the changes between each commit and its
predecessor. Ekstazi selects the tests based on ﬁle-level de-
pendencies , i.e., if a ﬁle changed between two commits, Ek-
stazi selects every test that depends on that ﬁle. Moreover,
Ekstazi selects at the level of test classes, i.e., if any one test
240LOC Tests
IDProject Commits Start SHA Min Median Max Min Median Max Time
P1Commons-Lang 5099ad1a4df 65621 67798 69952 2312 24912609 42.546s
P2Caelum Stella 30939e50b7f 21374 29418 38153 659 73780119.582s
P3Caelum V |raptor 165443cf0ed 33972 33972 34743 1052 11131147 21.554s
P4Cloudfoundry UAA 20976cb4b8c 18059 22794 39051 312 38785830.927s
P5Dropwizard 2706bf66144 20395 20690 20690 274 31342930.149s
P6Scribe-Java 2140222f08f 5498 5652 5652 51 79944.182s
P7SQL-Parser 251a1ddf59b 46655 46910 46923 49 200431 9.812s
P8JodaTime 25607002501 91448 91482 91482 3866 40004136 13.106s
P9AssertJ-Core 348df1adedd 55467 64091 76468 4534 52816139 23.259s
P10MessagePack 34039c7fa7f 6129 6129 6131 814991532 4.009s
P11JOPT-Simple 153e4c251d6 9078 9655 9658 562 691786 8.255s
P12SLF4J 335cfd6bdba 14146 14146 14146 63 11414111.864s
P13Jasmine 1808d9121ab 11523 11523 11523 34 12614734.401s
P14Square Wire 1813cad6d0c 14366 14549 14549 23 6210611.188s
P15LA4J 4377bd10910 7807 8911 8911 245 6111646 6.741s
P16Square Retroﬁt 3354f3798e6 4641 8096 9923 101 18325030.206s
P17Apache PDFBox 3017ﬀﬀ962 101922 110937 115718 626 6471014112.464s
Figure 2: Statistics of projects used in our experiments
from a test class depends on a changed ﬁle, Ekstazi selects
the entire test class with all of its tests.
To measure test quality, we use covered statements and
killed mutants as the requirements that the tests must sat-
isfy. We use the mutation testing tool PIT [4] to map each
test from the full test suite to the statements covered and
killed mutants. Due to the high cost of mutation testing, we
donotmapthisfor everycommit butonlyevery30commits,
starting from the starting commit speciﬁed in Figure 2. At
every commit where we record the satisﬁed requirements, we
also perform test-suite reduction to generate a reduced tes t
suite at that commit. We constructed the reduced test suite
by applying the Greedy algorithm [10] on the statements
covered. From each reduced test suite, we then create an
evolved reduced test suite in each subsequent commit; to
perform this evolution, we use the procedure described in
Section 3.1.
4.3 RQ1: Comparing Number of Tests to Run
To answer RQ1 (and RQ3), we measure how many tests
would be chosen-to-run by test-suite reduction and regres-
sion test selection (and their combination) across the rang e
of commits for each project. To visualize how the num-
ber of tests changes from commit to commit, we show the
number as a line plot. Figure 3 shows these plots for three
projects: Commons-Lang, JodaTime, and LA4J. In each
plot, the lines show the numberof tests chosen-to-runby the
diﬀerent approaches at each commit. The greentsline (at
the top) represents the total number of tests for each com-
mit, the redtsline (in the middle) represents the number
of tests in the evolved reduced test suite (where test-suite
reduction is applied at the starting SHA speciﬁed in the cap-
tion), the bluetsline (with bigger“zig-zags”) represents the
number of tests selected by regression test selection, and t he
orangetsline (with smaller “zig-zags”) represents the num-
ber of tests selected by regression test selection from the
evolved reduced test suite.
We use these three projects because they show interesting
varying behaviors. For Commons-Lang, regression test se-
lection is relatively small, and for most commits, the num-
ber of tests selected by regression test selection is smalle rthan the number of tests in the evolved reduced test suite.
However, for JodaTime, we see many commits throughout
the history where regression test selection would have se-
lected many more tests than the evolved reduced test suite.
Commons-Lang and JodaTime are both rather mature, and
the growth of the test suite seems relatively small and sta-
ble, i.e., the number of tests in the full test suite does not
change substantially across the commits.
In contrast, for LA4J, the test suite changes greatly as
the software evolves. Because of that, we show two plots for
LA4J, which also highlight how we apply test-suite reduc-
tion at various points in our evaluation. In Figure 3c, we
apply test-suite reduction on the starting SHA and evolve
the reduced test suite across the entire range of commits
used in the evaluation. In Figure 3d, we apply test-suite
reduction on a later SHA and evolve it until the end of the
range. In both cases we see that regression test selection
frequently selects more tests than the evolved reduced test
suite. We also see that reapplying test-suite reduction on a
later commit reduces the number of tests chosen-to-run by
test-suite reduction. For example, from the starting point
showninFigure 3d(the241st commit from thestartingSHA
we evaluate on), the evolved reduced test suite in Figure 3c
had grown to 486 tests, whereas rerunning test-suite reduc-
tion at that point creates a smaller reduced test suite that
now has 115 tests.
We do not show detailed plots for the other projects and
starting points, because they are similar in shape to the
four plots shown and would take a lot of space; we instead
summarize their results. For each project and each starting
point, we collect for each commit the ratio of the number
of tests chosen-to-run by each approach over the number
of tests in the full test suite in that commit. Figure 4a
shows the distribution of these ratios as violin plots. For
each project, we show three violin plots, one for each ap-
proach: the redts(leftmost) for the evolved reduced test
suite, the bluets(middle) for the tests selected by regres-
sion test selection, andtheorangets(rightmost) for thetests
selected by regression test selection on the evolved reduce d
test suite. Each violin plot shows the minimum, median
(horizontal line), mean (black dot), and maximum values,
2410 100 200 300 400 500
Commits050010001500200025003000Number of Tests to Run
(a) Commons-Lang reduced starting from 9ad1a4df0 50 100 150 200 250
Commits050010001500200025003000350040004500Number of Tests to Run
(b) JodaTime reduced starting from 07002501
0 50 100 150 200 250 300 350 400
Commits0100200300400500600700800900Number of Tests to RunTotal Tests
Regression Test Selection
Test-Suite Reduction
Combined
(c) LA4J reduced starting from 7bd109100 20 40 60 80 100 120 140 160
Commits0100200300400500600700800900Number of Tests to Run
(d) LA4J reduced starting from c8e61571
Figure 3: Tests chosen-to-run across multiple commits for s elect projects
and the width of the plot depends on the number of points
for each value.
We ﬁnd that, across the range of commits we evaluated
for each project, regression test selection would have, on
average, run fewer tests than test-suite reduction . (As ex-
pected, the number of tests selected by combining the two
approaches is even smaller than the number of tests chosen-
to-run by either approach.) In particular, the median ratio
of tests for evolved reduced test suite is higher than the me-
dian ratio of tests for regression test selection for all pro jects
but three (LA4J, MessagePack, and SQL-Parser). In other
words, the diﬀerence in the median ratio of tests for evolved
reducedtestsuiteandthemedianratiooftestsfor regressi on
test selection is positive for all projects but three. The me an
of these diﬀerences shows that the evolved reduced test suit e
size is 40.15pp1higher than the number of tests selected by
regression test selection. However, we note that the num-
ber of tests selected by regression test selection across th e
commits has a much wider distribution than test-suite re-
duction, i.e., one cannot easily predict the number of tests
when using regression test selection.
We also note that regression test selection relatively of-
ten selects no tests (because no relevant change was made
1The“pp”(“percentage point”) unit shows the diﬀerence be-
tween two percentages by subtracting one from the other.between commits that would aﬀect any of the tests). The
average values for regression test selection could be skewe d
by these zero cases, and in theory, a developer may realize
that no relevant change is made and could manually choose
to not run any tests even from a reduced test suite. To
explore this further, we plot in Figure 4b the number of
tests selected by regression test selection only for the com -
mits when a non-zero number of tests is selected. We ﬁnd
that there are now four projects (JOPT-Simple in addition
to LA4J, MessagePack, and SQL-Parser) where regression
test selection on average selects more tests than the evolve d
reduced test suite. Also, the median evolved reduced test
suite size is now on average only 22.61pp higher than the
median number of tests selected by regression test selectio n
across all projects.
Finally, utilizingtheWilcoxonsignedranktesttocompare
the distributions of the numbers of tests chosen-to-run by
test-suite reduction and regression test selection yields p-
values signiﬁcant at an α-level of 0.001, indicating that there
is a high probability that the two distributions diﬀer.
RQ1:In sum, for both scenarios (with and without
zero cases), regression test selection selects to run, on
average, fewer tests than the evolved reduced test suite.
242P1 P2 P3 P4 P5 P6 P7 P8 P9 P10 P11 P12 P13 P14 P15 P16 P17
Project020406080100Size (%)Reduction/All
Selection/AllSelection of Reduction/All
(a) Sizes for all commits (including zeroes for selection)
P1 P2 P3 P4 P5 P6 P7 P8 P9 P10 P11 P12 P13 P14 P15 P16 P17
Project020406080100Size (%)Reduction/All
Selection/AllSelection of Reduction/All
(b) Sizes for commits where selection was not zero
Figure 4: Violin plots showing distribution of sizes for red uction, selection, and selection of reduction
Thisisasurprisingresult, becauseregression testselect ion
aims to be safe and selects all tests that could be aﬀected by
the changes (and potentially selects many more tests given
thecoarse, imprecise level of ﬁles atwhich Ekstazi operate s),
whereas test-suite reduction is unsafe and could miss some
test that is aﬀected by the changes.
4.4 RQ2: Comparing Quality of Tests
Toevaluatehowmuchtest-suitereductioncouldmiss, and
to answer RQ2, we measure the quality of the chosen-to-run
tests using two kinds of requirements—statement coverage
and killed mutants—tocompute CRR (Deﬁnition2) and the
quality loss (Deﬁnition 3). We evaluate the quality loss at
each point where we collected the mapping from the tests to
the satisﬁed requirements, by default every 30 commits. For
cases where regression test selection did not select any tes ts
(e.g., betweencommits 29and30), wewentbackonecommit
at atime(to28, 27, andsoon)andaddedinall testsselected
byregression testselection duetothechanges inearlier co m-
mits until the selected tests satisfy non-zero requirement s.
Going back simulates havinghada bigger change (i.e., in the
Git terminology, as if the developers squashed several com-
mits), because the tests selected by regression test select ion
reﬂect the changes between a wider range of commits.
Before we summarize the results for quality loss across all
of the projects and various commit points, we show more
detailed results for one project. Figure 5 shows, for LA4J,
how the change-related statement coverage and killed mu-
tants of evolved reduced test suites compare to the corre-
sponding metrics for the test suites selected by regression
test selection, as measured using CRR. Each column shows,
for the speciﬁed commit (every 30 commits from the begin-
ning), the percentage of change-related statements covere d
(Figure 5a) or killed mutants (Figure 5b) that are missed byV1V2V3V4V5V6V7V8
V01.901.151.110.001.730.001.441.45
V1 0.000.000.000.490.000.410.41
V2 0.370.000.820.000.720.79
V3 0.000.490.000.410.41
V4 0.910.000.760.76
V5 0.000.210.41
V6 0.890.90
V7 0.21
(a) Loss in change-related statement coverage (%)
V1V2V3V4V5V6V7V8
V03.432.532.430.002.770.002.252.25
V1 1.001.020.001.410.001.181.17
V2 1.810.002.110.001.641.61
V3 0.001.800.001.381.37
V4 3.210.002.212.21
V5 0.002.732.97
V6 9.289.48
V7 9.68
(b) Loss in change-related killed mutants (%)
Figure 5: Change-related loss in quality of evolved
reduced test suite, with reduced test suite con-
structed at Vi(iis row) and evaluated at Vj(jis
column) for LA4J; distance between row/column is
about 30 commits.
the evolved reduced test suite for that commit. Each row
represents the evaluation of an evolved reduced test suite
starting from a diﬀerent commit: the ﬁrst row is for the
evolved reduced test suite computed at the starting SHA,
and each subsequent row starts from a later commit (typ-
ically 30 commits later). We note that for LA4J, for the
most part, there is a very small loss in change-related kille d
mutants of the evolved reduced test suite, although there
are several points where the loss in killed mutants is higher ,
going up to 9.68%.
243P1 P2 P3P4P5 P6 P7P8 P9 P10 P11 P12 P13P14P15 P16 P17
Project0246810Statement Coverage (%)Loss in changed Loss in all
(a) Statement coverage
P1 P2 P3P4P5 P6 P7P8 P9 P10 P11 P12 P13P14P15 P16 P17
Project05101520Killed Mutants (%)Loss in changed Loss in all
(b) Killed mutants
Figure 6: Violin plots showing distribution of loss in
quality due to reduction for diﬀerent projects
We do not show detailed tables for the other projects and
starting points, but we instead summarize their results. Fi g-
ures 6a and 6b show, for each project, a lavenderts(left-
most) violin plot representing the distribution of the loss in
CRR, of both statement coverage and killed mutants, re-
spectively. To contrast CRR, we also show for each project
a magentats(rightmost) violin plot for the traditional met-
ric of loss for the evolved reduced test suite, where the loss
is measured with respect to allrequirements satisﬁed by the
full test suite: 100 ×|req(O)\req(R)|/|req(O)|.
We see that the change-related loss in statement cover-
age typically has low absolute values, with the median for a
project being at most 2.74% (for JOPT-Simple). The values
for the traditional metric are also fairly similar, but reca ll
that change-related loss is the metric that matters for evol v-
ing code. In contrast, the loss in killed mutants has higher
absolute values across all projects, with the median value o f
loss being as high as 5.93% (for JOPT-Simple) These higher
values mean that, after applying test-suite reduction, the
resulting evolved reduced test suite at each commit could
miss detecting many change-related faults. This is the peri l
of usingtest-suitereduction. Comparedtothefull testsui te,
test-suite reduction makes the test-suite smaller but can
miss some faults that the full test suite can ﬁnd. This was
well understoodfrom previousstudies [24,25,28,30,31,33 ,37]
that considered all faults, but we focus on change-related
faults and compare with regression test selection.
RQ2:Test-suite reduction can miss on median up
to 2.74% change-related statements and 5.93% change-
related mutants that regression test selection ﬁnds, while
(safe) regression test selection cannot miss any change-
related fault that the full test suite can ﬁnd.
4.5 RQ3: Evaluating Selection of Reduction
Despite the peril of test-suite reduction, test engineers
may still use it to speed up regression testing; one can then
apply regression test selection on the reduced test suite to
speed up regression testing even more. To answer RQ3, we
evaluate how eﬀective the combined selection of reduction i s
compared to individual test-suite reduction and regressio n
test selection. As a comparison metric, we use the number
of tests that selection of reduction selects to run at eachcommit for a project. Note that we do not need to em-
pirically evaluate the loss in change-related fault-detec tion
capability of selection of reduction: because selection of re-
duction selects from the reduced test suite all the tests tha t
are aﬀected by the changes, assuming a safe regression test
selection, the change-related loss of selection of reducti on is
the same as the change-related loss of test-suite reduction .
Figure 4 shows the violin plots for the number of tests se-
lected by selection of reduction, making it easy to compare
the numbers for the combined approach to the two individ-
ual approaches. The distribution across all projects shows
that, as expected, selection of reduction always selects fe wer
tests to run than any of the other two approaches. How-
ever, as selection of reduction performs selection on top of
the reduced test suite, it is not known a priori how the ra-
tio of tests selected by regression test selection from the f ull
test suite compares to the ratio of tests selected by selecti on
of reduction from the reduced test suite. Figure 7a shows
the distributions of these two ratios as violin plots for eac h
project. As regression test selection often selects no test s,
Figure 7b plots only the non-zero numbers of tests selected
by both regression test selection and selection of reductio n.
Utilizing the Wilcoxon signed rank test to compare the
ratios of tests selected from the full test suite to the ratio
of tests selected from the reduced test suite yields p-value s
signiﬁcant at the level of 0.001 for all projects except Clou d-
foundry UAA (with a p-value of 0.091) and Square Retroﬁt
(with a p-value of 0.293). With the exception of Square
Retroﬁt, there is a high probability that the distributions of
the ratio of tests selected by regression test selection fro m
the full test suite and from the reduced test suite diﬀer.
Note, however, that the mean (taken over all projects)
pairwise diﬀerence between the ratios of the two distribu-
tions is only 0.72pp. Therefore, although the Wilcoxon sta-
tistical test establishes a high probability that the distr ibu-
tions of these ratios diﬀer for each project (except Square
Retroﬁt), their medians do not diﬀer greatly.
RQ3:Selection of reduction selects fewer tests than
either test-suite reduction or regression test selection,
and the ratio of tests that selection of reduction selects
from the evolved reduced test suite is about the same
as the ratio of tests that regression test selection selects
from the full test suite.
4.6 Threats to Validity
External: Our conclusions may not generalize beyond
the projects and revisions we evaluated. To mitigate this
issue, we chose actively developed projects from GitHub,
varying in size, number of tests, length of history, and appl i-
cation domain. Many of these projects were also previously
used to study the eﬀects of software evolution on test-suite
reduction [27].
Internal: We automated the process of recording what
tests are selected by the Ekstazi tool at each commit in
each project used in our evaluation. We also implemented
the Greedy algorithm to perform test-suite reduction. We
increased the conﬁdence in our code through many small
experiments and peer code review.
Construct: We deﬁne a new metric, CRR, to measure
change-related quality of a reduced test suite. To deter-
mine the change-related requirements to be satisﬁed, we ef-
244P1 P2 P3 P4 P5 P6 P7 P8 P9 P10 P11 P12 P13 P14 P15 P16 P17
Project020406080100Size (%)Selection/All Selection of Reduction/Reduction
(a) Ratios for all commits (including zeroes for selection)
P1 P2 P3 P4 P5 P6 P7 P8 P9 P10 P11 P12 P13 P14 P15 P16 P17
Project020406080100Size (%)Selection/All Selection of Reduction/Reduction
(b) Ratios for commits where selection was not zero
Figure 7: Violin plots showing selection/all and selection of reduction/reduction
fectively assume thatregression test selection is notonly safe
but also relatively precise. We need regression test select ion
to be safe to get all of the change-related requirements, and
to be precise to help ﬁlter out the requirements not aﬀected
by change but accidentally satisﬁed by the tests selected by
regression test selection. If regression test selection se lects
tests that are not aﬀected by the changes (in the extreme
selecting all tests), then we cannot ﬁlter out the accidenta lly
satisﬁed requirements.
The regression test selection tool we use, Ekstazi, tracks
dependencies at the level of ﬁles (including classes for cod e)
and is fairly safe (with respect to code changes). While
Ekstazi is not as precise as tools that track ﬁner-grained
dependenciesthanﬁles [35], Ekstazi is safer thanthosetoo ls,
and safety is more important to properly determine which
requirements are change-related. Also, Ekstazi is publicl y
available [1]. Ekstazi collects dependencies at the level o f
test classes not test methods: if a single test (method) is
aﬀected by a change, its entire test class is selected to be
run. In contrast, our test-suite reduction is at the level of
individual tests, which allows removing each test based on
the requirements it satisﬁes. Hence, there is adiscrepancy in
the level of the chosen-to-run tests for the two approaches.
Using Ekstazi, our results for regression test selection ma y
select more tests than actually necessary, so our compariso n
of test-suite sizes for test-suite reduction and regressio n test
selection overestimates the sizes for regression test sele ction.
Therefore, one of our key ﬁndings, that the size of selected
test suites is, on average, smaller than the size of reduced
test suite, could be even stronger.
We evolved a reduced test suite across many commits us-
ing a particular procedure (Section 3.1) to simulate how a
test engineer who used a reduced test suite in lieu of the fulltest suite may have evolved the reduced test suite in future
commits. This simulation allows a comparison to regres-
sion test selection, which directly considers changes, inc lud-
ing those that modify the test suite, adding or removing
tests. Speciﬁcally, our procedure for evolving the reduced
test suite considers all changes made to the full test suite
and adds all new tests. However, had the developer actually
performed test-suite reduction, the reduced test suite cou ld
have evolved diﬀerently than the way we simulated it. Fur-
thermore, we track tests between revisions by name only, so
if a test is renamed, that test would be considered a newly
added test. Not handling renames can aﬀect the results if
a test that was originally considered redundant (and there-
fore excluded from the reduced test suite) was renamed in
a subsequent revision: that test would be considered a new
test and“re-added”to the reduced test suite, possibly intr o-
ducing the same redundancy back in.
4.7 Discussion
Change-Related Requirements: Automatically eval-
uating the fault-detection capability of test suites with re-
spect to software changes is challenging. We deﬁne CRR
(Deﬁnition 2) by contrasting the requirements satisﬁed by
tests selected byregression test selection totherequirem ents
satisﬁed by tests not selected. While this deﬁnition is not
ideal, it is better than some alternatives. For example, we
could have considered only the requirements directly in the
changed code, e.g., ﬁnding changed lines between two revi-
sions and inserting mutants only on those lines. However,
that would favor regression test selection over test-suite re-
duction and miss some requirements that are aﬀected by
the changed code but not directly in the changed code. If a
fault is in some unchanged part of code, it is possible that a
change elsewhere in the code could trigger the detection of
245this fault. For instance, consider a fault that is control de -
pendent on some branch condition, and a code change that
aﬀects the branch condition. If we ignore whether a test
suite can ﬁnd this fault, we could incorrectly label two test
suites as equally good even when one ﬁnds this fault and
the other does not ﬁnd it. Future work should seek a better
research methodology to evaluate quality of test suites wit h
respect to changes.
Reduction of Selection: For research evaluation pur-
poses, one could consider performing test-suite reduction on
test suites selected byregression test selection, andthis com-
bined approach—called reduction of selection —could yield
diﬀerent results than selection of reduction. However, re-
duction of selection is impractical for speeding up regress ion
testing, because performing (precise) reduction requires ﬁrst
runningall thetestsonthecurrentrevision toconstructar e-
quirementsmatrixusedtodetermineredundanttests. Thus,
one could only determine which tests not to run after actu-
ally running those tests. An alternative could be to use a
stale requirements matrix computed on some previous revi-
sion to reduce the selected test suite, but this requirement s
matrixwouldbeimprecisefor thecurrentrevisionandwould
become more imprecise as some tests would not be run due
to the reduction. Moreover, new requirements could have
been introduced due to a change, and reduction on a stale
matrix, being unaware of these new requirements, cannot
guarantee that the test suite reduced from the selected test
suite would satisfy all the requirements, which goes agains t
the purpose of (precise) test-suite reduction. Future work
could evaluate such imprecise test-suite reduction and com -
pare it with unsafe regression test selection.
5. RELATED WORK
There is a large body of work on test-suite reduction
and regression test selection, but to the best of our knowl-
edge, there was no prior work on comparing and combining
those two approaches for code. Korel et al. [21] propose a
technique that combines test-suite reduction and regressi on
test selection, which (1) is similar to reduction of selecti on,
but diﬀers from our selection of reduction, and (2) oper-
ates on models, speciﬁcally, extended ﬁnite state machines
(EFSMs), unlike our combination that operates on real code
and tests. Given two EFSMs, their technique ﬁrst computes
a set of elementary changes, then uses a static dependence
analysis to ﬁnd which test covers what changes, and ﬁnally
selects to run only tests that are non-redundantwith respec t
to the changes. In contrast, our evaluation does not assume
any static analysis and measures the change-related loss in
quality for the reduced test suite.
There are also studies of the eﬀects of software evolution
on test information. Elbaum et al. [12] studied the eﬀects on
test coverage and found that coverage information changed
greatly even for small software changes. We recently studie d
the eﬀects of software evolution on test-suite reduction [2 7].
We conducted experiments on 18 open-source projects and
measured the quality of the constructed reduced test suite
across multiple commits. We found that the quality of the
reduced test suite does not drop much as software evolves,
comparing the reduced test suite to the full test suite us-
ing requirements of the entire software. The current study
focuses on measuring the test quality using change-related
requirements, and we ﬁnd that the reduced test suite can
suﬀer some greater loss in quality.There is prior work on combining various regression test-
ing approaches. For example, Zhang et al. [36] combined
test-suite prioritization and test-suite reduction for mu ta-
tion testing. We are the ﬁrst to combine test-suite reductio n
and regression test selection.
There is also other work on speedingupregression testing,
e.g., using history of test runs to perform unsafe test selec -
tion on large industrial codebases. Most recently, Herzig
et al. [18] proposed a technique that selects tests based on
historical data about test results. Their technique balanc es
the cost of running a test many times in certain develop-
ment branches versus running the test just once before re-
lease at the risk of it revealing a fault that requires much
more time to debug. Their simulation of the technique on
Microsoft products found a reduction of 50% of test execu-
tions. Elbaum et al. [13] combine unsafe test selection in
the pre-commit stage with test prioritization in the post-
commit stage, and evaluate their technique at Google. Yoo
et al. [34] propose a related technique using multi-objecti ve
sampling [32] based on coverage, cost, and fault history, an d
also evaluate their techniqueat Google. We evaluate our ap-
proaches onsmall open-sourceprojects butexpectthatsome
of the key ﬁndings would carry over to large codebases.
6. CONCLUSIONS
This paper is theﬁrst toempirically compare and combine
test-suite reduction and regression test selection, two ap -
proaches that can speed up regression testing but were eval-
uated only separately. We propose a new criterion, Change-
Related Requirements, to evaluate the quality of running a
set of tests with respect to the changes made in software.
We also use test-suite size to evaluate chosen-to-run tests .
Our results on 17 open-source projects show three interest-
ing conclusions. First, regression test selection on avera ge
selectsfewertests than test-suite reduction. Second, test-
suite reduction can lose change-related fault-detection c a-
pability (of up to 5.93% for killed mutants), while (safe)
regression test selection has no loss. Third, our proposed s e-
lectionofreductionapproachprovidesthegreatest speed- up,
though with the same loss in change-related fault-detectio n
capability as the reduced test suite, and has about the same
selection ratio from the reduced test suite as regression te st
selection has from the full test suite.
In summary, our results show that if only one approach
must be chosen, either test-suite reduction or regression t est
selection, to speed up regression testing, the test enginee r
should choose regression test selection, as it selects fewe r
tests andpreserves change-related fault-detection capab ility,
where the faults are most likely to appear. If there is a need
to speed up testing even further, then combining both test-
suite reduction and regression test selection is a worthwhi le
approach that provides even greater savings in the number
of tests as long as one is willing to tolerate the possible los s
in fault-detection capability for some changes in software .
7. ACKNOWLEDGMENTS
Wewould like tothankMilos Gligoric for his helpwith the
Ekstazi tool and the anonymous reviewers for feedback on a
previous draft of this paper. This research was partially su p-
ported bythe NSF Grant Nos. CCF-1012759, CCF-1421503,
CCF-1434590, and CCF-1439957. Alex Gyori was partially
supported by the Saburo Muroga Endowed Fellowship.
2468. REFERENCES
[1] Ekstazi. http://www.ekstazi.org/ .
[2] GitHub. https://github.com/ .
[3] Maven. http://maven.apache.org/ .
[4] PIT mutation testing. http://pitest.org/ .
[5] SLOCCount. http://www.dwheeler.com/sloccount/ .
[6] A. Alali, H. Kagdi, and J. I. Maletic. What’s a typical
commit? A characterization of open source software
repositories. In ICPC, 2008.
[7] C. Bird, P. C. Rigby, E. T. Barr, D. J. Hamilton,
D. M. German, and P. Devanbu. The promises and
perils of mining Git. In MSR, 2009.
[8] M. B ¨ohme and A. Roychoudhury. CoREBench:
Studying complexity of regression errors. In ISSTA,
2014.
[9] C. Brindescu, M. Codoban, S. Shmarkatiuk, and
D. Dig. How do centralized and distributed version
control systems impact software changes? In ICSE,
2014.
[10] T. Y. Chen and M. F. Lau. A simulation study on
some heuristics for test suite reduction. IST, 40(13),
1998.
[11] D. Dig, C. Comertoglu, D. Marinov, and R. Johnson.
Automated detection of refactorings in evolving
components. In ECOOP, 2006.
[12] S. Elbaum, D. Gable, and G. Rothermel. The impact
of software evolution on code coverage information. In
ICSM, 2001.
[13] S. Elbaum, G. Rothermel, and J. Penix. Techniques
for improving regression testing in continuous
integration development environments. In FSE, 2014.
[14] M. Gligoric, L. Eloussi, and D. Marinov. Ekstazi:
Lightweight test selection. In ICSE, 2015.
[15] M. Gligoric, L. Eloussi, and D. Marinov. Practical
regression test selection with dynamic ﬁle
dependencies. In ISSTA, 2015.
[16] M. Gligoric, R. Majumdar, R. Sharma, L. Eloussi, and
D. Marinov. Regression test selection for distributed
software histories. In CAV, 2014.
[17] D. Hao, L. Zhang, X. Wu, H. Mei, and G. Rothermel.
On-demand test suite reduction. In ICSE, 2012.
[18] K. Herzig, M. Greiler, J. Czerwonka, and B. Murphy.
The art of testing less without sacriﬁcing quality. In
ICSE, 2015.
[19] JUnit home page.
https://github.com/kentbeck/junit/wiki .
[20] R. Just, D. Jalali, and M. D. Ernst. Defects4J: A
database of existing faults to enable controlled testing
studies for Java programs. In ISSTA, 2014.
[21] B. Korel, L. Tahat, and B. Vaysburg. Model based
regression test reduction using dependence analysis. InICSM, 2002.
[22] A. J. Oﬀutt, J. Pan, and J. M. Voas. Procedures for
reducing the size of coverage-based test sets. In
ICTCS, 1995.
[23] G. Rothermel and M. J. Harrold. A safe, eﬃcient
regression test selection technique. TOSEM, 6(2),
1997.
[24] G. Rothermel, M. J. Harrold, J. Ostrin, and C. Hong.
An empirical study of the eﬀects of minimization on
the fault detection capabilities of test suites. In ICSM,
1998.
[25] G. Rothermel, M. J. Harrold, J. von Ronne, and
C. Hong. Empirical studies of test-suite reduction.
STVR, 12(4), 2002.
[26] G. Rothermel, R. H. Untch, C. Chu, and M. J.
Harrold. Test case prioritization: An empirical study.
InICSM, 1999.
[27] A. Shi, A. Gyori, M. Gligoric, A. Zaytsev, and
D. Marinov. Balancing trade-oﬀs in test-suite
reduction. In FSE, 2014.
[28] S. Tallam and N. Gupta. A concept analysis inspired
greedy algorithm for test suite minimization. In
PASTE, 2005.
[29] Testing at the speed and scale of Google, Jun 2011.
http://goo.gl/2B5cyl .
[30] W. E. Wong, J. R. Horgan, S. London, and A. P.
Mathur. Eﬀect of test set minimization on fault
detection eﬀectiveness. In ICSE, 1995.
[31] W. E. Wong, J. R. Horgan, A. P. Mathur, and
A. Pasquini. Test set size minimization and fault
detection eﬀectiveness: A case study in a space
application. In COMPSAC , 1997.
[32] S. Yoo and M. Harman. Pareto eﬃcient
multi-objective test case selection. In ISSTA, 2007.
[33] S. Yoo and M. Harman. Regression testing
minimization, selection and prioritization: A survey.
STVR, 22(2), 2012.
[34] S. Yoo, R. Nilsson, and M. Harman. Faster fault
ﬁnding at Google using multi objective regression test
optimisation. In ESEC/FSE , 2011.
[35] L. Zhang, M. Kim, and S. Khurshid. FaultTracer: A
change impact and regression fault analysis tool for
evolving Java programs. In SIGSOFT FSE , 2012.
[36] L. Zhang, D. Marinov, and S. Khurshid. Faster
mutation testing inspired by test prioritization and
reduction. In ISSTA, 2013.
[37] L. Zhang, D. Marinov, L. Zhang, and S. Khurshid. An
empirical study of JUnit test-suite reduction. In
ISSRE, 2011.
247