Feedback-DirectedDifferentialTesting ofInteractiveDebuggers
Daniel Lehmann
Departmentof ComputerScience
TUDarmstadt, Germany
mail@dlehmann.euMichael Pradel
Departmentof ComputerScience
TUDarmstadt, Germany
michael@binaervarianz.de
ABSTRACT
To understand, localize, and fix programming errors, developers
oftenrelyoninteractivedebuggers.However,asdebuggersaresoft-
ware, they may themselves have bugs, which can make debugging
unnecessarilyhard or evencause developers to reasonabout bugs
that do not actually exist in their code. This paper presents the
first automated testing technique for interactive debuggers. The
problemoftestingdebuggersisfundamentallydifferentfromthe
well-studiedproblemoftestingcompilersbecausedebuggersarein-
teractiveandbecausetheylackaspecificationofexpectedbehavior.
Ourapproach,calledDBDB,generatesdebuggeractionstoexercise
the debugger and records traces that summarize the debugger’s be-
havior. By comparing traces of multiple debuggers with each other,
we finddiverging behavior that points to bugs and other notewor-
thy differences. We evaluate DBDB on the JavaScript debuggers
ofFirefoxandChromium,finding19previouslyunreportedbugs,
eightofwhichare already fixedbythe developers.
CCS CONCEPTS
·Software and its engineering →Software testing and de-
bugging;Softwaremaintenance tools ;
KEYWORDS
Interactive debuggers, Differential testing,JavaScript
ACMReference Format:
Daniel Lehmann and Michael Pradel. 2018. Feedback-Directed Differential
TestingofInteractiveDebuggers. In Proceedingsofthe26thACMJointEuro-
peanSoftwareEngineeringConferenceandSymposiumontheFoundations
of Software Engineering (ESEC/FSE ’18), November 4ś9, 2018, Lake Buena
Vista, FL, USA. ACM, New York, NY, USA, 11pages.https://doi.org/10.1145/
3236024.3236037
1 INTRODUCTION
Interactivedebuggersareapowerfultooltofindandcorrectbugsin
programs.Unlikemuchsimplermethods,suchas printf-debugging,
interactive debuggers allow the developer to directly follow the
programatruntime.Inparticular,onecanpausetheexecutionat
pointsofinterestthrough breakpoints ,closelyexaminecontrol-flow
throughstepping,andinspect intermediateprogramstate ,suchas
the callstack andthe valuesofvariables.
Permissionto make digitalor hard copies of allorpart ofthis work for personalor
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthefirstpage.Copyrights forcomponentsofthisworkownedbyothersthanthe
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/or a fee. Request permissions from permissions@acm.org.
ESEC/FSE ’18, November 4ś9, 2018, Lake BuenaVista,FL,USA
©2018 Copyright heldby the owner/author(s). Publicationrightslicensed to ACM.
ACM ISBN 978-1-4503-5573-5/18/11...$15.00
https://doi.org/10.1145/3236024.3236037
Figure 1: JavaScript debugger of Firefox 54 incorrectly
pausesatabreakpointindeadcode.BugfoundwithDBDB.
As debuggers are a crucial tool in the development workflow,
theyobviouslyshouldbecorrect.Figure 1demonstratesthatthis
isnotalwaysthecase,evenwhendebuggingaseeminglysimple
program:Sincetheif-conditionevaluatesto false,thestatement
at line 2 is never executed and nothing gets written to the console.
Yet, when a breakpoint is set at line 2 in the Firefox debugger, it
pauses, whichgives the impressionthat deadcode isexecuted.1
Suchbugsindebuggersareveryconfusingbecausetheycanlead
developers to believe their code is wrong even though it is correct.
Evenworse,bugsindebuggerscanmakeithardorevenimpossible
to understand actual bugs, e.g., when a developer cannot set a
breakpointatavalidcodelocation,2whenvariablesareshownwith
wrongvalues,3orwhenthe debuggerobscuresthe actualcontrol-
flow by not pausing where it should.4Even when a debugger is
not as blatantly wrong as shown in Figure 1, there are often subtle
differencesbetweentwodebuggerimplementationsforthesame
programming language. Such differencesare equally worrying,not
justbecausedevelopersareconfusedwhenbehaviorchangesjustby
switchingtools,butalsobecauseitshowsthatsomeoftheintended
behaviorofdebuggers isnot well-specified.
RelatedIdeas. Findingbugsandotherunexpectedbehaviorinde-
buggersisasurprisinglyunderstudiedproblem.Theclosestexisting
lineofworkaddressesthecorrectnessofcompilersandinterpreters.
One approach is software verification, which has been successfully
applied, e.g., in CompCert [ 21]. When the programming language
is well-specified, many compilers and interpreters can also build
onextensiveconformancesuites[ 12]orlargetestsuitesthatare
manuallywrittenbythedevelopers[ 33,38].Unfortunately,manual
tests are laborious to write and thus often insufficient. The lack of
manualtestinghasleadtoresearchon automatictesting ofdevel-
oper tools, where the tool under test is executed with generated
programs.Suchtechniqueshavefoundhundredsofbugsincompil-
ers[20,39],interpreters[ 14],andotherprogrammingtools,such
as refactoring engines[ 9].
Challenges. Unfortunately, debuggers differ from compilers and
interpretersinkeyaspectsthatmakeitchallengingtodirectlyapply
theaboveideas.First,unlikecompilers,debuggerstakenotjusta
program as input but also allow the user to steer the debugging
1https://bugzilla.mozilla.org/show_bug.cgi?id=1370648 .
2https://bugs.chromium.org/p/chromium/issues/detail?id=784852 .
3https://bugzilla.mozilla.org/show_bug.cgi?id=1362432 .
4https://bugzilla.mozilla.org/show_bug.cgi?id=1362403 .
610ESEC/FSE’18,November4ś9, 2018, Lake Buena Vista,FL,USA DanielLehmann andMichaelPradel
session,oftenthroughagraphicaluserinterface(GUI)orcommand-
line interface. The debugging actions a user issues there, such as
setting breakpoints and stepping, determine the debugger’s behav-
ior.Achallengeforeffectivedebuggertestingistogeneratesuch
debuggingactions alongside agiven program-to-debug.
Second, interactive debuggers,as thename implies, differfrom
compilers because debuggers interact with the user, instead of
takingallinputsatonceinthebeginningandproducingoutputs
only at the end of the execution. After each debugger input (e.g.,
astepaction),thedebuggerpausesandonlythenshowsthenext
outputs(e.g.,thelinethatisexecuted),waitingforthenextinput
fromtheuser.Theproblemiscompoundedbythefactthatprevious
outputsdeterminethenextpossibleinputs.Forexample,steppingis
nolongerpossiblewhentheexecutionisfinished.Forthesereasons,
automatictestingofdebuggerscannotsimplygenerateallinputs
łofflinežbefore the debuggingsession,unlikeincompilertesting.
Third,debuggersoften lackaprecisespecification oftheintended
behavior.Whereasmanycompilersandinterpretersfollowalan-
guage standard [ 1,2,16], we are only aware of few attempts to
specify the behavior of debuggers, none of which target real-world
debuggers[ 3,35].Withoutaspecification,conformancetestingand
formalmethodscannotbe appliedto debuggers.
Approach. We present DBDB, an automatic, differential testing
techniqueto detect divergingbehaviorsof debuggers. Thebasic
idea is to automatically interact with two supposedly equivalent
debuggerstofindadivergenceoftheirbehavior.Webaseourwork
on a simple model of interactive debuggers as finite-state trans-
ducers. Inthismodel,debuggerstakedebuggingactionsas inputs,
suchassettingabreakpointinaspecificline,resuming,orstepping,
and returnoutputsthat capture thebehavior of the debugger, e.g.,
which line is currently executing or the values of local variables.
Given twodebugger implementationsfor thesameprogramming
language, DBDB generates debugging actions, executes them in
parallelinboth debuggers, and compares theirbehavior.Sincede-
buggers are interactive,the approach iterates betweengenerating
actions and comparing outputs until finding diverging behavior.
Thelackofacommonlyacceptedspecificationofdebuggersresults
inalargenumberofthesedivergingbehaviors,toomanyfordirect
manual inspection. To reduce the inspection effort, we assign each
divergingbehavioranequivalenceclassandthenonlyinspectthese
equivalence classes.
Results.We evaluate DBDB on the widely used JavaScript de-
buggers of Firefox and Chromium. We exercised them with a total
of 26,931 generated actions in 2,050 debugging sessions. After less
then 20 executed actions, already 82.5% (1,692) of the debugging
sessions show diverging behavior between Firefox and Chromium.
Among these diverging behaviors are at least 20 bugs, 19 of
which were previously unknown and we thus reported to the re-
spective developers. Of the five reported Chromium bugs, four are
alreadyfixedbythedevelopers.IntheFirefoxdebuggerwereported
14bugs, fourofwhichare nowmarkedasfixed.Apartfrombugs,
we have also found many subtle differences between the Firefox
andChromiumdebuggers.Understandingtheseundocumenteddif-
ferencescanpavethewaytoaprecisespecificationoftheintended
behaviorofdebuggersandcouldhelpdifferentvendorstoagreeon
acommon debugger interface.
var foo = 2;
function aȺȻ {
  ...
}
Programs-
to-DebugGenerate
ActionsCompare 
OutputsDebugger
A
Debugger
B
Differential Testing
Feedback
A: paused in
   line ǣ
B: paused in
   line Ǥ
Traces with
Diverging Behavior
Assign to 
Equivalence Class
Equivalence Classes of 
Diverging BehaviorsFigure2:DBDBoverview:differentialtestingfindsdiverging
behaviors,which are then assigned to equivalence classes.
Contributions. In summary, this paper contributes the following:
·Weidentifycommonfunctionalitiesofinteractivedebuggersand
modelthemasafinite-statetransducerthattranslatesdebugging
actions to outputs (Section 3.1).
·We present the first differential testing technique for debuggers.
Thetechniqueusesfeedback-directedtestinputgenerationto
accountfortheinteractivenatureofdebuggersandassignsdi-
verging behavior to equivalence classes to reduce the manual
inspection effort.(Section 3.2to3.4).
·We provide empirical evidence that the approach effectively
revealsbugsandotherdivergingbehaviorinwidelyuseddebug-
gers. (Section 5).
·We make our implementation and results available to the public
underhttps://github.com/sola-da/DifferentialDebuggerTesting .
2 OVERVIEW AND MOTIVATING EXAMPLE
This section gives an overview of our approach and explains the
mainstepswithasimpleexample.Figure 2showsthemainpartsof
ourapproach.TheinputtoDBDBisaprogram-to-debugandtwo
debuggersthataresupposedtoprovidethesamedebuggingfunc-
tionalities.For ourrunningexample, supposethesedebuggersare
the JavaScriptdebuggers of Firefox and Chromium. The approach
interactswithbothdebuggersside-by-sidebygenerating debugging
actions, such as setting breakpoints andstepping through the pro-
gram. During this interaction, the approach tracks the behavior of
both debuggers and searches for any inconsistencies. When DBDB
detects any diverging behavior, it stops the interaction and reports
the difference as apair of executiontraces.
Asarunningexample,considertheJavaScriptcodeinFigure 3.
The code iterates through an array and then executes two if-
statements that each define a local variable. The letkeyword indi-
cates that these variables are local to the surrounding block scope,
i.e., they are visible only within their respective then-branches.
SupposethatDBDBstartsdebuggingthecodeinFigure 3bysetting
a breakpoint at line 1. Because both debuggers indicate that set-
tingthebreakpointwassuccessful,theapproachgeneratesanother
action by setting a second breakpoint at line 3. The Chromium
611Feedback-DirectedDifferential TestingofInteractive Debuggers ESEC/FSE’18,November4ś9, 2018, Lake Buena Vista,FL,USA
1vararray = [1, 2, 3];
2for(propinarray) {
3console.log( "hi");// Bug: Cannot set breakpoint here
4}
5if(array.length > 0) {
6letfirstVar = "first" ;
7}
8if(array.length > 1) {
9letsecondVar = 23; // Bug: Appears to be "first"
10}
Figure 3:Runningexample.
debugger confirms the breakpoint at line 3. In contrast, the Firefox
debuggerslidesthebreakpointtoline 5.Breakpointsliding isacom-
mon debugger feature to prevent users from setting breakpoints
at, e.g., empty lines and comment-only lines. But the difference ob-
servedhereisclearlyunintended,asitmakesitimpossibletodebug
the statement at line 3. Our approach detects this difference, which
has been fixed in Firefox 55 after we reported it to the developers.5
After finding a difference between the two debuggers, DBDB
continues to explore more behavior. Suppose that DBDB starts
debugging the code by setting a breakpoint at line 8. Then, the
approach starts the program’s execution and the code hits the
breakpoint in both debuggers. As the next action, suppose that
DBDBissuesa stepinactionandthedebuggersubsequentlypauses
executionatline 9.Wheneverthedebuggerispaused,ourapproach
compares the program state reported by both debuggers. In this
example,thecomparisonshowsthatFirefoxclaims secondVar to
havethevalue "first",whereastheChromiumdebuggerclaims
secondVar tobeundefined .Thissurprisinginconsistencyisdueto
abuginFirefox,whichaccidentallyshowsthevalueof firstVar,
eventhoughthatvariableisnotinscopeatline 9.Wehavereported
thisproblemandtheFirefoxdevelopersconfirmedthatitisabug.6
Theexampleillustratesthatreal-worldinteractivedebuggers,as
anyothersoftware,arenotfreeofbugs.Sincedevelopersheavily
rely on these tools to debug their own code, finding such prob-
lems is important. The following section presents our approach for
finding unexpectedand underspecifiedbehaviorofdebuggers via
differential testing.
3 APPROACH
This section formally defines the problem addressed by DBDB and
describesourapproachindetail.Thebasisofourapproachisafinite-
statemodelof interactive debuggers(Section 3.1). Buildingontop
of thismodel, Section 3.2defines the problem of finding diverging
behaviors for a given pair of debuggers. Section 3.3presents our
algorithmtoaddressthisproblemthroughautomatic,differential
testing.Finally,Section 3.4describesatechniquetoassigndiverging
behaviors to equivalence classes, which eases the task of manually
inspecting andunderstanding differences between debuggers.
3.1 Finite-State Model ofInteractiveDebuggers
Implementations of real-world interactive debuggersare complex
and analyzing them is non-trivial. To keep our approach generic
and independent of a specific debugger or programming language,
we abstract interactive debuggers intoaformalmodel.The model
5https://bugzilla.mozilla.org/show_bug.cgi?id=1362416 .
6https://bugzilla.mozilla.org/show_bug.cgi?id=1363328 .Σ::=BpAction|ExecAction|ε (Actions)
BpAction ::=Setbreakpointat line|
Remove breakpointat line
ExecAction ::=Start execution|Resume|
Stepin|Stepout|Stepover
Γ::=BpOutput| (Outputs)
ProgramState|
Executionfinished |
ε
BpOutput ::=Breakpointsetat line|Removed|Not removed
ProgramState ::=⟨line,callStack,vars⟩
callStack ::=name∗
vars::=(name:type=value?)∗
line∈N,linenumbers (Meta Variables)
name∈identifiers
type∈types
value∈primitive values
Figure 4:Grammars ofdebugging actions and outputs.
focuses on features common tomost real-world interactive debug-
gers and abstracts away properties of debuggers that are irrelevant
for analyzing them.
Ourmodelisbasedonfinite-statetransducers(FSTs).FSTsare
avariantoffinite-statemachineswhereeachtransitioncanboth
consumeinputandproduceoutput.Theyarewidelyusedinnatural
languageprocessing,e.g.,formachinetranslationorpart-of-speech
tagging[27].AnFSTmodelfitstheinteractivenatureofdebuggers,
whereinputsaredebuggingactionstriggeredbyauserandoutputs
represent the resulting behavior of the debugger. More formally,
we represent adebugger as follows:
Definition1(Debugger). Adebuggerforaprogram Pisa5-tuple
(Q,q0,Σ,Γ,δP)where
·Qisafinitesetofstates,
·q0∈Qisthe initialstate,
·Σis a set of input symbols that represent debugging actions the
usercan trigger,
·Γisasetofoutputsymbolsthat represent debugger behavior,
·δP:(Q×Σ)×(Γ×Q)isatransitionrelationthatmapsthecurrent
state and some debugging action to the behavior produced by
the debugger andthe nextdebugger state.
Figure4summarizestheinputsandoutputsconsideredinthis
workbyshowingthegrammarsfortheinputandoutputlanguages
ΣandΓ.Debuggingactionsin Σareeitherrelatedtobreakpoints
(BpAction) or control the execution of the program ( ExecAction ).
Debuggingoutputsin Γareeitherbreakpoint-related( BpOutput ),
provide details about thecurrent program state ( ProgramState ),or
indicate that the program has terminated. We will explain the sym-
bols and how they relate to real-world debuggers in the following.
Figure5showsthestates Qandthetransitionrelation δPofa
debugger. We model debuggers as having three states: not running ,
running,andpaused.Thedebuggertransitionsbetweentwoofthese
statesqandrwhentakinganaction xasinputandinturnexhibits
somebehaviorrepresentedbyadebuggingoutput y.Inthefigure
612ESEC/FSE’18,November4ś9, 2018, Lake Buena Vista,FL,USA DanielLehmann andMichaelPradel
ε :Paused at line
Call stack: fnǒ, fnǓ, …
Variables: nameǒ  = valueǒ , …
Start execution  : εSet breakpoint at line : Breakpoint set at line
ε : Execution finished
Resume
Step in/out/over: εRemove breakpoint at lineRemoved
Not removed:
Running
Running PausedNot
Figure5:Interactivedebuggersasfinitestatetransducers.In
transitions,ł:žseparatesinputs(green)fromoutputs(blue).
and in the following definitions, we use the notation qx:y−−−→rfor
suchatransition(q,x,y,r)∈δP.Atransitionwheretheinputisthe
emptyword εindicatessomebehaviorthatisnotdirectlytriggered
bytheuser.Likewise,weassignanoutput εtodebuggingactions
that donot have an immediate output.
Besides debugging actions, the behavior of a debugger depends
ontheprogram-to-debug P.Wemodelthisdependencybyindexing
the transition relation δPwithP. SincePcan keep internal state,
thenextdebuggerstateandoutputdependsnotonlyonthecurrent
debugger state and action, but also on P’s state. Apart from δP, all
other parts of our model are independent of the program-to-debug.
Initially, the program-to-debug Pis not executing and the de-
buggerisinthestate q0=not running .Inthisstate,userscanset
and remove breakpoints, either by clicking on line numbers in a
GUIorinapromptof,e.g.,GDBorLLDB.Setbreakpointactions
and remove breakpoint actions have an immediate output: the line
where the breakpoint was set and whether removing a breakpoint
wassuccessful,respectively.
Thestartexecutionactiontransitionsthedebuggertothe run-
ningstate and runs the program P. In the running state, debuggers
do not display any information about the execution to the user.
Only when a breakpoint is hit or a step completes, the debugger
transitionstothe pausedstateandduringthattransitionoutputs
information about the current program state. In our model, this
output of program state comprises three pieces of information
⟨line,callStack,vars⟩:
· inwhich linethe program has paused,
· thecallStack as asequenceoffunctionnames, and
·the setvarsof local variables, along with their types and, for
primitive types, theirvalues.
Issuing a resume action, e.g., by clicking on
 in a graphical de-
buggerorbytyping continue inaprompt,transitionsbacktothe
runningstateuntilthenextbreakpointishit.Similarly,stepping
through the program continues execution until the next step of
computationcompletes.We modelthree kindsofsteps:
·Step in (
 ), which executes the next statement and, if it is a
function call,stops at the beginningofthe callee.
·Step out (
 ), which executes all statements until the end of the
currentfunction.
·Stepover(
 ),whichexecutesthenextstatementand,ifitisa
function call,does not enter the function for debugging.The execution of the program-to-debug can also just finish with
a transition back to the not running state. This case happens when
theprogramterminateswithouthittinganyfurtherbreakpointand
withoutpausing after astep.
Itis important to pointout thatitdiffers significantly from pro-
grammingtoolsconsideredinrelatedwork,inparticular,compilers.
Whentestingcompilers[ 20,39]andotherdevelopmenttools[ 9],
the input consists of only a program. In contrast, debuggers expect
user actions as an additional input. Another difference is that exist-
ing compiler testing is non-interactive, i.e., the produced program
produces only a single output, such as its exit code. In contrast,
debuggers alternate between taking inputandproducing output.
3.2 ProblemStatement
Basedonthefinite-statemodelofdebuggersinDefinition 1,wenow
define the problem addressed in this work. The overall goal is to
findand understand diverging behaviors between two supposedly
equivalent debuggers for the same programming language. The
following formally definesdivergingbehavior.
Definition2(Divergingbehavior). Givenaprogram P,twodebug-
gers(Q,q0,Σ,Γ,δP)and(/tildewideQ,/tildewideq0,/tildewideΣ,/tildewideΓ,/tildewiderδP)havedivergingbehaviorif
twosequencesoftransitionsexist:
q0x:y−−−→..x′:y′
−−−−→rand/tildewideq0/tildewidex:/tildewidey−−−→../tildewidex′:/tildewidey′
−−−−→/tildewider
whereq0=/tildewideq0,x=/tildewidex,y=/tildewidey,x′=/tildewidex′,butwhereeither y′/nequal/tildewidey′orr/nequal/tildewider.
Thatis,giventhesamesequenceofactionsdispatchedtoboth
debuggers,divergingbehaviormeansthateithertheoutputs y′and
/tildewidey′differorthat the states rand/tildewiderreachedbythe debuggers differ.
Thegoalofourapproachistofindsequencesofinputsthatlead
todivergingbehavior.Weassumethattheprograms-to-debugare
provided. The problem of finding suitable programs is related to
generatinginputsforcompilertesting,andourapproachmaybe
combined with existing work on generating programs. We further
assumethatthedebuggerandtheprograms-to-debugaredetermin-
istic, which is a common assumption to make testing reproducible.
3.3 InteractiveDifferential Testing
Weaddresstheproblemoffindingdivergingbehaviorbetweentwo
debuggersthroughinteractivedifferentialtesting.Thebasicidea
is to compare two supposedly equivalent debuggers by continu-
ously generating debugging actions and by checking the resulting
behaviorfor inconsistencies.
Challenges. Tomotivateourinteractiveapproach,wefirstout-
line several challenges inherent to testing of debuggers. Many
typical uses of debuggers involve a combination of actions, e.g.,
firstsettingabreakpointandthensteppingonceitishit.Unfortu-
nately,generatingasequence ofsuchactions aheadoftime based
on the grammar for Σis ineffective for three reasons. First, once
the program-to-debug terminates, the debugger cannottakemore
actions and any remaining actions have been generated in vain.
Second, some actions that are legal according to the grammar of Σ
areillegalforreasonsonlyknownatruntime.Considersettinga
breakpointatline 7ofFigure 3.Bothdebuggersslidethebreakpoint
tothenextlinebecausethereisnocodetoexecuteattheclosing
brace. Now, setting a second breakpoint in line 8is not allowed
613Feedback-DirectedDifferential TestingofInteractive Debuggers ESEC/FSE’18,November4ś9, 2018, Lake Buena Vista,FL,USA
andourapproachmustnotgeneratesuchanaction.7Third,once
anactioncausesdiverging behaviorbetween debuggers,the anal-
ysis has reached an inconsistent state that will only cause more
diverging behavior. Consider again the example in Figure 3. When
requesting a breakpoint at line 3, Firefox slides it to some later line
due to a bug, whereas Chromium correctly sets it at line 3. Execut-
ing more actions, e.g., resumes and steps, after this first divergence
is uninformative since they will likely result in more diverging
behaviorbecausethebreakpointsweredifferent.Suchfollow-up
divergences are not relevant on their own but merely the result of
havingreachedan inconsistentstate earlieron.
Our algorithm addresses the challenge of testing interactive de-
buggers by issuing actions to two debuggers side-by-side and com-
paring theirbehaviorafter eachaction.The approach is feedback-
driven in the sense that the behavior triggered by previous actions
influenceswhatactionstotriggernext.Oncetheapproachobserves
adifferenceinbehaviorbetweenthetwodebuggers,itstopsand
reports twotraces that summarize the executions.
Definition 3 (Trace). A tracetis the result of interacting with
a debugger Don a program P. The trace is a sequence ⟨e1,..,en⟩
ofnevents,whereeacheventiseitheranactionoranoutput,i.e.,
ei∈Σ∪Γ∀1≤i≤n.
The goal of DBDB is to find traces tAandtB, one per debugger,
that share a common prefix but then diverge in the resulting be-
havior after the final action. For an example of two such traces,
considerFigure 6a.Thefirsteventinbothtracesisasetbreakpoint
action, followed by the corresponding debugger output. The traces
sharethesameactionsandoutputsuptothefinalłStepoverž.Only
then, thedebugger behaviors diverge, with Chromium pausing in
line25 ofthe program, whereas Firefox pauses inline26.
Algorithm 1summarizes the main steps of our approach for
obtaining such traces by automatically interacting with two debug-
gers,DAandDB,onaprogram P.Thecurrentstateofadebugger D
isgivenby D.state.Weindicatewith D.action()thatDBDBtriggers
oneoftheactionsdefinedby Σ(Figure4).Weassumeeachtriggered
action isaddedto the trace ofthe corresponding debugger.
The algorithm consists of three parts: manipulating breakpoints
beforerunningtheprogram,startingtheprogramexecution,and
stepping and resuming during program execution. The first part
(lines2to13) sets breakpoints at randomly chosen lines and prob-
abilistically removes them again until obtaining a configurable
overallnumberofbreakpoints.Foreachsetbreakpoint,thealgo-
rithmcomparestheactualbreakpointlocationchosenbythetwo
debuggers.Theintendedandtheactualbreakpointlocationsmay
differ,e.g.,becausedebuggersslidebreakpointsinsteadofadding
them to empty lines. If the actual locations differ, the algorithm
has detected diverging behavior and therefore stops and returns
the trace. Otherwise, the algorithm tries to remove the breakpoints
again and checks whether both debuggers agree that removing the
breakpoint is possible. While it may appear obvious that removing
breakpoints is possible, we found a bug in a debugger that ignored
ausersrequest to remove abreakpoint.8
7SettingtwobreakpointsinthesamelineispreventedinGUI-baseddebuggersbecause
clicking on line numbers toggles a breakpoint. But in lower-level debugging APIs, e.g.,
of Chromium,anerroris thrown when settingtwobreakpoints at the same line.
8https://bugzilla.mozilla.org/show_bug.cgi?id=1362439 .Algorithm1 Interactive differential analysis
Input:Debugger DA,DBandprogram P
Output: TracestA,tBof actions andoutputs
1:Assume:DA.state=DB.state=not running
▷Manipulate breakpoints:
2:BPs←∅
3:while|BPs|<max number of breakpoints do
4:l←randLine(P)
5:ifl∈BPsthen
6:continue
7:lA←DA.setBp(l);lB←DB.setBp(l)
8:iflA/nequallBthen
returnłDivergingbehavior: Bp locationž
9:ifrandProb()<prob.of removingbreakpoint then
10:okA←DA.rmBp(l1);okB←DB.rmBp(l2)
11:ifokA/nequalokBthen
returnłDivergingbehavior: Bp removalž
12:else
13:BPs←BPs∪{lA,lB}
▷Start program execution:
14:DA.startExec();DB.startExec()
▷Stepandresume:
15:repeat
16:Waituntil DA.state/nequalrunningandDB.state/nequalrunning
17:ifDA.state=DB.state=not running then
returnłProgram finishedž
18:ifDA.state/nequalDB.statethen
returnłDivergingbehavior: Terminationž
19:ifDA.programState/nequalDB.programState then
returnłDivergingbehavior: Program statež
20:action←randPick({resume,stepIn,stepOut,stepOver})
21:DA.action();DB.action()
22:untilmax number of executionactions
Thesecondpartofthealgorithm(line 14)startstheprogram’s
execution with the debugger attached to it. The execution will con-
tinueuntilstoppingatabreakpointoruntiltheprogramterminates.
The third part of the algorithm (lines 15to22) repeatedlysteps
through the program or resumes execution until hitting a break-
pointorprogramtermination.Thealgorithmwaitsuntilbothde-
buggersleavetherunningstateandthenchecksiftheirbehavior
is consistent.9If one but not the other debugger has reached the
end of the program, then the algorithmreports adiverging behav-
ior. Otherwise, if both debuggers pause the execution, then the
algorithmcomparestheprogramstates (line,callStack,vars)and
(/tildewidestline,/tilde6callStack,/tildewidestvars)reportedbythedebuggers.Ifthestatesdiffer,
e.g., because the debuggers have paused at different locations or
because they show different call stacks, then the algorithm returns
a trace that summarizes the diverging behavior. Finally, if there
isnoobservabledifferencebetweenthetwodebuggers,thealgo-
rithm triggers a randomly selected execution action (resume or
steps)inbothdebuggers.Thisprocesscontinuesuntilreachinga
configurable maximum number of actions.
9We assume that the program-to-debug terminates. If a debugger never leaves the
running state, then it has a bug that can be detected without any differential analysis.
Wehavenot encountered thiscase in ourevaluation.
614ESEC/FSE’18,November4ś9, 2018, Lake Buena Vista,FL,USA DanielLehmann andMichaelPradel
23// Program 1
24...
25for(leti=0; i < 3; i++) {
26 console.log(i);
27}
28...
Firefox debugger :
Setbreakpoint at line 25
Breakpoint setat line 25
...
Start execution
...
At line25, call stack...
Stepover
Atline26, call stack...Chromium debugger :
Setbreakpoint at line 25
Breakpoint setat line 25
...
Start execution
...
At line25, call stack...
Stepover
Atline25, call stack...
(a)Program1and corresponding pair of traces.≈40// Program 2
41...
42varx = 10;
43for(varj = 7; j < x; j++) {
44 // Some statements
45 ...
46}
Firefox debugger :
Setbreakpoint at line 43
Breakpoint setat line 43
...
Start execution
...
At line43, call stack...
Stepover
Atline44, call stack...Chromium debugger :
Setbreakpoint at line 43
Breakpoint set at line 43
...
Start execution
...
At line43, call stack...
Stepover
Atline43, call stack...
(b) Program2and corresponding pair of traces.
Figure 6:Example ofequivalent divergingbehaviorsexposed by twodifferentprograms.
3.4 EquivalenceClassesofDivergingBehavior
Running DBDB once with a single program-to-debug may or may
notexposedivergingbehavior.Foreffectivetesting, weapplythe
approachtomultipleprogramsandrepeatedlydebugeachprogram
with different random seeds. The random seed controls all non-
deterministicdecisionsmadebyAlgorithm 1,suchaswhichactions
totrigger.AsweshowinSection 5,repeatedlycomparingreal-world
debuggersproducesthousandsoftraceswithdivergingbehaviorsin
a few minutes. While this abundance of diverging behavior shows
theeffectivenessofourdifferentialtestingapproach,italsoleads
tothenon-trivialchallengeofinspectingthedivergingbehaviors.
Manuallyinspectingalldivergingbehaviorsispracticallyinfeasible.
One way to address this challenge is to uniformly sample all
divergingbehaviorsandtoinspectonlyasubsetofthem.However,
we findthatmanydivergent behaviorsare similarand likelyhave
thesamerootcause.Asamotivatingexample,considerFigure 6,
whichshowstwoprograms-to-debugthateachexposedivergingbe-
haviorbetweentheJavaScriptdebuggersofFirefoxandChromium.
BelowProgram1and2,therespectivepairsoftracesthatexpose
thedivergingbehaviorareshown.Eventhoughthedivergingbe-
haviors are caused by different programs, three key characteristics
are common between Figure 6aand6b: (1) the last action is a łStep
overž,(2)thatactionisissuedatafor-loop,and(3)thediverging
behaviorsareduetoprogramstate,inparticular,thelineswhere
the debuggers pause.
Weavoidmanuallyinspectingtoomanyofthesesimilarcases
byfirstdividing divergingbehaviors intoequivalence classes:
Definition4(Equivalenceclassesofdivergingbehavior). LetP1and
P2betwoprograms-to-debug,andlet DAandDBbetwodebuggers.
Suppose that the pairsoftraces (t1
A,t1
B)and(t2
A,t2
B)both expose a
divergingbehavior,where ti
Xistheresultofdebuggingprogram
Piindebugger DX.Thetwodivergingbehaviorsareinthesame
equivalence class if
· the last debuggingaction isthe same in t1
A,t1
B,t2
A,andt2
B,
·the AST node type of the source code line where the last action
wastriggeredisthe same for both P1andP2,and
·the type of diverging behavior between t1
Aandt1
Bis the same as
for the divergingbehaviorbetween t2
Aandt2
B.To compute the AST node type of a source code line l, we parse
theprogram-to-debugandthensearchthelowestASTnodethatin-
cludesalltokensin l.WecompareASTnodetypesandnotprogram
lines directly, because AST node types abstract away program-
specific properties,such as concrete linenumbers oridentifiers.
The type of diverging behavior is determined by the return
valueofAlgorithm 1.Specifically,therearesixtypesofdiverging
behaviors (linenumbers refer to Algorithm 1):
· Breakpointsare setat differentlocations (line 8).
· Breakpointscannotbe consistently removed(line 11).
· One but not the otherdebugger terminates(line 18).
·The program state in both debuggers is different (line 19), subdi-
videdbywhichpart exactly differs(see Figure 4):
◦the debuggers pausedat differentlines,or
◦the function names onthe callstack differ,or
◦the variables andtheir types andvaluesdiffer.
Then, we draw our samples for manual inspection from the
equivalence classes inaroundrobinmanner instead ofuniformly
sampling all diverging behaviors. The technique is a heuristic that
is independent of the programming language and we show in Sec-
tion5that it results in a more diverse set of inspected diverging
behaviors,whichultimately leads to more foundbugs.
Reiterating on the example in Figure 6, our technique assigns
the two diverging behaviors to the same equivalence class because
all three conditions from Definition 4are met. In this particular
case,thetwodivergingbehaviorsareevencausedbythesameroot
cause:theChromiumdebuggeralwayspausesateachsubstatement
whensteppingoverafor-loopheader,whereasinFirefoxałStep
overž alwaysgoes to the nextline.
4 IMPLEMENTATION
WehaveimplementedDBDBinTypeScriptandrunit,aftercom-
pilation to JavaScript, in Node.js. Firefox and Chromium offer a
programmaticinterfacetotheirdebuggersthroughtheirrespective
remotedebuggingprotocols(RDP).Thatis,wedonottestthede-
buggers, for example, by clicking in the GUI, but directly exchange
RDP messages with the debuggers via a WebSocket connection.
615Feedback-DirectedDifferential TestingofInteractive Debuggers ESEC/FSE’18,November4ś9, 2018, Lake Buena Vista,FL,USA
For Chromium, we build on the chrome-remote-interface library,10
which already offers a simple abstraction, e.g., to attach the debug-
ger to a JavaScript program, set breakpoints, and perform other
debuggingactions.ForFirefox,weimplementtheRDPourselves
sincenoup-to-dateRDPlibraryhasbeenavailable.Ontopofthese
low-level remote debugging protocols, we implemented a common,
higher-levelAPIforbothdebuggers.Tomakesurethateveryfound
divergingbehaviorisnotjustvisibleattheremotedebuggingproto-
col level, we also manually reproduced every bug in the debuggers’
GUIs. In particular, all our bug reports include videos of the visibly
wrongbehaviorintheGUI.Theimplementationispubliclyavailable
underhttps://github.com/sola-da/DifferentialDebuggerTesting .
5 EVALUATION
5.1 ExperimentalSetup
We apply DBDB to two popular JavaScript debuggers: Firefox 54.0
and Chromium 59.0.3071.109. We run DBDB on a laptop with 8GB
of system memory and an Intel Core i5-5200U CPU. The operating
systemisUbuntu16.10 64-bit.
We use 41 JavaScript programs-to-debug that are obtained from
threesources:First,weuse26programsfromSunSpider,aJavaScript
benchmark,version1.0.2,whichaccordingtotheoriginalannounce-
ment cover a łwide variety of numerical, array-oriented, object-
oriented, and functional idiomsž [ 30]. Second, we use 11 JavaScript
puzzlesforstudentsfromaprogramanalysislectureatTUDarm-
stadt.Thepuzzlescovercornercasesofthelanguage,makingthese
programs also a potential challenge for developers of debuggers.
Third, we use four programs written by us that cover newer lan-
guage features, such as letorconst, as these features are not used
intheotherprograms.Toensurethattheprograms-to-debugare
deterministic,wefixthecurrenttimeandreplace Math.random with
adeterministic function.
5.2 QualitativeAnalysis
Applying DBDB to the 41 programs-to-debug reveals various di-
verging behaviors between debuggers. They range from clear bugs
in one of the debuggersto underspecifiedbehavior,where neither
of the debuggers is clearly wrong, but that is nevertheless inter-
esting.Wehavefound20clearbugs,19of whichwerepreviously
unknownandwhichwehavesubsequentlyreportedtothedevel-
opersofFirefoxandChromium.Sevenofthereportedbugshave
alreadybeenfixed.Thefollowingdiscussesaselectionofdiverging
behaviors inadditionto the ones presentedearlierinthe paper.
ChromiumIssue730177. Example1inTable 1showsabugrelated
tobreakpointsintheChromiumdebugger.Abreakpointissetin
the last line of the 3d-cube.js SunSpider program, shown as an
excerpt. Although line 4 is empty, we would expect the breakpoint
to stay there since the program ends with this line. Firefox exhibits
thecorrectbehavior,butChromiummovesthebreakpointtothe
first line of the program. This diverging behavior was caught by
DBDB, we subsequently submitted a bug report, which has been
confirmedandfixed.11
10https://github.com/cyrus-and/chrome-remote-interface/
11https://bugs.chromium.org/p/chromium/issues/detail?id=730177Firefox Issue 1362403.12Example 2 demonstrates a bug in the
pausing and stepping behavior of a debugger. By comparing pause
locations between debuggers, DBDB has found that the Firefox
debuggerdoesnotstepthrougheachiterationofafor-in-loopinthis
excerptfromtheSunSpiderprogram regexp-dna.js .Inparticular,
whenpausedattheloopheader(line2),issuingonlytwoStepIn
actions(
 )takesthedebuggerpastthelooptoline5,eventhough
its body executes more than once. Chromium behaves correctly
andpauses twice (andmore often)at line3.
FirefoxIssue1362432.13InExample3,thedebuggerseverelymis-
represents the actual program state during execution. It was found
byDBDBwhendebuggingoneoftheaforementionedJavaScript
puzzlers.JavaScriptallowsdeveloperstorepeatparameternames
when declaring functions. Inside the function foo,paramshould
be bound to the second supplied argument ( "second" ). The run-
timebehavioriscorrect (confirmed by theoutput), but theFirefox
debugger showsthe variable withthe wrongvalue "first".
Besidesclearmisrepresentationsoftheactualruntimebehavior,we
havealsofoundotherdivergingbehaviorsbetweentheFirefoxand
Chromium debuggers. These diverging behaviors are also valuable
todetect;firstly,becausedivergingbehaviorsareconfusingwhen
switchingdebuggersandsecondly,becausetheyindicatethatthe
intendeddebugger behaviorisnot well-specified.
PossibleBreakpointLocations. Severalinstancesofunderspeci-
fiedbehaviorarerelatedtothequestionwhereitshouldbepossible
to set breakpoints. Example 4 in Table 1demonstrates that Fire-
fox allows to set a breakpoint at the literal truein line 4 (and
subsequently pauses there when execution is started), whereas
Chromium slides the breakpoint to the next line. Neither is clearly
wrong and it is open for specification whether setting breakpoints
atallfunctionargumentsshouldbepossible(forconsistencyand
so that developers can inspect each individually) or only at non-
literals(becauseitisunclearwhatisactuallyexecutedatliterals).14
DBDB found several more such cases, e.g., Firefox allows to set
breakpointsat while(true),but Chromium does not.
Step Semantics and Whitespace. Another large class of under-
specifiedbehaviorsisrelatedtosteps.WhileFirefoxandChromium
agreeonsteppingoverasinglefunctioncall,itisnotclearwhatthe
correctbehaviorshouldbewhensteppingoverotherstatements.
ThelastexampleinTable 1showsthatasinglestepinFirefoxjumps
over multiple statements if they are in a single line. Chromium, on
the other hand, steps over each statement individually and thus
pauses multiple times in the same line. Even to a Firefox devel-
oper,the correct intendedbehavior was not clear.15Phrasedmore
generally,itisopenwhetherdebuggingshouldbealtogetherłin-
dependentž of whitespace (or other non-semantic tokens). That is,
should,e.g.,thenumberofstepstoreachsomestatementalways
be the same,even if such tokens are inserted?
Overall, the examples illustrate that the diverging behaviors found
byDBDBaffectallkindsofactionsandoutputsourdebuggermodel
12https://bugzilla.mozilla.org/show_bug.cgi?id=1362403
13https://bugzilla.mozilla.org/show_bug.cgi?id=1362432
14Seecomment at https://bugzilla.mozilla.org/show_bug.cgi?id=1370641#c4 .
15Seecomment at https://bugzilla.mozilla.org/show_bug.cgi?id=1370655#c2 .
616ESEC/FSE’18,November4ś9, 2018, Lake Buena Vista,FL,USA DanielLehmann andMichaelPradel
Table 1:Examples ofdetected divergingbehaviors.
IDAffected
debuggerExcerpt ofprogram-to-debugExcerpt oftrace with
detecteddifferenceDescription
1Chromium1// Beginning of program
2DisplArea = null;
3EOFSetbreakpointat line3
Breakpointsetat line 1Breakpointsetinemptylast line
wrapsaround andissetat
beginningof program.
2Firefox1for(kinsubs)
2dnaInput = ...
3
4varexpectedDNAOutputString = ...Pausedat line2...
Stepin
Pausedat line3...
Stepin
Pausedat line 5Stepinpauses only onceat
for-in-loopbody,even though
multiple iterations are executed.
Gives impressionthat loopis
executedonly once.
3Firefox1function foo(param, param) {
2console.log(param);
3}
4foo("first" ,"second" );Pausedat line2...
Variables:
param="first"Function parameterwithrepeated
name isshownwithwrongvalue
(but executioniscorrect).
4Both1function bar() { ... }
2function foo(a, b) {}
3foo(
4true,
5bar()
6);Setbreakpointat line4
Breakpointsetat line 4/5
...
Start execution
Pausedat line 4/5Firefoxallowssettingbreakpointsat
literals andalsopauses there,
Chromium does neither.
5Both 1a ="foo"; b ="bar"; c ="baz";Pausedat line2
Stepover
Pausedat line 3/2Stepover inFirefox pauses at next
line,even if currentlinehas
multiple statements.Chromium
steps onceper statement.
considers.Outofthe20foundbugs,eightarerelatedtosettingand
removingbreakpoints,e.g.,whenbreakpointscannotbesetbutare
slidedawayfromvalidprogramconstructs,orwhenbreakpoints
cannotberemoved.Sevenbugsarerelatedtosteppingandpausing,
e.g., when the debugger does not halt at breakpoints or after steps,
orwhenthedebuggerpausestoooftenorevenatdeadcode(see
Figure1).Finally,five bugs are related to otherprogram state,e.g.,
whenthedebuggerdoesnotshowsomevariablesatallorshows
themwithwrongvalues.
5.3 QuantitativeAnalysis
5.3.1 Effectiveness of Finding Diverging Behavior. We evaluate the
effectiveness ofDBDB bymeans ofthree questions:
·Howoftendoestheapproachfinddivergingdebuggerbehavior?
· Howmanyactions are requiredto find divergingbehavior?
· Whichtypes ofdivergingbehaviorare the mostcommon?
We can answer all three questions with the help of Figure 7. Thex-
axis shows how many actions have been generated. In the leftmost
case,onlybreakpointactionsweregeneratedandtheprogram-to-
debug is not yet running. At x=1, only the start execution action
is generated and from x>1 resumes and steps are generated as
well. The y-axis shows how many test runs have (cumulatively)
completedafter xactions.Adifferentialtestruncompleteseither
becausethe program-to-debugfinishes executingwithout finding
divergingbehavior (łprogramfinishedž,green part ofthe bar)or
because of diverging debugger behavior (the rest of the bar, all
othercolors).
Effectiveness. Afteratmost x=20generatedexecutionactions
(i.e., start execution, resume, and steps), we have found some typeof diverging behavior in 82.5% of the runs, which substantiates our
claimthatdifferentialtestingiseffectivefordebuggers.Thehigh
numberoffounddivergingbehaviors(1692intotal)hasalsomo-
tivated assigning them into equivalence classes for more effective
manual inspection.
NumberofActions. Weseeat x=0thatonlybysettingbreak-
points and without starting the programs-to-debug, 47.9% (983) of
thetestrunsalreadyfinddivergingbehaviorbetweendebuggers.
After a single start execution action ( x=1), an additional 6% (124)
of the test runs find diverging debugger behavior not due to break-
points. Several programs-to-debug (5.4%, 112) also immediately
finish execution, presumably because all breakpoints were set in
not executed code. With each additionally generated action, more
diverging behaviors are found, which affirms that debugger testing
is more effective when combining several debugging actions. After
fiveactions,however,thegraphquicklysaturatesandadditional
actions find only marginally more diverging behaviors. After 20
generated execution actions, 99.7% (2236) of the runs have com-
pleted and only 14 runs have to be stopped becausethe maximum
numberofactionsisreached.Thismakesclearthat20execution
actions issufficient inmostexecutions.
DifferenceTypes. Assaidbefore,mostdivergingbehaviors(47.9%,
983)arefoundwithout everexecutingtheprogram-to-debug,but
simply by comparing breakpoints between debuggers. When com-
paringtheprogramstatebetweendebuggersfrom x≥1on,wesee
that the majority of differences are either because one debugger
already finished executing while the other is still running (light
blue, 8.7% at x=20) or because the debuggers do not agree on
their pause location (dark blue, 18.8%). Both cases are indicative
617Feedback-DirectedDifferential TestingofInteractive Debuggers ESEC/FSE’18,November4ś9, 2018, Lake Buena Vista,FL,USA
0500100015002000
0%25%50%75%100%
BPs 
Only1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
Maximum number of execution actionsCompleted runs (cumulative)Completed with result:     
Program finished
Variable di ﬀerence
Call stack di ﬀerencePause location di ﬀerence
Termination di ﬀerence
Breakpoint di ﬀerence
Figure 7: Completed test runs (and their results) per maxi-
mum numberofgenerated execution actions.
Table2:Thetop6equivalenceclassesofdivergingbehavior.
Diff. Type Last Action AST Node Size in%
Setbreakpoint Setbreakpoint Program 538 31.8%
Setbreakpoint Setbreakpoint ArrayExpr 72 4.3%
Setbreakpoint Setbreakpoint BinaryExpr 64 3.8%
Pausedline Stepin ForStmt 61 3.6%
Setbreakpoint Setbreakpoint Property 55 3.3%
Pausedline Stepover ForStmt 51 3.0%
of a debugger either not pausing where it should or pausing too
often.Onlyaminorityofthedifferencescomefromcomparingcall
stackandvariables(2.5%and4.4%,respectively),whichcorrelates
with the smaller number of bugs we have found related to these
debugger features.
5.3.2 Effectiveness of Equivalence Class Sampling. The large num-
ber of found diverging behaviors cannot all be manually inspected
and we thus need to restrict ourselves to a subset. In Section 3.4,
wearguedthatmorediversedivergingbehaviorscanbefoundby
first assigning each diverging behavior to an equivalence class and
then sampling these classes in a round robin manner for manual
inspection.We evaluatethe effectivenessof thisheuristic through
twoquestions:
·How many equivalence classes exist, and is it feasible to inspect
at leastone divergingbehaviorper class?
·Doessamplingdivergingbehaviorsfromtheseclasses,instead
ofsamplinguniformlyoveralldivergingbehaviors,helpfinding
more unique root causes per inspecteddivergingbehavior?
Equivalence Classes. In total there are 112 equivalence classes,
which is a manageable number to manually inspect, in particu-
lar compared to the 1,692 total number of traces with diverging
behavior. This strong reduction is mainly caused by some large
equivalenceclasses,thetopsixareshowninTable 2.Thelargest
equivalenceclasscontainsalmostathirdofalldivergentbehaviors.
As indicated by the difference type and last action, these diverg-
ing behaviors are caused by setting a breakpoint at the Program
AST node. Programis the root of the JavaScript AST, so this means
a breakpoint was set in an empty or comment line. Almost all
divergent behaviors in this class are instances of a single bug inChromiumrelatedtobreakpointslidingforemptylinesandcom-
ments.16Similarly,theequivalenceclassesofsettingbreakpoints
atarrays,binaryexpressions,orobjectpropertiesarelargebecause
Firefox allows to set breakpoints at literals (which are common at
theseASTnodes),whereasChromiumdoesnot.Finally,weseetwo
largeclassesrelatedtosteppingand ForStmt,whichinstancesof
the difference in step semantics between Firefox and Chromium
that we explainedinSection 5.2.
MoreEffectiveManualInspection. Inresponsetothesecondques-
tion, we evaluated the number of unique root causes we find when
uniformly sampling diverging behaviors compared to when we
sampleoutoftheequivalenceclasses. Arootcause iseitheranID
fromtheissuetrackersforbugsthatwereported,oranidentifier
for consistent semantic differences we foundbetween Firefox and
Chromium. Figure 9shows that manually inspecting 50 diverg-
ingbehaviorsfromequivalenceclasseshaveledustoidentify24
uniquerootcauses,whereasuniformlysamplingfromalldiverging
behaviorsuncoversonly12.Oneexplanationforthisisrootedin
the large equivalence classes discussed before. When uniformly
sampling diverging behaviors, we get many instances out of these
large classesthat are mostly causedbyjust asinglebug.
Regardingthetimespentonanalyzingdivergingbehaviorsfrom
equivalence classes and reporting all bugs (including manually
creatingminimaltestcases),weestimatetheeffortaslessthana
weekforoneperson.Wedonotevaluatethetruepositiverateof
DBDB because defining true positives is difficult in the absence of
aclear specification ofexpecteddebugger behavior.
5.3.3 Performance. We evaluate the runtime of our approach to
assesswhetherDBDBcouldscaletomanymoreprograms-to-debug.
Since the assignment of diverging behaviors to equivalence classes
isperformedjustonceafteralldifferentialtestingcompletedand
takes less than two seconds, we exclude it from our measurements.
To evaluate the differential testing of our approach, we debug each
of the 41 programs 50 times with different seeds. That is, even
though the program execution itself remains the same, DBDB sets
different breakpoints andperforms, e.g.,different steps or resume
actions. Figure 8summarizes the runtimes of the approach. The
averagetestruntook448ms,soperformance-wiseourapproach
can be applied to many more programs-to-debug. Obviously the
runtime includes execution ofthe program-to-debug itself, which
iswhy someprograms taking consistently more time thanothers.
6 RELATED WORK
Compiler Testing. Compiler testing has a long history [ 5,11,34].
More recent approaches include CSmith [ 39], which randomly cre-
ates C programs, and łequivalence modulo inputsž [ 20], which cre-
atesvariantsofsuchprogramswithsupposedlyequivalentbehavior.
Other work empirically compares compiler testing techniques [ 7],
prioritizesgeneratedtestprogramstoexecutebug-revealingpro-
grams earlier [ 6], generates test programs guided by a type- and
effect-system[ 26],andenumeratesalltestprogramswithinsome
bounds [40]. Lidbury et al . [22]and Donaldson et al . [10]target
compilersforOpenCLandGLSLprogramsrunningonGPUs.All
of these approaches put an emphasis on generating programs as
16https://bugs.chromium.org/p/chromium/issues/detail?id=784852
618ESEC/FSE’18,November4ś9, 2018, Lake Buena Vista,FL,USA DanielLehmann andMichaelPradel
1010010003000
controlflow-recursivebitops-nsieve-bitsaccess-nsieve
access-fannkuch
math-spectral-normmath-partial-sumsaccess-nbodycrypto-sha1string-fastapuzzler-6
access-binary-treescrypto-md5manual-4
date-format-xparbdate-format-to�e
3d-raytracecrypto-aespuzzler-9
string-validate-inputpuzzler-11manual-3
string-base64puzzler-3puzzler-7puzzler-1puzzler-5puzzler-2puzzler-83d-cubepuzzler-10puzzler-4manual-2math-cordicmanual-1
bitops-bits-in-bytebitops-bitwise-and
bitops-3bit-bits-in-bytestring-tagcloud3d-morph
string-unpack-coderegexp-dna
Program-to-debugRuntime in ms (log scale)
Figure 8: Runtime of the differential testing per program-to-debug. Since actions are randomly generated, medians (middle
band)andfirstandthirdquartile(bottomandtopofbox)areshownafter50runs.Whiskersindicateminimumandmaximum.
0510152025
0 10 20 30 40 50
Number of inspected diverging behaviorsUnique root causes foundSampling strategy     
 Round robin from equivalence classes  All uniformly
Figure 9: Found unique root causes when sampling differ-
enceequivalence classes compared to raw differences.
inputsfortesting,whereasourworkondebuggersalsoconsiders
the problem ofhowandwhen to create useractions.
DifferentialTesting. Thetermłdifferentialtestingžwasoriginally
coined by McKeeman [24]. Apart from compilers, it has been ap-
pliedtootherdevelopmenttools,suchasrefactoringengines[ 9],
symbolicexecutionengines[ 17],x86disassemblers[ 28],andbinary
lifters [18]. Our work differs by generating not only programs but
alsodebuggeractionsassociatedwiththeprogram,andbyusing
executionfeedbacktoguidethegeneration.Beyonddevelopertools,
differential testing can, e.g., be applied to code clones [ 41], similar
library implementations [ 37], and supposedly behavior-preserving
subclasses[ 32].Arelatedconceptis N-version-programming ,where
multiple versions of a program are created to increase fault tol-
erance.Experimentswithithaveshownthatsimilarerrorswere
made by independent programmers [ 19], hinting at a potential
shortcomingofourapproach:ifbothdebuggershavethesamebug,
DBDB cannotfind it.
Inspecting Warnings. A recurring problem in automated testing
isthehighnumberoftestcasesthatneedtobemanuallyinspected.
Our heuristic of drawing diverging behaviors from equivalence
classesinsteadofuniformlysamplingrelatestoworkbyPodgurski
et al. [31], who try to cluster software faults. For evaluating our
equivalence classes, we use a method similar to Chen et al . [8].
Anotherwaytoimprovehowusersinspectwarningsreportedbya
toolistoaskspecificquestionsthatmayeliminatefalsealarms[ 42].
InteractiveTesting. Unlikecompilers,debuggersareinteractive,
whichprecludesgeneratingalltestinginputsbeforehand.Ourin-
teractiveapproachthatalternatesbetweengeneratinginputsand
comparingoutputsissimilartopriorworkonautomatedtestingof
GUIs [13,15,23], whichare alsointeractive.
(Cross-)Browser Testing. Our implementation of DBDB targets
thedebuggersinFirefoxandChromium.Assuch,itisalsointhelineofmanyworksthatperformcross-browsertesting.RoyChoudhary
et al. [36]visually compare websites to identify issues in render-
ing by different browsers and Mesbah and Prasad [25]additionally
incorporateuserinteractionintothecomparisonofbrowserbehav-
ior. TreeFuzz compares JavaScript implementations of browsers by
fuzz-generatingJavaScript programs [ 29].
DebuggerandToolingCorrectness. Animportantsteptoimprove
the correctness of debuggers is specifying their intendedbehavior.
BernsteinandStark [3]formallydefinethesemanticsofadebugger
for a small functional language, but not for a debugger used in
practice. The differences discovered by DBDB may help to identify
situationsthatrequiremoreprecisespecification.Similarinspirit
to our work is the vision of Cadar and Donaldson [4]. They postu-
late that even though much effort goes into developing program
analyzers, the tools themselves are often not put under enough
scrutiny.Theycallforłanalyzingtheprogramanalyzeržandour
firststep isto find bugsindebuggers.
7 CONCLUSION AND OUTLOOK
This paper presents the first approach for automatically testing
interactive debuggers. DBDB compares the behavior of two debug-
gers by exercising them with generated sequences of debugging
actions. Our work builds upon a finite-state model that captures
commonfeaturesofreal-worlddebuggers.WeevaluateDBDBwith
the JavaScript debuggers of Firefox and Chromium, where we find
20 clear bugs and several other noteworthy differences. Eight of
these bugs have already been fixed by the respective developers,
andourresultshavespurreddiscussionsabouttheintendedseman-
tics of debuggers. While testing compilers has received significant
attention, we hope this work motivates more researchers to also
put other development tools under scrutiny. It is worrying that
debuggers, a fundamental development tool for understanding pro-
grams,arethemselvesbuggyandevenmoresothattheintended
semanticsofseeminglysimplefeatures,suchasstepping,arenot
preciselyspecified.Findingbehavioraldifferencesbetweenexisting
debuggers isafirststep towardsfixing theseproblems.
ACKNOWLEDGMENTS
This work was supported by the Hessian LOEWE initiative within
theSoftware-Factory4.0project,bytheGermanFederalMinistryof
EducationandResearchandbytheHessianMinistryofScienceand
the Arts within CRISP, and by the German Research Foundation
within the ConcSys andPerf4JSprojects.
619Feedback-DirectedDifferential TestingofInteractive Debuggers ESEC/FSE’18,November4ś9, 2018, Lake Buena Vista,FL,USA
REFERENCES
[1]TechnicalCommitteeISO/IECJTC1/SC22.2011. ISO/IEC9899:2011śInformation
technologyśProgramminglanguagesśC (thirded.). Standard. Availablefrom:
https://www.iso.org/standard/57853.html . Accessed:2017-11-15.
[2]TechnicalCommitteeISO/IECJTC1/SC22.2014. ISO/IEC14882:2014śInformation
technology ś Programming languages ś C++ (fourth ed.). Standard. Available
from:https://www.iso.org/standard/64029.html . Accessed:2017-11-15.
[3]Karen L. Bernstein and Eugene W. Stark. 1995. Operational Semantics of a
Focusing Debugger. Electronic Notes in Theoretical Computer Science 1 (1995),
13ś31.https://doi.org/10.1016/S1571-0661(04)80002-1
[4]Cristian Cadar and Alastair F. Donaldson. 2016. Analysing the Program Anal-
yser. InProceedings ofthe 38th InternationalConference onSoftware Engineering
Companion (ICSE’16) .ACM,NewYork,NY,USA,765ś768. https://doi.org/10.
1145/2889160.2889206
[5]David Callahan, Jack J. Dongarra, and D. Levine. 1988. Vectorizing compilers:
a test suite and results. In Proceedings Supercomputing ’88, Orlando, FL, USA,
November 12-17, 1988 . 98ś105. https://doi.org/10.1109/SUPERC.1988.44642
[6]Junjie Chen, Yanwei Bai, Dan Hao, Yingfei Xiong, Hongyu Zhang, and Bing Xie.
2017. Learning to prioritize test programs for compiler testing. In Proceedings
ofthe39thInternationalConferenceonSoftwareEngineering,ICSE2017,Buenos
Aires, Argentina, May 20-28, 2017 . 700ś711. https://doi.org/10.1109/ICSE.2017.70
[7]JunjieChen, Wenxiang Hu, Dan Hao, Yingfei Xiong,HongyuZhang, LuZhang,
andBingXie.2016. AnEmpiricalComparisonofCompilerTestingTechniques.In
Proceedingsofthe38thInternationalConferenceonSoftwareEngineering (ICSE’16) .
ACM,NewYork, NY, USA,180ś190. https://doi.org/10.1145/2884781.2884878
[8] Yang Chen,AlexGroce, ChaoqiangZhang,Weng-Keen Wong,XiaoliFern,Eric
Eide,andJohnRegehr.2013. TamingCompilerFuzzers.In Proceedingsofthe34th
ACMSIGPLANConferenceonProgrammingLanguageDesignandImplementation
(PLDI ’13) . ACM, New York, NY, USA, 197ś208. https://doi.org/10.1145/2491956.
2462173
[9]Brett Daniel, Danny Dig, Kely Garcia, and Darko Marinov. 2007. Automated
Testing of Refactoring Engines. In Proceedings of the the 6th JointMeeting of the
European Software Engineering Conference and the ACM SIGSOFT Symposium on
The Foundations of Software Engineering (ESEC-FSE ’07) . ACM, New York, NY,
USA,185ś194. https://doi.org/10.1145/1287624.1287651
[10]AlastairF.Donaldson,HuguesEvrard,AndreiLascu,andPaulThomson.2017.
AutomatedTestingofGraphicsShaderCompilers. Proc.ACMProgram.Lang. 1,
OOPSLA, Article93(Oct.2017),29pages. https://doi.org/10.1145/3133917
[11]Jack J. Dongarra, Mark Furtney, Steven P. Reinhardt, and J. Russell. 1991. Par-
allel loops - a test suite for parallelizing compilers: description and example
results.Parallel Comput. 17, 10-11 (1991), 1247ś1255. https://doi.org/10.1016/
S0167-8191(05)80036-5
[12]Technical Committee 39 Ecma International. 2017. Test262: ECMAScript Test
Suite (ECMA TR/104). https://github.com/tc39/test262 . Accessed:2017-11-15.
[13]MarkusErmuthandMichaelPradel.2016. MonkeySee,MonkeyDo:Effective
GenerationofGUITestswithInferredMacroEvents.In InternationalSymposium
onSoftwareTestingand Analysis(ISSTA) . 82ś93.
[14]Christian Holler, Kim Herzig, and Andreas Zeller. 2012. Fuzzing with Code
Fragments. In Proceedings of the 21st USENIX Conference on Security Symposium
(USENIXSecurity12) . USENIXAssociation, Berkeley, CA, USA,38ś38.
[15]Cuixiong Hu and Iulian Neamtiu. 2011. Automating GUI Testing for Android
Applications. In Proceedings ofthe 6th InternationalWorkshop on Automation of
Software Test (AST ’11) . ACM, New York, NY, USA, 77ś83. https://doi.org/10.
1145/1982595.1982612
[16]Ecma International. 2017. Standard ECMA-262 ś ECMAScript 2017 Language
Specification (8thed.). Standard. Availablefrom: https://www.ecma-international.
org/publications/standards/Ecma-262.htm . Accessed:2017-11-15.
[17]TimotejKapusandCristianCadar.2017. Automatictestingofsymbolicexecution
engines via program generation and differential testing. In Proceedings of the
32ndIEEE/ACMInternationalConferenceonAutomatedSoftwareEngineering,ASE
2017, Urbana, IL, USA, October 30 -November 03,2017 . 590ś600.
[18]Soomin Kim, Markus Faerevaag, Minkyu Jung, SeungIl Jung, DongYeop Oh,
JongHyupLee,andSangKilCha.2017. TestingIntermediateRepresentationsfor
BinaryAnalysis.In Proceedingsofthe32NdIEEE/ACMInternationalConference
on Automated Software Engineering (ASE 2017) . IEEE Press, Piscataway, NJ, USA,
353ś364.
[19]JohnC.KnightandNancyG.Leveson.1986. AnExperimentalEvaluationOfThe
Assumption Of Independence In Multi-Version Programming. IEEE Transactions
onSoftwareEngineering 12(1986), 96ś109.
[20]Vu Le, Mehrdad Afshari, and Zhendong Su. 2014. Compiler Validation via Equiv-
alence Modulo Inputs. In Proceedings of the 35th ACM SIGPLAN Conference onProgrammingLanguageDesignandImplementation (PLDI’14) .ACM,NewYork,
NY, USA,216ś226. https://doi.org/10.1145/2594291.2594334
[21] Xavier Leroy. 2009. Formal Verificationof a Realistic Compiler. Commun. ACM
52,7 (July2009),107ś115. https://doi.org/10.1145/1538788.1538814
[22]ChristopherLidbury,AndreiLascu,NathanChong,andAlastairF.Donaldson.
2015. Many-core Compiler Fuzzing. In Proceedings of the 36th ACM SIGPLAN
Conference on Programming Language Design and Implementation (PLDI ’15) .
ACM,NewYork, NY, USA,65ś76. https://doi.org/10.1145/2737924.2737986
[23]Aravind Machiry, Rohan Tahiliani, and Mayur Naik. 2013. Dynodroid: An Input
GenerationSystemforAndroidApps.In Proceedingsofthe20139thJointMeeting
on Foundations of Software Engineering (ESEC/FSE 2013) . ACM, New York, NY,
USA,224ś234. https://doi.org/10.1145/2491411.2491450
[24]William M McKeeman. 1998. Differential Testing for Software. Digital Technical
Journal10,1 (1998), 100ś107.
[25]AliMesbahandMukulR.Prasad.2011. AutomatedCross-BrowserCompatibility
Testing.In Proceedingsofthe33rdInternationalConferenceonSoftwareEngineering
(ICSE ’11) . ACM, New York, NY, USA, 561ś570. https://doi.org/10.1145/1985793.
1985870
[26]JanMidtgaard,MathiasNygaardJustesen,PatrickKasting,FlemmingNielson,and
Hanne Riis Nielson. 2017. Effect-driven QuickChecking of compilers. PACMPL1,
ICFP (2017), 15:1ś15:23. https://doi.org/10.1145/3110259
[27]MehryarMohri,FernandoPereira,andMichaelRiley.2002. Weightedfinite-state
transducers in speech recognition. Computer Speech & Language 16, 1 (2002),
69ś88.https://doi.org/10.1006/csla.2001.0184
[28]RobertoPaleari,LorenzoMartignoni,GiampaoloFresiRoglia,andDaniloBruschi.
2010. N-version Disassembly: Differential Testing of x86 Disassemblers. In
Proceedings of the 19th International Symposium on Software Testing and Analysis
(ISSTA’10) . ACM,NewYork, NY, USA,265ś274.
[29]Jibesh Patra and Michael Pradel. 2016. Learning to Fuzz: Application-Independent
FuzzTestingwithProbabilistic,GenerativeModelsofInputData . TechnicalReport
TUD-CS-2016-14664. TUDarmstadt.
[30]Filip Pizlo. 2013. Announcing SunSpider 1.0. https://webkit.org/blog/2364/
announcing-sunspider-1-0/ . Accessed:2017-11-15.
[31]Andy Podgurski, David Leon, Patrick Francis, Wes Masri, Melinda Minch, Ji-
ayangSun,andBinWang.2003. AutomatedSupportforClassifyingSoftware
FailureReports.In Proceedingsofthe25thInternationalConferenceonSoftware
Engineering (ICSE ’03) . IEEE Computer Society, Washington, DC, USA, 465ś475.
http://dl.acm.org/citation.cfm?id=776816.776872
[32]Michael Pradel and Thomas R. Gross. 2013. Automatic Testing of Sequential and
Concurrent Substitutability. In International Conference on Software Engineering
(ICSE). 282ś291.
[33]LLVM Project. 2017. LLVM Testing Infrastructure Guide. https://llvm.org/docs/
TestingGuide.html . Accessed:2017-11-15.
[34]Paul Purdom. 1972. A sentence generator for testing parsers. BIT Numerical
Mathematics 12,3 (1972), 366ś375.
[35]Norman Ramsey. 1994. Correctness of Trap-based Breakpoint Implementations.
InProceedings of the 21st ACM SIGPLAN-SIGACT Symposium on Principles of
ProgrammingLanguages (POPL’94) .ACM,NewYork,NY,USA,15ś24. https:
//doi.org/10.1145/174675.175188
[36]Shauvik Roy Choudhary, Husayn Versee, and Alessandro Orso. 2010. WEB-
DIFF: Automated Identification of Cross-browser Issues in Web Applications.
InProceedings of the 2010 IEEE International Conference on Software Mainte-
nance (ICSM’10) .IEEEComputerSociety,Washington,DC,USA,1ś10. https:
//doi.org/10.1109/ICSM.2010.5609723
[37]MarijaSelakovic,MichaelPradel, RezwanaKarim Nawrin,and FrankTip. 2018.
TestGenerationforHigher-OrderFunctionsinDynamicLanguages.In OOPSLA.
[38]GCCteam.2017. GCCTestingEfforts. https://gcc.gnu.org/testing/ . Accessed:
2017-11-15.
[39]Xuejun Yang, Yang Chen, Eric Eide, and John Regehr. 2011. Finding and Un-
derstanding Bugs in C Compilers. In Proceedings of the 32nd ACM SIGPLAN
Conference on Programming Language Design and Implementation (PLDI ’11) .
ACM,NewYork, NY, USA,283ś294. https://doi.org/10.1145/1993498.1993532
[40]QirunZhang,ChengnianSun,andZhendongSu.2017. SkeletalProgramEnu-
merationfor Rigorous CompilerTesting.In PLDI.
[41]TianyiZhangandMiryungKim.2017.Automatedtransplantationanddifferential
testing for clones. In Proceedings of the 39th International Conference on Software
Engineering, ICSE 2017, Buenos Aires, Argentina, May 20-28, 2017 . 665ś676. https:
//doi.org/10.1109/ICSE.2017.67
[42]Xin Zhang,Radu Grigore,XujieSi,and MayurNaik. 2017. Effective interactive
resolution of static analysis alarms. PACMPL 1, OOPSLA (2017), 57:1ś57:30.
https://doi.org/10.1145/3133881
620