Is the Cure Worse Than the Disease?
OverÔ¨Åtting in Automated Program Repair
Edward K. Smith
 Earl T. Barr? Claire Le Goues‚Ä† Yuriy Brun
University of Massachusetts?University College, London‚Ä†Carnegie Mellon University
Amherst, MA, USA London, UK Pittsburgh, PA, USA
{tedks, brun}@cs.umass.edu, e.barr@ucl.ac.uk, clegoues@cs.cmu.edu
ABSTRACT
Automated program repair has shown promise for reducing the sig-
niÔ¨Åcant manual effort debugging requires. This paper addresses a
deÔ¨Åcit of earlier evaluations of automated repair techniques caused
by repairing programs and evaluating generated patches‚Äô correctness
using the same set of tests. Since tests are an imperfect metric of
program correctness, evaluations of this type do not discriminate be-
tween correct patches and patches that overÔ¨Åt the available tests and
break untested but desired functionality. This paper evaluates two
well-studied repair tools, GenProg and TrpAutoRepair, on a pub-
licly available benchmark of 998 bugs, each with a human-written
patch. By evaluating patches using tests independent from those
used during repair, we Ô¨Ånd that the tools are unlikely to improve
the proportion of independent tests passed, and that the quality of
the patches is proportional to the coverage of the test suite used
during repair. For programs that pass most tests, the tools are as
likely to break tests as to Ô¨Åx them . However, novice developers also
overÔ¨Åt, and automated repair performs no worse than these develop-
ers. In addition to overÔ¨Åtting, we measure the effects of test suite
coverage, test suite provenance, and starting program quality, as
well as the difference in quality between novice-developer-written
and tool-generated patches when quality is assessed with a test suite
independent from the one used for patch generation.
Categories and Subject Descriptors:
D.1.2 [Automatic Programming]: Program modiÔ¨Åcation
D.2.5 [Testing and Debugging]: Testing tools
General Terms: Experimentation
Keywords: automated program repair, empirical evaluation, inde-
pendent evaluation, GenProg, TrpAutoRepair, I NTRO CLASS
1. INTRODUCTION
Automated program repair [4,13,17,18,26,28,29,32,36,37,39,39,
43,46,47,50,55,58,60,61] holds great potential to reduce debugging
costs and improve software quality. For example, GenProg quickly
and cheaply generated patches for 55 out of 105 C bugs [32], while
PARshowed comparable results on 119 Java bugs [29]. While some
techniques validate patch correctness with respect to user-provided
or inferred contracts [26, 46, 60], a larger proportion use test cases.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proÔ¨Åt or commercial advantage and that copies bear this notice and the full cita-
tion on the Ô¨Årst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciÔ¨Åc permission
and/or a fee. Request permissions from permissions@acm.org.
ESEC/FSE‚Äô15 , August 30‚ÄìSeptember 4, 2015, Bergamo, Italy.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-3675-8/15/08 . . . $15.00.
http://dx.doi.org/10.1145/2786805.2786825.The most common prior evaluations of automatic repair provide
evidence of techniques‚Äô feasibility with respect to this test-case-
based deÔ¨Ånition of patch correctness (e.g., [17, 35, 46, 60]). How-
ever, in practice, test suites are rarely exhaustive [51], and repair
techniques must avoid breaking undertested functionality. When
evaluations of repair techniques use the same test cases to both
construct patches and validate their correctness, they fail to measure
if, or to what degree, those repair techniques break undertested func-
tionality. In our review of the literature, most of the prior evaluations
of automated repair techniques that relied on test cases or workloads
to validate candidate patches failed to evaluate those patches inde-
pendently of patch construction. More recent work (e.g., [50, 64])
has begun to consider independent quality measures, though less
extensively than we do here. And while some evaluations have
used humans to independently measure repair acceptability [19, 29]
and maintainability [21], unlike our work, they neither directly nor
objectively evaluate functional patch correctness. Meanwhile, our
recent evaluation of SearchRepair uses the same methodology as
the evaluation we propose here [28].
This paper focuses on repair techniques that use test cases or
workloads to validate patch construction, or generate and validate
(G&V ) techniques. As Section 2 describes, G&V techniques are
worth our investigation because they have broad applicability to
mature, deployed, legacy software. (Our evaluation does not use
mature legacy software, but is motivated by the techniques‚Äô appli-
cability to such software.) Investigating other approaches, such as
synthesis-based repair [26, 46, 60] techniques, is also of great value,
but is outside the scope of this paper.
Our contribution is a controlled investigation of GenProg [35, 62]
and TrpAutoRepair [49], both test-case-guided, search-based auto-
matic program repair tools with freely available implementations
that scale to large programs. The evaluation identiÔ¨Åes the circum-
stances under which these techniques succeed, as well as those
under which they break undertested functionality despite producing
patches that pass all test cases used for patch construction. The eval-
uation uses two test suites per program: one suite to construct the
patch and another to evaluate it. To borrow from machine learning
vocabulary, one test suite is training data used to construct a patch,
and the other is evaluation orheld-out data used to evaluate the
quality of the patch. Patches that are overly speciÔ¨Åc to the training
tests and fail to generalize to the held-out tests overÔ¨Åt to the training
tests. Techniques that produce overÔ¨Åtting patches tend to Ô¨Åx certain
program behavior while breaking other behavior.
The goals of our study are to (1) evaluate the quality of auto-
matically-generated patches independently of their construction,
and (2) measure the effects of properties of the input program and
test suite on patch quality. This requires a large corpus of bugs in
programs that each has at least two independent, exhaustive with
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proÔ¨Åt or commercial advantage and that copies bear this notice and the full citation
on the Ô¨Årst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speciÔ¨Åc permission and/or a
fee. Request permissions from Permissions@acm.org.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ESEC/FSE‚Äô15 , August 30 ‚Äì September 4, 2015, Bergamo, Italy
ACM. 978-1-4503-3675-8/15/08...$15.00
http://dx.doi.org/10.1145/2786805.2786825
532
respect to some speciÔ¨Åcation representation test suites. We produce
theINTRO CLASS dataset [34] for our evaluation by collecting 998
student-written programs with defects, submitted as homework in
a freshman programming class, and all with student-written, bug-
Ô¨Åxing patches. Each program is accompanied by two independent
test suites: a black-box test suite written by the course instructor
to the natural-language speciÔ¨Åcation, and a white-box test suite
constructed using the symbolic execution engine Klee [12] on a
reference solution. This dataset is publicly available: http://
repairbenchmarks.cs.umass.edu . Our study admittedly uses
small programs written by novice programmers, which threatens the
generalizability of our results. We make this tradeoff because we
need many programs with multiple exhaustive test suites to evaluate
the repair properties we are interested in. Understanding repair
techniques at this scale increases understanding of repair techniques
in general. Our study increases the understanding of how, why, and
under what circumstances search-based repair succeeds and fails.
To the best of our knowledge, this is the Ô¨Årst systematic effort to
evaluate the correctness of automated repair with respect to objec-
tive independent measures. We measure overÔ¨Åtting and character-
ize repair quality along several previously unexplored dimensions,
including test suite coverage, quality, and provenance. We also
explicitly compare automatically generated and novice-developer-
written patches with respect to functionality, as opposed to human
judgments. We Ô¨Ånd that:
GenProg and TrpAutoRepair are less likely to repair programs
that fail more training tests.
Patches overÔ¨Åt to the training test suite, often breaking under-
tested functionality. Patch minimization does not reduce this
effect.
Higher coverage test suites lead to higher quality patches. Patches
generated with lower coverage test suites overÔ¨Åt more.
TrpAutoRepair is more likely to break undertested functionality
when patching programs that pass more tests to start with, such
that the ‚Äúpatched‚Äù program is often worse than the un-patched
program. GenProg patches overÔ¨Åt less for programs that pass
more tests, but do not substantially improve test suite perfor-
mance on the held-out set.
Using human-generated, requirements-based test suites to guide
automated repair leads to higher quality patches than using auto-
matically generated test suites.
Novice developers also overÔ¨Åt to provided test suites when Ô¨Åxing
their own programs. However, neither tool overÔ¨Åts signiÔ¨Åcantly
less than novice developers.
GenProg and TrpAutoRepair can often generate multiple patches
for the same bug. Combining multiple patches can slightly de-
crease overÔ¨Åtting, but the practical effect is quite small.
The rest of this paper is structured as follows. Section 2 summa-
rizes automated repair. Section 3 describes our dataset. Section 4
discusses the results of a series of experiments measuring how the
quality of inputs to automatic program repair affects the output
patches. Section 5 presents a case study demonstrating overÔ¨Åtting.
Finally, Section 6 acknowledges threats to the validity of our results,
Section 7 places our work in the context of related research, and
Section 8 summarizes our contributions.
2. AUTOMATED PROGRAM REPAIR
Automatic repair techniques can be classiÔ¨Åed broadly into two
classes: (1) Generate-and-validate (G&V ) techniques create can-
didate patches (often via search-based software engineering [25])and then validate them, typically through testing (e.g., [4, 13, 14,
17, 18, 28, 29, 37, 39, 43, 47, 50, 55, 58, 61, 62]. (2) Synthesis-based
techniques use constraints to build correct-by-construction patches
via formal veriÔ¨Åcation or inferred or programmer-provided contracts
or speciÔ¨Åcations (e.g., [26, 46, 60]. This paper focuses on G&V
techniques for two reasons:
First, such a focus is necessary because while both synthesis-
based and G&V techniques share high-level goals, they work best in
different settings, and have different limitations and challenges. For
example, the performance of synthesis-based repair relates strongly
to the power of the underlying proof system, which is typically
irrelevant to G&V repair.
Second, G&V is particularly promising for deployed, legacy
software, because it typically does not require that the program
be written in a novel language or include special annotations or
speciÔ¨Åcations. As examples, Clearview [47], GenProg, Par, and
Debroy and Wong [18] have successfully Ô¨Åxed bugs in legacy soft-
ware. Although new projects appear to be increasingly adopting
contracts [20], their penetration into existing systems and languages
remains limited. Few maintained contract implementations exist
for widely-used languages such as C. As an example, as of March
2014, in the Debian main repository, only 43 packages depended
onZope.Interfaces (by far the most popular Python, contract-
speciÔ¨Åc library in Debian) out of a total of 4,685 Python-related
packages. For Ubuntu, 144 out of 5,594 Python-related packages
depended on Zope.Interfaces . Synthesis-based techniques show
great promise for new or safety-critical systems written in suitable
languages, and adequately enriched with speciÔ¨Åcations. However,
the signiÔ¨Åcance of defects in existing software demands that research
attention be paid at least in part to techniques that address software
quality in existing systems written in legacy languages. Since legacy
codebases often are often idiosyncratic to the point of not adher-
ing to the speciÔ¨Åcations of their host language [9], it might not be
possible even to add contracts to such projects.
G&V repair works by generating multiple candidate patches that
might address a particular bug and then validating the candidates to
determine if they constitute a repair. In practice, the most common
form of validation is testing. A G&V approach‚Äôs input is therefore
a program and a set of test cases. The passing tests validate the
correct, required behavior, and the failing tests identify the buggy
behavior to be repaired. G&V approaches differ in how they choose
which locations to modify, which modiÔ¨Åcations are permitted, and
how the candidates are evaluated.
Of existing G&V techniques, to the best of our knowledge, Gen-
Prog [32,62], TrpAutoRepair [49], and AE [61] are the only publicly
available repair tools that both repair programs written in C and tar-
get general-purpose bugs (as opposed to focusing on one domain
of bugs, such as concurrency or integer overÔ¨Çow). Therefore, in
this paper, we use GenProg and TrpAutoRepair as exemplars of
G&V program repair. Unlike GenProg and TrpAutoRepair, AE is
deterministic, and so much of our experimental methodology does
not apply. However, we do Ô¨Ånd that AE similarly overÔ¨Åts to input
tests (Section 4.2).
GenProg [32, 62] uses a genetic programming heuristic [31] to
search the space of candidate repairs. Given a buggy program and
a set of tests, GenProg generates a population of random patches
by using statistical fault localization to identify which program el-
ements to change (those that execute only on failing test cases or
on both failing and passing text cases), and selecting elements from
elsewhere in the program to use as candidate patch code. The Ô¨Åtness
of each patch is computed by applying it to the input program and
running the result on the input test cases; a weighted sum of the
count of passed tests informs a random selection of a subset of the
533population to propagate into the next iteration. These patch can-
didates are recombined and mutated to form new candidates until
either a candidate causes the input program to pass all tests, or a pre-
set time or resource limit is reached. Because genetic programming
is a random search technique, GenProg is typically run multiple
times on different random seeds to repair a bug.
TrpAutoRepair [49] uses random search instead of genetic pro-
gramming to traverse the search space of candidate solutions. In-
stead of running an entire test suite for every patch, TrpAutoRepair
uses heuristics to select the most informative test cases Ô¨Årst, and
stops running the suite once a test fails. TrpAutoRepair limits its
patches to a single edit. It is more efÔ¨Åcient than GenProg in terms
of time and test case evaluations [49]. (TrpAutoRepair was also
published under the name RSRepair in ‚ÄúThe strength of random
search on automated program repair‚Äù by Yuhua Qi, Xiaoguang Mao,
Yan Lei, Ziying Dai, and Chengsong Wang in the 2014 International
Conference on Software Engineering; we refer to the original name
in this paper.)
There are three key hurdles that G&V must overcome to Ô¨Ånd
patches [61]. First, there are many places in the buggy program
that may be changed. The set of program locations that may be
changed and the probability than any one of them is changed at a
given time describes the fault space of a particular program repair
problem. GenProg and TrpAutoRepair tackle this challenge by
using existing fault localization techniques to identify good repair
candidates. Second, there are many ways to change potentially
faulty code in an attempt to Ô¨Åx it. This describes the Ô¨Åx space of
a particular program repair problem. GenProg and TrpAutoRepair
tackle this challenge using the observation that programs are often
repetitive [7, 22] and logic implemented with a bug in one place is
likely to be implemented correctly elsewhere in the same program.
GenProg and TrpAutoRepair therefore limit the code changes to
deleting constructs and copying constructs from elsewhere in the
same program. Finally, as a challenge that applies to GenProg
in particular, genetic programming is known to lead to bloat, in
which solutions contain more code than necessary to maximize
Ô¨Åtness [24]. GenProg minimizes code bloat post facto; prior work
has claimed that minimization reduces patches overÔ¨Åtting to the
training tests [35]. TrpAutoRepair only attempts single-edit patches,
and thus does not further minimize successful patches.
GenProg and TrpAutoRepair share sufÔ¨Åcient common features
to allow consistent empirical and theoretical comparisons. For
example, our experiments use the same fault localization strategy
and Ô¨Åx space weighting schemes for both. This allows us to focus
on particular experimental concerns and mitigates the threat that
unrelated differences between the algorithms confound the results.
However, the algorithms vary both in the way they traverse the
search space and in the way they evaluate candidate patches, and
thus we expect our Ô¨Åndings to generalize to other G&V techniques,
especially in light of recent successes in modeling and characterizing
the similarities in G&V approaches [61].
3. THE DATASET
This section describes the INTRO CLASS dataset [34] of 998 bugs
in versions of six small C programs, together with two types of tests
and human-written bug Ô¨Åxes. This dataset is available at:
http://repairbenchmarks.cs.umass.edu .
3.1 The Subject Programs
Our dataset is drawn from an introductory C programming class
at UC Davis with an enrollment of about 200 students. The use of
this anonymized dataset for research was approved by the UC Davis
IRB. To prevent identity recovery, students‚Äô names in the datasetprogram LoCtests buggy versionscomputationbb wb bb wb
checksum 13 6 10 29 49 checksum of a string
digits 15 6 10 91 172 digits of a number
grade 19 9 9 226 224 grade from score
median 24 7 6 168 152 median of 3 numbers
smallest 20 8 8 155 118 min of 4 numbers
syllables 23 6 10 109 130 count syllables
total 114 42 53 778?845?
?998 of the 778 black box and 845 white box buggy versions are
unique.
Figure 1: The instructor-written implementations of the six
subject programs vary in size from 13 to 24 LOC. The black-
box (bb) tests are instructor-written to cover the speciÔ¨Åcation.
The white-box (wb) tests are automatically generated for com-
plete coverage of a reference implementation. The programs‚Äô
revision histories contain 778 versions that pass at least one and
fail at least one bb test, and 845 versions that pass at least one
and fail at least one wb test, with a total of 998 unique buggy
versions.
were securely hashed, and all code comments were removed.
The dataset includes six programming assignments (Figure 1).
Each assignment requires students to individually write a program
that satisÔ¨Åes a provided set of requirements. The requirements were
of relatively high quality, and a good deal of effort was spent to
make them as clear as possible, given their role in a beginning
programming class. Further, the students were taught to Ô¨Årst under-
stand the requirements, then design, then code, and Ô¨Ånally test their
submissions.
Students working on their assignments submit their code by push-
ing to a personal git repository. The students may submit as many
times as they desire without penalty until the deadline. On every
submission, a system called GradeBot runs the student program
against a set of black-box test cases (described next), comparing
the output against an instructor-written reference implementation.
The students learn how many tests run and how many pass, but
no other information. The grade is proportional to the number of
tests the latest submission (before the deadline) passes. Students do
notknow the test cases used by the GradeBot, so when a submis-
sion fails a test, the student has to carefully reconsider the program
requirements.
TheG&V techniques evaluated in this paper rely on a pool of
candidate source code elsewhere in the program. We were initially
concerned that the programs‚Äô small size will impede patch construc-
tion. However, as Section 4.1 shows, automated repair was often
able to produce patches. Further, we found that increasing the pool
of candidate source code lines showed neither an increase in repair
rate nor a decrease in overÔ¨Åtting behavior.
3.2 Test Suites and Measure of Patch Quality
Each program has two test suites: a black-box test suite and a
white-box test suite. The instructor-written black-box test suite is
based solely on the program speciÔ¨Åcation. The instructor separated
the input space into equivalence partitions and selected an input
from each partition. While the instructor paid special attention to
writing high-quality test suites, the instructor is human and not infal-
lible. The white-box test suite achieves edge coverage (also called
534branch and structural coverage) on the instructor-written reference
implementation. We created the white-box test suite using KLEE,
a symbolic execution tool that automatically generates tests that
achieve high coverage [12]. When KLEE failed to Ô¨Ånd a covering
test suite, we manually added tests to achieve full edge coverage.
The black-box and white-box test suites were developed inde-
pendently and independently describe desired program behavior.
Because students can query how well their submissions do on the
black-box tests (without learning the tests themselves), they can use
the results of these tests to guide their development. A repair tool
can analogously use the black-box tests to guide automated repair.
We use the two test suites to measure functional patch quality.
When a human or a tool uses black-box tests to construct a patch,
we evaluate how well the patch performs on the held-out white-box
test suite. If the patch passes all black-box tests provided as input to
the repair tool but fails some white-box tests, then the patch overÔ¨Åts
to the black-box tests, and fails to generalize to the held-out tests.
We can similarly measure overÔ¨Åtting to white-box tests. Several
experiments described in Section 4 use this method for measuring
patch quality in terms of overÔ¨Åtting and generalizability (the inverse
of overÔ¨Åtting).
3.3 Buggy Program Versions
Because the homework is submitted to a git repository, student
submissions to GradeBot provide a detailed history of student efforts
to solve each problem. Inevitably, some submissions contain bugs,
in that they do not satisfy all of the requirements for the assignment.
We can approximate if a submission is buggy by evaluating its
performance on the two test suites. Many, though not all, of the Ô¨Ånal
submitted versions are correct. To identify a speciÔ¨Åc buggy version
of a program, pick a test suite (e.g., black-box) and Ô¨Ånd all versions
that pass at least one and fail at least one test in that suite. Overall,
we identiÔ¨Åed 778 buggy versions using the black-box suites, and
845 buggy versions using the white-box suites (Figure 1); the union
of these sets constitutes 998 unique buggy programs.
For each of the 998 versions, we ran each test and observed the
version‚Äôs behavior on that test. We observed 8,884 failures. The
overwhelming majority of errors were caused by incorrect output;
this accounted for 8,469 cases. Segmentation faults accounted for
76 test failures; other errors detected by program exit status codes
accounted for 254 errors. The remaining 85 errors were due to
timeouts, likely caused by inÔ¨Ånite loops.
4. EMPIRICAL EV ALUATION
We evaluate G&V repair via a series of controlled experiments
using the dataset from Section 3. Section 4.1 outlines our experimen-
tal procedure and reports baseline results for successful patching.
Section 4.2 examines overÔ¨Åtting in G&V repair and measures how
various factors affect overÔ¨Åtting. Section 4.3 compares G&V repair
to novice developers in terms of overÔ¨Åtting. Finally, Section 4.4
tests previously proposed approaches to combat overÔ¨Åtting.
4.1 Evaluation Methodology
This section outlines the methodology we use to evaluate GenProg
and TrpAutoRepair and presents baseline results. We use each
tool to attempt to repair each of the 778 program versions that
fail at least one speciÔ¨Åcation-based black-box test, providing the
black-box test suite as the training suite to both tools. For each
buggy version, we compute the black-box tests it passes and fails,
and then sample randomly those tests to produce 25%, 50%, 75%,
and 100% subsets of the training suite of the same pass-fail ratio
(rounding up to the nearest test). These test suite subsets represent
test suites of varying levels of coverage. We use the term scenariotool runs scenarios buggy programs
GenProg16104
62240=25:9%1428
3112=45:9%466
778=59:9%
TrpAutoRepair19326
62240=31:1%1298
3112=41:7%444
778=57:1%
(a)
‚óè ‚óè ‚óè‚óè‚óè‚óè
‚óè
‚óè ‚óè‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè
‚óè‚óè ‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè
0%20%40%60%80%100%
20% 40% 60% 80% 100%
before‚àírepair training suite passing rateRepair success rate‚óèGenProg TrpAutoRepair
(b)
Figure 2: (a) GenProg and TrpAutoRepair patch creation rates.
(b) GenProg‚Äôs and TrpAutoRepair‚Äôs scenario patch creation
rates (producing at least one patch that passes all the black-
box tests in 20 attempts on different seeds) improve as the num-
ber of passing before-repair training suite tests increased. This
relationship is signiÔ¨Åcant for GenProg (p <0:01) and for Trp-
AutoRepair (p <0:05).
to refer to the pair consisting of the buggy program version and
a coverage measure. Thus for black-box tests, there are 778
4=3;112scenarios. We attempt to repair each scenario 20 times,
providing a new randomly generated seed each time, for a total
of3;11220=62;240attempted repairs; 20 repair attempts is
sufÔ¨Åcient to achieve statistically signiÔ¨Åcant results. As is standard
for GenProg and TrpAutoRepair, when repairing a buggy version,
the tool uses the code in that version to construct potential patches.
When a tool exits successfully after generating a patch that passes
100% of the training suite, we run the (KLEE-generated white-box)
held-out evaluation suite over the patch to measure its quality.
Figure 2(a) summarizes the fraction of the time each run, each
scenario, and each buggy version was Ô¨Åxed by each of the two tools.
While fewer GenProg runs Ô¨Ånd patches (25.9% vs. 31.1%), GenProg
is able to patch more scenarios (45.9% vs. 41.7%) and more distinct
buggy program versions (59.9% vs. 57.1%) than TrpAutoRepair.
Figure 2(b) shows the relationship between the number of black-
box tests the un-patched buggy program fails and patch success.
GenProg is slightly more likely to patch buggy versions that fail
fewer tests: A linear regression conÔ¨Årms a slight positive trend (with
signiÔ¨Åcance, p<0:01). The similar trend detected for TrpAutoRe-
pair also statistically signiÔ¨Åcant ( p<0:05).
Based on these results, we conclude that GenProg and TrpAuto-
Repair generate patches sufÔ¨Åciently often to enable further empirical
experiments.
All relationships reported in the following sections are evaluated
via linear regression, unless otherwise speciÔ¨Åed. While we give
signiÔ¨Åcance when appropriate, none of the detected relationships
had large effects measured by R2, and we do not conclude that any
of the relationships are strongly linear.
535GenProg TrpAutoRepair
‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè
‚óè ‚óè‚óè
‚óè ‚óè ‚óè ‚óè‚óè‚óè
‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè‚óè‚óè‚óè‚óè ‚óè ‚óè ‚óè
0%25%50%75%100%
25% 50% 75% 100% 25% 50% 75% 100%
available training suite coverageevaluation suite passing rateFigure 3: The coverage of the test suite GenProg and TrpAuto-
Repair use to repair the buggy program strongly correlates
(p<0:001) with the portion of the white-box tests the patched
program passes.
4.2 OverÔ¨Åtting
Research Question 1: How often do the patches produced
byG&V techniques overÔ¨Åt to the training suite, and fail to
generalize to the held out evaluation suite, and thus ultimately
to the program speciÔ¨Åcation?
Having shown that the repair techniques often Ô¨Ånd patches that
cause a program to pass all of the training test suite, we next eval-
uate the quality of those patches. SpeciÔ¨Åcally, we are interested
in learning if G&V techniques produce patches that overÔ¨Åt to the
training test suite.
We Ô¨Ånd that the median GenProg patch (which passes 100%
of the speciÔ¨Åcation-based black-box training suite, by deÔ¨Ånition)
passes only 75.0% of the KLEE-generated white-box evaluation
suite (mean 68.7%). The median TrpAutoRepair patch passes 75.0%
of the evaluation suite (mean 72.1%).1
We conclude that tool-generated patches often overÔ¨Åt to the train-
ing suite used in constructing the patch. For programs that pass
most tests before repair, both GenProg and TrpAutoRepair are more
likely to decrease the correctness of the program (as measured by the
number of independent tests it passes) under repair than to increase
it.
Research Question 2: How does training suite coverage affect
patch overÔ¨Åtting?
In practice, test suites are typically incomplete. To measure how
G&V techniques perform when given incomplete test suites, we
use subsets of the speciÔ¨Åcation-based black-box test suites as the
training suites, and measure the relationship between the coverage
of the training suite and the patch‚Äôs overÔ¨Åtting.
For each buggy program, we use the test suite sampling procedure
from Section 4.1 to produce 25%-, 50%-, 75%-, and 100%-sized
1For completeness, we also evaluated if AE [61], another publicly
available G&V tool, overÔ¨Åts. AE produces patches for 35.7% of
buggy programs, and the median patch passes 50.0% of the eval-
uation suite (mean 64.2%). Because AE is deterministic and only
produces one patch per buggy program version, our other experi-
ments that rely on using multiple random seeds do not apply.
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè ‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè ‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè ‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè ‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè ‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè ‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè ‚óè‚óè‚óè‚óè ‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè ‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè ‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè ‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè ‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè ‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè ‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè ‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè ‚óè‚óè‚óè
0%25%50%75%100%
25% 50% 75% 100%
before‚àírepair training passing rateafter‚àírepair evaluation passing rate‚óèGenProg TrpAutoRepair
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè ‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè ‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè ‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè ‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè ‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè ‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè ‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè ‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè ‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè ‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè ‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè ‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè ‚óè ‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè ‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè ‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè ‚óè‚óè‚óè
‚óè‚óè‚óè ‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè ‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè ‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè ‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè ‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè ‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè ‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè ‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚àí100%‚àí50%0%50%100%
25% 50% 75% 100%
before‚àírepair training passing ratechange in evaluation test passing rate‚óèGenProg TrpAutoRepairFigure 4: Top: The fraction of evaluation tests the patched pro-
gram passes is signiÔ¨Åcantly positively correlated with the frac-
tion of training tests the un-patched version passes (p <0:001
for both tools). Bottom: However, for un-patched programs
that pass more of the training tests to start with, both tools
are more likely to break functionality than Ô¨Åx it. The corre-
lation between before-repair training suite pass rate and the
evaluation tests Ô¨Åxed is signiÔ¨Åcantly negative for TrpAutoRe-
pair. GenProg exhibits a positive trend, but breaks more tests
than Ô¨Åxes for buggy programs that pass less than 75% of the
training suite (p <0:001 for both trends).
test suites that keep consistent the pass-fail ratio of every buggy
version, but vary the test suite coverage. As before, for each tool,
we repeat this process 20 times, each time resampling the test suites
and using a different random seed.
Figure 3 shows the relationship between training suite cover-
age and overÔ¨Åtting (the fraction of the held-out white-box tests the
patched program passes). For both GenProg and TrpAutoRepair,
higher-coverage training suites improve the quality (reduce the over-
Ô¨Åtting) of a patch: the patch passes more white-box tests, on average.
A linear regression conÔ¨Årms both positive trends (with signiÔ¨Åcance,
p<0:001).
We conclude that GenProg and TrpAutoRepair beneÔ¨Åt from high-
coverage test suites in repairing bugs. Using low-coverage test
suites, which are unfortunately common in practice, poses a risk of
automated patches that overÔ¨Åt to that test suite.
Research Question 3: How does the number of tests that a
buggy program fails affect the degree to which the generated
patches overÔ¨Åt?
536Section 4.1 showed that the number of training tests the buggy
version fails is related to a technique‚Äôs ability to produce a patch.
Now, we explore if it is also related to overÔ¨Åtting.
Figure 4 relates the quality of the generated patch as measured by
its performance on the held-out KLEE-generated white-box tests to
the number of training black-box tests the original program passes.
The top of Figure 4 shows that programs that pass more training
tests before repair are more likely to pass the evaluation tests post-
repair. Linear regression conÔ¨Årms the positive trend for both tools
(with signiÔ¨Åcance, p<0:001). However, the bottom of Figure 4
shows that both GenProg and TrpAutoRepair are also more likely to
break the held-out test cases than Ô¨Åx them when repairing programs
that initially pass most of the black-box tests. A linear regression
conÔ¨Årms a negative trend for TrpAutoRepair (with signiÔ¨Åcance, p<
0:001). GenProg exhibits a slight positive trend (with signiÔ¨Åcance,
p<0:001), but still tends to break tests rather than Ô¨Åx them for
most programs. The trendline intercepts with zero at x=75%, and
advances very little above this point.
We conclude that G&V repair presents a danger when Ô¨Åxing high-
quality programs that pass most of their test suites. The patches are
likely to overÔ¨Åt to the tests, breaking other, previously correct func-
tionality. For low-quality programs that fail many tests, GenProg
and TrpAutoRepair repair more functionality than they break, on
average.
Research Question 4: How does the training test suite‚Äôs prove-
nance (automatically generated vs. human-written) inÔ¨Çuence
the patches‚Äô overÔ¨Åtting?
We have shown that using low-coverage test suites to Ô¨Åx bugs
can lead to lower-quality patches. This suggests that automatic
test generation might be used to improve test suite coverage prior
to a repair attempt; such an approach would require an oracle to
generate expected test outputs, although perhaps in some situations,
the expected behavior could be deÔ¨Åned by the un-patched program
behavior. Here, we evaluate if automatically generated tests (gener-
ated with KLEE [12] as described in Section 3.2) are as effective for
use by G&V repair as human-written tests. We refer to the method
by which the tests are created as test provenance .
Figures 5(a) and 5(c) summarize the relationship between test
suite provenance and GenProg patch overÔ¨Åtting. When GenProg
repaired buggy programs using (all of) the black-box tests as the
training suite (Figure 5(a)), its patches did relatively well on the
white-box evaluation tests. However, the same was not true when
GenProg constructed patches using white-box tests as the training
suite, with the black-box tests as the held-out suite (Figure 5(c)).
In the latter case, GenProg overÔ¨Åt signiÔ¨Åcantly to the white-box
tests. Figure 5(e) directly compares the two provenance methods. A
two-sample test supports our conclusion that the black-box patches
pass more of the white-box tests than the white-box patches do the
black-box tests (with signiÔ¨Åcance, p<0:001). Cliff‚Äôs Delta test
reports a large magnitude effect (magnitude >0:5).
Similarly, Figures 5(b) and 5(d) summarize the effect of test suite
provenance on TrpAutoRepair patch quality, and Figure 5(f) di-
rectly compares the two provenance methods. The effect is nearly
identical to GenProg, although TrpAutoRepair has slightly worse
performance, even with black-box tests. The two-sample test simi-
larly supports this conclusion ( p<0:001), and a Cliff‚Äôs Delta test
reports a similar large magnitude effect.
We conclude that test suite provenance plays an important role in
GenProg-generated patch quality. Some methods for generating test
suites may be better suited for automated repair than others.GenProg TrpAutoRepair
0%20%40%60%
0% 20% 40% 60% 80% 100%
(a) White-box passing rate of
GenProg patches generated
with black-box tests.
0%20%40%60%
0% 20% 40% 60% 80% 100%(b) White-box passing rate of
TrpAutoRepair patches gener-
ated with black-box tests.
0%20%40%60%
0% 20% 40% 60% 80% 100%
(c) Black-box passing rate of
GenProg patches generated
with white-box tests.
0%20%40%60%
0% 20% 40% 60% 80% 100%(d) Black-box passing rate of
TrpAutoRepair patches gener-
ated with white-box tests.
‚óè
‚óè
‚óè ‚óè‚óè
‚óè‚óè
‚óè
‚óè ‚óè ‚óè ‚óè‚óè ‚óè
‚óè‚óè ‚óè
‚óè‚óè ‚óè ‚óè
‚óè‚óè ‚óè ‚óè
‚óè‚óè ‚óè
‚óè ‚óè‚óè ‚óè‚óè ‚óè ‚óè
‚óè‚óè
‚óè‚óè ‚óè
‚óè ‚óè‚óè
‚óè ‚óè ‚óè‚óè ‚óè
‚óè ‚óè ‚óè‚óè
‚óè‚óè
‚óè ‚óè‚óè
‚óè ‚óè
‚óè ‚óè‚óè ‚óè ‚óè
‚óè‚óè ‚óè ‚óè
‚óè ‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè ‚óè ‚óè ‚óè‚óè
‚óè‚óè ‚óè
‚óè ‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè ‚óè ‚óè ‚óè‚óè
‚óè‚óè ‚óè
‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè‚óè
0%20%40%60%80%100%
black‚àíbox white‚àíbox% of held‚àíout tests passed
(e) Direct comparison of Gen-
Prog patches trained with black-
box and white-box suites.
‚óè
‚óè
‚óè‚óè ‚óè ‚óè ‚óè ‚óè‚óè
0%20%40%60%80%100%
black‚àíbox white‚àíbox% of held‚àíout tests passed(f) Direct comparison of Trp-
AutoRepair patches trained with
black-box and white-box suites.
Figure 5: (a) and (b): When black-box tests guided repair
search, the resulting patches did well on the evaluation white-
box tests. (c) and (d): However, the same was not true when
using white-box tests to guide the search for patches. (e) and (f):
The direct comparisons show that patches generated using the
black-box suite generalize to evaluation tests much better than
patches generated using the white-box suite. (The line shows
the median, and the dot the mean.) For both tools, Wilcoxon
signed-rank tests detected a signiÔ¨Åcant difference, p <0:001
with a large Cliff‚Äôs Delta in both cases.
4.3 Do Tools Outperform Novice Developers?
One of the advantages of our dataset is that every buggy program
has an associated human Ô¨Åx corresponding to that program‚Äôs student
author‚Äôs Ô¨Ånal submission. The students who produced the programs
in our dataset are faced with a challenge similar to that presented
5370%20%40%60%
0% 20% 40% 60% 80% 100%(a) White-box passing rate
of novice-developer-written
patches using black-box tests.
‚óè
‚óè ‚óè‚óè
‚óè‚óè
‚óè
‚óè ‚óè ‚óè ‚óè‚óè ‚óè
‚óè‚óè ‚óè
‚óè‚óè ‚óè ‚óè
‚óè‚óè ‚óè ‚óè
‚óè‚óè ‚óè
‚óè ‚óè‚óè ‚óè‚óè ‚óè ‚óè
‚óè‚óè
‚óè‚óè ‚óè
‚óè ‚óè‚óè
‚óè ‚óè ‚óè‚óè ‚óè
‚óè ‚óè ‚óè‚óè
‚óè‚óè
‚óè ‚óè‚óè
‚óè ‚óè
‚óè ‚óè‚óè ‚óè ‚óè
‚óè‚óè ‚óè ‚óè
‚óè ‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè ‚óè ‚óè ‚óè‚óè
‚óè‚óè ‚óè
‚óè ‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè ‚óè ‚óè ‚óè‚óè
‚óè‚óè ‚óè
‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè‚óè
‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè
‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè
‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè
‚óè
‚óè‚óè ‚óè ‚óè ‚óè ‚óè‚óè
0%20%40%60%80%100%
GenProghuman
TrpAutoRepair% of white‚àíbox tests passed(b) Direct GenProg, novice devel-
oper, and TrpAutoRepair compar-
ison.
Figure 6: (a): Novice-developer-written patches also overÔ¨Åt to
the black-box tests used during development. (b) The median
(shown as the line) human-written patch overÔ¨Åts slightly less
than those generated by GenProg and TrpAutoRepair, but the
mean (shown as the dot) GenProg-generated patches overÔ¨Åt
slightly less than the others, in part because the student-written
patches show higher variance than both automatic techniques.
However, this is not statistically signiÔ¨Åcant.
to our repair tools: They write and submit code, gain information
about how many tests their code passes and fails, make changes,
and resubmit. Those who have taught introductory programming
courses know that students follow a number of search strategies
while constructing repairs, ranging from structured reasoning to
random search. This section compares the patches produced by
the automated repair tools to the results of novice developers‚Äô re-
pair attempts. As before, repair tools (and now, humans) have the
speciÔ¨Åcation-based black-box test suite available during repair to
serve as a training suite, and the KLEE-generated white-box tests
are held out and can be used to evaluate the quality of the repair.
Research Question 5: Do tool-generated patches overÔ¨Åt more
than novice-developer-written patches?
Figure 6(a) shows that student solutions do, in fact, overÔ¨Åt to the
provided test suites and often fail to generalize to held-out tests.
Figure 6 compares the quality of GenProg, novice developer, and
TrpAutoRepair patches.
Both GenProg and TrpAutoRepair patches have both a lower
mean and median passing rate for held-out tests than the student-
written patches. While there is a visual difference in Figure 6(b)
between student-written and tool-generated patches, the Wilcoxon
signed rank test reports no signiÔ¨Åcant difference between the sam-
ples. Patches generated by either tool demonstrate signiÔ¨Åcantly less
variability in quality than student-written patches.
Comparing automatically generated patches to novice-developer-
written patches might seem unfair, since repair tools can only access
tests that represent a partial speciÔ¨Åcation, while humans can reason
abstractly about the program speciÔ¨Åcation. However, while humans
can reason about program faults abstractly above the level of a repair
tool, they are also subject to a large array of cognitive biases [2, 38]
that can hamper their debugging effort. Repair tools have no such
biases, and will mechanically explore the solution space as guided
by an objective function, without becoming irrationally Ô¨Åxated on
particular solutions.4.4 Mitigating OverÔ¨Åtting
Research Question 6: Does GenProg‚Äôs patch minimization
reduce overÔ¨Åtting?
GenProg uses patch minimization, via delta debugging [63], to
reduce code bloat. TrpAutoRepair does not perform minimization,
because the produced patch is only ever a single edit. Intuitively, a
small change to a program is less likely to encode special behavior
that handles just the training tests in a separate way [62]. Thus far,
all results we have described for GenProg have used GenProg‚Äôs built-
in patch minimization procedure. We now investigate if disabling
this feature increases overÔ¨Åtting.
We compared unminimized patches produced by GenProg to their
minimized versions in terms of the number of black-box and white-
box tests the patched versions passed. In all experiments, regardless
of the tests used, paired Wilcoxon tests show that the test-passing
rates of the minimized and unminimized patches were drawn from
the same distribution, and fail to reject the null hypothesis ( p>0:1
in all cases, after Benjamini-Hochberg correction for false discovery
rates). This indicates that minimization does not reduce the degree
to which GenProg overÔ¨Åts.
Research Question 7: Can overÔ¨Åtting be averaged out by
exploiting randomness in the repair process? Do different
random seeds overÔ¨Åt in different ways?
Some repair tools, including GenProg and TrpAutoRepair, can
generate multiple patches for the same defect (such as when run on
multiple different random seeds). This affords a unique opportunity:
Even if patches do overÔ¨Åt to their test suites, it is possible that a
group of patches better represents the desired program behavior
than an individual patch. SpeciÔ¨Åcally, even if each patch overÔ¨Åts on
some subset of desired behavior, if each patch in a group encodes
most of that behavior, a group vote on the behavior may outperform
each individual patch. N-version patches may therefore provide an
avenue to mitigate overÔ¨Åtting. Human-written code typically lacks
sufÔ¨Åcient diversity [30] to enable true n-version programming [15],
but randomized G&V repair may not.
We created the n-version program Pnin the following way: For
each buggy version-test suite subset pair Pb, run GenProg on Pb
20 times. If fewer than three of the runs result in a patch, we
exclude this pair from this experiment. We call these ( n3) patched
versions P1p:::Pnp. Next, we create a new program, Pn, that on input
i, runs each of P1p:::Pnponi, and returns the output most frequently
returned by output by those program. If two or more return values
tie,Pnreturns one of those values at random.
Figure 7 shows that n-version patches constructed from GenProg‚Äôs
output do not perform statistically signiÔ¨Åcantly better than either
individual GenProg-generated patches or novice-developer-written
patches. The only case in which n-version programs outperformed
individual patches was when TrpAutoRepair constructed patches
using KLEE-generated white-box suites for training (not shown in
Figure 7). Recall that training on white-box suites produced poor-
quality patches (Research Question 4). While n-version TrpAuto-
Repair patches are statistically signiÔ¨Åcantly better than individual
TrpAutoRepair patches ( p<0:05), the Cliff‚Äôs Delta is negligible,
and n-version TrpAutoRepair patches do not signiÔ¨Åcantly outper-
form those written by novice developers.
We conclude that when tools can produce quality patches (using
high-quality test suites), there is insufÔ¨Åcient diversity in the patches
538‚óè
‚óè ‚óè‚óè
‚óè‚óè
‚óè
‚óè ‚óè ‚óè ‚óè‚óè ‚óè
‚óè‚óè ‚óè
‚óè‚óè ‚óè ‚óè
‚óè‚óè ‚óè ‚óè
‚óè‚óè ‚óè
‚óè ‚óè‚óè ‚óè‚óè ‚óè ‚óè
‚óè‚óè
‚óè‚óè ‚óè
‚óè ‚óè‚óè
‚óè ‚óè ‚óè‚óè ‚óè
‚óè ‚óè ‚óè‚óè
‚óè‚óè
‚óè ‚óè‚óè
‚óè ‚óè
‚óè ‚óè‚óè ‚óè ‚óè
‚óè‚óè ‚óè ‚óè
‚óè ‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè ‚óè ‚óè ‚óè‚óè
‚óè‚óè ‚óè
‚óè ‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè ‚óè ‚óè ‚óè‚óè
‚óè‚óè ‚óè
‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè‚óè
‚óè
‚óè‚óè ‚óè ‚óè ‚óè ‚óè‚óè
‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè
‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè
‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè
‚óè‚óè
0%20%40%60%80%100%
GenProg
GenProg n‚àíversionHuman
TrpAutoRepair
TrpAutoRepair n‚àíversionwhite‚àíbox passing rateFigure 7: Tool-generated patches and n-version programs
made up of those patches perform worse than humans-written
patches, on average. N-version GenProg programs under-
perform even the individual GenProg patches, and n-version
TrpAutoRepair programs perform negligibly worse than indi-
vidual TrpAutoRepair patches while not statistically differing
from human-written patches.
to further improve quality. However, when repair tools produce
poor quality patches, diversity sometimes provides a modest beneÔ¨Åt.
N-version programming may indeed provide an avenue to mitigate
the worst cases of overÔ¨Åtting.
5. CASE STUDY
Section 4.2 showed that test suite provenance has the largest effect
on the quality of automatically generated patches. This section de-
scribes a case study of a buggy student program and two patches that
GenProg produced for it using the white-box test suite to highlight
the ways that some test suites can lead to increased overÔ¨Åtting.
Themedian homework assignment asks students to produce a C
function that takes as input three integers and outputs their median.
Figure 8 shows the black- and white-box test suites for the median
program.
One of the student‚Äôs buggy (non-Ô¨Ånal) submissions to the home-
work was:
1int med(int n1, int n2, int n3) {
2 if ((n1==n2) || (n1==n3) ||
3 (n2<n1 && n1<n3) || (n3<n1 && n1<n2))
4 return n1;
5 if ((n2==n3) || (n1<n2 && n2<n3) ||
6 (n3<n2 && n2<n1))
7 return n2;
8 if (n1<n3 && n3<n2)
9 return n3;
10 }
This submission is close to correct. Despite its incorrect logic
(e.g., the equality checks on lines 2 and 5), it passes Ô¨Åve of the six
white-box and six of the seven the black-box tests. The execution
fails to reach a return statement for the Ô¨Åfth black-box and for the
second white-box tests, for which n3is the median and n1 > n2 .
Given this program and the white-box suite, GenProg generated
several patches of varying quality. One such low-quality, GenProg-
patched program is:
1int med(int n1, int n2, int n3) {
2 if ((n1==n2) || (n1==n3) || ((n3<n1) && (n1<n2)))
3 return n1;black-box tests white-box tests
med(2, 6, 8) = 6 med(0, 0, 0) = 0
med(2, 8, 6) = 6 med(2, 0, 1) = 1
med(6, 2, 8) = 6 med(0, 0, 1) = 0
med(6, 8, 2) = 6 med(0, 1, 0) = 0
med(8, 2, 6) = 6 med(0, 2, 1) = 1
med(8, 6, 2) = 6 med(0, 2, 3) = 2
med(9, 9, 9) = 9
Figure 8: White- and black-box suites for median .
4 if (n2<n1)
5 return n3;
6 if ((n2==n3) || ((n1<n2) && (n2<n3)) ||
7 ((n3<n2) && (n2<n1)))
8 return n2;
9 if ((n1 < n3) && (n3 < n2))
10 return n3;
11 }
One of the conditions in the check on line 2 has been removed,
and this program returns n1as the median if it is coincidentally
equal to either n2orn3, or if it is actually the median and n3 < n2 .
Ifn1is not the median, but n2 < n1 (the check moved to line 5),
this code will (possibly, but not necessarily incorrectly) return n3.
The rest of the logic is unaffected.
This patch addresses the original problem in the student‚Äôs code, at
least with respect to the white-box suite. This code is correct when
n1is the median and n3 < n2 ,n2is the median and n2 > n1 , or
n3is the median and n2n1. Although this code passes all of the
white-box tests (improving on the original student submission), it
passes fewer black-box tests than the original, failing tests 3 and 6
in Figure 8.
This patch is an excellent example of overÔ¨Åtting the Ô¨Åtness func-
tion, and highlights weaknesses in the white-box test suite: Many
of the inputs have repeated elements. As a result, the student‚Äôs
otherwise logically incorrect equality checks on lines 2 and 5 of
the original submission mask the larger problems in the low-quality
patch.
Running GenProg with the same white-box test suite but a dif-
ferent random seeds can lead to different patches for the same bug.
For example, for this buggy program, GenProg also produced the
following patched program:
1int med(int n1, int n2, int n3) {
2 if ((n1==n2) || (n1==n3) || ((n2<n1) && (n1<n3)) ||
((n3<n1) && (n1<n2)))
3 return n1;
4 if ((n2==n3) || ((n1<n2) && (n2<n3)) ||
5 ((n3<n2) && (n2<n1)))
6 return n2;
7 if ((n1 < n3) && (n3 < n2))
8 return n3;
9 else
10 return n3;
11 }
The incorrect equality checks on lines 2 and 4 remain. This patch
inserted return n3 into the else block of the last set of conditions
that seek to determine if n3is the median. Ignoring the equality
checks, this is actually a reasonable solution, because by that point,
the only remaining option should be that n3is the median.
For this buggy program, the student rewrote the logic consider-
ably, eliminating the equality checks on lines 2 and 4 and properly
handling the last set of conditionals:
5391int med(int n1, int n2, int n3) {
2 if ((n2<=n1 && n1<=n3) || (n3<=n1 && n1<=n2))
3 return n1;
4 if ((n1<n2 && n2<=n3) || (n3<=n2 && n2<n1))
5 return n2;
6 if ((n1<n3 && n3<n2) || (n2<n3 && n3<n1))
7 return n3;
8}
In this example, GenProg solutions overÔ¨Åt to the test suite, while
the student-written patch is more general. This example highlights
weaknesses in the white-box test suite, which fails to encode key
behavior. This raises interesting questions about the potential of
automatic test case generation to augment the input given to G&V
repair techniques; more work is required to improve the quality of
the output of such techniques before the two approaches can be
usefully integrated.
6. THREATS TO V ALIDITY
Our experiments may not generalize. We only experiment with
GenProg and TrpAutoRepair, two of several G&V repair techniques,
and our results may not extend to other automatic program repair
mechanisms. However, recent work has started to unify the the-
ory underlying G&V repair [61], suggesting that results from two
different techniques may extend to others. Our subjects are small
student-written programs, with fairly small test suites. Therefore,
our results may not generalize to large, real-world programs. How-
ever, this is a necessary tradeoff, as the goals of our study require
programs that can be tested exhaustively with respect to multiple
speciÔ¨Åcation representations. Understanding repair techniques at
the scale of our experiments increases understanding of the repair
techniques in general. Additionally, while our subjects‚Äô size allows
for a very large dataset for conducting controlled trials, it may also
affect the ability to Ô¨Ånd diverse patches. We ran 20 seeds per repair
effort, a relatively small number by the standards of metaheuristic
search algorithms, but comparable to previous program repair evalu-
ations. More attempts may have revealed more solutions. Finally,
we used the recommended GenProg parameters deÔ¨Åned in previ-
ous work [33]; a full parameter sweep is outside the scope of this
investigation.
Our INTRO CLASS dataset ‚Äî http://repairbenchmarks.cs.
umass.edu ‚Äî includes all the buggy versions, student-written so-
lutions, and test suites. This makes our experiments repeatable.
However, parts of the creation of the dataset were manual. While
the white-box suites were generated automatically to the extent pos-
sible, and black-box suites were generated by a rigorous manual
analysis of the requirements, at least the latter is subject to human
interpretation. Thus, a replication of our experiments on differ-
ent programs or with different test suites on our programs may be
affected by human subjectivity and may produce different results.
GenProg, TrpAutoRepair, and many other related repair tech-
niques rely on randomized algorithms. Evaluating systems that
involve randomized algorithms is particularly difÔ¨Åcult and requires
paying special attention to the sample sizes, statistical tests, cross-
validation, and uses of bootstrapping. Our work is consistent with
the guidelines for evaluating randomized algorithms [5] to enhance
the credibility of our Ô¨Åndings. SpeciÔ¨Åcally, we used a large sample
of 998 buggy student programs, controlled for a variety of potential
inÔ¨Çuencers in our experiments, and used Ô¨Åxed-effects regression
models and two sample tests along with false-discovery rate correc-
tion to lend statistical support to our Ô¨Åndings.7. RELATED WORK
Most prior evaluations of G&V repair techniques demonstrate by
construction that the technique is feasible and reasonably efÔ¨Åcient in
practice [17,28,35,37,39,43,46,47,58,60,62]. Some show that the
resulting patches withstand red team attacks [47], some illustrate
with a small number of examples that G&V -generated patches for se-
curity vulnerabilities protect against exploits and fuzzed variants of
those exploits on typical user workloads [35], and some consider the
fraction of a set of bugs their technique can repair [19,28,29,32,43].
These evaluations have demonstrated that G&V techniques can re-
pair a moderate number of bugs in medium-sized programs, as well
as evaluated the monetary and time costs of automatic repair [32],
the relationship between operator choices and test execution parame-
ters and success [33,61], and human-rated patch acceptability [1,29]
and maintainability [21]. However, these evaluations have generally
not used an objective metric of correctness independent of patch
construction. Our evaluation measures patch correctness indepen-
dently of patch construction. We empirically examine how test
suite coverage and provenance, number of test failures, and patch
minimization affect repair effectiveness, deÔ¨Åned by both success
and functional correctness. We perform these experiments using a
much larger number of bugs than ever before, designed to permit
controlled evaluations that isolate particular features of the inputs,
such that we can examine their effects on automatic repair in a
statistically signiÔ¨Åcant way.
Concurrent research is starting to evaluate repair techniques in
terms of overÔ¨Åtting [50, 58]. Evaluating the degree to which reli-
Ô¨Åxand GenProg introduce regression errors [58] is a step toward
the independent correctness evaluation we advocate here, where
we use independent test suites to measure patch quality. By con-
trast, those experiments use the subset of the original test suite that
does not execute any of the lines associated with the bug under
repair, ignoring speciÔ¨Åcally regressions a patch is most likely to
introduce. Poor-quality test suites result in patches that overÔ¨Åt to
those suites [50]. Our evaluation goes further, demonstrating that
high-quality, high-coverage test suites still lead to overÔ¨Åtting, and
identifying other relationships between test suite properties and
patch quality. Finally, prior and concurrent human evaluations of au-
tomatically generated patches have measured acceptability [19, 29]
and maintainability [21]. While the human judgment is a criterion
not used by the repair tools for patch construction, it is fundamen-
tally different from the correctness criterion we use in our evaluation,
as it is often difÔ¨Åcult for humans to spot bugs even when told exactly
where to look for them [45]. Meanwhile, our recent evaluation
of SearchRepair uses the same methodology as the evaluation we
present here [28].
Our work evaluates automated repair so that it can be improved.
Empirical studies of Ô¨Åxes of real bugs in open-source projects can
also improve repair by helping designers select change operators
and search strategies [27, 64]. Understanding how automated repair
handles particular classes of errors, such as security vulnerabili-
ties [35, 47] can guide tool design. For this reason, some auto-
mated repair techniques focus on a particular defect class, such
as buffer overruns [54, 57], unsafe integer use in C programs [17],
single-variable atomicity violations [26], deadlock and livelock de-
fects [36], concurrency errors [37], and data input errors [4]. Other
techniques tackle generic bugs. For example, the ARMOR tool
replaces buggy library calls with different calls that achieve the
same behavior [13], and reliÔ¨Åx uses a set of templates mined from
regression Ô¨Åxes to automatically patch generic regression bugs. Our
evaluation has focused on tools that Ô¨Åx generic bugs, but our method-
ology can be applied to focused repair as well.
540User-provided code contracts, or other forms of invariants, can
help to synthesize correct-by-construction patches, e.g., via AutoFix-
E [46, 60] (for Eiffel code) and SemFix [43] (for C). DirectFix [39]
aims to synthesize minimal patches to be less prone to overÔ¨Åtting,
but only works for programs using a subset of C language features,
and has only been tested on small programs. Synthesis techniques
provide the beneÔ¨Åt of provable correctness for patches, but require
contracts, so they are unsuitable for legacy systems. Synthesis
techniques can also construct new features from examples [16, 23],
rather than address existing bugs. Our work has focused on G&V ap-
proaches, and investigating overÔ¨Åtting and patch quality in synthesis-
based techniques is a complementary and worthwhile pursuit.
The techniques evaluated in this paper, GenProg and TrpAuto-
Repair, are representative of G&V approaches. Our work does not
create a new bug-Ô¨Åxing technique, but rather evaluates existing tech-
niques in a new way to expose previously hidden limitations to G&V
program repair. Our Ô¨Åndings may extend to other search-based or
test suite-guided repair techniques (e.g., [6,18,29,39,43,44,47,61]).
Monperrus [42] has recently discussed the challenges of experimen-
tally comparing program repair techniques. For example, the selec-
tion of test subjects (defects) can introduce evaluation bias [10, 48].
Our evaluation focuses precisely on the limits and potential of repair
techniques on a large dataset of defects, and controls for a variety of
potential inÔ¨Çuencers, addressing some of Monperrus‚Äô concerns [42].
Genetic programming tends to produce extraneous code that does
not contribute to the Ô¨Åtness of the solution [24, 56]. GenProg at-
tempts to mitigate this through solution minimization, which may
reduce the chances of breaking undertested functionality. OverÔ¨Åt-
ting is also a well-studied problem in machine learning [41]. Our
experiments suggest that minimization and overÔ¨Åtting are unrelated,
which is consistent with prior results in machine learning [52]. To
the best of our knowledge, ours is the Ô¨Årst consideration of this
relationship in the program repair domain.
G&V approaches fall in the space of search-based software en-
gineering [25], which adapts search methods, such as genetic pro-
gramming, to software engineering tasks. Search-based software
engineering has been used for developing test suites [40,59], Ô¨Ånding
safety violations [3], refactoring [53], and project management and
effort estimation [8]. Good Ô¨Åtness functions are critical to search-
based software engineering. Our Ô¨Åndings indicate that using test
cases alone as the Ô¨Åtness function leads to patches that may not
generalize to the program requirements, and more sophisticated
Ô¨Åtness functions may be required for search-based program repair.
N-version programming [15] combines multiple different pro-
grams trying to solve the same problem in the interest of achieving
resiliency and correctness through redundancy. N-version program-
ming works poorly with human-written systems because the errors
humans make do not appear to be independent [30]. Our evaluations
have shown that n-versions of automatically generated patches has
a minor positive effect but failed to fully generalize to the desired
behavior.
8. CONCLUSIONS AND IMPLICATIONS
G&V automated repair shows promise for reducing the manual
bug-Ô¨Åxing burden and improving software quality. However, if these
techniques are to gain practical traction, we must augment feasibility
demonstrations with qualitative evaluations that address the quality
and applicability. In this paper, we systematically evaluated the fac-
tors affecting the output quality of GenProg and TrpAutoRepair, two
representative G&V techniques, through a controlled evaluation on
a large set of programs written by novice developers with naturally
occurring bugs and human-written patches. Based on our Ô¨Åndings,
the open research challenges include:Repair techniques must go beyond testing on the training data
to characterize functional correctness. GenProg and TrpAutoRe-
pair produced patches for more than half of the bugs in our dataset
(59.9% and 57.1%, respectively). The ability to produce a patch
was correlated with input program quality as measured by the test
suites. However, those patches tended to overÔ¨Åt to the test suite used
to generate the patch. When using requirements-based (black-box)
tests, GenProg and TrpAutoRepair overÔ¨Åt signiÔ¨Åcantly less than
when using generated (white-box) tests. Interestingly, the novice
programmers (students) also overÔ¨Åt to the provided test cases. These
results highlight both the signiÔ¨Åcant promise of automatic repair and
the fact that more work is needed to improve repair output quality.
We advocate that future evaluations of G&V repair tools withhold
some portion of tests from the repair tool, at least some of which
share code-under-test with the tests exposing the buggy behavior.
This is similar to the machine learning evaluational technique of
cross-validation , and provides a higher level of conÔ¨Ådence that a
repair technique is able to repair isolated defects without introducing
regressions.
Automatic repair should be used in appropriate contexts. Both
test suite coverage and input program quality appear related to the
quality of the automatically generated patches. Higher-coverage test
suites were more likely to lead to more general patches. Patches pro-
duced for higher-quality programs were, at best, unlikely to improve
functionality, and at worst, likely to break existing functionality.
This suggests that automatic repair techniques might be best applied
early in the development lifecycle, though unfortunately, this is the
time when the program quality itself is likely low (reducing the
likelihood of repair success), and the test suite is least likely to be
comprehensive. Different repair techniques are likely to be useful at
different times, and more study is needed to explore this space.
The quality of repair test suites should be measured and im-
proved appropriately. The provenance of the test suites ‚Äî auto-
matically-generated or human-written ‚Äî had a striking relationship
with the resulting patch quality. Automatic test-input generation
techniques should Ô¨Åt naturally into a toolchain for automatic re-
pair, particularly when user-provided test cases fail to fully cover
the program functionality, or when critical functionality should be
independently tested post-repair, to ensure that overÔ¨Åtting has not
occurred. Our results suggest that more work is needed to fully un-
derstand and characterize test suite quality beyond coverage metrics
alone.
Patch diversity might improve repair quality. Low-quality patches,
especially those generated using automatically generated tests, demon-
strated sufÔ¨Åcient functional diversity to improve on the patched pro-
grams via plurality voting. Plurality voting may thus mitigate the
risks of low-quality test suites in the appropriate settings.
While G&V techniques have not yet become a silver bullet of
program repair, in some cases and settings, they already outperform
beginner developers. Our results suggest that if several shortcom-
ings are addressed, there is signiÔ¨Åcant promise that automated repair
techniques can be impactful and helpful parts of the software devel-
opment process.
9. ACKNOWLEDGMENTS
This work is supported by the National Science Foundation under
grants CCF-1446683, CCF-1446966, and CCF-1453508, and by
Microsoft Research via the Software Engineering Innovation Foun-
dation Award. Prem Devanbu and Ming Xiao were instrumental in
the creation of an earlier version of the student programs dataset and
early GenProg experiments [11].
54110. REFERENCES
[1]M. Abd-El-Malek, G. R. Ganger, G. R. Goodson, M. K. Reiter,
and J. J. Wylie. Fault-scalable Byzantine fault-tolerant services.
InACM Symposium on Operating Systems Principles (SOSP) ,
pages 59‚Äì74, Brighton, UK, 2005.
[2]R. E. Adamson. Functional Ô¨Åxedness as related to problem
solving: A repetition of three experiments. Journal of Experi-
mental Psychology , 44(4):288‚Äì291, 1952.
[3]E. Alba and F. Chicano. Finding safety errors with ACO.
InConference on Genetic and Evolutionary Computation
(GECCO) , pages 1066‚Äì1073, London, England, UK, July
2007.
[4]M. Alkhalaf, A. Aydin, and T. Bultan. Semantic differential
repair for input validation and sanitization. In International
Symposium on Software Testing and Analysis (ISSTA) , pages
225‚Äì236, San Jose, CA, USA, July 2014.
[5]A. Arcuri and L. Briand. A practical guide for using statistical
tests to assess randomized algorithms in software engineering.
InACM/IEEE International Conference on Software Engineer-
ing (ICSE) , pages 1‚Äì10, Honolulu, HI, USA, 2011.
[6]A. Arcuri and X. Yao. A novel co-evolutionary approach to
automatic software bug Ô¨Åxing. In Congress on Evolutionary
Computation , pages 162‚Äì168, 2008.
[7]E. T. Barr, Y . Brun, P. Devanbu, M. Harman, and F. Sarro. The
plastic surgery hypothesis. In Symposium on the Foundations
of Software Engineering (FSE) , pages 306‚Äì317, Hong Kong,
China, November 2014.
[8]A. Barreto, M. Barros, and C. Werner. StafÔ¨Ång a software
project: A constraint satisfaction approach. Computers and
Operations Research , 35(10):3073‚Äì3089, 2008.
[9]A. Bessey, K. Block, B. Chelf, A. Chou, B. Fulton, S. Hallem,
C. Henri-Gros, A. Kamsky, S. McPeak, and D. Engler. A few
billion lines of code later: Using static analysis to Ô¨Ånd bugs
in the real world. Communications of the ACM , 53(2):66‚Äì75,
Feb. 2010.
[10] C. Bird, A. Bachmann, E. Aune, J. Duffy, A. Bernstein,
V . Filkov, and P. Devanbu. Fair and balanced?: Bias in bug-
Ô¨Åx datasets. In European Software Engineering Conference
and the ACM SIGSOFT International Symposium on Founda-
tions of Software Engineering (ESEC/FSE) , pages 121‚Äì130,
Amsterdam, The Netherlands, August 2009.
[11] Y . Brun, E. Barr, M. Xiao, C. Le Goues, and P. Devanbu. Evo-
lution vs. intelligent design in program patching. Technical Re-
porthttps://escholarship.org/uc/item/3z8926ks , UC
Davis: College of Engineering, 2013.
[12] C. Cadar, D. Dunbar, and D. Engler. KLEE: Unassisted and au-
tomatic generation of high-coverage tests for complex systems
programs. In USENIX Conference on Operating Systems De-
sign and Implementation (OSDI) , pages 209‚Äì224, San Diego,
CA, USA, 2008.
[13] A. Carzaniga, A. Gorla, A. Mattavelli, N. Perino, and M. Pezz√®.
Automatic recovery from runtime failures. In ACM/IEEE Inter-
national Conference on Software Engineering (ICSE) , pages
782‚Äì791, San Francisco, CA, USA, 2013.
[14] A. Carzaniga, A. Gorla, N. Perino, and M. Pezz√®. Automatic
workarounds for web applications. In ACM SIGSOFT Inter-
national Symposium on Foundations of Software Engineering
(FSE) , pages 237‚Äì246, Santa Fe, New Mexico, USA, 2010.
[15] L. Chen and A. Avi≈æienis. N-version programming: A fault-
tolerance approach to reliability of software operation. In
IEEE International Symposium on Fault-Tolerant Computing
(FTCS) , pages 3‚Äì9, 1978.[16] R. Cochran, L. D‚ÄôAntoni, B. Livshits, D. Molnar, and
M. Veanes. Program boosting: Program synthesis via crowd-
sourcing. In Symposium on Principles of Programming Lan-
guages (POPL) , pages 677‚Äì688, Mumbai, India, January 2015.
[17] Z. Coker and M. HaÔ¨Åz. Program transformations to Ô¨Åx C
integers. In ACM/IEEE International Conference on Software
Engineering (ICSE) , pages 792‚Äì801, San Francisco, CA, USA,
2013.
[18] V . Debroy and W. Wong. Using mutation to automatically
suggest Ô¨Åxes for faulty programs. In International Conference
on Software Testing, VeriÔ¨Åcation, and Validation , pages 65‚Äì74,
Paris, France, 2010.
[19] T. Durieux, M. Martinez, M. Monperrus, R. Sommerard, and
J. Xuan. Automatic repair of real bugs: An experience report
on the Defects4J dataset. CoRR , abs/1505.07002, 2015.
[20] H.-C. Estler, C. A. Furia, M. Nordio, M. Piccioni, and
B. Meyer. Contracts in practice. In International Symposium
on Formal Methods (FM) , pages 230‚Äì246, Singapore, May
2014.
[21] Z. P. Fry, B. Landau, and W. Weimer. A human study of patch
maintainability. In International Symposium on Software Test-
ing and Analysis (ISSTA) , pages 177‚Äì187, Minneapolis, MN,
USA, July 2012.
[22] M. Gabel and Z. Su. Testing mined speciÔ¨Åcations. In ACM SIG-
SOFT International Symposium on Foundations of Software
Engineering (FSE) , Cary, NC, USA, 2012.
[23] S. Gulwani. Automating string processing in spreadsheets us-
ing input-output examples. In Symposium on Principles of
Programming Languages (POPL) , pages 317‚Äì330, Austin,
TX, USA, 2011.
[24] S. Gustafson, A. Ek√°rt, E. Burke, and G. Kendall. Problem
difÔ¨Åculty and code growth in genetic programming. Genetic
Programming and Evolvable Machines , pages 271‚Äì290, Sept.
2004.
[25] M. Harman. The current state and future of search based soft-
ware engineering. In ACM/IEEE International Conference on
Software Engineering (ICSE) , pages 342‚Äì357, 2007.
[26] G. Jin, L. Song, W. Zhang, S. Lu, and B. Liblit. Automated
atomicity-violation Ô¨Åxing. In ACM SIGPLAN Conference on
Programming Language Design and Implementation (PLDI) ,
pages 389‚Äì400, San Jose, CA, USA, 2011.
[27] R. Just, D. Jalali, and M. D. Ernst. Defects4J: A database of
existing faults to enable controlled testing studies for Java
programs. In Proceedings of the International Symposium on
Software Testing and Analysis (ISSTA) , pages 437‚Äì440, San
Jose, CA, USA, July 2014.
[28] Y . Ke, K. T. Stolee, C. Le Goues, and Y . Brun. Repairing pro-
grams with semantic code search. In Proceedings of the 30th
IEEE/ACM International Conference On Automated Software
Engineering (ASE) , Lincoln, NE, USA, November 2015.
[29] D. Kim, J. Nam, J. Song, and S. Kim. Automatic patch gen-
eration learned from human-written patches. In ACM/IEEE
International Conference on Software Engineering (ICSE) ,
pages 802‚Äì811, San Francisco, CA, USA, 2013.
[30] J. C. Knight and N. G. Leveson. An experimental evaluation of
the assumption of independence in multiversion programming.
IEEE Transactions on Software Engineering (TSE) , 12(1):96‚Äì
109, 1986.
[31] J. R. Koza. Genetic Programming: On the Programming of
Computers by Means of Natural Selection . MIT Press, 1992.
[32] C. Le Goues, M. Dewey-V ogt, S. Forrest, and W. Weimer. A
systematic study of automated program repair: Fixing 55 out
542of 105 bugs for $8 each. In AMC/IEEE International Confer-
ence on Software Engineering (ICSE) , pages 3‚Äì13, Zurich,
Switzerland, 2012.
[33] C. Le Goues, S. Forrest, and W. Weimer. Representations and
operators for improving evolutionary software repair. In Con-
ference on Genetic and Evolutionary Computation (GECCO) ,
pages 959‚Äì966, Philadelphia, PA, USA, July 2012.
[34] C. Le Goues, N. Holtschulte, E. K. Smith, Y . Brun, P. De-
vanbu, S. Forrest, and W. Weimer. The ManyBugs and Intro-
Class benchmarks for automated repair of C programs. IEEE
Transactions on Software Engineering (TSE), in press , 2015.
[35] C. Le Goues, T. Nguyen, S. Forrest, and W. Weimer. Gen-
Prog: A generic method for automatic software repair. IEEE
Transactions on Software Engineering (TSE) , 38:54‚Äì72, 2012.
[36] Y . Lin and S. S. Kulkarni. Automatic repair for multi-threaded
programs with deadlock/livelock using maximum satisÔ¨Åability.
InInternational Symposium on Software Testing and Analysis
(ISSTA) , pages 237‚Äì247, San Jose, CA, USA, July 2014.
[37] P. Liu, O. Tripp, and C. Zhang. Grail: Context-aware Ô¨Åxing of
concurrency bugs. In ACM SIGSOFT International Symposium
on Foundations of Software Engineering (FSE) , pages 318‚Äì
329, Hong Kong, China, Nov. 2014.
[38] A. S. Luchins. Mechanization in problem solving: The effect
of Einstellung. Psychological Monographs , 54(6):i‚Äì95, 1942.
[39] S. Mechtaev, J. Yi, and A. Roychoudhury. DirectFix: Looking
for simple program repairs. In International Conference on
Software Engineering (ICSE) , Florence, Italy, May 2015.
[40] C. C. Michael, G. McGraw, and M. A. Schatz. Generating
software test data by evolution. IEEE Transactions on Software
Engineering (TSE) , 27(12):1085‚Äì1110, Dec. 2001.
[41] T. M. Mitchell. Machine Learning . McGraw-Hill, New York,
1997.
[42] M. Monperrus. A critical review of ‚ÄúAutomatic patch gen-
eration learned from human-written patches‚Äù: Essay on the
problem statement and the evaluation of automatic software
repair. In ACM/IEEE International Conference on Software
Engineering (ICSE) , pages 234‚Äì242, Hyderabad, India, June
2014.
[43] H. D. T. Nguyen, D. Qi, A. Roychoudhury, and S. Chandra.
SemFix: Program repair via semantic analysis. In ACM/IEEE
International Conference on Software Engineering (ICSE) ,
pages 772‚Äì781, San Francisco, CA, USA, 2013.
[44] M. Orlov and M. Sipper. Flight of the FINCH through the Java
wilderness. IEEE Transactions on Evolutionary Computation ,
15(2):166‚Äì182, Apr. 2011.
[45] C. Parnin and A. Orso. Are automated debugging techniques
actually helping programmers? In International Symposium
on Software Testing and Analysis (ISSTA) , pages 199‚Äì209,
Toronto, ON, Canada, 2011.
[46] Y . Pei, C. A. Furia, M. Nordio, Y . Wei, B. Meyer, and A. Zeller.
Automated Ô¨Åxing of programs with contracts. IEEE Transac-
tions on Software Engineering (TSE) , 40(5):427‚Äì449, 2014.
[47] J. H. Perkins, S. Kim, S. Larsen, S. Amarasinghe, J. Bachrach,
M. Carbin, C. Pacheco, F. Sherwood, S. Sidiroglou, G. Sul-
livan, W.-F. Wong, Y . Zibin, M. D. Ernst, and M. Rinard.
Automatically patching errors in deployed software. In ACM
Symposium on Operating Systems Principles (SOSP) , pages
87‚Äì102, Big Sky, MT, USA, October 12‚Äì14, 2009.
[48] D. Posnett, V . Filkov, and P. Devanbu. Ecological inference in
empirical software engineering. In International Conference
on Automated Software Engineering (ASE) , pages 362‚Äì371,Lawrence, KS, USA, November 2011.
[49] Y . Qi, X. Mao, and Y . Lei. EfÔ¨Åcient automated program repair
through fault-recorded testing prioritization. In International
Conference on Software Maintenance (ICSM) , pages 180‚Äì189,
Eindhoven, The Netherlands, Sept. 2013.
[50] Z. Qi, F. Long, S. Achour, and M. Rinard. An analysis of patch
plausibility and correctness for generate-and-validate patch
generation systems. In International Symposium on Software
Testing and Analysis (ISSTA) , pages 24‚Äì36, Baltimore, MD,
USA, 2015.
[51] Research Triangle Institute. The economic impacts of inade-
quate infrastructure for software testing. NIST Planning Report
02-3, May 2002.
[52] J. Rissanen. Modelling by the shortest data description. Auto-
matica , 14:465‚Äì471, 1978.
[53] O. Seng, J. Stammel, and D. Burkhart. Search-based deter-
mination of refactorings for improving the class structure of
object-oriented systems. In Conference on Genetic and Evo-
lutionary Computation (GECCO) , pages 1909‚Äì1916, Seattle,
WA, USA, July 2006.
[54] S. Sidiroglou and A. D. Keromytis. Countering network worms
through automatic patch generation. IEEE Security and Pri-
vacy, 3(6):41‚Äì49, Nov. 2005.
[55] S. Sidiroglou-Douskos, E. Lahtinen, F. Long, and M. Ri-
nard. Automatic error elimination by horizontal code transfer
across multiple applications. In ACM SIGPLAN Conference on
Programming Language Design and Implementation (PLDI) ,
pages 43‚Äì54, Portland, OR, USA, 2015.
[56] S. Silva and E. Costa. Dynamic limits for bloat control in ge-
netic programming and a review of past and current bloat
theories. Genetic Programming and Evolvable Machines ,
10(2):141‚Äì179, June 2009.
[57] A. Smirnov and T. cker Chiueh. Dira: Automatic detection,
identiÔ¨Åcation and repair of control-hijacking attacks. In Net-
work and Distributed System Security Symposium (NDSS) , San
Diego, CA, USA, Feb. 2005.
[58] S. H. Tan and A. Roychoudhury. reliÔ¨Åx: Automated repair of
software regressions. In International Conference on Software
Engineering (ICSE) , Florence, Italy, May 2015.
[59] K. R. Walcott, M. Soffa, G. M. Kapfhammer, and R. S. Roos.
Time-aware test suite prioritization. In International Sympo-
sium on Software Testing and Analysis (ISSTA) , pages 1‚Äì12,
Portland, ME, USA, July 2006.
[60] Y . Wei, Y . Pei, C. A. Furia, L. S. Silva, S. Buchholz, B. Meyer,
and A. Zeller. Automated Ô¨Åxing of programs with contracts.
InInternational Symposium on Software Testing and Analysis
(ISSTA) , pages 61‚Äì72, Trento, Italy, 2010.
[61] W. Weimer, Z. P. Fry, and S. Forrest. Leveraging program
equivalence for adaptive program repair: Models and Ô¨Årst
results. In IEEE/ACM International Conference on Automated
Software Engineering (ASE) , Palo Alto, CA, USA, 2013.
[62] W. Weimer, T. Nguyen, C. Le Goues, and S. Forrest. Au-
tomatically Ô¨Ånding patches using genetic programming. In
ACM/IEEE International Conference on Software Engineering
(ICSE) , pages 364‚Äì374, Vancouver, BC, Canada, 2009.
[63] A. Zeller and R. Hildebrandt. Simplifying and isolating failure-
inducing input. IEEE Transactions on Software Engineering ,
28(2):183‚Äì200, February 2002.
[64] H. Zhong and Z. Su. An empirical study on real bug Ô¨Åxes. In
ACM/IEEE International Conference on Software Engineering
(ICSE) , Florence, Italy, May 2015.
543