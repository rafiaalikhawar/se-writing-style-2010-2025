The Impact of Continuous Integration on
Other Software Development Practices:
A Large-Scale Empirical Study
Yangyang Zhao⇤
Nanjing University
China
njuzhyy@gmail.comAlexander Serebrenik
Eindhoven U of Technology
The Netherlands
a.serebrenik@tue.nlYuming Zhou
Nanjing University
China
zhouyuming@nju.edu.cnVladimir Filkov
UC Davis
USA
ﬁlkov@cs.ucdavis.eduBogdan Vasilescu⇤
Carnegie Mellon University
USA
vasilescu@cmu.edu
Abstract —Continuous Integration (CI) has become a disrup-
tive innovation in software development: with proper tool support
and adoption, positive effects have been demonstrated for pullrequest throughput and scaling up of project sizes. As any otherinnovation, adopting CI implies adapting existing practices inorder to take full advantage of its potential, and “best practices”to that end have been proposed. Here we study the adaptationand evolution of code writing and submission, issue and pullrequest closing, and testing practices as T
RA VIS CIis adopted by
hundreds of established projects on GITHUB. To help essentialize
the quantitative results, we also survey a sample of GITHUB
developers about their experiences with adopting TRA VIS CI. Our
ﬁndings suggest a more nuanced picture of how GITHUBteams
are adapting to, and beneﬁting from, continuous integrationtechnology than suggested by prior work.
I. I NTRODUCTION
The D EVOPSmovement, made popular in recent years, is a
paradigm shift [1]–[4]. It aims to get changes into production
as quickly as possible, without compromising software quality.While no standard deﬁnitions exist (the term is often over-loaded), here we refer to D
EVOPSas a culture that emphasizes
automation of the processes of building, testing, and deployingsoftware. In practice, D
EVOPSis supported by a multitude
of tools for conﬁguration management, cloud-based contin-uous integration, and automated deployment, which enjoywidespread open-source [5] and industrial adoption [6], [7].
In this study we focus on Continuous Integration (CI),
the key enabler of D
EVOPS. CI is a well known concept in
Extreme Programming, promulgated in Martin Fowler’s 2000blog post [8]. As a practice, CI is seeing broad adoptionwith the increasing popularity of the G
ITHUBpull-based
development model [9] and the plethora of open-source,
GITHUB-compatible, cloud-based CI tools, such as T RA VIS
CI, C LOUD BEES, and C IRCLE CI. In a decentralized, social
coding context such as G ITHUB, CI is particularly relevant.
By automatically building and testing a project’s code base, inisolation, with each incoming code change (i.e., push commit,pull request), CI has the potential to: (i) speed up development(code change throughput) [5], [10], [11]; (ii) help maintain
⇤These authors contributed equally to this workcode quality [12], [13]. Clearly, CI promises to be a disruptivetechnology in distributed software development.
For it to be effective, CI must allow for a seamless back
and forth between development, testing (e.g., unit, integration,code review), and deployment. However, the road to efﬁciencyis riddled with choices and trade-offs. For example, workingin large increments may lead to more meaningful changesets, but it may also complicate synchronization between teammembers and, if necessary, reverting changes. Conversely,more frequent changes facilitate merging, but they also requiremore computing infrastructure for CI, since by default thecode is built and all tests are executed with every change.
Moreover, while CI runs on smaller, more frequent changeswould provide earlier feedback on potential problems, theymay also lead to process “noise”, where developers startto ignore the CI build status due to information overload,irrespective of whether the build is clean or broken [14].
Several CI “best practices” have been proposed, e.g., by
Fowler in his inﬂuential blog post [8], such as Everyone
Commits To the Mainline Every Day, Fix Broken Builds
Immediately, and Keep the Build Fast. However, despite the
large scale adoption of CI, we know relatively little aboutthe state of the practice in using this technology and whetherdevelopers are aligning their practices with Fowler’s proposed“best practices”. Such knowledge can help developers tooptimize their practices, project maintainers to make informeddecisions about adopting CI, and researchers and tool buildersto identify areas in need of attention.
In this paper we report on a study of a large sample of
G
ITHUBopen-source projects that adopted T RA VIS CI, by
far the most popular CI infrastructure used on G ITHUB[5].
In particular, we focus on the transition to using T RAVIS CI, and
investigate how development practices changed following thisshift. To this end, we introduce regression discontinuity design
analyses to quantitatively evaluate the effect of an intervention,
in our case adoption of T
RA VIS CI, on the transition toward
expected behaviors in the above three practices (measuredfrom trace data in a large sample of G
ITHUBprojects,
appropriately selected). Qualitatively, to help essentialize thequantitative results, we survey a sample of G
ITHUBdevelopers
978-1-5386-2684-9/17/$31.00 c2017 IEEEASE 2017, Urbana-Champaign, IL, USA
Technical Research60
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:47:57 UTC from IEEE Xplore.  Restrictions apply. about their experiences with adopting T RA VIS CI. Applying
this mixed methodology, we ﬁnd that:
•the increasing number of merge commits aligns with the
“commit often” guideline of Fowler, but is likely to befurther encouraged by the shift to a more distributedworkﬂow with multiple branches and pull requests;
•the “commit small” guideline, however, is followed onlyto some extent, with large differences between projectsin terms of adherence to this guideline;
•the expected increasing trend in the number of closed pullrequests manifests itself after the introduction of T
RA VIS
CI, and even then only after the initial plateau period;
•the pull request latency increases despite the codechanges becoming smaller;
•while the number of issues closed increases, this trend isunexpectedly slowed down by T
RA VIS CI;
•after initial adjustments when adopting T RA VIS CI, test
suite sizes seem to increase.
II. D EVELOPMENT OF RESEARCH QUESTIONS
Transitioning to an integrated CI platform, like T RA VIS , in-
volves adaptation of established processes to the new environ-ment. During this transition, some developers will experiencea more streamlined process evolution trajectory than others.Studying those trajectories can provide lessons learned.
Continuous integration encourages developers to “break
down their work into small chunks of a few hours each”, assmaller and more frequent commits helps them keep trackof their progress and reduces the debugging effort [15], [16].Miller has observed that, on average, Microsoft developerscommitted once a day, while off-shore developers committedless frequently due to network latencies [17]; Stolberg expectseveryone to commit every day [10] and Y ¨uksel reports 33
commits per day after introduction of CI [18]. On a relatednote, the interviewees in the study of Leppanen et al. [19]
saw higher release frequency as an advantage and reportedthat the CI “radically decreased time-to-market”. Hence, weask in RQ
1:Are developers committing code more frequently?
Frequent commits can be expected to be smaller in size:
indeed, the quote from Fowler’s blog post refers to “smallchunks” [15], [16]. Hence, we formulate RQ
2:Are developers
reducing the size of code changes in each commit post CIadoption? Do they continue to do so over time?
In the G
ITHUB-common pull-based development model,
wherein project outsiders (and sometimes insiders too) sub-mit changes in the form of pull requests (PRs), evaluationthroughput is key. Indeed, popular G
ITHUBprojects receive
increasingly many PRs, all of which need to be tested andreviewed pre-merge. One of the most commonly advocatedbeneﬁts of T
RA VIS CI is the ability to “scale up” development,
by automating PR testing and delegating the workload tocloud-based infrastructure; in this sense, Vasilescu et al. [12]
report that G
ITHUBteams are able to process (i.e., close) more
PRs per unit time after adopting T RA VIS CI. This high level of
automation should also impact the speed, not just the volume,of PR evaluations. Hence, we ask in RQ 3:Are pull requests
closed more quickly post CI adoption?
For CI, and D EVOPSin general, to have the stated bene-
ﬁts, effective coordination between all project contributors isimportant; a common mechanism for this are issue reports.Similarly to the pull requests, the number of closed issuesper time period may change after CI adoption, in response toincreased project activity or efﬁciency. Hence, we ask in RQ
4:
Are more issues being closed after adopting CI?
Finally, CI is closely related to the presence of automated
tests [15]. Duvall even claims that CI without such tests shouldnot be considered CI at all [16], while Cannizzo et al. deem an
extensive suite of unit and acceptance tests to be an essentialﬁrst step [20]. Moreover, CI is frequently introduced togetherwith test automation [18]. However, developing tests suitedfor automation requires a change in developers’ mindset, andpresence of a comprehensive set of tests incurs additionalmaintenance costs [21]. Naturally, more automated testing willexpose more errors, and CI makes it possible to track andreact to different error types differently [22]. Therefore, weformulate RQ
5:How does the usage of automated testing
change over time after CI adoption?
III. M ETHODS
We collected and statistically analyzed data from a large
sample of open-source projects on G ITHUB, that adopted
TRA VIS CI at some point during their history. We further
surveyed a sample of those projects’ T RA VIS CI adopters.
A. Data Gathering
Data collection involved mining multiple sources: GHT OR-
RENT [23], the G ITHUBAPI, project version control (git)
logs, and the T RA VIS CI API. The goal was to select non-
trivial projects that adopted T RA VIS CI, and had sufﬁcient
activity both before and after adoption, in order to observepotential transition effects. Note that for the purpose of thisstudy we don’t distinguish between “project” and “repository”;the term “project” has also been used to refer to a collectionof interrelated repositories in the literature [24].
Candidate Projects: We started by identifying G
ITHUB
projects that use T RA VIS . To our knowledge, no such list
exists (T RA VIS TORRENT [25] is restricted to Java and Ruby
projects), so we wrote a script to iterate over non-fork
GHT ORRENT projects (oldest to newest) and poke the T RA VIS
CI API (cf. [26]) to determine if the project used T RA VIS ; we
ran and stopped the script after identifying approximately half
(165,549) of all G ITHUBprojects that ever used T RA VIS CI.1
Next, we cloned the G ITHUBrepositories of all these
projects locally, and extracted their main-branch commit his-tories using P
ERCEV AL , an open-source repository mining
tool part of the G RIMOIRE LABtool suite. Then, we traversed
each project’s commit history to determine when maintainersintroduced T
RA VIS CI, by identifying the earliest commit
1At the time of writing, T RA VIS self reports being used in over 318,000
open source G ITHUBprojects, see https://travis-ci.org.
61
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:47:57 UTC from IEEE Xplore.  Restrictions apply. timeTravis CI adoption
(ﬁrst .travis.yml  commit)
-15 
days+15 
days-1… … -12 +1 +12
+45 
days+375 
days-375 
days-45 
daysUnstable period excluded
Fig. 1. Overview of our time series assembly method.
to the .travis.yml conﬁguration ﬁle, and recorded its
authored timestamp as the timestamp of the T RA VIS adoption.
Time Series: We proceeded to aggregate data about the differ-
ent practices considered in 30-day windows, 12 on each side
around the T RA VIS CI adoption event (Figure 1).
During initial exploration, we saw several occasions of
TRA VIS being adopted as part of a larger restructuring ef-
fort, which lasted several days. Examples include changingthe build system from Ant to Gradle and, consequently,reorganizing project folders; updated library dependencies;and restructured tests. Based on this anecdotal evidence, weconcluded that the activity immediately prior to and imme-diately following the introduction of T
RA VIS CI might not
be representative of the project’s overall trends. Therefore, tolimit the risk of bias due to extraordinary project activity inthe transition period, we excluded one month of data centeredaround the adoption event in our quantitative analyses below.
Measures: We collected global and time-window measures:
•total number of commits in a project’s history, as a
proxy for project size / activity.
•total number of commit authors, as a proxy for the size
of a project’s community; commit authors include bothcore developers, who are also committers, and externalcontributors, without “write” access, whose commits aremerged in by the core developers. Since it is commonfor open-source developers to contribute under differentaliases (name-email tuples), e.g., as a result of using
different machines and changes in git settings over time,we ﬁrst performed identity merging. We used heuristics
that match ﬁrst and last names, email preﬁxes, and emaildomains, cf. prior work [27], [28], and found an averageof 1.15 (max 7) aliases per person in our dataset.
•project age at the time of adopting T RA VIS CI, in
months, computed since the earliest recorded commit.
•main programming language, automatically computedby G
ITHUBbased on the contents of each repository and
extensions of ﬁle names therein.
•number of non-merge commits, number of merge
commits per time window. Since git is a distributed
version control system, developers can work locally, inincrements, before pushing their changes to G
ITHUBor
opening a pull request. E.g., a developer can choose topartition a large change into many smaller ones, andmake multiple local commits to better manage the work.This would have no effect on CI runs, since CI is onlytriggered by G
ITHUBpush events, and events happening
after (and including when) a pull request is opened.Consequently, to study Commits To the Mainline Every
Day (Fowler’s best practice), as opposed to potentially
local git commits, we distinguish between non-merge
commits and merge commits as a proxy. We recognize
non-merge commits as those having at most one parent,and merge commits otherwise.
•mean commit churn per time window. Churn is the total
number of lines added plus the total number of linesremoved per commit (in git modiﬁed lines appear as ﬁrstremoved, then added), extracted from git logs. The meanis computed over all commits in that time window.
•number of issues opened / closed, number of pull
requests opened / closed per time window, extracted
using the G ITHUBAPI.
•mean pull request latency per time window, in hours,
computed as the difference between the timestamp whenthe PR was closed and that when it was opened. Themean is computed over all PRs in that time window.
•number of tests executed per build. Each T RA VIS
build runs at least one job, corresponding to a particularbuild/test environment (e.g., jdk version). Once the jobstarts, a log is generated, recording the detailed infor-mation of the build lifecycle, including installation stepsand output produced by the build managers. On a sampleof Java projects that used Maven, Ant, or Gradle astheir build systems, for which we could make reasonableassumptions about the structure of their build log ﬁles, weparsed the T
RA VIS build logs and extracted information
about the number of tests executed (we take the maximumnumber of tests across jobs as the test count for a build),and the reasons causing builds to break (similar to [29]).
Filtering: As a large fraction of projects on G
ITHUBare small
and not highly active [9], we ﬁltered out projects inconsistentlyactive during our 24-month observation period, to avoid bias-ing our conclusions due to an inﬂation of zero values in ourdata. Depending on the research question, this means eitherrequiring at least one merge and one non-merge commit on themain branch / closed pull request / closed issue in each of the
24 time windows of observation. Furthermore, our multivariateregression analysis below requires enough variance along eachof the dimensions being modeled, thus we additionally ﬁlterout programming languages not represented by many projectseach. The resulting dataset spans seven popular programminglanguages: C, Java, Ruby, PHP, JavaScript, C++, and Python.Table I contains an overview.
B. Time Series Analysis Method
We use data visualization and statistical modeling to dis-
cover longitudinal patterns indicative of CI adoption effects.
As one of our contributions, we introduce the statisticalmodeling framework of regression discontinuity design [30]
to assess the existence and extent of a longitudinal effect,associated with the T
RA VIS CI adoption.
To evaluate the effect of a treatment, e.g., a new drug,
on a disease progression, randomized experimental trials are
62
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:47:57 UTC from IEEE Xplore.  Restrictions apply. TABLE I
PROJECTS PER LANGUAGE ,FOR DIFFERENT FILTERS .
24 active periods with
Language Commits Pull reqs Issues All
C 30 13 13 6
Java 57 26 30 6
Ruby 54 26 37 5PHP 71 32 33 14JavaScript 75 38 69 18C++ 80 40 36 13Python 100 55 44 15
Total 467 230 262 77
usually conducted: the experimental cohort is randomly split
into a treatment group, i.e., those given the treatment, and a
control group, i.e., those not given the treatment; then, the
effect is evaluated based on the difference in disease progres-sion between the two groups. In the absence of randomizedtrials, as is often the case with software engineering trace data,weaker techniques such as quasi-experiments are employed.
Among quasi-experimental designs to evaluate longitudinal
effects of an intervention, regression discontinuity designs(RDDs) [31] are the most powerful. RDD is used to modelthe extent of a discontinuity of a function between its valuesat points just before/after an intervention. It is based onthe assumption that in the absence of the intervention, thetrend of the function would be continuous in the same wayas prior to the intervention. Therefore, one can statisticallyassess how much an intervention (in our case the T
RA VIS
adoption) changed an outcome of interest, immediately andover time; and, if implemented using multiple regression, alsoevaluate whether the change could be attributed to other factorsthan the intervention. Figure 2 illustrates a discontinuity; theRDD approach, in a nutshell, aims to uncover the differentregression lines before and after the discontinuity.
There are different formalizations of RDD, most promi-
nently sharp RDD and fuzzy RDD [30]. To model the effectof CI adoption on developer practices, here we chose one im-plementation of the simpler, sharp RDD approach: segmented
regression analysis of interrupted time series data [32]. We
summarize our approach next, following the description byWagner et al. [32], and refer to Figure 2. Let Ybe the outcome
variable in which we are looking for a discontinuity, e.g.,
commit churn per month. We can specify the following linearregression model to estimate the level and trend in Ybefore CI
adoption, and the changes in level and trend after CI adoption:
y
i=↵+ ·time i+ ·intervention i
+ ·time after intervention i+✏i,
where time indicates time in months at time ifrom the start of
the observation period; intervention is an indicator for time i
occurring before (intervention =0) or after (intervention =1)
CI adoption, which was at month 0 based on our encodingin Figure 1; and time after intervention counts the number
of months at time iafter the intervention, coded 0 before CI
adoption and (time  12) after CI adoption.
This model encapsulates two separate regressions. For
points before the treatment, the resulting regression line has aslope of  , and after the treatment  + . The size of the effect
Fig. 2. RDD: the treatment effect (  ) is negative, and there is an interaction
effect (  6=0) which changes the slope of the regression after the treatment.
of the treatment is the difference between the two regression
values of yiat the intervention time, and is equal to  .
Our goal is to capture, using the RDD model above, changes
in trends after CI adoption across our sample of projects, whilecontrolling for confounding variables (e.g., project size, age,and programming language). Our data is centered at the timeof CI adoption, and has an equal number of points, 12, oneach side. Since the data is inherently nested (each projectcontributes multiple observations; similarly for programminglanguage), we implement the RDD model as a mixed-effectslinear regression (functions lmer andlmer.test in R) with
a random-effects term for project and another random effects
term for programming language; this way, we can capture
project-to-project and language-to-language variability in theresponse. All other variables were modeled as ﬁxed effects.
Solving the regression gives us the coefﬁcients, which, if
signiﬁcant, can help us reason about the treatment and itseffects, if any. We report on the models having signiﬁcantcoefﬁcients in the regressions (p<0. 05) as well as their effect
sizes, obtained from ANOV A analyses. Model ﬁt was evalu-ated using a marginal (R
2
m) and a conditional (R2
c) coefﬁcient
of determination for generalized mixed-effects models [33],[34]; R
2
mdescribes the proportion of variance explained by
the ﬁxed effects alone; R2
cdescribes the proportion of variance
explained by the ﬁxed and random effects together. To improverobustness, the top 1% of the data was ﬁltered out as outliers.Finally, we check for multicolinearity using VIF (below 3).
C. Survey
To obtain additional insights into the adoption of T
RA VIS
CI we conducted a survey with a sample of the maintainers
responsible for introducing T RA VIS . We randomly selected
335 projects from our dataset, stratiﬁed by how much of
a discontinuity in commit activity we detected at T RA VIS
adoption time. For each project we identiﬁed the developerresponsible for introducing T
RA VIS , i.e., committing the ﬁrst
version of .travis.yml, and removed duplicates (same de-
veloper in different projects). In the invitation email (deliveryof 23 messages failed; we received 55 responses, or 17.63%response rate) we explicitly stated the name of the project. Weasked three questions: what made developers decide to startusing CI and T
RA VIS CI; whether they had to change anything
in their development process to accommodate CI/T RA VIS ; and
how did their development process change with time, if at all,to use CI/T
RA VIS CI efﬁciently. No question was mandatory.
63
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:47:57 UTC from IEEE Xplore.  Restrictions apply. ●
●●
● ●●
●
●●
●●
●● ● ●
●●
● ●● ●●
● ●●
●●
●●
● ●
●● ●
●●
● ●●
●●●
●●
●● ● ● ● ●
●●●
●●
●
●●
●●
●●
● ● ● ● ●
●● ●●●
●●
●●
●●
● ●
●●
●
●●●●
●●
●● ●
●●
●● ●
● ●●● ●●
●●●
●●
●● ● ●
● ● ● ● ● ● 15102050100200500
−12−11−10 −9−8−7−6−5−4−3−2−1 1 2 3 4 5 6 7 8 9 10 11 12
Month index w.r.t. Travis CI adoptionNum. non−merge commits
●●●●●
●●●●●●●●
●●
●●
● ● ●●
●●
● ●●
● ● ●●●
●●●● ●
●●●●
●●●
● ●●
15102050100200500
−12−11−10 −9−8−7−6−5−4−3−2−1 1 2 3 4 5 6 7 8 9 10 11 12
Month index w.r.t. Travis CI adoptionNum. merge commits
Fig. 3. Commit frequency before and after the T RA VIS CI adoption.
IV. R ESULTS AND DISCUSSION
We discuss changes in development practices after CI adop-
tion along four dimensions: commit frequency, code churn,
pull requests resolution efﬁciency, issue tracking, and testing.
A. RQ 1: Trends in Commit Frequency
The ﬁrst practice we examine is commit frequency. As we
investigate the “Commits to the Mainline Every Day” practice,
it is important to distinguish non-merge from merge commits.Indeed, local git commits, in a developer’s ofﬂine repository,happen in isolation and can be seen as simply a mechanism topartition the work. However, what matters for T
RA VIS CI are
pushes and pull requests to the blessed G ITHUBrepository,
i.e., the “main” repository of the project as only then T RA VIS
would be triggered. Push events are not readily accessiblein our data, hence we analyze merge commits on the maindevelopment branch as a proxy for work that would havebeen subjected to CI by T
RA VIS . Since not all our projects
have recorded merge commits in all24 periods, we restrict
this analysis to only 575 projects that had at least one mergecommit in each time window.
Exploratory Study: Figure 3 shows the boxplots of per-project
number of non-merge commits (top) and number of mergecommits (bottom), respectively, for each of twelve consecutive30-day time intervals before and after the T
RA VIS CI adoption.
Note the log scale. Recall that due to instability, we omitthe 30-day time interval centered around t=0, when the
earliest commit to the T
RA VIS conﬁguration ﬁle, which signals
adoption, was recorded (Figure 1). The horizontal line in eachboxplot is the median value across all projects.
First, we observe relative stability in the number of non-
merge commits prior to the T
RA VIS CI adoption (Figure 3,
top), with the across-projects medians around 78 commits, anda slight decreasing trend after the adoption, with the across-projects median dropping to 67 commits in period 12. Second,after an initial dip in periods -12 to -10, the merge commits(Figure 3, bottom) appear to display a slight increasing trendprior to the T
RA VIS CI adoption, with the across-projects
median reaching 18 commits immediately prior to the adoptionperiod, followed by apparent stabilization after that. We alsoTABLE II
COMMIT FREQUENCY MODEL .THE RESPONSE IS LOG(NUMBER OF
MERGE COMMITS )PER MONTH .R2
m=0.58.R2
c=0.83.
Coeffs (Errors) Sum Sq.
(Intercept)  0.958 (0.232)⇤⇤⇤
log(TotalCommits)  0.016 (0.032) 0.07
log(NumNonMergeCommits) 0.777 (0.007)⇤⇤⇤3095.91⇤⇤⇤
AgeAtTravis  0.001 (0.001) 0.96
log(NumAuthors) 0.120 (0.026)⇤⇤⇤5.67⇤⇤⇤
time 0.019 (0.002)⇤⇤⇤30.39⇤⇤⇤
interventionTRUE  0.024 (0.027) 0.21
time after intervention  0.016 (0.003)⇤⇤⇤10.96⇤⇤⇤
⇤⇤⇤p<0.001,⇤⇤p<0.01,⇤p<0.05
observe a discontinuity at t=0: the across-projects median
is 21 commits right after the adoption period. Note, in bothplots, the large variance in the data.
Statistical Modeling Study: We ﬁtted a mixed-effects RDD
model, as described before, to model trends in the number of
merge commits per project, over time, as a function of T
RA VIS
CI adoption, and while controlling for confounds; most no-
tably, we control for the number of non-merge commits, as
these may have not all been subjected to CI, since they appearto display a decreasing trend with time.
We modeled a random intercept for programming language,
to allow for language-to-language variability in the response(i.e., code in some languages being naturally more verbosethan in others, resulting in different ways to split the workacross commits; or community-level norms for committing);we also modeled a random intervention slope and intercept
forproject, to allow for project-to-project variability in the
response and the possibility that, on average, projects withlower initial activity may be less strongly affected by adopting
T
RA VIS CI than high-frequency projects. Recall also that the
coefﬁcient for time is the slope before adoption, the coefﬁcient
forintervention is the size of the effect of CI introduction, the
coefﬁcient for time after intervention is the divergence in the
slopes before and after T RA VIS CI adoption, and the sum of
the coefﬁcients for time andtime after intervention is the slope
of the linear trend after T RA VIS CI adoption.
Table II summarizes the results. In addition to the model
coefﬁcients and corresponding standard errors, the table showsthe sum of squares, a measure of variance explained, for eachvariable. The statistical signiﬁcance is indicated by stars. Theﬁxed-effects part of the model ﬁts the data well (R
2
m=0.58).
There is also a considerable amount of variability explainedby the random effects, i.e., project-to-project and language-
to-language differences, not explicitly modeled by our ﬁxedeffects (R
2
c=0.83). Among the ﬁxed effects, we observe that
thenumber of non-merge commits, our main control, explains
most of the variability in the response. The coefﬁcient ispositive, i.e., the direction of the correlation is expected: other
things constant, the more non-merge commits there are, themore merge commits there will be as well. Neither project size
(total number of commits over the entire history) nor project
ageat adoption time have any statistically signiﬁcant effects.
Next we turn to our T
RA VIS -related variables. The model
conﬁrms a statistically signiﬁcant, positive, baseline trend inthe response with time (with a small effect), as suggested by
64
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:47:57 UTC from IEEE Xplore.  Restrictions apply. ●●●
●●
●●
●
●●●
●
●●●
●●●
●●●
●●
●
●●●
●
●●
●●●
●●
●●●
●●
● ●●●●●
●●
●●●●●
●●
●●
●●
●●●●●
●●●●●
●●●●
●●●●
●●
●●
●●●●●
●
●●●
●●
●
●●
●●●●
●●●
●●●●●
●●●
●●
●●
●●●●●
●●
●●
●●
●●●
●●●●
●
●●
●●
●
●●
●●●
●●
●●
●●●●●
●●●●
● ●●●●●●
●●●
●●●●●●
●●
●●●
●
●●
●●●●●●
●●
●●●
●●
●●
●●●
●●●●●
●
●●
●●
●
●●
●●● ●●
●
●●●●
●●●●●
●●
●●
●●●
●●●●●●
●●●
●●●
●●●●
●
●●●
●●●●●●
●●
●●
●●●●●
●
●●
●●●
●●
●●
●●
●
●●
●●
●
●●
●●●●●
●●●●
●
●●
●
●●●
●●●
●●
●●
●●
●●
●●●
●
●●●●●
●●
●
●●●
●●●
●●●
●
●●
●
●●
●●●
●●
●●
●●●
●●●●
●●●
●●
●●
151020501002005005000
−12−11−10 −9−8−7−6−5−4−3−2−1 1 2 3 4 5 6 7 8 9 10 11 12
Month index w.r.t. Travis CI adoptionMean non−merge churn (LOC)
● ● ●●●● ●
● ●●●
●●
●●● ●●● ●●●
●● ● ●
●●●● ● ● ●
●●
●●●● ●● ●● ●
●●
●●
●●
●
●●
● ●● ● ●
●●●●●
●●
●● ● ● ●●
● 151020501002005005000
−12−11−10 −9−8−7−6−5−4−3−2−1 1 2 3 4 5 6 7 8 9 10 11 12
Month index w.r.t. Travis CI adoptionMean merge churn (LOC)
Fig. 4. Mean code churn per commit per project, before and after T RA VIS .
our exploratory study. The model does not detect any disconti-
nuity at adoption time, since the coefﬁcient for intervention is
not statistically signiﬁcant. Post adoption, there is a decreasein the slope of the time trend, but the overall trend remainsascending (the sum of the coefﬁcients for time andtime after
intervention is positive): more merge-commits as time passes.
Discussion: The exploratory study suggests a slight decreasingtrend in the number of non-merge commits with time. Sepa-rately modeling the trend in number of non-merge commits
(not shown) using a similar approach as above conﬁrmed thisoverall decreasing trend. This is consistent with the commonobservation that, as projects move past the initial developmenthurdle and age, development generally slows down, and thefocus shifts to bug-ﬁxing rather than adding new features,noted, e.g., by Brindescu et al. [35].
In contrast, the statistical modeling study above revealed
a consistent increase in merge commits with time, including
post-CI adoption, albeit with a slowdown. This overall trendaligns with the expected increase in commit frequency afterswitching to CI, as expressed and encouraged by Fowler.Given the overall decreasing trend with time in the numberof non-merge commits, the increase in merge commits isnoteworthy. One explanation is that projects are switching tomore distributed development workﬂows, using branches andpull requests. Indeed, in our survey, R38 has indicated thattheir project “shifted towards pull-request - merge develop-ment strategy as it made the distributed development moremanageable.” The development process change reported byR38 is not exceptional. Indeed, the idea of a shift towards morefocused development in separated branches has been voicedby R32 (“shorter lived (and generally single contributor)branches”) and R49 (“feature branches”), R6, and R47.
B. RQ
2: Trends in Code Churn
Next we examine code churn. As before, we distinguish
between merge and non-merge commits.TABLE III
COMMIT CHURN MODEL 1. T HE RESPONSE IS LOG(MEAN NON -MERGE
COMMIT CHURN ).R2
m=0.09.R2
c=0.48.
Coeffs (Errors) Sum Sq.
(Intercept) 1.336 (0.386)⇤⇤⇤
log(TotalCommits) 0.529 (0.049)⇤⇤⇤68.03⇤⇤⇤
AgeAtTravis  0.003 (0.001)⇤0.03
log(NumAuthors)  0.233 (0.041)⇤⇤⇤14.42⇤⇤⇤
time  0.007 (0.004) 1.57
interventionTRUE 0.071 (0.047) 1.92
time after intervention  0.009 (0.006) 7.86⇤
⇤⇤⇤p<0.001,⇤⇤p<0.01,⇤p<0 .05
TABLE IV
COMMIT CHURN MODEL 2. T HE RESPONSE IS LOG(MEAN
MERGE -COMMIT CHURN ).R2
m=0.20.R2
c=0.51.
Coeffs (Errors) Sum Sq.
(Intercept)  1.297 (0.484)⇤⇤
log(TotalCommits) 1.113 (0.062)⇤⇤⇤625.20⇤⇤⇤
AgeAtTravis  0.005 (0.001)⇤⇤18.43⇤⇤
log(NumAuthors)  0.522 (0.052)⇤⇤⇤196.48⇤⇤⇤
time  0.012 (0.005)⇤8.86⇤
interventionTRUE 0.220 (0.066)⇤⇤⇤21.75⇤⇤⇤
time after intervention  0.022 (0.008)⇤⇤15.51⇤⇤
⇤⇤⇤p<0.001,⇤⇤p<0.01,⇤p<0 .05
Exploratory Study: Figure 4 shows boxplots of per-project per-
commit mean code churn (medians behave similarly) for eachof twelve consecutive 30-day time intervals before, and after,the T
RA VIS CI adoption. The horizontal line in each boxplot
is the median value over all projects.
In the non-merge commits, the medians are quite stable
across the entire interval, from before to after the T RA VIS CI
adoption, dancing around 100 lines of code churn per commiton average, with large variance. In comparison, in the merge-commits, we observe more than usual variance in the twomonths preceding the adoption, as well as a slight downwardtrend in the medians following adoption, which drop to about263 lines of code churn per commit on average in the lastperiod (12), compared to about 405 right after adoption (period1). The variance around the medians is still large in all periods.
In conclusion, non-merge commits seem mostly unaffected
by time passing and the T
RA VIS CI adoption, while merge
commits seem to be getting smaller with time.
Statistical Modeling Study: Guided by our exploratory obser-
vations above, we proceed to quantify the trends we observedusing two mixed-effects RDD models, for non-merge andmerge commits, respectively, as described before.
Table III summarizes the RDD model for non-merge com-
mits. First, we observe that only the combined ﬁxed-and-random effects model ﬁts the data well (R
2
c=0.48compared
toR2
m=0.09), i.e., most of the explained variability in
the data is attributed to project-to-project and language-to-language variability, rather than any of the ﬁxed effects.
Next, we turn to the ﬁxed effects. From among the controls,
we note that overall bigger projects (TotalCommits) tend tochurn more, other things held constant; and projects with alarger contributor base (NumAuthors), which typically indicatemany occasional contributors, also tend to churn less. Noneof the T
RA VIS -related predictors have statistically signiﬁcant
effects, i.e., the churn trend in non-merge commits is stationary
over time, and remains unaffected by the T RA VIS CI adoption.
65
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:47:57 UTC from IEEE Xplore.  Restrictions apply. Table IV summarizes the RDD model for merge commits.
Similarly as with the previous model of churn in non-merge
commits, most of the explained variability in the data isattributed to project-to-project and language-to-language vari-ability, rather than any of the ﬁxed effects variables. Thecontrols also behave similarly as before: older projects tendto churn less, perhaps as they have reached maturity and amore stationary stage in their evolution; projects with a largercontributor base also tend to churn less, which is in line withthe expectation that occasional contributors to open sourceprojects are generally less active than core developers; smallpull request contributions would be reﬂected here.
After controlling for confounds, we move on to the main
time series predictors, all of which now have statistically sig-niﬁcant effects. The coefﬁcient for time is negative, suggesting
a small decreasing baseline trend in commit churn before CIadoption; the intervention coefﬁcient signals a discontinuity
in the time series at the time of the T
RA VIS CI adoption; the
negative coefﬁcient time after intervention signals an accel-
eration in the baseline trend. Together, this conﬁrms a changein merge commit churn at the time of adoption, followed byan accelerated decreasing trend after adoption.
Discussion: Our modeling study revealed a statistically signiﬁ-
cant discontinuity in the merge commit churn time series whenadopting T
RA VIS CI; and a statistically signiﬁcant decreasing
linear trend with time in merge commit churn after adopting
TRA VIS CI. However, churn in non-merge commits remains
unaffected by either time or switching to T RA VIS CI.
The discontinuity is not unexpected, as one can reasonably
expect more maintenance work in preparation for transitioningto T
RA VIS CI, and some adjustment/cleanup period right after.
The relative instability in code churn on both sides near theCI adoption time is also indication of this, as are the surveyresults. Indeed, when asked about the introduction of T
RA VIS
CI, respondents frequently refer to test automation being
introduced around the same time as T RA VIS “as contributors
couldn’t be trusted to run test suite on their own” (R25).Furthermore, respondents indicate that T
RA VIS has been intro-
duced as “a part of automated package/release effort” (R38)and to “deploy artifacts to S3 on each commit, as part of ourdeployment process using Amazon CodeDeploy” (R34).
The decreasing trend in code churn post T
RA VIS is visible
only among merge commits, which are more likely to beaffected by switching to a fast-paced CI workﬂow than non-merge commits, which may live on local, isolated developerbranches for some time before being merged to the remote.This ﬁnding is consistent with Fowler’s recommended goodpractices of CI, of committing smaller changes to the code.The expected decrease in the size of the commits is alsoechoed by one of the survey respondents: “commits becamesmaller and more frequent, to check the build; pull requestsbecame easier to check” (R4). However, the decreasing trendwe observed is not particularly steep. Survey responses providea possible explanation: several developers referred to pullrequest integration as the reason for introducing T
RA VIS CI,● ● ● ● ● ●●●●●● ●● ●
15102050100200500
−12 −11 −10 −9 −8 −7 −6 −5 −4 −3 −2 −1 1 2 3 4 5 6 7 8 9 10 11 12
Month index w.r.t. Travis CI adoptionNum closed PRs
Fig. 5. Closed pull requests before and after the T RA VIS CI adoption.
TABLE V
PULL REQUEST MODEL .THE RESPONSE IS LOG(NUMBER OF PULL
REQUESTS CLOSED )PER MONTH .R2
m=0.32.R2
c=0.65.
Coeffs (Errors) Sum Sq.
(Intercept)  3.132 (0.356)⇤⇤⇤
log(TotalCommits) 0.605 (0.049)⇤⇤⇤71.17⇤⇤⇤
AgeAtTravis  0.008 (0.001)⇤⇤⇤14.17⇤⇤⇤
log(NumAuthors) 0.136 (0.043)⇤⇤4.64⇤⇤
time 0.032 (0.004)⇤⇤⇤32.98⇤⇤⇤
interventionTRUE 0.005 (0.055) 0.00
time after intervention  0.033 (0.005)⇤⇤⇤18.06⇤⇤⇤
⇤⇤⇤p<0.001,⇤⇤p<0.01,⇤p<0 .05
and while R31 and R50 state that T RA VIS CI is used both for
commits and for pull requests, R11, R21, and R37 explicitly
state that in their projects push commits are outside the scopeof T
RA VIS CI. In other words, if in a project not all commits
are subjected to T RA VIS CI, then there may be less incentive
to follow Fowler’s recommendations related to commit churn.
At the same time, a different respondent indicated that
TRA VIS CI discourages him/her from making trivial commits
(R11), suggesting instead that the commits he/she makes arelikely to be larger; therefore, our global decreasing trendmay be weakened by local trends in the opposite direction.Our model found evidence for strong project-speciﬁc effects:project-to-project and language-to-language differences, ascaptured by our random effects, contribute substantially toexplaining the overall data variability. This suggests that anyphenomena giving rise to pressures to increase or reduce codechurn are perhaps subordinate in magnitude to other, morepressing phenomena of local, i.e., project-speciﬁc, character.
Alternatively, the decreasing trend in commit churn is also
consistent with the observation that as projects age, bug-ﬁxingcommits, which on average are smaller, become more commonthan new-feature commits, which on average are larger [35].
Our results suggest that while Fowler’s good practice of
small commits is followed to some extent, the project-to-project and language-to-language differences are more impor-tant and might overshadow the overall trend.
C. RQ
3: Trends in Pull Request Closing
Next we consider pull request closing, which we analyze
along two dimensions: the number of pull requests closed, and
the average pull request latency (i.e., time to close) per timewindow. Since T
RA VIS CI runs every time a pull request is
submitted, the developers are rapidly notiﬁed whether theirpull requests pass the T
RA VIS quality gate, and can then
rapidly react by correcting the pull request source code. Hence,
66
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:47:57 UTC from IEEE Xplore.  Restrictions apply. ●●●
●●●
●●
●
●●●
●●
●●●
●●
●
●●
●
●●
●
●●●
●●
●●●●
●●●
●
●●
●●
●●●
●●
●●● ●●
●●●
●●●
●
●●
●●
●●
●●
●
●
●●●
●●●●●●
●
●
●●●
●
●●●
●
●●
●●
●●●
●●
●●
●●●●
●●●●
●●●●●●●●
●●●●
●
●●●
●●●●
●
●●●●●●
●
●●
●●●
●
●●
●●●●
●
●151020501002005005000
−12−11−10 −9−8−7−6−5−4−3−2−1 1 2 3 4 5 6 7 8 9 10 11 12
Month index w.r.t. Travis CI adoptionMean PR latency
Fig. 6. Mean pull request latency before and after the T RA VIS CI adoption.
TABLE VI
PULL REQUEST LATENCY MODEL .THE RESPONSE IS LOG(MEAN PULL
REQUEST LATENCY )PER MONTH ,IN HOURS .R2
m=0.05.R2
c=0.31.
Coeffs (Errors) Sum Sq.
(Intercept) 0.052 (0.519)
log(TotalCommits) 0.157 (0.068)⇤7.89⇤
AgeAtTravis  0.002 (0.002) 1.00
log(NumAuthors) 0.225 (0.062)⇤⇤⇤19.72⇤⇤⇤
time 0.030 (0.012)⇤8.60⇤
interventionTRUE  0.160 (0.153) 1.65
time after intervention  0.022 (0.019) 2.03
⇤⇤⇤p<0.001,⇤⇤p<0.01,⇤p<0.05
we expect that T RA VIS CI shortens the pull request latency and
increases the number of closed pull requests.
Exploratory Study: Figure 5 shows boxplots of per-project
number of closed pull requests. We observe that the mediannumber of closed pull requests per months ﬂuctuates between11 and 15 before T
RA VIS introduction, and between 14 and
20 after. We note an apparent increasing trend pre-T RA VIS ,
which also seems to continue post-T RA VIS . Both pre-T RA VIS
and post-T RA VIS we see large variance around the medians.
Figure 6 provides a complementary perspective on the
efﬁciency of pull request resolution, namely the pull requestlatency. The median latency seems to increase prior to intro-duction of T
RA VIS CI, and continues to do so afterwards.
Statistical Modeling Study: We apply RDD as above. Thestatistical models for the number of pull requests closed andthe mean pull request latency are summarized in Tables V andVI. As above, the combined ﬁxed-and-random effects modelsﬁt the data much better than the basic ﬁxed-effect models,indicating that the project-to-project and language-to-languagevariability are responsible for most of the variability explained.
Regarding closed pull requests (Table V), we note an
increasing time baseline trend pre-adoption; no statistically
signiﬁcant discontinuity at the adoption time; and an apparentneutralization of the aforementioned time trend post-adoption,as (time) + (time
after intervention) '0. Turning to the
pull request latency model (Table VI), we note a statisticallysigniﬁcant, increasing baseline time trend, unaffected by the
T
RA VIS CI adoption, as neither coefﬁcient for discontinuity
and change in trend has statistically signiﬁcant effects.
Discussion: The statistical modeling study generated two note-
worthy ﬁndings. First, despite a visually apparent increasingtrend in the number of pull requests closed per time pe-riod, across the entire interval under observation, our modelsuggests that, after controlling for project size, project age,and size of developer community, as well as local project-to-●●
● ● ● ● ● ● ● ● ● ● ● ● ● ● 15102050100200
−12 −11 −10 −9 −8 −7 −6 −5 −4 −3 −2 −1 1 2 3 4 5 6 7 8 9 10 11 12
Month index w.r.t. Travis CI adoptionNum closed issues
Fig. 7. Number of closed issues before and after the T RA VIS CI adoption.
TABLE VII
ISSUES MODEL .THE RESPONSE IS LOG(NUMBER OF ISSUES CLOSED )
PER MONTH .R2
m=0.17.R2
c=0.53.
Coeffs (Errors) Sum Sq.
(Intercept)  1.680 (0.364)⇤⇤⇤
log(TotalCommits) 0.443 (0.049)⇤⇤⇤42.54⇤⇤⇤
AgeAtTravis  0.006 (0.001)⇤⇤⇤8.83⇤⇤⇤
log(NumAuthors) 0.119 (0.039)⇤⇤4.72⇤⇤
time 0.021 (0.004)⇤⇤⇤16.00⇤⇤⇤
interventionTRUE 0.030 (0.049) 0.19
time after intervention  0.019 (0.005)⇤⇤⇤5.99⇤⇤⇤
⇤⇤⇤p<0 .001,⇤⇤p<0.01,⇤p<0.05
project and language-to-language differences, the increasing
trend manifests itself, in aggregate, only pre-T RA VIS . More-
over, surprisingly, we ﬁnd that the initial trend is ﬂattened post-
TRA VIS , resulting in relatively stationary behavior, on average.
This paints a more nuanced picture of how G ITHUBteams
are adapting to, and beneﬁting from, continuous integrationtechnology than suggested by prior work [12], and speaks tothe strength of time-series-based analysis methods, such as theRDD we use here, at detecting ﬁne resolution effects.
Second, our model ﬁnds that, on average, pull requests are
taking longer to close over time, and this trend is unaffected bythe switch to T
RA VIS CI. This, again, is quite surprising, as we
have seen in RQ 2above that the size of code changes becomes,
on average, smaller over time; in turn, this would imply thatchanges are also easier (quicker) to evaluate. One possibleexplanation for the increased latency is the T
RA VIS ’ slowness,
reported by some survey participants (R1, R27, R53).
D. RQ 4: Trends in Issue Closing
Next, we examine issue closing trends.
Exploratory Study: We follow the same approach as above and
compare the medians in the number of issues per time period,in the months before and after CI adoption. Fig. 7 shows theboxplots of the number of issues per unit time period for 12consecutive 30-day time intervals, before and after T
RA VIS CI
adoption. We note that the month immediately preceding andthe two following the adoption exhibit the highest number ofissues. Besides those, before CI adoption the median numberof issues per period seems to vary between 10 and 13, withmost of them being in between. Following CI adoption, wenote a slight increasing trend in the median of the number ofissues closed, with the median between 12 and 14.
Statistical Modeling Study: We apply RDD as above. The
statistical model for the number of issues closed is summarizedin Table VII. As above, the combined ﬁxed-and-random effectsmodels ﬁt the data much better than the basic ﬁxed-effect
67
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:47:57 UTC from IEEE Xplore.  Restrictions apply. 60 120 180 240 30050 150 300
Time Since CI Adoption/daysMedian No. Tests
Fig. 8. Unit tests per build following CI adoption.
models, indicating that the project-to-project and language-
to-language variability are responsible for most of the vari-ability explained. We note an increasing time baseline trend
pre-adoption; no statistically signiﬁcant discontinuity at theadoption time; and a slight positive trend post-adoption, as  
(time) + (time
after intervention) >0.
Discussion: The statistical modeling study conﬁrmed the visu-ally apparent increasing trend in the number of issues closedper time period, across the entire interval under observation,although the trend slows down following T
RA VIS CI adoption,
and is smaller than expected from the visual study, indicatingthe moderating inﬂuence of the control variables. Thus, ourmodel ﬁnds that, on average, more issues are being closedover time, but this trend slows with the switch to T
RA VIS CI.
Survey participants experience T RA VIS as beneﬁcial for bug
detection: “I think we produce less bugs” (R45), “there was animmediately noticeable improvement in terms of the numberof serious bugs in production” (R51). An interesting insightinto this matter is provided by R16: while several surveyrespondents have indicated that T
RA VIS CI is not consistent in
terms of performance and relatively slow, those performanceaspects provided R16 with means to detect “more ﬂaky issues”“which where only visible on T
RA VIS ”.
E. RQ 5: Trends in Testing
The last development practice we examine is testing. Here
we only consider a sample of Java projects, for which we couldconﬁdently identify their build system and, consequently, parsetheir T
RA VIS log ﬁles. The sample consists of 250 projects,
each with at least 100 builds, 90+% of builds executing tests.
Exploratory Study: As before, we ﬁrst look for general trends.
Fig. 8 shows a boxplot of per-project median number of testsper build, for each of ﬁve consecutive 60-day time intervalsbefore CI adoption. We aggregated the data here in 60 dayintervals to make it easier to visualize the trend, since thedifferences are small. The horizontal line in each boxplot isthe median value of all per-project medians, and the black dotis their average value. We observe a monotonically increasingtrend in both the medians, from 140 to 160 tests per build,and the means, from 205 to 245, i.e., 15% to 20% increase.
To ascertain if the complexity of builds increases over time,
we also calculated the average number of jobs per build. Themedian is 1for all ﬁve time intervals. These two ﬁndings
suggest strongly that there is an increase in the number ofunit tests per build over time. This, coupled with our ﬁndingthat builds are not getting more complex over time indicatesthat developers are likely spending more time on automatedtesting since CI adoption. This is consistent with Fowler’s“good practices” proposal.
Error Types Study: We also looked at the evolution of the
error types in builds over time after CI adoption. For all250 projects above, we looked at the builds that resulted inerrors, and deconvolved the errors into 8 types after opencoding. We did this over 3 intervals: 0-60 days, 61-180 days,and 181-300 days (i.e., corresponding to the ﬁrst, third, andﬁfth interval in Figure 8). We ﬁnd an apparent upward trendover time in most error type categories, most notably compileerrors, execution errors, failed tests, and skipped test (medianremained 2). All these are consistent with an increase in theamount of code being built and tested per build, as well as anincreasing management of errors by skipping tests, likely toaid in debugging. On the other hand, we ﬁnd a decreasing trendamong errors related to missing ﬁles/dependencies, and time-outs, both consistent with those errors being less of an issueover time, as developers are acculturating to the CI mediatedprocesses. Thus, overall we ﬁnd an expected adjustment to theautomated testing and error handling, with an indication thatdebugging complexity grows over time.
Improving software quality through test automation has
been repeatedly mentioned by the survey respondents as areason to switch to T
RA VIS CI. By using T RA VIS CI they
became more aware of testing-related aspects of their devel-opment process (R32, R52), spent more time and effort ondesigning testing strategies (R5, R53), and encourage otherprojects to embrace T
RA VIS (R26). The efﬁciency of T RA VIS
CI for long-running tests is, however, a concern (R10).
V. R ELATED WORK
Impact of CI has attracted quite some attention from the
research community. The earlier studies [36], [37] interpretCI as distributed development and obligation to integrate
one’s own contributions . Under this deﬁnition no commit size
reduction has been observed in 5,122 Ohloh projects [37].
More recently, the ease of use of the T
RA VIS CI [38] system
led to its popularity on G ITHUB, and triggered a series of
research studies [5], [12], [22], [26], [39], [40]. This line ofresearch is closer to the current work as it performs empiricalanalysis of T
RA VIS CI data. Moreover, similarly to Vasilescu
et al. [26] and Hilton et al. [5] our work can be seen as related
to adoption of T RA VIS CI, and similarly to Beller et al. [22]
we study build failures. Closest in spirit to ours is probablythe work of Vasilescu et al. [12], who report, using different
methodology, increases in a project team’s ability to integrateoutside contributions following adoption of T
RA VIS CI. In our
work we adopt a similar evolutionary perspective, focusing onchanges in development practices in the projects before andafter adoption of T
RA VIS CI; most prior work has compared
projects that adopt CI with others that do not.
Gousious et al. studied work practices and challenges in
pull-based development on G ITHUB[13], [41]. They report
that 75% of the projects surveyed use CI tools to evaluate
68
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:47:57 UTC from IEEE Xplore.  Restrictions apply. code quality [13] and that more than 60% of the surveyed con-
tributors employ automatic testing [41]. Importance of toolingfacilitating the testing tasks has been already recognized byPham et al. [11]: back in 2012 when the authors conducted the
interviews, T
RA VIS CI has only started to support mainstream
languages such as Java and lack of such tooling has beenreported as an important challenge. These ﬁndings furtheremphasize importance of CI in modern software development.
A March 2016 survey of 1,060 IT professionals indicated
that 81% of the enterprises and 70% small and midsizebusinesses implement D
EVOPS[6]. Not surprisingly, industrial
adoption of CI has attracted substantial research attention [7],[19], [42]–[46] and is a subject of recent literature surveyby Eck et al. [47]. However, this line of work is based on
interviews or surveys rather than on analysis of repository dataand, as such, is to a larger extent susceptible to perception bias:e.g., Leppanen et al. report that CI introduction is beneﬁcial
for productivity [19], Eck et al. stress that productivity is likely
to decrease before positive effects can be observed [47], andSt˚ahl and Bosch validated this hypothesis only partially [46].
Automated testing is an important factor that affects the
cost-effectiveness of CI and much effort has been devoted toimprove the quality and efﬁciency of automated testing in CI.Campos et al. [48] enhanced CI with automated test genera-tion. Elbaum et al. [49] and Hu et al. [50] applied test case
selection and test case prioritization to test suites. D ¨osinger
et al. [51] proposed the continuous change impact analysis
technique to improve the effectiveness of automated testing.Long et al. [52] designed tools to support collaborative testing
between testers. Nilsson et al. [53] developed a technique tovisualize end-to-end testing activities in CI processes. All theseefforts aim at improving the cost-effectiveness of CI.
VI. T
HREATS TO VALIDITY
We focus on construct and internal validity [54], as we do
not claim generalizability.
Construct Validity: One of our constructs is a project’s
“CI adoption time,” operationalized as “ﬁrst commit to.travis.yml”. A more precise operationalization wouldhave involved reconciling two additional timestamps: regis-tration of the repository with T
RA VIS CI, and ﬁrst build; the
three timestamps may not coincide, e.g., because T RA VIS CI
can also start a build using some default environment settings(Ruby) without .travis.yml being present. Hence, validity
of the “CI adoption time” construct might have been threatenedby our operationalization. We reduce this threat by excludingthe period immediately before and after our t=0 from all
analyses. Another construct is “size of a code change”. Weoperationalize this as the number of churned lines, customarilyinterpreted as the sum of the number of added and removedlines [55]. In this way, moved lines are counted twice, as beingadded and as being removed. Moreover, we do not distinguishbetween lines of source code and other lines, since it is non-trivial when dealing with many languages.
Internal Validity: To reduce these threats we have opted
for RDD [30], a sound approach to statistical modeling ofdiscontinuity in time series. Application of RDD to SE datahas been recently advocated by Wieringa [56].
Multiple data ﬁltering steps have been applied above. We
tested the robustness of our models by varying the data ﬁlteringcriteria (e.g., 9 month windows instead of 12), and observedsimilar phenomena. We encourage independent replications tofurther assess the robustness of our results.
VII. C
ONCLUSION
This paper focused on the switch to continuous integration
(CI): while several guidelines exist, relatively little has beendone to evaluate the state of practice. We empirically studiedthe impact of adopting T
RA VIS CI on development practices in
a collection of G ITHUBprojects, and surveyed the developers
responsible for introducing T RA VIS in those projects.
We ﬁnd that the reality of adopting T RA VIS CI is much
more complex than suggested by previous work. The increas-ing number of merge commits aligns with the “commit often”guideline, but is likely to be further encouraged by the shiftto a more distributed workﬂow with multiple branches andpull requests (RQ
1). The “commit small” guideline, however,
is followed only to some extent, with large variation betweenprojects (RQ
2). As expected, we observe a general increasing
trend in the number of issues closed over time; however, itwas surprising that this trend slows down after T
RA VIS CI is
introduced (RQ 4). In terms of testing, we ﬁnd that after some
(expected) initial adjustments, the amount (and potentially thequality) of automated tests seems to increase (RQ
5).
The most interesting observations relate to pull request
(PR) evaluations: While we also ﬁnd that, in aggregate, morePRs are being closed after adopting T
RA VIS , as did prior
work [12], our time-series-based analysis suggests that theexpected increasing trend in PRs closed over time manifestsitself only before adopting T
RA VIS ; after, the number of
closed PRs remains relatively stable (RQ 3). At the same time,
PR latencies increase steadily over time, despite the codechanges getting relatively smaller. Future work should employqualitative methods to understand potential causes for thiseffect. We can only speculate here that even with the high levelof automation provided by CI, the ability of teams to scale updistributed development is limited by the availability of humanresources for manual code review (i.e., the project integrators).This calls for a more profound investigation of how G
ITHUB
teams change their PR review practices in response to theintroduction of T
RA VIS CI, as well as additional tool support,
e.g., for automatic prioritization and automatic post-mergedefect prediction, which may help. Future work should alsoexplicitly consider the design decisions and trade-offs betweenseemingly equivalent CI pipeline implementations. Projectspeciﬁc concerns may drive individual implementations, us-age, and practices, as the high amount of variance explainedby our project random effects suggest.
Acknowledgements. BV and VF are supported by the NSF
(1717415, 1717370). YZ is supported by the National Natural
Science Foundation of Jiangsu Province (BK20130014).
69
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:47:57 UTC from IEEE Xplore.  Restrictions apply. REFERENCES
[1] D. DeGrandis, “DevOps: So you say you want a revolution?” Cutter IT
Journal, vol. 24, no. 8, p. 34, 2011.
[2] M. Loukides, What is DevOps? O’Reilly Media, Inc., 2012.
[3] J. Humble and J. Molesky, “Why enterprises must adopt DevOps to
enable continuous delivery,” Cutter IT Journal, vol. 24, no. 8, p. 6,
2011.
[4] J. Roche, “Adopting DevOps practices in quality assurance,” Communi-
cations of the ACM, vol. 56, no. 11, pp. 38–43, 2013.
[5] M. Hilton, T. Tunnell, K. Huang, D. Marinov, and D. Dig, “Usage,
costs, and beneﬁts of continuous integration in open-source projects,”
inInternational Conference on Automated Software Engineering (ASE).
ACM, 2016, pp. 426–437.
[6] RightScale, “State of the cloud report: DevOps trends,”
http://www.rightscale.com/blog/cloud-industry-insights/new-devops-trends-2016-state-cloud-survey, 2016.
[7] M. Hilton, N. Nelson, D. Dig, T. Tunnell, D. Marinov et al., “Continuous
integration (CI) needs and wishes for developers of proprietary code,”Corvallis, OR: Oregon State University, Dept. of Computer Science,Tech. Rep., 2016.
[8] M. Fowler, “Continuous integration,” http://martinfowler.com/articles/
originalContinuousIntegration.html, 2000, accessed: 2016-10-8.
[9] G. Gousios, M. Pinzger, and A. v. Deursen, “An exploratory study of the
pull-based software development model,” in International Conference on
Software Engineering (ICSE). ACM, 2014, pp. 345–355.
[10] S. Stolberg, “Enabling agile testing through continuous integration,” in
Agile Conference (AGILE). IEEE, 2009, pp. 369–374.
[11] R. Pham, L. Singer, O. Liskin, F. Figueira Filho, and K. Schneider,
“Creating a shared understanding of testing culture on a social codingsite,” in International Conference on Software Engineering (ICSE).
IEEE, 2013, pp. 112–121.
[12] B. Vasilescu, Y. Yu, H. Wang, P. T. Devanbu, and V. Filkov, “Quality
and productivity outcomes relating to continuous integration in GitHub,”inJoint Meeting on Foundations of Software Engineering (ESEC/FSE).
ACM, 2015, pp. 805–816.
[13] G. Gousios, A. Zaidman, M.-A. Storey, and A. Van Deursen, “Work
practices and challenges in pull-based development: the integrator’s per-spective,” in International Conference on Software Engineering (ICSE).
IEEE, 2015, pp. 358–368.
[14] Y. Bugayenko, “Continuous integration is dead,” http://www.yegor256.
com/2014/10/08/continuous-integration-is-dead.html, 2014, accessed:2016-10-8.
[15] M. Fowler, “Continuous integration,” http://martinfowler.com/articles/
continuousIntegration.html, 2006, accessed: 2016-10-8.
[16] P. Duvall, S. M. Matyas, and A. Glover, Continuous Integration:
Improving Software Quality and Reducing Risk. Addison-Wesley, 2007.
[17] A. Miller, “A hundred days of continuous integration,” in Agile Confer-
ence (AGILE), 2008, pp. 289–293.
[18] H. M. Y ¨uksel, E. T ¨uz¨un, E. Gelirli, E. Bıyıklı, and B. Baykal, “Using
continuous integration and automated test techniques for a robust C4ISRsystem,” in International Symposium on Computer and Information
Sciences, 2009, pp. 743–748.
[19] M. Leppanen, S. Makinen, M. Pagels, V.-P. Eloranta, J. Itkonen, M. V.
M¨antyl ¨a, and T. Mannisto, “The highways and country roads to contin-
uous deployment,” IEEE Software, vol. 32, no. 2, pp. 64–72, 2015.
[20] F. Cannizzo, R. Clutton, and R. Ramesh, “Pushing the boundaries of
testing and continuous integration,” in Agile Conference (AGILE), 2008,
pp. 501–505.
[21] M. Coram and S. Bohner, “The impact of agile methods on software
project management,” in International Conference and Workshops on the
Engineering of Computer-Based Systems (ECBS’05), 2005, pp. 363–370.
[22] M. Beller, G. Gousios, and A. Zaidman, “Oops, my tests broke the build:
An analysis of Travis CI builds with GitHub,” PeerJ PrePrints, vol. 4,
p. e1984, 2016.
[23] G. Gousios and D. Spinellis, “GHTorrent: GitHub’s data from a ﬁre-
hose,” in Working Conference on Mining Software Repositories (MSR).
IEEE, 2012, pp. 12–21.
[24] B. Vasilescu, K. Blincoe, Q. Xuan, C. Casalnuovo, D. Damian, P. De-
vanbu, and V. Filkov, “The sky is not the limit: Multitasking on GitHubprojects,” in International Conference on Software Engineering (ICSE).
ACM, 2016, pp. 994–1005.
[25] M. Beller, G. Gousios, and A. Zaidman, “TravisTorrent: Synthesizing
Travis CI and GitHub for full-stack research on continuous integration,”inInternational Conference on Mining Software Repositories (MSR).
ACM, 2017.
[26] B. Vasilescu, S. van Schuylenburg, J. Wulms, A. Serebrenik, and
M. G. J. van den Brand, “Continuous integration in a social-codingworld: Empirical evidence from GitHub,” in International Conference
on Software Maintenance and Evolution (ICSME). IEEE, 2014, pp.401–405.
[27] C. Bird, A. Gourley, P. Devanbu, M. Gertz, and A. Swaminathan,
“Mining email social networks,” in International Workshop on Mining
Software Repositories (MSR). ACM, 2006, pp. 137–143.
[28] B. Vasilescu, A. Serebrenik, and V. Filkov, “A data set for social diversity
studies of GitHub teams,” in Working Conference on Mining Software
Repositories (MSR). IEEE, 2015, pp. 514–517.
[29] T. Rausch, W. Hummer, P. Leitner, and S. Schulte, “An empirical
analysis of build failures in the continuous integration workﬂows of Java-based open-source software,” in International Conference on Mining
Software Repositories (MSR). IEEE, 2017.
[30] G. W. Imbens and T. Lemieux, “Regression discontinuity designs: A
guide to practice,” Journal of Econometrics, vol. 142, no. 2, pp. 615–
635, 2008.
[31] T. D. Cook, D. T. Campbell, and A. Day, Quasi-experimentation: Design
& analysis issues for ﬁeld settings. Houghton Mifﬂin Boston, 1979,vol. 351.
[32] A. K. Wagner, S. B. Soumerai, F. Zhang, and D. Ross-Degnan, “Seg-
mented regression analysis of interrupted time series studies in medi-cation use research,” Journal of Clinical Pharmacy and Therapeutics,
vol. 27, no. 4, pp. 299–309, 2002.
[33] S. Nakagawa and H. Schielzeth, “A general and simple method for
obtaining r
2from generalized linear mixed-effects models,” Methods
in Ecology and Evolution, vol. 4, no. 2, pp. 133–142, 2013.
[34] P. C. Johnson, “Extension of nakagawa & schielzeth’s r2
GLMMto
random slopes models,” Methods in Ecology and Evolution, vol. 5, no. 9,
pp. 944–946, 2014.
[35] C. Brindescu, M. Codoban, S. Shmarkatiuk, and D. Dig, “How do
centralized and distributed version control systems impact softwarechanges?” in International Conference on Software Engineering (ICSE).
ACM, 2014, pp. 322–333.
[36] J. Holck and N. Jørgensen, “Continuous integration and quality assur-
ance: a case study of two open source projects,” Australasian J. of Inf.
Systems, vol. 11, no. 1, 2003.
[37] A. Deshpande and D. Riehle, Continuous Integration in Open Source
Software Development. Boston, MA: Springer US, 2008, pp. 273–280.
[38] M. Meyer, “Continuous integration and its tools,” IEEE Software ,
vol. 31, no. 3, pp. 14–16, 2014.
[39] Y. Yu, H. Wang, V. Filkov, P. Devanbu, and B. Vasilescu, “Wait for it:
Determinants of pull request evaluation latency on GitHub,” in Working
Conference on Mining Software Repositories (MSR). IEEE, 2015, pp.367–371.
[40] Y. Yu, G. Yin, T. Wang, C. Yang, and H. Wang, “Determinants of pull-
based development in the context of continuous integration,” Science
China Information Sciences, vol. 59, no. 8, pp. 1–14, 2016.
[41] G. Gousios, M.-A. Storey, and A. Bacchelli, “Work practices and
challenges in pull-based development: the contributor’s perspective,” inInternational Conference on Software Engineering (ICSE). ACM, 2016,pp. 285–296.
[42] E. Laukkanen, M. Paasivaara, and T. Arvonen, “Stakeholder perceptions
of the adoption of continuous integration – a case study,” in Agile
Conference (AGILE), 2015, pp. 11–20.
[43] A. Debbiche, M. Dien ´er, and R. Berntsson Svensson, Challenges When
Adopting Continuous Integration: A Case Study. Cham: SpringerInternational Publishing, 2014, pp. 17–32.
[44] D. St ˚ahl and J. Bosch, “Automated software integration ﬂows in in-
dustry: A multiple-case study,” in International Conference on Software
Engineering (ICSE) Companion. ACM, 2014, pp. 54–63.
[45] ——, “Modeling continuous integration practice differences in industry
software development,” Journal of Systems and Software, vol. 87, pp.
48–59, 2014.
[46] ——, “Experienced beneﬁts of continuous integration in industry soft-
ware product development: A case study,” in IASTED International
Conference on Software Engineering, 2013, pp. 736–743.
[47] A. Eck, F. Uebernickel, and W. Brenner, “Fit for continuous integration:
How organizations assimilate an agile practice,” in 20th Americas
Conference on Information Systems (AMCIS). AIS, 2014.
70
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:47:57 UTC from IEEE Xplore.  Restrictions apply. [48] J. Campos, A. Arcuri, G. Fraser, and R. Abreu, “Continuous test gener-
ation: enhancing continuous integration with automated test generation,”
inInternational Conference on Automated Software Engineering (ASE).
ACM, 2014, pp. 55–66.
[49] S. Elbaum, G. Rothermel, and J. Penix, “Techniques for improving
regression testing in continuous integration development environments,”inInternational Symposium on Foundations of Software Engineering
(FSE). ACM, 2014, pp. 235–245.
[50] J. Hu and Y. Li, “Implementation and evaluation of automatic prioriti-
zation for continuous integration test cases,” Master’s thesis, ChalmersUniversity of Technology — University of Gothenburg, 2016.
[51] S. D ¨osinger, R. Mordinyi, and S. Bifﬂ, “Communicating continuous
integration servers for increasing effectiveness of automated testing,”inInternational Conference on Automated Software Engineering (ASE).
ACM, 2012, pp. 374–377.[52] T. Long, “Collaborative testing across shared software components
(doctoral symposium),” in International Symposium on Software Testing
and Analysis (ISSTA). ACM, 2015, pp. 436–439.
[53] A. Nilsson, J. Bosch, and C. Berger, “Visualizing testing activities to
support continuous integration: A multiple case study,” in International
Conference on Agile Software Development. Springer, 2014, pp. 171–186.
[54] D. E. Perry, A. A. Porter, and L. G. Votta, “Empirical studies of soft-
ware engineering: a roadmap,” in International Conference on Software
Engineering (ICSE). ACM, 2000, pp. 345–355.
[55] E. Giger, M. Pinzger, and H. C. Gall, “Comparing ﬁne-grained source
code changes and code churn for bug prediction,” in Working Conference
on Mining Software Repositories (MSR). ACM, 2011, pp. 83–92.
[56] R. J. Wieringa, Abductive Inference Design. Springer, 2014, pp. 177–
199.
71
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:47:57 UTC from IEEE Xplore.  Restrictions apply. 