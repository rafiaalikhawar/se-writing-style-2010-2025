FaCoY – A Code-to-Code Search Engine
Kisub Kim1, Dongsun Kim1∗, Tegawendé F. Bissyandé1∗,
Eunjong Choi2,L iL i3∗, Jacques Klein1, Yves Le Traon1
1SnT, University of Luxembourg - Luxembourg
2Nara Institute of Science and Technology (NAIST) - Japan
3Faculty of Information Technology, Monash University - Australia
ABSTRACT
Code search is an unavoidable activity in software development.
Various approaches and techniques have been explored in the liter-
ature to support code search tasks. Most of these approaches focus
on serving user queries provided as natural language free-form
input. However, there exists a wide range of use-case scenarios
whereacode-to-codeapproachwouldbemostbeneficial.Forex-
ample,researchdirectionsincodetransplantation,codediversity,
patch recommendation canleverage a code-to-code search engine
to find essential ingredients for their techniques. In this paper, we
proposeFaCoY,anovelapproachforstaticallyfindingcodefrag-
mentswhichmaybe semantically similartouserinputcode.FaCoY
implements a query alternation strategy: instead of directly match-
ingcodequerytokenswithcodeinthesearchspace,FaCoYfirst
attemptstoidentifyothertokenswhichmayalsoberelevantinim-plementingthefunctionalbehavioroftheinputcode.Withvarious
experiments, we show that (1) FaCoY is more effective than online
code-to-codesearchengines;(2)FaCoYcandetectmoresemantic
codeclones(i.e.,Type-4)inBigCloneBenchthanthestate-of-the-
art;(3)FaCoY,whilestatic,candetectcodefragmentswhichare
indeed similar with respect to runtime execution behavior; and (4)
FaCoY can be useful in code/patch recommendation.
1 INTRODUCTION
Insoftwaredevelopmentactivities,sourcecodeexamplesarecriticalforunderstandingconcepts,applyingfixes,improvingperformance,
andextendingsoftwarefunctionalities[ 6,46,63,87,88].Previous
studieshaveevenrevealedthatmorethan60%ofdeveloperssearch
for source code every day [ 30,80]. With the existence of super-
repositoriessuchas GitHubhostingmillionsofopensourceprojects,
there are opportunities to satisfy the search need of developers for
resolving a large variety of programming issues.
Oftentimes,developersarelookingforcodefragmentsthatoffer
similar functionalitythan someother codefragments. For example,
adevelopermayneedtofindJavaimplementationsofallsorting
algorithms that could be more efficient than the one she/he has.
We refer to such code fragments which have similar functional
behavior even if their code is dissimilar as semantic clones. The
∗Corresponding authors.
Publication rights licensed to ACM. ACM acknowledges that this contribution was
authoredor co-authoredbyanemployee, contractororaffiliateof anationalgovern-
ment.Assuch,theGovernmentretainsanonexclusive,royalty-freerighttopublishor
reproduce this article, or to allow others to do so, for Government purposes only.
ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
©2018 Copyright held by the owner/author(s). Publication rights licensed to the
Association for Computing Machinery.
ACM ISBN 978-1-4503-5638-1/18/05...$15.00
https://doi.org/10.1145/3180155.3180187literature also refers to them as Type-4clones for consistency with
the taxonomy of code clones [ 13,73]. Besides the potential of help-
ing developers collect relevant examples to improve their code,
finding similar code fragments is an important endeavor, at they
can provide essential ingredients for addressing challenges in vari-
ous software engineering techniques. Among such challenges, we
canenumerateautomatedsoftwaretransplantation[ 9],software
diversification [11], and even software repair [41].
Findingsemanticallysimilarcodefragments is,howev er,chal-
lenging to perform statically, an essential trait to ensure scalability.
Afewstudies[ 21,35]haveinvestigatedprograminputsandoutputs
tofindequivalentcodefragments.Morerecently,Suetal.[ 81]have
proposed an approach to find code relatives relying on instruction-
levelexecution traces(i.e.,codewith similarexecutionbehaviors).
Unfortunately,allsuchdynamicapproachescannotscaletolarge
repositories because of their requirement of runtime information.
Ourkeyinsighttostaticallyfindcodefragmentswhicharese-
manticallysimilarisfirsttoundertakeadescriptionofthefunction-
alityimplementedbyanycodefragment.Then,suchdescriptions
can be used to match other code fragments that could be described
similarly. This insight is closely related to the work by Marcus
and Maletic on high-level concept clones [62] whose detection is
based on source code text (comments and identifiers) providing an
abstract view of code. Unfortunately, their approach can only help
to identify high-level concepts (e.g., abstract data types), but is not
targeted at describing functionalities per se.
Because of the vocabulary mismatch problem [ 24,29,89,90]be -
tweencodetermsandhumandescriptionwords,itischallengingto
identifythemostaccuratetermstosummarize,innaturallanguage,
the functionality implemented by a code fragment.
To work around the issue of translating a code fragment into
natural language description terms, one can look2up to a devel-
oper community. Actually, developers often resort to web-based re-sourcessuchasblogs,tutorialpages,andQ&Asites.
StackOverflow
is one of such leading discussion platforms, which has gained pop-
ularity among software developers. In StackOverflow , an answer
to a question is typically short texts accompanied by code snip-pets that demonstrate a solution to a given development task orthe usage of a particular functionality in a library or framework.
StackOverflow providessocialmechanismstoassessandimprove
thequalityofpoststhatleadsimplicitlytohigh-qualitysourcecode
snippets on the one hand as well as concise and comprehensive
questionsontheotherhand.Ourintuitionisthatinformationin
Q&Asitescanbeleveragedasacollectiveknowledgetobuildan
2Becausesoftwareprojectscanbemoreorlesspoorlydocumented,wedonotconsider
sourcecodecommentsasareliableandconsistentdatabaseforextractingdescriptions.
Instead, we rely on developer community Q&A sites to collect descriptions.
9462018 ACM/IEEE 40th International Conference on Software Engineering
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:38:44 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden K. Kim et al.
intermediatetranslationstepbeforetheexplorationoflargecode
bases.
This paper. We propose FaCoY ( FindaCodeother than Yours)
as a novel, static, scalable and effective code-to-code search engine
for finding semantically similar code fragments in large code bases.
Overall, we make the following contributions:
•TheFaCoYapproach for code-to-code search :W ep r o po s ea
solutiontodiscovercodefragmentsimplementingsimilarfunction-
alities. Our approach radically shifts from mere syntactic patterns
detection. It is further fully static (i.e., relies solely on source code)
with no constraint of having runtime information. FaCoY is based
onqueryalternation:afterextractingstructuralcodeelementsfrom
a code fragment to build a query, we build alternate queries us-
ingcodefragmentsthatpresentsimilardescriptionstotheinitial
code fragment. We instantiate the FaCoY approach based on in-
dices on Java files collected from GitHub and Q&A posts from
StackOverflow tofindthebestdescriptionsoffunctionalitiesim-
plemented in a large and diversified set of code snippets.•A comprehensive empirical evaluation
. We present evalua-
tionresultsdemonstrating thatFaCoYcanaccuratelyhelp search
for (syntactically and semantically) similar code fragments, outper-
forming popular online code-to-code search engines. We further
show,withtheBigCloneBenchbenchmark[ 83],thatweperform
better than the state-of-the-art on static code clone detectors iden-
tifying semantic clones; our approach identifies over 635,000 se-
manticclones,whileothersdetectfewtonosemanticclones.We
also break down the performance of FaCoY to highlight the added-
valueofour queryalternation scheme.UsingtheDyCLINKdynamic
tool[81],wevalidatethat,in68%ofthecases,ourapproachindeed
finds code fragments that exhibit similar runtime behavior. Finally,
weinvestigatethecapabilityofFaCoYtobeleveragedforrepair
patch recommendation.
2 MOTIVATION AND INSIGHT
Finding similar code fragments beyond syntactic similarities has
severalusesinthefieldofsoftwareengineering.Forexample,de-
velopers can leverage a code-to-code search tool to find alternative
implementations of some functionalities. Recent automated soft-
ware engineering research directions for software transplantation
or repair constitute further illustrations of how a code-to-code
search engine can be leveraged.
Despiteyearsofactiveresearchinthefieldofcodesearchand
codeclonesdetection,fewtechniqueshaveexplicitlytargetedse-
mantically similar code fragments. Instead, most approaches focus
ontextually,structurallyorsyntacticallycodefragments.Thestate-
of-the-art techniques on static detection of code clones leverage
various intermediate representations to compute code similarity.
Token-based[ 8,39,56]representationsareusedinapproachesthat
targetsyntacticsimilarity.AST-based[ 12,34]representationsare
employedinapproachesthatdetectsimilarbutpotentiallystruc-turally different code fragments. Finally, (program dependency)
graph-based[ 49,57]representationsareusedindetectingclones
where statements are not ordered or parts of the clone code are
intertwinedwitheachother.Althoughsimilarcodefragmentsiden-
tified by all these approaches usually have similar behavior, the
contemporarystaticapproachesstillmissfindingsuchfragments
which have similar behavior even if their code is dissimilar [36].To find similarly behaving code fragments, researchers have
relied upon dynamic code similarity detection which consists in
identifyingprogramsthatyieldsimilaroutputsforthesameinputs.
State-of-the-art dynamic approaches generate random inputs [ 35],
relyonsymbolic[ 55]orconcolicexecution[ 50]andcheckabstract
memory states [ 45] to compute function similarity based on exe-
cution outputs. The most recent state-of-the-art on dynamic clone
detection focuseson thecomputations performedby thedifferent
programs and compares instruction-level execution traces to iden-
tify equivalent behavior [ 81]. Although these approaches can be
very effective in finding semantic code clones, dynamic execution
of code is not scalable and implies several limitations for practical
usage(e.g.,the needofexhaustivetestcasesto ensureconfidence
in behavioral equivalence).
Tosearchforrelevantcodefragments,usersturntoonlinecode-
to-code search engines, such as Krugle [ 1], which statically scan
open source projects. Unfortunately, such Internet-scale searchengines still perform syntactic matching, leading to low-quality
output in terms of semantic clones.
Onthekeyidea ConsiderthecodefragmentsshowninFigure1.
Theyconstitutevariantimplementationsforcomputingthehashof
astring.TheseexamplesarereportedinBigCloneBench[ 83]astype-
4 clones (i.e., semantic clones). Indeed, their syntactic similarity is
limited to a few library function calls. Textually, only about half of
the terms are similar in both code fragments. Structurally, the first
implementationpresentsonlyoneexecutionpathwhilethesecond
includes two paths with the try/catch mechanism.
public String getHash(final String password)
throws NoSuchAlgorithmException, UnsupportedEncodingException {
final MessageDigest digest = MessageDigest.getInstance("MD5");
byte[] md5hash;digest.update(password.getBytes("utf-8"), 0, password.length());md5hash = digest.digest();return convertToHex(md5hash);
}
(a) Excerpt from MD5HashHelperImpl.java in theyes-cart project.
public static String encrypt(String message) {
try {
MessageDigest digest = MessageDigest.getInstance("MD5");digest.update(message.getBytes());BASE64Encoder encoder = new BASE64Encoder();return encoder.encode(digest.digest());
} catch (NoSuchAlgorithmException ex) {...
(b) Excerpt from Crypt.java in theBettaServer project.
Figure 1: Implementation variants for hashing.
Tostaticallyidentifythecodefragmentsaboveassemantically
similar, a code-to-code search engine would require extra hint that
(i)getHashandencryptdeal with related concepts and that (ii)
BASE64Encoder APIisofferingsimilarfunctionalityas convertTo-
Hex. Such hints can be derived automatically if one can build a
comprehensive collection of code fragments with associated de-
scriptionsallowingforhigh-levelgroupingsoffragments(basedon
naturallanguagedescriptions)toinferrelationshipsamongcode
tokens. The inference of such relationships will then enable the
generationof alternatequeries displayingrelated,butpotentially
syntacticallydifferent,tokensb orrowedfrom othercodefragments
havingsimilardescriptionsthantheinputfragment.Thus,given
thecodeexampleofFigure1(a),thesystemwilldetectsimilarcode
947
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:38:44 UTC from IEEE Xplore.  Restrictions apply. FaCoY – A Code-to-Code Search Engine ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
fragmentsbymatchingnotonlytokensthataresyntactically3simi-
lartotheonesinthiscodefragment(i.e., gethash,messagedigest ,
andconverttohex ),butalsootherssimilartoknownrelatedtokens
(e.g.,base64encoder ,encrypt, etc.). Such a system would then be
abletoidentifythecodefragmentofFigure1(b)asasemantically
similar code fragment (i.e., a semantic clone).
We postulate that Q&A posts and their associated answers con-
stitute a comprehensive dataset with a wealth of information on
how different code tokens (found in code snippets displayed as ex-
ampleanswers)canbematchedtogetherbasedonnaturallanguage
descriptions (found in questions). Figure 2 illustrates the steps that
could be unfolded for exploiting Q&A data to find semantic clones
in software repositories such as Github.
What functionality 
is implemented? 
What are related 
implementations?What are other representative 
tokens for the functionality?
Which code fragments best match 
these tokens?
Q&A Posts (questions + code snippets)
Target code baseCode fragment
Similar Code
fragments
Figure 2: Conceptual steps for our search engine.
Givenacodefragment,thefirststepwouldconsisttoinfernatu-
rallanguagetermsthatbestdescribethefunctionalityimplemented.
To that end, we must match the code fragment with the closest
codeexampleprovidedinaQ&Apost.Then,wesearchintheQ&A
datasetallpostswithsimilardescriptionstocollecttheirassociated
codeexamples.Bymixingallsuchcodeexamples,wecanbuilda
morediversesetofcodetokensthatcouldbeinvolvedintheimple-mentationoftherelevantfunctionality.Usingthissetoftokens,we
can then search real-world projects for fragments which may be
syntactically dissimilar while implementing similar functionality.
Basic definitions
We use the following definitions for our approach in Section 3.
•Code Fragment : A contiguous set of code lines that is fed as
input to the search engine. The output ofthe engine is also a listof code fragments. We formalize it as a finite sequence of tokens
representing (full or partial) program source code at different gran-
ularities: e.g., a function, or an arbitrary sequence of statements.•CodeSnippet
:AcodefragmentfoundinQ&Asites.Wepropose
this terminology to differentiate code fragments that are leveragedduring the search process from those that are used as input or that
are finally yielded by our approach.
•Q&APost :Apairp=(q,A)alsonoted pq
A,whereqisaquestion
andAisalistofanswers.Forinstance,foragivenpost pq
A,question
qis a document describing what the questioner is trying to ask
aboutand a∈Aisadocumentthatanswersthequestionin q.Each
answeracanincludeoneorseveralcodesnippets: S=snippets (a),
whereSis a set of code snippets.
We also recall for the reader the following well-accepted defini-
tions of clone types [13, 73, 81]:
•Type-1: Identical code fragments, except for differences in white-
space, layout, and comments.
3Naive syntactic matching may lead to false positives. Instead, there is a need to
maintain qualification information about the tokens (e.g., the token represents a
method call, a literal, etc.) cf. Section 3.•Type-2: Identical code fragments, except for differences in identi-
fiernamesandliteralvalues,inadditiontoType-1clonedifferences.
•Type-3: Syntactically similar code fragments that differ at the
statement level. The fragments have statements added, modified
and/orremovedwithrespecttoeachother,inadditiontoType-1
and Type-2 clone differences.•Type-4
: Syntacticallydissimilar code fragmentsthat implement
the same functionality. They are also known as semantic clones.
Disclaimer. Inthiswork,werefertoapairofcodefragmentswhich
are semantically similar as semantic clones, although they might
have been implemented independently (i.e., no cloning, e.g., copy/-
paste, was involved). Such pairs are primary targets of FaCoY.
3 APPROACH
FaCoYtakesacodefragmentfromauserandsearchesinasoftware
projects’ code base to identify code fragments that are similar to
theuser’sinput.AlthoughthedesignofFaCoYistargetedattaking
intoaccountfunctionallysimilarcodewithsyntacticallydifferent
implementations,thesearchoftenreturnsfragmentsthatarealso
syntactically similar to the input query.
Figure 3 illustrates the steps that are unfolded in the working
process of the search:
User Input
Snippet
IndexCode
IndexCode
Query
Generating
Expanded 
Code QueryCode
Queries
Code Fragment
Search Results(2)
(3)
(4) (5)
Question
Index
Generating 
Code Query
(1)
Question
Answer
Snippet
Question
Answer
Snippet
()
()
Searching for 
Similar Code 
Snippets
()
Searching for 
Similar 
Questions
(5)
(5)Searching for 
Code 
Examples
Figure 3: Overview of FaCoY.
(1)WhenFaCoYreceivesacodefragment,itgeneratesastructured
query called Code Query based on the code elements present in
the fragment (Section 3.2.1).
(2)Given a code query, FaCoY searches for Q&A posts that in-
clude the most syntactically similar code snippets. To that end,
the query is matched against the Snippet Index of Q&A posts
(Section 3.2.2).
(3)Oncetherelevantpostsareidentified,FaCoYcollectsnatural
languagedescriptivetermsfromtheassociatedquestionsand
matches them with the Question index of Q&A posts to find
other relevant posts. The objective is to find additional code
snippets that could implement similar functionalities with a
diversified set of syntactic tokens (Section 3.2.3).
(4)Using code snippets in answers of Q&A posts collected by pre-
vioussteps,FaCoYgeneratescodequeriesthateachconstitutes
an alternative tothe first code query obtained fromuser input
in Step (1) (Section 3.2.4).
(5)As the final step, FaCoY searches for similar code fragmentsby matching the code queries yielded in Step (4) against the
Code Index built from the source code of software projects (Sec-
tion 3.2.5).
948
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:38:44 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden K. Kim et al.
3.1 Indexing
Ourapproachconstructsthreeindices, snippet,question,andcode,
in advance to facilitate the search process as well as ensuring a
reasonablesearchspeed[ 61].Tocreatetheseindices,weuseApache
Lucene,oneofthemostpopularinformationretrievallibraries[ 64].
Luceneindicesmaptokensintoinstanceswhichinourcasescan
be natural language text, code fragments or source code files.
3.1.1Snippet Index. TheSnippetIndexinFaCoYmaintains
themappinginformationbetweenanswerdocumentIDsofQ&A
posts and their code snippets. It is defined as a function:
InxS:S→2P, whereSis a set of code snippets, and Pis a set of
Q&Aposts.2Pdenotesthepowersetof P(i.e.,thesetofallpossible
subsetsof P).Thisindexfunctionmapsacodesnippetintoasubset
ofP, in which the answer in a post has a similar snippet to the
input. Our approach leverages the Snippet Index to retrieve the
Q&A post answers that include the most similar code snippets to a
given query.
Tocreatethisindex,wemustfirstcollectcodeexamplesprovided
as part of a Q&A post answer. Since code snippets are mixed in
the middle of answer documents, it is necessary to identify such
regions containing the code snippets. Fortunately, most Q&A sites,
including StackOverflow , make posts available in a structured
document (e.g., HTML or XML) and explicitly indicate source code
elements with ad-hoc tags such as <code>···<code>allowing
FaCoYtoreadilyparseanswerdocumentsandlocatecodesnippets.
Aftercollectingacodesnippetfromananswer,FaCoYcreates
its corresponding index information as a list of index terms. Anindex term is a pair of the form ‘
token_type :actual_token” (e.g.,
used_class :ActionBar).Table1enumeratesexamplesoftokentypes
considered by FaCoY. The complete list is available in [23].
Table1:Examplesoftokentypesforsnippetindexcreation.
TypeDescription
typed_method_call (Partially) qualified name of called method
unresolved_method_call Non-qualified name of called method
str_literal String literal used in code
Figure 4 shows an example of code fragment with the corre-
sponding index terms.
public class BaseActivity extends AppCompatActivity{
public static final int IMAGE_PICK_REQUEST_CODE = 5828;
public static final int MUSIC_PICK_REQUEST_CODE = 58228;protected ActionBar actionBar;@Override
protected void onCreate(Bundle savedInstanceState){
super.onCreate(savedInstanceState);
setContentView(R.layout.activity_based);
}}
(a) Example code fragment.
extends:AppCompatActivity
used_class:BaseActivity
used_class:Rused_class:R.layoutused_class:ActionBarused_class:Bundlemethods_declaration:onCreate
typed_method_call:AppCompatActivity.onCreate
typed_method_call:AppCompatActivity.setContentView
(b) Corresponding index terms.
Figure 4: Extraction of index terms from a code fragment.
To generate index terms, FaCoY must produce the abstract syn-
taxtree(AST)ofacodesnippet.EachASTnodethatcorrespondsto a tokentype in Table 14is then retainedto form an index term.
Finally,FaCoYpreserves,foreachentryintheindex,theanswer
document identifier to enable reverse lookup.
Themainchallengeinthisstepisduetothedifficultyofparsing
incompletecode,acommontraitofcodesnippetsinQ&Aposts[ 79].
Indeed, it is common for such code snippets to include partial
statements or excerpts of a program, with the purpose to give only
somekeyideasinaconcisemanner.Often,snippetsincludeellipses
(i.e., “...”)b efore and after the main code blocks. To allow parsing
bystandardJavaparsers,FaCoYresolvestheproblembyremoving
the ellipses and wrapping code snippets with a custom dummy
class and method templates.
Besidesincompleteness,codesnippetspresentanotherlimitation
due tounqualified names. Indeed, in code snippets, enclosing class
namesofmethodcallsareoftenambiguous[ 20].Arecentstudy[ 82]
even reported that unqualified method names are pervasive in
codesnippets.Recoveringunqualifiednamesis,however,necessary
for ensuring accuracy when building the Snippet Index. To that
end,FaCoYtransformsunqualifiednamesto(partially)qualified
names by using structural information collected during the AST
traversal of a given code snippet. This process converts variable
namesonwhichmethodsareinvokedthroughtheircorresponding
classes. Figure 5 showcases the recovering of name qualificationinformation. Although this process cannot recover all qualified
names, it does improve the value of the Snippet Index.
SAXParserFactory ft;
InputSource is ;URL ul = new URL(feed)ft = SAXParserFactory.newInstance();SAXParser pr = factory.newSAXParser();XMLReader rd = pr.getXMLReader();
RssHandler hd = new RssHandler();
rd.setContentHandler(hd);is = new InputSource(url.openStream());xmlreader.parse(is);return hd.getFeed();
(a) Fragment before recovering
name qualification.SAXParserFactory ft;InputSource is;URL ul = new URL(feed)ft = SAXParserFactory.newInstance();SAXParser pr = _SAXParserFactory_.newSA...
XMLReader rd = _SAXParser_.getXMLReader();
RssHandler hd = new RssHandler();_XMLReader_.setContentHandler(hd);is = new InputSource(_URL_.openStream());_XMLReader_.parse(is);return_RssHandler_.getFeed();
(b) Fragment after recovering name
qualification.
Figure5:Recoveryofqualificationinformation[79].Recov-
ered name qualifications are highlighted by red color.
Disclaimer. FaCoYiscompliantwiththeCreativeCommonsAttribute-
ShareAlikelicense[ 4,19]ofStackOverflow :wedonotredistribute
any code from Q&A posts. We only mine developer code examples
inStackOverflow to learn relationships among tokens.
3.1.2QuestionIndex. TheQuestionIndexmapsasetofword
tokensintoQ&Aposts.Themappinginformationservestoiden-
tify Q&A posts where the questions are similar to the questions
retrievedinStep(2)(cf.Figure3)whoseanswerscontaincodesnip-
pets that aresimilar to the userinput. FaCoY leverages this index
toconstructalternatequeriestotheonederivedfromuserinput.
These alternate queries are necessary to increase the opportunities
for finding semantically similar code fragments rather than only
syntactically similar fragments. The following equation defines
the Question Index function: InxQ:Q→2P, whereQis a set of
questions, and Pis a set of Q&A posts. This function maps a set
of words from the input into a subset of posts ( PE∈P) that are
similar to the input.
4Our approach focuses on the types in the table since they represent most of the
characteristics in a code snippet.
949
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:38:44 UTC from IEEE Xplore.  Restrictions apply. FaCoY – A Code-to-Code Search Engine ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
To buildtheQuestionIndex,FaCoYtakesthequestion part( q)
of each Q&A post and generates index terms. To that end a pre-
processing of the question text is necessary. This pre-processingincludes tokenization (e.g., splitting camel case), stop word re-
moval5[61],andstemming.Fromthepre-processingoutput,FaCoY
builds index terms in the form of “ term:token”. Each index term
isfurthermappedwiththequestionwhereitstokenisoriginated
from, to keep an inverse link. Figure 6 illustrates how, given a
question text, index terms are generated.
(a) Description text in a question.
term:simpl, term:java, term:algorithm, term:generat, term:pseudo, term:random, term:alpha, 
term:numer, term:string, term:situat, term:uniq, term:session, term:key, term:identifi, term:uniq, 
term:500k, term:requir, term:sophist, term:ideal, term:length, term:depend, term:string, 
term:length, term:12, term:aeygf7k0dm1x 
(b) Resulting index terms generated from (a).
Figure 6: Example of question index creation.
3.1.3Code Index. TheCode Index maintains mappinginfor-
mationbetweentokensandsourcecodefiles.FaCoYleveragesthis
indextosearchforcodeexamplescorrespondingtoacodequery
yielded at the end of Step (4) (cf. Figure 3). This index actually
definesthesearchspaceofourapproach(e.g.,F-droidrepository
ofAndroidapps,or Javaprojects inGithub, ora subsetof Mozilla
projects). The Code Index function is defined as: InxC:S→2F,
whereSisasetofcodesnippetsand Fisasetofcodefragments. F
actually defines the space of FaCoY.
The creation process of the Code Index is similar to the process
for building the Snippet Index that is described in Section 3.1.1.
FaCoYfirstscansallavailablesourcecodefilesintheconsidered
codebase.Then,eachfileisparsed6togenerateanASTfromwhich
FaCoY collects the set of AST nodes corresponding to the token
types listed in Table 1. The AST nodes and their actual tokens are
usedforcreatingindextermsinthesameformatasinthecaseof
theSnippetIndex.Finally,eachindextermismappedtothesource
code file where the token of the term has been retrieved.
3.2 Search
Once the search indices are built, FaCoY is ready to take a userquery and search for relevant code fragments. Algorithm 1 for-
mulates the search process for a given user query. Its input also
considers the three indices described in Section 3.1 and stretch
parametersusedinthealgorithm.Thefollowingsectionsdetailthe
whole process.
3.2.1GeneratingaCodeQueryfromaUserInput. Asthe
first step of code search, FaCoY takes a user input and generates a
code query from the input to search the snippet index (Line 2 in
Algorithm 1). The code query is in the same form of index terms
illustrated in Figure 4 so that it can be readily used to match the
index terms in the Snippet Index.
5Using Lucene’s (version 4) English default stop word set.
6Thisstepalsorecoversqualifiednamesbyapplying,whenevernecessary,thesame
procedure described in Section 3.1.1.Algorithm 1: Similar code search in FaCoY.
Input :c: code fragment (i.e., user query).
Input :InxS(qs): a function of a code snippet, s, to a sorted list of posts, Ps.
Input :InxQ(qq): a function of a question, q, to a sorted lis of questions, Qq.
Input :InxC(qs): a function of a code snippet, s, to a sorted list of code
fragments, Fs.
Input :ns,nq,nc: stretch parameters for snippet, question, and code search,
respectively (e.g., consider Top nsimilar snippets).
Output:F: a set of code fragments that are similar to c.
1Function SearchSimilarCodeExamples( c,InxS,InxQ,InxC,ns,nq,nc)
2qin⇐genCodeQuery( c);
3Ps⇐InxS(qin).top(ns);
4Qs⇐Ps.foreach(pi=>takeQuestion (pi));
5Pe⇐Qs.foreach(qi=>InxQ(qi).top(nq));
6letF⇐∅;
7foreachpi∈Pedo
8 s⇐takeSnippet (дetAnswer (pi));
9 qex⇐genCodeQuery( s);
10 F⇐F∪InxC(qex).top(nc));
11end
12returnF;
13end
To generate a code query, our approach follows the process
described in Section 3.1.1 for generating the index terms of anygiven code snippet. If the user input is also an incomplete codefragment (i.e., impossible to parse), FaCoY seamlessly wraps thefragmentusingadummyclassandsomemethodtemplatesafterremoving ellipses. It then parses the code fragment to obtain an
ASTand collectthenecessary ASTnodesto generateindexterms
in the form of token_type:actual_token.
3.2.2Searching for Similar Code Snippets. After generat-
ingacodequeryfromauserinput,ourapproachtriestosearchfor
similarsnippetsinanswersofQ&Aposts(Line3inAlgorithm1).
Since thecode queryand index termsin the snippet index are in
thesameformat,ourapproachusesfull-textsearch(i.e.,examining
all index terms for a code snippet to compare with those in a code
query). The full-text function implemented by Lucene is utilized.
Our approach computes rankings of the search results based
on a scoring function that measures the similarity between the
code query and matched code snippets. FaCoY integrates two scor-
ing functions, Boolean Model (BM) [ 51] and Vector Space Model
(VSM) [76], which are already implemented in Lucene. First, BM
reducestheamountofcodesnippetstoberanked.Ourapproach
transforms the code query of the previous step, qin, into a nor-
malform andmatches codesnippets indexedinthe snippetindex.
Weadoptbestmatchretrievaltofindasmanysimilarsnippetsas
possible. Then, for the retained snippets, VSM computes similarity
scores. After computing TF-IDF (Term Frequency - Inverse Doc-ument Frequency) [
75] of terms in each snippet as a weighting
scheme, it calculates Cosine similarity values between the code
query and indexed snippets.
From the sorted list of similar snippets, FaCoY takes top ns
snippets(i.e.,thosethatwillallowtoconsideronlymostrelevant
natural language descriptions to associate with the user input).Bydefault,inallourexperimentsinthispaper,unlessotherwise
indicated, we set the value of ns(stretch parameter) to 3.
3.2.3Searching for Similar Questions. In this step (Line 4
in Algorithm 1), our approach searches for questions similar to
950
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:38:44 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden K. Kim et al.
thequestionsofQ&A postsretrievedinthepreviousstep(cf. Sec-
tion 3.2.2). The result of this step is an additional set of Q&A posts
containing questions that are similar to the given questions identi-
fied as describing best the functionality implemented in the user
input.Thankstothis searchspaceenrichment approach,FaCoY
canincludemorediversecodesnippetsforenumeratingmorecode
tokens which are semantically relevant.
Tosearchforsimilarquestions,weusetheQuestionIndexde-
scribed in Section 3.1.2. Since all questions are indexed beforehand,
theapproachsimplycomputessimilarityvaluesbetweenquestions
as the previous step does (cf. Section 3.2.2), i.e., filtering questions
based on BM and calculating cosine similarity based on VSM.
Once similarity scores are computed, we select the top nqposts
based on the scores of their questions, as the goal is to recommend
most relevant questions rather than listing up all similar questions.
Since it takes nsposts for each of nqquestions retrieved in Line
3 of Algorithm 1, the result of this step consists of ns×nqposts
when using the same stretch parameter for both steps. FaCoY can
be tuned to consider different stretch values for each step.
3.2.4GeneratingAlternateCodeQueries. Thisstep(Line5
in Algorithm 1) generates code queries from code snippets present
in newly retrieved Q&A posts at the end of the previous step (cf.Section 3.2.3). Our approach in this step first takes Q&A postsidentifiedinLine4andextractscodesnippetsfromtheiranswer
parts. It then follows the same process described in Section 3.2.1 togenerate code queries. Since the result of the previous step (Line 4)
isns×nqposts(whenusingthesamevalueforstretchparameters),
this step generates at most ns×nqcode queries, referred to as
alternate queries.
3.2.5Searching for Similar Code fragments. As the last
step, FaCoY searches the Code Index for similar code fragments
to output (Lines 6–12 in Algorithm 1). Based on the alternate code
queries generated in the previous step (cf. Section 3.2.4), and since
codequeriesandindextermsarerepresentedinthesameformat,
FaCoY can leverage the same process of Step (2) illustrated in Sec-
tion 3.2.2 to match code fragments. While the step described in
Section 3.2.2 returns answers containing code snippets similar to a
userquery,theresultofthisstepisasetofsourcecodefilescontain-ingcodefragmentscorrespondingtothealternatecodequeryfrom
the previous step.Note that FaCoY provides at most ns×nq×nc
codefragmentsasLine10inAlgorithm1uses nctotaketopresults.
Delimitatingcodefragments :Sincedisplayingtheentirecon-
tent of a source code file will be ineffective for users to readily
locate the identified similar code fragment, FaCoY specifies a code
range after summarizing the content [ 61]. To summarize search
results into a specific range, FaCoY uses a query-dependent ap-proach that displays segments of code based on the query termsoccurring in the source file. Concretely, the code fragment starts
fromklines preceding the first matched token and spreads until k
lines following the last matched token.
4 EVALUATION
In this section, we describe the designof different assessment sce-
narios for FaCoY and report on the evaluation results. Specifically,
our experiments aim to address the following research questions:•RQ1:HowrelevantarecodeexamplesfoundbyFaCoYcom-
pared to other code-to-code search engines?
•RQ2:WhatistheeffectivenessofFaCoYinfindingsemantic
clones based on a code clone benchmark?
•RQ3:Dothesemanticallysimilarcodefragmentsyieldedby
FaCoY exhibit similar runtime behavior?
•RQ4: CouldFaCoY recommend correctcode as alternative
of buggy code?
Toanswertheseresearchquestions,webuildaprototypeversion
of FaCoY where search indices are interchangeable to serve thepurpose of each assessment scenario. We provide in Section 4.1
somedetailsontheimplementationbeforedescribingthedesign
and results for the different evaluations.
4.1 Implementation details
Accuracy and speed performance of a search engine are gener-
ally impacted by the quantity of data and the quality of the in-dices [
70]. We collect a comprehensive dataset from GitHub,a
popularandverylargeopensourceprojectrepository,aswellfrom
StackOverflow , a popular Q&A site with a large community to
curate and propose accurate information on code examples. We
furtherleveragetheApacheLucenelibrary,whosealgorithmshavebeen tested and validated by researchers and practitioners alike for
indexing and searching tasks.
For building the Code Index representing the search space of
the code base where to code fragments, we consider the GitHub
repository.WefocusonJavaprojectssinceJavaremainspopularinthedevelopmentcommunityandisassociatedwithalargenumber
of projects in GitHub[15]. Overall, we have enumerated 2,993,513
projects where Java is set as the main language. Since there aremany toy projects on
GitHub[38], we focused on projects that
have been forked at least once by other developers and droppedout projects where the source code include non-ascii characters.
Table 2 summarizes the collected data.
Table 2: Statistics on the collected GitHub data.
Feature Value Feature Value
Number of projects 10,449 Number of duplicate files 382,512
Number of files 2,163,944 LOCs >384M
ForbuildingtheSnippetandQuestionindices,wedownloaded
adumpfile fromthe StackOverflow websitecontainingall posts
between July 2008 and September 2016 in XML format. In total, we
have collected and indexed 1,856,592 posts tagged as about Java or
Android coding. We have used a standard XML parser to extract
natural language elements (tagged with <p>. . . </p> markers)
and code snippets (tagged with <code>. . . </code> ). It should
be noted that we first filter in code snippets from answers that
havebeenacceptedbythequestioner.Thenweonlyretainedthose
accepted answers that have been up-voted at least once. These
precautionsaimatensuringthatweleveragecodesnippetsthatare
ofhighqualityandarereallymatchingthequestions.Asaresult,
we eventually used 268,264 Q&A posts to build the snippet and
question indices. By default, we set all three stretch parametersto
ns=nq=nc=3. The stretch for delimitating output code
fragments is also set to k=3.
951
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:38:44 UTC from IEEE Xplore.  Restrictions apply. FaCoY – A Code-to-Code Search Engine ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
4.2 RQ1: Comparison with code search engines
Design:In this experiment, we compare the search results of
FaCoYwiththosefromonlinecodesearchengines.Wefocuson
Krugle [1] and searchcode [ 2] since these engines support code-to-
code search. As input code fragments, we consider code examples
implementing popular functionalities that developers ask about.
To that end, we select snippets from posts in StackOverflow . The
snippetsareselectedfollowingtworequirements:(1)theassociated
postisrelatedto“Java”and(2)theanswerincludecodesnippets.Weselectcodesnippetsinthetop10postswiththehighestviewcounts
(fortheirquestions).Table3liststhetitlesof
StackOverflow posts
whosecodesnippetsareusedinourexperiment.Notethat,forafair
comparison and to avoid any bias towards FaCoY, the actual posts
(includingthecodesnippetsintheiranswers)showninthetable
areremovedfromthesnippetandquestionindices;thisprevents
our tool from leveraging answer data in advance, which would be
unfair.
Table3:Top10 StackOverflow Javapostswithcodesnippets.
Query # Question title
Q1 How to add an image to a JPanel?
Q2 How to generate a random alpha-numeric string?
Q3 How to save the activity state in Android?
Q4 How do I invoke a Java method when given the method name as a string?
Q5 Remove HTML tags from a String
Q6 How to get the path of a running JAR file?
Q7 Getting a File’s MD5 Checksum in Java
Q8 Loading a properties file from Java package
Q9 How can I play sound in Java?
Q10 What is the best way to SFTP a file from a server?
Figure 7 shows an example of input code fragments collected
fromStackOverflow thatisusedinourevaluation.10codesnip-
pets7are then used to query FaCoY, Krugle, and searchcode.
import java.security.SecureRandom;
import java.math.BigInteger;public final class SessionIdentifierGenerator {
private SecureRandom random = new SecureRandom();public String nextSessionId() {
return new BigInteger(130, random).toString(32);
}
}
Figure 7: Code snippet associated to Q2 in Table 3.
On each search engine, we consider at most the top 20 search
resultsforeachqueryandmanuallyclassifythemintooneofthe
four clone types defined in Section 2.
Table4:Statisticsbasedonmanualchecksofsearchresults.
FaCoY Searchcode Krugle
Query# outputs Type-2Type-3Type-4# outputs Type-1Type-3# outputs Type-1
Q1 18 5(27.7%)4(22.2%) 0 0
Q2 21 6(28.5%) 2(9.5%) 0 0
Q3 18 9(50%) 0 0
Q4 0 2020(100%) 11(100%)
Q5 191(5.2%)2(10.5%)6(31.5%) 32(66.6%)1(33.3%) 0
Q6 91(11.1%) 3(30%)1(11.1%) 2020(100%) 0
Q7 17 0 0
Q8 17 7(41.1%)7(41.1%) 0 0
Q9 0 2 2(100%) 0
Q10 9 1(11.1%)7(77.7%) 0 0
Result:Table 4 details statistics on the search results for the
differentsearchengines.FaCoY, Krugle,andsearchcode produce
searchresultsforeight,fourandonequeriesrespectively.Search
results can also be false positives. We evaluate the efficiency of
FaCoY using the Precision @kmetric defined as follows:
7Code snippets available on project web page [23].Precision @k=1
|Q||Q|/summationdisplay
i=1|relevant i,k|
k(1)
whererelevant i,krepresentstherelevantsearchresultsforquery
iin the top kreturned results, and Qis a set of queries.
FaCoY achieves 57.69% and 48.82% scores for Precision @10 and
Precision @20 respectively.
We further categorize the true positive code fragments based on
theclonetype. KrugleappearstobeabletoidentifyonlyType-1
clones.searchcode ontheotherhandalsoyieldssomeType-3code
clonesfor2queries.Finally,FaCoYmostlysuccessfullyretrieves
Type-3 and Type-4 clones.
Unlikeonlinecode-to-codesearchengines,FaCoYcanidentify(1)similar
codefragments fora more diverseset offunctionalityimplementations.
Those code fragments can be syntactically dissimilar to the query while
implementing similar functionalities.
4.3 RQ2: Finding similar code in IJaDataset
Design: This experiment aims at evaluating FaCoY against an
existingbenchmark.Sinceourcode-to-codesearchengineissimilar
toacodeclone detectorinmanyrespects,we focus onassessing
which clones FaCoY can effectively identify in a code clone bench-
mark. A clone benchmark contains pairs of code fragments which
are similar to each other.
We leverage BigCloneBench [83], one of the biggest (8 mil-
lion clone pairs) code clone benchmarks publicly available. This
benchmark is built by labeling of pairs of code fragments from the
IJaDataset-2.0[ 3].IJaDatasetincludesapproximately25,000open-
source Java projects consisting of 3 million source code files and
250 millions of lines of code (MLOC). BigCloneBench maintainers
have mined this dataset focusing on a specific set of functionalities.
Theythenrecordmetadatainformationabouttheidentifiedcode
clone pairs for the different functionalities. In this paper, we use a
recent snapshot of BigCloneBench including clone pairs clustered
in 43 functionality groups made available for the evaluation of
SourcererCC [74].
Weconsider8,345,104clonepairsin BigCloneBench basedon
the same criteriaused in [ 74]: bothcode fragments in aclone pair
haveat least6linesand 50tokensinlength,a standardminimum
clone size for benchmarking [13, 84].
Clone pairs are further assigned a type based on the criteria
in[74]:Type-1andType-2clonepairsareclassifiedaccordingto
the classical definitions recalled in Section 2. Type-3 and Type-4
clonesarefurtherdividedintofoursub-categoriesbasedontheir
syntacticalsimilarity:VeryStronglyType3(VST3),StronglyType3
(ST3),ModeratelyType3(MT3),andWeaklyType3/Type4(WT3/4).
Each clone pair (unless it is Type 1 or 2) is identified as one of four
if its similarity scorefalls into a specific range; VST3:[90% ,100% ),
ST3: [70% ,90% ), MT3: [50% ,70% ), and WT3/4: [0% ,50% ).
For this experiment, we adapt the implementation described
in Section 4.1. Since the experiment conducted in [ 74] detected
clones only from IJaDataset, the GitHub-based code index in our
tool is replaced by a custom index generated from IJaDataset for
a fair comparison.This makes FaCoY search only codefragments
in IJaDataset. In addition, the stretch parameters (see Algorithm 1)
952
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:38:44 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden K. Kim et al.
are set to ns=nq=3,nc=100, making FaCoY yield as many
snippets, posts and fragments as possible in each step.
WefeedFaCoYwitheachcodefragmentreferencedinthebench-
mark in order to search for their clones in the IJaDataset. We com-
pare each pair, formed by an input fragment and a search result,
against the clone pairs of BigCloneBench. We then compute the
recall of FaCoY following the definition proposed in the bench-
mark [83]:
Recall =D∩Btc
Btc(2)
whereBtcisthesetofalltrueclonepairsin BigCloneBench,and
Dis the set of clone pairs found by FaCoY.
To quantify the improvement brought by the two main strate-
gies proposed in this work, namely query alternation andquery
structuring, we define four different search engine configurations:
•Baseline SE: The baseline search engine does not implement
any query structuring or query alternation. Input code query, as
well as the search corpus, are treated as natural language text
documents.Searchisthendirectlyperformedbymatchingtokens
with no type information.
•FaCoYnoQA: In this version, only query structuring is applied.
No query alternation is performed, and thus only input code
query is used to match the search corpus.
•FaCoYnoUQ: In this version, query alternation is performed
along with query structuring, but initial input query is left out.
•FaCoY: This version represents the full-feature version of the
code-to-codesearchengine:queriesarealternatedandstructured,
and initial input code query also contributes in the matching
process.
Result:Table 5 details the recall8scores for the baseline SE,
FaCOYnoQA, FaCOY noUQand FaCoY. Recall scores are summa-
rizedperclonetypewiththecategoriesintroducedabove.Sincewe
are reproducing for FaCoY the experiments performed in [ 74], we
directlyreportinthistableallresultsthattheauthorshaveobtained
on the benchmark for state-of-the-art Nicad [ 18], iClones [ 28],
SourcererCC [74], CCFinderX [39], Deckard [34] clone detectors.
Table 5: Recall scores on BigCloneBench [83].
Clone Types
T1 T2 VST3 ST3 MT3 WT3/T4
(# of Clone Pairs) (39,304) (4,829) (6,171) (18,582) (90,591) (6,045,600)
FaCoY 65 90 67 69 4110(635,844)⋆
FaCoY noUQ 35 74 45 55 37 10
FaCoY noQA 66 26 56 54 20 2
Baseline SE 66 26 54 50 20 2
SourcererCC 100 98 93 61 5 0
CCFinderX†100 93 62 15 1 0
Deckard†60 58 62 31 12 1
iClones†100 82 82 24 0 0
NiCad†100 100 100 95 1 0
⋆Cumulative number of WT3/4 clones that FaCoY found.
†The tools could not scale to the entire files of IJaDataset [3].
WerecallthatFaCoYisacode-to-codesearchengineandthus
theobjectiveistofindsemanticclones(i.e.,towardsType-4clones).
Nevertheless,foracomprehensiveevaluationoftheaddedvalue
of the strategies implemented inthe FaCoY approach, we provide
comparison results of recall values across all clone types.
8It should be noted that Recall is actually Recall@MAX.Overall,FaCoYproducesthehighestrecallvaluesformoderately
Type-3 as well as Weakly Type-3 and Type-4 clones. The recall
performance of FaCoY for MT3 clones is an order of magnitude
higher than that of 4 out the 5 detection tools. While most tools
detectlittletonoWT3/T4codeclonepairs,FaCoYisabletofind
over 635,000 clones in the IJADataset. Furthermore, apart from
SourcererCC,the othertools couldnot covertheentire IJaDataset
as reported in [74].Benefitof querystructuring.
Thedifferenceofperformancebe-
tween Baseline SE and FaCOY noQAindicates that query struc-
turing helps to match more code fragments which are not strictly,
syntactically, identical to the query (cf. VST3 & ST3).Benefit of query alternation.
Thedifferenceofperformancebe-
tween FaCoY and FaCOY noQAdefinitively confirms that query
alternationisthestrategythatallowscollectingsemanticclones:recall for WT3/T4 goes from 2% to 10% and recall for MT3 goes
from 20 to 41.Benefit of keeping input query.
The difference of performance
betweenFaCoYandFaCOY noUQfinallyindicatesthatinitialinput
code query is essential for retrieving some code fragments that are
more syntactically similar, in addition to semantically similar code
fragments matched by alternate queries.
With 10% recall for semantic clones (WT3/T4), FaCoY achieves
the best performance score in the literature. Although this scoremay appear to be small, it should be noted that this corresponds
to the identification of 635,844 clones, a larger set than the accu-
mulatedsetofallclonesofothertypesinthebenchmark.Finally,
it should also be noted that, while the dataset includes over 7.7
million WT3/T4 code clones, state-of-the-art detection tools can
detect only 1% or less of these clones.
WefurtherinvestigatetherecallofFaCoYwithregardstothe
functionalities implemented by clone pairs. In BigCloneBench,e v -
ery clone pair is classified into one of 43 functionalities, including
“Download From Web” and “Decompress zip archive”. For each
clonetype,wecountthenumberofclonesthatFaCoYcanfind,per
functionality. Functionalities with higher recall tend to have imple-
mentations based on APIs and libraries while those with low recall
aremorecomputation intensivewithoutAPIs.Thisconfirmsthat
FaCoYperformsbetterforprogramsimplementedbydescriptive
APInamessinceitleverageskeywordsinsnippetsandquestions.
This issue is discussed in Section 5 in detail. Because of space con-
straints, we refer the reader to the FaCoY project page for more
statistics and information details on its performance.Double-checking
FaCoY’s false positives: Although it is one of
thelargestbenchmarksavailabletotheresearchcommunity, Big-
CloneBench clone information may not be complete. Indeed, as de-
scribed in [ 83],BigCloneBench is built via an incremental additive
process (i.e., gradually relaxing search queries) based on keyword
andsourcepatternmatching.Thus,itmaymisssomeclonesdespitethe manual verification. In any case, computing precision of a code
search engine remains an open problem [ 74]. Instead, we chose to
focus on manually analysing sampled false positives.
We manually verify the clone pairs that are not associated in
BigCloneBench, but FaCoY recommended as code clones, i.e., false
positives. Our objective is then to verify to what extent they are
indeedfalsepositivesandnotmissesby BigCloneBench.Wesample
10falsepositivesperclonetypecategoryforamanualcheck.For32
953
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:38:44 UTC from IEEE Xplore.  Restrictions apply. FaCoY – A Code-to-Code Search Engine ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
out of 60 cases, it turns out that BigCloneBench actually missed to
include them. Specifically, it missed 25 Type-4, 2 Type-3, 1 Type-2,
andeven4Type-1clones.Amongthe28cases,for26cases,FaCoY
points to the correct file but another location than actual clones.
In only two cases FaCoY completely fails. We provide this data
inthe projectweb page[ 23]as afirst steptowardsinitiating acu-
rated benchmark of semantic code clones, which can be eventually
integrated into BigCloneBench.
FaCoYcanfindmoreType-3,WeaklyType-3andType-4clonesthanthe
state-of-the-art, thus fulfilling the objective for which it was designed.
4.4 RQ3: Validating semantic similarity
Design: Since FaCoY focuses on identifying semantically simi-
larcodesnippetsratherthansyntactic/structuralclones,itisneces-
sary to verify whether the search results of the approach indeed
exhibitsimilarfunctionalbehavior(beyondkeywordmatchingwith
high syntactic differences implied in BigCloneBench). The datasets
used in Sections 4.2 and 4.3 are however not appropriate for dy-
namicanalysis:thecodemustcompileaswellasexecute,andthere
must be test cases for exercising the programs.
To overcome these challenges, we build on DyCLINK [ 81], a
dynamic approach that computes the similarity of execution traces
todetectthattwocodefragmentsarerelatives(i.e.,thattheybehave
(functionally) similarly). The tool has been applied to programs
writtenfor GoogleCodeJam [26]toidentifycoderelativesatthe
granularity of methods. We carefully reproduced their results with
thepubliclyavailableversionofDyCLINK.Amongthe642methods
in the code base, DyCLINK matches 411 pairs as code relatives9.
WeconsiderallmethodsforwhichDyCLINKfindsarelativeand
useFaCoYtosearchforitsclonesin codejam,andwecheckthat
the found clones are relatives of the input.
Since FaCoY provides a ranked list of code examples for a given
query, we measure the hit10ratio of the top Nsearch results. Here,
weusethedefaultstretchparametersspecifiedinSection4.1and
thusN=27.In additionto hitratio,we computethe MeanRecip-
rocal Rank (MRR) of the hit cases. To calculate MRR for each clone
pair, we use the following formula:
MRR =1
|Q||Q|/summationdisplay
i=11
rank i(3)
whererankiistherankpositionofthecorrespondingcodefragment
for the given peer in a clone pair. Qis the number of all queries.
Result:Asaresult,FaCoYcanidentify278outof411coderela-
tivesandthehitratiois68%.Asforefficiency,FaCoYachieves45%
and 88% scores respectively for Precision @10 and Precision @20,
andexhibitsanMRRof0.18,whichmeansFaCoYrecommendsthe
code relatives into lower rankings.
On the one hand, since many programs in Google Code Jam
oftenusevariableswithnomeaning(suchas void s(int a){} ),
FaCoYcannotfindrelatedcodein StackOverflow andthuscannot
build alternate queries, limiting the hit ratio. On the other hand,
9Wedid our bestto reproducethe resultsof DyCLINK. We checkedwith the authors
that the found 411 relatives are consistent with the released tool version.
10A “hit” indicates that the corresponding code fragment is in the top Nresults for a
given query.since DyCLINK also uses a similarity metric to decide on code
relativeness, the MRR score of FaCoY could be higher with a more
relaxed threshold (currently set at 82%) in DyCLINK.
FaCoYcanindeedfindalternativefragmentsthatexhibitsimilarruntime
behavior with input code fragment.
4.5 RQ4: Recommending patches with FaCoY
Design:ThisexperimentattemptstouseFaCoYtosearchfor
correct code that can help fix buggy code. Code search has indeed
been proposed recently as a potential step for patch recommenda-
tion [25], and even automated repair [ 40]. Since FaCoY can find
codesnippetsthataresemanticallysimilartoagivenquerycode,
we conjecture that it can be used for helping find alternative im-
plementations which may turn out to be more correct than the
user’scode.Weassesssuchapotentialapplicationbyleveraging
the Defects4J benchmark [37].
Defects4J include 395 real bugs: for each bug, the buggy code
andtheassociatedfixedversionaremadeavailable,alongwithtest
suitesforexecution.Foreachbug,wetakebuggycodefragment
(generally a function) and query FaCoY. By going through the
search results from the top, we manually compare each result with
the actual paired fixed code. Our objective is to check whether
FaCoY’s output code fragments can help build a patch that would
havebeencorrectw.r.t.tothebenchmarkfix.Weperformthesame
experiments using Krugle[1] andsearchcode [2].
public static boolean equals(CharSequence cs1, CharSequence cs2)
{ return cs1 == null ? cs2 == null : cs1.equals(cs2); }
(a) Defects4J buggy code fragment from Commons-LANG†.
public static boolean equals(CharSequence a, CharSequence b) {
if (a == b) return true;
int length;
if (a != null && b != null && (length = a.length()) == b.length()) {
if (a instanceof String && b instanceof String) {
return a.equals(b);
}
f o r( i n ti=0 ;i<length; i++) {
if (a.charAt(i) != b.charAt(i)) return false;
}
return true;
}
return false;
}
(b) Code fragment found in GitHub by FaCoYas similar to fragment in (a)∗.
public static boolean equals(CharSequence cs1, CharSequence cs2) {
- return cs1 == null ? cs2 == null : cs1.equals(cs2);
+ if (cs1 == cs2) {
+ return true;+}+ if (cs1 == null || cs2 == null) {+ return false;+}+ if (cs1 instanceof String && cs2 instanceof String) {
+ return cs1.equals(cs2);
+}+ return CharSequenceUtils.regionMatches(...);
}
(c) Actual patch‡that was proposed to fix the buggy code in (a).
†https://goo.gl/5kn6Zr∗https://goo.gl/URdriN‡https://goo.gl/PD6KL5
Figure 8: Successful patch recommendation by FaCoY.
For each bug, one of the authors of this paper examined at most
top 15 search results from each search engine. When the author
marks a result as a good candidate for patch recommendation,
two other authors double check, and the final decision is made
954
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:38:44 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden K. Kim et al.
by majority voting. Note that, since Defects4J projects are also
available in GitHub, the fixed code may be in FaCoY corpus. Thus,
wehavefilteredoutfromthesearchresultsanycodefragmentthat
is collected from the same project file as the buggy code used as
query. Figure 8a shows an example buggy function that we used as
querytoFaCoY.Fig.8bshowsoneofthesimilarcodefragments
returnedbyFaCoYandwhichwefoundthatitwasagoodcandidate
for recommendingthe patchthat was actuallyapplied (cf. Fig.8c).
Result:Out of 395 bugs in Defects4J, our preliminary results
showthatFaCoYfoundsimilarfixedcodeexamplesfor21bugs.In
contrast,searchcode located a single code example, while Krugle
provided no relevant results at all. Specifically, project-specific
resultsareasfollows.Lang:6/65,Mockito:3/38,Chart:3/26,Closure:
2/133, Time: 2/27, and Math: 5/106. searchcode was successful only
for 1/38 Mokito bug. All details are available in [23].
FaCoY-basedsearchofsemanticallysimilarcodefragmentscansupport
patch/coderecommendation,softwarediversificationortransplantation.
5 DISCUSSIONS
Exhaustivity of Q&A data: The main limitation of FaCoY comes
from the use of code snippets and natural language descriptions in
Q&Apoststoenablethegenerationofalternatequeriestowards
identifying semantically similar code fragments. This data may
simply be insufficient with regards to a given user input fragment
(e.g., uncommon functionality implementation).
Threats to Validity: As threat to External validity, we note
that we only used Java subjects for the search. However, the same
processcanbedevelopedwithotherprogramminglanguagesby
changingthelanguageparser,theindicesforrelatedQ&Apostsand
projectcode.Anotherthreatstemsfromtheuseof StackOverflow
andGitHubwhichmaybelimited.Wenotehoweverthattheirdata
can be substituted or augmented with data from other repositories.
Internalvalidity: We usesubjectsfromBigCloneBench andDy-
CLINKdatasetstovalidateourwork.Thosesubjectsmaybebiasedforclonedetection.Nevertheless,thesesubjectsarecommonlyused
and allow for a fair comparison as well as for reproducibility.
6 RELATED WORK
Code search engines. Code search literature is abundant [ 5,22,
33,42,59,66,69,72,77,79]. CodeHow [ 59] finds code snippets
relevanttoauserquerywritteninnaturallanguage.ItexploresAPIdocumentstoidentifyrelationshipsbetweenquerytermsandAPIs.Sourcerer[
5]leveragesstructuralcodeinformationfromacomplete
compilationunittoperformfine-grainedcodesearch.Portfolio[ 66]
is a code search and visualization approach where a chain of func-
tion calls are highlighted as usage scenario. CodeGenie [ 53,54]
expands queries for interface-driven code search (IDCS). It takestest cases rather than free-form queries as inputs and leverages
WordNetand a code-related thesaurus forquery expansion. Sirres
et al. [79], also use StackOverflow data to implement a free-form
code search engine.Clone detection and search.
Clone detection has various applica-
tions [16,52,78] such as plagiarism detection. However, most tech-
niquesdetectsyntacticallysimilarcodefragmentsinsourcecodeusingtokens[ 8,39,56],ASTtrees[ 12,34],or(programdependency)
graphs[49,57].Onlyafewtechniquestargetsemanticallysimilar
sourcecodeclones[ 35,45,47].KomondoorandHorwitzsearchfor
isomorphicsub-graphsofprogramdependencegraphsusingpro-
gram slicing [ 47]. Jiang and Su compare program execution traces
using automated random testing to find functionally equivalentcode fragments [
35]. MeCC detects semantically-similar C func-
tionsbasedonthesimilarityoftheirabstractmemorystates[ 45].
Whiteetal.[ 86]proposetousedeeplearningtofindcodeclones.
TheirapproachismoreeffectiveforType-1/2/3clonesthanType-4.
Coderecommendation systems[31,32,65,71]supportdevelopers
withreusablecodefragmentsfromotherprograms,orwithpointers
toblogsandQ&Asites.Strathcona[ 31]generatesqueriesfromuser
codeandmatchesthemagainstrepositoryexamples,Prompter[ 71]
directlymatchesthecurrentcodecontextwithrelevantQ&Aposts.
Although several studies have explored StackOverflow posts [10,
25,60,68,79,85], none, to the best of our knowledge, leveraged
StackOverflow data to improve clone detection.
Programrepair [14,27,44]canalsobenefitfromcodesearch.Gao
et al. [25] proposed an approach to fix recurring crash bugs by
searchingforsimilarcodesnippetsin StackOverflow .SearchRe-
pair [40] infers potential patch ingredients by looking up code
fragmentsencodedasSMTconstraints.Koyuncuetal.[ 48]showed
that patching tools yield recurrent fix actions that can be explored
tofixsimilarcode.Liuetal.[ 58]explorethepotentialoffixpatterns
for similar code fragments that may be buggy w.r.t. FindBugs rules.
API recommendation is a natural application of code search. The
Baker approach connects existing source code snippets to API doc-
umentation [ 82]. MUSE [ 67] builds an index of real source code
fragments by using static slicing and code clone detection, andthen recommends API usage examples. Keivanloo et al. [
43]p r e -
sentedanInternet-scalecodesearchenginethatlocatesworking
code examples. Buse and Weimer [ 17] proposed an approach to
recommend API usage examples by synthesizing code snippets
basedondataflowanalysisandpatternabstraction.Bajracharya[ 7]
proposed Structural Semantic Indexing which examines the API
calls extracted in source code to determine code similarity.
7 CONCLUSION
WehavepresentedFaCoY,acode-to-codesearchenginethatac-
cepts code fragments from users and recommends semantically
similar code fragments found in a target code base. FaCoY is based
onquery alternation : after generating a structured code query
summarizingstructuralcodeelementsintheinputfragment,we
searchinQ&Apostsothercodesnippetshavingsimilardescriptions
but which may present implementation variabilities. These variant
implementations arethen used togenerate alternate codequeries.
WehaveimplementedaprototypeofFaCoYusing StackOverflow
andGitHubdata on Java. FaCoY achieves better accuracy than
onlinecode-to-codesearchenginesandfindsmoresemanticcode
clones in BigCloneBench than state-of-the-art clone detectors. Dy-
namic analysis shows that FaCoY’s similar code fragments are
indeedrelated execution-wise.Finally,wehaveinvestigateda po-
tential application of FaCoY for code/patch recommendation on
buggy code in the Defects4J benchmark.
955
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:38:44 UTC from IEEE Xplore.  Restrictions apply. FaCoY – A Code-to-Code Search Engine ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
ACKNOWLEDGEMENTS
WeextendourthankstoSeungdeokHan,MinsukKim,JaekwonLee,
andWoosungJungfromChungbukNationalUniversityfortheir
insightful comments on earlier versions of this manuscript. This
work was supported by the Fonds National de la Recherche (FNR),
Luxembourg, under projects RECOMMEND C15/IS/10449467, FIX-
PATTERN C15/IS/9964569, and FNR-AFR PhD/11623818, and by
the Japan Society for the Promotion of Science (JSPS) KAKENHIGrant Number JP15H06344. It is also supported by the NationalResearch Foundation of Korea (NRF) grant funded by the Koreagovernment (Ministry of Science, ICT & Future Planning) (No.
2015R1C1A1A01054994).
REFERENCES
[1] 2017. http://krugle.com/. (July. 2017).
[2] 2017. https://searchcode.com/. (July. 2017).[3] Ambient Software Evoluton Group. 2017. IJaDataset 2.0, http://secold.org/
projects/seclone. (July. 2017).
[4]
LeAn,OnsMlouki,FoutseKhomh,andGiulianoAntoniol.2017. StackOverflow:
A Code Laundering Platform?. In Proceedings of the 24th International Conference
on Software Analysis, Evolution and Reengineering (SANER). IEEE, 283–293.
[5]SushilBajracharya,TrungNgo,ErikLinstead,YimengDou,PaulRigor,Pierre
Baldi, and Cristina Lopes. 2006. Sourcerer: A Search Engine for Open Source
CodeSupportingStructure-basedsearch.In Companiontothe21stACMSIGPLAN
SymposiumonObject-orientedProgrammingSystems,Languages,andApplications
(OOPSLA). ACM, 681–682.
[6]Sushil Krishna Bajracharya and Cristina Videira Lopes. 2012. Analyzing and
Mining a Code Search Engine Usage Log. Empirical Software Engineering (EMSE)
17, 4-5 (Aug. 2012), 424–466.
[7]SushilK.Bajracharya,JoelOssher,andCristinaV.Lopes.2010. LeveragingUsageSimilarityforEffectiveRetrievalofExamplesinCodeRepositories.In Proceedings
of the 18th ACM SIGSOFT International Symposium on Foundations of Software
Engineering (FSE). ACM, 157–166.
[8]BrendaS.Baker.1992. AProgramforIdentifyingDuplicatedCode. Computing
Science and Statistics (1992).
[9]EarlTBarr,MarkHarman,YueJia,AlexandruMarginean,andJustynaPetke.2015.
Automated Software Transplantation. In Proceedings of the 24th International
Symposium on Software Testing and Analysis (ISSTA). ACM, 257–269.
[10]Ohad Barzilay, Christoph Treude, and Alexey Zagalsky. 2013. Facilitating Crowd
Sourced Software Engineering via Stack Overflow. In Finding Source Code on the
Web for Remix and Reuse. Springer, 289–308.
[11]Benoit Baudry, Simon Allier, and Martin Monperrus. 2014. Tailored Source Code
TransformationstoSynthesizeComputationallyDiverseProgramVariants.In
Proceedings of the 23th International Symposium on Software Testing and Analysis
(ISSTA). ACM, 149–159.
[12]Ira D Baxter, Andrew Yahin, Leonardo Moura, Marcelo Sant’Anna, and Lorraine
Bier.1998. CloneDetectionUsingAbstractSyntaxTrees.In Proceedingsofthe
International Conference on Software Maintenance (ICSM). IEEE, 368–377.
[13]StefanBellon,RainerKoschke,GiulioAntoniol,JensKrinke,andEttoreMerlo.
2007. ComparisonandEvaluationofCloneDetectionTools. IEEETransactions
on Software Engineering (TSE) 33, 9 (2007), 577–591.
[14]Tegawendé F Bissyandé. 2015. Harvesting Fix Hints in the History of Bugs.
arXiv:1507.05742 [cs] (July 2015). arXiv: 1507.05742.
[15]Tegawendé F. Bissyandé, Ferdian Thung, David Lo, Lingxiao Jiang, and Laurent
Réveillere.2013. Popularity,Interoperability,andImpactofProgrammingLan-
guagesin100,000OpenSourceProjects.In Proceedingsofthe37thIEEEComputer
Software and Applications Conference (COMPSAC). IEEE, 303–312.
[16]R. Brixtel, M. Fontaine, B. Lesner, C. Bazin, and R. Robbes. 2010. Language-
Independent Clone Detection Applied to Plagiarism Detection. In 2010 10th IEEE
Working Conference on Source Code Analysis and Manipulation (SCAM). 77–86.
[17]RaymondP.L.BuseandWestleyWeimer.2012. SynthesizingAPIUsageExamples.InProceedingsof the34thInternationalConferenceonSoftwareEngineering(ICSE).
IEEE Press, 782–792.
[18]J. R. Cordy and C. K. Roy. 2011. The NiCad Clone Detector. In Proceedings of the
19th International Conference on Program Comprehension (ICPC) . IEEE, 219–220.
[19]Creative Commons Attribution-ShareAlike 3.0 Unported License. 2016. https:
//creativecommons.org/licenses/by-sa/3.0/legalcode. (2016). last accessed
25.02.2017.
[20]Barthélémy Dagenais and Martin P Robillard. 2012. Recovering traceability linksbetweenanAPIanditslearningresources.In Proceedingsofthe34thInternational
Conference on Software Engineering (ICSE). IEEE, 47–57.[21]ManuelEgele,MaverickWoo,PeterChapman,andDavidBrumley.2014. Blanket
Execution:DynamicSimilarityTestingforProgramBinariesandComponents.
InProceedings of the 23rd USENIX Security Symposium. 303–317.
[22]T.Eisenbarth,R.Koschke,andD.Simon.2003. LocatingFeaturesinSourceCode.
IEEE Transactions on Software Engineering (TSE) 29, 3 (March 2003), 210–224.
[23] FaCoY. 2017. https://github.com/facoy/facoy. (2017).
[24]G.W.Furnas,T.K.Landauer,L.M.Gomez,andS.T.Dumais.1987.TheVocabulary
ProbleminHuman-systemCommunication. Commun.ACM 30,11(Nov.1987),
964–971.
[25]Q.Gao,H.Zhang,J.Wang,Y.Xiong,L.Zhang,andH.Mei.2015. FixingRecurring
CrashBugsviaAnalyzingQ&ASites(T).In 201530thIEEE/ACMInternational
Conference on Automated Software Engineering (ASE). 307–318.
[26] Google Code Jam. 2017. https://code.google.com/codejam/. (Jan. 2017).
[27]C.LeGoues,T.Nguyen,S.Forrest,andW.Weimer.2012. GenProg:AGeneric
MethodforAutomaticSoftwareRepair. IEEETransactionsonSoftwareEngineering
(TSE)38, 1 (Jan. 2012), 54–72.
[28]N. GÃűde and R. Koschke. 2009. Incremental Clone Detection. In Proceedings of
the13thEuropeanConferenceonSoftwareMaintenanceandReengineering(CSMR).
219–228.
[29]SoniaHaiduc,GabrieleBavota,AndrianMarcus,RoccoOliveto,AndreaDeLucia,
and Tim Menzies. 2013. Automatic Query Reformulations for Text Retrieval
inSoftwareEngineering.In Proceedingsofthe35thInternationalConferenceon
Software Engineering (ICSE). IEEE Press, 842–851.
[30]RaphaelHoffmann,JamesFogarty,andDanielSWeld.2007. Assieme:Findingand
LeveragingImplicitReferencesinaWebSearchInterfaceforProgrammers.In
Proceedingsofthe20thACMSymposiumonUserInterfaceSoftwareandTechnology
(UIST). ACM, 13–22.
[31]ReidHolmesandGailC.Murphy.2005. UsingStructuralContexttoRecommend
Source Code Examples. In Proceedings of the 27th International Conference on
Software Engineering (ICSE). ACM, 117–125.
[32]R.Holmes,R.J.Walker,andG.C.Murphy.2006. ApproximateStructuralContext
Matching:AnApproachtoRecommendRelevantExamples. IEEETransactions
on Software Engineering (TSE) 32, 12 (Dec. 2006), 952–970.
[33]Katsuro Inoue, Yusuke Sasaki, Pei Xia, and Yuki Manabe. 2012. Where Does
ThisCodeComefromandWhereDoesItGo?-IntegratedCodeHistoryTracker
for OpenSource Systems. In Proceedings ofthe 34thInternational Conference on
Software Engineering (ICSE). IEEE Press, 331–341.
[34]LingxiaoJiang,GhassanMisherghi,ZhendongSu,andStephaneGlondu.2007.
Deckard: Scalable and Accurate Tree-based Detection of Code Clones. In Pro-
ceedings of the 29th International Conference on Software Engineering (ICSE). IEEE
Computer Society, 96–105.
[35]LingxiaoJiangandZhendongSu.2009. AutomaticMiningofFunctionallyEquiv-
alentCodeFragmentsviaRandomTesting.In Proceedingsofthe18thInternational
Symposium on Software Testing and Analysis (ISSTA). ACM, 81–92.
[36]Elmar Juergens, Florian Deissenboeck, and Benjamin Hummel. 2010. Code
SimilaritiesBeyondCopy&Paste.In Proceedingsofthe14thEuropeanConference
on Software Maintenance and Reengineering (CSMR). IEEE, 78–87.
[37]René Just, Darioush Jalali, and Michael D. Ernst. 2014. Defects4J: A Databaseof Existing Faults to Enable Controlled Testing Studies for Java Programs. In
Proceedings of the 2014 International Symposium on Software Testing and Analysis
(ISSTA). ACM, 437–440.
[38]Eirini Kalliamvakou, Georgios Gousios, Kelly Blincoe, Leif Singer, Daniel M
German, and Daniela Damian. 2014. The Promises and Perils of Mining GitHub.
InProceedings of the 11th Working Conference on Mining Software Repositories
(MSR). ACM, 92–101.
[39]Toshihiro Kamiya, Shinji Kusumoto, and Katsuro Inoue. 2002. CCFinder: A
MultilinguisticToken-basedCodeCloneDetectionSystemforLargeScaleSource
Code.IEEE Transactions on Software Engineering (TSE) 28, 7 (2002), 654–670.
[40]Y. Ke, K. T. Stolee, C. L. Goues, and Y. Brun. 2015. Repairing Programs with
Semantic Code Search (T). In 2015 30th IEEE/ACM International Conference on
Automated Software Engineering (ASE). 295–306.
[41]Yalin Ke, Kathryn T Stolee, Claire Le Goues, and Yuriy Brun. 2015. Repairing
ProgramswithSemanticCodeSearch(T).In Proceedingsofthe30thIEEE/ACM
InternationalConferenceonAutomatedSoftwareEngineering(ASE).IEEE,295–306.
[42]I. Keivanloo,J. Rilling,and P.Charland. 2011. SeClone- AHybrid Approachto
Internet-Scale Real-Time Code CloneSearch.In Proceedings of the 19thInterna-
tional Conference on Program Comprehension (ICPC). 223–224.
[43]ImanKeivanloo,JuergenRilling,andYingZou.2014. SpottingWorkingCodeEx-
amples.In Proceedingsofthe36thInternationalConferenceonSoftwareEngineering
(ICSE). ACM, 664–675.
[44]D. Kim, J. Nam, J. Song, and S. Kim. 2013. Automatic Patch Generation Learned
fromHuman-writtenPatches.In 201335thInternationalConferenceonSoftware
Engineering (ICSE). 802–811. DOI:http://dx.doi.org/10.1109/ICSE.2013.6606626
[45]H. Kim, Y. Jung, S. Kim, and K. Yi. 2011. MeCC: Memory Comparison-based
CloneDetector.In Proceedingsofthe33rdInternationalConferenceonSoftware
Engineering (ICSE). IEEE, 301–310.
956
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:38:44 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden K. Kim et al.
[46]A. J. Ko, B. A. Myers, M. J. Coblenz, and H. H. Aung. 2006. An Exploratory
Study of How Developers Seek, Relate, and Collect Relevant Information during
SoftwareMaintenanceTasks. IEEETransactionsonSoftwareEngineering(TSE)
32, 12 (Dec. 2006), 971–987.
[47]RaghavanKomondoorandSusanHorwitz.2001. UsingSlicingtoIdentifyDu-
plicationinSourceCode.In Proceedingsofthe8thInternationalSymposiumon
Static Analysis (SAS). Springer-Verlag, 40–56.
[48]Anil Koyuncu, Tegawendé F Bissyandé, Dongsun Kim, Jacques Klein, Martin
Monperrus, and Yves Le Traon. 2017. Impact of Tool Support in Patch Con-
struction.In Proceedingsofthe26thACMSIGSOFTInternationalSymposiumon
Software Testing and Analysis (ISSTA). ACM, 237–248.
[49]J. Krinke. 2001. Identifying Similar Code with Program Dependence Graphs.
InProceedings of the 8th Working Conference on Reverse Engineering (WCRE).
301–309.
[50]D. E. Krutz and E. Shihab. 2013. CCCD: Concolic code clone detection. In Pro-
ceedingsofthe20thWorkingConferenceonReverseEngineering(WCRE).489–490.
[51]Frederick Wilfrid Lancaster and Emily Gallup Fayen. 1973. Information Retrieval:
On-line. Melville Publishing Company.
[52]Mu-Woong Lee, Jong-Won Roh, Seung-won Hwang, and Sunghun Kim. 2010.
InstantCodeCloneSearch.In Proceedingsofthe18thInternationalSymposiumon
Foundations of Software Engineering (FSE). ACM, 167–176.
[53]OtÃąvioA.L.Lemos,AdrianoC.dePaula,FelipeC.Zanichelli,andCristinaV.
Lopes.2014. Thesaurus-basedAutomaticQueryExpansionforInterface-driven
CodeSearch.In Proceedingsofthe11thWorkingConferenceonMiningSoftware
Repositories (MSR). ACM, 212–221.
[54]O. A. L. Lemos, A. C. de Paula, H. Sajnani, and C. V. Lopes. 2015. Can the
Useof TypesandQueryExpansion HelpImproveLarge-Scale CodeSearch?.In
Proceedings of the 15th IEEE International Working Conference on Source Code
Analysis and Manipulation (SCAM). 41–50.
[55]Sihan Li, Xusheng Xiao, Blake Bassett, Tao Xie, and Nikolai Tillmann. 2016.
Measuring Code Behavioral Similarity for Programming and Software Engineer-
ing Education. In Proceedings of the 38th International Conference on Software
Engineering Companion (ICSE). ACM, 501–510.
[56]Zhenmin Li, Shan Lu, Suvda Myagmar, and Yuanyuan Zhou. 2004. CP-Miner:
ATool forFindingCopy-pasteandRelatedBugsinOperatingSystemCode.In
Proceedingsofthe6thConferenceonSymposiumonOpeartingSystemsDesign&
Implementation (OSDI). USENIX Association.
[57]Chao Liu, Chen Chen, Jiawei Han, and Philip S. Yu. 2006. GPLAG: Detection of
SoftwarePlagiarismbyProgramDependence GraphAnalysis.In Proceedingsof
the 12th ACM International Conference on Knowledge Discovery and Data Mining
(KDD). ACM, 872–881.
[58]Kui Liu, Dongsun Kim, Tegawendé F Bissyandé, Shin Yoo, and Yves Le Traon.
2017.MiningFixPatternsforFindBugsViolations. arXivpreprintarXiv:1712.03201
(2017).
[59]Fei Lv, Hongyu Zhang, Jian-guang Lou, Shaowei Wang, Dongmei Zhang, and
JianjunZhao.2015.CodeHow:EffectiveCodeSearchbasedonAPIUnderstanding
and Extended Boolean Model. In Proceedings of the 2015 30th IEEE/ACM Inter-
nationalConferenceonAutomatedSoftwareEngineering(ASE).IEEEComputer
Society, 260–270.
[60]Lena Mamykina, Bella Manoim, Manas Mittal, George Hripcsak, and Björn Hart-
mann.2011. DesignLessonsfromtheFastestQ&ASiteintheWest.In Proceedings
oftheSIGConferenceonHumanFactorsinComputingSystems(CHI).ACM,2857–
2866.
[61]Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schütze. 2008. Intro-
duction to Information Retrieval. Cambridge University Press.
[62]A. Marcus and J. I. Maletic. 2001. Identification of High-level Concept Clones in
SourceCode.In Proceedingsofthe16thInternationalConferenceonAutomated
Software Engineering (ASE). 107–114.
[63]LeeMartie,AndrévanderHoek,andThomasKwak.2017. Understandingthe
Impact ofSupport forIteration onCode Search.In Proceedings ofthe 11thJoint
Meeting on Foundations of Software Engineering (FSE). ACM, 774–785.
[64]MichaelMcCandless,ErikHatcher,andOtisGospodnetic.2010. LuceneinAction,
Second Edition: Covers Apache Lucene 3.0. Manning Publications Co.
[65]C.McMillan,M.Grechanik,D.Poshyvanyk,C.Fu,andQ.Xie.2012. Exemplar:
ASourceCodeSearchEngineforFindingHighlyRelevantApplications. IEEE
Transactions on Software Engineering (TSE) 38, 5 (Sept. 2012), 1069–1087.
[66]CollinMcMillan,MarkGrechanik,DenysPoshyvanyk,QingXie,andChenFu.
2011. Portfolio:FindingRelevantFunctionsandTheirUsage.In Proceedingofthe
33rd International Conference on Software Engineering (ICSE). ACM, 111–120.
[67]Laura Moreno, Gabriele Bavota, Massimiliano Di Penta, Rocco Oliveto, and
AndrianMarcus.2015. HowCanIUseThisMethod?.In Proceedingsofthe37th
International Conference on Software Engineering (ICSE). IEEE Press, 880–890.
[68]SeyedMehdiNasehi,JonathanSillito,FrankMaurer,andChrisBurns.2012. WhatMakes a Good Code Example?: A Study of Programming Q&A in StackOverflow.InProceedings of the 28th IEEE International Conference on Software Maintenance
(ICSM). IEEE, 25–34.
[69]Haoran Niu, Iman Keivanloo, and Ying Zou. 2017. Learning to Rank Code
Examples for Code Search Engines. Empirical Software Engineering (EMSE) 22, 1
(Feb. 2017), 259–291.
[70]Praveen Pathak, Michael Gordon, and Weiguo Fan. 2000. Effective Information
RetrievalUsingGeneticAlgorithmsbasedMatchingFunctionsAdaptation.In
Proceedingsofthe33rdHawaiiInternationalConferenceonSystemSciences(HICSS).
IEEE, 8–pp.
[71]Luca Ponzanelli, Gabriele Bavota, Massimiliano Di Penta, Rocco Oliveto, and
MicheleLanza.2014. MiningStackoverflowtoTurntheIDEintoaSelf-confidentProgrammingPrompter.In Proceedingsofthe11thWorkingConferenceonMining
Software Repositories (MSR). ACM, 102–111.
[72]D.Poshyvanyk,Y.G.Gueheneuc,A.Marcus,G.Antoniol,andV.Rajlich.2007.
Feature Location Using Probabilistic Ranking of Methods Based on Execution
Scenarios and Information Retrieval. IEEE Transactions on Software Engineering
(TSE)33, 6 (June 2007), 420–432.
[73]ChanchalKRoy,JamesRCordy,andRainerKoschke.2009. ComparisonandEval-
uation of Code Clone Detection Techniques and Tools: A Qualitative Approach.
Science of Computer Programming 74, 7 (May 2009), 470–495.
[74]Hitesh Sajnani, Vaibhav Saini, Jeffrey Svajlenko, Chanchal K Roy, and Cristina V
Lopes.2016. SourcererCC:ScalingCodeCloneDetectiontoBigCode.In Proceed-
ings of the 38th International Conference on Software Engineering (ICSE) . ACM,
1157–1168.
[75]GerardSaltonandMichaelJ.McGill.1986. IntroductiontoModernInformation
Retrieval. McGraw-Hill, Inc.
[76]G.Salton,A.Wong,andC.S.Yang.1975. AVectorSpaceModelforAutomatic
Indexing. Commun. ACM 18, 11 (Nov. 1975), 613–620.
[77]HuascarSanchez.2013. SNIPR:ComplementingCodeSearchwithCodeRetarget-
ingCapabilities.In Proceedingsofthe2013InternationalConferenceonSoftware
Engineering (ICSE). IEEE Press, 1423–1426.
[78]Niko Schwarz, Mircea Lungu, and Romain Robbes. 2012. On How Often Code is
ClonedAcrossRepositories.In Proceedingsofthe34thInternationalConference
on Software Engineering (ICSE). IEEE Press, 1289–1292.
[79]Raphael Sirres, Tegawendé F. Bissyandé, Dongsun Kim, David Lo, Jacques Klein,
Kisub Kim, and Yves Le Traon. 2018. Augmenting and Structuring User Queries
to Support Efficient Free-Form Code Search. Empirical Software Engineering
(EMSE)(2018), (to appear).
[80]KathrynT.Stolee,SebastianElbaum,andDanielDobos.2014. SolvingtheSearch
for Source Code. ACM Transactions on Software Engineering and Methodology
(TOSEM) 23, 3 (May 2014), 26:1–26:45.
[81]Fang-Hsiang Su, Jonathan Bell, Kenneth Harvey, Simha Sethumadhavan, GailKaiser, and Tony Jebara. 2016. Code Relatives: Detecting Similarly Behaving
Software.In Proceedingsofthe24thACMSIGSOFTInternationalSymposiumon
Foundations of Software Engineering (FSE). ACM, 702–714.
[82]SiddharthSubramanian,LauraInozemtseva,andReidHolmes.2014. LiveAPI
Documentation.In Proceedingsofthe36thInternationalConferenceonSoftware
Engineering (ICSE). ACM, 643–652.
[83]Jeffrey Svajlenko, Judith F Islam, Iman Keivanloo, Chanchal K Roy, and Moham-
madMamunMia.2014. TowardsABigDataCuratedBenchmarkofInter-Project
Code Clones. In Proceedings of the IEEE International Conference on Software
Maintenance and Evolution (ICSME). IEEE, 476–480.
[84]JeffreySvajlenkoandChanchalKRoy.2015. EvaluatingCloneDetectionTools
with Bigclonebench. In Proceedings of the IEEE International Conference on Soft-
ware Maintenance and Evolution (ICSME). IEEE, 131–140.
[85]ChristophTreudeandMartinPRobillard.2016. AugmentingAPIDocumenta-
tionwithInsightsfromStackOverflow.In Proceedingsofthe38thInternational
Conference on Software Engineering (ICSE). ACM, 392–403.
[86]MartinWhite,MicheleTufano,ChristopherVendome,andDenysPoshyvanyk.
2016. Deep Learning Code Fragments for Code Clone Detection. In Proceedings
ofthe31stIEEE/ACMInternationalConferenceonAutomatedSoftwareEngineering
(ASE). ACM, 87–98.
[87]Xin Xia, Lingfeng Bao, David Lo, Pavneet Singh Kochhar, Ahmed E. Hassan, and
Zhenchang Xing. 2017. What Do Developers Search for on the Web? Empirical
Software Engineering (EMSE) 22, 6 (April 2017), 3149–3185.
[88]Tao Xie and Jian Pei. 2006. MAPO: Mining API Usages from Open Source
Repositories. In Proceedings of the International Workshop on Mining Software
Repositories (MSR). ACM, 54–57.
[89]Le Zhao and Jamie Callan. 2010. Term Necessity Prediction. In Proceedings of the
19th ACMInternational Conferenceon Information andKnowledge Management
(CIKM). ACM, 259–268.
[90]Le Zhao and Jamie Callan. 2012. Automatic Term Mismatch Diagnosis for Selec-
tiveQueryExpansion.In Proceedingsofthe35thInternationalACMConference
on Research and Development in Information Retrieval (SIGIR). ACM, 515–524.
957
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:38:44 UTC from IEEE Xplore.  Restrictions apply. 