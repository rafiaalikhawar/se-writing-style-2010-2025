Automated Directed Fairness Testing
Sakshi Udeshi
Singapore Univ. of Tech. and Design
Singapore
sakshi_udeshi@mymail.sutd.edu.sgPryanshu Arora
BITS Pilani
India
pryanshu23@gmail.comSudipta Chattopadhyay
Singapore Univ. of Tech. and Design
Singapore
sudipta_chattopadhyay@sutd.edu.sg
ABSTRACT
Fairness is a critical trait in decision making. As machine-learning
modelsareincreasinglybeingusedinsensitiveapplicationdomains
(e.g.educationandemployment)fordecisionmaking,itiscrucial
thatthedecisions computedbysuchmodels are freeofunintended
bias.Buthowcanweautomaticallyvalidatethefairnessofarbitrarymachine-learningmodels?Foragivenmachine-learningmodeland
asetofsensitiveinputparameters,ourAeqitasapproachauto-
matically discovers discriminatory inputs that highlight fairness
violation. At the core of Aeqitas are three novel strategies to
employ probabilistic search over the input space with the objectiveofuncoveringfairnessviolation.OurAeqitasapproachleveragesinherentrobustnesspropertyincommonmachine-learningmodels
to design and implement scalable test generation methodologies.
Anappealingfeatureofourgeneratedtestinputsisthattheycan
besystematicallyaddedtothetrainingsetoftheunderlyingmodel
and improve its fairness. To this end, we design a fully automatedmodule that guarantees to improve the fairness of the model.
WeimplementedAeqitasandwehaveevaluateditonsixstate-
of-the-art classifiers. Our subjects also include a classifier that was
designedwithfairnessinmind.WeshowthatAeqitaseffectively
generates inputs to uncover fairness violation in all the subjectclassifiers and systematically improves the fairness of respective
modelsusingthegeneratedtestinputs.Inourevaluation,Aeqitas
generatesupto70%discriminatoryinputs(w.r.t.thetotalnumber
of inputs generated) and leverages these inputs to improve the
fairness up to 94%.
CCS CONCEPTS
•Software and its engineering →Software testing and de-
bugging;
KEYWORDS
Software Fairness, Directed Testing, Machine Learning
ACM Reference Format:
Sakshi Udeshi, Pryanshu Arora, and Sudipta Chattopadhyay. 2018. Auto-
mated Directed Fairness Testing. In Proceedings of the 2018 33rd ACM/IEEE
International Conference on Automated Software Engineering (ASE ’18), Sep-
tember3–7,2018,Montpellier,France. ACM,NewYork,NY,USA, 11pages.
https://doi.org/10.1145/3238147.3238165
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ASE ’18, September 3–7, 2018, Montpellier, France
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5937-5/18/09...$15.00
https://doi.org/10.1145/3238147.3238165A
B
A1 B1A2
B2Decision Boundary
Figure 1: Classifier fairness
1 INTRODUCTION
Nondiscrimination is one of the most critical factors for social
protection and equal human rights. The basic idea behind non-discrimination is to eliminate any societal bias based on sensi-tive attributes, such as race, gender or religion. For example, itis not uncommon to discover the declaration of following non-
discrimination policy in universities [12]:
“TheUniversityiscommittedtoapolicyofequaloppor-
tunity for all personsand does not discriminate on
the basis of race, color, national origin, age, marital
status, sex, sexual orientation, gender identity, gen-
der expression, disability, religion, height, weight,
orveteranstatusinemployment,educationalpro-
grams and activities, and admissions"
Due to the massive progress in machine learning in the last few
decades,itsapplicationhasnowescalatedoveravarietyofsensitivedomains,includingeducationandemployment.Thekeyinsightisto
primarily automate decision making via machine-learning models.
On the flip side, such models may introduce unintended societalbias due to the presence of bias in their training dataset. This, inturn, violates the non-discrimination policy that the respective
organizationorthenationisintendedtofightfor.Thevalidation
ofmachine-learningmodels,tocheckforpossiblediscrimination,
is therefore critically important.
In this paper, we are concerned about the case that any two
individuals who are similar with respect to a job at hand should
alsobetreatedinasimilarfashionduringdecisionmaking.Thus,
we focus towards individual fairness, as it is critical for eliminating
societal bias and aim to check for discrimination that might violate
individual fairness [2]. The precise nature of such discrimination
dependsonthemachine-learningmodelanditsinputfeatures.Con-
sequently,given amachine-learningmodeland theinputfeatures
ofthemodel,itispossibletosystematicallyexploretheinputspace
and discover inputs that induce discrimination. We call such in-
putsdiscriminatoryinputs.Theprimaryobjectiveofthispaperis
98
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:26:09 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Sakshi Udeshi, Pryanshu Arora, and Sudipta Chattopadhyay
to design scalable techniques that facilitate rapid discovery of dis-
criminatory inputs. In particular, given a machine-learning model
and aset ofdiscriminatory input features(e.g. race, religion,etc.),
our Aeqitas approach automatically discovers inputs to clearly
highlight the discriminatory nature of the model under test.
As an example, consider the decision boundary of a classifier
shown in Figure 1. Assume the two points AandBthat differ only
in beingGenderAorGenderB. Despite being vastly similar, except
in the gender aspect, the model classifies the points AandBdiffer-
ently.Ifweconsiderthatsuchaclassifierisusedtopredictthelevel
ofsalary,thenitcertainlyintroducesunintendedsocietalbiasbasedon gender. Such unfair social biases not only affect the decisions of
today but also might amplify it for future generations. The reason
behindthediscrimination(i.e.unfairness),asshownbetweenpoints
AandB, can be due to outdated training data that unintentionally
introduces bias in certain attributes of the classifier model, e.g.,gender in Figure 1. Using our Aeqitas approach, we automati-
cally discover the existence of inputs similar to AandBwith high
probabilities. These inputs, then, are used to systematically retrain
the model and reduce its unfairness.
ThereasonAeqitasworksisduetoitsdirectedstrategyfortest
generation.Inparticular,Aeqitasexploitstheinherentrobustness
propertyofcommonmachinelearningmodelsforsystematically
directingtestgeneration.Asaresultofthisrobustnessproperty,the
modelsshouldexhibitlowvariationintheiroutput(s)withsmall
perturbations in their input(s). For example, consider the points A1
andA2whichareintheneighbourhoodofthepoint A.Sincethe
pointAexhibits discriminatory nature, it is likely that both points
A1andA2will be discriminatory, as reflected via the presence
of points B1andB2, respectively. In our Aeqitas approach, we
firstrandomlysampletheinputspacetodiscoverthepresenceof
discriminatoryinputs(e.g.point AinFigure1).Then,wesearchthe
neighbourhoodoftheseinputs,asdiscoveredduringtherandom
sampling,tofindthepresenceofmoreinputs(e.g.points A1and
A2inFigure 1) of the same nature.
An appealing feature of Aeqitas is that it leverages the gener-
atedtestinputsandsystematicallyretrainsthemachine-learning
modelundertesttoreduceitsunfairness.Theretrainingmodule
is completely automatic and it therefore acts as a significant aidto the software engineers to improve the (individual) fairness of
machine-learningmodels.Thedirectedtestgenerationandauto-
mated retraining set Aeqitas apart from the state-of-the-art in
fairnesstesting[ 5].Whileexistingwork[ 5]alsoconsiderstestgen-
eration, such tests were generated randomly. If the discriminatory
inputsarelocatedonlyinspecializedlocationsoftheinputspace,
then random test generators are unlikely to be effective in finding
individuals discriminated by the corresponding model. To this end,
Aeqitasempiricallyvalidatesthatadirectedtestgeneration,to
uncover the discriminatory input regions, is indeed more desir-able than random test generation. Moreover, Aeqitas providesstatistical evidence that if it fails to discover any discriminatory
input,thenthemachine-learningmodelundertestisfairwithhigh
probability.
Theremainderofthepaperisorganizedasfollows.Afterprovid-
inganoverviewof Aeqitas(Section2),wemakethefollowing
contributions:(1)We present Aeqitas, a novel approach to systematicallygenerate discriminatory test inputs and uncover the fair-
ness violation in machine-learning models. To this end, we
proposethreedifferentstrategieswithvaryinglevelsofcom-
plexity (Section 4).
(2)We present a fully automated technique to leverage the gen-
erateddiscriminatoryinputsandsystematicallyretrainthe
machine-learning models to improve its fairness (Section 4).
(3)Weprovideanimplementationof Aeqitasbasedonpython.
Ourimplementationandallexperimentaldataarepublicly
available (Section 5).
(4)We evaluate our Aeqitas approach with six state-of-the-
artclassifiersincludingaclassifierthatwasdesignedwith
fairness in mind. Our evaluation reveals that Aeqitas is
effective in generating discriminatory inputs and improving
the fairness of the classifiers under test. In particular, Ae-
qitas generated up to 70% discriminatory inputs (w.r.t. the
totalnumberofinputsgenerated)andimprovedthefairness
up to 94% (Section 5).
Afterdiscussingtherelatedwork(Section6),weoutlinedifferent
threats to validity (Section 7) before conclusion and consequences
(Section 8).
2 BACKGROUND
Inthissection,wewilldiscussthecriticalimportanceoffairness
testing and outline the key insight behind our approach.
Importanceoffairness Theusageofmachinelearningisincreas-
ingly being observed in areas that are under the purview of anti-
discrimination laws. In particular, application domains such as law
enforcement,credit,educationandemploymentcanallbenefitfrom
machine learning. Hence, it is crucial that decisions influenced by
any machine-learning model are free of any unnecessary bias.
As an example, consider a machine-learning model that predicts
the income levels of a person. It is possible that such a model was
trainedonadataset,which,inturnwasunfairlybiasedtoacertain
genderoracertainrace.Asaresult,forallequivalentcharacteristics,
barringthegenderorrace,thecreditworthinessofapersonwill
be predicted differently by this model. If financial institutions used
suchamodeltodeterminethecreditworthinessofanindividual,
then individuals might be disqualified only on the basis of their
gender or race. Such a discrimination is certainly undesirable, as it
reinforcesandamplifiestheunfairbiasesthatwe,asasocietyare
continuously fighting against.
Fairness in Aeqitas Aeqitas aims to discover the violation
ofindividual fairness [2] inmachine-learning models. Thismeans,
Aeqitasaimstofindinstancesofpairofinputs IandI/primethatare
classified differently despite being vastly similar. The similarity
between inputs IandI/primeis based on a set of potentially discrimina-
tory input parameters (see Definition 1). Detecting the violation
of individual fairness is challenging. This is because inputs that
arepronetotheviolationofindividualfairnessmightbelocated
onlyinspecificregionsoftheinputspaceofamodel.Consequently,
specializedanddirectedtechniquesarerequiredtorapidlylocate
these input regions. This is theprimary motivation behind the de-
velopmentof Aeqitas.Fortherestofthepaper,wewillsimply
99
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:26:09 UTC from IEEE Xplore.  Restrictions apply. Automated Directed Fairness Testing ASE ’18, September 3–7, 2018, Montpellier, France
use the term fairness (instead of individual fairness) in the light of
our Aeqitas approach (see Definition 1).
Towards fair machine-learning models A naive approach to
designfairmachine-learning models is to ignore certain sensitive
attributes suchas race, color, religion,gender, disability, orfamily
status.Itisnaturaltoassumethatifsuchattributesareheldback
from decision making, then the respective model will not discrimi-
nate. Unfortunately, such an approach of accomplishing fairness
throughblindness fails.Thisisbecauseofthepresenceofredundant
encoding in the training dataset [ 15]. Due to the redundant encod-
ing, it is frequently possible to predict the unknown (sensitive)
attributesfromotherseeminglyinnocuousfeatures.Forexample,
consider certain ethnic groups in a city that are geographically
boundtocertainareas.Insuchcases,evenifamachine-learning
model in a financial institute does not use ethnicity as a parameter
todecidecreditworthiness,itispossibletoguessethnicityfromgeo-graphiclocations,whichindeedmightbeaparameterforthemodel.
Therefore, it is critical to systematically test a machine-learning
model to validate its fairness property.
Why fairness testing is different In contrast to classic software
testing,testingmachine-learningmodelsfaceadditionalchallenges.
Typically, these models are deployed in contexts where the formal
specification of the software functionality is difficult to develop.In fact, such models are designed to learn from existing data be-
cause ofthe challengesin creating amathematical definitionof the
desiredsoftwareproperties.Moreover,anerroneoussoftwarebe-
haviourcanberectifiedbyretrainingthemachine-learningmodels.
However, for classic software, a software bug is typically fixed via
modifying the responsible code.
State-of-the-art in fairness testing The state-of-the-art in sys-
tematictestingofsoftwarefairnessisstillatitsinfancy.Incontrast
to existing work [ 5], Aeqitas focuses on directed test generation
strategy. As evidenced by our evaluation, this is crucial to locate
specific input regions that violate individual fairness. To illustrate
ourobjective,consideramachine-learningmodel fanditsinputs I
andI/prime.Idiffersfrom I/primeonlyinbeingassignedadifferentvalueina
potentially discriminatory input parameter. For example, if gender
is the potentially discriminatory input parameter, then Iwill be
differentfrom I/primeonlyinbeing GenderAorGenderB.Weareinter-
estedtodiscoverinputs IorI/prime,wherethedifferenceinoutputsof
the model, captured via |f(I)−f(I/prime)|, is beyond a pre-determined
threshold. We call such inputs IorI/primeto bediscriminatory inputs
for the model f. It is important to note that the discrimination
thresholdand thepotentially discriminatoryinputparameters are
supplied by the users of our tool. In the preceding example, the
potentially discriminatory input parameter, i.e., gendercan be spec-
ified by the user. Similarly, users can also fine tune the value at
which|f(I)−f(I/prime)|is considered to be discriminatory.
Robustness in machine learning Robustness is a notion that
saysthattheoutputofamachine-learningmodelisnotdramatically
affected by small changes to its input [ 3]. Assume a model f, let
ibe the input to fandδbe a small value. If fis robust, then
f(i)≈f(i+δ). Nevertheless, existing techniques provide evidence
to find inputs that violate this robustness property. Such inputs are
called adversarial inputs [ 14][7][13]. However, adversarial inputs
generallycoveronlyasmallfractionoftheentireinputspace.Thisisevidentbythefactthatadversarialinputsneedtobecraftedusing
very specialized techniques. Additionally, Aeqitas is designed to
avoidtheseadversarialinputregionsbysystematicallydirectingthe
test generators.Intuitively,Aeqitasachievesthis by reducingthe
probability to explore an input region when tested inputs from the
regiondidnotexhibitdiscriminatorynature(seeAlgorithm 2for
details).Consequently,ifadversarialornon-robustinputregions
do not exhibit discriminatory nature, such regionswill eventually
be explored only with very low probability.
3 APPROACH AT A GLANCE
We propose, design and evaluate three schemes, with varying lev-
els of complexities, to systematically uncover software fairness
problems. The crucial components of our approach are outlined
below.
Globalsearch Inthefirststepofallourproposedschemes,weuni-
formlysampletheinputsandrecordthediscriminatoryinputsthat
we find. In the light of uniformly sampling the input space, we can
guarantee, with very high probability, to discover a discriminatory
input, if such an input exists. For instance, Figure 2(a) highlights
the probability of finding a discriminatory input in an input space
with only 1% discriminatory inputs. Therefore, if discriminatory
inputs exist, the first step of our proposed schemes guarantee to
find at least one such input with high probabilities.Local search
The second step of our proposed schemes share the
following hypothesis: If there exists a discriminatory input I∈I,
where Icaptures the input domain, then there exist more discrimi-
natoryinputs inthe inputspacecloser to I.The inputdomain Ican
be considered as the cartesian product of the domain of ninput
parameters, say P1,P2,...,Pn. We assume Ikcaptures the domain
ofinputparameter Pk.Therefore, I=I1×I2×...×In.Aninput
parameter p∈/uniontext.1n
i=1Pican be potentially discriminatory if the out-
putofthemachine-learningmodelshouldnotbebiasedtowards
specificvaluesin Ip.Withoutlossofgenerality,weassumeasubset
of parameters Pdisc⊆/uniontext.1ni=1Pito be potentially discriminatory.
For an input I∈I,w eu s e Ikto capture the value of parameter
Pkwithininput I.Basedonthisnotion,weexplorethefollowing
methods to realize our hypothesis.Our methods differ on how we
systematicallyexploretheneighbourhoodofadiscriminatoryinput
I(d).I(d), in turn, was discovered in the first step of Aeqitas.
(1)First a parameter p∈/uniontext.1ni=1Pi\Pdiscis randomly chosen.
Thenasmallperturbation(i.e.change) δisaddedto I(d)
p.Typ-
icallyδ∈{ −1,+1}as we consider integer and real-valued
input parameters in our evaluation.
(2)Inthesecondmethod,weassignprobabilitiesonhowtoper-
turbachosenparameter.Aspecificparameter p∈/uniontext.1n
i=1Pi\
Pdiscis still chosen uniformly at random. However, if a
givenperturbation δofI(d)
pconsistentlyyieldsdiscrimina-
toryinputs,thentheperturbation δisemployedwithhigher
probability.Since δtypicallybelongstoasmallsetofvalues,
such a strategy works efficiently in practice.
(3)Thethird methodaugments thesecondmethod byrefining
probabilities to perturb an input parameter. Concretely, if
100
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:26:09 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Sakshi Udeshi, Pryanshu Arora, and Sudipta Chattopadhyay
0 200 400 600 800 1,00000.20.40.60.81
IterationsProbability of ﬁnding a discriminatory input
(a) (b)
Figure 2: (a) Probability of finding discriminatory inputs, (b) Estimation of the percentage of discriminatory inputs
X I1 Ik Im In
I and I’ only  
differ hereI’ I
Neighbourhood of I is obtained by adding small change δ to an input parameterPdisc
X’ I1 Ik Im In
I and I’ only  
differ herePdisc
X I1 + δ Ik Im In… X In + δ Ik Im I1
Figure 3: Our Aeqitas approach at a glance
perturbingthevalueofparameter p∈/uniontext.1n
i=1Pi\Pdiscconsis-
tentlyyieldsdiscriminatoryinputs,thentheparameter pwill
be significantly more likely to be chosen for perturbation.
Ourproposedmethodologiesarefullyautomated,theydonotre-
quirethesourcecodeofthemodelsandworkefficientlyinpractice
for state-of-the-art classifiers.
Figure 3illustrates Aeqitas approach when IandI/primewere dis-
covered in the first step. Then, the second step explored the neigh-
bourhood of Iby adding small changes δto an input parameter.
Estimation of discriminatory inputs An appealing feature of
Aeqitas is that we can estimate the percentage of discriminatory
inputsin I.Tothisend,weleveragethelawoflargenumbers(LLN)
inprobabilitytheory.Inparticular,wegenerate Kinputsuniformly
atrandomandcheckwhethertheycanleadtodiscriminatoryinputs.
Assume that K/prime≤Kinputs turn out to be discriminatory. We
compute theratioK/prime
Kover a largenumber of trials.Accordingto
LLN, the average of these ratios closely approximates the actual
percentageofdiscriminatoryinputsin I.Figure2(b)highlightssuch
convergence after only 400 trials when Kwas chosen to be 1000.
Why Aeqitas works? ThereasonAeqitasworksisbecause
oftherobustnesspropertyofcommonmachine-learningmodels.In
particular,ifweperturbtheinputtoamodelbysomesmall δ,then
the output is not expected to change dramatically. As we expectthe machine-learning models under test to be relatively robust,
we can leverage their inherent robustness property to systemati-
cally generate test inputs that exhibit similar characteristics. In our
Aeqitas approach, we focus on the discriminatory nature of a
given input. We aim to discover more discriminatory inputs in theTable 1: Notations used in Aeqitas approach
nThe number of input parameters to the machine-
learning model under test
IThe input domain of the model
PiThei-th input parameter of the model
PSet of all input parameters, i.e., P=/uniontext.1n
i=1Pi
PdiscSetofsensitiveorpotentiallydiscriminatoryinputpa-
rameters (e.g. gender). Clearly, Pdisc⊆/uniontext.1n
i=1Pi
IpThe value of input parameter pin inputI∈I
γA pre-determined discrimination threshold
proximityofanalreadydiscovereddiscriminatoryinputleveraging
the robustness property.
How Aeqitas can be used to improve software fairness?
We have designed a fully automated module that leverages on
the discriminatory inputs generated by Aeqitas and retrains the
machine-learningmodelundertest.Weempiricallyshowthatsuch
astrategyprovidesusefulcapabilitiestoadeveloper.Specifically,
our Aeqitas approach automatically improves the fairness of
machine-learningmodelsviaretraining.Forinstance,incertainde-
cision tree classifiers, our Aeqitas approach reduced the fraction
of discriminatory inputs up to 94%.
4 DETAILED APPROACH
Inthissection,wediscussourAeqitasapproachindetail.Tothis
end, we will use the notations captured in Table 1.
Our approach revolves around discovering discriminatory inputs
via systematic perturbation. We introduce the notion of discrimi-
natory inputs and perturbation formally before delving into the
algorithmic details of our approach.
Definition 1. (Discriminatory Input and fairness) Letf
be a classifier under test, γbe the pre-determined discrimination
threshold (e.g. chosen by the user), and I∈I. AssumeI/prime∈Isuch that
there exists a non-empty set Q⊆Pdiscand for all q∈Q,Iq/nequalI/primeq
and for all p∈P\Q,Ip=I/primep.I f|f(I)−f(I/prime)|>γ, thenIis called
a discriminatory input of the classifier fand is an instance that
manifests the violation of (individual) fairness in f.
101
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:26:09 UTC from IEEE Xplore.  Restrictions apply. Automated Directed Fairness Testing ASE ’18, September 3–7, 2018, Montpellier, France
Model
Local Search
(LOCAL_EXP)disc_inpsGlobal Search
(GLOBAL_EXP)
TestFairness
Improvement ModuleTraining DataImproved Model
Threshold
(γ)
Pdisc
Figure 4: An overview of our Aeqitas approach
Definition 2. (Perturbation) We define perturbation дas a
functionд:I×(P\Pdisc)×Γ→ Iwhere Γ={−1,+1}captures
the set of directions to perturb an input parameter. If I/prime=д(I,p,δ)
whereI∈I,p∈P\Pdiscandδ∈Γ, thenI/primep=Ip+δand for all
q∈P\{p}, we have I/primeq=Iq.
It is worthwhile to mention that the set of directions to perturb
aninputparameter,i.e. Γcaneasilybeextendedwithmorepossi-
bilitiestoperturb.Besides,itcanalsobecustomizedwithrespect
to different input parameters. However, for the sake of brevity, we
will stick with the simplified version stated in Definition 2.
An overview of our overall approach appears in Figure 4. The
maincontributionofthispaperisanautomatedtestgeneratorto
discoverfairnessviolation.Thisinvolvestwostages:1)globalsearch
(GLOBAL_EXP ) and 2) local search ( LOCAL_EXP ) over the input
domain I. Optionally, the generated test inputs can be leveraged to
retrain the model under test and improve fairness.
In the following, we will describe the crucial components of our
Aeqitas approach, as shown in Figure 4.
4.1 Global Search
Themotivationbehindourglobalsearch(cf.procedureglobal_exp
in Algorithm 1) is to discover some points in Ithat can be used
todriveourlocalsearchalgorithm.Tothisend,wefirstselectan
inputIrandomlyfromtheinputdomain.Input I,then,isusedto
generateasetofinputsthatcoverallpossiblevaluesofsensitive
parameters Pdisc⊆P. This leads to a set of inputs I(d). We note
thatthesetofsensitiveparameters(e.g.race,religion,gender) Pdisc
typically has a small size. Therefore, despite the exhaustive nature
ofgenerating I(d),thisispracticallyfeasible.Finally,wediscover
the discriminatory inputs (cf. Definition 1) within I(d)and use the
resulting discriminatory input set for further exploration during
our local search over I.
4.2 Local Search
Inthistestgenerationphase,wetaketheinputsgeneratedbyour
global search (i.e. disc_inputs) and then search in the neighbour-
hood ofdisc_inputsto discover other inputs with similar char-
acteristics (cf. procedure local_exp in Algorithm 2). Our search
strategy is motivated from the robustness property inherent inAlgorithm 1 Global Search
1:procedure global_exp( P,Pdisc)
2:disc_inps←ϕ
3:⊿Nis the number of trials in global search
4:foriin (0, N)do
5: Select an input I∈Iat random
6: ⊿I(d)extendsIwith all possible values of Pdisc
7: I(d)←{I/prime|∀p∈P\Pdisc.Ip=I/primep}
8: if(∃I,I/prime∈I(d),|f(I)−f(I/prime)|>γ)then
9: disc_inps←disc_inps∪{I}
10: end if
11:end for
12:returndisc_inps
13:end procedure
Algorithm 2 Local Search
1:procedure local_exp( disc_inps,P,Pdisc,Δv,Δpr)
2:Test←ϕ
3:LetP/prime=P\Pdisc
4:Letσpr[p]=1
|P/prime|for allp∈P/prime
5:Letσv[p]=0.5 for allp∈P/prime
6:forI∈disc_inpsdo
7: ⊿Nis the number of trials in local search
8: foriin (0, N)do
9: Selectp∈P/primewith probability σpr[p]
10: Selectδ=−1 with probability σv[p]
11: ⊿Note that Iis modified as a side-effect of modifying Ip
12: Ip←Ip+δ
13: ⊿I(d)extendsIwith all values of Pdisc
14: I(d)←{I/prime|∀p∈P\Pdisc.Ip=I/primep}
15: if(∃I,I/prime∈I(d),|f(I)−f(I/prime)|>γ)then
16: ⊿Add the perturbed input I
17: Test←Test∪{I}
18: end if
19: update_prob( I,p,Test,δ,Δv,Δpr)
20: end for
21:end for
22:returnTest
23:end procedure
common machine-learning models.According to the notionof ro-
bustness, the neighbourhood of an input should produce similar
output. Therefore, it becomes logical to search the neighbourhood
ofdisc_inputs, as these are the discriminatory inputs and their
neighbourhood are likely to be discriminatory for robust models.
Tosearchtheneighbourhoodof disc_inputs,Aeqitasperturbs
an inputI∈disc_inputsby changing the value of some parameter
p∈P\Pdisc(i.e.Ip). The value of the parameter pis perturbed by
δ∈{ −1,+1}. We note that as a side-effect of changing Ip, input
Iis automatically modified. This modified version of Iis further
perturbedinsubsequentiterationsoftheinnerloopinAlgorithm 2.
Our Aeqitas approach chooses a parameter p∈P\Pdiscwith
probability σpr[p](cf.Algorithm 2).Forall p∈P\Pdisc,initially
σpr[p]was assigned to1
|P\Pdisc|. Oncepis chosen its value is
perturbed by δ=−1 with probability σv[p]and byδ=+1 with
probability1 −σv[p].σv[p]isinitializedto0 .5forallparameters
inp∈P\Pdisc.
102
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:26:09 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Sakshi Udeshi, Pryanshu Arora, and Sudipta Chattopadhyay
Algorithm 3 Aeqitas semi-directed update probability
1:procedure update_prob( I,p,Test,δ,Δv,Δpr)
2:if(I∈Test∧δ=−1)∨(I/nelementTest∧δ=+1)then
3: σv[p]←min(σv[p]+Δv,1)
4:end if
5:if(I/nelementTest∧δ=−1)∨(I∈Test∧δ=+1)then
6: σv[p]←max(σv[p]−Δv,0)
7:end if
8:end procedure
Algorithm 4 Aeqitas fully-directed update probability
1:procedure update_prob( I,p,Test,δ,Δv,Δpr)
2:if(I∈Test∧δ=−1)∨(I/nelementTest∧δ=+1)then
3: σv[p]←min(σv[p]+Δv,1)
4:end if
5:if(I/nelementTest∧δ=−1)∨(I∈Test∧δ=+1)then
6: σv[p]←max(σv[p]−Δv,0)
7:end if
8:ifI∈Testthen
9: σpr[p]←σpr[p]+Δpr
10: σpr[p]←σpr[p]/summationtext.1
x∈P\Pdiscσpr[x]for allp∈P\Pdisc
11:end if
12:end procedure
Aeqitas employs three different strategies, namely Aeqitas
random,Aeqitassemi-directedandAeqitasfully-directed,to
update the probabilities in σprandσv. This is to direct the test
generation process with a focus on discovering discriminatory
inputs. In the following, we will outline the different strategies
implemented within Aeqitas.
Aequitas random .Aeqitas random does not update the initial
probabilitiesassignedto σprandσv.Thisresultsin δ(i.e.perturba-
tion value) and p(i.e. the parameter to perturb) both being chosen
randomly. Intuitively, Aeqitas random explores inputs around
the neighbourhood of disc_inputs(i.e. set of discriminatory inputs
discoveredviaglobalsearch)uniformlyatrandom.Nevertheless,
Aeqitasrandomempiricallyoutperformsapurelyrandomsearch
over the input space. This is because it still performs a random
searchinaconstrainedinputregion–specifically,theinputregion
that already contains discriminatory inputs.Aequitas semi-directed
.Aeqitas semi-directed drives the test
generation bysystematically updating σv,i.e., theprobabilities to
perturbthevalueofaninputparameterby δ=−1(cf.Algorithm 3).
Theparameter p,toperturb,isstillchosenrandomly.Initially,we
chooseδ∈{ −1,+1}where the probability that δ=−1i sσv[p]
and the probability that δ=+1i s1−σv[p]. If the perturbed input
isdiscriminatory(cf.Definition 1),thenweincreasetheprobability
associated with σv[p]by a pre-determined offset Δv. Otherwise,
σv[p]isreducedbythesameoffset Δv.Intuitively,theupdatesto
probabilities in σvprioritise a direction δ∈{ −1,+1}when the
respective direction results in discriminatory inputs.
Aequitas fully-directed .Aeqitas fully-directed extends Ae-
qitas semi-directed by systematically updating the probabilities
to choose a parameter for perturbation. To this end, we updateprobabilities in σprduring the test generation process (cf. Algo-
rithm4). Assume we pick a parameter p∈P\Pdiscto perturb.
Initially, we have σpr[p]=1
|P\Pdisc|. If the perturbation of the
givenparameter pbyδresultsinadiscriminatoryinput,thenwe
add a pre-determined offset Δprtoσpr[p]. To reflect this change
inprobability,we normalize σpr[p/prime]toσpr[p/prime]/summationtext.1
x∈P\Pdiscσpr[x]forevery
p/prime∈P\Pdisc. Intuitively, the updates to probabilities in σprprior-
itize a parameter when perturbing the respective parameter results
in discriminatory inputs.
4.3 Estimation using LLN
Anattractivefeatureof Aeqitasisthatwecanestimatetheper-
centage of discriminatory inputs in Ifor any given model. We
leverage the Law of Large Numbers (LLN) from probability theory
to accomplish this. Let Λbe an experiment. In this experiment, we
generateminputsuniformlyatrandom.Theseareindependentand
identically distributed (IID) samples I1,I2...Im. We execute these
inputsandcountthenumberofinputsthatarediscriminatoryin
nature.Let m/primebethenumberofinputsthatarediscriminatory. Λ
then outputs the percentage m=m/prime×100
m.
Λis conducted Ktimes.In eachinstance ofthe experiment,we
collecttheoutcome m1,m2...mK.LetM=K−1/summationtext.1K
i=1mi.Accord-
ing to LLN, the average of the results, i.e. M, obtained from a large
number of trials, should be close to the expected value, and it will
tendtobecomecloserasmoretrialsareperformed.Thisimpliesas,
K→∞
M→M∗
whereM∗isthetruepercentageofthediscriminatoryinputspresent
inIfor the machine-learning model under test. This phenomenon
was observed in our experiments. Figure 2(b) shows that the M
converges only after 400 trials (i.e. K=400).
4.4 Improving Model Fairness
It has been observed that generated test inputs showing the vi-
olation of desired-properties in machine-learning models can be
leveraged for improving the respective properties. This was accom-plishedviaaugmentingthetrainingdatasetwiththegeneratedtest
inputs and retraining the model [17].
Hence, we intend to evaluate the usefulness of our generated
testinputstoimprovethemodelfairnessviaretraining.Tothisend,
Aeqitas has a completely automated module that guarantees re-
ductionofthe percentageofdiscriminatoryinputs in I.Weachieve
this by systematically adding portions of generated discriminatory
inputs to the training dataset.
AssumeTestbe the set of discriminatory inputs generated by
Aeqitas.Aeqitasiseffectiveingeneratingdiscriminatoryinputs
and the size of the set Testis usually large. A naive approach to
retrainthemodelwillbetoaddallgenerateddiscriminatoryinputs
tothetrainingdataset.Suchanapproachislikelytofailtoimprove
the fairness of the model. This is because the generated test inputs
are targeted towards finding discrimination and are unlikely to
follow the true distribution of the training data. Therefore, blindly
addingallthetestinputstothetrainingsetwillbiasitsdistribution
towards the distribution of our generated test inputs. To solve this
103
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:26:09 UTC from IEEE Xplore.  Restrictions apply. Automated Directed Fairness Testing ASE ’18, September 3–7, 2018, Montpellier, France
Algorithm 5 Retraining
1:procedure Retraining( f,Test,training_data)
2:N←∞
3:fcur←f
4:foriin (2, N)do
5: pi←a random real number between (2i−2,2i−1)
6: ifpi>100then
7: Exit the loop
8: end if
9: k←len(training _data)
10: naddn←pi·k
100
11: TDaddn←randomly selected naddninputs from Test
12: TDnew←training_data∪TDaddn
13: fnew←model trained using TDnew
14: ⊿Estimate the number of discriminatory inputs (section 4.3)
15: faircur←LLN_Fairness_Estimation ( fcur)
16: fairnew←LLN_Fairness_Estimation ( fnew)
17: if(faircur>fairnew)then
18: fcur←fnew
19: else
20: Exit the loop
21: end if
22:end for
23:returnfcur
24:end procedure
challenge,itisimportantthatonlyportionsofdiscriminatoryinputs
fromTestare added to the training dataset.
Letpibe the percentage, with respect to the size of the training
data, that we choose at any given iteration i. If size of training
dataisM,thenweselectpi·M
100discriminatoryinputsfrom Testat
randomandaddthesediscriminatoryinputstothetrainingdataset.
Fori∈[2,N],wesetpirandomlyinarangebetween [2i−2,2i−1].
Theintuitionbehindthisistofindanefficientmechanismtosys-
tematically add inputs from Testto the training dataset and to
approximate the optimal reduction in discriminatory inputs. We
terminatetheprocesswhenaddinginputsfrom Testtothetraining
datasetdoesnot decreasetheestimatedfraction ofdiscriminatory
inputsin I.Thecurrentlytrainedmodel(i.e. fcurinAlgorithm 5)is
then taken as the improved model with better (individual) fairness
score. In this way, we can guarantee that our retraining process
always terminates with a reduction in discriminatory inputs.
Ourretraining strategyis designedto befast withoutsacrificing
the fairness significantly. Our main objective is to demonstrate
that Aeqitas generated test inputs can indeed be used by the
developers to improve the individual fairness of their models. The
amount of added test inputs (generated by Aeqitas) is chosen
fromexponentiallyincreasingintervals(i.e.theinterval [2i−2,2i−1]
inAlgorithm 5).Suchastrategyistakentoquicklyscopethesensi-
tivityofthemodelwithrespecttothegeneratedtestdata.Moreover,bychoosingarandomnumber
piintheinterval,wetrynottoover-
shoot the value of piby a large margin that causes the optimal
reductionofdiscriminatoryinputsin I.Asaresult,ourproposed
retraining strategy maintains a balance between improving model
fairness and the efficiency of retraining.
It is well known that adding more data to a machine-learning
algorithm is likely to lead to increased accuracy [ 8]. A relevantchallengehereisattributedtothelabelingofthegeneratedtestdata.
There exists a number of effective strategies to tackle this problem.
One such strategy is finding the label via a simple majority of anumberofclassifiers[
10].Majorityvotinghasbeenshowntobe
veryeffectiveforawiderangeofproblems[ 16]andwebelieveit
shouldbereadilyapplicableinourcontextofimprovingfairness
aswell.Nevertheless,testdatalabelingisanorthogonalproblem
inthedomainofmachinelearningandweconsiderittobebeyond
the scope of the problem targeted by Aeqitas.
4.5 Termination
Aeqitascanbeconfiguredtohavevariousterminationconditions
dependingontheparticularusecaseofthedeveloper.Inparticular,
Aeqitascanbeterminatedwiththefollowingpossibleconditions:
(1)Aeqitascanterminateafterithasgeneratedauserspec-
ified number of discriminatory inputs from I. This feature
canbeusedwhenacertainnumberofdiscriminatoryinputs
need to be generated for testing, evaluation or retraining of
the model.
(2)Aeqitascanalsoterminatewithinagiventimebound.Thisisusefultoquicklycheckifthemodelexhibitsdiscrimination
for a particular set of sensitive parameters.
Inourevaluation,weusedboththeterminationcriteriatoevaluate
the effectiveness and efficiency of Aeqitas.
5 RESULTS
Experimentalsetup .WeevaluateAeqitasacrossawidevariety
ofclassifiers,includingaclassifierwhichwasdesignedtobefair.Some salient features of these classifiers are outlined in Table 2.
In particular, Fair SVM (cf. Table 2) was specifically designed with
fairnessinmind[ 19].Therestoftheclassifiersundertestarethe
standard implementations found in Python’s Scikit-learn machine
learning library. These classifiers are used in a wide variety of
applications by machine-learning engineers across the world.
OtherthanFairSVM[ 19],wehaveusedScikit-learn’sSupport
VectorMachines(SVM),Multi LayerPerceptron(MLPC),Random
ForestandDecisionTreeimplementationsforourexperiments.We
alsoevaluateanEnsembleVotingClassifier(Ensemble),inwhich
wetakethecombinationoftwoclassifierpredictions.TheclassifiersweuseareRandomForestandDecisionTreeestimators(cf. Table2).
Table 2: Subject classifiers used to evaluate Aeqitas
Classifier name Lines of python code Input domain
Fair SVM [19] 913
106SVM 1123
MLPC 1308
Random Forest 1951
Decision Tree 1465
Ensemble 3466
Allclassifierslistedin Table2areusedforpredictingtheincome.
These classifiers are trained with the data obtained from the US
census[1].Thesizeofthistrainingdatasetisaround32,000.We
train all the six classifiers on this training data. The objective is
to classify whether the income of an individual is above $50,000
(capturedviaclassifieroutput“+1")orbelow(capturedviaclassifier
output“-1").Foralltheclassifiers,setofdiscriminatoryparameters,
104
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:26:09 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Sakshi Udeshi, Pryanshu Arora, and Sudipta Chattopadhyay
Figure 5: The effectiveness of Aeqitas
i.e.Pdiscis thegenderof an individual. The threshold value for
identifying a discriminatory input is set to zero. This means, if I
differsfrom I/primeonlyinbeing GenderAorGenderB,thenIorI/primeare
discriminatoryinputsofaclassifier fwhen|f(I)−f(I/prime)|≥0.In
our experiments we set the perturbation δ∈{ −1,+1}and both
ΔvandΔpras 0.001. These are user defined variables that guide
ourAeqitasapproach.Inparticular,thesevariablesareusedto
systematicallyrefinetheprobabilitiestochooseaninputparameter
to perturb and to choose a perturbation value δ(cf.Section 4).
We implement Aeqitas in Python, as it is a popular choice
oflanguageforthedevelopmentofmachine-learningmodelsand
related applications. The implementation is around 600 lines of
pythoncode.AllourexperimentswereperformedonanIntel i7
processor having 64GB of RAM and running Ubuntu 16.04.
Keyresults .Weusethreedifferenttestgenerationmethodologies,
namely Aeqitas random, Aeqitas semi-directed and Aeqitas
fully-directed. These methodologies differ with respect to the in-
creasing levels of sophistication in systematically searching the
input space (cf. Section 4.2). In particular, Aeqitas fully-directed
involves the highest level of sophistication in searching the in-
putspace.Asexpected,Aeqitasfully-directedconsistentlyout-
performs the Aeqitas random and Aeqitas semi-directed, as
observed from Figure 5. However, Aeqitas fully-directed and Ae-
qitassemi-directeddemandmorecomputationalresourcesper
unit time than Aeqitas random. As a result, Aeqitas random is
moreappropriatetouse,ascomparedtotherestofourapproaches,fortesting withlimited computationalresourcesperunit time.The
test subject used in Figure 5was the Fair SVM (cf. Table 2).
ToillustratethepowerofourAeqitasapproachoverthestate-
of-the-art fairness testing [ 5], we also compare our approaches
withthestate-of-the-art,which,inturniscapturedvia“Random"
inFigure 5. It is evident that even the least powerful technique
implementedwithinourAeqitasapproach(i.e.Aeqitasrandom)significantlyoutperformsthestate-of-the-art.Inourevaluation,we
discoveredthatAeqitasismoreeffectivethanthestate-of-the-art
randomtestingbyafactorof9.6onaverageanduptoafactorof20.4.
We measured the effectiveness via the number of discriminatory
inputs generated by a test generation technique. Aeqitas also
providescapabilitiestoautomaticallyretrainamachine-learning
modelwiththeobjectivetoreducethenumberofdiscriminatory
inputs.Tothisend,Aeqitasreducedthenumberofdiscriminatory
inputs by 43.2% on average with a maximum reduction of 94.36%.RQ1: How effective is Aeqitas in finding discrimi-natory inputs?
Weevaluatethecapabilityof Aeqitasineffectivelygenerating
discriminatory inputs. For all the subject classifiers, we measure
the effectiveness of our test algorithms via the number of discrimi-
natory inputs generated with respect to the number of total inputs
generated.
Apurelyrandomapproachisnoteffectiveingeneratingdiscrim-
inatory inputs.As observedfrom Figure5, thenumber ofdiscrim-
inatory inputs generated by such an approach does not increase
rapidlyoverthenumberofinputsgenerated.Thisisexpected,as
a purely random approach does not incorporate any systematic
strategytodiscoverinputsviolatingfairness.Theineffectivenessofrandomtestingpersistsacrossallthesubjectclassifiers,asobserved
inTable 3.
As observed from Table 3, all test generation approaches imple-
mentedwithinAeqitasoutperformapurelyrandomapproach.
Inparticular,therateatwhichourAeqitasapproachgenerates
discriminatory inputs is significantly higher than a purely random
approach. As a result, Aeqitas provides scalable and effective
techniquefor machinelearningengineers whoaimto rapidlydis-
cover fairness issues in their models. Aeqitas random, Aeqitas
semi-directed and Aeqitas fully-directed involve increasing level
of sophistication in directing the test input generation. As a result,
Aeqitas fully-directed approach performs the best among all our
testgenerators.Inparticular,Aeqitassemi-directedisonanav-
erage 46.7% and up to 64.9% better than Aeqitas random. Finally,
Aeqitas full-directed is on an average 29.5% and up to 56.56%
better than Aeqitas semi-directed.
By design, Aeqitas does not generate any false positives. This
meansthatanydiscriminatoryinputgeneratedbyAeqitasare
indeeddiscriminatorytothemodelundertest,subjecttothechosen
threshold of discrimination.
Finding: Aeqitas fully-directed approach outperform
apurelyrandomapproachuptoafactorof20.4interms
ofthenumberofdiscriminatoryinputsgenerated.Italso
performs up to 56.7% better than Aeqitas semi-directed,
which, in turn performs up to 64.9% better than Aeqitas
random, our least sophisticated approach.
RQ2: How efficient is Aeqitas in finding discrimi-
natory inputs?
Table 4summarizes how muchtime eachof themethods takes
to generate 10,000 discriminatory inputs. On an average Aeqitas
random performs 64.42% faster than the state of the art. The im-
provement in Aeqitas fully-directed is even more profound. On
an average, Aeqitas fully-directed is 83.27% faster than the state
oftheart,withamaximumimprovementof96.62%inthecaseof
Multi Layer Perceptron.
It is important to note that the reported time in Table 4includes
both the time needed for test generation and for test execution.
105
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:26:09 UTC from IEEE Xplore.  Restrictions apply. Automated Directed Fairness Testing ASE ’18, September 3–7, 2018, Montpellier, France
Table 3: Effectiveness of Aeqitas approach
Classifier Random [5]Aeqitas
randomAeqitas
semi-directedAeqitas
fully-directed
% discriminatory
input% discriminatory
input# inputs
generated% discriminatory
input# inputs
generated% discriminatory
input# inputs
generated
Fair SVM 3.45 39.4 315640 65.2 322725 70.32 357375
SVM 0.18 0.53 54683 0.574 88095 1.22 100101
MLPC 0.3466 2.15 218727 2.39 129556 2.896 141666
Random Forest 8.34 18.312 218727 21.722 264523 34.98 282973
Decision Tree 0.485 2.33 153166 2.89 179364 6.653 248229
Ensemble 8.23 22.34 187980 36.08 458910 37.9 545375
Table 4: Test generation efficiency
Classifier RandomAeqitas
randomAeqitas
semi-directedAeqitas
fully-directed
Fair SVM 1589.87s 534.47s 345.65s 228.14s
SVM 7159.54s 3589.9s 2673.8s 2190.21s
MLPC 6157.23s 759.63s 431.76s 207.87s
Random Forest 9563.12s 2692.98s 1334.67s 1145.34s
Decision Tree 1035.32s 569.13s 371.89s 254.25s
Ensemble 6368.79s 2178.45s 1067.75s 989.43s
Hence,thereportedtimeishighlydependentontheexecutiontime
of the model under test.
Finding: Aeqitasfully-directedis83.27%fasterthanthe
stateoftheart,withamaximumimprovementof96.62%
in the case of Multi Layer Perceptron.
RQ3:Howusefularethegeneratedtestinputstoim-
prove the fairness of the model?
Table 5: Retraining Effectiveness
Classifierestimated % of disc input
inI(95% confidence interval)%Impr%Inps
added
before
retrainingafter
retraining
Fair SVM 3.86 (3.76, 3.95) 2.89 (2.64, 3.14) 25.15 15.6
SVM 0.33 (0.14, 0.51) 0.12 (0.09, 0.14) 63.54 26.9
MLPC 0.39 (0.36, 0.42) 0.28 (0.27, 0.29) 30.12 23.7
Random Forest 8.84 (8.78, 8.91) 6.68 (6.35, 7.01) 24.48 32.4
Decision Tree 0.48 (0.45, 0.51) 0.027 (0.026, 0.028) 94.36 10.6
Ensemble 7.73 (7.14, 8.32) 6.06 (5.64, 6.48) 21.58 28.3
Aeqitas has a completely automated module which guaran-
tees a decrease in the percentage of discriminatory inputs in I. The
discriminatory inputs, as discovered by Aeqitas, were systemati-
callyaddedtothetrainingdataset(cf. Section4.4).Theresultsof
retraining the classifiers appear in Table 5. In general, retraining
the classifiers is not significantly time consuming. In particular,
eachclassifierwasretrainedwithinanhour.Forsomeclassifiers,
such as the SVM, our retraining scheme only took a few minutes.
We leverage the law of large numbers (LLN) from statistical
theory to estimatethe percentage ofdiscriminatory inputs in I(cf.
Section 4.3). In particular, we randomly sample a large numberof inputs from
Iand compute the ratio of discriminatory inputs
to the total inputs sampled. This experiment is repeated a large
numberoftimesandtheaverageofthecomputedratioisusedas
theestimateforthepercentage ofdiscriminatoryinputs in I.W e
note from statistical theory that as the number of experiment is
repeatedalargenumberoftimes,theaverageofthecomputedratio
shouldbeclosetotheexpectedfractionofdiscriminatoryinputs
inI. We also compute the 95% confidence interval estimate for thepercentage of discriminatory inputs in I. It is useful to note that
theseintervalsarefairlytightandthataddstotheconfidencewe
have in our point estimates as well.
As observed from Table 5, Aeqitas is effective in reducing the
percentageofdiscriminatoryinputsin Iforalltheclassifiersunder
test. Specifically, we observe an average improvement of 43.2%, in
terms of reducing the discriminatory inputs.Using our retraining
module, we added an average of only 7463 datapoints (22.92% of
the original training data) to achieve the result obtained in Table 5.
Finding: RetrainingusingAeqitaslowersthediscrim-
inationpercentagein Ibyanaverageof43.2%andupto
94.36%.
6 RELATED WORK
In this section, we review the related literature and position our
work on fairness testing.
Fair MachineLearning Models Themachinelearningresearch
communityhaveturnedtheirattentionondesigningclassifiersthat
avoid discrimination [ 2,4,6,9,19]. These works primarily focus
onthetheoreticalaspectsofclassifiermodelstoachievefairness
in theclassification process. Sucha goal iseither achieved bypre-
processing training data or by modifying existing classifiers to
limitdiscrimination.Ourworkiscomplementarytotheapproaches
that aim to design fair machine-learning models. We introduce an
efficientwaytosearchtheinputdomainofclassifierswhosegoal
is to achieve fairness in decision making. We wish to provide a
mechanismfortheseclassifierstoquicklyevaluatetheirfairness
propertiesandhelpimprovetheirfairnessindecisionmakingvia
retraining, if necessary.
Fairness Testing From the software engineering point of view,
the research on validating the fairness of machine-learning models
isstillatitsinfancy.Arecentwork[ 5]alongthislineofresearch
defines software fairness and discrimination, including a causality-
based approach to algorithmic fairness. However, in contrast to
our Aeqitas approach, the focus of this work is more on defining
fairness and tests were generated in random [5]. In particular, Ae-
qitascanbeusedasadirectedtestgenerationmoduletouncover
discriminatoryinputs anddiscoveryof theseinputsis essentialto
understand individual fairness [ 2] of a machine-learning model. In
additiontothisandunlikeexistingapproach[ 5],Aeqitasprovides
amoduletoautomaticallyretrainthemachine-learningmodelsand
reduce discrimination in the decisions made by these models.
106
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:26:09 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Sakshi Udeshi, Pryanshu Arora, and Sudipta Chattopadhyay
Testing and Verification of Machine Learning models Deep-
Xplore [16] is a whitebox differential testing algorithm for system-
atically finding inputs that can trigger inconsistencies between
multiple deep neural networks (DNNs). The neuron coverage was
usedasasystematicmetricformeasuringhowmuchoftheinternal
logic of a DNNs have been tested. More recently, DeepTest [ 17]
leveragesmetamorphicrelationstoidentifyerroneousbehaviorsin
aDNN.Theusageofmetamorphicrelationssomewhatsolvesthe
limitationof differential testing,especiallyto liftthe requirement
of having multiple DNNs implementing the same functionality. Fi-
nally, a feature-guided black-box approach is proposed recently
to validate the safety of deep neural networks [ 18]. This work
uses their method to evaluate the robustness of neural networks in
safety-critical applications such as traffic sign recognition.
Theobjectiveoftheseworks,asexplainedintheprecedingpara-
graph, is largely to evaluate the robustness property of a given
machine-learning model. In contrast, we are interested in the fair-
ness property, which is fundamentally different from robustness.
Therefore,validatingfairnessrequiresspecialattentionalongthe
line of systematic test generation.
Searchbased testing Search-basedtestinghasalongandvaried
history. The most common techniques are hill climbing, simulated
annealing and genetic algorithms [ 11]. These have been applied
extensivelytotestapplicationsthatlargelyfallintheclassofde-
terministic software systems. Aeqitas is the first instance in our
knowledgethatemploysanovelsearchalgorithmtotestthefair-
ness of machine-learning systems. We believe that we can port
Aeqitas for the usage in a much wider machine-learning context.
7 THREATS TO VALIDITY
The effectiveness and efficiency of Aeqitas critically depends on
the following factors:Robustness
:OurAeqitasapproachisbasedonthehypothesis
that the machine-learning models under test exhibit robustness.
Thisisareasonableassumption,asweexpectthemodelsundertest
to be deployed in production settings. As evidenced by our evalua-
tion, Aeqitas approach, which is based on the aforementioned
hypothesis, was effective to localize the search in the vicinity of
discriminatory input regions for state-of-the-art models.
Trainingdataandaccesstomodel :Aeqitasneedsaccesstothe
training data and the training mechanism of the machine-learning
modeltobeabletoevaluateandretrainthemodel.Withoutaccesstothetrainingdata,Aeqitaswillnotbeabletosuccessfullyimprove
the fairness of the model. This is because Aeqitas is used to
generate test inputs that violate fairness and augment the original
training set to improve the model under test. The generated test
inputs,however,isnotsufficienttotrainamachine-learningmodel
from scratch.
InputStructure :Aeqitasworksonreal-valuedinputs.Aeqitas,
initscurrentform,doesnothandleimage,soundorvideoinputs.
This, however, does not diminish the applicability of Aeqitas.
Numerousreal-worldapplicationsstilluseonlyreal-valueddatafor
prediction. These include applications in finance, security, social
welfare,education, healthcareandhuman resources.Examples of
applicationsincludeincomeprediction,crimeprediction,disease
prediction,jobshort-listingandcollegeshort-listing,amongothers.For models that take inputs such as images and videos, we need
toincorporateadditionaltechniquesforautomaticallygenerating
valid input data. However, we believe that the core idea behind
our Aeqitas approach, namely the global and the local search
employed over the input space, will still remain valid.
Probabilitychangeparameter :Theusersof Aeqitaswillhave
to experiment and carefully choose ΔvandΔprvalues which
change the probabilities of choosing p(i.e. the input parameter
toperturb)and δ(i.e.theperturbationvalue).If Δv(respectively,
Δpr)istoohigh,thenanovershootmightoccurandacertaindis-
criminatoryinputregionmayneverbeexplored.If Δv(respectively,
Δpr)istoolow,thentheeffectivenessof Aeqitassemi-directed
and Aeqitas fully-directed would be very similar to Aeqitas
random. In our experiments, we evaluated with a few ΔvandΔpr
values before our results stabilized.
Limited discriminatory input features : We evaluate Aeqitas
with discriminatory input feature gender. Hence, we cannot con-
clude the effectiveness of Aeqitas for other potentially discrimi-
natory input features. However, the mechanism behind Aeqitas
is generic and allows extensive evaluation for other discriminatory
input features in a future extension of the tool.
8 CONCLUSION
Inthispaper,weproposeAeqitas–afullyautomatedanddirected
test generation strategyto rapidlygenerate discriminatoryinputs
in machine-learning models. The key insight behind Aeqitas
is to exploit the robustness property of common machine learn-
ingmodelsanduseittosystematicallydirectthetestgeneration
process.Aeqitasprovidesstatisticalevidenceonthenumberof
discriminatoryinputsinamodelundertest.Moreover,Aeqitas
incorporatesstrategiestosystematicallyleveragethegeneratedtestinputs to improve the fairness of the model. We evaluate Aeqitas
withstate-of-the-artclassifiersanddemonstratethatAeqitasis
effective in generating discriminatory test inputs as well as im-proving the fairness of machine-learning models. At its current
state,however,Aeqitasdoesnothavethecapabilitytolocalize
the cause of discrimination in a model. Further work is required to
isolate the cause of discrimination in the model.
Aeqitas provides capabilities to lift the state-of-the-art in test-
ingmachine-learningmodels.WeenvisiontoextendourAeqitasapproach beyond fairness testing and for machine-learning models
taking complex inputs including images and videos. We hope that
thecentralideabehindourAeqitasapproachwouldinfluencetherigoroussoftwareengineeringprinciplesandhelpvalidatemachine-learningapplicationsusedinsensitivedomains.Forreproducibility
and advancing the state of research, we have made our tool and all
experimental data publicly available:
https://github.com/sakshiudeshi/Aequitas
ACKNOWLEDGMENT
TheauthorswouldliketothankChundongWangandtheanony-
mousreviewersfortheirinsightfulcomments.Thefirstauthoris
supported by the President’s Graduate Fellowship funded by the
Ministry of Education, Singapore.
107
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:26:09 UTC from IEEE Xplore.  Restrictions apply. Automated Directed Fairness Testing ASE ’18, September 3–7, 2018, Montpellier, France
REFERENCES
[1]DuaDheeruandEfiKarraTaniskidou. UCImachinelearningrepository,2017.
URL:http://archive.ics.uci.edu/ml.
[2]CynthiaDwork,MoritzHardt,ToniannPitassi,OmerReingold,andRichardS.
Zemel. Fairness through awareness. In Innovations in Theoretical Computer
Science 2012, Cambridge, MA, USA, January 8-10, 2012, pages 214–226, 2012.
[3]Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Analysis of classifiers’ ro-
bustness to adversarial perturbations. Machine Learning, 107(3):481–508, Mar
2018.
[4]Michael Feldman, Sorelle A. Friedler, John Moeller, Carlos Scheidegger, andSuresh Venkatasubramanian. Certifying and removing disparate impact. InProceedings of the 21th ACM SIGKDD International Conference on KnowledgeDiscovery and Data Mining, Sydney, NSW, Australia, August 10-13, 2015, pages
259–268, 2015.
[5]SainyamGalhotra,YuriyBrun,andAlexandraMeliou. Fairnesstesting:testing
softwarefordiscrimination. In Proceedingsofthe201711thJointMeetingonFoun-
dationsof SoftwareEngineering,ESEC/FSE 2017,Paderborn,Germany,September
4-8, 2017, pages 498–510, 2017.
[6]Gabriel Goh, Andrew Cotter, Maya R. Gupta, and Michael P. Friedlander. Satisfy-
ing real-world goals with dataset constraints. In Advances in Neural Information
ProcessingSystems29:AnnualConferenceonNeuralInformationProcessingSystems
2016, December 5-10, 2016, Barcelona, Spain, pages 2415–2423, 2016.
[7]Kathrin Grosse, Nicolas Papernot, Praveen Manoharan, Michael Backes, and
Patrick D. McDaniel. Adversarial examples for malware detection. In Computer
Security - ESORICS 2017 - 22nd European Symposium on Research in Computer
Security,Oslo,Norway,September11-15,2017,Proceedings,PartII,pages62–79,
2017.
[8]Alon Y. Halevy, Peter Norvig, and Fernando Pereira. The unreasonable effective-
ness of data. IEEE Intelligent Systems, 24(2):8–12, 2009.
[9]Toshihiro Kamishima,Shotaro Akaho,Hideki Asoh,and JunSakuma. Fairness-
aware classifier with prejudice remover regularizer. In Machine Learning and
Knowledge Discovery in Databases - European Conference, ECML PKDD 2012,
Bristol, UK, September 24-28, 2012. Proceedings, Part II, pages 35–50, 2012.
[10]Louisa Lam and Ching Y. Suen. Application of majority voting to pattern recog-
nition:ananalysisofitsbehaviorandperformance. IEEETrans.Systems,Man,and Cybernetics, Part A, 27(5):553–568, 1997.
[11]Phil McMinn. Search-based software test data generation: a survey. Softw. Test.,
Verif. Reliab., 14(2):105–156, 2004.
[12]University of Michigan. Nondiscrimination policy notice. URL: https://hr.
umich.edu/working-u-m/workplace-improvement/office-institutional-equity/
nondiscrimination-policy-notice/.
[13]Nicolas Papernot, Patrick D. McDaniel, Ian J. Goodfellow, Somesh Jha, Z. Berkay
Celik, and Ananthram Swami. Practical black-box attacks against machinelearning. In Proceedings of the 2017 ACM on Asia Conference on Computer and
Communications Security, AsiaCCS 2017, Abu Dhabi, United Arab Emirates, April
2-6, 2017, pages 506–519, 2017.
[14]NicolasPapernot,PatrickD.McDaniel,AnanthramSwami,andRichardE.Harang.
Crafting adversarial input sequences for recurrent neural networks. In 2016
IEEEMilitaryCommunicationsConference,MILCOM2016,Baltimore,MD,USA,
November 1-3, 2016, pages 49–54, 2016.
[15]Dino Pedreshi, Salvatore Ruggieri, and Franco Turini. Discrimination-aware
data mining. In Proceedings of the 14th ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 560–568, 2008.
[16]KexinPei,YinzhiCao,JunfengYang,andSumanJana. Deepxplore:Automated
whitebox testing of deep learning systems. In Proceedings of the 26th Symposium
onOperatingSystemsPrinciples,Shanghai,China,October28-31,2017,pages1–18,
2017.
[17]Yuchi Tian, Kexin Pei, Suman Jana, and Baishakhi Ray. Deeptest: Automated
testing of deep-neural-network-driven autonomous cars. CoRR, abs/1708.08559,
2017. URL: http://arxiv.org/abs/1708.08559, arXiv:1708.08559.
[18]Matthew Wicker, Xiaowei Huang, and Marta Kwiatkowska. Feature-guidedblack-box safety testing of deep neural networks. In Tools and Algorithms for
the Construction and Analysis of Systems -24th International Conference, TACAS
2018, Held as Part of the European Joint Conferences on Theory and Practice of
Software,ETAPS2018,Thessaloniki,Greece,April14-20,2018,Proceedings,PartI,
pages 408–426, 2018.
[19]Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez-Rodriguez, and Krishna P.
Gummadi. Fairnessconstraints:Mechanismsforfairclassification. In Proceedings
ofthe20thInternationalConferenceonArtificialIntelligenceandStatistics,AISTATS
2017, 20-22 April 2017, Fort Lauderdale, FL, USA, pages 962–970, 2017.
108
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:26:09 UTC from IEEE Xplore.  Restrictions apply. 