Local-Based Active ClassiÔ¨Åcation of Test Report to Assist
Crowdsourced Testing
Junjie Wang1;3, Song Wang4, Qiang Cui1;3, Qing Wang1;2;3
1Laboratory for Internet Software Technologies,2State Key Laboratory of Computer Science,
Institute of Software Chinese Academy of Sciences, Beijing, China
3University of Chinese Academy of Sciences, Beijing, China
4Electrical and Computer Engineering, University of Waterloo, Canada
{wangjunjie, cuiqiang, wq}@itechs.iscas.ac.cn, song.wang@uwaterloo.ca
ABSTRACT
In crowdsourced testing, an important task is to identify the
test reports that actually reveal fault { true fault , from
the large number of test reports submitted by crowd work-
ers. Most existing approaches towards this problem utilized
supervised machine learning techniques, which often require
users to manually label a large amount of training data.
Such process is time-consuming and labor-intensive. Thus,
reducing the onerous burden of manual labeling while still
being able to achieve good performance is crucial. Active
learning is one potential technique to address this challenge,
which aims at training a good classier with as few labeled
data as possible. Nevertheless, our observation on real in-
dustrial data reveals that existing active learning approaches
generate poor and unstable performances on crowdsourced
testing data. We analyze the deep reason and nd that the
dataset has signicant local biases.
To address the above problems, we propose LOcal-based
Active Classi Fication (LOAF) to classify true fault from
crowdsourced test reports. LOAF recommends a small por-
tion of instances which are most informative within local
neighborhood, and asks user their labels, then learns classi-
ers based on local neighborhood. Our evaluation on 14,609
test reports of 34 commercial projects from one of the Chi-
nese largest crowdsourced testing platforms shows that our
proposed LOAF can generate promising results. In addition,
its performance is even better than existing supervised learn-
ing approaches which built on large amounts of labelled his-
torical data. Moreover, we also implement our approach and
evaluate its usefulness using real-world case studies. The
feedbacks from testers demonstrate its practical value.
CCS Concepts
Software and its engineering !Software testing
and debugging;
Corresponding author.Keywords
Crowdsourced Testing, Test Report Classication, Active
Learning
1. INTRODUCTION
Crowdsourced testing is an emerging trend in both the
software engineering community and industrial practice. In
crowdsourced testing, crowd workers are required to sub-
mit test reports after performing testing tasks in crowd-
sourced platform [1]. A typical test report contains de-
scription, screenshots, and an assessment as to whether the
worker believed that the software behaved correctly (i.e.,
passed ), or behaved incorrectly (i.e., failed ). In order to
attract workers, testing tasks are often nancially compen-
sated, especially for these failed reports. Under this contex-
t, workers may submit thousands of test reports. However,
these test reports often have many false positives, i.e., a test
report marked as failed that actually involves correct behav-
ior or behavior that was considered outside of the studied
software system. Project managers or testers need to man-
ually verify whether a submitted test report reveals fault {
true fault (as demonstrated in Table 1).
However, such process is tedious and cumbersome, given
the high volume workload. Our observation on one of the
Chinese largest crowdsourced testing platforms shows that,
approximately 100 projects are delivered per month from
this platform, and more than 1,000 test reports are submit-
ted per day on average. Inspecting 1,000 reports usually
takes almost half a working week for a tester. Thus, auto-
matically classifying true fault from the large amounts of
test reports would signicantly facilitate this process.
Several existing researches have been proposed to classify
issue reports of open source projects using supervised
machine learning algorithms [2{5]. Unfortunately, these
approaches often require users to manually label a large
number of training data, which is both time-consuming and
labor-intensive in practice. Therefore, it is crucial to reduce
the onerous burden of manual labeling while still being able
to achieve good performance.
In this paper, we try to adopt active learning to mit-
igate this challenge. Active learning aims to achieve high
accuracy with as few labeled instances as possible. It rec-
ommends a small portion of instances which are most infor-
mative, and asks user their labels. These labeled reports are
then used to train a model to classify the remaining reports.
However, our experiments reveal that existing active learn-
ing techniques generate poor and unstable performances on
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proÔ¨Åt or commercial advantage and that copies bear this notice and the full citation
on the Ô¨Årst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speciÔ¨Åc permission and/or a
fee. Request permissions from Permissions@acm.org.
ASE‚Äô16 , September 3‚Äì7, 2016, Singapore, Singapore
c2016 ACM. 978-1-4503-3845-5/16/09...$15.00
http://dx.doi.org/10.1145/2970276.2970300
190
these crowdsourced testing data (details are in Section 5.2).
We further analyze the deep reason and nd that previous
active learning techniques assumed data are identically dis-
tributed [6]. However, crowdsourced test reports often have
signicant local bias, i.e., the contained technical terms may
be shared by only a subset of reports rather than all the re-
ports (details are in Section 2.3).
To address the local bias and more eectively classify the
crowdsourced reports, we proposed LOcal-based Active Clas-
siFication approach (LOAF). The idea behind is to query the
most informative instance within local neighborhood, then
learn classiers based on local neighborhood (details are in
Section 3.2).
We experimentally investigate the eectiveness and ad-
vantages of LOAF on 14,609 test reports of 34 commercial
projects from one of the Chinese largest crowdsourced test-
ing platforms. Results show that LOAF can achieve 1.00
accuracy with the eort of labeling 10% reports on median.
It outperforms both existing active learning techniques and
supervised learning techniques which built on large amounts
of historical labeled data. In addition, we implement our
approach1, conduct a case study and a survey, to further
evaluate the usefulness of LOAF. Feedback shows that 80%
testers agree with the usefulness of LOAF and would like to
use it in real practice of crowdsourced testing.
These results imply that when historical labeled data are
not available, our approach can still facilitate the report
classication with high accuracy and little eort. This can
reduce the eort for manually inspecting the reports, or
collecting large number of high-quality labeled data.
This paper makes the following contributions:
We propose LOcal-based Active Classi Fication (LOAF)
to address the two challenges in automating crowd-
sourced test reports classication, i.e., local bias
problem and lacking of labeled data. To the best of
our knowledge, this is the rst work to address
these two problems of automating test report
classication in real industrial crowdsourced
testing practice.
We evaluate our approach on 14,609 test reports of 34
projects from one of the Chinese largest crowdsourced
testing platforms, and results are promising.
We implement our approach and evaluate its usefulness
using real-world case studies.
The rest of this paper are organized as follows. Section 2
describes the basic background of this study. Section 3 dis-
cusses the design of our proposed approach. Section 4 shows
the setup of our experimental evaluation. Section 5 presents
the results of our research questions. Section 6 discloses the
threats to the validity of this work. Section 7 surveys related
work. Finally, we summarize this paper in Section 8.
2. BACKGROUND
2.1 Crowdsourced Testing
In this section, we describe the background of crowd-
sourced testing to help better understand the challenges we
meet in real industrial crowdsourced testing practice.
1The implementation of LOAF are available at http://itechs.iscas.
ac.cn/cn/membersHomepage/wangjunjie/wangjunjie/LOAF.zip.
Figure 1: The procedure of crowdsourced testing [1]
Table 1: An example of crowdsourced test report
Attribute Description: example
Environment Phone type: Samsung SN9009
Operating system: Android 4.4.2
ROM information: KOT49H.N9009
Network environment: WIFI
Crowd work-
erId:123456
Location: Beijing Haidian District
Testing task Id:01
Name: Incognito mode
Input and
operation
stepsInput \sina.com.cn" in the browser, then click
the rst news. Select \Setting" and then set
\Incognito Mode". Click the second news in the
website. Select \Setting" and then select \Histo-
ry".
Result de-
scription\Incognito Mode" does not work as expected.
The rst news, which should be recorded, does
not appear in \History".
Screenshot
Assessment Passed or failed given by crowd worker: Failed
Our experiment is conducted with Baidu crowdsourced
testing platform2. The general procedure of such crowd-
sourced testing platform is shown in Figure 1.
In general, testers in Baidu prepare packages for crowd-
sourced testing (software under test and testing tasks) and
distribute them online using their crowdsourced testing plat-
form. Then, crowd workers could sign in to conduct the
tasks and are required to submit crowdsourced test report-
s3. Table 1 demonstrates the attributes of a typical crowd-
sourced report4. The platform can automatically record the
crowd worker's information and environment information on
which the test is carried on. A worker is required to sub-
mit the testing task s/he carried on and descriptions about
the task including input, operation steps, results description
and screenshots. The report is also accompanied with an as-
sessment as to whether the worker believes that the software
behaved correctly (i.e., passed ) or incorrectly (i.e., failed ).
In order to attract more workers, testing tasks are often
nancially compensated. Workers may then submit thou-
sands of test reports due to nancial incentive and other
motivations. Usually, this platform delivers approximately
100 projects per month, and receives more than 1,000 test
reports per day on average. Among those reports, more than
70% are reported as failed . Nevertheless, they have many
2Baidu (baidu.com) is the largest Chinese search service provider.
Its crowdsourcing test platform (test.baidu.com) is also the
largest ones in China.
3We will simplify \crowdsourced test report" as \crowdsourced
report" or \report", potentially avoiding the confusion with \test
set" in machine learning techniques.
4Reports are written in Chinese in our projects. We translate
them into English to facilitate understanding.
191false positives, i.e., a test report marked as failed that ac-
tually involves correct behavior or behavior outside of the
studied software system. This is due to the nancial com-
pensation mechanism, which usually favors failed reports.
Currently in this platform, testers need to manually in-
spect these failed test reports to judge whether they actu-
ally reveal fault { true fault . However, inspecting 1,000
reports manually could take almost half a working week
for a tester. Besides, only less than 50% of them are -
nally determined as true fault. Obviously, such process is
time-consuming, tedious, and low-ecient. In consequence,
this motivates us to eciently automate the classication of
crowdsourced test reports.
2.2 Active Learning
Active learning is a subeld of machine learning. The key
hypothesis is that, if the learning algorithm is allowed to
choose the data from which it learns, it will perform better
with less training data [6].
Consider that, for any supervised learning system to per-
form well, it must often be trained on hundreds (even thou-
sands) of labeled instances. For most of the learning tasks,
labeled instances are very dicult, time-consuming, or ex-
pensive to obtain. Active learning attempts to overcome the
labeling bottleneck by selecting unlabeled instances and ask-
ing user their labels (simplied as ` select a query ', or ` query '
in the following paper). In this way, the active learner aim-
s to achieve high accuracy using as few labeled instances
as possible, thereby reducing the cost of obtaining labeled
data [6].
All active learning techniques involve measuring the in-
formativeness of unlabeled instances. Informativeness rep-
resents the ability of an instance in reducing the uncertainty
of the classication. For example, the chosen instance is the
one which the classier is least certain how to label [7].
2.3 Local Bias
Local bias refers to the phenomenon that data are het-
erogeneous within dataset, and their distributions are often
dierent among dierent parts of the dataset [8]. Crowd-
sourced reports naturally have local biases, where the con-
tained technical terms may be shared by only a subset of
reports rather than all the reports.
The main reason is as follows. Software often has several
technical aspects, e.g., display, location, navigation, etc. In
crowdsourced testing, each report usually focuses on specif-
ic technical aspects of the project, and describes the soft-
ware behavior with specic technical terms. When classi-
fying a particular report, reports which share more techni-
cal terms might be helpful to build classiers, while reports
which share less or none terms might contribute less to the
classication.
We present an illustrative example to illustrate the local
bias of crowdsourced reports and its inuence. We rst ran-
domly select an experimental project (P1, a map app) and
use K-Means [9] to group the reports into four clusters based
on their technical terms (details are in Section 3.1). Each
cluster contains a set of reports that share similar techni-
cal terms, which should be the testing outcomes for same
technical aspect or similar aspects. We then generate the
term cloud for each cluster in Figure 2. The size and color
demonstrate the frequency of technical terms, with larger
and darker ones denoting the terms of higher frequency.
Figure 2: Illustrative example of local bias
We can easily observe that the technical terms exert great
dierences among clusters. The top left cluster might con-
cern with the technical aspect of speech recognition, with
terms like recognition, speech, and voice . The top right clus-
ter contains such terms as location, GPS, and deviation ,
which might relate to the technical aspect of location. Taken
in this sense, models, built on the reports from recognition
cluster, may fail to classify the reports in location cluster.
Note that, we have tried the number of clusters ranging from
2 to 10, all exposing the local bias problem. Here we only
show the results with four clusters for better visualization.
Local bias has been widely investigated in eort estima-
tion and defect prediction studies [10{14]. The most com-
mon technique to overcome the local bias is relevancy l-
tering, e.g., nearest-neighbor similarity [13,14]. To be more
specic, it allows the use of only certain train instances,
which are closer to the test instances, for model construc-
tion. However, there are scarcely any studies focusing on
dealing with the local bias in active learning methods. This
paper borrows the idea of relevancy ltering and designs a
novel approach to mitigate the local bias in active learning.
3. APPROACH
Figure 3 demonstrates the overview of our approach. We
will rst illustrate the feature extraction, followed with the
details of our LOcal-based Active Classi Fication (LOAF).
3.1 Extracting Features
The goal of feature extraction is to obtain features from
crowdsourced reports which can be used as input to train
machine learning classiers. We extract these features from
the text descriptions of crowdsourced reports.
We rst collect dierent sources of text description to-
gether (input and operation steps, result description). Then
we conduct word segmentation , as the crowdsourced re-
ports in our experiment are written in Chinese. We adopt
ICTCLAS5for word segmentation, and segment description-
s into words. We then remove stopwords (i.e., \am", \on",
\the", etc.) to reduce noise. Note that, workers often use
dierent words to express the same concept, so we intro-
duce the synonym replacement technique to mitigate this
problem. Synonym library of LTP6is adopted.
Each of the remaining term corresponds to a feature. For
each feature, we take the frequency it occurs in the descrip-
tion as its value. We use the TF (term frequency) instead of
5ICTCLAS (http://ictclas.nlpir.org/) is widely used Chinese
NLP platform.
6LTP (http://www.ltp-cloud.com/) is considered as one of the
best cloud-based Chinese NLP platforms.
192Figure 3: Overview of our approach
TF-IDF because the use of the inverse document frequency
(IDF) penalizes terms appearing in many reports. In our
work, we are not interested in penalizing such terms (e.g.,
\break",\problem") that actually appear in many reports be-
cause they can act as discriminative features that guide ma-
chine learning techniques in classifying reports. We organize
these features into a feature vector .
3.2 Local-Based Active ClassiÔ¨Åcation (LOAF)
The primary focus of active learning techniques is mea-
suring the informativeness of unlabeled data (details are in
Section 2.2), in order to query the most informative instance
to help build eective classier. The design of existing tech-
niques is based on such assumption that data are identically
distributed [6]. However, crowdsourced reports usually have
signicant local bias (details are in Section 2.3), so existing
techniques would generate poor and unstable performance
on these data (details are in Section 5.2).
To address such problem, we propose the LOcal-based
Active Classi Fication (LOAF). The main idea is to query
the most informative instance within local neighborhood,
then learn classiers based on local neighborhood. Local
neighborhood is dened as one or several nearby (with
smallest distance) labeled instances. To cope with the
local bias in the investigated dataset, the informativeness
in our approach is measured within local neighborhood
{ denoted as ` local informativeness '. In details, we rst
obtain the local neighborhood for each unlabeled instance,
and measure its local informativeness. We then select the
most local informative unlabeled instance and query its
label.
Algorithm 1 and Figure 3 summarize the process of LOAF.
LOAF rst chooses the initial instance and ask user its la-
bel. After that, LOAF would iteratively select one instance,
ask user its label, and learn classiers from the labeled in-
stances. LOAF then leverages the up-to-date labeled infor-
mation to choose which instance to select next. The process
will continue until satisfying one of the termination criteria.
We will illustrate the process in more detail in the following
subsections.
3.2.1 Initializing
This step is to choose the initial test report and ask user
its label. Because of the existence of local bias, randomly
choosing the initial instance in existing active learning tech-
niques cannot work well [6]. Hence, our approach proposes
a new initial selection method to choose the initial test re-
port for labeling. This can mitigate the inuence of initial
instance and obtain stable good performance.
LOAF rst computes the distance between each pair of
report, then obtains the nearest distance for each report.
Second, LOAF selects the report whose nearest distance isAlgorithm 1 Local-based active classication (LOAF)
Input:
Unlabeled report set U; Labeled report set L= null;
Tag for termination TM= true; Parameter prlocal andprstop;
Output:
Classication results C;
1://Initialize
2:Choose the initial report UR iaccording to eq.(1) and query its
label;
3:L=L[UR i,U=U-UR i;
4:while (TM) do
5: //Select a query
6: Select unlabeled report UR ifrom Uaccording to eq.(2) and
query its label;
7: L=L[UR i,U=U-UR i;
8: //Learn classiers
9: foreach unlabeled report UR iinUdo
10: Choose prlocal labeled report LR jaccording to eq.(3),
learn classier and obtain the classication result Cifor
report UR i.
11: end for
12: //Whether to terminate
13: Judge whether can terminate based on prstop, if yes, set TM
as false.
14:end while
the largest among all reports. This is to ensure the most
sparse local region can also build eective classiers (details
are in Section 3.2.2 and 3.2.3).
In detail, the chosen initial instance is computed as fol-
lows:
arg max
i2Ufmin
j2U;j6=i(distance (UFi; UF j)g (1)
where Urepresents the initial unlabeled dataset, UFiand
UFjdenote the feature vector of ithandjthtest report
inU, respectively. We apply cosine similarity between t-
wo feature vectors to measure their distance. This is be-
cause prior study showed that it performs better for high-
dimensional text documents than other distance measures
(e.g., euclidean distance, manhattan distance) [15].
3.2.2 Selecting a Query
This step aims at selecting the most informative test re-
ports and asking user their labels. In each iteration, given a
set of labeled reports and unlabeled reports, LOAF will se-
lect an unlabeled report based on its local informativeness.
In detail, rstly, LOAF obtains the local neighborhood for
each unlabeled report, and measures its local informative-
ness. This is done through computing the distance between
the unlabeled report and every labeled report. The labeled
report with nearest distance is treated as the local neigh-
borhood for each unlabeled report, and the local informa-
tiveness is measured using the nearest distance. Secondly,
LOAF selects the unlabeled report whose local informative-
ness is the largest, which can better serve the local-based
classication in Section 3.2.3. This is done by selecting the
193unlabeled report whose nearest distance is the largest, a-
mong all unlabeled reports.
The rationale behind is as follows: to serve as the local-
based classication in the next step, it should be guaranteed
that each unlabeled instance has nearby reports which can
be used to classify itself. Hence, if one instance's nearest
neighbor has already been labeled, the local-based classier
would have the higher probability of correctly classifying it.
On the other hand, if the nearest labeled instance turns out
to be far away, this might imply big uncertainty in model
building and classication. From this point of view, the
farthest of the nearest labeled instance turns out to be the
most informative one for local-based classication.
In detail, the selected instance is computed as follows:
arg max
i2Ufmin
j2L(distance (UFi; LFj)g (2)
where UandLrepresent the unlabeled dataset and labeled
dataset in this iteration, respectively. UFiandLFjdenote
the feature vector of ithunlabeled report and feature vector
ofjthlabeled report, respectively. Similarly, we apply cosine
similarity to measure the distance of two feature vectors.
3.2.3 Learning ClassiÔ¨Åers
This step aims at learning classiers to classify unlabeled
reports, using labeled reports. Since our investigated dataset
has local bias, LOAF uses the local-based classication, i.e.,
utilizing the reports in one's local neighborhood to conduct
the classication. In detail, for each unlabeled report, LOAF
uses the prlocal most nearest labeled test reports to build
classier.
Theprlocal labeled reports are chosen as follows:
argprlocalfmin
j2L(distance (UFi; LFj)g (3)
where UFidenotes the feature vector of ithunlabeled report
which needs to be classied. LFjrepresents the feature
vector of jthlabeled report, and Lrepresents the labeled
dataset in this iteration. Note that, if the size of Lis smaller
thanprlocal, then prlocal is set as the size of L. Similarly,
we apply cosine similarity to calculate the distance of the
two feature vectors.
3.2.4 Whether to Terminate
This step is to judge whether the labeling process could be
terminated. Our approach considers two dierent scenarios
of termination. The rst one is that user can input the max-
imum eort (e.g., number of labeling instances) they can af-
ford. When the eort is reached, the labeling process will be
terminated. The second one is that the approach will decide
whether to terminate according to the classication results.
If the classication for each unlabeled report remains un-
changed in the successive prstop iterations, LOAF would
suggest termination. For both scenarios, the performance in
the last iteration is treated as the nal classication results.
Note that, as the classication performance in the rst
scenario is unguaranteed, this paper only evaluates the ef-
fectiveness of LOAF under the second scenario.
4. EXPERIMENT SETUP
4.1 Research Questions
We evaluate our approach through three dimensions: ef-
fectiveness, advantage, and usefulness. Specically, our e-
valuation addresses the following research questions:Table 2: Projects under investigation
# #Fa #TF %TF # #Fa #TF %TF
P1 321 267 183 68.5 P2 874 717 144 20.1
P3 302 168 125 74.4 P4 216 144 31 21.5
P5 492 455 280 61.5 P6 403 304 25 8.2
P7 688 647 253 39.1 P8 1094 727 447 61.4
P9 504 394 157 39.8 P10 320 163 61 37.4
P11 637 537 220 40.9 P12 436 232 165 71.1
P13 297 223 129 57.8 P14 217 157 46 29.3
P15 423 272 250 91.9 P16 556 398 251 63.1
P17 815 667 262 39.3 P18 307 180 122 67.8
P19 632 545 183 33.5 P20 580 323 112 34.7
P21 802 672 177 26.3 P22 466 402 102 25.3
P23 1414 1034 500 48.3 P24 1502 1152 583 50.1
P25 342 181 76 41.9 P26 824 503 131 26.0
P27 524 424 163 38.4 P28 391 317 253 35.2
P29 390 334 87 26.0 P30 495 417 83 19.9
P31 832 754 465 61.7 P32 806 522 199 38.1
P33 358 93 49 52.7 P34 452 284 58 20.4
Summary
# #Fa #TF %TF
19,712 14,609 6,372 43.4
Projects for case study
C1 231 177 87 49.1 C2 690 455 182 40.0
C3 1428 1004 392 39.0
RQ1 (Eectiveness) : How eective is LOAF in clas-
sifying crowdsourced reports?
We rst investigate the performance of LOAF in classi-
fying crowdsourced report for each experimental project.
Then we study the inuence of parameter prlocal and
prstop on model performance, as well as suggest the
optimal prlocal andprstop. Finally, we explore the
inuence of initial instance on model performance and
further demonstrate the eectiveness of our initial selection
method.
RQ2 (Advantage) : Can LOAF outperform existing
techniques in classifying crowdsourced reports?
To demonstrate the advantages of our approach, we com-
pare the performance of LOAF with both active learning
techniques and supervised learning technique (details in Sec-
tion 4.3).
RQ3 (Usefulness) : Is LOAF useful for software
testers?
We conduct a case study and a questionnaire survey in
Baidu crowdsourced testing group to further evaluate the
usefulness of the proposed LOAF.
4.2 Data Collection
Our experiment is based on crowdsourced reports from
the repositories of Baidu crowdsourced testing platform. We
collect all crowdsourced testing projects closed between Oc-
t. 20th 2015 and Oct. 30th 2015. There are totally 34
projects. Table 2 provides more details with total number
of crowdsourced reports submitted (#), number of failed
reports (#Fa), number and ratio of reports assessed as true
fault (#TF, %TF). Due to commercial consideration, we
replace detailed project names with serial numbers.
Additionally, we randomly collect three other projects
closed in Mar. 10th 2016 to conduct the case study in
Section 5.3.
Note that, our classication is conducted on failed re-
ports, not the complete set. We exclude the passed reports
because of the following reason. As we mentioned, failed
reports can usually involve both correct behaviors and true
faults. However, through talking with testers in the compa-
ny, we nd that almost none of the passed reports involve
true fault.
194We use the assessment attribute (Table 1) of each report
as the groundtruth label of classication. To verify the va-
lidity of these stored labels, we additionally conduct the ran-
dom sampling and relabeling. In detail, we randomly select
10 projects, and sample 10% of crowdsourced reports from
each selected project. A tester from the company is asked
to relabel the data, without knowing the stored labels. We
then compare the dierence between the stored labels and
the new labels. The percentage of dierent labels for each
project is all below 4%.
4.3 Experimental Setup and Baselines
To demonstrate the advantages of LOAF, we rst compare
our approach to three active learning techniques , which
are commonly-used or the state-of-the-art techniques:
Margin sampling [6]: Randomly choose the initial test
report to label. Use all current labeled reports to build
classier, and query a report for which the classier has the
smallest dierence in condence for each classication type
(true fault or not). Label it and repeat the process.
Least condence [6]: Randomly choose the initial test
report to label. Use all current labeled reports to build clas-
sier, and query a report that the classier is least condent
about. Label it and repeat the process.
Informative and representative [16]: Randomly
choose the initial test report to label. Use all current
labeled reports to build classier. Then query a report
which can both reduce the uncertainty of classier and
represent the overall input patterns of unlabeled data.
Label it and repeat the process. This is the state-of-the-art
technique. We use the package QUIRE7for experiments.
To alleviate the inuence of initial report on model per-
formance, for each method, we conduct 50 experiments with
randomly chosen initial reports.
As there might be some historical data which can be uti-
lized for model building, we also compare our approach with
supervised learning technique . Inspired by the cross-
project prediction in defect prediction [17] and eort esti-
mation [18], our experimental design is as follows: for each
project under testing, we choose the most similar project
from all available crowdsourced testing projects, then build
a classier based on the reports of that selected project.
The similarity between two projects is measured using the
method in [19], which is based on the marginal distribution
of training set and test set.
Both our approach and these baselines all involve utiliz-
ing machine learning classication algorithm to build clas-
sier. We have experimented with Support Vector Machine
(SVM) [20], Decision Tree [20], Naive Bayes [21], and Lo-
gistic Regression [21]. Among them, SVM can achieve good
and stable performance. Hence, we only present the results
of SVM in this paper, due to space limit.
4.4 Evaluation Metrics
As the main consideration for active learning is cost-
ecient, we utilize accuracy andeort to evaluate the
classication performance.
The F-Measure of classifying true fault after termination,
which is the harmonic mean of precision and recall [22], is
used to measure the accuracy . The reason we do not use
7http://lamda.nju.edu.cn/code QUIRE.ashxprecision and recall separately is because most of the F-
Measure in our experiments is 1.00, with 100% precision
and 100% recall.
We record the percentage of labeled reports among the
whole set of reports after termination, to measure the eort
for the classication.
5. RESULTS
5.1 Answering RQ1 (Effectiveness)
RQ1.1: What is the performance of LOAF in clas-
sifying crowdsourced reports?
Table 3 demonstrates the accuracy and eort of LOAF
for each experimental project (under the optimal prlocal
(15) and prstop (10)), as well as the statistics. Results re-
veal that in 70% projects (24/34), LOAF can achieve the
accuracy of 1.00, denoting 100% precision and 100% recall.
In 85% projects (29/34), the accuracy is above 0.98. Fur-
thermore, the minimum accuracy attained by LOAF is 0.95,
with precision and recall both above 0.90.
For eort, LOAF merely requires to label 10% of all test
reports on median for building eective classier. Beside,
for 88% (30/34) projects, LOAF only needs to label less
than 20% of all reports, which can support classifying all
the remaining test reports eectively.
The above analysis indicates that LOAF can facilitate the
report classication with high accuracy and little eort.
RQ1.2: What is the impact of parameter prlocal
on model performance and what is the optimal
prlocal for LOAF?
We use prlocal to control the local neighborhood when
conducting local-based classication (Section 3.2.3). To an-
swer this question, we vary prlocal from 2 to 50 with prstop
as 10, and compare the classication performance. We nd
that for all experimental projects, there are two patterns of
performance trend under changing prlocal. Due to space
limitation, we only present the performance trend of one
random-chosen project for each pattern in Figure 4.
We can observe that prlocal indeed could inuence the
model performance, which indicates the need for nding the
optimal prlocal. In the rst pattern, the performance would
remain best from prlocal is 15, thus the optimal prlocal
ranges from 15 to 50. In the second pattern, only when
prlocal is between 13 and 23, the accuracy is highest while
the eort is lowest, thus the optimal prlocal is in a narrower
range (from 13 to 23). The reason why performance keeps
unchanged in the rst pattern is that the actual labeled
instances are fewer than prlocal, so that the actual prlocal
used in such scenario is the number of labeled instances, not
the demonstrated prlocal. Generally speaking, too small
and too large prlocal can both result in low accuracy and
high eort. This may be because small prlocal will result
in the overtting of classiers, while large prlocal can easily
bring noise to the model.
To determine the optimal prlocal in our approach, we
rst obtain the value of optimal prlocal for each experi-
mental project. Then we count the occurrence of each value
and consider the value with most frequent occurrence as op-
timal prlocal, which is 15, 16, and 17 in our context. We
simply use 15 in the following experiments for saving com-
puting cost.
195Table 3: Comparison of classication performance
with dierent active learning techniques (RQ1.1 &
RQ2.1)
LOAF Margin Samp. Least Conf. Infor. Repre.
acc. e. acc. e. acc. e. acc. e.
P1 1.00 13 0.74,0.94 28,55 0.74,0.97 27,42 0.75,0.97 34,45
P2 1.00 15 0.62,0.97 26,42 0.84,0.95 16,32 0.80,0.96 16,42
P3 1.00 52 0.85,0.95 29,56 0.87,0.95 25,58 0.87,0.95 28,40
P4 1.00 8 0.18,0.98 43,73 0.22,0.98 20,44 0.20,0.93 20,56
P5 1.00 13 0.30,0.99 38,88 0.43,0.95 32,44 0.40,0.94 17,54
P6 0.97 14 0.07,0.96 15,24 0.12,0.95 20,34 0.10,0.95 26,40
P7 1.00 6 0.32,0.96 28,78 0.42,0.99 32,54 0.31,0.99 33,65
P8 1.00 2 0.58,0.96 24,51 0.82,0.98 11,24 0.76,0.97 13,24
P9 1.00 3 0.61,0.96 34,54 0.84,0.98 38,49 0.80,0.95 37,49
P10 1.00 11 0.47,0.95 49,89 0.41,0.99 48,53 0.40,0.98 47,66
P11 1.00 2 0.92,0.97 41,81 0.92,0.98 32,53 0.86,0.99 56,80
P12 1.00 10 0.90,0.99 44,84 0.88,0.98 26,34 0.88,0.97 32,38
P13 1.00 14 0.92,0.99 45,95 0.92,0.98 27,45 0.92,0.98 32,80
P14 0.98 25 0.78,0.97 48,68 0.78,0.98 48,72 0.76,0.98 51,71
P15 0.96 22 0.84,0.95 26,38 0.86,0.99 22,35 0.84,0.95 25,36
P16 0.98 14 0.52,0.95 36,86 0.60,0.98 36,78 0.52,0.96 19,64
P17 1.00 4 0.38,0.98 38,68 0.36,0.99 35,52 0.31,0.96 17,23
P18 0.99 25 0.90,0.98 28,98 0.90,0.99 27,54 0.92,0.99 27,34
P19 1.00 2 0.47,0.97 34,84 0.64,0.97 26,74 0.40,0.96 26,43
P20 1.00 3 0.80,0.99 34,84 0.76,0.97 22,46 0.80,0.99 32,68
P21 0.99 12 0.04,0.98 13,44 0.43,0.97 17,37 0.26,0.97 13,32
P22 1.00 7 0.17,0.97 12,32 0.68,0.97 26,38 0.34,0.96 21,38
P23 1.00 1 0.86,0.98 56,86 0.90,0.94 43,63 0.88,0.97 47,62
P24 1.00 1 0.72,0.92 37,57 0.76,0.95 35,45 0.78,0.96 35,43
P25 1.00 11 0.90,0.93 36,90 0.91,1.00 36,52 0.90,0.98 47,62
P26 1.00 3 0.44,0.94 28,56 0.64,0.97 30,53 0.50,0.98 34,50
P27 1.00 3 0.30,0.97 31,81 0.37,0.99 21,46 0.22,0.99 24,59
P28 1.00 4 0.90,0.98 17,47 0.92,0.98 24,32 0.90,0.98 24,42
P29 0.95 20 0.59,0.98 28,80 0.56,0.98 23,47 0.44,0.98 24,42
P30 0.99 14 0.90,0.96 20,60 0.89,0.96 23,50 0.88,0.98 35,54
P31 1.00 2 0.82,0.99 30,40 0.80,0.98 25,42 0.82,0.98 24,38
P32 0.95 9 0.90,0.97 45,78 0.94,0.97 41,56 0.90,0.95 44,62
P33 1.00 15 0.96,1.00 62,98 0.96,0.98 37,64 0.96,0.99 46,64
P34 0.95 15 0.40,0.96 33,48 0.64,0.98 22,35 0.52,0.97 32,42
min 0.95 1 0.04,0.92 13,24 0.12,0.94 11,24 0.10,0.93 13,23
max 1.00 52 0.96,0.99 62,98 0.96,1.00 48,78 0.96,0.99 56,80
med. 1.00 10 0.67,0.97 33,70 0.77,0.98 26,46 0.77,0.97 30,47
avg. 0.99 11 0.61,0.96 33,67 0.69,0.97 28,48 0.64,0.96 30,50
Note: The two numbers in one cell represent minimum and
maximum value of 50 random experiments.
(a) Project 3
 (b) Project 10
Figure 4: Inuence of pr local on performance
(RQ1.2)
RQ1.3: What is the impact of parameter prstopon
model performance and what is the optimal prstop
for LOAF?
In our work, we use prstop to decide whether to ter-
minate (Section 3.2.4). To answer this question, we use
the optimal prlocal obtained earlier, which is 15, and vary
prstop from 2 to 20 for experiments. Through examining
the performance trend for all experimental projects, we nd
that there are also two patterns of performance trend under
changing prstop. We only present the performance trend
of one random-chosen project for each pattern in Figure 5,
because of space limitation.
Similarly, we can observe that prstop indeed could in-
uence the model performance, thus suggesting the optimal
prstop would be helpful. It is easily understood that with
(a) Project 3
 (b) Project 10
Figure 5: Inuence of pr stop on performance
(RQ1.3)
the increase of prstop, the eort would increase correspond-
ingly. From the rst pattern, we can observe that accura-
cy might occasionally decrease with the increase of prstop,
but can recover when prstop is 10. The second pattern
shows that the accuracy can reach the highest and remain
unchanged, from prstop is 7.
To determine the optimal prstop, we rst obtain the value
ofprstop when the accuracy reach the highest and keep
stable for each project (10 and 7 for the two demonstrated
projects). We then consider the maximum among these
values as optimal prstop, which is to ensure all the projects
can reach the highest accuracy. In our experimental context,
the optimal prstop is 10.
RQ1.4: What is the impact of initial instance on
model performance and how eective of our initial
selection method?
To answer this question, we experiment with random-
chosen report acting as the initial instance, and repeat
N times ( N is set as half of project size). Figure 6
demonstrates the min, average and max value of model
performance for random selection of initial report.
Nearly 40% (13/34) projects would undergo quite low ac-
curacy (min value is less than 0.2) when randomly choosing
initial report. For eort, nearly 60% (20/34) projects would
involve quite high eort (max value is more than 30%) when
choosing some random report for initial labeling. This re-
veals that random selection of initial instance can usually
fall into low performance. The results further indicate the
great necessity to design a method for the selection of initial
instance to ensure a stable performance.
We then compare the performance of our initial selection
method (details in Section 3.2.1) with the statistics of ran-
dom selection. For accuracy, in 70% (24/34) projects, our
method of initial selection can achieve the maximal value
which the random selection can ever achieve. For other
projects, the dierence between our method and the maxi-
mal value of random selection is almost negligible, ranging
from 0.008 to 0.04. These ndings reveal that our initial
selection method can achieve high and stable accuracy even
compared with the best accuracy which random selection
can ever reach.
For eort, in 97% (33/34) projects, our method of initial
selection requires less eort than the maximal eort which
random selection would require. In 55% (19/34) projects,
our method requires less eort than the average eort re-
quired by random selection. In 86% (13/15) of remaining
projects, the dierence between the eort of our method
196Figure 6: Inuence of initial instance on model performance (RQ1.4)
and the average eort of random selection is smaller than
10%. These illustrations further reveal that our initial se-
lection method is superior to random selection, considering
its stable results, high accuracy, and little eort.
5.2 Answering RQ2 (Advantage)
RQ2.1: How does LOAF compare to existing ac-
tive learning techniques in classifying crowdsourced
reports?
Table 3 illustrates the performance of LOAF and three
active learning baselines. As we mentioned in Section 4.3,
we conduct 50 experiments for each active learning method
because their random initialization can eect the nal per-
formance. We present the minimum and maximum perfor-
mance of the random experiments for these methods. LOAF
has well-designed initial selection step (Section 3.2.1), so it-
s performance is unique. The value with dark background
denotes the best accuracy or eort for each project.
At rst glance, we can nd that all the three baseline
methods can occasionally fall into quite low accuracy and
more eort, for most of the projects. On the contrary, as
we have well-designed method to choose the initial report,
LOAF can achieve high and stable performance. We also no-
ticed that even the state-of-the-art method (`Infor. Repre.'
method in Table 3) can not perform well for crowdsourced
testing reports. This may stem from the fact that it does
not consider the local nature of the dataset.
We then compare the performance of LOAF with the best
performance which the baselines can ever achieve (highest
accuracy or least eort). We can observe that all statistics
(min, max, median and average) of accuracy for LOAF is
higher than the best performance of baselines, denoting our
approach can achieve a more accurate classication perfor-
mance (the median value is 1.00). In addition, these statis-
tics for eort of our approach are all less than the baselines
(10% vs. 26% for median value).
We then shift our focus to the performance of each project.
For more than 85% of all projects (29/34), our approach
can attain both the highest accuracy and least eort, even
compared with the best performance of baselines. Besides,
for one of the remaining projects, our approach can achieve
much higher accuracy, with 27% more eort at worst (P3),
than the best baseline approach. For other four projects,
our approach has slight decline in accuracy but with least
eort. We also noticed that the decline is quite small with
the maximum being 0.03 (0.95 vs. 0.98), and all the accuracy
of our approach is above 0.95.
When it comes to the time and space cost of the men-
tioned techniques, it takes less than 1.0 seconds and less
Figure 7: Comparison of classication performance
with supervised learning technique (RQ2.2)
then 4.0MB memory for each step. Due to space limit, we
would not present the details.
RQ2.2: How does LOAF compare to existing
supervised learning technique in classifying crowd-
sourced reports?
Figure 7 illustrates the accuracy of LOAF and supervised
prediction technique. We do not present the eort because
supervised learning relies on historical data for classication,
thus the eort is labeling all the training data.
We can easily observe for all the projects, the accuracy of
LOAF is higher than the accuracy obtained by supervised
learning. The median accuracy of supervised learning for all
projects is 0.77, which is much smaller than that of LOAF
(1.00). This indicates that LOAF can perform even bet-
ter than supervised classication, which built on the large
amount of historical labelled data. The reason might be o-
riginated from the fact that existing techniques cannot well
deal with the local bias of crowdsourced testing data.
5.3 Answering RQ3 (Usefulness)
RQ3: Is LOAF useful for software testers?
To further assess the usefulness of LOAF, we use the im-
plementation of our proposed LOAF to conduct a case study
and a questionnaire survey in Baidu. We randomly select
three projects for our case study (details are in Table 2). We
collect these test reports as soon as it was closed, without
any labeling information. Six testers from the crowdsourced
testing group are involved. We divide them into two groups
according to their experience, with details summarized in
Table 4.
The goal of this case study is to evaluate the usefulness of
LOAF in classifying the true fault from the crowdsourced
test reports. Firstly, each practitioner in Group A is asked
197Table 4: Participant of case study (RQ3)
Group A Group B
A1 3-5 years experience in testing B1
A2 1-3 years experience in testing B2
A3 <1 years experience in testing B3
to do the classication using LOAF. As a comparison, prac-
titioners in Group B are asked to do it manually. To build
the ground truth, we gather all the classication outcomes
from the practitioners. Follow-up interviews are conducted
to discuss the dierences among them. Common consensus
is reached on all the dierence and a nal edition of classi-
cation is used as the ground truth, details in Table 2.
Besides the accuracy and eort evaluation metrics in prior
experiments, we also record the time taken for the classi-
cation. Table 5 presents the detailed results.
Table 5: Results of case study (RQ3)
Results from Group A Results from Group B
Project C1 C2 C3 C1 C2 C3
Accuracy 0.98,0.99 0.99,1.00 0.98,0.99 0.97,1.00 0.95,1.00 0.86,0.95
Eort (%) 15,15 4,4 15,15 100,100 100,100 100,100
Time (min) 15,18 12,18 64,87 72,90 200,240 370,410
Note: The two numbers in one cell represent minimum and
maximum value from the three practitioners.
The classication performance of LOAF (Group A) is
much better than the performance by manual (Group
B), with higher accuracy, lower eort and less time. In
particular, with the increase of project size, the consumed
eort and time of manual classication can dramatically
increase, while its accuracy would also drop obviously.
Although testers can assign wrong labels to the queried
reports when using LOAF, the nal accuracy is less aected
by these mistakes. This further denotes the eectiveness
and usefulness of LOAF in real practice.
In addition, we design a questionnaire and conduct a sur-
vey to consult testers about the usefulness of LOAF. The
questionnaire rst demonstrates a short description about
LOAF, a visualized classication process and a summarized
evaluation result on 34 projects. Then it asks three ques-
tions shown in Table 6. We provide ve options for the rst
two questions, and allow respondents freely express their
opinion for the third question.
We send invitation emails to the testers who are involved
in the report classication in Baidu crowdsourced testing
group. We totally received 24 responses out of 37 requests.
As indicated in Table 6, of all 24 respondents, 19 of them
(80%) agree that LOAF is useful for report classication and
they would like to use it. This means testers agree the use-
fulness of LOAF in general. Only 2 (8%) hold conservation
options and 3 (12%) disagreed. When it comes to the reason
of disagreement, they mainly worry about its exibility and
performance on new project, as well as the recall of faults.
This paves the direction for further research. In addition,
the project manager shows great interest in LOAF, and is
arranging to deploy LOAF on their platform to assist the
classication process.
6. THREATS TO V ALIDITY
The external threats concern the generality of this study.
First, our experiment data consist of 34 projects collected
from one of the Chinese largest crowdsourced testing plat-
forms. We can not assume a priori that the results of ourstudy could generalize beyond this environment in which it
was conducted. However, the various categories of projects
and size of data relatively reduce this risk. Second, al-
l crowdsourced reports investigated in this study are written
in Chinese, and we cannot assure that similar results can be
observed on crowdsourced projects in other languages. But
this is alleviated as we did not conduct semantic compre-
hension, but rather simply tokenize sentence and use word
as token for learning.
Regarding internal threats, we only utilize textual fea-
tures to build classiers, without including other features.
Besides, we only use cosine similarity for measuring the dis-
tance between crowdsourced reports, without trying other
measures. The experiment outcomes have proved the eec-
tiveness of our method. Anyhow, we will try more features
(e.g., the attributes of report) and other metrics for distance,
to further investigate their inuence on model performance.
Construct validity of this study mainly questions the data
processing method. We rely on the assessment attribute of
crowdsourced reports stored in repository to construct the
ground truth. However, this is addressed to some extent due
to the fact that testers in the company have no knowledge
that this study will be performed for them to articially
modify their labeling. Besides, we have veried its validity
through random sampling and relabeling.
7. RELATED WORK
7.1 Crowdsourced Testing
Crowdsoucing is the activity of taking a job traditional-
ly performed by a designated agent (usually an employee)
and outsourcing it to an undened, generally large group of
people in the form of an open call [23]. Chen and Kim [24]
applied crowdsourced testing to test case generation. They
investigated object mutation and constraint solving issues
underlying existing test generation tools, and presented a
puzzle-based automatic testing environment. Musson et al.
[25] proposed an approach, in which the crowd was used to
measure real-world performance of software products. The
work was presented with a case study of the Lync commu-
nication tool at Microsoft. Gomide et al. [26] proposed an
approach that employed a deterministic automata to help
usability testing. Adams et al. [27] proposed MoTIF to de-
tect and reproduce context-related crashes in mobile apps
after their deployment in the wild. All the studies above use
crowdsourced testing to solve some problems in traditional
software testing activities. However, our approach is to solve
the new encountered problem in crowdsourced testing.
Feng et al. [1] proposed test report prioritization methods
for use in crowdsourced testing. They designed strategies to
dynamically select the most risky and diversied test report
for inspection in each iteration. Their method was evaluated
only on 3 projects with students acting as crowd workers,
while our evaluation is conducted on 34 projects with case
study in one of the Chinese largest crowdsourced testing
platform. Besides, our approach requires much less eort
than theirs. Our previous work [28] proposed a cluster-based
classication approach to eectively classify crowdsourced
reports when facing with plenty of training data. However,
training data is often not available. This work can reduce
the eort for collecting training data while still being able
to achieve good performance.
198Table 6: Results of survey (RQ3)
Questions Strongly
DisagreeDisagree Neither Agree Strongly
AgreeTotal
Q1. Do you think LOAF is useful to classify \true fault" from crowdsourced test report? 0 1 0 3 20 24
Q2. Would you like to use LOAF to help with the report classication task? 0 3 2 8 11 24
If Disagree for either of the question, please give the reason. Worry about its accuracy on other projects;
Flexibility on new projects;
Might miss crucial fault;3
7.2 Automatic ClassiÔ¨Åcation in Software En-
gineering
Issue reports are valuable resources during software main-
tenance activities. Automated support for issue report clas-
sication can facilitate understanding, resource allocation,
and planning. Menzies and Marcus [2] proposed an auto-
mated severity assessment method by text mining and ma-
chine learning techniques. Tian et al. [3] proposed DRONE,
a multi factor analysis technique to classify the priority of
bug reports. Wang et al. [4] proposed a technique combining
natural language and execution information to detect dupli-
cate failure reports. Zanetti et al. [29] proposed a method to
classify valid bug reports based on nine measures quantifying
the social embeddedness of bug reporters in the collabora-
tion network. Zhou et al. [5] proposed a hybrid approach
by combining both text mining and data mining techniques
of bug report data to automate the classication process.
Wang et el. [30] proposed FixerCache, an unsupervised ap-
proach for bug triage by caching developers based on their
activeness in components of products. Mao et al. [31] pro-
posed content-based developer recommendation techniques
for crowdsourcing tasks. Our work is to classify test re-
port in crowdsourced testing, which is dierent from the
aforementioned studies in two ways. Firstly, crowdsourced
reports are more noise than issue reports, because they are
submitted by non-specialized crowd workers and often under
nancial incentives. In this sense, classifying them is more
valuable, yet possesses more challenges. Secondly, previous
studies utilized supervised learning techniques to conduc-
t the classication, which often requires users to manually
label plenty of training data. Our proposed approach re-
duces the burden of manual labeling while still being able
to achieve good performance.
There were researches to classify app reviews as bug re-
ports, feature requests, etc. [32{34], which can help deal
with the large amounts of reviews. App reviews are often
considered as issue reports by users, who behave unprofes-
sionally as crowd workers in our context. But these related
methods need large number of labeled data to learn the su-
pervised model, which is time-consuming. Moreover, the
performances of our approach surpass theirs (F-measure is
0.72 to 0.95 in [34]) a lot.
Some other studies focus on dierentiating between mali-
cious behaviors and benign behaviors of app, based on data
ow or context information [35, 36] . This motivates us to
utilize new sources of information to conduct the report clas-
sication in future work.
7.3 Active Learning in Software Engineering
There is a wealth of active learning studies in machine
learning literature, such as [16, 37]. Several studies have
utilized active learning techniques for software engineering
tasks. Bowring et al. [38] proposed an automatic approach
for classifying program behavior by leveraging Markov mod-
el and active learning. Lucia et al. [39] proposed an ap-
proach to actively incorporate user feedback in ranking cloneanomaly reports. Wang et al. [40] proposed a technique
that can rene the results from a code search engine by ac-
tively incorporating incremental user feedback. Thung et
al. [41] proposed an active semi-supervised defect prediction
approach to classify defects into ODC defect type. Ma et
al. [42] proposed an active approach for detecting malicious
apps. These aforementioned studies merely utilized existing
active learning techniques to solve the software engineering
tasks, without considering the speciality of these tasks.
Kocaguneli et al. [43] proposed QUICK, which is an ac-
tive learning method that assists in nding the essential
content of software eort estimation data, so as to simplify
the complex estimation methods. Nam et al. [44] proposed
novel approaches to conduct defect prediction on unlabeled
datasets in an automated manner. These two researches
have something in common with our work { they are new
designed active learning methods for specialized task. But
their methods can not be directly used for the classication
of crowdsourced reports and could not handle the local bias
in dataset.
8. CONCLUSION
This paper proposes LOcal-based Active Classi Fication
(LOAF) to address the two challenges in automating
crowdsourced test reports classication, i.e., local bias
problem and lacking of historical labeled data. We evaluate
LOAF from the standpoints of eectiveness, advantage,
and usefulness in one of the Chinese largest crowdsourced
testing platforms, and results are promising.
Active learning techniques are promising when facing such
situation that data are abundant but labels are scarce or
expensive to obtain. This is becoming quite common in
software engineering practice for the last decade. Hence,
our approach can well motivate the various classication
problems which have required plenty of labeled data, e.g.,
report classication [4,5], app review classication [32,33].
It should be pointed out, however, that the presented ma-
terial is just the starting point of the work in progress. We
are closely collaborating with Baidu crowdsourced platform
and planning to deploy the approach online. Returned re-
sults will further validate the eectiveness, as well as guide
us in improving our approach. Future work will also include
exploring other features and techniques to further improve
the model performance and stability.
9. ACKNOWLEDGMENTS
This work is supported by the National Natural Sci-
ence Foundation of China under grant No.61432001,
No.91318301, No.91218302, and No.61303163. We would
like to thank the testers in Baidu for their great eorts in
supporting this work.
10. REFERENCES
[1] Y. Feng, Z. Chen, J. A. Jones, C. Fang, and B. Xu,
\Test report prioritization to assist crowdsourced
199testing," in Proceedings of the 10th Joint Meeting of
the European Software Engineering Conference and the
ACM SIGSOFT Symposium on The Foundations of
Software Engineering (FSE 2015) , 2015, pp. 225{236.
[2] T. Menzies and A. Marcus, \Automated severity
assessment of software defect reports," in Proceedings
of IEEE International Conference onSoftware
Maintenance (ICSM 2008) , 2008, pp. 346{355.
[3] Y. Tian, D. Lo, and C. Sun, \Drone: Predicting
priority of reported bugs by multi-factor analysis," in
Proceedings of the 29th IEEE International
Conference on Software Maintenance (ICSM 2013) ,
2013, pp. 200{209.
[4] X. Wang, L. Zhang, T. Xie, J. Anvik, and J. Sun, \An
approach to detecting duplicate bug reports using
natural language and execution information," in
Proceedings of the 30th International Conference on
Software Engineering (ICSE 2008) , 2008, pp. 461{470.
[5] Y. Zhou, Y. Tong, R. Gu, and H. Gall, \Combining
text mining and data mining for bug report
classication," in Proceedings of the 30th IEEE
International Conference on Software Maintenance
(ICSM 2014) , 2014, pp. 311{320.
[6] B. Settles, \Active learning literature survey,"
University of Wisconsin-Madison, Tech. Rep., 2010.
[7] D. D. Lewis and W. A. Gale, \A sequential algorithm
for training text classiers," in Proceedings of
ACM-SIGIR Conference on Research and
Development in Information Retrieval (SIGIR 1994) ,
1994.
[8] A. Storkey, Dataset shift in machine learning . The
MIT Press, Cambridge, MA, 2009.
[9] I. H. Witten and E. Frank, Data Mining: Practical
machine learning tools and techniques . Morgan
Kaufmann, 2005.
[10] Y. Yang, Z. He, K. Mao, Q. Li, V. Nguyen, B. Boehm,
and R. Valerdi, \Analyzing and handling local bias for
calibrating parametric cost estimation models,"
Information and Software Technology , vol. 55, no. 8,
pp. 1496 { 1511, 2013.
[11] T. Zimmermann, N. Nagappan, H. Gall, E. Giger, and
B. Murphy, \Cross-project defect prediction: A large
scale experiment on data vs. domain vs. process," in
Proceedings of the 7th Joint Meeting of the European
Software Engineering Conference and the ACM
SIGSOFT Symposium on The Foundations of
Software Engineering (FSE 2009) , 2009, pp. 91{100.
[12] T. Menzies, A. Butcher, A. Marcus, T. Zimmermann,
and D. Cok, \Local vs. global models for eort
estimation and defect prediction," in Proceedings of
the 2011 26th IEEE/ACM International Conference
on Automated Software Engineering (ASE 2011) ,
2011, pp. 343{351.
[13] T. Menzies, A. Butcher, A. Marcus, D. Cok, F. Shull,
B. Turhan, and T. Zimmermann, \Local versus global
lessons for defect prediction and eort estimation,"
IEEE Transactions on software engineering , vol. 39,
no. 6, pp. 822{834, 2013.
[14] Z. He, F. Peters, T. Menzies, and Y. Yang, \Learning
from open-source projects: An empirical study on
defect prediction," in Proceedings of ACM/IEEEInternational Symposium on Empirical Software
Engineering and Measurement (ESEM 2013) , Oct
2013, pp. 45{54.
[15] H. Finch, \Comparison of distance measures in cluster
analysis with dichotomous data," Journal of Data
Science , vol. 3, pp. 85{100, 2005.
[16] S.-J. Huang, R. Jin, and Z.-H. Zhou, \Active learning
by querying informative and representative examples,"
IEEE Transactions on Pattern Analysis and Machine
Intelligence , in press.
[17] B. Turhan, T. Menzies, A. B. Bener, and
J. Di Stefano, \On the relative value of cross-company
and within-company data for defect prediction,"
Empirical Software Engineering , vol. 14, no. 5, Oct.
2009.
[18] L. L. Minku and X. Yao, \How to make best use of
cross-company data in software eort estimation?" in
Proceedings of the 36th International Conference on
Software Engineering (ICSE 2014) , 2014, pp. 446{456.
[19] S. Hido, T. Id e, H. Kashima, H. Kubo, and
H. Matsuzawa, \Unsupervised change analysis using
supervised learning," in Proceedings of 12th
Pacic-Asia Conference on Knowledge Discovery and
Data Mining (PAKDD 2008) , 2008, pp. 148{159.
[20] S. Kotsiantis, \Supervised machine learning: A review
of classication techniques," Informatica , vol. 31, pp.
249{268, 2007.
[21] A. Berson, S. Smith, and K. Thearling, \An overview
of data mining techniques," Building Data Mining
Application for CRM , 2004.
[22] C. D. Manning, P. Raghavan, and H. Sch l ztze,
Introduction to Information Retrieval . Cambridge
University Press, 2008.
[23] K.-J. Stol and B. Fitzgerald, \Two's company, three's
a crowd: A case study of crowdsourcing software
development," in Proceedings of the 36th International
Conference on Software Engineering (ICSE 2014) ,
2014, pp. 187{198.
[24] N. Chen and S. Kim, \Puzzle-based automatic testing:
Bringing humans into the loop by solving puzzles," in
Proceedings of the 27th IEEE/ACM International
Conference on Automated Software Engineering (ASE
2012) , 2012, pp. 140{149.
[25] R. Musson, J. Richards, D. Fisher, C. Bird,
B. Bussone, and S. Ganguly, \Leveraging the crowd:
How 48,000 users helped improve lync performance,"
IEEE Software , vol. 30, no. 4, pp. 38{45, 2013.
[26] V. H. M. Gomide, P. A. Valle, J. O. Ferreira, J. R. G.
Barbosa, A. F. da Rocha, and T. M. G. d. A. Barbosa,
\Aective crowdsourcing applied to usability testing,"
International Journal of Computer Science and
Information Technologies , vol. 5, no. 1, pp. 575{579,
2014.
[27] M. G omez, R. Rouvoy, B. Adams, and L. Seinturier,
\Reproducing context-sensitive crashes of mobile apps
using crowdsourced monitoring," in Proceedings of the
IEEE/ACM International Conference on Mobile
Software Engineering and Systems (MOBILESoft) ,
2016, pp. 88{99.
[28] J. Wang, Q. Cui, Q. Wang, and S. Wang, \Towards
eectively test report classication to assist
crowdsourced testing," in Proceedings of ACM/IEEE
200International Symposium on Empirical Software
Engineering and Measurement (ESEM 2016) , 2016.
[29] M. S. Zanetti, I. Scholtes, C. J. Tessone, and
F. Schweitzer, \Categorizing bugs with social
networks: A case study on four open source software
communities," in Proceedings of the International
Conference on Software Engineering (ICSE 2013) ,
2013, pp. 1032{1041.
[30] S. Wang, W. Zhang, and Q. Wang, \FixerCache:
Unsupervised caching active developers for diverse bug
triage," in Proceedings of the International Symposium
on Empirical Software Engineering and Measurement
(ESEM 2014) , 2014, pp. 25:1{25:10.
[31] K. Mao, Y. Yang, Q. Wang, Y. Jia, and M. Harman,
\Developer recommendation for crowdsourced software
development tasks," in Proceedings of IEEE
Symposium on Service-Oriented System Engineering
(SOSE 2015) , 2015, pp. 347{356.
[32] W. Maalej and H. Nabil, \Bug report, feature request,
or simply praise? On automatically classifying app
reviews," in Proceedings of the 23rd IEEE
International Requirements Engineering Conference
(RE 2015) , 2015, pp. 116{125.
[33] E. Guzman, M. El-Halaby, and B. Bruegge, \Ensemble
methods for app review classication: An approach for
software evolution," in Proceedings of the 30th
IEEE/ACM International Conference on Automated
Software Engineering (ASE 2015) , 2015.
[34] S. Panichella, A. D. Sorbo, E. Guzman, C. A.Visaggio,
G. Canfora, and H. C. Gall, \How can i improve my
app? Classifying user reviews for software
maintenance and evolution," in Proceedings of the 31st
IEEE International Conference on Software
Maintenance (ICSM 2015) , 2015, pp. 281{290.
[35] V. Avdiienko, K. Kuznetsov, A. Gorla, A. Zeller,
S. Arzt, S. Rasthofer, and E. Bodden, \Mining apps
for abnormal usage of sensitive data," in Proceedings
of the IEEE/ACM 37th IEEE International
Conference on Software Engineering (ICSE 2015) ,
vol. 1, May 2015, pp. 426{436.
[36] W. Yang, X. Xiao, B. Andow, S. Li, T. Xie, and
W. Enck, \AppContext: Dierentiating malicious and
benign mobile app behaviors using context," inProceedings of the 2015 IEEE/ACM 37th IEEE
International Conference on Software Engineering
(ICSE 2015) , vol. 1, May 2015, pp. 303{313.
[37] S. Chakraborty, V. Balasubramanian, A. R. Sankar,
S. Panchanathan, and J. Ye, \Batchrank: A novel
batch mode active learning framework for hierarchical
classication," in Proceedings of the 21th ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining (KDD 2015) , 2015, pp.
99{108.
[38] J. F. Bowring, J. M. Rehg, and M. J. Harrold, \Active
learning for automatic classication of software
behavior," in Proceedings of the ACM SIGSOFT
International Symposium on Software Testing and
Analysis (ISSTA 2004) , 2004, pp. 195{205.
[39] Lucia, D. Lo, L. Jiang, and A. Budi, \Active
renement of clone anomaly reports," in Proceedings of
the 34th International Conference on Software
Engineering (ICSE 2012) , June 2012, pp. 397{407.
[40] S. Wang, D. Lo, and L. Jiang, \Active code search:
Incorporating user feedback to improve code search
relevance," in Proceedings of the 29th ACM/IEEE
International Conference on Automated Software
Engineering (ASE 2014) , 2014, pp. 677{682.
[41] F. Thung, X.-B. D. Le, and D. Lo, \Active
semi-supervised defect categorization," in Proceedings
of the 23rd International Conference on Program
Comprehension (ICPC 2015) , 2015, pp. 60{70.
[42] S. Ma, S. Wang, D. Lo, R. H. Deng, and C. Sun,
\Active semi-supervised approach for checking app
behavior against its description," in Proceedings of the
39th Annual Computer Software and Applications
Conference (COMPSAC 2015) , vol. 2, July 2015, pp.
179{184.
[43] E. Kocaguneli, T. Menzies, J. Keung, D. Cok, and
R. Madachy, \Active learning and eort estimation:
Finding the essential content of software eort
estimation data," IEEE Transactions on Software
Engineering , vol. 39, no. 8, pp. 1040{1053, Aug 2013.
[44] J. Nam and S. Kim, \CLAMI: Defect prediction on
unlabeled datasets," in Proceedings of the 30th
IEEE/ACM International Conference on Automated
Software Engineering (ASE 2015) , Nov 2015, pp.
452{463.
201