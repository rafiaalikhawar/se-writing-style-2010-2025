Tracking Static Analysis Violations Over Time to
Capture Developer Characteristics
Pavel Avgustinov, Arthur I. Baars, Anders S. Henriksen, Greg Lavender, Galen Menzel,
Oege de Moor, Max Sch√§fer, Julian Tibble
Semmle Ltd.
Oxford, United Kingdom
publications@semmle.com
Abstract ‚ÄîMany interesting questions about the software qual-
ity of a code base can only be answered adequately if Ô¨Åne-
grained information about the evolution of quality metrics over
time and the contributions of individual developers is known.
We present an approach for tracking static analysis violations
(which are often indicative of defects) over the revision history
of a program, and for precisely attributing the introduction and
elimination of these violations to individual developers. As one
application, we demonstrate how this information can be used to
compute ‚ÄúÔ¨Ångerprints‚Äù of developers that reÔ¨Çect which kinds of
violations they tend to introduce or to Ô¨Åx. We have performed an
experimental study on several large open-source projects written
in different languages, providing evidence that these Ô¨Ångerprints
are well-deÔ¨Åned and capture characteristic information about the
coding habits of individual developers.
I. I NTRODUCTION
Static code analysis has become an integral part of the mod-
ern software developer‚Äôs toolbox for assessing and maintaining
software quality. There is a wide variety of static analysis
tools, particularly for Java and C/C++, which examine code
for potential bugs or performance bottlenecks, Ô¨Çag violations
of best practices, and compute software metrics. It has been
shown that static analysis warnings and sub-par metric scores
(below collectively referred to as violations ) are indicative of
software defects [27], [29], [35], [37], [38].
Some static analysis tools are tightly integrated into the
development cycle and examine code as it is being written [1],
[6], while others run in batch mode to produce detailed reports
about an entire code base [4], [7], [31]. The latter approach
is particularly useful for obtaining a high-level overview of
software quality. Moreover, most modern tools can aggregate
violations at different levels of detail, thus making it easy, for
instance, to identify modules that appear to be of worse quality
than others and should thus receive special attention.
However, this view of software quality is both static and
coarse-grained : it is static because it only concerns a single
snapshot of the code base at one point in time, and it is
coarse-grained because it does not differentiate contributions
by individual developers.
Most software is in a constant state of Ô¨Çux where developers
implement new features, Ô¨Åx bugs, clean up and refactor code,
add tests, or write documentation. While existing tools can
certainly analyse multiple versions of a code base separately,
they cannot easily assess changes between revisions. Forinstance, we might want to know what violations were Ô¨Åxed
in a given revision, or which new violations were introduced.
In short, analysing individual snapshots cannot provide an
accurate picture of the evolution of software quality over time.
Similarly, most code bases have many authors: there may
be seasoned developers and novice programmers, proliÔ¨Åc core
hackers and occasional bugÔ¨Åx contributors. By just looking at
a single snapshot, it is impossible to understand the quantity
and quality of contributions of individual developers.
Yet there are many situations where more dynamic and
Ô¨Åne-grained information is desirable. We brieÔ¨Çy outline three
example scenarios:
1) Alice wants to delegate a crucial subtask of her program
to a third-party library and is looking for an open-source
library that Ô¨Åts her needs. As is usually the case with
open-source projects, many different implementations
are available, and Alice has to carefully choose one that
is not only of high quality but also actively maintained.
She could use an off-the-shelf static analysis tool to
gain insight into the quality of the latest version of the
library. In addition, however, she might want to know
how its quality has evolved over time, and how different
developers have contributed to the code. For instance,
she may want to avoid a library where a core developer
who has contributed a lot of new code and bug Ô¨Åxes for
many years has recently become inactive.
2) Bob is managing a team of developers, and would like
to better understand their strengths and weaknesses. If
he knew, for instance, that developer Dave often intro-
duces static analysis violations related to concurrency,
he could arrange for Dave to receive additional training
in concurrency. If there is another developer on the team
who often Ô¨Åxes such violations, Bob could team them
up for code reviews.
3) In an effort to improve software quality, Bob‚Äôs colleague
Carol uses a static analyser to Ô¨Ånd modules that have a
particularly high violation density and hence are likely to
be of bad quality. In deciding which developer to put to
work on this code, it again helps if she understands their
expertise. If, for instance, the code in question has many
violations related to possible null pointer exceptions, it
may be a good idea to assign a developer who has a
strong track record of Ô¨Åxing such violations.All three scenarios rely on being able to precisely attribute
new and Ô¨Åxed violations to individual developers. This can be
achieved by integrating static analysis with revision control
information: if a violation appears or disappears in a revision
authored by developer D, then this suggests that Dwas
responsible for introducing or Ô¨Åxing this violation, and it can
justiÔ¨Åably be attributed to them.1
For this method to work, however, we need a reliable way
of tracking violations between revisions: if both revision n
and revision n+ 1 exhibit a violation of the same type, we
need to determine whether this is, in fact, the same violation,
or whether the violation in nwas Ô¨Åxed and a new one of the
same type just happened to be introduced in n+ 1.
Further challenges to be handled include merge commits
(which have more than one parent revision) and un-analysable
revisions: in almost any real-world code base, there is bound to
be an occasional bad commit that is not compilable, or cannot
meaningfully be analysed for other reasons. Such commits
require special care in order not to wrongly attribute violations
to authors of later commits.
Finally, there is an implicit assumption behind the scenarios
outlined above, namely that individual developers have a
distinctive ‚ÄúÔ¨Ångerprint‚Äù of violations that they introduce or Ô¨Åx.
If all developers tend, on average, to make the same mistakes,
then attribution information would not be very useful.
In this paper, we present Team Insight, a tool for Ô¨Åne-
grained tracking of software quality over time. At its core
is a method for tracking violations across revisions based
on a combination of diff-based location matching and hash-
based context matching. This approach is robust in the face
of unrelated code changes or code moves, and fast enough to
work on large, real-world code bases. Team Insight integrates
with many popular revision control systems to attribute viola-
tions to developers. It uses a simple distributed approach for
efÔ¨Åciently analysing and attributing a large number of revisions
in parallel.
We also present an approach for computing Ô¨Ångerprints
from attribution data, which compactly represent a summary
of which violations a developer tends to introduce or Ô¨Åx.
We have used Team Insight to analyse the complete revision
history of several large open source projects written in Java,
C++, Scala and JavaScript. We performed experiments to
verify that our violation tracking algorithm does not spuri-
ously match up unrelated revisions, and to gauge the relative
importance of the different matching strategies. Finally, we
used the information about new and introduced violations to
test the robustness of our developer Ô¨Ångerprints: selecting
a training set and a test set from the analysed snapshots,
we computed Ô¨Ångerprints from both sets of snapshots and
compared them. We found that Ô¨Ångerprints were both stable
1Many version control systems provide a way to determine which developer
last changed a given line of code. This information, however, is not usually
enough to determine who introduced a violation: the last developer to touch
the relevant source code may have been making an entirely unrelated edit.
Also, many violations are non-local in that the code that causes them is far
removed from the code that is Ô¨Çagged by the static analysis. Finally, this
approach does not provide any information about Ô¨Åxed violations.127 Set<String> revs;
128 ...
162 for (IRevision rev : new ArrayList<IRevision>(keep)) {
163 if(!revs.contains(rev)) {
164 ...
179 }
180 }
Fig. 1. Example of a violation
(i.e., the Ô¨Ångerprints computed for a single developer from the
test and the training set are very similar) and characteristic
of individual developers (i.e., the Ô¨Ångerprints computed for
different developers are quite different). Hence our Ô¨Ångerprints
could be useful in scenarios such as the ones outlined above.
We now turn to a brief exposition of some basic concepts
underlying Team Insight (Section II). We then explain our
violation matching technique in more detail (Section III) and
discuss how it is used to attribute new and Ô¨Åxed violations
to developers (Section IV). Next, we describe our approach
for computing developer Ô¨Ångerprints in Section V. These
developments are then pulled together in Section VI, which
reports on our experimental evaluation. Finally, Section VII
discusses related work, and Section VIII concludes.
II. B ACKGROUND
We start by motivating the concept of violation matching
with a real-world example from a Java project we analysed.
At one point, the Ô¨Åle DeleteSnapshots.java in the code
base contained the code fragment shown in Figure 1. On
line 127, the variable revsis declared to be of type Set<String> ,
yet the contains test on line 163 checks whether rev, which is
of type IRevision , is contained in it.
This is highly suspicious: unless unsafe casts were used to
deliberately break type safety, every element in revsmust be of
type String , which revis not. Thus, the test must always return
false , which is probably not what the programmer intended.
In this case the code should have checked whether rev.getId()
(which is a string representation of rev) is contained in revs.
Arguably, this code should be rejected by the type checker,
but for historical reasons the contains method in Java‚Äôs col-
lections framework is declared to have parameter type Object ,
so any object can be tested for membership in any collection,
regardless of the collection‚Äôs type. This problem is common
enough that many static analysis tools for Java, including our
own static analysis tool Project Insight [31], check for it.
To trace the life cycle of this violation, we consider seven
revisions of the code base, which we refer to as revisions 0
to 6. Figure 2 gives a brief summary of the relevant changes
in each revision, and the source location associated with the
violation (here, the call to contains ). In each case, the violation
location is given as a pair of a Ô¨Åle name and a line number.2
For now, we assume that these revisions were committed in
sequence as shown in Figure 3 (a).
2In practice, of course, locations are more precise: they include a start and
an end position, and specify both line and column number.Revision Change Summary Violation Location
0 DeleteSnapshots.java created N/A
1 violation introduced DeleteSnapshots.java :163
2 code added before violation DeleteSnapshots.java :173
3 code added before violation DeleteSnapshots.java :175
4 containing Ô¨Åle renamed FindObsoleteSnapshots.java :175
5 code added after violation FindObsoleteSnapshots.java :175
6 violation Ô¨Åxed N/A
Fig. 2. Relevant revisions of the code in Figure 1
0
1
2
3
4
5
60
1 2
m
3
4
5 6
n
(a) (b)
Fig. 3. Two commit graphs for the revisions in Figure 2; revisions where the
violation is present are shaded.
The Ô¨Åle containing the violation was Ô¨Årst created in Re-
vision 0, and the violation itself was introduced in Revi-
sion 1; at this point, it was located at line 163 of Ô¨Åle
DeleteSnapshots.java . In Revision 2 the violation was still
present, but some code had been inserted before it, so it
had moved to line 173. In Revision 3, it moved to line
175. In Revision 4, its line number number did not change,
but the enclosing class was renamed to FindObsoleteSnapshots ,
and the enclosing Ô¨Åle to FindObsoleteSnapshots.java . In
Revision 5, the Ô¨Åle was again changed, but since the changes
were textually after the violation its location did not change.
Finally, the violation was Ô¨Åxed in Revision 6.
If we want to automatically and precisely attribute violations
to developers, we have to carefully keep track of violations
across revisions. For instance, a violation tracking approach
based purely on source locations would consider the violation
in Revision 2 to be different from the violation in Revision 1,
since they have different source locations. Consequently, the
author of Revision 2 would erroneously be considered to have
Ô¨Åxed a violation on line 163 and introduced a violation of the
same type on line 173.
A more lenient location-based approach might try to identify
violations based on the name of the method and class enclosingits source location. This, however, would fail in Revision 4,
where the enclosing class is renamed (along with the Ô¨Åle).
Ignoring source locations entirely, one could attempt to
match up violations based on the similarity of their surround-
ing code: if two subsequent revisions contain two violations
of the same type that appear in identical or very similar
fragments of code, then there is a good chance that both are
occurrences of the same violation. In Revision 4, for instance,
there were only two minor textual changes (in addition to the
Ô¨Åle renaming), so a similarity-based approach could easily
determine that the violation is still present and has simply
moved to another Ô¨Åle.
Note, however, that neither location-based matching nor
similarity-based matching is strictly superior to the other:
while the former cannot deal with code movement or Ô¨Åle
renaming, the latter can become confused by unrelated changes
in code close to the violation. In such a case, a similarity-based
matcher may not be able to identify violations even if their
location has not changed.
Team Insight uses a combined approach detailed in the next
section: Ô¨Årst, it tries to match up as many violations as possible
based on their source location. For those violations that could
not be matched up, it computes a hash of the surrounding
tokens (similar to techniques used in clone detection [17]),
and then matches up violations with the same hash.
Additional care has to be taken when considering non-
linear commit graphs with branches and merges. Assume, for
instance, that the revisions of Figure 2 were committed as
shown in the commit graph of Figure 3 (b). Here, Revision 1
and Revision 2 are committed independently on separate
branches, and then merged together by a merge commit m.
Similarly, Revisions 5 and 6 are committed independently
and merged by n. Note in particular that the violation is
now no longer present in Revision 2, which branched off
before the violation was introduced in Revision 1. In the
merge commit m, however, the violation is merged in with
the changes from Revision 2.
The author of the merge commit m should clearly not be
blamed for introducing the violation, since it is already present
in Revision 1. In general, we can only consider a merge
commit to introduce a violation if that violation is absent in
allits parent revisions (but not the merge commit itself).
Similarly, the author of the merge commit n should not be
considered to have Ô¨Åxed the violation, since it was alreadyabsent in Revision 5. Again, a merge commit can only be
considered to Ô¨Åx a violation if that violation is present in all
its parent revisions but not the merge commit itself.
Finally, it should be noted that in any non-trivial code
base there are revisions that for one reason or another cannot
be analysed. For example, this could be due to a partial
or erroneous commits that results in uncompilable code, or
simply because some external libraries required by a very old
revision are no longer available. Such un-analysable revisions
have to be treated with care when attributing violations. If, say,
Revision 3 of our example was not analysable, then it would
not be possible to decide whether the violation present in
Revision 4 was introduced in Revision 4 itself or was already
present in Revision 3. Looking further back, we can, of course,
note that the revision was already present before Revision 3,
so it is likely that neither 3 nor 4 introduced the violation, but
this is at best an educated guess.
This concludes our informal discussion. We will now de-
scribe the Team Insight attribution algorithm in more detail.
III. M ATCHING VIOLATIONS
We start by establishing some terminology.
Aproject is a code base that can be subjected to static
analysis. A snapshot is a version of a project at a particular
point in time; for instance, if the project‚Äôs source code is stored
in a version control system, every revision is a snapshot. We
assume that there is a parent-of relation between snapshots.
There may be multiple snapshots with the same parent (due
to branching), and conversely a snapshot may have multiple
parents (due to merging).
We do not assume a particular underlying static analysis
system. All we require is that the static analysis can, for a
given snapshot S, produce a set of violations , where each
violation is uniquely identiÔ¨Åed by a source location land a
violation type t. The source location, in turn, is assumed to
be given by a start position and an end position delimiting
the piece of code that gives rise to the violation. Thus, the
violation can be modelled as a triple (S;l;t ). We explicitly
allow for the case that the static analysis may not be able
to analyse some snapshots, in which case there will be no
violation triples for that snapshot.3
With this terminology in place, the violation matching
problem can be stated succinctly as follows:
Given two snapshots SpandScof the same project,
whereSpis a parent snapshot of Sc, and two viola-
tion triplesVp:= (Sp;lp;tp)andVc:= (Sc;lc;tc),
doVpandVcindicate the same underlying defect?
We will not attempt to give a precise semantic deÔ¨Ånition
of when two violations indicate the same defect. Previous
studies [21] have shown that even developers familiar with
a code base often disagree about the origin of code snippets,
so there may not be a single correct deÔ¨Ånition anyway. Instead,
3In practice, the static analysis may be able to analyse parts of a snapshot,
but we do not use this partial information since it is very hard to compare
between snapshots.we describe the syntactic violation matching approach taken
by Team Insight, and leave it to our experimental evaluation
to provide empirical support for its validity.
To decide whether two violations (Sp;lp;tp)and(Sc;lc;tc)
match we employ a combination of three different matching
strategies: a location-based strategy that only takes the viola-
tion locations lpandlcinto account, a snippet-based strategy
that considers the program text causing the violations, and a
hash-based strategy that also considers the program text around
the violations.
We will now explore these strategies in detail. Note that
clearly two violations can only match if they are of the same
type, so in the following we implicitly assume that tp=tc.
A. Location-based violation matching
The idea of location-based violation matching is to use a
difÔ¨Ång algorithm to derive a mapping from source positions
inSpto source positions in Sc, and then match up violations
if they have the same or almost the same start position under
this mapping.
Our implementation uses the well-known difÔ¨Ång algorithms
of Myers [26] and of Hunt and Szymanski [15] to derive
source position mappings for individual Ô¨Åle in the snapshot.
SpeciÔ¨Åcally, we use the former algorithm for dense diffs where
there is a lot of overlap between the two Ô¨Åles, and the latter
for sparse diffs. Clearly, computing pairwise diffs for all the
Ô¨Åles in the parent and child snapshots would be much too
expensive; hence, we only compute diffs for Ô¨Åles with the
same path in both snapshots. In particular, if a Ô¨Åle is renamed
or moved to a different directory, the location-based violation
matching will not be able to match up any violations occurring
in it. We rely on the hash-based matching explained below to
catch such cases.
We will use the Java code snippets shown in Figure 4 as
our running example in this section. The code on the left,
which we assume to be from the parent snapshot, contains
three violations: V1is an instance of the contains type mismatch
problem mentioned in Section II; V2Ô¨Çags a reader object that
(we assume for this example) is not closed, thus potentially
leading to a resource leak; V3Ô¨Çags a call to System.gc , which
is generally bad practice. These three violations reappear in
the child snapshot on the right, but the statements containing
them have been rearranged and new code has been inserted.
We will now show how our matching strategies match up these
violations.
When given two Ô¨Åles FpandFc, whereFpis from the
parent snapshot and Fcfrom the child snapshot, the difÔ¨Ång
algorithms essentially partition FpandFcinto sequences of
line rangesrp
1;:::;rp
n, andrc
1;:::;rc
n, respectively. Each pair
of line ranges (rp
i;rc
i)is either a matching pair , meaning that
the ranges are textually the same, or a diff pair , meaning that
they are not the same. Every line in FpandFcbelongs to
exactly one line range. For a location l, we write [l]to mean
the line range its start line belongs to, and (l)to mean the
distance (in lines) from lto the start of its line range.if(apples.contains(orange))
++count;
fr = new FileReader("tst");
System.gc();fr = new FileReader("tst");
if(DEBUG)
{ System.gc(); }
if(apples.contains(orange))
++count;6=
=
6=
6=
Parent Snapshot Child SnapshotV1
V2
V3V0
2
V0
3
V0
1
Fig. 4. Examples of location-based violation matching
In general, there is no unique way of performing this
partitioning; we simply rely on whatever partitioning the
underlying diff algorithm produces.
In our example, there are four line ranges as indicated by
the boxes. We draw lines between corresponding line ranges,
which are labelled with =for matching pairs and with 6=for
diff pairs. ([V1];[V0
1])and([V3];[V0
3])form diff pairs, while
([V2];[V0
2])is the single matching pair. All violations have a
distance of zero from the start of their region, except for V0
3
which has (V0
3) = 1 .
Consider now a violation Vcin the child snapshot at location
lc, and a violation Vpin the parent snapshot at location lp,
where the Ô¨Åles containing lcandlphave the same path. If
their ranges correspond, we try to match them up as follows:
1) If ([lc];[lp])is a matching pair, then the violations are
triggered by code that did not change between Spand
Sc, so we only match them up if their positions within
the region are exactly the same, i.e., (lc) = (lp).
In our example, there is only one matching pair: the one
containingV2andV0
2, respectively. Since their positions
in the region are the same, they are matched up.
2) If, on the other hand, ([lc];[lp])is a diff pair, we allow
the locations to vary slightly: VcandVpare considered
to match ifj(lc) (lp)jfor some threshold value
. In our implementation, we use = 3.
For example, consider V3on the left and V0
3on the right.
Their regions form a diff pair, and (V3) = 0 whereas
(V0
3) = 1 ; since their distance is less than three lines,
we match them up.
B. Snippet-based violation matching
Since location-based matching requires violation locations
to belong to corresponding line ranges, this matching strategy
will fail for violations like V1whose location has changed
signiÔ¨Åcantly between snapshots. We use an additional strategy
to catch simple cases where a violation has moved in the same
Ô¨Åle, but is triggered by exactly the same snippet of code: if lc
andlpbelong to the same Ô¨Åle and the source text of lcandlp
(i.e., the text between the respective start and end positions)
is the same, we match them up. In the example, this allows
us to also match up V1andV0
1.
C. Hash-based violation matching
Neither location-based matching nor snippet-based match-
ing apply to violations in Ô¨Åles that are renamed or moved
between snapshots. To match up those violations, we employa hash-based strategy that tries to match violations based on
the similarity of their surrounding code.
SpeciÔ¨Åcally, for a violation V= (S;l;t ), we compute two
hash values h<(V)andh>(V): the former is computed from
thentokens up to and including the Ô¨Årst token in l, and the
latter is computed from the ntokens starting with the Ô¨Årst
token inl, wherenis a Ô¨Åxed threshold value (by default, we
usen= 100 ). Two violations VcandVpare considered to
match ifh<(Vc) =h<(Vp)orh>(Vc) =h>(Vp).
If the location lstarts less than ntokens into the Ô¨Åle,
h<(V)is undeÔ¨Åned; similarly, h>(V)is only deÔ¨Åned if l
starts at least ntokens before the end of the Ô¨Åle. This avoids
spurious matches due to short token sequences, but it makes
it necessary to use two hashes, since otherwise violations near
the beginning or the end of a Ô¨Åle could never be matched.
IV. A TTRIBUTING VIOLATIONS
Now that we have discussed how to match violations be-
tween two snapshots, let us consider the problem of attributing
a snapshotS, that is, computing the sets of new and Ô¨Åxed
violations.
Clearly, a snapshot is only attributable if both it and all its
parent revisions can be analysed. As a special case, if Sdoes
not have any parents, we do not consider it attributable, since
it may contain code copied from other sources and we know
nothing about the history of the violations in this code.
Let us Ô¨Årst consider the case where Shas precisely one
parent snapshot P. We write V(S)andV(P)for the sets
of violations of SandP, respectively. Using the matching
strategies described earlier, we can determine for any two
violations whether they match.
In general, a single violation in V(S)may match more than
one violation in V(P)and vice versa. To decrease the chance
of accidentally matching up unrelated violations, we prioritise
our matching strategies as follows:
1) Apply location-based matching Ô¨Årst. If v2V(S)is
located in a diff range and may match more than one
violation in V(P), choose the closest one.
2) Only use snippet-based matching if diff-based matching
fails. If more than one violation in V(P)has the same
source text as v, choose the closest one.
3) Only use hash-based matching if location-based match-
ing and snippet-based matching both fail. If there aretwo or more violations in V(S)orV(P)that have the
same hash, exclude them from the matching.4
Furthermore, once a violation from V(P)has been matched
up with a violation from V(S), we exclude it from further
consideration. In this way, we obtain a matching relation
that matches every violation in V(S)with at most one
violation in V(P)and vice versa.
Now the new violations in Sare simply those for which
there is no matching violation in P:
N(S) :=fv2V(S)j:9v02V(P):vv0g
Dually, the set of Ô¨Åxed violations in Sare those violations
inPfor which there is no matching violation in S:
F(S) :=fv02V(P)j:9v2V(S):vv0g
Note that according to this deÔ¨Ånition, simply deleting a
piece of code Ô¨Åxes all the violations in it.
Merge commits have more than one parent. Generalising
the deÔ¨Ånition of N(S)to this case poses no problems, but
generalising F(S)is not so straightforward.5Thus, we choose
not to deÔ¨Åne any new or Ô¨Åxed violations for merge commits.
Finally, let us consider how to efÔ¨Åciently attribute multiple
snapshots. A common use case would be to attribute every
snapshot in the entire revision history of a project, or every
snapshot within a given time window.
Since all parents of a snapshot need to be analysed before
attribution is possible, we could perform a breadth-Ô¨Årst traver-
sal of the revision graph: start by analysing the Ô¨Årst snapshot
and all its children; assuming that all of them are analysable,
the child snapshots can then be attributed. Now analyse all of
their children for which all parents have been analysed in turn
and attribute them, and so on.
In practice, however, later revisions are often more inter-
esting: users normally Ô¨Årst want to understand recent changes
in code quality before they turn to historic data. Also, older
snapshots are more likely to be unanalysable due to missing
dependencies, so attribution may not even be possible in many
cases. This is why Team Insight attributes snapshots in reverse
chronological order.
Since analysing and attributing a large number of revisions
can take a very long time, the analysis and attribution tasks
need to be parallelised as much as possible. To this end,
Team Insight splits up all analysable snapshots into attribution
chunks : an attribution chunk consists of a set Aof snapshots to
attribute together with a set Uof supporting snapshots such
4Note that this case only arises if the violation has disappeared from its
original Ô¨Åle (since both location-based matching and diff-based matching fail),
and multiple new copies of the violation have appeared elsewhere. This is most
commonly caused by a piece of code (containing the violation) being cut from
its own Ô¨Åle and pasted into multiple other Ô¨Åles. By avoiding a match, we
force this to be considered as a single Ô¨Åxed violation and multiple introduced
violations, which seems like the most appropriate way to model it.
5F(S)consists of violations in the parent snapshot, and so for merge
commits we would have to identify corresponding violations across all parent
snapshots. This is made more difÔ¨Åcult by the fact that our deÔ¨Ånition of hash-
based matching is not transitive (since only one hash needs to be equal to
establish a match), so all pairs of parent snapshots would have to be compared
against each other.that for each snapshot in Aall of its parents are contained
in eitherAorU. Clearly, once all snapshots in A[Uhave
been analysed, all the snapshots in Acan be attributed without
referring to any other snapshots. Thus, different attribution
chunks can be processed in parallel.
V. D EVELOPER FINGERPRINTING
We now discuss an important application of attribution
information: computing violation Ô¨Ångerprints to characterise
which kinds of violations developers tend to introduce or Ô¨Åx.
We represent the violation Ô¨Ångerprint for a developer by
two vectors, one containing information about introduced
violations and one about Ô¨Åxed violations. Each vector has
one component per violation type, where the component
corresponding to some violation type tells us how often the
developer introduces or Ô¨Åxes violations of that type.
Given a set of attributed snapshots, we can compute a
Ô¨Ångerprint for every developer by simply summing up the
number of introduced and Ô¨Åxed violations of every type over
all the snapshots authored by the given developer.
However, such uncalibrated Ô¨Ångerprints are difÔ¨Åcult to com-
pare between developers: a developer who has changed more
lines of code is, in general, expected to have introduced and
Ô¨Åxed more violations than a developer with fewer contribu-
tions. Thus, it makes sense to scale the components of each
vector to account for such differences in productivity.
We consider two scaling factors:
1) Scaling by churn, where the violation counts are divided
by the total number of lines of code changed by the
developer across the set of considered snapshots.
2) Scaling by total number of violations, where the viola-
tion counts are divided by the total number of new/Ô¨Åxed
violations of the developer. This provides an overview
of what portion of the violations a developer introduces
or Ô¨Åxes are of a certain type.
As an example, assume we have two developers d1and
d2, and we have a set of snapshots where violations of two
types have been attributed. If d1has introduced a total of 10
violations of the Ô¨Årst type and Ô¨Åxed none, while introducing
5 violations of the second type and Ô¨Åxing 2, her uncalibrated
violation Ô¨Ångerprint is h(10;5);(0;2)i. Similarly, if the Ô¨Ånger-
print ofd2ish(4;1);(0;4)i, then this means that he introduced
four violations of the Ô¨Årst type and one of the second type,
while Ô¨Åxing four violations of the second type, but none of
the Ô¨Årst type.
Given these uncalibrated Ô¨Ångerprints, it may be tempting to
deduce that d1introduces more violations of the Ô¨Årst type
thand2. If, however, d1contributed 50000 lines of churn
whiled2only touched 1000 lines of code, this conclusion
is unwarranted: d1contributed 50 times as much code than
d2, but only introduced about twice as many violations of
the Ô¨Årst type. Scaling the Ô¨Ångerprints by the amount of
churn (in thousands of lines of code) d1‚Äôs Ô¨Ångerprint be-
comesh(0:2;0:1);(0;0:04)i, whiled2‚Äôs Ô¨Ångerprint is still
h(4;1);(0;4)i: this shows that, relatively speaking, d2is muchName Language # Snapshots Size (KLOC) Churn (KLOC) Total New Total Fixed
Hadoop Common [8] Java 27086 1200 6206 51294 20212
MongoDB [25] C++ 5057 429 536 2721 2075
Spark [9] Scala 7226 75 835 8179 3960
Gaia [10] JavaScript 27024 560 4468 75337 75938
TABLE I
PROJECTS USED IN THE EVALUATION
more likely to introduce violations of both types, but is also
more likely to Ô¨Åx violations of the second type.
If, instead, we want to compare how many of the Ô¨Åxed/in-
troduced violations of a developer are of a certain type we
can scale by the total number of Ô¨Åxed/introduced violations:
overall, developer d1introduced 15 violations and Ô¨Åxed two,
so her Ô¨Ångerprint becomes h(2
3;1
3);(0;1)i; developerd2intro-
duced Ô¨Åve and Ô¨Åxed four, giving the (very similar) Ô¨Ångerprint
h(0:8;0:2);(0;1)i.
Which kind of Ô¨Ångerprint is more useful depends on the
application area. Fingerprints scaled by churn are useful to
compare different developers, and could, for instance, be used
to Ô¨Ånd out which team member is most adept at Ô¨Åxing a
given kind of violation. Fingerprints scaled by the number
of violations, on the other hand, compare a single developer‚Äôs
performance in different areas, and could hence be used to
select appropriate training.
VI. E VALUATION
We now report on an evaluation of our violation matching
approach and the developer Ô¨Ångerprinting on four large open-
source projects with signiÔ¨Åcant revision histories. For the
violation matching, we investigate the quality of the match-
ings produced, and the relative importance of the different
matching strategies. For the Ô¨Ångerprinting, we assess whether
Ô¨Ångerprints are, in fact, characteristic of individual developers.
A. Evaluation subjects
As our evaluation subjects, we chose the four open-source
projects Hadoop Common, MongoDB, Spark and Gaia, as
shown in Table I. For each of our subjects, the table shows
the language they are implemented in; the total number
of snapshots that were attributed; the approximate size (in
thousand lines of code) of the latest attributed snapshot; the
total amount of churn across all attributed snapshots; and the
total number of new and Ô¨Åxed violations.6
To Ô¨Ånd violations, we used the default analysis suites of
our tool Project Insight, comprising 191 analyses for Java,
94 for C++, 115 for Scala, and 73 for JavaScript. The raw
analysis results our experiments are based on are available
from http://semmle.com/publications .
B. Evaluating violation matching
We evaluate our violation matching algorithm with respect
to two evaluation criteria:
6Note that for Gaia there are more Ô¨Åxed than new violations; this is because
some violations were introduced in unattributable revisions.Project Exact Fuzzy Snippet Hash
Hadoop 133,000,488 46,880 4,505 14,369
99.95% 0.04% 0.00% 0.01%
MongoDB 52,432,554 21,551 2,044 21,239
99.91% 0.04% 0.00% 0.04%
Spark 22,891,706 61,949 1,967 43,379
99.53% 0.27% 0.01% 0.19%
Gaia 63,855,193 28,830 6,280 39,154
99.88% 0.05% 0.01% 0.06%
TABLE II
VIOLATION MATCHINGS CONTRIBUTED BY INDIVIDUAL ALGORITHMS
EC1 How many violation matchings are contributed by
the different matching algorithms?
EC2 Do these violation matchings match up violations
that actually refer to the same underlying defect?
To answer EC1 , we randomly selected 5000 snapshots from
each of our subject programs and counted how many violation
matchings were contributed by each of the algorithms. The
results are shown in Table II: for the location-based violation
matching, we distinguish between exact matches (column
‚ÄúExact‚Äù) and matches in diff regions that are no further than
three lines apart (column ‚ÄúFuzzy‚Äù); columns ‚ÄúSnippet‚Äù and
‚ÄúHash‚Äù refer to the snippet-based matching and hash-based
matching algorithms, respectively.
As one might expect, the overwhelming majority of match-
ings are exact: usually, a single commit only touches a small
number of Ô¨Åles, so almost all violations remain unaffected.
Most of the remaining matchings are found by the fuzzy
matching algorithm, which applies if there were changes
within the same Ô¨Åle that do not affect the violation itself,
but only shift it by a few lines. Snippet-based matching only
applies in a few cases, while the contribution of the hash-based
algorithm varies considerably between projects: it contributes
very little on Hadoop, but is more important than the fuzzy
matching algorithm on Gaia.7
This suggests that exact matching may, in practice, be
enough for many applications. For our use case of attributing
violations to developers, however, this is not so: on Mon-
goDB, for instance, hash-based matching contributes 21239
matchings. This number is vanishingly small when compared
to the total number of matched violations, but it is ten
times the total number of Ô¨Åxed violations identiÔ¨Åed across
all attributed snapshots. Without hash-based matching, each
missing matchings would give rise to one new violation and
one Ô¨Åxed violation, dramatically inÔ¨Çuencing the overall result.
7Recall that the different algorithms are applied in stages, where more
sophisticated algorithms are only run for those violations that could not be
matched using the simpler algorithms.As for EC2 , it can ultimately only be answered by do-
main experts manually examining a large number of violation
matchings and deciding whether they are reasonable or not.
In lieu of a large-scale experiment (the outcome of which
would, in the light of [21], be of doubtful signiÔ¨Åcance) we
manually inspected 100 randomly selected hash-based match-
ings from each of our four subject programs. Most of these
400 matchings corresponded to Ô¨Åle renames or moves that
were conÔ¨Årmed either by the snapshot‚Äôs commit message or
by version control metadata. The remainder were due to code
being copied between Ô¨Åles, or moved within a Ô¨Åle with minor
changes being performed at the same time, thus preventing
the violations from being matched up by the snippet-based
algorithm. None of the matchings were obviously wrong.
We have not yet performed a comprehensive performance
evaluation of our violation matching approach. However, we
observed during our experiments that the location-based and
snippet-based algorithms each take about three to four mil-
liseconds to compare two Ô¨Åles. The hash-based algorithm is
global and hence more expensive, taking around four seconds
to compute matchings for a pair of snapshots.
C. Evaluating Ô¨Ångerprinting
The other main focus of our evaluation is to examine
how meaningful our violation Ô¨Ångerprints are. Recall that
Ô¨Ångerprints are computed from a set of attributed snapshots. If
Ô¨Ångerprints for the same developer vary wildly across different
sets of snapshots, we would have to conclude that they are not
well-deÔ¨Åned. Conversely, if different developers are assigned
very similar Ô¨Ångerprints, this would mean that Ô¨Ångerprints are
not characteristic. Finally, even if violation Ô¨Ångerprints are
characteristic, we have to show that they do not simply reÔ¨Çect
more basic characteristics of a developer such as average churn
per commit, or average number of violations per commit.
We distill these considerations into three evaluation criteria:
EC3 Are Ô¨Ångerprints stable across different snapshot sets?
EC4 Is there a measurable difference between the Ô¨Ånger-
prints computed for different developers?
EC5 Are violation Ô¨Ångerprints independent of churn and
total number of new and Ô¨Åxed violations?
To answer these questions, we designed an experiment in
which we take two sets AandBof snapshots and compute for
every developer da violation Ô¨Ångerprint f(d;A)based on the
snapshots in set A(the training set), and a violation Ô¨Ångerprint
f(d;B)based on the snapshots in set B(the test set). Now
we compare f(d;A)against the Ô¨Ångerprints f(d0;B)ofall
developers (including d) as computed from set Band rank
them by their Euclidean distance kf(d;A) f(d0;B)kfrom
the Ô¨Ångerprint of d.8
If Ô¨Ångerprints are highly dependent on the set of snapshots
used to compute them ( EC3 ), we would expect the outcome of
this ranking to be mostly random. Similarly, if all developers
8Recall that Ô¨Ångerprints are pairs of vectors; to determine their distance,
we simply concatenate the two constituent vectors into one larger vector, and
then compute their Euclidean distance.tend to have similar Ô¨Ångerprints ( EC4 ), a random outcome
would be expected.
To address EC5 , we perform our experiments with two
kinds of Ô¨Ångerprints: violation density Ô¨Ångerprints and viola-
tion vector Ô¨Ångerprints scaled by total violations. The former
are two-element vectors containing the total number of new
violations and the total number of Ô¨Åxed violations, scaled
by the number of lines changed. The latter are vectors with
one element per violation type, scaled by the total number of
Ô¨Åxed/new violations as described in Section V. If developers,
on average, introduce and Ô¨Åx the same number of violations
per line of code they touch, the ranking using violation density
Ô¨Ångerprints should be random, since these Ô¨Ångerprints are
scaled by churn. Similarly, if developers introduce and Ô¨Åx
different kinds of violations with the same frequency, the
violation vector Ô¨Ångerprints would produce random rankings,
since they are scaled by the total number of violations.
Care has to be taken in selecting the snapshot sets AandB
and in choosing which developers to compute Ô¨Ångerprints for.
We exclude commits with less than 10 lines of churn (since
they most likely have too few Ô¨Åxed or new violations to be
interesting), and commits with more than 10000 lines of churn
(since they are not likely to actually be the work of a single
developer). We also do not include commits by developers
who have contributed fewer than 50 commits or less than 5000
lines of churn, since their contributions are likely too small to
derive a signiÔ¨Åcant Ô¨Ångerprint from.
Overall, our experiment comprises the following steps:
1) Randomly partition the set of all considered snapshots
into two halves AandB.
2) Compute Ô¨Ångerprints for all developers on AandB, and
compute ranks as explained above.
3) For every kind of Ô¨Ångerprint, count how often a de-
veloper‚Äôs Ô¨Ångerprint was ranked as being closest to
themselves, second-closest to themselves, and so on.
To enhance statistical signiÔ¨Åcance, we perform these steps
100 times on every test subject and aggregate the counts. The
results of these experiments are shown in Figure 5, Figure 6,
Figure 7, and Figure 8. Every Ô¨Ågure contains two histograms,
showing the results for the violation density Ô¨Ångerprint on
the left, and for the violation vector Ô¨Ångerprint on the right.
The individual bars show how often (over the 100 runs of
the experiment) a developer was ranked as being closest to
themselves, second-closest to themselves, and so on.9
We note that none of the experiments yield a random
distribution. Instead, both kinds of Ô¨Ångerprints consistently
rank developers as similar to themselves and dissimilar to
others, thus suggesting a positive answer to EC3 andEC4 .
Since this is, in particular, true for the violation density
Ô¨Ångerprints on all four subject programs, we conclude that the
ratio of introduced and Ô¨Åxed violations per lines of changed
code is not the same for all developers. The results are
even more striking for the violation vector Ô¨Ångerprints: our
experiments give a strong indication that each developer has
9Note that for readability the two graphs are not on the same scale.0 10 20 30 40 5050100150200
0 10 20 30 40 5001;0002;000
violation density Ô¨Ångerprints violation vector Ô¨Ångerprints
Fig. 5. Ranking results for Hadoop Common
0 2 4 6 8 10 12 14 1650100150
0 2 4 6 8 10 12 14 160200400600
violation density Ô¨Ångerprints violation vector Ô¨Ångerprints
Fig. 6. Ranking results for MongoDB
024681012141618100200
0246810121416180200400600
violation density Ô¨Ångerprints violation vector Ô¨Ångerprints
Fig. 7. Ranking results for Spark
0 10 20 30 40 50 600100200300
0 10 20 30 40 50 6001;0002;000
violation density Ô¨Ångerprints violation vector Ô¨Ångerprints
Fig. 8. Ranking results for Gaiaa very characteristic signature in terms of the violations they
introduce and Ô¨Åx as a proportion of their overall number of
violations. EC5 can hence also be answered in the afÔ¨Årmative.
These results hold for all our subject programs. The com-
paratively weak results for MongoDB and Spark are most
likely due to the relatively sparse data: across the snapshots
we analysed, there were only 15 developers on MongoDB and
18 on Spark who accumulated enough churn and commits to
be considered in our experiments (compared to 52 on Hadoop
and 59 on Gaia), which makes the results less stable.
D. Threats to validity
Our static analysis examines source code as it is seen by the
compiler during a build, including generated code. Attributing
violations in generated code code is, however, not straight-
forward, so we manually excluded it from consideration on a
best-effort basis. Given the size and complexity of our subject
programs, we may have missed some Ô¨Åles, which could affect
our results. Another difÔ¨Åculty are bulk imports of third-party
code. Our experiments account for this by excluding revisions
with large amounts of churn, but this is only a heuristic.
Furthermore, we ran our static analysis with its standard
rule set. Using different rules might conceivably lead to a
different outcome, but preliminary experiments with subsets
of the standard rule set yielded similar conclusions.
In computing Ô¨Ångerprints, we only consider developers who
have contributed at least 50 commits and 5000 lines of churn.
We have not yet experimented with varying these thresholds.
We used violation density Ô¨Ångerprints and violation vector
Ô¨Ångerprints in our experiments. Many other ways of com-
puting Ô¨Ångerprinting could be devised, and some of them
may well be even more characteristic of individual developers
than the ones we used. For example, a syntactic Ô¨Ångerprint
based on preferred indentation style would probably be highly
characteristic. However, such shallow Ô¨Ångerprints do not yield
much insight into the contributions a developer makes to the
software quality of a code base, which is our main interest.
Finally, we note that care has to be taken in generalising our
results to other projects. We deliberately chose diverse projects
utilising different programming languages. Although they are
open source, all four projects have core developers from
large software companies. Therefore, we expect our results
to generalise to both commercial and open source projects.
VII. R ELATED WORK
Spacco et al. [33] discuss two location-based violation
matching techniques used in FindBugs [28] and Fortify [7]. To
allow for code movement, they relax their location matching in
various ways, for example by matching violations at different
locations within the same method, and by only considering the
name (but not the full path) of the enclosing Ô¨Åle. As in our
approach, they prioritise stricter matching criteria over more
relaxed ones. However, their approach cannot match violations
that have moved between methods or even classes.
Diff-based matching of code snippets has been used to track
violations forward [3] and Ô¨Åxed bugs backward [20] throughrevision history, as well as for tracking code clones [19].
¬¥Sliwerski et al. [32] directly use revision control metadata
to pair up Ô¨Åx inducing commits with later bug Ô¨Åxes. Kim
et al. [22] improve upon this by adding a form of diff-based
matching similar to our approach.
Hash-based matching does not seem to have been used for
tracking static analysis violations before, but the technique
itself is well established in multi-version analysis [18]. Clone
detectors, in particular, often use hashes to identify potential
code clones within a given code base. Our hashes are token-
based, similar to the one used by PMD‚Äôs code clone detec-
tor [30]. Other systems use more advanced similarity metrics
based on the AST [2] or the PDG [23], which are less scalable.
In origin analysis [12], [13], whole functions are hashed
based on attributes such as cyclomatic complexity as well as
call graph information. Our methods for violation matching
strive to be language and analysis independent, and hence
cannot directly employ such advanced hash functions.
Violation Ô¨Ångerprints as deÔ¨Åned in this paper appear to be
novel. Previous work has considered other developer character-
istics such as code ownership, that is, how much experience
a developer has with a piece of code [11], [14]. Typically,
these characteristics are computed entirely from source control
information without any static analysis.
Developer Ô¨Ångerprints based on layout information, lexical
characteristics and software metrics have been employed in
software forensics to identify authors of un-attributed code [5],
[24], [34]. Spafford et al. [34] suggest considering typical bugs
as well. Judging from our dataset, this seems difÔ¨Åcult, since
most revisions introduce or Ô¨Åx at most one or two violations,
which is insufÔ¨Åcient to derive a meaningful Ô¨Ångerprint.
VIII. C ONCLUSION
We have motivated the need for enriching static analysis re-
sults with revision information to track changes in code quality
over time, and attribute changes to individual developers. The
main enabling technique for such an integration is violation
tracking, which determines new and Ô¨Åxed violations in a
revision relative to its parent revisions. We have discussed one
approach for implementing violation tracking, and validated it
on several substantial open-source projects written in different
programming languages. We furthermore demonstrated that
developers have characteristic Ô¨Ångerprints of violations they
tend to introduce and Ô¨Åx.
It has been observed that static analysis violations are often
ignored by developers [16]. Some organisations have tried
to address this by imposing a commit gate that enforces
adherence to coding rules, but experience with our clients
has shown that this is counter-productive: sometimes busi-
ness circumstances necessitate the introduction of technical
debt, which manifests itself through an increased number of
violations [36]. Fingerprints, on the other hand, allow each
individual to keep track of their own coding habits, of where
they are doing well and where they can improve, thus helping
to establish an esprit de corps among a team of developers.REFERENCES
[1] Nathaniel Ayewah, David Hovemeyer, J. David Morgenthaler, John
Penix, and William Pugh. Using Static Analysis to Find Bugs. IEEE
Software , 25(5), 2008.
[2] Ira D. Baxter, Andrew Yahin, Leonardo Mendon√ßa de Moura, Marcelo
Sant‚ÄôAnna, and Lorraine Bier. Clone Detection Using Abstract Syntax
Trees. In ICSM , 1998.
[3] Cathal Boogerd and Leon Moonen. Evaluating the Relation Between
Coding Standard Violations and Faults Within and Across Software
Versions. In MSR , 2009.
[4] Coverity. Code Advisor. http://www.coverity.com, 2015.
[5] Haibiao Ding and Mansur H. Samadzadeh. Extraction of Java Program
Fingerprints for Software Authorship IdentiÔ¨Åcation. Journal of Systems
and Software , 72(1), 2004.
[6] Alex Eagle and Eddie Aftandilian. error-prone. https://code.google.com/
p/error-prone, 2015.
[7] HP Fortify. Static Code Analyzer. http://fortify.com, 2015.
[8] Apache Foundation. Hadoop. http://hadoop.apache.org, 2015.
[9] Apache Foundation. Spark. http://spark.incubator.apache.org, 2015.
[10] Mozilla Foundation. Gaia. https://github.com/mozilla-b2g/gaia, 2015.
[11] Tudor G√Ærba, Adrian Kuhn, Mauricio Seeberger, and St√©phane Ducasse.
How Developers Drive Software Evolution. In IWPSE , 2005.
[12] Michael W. Godfrey and Qiang Tu. Tracking Structural Evolution Using
Origin Analysis. In IWPSE , 2002.
[13] Michael W. Godfrey and Lijie Zou. Using Origin Analysis to Detect
Merging and Splitting of Source Code Entities. IEEE TSE , 31(2), 2005.
[14] Lile Hattori, Michele Lanza, and Romain Robbes. ReÔ¨Åning Code
Ownership with Synchronous Changes. ESE, 17(4-5), 2012.
[15] James W. Hunt and Thomas G. Szymanski. A Fast Algorithm for
Computing Longest Common Subsequences. CACM , 20(5), 1977.
[16] Brittany Johnson, Yoonki Song, Emerson R. Murphy-Hill, and Robert W.
Bowdidge. Why Don‚Äôt Software Developers Use Static Analysis Tools
to Find Bugs? In ICSE , 2013.
[17] Toshihiro Kamiya, Shinji Kusumoto, and Katsuro Inoue. CCFinder: A
Multilinguistic Token-based Code Clone Detection System for Large
Scale Source Code. IEEE TSE , 28(7), 2002.
[18] Miryung Kim and David Notkin. Program Element Matching for Multi-
Version Program Analyses. In MSR , 2006.
[19] Miryung Kim, Vibha Sazawal, David Notkin, and Gail C. Murphy. An
Empirical Study of Code Clone Genealogies. In FSE, 2005.[20] Sunghun Kim and Michael D. Ernst. Which Warnings Should I Fix
First? In FSE, 2007.
[21] Sunghun Kim, Kai Pan, and E. James Whitehead, Jr. When Functions
Change Their Names: Automatic Detection of Origin Relationships. In
WCRE , 2005.
[22] Sunghun Kim, Thomas Zimmermann, Kai Pan, and E. James Whitehead
Jr. Automatic IdentiÔ¨Åcation of Bug-Introducing Changes. In ASE, 2006.
[23] Raghavan Komondoor and Susan Horwitz. Using Slicing to Identify
Duplication in Source Code. In SAS, 2001.
[24] Ivan Krsul and Eugene H. Spafford. Authorship Analysis: Identifying
The Author of a Program. Computers & Security , 16(3), 1997.
[25] MongoDB, Inc. MongoDB. http://www.mongodb.org, 2015.
[26] Eugene W. Myers. An O(ND) Difference Algorithm and Its Variations.
Algorithmica , 1, 1986.
[27] Nachiappan Nagappan and Thomas Ball. Static Analysis Tools as Early
Indicators of Pre-Release Defect Density. In ICSE , 2005.
[28] University of Maryland. FindBugs. http://Ô¨Åndbugs.sourceforge.net,
2015.
[29] H.M. Olague, L.H. Etzkorn, S. Gholston, and S. Quattlebaum. Empirical
Validation of Three Software Metrics Suites to Predict Fault-Proneness
of Object-Oriented Classes Developed Using Highly Iterative or Agile
Software Development Processes. IEEE TSE , 33(6), 2007.
[30] PMD Source Code Analyzer. http://pmd.sf.net, 2015.
[31] Semmle. Project Insight. http://semmle.com, 2015.
[32] Jacek ¬¥Sliwerski, Thomas Zimmermann, and Andreas Zeller. When Do
Changes Induce Fixes? In MSR , 2005.
[33] Jaime Spacco, David Hovemeyer, and William Pugh. Tracking Defect
Warnings Across Versions. In MSR , 2006.
[34] Eugene H. Spafford and Stephen A. Weeber. Software Forensics: Can
We Track Code to its Authors? Computers & Security , 12(6), 1993.
[35] Ramanath Subramanyam and M. S. Krishnan. Empirical Analysis of
CK Metrics for Object-Oriented Design Complexity: Implications for
Software Defects. IEEE TSE , 29(4), 2003.
[36] Antonio Vetro‚Äô. Using Automatic Static Analysis to Identify Technical
Debt. In ICSE , 2012.
[37] Antonio Vetro‚Äô, Maurizio Morisio, and Marco Tochiano. An Empirical
Validation of FindBugs Issues Related to Defects. In EASE , 2011.
[38] Jiang Zheng, Laurie A. Williams, Nachiappan Nagappan, Will Snipes,
John P. Hudepohl, and Mladen A. V ouk. On the Value of Static Analysis
for Fault Detection in Software. IEEE TSE , 32(4), 2006.