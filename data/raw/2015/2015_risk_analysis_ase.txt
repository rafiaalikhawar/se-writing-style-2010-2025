Predicting Delays in Software Projects Using
Networked Classiﬁcation
Morakot Choetkiertikul⇤, Hoa Khanh Dam⇤, Truyen Tran†and Aditya Ghose⇤
⇤School of Computing and Information Technology
University of Wollongong, Australia
Email: {mc650,hoa,aditya }@uow.edu.au
†School of Information Technology
Deakin University, Australia
Email: truyen.tran@deakin.edu.au
Abstract —Software projects have a high risk of cost and
schedule overruns, which has been a source of concern for the
software engineering community for a long time. One of the
challenges in software project management is to make reliable
prediction of delays in the context of constant and rapid changes
inherent in software projects. This paper presents a novel ap-
proach to providing automated support for project managers and
other decision makers in predicting whether a subset of software
tasks (among the hundreds to thousands of ongoing tasks) in
a software project have a risk of being delayed. Our approach
makes use of not only features speciﬁc to individual software
tasks (i.e. local data) – as done in previous work – but also their
relationships (i.e. networked data). In addition, using collective
classiﬁcation, our approach can simultaneously predict the degree
of delay for a group of related tasks. Our evaluation results
show a signiﬁcant improvement over traditional approaches
which perform classiﬁcation on each task independently : achieving
46%–97% precision (49% improved), 46%–97% recall (28%
improved), 56%–75% F-measure (39% improved), and 78%–
95% Area Under the ROC Curve (16% improved).
I. I NTRODUCTION
Delays constitute a major problem in software projects
[1]. Approximately one-third of IT projects went over the
scheduled time, according to a recent study by Mckinsey
and the University of Oxford in 2012 on 5,400 large scale
IT projects [2]. In the Standish Group’s well-known CHAOS
report [3], the proportion of delayed projects recorded were
even higher – at 82%. These studies have also shown that
ineffective risk management is one of the main reasons for the
high rate of overrun software projects. An important aspect of
risk management is the ability to predict, at any given stage in
a project, which tasks (among hundreds to thousands tasks) are
at risk of being delayed. Foreseeing such risks allows project
managers to take prudent measures to assess and manage the
risks, and consequently reduce the chance of their project being
delayed.
Making a reliable prediction of delays is therefore an
important capability for project managers, especially when
facing with the inherent dynamic nature of software projects
(e.g. constant changes to software requirements). Current prac-
tices in software risk management however rely mostly on
high-level, generic guidance (e.g. Boehm’s “top 10 list of
software risk items” [4] or SEI’s risk management framework
[5]) or highly subjective expert judgements. Thus, there is astrong need for providing automated, contextual support for
identifying risks of delay in software projects.
In order to address that need, a number of recent proposals
have leveraged data mining and machine learning technologies
to predict delays in resolving software issues (e.g. [6]) or to
estimate the ﬁx time of a software bug (e.g. [7–11]). In line
with a large body of work in data mining for software engi-
neering, these approaches employ traditional machine learning
classiﬁcation techniques to perform classiﬁcation on each issue
or bug independently using its attributes or features. Such
approaches do nottake into account the role of the underlying
network of inter-relationships between the tasks of resolving
those bugs or issues. This is a gap, given the preponderance of
task dependencies in software projects – approximately 57%
of 11,851 tasks in the ﬁve open source projects selected for
our study were related to at least one other task. These task
dependencies form the networked data that we will seek to
leverage in this work.
Networked data are seen in many different forms in our
daily life, such as hyperlinked Web pages, social networks,
communication networks, biological networks and ﬁnancial
transaction networks. They are used in various applications
such as classifying Web pages [12], scientiﬁc research papers
[13, 14], protein interaction and gene expression data [15].
We demonstrate that a similar class of networked data (i.e.
networked tasks) can also provide valuable information for
predicting delays in software projects. For example, if a
task blocks another task and the former is delayed, then
the latter is also at risk of getting delayed. This example
demonstrates a common delay propagation phenomenon in
(software) projects, which has not been considered by previous
approaches.
We propose a novel approach to leverage task dependencies
for predicting delays in software projects. This paper makes
two main contributions:
• A technique for constructing a task network of
software development tasks
This technique enabled us to extract various relation-
ships between tasks in software projects to build a
task network. Task relationships can be explicit (those
that are explicitly speciﬁed in the task records) or
implicit (those that need to be inferred from other task
information). Explicit relations usually determine theorder of tasks, while implicit relations reﬂect other
aspects such as tasks assigned to the same developer,
tasks affecting the same software component or similar
tasks.
• Predictive models to predict delays in software
projects.
We developed accurate predictive models that can
predict whether a software task is at risk of getting de-
layed. Our predictive models have three components:
local classiﬁer ,relational classiﬁer andcollective in-
ference . The local classiﬁer uses 15non-relational (i.e.
local) features of a software task: discussion time,
waiting time, task type, number of repetition tasks,
percentage of delayed tasks that a developer involved
with, developer’s workload, task priority, number of
comments, changing of priority, number of ﬁx version,
number of affect version, changing of description,
number of votes, number of watches and reporter
reputation. The relational classiﬁer makes use of the
task relations in a task network to predict if a task
gets delayed based on the delay information of its
neighbors. Finally, collective inference allows us to
make such a prediction simultaneously for multiple re-
lated tasks. The performance of our predictive models
were evaluated on ﬁve different open source projects.
We achieved 46%-97% precision, 46%-97% recall,
56%-75% F-measure, and 78%-95% Area Under the
ROC Curve. The evaluation results compared to the
traditional approaches using only local classiﬁers show
a signiﬁcant improvement: it improves 49% precision,
28% recall, 39% F-measure, and 16% Area Under the
ROC Curve over the traditional approaches.
The remainder of this paper is organized as follows. Section
II describes a motivating example and Section III provides an
overview of our approach. Section IV serves to describe how a
task network is built for a software project. Section V presents
our networked predictive models. Section VI reports the exper-
imental evaluations of our approach. Threats to validity of our
study are discussed in Section VII. Related work is discussed
in Section VIII before we conclude and outline future work in
Section IX.
II. M OTIVATING EXAMPLE
Typically, a (software) project requires a number of activi-
ties or tasks be completed. Each task usually has an estimated
due date. It is important for project managers to ensure as many
tasks be completed in time (i.e. by their respective due date) as
possible since this has implications to the overall progress of
a project (e.g. releasing a major version on a speciﬁed date).
However, in practice project will never execute exactly as it
was planned due to uncertainty. For example, Figure 1 shows
seven tasks (represented by their ID) extracted from the JBoss1
project, only two of which (i.e. tasks JBIDE-1469 and JBAS-
14) were completed on time whilst there were three delayed
tasks (i.e. JBAS-15, JBWS-52 and JBOP-1). One of the main
challenges in project management is therefore predicting which
tasks have a risk of being delayed, giving the current situation
of a project, in order to come up with measures to reduce or
1http://www.jboss.orgmitigate such a risk. In this example, assume that we are trying
to predict if tasks JBAS-13 and JBAS-7 will be delayed.
JBIDE-1469JBAS-13JBOP-1JBAS-15JBAS-14JBWS-52blocksDelayed taskNon-delayed taskJBAS-7??Existing task?
Fig. 1: An example of task dependencies in the JBoss project
Recent work [6] has proposed 16 risk factors contributing
to the delay of a software task completion. Those factors reﬂect
a range of attributes associated with a software task (which are
referred to as local features ) such as task type, task priority, the
workload of developers assigned to the task, and so on. Based
on these risk factors, predict models were built to predict if
a task will be delayed. Like many tradition machine learning
methods in software engineering, the work in [6] has treated
tasks as being independent , which makes it possible to predict
delay risks on a task-by-task basis. For example, using the 16
local features, the approach in [6] predicted that tasks JBAS-13
would not be delayed.
In most cases, the tasks in a project are however related
to each other, and the delay status (e.g. major delayed, minor
delayed or non-delayed) of one task may have an inﬂuence
on that of its related task. For example, there are several
“blocking” relationships between the seven JBoss tasks in
Figure 1, e.g. task JBIDE-1469 blocks JBAS-15, indicating
that the former needs to be ﬁnished before the completion
of the latter. The task dependencies form the networked data
which contain interconnected tasks. Networked data provides
additional, valuable information which can be used to improve
the predictive performance of techniques solely relying local
features. For example, the local features are not sufﬁciently to
provide accurate prediction for task JBAS-13 – it was predicted
as non-delayed but it is in fact a delayed task. On the other
hand, by examining its relationships with other tasks whose
delay status are known – JBAS-13 were blocked by 3 delayed
tasks and 1 non-delayed task (see Figure 1) – we may be able
toinfer the risk of task JBAS-13 being delayed.
This example motivates the use of networked data to make
within-network estimation : tasks for which delay status (e.g.
delayed or non-delayed) is known are linked to tasks for which
the delay status must be predicted. Here, the data has an
important characteristics: tasks with known delay status are
useful in two aspects. They serves not only as training data
but also as background knowledge for the inference process.
Hence, the traditional separation of data into training and test
sets need to carefully take this important property into account.
In addition, networked tasks support collective classiﬁcation ,
i.e. the delay status of various related tasks can be predictedsimultaneously. For example, the prediction of task JBAS-13
can be used to inﬂuence the estimation of task JBAS-7 as they
are linked, thus we should do both predictions at the same
time.
This example only demonstrates links that are explicitly
speciﬁed in the tasks’ record. In practice, there are many
instances where tasks are related implicitly, e.g. assigned to the
same software engineers, affecting the same software compo-
nent, or similar to each other. Our approach is able to extract
different types of explicit and implicit task relationships, and
uses them for delay prediction.
III. O VERVIEW OF OUR APPROACH
Our approach leverages classiﬁcation techniques in ma-
chine learning to predict the riskiness of a task being de-
layed. A given task is classiﬁed into one of the classes in
{c1,c2,. . . ,c k}where each class cirepresents the risk impact in
terms of the degree of delay, e.g. major delayed, minor delayed
or non-delayed. Historical (i.e. completed) tasks are labeled,
i.e. assigned to a class membership, based on examining the
difference between their actual completion date and due date.
For example, in our studies tasks completed by their due date
are labeled as “non-delayed”, whilst tasks ﬁnished more than
60 days after their due date are labeled as “major delayed”.
The basic process of our approach is described in Figure 2.
The process has two main phases: the learning phase and the
execution phase. The learning phase involves using historical
data from past tasks to train classiﬁers, which are then used
for classify new tasks in the execution phase.
Our approach extracts data associated with software tasks
to build a task network which is deﬁned as below.
Deﬁnition 1 (Task network) .A task network is a directed
graph G = (V , E) where:
• each vertex v2Vrepresenting a software task in
the form of hID,c,attrs iwhere IDis a unique
identiﬁer of the task, cis the risk class, i.e. label (e.g.
non-delayed, minor delayed or major delayed), which
the task belongs to, and attrs is a set of the task’s
attribute-value pairs (attr i,va l i)(i.e. local features).
• each edge e2Erepresenting a link between tasks u
and vin the form of hhu, vi,t yp e s ,w e i gh t s iwhere
types is set of the link’s type and weigths is set of
link’s weight.
The set of tasks (or nodes) Vin a task network is further
divided into two disjoint groups: tasks with known class labels,
VK, and tasks whose labels need to be estimated (unknown
class), VUwhich VU=V\VK. Labeled tasks are used for
training, and also serve as background knowledge for inferring
the label of tasks in VU.
A set of attributes ( attrs ) for a task are also extracted
(see Table I). These features represents the local information
of each individual task in the network. The local features are
used to build a local classiﬁer which treats tasks independently
from each other. Traditional state-of-the-art classiﬁers (e.g.
Random Forest [16], Multiclass Support Vector Machines [17],TABLE I: Local features of a software task
Feature Short description
Discussion time The period that a team spends on ﬁnding solutions
to solve a task
Waiting time The time when a task is waiting for being acted
upon
Type Task type
Task repetition The number of times that an task is reopened
Priority Task priority
Changing of priority The number of times a task’s priority was changed
No. comments The number of comments from developers during
the discussion time
No. ﬁx versions The number of versions for which a task was or
will be ﬁxed
No. affect versions The number of versions for which a task has been
found
Changing of description The number of times in which the task description
was changed
Reporter reputation The measurement of the reporter reputation
Developer’s workload The number of opened tasks that have been as-
signed to a developer at a time
Per. of delayed tasks The percentage of delayed tasks in all of the tasks
which have been assigned to a developer
Number of votes The number of developers who voted a task
Number of watches The number of developers who watch a task
or Multiclass Logistic Regression [18]) can be employed for
this purpose.
The second important component in our approach is the
relational classiﬁer . Unlike the local classiﬁer, the relational
classiﬁer makes use of the relations between tasks in the
network (represented by edges) to estimate a task’s label using
the labels of its neighbors. Relational classiﬁer models exploit
a phenomenon that is widely seen in relational data: the label
of a node is inﬂuenced by the labels of its related nodes.
Relational classiﬁer models may also use local attributes of
the tasks. Links between tasks in the network are established
by extracting both explicit and implicit relations. Explicit
relations refer to the task dependencies explicitly set by the
developers (e.g. the block dependency). On the other hand,
implicit relations can be inferred from the resources assigned
to the tasks (e.g. assigned to the same developer) or the nature
of the tasks. We will discuss these types of task relations in
details in the next section. Each type of relationship can be
assigned to a weight which quantitatively reﬂects the strength
of the relationship.
Another novel aspect of our approach is the collective
inference component which simultaneously classiﬁes a set of
related tasks. Details of these approaches will be provided in
Section V.
IV. T ASK NETWORK CONSTRUCTION
An important part of our approach is building a task
network for past and current tasks. In most of modern task
tracking system (e.g. JIRA), some dependencies between tasks
are explicit recorded (i.e. in a special ﬁeld) in the task reports
and can be easily extracted from there. We refer to these
dependencies as explicit relationships . There are however other
types of task dependency that are not explicitly recorded (e.g.
tasks assigned to the same developer), and we need to infer
them from extracting other information of the tasks. TheseLearning phaseExecution phase
Archive of past tasks
Identifying  delay status of tasks
Extracting local features
Constructing task network
Building relationalclassifier
TrainedrelationalclassifierBuilding local classifierLabelledtasks
Predicted delaystatus of tasks
Trained local classifier
Collective inference
Existing tasks
Extracting local features
Task linksFig. 2: An overview of our approach
JBIDE-1469
(b.) Implicit network topologyJBIDE-788JBIDE-1329JBIDE-1769JBIDE-1457JBIDE-6217JBIDE-1636JBIDE-1547JBIDE-351JBIDE-1498blocksrelatesrelates
Fig. 3: Example of explicit task relationships in JBoss
are referred to as implicit relationships . We now discuss these
types of relationships in details.
1)Explicit relationships :There are a number of depen-
dencies among tasks which are explicitly speciﬁed in the task
records. These typically determine the order in which tasks
need to be performed. There are generally four different types
of relationships of the preceding tasks to the succeeding tasks:
ﬁnish to start (predecessor must ﬁnish before successor can
start), start to start (predecessor must start before successor can
start), ﬁnish to ﬁnish (predecessor must ﬁnish before successor
can ﬁnish), and start to ﬁnish (predecessor must start before
successor can ﬁnish). For example, blocking is a common type
of relationships that is explicitly recorded in issue/bug tracking
systems. Blocking tasks are software tasks that prevent other
tasks from being resolved, which could fall into the ﬁnish to
start or ﬁnish to ﬁnish category.
Figure 3 shows some explicit relationships between tasks
in the JBoss project, which uses the JIRA task tracking system.
JIRA provides the task link attribute to specify the relationship
between two or more related tasks. The explicit relationships
are extracted directly from the dataset. For example, JBIDE-
788 blocks JBIDE-1469, which is represented by a directed
edge connected the two nodes. In addition to blocking, JIRA
also provides three other default types of task links: relates
to, clones and duplicates. Figure 3 shows some examples of
the “relates to” relationship, e.g. task JBIDE-788 relates to
JBIDE-1547.
(c.) Combined network topologyJBIDE-788JBIDE-1492JBIDE-1694JBIDE-1694repJBIDE-1492JBIDE-1717
JBIDE-799verdev = same developerrep = same reportercom = same componentfix = same fix versionver= same affect versiontop = same topicJBDS-655Fig. 4: Example of implicit task relationships in JBoss
2)Implicit relationships :While explicit relationships are
speciﬁed directly in the task reports, implicit relationship
need to be inferred from other task information. There are
different task information that can be extracted to identify a
(implicit) relationship between tasks. We classiﬁed them into
three groups as described below.
• Resource-based relationship: this type of relationships
exists between tasks that share the same (human)
resource. The resource here could be the developers
assigned to perform the tasks or the same person who
created and reported the tasks. Resource-based rela-
tionship is important in our context since a resource’s
skills, experience, reputation and workload may affect
a chance of delayed tasks (i.e. a developer who causes
a delay of a current task may do so again in the
future). For example, from Figure 4, JBIDE-788 has a
relationship with JBIDE-1694 since both of them are
assigned to the same developer. Task JBIDE-788 is
also related to JBIDE-1694 since they were reported
by the same person.
• Attribute-based relationship: tasks can be related if
some of their attributes share the same values. For ex-
ample, there is a relationship between tasks performed
on the same component since they may affect the same
or related parts of code. For tasks recorded in JIRA,
we extract this type of relationship by examining threeTABLE II: Datasets and networks’ statistics
Project RelationshipNum
NodesNum
EdgesAvg. node
degreeNode
Assort.
Apache Explicit 496 246 1.597 0.256
Implicit 496 27,460 55.362 0.246
All 496 27,706 55.858 0.225
Duraspace Explicit 1,116 563 1.700 0.257
Implicit 1,116 383,677 343.796 0.240
All 1,116 384,240 344.301 0.230
JBoss Explicit 8,206 4,904 2.057 0.235
Implicit 8,206 4,908,164 598.118 0.249
All 8,206 4,913,068 598.716 0.247
Moodle Explicit 1,439 1,283 3.055 0.222
Implicit 1,439 197,176 137.022 0.215
All 1,439 198,748 138.115 0.208
Spring Explicit 597 222 1.219 0.250
Implicit 597 63,430 106.247 0.249
All 597 63,652 106.619 0.242
attributes: affect version ,ﬁx version andcomponent .
For example, JBIDE-788 and JBIDE-799 affects the
same version while JBIDE-1694 and JBIDE-1717
affects the same component as shown in Figure 4.
• Content-based relationship: tasks can be similar in
terms of how they are conducted and/or what they af-
fect. The similarity may form an implicit relationship
between tasks which can be established by extracting
the description of the tasks. Different extraction tech-
niques can be applied here, ranging from traditional
information retrieval techniques to recent NLP tech-
niques like topic modeling. We use Latent Dirichlet
Allocation [19] to build a topic model representing
the content of a software task. We then establish
relationships between on the basis that related tasks
share a signiﬁcant number of common topics. Figure
4 shows some example of content-based relationships
in JBoss, e.g. task JBIDE-788 has the same topic with
JBDS-655. The common topics shared between these
two tasks are “code, access control exception, and
document types”.
A task network is built by extracting both explicit and
implicit links among the tasks. We employ a number of
measures to describe different properties of a task network:
the number of nodes, the number of edges, and the average
node degree (i.e. the number of connections a node has to
other nodes). In addition, assortativity coefﬁcient [20] is used
to measure the correlation between two nodes: the preference
of network nodes to connect to other nodes that have similar
or different degrees. Positive values of assortativity coefﬁcient
indicate a correlation between nodes of similar degree (e.g.
highly connected nodes tends to be connected with other high
degree nodes), while negative values indicate relationships
between nodes of different degree (e.g. high degree nodes tend
to connect to low degree nodes). As can be seen from Table
II, the inclusion of implicit relationships signiﬁcantly increases
the density of the network tasks across all the ﬁve projects that
we studied. By contrast, the assortativity coefﬁcient remains
nearly the same with or without implicit relationships.A weight is also applied to each edge type in a task
network. This allows us to better quantify the strength of a
relationship between tasks. By default, each edge is equally
assigned the weight of 1. However, different weights can also
be applied to different types of relationships. More complex
approaches can also be applied here. For example, the weights
could be decreased over time to reﬂect the fading of the
relationships, e.g. the tasks have been assigned to the same
developer for long time ago.
V. P REDICTIVE MODELS
Our predictive models are built upon three components:
local classiﬁer (as done in previous work), relational classiﬁer,
and collective inference. Local classiﬁers treat tasks as being
independent, making it possible to estimate class membership
on a task-by-task basis. Relational classiﬁers posit that the
class membership of one task may have an inﬂuence on the
class membership of a related task in the network. Collective
inference infers the class membership of all tasks simultane-
ously [21]. In the following we discuss the details of each
components.
A. Local (non-relational) classiﬁer
There are several available state-of-the-art algorithms and
techniques that we could employ to develop local classiﬁers.
We employ the state-of-the-art classiﬁer which is Random
Forest (RF) [16] – the best performing technique in our
experiments.
B. Relational classiﬁer
Relational classiﬁers make use of information about related
tasks to estimate the label probability. For simplicity, we use
only direct relations for class probability estimation:
P(c|G)=P(c|Ni)
where Niis a set of the immediate neighbors of task vi(i.e.
those that are directly related to vi) in the task network G,
such that P(c|Ni)is independent of G\Ni.
This is based on a theoretical property known as the
Markov assumption which states that given the neighborhood
(also known as the Markov blanket), it is sufﬁcient to infer
about the current label without knowing the other labels in the
network [22].
For developing a relational classiﬁer, we employ two highly
effective methods. One is Weighted-Vote Relational Neighbor
(wvRN) [23] which is one of the best relational classiﬁcation
algorithms reported in [21]. The other is Stacked Graphical
Learning [24], where classiﬁers are built in a stage-wise
manner, making use of relational information in the previous
stage.
1)Weighted-Vote Relational Neighbor :Weighted-Vote
Relational Neighbor (wvRN) estimates class membership
probabilities based on two assumption [25]. First, the label
of a node depends only on its immediate neighbors. Second,
wvRN relies on the principle of homophily which assumes that
neighboring class labels were likely to be the same [26]. Thus,wvRN estimates P(c|vi)as the (weighted) mean of the class
membership of the tasks in the neighborhood ( Ni):
P(c|vi)=1
ZX
vj2Niw(vi,vj)P(c|Nj)
where Z=P
vj2Niw(vi,vj)andw(vi,vj)is the weight
of the link between task viand task vj. Our experiments
applied the same weight of 1 to all relationship types, i.e.
w(vi,vj)=1 . The optimized weights could be determined
using the properties of a network topology such as assortativity
coefﬁcient [21, 27, 28]). We denote the prior class probability
distributions from a relational classiﬁcation as MR.
2)Stacked Graphical Learning :One inherent difﬁculty of
the weighted-voting method is the computation of the neighbor
weights. Since there are multiple relations, estimating the
weights are non-trivial. Stacked learning offers an alternative
way to incorporate relational information.
The idea of stacking is to learn joint models by multiple
steps, taking into relational information of the previous step to
improve the current step. At each step, relational information
together with local features are fed into a standard classiﬁer
(e.g., Random Forests). We consider relations separately and
the contribution of each relation is learnt by the classiﬁer
through the relational features. The classiﬁer is then trained. Its
prediction on all data points (vertices in the network) will be
then used as features of the next stage. We adapt the idea from
[24]. Our contribution is in the novel use of Random Forests as
a strong local classiﬁer rather than linear classiﬁers as used in
[24].The stacked learning algorithm is described in Algorithm
1. It returns Tclassiﬁers for Tsteps. At the ﬁrst step, the local
classiﬁer is used. At subsequent steps, relational classiﬁers are
trained on both local features and relation-speciﬁc averaged
neighbor probabilities.
C.Collective inference
Collective inference is the process of inferring class prob-
abilities simultaneously for all unknown labels in the network
conditioned on the seen labels. We employ two methods:
Relaxation Labeling (RL) [29] and Stacked Inference (SI). RL
is applicable to any non-stagewise relational classiﬁers (e.g.
wvRN described in Section V-B1). It has been found to achieve
good performance in [21]. SI, on the other hand, is speciﬁc to
stacked classiﬁers (e.g., see Section V-B2).
1)Relaxation Labeling :Relaxation Labeling (RL) has
been shown to achieve good results in [21]. RL initializes the
class probabilities using the local classiﬁer model. RL then
iteratively corrects this initial assignment if the neighboring
tasks have labels that are unlikely according to the prior class
distribution estimated by MR(see Section V-B1). Algorithm
2 describes the Relaxation Labeling technique.
2)Stacked Inference :Following the stacked learning al-
gorithm in Section V-B2, stacked inference is described in
Algorithm 3. It involved Tclassiﬁers returned by the stack
learning algorithm. At the ﬁrst step, the local classiﬁer is
used to compute the class probabilities. At T 1subsequent
steps, relational classiﬁers receives both the local features and
relation-speciﬁc weighted neighbor probabilities and outputsclass probabilities. The ﬁnal class probabilities are the outcome
of the inference process.
VI. E VA L UAT I O N
Tasks were collected from the JIRA task tracking system
in ﬁve well-known open source projects: Apache, Duraspace,
JBoss, Moodle, and Spring (see Table II), and divided into a
training set and a test set. We try to mimic a real project
management scenario that prediction on a current task is
made using knowledge from the past tasks, the collected
tasks in training set are those that were opened before the
tasks in test set. The collected datasets are shown in Ta-
ble II. We have made our datasets publicly available at:
http://www.uow.edu.au/ ⇠mc650/. Since the number of delayed
tasks in our datasets is small, we chose to use two classes of
delay: major delayed and minor delayed (and the non-delayed
class).
Table III shows the number of tasks in training set and test
set for each project. Major delayed tasks are those that have
actual completed date (resolved date) greater than 30 days from
planned to completed date and less than 30 days of delays is
minor delayed. Note that the size of delayed can be deﬁned by
project managers who realize the impact of schedule overruns
to the projects. Since (major/minor) delayed tasks are rare and
imbalanced, we had to be careful in creating the training and
test sets. Speciﬁcally, we placed 60% of the delayed tasks into
the training set and the remaining 40% into the test set. In
addition, we tried to maintain a similar ratio between delayed
and non-delayed tasks in both test set and training set, i.e.
stratiﬁed sampling.
TABLE III: Experimental setting
ProjectTraining set Test set
Major Minor Non Major Minor Non
Apache 10 52 236 6 34 158
Duraspace 23 71 575 16 47 384
JBoss 666 679 3,579 444 452 2,386
Moodle 42 52 770 28 34 513
Spring 13 34 310 8 22 207
A. Performance Measure
Reporting the average of precision/recall across classes is
likely to overestimate the true performance, since our risk
classes are ordinal and imbalanced and no-delays are the
default and they are not of interest to the prediction of delayed
tasks. Hence, our evaluation is focus on the predicting of risk
classes as described below.
A confusion matrix is used to evaluate the performance
of our predictive models. As a confusion matrix does not
deal with a multi-class probabilistic classiﬁcation, we reduce
the classiﬁed tasks into two binary classes: delayed and non-
delayed using the following rule:
Ci=⇢
delayed, ifP (i, Maj )+P(i, Min )>P(i, Non )
non delayed, otherwiseAlgorithm 1 The stacked learning algorithm (adapted from [24])
1:Train of the 1-st local classiﬁers on training nodes, ignoring relations
2:forstep t=2,3,..,T do
3: Compute the class probabilities for all data points using the (t-1)th classiﬁer
4: foreach node ido
5: foreach relation rthat this node has with its neighbor do
6: ifrelation weight exist then
7: Average all probabilities of its neighbors jwho have the relation rwith relation weight
8: else
9: Set relation weight to 1
10: Average all probabilities of its neighbors jwho have the relation r
11: end if
12: Prepare k 1features using these averaged probabilities (k probabilities sum to 1)
13: end for
14: Concatenate all relational features together with the original features
15: end for
16: Train the t-th local classiﬁer on training nodes and new feature sets.
17:end for
18:Output T classiﬁers (one local, T-1 relational)
Algorithm 2 The Relaxation Labeling algorithm (adapted from [25])
1:Use the 1-st classiﬁer to predict the class probabilities using only local features
2:forstep t=2,3,..,T do
3: Estimate the prior class probabilities using the relational classiﬁer, MR, on the current state of network
4: Reassign the class of each vi2VUaccording to the current class probabilities estimation
5:end for
6:Output the class probabilities of vertices with unknown labels.
Algorithm 3 The stacked inference algorithm
1:Use the 1-st classiﬁer to predict the class probabilities using only local features
2:forstep t=2,3,..,T do
3: Prepare relational features using the neighbor probabilities computed from the previous step
4: Use the t-th classiﬁers to predict the class probabilities using local features and relational features.
5:end for
6:Output the class probabilities of vertices with unknown labels.
where Ciis the binary classiﬁcation of task i, and
P(i, Maj ),P(i, Min ), and P(i, Non )are the probabilities of
taskiclassiﬁed in the major delayed, minor delayed, and non-
delayed classes respectively. Basically, this rule determines
that a task is considered as delayed if the sum probability
of it being classiﬁed into the major and minor delayed classes
is greater than the probability of it being classiﬁed into the
non-delayed class. Note that our work on this paper focuses
on predicting delayed and non delayed tasks. Our evaluations
thus emphasize on measuring the performance of predicting
whether tasks will cause a delay. We however acknowledge
that the ability to distinguish between major and minor is
also important. Hence, future work involves using several
appropriate performance metrics (e.g. Macro-averaged mean
absolute error [30]) to measure the performance of our models
in distinguishing between the two delayed classes (major and
minor delayed).
The confusion matrix is then used to store the correct
and incorrect decisions made by a classiﬁer. For example,
if a task is classiﬁed as delayed when it truly caused a
delay, the classiﬁcation is a true positive (tp). If the task is
classiﬁed as delayed when actually it did not cause a delay,then the classiﬁcation is a false positive (fp). If the task is
classiﬁed as non-delayed when it in fact caused a delay, then
the classiﬁcation is a false negative (fn). Finally, if the task
is classiﬁed as non-delayed and it in fact did not cause a
delay, then the classiﬁcation is true negative (tn). The values
stored in the confusion matrix are used to compute the widely-
used Precision, Recall, and F-measure for the delayed tasks to
evaluate the performance of the predictive models:
• Precision: The ratio of correctly predicted delayed
task over all the tasks predicted as delayed task. It
is calculated as:
pr=tp
tp+fp
• Recall: The ratio of correctly predicted delayed task
over all of the actually task delay. It is calculated as:
re=tp
tp+fn00.20.40.60.81
TraditionalwvRN+RLStacked learningPrecision00.20.40.60.81
TraditionalwvRN+RLStacked learningRecall
00.20.40.60.81
TraditionalwvRN+RLStacked learningF-measure00.20.40.60.81
TraditionalwvRN+RLStacked learningAUC(a.) Precision(b.) Recall
(c.) F-measure(d.) AUCApacheDuraspaceJBossMoodleSpringFig. 5: Evaluation results of traditional classiﬁcation, wvRN+RL, and stacked learning
• F-measure: Measures the weighted harmonic mean of
the precision and recall. It is calculated as:
F measure =2⇤pr⇤re
pr+re
• Area Under the ROC Curve (AUC) is used to evaluate
the degree of discrimination achieved by the model.
The value of AUC is ranged from 0 to 1 and random
prediction has AUC of 0.5. The advantage of AUC
is that it is insensitive to decision threshold like
precision and recall. The higher AUC indicates a better
prediction.
B. Results
Comparison of different classiﬁcation approaches :W e
compare three different settings: local classiﬁer using Random
Forrests (traditional classiﬁcation), Weighted-Vote Relational
Neighbor (wvRN) with Relaxation Labeling (RL), and stacking
method (with stacked inference). Figure 5 shows the preci-
sion, recall, F-measure, and AUC achieved by three different
classiﬁcation approaches. The stacking method uses Random
Forests as the base classiﬁer. The evaluation results indicate
that the predictive performance achieved by stacked learning
is better and more consistent than traditional classiﬁcation
and relational classiﬁcation using wvRN+RL. As can be seen
in Figure 5(a.), stacked learning achieved the best precision
of 0.65 (averaging across ﬁve projects), while the traditional
classiﬁcation achieved only 0.39 precision (averaging across
ﬁve projects). It should however be noted that wvRN+RL
achieved the highest precision of 0.97 for Duraspace. In
addition, the precision achieved by stacked learning is more
consistent and steady in all projects. By contrast, the perfor-
mance of wvRN+RL are varied between projects. Relational
classiﬁcation with wvRN+RL is based on the principle of
homophily, which may not always hold in some projects. This
is reﬂected by its low performance in some cases (i.e. only
0.45 precision for Apache). On the other hand, stacked learning
provides a more generalized approach to learn the relationshipswithin networked data – it achieved above 0.5 precision across
the ﬁve projects.
Stacked learning also outperforms the other classiﬁcation
approaches in terms of recall and F-measure: it achieved the
highest recall of 0.83 and the highest F-measure of 0.71
(averaging across ﬁve projects) as can be seen in in Figure
5(b.) and 5(c.). The highest recall of 0.97 was also achieved
by stack learning for the Spring project.
The degree of discrimination achieved by our predictive
models is also high, as reﬂected in the AUC results. The AUC
quantiﬁes the overall ability of the discrimination between the
delayed and non-delayed classes. As can be seen in Figure
5(d.), the average of AUC across all classiﬁers and across all
projects is 0.83. All classiﬁers achieved more than 0.65 AUC
while stacked learning is the best performer with 0.88 AUC
(averaging across ﬁve projects) and 0.95 for Duraspace.
Overall, the evaluation results demonstrate the effectiveness
of our predictive models, achieving on average 46%–97%
precision, 46%–97% recall, 56%–76% F-measure, and 78%–
95% Area Under the ROC Curve. Our evaluation results also
show a signiﬁcant improvement over traditional approaches
(local classiﬁers): 49% improvement in precision, 28% in
recall, 39% in F-measure, and 16% in Area Under the ROC
Curve.
The usefulness of collective inference : The second aspect
of our evaluation focuses on evaluating the predictive perfor-
mance achieved by using collective inference. To do so, we
have setup two experiments: one using wvRN and the other
using both wvRN and RL. Figure 6 shows the comparison
of the precision, recall, F-measure, and AUC achieved by the
relational classiﬁcation with collective inference (wvRN+RL)
and without collective inference (only wvRN). Overall, the
predictive performance achieved by relational classiﬁcation
with collective inference is better than that without collective
inference in all measures. As can be seen in Figure 6, the
relational classiﬁcation with collective inference achieves the
highest precision of 0.61, recall of 0.70, F-measure of 0.62,00.20.40.60.81
Non CIWith CIPrecision00.20.40.60.81
Non CIWith CIRecall
00.20.40.60.81
Non CIWith CIF-measure00.20.40.60.81
Non CIWith CIAUC(a.) Precision(b.) Recall
(c.) F-measure(d.) AUCApacheDuraspaceJBossMoodleSpringFig. 6: Evaluation results of relational classiﬁer with collective inference and without collective
00.20.40.60.81
PrecisionRecallF-measureExplicitExplicit and Resource-basedExplicit and Attribute-basedExplicit and Content-basedAll
Fig. 7: Evaluation results on different sets of relationships
and 0.82 AUC (averaging across ﬁve projects). Although, the
predictive performance of Relaxation Labeling is lower than
stacked learning as we discussed earlier, the evaluation results
still support that collective inference signiﬁcantly improve
the performance of relational classiﬁers. However, collective
inference applied on top of the wvRN still follows a strong
assumption of homophily theory and as a result, it causes an
inconsistent predictive performance among different projects.
The inﬂuence of explicit and implicit relationships :W e
have also performed a number of experiments to evaluate
the predictive performance achieved by different sets of re-
lationships. We have tested with ﬁve different combinations:
networks with explicit relationships, networks with explicit
and resource-based relationships, networks with explicit and
attribute-based relationships, networks explicit and content-
based relationships, and networks with all explicit and implicit
relationships. As can be seen from Figure 7, the highest
predictive performance is achieved by using both explicit and
implicit relationships: it achieved the highest precision of 0.62
and the highest recall of 0.70 (averaging across ﬁve projects).
By contrast, the networks using only explicit relationships
achieved the lowest precision, i.e. 0.31, while the networks
using explicit and content-based relationships produced the
lowest recall. In general, using both explicit relationshipsand implicit relationships (resource-based, attribute-based, and
content-based) signiﬁcantly increases the predictive perfor-
mance: 66.23 % increased in precision and 21.62 % increased
in recall (compare to using only explicit relationships).
0.20.30.40.50.60.70.8
20%30%40%50%60%70%80%90%100%F-MeasureApacheDuraspaceJBossMoodleSpringavg
Fig. 8: Evaluation results on different sizes of training data
The effect of the size of training data : We have also
performed a number of experiments to assess the proportion
of past tasks (i.e. labeled tasks) is needed to achieve a good
predictive performance. Speciﬁcally, in these experiments,given a data set, G=(V,E),VK(i.e. labeled tasks) is
created by selecting samples of 20% – 100% of the training set
(see Table III). The test set, VU, is then deﬁned as V\VK.
Figure 8 shows the predictive performance from samples of
20% to 100% of Vin terms of F-measure. The results clearly
demonstrate that F-measure (averaging across ﬁve projects)
increases as more labeled data is used for training.
VII. T HREATS TO V ALIDITY
One relational setting involves the use of wvRN, which
assumes the homophily property among tasks, that is, related
tasks should have similar delay risk. This is a strong assump-
tion and may not hold in reality, and this has been revealed in
our experiments. We have addressed this threat by proposing
stacked learning approach which does not rely on the the
homophily assumption but rather estimates the contribution of
separate relationships.
We have attempted to identify all possible relationships
among typical software tasks. However, we acknowledge that
the implicit relationships we have inferred are by no means
comprehensive to represent all task dependencies. Another
threat to our study is that our data set has the class imbalance
problem (over 90% of the total data are non-delayed tasks),
which may affect a classiﬁer’s ability to learn to identify
delayed tasks. We have used stratiﬁed sampling to mitigate this
problem. We however acknowledge such a sampling approach
could be an external threat to validity. Further experiments to
evaluate sampling techniques used in practice are thus needed.
In addition, patterns that hold in the train data may not reﬂect
the situation in the test, e.g. the team and management having
changed their approach or managed the risks they perceived.
To address this threat, instead of splitting the data randomly
(as done in traditional settings), we deliberately chose the time
to split training and test sets to mimic a real deployment.
We have considered 11,851 task reports from the ﬁve
projects which differ signiﬁcantly in size, complexity, develop-
ment process, and the size of community. Although these are
real data, we however cannot claim that our data set would
be representative of all kinds of software projects, especially
in commercial settings. Although open source projects and
commercial projects share similarities in many aspects, they
are also different in the nature of contributors, developers and
projects stakeholders. For example, open source contributors
are free to join and leave the communities (i.e. high turn over
rate), while developers in the commercial setting tend to be
stable and fully commit to deliver the projects progress. Hence,
further study is need to understand how our predict models
perform for commercial projects.
VIII. R ELATED WORK
An automated risk prediction mainly supports software risk
management which is under the umbrella of project manage-
ment and crucial to the project success rate. Risk management
consists of two main activities: risk assessment and risk
control. Our current work focuses on risk assessment, which
is a process of identifying risks, analyzing and evaluating their
potential effects in order to prioritize them [4, 31]. Risk control
aims to develop, engage, and monitor risk mitigation plans
[32].There are a number of works on applying statistical and
machine learning techniques to use in different aspects of risk
management. For example, Letier et al. [33] used a statistical
decision analysis approach to provide a statistical support
on complex requirements and architecture. The work in [34]
analyzed the correlation and causality of risk factors using
Bayesian network.
Our work also related to the works on predicting and
mining bug reports, for example, mining bug reports for ﬁx-
time prediction (e.g. [7–11]), blocking bug prediction (e.g.
[35]), re-opened bug prediction (e.g. [36, 37]), severity/priority
prediction (e.g. [38, 39]), delays in the integration of a resolved
issue to a release (e.g. [40]), bug triaging (e.g. [41–44]),
and duplicate bug detection ([45–50]). Another groups of the
works on predicting is mining source code to predict software
defects, for example, mining change history (e.g. [51]), and
personalized defect prediction (e.g. [52])
Anothe thread of related work resides in the use of net-
worked data such as predicting software quality using social
network analysis (e.g. [53–56]), predicting software evolution
in terms of estimating bug severity, efforts, and defect-prone
releases using Graph-based analysis (e.g. [20]), and predicting
software defects using network analysis on dependency graphs
(e.g. [57]). Those approaches mostly work at the level of
source code and have not addressed delay prediction at the
task level as in our work.
IX. C ONCLUSIONS AND FUTURE WORK
In this paper, we have proposed a novel approach to predict
whether a number of existing tasks in a software project are at
risk of being delayed. Our approach exploits not only features
speciﬁc to individual tasks but also the relationships between
the tasks (i.e. networked data). We have developed several
prediction models using local classiﬁers, relational classiﬁers
and collective inference. The evaluation results demonstrate a
strong predictive performance of our networked classiﬁcation
techniques compared to traditional approaches: achieving 49%
improvement in precision, 28% improvement in recall, 39%
improvement in F-measure, and 16% improvement in Area
Under the ROC Curve. In particular, the stacked graphical
learning approach consistently outperformed the other tech-
niques across the ﬁve projects we studied.
The results from our experiments indicate that the re-
lationships between tasks have an impact on the predictive
performance. Hence, as part of future work, we will investigate
which types of task relationships should be selected to give
optimal results and how this can be done automatically for
software projects. A related future investigation would involve
applying different weights to different task relationships and
assessing their impact to the results. We also plan to investigate
if there are any other kinds of implicit relationships which can
be inferred from the task information and the general context of
a software project. Our future work would involve expanding
our study to commercial software projects and other large open
source projects to further assess our predictive models. Finally,
delay prediction which has been addressed in this paper is just
only the ﬁrst part of the solution. The next task is making
actionable recommendations such as which tasks having a risk
of being delayed should be dealt with ﬁrst, and which measures
could be used to mitigate the risks.REFERENCES
[1] B. Flyvbjerg and A. Budzier, “Why Your IT Project May Be Riskier
Than You Think,” Harvard Business Review , vol. 89, no. 9, pp. 601–
603, 2011.
[2] B. Michael, S. Blumberg, and J. Laartz, “Delivering large-scale IT
projects on time, on budget, and on value,” 2012.
[3] S. Group, “Chaos report,” West Yarmouth, Massachusetts: Standish
Group, Tech. Rep., 2004.
[4] B. W. Boehm, “Software risk management: principles and practices,”
Software, IEEE , vol. 8, no. 1, pp. 32–41, 1991.
[5] M. J. Carr and S. L. Konda, “Taxonomy-Based Risk Identiﬁcation,”
Software Engineering Institute, Carnegie Mellon University, Tech. Rep.
June, 1993.
[6] M. Choetkiertikul, H. K. Dam, T. Tran, and A. Ghose, “Characterization
and prediction of issue-related risks in software projects,” in Proceed-
ings of 12th Working Conference on Mining Software Repositories
(MSR-2015) , 2015, pp. 280–291.
[7] C. Weiss, R. Premraj, T. Zimmermann, and A. Zeller, “How Long Will
It Take to Fix This Bug?” in Proceedings - ICSE 2007 Workshops:
Fourth International Workshop on Mining Software Repositories, MSR
2007 . IEEE, May 2007.
[8] E. Giger, M. Pinzger, and H. Gall, “Predicting the ﬁx time of bugs,”
inProceedings of the 2nd International Workshop on Recommendation
Systems for Software Engineering - RSSE ’10 . ACM Press, May 2010,
pp. 52–56.
[9] L. D. Panjer, “Predicting Eclipse Bug Lifetimes,” in Fourth Inter-
national Workshop on Mining Software Repositories (MSR’07:ICSE
Workshops 2007) . IEEE, May 2007, pp. 29–29.
[10] L. Marks, Y. Zou, and A. E. Hassan, “Studying the ﬁx-time for bugs
in large open source projects,” in Proceedings of the 7th International
Conference on Predictive Models in Software Engineering - Promise
’11. New York, New York, USA: ACM Press, Sep. 2011, pp. 1–8.
[11] P. Bhattacharya and I. Neamtiu, “Bug-ﬁx time prediction models,”
inProceeding of the 8th working conference on Mining software
repositories - MSR ’11 . New York, New York, USA: ACM Press,
May 2011, p. 207.
[12] J. Neville and D. Jensen, “Collective Classiﬁcation with Relational
Dependency Networks,” in Proceedings of the Second International
Workshop on Multi-Relational Data Mining , 2003, pp. 77–91.
[13] B. Taskar, V. Chatalbashev, and D. Koller, “Learning Associative
Markov Networks,” Proc. of the International Conference on Machine
Learning , pp. 102–110, 2004.
[14] Q. Lu and L. Getoor, “Link-based classiﬁcation,” in ICML , vol. 3, 2003,
pp. 496–503.
[15] E. Segal, R. Yelensky, and D. Koller, “Genome-wide discovery of
transcriptional modules from DNA sequence and gene expression,”
Bioinformatics , vol. 19, pp. 273–282, 2003.
[16] L. Breiman, “Random forests,” Machine learning , pp. 5–32, 2001.
[17] C. Science, R. Holloway, and L. Egham, “Support Vector Machines for
Multi-Class Pattern Recognition,” in European Symposium on Artiﬁcial
Neural Networks, Computational Intelligence and Machine Learning ,
vol. 99, 1999, pp. 219–224.
[18] D. B ¨ohning, “Multinomial logistic regression algorithm,” Annals of the
Institute of Statistical Mathematics , vol. 44, no. 1, pp. 197–200, 1992.
[19] D. M. Blei, A. Y. Ng, and M. I. Jordan, “Latent Dirichlet Allocation,”
Journal of Machine Learning Research , vol. 3, no. 4-5, pp. 993–1022,
2012.
[20] P. Bhattacharya, M. Iliofotou, I. Neamtiu, and M. Faloutsos, “Graph-
based Analysis and Prediction for Software Evolution,” in Proceedings
of the 34th International Conference on Software Engineering (ICSE
2012) , 2012, pp. 419–429.
[21] S. a. Macskassy and F. Provost, “Classiﬁcation in Networked Data:
A toolkit and a univariate case study,” Journal of Machine Learning
Research , vol. 8, no. December 2004, pp. 1–41, 2005.
[22] S. L. Lauritzen, Graphical models . Oxford University Press, 1996.
[23] S. a. Macskassy and F. Provost, “A simple relational classiﬁer,” Pro-
ceeding of the 2nd Workshop on Multi-Relational Data Mining (MRDM
03), pp. 64–76, 2003.[24] Z. Kou and W. W. Cohen, “Stacked Graphical Models for Efﬁcient
Inference in Markov Random Fields,” SIAM International Conference
on Data Mining , pp. 533–538, 2007.
[25] S. a. Macskassy, “Relational classiﬁers in a non-relational world: Using
homophily to create relations,” Proceedings - 10th International Con-
ference on Machine Learning and Applications, ICMLA 2011 , vol. 1,
pp. 406–411, 2011.
[26] M. McPherson, L. Smith-Lovin, and J. M. Cook, “Birds of a Feather:
Homophily in Social Networks,” Annual Review of Sociology , vol. 27,
no. 1, pp. 415–444, 2001.
[27] P. Vojtek and M. Bielikov ´a, “Homophily of neighborhood in graph
relational classiﬁer,” Lecture Notes in Computer Science (including
subseries Lecture Notes in Artiﬁcial Intelligence and Lecture Notes in
Bioinformatics) , vol. 5901 LNCS, pp. 721–730, 2010.
[28] B. Golub and M. O. Jackson, “How homophily affects the speed of
learning and best-response dynamics,” Quarterly Journal of Economics ,
vol. 127, no. 3, pp. 1287–1338, 2012.
[29] R. a. Hummel and S. W. Zucker, “On the foundations of relaxation
labeling processes.” IEEE transactions on pattern analysis and machine
intelligence , vol. 5, no. 3, pp. 267–287, 1983.
[30] S. Baccianella, a. Esuli, and F. Sebastiani, “Evaluation Measures for
Ordinal Regression,” 2009 Ninth International Conference on Intelligent
Systems Design and Applications , 2009.
[31] Xu Ruzhi, Q. leqiu, and Jing Xinhai, “CMM-based software risk
control optimization,” in Proceedings Fifth IEEE Workshop on Mobile
Computing Systems and Applications . IEEE, 2003, pp. 499–503.
[32] M. Choetkiertikul and T. Sunetnanta, “A Risk Assessment Model
for Offshoring Using CMMI Quantitative Approach,” in 2010 Fifth
International Conference on Software Engineering Advances . IEEE,
Aug. 2010, pp. 331–336.
[33] E. Letier, D. Stefan, and E. T. Barr, “Uncertainty, risk, and information
value in software requirements and architecture,” in Proceedings of the
36th International Conference on Software Engineering - ICSE 2014 .
New York, New York, USA: ACM Press, May 2014, pp. 883–894.
[34] Y. Hu, X. Zhang, E. Ngai, R. Cai, and M. Liu, “Software project risk
analysis using Bayesian networks with causality constraints,” Decision
Support Systems , vol. 56, pp. 439–449, Dec. 2013.
[35] H. Valdivia Garcia, E. Shihab, and H. V. Garcia, “Characterizing and
predicting blocking bugs in open source projects,” in Proceedings of
the 11th Working Conference on Mining Software Repositories - MSR
2014 . ACM Press, May 2014, pp. 72–81.
[36] T. Zimmermann, N. Nagappan, P. J. Guo, and B. Murphy, “Character-
izing and predicting which bugs get reopened,” in 34th International
Conference on Software Engineering (ICSE), 2012 . IEEE Press, Jun.
2012, pp. 1074–1083.
[37] E. Shihab, A. Ihara, Y. Kamei, W. M. Ibrahim, M. Ohira, B. Adams,
A. E. Hassan, and K.-i. Matsumoto, “Studying re-opened bugs in open
source software,” Empirical Software Engineering , vol. 18, no. 5, pp.
1005–1042, Sep. 2012.
[38] A. Lamkanﬁ, S. Demeyer, E. Giger, and B. Goethals, “Predicting the
severity of a reported bug,” in 2010 7th IEEE Working Conference on
Mining Software Repositories (MSR 2010) . IEEE, May 2010, pp. 1–10.
[39] T. Menzies and A. Marcus, “Automated severity assessment of soft-
ware defect reports,” 2008 IEEE International Conference on Software
Maintenance , pp. 346–355, Sep. 2008.
[40] D. Alencar, S. L. Abebe, and S. Mcintosh, “An Empirical Study of
Delays in the Integration of Addressed Issues.”
[41] J. Anvik, L. Hiew, and G. C. Murphy, “Who should ﬁx this bug?” in
Proceeding of the 28th international conference on Software engineer-
ing - ICSE ’06 . New York, New York, USA: ACM Press, May 2006,
p. 361.
[42] J. Anvik and G. C. Murphy, “Reducing the effort of bug report triage,”
ACM Transactions on Software Engineering and Methodology , vol. 20,
no. 3, pp. 1–35, Aug. 2011.
[43] M. M. Rahman, G. Ruhe, and T. Zimmermann, “Optimized assignment
of developers for ﬁxing bugs an initial evaluation for eclipse projects,” in
2009 3rd International Symposium on Empirical Software Engineering
and Measurement . IEEE, Oct. 2009, pp. 439–442.[44] G. Murphy and D. ˇCubrani ´c, “Automatic bug triage using text catego-
rization,” in Proceedings of the Sixteenth International Conference on
Software Engineering & Knowledge Engineering , 2004.
[45] P. Runeson, M. Alexandersson, and O. Nyholm, “Detection of Duplicate
Defect Reports Using Natural Language Processing,” in 29th Interna-
tional Conference on Software Engineering (ICSE’07) . IEEE, May
2007, pp. 499–510.
[46] X. Wang, L. Zhang, T. Xie, J. Anvik, and J. Sun, “An approach to
detecting duplicate bug reports using natural language and execution
information,” in Proceedings of the 30th international conference on
Software engineering , 2008, pp. 461–470.
[47] N. Bettenburg, R. Premraj, and T. Zimmermann, “Duplicate bug reports
considered harmful . . . really?” in 2008 IEEE International Conference
on Software Maintenance . IEEE, 2008, pp. 337–345.
[48] C. Sun, D. Lo, S. C. Khoo, and J. Jiang, “Towards more accurate
retrieval of duplicate bug reports,” 2011 26th IEEE/ACM International
Conference on Automated Software Engineering (ASE 2011) , pp. 253–
262, Nov. 2011.
[49] N. Jalbert and W. Weimer, “Automated duplicate detection for bug track-
ing systems,” in 2008 IEEE International Conference on Dependable
Systems and Networks With FTCS and DCC (DSN) . IEEE, 2008, pp.
52–61.
[50] A. T. Nguyen, T. T. T. N. Nguyen, D. Lo, and C. Sun, “Duplicate
bug report detection with a combination of information retrieval and
topic modeling,” Proceedings of the 27th IEEE/ACM International
Conference on Automated Software Engineering - ASE 2012 , p. 70,
2012.
[51] F. Palomba, G. Bavota, M. Di Penta, R. Oliveto, A. De Lucia, and
D. Poshyvanyk, “Detecting bad smells in source code using changehistory information,” 2013 28th IEEE/ACM International Conference
on Automated Software Engineering, ASE 2013 - Proceedings , pp. 268–
278, 2013.
[52] T. Jiang, L. Tan, and S. Kim, “Personalized defect prediction,” 2013
28th IEEE/ACM International Conference on Automated Software En-
gineering, ASE 2013 - Proceedings , pp. 279–289, 2013.
[53] N. Bettenburg and A. E. Hassan, “Studying the impact of dependency
network measures on software quality,” Empirical Software Engineer-
ing, 2012.
[54] T. Wolf, A. Schr ¨oter, D. Damian, and T. Nguyen, “Predicting build
failures using social network analysis on developer communication,”
Proceedings - International Conference on Software Engineering , pp.
1–11, 2009.
[55] A. Meneely, L. Williams, W. Snipes, and J. Osborne, “Predicting fail-
ures with developer networks and social network analysis,” Proceedings
of the 16th ACM SIGSOFT International Symposium on Foundations
of software engineering - SIGSOFT ’08/FSE-16 , p. 13, 2008.
[56] W. Hu and K. Wong, “Using citation inﬂuence to predict software
defects,” in 2013 10th Working Conference on Mining Software Repos-
itories (MSR) . IEEE, May 2013, pp. 419–428.
[57] T. Zimmermann and N. Nagappan, “Predicting defects using network
analysis on dependency graphs,” in Proceedings of the 13th interna-
tional conference on Software engineering - ICSE ’08 . New York,
New York, USA: ACM Press, May 2008, p. 531.
View publication stats