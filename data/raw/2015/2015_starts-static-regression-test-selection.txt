STARTS: STAtic Regression Test Selection
Owolabi Legunsen, August Shi, Darko Marinov
Department of Computer Science
University of Illinois at Urbana-Champaign, USA
{legunse2,awshi2,marinov}@illinois.edu
Abstract —Regression testing is an important part of software
development, but it can be very time consuming. Regression
test selection (RTS) aims to speed up regression testing by
running only impacted tests—the subset of tests that can changebehavior due to code changes. We present STARTS, a tool for
STAtic Regression Test Selection. Unlike dynamic RTS, STARTS
requires no code instrumentation or runtime information to
ﬁnd impacted tests; instead, STARTS uses only compile-time
information. Speciﬁcally, STARTS builds a dependency graph
of program types and ﬁnds, as impacted, tests that can reach
some changed type in the transitive closure of the dependency
graph. STARTS is a Maven plugin that can be easily integratedinto any Maven-based Java project. We ﬁnd that STARTS selects
on average 35.2% of tests, leading to an end-to-end runtime that
is on average 81.0% of running all the tests. A video demo ofSTARTS can be found at https://youtu.be/PCNtk8jphrM.
I. I NTRODUCTION
Regression testing [1] is an important part of software devel-
opment. After every code change, a developer runs the tests in
the regression test suite to ensure that the changes do not breakany existing functionality. However, when a regression testsuite contains many tests, running all tests after every change,often called RetestAll, is very time consuming and slows down
the software development process. Companies such as Googleand Microsoft have reported how expensive it is for them toperform regression testing [2]–[7]. Regression test selection(RTS) is a way to reduce the cost of regression testing byselecting to run only the tests impacted by the changes [1].
An RTS technique works by ﬁnding the dependencies of each
test and selecting tests that depend on the changes. Runningfewer, but necessary, tests speeds up regression testing, while
aiming not to miss any test failures.
In our prior work [8], we used a prototype static RTS tool
to compare with dynamic RTS, which computes test depen-dencies dynamically. The results showed that static RTS withdependencies computed at the class-level is comparable withthe state-of-the-art dynamic RTS tool Ekstazi [9], [10]. Theresults are encouraging, showing that static RTS is feasible,and worthy of further research. Performing RTS staticallycould be particularly useful in contexts where dynamic RTSis not feasible, such as when dynamic instrumentation tocollect test dependencies breaks time-sensitive tests or whennon-determinism causes dynamic RTS to collect wrong orincomplete test dependencies.
We present STARTS (STAtic Regression Test Selection), a
robust tool for performing static RTS. STARTS constructs
a dependency graph relating all types (including classes,interfaces, and enums) in an application and computes a tran-
sitive closure for each test to ﬁnd its dependencies. STARTS
determines the types that changed by computing the checksum
of each type’s corresponding compiled classﬁle and comparingthe computed checksum with the one computed in the priorrun. STARTS selects to run impacted tests, which are testswhose transitive dependencies include some changed type.We made several changes to our initial prototype [8] to make
STARTS robust and usable on real, large software projects: weadded support for multi-module Maven projects and improvedthe speed, including parsing constant pools instead of entireclassﬁles, saving dependencies as type-to-tests instead of test-
to-types, using a faster graph library (yasgl [11], instead ofJGraphT [12]), and incrementally caching dependencies.
We evaluated STARTS on 32 Maven-based projects from
GitHub. We ﬁnd that STARTS selects on average 35.2% of
all tests, leading to an end-to-end runtime (consisting of thetime to select what tests to run plus time to run those tests) thatis 81.0% of the RetestAll time to run all tests. STARTS scales
well, and for 11 projects with longer-running tests that take
over one minute to run, STARTS selects on average 40.5% ofall tests, leading to an end-to-end runtime that is only 68.2% ofthe RetestAll time. STARTS source code is publicly availableon GitHub at https://github.com/TestingResearchIllinois/starts,and binary code is released on Maven Central.
II. U
SAGE
STARTS is a Maven plugin [13] and can be easily integrated
with any Maven-based Java project.
Integrating STARTS: The easiest way to integrate STARTS
with a project is to add the latest version of the STARTS plugin
from Maven Central to the project’s pom.xml ﬁle:
1<plugin>
2 <groupId>edu.illinois</groupId>
3 <artifactId>starts−maven−plugin</artifactId>
4 <version>${latest_ST ARTS_version}</version>
5</plugin>
Using STARTS: Developers can use STARTS to perform
several RTS-related tasks: (i) ﬁnding the types that changed
semantically at the bytecode level (ii) ﬁnding the types (notjust tests) that are impacted by the changes (i.e., change-
impact analysis), (iii) ﬁnding the tests that are impacted bythe changes without running those tests, and (iv) ﬁnding andrunning the tests that are impacted by the changes. To achieve
these tasks, STARTS provides several goals:
978-1-5386-2684-9/17/$31.00 c/circlecopyrt2017 IEEEASE 2017, Urbana-Champaign, IL, USA
T ool Demonstrations949
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:30:51 UTC from IEEE Xplore.  Restrictions apply. 1$ mvn starts:help # list all goals
2$ mvn starts:diff # ﬁnd types that changed semantically
3$ mvn starts:impacted # ﬁnd types impacted by changes
4$ mvn starts:select # ﬁnd (but not run) impacted tests
5$ mvn starts:starts # ﬁnd and run impacted tests
6$ mvn starts:clean # delete ST ARTS artifacts
The ﬁrst goal, starts:help , lists all the goals in STARTS
and what they can be used for. The other ﬁve goals are related
to RTS. starts:diff displays all the Java types (including
classes, interfaces, and enums) that changed since the last timeSTARTS was run.
starts:impacted displays all types (not
just test classes) that are impacted by the changes, thereby
providing a means for change-impact analysis. starts:select
displays, but does not run, the test classes that are impacted bythe changes since the last time STARTS was run—allowing de-velopers more ﬂexibility to ﬁrst select impacted tests and thenrun those tests later.
starts:starts runs the impacted tests; it
performs the functions of the previous RTS-related goals, plus
executing the impacted tests. Finally, starts:clean removes
all artifacts that STARTS stored from a previous run (in a
.starts directory), resetting STARTS so that in the next run,
all types are considered changed (and all tests are selected to
be run, if using starts:starts ).
STARTS provides several options that give some ﬂexibil-
ity to the user. The most important option is whether ornot to update the STARTS artifacts after invoking a goal.As described in Section III-B, STARTS keeps track of thechecksums of all types from the previous run, storing them todisk and using them in the new run to ﬁnd impacted tests. AllRTS-related goals in STARTS provide an
update∗Checksums
option, which, when true , updates the stored checksums after
a run. This option is set to true by default for the goal
starts:starts , but is by default false for all other goals.
III. T ECHNIQUE AND IMPLEMENTATION
We describe the STARTS technique implemented in
STARTS, and the STARTS Maven plugin.
A. T echnique
STARTS performs static RTS (SRTS) at the class level—
tests are selected at the test-class (not test-method) level and
the dependencies of these tests are also computed at the class/-type level. Our recent work [8], showed that such class-levelSRTS outperformed method-level SRTS and was comparablewith the state-of-the-art class-level dynamic RTS techniqueEkstazi [9]. Thus, we implemented STARTS to perform class-level SRTS based on the idea of a class ﬁrewall [14]–[16],
which encloses the types that need to be retested because
they may be impacted by a code change. The class ﬁrewall
is computed on a type-dependency graph (TDG) where each
node is a type in the application and there is a directededge from one type τto another type τ
/primeifτhas a direct
use or inheritance dependency on τ/prime. Test class nodes are
also included in the TDG. If τcis a type that changed,
thenτis impacted by the change to τciff/angbracketleftτ,τ c/angbracketright∈E∗,
whereEis the set of all edges in the TDG, and∗denotesthe reﬂexive and transitive closure. The class ﬁrewall is theset of all types that can transitively reach any of the typesthat changed in the TDG, and can therefore be deﬁned asfirewall (T
c)=T c◦(E−1)∗, where Tcis the set of all
types that changed,−1denotes the inverse relation, and ◦
denotes relation composition. Given (1) classﬁles for all types(obtained from compiling a new revision) in an application and(2) checksums of classﬁles from a prior revision, STARTScan output the set of changed types (T
c), the class ﬁrewall
(firewall (Tc)), andTi, the set of impacted tests. Tiis
computed as the set difference between the set of all tests in thenew revision and the set of tests that are not in firewall (T
c)
(which is computed from the old revision). We compute Ti
this way so that it includes any newly-added tests while stillusing the TDG computed on the old revision.
B. Implementation
Figure 1 shows the STARTS architecture, which comprises
components to: (i) ﬁnd dependencies among types in the
application, (ii) construct the TDG, (iii) ﬁnd the changed typesbetween two revisions of the application, (iv) store checksumsof all types from the current revision, (v) select the tests
impacted by the changed types, and (vi) run the impacted tests.
Finding Dependencies Among Types: STARTS needs to
compute the dependencies among all types in the application.
The prototype in our prior work [8] used ASM to parse allthe
bytecode in the compiled classﬁle of a given type in order to
compute its dependencies. However, parsing entire classﬁles
just to ﬁnd dependencies is rather slow because it requiresto recursively visit each type’s ﬁelds, methods, signatures,and annotations to collect all the types that are referenced.
STARTS improves on computing dependencies among types
by only reading the constant pool in each classﬁle to determineall types that the type in the classﬁle may depend on. We usethe recent Oracle jdeps tool [17], now part of the standard Java
library, to read the constant pools. After the new revision of anapplication has been compiled to produce classﬁles, STARTSmakes a single jdeps invocation (via the jdeps API) to parseall classﬁles in the application at once, and then processes thejdeps output in memory to ﬁnd the dependencies for each type.
Constructing the Dependency Graph: The TDG contains
an edge from one type to each of its dependencies. We use
a custom graph library called yasgl [11] to construct graphsand to ﬁnd tests that can transitively reach some changedtype. We add each type as a node in a yasgl graph and adddependencies computed by jdeps as edges between nodes inthe graph. With a yasgl graph, STARTS computes the transitiveclosure of each test class to ﬁnd all types that each test dependson. Our initial prototype [8] used JGraphT [12], but yasgl isfaster for computing the transitive closure. For example, yasgltakes 1.4 sec to compute the transitive closure for a graphwith 41,960 nodes and 509,946 edges (coming from a single
module of a project with 110 test classes). JGraphT takes
2.7 sec to compute the same transitive closure, a differencethat accumulates when considering all the modules in theproject. Note that the yasgl TDG that STARTS uses does
950
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:30:51 UTC from IEEE Xplore.  Restrictions apply. Fig. 1. STARTS Architecture
not distinguish between use edges and inheritance edges, as
done in the Intertype Relation Graph (IRG) used in our initial
prototype and in prior work [16].
Finding Changed Types: STARTS ﬁnds the types that
changed since the last time it was run. STARTS uses the
same checksum function from Ekstazi [9], [10] to computea checksum that ignores debug-related information for eachclassﬁle and stores that checksum to a ﬁle. STARTS trackschanges in classﬁles because the corresponding source ﬁlecan be different yet result in the same classﬁle that is actuallyexecuted, so tracking classﬁles is more precise. Also, STARTSuses checksums for checking whether a classﬁle is modiﬁedinstead of seemingly faster methods like timestamps, whichcan be unreliable (e.g., Maven’s incremental build system isbroken [18] and often recompiles every type on each run, soone cannot rely on the timestamps of the classﬁles). Oncecompilation is complete in the new revision, STARTS com-putes the checksums of all compiled classﬁles and comparesagainst the stored checksums computed from the previousrevision for each ﬁle. If the old and new checksums differ,STARTS considers that type to have changed. If the typehad no previously computed checksum (i.e., a new type wasadded), its checksum is stored for future runs. Finally, if a typefor which STARTS previously computed a checksum cannotbe found in the new revision (i.e., an old type was deleted),then that type is no longer stored in the checksum ﬁle forfuture runs. If there is no checksum ﬁle on disk (e.g., on thevery ﬁrst run, or after running
mvn starts:clean ), STARTS
considers all types as changed.
Computing and Storing Checksums: In our initial proto-
type [8], as well as in Ekstazi, the transitive closure of each
test class in the graph was stored as a mapping from each
test class to its dependencies, i.e., a test-to-types mapping.
Further, there was one dependency ﬁle per test. Once a toolcomputed the set of types that changed, it then checked the
dependency ﬁle of each test to see if the test depends on anyof the changed types. However, we observed that STARTSdiscovers many more test dependencies than Ekstazi, due toinherent imprecision of static analysis, and that many testsshared a lot of these dependencies. As a result, we reversed thedependency storage format in STARTS to reduce the amountof repetitive checking of test dependencies by storing a type-to-
tests mapping. STARTS stores in a single ﬁle a mapping from
each type in the application to the set of tests that depend onthat type. This ﬁle is stored in a directory called
.starts under
the root directory of the application. More precisely, if the
application is a multi-module Maven-based project, STARTS
creates multiple .starts directories, each with its own type-
to-tests-mapping ﬁle, under each module, and the types mayspan across modules if that is where the dependencies lead.
Updating the checksums that are stored on disk after invoking
a STARTS goal on a new revision can be turned on or off, asdescribed in Section II.
The type-to-tests storage format that STARTS uses, together
with processing only one ﬁle on disk, greatly improves theperformance of selecting impacted tests. For example, inone project, STARTS takes 22.9 sec to check if any of thedependencies changed when using the type-to-tests, single-ﬁle format, but the same check takes 79.8 sec with Ekstazi’stest-to-types, multiple-ﬁles format. One possible modiﬁcationof the test-to-types format could be to ﬁrst read all the ﬁlesand then reverse the mapping (in memory) to be from type totests before comparing checksums. However, this modiﬁcationwould still incur the cost of reading potentially many ﬁlesfrom disk, and it would put the mapping-reversal process onthe critical path from when testing is initiated until developersobtain test results—mapping reversal in STARTS can happenin a separate ofﬂine phase that is not on the critical path.
951
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:30:51 UTC from IEEE Xplore.  Restrictions apply. Selecting Impacted Tests: STARTS uses the type-to-tests
dependency mapping from the previous revision and the set
of all changed types to ﬁnd the tests that are not impacted
by changes. STARTS then computes the impacted tests as
the difference between the set of all tests in the currentrevision and the set of non-impacted tests. Thus, newly-addedtests are always in the set of impacted tests. Dependency
graph construction on the new revision is not required to
ﬁnd impacted tests (allowing quicker computation of impacted
tests). Rather, STARTS reads the type-to-tests dependencyﬁle which was computed based on the dependency graphconstructed in the previous revision. The fact that STARTSrequires only compile-time information to ﬁnd impacted tests
can allow a clean separation of phases: an analysis phase (a)
ﬁnds changes and impacted tests, an execution phase (e) runs
the impacted tests, and a graph computation phase (g) builds
the dependency graph and uses it to create a type-to-tests
mapping for the next revision
1. This separation can enable
the choice to run STARTS in an “online mode” (the a,e, and
gphases are run together) or an “ofﬂine mode” (the aand e
phases can run separately from or in parallel with the gphase).
We did not yet implement goals to toggle the online/ofﬂinemodes, but report times for ofﬂine mode as the time for onlinemode minus the time for the gphase.
starts:select displays
the impacted tests but does not run them.
Running Impacted Tests: STARTS computes the set of
selected tests to run as previously described: it excludes non-
impacted tests from the set of all tests in the application.
Speciﬁcally, STARTS dynamically adds the non-impacted teststo the set of tests that Sureﬁre plugin is already conﬁgured tonot run. As a result, when STARTS invokes the Maven Sureﬁreplugin to run the tests, Sureﬁre will run only the tests thatare impacted by the changes. The goal
starts:starts will
perform all the previous steps to ﬁnd changed types, select
impacted tests and run those selected tests.
C. Important STARTS Options
STARTS provides a number of other options, in addition to
turning on/off the checksum ﬁle updates (Section II).
Caching jdeps output: One consideration in the design of
STARTS is how to handle the output of running jdeps on
third-party libraries (JARs). Many projects do not frequentlychange their library versions, and using jdeps to parse thelibrary code on each revision would needlessly repeat work.STARTS therefore provides options to (i) use a preprocessedcache, (ii) incrementally build the cache on each revision, and(iii) parse the third-party libraries on each revision. The defaultis to incrementally build the cache on each revision. WhenSTARTS encounters a JAR in the application’s classpath, itﬁrst checks whether a corresponding jdeps output ﬁle existsin the
jdeps-cache directory, which is found in each module
of the application. If there is one, STARTS reads it; otherwise,STARTS runs jdeps on the JAR, uses the jdeps output for its
1The gphase in STARTS is analogous to the coverage-collection ( c) phase
in Ekstazi and other dynamic RTS techniques where separation of cand e
phases is harder to achieve in practice.current processing, and stores the jdeps output for that JAR in aﬁle in the
jdeps-cache directory. The names of the ﬁles in the
jdeps-cache directory match the Group/ArtifactId/Revision
convention of naming Maven-based projects [19], and havea
.graph extension.
If there is a cache (possibly computed elsewhere or even
from different applications), STARTS can be conﬁgured tospecify the location of this cache. The following commandshows using an RTS-related goal with a preprocessed cache,where
${GRAPH _CACHE} is the directory containing the prepro-
cessed jdeps output for each third-party library and the jdepsoutput ﬁles are organized as described for the default option:
1$ mvn starts:starts −DgCache=${GRAPH_CACHE}
If no cache is input or the cache is empty, STARTS runs
jdeps on all libraries on each revision.
File Formats for Checksums and Dependencies: STARTS
supports two formats for storing the checksums of all types in
the application and the tests that transitively depend on them:the new type-to-tests (
ZLC) format and the old test-to-types
(CLZ) format (proposed in Ekstazi). Section III-B describes
these formats and their tradeoffs. ZLC is the default ﬁle format
that STARTS uses. To run STARTS using the CLZ ﬁle format:
1$ mvn starts:starts −DdepFormat=CLZ
Controlling STARTS Artifact Storage: Conﬁguring different
logging levels can control the amount of information thatSTARTS stores between runs, where the logging levels arethe same as in the
java.util.logging API. When running
at the default logging Level.INFO , STARTS stores only the
checksum and dependency ﬁle, .starts/deps.zlc , between
runs. At Level.FINEST , STARTS will store all its ﬁles: the lists
of all/impacted/non-impacted tests, the dependencies that jdepscomputed, the classpath that STARTS used, the yasgl graphthat STARTS constructed internally, and the set of changedtypes. Running at logging level
Level.FINER will store only
.starts/deps.zlc , the set of impacted tests, and the set of
all tests. To run STARTS while storing all its ﬁles:
1$ mvn starts:starts −DstartsLogging=FINEST
IV . E V ALUATION
We ran all experiments on a 3.40 GHz Intel Xeon E3-
1240 V2 machine with 16GB of RAM, running Ubuntu Linux
16.04.3 LTS and Oracle Java 64-Bit Server version 1.8.0_144.We evaluated STARTS on 32 Maven projects. These projectsinclude 21 single-module Maven projects we used in our
previous study [8] and 11 multi-module Maven projects thatwe did not evaluate before, showing that STARTS can be
integrated into larger Maven projects. We ran STARTS oneach project over a number of revisions and measured thenumber of impacted tests that STARTS selected to run, relative
to the number of all tests. We also measured the percentage ofend-to-end time taken by STARTS relative to the end-to-endtime for running all tests, i.e., RetestAll. The STARTS end-
to-end time includes the time to compile, perform selection,
952
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:30:51 UTC from IEEE Xplore.  Restrictions apply. TABLE I
STATISTICS ABOUT SELECTED TESTS AND END -TO-END TIME OF STARTS COMPARED TO RETEST ALL
Project SHAs ALL Selected Selected RTA[s] Ofﬂine Online Breakdown
[#] [#] [%] Time [%] Time [%] a e g Comp.
headius/invokebinder 68 2.1 1.6 76.0 3.3 110.7 134.8 0.0 20.0 20.0 60.0
google/compile-testing 32 7.3 3.1 44.1 4.4 118.3 137.6 0.0 18.3 13.3 68.3
apache/commons-cli 52 23.0 10.2 44.1 4.8 109.4 126.1 0.0 10.3 10.3 79.3
logstash/logstash-logback-encoder 45 18.2 3.7 23.5 5.6 112.8 129.8 0.0 13.7 12.3 74.0
apache/commons-dbutils 15 24.6 8.2 33.0 5.6 109.1 121.6 0.0 13.0 13.0 73.9
apache/commons-validator 22 61.0 13.8 22.6 6.6 93.5 107.2 0.0 17.1 11.4 71.4
apache/commons-ﬁleupload 8 12.0 3.8 31.2 6.8 98.2 102.1 0.0 12.9 5.7 81.4
apache/commons-codec 65 47.4 2.1 4.5 9.3 69.5 73.9 0.0 10.1 7.2 82.6
srt/asterisk-java 47 38.1 2.3 6.0 9.6 70.2 79.4 0.0 18.4 10.5 71.1
apache/commons-functor 20 164.0 23.2 14.1 10.7 91.7 96.3 0.0 8.7 5.8 85.6
apache/commons-compress 12 89.4 23.8 26.6 13.3 79.8 83.5 0.0 25.9 4.5 69.6
apache/commons-email 10 18.0 5.4 30.0 16.2 68.0 71.9 0.0 39.7 6.0 54.3
square/retroﬁt 13 32.2 10.3 32.2 21.1 79.7 86.8 5.9 43.0 9.7 41.4
apache/commons-lang 63 133.7 42.8 32.0 24.8 73.3 76.8 0.0 35.3 4.7 60.0
apache/commons-collections 12 164.0 7.2 4.4 25.3 58.3 58.9 0.0 10.0 1.3 88.7
AdoptOpenJDK/jitwatch 23 26.0 10.6 40.6 26.4 58.4 60.9 0.0 62.5 4.4 33.1
graphhopper/graphhopper 8 106.8 70.1 65.7 29.8 90.8 97.3 0.0 45.2 6.9 47.9
apache/commons-imaging 89 58.8 21.5 37.9 29.5 65.2 67.5 0.0 51.5 3.5 45.0
cloudera/oryx 17 58.0 17.3 29.8 37.6 85.0 91.3 6.0 40.1 6.6 47.3
robovm/robovm 11 32.0 9.1 28.4 39.5 107.5 111.6 1.4 5.7 3.6 89.3
ninjaframework/ninja 6 102.0 55.0 53.9 40.5 93.6 120.3 7.2 42.0 22.5 28.3
Average(SHORT) 30.4 58.0 16.4 32.4 17.6 87.8 96.9 1.0 25.9 8.7 64.4
apache/commons-math 63 449.9 42.4 9.4 98.3 28.9 30.3 0.3 36.8 4.7 58.2
addthis/stream-lib 7 24.0 5.4 22.6 106.4 47.5 48.4 0.0 88.8 2.1 9.1
apache/commons-io 13 99.2 23.4 23.5 132.0 43.4 43.9 0.0 85.0 1.2 13.8
brettwooldridge/HikariCP 18 26.4 22.4 84.7 132.9 95.2 96.6 0.8 92.9 1.5 4.8
opentripplanner/OpenTripPlanner 9 136.0 76.4 56.2 179.3 82.3 85.4 1.8 83.9 3.7 10.5
undertow-io/undertow 28 220.1 151.5 68.8 181.0 80.5 82.9 1.1 82.4 2.9 13.6
Graylog2/graylog2-server 14 187.6 25.8 13.8 284.0 103.5 106.3 1.8 6.0 2.6 89.6
apache/commons-pool 16 20.0 6.7 33.4 303.1 56.8 57.0 0.0 96.1 0.3 3.5
openmrs/OpenMrs 20 244.1 101.7 41.7 315.0 48.1 49.8 1.8 83.6 3.5 11.1
aws/aws-sdk-java 7 134.1 58.0 43.5 424.0 96.5 97.2 0.2 45.2 0.7 54.0
jankotek/mapdb 7 173.6 81.7 47.3 449.1 67.3 68.5 0.6 77.9 1.6 19.9
Average(LONG) 18.4 155.9 54.1 40.5 236.8 68.2 69.7 0.8 70.8 2.3 26.2
Average(OVERALL) 26.2 91.7 29.4 35.2 93.0 81.0 87.6 0.9 41.3 6.5 51.3
run the impacted tests, and update dependencies for the next
run, while the RetestAll time is compile time plus time to runall tests. We include compile time because, after a change, acontinuous integration system, e.g., Travis [20], typically alsocompiles the application. We wanted to evaluate any savings inthe overall build time when using STARTS. Table I shows foreach project (sorted by increasing RetestAll time), the numberof revisions evaluated (SHAs [#]), average number of all testsacross all revisions (ALL), average number of tests selectedby STARTS (Selected [#]), average percentage of all tests
selected by STARTS (Selected [%]), RetestAll time (RTA[s]),and average percentage of RetestAll time that STARTS takes,for both the “online” mode (Online Time [%]) (Section III-B)
that includes time for the a,e, and gphases, and the “ofﬂine”
mode (Ofﬂine Time [%]) that excludes time for the gphase.
The last columns break down STARTS time into a,e,gand
compilation (Comp.) times. In the ofﬂine mode, a developer
can get test results faster by not having to wait until STARTSﬁnishes the computation of dependencies before seeing thosetest results—dependency and transitive closure computationcan be removed from the developer’s critical path.
We divide the projects in Table I into short-running if
RetestAll takes less than one minute (upper part), and long-0 100 200 300 400 500
Average time to run RetestAll [s]020406080100120ST ARTS time / RetestAll time [%]
Fig. 2. Correlation between project end-to-end test time vs. percentage of
time to run STARTS
running if RetestAll takes more than one minute (lower part).
Table I shows that STARTS runs fewer tests compared with
RetestAll: STARTS selects between 4.4% (apache/commons-collections) and 84.7% (brettwooldridge/HikariCP) of all tests,with an average of 35.2% of all tests across all projects.Table I shows that STARTS also provides time savings, with
953
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:30:51 UTC from IEEE Xplore.  Restrictions apply. an average end-to-end time of 81.0% of RetestAll time in
the ofﬂine mode, and 87.6% of RetestAll time in the on-line mode. STARTS provides greater time savings for long-running projects (68.2% in the ofﬂine mode and 69.7% inthe online mode) than for short-running projects (87.8% inthe ofﬂine mode and 96.9% in the online mode). STARTSismore expensive than RetestAll (i.e., the ofﬂine percentage
of RetestAll time is greater than 100%) in six (of 21) short-
running projects, and only one (of 11) long-running project. Asexpected, STARTS is better suited for long-running projects.Figure 2 plots the correlation between the average RetestAlltime per project (x-axis) and the percentage time savings fromthe STARTS ofﬂine mode (y-axis); the Kendall-τ
bvalue is
−0.3, andp<0. 01, a weak negative correlation. Finally, the
breakdown of the end-to-end time shows that STARTS spendsmost of its non-compilation time in the ephase (41.3% of end-
to-end time, on average), while aand gtake up much smaller
percentages. The time for short-running projects is dominatedby compilation and these projects likely cannot beneﬁt muchfrom any RTS, including STARTS
V. L
IMITATIONS AND FUTURE WORK
We discuss some limitations of STARTS as well as future
research and development directions.
Limitations of STARTS: In our previous study [8], we found
that SRTS performed comparably with dynamic RTS (we
evaluated against Ekstazi) in terms of time. However, we alsofound that SRTS is as expected, less precise than dynamicRTS and can be unsafe. (An RTS technique is precise if it
selects to run only the impacted tests, and safe if it does not
miss to select an impacted test.) We also found that reﬂectionwas the only cause of unsafety of SRTS when comparedwith Ekstazi. STARTS does not yet address these safety andprecision limitations of SRTS. STARTS can be unsafe whenthe path between tests and changed types can only be reachedvia reﬂection, and is inherently imprecise because the static
dependencies it ﬁnds among the types in the application may
not be runtime dependencies. STARTS also assumes that thereis no test-order dependence [21], [22].
Future Work: Open research directions are how to make
SRTS more precise, how to make SRTS safer with respect
to other potential sources of unsafety (such as dependency oftests on external ﬁles or native code ), and how to apply SRTSto other programming languages. Future development plans in-clude (i) developing faster checksum and dependency storageformats, (ii) supporting other build systems, such as Bazel orGradle, (iii) making STARTS usable in continuous-integrationsystems, e.g., Jenkins or Travis, and (iv) evaluating STARTSon even larger applications than those we have evaluated sofar and further improving the scalability of STARTS.
VI. C
ONCLUSION
We presented STARTS, a publicly available, purely static,
class-level RTS tool. STARTS is motivated by the need in
the research community to further investigate static RTSapproaches, because its counterpart, dynamic RTS, is gainingsome adoption in practice. We discussed the ﬁrewall technique
on which STARTS is based, and we presented the usage,design, and implementation of STARTS. Our evaluation on32 open-source projects showed that STARTS can save timecompared to RetestAll, and we highlighted some future re-search and development directions. We believe that STARTScan help to facilitate collaboration and contribute greatly tofurther (static) RTS research.
A
CKNOWLEDGMENTS
We thank Felicia Chandra, Alex Gyori, Milica Hadzi-
Tanovic, Farah Hariri, Hanjie Wang, Xin Wei, LingmingZhang, and Peiyuan Zhao for their contributions to STARTS.Deniz Arsan and David Craig provided valuable feedback ona draft of this paper. This work was partially supported byNational Science Foundation grants CCF-1409423 and CCF-1421503. We gratefully acknowledge support for regressiontesting from Google, Microsoft, and Qualcomm.
R
EFERENCES
[1] S. Yoo and M. Harman, “Regression testing minimization, selection and
prioritization: A survey,” STVR, vol. 22, no. 2, 2012.
[2] S. Elbaum, G. Rothermel, and J. Penix, “Techniques for improving
regression testing in continuous integration development environments,”
inFSE, 2014.
[3] P. Gupta, M. Ivey, and J. Penix, “Testing at the speed and
scale of Google,” http://google-engtools.blogspot.com/2011/06/testing-at-speed-and-scale-of-google.html.
[4] N. York, “Tools for continuous integration at Google scale,” Jan 2011,
https://www.youtube.com/watch?v=b52aXZ2yi08.
[5] H. Esfahani, J. Fietz, Q. Ke, A. Kolomiets, E. Lan, E. Mavrinac,
W. Schulte, N. Sanches, and S. Kandula, “CloudBuild: Microsoft’sdistributed and caching build service,” in ICSE SEIP, 2016.
[6] A. Srivastava and J. Thiagarajan, “Effectively prioritizing tests in devel-
opment environment,” in ISSTA, 2002.
[7] K. Herzig and N. Nagappan, “Empirically detecting false test alarms
using association rules,” in ICSE, 2015.
[8] O. Legunsen, F. Hariri, A. Shi, Y . Lu, L. Zhang, and D. Marinov, “An
extensive study of static regression test selection in modern software
evolution,” in FSE, 2016.
[9] M. Gligoric, L. Eloussi, and D. Marinov, “Practical regression test
selection with dynamic ﬁle dependencies,” in ISSTA, 2015.
[10] ——, “Ekstazi: Lightweight test selection,” in ICSE Demo, 2015.
[11] “Yet Another Simple Graph Library,” https://github.com/
TestingResearchIllinois/yasgl.
[12] “JGraphT,” http://jgrapht.org/.
[13] “Introduction to Maven 2.0 Plugin Development,” https://maven.apache.
org/guides/introduction/introduction-to-plugins.html.
[14] H. K. Leung and L. White, “A study of integration testing and software
regression at the integration level,” in ICSM, 1990.
[15] D. C. Kung, J. Gao, P. Hsia, J. Lin, and Y . Toyoshima, “Class ﬁrewall,
test order, and regression testing of object-oriented programs,” JOOP,
vol. 8, no. 2, 1995.
[16] A. Orso, N. Shi, and M. J. Harrold, “Scaling regression testing to large
software systems,” in FSE, 2004.
[17] “jdeps,” https://docs.oracle.com/javase/8/docs/technotes/tools/unix/
jdeps.html.
[18] “Maven is broken by design,” https://blog.ltgt.net/
maven-is-broken-by-design/.
[19] “Guide to naming conventions,” https://maven.apache.org/guides/mini/
guide-naming-conventions.html.
[20] “Travis-CI,” https://travis-ci.org/.[21] A. Gyori, A. Shi, F. Hariri, and D. Marinov, “Reliable testing: detecting
state-polluting tests to prevent test dependency,” in ISSTA, 2015.
[22] S. Zhang, D. Jalali, J. Wuttke, K. Mucslu, W. Lam, M. D. Ernst, and
D. Notkin, “Empirically revisiting the test independence assumption,”
inISSTA, 2014.
954
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:30:51 UTC from IEEE Xplore.  Restrictions apply. 