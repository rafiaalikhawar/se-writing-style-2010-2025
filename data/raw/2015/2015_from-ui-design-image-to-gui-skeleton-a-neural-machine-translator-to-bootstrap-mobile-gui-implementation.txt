From UI Design Image to GUI Skeleton: A Neural Machine
Translator to Bootstrap Mobile GUI Implementation
Chunyang Chen1, Ting Su1∗, Guozhu Meng1,3∗, Zhenchang Xing2, and Yang Liu1
1School of Computer Science and Engineering, Nanyang Technological University, Singapore
2Research School of Computer Sciecne, Australian National University, Australia
3SKLOIS, Institute of Information Engineering, Chinese Academy of Sciences, China
chen0966@e.ntu.edu.sg,{suting,gzmeng}@ntu.edu.sg,zhenchang.xing@anu.edu.au,yangliu@ntu.edu.sg
ABSTRACT
A GUI skeletonis the starting point for implementing a UI design
image.ToobtainaGUIskeletonfromaUIdesignimage,developers
have to visually understand UI elements and their spatial layout
in the image, and then translate this understanding into properGUIcomponentsandtheircompositions.Automatingthisvisual
understandingandtranslationwouldbebeneficialforbootstraping
mobile GUI implementation, but it is a challenging task due to the
diversityofUIdesignsandthecomplexityofGUIskeletonstogener-ate.Existingtoolsarerigidastheydependonheuristically-designed
visualunderstandingandGUIgenerationrules.Inthispaper,we
present a neural machine translator that combines recent advances
in computer vision and machine translation for translating a UI
designimageintoaGUIskeleton.Ourtranslatorlearnstoextract
visual features in UI images, encode these features’ spatial layouts,
and generate GUI skeletons in a unified neural network frame-
work,withoutrequiringmanualruledevelopment.Fortrainingour
translator, we develop an automated GUI exploration method to
automaticallycollectlarge-scaleUIdatafromreal-worldapplica-
tions.Wecarryoutextensiveexperimentstoevaluatetheaccuracy,
generality and usefulness of our approach.
CCS CONCEPTS
•Software and its engineering ;•Human-centered comput-
ing→Graphical user interfaces ;
KEYWORDS
User interface, reverse engineering, deep learning
ACM Reference Format:
ChunyangChen,TingSu,GuozhuMeng,ZhenchangXing,andYangLiu.
2018. From UI Design Image to GUI Skeleton: A Neural Machine Translator
toBootstrapMobileGUIImplementation.In Proceedings of ICSE ’18: 40th
International Conference on Software Engineering , Gothenburg, Sweden, May
27-June 3, 2018 (ICSE ’18), 12 pages.
https://doi.org/10.1145/3180155.3180222
∗Corresponding authors.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5638-1/18/05...$15.00
https://doi.org/10.1145/3180155.31802221 INTRODUCTION
Mobile applications (apps) are event-centric programs with rich
Graphical User Interfaces (GUIs). An app’s GUI should not only
provideaworkinginterfaceforuserinteractions,butalsocreateanintuitiveandpleasantuserexperience.Infact,thelateriscrucialfor
anapp’ssuccessinthehighlycompetitivemarket[ 46,56].Devel-
opingtheGUIofanapproutinelyinvolvestwoseparatebutrelated
activities: design a UI and implement a UI. Designing a UI requires
proper user interaction, information architecture and visual effects
of the UI, while implementing a UI focuses on making the UI work
with proper layouts and widgets of a GUI framework. A UI design
canbecreatedfromscratchoradaptedfromUIdesignkits[ 1]or
existing apps’ GUIs, and it is usually con veyed to developers in the
form of design images to implement.
A UI design image depicts the desired UI elements and their
spatiallayoutinamatrixofpixels.ToimplementaUIdesignimage
using a GUI framework, developers must be able to translate the
pixel-based depiction of the UI (or parts of the UI) into a GUI skele-ton.AsillustratedinFigure1,aGUIskeletondefineswhatandhowthecomponentsofaGUIbuilder(e.g.,Androidlayoutsandwidgets)
should be composed in the GUI implementation for reproducing
the UI elements and their spatial layout in the UI design image.
This GUI skeleton is like the initial “bootstrap instructions” which
enablesthesubsequentGUIimplementation(e.g.,settingupfont,
color, padding, background image, and etc.)
However,thereisaconceptualgapbetweenaUIdesignimage
(i.e., a UI design in a pixel language) and the GUI skeleton (i.e., the
UI design in a language of GUI framework component names). To
bridge this gap, developers need to have a good knowledge of a
GUI framework’s components and what visual effects, interactions
and compositions these components support in order to create
an appropriate GUI skeleton for different kinds of UI elements
andspatiallayouts.Ifdevelopersdonothavethisknowledge,the
GUI implementation will become stucked, because modern GUI
implementation cannot be achieved by hardcode-positioning some
texts, images and controls. This is especially the case for mobile
apps that have to run on a wide range of screen sizes.
To overcome the knowledge barrier between UI design image
and GUI skeleton, developers may attempt to figure out what and
howtheGUIcomponentsshouldbecomposedforaUIdesignimagethroughatrial-and-errorapproach.AlthoughmodernGUIbuildersprovidestronginteractivesupport(e.g.,drag&drop,what-you-see-
is-what-you-get)forcreating aGUIimplementation,thistypeof
trail-and-error attempt would be very cumbersome and frustrating.
First, a mobile app’s GUI often involves many GUI componentsand complex spatial layout (see Figure 6(b)). Second, a complex
6652018 ACM/IEEE 40th International Conference on Software Engineering
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:36:21 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden C. Chen, T. Su, G. Meng, Z. Xing, Y. Liu
ImageView
LinearLayoutRelativeLayoutScrollViewImageButtonTex t Vi ew
RelativeLayout
InteractivePlayerView
Figure 1: Translating a design image into a Android GUI skeleton (not all)
GUIframeworksupportsdozensoflayoutsandwidgets(somemay
beinterchangeable)andflexiblecompositionoftheselayoutsand
widgets.Developerscaneasilygetlostduringthetrial-and-error
of an unfamiliar GUI framework.
Alternatively, developers can learn from GUI framework tuto-
rials or existing GUI implementations. To that end, they must be
able to find some tutorials or GUI implementations that implement
theUI designsthat aresimilar tothe desiredUI. Findingsuchtu-
torials or GUI implementations through the UI design image is achallenging image search task. It is also difficult to formulate a
concise,accuratetextqueryoftheUIdesignandtheneededGUI
components for using information retrieval (IR) methods. Devel-opers can also seek solutions for implementing a UI design from
the developer community (e.g., Stack Overflow), but they may not
always be able to obtain useful advices in time.
The UIs of apps can be very sophisticated to support complex
tasks, and they may undergo many revisions during the apps’ lifes-
pan. Considering millions ofapps beingdeveloped and maintained,
automating the translation from UI design to GUI implementation
wouldbebeneficialformobileappdevelopment.Sometools[ 30,45]
can automatically generate the GUI implementation given a UI de-
sign image. This automatic, generative approach overcomes thelimitations of the trial-and-error, search-based or ask-developer-
community approaches for transforming UI design image into GUI
skeleton. However, existing tools are rigid because they depend
onhand-designedvisualunderstandingandGUIgenerationtem-
plates which incorporate only limited UI-image-to-GUI-skeleton
translation knowledge.
Inthiswork,wepresentadeeplearningarchitecturethatdistills
thecrowd-scaleknowledgeofUIdesignsandGUIimplementations
fromexistingappsanddevelopagenerativetooltoautomatically
generate the GUI skeleton given an input UI design image. Our
generative tool can be thought of as an “expert” who knows a vast
variety of UI designs and GUI skeletons to advise developers what
and how the components of a GUI framework should be composed
for implementing a UI design image.
To build this “expert”, we must tackle two fundamental chal-
lenges. First, to be a knowledgeable expert, the generative toolmust be exposed to a knowledge source of a vast variety of UI
designs and GUI skeletons from a large number of apps. Second, to
advise developers how to translate a UI design into a GUI skeleton,
the generative tool must capture not only the UI elements con-tained in a UI design image, but it also must express how theseUIelementsrelatetoeachotherintermsofacompositionoftheGUIcomponents.Inthispaper,wepresentanautomatedGUIex-
plorationtechniquefortacklingthefirstchallengeinknowledge
source, and develop a neural machine translator that combinesrecent advances in computer vision and machine translation for
tacklingthesecondchallengeinvisualunderstandingandskeleton
generation. The neural machine translator is end-to-end trainable
using a large dataset of diverse UI screenshots and runtime GUIskeletons that are automatically collected during the automated
GUI exploration of mobile app binaries.
WeimplementanAndroidUIdatacollector[ 52,53]anduseitto
automatically collect 185,277 pairs of UI images and GUI skeletons
from 5043 Android apps. We adopt this dataset to train our neural
machine translator and conduct unprecedented large-scale evalua-
tionoftheaccuracyofourtranslatorforUI-image-to-GUI-skeleton
generation.Ourevaluationshowsthatourtranslatorcanreliably
distinguish different types of visual elements and spatial layoutsin very diverse UI images and accurately generate the right GUIcomponents and compositions for a wide range of GUI skeleton
complexity.WealsoapplyourtranslatortotheUIsof20Android
appsthatarenotinourtrainingset,andthisstudyfurtherconfirms
the generality of our translator. Through a pilot user study, we
providetheinitialevidenceoftheusefulnessofourapproachfor
bootstraping GUI implementations.
Our contributions in this work are as follows:
•Wedevelopadeep-learningbasedgenerativetoolforover-
comingtheknowledgebarrierfortranslatingUIimagesto
GUI skeletons.
•Our generative tool combines CNN and RNN models forlearning a crowd-scale knowledge of UI images and GUI
skeletons from a large number of mobile apps.
•We develop an automated GUI exploration framework toautomatically build a large dataset of UI images and GUI
skeletons for training the deep learning models.
•We show our tool’s robust visual understanding and GUI
skeleton generation capability through large-scale experi-
ments, and provide initial evidence of our tool’s usefulness
by a pilot user study.
2 PROBLEM FORMULATION
We formulate the UI-image-to-GUI-skeleton generation as a ma-
chine translation task. The input ito the machine translator is a UI
designimage(canberegardedasaUIdesigninapixellanguage,
e.g.,RGBcolororgrayscalepixels).AsshowninFigure2,thema-
chine translator should be able to “translate” the input UI design
image into a GUI skeleton, i.e., a composition of some container
components (i.e., the non-leaf nodes) and atomic components (i.e.,
the leaf nodes) of a GUI framework.
A GUI skeleton can be regarded as the UI design in a GUI frame-
worklanguagewhosevocabularyconsistsofthecomponentnames
of the GUI framework, such as Android’s RelativeLayout, TextView,
ImageButton,andtwospecialtokens(e.g.,brackets“{”and“}”)ex-
pressingthecompositionofGUIcomponents.AsshowninFigure2,
a component hierarchy can be represented as an equivalent token
sequence via depth-first traversal (DFT) and using “{” and “}” to
enclose a container’s contained components in the token sequence.
In this work, we use the token sequence representation of the GUI
skeleton as the output of the machine translator.
666
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:36:21 UTC from IEEE Xplore.  Restrictions apply. From UI Design Image to GUI Skeleton: A Neural Machine
Translator to Bootstrap Mobile GUI ImplementationICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
RelativeLayout{ View{ ImageButton TextView} { LinearLayout { TextView
TextView EditText EditText LinearLayout { EditText ImageButton } Button } ButtonRelativeLayout
View LinearLayout
ImageButton TextView TextView TextView EditText EditText LinearLayout
EditText ImageButton
Figure 2: An example of UI-image-to-GUI-skeleton generation
3 NEURAL MACHINE TRANSLATOR
Unlike normal machine translation tasks where both source and
targetlanguagesaretextdata,ourmachinetranslationtaskrequires
jointprocessingofimageandtextdata.Furthermore,unliketext
whichis asequence ofwords,our inputUI designimage contains
the spatial layout information of UI elements, and our output GUI
skeleton is a hierarchical composition of GUI components. Taking
intoaccountthesecharacteristicsofourtask,wedesignaneural
machinetranslatorwhichintegratesavisionConvolutionalNeu-
ral Network (CNN) [ 35,57], a Recurrent Neural Network (RNN)
encoder and a RNN decoder [ 15,60] in a unified framework. As
showninFigure3,givenaninputUIimage,thevisionCNNextracts
a diverseset ofimage featuresthrough asequence ofconvolution
and pooling operations. The RNN encoder then encodes the spatial
layout information of these image features to a summary vector
C, which is then used by the RNN decoder to generate the GUI
skeleton in token sequence representation.
3.1 Feature Extraction by Vision CNN
To learn about visual features and patterns of numerous UI ele-
ments from a vast amount of UI images, we need a sophisticated
model with capability of visual understanding. Convolutional Neu-
ralNetworks(CNNs)constituteonesuchclassofmodels.CNNis
inspired by the biological findings that the mammal’s visual cortex
hassmallregionsofcellsthataresensitivetospecificfeaturesof
visual receptive field [ 31,32]. These cells act as local filters over
the inputspace and they visuallyperceive the worldaround them
usingalayeredarchitectureofneuronsinthebrain[ 23].CNNis
designed to mimic this phenomenon to exploit the strong spatially
local correlation present in images.
A CNN is a sequence of layers that transform the original im-
agespatiallyintoacompactfeaturerepresentation(called feature
map)[35,37].InourCNNarchitecture,weusetwomaintypesof
layers: convolutional layer (Conv) andpooling layer (Pool),following
the pattern Conv→Pool. We stack a few Conv→Poollayers to
createadeepCNN,becauseadeepCNNcanextractmorepower-
ful features of the input images [ 35,57,61]. We do not use fully
connected layers, since we want to preserve the locality of CNN
features in order to encode their spatial layout information later.
3.1.1 Convolutional Layer
A convolutional layer accepts an input volume I∈RWIHIDI
whereWI,HIandDIare the width, height and depthof the input
volumerespectively.Ifthefirstconvolutionallayertakesasinput
an image, then WIandHIis the width and height of the image,
andDIis 1 for gray-scale image, or 3 for RGB color image (i.e., red,
green, blue channels respectively). Each cell of the input volume
isapixelvaluefrom0to255.Thecellvaluesoftheinputvolume
forthesubsequentconvolutionallayersdependontheconvolution
and pooling operations of the previous layers.A convolutional layer performs convolution operations using
filters (or kernels). A filter is a neuron that learns to look for some
visualfeatures(e.g.,variousorientededges)intheinput.Thefilters
inalayerwillonlybeconnectedtothelocalregionsoftheinput
volume. The spatial extent of this local connectivity is called the
receptive field of the neuron (i.e., filter size). The extent of the
connectivityalongthedepthisequaltothedepth DIoftheinput
volume.Theconvolutionoperationperformsdotproductsofafilter
andthelocalregionsoftheinputfollowedbyabias b∈Roffset.We
apply the non-linear activation function ReLU(x)=max(0,x)[44]
totheoutputofaconvolutionoperation.Weperformzero-padding
aroundtheborderoftheinputvolumesothattheinformationat
the border will also be preserved in convolution [49].
Belowisanexampleofapplyinga3 ×3filtertoa3 ×3regionof
a gray-scale image (i.e., DI=1) followed by ReLUactivation:
Conv(⎡⎢⎢⎢⎢⎣p
1p2p3
p4p5p6
p7p8p9⎤⎥⎥⎥⎥⎦,⎡⎢⎢⎢⎢⎣w
1w2w3
w4w5w6
w7w8w9⎤⎥⎥⎥⎥⎦)=max(0,9/summationdisplay.1
n=1pnwn+b)
wherepnis the pixel value and wnis the weight of the filer. The
resultingconvolutionvaluerepresentstheactivationofaneuron
over a regionof the input image.Intuitively,this value represents
the likelihood of a neuron “seeing” a particular visual feature over
theregion.Duringthemodeltraining,theCNNwilllearnfiltersthat
activatewhenthey“see”variousvisualfeatures,suchasanedgeof
some orientation on the first convolutional layer, and shape-like
patterns(e.g.,rectangle,circle)andmoreabstractvisualpatterns
(e.g., image region, text) on higher layers of the network [61].
Aconvolutionallayercanhave Kfilters.The Kfiltersthatare
applied to the same region of the input produce Kconvolution
values.These Kvaluesforma feature vector,representingtheobser-
vations of all Kneurons over this particular region of the image. A
filterisappliedtoeachpossiblelocalregionsoftheinput,specified
by the stride S(the number of pixels by which we slide the filter
horizontally and vertically). This produces a kernel map containing
thevaluesofperformingaconvolutionofthe d-thfilteroverthe
inputvolumewithastrideof S.Intuitively,akernelmaprepresents
the observations of a neuron over the entire image. All Kkernel
maps form a feature map of a convolutional layer.
3.1.2 Pooling Layer
Poolinglayerstakeasinputtheoutputfeaturemapofthepre-
ceding Convlayers and produce a spatially reduced feature map.
Theyreducethespatialsizeofthefeaturemapbysummarizingthe
values of neighboring groups of neurons in thesame input kernel
map. A pooling layer consists of a grid of pooling units spaced S
pixels apart, each summarizing a region of size Z×Zof the input
volume. Different from the filters in the Convlayers, pooling units
havenoweights,butimplementafixedfunction.Inourarchitec-
ture, we adopt 1-max pooling [43] which takes the maximum value
intheZ×Zregion.Asthepoolinglayeroperatesindependentlyon
every input kernel map, the depth dimension of the output feature
map remains the same as that of the input feature map.
Poolinglayersprogressivelyreducethespatialsizeofthefeature
maptoreducetheamountofparametersandcomputationinthe
network, and hence to also control overfitting. Meanwhile, pooling
also keeps the most salient information as it preserves the maxi-
mum value of each region in the depth slice. It also benefits to the
667
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:36:21 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden C. Chen, T. Su, G. Meng, Z. Xing, Y. Liu
y
convolution                               pooling                                convolution                            pooling   RNN  encoder                                   RNN decoderC
…Feature Extraction by Vision CNN Spatial Encoding
LinearLayout
{
Button
ImageView
}
……Skeleton Generation
Figure 3: Architecture of Our Neural Machine Translator for UI-Image-to-GUI-Skeleton Generation
invariance to shifting, rotation and scaling. Even if the image is
shifted/rotated/scaledbyafewpixels,theoutputbymaxoperation
will still stay the same when pooling over a region.
3.2 Spatial Layout Encoding by RNN Encoder
Our neural machine translator takes as input only a raw UI image,
requiring no detailed annotations of the structure and positions
ofvisualelements.Therefore,giventhefeaturemapoutputtedby
the vision CNN, it is important to localize the relative positions of
visual features within the input image for the effective generation
of proper GUI components and their compositions. To encodes
spatial layout information of CNN features, we run a Recurrent
Neural Network (RNN) encoder over each of the feature vectors of
the feature map outputted by the vision CNN.
The input to a RNN is a sequence of vectors (e.g., words in a
sentenceintheapplicationofRNNstoNaturalLanguageProcessingtasks). To apply the RNN model in our task, we convert the feature
map
FM∈RWHDoutputtedbythevisionCNNintoasequenceof
D-dimensionalfeaturevectors( Disthedepthofthefeaturevector).
Thelengthofthisimage-basedsequenceis W×H(i.e.,thewidth
and height of the feature map). The conversion can be done by
scanning the feature map along the width axis first and then the
heightaxis,orviceversa.Inordertocapturetherow(orcolumn)informationinthesequence,weinsertaspecialvectorattheend
ofeachrow(orcolumn)(canbethoughtofasa“.”intext),which
are referred to as positional embeddings.
An RNN recursively maps an input vector xtand a hidden state
ht−1to a new hidden state ht:ht=f(ht−1,xt) wherefis a non-
liner activation function (e.g., a LSTM unit discussed below). After
readingtheendoftheinput,thehiddenstateoftheRNNencoderisavector
Csummarizingthespatiallayoutinformationofthewhole
input feature map. Modeling long-range dependencies between
CNNfeaturesiscrucialforourtask.Forexample,weneedtocapture
the dependency between the bottom-right and top-left features of
avisualelementinanimage-basesequence.Therefore,weadopt
Long Short-Term Memory (LSTM) [ 29]. An LSTM consists of a
memorycellandthreegates,namelytheinput,outputandforget
gates. Conceptually, the memory cell stores the past contexts, and
theinputandoutputgatesallowthecelltostorecontextsforalong
period of time. Meanwhile, some contexts can be cleared by the
forgetgate.ThisspecialdesignallowstheLSTMtocapturelong-
range dependencies, which often occur in image-based sequences.
3.3 GUI Skeleton Generation by RNN Decoder
Based on the RNN-encoder’s output summary vector C, the target
tokens of the GUI framework language (i.e., the names of GUI
componentsandthespecialtokens{and})arethengeneratedby
a decoder. The token sequence representation of a GUI skeleton<START>LinearLayout
RelativeLayout
FrameLayoutScrollView
Image
ImageButton
Image
ImageButton
ImageViewImage
ImageButton
ImageView…
…
ݐ଴ ݐଵ ݐଶ ݐଷ …
Figure 4: An illustration of beam search (beam width=2)
canbeconvertedtoacomponenthierarchyviadepth-firsttraversal
(DFT) as seen in Figure 2. As the length of the generated token
sequencevariesfordifferentUIimages,weadoptaRNNdecoder
which is capable of producing a variable-length sequence. The
hiddenstateofthedecoderattime tiscomputedas f(ht−1,yt−1,C)
(fisalsoLSTM).Theconditionaldistributionofthenexttoken ytis
computedas P(yt|/angbracketleftyt−1,...,y1/angbracketright,C)=softmax(ht,yt−1,C),where
softmax function produces valid probabilities over the language
vocabulary.Notethatthehiddenstateandthenexttokenarenot
only conditioned on pastcontexts, but also the summary vector C
of the CNN features of the input image.
3.4 Model Training
Althoughourneuralmachinetranslatoriscomposedofthreeneural
networks (a vision CNN, a spatial layout RNN encoder, anda GUI
skeletongenerationRNNdecoder),thesenetworkscanbejointly
trainedend-to-endwithonelossfunction.Thetrainingdataconsists
of pairs of UI images iand corresponding GUI skeletons s(see
Section 4 for how we construct a large-scale training dataset). The
GUI skeleton is represented as a sequence of tokens s={s0,s1,...}
where each token comes from a GUI framework language (i.e., the
namesofGUIcomponentsandthetwospecialtokens{and}).Each
token is represented as a one-hot vector.
Given a UI image ias input, the model tries to maximize the
conditional probability p(s|i)of producing a target sequence of the
GUI skeleton s={s0,s1,...}. Since the length of sis unbounded, it
iscommontoapplythechainruletomodelthejointlogprobability
overs0,...,sN, where N is the length of a particular sequence as:
logp(s|i)=N/summationdisplay.1
t=0logp(st|i,/angbracketlefts0,...,st−1/angbracketright) (1)
At training time, we optimize the sum of log probabilities over
the whole training set using stochastic gradient descent [ 11]. RNN
encoder and decoder backpropagates error differentials to its in-
put,i.e.,theCNN,allowingustojointlylearntheneuralnetwork
parameters to minimize the error rate in a unified framework.
668
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:36:21 UTC from IEEE Xplore.  Restrictions apply. From UI Design Image to GUI Skeleton: A Neural Machine
Translator to Bootstrap Mobile GUI ImplementationICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
3.5 GUI Skeleton Inference
Aftertrainingtheneuralmachinetranslator,wecanuseittogener-
atetheGUIskeleton sforaUIdesignimage i.ThegeneratedGUI
skeleton should have the maximum log probability p(s|i). Gener-
atingaglobaloptimalGUIskeletonhasanimmersesearchspace.
Therefore, we adopt the beam search [ 34] to expand only a limited
set of the most promising nodes in the search space. As illustrated
in Figure 4, at each step t, the beam search maintains a set of the k
(beam-width) best sequences as candidates to generates sequences
of sizet+1. At the step t+1, the neural machine translator pro-
ducesaprobabilitydistributionoverthelanguagevocabularyfor
expandingeachofthecurrentcandidatesequencestonewcandi-
datesequencesofsize t+1.Thebeamsearchkeepsonlythebest k
sequences among all new candidate sequences of size t+1. This
processcontinuesuntilthemodelgeneratestheend-of-sequence
symbol for the kbest sequences. Then, the top-1 ranked sequence
is returned as the generated GUI skeleton for the input UI image.
4 COLLECTING LARGE-SCALE MODEL
TRAINING DATASET
To train our neural machine translator, we need a large set of pairs
of UI images and GUI skeletons from existing mobile apps. This
requiresustoexploretheapps’GUIs,takeUIscreenshots,obtain
runtimeGUIcomponenthierarchies,andassociatescreenshotswith
component hierarchies. Although some tools (e.g., Apktool [ 7], UI
Automator[ 24])mayassistthesetasks,noneofthemcanautomate
the whole data collection. Inspired by automated GUI testing tech-
niques[16],wedevelopanautomatedtechnique,termedStoat[ 53],
to explore the GUIs. During exploration, the UI screenshots paired
with their corresponding runtime GUI component hierarchies will
be automatically dumped. The dumped UI images and correspond-
ing GUI component hierarchies are like the example in Figure 2.
4.1 Exploring Application GUIs
Mobile apps are event-centric with rich GUIs, and users interact
with them by various actions ( e.g.,click,edit,scroll). Stoat
emits various UI events to simulate user actions, and automatically
exploredifferentfunctionalitiesofanapp.Tothoroughlyexplorean
app’sGUIs,ourdatacollectortriestoidentify executable GUI compo-
nents(e.g.,clickable, long-clickable editable, scrollable)onthecurrent
UIandinfer actionsfromthesecomponents’type.Forexample, if
theUIcontainsa Button,Stoatcansimulatea clickactiononit.
However, mobile platforms like Android also permit developers to
implementactionsintheappcode.Forexample,a TextView widget
may be registered with a LongClick action, which is invisible in
theUI.Withoutincorporatingtheseimplicitactions,wemayfail
to execute some app functionalities ( i.e., miss some UI images). To
overcome this issue, we integrate static analysis method (e.g., [ 9])
to scan app code and detect actions that are either registered with
UI widgets ( e.g.,setOnLongClickListener ) or implemented by
overriding class methods (e.g., onCreateOptionsMenu).
Figure 5 shows an example of using Stoat to explore an Android
app(Budget [ 12])andcollecttherequireddata.Startingfromthe
Main Page whichliststhebalanceofeachexpensegroup(e.g.,Baby,
Bill, Car), Stoat can “click” ↑which opens the Distributed Page.
On the Distributed Page, Stoat can “edit” the amount of money
distributed to an expense group (e.g., Baby). It can also “scroll” the
(a) Main Page
(d) Setting Page(b) Balance Page
(c) Distribution Page
click(“Food”)
edit(“Baby”)
 ”)))
scroll(c)scroll(a)
long-click(“Fruit”)
backclick click
Figure 5: Automatically exploring an app’s GUIs
Distributed Page toshowmoreexpensegroupsor“click”theback
button(the hardwarebackbuttonon thephone)togo backtothe
Main Page. On the Main Page, Stoat can also “click” an expense
groupwhichopensthe Balance Page or“click”thesettingbutton
whichopensthe Setting Page.OntheBalance Page,itcan“longclick”
a transaction (e.g., Fruit) to select it.
4.2 Prioritizing UI exploration
Figure5showsthereareoftenseveralexecutablecomponents/actions
onaUI.Todeterminewhichcomponent/actiontoexecute,Stoat
implementsa prioritized UI exploration method.Weconducta for-
mativestudyoftheGUIsof50GooglePlayappsfrom10categories
(e.g.,Communications, Personal, Tools), and summarize three key
observations that can affect the UI exploration performance: (a)
Frequency of action. Each action should be given chance to execute.
When an action is more frequently executed than others, its prior-
ity should be lowered. (b) Number of subsequent UIs. If an action
exhibits moresubsequent UIsafter its execution,its shouldbepri-
oritizedinfuturesothatmorenewfunctionalitiescanbevisited.(c)
Type of action. Different types of actions should be given different
priorities.Forexample,a hardware back orascrollactionshouldbe
executedatrighttime.Or,itmaydiscardthecurrentUIpageand
prevent the execution of other normal actions (e.g., edit).
Basedontheseobservations,weassigneachactiononanexe-
cutable component on a UI with an execution weight, and dynami-
callyadjustthisvalueatruntime.Theactionwiththehighestweight
value will be queued as next action to execute. The weight of an
action is determined by the formula below: execution _weiдht(a)=
(α∗Ta+β∗Ca)/γ∗Fawhereais the action, Tais the weight
of different types of actions (1 for normal UI actions (e.g., click,
edit), 0.5 for hardware back andscroll, and 2 for menu selec-
tion),Cathe number of unexplored components on the current UI,
Fais the times that ahas been executed, and α,βandγare the
weight parameters which can be determined empirically.
4.3 Excluding Duplicate UIs
After simulating an action, our data collector takes the screenshot
of the current UI and dump its runtimeGUI component hierarchy.
As seen in Figure 5, the same UI may be visited many times, for
example, to execute different features on the Main Page,o rt ov i e w
thesameexpensegrouponthe Balance Page again.Furthermore,
afteranaction,theappmaystayonthesameUI,forexample,after
editing Baby amount on the Distribution Page . To collect as diverse
UIimagesandGUIcomponenthierarchiesaspossible,weshould
avoid collecting such duplicate UIs.
669
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:36:21 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden C. Chen, T. Su, G. Meng, Z. Xing, Y. Liu
0100200300400500600Appnumber
category
(a) App category
 (b) GUI skeleton complexity
Figure 6: Android UI dataset statistics
To that end, we compare the newly collected pair of UI image
and GUI component hierarchy /angbracketleftinew,snew/angbracketrightwith already collected
pairs/angbracketleftia,sa/angbracketright.Ascomparingimagesistime-consuming,wecompare
component hierarchies. We convert GUI component hierarchies
into their token sequence representation by a depth-first traversal,
andthencomputethehashvaluesfortheresultingtokensequences.
Onlyifthehashvalueof snewdoesnotmatchthatofany sa,the
newly collected pair /angbracketleftinew,snew/angbracketrightwill be kept. Otherwise, it will
bediscard.Forexample,whengoingbackfromthe Balance Page to
theMain Page, the Main Page UI data will not be collected again.
As another example, after editing Baby amount to a different value,
theBalance Page UI image will be slightly different, but the GUI
component hierarchy remains the same. Therefore, the Balance
PageUI data will not be collected after editing Baby amount.
SomeUIactionsmaychangetheUI’sruntimecomponenthier-
archies even the app stays on the same UI after the actions, like
deletinganexpensegrouporscrollingthe Distribution Page sothat
different numbers of expense groups are visible. If the resulting
GUI component hierarchy has not been collected before, the newly
collectedpair /angbracketleftinew,snew/angbracketrightwillbekept.Insuchcases,theUIimages
beforeandaftertheactionsmaybesimilar,butwillnotbeidentical.
5 CONSTRUCTING ANDROID UI DATASET
WeimplementedourautomatedAndroidUIdatacollector,Stoat,
asdescribedinSection4.StoatusesAndroidemulators(configured
withthepopularKitKatversion,SDK4.4.2,768 ×1280screensize)to
runAndroidapps.Ituses Android UI Automator [24,28]todump
pairs of UI images and corresponding runtime GUI component
hierarchies.Soot[ 20]andDexpler[ 9]areusedforstaticanalysis.
Stoatrunsona64-bitUbuntu16.04serverwith32IntelXeonCPUs
and 189G memory, and controls 16 emulators in parallel to collect
data (each app is run for 45 minutes).
5.1 Dataset of Android Application UIs
We crawl 6000 Android apps [ 41] with the highest installation
numbersfromGooglePlay.5043appsrunssuccessfullybyStoatand
they belong to 25 categories. Figure 6(a) shows the number of apps
ineachcategory.Theother957appsrequireextrahardwaresupport
or third-party libraries which are not available in the emulator.
Stoat collected totally 185,277 pairs of UI images and GUI skele-
tons (on average about 36.7 pairs per app)1. This UI dataset is used
for training and testing our neural machine translator (see Sec-
tion6).ThecollectedGUIskeletonsuse291uniqueAndroidGUI
components,includingAndroid’snativelayoutsandwidgetsand
those from third-party libraries. The box plots in Figure 6(b) shows
the complexity of the collected GUI skeletons which varies greatly.
1Dataset can be downloaded in http://tagreorder.appspot.com/ui2code.html.(a) AC distribution00.10.20.30.40.50.60.70.8Covergae
CategoryMonkey Ours
(b) Average AC over app category
Figure 7: Effectiveness of Automated GUI Exploration
On average, a GUI skeleton has 24.73 GUI components, 7.43 con-
tainers (non-leaf components) and 5.43 layers (the longest path
from the root to a leaf component).
5.2 Effectiveness of Automated UI Exploration
To train a “knowledgeable” neural machine translator, we need
a diverse set of UIs. Note that we already exclude duplicate UIsduring data collection (see Section 4.3). Therefore, the diversityof the collected UI data depends on Stoat’s ability to thoroughly
explore an app’s GUIs. To confirm the UI-exploration effectiveness
of Stoat, we compared it with Monkey[25], an automated GUI
testing tool developed by Google and released with Android SDKs.
We use Activity Coverage (AC), rather than code coverage crite-
ria[54,62],toevaluatetheUI-explorationeffectiveness.Asandroid
appsarecomposedofactivities,whichareresponsibleforrendering
UIpages,ACcanmeasurethepercentageofhowmanydifferent
activities (UI pages) have been explored. We randomly selected
400 apps from our crawled apps, and apply both tools on them. To
achieveafaircomparison,weallocatethesameexplorationtime
(45 minutes) for the two tools. Figure 7(a) shows the AC values
achieved by the two tools in box plots, and Figure 7(b) presents the
average AC values over different app categories. On average, Stoat
achieves0.513AC,12.7%higherthanMonkey(0.455).Amongall
23 categories of these 400 apps, Stoat also outperforms Monkey.
6 EVALUATION
We evaluate our neural machine translator in three aspects, i.e.,
accuracy, generality and usefulness as follows.
6.1 Implementing Neural Machine Translator
We implement the proposed neural machine translator with six
Conv→Poollayers in theCNN model. The first Convlayer uses
64filters,andeachsubsequentlayerdoublesthenumberoffilers.
Thisisbecausehigher Convlayershavetocapturemoreabstract
anddiversevisualfeatures[ 36,61],andthusneedmoreneurons.
FollowingtheCNNlayersizingpatterns[ 49]forvisiontasks,we
setthefiltersize3,thestride1andtheamountofzeropadding2for
convolutionallayers.Thissettingallowsustoleaveallspatialdown-samplingtothe Poollayers,withthe Convlayersonlytransforming
the input volume depth-wise (determined by the number of filters
of aConvlayer). For thepooling layers, we use the mostcommon
form of pooling layer setting [ 49], i.e., pooling units of size 2 ×2
appliedwithastride2.Thissettingdownsampleseverykernelmap
by2alongbothwidthandheight,discarding75%oftheneurons.
FortheRNNencoderanddecoder,thereare256hiddenLTSMunits
to store the hidden states.
We implement our model based on the Torch [ 17] framework
written in Lua. We train the model using randomly selected 90%
670
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:36:21 UTC from IEEE Xplore.  Restrictions apply. From UI Design Image to GUI Skeleton: A Neural Machine
Translator to Bootstrap Mobile GUI ImplementationICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
of the Android UI dataset (i.e., 165,887 pairs of UI images and GUI
skeletons),andfine-tunemodelhyperparameters(thenumberof
CNNlayers,thenumberof ConvlayerfilersandthenumberofRNN
hiddenstates)usinganotherrandomlyselected3%ofAndroidUI
dataset.ThemodelistrainedinaNvidiaM40GPU(24Gmemory)
with 10 epochs for about 4.7 days. At inference time, our translator
cangenerateGUIskeletonsfor20UIimagespersecond,whichis
about 180 times faster than existing UI reverse engineering tech-
niques[45]basedontraditionalcomputervisiontechniques(e.g.,
edge detection [13], Optical-Character-Recognition (OCR) [51]).
6.2 Evaluation Metric
Let/angbracketleftit,st/angbracketrightbe a pair of UI image and GUI skeleton in the testing
dataset.Wesay stistheground-truthGUIskeletonfortheUIimage
it. Letsдbe the generated GUI skeleton for the itusing our neural
machine translator. Both standsдare in their token sequence
representation (see Section 2) in our evaluation.
Thefirstmetricweuseis exact match rate,i.e.,thepercentage
of testing pairs whose stexactly match sд. Exact match is a binary
metric,i.e.,0ifanydifference,otherwise1.Itcannottelltheextentto
which a generated GUI skeleton differs from the ground-truth GUI
skeleton. For example, no matter one or 100 differences between
the two GUI skeletons, exact match will regard them as 0.
Therefore,weadoptBLEU(BiLingualEvaluationUnderstudy)[ 47]
as another metric. BLEU is an automatic evaluation metric widely
usedinmachinetranslationstudies.Itcalculatesthesimilarityof
machine-generatedtranslationsandhuman-createdreferencetrans-
lations(i.e.,groundtruth).BLEUisdefinedastheproductofn-gram
precisionİ and brevity penalty: BLEU=BP∗exp/parenleftBig/summationtext.1N
n=1wnloдpn/parenrightBig
whereeach pnistheprecisionofthen-grams,i.e.,theratiooflength
ntoken subsequences generatedby the machine translatorthat are
also present in the ground-truth translation. wnis the weight of
different length of n-gram summing to one. It is a common prac-
tice[55]tosetNas4andwn=1
N.BPisthebrevitypenaltywhich
prevents the system from creating overly short hypotheses (that
may have higher n-gram precision). BPis 1 (c>r), otherwise
e(1−r/c)whereris the length of ground-truth translation, and c
isthelengthofmachine-generatedtranslation.BLEUgivesareal
valuewithrange[0,1]andisusuallyexpressedasapercentage.The
higher the BLEU score, the more similar the machine-generated
translation is to the ground truth translation. If the translation
results exactly match the ground truth, the BLEU score is 1 (100%).
6.3 Accuracy Evaluation
We use randomly selected ∼7% of Android UI dataset (10804 pairs
ofUIimagesandGUIskeletons)astestdataforaccuracyevaluation.
None of the test data appears in the model training data.
6.3.1 Overall Performance
As seen in Figure 8(a), among all 10804 testing UI images, the
generatedGUIskeletonsfor6513(60.28%)UIimagesexactlymatch
thegroundtruthGUIskeletons,andtheaverageBLEUscoreover
all test UI images is 79.09, when the beam width is 1 (i.e., greedy
search). Furthermore, for only 9 of all test UI images, our model
fails to generate closed brackets. This result shows that our model
successfully captures the composition information of container
components. When the beam width increases to 5, the exact match020406080100
12345Value(percentage)
Beam widthBLEU
ExactMatch
(a) Beam width020406080100
3456789Value(percentage)
Hierarchy depthBLEU
ExactMatch
(b) The depth of component hierarchy
020406080100
5 1 01 52 02 53 03 54 04 55 05 56 06 5Value(percentage)
GUIcomponentBLEU
ExactMatch
(c) #GUI Components020406080100
3 6 9 1 21 51 82 12 4Value(percentage)
ContainerBLEU
ExactMatch
(d) #Container
Figure 8: Impact of beam-width and generation target complexity
rateandtheaverageBLEUscoreincreaseto63.5%and86.94,respec-
tively. However , the increase after beam−width=2 is marginal.
Therefore, we use beam−width=2 in the following experiments
considering a balance of computation cost and accuracy.
6.3.2 Performance by Generation Target Complexity
As we show in Figure 6(b), the ground-truth GUI skeletons (i.e.,
the targets to generate) in our dataset vary greatly in terms of the
number of GUI components (#GUI components), the number of
containercomponents(#containers/compositions), andthedepth
ofcomponenthierarchy.Thesethreedimensionsdefinethecom-
plexity of GUI skeleton generation tasks. To better understand the
capability of our neural machine translator, we further analyze the
accuracy of our translator for the ground-truth GUI skeletons with
different #GUI components, #containers and depth.
As#GUIcomponentshasawiderange,webuckettheground-
truth GUI skeletons by 5 intervals of #GUI components (i.e., 1-5,
6-10, ...). Similarly, we bucket the ground-truth GUI skeletons by3 intervals of #containers (i.e., 1-3, 4-6, ...). We average the exact
match rate or the BLEU score of a generated GUI skeleton and
the corresponding ground-truth GUI skeleton for each bucket. Fig-
ure 8(b), 8(c) and 8(d) present the results.
Intuitively,the moreGUIcomponentsandthe morecontainers
to generate,the deeper of acomponent hierarchy to generate,the
more challenginga generationtask is.However,our resultsshow
that our translator works very well for a wide range of generation
target complexity.The BLEUscore remainsvery stable(above 80)
when there are 15 or more GUI components to generate, 6 to 21
containerstogenerate,and/or4to8depthofcomponenthierarchies
to generate. The test data in these ranges accounts for 66.75%,
72.37% and 77.54% of all test data, respectively. The exact match
rateremainsaroundorabove60%forarelativelynarrowerrange
of #GUI components (15-55), #containers (6-15) or the depth of
component hierarchies (4-7). Although our translator is less likely
togenerateanexactmatchwhentheGUIskeletontogenerateis
too complex ( >55 GUI components, >15 containers and/or >7
depth), it can still generate a GUI skeleton that match largely with
the ground truth (i.e., high BLEU scores).
Asurprisingfindingisthatourtranslator’saccuracydegrades
whentheGUIskeletonsistoosimple( ≤10GUIcomponents, ≤3
671
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:36:21 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden C. Chen, T. Su, G. Meng, Z. Xing, Y. Liu
(a) Text-like image
 (b) UI elements on
background image
(c) Deep component
hierarchy
(d) Complexspatial
layout
Figure 9: Examples of visual understanding and generation capability
containers,and/or ≤3depth).Wewillelaboratethecommoncauses
for this result in Section 6.3.4.
6.3.3 Analysis of Visual Understanding and Generation Capability
Werandomlysampled10%ofthegeneratedGUIskeletonsfor
manualobservation.Wemanuallystudythedifferencesbetween
these generated GUI skeletons and their ground truth (input UI
images).Thissectionanalyzesourtranslator’svisualunderstanding
andGUIskeletongenerationcapability.Section6.3.4summarizes
common causes of generation errors.
We find that our translator can reliably distinguish different
typesofvisualelementsandgeneratetherightGUIcomponents.
Figure 9(a) and 9(b) show two challenging cases. Figure 9(a) shows
thesettingUIofapuzzlegameinwhichthegameicon(highlighted
in red box), contains a table of characters. Our translator correctly
recognizes the region in the red box as an image and generates
aImageView foritinsteadof TextView.TheUIinFigure9(b)con-
tainsabackgroundimagewithsomeUIelementsintheforeground
(highlightedinredbox).Ourtranslatorcorrectlyteasesapartthe
foregroundelementsandthebackgroundimage,ratherthancon-
sidering the UI elements as part of the background image.
An interesting observation is that our translator can reliably
determine what text elements in UI images look like even when
the texts are written in different languages (e.g., Figure 9(c) and
Figure 10(b)). During automated UI exploration, different language
settings of an app may be triggered so that we can collect UI im-
agescontaining textsof differentlanguages.For theGUIskeleton
generation task, the exact text content does not matter much. Our
translator canabstract language-independenttext-like features in
UI images, which makes it language independent.
Wefindthatourtranslatorisrobusttocomplexspatiallayout
of UI elements in a UI design. Figure 9(c) shows a UI design that
requires6depthofcomponenthierarchy,andFigure9(d)showa
UI design with 60 GUI components that vary in shape, size and
alignment.Forbothcases,ourtranslatorgeneratestheexact-match
GUI skeleton as the ground truth.
We observe that many differences between the generated and
ground-truth GUI skeletons represent alternative implementations,
rather than generation errors. For example, the ground truth for
thecontrolintheredboxinFigure11(a)isa ToggleButton,while
our translator generates a Switchfor the control. ToggleButton and
Switchwould be interchangeable for implementing this control.
Figure11(b),11(c)and11(d)showthreeexamplesofusingdifferent
Androidlayouts:composecomponentsinalayout(generated)or
(a) ToggleButton or
Switch
(b) Using layout or
hard-positioning
(c) ListVieworRecy-
clerView
(d) LinearLayout or
RelativeLayout
Figure 10: Examples of alternative implementations
(a) Many similar UI
elements
(b) Similar texts on
one line
(c) Partially visible
UI elements
(d) Image-likeUIel-
ements
Figure 11: Common causes of generation errors
(a) Low contrast
background
(b) Little context in-
formation
(c) Displayed con-
tentasUItogenerate
(d) Displayed con-
tentasUItogenerate
Figure 12: Common causes of generation errors (simple UIs)
hardcode position components (ground-truth), use ListView(gener-
ated)ornewerAPI RecyclerView (ground-truth),oruse LinearLayout
(generated)or RelativeLayout (ground-truth).Whichoptionismore
appropriateforanappdependsontheapp’sdevelopmenthistory
and usage scenarios, but both options would produce the same
spatial layout effects of the UI designs.
6.3.4 Common Causes for Generation Errors
Our qualitative analysis identifies some common causes of gen-
eration errors. First, when a UI has many similar UI elements (e.g.,
Figure 11(a)), our translator sometimes may not generate the exact
same number of GUI components. Second, when several neighbor-
hoodtextsinonelineusesimilarfontsandstyles(e.g.,Figure11(b)),
ourtranslatormayregardthemasonetextcomponent.Third,when
aUIelementisonlypartiallyvisible(e.g.,coveredbysuspension
menuinFigure11(c)),ourtranslatormaynotrecognizetheblocked
UIelement.Fourth,ourmodelsometimescannotdiscriminatesmall
UI elements on top of a complex image especially when the UI ele-
mentshassimilarvisualfeaturestosomepartsoftheimage(e.g.,
the red box in Figure 11(d)).
In addition, we identify two common causes for the degraded
accuracy on generating simple UIs. First, UI elements in simple
672
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:36:21 UTC from IEEE Xplore.  Restrictions apply. From UI Design Image to GUI Skeleton: A Neural Machine
Translator to Bootstrap Mobile GUI ImplementationICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
Table 1: The accuracy results for 20 completely unseen apps
ID App name Category #Installation #Image ExactMatch (%) BLEU
1Advanced Task Killer Productivity 50M-100M 89 78.65 95.05
2ooVoo Video Call, Text&Voice Social 50M-100M 46 78.26 95.75
3ColorNote Notepad Notes Productivity 50M-100M 53 77.36 95.79
44shared Entertainment 100M-500M 57 71.93 88.72
5Badoo - Meet New People Social 50M-100M 28 71.42 93.16
6Mono Bluetooth Router Music & Audio 1M-5M 79 69.62 99.17
7Automatic Call Recorder Tools 50M-100M 59 69.49 94.71
8Flashlight HD LED Tools 50M-100M 42 66.67 91.42
9Solitaire Game 50M-100M 30 66.67 85.83
10AVG AntiVirus Communication 100M-500M 68 60.29 86.66
11ASKfm Social 50M-100M 52 59.62 92.90
12Color X Theme-ZERO launcher Personalization 1M-5M 49 59.18 78.53
13Smart Connect Tools 100M-500M 31 58.06 72.83
14History Eraser-Privacy Clean Tools 10M-50M 70 57.14 96.00
15Pixlr-Free Photo Editor Photography 50M-100M 59 47.46 77.88
16SoundCloud-Music & Audio Music & Audio 100M-500M 49 44.90 80.19
17Office Documents Viewer (Free) Personalization 1M-5M 96 42.71 88.04
18Super Backup : SMS&Contacts Tools 5M-10M 92 40.22 93.55
19Photo Effects Pro Photography 50M-100M 90 37.78 88.14
20Mobile Security & Antivirus Tools 100M-500M 69 30.43 67.94
AVERAGE 60.4 59.39 88.11
UIs often contain little context information for determining the
appropriate GUI components for them. For example, the green
rectangle in Figure 12(b) is actually a button, but our translatormistakes it as an image. Other design factors like low contrast
background in Figure 12(a) could be more problematic for a simple
UIwithlittlecontextinformation.Second,somemobileapps,like
mapnavigationandwebbrowser,haveverysimplemainUIwith
just several high-level encapsulated components inside, but the
contentbeingdisplayedinthecomponentcanberathercomplex.
For example, the navigation map in Figure 12(c) and a web pagein Figure 12(d) can be displayed by one GUI component such as
MapView andWebView.However,ourtranslatormaymistakethe
content displayed as part of the UI to implement, and generate
unnecessary basic GUI components.
6.4 Generality Evaluation
To further confirm the generality of our translator, we randomly
select another 20 apps that are not in our UI dataset. To ensure
the quantity of test data, apps that we select have at least 1 millon
installations (popular apps often have rich-content GUIs). Among
theseapps,werandomlyselect20appsforwhichourdatacollec-
tor collects more than 20 UI images. These 20 apps belong to 10
categories.Wecollectintotal 1208UIimages(in average60.4per
app). We set beam width as 2 for generating GUI skeletons.
Table 1 summarizes the information of the selected 20 apps and
theaccuracyresultsofthegeneratedGUIskeletonsontheUIimagesof these apps (sortedby exact match rate in descending order). Theaverage exact match rate is 59.47% (slightly lower than the average
exactmatchrate(63.5%)ofAndroidUItestdata),andtheaverage
BLEUscoreis88.11(slightlyhigherthantheaverageBLEUscore
(86.94) of Android UI test data). These results demonstrate the
generality of our translator.
Wemanuallyinspecttheappswithlowexactmatchrate( <50%)
or BLEU score ( <80). We observe similar causes for generation
errors as those discussed in Section 6.3.4. For example, personal-ization and photography apps have UIs for users to upload and
manipulatedocumentsorimages.Similartothemapandwebpage
examples in Figure 12(c) and Figure 12(d), our translator may mis-
takesomecontentdisplayedintheUIsaspartoftheUIstogenerate,
whichresultsinlowexactmatchrateorBLEUscorefortheseapps.
However, although the exact match rate is low for some such apps
(e.g., 17-Office Document Viewer, 19-Photo Effects Pro), the BLEUscore is high which indicates that the generated GUI skeletons still
largely match the ground truth.
Theapp Mobile Security & Antivirus isaninterestingcase,which
has the lowest exact match rate and the lowest BLEU score. We
findthatitsdevelopersuse LinearLayout orRelativeLayout rather
randomlywhenthetwolayoutsproducethesameUIeffect(similar
to the example in Figure 10(d)). In contrast, our translator tends to
useoneGUIcomponentconsistentlyforatypeofUIspatiallayout,
for example just LinearLayout, which results in many mismatches
betweenthegeneratedGUIskeletonsandtheactualimplementation
ofMobile Security & Antivirus.
6.5 Usefulness Evaluation
We conduct a pilot user study to evaluate the usefulness of the
generated GUI skeleton for bootstraping GUI implementation.
6.5.1 Procedures
WerecruiteightPhDstudentsandresearchstaffsfromourschool.
Weask eachparticipanttoimplement thesamesetof 5UIimages
in Android. We select two relatively simple UI design images, two
medium-complex images, and one complex image for the study.
Participants need to implement only a skeleton GUI that replicates
UIelementsandtheirspatiallayoutinaUIimage,withouttheneed
tosetupcomponents’properties(e.g.,font,color,padding,etc.).The
studyinvolvestwogroupsoffourparticipants:theexperimental
groupP1,P2,P3,P4who start with the generated GUI skeletons by
ourtool,andthecontrolgroup P5,P6,P7,P8whostartfromscratch.
According to pre-study background survey, all participants have
morethantwo-yearsJavaandAndroidprogrammingexperience
andhavedevelopedatleastoneAndroidapplicationfortheirwork.
Each pair of participants /angbracketleftPx,Px+4/angbracketrighthave comparable development
experience so that the experimental group has similar expertise to
the control group in total. Participants are required to use Android
Studiotoavoidtoolbiasandhaveupto20minutesforeachdesign.
We record the time used to implement the UI design images.
After each UI image’s implementation, participants are asked to
rate how satisfied they are with their implementation in five-point
likert scale (1: not satisfied at all and 5: highly satisfied). After the
experiment, we ask a research staff (not involved in the study)to judge the similarity of the implemented skeleton GUIs to the
respectiveUIimages(alsofive-pointlikertscale,1:notsimilaratall
and 5: identical layout). This judge does not know which skeleton
GUI is implemented by which group.
6.5.2 Results
Box plot in Figure 13 shows that the experiment group imple-
mentstheskeletonGUIsfasterthanthecontrolgroup(withaverage
6.14 minutes versus 15.19 minutes). In fact, the average time of the
control group is underestimated, because three participants fail to
complete at least one UI image within 20 minutes, which means
that they may need more time in the real development. In contrast,
all participants inthe experimentgroup finishall thetasks within
15minutes.Theexperimentalgrouprates90%oftheirimplemented
GUIs as highly satisfactory (5 point), as opposed to 15% highly sat-
isfactorybythecontrolgroup.Thisisconsistentwiththesimilarity
ratings of the implemented GUIs to the UI images given by the
judge. On average, the satisfactory ratings for the experiment and
control group is 4.9 versus 3.8, and the similarity ratings for the
experiment and control group is 4.2 versus 3.65.
673
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:36:21 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden C. Chen, T. Su, G. Meng, Z. Xing, Y. Liu
Measures Control Experimental
Time (minutes) 15.19 6.13*
Satisfactoriness 3.8 4.9*
Similarity 3.65 4.2**
Figure 13: The comparison of the experiment and control group. * denotes
p<0.01and ** denotes p<0.05
We believe the above results are because the generated GUI
skeletons by our tool give participants a reliable starting point
forGUIimplementation.GuidedbythegeneratedGUIskeletons,
participants are clear about what components to use and how to
composetheminaproperorder.Then,theymainlyneedtofixsome
generation errors and make some adjustment of component lay-
outs. Without the help of the generated GUI skeletons, the control
grouphastodetermineinatrial-and-errormannerwhatcompo-
nentstouseandhowtocomposethem,whichresultsinthelonger
implementation time and less satisfactory GUI implementations.
To understand the significance of the differences between the
two groups, we carry out the Mann-Whitney U test [ 22] (specif-
ically designed for small samples) on the implementation time,
satisfactory and similarity ratings. The test results in Figure 13
table suggests that our tool can significantly help the experimental
group implements skeleton GUIs faster ( p−value<0.01), creates
more satisfactory GUIs ( p−value<0.01) that are more similar
to the UI design images ( p−value<0.05). According to our ob-
servations, starting with the generated GUI skeletons, even theless experienced participants in the experimental group achieve
thecomparableperformancetothemostexperiencedparticipant
inthecontrolgroup.Althoughbynomeansconclusive,thisuser
studyprovidesinitialevidenceoftheusefulnessofourapproach
for bootstraping GUI implementation.
7 RELATED WORK
UI design and implementation require different mindset and ex-
pertise. The former is performed by user experience designers and
architectsviadesigntools( e.g.,Sketch[4],Photoshop[ 1]),whilethe
latterperformedbydevelopersviadevelopmenttools( e.g.,Android
Studio [2], Xcode [ 5]). Our work lowers the transition barrier from
UIdesignimages(theartifactsfromUIdesign)toGUIskeletons(the
starting point of GUI implementation). Existing tools well support
these two phases respectively, but none of them supports effective
transition from UI design images to GUI skeletons.
Supporting this transition is challenging due to the diversity of
UI designs and the complexity of GUI skeletons (see Figure 6(b)).Some tools [
21,30,45] use blockwise histogram based features
(e.g., scale-invariant feature transform [ 39]) and image processing
methods(e.g.,edgedetection[ 13],OCR[51])toidentifyUIelements
fromimages.Other tools(e.g.,ExportKit [ 3])usethe metadataof
UI elements in complex image formats exported by design tools
(e.g., the PSD file by Photoshop) to assist the transition from UI
designimagestoGUIimplementations. However,these toolsare
rigid because they are built on limited, hand-designed rules for
visual feature extraction and image-to-GUI transformation.
Different from these rule-based tools, our work is inspired by
recentsuccessesofdeeplearning(DL)modelsinimageclassifica-
tion[35,57],captioning[ 19,36,58],andmachinetranslation[ 15,
60].DLmodelsareentirelydata-driven,anddonotrequiremanualruledevelopment.Themostrelatedworkareimagecaptioningtech-niques,buttheytakeasinputnaturalsceneordigitaldocumentim-agesandgenerateasequenceofwordsinnaturallanguages[
36,58]
or markup ones (e.g., latex expressions [ 19]). In software engineer-
ing community, some DL based methods have been proposed to
generatecodegiveninput-output examples[ 8,27],partiallycom-
pletedcode[ 59]orfeaturedescriptions[ 26,42],orgeneratecode
comments[ 6]orcode-changecommitmessages[ 50].Butourwork
isthefirstdeeplearningbasedtechnique,trainedwithreal-world
App UI data, to convert UI requirements (in the form of UI images)
into a hierarchy of GUI components.
Beforedeployingdeeplearningmodels,ahigh-qualitydatasetis
requiredforthemodelstolearnimportantdatafeaturesforagiven
task. Computer vision and NLP communities usually adopt crowd-
sourcingapproachtodevelopsuchdatasetsformodeldevelopment
and benchmark [ 18,38]. A major contribution of our work is to
developanautomatedprogramanalysistechniquetoautomatically
collectlarge-scaleUIdatafromreal-wordappsforimplementing
and evaluating the deep learning based UI-image-to-GUI-skeleton
generation. This makes our work significantly different from exist-
ingwork[ 10]whichhassimilargoalbutaredevelopedbasedon
artificial UI data generated by rules.
Our approach is generative based on the UI design and GUI im-
plementationknowledgelearnedfromexistingapps.Analternative
waytoreusesuchknowledgeinexistingappsissearch-basedmeth-
ods. To use IR based code search methods [ 14,33,40], an accurate,
concisedescriptionofUIelementsandspatiallayoutinanimageis
requiredbuthardtoachieve.ItwouldbedesirabletosearchGUI
implementation by UI image directly but a challenge in image-GUI
search is how to match two heterogeneous data. The only workhaving this flavor is Reiss’s work [
48]. But this work internally
uses templates to transform an input UI sketch into a structuredquery for matching GUI code. These sketch-to-query templates
limit the generality of the approach. Furthermore, the fundamental
differencebetweenourgenerativeapproachandsearch-basedap-
proaches is that our approach can generate GUI skeletons that are
not present in a code base, while the searching method can only
return the information available in the code base.
8 CONCLUSION
ThispaperpresentsagenerativetoolforUI-image-to-GUI-skeleton
generation. Our tool consists of two integral parts: a deep learning
architecture and an automated UI data collection method. Our tool
possesses several distinctive advantages: 1) it integrates feature
extraction,spatialencodingandGUIskeletongenerationintoan
end-to-end trainable framework. 2) it learns to abstract informa-
tiveUIfeaturesdirectlyfromimagedata,requiringneitherhand-
craft features nor image preprocessing. 3) it learns to correlate
UIfeaturesandGUIcomponentsandcompositionsdirectlyfrom
UI images and GUI skeletons, requiring no detailed annotationsof such correlations. 4) it is trained with the first large-scale UI-image-GUI-skeleton dataset of real-world Android applications.
Theseadvantagesgivesourtoolunpr ecedenteds peed,reliability,
accuracyandgeneralityinover12000UI-image-to-GUI-skeleton
generation tasks. In the future, we will further test our tool with
differentUIresolutionsandorientations.Wewillalsoextendour
neural network components to web design and implementation.
674
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:36:21 UTC from IEEE Xplore.  Restrictions apply. From UI Design Image to GUI Skeleton: A Neural Machine
Translator to Bootstrap Mobile GUI ImplementationICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
ACKNOWLEDGMENTS
Weappreciateallparticipantsforthehumanstudy,andweespe-
cially thank Yuekang Li for the constructive discussion in Android
UIdevelopment.WealsothanktheGPUsupportfromNvidiaAI
center, Singapore to accelerate the experiments.
REFERENCES
[1] 2017. Adobe Photoshop. http://www.adobe.com/Photoshop/. (2017).
[2]2017. Android Studio. https://developer.android.com/studio/index.html. (2017).
[3]2017. Convert a psd to android xml ui and java. http://exportkit.com/learn/
how-to/export-your-psd/convert-a-psd-to-android-xml-ui-and-java. (2017).
[4]2017. Sketch:ProfessionaldigitaldesignforMac. (2017). https://www.sketchapp.
com/
[5] 2017. Xcode. https://developer.apple.com/xcode/. (2017).
[6]Miltiadis Allamanis, Hao Peng, and Charles Sutton. 2016. A convolutional at-
tention network for extreme summarization of source code. In International
Conference on Machine Learning. 2091–2100.
[7]Apktool.2017. Apktool. (2017). Retrieved2017-5-18fromhttps://ibotpeaches.
github.io/Apktool/
[8]MatejBalog,AlexanderLGaunt,MarcBrockschmidt,SebastianNowozin,and
Daniel Tarlow. 2016. Deepcoder: Learning to write programs. arXiv preprint
arXiv:1611.01989 (2016).
[9]Alexandre Bartel, Jacques Klein, Martin Monperrus, and Yves Le Traon. 2012.
Dexpler: Converting Android Dalvik Bytecode to Jimple for Static Analysis with
Soot.
[10]Tony Beltramelli. 2017. pix2code: Generating Code from a Graphical User Inter-
face Screenshot. arXiv preprint arXiv:1705.07962 (2017).
[11]Léon Bottou. 2012. Stochastic gradient descent tricks. In Neural networks: Tricks
of the trade. Springer, 421–436.
[12]Budget. 2017. Budget. (2017). Retrieved 2017-5-18 from https://github.com/
notriddle/budget-envelopes
[13]JohnCanny.1986. Acomputationalapproachtoedgedetection. IEEE Transactions
on pattern analysis and machine intelligence 6 (1986), 679–698.
[14]Wing-Kwan Chan,Hong Cheng, andDavid Lo.2012. Searchingconnected API
subgraph via text phrases. In Proceedings of the ACM SIGSOFT 20th International
Symposium on the Foundations of Software Engineering. ACM, 10.
[15]Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau,
Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase
representations using RNN encoder-decoder for statistical machine translation.
arXiv preprint arXiv:1406.1078 (2014).
[16]Shauvik Roy Choudhary, Alessandra Gorla, and Alessandro Orso. 2015. Au-
tomated Test Input Generation for Android: Are We There Yet? (E). In 30th
IEEE/ACM International Conference on Automated Software Engineering, ASE 2015,
Lincoln, NE, USA, November 9-13, 2015. 429–440.
[17]Ronan Collobert, Koray Kavukcuoglu, and Clément Farabet. 2011. Torch7: A
matlab-like environment for machine learning. In BigLearn, NIPS Workshop.
[18]JiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi,andLiFei-Fei.2009. Ima-
genet: A large-scale hierarchical image database. In Computer Vision and Pattern
Recognition, 2009. CVPR 2009. IEEE Conference on. IEEE, 248–255.
[19]Yuntian Deng, Anssi Kanervisto, Jeffrey Ling, and Alexander M Rush. 2017.Image-to-Markup Generation with Coarse-to-Fine Attention. In International
Conference on Machine Learning. 980–989.
[20]Soot Developers. 2017. Soot. (2017). Retrieved 2017-5-18 from https://github.
com/Sable/soot
[21]Morgan Dixon and James Fogarty. 2010. Prefab: implementing advanced behav-
iorsusingpixel-based reverseengine eringofinterfacestructure.In Proceedings of
the SIGCHI Conference on Human Factors in Computing Systems.ACM,1525–1534.
[22]MichaelPFayandMichaelAProschan.2010. Wilcoxon-Mann-Whitneyort-test?
On assumptions for hypothesis tests and multiple interpretations of decision
rules. Statistics surveys 4 (2010), 1.
[23]Daniel J Felleman and David C Van Essen. 1991. Distributed hierarchical pro-
cessing in the primate cerebral cortex. Cerebral cortex 1, 1 (1991), 1–47.
[24]Google. 2017. Android UI Automator. (2017). Retrieved 2017-5-18 from http:
//developer.android.com/tools/help/uiautomator/index.html
[25]Google.2017. Monkey. (2017). Retrieved2017-2-18fromhttp://developer.android.
com/tools/help/monkey.html
[26]XiaodongGu,HongyuZhang,DongmeiZhang,andSunghunKim.2016. Deep
API learning. In Proceedings of the 2016 24th ACM SIGSOFT International Sympo-
sium on Foundations of Software Engineering. ACM, 631–642.
[27]XiaodongGu,HongyuZhang,DongmeiZhang,andSunghunKim.2017.DeepAM:
Migrate APIs with Multi-modal Sequence to Sequence Learning. arXiv preprint
arXiv:1704.07734 (2017).
[28]XiaocongHe.2017. PythonwrapperofAndroidUIAutomatortesttool. (2017).
Retrieved 2017-5-18 from https://github.com/xiaocong/uiautomator[29]SeppHochreiterandJürgenSchmidhuber.1997. Longshort-termmemory. Neural
computation 9, 8 (1997), 1735–1780.
[30]RuoziHuang,YonghaoLong,andXiangpingChen.2016. AutomaticlyGenerating
Web Page From A Mockup.. In SEKE. 589–594.
[31]DavidHHubelandTorstenNWiesel.1962. Receptivefields,binocularinteraction
and functionalarchitecture in thecat’s visualcortex. The Journal of physiology
160, 1 (1962), 106–154.
[32]David H Hubel and Torsten N Wiesel. 1968. Receptive fields and functional
architecture of monkey striate cortex. The Journal of physiology 195, 1 (1968),
215–243.
[33]Iman Keivanloo, Juergen Rilling, and Ying Zou. 2014. Spotting working code ex-
amples.In Proceedings of the 36th International Conference on Software Engineering.
ACM, 664–675.
[34]PhilippKoehn.2004. Pharaoh:abeamsearchdecoderforphrase-basedstatistical
machine translation models. Machine translation: From real users to research
(2004), 115–124.
[35]AlexKrizhevsky,IlyaSutskever,andGeoffreyEHinton.2012. Imagenetclassifica-tion with deep convolutional neural networks. In Advances in neural information
processing systems. 1097–1105.
[36]Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. 2015. Deep learning. Nature
521, 7553 (2015), 436–444.
[37]Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. 1998. Gradient-
based learning applied to document recognition. Proc. IEEE 86, 11 (1998), 2278–
2324.
[38]Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays,PietroPerona,Deva
Ramanan, Piotr Dollár, and C Lawrence Zitnick. 2014. Microsoft coco: Common
objects in context. In European conference on computer vision. Springer, 740–755.
[39]DavidGLowe.1999. Objectrecognitionfromlocalscale-invariantfeatures.In
Computer vision, 1999. The proceedings of the seventh IEEE international conference
on, Vol. 2. Ieee, 1150–1157.
[40]CollinMcMillan,MarkGrechanik,DenysPoshyvanyk,QingXie,andChenFu.
2011. Portfolio:finding relevantfunctions andtheir usage.In Proceedings of the
33rd International Conference on Software Engineering. ACM, 111–120.
[41]Guozhu Meng, Yinxing Xue, Jing Kai Siow, Ting Su, Annamalai Narayanan,
and Yang Liu. 2017. AndroVault: Constructing Knowledge Graph from Mil-lions of Android Apps for Automated Analysis. CoRRabs/1711.07451 (2017).
arXiv:1711.07451 http://arxiv.org/abs/1711.07451
[42]Lili Mou, Rui Men, Ge Li, Lu Zhang, and Zhi Jin. 2015. On end-to-end pro-gram generation from user intention by deep neural networks. arXiv preprint
arXiv:1510.07211 (2015).
[43]Naila Murray and Florent Perronnin. 2014. Generalized max pooling. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern Recognition. 2473–
2480.
[44]Vinod Nair and Geoffrey E Hinton. 2010. Rectified linear units improve re-
strictedboltzmannmachines. In Proceedings of the 27th international conference
on machine learning (ICML-10). 807–814.
[45]Tuan Anh Nguyen and Christoph Csallner. 2015. Reverse engineering mobile
applicationuserinterfaceswithremaui(t).In Automated Software Engineering
(ASE), 2015 30th IEEE/ACM International Conference on. IEEE, 248–259.
[46]Greg Nudelman. 2013. Android design patterns: interaction design solutions for
developers. John Wiley & Sons.
[47]KishorePapineni,SalimRoukos,ToddWard,andWei-JingZhu.2002. BLEU:a
method for automatic evaluation of machine translation. In Proceedings of the
40th annual meeting on association for computational linguistics.Association forComputational Linguistics, 311–318.
[48]
Steven P Reiss. 2014. Seeking the user interface. In Proceedings of the 29th
ACM/IEEE international conference on Automated software engineering. ACM,
103–114.
[49]PatriceYSimard,DavidSteinkraus,JohnCPlatt,etal .2003. Bestpracticesfor
convolutionalneuralnetworksappliedtovisualdocumentanalysis..In ICDAR,
Vol. 3. 958–962.
[50]AmeerArmalySiyuanJiangandCollinMcMillan.2017.AutomaticallyGenerating
CommitMessagesfromDiffsusingNeuralMachineTranslation.In Automated
Software Engineering (ASE), 2017 32th IEEE/ACM International Conference on.
IEEE.
[51]RaySmith.2007. AnoverviewoftheTesseractOCRengine.In Document Analysis
and Recognition, 2007. ICDAR 2007. Ninth International Conference on,Vol.2.IEEE,
629–633.
[52]Ting Su. 2016. FSMdroid: Guided GUI testing of android apps. In Proceedings of
the 38th International Conference on Software Engineering, ICSE 2016, Austin, TX,
USA, May 14-22, 2016 - Companion Volume. 689–691.
[53]TingSu,GuozhuMeng,YutingChen,KeWu,WeimingYang,YaoYao,GeguangPu,
YangLiu,and ZhendongSu.2017. Guided,StochasticModel-basedGUITesting
of Android Apps. In Proceedings of the 2017 11th Joint Meeting on Foundations of
Software Engineering (ESEC/FSE 2017). ACM, New York, NY, USA, 245–256.
[54]TingSu,KeWu,WeikaiMiao,GeguangPu,JifengHe,YutingChen,andZhendong
Su. 2017. A Survey on Data-Flow Testing. ACM Comput. Surv. 50, 1, Article 5
(March 2017), 35 pages.
675
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:36:21 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden C. Chen, T. Su, G. Meng, Z. Xing, Y. Liu
[55]IlyaSutskever,OriolVinyals,andQuocVLe.2014. Sequencetosequencelearning
withneuralnetworks.In Advances in neural information processing systems.3104–
3112.
[56]SeyyedEhsanSalamatiTaba,ImanKeivanloo,YingZou,JoannaNg,andTinnyNg.
2014. Anexploratorystudyontherelationbetweenuserinterfacecomplexityand
the perceived quality. In International Conference on Web Engineering. Springer,
370–379.
[57]YanivTaigman,MingYang,Marc’AurelioRanzato,andLiorWolf.2014. Deepface:
Closing the gap to human-level performance in face verification. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition. 1701–1708.
[58]Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. 2015. Showand tell: A neural image caption generator. In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition. 3156–3164.
[59]Martin White, Christopher Vendome, Mario Linares-Vásquez, and Denys Poshy-
vanyk. 2015. Toward deep learning software repositories. In Mining Software
Repositories (MSR), 2015 IEEE/ACM 12th Working Conference on. IEEE, 334–345.
[60]Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi,
WolfgangMacherey,MaximKrikun,YuanCao,QinGao,KlausMacherey,etal .
2016. Google’sNeural Machine Translation System: Bridgingthe Gap between
Human and Machine Translation. arXiv preprint arXiv:1609.08144 (2016).
[61]MatthewDZeilerandRobFergus.2014. Visualizingandunderstandingconvolu-
tional networks. In European conference on computer vision. Springer, 818–833.
[62]Hong Zhu, Patrick A. V. Hall, and John H. R. May. 1997. Software Unit Test
Coverage and Adequacy. ACM Comput. Surv. 29, 4 (Dec. 1997), 366–427.
676
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:36:21 UTC from IEEE Xplore.  Restrictions apply. 