 
 AtexRace : Across Thread and Execution  Sampling for In-House Race  
Detection  
Yu Guo  ‚Ä† 
Department of Computer Science   
Western Michigan University   
Kalamazoo, MI, USA  
yu.guo@wmich.edu  Yan Cai  ‚Ä†, ‚Ä° 
State Key  Laboratory of Computer Science  
Institute of Softwar e, Chinese Academy of 
Sciences, Beijing, China  
ycai.mail@gmail.com  Zijiang Yang  ‚Ä° 
Department of Computer Science   
Western Michigan University   
Kalamazoo, MI, USA  
zijiang.yang@wmich.edu  
ABSTRACT   
Data race is a major source of concurrency bugs. Dynamic 
data race detection tools (e.g.,  FastTrack ) monitor the executions 
of a program to report data races occur ring in run time. However, 
such tools incur  significant overhead tha t slow s down and per-
turbs execution s. To address the issue, the state -of-the-art dy-
namic data race detection tools (e.g., LiteRace ) apply  sampling 
technique s to selectively monitor  memory accesses . Although 
they reduce  overhead, they also miss many data ra ces as con-
firmed  by existing studies . Thus , practitioners face a dilemma on 
whether to use FastTrack , which  detects more data races but is 
much slower, or  LiteRace , which  is faster but detects less  data 
races. In this paper, we propose a new sampling appro ach to 
address  the major  limitation s of current sampling techniques, 
which ignore the  facts that a data race involves two threads and 
a program under testing is  repeated ly executed. We develop a 
tool called AtexRace  to sample memory accesses across both 
threads and executions. By selectively monitoring the pairs of 
memory accesses  that have not been frequently observed in cur-
rent and previous executions , AtexRace  detects as many data 
races as FastTrack  at a cost as low as LiteRace . We have com-
pared AtexRace  against FastTrack  and LiteRace  on both Parsec 
benchmark suite and a large -scale real -world MySQL Server with 
223 test cases. The experiments confirm that AtexRace  can be a 
replacement of FastTrack  and LiteRace . 
CCS CONCEPTS  
‚Ä¢ Software and its engineering  ‚ûù Software testing and de-
bugging  ‚Ä¢ Theory of computation  ‚ûù Program verification .  
KEYWORDS  
Data race, sampling, concurrency bugs  
 ACM Reference format:  
Yu Guo, Yan Cai, and Zijiang Yang. 2017. AtexRace: Across 
Thread and Execution Sampling for In -House Rac e Detection.  In 
Proceedings of 11th Joint Meeting of the European Software Engi-
neering Conference and the A CM SIGSOFT  Symposium on the 
Foundations of Software Engineering , Paderborn, Germany , Sep-
tember 4 -8 2017 ( ESEC/FSE'17 ), 11 pages.  
http://dx.doi.org/10 .1145/3106237.3106242  
1. INTRODUCTION  
A data race  (or race for short) occurs when two or more threads 
access the same memory location at the same time , and  at least 
one of them  is a write [16]. Race is a major source of  concurren-
cy bugs [38] and may result in real -world disasters [23][29][40]. 
Static race detection techniques are scalable but may report 
many false pos itives [25][37][42][51]. Various filters have been 
developed to address this issue. However, false positives remain 
and false negatives emer ge with these filters in the static race 
detection tools  [37]. Dynamic techniques report much fewer 
false positives. They are mainly based on either the lockset disci-
pline  [44] or the happens -before  relation  [16][27]. The former  
requires that all accesses to a shared memory location should be 
protected by a common set of locks. The latter  [27] is usually 
implemente d via vector clocks  [16] to track the status of thread s, 
locks and memory location s. Happens -before based race detec-
tors (HB detectors for short) report less false positives but incur 
higher overhead than the lockset based ones . FastTrack  [16], by 
avoiding  a large number of O(n) operations on memory accesses, 
reduce s the overhead to the level as that of  the lockset based race 
detectors . Even so , by continuously monitoring all memory ac-
cesses  of a multithreaded  program , FastTrack  still incurs from 
400% to 800%  overhead [10][16][54].  
Sampling [7][34][58] is a promising technique to reduce the 
overhead of dynamic detect ors by selectively  monitor ing 
memory accesses . There are two types  of sampling. With the 
assumptions that concurrency bugs cannot be eliminated during 
testing and dail y use s of released software provide  a large test 
bed, t he first type attempts to detect races  at user sites , including 
Pacer  [7], CRSampler  [12], and a possible adaption of DataCollid-
er [14]. This type of sampling  must  be extremely light -weight  
(i.e., < 5% overhead [3][26][31][59]). And they  usually detect a 
small number  of data races depending on the sampling rate and 
the overhead limit.  
The second type aims at reducing in -house testing overhead.  
Before releasing a software , the developers usually test the pro-
gram again st a large number of test cases , and for each test case, 
the program may be executed multiple times. Lower overhead ‚Ä† Co-first author.  
‚Ä° Corresponding author s. 
Permission to m ake digital or hard copies of all or part of this work for per-
sonal or classroom use is granted without fee provided that copies are not 
made or distributed for profit or commercial advantage and that copies bear 
this notice and the full citation on the fi rst page. Copyrights for components 
of this work owned by others than the author(s) must be honored. Abstracting 
with credit is permitted. To copy otherwise, or republish, to post on servers or 
to redistribute to lists, requires prior specific permission a nd/or a fee. Request 
permissions from Permissions@acm.org.  
ESEC/FSE'17, September 04 -08, 2017, Paderborn, Germany  
¬© 2017 Copyright is held by the owner/author(s). Publication rights licensed 
to ACM.  
ACM ISBN 978 -1-4503-5105-8/17/09‚Ä¶$15.00  
http://dx.doi.or g/10.1145/3106237.3106242  
315
ESEC/FSE'17, September 4 -8 2017, Paderborn, Germany  Y. Guo, Y. Cai, and Z. Yang  
 
enables more testing and thus less races in the tested software. 
LiteRace [34] is a representative tool in this category. It is based 
on the hypothesi s that undetected races often exist in cold func-
tions  that have not been frequently called . Therefore, LiteRace  
reduces overhead by avoiding the sampling of memory accesses 
in hot functions  that have been frequently executed.  
Figure 1  shows a code sketch  with two threads t1 and t2. 
Functions f1 and f2 are repeatedly executed in t1, and f3 and f4 are  
repeatedly executed in t2. Races occur when f1 and f4 execute 
simultaneously, and when f2 and f3 execute simultaneously. As-
sume that t1 is executed more freq uently than t2 and the then-
branches are executed more frequently than the else-branches . 
Initially all functions are cold, but quickly f1 becomes hot  while 
other three functions are still cold. At this moment LiteRace stops 
monitoring  f1 and becomes faste r than FastTrack  because the 
latter still continuously monitors f1. After a while f2 and f3 get a 
chance to be executed. Since both functions are cold, LiteRace  
still monitor their  execution s and thus can report the race be-
tween f2 and f3 at a cost lower t han that of FastTrac k. Next  f4 is 
executed  at the same time with  f1. In this case  LiteRace  fails to 
detect the race between  f1 and f4 because it already stopped 
tracking  f1. On the other hand, FastTrack  can catch the race be-
cause it still monitors  f1. This example  illustrates the dilemma in 
choosing between full scale tools and sampling based tools. A 
programmer has to either sacrifices efficiency for accuracy, or 
sacri fices accuracy for efficiency.  
We argue that programmers do not have to choose between 
efficiency and accuracy. This is achievable because there are two 
major limitation s in c urrent sampling techniques. From the defi-
nition,  a race occurrence requires two memory accesses  of differ-
ent threads . Therefore,  sampling memory accesses in isolation is  
ineffective. The aforementioned ex ample shows that a function f 
may become hot before any other functions that race with f. In 
this case, sampling those functions that race with f is useless. We 
call this inefficiency thread -local sampling  because it does  not 
consider other threads when it decides  whether to sample the 
current thread. The second major limitation  is that sampling 
algorithms remain the same for all the executions of a program. 
This is ineffective because in  in-house testing a program is usu-
ally executed repeatedly against a large set of test cases. For a 
multithreaded  program, a develop may even run it multiple 
times under a single test case. The net effect of current sampling 
strategy  is that those functions that are cold in individual execu-
tion but hot in accumulative executions are repeatedly sampled. 
We call this inefficiency execution -local sampling  as it does not 
consider previous executions when decides whether to sample 
the current execution.  
In this paper,  we propose AtexRace , a new d ynamic race de-
tection tool based on  across -thread and across -execution  sam-
pling . It is designed to sample memory access pairs  from differ-
ent threads and is also aware of executions.  However, several challenges must be resolved  to make it practical . Firstly , tracking 
memory accesses  across  threads incurs much larger  overhead 
than tracking thread -local data  only (e.g., higher  cache miss 
rate). Secondly, even if a pair of memory accesses is observed to 
be race -free before, it does not mean that the pair will not race 
later.  This is because while instruction s are static, the  memory 
addresses they access  are dynamic. Lastly, AtexRace  avoids sam-
pling previously observed memory pairs, which requires addi-
tional recording. With  increa sing number of executions, the re c-
orded data set may grow rapidly , which further slow s down the 
samp ling processes (e.g., the need of more  time to search 
memory access pairs).  
We have implemented  AtexRace , FastTrack , and LiteRace  on 
top of Pin  [32] and evalua ted them on five programs on Parsec 
benchmark suite [2] and a real -world program MySQL. In the 
experiment s, we run each Parsec program for 100 times and run 
MySQL under 223 different test cases. The experimental  results 
surprisingly show that AtexRace  detects  more races in Parsec 
benchmarks than FastTrack  does! As  for MySQL, AtexRace  de-
tects  almost the same number of dynamic races as that by 
FastTrack . LiteRace , as predicted, detects  significantly fewer races 
than both FastTrack  and AtexRace . If we do not consider the 
same races that are detected again, AtexRace  detects more unique 
races than FastTrack  and LiteRace . In terms of efficiency , LiteRace  
and AtexRace  reduce  almost the same percentage of overhead on 
top of FastTrack . This makes AtexRace  a replacement of 
FastTrack  and LiteRace . The main contribution s of this paper are:  
ÔÇ∑ We present  a novel sampling technique called AtexRace  to-
ward race detection. Unlike existing sampling techniques 
that are thread -local and  execution -local, AtexRace  is across -
thread and across -execution .  
ÔÇ∑ To make AtexRace  practical, we have designed  optimization 
heuristics  that include  (1) utilizing thread -local storage to 
avoid competing accesses to shared sampling data set, (2) 
exploiting burst sampling strategy to enhance race cover-
age, and (3) adopting n-frequent  (function) pairs  to improve 
map lookup efficiency .   
ÔÇ∑ We have implemented  AtexRace  and conducted a set of ex-
periments on benchmarks includ ing a real -world large -scale 
program MySQ L. Our experiments confirm that AtexRace  
detects as many races as FastTrack  at a cost as low as LiteR-
ace. The tool is at http://lcs.ios.ac.cn/~yancai/atexrace  . 
2. BACKGROUND  
2.1 Multithreaded Programs  
A multithreaded program  consists of a set of threads , a set o f 
locks (or lock /synchronization  objects), and a set of memory 
locations  (or locations for short) . Each thread ùë° has a unique 
thread identifier  ùë°ùëñùëë, denoted as ùë°.ùë°ùëñùëë. During an execution of a 
multithreaded program p, each thread ùë° performs a seque nce of 
events  ÔÉ°e1, e2, ‚Ä¶, ekÔÉ±. An event ca n be one of the following types: 
(1) acq(m) or rel(m): synchronization  events : to acquire  or re-
lease  a lock ùëö. (Other synchronization events can be similarly 
defined [16].) (2) read (x) or write (x): memory access events : to 
read from or write to a memory location  x, and (3) call(f) or 
return (f): control events : to execute events in function f or re-
turn to execute the events from the previous function f.  
1.
2.
3.
4.Thread ùë° 
for (‚Ä¶){
if(‚Ä¶)  f1();
else f2();
}5.
6.
7.
8.Thread ùë° 
for (‚Ä¶){
if(‚Ä¶)  f3();
else f4();
} 
Figure 1. A code sketch with two threads and four function 
calls . 
316AtexRace: Across Thread and Execution Samp ling for In -House 
Race Detection  ESEC/FSE'17, September 4 -8 2017, Paderborn, Germany  
 
 
 2.2 Data Races  
Data races can be  defined according to either the lockset disci-
pline [44] or the happens -before relation [27]. In this paper, we 
adopt the later one as it is relatively precise  [16]. However,  our 
sampling strategy is independent from concrete definition s. The 
happens -before relation  (denoted as ‚Ü£, HBR for short ) is defined 
by the three rules  [27]: (1) If two events ÔÅ° and ÔÅ¢ are performed 
by the same thread, and ÔÅ° appear s before ÔÅ¢, then ÔÅ° ‚Ü£ ÔÅ¢, (2) If ÔÅ° 
is a lock release event and ÔÅ¢ is a lock acquire event on the same 
lock, and ÔÅ° appear s before ÔÅ¢, then ÔÅ° ‚Ü£ ÔÅ¢, and (3) If ÔÅ° ‚Ü£ ÔÅ¢ and ÔÅ¢ 
‚Ü£ ÔÅß, then ÔÅ° ‚Ü£ ÔÅß. Given two memory access e1 and e2 that access 
the same memory location and one of them is a write events, a 
race occurs if neither e1 ‚Ü£ e2 nor e2 ‚Ü£ e1.  
3. MOTIVATIONS  
3.1 Motivating  Example  
Figure 2 shows a multithreaded program p that extends the code 
sketch given in Figu re 1. The program consists of two threads t1 
and t2 operating on two shared variables x and y. There are two 
locks m and n protecting accesses to shared variables x and/or y. 
Given two parameters ÔÉ°a, bÔÉ±, thread t1 consecutively call s func-
tion f1 for a times and then calls function f2 for a times within a 
loop ( lines 1 ‚Äì4); and thread t2 performs similar calls to functions 
f3 and f4 each for b times ( lines 17 ‚Äì20). The four functions in-
crease the values of x and y based on the passed parameters .  
Due to the pa rallel execution of the two threads  in Figure 2, 
any pair of functions between threads t1 and t2 can potentially be 
executed simultaneously. The four pairs of functions that can be 
executed at the sam e time are  ÔÉ°f1, f3ÔÉ±, ÔÉ°f1, f4ÔÉ±, ÔÉ°f2, f3ÔÉ±, and ÔÉ°f2, f4ÔÉ±. 
For the pairs ÔÉ°f1, f4ÔÉ± and ÔÉ°f2, f3ÔÉ±, as the variable y is protected by 
different  locks (i.e., lock m in function f1 and f3 but lock n in 
function f2 and f4), races may occur. For example, if line s 9 and 
30, or lines 14 and 25, are executed at the same time, the program 
may produce incorrect results due to the race on variable y.  
3.2 Heavy Overhead of Dynamic Data Race De-
tection  
Dynamic race detectors usually incur large  overhead  [16][12] 
due to heavy instrumentation and race checking per memory access. This is unavoidable because they  have to track  whether 
the pair of a current access and a previous access violates any 
HBR. We use the memory access " x += i" in Figure 2 (line 8) to 
illustrate the overhead. For each access to the location  x, one 
function call like onRead (x) or onWrite (x) is inserted  [16], see 
Figure 3. Within  these call s, there are two types of operations  
that cost  time [16][17][34].  
The first type is from fetching shadow data (or meta data 
[16][34]) for each thread and each memory location. For each 
memory location, d ynamic ones  track all accesses to it and store 
the information at shadow memory  (e.g., shadowMemory (x) in 
Figure 3). Similarly, shadow threads  (e.g., shadowThread (t) in 
Figure 3) are used for each thread . Therefore, a memory access in 
the original program is accompanied by several additional 
memory acc esses to get the shadow data for a memory location 
and a thread (e.g., Sx and St for memory location x and thread t, 
respectively ). For the shadow threads, many instrumentation 
framework s provide fast access interface (e.g., Thread Local 
Storage in Pin [32] and Thread Execution Blocks in Windows 
[49]). However, to the best of our knowledge, no fast access to 
shadow memory is supported. The latter is much difficult in 
practice. For Jav a program, the shadow memory  could be allocat-
ed together with the memory allocation in the original program  
[17]. However, for C/C++ programs, this becomes difficult.  
The second type  is from race checking . After fetching shadow  
data, the values from two shadow data (i.e., from the memory 
location and from the current thread ) are checked to detect any 
HBR violation. This process also involves additional memory 
accesses, especially the write  operations  to maintain the access 
infor mation (i.e., to update Sx in Figure 3). Note that, FastTrack  
optimizes the process on race detection but it still requires 
maintenance (read and write) on shadow data.   
3.3 Limitations of Existing Sampling Ap-
proaches  
Although dynamic approaches incur heavy overhead, they are 
usually precise for data race detection. Therefore, sampling ap-
proaches have been proposed to reduce the runtime overhead  by 
track ing a subset of events and to detect races among them.  
Existing s ampling approaches include  deployed sampling 
[7][12] and in-house sampling [3][14]. The former approaches 
are deployed at the users‚Äô sites a fter a software  is released. Such 
approaches are  based on the crowd -source testing: if there are 
many users, races escaped  during in -house testing may be de-
tected  by sampling a tiny portion  of an  execution by each user . 
Hence, deployed  sampling require s extremely low run time Instrumentation:  
x += i; 
ÔÉà 
tmp = x; onRead (x); 
tmp += i; 
x = tmp; onWrite (x); ÔÅ° 
(a) Dynamic Data Race Detection : 
onRead(x){ //or onWrite (x) 
   Sx ‚âî shadowMem ory(x); 
   St ‚âî shadowThread (t); //t is the current 
thread.  
   if any previous and the current access to x  
violates any HBR  (from Sx and St) then   
       report  the violation as a data race.  
   end if 
   update  Sx (from St).  
}                            (b) 
 
Figure 3. An illustration on the instrumentation and race 
detection for each memory access.  
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.
14.
15.
16.Thread ùë° 
for (i=1to 2 ÔÇ¥a){
if(i< a)        f1( i);
else f2( i);
}
Function f1(i){
acq(m)
x+= i;
y+= i;
rel(m)
}
Function f2(i){
acq(n)
y+= i;
rel(n)
}17.
18.
19.
20.
21.
22.
23.
24.
25.
26.
27.
28.
29.
30.
31.
32.Thread ùë° 
for (j =1to 2 ÔÇ¥b){
if( j<b)        f3( j);
else f4( j );
}
Function f3(j){
acq(m)
x+= j;
y+= j;
rel(m)
}
Function f4(j){
acq(n)
y+= j;
rel(n)
}Shared variables :intx = 0 ,y = 0 ;Lock m , n;
Input : ÔÉ°a, bÔÉ±;
 
Figure 2. A program with races on variable y between line 
9 and line 30, and between line 14 and line 25.   
317ESEC/FSE'17, September 4 -8 2017, Paderborn, Germany  Y. Guo, Y. Cai, and Z. Yang  
 
overhead (e.g.,  5% [7]). The lat ter attempts  to reduce runtime 
overhead during in -house testing phase . The representative tool 
is LiteRace [34]. As our approach falls into this category , we dis-
cuss LiteRace  in detail in the rest of this subsection.  
LiteRa ce is based on the cold -region hypothesis: races are 
likely to occur whe n a thread is executing a cold region (i.e., the 
program portion not frequently executed ). LiteRace tries to avoid 
tracking  those frequently executed functions (i.e., hot functions). 
Initially , it sets up a thread -local sampling rate of  100% for each 
function. T his sampling rate is then gradually reduced whenever 
a function is called by the corresponding thr ead until the rate 
reaches a low bound (e.g., 0.1%). For example, in Figure 2, LiteR-
ace initially checks all events from function f1. After the function 
is executed once, the thread -local sampling rate of function f1 by 
thread t1 is reduced. If thread t2 calls function f3, the sampling 
rate of function f3 by thread t2 is also reduced in the same way.  
LiteRace  reduce s runtime overhead  at the expense of its race 
detection capability . For example, in an evaluation, it  only de-
tected about 70% of freque nt data races and about 50% of rare 
data races of continuously monitoring  tools such as FastTrack  
[34]. This is also verified by other works [7]. We explain this 
limitation of LiteRace  via our running example in Figure 2.  
Figure 4 gives  four execution cases that illustrate  how the 
functions in the two threads interleave. In each case , a column 
shows the execution of a thread  in term of function calls . The 
difference between the four  cases is at how the last call to func-
tion f1 and the first call to function f2 by thread t1 interleaves 
with the last call to function f3 and the first ca ll to function f4 by 
thread t2.  
Recall that locks m and n protect the accesses to y in func-
tions f1 and f3, and in functions f2 and f4, respectively . Because 
two different locks are used, a race on variable y occurs when 
either functions f1 and f4 execute  in parallel or functions f2 and f3 
execute in parallel. No data race occurs i n either  case (a) or case 
(b) because neither pair of functions may execute in parallel. 
That is, we can infer that accesses in function f1 happen before 
accesses in function f4 by following lock acquisition order (i.e., 
the solid arrow s) and the program order within each thread (i.e., 
the dashed arrow s). The same reasoning also applies  on the func-
tions f3 and f2. However, for case (c), there is no strict order  be-
tween the accesse s in functions f1 and f4; hence, a HB detector 
may detect the race on y from the two functions. Due to same 
reason, for case (d), the race on y from functions f3 and f4 may 
also be detected.  When LiteRace  is applied to the four cases in Figure 4, a func-
tion is not tracked after it has been called by the same thread for 
certain number of times.  Therefore, function f1 executed by 
thread t1 and f3 executed by thread t2 are no longer tracked if 
they become hot function s. In case (c), even when function f1 and 
function f4 execute in parallel, LiteRace  may miss the  race. This is 
because LiteRace  only tracks the cold function f4 without track-
ing function f1. Similarly, I n case ( d), LiteRace  may also miss the  
race.  
We bel ieve t he main reason that LiteRace  frequently fails to 
detect races , as observ ed previously [7], is that its sampling 
across threads is not coordinated. Since a data race requires two 
conflict memory access es from two threads, sampling one 
memory access from one threa d but not the other is useless. This 
is illustrated by cases (c) and (d) above. Consider an extreme case 
where all races involve a function . If this particular function is 
considered hot after being visited several times, all future sam-
plings are in vein.  
Besides the issue of thread -local sampl ing, LiteRace  also suf-
fers from execution -local sampling. When testing  a multithread-
ed program by running it repeatedly against a large number of 
test cases , the same thread interleaving , with minor variations, 
tend to be exercised since thread schedulers generally switch 
among threads at the same program locations. In addition, alt-
hough the whole program execution may witness variants from 
one run to another, partial execution  may exhibit similar behav-
iour. For example,  even all the four cases  in Figure 4 are executed 
in different runs , the initial interleaving of two threads are simi-
lar. That is,  functions f1 and f3 interleave until fu nction s f2 or f4 is 
called. We highlight these function calls in grey background for 
illustration purpose.  As LiteRace  is unware of execution similari-
ties, it adopts the same sampling strategy across different execu-
tions. The net effect of strategy  is that  those functions that are 
cold in individual execution but hot in accumulative executions 
are repeatedly sampled. This defeats the principle of sampling 
that the real cold cases should be tracked.   
The two main limitations of current sampling  techniques mo-
tivate  our work in this paper.   
4. OUR APPROACH  
4.1 Goal and Challenges  
In this section , we present our approach to fix the two limita-
tions of current sampling techniques . In order to address thread -
local sampling , our  insight is that  whether  to sample a memory 
access event should also depend on the execution of other 
threads  and those already observed executions . That is, even if a 
memory address has been accessed by a thread  many times , we 
may still need to sample it if a second  thread access the memory 
Func fy Thread ty
Func fz Thread tzFunc fx Thread tx
Func pairs(fx, fy)
(fy, fz)
sampleRace 
detectorSaved sample info 
Figure 5. The overview of AtexRace  framework . 
Case (a) Case (b) Case (c) Case (d)
ùë° ùë° ùë° ùë° ùë° ùë° ùë° ùë° 
f1()
‚Ä¶
f1()
f2()
‚Ä¶
f2()f3()
‚Ä¶
f3()
f4()
‚Ä¶
f4()f1()
‚Ä¶
f1()
f2()
‚Ä¶
f2()f3()
‚Ä¶
f3()
f4()
‚Ä¶
f4()f1()
‚Ä¶
f1()
f2()
‚Ä¶
f2()f3()
‚Ä¶
f3()
f4()
‚Ä¶
f4()f1()
‚Ä¶
f1()
f2()
‚Ä¶
f2()f3()
‚Ä¶
f3()
f4()
‚Ä¶
f4()
No race. No race. Race: ÔÉ°f1, f4ÔÉ±
May be missed 
byLiteRace .Race: ÔÉ°f2, f3ÔÉ±
May be missed 
byLiteRace .Similar Executions: Lock order : Program order Legend: 
 
Figure 4. Three executions scenarios of the program in 
Figure 2 and the similarity of different executions .  
318AtexRace: Across Thread and Execution Samp ling for In -House 
Race Detection  ESEC/FSE'17, September 4 -8 2017, Paderborn, Germany  
 
 
 addres s for the first time. As for execution -local sampling , our 
idea is to keep and store sampling information from previous 
runs. Except the first execution that starts with a cold run, the 
subsequent executions load sampling infor mation of 
accumulated prior e xecutions. Although  such ap proach incurs 
overhead, we blieve less sampling with optimization heuristics 
can lead  to net benefit.  
The new sampling approach,  AtexRace , also works at function 
levels like LiteRace . But u nlike LiteRace , AtexRace  mainly samples  
accesses inside  a pair of  functions whose  simultaneous execu-
tions are not observed before , including previous executions . 
Unfortunately, a basic implementation of the idea is not very 
scalable. Firstly, tracking executions across threads usually incur 
larger overhead than thread -local tracking. Secondly, even two 
functions are observed to have executed in parallel before, data 
races may still occur within them. Thirdly, as AtexRace  perform s 
sampling across different execution s instead of  within a single 
execution, it must  effectively record function interleaving infor-
mation to be used in the subsequent executions.   
4.2 Basic AtexRace  Algorithm  
The overview of AtexRace  is shown in Figure 5. During execu-
tion. when functio n fy in thread ty is being executed, AtexRace  
collects all the f unctions  (e.g.,  fx and fz) that are being executed 
by other  threads. By doing so AtexRace  forms  pairs of functions 
that are being executed simultaneously (e.g.,  ÔÉ°fx, fyÔÉ±). It then 
makes a samp ling dec ision according to whether a pair of func-
tions have  been executed in parallel before. If so, neither func-
tion is sample; otherwise, both are sampled. If a function  is sam-
pled, all its events are passed to a race detector. At the end of an 
execution , all function  pairs are saved and will be used in the 
next execution. Note, in order not to report false positives , all 
synchronizations are fully sampled. This is the same as LiteRace .  
Algorithm 1  gives the  basic  AtexRace  algorithm  that takes a 
program p and a set of function pairs FPair  that have been ob-
served in the previous executions.  The first three lines initialize 
two necessary runtime data structures: a map  F that maintains 
the functions being executed by each thread, and a map  S that 
indicates w hether memory accesses from a thread should be 
sampled. Both F and S are empty initially.  
The function onCallFunc  (lines 5‚Äì19) is the core of our sam-
pling . Whenever a function f is to be executed  (i.e., at the en-
trance of function f) by a thread t, for every other thread  t' in 
program p, AtexRace  check s whether the pair ÔÉ°f, F(t')ÔÉ± already 
exists in FPairs . If not, S is updated to map both threads t and t' to 
true; otherwise, S maps t to false . A true value of S(t) mandate s 
sampling of the current memory ac cess in thread t and a false 
value does the opposite. Next, AtexRace  executes events  in func-
tion f (line 1 4) and samples its memory accesses (i.e., function 
onMemoryAccesses ) if S(t) is true. At the end of the call to func-
tion f, AtexRace  merges FPairs  and the observed function pair s ÔÉ°f, 
F(t')ÔÉ±, which  indicate s that the function f and another function 
F(t') in thread t' have been executed simultaneously .  
In practice, two functions from different threads are usually 
called at different time. Therefore, it i s the case that, a function f 
is initially not sampled but later it should be sampled as a differ-
ent thread t' calls a function f' = F (t') and the pair ÔÉ°f, F(t')ÔÉ± is nev-
er observed before. This is considered by AtexRace . We can see from lines 10 and 11 th at at the  call entrance to function f', 
thread t' also performs an iteration over other  threads  at line 7 . 
At the iteration on thread t, it cannot find the pair in FPair s. 
Then it maps both threads t' and t to be true value in structure S. 
So, the function  f executed by thread t has to be sampled.  
4.3 Limitations of Basic AtexRace  
The basic  sampling algorithm of  AtexRace  suffers from the two 
limitation s: (1) given two function f1 and f2, even if their parallel 
execution has been observed and tracked (thus beco me hot), 
races betw een them may still not detected; and (2) significant 
overhead resulted from across thread and execution sampling.  
The first limitation  is the issue of  Race Coverage . A function 
usually  contains multiple basic blocks  (BBLs ). An execution  of a 
function does not mean all its BBLs are executed. For example, 
Figure 6 shows  two functions f5 and f6 that contain  two races on  
variables x (lines 6 and 2 1) and y (lines 1 8 and 9).  There are four 
BBLs b11, b12, b21, and b22 (we omit other BBLs in the if statement  
for simplicity).  Since the  two threads in the example execute 
f5(10) and f6(100), respectively , only b11 and b22 are executed. 
Hence, the race on variable x (lines 6 and 2 4) is detected  while 
the race  on variable y (lines 19 and 10) is not . If the pair ÔÉ°f5, f6ÔÉ± is 
considered hot after this execution, the race on y can never be 
detected by the basic AtexRace . One approach to address this 
issue  is to degrade the sampling level from functions to BBLs and 
then apply either LiteRace  or the Part 1 of our AtexRace . Howev-
er, this bring heavy runtime overhead and may even  incur more 
overhead than a full detector  such as FastTrack . This is because, 
compared to a function, a BBL usually contains much fewer in-
struc tions. As a result, the sampling overhead (in time) per BBL 
may already larger than the race detection overhead  without 
sampling . Because sampling algorithm is not extremely light -
weight, it is not worthy to perform sampling at BBL level .  Algorithm 1 : Basic  AtexRace  
  
  
 
1.   
2.   
3.   
4.   
5.   
6.   
7.   
8.   
9.   
10.   
11.   
12.   
13.   
14.   
15.   
16.   
17.    
18.   
19.    
20.   
21.    
22.   
23.   
24.   
25.     Input : p ‚Äì a multithreaded  program.  
Input : FPairs ‚Äì a set containing functions.  
  
let F be an empty  map from a thread to a function  
let S be a map from a thread to a Boolean value.  
for each  thread t ÔÉé p, F(t) ‚âî ÔÉÜ, S(t) ‚âî true end for 
 
Function  onCallFunc  (Thread  t, Func f ) 
‚îÇ  let F(t) ‚âî f and St ‚âî false //St is a temp orary variable that keeps S(t)  
‚îÇ  for each  thread t' ÔÉé p, t ÔÇπ t' do 
‚îÇ  ‚îÇ  pair ‚âî ÔÉ°f, F (t')ÔÉ±  
‚îÇ  ‚îÇ  if pair ÔÉè FPairs then   
‚îÇ  ‚îÇ  ‚îÇ  St ‚âî true 
‚îÇ  ‚îÇ  ‚îÇ  S(t') ‚âî true 
‚îÇ  ‚îÇ  ‚îîend if 
‚îÇ  ‚îîend for  
‚îÇ  S(t) ‚âî St  
‚îÇ  execute  f 
‚îÇ  for each  thread t' ÔÉé p, t ÔÇπ t' do  
‚îÇ  ‚îÇ  FPairs ‚âî FPairs ‚à™ {ÔÉ°f, F (t')ÔÉ±}  
‚îÇ  ‚îîend for 
‚îî end Function  
Function onMemoryAccess (Thread t, Event e) 
‚îÇ  if S(t) =  true then   
‚îÇ  ‚îÇ  call data race detector  
‚îÇ  ‚îîend if 
‚îî end Function  
save FPairs  
 
319ESEC/FSE'17, September 4 -8 2017, Paderborn, Germany  Y. Guo, Y. Cai, and Z. Yang  
 
On the other han d, for C/C++ programs, even an instruction 
contains one or more memory accesses, it is possible that each 
execution of the instruction may accesses different memory loca-
tion. For example, considering the following two lines of code:  
1.  Object obj = &getObj  (‚Ä¶); 
2.  obj ->val ++; 
We can observe that, within the same and repeated execu-
tions of the two lines , if the point er obj points to different ob-
jects, it accesses different memory locations  at line 2 . Therefore, 
for sampled memory accesses, it is still necessa ry to track them.   
The second limitation  is the Sampling  Overhead  of AtexRace  
itself.  A sampling tool should sample as fewer memory accesses 
as possible to reduce the overhead. At the same time , it should 
also try to incur less overhead from its sampling s trategy. LiteR-
ace adopts thread -local sampling  and requires two thread -local 
counters per -function . This can be efficiently implemented [34].  
For AtexRace , there are expansive map queries  (i.e., FPairs ) on 
each function call ( lines 9 ‚Äì10). These operations bring heavy 
slowdown for two reasons. Firstly, with the increasing number 
of function calls by multiple threads, the size of FPair s also in-
creases, resulting in a large data set . For example, in our experi-
ment, after 223 execu tions  on MySQL, there are nearly 70,000 
function pairs . A query over such a large map is time consuming . 
Secondly, the map FPairs  is accessed by multiple threads. This 
requires synchronizations among different threads when they 
operate on the map. Such syn chronization incurs  further slow-
down. Besides, when different threads access  the map FPairs , the 
cache miss rate will be higher because  once a thread updates the 
map, all other threads that query the map must  wait until their 
local cache s are updated. This  again leads to additional time con-
sumption. All these reasons bring challenges to reduce the over-
head of our sampling algorithm AtexRace  itself.   
4.4 Optimizations  
Algorithm 2 is an enhancement to the basic AtexRace  algorithm 
that addresses the two kinds of limitations .  
To address the issue of race coverage, AtexRace  further sam-
ples those sampled function pairs in order to increase their  cov-
erage on data race detection. This corresponds to lines 18‚Äì24 in 
Algorithm 2. For this part, AtexRace  accept s a sampli ng rate (i.e., 
the input r to Algorithm 2) and samples the function pair accord-
ing the rate. Note that, AtexRace  does not   perform a simple 
sampling  that generate s a random number and compare s the random number with the given sampling rate . Instead, AtexRa ce 
adopts burst sampling strategy [34]. It samples the first n con-
secutive calls out of all m calls to a function such that the rate ( n 
√∑ m) ÔÇ¥ 100% equals to the given sampling rate r. For example, if 
the sampling rate is 10%, it samples the first 10 calls and discards  
the next 90 calls to the same function, resulting the sampling rate 
of 10%. Of course, to implement this functionality, a counter 
mapped from each function pair is required. Hence, the original 
set of function pai rs is changed into a map (see the fourth input 
and the lines 18, 19 and 29 in Algorithm 2).  
To overcome the second kind of limitations , we firstly pro-
pose to use thread -local maps.  In Algorithm 2, we use the symbol 
FP to denote the thread -local maps of fu nction pairs. That is, we 
allocate one map structure for each thread ; and w hen AtexRace  
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.Thread ùë° 
f5(10);
Function f5(i){
if ( i< 100) {
acq(m)
x++;
rel(m)
} else{
acq(n)
x += y;
rel(n);
}
}14.
15.
16.
17.
18.
19.
20.
21.
22.
23.
24.
25.
26.Thread ùë° 
f6(100);
Function f6(j){
if ( j< 100) {
acq(m)
y++;
rel(m)
} else{
acq(n);
y += x;
rel(n);
}
}BBL b11
BBL b12BBL b21
BBL b22 
Figure 6. A program consisting of two threads with two 
data races on variables x (lines 6 and 23) and y (lines 19 
and 10).   Algorithm 2 : Complete AtexRace  
  
  
 
 
 
  
  
1.   
2.   
3.   
4.   
5.   
6.   
7.   
8.   
9.   
10.   
11.   
12.   
13.   
14.    
15.   
16.    
17.   
18.   
19.   
20.   
21.   
22.   
23.    
24.   
25.   
26.   
27.   
28.   
29.   
30.   
31.   
32.   
33.   
34.   
35.   
36.   
37.   
38.   
39.    
40.   
41.   
42.   
43.      Input : p ‚Äì a multithreaded  program.  
Input : r ‚Äì a sampling rate . 
Input : n ‚Äì a number determine n -frequent value  
Input : FPairs ‚Äì a map (from functions pai rs to counters ) of the last 
n - 1 executions .  
 
//Initialization  
let F be an empty  map from a thread to a function  
let S be a map from a thread to a Boolean value.  
let FP be a map from a thread to a copy of FPair s. //thread -local map s 
for each  thread t ÔÉé p do 
‚îÇ  F(t) ‚âî ÔÉÜ 
‚îÇ  S(t) ‚âî true 
‚îÇ  FP(t) ‚âî FPair s //deep clone  
‚îîend for 
//Runtime Sampling  
Function  onEnterFunc (Thread  t, Func f) 
‚îÇ  let F(t) := f and St := false  //St is a temp variable that keeps S(t)   
‚îÇ  for each thread t' ÔÉé p, t ÔÇπ t' do 
‚îÇ  ‚îÇ  pair ‚âî ÔÉ°f, F (t')ÔÉ±  
‚îÇ  ‚îÇ  if pair ÔÉè FP(t) then   
‚îÇ  ‚îÇ  ‚îÇ  St ‚âî true 
‚îÇ  ‚îÇ  ‚îÇ  S(t') ‚âî true 
‚îÇ  ‚îÇ  else   
‚îÇ  ‚îÇ  ‚îÇ  FP(t) ‚âî FP(t) ‚à™ {ÔÉ°pair, Counter (FP, pair) + 1ÔÉ±}  
‚îÇ  ‚îÇ  ‚îÇ  if counter (pair, FP(t)) satisfies  r then   
‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  St ‚âî true 
‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  S(t') ‚âî true 
‚îÇ  ‚îÇ  ‚îÇ  else   
‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  St ‚âî false 
‚îÇ  ‚îÇ  ‚îÇ  ‚îîend if 
‚îÇ  ‚îÇ  ‚îîend if 
‚îÇ  ‚îîend for   
‚îÇ  S(t) ‚âî St 
‚îÇ  execute  f 
‚îÇ  for each  thread t' ÔÉé p, t ÔÇπ t' do  
‚îÇ  ‚îÇ  FP(t) ‚âî FP(t) ‚à™ {ÔÉ°pair, 1ÔÉ±} 
‚îÇ  ‚îîend for 
‚îî end Function  
Function onMemoryAccess (Thread t, Event e) 
‚îÇ  if S(t) =  true then   
‚îÇ  ‚îÇ  call data race detector  
‚îÇ  ‚îîend if 
‚îî end Function  
//The End of an Execution  
Let FPairs' be an empty map.  
for each  thread t ÔÉé p do 
‚îÇ  FPairs'  ‚âî FPairs'  ‚à™ F(t) 
‚îîend for 
save FPairs'  
 
320AtexRace: Across Thread and Execution Samp ling for In -House 
Race Detection  ESEC/FSE'17, September 4 -8 2017, Paderborn, Germany  
 
 
 starts an execution, it duplicates the given map data  (line 7). 
During an execution, AtexRace  only checks whether the pair 
exists in the map FP of the current thread ( lines 14 and 19). If a 
pair already exists in a thread -local map, its counter is incre-
mented by 1 at line 18. At the end of an execution, AtexRace  
merges all thread -local maps and saves the merged map (lines 
39‚Äì43).  
Secondly, we do not record all function  pairs observed in pre-
viously executions. Instead, we only keep the recently frequently  
observed function pairs . Given an execution e and a number n (n 
‚â•1), we define a function pair ÔÉ°fx, fyÔÉ± to be n-frequen t with respect 
to execution e if ÔÉ°fx, fyÔÉ± is obse rved in current and all the  n-1 
previous executions. Specially, when the value of n is 1, the 1-
frequent  function pairs are those observed in the current execu-
tion. By keep ing only,  the n-frequent  function pairs, the recorded 
function pairs are those frequ ently executed . This is reasonable 
not to  sample these frequent function pairs  to reduce sampling 
overhead. Hence , for each execution, the number of function 
pairs taken as input is  small and does not increase with increas-
ing number of executions. The thir d and the fourth inputs to 
Algorithm 2 reflects this design, where n determines the function 
pairs in FPairs .  
By adopt ing thread -local maps  and recording only n-frequent  
function pairs , the only side effect is that AtexRace  may sample 
function pairs that have been sampled in the same execution due 
to the content difference of different threads within the same 
execution. This may incur unnecessary overhead. However, it 
produces no bad result on the data race coverage as sampling the 
same functions more  than one time also increases the probability 
to detect those missed data races (see the first kind of limitations 
in Section 4.3).  
4.5 AtexRace  on Example Program  
In this section, we use the running example in Figure 2 to illus-
trate how AtexRace  sampling its  execution s in Figure 4. Initially, 
both functions f1 and f3 are sampled as the input FPairs  are emp-
ty. Such sampling continues until in each thread the recorded 
functions pairs contain ÔÉ°f1, f3ÔÉ±. Probably1, after a certain number 
of calls to both functions, AtexRace  stops continuous sampling of 
f1 and f3 because ÔÉ°f1, f3ÔÉ± is hot. Of course, in our algorithm, func-
tions in a hot pair still have chances to be sampled due to our 
burst sampling str ategy.  
Next, suppose thread t1 calls f2 for the first time while t2 is ex-
ecuting f3. Because pair ÔÉ°f2, f3ÔÉ± is cold, AtexRace  restarts to sample 
function f2. Of course, f3 is sampled as well. Similarly, AtexRace  
restarts to sample function f1 if functions f1 and f4 are executed at 
the same time. On the other hand, if it is f2 and f4 that are exe-
cuted at the same time, neither f1 nor f3 is sampled.  
Hence , for c ases (c) and (d), AtexRace  has a larger probability 
to detect the two races that are probably miss ed by LiteRace . 
However, for c ases (a) and (b), although there is no race, 
                                                                 
1 In this section, we frequently used the word "probably" because the execu-
tion of multiple threads is undetermined. E.g., we say that, if functions f1 and 
f3 are called mul tiple times (as shown in Figure 2), most of their executions 
are simultaneous . But in theory, it is possible that all executions of function 
f1 are executed before any execution of function f3. Or given two threads that 
can be executed in parallel, there are executions where they can be  sequen-
tially executed.  AtexRace  still samples the first calls to function f3 and f4. In the 
subsequent execution, after  functions f3 and f4 are called for sev-
eral times, AtexRace  stops the continuous  samp ling of the two 
functions.   
After one execution of the example program, AtexRace  rec-
ords the observed function pairs (probably the four pairs: ÔÉ°f1, 
f3ÔÉ±,ÔÉ°f1, f4ÔÉ±,ÔÉ°f2, f3ÔÉ±, and ÔÉ°f2, f4ÔÉ±). If the program is executed again, 
AtexRace  may not continuously sample  the function pairs al-
ready collected. Hence, the total overhead to detect data race can 
be reduced, not only within the same execution but also across  
different executions of the same program.   
4.6 Discussion on AtexRace  
We aim to  reduce race detection overhe ad without sacrificing 
race detection capability when there are many test  cases . 
AtexRace  does not target a single execution  as one of our innova-
tions is to record  the recently observed function pairs and skips 
their sampling in subsequent executions . Henc e, on a small 
number of execution s, it may initially incur larger overhead than 
that by FastTrack  and LiteRace  (see Figure 8 (a) and Figure 10 in 
our experiment) . AtexRace  is more suitable  for programs (e.g., 
industrial programs) that are tested against a large number of 
test cases.  Of course, as a dynamic sampling approach, it also 
reports false negatives.   
Figure 7 shows the ideal scenario of AtexRace. Initially, 
AtexRace  may incur higher overhead than LiteRace  or even 
FastTrack . However, with increasing number of executions, 
AtexRace  gradually inc urs lower overhead . 
5. EXPERIMENTS  
This section presents the evaluation on AtexRace. We compared 
it with LiteRace and FastTrack . Because FastTrack  is one of the 
fastest and most widely used tools in this category. It fully de-
tects data races and can be considered as a sampling tool with a 
rate of 100%. And LiteRace  is the state -of-the-art in -house sam-
pling to ol. Both are representative and well-known .  
5.1 Implementation and Benchmarks  
Implementation . We have implemented  AtexRace , FastTrack  
and LiteRace  on top of Pintool [11][32], a widely used binary 
instr umentation framework . Our implementation targets multi-
threaded programs with Pthread library on Linux 32 system.  
Note that, Pintool runs like a virtual machine [32] and incurs 
large overhead. A better  implementation can be done  as the orig-
inal LiteRace  implementation [34] (i.e., to integrate  sampling  
tools into the program under testing at compilation time ).   
Function  encoding . On Linux platform, Pintool modes each 
program as Image that contains Sec tions and each section con-
Cumulative number of executed test casesOverhead
FastTrack
LiteRace 
Figure 7. Ideal overhead changes with increasing executions . 
321ESEC/FSE'17, September 4 -8 2017, Paderborn, Germany  Y. Guo, Y. Cai, and Z. Yang  
 
sists of multiple Routines  (or functions) . We use a 32 -bit integer 
to encode a routine. The first 6 bits are used as the Image identi-
fier and the remaining  26 bits are used as routines identifier per 
image. This encoding allows at  most 26= 64 images and 2 6‚âà
64√ó106 routines per-image , which  is enough  in practice .   
Benchmarks . We choose  the Parsec  benchmark suite 3.1 [2] 
to evaluate  the race detectors . The suite consists of  13 bench-
marks. After eliminating the b enchmarks that are not multi-
threaded or cannot be compiled under the Pin environment , we 
obtain five benchmarks:  Blackscholes , Bodytrack , Canneal , 
Freqmine , and Streamcluster . In our experiments, we run each 
benchmark from Parsec for 100 times to collect t heir results.  
Table 1 gives the source code size ( SLOC) of the five  bench-
marks.  It can be observed that the lines of code range from 1.3K 
to 16K. To further evaluate the performance of  AtexRace , we 
select  the MySQ L database server (v6.0.4),  a widely  used real -
world program. The version we use, mysql -6.0.4-alpha, has 
1,114,980 lines of code. Among the 399 test cases that comes with 
its distribution, 223 of them can be successfully executed  in the 
Pin environment . We run all the 223 test cases in our 
experiment.  
5.2 Experimental Setup  
Our experiment s were  performed on a workstation  (ThinkPad 
W540)  with an i7 -4710MQ CPU (four cores), 16G memory, and 
250G SSD. The workstation was installe d with Ubuntu 14.04 x86 
system.  For AtexRace , we set its sampling rate and the value n 
(determining n-frequent  function pairs) to be 10/100 and 2, re-
spectively. For LiteRace , we adopt  the fixed thread -local  sampling 
configuration as defined in previous work  [34].  5.3 Result  Analysis on Parsec Benchmark Suite  
5.3.1 Overall Results . For all techniques, Table 1 gives the time of 
the executions spent by Pin and the three tools of the bench-
marks , the overhead of the race detectors compared to the time 
consumed by  Pin framework,  and the number of unique races  
(i.e., the number of variables in the source code)  detected by each 
tool.  
As expected, both LiteRace  and AtexRace  are much faster than 
FastTrack  by reducing  about 4 0% overhead based on Pin . It can 
also be observed that LiteR ace and AtexRace  incurred almost the 
same average overhead.  On race detection capability, both LiteR-
ace and AtexRace  outperform FastTrack . At first glance,  the re-
sults are surprising. However, it is known that sampling perturb s 
thread scheduling so a race detector with sampling runs different 
executions with the one without sampling, even under the same 
test case. Such phenomenon is previously observed [15]. Table 1 
shows  that LiteRace  detects  53% more unique races than 
FastTrack , all of the  additional races are from the single bench-
mark Freqmine . AtexRace  detects 12% more unique races than 
LiteRace .  
The above result s indicate  that AtexRace  detect the most 
number of races at a cost almos t the same as LiteRace . However, 
among the five benchmarks LiteRace  is better in  only two of 
them. On the other hand, the three benchmarks seem to have 
very few races (or even no races) so none of the race detectors 
can discover more races. When there are more races, the benefit 
of AtexRace  seems  obvious. Since these relatively small bench-
marks do not give a doubtless evaluation of AtexRace , we further 
evaluate our approach on a large real -world database server 
MySQL. But before we present our empirical stu dy on MySQL, Table 1. The statistics of Parsec benchmark and its overall result s. 
Benchmarks  Size ( SLOC ) Time (seconds)  Overhead  (%)  # of Unique Races  
Pin FT LR AR FT LR AR  FT LR AR 
Blackscholes  1,380  1048.53  1612.63  1652.39  1604.82  53.80%  57.59%  53.05%   0 0 0 
Bodytrack  16,479  633.396  884.281  718.466  686.812  39.61%  13.43%  8.43%  14 14 32 
Canneal  2,847  3314.22  7237.03  3640.18  4947.22  118.36%  9.84%  49.27%   2 2 2 
Freqmine  2,192  2812.04  6451.47  4398.5  3317.82  129.42%  56.42%  17.99%   160 263 280 
Streamcluster  1,795  111.626  135.767  131.325  131.915  21.63%  17.65%  18.18%      8 8 8 
     Avg.:  72.56%  30.98%  29.38%  Sum : 190  291 326 
  
 
 
Figure 8. The trend of overhead with increasing number of executions . 
0%20%40%60%80%100%120%140%
1 20 39 58 77 96FastTrack
LiteRace
AtexRace
0%20%40%60%80%100%120%140%
1 20 39 58 77 96FastTrack
LiteRace
AtexRace
0%5%10%15%20%25%30%
1 20 39 58 77 96FastTrack
LiteRace
AtexRace40%45%50%55%60%65%70%
1 20 39 58 77 96FastTrack
LiteRace
AtexRace
0%5%10%15%20%25%30%35%40%45%
1 20 39 58 77 96FastTrack
LiteRace
AtexRace
(a) Blackscholes (b) Bodytrack
(c) Canneal (d) Freqmine (e) Streamcluster
322AtexRace: Across Thread and Execution Samp ling for In -House 
Race Detection  ESEC/FSE'17, September 4 -8 2017, Paderborn, Germany  
 
 
 we use Parsec to illustrate the advantage of cross -execution 
sampling of AtexRace .  
5.3.2 Overhead  Trend Across Executions . One of key features  
of AtexRace  is its cross -execution  sampling, which may result in  
lower overhead with increasing num ber of executions. In Figure 
8, we show how the overhead changes with increasing number 
of executions for three techniques on the five benchmarks. In 
each subfigure, the x -axis shows the number of executions; and 
the y-axis shows the overhead incurred by three techniques on 
each execution. The cumulative overhead on the i-th execution is 
calculated by the following formula :  
ùëÇùë£ùëíùëü‚Ñéùëíùëéùëë(ùëñ)=‚àë(ùëáùë°ùëúùëúùëô(ùëñ))‚àí‚àë(ùëáùëùùëüùëúùëî(ùëñ))
‚àë(ùëáùëùùëüùëúùëî(ùëñ))√ó100%       (Eq. 1)  
where ùëáùë°ùëúùëúùëô(ùëñ) represents the execution time under a tool on the 
i-th execution, and ùëáùëùùëüùëúùëî(ùëñ) represents the native program ex e-
cution time  under Pin .  
From Figure 8, we see that, overall, FastTrack  and LiteRace  
incur  almost the same overhead across executions (i.e., nearly a 
horizontal line). Except the first benchmark , LiteRace 's overhead 
is much lower than that by FastTrack , which is expected due to 
the sampling of LiteRace . However, on Blackscholes , LiteRace  
incurs larger overhead than that by FastTrack . We have per-
formed several additional experiment s and confirmed the  results .  
For AtexRace , overall,  its overhead decreases  with i ncreasing 
number of executions, although the trend is less obvious in 
Steamcluster . This is consistent with our theoretical analysis in 
Section 4.6.  It can also be observed that , with increasing n umber 
of executions, AtexRace 's performance becomes the best  on three 
benchmarks (i.e., the subfigure (a), (b), and (d)). However, the 
overhead reduction reaches a plateau after a certain number of 
executions . This is not surprising because according to Se ction 
5.3.3 the number of recorded function pairs barely increases .  
5.3.3 Number of Function Pairs . As a Parsec benchmark is re-
peatedly executed under the same input , there is no obvious 
increase in the number of function pairs with more executions. 
After  100 executions, the number of  functions pairs  of the five 
benchmarks  are 9, 409, 46, 250, and 23. If we store 2 -frequent  
pairs only, the number of function pairs after 100 executions are 
8, 227, 32,  232 and 16 .  
5.4  Result Analysis on MySQL  
MySQL has one mil lion lines of code . We run it against 223 test 
cases in the default order of the test script " mysql -test-run".  
5.4.1 Number  of Detected Races . Figure 9(a) gives  the number 
of unique races that are detected by FastTrack , LiteRace  and AtexRace  after 223 executions of MySQL . Not surprisingly, com-
pared with  LiteRace , AtexRace  detect s 23% more unique races. 
What we have not expected is that AtexRace  detects even 15% 
more unique races than FastTrack .  Of course, as explained in 
Section 5.3.1, this is possible because sampling perturbs thread 
scheduling. However, we would like to have a  clearer picture of 
race detecting capability. Thus,  we collect data on all the races, 
not just unique races, that are detected by the three tools. Alt-
hough in theory unique races are more interesting, in practice 
the number of total races is helpful to de bugging because they 
can illustrate different scenarios how a race occurs. Detecting the 
same traces multiple times is also a good indicator of a race de-
tector‚Äôs capability.   
The results of total races are illustrated in Figure 9(b). The 
number of total races is significantly more than the number of 
unique races. It can be observe d that FastTrack  detects  the most 
races , but AtexRace  is a very close second . LiteRace , on the other 
hand, detects  significantly fewer number of races than the other 
two.  
5.4.2 Overhead. Figure 10 depicts  how the overhead ( y-axis) 
change s across 223 execution s (x-axis). Unlike benchmarks from 
Parsec where all repeated executions are conducted again st the 
same test cases, each of the  223 MySQL executions is conducted 
against a different test case . Therefore, on MySQL, FastTrack  (as 
well and LiteRace  and AtexRace ) may incur different overhead on 
different executions. The formula to calculate the cumul ative 
overhead of the first i executions is the same as that on Parsec 
(i.e., Eq. 1).  
The results shown in  Figure 10 are as expected, where 
FastTrack  incurs  the largest overhead  over native execution on 
Pin and LiteRace  incurs  the smallest . Although AtexRace ‚Äôs over-
head is larger than LiteRace ‚Äôs, the gap is  gradually shrinking . At 
the end of all 223 executions, AtexRace  incurs  almost the same 
overhead as that by LiteRace . Given more test cases, AtexRace  
may have a c hance to incur less overhead than LiteRace .  
Considering both Figure 9 and Figure 10, our experiments 
confirm that AtexRace  achieves a sweet spot between LiteRace  
and FastT rack, by detecting almost the same number of races as 
FastTrack  at a cost almost the same as LiteRace .  
5.4.3 Number of Function Pairs . AtexRace  does not record all 
observed function pairs but only keeps recently observed ones to 
avoid potentially unlimite d increase on the number of function 
pairs. Figure 11 shows a comparison on the cumulative number 
of function pairs (y-axis) with the increasing number of execu-
tions (up to 223). The two lines represent the data by recording 
all observed ones (" All Pairs ") and recording recently observed      
(a) # unique races.  (b) # dynamic races    
Figure 9. The number of races detected by three . 
1212
11361399
1000105011001150120012501300135014001450
FastTrack LiteRace AtexRace
0200040006000800010000120001400016000
121 41 61 81101 121 141 161 181 201 221FastTrack
LiteRace
AtexRace 
Figure 10. The cumulative overhead of three techniques 
with increasing executions . 
0%10%20%30%40%50%60%70%
1 21 41 61 81 101 121 141 161 181 201 221FastTrack
LiteRace
AtexRace
323ESEC/FSE'17, September 4 -8 2017, Paderborn, Germany  Y. Guo, Y. Cai, and Z. Yang  
 
ones (" 2-frequent Pairs "), respectively .  
We observe that, with increasing number of executions, the 
number of all function pairs also increases . After 223 executions, 
the number of  observed function pairs is nearly 70,000. If we 
keep all these function pairs , a large overhead on querying is 
inevitable, which may eventually offset the benefit of sampling. 
This is the reason we only rely  on the recently observed function 
pairs. From Figure 11, we see that this strategy is effective, as 
over all the 223 executions, the numbers of the 2-frequent  func-
tion pairs are almost always below 10,000 ( with  only six excep-
tions). And on 198 out of 223 (~89%)  executions, there are less 
than 5 ,000 function pairs. On average, there are 3,320 function 
pairs on each execution. Our experiments are all performed with 
2-frequent  function pairs and the data confirm its effectiveness.  
6. RELATED WORK  
Data races [10][16] are extremely difficult to be found and re-
produced.  Both  static techniques [25] [37][42][51] and dynamic 
techniques [16][41][44][47][54] aim to detect data races . Static 
ones  [51][42] can analyse  the source code of a whole program ; 
however, due to lack of runtime information, static approaches 
can easily report many false positives . Dynamic ones  analyse 
concrete executions to detect data races according to some rules  
(e.g., the lockset  discipline  [44][46][56] and the happens -before 
relation [6][16][41][43][50][52]). Although dynamic techniques 
are relatively precise , they incur  heavy overhead.  
We have heavily discussed sampling appro aches on data race 
detection . CRSampler  [12] also targets on sampling but its main 
purpose is at user site. It is based on hardware breakpoints and 
clock races to detect data races . DataCollider  [14] purely relies on 
hardware breakpoints to detect those occurred  data race by sus-
pending threads. AtexRace  aims at in-house sampling .  
To explore all possible execution s is one direction to find 
concurrency bugs  (e.g., M odel checking [53][35]). However, it is 
usually impossible to explore all the interleaving  although they 
may achieve certain coverage [28]. Practically, enumerating  each 
schedule is not practical for large-scale real-world programs , 
even with reduction techniques [18].  
Therefore, to explore a small portion of interleaving space 
that are error prone is also one direction.  Chess [35] sets a heu-
ristic  bound on the number of pre-emption s to explore the 
schedules. Also,  although systematic approaches avoid executing  
previously explored schedules, they usually incur large over-
head s and fail to scale up to handle long running programs. For 
example, Maple  [55] is a coverage -driven [8][19] tool to mine 
thread interleaving so as to expose unknown concurrency bugs.  
PCT  [9][36] randomly schedules a program to expose concur-rency bugs, which also requires large number of executions. 
However, it is diffi cult to apply these techniques to  large -scale 
program s such as MySQL .  
Other works aim to firstly predict  a set of potential data races 
and then to verify them. RVPredict  [22] achieve s a strictly higher 
coverage than HBR based  detectors . It firstly predicts a set of 
potential races and then relies on  a number of production execu-
tions to check against each predicted race. Racageddon  [15] aims 
to solve races that could be predicted in one execution but re-
quire different inputs. It still needs a larger number of execution s 
to check against each predicted race [39][45]. Both RVPredict  
and Racageddon  have to solve scheduling constraints for each 
predicted race, which may fail.  RaceMob  [26] statically detects  
data race warnings and distribut es them to a large number of 
users to validate real races. In such a  run, the schedules are guid-
ed by the set of data race warnings to trigger real data races. This 
kind of approach is able to confirm real ra ces but cannot elimi-
nate false positives.  Besides, it may miss real races if such races 
are not predicted in the (static) prediction phase.   
DrFinder  [10] tries to predict  the happens -before relation to 
further expose races hid den by the happens -before relation. It 
dynamically predicts and tries to reverse happens -before rela-
tions from  observed executions. However, it s active scheduling is 
also heavy (e.g., about 400% [10] for Java programs ).  
CCI [24] proposes cross -thread sampling strategies to find 
causes of concurrency bugs  based on ran domized sampling . 
Unlike race sampling techniques (e.g., CRSampler , DataCollider , 
Pacer , and LiteRace ), CCI focuses on failure diagnosi s. However, 
CCI may cause heavy over head  (e.g., up to 900% [24]) although 
it targets on lightweight sampling . Carisma  [58] improves Pacer  
by further sampling memory locations allocated at the same 
program locati on for Java. Valor  [4] infers data races by 
detecting region confilt, which has good performance compared 
with  FastTrack .  
Bedides multithreaded programs, data race may also exist in 
other kinds of pgorams, such ev en-driven programs such as 
android application s [33][21][20], concurrent librar y invocations  
[13], and modified program codes  [57]. AtexRace  could also be 
adapted to detect these races. We leave it as future work.  
7. CONCLUSION  
We have proposed a new cross -thread and cross -execution sam-
pling approach to achieve both high race detection rate and high 
efficiency. By ad opting several novel designs, our prototype 
AtexRace  shows its potential to replace FastTrack and LiteRace . 
This is confirmed by  the experiment s with benchmarks obtained 
from both Parsec benchmark suite and a real -world large -scale 
MySQL database.  
ACKNOWL EDGEMENT  
We thank anonymous reviewers for their invaluable comments  
and suggestions on improving this work.  This work is supported 
in part by National Natural Science Foundation of  China ( NSFC) 
(61502465 , 6147 2318 and 61632015 ), National 973 program of 
China (2014CB340702) , the Youth Innovation Promotion Associa-
tion of the Chinese Academy of Science s (YICAS) ( 2017151 ), and 
the National Science Foundation  (NSF) (DGE -1522883).   
Figure 11. The cumulative number of function pairs . 
010000200003000040000500006000070000
1
11
21
31
41
51
61
71
81
91
101
111
121
131
141
151
161
171
181
191
201
211
221All Pairs 2-frequent Pairs
324AtexRace: Across Thread and Execution Samp ling for In -House 
Race Detection  ESEC/FSE'17, September 4 -8 2017, Paderborn, Germany  
 
 
 REFERENCE  
[1] B. Alpern, C.R. Attanasio, A. Cocchi, D. Lieber, S. Smith, T. Ngo, J.J. B arton, 
S.F. Hummel, J.C. Sheperd, and M. Mergen. Implementing jalape√±o in Java. In 
Proc.  OOPSLA , 314 ‚Äì324, 1999.  
[2] C. Bienia. Ph.D. Thesis: Benchmarking modern multiprocessors. Princeton 
University, January 2011.   
[3] S. Biswas, M. Cao, M. Zhang, M.D. Bond, and B .P. Wook. Lightweight data 
race detection for production runs. In Proc. CC , 11 ‚Äì 21, 2017.  
[4] S. Biswas, M. Zhang, M. D. Bond, and B. Lucia. Valor: Efficient, Software -
Only Region Conflict Exceptions. In Proc. OOPSLA , 241 ‚Äì259, 2015.  
[5] S.M. Blackburn, R. Garne r, C. Hoffmann, A.M. Khang, K.S. McKinley, R. 
Bentzur, A. Diwan, D. Feinberg, D. Frampton, S .Z. Guyer, M. Hirzel, A. Hosk-
ing, M. Jump, H. Lee, J. Eliot B. Moss, A. Phansalkar, D. Stefanoviƒá, T. 
VanDrunen, D. von Dincklage, and B. Wiedermann. The Dacapo benchmarks: 
Java benchmarking development and analysis. In Proc. OOPSLA , 169 ‚Äì190, 
2006.  
[6] E. Bodd en and K. Havelund. Racer: effective race detection using AspectJ. In 
Proc. ISSTA , 155 ‚Äì166, 2008.  
[7] M.D. Bond, K. E. Coons and K. S. Mckinley. PACER: Proportional detection of 
data races. In Proc. PLDI , 255 ‚Äì268, 2010.  
[8] A. Bron, E. Farchi, Y. Magid, Y. Nir, an d S. Ur. Applications of synchroniza-
tion coverage. In Proc. PPoPP , 206 ‚Äì212, 2005.  
[9] S. Burckhardt, P. Kothari, M. Musuvathi, and S. Nagarakatte. A randomized 
scheduler with probabilistic guarantees of finding bugs. In Proc. ASPLOS , 
167‚Äì178, 2010.  
[10] Y. Cai and L. Cao. Effective and precise dynamic detection of hidden races for 
Java programs. In Proc. ESEC/FSE , 450 ‚Äì461, 2015.  
[11] Y. Cai and W.K. Chan. LOFT: Redundant synchronization event removal for 
data race Detection. in Proc. ISSRE , 160 ‚Äì169, 2011.  
[12] Y. Cai, J. Zhan g, L. Cao, and J. Liu. A deployable sampling strategy for data 
race detection. In Proc. FSE, 810 ‚Äì821, 2016.  
[13] D. Dimitro, V. Raychev, M. Vechev, and E. Koskinen. Commutativity race 
detection. In Proc. PLDI, 305 ‚Äì315, 2014 .  
[14] J. Erickson, M. Musuvathi, S. Burc khardt and K. Olynyk. Effective data -race 
detection for the kernel. In Proc. OSDI , 1‚Äì6, 2010.  
[15] M. Eslamimehr and J. Palsberg. Race directed scheduling of concurrent pro-
grams. In Proc. PPoPP , 301 ‚Äì314, 2014.  
[16] C. Flanagan and S. N. Freund. FastTrack: efficient and precise dynamic race 
detection. In Proc. PLDI , 121 ‚Äì133, 2009.  
[17] C. Flanagan and S. N. Freund. The RoadRunner dynamic analysis framework 
for concurrent programs. In Proc. PASTE , 1‚Äì8, 2010.  
[18] C. Flanagan and P. Godefroid. Dynamic partial -order reduction for model 
checking software. In Proc. POPL , 110 ‚Äì121, 2005.  
[19] S. Hong, J. Ahn, S. Park, M. Kim, and M.J. Harrold. Testing concurrent pro-
grams to achieve high synchronization coverage. In Proc. ISSTA , 210 ‚Äì220, 
2012.  
[20] S. Hong, Y. Park, and M. Kim. Detecting concurre ncy errors in client -side 
java script web applications. In Proc. ICST , 61‚Äì70, 2014.  
[21] C. Hsiao, Y. Yu, S. Narayanasamy, Z. Kong, C.L. Pereira, G.A. Pokam, P.M. 
Chen, and J. Flinn. Race detection for event -driven mobile applications. In 
Proc. PLDI, 326 ‚Äì336, 2 014. 
[22] J. Huang, P.O. Meredith, and G. Rosu. Maximal sound predictive race detec-
tion with control flow abstraction. In Proc. PLDI , 337 ‚Äì348, 2014.  
[23] J. Jackson. Nasdaq's Facebook glitch came from 'race conditions', May 21 
2012. http://www.computerworld.com/art icle/2504676/financial -it/nasdaq -s-
facebook -glitch -came -from --race-conditions -.html, last visited on March 
2016.  
[24] G. Jin, A. Thakur, B. Liblit and S. Lu. Instrumentation and sampling strategies 
for cooperative concurrency bug isolation. In Proc. OOPSLA , 241 ‚Äì225, 2010.  
[25] V. Kahlon, Y. Yang, S. Sankaranarayanan, and A. Gupta. Fast and accurate 
static data -race detection for concurrent programs. In Proc.  CAV , 226 ‚Äì239, 
2007.  
[26] B. Kasikci, C. Zamfir, and G. Candea. RaceMob: Crowdsourced data race 
detection. In Proc. SOSP , 406 ‚Äì422, 2013.  
[27] L. Lamport. Time, clocks, and the ordering of events. Communications of the 
ACM 21(7):558 ‚Äì565, 1978.  
[28] Z. Letko, T. Vojnar, and B. KÀárena. Coverage metrics for saturation -based and 
search -based testing of concurrent software. In Proc. RV , 177 ‚Äì192, 2011.  [29] N.G. Leveson and C. S. Turner. An investigation of the Therac -25 accidents. 
Computer , 26(7), 18 ‚Äì41, 1993.  
[30] S. Lu, S. Park, E. Seo, and Y.Y. Zhou, Learning from mistakes: A comprehen-
sive study on real world concurrency bug characteristics. In Proc. ASPLOS , 
329‚Äì339, 2008.  
[31] B. Lucia and L. Ceze. Cooperative empirical failure avoidance for multi-
threaded programs. I n Proc. ASPLOS , 39‚Äì50. 2013.  
[32] C.-K. Luk, R. Cohn, R. Muth, H. Patil, A. Klauser, G. Lowney, S. Wallace, V. J. 
Reddi, and K. Hazelwood. Pin: Building Customized Program Analysis Tools 
with Dynamic Instrumentation. In Proc. PLDI , 191 ‚Äì200, 2005.  
[33] P. Maiya, a. K anade, and R. Majumdar. Race detection for Android applica-
tions. In Proc. PLDI, 316 ‚Äì325, 2014.  
[34] D. Marino, M. Musuvathi, and S. Narayanasamy. LiteRace: effective sampling 
for lightweight data -race detection. In Proc. PLDI , 134 ‚Äì143, 2009.  
[35] M. Musuvathi, S. Qa deer, T. Ball, G. Basler, P. A. Nainar, and I. Neamtiu. 
Finding and reproducing heisenbugs in concurrent programs. In Proc. OSDI , 
267‚Äì280 2008.  
[36] S. Nagarakatte, S. Burckhardt, M. M.K. Martin, and M. Musuvathi. Multicore 
acceleration of priority -based schedu lers for concurrency bug detection. In 
Proc. PLDI , 2012, 543 ‚Äì554, 2012.  
[37] M. Naik, A. Aiken, and J. Whaley. Effective static race detection for Java. In 
Proc. PLDI , 308 ‚Äì319, 2006.  
[38] S. Narayanasamy, Z. Wang, J. Tigani, A. Edwards, and B. Calder. Automati-
cally classifying benign and harmful data races using replay analysis. In Proc. 
PLDI , 22‚Äì31, 2007.  
[39] C.S. Park, K. Sen, P. Hargrove, and C. Iancu. Efficient data race detection for 
distributed memory parallel programs. In Proc. SC, 2011.  
[40] K. Poulsen. Software bug c ontributed to blackout. 
http://www.securityfocus.com/news/8016, Feb. 2004.  
[41] E. Pozniansky and A. Schuster. Efficient on -the-fly data race detection in 
multithreaded C++ programs. In Proc. PPoPP , 179 ‚Äì190, 2003.  
[42] P. Pratikakis, J.S. Foster, and M. Hicks. LOCKS MITH: context -sensitive corre-
lation analysis for race detection. In Proc. PLDI , 320 ‚Äì331, 2006.   
[43] A.K. Rajagopalan and J. Huang. RDIT: race detection from incomplete traces. 
In Proc. ESEC /FSE, 914 - 917, 2015.  
[44] S. Savage, M. Burrows, G. Nelson, P. Sobalvarro and T. Anderson. Eraser: a 
dynamic data race detector for multithreaded programs. ACM TOCS , 15(4), 
391‚Äì411, 1997.  
[45] K. Sen. Race Directed Random Testing of Concurrent Programs. In Proc. PLDI , 
11‚Äì21, 2008.  
[46] K. Serebryany and T. Iskhodzhanov. ThreadSanitizer: d ata race detection in 
practice. In Proc. WBIA , 62‚Äì71, 2009.   
[47] Y. Smaragdakis, J. Evans, C. Sadowski, J. Yi, and C. Flanagan. Sound predic-
tive race detection in polynomial time. In Proc. POPL , 387 ‚Äì400, 2012.  
[48] F. Sorrentino, A. Farzan, and P. Madhusudan. PENE LOPE: weaving threads to 
expose atomicity violations. In Proc. FSE, 37‚Äì46, 2010.  
[49] Microsoft. Thread execution blocks. 
http://msdn.microsoft.com/enus/library/ms686708.aspx  
[50] K. Vineet and C. Wang. Universal causality graphs: a precise happens -before 
model for detecting bugs in concurrent programs. In Proc. CAV , 434 ‚Äì449, 
2010.  
[51] J.W. Voung, R. Jhala, and S. Lerner. RELAY: static race detection on millions 
of lines of code. In Proc. FSE, 205 ‚Äì214, 2007.  
[52] C. Wang, K. Hoang. Precisely Deciding Control State Reachabilit y in Concur-
rent Traces with Limited Observability. In Proc. VMCAI , 376‚Äì394, 2014.  
[53] C. Wang, M. Said, and A. Gupta. Coverage guided systematic concurrency 
testing. In Proc.  ICSE, 221 ‚Äì230, 2011.  
[54] X.W. Xie and J.L. Xue. Acculock: Accurate and Efficient detectio n of data 
races. In Proc. CGO , 201 ‚Äì212, 2011.  
[55] J. Yu, S. Narayanasamy, C. Pereira, and G. Pokam. Maple: a coverage -driven 
testing tool for multithreaded programs. In Proc. OOPSLA , 485 ‚Äì502, 2012.  
[56] Y. Yu, T. Rodeheffer, and W. Chen. RaceTrack: efficient detect ion of data race 
conditions via adaptive tracking. In Proc. SOSP , 221 ‚Äì234, 2005.  
[57] T. Yu, W. Srisa -an, and G. Rothermel. SimRT: An automated framework to 
support regression testing for data races. In Proc. ICSE, 48 ‚Äì59, 2014.  
[58] K. Zhai, B.N. Xu, W.K. Chan, and T.H. Tse. CARISMA: a context -sensitive 
approach to race -condition sample -instance selection for multithreaded ap-
plications. In Proc. ISSTA , 221 ‚Äì231, 2012.  
[59] W. Zhang, M. d. Kruijf, A. Li, S. Lu and K. Sankaralingam. ConAir: feather-
weight concurrency bug reco very via single -threaded idempotent execution. 
In Proc. ASPLOS , 113 ‚Äì126. 2013.  
 
 
 
325