Detecting Information Flow by Mutating Input Data
Bj¨orn Mathis∗, Vitalii Avdiienko∗, Ezekiel O. Soremekun∗, Marcel B ¨ohme†, and Andreas Zeller∗
∗Saarland University, Saarbr ¨ucken, Germany
{mathis| avdiienko| soremekun| zeller}@st.cs.uni-saarland.de
†National University of Singapore, Singapore
marcel.boehme@nus.edu.sg
Abstract —Analyzing information ﬂow is central in assessing
the security of applications. However, static and dynamic analyses
of information ﬂow are easily challenged by non-available orobscure code. We present a lightweight mutation-based analysis
that systematically mutates dynamic values returned by sensitive
sources to assess whether the mutation changes the values passed
to sensitive sinks. If so, we found a ﬂow between source and sink.In contrast to existing techniques, mutation-based ﬂow analysis
does not attempt to identify the speciﬁc path of the ﬂow and is
thus resilient to obfuscation.
In its evaluation, our M
UTA FLOW prototype for Android pro-
grams showed that mutation-based ﬂow analysis is a lightweightyet effective complement to existing tools. Compared to the
popular F
LOW DROID static analysis tool, M UTA FLOW requires
less than 10% of source code lines but has similar accuracy;
on 20 tested real-world apps, it is able to detect 75 ﬂows that
FLOW DROID misses.
I. I NTRODUCTION
When assessing the security of applications, information
ﬂows play an essential role: Which information sources does
the application access, and to which sinks does it send
these to? Consequently, static analysis tools that detect such
information ﬂows see a substantial interest both in practiceas in research; for the Android operating system, tools like
F
LOW DROID [1] or I CCTA [2] represent the state of the art.
Static ﬂow detection tools are effective but they suffer from
the principal limitations of static analysis, notably that allcode
must be available for analysis. This problem is illustrated inthe example app in Figure 1. The application ﬁrst accessessensitive information (A), namely the user’s phone numberviagetLine1Number(). This information is then sent via
SMS to some third party (B). The ﬂow between A and B can
be easily detected by static analysis, and is properly reported
by F
LOW DROID and I CCTA.
However, obfuscated ﬂows cannot be detected so easily. The
example app contains a native method, a piece of code that
runs directly on the processor and whose source is written inC or C++—in contrast to the Dalvik byte code derived from
the Java source. The devId() method (D) simply takes a
string and returns it. After the sensitive id passes through
devId(), xis set and passes into (C); however, F
LOW DROID
and I CCTA will miss the ﬂow from A to C because it passes
through native code, which these tools cannot analyze.
In principle, one could extend static analysis to also consider
machine code; and a simple identity function like devId()
would be easy to recognize. At the machine instruction level,public class HelloJni extends AppCompatActivity {
@Override
protected void onCreate(Bundle savedInstanceState) {
super.onCreate(savedInstanceState);setContentView(R.layout.activity_hello_jni);
TextView tv =
(TextView)findViewById(R.id.hello_textview);
SmsManager sms = SmsManager.getDefault();
String id = mgr.getLine1Number(); // Asms.sendTextMsg("0153", id); // B// ...String x = devID(id);tv.setText(x);// ...
sms.sendTextMsg("0153", x); // C
}
public native String devID(String id); // Dstatic {
System.loadLibrary("hello-jni");
}
}
Fig. 1. The HelloJNI Android app uses the Java Native Interface
(JNI) to obfuscate a ﬂow. The ﬂow from getLine1Number() (A) to
sms.sendTextMsg() (B) can be detected statically as well as dynamically.
However, the ﬂow from A to C can only be found by M UTA FLOW , because
idﬂows through the native method devID() (D).
though, it is even easier to obfuscate ﬂows, since there is virtu-
ally no limit to what the function can do; and any prediction isultimately thwarted by the halting problem. A static analysercan then either be optimistic, and assume nothing bad will
happen (as with runtime functions), or be pessimistic, and
assume anything may happen. Neither resolution is completelysatisfactory.
In contrast to static analysis, dynamic analysis allows to an-
alyze concrete executions rather than abstract code. Dynamic
tainting [3], for instance, tracks data throughout the execution,
and would just as well detect a ﬂow from A to B. Finding theﬂow from A to C via D, though, would require to track datathrough the hardware or a hardware interpreter, which is nosimple feat. Developers wishing to conceal what devID() is
doing can also resort to implicit information ﬂow [4] turning
data ﬂow dependencies into control ﬂow dependencies, andagain requiring static analysis to identify the alternative controlﬂows. Since existing dynamic and static tools need to analyzethe concrete path along which the information travels, theycannot detect deliberately hidden ﬂows, such as from A to C.
In this paper, we investigate a lightweight mutation-based
alternative to detect information ﬂows. Rather than staticallyanalyzing application code or dynamically tracking data ﬂow,
we use an experimental approach: We systematically mutate
the information sources of a program to assess whether
978-1-5386-2684-9/17/$31.00 c/circlecopyrt2017 IEEEASE 2017, Urbana-Champaign, IL, USA
T echnical Research263
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:53:15 UTC from IEEE Xplore.  Restrictions apply. the mutation impacts its information sinks. Speciﬁcally, our
MUTA FLOW prototype
1) takes an Android app as well as a set of test cases (given
or generated)
2) instruments sensitive data sources and sinks in the app
tomutate input values at sources and track output values
at sinks
3) executes tests on unmutated and mutated app versions
4) records the values passed to sensitive sinks, reporting a
ﬂow if the value changes due to a mutated sink value.
In our example (Figure 1), M UTA FLOW runs the app twice,
the ﬁrst time unmodiﬁed, and the second time injecting a
different input value for getLine1Number() (A). It then
detects that this change causes a change in the calls to
sms.sendTextMsg() (B) as well as sms.sendText-
Msg() (C). The previous problem that id ﬂows through a
native method (D) has no consequences for M UTA FLOW ;i t
only cares about how changes in sources affect sinks, without
having to track the path. All changes made by M UTA FLOW
simulate changes in the external input data; the actual program
functionality is never altered.
The advantage of M UTA FLOW over static analysis ap-
proaches is that it hardly overapproximates: It is sound in
the sense that if it detects a ﬂow, this ﬂow is most likely
real. However, as any dynamic approach, it is also incomplete,
as there may always be executions which exercise ﬂows notpreviously detected. Hence, M
UTA FLOW could be seen as a
complement to static analysis approaches, focusing on problem
areas such as non-analyzable code. However, mutation-based
ﬂow analysis can also be seen as an alternative analysis,
should static analysis not be available or possible. This is
because as we show in this paper the accuracy of MUTA FLOW
in detecting ﬂows is similar , if not superior , to static analysistools such as F
LOW DROID orICCTA.This poses mutation-
based ﬂow analysis as a new and promising alternative in our
portfolio of program analysis techniques.
In summary, this paper makes the following contributions:
1) We introduce mutation-based ﬂow analysis, a lightweight
program analysis technique to detect information ﬂows.
2) We introduce M UTA FLOW as a prototype implementation
for analyzing Android apps (Section II). M UTA FLOW is
less than 10% of the size of F LOW DROID .
3) In its evaluation against F LOW DROID (Section III), we
ﬁnd that M UTA FLOW shows comparable performance
as F LOW DROID in terms of precision and recall, and
is able to detect several ﬂows that F LOW DROID misses
(Section IV).
After discussing related work (Section V), we close with
conclusion and consequences (Section VI). To facilitate repli-cation, M
UTA FLOW and all data from the experiments are
available as open source.
II. A PPROACH AND IMPLEMENTA TION
Mutation-based ﬂow analysis attempts to detect information
ﬂow between a source aand a sink b. Both fundamentals aswell as implementation are illustrated using the example inFigure 1.
A. Prerequisites
We start with a program pand an execution e; the execution
can either be given (e.g., from a given test case) or generated
(e.g., from a test generator).
M
UTA FLOW starts with an application package (APK) that
contains the app binary as well as all resources to execute it. Astests, M
UTA FLOW can use supplied tests as well as leverage
the Monkey testing tool [5] to generate executions. A largernumber of executions with high coverage of functionality
increases the chances of detecting ﬂows.
In our example (Figure 1), the method onCreate() is invoked
automatically as the app starts—which is actually a plausibleattack vector for a malware, in order to collect as much
information from as many users as possible.
B. Logging
Given a source aand a sink b, within the execution e,w e
log the concrete values of aandb, denoted as a
0andb0.
MUTA FLOW instruments the APK as follows. The instru-
menter gets the APK the user wants to analyze and converts it
from the compiled code to Jimple code with Soot [6]. Jimplecode is a meta-representation of Java code and is used by Soot,a framework with which one can also iterate over the code andinject method calls. In a second step a log-caller and mutation
class ﬁle we created is compiled with Soot to Jimple code and
injected into the decompiled APK. Now we can iterate over
the source code line by line and inject methods from this classto write values to the log or mutate source values.
The Soot framework converts the APK into classes, the
containing methods and for each method a chain of statements.Now we can iterate over those chains of statements and injectmethod calls for logging and mutating at each source and sink.These sources and sinks were originally deﬁned by SuSi [7], atool that detects lines of code where private information ﬂowsin or out of the application. We use the pre-computed lists ofsources and sinks from SuSi
1.
In our example (Figure 1), the method TelephonyMana-
ger.getLine1Number() is listed by SuSi as a sensitive
source; hence, M UTA FLOW can inject the code
Log.write_to_log("Telephony.getLine1Number()");
Log.write_to_log(id);
right after the assignment to id (A). Likewise, M UTA FLOW
can identify the existing logging as sensitive information sinks,
and insert the code
Log.write_to_log("sms.sendTextMsg()");
Log.write_to_log(id);
1MUTA FLOW currently handles only Java primitive types and String, thus
some sources and sinks in the SuSi set originally used by F LOW DROID are
not considered by M UTA FLOW . Speciﬁcally, we excluded 73 sources and 63
sinks from the SuSi set which do not return basic types, or are not privacy-invasive, i.e. these sources do not read private information or these sinks
cannot be used by malware to send private information from the device. We
also added 3 sources that we deem privacy-invasive.
264
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:53:15 UTC from IEEE Xplore.  Restrictions apply. public class HelloJni extends AppCompatActivity {
@Override
protected void onCreate(Bundle savedInstanceState) {
super.onCreate(savedInstanceState);
setContentView(R.layout.activity_hello_jni);
TextView tv =
(TextView)findViewById(R.id.hello_textview);
String id = mgr.getLine1Number(); // Aid = Mutator.mutate_string(id);Log.write_to_log("Telephony.getLine1Number()");Log.write_to_log(id);// ...Log.write_to_log("sms.sendTextMsg()");
Log.write_to_log(id);
sms.sendTextMsg("0153", id); // B// ...String x = devId(id);tv.setText(x);// ...
Log.write_to_log("sms.sendTextMsg()");
Log.write_to_log(x);sms.sendTextMsg("0153", x); // C
}
public native String devID(String id); // Dstatic {
System.loadLibrary("hello-jni");
}
}
Fig. 2. The HelloJNI program in Figure 1, instrumented by M UTA FLOW .
and
Log.write_to_log("sms.sendTextMsg()");
Log.write_to_log(x);
before locations B and C, respectively.
C. Mutation
We now generate alternative executions by mutating the
source value. This is done by interposing mutation code into
the assignment of atoa0such that a0is changed to a/prime
0.
MUTA FLOW uses instrumentation not only for logging
values, but also for mutating source values. To this end, it
injects mutation code after each information source, which
mutates the external input value returned. In most SuSi meth-ods
2, sensitive information is either passed as a string or a
number. Hence, M UTA FLOW provides a Mutator class that
allows to mutate all Java base types including strings (mu-tate_strings()) and numbers (e.g. mutate_int()).
The string mutator replaces the middle character in the string
by another one; the number mutator replaces the numeric valuewith a random one. However, M
UTA FLOW does not prevent
violating input pre-conditions. For instance, our Mutator
class may produce a random source value that violates an
input-validation condition, such violation could lead to falsepositives or reveal exceptional ﬂows to error handling methods.
In our example (Figure 1), M
UTA FLOW would inject the
mutation code
id = Mutator.mutate_string(id);
after A and before the (also) injected logging. The fullyinstrumented program is shown in Figure 2.
2SuSi methods are privacy-relevant API calls found by SuSi.D. Detecting Flows
During the subsequent logging of sinks, we check each sink b
for whether its value has changed from b0tob/prime
0. If so, we have
shown that there is a ﬂow from atob.
MUTA FLOW creates multiple versions of the APK, one p
with mutation disabled (providing reference values for b0),
and, for each sensitive source a, one pawith mutation enabled
for this source (providing potential values b/prime0). It then runs the
mutated variants paand checks for differences between the
b0reference values and the b/prime
0values found in the mutated
apps p/prime. If a value b/prime
0for some padiffers from the reference
value b0, then the mutation in ahas caused a value change in
b; in other words, there was information ﬂow from atob.
MUTA FLOW then reports this ﬂow, including the concrete
values (witnesses) a0,a/prime0,b0andb/prime0.
E. Soundness
In the absence of non-determinism, mutation-based ﬂow
analysis is conceptually sound : It shows that a change in a
can cause a change in b,a sa precedes band changing aalso
changes b—a counterfactual causality [8] that also proves the
existence of information ﬂow from atob. This is in contrast
to static analysis or dynamic tainting, where most relation-
ships are possible ﬂows rather than causal relationships: In
b = zero() *a, where zero() always returns 0, both
techniques would detect a possible ﬂow, whereas our approach
would fail to ﬁnd a ﬂow that changes a, a true negative.
Note that in our setting, causality (and thus soundness)
requires perfect reproducibility: Only if we can ensure that
no other input value has changed can we be sure that itwas athat caused the change to b. In practice, such perfect
reproducibility is hard to achieve due to non-determinism
in executions (timing, thread schedules, load, randomness).
M
UTA FLOW reduces non-determinism by two means. First,
MUTA FLOW runs the original app pat least twice. If the values
b0in the sink bvary across executions, then bis excluded.
Second, if M UTA FLOW runs an app variant pa, the source a
is not triggered, but we still observe a difference between b/prime
0
andb0forb, then bis excluded.
F . Completeness
Mutation-based ﬂow analysis is incomplete in the sense that
if it fails to detect a ﬂow, this does not mean the absence of
ﬂows. In the code
if (hash(input) == 0xdeadbeef)
output = sensitiveData();
for instance, generating an input that fulﬁlls the condi-
tion is computationally hard; hence, it is unlikely that M U-
TAFLOW will ever report the ﬂow from sensitiveData()
tooutput.
An analysis that is both sound and complete for real-world
apps, reporting all possible ﬂows without false positives, isprevented by the halting problem. In the above example, astatic or symbolic analysis can also not know whether thecondition can be fulﬁlled, hence possibly issuing a false alarm.
265
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:53:15 UTC from IEEE Xplore.  Restrictions apply. The halting problem as well as general issues of scale are the
reasons why static analysis tools like F LOW DROID or I CCTA
cannot claim completeness either.
In practice, ways to address the limitations of incomplete-
ness include:
Have a suite of test cases. In our experiments on real-world
apps (Section III-C), we had a student assistant record
comprehensive interactions with the apps, which wecould replay at will. Such an interaction, which need nottake more than 5–10 minutes per app, may already be partof the investigation process
3. Also, the effort is easily
offset by the modest false positive rate in M UTA FLOW ;
note that the manual investigation of reported ﬂows caneasily take 1–2 hours per app.
Integrate with static analysis. Our mutation-based analysis
can easily be complemented with static analysis, such thatboth sets of reported ﬂows are joined, as we do in theevaluation (Section IV). Further integration might guidetest generation (and thus M
UTA FLOW ) towards locations
in the code that static analysis has determined to accessand propagate sensitive information.
Use run-time sandboxing. If one has a good source of tests,
one could easily apply run-time checks to disable behav-
ior not seen during testing. During production, the BOX-MA TE sandboxing approach [10], for instance, wouldprohibit access to all sensitive sources not accessed duringtesting (e.g. sensitiveData(), above) and thus dis-
able its originating ﬂows. A combination of M
UTA FLOW
and sandboxing could make M UTA FLOW complete by
construction, but may also limit desirable functionality.
G. Implementation Complexity
From the previous description, it should be obvious that
MUTA FLOW is a much simpler approach than a full-ﬂedged
static analysis for Android, let alone full-ﬂedged symbolicreasoning and checking
4. To put things into perspective, the
full F LOW DROID framework (soot-infoﬂow-android-develop
and soot-infoﬂow-develop) currently sports ∼36,000 LOC, not
counting an additional ∼200,000 LOC for the required Soot,
Heros, and Jasmin frameworks.5In contrast, M UTA FLOW
sports only ∼2,500 lines of Java code, which only is ∼7%
of F LOW DROID .
III. E V ALUA TION DESIGN
To evaluate the effectiveness of M UTA FLOW , we compared
our tool with F LOW DROID 1.5, a static taint analysis tool
for detecting information ﬂows in Android apps. For our
3This process is similar to Apple’s manual app review [9]. This review
process can be recorded for the application and the recorded review can then
be used as input for the M UTA FLOW evaluation.
4There are many opportunities to optimize M UTA FLOW . For instance,
MUTA FLOW currently monitors the covered sources and sinks for each
run, but does not use this information while running the mutated versions.
MUTA FLOW also completely rebuilds the application for each mutation.
5Java source code only, omitting test code; determined using
cloc $(find -f heros-develop/src -f jasmin-develop/src -f
soot-develop/src -f soot-infoflow-android-develop/src -f
soot-infoflow-develop | grep ’. *.java$’) ; retrieved 2017-05-07.evaluation, we used 131 Android apps in three benchmarks—
two micro-benchmarks with small apps designed to evaluate
information ﬂow detection tools, and one macro-benchmark
with real-world apps from the Google play store.
Our evaluation addresses three research questions:
RQ1 Effectiveness for micro-benchmarks. Compared to the
state-of-the-art, how does M UTA FLOW perform in terms
of precision, recall, and F-measure for the micro-benchmarks?
RQ2 Effectiveness for macro-benchmark. Compared to the
state-of-the-art, how does M
UTA FLOW perform in terms
of precision, recall, and F-measure for real world appsfrom the Google Play market place? Recall that weestablished the ground truth for real world apps by crossvalidation rather than by exhaustive means.
RQ3 Performance and Scalability. What is the runtime per-
formance of M
UTA FLOW in comparison to the state-of-
the-art for both benchmarks?
A. Baseline
FLOW DROID is a highly inﬂuential static analysis tool for
Android apps, gathering more than 500 citations since itsinitial release in 2014. V ersion 1.5 was released in October
2016 and represents the state-of-the-art in information ﬂow
detection for Android apps
6. Like M UTA FLOW ,FLOW DROID
works directly at the bytecode level and does not require accessto the app source code.
B. Micro-Benchmarks
TABLE I
MICRO -BENCHMARKS :D ROID BENCH AND DROID RA
Category #Apps #Flows Avg. Size
DroidBench 2.0Aliasing 1 0 75 LoC
Android-speciﬁc 9 8 45 LoC
Arrays and Lists 6 2 41 LoC
Callbacks 14 18 82 LoC
Emulator Detection 3 6 65 LoC
Field and Object Sensitivity 7 2 63 LoC
General Java 20 17 43 LoC
Implicit Flows 1 1 83 LoC
Inter-App Communication 2 8 79 LoC
Inter-Component Communication 14 14 54 LoC
Lifecycle 16 16 54 LoC
Reﬂection 4 4 81 LoC
Threading 5 5 40 LoC
DroidRA Reﬂection 9 9 62 LoC
111 110 47 LoC
As micro-benchmarks, we chose DroidBench and
DroidRA. DroidBench 2.0 [11] is a collection of 120 small
Android apps with several categories of information ﬂows
that are obfuscated in one way or another. V ersion 2.0 ofDroidBench signiﬁcantly extends the micro-benchmark that
was originally published with F
LOW DROID [1]. DroidRA [12]
provides more information ﬂows in the reﬂection categories.
Information ﬂows via Java Reﬂection are particularly hard todiscover because functions are not called directly but in a
6TaintDroid[3] is also a highly inﬂuential dynamic analysis tool for Android
apps, however it is no longer supported, since it was designed for an outdated
Android OS version 4.3, thus it does not work on the Android OS version of
our real world apps - Android Marshmallow 6.0.1.
266
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:53:15 UTC from IEEE Xplore.  Restrictions apply. convoluted manner using Java-speciﬁc internals, such as class
loaders. The average size of an app in the micro-benchmarkis 47 Lines of Code (LoC). From DroidBench, we excluded18 test subjects. Nine apps would crash when executed, dueto bugs or missing permissions. For the other nine apps, therespective sources and sinks were excluded because they arenot in the SuSi set we used (i.e. they do not use basic types).So, in total we analyze 111 apps in the micro-benchmark. Forthe remaining test subjects, the categories, and the number ofinformation ﬂows are listed in Table I.
Ground truth. The actual information ﬂows from a speciﬁc
source to a speciﬁc sink are determined manually by investi-
gating the small programs. This establishes the ground truth.As true positive, we consider a reported ﬂow that actually
exists. As false positive, we consider a reported ﬂow that does
not actually exist. As true negative, we consider an unreported
ﬂow that does also not actually exist. As false negative,w e
consider an unreported ﬂow that does actually exist.
Executions. To generate executions for DroidBench, we
leverage Google’s UI Exerciser Monkey [5]. Monkey takes an
Android app and generates a random sequence of user eventssuch as clicks, touches, or gestures, as well as a number of
system-level events. In our experiments, we ﬁx the length ofa sequence at 10,000 user events and run one test sequencefor each variant of an app. Since Monkey is essentially a
random test generation tool, we repeat each experiment 10
times with different random seeds to gain statistical power.
However, within one experiment, we use the same test se-
quence for all (mutated) variants of the app. For DroidBench,
Monkey demonstrates that M
UTA FLOW does not depend on
pre-existing test cases. However, for more complicated usagescenario (e.g. log-in scenario) sophisticated input-generatorswould be required to reveal ﬂows.
TABLE II
MACRO -BENCHMARK :A PPS FROM THE GOOGLE PLA Y STORE
Name Jimple LoC Size in Bytes
Mysugr 590 kLoC 12.8 MB
Ab Workouts 572 kLoC 6.3 MB
Adidas miCoach 599 kLoC 41.7 MB
Fitness at Home 400 kLoC 18.8 MB
7 Minute Workout 515 kLoC 6.9 MB
Fast Calorie Counter 340 kLoC 2.4 MB
Water Drink Reminder 595 kLoC 9.9 MB
Abs Workout 7 Minutes 344 kLoC 4.7 MB
BMI and Weight Tracker 324 kLoC 4.1 MB
Fabulous – Motivate Me! 715 kLoC 20.3 MB
Test Diabetes Sugar-Joke 367 kLoC 6.9 MB
Kegel Trainer – Exercises 448 kLoC 6.8 MB
Fitness Recipe of the Day 351 kLoC 2.0 MB
Lifesum: Healthy lifestyle 678 kLoC 31.5 MB
Calorie, Carb & Fat Counter 409 kLoC 10.9 MB
30 Day Butt Challenge FREE 569 kLoC 4.6 MB
Blood Pressure Log (bpresso) 282 kLoC 3.2 MB
30 Day Fit Challenges Workout 119 kLoC 7.1 MB
Calorie Counter—FDDB Extender 674 kLoC 7.3 MB
Runkeeper—GPS Track Run Walk 618 kLoC 39.3 MB
Sum 9,509 kLoC 247.5 MB
C. Macro-Benchmark
Asmacro-benchmark, we chose 20 random apps from the
Google Play store—by ﬁrst randomly selecting a category:1) Decompile app with JADX
2) Open the code in Android Studio (so we can use features like ﬁnding
the usage of methods)
3) Find the source and the sink reported from the log (the logs provide
information about containing class and method)
4) For each source:
a) If the value ﬂows into the return of a method, the method usages
have to be checked
b) If the value ﬂows into a call parameter, the called method has to
be checked
c) If the value ﬂows into a ﬁeld, the read usages of the ﬁeld have to
be checked
5) For each sink:
a) If the value comes from the methods parameter, the usage of the
method has to be checked
b) If the value comes from a ﬁeld, the write usages of the ﬁeld have
to be checked
6) A ﬂow is found if a feasible path between source and sink is found
(e.g. there must not be any checks that prevent the path to be taken)
7) For M UTA FLOW , the log can also be consulted:
a) If a value occurs only once in the mutated execution but the API
method is still called in all execution, the ﬂow is also categorized
as true positive
b) If the mutated value is found in plain text in the sink, the ﬂow is
categorized as true positive
Fig. 3. Policy for manual classiﬁcation
“Health & Fitness” and then randomly selecting 20 apps from
this category. Table II provide more details about these real
world apps; in the absence of source code, “LoC” refers tothe length of the decompiled Jimple code.
Ground truth . Unlike for the micro-benchmark, for the real-
world apps we cannot obtain the absolute ground truth but wecan cross-validate. If there are only 100 sources and 100 sinks,we would need to manually check 20×100×100 = 200 ,000
potential ﬂows to identify the complete set of true informationﬂows for all apps. This is clearly impractical. However, on abest effort basis we manually checked all reported information
ﬂows in order to distinguish true from false positives. To
mitigate experimenter bias, we follow a strict coding protocol
involving the independent classiﬁcation by two researchers R1
and R2:
7
1)R1and R2agree on a policy how to classify the reported
ﬂows into true and false positives.
2)R1classiﬁes all ﬂows and reﬁnes the coding policy.
3)R1and R2discuss the reﬁned policy.
4)R2(independently) classiﬁes all ﬂows.
5)R1and R2check the rate of agreement.
6) If the rate is too low, they discuss policy and recode.
7) Otherwise, they proceed to resolve any contentions.
The ﬁnal policy used is listed in Figure 3.
To retrieve the source code from the Android apps, we used
the Dex to Java decompiler JADX [17]. We consider as false
negatives all ﬂows that are true positives for one technique
but not reported by the other. For instance, if F LOW DROID
reported an information ﬂow from source Ato sink B,w e
ﬁrst manually checked whether there really does exist an
7Coding is a methodology from grounded theory that is used in sociology
and psychology to evaluate qualitative properties [13]. In the context of
software engineering [14], [15], this methodology is also referred to as open
card sort [16].
267
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:53:15 UTC from IEEE Xplore.  Restrictions apply. information ﬂow from AtoBand then checked whether
MUTA FLOW also reports the same information ﬂow. If it did
not, the ﬂow was marked as false negative for M UTA FLOW .
The manual validation was done by inspecting the source code
of each app. If the path of a reported ﬂow was not feasible, forinstance due to constants, dynamic types, or other inﬂuences,then the ﬂow was categorized as false positive.
Executions. Almost all health apps require sign-up, log-in or
otherwise entering speciﬁc data, which cannot be synthesizedby the Monkey test generator. We therefore had a user generate
test sequences. Speciﬁcally, we hired a student assistant whose
task was to click through the app with the intention to exploremost of its features, while his interaction would be recorded.The student would sign-up and log-in as needed and enter thedata that was required to proceed to the next user dialog. Therecorded test sequences can be replayed deterministically atwill. For each app, there is one sequence with a length of upto 30 user interactions.
D. Physical Setup and Infrastructure
Being a static analysis, F
LOW DROID can execute on an
arbitrary machine that has access to the app byte code; in
contrast, M UTA FLOW executes the app on an Android device,
emulated or real:
•We execute F LOW DROID on one of our compute servers
with 144 cores and 700 GB of RAM.8
•For M UTA FLOW running the micro-benchmarks, we use
the Android emulator on a PC,9emulating a Nexus 5
running Android Marshmallow 6.0. We only ran onesingle emulator on the machine, as we found parallel
emulators interfering with each other.
•For M UTA FLOW running the macro-benchmarks, we use
a single Android device with six cores and 2GB of
RAM,10controlled by a server via the Android Debug
Bridge (ADB). We use a real device instead of anemulator for two reasons. First, real world apps oftencannot be installed on an emulator, as it lacks featuressuch as the Google play store; second, real devices reportrealistic values for all sources and sinks.
As a 144-core compute server is way more powerful than anAndroid device, let alone an emulated one, the increase incomputing power might seem generous towards F
LOW DROID ;
however, it corresponds to a realistic setting where a usermight have a lot of computing power but access to only oneAndroid device during the execution of M
UTA FLOW .
IV . E V ALUA TION RESULTS
A. Effectiveness for Micro-Benchmarks
We start with RQ1: How effective are M UTA FLOW and
FLOW DROID on our set of micro-benchmarks?
8Speciﬁcally, a 4 ×Intel(R) Xeon(R) CPU E7-8867 v4 @ 2.40GHz with
144 virtual cores (Intel Hyperthreading), running Debian 3.16 Linux.
9Speciﬁcally, an Intel i7 4770S with 8 virtual cores with 32 GB RAM
running Ubuntu 14.04 LTS.
10Speciﬁcally, a a Nexus 5X that has a 64-bit Adreno 418 GPU and a
Qualcomm Snapdragon 808 Processor @ 1.8GHz with 6 cores and 2GB of
main memory, running Android Marshmallow 6.0.1.1) Accuracy: Table III shows the results for F LOW DROID ,
whereas Table IV shows the results for M UTA FLOW . (Note
that the M UTA FLOW results are averaged over 10 runs.)
TABLE III
ACCURACY OF FLOW DROID ON MICRO -BENCHMARKS
Classiﬁed as
Input Flow No Flow Total Precision = 86%
Flow TP = 64 FN = 46 110 Recall = 58%
No Flow FP = 10 TN = 12 22 Accuracy = 58%
Total 74 58 132 F-Measure = 70%
TABLE IV
ACCURACY OF MUTA FLOW ON MICRO -BENCHMARKS
Classiﬁed as
Input Flow No Flow Total Precision = 98%
Flow TP = 74.3 FN = 35.7 110 Recall = 68%
No Flow FP = 1.5 TN = 19.8 21.3 Accuracy = 72%
Total 75.8 55.5 131.3 F-Measure = 80%
We see that on average, M UTA FLOW reports only 1.5 false
positives11, whereas F LOW DROID reports 10 false positives
(6 times as many) .
With a precision of 98%, almost all ﬂows reported by
MUTA FLOW are actual ﬂows.
Interestingly, the recall of M UTA FLOW is higher, too;
MUTA FLOW detects 68% of all ﬂows, whereas F LOW DROID
detects 58%. The higher accuracy of M UTA FLOW over F LOW -
DROID is also indicated by the measures of accuracy and F-
measure.
MUTA FLOW exhibits a better precision, recall, and accu-
racy than the state-of-the-art, FLOW DROID .
So, why is the precision of M UTA FLOW “only” 98% if,
in principle, it should be 100%? The reason again is the non-
determinism, as discussed in Section II-E. Some tests are ﬂaky
in the sense that executing the same test case twice might givedifferent results. This ﬂakyness stems from the randomnessthat is inherent to the Android environment. For instance, whena time stamp is appended to a message, it might seem as if
the monitored sink that receives the message is impacted by
a mutated source while it is not.
2) Complementarity: The aim of M
UTA FLOW is not to be
an alternative to F LOW DROID , but rather complement it—and
this makes perfect sense, as each technique can ﬁnd ﬂows theother does not. As shown in Figure 4, without M
UTA FLOW ,
FLOW DROID would ﬁnd only 58% of the existing ﬂows. Using
both techniques, 90% of all existing ﬂows would be detected.Averaged over ten runs, M
UTA FLOW ﬁnds 35.5 actual ﬂows
that F LOW DROID does not ﬁnd, while F LOW DROID reports
25.2 ﬂows that M UTA FLOW does not ﬁnd.
3) Strengths of FLOW DROID over MUTA FLOW :What are
the strengths and weaknesses of each technique? Figure 6 sum-marizes the detection rates for the individual categories. Wesee that M
UTA FLOW detects fewer ﬂows than FLOW DROID
in the categories
11For 5 applications, M UTA FLOW had at least one false positive.
268
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:53:15 UTC from IEEE Xplore.  Restrictions apply. FlowDr oid — 25.2 38.8 35.5 — MUTA FLOW10.5 (remaining undetected)
Fig. 4. V enn Diagram showing the intersection of found ﬂows (true positives)
for FlowDroid and M UTA FLOW on the micro-benchmark.
1@Override
2protected void onCreate(Bundle savedInstanceState) {
3 super.onCreate(savedInstanceState); // [...]
4
5 // Acquire a reference
6 // to the system Location Manager
7 locationManager = (LocationManager)
8 getSystemService(Context.LOCATION_SERVICE);
9
10 // Register the listener with the Location
11 // Manager to receive location updates
12 locationManager.requestLocationUpdates(
13 LocationManager.GPS_PROVIDER, 5000, 10,
14 locationListener);
15}
Fig. 5. The AnonymousClass1 benchmark uses callbacks to send out location
changes. F LOW DROID detects the associated ﬂow, but M UTA FLOW misses it
because the emulator takes too long to report the changed location.
•callbacks,
•emulator detection, and
•inter-app communication.
We explain the reduced effectiveness by the fact that M U-
TAFLOW requires test cases that actually trigger the ﬂow. For
instance, to detect the ﬂows in the callbacks-category, Monkey
would need to click a speciﬁc sequence of buttons in a speciﬁc
order. The probability that our random testing tool makesthe right sequence of clicks decreases exponentially with thelength of the required sequence. To detect the inter-application
ﬂows, Monkey would need to open one app, trigger the source,
close the app, open the other app, and trigger the sink. With
Monkey as automated test generation tool, M
UTA FLOW can
detect only one of six inter-application ﬂows.
Again, we illustrate the strength of one tool over the other
using an example. In the DroidBench app AnonymousClass1,
Figure 5 shows the essential function, registering a callback
handler for changed locations. In the M UTA FLOW setting, the
emulator does change the location of the device during testing,
but the emulator takes too long to report the change to the app;hence, the callback is never called, and M
UTA FLOW cannot
detect the dynamic ﬂow.
4) Strengths of MUTA FLOW over FLOW DROID :Let us
now go back to Figure 6. We see that M UTA FLOW detects
more ﬂows than F LOW DROID for
•implicit ﬂows,
•inter-component communication, and
•the reﬂection category.
We explain the improved performance with F LOW DROID ’s
difﬁculty to analyze indirect ﬂows along convoluted paths.Unlike F
LOW DROID ,M UTA FLOW is ignorant of the speciﬁc
path along which an important information travels. If there isa test case that exercises both the source and the sink, thenit is quite likely that M
UTA FLOW detects the ﬂow. Hence,Android−specificArrays and ListsCallbacksEmulator DetectionGeneral JavaImplicit FlowsInter−App Comm.Inter−Comp. Comm.LifecycleObject SensitivityReflectionThreading
0% 25% 50% 75% 100%
Detected Flows
Tool FlowDroid MutaFlow
Fig. 6. Histogram showing the number of detected ﬂows as percentage of the
total number of ﬂows per category for both, M UTA FLOW and FlowDroid.
16private String obfuscateIMEI(String imei){
17 String result = "";
18
19 for(char c : imei.toCharArray()){
20 switch(c){
21 case ’0’ : result += ’a’; break;
22 case ’1’ : result += ’b’; break;
23 case ’2’ : result += ’c’; break;
24 case ’3’ : result += ’d’; break;
25 case ’4’ : result += ’e’; break;
26 case ’5’ : result += ’f’; break;
27 case ’6’ : result += ’g’; break;
28 case ’7’ : result += ’h’; break;
29 case ’8’ : result += ’i’; break;
30 case ’9’ : result += ’j’; break;
31 default: // [...]
32 }
33 }
34 return result;
35}
Fig. 7. The ImplicitFlow1 benchmark obfuscates a sensitive device identiﬁer.
The implicit ﬂow is missed by F LOW DROID , but detected by M UTA FLOW .
MUTA FLOW performs well for implicit ﬂows, i.e., where the
data is modiﬁed or obfuscated along the path, for inter-
component communication, i.e., where intents or activities are
used to communicate between different components of thesame app, and for reﬂection, where Java-speciﬁc calls to the
reﬂection framework are used to construct and send messages.
As a typical example of a data ﬂow ignored by F
LOW -
DROID , but detected by M UTA FLOW , consider the Droid-
Bench app ImplicitFlow1. Figure 7 shows the essential func-
tion, obfuscating a sensitive device identiﬁer. Since there isno explicit ﬂow, i.e., a direct assignment of any data inimei toresult,F
LOW DROID misses the ﬂow. (Note that
dynamic tainting approaches such as TaintDroid [3] would also
miss such implicit ﬂows for the same reason.) M UTA FLOW ,
however, easily detects the ﬂow since any change to imei also
results in a change in result; this change then propagates
to the sensitive sink, where M UTA FLOW can detect it.
B. Effectiveness for Real World Apps
Let us now turn to real-world applications and address RQ2:
How effective are M UTA FLOW and F LOW DROID on our
set of macro-benchmarks? In the remainder of this section
we discuss our results listed in Table V.
269
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:53:15 UTC from IEEE Xplore.  Restrictions apply. TABLE V
ANALYSIS RESULTS ON THE MACRO -BENCHMARK
Analysis time (seconds) Number of ﬂows
FLOW DROID MUTA FLOW FLOW DROID MUTA FLOW
Name analysis instr exec sum TP FP unknown sum TP FP unknown
Mysugr crash /Lightning 355 13,724 000 0 23 23 0 0
Ab Workouts 90 519 9,739 916 2 3.5 2 1.5 0
Adidas miCoach 1,628 259 6,608 11 9 1 1 44 0 0
Fitness at Home 43 265 3,479 820 6 00 0 0
7 Minute Workout crash /Lightning 328 4,368 000 0 2.25 1 1.25 0
Fast Calorie Counter 19 135 2,106 000 0 00 0 0
Water Drink Reminder timeout /Lightning 643 16,095 000 0 13.25 13.25 0 0
Abs Workout 7 Minutes crash /Lightning 165 3,280 000 0 11 0 0
BMI and Weight Tracker out of mem /Lightning 141 2,202 000 0 00 0 0
Fabulous – Motivate Me! timeout /Lightning 475 6,124 000 0 11 0 0
Test Diabetes Sugar-Joke 40 194 1,698 110 0 00 0 0
Kegel Trainer – Exercises 21 214 2,293 000 0 00 0 0
Fitness Recipe of the Day 13 153 837 000 0 00 0 0
Lifesum: Healthy lifestyle crash /Lightning 867 15,422 000 0 00 0 0
Calorie, Carb & Fat Counter 32 286 6,856 220 0 4.5 4.5 0 0
30 Day Butt Challenge FREE 286 393 4,138 60 0 0 60 00 0 0
Blood Pressure Log (bpresso) 16 208 3,695 11 4 7 0 22 0 0
30 Day Fit Challenges Workout 26 49 845 000 0 00 0 0
Calorie Counter—FDDB Extender crash /Lightning 585 11,961 000 0 00 0 0
Runkeeper—GPS Track Run Walk12crash /Lightning 1623 timeout /Lightning 000 0 27 25 2 0
Sum 2,217 7,866 149,856 102 19 14 69 81.5 76.75 4.75 0
Average (w/o /Lightningin M UTA FLOW ) 202 329 6,077
Average (w/o /Lightningin F LOW DROID ) 202 243 3,845
Running F LOW DROID and M UTA FLOW on the macro-
benchmark not only took considerable time; we also en-
countered a large number of crashes and timeouts.13When
FLOW DROID crashes, it does not report any ﬂows; hence, the
respective set of ﬂows found is empty.
Following our process for establishing ground truth manu-
ally Section III-C, it took us between one and two hours perapp and person to validate the reported ﬂows by F
LOW DROID
or M UTA FLOW ; for M UTA FLOW , validation was easier as we
had an execution with concrete values to examine.
For a signiﬁcant number of ﬂows reported by F LOW DROID ,
we could not determine whether they were true positives orfalse positives, due to their complexity. Sixty uncategorizableﬂows (87%) where reported by F
LOW DROID for the 30 Day
Butt Challenge FREE app. These ﬂows went through a very
large hashmap that is used throughout the app. If a singletainted value ﬂows into a hashmap, F
LOW DROID marks the
complete hashmap as tainted, spreading the taints throughoutthe program. We believe that most ﬂows are false positivesbut conservatively mark them as uncategorizable.
The Runkeeper app is special in that it drove tools and
humans to their limits. F
LOW DROID crashed on it and M U-
TAFLOW was not done after 10 hours of testing. For M U-
TAFLOW , we would make use of the ﬂows found until the
timeout. In Runkeeper, M UTA FLOW detected 12 ﬂows that
originated from a sensitive source, ended in a SQL database,and later impact a sensitive sink; here, we assumed that the
12Unlike the other apps in our macro-benchmark, Runkeeper was executed
only once, due to changes in the back-end login authentication of the app.
After our ﬁrst execution, we discovered that our human-generated test cases
for this version of Runkeeper could no longer be executed, because we could
no longer login into this version of the app.
13All bugs encountered in F LOW DROID have been reported.ﬂows went through the database. All numbers are reported in
Table V.
1) Accuracy: Table VI summarizes the results for F LOW -
DROID , whereas Table VII summarizes the results for M U-
TAFLOW . (Note that M UTA FLOW results are averaged over
four runs, in order to account for the inherent non-determinismin the Android environment.)
TABLE VI
ACCURACY OF FLOW DROID ON MACRO -BENCHMARK
Classiﬁed as
Input Flow No Flow Total Precision = 58%
Flow TP = 19 FN = 74.75 93.75 Recall = 20%
No Flow FP = 14 TN = 4.75 18.75 Accuracy = 21%
Total 33 79.5 112.5 F-Measure = 30%
69 ﬂows could not get categorized (60 ﬂows arise from 1 app)
TABLE VII
ACCURACY OF MUTA FLOW ON MACRO -BENCHMARK
Classiﬁed as
Input Flow No Flow Total Precision = 94%
Flow TP = 76.75 FN = 17 93.75 Recall = 82%
No Flow FP = 4.75 TN = 14 18.75 Accuracy = 81%
Total 81.5 31 112.5 F-Measure = 88%
The results are in line with those already seen for the micro-
benchmark (Section IV-A): Most notably, M UTA FLOW sports
a precision of 94%, whereas with F LOW DROID , only 58%
of ﬂows reported are true positives. Given the effort it takesto manually identify a ﬂow as true or false positive, a highprecision is deﬁnitely an important goal.
On our set of real-world apps, 94% of all ﬂows reportedbyM
UTA FLOW are actual ﬂows.
Considering the total set Sof true positives (reported by ei-
ther tool and manually classiﬁed as actual ﬂow), M UTA FLOW
270
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:53:15 UTC from IEEE Xplore.  Restrictions apply. reports 82% of these, whereas F LOW DROID reports only 20%;
here, the low recall of F LOW DROID is easily explained by
completing the analysis only for 11 out of the 20 apps.
However, one can see that the high precision of M UTA FLOW
is not offset by a low recall, as indicated by the high overallF-measure. In our macro-benchmark, M
UTA FLOW does not
report any ﬂow for 10 apps, because of the following reasons:
the use of a small set of SuSi sources and sinks, inability to
trigger certain sources (e.g. due to lack of sophisticated testcases), and non-determinism at certain sinks.
2) Complementarity: As already indicated by the false
negative numbers, again each tool misses ﬂows that wouldbe reported by the other one. Figure 8 shows the found ﬂowsfor each of the tools; and again, we see how both approachescomplement each other in their respective strengths.
FLOW DROID — 17 2 74.75 — MUTA FLOW
Fig. 8. V enn Diagram showing the intersection of found ﬂows (true positives)
for FlowDroid and M UTA FLOW on the macro-benchmark.
C. Performance and Scalability
Let us now close the evaluation with RQ3: What is the
performance of M UTA FLOW , and how does it compare
to F LOW DROID ?As already discussed in Section III-D, the
machines we use for M UTA FLOW (a single Android device,
real or emulated) and F LOW DROID (a 144-core compute
server) are very different, so we may well compare peanuts
and pumpkins here. Still, as compute servers are still waymore common than large farms of Android devices, such asetting may well represent the typically available distributionof computing power.
1) Performance on the Micro-Benchmark: On our micro-
benchmark, F
LOW DROID takes an average time of 4.9s per ap-
plication. In contrast, M UTA FLOW takes 9.9s per application.
The longer time of M UTA FLOW is attributed to the overhead
it takes to instrument, install, and execute an application, aswell as the performance penalty of the emulator; F
LOW DROID
need only analyze the (very short) byte code of each app.
On our micro-benchmark, both FLOW DROID and MU-
TAFLOW are very fast, with an average time of 4.9s and
9.9s per app, respectively.
2) Performance on the Macro-Benchmark: On our macro-
benchmark, performance is a more interesting story. Table V
lists the time taken by F LOW DROID (ﬁrst column) vs. the time
taken by M UTA FLOW , whose time is split into instrumentation
(second column) and actual test execution (third column).Looking at the times, let us only consider the 11 appswhere F
LOW DROID could determine the ﬂows. For these
apps, F LOW DROID is very fast, with an average running time
of 202 seconds, or 3.5 minutes; M UTA FLOW is about 20×
slower, taking on average 243 seconds (4 minutes) for creatingthe mutated app versions; and ∼1 hour (3,845 seconds) per
app for running the tests on the individual mutants. Over
all 20 apps (including the 10 hour timeout for Runkeeper),
MUTA FLOW takes an average of 7,493 seconds, or ∼2 hours.
On our set of real-world apps, the MUTA FLOW analysis
takes 1–2 hours of analysis per app and device.
However, keep in mind that F LOW DROID is running on
a 144-core compute server, whereas M UTA FLOW runs on a
single Android device. Both mutant creation and test executionare embarrassingly parallel problems, and easily distributed
across multiple devices. A rack of 20 Android devices wouldreduce the average M
UTA FLOW testing time down to 3 min-
utes, and thus easily catch up with F LOW DROID —and still
be a much smaller investment than a compute server. Again,for the practical analysis of information ﬂows, we would
recommend to have both compute servers for static analysisas well as testing devices for checking concrete ﬂows andmutations.
Mutation-based ﬂow analysis is embarrassingly parallel.
D. Threats to V alidity
Like any empirical investigation, our evaluation is subject
to threats to validity. The ﬁrst concern is external validity,
and notably generality. First, the efﬁciency of mutation-based
ﬂow analysis and static analysis, respectively, is dependent
on a large set of factors, including analyzability of the code,the effort it takes to identify sources and sinks, the ability toautomatically test the code, the effort it takes to create a test,the time it takes to run a test, the value of true positives, and thecost of false positives. Hence, our results do not generalize toarbitrary programs, and the choice of which method(s) to usewill always be left to the user. The aim of this work is simplyto point out mutation-based ﬂow analysis as a relatively simplealternative that enriches the portfolio of program analysis.
Regarding internal validity, our investigation of the ﬂows
may be subject to researcher bias, that is, we may consciously
or unconsciously favor the results of our own tool overthe F
LOW DROID alternative. For the macro-benchmark,w e
counter this threat by following a strict coding policy, asdetailed in Section III-C; for the micro-benchmark, this threat
is countered by having the benchmark as well as its groundtruth all being constructed by the F
LOW DROID team, who,
if at all, would have a bias towards demonstrating the powerof their tool. Both tools are provided with the same set ofsensitive sources and sinks. All our data and assessments areavailable for replication and scrutiny (Section VI).
V. R
ELA TED WORK
Work related to mutation-based ﬂow analysis falls into three
categories.
Static analysis. Static information ﬂow analysis attempts to
detect (sensitive) information ﬂows from static code
271
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:53:15 UTC from IEEE Xplore.  Restrictions apply. analysis. F LOW DROID [1] is the most inﬂuential repre-
sentative in the Android area, and the gold standard for
detecting ﬂows; notable extensions include I CCTA [2] to
analyze inter-process communication and DroidRA [12]
to handle reﬂection. The recent DFlow [18] system is aﬂow detection alternative that focuses on scalability andprecision. In contrast to all these static code analyses,
M
UTA FLOW is a dynamic experimental approach, and
thus can detect ﬂows that static code analysis cannot,as discussed in Section II-E (Soundness), and vice versa(Section II-F; Completeness). This is also the subject ofour evaluation (Section III and Section IV).
Dynamic analysis. Dynamic information ﬂow analysis tracks
data as it is being processed through an execution.The most inﬂuential representative in the Android areais TaintDroid [3]. Its dynamic tainting tags sensitiveinput data with a label (“taint”), which is passed alongto further variables in each computation that involvesa tainted variable; if tainted data reaches a sensitivesink, the tool reports a ﬂow. As dynamic ﬂow anal-ysis can considerably slow down program execution,researchers also have searched for correlations betweeninputs and outputs [19]. In contrast to these approaches,
M
UTA FLOW is an experimental approach which shows
true counterfactual causality and thus soundness by
construction (Section II-E).
Experimental analysis. Experimental program analysis tech-
niques [20] introduce a change in the program execution
and determine its impact. M UTA FLOW , as its name
suggests, is inspired by mutation analysis, where arti-
ﬁcial defects are introduced into the code to determinewhether they will be caught by a test; it is most relatedto the JA V ALANCHE approach [21], which determinesthe impact of the change in the remaining execution.Given a speciﬁc statement, S
ENS A [22] modiﬁes the
statements during test execution in order to determineand quanitify the impact of this statement on the originalexecution. The ORBS approach [23] selectively removesprogram statements to determine a reduced programthat observationally behaves the same as the originalprogram w.r.t. to a slicing criterion. However, all of thesetechniques substantially change the original execution byaltering the program rather than the input coming froman information source, which is arguably minimallyinvasive. Moreover, none of these approaches is gearedtowards detecting the existence of ﬂows at analysis time,as mutation-based ﬂow analysis is.
VI. C
ONCLUSION
To detect information ﬂows, it can already sufﬁce to mutate
an input from a sensitive source and to see whether, whilekeeping everything else unchanged, this change impacts somevalue passed to some sensitive sink. Mutation-based ﬂowanalysis may seem annoyingly simple, but it is very effective:It can reveal ﬂows that static analysis cannot detect; and wherea static analysis tool is not available, not possible, or crashes,it may even serve as a simple alternative. Mutation-basedﬂow analysis thus complements and augments state-of-the-artanalysis tools.
In contrast to static analysis, mutation-based ﬂow analysis
requires an execution and thus input data—either generated ormanually crafted. This requirement is offset by having to spendlittle to no effort on false positives: By construction, mutation-based ﬂow analysis achieves near-perfect precision, meaningthat close to 100% of reported ﬂows are actual ones. In thelong run, we see static analysis and mutation-based analysistools work hand in hand, such that they further strengthen theirrespective ﬁndings.
Besides general improvements such as performance or sta-
bility, our future work will focus on the interplay betweenstatic analysis and mutation-based analysis:
Focused test generation. Rather than using a pure random
testing tool such as Monkey, one could guide a directed
test generator [24] towards code where static analysisalready has determined the existence of potential ﬂows.Static analysis could also tell a test generator whichvalues to provide for which source, and which code toexecute in order to reach a particular sink.
Mutations at function level. The impact of mutations at sen-
sitive sources can also be tracked at other locations in
the program code, not only sensitive sinks. In Figure 1,
this can help to establish the information ﬂow within thedevId() function; in Figure 7, this shows that there
is information ﬂow through the obfuscateIMEI()
function. Such information at the function level not onlygives more detail about the actual information ﬂow, italso provides important function summaries for static
analysis: If F
LOW DROID knows from M UTA FLOW that
obfuscateIMEI() has information ﬂow from input
to its return value, F LOW DROID need no longer miss
the overall ﬂow.
Intertwined analyses. The future of program analysis lies in
the integration of several techniques: Static analysis, dy-namic analysis, test generation, experimental approachesas well as symbolic approaches all must work handin hand to mitigate their respective weaknesses, andturn their integration into strength. Only with a broadknowledge and an open mind can we defeat today’s andtomorrow’s challenges of program analysis.
M
UTA FLOW and all experimental data referred to in this
paper are available as open source; see our package at
https://github.com/anosubmission/mutaﬂow-data
ACKNOWLEDGMENT
Our utmost thanks go to Siegfried Rasthofer, Steven Arzt,
Eric Bodden, and the entire F LOW DROID team for making all
of their tool chain and benchmark data available and keepingit up to date for application, replication, and assessment. Theirsupport has been exemplary in every aspect.
272
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:53:15 UTC from IEEE Xplore.  Restrictions apply. REFERENCES
[1] S. Arzt, S. Rasthofer, C. Fritz, E. Bodden, A. Bartel, J. Klein,
Y . Le Traon, D. Octeau, and P . McDaniel, “FlowDroid: Precise context,
ﬂow, ﬁeld, object-sensitive and lifecycle-aware taint analysis for Androidapps,” in Proceedings of the 35th ACM SIGPLAN Conference on
Programming Language Design and Implementation, ser. PLDI ’14,2014, pp. 259–269.
[2] L. Li, A. Bartel, T. F. Bissyand ´e, J. Klein, Y . Le Traon, S. Arzt,
S. Rasthofer, E. Bodden, D. Octeau, and P . McDaniel, “IccTA: Detectinginter-component privacy leaks in Android apps,” in Proceedings of the
37th International Conference on Software Engineering - V olume 1, ser.
ICSE ’15, 2015, pp. 280–291.
[3] W. Enck, P . Gilbert, B.-G. Chun, L. P . Cox, J. Jung, P . McDaniel, and
A. N. Sheth, “TaintDroid: An information ﬂow tracking system for real-
time privacy monitoring on smartphones,” Communications of the ACM,
vol. 57, no. 3, pp. 99–106, Mar. 2014.
[4] W. Y ou, B. Liang, J. Li, W. Shi, and X. Zhang, “Android implicit
information ﬂow demystiﬁed,” in Proceedings of the 10th ACM
Symposium on Information, Computer and Communications Security,
ser. ASIA CCS ’15. New Y ork, NY , USA: ACM, 2015, pp. 585–590.
[Online]. Available: http://doi.acm.org/10.1145/2714576.2714604
[5] Website, “UI/Application Exerciser Monkey,” https://developer.android.
com/studio/test/monkey.html, online; accessed 25-April-2017.
[6] P . Lam, E. Bodden, O. Lhot ´ak, and L. Hendren, “The Soot framework
for Java program analysis: a retrospective,” in Cetus Users and Compiler
Infastructure Workshop (CETUS 2011), vol. 15, 2011, p. 35.
[7] S. Arzt, S. Rasthofer, and E. Bodden, “SuSi: A Tool for the Fully
Automated Classiﬁcation and Categorization of Android Sources andSinks,” Technical Report. SuSi source code available at https://github.
com/secure-software-engineering/SuSi, 2013.
[8] D. Lewis, “Causation,” Journal of Philosophy, vol. 70, no. 17, pp. 556–
567, 1973.
[9] Website, “Macworld,” https://www.macworld.com/article/2047567/
how-apple-is-improving-mobile-app-security.html, online; accessed
25-April-2017.
[10] K. Jamrozik, P . von Styp-Rekowsky, and A. Zeller, “Mining sandboxes,”
inProceedings of the 38th International Conference on Software
Engineering, ser. ICSE ’16. New Y ork, NY , USA: ACM, 2016, pp. 37–
48. [Online]. Available: http://doi.acm.org/10.1145/2884781.2884782
[11] Website, “Droidbench 2.0,” https://github.com/
secure-software-engineering/DroidBench, online; accessed 25-April-
2017.
[12] L. Li, T. F. Bissyand ´e, D. Octeau, and J. Klein, “DroidRA:
taming reﬂection to support whole-program analysis of androidapps,” in Proceedings of the 25th International Symposium on
Software Testing and Analysis, ser. ISSTA 2016. New Y ork,
NY , USA: ACM, 2016, pp. 318–329. [Online]. Available: http:
//doi.acm.org/10.1145/2931037.2931044[13] K. Charmaz, Constructing grounded theory: a practical guide through
qualitative analysis. London; Thousand Oaks, Calif.: Sage Publications,
2006.
[14] D. Lo, N. Nagappan, and T. Zimmermann, “How practitioners perceive
the relevance of software engineering research,” in Proceedings of the
2015 10th Joint Meeting on F oundations of Software Engineering, ser.
ESEC/FSE 2015, 2015, pp. 415–425.
[15] A. Begel and T. Zimmermann, “Analyze this! 145 questions for data
scientists in software engineering,” in Proceedings of the 36th Interna-
tional Conference on Software Engineering, ser. ICSE 2014, 2014, pp.
12–23.
[16] W. Hudson, Card Sorting In ”The Encyclopedia of Human-Computer
Interaction, 2nd Ed. ”. Interaction Design Foundation, 2013.
[17] Website, “JADX: Dex to Java decompiler,” https://github.com/skylot/
jadx, online; accessed 25-April-2017.
[18] W. Huang, Y . Dong, A. Milanova, and J. Dolby, “Scalable and precise
taint analysis for android,” in Proceedings of the 2015 International
Symposium on Software Testing and Analysis , ser. ISSTA 2015. New
Y ork, NY , USA: ACM, 2015, pp. 106–117. [Online]. Available:
http://doi.acm.org/10.1145/2771783.2771803
[19] J. Huang, X. Zhang, and L. Tan, “Detecting sensitive data
disclosure via bi-directional text correlation analysis,” in Proceedings
of the 2016 24th ACM SIGSOFT International Symposium on
F
oundations of Software Engineering, ser. FSE 2016. New Y ork,
NY , USA: ACM, 2016, pp. 169–180. [Online]. Available: http://doi.acm.org/10.1145/2950290.2950348
[20] J. R. Ruthruff, S. Elbaum, and G. Rothermel, “Experimental program
analysis: A new program analysis paradigm,” in Proceedings of the
2006 International Symposium on Software Testing and Analysis, ser.
ISSTA ’06. New Y ork, NY , USA: ACM, 2006, pp. 49–60. [Online].Available: http://doi.acm.org/10.1145/1146238.1146245
[21] D. Schuler and A. Zeller, “Javalanche: Efﬁcient mutation testing for
Java,” in Proceedings of the the 7th Joint Meeting of the European
Software Engineering Conference and the ACM SIGSOFT Symposiumon The F oundations of Software Engineering, ser. ESEC/FSE ’09.New Y ork, NY , USA: ACM, 2009, pp. 297–298. [Online]. Available:
http://doi.acm.org/10.1145/1595696.1595750
[22] H. Cai, S. Jiang, R. Santelices, Y . J. Zhang, and Y . Zhang, “Sensa:
Sensitivity analysis for quantitative change-impact prediction,” in 2014
IEEE 14th International Working Conference on Source Code Analysis
and Manipulation, ser. SCAM ’14, Sept 2014, pp. 165–174.
[23] D. Binkley, N. Gold, M. Harman, S. Islam, J. Krinke, and S. Y oo, “Orbs:
Language-independent program slicing,” in Proceedings of the 22Nd
ACM SIGSOFT International Symposium on F oundations of Software
Engineering, ser. FSE 2014, 2014, pp. 109–120.
[24] M. B ¨ohme, V .-T. Pham, M.-D. Nguyen, and A. Roychoudhury, “Directed
greybox fuzzing,” in Proceedings of the 24th ACM Conference on
Computer and Communications Security, ser. CCS, 2017, pp. 1–16.
273
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:53:15 UTC from IEEE Xplore.  Restrictions apply. 