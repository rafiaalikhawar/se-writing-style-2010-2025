TheImpact ofFailing, Flaky,and High FailureTestsonthe
NumberofCrashReports Associated with Firefox Builds
MdTajmilurRahman∗
PBSC UrbanSolutions
Longueuil, QC, Canada
trahman@pbsc.comPeterC. Rigby
Departmentof ComputerScienceand Software
Engineering,ConcordiaUniversity
Montreal, QC, Canada
peter.rigby@concordia.ca
ABSTRACT
Testingisanintegralpartofreleaseengineeringandcontinuous
integration.Intheory,afailedtestonabuildindicatesaproblem
thatshouldbeﬁxedandthebuildshouldnotbereleased.Inpractice,
tests decay and developers often release builds, ignoring failing
tests.Inthispaper,westudyingthelinkbetweenbuildswithfailing
testsandthenumberofcrashreportsontheFirefoxwebbrowser.
Buildswithalltestspassinghaveamedianofonlytwocrashreports.
Incontrast,buildswithoneormorefailingtestsareassociatedwith
a median of 508 and 291 crash reports for Beta and Production
builds, respectively. We further investigate the impact of “ﬂaky”
tests, which can both pass and fail on the same build, and ﬁnd
that they have a median of 514 and 234 crash reports for Beta and
Productionbuilds.Finally,buildingonpreviousresearchthathas
shown that tests that have failed frequently in the past will fail
frequently inthe future, we ﬁndthat Builds with HighFailureTests
haveamedianof585and780crashreportsforBetaandProduction
builds.Unlikeothertypesoftestfailures, HighFailureTests havea
larger impact on Production releases than on Beta builds, and they
haveamedianof2.7timesmorecrashesthanbuildswithnormal
testfailures. We conclude that ignoringtestfailures is related toa
dramatic increaseinthe number ofcrashesreportedbyusers.
CCS CONCEPTS
•Software andits engineering →Software testing ;
KEYWORDS
Software Testing, UserCrash Reports, Builds,FlakyTests
ACMReference Format:
MdTajmilurRahmanandPeterC.Rigby.2018.TheImpactofFailing,Flaky,
and High Failure Tests on the Number of Crash Reports Associated with
Firefox Builds. In Proceedings of the 26th ACM Joint European Software
EngineeringConferenceandSymposiumontheFoundationsofSoftwareEn-
gineering (ESEC/FSE ’18), November 4–9, 2018, Lake Buena Vista, FL, USA.
ACM,NewYork,NY,USA, 6pages.https://doi.org/10.1145/3236024.3275529
∗Thisworkwas completed while Rahmanwas a PhD student at Concordia
Permissionto make digitalor hard copies of allorpart ofthis work for personalor
classroom use is granted without fee provided that copies are not made or distributed
forproﬁtorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the ﬁrst page. Copyrights for components of this work owned by others than ACM
mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,
topostonserversortoredistributetolists,requirespriorspeciﬁcpermissionand/ora
fee. Request permissions from permissions@acm.org.
ESEC/FSE ’18, November 4–9, 2018, Lake BuenaVista,FL,USA
©2018 Associationfor Computing Machinery.
ACM ISBN 978-1-4503-5573-5/18/11...$15.00
https://doi.org/10.1145/3236024.32755291 INTRODUCTION
Software builds are tested to ensure that the functionality of the
systemisnotbrokenbyachange.Developerswritetestcaseswhen
they are developing new features or ﬁxing bugs. In a rapid release
model,ﬁxedandshortenedreleaseschedulesreducethetimefor
investigationofthetestregressions[ 13].Weexaminetheimpactof
ignoredfailingtests,ﬂakytests,and HighFailureTests onthebuild
qualityasmeasuredbythenumberofusercrashreportsassociated
withabuild.
We organize our research around the following questions:
(1)RQ1,NumberofCrashes:Howmanycrashesarethere
forbuilds on dev,beta, and production?
Firefoxstagesitsdevelopmentintothreechannels.Thede-
velopment channel contains the current work being done
by developers. The beta channel is used by early testers and
users. The production channel is released to end users. The
stability of the code and the number of users increase as we
movefromtheDevtoProductionchannel.Thisﬁrstresearch
question quantiﬁes the numberof crashes on each channel.
This basic information is important to put the remaining re-
search questions into context as low use channelswill likely
have fewcrashesbut maynot be of high quality.
(2)RQ2, Test Failures: How many crashes are associated
with builds that contain testfailures?
This researchquestion quantiﬁes theimpactoftest failures
on crashes. Our goal is to understand if ignored test failures
leadto an increaseinend usercrash reports.
(3)RQ3, Flaky Tests: How many crashes are associated
with builds that contain ﬂaky tests?
Flaky tests fail non-deterministically[ 12]. For example, a
test may both pass and fail on the same build. As a result,
developers cannot trust a ﬂaky test to determine software
quality.Ourgoalistounderstandifignoredﬂakytestfailures
leadto an increaseinthe number of browsercrashes.
(4)RQ4,HistoricalFailures:Dofailuresofteststhathave
failed many times in the past lead to an increase in
crashes?
Researchershaveshowntheteststhathavefailedinthepast
tendtocontinuetofailathighlevels[ 10].TheseHighFail-
ureTestsallowedresearchestore-ordertestsbasedontheir
historical likelihood to fail [ 1,18]. We consider tests that
historically fail 10% of the time to be HighFailureTests . We
investigatewhetherfailuresofthesetestsleadtoanincrease
inthe number ofcrash reports.
857ESEC/FSE’18,November4–9, 2018, Lake Buena Vista,FL,USA MdTajmilurRahmanandPeterC.Rigby
Thepaperisorganizedasfollows,Section 2providestheback-
groundontheFirefox project’s build processand crash collection.
Section3describes the research methodology. This section also
describesthedatausedinthiscasestudy.Section 4discussesthe
results for each research question. Section 5positions our work in
the literature on build systems and testing. Section 6concludes the
paper.
2 FIREFOXRELEASE PROCESS
Firefox is a popular open source modern web browser and has
been funded by the Mozilla Corporation since November 2004.
Firefox’s release process involves three channels: Development,
Beta,1and Production [ 7–9]. Code remains on each channel for six
weeks before transitioning to the next channel. Each channel has a
diﬀerentlevel ofstability, purpose,and number of developers and
userswhoexerciseit.McIntosh et al.[15]foundthatthenumber
of users per channel is 100K for Development, 1M for Beta, and
100M+for the Release channel.
To create a release, a continuous integration tool, Buildbot, is
usedthrough Bootstrap automationscriptstobuildnewlycommit-
ted features into a new release [ 5]. The Buildbot master creates the
build logs and manages the overall process. Each build has a report
thatcontainsthelogsforeachbuildandincludesbasicinformation
about the build-setup, environment, test steps, the test verdict, and
the overallbuildresult.
When the browser closes unexpectedly a dialogue box allows
users to submit crash reports [ 6]. Each submitted crash report
contains a crash dump including the crashing page address, user’s
local environment, andthe Firefox buildid.
3 METHODOLOGYAND DATA
Ourgoalistoinvestigatetheimpactofignoredfailingtestsandﬂaky
testsonthenumberofreportedenduserbrowsercrashes.Wefollow
a straight-forward method for our study. After loading the data
intoadatabasewenormalizetheteststatusintothreecategories:
“Pass”, “Fail” and “Flaky”. We calculate which tests are historically
HighFailureTests .Wethenlinkthebuildandcrashreportsbasedon
thebuildid.WeuseRtoprovidestatisticalanswerstoourresearch
questions.Figure 1illustrates our researchmethodology.
3.1 Data
We collect the historical build logs and crash reports for Mozilla
FirefoxspanningfromDecember2010toDecember2012.Weparse
thebuildlogsandstoretheextractedinformationinadatabase.The
top portion of the log ﬁle contains the basic build summary includ-
ing information about the builder, slave process, start time, pass
or failverdict,build id,andsource coderevision number (commit
hash).
The test information is contained at the end of the build log ﬁle
and includes the test status, test path, and a short description of
the test. We use the test path to uniquely identify each test. The
pathisaURL that islinkedto the test steps.
Wethenparseandextractallthecrashreportsintothedatabase.
Each crash report contains a crash signature, URL withan unique
id, build id, operating system and other information that may be
1Betawas originaldivided intotwochannels: Auroraand Betausefultodevelopers.Onceweloadthedataintothedatabase,we
remove incomplete data-rows that have missing information, such
as the crasheswithnobuildid.
After extracting the build logs and crash reports we have two
data sets containing the crash information and the build history.
The attributes are listed in Table 1. By joining the two data sets
ofbuildsandcrashesweextractthebuildsthatcouldbemapped
withoneormorecrashes.Foreachbuildweextracttheteststeps
fromthebuildlogandstorethemseparately.Welinkthembased
onbuild_idand we found a total of 2.8K unique build ids that have
bothcrashandtestinformation.Associatedwiththesebuildsare
729K crashes.
Table 1:Attributes forthe buildand crashdata
BuildData Attr. Crash Data Attr.
build_id build_id
build_uid url
revision uuid_url
start_time crash_date
test_info signature
test_description -
test_name -
3.2 TestStatus Mapping
We found six statuses that Firefox developers use to label their
tests. Since there is no formal deﬁnition for these test labels, we
examinedthecodeofthetestscripts[ 4][3].Forthispaper,wemap
the test statuses into three categories: Pass, Fail, and Flaky tests.
The mapping between Firefox statuses and the categories is found
in Table2along with the number of test runs associated with each
status.
In Table2the status “PASS” maps to a normal test pass. The
“FAIL”,“UNEXPECTED-PASS”,and“UNEXPECTED-FAIL”arecate-
gorizedunderthe“Fail”category.Incontrast,the“PASS(EXPECTED
RANDOM)” and “KNOWN-FAIL(EXPECTED RANDOM)” are seen
as failing and passing non-deterministically and we consider them
to be ﬂakytests.
3.3 Identifying FlakyTests
Flaky tests non-deterministically lead to a pass or fail verdict. Lou
et al.identiﬁedﬂakytestsintheirstudy[ 2]bysearchingforthekey-
words “intermittent” and “ﬂak” within the commit history. They
used commit logs for identifying ﬂaky tests because they were
mostly interested in ﬂaky tests that are already ﬁxed. However, we
donotusea keywordsearchto identifyﬂaky tests.We usetheex-
istingFirefoxclassiﬁcationinthebuildlog.Inthetestlogsthetests
thataremarkedas “*-RANDOM”,weincludetheminthe“Flaky”
category which means, tests that are labelled with the statuses
“PASS(EXPECTED RANDOM)” and “KNOWN-FAIL(EXPECTED
RANDOM)”are consideredto be the ﬂakytests (See table 2).
858The ImpactofFailing, Flaky,andHigh Failure Tests on ... ESEC/FSE’18,November4–9, 2018, Lake Buena Vista,FL,USA
Figure 1:Steps inour ResearchMethodology
Table 2:Mappingbetween Firefox teststatusandcategoriesused inthispaper
Firefox Test Status Normalized Categories Numberoftestverdicts
PASS Pass 120M
PASS(EXPECTED-RANDOM) Flaky 265K
KNOWN-FAIL(EXPECTED-RANDOM) Flaky 10K
FAIL Fail 2M
UNEXPECTED-PASS Fail 1K
UNEXPECTED-FAIL Fail 705
3.4 Identifying HighFailureTests
The distribution of failures is not normal.Certain HighFailureTests
accountforalargeproportionoftotalfailures.Previousworkshave
used this historical property to re-prioritize tests so that those that
have failed frequently in the past will be run ﬁrst [ 1,10,18]. We
investigateifbuildswith HighFailureTests haveanincreaseinthe
number ofcrash reports.
We deﬁne a HighFailureTest to be one that has failed on 10%
ormoretestruns.AsanexamplefromtheFirefoxdataset,atest
“brokenUTF-16” ran131K timesand 79% of the total runs resulted
with a “Pass” while 21% times it resulted as a “Fail”. We consider
thistesttobea HighFailureTests .Incontrast,thetest“ hiddenpaging ”
which ran 275K times passed 97% of the time with only 3% failures.
This test would not be considered a HighFailureTest even though it
has failedonpastbuilds.
4 RESULTS
4.1 RQ1: NumberofCrashes
HowmanycrashesarethereforbuildsonDev,Beta,andProduction?
ThedistributionsinFigure 2aretheper-buildnumberofcrashes
on development, beta and production channels. A box plot, is also
containedwithinthedistributionwiththebottomandtopofthebox
showing the 25th and 75th percentiles, respectively. The vertical
lineshowsthe median.
In totaltheirare2.8K buildsassociatedwithone or morecrash
reportsand3.8Kbuildsthatdonothaveanycrashreports.Although
the Dev channel contains experimental code and is likely not as
stableastheotherchannels,weseefewercrashesonthischannel.Inthe median case, development builds are associated with 0crashes
andwith3crashesat the75thpercentile.The Betachannelbuilds
are associated with a median of 437 crashes, and the Production
builds are associatedto 233crashes.
SincebuildsontheDevelopmentchannelarenottypicallyrun
bymain-streamendusers,thenumberoftotalusersislesslikely
explaining thelimitednumber ofcrash reportsforbuildsonthis
channel.Asaresult,wedonotconsiderdevelopmentchannelin
the remainderofthis paper.
ThepurposeoftheBetachannelistostabilizecode.Earlyadopters
usethesebuildsandprovidecrashandbugreportstohelpdevel-
opers to stabilize the code. Despite having fewer users than the
Productionchannel[ 15],buildsonthischannelhavethehighest
number ofcrashes.
CodethatreachestheProductionchannelhaspassedthrough
variousstabilizationandbugﬁxingstageswhichareintendedto
reduce the number end user crashes. Although there are many
crash reports, given the expanded number of users the production
code does appear to be the moststable.
There are a median of 437 and 233 crashes for builds on
theBetaandProductionchannels.Despitehavingmoreend
usersontheProductionchannel,therearefewercrasheslikely
indicating that production code has high stability.
859ESEC/FSE’18,November4–9, 2018, Lake Buena Vista,FL,USA MdTajmilurRahmanandPeterC.Rigby
1 5 50 500 5000
Dev Beta ProductionCrash reports per build + 1 (log scale)
Figure 2:Numberofcrashesforeachchannel
4.2 RQ2: TestFailures
Howmanycrashesareassociatedwithbuildsthatcontaintestfail-
ures?
Our conjecture is that when developer ignore quality assurance
indicatorstherewillbemorecrashesonthesebuilds.Inthisresearch
question,weexaminethenumberofignoredtestfailuresforbuilds
and relate them with the number ofcrashes. We expect that more
browser crashes will be associated with builds that have failing
tests (i.e.failing builds) compared to those that do not have failing
tests (i.e.passingorcleanbuilds).
Firefox runs a large number of tests during each build. In the
median case 3M tests are run on each build with a maximum
of 11M. Table 2shows the number of passing and failing tests:
120M and slightly over 2M, respectively. We exclude “random”,
non-deterministic tests examining theminthe nextsection.
Figures3and4contrasts the number of crash reports for builds
withatleastonefailingtestwithbuildsthathaveonlypassingtests.
FortheBetachannel,wefoundthatbuildsthathavefailingtests
have a median of 508 crash reports. In contrast, passing test builds
have amedian of2crash reports with5at the 75thpercentile and
amaximum of25.
IntheProductionchannelbuildswithfailingtestshaveamedian
of 291 crashes. In contrast, passing build are associated with a
median of2crash reports with13 at the 75thpercentile.
Inthemediancase,buildsthathavefailingtestsareassociated
with 508 and 291 crash reports for the Beta and Production
channels.Incontrast,buildswithpassingtestshaveamedianof
only two crash reports, with some outlier pass production builds
withmanycrashes.4.3 RQ3: FlakyTests
Howmanycrashesareassociatedwithbuildsthatcontainﬂakytests?
Flaky tests fail in a non-deterministic manner and potentially
hidebugs.Forexample,ifaﬂakytestfailsfrequently,developers
tendto ignorethe failuresand couldmissthe realbugs. We investi-
gatewhetherignoringﬂakytestsisapotentialreasonforincreased
browsercrashes.WeusetheFirefoxtestoutcomelabelsthatcontain
“-RANDOM"todeterminewhichtestsareﬂaky(SeetheMethod-
ology Section 3for more details on the process of classiﬁcation).
In table2, we see that 275K ﬂaky test-runs are labelled with the
“-RANDOM”verdict acrossallbuilds.
In the median case each Production channel build that contains
atleastoneﬂaky testfailure isassociated with234crashes.There
isahighdegreeofvariationwith585crashesatthe75thpercentile
and a maximum of 93k crash reports. Production builds with ﬂaky
tests are associated with almost the same number of crashes as
thosewithregularfailingtests.
FortheBetachannelinFigure 3weobservedasimilarpatterwith
a514,1.2K,21Kcrashreportsforthemedian,75thpercentileand
maximum, respectively. Beta builds with Flaky tests are associated
with almost the same number of crashes as those with regular
failingtests.
A Wilcoxon test comparing the crashes for ﬂaky builds shows a
statistically signiﬁcant diﬀerence between the Beta and Production
channelswiththep-value p<.001.Whilefutureworkisnecessary,
weconjecturethatdevelopersaremoreconservativewithreleasing
productionbuildswithknownfailingandﬂakyteststhanwithbeta
builds.
In the median case, builds with failing ﬂaky tests we asso-
ciatedwith514and234crashreportsforBetaandProduction,
respectively.
4.4 RQ4: Historically HighFailureTests
Do failures of tests that have failed many times in the past lead to an
increase in crashes?
Insoftwaresystems,problemsclusterarounddefectivecodeand
teststhat havefailed frequentlyin thepastare likelyto failin the
future[1,10,18].Toinvestigatethesetests,weclassifytestthatfail
in10%ormoreoftheirtotalrunsashistorically HighFailureTests .
In the last distribution in Figures 3and4, we show that builds that
have a failing test that is classiﬁed as HighFailureTest lead to lower
qualitybuildsasmeasuredbyanincreaseinreportedcrasheson
both the Production andBetachannels.
The crash distribution for production builds that have one or
morefailingteststhatarecategorizedas HighFailureTests showa
medium, 75th percentile, and maximum of 780, 1.4k, and 21k crash
reports,respectively.Productionbuildswith HighFailureTests are
associated with 2.7 times more crashes than builds with regular
failingtests and390times higher thanthe passingbuilds.
ForBetabuildsthecorrespondingvaluesare585,1.5k,and92k
crashes reports for the median, 75th percentile, and maximum, re-
spectively.Betachannelbuildswith HighFailureTests areassociated
860The ImpactofFailing, Flaky,andHigh Failure Tests on ... ESEC/FSE’18,November4–9, 2018, Lake Buena Vista,FL,USA
1 5 50 500 5000
Pass Fail Flaky High FailCrash reports per Beta build (log scale)
Figure 3:Crashes pertype forBetabuilds
1 5 50 500 5000
Pass Fail Flaky High FailCrash reports per Production build (log scale)
Figure 4:Crashes pertype forProductionbuilds
with1.3timesmorecrashesthanbuildswithregularfailingtests
and292times higher thanthe passingbuilds.
A Wilcoxon test comparing HighFailureTests on the Beta and
Production channels shows a statistically signiﬁcant diﬀerence
betweenthe crashesonthesetwochannels withthep-value p<
0.022.UnliketheﬂakyteststhatarelabeledbyFirefoxdevelopers, High-
FailureTests arenotdiﬀerentiatedfromothertypesoffailingtestsby
the developers. This is especially problematic on production builds
astheseignoringthese HighFailureTests leadtomanycrashreports.
Firefoxdevelopersmightbeneﬁtfromidentifyingandmonitoring
this classiﬁcation oftests.
In the median case, builds that contain failing historically
HighFailureTests are associated with 585 and 780 crashes for
BetaandProductionrespectively. HighFailureTests arepartic-
ularly problematic with production builds where there are 2.7
times more crash reports when compared to builds with normal
test failures.
5 RELATED WORK
We divide the related work into testing and build maintenance and
quality.Weareunawareofanyworkthathasstudiedtheimpact
oftestingonﬁeld crash reports.
Testing and Flaky Tests. Labuschagne et al.studied the cost of
regression testing in practice [ 11]. They found that 18% of the total
testsuiteexecutionsfail.Moreinterestingly,13%ofthesefailures
areﬂaky.Ofthenon-ﬂakyfailures,only74%werecausedbyabugin
the system under test and the remaining 26% were due to incorrect
orobsoletetests.Theyalsofoundthatinthefailedbuilds,only0.38%
ofthetestcaseexecutionsfailedand64%offailedbuildscontaining
more than one failedtest. This study illustrates the importance of
dealingwiththeﬂakyteststoimprovethequalityoftheregression
testing. Our study addsto this knowledge by studying theimpact
oftest failures onﬁeld crashes.
Recent works on ﬂaky tests identiﬁed the root cause of the ﬂaki-
ness.Forexample,Eloussiidentiﬁedﬂakytestsfromtestresults[ 2]
inherdoctoralresearchwheresheproposesthreeimprovements
forthebasictechniquetoidentifyﬂakinessoftests.Bymanually
examining the ﬂaky test Eloussi divided the tests into three types:
Non-Burstly, Burstly and State-Dependent Burstly. Another study
on ﬂaky test by Memon et al.[16] provides a detail post-mortem
of ﬂaky tests that provides actionable information about avoiding,
detecting and ﬁxing these types of non-deterministic tests. They
inspectthe test code by analyzingthe code commits thatlikelyﬁx
ﬂaky tests. They also identiﬁed the root causes of ﬂakiness but not
theimpactafterrelease.Inourwork,wemeasuredtheimpactof
ﬂakytestsoncrashes.Futureworkcould usethecrashreportson
ﬂaky teststo validate thecauses identiﬁed by Eloussi and Memon.
Generalbuildsystemstudies. Xinet al.performedanempirical
studyonbugsinbuildsystems[ 17].Theycategorizedbugsbasedon
theirtypeandseveritiesandfoundthatthethirdhighestpercentage
of bugs belong to the build-conﬁguration category. They examined
the association between the bugs and the build conﬁgurations,
while we associate browser crashes with the failing tests in a build.
McIntoshempiricallystudiedbuildsystemsinhisdissertation
[14]. His publications include a build maintenance study of the
eﬀort spent on maintaining the build process and the ownership of
thesebuildscripts[ 15].Hefoundthatmaintainingthebuildsystem
required signiﬁcant eﬀort, with an overhead of 27% on source code
861ESEC/FSE’18,November4–9, 2018, Lake Buena Vista,FL,USA MdTajmilurRahmanandPeterC.Rigby
developmentand44%ontestdevelopment.Ourworkaddsevidence
thatbuildmaintenanceisanimportantproblembyshowingthat
ignoringbuildandtestproblemsleadtosubstantiallymorecrashes
increasing developer eﬀortandimpact onend users.
6 CONCLUSION AND FUTUREWORK
In this paper, we investigate the association between builds and
browsercrashesonBetaandtheProductionchannelsoftheFirefox
web browser. We study the impact of ignoring failing, ﬂaky, and
HighFailureTests onthe number ofcrashesfor abuild.
WeobservethatignoringfailingtestsmakestheFirefoxbuilds
much more crash prone compared to the builds that do not have
any failing tests (passing builds). Passing builds have a median of 2
crashesforbothBetaandProduction.Incontrast,buildswithfailing
tests have 508 and 291 crashes in the median case, respectively.
Flaky tests non-deterministically pass or fail reducing developer
conﬁdence in the test. In the median case, builds with failing ﬂaky
testshad514and234crashesfortheBetaandProductionchannels,
respectively. Previous works have shown that tests that have failed
in the past are more likely to fail in the future [ 1,10,18]. We
quantiﬁed HighFailureTests as those thathave failed in 10% of past
runs. In the median case, builds with failing HighFailureTests have
585and780crashesfor BetaandProduction.
Our results show that ignoring failing and ﬂaky tests results in
morecrashesinBetathanProduction.However,ignoring HighFail-
ureTeststests leads to more crashes on the Production than Beta
channel.Ignored HighFailureTests wereassociatedwithamedian
of 780 crashes on the Production channel. This is the most crashes
associated with any type of failing test and channel. In the median
case there are 2.7 times more crashes per build than builds with
normaltestfailures.Afailing HighFailureTests clearlywarrantsa
detailedinvestigation before release byFirefox developers.
We hope that our work will inspire developers to understand
the high risk of ignoring failing tests. We hope that researchers
willextendourworkbyexaminingboth therootcausesoffailing
andﬂakytests andby contributingadvanced statisticalmodelsto
enhance our understanding ofthe associatedrisks.
ACKNOWLEDGEMENTS
We would like to thank Bram Adams from Ecole Polytechnique
MontrealforprovidingtheFirefoxdataandforhelpingusunder-
stand someits key features.REFERENCES
[1]Sebastian Elbaum, Gregg Rothermel, and John Penix. 2014. Techniques for
Improving Regression Testing in Continuous Integration Development Envi-
ronments. In Proceedings of the 22Nd ACM SIGSOFT International Symposium
on Foundations of Software Engineering (FSE 2014) . ACM, New York, NY, USA,
235–245. https://doi.org/10.1145/2635868.2635910
[2]LamyaaEloussi.2015. Determiningﬂakytestsfromtestfailures . Ph.D.Dissertation.
Universityof Illinois at Urbana-Champaign.
[3]Mozilla Firefox. 2012. Test Labeling in Mozilla : integration-mozilla-inbound.
http://bit.ly/2riO0Nh.
[4]Mozilla Firefox. 2012. Test Labeling in Mozilla : Xpcshell Self Test.
http://bit.ly/2scySB1.
[5]Mozilla Firefox. 2017. Build:Release Automation.
https://wiki.mozilla.org/Build:Release_Automation.
[6]Mozilla Firefox. 2017. Firefox Crash Reporter. https://support.mozilla.org/en-
US/kb/mozillacrashreporter.
[7]Mozilla Firefox. 2017. Release Management/Release Process.
https://mzl.la/1KhlZf9.
[8]Foutse Khomh, Bram Adams, Tejinder Dhaliwal, and Ying Zou. 2015. Under-
standing the impact of rapid releases on software quality. Empirical Software
Engineering 20,2 (2015), 336–373.
[9]FoutseKhomh,TejinderDhaliwal,YingZou,andBramAdams.2012. Dofaster
releasesimprovesoftwarequality?:anempiricalcasestudyofMozillaFirefox.In
Proceedingsofthe9thIEEEWorkingConferenceonMiningSoftwareRepositories .
IEEE Press,179–188.
[10]Jung-MinKimandAdamPorter.2002. Ahistory-basedtestprioritizationtech-
niqueforregressiontestinginresourceconstrainedenvironments.In Software
Engineering, 2002. ICSE 2002. Proceedings of the 24rd International Conference on .
IEEE,119–129.
[11]AdriaanLabuschagne,LauraInozemtseva,andReidHolmes.2017. Measuring
theCostofRegressionTestinginPractice:AStudyofJavaProjectsUsingCon-
tinuous Integration. In Proceedings of the 2017 11th Joint Meeting on Foundations
of Software Engineering (ESEC/FSE 2017) . ACM, New York, NY, USA, 821–830.
https://doi.org/10.1145/3106237.3106288
[12]Eloussi Luo, Hariri. 2014. An empirical analysis of ﬂaky tests. In Proceedings
ofthe22ndACMSIGSOFTInternationalSymposiumonFoundationsofSoftware
Engineering . ACM,643–653.
[13]MikaV.Mäntylä,FoutseKhomh,BramAdams,EmelieEngström,andKaiPetersen.
2013. OnRapidReleasesandSoftwareTesting.In Proceedingsofthe2013IEEE
International Conference on Software Maintenance (ICSM ’13) . IEEE Computer
Society, Washington, DC, USA,20–29. https://doi.org/10.1109/ICSM.2013.13
[14]Shane McIntosh. 2015. Studying the Software Development Overhead of Build
Systems. PhD dissertation. Queen’s University.
[15]S.McIntosh,B.Adams,T.H.D.Nguyen,Y.Kamei,andA.E.Hassan.2011. An
empiricalstudyofbuildmaintenanceeﬀort.In 201133rdInternationalConference
onSoftwareEngineering(ICSE) .141–150. https://doi.org/10.1145/1985793.1985813
[16]Cohen Memon. 2013. Automated testing of gui applications: models, tools,
andcontrollingﬂakiness.In Proceedingsofthe2013InternationalConferenceon
SoftwareEngineering . IEEE Press,1479–1480.
[17]X.Xia,X.Zhou,D.Lo,andX.Zhao.2013. AnEmpiricalStudyofBugsinSoftware
BuildSystems.In 201313thInternationalConferenceonQualitySoftware .200–203.
https://doi.org/10.1109/QSIC.2013.60
[18]Y. Zhu, E. Shihab, and Rigby PC. 2018. Test Re-prioritization in Continuous Test-
ingEnvironments.In 2018IEEEInternationalConferenceonSoftwareMaintenance
and Evolution . 10.
862