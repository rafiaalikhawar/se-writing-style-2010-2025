Automating Non-Blocking Synchronization In
Concurrent Data Abstractions
Jiange Zhang
University of Colorado at
Colorado Springs, CO, USA
jzhang3@uccs.eduQing Yi
University of Colorado at
Colorado Springs, CO, USA
qyi@uccs.eduDamian Dechev
University of Central Florida
Orlando, FL, USA
dechev@cs.ucf.edu
Abstract ‚ÄîThis paper investigates using compiler
technology to automatically convert sequential C++
data abstractions, e.g., queues, stacks, maps, and trees,
to concurrent lock-free implementations. By auto-
matically tailoring a number of state-of-the-practice
synchronization methods to the underlying sequen-
tial implementations of diÔ¨Äerent data structures, our
automatically synchronized code can attain perfor-
mance competitive to that of concurrent data struc-
tures manually-written by experts and much better
performance than heavier-weight support by software
transactional memory (STM).
I. Introduction
The advent of the multi-core era has brought multi-
threaded programming to the mainstream and with it the
challenges of managing shared data among the threads.
Compared to traditional lock-based mechanisms [1], [2],
[3], [4], [5], non-blocking synchronization oÔ¨Äers lock-free
progress guarantees and better fault tolerance, but at the
expense of requiring much more extensive modiÔ¨Åcations to
the sequential code, limiting their wide-spread use. This
paper aims to ameliorate this diÔ¨Éculty.
Nowadays lock-free synchronization is programmed ei-
ther by manually following various construction meth-
ods [6], [7], [8], [9], [10], [11], [12], [13], [14] or automatically
by using the higher-level interface supported by software
transactional memory (STM) [15], [16], [17]. This paper
presents an alternative approach to STM, by using source-
to-source compiler technology to automatically convert
sequential C++ data abstractions, e.g., queues [6], [7],
sets [9], [8], [10], and trees [14], [13], [12], to lock-free
concurrent implementations, to relieve developers from the
error-prone task of low-level concurrent programming.
Our compiler supports the synchronization of a single
abstraction and therefore is not as general-purpose as
STM, which supports the synchronization of whole soft-
ware applications. However, a key insight from our work
is that by restricting the scope of synchronization to a
single self-contained abstraction and thereby selecting the
best synchronization strategy to tailor to the underlying
sequential implementation, runtime overhead of synchro-
nization can be signiÔ¨Åcantly reduced, resulting in much
better performance scalability than using existing heavier-
weight STM implementations to support all synchroniza-tion needs. We show that by automatically weaving syn-
chronization schemes with sequential implementations of
data structures, many eÔ¨Écient lock-free data structures
can be made readily available, with performance compet-
itive to that of the manually crafted ones by experts.
The key technical diÔ¨Äerence between our work and exist-
ing STM research is that we support multiple synchroniza-
tion strategies and automatically select the best strategy
for each piece of code at compile time. No existing compiler
for STM supports automatic tailoring of synchronizations
to what is needed by diÔ¨Äerent pieces of code. The compiler
algorithms we present are the Ô¨Årst to do this. While we
mostly rely on standard compiler techniques (e.g., pointer
and data-Ô¨Çow analysis), the problem formulations and
solution strategies do not yet exist in any compilers.
Our programming interface is easy to use as it does not
require the user to declare anything (in contrast, STM
requires all shared data references to be fully wrapped in-
side special regions called transactions [15], [16]). However,
our compiler does require that shared data, together with
all their operations, must be encapsulated into a single
C++ class, which contains all the sequential implemen-
tations as the baseline for synchronization. The goal of
our compiler-driven approach is to close the performance
gap between using STM to support all synchronization
needs of an application vs manually crafting much lighter
weight (and thus more eÔ¨Écient) synchronizations for each
individual shared concurrent data structure [5], [18], [11].
The performance attained by our auto-generated code can
be limited by their original sequential implementations,
e.g., by whether a linked list or an array is used to organize
the data. To quantify such impact, we have experimented
with using both data layout schemes for selected data
structures. Overall, our technical contributions include:
‚Ä¢We present a cohesive strategy to eÔ¨Äectively adapt
and combine state-of-the-practice synchronization
techniques, to automatically convert sequential data
abstractions into concurrent lock-free ones.
‚Ä¢We present formulations and algorithms to automat-
ically classify shared data based on how they are
referenced in the sequential code and to automatically
tailor their synchronizations to the varying character-
istics of their concurrent operations.
7352019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE)
978-1-7281-2508-4/19/$31.00 ¬©2019 IEEE
DOI 10.1109/ASE.2019.00074
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:39 UTC from IEEE Xplore.  Restrictions apply. 1template <class T> SinglyLinkedList {
2 Node <T>* head; Node <T>* tail; unsigned count;
3
4void pushback( const T& o){
5 Node <T>* e= new Node <T> (o); ++ count;
6 if (tail==0) {head=tail=e;} else {tail -> next=e; tail=e;} }
7
8bool is_empty(){ return count == 0; }
9
10 bool lookup( const T& t) {
11 Node <T> *e = head; while (e!=0 && e->content!=t) e = e->next;
12 return (e!=0); }
13 ... };
Fig. 1: Example: a sequential singly-linked list
‚Ä¢We have implemented a prototype source-to-source
compiler to automatically support the lock-free
synchronization of eight data structures, including
queues, stacks, hash maps, and trees. We show that
the performance of our auto-generated implemen-
tations are competitive against existing manually
crafted implementations by experts and better than
auto-generated implementations by using heavier-
weight Software Transactional Memory.
Our prototype compiler, together with all the data struc-
tures evaluated in this paper, will be released as part of
the source code release of the POET language [19].
The rest of the paper is organized as follows. Section II
details the synchronizations supported by our compiler.
Section III summarizes our overall compilation strategies.
Section IV presents additional details of the compile-time
formulations. Section V presents experimental results. Sec-
tion VI discusses related work. Section VII concludes.
II. Details Of Synchronization
Given nconcurrent operations op1,op2, ..., op n,t h e y
arelinearizable and thus properly synchronized, if every
time they are evaluated, there exists a sequential ordering
of the same operations that maintains real-time ordering
and produces the same results [20]. The synchronization is
non-blocking if the failure of any operation never disrupts
the completion of others. It is additionally lock-free if some
operation is always guaranteed to make progress.
Our compiler automatically combines two widely-
used non-blocking synchronization techniques: read-copy-
update [21] (RCU) and read-log-update [22] (RLU),
into four overall schemes: single-RCU, single-RCU+RLU,
RLU-only, and multi-RCU+RLU, each scheme incremen-
tally extending the previous ones to be more sophisticated.
Single-word compare and swap (CAS) and total store
ordering are the only hardware supports required. All
schemes guarantee lock-free progress in that if multiple
operations try to modify an abstraction via CAS at
the same time, at least one of them will succeed, while
the others restart. The RLU-only and multi-RCU+RLU
schemes allow independent modiÔ¨Åcations to move forward
concurrently, while the lighter weight single-RCU/single-
RCU+RLU schemes essentially sequentialize all concur-
rent modiÔ¨Åcations. All schemes allow read-only operations
to make full progress independent of ongoing modiÔ¨Åca-
tions. The following details each of these schemes.1typedef struct List_state {
2Node<T>* head; Node<T>* tail; unsigned count; unsigned id;
3typedef struct ModLog {
4 Node <T>** addr; /*address being modified*/
5 Node <T> *old , * new ;/*old and new values of addr*/ } ModLog;
6 atomic <vector <ModLog >*> modlogs;
7
8void copy(List state *that) {id=that- >id+1;...copy members... }
9
10 void new_modlog( Node <T>** _addr ,Node <T>* _old ,Node <T>* _new)
11 { check_conflict( this ,_addr , _old);
12 vector <ModLog > *c = load_modlog(modlogs);
13 c->push_back(ModLog( _addr ,_old , _new)); }
14
15 void modlog_apply()
16 { vector <ModLog > *c=fin alize_modl og(modlogs); if (c==0) return ;
17 for (vector <ModLog >:: iterator p=c->begin();p!=c->end();p++)
18 { ModLog t = (*p);
19 atomic <Node <T>*>* t1 =
20 reinterpret_cast <atomic <Node <T>*>*>(t.addr);
21 cas_mod(t1,&t.old,t. new ,id); /*use weak CAS to modify t1*/
22 }}
23 } List_state;
24
25atomic <List state* >state;
Fig. 2: Relocating data of Figure 1 (text in red is related
to RCU synchronization; the rest is related to RLU)
1void pushback( const T& o) {
2 Node <T>* e= new Node <T> (o);
3 SinglyLinkedList state *oldp=0, *newp=allocate state();
4 while (true )
5 {try {oldp=load current state(MOD RCU); oldp -> modlog_apply();
6 newp->copy(oldp); ++newp- >count;
7 if (newp->tail ==0) { newp->head=newp- >tail=e; }
8 else { newp->new_modlog(&(newp-> tail -> next),
9 newp ->tail ->next ,e);
10 newp->tail = e; }
11 if (state.compare exchange strong(oldp,newp))
12 { newp -> modlog_apply(); break; }
13 }catch (const ModLogConflict& e){} } }
14
15 bool is_empty()
16 {SinglyLinkedList state *oldp=load current state(READ RCU);
17 oldp ->m odlog_apply(); return oldp->count == 0; }
18
19 bool lookup( const T& t)
20 { SinglyLinkedList state *oldp=0;
21 while (true )
22 {try {oldp=load current state(READ RLU); oldp->modlog_apply();
23 Node<T>*e = oldp- >head;
24 while (e!=0 && e->content!=t)
25 { Node <T>* n=e-> next; check_conflict(oldp ,&e-> next ,n);
26 e=n;}
27 return e! =0 ;
28 }catch (const ModLogConflict& e){} } };
Fig. 3: Example: synchronizing a singly-linked list (text in
red is related to RCU synchronization)
a)single-RCU: where all shared data of an ab-
straction are considered a single jointly-accessed group,
to be collectively copied and synchronized via the RCU
(Read-Copy-Update) scheme, which includes four steps:
(1) collect all shared data in the group into a continuous
region whose address is stored in an atomic pointer p;
(2) at the beginning of each operation f, atomically load
the address estored in p;i f fmodiÔ¨Åes shared data,
additionally copy data of einto a new region e2( 3 ) f
proceeds as in a sequential setting, except all shared data
accesses are redirected through address e(or e2 if the data
inehas been copied to e2); and (4) upon completion, if
fhas modiÔ¨Åed data in e2, it tries to use e2t or e p l a c e
the content of p, using a single compare-and-swap (CAS):
if the CAS succeeds, freturns; otherwise, a conÔ¨Çict is
detected, and frestarts by going back to step (2). If fdoes
not modify shared data, it simply completes and returns,
as the content of address eis intact, even if other threads
have modiÔ¨Åed the atomic pointer pin the mean time.
To illustrate, Figure 2 shows the new type deÔ¨Åned to
relocate the three member variables declared at line 2
of the singly-linked list in Figure 1. A shared atomic
736
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:39 UTC from IEEE Xplore.  Restrictions apply.    	


 

	 
  !"#
 
   $%#&$'

$
$'
('

$
$&) ')




$
$

$%#&$'

$
$'
( ' 


$
$'&*
	 &   
&


$*
$'*


$
$'' 	 




$*
$'*
	 &   
&

$*
$' * 
' 
&+

$
$

$
$

$
$'

$
,

,
,
&-.,  !"#
 
 & 	 &   
&
  !"#
 
    



(a) Synchronizing via single-RCU+RLU
	
 	


	 	 !"#
!$%"	
$

!&

	 !%%		
 
	!&'

	 

	 !%%		
 
	!%	%	
 %% 
%	%'	  %% 
	 !%%		
 
	!
 ' ( 	
	
	

"

		
	 
' (  "	


		
'	 




	


		"
(   "	
'(#
	
	 !"#
	
"
	
'&#
	
	
'(#
	


%	%	
 %%!
 
&(
	
(&#
	
	
(&#
	
& (  "
(b) Synchronizing via RLU-only
Fig. 4: Synchronizing via single-RCU+RLU vs RLU-only
pointer, state , is then declared at line 25 to hold the
most current address of the relocated data, termed a RCU
object . Each RCU object is given a unique number (named
idat line 2 of Figure 2), to track the linear sequence of suc-
cessively committed RCU Objects whose addresses have
been held by the shared atomic pointer. Additional details
of synchronization are illustrated in Figure 3 via the red-
colored text. Here each operation starts by instantaneously
obtaining the most recent address of the state atomic
pointer, by invoking load current state at lines 5, 16 and
22. Each read-only operation (e.g., isempty andlookup )
only has to redirect all the reads through the loaded
address (in variable oldp). Each modiÔ¨Åcation operation
(e.g., pushback ) Ô¨Årst makes a copy of the shared data
(line 6), then redirects all modiÔ¨Åcations to its local copy
(lines 7-10), and Ô¨Ånally uses the address of the local copy to
modify the shared atomic pointer using a CAS at line 11.b)Single-RCU+RLU: where single-RCU is com-
bined with the RLU (Read-Log-Update) scheme to address
situations when some internal data of the abstraction
cannot be easily copied or are too expensive to copy.
A RLU synchronization includes three steps: (1) each
operation fsaves its shared data modiÔ¨Åcations as private
logs locally, (2) when done with modiÔ¨Åcations, ftries to
publicize its delayed logs to all threads; if the publication
succeeds, fuses a sequence of weak atomic updates to
apply the logs to physical shared memory before returning;
and (3) if ffails to publicize its logs, a conÔ¨Çict is detected,
and it starts over by going back to step (1). By enforcing
that all threads that observe a set of publicized logs
immediately help to complete these logs via weak atomic
updates, where concurrent redundant updates are simply
ignored, all the publicized logs can be sorted in the shared
space and applied atomically in their real-time ordering.
To make sure all publicized modiÔ¨Åcations are observed, all
threads also dynamically examine the publicized logs for
each shared address they access to detect conÔ¨Çicts in time.
To combine RCU with RLU, our compiler augments
each RCU struct with an array of modiÔ¨Åcation logs, e.g.,
at lines 3-5 of Figure 2. These logs are created and saved
locally inside a local copy of the RCU struct, by invoking
its new modlog method, e.g., at line 8 of Figure 3. All
the delayed logs are Ô¨Ånally publicized, e.g., at line 11
of Figure 3, when the local copy is used to replace the
shared atomic pointer. Once successfully publicized, these
logs are applied as soon as they are observed by all
threads, by having these threads immediately invoke the
modlog apply method of each newly loaded RCU object
to help complete its logged modiÔ¨Åcations (e.g., at lines 5,
12, 17, and 22 of Figure 3). To ensure each operation
observes the new publicized logs in time, a check conÔ¨Çict
function is invoked (e.g., at line 25 of Figure 3) to dynami-
cally detect conÔ¨Çicted logs immediately after loading each
shared address eand immediately before recording any
modiÔ¨Åcation log for e. Upon detecting a conÔ¨Çict, it throws
an exception if the ongoing operation modiÔ¨Åes shared data.
However, if the ongoing operation is read-only, the original
value before the conÔ¨Çicted modiÔ¨Åcation is retrieved, to
allow the read-only operation to complete using its old
data irrespective of the detected conÔ¨Çicts.
Figure 4(a) illustrates how three concurrent threads
operating on the singly-linked list in Figure 3 interact
with each other when synchronized through this scheme.
Here the three dependent modiÔ¨Åcations by threads (a)
and (b) are essentially sequentialized, while the read-only
operation by thread (c) is allowed to fully complete using
old data irrespective of the ongoing modiÔ¨Åcations.
c)RLU-only: where dynamic conÔ¨Çict detection is
used to synchronize each shared address of the abstraction
via the RLU scheme, while using the RCU objects only
to group related RLU logs and in turn to linearize these
groups. Figure 5 illustrate details of this scheme. Here
a new local array (declared at line 1) is declared to
737
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:39 UTC from IEEE Xplore.  Restrictions apply. 1std::vector < void *> addresses; /*all shared addresses referenced*/
2abstraction_state * oldp=0, *newp= allocate_state();
3while (true ){
4try {
5 oldp= load_current_state( MOD_RLU); oldp -> modlog_apply();
6 newp -> copy(oldp);
7 ... addresses.push_back (addr);
8 check_conflict( oldp ,addr ,* addr); /*is addr out -of-date?*/
9 ... addresses.push_back (addr);
10 newp -> new_modlog( addr ,oldv , newv); ...
11 if (newp -> modlog_empty()) break ;/*no modification*/
12 else if (state. compare_exchange_strong( oldp , newp))
13 { newp -> modlog_apply(); break ;}
14 else {
15 while (true ){
16 oldp= load_current_state( MOD_RLU); oldp -> modlog_apply();
17 newp -> copy(oldp);
18 for (std::vector < void *>::const_iter ator p = addresses
19 .begin(); p != addresses.end(); ++p)
20 {void * addr = *p;
21 check_conflict( oldp ,addr ,* addr); /*addr out -of-date?*/ }
22 if (state. compare_exchange_strong( oldp , newp))
23 { newp -> modlog_apply(); break ;}}
24 break ;}
25 }catch (const ModLogConflict& e){}
26 }
Fig. 5: Synchronizing via RLU only
accumulate all the shared addresses read/modiÔ¨Åed before
each address is checked for conÔ¨Çicts (lines 7-10). At the
end of the operation, if no modiÔ¨Åcation log has been
created (line 11), the operation has not modiÔ¨Åed any
shared data (hence is read-only) and can simply return.
If the operation has modiÔ¨Åcation logs to commit, it tries
to publicize the logs using the CAS at line 12. If the
CAS fails, the shared RCU atomic pointer must have
been modiÔ¨Åed by another thread. Here the most recent
RCU object is re-loaded, and all the shared addresses
that have been referenced are re-validated by invoking
check conÔ¨Çict before a new CAS is used to re-commit the
logs (lines 18-23). If the re-validation fails, an exception
is thrown by the conÔ¨Çict detection invocation; otherwise,
the operation returns when the next CAS succeeds.
Figure 4(b) illustrate how two concurrent threads mod-
ifying a singly-linked list interact with each other when
synchronized via RLU-only. Here in spite of two failed
C A Sa t t e m p t s ,b o t ht h ep o pi n v o c a t i o nb yt h r e a d( b )a n d
the second push invocation by thread (a) were allowed to
complete after re-validating the addresses they referenced.
So the independent modiÔ¨Åcations are allowed to move
forward concurrently.
d)Multi-RCU+RLU: where the shared data are
partitioned into multiple disjoint groups, each group inde-
pendently synchronized via the single-RCU+RLU scheme.
To use this scheme, each data item in the abstraction must
belong to exactly one of the groups, and each interface
function of the abstraction must access data from at most
a single group. Since there is no intersection among the
independently synchronized groups, no conÔ¨Çict can arise
from operations on diÔ¨Äerent groups of data.
e)Dynamic conÔ¨Çict detection: For each shared
address esynchronized via RLU logs, our dynamic conÔ¨Çict
detection code keeps track of the latest version of RLU logs
that have modiÔ¨Åed e, by recording this version number
inside a unique wrapper object allocated for e. Addition-
ally, a shared activity array is used to map each thread
id to a classiÔ¨Åcation of its ongoing activity (e.g., whether
its operation is read-only and whether it modiÔ¨Åes RCU-synchronized data), and another shared RCU- modlog ar-
r a yi su s e dt om a pt h ev e r s i o no fe a c hs h a r e dR C Uo b j e c t
to its RLU logs, so that given an arbitrary version number,
all the later committed RLU logs can be readily retrieved.
If a thread tinvokes conÔ¨Çict detection, the RCU object c
currently in use by tand the classiÔ¨Åcation of its ongoing
function fare Ô¨Årst retrieved. If ci st h es a m ea st h e
one held by the shared RCU atomic pointer, no conÔ¨Çict
exists; otherwise, the classiÔ¨Åcation of fis examined. If
fis a modiÔ¨Åcation operation synchronized via combined
RCU+RLU, an exception is thrown, causing fto abort
due to its obsolete RCU object. If fis a modiÔ¨Åcation
operation synchronized via RLU only, the latest RCU
version vthat has modiÔ¨Åed eis retrieved, and an exception
is thrown only if vis later than the version of c.I f f
is a read-only operation, it is allowed to Ô¨Ånish by all
means, by examining all the RLU logs committed later
than cto recover the original value for ebefore these
logs are carried out, and the recovered value is returned
tofto continue. Overall, each dynamic invocation of
conÔ¨Çict detection is fairly lightweight (involving one or
two memory operations) unless it is inside a read-only
operation, where the high cost is allowed so that read
operations can always complete.
f)Correctness Guarantee: We rely on existing
literature to argue for the correctness of the standalone
RCU and RLU synchronizations [23], [21], [22]. How we
combine them do not change how each fundamentally
works. In particular, for each group of data protected
by a single RCU struct (via the single-RCU or single-
RCU+RLU scheme), all concurrent modiÔ¨Åcations to this
group of data are linearized by the use of a single shared
RCU object, which can be modiÔ¨Åed only with a sequence
of successful atomic CAS operations. The linear sequence
ofdiÔ¨Äerent RCU objects saved as values of the shared
atomic pointer is therefore versioned exactly according to
thereal-time ordering of the successful CAS operations
(the linearization point of the modiÔ¨Åcations). The RLU
logged modiÔ¨Åcations are made part of this versioning
scheme, because the RLU logs inside each RCU object
are always carried out immediately after the RCU object
is committed, concurrently by each thread that observes
the publicized RCU object. To ensure all the committed
RLU logs are correctly observed, the version of each shared
memory address that has been modiÔ¨Åed is dynamically
traced, each shared memory access is checked for conÔ¨Çicts,
and its value is used (considered valid) only if the version
number of the value matches that of the RCU struct being
used by the accessing thread.
Our RLU-only synchronization uses dynamic conÔ¨Çict
detection to synchronize all internal data of an abstraction.
Here our strategy diÔ¨Äers from that in the literature [22]
only in that instead of relying on a real-time clock, we use a
shared RCU atomic pointer to group and linearize related
RLU logs, Similar to existing work, when concurrent
modiÔ¨Åcations are detected, re-validation of all the shared
738
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:39 UTC from IEEE Xplore.  Restrictions apply. data accsssed by the conÔ¨Çicting thread (including both
the read-set and the modiÔ¨Åcation-set) must be conducted
before its modiÔ¨Åcation is allowed to move forward.
Our multi-RCU+RLU scheme is used only when the
data of an abstraction can be partitioned into multiple
independently accessed groups, where each concurrent
operation can access (modify, or read, or both) at most a
single group. If two concurrent operations access distinct
groups of data, they do not need to be synchronized, since
no sharing is present; on the other hand, if they access
the same group of data, they are synchronized via the
single-RCU+RLU scheme for the involved group, where
the correctness is guaranteed by the single-RCU+RLU.
All synchronizations are guaranteed to be correct only
if they are implemented correctly. Our compiler-driven
approach is advantageous in this case because the same
implementation strategies are automatically applied to
all data structures. In our experimental evaluation, the
correctness of our compiler and its auto-generated code is
empirically validated by manually comparing the results
of running each workload sequentially vs concurrently and
then verifying the equivalence of the results.
III. Overall Strategies
Our compiler serves to automatically apply the synchro-
nization designs in Section II to an existing sequential data
abstraction, represented by a self-contained C++ class
(with or without template parameters), while guarantee-
ing correctness through conservativeness ‚Äî speciÔ¨Åcally if
the compiler cannot use program analysis to guarantee
correctness for any piece of the input code, the problem-
atic code piece is deemed ineligible for conversion, and
the unsafe operation moved to a private section of the
converted abstraction. Our compiler Ô¨Årst analyzes and pre-
classiÔ¨Åes all data references inside the abstraction to select
a most appropriate synchronization scheme (single-RCU,
single-RCU+RLU, RLU-only, or multi-RCU+RLU). It
then tailors the selected scheme to the underlying abstrac-
tion implementation through a set of systematic program
transformations. Custom lock-free garbage collection [24],
[25] and ABA prevention [26], synthesized from scratch,
are then inserted to ensure correctness of the Ô¨Ånal code.
The following provide details of these steps.
a)Pruning of unsafe operations: A concurrent
abstraction must not allow addresses of internal data to
escape to the outside, e.g., by being passed as external
function parameters or results, because the address can
become obsolete at any moment and cause memory cor-
ruption. Given an arbitrary abstraction x, all the internal
data, including all member variables of x,m u s tb em a d e
private so that they can be synchronized exclusively in
x.F u r t h e r , xmust be free of function calls that have
unknown side eÔ¨Äects or have their own internal synchro-
nizations. To guarantee safety, our compiler Ô¨Årst prunes all
such unsafe operations from the abstraction interface be-
fore proceeding to synchronize the remaining operations.For example, Figure 1 shows a subset of the pruned
interface functions of a sequential singly-linked list. Note
that while additional operations (e.g., pushfront, popback,
isfull) can be added to the interface, some operations,
e.g., those inserting or erasing from an arbitrary location in
the list, must be eliminated from the concurrent interface
because they allow internal addresses to be passed from
or returned to the outside. Most C++ data abstractions
in principle can be at least partially converted by our
compiler, by excluding public functions that are unsafe.
b)ClassiÔ¨Åcation and partitioning of data: Be-
fore determining how to synchronize an abstraction, our
compiler needs to classify all data references of the ab-
straction to be synchronized via RCU or RLU. To this end,
it classiÔ¨Åes RCU-synchronized data to include all variable
references that are never aliased (that is, the data either
reside in a unique variable or can be reached only through
a unique path of pointers, starting from a unique vari-
able). All the shared data references that can be aliased
(that is, the data can be potentially reached through
multiple paths of pointer referencing) are classiÔ¨Åed to be
synchronized via RLU. For each data reference classiÔ¨Åed as
RCU-synchronized, a unique pointer chasing path, starting
from a unique member variable of the abstraction, is
constructed, so that all data on the reference path can be
copied if needed in a straightforward fashion. For each data
reference classiÔ¨Åed as RLU-synchronized, a set of member
variables of the abstraction are similarly identiÔ¨Åed, as the
only places through which these data can be reached.
If all data referenced in an abstraction are reached
only through member variables of the abstraction, these
member variables can be potentially partitioned into dis-
joint groups that can be synchronized independently. For
example, a hash map may contain many buckets that are
mapped to distinct hash keys, and operations that modify
or read distinct buckets are fully independent of each
other, so they can be independently synchronized if each
function accesses at most a single hash key. On the other
hand, in Figure 1, although the head and tailvariables
are often independently accessed, they must be cohesively
updated when the list has ‚â§1 items, so they cannot be
partitioned into independently synchronized groups.
To partition member variables of an abstraction, in-
cluding those that have array types, into independently
synchronized groups, our compiler analyzes each array
variable to determine whether all the data referenced by
each interface function can be reached from at most a
single array entry, so that distinct entries can be indepen-
dently synchronized. The rest of the member variables of
the abstraction are then partitioned similarly, so that the
data referenced by each interface function can be reached
by at most a single group of variables.
c)Selection of synchronization schemes:
Among the four synchronization schemes we support,
the single-RCU+RLU scheme is the most general as it
can always be used to correctly synchronize an arbitrary
739
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:39 UTC from IEEE Xplore.  Restrictions apply. abstraction. It is therefore used as the default scheme
selected by our compiler at the beginning before any
further analysis is performed. It is then simpliÔ¨Åed into
the single-RCU scheme if the compiler discovers that all
data references of the abstraction are classiÔ¨Åed to be RCU-
synchronized. On the other hand, it is changed into the
RLU-only scheme if the compiler discovers all references
are classiÔ¨Åed to be RLU-synchronized. Finally, as long
as RLU-only is not selected, the compiler proceed to try
partition the internal data of the abstraction into distinct
groups, so that diÔ¨Äerent groups can be safely synchronized
independently via the multi-RCU+RLU.
d)Classifying and synchronizing each function:
After analyzing the internal data references of an abstrac-
tion and then selecting an overall synchronization scheme,
the selected scheme is implemented by modifying the
source code of each interface function f. To this end, our
compiler Ô¨Årst collects all the memory references modiÔ¨Åed
and read by f.I f fdoes not access shared data, no syn-
chronization is needed. Otherwise, a single RCU struct and
its associated atomic pointer are identiÔ¨Åed, by searching
through all the data referenced by fand matching them
to a single synchronization group. Additional details are
then synthesized by classifying finto four types:
‚Ä¢READ RCU: if fdoesn‚Äôt modify anything and reads
only RCU-synchronized data, it is synchronized by
following the is empty method in Figure 3;
‚Ä¢READ RLU: if freads both RCU and RLU synchro-
nized data, without modifying anything, it is synchro-
nized by following the lookup function in Figure 3;
‚Ä¢MOD RLU: if fmodiÔ¨Åes RLU-synchronized data,
and RLU-only is selected as the overall scheme, fis
synchronized via RLU logs only, by following Figure 5.
‚Ä¢MOD RCU: if fmodiÔ¨Åes shared data, and the over-
all scheme is not RLU-only, it is synchronized by
following the pushback method in Figure 3.
e)ABA prevention, lock-free Garbage Collec-
tion, and grace periods: Our compiler uses weak CAS
updates to physically apply all RLU logs. These updates
are guaranteed to be correct only if each log uses a new
unique value to replace an old unique one. To address
the ABA problem [26], where multiple CAS updates set
a value Ô¨Årst to A,t h e nt o B, and then back to A, while
violating real-time ordering of the updates, when logging
RLU synchronized modiÔ¨Åcations, our compiler creates a
new uniquely addressed wrapper object to hold each new
value. These wrapper objects are eventually garbage col-
lected together with their containing RLU logs.
To avoid allocating too many small objects (which is
prohibitive at runtime), for each abstraction, our compiler-
generated code pre-allocates a large memory pool shared
by all threads to hold all the RCU objects and RLU logs,
and two thread-local pools inside each RCU object to hold
its aÔ¨Éliated ABA wrappers and user-freed data. Each
pointer-free instruction in the original code is replaced
with a special instruction that saves the freed pointer inthe thread-local pool of the RCU-object, which will be
garbage collected together with the RCU object.
Our compiler automatically synthesizes our custom
lock-free garbage collector, which is triggered only when
memory is exhausted, to manage these memory pools
and reclaim unreachable memory. To safely reclaim RCU
objects, a shared activity array is maintained to map each
thread idto the address of the shared RCU object it
currently uses, so that if any thread runs out of memory,
it can simply reclaim a RCU object whose address is no
longer present in the activity array. The RLU logs that
are contained inside the reclaimed RCU object can be
reclaimed if no older RCU object is still active, i.e., no
active function can conÔ¨Çict with addresses modiÔ¨Åed by
these logs; otherwise, these RLU logs are saved together
with their RCU version number until all the older RCU
objects have become obsolete and inactive.
As contention is a major source of performance degra-
dation when a large number of threads concurrently op-
erate, our compiler optionally make each thread sleep a
designated amount of time before retrying upon detecting
a conÔ¨Çict. This grace period is triggered inside the check-
conÔ¨Çict function, invoked at lines 25 of Figure 3, after
a conÔ¨Çict is detected but before an exception is thrown.
Currently our compiler simply uses a constant, made re-
conÔ¨Ågurable at compile-time in the generated code via
a cpp macro, to conÔ¨Ågure how long each thread will
sleep before throwing an exception. In our experimental
evaluation, we empirically tried diÔ¨Äerent values for this
macro and then selected a duration for the grace-period
that provides the best overall scalability for each work-
load. This approach is likely to be inferior to a more
sophisticated strategy that makes each thread dynamically
adjust. However, it is extremely easy for a compiler-driven
approach to implement and works reasonably well in
our experimental evaluation. Exploring more sophisticated
grace-period settings belongs to our future work.
IV. Compiler Automation
The compiler solutions in this paper are developed by
adapting standard techniques (e.g., pointer aliasing, which
determines whether multiple pointer variables may refer to
the same object) to support automatic synchronization of
concurrent data abstractions. The main technical novelty
lies in formulating the necessary solutions to solve the new
problems at hand, speciÔ¨Åcally to synchronize an arbitrary
data abstraction x. The overall solution includes:
‚Ä¢Structural analysis and normalization , which identi-
Ô¨Åes key components of x, speciÔ¨Åcally member vari-
ables and public methods that do not contain unsafe
operations, to synchronize. Then, it normalizes xto
be processed by later stages, e.g., by moving unsafe
methods to private sections of the C++ class, inlining
base classes, and inlining function calls inside the
public methods, so that these public methods serve
as a closed set of concurrent operations on x.
740
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:39 UTC from IEEE Xplore.  Restrictions apply. ‚Ä¢Pointer and data Ô¨Çow analysis , including object con-
nectivity analysis, function side-eÔ¨Äect analysis, and
reaching deÔ¨Ånition analysis, to determine aliasing
relations among internal data of x,s i d ee Ô¨Ä e c t so f
each method, and data-Ô¨Çow relations among memory
references inside all member methods of x.
‚Ä¢Data relocation , which re-organizes the data of x
after using the object-connectivity analysis to classify
each memory reference inside xto be synchronized
either by RCU or RLU. RCU-synchronized data are
then partitioned into independent groups, with a
new struct type and a new atomic pointer variable,
illustrated in Figure 2, deÔ¨Åned to relocate each group
of data. All references to these relocated data are then
redirected through their new atomic pointers.
‚Ä¢Synchronization , which uses the results of side eÔ¨Äect
and reaching deÔ¨Ånition analysis to classify and aug-
ment each interface function fofxfor synchroniza-
tion. First, fis restructured after selecting the syn-
chronization template (READ RCU, READ RLU,
MOD RCU, or MOD RLU) best suited for its
purpose, based on results of its function side-eÔ¨Äect
analysis. Then using the result of data relocation
analysis, memory references inside fare modiÔ¨Åed,
e.g., to use the proper RCU copy or to create the
proper RLU logs. Next, reaching deÔ¨Ånition analysis
is used to relocate uses of these locally modiÔ¨Åed data,
before inserting Ô¨Ånal augmentations, e.g., for ABA
prevention and garbage collection.
The correctness of the above steps is guaranteed by the
conservativeness of the analysis algorithms they use ‚Äî
if these algorithms cannot suÔ¨Éciently understand some
piece of code, the most conservative assumption is made
to guarantee correctness. The scope of each analysis is
Ô¨Çow-sensitive and intra-procedural (global but within a
single function) [27]. In particular, each algorithm tries to
analyze each individual interface function of xin isolation
while assuming arbitrary values for non-local variables
(the most conservative assumption). It then simply com-
bines the results from analyzing all functions to represent
all possible situations for x. This approach ensures the cost
of all analysis algorithms is manageable (not overly steep)
and is found to be suÔ¨Écient for our purpose. The following
text outlines the main analysis algorithms.
a)Object Connectivity Analysis: Detailed in
Algorithm 1, this analysis aims to model the objects
created and connected by each interface function of x.
The algorithm follows the standard structure of data-Ô¨Çow
analysis by Ô¨Årst constructing a control Ô¨Çow graph (cfg)
for the input at line 2. Each cfg node is initially associated
with an empty set except the entry node, which is assigned
with a graph given as a parameter to the algorithm.
The connectivity graphs associated with each cfg node
bare then iteratively modiÔ¨Åed, by collecting the results
of using statements in bto modify connectivity graphs of
all predecessors of b, through the points tomodiÔ¨ÅcationAlgorithm 1: Object connectivity analysis
1F unction analyze connectivity( g0: initial graph, input: a single
interface function)
2 cfg = build control Ô¨Çow graph(input);
3 foreach basic block b‚ààcf g dopt[ b]=‚àÖ;
4 pt[entry node(cfg)] = g0; change=true;
5 while change == true do
6 change = false;
7 foreach basic block b in cfg do
8 g1=p t [ b]; pt[ b]=‚àÖ/* recompute pt[ b]*/;
9 foreach predecessor pof bin cfg do
10 pt[ b]=p t [ b]‚à™{points tomodiÔ¨Åcation( b, pt[t])};
11 ifpt[ b]/negationslash=g1then change = true;
12 res = ‚àÖ; // collect graphs at the exit of function
13 foreach basic block b‚ààcf g that has no successor do
14 res = res ‚à™pt[ b];
15 return res;
e=new Node<T>(o);
++count;
if (tail == 0)
head=tail=e;tail->next=e;
tail=e;
return;n1
n2
n3head
tailhead
tail
tailehead
nexte
Unknown nodePointern1
n2
n1
n2n3
n1
n2 n3tail
n3tailehead
nexten1
n2n1
n2 n3tailb0
b1
b2
b4b0 b1
b2 b3
b4
n
nUnique nodeb3e
headhead
Fig. 6: Example connectivity graphs
function at line 10. The algorithm terminates when the
results for all cfg nodes no longer change. Due to space
constraints, we omit details of the points tomodiÔ¨Åcation
function, which essentially modiÔ¨Åes a given set of connec-
tivity graphs based on the semantics of each statement
that modiÔ¨Åes any pointer variables. The points-to analysis
is Ô¨Åeld sensitive (traces the addresses of diÔ¨Äerent member
variables of an object) but not array index sensitive (does
not distinguish diÔ¨Äerent subscripts of array references).
Figure 6 shows the resulting connectivity graphs from
using this algorithm to analyze the pushback function
at lines 3-6 of Figure 1(a). Here the entry node b0is
associated with the initial connectivity graph, which in-
cludes two unknown objects, n1a n d n2, pointed to by
the head and tail member variables respectively. The
compiler knows nothing about n1a n d n2, so they can
be both null or aliased to each other. Node b1h a s b0a s
a single predecessor and contains a single pointer-related
operation, e=new Node <T>(o). Its connectivity graph
therefore extends g0with a new unique node n3, pointed
to by e.N o d e b2a n d b3b o t hh a v e b1 as predecessor
but modify the pointers diÔ¨Äerently. They therefore each
have their own connectivity graphs. These graphs are
then collected together at node b4, which has both b2
and b3 as predecessors. The connectivity graphs of b4
would then be returned by the algorithm. In the end, the
connectivity graphs from all interface functions represent
all the possible ways diÔ¨Äerent objects can be connected.
Since the algorithm terminates when the set of con-
741
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:39 UTC from IEEE Xplore.  Restrictions apply. Algorithm 2: Relocation Analysis
1F unction data analysis(c: input, ocg: object connect graph)
2cpvars=unaliased member variables(c, ocg);
3partition = {cpvars };foreach interface function fof cdo
4 ref vars = trace tovariables(side eÔ¨Äect analysis( f));
5 p1= ref vars ‚à©cp vars ;ifp1= = cp vars then
6 partition = {cpvars };b r e a k ;
7 p2 = p1; foreach p3‚ààpartition do
8 ifp1‚äÜp3then
9 break;
10 ifp1‚à©p3= = ‚àÖthen
11 continue;
12 p2 = p1‚à™p3; partition = partition - {p3};
13 ifp2= = cp vars then
14 partition = {cpvars };b r e a k ;
15 ifp2‚äÉp1(p2 subsumes p1 and other members) then
16 partition = partition ‚à™{p2};
17 log vars= ‚àÖ;
18 foreach unknown node xin ocg do
19 log vars = log vars ‚à™memory references of(x);
20 foreach unique node xreachable from cp vars in ocg do
21 ifx is never modiÔ¨Åed in c then continue;
22 else
23 p= ‚àÖ;
24 foreach g‚ààocg do
25 p=p ‚à™summarize paths( g,cp vars, x);
26 ifphas only one entry from member variable ethen
27 cpvars = cp vars ‚à™{references of( x)‚Üí e};
28 else log vars = log vars ‚à™{references of( x)};
nectivity graphs assigned to each cfg node no longer
changes, the termination of the algorithm is guaranteed
if the overall number of diÔ¨Äerent connectivity graphs is
bounded by a constant, as each iteration of the algo-
rithm can only add new connectivity graphs to the result
already computed by previous iterations. To guarantee
termination, our algorithm allocates at most one object
for each memory reference. The number of nodes in each
connectivity graph is thus bounded by the number of
memory references analyzed (R), and the number of edges
byR2. Since all graphs have the same nodes, the number
of diÔ¨Äerent graphs is bounded by a constant.
b) Side EÔ¨Äect and Reaching DeÔ¨Ånition Analysis:
which respectively identiÔ¨Åes the memory references read
and modiÔ¨Åed by each interface function of xand discovers
the set of data modiÔ¨Åcations that can reach each memory
reference. Both use standard program analysis algorithms
available in most compiler books [27]. Our main extension
is that when considering modiÔ¨Åcations to indirectly ref-
erenced objects, the object connectivity graphs are used
to help resolve pointer aliasing issues. In particular, each
memory reference is mapped to a node in the connectivity
graphs, and if the node is tagged as unknown ,i tm a yb e
aliased with all the other unknown nodes. No distinction
is made between array variables vs pointer variables ‚Äî
each of them is by default assumed to refer to an arbitrary
memory region. All array references are simply treated as
arbitrary indices into the array memory regions,
c) Data Relocation Analysis: Algorithm 2 shows our
algorithm for classifying and partitioning the internal data
of an abstraction x. It Ô¨Årst Ô¨Ånds all member variables(including array/pointer variables) whose addresses are
never taken to be later relocated to a RCU-synchronized
struct type (cp vars at line 2). Then, the set of member
variables accessed by each interface function fis collected
(lines 4-5) and used to generate a partitioning of cp vars,
with variables in diÔ¨Äerent partitions never accessed to-
gether inside any function (lines 3-16). Our algorithm
supports partitioning of arrays by allowing each distinct
entry of an array to be an independently synchronized
group, if it can be veriÔ¨Åed that each interface function
of the abstraction uses array subscripting to access only
a single entry of the array/pointer variable. Finally, for
each memory reference mapped to a unique node in the
connectivity graphs, the algorithm examines the number
of paths reaching the object from the non-aliased member
variables and categorizes the object as non-aliased (RCU
data) only if it is reachable through only a single path
from some variable in cp vars (lines 20-28). For example,
in Figure 6, the n3 unique node can be reached either
through the head or the tail member variables of the list, so
n3 cannot be relocated. All memory references that do not
belong to cp vars are simply classiÔ¨Åed to be synchronized
by RLU and saved in log vars (lines 17-19 and 28).
V. Experimental Evaluation
We have implemented a prototype compiler using the
POET language [28] and have used our compiler to au-
tomatically convert eight sequential C++ classes, shown
in the second column of Table I. These C++ classes are
collected from two existing C/C++ open-source projects:
the ROSE compiler [29] and the Tervel framework [30].
By default, our compiler has used single-RCU+RLU to
synchronize the Ô¨Årst Ô¨Åve abstractions in Table I, multi-
RCU+RLU to synchronize the hash-set abstraction, and
RLU-only to synchronize the binary tree (BST) and Multi-
dimensional list (MDlist). To additionally compare the
multi-RCU+RLU and RLU-only synchronization schemes,
we have slightly modiÔ¨Åed the top-level data organization
of the original sequential implementations of BST and
MDlist, by using an array to store their top-level nodes
to enhance concurrency among operations that access
diÔ¨Äerent portions of the linked data structures. The array-
based and list-based implementations are identical except
diÔ¨Äerences in a few lines, including the top-level declara-
tion of the abstraction member variable, and the beginning
of each operation that accesses the top-level variable. To
reduce contention, each auto-generated version uses a Cpp
macro to set how long each thread should sleep before
throwing an exception to restart its operation.
Our compiler is general purpose in that all of its algo-
rithms take an arbitrary C/C++ class as input, with no
inherit assumption about speciÔ¨Åcs of the underlying data
structure. Our prototype implementation is limited only
by idiosyncrasies of the C++ language and completeness
of the internal analysis algorithms in handling all such
idiosyncrasies. Out of the eight C++ classes in Table I, our
742
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:39 UTC from IEEE Xplore.  Restrictions apply. Workload Auto-generateRSTM
[31]TBB
[32]FC[33]&
SIM[34]Boost[35]
Back-oÔ¨ÄCDS
[36]
Lightweight
write-onlyRingbuÔ¨Äer
Stack(array-based)
Deque(list-based)
Singly/Doubly linked listQueueQueue
StackMichael-Scott
queue[6]
Treiber stack[37]
Heavyweight
write-only&
mostly-readHashSet(array of lists)
Binary tree(unbalanced)
Multi-dimensional list[38]Hash-
map-Michael‚Äôs
hashset[9]
Herlihy‚Äôs
skiplist[2]
Ellen‚Äôs
BST[12]
TABLE I: Benchmark and workload conÔ¨Ågurations
compiler was able to successfully synchronize 40-100% of
their original interface functions, while pruning the others.
Our results indicate that the performance of our auto-
synchronized code is competitive to that of manually
written implementations by experts. One potential threat
to the validity of our conclusion is that there might be
better manual data structure implementations that we did
not compare with. Similarly a new hardware architecture
might favor a diÔ¨Äerent set of synchronization techniques
than those we support. An expert can certainly Ô¨Åne tune
an existing implementation to far exceed the performance
of our auto-generated code. However, our purpose of
investigating a compiler-driven approach is exactly to
automate such manual state-of-practice so that an expert
does not need to repeatedly apply his/her expertise to
implement diÔ¨Äerent data structures. The generality of
our approach makes advanced lock-free synchronization
techniques much more readily available to average users.
This paper is only a Ô¨Årst step towards this direction.
The last four columns of Table I show a set of manu-
ally synchronized concurrent data structures we can Ô¨Ånd
for comparison, each crafted from scratch by expert re-
searchers in the Ô¨Åeld. To compare with STM, for each
of the eight sequential data abstractions in Table I, we
also manually synchronized them via RSTM [31], one of
the fastest obstruction-free STM implementations. Exper-
imentation is used to Ô¨Ånd the best conÔ¨Åguration param-
eters for each implementation (including the length of
hash keys in all the hast-set implementations, and back-
oÔ¨Ä/sleep time in michael-scott queue, treiber stack, and
our auto-generated code) when using diÔ¨Äerent numbers
of threads. We manually implemented a set of micro-
benchmarks to test all data structures in Table I using
three workloads: light-weight write-only , which tests ab-
stractions in the Ô¨Årst row, by treating each as a concurrent
queue/stack and concurrently invoking two operations, a
push and a pop, each with 50% probability; heavy-weight
write-only , which tests the heavier weight abstractions
in the second row, by treating each as a set/map and
concurrently modifying it via two operations, an insert
and an erase, each with 50% probability; and heavy-weight
mostly-read , which alternatively tests the heavy-weight ab-
stractions by reducing modiÔ¨Åcations to 10% while adding
90% probability of a read-only operation that searches
through the data. To make sure each data structure always
contains enough elements for meaningful operations, and
the test runs a suÔ¨Éciently long time for timing stability,
our benchmark Ô¨Årst initializes each data structure with alarge number (set to be 2 .56‚àó106)o fe l e m e n t sa n dt h e n
spawns a pre-conÔ¨Ågured number of threads to collectively
invoke a pre-determined number (again set to 2 .56‚àó106)
of the pre-selected operations. This conÔ¨Åguration poten-
tially oÔ¨Äers more concurrency among operations in the
light-weight write-only workload and therefore magniÔ¨Åes
the importance of exploring such concurrency, which is
currently not explored (a weakness) in our synchronization
schemes. All workloads are measured by their throughput,
computed by dividing the number of operations completed
with the average time taken to complete the operations.
We evaluated all the tests on two platforms: a 64-core
Intel Xeon Phi 7210 processor, with 4 hyper threads on
each core; and an SMP node with two Intel Xeon E5-
2695 v3 processors, each with 14 cores, from one of the
Xsede servers [39]. Both machines run CentOS Linux as
the underlying operating system. All data structure im-
plementations are compiled using g++ 4.8.5 with the -O3
Ô¨Çag and c++11 enabled. Each measurement is repeated 10
times, and the averages reported. Variations across runs of
the same measurement are under 10%.
A. Performance of RCU Synchronized Implementations
Figure 7 compares the best performance of our auto-
synchronized data structures, speciÔ¨Åcally those via single-
and multi-RCU+RLU schemes, with that of manually
synchronized implementations. To improve readability, we
grouped the implementations so that related ones are
displayed using the same line style. The members in each
group are listed in decreasing order of their performance.
The diÔ¨Äerent groups of data structures all also listed in
decreasing order of their collective performance.
Overall, the performance of our auto-generated RCU-
synchronized implementations are among the best across
all workloads, ranking Ô¨Årst in the heavy-weight (hash-
set/MDlist/BST) write-only workload and second in the
other two workloads, based on the best throughput that
was eventually attained when using 1-256 threads for the
workloads. The main observation is that the multi-RCU
synchronization (used by our compiler in the heavy-weight
workloads) produces superb scalability by allowing a high-
degree of concurrency, as no interaction is required among
fully independent operations. In the case of the hashset,
each RCU object synchronizes all updates to the same
hash key together. The auto-synchronized MDlist and
BST lag behind the hashset, because only a constant num-
ber of RCU objects are used at the top level, in contrast
to the much larger array of atomic pointers used in the
hashset. The Single-RCU strategy is used by our compiler
to synchronize the Ô¨Åve light-weight data structures. Here
because it sequentializes all the update operations, it lags
behind the manual implementations when using a small
number of threads. However, it scales better when the
number of threads exceeds 32, mostly because the tuning
of sleep-time reduces contention when there is no concur-
rency among the operations. Flat combining performed
743
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:39 UTC from IEEE Xplore.  Restrictions apply. 1.02.04.08.016.0
 1  2  4  8  16  32  64  128  256number of physical cores
Number of threads0.51.02.04.08.016.0Light-weight write only
Flat-combining/SIM Stack/Queue
Auto-gen Slist/Dlist/Stack/Ringbuffer/Deque
Back-off TreiberStack/MSQueue
TBB queue;Boost/CDS queue/stack
0.20.51.02.04.08.016.032.064.0
 1  2  4  8  16  32  64  128  256number of physical cores
Number of threads0.10.21.04.016.064.0Heavy-weight write only
Auto-gen array-based HashSet/MDlist/BST
TBB Hashmap
CDS HashSet/Skiplist/BST
0.20.51.02.04.08.016.032.064.0
 1  2  4  8  16  32  64  128  256number of physical cores
Number of threads0.10.21.04.016.064.0Heavy-weight mostly read
TBB Hashmap
Auto-gen array-based HashSet/MDlist/BST
CDS HashSet/Skiplist/BST
* Top graphs: on the Intel Xeon Phi server; Bottom graphs: On the Xsede server
Fig. 7: Throughput (million ops/second) of RCU-synchronized and manually synchronized implementations
the best in the light-weight workload because it combines
multiple updates to reduce overhead.
B. Comparing With STM
Figure 8 compares all variations of our auto-generated
code, including those synchronized via single-RCU+RLU
with and without tuning of sleep-time-before-retry, those
synchronized via multi-RCU+RLU (hashset, MDlist, and
BST), and those synchronized via RLU-only (MDlist and
BST), with the corresponding implementations synchro-
nized via STM. Here signiÔ¨Åcant performance improvement
is observed by the tuning of sleep-time for the light-
weight workload and by using the array-based sequential
implementation for MDlist and BST. Note that even
without any manual intervention, our compiler synchro-
nized implementations performed much better than the
RSTM implementations, which performed competitively
only under the heavy-weight mostly-read workload. For
the write-only workloads, the STM versions performed
badly when the number of threads exceeds 16 and conÔ¨Çicts
among operations become high. In the heavy-weight write-
only workload, the RSTM implementations reached their
max throughput when the number of threads is only 4,
due to heavier modiÔ¨Åcations to the shared addresses. For
the lightweight workload, the RSTM versions performed
reasonably only when the number of threads are ‚â§4
and demonstrated a worst level performance degradation
among all implementations afterwards.
C. Estimating Synchronization Overhead
Figure 9 shows the run-time overhead introduced by our
auto-inserted synchronizations for the various concurrent
data structures, estimated using equation ( Tp‚àóP‚àíTs)/T s,where Tpis the elapsed time of using Pthreads to concur-
rently complete a workload, and Tsis the time required
when using their original sequential implementations. Here
because we plot the cumulative overhead of all the threads,
the overhead can scale linearly as the number of threads
increases, as we observe for the light-weight write-only
workload, where the single-RCU+RLU scheme virtually
sequentialized their concurrent modiÔ¨Åcations. However, for
the heavy-weight write-only and mostly-read workloads,
the overhead stayed relatively low and constant irrespec-
tive of the increasing number of threads, until there are
more threads than the number of CPU cores.
VI. Related Work
In contrast to existing work on the manual design of
non-blocking concurrent data structures, e.g., queues [6],
[7], lists [8], [9], maps [9], [10], and trees [11], [12], [13], [14],
this paper represents the Ô¨Årst attempt at using compiler
technology to automate the process. Note that existing
compilers that support transactional memory program-
ming merely translate the higher-level STM programming
interface down to lower-level library invocations, without
involving any compile-time analysis to tailor synchroniza-
tions to the characteristics of diÔ¨Äerent pieces of code.
Michael and Scott [5] studied the performance of non-
blocking algorithms vs locking and observed that eÔ¨É-
cient data-structure-speciÔ¨Åc non-blocking algorithms out-
perform the other alternatives. This paper automatically
tailors general techniques to the needs of individual data
structures and has demonstrated similar advantages.
The synchronization adopted by our compiler is a
combination [23] of read-copy-update [21] and read-log-
update [22]. Our retrieval of older values through saved
RLU-logs for read-only functions is similar in ideas to
744
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:39 UTC from IEEE Xplore.  Restrictions apply. 1.02.04.08.0
 1  2  4  8  16  32  64  128  256
Number of threads0.51.02.04.0Light-weight write only
with tuned sleep time
with no sleep time
RSTM
0.20.51.02.04.08.016.032.064.0
 1  2  4  8  16  32  64  128  256
Number of threads0.10.21.04.016.064.0Heavy-weight write only
Auto-gen array-based HashSet/MDlist/BST
Auto-gen list-based MDlist/BST
RSTM HashSet/MDlist/BST
0.51.02.04.08.016.032.064.0
 1  2  4  8  16  32  64  128  256
Number of threads0.10.21.04.016.064.0Heavy-weight mostly read
Auto-gen array-based HashSet/MDlist/BST
Auto-gen list-based MDlist/BST
RSTM HashSet/MDlist/BST
* Top graphs: on the Intel Xeon Phi server; Bottom graphs: On the Xsede server
Fig. 8: Comparing Throughput (million ops/second) with STM-synchronized abstractions
 0.25 1 4 16 64 256
 1  2  4  8  16  32  64  128  256
Number of threads 0.25 1 4 16 64 256Estimation of synchronization overhead
Light-weight write only
Heavy-weight write only
Heavy-weight mostly read
* Top: Intel Xeon Phi; Bottom: Xsede
Fig. 9: Synchronization overhead of auto-generated code
the multi-versioning extension of RLU [40]. Single-word
compare and swap (CAS) has been widely used as a
primitive for implementing lock-free or wait-free synchro-
nizations [41], [23]. Our compiler-driven approach can be
potentially used to automate other advanced synchroniza-
tion mechanisms as well, e.g., Ô¨Åne-grained locking [14], Ô¨Çat
combining [33], among others [42], [43], [44], [45], [46].
Herlihy and Moss [47] Ô¨Årst proposed transactional mem-
ory as a hardware architecture, the materialization of
which includes Intel TSX [48] and AMD ASF [49]. Our
auto-generated code can be converted to using hardware-
level transactional operations if needed. However, since we
focus on software-level synchronization, our experimental
study uses conventional hardware architectures.
Software transactional memory was Ô¨Årst proposed by
Shavit and Touitou [50] and was later extended to sup-port dynamically sized data structures [16], conditional
critical regions [51], transactional monitors [52], compo-
sition of blocking transactions [17], among others [53].
Modern STM systems have been implemented both by
using lock-based [54], [55] and non-blocking synchroniza-
tions [51], [16], [18], [11], Steep runtime cost is required
when implementing STM [11], [18], [56]. Many optimiza-
tions, e.g., obstruction-free synchronization [16], transac-
tion logging [17], Ô¨Åne-grained object disambiguation [51],
have been developed to reduce such overhead. Our work
similarly follows this direction.
Our work is complementary to automated refactoring
of existing code for concurrency via libraries [57] and
automated Ô¨Åxing of concurrency bugs [58], [59], [60]. The
sketch synthesis algorithm [61] tries to iteratively complete
the sketch of a concurrent data structure from developers
until reaching a given criteria. Vechev et al [62], [63] auto-
matically inferred synchronizations to avoid interleavings
that violate user speciÔ¨Åcations. Our compiler requires only
the sequential implementation of C++ classes and aims to
automate non-blocking synchronization.
VII. Conclusion
This paper presents compiler techniques to automati-
cally convert sequential data abstractions into concurrent
lock-free ones, by adapting existing state-of-the-practice
synchronization mechanisms to maximize concurrency.
We present experimental results to show that our auto-
generated implementations can attain performance that is
competitive to manually crafted ones by experts.
Acknowledgement
This research is funded by NSF through grants CCF-
1261584, CCF-1421443, CCF-1717515, and OAC-1740095.
745
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:39 UTC from IEEE Xplore.  Restrictions apply. References
[1] E. W. Dijkstra, ‚ÄúSolution of a problem in concurrent program-
ming control,‚Äù Communications of the ACM , vol. 8, p. 569, Sep.
1965.
[2] M. Herlihy and N. Shavit, The Art of Multiprocessor Program-
ming . San Francisco, CA, USA: Morgan Kaufmann Publishers
Inc., 2008.
[3] A. Karlin, K. Li, M. Manasse, and S. Owicki, ‚ÄúEmpirical studies
of competitve spinning for a shared-memory multiprocessor,‚Äù
Proceedings of the 13th ACM Symposium on Operating Systems
Principles , pp. 41‚Äì55, Oct. 1991.
[4] J. M. Mellor-Crummey and M. L. Scott, ‚ÄúAlgorithms for scal-
able synchronization on shared-memory multiprocessors,‚Äù ACM
Transactions on Computer Systems , vol. 9, pp. 21‚Äì65, Feb. 1991.
[5] M. M. Michael and M. L. Scott, ‚ÄúNonblocking algorithms and
preemption-safe locking on multiprogrammed shared memory
multiprocessors,‚Äù J. Parallel Distrib. Comput. , vol. 51, pp. 1‚Äì
26, May 1998.
[6] ‚Äî‚Äî, ‚ÄúSimple, fast, and practical non-blocking and blocking
concurrent queue algorithms,‚Äù in Proceedings of the Fifteenth
Annual ACM Symposium on Principles of Distributed Comput-
ing, ser. PODC ‚Äô96. New York, NY, USA: ACM, 1996, pp.
267‚Äì275.
[7] M. Herlihy, V. Luchangco, and M. Moir, ‚ÄúObstruction-free
synchronization: Double-ended queues as an example,‚Äù in Pro-
ceedings of the 23rd International Conference on Distributed
Computing Systems , ser. ICDCS ‚Äô03. Washington, DC, USA:
IEEE Computer Society, 2003, pp. 522‚Äì.
[ 8 ]V .L .M .M .W .N .S .I .S t e v eH e l l e r ,M a u r i c eH e r l i h ya n d
N. Shavit, ‚ÄúA lazy concurrent list-based set algorithm,‚Äù in
Proceedings of the 9th International Conference On Principles
Of Distributed Systems . Berlin, Heidelberg: Springer-Verlag,
2005, pp. 3‚Äì16.
[9] M. M. Michael, ‚ÄúSafe memory reclamation for dynamic lock-
free objects using atomic reads and writes,‚Äù in Proceedings of
the Twenty-Ô¨Årst Annual Symposium on Principles of Distributed
Computing , ser. PODC ‚Äô02. New York, NY, USA: ACM, 2002,
pp. 21‚Äì30.
[10] N. G. Bronson, J. Casper, H. ChaÔ¨Å, and K. Olukotun, ‚ÄúTransac-
tional predication: High-performance concurrent sets and maps
for stm,‚Äù in Proceedings of the 29th ACM SIGACT-SIGOPS
Symposium on Principles of Distributed Computing ,s e r .P O D C
‚Äô10. New York, NY, USA: ACM, 2010, pp. 6‚Äì15.
[11] K. Fraser and T. Harris, ‚ÄúConcurrent programming without
locks,‚Äù ACM Trans. Comput. Syst. , vol. 25, no. 2, May 2007.
[12] F. Ellen, P. Fatourou, E. Ruppert, and F. van Breugel, ‚ÄúNon-
blocking binary search trees,‚Äù in Proceedings of the 29th ACM
SIGACT-SIGOPS Symposium on Principles of Distributed
Computing , ser. PODC ‚Äô10. New York, NY, USA: ACM, 2010,
pp. 131‚Äì140.
[13] T. Crain, V. Gramoli, and M. Raynal, ‚ÄúA contention-friendly
binary search tree,‚Äù in Euro-Par 2013 Parallel Processing ,
F. Wolf, B. Mohr, and D. an Mey, Eds. Berlin, Heidelberg:
Springer Berlin Heidelberg, 2013, pp. 229‚Äì240.
[14] M. Arbel and H. Attiya, ‚ÄúConcurrent updates with rcu: Search
tree as an example,‚Äù in Proceedings of the 2014 ACM Symposium
on Principles of Distributed Computing , ser. PODC ‚Äô14. New
York, NY, USA: ACM, 2014, pp. 196‚Äì205.
[15] N. Shavit and D. Touitou, ‚ÄúSoftware transactional memory,‚Äù
Distributed Computing , vol. 10, pp. 99‚Äì116, 1997.
[16] M. Herlihy, V. Luchangco, M. Moir, and W. N. Scherer, III,
‚ÄúSoftware transactional memory for dynamic-sized data struc-
tures,‚Äù in Proceedings of the Twenty-second Annual Symposium
on Principles of Distributed Computing , ser. PODC ‚Äô03. New
York, NY, USA: ACM, 2003, pp. 92‚Äì101.
[17] T. Harris, S. Marlow, S. Peyton-Jones, and M. Herlihy, ‚ÄúCom-
posable memory transactions,‚Äù in Proceedings of the Tenth ACM
SIGPLAN Symposium on Principles and Practice of Parallel
Programming , ser. PPoPP ‚Äô05. New York, NY, USA: ACM,
2005, pp. 48‚Äì60.
[18] V. J. Marathe, W. N. Scherer, and M. L. Scott, ‚ÄúAdaptive
software transactional memory,‚Äù in Proceedings of the 19th In-ternational Conference on Distributed Computing ,ser. DISC‚Äô05.
Berlin, Heidelberg: Springer-Verlag, 2005, pp. 354‚Äì368.
[19] Q. Yi, ‚ÄúThe POET language manual,‚Äù 2008,
www.cs.utsa.edu/ ‚àºqingyi/POET/poet-manual.pdf.
[20] M. P. Herlihy and J. M. Wing, ‚ÄúLinearizability: A correctness
condition for concurrent objects. ‚Äù ACM, 1990, pp. 463‚Äì492.
[21] P. E. McKenney and J. D. Slingwine, ‚ÄúRead-copy-update: Using
execution history to solve concurrency problems,‚Äù in Parallel
and Distributed Computing and Systems , 1998, p. 509√¢ƒÇ≈û518.
[22] A. Matveev, N. Shavit, P. Felber, and P. Marlier, ‚ÄúRead-log-
update: A lightweight synchronization mechanism for concur-
rent programming,‚Äù in Proceedings of the 25th Symposium on
Operating Systems Principles , ser. SOSP ‚Äô15. New York, NY,
USA: ACM, 2015, pp. 168‚Äì183.
[23] M. Herlihy, ‚ÄúA methodology for implementing highly concurrent
data objects,‚Äù ACM Trans. Program. Lang. Syst. , vol. 15, pp.
745‚Äì770, Nov. 1993.
[24] D. L. Detlefs, P. A. Martin, M. Moir, and G. L. Steele, Jr.,
‚ÄúLock-free reference counting,‚Äù Distrib. Comput. , vol. 15, no. 4,
Dec. 2002.
[25] A. Gidenstam, M. PapatriantaÔ¨Ålou, H. Sundell, and P. Tsigas,
‚ÄúEÔ¨Écient and reliable lock-free memory reclamation based on
reference counting,‚Äù IEEE Trans. Parallel Distrib. Syst. , vol. 20,
no. 8, Aug. 2009.
[26] D. Dechev, P. Pirkelbauer, and B. Stroustrup, ‚ÄúUnderstanding
and eÔ¨Äectively preventing the aba problem in descriptor-based
lock-free designs,‚Äù in Proceedings of the 2010 13th IEEE Inter-
national Symposium on Object/Component/Service-Oriented
Real-Time Distributed Computing , ser. ISORC ‚Äô10. Washing-
ton, DC, USA: IEEE Computer Society, 2010, pp. 185‚Äì192.
[27] K. Cooper and L. Torczon, Engineering a Compiler . Morgan
Kaufmann, 2004.
[28] Q. Yi, ‚ÄúPOET: A scripting language for applying parameterized
source-to-source program transformations,‚Äù Software: Practice
& Experience , pp. 675‚Äì706, May 2012.
[29] D. Quinlan, M. Schordan, R. Vuduc, and Q. Yi, ‚ÄúAnnotat-
ing user-deÔ¨Åned abstractions for optimization,‚Äù in POHLL06:
Workshop on Performance Optimization for High-Level Lan-
guages and Libraries , Rhode Island, Greece, 2006.
[30] D. Dechev, P. LaBorde, and S. Feldman, ‚ÄúLc/dc: Lockless
containers and data concurrency: A novel nonblocking container
library for multicore applications,‚Äù 2013.
[31] V. J. Marathe, M. F. Spear, C. Heriot, A. Acharya, D. Eisenstat,
W. N. Scherer III, and M. L. Scott, ‚ÄúLowering the overhead of
nonblocking software transactional memory,‚Äù in Proceedings of
the 1st ACM SIGPLAN Workshop on Languages, Compilers,
and Hardware Support for Transactional Computing , Ottawa,
ON, Canada, 2006.
[32] Intel Threading Building Blocks Reference Man-
ual, Intel Corporation, 2014. [Online]. Available:
http://www.threadingbuildingblocks.org/docs/help/reference
[33] D. Hendler, I. Incze, N. Shavit, and M. Tzafrir, ‚ÄúFlat combining
and the synchronization-parallelism tradeoÔ¨Ä,‚Äù in Proceedings of
the Twenty-second Annual ACM Symposium on Parallelism in
Algorithms and Architectures , ser. SPAA ‚Äô10. New York, NY,
USA: ACM, 2010, pp. 355‚Äì364.
[34] P. Fatourou and N. D. Kallimanis, ‚ÄúA highly-eÔ¨Écient wait-
free universal construction,‚Äù in Proceedings of the Twenty-third
Annual ACM Symposium on Parallelism in Algorithms and
Architectures , ser. SPAA ‚Äô11. New York, NY, USA: ACM,
2011, pp. 325‚Äì334.
[35] Boost 1.56.0 Library Documentation , 2014. [Online]. Available:
http://www.boost.org/doc
[36] M. K. aka khizmax, Concurrent Data Structures library , 2012.
[Online]. Available: http://libcds.sourceforge.net/doc
[37] R. K. Treiber, Systems Programming: Coping with Parallelism ,
ser. Research Report RJ. International Business Machines
Incorporated, Thomas J. Watson Research Center, 1986.
[38] D. Zhang and D. Dechev, ‚ÄúAn eÔ¨Écient lock-free logarithmic
search data structure based on multi-dimensional list,‚Äù in 2016
IEEE 36th International Conference on Distributed Computing
Systems (ICDCS) , June 2016, pp. 281‚Äì292.
[39] Bridges User Guide . [Online]. Available:
https://portal.xsede.org/psc-bridges
746
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:39 UTC from IEEE Xplore.  Restrictions apply. [40] J. Kim, A. Mathew, S. Kashyap, M. K. Ramanathan,
and C. Min, ‚ÄúMv-rlu: Scaling read-log-update with multi-
versioning,‚Äù in Proceedings of the Twenty-Fourth International
Conference on Architectural Support for Programming
Languages and Operating Systems , ser. ASPLOS ‚Äô19. New
York, NY, USA: ACM, 2019, pp. 779‚Äì792. [Online]. Available:
http://doi.acm.org/10.1145/3297858.3304040
[41] M. Herlihy, ‚ÄúWait-free synchronization,‚Äù ACM Trans. Program.
Lang. Syst. , vol. 13, pp. 124‚Äì149, Jan. 1991.
[42] J. Turek, D. Shasha, and S. Prakash, ‚ÄúLocking without block-
ing: Making lock based concurrent data structure algorithms
nonblocking,‚Äù in Proceedings of the El eventh ACM SIGACT-
SIGMOD-SIGART Symposium on Principles of Database Sys-
tems , ser. PODS ‚Äô92. New York, NY, USA: ACM, 1992, pp.
212‚Äì222.
[43] Y. Afek, D. Dauber, and D. Touitou, ‚ÄúWait-free made fast,‚Äù in
Proceedings of the Twenty-s eventh Annual ACM Symposium on
Theory of Computing , ser. STOC ‚Äô95. New York, NY, USA:
ACM, 1995, pp. 538‚Äì547.
[44] A. Turon, ‚ÄúReagents: Expressing and composing Ô¨Åne-grained
concurrency,‚Äù in Proceedings of the 33rd ACM SIGPLAN Con-
ference on Programming Language Design and Implementation ,
ser. PLDI ‚Äô12. New York, NY, USA: ACM, 2012, pp. 157‚Äì168.
[45] M. Arbel and A. Morrison, ‚ÄúPredicate rcu: An rcu for scalable
concurrent updates,‚Äù in Proceedings of the 20th ACM SIGPLAN
Symposium on Principles and Practice of Parallel Program-
ming , ser. PPoPP 2015. New York, NY, USA: ACM, 2015,
pp. 21‚Äì30.
[46] J. F. Mart¬¥ ƒ±nez and J. Torrellas, ‚ÄúSpeculative synchronization:
Applying thread-level speculation to explicitly parallel applica-
tions,‚Äù in Proceedings of the 10th International Conference on
Architectural Support for Programming Languages and Operat-
ing Systems , ser. ASPLOS X. New York, NY, USA: ACM,
2002, pp. 18‚Äì29.
[47] M. Herlihy and J. E. B. Moss, ‚ÄúTransactional memory: Archi-
tectural support for lock-free data structures,‚Äù in Proceedings
of the 20th Annual International Symposium on Computer
Architecture , ser. ISCA ‚Äô93. New York, NY, USA: ACM, 1993,
pp. 289‚Äì300.
[48] P. Hammarlund, A. J. Martinez, A. A. Bajwa, D. L. Hill, E. Hall-
nor, H. Jiang, M. Dixon, M. Derr, M. Hunsaker, R. Kumar,
R. B. Osborne, R. Rajwar, R. Singhal, R. D‚ÄôSa, R. Chappell,
S. Kaushik, S. Chennupaty, S. Jourdan, S. Gunther, T. Piazza,
and T. Burton, ‚ÄúHaswell: The fourth-generation intel core pro-
cessor,‚Äù IEEE Micro , vol. 34, no. 2, pp. 6‚Äì20, Mar 2014.
[49] D. Christie, J.-W. Chung, S. Diestelhorst, M. Hohmuth,
M. Pohlack, C. Fetzer, M. Nowack, T. Riegel, P. Felber, P. Mar-
lier, and E. Rivi` ere, ‚ÄúEvaluation of amd‚Äôs advanced synchro-
nization facility within a complete transactional memory stack,‚Äù
inProceedings of the 5th European Conference on Computer
Systems , ser. EuroSys ‚Äô10. New York, NY, USA: ACM, 2010,
pp. 27‚Äì40.
[50] N. Shavit and D. Touitou, ‚ÄúSoftware transactional memory,‚Äù
inProceedings of the Fourteenth Annual ACM Symposium on
Principles of Distributed Computing , ser. PODC ‚Äô95. New
York, NY, USA: ACM, 1995, pp. 204‚Äì213.
[51] T. Harris and K. Fraser, ‚ÄúLanguage support for lightweight
transactions,‚Äù in Proceedings of the 18th Annual ACM SIG-
PLAN Conference on Object-oriented Programing, Systems,
Languages, and Applications , ser. OOPSLA ‚Äô03. New York,
NY, USA: ACM, 2003, pp. 388‚Äì402.
[52] A. Welc, S. Jagannathan, and A. L. Hosking, ‚ÄúTransac-
tional monitors for concurrent objects,‚Äù in In Proceedings of
the European Conference on Object-Oriented Programming .
SpringerVerlag, pp. 519‚Äì542.
[53] M. Moir, ‚ÄúTransparent support for wait-free transactions,‚Äù in
Proceedings of the 11th International Workshop on Distributed
Algorithms , ser. WDAG ‚Äô97. London, UK, UK: Springer-
Verlag, 1997, pp. 305‚Äì319.
[54] B. Saha, A.-R. Adl-Tabatabai, R. L. Hudson, C. C. Minh, and
B. Hertzberg, ‚ÄúMcrt-stm: A high performance software transac-
tional memory system for a multi-core runtime,‚Äù in Proceedings
of the Eleventh ACM SIGPLAN Symposium on Principles andPractice of Paral lel Programming , ser. PPoPP ‚Äô06. New York,
NY, USA: ACM, 2006, pp. 187‚Äì197.
[55] W. N. Scherer, III and M. L. Scott, ‚ÄúAdvanced contention
management for dynamic software transactional memory,‚Äù in
Proceedings of the Twenty-fourth Annual ACM Symposium on
Principles of Distribute d Computing , ser. PODC ‚Äô05. New
York, NY, USA: ACM, 2005, pp. 240‚Äì248.
[56] D. Dice and N. Shavit, ‚ÄúUnderstanding tradeoÔ¨Äs in software
transactional memory,‚Äù in Proceedings of the International Sym-
posium on Code Generation and Optimization , ser. CGO ‚Äô07.
Washington, DC, USA: IEEE Computer Society, 2007, pp. 21‚Äì
33.
[57] D. Dig, J. Marrero, and M. D. Ernst, ‚ÄúRefactoring sequential
java code for concurrency via concurrent libraries,‚Äù in
Proceedings of the 31st International Conference on Software
Engineering , ser. ICSE ‚Äô09. Washington, DC, USA: IEEE
Computer Society, 2009, pp. 397‚Äì407. [Online]. Available:
http://dx.doi.org/10.1109/ICSE.2009.5070539
[58] G. Jin, W. Zhang, D. Deng, B. Liblit, and S. Lu,
‚ÄúAutomated concurrency-bug Ô¨Åxing,‚Äù in Proceedings of the
10th USENIX Conference on Operating Systems Design
and Implementation , ser. OSDI‚Äô12. Berkeley, CA, USA:
USENIX Association, 2012, pp. 221‚Äì236. [Online]. Available:
http://dl.acm.org/citation.cfm?id=2387880.2387902
[59] P. ÀáCern¬¥ y, T. A. Henzinger, A. Radhakrishna, L. Ryzhyk, and
T. Tarrach, ‚ÄúEÔ¨Écient synthesis for concurrency by semantics-
preserving transformations,‚Äù in Proceedings of the 25th Inter-
national Conference on Computer Aided VeriÔ¨Åcation - Volume
8044 , ser. CAV 2013. New York, NY, USA: Springer-Verlag
New York, Inc., 2013, pp. 951‚Äì967.
[60] H. Lin, Z. Wang, S. Liu, J. Sun, D. Zhang, and G. Wei, ‚ÄúPÔ¨Åx:
Fixing concurrency bugs based on memory access patterns,‚Äù in
Proceedings of the 33rd ACM/IEEE International Conference
on Automated Software Engineering , ser. ASE 2018. New
York, NY, USA: ACM, 2018, pp. 589‚Äì600. [Online]. Available:
http://doi.acm.org/10.1145/3238147.3238198
[61] A. Solar-Lezama, C. G. Jones, and R. Bodik, ‚ÄúSketching con-
current data structures,‚Äù in Proceedings of the 29th ACM
SIGPLAN Conference on Programming Language Design and
Implementation , ser. PLDI ‚Äô08. New York, NY, USA: ACM,
2008, pp. 136‚Äì148.
[62] M. Vechev, E. Yahav, and G. Yorsh, ‚ÄúAbstraction-guided syn-
thesis of synchronization,‚Äù in Proceedings of the 37th Annual
ACM SIGPLAN-SIGACT Symposium on Principles of Pro-
gramming Languages , ser. POPL ‚Äô10. New York, NY, USA:
ACM, 2010, pp. 327‚Äì338.
[63] M. Vechev and E. Yahav, ‚ÄúDeriving linearizable Ô¨Åne-grained
concurrent objects,‚Äù in Proceedings of the 2008 ACM SIGPLAN
conference on Programming language design and implementa-
tion, ser. PLDI ‚Äô08. New York, NY, USA: ACM, 2008, pp.
125‚Äì135.
747
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:39 UTC from IEEE Xplore.  Restrictions apply. 