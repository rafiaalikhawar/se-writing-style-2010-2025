Automated Test Suite Generation for Time-continuous
Simulink Models
Reza Matinnejad, Shiva Nejati, Lionel C. Briand
SnT Centre, University of Luxembourg, Luxembourg
{reza.matinnejad,shiva.nejati,lionel.briand}@uni.luThomas Bruckmann
Delphi Automotive Systems, Luxembourg
thomas.bruckmann@delphi.com
ABSTRACT
All engineering disciplines are founded and rely on models, al-
though they may differ on purposes and usages of modeling. Inter-disciplinary domains such as Cyber Physical Systems (CPSs) seekapproaches that incorporate different modeling needs and usages.
Speciﬁcally, the Simulink modeling platform greatly appeals to
CPS engineers due to its seamless support for simulation and codegeneration. In this paper, we propose a test generation approachthat is applicable to Simulink models built for both purposes of
simulation and code generation. We deﬁne test inputs and outputs
as signals that capture evolution of values over time. Our test gener-ation approach is implemented as a meta-heuristic search algorithmand is guided to produce test outputs with diverse shapes accordingto our proposed notion of diversity. Our evaluation, performed on
industrial and public domain models, demonstrates that: (1) In con-
trast to the existing tools for testing Simulink models that are onlyapplicable to a subset of code generation models, our approach isapplicable to both code generation and simulation Simulink mod-
els. (2) Our new notion of diversity for output signals outperforms
random baseline testing and an existing notion of signal diversity inrevealing faults in Simulink models. (3) The fault revealing abilityof our test generation approach outperforms that of the SimulinkDesign V eriﬁer, the only testing toolbox for Simulink.
CCS Concepts
•Software and its engineering →Software testing;
Keywords
Simulink models; Software testing; Time-continuous behaviors;Search-based software testing; Output diversity; Signal features;Structural coverage; Simulink Design V eriﬁer (SLDV)
1. INTRODUCTION
Modeling has a long tradition in software engineering. Soft-
ware models are particularly used to create abstract descriptionsof software systems from which concrete implementations are pro-
duced [17]. Software development using models, also referred to
as Model Driven Engineering (MDE), is largely focused around theidea of models for code generation [16] or models for test gener-
ation [37]. Code or test generation, although important, is not the
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributedfor proﬁt or commercial advantage and that copies bear this notice and the full cita-tion on the ﬁrst page. Copyrights for components of this work owned by others thanACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-publish, to post on servers or to redistribute to lists, requires prior speciﬁc permissionand/or a fee. Request permissions from permissions@acm.org.
ICSE ’16 May 14–22, 2016, Austin, TX, USA
c/circlecopyrt 2016 ACM. ISBN 978-1-4503-3900-1. . . $15.00
DOI: http://dx.doi.org/10.1145/2884781.2884797primary reason for software modeling when software development
occurs in tandem with control engineering. In domains where soft-
ware closely interacts with physical processes and objects such asCyber Physical Systems (CPSs), one main driving force of model-ing is simulation, i.e., design time testing of system models. Simu-
lation aims to identify defects by testing models in early stages and
before the system has been implemented and deployed.
In the CPS domain, models built for simulation have major dif-
ferences from those from which code can be generated. Simulationmodels are heterogeneous, encompassing software, network and
physical parts, and are meant to represent as accurately as possi-ble the real world and its continuous dynamics. These models havetime-continuous behaviors (described using differential equations)
since they are expected to capture and continuously interact with
the physical world [25, 21]. Code generation models, on the other
hand, capture software parts only, and have discrete time behavior
(described using some form of discrete logic or discrete state ma-chines) [45, 24]. This is because the generated code will run on
platforms that support discrete computations, and further, the code
will receive input data as discrete sequences of events.
When simulation models are available, testing starts very early
typically by running those models for a number of selected scenar-ios. Early testing of simulation models pursues, among others, two
main objectives: (1) Ensuring correctness of simulation models so
that these models can act as oracle. This can signiﬁcantly reduceengineers’ reliance on expensive and late testing and measurementson the ﬁnal hardware. (2) Obtaining an initial test suite with high
fault revealing power to be used for testing software code, or at
later stages for testing the system on its ﬁnal hardware platform.
Our goal is to provide automated techniques to generate effec-
tive test suites for Simulink models [47]. Simulink is an advancedplatform for developing both simulation and code generation mod-
els in the CPS domain. The existing approaches to testing and
verifying Simulink models almost entirely focus on models withtime-discrete behavior, i.e., code generation models. These ap-proaches generate discrete test inputs for Simulink models with
the goal of reaching runtime errors to reveal faults [49, 22], vi-
olating assertions inserted into Simulink models based on someformal speciﬁcation [40, 14], and achieving high structural cov-erage [35, 41]. Discrete test inputs, however, are seldom sufﬁcientfor testing Simulink models, in particular, for those models with
time-continuous behaviors. Many faults may not lead to runtime
crashes. Formal speciﬁcations are rather rare in practice, and fur-ther, are not amenable to capturing continuous dynamics of CPSs.Finally, effectiveness of structural coverage criteria has yet to be
ascertained and empirically evaluated for Simulink model testing.
In this paper, we provide test generation techniques for Simulink
models with time-continuous behaviors. We generate test inputs
2016 IEEE/ACM 38th IEEE International Conference on Software Engineering
   595
ascontinuous signals and do not rely on the existence of implicit
oracles (e.g., runtime failures) or formal speciﬁcations. Assuming
that automated oracles are not available, we focus on providing test
generation algorithms that develop small test suites with high faultrevealing power, effectively reducing the cost of human test ora-cles [6, 31]. Instead of focusing on increasing structural coverage,we propose and evaluate a test data generation approach that aims
to maximize diversity in output signals of Simulink models. Output
signals provide a useful source of information for detecting faultsin Simulink models as they not only show the values of model out-puts at particular time instants, but also they show how these values
change over time. By inspecting output signals, one can determine
whether the model output reaches appropriate values at the righttimes, whether the time period that the model takes to change itsvalues is within acceptable limits, and whether the signal shape isfree of erratic and unexpected changes that violate continuous dy-
namics of physical processes or objects.
Our intuition is that test cases that yield diverse output signals
may likely reveal different types of faults in Simulink models. The
key here is the deﬁnition of output diversity. In our earlier work, weproposed a notion of output diversity based on the Euclidean dis-tance between signal vector outputs of mixed discrete-continuousStateﬂows [28]. Stateﬂow is a subset of Simulink for capturingstate-based behaviors. In this work, we introduce a new notion ofdiversity for signals that is deﬁned based on a set of representative
and discriminating signal feature shapes. We refer to the former as
vector-based and to the latter as feature-based diversity objectives.
We develop a meta-heuristic search algorithm that generates testsuites with diversiﬁed output signals where the diversity objective
can be either vector-based or feature-based. Our algorithm uses the
whole test suite generation approach [18]. Further, our algorithmadapts a single-state search optimizer [27] to generate continuousinput signals, and proposes a novel way to dynamically increasevariations in test input signals based on the amount of structural
coverage achieved by the generated test suites. We evaluate our al-
gorithm using four industrial and public-domain Simulink models.
Our contributions are as follows:(1) We identify the problem of
testing simulation models and argue that, though simulation models
are essential in the CPS domain, few systematic testing/veriﬁcation
techniques exist for them.(2) We propose a new notion of diversity
for output signals and develop a novel algorithm based on this no-tion for generating test suites for both simulation and code genera-tion Simulink models. We show that our new notion of diversity for
output signals outperforms random baseline testing and an exist-
ing notion of signal output diversity in revealing faults in Simulinkmodels.(3) We compare our test generation approach that diversi-
ﬁes test outputs with the Simulink Design V eriﬁer (SLDV), the only
Simulink testing toolbox. SLDV automatically generates test suites
for a subset of Simulink models with the goal of achieving highstructural coverage. We argue that while our approach is applica-ble to the entire Simulink, SLDV supports a subset. We show that,when considering the SLDV -compatible subset, our output diver-
sity approach is able to reveal signiﬁcantly more faults compared
to SLDV , and further, it subsumes SLDV in revealing faults: Anyfault identiﬁed by SLDV is also identiﬁed by our approach.
2. MOTIV ATION AND BACKGROUND
In this section, we provide examples of simulation and code
generation models. We then introduce SimuLink Design V eriﬁer(SLDV) and motivate our output diversity test generation approach.
Example. We motivate our work using a simpliﬁed Fuel Level
Controller (FLC) which is an automotive software component used
in cars’ fuel level management systems. FLC computes the fuelvolume in a tank using the continuous resistance signal that it re-
ceives from a fuel level sensor mounted on the fuel tank. The sensor
data, however, cannot be easily converted into an accurate estima-tion of the available fuel volume in a tank. This is because the rela-
tionship between the sensor data and the actual fuel volume is im-
pacted by the irregular shape of the fuel tank, dynamic conditionsof the vehicle (e.g., accelerations and braking), and the oscillationsof the indication provided by the sensors. Hence, FLC has to rely
on complex ﬁltering algorithms involving algebraic and differential
equations to accurately compute the actual fuel volume [46].
Simulation models. In the automotive industry, engineers build
simulation models prior to any software coding and often at the
time of system-level design and engineering. Simulation models
most often contain time-continuous mathematical operations [12].For example, Figure 1(a) shows a very simpliﬁed FLC Simulinkmodel which is adopted from [58] and includes a time-continuous
integral operator (/integraltext
). We refer to the model in Figure 1(a) as a
simulation model of FLC. The input of this model is the resistance
signal from the sensor, and its output shows the fuel volume. TheSimulink model in Figure 1(a) is executable. Engineers can run themodel for any desired input signal and inspect the output. Automo-
tive engineers often rely on their knowledge of mechanics and con-
trol theory to design simulation models. These models, however,need to be veriﬁed or systematically tested as they are complex andmay include several hundreds of blocks.
Code generation models. Figure 1(b) shows an example FLC code
generation model, (i.e., the model from which software code can be
automatically generated). The code generation model is discrete:The time-continuous integrator block (/integraltext
) in the simulation model is
replaced by a time-discrete integrator (sum) in the code generationmodel. The behavior of code generation models may deviate fromthat of simulation models since the latter often has time-continuousoperations while the former is purely discrete. Typically, some de-
gree of deviations between simulation and code generation model
outputs are acceptable. The level of acceptable deviations, how-ever, have to be determined by domain experts.
Simulation model outputs vs. code generation model outputs.
Figure 1(c) shows an example continuous input signal for FLC over
a10sec time period. The signal represents the resistance values re-
ceived from the tank sensor. This input signal can be applied toboth simulation and code generation models of FLC. Note that a
time-continuous model has to be provided with a continuous in-
put signal (i.e., a continuous function over time). A time-discretemodel, on the other hand, only requires single values at discretetime steps which can be extracted from continuous signals as well.Models in Figures 1(a) and (b) respectively produce the outputs in
Figures 1(d) and (e) once they are provided with the input in Fig-
ure 1(c). As shown in Figures 1(d) and (e), the percentages of fuelvolume in the continuous output signal differ from those in the dis-crete output signal. For example, after one second, the output of
the simulation model is 91.43, while that of the code generation
model is88.8 . These deviations are due to the differences between
the continuous and discrete integrals. For example, the grey area
in the left part of Figure 1(f) shows the value computed by/integraltext
after
three seconds, while the value computed by the discretized integral
operator after three seconds is the grey area in the right part of Fig-
ure 1(f). Clearly, these two operators, and hence the two models inFigures 1(a) and (b), generate different values for the same input.
Simulation models as oracles for code. In the CPS domain, time-
continuous simulation models play a crucial role. Not only that
they are the blueprints for code generation models, and later, thesoftware code, but also they serve as oracle generators for test-
596(d) FLC Simulation Model Output (Fuel Level)
05 1 050150250(c) FLC Model Input (Fuel Level Sensor)
05 1 0Fuel Level Sensor
Fuel Level
05 1 0
(e) FLC Code-Generation Model Output (Fuel Level)50100
(f) Continuous vs. Discrete Integral
05 1 0Fuel Level
50100
75
05 1 0Time (s) Time (s)
03
Time (s) Time (s)03
Time (s)
Time (s) Time (s)
Fuel Level
50100
75(g) Faulty FLC Model Output (Test Input 1) (h) Faulty FLC Model Output (Test Input 2)250Fuel Level Sensor150
50
BA
60708090Fuel Level
50100
60708090
(a) FLC Simulation Model (b) FLC Code Generation Model 
Faulty Model Output
Correct Model OutputFaulty Model Output
Correct Model Output
Figure 1: A Fuel Level Controller (FLC) example: (a) A sim-
ulation model of FLC; (b) a code generation model of FLC;(c) an input to FLC; (d) output of (a) when given (c) as input;(e) output of (b) when given (c) as input; (f) comparing outputs
of the blocks/integraltext
andsum from models (a) and (b), respectively;
(g) A test output of a faulty version of (a); and (h) another test
output a faulty version of (a).
ing software code on various platforms. While oracles obtained
based on formal speciﬁcations or runtime errors are often preciseand deterministic [33, 34], those obtained based on simulation out-puts are inexact as some deviations from oracles are acceptable.
These oracles provide engineers with a reasonable and yet valuableassessment of the code behavior. Further, the oracle informationthat is obtained from simulation models is not readily available inother artifacts, e.g., requirements and speciﬁcations. Therefore, it is
important to develop accurate and high quality simulation models.
Hence, our goal in this paper is to provide systematic algorithms tohelp with testing of time-continuous simulation models.
Simulink Design Veriﬁer. SimuLink Design V eriﬁer (SLDV) is
a product of Mathworks and a Simulink toolbox. It is the only
Simulink toolbox that is dedicated to test generation. It automat-ically generates test input signals for Simulink models using con-straint solving and model-checking techniques [49]. SLDV pro-
vides two usage modes: (1) generating test suites to achieve some
form of structural coverage, and (2) generating test scenarios (counter-examples) indicating violation of some given properties (assertions).In the ﬁrst usage mode, SLDV creates a test suite satisfying a givenstructural coverage criterion [50]. In the second usage mode, SLDV
tries to prove that assertions inserted into Simulink models cannot
be reached, or otherwise, it generates inputs triggering the asser-tions, hence, disproving the desired properties.
In this paper, we compare the fault revealing ability of our algo-
rithm with that of the ﬁrst usage mode of SLDV , i.e., test generationguided by structural coverage. Note that the second usage mode ofSLDV requires exact oracles which is out of the scope of this paper.We chose to compare our work with SLDV as it is distributed bythe Mathworks and is among the most well-known tools for test-
ing and veriﬁcation of Simulink models. Other existing tools in
that category, e.g., Reactis [39], rely on formal techniques as well.Based on our experience working with SLDV and according to theMathworks white papers [20], SLDV has the following practical
limitations: (1) Model Compatibility. SLDV supports a subset of
the Simulink language (i.e., discrete fragment of Simulink) [12],
and is not applicable to time-continuous blocks of Simulink suchas the continuous integrator in Figure 1(a). Speciﬁcally, SLDV isapplicable to the model in Figure 1(b), but not to that in Figure 1(a).
Further, there are a number of blocks that are acceptable by the
Simulink code generator but are not yet compatible with SLDV inparticular, the S-Function block which provides access to system
functions and custom C code from Simulink models. (2) Scala-
bility. The lack of scalability of SLDV is recognized as an issue
by Mathworks [20]. Further, as the models get larger and morecomplicated, it is more likely that they contain blocks that are notsupported by SLDV . So, for SLDV , the problem of compatibilityoften precedes and overrides the problem of lack of scale [20].
Coverage vs. Output Diversity. In contrast to SLDV which aims
to maximize structural coverage, we propose a test generation al-
gorithm that tries to diversify output signals. We illustrate the dif-
ferences between test generation based on structural coverage and
based on output diversity using a faulty version of the simulationmodel in Figure 1(a). Suppose the line connected to point A inthis model is mistakenly connected to point B. Figures 1(g) and (h)show two different output signals obtained from this faulty model
along with the expected outputs. The faulty output is shown by a
solid line and the correct one (oracle) is shown by a dashed line.The faulty output in Figure 1(g) almost matches the oracle, whilethe one in Figure 1(h) drastically deviates from the oracle. Given
that small deviations from oracle are acceptable, engineers are un-
likely to identify any fault when provided with the output in Fig-ure 1(g). When the goal is high structural coverage, the test inputsyielding the two outputs in Figures 1(g) and (h) are equally de-sirable. Indeed, for the FLC model, one test input is sufﬁcient to
achieve full structural coverage. If this test input happens to pro-
duce the output in Figure 1(g), the fault goes unnoticed. In contrast,our approach attempts to generate test cases that yield diverse out-put signals to increase the probability of generating outputs that
noticeably diverge from the expected result.
3. TEST GENERATION ALGORITHMS
We propose a search-based whole test suite generation algorithm
for Simulink models. Our test generation algorithm aims to maxi-
mize diversity among output signals generated by a test suite. Wedeﬁne two notions of diversity among output signals: vector-based
and feature-based. We ﬁrst ﬁx a notation, and will then describe
our notions of output diversity and our test generation algorithm.
Notation. Let M=(I,o)be a Simulink model where I=
{i
1,...,i n}is the set of input variables and ois the output vari-
able ofM. Each input/output variable of M is a signal, i.e., a
597function of time. Irrespective of M being a simulation or a code
generation model, each input or output of M is stored as a vector
whose elements are indexed by time. Assuming that the simula-
tion time is T, the simulation interval [0..T]is divided into small
equal time steps denoted by Δt. We deﬁne a signal sgas a func-
tionsg:{0,Δt,2·Δt,...,k ·Δt}→R , whereΔt is the simulation
time step, kis the number of observed simulation steps, and Ris
the signal range. The signal range Ris bounded by its min and max
values denoted minRandmaxR, respectively. For the example in
Figure 1, we have T=1 0s,Δt=1s, andk=1 0 . Note that in
that example, to better illustrate the input and output signals, Δt
is chosen to be larger than normal. In one of our experiments, forexample, we have Δt=0.001s ,T=2s, andk= 2000.
Our goal is to generate a test suite TS={I
1,...,I q}. Each
test input Ijis a vector (sgi1,...,sg in)of signals for the input
variables i1toinofM. By simulating M using each test input
Ij, we obtain an output signal sgofor the output variable oofM.
All the input signals sgijand the output signal sgoshare the same
simulation time interval and simulation time steps, i.e., the valuesofΔt,T, andkare the same for all of the signals.
Test inputs. In Simulink, every variable even those representing
discrete events are described using signals. In this context, testinput generation is essentially signal generation. Each test inputis a vector (sg
i1,...,sg in)of signals. Each signal sgijis also
a vector with a few thousands of elements, and each element cantake an arbitrary value from the signal range. To specify an in-put signal, however, engineers never deﬁne a few thousands ofvalues individually. Instead, they specify a signal by deﬁning asequence of signal segments . To formalize input signals gener-
ated in practice, we characterize each input signal sg with a set
{(k
1,v1),...,(kp,vp)}of points where k1tokpare the simula-
tion steps s.t. k1=0 ,k1≤k2≤...≤kp, andv1tovpare the
values that sgtakes at simulation steps k1tokp, respectively. The
set{(k1,v1),...,(kp,vp)}speciﬁes a signal sgwithpsegments.
The ﬁrstp−1segments of sgare deﬁned as follows: For 1≤j<
p, each pair (kj,vj)and(kj+1,vj+1)speciﬁes a signal segment s.t.
∀l·kj≤l<k j+1⇒sg(l·Δt)=vj∧sg(kj+1·Δt)=vj+1. The last
segment of sgis a constant signal that starts at (kp,vp)and ends at
(k,v p)wherekis the maximum number of simulation steps. For
example, {(0,v)}speciﬁes a constant signal at v(i.e., one segment
p=1 ). A step signal going from v0tov1and stepped at k/primeis
speciﬁed by {(0,v0),(k/prime,v1)}(i.e., two segments p=2 ).
Input signals with fewer segments are easier to generate but they
may fail to cover a large part of the underlying Simulink model.By increasing the number of segments in input signals, structuralcoverage increases, but the output generated by such test inputs
becomes more complex, and engineers may not be able to deter-
mine expected outputs (oracle). Furthermore, highly segmented in-put signals may not be reproducible on hardware platforms as theymay violate physical constraints of embedded devices. For each
input variable, engineers often have a good knowledge on the max-
imum number of segments that a signal value for that variable maypossibly contain and still remains feasible. In our test generationalgorithm discussed at the end of this section, we ensure that, foreach input variable, the generated input signals achieve high struc-
tural coverage while their segment numbers remain lower than the
limits provided by domain experts.
Vector-based output diversity. This diversity notion is deﬁned di-
rectly over output signal vectors. Let sg
oandsg/prime
obe two signals
generated for output variable oby two different test inputs of M.I n
our earlier work [28], we deﬁned the vector-based diversity mea-
sure between sgoandsg/prime
oas the normalized Euclidean distancebetween these two signals. We denote the vector-based diversity
betweensgoandsg/prime
obyˆdist(sgo,sg/prime
o).
Our vector-based notion, however, has two drawbacks: (1) It is
computationally expensive since it is deﬁned over signal vectors
with a few thousands of elements. Using it in a search algorithmamounts to computing the Euclidean distance between many pairs
of output signals at every iteration of the search. (2) A searchdriven by vector-based distance may generate several signals withsimilar shapes whose vectors happen to yield a high Euclidean dis-
tance value. For example, for two constant signals sg
oandsg/prime
o,
ˆdist(sgo,sg/prime
o)is relatively large when sgois constant at the max-
imum of the signal range while sg/prime
ois constant at the minimum of
the signal range. A test suite that generates several output signals
with similar shapes may not help with fault ﬁnding.
Feature-based output diversity. In machine learning, a feature
is an individual measurable and non-redundant property of a phe-nomenon being observed [56]. Features serve as a proxy for largeinput data that is too expensive to be directly processed, and further,is suspected to be highly redundant. In our work, we deﬁne a set
of basic features characterizing distinguishable signal shapes. We
then describe output signals in terms of our proposed signal fea-tures, effectively replacing signal vectors by feature vectors. Fea-
ture vectors are expected to contain relevant information from sig-
nals so that the desired analysis can be performed on them instead
of the original signal vectors. To generate a diversiﬁed set of outputsignals, instead of processing the actual signal vectors with thou-sands of elements, we maximize the distance between their corre-sponding feature vectors with tens of elements.
Figure 2(a) shows our proposed signal feature classiﬁcation. Our
classiﬁcation captures the typical, basic and common signal pat-terns described in the signal processing literature, e.g., constant,decrease, increase, local optimum, and step [36]. The classiﬁcation
in Figure 2(a) identiﬁes three abstract signal features: value, deriva-
tive and second derivative. The abstract features are italicized. Thevalue feature is extended into: instant-value and constant-value fea-tures that are respectively parameterized by (v)and(n,v). The
former indicates signals that cross a speciﬁc value vat some point,
and the latter indicates signals that remain constant at vforncon-
secutive time steps. These features can be instantiated by assigningconcrete values to norv. Speciﬁcally, the constant-value (n,v)
feature can be instantiated as the one-step constant-value (v)and
always constant-value( v)features by assigning nto one and k(i.e.,
the simulation length), respectively. Similarly, speciﬁc values for v
are zero, and max and min of signal ranges (i.e., max
RandminR).
The derivative feature is extended into sign-derivative and extreme-
derivative features. The sign-derivative feature is parameterized by
(s,n) wheresis the sign of the signal derivative and nis the num-
ber of consecutive time steps during which the sign of the signal
derivative is s. The sign scan be zero, positive or negative, re-
sulting in constant( n), increasing( n), and decreasing( n)features,
respectively. As before, speciﬁc values of nare one and k. The
extreme-derivatives feature is non parameterized and is extendedinto one-sided discontinuity, one-sided discontinuity with local op-timum, one-sided discontinuity with strict local optimum, discon-tinuity, and discontinuity with strict local optimum features. The
second derivative feature is extended into more speciﬁc features
similar to the derivative feature. Due to space limit, we have notshown those extensions in Figure 2(a).
Figures 2(b) to (e) respectively illustrate the instant-value (v), the
increasing( n), the one-sided continuity with local optimum, and
the discontinuity with strict local optimum features. Speciﬁcally,the signal in Figure 2(b) takes value vat point A. The signal in
Figure 2(c) is increasing for nsteps from B to C. The signal in
598increasing (n)
decreasing (n)constant-value (n, v) signal features
derivative second derivative
sign-derivative (s, n)
extreme-derivatives
1-sided 
discontinuity
discontinuity
1-sided continuity
with local optimum1-sided continuity
with strict local optimum one-step 
constant-value (v) all-step 
constant-value (v)value 
 instant-value (v)
constant (n)  instant-value (0) ......
 instant-value ( minRminR)
......
...Always 
Constant1-step 
Constant......(a) Features Classiﬁcation (b)  instant-value (v)
...
(d) 1-sided continuity
with local optimum(e) discontinuity with 
strict local optimumv
k.Δt 0 k.Δt 0k/prime.Δt(k/prime+n).Δt
k.Δt 0 k.Δt 0discontinuity
with strict local optimum(c)  increasing (n)
v=0v=0
v=0v=0 v=0v=0......
...k/prime.ΔtABC
D E  v=maxR v=maxR       
 v=minR v=minR        n=1n=1    n=k n=k
n=1n=1    n=k n=kn=1n=1  
n=1n=1    s=0s=0  s=1s=1   s=−1 s=−1
Figure 2: Signal Features: (a) Our signal feature classiﬁcation, and (b)–(e) Examples of signal features from the classiﬁcation in (a).
Figure 2(d) is right-continuous but discontinuous from left at point
D, and further, the signal value at D is not less than the values at
its adjacent point, hence making D a local optimum. Finally, the
signal in Figure 2(e) is discontinuous from both left and right atpoint E which is a strict local optimum point as well.
We deﬁne a function F
ffor each (non-abstract) feature fin Fig-
ure 2(a). We refer to Ffasfeature function. The output of func-
tionFfwhen given signal sgas input is a value that quantiﬁes the
similarity between shapes of sgandf. More speciﬁcally, Ffdeter-
mines whether any part of sgis similar to feature f. For example,
suppose functions lds(sg,i) and rds(sg,i) respectively compute the
left and right derivative signs of sgat simulation step i. Speciﬁ-
cally, they generate 1,−1, and0if the derivative value is positive,
negative, and zero, respectively. We deﬁne Fffor the feature in
Figure 2(d) as follows:
Ff(sg)=kmax
i=1(|derivative( sg,i)|×localOpt( sg,i)) such that
derivative( sg,i)=sg((i)·Δt)−sg((i−1)·Δt)
Δtand
localOpt( sg,i)=/braceleftBigg
1,lds(sg,i)/negationslash=rds(sg,i)
0, Otherwise
Function Ff(sg)computes the largest left or right derivative of sg
that occurs at a local optimum point, i.e., the largest one-sidedderivative that occurs at a point isuch that the derivative of sg
changes its sign at i. The higher
Ff(sg), the more similar sgis to
the feature in Figure 2(d). Our complete signal feature classiﬁca-tion and the corresponding feature functions are available at [29].
Having deﬁned features and feature functions, we now describe
how we employ these functions to provide a measure of diversitybetween output signals sg
oandsg/prime
o. Letf1,...,f mbemfea-
tures that we choose to include in our diversity measure. We com-pute feature vectors F
v(sgo)=(Ff1(sgo),...,F fm(sgo))and
Fv(sg/prime
o)=(Ff1(sg/prime
o),...,F fm(sg/prime
o))corresponding to signals
sgoandsg/prime
o, respectively. Since the ranges of the feature func-
tion values may vary widely, we standardize these vectors beforecomparing them. Speciﬁcally, we use feature scaling which is acommon standardization method for data processing [56]. Having
obtained standardized feature vectors ˆF
v(sgo)andˆFv(sg/prime
o)cor-
responding to signals sgoandsg/prime
o, we compute the Euclidean dis-
tance between these two vectors, (i.e., ˆdist(ˆFv(sgo),ˆFv(sg/prime
o))),
as the measure of feature-based diversity between signals sgoand
sg/prime
o. Below, we discuss how our diversity notions are used to gen-
erate test suites for Simulink models.
Whole test suite generation based on output diversity. We pro-
pose a meta-heuristic search algorithm to generate a test suite TS=
{I1,...,I q}for a given model M=(I,o)to diversify the set of
output signals generated by TS . We denote by
TSO={sg1,...,sg q}the set of output signals generated by TS .
We capture the degree of diversity among output signals in TSOusing objective functions OvandOfthat correspond to vector-
based and feature-based notions of diversity, respectively:
Ov(TSO)=q/summationtext
i=1MIN∀sg∈TSO\{sg i}ˆdist(sgi,sg)
Of(TSO)=q/summationtext
i=1MIN∀sg∈TSO\{sg i}ˆdist(Fv(sgi),Fv(sg))
Function Ovcomputes the sum of the minimum distances of each
output signal vector sgifrom the other output signal vectors in
TSO . Similarly, Ofcomputes the sum of the minimum distances
of each feature vector Fv(sgi)from feature vectors of the other
output signals in TSO . Our test generation algorithm aims to max-
imize functions OvandOfto increase diversity among the signal
vectors and feature vectors of the output signals, respectively.
Our algorithm adapts the whole test suite generation approach [18]
by generating an entire test suite at each iteration and evolving, ateach iteration, every test input in the test suite. The whole test suitegeneration approach is a recent and preferred technique for test data
generation specially when, similar to O
vandOf, objective func-
tions are deﬁned over the entire test suite and aggregate all testing
goals. Another beneﬁt of this approach for our work is that it allowsus to optimize our test objectives while ﬁxing the test suite size at
a small value due to the cost of manual test oracles.
Our algorithm implements a single-state search optimizer that
only keeps one candidate solution (i.e, one test suite) at a time, as
opposed to population-based algorithms that keep a set of candi-dates at each iteration. This is because our objective functions are
computationally expensive as they require to simulate the underly-
ing Simulink model and compute distance functions between ev-ery test input pair. When objective functions are time-consuming,population-based search may become less scalable as it may have to
re-compute objective functions for several new or modiﬁed mem-
bers of the population at each iteration.
Figure 3 shows our output diversity test generation algorithm for
Simulink models. We refer to it as OD. The core of OD is a Hill-Climbing search procedure [27]. Speciﬁcally, the algorithm gen-
erates an initial solution (lines 2-3), iteratively tweaks this solution
(line 12), and selects a new solution whenever its objective functionis higher than the current best solution (lines 16-18). The objectivefunctionOin OD is applied to the output signals in TSO that are
obtained from test suites. The objective function can be either O
f
orOv, respectively generating test suites that are optimized based
on feature-based and vector-based diversity notions.
While being a Hill-Climbing search in essence, OD proposes two
novel adaptations: (1) It initially generates input signals that con-
tain a small number of signal segments P. It then increases Ponly
when it is needed while ensuring that Pis never more than the limit
provided by the domain expert Pmax . Recall that, on one hand, in-
creasing segments of input signals makes the output more difﬁcultto analyse, but that, on the other hand, input signals with few seg-
599Algorithm. The test generation algorithm applied to a Simulink model M.
1.P←initial number of signal segments for test inputs
2.TS←GENERA TE INITIAL TEST SUITE(P)
3.BestFound ←O(TSO)
4.Pmax←maximum number of signal segments permitted in test inputs
5.TSO←signal outputs obtained by simulating Mfor every test input in TS
6. whole-test-suite-coverage ←coverage achieved by TS overM
7. initial-coverage ←whole-test-suite-coverage
8. accumulative-coverage ←initial-coverage
9. Letσ-exploration andσ-exploitation be the max and min tweak parameters, respectively.
10.σ←σ-exploitation /*tweak parameter */
11. repeat
12. newTS =T WEAK (TS,σ,P)/* generating new candidate solution */
13. TSO←signal outputs obtained by simulating Mfor every test input in newTS
14. whole-test-suite-coverage ←coverage achieved by newTS overM
15. accumulative-coverage ←accumulative-coverage +whole-test-suite-coverage
16. ifO(TSO)>highestFound :
17.highestFound =O(TSO)
18.TS=newTS
19. ifaccumulative-coverage has reached a plateau at a value less than %100 andP<P max :
20.P=P+1
21. Reduce σproportionally as accumulative-coverage increases over initial-coverage
22. until maximum resources spent
23. returnTS
Figure 3: Our output diversity (OD) test generation algorithm
for Simulink models.
ments may not reach high model coverage. In OD, we initially gen-
erate test inputs with Psegments (lines 1-2). The tweak operator
does not change the number of segments either (line 12). We in-creasePonly when the accumulative structural coverage achieved
by the existing generated test suites reaches a plateau at a valueless than %100, i.e., remains constant for some consecutive itera-
tions of the algorithm (lines 19-20). Further, although not shown
in the algorithm, we do not increase Pif the last increase in Phas
not improved the accumulative coverage.
(2) The tweak operator in OD (line 12) is explorative at the be-
ginning and becomes more exploitative as the search progresses.
Our tweak is similar to the one used in (1+1) EA algorithm [27]. At
each iteration, we shift every signal sg∈TS denoted by
{(k
1,v1),...,(kp,vp)}as follows: We add values xi(respectively
yi) to every vi(respectively ki) for1≤i≤p. Thexi(respectively
yi) values are selected from a normal distribution with mean μ=0
and variance σ×(maxR−minR) (respectively σ×k), where
Ris the signal range and kis the number of simulation steps. We
control the degree of exploration and exploitation of our search us-ingσ. Given that the search space of input signals is very large,
if we start by a purely exploitative search (i.e., σ=0.01), our re-
sult will be biased by the initially randomly selected solution. Toreduce this bias, we start by performing a more explorative search(i.e.,σ=0.5). However, if we let the search remain explorative,
it may reduce to a random search. Hence, we reduce σiteratively
in OD such that the amount of reduction in σis proportional to the
increase in the accumulative structural coverage obtained by thegenerated test suites (line 21). Finally, we note that the tweak op-
erator takes the signal segments Pas an input (line 12) and, in case
the number of signal segments has increased from the previous it-eration, it ensures to increase the number of segments in signal sg.
We note that our input signal generation algorithm, described
above, is geared towards the speciﬁc needs of the automotive do-
main where the dynamic behavior of the system is tested using se-
quences of step signals as input. This is sufﬁcient for capturingautomotive environment events, which are largely aperiodic, e.g.,driver’s commands. For other domains, e.g., the communicationdomain, our test generation algorithm may need to be adapted to
effectively capture highly frequent periodic signals.
4. EXPERIMENT SETUP
In this section, we present the research questions. We further de-
scribe our study subjects, our metrics to measure the fault revealingability of test generation algorithms and the way we approximate
their oracle cost. We ﬁnally provide our experiment design.
4.1 Research Questions
RQ1 (Sanity check). How does the fault revealing ability of the
OD algorithm compare with that of a random test generation strat-
egy? We investigate whether OD is able to perform better than
random testing which is a baseline of comparison. We compare the
fault revealing ability of the test suites generated by OD when usedwith each of the O
vandOfobjective functions with that of the test
suites generated by a random test generation algorithm.
RQ2 (Comparing OvandOf).How does the Ofdiversity objec-
tive perform compared to the Ovdiversity objective? We compare
the ability of the test suites generated by OD with OvandOfin
revealing faults in time-continuous Simulink models. In particular,
we are interested to know if, irrespective of the size of the gen-
erated test suites, any of these two diversity objectives is able toconsistently reveal more faults across different study subjects anddifferent fault types than the other.
RQ3 (Comparison with SLDV). How does the fault revealing
ability of the OD algorithm compare with that of SLDV? With this
question, we compare an output diversity approach (OD) with an
approach based on structural coverage (SLDV) in generating effec-
tive test suites for Simulink models. This question, further, enables
us to provide evidence that our approach is able to outperform themost widely used industry strength Simulink model testing tool.Finally, in contrast to RQ1 and RQ2 where we applied OD to time-
continuous Simulink models, this question has to focus on discretemodels because SLDV is only applicable to time-discrete Simulinkmodels. Hence, this question allows us to investigate the capabili-ties of OD in ﬁnding faults for discrete Simulink models, as well.
4.2 Study Subjects
We use four Simulink models in our experiments: Two indus-
trial models, Clutch Position Controller (CPC) and Flap Position
Controller (FPC), from Delphi, and two public domain models,
Cruise Controller (CC) [48] and Clutch Lockup Controller (CLC),from the Mathworks website [44]. Table 1 shows key character-istics of these models. CPC and CC include Stateﬂows, and FPCand CLC are Simulink models without Stateﬂows. FPC and CPC
are time-continuous models and incompatible with SLDV . The CC
model, which is the largest model from the SLDV tutorial exam-ples, is compatible with SLDV . Since the other tutorial examplesof SLDV were small, we modiﬁed the CLC model from the Math-
works website to become compatible with SLDV by replacing the
time-continuous and other SLDV -incompatible blocks with theirequivalent or approximating discrete blocks. We have made themodiﬁed version of CLC available at [29]. Note that we werenot able to make CPC, FPC or any other Delphi Simulink models
compatible with SLDV since they contained complex S-Function
blocks, and hence, they have to be almost reimplemented beforeSLDV can be applied to them. The coverage criterion used by bothSLDV and OD in Figure 3 is decision coverage [50], also known
as branch coverage, that aims to ensure that each one of the possi-ble branches from each decision point is executed at least once andthereby ensuring that all reachable blocks are executed. We chosebranch coverage as it is the predominant coverage criterion in theliterature [18]. Table 1 reports the total number of decision points
in our study subjects. In addition, we report the total number of
Simulink blocks and Stateﬂow states as well as input variables andconﬁguration parameters for each model. CPC and FPC are rep-resentative models from the automotive domain with many input
600Table 1: Characteristics of our study subject Simulink models.
Publicly 
AvailableNameNo. 
InputsNo. Blocks/
States
CPC No 10 590No. Decision
Points
126
FPC
CC
CLCNo
YesNo. 
Conﬁgs
41
810 65 21 120
6 3 43
Yes12
2 0 81 10
variables and blocks. In order to compare OD with SLDV , we use
CC from the Mathworks website and the modiﬁed version of CLCas both models are reasonably large and complex, and yet compat-ible with SLDV .
4.3 Measuring Fault Revealing Ability
We need test oracles to automatically assess the fault revealing
ability of generated test suites in our experimental setting. As dis-cussed earlier, in our work, test oracles depend on manual inspec-
tion of output signals and on engineers’ estimates of acceptable de-
viations from the expected results. To measure the fault revealing
ability of a test suite, we use a quantitative notion of oracle de-ﬁned as the largest normalized distance, among all output signals,between test results and the ground truth oracle [6]. For the pur-pose of experimentation, we use fault-free versions of our subject
models to produce the ground truth oracle. Let TS be a test suite
generated by either OD or SLDV for a given faulty model M, let
O={sg
1,...,sg q}be the set of output signals obtained by run-
ningMfor the test inputs in TS , and letG={g1,...,g q}be the
corresponding ground truth oracle signals. We deﬁne our quantita-
tive oracle (QO ) as follows: QO(M,TS)=MAX 1≤i≤qˆdist(sgi,gi).
We use a threshold value THR to translate the quantitive oracle
QO into a boolean fault revealing measure denoted by FR . Specif-
ically,FR returns true (i.e, QO(M,TS)>THR ) if there is at least
one test input in TS for which the output of Msufﬁciently deviates
from the ground truth oracle such that a manual tester conclusively
detects a failure. Otherwise, FR returns false. In our work, we set
THR to0.2. We arrived at this value for THR based on our expe-
rience and discussions with domain experts. In our experiments, inaddition, we obtained and evaluated the results for THR=0.15
andTHS=0.25 and showed that our results were not sensitive to
such small changes in THR .
4.4 Test Oracle Cost Estimation
Since we assume that test oracles are evaluated manually, to
compare the fault revealing ability of OD and SLDV (RQ3), weneed to ensure that the test suites used for comparison have thesame (oracle) cost. The oracle cost of a test suite depends on thesize of the test suite and the complexity of input data. The latter in
our work is determined by the number of signal segments (P )o f
each input signal. More precisely, test suites TS={I
1,...I q1}
andTS/prime={I/prime
1,...I/prime
q2}have roughly the same oracle cost if (1)
they have the same size (q 1=q2), and (2) the input signals in TS
andTS/primehave the same number of segments. That is, for every
test input Ii=(sg1,...,sgn)inTS (respectively TS/prime), there ex-
ists some test input Ij=(sg/prime
1,...,sg/primen)inTS/prime(respectively TS )
such that sgkandsg/prime
k(for1≤k≤n) have the same number of
segments. In our experiments described in Section 4.5, we ensure
that the test suites used for comparison of different test generation
algorithms satisfy the above two conditions, and hence, can be usedas a proper basis to compare algorithms.
4.5 Experiment Design
We developed a comprehensive list of Simulink fault patterns
and have made it available at [29]. Examples of fault patterns in-clude incorrect signal data type, incorrect math operation, and in-correct transition condition. We identiﬁed these patterns throughour discussions with senior Delphi engineers and by reviewing theexisting literature on mutation operators for Simulink models [60,
9, 7, 57]. We have developed an automated fault seeding program
to automatically generate 44 faulty versions of CPC, 30 faulty ver-sions of FPC, 17 faulty versions of CC, and 13 faulty versions ofCLC (one fault per each faulty model). In order to achieve diver-
sity in terms of the location and the types of faults, our automation
seeded faults of different types and in different parts of the mod-els. We ensured that every faulty model remains executable (i.e.,no syntax error).
Having generated the fault-seeded models, we performed two
sets of experiments, EXP-I and EXP-II, described below.
EXP-I focuses on answering RQ1 and RQ2 using the 74 faulty
versions of the time-continuous models from Table 1, i.e., CPC
and FPC. We ran the OD algorithm in Figure 3 with vector-based
(O
v) and feature-based ( Of) objective functions. For each faulty
model and each objective function, we ran OD for 400 sec and
created a test suite of size qwhereqtook the following values: 3,
5, and10. We chose to examine the fault revealing ability of small
test suites to emulate current practice where test suites are small sothat the test results can be inspected manually. We repeated OD 20times to account for its randomness. Speciﬁcally, we sampled 444different test suites and repeated each sampling 20 times (i.e., in
total, 8880 different test suites were generated for EXP-I). Overall,
EXP-I took about 1000 hours in execution time on a notebook with
a 2.4GHz i7 CPU, 8 GB RAM, and 128 GB SSD.
EXP-II answers RQ3 and is performed on the SLDV -compatible
subject models from Table 1, i.e., CC and CLC. To answer RQ3,
we compare the fault revealing ability of the test suites generated
by SLDV with that of the test suites generated by OD. We giveSLDV and OD the same execution time budget (120 sec in our ex-
periment). This time budget was sufﬁcient for SLDV to achieve a
high level of structural coverage over the subject models. Further,we ensure that the generated test suites have the same test oraclecost. Speciﬁcally, for each faulty model M, we ﬁrst use SLDV to
generate a test suite TS
Mbased on the decision coverage criterion
within the time allotted (120 sec). We then apply OD to Mto gen-
erate a test suite TS/prime
Msuch that TS MandTS/prime
Mhave the same
test oracle cost (see Section 4.4). We have implemented a Matlabscript that enables us to extract the size of the test suites as well as
the number of input signal segments for each individual test input
ofTS
M. Further, we have slightly modiﬁed the OD algorithm in
Figure 3 so that it receives as input the desired number Pof sig-
nal segments for each input signal and it does not modify Pduring
search. Finally, we note that while SLDV is deterministic and isexecuted once per input model, OD is randomized, and hence, wererun it 20 times for each faulty model.
5. RESULTS AND DISCUSSIONS
This section provides responses, based on our experiment design,
for research questions RQ1 toRQ3 described in Section 4.
RQ1 (Sanity). To answer RQ1, we ran EXP-I, and further, in or-
der to compare with random testing, for each faulty version, werandomly generated test suites with size 3, 5 and 10. We ensuredthat each test suite generated by random testing has the same ora-cle cost as the corresponding test suites generated by the OD algo-
rithm. Moreover, similar to OD, we reran random testing 20times.
Figures 4(a) to (c) compare the fault revealing ability of random
testing and OD with the objective functions O
fandOv. Each dis-
tribution in Figures 4(a) to (c) contains 74 points. Each point relates
to one faulty model and represents either the average quantitive ora-
601FR (THR=0.2)QO
FR (THR=0.15)
FR (THR=0.25)0.01.0
0.5
FR (THR=0.2)
R0.01.0
0.5QO
FR (THR=0.15)0.01.0
0.5
FR (THR=0.25)
OD(O  ) f OD(O  ) v 
0.01.0
0.5ROD(O  ) f OD(O  ) v 
ROD(O  ) f OD(O  ) v 
0.01.0
0.5
ROD(O  ) f OD(O  ) v ROD(O  ) f OD(O  ) v ROD(O  ) f OD(O  ) v 0.01.0
0.5ROD(O  ) f OD(O  ) v ROD(O  ) f OD(O  ) v 
0.01.0
0.5
0.01.0
0.5
(a) Average QO and FR values for q=3 (for 74 faulty models)
(b) Average QO and FR values for q=5 (for 74 faulty models)
(c) Average QO and FR values for q=10 (for 74 faulty models)
FR (THR=0.2)QO
FR (THR=0.15)
FR (THR=0.25)0.01.0
0.5
ROD(O  ) f OD(O  ) v ROD(O  ) f OD(O  ) v ROD(O  ) f OD(O  ) v ROD(O  ) f OD(O  ) v 0.01.0
0.5
0.01.0
0.5
0.01.0
0.5
00
00
55
Figure 4: Boxplots comparing average quantitative oracle val-
ues (QO ) and fault revealing measures ( FR) of OD (with both
diversity objectives) and random test suites for different thresh-
olds and different test suite sizes.
cleQO or the average fault revealing measure FR over 20 different
test suites with a ﬁxed size and obtained by applying a test gener-
ation algorithm to that faulty model. Note that the FR values are
computed based on three different thresholds THR of0.2,0.15,
and0.25. For example, a point with (x = R) and (y = 0.104) in
theQO plot of Figure 4(a) indicates that the 20 different random
test suites with size 3 generated for one faulty model achieved anaverageQO of0.104. Similarly, a point with (x = OD(O
f)) and (y
= 0.65) in any of the FR plots of Figure 4(b) indicates that among
the 20 different test suites with size 5 generated by applying ODwith objective function O
fto one faulty model, 13 test suites were
able to reveal the fault (i.e., FR=1 ) and 7 could not reveal that
fault (i.e., FR=0 ).
To statistically compare the QO andFR values, we performed
the non-parametric pairwise Wilcoxon Pairs Signed Ranks test [11],and calculated the effect size using Cohen’s d[15]. The level
of signiﬁcance (α ) was set to 0.05, and, following standard prac-
tice,dwas labeled “small” for 0.2≤d<0.5, “medium” for
0.5≤d<0.8, and “high” for d≥0.8[15].
Testing differences in the average QO andFR distributions, for
all the three thresholds and with all the three test suite sizes, showsthat OD with both objective functions O
fandOvperforms sig-
niﬁcantly better than random test generation. In addition, for allthe comparisons between OD and random, the effect size is con-sistently “high” for OD with O
fand “medium” for OD with Ov.
To summarize, the fault revealing ability of OD outperforms that of
random testing.
RQ2 (Comparing OfwithOv). The results in Figure 4 compare
the average QO andFR values for the feature-based, OD(O f), and
the vector-based, OD( Ov), output diversity algorithms. As for the
QO distributions, the statistical test results indicate that OD(O f)
performs signiﬁcantly better than OD(O v) for test suite sizes 3 and
5 with a “small” effect size. For test suite size 10, there is no sta-
tistically signiﬁcant difference, but OD(O f) achieves higher mean
and median QO values compared to OD(O v). As for the FR distri-
butions, the improvements of OD(O f) over OD( Ov) are not statis-
tically signiﬁcant. However, for all the three thresholds and with all
the test suite sizes, OD(O f) consistently achieves higher mean and
medianFR values compared to OD(O v). Speciﬁcally, with thresh-
old0.2, the average FR is .33, .35 and .48 for OD(O f), and .23, .240.01.0
0.5
FR (THR=0.2)
SLDV OD0.01.0
0.5QO
SLDV OD
FR (THR=0.15)0.01.0
0.5
SLDV OD
FR (THR=0.25)0.01.0
0.5
SLDV OD
Figure 5: Boxplots comparing quantitative oracle values (QO )
and fault revealing abilities of OD and SLDV for different
thresholds.
and .36 for OD(O v) for test suite sizes 3,5, and10, respectively.
That is, across all the faults and with all test suite sizes, the aver-
age probability of detecting a fault is about %10 higher when we
use OD(O f) instead of OD(O v).To summarize, the fault revealing
ability of the OD algorithm with the feature-based diversity objec-tive is higher than that of the OD algorithm with the vector-based
diversity objective.
RQ3 (Comparison with SLDV) . To answer RQ3, we used the bet-
ter diversity objective from RQ2 (i.e., OD with the feature-based
diversity objective) and performed EXP-II on30faulty models of
CC and CLC. We evaluate the fault revealing ability of SLDV and
OD by comparing the quantitative oracle QO and the fault reveal-
ing measure FR values obtained over these 30 faulty models. In
addition, we investigate if any of SLDV and OD subsumes the other
technique (fault revealing subsumption). That is, we determine if
any of OD and SLDV does not ﬁnd any additional faults missedby the other technique. Finally, we compare the structural cover-age achieved by each of SLDV and OD over these 30 faulty models.We report coverage results for two reasons: (1) We conﬁrm our ear-
lier claim that with the timeout of 120 sec used in EXP-II, SLDV
has been able to achieve high structural coverage. (2) We provide
evidence that achieving higher structural coverage does not neces-sarily lead to better fault revealing ability.
Comparing fault revealing ability. We computed QO andFR
with three THR values of 0.15,0.20, and0.25 over the test suites
generated by OD and SLDV . Figure 5 compares the distributionsobtained for the 30 faulty models of CC and CLC. Recall that
SLDV is deterministic, and OD is randomized. So, in Figure 5,each point in distributions related to SLDV shows the value of QO
orFR obtained for one and the only one test suite generated by
SLDV for one faulty model. In contrast, each point in distributions
related to OD shows the average value ofQO orFR for 20 differ-
ent test suites generated by OD for each faulty model. Further, for
each faulty model, SLDV yields a FR value of one or zero, respec-
tively indicating whether SLDV reveals the fault or not. However,for OD, we compute an average FR value over 20 different runs,
which is between zero and one, indicating an estimated probabilityfor OD to reveal a fault.
As shown in Figure 5, the QO andFR values obtained for SLDV
are very small compared to those obtained by OD . Testing dif-
ferences in QO andFR distributions with all the three thresholds
shows that OD performs signiﬁcantly better than SLDV . In addi-tion, for all the four comparisons depicted in Figure 5, the effectsize is “high”.
Fault revealing subsumption. Tables 2 and 3 compare perfor-
mance of OD and SLDV for individual faults. Speciﬁcally, Table 2shows, for each faulty model, the distribution of QO values ob-
tained by OD and the single value of QO obtained by SLDV . Note
that¯xshows the mean, and Q
1,Q2, andQ3refer to the three quar-
tiles of the QO distribution obtained by 20 different runs of OD
(i.e.,Q1,Q2, andQ3are the 25th, 50th, and 75th percentiles, re-
spectively). In addition, we report (as denoted by Pin Table 2)
the percentages of OD runs that achieve a QO value greater than
or equal to that obtained by SLDV . For example, for faults 1, 2,
602Table 2: Quantitative oracle QO distributions for OD and sin-
gleQO values for SLDV per each faulty model.
OD SLDV1
¯x
Q1
Q2
Q30.631
0.566
0.628
0.7050.013OD SLDV2
00
0
00OD SLDV3
OD SLDV4
OD SLDV5
0.012OD SLDV6
OD SLDV7
OD SLDV8
OD SLDV9
¯x
Q1
Q2
Q3OD SLDV10
OD SLDV11
OD SLDV12
OD SLDV13
OD SLDV14
OD SLDV15
OD SLDV16
OD SLDV17
¯x
Q1
Q2
Q3OD SLDV18
OD SLDV19
OD SLDV20
OD SLDV21
OD SLDV22
OD SLDV23
OD SLDV24
OD SLDV25
¯x
Q1
Q2
Q3OD SLDV26
OD SLDV27
OD SLDV28
OD SLDV29
OD SLDV300.011
0
0
0.00200.002
0.001
0.002
0.00300.3230.246
0.347
0.4620.5810.575
0.589
0.5960.0130.228
0
0.2480.425
00.1750.027
0.043
0.1860
0.913
0.81
0.925
0.9870.0230.356
0.066
0.345
0.6650.0120.651
0.374
0.762
0.9250.0110.238
0.008
0.232
0.4490.0120.512
0.405
0.579
0.5790.0110.082
0
0.006
0.180.0110.119
0
0.062
0.2570.0120.306
0.141
0.032
0.4610
0.102
0.041
0.061
0.1990.0070.733
0.716
0.744
0.7630.0820
0
0
0000
0
000.239
0.224
0.25
0.250.1580.250.25
0.25
0.250.250
0
0
000.7560.752
0.765
0.7690.082
0.76
0.753
0.766
0.7680.1350
0
0
000.764
0.762
0.767
0.770.1850
0
0
000.395
0.381
0.397
0.41200.332
0.1
1
0.4420.4450P
P
P
P1.0 0.3 1.0 1.0
0.4
1.00.9 1.0 0.65
1.0 0.85 0.85 0.75 1.0 0.55 1.0
1.0 1.0
1.0 1.0 1.0 1.01.0 1.0 1.0 1.0 1.0
1.0 1.01.0
Table 3: The number of fault revealing runs of OD (out of 20)
for our 30 faulty models, and the fault(s) that SLDV is able to
ﬁnd with a threshold (THR )o f0.2.
Faults 123456789 1 0 123456789 2 0 123456789 3 0
20
SLDVOD 0 0 0 1620 11 5 20 14 17 11 20 4 5 14 2 20 0 0 0 20 20 20 00 2 0 0 1 5 1 5
and 3 in Table 2, 100%,100% and30% of the OD runs, respec-
tively, yield QO values that are not worse than those generated by
SLDV . Table 3 shows, for each faulty model and when consideringFR with threshold 0.2, whether SLDV is able to identify the fault
or not. We depict detection of a fault by SLDV with /squaresolid. Further,
the table shows, out of the 20 runs, how many times OD is able toﬁnd the fault. Note that the results for the thresholds 0.15 and0.25
were similar to those in Table 3.
Based on Table 2, six faults go undetected by both SLDV and
OD irrespective of the threshold value (i.e., for six faults, we haveQO=0 for both SLDV and all OD runs). There is no fault that
SLDV can possibly detect (QO > 0) but OD cannot. For 23 faults,
all 20 runs of OD yield results that are at least as good as those ofSLDV (P =1 ). For all the faults, the average of QO obtained
by OD (denoted by ¯x) is higher than the value of QO obtained by
SLDV . Finally, SLDV totally fails to detect faults 3, 4, 7, 8, 16,
29, and 30, while some runs of OD are able to identify these seven
faults. Based on Table 3, SLDV identiﬁes only one fault with athreshold of 0.2, and that particular fault is also detected by all the
20 runs of OD. The results for thresholds 0.15 and0.25 are similar.
Comparing coverage. Figure 6 compares the structural coverage
percentages (i.e., decision coverage) achieved by test suites gener-ated by OD and SLDV over the faulty models of CC and CLC. Asbefore, the distribution for SLDV shows the percentages of struc-
tural coverage achieved by individual test suites generated for in-
dividual faulty models, while the distribution for OD shows theaverage of structural coverage percentages obtained by 20 differentruns of OD for each faulty model. As shown in the ﬁgure, SLDV
was able to achieve, on average, a coverage of 94% for CC models
and85% for CLC models. In contrast, OD achieved, on average,
CoverageCC
100%
50%CLC
SLDV OD84%94%
SLDV OD75%85%
Figure 6: The percentages of branch (decision) coverage
achieved by OD and SLDV over the faulty versions of CC andCLC subject models.
01(a) OD Input example 
1
0(c) SLDV Input example  
0.8
0.0701000
0 5.0 10.0
01
0 0.01 0.02 0.03 0.04 0.050 5.0 10.0
0 5.0 10.0(b) OD Output example 
Time (s) Time (s)
Time (s) Time (s)(d) SLDV Output exampleSet Input Set Input
Throttle OutputThrottle Output500
Faulty Model Output
Ground Truth Oracle
Figure 7: Examples of test inputs and output signals generated
by SLDV and OD algorithm.
a coverage of 84% for CC and 75% for CLC. In addition, SLDV
has been able to cover 29 out of the 30 fault-seeded blocks, and
OD covered 28 out of the 30 fault-seeded blocks. This shows thatwithin 120 sec, SLDV had sufﬁcient time to cover the structureof the 30 faulty models (only one fault-seeded block was missed).
Indeed, for 22 out of 30 faults, SLDV terminated before the time-
out period elapsed. Hence, by increasing the execution time, it isunlikely that SLDV’s fault revealing ability would be impacted.
In summary, our comparison of SLDV and OD shows that: (1) For
both studied models, OD is able to reveal signiﬁcantly more faultscompared to SLDV . (2) OD subsumes SLDV in revealing faults:Any fault identiﬁed by SLDV is also identiﬁed by OD. (3) SLDVwas able to cover a large part of the underlying models within the
given timeout period (i.e., 29 out of the 30 fault-seeded blocks),
and further, it achieved slightly higher decision coverage over studysubjects compared to OD. However, covering a fault does not nec-essarily lead to detecting that fault. In particular, SLDV was ableto reveal only one out of the 29 faults that it could cover. (4) Fi-
nally, our results on comparing SLDV and OD are not impacted
by small modiﬁcations in the threshold values used to compute thefault revealing measure FR .
Discussion. Why does SLDV perform poorly compared to OD? Our
results in RQ3 show that, compared to the output diversity (OD)
algorithm, SLDV is less effective in revealing faults in Simulinkmodels. In our experiment, even though test suites generated bySLDV cover most faulty parts of the Simulink models, the outputs
produced by these test suites either do not deviate or only slightly
deviate from the ground truth oracle, hence yielding very small QO
values. In contrast, OD generates test suites with output signalsthat are more distinct from the ground truth oracle. Note that, as
discussed in Section 2, any deviation should exceed some threshold
to be conclusively deemed a failure. For example, Figures 7(b) and
603(d) show two output signals (solid lines) of a faulty model together
with the oracle signals (dashed lines) generated by OD and SLDV ,
respectively. Note that the range of the Y -axis in Figure 7(b) is 1000
times larger than that in Figure 7(d). Hence, the deviation from theoracle in Figure 7(b) is much larger than that in Figure 7(d). Inparticular, the signals in Figures 7(b) and (d) respectively produceQO values of 0.43 and0.01. Therefore, the output in Figure 7(b)
is more fault revealing that the one in Figure 7(d).
Since SLDV is commercial and its technical approach descrip-
tion is not publicly available, we cannot precisely determine thereasons for its poor performance. We conjecture, however, that the
reason for SLDV’s poor fault ﬁnding lies in its input signal gener-
ation strategy. Speciﬁcally, all value changes in the input signalsgenerated by SLDV typically occur during the very ﬁrst simulationsteps, and then, the signals remain constant for the most part anduntil the end of the simulation time. In contrast, changes in the in-
put signals generated by OD can occur at any time during the entire
simulation period. For example, Figures 7(a) and (c) show two ex-amples of input signals generated by OD and SLDV , respectively.In both case, the signal value changes from one to zero. However,
for the signal in Figure 7(a), the change occurs almost in the middleof the simulation (at 6sec), while in Figure 7(c), the change occurs
after the ﬁrst step (at 0.01 sec). Signals in Figures 7(a) and (c) hap-
pen to cover exactly the same branches of the underlying model.
However, they, respectively, yield the outputs in Figures 7(b) and
(d) with drastically different fault revealing ability.
6. RELATED WORK
Modeling is by no means new to the testing and veriﬁcation
community and has already been the cornerstone of a number of
well-studied techniques. In particular, two well known techniques,
model-based testing and model checking, have been previously ap-
plied to test and verify Simulink models. Model-based testing relies
on models to generate test scenarios and oracles for implementation-level artifacts. A number of model-based testing techniques have
been applied to Simulink models with the aim of achieving high
structural coverage or detecting a large number of mutants. For ex-ample, search-based approaches [53, 54], reachability analysis [32,20], guided random testing [42, 43], and a combination of thesetechniques [49, 38, 41, 35, 19, 8] have been previously applied to
Simulink models to generate coverage-adequate test suites. Alter-
natively, various search-based [60, 61] and bounded reachabilityanalysis [9] techniques have been used to generate mutant-killingtest suites from Simulink models. These techniques aim to generate
test suites as well as oracles from models that are considered to be
correct. In reality, however, Simulink models might contain faults.Hence, in our work, we propose techniques to help testing complexSimulink models for which automated and precise test oracles arenot available. Further, even though in Simulink, every variable is
described using signals, unlike our work, none of the above tech-
niques generate test inputs in terms of signals.
Model checking is an exhaustive veriﬁcation technique and has
a long history of application in software and hardware veriﬁca-
tion [13]. It has been previously used to detect faults in Simulink
models [14, 20, 5, 30] by showing that a path leading to an error(e.g., an assertion or a runtime error) is reachable, or by maximizingstructural coverage (e.g., by executing as many paths as possible ina model). To solve the reachability problem or to achieve high cov-
erage, these techniques often extract constraints from the underly-
ing Simulink models and feed the constraints into some constraintsolver or SA T solver. Some alternative techniques [51, 22, 3] trans-late Simulink models into code and use existing code analysis tools
such as Java PathFinder [23] or KLEE [10] to detect faults. Allthese approaches only work for code generation models with lin-
ear behavior and fail to test or verify simulation models with time-continuous behavior. Our approach, however, is applicable to bothsimulation and code generation Simulink models.
Recent work in the intersection of Simulink testing and signal
processing has focused on test input signal generation using evolu-tionary search methods [4, 55, 26, 52]. These techniques, however,assume automated oracles, e.g., assertions, are provided. Since test
oracles are automated, they do not pose any restriction on the shape
of test inputs. In our work, however, we restrict variations in inputsignal shapes as more complex inputs increase the oracle cost. Sim-ilar to our work, the work of [59] proposes a set of signal features.These features are viewed as basic constructs which can be com-
posed to specify test oracles. In our work, since oracle descriptions
do not exist, we use features to improve test suite effectiveness bydiversifying feature occurrences in test outputs.
Our algorithm uses whole test suite generation [18] that was pro-
posed for testing software code. This approach evolves an entire
test suite, instead of individual test cases, with the aim of covering
all structural coverage goals at the same time. Our algorithm, in-stead, attempts to diversify test outputs by taking into account allthe signal features (see Figure 2) at the same time. The notion of
output diversity in our work is inspired by the output uniqueness
criterion [1, 2]. As noted in [2], effectiveness of this criterion de-pends on the deﬁnition of output difference and differs from one
context to another. While in [1, 2], output differences are described
in terms of the textual or structural aspects of HTML code, in our
work, output differences are characterized by signal shape features.
7. CONCLUSIONS
Simulink is a prevalent modeling language for Cyber Physical
Systems (CPSs) and supports the two main CPS modeling goals:automated code generation and simulation, i.e., design time test-
ing. In this paper, we distinguished Simulink simulation and code
generation models and illustrated differences in their behaviors us-ing examples. In contrast to the existing testing approaches that areonly applicable to code generation Simulink models, we proposed
a testing approach for both kinds of Simulink models based on our
notion of feature-based output diversity for signals. Our testing ap-proach is implemented using a meta-heuristic search algorithm thatis guided to produce test outputs exhibiting a diverse set of signalfeatures. Our evaluation is performed using two industrial and two
public domain Simulink models and shows that (1) Our approach
signiﬁcantly outperforms random test generation. (2) Our algo-rithm when used with the feature-based notion of output diversitygenerates higher fault revealing rates compared to when output di-
versity is measured based on the Euclidean distance between signalvectors. (3) Our approach is able to reveal signiﬁcantly more faults
compared to Simulink Design V eriﬁer (SLDV), and further, it sub-sumes SLDV , as it is able to ﬁnd the faults identiﬁed by SLDV witha 100% probability.
In this paper, we showed that the fault-revealing ability of our
output diversity algorithm signiﬁcantly outperforms that of SLDVwhen used with the decision coverage (branch coverage) criterion.We expect no notable changes in our results if we use SLDV with
a stronger coverage criterion, e.g., MC/DC, since the limitations of
SLDV is due to its test generation strategy and not in its ability tocover Simulink models. Nevertheless, in future, we plan to repeatour experiments with MC/DC test suites generated by SLDV .
Acknowledgments
Supported by the Fonds National de la Recherche, Luxembourg(FNR/P10/03 - V eriﬁcation and V alidation Laboratory, and FNR4878364), and Delphi Automotive Systems, Luxembourg.
6048. REFERENCES
[1] N. Alshahwan and M. Harman. Augmenting test suites
effectiveness by increasing output diversity. In ICSE 2012,
pages 1345–1348. IEEE Press, 2012.
[2] N. Alshahwan and M. Harman. Coverage and fault detection
of the output-uniqueness test selection criteria. In ISSTA
2014, pages 181–192. ACM, 2014.
[3] D. Balasubramanian, C. S. Pasareanu, M. W . Whalen,
G. Karsai, and M. Lowry. Polyglot: modeling and analysis
for multiple statechart formalisms. In ISSTA 2011, pages
45–55. ACM, 2011.
[4] A. Baresel, H. Pohlheim, and S. Sadeghipour. Structural and
functional sequence test of dynamic and state-based softwarewith evolutionary algorithms. In GECCO 2003, pages
2428–2441. Springer, 2003.
[5] J. Barnat, L. Brim, J. Beran, T. Kratochvila, and I. R.
Oliveira. Executing model checking counterexamples inSimulink. In TASE 2012, pages 245–248. IEEE, 2012.
[6] E. T. Barr, M. Harman, P . McMinn, M. Shahbaz, and S. Y oo.
The oracle problem in software testing: A survey. TSE,
41(5):507–525, 2015.
[7] N. T. Binh et al. Mutation operators for Simulink models. In
KSE 2012, pages 54–59. IEEE, 2012.
[8] F. Bohr and R. Eschbach. SIMOTEST: A tool for automated
testing of hybrid real-time Simulink models. In ETF A 2011,
pages 1–4. IEEE, 2011.
[9] A. Brillout, N. He, M. Mazzucchi, D. Kroening,
M. Purandare, P . Rümmer, and G. Weissenbacher.
Mutation-based test case generation for Simulink models. In
FMCO 2010, pages 208–227. Springer, 2010.
[10] C. Cadar, D. Dunbar, and D. R. Engler. Klee: Unassisted and
automatic generation of high-coverage tests for complexsystems programs. In OSDI 2008, volume 8, pages 209–224,
2008.
[11] J. A. Capon. Elementary Statistics for the Social Sciences:
Study Guide. Wadsworth Publishing Company, 1991.
[12] D. K. Chaturvedi. Modeling and simulation of systems using
MATLAB and Simulink. CRC Press, 2009.
[13] E. M. Clarke, Jr., O. Grumberg, and D. A. Peled. Model
Checking. MIT Press, 1999.
[14] R. Cleaveland, S. A. Smolka, and S. T. Sims. An
instrumentation-based approach to controller modelvalidation. In MDRAS 2008, pages 84–97. Springer, 2008.
[15] J. Cohen. Statistical power analysis for the behavioral
sciences (rev). Lawrence Erlbaum Associates, Inc, 1977.
[16] K. Czarnecki and U. W . Eisenecker. Generative
programming. Edited by G. Goos, J. Hartmanis, and J. van
Leeuwen, page 15, 2000.
[17] R. France and B. Rumpe. Model-driven development of
complex software: A research roadmap. In FOSE 2007,
pages 37–54. IEEE Computer Society, 2007.
[18] G. Fraser and A. Arcuri. Whole test suite generation. TSE,
39(2):276–291, 2013.
[19] A. A. Gadkari, A. Y eolekar, J. Suresh, S. Ramesh,
S. Mohalik, and K. Shashidhar. Automotgen: Automaticmodel oriented test generator for embedded control systems.InCA V 2008, pages 204–208. Springer, 2008.
[20] G. Hamon, B. Dutertre, L. Erkok, J. Matthews, D. Sheridan,
D. Cok, J. Rushby, P . Bokor, S. Shukla, A. Pataricza, et al.
Simulink Design V eriﬁer - Applying Automated FormalMethods to Simulink and Stateﬂow. In AFM 2008. Citeseer,
2008.
[21] M. P . Heimdahl, L. Duan, A. Murugesan, and
S. Rayadurgam. Modeling and requirements on the physical
side of cyber-physical systems. In TwinPeaks 2013, pages
1–7. IEEE, 2013.
[22] D. Holling, A. Pretschner, and M. Gemmar. 8Cage:
lightweight fault-based test generation for Simulink. In ASE
2014 , pages 859–862. ACM, 2014.
[23]
JPF. Java pathﬁnder tool-set. http://babelﬁsh.arc.nasa.gov/trac/jpf .
[Online; accessed 17-Aug-2015].
[24] J. Krizan, L. Ertl, M. Bradac, M. Jasansky, and A. Andreev.
Automatic code generation from MA TLAB/Simulink for
critical applications. In CCECE 2014, pages 1–6. IEEE,
2014.
[25] E. A. Lee and S. A. Seshia. Introduction to embedded
systems: A cyber-physical systems approach. Lee & Seshia,
2011.
[26] F. Lindlar, A. Windisch, and J. Wegener. Integrating
model-based testing with evolutionary functional testing. In
ICSTW 2010, pages 163–172. IEEE, 2010.
[27] S. Luke. Essentials of metaheuristics, volume 113. Lulu
Raleigh, 2009.
[28] R. Matinnejad, S. Nejati, L. Briand, and T. Bruckmann.
Effective test suites for mixed discrete-continuous stateﬂow
controllers. In ESEC/FSE 2015, 2015.
[29] Matinnejad, Reza. The paper extra resources (technical
reports and the models). https://sites.google.com/site/myicseresources/ .
[30] M. Mazzolini, A. Brusaferri, and E. Carpanzano.
Model-checking based veriﬁcation approach for advancedindustrial automation solutions. In ETF A 2010, pages 1–8.
IEEE, 2010.
[31] P . McMinn, M. Stevenson, and M. Harman. Reducing
qualitative human oracle costs associated with automaticallygenerated test data. In ISSTA 2010, pages 1–4. ACM, 2010.
[32] S. Mohalik, A. A. Gadkari, A. Y eolekar, K. Shashidhar, and
S. Ramesh. Automatic test case generation fromSimulink/Stateﬂow models using model checking. STVR,
24(2):155–180, 2014.
[33] P . Nardi, M. E. Delamaro, L. Baresi, et al. Specifying
automated oracles for Simulink models. In RTCSA 2013,
pages 330–333. IEEE, 2013.
[34] P . A. Nardi. On test oracles for Simulink-like models. PhD
thesis, Universidade de Sao Paulo, 2014.
[35] P . Peranandam, S. Raviram, M. Satpathy, A. Y eolekar,
A. Gadkari, and S. Ramesh. An integrated test generationtool for enhanced coverage of Simulink/Stateﬂow models. InDATE 2012, pages 308–311. IEEE, 2012.
[36] B. Porat. A course in digital signal processing, volume 1.
Wiley New Y ork, 1997.
[37] A. Pretschner, W . Prenninger, S. Wagner, C. Kühnel,
M. Baumgartner, B. Sostawa, R. Zölch, and T. Stauner. Oneevaluation of model-based testing and its automation. InICSE 2005, pages 392–401. ACM, 2005.
[38] Reactive Systems Inc. Reactis Tester.
http://www.reactive-systems.com/simulink-testing-validation.html , 2010.
[Online; accessed 17-Aug-2015].
[39] Reactive Systems Inc. Reactis V alidator.
http://www.reactive-systems.com/simulink-testing-validation.html , 2010.
[Online; accessed 17-Aug-2015].
605[40] D. J. Richardson, S. L. Aha, and T. O. O’malley.
Speciﬁcation-based test oracles for reactive systems. In ICSE
1992, pages 105–118. ACM, 1992.
[41] M. Satpathy, A. Y eolekar, P . Peranandam, and S. Ramesh.
Efﬁcient coverage of parallel and hierarchical stateﬂow
models for test case generation. STVR, 22(7):457–479, 2012.
[42] M. Satpathy, A. Y eolekar, and S. Ramesh. Randomized
directed testing (REDIRECT) for Simulink/Stateﬂow
models. In EMSOFT 2008, pages 217–226. ACM, 2008.
[43] S. Sims and D. C. DuV arney. Experience report: the Reactis
validation tool. SIGPLAN, 42(9), 2007.
[44] The MathWorks Inc. Building a Clutch Lock-Up Model.
http://nl.mathworks.com/help/simulink/examples/
building-a-clutch-lock-up-model.html?refresh=true . [Online; accessed
17-Aug-2015].
[45] The MathWorks Inc. C Code Generation from Simulink.
http://nl.mathworks.com/help/dsp/ug/generate-code-from-simulink.html .
[Online; accessed 17-Aug-2015].
[46] The MathWorks Inc. Modeling a Fault-Tolerant Fuel Control
System. http://nl.mathworks.com/help/simulink/examples/
modeling-a-fault-tolerant-fuel-control-system.html . [Online; accessed
17-Aug-2015].
[47] The MathWorks Inc. Simulink.
http://www.mathworks.nl/products/simulink . [Online; accessed
17-Aug-2015].
[48] The MathWorks Inc. Simulink Deign V eriﬁer Cruise Control
Test Generation. http://nl.mathworks.com/help/sldv/examples/
extending-an-existing-test-suite.html?prodcode=DV&language=en .
[Online; accessed 17-Aug-2015].
[49] The MathWorks Inc. Simulink Design V eriﬁer.
http://nl.mathworks.com/products/sldesignveriﬁer/?refresh=true . [Online;
accessed 17-Aug-2015].
[50] The MathWorks Inc. Types of Model Coverage.
http://nl.mathworks.com/help/slvnv/ug/types-of-model-coverage.html .
[Online; accessed 17-Aug-2015].[51] R. V enkatesh, U. Shrotri, P . Darke, and P . Bokil. Test
generation for large automotive models. In ICIT 2012, pages
662–667. IEEE, 2012.
[52] B. Wilmes and A. Windisch. Considering signal constraints
in search-based testing of continuous systems. In ICSTW
2010, pages 202–211. IEEE, 2010.
[53] A. Windisch. Search-based testing of complex simulink
models containing stateﬂow diagrams. In ICSE 2009, pages
395–398. IEEE, 2009.
[54] A. Windisch. Search-based test data generation from
stateﬂow statecharts. In GECCO 2010, pages 1349–1356.
ACM, 2010.
[55] A. Windisch, F. Lindlar, S. Topuz, and S. Wappler.
Evolutionary functional testing of continuous control
systems. In GECCO 2009, pages 1943–1944. ACM, 2009.
[56] I. H. Witten, E. Frank, and M. A. Hall. Data Mining:
Practical Machine Learning Tools and Techniques. Elsevier,2011.
[57] Y . F. Yin, Y . B. Zhou, and Y . R. Wang. Research and
improvements on mutation operators for Simulink models. In
AMM 2014, volume 687, pages 1389–1393. Trans Tech Publ,
2014.
[58] J. Zander, I. Schieferdecker, and P . J. Mosterman.
Model-based testing for embedded systems. CRC Press,2012.
[59] J. Zander-Nowicka. Model-based testing of real-time
embedded systems in the automotive domain.
Fraunhofer-IRB-V erlag, 2008.
[60] Y . Zhan and J. A. Clark. Search-based mutation testing for
Simulink models. In GECCO 2005, pages 1061–1068. ACM,
2005.
[61] Y . Zhan and J. A. Clark. A search-based framework for
automatic testing of MA TLAB/Simulink models. JSS,
81:262–285, 2008.
606