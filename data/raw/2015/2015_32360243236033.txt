One Size Does Not Fit All: An Empirical Study of Containerized
Continuous Deployment Workflows
Yang Zhang
National University of Defense Technology, China
yangzhang15@nudt.edu.cnBogdan Vasilescu
Carnegie Mellon University, USA
vasilescu@cmu.edu
Huaimin Wang
National University of Defense Technology, China
hmwang@nudt.edu.cnVladimir Filkov
DECAL Lab, University of California, Davis, USA
filkov@cs.ucdavis.edu
ABSTRACT
Continuousdeployment(CD)isasoftwaredevelopmentpractice
aimedatautomatingdeliveryanddeploymentofasoftwareproduct,
following any changes to its code. If properly implemented, CD to-
getherwithotherautomationinthedevelopmentprocesscanbring
numerous benefits, including higher control and flexibility over
releaseschedules,lowerrisks,fewerdefects,andeasieron-boarding
of new developers. Here we focus on the (r)evolution in CD work-
flowscausedby containerization ,thevirtualizationtechnologythat
enablespackaginganapplicationtogetherwithallitsdependencies
andexecutionenvironment inalight-weight,self-containedunit,
of which Docker has become the de-facto industry standard. There
are manyavailable choices for containerizedCD workflows, some
moreappropriatethanothersforagivenproject.Owingtocross-
listingofGitHubprojectsonDockerHub,inthispaperwereport
onamixed-methodsstudytoshedlightondevelopers’experiencesandexpectationswithcontainerizedCDworkflows.Startingfroma
survey, we explore the motivations, specific workflows, needs, and
barriers with containerized CD. We find two prominent workflows,
basedontheautomatedbuildsfeatureonDockerHuborcontinu-
ous integration services, with different trade-offs. We then propose
hypotheses and test them in a large-scale quantitative study.
CCS CONCEPTS
•Software and its engineering →Software maintenance tools ;
KEYWORDS
Continuous Deployment, Containerization, Docker, GitHub
ACM Reference Format:
Yang Zhang,Bogdan Vasilescu, HuaiminWang, and VladimirFilkov. 2018.
OneSizeDoesNotFitAll:AnEmpiricalStudyofContainerizedContinuous
DeploymentWorkflows.In Proceedingsofthe26thACMJointEuropeanSoft-
ware Engineering Conference and Symposium on the Foundations of Software
Engineering (ESEC/FSE ’18), November 4–9, 2018, Lake Buena Vista, FL, USA.
ACM,NewYork,NY,USA, 12pages.https://doi.org/10.1145/3236024.3236033
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ESEC/FSE ’18, November 4–9, 2018, Lake Buena Vista, FL, USA
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5573-5/18/11 ...$15.00
https://doi.org/10.1145/3236024.32360331 INTRODUCTION
Continuous deployment (CD), also referred to as continuous de-
livery,1is the fast-paced, automation-heavy software engineering
approachinwhichteamsworkinshortiterationstoproducesoft-
warethatisdeployable(productionready)atanytime[ 31].CDhas
promisedtodeliverarevolutionoverthetwice-yearlyorsostan-
dard for software releasing, through greater control and flexibility
overfeaturereleases,incrementaldeploymentofvalue,lowerrisks,
fewer defects, easier on-boarding of new developers, less off-hours
work,andaconsiderableuptickinconfidence[ 45].Itisnotsurpris-
ingthenthatthePerforcereport[ 49]foundthat65%ofsoftware
developers, managers, and executives have used CD. Moreover,
the “State of DevOps” survey [ 52], with 3,200 participants from
aroundtheworld,foundCDpositivelyimpactsITperformanceand
negatively impacts deployment pain.
On the other hand, industry reports and academic studies have
foundthatimplementingtheautomation, i.e.,pipelines(workflows),
needed to properly provide CD is challenging and takes a lot of
time and tuning, due to the many moving parts and the specific
needs of each project or organization [ 8,31,49,56]. A prototypical
CDworkflowinvolvesacontinuousintegration(CI)service,like
Jenkins [60] and Travis [ 41], which is triggered by new changes
in the version control system to build, test, and deploy the pack-aged application. Many separate and often redundant tools canbe pipelined to assemble a CD system for a project or organiza-tion. How-to guides exist [
2,30], and turn-key solutions, mainly
commercial,arealsoavailable[ 34].Previousstudieshavelooked
atimplementationsofCDinindividualorganizations[ 45,55,63]
and one study has compared implementations in 15 different or-
ganizations [ 56]. It is commonly reported in those studies that the
benefits gained are many but that implementing CD takes time.
Asmore andmore experienceis beinggainedwith differentways
to implement CD, it has become obvious that different solutions,
i.e.,workflows,arepossible,andthattheymayfitdifferentneeds.
Stateddifferently,choosingoneavailableworkflowvsanothermay
make a big difference to a specific project. Thus, maps of project
needs onto prototype CD workflows would be considered helpful.
Here we focus on studying the (r)evolution in CD workflows
causedby containerization ,thevirtualizationtechnologythaten-
ablespackaginganapplicationtogetherwithallitsdependencies
andexecutionenvironmentinalight-weight,self-containedunit.
1Although technically continuous deployment encompasses continuous delivery, the
two terms tend to be used interchangeably in practice by developers. We don’t distin-
guish between the two here; we define the workflows precisely, below.
295
ESEC/FSE ’18, November 4–9, 2018, Lake Buena Vista, FL, USA Yang Zhang, Bogdan Vasilescu, Huaimin Wang, and Vladimir Filkov
Containerization has transformed CD workflows, promising ad-
ditional speedups and higher level of abstraction. Containers (orimages) encapsulating a packaged application ready for deploy-
ment can be specified declaratively, versioned together with the
restoftheinfrastructurecode,builtautomatically,andpublished
to some cloud-based registry for easy access by users and other
applications.Amongcontainerizationtechnologies, Dockercontain-
ershave become the de-facto industry standard. Since inception in
2013,Dockercontainershavebeendownloaded29B+times2and
their usage is spreading rapidly; e.g., the “Annual Container Adop-
tion”report[ 50]foundthat79%ofcompanieschoseDockerastheir
primary container technology. Thus, studying Docker container
usage is relevant to most of the containerization community.
In this paper we seek to aid in deciding how to appropriately
choosebetweenDocker-enabledCDworkflows,bycollatinglessons
learned and offering data-driven evidence from different CD imple-
mentations in open source software (OSS) projects. We focus on
OSSprojectsonGitHub,thelargestpubliccoderepositoryhost,and
Docker Hub3, the most popular cloud-based registry for Docker
containers,whichhostsover2millionDockerimagerepositories
asofMarch2018;94%oftheseimagesarelinkedtoaGitHubrepos-
itory, enabling the data mining for our study.
As with any CD pipeline, developers have considerable freedom
todefinecustomDocker-enabledworkflows,choosing, e.g.,whatCI
service to use, what to include in the images, and how to automate
their construction and publication. Starting from these two public
sourcesofdata,GitHubandDockerHub,inthispaperwereport
on a mixed-methods study to explore the following questions:
RQ1:Whatmotivations,unmetneeds,andbarriersdodevelopersface
with their Docker-enabled CD workflows? (see §3)
RQ2:WhatarethedifferentialbenefitsamongspecificDocker-enabled
CD workflows? (see §4)
Thefirstpartofourstudyisamulti-stage150+developersurvey,
theresultsofwhich revealedsev eralcommonCDimplementations
in projects publishing images to Docker Hub: while some projects
writetheirownscriptstodeployimages,mostuseavailabletools
which fittogether withoutextensive retoolingwith their existing
solutions, e.g.,standardCIserviceslikeJenkins,Travis,andCircleCI.
We found that two Docker image deployment workflows4were
most prominent:
(1) aDocker Hub auto-builds Workflow (denoted DHW), where the
registry itself builds the image automatically whenever GitHub
source files change; and
(2)aCI-basedWorkflow (denotedCIW),whereCItoolsbuildimages
during the build and test stage, then publish to Docker Hub.
Additionally,thesurveyanswersalsogeneratedhypothesesre-
latedtospecificCDworkflowoutcomes:releasefrequency,build
results,stability,andbuildlatency,suchas imagebuildlatencytends
to worsen over time , andCIW tends to have higher image release
frequency than DHW (see §3.4and §3.6). To test these hypothe-
ses,inthesecondpartofourstudyweperformeddatagathering
andstatisticalmodeling.Wecollecteddatafrom1,125projectson
2https://www.docker.com/company , as of March 2018.
3https://hub.docker.com/
4TheseresembletheGitHub pushandpull-based models:theCIworkflow“pushes”
the Docker image, while the DH workflow “pulls” it.Docker Hub, measuring build latency, release frequency, config
filesizesandchanges,commitsizes,testingtimes,anddiscussion
lengths. The above four outcomes (release frequency, build results,
stability, and build latency) were regressed against an extensive set
of variables, fitted to the processed data, and well fitting models
were obtained. In summary, we found that:
•CIW is associated with higher image release frequency than
DHW.Butovertime,thereleasefrequencyofbothworkflows
tends to drop;
•Image build latency tends to increase over time. Interestingly,
CIW tends to have shorter build latency than DHW;
•Imagebuildconfigurationstabilitytendstoincreaseovertime.
But CIW tends to have lower Dockerfile stability than DHW;
•CIW is associated with more image build errors than DHW;
•There are notable differences withinCIWs, not just between the
DH and CI workflows.
Our survey questions, scripts, and data are online at https://
github.com/yangzhangs/cd_replication .
2 BACKGROUND AND RELATED WORK
Docker and Docker Hub . Docker ( https://www.docker.com )i s
anOSSprojectimplementingoperatingsystem-levelvirtualization;
it builds on many technologies from operating systems research:
LXC[22](LinuxContainers),virtualizationoftheOS[ 7],etc.The
technology is primarily intended for developers to create and pub-
lishcontainers [39]. With containers, applications can share the
same operating system and, whenever possible, libraries and bina-
ries[6].Thecontentofthecontainerisdefinedbydeclarationsin
theDockerfile [40] which specifies the Docker commands and
theorderoftheirexecution.Dockerlaunchesitscontainersfrom
Dockerimage ,whichisaseriesofdatalayersontopofabaseim-
age [23]. When developers make changes to a container, instead of
directly writing the changes to the image of the container, Docker
adds an additional layer with the changes to the image [ 42]. Since
productionenvironment replicascanbe easilymadeinlocal com-
puters, developers can test their changes in a matter of seconds.
Also,changestothecontainerscanbemaderapidlyasonlyneeded
sections are updated following a change. This makes Docker very
suitable for CI and CD implementations [ 1].
ExistingstudiesrelatedtoDockercontainershavetypicallyfo-
cused on performance aspects [ 43], security vulnerabilities [ 20],
and basic usage [ 17]. In particular, Cito et al.[17] found that de-
ployment pipelines are, in most part, structured in multiple and
consecutive phases, but that, thus far, there has been little research
on Docker-enabled workflows in CD processes. A recent study has
pointedoutthatsoftwareengineeringtaskscanbenefitfromthe
mining of container image repositories, like Docker Hub [ 65].
DockerHubisDocker’scloud-basedregistry,containing2,018,057
Docker images as of March 2018. Docker Hub provides GitHub in-
tegrationaswellassomefeaturedtools, e.g.,automatedbuilds[ 29],
whichallowdeveloperstobuildtheirimagesautomaticallyfrom
GitHubsources[ 7].ThebuilddataandDockerfileinformationon
Docker Hub is available for mining, if the repositories are public.
CDandDeploymentPipelines .ContinuousDeployment(CD)is
apracticeinwhichincrementalsoftwareupdatesaretested,vetted,
anddeployedtoproductionenvironments[ 54].CDleveragesthe
296One Size Does Not Fit All: An Empirical Study of Containerized ... ESEC/FSE ’18, November 4–9, 2018, Lake Buena Vista, FL, USA
CI[41]processandextendsittoincludetheimmediatedeployment
ofsoftware[ 18].Humble etal.[32]reportedonanearlyoverviewof
CD practices and introduced several guidelines. Vassallo et al.[63]
investigated CD practices at ING [ 62], focusing on their impact on
the development process and management of technical debt. Savor
et al.[56] reported on anempirical study conducted in two high-
profile Internet companies; they found that the adoption of CD
doesnotlimitscalabilityintermsofproductivityinanorganizationeven if the system grows in size and complexity. In summary, prior
work mostly focused on defining CD and describing particular
implementations in a small sample of organizations or projects.
TheemergenceofCDalsoincreasestheimportanceofdeploy-
ment pipelines [ 36]. A deployment pipeline should include explicit
stages,e.g., building and packaging, to transfer code from a source
repository to the production environment [ 3]. In each stage, devel-
opers can choose different tools or services, which, in turn, will
producedifferentCDworkflows.It isbecomingincreasinglyclear
thatonesizedoesnotfitall,withrecentstudiesbyShahin etal.[58],
Zhaoet al.[66], or Widder et al.[64] showing that the choice of
CI/CD tools and infrastructures is highly context dependent.
Despite the importance of containerization and Docker in in-
dustry,tothebestofourknowledge,noexistingresearchhasin-
vestigated the barriers and needs developers face when using con-
tainerized CD workflow, or what trade-offs developers must make
when choosing different CD workflows. With this paper, we at-
tempttoaddressthisliteraturegap,andprovideinsightsintothe
Docker-enabled CD workflows in the OSS community.
3 DEVELOPER SURVEY
Our study starts with a qualitative exploration of developers’ ex-
periences and expectations using Docker containers as part of CD
workflows( RQ1),forwhichweconductedasurvey.Ourgoalswere:
(1)togainunderstandingof howpeopleusecontainerizedCD,fo-
cusingonwhat motivations ,barriers,andunmetneeds developers
face with their Docker-enabled CD workflows; and (2) to generate
hypotheses to be tested in a follow-up quantitative study.
3.1 Survey Methods
Survey design and participants. Since little is published about
containerizedCD,wedesignedthesurveybroadly,aroundusecases
and pain points, and ran a pilot to refine the protocol. Questions
wereinspiredmainlybySEliteratureontrade-offsinCI[ 27]and
online discussionsabout CDand Dockercontainers.5Specifically,
the survey included multiple choice and open-ended questions, or-
ganized in four parts: (1) motivations for doing CD; (2) current CD
tools and workflows; (3) unmet needs; and (4) barriers and pain
points. We piloted the survey before full deployment. To obtaindeveloper contact information, we first mined all projects with
DockerimageshostedonDockerHub,thathadsourcecodereposi-
tories on GitHub.6Using the GitHub API, we then identified those
projects’ownersandtheircontacts,sampled1,000,andsentthem
email invitations with a link to the online form. We only surveyed
project owners as we felt they would be most familiar with the
overall development of the project.
5From the Docker forum, https://forums.docker.com
6Following “Source Repository” links on Docker Store pages, https://store.docker.comRespondents and analysis. Within10days,wereceived168re-
sponses,foraresponserateof16.8%,consistentwithothersoftware
engineering online surveys [ 51]. Respondents indicated that their
experienceinOSSwas8.6yearsonaverage(median:7;range1—30),
while their CI/CD experience was 4.3 years on average (median:
3; range 1—20). Additionally, 32 participants replied to our emailinvitations to show their interest in this research and to provide
valuable suggestionsand feedback. Sinceno question wasmanda-
tory, the number of responses per question may vary; we reportactual numbers below. For open-ended questions, we used open
coding[21]intwophases.Duringafirstround,wecarefullyread
thecontentofeachanswerandmarkeditskeywordsorstatements.Later,duringasecondround,weiterativelyaggregatedthedescrip-
tionsandsummarizedthecategories.Oneauthorwasinvolvedin
coding, all authors in discussion and refinements.
3.2 Motivations for Doing CD
We start by gauging developer’s motivationsfor doing CD in gen-
eral (open ended, 140 answers), to evaluate how much our popula-
tion shares CD insights derived in other contexts. We uncovered a
spectrumofmotivationsfordoingCD(Table 1).Next,wecontextu-
alizetheseuncoveredmotivationcategorieswithpriorwork,and
illustrate each with representative quotes.
[M1]CD helps us deploy automatically instead of doing it
manually. We expected that automation is a major concern for
developers, and the survey answers confirmed that. E.g., R79re-
marked“becausetheprojectwillbebuiltautomatically,noneedfor
metobuildtheimageandpushittotheserver ”.Thisisconsistent
with Neely et al.[45], who pointed out that it is important to elimi-
nateallmanualstepsfromabuildinordertoextendCIwithrelease
and deployment automation.
[M2]CD gives us smoother and easier deployments. As per
Fowler [25], CI was presented as a way to avoid painful integra-
tions. CD goes one step further to automate a software release, as
it makes sure the software is production-ready, which provides de-
veloperswitheasierdeployments.E.g., R71responded,“ Continuous
integration to accelerate the development and continuous deployment
to simplify the passes to production reducing the complexity ”.
[M3]CDallowsuskeepourproductionreliable. Benefield[ 4]
reportedthatthedeploymentinfrastructure,coupledwithintensive
automated testing and fast rollback mechanisms, improves release
reliabilityandquality.WithCD,thedeploymentprocessandscriptsaretestedrepeatedlybeforedeploymenttoproduction,whichkeepstheproductionmorereliable.E.g.,
R26remarked,[CDis]“ convenient
and more reliable, less error prone as a manual deployment ”.
[M4]CD makes releasing faster. Chen [8] reportedthat CD al-
lowsdeliveringnewsoftwarereleasestocustomersmorequickly.
ByleveragingassistanceprovidedwithCD,projectteamscanre-
lease once a week on average, or more frequently if desired. Some
ofourrespondentsconfirmedthis; e.g.,R120said,[CDletsthem]
“ship as early as possible ”.
[M5]CD lets us catch errors earlier to minimize failures. By
interviewing developers, Hilton et al.[27] found that the biggest
perceived benefit of CI is early bug detection. So we would expect
thatCDalsohelpscatcherrorsearliertopreventthedeployment
ofbrokencode.Oursurveyresponsesconfirmedthis.E.g., R51said,
297ESEC/FSE ’18, November 4–9, 2018, Lake Buena Vista, FL, USA Yang Zhang, Bogdan Vasilescu, Huaimin Wang, and Vladimir Filkov
Table 1: Developers’ motivation for doing CD. N=140.
Motivation Total Perc.∗
Helps us deploy automatically instead of doing it manually 6042.9%
Gives us smoother and easier deployments 27 19.3%
Allows us to keep our production reliable 2215.7%
Makes releasing faster 14 10.0%
Lets us catch errors earlier to minimize failures 11 7.9%
Can enforce a deterministic workflow 11 7.9%
Lets us spend less time on maintenance and configuration 96.4%
Enhances the testing and validity checking 8 5.7%
Allows us to share our work and get continuous feedback 42.9%
∗One answer may contain multiple codes; percentages need not add up to 100.
[CDhelpsthem]“ findtheproblemassoonaspossible.Avoidbuilding
break that other developer won’t be affected ”.
[M6]CD can enforce a deterministicworkflow. Schermann et
al.[57]foundthatdeploymentworkflows, i.e.,structuringthere-
lease process into multiple and consecutive phases, is widespread.
Their study reported that 68% of the survey respondents use the
sameCDworkflowfordealingwithissuesasforeveryotherchange.
And74%ofrespondentsagreedthattheywouldreleasemorefre-
quently than they actually do by following a specific workflow.
E.g., R115said,[CDgavethem]“ deterministicworkflow ”and R18
said,[theywanttouseCD]“ becauseitcanhighlyimporttherelease
phase. Change in one place will take an effort on other systems ”.
[M7]CD lets us spend less time on maintenance and config-
uration. Developersperceivethatnottreatingconfigurationlike
code leads to a significant number of production issues [ 47]. Devel-
opersusedtospend20%oftheirtimesettingupandfixingtheirtest
environments.CDcanautomaticallysetuptheenvironments[ 8],
which allows developers to spend their effort and time on more
valuableactivities.As R64said,[theyuseCDbecausetheywant]
“to optimize time, focusing on developing the actual application ”.
[M8]CDenhancesthetestingandvaliditychecking. Mantyla
etal.[38]analyzedtheeffectsofmovingfromtraditionaltorapid
releasesonFirefox’ssystemtesting.TheirstudyrevealedthatCD
allows less time for testing activities but enables fast and thorough
investigation of software features. Not surprisingly, this was a
common theme among our survey respondents. E.g., R78noted
that [CD] “ helps with testing, less hassle ”.
[M9]CDallowsustoshareourworkandgetcontinuousfeed-
back.Kruscheet al.[35] reported that with CD, customers can
evaluate the enhancements and provide feedback immediately and
in a continuous way, which improves communication between the
companyanditscustomers.Continuousfeedbackletsdevelopers
spend time developing the right things rather than correcting mis-
takesinfunctionality[ 46].Inoursurvey, R167answeredthat,“ It
helps me share my work with other contributors easily ”, and R156
pointed out that [under CD, they can get] “ fast, reliable feedback ”.
DevelopersreportdoingCDtoreducework,cost,andtimespent
onmaintenanceandconfiguration.TheyalsoreportthatCDhelps
them guarantee quality, consistency, reliability, and enhances
their development process.
3.3 Tools and Workflows
We asked developers which Docker workflows they subscribe to in
theircurrentCDworkflow.MostreporteithertheDockerHubauto-builds (DH; 44.1%) or the CI-based (CI; 34.5%) workflows (Figure 1):Sources & 
DockerfileBuild images with sources Run tests on images
Docker Hub CI servers 
 GitHub
Deploy 
(push images)
DevelopersCodeCD automated pipeline
Docker Hub automated builds (auto-builds)
auto-test
Figure 1: Overview of Docker-enabled CD workflows.
machine: 
  services: 
     - docker 
dependencies: 
  override: 
     - docker info 
     - docker build --rm=false -t circleci/elasticsearch. 
test: 
  override: 
     - docker run -d -p 9200:9200 circleci/elasticsearch; sleep 10 
     - curl --retry 10 --retry-delay 5 -v http://localhost:9200 
deployment: 
  hub: 
    branch:  master 
    commands:  
     - docker login -e $DOCKER_EMAIL -u $DOCKER_USER -p $DOCKER_PASS 
            - docker push circleci/elasticsearch 
Figure 2: An example of Docker settings in CircleCI.
(1)DHWorkflow(DHW) :Usingautomatedbuilds[ 29](auto-builds),
DockerHubcanautomaticallybuildimagesfromGitHubsource
filesandpushthemtothecorrespondingDockerHubrepository.
Whensettingupauto-builds,developerscreatealistofbranches
and tags they want to include in the Docker images. When they
pushcodetoasourcecodebranchforoneofthoselistedimagetags,
the push uses a webhook to trigger a new build, which produces a
Docker image. The built image is then pushed to Docker Hub.
(2)CI Workflow (CIW) : Developers automatically build images
fromsourcecodeusingdockercommandsinsidetheirCIbuilds;the
built image is then pushed to Docker Hub. The CI tools themselves
mostly integrate Dockerservices which,in turn,allow developers
tousedockercommandstobuildanddeployimages.Figure 2shows
an example of a circle.yml file that specifies the standard Elas-
ticSearch Docker image and deploys it to Docker Hub in CircleCI.
CircleCI pre-installs the Docker Engine in the Linux build images,
asspecifiedin circle.yml .Thendeveloperscanusethe“ docker
build” and “ docker push ” commands to build and deploy images.
We also asked respondents who use CIW which specific tools
they use. The top-3 most frequently used CI tools are Travis CI
(65.5%ofrespondents),Jenkins(24.1%),andCircleCI(17.2%).7We
also found that, unexpectedly, 29.3% of respondents used two or
moreCItools simultaneously .Thereasonsmentionedweregeneric,
with the most common being that each CI tool has its respective
jobsortargetprojectsthatitisgoodfor.E.g., R39said,“Youhave
to use the part of CI tools that works best for you. ... So it is really
aboutwhichcombinationallowsthebestmanagement ”.Futurework
should examine the interplay between these seemingly equivalent
and competing tools using qualitative methods.
Inaddition,21.4%ofrespondentsuseotherworkflows.Weasked
them what their CD workflow consists of, and coded their answers
7In GitHub report [ 53]: the Top-3 CI tools are Travis CI, CircleCI, and Jenkins.
298One Size Does Not Fit All: An Empirical Study of Containerized ... ESEC/FSE ’18, November 4–9, 2018, Lake Buena Vista, FL, USA
Table 2: Other workflows. N=36.
Other workflow Total Perc.
Deploy by using other services or software tools 1233.3%
Use custom scripts 9 25.0%
Use both the DH and CI workflows 822.2%
Automatically test with CI but manually build and deploy 7 19.4%
into 4 groups, summarized in Table 2:
[O1]Deploybyusingotherservicesorsoftwaretools .Forex-
ample, R17told us their CD workflow is “ a redhat open shift service
which is running based on kubernetes ”;
[O2]Use custom scripts . For example, R48said their CD work-
flow comprised “ shell scripts written by me ”;
[O3]UseboththeDHandCIworkflows .E.g., R74answered,“ I
have a base image that is auto-built from a different repo by docker-
cloud, and the main image which is built by CircleCI after a green
build, and pushed to docker hub ”;
[O4]Automatically test with CI but manually build and de-
ploy. E.g., R53told us their CD workflow is “ Push changes →Pull
request→Travisconfirmconfigit’sok →ManualbuildsinDockerHub
after QA in my current company ”.
ThereislargeuniformityinDocker-enabledCDworkflows,with
twoprevalentworkflows:DockerHubauto-builds(pull-based)
and continuous integration (push-based).
3.4 Unmet Needs
Weaskeddevelopersaboutunmetneedsandpainpointswiththeir
current CD workflows (open ended, 83 valid responses), and found
that 89.9% of respondents are satisfied with their current workflow.
We coded the remaining answers as listed in Table 3:
[N1]Quickerbuilds peedandhigher throughput. Like R4told
us, in their CD workflow “ one dockerfile takes more than 2 hours
to build and timeouts ”. The CD processing speed will affect the
software release and developers’ work efficiency. As more tests
arewrittenandmoreartifactsareadded,theimagebuildlatency
islikelytoincrease.21.3%ofrespondentsexperiencedincreasing
processing latency in their CD workflows over time, and 17.7%
would change their workflow because of the increasing latency.
[N2]Easier to learn and config. While Docker Hub and CI tools
offeragreatdealofflexibilityinhowtheycanbeused,thisflexibility
still requires a large amount of configuration even for a simple
workflow.Like R109toldus,“sometimes,circleCIconfigandsetup
ispain. Docssometimesdoesn’t help ”. Also,complexconfiguration
wouldaffectthedeveloperscollaboration,like R130said,“Itmay
spent some time to teach your partner use the CD pipeline ”.
[N3]Betterbuildtestingsupport. R27toldus,intheirCDwork-
flow, “Build testing is quite a pain. I had playing docker to build
OpenCV+NodeJS+Cairowhichbreakwhilebuildingtheimage.The
buildprocesscantakeupto20mins.Ifit’sbreak,Ineedtotryother
configuration and rebuild again ”. As we known, CI tools provide
goodtestintegration,thusthebuildtestingneedstobeenhanced
forprojectsthatuseDHworkflow.Like R62said,“ProbablyIneeda
Jenkins or Travis container in the chain to produce more code control
using some unit testing ”.
[N4]Better multi-platform build support. Like R19suggested,
“Forapplications,it’smoreimportanttoprovidemulti-platformbuild ”.Table 3: Developers’ unmet needs. N=83.
Needs Total Perc.
Quicker build s peedandhigher throughput 1821.7%
Easier to learn and config 14 16.9%
Better build testing support 1214.5%
Better multi-platform build support 11 13.3%
More features and tools integrated 1113.3%
More flexibility and control 7 8.4%
Better support for getting info about failures and logs 67.2%
Better security and access controls 6 7.2%
Multi-platform build support is to meet the specific needs of dif-
ferent software development. Like R79said, “Docker hub support
onlyx86_64platformonly.IhopethatARMsupport,likeraspberry
pi, will be added in the future. ... ”.
[N5]More features and tools integrated. Some respondents
would like their CI/CD system to integrate with more features
and tools. Like R81said, “Docker cache still not supported without
big hacks on most CI suites (e.g. Travis) ”.
[N6]More flexibility and control. Like R147told us, “at some
point(we)willwanttouseapipelinewithmorecontrolandflexibility ”.
In some CD workflows, there is still lack of flexibility of builds, i.e.,
the Dockerfile optimization. Like R94said, “Its mainly a Docker-
related drawback: I would love to be able to build Dockerfiles that
have multiple images as base blocks (e.g. FROM Java8, Redis). In our
workflow we alwaysend-up copying dockerfilesfrom other sources
and merging them in one. ”.
[N7]Better support for getting info about failures and logs.
When CD fails, developers need to identify why their CD failed.
Better logging and storing test artifacts would make it easier to
examine failures. But the current CD platforms are still lack of
better support. Like R143said, “DockerHub doesn’t give a whole lot
ofdetailvs.someothersolutions(CodeShip,etc) ”.R60toldustheir
need is “getting info about failures and debugging of broken build ”.
[N8]Better security and access controls. Since CD workflows
have access to the entire source code of a project, security and
accesscontrolsarevitallyimportant.Shu etal.[59]reportedthat
morethan80%ofDockerHubimageshaveatleastonehighseverity
levelvulnerability.Like R111toldus,theirpainpointaboutCDis
the “lack of automatic security upgrades ”.
Developers would like their CD workflows to be both speedy
andsimpletosetupandmaintain.Thiscancausesometension,
since adding configurability tends to increase complexity and
simplification may reduce flexibility.
Based on the previous discussion, we expect that unavoidable
complexityincreases over timein CDworkflowswould slowdown
the developers’ workflows. We hypothesize:
H1.Image release frequency tends to decrease over time.
Inaddition,alackofbettertestinganddebuggingsupportwould
cause developers to write more test scripts or add more complexity
totheirbuildconfigurations,makingtheirimagebuildprocesses
burdensome. So we hypothesize:
H2.Image build latency tends to increase over time.
WithmoreexperiencedoingCDshouldalsocomemorestability
of the Docker image configurations ( i.e., the Dockerfile). After the
initialconfiguration,developersmayneedfeweradditionalchanges
or improvements to their Dockerfiles over time. We hypothesize:
H3.Dockerfile stability tends to increase over time.
299ESEC/FSE ’18, November 4–9, 2018, Lake Buena Vista, FL, USA Yang Zhang, Bogdan Vasilescu, Huaimin Wang, and Vladimir Filkov
3.5 Workflow Evolution
WealsoaskedparticipantsaboutchangestotheirCDworkflows
overtime(openended,71answers).Amongtherespondents,45.8%
report having changed their CD workflows at least once before,
with the common reasons (barriers) given being listed in Table 4.
[B1]Difficult to setup and maintain. Configuration and main-
tenance costs cause many developers to change their workflows.For example,
R119switched their workflow because “ the old CD
pipeline is a little harder to setup. It was necessary to write severalscripts to get everything working properly. The new CD pipeline is
easier to setup and maintain ”.
[B2]MissingfeaturesIneed. Like R37described,theiroldwork-
flowwas “ tooslow andmissingfeatures ”;the poorfeaturesupport
made some developers switch to a different workflow.
[B3]Weak support for automation. Some developers changed
theirworkflowbecausetheiroldworkflowhadweaksupportfor
automation.Like R128toldus,theiroldworkflowcontained“ many
manual steps prone to errors” , while with the new workflow “ every-
thing goes smoothly ”.
[B4]Overlylong buildtimes. Aswefoundearlierwhenasking
aboutCDneeds,buildspeedaffectsthedevelopers’workefficiency.
Some developers changed their workflow due to slow image build
speed. For example, R159told us, in their old CD workflow, “ Cache
wasdroppedwhenthebuildexecuted,andfornogoodreason.Soit
took too much time to build the image ”.
[B5]Morefrictionandfailures. Brittlebuildsmaketheworkflow
unreliable, which cause some developers to change their workflow.
For example, R66said, “we noticed that the builds are not really
reliable since there wasn’t any testing. So we refactored the workflow
to TravisCIwhich isway betterfor testing,but hasdisadvantages in
speed of pushing and handling images on Docker Hub. But with a
little bit of scripting the problems went away ”.
[B6]Experimenting with new tools. Some developers changed
theirworkflowbecausetheywantedtousesomenewtools.Like
R43answered, “ just tried new stuff ”.
[B7]Steep learning curve. Complex processes and unfamiliar
configurations make some developers abandon their old workflow.
Like R76told,theiroldworkflowwas“ hardtolearn,configure,or
plain inefficient ”.
Developers encountered increased complexity, increased latency,
and decreased reliability in previous CD workflows. These barri-
ers caused them to switch to new CD workflows.
In a CIW, CI tools may provide more effective testing support
thaninaDHW,whichhelpsdevelopersfindmoredefectsbefore
deploying.ButaCIWmayrequiremorecomplexconfigurationand
maintenance than a DHW, which increases the likelihood of build
failures. We hypothesize:
H4.CIW tends to have more failed builds than DHW.
Moreover,becauseCIWsrequiremoreconfiguration,itmaytake
more time to run these operations. We hypothesize:
H5.CIW tends to have longer build latency than DHW.
3.6 Specifics of DH and CI Workflows
Weaskedparticipantsformoredetailsontheperceivedadvantages
of DHW or CIW, giving them a choice of 7 predefined answers (Ta-
ble5) plus the option to provide others. We collected 132 answers,Table 4: Barriers with previous CD workflows. N=71.
Barriers Total Perc.
Difficult to setup and maintain 2535.2%
Missing features I need 15 21.1%
Weak support for automation 1318.3%
Overly long build times 10 14.1%
More friction and failures 1014.1%
Experimenting with new tools 6 8.5%
Steep learning curve 45.6%
Table 5: Specific reasons for using a DH or CI workflow.
NDH=74, N CI=58.
Reasons DH workflow CI workflow
Reduce the time spent on setting up 63 (85.2%) 35 (60.3%)
Deploy more frequently 34 (45.9%) 34 (58.6%)
Increase confidence in build quality and results 32 (43.2%) 46 (79.3%)
Less CD processing latency 23 (31.1%) 22 (37.9%)
Allow higher flexibility of builds 17 (23.0%) 13 (22.4%)
Create more visibility into team’s workflow 16 (21.6%) 21 (36.2%)
Convenient custom settings and modifications 14 (18.9%) 17 (29.3%)
74 for DHW and 58 for CIW. For the DHW, the most important
reasongivenwasto reducethetimespentonsettingup (85.2%),
followedbyto deploymorefrequently (45.9%).Respondentsalso
gave other reasons, e.g.,“it’s free and ready to work with GitHub
projects”, “Easy to share with other Docker users ”. As for CIW, the
mostimportantreasonwasto increaseconfidenceinbuildqual-
ityandresults (79.3%),followedbyto reducethetimespenton
setting up (60.3%). Other reasons given were similar to the prede-
fined answer we provided, e.g.,“automated test and quality control ”.
Overall,wefoundthatCIWsmayprovidedevelopersmorein-
tegration tests to help them find errors easier and faster before
publishing images to Docker Hub. This could make CIWs more
reliable,increasingdeveloperconfidenceinthebuildquality.But
the DHW may provide developers with more automation and sim-
pler configuration, which allows them to shift the time spent onsetting up to other development activities. Also, we found some
other differences in developers’ goals when using the two work-
flows.Morerespondents(29.3%)thoughtCIWshave convenient
custom settings and modifications than respondents using the
DHW (18.9%). And more respondents (36.2%) thought CIWs cre-
atemorevisibilityintotheteam’sworkflow thanrespondents
using the DHW (21.6%).
The DH and CI workflows may differ in release frequency, build
outcomes, build configuration stability, and build latency.
Fromtheresponses,CIWsmayprovidedevelopershighercon-
figurability than the DHW, which should result in more efficient
CD workflows. So, we hypothesize:
H6.CIW tends to have higher release frequency than DHW.
Similarly,thehigh configurability ofaCIWmayprovidedevel-
opersmorepossibilitiestorevisetheirDockerfileconfigurations.
We hypothesize:
H7.CIW tends to have lower Dockerfile stability than DHW.
It is very intuitive for us to expect a significant difference be-
tweenCIWandDHW.ButwithinCIWs,differentCItoolsshould
have similar functions and roles in the CD workflow. Thus, we
hypothesize:
H8.Within CIWs, there should not be significant differences be-
tween different CI tools.
300One Size Does Not Fit All: An Empirical Study of Containerized ... ESEC/FSE ’18, November 4–9, 2018, Lake Buena Vista, FL, USA
4 LARGE-SCALE QUANTITATIVE STUDY
Basedonthefindingsandhypothesesfromourqualitativestudy
(detailedabove,in§ 3),weconductedaquantitativestudytoexplore
differences between the two CD workflows ( RQ2).
4.1 Methods
Projects selection. From the container list in Docker Store, we
collected basic information for all containers listed on or before
July 2017. Our survey responses show that the two widely used
CD workflows are DHW and CIW (§ 3.3); we selected projects that
usedonlythosetwoworkflows.ForprojectsusingCIW,welimited
our study to two cloud-based CI platforms, Travis CI and CircleCI,
since in the Docker Hub documentation and in our survey, we
found these to be among the most popular three; the third, Jenkins,
runs locally and thus has no publicly-available data or API. We callprojects that use the DH workflow
DH projects , those that use the
TravisCIworkflow Travisprojects ,andthosethatusetheCircleCI
workflow Circle projects .WeidentifiedDHprojectsbychecking
for the presence of the string “ is_automated ” through the Docker
Hub API (True means the project has auto-builds); this yielded 500
DH projects. For Travis and Circle projects, we identified them
by checkingthe Docker-enabled deployment settings[ 12,13],e.g.,
“docker push ” and “ docker build ”, in their “ .travis.yml ”o r
“circle.yml ”configurationfiles;thisyielded282Travisprojects
and 343 Circle projects; each of them only used one type of CD
workflow in their history.
Data collection and filtering. Out datacollection involved min-
ing threetypes of sources: (1)Docker Hub data, i.e., DockerHub
builds, using the Docker Hub API; (2) GitHub data, i.e., commits
andgitlogsofDockerfile,usingtheGitHubAPI;and(3)CIdata, i.e.,
CI builds, using the Travis CI and CircleCI APIs. For CI builds, the
mainworkdonebytheCItoolsisintegrationtesting,soweparsed
the CI build scripts8to distinguish between deployment builds
(theaimofthisCIbuildistodeployimages)andgeneraltestbuilds.
Weonlyconsiderdeploymentbuildsinourstudy.Then,wefiltered
outprojectswithlessthan10successfulbuilds,asthesemightindi-
cate experiments with the infrastructure rather than more serious
CD practice. After this filtering, we obtained our final set of 855projects for the quantitative study, 428 of them DH projects, 236
Circle projects, and 191 Travis projects.
Intotal,ourdatasetcontains133,593imagebuilds.Amongthem,
39,094 (29.3%) are Docker Hub builds, 30,990 (23.2%) are Travis
CI builds and 63,509 (47.5%) are CircleCI builds. Table 6presents
aggregate descriptive statistics over the 855 projects.
Regressionanalysis. Totestourhypotheses,webuiltfourmixed-
effectslinearregressionmodels(packages lme4andlmerTest in
R) with thesame random-effect term forthe base image . The base
images specified in the Dockerfile (defined in the FROMinstruction)
can give a first indication of what it is that the projects use Docker
for[17].EveryDockerimagestartsfromabaseimage, e.g.,Ubuntu
baseimage.Soweexpectedthatthebaseimagehasanimportant
effectontheimagebuildprocess,especiallythebuildlatency.We
capturedthebaseimageinformationbyextractingitsnamefromthe
specification, i.e.,atupleoftheform namespace/name(:version) .
8Check if the script has “ docker build ” and “ docker push ” commands.Table 6: Aggregate statistics of the 855 projects.
Group Statistic Mean St. Dev. Min Median Max
DH projects#Total builds 91.3 155.7 11 30 1,000
#Successful builds 80.5 144.4 10 25 942#Errored builds 10.9 43.1 0 3 783
Travis projects#Total builds 162.3 341.5 12 51 3,366
#Successful builds 121.8 270.1 10 42 2,799#Errored builds 40.4 82.8 0 12 567
Circle projects#Total builds 269.1 790.8 14 63 7,506
#Successful builds 172.3 506.3 10 34 5,494#Errored builds 96.9 363.8 1 31 4,133
The random effect allows us to avoid modeling each base image
separately, which would use up degrees of freedom unnecessarily,
but still capture base image variability in the response. All other
variables were modeled as fixed effects. We divide the build data of
each project into different stages, in 30-day windows.
The following outcomes, or dependent variables, were observed
during those time-windows:
•nSuccessBuilds : number of successful builds per time window, as a
proxy for release frequency.
•nErrorBuilds : number of errored builds per time window, as a proxy
for build results.
•nDockerfileChanges : number of Dockerfile changes per time window,
as a proxy for configuration stability of builds.
•avgBuildLatency :mean latencyof successfulbuildsper timewindow,
as a proxy for build speed.Build latency is the time duration from build
start to end, in minutes.
Ourindependentvariablescomefromtwocovariateareas:global
(or aggregate level) and local (or time-window level):
•totalCommits andtotalBuilds : total number of commits and total
numberofimagebuildsintheproject’shistory,asaproxyforproject
size/activity.
•ageAtCD : project age at the time of adopting CD, in days, computed
since the earliest recorded image build.
•workflow :differenttypesofCDworkflows,wedistinguishedDHwork-
flow, Travis CI workflow, and CircleCI workflow. We used effect cod-
ing[26]tosetthecontrastsofthisthree-wayfactor, i.e.,comparingeach
level to the grand mean of all three levels.
•timeFlag : label of the time window, in months, computed since the
earliest image build.
•nLinesOfDockerfile : number of lines of Dockerfile per time window.
We removed the blank lines and comments.
•nIssuesOfDockerfile : number of quality issues of the Dockerfile per
time window, computed by Dockerfile Linter [ 17,37].
In our models, where necessarywe log-transformed dependent
variables to stabilize their variance and reduce heteroscedastic-
ity [19]. We also removed the top 1% of the data for highly-skewed
variables to control outliers and improve model robustness, in line
with best practices [ 48]. The variance inflation factors, which mea-
suremulticollinearityofthesetofpredictorsinourmodels,were
safe, below 3. For each model variable, we report its coefficients,
standard error, significance level, and sum of squares (via ANOVA).
Because each coefficient in the regression amounts to a hypothesis
test, we employ multiple hypothesis correction over all coefficient
results,tocorrectforfalsepositives,usingtheBenjamini-Hochberg
step-down procedure [ 5]. We consider the such corrected coeffi-
cients noteworthy if they were statistically significant at p<0.05.
Modelfitwasevaluatedusingamarginal( R2m)andaconditional
(R2c)coefficientofdeterminationforgeneralizedmixed-effectsmod-
els[33,44].R2mdescribestheproportionofvarianceexplainedby
301ESEC/FSE ’18, November 4–9, 2018, Lake Buena Vista, FL, USA Yang Zhang, Bogdan Vasilescu, Huaimin Wang, and Vladimir Filkov
the fixed effects alone, and R2cdescribes the proportion of variance
explained by the fixed and random effects together.
4.2 Results
Difference in release frequency. First, we examined the release
frequencyintermsofthenumberofsuccessfulbuildsper30-day
time-window. Table 7shows the results of the release frequency
model (Model-1). The fixed-effects part of the model explained
R2m=0.28 of the deviance. A considerable amount of variability
is explained by the random effect ( R2c=0.43),i.e., base image, not
explicitly modeled by our fixed effects.
Among the fixed effects, as expected, totalCommits andtotal-
Buildshavesignificant,positiveeffects,togetherexplaining64%of
the variance. Thus, big or active projects may associate with higher
release frequency . We also note that timeFlag has a significant,
negativeeffectonreleasefrequency(24%ofthevarianceexplained).
Thisindicatesthatreleasefrequencytendstodecreaseovertime,
holdingallothervariablesconstant,whichofferssupportfor H1.
Compared to the overall mean across all workflows, the DH work-
flow has a significant negative effect on release frequency (11% of
the variance explained), while the Travis CI and CircleCI work-
flowshavesignificantpositiveeffects.Holdingallothervariables
constant,DHWtendstohavelowerreleasefrequencythanCIW.
This is consistent with H6. Thus,
Releasefrequencytendstodecreaseovertime.ButDHWtends
to have lower release frequency than CIW.
Differenceinbuildresults. Next,weusedthenumberoferrored
buildstocomparetheworkflows.Table 8showsthesummaryof
thebuildresultsregressionmodel( Model-2).Thefixed-effectspart
ofthemodelexplained R2m=0.21ofthedeviance,foratotal R2c=0.27
with the random effect.
Amongthemodelresults, totalCommits andtotalBuilds have
a significant positive effect,9but they explain different proportions
of variance (40% vs 4%), consistent with more code may bring more
errorstothebuild .timeFlag hasasignificantnegativeeffect(12%
of the variance explained). Thus, the number of errored builds tends
to become smaller over time . Compared to the overall mean of three
workflows,theDHworkflowhasasignificantnegativeeffect(37%of
thevarianceexplained).WhiletheTravisCIandCircleCIworkflowshavesignificantpositiveeffectsontheoutcome.Thisindicatesthat
compared to DHW, CIW is associated with more errors in the
builds, holding all other variables constant; this is consistent withH
4. Thus,
CIW tends to have more errored image builds than DHW.
Difference in build configuration stability. We next used the
number of Dockerfile changes per 30-day time-windows to com-
pare the build configuration stability between the workflows; thus,
higher#changesindicateslowerstability.Table 9showsthecon-
figuration stability model ( Model-3) summary. The fixed-effects
explained R2m=0.07 of the deviance; with the random effect the
model explained R2c=0.13 of the deviance, and this was our poorest
fitting model.
9Notethatinthismodel,“positive”effectmeansmoreerroredbuildsand“negative”
effect means fewer errored builds.Table 7: Release frequency model. The response is
log(nSuccessBuild) .R2m=0.28, R2c=0.43.
Coeffs (Error) Sum Sq.
(Intercept) 0.3631 (0.0398)***
totalCommits 0.4041 (0.0202)*** 265.84***ageAtCD -0.0602 (0.0156)*** 9.89***totalBuilds 0.4625 (0.0160)*** 551.21***timeFlag -0.0385 (0.0018)*** 306.15***nIssuesOfDockerfile -0.0229 (0.0142) 1.72nLinesOfDockerfile -0.0359 (0.0138)* 4.44**
workflow=DH -0.3244 (0.0219)*** 145.76***
workflow=Travis CI 0.1922 (0.0228)***
workflow=CircleCI 0.1322 (0.0213)***
***p<0.001,* *p<0.01,*p<0.05
Table 8: Build results model. The response is
log(nErrorBuild) .R2m=0.21, R2c=0.27.
Coeffs (Error) Sum Sq.
(Intercept) 0.0806 (0.0382)*
totalCommits 0.3754 (0.0241)*** 184.56***
ageAtCD -0.1426 (0.0227)*** 30.21***
totalBuilds 0.1135 (0.0238)*** 17.32***timeFlag -0.0243 (0.0028)*** 56.03***nIssuesOfDockerfile -0.0315 (0.0209) 1.74nLinesOfDockerfile -0.0058 (0.0210) 0.06
workflow=DH -0.4566 (0.0332)*** 169.40***
workflow=Travis CI 0.1293 (0.0298)***
workflow=CircleCI 0.3273 (0.0271)***
***p<0.001,* *p<0.01,*p<0.05
Table 9: Configuration stability model. The response is
log(nDockerfileChanges) .R2m=0.07, R2c=0.13.
Coeffs (Error) Sum Sq.
(Intercept) 0.2257 (0.0401)***
totalCommits 0.1193 (0.0235)*** 23.11***ageAtCD -0.0845 (0.0253)** 9.97***totalBuilds -0.0027 (0.0222) 0.01timeFlag -0.0346 (0.0035)*** 87.40***nIssuesOfDockerfile 0.0479 (0.0235) 3.72*
nLinesOfDockerfile 0.0775 (0.0240)** 9.35**
workflow=DH -0.1668 (0.0326)*** 27.93***
workflow=Travis CI 0.0350 (0.0368)
workflow=CircleCI 0.1318 (0.0331)***
***p<0.001,* *p<0.01,*p<0.05
timeFlag has a significant negative effect, accounting for 54%
ofthevarianceexplained.Holdingallothervariablesconstant,it
indicates that the build configuration tends to become more stable
(i.e., have fewer changes) over time; this is consistent with H3.
Comparedtotheoverallmeanofthreeworkflows,theDHworkflow
hasasignificantnegativeeffect(17%ofthevarianceexplained),but
theTravisCIworkflowhasnosignificantdifference;theCircleCI
workflowhasasignificantpositiveeffect.ThisisevidencethatCIW
mayhavelowerDockerfilestabilitythanDHW,holdingallother
variables constant. Hence, this is consistent with H7.
Containerconfigurationstabilitytendstoincreaseovertime.But
CIW tends to have lower Dockerfile stability than DHW.
Difference in build latency. Finally,weexaminedbuildlatency.
Table10shows the build latency model ( Model-4) result. The frac-
tionoftotaldevianceexplainedbythefixed-effectspartofthemodel
isR2m=0.20. A considerable amount of variability is explained by
therandomeffect( R2c=0.57).Thisisconsistentwithourdescription
in§4.1,ofthestrongeffectthebaseimagelatencymayhaveonthe
total build latency.
302One Size Does Not Fit All: An Empirical Study of Containerized ... ESEC/FSE ’18, November 4–9, 2018, Lake Buena Vista, FL, USA
Table 10: Build latency model, The response is
log(avgBuildLatency) .R2m=0.20, R2c=0.57.
Coeffs (Error) Sum Sq.
(Intercept) -0.1974 (0.0544)***
totalCommits 0.1551 (0.0170)*** 44.56***ageAtCD 0.0139 (0.0142) 0.52totalBuilds 0.2023 (0.0148)*** 99.75***timeFlag 0.0110 (0.0016)*** 24.39***nIssuesOfDockerfile 0.0387 (0.0131)** 4.70**nLinesOfDockerfile 0.1381 (0.0127)*** 63.07***
workflow=DH 0.4336 (0.0204)*** 245.90***
workflow=Travis CI -0.2891 (0.0209)***
workflow=CircleCI -0.1445 (0.0196)***
***p<0.001,* *p<0.01,*p<0.05
As expected, more lines in a Dockerfile ( nLinesOfDockerfile )
are associated with longer build latency (13% of the variance ex-
plained).timeFlag hasasmall,significantpositiveeffect(5%ofthe
varianceexplained),meaningbuildlatencytendstoincreaseover
time,holdingothervariablesconstant;thisisconsistentwith H2.
Compared to the overall mean across all workflows, the DH work-
flow has a strong, positive effect (51% of the variance explained),
andtheTravisCIandCircleCIworkflowshavesignificantnegative
effects.ThismeansthatDHWtendstohavelongerbuildlatency
thanCIW,holdingallothervariablesconstant,whichiscontrary
to our expectation. So, H5is rejected.10
Buildlatencytendstoincreaseslightlyovertime.Interestingly,
DHW tends to have longer build latency than CIW.
Differences among CI workflows. From our models, we find
thatusageofCIWorDHWassociatewithsignificantdifferences
inoutcomes,consistentwithourhypotheses.Butwithrespectto
erroredbuildsandbuildlatency,wealsofinddifferencesbetween
the Travis CI and CircleCI workflows. This indicates that different
CI tools may perform the same or similar role, but be associated
with different effects. So H8is rejected. Thus,
DHWandCIWaresignificantlydifferent.UsingdifferentCItools
can also associate with different outcomes.
5 DISCUSSION
Herewediscussthepracticaldifferencesandthetrade-offsbetween
the DH workflow (DHW) and CI workflows (CIWs), followed by
the practice implications.
5.1 Practical Differences
WerecapitulatethepracticaldifferencesbetweenDHWandCIW
based on their support for CD automation and Docker, build en-
vironment, aswell asdeveloper experience.These recapitulations
allowed us to develop a deeper understanding of our survey and
quantitative study results.
Support forautomated testing. InCIW,CItoolssetup“hooks”
with GitHub to automatically run tests (typically unit and integra-
tion)atspecifiedtimes.Bydefault,thesearesetuptorunaftera
pullrequestiscreatedorwhencodeispushedtoGitHub.DHWhas
10Some developers have posted about the build latency problem of Docker Hub auto-
builds onthe Dockerforum (https://forums.docker.com/t/why-does-it-take-so-long-
for-the-docker-hub-automated-builds-to-upload-the-built-image). Some in our survey
are ok with it. R23said, “The latency and build times are totally okay due to the fact
that it’s free. I would have a different opinion if I paid for the same service though ”.a complementary automated testing tool for deployment images
(auto-test) [ 28], provided by Docker Hub. Before using auto-test,
adocker-compose.test.yml automated test file must be set up.
This file defines a sutservice that lists the tests to be run and it
shouldbelocatedinthesamedirectorythatcontainstheDockerfile.
Sincethe docker-compose.test.yml isastandardComposefile,
developers could also just invoke Compose in the CI configuration
file to run those tests, which implies CIW may be more powerful.
SupportforDockerandDockertools. DHW,naturally,hasbet-
tersupportforDockerandDockertoolsthanCIW,sinceDocker
Hub itself is a cloud-based service provided by Docker. In addi-
tion, CI tools differ in the amount of Docker support they pro-
vide. Docker version support provided in Travis CI is more re-
cent and more diverse than that in CircleCI. Travis CI develop-ers can manually upgrade Docker to the latest version by updat-
ing.travis.yml [10], whereas CircleCI currently supports only 3
fixed Docker versions [ 14]. Also, we found that Travis CI has some
Dockertoolspre-installed, e.g.,theDockerComposetool[ 11].In
CircleCI, developers need to install and configure this tool in their
container in order to use it [ 15].
Build environment. As reported on the Docker forum [ 24], the
currentlimitsonDockerHubauto-buildsare1CPUand2GBRAM,
whichinpracticemeanspotentiallatencyproblemsforlargebuilds.
Ontheotherhand,TravisCIandCircleCIbothprovide2CPUsandlargerRAMlimits(4GBand8GB)forthebuildenvironment[
9,16].
We foundlatencyto bean issueinDocker Hubinour surveyand
quantitative study.
Developer work experience. In our survey, we found that the
average OSS work experience of respondents who use DHW is
7.8years(median6),whileforrespondentswhouseCIWitis8.8
years (median 7). While CIW users seem to have one additional
yearofOSSexperience,thedifferenceisnotstatisticallysignificant
(Wilcoxontest; p=0.42).Ontheotherhand,theaverageCI/CDwork
experienceofDHWrespondentsis3.6years(median3),andofCIW
respondentsitis4.5years(median4).Thestatisticaltestshowsthat
this difference is significant (Wilcoxon test; p=0.04). So, in practice
thismaymeanthattheuse/implementationofCIWassociateswith
more developer CI/CD experience than DHW.
5.2 Trade-Offs between CD Workflows
Oursurveyanddata analysisrevealed thatwhenchoosingbetween
CIWandDHWonemayhavetotradesomefeaturesforothersand
that it is unlikely that one workflow will fit all:
Higher configurability (CIW) vs. Higher simplicity (DHW)
(see M7, B1, N2, N6, Model-3 and § 5.1). Highly configurable work-
flow means also one that is harder to use due to its complexity.
On the other hand, simplicity means higher build configuration
stability, but may also mean less control and lower flexibility.
Higherperformance(CIW)vs.Diverseneeds(DHW) (seeN1,
B2, B3, B4, Model-1, Model-4 and § 5.1). Specific requirements ( e.g.,
differentDockerversions)maybringaboutlowerperformance.But
higher performance may not meet more diverse needs.
Higherreliability(CIW)vs.Lowermaintenance(DHW) (see
M3, M5, M8, N3, B5, Model-2 and § 5.1). More testing means higher
reliability, but also more errored builds, i.e., more maintenance.
303ESEC/FSE ’18, November 4–9, 2018, Lake Buena Vista, FL, USA Yang Zhang, Bogdan Vasilescu, Huaimin Wang, and Vladimir Filkov
Higher scalability (CIW) vs. Lower experience (DHW) (see
N2,N5,B7and§ 5.1).Higherscalabilitymeansthatmoretoolsor
servicescanbeintegratedanddonotstronglylimittheCDprocess
(e.g.,build speed) inaproject,even iftheimagegrowsinsizeand
complexity. But CIW may require more experienced developers.
5.3 Implications
Forresearchers .Ourstudiesprovidearichresourceofinitialideas
forfurtherstudy.Oursurveyshowedthat45.8%oftherespondentshavechangedfromonetoanotherCDworkflows(§ 3.5).Examining
the costs and benefits that arise from switching CD workflows
may point to best practices for developers needing to change their
solutions.Hence,ourstudymotivatesfutureworktoexplorethe
CD workflow evolution.
WealsofoundthatdevelopershaveachoiceofdifferentCItools
(§3.3),andthatusingdifferentCItoolsassociateswithdifferentout-
comes (§4.2). Therefore, researchers should investigate the barriers
and benefits developers face when using particular CI tools.
Our quantitative study mostly focused on comparing CD out-
comesbetweenDHWandCIW(§ 4).HowthetwoCDworkflows
differ in other dimensions should be further empirically evaluated.
E.g., wehave found inour data, onlyanecdotally, that sometypes
ofprojectsmaygravitatetowardoneortheotheroftheworkflows.
Withmuchmoredataandcarefulprojectclassificationalongdiffer-
ent dimensions, some patterns may become apparent. Our findingsmotivatetheneedforcollectingmoreempiricalevidencesthathelp
developers, who wish to reduce complexity and improve perfor-
mance, to choose appropriate CD tools and establish CD workflow
without arbitrary decisions.
For developers .Ourstudyshowsthatdevelopersfacetrade-offs
when choosing different CD workflows (§ 5.2). A direct implication
is that developers should not only consider their own experiences
and needs, but also consider the different CD support in the CD
workflows.E.g.,lessexperienceddevelopersmaybenefitmorefrom
usingDHWinsteadofCIW,becauseDHWhashighersimplicity
and lower maintenance cost. More experienced developers may
insteadbenefitfromCIW,whichcanbringhigherconfigurability
andperformance.Hence,thereisaneedforalistof“bespokeCD
best practices” for developers with different experiences and needs.
OurquantitativestudiesrevealedthatDockerfileconfiguration
details,e.g.,baseimage,haveimportanteffectsontheCDworkflow
outcomes (§ 4.2). Developers should select appropriate base image
and instructions. Also, developers should simplify their Dockerfile
content and optimize the image structures, i.e., image layers and
instruction orders. Therefore, the issue of how to manage and help
developers configure the best Dockerfile needs to be addressed.
For service providers . Based on the trade-offs developers face
(§5.2),therearetwosuggestionsforserviceproviders,oneissimpli-
fyingtheconfigurationcomplexity,tolowertheinitiationobstacles
for more inexperienced developers. The other is improving their
supportforCDautomation, e.g.,integratingmorepowerfulDocker
tools and providing more virtual machine environments.
Fortoolbuilders .Ourrespondentsexpressedtheirneedsforbuild
testingandfailurelogging(N3andN7).Hence,toolbuildersmay
look into creating modern tools that enhance build testing and in-
tegrate with different workflows. Also, developerscould use moresophisticated( e.g.,socialcodingintegrated/enabled)toolsthatman-
age and analyze logs of build failures.
6 THREATS TO VALIDITY
Internal validity. Surveys can be affected by bias and inaccurate
responses,whichmaybeintentionalorunintentional.Toameliorate
thisthreat, wedesigned anddelivered oursurveyby followinges-
tablishedguidelines[ 51,61].Mostofourquestionsareopen-ended
so that participants can freely fill their own answers, and during
our manual analysis, we carefully removed unrelated answers.
In the quantitative study, we controlled for the build complexity
withthenumberoflinesintheDockerfile,andwesetthebaseimage
as a random-effect. But the Dockerfile may have many different
instructions inside, which may cause some bias, although in our
manualexaminationwedidnotfindevidenceforit.Wenotethat
ourmodels’fittothedataisaround25%ofthedeviance,andlower
for the build configuration stability model (Model-3). That is not
necessarilyaproblemforourpurposesasweareonlyinterestedinthe coefficients’ effect and not relying on the models to explain the
full phenomena, which would require many more variables, and is
beyond the scope of this work.
External validity. We only considered Docker repositories that
are on GitHub. Thus, our findings cannot be assured to generalize
to projects hosted on other services, e.g., Bitbucket and GitLab,
although there is no inherent reason why they would be biased.
Moreover,weonlyanalyzedopensourcesoftware.CDworkflow
and its influence might be different in closed source environments.
Finally, we conducted our study on the Docker-enabled CD work-
flow.WecannotassumethatourfindingsgeneralizetootherCD
workflows that are not using Docker.
7 CONCLUSION
We conducted the first large-scale study of Docker-enabled CD
workflows on Docker Hub/GitHub to shed light on the developers’
experiencesandexpectationswhenusingCD.Ourmixedqualita-
tiveandquantitativeapproachenabledustoteaseoutcategoriesofdevelopers’opinionsonCDandthetwoworkflows,aswellas
test hypotheses arising from them using large data sets. Most ofour survey findings were confirmed in the data, but some were
not, emphasizing the power of mixed methods to produce holistic
findings.Ourfindingsindicatethatdevelopersfacetrade-offswhen
choosingbetweendifferentCDworkflowswithrespecttoconfig-
urability,simplicity,requirements,performance,stability,developer
experience, etc.,andwewereabletodistillsomeimplicationsfor
different stakeholders.
ACKNOWLEDGMENTS
Thebulkofthisworkwasproducedwhilethefirstauthorwasvisit-
ing DECAL at UC Davis. We thank members of DECAL, especially
Prof. Devanbu, for their comments and directions on this research.
We also thank the 168 survey respondents for their valuable an-
swers and the anonymous reviewers for their insightful comments
onearlierversionsofthispaper.Thisworkwassupportedbythe
NSF (Grants No. 1717370 and 1717415), the National Natural Sci-
ence Foundation of China (Grant No. 61502512 and 61432020), and
China Scholarship Council.
304One Size Does Not Fit All: An Empirical Study of Containerized ... ESEC/FSE ’18, November 4–9, 2018, Lake Buena Vista, FL, USA
REFERENCES
[1]Charles Anderson. 2015. Docker [software engineering]. IEEE Software 32, 3
(2015), 102–c3.
[2]ValentinaArmenise.2015. ContinuousdeliverywithJenkins:Jenkinssolutionstoimplementcontinuousdelivery.In InternationalWorkshoponReleaseEngineering
(RELENG) . IEEE, 24–27.
[3]Len Bass, Ingo Weber, and Liming Zhu. 2015. DevOps: A Software Architect’s
Perspective . Addison-Wesley Professional.
[4]RobertBenefield.2009. Agiledeployment:Leanservicemanagementanddeploy-
ment strategies for the SaaS enterprise. In Hawaii International Conference on
System Sciences (HICSS) . IEEE, 1–5.
[5]YoavBenjaminiandYosefHochberg.1995. Controllingthefalsediscoveryrate:apracticalandpowerfulapproachtomultipletesting. Journaloftheroyalstatistical
society. Series B (Methodological) (1995), 289–300.
[6]David Bernstein. 2014. Containers and cloud: From lxc to docker to kubernetes.
IEEE Cloud Computing 1, 3 (2014), 81–84.
[7]Carl Boettiger. 2015. An introduction to Docker for reproducible research. ACM
SIGOPS Operating Systems Review 49, 1 (2015), 71–79.
[8]Lianping Chen. 2015. Continuous delivery: Huge benefits, but challenges too.
IEEE Software 32, 2 (2015), 50–54.
[9]Travis CI. 2018. Build Environment Overview. Retrieved July 17, 2018 from
https://docs.travis-ci.com/user/reference/overview/
[10]TravisCI.2018. InstallinganewerDockerversion. RetrievedJuly17,2018from
https://docs.travis-ci.com/user/docker/#Installing-a-newer-Docker-version
[11]Travis CI. 2018. Using Docker Compose. Retrieved July 17, 2018 from https:
//docs.travis-ci.com/user/docker/#Using-Docker-Compose
[12]Travis CI. 2018. Using Docker in Builds. Retrieved July 17, 2018 from https:
//docs.travis-ci.com/user/docker/
[13]CircleCI. 2018. Continuous Integration and Delivery with Docker. Retrieved
July 17, 2018 from https://circleci.com/docs/1.0/docker/
[14]CircleCI. 2018. Docker version. Retrieved July 17, 2018 from https://circleci.
com/docs/2.0/building-docker-images/#docker-version
[15]CircleCI.2018. InstallingandUsingdocker-compose. RetrievedJuly17,2018
fromhttps://circleci.com/docs/2.0/docker-compose/
[16]CircleCI. 2018. Remote Docker Environment. Retrieved July 17, 2018 from
https://circleci.com/docs/2.0/building-docker-images/#specifications
[17]Jürgen Cito, Gerald Schermann, John Erik Wittern, Philipp Leitner, Sali Zumberi,
andHaraldCGall.2017. AnempiricalanalysisoftheDockercontainerecosystem
on GitHub. In International Conference on Mining Software Repositories (MSR) .
IEEE Press, 323–333.
[18]Gerry Gerard Claps, Richard Berntsson Svensson, and Aybüke Aurum. 2015. On
the journey to continuous deployment: Technical and social challenges along
the way. Information and Software Technology 57 (2015), 21–31.
[19]Jacob Cohen, Patricia Cohen, Stephen G West, and Leona S Aiken. 2013. Applied
multiple regression/correlation analysis for the behavioral sciences . Routledge.
[20]T. Combe, A. Martin, and R. Di Pietro. 2016. To Docker or Not to Docker: A
Security Perspective. IEEE Cloud Computing 3, 5 (2016), 54–62.
[21]Juliet Corbin andAnselm Strauss. 1990. Grounded theory research: Procedures,
canons and evaluative criteria. Zeitschrift für Soziologie 19, 6 (1990), 418–427.
[22]Rajdeep Dua, A Reddy Raja, and Dharmesh Kakadia. 2014. Virtualization vs
containerizationtosupportpaas.In InternationalConferenceonCloudEngineering
(IC2E). IEEE, 610–614.
[23]WesFelter,AlexandreFerreira,RamRajamony,andJuanRubio.2015. Anupdated
performancecomparisonofvirtualmachinesandlinuxcontainers.In Interna-
tional Symposium On Performance Analysis of Systems and Software (ISPASS) .
IEEE, 171–172.
[24]Dockerforum.2015. AutomatedBuildresourcerestrictions. RetrievedJuly17,
2018from https://forums.docker.com/t/automated-build-resource-restrictions/
1413/2
[25]MartinFowlerandMatthewFoemmel.2006. Continuousintegration. Thought-
Works http://www.thoughtworks.com/Continuous Integration.pdf 122 (2006).
[26]Alkharusi H. 2012. Categorical variables in regression analysis: A comparison of
dummyandeffectcoding. InternationalJournalofEducation 4,2(2012),202–210.
[27]MichaelHilton, NicholasNelson,Timothy Tunnell,DarkoMarinov,and Danny
Dig. 2017. Trade-Offs in Continuous Integration: Assurance, Security, and Flexi-
bility.InInternationalSymposiumonFoundationsofSoftwareEngineering(FSE) .
ACM, 197–207.
[28]Docker Hub. 2018. Automated repository tests. Retrieved July 17, 2018 from
https://docs.docker.com/docker-cloud/builds/automated-testing/
[29]Docker Hub. 2018. Configure automated builds on Docker Hub. Retrieved July
17, 2018 from https://docs.docker.com/docker-hub/builds/
[30]Jez Humble. 2018. Continuous Delivery. Retrieved July 17, 2018 from https:
//continuousdelivery.com/
[31]JezHumbleandDavidFarley.2010. ContinuousDelivery:ReliableSoftwareReleases
through Build, Test, and Deployment Automation . Pearson Education.
[32]Jez Humble, Chris Read, and Dan North. 2006. The deployment production line.
InAgile Conference (AGILE) . IEEE, 6–pp.[33]Paul CD Johnson. 2014. Extension of Nakagawa & Schielzeth’s R2GLMM to
random slopes models. Methods in Ecology and Evolution 5, 9 (2014), 944–946.
[34]SebastianKlepper, StephanKrusche,SebastianPeters, BerndBruegge,andLukas
Alperowitz. 2015. Introducing continuous delivery of mobile apps in a corpo-
rateenvironment:Acasestudy.In InternationalWorkshoponRapidContinuous
Software Engineering (RCoSE) . IEEE, 5–11.
[35]Stephan Krusche and Lukas Alperowitz. 2014. Introduction of continuous deliv-
eryinmulti-customer projectcourses. In InternationalConferenceonSoftware
Engineering (ICSE) . ACM, 335–343.
[36]M.Leppanen,S.Makinen,M.Pagels,V.P.Eloranta,J.Itkonen,M.V.Mantyla,and
T. Mannisto. 2015. The highways and country roads to continuous deployment.
IEEE Software 32, 2 (2015), 64–72.
[37]Linter. 2018. Dockerfile Linter. Retrieved July 17, 2018 from http://hadolint.
lukasmartinelli.ch/
[38]MikaVMantyla,FoutseKhomh,BramAdams,EmelieEngstrom,andKaiPetersen.
2013. On rapid releases and software testing. In International Conference on
Software Maintenance (ICSM) . IEEE, 20–29.
[39]AR Manu, Jitendra Kumar Patel, Shakil Akhtar, VK Agrawal, and KN Bala Subra-
manyaMurthy.2016. Dockercontainersecurityviaheuristics-basedmultilateral
security-conceptual and pragmatic study. In International Conference on Circuit,
Power and Computing Technologies (ICCPCT) . IEEE, 1–14.
[40]DirkMerkel. 2014. Docker:lightweight linuxcontainersfor consistentdevelop-
ment and deployment. Linux Journal 2014, 239 (2014), 2.
[41]MathiasMeyer.2014. Continuousintegrationanditstools. IEEESoftware 31,3
(2014), 14–16.
[42]Adrian Mouat. 2015. Using Docker: Developing and Deploying Software with
Containers . O’Reilly Media, Inc.
[43]Preeth E N, F. J. P. Mulerickal, B. Paul, and Y. Sastri. 2015. Evaluation of Docker
containers based on hardware utilization. In International Conference on Control
Communication Computing India (ICCC) . 697–700.
[44]ShinichiNakagawaandHolgerSchielzeth.2013. Ageneralandsimplemethodfor
obtainingR2from generalizedlinearmixed-effectsmodels. MethodsinEcology
and Evolution 4, 2 (2013), 133–142.
[45]Steve Neely and Steve Stolt. 2013. Continuous delivery? easy! just change ev-
erything (well, maybe it is not that easy). In Agile Conference (AGILE) . IEEE,
121–128.
[46]Helena Olsson Holmström, Hiva Alahyari, and Jan Bosch. 2012. Climbing the
"Stairway to Heaven" A multiple-case study exploring barriers in the transition
fromagiledevelopmenttowardscontinuousdeploymentofsoftware.In Euromicro
ConferenceonSoftwareEngineeringandAdvancedApplications .IeeeComputer
Soc, 392–399.
[47]Chris Parnin, Eric Helms, Chris Atlee, Harley Boughton, Mark Ghattas, Andy
Glover, James Holman, John Micco, Brendan Murphy, Tony Savor, et al .2017.
The top 10 adages in continuous deployment. IEEE Software 34, 3 (2017), 86–95.
[48]Jagdish K Patel, CH Kapadia, and Donald Bruce Owen. 1976. Handbook of
statistical distributions . M. Dekker.
[49]Perforce.2017.ContinuousDelivery:TheNewNormalforSoftwareDevelopment.
Retrieved July17, 2018from https://www.perforce.com/sites/default/files/files/
2017-09/continuous-delivery-report.pdf
[50]Portworx. 2017. 2017 Annual Container Adoption Survey: Huge Growthin Containers. Retrieved July 17, 2018 from https://portworx.com/
2017-container-adoption-survey/
[51]Teade Punter, Marcus Ciolkowski, Bernd Freimut, and Isabel John. 2003. Con-
ducting on-line surveys in software engineering. In International Symposium on
Empirical Software Engineering (ISESE) . IEEE, 80–88.
[52]Puppet. 2017. 2017 State of DevOps Report. Retrieved July 17, 2018 from
https://puppet.com/resources/whitepaper/state-of-devops-report
[53]GitHubreport.2017. GitHubwelcomesallCItools. RetrievedJuly17,2018from
https://blog.github.com/2017-11-07-github-welcomes-all-ci-tools
[54]PilarRodríguez,AlirezaHaghighatkhah,LucyEllenLwakatare,SusannaTeppola,
TanjaSuomalainen,JuhoEskeli,TeemuKarvonen,PasiKuvaja,JuneMVerner,
and Markku Oivo. 2017. Continuous deployment of software intensive products
and services: A systematic mapping study. Journal of Systems and Software 123
(2017), 263–291.
[55]ChuckRossi,ElisaShibley,ShiSu,KentBeck,TonySavor,andMichaelStumm.
2016. Continuous deployment of mobile software at facebook (showcase). InInternational Symposium on Foundations of Software Engineering (FSE) . ACM,
12–23.
[56]TonySavor, MitchellDouglas, Michael Gentili,Laurie Williams, KentBeck, and
Michael Stumm. 2016. Continuous deployment at Facebook and OANDA. In
International Conference on Software Engineering (ICSE) . ACM, 21–30.
[57]Gerald Schermann, Jürgen Cito, Philipp Leitner, Uwe Zdun, and Harald Gall.2016.An empirical study on principles and practices of continuous delivery and
deployment . Technical Report. PeerJ Preprints.
[58]Mojtaba Shahin, Muhammad Ali Babar, and Liming Zhu. 2017. ContinuousIntegration, Delivery and Deployment: A Systematic Review on Approaches,
Tools, Challenges and Practices. IEEE Access 5 (2017), 3909–3943.
305ESEC/FSE ’18, November 4–9, 2018, Lake Buena Vista, FL, USA Yang Zhang, Bogdan Vasilescu, Huaimin Wang, and Vladimir Filkov
[59]RuiShu,XiaohuiGu,andWilliamEnck.2017. AStudyofSecurityVulnerabilities
on Docker Hub. In International Conference on Data and Application Security and
Privacy. ACM, 269–280.
[60]John Ferguson Smart. 2011. Jenkins: The Definitive Guide: Continuous Integration
for the Masses . O’Reilly.
[61]EdwardSmith,RobertLoftin,EmersonMurphy-Hill,ChristianBird,andThomas
Zimmermann. 2013. Improving developer participation rates in surveys. In
InternationalWorkshoponCooperativeandHumanAspectsofSoftwareEngineering
(CHASE). IEEE, 89–92.
[62]CarmineVassallo,GeraldSchermann,FiorellaZampetti,DanieleRomano,Philipp
Leitner, Andy Zaidman, Massimiliano Di Penta, and Sebastiano Panichella. 2017.
ATaleofCIBuildFailures:anOpenSourceandaFinancialOrganizationPerspec-
tive. InInternational Conference on Software Maintenance and Evolution (ICSME) .
IEEE, 183–193.
[63]CarmineVassallo,FiorellaZampetti,DanieleRomano,MoritzBeller,Annibale
Panichella,MassimilianoDiPenta,andAndyZaidman.2016.Continuousdeliverypracticesinalargefinancialorganization.In InternationalConferenceonSoftware
Maintenance and Evolution (ICSME) . IEEE, 519–528.
[64]DavidWidder,MichaelHilton,ChristianKästner,andBogdanVasilescu.2018. I’m
LeavingYou,Travis:AContinuousIntegrationBreakupStory.In International
Conference on Mining Software Repositories (MSR) . ACM, 165–169.
[65]Tianyin Xu and Darko Marinov. 2018. Mining Container Image Repositories
—MSRforSoftwareConfigurationandBeyond.In InternationalConferenceon
Software Engineering: New Ideas and Emerging Results (ICSE-NIER) . ACM, 49–52.
[66]Yangyang Zhao, Alexander Serebrenik, Yuming Zhou, Vladimir Filkov, and Bog-
dan Vasilescu. 2017. The impact of continuous integration on other software
development practices: A large-scale empirical study. In International Conference
on Automated Software Engineering (ASE) . IEEE, 60–71.
306