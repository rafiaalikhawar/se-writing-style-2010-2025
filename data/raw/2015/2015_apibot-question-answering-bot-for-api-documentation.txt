APIBot: Question Answering Bot
for API Documentation
Yuan Tian, Ferdian Thung, Abhishek Sharma, and David Lo
School of Information Systems, Singapore Management University, Singapore
fyuan.tian.2012,ferdiant.2013,abhisheksh.2014,davidlog@smu.edu.sg
Abstract ‚ÄîAs the carrier of Application Programming Inter-
faces (APIs) knowledge, API documentation plays a crucial role
in how developers learn and use an API. It is also a valuable infor-
mation resource for answering API-related questions, especially
when developers cannot Ô¨Ånd reliable answers to their questions
online/ofÔ¨Çine. However, Ô¨Ånding answers to API-related questions
from API documentation might not be easy because one may
have to manually go through multiple pages before reaching the
relevant page, and then read and understand the information
inside the relevant page to Ô¨Ågure out the answers. To deal with
this challenge, we develop APIBot, a bot that can answer API
questions given API documentation as an input. APIBot is built
on top of SiriusQA, the QA system from Sirius, a state of the art
intelligent personal assistant. To make SiriusQA work well under
software engineering scenario, we make several modiÔ¨Åcations
over SiriusQA by injecting domain speciÔ¨Åc knowledge. We
evaluate APIBot on 92 API questions, answers of which are
known to be present in Java 8 documentation. Our experiment
shows that APIBot can achieve a Hit@5 score of 0.706.
Index Terms‚ÄîAPI Documentation, Question Answering, Bot
I. I NTRODUCTION
When developing applications using Application Program-
ming Interfaces (APIs), the ofÔ¨Åcial API documentation is one
of the excellent sources for Ô¨Ånding answers to API-related
questions. Unfortunately, to Ô¨Ånd a desired piece of informa-
tion, developers may need to sift through numerous pages
in documentation, which is a tedious and time consuming
activity. The process is even worse for APIs having a sharp
learning curve [1]. As a consequence, developers often do
not read API documentation [2], [3], [4]. This may cause
developers to use APIs incorrectly, resulting in bugs and even
security vulnerabilities [5], [6].
Finding answers in API documentation can be made much
simpler through a question answering bot. The bot can sim-
ulate an expert, answering developer queries directly and
reducing the need of developers to browse multiple documents
to Ô¨Ånd answers. Such a bot can be particularly helpful for new
or closed APIs which have few available experts. A bot can
automatically learn from documentation of these new or closed
APIs and help developers with their queries.
There are recent interests in creating bots for software
engineering purposes. Murgia et al. created Joey, a question
answering bot for StackOverÔ¨Çow that is able to answer ques-
tions that are asked before [7]. Storey and Zagalsky visioned
The Ô¨Årst 3 authors have contributed equally to the work.the use of bots to automate tasks in software development [8].
In this paper, we build a bot that is able to answer API-
related questions by analyzing API documentation. Different
from Joey, our approach is able to answer questions regardless
whether it has been asked before or not.
Recently, Hauswald et al. developed an open end-to-end per-
sonal assistant that they named Sirius [9]. Sirius makes use of
existing state of the art technologies such as Pocketsphinx [10],
Kaldi [11], and RWTH‚Äôs RASR [12] for speech recognition,
OpenEphyra [13] for question answering, and SURF [14] for
image detection. SiriusQA, i.e., the QA system inside Sirius,
works by relying on a set of question and answer patterns.
Question patterns are used to extract important phrases from
an input question, while answer patterns are used to rank
candidate answers extracted from a text corpus. SiriusQA has
been shown to work well with general knowledge questions.
However, it is unknown whether SiriusQA would still work for
domain-speciÔ¨Åc questions like API-related questions due to its
use of general question and answer patterns. These patterns
do not capture API domain knowledge.
Faced with SiriusQA‚Äôs inherent limitations for the API do-
main, we are motivated to build APIBot, a speciÔ¨Åc question an-
swering bot for API documentation. We address limitations of
SiriusQA to make it work well for API documentation. First,
we naturalize API documentation to make hidden structural
information appear in the form of natural language sentences.
Second, we create API-speciÔ¨Åc question patterns to make
SiriusQA understand important parts of API questions and
categorize them. Third, we modify SiriusQA to consider API-
speciÔ¨Åc answer patterns and terms. Finally, we learn answer
patterns by building a probabilistic model for each answer
category. These answer patterns require a smaller training
corpus than SiriusQA, which needs to learn answer patterns
in form of regular expressions from a large set of question-
answer pairs.
To evaluate the performance of APIBot, we collect a bench-
mark by Ô¨Årst inviting a group of people to ask questions about
JDK 8 API documentation. We then invite another group of
people to provide the ground truth answers for the collected
questions. Our experimental result shows that APIBot can
achieve a Hit@5 score of 0.706 on the evaluation dataset.
The rest of this paper is structured as follows. Section II de-
scribes background knowledge of SiriusQA and its limitations
when applied on API documentation. Section III introduces
the design of APIBot. Section IV presents our evaluation
978-1-5386-2684-9/17/$31.00 c2017 IEEEASE 2017, Urbana-Champaign, IL, USA
Technical Research - New Ideas153
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:48:35 UTC from IEEE Xplore.  Restrictions apply. methodology and results. Section V describes related studies.
Section VI gathers the conclusions derived from this work and
presents future work.
II. S IRIUS QA AND ITSLIMITATIONS
A. Overview of SiriusQA
Sirius is the leading state-of-the-art open source intelligent
personal assistant (IPA) system. It consists of three major
components: automatic speech recognition (ASR), question-
answering (QA) and image matching (IMM). In this work,
we enhance the question-answering component of Sirius,
henceforth referred to as SiriusQA. SiriusQA is based on the
open source OpenEphyra system [13]. The system incorporates
a number of NLP algorithms to effectively answer natural
language questions.
SiriusQA consists of three major components: question
interpretation, document search, and answer selection. We
describe them brieÔ¨Çy below:
Question Interpretation. Inquestion interpretation compo-
nent, SiriusQA identiÔ¨Åes the target (i.e., the entity that a ques-
tion asked about), contexts (i.e., the qualifying attributes of the
target), and category of a question based on a pre-deÔ¨Åned set of
manually constructed question patterns. A question pattern is
a regular expression with placeholders to identify target and
context phrases from questions. These patterns are grouped
into different categories.
For example, consider the following question: ‚ÄúWhat medal
has Joseph Schooling won in 2016 Olympics?‚Äù. Given the
following question pattern , ‚ÄúWhat medal has htargetiwon in
hcontexti?‚Äù,we can Ô¨Ånd that the target is ‚ÄúJoseph Schooling‚Äù
and the context is ‚Äú2016 Olympics‚Äù.
Document Search. Indocument search component, relevant
documents matching a query are returned. The query is
generated from the target, context, and keywords extracted by
thequestion interpretation component.
Answer Selection. Inanswer selection component, candidate
answers are identiÔ¨Åed from returned documents by matching
series of consecutive words in the documents with answer
patterns. The answer patterns are again in the form of regular
expressions, with placeholders to identify speciÔ¨Åc text that is
to be returned as an answer. A conÔ¨Ådence score is assigned to
each answer pattern, and this score is automatically inferred
based on a training set of question-answer pairs.
B. Limitations of SiriusQA
SiriusQA has been demonstrated to work well in answering
general questions by analyzing a corpus of textual articles.
However, it is unlikely to work well in answering API-
speciÔ¨Åc questions by analyzing API documentations, due to
the following limitations.
Limitation 1: SiriusQA only handles unstructured textual doc-
uments. API documentation, on the other hand, is a structured
document. For example, in an API documentation of a library
written in an object oriented language (e.g., a Javadoc page),
it would have pages explaining about classes. Throughout
the page, there would also be links to other pages in theAPI documentation representing inheritance or other kinds
of relationships. Unfortunately, such structural information is
fully ignored in SiriusQA.
Limitation 2: SiriusQA uses a set of manually crafted ques-
tion patterns, which are designed for general knowledge ques-
tions rather than API-speciÔ¨Åc questions. An example of gen-
eral knowledge question that SiriusQA can handle is ‚ÄúWhat
is the name of the actor who star in Titanic?‚Äù. This question
is far from questions developers might ask about an API. In
addition, each question pattern is assigned to a category in
SiriusQA, such as author, capital, etc. However, none
of the categories are related to the type of knowledge that are
commonly contained in API documentation.
Limitation 3: Answer pattern learning in SiriusQA requires
a large amount of manually created training data. However,
manually creating a large amount of API-speciÔ¨Åc question-
answer pairs is difÔ¨Åcult and time consuming. StackOverÔ¨Çow
records many questions (including those that are related to
APIs) and their corresponding answers. However, converting
StackOverÔ¨Çow data into something that SiriusQA can handle
requires expensive manual cleaning step. Thus, a strategy is
needed to learn answer patterns from small amount of data.
Limitation 4: SiriusQA does not have any knowledge about
software terms, e.g., it does not know that ArrayList is a class
in Java 8. It thus cannot differentiate software-speciÔ¨Åc words
from other words, and cannot use domain-speciÔ¨Åc heuristics
to return better answers.
III. P LUGGING INSOFTWARE KNOWLEDGE TO SIRIUS QA
A. Overview
We have built our system APIBot on top of SiriusQA by
modifying and enhancing some of its components. The overall
framework of APIBot is shown in Figure 1, with boxes shaded
in grey representing our new or enhanced components. It con-
sists of two major parts: Domain Adaptation, and Enhanced
SiriusQA. A brief overview of each part is given below.
Domain Adaptation. The Domain Adaptation component
enables APIBot to understand software engineering questions
and produce appropriate answers through a one-time training
process. At the end of the process, this component produces 3
outputs, i.e., Enhanced API Documentation, Question Patterns
andAnswer Patterns.
The Enhanced API Documentation consists of natural lan-
guage sentences which describe all the structured and unstruc-
tured information present in the original API documentation.
An Enhanced API Documentation enables us to take into
consideration the structure of an API documentation and hence
addresses limitation 1.
Question Patterns, which consist of regular expressions
with placeholders to identify target and context words from
questions, are another output. These expressions are grouped
into different categories capturing different kinds of knowledge
stored in an API documentation [15]. Our question patterns,
which are grouped into these domain-speciÔ¨Åc categories, ad-
dress limitation 2.
154
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:48:35 UTC from IEEE Xplore.  Restrictions apply. The Domain Adaptation component also outputs a proba-
bilistic model for each category of questions. Each model is
able to assign a probability to a candidate answer sentence in
the Enhanced API Documentation based on its likelihood to
be an answer to a target question of a given category. These
models constitute the third output, i.e., Answer Patterns. Our
answer patterns are different from those of SiriusQA; rather
than being regular expressions, they are probabilistic models.
Both SiriusQA and APIBot automatically learn answer pat-
terns from a set of question-answer pairs. We choose to learn
a probabilistic model over regular expressions since the latter
tends to overÔ¨Åt, especially when we only have a small set of
training data. Our probabilistic answer patterns thus addresses
limitation 3.
Enhanced SiriusQA. All of the three outputs produced are
then plugged in to an enhanced SiriusQA. We enhance Sir-
iusQA by modifying its Answer Selection component to be
able to use the probabilistic Answer Patterns generated by
the Domain Adaptation component and address limitation 4.
The enhanced SiriusQA receives a question and produces an
answer by executing the following process:
The Question Interpretation component interprets the
question by matching it against each of the Question
Patterns generated by the Domain Adaptation component.
It produces a query for the Document Search component
and a question category. It follows the same process
described in Section II-A.
The Document Search component takes in a query and
outputs a ranked list of documents that are then fed to the
Answer Selection component. It follows the same process
described in Section II-A.
The Answer Selection component Ô¨Årst breaks returned
documents into candidate answer sentences. It then ranks
candidate answer sentences using the answer pattern
of the identiÔ¨Åed question category and a customized
keyword matching strategy that addresses limitation 4.
Answer 
SelectionEnhanced API 
Documentation
Original  Question
Interpretation
Document 
SearchQuery
AnswerQuestion
New/EnhancedLegendAnswer 
PatternsQuestion 
Patterns
Domain 
Adaptation
Enhanced SiriusQA
Fig. 1. APIBot Framework
B. Domain Adaptation
The Domain Adaptation is completed through two steps.
Each step is described in detail below.1) Step 1: Document Naturalization: In an API documenta-
tion, much information is stored in the document structure. For
example, the structure indicating API elements (e.g., classes,
methods, constructors, Ô¨Åelds, etc.) is described by different
description block. Unfortunately, as discussed in Section II,
SiriusQA cannot deal with structured documents. Simply
Ô¨Çattening structured documents into regular text Ô¨Åles would
not work since essential pieces of information stored in the
document structure would be lost. To tackle this challenge,
we convert the structured information in API documentation
to natural language sentences which can be understood by Sir-
iusQA. We design a number of rule-based heuristics that make
use of the regularities in the structure of API documentation to
create natural language sentences that explicitly describe the
implicit information stored in the API documentation struc-
ture. In this work, we focus on Javadoc API documentation
which is one of the most popular API documentation formats.
Similar rule-based heuristics can be designed for other API
documentation formats, which we leave as future work.
The Documentation Naturalization step takes in a Javadoc
API documentation and converts it to a collection of natural
language documents which we refer to as enhanced API
documentation. This step iterates over all documentation pages
(for classes and interfaces) and then transforms implicit infor-
mation stored in the Javadoc structure into plain sentences. For
instance, information from the inheritance block in a Javadoc
HTML document will be transformed into a sentence like
‚ÄúArrayList extends AbstractList.‚Äù.
2) Step 2: Question-Answer Pattern Generation: In this
step, we generate question and answer patterns from a set
of training question-answer pairs. Figure 2 shows a overview
of this step. The question patterns are generated through an
annotation process, while the answer patterns are generated
through an automated inference process. Both patterns are
grouped according to a predeÔ¨Åned category.
Original¬†Process¬† New/Enhanced ¬†ProcessLegendCategoryAnnotation
Training¬†QA¬†
PairsQuestion¬†
Patterns
InferenceAnswer¬†
Patterns
Fig. 2. Question-Answer Pattern Generation
Question Pattern Annotation: We consider a similar method-
ology as SiriusQA to create question patterns from a set of
training question-answer pairs (See Section II-A). The major
difference is that we consider a new set of domain speciÔ¨Åc
categories. SiriusQA has categories that are not suitable for
API documentation. We use the categories related to API
domain by following the work of Maalej and Robillard [15].
Excluding non-information, they deÔ¨Åned 11 types of domain
155
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:48:35 UTC from IEEE Xplore.  Restrictions apply. TABLE I
TAXONOMY OF QUESTION CATEGORIES
Category Description Example Question
Code ExampleQuestions requesting a code example for a speciÔ¨Åc task or
functionality.Do you have a sample code to append a string to a
StringBuilder?
ConceptsQuestions related to explanation of a particular API entity
or domain concepts.What is StringBuilder?
Control FlowQuestions related to actions which need to follow a partic-
ular execution order.What happens when you call notifyObservers() method in
Observable class?
DirectivesQuestions asking about allowed or disallowed contracts
provided by API elements.Can I control the number of threads to execute Completable-
Futures?
EnvironmentQuestions about API version, licences, compatibility issues
etc.What are the new things provided by arraylist class in this
version of API?
Functionality and Be-
haviorQuestion asking about a feature or functionality provided by
the API.What will happen if there are multiple duplicate objects exist
when using remove(Object o) of arraylist?
PatternsQuestions describing a speciÔ¨Åc operation and query about
how the API can be used to complete the operation.How can I add element into ArrayList?
Purpose and RationaleQuestions related to a design and/or purpose rationale of an
API element.Why is HashMap not thread safe?
Quality Attribute and In-
tenal AspectsQuestions related to non-fucntional aspects of an API.What are the two parameters of an instance of HashMap
that affect its performance?
ReferenceQuestions asking for a reference or extra information related
to an API element.What classed or interfaces I can read about which are most
similar to ArrayList?
Structure and Relation-
shipsQuestions pertaining to the hierarcy or organization of API. What are the implemented interfaces of HashMap?
knowledge inside API documentation. We create a category for
each one of the knowledge types. Table I shows the deÔ¨Ånition
and a sample question for each category. Given a new question,
APIBot makes use of annotated question patterns to decide the
category, target and contexts of a new question.
Answer Pattern Inference: This step takes as input a training
set of question-answer pairs from a given category. For every
question, its target phrase has been identiÔ¨Åed, and this target
must appear in the corresponding answer.
We abstract an answer into a generalized answer by fol-
lowing these steps: (1) We replace the target phrase with
htargeti; (2) We identify API elements, e.g., class names,
package names, etc., from each answer sentence and replace
them with their types, e.g., hclassi,hinterfacei,hpackagei;
(3) We replace every be-verb (e.g., is, are, etc.) with hBEi
and every verb with hVERBi1; (4) We use the symbol ‚Äú#‚Äù to
represent the starting point of a sentence; (5) All other words
are kept unchanged.
Next, we reduce the size of each generalized answers
by considering Nwords before and after the htargeti. For
instance, in the experiment, Nis set to 3 by default, then
every generalized answer is reduced to a sequence of 7 words,
including 3 words before and 3 words after the htargeti. We
refer to this sequence of words as a reduced answer.
For each question category, we then create an answer pattern
based on reduced answers of the category. An answer pattern
is a probability model consisting of a collection of word
probability distributions. Each distribution corresponds to one
of theNpositions before and after htargeti. It is created by
computing the relative frequency of words appearing on the
corresponding position in the reduced answers.
Given a question which has been interpreted by the Ques-
tion Interpretation component, question category and htargeti
are identiÔ¨Åed. Answer pattern of the corresponding question
1We identify verbs by using a part-of-speech (POS) taggercategory is then used to assign a probability score to a
candidate answer sentence. Each candidate answer sentence
is Ô¨Årst abstracted to a generalized answer and then reduced
to a sequence of 2N+ 1 words, with thehtargetiin the
center. The candidate answer sentence‚Äôs score is calculated by
multiplying probabilities of words appearing before and after
thehtargeti. These probabilities are inferred from the learned
answer pattern of the corresponding category.
C. Answer Selection
The output of Document Search component is a ranked list
of documents that contain query keywords. We consider only
top 100 documents for Ô¨Ånding the correct answer to a given
question. From these documents, we extract candidate answer
sentences and rank them. The answer ranking process contains
three steps: 1) document splitting and sentence Ô¨Åltering, 2)
sentence scoring, and 3) sentence ranking. We describe the
details of these steps below.
1) Step 1: Document Splitting and Sentence Filtering:
We split the retrieved documents into sentences. We consider
that a new sentence begins when a dot sign is followed by
whitespace(s) and the following word starts with a capitalized
Ô¨Årst character. Next, we remove sentences that are unlikely to
be an answer to the question. In particular, we check if names
of classes or interfaces described in the API documentation
appear in the question. If the names of such classes or
interfaces appear, then we remove sentences that do not appear
in the Javadoc API pages for those classes or interfaces, except
those that contain the names.
2) Step 2: Sentence Scoring: After retrieving documents,
splitting them into sentences, and Ô¨Åltering out-of-scope sen-
tences, we now have candidate answer sentences. To rank
these sentences, we use two scoring strategies: Pattern-SpeciÔ¨Åc
andDomain-SpeciÔ¨Åc. If the question category is known, we
compute two scores for each candidate answer sentence using
the two strategies. These scores are combined together into a
156
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:48:35 UTC from IEEE Xplore.  Restrictions apply. Ô¨Ånal score. However, if the category is not known, we only
run the domain-speciÔ¨Åc strategy. Both scoring strategies and
the score combination strategy are described below.
Pattern-SpeciÔ¨Åc Scoring. We use the answer pattern corre-
sponding to the category of an input question to assign a score
to a candidate sentence. The answer pattern is a probabilistic
model described in Section III-B2.
Domain-SpeciÔ¨Åc Scoring. While an answer pattern captures
the general pattern of nearby words surrounding a question‚Äôs
target, it ignores words that are located some distance away
from the target. These words may include important domain-
speciÔ¨Åc words. In this work, we consider all class and interface
names in a target Javadoc API documentation as domain-
speciÔ¨Åc words. All other words are considered general words.
Next, we match the domain-speciÔ¨Åc and general words
in the question with the words in the answer candidate. A
matching with a domain speciÔ¨Åc word is given a higher weight
and boosts the score higher compared to that of a general word.
Based on the matching, we compute the following domain-
speciÔ¨Åc score DMScore:
DMScore =w1#GenWord +w2#DomainWord
#Words
whereGenWord andDomainWord are the number of
matched general and domain words, respectively. #Words
is the total number of words. w1andw2are the weights for
general and domain words, respectively. We set w1andw2
to be 1 and 2, respectively. Consequently, we boost the score
by giving a matching of a domain-speciÔ¨Åc word twice the
importance of that of a general word.
3) Step 3: Score Normalization and Ranking: If a question
category can be inferred, for each answer candidate, we have
two scores, and these scores need to be uniÔ¨Åed. To unify
the scores, we Ô¨Årst normalize them. Next, we combine the
normalized scores as follows:
FinalScore =PMNormScore +DBNormScore
In the above equation, PMNormScore andDBNormScore
are the normalized pattern-speciÔ¨Åc and domain-speciÔ¨Åc scores,
respectively. By default, we consider both scores to be equally
important so we set both andto be 0.5.
IV. P RELIMINARY EXPERIMENTS AND RESULTS
A. Dataset
We select Javadoc of Java Development Kit 8 as our API
documentation corpus as it is one of the largest and most
popular APIs. As most Java API documentation follows the
same structure we believe results achieved on current dataset
should be generalizable enough for other Java APIs as well.
For training data, we create our own question patterns for
categories in Table I. We create the questions by Ô¨Åguring
out varying ways one can ask a question about a particular
category. We also make sure that our questions have answers
inside the API documentation.For evaluation data, we ask 4 PhD students and 2 Java
developers, all with more than 5 years of programming ex-
perience in Java to ask questions about each category with
a requirement that the answer to each question should be
found in an API documentation page. We collect a total of
100 questions: 10 for Concept category and 9 for each of
the remaining categories. After the collection of questions, we
assemble a team of 2 PhD students. We give the collected 100
questions to this team, who are required to go through all the
questions, and then Ô¨Ånd a sentence in the API documentation
(both enhanced and original) which could be considered as
the correct answer for each question assigned. Both the team
members had to discuss and come to an agreement before
coming up with sentence(s) that constitute as an answer to
each question. For 8 of the questions, the team cannot properly
understand them or Ô¨Ånd sentences in the documentation that
answer them. So, in the end, we had a total of 92 question
answer pairs, and we use this data as the ground truth to
evaluate the performance of APIBot. Our dataset is of similar
size as used in evaluating other specialized QA systems,
e.g., [16]. Although it is not very large, it covers all the
categories identiÔ¨Åed by Maalej and Robillard [15], which we
believe to be sufÔ¨Åcient for preliminary experiments.
B. Experiment Setting
Evaluation Metric: In the experiment, we query each question
and record the returned answers from APIBot. By default,
APIBot returns Ô¨Åve possible answers per question. We evaluate
the answers by matching them against the ground truth. To
measure the effectiveness of APIBot in answering questions
correctly, we use Hit@N metric [17], [18], [19], which
measures the percentage of questions for which we are able to
Ô¨Ånd the correct answer in the top N(e.g., 5) returned answers.
Baselines: We consider 3 baselines to compare with APIBot.
Baseline 1: It is the original SiriusQA system [9]. As the
original SiriusQA system returns only one top ranked answer
for each question, we modify it to return the top 5 ranked
answers so that it can be compared against APIBot using the
Hit@N evaluation criteria discussed above (for N=5). For this
baseline system, we use the unnormalized text extracted from
original API HTML documents as the knowledge source.
Baseline 2: This baseline consists of the original SiriusQA
system using the Enhanced API documentation as the knowl-
edge source. We use this baseline to measure the performance
beneÔ¨Åts provided by the Question-Answer Pattern Generation
step and enhanced Answer Selection component.
Baseline 3: This baseline is our APIBot system which
uses the original API documentation as the knowledge source
(rather than the Enhanced API Documentation). This baseline
is used to measure the performance beneÔ¨Åts provided by the
Document Naturalization step discussed in Section III.
C. Research Questions
RQ1: How effective is our approach in answering the
questions related to API documentation?
157
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:48:35 UTC from IEEE Xplore.  Restrictions apply. Table II shows the Hit@5 score of APIBot system as
compared to Baseline 1. The Hit@5 score of APIBot is
70.6% as compared to 2.2% for the baseline system. Thus, the
result shows that APIBot greatly improves the performance of
Baseline 1 (SiriusQA) showing the value of injecting domain
knowledge to it. Without domain knowledge, Sirius cannot
make use of question patterns and unable to Ô¨Ånd information
that is encoded in documentation structure.
TABLE II
PERFORMANCE OF APIB OT AND SIRIUS QA
Approach Hit@5
APIBot 70.6%
Baseline 1 (Original SiriusQA) 2.2%
RQ2: How important are Document Naturalization and
Answer Pattern Inference steps for accurately answering
questions related to API documentation?
The results of of our experiment w.r.t. RQ2 are shown in
Table III. It shows that Baseline 2, which performs Document
Naturalization step before using SiriusQA, does not result in
performance gain. This is because SiriusQA cannot identity
targets andcontexts for API related questions. On the other
hand, removing Document Naturalization step from APIBot
results in a big performance drop (i.e., from 70.6% to 18.5%).
Thus, the two steps need to be performed together to build an
effective question answering system for API documentation.
TABLE III
BENEFIT OF MAIN STEPS OF APIB OT
Approach Hit@5
Baseline 1 (Original SiriusQA) 2.2%
Baseline 2 (SiriusQA + Enhanced API Doc.) 2.2%
Baseline 3 (EnhancedSiriusQA + Original API Doc.) 18.5%
APIBot (EnhancedSiriusQA + Enhanced API Doc.) 70.6%
V. R ELATED WORK
Murgia et al. developed an experimental bot to answer
questions on StackOverÔ¨Çow [7]. It was trained to handle
simple (and reoccurring) questions related to giterror mes-
sages, and provide answers based on previous solutions to
similar problems. The main focus was to understand how
the bot is perceived when presented as a person versus as
a bot. Storey and Zagalsky suggested that we could use bots
to automate many tasks in software development [8]. There
also has been work on retrieving relevant information from
API documentation [1], [20]. Our work enriches the above
mentioned studies by proposing a support bot to help in
answering questions related to API documentation.
In information retrieval and natural language processing
domain, many work has been done in building general-purpose
QA systems, e.g., [13], [21]. In this paper, we propose a
domain-speciÔ¨Åc QA system by incorporating domain knowl-
edge to customize a general-purpose QA system.
VI. C ONCLUSION AND FUTURE WORK
In this paper, to help developers Ô¨Ånd answers to API-related
questions, we developed the Ô¨Årst question answering bot for
API documentation namely APIBot. APIBot is built on top
of SiriusQA. We made various efforts in adapting SiriusQAfor API documentation. Our preliminary experiment with 92
API-related questions demonstrates that APIBot can achieve a
promising Hit@5 score of 0.706, and highlights the value of
our adaptation strategies.
As a future work, we plan to pursue the following di-
rections. First, we would like to build a larger benchmark
data from various API documentations. Second, we would
like to investigate possibility to reuse answer patterns learned
from one API documentation for another. Third, we want to
investigate the possibility of using other underlying general-
purpose QA systems beyond SiriusQA by following similar
adaptation strategy presented in this paper. Fourth, we also
plan to develop additional strategies to better handle structural
information inherent in API documentation. The strategy that
we employ in the document naturalization step may not be the
optimal one.
REFERENCES
[1] J. Stylos and B. A. Myers, ‚ÄúMica: A web-search tool for Ô¨Ånding api
components and examples,‚Äù in VL/HCC, 2006.
[2] H. Zhong, L. Zhang, T. Xie, and H. Mei, ‚ÄúInferring resource speciÔ¨Åca-
tions from natural language API documentation,‚Äù in ASE, 2009.
[3] R. Pandita, X. Xiao, H. Zhong, T. Xie, S. Oney, and A. Paradkar, ‚ÄúIn-
ferring method speciÔ¨Åcations from natural language API descriptions,‚Äù
inASE, 2012.
[4] D. G. Novick and K. Ward, ‚ÄúWhy don‚Äôt people read the manual?‚Äù in
SIGDOC, 2006.
[5] M. Kajko-Mattsson, ‚ÄúA survey of documentation practice within correc-
tive maintenance,‚Äù EMSE, 2005.
[6] S. Ma, D. Lo, T. Li, and R. H. Deng, ‚ÄúCdrep: Automatic repair of
cryptographic misuses in android applications,‚Äù in AsiaCCS, 2016.
[7] A. Murgia, D. Janssens, S. Demeyer, and B. Vasilescu, ‚ÄúAmong the
machines: Human-bot interaction on social Q&A websites,‚Äù in CHI,
2016.
[8] M.-A. Storey and A. Zagalsky, ‚ÄúDisrupting developer productivity one
bot at a time,‚Äù in FSE, 2016.
[9] J. Hauswald, M. A. Laurenzano, Y . Zhang, C. Li, A. Rovinski, A. Khu-
rana, R. G. Dreslinski, T. Mudge, V . Petrucci, L. Tang et al., ‚ÄúSirius: An
open end-to-end voice and vision personal assistant and its implications
for future warehouse scale computers,‚Äù in ASPLOS, 2015.
[10] D. Huggins-Daines, M. Kumar, A. Chan, A. W. Black, M. Ravishankar,
and A. I. Rudnicky, ‚ÄúPocketsphinx: A free, real-time continuous speech
recognition system for hand-held devices,‚Äù in ICASSP, 2006.
[11] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel,
M. Hannemann, P. Motlicek, Y . Qian, P. Schwarz et al., ‚ÄúThe Kaldi
speech recognition toolkit,‚Äù in ASRU, 2011.
[12] D. Rybach, S. Hahn, P. Lehnen, D. Nolden, M. Sundermeyer, Z. T ¬®uske,
S. Wiesler, R. Schl ¬®uter, and H. Ney, ‚ÄúRASR-The RWTH Aachen
university open source speech recognition toolkit,‚Äù in ASRU, 2011.
[13] N. Schlaefer, J. Ko, J. Betteridge, M. A. Pathak, E. Nyberg, and
G. Sautter, ‚ÄúSemantic extensions of the Ephyra QA system for TREC
2007.‚Äù in TREC, 2007.
[14] H. Bay, T. Tuytelaars, and L. Van Gool, ‚ÄúSurf: Speeded up robust
features,‚Äù in ECCV, 2006.
[15] W. Maalej and M. P. Robillard, ‚ÄúPatterns of knowledge in api reference
documentation,‚Äù TSE, 2013.
[16] O. Tsur, M. de Rijke, and K. Sima‚Äôan, ‚ÄúBiographer: Biography questions
as a restricted domain question answering task,‚Äù in ACL, 2004.
[17] R. K. Saha, M. Lease, S. Khurshid, and D. E. Perry, ‚ÄúImproving bug
localization using structured information retrieval,‚Äù in ASE, 2013.
[18] P. Runeson, M. Alexandersson, and O. Nyholm, ‚ÄúDetection of duplicate
defect reports using natural language processing,‚Äù in ICSE, 2007.
[19] X. Wang, L. Zhang, T. Xie, J. Anvik, and J. Sun, ‚ÄúAn approach to
detecting duplicate bug reports using natural language and execution
information,‚Äù in ICSE, 2008.
[20] C. Treude, M. P. Robillard, and B. Dagenais, ‚ÄúExtracting development
tasks to navigate software documentation,‚Äù TSE, 2015.
[21] X. Yao, B. Van Durme, and P. Clark, ‚ÄúAutomatic coupling of answer
extraction and information retrieval.‚Äù in ACL, 2013.
158
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:48:35 UTC from IEEE Xplore.  Restrictions apply. 