Feature-Model Interfaces: The Highway to Compositional
Analyses of Highly-ConÔ¨Ågurable Systems
Reimar Schr√∂ter,1Sebastian Krieter,1Thomas Th√ºm,2Fabian Benduhn,1and Gunter Saake1
1University of Magdeburg, Germany
2TU Braunschweig, Germany
ABSTRACT
Today's software systems are often customizable by means
of load-time or compile-time conguration options. These
options are typically not independent and their dependen-
cies can be specied by means of feature models. As many
industrial systems contain thousands of options, the mainte-
nance and utilization of feature models is a challenge for all
stakeholders. In the last two decades, numerous approaches
have been presented to support stakeholders in analyzing
feature models. Such analyses are commonly reduced to sat-
isability problems, which suer from the growing number
of options. While rst attempts have been made to decom-
pose feature models into smaller parts, they still require to
compose all parts for analysis. We propose the concept of
a feature-model interface that only consists of a subset of
features, typically selected by experts, and hides all other
features and dependencies. Based on a formalization of fea-
ture-model interfaces, we prove compositionality properties.
We evaluate feature-model interfaces using a three-month
history of an industrial feature model from the automotive
domain with 18,616 features. Our results indicate perfor-
mance benets especially under evolution as often only parts
of the feature model need to be analyzed again.
CCS Concepts
Software and its engineering !Software product lines;
Formal software verication; Feature interaction; Abstrac-
tion, modeling and modularity;
Keywords
Congurable Software, Software Product Line, Variability
Modeling, Feature Model, Modularity, Compositionality
1. INTRODUCTION
There is a growing need to customize software. This
demand is often based on conicting functional and non-
functional requirements of each customer. Systematic reuse
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proÔ¨Åt or commercial advantage and that copies bear this notice and the full cita-
tion on the Ô¨Årst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciÔ¨Åc permission
and/or a fee. Request permissions from permissions@acm.org.
ICSE ‚Äô16, May 14-22, 2016, Austin, TX, USA
c2016 ACM. ISBN 978-1-4503-3900-1/16/05. . . $15.00
DOI:http://dx.doi.org/10.1145/2884781.2884823between customized software systems can be achieved by
means of load-time or compile-time variability [7]. Com-
mon and variable artifacts of such congurable software are
specied in terms of features, each representing a unit of
functionality. However, not all features are compatible with
one another, and some features force the existence of oth-
ers. Feature models are typically used to describe all valid
combinations of features [24], and can be represented as a
hierarchical model or as a propositional formula [8].
As feature models specify valid combinations of features,
they inuence all phases of the development process for con-
gurable software from requirements engineering to verica-
tion. In particular, all software analyses, such as type check-
ing (e.g., [16, 45]) or model checking (e.g., [14, 28]), have to
incorporate feature models to produce sound and complete
results [46]. Hence, we are interested in all defects for valid
feature combinations while defects for invalid combinations
are not of interest. Besides such lifting of existing analyses,
also numerous new analyses devoted to feature models have
been presented [9]. For instance, it is intended that all fea-
tures occur in at least one valid combination [24]. All these
analyses have in common that they are reduced to one or
more satisability problems by translating the feature model
into a propositional formula [9, 46]. Hence, when the feature
model evolves, usually the complete analysis has to be exe-
cuted again even if the feature model changes only slightly.
This work is motivated by recent experiences in applying
our feature-modeling tool FeatureIDE [47] to real feature
models of our industrial partner. Dozens of stakeholders
maintain a feature model from the automotive domain with
18,616 features and they face scalability problems for anal-
yses, especially as they need to analyze the complete model
again after every change. In addition, they expect that the
feature model grows even further, which is also known from
other industrial models [11], and from the Linux kernel with
thousands of features [30]. We studied existing approaches
to compose feature models from smaller parts [3, 6, 12, 13,
36, 41] but none of them considers compositional reasoning.
That is, even if only a single part changes, we still have to
compose all parts and perform all analyses again.
We propose to use feature-model composition with fea-
ture-model interfaces to enable compositional analyses. A
feature-model interface is a feature model with a subset of all
features that are selected and intended to be used by domain
experts in a specic composition scenario. Similar to in-
terfaces of programming languages, feature-model interfaces
hide information (i.e., features) and can be used instead of
the original feature model. As result, the feature-model in-
2016 IEEE/ACM 38th IEEE International Conference on Software Engineering
   667
terfaces can be considered as placeholder and reduce the
complexity of the composed model. In contrast to program-
ming interfaces, the consumer denes the interface and not
the provider. In this paper, we prove compositionality prop-
erties for feature-model interfaces. In addition, we illustrate
their benets by applying a time-consuming feature-model
analysis to our industrial feature model. In particular, we
make the following contributions:
We formally dene and illustrate feature-model composi-
tion and feature-model interfaces.
We prove compositionality between the analysis results of
compositions with and without feature-model interfaces.
We evaluate the potential of feature-model interfaces for
the three-month evolution of a real-world feature model.
2. FEATURE MODELS AND ANALYSES
In this section, we introduce feature models, formalize
them, and present challenges regarding their correctness.
Based on this, we also formalize common analyses that we
investigate in the remainder of this paper.
2.1 Feature Models
A feature model describes a set of features and their valid
combinations. Typically, feature diagrams are used to rep-
resent feature models graphically by arranging the features
in a tree structure with additional cross-tree constraints to
describe their dependencies [8, 24]. In Figure 1, we illustrate
dierent types of feature dependencies by using the exam-
ple of feature model Index , which represents a set of index
structures to optimally support direct access of data items
in a database. An index can only support one data type at
a time. Thus, the features Int,Double , and Float are ar-
ranged in an alternative group. Furthermore, the developer
can choose one or both of the search algorithms k-nearest
neighbor ( Knn) and Range . Since the query algorithms are
independent of each other, they are represented by an or
group. In addition, it is optional to force unique keys in
an index structure for which we include an optional feature
UniqueKeys . Since it is only possible to support unique keys
for integer values, the model includes the additional cross-
tree constraint UniqueKeys)Intas propositional formula.
Besides feature diagrams, some other representations of
feature models exist, such as propositional formulas, textual
representations, or an enumeration of all valid congura-
tions. To simplify our proofs, we formalize feature models
as the set of all valid congurations:
Denition 1. A feature modelMxis a tuple (Fx;Px),
where (a)Fxis a set of features, and
(b)Pxis a set of products with Px2Fx.
In Figure 1, we exemplify this denition using feature mo-
delIndex . However, this representation does not scale well
in an actual implementation of any analysis and, thus, we
rely on a conventional feature-model representation (i.e., a
propositional formula) for our evaluation.
2.2 Feature-Model Analyses
In industry, feature models may consist of thousands of
features [11]. Thus, automated consistency checks for large-
scale feature models are important. Benavides et al. present
an overview of automated analyses and consider them as
an information extraction process that is executed in two
steps [9]. First, an analysis tool translates the feature mo-
Figure 1: Feature model MIndex = (F Index ;PIndex )
(highlighted characters of the feature diagram are
used to represent FIndex andPIndex).
del into a specic representation (e.g., a propositional for-
mula). Second, a specic algorithm uses a corresponding
solver to perform the analysis. In the following, we dene
several analyses based on our feature-model formalization
and illustrate their usage with feature model MIndex.
Denition 2. LetMx= (F x;Px)be a feature model and
Mthe set of all feature models of the universe, then
void =fM x2MjP x=;g (2.1)
core(M x) =\
p2Pxp (2.2)
dead(M x) =Fxn[
p2Pxp (2.3)
pConf (M x) =f(F S;FD)j
9p2P x:FSp^F DF xnpg(2.4)
aSet(M x) =fp2aSub (M x)j
8q2aSub (M x) :p6qg(2.5)
aSub (M x) =fqjq6=;;Px6=;;
8p2P x:qp_qF xnpg(2.6)
A feature model is void if and only if it represents no
products [8, 9, 24]. The analysis is particularly helpful for
huge feature models with thousands of features to check the
feature-model correctness [8, 9, 24, 50]. Using our formal-
ization (see void in Eq. 2.1), we get the result that feature
modelMIndex is not void, because it is not contained in the
set of all void feature models (i.e., MIndex62void).
Acore feature is a feature that is included in all products
of a non-void product line [9, 52]. The analysis can be used
to determine feature priorities, as it may be useful to de-
velop core features rst [9, 51]. Using our formalization (see
core(M x) in Eq. 2.2) with feature model MIndex, we obtain
the setfIndex ;Queries ;Typesg.
A feature of a feature model is a dead feature if and only
if it is not part of any valid product of a non-void product
line [9, 24]. The analysis is particularly useful to identify
contradictions in feature models [21]. Furthermore, it avoids
implementing features that are not part of any product. For
feature modelMIndex, the application of function dead (see
dead(M x) in Eq. 2.3) results in an empty set and, thus, the
feature model does not contain any dead features.
Apartial conguration is a tuple consisting of a set of se-
lected featuresFSand a set of deselected features FDwith
the restrictionFS[F DF xandFS\F D=;. If the union
ofFSandFDis equal toFxthe tuple represents a full con-
guration, which describes exactly one product [9]. Hence,
a full conguration is included in our denition as a spe-
cial case. The analysis of partial congurations investigates
668whether a partial conguration fullls all the dependencies
of the corresponding feature model [8, 9, 24]. Using our for-
malization (see pConf (M x) in Eq. 2.4) for feature model
MIndex, the conguration ( fI,Q,T,K,ng;fU,R,D,Fg) is part
of the resulting congurations of function pConf and, thus,
a valid conguration. By contrast, the partial conguration
(fI,U,Dg;;) is invalid because it does not conform to the
cross-tree constraint and alternative group.
Anatomic set is a non-empty set of features, which is ei-
ther completely included or completely absent in each prod-
uct. The analysis is used to reduce the size of a feature
model as input for further analyses [9, 42, 53]. However,
most implementations only consider a mandatory feature
and its parent as atomic set [9, 53]. By contrast, similar
to Dur an et al. [18], we formally dene the function aSet
(atomic Set) , which uses a feature model Mxas input and
returns the set of all atomic sets (see aSet(M x) in Eq. 2.5).
To ease the denition of atomic sets with function aSet, we
use a second function aSub (atomic Subset) that determines
allatomic subsets , i.e., all subsets of features that are either
completely included or completely absent in each product.
However, function aSet is used to nd all super sets in aSub 's
result, i.e., the function removes all subsets that are com-
pletely contained in other sets. Hence, the set of atomic
sets is always a subset of the set of atomic subsets (i.e.,
aSet(M x)aSub (M x)). As an example, we use function
aSet to determine all atomic sets of feature model MIndex.
Besides the atomic sets with one feature, there is one atomic
set containing all core features.
3. PROBLEM STATEMENT
As seen in the Linux kernel, in the application scenario
of our industrial partner, and in other case studies [11, 44,
47], the complexity of feature models can be challenging for
humans and machines. In the following, we explain some of
the most important problems.
Manual construction and maintenance of a feature mo-
del with thousands of features is almost impossible because
the size is overwhelming and blurs important feature de-
pendencies. Furthermore, feature diagrams do not scale for
this feature-model size. Therefore, decomposition is used
to handle large feature models [11, 35]. Using this strat-
egy, dierent groups of domain experts can work on smaller
feature models, which are easier to understand and to visu-
alize. If we use feature-model composition to reuse existing
feature models in another one, it is possible that only some
features are needed to describe or understand the feature-
model dependencies. A complete feature-model reuse can
aect the comprehension of the composition and again blur
the important dependencies.
Decomposition of a large feature model into smaller frag-
ments is one state-of-the-art strategy to ease human com-
prehensibility and maintainability. However, to use the pre-
viously described analyses for a large, decomposed feature
model, it is necessary to combine its fragments into a rep-
resentation that is analyzable using existing techniques [41].
The result is again a large feature model for which not all
analyses scale. Thus, the scalability problem of complex
analyses in the context of large feature models is not solved
yet. A compositional procedure in which we can reuse the
results of analyses in smaller feature models for the compu-
tation in a feature-model composition is desirable.The previous problems are by themselves hard to solve
but if we take evolution of decomposed feature models into
account, the situation gets even worse. In this scenario, we
consider changes in feature-model fragments, where we need
to re-execute all desired analyses. In particular, it is chal-
lenging to reuse existing analysis results of the previous fea-
ture-model version and, thus, it is necessary to re-compute
the complete analysis. However, evolutionary changes are
not unusual [2, 11, 17], e.g., within three months, the fea-
ture model of our industrial partner was extended by more
than 4,000 features. Therefore, it is desirable to reduce the
amount of (complete) re-computations, when feature-model
fragments change.
4. COMPOSITIONALITY BASICS
As we aim to reduce the mentioned problems, we pro-
pose to use a combination of feature-model composition and
feature-model interfaces. Based on their formalization, we
prove compositionality properties in the remaining paper.
4.1 Feature-Model Composition
Multiple mechanisms exist that allow us to combine fea-
ture models [3, 6, 12, 13, 36]. In this paper, we consider the
composition through aggregation, i.e., by inclusion of one
feature model as an instance in another feature model [36].
In our running example, we want to create an instance of
feature modelMIndex by connecting it to the feature Access
in the database feature model MDBMS . To further spec-
ify their relationship, we add two additional cross-tree con-
straints to the resulting feature model. We depict the com-
position result on the left side of Figure 2 (feature model
MCis used to describe the dependencies of the models).
We dene semantics of the composition as follows:
Denition 3. LetMx= (Fx;Px),My= (Fy;Py), and
MC= (F C;PC)be feature models with FCF x[F y. We
dene the function compositionusingMx;My, andMC
with inx notation MCbased on the join function and
functionRto achieve the composed feature model Mx=y:
Mx=y=(M x;My;MC) =MxMCMy=
(M xR(M y))M C (3.1)
R(M y) =R((F y;Py)) = (F y;Py[f;g) (3.2)
MxM y= (Fx;Px)(Fy;Py) =
(Fx[F y;fp[qjp2P x;q2P y;p\F y=q\F xg) (3.3)
The denition of the composition function is based on
the functions(join ) and R(remove core property ). Func-
tion Rtakes one feature model as input and converts it to
a new feature model in which the empty product is a valid
product. Thus, the feature set is identical to the input fea-
ture model and the set of products is extended by the empty
set. Ris used in function to ensure thatMy's core features
are not necessarily core in the composed feature model.
The join function takes two feature models as input and
returns a new combined feature model, which is a merge
of all input information (i.e., features and products). In
detail, the resulting feature model consists of all features
from the input feature models. To combine the product sets
of both input models, we use an operation that is similar to
a join as known from relational algebra [15]. Like the join,
we only combine two products if the additional condition
669Figure 2: Composition of the feature models MDBMS andMIndexusing the feature model MC, which describes
dependencies between both feature models.
p\F y=q\F xis fullled. Consequently, the function's
result is a feature model that conforms to our Denition 1.
In particular, if both feature sets are disjoint, the condition
p\F y=q\F x=;is always true. Hence, our function
behaves similar to a cross product from relational algebra
and creates all combinations between both product sets.
The composition function , based on the functions R
and, uses three feature models as input to create a new
one. The second feature model Myis instantiated in the rst
feature modelMx. The third feature model MCdescribes a
parent-child relationship and other inter-model constraints
betweenMxandMyin order to connect both models.
While functionallows us to combine arbitrary feature mo-
dels, our proofs (cf. Section 5) are based on the assumption
thatFxandFydo not share features (i.e., Fx\F y=;).
In Figure 2, we exemplify all three functions using MDBMS
andMIndex. To instantiate feature model MIndex in feature
modelMDBMS , we have to transform feature model MIndex
using our function Rand create all product combinations
using our join function (i.e., MDBMSR(M Index)). This
results in additional product combinations that are not part
of the nal feature model MDBMS =Index due to the absence
of feature modelMC. We depict the intermediate result in
Figure 2. Finally, we eliminate all unintended products us-
ing the join function with the intermediate result and the
feature modelMC, which contains the desired parent-child
dependency and the two cross-tree constraints. The high-
lighted products of feature model MDBMS =Index represent
the nal result of our composition function .
4.2 Feature-Model Interfaces
We now formally dene feature-model interfaces and prove
algebraic properties of feature-model interfaces that we need
for our compositionality proofs.
4.2.1 Formalization of Feature-Model Interfaces
We dene a feature-model interface as follows:
Denition 4. A feature modelMInt= (FInt;PInt)is an in-
terface of feature model Mx= (F x;Px)denoted asMInt
Mx, if and only if (a) FIntF xand
(b)PInt=fp\F Intjp2P xg.
If two feature models MxandMIntdo not fulll this de-nition (i.e.,MInt6M x), we callMIntincompatible toMx.
From Denition 4, we can infer that for each pair Mxand
FIntthere exists exactly one feature-model interface. A vital
characteristic of a feature-model interface is that it is a fea-
ture model itself. Therefore, we are able to use an interface
instead of a feature model for composition. In detail, the
feature-model interface MInthas a possibly reduced set of
features compared to feature model Mx. Furthermore, each
product ofMIntis a subset of a product of Mx, including
only features from FInt. Moreover, each product of Mxis a
super set of one or more products of MInt.
Corollary 5. 8p2P Int9q2P x:p=q\F Int
8q2P x9p2P Int:p=q\F Int
For our theoretical investigation of feature-model inter-
faces, we dene a function S(slice ) (similar to the slice op-
erator proposed by Acher et al. [4]) that allows us to generate
a feature-model interface by removing a given set of features,
which are of no interest for a specic target domain.
Denition 6. We dene a function S that takes a feature
modelMx= (F x;Px)and a set of features FRas input and
returns a feature model MIntwithMIntM x.
MInt=S(M x;FR) = (F xnFR;fpnFRjp2P xg)
For our running example, we want to reuse MIndex as
enhancement ofMDBMS . However, some features are not of
our interest and, thus, we apply function SonMIndex with
the set of features FRto be removed,FR=fRange ;Unique -
Keys ;Floatg(cf. Figure 3). In practice, FRdepends on the
specic reuse scenario in agreement with the stakeholders.
4.2.2 Algebraic Properties of Interfaces
Next, we take a look at certain properties of function S,
which we will use in our proofs for compositionality. In de-
tail, we investigate its right identity for certain feature sets
and the distributivity with the functions and R.
Right Identity. A feature setFRis a right identity element
toSifFxdoes not contain any feature from FR. As result,
the application of Shas no eect on a feature model that
does not contain a feature of the feature set FR.
Lemma 7. LetMx= (F x;Px)be a feature model and FR
a set of features with Fx\F R=;, then S (M x;FR) =Mx.
670Proof. As the intersection of FxandFRis the empty set,
there will be no feature that is removed from the set Fx. The
result is the identical feature set Fx. Similarly, the intersec-
tion between each product and the set of features FRis also
empty and, thus, each product will be the same as before.
S((Fx;Px);FR) = ((F xnFR);fpnFRjp2P xg) (7.1)
= (Fx;Px) =Mx
Distributivity of and S.The order in which we apply the
functionsand Sis not relevant for the result.
Lemma 8. LetMx= (F x;Px),My= (F y;Py)be feature
models andFRa set of features, then
S(M xM y;FR) =S(M x;FR)S(M y;FR):
Proof. In general, we separate the application of the func-
tion S on each part of the composed feature model so that
we can apply function later on.
S((Fx;Px)(Fy;Py);FR) (8.1)
= (Fz;Pz) (8.2)
= ((Fx[F y)nFR;Pz) (8.3)
= ((FxnFR)[(FynFR);Pz) (8.4)
Next, without loss of generality, we introduce the sets r and
s to represent the results of function S, which are then used
as input for function .
= (Fz;f(p[q)nFRj
p2P x;q2P y;p\F y=q\F xg) (8.5)
= (F z;f(pnFR)[(qnFR)j
p2P x;q2P y;p\F y=q\F xg) (8.6)
(Denition 4)= (F z;fr[sjr2fpnFRjp2P xg;
s2fqnFRjq2P yg;
r\F y=s\F xg) (8.7)
(Denition 3)=S(M x;FR)S(M y;FR)
Distributivity of Rand S.Finally, we prove that the order
in which we apply the functions Rand Sis not relevant.
Lemma 9. LetMx= (F x;Px)be a feature model and FR
a set of features, then S (R(M x);FR) =R(S(M x;FR)).
Proof. Function R adds the empty set to the set of prod-
ucts. To prove the interaction of R with S, it is necessary
to extract this empty set from the input feature model of S.
S(R(M x);FR) = (F xnFR;fpnFRjp2(Px[f;g)g) (9.1)
= (FxnFR;fpnFRjp2P xg[f;gg ) (9.2)
=R((FxnFR;fpnFRjp2P xg)) (9.3)
=R(S(M x;FR))
5. COMPOSITIONALITY IN THEORY
First, we present the general idea of compositionality that
is based on feature-model composition and feature-model in-
terfaces. Second, we show the potential of this combination
using the presented feature-model analyses of Section 2.
5.1 Compositionality Principle
To introduce our general concept of feature-model compo-
sitionality, we assume that two feature models MxandMy
Figure 3: Application of function Son feature model
MIndex. The highlighted products are part of the
resulting feature-model interface MInt.
are composed to Mx=y=MxMCMy. Typically, not all
features of feature model Myare of interest for the compo-
sition with feature model Mx. Given the knowledge about
those features, it is possible to create a feature-model inter-
faceMIntbased onMy(i.e.,MIntM y) with all features
of interest. Now, it is possible to use feature model MInt
instead of feature model Myfor feature-model composition
withMx(i.e.,Mx=Int=MxMCMInt) and subsequently
useMx=Intfor automated analyses. However, analysis re-
sults forMx=Intmight dier from the results for Mx=y, due
to the reduced feature set of MInt. Therefore, we identify
and prove specic relations between analysis results based
onMx=IntandMx=yfor each analysis of Section 2. For
instance, the analysis result of dead features for Mx=Intis
a subset of the result for Mx=y. However, the main prot
of this dependency exists in an evolution scenario. If a new
version ofMyexists that still conforms to the interface, the
results forMx=Intare identical.
For our proofs regarding the analysis-result relations of
feature modelMx=IntandMx=y, we have to consider an
additional property. In detail, we prove that a feature-mod-
el composition based on a feature-model interface MIntis
itself a feature-model interface in relation to a composition
based onMy(i.e.,Mx=IntM x=y). This property is based
on the assumption, that we only remove features from the set
Fywhereas the feature sets FxandFCremain unchanged.
Lemma 10. LetMx=y=MxMCMy,Mx=Int=MxMC
MIntbe composed feature models based on the models Mx=
(Fx;Px),My= (Fy;Py),MC= (FC;PC),MInt=S(M y;
FR)withFR\F x=FR\F C=;, then:Mx=IntM x=y.
Proof. Given the algebraic properties of the function S and
the denition of our composition function MC, the follow-
ing relations hold:
Mx=yS(M x=y;FR) (10.1)
=S(M xMCMy;FR) (10.2)
(Eq. 3.1)=S((M xR(M y))M C;FR) (10.3)
(Lemma 8)= (S (M x;FR)S(R(M y);FR))
S(M C;FR) (10.4)
(Lemma 7)= (M xS(R(M y);FR))M C (10.5)
671(Lemma 9)= (MxR(S(M y;FR)))M C (10.6)
(Denition 6)= (MxR(M Int)))M C (10.7)
(Eq. 3.1)=MxMCMInt (10.8)
=Mx=Int
As a result of this proof, we can consider feature model
Mx=Intas an ordinary feature-model interface of Mx=y, in
which the knowledge about the initial composition is not
relevant. Thus, it is sucient to prove the analysis-result
relations between feature model MIntandMybecause the
same dependency holds for feature model Mx=IntandMx=y.
5.2 Compositionality of Existing Analyses
In this section, we investigate the compositionality of the
analyses of void feature model ,core features ,dead features ,
valid partial congurations , and atomic sets . For each analy-
sis, we rst examine the analysis-result relation between fea-
ture modelMyandMIntfollowed by an investigation of the
composed feature models using the following two premises:
Premise 1. LetMy= (F y;Py)be a feature model and
MInt=S(M y;FR) = (F Int;PInt)its feature-model inter-
face (i.e.,MIntM y).
Premise 2. LetMx=y=MxMCMy,Mx=Int=MxMC
MIntbe composed feature models based on the feature models
Mx= (F x;Px),My= (F y;Py),MC= (F C;PC),MInt=
S(M y;FR)withFR\F x=FR\F C=;.
5.2.1 Void Feature Model
With respect to Premise 1, MIntis void if and only if My
is void.
Theorem 11.My2void,M Int2void:
Proof. With Corollary 5, the following equivalences hold:
My2void,P y=; (11.1)
(Corollary 5),P Int=; (11.2)
,M Int2void
Based on this knowledge and Premise 2, we deduce that
a feature modelMx=Intis void if and only if Mx=yis void.
Theorem 12.Mx=y2void,M x=Int2void.
Proof. From Lemma 10 and Theorem 11, we infer that the
same analysis-result relation is also valid for Mx=Int and
Mx=y.
5.2.2 Core Features
With respect to Premise 1, a feature f2F Intis a core
feature ofMIntif and only if fis a core feature of My.
Theorem 13. core(M y)\F Int=core(M Int):
Proof. With Denition 4, the following equation holds:
core(M Int) =\
p2PIntp (13.1)
(Denition 4)=\
p02Py(p0\F Int) (13.2)
= (\
p02Pyp0)\F Int (13.3)
=core(M y)\F IntTherefore, we can conclude that, if a feature fis a core
feature ofMInt, it is also a core feature of My. In addition,
if we determine core features of Mythat are also part of
MInt, these are also core features of feature model MInt.
f2core(M Int))f2core(M y)
f2core(M y)\F Int)f2core(M Int)
Using Theorem 13 and Premise 2, for composed feature
models, we can deduce that a feature f2F x=Intis a core
feature ofMx=Intif and only if fis a core feature in Mx=y.
Theorem 14. core(M x=y)\F x=Int=core(M x=Int):
Proof. Analogous to Theorem 12.
5.2.3 Dead Features
In compliance with Premise 1, a feature f2F Intis a dead
feature ofMIntif and only if fis a dead feature of My.
Theorem 15. dead(M y)\F Int=dead(M Int)
Proof. Based on Denition 4, the following equations hold:
dead(M Int) =FIntn[
p2PIntp (15.1)
(Denition 4)= (Fy\F Int)n([
p02Py(p0\F Int)) (15.2)
= (Fy\F Int)n(([
p02Pyp0)\F Int) (15.3)
= (Fyn[
p02Pyp0)\F Int (15.4)
=dead(M y)\F Int
Therefore, if a feature fis a dead feature in MInt, it is
also a dead feature in My. Furthermore, if a feature fis a
dead feature in feature model Myandfis also part ofMInt,
it is also a dead feature in feature-model interface MInt.
f2dead(M Int))f2dead(M y)
f2dead(M y)\FInt)f2dead(M Int)
Again, we take a look into the relations of analysis results
regarding feature-model compositions. Using Premise 2, a
feature f2F x=Intis a dead feature of feature model Mx=Int
if and only if fis a dead feature of Mx=y.
Theorem 16. dead(M x=y)\F x=Int=dead(M x=Int)
Proof. Analogous to Theorem 12.
5.2.4 Valid Partial ConÔ¨Ågurations
Regarding Premise 1, a conguration C= (F S;FD) with
FSF Int,FDF Intis a valid partial conguration of
MIntif and only if Cis a valid partial conguration of My.
Theorem 17. pConf (M Int) =
f(F S\F Int;FD\F Int)j(FS;FD)2pConf (M y)g
Proof. With Denition 4, the following equation holds:
pConf (M Int)
(Denition 2)=f(F S;FD)j9p2P Int:
FSp^F DF Intnpg (17.1)
(Corollary 5)=f(F S;FD)j9q2P y:
FSq\F Int^
FDF Intn(q\F Int)g (17.2)
672(FIntF y)=f(F S;FD)j9q2P y:
FSq\F Int^
FD(Fy\F Int)n(q\F Int)g (17.3)
=f(F S;FD)j9q2P y:
FSq\F Int^
FD(Fynq)\F Intg (17.4)
=f(F S\F Int;FD\F Int)j9q2P y:
FSq^F DF ynqg (17.5)
(Denition 2)=f(F S\F Int;FD\F Int)j
(FS;FD)2pConf (M y)g
As result, we know that each valid partial conguration
ofMIntis also a valid partial conguration of Myand valid
partial congurations of Myare also valid partial congu-
rations ofMIntifFSandFDare intersected with FInt.
(FS;FD)2pConf (MInt))(FS;FD)2pConf (My)
(FS;FD)2pConf (My))
(FS\FInt;FD\FInt)2pConf (MInt)
Based on Theorem 17 and Premise 2, we consider the
relationship of analysis results of composed feature models.
Hence, a partial conguration with FSFx=Int andFD
Fx=Int is a valid partial conguration of Mx=Intif and only
if (F S;FD) is a valid partial conguration of Mx=y.
Theorem 18. pConf (M x=Int) =
f(F S\F x=Int;FD\F x=Int)j(FS;FD)2pConf (M x=y)g
Proof. Analogous to Theorem 12.
5.2.5 Atomic Sets
With respect to Premise 1, a feature set Awith AF y
is an atomic subset with A\F Intof feature model MIntif
and only if Ais an atomic subset of feature model My. We
prove this relation using function aSub .
Theorem 19. aSub (M Int) =
fq\F Intjq2aSub (M y);q\F Int6=;g
Proof. aSub (M Int)
(Denition 2)=fqjq6=;;PInt6=;;
8p2P Int: (qp)_(qF Intnp)g(19.1)
(Corollary 5)=fqjPy6=;;q6=;;
8p2P y: (qp\F Int)_
(q(Fynp)\F Int)g (19.2)
=fq\F IntjPy6=;;q6=;;q\F Int6=;;
8p2P y: (qp)_(qF ynp)g (19.3)
(Denition 2)=fq\F Intjq2aSub (M y);q\F Int6=;g
Thus, we know that each atomic subset AofMIntis also
an atomic subset of My. In addition, an atomic subset Aof
Myintersected withFIntis also an atomic subset of MInt.
A2aSub(M Int))A2aSub(M y)
A2aSub(M y))(A\FInt)2aSub(M Int)
Using Theorem 19 and the Premise 2, we investigate the
relation of atomic sets in composed feature models. In detail,
a set Awith AF x=Int is an atomic subset of Mx=Intif
and only if Ais an atomic subset of Mx=y.Theorem 20. aSub (M x=Int) =
fq\F x=Intjq2aSub (M x=y);q\F x=Int6=;g
Proof. Analogous to Theorem 12.
5.3 Discussion
To exemplify the obtained properties of compositionality
with feature-model interfaces, we reconsider the identied
problems of Section 3 and use our running example with
the analysis of atomic sets as illustration. Of course, the
approach is also applicable for other analyses.
First, we considered the problem of feature-model scala-
bility for humans. Compared to the composed feature model
MDBMS =Index ,MDBMS =Int(withMIntM Index ) is slightly
smaller and, thus, it might be easier for humans to identify
all relevant features of the feature-model composition. This
benet increases even more if more than one feature model
is used for the feature-model composition (see Section 6).
Second, we took the scalability problem with feature-mod-
el analyses into account. Here, we focus on a scenario, where
we are interested in the analysis of atomic sets for the fea-
ture modelMDBMS =Int. If we use state-of-the-art imple-
mentations of this analysis, we perform the analysis for fea-
ture modelMDBMS =Index and nally lter the results to re-
ceive atomic sets that only contain features of interest (i.e.,
FDBMS=Int ). By contrast, applying our concept of feature-
model interfaces, we can use the feature model MDBMS =Int
for the analysis of atomic sets. Thus, we achieve the same
results as in the state-of-the-art approach but with perfor-
mance benets due to the reduced propositional formula of
feature modelMDBMS =Intcompared toMDBMS =Index .
Third, we identied the support of feature-model evolu-
tion as one of our main challenges. Usually, an evolutionary
change of feature model MIndex implies a complete re-com-
putation of atomic sets for MDBMS =Int. By contrast, if we
use feature-model interfaces, we only have to re-compute the
analysis if the feature-model interface MIntis no longer com-
patible with the evolved feature model MIndex (i.e.,MInt
6M Index ). To check this compatibility, we only need to
compute the interface of the evolved feature model MIndex
and to compare it to the previous interface version.
6. COMPOSITIONALITY IN THE WILD
Next, we explore feature-model interfaces in practice with
thousands of features as given in industrial cases [11]. We in-
vestigate the typical size of interfaces and their potential to
support humans and machines during evolution. Further-
more, we examine how often feature-model interfaces be-
come incompatible to their corresponding, evolved feature
models. In detail, we investigate the research questions:
RQ1: How small can feature-model interfaces get compared
to their corresponding feature models?
RQ2: How often does a feature-model interface become in-
compatible to an evolved feature model?
Additionally, using the analysis of atomic sets, as it is the
most computationally intensive analysis of our considera-
tions with exponential complexity [18], we give an outlook
on potential performance benets of compositional analysis.
Therefore, we investigate the following question:
RQ3: Is it possible to achieve performance benets using
compositional analysis for atomic sets compared to an
analysis of the complete feature model?
6736.1 Experimental Design and Subject
In our experiment, we investigate four monthly snapshots
of one real-world feature model from the automotive domain,
which includes features of hardware and software. We re-
ceived it from our industrial partner in an obfuscated way
(i.e., feature names are replaced by unique IDs). In snap-
shot V1, the feature model consists of 14,010 features with
666 constraints, whereas snapshot V4 has 18,616 features
with 1,369 constraints (the feature models are available in
theExample Wizard of FeatureIDE [20]).
The complete feature model of a snapshot originally con-
tained more than 40 smaller feature-model instances. For
our evaluation, we need the original feature-model instances
because we want to investigate the relations to their feature-
model interfaces. Fortunately, we are able to decompose the
complete model into one root model and depending feature-
model instances since we know the position of each instance
in the complete feature model (i.e., the ID of its root fea-
ture). Regarding cross-tree constraints of the complete fea-
ture model, we distinguish between intra-model constraints ,
which describe dependencies within an instantiated feature
model, and inter-model constraints , which describe depen-
dencies between dierent models. We insert intra-model
constraints in the corresponding feature model, whereas we
save inter-model constraints for later usage.
For the evaluation, we use each instantiated feature model
as input for our interface-generation algorithm and search
for a strategy to select relevant features. Due to the lack of
specic domain knowledge, we declared each feature that is
included in an inter-model constraint as relevant (most no-
tably, the root feature, due to its parent-child relationship
to the root model). We call the resulting interfaces mini-
mal because they only consist of features that are relevant
for the composition (i.e., they can dier from interfaces de-
signed by domain experts). Afterwards, we reconstruct a
reduced feature model of the specic snapshot by recompos-
ing the minimal feature-model interfaces and the root fea-
ture model. We perform this procedure for each snapshot
and use the results to answer RQ1 and RQ2.
In order to use feature-model interfaces for this evalua-
tion, we need a scalable generation algorithm. Although an
algorithm of our previous work is suitable for the elimina-
tion of features in propositional formulas [48], the algorithm
does not scale for the generation of feature-model interfaces.
Therefore, we designed a new algorithm that is based on
multiple satisability tests and logical resolution. However,
the algorithm itself is out of our scope and discussed else-
where in detail [26]. We refer interested readers to our open-
source implementation in FeatureIDE v3.0 [20, 47].
To exemplify the compositionality properties of feature-
model interfaces and to answer RQ3, we investigate the anal-
ysis of atomic sets and the results for the reduced feature
model of snapshot V1. Contrary to the proposed atomic-set
algorithm that only combines features with their manda-
tory children [42, 53], we use our Denition 2 of atomic sets,
which may also combine features of dierent sub trees. For
the computation of atomic sets for the reduced feature mo-
del, we consider two ways: (1) using the complete model
with a subsequent ltering on the features of interest, and
(2) using our recomposed feature model from the minimal
interfaces. Afterwards, we compare the individual compu-
tation times.6.2 Results and Discussion
We divide this section according to our research questions.
RQ1. In Figure 4, we depict the results of our investiga-
tion for research question RQ1 using boxplots (please ignore
the bars for now). Each boxplot illustrates one snapshot of
the automotive feature model and presents the percentage of
features given in the interface relative to the features in the
corresponding feature model. To further improve the illus-
tration, we removed feature models with only one feature.
For instance, in snapshot V1 there exists an instantiated
feature model with exactly 7,800 features whereas the cor-
responding interface only consists of the root feature. The
median of boxplot V1 and V2 is less than 2%, whereas the
boxplots of snapshot V3 and V4 have a median less than
4%. Thus, half of all existing feature-model interfaces only
consist of less than 4% of features relative to the correspond-
ing feature models. Furthermore, 94% of all feature-model
interfaces consist of less than 20% of the features.
In summary, the dierence between the number of features
in a feature-model interface and its corresponding feature
model is signicant. In most cases, the resulting feature-
model interface consists of less than 20% of the features.
RQ2. For research question RQ2, we investigate the benet
of feature-model interfaces for an evolutionary scenario. In
detail, we check how often adaptations of a feature-model
interface are necessary, due to evolution of its correspond-
ing feature model. In Figure 4, we depict the result of this
investigation using bar charts that present the percentage
of incompatible feature-model interfaces between all snap-
shots. For instance, the rst bar (i.e., V1 !V2) presents
whether the feature-model interfaces of snapshot V1 are
still compatible with the feature models of snapshot V2
(MIntV1M V2). The results can be divided into three
categories: (a) the feature-model interface is still compatible
(green), (b) the interface is incompatible (red), and (c) the
desired, minimal interface changed (yellow). In the last case,
the new feature model is incompatible to its interfaces be-
cause the interface has a changed feature set due to new
inter-model constraints. However, the dependencies within
the feature model did not change and all features that are
necessary to describe the inter-model constraints are avail-
able in the previous snapshot. Thus, they could be used to
create the same interface as given in the current snapshot.
Therefore, if a domain engineer creates ideal interfaces (i.e.,
using the knowledge of all future dependencies) instead of
the minimal ones, the result would be a compatible feature-
model interface for both snapshots.
In the bar V1!V2, we present all 19 cases in which the
feature model has been changed. Here, more than 84% of
all feature-model interfaces are equal in both snapshots, less
than 6% are not equal, and more than 10% are not equal
because of minimality. In the other bars, only 14 and 13
feature models have been changed. As result, bar V2 !V3
presents more than 42% not equal feature-model interfaces,
whereas the bar V3 !V4 again presents less than 8%. The
equality of the new feature-model interface was decreased
from 84% in bar V1 !V2 over 50% in V2 !V3 to 23% in
bar V3!V4. By contrast, the results from category (c)
increase from 7% to 69% and, thus, the success of interfaces
depends on the choice of removed features.
For research question RQ2, we investigated the interface
dependency of evolved feature models to the interfaces of the
674Features
 in V114,010
0%20%40%60%80%100%
V1
‚Üì
V219
Features
 in V217,742
V2
‚Üì
V314
Features
 in V318,434
V3
‚Üì
V413
Features
 in V418,616Figure 4: Percentage of features in the minimal fea-
ture-model interface compared to their feature mo-
dels (
 ) and percentage of compatible interfaces (
 ),
incompatible interface because of minimality (
 ),
and incompatible interfaces (
 ).
previous version. We get the result that, in more than half
of all cases in which the feature model has been changed,
the interface dependency holds. For all positive cases, it is
not necessary to change anything in the composed feature
model and, thus, we need no further computations.
RQ3. Using the analysis atomic sets, we evaluate the po-
tential of feature-model interfaces for compositional analy-
ses. As described in our experimental design, we use the
feature model of snapshot V1 as input and are interested
in the atomic sets of the corresponding reduced feature mo-
del. The analysis based on the complete feature model takes
more than 50 hours for the computation of all atomic sets,
while the subsequent ltering is negligible. By contrast, in
the composed analysis, we need in total less than 5 seconds
for the generation of feature-model interfaces, reconstruc-
tion of the feature model, and computation of all atomic
sets. Indeed, it is also possible to optimize the internals of
the atomic sets algorithm to only consider relevant features.
However, these optimizations are out of scope of this paper.
Hence, we considered the algorithm as black box and used
the described evaluation strategy. In summary, using the
analysis of atomic sets, we illustrated that it is possible to
reduce computational time using our concept of composi-
tional analyses.
6.3 Threats to Validity
External Validity. The results of our study strongly de-
pend on the analyzed feature model of the automotive do-
main, the distribution of the root features of the instanti-
ated models, and on the features of interest ( FInt) that we
declared based on inter-model constraints due to the lack of
domain knowledge. We plan to investigate whether the re-
sults can be generalized to other snapshots, feature models,
or domains. Nevertheless, we had no inuence on the se-
lection of snapshots and root features we received from our
industrial partner.
The snapshots of the automotive feature model seems to
be extracted from an early state of the development process
(i.e., snapshot V1 with 14,010 features and V4 with 18,616
features). It is possible that a more stable version leads to
dierent results regarding necessary interface changes. How-
ever, we plan to investigate more snapshots and other case
studies to get more insights.
Furthermore, we automatically generated minimal inter-
faces. In practice, developers will be able to incorporate do-
main knowledge to create interfaces. However, we receivedthe feature model in an obfuscated way, which does not allow
us to use domain knowledge for the interface generation.
Internal Validity. We used a prototypical implementation
of our optimized algorithm to compute feature-model in-
terfaces in a scalable manner. To reduce the probability
of errors, we used unit tests in which the results of this
algorithm were compared to the state-of-the-art algorithm
for abstract features [48]. Because of scalability problems, it
was not possible to compare the results of both algorithms
with huge feature models but we used smaller feature models
for the comparison with dierent sets of relevant features.
With refactorings, it is possible to further reduce our min-
imal feature-model interfaces. For instance, let us consider
the constraint A)B^Cas inter-model constraint, in which
the features A and B are features of the same instantiated
feature model and C is part of another feature model. Here,
we include both features in the corresponding feature-model
interface. However, it is possible to refactor this constraint
into two new constraints A)Bas intra-model constraint
andA)Cas inter-model constraint. As result, it is suf-
cient to only include A into the feature-model interface
instead of A and B. This approach could lead to even better
results but may harm readability.
7. RELATED WORK
Here, we present several works in the domains of interfaces
in product lines, feature-model composition, the analysis of
feature models, and further analyses based on interfaces.
Interfaces and Views for Product Lines .As stated
above, the denition of the slice operator presented by Acher
et al. is similar to our denition of feature-model inter-
faces [4]. Thus, the slice operator uses a feature model as
input to create a new one that only consists of a subset
of features with unchanged feature dependencies. However,
Acher et al. use the slice operator to focus on the property
of feature-model decomposition. In subsequent work, Acher
et al. use the slice operator in combination with a merge
operator to consider evolutionary changes of extracted vari-
ability models in a real-world plugin system [2]. For the
extraction process, the authors used feature-model aggrega-
tion and slicing and compare the dierent models of each
system's version. Whereas we aim to support evolution us-
ing a stable feature-model interface so that we prevent a
re-evaluation of the system, Acher et al. aim to detect dif-
ferences of the feature-model versions during evolution.
Furthermore, Dhungana et al. present an interface that is
mainly used for information hiding to other parts of the fea-
ture model (called fragments) and to support evolution [17].
In detail, the authors save a merge history of fragments at
one point in time, give feedback to the single fragments to
ease their maintenance and use the history to re-merge the
fragments in the future. However, the approach does not
consider automated analyses.
A further concept related to feature-model interfaces are
feature-model views [23, 31, 37]. Views also present a subset
of relevant features based on a master feature model and are
generally used to ease the conguration of large scale feature
models. Thus, dierent views regarding one master feature
model are combined to get a valid conguration based on the
view's partial congurations. By contrast, a feature-model
interface can be an interface of a set of dierent feature
models and is not bound to a specic one.
675Automated Analyses of Feature Models. There exists
a wide range of research for automated analyses. Benavides
et al. present a survey about existing analyses of feature mo-
dels with information regarding the analysis concept, tool
support, and references to work on particular analyses [9].
Based on this, Dur an et al. present a formal framework in
which these analyses can be described [18]. However, the
necessity for automated analyses of feature models was in-
troduced together with feature models themselves. Kang et
al. already recognize that tool support is essential to create
accurate feature models for complex domains [24]. There-
fore, they propose a tool based on Prolog using a fact base
and composition rules [24]. By contrast, todays tools are
typically based on satisability solvers or binary decision
diagrams [10, 32, 43, 47]. While the check for satisability
of a propositional formula is an NP-complete problem, Men-
donca et al. claim that satisability checks in the domain of
feature models scale well in most cases [33]. In contrast, the
computations of some other analyses, such as atomic sets,
do not scale for large feature models.
Feature-Model Composition .Composition mechanisms
are often used for multi software product lines that are a
set of multiple dependent product lines [22, 27, 34]. In this
context, large-scale variability modeling is essential to fulll
the system's requirements. Eichelberger and Schmid give an
overview of textual-modeling languages that can be used for
large-scale variability modeling [19]. The authors identify
several languages, such as, FAMILIAR [5], VELVET [35],
TVL [13], and VSL [1], which support variability-model
composition and compare the languages regarding their fa-
cility to support composition ,modularity , and evolution. In
general, it is also possible to integrate our approach into
other languages to facilitate compositional analyses. Fur-
thermore, as integrated in the language and tool FAMIL-
IAR, Acher et al. compare a set of composition operators,
such as a merge operator based on union and intersection of
feature sets [6]. In detail, the authors investigate dierent
implementation options and present advantages and draw-
backs. For our investigation of compositional analyses, we
relied on an own formal description inspired by the aggre-
gation mechanism of the modeling language VELVET [35].
However, the formal denition of our composition mecha-
nism can be regarded as a combination of the aggregation
and the merge operator introduced in FAMILIAR [5].
Product-Line Analyses Based on Interfaces .It is an
open question how much the dierent kinds of product-line
analyses, such as family-based analysis of product lines [46],
can benet from using feature-model interfaces. However,
in our previous work, we presented an overall concept of
interacting interfaces [39], in which the feature-model in-
terfaces represent the central part for the subsequent inter-
faces. In detail, we also introduced syntactical and behav-
ioral product-line interfaces. Syntactical interfaces support
users during the implementation of the product line and
present a view on reusable programming artifacts [40]. By
contrast, behavioral interfaces are used to ease the product-
line verication [49]. Besides our own investigations, K ast-
ner et al. introduced a variability-aware module system that
allows for type checking modules in isolation. For this pur-
pose, the authors dene interfaces between these modules,
in which also the variability is encoded [25]. However, these
interfaces do not hide variability as it is focused by feature-model interfaces. Furthermore, Li et al. also present inter-
faces for feature-oriented systems to verify the product-line
behavior [29]. For the formal representation of these sys-
tems, the authors use state machines as input for the veri-
cation of properties that they describe in temporal logic. To
ease the verication and to facilitate a feature-based verica-
tion, interfaces are used to encapsulate the connection states
to other feature's state machines. This is similar to our con-
sideration of minimal interfaces in which we only considered
features that are involved in an inter-model constraint.
8. CONCLUSION
Highly-congurable systems can have thousands of op-
tions and each option may have dependencies to other op-
tions. These dependencies are typically specied by means
of feature models. However, experiences with applying our
tool FeatureIDE in industry is that large feature models are
overwhelming and hard-to-understand for all stakeholders.
Furthermore, large feature models often slow down analyses
for congurable programs as they typically involve satis-
ability problems with a variable for each option. This sit-
uation gets even worse during evolution; stakeholders and
analyses need to consider the complete model when reason-
ing about the impact of a change even though it may have
only local eects. While there are some recent attempts to
decompose feature models, they still require the composition
into a large model prior to analysis.
We propose feature-model interfaces for composition of
feature models to establish information hiding. A feature-
model interface is a feature model that represents a subset
of features that are relevant to certain stakeholders, and is
used for composition with other feature models. On the one
hand, we proved compositionality properties for several anal-
yses, i.e., we state under which circumstances a change does
not aect other parts. On the other hand, we empirically
investigated a real-world feature model from the automotive
domain with 18,616 features and its three-month-history, for
which a decomposition was already available. In the major-
ity of all cases, feature-model interfaces may contain less
than 4% of the features. Furthermore, changes to the hid-
den feature models do not require changes to the according
interfaces in more than half of the cases. For a particular
analysis, we measured a decrease in computation time.
While our results make us condent that the composi-
tionality properties of feature-model interfaces have a pos-
itive eect on numerous existing analyses for congurable
programs, we are still at the beginning of a long highway
requiring further eorts in both, theory and empirical eval-
uation. We are currently working on further analyses and
an empirical evaluation with the Linux kernel.
ACKNOWLEDGMENTS This work is partially funded
by BMBF grant (01IS14017B), DFG grant (SCHA1635/4-
1) and by the European Commission (ERC H2020-644298).
For the snapshots of the real-world feature model, we thank
our industrial partner. We also thank the participants of
the yearly FOSD meeting, in particular Christian K astner,
Sven Apel, Christoph Seidl and the reviewers of this paper
for their constructive feedback. An early draft of this paper
is also available as technical report [38].
6769. REFERENCES
[1] A. Abele, Y. Papadopoulos, D. Servat, M. T orngren,
and M. Weber. The CVM Framework - A Prototype
Tool for Compositional Variability Management. In
Proc. Int'l Workshop Variability Modelling of
Software-intensive Systems (VaMoS) , pages 101{105.
Universit at Duisburg-Essen, 2010.
[2] M. Acher, A. Cleve, P. Collet, P. Merle, L. Duchien,
and P. Lahire. Extraction and Evolution of
Architectural Variability Models in Plugin-Based
Systems. Software and System Modeling (SoSyM) ,
13(4):1367{1394, 2014.
[3] M. Acher, P. Collet, P. Lahire, and R. B. France.
Comparing Approaches to Implement Feature Model
Composition. In Proc. Europ. Conf. Modelling
Foundations and Applications (ECMFA) , pages 3{19.
Springer, 2010.
[4] M. Acher, P. Collet, P. Lahire, and R. B. France.
Slicing Feature Models. In Proc. Int'l Conf.
Automated Software Engineering (ASE) , pages
424{427. IEEE, 2011.
[5] M. Acher, P. Collet, P. Lahire, and R. B. France.
FAMILIAR: A Domain-Specic Language for Large
Scale Management of Feature Models. Science of
Computer Programming (SCP), 78(6):657 { 681, 2013.
[6] M. Acher, B. Combemale, P. Collet, O. Barais,
P. Lahire, and R. B. France. Composing Your
Compositions of Variability Models. In Proc. Int'l
Conf. Model Driven Engineering Languages and
Systems (MODELS) , pages 352{369. Springer, 2013.
[7] S. Apel, D. Batory, C. K astner, and G. Saake.
Feature-Oriented Software Product Lines: Concepts
and Implementation . Springer, 2013.
[8] D. Batory. Feature Models, Grammars, and
Propositional Formulas. In Proc. Int'l Software
Product Line Conf. (SPLC) , pages 7{20. Springer,
2005.
[9] D. Benavides, S. Segura, and A. Ruiz-Cort es.
Automated Analysis of Feature Models 20 Years
Later: A Literature Review. Information Systems ,
35(6):615{708, 2010.
[10] D. Benavides, S. Segura, P. Trinidad, and
A. Ruiz-Cort es. FAMA: Tooling a Framework for the
Automated Analysis of Feature Models. In Proc. Int'l
Workshop Variability Modelling of Software-intensive
Systems (VaMoS) , pages 129{134. Technical Report
2007-01, Lero, 2007.
[11] T. Berger, R. Rublack, D. Nair, J. M. Atlee,
M. Becker, K. Czarnecki, and A. W,asowski. A Survey
of Variability Modeling in Industrial Practice. In Proc.
Int'l Workshop Variability Modelling of
Software-intensive Systems (VaMoS) , pages 7:1{7:8.
ACM, 2013.
[12] M. Bo skovi c, G. Mussbacher, E. Bagheri, D. Amyot,
D. Ga sevi c, and M. Hatala. Aspect-Oriented Feature
Models. In Proc. Int'l Conf. Models in Software
Engineering (MODELSWARD) , pages 110{124.
Springer, 2011.
[13] A. Classen, Q. Boucher, and P. Heymans. A
Text-based Approach to Feature Modelling: Syntax
and Semantics of TVL. Science of Computer
Programming (SCP) , 76(12):1130{1143, 2011.[14] A. Classen, P. Heymans, P.-Y. Schobbens, A. Legay,
and J.-F. Raskin. Model Checking Lots of Systems:
Ecient Verication of Temporal Properties in
Software Product Lines. In Proc. Int'l Conf. Software
Engineering (ICSE) , pages 335{344. ACM, 2010.
[15] E. F. Codd. A Relational Model of Data for Large
Shared Data Banks. Communications of the ACM ,
13(6):377{387, 1970.
[16] K. Czarnecki and K. Pietroszek. Verifying
Feature-Based Model Templates Against
Well-Formedness OCL Constraints. In Proc. Int'l
Conf. Generative Programming and Component
Engineering (GPCE) , pages 211{220. ACM, 2006.
[17] D. Dhungana, P. Gr unbacher, R. Rabiser, and
T. Neumayer. Structuring the Modeling Space and
Supporting Evolution in Software Product Line
Engineering. Journal of Systems and Software (JSS) ,
83(7):1108{1122, 2010.
[18] A. Dur an, D. Benavides, S. Segura, P. Trinidad, and
A. Ruiz-Cort es. FLAME: A Formal Framework for the
Automated Analysis of Software Product Lines
Validated by Automated Specication Testing.
Software and System Modeling (SoSyM) , pages 1{34,
2015. In Press.
[19] H. Eichelberger and K. Schmid. A Systematic
Analysis of Textual Variability Modeling Languages.
InProc. Int'l Software Product Line Conf. (SPLC) ,
pages 12{21. ACM, 2013.
[20] FeatureIDE Development Team. FeatureIDE GitHub,
2015. https://github.com/FeatureIDE/FeatureIDE.
[21] A. Hemakumar. Finding Contradictions in Feature
Models. In Proc. Int'l Workshop on Analyses of
Software Product Lines (ASPL) , pages 183{190. Lero
Int. Science Centre, University of Limerick, Ireland,
2008.
[22] G. Holl, P. Gr unbacher, and R. Rabiser. A Systematic
Review and an Expert Survey on Capabilities
Supporting Multi Product Lines. J. Information and
Software Technology (IST) , 54(8):828{852, 2012.
[23] A. Hubaux, P. Heymans, P.-Y. Schobbens, and
D. Deridder. Towards Multi-View Feature-Based
Conguration. In Proc. Int'l Working Conf.
Requirements Engineering: Foundation for Software
Quality (REFSQ) , pages 106{112. Springer, 2010.
[24] K. C. Kang, S. G. Cohen, J. A. Hess, W. E. Novak,
and A. S. Peterson. Feature-Oriented Domain
Analysis (FODA) Feasibility Study. Technical Report
CMU/SEI-90-TR-21, Software Engineering Institute,
1990.
[25] C. K astner, K. Ostermann, and S. Erdweg. A
Variability-Aware Module System. In Proc. Conf.
Object-Oriented Programming, Systems, Languages
and Applications (OOPSLA) , pages 773{792. ACM,
2012.
[26] S. Krieter, R. Schr oter, T. Th um, and G. Saake. An
Ecient Algorithm for Feature-Model Slicing.
Technical Report FIN-001-2016, University of
Magdeburg, Germany, 2016.
[27] C. W. Krueger. New Methods in Software Product
Line Development. In Proc. Int'l Software Product
Line Conf. (SPLC) , pages 95{102. IEEE, 2006.
[28] K. Lauenroth, K. Pohl, and S. Toehning. Model
677Checking of Domain Artifacts in Product Line
Engineering. In Proc. Int'l Conf. Automated Software
Engineering (ASE) , pages 269{280. IEEE, 2009.
[29] H. Li, S. Krishnamurthi, and K. Fisler. Interfaces for
Modular Feature Verication. In Proc. Int'l Conf.
Automated Software Engineering (ASE) , pages
195{204. IEEE, 2002.
[30] R. Lotufo, S. She, T. Berger, K. Czarnecki, and
A. W,asowski. Evolution of the Linux Kernel
Variability Model. In Proc. Int'l Software Product
Line Conf. (SPLC) , pages 136{150. Springer, 2010.
[31] M. Mannion, J. Savolainen, and T. Asikainen.
Viewpoint-Oriented Variability Modeling. In Proc.
Computer Software and Applications Conf.
(COMPSAC) , pages 67{72. IEEE, 2009.
[32] M. Mendon ca, M. Branco, and D. Cowan. S.P.L.O.T.:
Software Product Lines Online Tools. In Proc. Conf.
Object-Oriented Programming, Systems, Languages
and Applications (OOPSLA) , pages 761{762. ACM,
2009.
[33] M. Mendon ca, A. W,asowski, and K. Czarnecki.
SAT-Based Analysis of Feature Models is Easy. In
Proc. Int'l Software Product Line Conf. (SPLC) ,
pages 231{240. Software Engineering Institute, 2009.
[34] M. Rosenm uller and N. Siegmund. Automating the
Conguration of Multi Software Product Lines. In
Proc. Int'l Workshop Variability Modelling of
Software-intensive Systems (VaMoS) , pages 123{130.
Universit at Duisburg-Essen, 2010.
[35] M. Rosenm uller, N. Siegmund, T. Th um, and
G. Saake. Multi-Dimensional Variability Modeling. In
Proc. Int'l Workshop Variability Modelling of
Software-intensive Systems (VaMoS) , pages 11{22.
ACM, 2011.
[36] M. Rosenm uller, N. Siegmund, S. S. ur Rahman, and
C. K astner. Modeling Dependent Software Product
Lines. In Proc. Workshop on Modularization,
Composition and Generative Techniques for Product
Line Engineering (McGPLE) , pages 13{18.
Department of Informatics and Mathematics,
University of Passau, 2008.
[37] J. Schroeter, M. Lochau, and T. Winkelmann.
Multi-Perspectives on Feature Models. In Proc. Int'l
Conf. Model Driven Engineering Languages and
Systems (MODELS) , pages 252{268. Springer, 2012.
[38] R. Schr oter, S. Krieter, T. Th um, F. Benduhn, and
G. Saake. Feature-Model Interfaces for Compositional
Analyses. Technical Report FIN-001-2015, University
of Magdeburg, Germany, 2015.
[39] R. Schr oter, N. Siegmund, and T. Th um. Towards
Modular Analysis of Multi Product Lines. In Proc.
Int'l Software Product Line Conference co-located
Workshops , pages 96{99. ACM, 2013.
[40] R. Schr oter, N. Siegmund, T. Th um, and G. Saake.
Feature-Context Interfaces: Tailored Programming
Interfaces for Software Product Lines. In Proc. Int'l
Software Product Line Conf. (SPLC) , pages 102{111.
ACM, 2014.
[41] R. Schr oter, T. Th um, N. Siegmund, and G. Saake.Automated Analysis of Dependent Feature Models. In
Proc. Int'l Workshop Variability Modelling of
Software-intensive Systems (VaMoS) , pages 9:1{9:5.
ACM, 2013.
[42] S. Segura. Automated Analysis of Feature Models
Using Atomic Sets. In Proc. Int'l Workshop on
Analyses of Software Product Lines (ASPL) , pages
201{207. Lero Int. Science Centre, University of
Limerick, Ireland, 2008.
[43] S. Segura, J. A. Galindo, D. Benavides, J. A. Parejo,
and A. Ruiz-Cort es. BeTTy: Benchmarking and
Testing on the Automated Analysis of Feature Models.
InProc. Int'l Workshop Variability Modelling of
Software-intensive Systems (VaMoS) , pages 63{71.
ACM, 2012.
[44] R. Tartler, D. Lohmann, C. Dietrich, C. Egger, and
J. Sincero. Conguration Coverage in the Analysis of
Large-Scale System Software. ACM SIGOPS
Operating Systems Review, 45(3):10{14, 2012.
[45] S. Thaker, D. Batory, D. Kitchin, and W. Cook. Safe
Composition of Product Lines. In Proc. Int'l Conf.
Generative Programming and Component Engineering
(GPCE) , pages 95{104. ACM, 2007.
[46] T. Th um, S. Apel, C. K astner, I. Schaefer, and
G. Saake. A Classication and Survey of Analysis
Strategies for Software Product Lines. ACM
Computing Surveys , 47(1):6:1{6:45, 2014.
[47] T. Th um, C. K astner, F. Benduhn, J. Meinicke,
G. Saake, and T. Leich. FeatureIDE: An Extensible
Framework for Feature-Oriented Software
Development. Science of Computer Programming
(SCP) , 79(0):70{85, 2014.
[48] T. Th um, C. K astner, S. Erdweg, and N. Siegmund.
Abstract Features in Feature Modeling. In Proc. Int'l
Software Product Line Conf. (SPLC) , pages 191{200.
IEEE, 2011.
[49] T. Th um, T. Winkelmann, R. Schr oter, M. Hentschel,
and S. Kr uger. Variability Hiding in Contracts for
Dependent Software Product Lines. In Proc. Int'l
Workshop Variability Modelling of Software-intensive
Systems (VaMoS) , pages 97{104. ACM, 2016.
[50] P. Trinidad, D. Benavides, A. Dur an, A. Ruiz-Cort es,
and M. Toro. Automated Error Analysis for the
Agilization of Feature Modeling. Journal of Systems
and Software (JSS) , 81(6):883{896, 2008.
[51] P. Trinidad, D. Benavides, and A. Ruiz-Cort es.
Improving decision making in software product lines
product plan management. In Proc. Workshop on
Decision Support in Software Engineering (ADIS) ,
pages 1{8. RWTH Aachen, 2004.
[52] P. Trinidad and A. Ruiz-Cort es. Abductive Reasoning
and Automated Analysis of Feature Models: How are
They Connected? In Proc. Int'l Workshop Variability
Modelling of Software-intensive Systems (VaMoS) ,
pages 145{153. Universit at Duisburg-Essen, 2009.
[53] W. Zhang, H. Zhao, and H. Mei. A Propositional
Logic-Based Method for Verication of Feature
Models. In Proc. Int'l Conf. Formal Methods and
Software Engineering (ICFEM) , pages 115{130.
Springer, 2004.
678