Mining Performance Speciﬁcations
Marc Brünink
National University of Singapore
NUS Graduate School for Integrative Sciences
and Engineering, School of Computing
Singapore, Singapore
marc@comp.nus.edu.sgDavid S. Rosenblum
National University of Singapore
School of Computing
Singapore, Singapore
david@comp.nus.edu.sg
ABSTRACT
Functional testing is widespread and supported by a mul-
titude of tools, including tools to mine functional speciﬁ-
cations. In contrast, non-functional attributes like perfor-
mance are often less well understood and tested. While
many proﬁling tools are available to gather raw performance
data, interpreting this raw data requires expert knowledge
and a thorough understanding of the underlying software
and hardware infrastructure.
In this work we present an approach that mines perfor-
mance speciﬁcations from running systems autonomously.
The tool creates performance models during runtime. The
mined models are analyzed further to create compact and
comprehensive performance assertions. The resulting asser-
tions can be used as an evidence-based performance speciﬁ-
cation for performance regression testing, performance mon-
itoring, orasafoundationformoreformalperformancespec-
iﬁcations.
CCS Concepts
•General and reference →Performance; •Software
and its engineering →Software performance; Soft-
ware evolution; Requirements analysis;
Keywords
Speciﬁcation mining, performance modeling, regression test-
ing
1. INTRODUCTION
Previous work on performance focuses often on high per-
formancecomputing[2, 6]. Typicallyhigh-performancecom-
puting incurs high execution costs. This justiﬁes signiﬁcant
investments in performance optimizations and detailed per-
formance modeling [12]. Other scenarios require up front
performance speciﬁcations, too. For instance, if latency is a
key consideration. In contrast, in many other applications
performance is less mission critical. In these scenarios it ishard to justify the heavy investment required to explicitly
specify and verify a performance model.
We target applications in which performance is not a pri-
mary concern but an important secondary objective. Many
consumer-grade applications fall into this area (e.g. word
processors, web browsers). We mine performance models
during runtime automatically and autonomously. Our goal
is to mine performance speciﬁcations of deployed applica-
tions in the ﬁeld. The mined models specify the expected
performance behavior and can be used to monitor deployed
systems and detect performance deviations. Insights gained
by inspecting the gathered models might also be used to ver-
ify previously speciﬁed performance requirements and aug-
ment traditional approaches towards elicitation of perfor-
mance speciﬁcations. In this paper we make the following
contributions:
•We present a tool that extracts performance speciﬁ-
cations during runtime. These speciﬁcations come in
the form of performance models. The models are an
evidence-basedperformancespeciﬁcationthatdescribe
the expected behavior of a system. We automatically
detect interesting key features in the models and an-
alyze their root causes. The root causes are captured
in performance assertions. Assertions are small and
easy to understand. We use dynamic binary instru-
mentation to enable adaptation of data collection dur-
ing runtime. The adaptation operates autonomously
and collects data incrementally. It does not require
human guidance.
•Wepresentadataanalysisapproachthatextractscom-
prehensive performance models. We show that mined
models are reliable and accurate. The models cap-
tureperformanceinrelativeterms, notabsolutevalues.
This increases platform independence and is crucial if
we want to mine performance speciﬁcations collabora-
tively across platforms.
•In our evaluation, we show that the tool mines per-
formance models autonomously. Mined models are re-
producible and consistent between multiple executions
and across platform boundaries.
The paper is structured as follows. In Section 2 we give a
brief, high-level overview of the system. Speciﬁc aspects are
discussed in detail in Section 3. We evaluate our approach
in Section 4. We review related work in Section 5. Finally,
we examine possible future work and conclude the paper in
Section 6.Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a
fee. Request permissions from Permissions@acm.org.
FSE’16 , November 13–18, 2016, Seattle, WA, USA
c/circlecopyrt2016 ACM. 978-1-4503-4218-6/16/11...$15.00
http://dx.doi.org/10.1145/2950290.2950314
39proc_mutex_sysv_acquire
20.5%
 55.0%
 24.5%
20.5%
 5.5%
55.0%
 20.5%
 24.5%
55.0%
 19.0%
 5.5%
 20.5%
e1
proc_mutex_sysv_releasestartup_childrenap_mpm_runmain
__semope2behavior #1
behavior #2
behavior #3
selected edges
Figure 1: A mined performance model for the
__semop function. Di ﬀerent colored edges denote dif-
ferent performance behavior.
2. OVERVIEW
Thorough performance testing helps to increase the qual-
ity of software and reduce the number of in-ﬁeld defects.
However, we observe that performance testing sometimes
fail to get the required attention. We believe this is due to
two factors: cost and time. In this work we introduce an
automatic performance speciﬁcation mining approach that
can operate on deployed software and helps to reduce costs
by extracting performance models from deployed systems
autonomously without human guidance.
Figure 1 shows a performance model mined by our ap-
proach from an execution of Apache 2.0.64. Depending on
the calling context the __semop functions exhibits 3 di ﬀerent
performance behaviors. If the call path to __semop includes
the edge e1, then we expect the __semop function to follow
performance behavior #1. If the call path includes edge e2,
we expect behavior #2. Finally, if none of the 2 edges is
included, we expect behavior #3. The key insight is that
behavior #3 is very di ﬀerent from #1 and #2. Behavior #3
postulates that the execution of __semop will be faster than
70ms in 99% of the cases, compared to 41 .9% for behavior
#1 and 35 .5% for #2. It is easy to understand why there is
a performance di ﬀerence: behavior #3 reﬂects the path that
releases a mutex, whereas behavior #1 and #2 are on paths
that acquire one. The model was collected autonomously
and illustrates the capabilities of our approach. We discuss
this model in greater detail in our evaluation in Section 4.3.
Mining such a performance speciﬁcation is not easy. Be-
cause software is complex, it is possible to collect many dif-
ferent types of data at many di ﬀerent locations. Collecting
all the data at all locations in parallel would result in a
high performance overhead, probably rendering the system
unusable. To tackle this challenge, our tool explores the
monitored system incrementally. It starts with a minimal
amount of data collection and uses the collected data to
steer the mining process to promising targets, progressively
accumulating an expressive body of knowledge.
Figure 2 provides a high level overview of our system. A
softwaresystemisdeployedacrossmultiplesites. Tosupport
ongoing software engineering tasks (e.g. software evolution),
Storage
Server
Mining
ControllerAnalyserAnalyserAnalysis
ControlDataDeployed
SystemStubDeployed
SystemStub
Figure 2: Overview of the approach.
Start
Measure timeDetect hot function
Analyze timing behavior
has 
stable 
behavior
has unique 
classesYes
Measure time and collect call stack
Extract call path patterns
Store call path patternsYesno. of 
samples > 
thresholdNo
YesNo
Detect callerNo
Figure 3: Incremental data collection ﬂowchart.
we want to mine in-ﬁeld performance speciﬁcations. The de-
ployed and running systems are registered with a local stub
controller that connects to a remote mining controller. The
mining controller controls which information is collected in
each deployed system. Once registered, the deployed sys-
tems start to generate and aggregate data, which is sent to
a storage backend. One or more analysis processes retrieve
the collected data, correlate it, and mine performance mod-
els and assertions. During the analysis the tool also judges
about the stability and quality of the data and recommends
next steps to the mining controller. The mining controller
reviews the recommendations and selects a subset. Next,
it sends instrumentation requests to the stub controller of
the deployed systems leading to an adaptation of the infor-
mation collected at each deployed system. This design is
similar to the one presented in [22].
3. MINING AND ANALYSING DATA
Figure 3 presents a ﬂowchart, illustrating how the tool
decides which data to collect where. Assuming that the
user did not provide hints on where to collect which kind
of data via command line options, collection starts by de-
tecting hot code functions. These are functions that are ex-
ecuted frequently. The tool simply samples the instruction
40pointer at a 100 ms interval. In the absence of user-supplied
hints, measuringfrequentlyexecutedfunctionsisreasonable;
stakeholders are typically more interested in the execution
characteristics of frequently executed functions.
Once the system detects a hot function, it starts to mon-
itor the function by accumulating timing data. The tool
aggregates individual timings into histograms that are sam-
pled as observations. After data is collected, it is analyzed.
As a ﬁrst step the system detects whether it collected a su ﬃ-
cient amount of data. To this end we partition the data into
multiple clusters (Section 3.1) and compare the partitioning
across time (cf. Section 3.2). If the partitioning does not
change, we call it stable.
If the timing behavior is not stable, the system obtains
more data until a threshold is met. The default threshold is
set at 25,000 data points. It can be changed via a command
line option. However, in our experience 25,000 data points
are suﬃcient to detect stable key features if they exist.
If the system ﬁnds a stable partitioning, then this indi-
cates that the function shows distinguishable timing behav-
ior. As a next step the system continues to collect not only
the time but also the call stack. The goal of collecting the
call stack is to explain why the di ﬀerent clusters exist. The
tool uses the gathered call stacks to extract call path pat-
terns and stores them for later reference (cf. Section 3.3 and
3.4)).
Finally, afterthesystemcollectedtimingbehaviorandcall
path patterns where appropriate, there is nothing else to do
for this speciﬁc function at the moment.1The tool continues
by detecting call sites of the function. Detected callers are
fed back to the system and used as new starting points. In
this way the system propagates data collection up the call
tree and incrementally covers the monitored application.
3.1 Data Partitioning
To partition the data, we tried Gaussian mixture models
and peak detection, with limited success. Only a few of our
collected distributions ﬁt a Gaussian mixture model well.
Peak detection works well with exponential distributions,
but the resulting partitions tend to be fragile.
We settled on a di ﬀerent, third approach. The tool ex-
tracts the local minima of the gradient of the cumulative
distribution function. The gradient is low at locations with
a low density of observed values. The system partitions the
data into multiple disjoint clusters along those areas of low
density. Since each location partitions the data into two
clusters, we call these locations cluster borders .
Sometimes our tool ﬁnds multiple cluster borders (local
minima) in close proximity. These neighboring borders do
not represent multiple useful partitions. Instead, they are
artifacts caused by the limitations of the collected data. In
particular the data is discrete, has limited precision and ac-
curacy, and is ﬁnite.
To accommodate this, the tool aggregates detected bor-
ders that are close to each other. It does not match borders
directly. Small ﬂuctuations in the collected data might have
a big impact on the precise location of the detected borders.
Instead, the tool matches two borders if their mapped cu-
mulative distribution function (CDF) values are close. For
example, consider Figure 4. Let us assume the tool detected
two local minima, x1andx2. The distance between the
1The system validates the results periodically to make sure
that they did not change.
Execution timeCDF1
x1x2
A1B1
A2B20
0Figure 4: Two positions on the x-axis, x1andx2,
might be signiﬁcantly di ﬀerent. However, the di ﬀer-
ence vanishes is we map them to their corresponding
CDF values F(x1)andF(x2).
locations of these two minima might be large. However,
their CDF values are very close. Because the CDF values
do not di ﬀer much, the number of elements between the two
is small. These few elements are the only data points that
will change cluster membership depending on whether the
tool partitions at x1or atx2. Because the di ﬀerence in the
resulting labeling is small, both minima partition the data
virtually in the same way. Our tool considers both borders
as representatives of the same key feature if the di ﬀerence
between their CDF values is less than 2%.
Definition 1.LetFbe a cumulative distribution func-
tion. Two positions x1andx2on the x-axis match with
respect to FifF(x1) andF(x2) diﬀer by less than 2% in
absolute terms.
match(x1,x2,F)def=|F(x1)−F(x2)|<0.02
The threshold value of 2% can be set by the user. The
value depends on the noise in the data as well as the pre-
ferred level of detail. We established 2% as a good threshold
value empirically. Besides accommodating noise it prevents
the detection of very small clusters by aggregating them into
largerones. Thisreducesthecomplexityoftheresultingpar-
titioning and led to good results in our evaluation studies.
After borders are matched, the tool picks one to represent
the whole set. Most often the set only contains a single
element and choosing one is trivial. In all other cases it
chooses the leftmost one. Since all matched borders are
close neighbors, the di ﬀerence between them is small and
the method of selection is non-critical.
3.2 Utility Score
Data collection is separated into whatto collect and where
to collect it. The decision is based on a utility score that
quantiﬁes how useful it would be to collect a speciﬁc type
of data at a speciﬁc location. We use the utility score to
decide which type of data to collect next. We only collect
data for the functions with the highest utility score. The
utility score should have three properties:
1. If new data is di ﬀerent from previously collected data,
collecting it did add information and potentially in-
creased our understanding of the system. The utility
score should be high.
412. If collected data stays the same over an extended pe-
riod of time, we expect it to stay the same in the near
future as well. Collecting this data will not add much
value, and the tool could use the available resources
more productively. For example, it could collect a dif-
ferent type of data instead. Thus, if the data stays the
same for some time, then the utility of collecting this
type of data at this speciﬁc location is low; the utility
score should reﬂect this and should be low as well.
3. As the monitored system continues to be used, ob-
servedbehaviormightchangeduetoshiftingworkload,
evolving complexity of state, or a multitude of other
reasons. Thus, collected data gets stale after some
time; its value decreases over time. We have to re-
collect it regularly to conﬁrm that it is still accurate.
Consequently, the expected utility of re-collecting the
data increases over time. Thus, the utility score should
increase over time, too.
From these three properties it is apparent that we have to
be able to judge whether the data is the same over a period
of time. To this end, we separate the stream of observed
data into discrete observations.
Definition 2.An observation ois a tuple ( ts,te,d,⌧)
withtsbeing the time collection of this observation started,
tethe time collection stopped, and dthe raw data collected
in the time frame ( ts,te).⌧identiﬁes the type of the col-
lected data, for example whether it is timing data or call
stack data. Then start(o) returns the start time tsofo,a n d
end(o) returns the end time teofo.
Definition 3.LetOτbe the set of all observations col-
lected so far that are of type ⌧. We can deﬁne an order
relation using the start time of the observations.
o1<o2def=start(o1)<s t a r t(o2)
Since no two observations in Oτhave the same start time,
Oτis a totally ordered set.
Definition 4.LetOτbe a totally ordered set of obser-
vations. Then last(Oτ) emits the newest observation in Oτ.
o1=last(Oτ)⇐⇒o1∈Oτ,∀o2∈Oτ\{o1},
start(o1)>s t a r t(o2)
prev(o) returns the observation immediately preceding o.
o1=prev(o2)⇐⇒o1,o2∈Oτ,start(o1)<s t a r t(o2),
@o3∈Oτ,start(o1)<s t a r t(o3)<s t a r t(o2)
To determine whether to collect data of a certain type ⌧
at a speciﬁc location, the tool calculates the utility score.
It compares newly gathered observations with all the other
observations in Oτ. Our tool leverages the partitioning ap-
proach introduced in Section 3.1. It compares detected bor-
ders in new observations with borders extracted from previ-
ously collected observations. If the borders match, then all
observations support the same partitioning.
Definition 5.A set of borders B1matches another set of
bordersB2with respect to a cumulative distribution func-
tionFif we can match every border in B1with at least one
border in B2.
matchBorderSets (B1,B2,F)⇐⇒ ∀b1∈B1∃b2∈B2
match(b1,b2,F)1:function FindStableBorders (O,✓)
2:F←CDF(Merge(O))
3:B←∅
4:BCDF←∅
5:for allo∈Odo
6: for allb∈GetBorders (o)do
7: BCDF←BCDF∪{F(b)}
8:BorderClusters ←Meanshift (BCDF)
9:for allC∈BorderClusters do
10: if|C|≥✓∗|O⌧|then
11: B←B∪{centroid (C)}
12:returnB
Algorithm 1: Calculate a set of common borders for
the observations in O.
function GetFirstStable (Oτ,B,F)
lastmatch←undefined
o←last(Oτ)
whilematchBorderSets (GetBorders (o),B,F)do
lastmatch←o
o←prev(o)
returnlastmatch
Algorithm 2: Find the ﬁrst observations that
matches Bwith all subsequent observations also
matching B.
The tool does not directly compare new observations with
old ones. Instead, it extracts a set of borders that can be
found in the vast majority of previous observations. The
reason for this extra step is simple: while most borders can
be detected reliably in all observations, a few are more frag-
ile. Sometimes a rather small di ﬀerence in the collected data
causes our data classiﬁcation to detect a border in one ob-
servation but not in the other.
Algorithm 1 presents how our tool extracts stable bor-
ders. First, the algorithm merges all input observations O
and obtains the corresponding cumulative distribution func-
tion (CDF) (line 2). Next, it iterates over all observations
inO.GetBorders implements the data partitioning ap-
proach described in Section 3.1. It maps the detected bor-
ders to their respective CDF value and accumulates them
inBCDF(lines 5 to 7). Next, it applies the common mean
shift algorithm [8]. The mean shift algorithm returns a set
of sets, each containing borders that belong to the same
cluster. The algorithm ﬁlters clusters that have less than a
threshold of ✓∗|Oτ|elements2and aggregate the centroids
of the remaining clusters in B(lines 9 to 11). Finally, it
returnsB.
The tool compares new incoming observations to this set
of stable borders B. Algorithm 2 iterates over all observa-
tionsstartingwiththenewest(i.e. last)observation, last(Oτ).
It stops as soon as it ﬁnds a non-matching observation. It
returns the last observation that matched; thus, it returns
the earliest observation in a chain of matching observations.
Using Algorithm 1 and Algorithm 2, the tool calculates
the utility score in Algorithm 3. First, it determines sta-
ble borders for the last 10 observations, using Algorithm 1
(line 2 and 3). Next, it ﬁnds the ﬁrst observation ofirst
2In our evaluation we use a threshold value ✓of 0.9.
421:function UtilityScore (Oτ)
2:Olast10←Last10Observations (Oτ)
3:B←FindStableBorders (Olast10,0.9).Algo. 1
4:F←CDF(Olast10)
5:ofirst←GetFirstStable (Oτ,B,F).Algo. 2
6:tstart←start(ofirst)
7:tend←end(last(Oτ)))
8:u←(current time −tend)/(tend−tstart)
9:ifu>1then return 1
10:returnu
Algorithm 3: Calculate the utility score.
that ﬁts the extracted stable borders under the condition
that all newer observations ot>ofirstalso ﬁt, using Algo-
rithm 2 in line 5. Finally, with tstart=start(ofirst) and
tend=end(last(Oτ)), the utility score is calculated as
u=min✓
1,current time −tend
tend−tstart◆
Note that the utility score fulﬁlls our requirements and is
(1) low if observations are stable for a longer time, (2) high
if new observations are signiﬁcantly di ﬀerent from previous
ones, and (3) increases over time so that data is periodically
re-veriﬁed.
3.3 Capturing Performance Models
If the tool detects multiple performance classes for a spe-
ciﬁcfunctionusingourpartitioningapproachfromSection3.1,
then it attempts to explain whythere are di ﬀerent classes
(compare the ﬂowchart in Figure 3 introduced in Section 3).
To locate the cause for the existing clusters, the tool has
to collect additional data. At the moment it collects the call
paths that lead to a speciﬁc function. The assumption is
that the calling context is an important factor that might
contain crucial information to help discriminate between dif-
ferent performance behaviors.
Alternatively, one might also collect di ﬀerent data, for
example function arguments. Arguments are likely to in-
ﬂuence the executing time of the called function. However,
arguments of functions su ﬀer from the deﬁcit that they re-
quire interpretation. Except for trivial arguments, interpre-
tationrequiresin-depthunderstandingoftheirstructureand
contained data. This calls for either manual annotation or
elaborate static or dynamic analysis. In contrast, call paths
are easy to capture and easy to interpret. Furthermore, in
our experience they are su ﬃcient to gain signiﬁcant insights
into system behavior.
If our approach ﬁnds interesting features in the timing
behaviorofaspeciﬁcfunction(i.e., itﬁndsatleastonestable
cluster border), then it automatically augments the data
collection to extract not only the execution time but also
call path information.
Using the call path information the tool examines whether
the diﬀerent classes can be explained by di ﬀerences in the
call paths traversed to reach the function. Is there a spe-
ciﬁc set of call paths that show similar performance behavior
compared to all other call paths? If yes, we found a perfor-
mance pattern that occurs only if the function is reached via
a speciﬁc set of call paths.
Findingthesesetsofcallpathsisatwostepprocess. First,
our system analyzes the aggregated data of a speciﬁc func-tionfand detects stable cluster borders. We described this
step previously in Section 3.2. Algorithm 1 returns a set of
stable borders. Second, it uses the detected cluster borders
to classify observed execution data and group call paths that
behave similar. The borders partition the data into multiple
stable clusters, C.
Definition 6.LetTpbe the set of execution times ob-
served while reaching the function via call path p.L e tCbe
a set of stable clusters. We assign each t∈Tpto a cluster
that ﬁts best, using fit(t). Next, we count for each cluster
c∈Chow many of the observed execution times t∈Tpare
a member of this cluster.
count(c)def=|{t|t∈Tp,fit(t)=c}|
We normalize the result and obtain the normalized cluster
distribution vector bpfor call path p.
bp=(count(c1),count(c2),...,count (cn))
|Tp|
Thei-th element in bprepresents the percentage the exe-
cutions times for path pfall into cluster ci∈C.
To decide whether two call paths p1andp2have similar
behavior, the tool groups call paths using their normalized
cluster distribution vector bp. It clusters these points using
KMeans clustering. On important aspect of using KMeans
is to choose k, the number of clusters. We start with k=1
and increase kas long as the adjusted mutual information
score [26] increases. KMeans groups the call paths into dis-
joint sets G1,G2,...,G n. Call paths that end up in the same
group have similar bp. During runtime they are expected to
behave similarly.
Definition 7.A performance model Mis a tuple
(f,G,B, ) with f being a function in the binary of an appli-
cation. Let Gbeagroupingofcallpaths G={G1,G2,...,G n},
Ba set of normalized cluster distribution vectors B={bG1,
bG2,...,bGn}, and a bijective mapping  :G→B.
Thus, aperformancemodelconsistsofasetofgroupedcall
paths. The behavior of each call path group is described by
one cluster distribution vector.
To denote the performance model for function f, we write
Mffor short. Whenever we need to distinguish models col-
lected in di ﬀerent executions we write Mn
fto identify the
modelMfcollected in the n-th execution.
We compare multiple performance models in our evalua-
tion to reason about reliability and platform independence
of mined performance models. It is not di ﬃcult to compare
two performance models. A di ﬀerence occurs when two call
pathp1andp2are members of the same group in one model
while they belong to two di ﬀerent groups in the other model.
If we detect such a di ﬀerence between the models, than one
model postulates that both call paths should behave the
same while the other model postulates that they should be-
have diﬀerently.
It is noteworthy that a performance model itself does not
containanydirectnotionofexecutiontime. Executiontimes
areexpectedtobeverydi ﬀerentacrossplatforms. Instead, it
groups call paths by behavior. Call paths in the same group
are expected to behave similarly independent of concrete
values of execution times. Abstracting the execution times
and only retaining their relationships is crucial to make a
performance model portable across platforms.
433.4 Extracting Performance Assertions
Performance models tend to be large and contain a lot of
information. Except for very small models the complexity
of the models directly impacts readability. Complex models
are diﬃcult to understand.
To simplify a performance model Mf, our tool generalizes
it. It searches for a minimal set of call edges that is su ﬃcient
to distinguish all call path groups from each other.
Our approach uses a common A* algorithm to ﬁnd these
edges. As input we supply the call path groups ( G1,G2,...,
Gn). For each input path group Githe algorithm returns a
boolean expression, Ei. Thus, as output we obtain a vector
of boolean expressions ( E1,E2,...,E n). The atoms of the
boolean expression represent call edges. The tool uses the
total number of call edges as a cost function. Thus, the re-
turned expressions are guaranteed to use a minimal number
of call edges to distinguish between the call path groups.
Abooleanexpressionscanbeinstantiatedwithacallpath.
Toinstantiateanexpressionwithacallpath p, wesubstitute
an atom with Trueif and only if the corresponding call edge
is present in p. We write Ei(p) to denote an expression Ei
that was instantiated with call path p.
The boolean expressions ﬁlter call paths. If and only if a
previously observed call path is a member of the path group
Gi, then the boolean expression evaluates to true. Thus, for
all observed call paths PwithP=Sn
i=0Githe resulting
expressions satisfy the two constraints:
Ei(p),∀p∈Gi¬Ei(p),∀p∈P\Gi
The expressions are a building block for compact perfor-
mance assertions.
Definition 8.A performance assertion is a tuple
(f,{(E1,bG1),(E2,bG2),...,(En,bGn)}) with f being a func-
tion in the binary of an application. Let Eibe a boolean
expression that identiﬁes all paths in cluster Gi, andbGithe
aggregated normalized cluster distribution vector of Gi.
For example, assume the tool observed that function zis
reached via three di ﬀerent call paths ( p1,p2, andp3).
G1 G2
p1=(a→b→c→z)p2=(a→b→z)
p3=(a→c→z)
a,b,c,zare functions in the monitored application. a→b
means that function acalls function b. Furthermore, assume
function zshows diﬀerent performance behavior during run-
time depending on the context of the call. For example, z
always executes very fast if called via p1. However, if zis
called via p2orp3it takes a bit longer to complete in 20%
of the cases.
Ourtoolwilldetectthedi ﬀerenceinperformancebehavior
and group the three call paths into two groups ( G1andG2),
using the mechanisms described in Section 3.3. Next, the
system will use the A* algorithm to extract the minimal set
of edges that are su ﬃcient to distinguish all call paths in G1
from all call paths in G2. The minimal solution is E1=e
andE2=¬ewitherepresenting the edge b→c. The path
groupsG1andG2can be distinguished by the single edge e.
All call paths in G1contain the edge b→cwhile all paths
inG2do not contain it.
Using these two expressions the tool will create the fol-
lowing performance assertion a1:
a1=(z,{((b→c),(1.0,0.0)),(¬(b→c),(0.8,0.2))})This performance assertion a1captures the essence of the
collected data. If function zis called via a call path that
contains edge b→c, then we expect all calls to behave the
same and execute fast. However, if edge b→cis not present
in the call path, only 80% of the calls should execute fast
while the remaining 20% are expected to behave di ﬀerently;
they should execute slower.
4. EV ALUATION
Our evaluation is designed to answer the following re-
search questions:
RQ1:Can we mine performance models reliably?
RQ2:Aretheseperformancemodelsplatformindependent?
RQ3:How diﬃcult is it to understand the resulting perfor-
mance models and assertions?
RQ4:Howhighistheoverheadincurredbydatacollection?
RQ5:Does data collection perturb the system behavior?
Weevaluateourapproachonthreesubjects, Apache2.0.64,
MySQL 5.0.88, and Okular 0.19.3. These subjects cover dif-
ferent aspects of the design space. MySQL 5.0.88 is mul-
tithreaded; Apache httpd 2.0.64 uses multi-processing; and
Okular 0.19.3 is a GUI application.
To answer RQ1-RQ3, we run Apache and expose it to
the banking workload of the SPEC-web2009 benchmark ver-
sion 1.00.3We run a single SPEC-web2009 client on the
same machine as Apache to simplify the setup. We reduced
the mean wait time between requests in SPEC-web2009 to
speed-up execution of the benchmark. We run our exper-
iments on two di ﬀerent machines. Machine A is equipped
with a Intel Core i7-2600 processor with 3.4 GHz and 8 GB
of RAM. Machine B is equipped with two Intel Xeon E5-
2680 processor with 2.7 GHz and 32 GB of RAM.
4.1 RQ1: Reliability
To show that our approach mines performance patterns
reliably, we run the experiment ﬁve times each on two dif-
ferent machines. Then we compare the models mined on the
same machine with each other. In this way we show that we
can mine coherent models in ﬁxed environments.
Each execution of the experiments creates a set of per-
formance models Mi={Mi
f1,Mi
f2,...,Mi
fn}for functions
f1,f2,...,fn. Figure 5 summarizes our ﬁndings for all ten
measurements using box plots. To create a graph, we com-
pare collected performance models. We compare models
using the mechanism introduced in Section 3.3, that is we
check whether two models group the call paths in the same
way. Each data point represents the statistics of a perfor-
mance model Mfof a speciﬁc function f.
On the x-axis we show four entries #2 to #5. Each
data point in entry #2 is created by comparing the model
M1
ffor a single function fobtained during the ﬁrst exe-
cution with the corresponding model M2
fcollected during
the second execution, i.e. cmp(M1
f,M2
f). To create the data
points for entry #3, we merge the two models and com-
pare it with the model gathered during the third execu-
tion:cmp(merge(M1
f,M2
f),M3
f). Similarly, for data points
3https://www.spec.org/web2009/
44(a) Machine A.
(b) Machine B.
(c) Combined performance models of machine A and B.
Figure 5: Percentage of call paths (left) and data
points (right) that show consistent behavior across
multiple runs for two machines. We use Tukey
whiskers.
in entry #4 we again merge all previous models and com-
pare it with the current one: cmp(merge(merge(M1
f,M2
f),
M3
f),M4
f). Entry #5 is created accordingly.
IntheleftgraphinFigure5aweplotthepercentageofcall
paths that show consistent behavior for each model. Each
data points represents one model Mffor function f.W ec a n
see that most call paths for most models match reasonably
well. Most call paths show consistent performance behavior
across executions of the experiment.
However, we also note that for some models we can only
match a relatively low number of call paths, down to less
than 60%. We investigated further and found that call paths
that could be matched tend to have a higher number of
supporting observations compared to call paths that show
inconsistencies during merging.
Since call paths with a small number of supporting ob-
servations are over-represented in the set of call paths that
cannot be matched, we present another graph in which we
weight the call paths with the number of supporting obser-
vations (right-hand graphs in Figure 5). Each data point is
a performance model for a function. For each performance
modelMfwe plot the percentage of observations that sup-
port call paths with consistent behavior. For example, a
valueof90%meanswecanalign90%oftheobservationscol-
lected at a speciﬁc function facross multiple experiments.Even after merging all ﬁve executions into a single model,
we can still match more than 80% of the data points col-
lected at machine A for most models (Figure 5a, right). In
fact, the box plots are hardly noticeable because the tool
can align 100% of the observations for many models. There
are two outliers at about 40% for which our analysis fails
to create reliable performance models. We tested whether
problems during the merging of behavioral clusters could be
caused by complex models. We were unable to ﬁnd a corre-
lation. We conclude that the performance behavior of these
two functions is just too erratic to allow merging.
InFigure5bwepresentequivalentmeasurementscollected
on machine B. The results are even slightly better than on
machine A. In summary we succeed in collecting reliable
performance models on both machines.
Answer to RQ1 : Most performance models can be reli-
ably aligned across multiple executions.
4.2 RQ2: Platform Independence
After showing that performance models can be reliably
created and matched across multiple executions on the same
machine, weproceedtoshowthatmodelscanalsobemerged
across machines. This illustrates the ability to mine stable
performance models across di ﬀerent hardware platforms.
We align the models collected on machine A with models
created on machine B. Note that the two machines not only
have diﬀerent hardware conﬁgurations, but also run slightly
diﬀerent binaries; even though we run the same software, we
compiled the binary on each machine; thus, the binaries are
diﬀerent. Especially some functions present in the version
on machine B have been inlined in the version on machine
A. While this creates some di ﬃculties during merging, a
reliable approach should handle these challenges.
Toshowwhetherperformancemodelscanbealignedacross
platforms, we merge the models collected on machine A with
the corresponding models collected on machine B. We merge
the models at position nin Figure 5a with the correspond-
ing models at the same position in Figure 5b. The resulting
statistics for the merged models is depicted in Figure 5c.
For most functions the performance models can be merged
across di ﬀerent executions on di ﬀerent machines.
Answer to RQ2 : Performance models collected on dif-
ferent platforms and with di ﬀerent binaries show high sta-
bility and can be matched successfully to a large extent.
4.3 RQ3: Complexity
Mined performance models tend to be complex. We al-
ready presented a small example of a performance model
extracted from Apache 2.0.64 in Figure 1.4Other models
tend to be much larger. The size of the performance mod-
els motivated our approach to reduced them to their key
components (cf. Section 3.4).
Our tool found the model depicted in Figure 1 in the fol-
lowing way. The approach detects __semop as a frequently
executed function; it starts to measure the execution time.
After collecting some data the tool analyses it and ﬁnds one
stable border at 70 ms. This border separates the observed
data into two distinct behavioral clusters. Instead of re-
4We chose to show the model for the __semop func-
tion mainly because the model is small enough to ﬁt
into the manuscript. A larger model for polland
Apache 2.0.64 is available at https://drive.google.com/ﬁle/
d/0B9FeGNLlmM1aTW1xdkhlREhiZDA
45Table 1: The performance assertion extracted from
the model of __semop (cf. Figure 1).
Behavioral Clusters
Behavior Expr. t<70mst≥70ms
1 e1∧¬e241.9% 58.1%
2 ¬e1∧e235.5% 64.5%
3¬e1∧¬e299% 1%
Figure 6: The number of atomic formulae necessary
to distinguish between clusters depends on the num-
ber of clusters.
porting the whole performance model and hoping for proper
interpretation by the stakeholders, the system extracts dis-
tinguishing features that discriminate di ﬀerent performance
behavior (Section 3.4). After ﬁnding the stable border at
70msour tool continues and collects the execution times as
well as the call stack.
Analyzing the gathered call stacks, the tool detects three
distinct performance behaviors. Using the A* algorithm it
ﬁnds two edges that are su ﬃcient to distinguish between the
three performance behaviours. Our algorithm chooses edges
e1ande2in Figure 1. The resulting performance assertion
annotated with concrete time values is shown in Table 1.
Performance behavior #3 is the most distinguished one.
Whenever we enter the __semop function and we neither
passed edge e1nor edge e2, that is we reached __semop from
proc_mutex_sys_release ,__semop returns after less than
70msin 99% of the cases. The other two performance be-
haviors, #2 and #3, are very di ﬀerent from #1. For be-
havior #2 64.5% of the executions take longer than 70 ms.
Behavior #3 is similar: 58.1% of the executions take longer
than 70ms. Both pass through proc_mutex_acquire .B e -
haviors #2 and #3 are similar, but the di ﬀerence is sig-
niﬁcant across multiple executions. Using only two unique
atoms, edges e1ande2, we can reduce the performance
model of __semop with three di ﬀerent behavioral clusters
to three simple boolean expressions. These expressions are
shown in the second column of Table 1.
We believe that these expressions can help stakeholders to
understand performance di ﬀerences. Investigating perfor-
mance behavior by scrutinizing the complete call tree can
be a daunting task, especially for larger trees. Instead, a
stakeholder can inspect the boolean expressions. These ex-
pressions quickly point to the important di ﬀerences between
call paths that result in di ﬀerent performance behavior.
How diﬃcult it is to understand the mined performance
models and assertions? To answer this question (RQ3), the
bestapproachwouldbetomeasuretheperceivedcomplexity
using a large population of engineers that leads to statisti-
cally sound results. Unfortunately, we do not have such a
010 20 30 40 507580859095100105
Instrumented
Uninstrumented
Benchmark iterationRuntime [s]Figure 7: Performance overhead of MySQL 5.0.88 .
large population at our disposal.5Thus, we use a proxy
metric and quantify the complexity of a performance model
using the number of unique edges in the performance asser-
tion. The number of unique edges in the performance asser-
tion is just the number of atoms in the boolean expressions.
In Figure 6 we compare the number of behavioral clusters
to the number of atoms. Not surprisingly, the number of
atoms increases with the number of clusters. Performance
models that represent more complex performance behavior,
that is models with many clusters, require expressions with
more atoms to distinguish between these behaviors.
Answer to RQ3 : Mined performance models tend to be
complex. We succeed to reduce them to performance as-
sertions that use small boolean expressions to distinguish
performance behavior. With a maximum number of eight
required atomic formulae we believe our performance asser-
tions are compact enough to be read and understood by
stakeholders.
4.4 RQ4: Performance Overhead
We collect performance data during execution. To collect
this data, we add instructions into the running application.
Executing these instructions has an e ﬀect on many measur-
able metrics; especially execution time is easily a ﬀected.
We evaluated the performance impact of our mining ap-
proach using all three subjects, Apache, MySQL, and Oku-
lar. The experiments in this subsection are executed on
machine A.
Okular is the default pdf viewer under KDE. To get a
ﬁrst impression, we mined performance speciﬁcations from
Okular 0.19.3 while we were using it for our daily work. Not
surprisingly, we did not perceive any performance impact.
Okular is a GUI application and thus waits for user input
most of the times. To asses the actual overhead incurred,
we measured the time it took to parse and render a 19 MB
large pdf ﬁle containing 3883 pages. Once the ﬁle is loaded
we load it again. In this way we eliminate idle time and
keep the application busy at 100% CPU load. In total we
load the ﬁle 10 times. Mean as well as median time to load
the ﬁle is 12.74 seconds with  =0.03. Next, we employ
our performance mining approach. Mean time increases by
16.0% to 14 .8 seconds and median time increases by 13 .9%
to 14.5 seconds (  =0.94).
The impact of data collection on execution time is hard to
predictandoftendoesnotcorrelatelinearlywiththeamount
of instrumentation inserted. Figure 7 presents an extreme
example. We started a single instance of the MySQL server.
Next, we ran the test-select benchmark. Once the bench-
mark terminates, we start it again leading to another iter-
5A comprehensive ﬁeld study is left as future work.
46ation (x-axis). We leave the same MySQL server running
across benchmark invocations. Mining performance speci-
ﬁcations for MySQL 5.0.88 leads to a speedupfor most it-
erations. We attribute this speedup to cache and pipeline
eﬀects. Even minor and seemingly harmless changes to the
execution environment can have a severe impact on execu-
tion times [19]; our approach modiﬁes and rearranges the
execution image, a much larger change.
Figure 7 also conﬁrms an observation we made while eval-
uating Okular; the variability in the runtime measurements
for the instrumented case is much larger. This is caused by
our automatic adaptation approach; new data is constantly
pouring in, leading to adaptations of the instrumentation
and changing overhead levels.
AsathirdsubjectweranApachehttp2.0.64. Thisversion
uses multiprocessing instead of multithreading in the default
conﬁguration. We use the same setup as in RQ1 to RQ3
except that we increased the execution time to 7.5 hours to
observe long term e ﬀects. Average response time increases
from 390 ms( =0.5) to 435 ms( =6 1.2), an increase of
11.1%.
All numbers in this subsection should be taken with a
grain of salt; the incurred overhead can be easily manipu-
lated and reduced using sampling. Our approach does not
aim at minimal overhead; instead, we try to gather data
as fast as possible, without incurring a too high overhead.
Whatdeﬁnesatoohighoverheadissubjecttodebate; there-
fore, a user can specify the target amount of overhead. In
this section we used a target overhead of 10%. We could
have used a smaller target overhead which would have de-
creased the reported performance overheads as well.
Answer to RQ4 : Collecting data a ﬀects execution time.
Incurred overhead is modest and stays reasonably close to
our conﬁgured target overhead of 10%.
4.5 RQ5: Perturbation
The eﬀect that data collection has on an application is in-
teresting from two di ﬀerent perspectives. First, the perfor-
mance overhead has to be reasonably small to ensure that
the impact on total performance and throughput is man-
ageable and does not render the system unusable. Second,
measuring should not perturb the behavior of an application
beyond reasonable bounds.
Collecting data at runtime not only delays execution, it is
likely to perturb the collected data in unpredictable ways.
To minimize perturbation of collected data, we carefully
designed the collection process to inject an almost constant
number of instructions into the application. We strive for
a constant overhead. A constant overhead has the main
beneﬁt that it only a ﬀects the nominal execution time but
not the shape of the distribution itself. To illustrate trends
and get a grip on worst-case overhead and perturbation,
we looked into how data collection a ﬀects the runtime of a
fast executing function. We executed MySQL 5.0.88 and ran
thetest-select benchmark. We sampled the instruction
pointer at random intervals. We collected 20,727 samples
that map to 264 di ﬀerent unique functions. From these 264
functions we randomly selected a function that (1) executes
very fast, (2) is called frequently, and (3) has a non-trivial
distribution of execution times.
By chance we selected the _mi_bin_search function. To
measure the e ﬀects of runtime instrumentation, we man-
ually augmented the source code of _mi_bin_search withFigure 8: Measured execution time of _mi_bin_search
running MySQL 5.0.88 and the test-select bench-
mark. Unaltered application (top), perturbation
due to dynamic instrumentation (middle), and ex-
ecution time collected by dynamic instrumentation
(bottom). We use a bucket size of 10ns.T h e l a -
bels on the x-axis represent the lower bound of the
bucket.
two calls to clock_gettime to measure the execution time.
Note that manually adding instructions to the source code
will itself perturb the time measurements. There is an in-
herent limitation towards achievable accuracy and precision
regarding time measurements without relying on extra hard-
ware support or full scale simulation. However, we expect
static instrumentation to a ﬀect time measurements less than
dynamic instrumentation. Thus, we feel comfortable to use
the executions times collected using static instrumentation
as a reference point to evaluate the relative overhead and
perturbation of our dynamic instrumentation.
Figure 8 shows three graphs generated from 20 runs each.
The graph at the top plots the execution times measured by
static instrumentation without any instrumentation. The
graph in the middle plots the execution times measured by
static instrumentation while we also injected instructions to
collect execution time dynamically. Thus, it represents the
overhead added to a function by measuring execution times
dynamically. Finally, the graph at the bottom plots the
execution times measured by dynamic instrumentation. We
can draw several conclusions.
First, using dynamic runtime instrumentation to measure
execution time adds an average overhead of 424 nsand a
median overhead of 420 ns.
Second, comparing the top graph with the graph in the
center we notice that dynamic instrumentation not only
adds overhead, but also perturbs the execution time. It
follows that the overhead is not constant. We attribute
the perturbation largely to cache e ﬀects. Comparing the
statically instrumented version with the dynamically instru-
mented one, we noticed an increase in the number of in-
struction cache misses by 66 .3% from 0 .96 billion to 1 .59
billion.
Third, the execution times measured by an external ob-
server (graph in the middle) are more perturbed that the
execution times collected by our approach (bottom graph).
The data collected by our approach is less perturbed, be-
47cause after time is measured, it is aggregated and communi-
cated; this adds another layer of overhead and perturbation.
This observation led to another improvement in our ap-
proach. Perturbation is especially problematic if the collec-
tion of executions times is nested. We detect nested col-
lections and annotate gathered data with statistics about
the level of nesting. The analysis module takes these statis-
tics into consideration. If the analysis module decides the
perturbation is too large to draw reliable conclusions, it ig-
nores the data and re-collects it in isolation, that is it only
collects data for one function. In this way we prevent the
accumulation of perturbation and maximize data validity.6
Answer to RQ5 : Collected data is perturbed by the
observer e ﬀect. However, even for a function we chose de-
liberately to illustrate the expected worst-case scenario, key
features of the collected data are kept intact. This supports
ourperceptionthatminedperformancemodelsreﬂectactual
behavior.
4.6 Threats to Validity
We limited our evaluation to three subjects and two ma-
chines. This threatens the external validity of our results.
Furthermore, we used the SPEC-web2009 banking bench-
mark for our evaluation. While the benchmark randomizes
the workload, requests are still relatively structured with
limited diversity. This might decrease the complexity of our
models and might increase the correlation between mined
performance models. An extensive ﬁeld study would pro-
vide more support and could eradicate these threats but is
beyond the scope of this paper.
5. RELATED WORK
Mining functional speciﬁcations focuses on extracting in-
teraction patterns [1, 15]. System logs can also be used to
mine behavioral performance models by leveraging existing
timestamps [21]. It is hard to reason about non-functional
properties and measure them precisely [16]. This work con-
tributes to the existing approaches. Our approach is scal-
able, collects data collection in-ﬁeld without human inter-
vention, and creates meaningful performance models that
aid in comprehension and can be further reﬁned into perfor-
mance assertions.
Our performance assertions can be interpreted as a spec-
iﬁcation of an acceptable performance spectrum and thus
might be considered an instantiation of a program spec-
trum [11].
In an attempt to determine the complexity of a piece of
code, some related work correlates di ﬀerent run time pa-
rameters, most often input size, with execution cost [2, 27].
Even though it might be possible to calculate the input size
dynamically at runtime for standard structures [27, 9], mea-
suringinputsizeformorecomplexstructures(e.g. graphs)is
harder and might require consultation of an expert. Due to
this potential pitfall we do not take input sizes into account.
The notion of a performance assertion is not new [25, 24].
However, our performance assertions di ﬀer signiﬁcantly in
that they incorporate information about the call stack and
are collected autonomously.
6We detect recursive functions, but currently do not handle
them in a special way. One could sample recursive function
at a recursion depth that is randomly picked each time the
function is entered and current recursion depth is 0.Automatic behavioral regression testing is an in-house
functional testing tool that runs di ﬀerent versions of a soft-
ware against generated test inputs, records changes in the
behavior, and reports it to the developer [14]. Another
approach uses probabilistic symbolic execution during in-
house testing to approximate the distribution of execution
times [7]. One major challenge of in-house performance test-
ing is the dependency on a representative workload proﬁle
that reﬂects real world usage. Creating a representative test
workload that reﬂects real-world usage is challenging [10].
As a consequence wrong workload assumptions account for
more than 35% of all performance bugs [13]. Our work dif-
fers. We target in-ﬁeld performance regression testing. We
believe it can be used to leverage the user base and ex-
tract patterns in deployed systems to augment in-house test-
ing [22]. In-ﬁeld testing is not new. For example, it has been
used to monitor di ﬀerent coverage criteria [23, 3], as well as
exploring the conﬁguration space of software programs [18].
To gain platform independence, Stochastic Performance
Logic compares relative executions times of functions [5]. In
our work we compare the grouping of call paths leading to
a single functions based on the execution times instead.
We use Dyninst as an instrumentation backend [4]. We
believe it is possible to use existing proﬁlers to replace or
augment our data collection. One prerequisite is to have
ﬁne grained control about what data is collected when. For
example, we believe we could leverage the probes mecha-
nism in the YourKit7proﬁler to extend our data analysis to
Java processes. We could also use OProﬁle8to include hard-
ware performance counters. However, the sampling nature
of OProﬁle might proof problematic especially for functions
with low total runtime. Our approach tries to be as accurate
as possible; we do not sample unless the overhead actually
observed during runtime is too high. Other proﬁlers, e.g.
gprof9, require static instrumentation during compilation.
We believe dynamic instrumentation is crucial to enable the
monitoring of deployed applications.
Our tool is lightweight as compared to more heavyweight
binary instrumentation approaches like e.g. Valgrind [20] or
Pin [17].
6. CONCLUSION
Deﬁning non-functional speciﬁcations like performance is
costly. Regularly, especially in agile environments, precisely
deﬁning performance speciﬁcations up-front is counterpro-
ductive. We tackle these challenges and mine performance
speciﬁcations automatically, without human intervention.
We demonstrated that we can match models across machine
boundaries. This is a crucial step to achieve our overall
goal, a system that uses volunteer computing to collabora-
tive mine performance speciﬁcations.
There are multiple interesting avenues that should be ex-
plored in the future. It would be worthwhile to study the
stability of performance models across an exhaustive set of
subjects. The goal of performance speciﬁcation mining is to
help stakeholders accomplish their goals. A comprehensive
ﬁeld study that evaluates the usability and applicability of
mined performance speciﬁcations would be valuable.
7http://www.yourkit.com
8http://oproﬁle.sourceforge.net
9http://sourceware.org/binutils/docs/gprof/
487. REFERENCES
[1] G. Ammons, R. Bod´ ık, and J. R. Larus. Mining
speciﬁcations. In Proceedings of the 29th Symposium
on Principles of Programming Languages ,P O P L’ 0 2 ,
pages 4–16, New York, NY, USA, 2002. ACM.
[2] A. Bhattacharyya and T. Hoeﬂer. Pemogen:
Automatic adaptive performance modeling during
program runtime. In Proceedings of the 23rd
International Conference on Parallel Architectures and
Compilation , PACT ’14, pages 393–404, New York,
NY, USA, 2014. ACM.
[3] J. Bowring, A. Orso, and M. J. Harrold. Monitoring
deployed software using software tomography. In Proc,
of the 2002 workshop on Program analysis for software
tools and engineering , PASTE ’02, pages 2–9, New
York, NY, USA, 2002. ACM.
[4] B. Buck and J. K. Hollingsworth. An api for runtime
code patching. Int. J. High Perform. Comput. Appl. ,
14(4):317–329, Nov. 2000.
[5] L. Bulej, T. Bureˇ s, J. Keznikl, A. Koubkov´ a,
A. Podzimek, and P. T˚ uma. Capturing performance
assumptions using stochastic performance logic. In
Proceedings of the 3rd ACM/SPEC International
Conference on Performance Engineering , ICPE ’12,
pages 311–322, New York, NY, USA, 2012. ACM.
[6] A. Calotoiu, T. Hoeﬂer, M. Poke, and F. Wolf. Using
automated performance modeling to ﬁnd scalability
bugs in complex codes. In Proc. of the Inter. Conf. on
High Performance Computing, Networking, Storage
and Analysis , SC ’13, pages 45:1–45:12, New York,
NY, USA, 2013. ACM.
[7] B. Chen, Y. Liu, and W. Le. Generating performance
distributions via probabilistic symbolic execution. In
Proceedings of the 38th International Conference on
Software Engineering , ICSE ’16, pages 49–60, New
York, NY, USA, 2016. ACM.
[8] D. Comaniciu and P. Meer. Mean shift: a robust
approach toward feature space analysis. Pattern
Analysis and Machine Intelligence, IEEE Transactions
on, 24(5):603–619, May 2002.
[9] E. Coppa, C. Demetrescu, and I. Finocchi.
Input-sensitive proﬁling. In Proceedings of the 33rd
ACM SIGPLAN Conference on Programming
Language Design and Implementation , PLDI ’12,
pages 89–98, New York, NY, USA, 2012. ACM.
[10] A. B. Downey and D. G. Feitelson. The elusive goal of
workload characterization. SIGMETRICS Perform.
Eval. Rev. , 26(4):14–29, Mar. 1999.
[11] M. J. Harrold, G. Rothermel, K. Sayre, R. Wu, and
L. Yi. An empirical investigation of the relationship
between spectra di ﬀerences and regression faults.
Software Testing, Veriﬁcation and Reliability ,
10(3):171–194, 2000.
[12] T. Hoeﬂer, W. Gropp, W. Kramer, and M. Snir.
Performance modeling for systematic performance
tuning. In State of the Practice Reports , SC ’11, pages
6:1–6:12, New York, NY, USA, 2011. ACM.
[13] G. Jin, L. Song, X. Shi, J. Scherpelz, and S. Lu.
Understanding and detecting real-world performance
bugs.SIGPLAN Not. , 47(6):77–88, June 2012.
[14] W. Jin, A. Orso, and T. Xie. Automated behavioral
regression testing. In Proceedings of the 2010 ThirdInternational Conference on Software Testing,
Veriﬁcation and Validation , ICST ’10, pages 137–146,
Washington, DC, USA, 2010. IEEE Computer Society.
[15] S. Kumar, S.-C. Khoo, A. Roychoudhury, and D. Lo.
Mining message sequence graphs. In Proceedings of the
33rd International Conference on Software
Engineering , ICSE ’11, pages 91–100, New York, NY,
USA, 2011. ACM.
[16] E. Letier and A. van Lamsweerde. Reasoning about
partial goal satisfaction for requirements and design
engineering. SIGSOFT Softw. Eng. Notes ,
29(6):53–62, Oct. 2004.
[17] C.-K. Luk, R. Cohn, R. Muth, H. Patil, A. Klauser,
G. Lowney, S. Wallace, V. J. Reddi, and
K. Hazelwood. Pin: building customized program
analysis tools with dynamic instrumentation. In PLDI
’05, pages 190–200, New York, NY, USA, 2005. ACM.
[18] A. Memon, A. Porter, C. Yilmaz, A. Nagarajan,
D. Schmidt, and B. Natarajan. Skoll: distributed
continuous quality assurance. In Software Engineering,
2004. ICSE 2004. Proceedings. 26th International
Conference on , pages 459–468, 2004.
[19] T. Mytkowicz, A. Diwan, M. Hauswirth, and P. F.
Sweeney. Producing wrong data without doing
anything obviously wrong! SIGARCH Comput.
Archit. News , 37(1):265–276, Mar. 2009.
[20] N. Nethercote and J. Seward. Valgrind: a framework
for heavyweight dynamic binary instrumentation. In
PLDI ’07 , pages 89–100, New York, NY, USA, 2007.
[21] T. Ohmann, M. Herzberg, S. Fiss, A. Halbert,
M. Palyart, I. Beschastnikh, and Y. Brun. Behavioral
resource-aware model inference. In Proceedings of the
29th ACM/IEEE International Conference on
Automated Software Engineering , ASE ’14, pages
19–30, New York, NY, USA, 2014. ACM.
[22] A. Orso. Monitoring, analysis, and testing of deployed
software. In Proceedings of the FSE/SDP workshop on
Future of software engineering research , FoSER ’10,
pages 263–268, New York, NY, USA, 2010. ACM.
[23] A. Orso, D. Liang, and M. J. Harrold. Gamma
system: Continuous evolution of software after
deployment. In Proceedings of the International
Symposium on Software Testing and Analysis (ISSTA
2002), pages 65–69, Rome,Italy, July 2002.
[24] S. E. Perl and W. E. Weihl. Performance assertion
checking. In Proceedings of the Fourteenth ACM
Symposium on Operating Systems Principles , SOSP
’93, pages 134–145, New York, NY, USA, 1993. ACM.
[25] J. S. Vetter and P. H. Worley. Asserting performance
expectations. In Proceedings of the 2002 ACM/IEEE
Conference on Supercomputing , SC ’02, pages 1–13,
Los Alamitos, CA, USA, 2002. IEEE Computer
Society Press.
[26] N. X. Vinh, J. Epps, and J. Bailey. Information
theoretic measures for clusterings comparison:
Variants, properties, normalization and correction for
chance.J. Mach. Learn. Res. ,1 1 : 2 8 3 7 – 2 8 5 4 ,2 0 1 0 .
[27] D. Zaparanuks and M. Hauswirth. Algorithmic
proﬁling. In Proceedings of the 33rd ACM SIGPLAN
conference on Programming Language Design and
Implementation , PLDI ’12, pages 67–76, New York,
NY, USA, 2012. ACM.
49