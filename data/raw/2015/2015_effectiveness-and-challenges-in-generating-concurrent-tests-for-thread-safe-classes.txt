Effectiveness and Challenges in Generating
Concurrent Tests for Thread-Safe Classes
Valerio Terragni
USI Università della Svizzera italiana, Switzerland
valerio.terragni@usi.chMauro Pezzè
USI Università della Svizzera italiana, Switzerland
University of Milano-Bicocca, Italy
mauro.pezze@usi.ch
ABSTRACT
Developingcorrectandefficientconcurrentprogramsisdifficult
anderror-prone, duetothecomplexity ofthreadsynchronization.
Often, developers alleviate such problem by relying on thread-safe
classes,whichencapsulatemostsynchronization-relatedchallenges.
Thus,testingsuchclassesiscrucialtoensurethereliabilityofthe
concurrency aspects of programs. Some recent techniques and cor-
respondingtoolstackletheproblemoftestingthread-safeclassesby
automaticallygeneratingconcurrenttests.Inthispaper,wepresent
a comprehensive study of the state-of-the-art techniques and anindependent empirical evaluation of the publicly available tools.
We conducted thestudybyexecutingalltoolsonthe JaConTeBe
benchmarkthatcontains47well-documentedconcurrencyfaults.
Ourresultsshowthat8outof47faults(17%)weredetectedbyat
leastonetool.Bystudyingtheissuesofthetoolsandthegenerated
tests, we derive insights to guide future research on improving the
effectiveness of automated concurrent test generation.
CCS CONCEPTS
•Software and its engineering →Concurrent program-
ming languages; Software testing and debugging;
KEYWORDS
Test generation, Concurrency faults, Thread-safety
ACM Reference Format:
Valerio Terragni and Mauro Pezzè. 2018. Effectiveness and Challenges in
Generating Concurrent Tests for Thread-Safe Classes. In Proceedings of
the 2018 33rd ACM/IEEE International Conference on Automated Software
Engineering(ASE’18),September3–7,2018,Montpellier,France. ACM,New
York, NY, USA, 12pages.https://doi.org/10.1145/3238147.3238224
1 INTRODUCTION
Concurrent programming is pervasive across application do-
mains due to the widespread of multi-core chip technology. De-
veloping correctand efficientconcurrent programsis harddue to
the complexity of thread synchronization that suffers from under-
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ASE ’18, September 3–7, 2018, Montpellier, France
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5937-5/18/09...$15.00
https://doi.org/10.1145/3238147.3238224and over-synchronization problems. Under-synchronization intro-
ducessubtle concurrencyfaults,likedata racesandatomicityvio-
lations,thataredifficulttoexposeattestingtimesincetheyman-
ifestunderspecificnon-deterministicthreadinterleavings.Over-
synchronizationcausesdeadlocksandaffectsperformancebyin-
troducing overhead and reducing parallelism [36, 42].
Oftendevelopersreducethecomplexityofdevelopingreliable
concurrent programs in object-oriented shared-memory languages,
for instance JavaandC++, by relying on thread-safe classes [22],
whichaddresstheimportantchallengeofsynchronizingconcurrent
memory accesses in a correct and efficient way [ 42]. By delegat-
ing the burden of thread synchronization to thread-safe classes,
developers can use the same instance of such classes from multiplethreadswithoutadditionalsynchronization[
22],thusrelyingonthe
correctnessof thread-safe classestoavoidconcurrencyfailures[ 36].
Ensuringthecorrectnessofthread-safeclassesisimportant.Itiden-
tifiesconcurrencyfaultsintheimplementationofthethread-safe
classes, and thus in the programs that rely on them.
An effective approach to validate the correctness of thread-safe
classesconsistsinautomaticallygeneratingconcurrentunittests,
andcheckingifthegeneratedteststriggerfault-revealingthreadin-terleavings,byrelyingononeofthemanyinterleavingexploration
techniques [ 10,17,51,60]. A concurrent unit test, concurrent test
hereafter, consists of multiple concurrently executing threads that
exercisethepublicinterfaceofaclassundertest.Figure 2shows
anexampleofconcurrenttestthattriggersaconcurrencyfaultin
the thread-safe class AppenderAttachableImpl of theLog4Jlibrary,
shown in Figure 1.
Inrecentyears,researcheshaveproposedtechniquesforauto-
matically generating concurrent tests [ 12,35,40,45,48–50,57,59],
inspired by recent advances in sequential unit test generation [ 20,
37]. Generating concurrent tests faces new challenges that are
not present when generating sequential tests, like multi-threading,
non-determinism, shared states, and huge spaces of thread inter-
leavings [12, 59].
Current generators of concurrent tests address the new chal-
lenges with different approaches, for which there is no clear ev-idence of their effectiveness and limitations yet. Although, the
authors of these approaches have performed experiments to evalu-
ateandcomparethem[ 12,59],suchexperimentsaretoonarrowin
thecriteriatoselectsubjectsandalsointheanalysisoftheresultsto
provide solid evidence of effectiveness and limitations. To address
thisgap, thispaper reportsanempirical studyofthe sixcurrently
openly-accessibleconcurrenttestgeneratorsforthread-safeclasses.
The goal of the study is to (i) assess the effectiveness of the tech-
niques in generating fault-revealing tests, (ii) identify and better
understand their limitations, and (iii) shed light on future research
64
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:47:06 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Valerio Terragni and Mauro Pezzè
1/** faulty AppenderAttachableImpl class - Log4j v. 1.2.17
2https://bz.apache.org/bugzilla/show_bug.cgi?id=54325 **/
3 public void removeAllAppenders() {
4
5 if(appenderList != null){
6 intlen = appenderList.size();
7 for(int i = 0; i < len; i++) {
8 Appender a = (Appender) appenderList.elementAt(i);
9 a.close();
10 }
11 appenderList.removeAllElements();
12 appenderList = null;
13 }
14}
Figure 1: Code related to the Log4j concurrency bug 54325.
directions identifying ways in which existing techniques can be
improvedornewtechniquesbedefined.Inourstudy,wereferto
the recent JaConTeBe benchmark of concurrency faults in thread-
safe classes, published in ASE 2015, a repository of 47 concurrency
faults experienced in the field [ 31]. We would like to notice that
weselectedthebenchmarkindependentlyfromthestudiedtools,
toavoidbiasesinselectingthesubjects.Ourresultsindicatethat
only 17% of the faults can be detected by at least one of the genera-
tors of concurrent tests evaluated in this paper. This indicates both
an impressive effectiveness and a large space for improvements.
Weanalyzedtheresultsofboththeautomatically-generatedand
the JaConTeBe test suites, and examined the characteristics of
both detected and undetected faults to study the effectiveness and
limitationsoftheconcurrenttestgenerators.Weidentifiedthree
mainissuesthatpreventthemfromgeneratingfault-revealingtests,
and we observed that the issues are shared among the different
techniques.Wedevisefutureresearchdirectionstoaddressthese
problemsandincreasetheoveralleffectivenessofconcurrenttest
generation.Toeasereproducibility,ourexperimentaldataareavail-
able in our website [1].
In summary, this paper contributes to the state-of-the-art by
presenting:
•Asurveyontheexistingtechniquesforgeneratingconcur-
rent unit tests for thread-safe classes;
•A large-scale experimental and comparative evaluation of
the six generators of concurrent tests for Java thread-safe
classesconductedontheJaConTeBebenchmark[ 31],which
includes 47 concurrency faults;
•An analysis of the effectiveness of these six techniques in
detecting concurrency faults;
•A discussion of a set of insights that we gained from the
study,andthatgivesomeguidelinesforfutureresearchin
this area;
2 GENERATING CONCURRENT TESTS
Thissectionpresentsthepreliminariesandbackgroundoftest
generationforexposingconcurrencyfaultsinthread-safeclasses,in
thecontextofconcurrentobject-orientedprogramsthatimplement
the shared-memory programming paradigm.
Aconcurrentshared-memoryobject-orientedprogramiscom-
posed of a set of classes, each composed of a set of methods and
fieldsthatcanbeexecutedandaccessedconcurrentlybymultiple
threads,respectively.Aclassis thread-safe ifitencapsulatessyn-
chronization mechanisms that prevent incorrect accesses to the
classfrommultiplethreads[ 22].Aclassisthread-unsafeotherwise.1// Sequential Prefix
2AppenderAttachableImpl var0 = newAppenderAttachableImpl();
3ConsoleAppender var1 = newConsoleAppender();
4var0.addAppender((Appender) var1);
5
6 newThread(new Runnable() {
7 public void run() {
8 var0.removeAllAppenders(); // Suffix 1
9 }}).start();
10
11 newThread(new Runnable() {
12 public void run() {
13 var0.removeAllAppenders(); // Suffix 2
14 }}).start();
Figure 2: Concurrent test that exposes the bug in Figure 1.
Example of synchronization mechanisms are synchronized blocks
inJava, and locks, mutexes and semaphores in C. Thread-safety
guarantees that the same instance of a thread-safe class can be
correctly accessed by multiple threads without additional synchro-
nization other than the one implemented in the class [40].
Writing correct, efficient and reliable thread-safe classes is hard,
due to the non-deterministic order of memory accesses across
threads,whichcanleadto thread-safetyviolations,thatis,deviations
fromtheexpectedbehavioroftheclasswhenconcurrentlyaccessed.
A key characteristic of such violations is that they manifest non-
deterministically,duetothenon-determinismoftheschedulerthat
determines the execution order of threads. The order of accesses to
sharedmemorylocationsisfixedwithinonethread,butcanvary
across threads. An interleaving is a total order relation of shared
memory accesses amongthreads [ 32]. Concurrent executionscan
manifestmanydifferentinterleavings,andonlysome–usuallyfew–
of them expose thread-safety violations [38].
Asanexampleofthread-safetyviolation,Figure 1showsthecode
snippet of class AppenderAttachableImpl of theLog4jlibrary, part
of the JaConTeBe benchmark [ 31]. Method removeAllAppenders
checks whether the object instance field appenderList is initialized
(line 5) before dereferencing it (line 6), and setting the reference to
null(line12).Theseaccessesarenotproperlysynchronized:two
threads that concurrently invoke removeAllAppenders may cause a
NullPointerException bothinaspecificprogramstateandwitha
particular thread interleaving (line 6). In a thread interleaving that
triggers the exception, a thread t1executes line 12 after a thread t2
executed line 5 and before t2executes line 6.
Aneffectiveapproachforvalidatingthecorrectnessofthread-
safeclassesistheautomatedgenerationofconcurrenttests.Figure 3
shows a logical architecture of such approach, which is shared
among all the surveyed techniques.
In a nutshell, given a class under test and, optionally, a set of
auxiliaryclassesthattheclassundertestdependson,thetechniques
automatically exposes thread-safety violations in four consecutive
steps: (i) they generate sequential (single-threaded) method call
sequences that exercise the public interface of the class under test,
(ii) assemble such sequences in a concurrent test that executes
concurrently the method call sequences from multiple threads,
(iii)exploretheinterleavingspaceofthegeneratedconcurrenttests
by means of state-of-the-art interleaving explorers, (iv) check if
any of the explored interleavings exposes a thread-safety violation.
Figure2shows an example of a concurrent test that can reveal the
fault-revealing interleaving described above. In this paper, we refertothegeneraldefinitionofconcurrenttestpresentedintheseminal
paper of Pradel and Gross [40].
65
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:47:06 UTC from IEEE Xplore.  Restrictions apply. Effectiveness and Challenges in Generating Concurrent Tests for Thread-Safe Classes ASE ’18, September 3–7, 2018, Montpellier, France
Class 
Under T est
Auxiliary 
ClassesInput
Thread
InterleavingsOutput
Thread 2 Thread 1 Call Sequence 
GeneratorCall Sequences 
AssemblerInterleaving 
ExplorerConcurrent Test
Method Call 
Sequences
S1,S2, S3…S1
S2 S3
Thread-safety 
Oracle
Thread-safety 
ViolationsConcurrent Test Generation Interleaving Exploration
Step 1 Step 4 Step 2 Step 3
Figure 3: Logical architecture of concurrent test generation.
Aconcurrenttest isasetofmethodcallsequences,whereeach
call in a sequence consists of a method signature, a possibly empty
list of input variables (method parameters), and an optional output
variable (method return value). In an instance method call, the first
input variable is the object that receives the call, for example, var0
in the call at line 8 Figure 2. A test consists of a prefix and a set
of suffixes. A prefixis a call sequence to be executed in a single
thread that instantiates the class under test, to create the object
instances that will be accessed concurrently from multiple threads.
A prefix may need to invoke additional methods to bring an ob-
ject instance into a particular state that may enable the suffixes
totriggerathread-safetyviolation.Forinstance,themethodcall
var0.addAppender((Appender) var1); at line 4 in Figure 2instan-
tiates the shared object field appenderList , in order to satisfy the
condition of the if statement at line 5 in Figure 1, during the execu-
tionoftheconcurrentsuffixes,totriggera NullPointerException
whenexecutingthestatementatline6.A suffixisacallsequence
to be executed concurrently with other suffixes, after executing
thecommonprefix.Allsuffixessharetheobjectinstancescreated
bytheprefix,andcanusethemasinputvariablesforsuffixcalls,
toinvokemethodsthataccessthesharedinstancesconcurrently.
For example, all the suffixes in Figure 2use the same shared object
instance var0of the class under test as an object receiver.
3 STATE-OF-THE-ART GENERATORS OF
CONCURRENT TESTS
Foracompletesurveyofthemaintechniquestogeneratecon-
current tests, we identified a list of relevant papers, by querying
scholarlywebengines(GoogleScholar,ACMandIEEEDigitalLi-
braries)withthequery: test generation + concurrency ,andrefined
the list, by ignoring the papers unrelated to concurrent test gener-
ation and concurrency fault detection. We discarded papers that
presenttechniquesforgeneratingsequentialtests[ 20,37,62],for
generatingconcurrentteststoexposeperformanceissues[42],in-
correct substitutability faults [ 41], or for reproducing concurrency
failuresfromcrashstacks[ 7],whichareoutsidethefocusofthis
study. We discarded techniques presented only in short workshop
papers[52,53].Weexcludedtechniquesthatgeneratetestinputs
forconcurrentprograms,notablytechniquesthatuseconcolicex-
ecution or symbolic analysis [ 16,24,44], since these techniques
generatesonlyinputsandnotconcurrenttests,astheoneshown
in Figure 2.
Table1summarizes the nine relevant techniques that we identi-
fiedinourstudy.Thetableindicatesthenameofthetool,themain
reference,thevenueandyearofpublication,andthecategoryof
thetechnique,accordingtothetaxonomyofChoudharyetal.’,who
classifyconcurrenttestgeneratorsas random-based, coverage-based
andsequential-test-based [12].Table 1: State-of-the-art Generators of Concurrent Tests
Tool name Reference Venue Year Category
Ballerina [35] ICSE 2012random-basedConTeGe [40] PLDI 2012
ConSuite [57] ICST 2013
coverage-based AutoConTest [59] ICSE 2016
CovCon [12] ICSE 2017
Omen [45–47] FSE 2014
sequential-test
basedNarada [49] PLDI 2015
Intruder [48] FSE 2015
Minion [50] OOPSLA 2016
InSections 3.1,3.2and3.3,weoverviewthetestgenerationtech-
niques, grouped per category, corresponding to Steps 1 and 2 in
Figure3.InSection 3.4,wediscusstheinterleavingexplorersand
thread-safety oracles used by the different techniques, correspond-
ing to Step 3 and 4 in Figure 3.
3.1 Random-Based Techniques
Thepioneerrandom-basedtechniquesareBallerina[ 35]and
ConTeGe [ 40], which generate concurrent tests by randomly com-
bining randomly generated method call sequences with random
input parameters. Both Ballerina and ConTeGe rely on existing
interleaving explorers and use linearizability [ 26] as test oracle.
Linearizability reportsa violationwhenevera threadinterleaving
produces a behavior that cannot be produced in any linearized test
execution where all methods are execute atomically (sequentially).
Ballerina ICSE 2012 [35] . Nistor et al. identify as a major
challengeofconcurrenttestgenerationthehighcomputationalcost
ofexploringtheinterleavingspacesofthegeneratedtests,which
size grows factorially with the number of shared memory accesses
executed by the concurrent suffixes [ 32]. Ballerina addresses this
challengebyconfiningeachconcurrentsuffixtoasinglemethod
call,limitingthetesttoasinglesharedobjectundertest,andgener-
atingtestswithexactlytwoconcurrentthreads.Ballerinaclusters
oracle violationsto reducethe cost ofinspecting them: oraclevio-
lations belonging to the same cluster are likely to be either all false
alarms or all true errors [35].
ConTeGePLDI2012[40] .ConTeGeaddressesthesamechal-
lenges of Ballerina, with a novel linearizability checker that im-
proves efficiency and reduces false alarms over Ballerina [9].
Pros:Random-based techniques can efficiently generate con-
current tests, since they do not require complex analysis; they can
generate thousands of concurrent tests in few seconds [ 12], and
can effectively expose easy-to-find concurrency faults [ 35], whose
manifestation does not depend on a particular program state.
Cons:Random-basedtechniquesarelesseffectiveinrevealing
hard-to-find concurrency faults, because randomly generated tests
tendtorepetitivelytestsimilarprogrambehaviors[ 12].Weneedto
randomly generate thousands or even millions of concurrent tests
66
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:47:06 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Valerio Terragni and Mauro Pezzè
to effectively detect hard-to-find faults, due to the low probability
ofrandomlygeneratingafailure-inducingtest[ 40].Forexample,
ConTeGe requires more than a million tests to expose a single
concurrencyfault[ 40].Thisisanissue,becauseofthehighcom-
putational cost of exploring the interleaving space of all generated
tests. In practice, we can explore only the interleaving space of few
tests within an affordable time budget [12, 35,59].
3.2 Coverage-Based Techniques
Coverage-basedtechniquesaddressthelimitationsofrandom-
based techniques by driving the generation of concurrent tests
withinterleavingcoveragecriteria[ 12,57,59].Thesetechniques
identifyandpruneconcurrentteststhatleadtoredundantbehav-
iors (thread interleavings) to steer test generation towards newprogram behaviors, thus avoiding the high cost of exploring the
interleaving spaces of redundant tests. A major challenge faced by
coverage-basedtechniquesisthehighcostofcomputingtheprecise
executable domain of interleaving coverage criteria for concurrent
programs [ 43,55,59]. To address this challenge coverage-based
techniquesrelyoncoveragecriteriathatareefficienttocompute,
at the cost of approximating the coverage of thread interleavings.
ConSuiteICST2013[57] .Theseminalcoverage-basedconcur-
rent test generation approach, ConSuite, extends Evosuite [ 20]
to generate concurrent tests. ConSuite statically estimates the
coverage requirements as a set of thread interleavings, and selects
candidate concurrent tests for covering uncovered interleavings,
by checking if the sequential executions of the suffixes of the tests
coverthememoryaccessesthatcomprisethetargetinterleaving.
ConSuite does not consider the execution context of shared mem-
ory accesses, and thus ignores concurrency faults that manifestfailure-inducinginterleavingsonlyunderspecificexecutioncon-
texts. Moreover, even if the concurrent suffixes cover the target
shared-memoryaccesseswhenexecutedsequentially,thereisno
guaranteethatthesuffixesexecutetheaccessesinthespecificor-
der prescribed by the interleaving coverage target, when executed
concurrently.
AutoConTest ICSE 2016 [59] . AutoConTest improves over
ConSuite by introducing a context-sensitivity coverage metric
that can be efficiently computed, as it analyses sequential execu-
tions,andthatincludessynchronizationsensitive(lockacquisitions
and releases) and calling context information. AutoConTest over-
comesthelimitationsofcomputinganapproximatedsetofcoverage
requirements statically (prior testing), by generating concurrent
testsiteratively,sothateachtestincreasesthecoveragebasedon
the coverage data that are collected during the test generation.
CovCon ICSE 2017 [12] . CovCon exploits the concept of con-
current method pairs, proposed by Deng et al. [ 14], that is, the set
ofpairsofmethodsthatexecuteconcurrently[ 14].CovConmea-
suresthe frequencyof concurrentexecutionsof pairsofmethods,
to focus the test generation on infrequently or not at all covered
pairs [12].
Pros:Coverage-based techniques limit redundant tests, thus
reducingthenumberofgeneratedtests,andconsequentlytheinter-leavingexplorationcosts,asinprincipleonlytheteststhatincrease
interleaving coverage will be analyzed by the (computationally
expensive) interleaving explorer.Cons:Theeffectivenessofcoverage-basedtechniquesdepends
on the coverage criterion, which might either be too expensive
tocomputeoritmightmissrelevantcoveragerequirements.The
optimaltrade-offbetweenanalysiscomputationalcostandpreci-
sionofthecomputedrequirementsisstillunclear.Forexample,the
ConSuitecoveragecriterionlikelymissescoveragerequirements
because of being both statically computed and specific to prede-
fined faulty interleaving patterns. AutoConTest efficiently learns
coverage requirements while generating method call sequences,
butcanmisscoveragerequirementsifspecificinputparametersareneededtoobservenewcoveragerequirements.TheCovConcover-
agecriterioniscomputedefficientlyatmethodlevel,butdoesnot
capture the many interleaving coverage requirements that involve
the shared memory accesses triggered by the methods.
3.3 Sequential-Test-Based Techniques
Thesequential-test-basedtechniquesproposedbySamaketal.
apply the same overall approach to different kinds of concurrency
faults [46,48–50]: Omen targets deadlocks, Narada data races,
Intruder atomicity violations, and Minion assertion violations.
They analyse concurrent programs starting from a suite of sequen-
tial(single-threaded)tests,whichcanbeeithermanually-written
orgeneratedbyexistingsequentialtestgenerators[ 20,37].They
analyzetheexecutiontracesobtainedbyexecutingtheinitialtest
suitessequentially,toidentifyconcurrencyfaultsthatmayoccur
when combining multiple sequential tests into concurrent tests. If
such faults are identified, these techniques generate concurrenttests, which they analyze with interleaving explorers to check if
they indeed expose the fault during a concurrent execution.
Pros:Sequential-test-based techniques do not generate concur-
rent tests that are irrelevant with respect to the considered type of
concurrency fault.
Cons:Theireffectivenessdependsontheinitialsetofsequential
tests.Thehypothesisthatsequentialtestsexecutedconcurrently
are always adequate to expose concurrency faults is not always
valid. Sequential tests do not refer to the concurrency structure:
manuallywrittensequentialtestsaredesignedwithoutconsidering
concurrency issues, while automatically generated sequential tests
are produced referring to sequential-based coverage criteria, forexample, branch coverage [
20]. Moreover, each sequential-test-
basedtechniquesimposesarelativelyhighcomputationalcost[ 12].
3.4 Interleaving Explorers and Thread-Safety
Oracles
Generatorsofconcurrenttestscheckifthegeneratedtestsexpose
thread-safety violations (Step 3 and 4 in Figure 3) by adopting
different interleaving explorers and thread-safety oracles, which
comewithdifferentcomputationalcosts,presentdiverseproneness
to false positives, and target different types of faults (data races,
atomicity violations) and related failure types (exceptions, endless
hang, and logical issues).
Table2summarizesthemaininterleavingexplorers:selective,
randomandexhaustivetechniques. Selectivetechniqueslimitthe
interleavingspaceexplorationofthegeneratedteststointerleav-
ingsthatmatchpredefinedpatternsofconcurrencyfaults,likedata
67
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:47:06 UTC from IEEE Xplore.  Restrictions apply. Effectiveness and Challenges in Generating Concurrent Tests for Thread-Safe Classes ASE ’18, September 3–7, 2018, Montpellier, France
Table 2: Interleaving Explorers and Thread-safety Oracles
Interleaving Implicit Oracle Internal Oracle
Explorer (exceptions, endless hang) (data races, atomicity violations)
Random ConTeGe, CovCon Narada
Selective Omen AutoConTest, Intruder
Exhaustive Ballerina
races and atomicity violations [ 30,38].Randomtechniques ran-
domly explore the space of possible thread interleavings, while
exhaustive techniquesexhaustivelyexploreallnon-redundantinter-
leavings[ 25,34,63].Redundantinterleavingsareoftenidentified
with partial order reduction techniques [ 18]. Both random and sys-
tematic techniques do not restrict the interleavings to be explored,
andthereforearenotspecifictoanyparticularkindofconcurrency
faults,thusdifferingfromselectivetechniques.Bothrandomand
exhaustivetechniqueshardlyscaletoconcurrenttestswithhuge
interleaving spaces, since exhaustive techniques are computation-
allyexpensive,andrandomtechniqueshavealowprobabilityof
detecting concurrency faults [ 38]. In contrast, selective techniques
aremoreefficient,becausetheyfocusonasmallportionoftheinter-
leavingspace[ 38].ConTeGe,CovConandNaradarelyonrandom
interleavingexplorers.Ballerinareliesontwodifferentexhaus-
tiveinterleavingexplorers.AutoConTest,OmenandIntruder
relyonselectivetechniques,AssetFuzzer[ 30],iGoodLock[ 28],
and CTrigger [38], respectively.
Thread safety oracles are either implicit or internal. Implicit ora-
clesreportaconcurrencyfaultifanexploredthread-interleaving
manifests an “obvious” and visible oracle violation like runtime
exceptionsorendlesshangs[ 4].Implicitoraclescannotdetectfaults
thatmanifestaslogicalerrors(wrongoutput). Internaloracles de-
tect faults by monitoring the internal program states. For example,
theyreportanoracleviolationifanexploredthreadinterleaving
matches a predefined pattern of concurrency faults [ 67], regardless
ofobservingaruntimeexceptionoranendlesshang.Thus,internal
oracles can both fail to detect the presence of faults (false nega-
tives),andsignalthepresenceofanomaliesthatarenotfaults(false
positives) [ 67]. ConTeGe, CovCon, Omen and Ballerina rely on
implicit oracles, while the other techniques on internal oracles.
Generating tests and exploring interleaving are two orthogonal
problems.Assuch,inprinciple,thetestsgeneratedwhitanytestgeneratorcouldbeanalyzedbyanyinterleavingexplorer[
12,35,
40].Inpractice,thisisnotalwaysthecase.Forexample,AutoCon-
Test generates concurrent tests with tens of method calls in the
concurrentsuffixes.Thismaybeincompatiblewithbothexhaustiveandrandominterleavingexplorer,sinceexhaustiveexplorationmaybecometooexpensive,whilerandomexplorationmaybeineffective
as the probability of triggering faulty-interleavings decreases with
the increase of test length [38].
4 EXPERIMENTS
In this section, we describe the experiments that we designed
to evaluate and compare the six openly-accessible state-of-the-art concurrent test generators that we selected as a result of our
analysisoftheliterature.Thegoaloftheexperimentsistoevaluate
andcompare howeffectivelyand efficientlythese generatorsfind
concurrency faults.Table 3: JaConTeBe Benchmark [31]
Code base (label) # Subjects Description
Apache DBCP (dbcp) 4 Database connection pool
Apache Derby (derby) 5 Relational databaseApache Groovy (groovy) 6 Dynamic language for JVMOpenJDK (jdk) 20 Java Development Kit
Apache Log4j (log4j) 5 Logging library
Apache Lucene (lucene) 2 Search library
Apache Pool (pool) 5 Object-pooling API
4.1 Tool Selection
We experimented with all state-of-the-art techniques for which
there exists a publicly available tool: ConTeGe, CovCon, Auto-ConTest, Omen, Narada and Intruder. We were not able to
experimentwithBallerina,ConSuiteandMinionbecausethe
toolswerenotpublicly availableat thetime ofconducting theex-
periments.1The empirical comparison of the tools is facilitated by
the fact that all tools generate tests for programs written in Java.
We experimented ConTeGe and CovCon both with the default
configurationthatusesstresstestingasinterleavingexplorerand
withtheconfigurationthatusesthe exhaustive interleavingexplorer
JPF [25]. Such configuration is supported by both tools, and we
indicate the variants of ConTeGe and CovCon that rely on JPF as
ConTeGeJPF and CovConJPF, respectively.
4.2 Subject Selection
The criteria for selecting subjects play a crucial role in evalu-
ating and comparing testing techniques [ 5,21,65]. Most of the
experiments reported in the surveyed papers refer to previously
published papersas abasis for selectingsubjects. Intheir seminal
papers, both Pradel and Nistor propose an interesting set of experi-
ments,butdonotdiscussthecriteriaforselectingtheexperimental
subjects[ 35,40].Inthecontextofsequentialtestgeneration,Fraser
and Arcuri argue that “if the subject selection is unclear, in principle
it could mean that the presented set of classes represents the entire
set of classes on which the particular tool was ever tried, but it could
also mean it is a subset on which the technique performs particularly
well”, andsuggest to randomlysample open-source codereposito-
riesinordertoavoidbiases[ 21].FraserandArcuri’sapproachis
suitabletoevaluatetestgeneratortechniqueswithrespecttocov-
erage, since this does not require that the selected subjects contain
concurrency faults. The approach is not well suited for evaluating
testgenerationtechniquesthataimtoexposeconcurrencyfaults
quickly, without aiming to maximise coverage, like random-based
andsequential-test-basedtechniques.Ourexperimentsaimstoeval-
uate the different techniques referring to an unbiased benchmark,
thatisabenchmarkofthread-safeclasseswithdocumentedconcur-rencyfaultsselectedwithoutneitherconcurrenttestgenerationnorsomespecifictechniquesinmind.Abenchmarkthatsatisfiesthisre-
quirement is the JaConTeBe benchmark, recently proposed by Lin
et al. [31]. JaConTeBe is composed of 47 concurrency faults from
sevenJavapopularopensourceprojects(seeTable 3),andconsiders
a wide range of concurrency faults types: data races, atomicity vio-
lations,resourceand wait-notify deadlocks.Thereaderscanfind
detaileddescriptionofthesubjectsintheoriginalpaper[ 31]and
1TheMinionwebsite(https://sites.google.com/site/miniontool/)isunderconstruction
at the time of conducting the experiments
68
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:47:06 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Valerio Terragni and Mauro Pezzè
companionwebsite[ 2],wheretheauthorsclaimthatJaConTeBe
isthemostadvisablebenchmarkforevaluatingconcurrencytest-
ingtechniques.Competingbenchmarksareeithercomposedoftoy
concurrentprograms(IBMbenchmarksuite[ 15])ornotspecifically
designedforconcurrencyfaultdetection(JPG[ 56]andDaCapo[ 8]),
thus without documented concurrency faults.
For each of the 47 concurrency faults (subjects), the JaConTeBe
benchmarkprovidesthefollowingartifacts:(i)thebinariesofthe
code base’s buggy version; (ii) the link to the bug report; (iii) the
source code of a manually-written fault-revealing concurrent test;
JaConTeBe was built specifically to evaluate interleaving explo-
ration techniques (Step 3 and 4 in Figure 3), and thus it includes
fault-revealingconcurrenttests.ThesecharacteristicsmakeJaCon-
TeBe an excellent benchmark for our study, since our goal is to
assess the capability of test generators to synthesize concurrenttests (Step 1 and 2 in Figure 3) that can expose the concurrency
faults that can be revealed with manually-written tests.
4.3 Subject and Tool Preparation
Foreachsubject,wecollectedtheinputsofthetools:theclass
undertestandthesetofauxiliaryclasses(seeFigure 3).Weretrieved
the class under test by examining the bug report and the manually-
written test. As discussed in Section 2, theauxiliary classes are
those in which the class under test depends upon. For example, if a
methodmofthe classunder testhas aparameter ofnon-primitive
typeA, thenAis an auxiliary class. Most tools require auxiliary
classesasinputtocreateobjectsoftype Atobeusedasaparameter
for invoking m. We identified a proper set of auxiliary classes by
relyingonthecorrespondingmanually-writtentestsof JaConTeBe.
We set the auxiliary classes as all the classes of the program under
test(excludingtheclassundertestitself)thathavebeenreferenced
to in the manually-written test.
While most of the evaluated tools accept binaries as an input,
CovCon needs the source code, because it instruments the source
code with an eclipse plug-in2. Therefore, we retrieved the relevant
source code from the corresponding code repositories, and we
confirmed that the JaConTeBe tests fail if executed on the source
code. We instrumented the source code of the class under test and
all of its superclasses3.
Thesequential-test-basedtools,Omen,NaradaandIntruder,
require a set of sequential tests in input as they do not perform
Step1ofFigure 3.Wecouldobtainsuchtestsbyextractinghuman-
written sequential tests from public repositories, but it would be
unfairwithrespecttocoverage-basedandrandom-basedtechniques
that generate concurrent tests relying exclusively on automatically
generated codes. To avoid biases, we generated the tests with arandom-based generator of sequential tests, following the exper-
imentsofChoudharyetal.[ 12].WeoptedforRandoop[ 37],the
most popular tool of this type4. An important configuration choice
isthenumberofsequentialtestsforthethreesequential-testbasedtools.Toomanytestscouldintroduceoverheadduringtheanalysis
phase, while too few tests could be insufficient to find the fault.
We addressed this issue by iteratively alternating the execution of
2https://github.com/michaelpradel/ConTeGe/tree/CovCon
3Excluding java.lang.Object .
4WeranRandoopignoringallteststhatdonotinstantiatetheCUTatleastonceby
including the option --include-if-classname-appearsRandoopandofthesequential-testbasedtools.Ateachiteration
we doubled the time budget tand the maximum number of tests k
of Randoop.Wechoseaninitialvalueof t=30secondsandk=50,
as a recent empirical study indicates that Randoop saturates code
coverage in less than 60 seconds [54].
4.4 Evaluation Setup
We executed the eight tools (six techniques and two alternative
configurations) with a time-budget of an hour for each of the 47
JaConTeBesubjects.Werepeatedtheexperimentstentimestocope
withtherandomnessofthechoicesofthetoolswhilegenerating
tests and exploring interleaving spaces. The overall machine time
of the experiments was 156 days. We executed our experiment
onaserverUbuntu16.04.2with64octa-coreCPUsIntel®Xeon®
E5-4650L @ 2.60 GHz and ∼529 GB of RAM.
We evaluated the effectiveness of the tools in terms of bug find-
ing capability, and their efficiency in terms of the time required to
detect the faults, which includes the time of both generating the
testsandexploringtheirinterleavingspaces(Step1to4inFigure 3).
Wedidnotusecoverageasanevaluationcriterion,sinceinthecon-
text of concurrency testing the only pertinent coverage criteria are
those related to thread interleavings [ 32]. Computing the complete
set of dynamically covered interleavings for all the generated tests
would be too computationally expensive. Furthermore, instrument-
ing shared memory accesses to collect the covered interleavings
would introduce delays that could prevent the manifestation of the
fault-revealing interleavings. We do not compare the number oftests generated by each technique, because it can be misleading,
since the length of method call sequences in a concurrent test can
varyalotacrosstechniques.Forexample,AutoConTestgenerates
longsuffixes,whileConTeGeshortones.Moreover,sequential-test-basedtoolsdonotreportalltheconcurrentteststhattheygenerate,
but only those that they deem as fault-revealing.
We manually analyzed the results of each run, to identify un-
successful and successful runs. We checked the relations of all
thread-safetyviolationsreportedbythetoolswiththeconcurrency
faults under analysis, by relying on both the bug report and themanually-written concurrent test. It is important to clarify that
wedonotrequiretheautomated-generatedconcurrenttestsand
thecorrespondingmanually-writtenteststobeidentical,sinceof-
ten the same concurrency fault can be manifested with different
concurrent tests.
4.5 Results
Table4summarises the results of our experiments by providing
fault-finding and execution time data for all subjects. The first and
thirdcolumnspresentthecategoryandtheIDofeachfault,respec-
tively,asdefinedbythecuratorsoftheJaConTeBebenchmark[ 31].
The second column indicates the type of failure that manifests the
fault when executing the manually-written test.
Thesubjectsareorderedbythefirstandsecondcolumns.The
remaining columns show the results for each tool by indicating
four possible outcomes: (i) the tool detects the fault ( /check) with the
number of successful runs that detected the fault, and the average
detectiontimeofsuccessfulruns;(ii)thetoolcrashes( ×);(iii)the
tool does not generate any test ( •); (iv) the tool generates tests but
69
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:47:06 UTC from IEEE Xplore.  Restrictions apply. Effectiveness and Challenges in Generating Concurrent Tests for Thread-Safe Classes ASE ’18, September 3–7, 2018, Montpellier, France
Table 4: Summary of Fault-finding Results for Each Tool and JaConTeBe Fault
Fault Category Failure Type Fault ID ConTeGe ConTeGeJPF AutoConTest CovCon CovConJPF Omen Narada Intruder
inconsistent synch.endless loop dbcp4 −− ×−− − − −
logic pool5 − − × − − − − −
race/atomicity
violationsendless loop groovy6 /check(1/10) 231 sec. −×−− −− −
runtime
exceptiondbcp3 −−× −− •• •
derby3 ••• •• •• •
groovy1 ••× •• −− −
groovy3 −−• −− −− −
groovy4 −−• −− −× ×
jdk6_3 −− /check(10/10) 58 sec /check(10/10) 54 sec /check(7/10) 312 sec −− /check(10/10) 307 sec
jdk6_4 /check(8/10) 710 sec /check(10/10) 59 sec × /check(10/10) 75 sec /check(2/10) 300 sec −× ×
jdk6_13 /check(1/10) 2,617 sec /check(3/10) 1,092 sec × /check(6/10) 916 sec /check(3/10) 1,549 sec −× ×
jdk7_3 −−− − −− −
log4j_3 /check(10/10) 22 sec /check(10/10) 15 sec /check(10/10) 37 sec /check(10/10) 58 sec /check(10/10) 22 sec −− −
pool1 −−× −− −− −
logicgroovy5 − −− − − −× ×
jdk6_1 − −• − − •• •
jdk6_2 − −− − − −− −
jdk6_5 − −− − − −− −
jdk6_14 − −− − − −− −
jdk7_1 − −− − − −− −
jdk7_6 − −− − − −− −
log4j_1 − −× − − − /check(10/10) 35 sec. −
resource
deadlockendless hangdbcp1 −− •−− • • •
dbcp2 −− •−− • • •
derby1 •• ••• • • •
derby2 •• ••• • • •
derby4 •• ••• • • •
derby5 −− −−− − − −
groovy2 −− −−− × × ×
jdk6_6 −− −−− − − −
jdk6_7 −− • /check(3/10) 17 sec. −• • •
jdk6_8 −− −−− − − −
jdk6_10 /check(2/10) 321 sec. − • /check(5/10) 69 sec. −• • •
jdk6_11 −− ×−− − − −
jdk6_12 −− −−− − − −
jdk7_2 −− −−− − − −
jdk7_4 −− −−− − − −
log4j_2 −− −−− − − −
wait-notify
deadlockendless hangjdk6_9 −− −−− − − −
jdk7_5 −− −−− − − −
log4j_4 −− −−− − − −
log4j_5 −− −−− − − −
lucene1 −− •−− − − −
lucene2 −− •−− − − −
pool2 −− −−− − − −
pool3 −− −−− − − −
pool4 −− −−− − − −
summary (# faults detected, average time) (5, 780 sec) (3, 388 sec) (2, 48 sec) (6, 198 sec) (4, 546 sec) (0, −) (1, 35 sec) (1, 307 sec)
does not detect the fault ( −); The last row in Table 4summarize
theresultsforeachtoolasnumberofdetectedfaultsandaverage
detection time.
As discussed in Section 3.4, the surveyed techniques rely on
different interleaving explorers and thread-safety oracles to detect
faults of specific categories and failure types. In Table 4, a gray
backgroundindicatesthefaultsthatarenotdetectablewithsome
tools by either the interleaving explorer or the thread-safety oracle
adopted by the tool. CovCon, ConTeGe, CovConJPF and Con-
TeGeJPFuseinterleavingexplorersthatarenotlimitedtoanyfault
category, thus they can potentially detect all faults in JaConTeBe,
butrelyonimplicitoraclesthatcandetectonlyfaultsthatmanifest
exceptionsorendlesshangsastypeoffailure.Therefore,thesetools
coulddetectatmost ∼80%oftheJaConTeBefaults.Omencandetect
only "resource deadlocks" faults and "endless hang" failures, which
amountof34%oftheJaConTeBefaults.AutoConTest,Narada
andIntruderrelyoninternaloracles,whichdonotimposeany
restrictionsonthefailuretype,butrelyoninterleavingexplorers
andoraclesthatarelimitedto"race/atomicity"faults,whichamount
of∼43%oftheJaConTeBefaults.Weexecutedalltoolsonallthe
subjects(eventhecombinationswiththegraybackground),since
someconcurrencyfaultscouldmanifestwithfailuretypesdifferent
from the ones specified in the JaConTeBe benchmark.
Effectiveness. ConTeGedetectsfivefaultsthatbelongtotwo
differentcategories,andthatleadtofailuresofthreetypes.Itdetects
groovy6, which is not detected by any other tools. ConTeGeJPFdetects three of the faults detected with ConTeGe. AutoConTest
detects two atomicity violations. CovCon detects six faults of two
categories and two failure types, detecting more faults than any
othertool.CovConJPFdetectsfourofthefaultsdetectedbyCov-
Con. Omen does not detect any fault. Narada detects one fault
log4j_1, which is not detected by any other tool. Intruder detects
the fault log6_3, which is also detected by three other tools. Auto-
ConTest, Narada and Intruder detect the faults in all the ten
runs,showingstabilitywithrespecttonon-determinismcomparing
withtheothertools,whichhavealowerfaultdetectionrate.Itis
worthmentioningthat,forsomefaults,AutoConTestcrashesdue
toincompatibilityissueswiththeinstrumentationframeworkused
to compute the coverage. For subjects dbcp1,dbcp2,derby1,derby2
andderby4, most of the tools (including Randoop) fail to generate
anyteststhatsuccessfullyinstantiatetheclassundertest,which
requires a complex method call sequence.
Efficiency. The overall detection time varies from 15 seconds
for subject log4j_3to 2,617 seconds for jdk6_13. CovCon is the
fastestamongthefourtoolsthatdetectedatleastthreefaults,with
anaveragedetectiontimeof198seconds(seelastrowinTable 4),
followed by ConTeGeJPF (388 seconds), CovConJPF (546 seconds)
andConTeGe(780seconds).The readersshouldnotice thatCon-
TeGeJPF and CovConJPFdetect thefaults faster thanthe original
configurations,despitethefactthatJPFiscomputationallymoreex-pensivethantherandominterleavingexplorationof ConTeGeand
CovCon.ThisisbecauseConTeGeJPFandCovConJPFexplored
70
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:47:06 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Valerio Terragni and Mauro Pezzè
Table 5: Common Issues and Their Distribution Across The 47 Subjects
Problems Percentage Faults IDs
Invalid assumptions (1, 2, 3) 40.42% dbcp{1-2-3}, derby{2-4-5}, groovy{1-2-6}, log4j{2-4-5}, lucene1, jdk6_{7-8-9-12}, jdk7_{2-4}
|-Invalid assumption 1 (two threads only) 12.77% groovy{1-2}, log4j{2-4-5}, lucene1
|-Invalid assumption 2 (one shared object only) 25.53% dbcp{1-2-3}, derby{2-4-5}, groovy6, jdk6_7, jdk7_4, log4_2|-Invalid assumption 3 (no static invocations) 17.02% dbcp3, groovy6, jdk6_{8-9-12}, jdk7_{2-4}, log4_2
Environmental dependencies 25.53% derby1, groovy{1-2-3}, jdk6_{2-9-10-12}, jdk7_{1,2}, lucene{1-2}Inadequacy for wait-notify 19.15% jdk6_9, jdk7_5, log4j{4-5}, lucene{1-2}, pool{2-3-4}
theinterleavingspaceoffewertestsbeforeexposingthefault.This
suggests that the random interleaving exploration of ConTeGe
and CovCon easily miss failure-inducing interleavings even if the
generated test can manifest one.
Alltoolsreportsomethread-safetyviolationsunrelatedtothe
faultsunderanalysis,whichmightormightnotbetrueconcurrency
faults.ConTeGe,CovCon,ConTeGeJPFandCovConJPFreport
aunrelated ConcurrentModificationException forjdk6_7,jdk6_10,
andjdk6_11. Intruder and AutoConTest detect the jdk6_3fault,
and also report unrelated atomicity violations when analyzing this
subject. Omen detect an unrelated deadlock for jdk6_4.
Automated concurrent test generators find 17%of the JaConTeBe
faults, and none of them alone finds more than 13%of the faults. The
average fault detection time ranges from 15to2,617seconds.
5 RESULTS ANALYSIS AND DISCUSSION
Theexperimentalresultsthatwediscussedintheprevioussec-
tionindicatelimitedcomplementarityamongtheconcurrenttest
generators:halfofthedetectedfaultsarerevealedbyatleastfour
differenttools.Theresultsalsoindicatesomeoverallincomplete-
nessofthecurrentapproaches,since83%oftheconcurrencyfaults
in the JaConTeBe benchmark are not detected by any tool.
The absence of effective oracles is not the main reason of many
undetected faults. In fact, the combinations of failure types and
fault categories that characterise each subject of the JaConTeBe
benchmark(withtheonlyexceptionof pool5)canberevealedbythe
interleavingexplorerandthread-safetyoracleofatleastthreetools
(see Table 4). This confirms that while automatically generating
effective test oracles is a major challenges faced by sequential test
generators[ 11,23,39,62],generatingoraclesforconcurrenttestsis
less critical [ 33,67]: Lu et al. shown that 70% of concurrency faults
lead to exceptions or hangs [ 33]. Moreover, Yu et al. proved that
internal oracles are effective in detecting those concurrency faults
that do not manifest visible oracle violations [67].
We manually analyzed the results of the experiments to identify
andbetterunderstandwhythetoolsfailtoexposetheconsidered
concurrency faults. We relied on the manually-written tests in
JaConTeBe to learn the characteristics of the concurrent tests that
find faults that are not revealed with automatically generated tests.
Weidentifiedthreemainissuesrelatedtoconcurrenttestgen-
eration: invalid assumptions, environmental dependencies, and
inadequacy for wait-notify , as shown in Table 5. Each subject can
suffer from more than one issue.5.1 Invalid Assumptions
Allsurveyedtechniquesreducethesearchspaceandfacilitate
faultdetection byrelying onsome predefinedassumptions onthe
concurrentteststhattheygenerate:twothreadsonly,oneshared
objectonlyandnostaticinvocations.Ourresultsindicatethatabout
40% of the faults in the JaConTeBe benchmark cannot be revealed
without violating at least one of such assumptions (see Table 5).
Assumption1:exactlytwoconcurrentthreads. Allsurveyed
techniques generate concurrent tests with exactly two concurrent
suffixes, following the Lu et al.’s study that shows that 96% of con-
currency faults can be manifested by enforcing a certain partialorder between two concurrent threads only [
33]. By inspecting
the 47 failure-inducing tests in the JaConTeBe benchmark, we
observe that 70.21% spawn two concurrent threads only. This is an
under-approximation,becausesometestsaredesignedaccording
to a stress-test methodology, which spawns many identical threads
forincreasingthechancetotriggerafault-revealinginterleaving.
Thus, some of the faults revealed with tests with more than two
concurrentthreadsmaybealsorevealedwithtestswithonlytwo
concurrentthreads.Werefinedthestudybymanuallyidentifying
thefault-revealingteststhatadoptastress-testmethodology,and
concludedthat12.77%ofthefaultscannotberevealedwithtests
with two concurrent threads only. It is fairly easy to modify the
toolsforgeneratingconcurrenttestswithanarbitrarynumberof
suffixes,totargetalsofaultsthatcanberevealedonlywithmore
thantwoconcurrentthreads,butitmaydramaticallyimpactonthe
performance and effectiveness of the tools.
Assumption 2: at most a shared object instance. Most sur-
veyed techniques generate concurrent tests that access from multi-
plethreadsonlyonesharedobjectoftheclassundertest,according
to the intuition that accessing a single shared object is enough
totriggerfaultsrelatedtoconcurrentaccessestosharedmemory
locations5. However, in some cases two (or more) object instances,
althoughdistinct,mayaccessthesamememorylocations,andthus
they could trigger concurrency failures. Such failures cannot beexposed with tests that instantiate a single shared object. For in-
stance,in thefault-revealingconcurrenttest ofthesubject derby5
in Figure 4, the concurrent execution of the suffixes leads to a con-
currency failure even if the two suffixes do not access the same
sharedobject,buttwodifferentobjects( baseContainerHandle and
storedPage ). This happens because the setter methods in the se-
quentialprefixmutuallyseteachotherreferences(line4and5in
Figure4). By inspecting the JaConTeBe benchmark, we observe
that Assumption 2 is violated in 25.53% faults.
5OnlyConTeGeandConTeGeJPFgenerateteststhataccessmorethanoneshared
object.
71
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:47:06 UTC from IEEE Xplore.  Restrictions apply. Effectiveness and Challenges in Generating Concurrent Tests for Thread-Safe Classes ASE ’18, September 3–7, 2018, Montpellier, France
1// Sequential Prefix
2... // omitted for brevity
3...
4storedPage.setExclusive(baseContainerHandle);
5baseContainerHandle.addObserver(storedPage);
6
7newThread(new Runnable() {
8 public void run() {
9 baseContainerHandle.close(); // Suffix 1
10}}).start();
11
12 newThread(new Runnable() {
13 public void run() {
14 storedPage.releaseExclusive(); // Suffix 2
15}}).start();
Figure 4: Manually-written test of the subject derby5, show-
ing the case of two shared variables.
Assumption 3: only non-static methods. All techniques but
ConTeGe and ConTeGeJPF generate concurrent tests that invoke
non-staticmethodsofsharedobjectinstancesonly,followingthein-
tuition that most concurrency faults derive from incorrect accesses
to dynamic instances. However, some JaConTeBe concurrencyfaults are exposed only with concurrent tests that either invoke
staticmethodsoraccesspublicstaticfields.ByinspectingtheJa-
ConTeBe benchmark, we observe that Assumption 3 is violatedin 17.02% faults. Only ConTeGe detects the fault
groovy6, which
requires the invoking of static methods (see Table 4).
5.2 Environmental Dependencies
A major challenge in generating both sequential and concurrent
tests is to properly instantiate environmental dependencies to suit-
ably exercise method call sequences, for instance, create certain
files or database connections. For example, the fault-revealing con-
current test for subject jdk6_6in Figure 5requires a specific folder
structure in the file system. Classic tools for generating sequential
testssimulatedependenciesonfilesanddatabaseconnectionswith
mocking techniques [ 3,58]. Combining concurrent test generators
with such techniques may address the environment dependency
issues that account for 25.53% of the JaConTeBe faults.
5.3 Inadequacy for Wait-Notify
A relevantaspect of shared memorysynchronization is theuse
ofsharedobjectstopassmessages:threadsblocktheirexecution
waitingto receive messagesfromother threads. Thismechanism is
implementedwithprimitives wait(), notify() andnotifyAll() in
Java6andwait(), signal() andbroadcast() inC++.
None of the surveyed techniques supports such mechanism,
and therefore none generates concurrency tests that can expose
failuresinvolvingtheexecutionof waitandnotify.Thetwo-step
concurrent test generation approach illustrated in Figure 3and
implemented by all the techniques cannot address waitandnotify
relatedfailures.Thegeneralapproachworksasfollows:Step1gen-eratessingle-threadedmethodcallsequences,andStep2assembles
such sequences in concurrent tests. Step 1 executes each newlygenerated sequence, and discards those that either throw excep-tions or hang indefinitely. Such sequences are neither extendednor used to assemble new concurrent tests since they are likely
6From Java 1.5 the mechanism is also implemented with await(), signal() in
thejava.util.concurrent package.1// Sequential Prefix
2finalString dirA = projectBase + "/base/a";
3finalString dirB = projectBase + "/base/b";
4
5newThread(new Runnable() {
6 public void run() {
7 File file = newFile(dirA); // Suffix 1
8 file.mkdirs();
9}}).start();
10
11 newThread(new Runnable() {
12 public void run() {
13 File file = newFile(dirB); // Suffix 2
14 file.mkdirs();
15}}).start();
Figure 5: Manually-written test of the subject jdk6_2, show-
ing the case of the environmental dependencies problem.
illegal[37].Discardingsequencesthathangindefinitelyonasin-
glethreadraisesanissueinthepresenceof waitinvocations:ifa
method invocation of the class under test puts the executor thread
onanobjectwait,thenthesequentialtestgeneratorcannotcalla
newmethodonadifferentthreadthatwillwakeuptheexecutor,
a time-out exception is raised, and the sequence is discarded. Asa result, all the concurrent tests that contain invocations of
wait
arediscarded.However,theexecutionof waitisessentialtotrigger
wait-notify deadlocks. Wait-notify faults amount to 19% of the
JaConTeBe subjects.
6 THREATS TO VALIDITY
Relevant threats to the validity of the results derive from the
choice of the subjects, the time budget for the experiments, the se-
lection of auxiliary classes and the choice of configuration options.
Choice of the subjects. We refer to faults present in open-
source Java projects, and this may not generalize to all program-
minglanguagesandprogramcharacteristics.Generalisingthere-
sults would require extending the techniques originally defined for
Java programs to a suitable variety of concurrent programming
languages.
Time budget for the experiments. The time budgets of the
experimentmayimpactonthenumberofdetectedfaults:increasing
thetimebudgetmayleadtodetectmorefaults.Thenatureofthe
undetected faults discussed in Table 5and the characteristics of
theevaluatedtechniquesindicatethatformanycasesnotechnique
would detect any additional faults even with an unlimited time
budget. In our experiments, we set the time budget to an hour per
subject, according to the experiments of the most recent of the
surveyed techniques [12].
Selection of auxiliary classes. We select the auxiliary classes
byrelyingonfault-triggeringmanually-writtentestsprovidedin
the JaConTeBe benchmark. The existence of such tests nullifies
theneedofrunningconcurrenttestgenerators,butitisusefulin
theevaluation,sinceitprovidesasetofauxiliaryclassesthatare
sufficient for revealing the fault.
Configurationoptions. Theconfigurationoptionsofthetools
and the setup of the execution environments may impact on theeffectivenessoftheconsideredtools.Wemitigatedthisthreatby
comparingthepossibleconfigurationoptionsandinspectingthe
feedback from the execution environment. We also verified thatthe tools when executed in our environment were able to detect
the concurrency failures reported in the corresponding papers.
72
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:47:06 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Valerio Terragni and Mauro Pezzè
7 RELATED WORK
To our knowledge, this paper presents the first independent
empiricalstudythatevaluates,comparesanddiscussesthestate-of-
the-artgeneratorsofconcurrenttests.Bianchietal.’srecentsurvey
proposes acomprehensive viewof thestate-of-the-art techniques
forconcurrenttesting,anddiscussesthetechniquesbasedonthe
published papers [ 6]. Conversely, this paper focuses on concurrent
testgeneratorsonlyandprovidesimportantadditionalempirical
data. Related empirical studies focus either on techniques that ex-
plore the interleaving spaces of manually-written concurrent tests
or on test generators for sequential programs.
Thomson et al. evaluated schedule bounding techniques for sys-
tematicconcurrencytesting,preparingabenchmarkof52faulty
concurrentprograms[ 61].Linetal.evaluatedinterleavingexplo-
ration techniques (Step 3 and 4 in Figure 3) on the JaConTeBe
benchmark[ 31]giventhemanually-writtentestsininput.Hong
and Kim empirically evaluated detectors of data races [27].
Severalempiricalstudiesevaluatedtestgenerationtechniques
for sequential object-oriented programs [ 13,19,21,66]. Xiao et
al. [66] inspected the issues that limit test generators tools in ob-
taining high structural coverage. They found that the main issues
are(i)dependenciesonexternalmethods,and(ii)findingasuitable
methodsequencetoderivedesiredinputobjectstates.Shamshiri
et al. [54] compared three popular generators of sequential tests:
Randoop[ 37],Evosuite[ 20]andAgitarOne(www.agitar.com)
usingtheDefects4J[ 29]benchmark.Theyfoundthattheoracle
problem remains the major obstacle, as 63.3% of the undetected
faultswerecoveredby automaticallygeneratedtestsatleastonce.
FraserandArcurievaluatedEvosuite[ 20]onasetofrandomlyse-
lectedopensourceprojects[ 21].Unsurprisingly,sinceaconcurrent
testisaconcatenationofsequentialtests,someofthechallenges
faced by concurrent test generators are inherited from those faced
bysequentialtestgenerators.Forexample,theobjectcreationprob-lem in the presence of complex inputs [
66]. The study presented in
thispaperraisesadditionalinsightsandchallengesthatarespecific
to concurrent test generation.
8 CONCLUSION AND FUTURE RESEARCH
DIRECTIONS
In this paper, we surveyed the main state-of-the-art techniques
forgeneratingconcurrenttests,andweempiricallyevaluateand
compare the ones for which we have been able to access a publicly
availabletool.Wesurveyedninetechniquesandcomparedsixof
them, by referring to the 47 JaConTeBe benchmark. Our resultsshowthatoveralltheevaluatedtechniquescandetect17%ofthe
JaConTeBe faults, which is an impressive result, if we consider
thatthefaultsremainedundetectedforyearsinpopularcodebases
(many detected faults are in Java, which runs in over 15 billions
devices).Currenttestgeneratorscouldhaverevealedthefaultsand
avoided their manifestation in the field.
The generation of concurrent tests is a relatively young but
promisingapproach. Ourevaluation resultsindicatea largespace
for improving its overall effectiveness. The analysis of the faults
that current techniques do not detect yields insights into the main
limitations, and indicates future research directions: adaptive con-
figuration, search space reduction, and wait-notify handling.Adaptiveconfiguration. Currentgeneratorsofconcurrenttests
work under some predefined assumptions on the tests being gener-
ated, as discussed in Section 5.1. These assumptions can drastically
reduce the search space, thus improving the efficiency of the tech-
niques, but can also prevent the generation of fault-revealing tests.
Sometoolsallowuserstoenable/disablesomeoftheseassumptionsintheformofconfigurationparameters[
40].However,notoolpro-
videsupporttounderstandtheimpactofdifferentconfigurations
onthefault-detectioncapabilityfor agivenprogramunderanal-
ysis.Interestingresearchdirectionsarebothstudyingtechniques
to automatically identify a proper configuration and defining self-
adaptive interleaving explorers. CovCon and ConTeGe are exam-
ples of toolsthat canbenefit fromself-adaptive strategies.Both of
them execute each test a fixed number of iterations independently
from the nature of the test. The choice of the number of iterations
is important, since too few iterations may miss a fault-revealing
interleaving while too many could waste testing resources. A self-
adaptive strategy could identify an optimal number of iterations
foratestbyapproximatingthenumberofthreadinterleavingsthat
can be manifested when executing the test itself. Intuitively, thenumber of iterations should grow proportionally with respect to
suchnumber.Luetal.’sformulascouldprovideusefulhintsfora
cost-effective way to compute such approximations [32].
Search space reduction. Concurrent test generators explore
a huge space of tests when generating concurrent tests. Given a
classundertest,itoftenexistsamyriadofpossiblecombinationsof
method invocations and input parameters. Generating all possible
testsandexploringtheirinterleavingspaceswithinanaffordable
time-budget remain infeasible. An interesting research directionistodefineapproachesthatidentifymethodsthatcannotleadto
athread-safetyviolationifassembledinthesameconcurrenttest,
beforegeneratingthetests.Generatingconcurrentteststhatinvolve
such methods can be avoided without affecting fault-detection
capabilities.
Wait-notifyhandling. AsdiscussesinSection 5.3,currenttech-
niques cannot generate tests that expose wait-notify concurrency
faults. To size the impact of this limitation, we observe that search-
ingfor.wait() AND .notify() ingithub.com produces ∼60millions
coderesults.Using wait-notify synchronizationprimitivesisnota
matter of code style, since their synchronization behaviour cannot
besimulatedusingothersynchronizationprimitiveslikelocks[ 64].
An important research direction is to update the four-steps frame-
work for concurrent test generation presented in Figure 3. For
instance, by recombining all steps to enable simultaneous test gen-
erationandinterleavingexploration.Testgeneratorscouldgenerate
and execute method call sequences on multiple threads simultane-
ouslysothat,ifa waitisexecutedputtingtheexecutingthreadina
suspended state, another thread can unblock the suspended thread
by generating and executing another method call sequence.
ACKNOWLEDGMENTS
This work is partially supported by the Swiss SNF project AS-
TERIx:AutomaticSystemTEstingofinteRactivesoftwareapplications
(SNF 200021_178742).
73
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:47:06 UTC from IEEE Xplore.  Restrictions apply. Effectiveness and Challenges in Generating Concurrent Tests for Thread-Safe Classes ASE ’18, September 3–7, 2018, Montpellier, France
REFERENCES
[1]2018. Effectivenessand ChallengesinGenerating ConcurrentTestsfor Thread-
safe Classes. http://star.inf.usi.ch/star/software/contest2018/index.htm. (2018).
[2] 2018. JaConTeBe. http://sir.unl.edu/portal/bios/JaConTeBe.php. (2018).
[3]AndreaArcuri,GordonFraser,andJuanPabloGaleotti.2014. AutomatedUnit
TestGenerationforClasseswithEnvironmentDependencies.In Proceedingsof
the International Conference on Automated Software Engineering (ASE ’14). ACM,
79–90.
[4]Earl T. Barr, Mark Harman, Phil McMinn, Muzammil Shahbaz, and Shin Yoo.
2015. TheOracleProbleminSoftwareTesting:ASurvey. IEEETransactionson
Software Engineering 41, 5 (2015), 507–525.
[5]Victor R Basili, Richard W Selby, and David H Hutchens. 1986. Experimentation
in Software Engineering. IEEE Transactions on Software Engineering 7 (1986),
733–743.
[6]Francesco A. Bianchi, Alessandro Margara, and Mauro Pezzè. 2017. A Survey of
Recent Trends in Testing Concurrent Software Systems. IEEE Transactions on
Software Engineering (2017).
[7]Francesco A. Bianchi, Mauro Pezzè, and Valerio Terragni. 2017. Reproducing
ConcurrencyFailuresfrom CrashStacks.In ProceedingsoftheJointMeetingon
Foundations of Software Engineering (ESEC/FSE ’17). ACM, 705–716.
[8]StephenM.Blackburn,RobinGarner,ChrisHoffmann,AsjadM.Khang,KathrynS.
McKinley, Rotem Bentzur, Amer Diwan, Daniel Feinberg, Daniel Frampton,
SamuelZ.Guyer,MartinHirzel,AntonyHosking,MariaJump,HanLee,J.EliotB.
Moss,AashishPhansalkar,DarkoStefanović,ThomasVanDrunen,Danielvon
Dincklage,andBenWiedermann.2006. TheDaCapoBenchmarks:JavaBench-
markingDevelopmentandAnalysis.In ProceedingsoftheConferenceonObject-
Oriented Programming Systems and Applications (OOPSLA ’06). ACM, 169–190.
[9]SebastianBurckhardt,ChrisDern,MadanlalMusuvathi,andRoyTan.2010. Line-
up: A Complete and Automatic Linearizability Checker. In Proceedings of the
Conference on Programming Language Design and Implementation (PLDI ’10).
ACM, 330–340.
[10]Yan Cai, Shangru Wu, and W. K. Chan. 2014. ConLock: A Constraint-based
Approach to Dynamic Checking on Deadlocks in Multithreaded Programs. In
Proceedings of the International Conference on Software Engineering (ICSE ’14).
ACM, 491–502.
[11]Antonio Carzaniga, Alberto Goffi, Alessandra Gorla, Andrea Mattavelli, and
Mauro Pezzè. 2014. Cross-checking Oracles from Intrinsic Software Redundancy.
InProceedings of the International Conference on Software Engineering (ICSE ’14).
ACM, 931–942.
[12]Ankit Choudhary, Shan Lu, and Michael Pradel. 2017. Efficient Detection of
ThreadSafetyViolationsviaCoverage-GuidedGenerationofConcurrentTests.
InProceedings of the International Conference on Software Engineering (ICSE ’17).
IEEE Computer Society, 266–277.
[13]Shauvik Roy Choudhary, Alessandra Gorla, and Alessandro Orso. 2015. Auto-
matedTestInputGenerationforAndroid:AreWeThereYet?.In Proceedingsof
theInternationalConferenceonAutomatedSoftwareEngineering(ASE’16).IEEE
Computer Society, 429–440.
[14]Dongdong Deng, Wei Zhang, and Shan Lu. 2013. Efficient Concurrency-bug
Detection Across Inputs. In Proceedings of the Conference on Object-Oriented
Programming Systems and Applications (OOPSLA ’13). ACM, 785–802.
[15]Yaniv Eytani, Klaus Havelund, Scott D Stoller, and Shmuel Ur. 2007. Towards
aFrameworkandaBenchmarkforTestingToolsforMulti-threadedPrograms.
Concurrency and Computation: Practice and Experience 19, 3 (2007), 267–279.
[16]Azadeh Farzan, Andreas Holzer, Niloofar Razavi, and Helmut Veith. 2013.
Con2colic Testing. In Proceedings of the ACM SIGSOFT International Symposium
on Foundations of Software Engineering (FSE ’13). ACM, 37–47.
[17]Cormac Flanagan and Stephen N. Freund. 2004. Atomizer: A Dynamic Atom-
icityCheckerforMultithreadedPrograms.In ProceedingsoftheSymposiumon
Principles of Programming Languages (POPL ’04). ACM, 256–267.
[18]Cormac Flanagan and Patrice Godefroid. 2005. Dynamic Partial-order Reduction
forModelCheckingSoftware.In ProceedingsoftheSymposiumonPrinciplesof
Programming Languages (POPL ’05). ACM, 110–121.
[19]Gordon Fraser and Andrea Arcuri. 2013. EvoSuite: On the Challenges of Test
Case Generation in the Real World. In Proceedings of the International Conference
onSoftwareTesting,VerificationandValidation(ICST’13).IEEEComputerSociety,
362–369.
[20]Gordon Fraser and Andrea Arcuri. 2013. Whole Test Suite Generation. IEEE
Transactions on Software Engineering 39, 2 (2013), 276–291.
[21]Gordon Fraser and Andrea Arcuri. 2014. A Large-Scale Evaluation of Automated
Unit Test Generation Using EvoSuite. ACM Transactions on Software Engineering
and Methodology 24, 2, Article 8 (Dec. 2014), 42 pages.
[22]Brian Goetz and Tim Peierls. 2006. Java Concurrency in Practice. Pearson Educa-
tion.
[23]AlbertoGoffi,AlessandraGorla,MichaelD.Ernst,andMauroPezzè.2016. Au-
tomaticGenerationofOraclesforExceptionalBehaviors.In Proceedingsofthe
International Symposium on Software Testing and Analysis (ISSTA ’16). ACM,
213–224.[24]Shengjian Guo, Markus Kusano, Chao Wang, Zijiang Yang, and Aarti Gupta.2015. Assertion Guided Symbolic Execution of Multithreaded Programs. InProceedings of the ACM SIGSOFT International Symposium on Foundations of
Software Engineering (FSE ’13). ACM, 854–865.
[25]Klaus Havelund and Thomas Pressburger. 2000. Model Checking Java Programs
Using Java Pathfinder. International Journal on Software Tools for Technology
Transfer2, 4 (2000), 366–381.
[26]Maurice P. Herlihy and Jeannette M. Wing. 1990. Linearizability: A Correctness
ConditionforConcurrentObjects. ACMTransactionsonProgrammingLanguages
and Systems 12, 3 (1990), 463–492.
[27]ShinHongandMoonzooKim.2015. ASurveyofRaceBugDetectionTechniques
for MultithreadedProgrammes. SoftwareTesting,Verification andReliability 25,
3 (2015), 191–217.
[28]PallaviJoshi,Chang-SeoPark,KoushikSen,andMayurNaik.2009.ARandomized
Dynamic Program Analysis Technique for Detecting Real Deadlocks. In Proceed-
ings of the Conference on Programming Language Design and Implementation
(PLDI ’09). ACM, 110–120.
[29]René Just, Darioush Jalali, and Michael D. Ernst. 2014. Defects4J: A Databaseof Existing Faults to Enable Controlled Testing Studies for Java Programs. InProceedings of the International Symposium on Software Testing and Analysis
(ISSTA ’14). ACM, 437–440.
[30]Zhifeng Lai, S. C. Cheung, and W. K. Chan. 2010. Detecting Atomic-set Seri-
alizabilityViolationsinMultithreadedProgramsThroughActiveRandomized
Testing. In Proceedings of the International Conference on Software Engineering
(ICSE ’10). ACM, 235–244.
[31]Ziyi Lin, Darko Marinov, Hao Zhong, Yuting Chen, and Jianjun Zhao. 2015.
JaConTeBe: A Benchmark Suite of Real-World Java Concurrency Bugs (T). In
ProceedingsoftheInternationalConferenceonAutomatedSoftwareEngineering
(ASE ’15). IEEE Computer Society, 178–189.
[32]Shan Lu, Weihang Jiang, and Yuanyuan Zhou. 2007. A Study of Interleaving
CoverageCriteria.In ProceedingsoftheEuropeanSoftwareEngineeringConference
heldjointlywiththeACMSIGSOFTInternationalSymposiumonFoundationsof
Software Engineering (ESEC-FSE companion ’07). ACM, 533–536.
[33]ShanLu,SoyeonPark,EunsooSeo,andYuanyuanZhou.2008. Learningfrom
Mistakes:AComprehensiveStudyonRealWorldConcurrencyBugCharacter-
istics.InProceedingsoftheInternationalConferenceonArchitecturalSupportfor
Programming Languages and Operating Systems (ASPLOS ’08). ACM, 329–339.
[34]Madanlal Musuvathi, Shaz Qadeer, Thomas Ball, Gerard Basler, Pira-
manayagam Arumuga Nainar, and Iulian Neamtiu. 2008. Finding and Reproduc-
ing Heisenbugs in Concurrent Programs. In Proceedings of the Symposium on
OperatingSystemsDesignandImplementation(OSDI’08).USENIXAssociation,
267–280.
[35]Adrian Nistor, Qingzhou Luo, Michael Pradel, Thomas R. Gross, and Darko
Marinov. 2012. BALLERINA:AutomaticGenerationand ClusteringofEfficient
RandomUnitTestsforMultithreadedCode.In ProceedingsoftheInternational
Conference on Software Engineering (ICSE ’12). IEEE Computer Society, 727–737.
[36]SemihOkurandDannyDig.2012. HowDoDevelopersUseParallelLibraries?.
InProceedingsoftheACMSIGSOFTInternationalSymposiumonFoundationsof
Software Engineering (FSE ’12). ACM, 54:1–54:11.
[37]Carlos Pacheco, Shuvendu K. Lahiri, Michael D. Ernst, and Thomas Ball. 2007.
Feedback-Directed RandomTest Generation. In Proceedings ofthe International
Conference on Software Engineering (ICSE ’07). ACM, 75–84.
[38]Soyeon Park, Shan Lu, and Yuanyuan Zhou. 2009. CTrigger: Exposing Atomicity
Violation Bugs from Their Hiding Places. In Proceedings of the International
ConferenceonArchitecturalSupportforProgrammingLanguagesandOperating
Systems (ASPLOS ’09). ACM, 25–36.
[39]Mauro Pezzè and Cheng Zhang. 2015. Automated Test Oracles: A Survey. In
Advances in Computers. Vol. 95. Elsevier, 1–48.
[40]MichaelPradelandThomasR.Gross.2012.FullyAutomaticandPreciseDetection
of Thread Safety Violations. In Proceedings of the Conference on Programming
Language Design and Implementation (PLDI ’12). ACM, 521–530.
[41]Michael Pradel and Thomas R. Gross. 2013. Automatic Testing of Sequential
and Concurrent Substitutability. In Proceedings of the International Conference on
Software Engineering (ICSE ’13). IEEE Computer Society, 282–291.
[42]Michael Pradel, Markus Huggler, and Thomas R. Gross. 2014. Performance
Regression Testing of Concurrent Classes. In Proceedings of the International
Symposium on Software Testing and Analysis (ISSTA 2014). ACM, 13–25.
[43]Ganesan Ramalingam. 2000. Context-sensitive Synchronization-sensitive Analy-
sisisUndecidable. ACMTransactionsonProgrammingLanguagesandSystems 22,
2 (2000), 416–430.
[44]NiloofarRazavi,FranjoIvančić,VineetKahlon,andAartiGupta.2012.Concurrent
Test Generation Using Concolic Multi-trace Analysis. In Asian Symposium on
Programming Languages and Systems (ASPLS ’10). Springer, 239–255.
[45]Malavika Samak and Murali Krishna Ramanathan. 2014. Multithreaded Test
Synthesis for Deadlock Detection. In Proceedings of the Conference on Object-
Oriented Programming Systems and Applications (OOPSLA ’14). ACM, 473–489.
[46]Malavika Samak and Murali Krishna Ramanathan. 2014. Omen+: A Precise
DynamicDeadlockDetectorforMultithreadedJavaLibraries.In Proceedingsofthe
74
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:47:06 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Valerio Terragni and Mauro Pezzè
ACM SIGSOFT International Symposium on Foundations of Software Engineering.
ACM, 735–738.
[47]Malavika Samak and Murali Krishna Ramanathan. 2014. Trace Driven Dynamic
Deadlock Detection and Reproduction. In Proceedings of the Symposium on Prin-
ciples and Practice of Parallel Programming (PPoPP ’14). ACM, 29–42.
[48]MalavikaSamakandMuraliKrishnaRamanathan.2015. SynthesizingTestsfor
DetectingAtomicityViolations.In ProceedingsoftheACMSIGSOFTInternational
Symposium on Foundations of Software Engineering (FSE ’15). ACM.
[49]Malavika Samak, Murali Krishna Ramanathan, and Suresh Jagannathan. 2015.
SynthesizingRacyTests.In ProceedingsoftheConferenceonProgrammingLan-
guage Design and Implementation (PLDI ’15). ACM, 175–185.
[50]Malavika Samak, Omer Tripp, and Murali Krishna Ramanathan. 2016. Directed
Synthesis of Failing Concurrent Executions. In Proceedings of the Conference
onObject-OrientedProgrammingSystemsandApplications(OOPSLA’16).ACM,
430–446.
[51]StefanSavage,MichaelBurrows,GregNelson,PatrickSobalvarro,andThomasE.
Anderson. 1997. Eraser: A Dynamic Data Race Detector for Multithreaded
Programs. ACM Transactions on Computer Systems 15, 4 (1997), 391–411.
[52]Jochen Schimmel, Korbinian Molitorisz, Ali Jannesari, and Walter F Tichy. 2013.
AutomaticGenerationofParallelUnitTests.In ProceedingsoftheInternational
Workshop on Automation of Software Test (AST ’10). IEEE Computer Society,
40–46.
[53]Jochen Schimmel, Korbinian Molitorisz, Ali Jannesari, and Walter F Tichy. 2015.
CombiningUnitTestsforDataRaceDetection.In ProceedingsoftheInternational
Workshop on Automation of Software Test (AST ’15). IEEE Computer Society,
43–47.
[54]SinaShamshiri,RenéJust,JoséMiguelRojas,GordonFraser,PhilMcMinn,and
AndreaArcuri.2015. DoAutomaticallyGeneratedUnitTestsFindRealFaults?
AnEmpiricalStudyofEffectivenessandChallenges.In ProceedingsoftheInterna-
tional Conference on Automated Software Engineering (ASE ’15). IEEE Computer
Society, 201–211.
[55]ElenaSherman,MatthewB.Dwyer,andSebastianElbaum.2009.Saturation-based
TestingofConcurrentPrograms.In ProceedingsoftheACMSIGSOFTInternational
Symposium on Foundations of Software Engineering (FSE ’09). ACM, 53–62.
[56]L.A.Smith,J.M.Bull,andJ.Obdrizalek.2001. AParallelJavaGrandeBenchmark
Suite. InSupercomputing, ACM/IEEE 2001 Conference. 6–6.[57]Sebastian Steenbuck and Gordon Fraser. 2013. Generating Unit Tests for Concur-
rentClasses.In ProceedingsoftheInternationalConferenceonSoftwareTesting,
Verification and Validation (ICST ’13). IEEE Computer Society, 144–153.
[58]Kunal Taneja, Yi Zhang, and Tao Xie. 2010. MODA: Automated Test Generation
forDatabaseApplicationsviaMockObjects.In ProceedingsoftheInternational
Conference on Automated Software Engineering (ASE ’10). ACM, 289–292.
[59]Valerio Terragni and Shing-Chi Cheung. 2016. Coverage-driven Test Code Gen-
eration for Concurrent Classes. In Proceedings of the International Conference on
Software Engineering (ICSE ’16). ACM, 1121–1132.
[60]Valerio Terragni, Shing-Chi Cheung, and Charles Zhang. 2015. RECONTEST:
Effective Regression Testing of Concurrent Programs. In Proceedings of the Inter-
nationalConferenceonSoftwareEngineering(ICSE’15) .IEEEComputerSociety,
246–256.
[61]PaulThomson,AlastairF.Donaldson,andAdamBetts.2014.ConcurrencyTesting
Using Schedule Bounding: An Empirical Study. In Proceedings of the Symposium
on Principles and Practice of Parallel Programming (PPoPP ’14). ACM, 15–28.
[62]Paolo Tonella. 2004. Evolutionary Testing of Classes. In Proceedings of the Inter-
nationalSymposiumonSoftwareTestingandAnalysis(ISSTA’04).ACM,119–128.
[63]Willem Visser, Klaus Havelund, Guillaume Brat, SeungJoon Park, and Flavio
Lerda.2003. ModelCheckingPrograms. AutomatedSoftwareEngineering 10,2
(2003), 203–232.
[64]ChaoWang,RhishikeshLimaye,MalayGanai,andAartiGupta.2010.Trace-Based
SymbolicAnalysisforAtomicityViolations.In ProceedingsoftheInternational
Conference on Tools and Algorithms for Construction and Analysis of Systems
(TACAS ’10). Springer, 328–342.
[65]Claes Wohlin, Per Runeson, Martin Höst, Magnus C Ohlsson, Björn Regnell, and
AndersWesslén.2012. ExperimentationinSoftwareEngineering. SpringerScience
& Business Media.
[66]XushengXiao,TaoXie,NikolaiTillmann,andJonathandeHalleux.2011. Precise
IdentificationofProblemsforStructuralTestGeneration.In Proceedingsofthe
International Conference on Software Engineering (ICSE ’11). ACM, 611–620.
[67]TingtingYu,WitawasSrisa-an,andGreggRothermel.2013. Anempiricalcom-
parison of the fault-detection capabilities of internal oracles. In Proceedings of
the International Symposium on Software Reliability Engineering (ISSRE ’13). IEEE
Computer Society, 11–20.
75
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:47:06 UTC from IEEE Xplore.  Restrictions apply. 