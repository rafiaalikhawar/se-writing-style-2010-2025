Automated Cross-Platform Inconsistency Detection
for Mobile Apps
Mattia Fazzini
Georgia Institute of Technology, USA
mfazzini@cc.gatech.eduAlessandro Orso
Georgia Institute of Technology, USA
orso@cc.gatech.edu
Abstract —Testing of Android apps is particularly challenging
due to the fragmentation of the Android ecosystem in terms of
both devices and operating system versions. Developers must in
fact ensure not only that their apps behave as expected, but
also that the apps’ behavior is consistent across platforms. Tosupport this task, we propose D
IFFDROID , a new technique that
helps developers automatically ﬁnd cross-platform inconsistencies(CPIs) in mobile apps. D
IFFDROID combines input generation
and differential testing to compare the behavior of an app on
different platforms and identify possible inconsistencies. Given an
app, D IFFDROID (1) generates test inputs for the app, (2) runs the
app with these inputs on a reference device and builds a modelof the app behavior, (3) runs the app with the same inputs on aset of other devices, and (4) compares the behavior of the app
on these different devices with the model of its behavior on the
reference device. We implemented D
IFFDROID and performed
an evaluation of our approach on 5benchmarks and over 130
platforms. Our results show that D IFFDROID can identify CPIs
on real apps efﬁciently and with a limited number of falsepositives. D
IFFDROID and our experimental infrastructure are
publicly available.
I. I NTRODUCTION
Testing is a difﬁcult and costly activity in general. When
testing Android apps, the task is further complicated by the
extensive fragmentation of the Android ecosystem. Androidapps must be able to run on a myriad of devices and oper-ating systems; developers are thus faced with the problem ofensuring not only that their apps behave as expected, but alsothat the behavior of the apps is consistent across platforms.Given the large number of possible hardware and softwareconﬁgurations in Android, this makes it extremely difﬁcultand expensive to perform adequate testing of an app [1].
The problem of cross-platform inconsistencies (CPIs)i s
therefore prevalent in the Android environment, where userscan observe failures and unexpected behaviors that are causedby differences between their platform and those on which theapp they are using was tested [2].
To mitigate this problem, and help developers identify
inconsistencies in behavior before an app is released, inthis paper we propose D
IFFDROID , an automated technique
whose goal is to identify CPIs by combining input generation,user interface (UI) modeling, and differential testing. Moreprecisely, D
IFFDROID takes as input an app and performs
four main steps. First, it automatically generates a large setof test inputs for the app. Second, it runs the app with theseinputs on a reference device and builds a UI model of theapp. Third, it runs the app against the same inputs on a largeset of different platforms. Finally, D
IFFDROID compares the
UI models of the app on these different platforms with theUI model of the reference device and reports the differencesidentiﬁed, suitably ranked and visualized, to the app developer.
In order to assess the effectiveness of our approach, we
implemented D
IFFDROID in a tool and performed an empir-
ical evaluation on 5 real-world apps and over 130 different
platforms. In the evaluation, D IFFDROID was able to ﬁnd
96inconsistencies due to differences in the version of the
Android system used or in the screen conﬁguration. Overall,our results show that D
IFFDROID can identify CPIs on real
apps efﬁciently, while generating only a limited number offalse positives. Our implementation of D
IFFDROID and our
experimental infrastructure are publicly available at http://www.cc.gatech.edu/~orso/software/diffdroid.
The main contributions of this paper are:
•A new technique that combines input generation, UI mod-eling, and differential testing to automatically identifycross-platform inconsistencies in the UI of Android apps.
•A publicly available implementation of our technique thatcan be used to replicate our experiments or build on andextend our approach.
•An empirical evaluation, performed on a large number ofplatforms, that provides initial evidence of the effective-ness of our approach.
II. M
OTIV ATING EXAMPLE
To motivate our work we provide an example from a real-
world app called D AILY DOZEN [3]—a diet tracking app that
has been downloaded more than 50,000 times and reviewed by
more than 1,000 users. Figure 1 shows the MainAcitivity
of the app running on a LG G3 device, while Figure 2 shows
the same activity running on a LG Optimus L70 device. Users
can use this activity to track their daily food intake by clickingon the displayed checkbox elements.
Figures 1 and 2 show a CPI for the app. Users can
tick the checkbox element associated with the “CruciferousVegetables” label on a LG G3 device, but they cannot do the
same on an LG Optimus L70, as that checkbox element is not
visible when the app runs on such device. This inconsistencyis caused by a bug in the layout ﬁle associated with theMainAcitivity and is revealed because of the different
978-1-5386-2684-9/17/$31.00 c/circlecopyrt2017 IEEEASE 2017, Urbana-Champaign, IL, USA
T echnical Research308
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:57:18 UTC from IEEE Xplore.  Restrictions apply. Fig. 1: DAILY DOZEN running on LG G3 .
screen conﬁgurations (screen resolution and pixel density) of
the two devices.
Bugs of this type can manifests in one of two ways: (1)
the checkbox element is present on one device, but not onthe other, or (2) the checkbox element is present on bothdevices, but its visual appearance is different. The checkboxelement associated with the “Cruciferous Vegetables” label isan example of the former case. In this case, the differencecan be visually perceived, but it can also be identiﬁed bycomparing the UI hierarchies of the two devices. In fact,the UI hierarchy of the app running on the LG OptimusL70 device does not have a node representing the checkbox
element, while such a node is present in the UI hierarchy of the
app running on the LG G3 device. The rightmost checkbox
element associated with the “Other Vegetables” label is an
example of the latter case. In this case, the difference betweenthe two devices can only be perceived visually, as both nodesare present the UI hierarchies of the two devices. These typesof issues are far from rare because developers tend to use alimited set of devices (when not only one) during developmentand testing. In addition, these inconsistencies are hard to detectbecause this testing process tends to be mostly manual.
Figures 1 and 2 also highlight the challenges in ﬁnding CPIs
on mobile devices. Because mobile devices can have different
screen conﬁgurations, certain differences should not be classi-ﬁed as CPIs. For example, the “Nuts” label is displayed on the
LG G3 device, while it is not displayed on the LG Optimus
L70 device. This difference should not be considered a CPI:
the label is part of a scrollable list, and the former devicesimply accommodates more list items due to its larger screen.
III. T
HEDIFFDROID TECHNIQUE
In this section, we present D IFFDROID , our technique for
detecting CPIs on mobile devices. The basic idea behind
DIFFDROID is to use differential testing to identify such
inconsistencies. Figure 3 provides a high-level overview of our
technique and shows its main phases. Given an app under test
Fig. 2: DAILY DOZEN running on LG Optimus L70.
(AUT ) and a reference device, in its input generation phase,
DIFFDROID dynamically generates inputs with the goal of
testing the app’s functionality. Before providing the generatedinputs to the app, the technique captures the UI state of theapp by storing the tree of its UI hierarchy [4] and taking ascreenshot of its appearance on the device. The technique logs
UI hierarchy trees, screenshots, and generated inputs into the
trace, which is the input to the following phase: the test case
encoding phase. In this phase, our technique suitably analyzes
the inputs together with UI hierarchy trees to generate aplatform-independent test case. While doing so, the techniquealso creates a UI model of the app. The UI model is composed
of a list of window models; and each window model contains
a UI hierarchy tree and corresponding screenshot. We call this
UI model the reference UI model, as it was generated using
the reference device. The test case execution phase takes as
input a set of test devices, executes the test case generated by
the previous phase on the devices, and produces as output aUI model for each device (test UI models). Finally, in the CPI
analysis phase,D
IFFDROID performs a differential analysis to
compare the reference UI model with the test UI models andgenerates a report that contains the detected CPIs. The CPI
report is the output of our technique.
A. Input Generation
The input generation phase aims to test the functionality
of the AUT on a reference device by dynamically generatinginputs and providing them to the app. We describe this phasein Algorithm 1. The algorithm takes as inputs the referencedevice (rd ), the AUT (AUT ), and a timeout (T ), and produces
as output a trace (trace) that contains window models andgenerated inputs. We present the abstrax syntax of the trace
produced by the algorithm in Figure 4.
The algorithm begins with an empty trace (line 2). It then
starts the AUT (S
TART ) on the reference device (line 4) and
subsequently enters its main loop, where it iterates until atimeout is reached.
309
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:57:18 UTC from IEEE Xplore.  Restrictions apply. Reference
Device
App Under
Test
AppInput
Generation
TraceTest DevicesTest Case
Encoding
Reference
UI ModelTest Case
Test Case
Execution
Test
UI Models
CPI
Analysis
CPI Report
Fig. 3: High-level overview of the technique.
Algorithm 1: Input generation.
Input : rd: reference device
AUT : application under test
T: input generation timeout
Output: trace : window models and generated inputs on reference device
1begin
2trace =∅
3t=G ET-CURRENT -TIME()
4 START (rd,AUT )
5 whileT<G ET-ELAPSED -TIME(t)do
6 root =G ET-ROOT (rd)
7 tree =T RA VERSE (root )
8 screenshot =G ET-SCREENSHOT (rd)
9 trace. ADD(WINDOW -MODEL (tree ,screenshot ))
10 input =G ENERATE -INPUT (w1,KEY,w2,SYSTEM ,w3,TOUCH )
11 INJECT (rd,input )
12 trace. ADD(input )
13 returntrace
In the ﬁrst part of the loop iteration (lines 6-9), the algorithm
retrieves the root node of the UI hierarchy (G ET-ROOT ) and
traverses the UI hierarchy (T RA VERSE ) to build a tree repre-
sentation of such hierarchy. Each node in the tree is character-
ized by the following set of properties: node-id, node-type,
left ,right ,top,bottom ,text,checked, enabled, focused ,
selected ,clickable, checkable, focusable, scrollable, and
long -clickable. We selected this set of properties because it is
the minimal set of properties that allows the technique to best
differentiate nodes in the UI hierarchy (see Section III-D). Af-
ter building a tree representation of the UI hierarchy, the algo-rithm captures a screenshot of the AUT (G
ET-SCREENSHOT ).
The algorithm then pairs the tree representation of the UI
hierarchy with the screenshot of the AUT to deﬁne the currentwindow model and adds the model to the trace.
In the second part of the loop iteration (lines 10-12), the
algorithm generates an input (G
ENERATE -INPUT ), provides
the input to the AUT (I NJECT ), and adds the input to the trace.
DIFFDROID generates three types of inputs (K EY,SYSTEM ,
and T OUCH ) using a weighted random distribution, as done in
related work [5]. (It is worth noting that the technique would
also work with a different dynamic input generation approach,
such as [6], [7], [8], [9], [10], [11], [12], [13].) Key inputs are
characterized by the value of the key they are representing;system inputs express a change in the orientation of the deviceor data used to transfer control between components of theAUT; and touch inputs represent clicks or gestures on thedevice. Note that our technique does not currently remove
inputs that do not affect the state of the AUT, but they couldbe discarded using an approach based on delta debugging [14].trace -def ::= traceitems
items ::=window -model -def input- def
::=|window -model -def input- def items
window -model -def ::= window-model tree-def screenshot -def
tree -def ::= treeroot -reference -id∗nodes
nodes ::=node -def|node -def nodes
node -def ::= nodereference -id∗node -props children- ids
node -props ::=node -id†node -type†text†checkable‡
::=clickable‡focusable‡scrollable‡
::=long -clickable‡checked‡enabled‡focused‡
::=selected‡left∗right∗top∗bottom∗
children- ids ::=|reference -id∗children- ids
screenshot- def ::= screenshot image
input-def ::= inputinput-type
input-type ::=key-input-def|system -input-def
::=|touch-input-def
key-input-def ::= keykey-value†
system- input-def ::= systemsystem- input-type system- input-props
system- input-type ::= rotate|data
system- input-props ::=exprs
touch-input-def ::= touch|coords
coords ::=x-coord∗y-coord∗pointer -id∗
::=|x-coord∗y-coord∗pointer -id∗coords
exprs ::=expr|expr exprs
expr ::=boolean |number |string
Fig. 4: Abstract syntax of the generated trace. “∗” indicates that the
value is a number, “†” indicates that the value is a string, and “‡”
indicates that the value is a boolean.
B. Test Case Encoding
The test case encoding phase aims to generate a platform-
independent test case based on the content of the trace createdby the input generation phase. We present this phase in
Algorithm 2.
The algorithm takes as input the trace (trace) generated
by the previous phase of the technique, and it produces two
outputs: a platform independent test case (tc ) and a UI model
of the reference decvice (RUIM ). The algorithm begins with
an empty test case (line 2) and an empty UI model (line 3).
It then processes the content of the trace in its main loop(lines 4-17).
In the ﬁrst part of the loop iteration (lines 5-14), the
algorithm processes window models. If the currently processedwindow model has the same tree representation (S
AME -TREE)
and the same screenshot (S AME -IMAGE ) of a window model
already added into the reference UI model (lines 7- 10),
the model is discarded as superﬂuous. Function S AME -TREE
performs a breadth-ﬁrst traversal of two trees and comparesthe value of the properties of the traversed nodes. If thetwo trees have different structure, or if their nodes havedifferent properties, the algorithm considers the two treesand corresponding window models to be different. Function
310
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:57:18 UTC from IEEE Xplore.  Restrictions apply. Algorithm 2: Test case encoding.
Input : trace : window models and generated inputs on reference device
Output: tc: test case
RUIM : UI model of the reference device
1begin
2tc=∅
3RUIM =∅
4 foreachitem∈trace do
5 ifitem≡WINDOW -MODEL then
6 newModel =T RUE
7 foreachwModel ∈RUIM do
8 ifSAME -TREE(wModel. GET-TREE(),
item. GET-TREE()) and
9 SAME -IMAGE (wModel. GET-SCREENSHOT (),
item. GET-SCREENSHOT ())then
10 newModel =F ALSE
11 ifnewModel == T RUE then
12 RUIM. ADD(item )
13 stmt =
GENERATE -WINDOW -MODEL -STATEMENT (item )
14 tc.ADD(stmt)
15 else
16 stmt =G ENERATE -INPUT -STATEMENT (item )
17 tc.ADD(stmt)
18 returntc,RUIM
SAME -IMAGE compares two screenshots using the complex
wavelet structural similarity (CW-SSIM) index [15], which
can range between zero (different images) and one (similarimages). We motivate the use of this index to compute image
similarity in Section III-D. If two screenshots do not haveCW-SSIM index equal to one, we consider the correspondingwindow models to be different. If a window model is not
redundant (lines 11-14), the algorithm adds the window modelto the reference UI model. It also generates a test casestatement (G
ENERATE -WINDOW -MODEL -STATEMENT ) that
builds a tree representation of the UI hierarchy and takes a
screenshot of the device.
In the second part of the loop iteration (lines 16-17),
the algorithm generates a platform-independent statement
(GENERATE -INPUT -STATEMENT ) that replicates the action
of the input in the trace; it then adds the statement to thetest case. We deﬁne generated statements as being platform-
independent because they can run on any device independently
from the operating system version and the device conﬁguration(e.g., screen size). The algorithm creates platform-independent
statements following the principles presented in our previouswork [16]. Platform-independent test cases allow our tech-nique to collect UI models on many different devices, thusincreasing the likelihood of identifying CPIs.
C. Test Case Execution
The test case execution phase aims to collect UI models
from a set of test devices. This phase takes as inputs the
test case generated by the previous phase of the techniqueand a set of test devices, and it executes the test case onthe set of test devices. The execution is driven by two typesof statements: W
INDOW -MODEL -STATEMENT and I NPUT -
STATEMENT . Statements of the former type traverse the UI
hierarchy of the AUT to build a tree representation of suchhierarchy and capture a screenshot of the AUT. The tree andthe screenshot are paired together to form a window modelof the test device; this window model is then added to the UImodel of the test device. Statements of the latter type providean input to the AUT. These inputs are meant to exercise theAUT on the test device in the same way it was exercised inthe generation phase.
The output of this phase is a mapping between test devices
and corresponding UI models. Generation of UI models for
test devices is amenable to parallelization, as the computationof a UI model for one device is completely independent fromthe computation of the UI model for a different device.
D. CPI Analysis
The CPI analysis phase aims to identify CPIs in the AUT
and is the core of our technique. We present this phase in
Algorithm 3. Algorithm 3 takes as inputs the reference UImodel (RUIM ) and the map of UI models of test devices
(TUIMMap ) generated by the previous phase. The algo-
rithm produces as output a report that lists the identiﬁedCPIs (CPIReport ); this report is also the overall output of
D
IFFDROID .
The algorithm begins with an empty CPI report (line 2),
iterates over each window model (rdModel ) in the reference
UI model (lines 3-44), and compares the window model athand to the corresponding window model (tdModel ) in all test
UI models (lines 5-44). The comparison between a referencewindow model and a test window model is divided into twosteps. The ﬁrst step (lines 6-33) matches nodes from the tree
representing the UI hierarchy of the reference device ( rdTree )
to nodes from the tree representing the UI hierarchy of the
test device (tdTree). The second step (lines 34-44) compares
the visual representation (image) of matched nodes. The ﬁrststep can detect structural CPIs, which consist of missing or
additional nodes. The second step can detect visual CPIs,
which consist of nodes with different visual representations.Considering the motivating example of Section II, the check-box element associated with the “Cruciferous Vegetables”label is an example of a structural CPI, while the rightmostcheckbox element associated with the “Other Vegetables” label
is an example of a visual CPI.
The node mapping process begins by initializing the map-
ping (nodeMappingMap) between nodes in the reference
tree and nodes in test tree to the empty value (line 10).For each node (rdNode ) in the reference tree, the algorithm
then computes a node similarity value (nodeSim ) between
the reference node and each node (tdNode) in the test tree
using function C
OMPUTE -STRUCTURAL -SIMILARITY . This
function computes a value between zero and one that repre-sents the structural similarity of two nodes (see Section III-D1)If the similarity value is greater or equal than a threshold α,
the algorithm stores the similarity value, together with the testnode, in a list (mappedNodeList ). Threshold αis used to
avoid matching nodes that are too dissimilar. The choice ofthe value of αis related to function C
OMPUTE -STRUCTURAL -
SIMILARITY and we describe it in Section III-D1. When the
algorithm has processed all nodes in the test tree, it stores themapping between the reference node and the computed listintonodeMappingMap (line 19).
311
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:57:18 UTC from IEEE Xplore.  Restrictions apply. OncemappedNodeList is computed for all reference
nodes, the algorithm computes the optimal mapping between
reference nodes and test nodes using function F IND-BEST-
MAPPING (line 20). For every reference node, this function
sorts the elements in the mappedNodeList in descending
order based on their node similarity value. The function then
ﬁnds the mappedNodeList containing the element with the
highest node similarity value, maps the reference node asso-ciated with the mappedNodeList to the test node associated
with the element of the list, and marks the reference node asprocessed. Finally, the function removes all occurrences of thetest node from the mappedNodeList associated with other
reference nodes. This process continues until all reference
nodes are processed.
After ﬁnding the best mapping between nodes in the refer-
ence tree and nodes in the test tree (line 20), the algorithm
analyzes the mapping to ﬁnd structural CPIs. The algorithmiterates over each node in the reference tree (lines 21-25) toﬁnd reference nodes that do not have a mapping to a node inthe test tree. If such a node is found, it means that the node ispresent in the AUT while running on the reference device, butit is not present in the AUT while running on the test device. Achallenging aspect for the classiﬁcation of the missing nodes isgiven by the fragmentation of the Android ecosystem. Devicescome with different screen conﬁgurations, and it could benormal that two devices have a different number of nodes intheir UI hierarchies.
Consider again the motivating example of Section II, in
which the M
AINACITVITY displays a list of servings. When
the app is running on the LG G3, the device displays eight
elements. When the app is running on the LG Optimus L70,
conversely, the device displays only seven elements. In thiscase, the eighth element of the list should not be classiﬁedas an inconsistency because the Android system does notrepresent a node in the UI hierarchy if the node is notvisible. For this reason, the algorithm further analyzes the node
using function W
ITHIN -DYNAMICALLY -SIZED -ELEMENT to
determine whether or not the node should be reported as
inconsistency. If the node does not have an ancestor that isscrollable, the node is reported as an inconsistency. Otherwise,if the node (1) has a scrollable ancestor and (2) has itspreceding or subsequent sibling (depending on the position ofthe node in the tree) that is matched to a node that is visibileand it is not at the end of the dynamically sized element, thenthe node is also reported as an inconsistency. In all other cases,the node is not reported as an inconsistency. In case function
W
ITHIN -DYNAMICALLY -SIZED -ELEMENT conﬁrms that the
node is an inconsistency, the node is added to the CPI reportas a structural CPI. Similarly, the algorithm iterates over nodesin the test tree (lines 26-33) to ﬁnd test nodes that do not have
a mapping to a node in the reference tree. If such a node isfound, and the node is not part of a dynamically sized element,the algorithm reports it as a structural CPI.
After these steps, the algorithm visually compares mapped
nodes (lines 34-44). This part of the algorithm starts by re-trieving (G
ET-SCREENSHOT ) the screenshots of the referenceAlgorithm 3: CPI detection analysis.
Input : RUIM : UI model of reference device
TUIMMap : map of UI models of test devices
Output: CPIReport: set of cross device inconsistencies
1begin
2CPIReport =∅
3 fori=0 ;i < RUIM.length;+ + ido
4 rdModel =RUIM. GET(i)
5 foreachtd∈TUIMMap. KEY-SET()do
6 //node mapping
7 tdModel =TUIMMap [td]. GET(i)
8 rdTree =rdModel. GET-TREE()
9 tdTree =tdModel. GET-TREE()
10 nodeMappingMap =∅
11 foreachrdNode ∈rdTree do
12 mappedNodeList =∅
13 foreachtdNode ∈tdTree do
14 nodeSim =
15 COMPUTE -STRUCTURAL -SIMILARITY (rdTree ,
rdNode ,tdTree ,tdNode )
16 ifnodeSim ≥αthen
17 mappedNodeList. ADD(MAPPING (
18 nodeSim ,tdNode ))
19 nodeMappingMap[ rdNode ]=mappedNodeList
20 FIND-BEST-MAPPING (nodeMappingMap )
21 //structural comparison
22 foreachrdNode ∈rdTree do
23 ifnodeMappingMap[rdNode ]==∅and
¬WITHIN -DYNAMICALLY -SIZED -
ELEMENT (rdNode ,nodeMappingMap )
then
24 CPIReport. ADD(STRUCTURAL -CPI(td,
rdNode ))
25 nodeMappingMap. REMOVE (rdNode )
26 foreachtdNode ∈tdTree do
27 mapped = FALSE
28 foreachrdNode ∈rdTree do
29 ifnodeMappingMap[ rdNode ].
30 CONTAINS (tdNode )then
31 mapped =T R U E
32 ifmapped ==FALSE and ¬WITHIN -DYNAMICALLY -
SIZED -ELEMENT (tdNode ,nodeMappingMap )
then
33 CPIReport. ADD(STRUCTURAL -CPI(td,
tdNode ))
34 //visual comparison
35 rdScreenshot =rdModel. GET-SCREENSHOT ()
36 tdScreenshot =tdModel. GET-SCREENSHOT ()
37 foreachrdNode ∈nodeMappingMap. KEY-SET()do
38 tdNode =nodeMappingMap[rdNode ].REMOVE (0)
39 rdNodeImage =rdScreenshot. CROP(rdNode )
40 tdNodeImage =tdScreenshot. CROP(tdNode )
41 isInconsistency =
42 COMPUTE -IMAGE -SIMILARITY (rdNodeImage ,
tdNodeImage )
43 ifisInconsistency then
44 CPIReport. ADD(VISUAL -CPI(td,
rdNodeImage ,tdNodeImage ))
45 RANK (CPIReport )
46 returnCPIReport
and test devices from their window models. Then, for eachnode in the reference tree, the algorithm retrieves the testnode mapped to it and creates two images (rdNodeImageandtdNodeImage) from the two screenshots (rdScreenshot
andtdScreenshot ) using function C
ROP. At this point, the
algorithm compares the two images using function C OMPUTE -
IMAGE -SIMILARITY , which uses a decision tree classiﬁer
to recognize inconsistencies. We describe the decision treeclassiﬁer we use in Section III-D2. The function uses similarprinciples as the ones we discussed in the context of function
W
ITHIN -DYNAMICALLY -SIZED -ELEMENT : it does not report
as inconsistencies nodes that are partially visible because theyare part of a dynamically sized element. If the classiﬁer
312
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:57:18 UTC from IEEE Xplore.  Restrictions apply. identiﬁes an inconsistency, the algorithm reports that the
reference node and the test node have a visual inconsistency(line 44).
The algorithm then ranks (R
ANK , in line 45) inconsistencies
according to the following principles: (1) structural inconsis-
tencies are ranked at the top, (2) visual inconsistencies thataffect all devices with the same characteristics (e.g., version
of the Android operating system) are ranked next, and (3) the
remaining inconsistencies are listed last. Finally the algorithmreturns the CPI report (line 46), which is the output of thetechnique.
1) Node Structural Similarity: Function C
OMPUTE -
STRUCTURAL -SIMILARITY in Algorithm 3 computes the
structural similarity between two nodes. The inputs to thefunction are a reference tree (rdTree ), a reference node
(rdNode ), a test tree (tdTree), and a test node (tdNode).
The output of the function is a value between zero and one(nodeSim ) that indicates the similarity between the reference
node and the test node. This function is necessary, as nodesin the tree are not required to have identiﬁers, and differentversions of the operating system can use different node typesto represent the same node.
The function starts by comparing the identiﬁers of the
reference and test nodes. If the identiﬁers are the same, andthey are unique in both reference and test trees, the functionsets the node similarity value to one and returns it. In thiscase, the function sets the similarity value to its highest valuebecause the identiﬁer is a property manually deﬁned by thedeveloper that is meant to uniquely identify nodes in the tree.
If identiﬁers are not unique, or they are different, the
function checks the position of the two nodes in the treeby comparing their XPaths (using the path expression fromthe root of the tree). If the nodes have the same XPath, thefunction returns one as their similarity. If the path expressionsof the two nodes differ in more than one path component,the function returns zero as similarity value (to avoid matches
between nodes that are too distant in the tree). If only one
path component is different, which may be due to smalldifferences in the tree representation of the test devices, thefunction computes the similarity value based on the followingproperties of the nodes: checkable, clickable, focusable ,
scrollable, text,checked ,selected ,long -clickable, enabled,
andfocused . The similarity value, in this case, is given by
the number of matching properties divided by the number ofproperties. For the evaluation of D
IFFDROID we chose 0.9
as the value of αin Algorithm 3 to indicate that we do not
want to match nodes having more than one property value thatdiffers.
2) Decision Tree Classiﬁer: Function C
OMPUTE -IMAGE -
SIMILARITY in Algorithm 3 uses a decision tree classiﬁer [17]
to compute whether the visual representation of two nodesshould be reported as a CPI. The decision tree classiﬁeralgorithm creates a model that predicts the value of a targetvariable based on a set of input variables. The algorithm learnsthe model using a set of training data. In our context, thetraining data corresponds to images of nodes from the UIhierarchies of apps running on different devices. The trainingset must also include a set of images exhibiting CPIs. Afterbuilding the model, function C
OMPUTE -IMAGE -SIMILARITY
follows the set of decisions in the model to predict the targetvariable. D
IFFDROID uses the following variables as inputs to
the the classiﬁer:
Complex-Wavelet Structural Similarity Index. Our tech-
nique uses the Complex-Wavelet Structural Similarity (CW-
SSIM) index [15] to compare the structural similarity of the
content of two images. CW-SSIM is an image similarity metric
robust to small rotations and translations in the images beingcompared. This characteristic makes the metric especiallysuitable in our context because different devices have differentscreen conﬁgurations; therefore, the visual representation oftwo nodes may present minor differences that should not be
reported as CPIs.
Earth’s Mover Distance of Color Histograms. D
IFFDROID
uses the Earth’s Mover Distance [18] (EMD) of the color
histograms of two images to compare the color composition
of the images. EMD is a measure of the distance betweentwo distributions and, intuitively, consists of the minimal costthat must be paid to transform one color distribution into theother. We decided to use this metric to take into account thefact that two images may have similar structure but display
different colors.
Relative Ratio Change. Our technique uses the relative ratio
change to assess whether two images differ signiﬁcantly in
their proportions. The relative ratio change is deﬁned as
RRC =( (w
t/ht)−(wr/hr))/(wr/hr), wherewtandht
are the width and height of the test node, while wrandhr
are the width and height of the reference node. This value
allows D IFFDROID to identify nodes whose ratio is altered as
a consequence of the placement of other nodes.
Optical Character Recognition Output. DIFFDROID uses
the output of optical character recognition [19] (OCR) to
assess whether two images display the same text. The classiﬁertakes as input the value of the comparison (equal/not equal).
We decided to use the output of OCR because nodes might
have the same text in their tree representation but might displaythe text differently.
The target variable predicted by the classiﬁer indicates
whether the visual representation of two nodes should be
reported as an inconsistency.
IV . I
MPLEMENTATION
DIFFDROID ’s input generation module is built on top of
Monkey [5], an input generator for Android apps that generates
pseudo-random sequences of user and system events. Weextended Monkey to encode generated inputs into the traceand to save UI hierarchies together with visual representations
(screenshots) of the app being tested. The tool is able to inspectthe screen content of the app using UiAutomation [20], which
is a special accessibility service of the Android platform.The input generation module can run on any device withoutmodifying the Android system.
313
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:57:18 UTC from IEEE Xplore.  Restrictions apply. TABLE I: BENCHMARKS USED IN THE EMPIRICAL EV ALUATION .
ID Name Category V ersion LOC (#K)
A1 BUILDM LEARN Education 2.5.0 23.6
A2 DAILY DOZEN Health 10.3 6.3
A3 KITCHEN TIMER Tools 1.1.6 4.3
A4 OUTLAY Finance 1.1.3 8
A5 TRANSLATION STUDIO Books 9.0 51.2
The test case encoding module uses the J AVAPOET 1.8
library [21] to generate the source code of test cases. The
implementation of D IFFDROID extends the Espresso [22]
framework to generate and execute test cases. The Espressoframework synchronizes test operations with the app beingtested by waiting for UI events to be handled and for defaultinstances of AsyncTask (computation that runs on a back-
ground thread) to complete. This capability allows D
IFFDROID
to execute the same test case on different devices even when
devices have different hardware conﬁgurations, which couldlead to different timings in the execution of an app. However,there are cases in which apps perform background operationsusing non-standard means (e.g., direct creation and manage-ment of threads). The developer needs to manually handlesuch cases. For the benchmarks of Section V, this additionaltask was not necessary. The test case encoding module usesthe implementation of CW-SSIM offered by pyssim [23] tocompare screenshots taken by the input generation module
and minimize the number of UI hierarchies and screenshotsgenerated during test execution.
The test case execution module leverages the AWS Device
Farm [24] to execute test cases on real devices. The moduleuses the AWS Command Line Interface (CLI) to automate theprocess of running test cases and retrieving execution artifacts(UI hierarchies and screenshots).
Finally, the CPI analysis module generates the decision
tree classiﬁer used to detect CPIs by leveraging the Wekadata mining framework [25]. The CW-SSIM index used inthe classiﬁer is computed using pyssim, the EMD value iscomputed using the OpenCV [26] library, and the characterrecognition task is performed used Tesseract OCR [27] engine.
V. E
MPIRICAL EV ALUATION
To determine the practicality and effectiveness of our tech-
nique, we performed an empirical evaluation of D IFFDROID
on a set of real-world apps and targeted the following researchquestions:
•RQ1: Can D IFFDROID detect cross-platform inconsis-
tencies in mobile applications while reporting a limitednumber of false positives?
•RQ2: What is the cost of running D IFFDROID ?
•RQ3: Are there similarities among devices exhibiting
CPIs?
A. Experimental Benchmarks and Setup
For the empirical evaluation, we used a set of real-world
Android apps. More speciﬁcally, we selected ﬁve open-source
apps from GitHub [28]. We used open-source apps because theTABLE II: NUMBER OF TEST DEVICES DIVIDED BY RESOLUTION
AND VERSION OF THE OPERATING SYSTEM .
Android V ersion
Resolution 19 21 22 23 24 25
720 x1280 18 2 6 2 0 0
768 x1280 1 0 0 0 0 0
1080 x1920 33 12 5 6 0 1
1440 x2560 8 7 8 13 5 1
480 x800 8 0 0 0 0 0
540 x960 5 1 3 0 0 0
480 x854 1 0 1 0 0 0
testing environment (Espresso) used in the implementation of
our technique requires the source code of an app to build andrun test cases for it. Our technique could be directly appliedto app executables by changing testing framework.
We selected apps based on three parameters: (1) presence
of at least one known UI-based CPI in the app, (2) self-containment, and (3) diversity. In order to ﬁnd apps contain-ing at least one known CPI, we searched GitHub’s trackersystem for the following keywords: “android not clickable”,“android cut off”, and “android missing button”. We usedthese keywords instead of more generic keywords, such as“android compatibility issue”, to eliminate results that werenot UI issues, which are out of scope for our technique.
From the search results, we removed issues that did notcorrespond to Android apps and issues that we could not
reproduce. Finally, we selected apps from different categoriesto have a diverse corpus of benchmarks, while prioritizingapps for which we did not have to build extensive stubs.Table I provides a summary description of the apps considered.
Columns Name, Category, V ersion, and LOC report the name,
category, version, and number of lines of code for an app.
The analysis performed by D
IFFDROID relies on the use of
a reference device. We selected an LG G3 running Android
22as reference device for the empirical evaluation because
we had the device, and it did not exhibit any of the CPIs
already known in the benchmarks. To compute the results
of Section V-B, we executed the input generation phase of
DIFFDROID on the reference device with a timeout of 10
minutes. We chose this value because in previous work [29]
we found that a set of dynamic input generation tools forAndroid apps hit their maximum coverage within 10 minutesof execution.
D
IFFDROID ’s analysis also requires a set of test devices. We
used the AWS Device Farm [24] for this purpose. The AWS
Device Farm is an app testing service provided by Amazon
that allows to run tests on real mobile devices. Table II reportsthe number of devices used in the empirical evaluation groupedby resolution (Resolution) and version of the operating system
(Android V ersion). The versions of the operating system were
the ones available to us and supported by the technologies usedfor the implementation of D
IFFDROID . The total number of
devices used was 147.
Finally, D IFFDROID uses a decision tree classiﬁer to rec-
ognize CPIs. We trained the classiﬁer using the following
314
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:57:18 UTC from IEEE Xplore.  Restrictions apply. TABLE III: RESULTS OF RUNNING DIFFDROID .FOR EACH BENCHMARK
CONSIDERED :D(#) =NUMBER OF TEST DEVICES ;WM(#) =NUMBER OF
WINDOW MODELS ;NR(#)=NUMBER OF NODES IN UI HIERARCHY TREES
OF THE REFERENCE DEVICE ;NT(#)=A VERAGE NUMBER OF NODES IN UI
HIERARCHY TREES PER TEST DEVICES ;CPIS(#)=NUMBER OF STRUCTURAL
CPI S;CPIF(#)=NUMBER OF FUNCTIONAL CPI S;CPIV(#)=NUMBER OF
CPI S RELATED TO CHANGES IN THE VERSION OF THE ANDROID SYSTEM ;
CPIC(#)=NUMBER OF COSMETIC CPI S;FP(#) =FALSE POSITIVES REPORTED
BY THE TECHNIQUE .
ID D(#) WM(#) NR(#) NT(#) CPIS(#) CPIF(#) CPIV(#) CPIC(#) FP(#)
A1 135 19 491 465.2 0 2 7 14 1
A2 138 22 1199 1174.8 2 0 0 22 4
A3 129 13 286 276.6 2 3 0 2 1
A4 125 14 505 481.8 0 1 0 17 2
A5 136 17 486 466 2 3 0 19 8TABLE IV: COST OF RUNNING DIFFDROID .TG(s)=
TIME TO ENCODE INPUTS AND COMPUTE THE REFER -
ENCE UI MODEL ;TE(s)=A VERAGE TEST EXECUTION
TIME PER DEVICE ;TS(ms) =A VERAGE TIME TO COM -
PARE UIHIERARCHIES PER DEVICE ;TCW (s)=AV E R AG E
CW-SSIM COMPUTATION TIME PER DEVICE ;TEMD (s)
=A VERAGE EMD COMPUTATION TIME PER DEVICE ;
AND TOCR (s)=AV E R AG E OCR COMPUTATION TIME
PER DEVICE .
ID TG(s) TE(s) TS(ms) TCW (s)TEMD (s)TOCR (s)
A1 2080 474.9 139 443.8 14.8 172
A2 1102 512.2 581 575.7 50.8 586.5
A3 2772 329.9 115 246.4 11.9 134.5
A4 851 651.9 210 275 15.1 103.7
A5 1803 376 166 217.5 14.1 153.5
procedure. First, we selected one device from each category
(combination of resolution and Android version) in Table IIand used this set of devices to compute the training set. Wethen collected CW-SSIM index, EMD value, OCR output, and
relative ratio change (inputs to the classiﬁer) for all the nodes
in the view hierarchies showing the known UI-based CPIs.(These nodes are not included in the results of the evaluation.)This procedure produced 5,558 entries on which to train the
classiﬁer. We labeled the entries either true orfalse based
on whether they represented CPIs or not, respectively. Welabeled entries by looking at their visual representation andlabeled as true entries such that, compared to the reference
entry, (1) differed in their content structure, (2) differed interms of color, (3) differed in terms of visibility, (4) visualizeda different text, and (5) had a different aspect ratio. Followingthese guidelines, we labeled 282 entries. We used the Weka
data mining framework [25] to generate a C 4.5decision tree
classiﬁer. The framework created a classiﬁer of size 33with
17leaves in 0.09seconds. We evaluated the classiﬁer using
10-fold cross validation, resulting in a precision of 0.978 and
recall of 0.957. The CPI analysis phase was performed on a
workstation with 64GB of memory, one Intel Xeon i 7-6700K
Skylake 4.0GHz processor, running Ubuntu 14.04.
B. Results
1) RQ1: To answer RQ1, we applied our technique to the
experimental benchmarks. Table III reports the results of theevaluation.
The ﬁrst part of Table III (columns D(#), WM(#), N
R(#), and
NT(#)) provides a picture of the scale of the analysis. For each
benchmark: column D(#) reports the number of test devices
used in the test execution phase; column WM(#) provides the
number of window models generated by the test case encodingphase; N
R(#)is the number of nodes in the UI hierarchy trees
for the reference device; and NT(#)is the average number of
nodes in the UI hierarchies for the test devices. The numberof devices used for each benchmark differs because, whenrunning the evaluation, certain devices were not available inthe AWS Device Farm. For unavailable devices, we attemptedto run test cases three times before moving forward. ColumnsN
R(#)and NT(#)differ for two reasons: the app might contain
a structural CPI, and different devices display a differentnumber of nodes for dynamically sized elements (e.g., list
containers). The total number of nodes analyzed across allbenchmarks and devices is 387, 174.
The second part of Table III (columns CPI
S(#), CPIF(#),
CPIV(#), and CPIC(#)) presents the CPIs reported by D IFF-
DROID . We analyzed CPIs reported by our technique and
classiﬁed them in four categories: inconsistencies in the UI
hierarchy tree that affect the functionality of the app ( structural
CPIs, CPIS(#)); inconsistencies in the visual representation
of a node that affect the functionality of the app (functionalCPIs, CPI
F(#)); inconsistencies generated by the version of
the Android system used to run the benchmark (version CPIs,CPI
V(#));and inconsistencies in the visual representation of a
node that do not affect the functionality of the app becausethe user can infer their meaning given the context in whichthey are visualized (cosmetic CPIs, CPI
C(#)). Structural CPIs
correspond to the inconsistencies with the same name we
discussed in Section III-D, while functional CPIs, versionCPIs, and cosmetic CPIs correspond to the visual CPIs thatwe also discussed in Section III-D. The results presented inthis section are deterministic, as the classiﬁcation part of thetechnique is itself deterministic. In addition, we also classiﬁedCPIs reported by D
IFFDROID that did not correspond to an
inconsistency as false positives (FP(#)). Finally, we randomly
selected 5nodes in each benchmark on all test devices (3, 315
total), checked for possible false negatives, and did not ﬁndany.
Our technique found CPIs in all the benchmarks analyzed:
6structural CPIs, 9functional CPIs, 7version CPIs, and
74cosmetic CPIs. We now provide an example from each
category to better illustrate the identiﬁed CPIs and how we
classiﬁed them.
T
RANSLATION STUDIO is a translation app. In the regis-
tration form of the app, there is an icon that, when clicked,
presents a privacy note to the user. However, on certaindevices, the icon is not present, and the user will miss theopportunity to read the privacy note. On these devices, thenode of the icon is not present in the UI hierarchy tree
315
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:57:18 UTC from IEEE Xplore.  Restrictions apply. of the app, and D IFFDROID reports this difference as an
inconsistency (structural CPI).
KITCHEN TIMER offers a timer functionality. The app can
be used to start and stop three timers. If one of the timers
is started, the label of the timer increases in size, movingthe button to stop the timer at the bottom of the screen.On certain devices, the size of the button becomes small
enough to prevent users from stopping the timer. In these
devices, the node representing the button is present in the UIhiearchy tree, but its visual appearance differs from that ofthe corresponding node on the reference device. D
IFFDROID
reports this difference as an inconsistency (functional CPI).
BUILDM LEARN is an app that assists users in developing
Android apps. The app has a menu that can be used to navigatethe app. The color of the background of the items in the menuis different when the app is running on devices using Androidversion 19. In these devices, the color of the background is
similar to the color of the text of the menu items, makingdifﬁcult to read the entries in the menu. D
IFFDROID reports
this difference as an inconsistency (version CPI).
OUTLAY helps users track their expenses. Users can enter
their expenses using a numpad. On certain devices, onlyroughly one fourth of the numbers is visible. Also in thiscase, D
IFFDROID reports this difference as an inconsistency
(cosmetic CPI).
We looked at the nature of the structural, functional, and
cosmetic CPIs mentioned above, and discovered tha they canbe ﬁxed by changing properties of corresponding elements inthe layout ﬁles for the apps. We reported the issues found, andtheir possible solutions, to the developers of the apps involved.
D
IFFDROID also reported 16false positives for the ﬁve
benchmarks we considered. The false positives reported canbe grouped into two categories. The ﬁrst category (14 false
positives) includes nodes that display text with additionalspacing at the end. This behavior causes test nodes to have abig relative ratio change, leading the classiﬁer to report themas inconsistencies. To address this issue, we plan to leverageOCR to recognize text boundaries and compute relative ratiochanges based on such boundaries. The second category (2false positives) includes test nodes whose image differed fromthe reference one in the color distribution, but the differenceis such that it cannot be perceived by the human eye. Thischaracteristic resulted in a signiﬁcantly high EMD value,leading the classiﬁer to report these nodes as inconsistencies.To reduce the number of this kind of false positives, we planto investigate how the number of bins in the computation ofthe EMD value affects false positives and performance.
Overall, we feel that the current number of false positives
generated by D
IFFDROID is acceptable. (Moreover, they can
be further reduced through improvements of the technique.)We therefore believe that the results presented in this sectionprovide initial evidence that D
IFFDROID can detect CPIs in
mobile applications while reporting a limited number of false
positives.
2) RQ2: To answer RQ2, we measured the time taken
by each phase of the technique to process the experimentalTABLE V: DEVICES WITH THE HIGHEST NUMBER OF CPI SI N
OUR EV ALUATION .C OLUMN AV(ANDROID VERSION )REPORTS
THE VERSION OF THE ANDROID SYSTEM RUNNING ON THE DE -
VICE .
Device Resolution Density AV
LG Optimus L70 480 x800 207 19
Samsung Galaxy S3 Mini 480 x800 233 19
Samsung Galaxy J1 Ace 480 x800 217 19
Samsung Galaxy J1 Duos 480 x800 217 19
Samsung Galaxy S Duos 480 x800 233 19
Samsung Galaxy Grand Neo Plus 480 x800 187 19
Intex Aqua Y2 Pro 480 x854 218 19
Samsung Galaxy Light 480 x800 233 19
Samsung Galaxy Star Advance 480 x800 217 19
Samsung Galaxy Note 2 720 x1280 267 19
benchmarks. Table IV summarizes the results and reports:
the time required to encode dynamically generated inputs as
a test case while computing the UI model of the referencedevice (T
G(s)); the average test case execution time per device
(TE(s)); the average time required to compare reference UI
hierarchies with test UI hierarchies per device (T S(ms)); the
average time required to compute CW-SSIM indexes per
device (T CW (s)); the average time required to compute EMD
values per device (T EMD (s)); and the average time required
to extract text with OCR per device (T OCR (s)).
The values in column TG(s)show that the cost to compute
the UI model based on the dynamically generated inputs is notlow (but still acceptable), which validates our choice of notperforming this task during the input generation phase. Theaverage time to execute test cases is less than the time takento generate inputs. This happens mainly because test cases aresaving signiﬁcantly less UI hierarchies and screenshots. (Theonly exception is A4, for which we had to add a 60sec sleep
time to make sure the test would go past the login screen onthe test devices.) In the worst case (A4), test cases took a total
of1,358 minutes to execute (D from Table III times T
Efrom
Table IV). During the evaluation we took advantage of the fact
that this task is highly parallelizable and executed test caseson10devices at the time, thus reducing the cost roughly by
an order of magnitude.
Finally, the last part of Table IV shows that the time required
to compare reference UI hierarchies to test UI hierarchiesis negligible compared to the time to compute values forthe features of the classiﬁer. In the worst case (A2), theCPI analysis phase took 2,791 minutes to complete (D from
Table III times the sum of T
S,TCW,TEMD , and TOCR
from Table IV). This task is also highly parallelizable. and
when running the evaluation we analyzed eight devices ata time. Finally, the most expensive part of the CPI analysis
phase consists of the computation of CW-SSIM indexes, whichacross all apps and all devices took 351.7seconds on average.
Based on these results, we can conclude that the analysis
performed by D
IFFDROID can run overnight, at least for the
cases considered.
3) RQ3: To answer RQ3, in Table V we ranked devices
based on the number of CPIs they exhibited, with the device
316
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:57:18 UTC from IEEE Xplore.  Restrictions apply. exhibiting the highest number of CPIs at the top. For each
device: column Device shows the name of the device; columns
Resolution and Density shows the pixel resolution and density,
respectively, of the device; and column AVreports the version
of the Android system running on the device. The top nine
devices all have low values for resolution and density, and noother test device has these characteristics. This result suggests
that developers should consider to include a device with these
characteristics when testing their apps. While looking at therelation between inconsistencies and device characteristics,however, we also observed that considering testing devicessolely based on resolution and density would have not allowedus to identify all the inconsistencies reported in Table III.
In fact, there are inconsistencies that derive from different
hardware conﬁgurations of the devices (e.g., the presence ofa physical menu button). It is also worth noting that, evenif all devices in Table V happen to run Android version 19,
we could not ﬁnd any reason why that this version should beparticularly problematic.
VI. T
HREATS TOVALIDITY
As it is the case for most empirical evaluation, there are
both construct and external threats to validity associated withour results. In terms of construct validity, there might be errorsin the implementation of our technique. To mitigate this threat,we extensively inspected the results of the evaluation manually.In terms of external validity, our results might not generalizeto other apps or CPIs. In particular, we only considered alimited number of apps. This limitation is an artifact of thecomplexity involved in manually inspecting results derivingfrom executions on a large set of devices (over 130). To
mitigate this threat, we used randomly selected real-world appsfrom different domains.
VII. R
ELATED WORK
The fragmentation of the Android ecosystem has been stud-
ied in the literature [30], [31], [2], [32], [33], [34], [35], [36].
Han and colleagues [30] are among the ﬁrst to study the issuesgenerated by such fragmentation. Their work systematicallyanalyzes bug reports from two popular mobile device vendors
and proposes a method for tracking fragmentation. In thisline of research, Holzinger and colleagues [31] discuss the
challenges involved in developing apps due to the differencesin size and display resolution of different devices. Our work
helps developers in this challenging task by automaticallyidentifying inconsistencies across devices.
The work on Android fragmentation led to studies on
device prioritization for app testing [37], [38], [39]. The recentwork from Lu and colleagues [37], in particular, proposes atechnique to prioritize Android device models for individualapps, based on mining large-scale usage data. We believe thisline of research to be complementary to ours, as it could beintegrated within D
IFFDROID in case resources are constrained
in terms of number of test devices considered.
Other related work tries to ﬁnd compatibility issues in
Android apps [40]. Wei and colleagues propose a techniquebased on static analysis that identiﬁes compatibility issuesusing an API-Context pair model. The issues identiﬁed by theirtechnique are different from those identiﬁed by D
IFFDROID ,
as they are related to platform API evolution and drivers im-plementation. This technique could therefore also be combinedwith D
IFFDROID to identify a broader set of issues.
Finally, our work relates to the work on inconsistency iden-
tiﬁcation for web apps [41], [42], [43], [44], [45]. Roy Choud-hary and colleagues [41] propose a technique that crawlsthe web app under test in different browsers, collects DOMtrees and screenshots for web pages in the app, and comparescollected trees and images to identify inconsistencies. Thereare differences between mobile and web apps that prevent thisand similar techniques to be straightforwardly applied in ourcontext. This technique, for instance, runs browsers so that the
size of their visible area is the same, which is an assumption
that cannot be made for mobile apps.
VIII. C
ONCLUSION
Because of the fragmentation of the Android ecosystem, An-
droid apps can exhibit inconsistencies in their behavior whenthey are run on different platforms. To help developers identifythese behavioral inconsistencies before developers release theirapps, we propose a technique called D
IFFDROID .DIFFDROID
aims to identify and suitably report behavioral inconsistenciesin Android apps by combining input generation, behaviormodeling, and differential testing.
We implemented and empirically evaluated D
IFFDROID by
running it on 5real-world benchmark apps and over 130
different platforms. Our results provide initial evidence that
DIFFDROID can identify CPIs on real apps efﬁciently and with
a limited number of false positives.
In future work, we will ﬁrst extend our evaluation by con-
sidering additional apps. Second, we will perform a user studywith app developers to assess how useful is the informationour technique provides and how effective is the format inwhich it is provided. Third, we will extend D
IFFDROID so
that it handles inconsistencies other than visual ones (e.g.,inconsistencies in the state of the app after an event isprocessed). Fourth, we will investigate whether precision and
accuracy of CPIs identiﬁcation can improve by using a multi-class classiﬁer approach. Fifth, we will investigate techniquesfor suggesting repairs for the CPIs identiﬁed by D
IFFDROID .
Finally, and more on the engineering side, we will investigate
ways to automatically build stubs for the apps being tested,
so as to speed up the execution on the different devices,enforce determinism, and in general allow for performing testexecutions in a sandbox.
A
CKNOWLEDGMENTS
This work was partially supported by the National Science
Foundation under grants CCF-1320783 and CCF-1161821,
and by funding from Amazon under the AWS Cloud Creditsfor Research program, Google, IBM Research, and MicrosoftResearch.
317
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:57:18 UTC from IEEE Xplore.  Restrictions apply. REFERENCES
[1] Recode, “Salesforce will only support Nexus and Samsung Galaxy
phones to avoid Android fragmentation,” https://www.recode.net/2016/
7/18/12217580/salesforce-support-nexus-samsung-galaxy-android.
[2] M. E. Joorabchi, A. Mesbah, and P. Kruchten, “Real Challenges in
Mobile app Development,” in 2013 ACM/IEEE International Symposium
on Empirical Software Engineering and Measurement (ESEM), 2013.
[3] Google, “Daily Dozen,” https://play.google.com/store/apps/details?id=
org.nutritionfacts.dailydozen.
[4] ——, “UI Overview,” https://developer.android.com/guide/topics/ui/
overview.html.
[5] ——, “Monkey,” https://developer.android.com/studio/test/monkey.html.[6] A. Machiry, R. Tahiliani, and M. Naik, “Dynodroid: An Input Generation
System for Android Apps,” in Proceedings of the 2013 Joint Meeting
on F oundations of Software Engineering, 2013.
[7] D. Amalﬁtano, A. R. Fasolino, P. Tramontana, S. De Carmine, and A. M.
Memon, “Using GUI Ripping for Automated Testing of Android Appli-cations,” in Proceedings of the IEEE/ACM International Conference on
Automated Software Engineering, 2012.
[8] W. Yang, M. R. Prasad, and T. Xie, “A Grey-box Approach for Auto-
mated GUI-model Generation of Mobile Applications,” in Proceedings
of the 16th International Conference on Fundamental Approaches toSoftware Engineering, 2013.
[9] T. Azim and I. Neamtiu, “Targeted and Depth-ﬁrst Exploration for
Systematic Testing of Android Apps,” in Proceedings of the 2013 ACM
SIGPLAN International Conference on Object Oriented ProgrammingSystems Languages, and Applications, 2013.
[10] W. Choi, G. Necula, and K. Sen, “Guided GUI Testing of Android Apps
with Minimal Restart and Approximate Learning,” in Proceedings of
the 2013 ACM SIGPLAN International Conference on Object OrientedProgramming Systems Languages, and Applications, 2013.
[11] S. Hao, B. Liu, S. Nath, W. G. Halfond, and R. Govindan, “PUMA: Pro-
grammable UI-automation for Large-scale Dynamic Analysis of MobileApps,” in Proceedings of the 12th Annual International Conference on
Mobile Systems, Applications, and Services, 2014.
[12] R. Mahmood, N. Mirzaei, and S. Malek, “EvoDroid: Segmented Evo-
lutionary Testing of Android Apps,” in Proceedings of the 22nd ACM
SIGSOFT International Symposium on F oundations of Software Engi-neering, 2014.
[13] S. Anand, M. Naik, M. J. Harrold, and H. Yang, “Automated Concolic
Testing of Smartphone Apps,” in Proceedings of the 20th ACM SIGSOFT
International Symposium on the F oundations of Software Engineering,2012.
[14] A. Zeller and R. Hildebrandt, “Simplifying and Isolating Failure-
Inducing Input,” IEEE Transactions on Software Engineering, 2002.
[15] M. P. Sampat, Z. Wang, S. Gupta, A. C. Bovik, and M. K. Markey,
“Complex wavelet structural similarity: A new image similarity index,”IEEE transactions on image processing, 2009.
[16] M. Fazzini, E. N. d. A. Freitas, S. Roy Choudhary, and A. Orso, “Barista:
A Technique for Recording, Encoding, and Running Platform Indepen-dent Android Tests,” in Proceedings of the 2017 IEEE International
Conference on Software Testing, V eriﬁcation and V alidation, 2017.
[17] J. R. Quinlan, C4. 5: programs for machine learning. Elsevier, 2014.
[18] Y . Rubner, C. Tomasi, and L. J. Guibas, “The earth mover’s distance as
a metric for image retrieval,” International journal of computer vision,
2000.
[19] C. Patel, A. Patel, and D. Patel, “Optical character recognition by open
source OCR tool tesseract: A case study,” International Journal of
Computer Applications, 2012.
[20] Google, “UiAutomation,” https://developer.android.com/reference/
android/app/UiAutomation.html.
[21] Square, “JavaPoet,” https://github.com/square/javapoet.
[22] Google, “Espresso,” https://google.github.io/
android-testing-support-library.
[23] A. Vacavant, C. Godfrey, and J. Terrace, “pyssim,” https://github.com/
jterrace/pyssim.[24] Amazon, “Device Farm,” https://aws.amazon.com/device-farm.
[25] University of Waikato, “Weka,” http://www.cs.waikato.ac.nz/ml/weka.
[26] OpenCV, “OpenCV,” http://opencv.org.
[27] Google, “Tesseract OCR,” https://github.com/tesseract-ocr/tesseract.[28] GitHub, “GitHub,” https://github.com/.
[29] S. R. Choudhary, A. Gorla, and A. Orso, “Automated Test Input
Generation for Android: Are We There Yet? (E),” in Proceedings of
the 2015 IEEE/ACM International Conference on Automated Software
Engineering (ASE),
 2015.
[30] D. Han, C. Zhang, X. Fan, A. Hindle, K. Wong, and E. Stroulia,
“Understanding Android Fragmentation with Topic Analysis of Vendor-
Speciﬁc Bugs,” in Proceedings of the 2012 Working Conference on
Reverse Engineering, 2012.
[31] A. Holzinger, P. Treitler, and W. Slany, “Making apps useable on
multiple different mobile platforms: On interoperability for business
application development on smartphones,” in International Conference
on Availability, Reliability, and Security, 2012.
[32] H. Li, X. Lu, X. Liu, T. Xie, K. Bian, F. X. Lin, Q. Mei, and
F. Feng, “Characterizing smartphone usage patterns from millions of
Android users,” in Proceedings of the 2015 ACM Conference on Internet
Measurement Conference, 2015.
[33] A. Pathak, Y . C. Hu, and M. Zhang, “Bootstrapping energy debugging
on smartphones: a ﬁrst look at energy bugs in mobile devices,” in
Proceedings of the 10th ACM Workshop on Hot Topics in Networks,2011.
[34] Y . Liu, C. Xu, and S.-C. Cheung, “Characterizing and detecting perfor-
mance bugs for smartphone applications,” in Proceedings of the 36th
International Conference on Software Engineering, 2014.
[35] L. Wu, M. Grace, Y . Zhou, C. Wu, and X. Jiang, “The impact of vendor
customizations on android security,” in Proceedings of the 2013 ACM
SIGSAC conference on Computer & communications security, 2013.
[36] X. Zhou, Y . Lee, N. Zhang, M. Naveed, and X. Wang, “The peril of frag-
mentation: Security hazards in android device driver customizations,” in
2014 IEEE Symposium on Security and Privacy (SP), 2014.
[37] X. Lu, X. Liu, H. Li, T. Xie, Q. Mei, D. Hao, G. Huang, and F. Feng,
“PRADA: Prioritizing android devices for apps by mining large-scale
usage data,” in Proceedings of the 38th International Conference on
Software Engineering, 2016.
[38] S. Vilkomir and B. Amstutz, “Using combinatorial approaches for
testing mobile applications,” in 2014 IEEE International Conference on
Software Testing, V eriﬁcation and V alidation Workshops (ICSTW), 2014.
[39] H. Khalid, M. Nagappan, E. Shihab, and A. E. Hassan, “Prioritizing the
devices to test your app on: A case study of android game apps,” inProceedings of the 22nd ACM SIGSOFT International Symposium on
F oundations of Software Engineering, 2014.
[40] L. Wei, Y . Liu, and S.-C. Cheung, “Taming Android fragmentation:
Characterizing and detecting compatibility issues for Android apps,”
in2016 IEEE/ACM International Conference on Automated Software
Engineering (ASE), 2016.
[41] S. R. Choudhary, H. Versee, and A. Orso, “Webdiff: Automated Iden-
tiﬁcation of Cross-Browser Issues in Web Applications,” in 2010 IEEE
International Conference on Software Maintenance (ICSM), 2010.
[42] A. Mesbah and M. R. Prasad, “Automated Cross-Browser Compatibil-
ity Testing,” in Proceedings of the 33rd International Conference on
Software Engineering, 2011.
[43] S. R. Choudhary, M. R. Prasad, and A. Orso, “Crosscheck: Combining
Crawling and Differencing to Better Detect Cross-Browser Incompati-
bilities in Web Applications,” in 2012 IEEE International Conference
on Software Testing, V eriﬁcation and V alidation (ICST), 2012.
[44] V . Dallmeier, M. Burger, T. Orth, and A. Zeller, “Webmate: a Tool
for Testing Web 2.0 Applications,” in Proceedings of the Workshop on
JavaScript Tools, 2012.
[45] S. Roy Choudhary, M. R. Prasad, and A. Orso, “X-PERT: Accurate Iden-
tiﬁcation of Cross-Browser Issues in Web Applications,” in Proceedings
of the 2013 International Conference on Software Engineering, 2013.
318
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:57:18 UTC from IEEE Xplore.  Restrictions apply. 