Detecting Unknown Inconsistencies
in Web Applications
Frolin S. Ocariza, Jr. Karthik Pattabiraman Ali Mesbah
University of British Columbia, V ancouver, BC, Canada
{frolino, karthikp, amesbah}@ece.ubc.ca
Abstract —Although there has been increasing demand for
more reliable web applications, JavaScript bugs abound in web
applications. In response to this issue, researchers have proposed
automated fault detection tools, which statically analyze the web
application code to ﬁnd bugs. While useful, these tools either
only target a limited set of bugs based on predeﬁned rules, or
they do not detect bugs caused by cross-language interactions,which occur frequently in web application code. To address this
problem, we present an anomaly-based inconsistency detection
approach, implemented in a tool called H
OLOCRON . The main
novelty of our approach is that it does not look for hard-codedinconsistency classes. Instead, it applies subtree pattern matchingto infer inconsistency classes and association rule mining to
detect inconsistencies that occur both within a single language,
and between two languages. We evaluated H
OLOCRON , and it
successfully detected 51 previously unreported inconsistencies –
including 18 bugs and 33 code smells – in 12 web applications.
Index T erms—JavaScript, fault detection, cross-language inter-
actions
I. I NTRODUCTION
The JavaScript programming language has rapidly grown
in popularity over the past decade, even topping the “Most
Popular Technologies” category of the two most recent Stack-Overﬂow Developer Surveys [1]. Although JavaScript pro-
gramming has extended to the full web stack, its most frequentusage remains at the client-side. Unfortunately, despite itspopularity, JavaScript is still notoriously error-prone [2], and
these errors often lead to high-impact consequences such asdata loss and security ﬂaws [3], [4]. To mitigate this problem,web developers rely heavily on testing, and many tools have
been developed for testing [5], [6], [7], [8], [9].
To complement testing, developers use static code analysis
tools, which ﬁnd bugs by reasoning about the program,
without having to execute it. Several techniques have beenproposed to automatically detect JavaScript bugs through staticanalysis. For example, JSLint [10] detects syntactic errors inJavaScript programs; Jensen et al. [11], [12] analyze JavaScriptcode to ﬁnd type inconsistencies; Ocariza et al. proposeAurebesh [13] for automatically detecting inconsistencies inAngularJS web applications. A common issue with the abovetechniques is that they detect bugs based on a predeﬁned
list of inconsistency rules or bug patterns. As a result, theissues they detect will be limited to those encompassed bythese hardcoded rules. This is especially problematic for webapplications which use a wide variety of frameworks (e.g.,AngularJS, BackboneJS, Ember) and libraries, each with itsown coding rules and conventions. Moreover, web frameworkstypically evolve fast, and hence hardcoded rules may becomeobsolete quickly, thereby necessitating expensive updates.
In this paper, we propose an anomaly-based inconsistency
detection approach for JavaScript-based web applications.Anomaly-based approaches [14] learn the rules based on codesamples, and can hence ﬁnd unknown inconsistencies withouthardcoded rules or patterns. Our approach differs from prioranomaly-based approaches in that it is cross-language and can
hence ﬁnd inconsistencies within and between two different
languages, namely HTML and JavaScript. Prior work hasshown that many cross-language interactions are highly error-prone in web applications [3], and hence it is important toconsider such cross-language inconsistencies. In addition tobugs, JavaScript is also prone to code smells [15], which are
pieces of code that are difﬁcult to maintain, and are therefore
prone to becoming an error when the code is modiﬁed. These
code smells often also manifest as inconsistencies, particularlywhen there is a deviation in the code style. Therefore, byapplying inconsistency detection, we can also detect (many)code smells, because our approach looks for code deviations.
We focus on detecting inconsistencies in MVC applications
– that is, web applications implemented using JavaScriptModel-View-Controller (MVC) frameworks such as Angu-larJS, BackboneJS, and Ember.js. We target these MVC frame-works due to their rising popularity [16], and because they do
not interact directly with the DOM (Document Object Model).
This makes them more amenable to static analysis than non-MVC applications. We make the following contributions:
(1) We demonstrate that there are many inconsistency
classes in MVC applications, and that there is no singleinconsistency class that dominates over the others. Further,many of these inconsistencies span multiple programminglanguages, thereby motivating approaches such as ours;
(2) We propose a technique for automatically detecting
inconsistencies in JavaScript MVC applications. Unlike priorwork, our approach does not look for hard-coded inconsistencyclasses, but instead uses subtree pattern matching to infer theseclasses. Further, it uses association rule mining to ﬁnd thecross-language links;
(3) We implement our technique in a tool called H
OLOCRON
and we evaluate it on 12 JavaScript applications, from threedifferent MVC frameworks. We ﬁnd that H
OLOCRON can ﬁnd
a total of 18 unknown bugs in these applications, ﬁve of which
are cross-language. Further, H OLOCRON ﬁnds 33 code smells
in these applications.
978-1-5386-2684-9/17/$31.00 c/circlecopyrt2017 IEEEASE 2017, Urbana-Champaign, IL, USA
T echnical Research566
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:08:29 UTC from IEEE Xplore.  Restrictions apply. II. B ACKGROUND AND MOTIV A TION
We target a general class of bugs that we call inconsisten-
cies, in JavaScript MVC applications. As the name suggests,
an MVC application consists of a model, which deﬁnes the
application data; a controller, which deﬁnes the functions that
manipulate the values of the application data; and a view,which uses the data and functions deﬁned in the model andcontroller to deﬁne a user interface. Static analysis is sufﬁcientfor MVC applications, as they rely primarily on JavaScriptbindings instead of DOM interactions; hence, even thoughthe DOM still changes, the JavaScript code interacts primarilywith these static bindings instead of directly with the DOM.
A. Deﬁnitions
We deﬁne a code component to be any contiguous piece of
JavaScript or HTML code that could span a single line (e.g.,
function call, HTML text, etc.) or multiple lines (e.g., functiondeﬁnition, view deﬁnition, etc.). These code components canbe represented by subtrees of the JavaScript code’s AbstractSyntax Tree (AST) or the HTML code’s DOM representation;we use these subtrees in our design (Section III).
Deﬁnition 1 (Inconsistency): Two code components C
Aand
CBare inconsistent if CAmakes an erroneous assumption about
CB, where the erroneous assumption can be implicitly inferred
from the code (e.g., without having to rely on speciﬁcations).
The pair ( CA,CB) is an inconsistency.
Therefore, an inconsistency is a bug that can be discovered
without the help of external speciﬁcations, and hence can bedetected through an automated analysis of the web application
code. An inconsistency is considered cross-language ifC
Aand
CBbelong to different programming languages, i.e., in our
work, HTML and JavaScript.
Ideally, we would like to be able to label a web application
as inconsistent by using a search approach that ﬁnds all
inconsistent code components as described above. However, nosuch approach currently exists, nor do we aim to propose suchan approach in this paper. Our goal, rather, is to ﬁnd as manyof these inconsistencies as possible by detecting anomaliesin the AST and the DOM. The approach is described inSection III, and its differences compared to other anomalydetection techniques are outlined in Section VII.
A recent study provides some evidence that four classes
of these inconsistencies occur in MVC applications [13].
For example, the view components in the HTML code use
variables that are erroneously assumed to be deﬁned in themodel components in the JavaScript code. In Section V-D, weﬁnd through a study of bug reports that these inconsistenciesabound in MVC applications, and often go much beyond theclasses found in this prior study. Thus, this prior approach will
not work for these other classes.
B. Motivating Examples
To illustrate the problem, we introduce examples of two real
bugs and one code-smell that result from inconsistencies.
AngularJS Example. In this application [17], the JavaScript
code closes a modal instance by calling the close() method:1$modalInstance.close( 'close' );
However, this leads to incorrect application behaviour (i.e., a
dialog box becomes broken), as the $modalInstance service
has been replaced in the newer version of AngularJS being
used by the application by $uibModalInstance. In this
case, the function call above incorrectly assumes that theservice object being dereferenced is valid, thereby leadingto the inconsistency. This example demonstrates the potential
usefulness of a learning-based approach for ﬁnding theseinconsistencies, as the evolution of framework APIs often
modiﬁes or introduces new coding rules.
BackboneJS Example. In this application [18], the JavaScript
code is attempting to bind an element from an HTML view
template to a layout view object by assigning the elproperty
with an element selector, as shown below.
1Marionette.LayoutView.extend({
2 el: '.some- view' ,
3 ...
4});
In this case, the selector ’.some-view’ does not correspond
to any element in the HTML template, which causes thebinding to fail. In other words, the view incorrectly assumesthat a particular element with the class “some-view” is deﬁnedin the HTML template. This shows the difﬁculty of reasoningabout consistency across languages.
Code Smell. Code smells are important because if ignored,
they are prone to turning into bugs, and can therefore lead the
program to a faulty state. These code smells often manifestas inconsistencies in the JavaScript code, particularly when a
piece of code is ‘smelly’ because a coding style is applied toit that deviates from the coding style adhered to in the rest
of the code. For example, consider a function func() that
takes a number as a parameter. Suppose that in most calls
tofunc(), the argument is passed to func() as a named
constant NUM; however, in one sole call, the argument is passed
tofunc() as a hardcoded number literal, with the same value
asNUM. When the developer updates the value of NUM, they
may forget that the number literal in the latter call also needsto be updated, since it is an additional portion of the code that
needs to be kept track of; in this case, a functional regressionwill be introduced. This example, which we discover to existin several applications as we report later in Section V-E,demonstrates that it is not only functional bugs that manifest
as inconsistencies, but code smells as well.
C. Challenges
One of the main challenges is that we need to infer program-
mer intent in order to label code components as inconsistent.
For example, in the AngularJS example above, how do weknow that $modalInstance is an incorrect service name in
the absence of speciﬁcations? One approach is to leverage
repeated appearances of the same code pattern to infer intent.
Any deviations from this pattern are likely to be inconsis-tencies. Further, the more the examples of the same pattern,
567
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:08:29 UTC from IEEE Xplore.  Restrictions apply. JavaScript
HTMLTransform 
AST and 
DOM into
CodeTreesFind Code 
Patterns 
from 
Subtrees
Infer 
Consistency 
RulesDetect Rule 
ViolationsIntra-Pattern
Rules
Link
RulesInconsistencies
Fig. 1. Overview of our inconsistency detection approach
and the fewer the counterexamples, the more likely it is to be
an actual pattern. In the AngularJS example, there are manyinstances of $uibModalInstance.close(...), which is a
near-match, though the service name is different, indicating
that the service name $modalInstance is incorrect.
Another challenge is that we have to deal with cross-
language inconsistencies, as this forces our design to infer
“links” between code components coming from different pro-gramming languages. For instance, in the BackboneJS exampleabove, our design needs to infer that the value of the el
property needs to be a selector for an element in the HTML
template. We can decide to simply hardcode this relationshipin our detector, but the problem is that this link is speciﬁc tothe BackboneJS framework.
III. A
PPROACH
The block diagram in Figure 1 presents an overview of
our approach. As the diagram shows, our approach takesthe web application’s
1JavaScript and HTML code as input,
and transforms these pieces of code into their correspondingAST and DOM representations, respectively. As explained in
Section III-A, the AST and the DOM trees are transformed
into another tree object called a CodeTree, which allows the
approach to perform standardized operations on those trees. Inaddition to the trees generated from the input web application,our technique also retrieves the AST and DOM of otherweb applications that use the same framework; these webapplications are retrieved from the web (Section III-C).
Once the CodeTrees are generated for the input and sample
code, the approach analyzes the trees to ﬁnd commonlyrepeated patterns in the trees (Section III-B). To do so, itlooks for subtree repeats, which by deﬁnition are subtrees that
appear multiple times in the CodeTrees; these subtree repeats
represent common code patterns in the web application.
After ﬁnding the subtree repeats, the approach examines
each code pattern found in the previous module and formulatesconsistency rules based on them. There are two levels ofconsistency rules, (1) intra-pattern consistency rules, which
are deﬁned by the individual code patterns themselves, and
(2)inter-pattern consistency rules (i.e., link rules ), which are
inferred based on pairs of code patterns. These link rules allow
1From here on, when we say web application, we mean MVC applications.our approach to ﬁnd consistency rules that span code writtenin different languages (Section III-D), unlike prior work.
Finally, our approach ﬁnds inconsistencies, based on a
comparison between the CodeTree objects and the inferred
consistency rules (Section III-E). These represent both codesmells and bugs. Later, in Section V, we demonstrate theusefulness of our approach in detecting bugs and code smells.
A. Transforming Code into Trees
The ﬁrst module of our approach transforms the JavaScript
and the HTML code of the input web application into their cor-
responding AST and DOM representations. More speciﬁcally,an AST is constructed for each JavaScript ﬁle (or JavaScriptcode within the same script tag), and a DOM representation
is created for each HTML ﬁle. These transformations are doneto simplify analysis, as trees are a well-studied data structurefor which many search and comparison algorithms have beenproposed. It also makes our approach easier to extend to otherlanguages, as it does not need complicated parsing algorithmsthat rely on knowledge of the syntax of speciﬁc languages.
In order to standardize the way that our approach operates
on the ASTs and the DOMs, we transform them both into adata structure called the CodeTree.A CodeTree is deﬁned as
a tree T(V,E), where Vis the set of nodes in the tree, and E
is the set of edges. For every node v∈V, we deﬁne:
v.type : Set to “ast” (”dom”) if vis an AST (DOM) node;
v.label : Set to the node label. If vis an AST node, the
label is set to either the node type (e.g., ExpressionStatement,Identiﬁer, etc.), or the corresponding identiﬁer or literal value.Ifvis a DOM node, the label is set to a tag name (Element
node), attribute name (Attribute node), or text (Text node,
or an attribute value);
In addition to the above properties, for each CodeTree node,
we also keep track of its parent and childNodes, as well as
the lineNumber, columnNumber, and sourceFile.
B. Finding Common Patterns
The goal of our next module is to ﬁnd patterns of repeating
subtrees in the CodeTrees. These patterns will form the basis
of the consistency rules. We ﬁrst deﬁne the following:
Deﬁnition 2 (Subtree Repeats): Let T
1,T2,..., TN be
CodeTrees, and let R(Vr,Er)and S(Vs,Es)be two different
subtrees of any of these CodeTrees. Then, Rand Sare
deﬁned to be subtree repeats of each other if Rand Sare
isomorphic, where two nodes are considered equal iff theyhave the same type and label. Hence, each node v
r∈Vrhas
a corresponding node vs∈Vssuch that vr.type =vs.type and
vr.label =vs.label .
Deﬁnition 3 (Code Pattern): Acode pattern Cis deﬁned as
a set of subtrees, such that for every pair of subtrees R,S∈C,
Rand Sare subtree repeats.
Hence, the goal of this module is to ﬁnd all the code patterns
in the CodeTrees generated earlier in the previous module.
Our technique for ﬁnding these code patterns is similar tothe approach used by Baxter et al. [19] to detect clones in thesource code of a program using the AST. More speciﬁcally, our
568
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:08:29 UTC from IEEE Xplore.  Restrictions apply. design looks for all full subtrees in each CodeTree, and assigns
a hash value to each of these subtrees; note that a full subtree
pertains to a subtree that contains all the descendant nodes
from the subtree’s root. All subtrees that hash to the samevalue are placed in their own hash bin. The subtrees in eachhash bin are then compared to detect any collisions; if thereare collisions, the hash bin is split to resolve the collisions.These hash bins represent the code patterns.
The difference with Baxter et al.’s technique is that when
comparing subtrees, our design abstracts out the labels of
nodes pertaining to variable and function identiﬁers, as wellas attribute values. Our design also abstracts out any labelsthat identify the data type of a literal node (e.g., StringLiteral,NumberLiteral, etc.). Doing so enables our design to ﬁnd intra-pattern consistency rules (see Section III-D).
C. Using Code Examples from the Web
In addition to the target application, our design also looks
for patterns that are found in example web applications down-
loaded from web. The purpose of using these example ap-plications is to allow code patterns to appear more frequently,thereby giving our design greater conﬁdence about the validityof the pattern found. Further, using these examples will alsoallow “non-patterns” to appear less frequently, percentage-wise, thereby decreasing the rate of false positives.
The example web applications retrieved – chosen based on a
GitHub search result of the corresponding framework’s name– must use the same framework and framework version asthe target web application, determined via the script tag of
the target web application. In addition, each application mustinclude at least one ﬁle with both a .html extension and a
.js extension, not including library code included in the lib
folder. In our experiments, we choose a total of ﬁve sampleapplications, as we ﬁnd that this number allows our techniqueto ﬁnd enough inconsistencies while keeping the runtime low(see Section V-G).
D. Establishing Rules from Patterns
After ﬁnding the patterns, our design then analyzes these
patterns to infer consistency rules. In this case, the design
looks for both intra-pattern and inter-pattern consistency rules.
1) Intra-Pattern Consistency Rules: As mentioned earlier,
intra-pattern consistency rules are deﬁned by individual codepatterns. Algorithm 1 shows the pseudocode for ﬁnding theserules, and reporting violations. The main idea is to concretize
the nodes that were abstracted out in the previous module.
The algorithm ﬁrst stores each code pattern in a queue
(line 2). For each code pattern Cin the queue, the design
determines the earliest node – in depth-ﬁrst, pre-order – that
is still abstracted out among the subtrees in C. It achieves this
by calling the getNextNodeToConretize() function, which
returns the pre-order number of the earliest node (line 5).
Once the pre-order number of the earliest node is determined,the actual nodes in the subtrees in Cthat correspond to this
pre-order number are compared and marked as concretized
(lines 11-12), and the subtrees are partitioned according to theAlgorithm 1: FindIntraPatternInconsistencies
Input: Cset: The set of code patterns
Input: t: The threshold for dominant subpatterns
Output: PI: Set of intra-pattern inconsistencies
1PI← /0,remaining ← /0;
2codePatternQueue ←{ C|C∈Cset};
3 while codePatternQueue is not empty do
4 C←codePatternQueue.dequeue();
5 preorderNum ←getNextNodeToConcretize(C);
6i f preorderNum <1then
7 remaining ←remaining ∪{C}; continue;
8 end
9 subPatterns ← /0;
10 foreach subtree S ∈Cdo
11 node←getPreOrderNode(S, preorderNum);
12 markAsConcretized(node);
13 if subPatterns.hasKey(node.label) then
14 subPatterns[node.label].add(S);
15 end16 else
17 subPatterns[node.label] = {S};
18 end
19 end20 D←getDominantPattern(subPatterns);
21 if 100|D|
|C|>= tthen
22 expected ←getPreOrderNode(D[0], preorderNum);
23 foreach code pattern CP ∈subPatterns do
24 ifCP/negationslash=Dthen
25 foreach subtree S ∈CP do
26 inc←getPreOrderNode(S, preorderNum);
27 PI←PI∪{(inc, expected )};
28 end
29 end
30 end
31 codePatternQueue.enqueue(D);
32 end33 else
34 codePatternQueue ←codePatternQueue ∪subPatterns;
35 end36 end37 C
set←mergeRemaining(remaining);
label of the concretized node (lines 13-18). The partitions are
included in an associative array called subPatterns (line 9).
Once the partitions are found, the algorithm looks for
the dominant pattern, which represents the largest partition
(line 20). If the number of subtrees in the dominant patternconstitutes greater than t% of all the subtrees in the original
code pattern C, where tis a user-set threshold, all the subtrees
belonging to the non-dominant patterns are considered intra-
pattern inconsistencies (lines 22-32) and are discarded; here,an intra-pattern inconsistency is represented by a tuple of the
inconsistent node – i.e., the node that was just concretized inthe inconsistent subtree – and the expected node – i.e., thenode that was just concretized in any subtree belonging to the
dominant pattern (line 27). This process is repeated until thereare no further nodes to concretize, after which all remainingpartitions belonging to the same original code pattern at the
start of the algorithm are merged (line 37).
As an example, consider the subtrees in Figure 2, which
form a code pattern; this code pattern is found in the AngularJS
example introduced in Section II-B. Here, the current nodebeing concretized is the left-most leaf node of each subtree,which, in this case, represents the name of the service beingdereferenced. The subtrees are then partitioned according tothe label of this concretized node. In this case, there are twopartitions – one containing the left-most subtree, with the
569
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:08:29 UTC from IEEE Xplore.  Restrictions apply. CallExpression
MemberExpression StringLiteral
Identiﬁ er Identiﬁ er "close"
$modalInstance closeCallExpression
MemberExpression StringLiteral
Identiﬁ er Identiﬁ er "close"
$uibModalInstance close...CallExpression
MemberExpression StringLiteral
Identiﬁ er Identiﬁ er "close"
$uibModalInstance closeCallExpression
MemberExpression StringLiteral
Identiﬁ er Identiﬁ er "close"
$uibModalInstance close
Fig. 2. Example of an intra-pattern consistency rule violation
Algorithm 2: FindLinkRules
Input: (Cfr o m,Cto): Pair of code patterns
Output: L: Set of link rules
1L← /0;
2 foreach (Sfr o m,Sto)∈Cfr o m×Ctodo
3 i←1;
4 node fr o m←getPreOrderNode(S fr o m ,i);
5 while node fr o m/negationslash=null do
6 j←1;
7 node to←getPreOrderNode(S to,j);
8 while node to/negationslash=null do
9i fnode fr o m/negationslash=node toand node fr o m.label =node to.label then
10 lr←(i,Sfr o m,j,Sto);
11 L←L∪{lr}
12 end
13 node to←getPreOrderNode(S to,+ + j);
14 end
15 node fr o m←getPreOrderNode(S fr o m ,+ + i);
16 end
17 end
concretized node coloured red, and another containing the rest
of the subtrees, with the concretized node coloured blue. Thelatter partition is deemed to be dominant, so the subtree in theother partition is labeled as inconsistent.
2) Inter-Pattern Consistency Rules (i.e., Link Rules): In
addition to ﬁnding the intra-pattern consistency rules, ourdesign also looks for consistency rules that describe therelationship between code patterns - we call these link rules.
This process allows our design to ﬁnd relationships betweenpieces of code in the same programming language and acrosslanguages, i.e., cross-language relationships. All link rules are
of the following form: The i
thpre-order node in Subtree S 1
is equal to the jthpre-order node in Subtree S 2. Our design
ﬁnds the link rules for each pair of code patterns (Cfr o m,Cto),
as shown in Algorithm 2. In this case, the algorithm iteratesthrough every pair of subtrees between the two code patterns(line 2). For each of these pairs of subtrees, the algorithm goesthrough every pair of nodes between the two subtrees (lines 3-
16), and compares the two nodes to see if they have the samelabel. If they have the same label, a new link rule is added tothe list, uniquely identiﬁed by the subtree pair S
fr o m and Sto,
and their respective pre-order indices iand j.
E. Detecting Violations
Violations to the intra-pattern consistency rules are detected
in conjunction with ﬁnding those rules, as described in Sec-
tion III-D1. For the link rules, we make a distinction betweenunconditional and conditional link rule violations.
1) Unconditional Link Rule Violations: A link rule viola-
tion is unconditional if the link rule is violated by a codecomponent regardless of where the component is located
in the code. The BackboneJS example (Section II-B) is anunconditional link rule violation. To determine whether a linkrule lris violated, our design examines each pair of code
patterns C
fr o m and Cto, as before. It then determines which
pairs of subtrees between Cfr o m and Ctosatisfy lr. There are
two ways in which a subtree can be an inconsistency.
First, if a subtree Sfr o m∈Cfr o m does not satisfy the link
rule lrwhen paired with any subtree Sto∈Cto,and a large
percentage pv% (a parameter chosen by the user) of the other
subtrees in Cfr o m satisfy lrat least once, then Sfr o m will
be considered an inconsistency. For instance, the left box in
Figure 3 shows the code pattern to which the inconsistentcode in the BackboneJS example (Section II-B) belongs. Asindicated by the arrows in this ﬁgure, almost each subtree inthis code pattern corresponds to a class attribute deﬁnition inthe HTML code (right box in Figure 3); the only exception isthe subtree with the node highlighted in red (‘‘some-view’’).
This subtree is labeled an inconsistency, assuming pv≤75%.
el
Property
"some-view"
el
Property
"some-region"
el
Property
"layout"
el
Property
"main"
Attribute
class
 "some-region"
Attribute
class
 "layout"
Attribute
class
 "main"
Code Pattern from the 
HTML CodeCode Pattern from the 
JavaScript Code
Fig. 3. Example of an unconditional link rule violation. The subtrees are
slightly altered for simplicity.
Second, if a subtree Sfr o m∈Cfr o m does not satisfy the link
rule lrwhen paired with a speciﬁc subtree Sto∈Cto, and a
large percentage of the other subtrees in Cfr o m satisfy the link
rule lrwith Sto, then Sfr o m will also be an inconsistency.
2) Conditional Link Rule Violations: A link rule violation
is conditional if the link rule is violated given that the code
component is located in a speciﬁc area in the code. For
example, suppose a view Vin the HTML code is associated
with a model Min the JavaScript code. Further, suppose
that the following link rule has been found: The identiﬁer
<x> in the subtree with pattern ng-model=‘<x> ’is equal
to the identiﬁer <y> in the subtree with pattern $scope.< y>.
In this case, if there exists no subtrees in the model M with
pattern $scope.< y> that matches a certain subtree in the view
Vwith pattern ng-model=‘< x>’, then this latter subtree is
570
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:08:29 UTC from IEEE Xplore.  Restrictions apply. considered a violation of the link rule (i.e., Vis using an
identiﬁer that is undeﬁned in the corresponding model M).
Because this link rule violation only occurs given that the
subtrees being compared are located in Mand V, this is a
conditional link rule violation.
To ﬁnd the conditional link rule violations, we use a well-
known data mining technique called association rule learn-
ing [20]. This technique takes a set of transactions as input,
where each transaction contains a set of items that apply to
that transaction. Based on an analysis of these transactions,
the technique looks for rules of the form {a1,a2,..., an}⇒
{b1,b2,..., bm}, where both the left and right side of the
implication are subsets of all the items. In addition, the
technique only reports rules that exceed a particular conﬁdence
value, i.e., the percentage of transactions that follow the rule.
Hence, when ﬁnding the conditional link rule violations
between pairs of code patterns Cfr o m and Cto, we create
a transaction for each subtree pair (S fr o m,Sto). The items
included in each transaction include all the link rules satisﬁedby the subtree pair, as well as the ancestor nodes of theroot of each subtree; these ancestor nodes dictate which
areas in the source code the subtrees are located. We usethe apriori algorithm [21] to infer association rules with a
conﬁdence value greater than a user-set parameter cv%; we are
particularly interested in association rules of the form {an
fr o m,
anto}⇒{ lr}, where anfr o m and antoare ancestor nodes of the
subtrees Sfr o m and Sto, respectively, and lris a link rule. These
rules are compared against each subtree pair; non-satisfying
subtree pairs are reported as inconsistencies.
IV . I MPLEMENTA TION
We implement our technique in an open-source tool called
HOLOCRON2HOLOCRON is implemented in JavaScript as a
plugin for Brackets, which is an Integrated Development Envi-
ronment (IDE) for web development developed by Adobe [22].To use H
OLOCRON , the user only needs to specify the top
folder of the target web application. The output of the toolis a list of the inconsistencies found; each inconsistency is
shown to the user as a message identifying the inconsistent
line of code, and an example of what is expected based on theconsistency rule. The JavaScript code is parsed into an ASTusing Esprima [23], and the HTML code is parsed into itsDOM representation using XMLDOM [24]. For ﬁnding theassociation rules, we adopt an existing implementation of the
apriori algorithm [25].
V. E
V ALUA TION
A. Research Questions (RQs)
RQ1 (Prevalence of Inconsistencies): Do inconsistencies
occur in MVC applications and if so, what are the
characteristics of these inconsistencies?
RQ2 (Real Bugs and Code Smells): Can H OLOCRON be
used by developers to detect bugs and code smells in
real-world MVC applications?
2http://ece.ubc.ca/ ∼frolino/projects/holocron/RQ3 (Thresholds): How generalizable are the user-deﬁned
thresholds across applications?
RQ4 (Performance): How quickly can H OLOCRON detect
inconsistencies?
B. Subject Systems
For our experiments which answer RQ2 to RQ4, we con-
sider four open-source applications from each of the three
main MVC frameworks (AngularJS, BackboneJS, and Em-ber.js), for a total of 12 applications. These three frameworks
are the most widely used JavaScript MVC frameworks, ex-
periencing a 538% growth in popularity from January 2013to April 2016 [26]. The applications are listed in Table I,with the sizes ranging from 6 to 43 KB (185-1659 LOC).These sizes are representative of popular MVC applications;for example, a sample of 10 GitHub projects with at least 50
stars that use AngularJS (retrieved from the top 10 GitHub
’issue’ search results) has an average of 1689 lines of code,and a median of 1104 LOC. In addition, half of these projectshave fewer than 1000 LOC, not including libraries. Theseapplications were taken from various lists of MVC applicationsavailable on GitHub [27], [28], [29]. In particular, we chose
the ﬁrst four applications from each framework found from
these lists, ﬁltering out the applications that did not have aGitHub repository and a working demo, as this simpliﬁed thetask of reproducing the functional bugs found by our tool.
C. Experimental Methodology
Prevalence of Inconsistencies (RQ1). To answer RQ1, we
manually analyze bug reports that have been ﬁled for MVC
applications on GitHub. More speciﬁcally, we examine 30 bugreports for applications implemented in each of the three mainMVC frameworks – AngularJS, BackboneJS, and Ember.js –for a total of 90 bug reports. We only consider ﬁxed or closedbugs to prevent spurious reports. To ﬁnd the bug reports, weuse GitHub’s advanced search feature, searching in particularfor GitHub issues that are given the label “bug”, and whosestatus is “closed”. We perform the same search for each ofthe three MVC frameworks, using the keywords “angularjs”,
“backbone”, and “emberjs”, respectively. We discard any
search results that correspond to applications not written in
any of these three frameworks, as well as results that do notpertain to the web application’s client-side code. We then takethe ﬁrst 30 bug reports that satisfy the conditions describedfrom each of the three search results, and use those bug reportsfor our analysis. Note that we did not conﬁne ourselves to
the 12 subject systems listed in Section V-B. Further, notethat even though H
OLOCRON could be applied to the web
applications we encountered in RQ1, we opted not to use theseweb applications as the subjects for RQ2-RQ4. This is becausethe bugs are dispersed across many different web applications,and it would be infeasible for us to perform tests on each one.
For each of the bug reports, we ﬁrst determine whether the
bug corresponds to an inconsistency, as deﬁned in Section II.If so, we determine the bug’s inconsistency category, which is
deﬁned in Section II-A; some categories may possibly contain
571
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:08:29 UTC from IEEE Xplore.  Restrictions apply. only one inconsistency. We also determine whether the bug is
a cross-language one.
Real Bugs and Code Smells (RQ2). For RQ2, we run
HOLOCRON on the subject systems described in Section V-B
and record all the inconsistencies reported by the tool. We
examine each of these reported inconsistencies to determineif it corresponds to a real bug (i.e., it represents an errorthat leads the application to a failure state) or code smell. Todetermine if a reported inconsistency represents a code smell,we compare the reported inconsistency with the examples of‘correct lines’ provided by H
OLOCRON (Section IV), and we
qualitatively assess whether this deviation has any negativeimpact on maintenance.
We set the intra-pattern violation threshold tto 90%, the
unconditional link rule violation thresholds pvto 95%, and
the conditional link rule violation threshold cvto 85%; in
Section V-F, we investigate the thresholds’ generalizability.
Finally, using the criteria outlined in Section III-C, we
use example code from ﬁve open-source web applicationsto train the analysis with more samples. These ﬁve applica-tions include the other three subject systems using the sameframework (e.g., if the target web application is angula-
r-puzzle, we include the three other subject systems thatalso use AngularJS, namely projector, cryptography, and
twittersearch as example code), as well as two additional
applications – using the same MVC framework – found onGitHub [27], [28], [29]. We report the number of bugs andcode smells found by H
OLOCRON , as well as its precision
(i.e., number of bugs and code smells per inconsistency).
Thresholds (RQ3). To answer RQ3, we perform a 4-fold
cross-validation to test if the applications tested in RQ2 have
similar accuracy at the given thresholds (i.e., t= 90%, pv=
95%, and cv= 85%). If they have similar accuracy, it would
demonstrate the generalizability of the chosen thresholds,which would strongly indicate they can safely be used whenrunning H
OLOCRON for other applications. When perform-
ing the cross-validation, the applications are partitioned intofour groups, with each group containing three applications
(one from each framework). At each iteration of the cross-
validation, we use one partition as the training set, and wecalculate the aggregated precision value for the applicationsin that partition; we then calculate the mean-squared error(MSE) of the remaining applications’ precision values, withrespect to the aggregated value of the training set. Note
that we only consider the reported violations correspondingto each parameter; for example, when varying the intra-
pattern violation threshold t, we only consider the intra-pattern
violations when calculating precision.
Performance (RQ4). We measure the time taken by
H
OLOCRON to run on each subject application. We run our
experiments on a Mac OS/X 10.6.6 machine (2.66 GHz Intel
Core 2 Duo, with 4 GB RAM).
D. Prevalence of Inconsistencies (RQ1)
Of the 90 bug reports we studied, we found that 70% of
these bug reports correspond to an inconsistency. These didAngularJS BackboneJS Ember.js AllNot Inconsistency
Inconsistency
MVC Framework% of Bug Reports
0 20 40 60 80 100
Fig. 4. Percentage of bug reports classiﬁed as an inconsistency for each MVC
framework
1 2 3 4 5 6 7
FrequencyNumber of Categories
0 5 10 15 20 25 30
Fig. 5. Number of inconsistency categories with a particular frequency. Mostinconsistency categories have just 1-2 inconsistencies in them. This shows
why we need an automated approach to infer the inconsistency categories.
not need the application’s speciﬁcations to detect, pointing to
the promise of a tool such as ours which ﬁnds inconsistencies.For example, one of the Ember.js applications passed a modalobject to the buildUrl() method, even though this method,
which is part of the Ember.js API, expects a string as itsﬁrst parameter. This inconsistency could be inferred based onother usages of the method which were correct. The remaining30% of the bugs, however, required prior knowledge of theapplication’s speciﬁcations. For example, one of the bugs wascaused by the fact that the programmer did not update thedisplay style of an element to “block”. Prior speciﬁcation
was needed to establish that the programmer intended tomodify the style to “block”, and hence this bug would notbe detected by our approach.
The per-framework results are summarized in Figure 4. As
this ﬁgure shows, 73% of bug reports correspond to inconsis-tencies for the AngularJS and Ember.js applications, and 63%of bug reports correspond to inconsistencies for BackboneJS
applications. These results suggest that inconsistencies areprevalent in web applications created using JavaScript MVC
frameworks. We further found that 35% of the inconsistencies
are cross-language. For example, one of the bugs resulted fromthe programmer erroneously using the data-src attribute
instead of the src attribute in the HTML code, which led
to incorrect bindings with the JavaScript code. Therefore,existing tools that are based on a single language will not beable to detect a signiﬁcant percentage of these inconsistencies.
Figure 5 shows the distribution of inconsistency categories
we found; again, an inconsistency category is uniquely iden-
572
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:08:29 UTC from IEEE Xplore.  Restrictions apply. TABLE I
NUMBER OF REAL BUGS FOUND PER APPLICA TION ,WITH
CORRESPONDING ISSUE NUMBERS IN PARENTHESES (SOME ISSUES ARE
AGGREGA TED INTO ONE BUG REPORT ). T HE SIZE IS SHOWN IN KB, WITH
LINES OF CODE (LOC) IN PARENTHESES .
Framework Application Size (loc) #o f Code Total
Bugs Smells
AngularJS angular-puzzle 20 (608) 3 (4,5) 3 8
projector 19 (569) 3 (1) 0 7
cryptography 20 (582) 1 (2) 0 2
twittersearch 10 (357) 1 (1) 0 1
BackboneJS cocktail-search 10 (396) 2 (13) 6 13
contact-manager 19 (701) 2 (1,2) 2 9
webaudiosequencer 43 (1659) 1 (1) 7 15
backbone-mobile 9 (240) 1 (3) 0 2
Ember.js todomvc 8 (299) 0 2 3
emberpress 21 (610) 2 (5,6) 11 22
giddyup 12 (386) 1 (189) 2 9
bloggr 6 (185) 1 (17) 0 4
OVERALL 18 33 95
tiﬁed by the two components that are inconsistent, as well as
the incorrect assumption made by one of the components. Asthis ﬁgure illustrates, most inconsistency categories – which is
deﬁned in Section V-C – appear only once in the bug reportswe studied; for example, 30 categories had only a singleinconsistency each. Further, we found a total of 41 differentinconsistency categories in our experiment. The large number
of categories suggests that there are many different rules thatare used by programmers in writing JavaScript MVC based
applications, and hence motivates an approach that discoversthe rules automatically (such as ours).
E. Real Bugs and Code Smells (RQ2)
Table I shows the result of running H
OLOCRON on the
subject systems. In total, H OLOCRON was able to detect 18
unreported bugs from 12 MVC applications. We have reported
these bugs to the developers, with two of the bugs’ descriptions
acknowledged (i.e., those from cocktail-search), and the
rest still pending3; further, we were able to manually reproduce
the bugs and conﬁrm them ourselves. As seen in this table,
HOLOCRON was able to ﬁnd a bug in allof the applications
tested, except for one (todomvc). Further, H OLOCRON found
12 unconditional link rule violations, 4 conditional link ruleviolations and 2 intra-pattern consistency rule violations. Thus,
H
OLOCRON can be used by web developers to ﬁnd bugs
representing various types of consistency rule violations.2
Further, out of 18 real bugs found, 5 were cross-language
inconsistencies. The bug in cryptography, for instance, re-
sults from an assignment in the JavaScript code incorrectly
assuming that an element in the HTML code has a numerical
value, even though it is a string. In addition, the bug intwittersearch resulted from a controller in the JavaScript
code assuming that an input element in the HTML code hasits type attribute deﬁned, which is not the case. Detectionof these cross-language inconsistencies is made possible by
H
OLOCRON looking for link rules in the applications.
3Some of these applications are maintained sporadically, which could
explain the delay in response from the developersIn all, H OLOCRON was able to ﬁnd bugs spanning ﬁfteen
inconsistency categories. For example, the bug in giddyu-
pis caused by a property assignment erroneously assuming
that it has a corresponding route assignment in the Ember.jsrouter. In contrast, Aurebesh [13], which also targets MVCapplications, will not be able to detect this bug because (1)it only considers four pre-determined inconsistency categories,to which this inconsistency found in giddyup does not belong,
and (2) it only works for AngularJS applications, whereas g-
iddyup is developed with Ember.js. In total, Aurebesh only
detected two of the inconsistencies that H
OLOCRON identiﬁed,
both from angular-puzzle, across all applications.
Finally, many of the bugs found have potentially severe
consequences on the application. For example, the bugs inprojector – which are caused by an incorrect assumption
about the type of value being assigned to an object property– all cause the application to hang. Further, the bug inwebaudiosequencer, an audio player, makes an audio clip un-playable after a sequence of input events. Thus, H
OLOCRON
ﬁnds bugs that potentially have high impact on the application.
Code Smells. We also found that 33 of the reported incon-
sistencies correspond to code smells, the detection of which
could help the developer improve the quality of the code. Thecode smells found in our evaluation belong to three categories:
An inconsistency is labeled as a “Hardcoded Constants”
(HC) code smell if (1) the inconsistent code uses a hardcoded
literal value in calculations and method calls; (2) other similar
pieces of code use a non-hardcoded value (e.g., variables,named constants, etc.); and (3) the hardcoded literal value
could safely be replaced with a non-hardcoded value to make itmore maintainable. We found 15 instances of HC code smells.
Further, an inconsistency is labeled as an “Unsafe Value
Usage” (UVU) code smell if an object is dereferenced without
accounting for the possibility of it being null, even though a
null check is applied to similar objects in other parts of thecode, or a “safer” object is used to call the same method inother parts of the code. For instance, in emberpress, one of
the inconsistencies occurs as a result of the get() method
being called directly through a model object repeatedly; this
is potentially unsafe if the object is null, and it is good
practice to call the method via the Ember object instead
(i.e., Ember.get()), as is done in other lines of code. We
found 14 instances of UVU code smells.
Finally, an inconsistency is labeled as a “Multi-Purpose
Identiﬁers” (MPI) code smell if the same identiﬁer is being
used for multiple unrelated objects. This code smell manifests
as an inconsistency when the identiﬁer is used in a speciﬁcway in some parts of the code, but used in a different way inanother part of the code. For example, in angular-puzzle,
the identiﬁer “src” is used both as a class name for a div
element in the HTML code and as a name for a puzzle object
in the JavaScript code. We found 4 instances of such smells.
Precision. Like most static analysis tools, H
OLOCRON incurs
false positives (i.e., inconsistencies that are neither bugs nor
code smells). False positives occurred in all but one of the
573
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:08:29 UTC from IEEE Xplore.  Restrictions apply. applications (twittersearch). In total, H OLOCRON reported
95 inconsistencies, 51 of which were either real bugs or code
smells. Hence, approximately half the reports are potentially
useful in improving the web applications’ quality.
The remaining 44 reports are false positives. While the
false positive rate may seem high to users of static analysis
tools [30], [31], we believe that they will not deter signiﬁcantlythe usability of H
OLOCRON , based on our own analysis of the
false positives that appeared in our experiment. In particular,
we found that these false positives fall under two very speciﬁccategories. First, we found that 22 of these 44 (i.e., half) false
positives occur because of frequent usage of certain kinds of
literals, contrasted with infrequent usage of another kind ofliteral. For example, in projector, most of the object method
calls take string literals as parameters, but there are three suchcalls that take number literals as parameters, which thoughcorrect, are reported as inconsistencies. In our experiments, ittook us only an average of 1–2 minutes to discard each falsepositive of this kind, without prior knowledge of the code base.
Additionally, the other 22 false positives occur because of
frequent usage of a certain identiﬁer contrasted with infrequentusage of another identiﬁer. For instance, in angular-puzzle,
there are two main arrays, both of which are accessed throughthethis identiﬁer – words andgrid. While grid is used
almost 20 times in the code, words is used only twice, leading
to the false positive. We found these false positives to betrickier to discard – again, given our limited knowledge ofthe code base – since it was not immediately clear to uswhat certain identiﬁers are being used for; on average, ittook us about 5 minutes before deciding to discard falsepositives of this kind. We strongly suspect, however, that thosefamiliar with the code base would also be able to discardmany of these false positives very quickly. For instance, in theangular-puzzle example, it was evident that words should
not be replaced by grid, since words is being used in a
function whose purpose is to iterate through previously foundwords in the puzzle (represented by words). Further, for each
inconsistency, H
OLOCRON includes examples of the correct
lines of code that the reported inconsistency deviates from,which would help the developers gain more context about whythe inconsistency is reported and determine if it is valid.
F . Thresholds (RQ3)
As mentioned earlier, we used 4-fold cross validation to
investigate the generality of the user-deﬁned thresholds and
measured the MSE. The MSE can range between 0 and 1,with lower values indicating lower error, and are hence better.When setting the intra-pattern violation threshold to the oneused in RQ2 experiment (i.e., 90%), the average MSE =0.198.
An MSE of 0.198 is non-negligible, as it indicates that
the precision can vary by as much as 40%. However, closerinvestigation reveals that the aggregated precision for all theapplications when considering only intra-pattern violations is50%, and only one application out of 12 fell below this average
(contact-manager). Similarly, when setting the unconditionallink rule violation threshold to 95%, the average MSE value is0.126; the aggregated precision for all the applications when
considering only unconditional link rule violations is 67.35%,
and only three of the 12 applications (cocktail-search,giddyup, and bloggr-client) fell below this precision value.
These results indicate that the error represented by the MSEvalues primarily represent how much the aggregated precisionvalues are exceeded by the applications; for example, for the
intra-pattern inconsistencies, two of the applications ( todom-
vcandgiddyup) had perfect precision, and the remainder of
the applications had precisions within the range 50% to 60%.Hence, the precision values reported earlier are conservativeones, and the thresholds chosen work well across the appli-
cations, showing their generality. Note that the MSE value
is 0 when setting the conditional link rule violation threshold
to the chosen percentage (i.e., 85%), as the applications inour experiment had perfect precision when only consideringconditional link rule violations.
G. Performance (RQ4)
We ﬁnd that on average, H
OLOCRON ran for 1.14 minutes
for each of the 12 applications. Because H OLOCRON will typ-
ically be run prior to deployment of the application, this is an
acceptable overhead. The worst-case overhead occurred withwebaudiosequencer – which is also the largest application
– where H
OLOCRON ran for almost 8 minutes; however, note
that many popular MVC applications are smaller in size thanthis on average (Section V-B). In this case, most of the timewas spent on ﬁnding the link rule violations, as all pairs ofsubtree classes are compared with each other, as well as all
subtrees within these classes.
H. Threats to V alidity
An external threat to validity is that we used a limited
number of applications in our evaluation. To mitigate this
threat, we chose applications coming from three popular MVCframeworks, sizes, and various application types, as seen inTable I. Further, for our study of bug reports in RQ1, wecategorize the inconsistencies based on a qualitative analysisof the reports, some of which may be misleading. To mitigatethis, we also look at other aspects of the bug, including patchesand commits associated with the bug. Finally, the bugs thatwe studied for RQ1 are limited to ﬁxed bugs with the label“bug” and with the status “closed”; however, many GitHubdevelopers do not label bug reports, and hence we may havemissed certain bugs in our analysis. Nonetheless, we choosebug reports this way to minimize spurious or invalid bugs.
VI. D
ISCUSSION
The main assumption behind our approach is that there are
sufﬁcient examples of a consistency rule that appear for it toboth successfully learn the consistency rule and detect anyviolations to that rule. While this is the case for large webapplications, small web applications have very few samples to
learn from, and hence may incur large numbers of false posi-
tives and false negatives. To mitigate this problem, we augmentour code patterns with subtrees found in code examples from
574
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:08:29 UTC from IEEE Xplore.  Restrictions apply. other web applications, as discussed in Section III-B. Doing
so allows our design to be more conﬁdent about the validityof the consistency rules, as well as “debunk” any consistencyrules that may lead to false positives. In fact, we found thatwithout the example code, our false positive rates more thandoubled. Using more and better examples can likewise bringdown the false positive rate. This is a subject of future work.
Furthermore, our main focus in this paper is in using
H
OLOCRON to detect inconsistencies in MVC applications.
Nonetheless, our design can be run on web applications usingnon-MVC JavaScript frameworks, such as jQuery. This maylead to a large number of inaccuracies, as the JavaScriptcode in these frameworks interacts directly with the DOM,
which undergoes many changes throughout the execution ofthe web application. However, H
OLOCRON may be able to
detect inconsistencies within the JavaScript code, as well as
inconsistencies between the JavaScript code and any com-
ponent of the DOM that does not get modiﬁed. To detectthe remaining inconsistencies, we may need to use dynamicanalysis - this is also a subject for future work.
VII. R
ELA TED WORK
Bug and Code Smell Detection. Considerable work has been
done on software fault detection [32], [33], [34], [35], [36],
[37], [38], [39], [40] and code smell detection [15], [41],[42], [43] by identifying code patterns. An alternate approachfor ﬁnding bugs is anomaly detection. This technique wasproposed by Engler et. al. [14] and commercialized as theCoverity [44] tool, which also supports JavaScript to someextent. Instead of hardcoding rules as the above techniquesdo, this approach looks for deviant behaviours in the input
application’s code, with these deviations providing an indi-cation of potential bugs in the program. Reiss [45] has also
proposed a similar tool that ﬁnds “unusual code” in programs.
This approach has the advantage that it can learn rules fromcommon patterns of behaviour, and hence the rules do notneed to be updated for each framework.
The main difference with our work is that these prior
techniques do not use subtree patterns as the basis forthe consistency rules, and they also cannot detect cross-language inconsistencies, as they implicitly assume a single-
language model. Further, static analysis techniques such asFindBugs [46] detect faults based on hardcoded rules or bug
templates. Additionally, dynamic analysis techniques such asDLint [47] check consistency rules based on “bad codingpractices”. As shown in our evaluation (RQ1), this can lead
to many missed bugs, especially for JavaScript MVC applica-tions, as there are no speciﬁc inconsistency categories that
dominate the others. Further, the frameworks used in webapplications evolve fast.
Tools such as Flow [48] and TypeDevil [49] are capable
of ﬁnding type-related faults. These tools analyze the staticor dynamic data ﬂow of the program to ﬁnd these faults,and hence this approach will not be able to detect faults thatdo not stem directly from this data ﬂow. While TypeDevilalso leverages the structure of dynamically observed data toﬁnd inconsistent types, it does not consider the link rules;
hence, unlike H
OLOCRON , it will not be able to detect cases
where a variable deﬁned in the model is assigned a type
inconsistent with how it is used in the HTML templaterepresenting the view (e.g., a variable assigned a string beingused as the value for the “count” attribute in AngularJS, whichexpects a number). In addition, Nguyen et al. [50] propose atechnique that can detect dangling (i.e., undeﬁned) referencesin JavaScript code that is generated from PHP code; however,unlike our tool, this technique is only capable of ﬁnding theseinconsistencies if the references are embedded in the PHPstrings that generated them. In most of the applications we
studied, PHP is not used (nor any other server-side scriptinglanguage), and hence this approach would not work.
Lastly, H
OLOCRON shares some similarities with A U-
REBESH [13] in terms of its goal, namely detecting inconsis-
tencies in JavaScript MVC applications. However, H OLOCRON
conceptually differs from A UREBESH in three ways. First, A U-
REBESH only supports AngularJS, while H OLOCRON supports
two of the most commonly used MVC frameworks in addition
to AngularJS. Secondly, H OLOCRON is also able to detect
more types of inconsistencies, as it infers the consistency
rules automatically instead of hardcoding them into the design;
indeed, as we pointed out in Section V-E, A UREBESH only
detected 2 of the 18 bugs that H OLOCRON identiﬁed. Finally,
HOLOCRON is able to also infer cross-language relations
between JavaScript and HTML, while A UREBESH is not.
Cross-Language Computing. Much of the work done on
cross-language computing has focused on detecting the depen-
dencies between multiple programming languages [51], [52].Only a few techniques perform analysis in a cross-language-aware manner, including XLL [53] and X-Develop [54], both
of which perform code refactoring. In recent work, Nguyen
et al. [55] proposed a tool to perform cross-language programslicing for web applications, with particular focus on PHP codeand its interaction with client-side code. Unlike H
OLOCRON
however, none of these above techniques deal with the incon-
sistencies in cross-language interactions.
VIII. C ONCLUSIONS
We presented an automatic fault detection technique that
ﬁnds inconsistencies in JavaScript MVC applications. Our
technique analyzes the AST and DOM representations of theweb application code, and it ﬁnds both intra-pattern consis-tency rules and link rules; violations to these rules are thereby
reported to the user. We implemented this approach in an open-
source tool called H
OLOCRON , and in our evaluation of 12
open-source MVC applications, H OLOCRON was able to ﬁnd
18 previously unreported bugs, and a signiﬁcant number (33)of code smells. Further, it took just over a minute on average.
A
CKNOWLEDGMENT
This research was supported in part by the Natural Sciences
and Engineering Research Council of Canada (NSERC), and aresearch gift from Intel Corporation. We thank the ASE 2017reviewers for their insightful comments.
575
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:08:29 UTC from IEEE Xplore.  Restrictions apply. REFERENCES
[1] StackOverﬂow, “StackOverﬂow Developer Survey 2015,” 2015,
http://stackoverﬂow.com/research/developer-survey-2015#tech
(Accessed: May 16, 2015).
[2] F. Ocariza, K. Pattabiraman, and B. Zorn, “JavaScript errors in the wild:
an empirical study,” in Proceedings of the International Symposium on
Software Reliability Engineering (ISSRE). IEEE Computer Society,2011, pp. 100–109.
[3] F. Ocariza, K. Bajaj, K. Pattabiraman, and A. Mesbah, “A study of causes
and consequences of client-side JavaScript bugs,” IEEE Transactions on
Software Engineering (TSE), p. 17 pages, 2017.
[4] G. Richards, C. Hammer, B. Burg, and J. Vitek, “The eval that men do:
A large-scale study of the use of eval in JavaScript applications,” in Pro-
ceedings of the European Conference on Object-Oriented Programming(ECOOP). Springer, 2011, pp. 52–78.
[5] A. Marchetto, P . Tonella, and F. Ricca, “State-based testing of Ajax
web applications,” in Proceedings of the International Conference on
Software Testing, V eriﬁcation and V alidation (ICST). IEEE ComputerSociety, 2008, pp. 121–130.
[6] A. Mesbah and A. van Deursen, “Invariant-based automatic testing of
Ajax user interfaces,” in Proceedings of the International Conference
on Software Engineering (ICSE). IEEE Computer Society, 2009, pp.210–220.
[7] K. Pattabiraman and B. Zorn, “DoDOM: leveraging DOM invariants
for web 2.0 application robustness testing,” in Proceedings of the
International Symposium on Software Reliability Engineering (ISSRE).IEEE Computer Society, 2010, pp. 191–200.
[8] S. Artzi, J. Dolby, S. Jensen, A. Møller, and F. Tip, “A framework for
automated testing of JavaScript web applications,” in Proceedings of
the International Conference on Software Engineering (ICSE). ACM,2011, pp. 571–580.
[9] S. Mirshokraie, A. Mesbah, and K. Pattabiraman, “Guided mutation
testing for JavaScript web applications,” Transactions on Software
Engineering (TSE), vol. 41, no. 5, pp. 429–444, 2015.
[10] Douglas Crockford, “JSLint,” 2012, http://www.jslint.com (Accessed:
April 18, 2012).
[11] S. Jensen, A. Møller, and P . Thiemann, “Type analysis for JavaScript,”
Proceedings of the International Static Analysis Symposium (SAS), pp.238–255, 2009.
[12] S. H. Jensen, M. Madsen, and A. Møller, “Modeling the HTML DOM
and browser API in static analysis of JavaScript web applications,” in
Proceedings of the Joint Meeting of the European Software Engineering
Conference and the Symposium on the F oundations of Software Engi-neering (ESEC/FSE). ACM, 2011, pp. 59–69.
[13] F. Ocariza, K. Pattabiraman, and A. Mesbah, “Detecting inconsistencies
in JavaScript MVC applications,” in Proceedings of the International
Conference on Software Engineering (ICSE). IEEE Computer Society,
2015.
[14] D. Engler, D. Y . Chen, S. Hallem, A. Chou, and B. Chelf, “Bugs as
deviant behavior: A general approach to inferring errors in systemscode,” in Proceedings of the ACM Symposium on Operating Systems
Principles (SOSP). ACM, 2001, pp. 57–72.
[15] A. Milani Fard and A. Mesbah, “JSNose: Detecting JavaScript code
smells,” in Proceedings of the International Working Conference on
Source Code Analysis and Manipulation (SCAM). IEEE ComputerSociety, 2013, pp. 116–125.
[16] D. Synodinos, “Top JavaScript MVC frameworks,” 2013, http://www.
infoq.com/research/top-javascript-mvc-frameworks (Accessed: May 16,2015).
[17] Two Sigma, “Beaker,” 2016, https://github.com/twosigma/
beaker-notebook (Accessed: April 29, 2016).
[18] MarionetteJS, “Backbone Marionette,” 2016, https://github.com/
marionettejs/backbone.marionette (Accessed: April 29, 2016).
[19] I. D. Baxter, A. Y ahin, L. Moura, M. S. Anna, and L. Bier, “Clone
detection using abstract syntax trees,” in Proceedings of the International
Conference on Software Maintenance (ICSM). IEEE Computer Society,1998, pp. 368–377.
[20] R. Agrawal, T. Imieli ´nski, and A. Swami, “Mining association rules
between sets of items in large databases,” in Proceedings of the Inter-
national Conference on Management of Data (SIGMOD). ACM, 1993,
pp. 207–216.
[21] R. Agrawal and R. Srikant, “Fast algorithms for mining association rules
in large databases,” in Proceedings of the International Conference onV ery Large Databases (VLDB).
 Morgan Kaufmann Publishers Inc.,
1994, pp. 487–499.
[22] Adobe Systems, “Brackets,” 2015, http://www.brackets.io (Accessed:
May 16, 2015).
[23] A. Hidayat, “Esprima,” 2015, http://www.esprima.org/ (Accessed: May
16, 2015).
[24] jindw, “XMLDOM,” 2016, https://www.github.com/jindw/xmldom (Ac-
cessed: April 29, 2016).
[25] K. Sera, “apriori.js,” 2016, https://github.com/seratch/apriori.js (Ac-
cessed: April 29, 2016).
[26] U. Shaked, “AngularJS vs. BackboneJS vs. EmberJS,” 2014, http:
//www.airpair.com/js/javascript-framework-comparison (Accessed: May16, 2015).
[27] Google, “Built with AngularJS,” 2015, https://github.com/angular/
builtwith.angularjs.org/blob/master/projects/projects.json (Accessed:May 16, 2015).
[28] J. Ashkenas, “BackboneJS: Tutorials, blog posts and example
sites,” 2016, https://github.com/jashkenas/backbone/wiki/Tutorials,-blog-posts-and-example-sites (Accessed: April 29, 2016).
[29] EmberSherpa, “Open source Ember apps,” 2016, https://github.com/
EmberSherpa/open-source-ember-apps (Accessed: April 29, 2016).
[30] B. Johnson, Y . Song, E. Murphy-Hill, and R. Bowdidge, “Why don’t
software developers use static analysis tools to ﬁnd bugs?” in Proceed-
ings of the International Conference on Software Engineering (ICSE).
IEEE Computer Society, 2013, pp. 672–681.
[31] N. A yewah, W . Pugh, J. D. Morgenthaler, J. Penix, and Y . Zhou,
“Evaluating static analysis defect warnings on production software,” inProceedings of the ACM SIGPLAN-SIGSOFT Workshop on Program
Analysis for Software Tools and Engineering (PASTE). ACM, 2007,
pp. 1–8.
[32] S. Hangal and M. S. Lam, “Tracking down software bugs using
automatic anomaly detection,” in Proceedings of the International Con-
ference on Software Engineering (ICSE). ACM, 2002, pp. 291–301.
[33] Z. Li and Y . Zhou, “PR-Miner: automatically extracting implicit pro-
gramming rules and detecting violations in large software code,” inProceedings of the International Symposium on F oundations of Software
Engineering (FSE). ACM, 2005, pp. 306–315.
[34] Z. Li, S. Lu, S. Myagmar, and Y . Zhou, “CP-Miner: Finding copy-paste
and related bugs in large-scale software code,” Transactions on Software
Engineering (TSE), vol. 32, no. 3, pp. 176–192, 2006.
[35] W . Weimer and G. C. Necula, “Mining temporal speciﬁcations for error
detection,” in Proceedings of the International Conference on Tools
and Algorithms for the Construction and Analysis of Systems (TACAS).
Springer, 2005, pp. 461–476.
[36] A. Wasylkowski, A. Zeller, and C. Lindig, “Detecting object usage
anomalies,” in Proceedings of the Joint Meeting of the European Soft-
ware Engineering Conference and the Symposium on the F oundations
of Software Engineering (ESEC/FSE). ACM, 2007, pp. 35–44.
[37] L. Jiang, Z. Su, and E. Chiu, “Context-based detection of clone-related
bugs,” in Proceedings of the Joint Meeting of the European Software
Engineering Conference and the Symposium on the F oundations of
Software Engineering (ESEC/FSE). ACM, 2007, pp. 55–64.
[38] C.-H. Hsiao, M. Cafarella, and S. Narayanasamy, “Using web corpus
statistics for program analysis,” in Proceedings of the International
Conference on Object Oriented Programming Systems Languages &
Applications (OOPSLA). ACM, 2014, pp. 49–65.
[39] S. Thummalapenta and T. Xie, “Alattin: Mining alternative patterns
for detecting neglected conditions,” in Proceedings of the International
Conference on Automated Software Engineering (ASE). IEEE Computer
Society, 2009, pp. 283–294.
[40] N. Gruska, A. Wasylkowski, and A. Zeller, “Learning from 6,000
projects: lightweight cross-project anomaly detection,” in Proceedings of
the International Symposium on Software Testing and Analysis (ISSTA) .
ACM, 2010, pp. 119–130.
[41] H. V . Nguyen, H. A. Nguyen, T. T. Nguyen, A. T. Nguyen, and T. N.
Nguyen, “Detection of embedded code smells in dynamic web appli-
cations,” in Proceedings of the International Conference on Automated
Software Engineering (ASE). IEEE Computer Society, 2012, pp. 282–285.
[42] N. Tsantalis, T. Chaikalis, and A. Chatzigeorgiou, “Jdeodorant: Identi-
ﬁcation and removal of type-checking bad smells,” in Proceedings of
the European Conference on Software Maintenance and Reengineering
(CSMR). IEEE Computer Society, 2008, pp. 329–331.
576
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:08:29 UTC from IEEE Xplore.  Restrictions apply. [43] E. V an Emden and L. Moonen, “Java quality assurance by detecting
code smells,” in Proceedings of the Working Conference on Reverse
Engineering (WCRE). IEEE Computer Society, 2002, pp. 97–106.
[44] Synopsys, “Coverity,” 2016, http://www.coverity.com/ (Accessed: April
29, 2016).
[45] S. P . Reiss, “Finding unusual code,” in Proceedings of the International
Conference on Software Maintenance. IEEE Computer Society, 2007,
pp. 34–43.
[46] D. Hovemeyer and W. Pugh, “Finding bugs is easy,” in Companion
Proceedings of the International Conference on Object-Oriented Pro-
gramming, Systems, Languages and Applications (OOPSLA). ACM,2004, pp. 132–136.
[47] L. Gong, M. Pradel, M. Sridharan, and K. Sen, “DLint: Dynamically
checking bad coding practices in JavaScript,” in Proceedings of the
International Symposium on Software Testing and Analysis (ISSTA).
ACM, 2015.
[48] Facebook, “Flow: a static type checker for JavaScript,” 2016, http://
ﬂowtype.org/ (Accessed: April 29, 2016).
[49] M. Pradel, P . Schuh, and K. Sen, “TypeDevil: Dynamic type incon-
sistency analysis for JavaScript,” in Proceedings of the International
Conference on Software Engineering (ICSE). IEEE Computer Society,
2015, pp. 314–324.
[50] H. V . Nguyen, H. A. Nguyen, T. T. Nguyen, A. T. Nguyen, and T. N.
Nguyen, “Dangling references in multi-conﬁguration and dynamic PHP-based web applications,” in Proceedings of the International Conference
on Automated Software Engineering (ASE). IEEE Computer Soci-
ety/ACM, 2013, pp. 399–409.
[51] T. Polychniatis, J. Hage, S. Jansen, E. Bouwers, and J. Visser, “Detecting
cross-language dependencies generically,” in Proceedings of the Euro-
pean Conference on Software Maintenance and Reengineering (CSMR).IEEE, 2013, pp. 349–352.
[52] R.-H. Pfeiffer and A. Wasowski, “Taming the confusion of languages,”
inProceedings of the European Conference on Modelling F oundations
and Applications (ECMF A). Springer, 2011, pp. 312–328.
[53] P . Mayer and A. Schroeder, “Cross-language code analysis and refactor-
ing,” in Proceedings of the International Working Conference on Source
Code Analysis and Manipulation (SCAM). IEEE Computer Society,2012, pp. 94–103.
[54] D. Strein, H. Kratz, and W. L ¨owe, “Cross-language program analysis and
refactoring,” in Proceedings of the International Workshop on Source
Code Analysis and Manipulation (SCAM). IEEE Computer Society,2006, pp. 207–216.
[55] H. V . Nguyen, C. K ¨astner, and T. N. Nguyen, “Cross-language program
slicing for dynamic web applications,” in Proceedings of the Joint
Meeting of the European Software Engineering Conference and the
Symposium on the F oundations of Software Engineering (ESEC/FSE).
ACM, 2015, pp. 369–380.
577
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:08:29 UTC from IEEE Xplore.  Restrictions apply. 