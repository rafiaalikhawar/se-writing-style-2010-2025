Effective Test Suites for Mixed Discrete-Continuous
Stateï¬‚ow Controllers
Reza Matinnejad, Shiva Nejati, Lionel C. Briand
SnT Centre, University of Luxembourg,Luxembourg
{reza.matinnejad,shiva.nejati,lionel.briand}@uni.luThomas Bruckmann
Delphi Automotive Systems,Luxembourg
thomas.bruckmann@delphi.com
ABSTRACT
Modeling mixed discrete-continuous controllers using Stateï¬‚ow is
common practice and has a long tradition in the embedded soft-
ware system industry. Testing Stateï¬‚ow models is complicated by
expensive and manual test oracles that are not amenable to full au-
tomation due to the complex continuous behaviors of such models.
In this paper, we reduce the cost of manual test oracles by providing
test case selection algorithms that help engineers develop small test
suites with high fault revealing power for Stateï¬‚ow models. We
present six test selection algorithms for discrete-continuous State-
ï¬‚ows: An adaptive random test selection algorithm that diversiï¬es
test inputs, two white-box coverage-based algorithms, a black-box
algorithm that diversiï¬es test outputs, and two search-based black-
box algorithms that aim to maximize the likelihood of presence of
continuous output failure patterns. We evaluate and compare our
test selection algorithms, and ï¬nd that our three output-based algo-
rithms consistently outperform the coverage- and input-based algo-
rithms in revealing faults in discrete-continuous Stateï¬‚ow models.
Further, we show that our output-based algorithms are complemen-
tary as the two search-based algorithms perform best in revealing
speciï¬c failures with small test suites, while the output diversity
algorithm is able to identify different failure types better than other
algorithms when test suites are above a certain size.
Categories and Subject Descriptors [Software Engineering]: Soft-
ware/Program Veriï¬cation
Keywords: Stateï¬‚ow testing; mixed discrete-continuous behav-
iors; structural coverage; failure-based testing; output diversity.
1. INTRODUCTION
Automated software testing approaches are often hampered by
thetest oracle problem , i.e., devising a procedure that distinguishes
between the correct and incorrect behaviors of the system under
test [5, 38]. Despite new advances in test automation, test oracles
most often rely on human knowledge and expertise, and thus, are
the most difï¬cult testing ingredient to automate [5, 26]. In this
situation, in order to reduce the cost of human test oracles, test case
selection criteria have been proposed as a way to obtain minimal
test suites with high fault revealing power [15, 17].
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for proï¬t or commercial advantage and that copies bear
this notice and the full citation on the ï¬rst page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior speciï¬c permission and/or a fee. Request
permissions from permissions@acm.org.
ESEC/FSE â€™15, August 30-September 04, 2015, Bergamo, Italy.
Copyright 2015 ACM
http://dx.doi.org/10.1145/2786805.2786818.Many embedded software systems in various industry sectors are
developed using Stateï¬‚ow [43, 13, 46], which is integrated into the
Matlab/Simulink language. Stateï¬‚ow is a hierarchical state ma-
chine language that is most commonly used in practice to specify
mixed discrete-continuous behaviors [18, 39, 19]. Such behavior
evolves in continuous time with discrete jumps at particular time
instances [45], and is typically captured using a Stateï¬‚ow model
consisting of some discrete states such that during each state, the
system may behave based on a continuous-time differential or a dif-
ference equation. The state equations may change when the system
transitions to its subsequent states due to some discrete-time event.
Stateï¬‚ow models, being detailed enough to enable code genera-
tion and simulation, are subject to extensive testing. Testing State-
ï¬‚ow models allows engineers to verify the behavior of software
functions, and is more likely to help with fault ï¬nding compared
to testing code as Stateï¬‚ow models are more abstract and more
informative for engineers. Testing Stateï¬‚ow models, however, is
complicated mostly due to their continuous behaviors [51].
Existing work on formalizing and automating test oracles for
Stateï¬‚ow, and in general for other state machine dialects, has pri-
marily focused on implicit test oracles [40] such as runtime errors,
e.g., division by zero and data type overï¬‚ow, or on oracles based
on discrete behavior properties [35] such as temporal reachability,
termination, or state invariants which can be speciï¬ed using logical
assertions or temporal logic [4, 14]. Compared to test oracle au-
tomation for discrete system behaviors, the problem of developing
and automating test oracles for continuous behaviors has received
signiï¬cantly less attention, and is left unexplored.
In our earlier work, we proposed an approach to automate test or-
acles for a class of embedded controllers known as closed-loop con-
trollers and for three types of continuous output failures: stability ,
smoothness andresponsiveness [24, 25]. Our approach used meta-
heuristic search to generate test cases maximizing the likelihood of
presence of failures in controller outputs (i.e., test cases that pro-
duce outputs that break or are close to breaking stability, smooth-
ness and responsiveness requirements). This approach, however,
fails to automate test oracles for Stateï¬‚ows because for closed-loop
controllers, the environment (plant) feedback and the desired con-
troller output (setpoint) [16] are both available. Hence, test oracles
could be formalized and automated in terms of feedback and set-
point. For Stateï¬‚ow models, which typically implement open-loop
controllers [48], the plant feedback is not available.
Given that test oracles for Stateï¬‚ow models are not amenable to
full automation mostly due to their complex continuous-time be-
haviors [51], in this paper, we focus on providing test case selec-
tion algorithms for Stateï¬‚ow models. Our algorithms help engi-
neers develop small test suites with high fault revealing power for
continuous behaviors, effectively reducing the cost of human test
1oracles [5, 26]. In this paper, we present and evaluate six test selec-
tion algorithms for mixed discrete-continuous Stateï¬‚ow models: A
black-box adaptive random input-based algorithm, two white-box
adaptive random coverage-based algorithms, a black-box adaptive
random output-based algorithm, and two black-box search-based
output-based algorithms . Our adaptive random input-based algo-
rithm simply attempts to generate a test suite by diversifying test
inputs. Our two white-box adaptive random coverage-based algo-
rithms aim to achieve high structural coverage. Speciï¬cally, we
consider the well-known state and transition coverage criteria [6]
for Stateï¬‚ow models. Our black-box adaptive random output-based
algorithm aims to maximize output diversity , i.e., diversity in con-
tinuous outputs of Stateï¬‚ow models. Output diversity is an adap-
tation of the recently proposed output uniqueness criterion [1, 2]
to Stateï¬‚ow. Output uniqueness has been studied for web applica-
tions and has shown to be a useful surrogate to white-box selection
techniques. We consider this criterion in our work because State-
ï¬‚ows have rich time-continuous outputs, providing a useful source
of information for fault detection.
Our black-box search-based output-based algorithms rely on meta-
heuristic search [21] and aim to maximize objective functions cap-
turing the degree of presence of continuous output failure patterns.
Inspired by discussions with control engineers, we propose and for-
malize two continuous output failure patterns, referred to as insta-
bility anddiscontinuity . The instability pattern is characterized by
quick and frequent oscillations of the controller output over a time
interval, and the discontinuity pattern captures fast, short-duration
and upward or downward pulses (i.e., spikes [49]) in the controller
output. Presence of either of these failure patterns in Stateï¬‚ow out-
puts may have undesirable impact on physical processes or objects
that are controlled by or interact with a Stateï¬‚ow model.
Our contributions: (1) We focus on the problem of testing mixed
discrete-continuous Stateï¬‚ow models. We propose two new failure
patterns capturing undesirable behaviors in Stateï¬‚ow outputs that
may potentially harm physical processes and objects. We develop
black-box search-based test selection algorithms generating test in-
puts that are likely to produce continuous outputs exhibiting these
failure patterns. In addition, we deï¬ne a black-box output diver-
sity test selection criterion for Stateï¬‚ow, and present an adaptive
random test selection algorithm based on this criterion.
(2) We evaluate our six algorithms, comparing the output-based
selection algorithms with the coverage-based and input-based se-
lection algorithms. Our evaluation uses three Stateï¬‚ow models, in-
cluding two industrial ones. Our results show that the output-based
algorithms consistently outperform the coverage-based and input-
based algorithms in revealing Stateï¬‚ow model faults. Further, for
relatively larger test suites, the coverage-based algorithms are sub-
sumed by the output diversity algorithm, i.e., any fault found by the
coverage-based algorithms are also found with the same or higher
probability by the output diversity algorithm. Finally, we show that
our adaptive random and search-based output-based algorithms are
complementary as the search-based algorithms perform best in re-
vealing instability and discontinuity failures even when test suites
are small, while the adaptive random output diversity algorithm is
able to identify different failure types better than the search-based
algorithms when test suites are above a certain size.
2. BACKGROUND AND MOTIV ATION
Motivating example. We motivate our work using a simpliï¬ed
Stateï¬‚ow from the automotive domain which controls a supercharger
clutch and is referred to as the Supercharger Clutch Controller (SCC).
Figure 1(a) represents the discrete behavior of SCC specifying that
the supercharger clutch can be in two quiescent states: engaged
(a) SCC -- Discrete Behavior(b) SCC -- Timed BehaviorEngagedDisengaged
Engaging
(c) Engaging state of SCC -- mixed discrete-continuous behaviorDisengagingDisengagedEngagedtime++ ;
[disengageReq]/time:= 0[time 600][time 600]time++ ;[(engspd>smin^engspd<smax)^(tmp>tmin^tmp<tmax)]/ctrlSig:= 1
[engageReq]/time:= 0[Â¬(engspd>smin^engspd<smax)_Â¬(tmp>tmin^tmp<tmax)] /ctrlSig:= 1
OnMovingOnSlippingOnCompletedtime++ ;ctrlSig:=f(time)Engagingtime++ ;ctrlSig:=g(time)time++ ;ctrlSig:= 1.0[Â¬(vehspd= 0)^time 300][(vehspd= 0)^time 400][time 500]Figure 1: Supercharge Clutch Controller (SCC) Stateï¬‚ow.
or disengaged. Further, the clutch moves from the disengaged to
the engaged state whenever both the engine speed engspd and the
engine coolant temperature tmp respectively fall inside the speci-
ï¬ed ranges of [smin::smax]and[tmin::tmax]. The clutch moves
back from the engaged to the disengaged state whenever either the
speed or the temperature falls outside their respective ranges. The
variable ctrlSig in Figure 1(a) indicates the sign and magnitude
of the voltage applied to the DC motor of the clutch to physically
move the clutch between engaged and disengaged positions. As-
signing 1:0toctrlSig moves the clutch to the engaged position,
and assigning 1:0toctrlSig moves it back to the disengaged
position. To avoid clutter in our ï¬gures, we use engageReq to re-
fer to the condition on the Disengaged!Engaged transition, and
disengageReq to refer to the condition on the Engaged!Disen-
gaged transition.
The discrete transition system in Figure 1(a) assumes that the
clutch movement takes no time, and further, does not provide any
insight on the quality of movement of the clutch. Figure 1(b) ex-
tends the discrete transition system in Figure 1(a) by adding a timer
variable, i.e., time , to explicate the passage of time in the SCC
behavior. The new transition system in Figure 1(b) includes two
transient states, engaging and disengaging, specifying that moving
from the engaged to the disengaged state and vice versa takes 600
ms. Since this model is simpliï¬ed, it does not show handling of al-
terations of the clutch state during the transient states. In addition to
adding the time variable, we note that the variable ctrlSig , which
controls physical movement of the clutch, cannot abruptly jump
from 1:0to 1:0, or vice versa. In order to ensure safe and smooth
movement of the clutch, the variable ctrlSig has to gradually
move between 1:0and 1:0and be described as a function over
time, i.e., a signal. To express the evolution of the ctrlSig signal
over time, we decompose the transient states engaging and disen-
gaging into sub-state machines. Figure 1(c) shows the sub-state
machine related to the engaging state. The one related to the disen-
gaging state is similar. At beginning (state OnMoving ), the func-
20.01.0
0.01.02.0TimeInput / Output SignalsInputOutput(Control Signal)A'C'B'
Figure 2: An example input (dashed line) and output (solid
line) signals for SCC in Figure 1. The input signal represents
engageReq , and the output signal represents ctrlSig .
tionctrlSig has a steep grade (i.e., function f) to move the sta-
tionary clutch from the disengaged state and accelerate it to reach a
certain speed in 300ms. Afterwards (state OnSlipping ),ctrlSig
decreases the speed of clutch based on the gradual function gfor
200ms. This is to ensure that the clutch slows down as it gets closer
to the crankshaft. Finally, at state OnCompleted ,ctrlSig reaches
value 1:0and remains constant, causing the clutch to get engaged
in about 100ms. When the car is stationary, i.e., vehspd is 0, the
clutch moves based on the steep grade function ffor400ms, and
does not have to go to the OnSlipping phase to slow down before
it reaches the crankshaft at state OnCompleted .
Input and Output. The Stateï¬‚ow inputs and outputs are signals
(functions over time). Each input/output signal has a data type,
e.g. boolean, enum or ï¬‚oat, specifying the range of the signal.
For example, Figure 2 shows an example input (dashed line) and
output (solid line) signals for SCC. The input signal is related to
engageReq and is boolean, while the output signal is related to
ctrlSig and is a ï¬‚oat signal. The simulation length, i.e., the time
interval during which the signals are observed, is two sec for both
signals. In theory, the input signals to Stateï¬‚ow models can have
complex shapes. In practice, however, engineers mostly test State-
ï¬‚ow models using constant or step input signals over a ï¬xed time
interval. This is because developing manual test oracles for arbi-
trary and complex input signals is difï¬cult and time consuming.
Stateï¬‚ow outputs might be either discrete or continuous. A dis-
crete output is represented by a boolean or an enum signal that takes
a constant value at each state. A continuous output is represented
by a ï¬‚oat signal that changes over time based on a difference or
differential equation (e.g., ctrlSig in Figure 1(c)). Our focus in
this paper is on Stateï¬‚ows with some continuous outputs.
Stateï¬‚ow requirements. The speciï¬cation of Stateï¬‚ow controllers
typically includes the following kinds of requirements: (1) Re-
quirements that can be speciï¬ed as assertions or temporal logic
properties over pure discrete behavior (e.g., the state machine in
Figure 1(a)). For example, If engine speed engspd and temperature
tmpfall inside the ranges [smin::smax]and[tmin::tmax], respec-
tively, the clutch should eventually be engaged . (2) Requirements
that focus on timeliness of the clutch behavior and rely on the time
variable (see Figure 1(b)). For example, moving the clutch from
disengaged to engaged or vice versa should take 600ms. Note that
SCC is an open-loop controller [48] and it does not receive any
information from the clutch to know its whereabouts. Hence, engi-
neers need to estimate the position (state) of the clutch using timing
constraints. (3) Requirements characterizing continuous dynamics
of controlled physical objects. For example, the clutch should move
smoothly without any oscillations, and it should not bump into the
crankshaft or other physical components close to it . Engineers need
to evaluate the continuous ctrlSig signal to ensure that it does not
exhibit any erratic or unexpected change with any undesirable im-
pact on physical processes or objects.
Existing literature such as model checking and formal veriï¬ca-
tion [10] largely focuses on properties that fall in groups one and
0.01.02.00.01.02.0-1.0-0.50.00.51.0(b)
TimeTime(a)CtrlSig Output 0.00.250.500.751.0
ABCFigure 3: The output signals ( ctrlSig ) for two faulty versions
of SCC: (a) an unstable output, and (b) a discontinuous output.
two above [29]. The third group of requirements above, although of
paramount importance for correct dynamic behavior of controllers,
are lesser studied in the software testing literature compared to the
requirements in the ï¬rst and second groups. To evaluate outputs
with respect to the requirements in the third group, engineers have
to evaluate the changes in the output over a time period . In con-
trast, model checkers focus on discrete-time behaviours only, and
evaluate outputs at a few discrete time instances (states), ignoring
the pattern of output changes over time.
Failure patterns. Figure 3 shows two speciï¬c patterns of fail-
ures in continuous output signals, violating requirements on de-
sired physical behaviors of controllers (group three). The failure
in Figure 3(a) shows instability , and the one in Figure 3(b) refers
todiscontinuity . Speciï¬cally, the former signal shows quick and
frequent oscillations of the controller output in the area marked
by a grey dashed rounded box, and the latter shows a very short-
duration pulse in the controller output at point A. In Section 3, we
provide a number of test selection algorithms to generate test cases
that reveal failures in mixed discrete-continuous Stateï¬‚ow outputs
including the two failure patterns in Figure 3.
3. TEST SELECTION ALGORITHMS
In our work, we propose the following test case selection algo-
rithms to develop test suites that can reveal erroneous continuous
outputs of Stateï¬‚ow models:
Black-box input diversity (ID). Our input diversity selection al-
gorithm is adaptive random and attempts to maximize diversity of
test inputs selected from the input search space. Adaptive random
test selection [3, 9] is a simple strategy that is commonly used as
a baseline for comparison. A selection algorithm has to perform at
least better than adaptive random to be considered worthwhile.
White-box coverage-based. Structural coverage criteria have been
extensively studied in software testing as a method for measuring
test suite effectiveness [28, 20]. We consider the two well-known
state coverage (SC) andtransition coverage (TC) criteria for State-
ï¬‚ows [6] mostly as another baseline of comparison.
Black-box output diversity (OD). In the context of web applica-
tions, recent studies have shown that selecting test cases based on
outputs uniqueness, i.e., selecting test cases that produce highly di-
verse or distinct outputs, enhance fault ï¬nding effectiveness of test
suites [1, 2]. Stateï¬‚ow outputs provide a primary source of data
for engineers to ï¬nd faults. Hence, in our work, we adapt the out-
put uniqueness proposed by [1, 2] and deï¬ne a notion of output
diversity over continuous control signals.
Black-box failure-based. The goal of failure-based test selection
algorithms is to select test inputs that are able to reveal common
failures speciï¬c to a particular domain [32]. We identify two fail-
ure patterns related to continuous dynamics of controllers: instabil-
ity and discontinuity. Based on these two patterns, we deï¬ne two
failure-based and output-based selection algorithms, output stabil-
ity (OS) andoutput continuity (OC) . Output stability aims to select
test inputs that are likely to produce outputs exhibiting a period
of instability, particularly in response to a sudden change in input.
3An example of an output with instability failure is shown in Fig-
ure 3(a). A period of instability in this signal, which is applied
to a physical device, may result in hardware damage and must be
investigated by engineers.
In contrast, output continuity attempts to select test inputs that
are likely to produce discontinuous outputs. The control output of
a Stateï¬‚ow is a continuous function with some discrete jumps at
state transitions. For example, for both the control signals in Fig-
ures 2 and 3(b), there is a discrete jump at around time 1.0 sec (i.e.,
point A0in Figure 2, and point A in Figure 3(b)). At discrete jumps,
and in general at every simulation step, the control signals are ex-
pected to be either left-continuous or right-continuous, or both. For
example, the signal in Figure 2 is right-continuous at point A0due
to the slope from A0to C0, and hence, this signal does not exhibit
any discontinuity failure at point A0. However, the signal in Fig-
ure 3(b) is neither right-continuous nor left-continuous at point A.
This signal, which is obtained from a faulty version of SCC, shows
a very short duration pulse (i.e., a spike) at point A. This behavior
is unacceptable because it may damage the clutch by imposing an
abrupt change in the voltage applied to the clutch [49]. Speciï¬cally,
the failure shown in Figure 3(b) is due to a fault in a transition con-
dition in the SCC model. Due to this faulty condition, the controller
leaves a state immediately after it enters that state and modiï¬es the
control signal value from B to A.
In the remainder of this section, we ï¬rst provide a formal def-
inition of the test selection problem, and we then present our test
selection algorithms.
Test Selection Problem . Let SF= (;; ;o)be a Stateï¬‚ow
model where  =fs1;:::;s ngis the set of states,  =fr1;:::
;rmgis the set of transitions,   =fi1;:::;i dgis the set of in-
put variables, and ois the controller output of the Stateï¬‚ow model
based on which we want to select test cases. Typically, embedded
software controllers have one main output, i.e., the control signal,
applied to the device under control. If a Stateï¬‚ow model has more
than one output, we can apply our approach to select test cases for
each individual output separately.
Note that Stateï¬‚ow models can be hierarchical or may have par-
allel states. Among our selection algorithms, only state and transi-
tion coverage algorithms, SC and TC, are impacted by the Stateï¬‚ow
structure. In our work, we assume that and, respectively, con-
tain the states and transitions in ï¬‚attened Stateï¬‚ow models [37].
However, our SC and TC algorithms do not require to statically
ï¬‚atten Stateï¬‚ow models as these algorithms dynamically identify
the (ï¬‚attened) states and (ï¬‚attened) transitions that are actually ex-
ecuted during simulation of Stateï¬‚ow models.
Each input/output variable of SFis a signal, i.e., a function of
time. When SFis simulated, its input/output signals are discretized
and represented as vectors whose elements are indexed by time.
Assuming that the simulation time is T, the simulation interval
[0::T]is divided into small equal time steps denoted by t. For
example for SCC, we set T= 2sandt= 1ms. We deï¬ne a
signal sgas a function sg:f0;t;2t;:::;ktg!R sg, where
tis the simulation time step, kis the number of observed simu-
lation steps, andRsgis the signal range. In our example, we have
k= 2000 . We further denote by minRsgandmaxRsgthe min
and the max ofRsg. For example, when sgis a boolean,Rsgis
f0;1g, and when sgis a ï¬‚oat signal,Rsgis the set of ï¬‚oat values
betweenminRsgandmaxRsg. As discussed in Section 2, to en-
sure the feasibility of the generated input signals, in this paper, we
only consider constant or step input signals.
Our goal is to select a test suite TS=fI1;:::;I qgofqtest
inputs where qis determined by the human test oracle budget. EachAlgorithm. ID-S ELECTION
Input: Stateï¬‚ow model SF.
Output: Test suite TS=fJ1;:::;J qg.
1. Let TS=fIgwhereIis a random test input of SF
2.forq 1times do:
3. MaxDist = 0
4. LetC=fI1;:::;I cgbe a candidate set of random test inputs of SF
5. for eachIi2Cdo:
6. Dist =MIN8I02TSdist(Ii;I0)
7. ifDist>MaxDist :
8. MaxDist =Dist;J=Ii
9. TS=TS[J
10. return TS
Figure 4: The input diversity test selection algorithm (ID).
test inputIjis a vector (sg1;:::;sg d)of signals for the SFinput
variablesi1toid. By simulating SFusing each test input Ij, we
obtain an output signal sgofor the continuous output oofSF.
3.1 Input Diversity Test Selection
The input diversity selection algorithm (ID) generates a test suite
with diverse test inputs. Given two test inputs I= (sg1;:::;sg d)
andI0= (sg0
1;:::;sg0
d), we deï¬ne the normalized Euclidean dis-
tance between each pair sgjandsg0
jof signals as follows:
^dist(sgj;sg0
j) =s
kP
i=0(sgj(it) sg0
j(it))2
pk+1(maxRsg minRsg)(1)
Note thatsgjandsg0
jare alternative assignments to the same SF
inputij, and hence, they have the same range. Further, we assume
that the values of kandtare the same for sgjandsg0
j. It is easy
to see that ^dist(sg;sg0)is always between 0and1.
We deï¬ne the distance between two test inputs I= (sg1;:::;sg d)
andI0= (sg0
1;:::;sg0
d)as the sum of the normalized distances be-
tween each signal pair:
dist(I;I0) =dP
j=1^dist(sgj;sg0
j) (2)
Figure 4 shows the ID-S ELECTION algorithm which, given a
Stateï¬‚ow model SF, generates a test suite TSwith sizeqand with
diverse test inputs. The algorithm ï¬rst randomly selects a single
test input and stores it in TS(line 1). Then, at each iteration, it
randomly generates ccandidate test inputs I1;:::;I c. It computes
the distance of each test input Iifrom the existing test suite TSas
the minimum of the distances between Iiand the test inputs in TS
(line6). Finally, the algorithm identiï¬es and stores in TSthe test
input among the ccandidates with the maximum distance from the
test inputs in TS(lines 7 9).
3.2 Coverage-based Test Selection
In order to generate a test suite TSbased on the state/transition
coverage criterion, we need to simulate SFusing each one of the
candidate test inputs and compute the state and the transition cover-
age reports for each test input simulation. The state coverage report
Sis a subset of  =fs1;:::;s ngcontaining the states covered by
the test input I, and the transition coverage report Ris a subset of
 =fr1;:::;r mgcontaining the transitions covered by I.
The state coverage test selection algorithm, SC-S ELECTION , is
shown in Figure 5. The algorithm for transition coverage, TC-
SELECTION , is obtained by replacing S(state coverage report) with
T(transition coverage report). At line 1, the algorithm selects a
random test input Iand adds it to TS. At line 2, it simulates SF
usingIand adds the corresponding state coverage report to a set
TSC . At each iteration the algorithm generates ccandidate test
inputs and keeps their corresponding state coverage reports in a set
CC. It then computes the additional coverage that each one of
the test inputs among the ccandidates brings about compared to
the coverage obtained by the existing test suite TS(line 8). At
the end of the iteration, the test input that leads to the maximum
4Algorithm. SC-S ELECTION
Input: Stateï¬‚ow model SF.
Output: Test suite TS=fJ1;:::;J qg.
1. Let TS=fIgwhereIis a random test input of SF
2. Let TSC =fSgwhereSis the state coverage reports of executing SFwithI
3.forq 1times do:
4. MaxAddCov = 0
5. LetC=fI1;:::;I cgbe a candidate set of random test inputs of SF
6. Let CC=fS1;:::;S cgbe the state coverage reports of executing SFwithI1toIc
7. for eachSi2CCdo:
8. AddCov =jSi [S02TSCS0j
9. ifAddCov>MaxAddCov :
10. MaxAddCov =AddCov
11.P=Si,J=Ii
12. ifMaxAddCov = 0:
13. Let P=Sj,J=IjwhereSj2CCandjSjj=MAX S02CCjS0j
14. TSC =TSC[P,TS=TS[J
15. return TS
Figure 5: The state coverage (SC) selection algorithm. The al-
gorithm for transition coverage, TC, is obtained by replacing S
(state coverage report) with T(transition coverage report).
Algorithm. OD-S ELECTION
Input: Stateï¬‚ow model SF.
Output: Test suite TS=fJ1;:::;J qg.
1. Let TS=fIgwhereIis a random test input of SF
2. Let TSO =fsgogwheresgois the output signal of executing SFwithI
3.forq 1times do:
4. MaxDist = 0
5. LetC=fI1;:::;I cgbe a candidate set of random test inputs of SF
6. Let CO=fsg1;:::;sg cgbe the output signals of executing SFwithI1toIc
7. for eachsgi2COdo:
8.Dist =MIN8sg02TSOdisto(sgi;sg0)
9. ifDist>MaxDist :
10.MaxDist =Dist
11.p=sgi,J=Ii
12 TSO =TSO[p,TS=TS[J
13. return TS
Figure 6: The output diversity test selection algorithm (OD).
additional coverage is selected and added to TS(line 14). More
precisely, a test input Ibrings about maximum additional coverage,
if, compared to other ctest input candidates, it covers the most
number of states that are not already covered by the test suite TS.
Note that if none of the ccandidates yields an additional coverage,
i.e.,MaxAddCov is0at line 12, we pick a test input with the
maximum coverage among the ccandidates (line 13).
3.3 Output Diversity Test Selection
The output diversity (OD) algorithm aims to generate a test suite
TSsuch that the diversity among continuous output signals pro-
duced by different test inputs in TSis maximized [2]. In order to
formalize this algorithm, we deï¬ne a measure of diversity ( disto)
between pairs of control output signals (sgo;sg0
o). Speciï¬cally, we
deï¬ne the diversity between sgoandsg0
obased on normalized Eu-
clidean distance and as deï¬ned by Equation 1 (i.e., disto(sgo;sg0
o) =
^dist(sgo;sg0
o)).
Figure 6 shows the OD algorithm, i.e., OD-S ELECTION . The al-
gorithm ï¬rst selects a random test input Iand simulates SFusing
I. It addsItoTS(line1) and the output corresponding to Ito an-
other set TSO (line2). Then, at each iteration, the algorithm ï¬rst
randomly generates ccandidate test inputs (line 5) together with
their corresponding test outputs and store the outputs in set CO
(line6). Then, in line 8, it uses distoto compute the distance be-
tween each test output sgiinCO and the test outputs correspond-
ing to the existing test inputs in TS. Among the test outputs in
CO, the algorithm keeps the one with the highest distance from the
test outputs in TSO (line11), and adds such a test output to TSO
and its corresponding test input to TS(line12).
3.4 Failure-based Test Selection
The goal of failure-based test selection algorithms is to gener-
ate test inputs that are likely to produce output signals exhibitingspeciï¬c failure patterns. We develop these algorithms using meta-
heuristic search algorithms [21] that generate test inputs maximiz-
ing the likelihood of presence of failures in outputs.
We propose two failure-based test selection algorithms, output
stability and output continuity that respectively correspond to insta-
bility and discontinuity failure patterns introduced in Section 2. We
ï¬rst provide two heuristic (quantitative) objective functions that es-
timate the likelihood for each of these failure patterns to be present
in control signals. We then provide selection algorithms that guide
the search to identify test inputs that maximize these objective func-
tions, and hence, are more likely to reveal faults.
Output stability. Given an output signal sgo, we deï¬ne the func-
tionstability (sgo)as the sum of the differences of signal values
for consecutive simulation steps:
stability (sgo) =kP
i=1jsgo(it) sgo((i 1)t)j
Speciï¬cally, function stability (sgo)provides a quantitative ap-
proximation of the degree of instability of sgo. The higher the
value of the stability function for a signal sgo, the more certain we
can be that sgoexhibits some instability failure. For example, the
value of the stability function applied to the signal in Figure 3(a)
is higher than that of the stability function applied to the signal in
Figure 3(b) since, due to oscillations in the former signal, the val-
ues ofjsgo(it) sgo((i 1)t)jare larger than those values
for the latter signal.
Output continuity. As discussed earlier, control signals, at each
simulation step, are expected to be either left-continuous or right-
continuous, or both. We deï¬ne a heuristic objective function to
identify signals that are neither left-continuous nor right-continuous
at some simulation step. Since in our work simulation time steps
(t) are not inï¬nitesimal, we cannot compute derivatives for sig-
nals, and instead, we rely on discrete change rates that approximate
derivatives when time differences of observable changes cannot be
arbitrarily small. Given an output signal sgo, let
lci=jsgo(it) sgo((i dt)t)j
tbe the left change rate at step i, and
letrci=jsgo((i+dt)t) sgo(it)j
tbe the right change rate at step
i. We deï¬ne the function continuity (sgo)as the maximum of the
minimum of the left and the right change rates at each simulation
step over all the observed simulation steps:
continuity (sgo) =3max
dt=1(k dtmax
i=dt(min( lci;rci))))
Speciï¬cally, we ï¬rst choose a value for dtindicating the max-
imum expected time duration of a spike. Then for a ï¬xed dt, for
every stepisuch that dtik dt, we take the minimum of
the left change rate and the right change rate at step i. Since we
expect the signal to be either left-continuous or right-continuous, at
least one of the right or left change rates should be a small value.
We then compute the maximum of all the minimum right or left
change rates for all the simulation steps to ï¬nd a simulation step
with the highest discontinuity from both left and right sides. Fi-
nally, we obtain the maximum value across the time intervals up to
length dt. For our work, we pick dtto be between 1and3. For
example, the signal in Figure 3(b) yields high right and left change
rates at point A. As a result, function continuity produces a high
value for this signal, indicating that this signal is likely to be dis-
continuous. In contrast, the value of function continuity for the
signal in Figures 2 is lower than that in Figure 3(b) because at ev-
ery simulation step, either the right change rate or the left change
rate yields a relatively low value.
As discussed earlier, we provide a meta-heuristic search algo-
rithm to generate test suites based on our failure patterns. Specif-
ically, we use the Hill-Climbing with Random Restarts (HCRR)
5Algorithm. OS-S ELECTION
Input: Stateï¬‚ow model SF.
Output: Test suite TS=fJ1;:::;J qg.
1. LetIbe a random test input of SFandsgothe output of executing SFwithI
2. LetAll=fIg
3.highestFound =stability (sgo)
4.for(q 1)citerations do:
5.newI =Tweak (I)
6. Letsgobe the output of executing SFwithnewI
7.All=All[fnewIg
8. ifstability (sgo)>highestFound :
9. highestFound =stability (sgo)
10.I=newI
11. ifTimeToRestart ():
12. Let Ibe a random test input of SFandsgothe output of executing SFwithI
13. highestFound =stability (sgo)
14.All=All[fIg
15. Let TSbe the test inputs in Allwith theq-highest values of stability function
16. return TS
Figure 7: The test selection algorithm based on output stabil-
ity. The algorithm for output continuity, OC-S ELECTION , is
obtained by replacing stability (sgo)withcontinuity (sgo).
algorithm [21]. In our earlier work on computing test cases vio-
lating stability, smoothness, and responsiveness requirements for
closed-loop controllers [25], HCRR performed best among a num-
ber of alternative single-state search heuristics. Figure 7 shows our
output stability test selection algorithm, OS-S ELECTION , based on
HCRR. The algorithm for output continuity, OC-S ELECTION , is
obtained by replacing stability (sgo)withcontinuity (sgo)in OS-
SELECTION . At each iteration, the algorithm tweaks the current
solution, i.e., the test input, to generate a new solution, i.e., a new
test input, and replaces the current solution with the new solution
if the latter has higher value for the objective function. Similar
to standard Hill-Climbing, the HCRR algorithm includes a Tweak
operator that shifts a test input Iin the input space by adding values
selected from a normal distribution with mean = 0 and variance
2to the values characterizing the input signals (line 5), and a re-
place mechanism (lines 8-10) that replaces IwithnewI , ifnewI
has a higher objective function value. In addition, HCRR restarts
the search from time to time by replacing Iwith a randomly se-
lected test input (lines 11-13). We run the algorithm for (q 1)c
iterations where qis the size of the test suites, and cis the size of
candidate sets in the greedy selection algorithms in Figures 4 to 6.
This is to ensure that OC-S ELECTION spends the same test execu-
tion budget as the other selection algorithms. The OC-S ELECTION
algorithm keeps all the test inputs generated during the execution
in a set All(lines 2,7and14). At the end of the algorithm, from
the set All, we pickqtest inputs that have the highest objective
function values (line 15) and return them as the selected test suite.
4. EXPERIMENT SETUP
In this section, we present the research questions, and describe
our study subjects, our metric to measure fault revealing ability of
different selection algorithms, and our experiment design.
4.1 Research Questions
RQ1 (Fault Revealing Ability) .How does the fault revealing abil-
ity of our proposed test selection algorithms compare with one an-
other? We start by comparing the ability of the test suites generated
using the different test selection algorithms discussed in Section 3
in revealing faults in Stateï¬‚ow models. In particular, we are in-
terested to know (1) if our selection algorithms outperform input
diversity (baseline)? and (2) if there is any selection algorithm that
consistently reveals the most faults across different study subjects
and different fault types?
RQ2 (Fault Revealing Subsumption) Is any of our selection al-
gorithms subsumed by other algorithms? or for each selection al-Table 1: Characteristics of our study subject Stateï¬‚ow models.
Publicly AvailableNameNo. of InputsHierarchicalStatesParallelStatesNo. of StatesSCCASSNoNo234213162No1NoGCSYes8100YesNo. of Transitions255327
gorithm, are there some faults that can be found by that algorithm,
but not by others? This question investigates if any of the selection
algorithms discussed in Section 3 is subsumed by other algorithms,
i.e., if any selection algorithm does not ï¬nd any additional faults
missed by other algorithms.
RQ3 (Fault Revealing Complementarity) .What is the impact of
different failure types on fault revealing ability of our test selection
algorithms? This question investigates whether any of our selec-
tion algorithms has a tendency to reveal a certain type of failures
better than others. This shows whether our selection algorithms are
complementary to each other. That is, they reveal different types of
failures, thus suggesting they may be combined.
RQ4 (Test Suite Size) .What is the impact of the size of test suites
generated by our selection algorithms on their fault revealing abil-
ity?With this question, we study the impact of size on fault reveal-
ing ability of test suites, and investigate whether some selection
algorithms already perform well with small test suite sizes, while
some may require to enlarge test suites to better reveal faults.
4.2 Study Subjects
We use three Stateï¬‚ow models in our experiments: Two indus-
trial models from Delphi, namely, SCC (discussed in Section 2) and
Auto Start-Stop Control (ASS); and one public domain model from
Mathworks website [41], (i.e., Guidance Control System (GCS)).
Table 1 shows key characteristics of these models. All of these
three models have a continuous control output signal. Speciï¬cally,
the continuous control signal in SCC controls the clutch position,
in ASS, it controls the engine torque, and in GCS, it controls the
position of a missile. These models have a large number of input
variables. SCC and ASS have hierarchical states (OR states) and
GCS is a parallel state machine. The number of states and transi-
tions reported in Table 1 are those obtained after model ï¬‚attenning.
We note that our industrial subject models are representative in
terms of the size and complexity among Stateï¬‚ow models devel-
oped at Delphi. The number of input variables, transitions and
states of our industrial models is notably more than that of the
public domain models from Mathworks [44]. Further, most pub-
lic domain Stateï¬‚ows are small exemplars created for the purpose
of training and are not representative of the models developed in in-
dustry. Speciï¬cally, while discrete-continuous controllers are very
common in many embedded industry sectors, among the models
available at [44], only GCS was a discrete-continuous Stateï¬‚ow
controller and had a continuous control signal, and hence, we chose
it for our experiment. But since GCS continuous behavior was too
trivial, we modiï¬ed it before using it in our experiments by adding
some conï¬guration parameters and some difference equations in
some states. We have made the modiï¬ed version available at [23].
4.3 Measuring Fault Revealing Ability
In our study, we measure the fault revealing ability of test suites
generated by different selection algorithms. To automate our ex-
periments, we use fault-free versions of our subject models to gen-
erate test oracles (i.e, the ground truth oracle [5]). Let TSbe a test
suite generated by one of our selection algorithms and for a given
(faulty) model SF. For the purpose of this experiment, we assume
thatSFcontains a single fault only. We measure the ability of TS
6TS  IDSCTCODOSOC{1.Fault Seeding2.SelectionAlgorithmSFFaultySF{zz(size q)Figure 8: Our experiment design: Step 2 was repeated for 100
times due to the randomness in our selection algorithms.
in revealing the fault in SFusing a boolean measure. Our measure
returns true if there exists at least one test input in TSfor which the
output of SFsufï¬ciently deviates from the grand truth oracle such
that a manual tester conclusively detects a failure. Otherwise, our
measure returns false. Formally, let O=fsg1;:::;sg qgbe the
set of output signals obtained by running SFfor the test inputs in
TS=fI1;:::;I qg, and letG=fg1;:::;g qgbe the correspond-
ing test oracle signals. The fault revealing rate, denoted by FRR ,
is computed as follows:
(1)FRR (SF;TS) =(
191iq^dist(sgi;gi)>THR
081iq^dist(sgi;gi)<=THR
where ^dist(sgi;gi)is deï¬ned by Equation 1, and THR is a given
threshold. If we set THR to zero, then a test suite detects a given
fault (i.e., FRR = 1), if it is able to generate at least one output
that deviates from the oracle irrespective of the amount of devia-
tion. For continuous dynamic systems, however, the system output
is acceptable when the deviation is small and not necessarily zero.
Furthermore, for such systems, it is more likely that manual testers
recognize a faulty output signal when the signal shape drastically
differs from the oracle. In our work, we set THR to0:2. As a re-
sult, a test suite detects a given fault (i.e., FRR = 1), if it is able
to generate at least one output that diverges from the oracle such
that the distance between the oracle and the faulty output is more
than0:2. We arrived at this value for THR based on our experi-
ence and discussions with domain experts. In our experiments, in
addition, we obtained and evaluated the results for THR = 0:25
andTHS = 0:15and showed that our results were not sensitive to
such small changes in THR .
4.4 Experiment Design
Figure 8 shows the overall structure of our experiments consist-
ing of the following two steps:
Step1: Fault Seeding. We asked a Delphi engineer to seed 30
faults in each one of our two industry subject models ( z= 30),
generating 30 faulty versions of SCC and ASS, i.e, one fault per
each faulty version. The faults were seeded before our experiments
took place. The engineer was asked to choose the faults based on
his experience in Stateï¬‚ow model development and debugging. In
addition, we required the faults to be seeded in different parts of
the Stateï¬‚ow models and to be of different types. We categorize
the seeded faults into two groups: (1) Wrong Output Computation
which indicates a mistake in the equations computing the continu-
ous control output, e.g., replacing a min function with a max func-
tion or a operator with a +operator in the equations. (2) Wrong
Stateï¬‚ow Structure which indicates a mistake in the Stateï¬‚ow struc-
ture, such as wrong transition conditions or wrong priorities of the
transitions from the same source. As for the publicly available
model (GCS), since it was smaller and less complex than the Del-
phi models, we seeded 15 faults into the model to create 15 faulty
versions (z= 15). Among all the faulty models for each case study,
around 40% and 60% of the faults belong to the wrong output com-
putation and wrong Stateï¬‚ow structure categories, respectively.
Step2: Test Case Selection. As shown in Figure 8, after seed-
ing faults, for each faulty model, we ran our six selection algo-
rithms, namely Input Diversity (ID), State Coverage (SC), Transi-
tion Coverage (TC), Output Diversity (OD), Output Stability (OS),
(a) Average FRR values for different test suite sizes and threshold THR = 0.2
0.01.0
IDSCTCODOSOCAverage FRR values (q=5)0.5
SCC (THR = 0.15)
0.01.0
IDSCTCODOSOCAverage FRR values (q=10) 0.5(b) Average FRR values for test suite size q = 10 and three different thresholdsSCC (THR = 0.2)
IDSCTCODOSOCSCC (THR = 0.25)
IDSCTCODOSOCAverage FRR values (q=10)IDSCTCODOSOC
0.01.00.5
0.01.0
IDSCTCODOSOCAverage FRR values (q=25)0.5
Average FRR values (q=50)IDSCTCODOSOC0.01.00.5
Figure 9: Boxplots comparing fault revealing abilities of our
test selection algorithms for different test suite sizes and differ-
ent thresholds.
and Output Continuity (OC) test selection algorithms. For each
faulty model and each selection algorithm, we created a test suite
of sizeqwhereqtook the following values: 3,5,10,25, and 50.
We repeated the test selection step of our algorithm for 100 times
to account for the randomness in the selection algorithms. In sum-
mary, we created 75 faulty models (30 versions for SCC and ASS,
and 15 versions of GCS). For each faulty model and for each se-
lection algorithm, we created ï¬ve different test suites with sizes 3,
5, 10, 25 and 50. That is, we sampled 2250 different test suites
and repeated each sampling for a 100 times (i.e., in total, 225,000
different test suites were generated for our experiment). Overall,
our experiment took about 1600 hours time on a notebook with a
2.4GHz i7 CPU, 8 GB RAM, and 128 GB SSD.
5. RESULTS AND DISCUSSIONS
This section provides responses, based on our experiment design,
for research questions RQ1 toRQ4 described in Section 4.
RQ1 (Fault Revealing Ability) . To answer RQ1 , we ran the ex-
periment in Figure 8 with test suite sizes q=5, 10, 25, and 50, and
for all the 75 faulty models (i.e., z=30 for SCC, z=30 for ASS,
andz=15 for GCS). We computed the fault revealing rates FRR
with three thresholds THR =0.2, 0.15 and 0.25. Figure 9(a) shows
four plots comparing the fault revealing ability of the test selec-
tion algorithms discussed in Section 3 with THR =0.2. Each plot
in Figure 9(a) compares six distributions corresponding to our six
test selection algorithms. Each distribution consists of 75 points.
Each point relates to one faulty model, and represents the average
fault revealing ability of the 100 different test suites with a ï¬xed
size and obtained by applying one of our test selection algorithms
to that faulty model. For example, a point with (x = SC) and (y
= 0.32) in the ( q= 5) plot of Figure 9(a) indicates that among the
100 different test suites with size 5 generated by applying SC to
one faulty model, 32 test suites were able to reveal the fault (i.e.,
7FRR = 1) and 68 could not reveal that fault (i.e., FRR = 0).
To statistically compare the fault revealing ability of different
selection algorithms, we performed the non-parametric pairwise
Wilcoxon Pairs Signed Ranks test [8], and calculated the effect
size using Cohenâ€™s d[12]. The level of signiï¬cance ( ) was set
to0:05, and, following standard practice, dwas labeled â€œsmallâ€ for
0:2d < 0:5, â€œmediumâ€ for 0:5d < 0:8, and â€œhighâ€ for
d0:8[12].
Comparison with Input Diversity . Testing differences in FRR
distributions with THR =0.2 shows that, for all the test suite sizes,
all the test selection algorithms perform signiï¬cantly better than
ID. In addition, for all the test suite sizes, the effect size is â€œhighâ€
for OD, OS and OC, and â€œmediumâ€ for SC and TC.
Coverage achieved by coverage-based algorithms . In our ex-
periments, on average for the 100 different test suites obtained
by SC/TC selection algorithms and for our three subject models,
we achieved 81/65%, 88/71%, 93/76% and 97/81% state/transition
coverage for the test suites with size 5,10,25and50, respectively.
Further, we noticed that the largest test suites generated by our
coverage-based selection algorithms (i.e., q= 50 ) were able to
execute the faulty states or transitions of 73out of the 75faulty
models.
Comparing output-based and coverage-based algorithms . For
all the test suite sizes, statistical test results indicate that OD, OS,
and OC perform signiï¬cantly better than SC and TC. For OS and
for all the test suite sizes, the effect size is â€œhighâ€. For OD with all
the test suite sizes except for q= 50 , the effect size is â€œmediumâ€,
and forq= 50 , the effect size is â€œhighâ€. For OC with all the test
suite sizes except for q= 50 , the effect size is â€œmediumâ€, and for
q= 50 , the effect size is â€œlowâ€.
Comparing output-based algorithms . Forq=5 and 10, OS is
signiï¬cantly better than OD and OC with effect sizes of â€œmediumâ€
(forq=5) and "low" (for q=10). However, neither of OC and OD
is better than the other for q=5 and 10. For q=25, OS is better
than OD with a "low" effect size, with no signiï¬cant difference
between OS and OC or OC and OD. Finally, for q= 50 , there is
no signiï¬cant difference between OS, OC and OD.
Modifying THR . The above statistical test results were con-
sistent with those obtained based on FRR values computed with
THR =0.25 and 0.15. As an example, Figure 9(b) shows average
FRR values forq=10, for THR =0.15, 0.2 and 0.15. Increasing the
threshold from 0:15to0:25decreases the FRR values but, how-
ever, does not change the relative differences in FRR values across
different selection algorithms.
In summary , the answer to RQ1 is that the test suites generated
by OD, OS, OC, SC, and TC, have signiï¬cantly higher fault re-
vealing ability than those generated by ID. Further, even though
coverage-based algorithms (SC and TC) were able to achieve a high
coverage and execute the faulty states or transitions of 73 faulty
models, the failure-based and output diversity algorithms (OS, OC,
and OD) generate test suites with signiï¬cantly higher fault reveal-
ing ability compared to those generated by SC and TC. For smaller
test suites (q<25), OS performs better than OC and OD, while for
q=50, we did not observe any signiï¬cant differences among the
failure-based and output diversity algorithms (OS, OC, and OD).
Finally, our results are not impacted by small modiï¬cations in the
threshold values used to compute the fault revealing measure FRR .
RQ2 (Fault Revealing Subsumption). To answer RQ2 , we con-
sider the results of the experiment in Figure 9(a). We applied the
Wilcoxon test to identify, for each of the 75 faulty models, which
selection algorithm yielded the highest fault revealing rate (i.e., the
highest average FRR over 100 runs). Figure 10 and Table 2 show
the results. Figure 10 shows which algorithms are best in ï¬nding
SCCASSGCSSCTCODOSOCFaults (q=10)123456789101234567892012345678930SCTCODOSOCSCTCODOSOCFaults (q=25)123456789101234567892012345678930
Faults (q=50)123456789101234567892012345678930SCCASSGCSSCTCODOSOCFaults (q=5)123456789101234567892012345678930SCTCODOSOCSCTCODOSOCFigure 10: The best selection algorithm(s) for each of the 75
faulty models.
Table 2: The number of faults (out of 75) found inclusively (I)
and exclusively (E) by each algorithm and for each test suite
size.
q=5SCq=108 / 08 / 011 / 0q=25q=50TCODOSOC8/120 / 851 / 3228 / 824 / 023 / 059 / 363 / 750 / 29/128 / 851 / 2432 / 813/041 / 655 / 1044 / 7(I / E)(I / E)(I / E)(I / E)(I / E)
each of the 75 faults (30 for SCC, 30 for ASS, and 15 for GCS) for
each test suite size ( q= 5, 10, 25 and 50). In this ï¬gure, an algo-
rithm A is marked as best for a fault F (denoted by ), if, based on
the Wilcoxon test results for F, there is no other algorithm that is
signiï¬cantly better than A in revealing F. Table 2 shows two num-
bers I/E for each algorithm and for each test suite size. Speciï¬-
cally, given a pair I/E for an algorithm A, I indicates the number
of faults that are best found by A and possibly by some other algo-
rithms (i.e., inclusively found by A), while E indicates the number
of faults that are best found by A only (i.e., exclusively found by
A). For example, when the test suite size is 5, OD is among the best
algorithms in ï¬nding 20 faults, and among these 20 faults, OD is
the only best algorithm for 8 faults.
Coverage algorithms. As shown in Table 2, SC is subsumed by
the other algorithms for every test suite size (E = 0). That is, SC
does not ï¬nd any fault exclusively, and any fault found by SC is also
found with the same or higher probability by some other algorithm.
TC is able to ï¬nd one fault exclusively for q<25, but is subsumed
by other algorithms for q25. Further, based on Figure 10, SC
and TC together are able to ï¬nd three faults exclusively for q=5
(11 of ASS, and 10 and 14 of GCS), and two faults exclusively for
q=10 (11 of ASS, and 14 of GCS). However, for q25, they are
subsumed by OD.
Output-based algorithms. As shown in Table 2, OS fares best
as it ï¬nds the most number of faults both inclusively and exclu-
sively for different values of q. In contrast, OD shows the highest
growth in the number of inclusively and exclusively found faults as
qincreases compared to OS and OC.
In summary, the answer to RQ2 is that coverage algorithms ï¬nd
the least number of faults both exclusively and inclusively, and
as test suite size increases, these algorithms are subsumed by the
output diversity (OD) algorithm. The output-based algorithms are
complementary (i.e., are not subsumed by one another) and while
output stability (OS) ï¬nds the highest number of faults both in-
clusively and exclusively, output diversity (OD) shows the highest
8q = 5(a) Average FRR values with THR =0.2 and for instability failures (20 failures)
0.01.0Average FRR values0.5(b) Average FRR values with THR =0.2 and for discontinuity failures (7 failures)
SCTCODOSOC
(c) Average FRR values with THR =0.2 and for other failures (48 failures)
q = 10
0.01.0Average FRR values0.5
0.01.0Average FRR values0.5q = 25
SCTCODOSOCSCTCODOSOC
q = 50
SCTCODOSOC
q = 5q = 10q = 25q = 50
SCTCODOSOCSCTCODOSOCSCTCODOSOCSCTCODOSOC
SCTCODOSOCSCTCODOSOCSCTCODOSOCSCTCODOSOCq = 5q = 10q = 25q = 50Figure 11: The average FRR values for different types of fail-
ures and for different test suite sizes.
improvement in fault ï¬nding as the test suite size increases.
RQ3 (Fault Revealing Complementarity). To answer RQ3 ,
we ï¬rst divide the 75 faulty models in our experiments based on
the failure type that they exhibit. To determine the failure type
exhibited by a faulty model, we inspect the output that yields the
highest FRR among the outputs produced by the test suites re-
lated to that model. We identiï¬ed three types of failures in these
outputs and divided the 75 faulty models into the following three
groups: (1) the faulty models exhibiting instability failure (20 mod-
els), (2) the faulty models exhibiting discontinuity failure (7 mod-
els), and (3) the other models that neither show instability nor dis-
continuity (48 models). Figures 11(a) to (c) compare the fault re-
vealing ability of our test selection algorithms for test suite sizes q
= 5, 10, 25, and 50 and for each of the above three categories of
failures (i.e., instability, discontinuity, and other).
Instability and discontinuity. The statistical test results show
that, for the instability failure, OS has the highest fault revealing
rate forq=5, 10, and 25. Similarly for the discontinuity failure,
OC has the highest fault revealing rate for q=5 and 10. How-
ever, for larger test suites ( q= 50 for instability, and q= 25 and 50
for discontinuity), OS, OC and OD are equally good at ï¬nding the
instability and discontinuity failures.
Other. As for the â€œotherâ€ failures, OS and OD are better able
to ï¬nd these failures compared to other algorithms for q= 5, 10,
and 50. For q= 25, there is no signiï¬cant difference between OS,
OD and OC in revealing these failures. However, as shown in Fig-
ure 11(c), for q= 50, the FRR value distribution for OD has the
highest average compared to other algorithms. Further, the variance
ofFRR values for OD in Figure 11(c) with q= 50 is the lowest,
making OD the best algorithm for ï¬nding failures other than insta-
bility and discontinuity when large test suites are available.
Discontinuity
SCTCODOSOC**++--InstabilityOthers
0.00.51.0
35102550****+++--
Test Suite SizeFRR Mean+--3510255035102550
***---+++---***+++++***---Average FRR values vs. Test suite size for different classes of failures (THR = 0.2)Figure 12: The impact of test suite size on the average FRR
over 100 test suites of different faulty models.
In summary, the answer to RQ3 is that when test suites are small,
OS and OC show a clear tendency to, respectively, reveal the insta-
bility and discontinuity failures better than other types of failures
and better than other algorithms. With large test suites, however,
OS, OC and OD are equally good at ï¬nding the instability and dis-
continuity failures. Further, with small test suites, OS and OD are
better than other algorithms in revealing failures other than insta-
bility and discontinuity. For large test suites, however, OD shows
a tendency to perform better for the â€œotherâ€ types of failures since
by diversifying outputs it increases the chances of ï¬nding failures
not following any speciï¬c pattern.
RQ4 (Test Suite Size). To answer RQ4 , we extended the exper-
iment in Figure 9 to include q=3, as well. Figure 12 shows how
the average of FRR over 100 test suites for different faulty mod-
els and in different failure groups is impacted by increasing the test
suite size. Speciï¬cally, for Figure 12, the 75 faulty models are di-
vided based on the failure type they exhibit (20 for instability, 7 for
discontinuity, and 48 for others).
According to Figure 12, OS performs best in revealing instability
failures even with small test suite sizes where its average FRR is
very close to one (0.97). Similarly, OC performs best in revealing
discontinuity failures and its average FRR for small test suites is
already very high, i.e., 0.95. For â€œotherâ€ kinds of failures, OS per-
forms best for very small test suites, but for q10, OD performs
the best. Finally, for instability and discontinuity, OS, OD and OC
perform better than SC and TC for all test suite sizes, while for
other failures, OS and OD perform better than OC, SC and TC for
all the test suite sizes.
In summary, the answer to RQ4 is that the fault revealing ability
of OS (respectively, OC) for instability (respectively, discontinuity)
failures is very high for small test suites and almost equal to the
highest possible fault revealing rate value. For failures other than
instability and discontinuity, the ability of OD in revealing failures
rapidly improves as the test suite size increases, making OD the
best algorithm for such failures for test suite sizes more than or
equal to 10.
Discussion. We present our observations as to why the cover-
age algorithms are less effective than the output-based algorithms
for generating test suites for mixed discrete-continuous Stateï¬‚ows.
Further, we outline our future research direction on effective com-
bination of our output-based test selection algorithms.
Why coverage algorithms are less effective? Overall, our results
show that, compared to output-based algorithms, coverage algo-
rithms are less effective in revealing Stateï¬‚ow faults, and as dis-
cussed in RQ2 , they are subsumed by the output diversity algo-
rithm. Based on our experiments, even though test suites generated
by SC and TC cover the faulty parts of the Stateï¬‚ow models, they
fail to generate output signals that are sufï¬ciently distinct from the
oracle signal, hence yielding a low fault revealing rate. That is, a
discrete notion of state or transition coverage does not help reveal
9continuous output failures. Note that these failures depend on the
value changes of outputs over a continuous time interval. The poor
performance of coverage algorithms might be due to the fact that
state and transition coverage criteria do not account for the time
duration spent at each state or for the time instance at which a tran-
sition is triggered. For example, an objective to cover states while
trying to reduce the amount of time spent in each state may better
help reveal discontinuity failures (see Figure 3(b)).
Combining output-based selection algorithms. Our results show
that for large test suites and for allfailure types, the fault revealing
ability of OS, OC and OD are the same with an average FRR of
0.75 to 0.87. However, for smaller test suites and for speciï¬c fail-
ures, some algorithms (i.e., OS for instability and OC for disconti-
nuity) perform remarkably well with an average FRR higher than
0.95. This essentially eliminates the need to use large test suites
for those speciï¬c failure types. These ï¬ndings offer the potential
for engineers to combine our output-based algorithms to achieve
a small test suite with a high fault revealing rate. Recall that test
oracles for mixed discrete-continuous Stateï¬‚ows are manual, and
hence the test suite size has to be kept as low as possible (typically
q100). For example, given our results on fault revealing ability
of OS, OC, and OD, and assuming that a test suite size budget of
qis provided, we may allocate a small percentage of the test suite
size to OC to ï¬nd discontinuity failures, and share the rest of the
budget between OS and OD by giving OD a higher share. This is
because OS is able to ï¬nd instability failures with small test suites,
but also, it performs well at ï¬nding other failures. However, only
OD was able to subsume SC/TC with large test suites, and given
that ODâ€™s performance increases with the test suite size, a larger
test suite size might be allocated to OD. This suggests future work
to investigate guidelines on dividing the test suite size budget across
different output-based test selection algorithms.
6. RELATED WORK
What distinguishes our work from the existing model-based test-
ing approaches is that in our work, Stateï¬‚ow models are the arti-
facts under test, while in model-based testing, test cases are derived
from Stateï¬‚ow models or other state machine variants in order to
exercise the system on its target platform. These model-based test-
ing approaches often generate test cases from models using var-
ious automation mechanisms, e.g., search-based techniques [50],
model checking [27, 42], guided random testing [37, 33, 11] or a
combination of these techniques [36, 30]. In [31], a model-based
testing approach for mixed discrete-continuous Stateï¬‚ow models is
proposed where test inputs are generated based on discrete frag-
ments of Stateï¬‚ow models, and are applied to the original models
to obtain test oracles in terms of continuous signals. In all these ap-
proaches, Stateï¬‚ow models, in addition to generating test cases, are
used to automate test oracles. In reality, however, Stateï¬‚ows might
be faulty and cannot be used to automate test oracles. Hence, we
focus on generating small and minimal test suites to identify faults
in complex, executable Stateï¬‚ow models for which automated test
oracles are not available, a common situation in industry.
Formal methods and model checking techniques [42, 34, 10]
have been previously applied to verify Stateï¬‚ow models. These
approaches largely focus on Stateï¬‚ows with discrete behaviours,
and attempt to maximize (discrete) state or transition coverage. In
our work, we test Stateï¬‚ows with mixed discrete-continuous. Fur-
ther, our results show that, for discrete-continuous Stateï¬‚ows, test
inputs that cover faulty parts of Stateï¬‚ow models may not be able
to reveal faults (i.e., may not yield outputs with sufï¬cient distance
from the oracle). Hence, focusing on coverage alone may not result
in test suites with high fault revealing ability.We proposed two failure patterns capturing speciï¬c errors in
continuous outputs of Stateï¬‚ow models. Our failure patterns are
analogous to the notion of fault models [32]. Fault models pro-
vide abstract descriptions for speciï¬c things that can go wrong in
a certain domain [32]. Several examples of fault models have been
proposed (e.g., for access control policies [22], or for speciï¬c con-
currency or security faults [7, 32]). Our work is the ï¬rst to deï¬ne
such notion for mixed discrete-continuous Stateï¬‚ows and apply it
using meta-heuristic search. Our failure patterns capture the intu-
ition of domain experts, and are deï¬ned over continuous controller
outputs. We further note that instability and discontinuity patterns
are, respectively, similar to the accuracy and change rate properties
that are typically used to characterize physical behaviour of cyber
physical software controllers [16].
Our output diversity selection algorithm is inspired by the out-
put uniqueness criterion that has been proposed and evaluated in
the context of web application testing [1, 2], and has shown to be a
useful surrogate to white-box coverage selection criteria [2]. How-
ever, while in [1, 2], output uniqueness is characterized based on
the textual, visual or structural aspects of HTML code, in our work,
we deï¬ne output diversity as Euclidean distance between pairs of
continuous output signals and apply it to Stateï¬‚ow models.
Several approaches to test input generation, when test inputs are
discrete, rely on techniques such as symbolic execution, constraint
solvers or model checkers (e.g., [47]). In our coverage-based algo-
rithms (SC and TC), we used adaptive random test input generation
because our test inputs are signals. As discussed in Section 5, with
our adaptive random strategy, SC and TC were able to achieve a
high coverage despite small test suite sizes. Adapting symbolic
techniques to generate test input signals is left for future work.
7. CONCLUSIONS
Embedded software controllers are largely developed using discrete-
continuous Stateï¬‚ows. To reduce the cost of manual test oracles as-
sociated with Stateï¬‚ow models, test case selection algorithms are
required. These algorithms aim at providing minimal test suites
with high fault revealing power. We proposed and evaluated six
test selection algorithms for discrete-continuous Stateï¬‚ows: three
output-based (OD, OS, OC), two coverage-based (SC, TC), and one
input-based (ID). Our experiments based on two industrial and one
public domain Stateï¬‚ow models showed that the output-based al-
gorithms consistently outperform the coverage-based algorithms in
revealing faults in mixed discrete-continuous Stateï¬‚ows. Further,
for test suites larger than 25, the output-based algorithms were able
to ï¬nd with the same or higher probability all the faults revealed by
the coverage-based algorithms, and hence subsumed them. In addi-
tion, OS and OC selection algorithms had very high fault revealing
rates, even with small test suites, for instability and discontinuity
failures, respectively. For the other failures, OD outperformed the
other algorithms in ï¬nding faults for test suite sizes larger than 10,
and further, its fault detection rate kept improving at a faster rate
than the others when increasing the test suite size.
In future, we will seek to develop optimal guidelines on divid-
ing test oracle budget across our output-based selection algorithms.
Further, we intend to apply our test selection algorithms to mod-
els consisting of Simulink blocks as well as Stateï¬‚ow models with
discrete-continuous behaviors, as Stateï¬‚ow models are most often
embedded in a network of Simulink blocks.
Acknowledgments
We thank Fabrizio Pastore for his useful comments on a draft of this
paper. Supported by the Fonds National de la Recherche, Luxem-
bourg (FNR/P10/03 - Veriï¬cation and Validation Laboratory, and
FNR 4878364), and Delphi Automotive Systems, Luxembourg.
108. REFERENCES
[1] N. Alshahwan and M. Harman. Augmenting test suites
effectiveness by increasing output diversity. In Proceedings
of the 34th International Conference on Software
Engineering , pages 1345â€“1348. IEEE Press, 2012.
[2] N. Alshahwan and M. Harman. Coverage and fault detection
of the output-uniqueness test selection criteria. In
Proceedings of the 2014 International Symposium on
Software Testing and Analysis , pages 181â€“192. ACM, 2014.
[3] A. Arcuri and L. Briand. Adaptive random testing: An
illusion of effectiveness? In Proceedings of the 2011
International Symposium on Software Testing and Analysis ,
pages 265â€“275. ACM, 2011.
[4] X. Bai, K. Hou, H. Lu, Y . Zhang, L. Hu, and H. Ye.
Semantic-based test oracles. In Computer Software and
Applications Conference (COMPSAC), 2011 IEEE 35th
Annual , pages 640â€“649. IEEE, 2011.
[5] E. T. Barr, M. Harman, P. McMinn, M. Shahbaz, and S. Yoo.
The oracle problem in software testing: A survey. IEEE
transactions on software engineering , 2015.
[6] R. Binder. Testing object-oriented systems: models, patterns,
and tools . Addison-Wesley Professional, 2000.
[7] M. Buchler, J. Oudinet, and A. Pretschner. Semi-automatic
security testing of web applications from a secure model. In
Software Security and Reliability (SERE), 2012 IEEE Sixth
International Conference on , pages 253â€“262. IEEE, 2012.
[8] J. A. Capon. Elementary Statistics for the Social Sciences:
Study Guide . Wadsworth Publishing Company, 1991.
[9] T. Y . Chen, F.-C. Kuo, R. G. Merkel, and T. Tse. Adaptive
random testing: The art of test case diversity. Journal of
Systems and Software , 83(1):60â€“66, 2010.
[10] E. M. Clarke, Jr., O. Grumberg, and D. A. Peled. Model
Checking . MIT Press, 1999.
[11] R. Cleaveland, S. A. Smolka, and S. T. Sims. An
instrumentation-based approach to controller model
validation. In Model-Driven Development of Reliable
Automotive Services , pages 84â€“97. Springer, 2008.
[12] J. Cohen. Statistical power analysis for the behavioral
sciences (rev) . Lawrence Erlbaum Associates, Inc, 1977.
[13] R. Colgren. Basic MATLAB, Simulink and Stateï¬‚ow . AIAA
(American Institute of Aeronautics and Astronautics), 2006.
[14] D. Coppit and J. M. Haddox-Schatz. On the use of
speciï¬cation-based assertions as test oracles. In Software
Engineering Workshop, 2005. 29th Annual IEEE/NASA ,
pages 305â€“314. IEEE, 2005.
[15] M. Gligoric, S. Negara, O. Legunsen, and D. Marinov. An
empirical evaluation and comparison of manual and
automated test selection. In Proceedings of the 29th
ACM/IEEE international conference on Automated software
engineering , pages 361â€“372. ACM, 2014.
[16] M. P. Heimdahl, L. Duan, A. Murugesan, and
S. Rayadurgam. Modeling and requirements on the physical
side of cyber-physical systems. In 2nd International
Workshop on the Twin Peaks of Requirements and
Architecture (TwinPeaks), 2013 , pages 1â€“7. IEEE, 2013.
[17] H. Hemmati, A. Arcuri, and L. Briand. Empirical
investigation of the effects of test suite properties on
similarity-based test case selection. In Software Testing,
Veriï¬cation and Validation (ICST), 2011 IEEE Fourth
International Conference on , pages 327â€“336. IEEE, 2011.
[18] T. Henzinger. The theory of hybrid automata. In LICS , pages
278â€“292, 1996.[19] T. Henzinger and J. Sifakis. The embedded systems design
challenge. In FM, pages 1â€“15, 2006.
[20] L. Inozemtseva and R. Holmes. Coverage is not strongly
correlated with test suite effectiveness. In Proceedings of the
36th International Conference on Software Engineering ,
pages 435â€“445. ACM, 2014.
[21] S. Luke. Essentials of metaheuristics , volume 113. Lulu
Raleigh, 2009.
[22] E. Martin and T. Xie. A fault model and mutation testing of
access control policies. In Proceedings of the 16th
international conference on World Wide Web , pages
667â€“676. ACM, 2007.
[23] R. Matinnejad. The modiï¬ed version of GCS Stateï¬‚ow.
https://drive.google.com/file/d/
0B104wPnuxJVhSU44WFlTVE84bmM/view?usp=sharing .
[Online; accessed 10-March-2015].
[24] R. Matinnejad, S. Nejati, L. Briand, T. Bruckmann, and
C. Poull. Automated model-in-the-loop testing of continuous
controllers using search. In Search Based Software
Engineering , pages 141â€“157. Springer, 2013.
[25] R. Matinnejad, S. Nejati, L. Briand, T. Bruckmann, and
C. Poull. Search-based automated testing of continuous
controllers: Framework, tool support, and case studies.
Information and Software Technology , 57:705â€“722, 2015.
[26] P. McMinn, M. Stevenson, and M. Harman. Reducing
qualitative human oracle costs associated with automatically
generated test data. In Proceedings of the First International
Workshop on Software Test Output Validation , pages 1â€“4.
ACM, 2010.
[27] S. Mohalik, A. A. Gadkari, A. Yeolekar, K. Shashidhar, and
S. Ramesh. Automatic test case generation from
simulink/stateï¬‚ow models using model checking. Software
Testing, Veriï¬cation and Reliability , 24(2):155â€“180, 2014.
[28] A. S. Namin and J. H. Andrews. The inï¬‚uence of size and
coverage on test suite effectiveness. In Proceedings of the
eighteenth international symposium on Software testing and
analysis , pages 57â€“68. ACM, 2009.
[29] P. A. Nardi. On test oracles for Simulink-like models . PhD
thesis, Universidade de SÃ£o Paulo, 2014.
[30] P. Peranandam, S. Raviram, M. Satpathy, A. Yeolekar,
A. Gadkari, and S. Ramesh. An integrated test generation
tool for enhanced coverage of simulink/stateï¬‚ow models. In
Design, Automation & Test in Europe Conference &
Exhibition (DATE), 2012 , pages 308â€“311. IEEE, 2013.
[31] J. Philipps, G. Hahn, A. Pretschner, and T. Stauner. Tests for
mixed discrete-continuous reactive systems. In 14th IEEE
International Workshop on Rapid Systems Prototyping,
Proceedings. , pages 78â€“84. IEEE, 2003.
[32] A. Pretschner, D. Holling, R. Eschbach, and M. Gemmar. A
generic fault model for quality assurance. In Model-Driven
Engineering Languages and Systems , pages 87â€“103.
Springer, 2013.
[33] Reactive Systems Inc. Reactis Tester.
http://www.reactive-systems.com/
simulink-testing-validation.html , 2010. [Online;
accessed 25-Nov-2013].
[34] Reactive Systems Inc. Reactis Validator.
http://www.reactive-systems.com/
simulink-testing-validation.html , 2010. [Online;
accessed 25-Nov-2013].
[35] D. J. Richardson, S. L. Aha, and T. O. Oâ€™malley.
11Speciï¬cation-based test oracles for reactive systems. In
Proceedings of the 14th international conference on
Software engineering , pages 105â€“118. ACM, 1992.
[36] M. Satpathy, A. Yeolekar, P. Peranandam, and S. Ramesh.
Efï¬cient coverage of parallel and hierarchical stateï¬‚ow
models for test case generation. Software Testing,
Veriï¬cation and Reliability , 22(7):457â€“479, 2012.
[37] M. Satpathy, A. Yeolekar, and S. Ramesh. Randomized
directed testing (redirect) for simulink/stateï¬‚ow models. In
Proceedings of the 8th ACM international conference on
Embedded software , pages 217â€“226. ACM, 2008.
[38] M. Staats, M. W. Whalen, and M. P. E. Heimdahl. Programs,
tests, and oracles: the foundations of testing revisited. In
Software Engineering (ICSE), 2011 33rd International
Conference on , pages 391â€“400. IEEE, 2011.
[39] T. Stauner. Properties of hybrid systems-a computer science
perspective. Formal Methods in System Design ,
24(3):223â€“259, 2004.
[40] The MathWorks Inc. Common Modeling Errors Stateï¬‚ow
Can Detect.
http://nl.mathworks.com/help/stateflow/ug/
common-modeling-errors-the-debugger-can-detect.html .
[Online; accessed 23-Fev-2015].
[41] The MathWorks Inc. Designing a Guidance System in
MATLAB/Simulink.
http://nl.mathworks.com/help/simulink/examples/
designing-a-guidance-system-in-matlab-and-simulink.
html. [Online; accessed 23-Fev-2015].
[42] The MathWorks Inc. Simulink Design Veriï¬er.
http://nl.mathworks.com/products/sldesignverifier/?refresh=true . [Online; accessed 6-May-2015].
[43] The MathWorks Inc. Stateï¬‚ow.
http://www.mathworks.nl/products/stateflow . [Online;
accessed 23-Fev-2015].
[44] The MathWorks Inc. Stateï¬‚ow Model Examples.
http://nl.mathworks.com/products/stateflow/
model-examples.html . [Online; accessed 23-Fev-2015].
[45] A. Tiwari. Formal semantics and analysis methods for
simulink stateï¬‚ow models. Unpublished report, SRI
International , 2002.
[46] A. K. Tyagi. MATLAB and SIMULINK for Engineers .
Oxford University Press, 2012.
[47] W. Visser, C. S. Pasareanu, and S. Khurshid. Test input
generation with java pathï¬nder. ACM SIGSOFT Software
Engineering Notes , 29(4):97â€“107, 2004.
[48] Wikipedia. Open-loop controller.
http://en.wikipedia.org/wiki/Open-loop_controller .
[Online; last accessed 10-Nov-2014].
[49] Wikipedia. V oltage Spike.
http://en.wikipedia.org/wiki/Voltage_spike . [Online;
accessed 23-Fev-2015].
[50] A. Windisch. Search-based test data generation from
stateï¬‚ow statecharts. In Proceedings of the 12th annual
conference on Genetic and evolutionary computation , pages
1349â€“1356. ACM, 2010.
[51] J. Zander, I. Schieferdecker, and P. J. Mosterman.
Model-based testing for embedded systems . CRC Press,
2012.
12