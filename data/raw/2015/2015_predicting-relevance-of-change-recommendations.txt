Predicting Relevance of Change Recommendations
Thomas Rolfsnes
Simula Research Laboratory
Oslo, Norway
thomgrol@simula.noLeon Moonen
Simula Research Laboratory
Oslo, Norway
leon.moonen@computer.orgDavid Binkley
Loyola University Maryland
Baltimore, Maryland, USA
binkley@cs.loyola.edu
Abstract ‚ÄîSoftware change recommendation seeks to suggest
artifacts (e.g., Ô¨Åles or methods) that are related to changes made
by a developer, and thus identiÔ¨Åes possible omissions or next
steps. While one obvious challenge for recommender systems is to
produce accurate recommendations, a complimentary challenge
is to rank recommendations based on their relevance. In this
paper, we address this challenge for recommendation systemsthat are based on evolutionary coupling. Such systems use
targeted association-rule mining to identify relevant patterns in
a software system‚Äôs change history. Traditionally, this processinvolves ranking artifacts using interestingness measures such as
conÔ¨Ådence and support. However, these measures often fall short
when used to assess recommendation relevance.
We propose the use of random forest classiÔ¨Åcation models to
assess recommendation relevance. This approach improves onpast use of various interestingness measures by learning fromprevious change recommendations. We empirically evaluate our
approach on fourteen open source systems and two systems fromour industry partners. Furthermore, we consider complimenting
two mining algorithms: C
O-CHANGE and T ARMAQ . The results
Ô¨Ånd that random forest classiÔ¨Åcation signiÔ¨Åcantly outperforms
previous approaches, receives lower Brier scores, and has su-
perior trade-off between precision and recall. The results are
consistent across software system and mining algorithm.
Index T erms‚Äîrecommendation conÔ¨Ådence, evolutionary cou-
pling, targeted association rule mining, random forests.
I. I NTRODUCTION
When software systems evolve, the amount and complexity
of interactions in the code grows. As a result, it becomesincreasingly challenging for developers to foresee and reasonabout the effects of a change to the system. One proposedsolution, change impact analysis [6], aims to identify softwareartifacts (e.g., Ô¨Åles, methods, classes) affected by a givenset of changes. The impacted artifacts form the basis forchange recommendations, which suggests to a developer alist of artifacts that are related to their (proposed) changesto the code, supporting in the process of addressing change
propagation and ripple-effects [17, 12, 40, 44].
One promising approach to change recommendation aims
to identify potentially relevant items based on evolutionary
(orlogical )coupling. This approach can be based on a range
of granularities of co-change information [14, 5, 33] as well
as code-churn [15] or even interactions with an IDE [42].Change recommendation based on evolutionary coupling hasthe intriguing property that it effectively taps into the inherentknowledge that software developers have regarding depen-dencies between the artifacts of a system. Our present work
considers co-change information extracted from a versioncontrol system such as Git, SVN, or Mercurial.One challenge faced by all recommender systems are false
positives. This challenge becomes acute if developers come
to believe that automated tools are ‚Äúmostly wrong‚Äù [31].
Clearly, algorithms with higher accuracy will help address
this challenge. However, we can also address this challengewith algorithms that assess the relevance of a proposed rec-
ommendation. In fact, the two approaches are complimentary.In this paper we hypothesize that history aware relevance
prediction, which exploits earlier change recommendations toassess the relevance of a current recommendation, and ranksrecommendations according to decreasing relevance, can helpmitigate the challenge of false positives.
Our approach consists of training a random forest classiÔ¨Å-
cation model [7] on previous change recommendations with
known relevance. The model is used to give a single likelihood
estimate of the relevance of future change recommendations.
Automatic assessment of recommendation relevance frees de-velopers from having to perform this time-consuming task.Our work facilitates further automation of the change recom-mendation process, only notifying the developer when relevantrecommendations are available. Furthermore, our approachcompliments existing research work on improving miningalgorithm accuracy, as its application is independent of themining algorithm used to generate recommendations.
Contributions: (a) We present twelve features describing
aspects of change sets, change histories, and generated change
recommendations. These features are used to build a random
forest classiÔ¨Åcation model for recommendation relevance. (b)
We assess our model in a large empirical study encompassingsixteen software systems, including two from our industrypartners Cisco and KM . Furthermore, change recommenda-
tions are generated using both C
O-C HANGE and T ARMAQ to
assess the external validity of our approach. (c) We evaluate
the importance of each of the twelve features used in ourrelevance classiÔ¨Åcation model.
II. O
VERALL APPROACH
The overarching goal of this paper is to attach appropriaterelevance scores to change recommendations based on associ-
ation rule mining. We consider this question of relevance froma developer viewpoint, where ‚Äúrelevant‚Äù means ‚Äúuseful for the
developer‚Äù. We therefore consider a change recommendationrelevant if it contains a correct artifact as one of its top ten
highest ranked artifacts.
We propose an approach that uses classiÔ¨Åcation based on
random forests [7] to learn from previous change recommen-
dations in order to assess the current one. Thus, relevance of
978-1-5386-2684-9/17/$31.00 c/circlecopyrt2017 IEEEASE 2017, Urbana-Champaign, IL, USA
T echnical Research694
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:19:43 UTC from IEEE Xplore.  Restrictions apply. a new recommendation is assessed based on the known rele-
vance of historically similar and dissimilar recommendations.
Traditionally, the same interestingness measure used to rank
the artifacts of a recommendation are also used to assess its
relevance [37, 28, 16]. For example, an interestingness mea-sure might weight artifacts on a range from 0 to 1, enablingan internal ranking. Given the ranking it is then up to the userto assess whether the recommendation is relevant. Naturally,if the top ranked artifacts have received weights close to themaximum (1 in this example), the recommendation is assumedrelevant and will likely be acted upon. Recent work found that,in the context of software change recommendation, Agrawal‚ÄôsconÔ¨Ådence interestingness measure [1] performs among the
top-in-class when compared to more complex measures [30].
Considering this result, we use conÔ¨Ådence as a baseline and setout to compare the relevance predictions given by our proposedapproach against those based on conÔ¨Ådence:
RQ 1 Does classiÔ¨Åcation based on random forests improve upon the
use of conÔ¨Ådence as a relevance classiÔ¨Åer?
To train our classiÔ¨Åcation model, a range of features mustbe introduced that describe attributes related to a change rec-ommendation. The better these features capture key attributes
the better we can learn from previous recommendations andconsequently the better we can assess the relevance of a currentrecommendation. Thus our second research question is
RQ 2 What are the most important features for classifying change
recommendation relevance?
In our study we use random forests for their proven per-formance [9] and intrinsic ability to assess variable (feature)
importance [7]. By answering our two research questions weseek to uncover whether change recommendation relevance is
a viable venue for further research. If so, our approach mayprove to be an important compliment to existing algorithms forchange recommendation, helping to gain both better recom-mendations, and higher conÔ¨Ådence in those recommendations.
III. R
ELA TED WORK
Recommendation Relevance: A shared goal in recommen-
dation systems is uncovering interesting Ô¨Åndings in a data-set,
which means that an important research question concernswhat actually characterizes the interestingness of a Ô¨Ånding.
Over the years, numerous measures for interestingness havebeen proposed [37, 28, 16, 24, 21]. A recent evaluation of 39of these measures in the context of software change recom-mendation found that the traditional measures of conÔ¨Ådence
and support perform just as well as more recent and often
more complex measures [30].
Cheetham and Price evaluated indicators (features) that can
be used to provide conÔ¨Ådence scores for case based reasoning
systems [10, 11]. To provide a recommendation for a new case,
thek-nearest neighbor algorithm was used, thus the evaluated
features were tightly woven with the kind of output thatthek-nearest neighbor algorithm produces. Example features
include the ‚ÄúNumber of cases retrieved with best solution‚Äù and
‚ÄúSimilarity of most similar case.‚Äù The features were tested forimportance using leave-one-out testing in combination with
the decision tree algorithm C4.5. In comparison, our use ofrandom decision trees avoids the need for leave out testing offeatures as feature importance is internally accounted for.
Le et al. propose an approach for predicting whether the
output of a bug localization tool is relevant [23, 22]. As forthis paper, an output is considered relevant if a true positiveis part of the top 10 recommended artifacts. While change
recommendation can be seen as stopping faults before they
happen, bug localization is a complementary approach for
already existing faults. State of the art bug localization is basedon comparing normal and faulty execution traces (spectrumbased). In order to predict relevance, Le et al. identify 50features related primarily to traces, but also considered theweights of recommended (suspicious) artifacts. These featureswere then used to train a classiÔ¨Åcation model for relevancebased on support vector machines.
Rule Aggregation, Clustering, and Filtering: Rolfsnes
et al. propose aggregation of association rules to combine
their evidence (or interestingness) [35]. Aggregation likelyincreases recommendation relevance: for example, considerthree rules, one recommending Awith conÔ¨Ådence 0.8 and two
recommending Bwith conÔ¨Ådence 0.7 and 0.6 respectively.
Traditional approaches would use the highest ranking ruleand thus prioritize Aover B. Rule aggregation combines the
evidence for Band thus leads to recommending Bover A.
Several authors propose methods to discover the most infor-
mative rules in a large collection of mined association rules, byeither clustering rules that convey redundant information [38,26, 20], or by pruning non-interesting rules [41, 3]. The overall
idea is that the removal of rules will reduce the noise in therecommendations made using the remaining rules. However,in contrast to rule aggregation, and to the approach proposedin this paper, recommendations will be based on only partof the available evidence. It remains open to future work toinvestigate how this affects their relevance.
Parameter Tuning: Recent research highlighted that the
conÔ¨Åguration parameters of data mining algorithms have a
signiÔ¨Åcant impact on the quality of their results [27]. Inthe context of association rule mining, several authors havehighlighted the need for thoughtfully studying how parametersettings affect the quality of generated rules [43, 25, 18].
Moonen et al. investigate how the quality of software change
recommendation varied depending on association rule miningparameters such as transaction Ô¨Åltering threshold, historylength, and history age [30, 29]. In contrast to that work,which focused on conÔ¨Ågurable parameters of the algorithm,this paper considers non-conÔ¨Ågurable features of the query,
the change history, and the recommendation history.
IV . G
ENERA TING CHANGE RECOMMENDA TIONS
A. Software Change Recommendation
Recommender (or recommendation) engines are information
Ô¨Åltering systems whose goal is to predict relevant items for
a speciÔ¨Åc purpose [32]. A common use of recommendersystems is in marketing where these systems typically leverage
695
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:19:43 UTC from IEEE Xplore.  Restrictions apply. a shopper‚Äôs previous purchases and the purchases of other
shoppers to predict items of potential interest to the shopper.
In the context of software engineering, these systems typi-
cally leverage a developer‚Äôs previous changes together with
the changes made by other developers to predict items ofinterest. Software change recommendation takes as input a
set of changed entities, referred to as a change set orquery,
and predicts a set of entities that are also likely in need ofchange. These entities may be any software artifact, such as
Ô¨Åles, methods, models, or text documents. The study described
in this paper considers both Ô¨Åles and methods as potential
artifacts. We extract these artifacts from the version controlhistory of a software system. Thus, software recommendationhelps answering questions such as ‚ÄúGiven that Ô¨Åles f
1and
f2and method m changed, what other Ô¨Åles and methods are
likely to need to be changed?‚Äù
A common strategy for change recommendation is to
capture the evolutionary coupling between entities [14]. In
this application, entities are considered coupled iff they havechanged together in the past. The key assumption behindevolutionary coupling is that the more frequently two or moreentities change together, the more likely it is that when onechanges, the others will also have to be changed. In the contextof software change recommendation, evolutionary coupling iscommonly captured using association rule mining [44].
B. Targeted Association Rule Mining
Association rule mining is an unsupervised learning technique
aimed at Ô¨Ånding patterns among items in a data set [1]. Asso-
ciation rule mining was Ô¨Årst applied for market basket anal-ysis, to Ô¨Ånd patterns (rules) describing items people typicallypurchase together. In this context, the data set is expressedas a list of transactions, where each transaction consists of aset of items. For example, in the domain of grocery stores,items likely include products such as ‚Äúmilk‚Äù and ‚Äúcookies‚Äù,and mining a rule such as ‚Äúcookies‚Äù ‚Üí‚Äúmilk‚Äù, uncovers the
tendency of people who buy cookies (the rule antecedent )t o
also buy milk (the rule consequent ). This provides valuable
knowledge, for example suggesting that placing these groceryitems in close proximity will increase sales.
Shortly after the introduction of association rule mining,
Srikant et al. acknowledged that for most applications only
a few speciÔ¨Åc rules are of practical value [36]. This led
to the development of constraint-based rule mining where
only rules that satisfy a given constraint are mined. Typically,constraints specify that particular items must be involved in therule‚Äôs antecedent. For example, consider a software engineerwho recently made a change to Ô¨Åle x. A constraint could
specify that rule antecedents must contain x, thus limiting
recommendation to those involving x. Constraints are usually
speciÔ¨Åed by the user in the form of a query, at which the
mining process is said to be targeted. The resulting Targeted
Association Rule Mining Algorithms Ô¨Ålter from the history all
transactions unrelated to the query, producing a more focusedset of rules. Doing so provides a signiÔ¨Åcant reduction inexecution time [36].To rank the resulting rules, numerous interestingness mea-
sures have been proposed [24, 16]. Such measure attempt to
quantify the likelihood that a rule will prove useful. The Ô¨Årst
interestingness measures introduced, frequency, support, and
conÔ¨Ådence, are also the most commonly used [1]. It is worth
formalizing these three. Each is deÔ¨Åned in terms of a history,
H, of transactions and an association rule A‚ÜíB, where A
andBare disjoint sets of items. To begin with rule frequency
is the number of times the antecedent and consequent havechanged together in the history:
DeÔ¨Ånition 1 (Frequency)
frequency( A‚ÜíB)def=|{T‚ààH :A‚à™B‚äÜT}|
Second, the support of a rule is its relative frequency with
respect to the total number of transactions in the history:
DeÔ¨Ånition 2 (Support)
support( A‚ÜíB)def=frequency( A‚ÜíB)
|H|
Finally, conÔ¨Ådence is its relative frequency of the rule with
respect to the number of historical transactions containing theantecedent A:
DeÔ¨Ånition 3 (ConÔ¨Ådence)
conÔ¨Ådence( A‚ÜíB)def=frequency( A‚ÜíB)
|{T‚ààH :A‚äÜT}|
Support and conÔ¨Ådence are often combined in the support-
conÔ¨Ådence framework [1], which Ô¨Årst discards rules below a
given support threshold and then ranks the remaining rulesbased on conÔ¨Ådence. Thresholds were originally required tominimize the number of potential rules, which can quicklygrow unwieldy. However, the constraints introduced by tar-geted association rule mining greatly reduce the number of
rules and thus do not depend on a support threshold forpractical feasibility.
C. Association Rule Mining Algorithms
A targeted association rule mining algorithm takes as input a
query Qand restricts the antecedents of the generated rules to
be various subsets of Q. The variation here comes from each
algorithm attempting to best capture relevant rules. Consider,for example, the query Q={a, b, c, d}. Potential rules include
a‚ÜíXandb, c‚ÜíY. In fact, the set of possible antecedents
is given by the power-set of Q.
One of most well known algorithms, R
OSE , limits the
number of rules by requiring that the antecedents are equalto the query, Q[44]. Thus for {a, b, c, d},R
OSE rules are all
of the form a, b, c, d‚ÜíX, where Xis only recommended
if there exists one of more transactions where X changed
together with all the items of the query. At the other end of
the spectrum, C O-C HANGE partitions Qinto singletons and
considers only rules for each respective singleton [19]. Thus
it produces rules of the form a‚Üíxandb‚Üíx.
While R OSE and C O-C HANGE makes use of the largest
and smallest possible rule antecedents, T ARMAQ identiÔ¨Åes the
largest subset of Qthat has changed with something else in
696
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:19:43 UTC from IEEE Xplore.  Restrictions apply. the past [34]. Thus T ARMAQ may return the same rules as
CO-C HANGE (when the history is made up of only two-item
transactions) or the same rules as R OSE (when Qis a proper
subset of at least one transaction). However, T ARMAQ can also
exploit partial matches (e.g., a history including transactions
larger than two items, but none having Qas proper subset).
While it may not be immediately evident, T ARMAQ is
deÔ¨Åned such that its recommendations are identical to those
of R OSE ,when ROSE is able to produce a recommendation.
On the other hand, T ARMAQ can produce recommendations
far more often than R OSE [34]. As a result, we performed
our empirical study using C O-C HANGE and T ARMAQ ,a s
the behavior of R OSE is subsumed by that of T ARMAQ .
As C O-C HANGE mines only singleton rules and T ARMAQ
potentially mines rules which maximize the antecedent with
respect to the query, they together cover a large range ofpossible recommendations.
V. O
VERVIEW OF MODEL FEA TURES
This section introduces the features that we use to buildour random forest classiÔ¨Åcation model. We consider threecategories of features:
‚Ä¢features of the query,
‚Ä¢features of the change history, and
‚Ä¢features of the recommendation.
It is worth noting that the features describing the queryare known a priori, while features of the change historyand the change recommendation are only known after arecommendation has been generated. Fortunately, a changerecommendations can be generated in mere milliseconds and
the corresponding feature set can therefore be included without
incurring undo computational expense.
A. Features of the Query
Query Size: The Ô¨Årst feature of a query we consider is simply
its size. For example, if a single method is changed the query
size is 1, if two different methods are changed, the query sizeis 2 and so on. Furthermore, some Ô¨Åles may not be able to beparsed for Ô¨Åne-grained change information, changes to theseÔ¨Åles only ever increase the query size by 1. Throughout therest of the paper we use the term artifact as a generic way
of referring to both (unparsable) Ô¨Åles and (parsable) methods.We hypothesize that query size may be important for relevance
as when it increases one of the following is likely occurring:(a) The developer is working on a complex feature, requiring
code updates in several locations. Here increased query sizeindicates speciÔ¨Åcity. (b) On the other hand, if a developer is not
compartmentalizing the work and thus is working on multiple
features at the same time, increased query size indicates chaos
as unrelated artifacts are added to the same commit.
Number of Files Changed/Number of Methods Changed:
We record the granularity of changes in two separate features:the number of Ô¨Åles changed and the number of methodschanged. For example, if the methods m1 andm2 change in
the Ô¨Åle f1, and the method m3 change in the Ô¨Åle f2, we record
that 3 methods and 2 Ô¨Åles have changed. By including thesemetrics of query granularity, we aim to capture the speciÔ¨Åcity
of the corresponding change recommendation.
Number of New Artifacts: If a new Ô¨Åle or new method
is included in a query, we know that nothing has changed
together with it previously, thus from an evolutionary perspec-
tive, the new artifact is uncoupled from all other artifacts. The
presence of new artifacts in combination with known artifactsadds uncertainty and is therefore considered as a feature.
B. Features of the Change History
Whenever an existing artifact, a, is changed, a list of relevant
transactions can be extracted from the change history. A
transaction is relevant if it contains the changed artifact. From
these transactions mining algorithms identify other artifactsthat typically changed with a, forming the basis for the
resulting change recommendation.
Number of Relevant Transactions: The number of relevant
transactions is the number of transactions with at least one
artifact from the query. This metrics provides a measure ofthe churn rate (i.e., how often the artifacts change).
Mean Size of Relevant Transactions: While the number of
relevant transactions tells us how often the artifacts found in aquery change, it does not tell us how often they change withother artifacts, the mean size of relevant transactions attempts
to capture this feature.
Mean/Median Age of Relevant Transactions: Two age
related features included involve the change in dependencies
between artifacts as software evolves (e.g., because of arefactoring) and code decay, where artifacts become ‚Äúharder
to change‚Äù (a known phenomena in long lived software sys-tems [13]). The feature ‚Äúage of relevant transactions‚Äù attemptsto capture these two age related aspects. Note that two featuresare actually used: the mean age and the median age.
Overlap of Query and Relevant Transactions: If there
are transactions that exhibit large overlap with the query, thismight indicate highly relevant transactions [44]. We capturethis through the ‚Äúoverlap percentage‚Äù. Note that the percentagereports the single largest match rather than the mean.
C. Features of the Recommendation
A recommendation boils down to a prioritized list of associ-
ation rules giving the potentially affected artifacts. However,
different mining algorithms may return different lists. While
the features described so far are independent of the mining
algorithm, in this section we consider features that are aspectsof the recommendation and thus the particular algorithm used.
ConÔ¨Ådence and Support of Association Rules: A recom-
mendation is constructed from association
 rules of the form
A‚Üí B. Here ‚ÄúA‚Äù includes changed artifacts while ‚ÄúB‚Äù the
recommended artifacts. To be able to distinguish rules, weights
can be provided through interestingness measures. One way
of providing these weights uses the support and conÔ¨Ådenceinterestingness measures (DeÔ¨Ånitions 2 and 3). Traditionally,interestingness measures are used in isolation to judge whethera recommendation is relevant [44, 40, 34]. In this paper weextend their use by considering aggregates of the top 10 rules
697
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:19:43 UTC from IEEE Xplore.  Restrictions apply. in order to indicate recommendation relevance. We include
three aggregates: The top 10 mean conÔ¨Ådence, the top 10 mean
support and the maximum conÔ¨Ådence. Each feature is meant
to capture the likelihood of whether there exist at least one
relevant artifact in the top ten.
Number of Association Rules: If a recommendation con-
sists of a large number of rules, two non-mutually exclusive
situations may exist: (a) the query is large and the contained
artifacts have changed with something else in the past, or (b)
at least one artifact of the query has changed with a largenumber of other artifacts in the past. In either case, a largerecommendation is a symptom of non-speciÔ¨Åcity and may thusprove a valuable feature for classifying true negatives.
VI. E
XPERIMENT DESIGN
Our empirical study is designed to answer one primary ques-tion: can we predict if a recommendation contains relevant ar-
tifacts? To answer this question we generate a large recommen-
dation oracle, over which we train random forest classiÔ¨Åcation
models using the features described in section V. Finally, weevaluate the resulting models by comparing their performanceagainst two conÔ¨Ådence based predictions of relevance. Theseaim to function as a baseline for whether a developer wouldact upon a given recommendation.
1)Maximum ConÔ¨Ådence: a recommendation is predicted
as relevant if the conÔ¨Ådence of the artifact with the
highest conÔ¨Ådence is larger than a given threshold.
2)Mean ConÔ¨Ådence 10: a recommendation is predicted as
relevant if the mean conÔ¨Ådence of the top ten artifacts isgreater than a given threshold.
The rationale behind maximum conÔ¨Ådence mimics a developer
who is only willing to consider a recommendation‚Äôs highestranked artifact, while that of mean conÔ¨Ådence 10 mimics a
developer who is willing to consider the recommendation‚Äôstop ten artifacts.
Our study encompasses change recommendations generated
from the change history of sixteen different software systemswith varying characteristics. Two of these systems come fromour industry partners, Cisco and KM .Cisco is a worldwide
leader in the production of networking equipment, We analyzethe software product line for professional video conferencingsystems developed by Cisco. KM is a leader in the produc-
tion of systems for positioning, surveying, navigation, andautomation of merchant vessels and offshore installations. Weanalyze the common software platform KM uses across various
systems in the maritime and energy domain.
The other fourteen systems are the well known open-source
projects reported in Table I along with change history statistics
illustrating their diversity. The table shows that the systems
vary from medium to large size, with over 280 000 unique
Ô¨Åles in the largest system. For each system, we extractedup to the 50 000 most recent transactions. This number of
transactions covers vastly different time spans across the
systems, ranging from almost twenty years in the case ofHTTPD, to a little over ten months in the case of the Linuxkernel. In the table, we report the number of unique Ô¨Åleschanged throughout the 50 000 most recent transactions, as
well as the the number of unique artifacts changed. These
artifacts include, in addition to Ô¨Åle-level changes, method-level changes for certain languages.
1Finally, the last column
of Table I shows the programming languages used in eachsystem, as an indication of heterogeneity.
The remainder of this section Ô¨Årst explains the setup used
to generate change recommendations using C
O-C HANGE and
TARMAQ . It then details how these recommendations are used
to train and evaluate models for relevance prediction. Theresults of our study are presented in section VII.
A. Generating Change Recommendations
Establishing the Ground Truth: The change history of
a software system exactly describes, transaction by transac-
tion, how to (re)construct the current state of the system.Consequently, we can assume the majority of the time thateach transaction has some intention behind it, and that thechanges in the transaction have some meaningful relation toeach other. In fact, if this assumption is completely misguided,recommendations based on change histories would degenerate
to random prediction, which is clearly not the case.
2Thus,
given a subset Qof transaction C, a good recommendation
algorithm should identify the complement E=C/Q as the
set of impacted items. Here Ecaptures the ground truth on
whether a change recommendation is truly relevant because
it includes those artifacts that actually changed alongsideQ. For this reason, Eis used when evaluating the change
recommendation generated for query Q.
Sampling Strategy: Construction of the change scenarios
involves two steps:
‚Ä¢sampling transactions and
‚Ä¢generating queries from each sampled transaction.
We start by fetching the 50 000 most recent transactions from
each subject system. From these, we then determine the 10 000
most recent transactions whose size is between 2 and 300.The minimum number, 2, ensures that there is always apotential impact set for any given query, while the maximumnumber, 300, covers at least 99% of all transactions whileaiming to omit large changes such as licensing changes. Fromeach sampled transaction C, the impact set Eis randomly
determined, but ensured to consist of at least ten items.
Ranking Rules: The rules mined by each algorithm all
share the property that their support is at least one because
we operate with an absolute minimal support constraint. Byincluding ‚Äúall rules‚Äù like this, we ensure that both highfrequency as well as low frequency (rare) rules are includedin the recommendations [39].
The support measure is not otherwise used for ranking.
When support is used for ranking, special care needs tobe taken as rare (infrequent) rules always rank lower thenmore frequent rules. This is a result of the downward closure
1We currently extract method-level change information from Ô¨Åles of C,
C++, C#, and Java code.
2The chance of randomly predicting a correct method is
1/(number of methods), which for any sizable system approaches zero.
698
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:19:43 UTC from IEEE Xplore.  Restrictions apply. T ABLE I
CHARACTERISTICS OF THE EV ALUA TED SOFTW ARE SYSTEMS (BASED ON OUR EXTRACTION OF THE LAST 50 000 TRANSACTIONS FOR EACH ).
Software System History Number of Number of Avg. transaction
(in yrs) unique Ô¨Åles unique artifacts size (artifacts) Languages used‚àó
CPython 12.05 7725 30090 4.52 Python (53%), C (36%), 16 other (11%)
Mozilla Gecko 1.08 86650 231850 12.28 C++ (37%), C (17%), JavaScript (21%), 34 other (25%)
Git 11.02 3753 17716 3.13 C (45%), shell script (35%), Perl (9%), 14 other (11%)
Apache Hadoop 6.91 24607 272902 47.79 Java (65%), XML (31%), 10 other (4%)
HTTPD 19.78 10019 29216 6.99 XML (56%), C (32%), Forth (8%), 19 other (4%)
Liferay Portal 0.87 144792 767955 29.9 Java (71%), XML (23%), 12 other (4%)
Linux Kernel 0.77 26412 161022 5.5 C (94%), 16 other (6%)
MySQL 10.68 42589 136925 10.66 C++ (57%), C (18%), JavaScript (16%), 24 other (9%)
PHP 10.82 21295 53510 6.74 C (59%), PHP (13%), XML (8%), 24 other (20%)
Ruby on Rails 11.42 10631 10631 2.56 Ruby (98%), 6 other (2%)
RavenDB 8.59 29245 47403 8.27 C# (52%), JavaScript (27%), XML (16%), 12 other (5%)
Subversion 14.03 6559 46136 6.36 C (61%), Python (19%), C++ (7%), 15 other (13%)
WebKit 3.33 281898 397850 18.12 HTML (29%), JavaScript (30%), C++ (26%), 23 other (15%)
Wine 6.6 8234 126177 6.68 C (97%), 16 other (3%)
Cisco 2.43 64974 251321 13.62 C++, C, C#, Python, Java, XML, other build/conÔ¨Åg
KM 15.97 35111 35111 5.08 C++, C, XML, other build/conÔ¨Åg
‚àólanguages used by open source systems are from http://www.openhub.net, percentages for the industrial systems are not disclosed.
property: any subset of a rule must have equal or larger support
relative to the origin rule [2]. For example, given the two rulesr
1={a}‚Üí{ x} andr2={a, b}‚Üí{ x},r2cannot have
higher support than r1.
By using the conÔ¨Ådence measure to rank rules, both rare
and non-rare rules may be ranked highly. Still, the frequencyof a pattern continues to inform the relevancy of a rule. Tothis end, recall that the top 10 mean support is included as a
feature in our prediction model.
B. Evaluation of Relevance Prediction
Blocked Cross-Validation: Last block validation is a fre-
quently used scheme for evaluating prediction models. Here
the data is split into two blocks, with the Ô¨Årst block being
used to train the model and the second block being used to
evaluate it. This setup has the advantage of respecting thetemporal nature of the data. However, a drawback is thatnot all data is used for both training and prediction [4].To address this a traditional cross-validation setup may beused. However, doing so violates the temporal nature of
time series data, and, in the worst case, may invalidate theresults [4]. Because in time series data future data naturally
depends on prior data, we use blocked cross-validation to
$% & ' ( ) * + , $#		
			

		 	-
' $&$% &
$#%

$,	
$
 		
Fig. 1. The blocked cross-validation scheme used in our study. Notice that
all blocks except B1 and B10 are used for both training and predictionpreserve the temporal order between training and evaluation.
In our conÔ¨Åguration we partition each set of transactions intoten equally sized blocks, preserving temporal order between
blocks. We then train nine random forest classiÔ¨Åcation models
for each software-system and mining algorithm combination.As illustrated in Figure 1, each random forest is trained on anincrementally larger subset of the available recommendations.In total, 16systems‚àó2algorithms ‚àó9forests = 288 random
forests are trained.
Measuring Relevance: The random forest and conÔ¨Ådence
based classiÔ¨Åcation models have probabilistic interpretations.
The conÔ¨Ådence interestingness measure itself is given by the
conditional likelihood P(B|A), where Bis the recommended
artifact and Ais one or more artifacts that changed (i.e., arti-
facts from the query) [1]. The maximum and mean conÔ¨Ådencemodels use this information to capture the likelihood thata developer will act upon a given change recommendation.Here a ‚Äú0‚Äù indicates very unlikely while a ‚Äú1‚Äù indicatesvery likely. In the case of random forests, the likelihood is
the result of the votes obtained from the collection of treesmaking up the random forest [7]. Each decision tree casts a
single vote. As each tree is built from a random selection ofchange recommendations, the end likelihood is an indicationof internal consistency within the data set for a particularscenario. In other words, if a certain scenario always results in
a certain outcome, it is very likely that similar new scenarios
will have the same outcome. Finally, in the evaluation sets
used throughout our study we encode the possible classes ina similar way, the binary options are either 0 (not correct intop 10) or 1 (correct in top 10).
VII. R
ESUL TS AND DISCUSSION
We organize the discussion of our results around the tworesearch questions proposed in section V. Throughout thissection we will consider prediction performance for each
individual software system, and for each of the mining al-gorithms: C
O-C HANGE and T ARMAQ . By doing so we get an
699
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:19:43 UTC from IEEE Xplore.  Restrictions apply. Co-Change TARMAQ
B2 B3 B4 B5 B6 B7 B8 B9 B100.30.40.5
0.30.40.5
Evaluation BlockMAEMean Conf. 10 Max. Conf. Random Forest
Fig. 2. Descriptive view of errors using the Mean Absolute Error (MAE).
Each line shows the mean MAE over all 16 systems, and the ribbon shows
the standard deviation.
indication of how generalizable the results are to other systems
and other algorithms. In the following we brieÔ¨Çy introduceeach performance metric before presenting the correspondingresults.
A. RQ 1: Comparison to ConÔ¨Ådence as a Relevance Predictor
While comparing the three classiÔ¨Åcation models, we focus on
two aspects of their relevance predictions:
1) the error with respect to the actual relevance, and
2) the performance across different classiÔ¨Åcation thresholds.
We start with a descriptive view of the errors exhibited by
each classiÔ¨Åcation model, for this we use the Mean AbsoluteError (MAE). In our case, MAE measures the mean absolutedifference between the actual and predicted value for whether
there is a correct artifact in the top ten artifacts of a recom-mendation. For example, given a certain feature as input, therandom forest might give the output 0.67, indicating a 67%
likelihood that the resulting recommendation will be relevant.If in actuality, we know that for this scenario there is indeed
a correct artifact in the top ten the prediction error would be
1‚àí0.67 = 0. 33because we encode ‚Äúknowing‚Äù as ‚Äú1‚Äù. Note
that lower is better. Figure 2 shows the MAE across the 16
software systems for the two mining algorithms. To reduceclutter, we present summarized information where each line isthe mean over all systems and the ribbon indicates the standarddeviation. For this Ô¨Årst look at the data we have also preserved
Co-Change TARMAQ
B2 B3 B4 B5 B6 B7 B8 B9 B100.20.30.4
0.20.30.4
Evaluation BlockBrierMean Conf. 10 Max. Conf. Random Forest
Fig. 3. Accuracy of classiÔ¨Åcation models using Brier scores. Each line shows
the mean MAE over all 16 systems, and the ribbon shows the standard
deviation. Models below the horizontal black line tend to classify correctly
with regards to a 0.5 classiÔ¨Åcation threshold.
results for each evaluation block. This enables a view into error
Ô¨Çuctuations across time. First, there is no apparent overall
trend across the evaluation blocks. This is good news asit provides evidence that the analysis is stable across time.This is also supported by Ô¨Åtting linear regression lines (leftout to minimize clutter). The random forest model shows
less variance in error across systems and algorithms, whilefor some systems the maximum conÔ¨Ådence model exhibits
less overall error. For the change recommendations where the
actual relevance was 1 (Correct in Top 10), the maximum
conÔ¨Ådence model frequently matches the prediction exactly
and therefore minimizes the error for these recommendations.
In terms of accuracy of each classiÔ¨Åcation model a proper
scoring rule must be used [8]. For proper scoring rules,
the maximum score is achieved if the prediction distribution
exactly matches the actual distribution. One such scoring rule
is the brier score. In the case of binary classiÔ¨Åcation, which
is our task, the brier score is the mean squared error :
BS=1
NN/summationdisplay
i=1(pi‚àíai)2
Where piis the predicted relevance for scenario i, and ai
is the actual relevance (1 or 0). Figure 3 presents the Brier
scores for each classiÔ¨Åcation model across each evaluationblock, software system, and mining algorithm. Note that lower
700
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:19:43 UTC from IEEE Xplore.  Restrictions apply. Co-Change TARMAQ
0.00 0.25 0.50 0.75 1.000.000.250.500.751.00
0.000.250.500.751.00
False Positive RateTrue Positive RateMean Conf. 10 Max. Conf. Random Forest
Fig. 4. ROC curves for the prediction models trained for each software system
and algorithm.
is better. In the Ô¨Ågure, the horizontal black line at y = 0.25
indicates the brier score of a neutral classiÔ¨Åcation model.
A neutral model always makes the prediction 0.5, whererelevance and non-relevance are equally likely. Brier scores
below the line indicate a prediction model that tends to
be on the ‚Äúright side of the midpoint‚Äù. We clearly see therandom forest model being consistently more accurate acrossalgorithms and software systems.
While the error informs us about the overall Ô¨Åt of a predic-
tion model, it does not capture performance across differentclassiÔ¨Åcation thresholds. When classiÔ¨Åcation models are used
in practice, thresholds must be set to balance true positivesagainst false positives and false negatives. To investigatethe ability of our three prediction models in these terms,we consider the ROC curve and the Precision/Recall curve.First, the ROC curve in Figure 4 plots the True PositiveRate (TPR =
TP
TP+FN) against the False Positive Rate
(FPR =FP
FP+TN) at classiÔ¨Åcation thresholds from 0 to 1. It
is immediately apparent that the conÔ¨Ådence based models do
not meaningfully respond to different classiÔ¨Åcation thresholds,
as data points are not evenly spread across the x-axis. Further-more, there are strong linear relationships between their TPRsand FPRs. This was also reÔ¨Çected in corresponding Pearsoncorrelation coefÔ¨Åcients, where all coefÔ¨Åcients were calculatedto be 0.97 or higher. Intuitively, the effect we see is that as
the classiÔ¨Åcation threshold is lowered, the conÔ¨Ådence models
Co-Change TARMAQ
0.00 0.25 0.50 0.75 1.000.500.751.00
0.500.751.00
RecallPrecisionMean Conf. 10 Max. Conf. Random Forest
Fig. 5. Precision/Recall curves for the prediction models trained for each
software system and algorithm.
for recommendation relevance classify comparably increased
amounts of recommendations both correctly and incorrectly.
For example, both TPR and FPR increase similarly. Further-more, observe that the range and domain of the TPR and FPR
for the conÔ¨Ådence models do not fully extend between 0 and1. This is the result of a high percentage of change scenariosbeing given the relevance ‚Äú1‚Äù, and these scenarios being evenlysplit between True Positives (TPs) and False Positives (FPs). In
other words, the lack of even distribution of data-points results
in less variation in TPR and FPR, which again is reÔ¨Çectedin the range and domain. To further support our Ô¨Åndings wecompared the partial Area Under the Curve (pAUC) betweenthe ROC of the random forests and each of the conÔ¨Ådencebased models. We used roc.test from the R package pROC,
the signiÔ¨Åcance test is based on bootstrapped sampling of
multiple AUCs and computing their difference. Across allsoftware systems and for both C
O-C HANGE and T ARMAQ
the pAUC were signiÔ¨Åcantly larger (p<0. 05) for the random
forest model. Thus, the random forest classiÔ¨Åer consistently
provide better estimates of relevance across various thresholdscompared to the purely conÔ¨Ådence based methods explored.
As laid out earlier, relevance prediction can be performed
in a background thread that only notiÔ¨Åes the developer when
there is a high likelihood for a true positive recommendation.In this application, the positive class (correct in top ten) istherefore of the most interest. NotiÔ¨Åcations that there are
701
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:19:43 UTC from IEEE Xplore.  Restrictions apply. no relevant artifacts would be of less use. With this view,
the precision (TP
TP+FP) of the classiÔ¨Åcation models become
imperative. The task is to Ô¨Ånd an appropriate classiÔ¨Åcation
threshold that makes true positives likely (high precision),while still maintaining practicality in that recommendationscan be regularly made (high recall). Figure 5 shows theprecision/recall curves for our three prediction models for eachsoftware system and mining algorithm. First, the abnormalityin slope, range and domain for the conÔ¨Ådence models canagain be attributed to the weak connection to threshold-changes. Furthermore, while one usually expects a decreasein precision as recall increases, this does not necessarily need
to be the case. The trends for the conÔ¨Ådence models inFigure 5 are the result of having slightly higher concentra-
tions of positive classiÔ¨Åcations than negative classiÔ¨Åcationson lower thresholds. As thresholds are lowered further, morerecommendations become TPs, and the ratio between TPs andFPs actually increases. An implication of this is that at leastfor the conÔ¨Ådence measure, its value cannot be used directlyas an indication of relevancy.
Turning to the random forest models, these exhibit greater
deÔ¨Åned behavior, where negative classiÔ¨Åcations are primarilylocated in lower likelihood thresholds, thus precision decreasesas recall increases. In terms of recommending concrete clas-siÔ¨Åcation thresholds for our random forest model we suggestthat this should be adjusted with respect to the system domain
knowledge of the developer for which the recommendation is
made. In Table II we have provided the mean precision and
recall across software systems for the example thresholds at0.5 and 0.9. As developers become more acquainted with a
system, they should also be able to better differentiate rele-vant and non-relevant recommendations. As such, experienceddevelopers might afford a higher rate of recall at the cost oflower precision. A classiÔ¨Åcation threshold of 0.50 becomes
reasonable for this group, assuming T
ARMAQ is used. For in-
experienced developers one wants to minimize confusion, and
therefore maximize the precision of change recommendations.Thus, a threshold such as 0.9 might be appropriate for thesedevelopers, resulting in change recommendations only havingfalse positives in about 1 to 3 per 100 recommendations.
B. RQ 2: Analysis of Features
Having empirically established that the random forest classi-
Ô¨Åcations models are superior at predicting change recommen-
dation relevance, we next consider which features bring the
T ABLE II
EXAMPLES OF PRECISION AND RECALL FOR THE RANDOM FOREST
CLASSIFICA TION MODEL .T HE ST ANDARD DEVIA TION (SD) CAPTURES
FLUCTUA TIONS BETWEEN SOFTW ARE SYSTEMS .
ClassiÔ¨Åcation Threshold 0.5 0.9
Algorithm Mean SD Mean SD
CO-C HANGE Precision 0.775 ¬± 0.075 0.971 ¬± 0.020
Recall 0.684 ¬± 0.046 0.100 ¬± 0.074
TARMAQ Precision 0.868 ¬± 0.062 0.993 ¬± 0.008
Recall 0.735 ¬± 0.032 0.277 ¬± 0.102
Co-Change TARMAQ
0 50 100 150 200Number of new artifactsNumber of methods changedNumber of Ô¨Åles changedQuery sizeNumber of rulesTop 10 mean supportTop 10 mean con Ô¨ÅdenceMaximum con Ô¨ÅdenceMean size of relevant txesMean age of relevant txesMed. age of relevant txesNumber of relevant txesOverlap percent
Number of new artifactsNumber of methods changedNumber of Ô¨Åles changedQuery sizeNumber of rulesTop 10 mean supportTop 10 mean con Ô¨ÅdenceMaximum con Ô¨ÅdenceMean size of relevant txesMean age of relevant txesMed. age of relevant txesNumber of relevant txesOverlap percent
Mean decrease in accuracySystem with Ô¨Åne-grained change history: FALSE TRUE
Fig. 6. Feature importance as determined by mean decrease in accuracy. Each
line represents a separate software system, the line color & style indicates if
a Ô¨Åne-grained change history was available.
most value to the models. Breiman introduced the concept of
variable importance for random forests [7]. Once the decision
trees of the random forests have been built, the process canself-assess the importance of each feature. The basic idea isto observe if randomly permuting a feature changes prediction
performance [7]. Averaging the accuracy changes over all treesgives the mean decrease in accuracy (when permuted) for each
feature.
The corresponding plot for the features included in our
model is provided in Figure 6. For two of the studied software
systems (KM and Rails), we only have Ô¨Åle-level changeinformation. These two systems are shown using the dashed(red) lines. Naturally, permuting the ‚ÄúNumber of methods
changed ‚Äù feature does not change accuracy for these two
systems, as the value is always 0, as reÔ¨Çected in Figure 6.
To begin with T
ARMAQ was constructed to produce a
focused recommendation that matches the query as closely aspossible [34]. This is evident in the Ô¨Ågure where the ‚ÄúNumber
of rules generated ‚Äù being an essential feature for T
ARMAQ .
Thus for T ARMAQ , variation in the number of rules generated
meaningfully correlates with recommendation relevance; if alarge subset of the query has changed with something else in
the past, this results in fewer possible rule antecedents andthus fewer rules, which evidently increases the likelihood of
702
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:19:43 UTC from IEEE Xplore.  Restrictions apply. a relevant recommendation. For C O-C HANGE this feature has
less importance. For the remaining features, Figure 6 shows
a rather clear picture; the query based features (the bottomfour) are the least important, the attributes they representare better captured by other features. Of the interestingnessmeasure based features the ‚ÄúTop 10 mean conÔ¨Ådence‚Äù proved
most useful. Finally, all history related features are comparably
important.
The high degree of co-variance between software systems
suggests that model transfer to other systems is viable. That is,
classiÔ¨Åcation models learned on one or more systems, should
be reusable for a new (unknown) system. If this can be shownto work reliably, systems that are early in their evolution can
still beneÔ¨Åt from models generated from more mature systems.
However, care must be taken to adapt the feature variation ofthe random forest to Ô¨Åt the variation found in new softwaresystem.
C. Threats to V alidity
Implementation: We implemented and thoroughly tested all
algorithms and our technique for model classiÔ¨Åcation in Ruby
and R respectively, However, we can not guarantee the absence
of implementation errors.
Variation in Software Systems: We have sought to obtain
generalizable results by evaluating over a range of heteroge-
neous software systems, however, we also uncovered that rele-
vance prediction performance varies between systems. Future
work should investigate the effects of system characteristicson prediction performance.
Mining Algorithms Studied: In our evaluation we have
studied classiÔ¨Åcation models for recommendations generated
using both the C
O-C HANGE and T ARMAQ mining algorithms.
For both algorithms we achieve strong results. However, weacknowledge that comparable results cannot be guaranteed forother mining algorithms.
Using Other Interestingness Measures: In our study we
focused on the conÔ¨Ådence interestingness measure, thus our
results are limited to this measure. As such, future work
should investigate the use of other interestingness measures,both for comparison to the random forest predictor, as wellas being included as part of the model. We also envision that
a variation of the relevance prediction presented here mightbe an interestingness measure recommender, thus essentially
creating an ensemble of measures where the most relevant is
used at a given time.
Recommendations Used for Training and Evaluation: We
train and evaluate our classiÔ¨Åcation model over a constructed
set of change recommendations. Each recommendation is the
result of executing randomly sampled a query from an existing
transaction where the complement of the query and the sourcetransaction is used as the expected outcome. However, this
approach does not account for the actual order in whichchanges were made before they were committed to the ver-sioning system. As a result, it is possible that queries containelements that were actually changed later in time than elementsof the expected outcome. As such, we cannot guaranteethat recommendations used in training and evaluation exactlyreÔ¨Çect each system‚Äôs evolution.
VIII. C
ONCLUDING REMARKS
Change recommendation is clearly an asset to a software de-veloper maintaining a complex system. However, its practical
adoption faces two challenges: (a) recommendations must be
both accurate and relevant. We believe that both challengescan be effectively addressed using historically proven change
recommendations.
This paper shows that random forest classiÔ¨Åcation using the
12 features that describe aspects of the change set (query),
change history (transactions) and generated change recommen-
dations is viable. We compare the random forest model against
the state-of-the-art (based on conÔ¨Ådence). We evaluate our
approach in a large empirical study across 16 software systemsand two change recommendation algorithms. Our Ô¨Åndings areas follows:
Finding 1: The random forest classiÔ¨Åcation model consis-
tently outperforms the conÔ¨Ådence based models in terms of
accuracy (Brier scores).
Finding 2: The random forest classiÔ¨Åcation model achieves
signiÔ¨Åcantly larger area under ROC curve than both conÔ¨Ådence
based models.
Finding 3: While the conÔ¨Ådence measure is appropriate
for ranking of artifacts, the values themselves should not be
interpreted in isolation as overall estimates of recommendationrelevance.
Finding 4: The importance of model features may varies
between algorithms. For example, the relevance of T
ARMAQ
recommendations is best predicted by considering the number
of rules generated, while this feature is less important for C O-
CHANGE . However, the remaining features studies showed
consistent importance between algorithms.
Directions for Future Work
Looking forward, it would be of interest to study the effects of
using stricter and looser deÔ¨Ånitions of relevance (e.g., relevantif correct in top three vs top twenty). Furthermore, rather thanclassiÔ¨Åcation, relevance can also be studied as a regressionproblem, predicting other recommendation metrics such asprecision, recall, average precision etc. In addition, we planto study the behavior of relevance classiÔ¨Åcation models overother interestingness measures, and investigate the viabilityof model transfer between software systems. Finally, we planto look into improving the classiÔ¨Åcation model by includingfeatures such as the number of relevant transactions authoredby core contributors vs occasional contributors, the weightingrecent relevant transactions higher than older transactions, andthe text similarity scores between change set and relevanttransactions.
A
CKNOWLEDGMENT
This work is supported by the Research Council of Norwaythrough the EvolveIT project (#221751/F20) and the CertusSFI (#203461/030). Dr. Binkley is supported by NSF grantIIA-1360707 and a J. William Fulbright award.
703
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:19:43 UTC from IEEE Xplore.  Restrictions apply. REFERENCES
[1] R. Agrawal, T. Imielinski, and A. Swami. ‚ÄúMining association
rules between sets of items in large databases‚Äù. In: ACM
SIGMOD International Conference on Management of Data.
ACM, 1993, pp. 207‚Äì216.
[2] R. Agrawal and R. Srikant. ‚ÄúFast Algorithms for Mining
Association Rules‚Äù. In: International Conference on V ery
Large Data Bases (VLDB). 1994, pp. 487‚Äì499.
[3] E. Baralis et al. ‚ÄúGeneralized association rule mining with
constraints‚Äù. In: Information Sciences 194 (2012), pp. 68‚Äì84.
[4] C. Bergmeir and J. M. Ben√≠tez. ‚ÄúOn the use of cross-validation
for time series predictor evaluation‚Äù. In: Information Sciences
191 (2012), pp. 192‚Äì213.
[5] D. Beyer and A. Noack. ‚ÄúClustering Software Artifacts Based
on Frequent Common Changes‚Äù. In: International Workshop
on Program Comprehension (IWPC). IEEE, 2005, pp. 259‚Äì268.
[6] S. Bohner and R. Arnold. Software Change Impact Analysis.
CA, USA: IEEE, 1996.
[7] L. Breiman. ‚ÄúRandom Forests‚Äù. In: Machine Learning 45.1
(2001), pp. 5‚Äì32.
[8] A. Buja, W . Stuetzle, and Y . Shen. ‚ÄúLoss functions for binary
class probability estimation and classiÔ¨Åcation: structure andapplication‚Äù. 2005.
[9] R. Caruana and A. Niculescu-Mizil. ‚ÄúAn empirical compari-
son of supervised learning algorithms‚Äù. In: Proceedings of the
23th International Conference on Machine Learning (2006),
pp. 161‚Äì168.
[10] W . Cheetham. ‚ÄúCase-Based Reasoning with ConÔ¨Ådence‚Äù. In:
European Workshop on Advances in Case-Based Reasoning(EWCBR). Lecture Notes in Computer Science, vol 1898.
Springer, 2000, pp. 15‚Äì25.
[11] W . Cheetham and J. Price. ‚ÄúMeasures of Solution Accuracy in
Case-Based Reasoning Systems‚Äù. In: European Conference on
Case-Based Reasoning (ECCBR). Lecture Notes in Computer
Science, vol 3155. Springer, 2004, pp. 106‚Äì118.
[12] D. Cubranic et al. ‚ÄúHipikat: a project memory for software de-
velopment‚Äù. In: IEEE Transactions on Software Engineering
31.6 (2005), pp. 446‚Äì465.
[13] S. Eick et al. ‚ÄúDoes code decay? Assessing the evidence from
change management data‚Äù. In: IEEE Transactions on Software
Engineering 27.1 (2001), pp. 1‚Äì12.
[14] H. Gall, K. Hajek, and M. Jazayeri. ‚ÄúDetection of logical
coupling based on product release history‚Äù. In: IEEE Interna-
tional Conference on Software Maintenance (ICSM). IEEE,
1998, pp. 190‚Äì198.
[15] H. Gall, M. Jazayeri, and J. Krajewski. ‚ÄúCVS release history
data for detecting logical couplings‚Äù. In: International Work-
shop on Principles of Software Evolution (IWPSE). IEEE,
2003, pp. 13‚Äì23.
[16] L. Geng and H. J. Hamilton. ‚ÄúInterestingness measures for
data mining‚Äù. In: ACM Computing Surveys 38.3 (2006).
[17] A. E. Hassan and R. Holt. ‚ÄúPredicting change propagation
in software systems‚Äù. In: IEEE International Conference on
Software Maintenance (ICSM). IEEE, 2004, pp. 284‚Äì293.
[18] N. Jiang and L. Gruenwald. ‚ÄúResearch issues in data stream
association rule mining‚Äù. In: ACM SIGMOD Record 35.1
(2006), pp. 14‚Äì19.
[19] H. Kagdi et al. ‚ÄúBlending conceptual and evolutionary cou-
plings to support change impact analysis in source code‚Äù. In:
Working Conference on Reverse Engineering (WCRE). 2010,
pp. 119‚Äì128.[20] S. Kannan and R. Bhaskaran. ‚ÄúAssociation Rule Pruning
based on Interestingness Measures with Clustering‚Äù. In: Jour-
nal of Computer Science 6.1 (2009), pp. 35‚Äì43.
[21] T.-d. B. Le and D. Lo. ‚ÄúBeyond support and conÔ¨Ådence: Ex-
ploring interestingness measures for rule-based speciÔ¨Åcationmining‚Äù. In: International Conference on Software Analysis,
Evolution, and Reengineering (SANER). IEEE, 2015, pp. 331‚Äì
340.
[22]
T.-D. B. Le, D. Lo, and F. Thung. ‚ÄúShould I follow this fault
localization tool‚Äôs output?‚Äù In: Empirical Software Engineer-
ing 20.5 (2015), pp. 1237‚Äì1274.
[23] T.-D. B. Le, F. Thung, and D. Lo. ‚ÄúPredicting Effectiveness of
IR-Based Bug Localization Techniques‚Äù. In: 2014 IEEE 25th
International Symposium on Software Reliability Engineering.IEEE, 2014, pp. 335‚Äì345.
[24] P . Lenca et al. ‚ÄúOn selecting interestingness measures for as-
sociation rules: User oriented description and multiple criteriadecision aid‚Äù. In: European Journal of Operational Research
184.2 (2008), pp. 610‚Äì626.
[25] W . Lin, S. A. Alvarez, and C. Ruiz. ‚ÄúEfÔ¨Åcient Adaptive-
Support Association Rule Mining for Recommender Sys-
tems‚Äù. In: Data Mining and Knowledge Discovery 6.1 (2002),
pp. 83‚Äì105.
[26] B. Liu, W . Hsu, and Y . Ma. ‚ÄúPruning and summarizing the dis-
covered associations‚Äù. In: SIGKDD International Conference
on Knowledge Discovery and Data Mining (KDD). ACM,
1999, pp. 125‚Äì134.
[27] O. Maimon and L. Rokach. Data Mining and Knowledge
Discovery Handbook. Ed. by O. Maimon and L. Rokach.Springer, 2010, p. 1383.
[28] K. McGarry. ‚ÄúA survey of interestingness measures for knowl-
edge discovery‚Äù. In: The Knowledge Engineering Review
20.01 (2005), p. 39.
[29] L. Moonen et al. ‚ÄúExploring the Effects of History Length
and Age on Mining Software Change Impact‚Äù. In: IEEE
International Working Conference on Source Code Analysisand Manipulation (SCAM). 2016, pp. 207‚Äì216.
[30] L. Moonen et al. ‚ÄúPractical Guidelines for Change Recom-
mendation using Association Rule Mining‚Äù. In: International
Conference on Automated Software Engineering (ASE). Sin-gapore: IEEE, 2016.
[31] C. Parnin and A. Orso. ‚ÄúAre automated debugging techniques
actually helping programmers?‚Äù In: International Symposium
on Software Testing and Analysis (ISSTA). ACM, 2011, p. 199.
[32] P . Resnick and H. R. V arian. ‚ÄúRecommender systems‚Äù. In:
Communications of the ACM 40.3 (1997), pp. 56‚Äì58.
[33] R. Robbes, D. Pollet, and M. Lanza. ‚ÄúLogical Coupling
Based on Fine-Grained Change Information‚Äù. In: Working
Conference on Reverse Engineering (WCRE). IEEE, 2008,
pp. 42‚Äì46.
[34] T. Rolfsnes et al. ‚ÄúGeneralizing the Analysis of Evolutionary
Coupling for Software Change Impact Analysis‚Äù. In: Inter-
national Conference on Software Analysis, Evolution, and
Reengineering (SANER). IEEE, 2016, pp. 201‚Äì212.
[35] T. Rolfsnes et al. ‚ÄúImproving change recommendation using
aggregated association rules‚Äù. In: International Conference on
Mining Software Repositories (MSR). ACM, 2016, pp. 73‚Äì84.
[36] R. Srikant, Q. Vu, and R. Agrawal. ‚ÄúMining Association
Rules with Item Constraints‚Äù. In: International Conference on
Knowledge Discovery and Data Mining (KDD). AASI, 1997,pp. 67‚Äì73.
704
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:19:43 UTC from IEEE Xplore.  Restrictions apply. [37] P .-N. Tan, V . Kumar, and J. Srivastava. ‚ÄúSelecting the right
interestingness measure for association patterns‚Äù. In: Interna-
tional Conference on Knowledge Discovery and Data Mining
(KDD). ACM, 2002, p. 32.
[38] H. Toivonen et al. ‚ÄúPruning and Grouping Discovered Asso-
ciation Rules‚Äù. In: Workshop on Statistics, Machine Learning,
and Knowledge Discovery in Databases. Heraklion, Crete,Greece, 1995, pp. 47‚Äì52.
[39] M.-C. Tseng and W .-Y . Lin. ‚ÄúMining Generalized Association
Rules with Multiple Minimum Supports‚Äù. In: Lecture Notes
in Computer Science (LNCS). V ol. 2114. 2001, pp. 11‚Äì20.
[40] A. T. T. Ying et al. ‚ÄúPredicting source code changes by
mining change history‚Äù. In: IEEE Transactions on Software
Engineering 30.9 (2004), pp. 574‚Äì586.[41] M. J. Zaki. ‚ÄúGenerating non-redundant association rules‚Äù. In:
SIGKDD International Conference on Knowledge Discoveryand Data Mining (KDD). ACM, 2000, pp. 34‚Äì43.
[42] M. B. Zanjani, G. Swartzendruber, and H. Kagdi. ‚ÄúImpact
analysis of change requests on source code based on in-teraction and commit histories‚Äù. In: International Working
Conference on Mining Software Repositories (MSR). 2014,pp. 162‚Äì171.
[43] Z. Zheng, R. Kohavi, and L. Mason. ‚ÄúReal world perfor-
mance of association rule algorithms‚Äù. In: SIGKDD Interna-
tional Conference on Knowledge Discovery and Data Mining(KDD). ACM, 2001, pp. 401‚Äì406.
[44] T. Zimmermann et al. ‚ÄúMining version histories to guide
software changes‚Äù. In: IEEE Transactions on Software En-
gineering 31.6 (2005), pp. 429‚Äì445.
705
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:19:43 UTC from IEEE Xplore.  Restrictions apply. 