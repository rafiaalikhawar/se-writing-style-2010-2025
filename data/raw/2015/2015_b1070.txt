Attributed Variability Models: Outside the Comfort Zone
Norbert Siegmund
Bauhaus-University Weimar,
GermanyStefan Sobernig
WU Vienna, AustriaSven Apel
University of Passau, Germany
ABSTRACT
Variabilitymodelsareoftenenrichedwithattributes,suchasper-
formance,thatencodetheinfluenceoffeaturesontherespective
attribute. In spite of their importance, there are only few attrib-
utedvariabilitymodelsavailablethathaveattributevaluesobtained
fromempirical,real-worldobservationsandthatcoverinteractions
between features. But, what does it mean for research and prac-tice when staying in the comfort zone of developing algorithms
and tools in a setting where artificial attribute values are used and
where interactions are neglected? This is the central question thatwe want to answer here. To leave the comfort zone, we use a com-
bination of kernel density estimation and a genetic algorithm to
rescaleagiven(real-world)attribute-valueprofiletoagivenvari-
ability model. To demonstrate the influence and relevance of realis-
ticattributevaluesandinteractions,wepresentareplicationofa
widelyrecognized,third-partystudy,intowhichweintroducere-
alisticattributevaluesandinteractions.Wefoundstatisticallysig-
nificant differences between the original study and the replication.
We infer lessons learned to conduct experiments that involve at-
tributedvariabilitymodels.Wealsoprovidetheaccompanyingtool
Thor for generating attribute values including interactions. Our
solution is shown to be agnostic about the given input distribution
and to scale to large variability models.
CCSCONCEPTS
•Software and its engineering →Search-based software en-
gineering ;
KEYWORDS
Variability modelling, attributed variability models, Thor
ACM Reference format:
NorbertSiegmund,StefanSobernig,andSvenApel.2017.AttributedVari-
ability Models: Outside the Comfort Zone. In Proceedings of 2017 11th Joint
Meeting of the European Software Engineering Conference and the ACM SIG-
SOFT Symposium on the Foundations of Software Engineering, Paderborn,
Germany, September 4–8, 2017 (ESEC/FSE’17), 11 pages.
https://doi.org/10.1145/3106237.3106251
Permissionto make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthe firstpage.Copyrights forcomponentsof thisworkowned byothersthan the
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/or a fee. Request permissions from permissions@acm.org.
ESEC/FSE’17, September 4–8, 2017, Paderborn, Germany
©2017 Copyright held by the owner/author(s). Publication rights licensed to Associa-
tion for Computing Machinery.
ACM ISBN 978-1-4503-5105-8/17/09 ...$15.00
https://doi.org/10.1145/3106237.31062510.0000.0010.002
−200 0 200 400 600 800 1000 1200
Apache's Web server performance in sDensityReal attribute−value distribution
0.00.10.2
51 0 1 5
Generated performance in sArtificial attribute−value distribution
Figure 1: Exemplary density plots of real-world (left) and
generated (right) attribute-value distributions. The density
captures the probability of a set of features or interactionstakingon attribute values over a specific range.
1 INTRODUCTION
The rise of variability modeling has been a success story both
in industry [ 9] and research [ 13]. Avariability model describes
all valid configurations of a configurable system by specifying
variationpointsintermsoffeaturesandconstraintsamongthem.
Often, features influence quality attributes of a system, such asperformance. To incorporate this influence in engineering tasks,
suchasfindinganoptimalconfiguration,developersanddomainexpertsmodelandreasonabout attributes offeaturesyieldingan
attributed variability model [8].
Attributed variability models form the conceptual basis of vari-
ous research areas, such as multi-objective configuration optimiza-
tion[19],runtimesoftwareadaptation[ 25],andservice-oriented
architecture [ 39]. In spite of their importance and general applica-
bility,there areonlyfew variabilitymodelspublicly available that
contain realistic attribute values [ 24], as we will discuss. Obtaining
realisticvaluesforperformanceandother quantitative qualityat-
tributesisexpensive,as thisrequiresmeasuringactualsystemvari-
ants (i.e., configurations ) and determining the effects of selecting
individualfeaturesontheattributes.Evenworse,effectsonqual-
ityattributescausedbyinteractionsbetweenfeaturesarelargely
ignored in prior work, although it has been shown thatfeature in-
teractionsare key to practicality and feasibility [32].
What does it mean for research and practice when staying in
the comfort zone of developing algorithms and tools in a setting
where artificial attribute values are used and where interactions
are neglected? This is the central question that we want to answer
here. Clearly, simply generating attribute values following stan-dard distributions hinders researchers to explore corner cases or
eventovarytheinputfortheirtechniquessystematically.Forex-
ample,ontheleftofFigure1,weshowanattribute-value(perfor-
mance)distributionobtainedfromareal-worldsystem(Apache’s
Web server). Compare this distribution with a normal distribution
(Figure1,right),frequentlyassumedintheliterature.Thisartificial
distributiongivesrisetoaconfigurationspaceinwhichtheoptimal
configurationscan beeasily foundvia simplehill climbing. Inthe
samevein,ignoringfeatureinteractionsyieldsprobleminstances
in which non-linearity does not occur – the resulting search space
268ESEC/FSE’17,September 4–8, 2017, Paderborn, Germany Norbert Siegmund, Stefan Sobernig, and Sven Apel
becomes homogeneous and steady. This renders a search task into
a convenient one, if not trivial. As a research community, we need
to ask ourselves: Have we enjoyed the comfort zone and kept ig-
noring attribute-value distributions and interactions as important
factorsin our experiments for years?
Our aim is not only to make the community aware of this prob-
lem, but also to provide a solution to step outside the comfort zone.
To mitigate the lack of realistic attributed variability models and
to enable systematic testing of algorithms and tools in this area,
weproposeanapproachandaccompanyingtool,calledThor,to
generaterealisticattributedvariabilitymodelsbasedonagivendis-
tribution of attribute values for features, interactions, and configu-
rations. Based on insights from feature-interaction detection [ 3],
Thorisabletogeneratedifferentdistributionpatternsofinterac-
tions, both in terms of their number and degree. To make a leap to-
ward a realistic setting, we have developed a novel technique that
combines kernel density estimation [ 35] and the genetic algorithm
NSGA-II[ 15].Ourtechniqueallowstoapplyagiven(empirically
determined) attribute-value distribution profile to a given variabil-
itymodel.Thatis,we rescaleagivendistributiontoanothervari-
ability model such that we yield a similar distribution, guaranteed
by kernel density estimation, as we will explain. So, for the first
time,onecansystematicallyexplorehowrealisticdistributionsand
interactions affect the quality, optimality, error-proneness, and per-
formanceof their respective algorithm or tool.
To summarize, we have two goals. First, we want to raise the
community’sawarenessoftheconsequencesofusingunrealistic
settings to develop, tune, and test their algorithms and tools on at-
tributed variability models. Second, we want to offer a comprehen-
sive solution to overcome the lack of realistic attributed variability
models for quantitative quality attributes. To reach these goals, we
makethe followingcontributions:
•Weproposeanapproachbasedonkerneldensityestimationand
ageneticalgorithmtoapply(empiricallydetermined)attribute-
valuedistributionsto agivenvariabilitymodel,andweincor-
porate interactions in a realistic manner. We demonstrate that
our approachis agnostic about the given value distribution.
•We provide the open-source tool Thorand evaluate the effec-
tivenessofkerneldensityestimationaswellasThor’sscalabil-
ity withrespect to a different number of configurations.
•Wereportonareplicationofastudyonmulti-objectiveopti-
mizationofattributedvariabilitymodelsbySayyadetal.[ 27],
demonstratingthattheinclusionofinteractionsandthechoice
of the attribute-value distribution indeed affect the outcome of
such a study. The study has been the basis of several follow-up
studies,renderingitareferencepointinthisarea.Thediffer-
ences between the original study and our replication offer new
insights into the effects of attribute-value distributions and in-
teractions and affects also very recent papers that reuse the
same experimental setting.
Background material, a replication package, all measurement data,
theopen-sourcetoolThor,andthecorpusofreal-worlddistribu-
tionsareavailableatasupplementaryWebsite:https://github.com/
se-passau/thor-avm.2 PRELIMINARIES AND PROBLEM
STATEMENT
In this section, we give background information on attribute-value
distributions as probability distributions and report on a literature
studyweconductedtoassessthecurrentstate ofusingattributed
variabilitymodels.
2.1 Probability Distributions
Anattribute-valuedistributionistheorderedsetofcontinuousprob-
abilities of features or interactions taking certain quality-attribute
values—hence, a probability distribution of quality-attribute val-
ues. That is, it captures how features distribute over different in-tervals of attribute values. For example, a distribution indicates
whethertheattributecanberepresentedbytypicalattributeval-
ues only (e.g., summary statistics, such as median and variance).
An attribute-value distribution is commonly reported using box-
plotstatisticsandvisualized,forexample,usingakernel-density
plot over the continuous attribute-value range (as in Figure 1).
The shape of a distribution can highlight majority and minority
groups of features and interactions as well as their relative influ-
ence on a given quality attribute. The capacity to highlight the rel-
ative influence of a minority is critical, because conventional sum-
mary statistics represent majority groups only. When analyzing
attributedvariabilitymodels,however,minorities(e.g.,singlefea-
tures)cancontributeheavilytocharacterizingasystem’squality
profile.Forexample,inacontentmanagementsystem,manyfea-
turesdonotcontributetotheperformanceofthesystem,butthe
choiceofthedatabasefeatureaffectsmostofthesystem’sperfor-
mance.Inthisvein,Siegmundandothersdemonstratedthatperfor-
mancemodelsderivedfromsmallsetsofindividualfeaturescap-
ture important system-wide performance influences [31].
2.2 State of the Art
We conducted a literature study to obtain an overview of (a) the
extent to which generated and real values for attributed variability
modelsareusedintheliterature,(b)theassumptionsandchoices
madebyresearchersthatusegeneratedattributevalues,and(c)the
awareness of the relevance of feature interactions. We present our
methodology(e.g.,theselectioncriteria),adescriptionoftheana-
lyzedpapers,andthreatstovalidityonoursupplementaryWebsite.Inwhatfollows,wesummarizetheprocedureandthekeyfindings:
Based on an iterative paper selection, we obtained a set of 2346
papers, which we analyzed for references to attributed variabil-
itymodels.Morespecifically,wewereinterestedinallpapers,in
whichattributedvariabilitymodelshavebeenusedtoaccomplish
a certain task (e.g., optimization). This analysis revealed 69 papers,
whichweanalyzedfurther.Wefoundthat52ofthe69papersuse
generatedattributevalues.Only8papersrelyonreal-worldvalues,
either obtained by actual measurements or extracted from other
sources(e.g.,theWeb).Fromthe52papers,21papersusedarandom
valuegeneratorfollowingauniformdistribution,10papersusea
normal distribution, and 13 papers assign values in an ad-hoc man-
ner without acertain targetdistribution. 17papers donot specify
the way how they generated the data. Remarkably, from the 69 pa-
pers we analyzed, not a single paper considered interactions. Only
8 papers discuss the absence of interactions as a threat to validity.
269Attributed Variability Models: Outside the Comfort Zone ESEC/FSE’17, September 4–8, 2017, Paderborn, Germany
Our findings raise the question of whether research in this area
is on solid grounds. Even some of the authors of the papers in
questionexplicitly state this threat:
"Finally, a threat is due to the artificial way the values of the
attributeswere assigned[..]" [19]
Furthermore, it seems that this threat to validity, explicitly men-
tionedthough,isnotcarefullyconsidered.IntheworkofSayyad
et al. [27], the authors state:
"A potential threat to construct validity is the use of artificial
attributevaluesasattributesoffeatures[...].Futureworkshould
attempttocollectrealdataforusewithIBEAandotherMEOAs
to best optimize product configuration."
Overall, there are 6 papers in our corpus published within justtwo years in top-ranked software-engineering conferences thatexplicitly cite Sayyad’s paper and use the sameartificial data as
attributevalues.Unfortunately,thecorrespondingthreattovalidity
isnotevenmentionedanymore.Likewise,thefactthatinteractions
are practically ignored in research is astonishing as much as it
isdisturbingespeciallyasinteractionsarecommoninreal-world
software systems [3].
3 GENERATING REALISTIC ATTRIBUTED
VARIABILITY MODELS WITH THOR
ZĞĂůͲǁŽƌůĚƐǇƐƚĞŵ
sĂƌŝĂďŝůŝƚǇŵŽĚĞůƚƚƌŝďƵƚĞǀĂůƵĞƐ
/ŶƚĞƌĂĐƚŝŽŶŐĞŶĞƌĂƚŝŽŶ/ŶƉƵƚĚŝƐƚƌŝďƵƚŝŽŶƐ
<ĞƌŶĞůĚĞŶƐŝƚǇ
ĞƐƚŝŵĂƚŝŽŶ
dĂƌŐĞƚĚŝƐƚƌŝďƵƚŝŽŶƐ
ƚƚƌŝďƵƚĞĚ
ǀĂƌŝĂďŝůŝƚǇŵŽĚĞů
DƵůƚŝͲŽďũĞĐƚŝǀĞŽƉƚŝŵŝǌĂƚŝŽŶ
Figure 2: Generating attributed variability models.
Inourapproach,weconsider quantitative qualityattributesmea-
suredonametricscale.Thekeyideaistogeneratecorrespondingat-
tributevaluesforagivenvariabilitymodelbasedonattributevalues
determinedbefore,eitherthrough(i)empiricalobservationsofreal-
worldapplications(e.g.,performancemeasurements),(ii)reverseen-
gineeredvalues(e.g.,fromWebsites[ 23]),or(iii)specificallygener-
ated attribute distributions (e.g., to test corner cases). Furthermore,
users may include different sorts of interactions with their respec-
tiveattributevaluesintothemodel.Forthefirsttime,thisapproach
allowsuserstocreate theirownandhighlycontrolledtest bed,in
whichtheycanpurposefullyvarythenumberanddistributionofin-
teractionsaswellasreuseprofilesofattributevaluesthathavebeen
obtainedfromrealapplications.Inwhatfollows,weusetheterm
distribution profile to describe a specific distribution of attribute
valuesover the set of features, interactions, and configurations.
Figure2providesanoverviewofallstepsinvolvedingenerat-
ing a realistic attributed variability model. Steps of the upper part
aimatobtainingdistributionprofilesofarealsystemforfeatures,
interactions, and configurations. There are a few data sets already
available[ 31,33,34],whichhavebeenincorporatedintoThor,the
tool accompanying the approach; the community can contribute
furtherdistributionprofiles.Here, we concentrate on the steps ofthe lower part in Figure 2:
Asinput,werequireavariabilitymodel(e.g.,inSPLOTorDIMACS
format),whichisgoingtobeaugmentedbyrealisticattributeval-
ues. In a next step, a user specifies whether we should generate
interactionsbetweenfeaturesofthegivenvariabilitymodel.This
stepaddressesthe problem that most variability models lack fea-
tureinteractions(cf.Section2.2).Then,theuserspecifiesthedis-
tribution profiles to be applied to features, interactions, and con-
figurations.Theseprofilesactasseparateobjectivesinthemulti-
objective optimization step, in which we generate attribute values
using a genetic algorithm (see Section 3.3).
3.1 Objective Formalization
Anattributedvariabilitymodelisatriple AVM=(F,C,M),where
Fis the set of features, Cis the set of constraints, and Mis a
mapping M={fc/mapsto→r|fc∈P ( F)∧ r∈R}from a feature or a
combination of features (i.e., an interaction, which is an element
ofthepowerset P)totheirqualityattribute,representedasareal
numberin R.1Sinceweareinterestedinthedistributionofattribute
values and not necessarily in the specific mapping, we compute an
outputdistributionprofile DOas a vector on real numbers with
DO=/angbracketleftr|r∈ M(P(F)) /angbracketright
Letusfirstreviewthegoalofourapproachfromasingleobjective
pointofview:Theuserspecifiesaninputdistributionprofile DI
as a vector of real numbers. The objective is now to generate an
outputdistributionprofile DOsuchthat DOissimilarto DIgiven
a certain similarity metric (following the notation by Boyd and
Vandenberghe [10]):
maximize
Msim(DI,DO),(1)
wheresimis a similarity measure that can be instantiated with dif-
ferent functions. In essence, we aim at finding an optimal mapping
fromfeaturesandinteractionstoattributevalues,suchthattheirdis-
tribution is similar to the given one.
0%1%2%3%
0 50 100 150
Attribute ValueDensityDensities
Input distribution
Output distribution
Figure 3: Comparison of an input
and outputdistribution.Let us assume that a
user specifies a distribu-
tionprofile(i.e.,the input
distributioninFigure3)us-
ingonly20elementsinthe
vector. The optimization
process now aims at find-
inganoutputdistribution
(cf.Figure3)foradesired
variability model with 500
features and interactions,
whilemaintainingthe overall distribution profile of the input.
Next,wehavetoextendouroptimizationproblemtothreetypes
of input distribution profiles. The background is that the attribute
values of individual features of a system often have a different dis-
tribution profile (e.g., in the effect strength and in their number of
values) than interactions among features. Hence, we consider DF
I
as the input profile for features and DI
Ias the input profile for in-
teractions. Furthermore, there is the distribution profile of valid
configurations(i.e.,systemvariants)denotedby DV
I.Theinclusion
1The size of Pis exponential in the size of F. In practice though, there are way fewer
interactions that have a relevant effect on an attribute value.
270ESEC/FSE’17,September 4–8, 2017, Paderborn, Germany Norbert Siegmund, Stefan Sobernig, and Sven Apel
of the latter renders our approach fundamentally different from ex-
istingwork.Thisisbecausemostofthealgorithmsintheliterature
do not work directly on the attribute values of features, but rather
on the attribute values of configurations [ 26]. For instance, finding
the fastest configuration using a genetic algorithm evaluates the
performance of configurations and not of individual features [27].
An attribute value of an individual configuration is computed
by aggregating the attribute values of features and their interac-
tions that appear in the configuration (i.e., all selected features andtheir corresponding interactions). A common aggregation function
Π:P(F)/mapsto→Rcomputes the linear combination of all relevant at-
tributevaluesof a given configuration fcas follows [31]:
Π(fc)=/summationdisplay.1
i∈P(fc)M(i)
Here, we sum the values of all elements in the powerset of configu-
rationfc,whichincludesallfeaturesandallinteractions.Further-
more, function Φ
Φ(F,C)={vc|vc∈P ( F)∧ vcis valid wrt. C}
computesthesetofallvalidconfigurationsbasedonthefeatures
and constraints in the attributed variability model. Each configura-
tion contains only the features that are selected. Function Φis usu-
ally realized by a SAT solver [ 6]. To obtain the distribution profile
DV
O, we compute the attribute values of all valid configurations:
DV
O=/angbracketleftr|r=Π(vc)∧∀vc∈Φ(F,C)/angbracketright
An important challenge for computing DV
Oto be similar to
DV
IisthatDV
Odependsonthemapping M,suchthatchanging
DF
Oand/orDI
Oinfluencesalso DV
O.Forexample,changingtheat-
tributevalueofasinglefeaturein Mchangestheattributevalue
of every configuration, inwhich the respective feature is selected.
Moreover, DV
Otakesonlyvalidconfigurationsintoaccount.There-
fore, constraints can have a substantial influence on which com-
binations of features and which interactions can occur in a valid
configuration,affecting DV
O.
In Figure 4, we give an example of how we compute DV
Obased
on a genetic encoding using (a) a configuration matrix ( m×r,
wheremis the number of configurations and ris the sum of the
number of features and interactions) to model feature selectionsand the resulting presence of interactions and (b) a value matrix
to model the mappings from features and interactions to their
values (
r×1). Each row in the configuration matrix on the left
representsageneticallyencodedvalidconfiguration.Thewholeset
of configurations is computed using function Φ. The presence of
interactionsdepends on whether their corresponding features are
also selected in the respective configuration. In the center of the
figure, we depict the value matrix, where features and interactions
aremappedtotheircorrespondingattributevalues.Weuseaglobal
ordering of features and interactions, such that we can use the dot
product to compute the attribute value of each configuration, as
shownwiththematrix( m×1)ontherightofthefigure.Wefurther
showtherelationshipbetweenthemappingfunction Mandthe
distributionprofiles DF
OandDI
Oas well as how we obtain DV
O.
Combiningthethreeinputprofilesresultsinthefollowingmulti-
objective optimization problem:Φ
Conf.1
Conf.2
...
Conf.mf1f2...fn
01..0
10..1
...
11..1
/bracehtipupleft/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehtipdownright/bracehtipdownleft /bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehtipupright
F{f1,f2}
{f1,f3}
...{f1,. . ,fn}
00..0
01..0
...
11..1
/bracehtipupleft/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehtipdownright/bracehtipdownleft /bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehtipupright
P(F)−F·M
f1/mapsto→10.5 ⎫⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎭D
F
Of2/mapsto→−3.2
...
fn/mapsto→34.8
{f1,f2}/mapsto→−0.8⎫⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎭D
I
O{f1,f3}/mapsto→−2.7
...
{f1,. . ,fn}/mapsto→12.0=Π
125.3 ⎫⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎭DV
O173.7
...
268.6
Figure 4: The process of computing the output distribution
profileDV
Obased on a genetic encoding of features and in-
teractionscombinedintoaconfigurationmatrix.Theconfig-
uration matrix is combined using the dot product with thematrixcontainingvaluesoffeaturesandinteractions,deter-mined by function M.
maximize
Mw1·sim(DF
I,DF
O)+w2·sim(DI
I,DI
O)+
w3·sim(DV
I,DV
O)(2)
wherew1+w2+w3=1 and 0≤w1,w2,w3≤1. The remaining
taskis to efficiently learn function M.
3.2 IncludingInteractions
As said previously, most variability models do not come with in-
teractions.So,wehavetoincludenewinteractionsintothegiven
model.However,we cannotincludeinteractionsblindly,because
theninteractionswill oftenhave no effect. For example,generat-
ing an interaction between mutually exclusive features will not af-
fect the attribute value of a configuration, because both features
will never be present together in any valid configuration. Further-
more,interactions arisenotrandomlyinpractice,butfollowcer-
tain patterns. Hence, in Thor, a user can specify (1) the number of
interactions and (2) the ratio of the degree of the interactions to
beincluded.2Forexample,wecanspecifythat80%ofallinterac-
tions are of degree two (i.e., between two features) and 20% of the
interactionsare of degree three (i.e., among three features).
Our algorithm to inject an interaction is as follows: First, we
select two features (for pair-wise interactions) at random. Second,
wequeryaSATsolvertoverify:Aretherevalidconfigurationswith
bothfeaturesselectedandwhereeachfeatureisselected,butnot
theotherone?Third,ifso,weaddaninteractionwiththechosen
features to the model; we repeat otherwise. For example, if we find
that the features f1,f2, andf5can be simultaneouslyselected in a
configuration and the selection of any of these features does not
implytheselectionofanotherfeatureinthisset,aninteractionwill
be generated such that there is a mapping M:{f1,f2,f5}→R
that needs to be learned in the optimization process.
3.3 GeneratingAttribute Values
Theremainingtaskistofindanoptimalmappingfromfeaturesand
interactions to real numbers such that we solve the multi-objectiveoptimizationproblemdefinedinEquation2.Westartfromarandomsetofvaluesanduseageneticalgorithmforadjustingthevaluestooptimizeforallthreeobjectives(i.e.,thethreetermsinEquation2).
2The degree of an interaction denotes the number of participating features in the
interaction.
271Attributed Variability Models: Outside the Comfort Zone ESEC/FSE’17, September 4–8, 2017, Paderborn, Germany
Asaresult,wecomparetheParetofrontofallsolutionswithrespect
to the distribution profiles. The Pareto front can be used to inspect
thesolutionstofindtheoutputdistributionthatfitsbestauser’s
requirements.Forexample,someusersmightpreferaprecisematch
between DF
IandDF
O, whereas others might tend to a mapping
closertothesimilarityofthedistributionprofilesofconfigurations.
Kernel Density Estimation. Starting from randomly initialized
values seems to be inefficient and can lead to suboptimal solutions
due to local optima. Hence, we propose an optimization to the pro-
cessdescribedbeforebasedon kerneldensityestimation [35].Ker-
nel densityestimation is a non-parametric method in statistics for
estimatingtheprobabilitydensityfunction P(x)ofarandomvari-
ablex:P(x)=∫∞
−∞K(x)dx, whereK(x)is the kernel function to
be determined. So, when we apply this function to a given inputdistribution profile
DI, we obtain a probability density function
that describes an underlying function from which the values in DI
originate.Now,wecandrawnewsamplesfromthisfunctionwhile
maintaining the overall distribution (as we did in Figure 3). This
method solves two problems: (1) we can scale a given distributionprofile toamuch largerdistribution profile
|DI|/lessmuch| DO|without
having the same values; (2) we provide a good starting assignment
for the mapping function to speed up the optimization process and
mitigatethethreadoflocaloptima.Toadditionallyimprovesimi-
larityfortheinitialassignmentofvalues,weusethetwo-sample
Kolmogorov–Smirnov test [ 36] to measure the extent (“goodness”)
of thefit between theinput distribution andthe intermediateout-
put distribution generated by kernel density estimation. Of course,
also the other metricsare applicable here.
Optimization Process. Starting from an initial set of solutions
(e.g., randomly assigned numbers or drawn from kernel density
estimation), the genetic algorithm changes the numbers in the
mapping function in an iterative process to maximize the objective
function(i.e.,toimprovethegoodness-of-fitscores).Changestothe
numbersaremadebystandardgeneticoperators,suchasmutation
and crossover, as provided by NSGA-II.
Tomakethecomputationof DV
Ofeasible,weselectandmeasure
only a sample set of configurations. The sample set is collected
usingauser-definedsamplingtechniqueincombinationwithaSAT
solver. The rows in the matrices on the left in Figure 4 represent
the sample set. Based on this set, we compute the presence of
interactionsbycheckingforeachinteractionandeachconfiguration
whetherthefeaturescausingtheinteractionareallpresentinthe
respectiveconfiguration(seetherightpartofthematrixontheleftofFigure4).Wecomputethematrixontheleftonlyoncebeforewestartthegeneticalgorithm.Allothervectorsandmatrixoperations
have to be populated and applied in each evolutionary step, as the
generatedattributevalueschangeaftereachiteration.Thatis,in
each iteration, we compute the dot product to obtain the attribute
value for eachconfiguration.
4 EVALUATION: REPLICATION
TodemonstratehowThorfacilitatesthesystematicinvestigationof
the relevance of interactions and the choice of attribute values, we
replicateapopularstudyonattributedvariabilitymodels[ 27,28].
Weselectedthisstudybecausewehadidentifieditasthemostfun-
damental one. Our literature study revealed the two correspondingpapers as the most cited ones. In addition, we found critical details
of Sayyad’s study adopted by several follow-up studies (e.g., the
attributedata).
Inaseriesofexperiments,Sayyadetal.[ 27,28]contrastedthe
performance(solutionquality andspeed)oft woevolutionarymeta-
heuristics(NSGA-II,IBEA)onfindingvalidconfigurationsofattrib-
utedvariabilitymodelsundermultipleobjectives.Theobjectives
were:validityof asolution,maximalconfiguration size,aswellas
attribute values of features, such as state of reuse, defect counts,
andcosts.Sayyadet al.performed theirexperimentson sevendif-
ferent, reverse-engineered variability models taken from the LVAT
repository3in DIMACS format. Each variability model was asso-
ciated with artificial attribute values having the same properties
(distributionalprofiles).Thekeyfindingswerethatthetestedmeta-
heuristicsarecapableoffindingvalidandoptimalconfigurations
inpracticaltimeboxes(30minutes)forconfigurationandoptimiza-tionspacesotherwiseunsuitableforexactapproaches.Inparticular,
IBEA wasfound toout-perform NSGA-II regardingsolution qual-
ity for the computation tasks at hand. Solution quality was mea-
sured in terms of Hypervolume (HV) and valid solutions in Pareto-
optimal solutionsets(PCORRECT).4
Whileconsideringadiversesetvariabilitymodels,theoriginal
studybySayyadetal.[ 27]doesnotincluderobustnesschecksofthe
meta-heuristics in the face of varying attribute-value distributions
and the presence of interactions. With the kind support of Sayyad
etal.,weperformedreplicationsforthesevenvariabilitymodelsfrom the baseline experiment to assess the robustness regardingsystematically varying attribute-value distributions and for the
inclusionof interactions.
Inourreplications,were-runtheexperimentswith identicalpro-
tocol and operationalization conditions of Sayyad’s baseline exper-
iment, while altering only two experimental conditions: the distri-
butional shapeof attribute valuesand the presenceof featureinter-
actions.Alltheotherdetailsofprotocol,operationalization,andthe
otherexperimentalobjectsremainunchanged,including:variabil-
itymodels,algorithmimplementation,numberofobjectivefunc-
tions, parameter settings, outcome measures, timeboxes. Design-
wise, this is a form of operational or differentiated replication [ 17].
This implies that, for the scope of such a replication, other threats
to the baseline experiment are not controlled for (e.g., threats tointernalorconstructvalidity).Wedesignedandreportthisrepli-
cation according to the guidelines on robustness of computational
tests[4,5]withemphasisonevolutionarymeta-heuristics[ 37].A
statisticalcompanionandthereplicationpackageareavailableat
the supplementary Web site.
In Section 4.1, we report on the replication for one LVAT model
(Toybox)inalldetail.Toyboxisacommand-lineutilitythatcom-
bines a subset the GNU shell commands into a single executable.
We selected the Toybox model as a showcase because it is illustra-
tiveforrobustnesseffectsfoundacrossthereplications.Then,in
Section4.2,wesummarizethereplicationresultsforthesixremain-
ing LVAT models, which were obtained from running the identical
replication protocol as for Toybox.
3https://code.google.com/archive/p/linux-variability-analysis-tools/
4Hypervolume (HV): a ratio capturing the coverage of a known Pareto front with
respecttotheobjectivespaceina2+-optimizationproblem[ 11];PCORRECT:theratio
of valid configurations contained by the Pareto-optimal solution set [27].
272ESEC/FSE’17,September 4–8, 2017, Paderborn, Germany Norbert Siegmund, Stefan Sobernig, and Sven Apel
4.1 Toybox Replication
Toestablishabaseline,wesuccessfullyconfirmedtheoriginalre-
sultsforToybox’svariabilitymodel(using50ratherthan10inde-
pendent runs): At comparable levels of HV (IBEA: median 0.22/
MAD50.0004;NSGA-II:0.21/0.011),asearchprocessusingIBEAre-
sultsinmorethantwiceasmanyvalidconfigurationsinthePareto-
optimalsolutionsetsthanNSGA-II(IBEA25.2%valid,onaverage;
NSGA-II: 10.83% valid,on average).
Setup.We are interested in whether the winning IBEA performs
equally well when varying the attribute values for one objective
(COST) as compared to the confirmatory replication. All other
four objectives remained unchanged. We selected the attribute
COST, because it has a continuous data scale. This allows us to test
a range of different distributions without introducing additional
assumptions. Planned variations of COST included (a) imposing an
empiricaldatadistributionderivedfromareal-worlddataset(as
opposedtotheoriginalnormaldistribution)and(b)theinclusionof
interactionsbetween featuresaffecting COST. As data set, weuse
measurementsfromx264,avideoencodinglibrary.WeincludedthesamenumberofinteractionsasfeaturesareintheToyboxvariability
model (544; hence, 100%, denoted as FI100); other numbers have
shown similar results. The interaction degrees follow artificial and
empirical(x264)interactiondata[31].
We tested the following null hypotheses on the twooutcome
variables available for Toybox from the original study (HV, PCOR-
RECT):
H10:There isnodifference between the mean outcome obtained
by IBEAwhen optimizingfor the original,normally distributed
(“artificial”)attributeCOST andthe x264-based (“empirical”) at-
tributeCOST.
H20:There isnodifference between the mean outcome obtained
byIBEAwhenoptimizingfortheoriginalCOSTattributewithout
interactions(F)andaCOSTattributeadjustedforinteractions
(FI100).
H30:There isnodifference between different combinations of
artificial/empiricaldistributionshapesandofCOSTcomputation
with/withoutinteractions.
Wetestedthesehypothesesusingatwo-wayanalysisofvariance
(ANOVA) procedure based on a 2 ×2 data layout: DIST (normal,
x264)×FINT (F, FI100). In preparation, we performed the standard
checks for an ANOVA (i.e., normality of residuals and the homo-
geneity of variances in each data cell). The nulls are discredited at
a significance level smaller or equal than 0.05.
The computational test for the extended replication included
50 runs per factor combination (i.e., 200 independent runs each
limitedto 30minutesruntime maximum).Weperformed theruns
on an HPC cluster, offering 17 nodes running Ubuntu 14.04 with64 GB RAM and 20 cores each (Intel Xeon E-5 2690v2 CPU, 10physical cores). Each run was allocated identical computational
resources to guarantee comparability. The job script is available at
the supplementary Web site.
Results.Wetestedthethreehypothesesforeachoutcomevari-
able, HV and PCORRECT, respectively; see Table 1. For both, we
foundsignificant,substantial,andnon-trivialvariabilityinthelight
5Median absolute deviation; a robustness measure.of differentattribute-value distributions (DIST: normal,x264) and
of the inclusion/exclusion of interactions (FINT: F, FI100).6
Figure 5 shows a line graph to examine the main effects of DIST
andFINTonHVandPC,respectively,aswellastheirinteraction
effects. Themeanoutcomes(HV,PC) for eachlevel of onefactor
(DIST:normal,x264)areplottedoveralllevelsofFINT(F,FI100).
Aninteraction line connects pairs of mean outcomes (HV, PC) from
the angle of the experimental factor on the x-axis (i.e., FINT; see
A–B and C–D in Fig. 5). The interaction lines and their relative
positionallowustovisuallyidentifythetypeandthestrengthofthe
(possible) interaction effects. For example, two parallel lines signal
thatthereisnointeractioneffectatallandtwointersectinglines
denote the presence of some interaction between the two factors.
Hypervolume(HV). Thereisasignificant,butsmallordinalin-
teraction (p <0.001,η2<0.01) between FINT and DIST (H3): The
effect of distributions (normal, x264) is different at the two FINT
levels(FandFI100),thatis,itappearsslightlystrongerforCOST
computations neglecting interactions (F). However, the order of
magnitude of the DIST effect is unchanged (hence, ordinal) across
the two different FINT levels (F and FI100). Despite this interaction
effect, there is a noteworthy—significant and large—effect of FINT
(p<0.001,η2=0.859)onHV(H2):85.9%ofthevarianceinHVisas-
sociated (covaries) with FINT (excluding or including interactions).
HV obtained by IBEA increases by 1.16 scores (confidence interval,
CI: 95%: 1.12 to 1.18; Tukey HSD) when including feature inter-
actions(FI100);ordecreasesbythesameamountwhenexcluding
them (see the steep slopes of the interaction lines A–B and C–D
in Fig. 5, on the left). Testing different value distributions (DIST:
normal,x264)resultsinasignificant,medium-leveleffectofDIST
(p<0.001,η2=0.134;H1).Thiseffectisvisualizedbythegapbetween
the mid-points of the interaction lines in Fig. 5 (on the left).
PCORRECT (PC). We found a significant and large disordinal
(“cross-over”)interactionbetweenDISTandFINT(p <0.001,η2=
0.428): 42.8% of the variance in PCORRECT can be linked to the
levelsofFINTandDISTinteractingwitheachother(H3).Theorder
of magnitude of the DIST effect changes (hence, disordinal ) across
thetwodifferentlevelsofcost(withandwithoutinteractions).In
Figure5(ontheright),thisdisordinalinteractionisdepictedasa
cross-overbetweenthetwointeractionlinesA–BandC–D.This
interactioneffectbetweenFINTandDISTgivesrisetoasignificant
and immediately tangible difference: Including interactions (FI100)
forthereal-worlddistribution(x264)yieldsadditionally9.6%(CI:
95%: 9.3 to 9.9%) of valid configurations found by IBEA when
compared to neglecting interactions for the same distribution (see
thedifferentslopesofthex264interactionlineC–Dandthenormal
interaction line A–B in Fig. 5, on the right). When compared to
the original setup (no interactions, normal; A vs. C), this difference
amounts to an averaged decrease of 4.7% (CI: 95%: 4.4 to 5). Hence,
depending on the combination of distribution and interactions,
IBEA yields considerably more or less valid configurations in its
solution set. Given their disordinal interaction, the results and
interpretationsoftheeffectsofDISTmustalwaysbequalifiedin
termsoftheeffectofFINT;andviceversa.Asaconsequence,we
6Bewarethattheterm interaction hastwomeaningsinthissection:“featureinteraction”
as an experimental factor in its own right (FINT: F, FI100) vs. “interaction effect”
between two experimental factors (DIST+FINT) in terms of the ANOVA.
273Attributed Variability Models: Outside the Comfort Zone ESEC/FSE’17, September 4–8, 2017, Paderborn, Germany




Figure 5: Nested box and interaction plots for the outcome
variables, with the lines A–B and C–D representing the
“interaction lines”; on the left (Hypervolume, HV): signifi-
cantordinalinteractioneffect(DIST+FINT),plussignificantmaineffects(DIST,FINT);ontheright(PCORRECT):signif-icantdisordinalinteractioneffect (DIST+FINT)
do not interpretthe effectsof FINTand DIST(althoughsignificant)
in isolation from each other (H1, H2).
Table 1: Variance tables; SS: sum of squares; Df: degrees of
freedom;MSS:Meansumofsquares;+:interactionbetweentwo factors
Src SS Df MSS F p η2
HVDIST (H1) 1.1e−3 11.1e−3 12714 <0.001 0.134
FINT (H2) 6.8e−3 16.8e−3 81544 <0.001 0.859
DIST+FINT (H3) 3.5e−5 13.5e−5 423 <0.001 0.005
1.6e−51968.3e−8
PCDIST (H1) 6.5 16.48 15 <0.001 0.003
FINT (H2) 1280 .0 11280 .0 3098 <0.001 0.535
DIST+FINT (H3) 1025 .0 11025 .0 2479 <0.001 0.428
81.0 196 0.41
Next, we summarize the replications of the computational ex-
periments for allotherLVAT models used by Sayyad et al. [27].
Table 2: Effects and effect sizes (mean differences) on Hy-
pervolume(HV)andtheratioofcorrectconfigurations(PC)from the replications of Sayyad et al. for six out of sevenLVAT models; significance level ≤0.05;CI95%
Model HV Mean Differences on FINT (F–FI100)
effect(s)aη2
Toybox FINT (DIST) 0.859 (0.134)
axTLSbFINT+DIST 0.401
eCos FINT (DIST) 0.966 (0.034)FreeBSD FINT (DIST) 0.993 (0.005)Fiasco FINT (DIST) 0.997 (0.002)
uClinux FINT (DIST) 0.914 (0.075)
Model PC Mean Differences on FINT+DISTc
effect(s)aη2
−10% −5% 0 5% 10%Toybox FINT+DIST 0.443
axTLS FINT+DIST 0.401
eCosbFINT, DIST 0.452, 0.542
FreeBSDbFINT, DIST 0.350, 0.551
Fiasco FINT+DIST 0.181
uClinux FINT+DIST 0.794
aFINT,DIST:twosignificant andsubstantialmaineffects;FINT(DIST):twosignificant
main effects, but DIST is not substantial; FINT+DIST: a significant and substantial
interaction effect, no main effects
baxTLS (for HV) as well as eCos and FreeBSD (for PC) represent important exceptions
from the otherwise observed effect patterns that are discussed in the text.
cMean differences between F/normal and FI100/x2644.2 FurtherReplications
We conducted the differentiated replication as detailed for Toybox
in Section 4.1 also for the other six LVAT models: axTLS, eCos,
FreeBSD,Fiasco,uClinux,andLinux.Asreportedforthebaseline
experiment[ 27],LinuxdoesnotyieldresultsforIBEA(unlessusing
aseedconfiguration),therefore,itisexcludedfromthefollowing
report. Table 2 summarizes the findings for the remaining six mod-
els (incl. Toybox). Overall, we found significant, substantial, and
non-trivialeffectsofdifferentattribute-valuedistributions(DIST:
x264 vs. normal) and of the presence of interactions (FINT: F vs.
FI100)onsolution-qualityindicators(HV,PC)whenapplyingthe
evolutionarymeta-heuristicIBEAonthesixoptimizationproblems
(see columns “effect(s)” in Table 2):
FINT, DIST :There are both main effects (significant and substan-
tial).
FINT (DIST) :FINTshowsasignificantandsubstantialeffect,DIST
is significant but notsubstantial.
FINT+DIST :There is a significant and substantial interaction ef-
fect between FINT and DIST (no main effects).
Hypervolume. First, in five replications, adding or removing in-
teractions from the computation (FINT) is capable of increasing or
decreasingthecoverageoftheobjectivespacebythesolutionset
(HV), independently from the underlying distribution (DIST). The
uppercompartmentofTable 2summarizesthetypicaldifferences
rangingbetween1.16(min,Toybox)to6.7HVscores(max,eCos).
The same replications also indicate a significant and, when com-
pared to interactions, very small effect of the underlying attribute-
value distribution (normal, x264). A noteworthy exception is the
axTLSreplication,whichcounterstheabovefindings.Thereisanin-
teractionFINT+DISTwithHVbeingreducedsubstantiallyforx264
attribute-value distribution with interactions, while it increases
for the normal distribution with interactions (by a smaller fraction
though). IBEA for axTLS with an x264 distribution and with inter-
actions (FI100/x264) obtains 11.43HV scores less than axTLS with
normaldistributionand withoutinteractions(F/normal).
PCORRECT. Second,intheoriginalstudy,theuseofIBEAwas
reportedsuperiorbecauseitresultsinmorevalidsystemconfigura-
tions(PC)beingfoundinatimeboxof30minutesthanusingNSGA-
II.Whileourreplicationsconfirmthisgeneralpicture,wefoundthatPCissensitivetothepresence/absenceofinteractions(FINT)aswellasthedistributionshape(DIST)forallsixmodels(seeTable2;lower
compartment). In three replications (Toybox, axTLS, and uClinux),
interactions (FINT) and distribution shape (DIST) interfere to an
extreme (cross-over): An IBEA search on a realistic attribute-value
distribution (x264) yields significantly less configurations than one
basedonanartificialone withoutinteractions(F/normal),andmore
valid configurationsfortherealistic distribution withinteractions
(FI100/x264).ThelattereffectamountstomeandifferencesonPC
between 1.73% (min, uClinux) and 8.33% (max, Fiasco; see Table 2).
ForeCosandFreeBSD,however,interactions(FINT)anddistribu-
tion (DIST) take effect independently from each other. The sizes of
theeffectsonPCarecomparativelysmallerthantheoneinteraction-
effect size for the other four models: up to approximately 1% in-
crease or decrease (FreeBSD) in valid configurations found.
274ESEC/FSE’17,September 4–8, 2017, Paderborn, Germany Norbert Siegmund, Stefan Sobernig, and Sven Apel
Summary. In our experiments, we learnt that the solution qual-
ity (HV, PC) in the Sayyad et al. experiment is sensitive to the pres-
ence of feature interactions and realistic attribute-value distribu-
tions. Varying both factors (FINT, DIST) at a time as the experi-
mentalconditionisalsocapableofrevealingexceptionalvariations
(axTLS,eCos,andFreeBSD).Notethatchangestothesettingwill
likely add to the observed effects on HV and PC (since we touched
only oneout of five objectives).Such changes include realisticdis-
tributions for the remaining attributes, varying the number of fea-
tureinteractions,7ordroppingtheoriginalvalueforFINT(5.0,15.0)
in favor of realistic values (due to negative boundaries and hetero-
geneous intervalsfor attributevalues;see Section 2).
4.3 Threatsto Validity
Running a differentiated replication gives rise to threats to con-
structvalidity:Consideringinteractionsrequiredustoadjustthe
originaltrueParetofrontsfortheHVcomputationtoincludelower
andupperboundariesbasedontheinteractions’attributevalues.
Alsonotethatwehadtoincorporatecorrectionstoaddressunre-
portedissuesintheoriginalstudy.Theseincludeacontradiction
regarding attribute-value distributions: Sayyad et al. stated a de-
pendency between two attributes (objectives) used in their experi-
ment (USED_BEFORE, DEFECTS), which effectively led to one setof test data not having the claimed normal distribution. Therefore,
we performed the replication with and without this dependency,
and report any variations in the statistical companion. However,
thebigpicturedoesnotchange.Athreattoexternalvalidityisthat
our robustness experiment is limited to one systematic variation
(FI100,x264,onevaluerange).Atthispoint,however,theobjective
wasto demonstratethe mereexistence ofsignificant andsubstan-
tial variations due to the choice of the distribution and the pres-
enceofinteractions.Infuturework,weshallinvestigatepossible
variation patterns on solution-quality indicators by systematically
varying attribute-valuedistributioncharacteristics.
5 EVALUATION: VALIDITY & SCALABILITY
Although we have already successfully applied Thor to conduct
thereplicationinSection4,inwhatfollows,weshedmorelighton
the validityandscalabilityof Thor.
5.1 Validity Experiments
The validity of Thor naturally depends on whether the generated
attributedvariabilitymodelsarerealistic.Byusingkerneldensity
estimation, we rescalea given distribution to another variability
model.Thatis,thedegreeofrealismdependsonwhethertheinput
distribution is realistic. What remains to be shown is whether our
main innovation—applying kernel density estimation as a prestep
of the genetic algorithm—actually improves the similarity of the
output distributions with respect to the input distributions. Tothis end, we conducted an experiment, in which we generate anattributed variability model for the same input distribution and
thesamevariabilitymodel,onceusingkerneldensityestimation
(KDE+GA)andonceusingonlythegeneticalgorithm(GA).This
way, we want to answer the following research question:
7Priorworkshowsthatquadraticnumbersofinteractionscanbefoundwhencompared
to the feature count [21].RQ1:Doeskerneldensityestimation(KDE+GA)improvethesim-
ilarity of the output distributions to the input distributions com-
pared to pure genetic optimization (GA)?
Setup.Toratethesimilaritybetweentheoutputandinputdis-
tribution,weneedground-truthattributedvariabilitymodels.We
were able to get hold of five models with feature attributes, in-
teraction attributes, and attribute values of all valid configura-
tions [18,31,32], which we use for this purpose. Our aim here is
to show that our technique is agnostic with respect to the given
distribution profile. As similarity measures, we use three tests: the
Anderson-Darling test, the Pearson’s correlation coefficient, and
theEuclideandistance.Therationaleofusingthreedifferentsta-
tisticsisthatasingleoneisusuallynotsufficientforcomparison,
due to implications of the central limit theorem: Ideally, we can
checkwhethertheoutputdistributionandtheinputdistribution
are drawn from the same probability distribution, which is why
we use the p-value of the Anderson-Darling test as a first mea-
sure. However, with a growing size of the data set, every changebecomes significant and so the p-value tends to become zero. Inthiscase,wecancombinetwootherstatistics.First,wecomputePearson’s correlation coefficient to assess the linear dependence
(correlation)betweentheinputandoutputdistribution.Ifthevalue
is 1, both distributions are linearily correlated. However, this mea-
sure alone can become misleading when the output distribution
has beenshifted towards smaller or largervalues. Hence, wecom-
pute,inaddition,thedistanceofbothdistributions dist(DI,DO)=
abs(/summationtext.1DI−/summationtext.1DO)/|DO|8tocapturesuchshifts.Weconsideran
outputdistributionsimilartoagiveninputwhenityieldsahigh
p-valueor a high correlation plus a low distance.
Since we have non-determinism in the genetic algorithm and
thekerneldensityestimation,weexecutethewholeoptimization
process 30 times. Also, we use all 50 solutions provided by the
geneticalgorithm. Thismeans thatour metricsare averagedover
30+50 output distributions. The plotted output distributions (red&
brightinTable3)representsthebestsolutionwithrespecttothe
p-values.Weprovidealsoacomparisonwithallotherdistributions
at our supplementary Web site.
Results.In Table 3, we show the results of our validity experi-
ment. For each model, we compare the three statistics when using
KDE+GA against GA. Furthermore, to illustrate how the actual dis-
tributionscompare,weplottheinputdistributions(blue&dark)andoutputdistributions(red&bright).Wehighlightedthecellsingreen,
in which the respective statistics suggest closer similarity. Table 3
clearly shows that, for most systems, KDE+GA provides more sim-
ilar output distributions. For Apache, h264, and LLVM, the high
p-values indicate that DF
OandDI
Oare drawn from the same prob-
abilitydistributionas DF
IandDI
I,respectively.ForApache,we
alsoseethat,althoughthep-valueisclosetozerofor DV
O,wehave
a nearly perfect match when looking at the plotted distributions of
theconfigurations.Fortheremainingmodels,thep-valueisusu-
ally too low such that we need to look at correlation and distance.
ForBDBC,weseethatallmetricsperformverysimilarforbothap-
proaches,whichcanalsobeseenintheplotteddistributions.For
BDBJ, we observe again a clear trend that KDE+GA approximates
8Wesumupallelementsofbothdistributionsandcomputetheabsolutedifference,
andwedividebythenumberofvaluesinthedistributiontoobtainthemeandistance.
275Attributed Variability Models: Outside the Comfort Zone ESEC/FSE’17, September 4–8, 2017, Paderborn, Germany
Table 3: Comparing output distributions with input distri-
butions using kernel density estimation (KDE) and geneticalgorithm (GA). AD=Anderson-Darling test; Cor=Pearson’scorrelation coefficient; Dist=Euclidean distance
AD Cor Dist DistributionsApache
GADF
O0.57 0.22 81 Features Interactions Configurations
DI
O0.020.61 461
DV
O0 0 1698KDE+GADF
O0.650.52 96 Features Interactionsee Configurations
DI
O0.66 0.46 51
DV
O00448BDBC
GADF
O0.170.570 Features Interactions Configurations
DI
O0 0.5 0
DV
O00 3KDE+GADF
O0.1 0.55 1 Features Interactions Configurations
DI
O0.390.55 4
DV
O001 3BDBJ
GADF
O0 0.49 2623 Features Interactions Configurations
DI
O00.56 5342
DV
O0 0 110525KDE+GADF
O0.010.531694 Features Interactions Configurations
DI
O0.17 0.4 1450
DV
O0014069h264
GADF
O0.27 0.37 24 Features Interactions Configurations
DI
O0.080.520
DV
O0 0 1277KDE+GADF
O0.230.54 45 Features Interactions oo Configurationsoo
DI
O0.21 0.44 93
DV
O001087LLVM
GADF
O0.08 0.32 77 Features Interactions Configurations
DI
O0 0.45 97
DV
O0 0 1606KDE+GADF
O0.430.644 Features Interactionstt Configurations
DI
O0.570.484
DV
O00298
the true distributions substantially better than GA. This is espe-
cially apparent in the plotted distributions.
Discussion. Theresultspaintaclearpicture:Kerneldensityes-
timation improves the similarity of the output distributions sub-
stantially (RQ1). It seems that relying only on genetic optimization
quicklysaturatesinalocaloptimum,suchthat,inmanycases, DF
O
andDI
Oare still uniformly distributed. We conclude that, to avoid
this general trap in meta-heuristic optimizations, initializing theoptimization process using a seed distribution is crucial. Moreover,
wealsoobservethatstatisticalmeasuresarenotentirelysuitable
as they may depend on the size of the distribution and other fac-
tors.This is why we suggest a visual comparison at the end of the
optimization process to select a proper candidate out of the Pareto
front generated by Thor.
5.2 Scaling Experiments
Beside validity, we want to assess the scaling capabilities of Thor
to larger-sized variability models. In particular, we want to answer
the followingresearch question:
RQ2:How does Thor’s performance scale with an increasing
number of configurations?
TherationaleofRQ2isthatcomputingthematrixofconfigurations
(cf.Figure4)hasitslimits,aswecanpotentiallygenerate2|F|valid
configurations. Hence, we want to assess how different sizes of the
matrixaffect optimization time.
Setup.To answer RQ2, and for the same reasons as in our repli-
cation (see Section 4), we use Toybox’s variability model. This vari-
abilitymodelisrealisticandfeasibletoobservehowThorscales
with respect to an increasing number of configurations. For this
purpose, we generate 109 interactions (20%) divided into 80% ofdegree 2, 10% of degree 3, and 10% of degree 4. We repeated the
run of the genetic algorithm 10 times, to account for measurement
bias and random effects. We aborted the genetic algorithm after
5000 iterations and evaluate 50 populations per iteration. As input
distributions, we use the distributions of binary size values (in KB)
of BDBC (other distribution profiles yielded similar results).
WeusedanIntelCorei7–4790@3.60GHzwith16GBRAMrun-
ning Windows 10 Pro, version 1607, build 14393.5 for all experi-
ments. We provide all data at the supplementary Web site.
Results.Figure6showstheperformancefactorsoftheindividual
steps when generating attribute values (RQ2). We can see that
the tasks to be executed only once have only a marginal effect
ontheoverallexecutiontime.Executing Φincludesthesampling
process (i.e., taking only a subset of all valid configurations) andthe SAT check. Matrix creation computes the binary matrix (leftin Figure 4) based on the set of determined configurations and
thesetofgeneratedinteractions.Computingthedotproductand
the fitness values have to be performed at each iteration of the
genetic algorithm, 50 times (one for each candidate). The largest
portion of computation time (80%) requires the computation ofthe dot product, in which the matrix of the selected features andinteractions are multiplied with the generated attribute values.
Forillustration,thisoperationrequiresapproximately1500sfora
matrix with 125,000 rows and 653 columns. The remaining 20% of
computationtimearemainlyconsumedbycalculatingandranking
the fitness of all solutions.
Discussion. ToanswerRQ2,theresultsofourperformanceexper-
imentsshowthatThorscaleslinearlywiththenumberofsampled
configurations,whichareusedtocompute DV
O.Allremainingtasks
haveonlyaconstantinfluenceonexecutiontime.Son,thelinear
dependence enables Thor to handle large-scale variability models.
276ESEC/FSE’17,September 4–8, 2017, Paderborn, Germany Norbert Siegmund, Stefan Sobernig, and Sven Apel
0500100015002000
25000 50000 75000 100000 125000
Number of sam pled confi gurationsTime in sTasks
Configuration Generation
Matrix Creation
Dot ProductFitness CalculationComputation time of the genetic algorithm
Figure 6: Processing time fractions with an increasing sam-
ple set, before and during executing the genetic algorithm.
5.3 Threatsto Validity
Internal Validity. There are threats resulting from the setting
choices on the genetic optimization (GA) and the computational
test environment. As for the GA, we adopted the built-in NSGA-II
operators plus parameter defaults as provided by the library JMet-
alCSharp, because the focus was on a comparison based on a stan-
dard setup. Systematically studying the effects of alternative GAs,
domain-specific operators [ 12], and parameter tuning is needed to
eliminatethisthreat,entirely.Thereportedexecutiontimesmay
havebeendependentonthemachineload.Wemitigatedthisthreatbyaggregatingover10independentruns.Anenvironmentalthreat
arises from possible bugs in our implementation. So, we tested for
multiplegoodness-of-fitmetricsandcomparedtheresultsmanu-
ally in multiple steps of the GA. Hence, we are convinced that our
metricsarecorrectlyimplemented.Furthermore,weuseexisting
open-source implementations for some metrics and the GA (JMet-
alCSharp),whichfurther mitigatesthisthreat.
ExternalValidity. Wehaveusedanumberofdifferentvariability
models and distribution profiles for the validity and the scaling
experiment.Whatwedidnotreporthere,duetospacelimitations,is
that we performed additional experiments with models taken from
the SPLOT repository as well as models generated using BeTTy.
Thesemodelshavedifferentsizesandconstraintssothatwecovered
alargespectrumofvariabilitymodelsandthebigpictureisthesame.
Furtherinformationis available at the supplementary Web site.
6 RELATED WORK
Variability-ModelGenerators. Apopularvariability-modelgen-
erator is provided by SPLOT [ 22]. SPLOT allows users to specify
the number of features, the ratio of alternative and optional fea-tures, and the number of cross-tree constraints. SPLOT contains
also an online repository of variability models. Most of the papers
inourliteraturestudyusedmodelsfromtheSPLOTrepository.The
SPLOT models can be used as an input forThor.
The closest approach to ours is BeTTy, a Java library for gen-
eratingvariability models[ 29]. It alsosupports theassignment of
constantsaswell astherandomgeneration ofattributevaluesfol-
lowing normal and uniform distributions. H owever, BeTTy (and
other generators, as well) does not support the generation of inter-
actionsanditisnotpossibletosupplyrealisticdistributionprofiles.
Variability-ModelSynthesis. Synthesizingrealisticvariabilitymod-
els has recently gained momentum. There are different approaches
that aim at reverse engineering variability models based on textualdescriptions and feature dependencies [ 30], documentation of indi-
vidualproductsandtheirrelationships[ 14],requirementsspecifica-
tion [1,38], and genetic programming [ 20]. Recent approaches aim
atimprovingthehierarchywithinthemodel,forinstance,based
onanontology[ 7].Allapproachesdonotconsiderattributevalues.
Onlyrecently,Nasrandothershavedemonstratedthatalsotech-
nicaldescriptions,suchasattributevalues,canbeextractedfrom
productdescriptions[ 23].Weseethisworkasanimportantsource
to obtain further realistic input distribution profiles.
Distribution Profiles. There are various domains, in which the
distribution of data is important and can affect the outcome andbehavior of an analysis. The testing community, for instance, is
constantly developing novel approaches to find bugs in programs.
But, it is always difficult to state to which degree an approachor a test suite can find unknown bugs and—similar to real-worldattributed variability models—there are often too few real faults
tobeusedinexperiments[ 2].Mutationtestingisawaytoassess
the quality of test suites and detection algorithms [ 16]. The idea is
toinjectfaultsintoaprogramsuchthatadevelopercanevaluate
whether the test suite can spot the faults. A related study explored
whetherfaultsgeneratedbyhandorfrommutationoperatorsare
representatives of real faults [ 2]. The study found that one must
carefullyselectthemutationoperatorsandthereisdangerinusing
manually seeded faults. We draw a similar conclusion for attribute-
value distributionsof variability models.
7 CONCLUSIONS
Attributed variability models are used in various areas, but, due to
the lack of realistic attribute values, the overwhelming majority of
algorithmsandtoolsoperatingonattributedvariabilitymodelshave
beendevelopedand trainedonartificialattributevalues,ignoring
featureinteractions.The overarchinggoalofour workistomake
researchers and practitioners aware of this problem, and we strive
for a more realistic and robust setting—leaving the comfort zone of
artificialattributed variability models. We conducted a literature
study with the main outcome that interactions are not considered
bystate-of-the-artexperimentsandthatattributevaluesaremostlygeneratedbasedonartificialdistributions.Asthefirstreplicationof
this kind, we reproduced a popular experimental setting of Sayyad
etal.[27]andfoundthat,whilethekeyresultsoftheoriginalstudy
hold, feature interactions and varying attribute-value distributions
leadto important deviations in solution quality.
As an actionable solution, we provide Thor, a tool for including
realistic attribute values and feature interactions into a given vari-
ability model. We employ kernel density estimation for this pur-
pose,torescaleaninputdistributiontothefeatures,interactions,
and configurations of a given variability model. Using a genetic al-
gorithm, we adjust the attribute values such that they match theinput distributions. In a series of experiments, we demonstrated
that (a) using kernel density estimation is key for obtaining a good
matchwithoutputdistributionsandthat(b)ourapproachscales
linearlywiththenumberofconfigurationsthatareusedforcom-
puting thedistributionprofile.
ACKNOWLEDGMENTS
Siegmund’s andApel’s work are supported by the DFGunder the
contractsSI 2171/2, AP 206/6, and AP 206/7.
277Attributed Variability Models: Outside the Comfort Zone ESEC/FSE’17, September 4–8, 2017, Paderborn, Germany
REFERENCES
[1]Vander Alves, Christa Schwanninger, Luciano Barbosa, Awais Rashid, Peter
Sawyer, Paul Rayson, Christoph Pohl, and Andreas Rummler. 2008. An Ex-ploratory Study of Information Retrieval Techniques in Domain Analysis. In
Proc. Int. Software Product Line Conference (SPLC) . IEEE, 67–76.
[2]J.H.Andrews,L.C.Briand,andY.Labiche.2005. IsMutationanAppropriate
Tool for Testing Experiments?. In Proc. Int. Conf. Software Engineering (ICSE) .
ACM, 402–411.
[3]Sven Apel, Sergiy Kolesnikov, Norbert Siegmund, Christian Kästner, and Brady
Garvin. 2013. Exploring Feature Interactions in the Wild: The New Feature-interaction Challenge. In Proc. GPCE Workshop on Feature-Oriented Software
Development (FOSD) . ACM, 1–8.
[4]Richard S. Barr, Bruce L. Golden, James P. Kelly, Mauricio G. C. Resende, and
WilliamR.Stewart.1995. DesigningandReportingonComputationalExperi-
ments with Heuristic Methods. J. Heuristics 1, 1 (1995), 9–32.
[5]Thomas Bartz-Beielstein and Mike Preuss. 2010. The Future of Experimental
Research. Springer, 17–49.
[6]DonBatory.2005. FeatureModels,Grammars,andPropositionalFormulas.In
Proc. Int. Software Product Line Conference (SPLC) . Springer, 7–20.
[7]Guillaume Bécan, Mathieu Acher, Benoit Baudry, and Sana Ben Nasr. 2015.
BreathingOntological KnowledgeInto FeatureModel Synthesis:An Empirical
Study.Empirical Software Engineering 21, 4 (2015), 1794–1841.
[8]David Benavides, Pablo Trinidad, and Antonio Ruiz-Cortes. 2005. AutomatedReasoning on Feature Models. In Proc. Conf. Advanced Information Systems
Engineering (CAiSE) . Springer, 491–503.
[9]Thorsten Berger, Ralf Rublack, Divya Nair, Joanne M. Atlee, Martin Becker,Krzysztof Czarnecki, and Andrzej Wasowski. 2013. A Survey of Variability
ModelinginIndustrialPractice.In Proc.Int.WorkshoponVariabilityModellingof
Software-intensive Systems (VaMoS) . ACM, 1–8.
[10]StephenBoydandLievenVandenberghe.2004. ConvexOptimization . Cambridge
University Press.
[11]Carlos A. Coello Coello, Gary B. Lamont, and David A. Van Veldhuizen. 2006.
EvolutionaryAlgorithmsforSolvingMulti-ObjectiveProblems (2nded.). Springer.
[12]Thelma Elita Colanzi and Silvia Regina Vergilio. 2016. A Feature-Driven
Crossover Operator for Product Line Architecture Design Optimization. Journal
of Systems and Software 121 (2016), 126–143.
[13]Krzysztof Czarnecki, Paul Grünbacher, Rick Rabiser, Klaus Schmid, and Andrzej
Wasowski.2012.CoolFeaturesandToughDecisions:AComparisonofVariability
ModelingApproaches.In Proc.Int.WorkshoponVariabilityModellingofSoftware-
intensive Systems (VaMoS) . ACM, 173–182.
[14]Jean-Marc Davril, Edouard Delfosse, Negar Hariri, Mathieu Acher, Jane Cleland-
Huang,andPatrickHeymans.2013. FeatureModelExtractionfromLargeCol-
lectionsofInformalProductDescriptions.In Proc.Europ.SoftwareEngineering
Conf./Foundations of Software Engineering (ESEC/FSE) . ACM, 290–300.
[15]Kanpur Deb, Amrit Pratap, Sameer Agarwal, and T. Meyarivan. 2002. A Fast
and Elitist Multiobjective Genetic Algorithm: NSGA-II. IEEE Transactions on
Evolutionary Computation 6, 2 (2002), 182–197.
[16]Richard A. DeMillo, Richard J. Lipton, and Frederick G. Sayward. 1978. Hints on
Test Data Selection: Help for the Practicing Programmer. Computer 11, 4 (1978),
34–41.
[17]OmarS.Gómez,NataliaJuristo,andSiraVegas.2014. UnderstandingReplication
of Experiments in Software Engineering: A Classification. Information and
Software Technology 56, 8 (2014), 1033–1048.
[18]JianmeiGuo,Krzysztof Czarnecki, Sven Apel,Norbert Siegmund, andAndrezj
Wasowski. 2013. Variability-Aware Performance Prediction: A Statistical Learn-
ingApproach.In Proc.Conf.AutomatedSoftwareEngineering(ASE) .IEEE,301–
311.
[19]ChristopherHenard,MikePapadakis,MarkHarman,andYvesLeTraon.2015.
CombiningMulti-objectiveSearchandConstraintSolvingforConfiguringLarge
Software Product Lines. In Proc. Int. Conf. on Software Engineering (ICSE) . IEEE,
517–528.
[20]Lukas Linsbauer, Roberto Erick Lopez-Herrejon, and Alexander Egyed. 2014.
FeatureModelSynthesiswithGeneticProgramming.In Proc.Int.Symp.onSearch-
Based Software Engineering (SSBSE) . Springer, 153–167.[21]Jia Liu, Don Batory, and Christian Lengauer. 2006. Feature Oriented Refactoring
ofLegacyApplications.In Proc.Int.Conf.SoftwareEngineering(ICSE) .ACM,112–
121.
[22]Marcilio Mendonca, Moises Branco, and Donald Cowan. 2009. SPLOT: Software
ProductLinesOnlineTools.In CompanionProc.Int.Conf.Object-OrientedPro-
gramming, Systems, Languages and Applications (OOPSLA) . ACM, 761–762.
[23]SanaBenNasr,GuillaumeBécan,MathieuAcher,JoãoBoscoFerreiraFilho,Nico-
lasSannier,BenoitBaudry,andJean-MarcDavril.2017. AutomatedExtraction
of Product Comparison Matrices from Informal Product Descriptions. Journal of
Systems Software 124, C (2017), 82–103.
[24]Lina Ochoa, Juliana Alves Pereira, Oscar González-Rojas, Harold Castro, andGunter Saake. 2017. A Survey on Scalability and Performance Concerns inExtended Product Lines Configuration. In Proc. Int. Workshop on Variability
Modelling of Software-intensive Systems (VaMoS) . ACM, 5–12.
[25]GustavoG.Pascual,RobertoE.Lopez-Herrejon,MónicaPinto,LidiaFuentes,and
Alexander Egyed. 2015. Applying Multiobjective Evolutionary Algorithms to
DynamicSoftwareProductLinesforReconfiguringMobileApplications. Journal
of Systems Software 103, C (2015), 392–411.
[26]AtriSarkar,JianmeiGuo,NorbertSiegmund,SvenApel,andKrzysztofCzarnecki.
2015. Cost-EfficientSamplingforPerformancePredictionofConfigurableSys-
tems. InProc. Int. Conf. Automated Software Engineering (ASE) . IEEE, 342–352.
[27]Abdel S. Sayyad, Joseph Ingram, Tim Menzies, and Hany Ammar. 2013. Scalable
Product Line Configuration: A Straw to Break the Camel’s Back. In Proc. Int.
Conf. Automated Software Engineering (ASE) . IEEE, 465–474.
[28]AbdelSalamSayyad,TimMenzies,andHanyAmmar.2013. OntheValueofUser
Preferences in Search-based Software Engineering: A Case Study in Software
Product Lines. In Proc. Int. Conf. on Software Engineering (ICSE) . IEEE, 492–501.
[29]SergioSegura,JoséAGalindo,DavidBenavides,JoséAParejo,andAntonioRuiz-
Cortés. 2012. BeTTy: Benchmarking and Testing on the Automated Analysis
of Feature Models. In Proc. Int. Workshop on Variability Modelling of Software-
intensive Systems (VaMoS) . ACM, 63–71.
[30]StevenShe,RafaelLotufo,ThorstenBerger,AndrzejWasowski,andKrzysztof
Czarnecki.2011. ReverseEngineeringFeatureModels.In Proc.Int.Conf.Software
Engineering (ICSE) . ACM, 461–470.
[31]NorbertSiegmund,AlexanderGrebhahn,SvenApel,andChristianKästner.2015.
Performance-Influence Models for Highly Configurable Systems. In Proc. Europ.
SoftwareEngineeringConf./FoundationsofSoftwareEngineering(ESEC/FSE) .ACM,
284–294.
[32]NorbertSiegmund,SergiyKolesnikov,ChristianKästner,SvenApel,DonBatory,
Marko Rosenmüller, and Gunter Saake. 2012. Predicting Performance via Au-
tomatedFeature-InteractionDetection.In Proc.Int.Conf.SoftwareEngineering
(ICSE). IEEE, 167–177.
[33]Norbert Siegmund, Marko Rosenmüller, Christian Kästner, Paolo Giarrusso,
SvenApel, andSergiyKolesnikov.2013. ScalablePrediction ofNon-functional
Properties in Software Product Lines: Footprint and Memory Consumption.
Information and Software Technology 55, 3 (2013), 491–507.
[34]Norbert Siegmund, Alexander von Rhein, and Sven Apel. 2013. Family-Based
Performance Measurement. In Proc. Int. Conf. Generative Programming and Com-
ponent Engineering (GPCE) . ACM, 95–104.
[35]Bernard W. Silverman. 1998. Density Estimation for Statisticsand Data Analysis .
Chapman and Hall/CRC.
[36]N.Smirnov.1948. TableforEstimatingtheGoodnessofFitofEmpiricalDistri-
butions.The Annals of Mathematical Statistics 19, 2 (1948), 279–281.
[37]Matej Črepinšek, Shih-Hsi Liu, and Marjan Mernik. 2014. Replication and Com-
parison of Computational Experiments in Applied Evolutionary Computing:
Common Pitfalls and Guidelines to Avoid Them. Applied Soft Computing 19
(2014), 161–170.
[38]Nathan Weston, Ruzanna Chitchyan, and Awais Rashid. 2009. A Framework for
Constructing Semantically Composable Feature Models from Natural Language
Requirements. In Proc. Int. Software Product Line Conference (SPLC) . Carnegie
Mellon University, 211–220.
[39]ErikWitternandChristianZirpins.2014. ServiceFeatureModeling:Modeling
and Participatory Ranking of Service Design Alternatives. Software and Systems
Modeling 15, 2 (2014), 1–26.
278