 
 
 
 
 
Edinburgh Research Explorer  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Suggesting Accurate Method and Class Names
Citation for published version:
Allamanis, M, Barr, ET, Bird, C & Sutton, C 2015, Suggesting Accurate Method and Class Names. in
Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering. ACM, pp. 38-49.
https://doi.org/10.1145/2786805.2786849
Digital Object Identifier (DOI):
10.1145/2786805.2786849
Link:
Link to publication record in Edinburgh Research Explorer
Document Version:
Peer reviewed version
Published In:
Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering
General rights
Copyright for the publications made accessible via the Edinburgh Research Explorer is retained by the author(s)
and / or other copyright owners and it is a condition of accessing these publications that users recognise and
abide by the legal requirements associated with these rights.
Take down policy
The University of Edinburgh has made every reasonable effort to ensure that Edinburgh Research Explorer
content complies with UK legislation. If you believe that the public display of this file breaches copyright please
contact openaccess@ed.ac.uk providing details, and we will remove access to the work immediately and
investigate your claim.
Download date: 11. Aug. 2025Suggesting Accurate Method and Class Names
Miltiadis Allamanis‚Ä† Earl T. Barr‚Ä° Christian Bird? Charles Sutton‚Ä†
‚Ä†School of Informatics‚Ä°Dept. of Computer Science?Microsoft Research
University of Edinburgh University College London Microsoft
Edinburgh, EH8 9AB, UK London, UK Redmond, WA, USA
{m.allamanis, csutton}@ed.ac.uk e.barr@ucl.ac.uk cbird@microsoft.com
ABSTRACT
Descriptive names are a vital part of readable, and hence maintain-
able, code. Recent progress on automatically suggesting names for
local variables tantalizes with the prospect of replicating that success
with method and class names. However, suggesting names for meth-
ods and classes is much more difÔ¨Åcult. This is because good method
and class names need to be functionally descriptive, but suggesting
such names requires that the model goes beyond local context. We
introduce a neural probabilistic language model for source code
that is speciÔ¨Åcally designed for the method naming problem. Our
model learns which names are semantically similar by assigning
them to locations, called embeddings , in a high-dimensional contin-
uous space, in such a way that names with similar embeddings tend
to be used in similar contexts. These embeddings seem to contain
semantic information about tokens, even though they are learned
only from statistical co-occurrences of tokens. Furthermore, we
introduce a variant of our model that is, to our knowledge, the Ô¨Årst
that can propose neologisms, names that have not appeared in the
training corpus. We obtain state of the art results on the method,
class, and even the simpler variable naming tasks. More broadly,
the continuous embeddings that are learned by our model have the
potential for wide application within software engineering.
Categories and Subject Descriptors:
D.2.3 [Software Engineering]: Coding Tools and Techniques
General Terms: Algorithms
Keywords: Coding conventions, naturalness of software
‚ÄúYou shall know a word by the company it keeps.‚Äù ‚ÄîJ. R. Firth
1. INTRODUCTION
Language starts with names. While programming, developers
must name variables, parameters, functions, classes, and Ô¨Åles. They
strive to choose names that are meaningful and conventional, i.e.
consistent with other names used in related contexts in their code
base. Indeed, leading industrial experts, including Beck [9], Mc-
Connell [34], and Martin [33], have stressed the importance of
identiÔ¨Åer naming in software. Finding good names for program-
ming language constructs is difÔ¨Åcult; poor names make code harder
to understand and maintain [29, 50, 30, 7]. Empirical evidence
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proÔ¨Åt or commercial advantage and that copies
bear this notice and the full citation on the Ô¨Årst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciÔ¨Åc
permission and/or a fee.
FSE ‚Äô15, August 30 ‚ÄìSeptember 4, 2015, Bergamo, Italy
Copyright 2015 ACM X-XXXXX-XX-X/XX/XX ...$15.00.suggests that poor names also lead to software defects [13, 1]. Code
maintenance exacerbates the difÔ¨Åculty of Ô¨Ånding good names, be-
cause the appropriateness of a name changes over time: an excellent
choice, at the time a construct is introduced, can degrade into a poor
name, as when a variable is used in new context or a function‚Äôs
semantics changes.
Names of methods and classes are particularly important, and
can be difÔ¨Åcult to choose. H√∏st et al. eloquently captured their
importance: ‚ÄúMethods are the smallest named units of aggregated
behavior in most conventional programming languages and hence
the cornerstone of abstraction‚Äù [26]. Semantically distinct method
names are the basic tools for reasoning about program behaviour.
Programmers directly think in terms of these names and their com-
positions, since a programmer chose them for the units into which
the programmer decomposed a problem. Moreover, method names
can be hard to change, especially when they are used in an API.
When published in a popular library, method naming decisions are
especially rigid and poor names can doom a project to irrelevance.
In this paper, we suggest that modern statistical tools allow us
to automatically suggest descriptive, idiomatic method and class
names to programmers. We tackle the method naming problem:
the problem of inferring a method‚Äôs name from its body (or a class
from its methods). As developers spend approximately half of their
development time trying to understand and comprehend code during
maintenance alone [17], any progress toward solving the method
naming problem will improve the comprehensibility of code [49]
leading to an increase programmer productivity [24].
In previous work, we introduced the NATURALIZE framework [2],
which learns the coding conventions used in a code base and tackles
one naming problem programmers face ‚Äî that of naming variables
‚Äî by exploiting the ‚Äúnaturalness‚Äù or predictability of code [25].
However, the method naming problem is much more difÔ¨Åcult than
the variable naming problem, because the appropriateness of method
and class names depends not solely on their uses but also on their
internal structure ‚Äî their body or their set of methods. An adequate
name must describe not just what the method is, but what it does.
Variable names, by contrast, can often be predicted solely from a few
tokens of local context; for example, it is easy to predict the variable
name that follows the tokens for ( int . Because method and class
names must be functionally descriptive, they often have rich internal
structure: method names are often verb phrases and class names are
often noun phrases. But this means that method and class names
are often neologisms , that is, names not seen in the training corpus.
Existing probabilistic models of source code, including the n-gram
models used in NATURALIZE , cannot suggest neologisms. These
aspects of the method naming problem severely exacerbate the data
sparsity problem faced by all probabilistic language models, because
addressing them by building models that consider more context
necessarily means that any individual context will be observed less
often. Therefore, the method naming problem requires models that1 private void createDefaultShader () {
2 String vertexShader = "literal_1";
3 String fragmentShader = "literal_2";
4 shader = new ShaderProgram(vertexShader,
5 fragmentShader);
6 if(shader.isCompiled() == false)
7 throw new IllegalArgumentException(
8 "literal_3" + shader.getLog());
9 }
Figure 1: A method from libgdx ‚ÄôsCameraGroupStrategy . A
programmer named it; automatically naming it requires invent-
ing a neologism, a very hard inference problem. Our subtoken
model understands that its name should start with create and
suggests createShaders .
can better exploit the structure of code, taking into account long
range dependencies and modeling the context surrounding their
deÔ¨Ånitions more precisely than at the token-level, while minimizing
the effects of data sparsity.
This paper tackles the method naming problem with a novel,
neural logbilinear context model for code, inspired by neural prob-
abilistic language models for natural language, which have seen
many recent successes [37, 28, 35, 31]. A particularly impressive
success of these models has been that they assign words to con-
tinuous vectors that support analogical reasoning. For example,
vector(‚Äôking‚Äô) - vector(‚Äôman‚Äô) + vector(‚Äôwoman‚Äô) results in a vector
close to vector(‚Äôqueen‚Äô) [35, 36]. Although many of the basic ideas
have a long history [10], this class of model is receiving increas-
ing recent interest because of increased computational power from
GPUs and because of more efÔ¨Åcient learning algorithms such as
noise contrastive estimation [21, 39].
Intuitively, our model assigns to every identiÔ¨Åer name used in a
project a continuous vector in a high dimensional space, in such a
way that identiÔ¨Åers with similar vectors, or ‚Äúembeddings‚Äù, tend to
appear in similar contexts. Then, to name a method (or a class), we
select the name that is most similar in this embedding space to those
in the function body. In this way, our model realizes Firth‚Äôs famous
dictum, ‚ÄúYou shall know a word by the company it keeps‚Äù. This
slogan encapsulates the distributional hypothesis , that semantically
similar words tend to co-occur with the same other words. Two
words are distributionally similar if they have similar distributions
over surrounding words. For example, even if the words ‚Äúhot‚Äù and
‚Äúcold‚Äù never appear in the same sentence, they will be distribution-
ally similar if they both often co-occur with words like ‚Äúweather‚Äù
and ‚Äútea‚Äù. The distributional hypothesis is a cornerstone of much
work in computational linguistics, but we are unaware of previous
work that explores whether this hypothesis holds in source code.
Earlier work on the naturalness of code [25] found that code tends
to repeat constructs and exploited this repetition for prediction, but
did not consider the semantics of tokens. In contrast, the distribu-
tional hypothesis states that you shall recognize semantically similar
tokens because they tend also to be distributionally similar.
Indeed, we qualitatively show in Section 4 that our context model
produces embeddings that demonstrate implicit semantic knowledge
about the similarity of identiÔ¨Åers. For instance, it successfully dis-
tinguishes getters and setters, assigns function names with similar
functionality (like grow andresize ) to similar locations, and dis-
covers matching components of names, which we call subtokens ,
likeminandmax, and height andwidth .
Furthermore, to allow us to suggest neologisms, we introduce
a new subtoken context model that exploits the internal structure
of identiÔ¨Åer names. In this model, we predict names by breaking
them into parts, which we call subtokens, such as get,create , and
Height , and then predicting names one subtoken at a time. The
subtoken model automatically infers conventions about the internal
structure of variable names, such as ‚Äúan interface starts with an I‚Äù,
or ‚Äúan abstract class starts with Abstract ‚Äù. Our subtoken model
also learns conventions like preÔ¨Åxing names of boolean methodswith isorhas. This model also allows us to propose neologisms,
by proposing sequences of subtokens that have not been seen before.
Consider Figure 1; our subtoken model builds and explores an
embedding space that allows it to suggest createShaders , which
is usefully close to the name a programmer actually chose.
Our contributions follow:
We introduce a log-bilinear neural network to model code
contexts that, unlike standard language models in NLP, inte-
grates information from preceding, succeeding, and non-local
tokens.
We are the Ô¨Årst to apply a neural context model to the method
naming problem; and
We demonstrate that our models can accurately suggest names:
for the simpler variable naming problem, they improve on the
state of the art, and for class and method naming, our best
model achieves F1 scores of 60% on method names and 55%
on class names, when required to predict names for 20% of
method and class declarations. Additionally, our subtoken
model, that can suggest previously unseen names, achieves
an F1 of 50% when required to suggest names for 50% of the
classes.
Example Suggestions To illustrate our model‚Äôs capabilities, we
present a few examples of names suggested by the model (for quan-
titative results, see Section 5). When evaluated on libgdx , a graphics
library for Android, and asked to suggest a name for the variable
that programmers had named isLooping , although its conÔ¨Ådence
was low, our model has learned that the name should start with is.
For multipart method names like getPersistentManifoldPool ,
it understood getwas a likely preÔ¨Åx, suggesting it with 38% conÔ¨Å-
dence and that Manifold was important, assigning its inclusion a
probability of 28%, and even included getManifoldPool among
its top Ô¨Åve suggestions. On shorter agglutinations, like setPad ,
it performed better: all Ô¨Åve top-ranked suggestions started with
set, four of its suggestions included the root Pad, and it ranked
setPad , the actual name, third. Its handling of class names was
similar. It learned that the name of an exception class should end
with Exception and inferred that the names of Action and Test sub-
classes should end in Action andTest . A particularly interesting
suggestion our model made that caught our eye was AndroidAudio
for the class AndroidMusic .
Use Cases Our suggestion model can be embedded within a variety
of tools to support code development and code review. During
development, suppose that the developer is adding a method or a
class to an existing project. After writing the body, the developer
may be unsure if the name she chose is descriptive and conventional
within the project. Our model suggests alternative names from
patterns it learned from other methods in the project. During code
review, our model can highlight those names to which our model
assigns a low score. In either case, the system has two phases: a
training phase, which takes as input a training set of source Ô¨Åles
(e.g.the current revision of the project) and returns a neural network
model that can suggest names; and a testing or deployment phase, in
which the input is a trained neural network and the source code of a
method or class, and the output is a ranked list of suggested names.
Any suggestion system has the potential to suffer from what we
have called the ‚ÄúClippy effect‚Äù [2], in which too many low quality
suggestions alienate the user. To prevent this, our suggestion model
also returns a numeric score that reÔ¨Çects its degree of conÔ¨Ådence in
its suggestion; practical tools would only make a suggestion to the
user if the conÔ¨Ådence were sufÔ¨Åciently high.
2. NEURAL CONTEXT MODELS OF CODE
In this section, we introduce four language models of code, start-
ing with the n-gram model to build intuition. Then we introduceneural probabilistic language modelling and follow it with two novel
models that, speciÔ¨Åcally designed for method naming, reÔ¨Åne the
underlying neural model: our logbilinear context model, which
adds context and features, and subtoken context model, which adds
subtokens and can be used to generate neologisms.
Language models (LM) are probability distributions over strings
of a language. These models assume that we are trying to predict
a token tgiven a sequence of other tokens c= (c0;c1;:::cN)that
we call the context . LMs are very general; for example, if the goal
is to sequentially predict every token in a Ô¨Åle, as a n-gram model
does, then we can take t=ymandc= (ym n+1ym n+2:::ym 1).
Alternately, for the method naming problem, we can take tto be the
identiÔ¨Åer token in the declaration that names the function, and cto
be a sequence that contains all identiÔ¨Åers in the function body. Obvi-
ously, we cannot store a probability value for every possible context,
so we must make simplifying assumptions to make the modeling
tractable. Different LMs make different simplifying assumptions.
2.1 Background
To build intuition, we begin by reviewing the n-gram LM, which
is a standard technique in NLP and speech processing, and which
has become increasingly popular in software engineering [25, 41,
2]. The n-gram model assumes that all of the information required
to predict the next token is contained within the previous n 1
tokens i.e. P (y1:::yM) =√ïM
m=1P(ymjym 1:::ym n+1). To specify
this model we need (in principle) a table of Vnnumbers, where V
is the number of possible lexemes, that speciÔ¨Åes the conditional
probabilities for each possible n-gram. These are the parameters of
the model that we learn from data.
There is a large literature on methods for training these mod-
els [16], which basically revolve around counting the proportion of
times that token ymfollows ym 1:::ym n+1. However, even when
n=4orn=5, we cannot expect to estimate the counts of all n-
grams reliably, as the number of possible n-grams is exponential in
n. Therefore, smoothing methods are employed, which generally
modify the count of a rare n-gram y1:::ynto make it more similar
to the count of a shorter sufÔ¨Åx y2:::yn, whose frequency we can
estimate more reliably. This procedure involves the implicit assump-
tion that two contexts are most similar if they share a long sufÔ¨Åx.
But this assumption does not always hold. Many similar contexts,
such as x + y versus x + z , might be treated very differently by a
n-gram model, because the Ô¨Ånal token is different.
Logbilinear models Neural LMs [10] address the challenge that
the simple n-gram model has by making similar predictions for
similar contexts. They predict the next token ymusing a neural
network that takes the previous tokens as input. This allows the
network to Ô¨Çexibly learn which tokens, like int, provide much
information about the immediately following token, and which
tokens, like the semicolon ‚Äô ;‚Äô, provide very little. Unlike an n-gram
model, a neural LM makes it easy to add general long-distance
features of the context into the prediction ‚Äî we simply add them
as additional inputs to the neural net. In our work, we focus on a
simple type of neural LM that has been effective in practice, namely,
the log-bilinear LM [37] (LBL). We start with a general treatment
of loglinear models considering models of the form
P(tjc) =exp(sq(t;c))
√•t0exp(sq(t0;c)): (1)
Intuitively, sqis a function that indicates how much the model likes
to see both tandctogether, the expfunction maps this to be always
positive, and the denominator ensures that the result is a probability
distribution. This choice is very general. For example, if sqis a
linear function of the features in c, then the discriminative model is
simply a logistic regression.
Logbilinear models learn a map from every possible target tto avector qt2RD, and from each context cto a vector ÀÜrc2RD. We
interpret these as locations of each context and each target lexeme in
aDdimensional space; these locations are called embeddings . The
model predicts that the token tis more likely to appear in context c
if the embedding qtof the token is similar to that ÀÜrcof the context.
To encode this in the model, we choose
sq(t;c) =ÀÜr>
cqt+bt; (2)
where btis a scalar bias which represents how commonly toccurs
regardless of the context. To understand this equation intuitively,
note that, if the vectors ÀÜrcandqthave norm 1, then their dot product
is simply the cosine of the angle between them. So sq, and hence
p(tjc);is larger if either vector has a large norm, if btis large, or if
ÀÜrcandqthave a small angle between them, that is, if they are more
similar according to the commonly used cosine similarity metric.
To complete this description, we deÔ¨Åne the maps t7!qtand
c7!ÀÜrc. For the targets t, the most common choice is to simply
include the vector qtfor every tas a parameter of the model. That
is, the training procedure has the freedom to learn an arbitrary map
between tandqt. For the contexts c, this choice is not possible, as
there are too many possible contexts. Instead, a common choice
[31, 39] is to represent the embedding ÀÜrcof a context as the sum of
embeddings of the tokens within it, that is,
ÀÜrc=jCj
√•
t=1Ctrct; (3)
where rct2RDis a vector for each lexeme that is included in the
model parameters. The variable tindexes every token in the context
c, so if the same lexeme occurs multiple times in c, then it appears
multiple times in the sum. The matrix Ctis a diagonal matrix that
serves as a scaling factor depending on the position of a lexeme
within the context. This allows, for example, a lexeme‚Äôs inÔ¨Çuence
onc‚Äôs position to depend on how close it is to the target. The D
non-zero values in Ctfor each tare also included in the model
parameters. Each lexeme vhas two embeddings: an embedding
qvfor when it is used as a target and an embedding rvfor when it
appears in the context.
To summarize, logbilinear models make the assumption that every
token and every context can be mapped in a D-dimensional space.
There are two kinds of embedding vectors: those directly learned
(i.e. the parameters of the model) and those computed from the
parameters of the model. To indicate this distinction, we place a
hat on ÀÜrcto indicate that it is computed from the model parameters,
whereas we write qtwithout a hat to indicate that it is a parameter
vector that is learned directly by the training procedure. These mod-
els can also be viewed as a three-layer neural network, in which the
input layer encodes all of the lexemes in cusing a 1-of- Vencoding,
the hidden layer outputs the vectors rctfor each token in the context,
and the output layer computes the score functions sq(t;c)and passes
them to a softmax nonlinearity. For details on the neural network
representation, see Bengio et al. [10].
To learn these parameters, it has recently been shown [39, 38]
that an alternative to the maximum likelihood method called noise
contrastive estimation (NCE) [21] is effective. NCE measures how
well the model p(tjc)can distinguish the real data in the training set
from ‚Äúfantasy data‚Äù that is generated from a simple noise distribution.
At a high level, this can be viewed as a black box alternative to
maximum likelihood that measures how well the model Ô¨Åts the
training data. We optimize the model parameters using stochastic
gradient descent. We employ NCE for all models in this paper.
2.2 Logbilinear Context Models of Code
Now we present a new neural network, a novel LBL LM for
code, which we call a logbilinear context model . The key ideaVariable: isDone
qisDone
ÀÜrcontextRD
sq(:) =ÀÜr>
context qisDone +bisDoneFeatures :
boolean ,in:MethodBody ,final
rboolean +rin:MethodBody +rfinal
ÀÜrcontext =√•
f2Ftcrf+1
jItj√•
i2It√•
8k:Kjkj>0Ckrti+kContexts :
final boolean isDone =false ;
C 2rfinal +C 1rboolean +C1r=+C2rfalse
while (!isDone ){
C 2r(+C 1r!+C1r)+C2r}
Figure 2: Visual explanation of the representation and computation of context in the D-dimensional space as deÔ¨Åned in Equation 4;
the Ô¨Ånal paragraph of Section 2.2 explains the sum over the Itlocations. Each source code token and feature maps to a learned D-
dimensional vector in continuous space. The token-vectors are multiplied with the position-dependent context matrix Ciand summed,
then added to the sum of all the feature-vectors. The resulting vector is the D-dimensional representation of the current source code
identiÔ¨Åer. Finally, the inner product of the context and the identiÔ¨Åer vectors is added to a scalar bias b, producing a score for each
identiÔ¨Åer. This neural network is implemented by mapping its equations into code.
is that logbilinear models make it especially easy to exploit long-
distance information; e.g.when predicting the name of a method, it
is useful to take into account all of the identiÔ¨Åers that appear in the
method body. We model long-distance context via a set of feature
functions , such as ‚ÄúWhether any variable in the current method is
named addCount ‚Äù, ‚ÄúWhether the return type of the current method
isint,‚Äù and so on. The logbilinear context model combines these
features with the local context.
As before, suppose that we are trying to predict a code token t
given a sequence of context tokens c= (c0;c1;:::; cN). We assume
thatccontains all of the other tokens in the Ô¨Åle that are relevant for
predicting t;e.g.tokens from the body of the method that tnames.
The tokens in cthat are nearest to the target tare treated specially.
Suppose that toccurs in position iof the Ô¨Åle, that is, if the Ô¨Åle is the
token sequence t1;t2;:::, then t=ti. Then the local context is the
set of tokens that occur within Kpositions of t, that is, the setfti+kg
for KkK;k6=0. The local context includes tokens that occur
both before and after t.
The overall form of the context model will follow the generic
form in (1)and(2), except that the context representation ÀÜrcis
deÔ¨Åned differently. In the context model, we deÔ¨Åne ÀÜrcusing two
different types of context: local and global. First, the local context is
handled in a very similar way to the logbilinear LM. Each possible
lexeme vis assigned to a vector rv2RD, and, for each token tkthat
occurs within Ktokens of tin the Ô¨Åle, we add its representation rtk
into the context representation.
The global context is handled using a set of features . Each feature
is a binary function based on the context tokens c, such as the
examples described at the beginning of this section. Formally, each
feature fmaps a cvalue to either 0or1. Maddison and Tarlow [31]
use a similar idea to represent features of a syntactic context, that is,
a node in an AST. Here, we extend this idea to incorporate arbitrary
features of long-distance context tokens c. The Ô¨Årst column of
Table 4 presents the full list of features that we use in this work.
To learn an embedding, we assign each feature function to a single
vector in the continuous space, in the same way as we did for tokens.
Mathematically, let Fbe the set of all features in the model, and let
Fc;for a context c, be the set of all features fwith f(c) =1. Then
for each feature f2F, we learn an embedding rf2RD;which is
included as a parameter to the model in exactly the same way that
rtwas for the language modeling case.
Now, we can formally deÔ¨Åne a context model of code as a prob-
ability distribution P(tjc)that follows the form (1)and(2), where
ÀÜrc=ÀÜrcontext , where ÀÜrcontext is
ÀÜrcontext =√•
f2Ftcrf+√•
8k:Kjkj>0Ckrti+k; (4)
where, as before, Ckis a position-dependent DDdiagonal contextmatrix that is also learned during training1. Intuitively, this equation
sums the embeddings of each token tkthat occurs near tin the Ô¨Åle,
and sums the embeddings of each feature function fthat returns
true ( i.e., 1) for the context c. Once we have this vector ÀÜrcontext , just
as before, we can select a token tsuch that the probability P(tjc)
is high, which happens exactly when ÀÜr>context qtis high ‚Äî in other
words, when the embedding qtof the proposed target tis close to
the embedding ÀÜrcontext of the context.
Figure 2 gives a visual explanation of the probabilistic model.
This Ô¨Ågure depicts how the model assigns probability to the token
isDone if the preceding two tokens are final boolean and the
succeeding two are = false . Reading from right to left, the Ô¨Ågure
describes how the continuous embedding of the context is computed.
Following the dashed (pink) arrows, the tokens in the local context
are each assigned to D-dimensional vectors rfinal ,rboolean , and so
on, which are added together (after multiplication by the C kmatri-
ces that model the effect of distance), to obtain the effect of the local
context on the embedding ÀÜrcontext . The solid (blue) arrows represent
the global context, pointing from the names of the feature func-
tions that return true to the continuous embeddings of those features.
Adding the feature embeddings to the local context embeddings
yields the Ô¨Ånal context embedding ÀÜrcontext . The similarity between
this vector and embedding of the target vector qisDone is computed
using a dot product, which yields the value of sq(isDone ;c)which
is necessary for computing the probability P(isDonejc)via (1).
Multiple Target Tokens Up to now, we have presented the model
in the case where we are renaming a target token tthat occurs at
only one location, such as the name of a method. Other cases,
such as when suggesting variable names, require taking all of the
occurrences of a name into account [2]. When a token tappears at a
set of locations It, we compute the context vectors ÀÜrcontext separately
for each token ti, for i2It, then average them. When we do this, we
carefully rename all occurrences of tto a special token called SELF
to remove tfrom its own context.
2.3 Subtoken Context Models of Code
A limitation of all of the previous models is that they are unable
to predict neologisms , that is, unseen identiÔ¨Åer names that have not
been used in the training set. The reason for this is that we allow the
map from a lexeme vto its embedding qvto be arbitrary ( i.e.without
learning a functional form for the relationship), so we have no basis
to assign continuous vectors to identiÔ¨Åer names that have not been
observed. In this section, we sidestep this problem by exploiting
the internal structure of identiÔ¨Åer names, resulting in a new model
which we call a subtoken context model .
1Note that kcan be positive or negative, so that in general C 26=C2.The subtoken context model exploits the fact that identiÔ¨Åer names
are often formed by concatenating words in a phrase, such as
getLocation orsetContentLengthHeader . We call each of the
smaller words in an identiÔ¨Åer a subtoken . We split identiÔ¨Åer names
into subtokens based on camel case and underscores, resulting in a
set of subtokens that we use to compose new identiÔ¨Åers. To do this,
we exploit the summation trick we used in ÀÜrcontext . Recall that we
constructed this vector as a sum of embedding vectors for particular
features in the context. Here, we deÔ¨Åne the embedding of a target
vector to be the sum of the embeddings of its subtokens.
Lettbe the token that we are trying to predict from a context
c. As in the context model, ccan contain tokens before and after
t, and tokens from the global context. In the subtoken model, we
additionally suppose that tis split up into a sequence of Msubto-
kens, that is, t=s1s2:::sM, where sMis always a special END
subtoken that signiÔ¨Åes the end of the subtoken sequence. That is,
the context model now needs to predict a sequence of subtokens
in order to predict a full identiÔ¨Åer. We begin by breaking up the
prediction one subtoken at a time, using the chain rule of probabil-
ity:P(s1s2:::sMjc) =√ïM
m=1P(smjs1:::sm 1;c):Then, we model
the probability P(smjs1:::sm 1;c)of the next subtoken smgiven all
of the previous ones and the context. Since preliminary experiments
with an n-gram version of a subtoken model showed that n-grams
did not yield good results, we employ a logbilinear model
P(smjs1:::sm 1;c) =expfsq(sm;s1:::sm 1;c)g
√•s0expfsq(sm;s1:::sm 1;c)g:(5)
As before, sq(sm;s1:::sm 1;c)can be interpreted as a score, which
can be positive or negative and indicates how much the model ‚Äúlikes‚Äù
to see the subtoken sm, given the previous subtokens and the context.
The exponential functions and the denominator are a mathematical
device to convert the score into a probability distribution.
We choose a bilinear form for sq, with the difference being that
in addition to tokens having embedding vectors, subtokens have
embeddings as well. Mathematically, we deÔ¨Åne the score as
sq(sm;s1:::sm 1;c) =ÀÜr>
SUBCqsm+bsm; (6)
where qsm2RDis an embedding for the subtoken sm, and ÀÜrSUBCis
a continuous vector that represents the previous subtokens and the
context. To deÔ¨Åne a continuous representation ÀÜrSUBCof the context,
we break this down further into a sum of other embedding features
as
ÀÜrSUBC=ÀÜrcontext +ÀÜrSUBC-TOK: (7)
In other words, the continuous representation of the context breaks
down into a sum of two vectors: the Ô¨Årst term ÀÜrcontext represents the
effect of the surrounding tokens c‚Äî both local and global ‚Äî and is
deÔ¨Åned exactly as in the context model via (4).
The new aspect is how we model the effect of the previous subto-
kens s1:::sm 1in the second term ÀÜrSUBC-TOK. We handle this by
assigning each subtoken sa second embedding vector rs2RDthat
represents its inÔ¨Çuence when used as a previous subtoken; we call
this a history embedding . We weight these vectors by a diagonal
matrix CSUBC
 k, to allow the model to learn that subtokens have decay-
ing inÔ¨Çuence the farther that they are from the token that is being
predicted. Putting this all together, we deÔ¨Åne
ÀÜrSUBC-TOK=M
√•
i=1CSUBC
 irsm i: (8)
This completes the deÔ¨Ånition of the subtoken context model. To
sum up, the parameters of the subtoken context model are (a) the
target embeddings qsfor each subtoken sthat occurs in the data,
(b) the history embeddings rsfor each subtoken s, (c) the diagonal
weight matrices CSUBC mform=1;2;:::; Mthat represent the effect ofdistance on the subtoken history (we use M=3, yielding a 4-gram-
like model on subtokens) and the parameters that we carried over
from the logbilinear context model: (d) the local context embeddings
rtfor each token tthat appears in the context, (e) the local context
weight matrices C kandCkfor KkK;k6=0, and (f) the
feature embeddings rffor each feature f(c)of the global context.
We estimate all of these parameters from the training corpus.
Although this may seem a large number of parameters, this is
typical for language models, e.g., consider the V5parameters, if Vis
the number of lexemes, required by a 5-gram language model. How
can we handle so many parameters? The reason is simple: in the era
of vast, publicly available source code repositories like GitHub and
Bitbucket, code scarcity is a thing of the past.
Generating Neologisms A Ô¨Ånal question is ‚ÄúGiven the context c,
how do we Ô¨Ånd the lexeme tthat maximizes P(tjc)?‚Äù. Previous mod-
els could answer this question simply by looping over all possible
lexemes in the model, but this is impossible for a subtoken model,
because there are inÔ¨Ånitely many possible neologisms. So we em-
ploy beam search (see Russell and Norvig [44] for details) to Ô¨Ånd
theBtokens (i.e., subtoken sequences) with the highest probability.
2.4 Source Code Features for Context Models
In this section, we describe the features we use to capture global
context. Identifying software measures and features that effectively
capture semantic properties like comprehensibility or bug-proneness
is a seminal software engineering problem that we do not tackle in
this paper. Here, we have selected measures and features heavily
used in the literature and industry. For instance, control Ô¨Çow is
indisputably important; we selected Cyclomatic complexity, despite
its correlation with code size, to measure it. The Ô¨Årst column of
Table 4 deÔ¨Ånes the features we used in this work. In the table,
‚ÄúVariable Type‚Äù tracks whether the type is generic, its type after
erasure, and, if the type is an array, its size. ‚ÄúContained Methods‚Äù
and ‚ÄúSibling Methods‚Äù exclude method overloads and recursion.
The features of a target token are its target features ; we assign a
rfvector to each of them; this vector is added in the left summation
of Equation 4 if a feature‚Äôs indicator function freturns 1 for a
particular token. Although features are binary, we describe some ‚Äî
like the modiÔ¨Åers of a declaration, the node type of a AST, etc. ‚Äî as
categorical. All categorical features are converted into binary using
a 1-of-K encoding. For methods, we include Cyclomatic complexity,
clipping it to 10 and treating it as categorical. When features do not
make sense for a particular token, like the Cyclomatic complexity
of a variable, the feature‚Äôs function simply returns zero.
3. METHODOLOGY
The core challenge of solving the method naming problem from
code is data sparsity. Our guiding intuition is that source code
contains rich structure that can alleviate the sparsity problem. We
therefore pose the following question: How can we better maximally
exploit the structure inherent to source code? This question in turn
leads us to the research questions:
RQ1 . Can we identify and extract long and short-range context
features of identiÔ¨Åers for naming?
RQ2 . Do identiÔ¨Åers contain exploitable substructure?
Answering both of these questions in the afÔ¨Årmative, we turn
our attention to exploiting the resulting naming information; here,
we ask if this new information is sufÔ¨Åciently rich to allow us to
accurately suggest names. More concretely:
RQ3 . Can we accurately suggest method declaration names, looking
only at the context of the declared method?
RQ4 . Can we do the same for class ( i.e.type) names?Table 1: Evaluation projects (Java). Ordered by popularity.
Name Git SHA Description
elasticsearch d3e10f9 REST Search Engine
Android-Universal-Image-Loader 19ce389 Android Library
spring-framework 2bf6b41 Application Framework
libgdx 789aa0b Game Dev Framework
storm bc54e8e Distributed Computation
zxing 71d8395 Barcode image processing
netty 3b1f15e Network App Framework
platform_frameworks_base f19176f Android Base Framework
bigbluebutton 02bc78c Web Conferencing
junit c730003 Testing Framework
rxjava cf5ae70 Reactive JVM extensions
retroÔ¨Åt afd00fd REST client
clojure f437b85 Programming Language
dropwizard 741a161 RESTful web server
okhttp 0a19746 HTTP+SPDY client
presto 6005478 Distributed SQL engine
metrics 4984fb6 Metrics Framework
spring-boot b542aa3 App Framework Wrapper
bukkit f210234 Mincraft Mod API
nokgiri a93cde6 HTML/XML/CSS parser
In reference to the Ô¨Årst two research questions we describe the
features that we use and how we capture substructure in the next
section. We then deÔ¨Ånitively answer these research questions by
comparing our approach with previous techniques for suggesting
variable names on a broad software corpus. There is little to no
research that tackles the second two research questions to compare
against. Nonetheless, we use an n-gram model as a point of compari-
son for naming methods and classes to demonstrate the performance
of our approach as that model has performed the best for variable
naming in the past and we hypothesize is reasonable for these new
naming tasks. The rest of this section describes our experimental
setup and methodology.
Data We picked the top active Java GitHub projects on January
22nd 2015. We obtained the most popular projects by taking the
sum of the z-scores of the number of watchers and forks of each
project, using the GitHub Archive. Starting from the top project,
we selected the top 20 projects excluding projects that were in a
domain that was previously selected. We also included only projects
with more than 50 collaborators and more than 500 commits. The
projects along with short descriptions are shown in Table 1. We
used this procedure to select a mature, active, and diverse corpus
with large development teams. Finally, we split the Ô¨Åles uniformly
into a training (70%) and a test (30%) set.
Methodology We train all models on the train sets formed over
the Ô¨Åles of each project. To evaluate the models, for each of the test
Ô¨Åles and for each variable (all identiÔ¨Åers that resolve to the same
symbol), method declaration or class declaration, we compute the
features and context of the location of the identiÔ¨Åer and ask the
model to predict the actual target token the developer used (which
is unknown to the model), as in Allamanis et al. [2]. In the use
cases we consider (see Section 1), models that are deployed with a
‚ÄúconÔ¨Ådence Ô¨Ålter‚Äù, that is, the model will only present a suggestion
to the user when the probability of the top ranked name is above
some threshold. This is to avoid annoying the user with low-quality
suggestions. To reÔ¨Çect this in our evaluation, we measure the degree
to which the quality of the results changes as a function of the
threshold. Rather than reporting the threshold, which is not directly
interpretable, we instead report the suggestion frequency , which is
the percentage of names in the test set for which the model decides
to make a prediction for a given threshold.
To measure the quality of a suggestion, we compute the F1 score
and the accuracy for the retrieval over the subtokens of each correct
token. Thus, all methods are given partial credit if the predicted
name is not an exact match but shares subtokens with the correct
name. F1 is the harmonic mean of precision and recall and is a
standard measure [32] because it conservative: as a harmonic mean,its value is inÔ¨Çuenced most by the lowest of precision and recall.
We also compute the accuracy of each prediction: a prediction is
correct when the model predicts exactly (exact match) the actual
token. When computing the F1 score for suggestion rank k>1, we
pick the precision, recall, and F1 of the rank lkthat results in the
highest F1 score.
Because this evaluation focuses on popular projects, the results
may not reÔ¨Çect performance on a low quality project in which many
names are poor. For such projects, we recommend training on a
different project that has high quality names, but leave evaluating
this approach to future work. Alternatively, one could argue that,
because we measure whether the model can reproduce existing
names, the evaluation is too harsh: if a predicted name does not
match the existing name, it could be equally good, or even an
improvement. Nonetheless, matching existing names in high quality
projects, as we do, still provides evidence of overall suggestion
quality, and, compared to a user study, an automatic evaluation has
the great advantage that it allows efÔ¨Åcient comparison of a larger
number of different methods.
Finally, during training, we substitute rare identiÔ¨Åers, subtokens
and features ( i.e.those seen less than two times in the training data)
with a special UNKtoken. During testing, when any of the models
suggests the UNKtoken, we do not make any suggestions; that is,
theUNKtoken indicates that the model expects a neologism that
it cannot predict. For the subtoken model, during testing, we may
produce suggestions that contain UNKsubtokens. In the unlikely
case that a context token ti+k=ti(i.e.is the same token), we replace
ti+kwith a special S ELFtoken. This makes sure that the context of
the model includes no information about the target token.
Training Parameters We used learning rate 0:07,D=50, mini-
batch size 100, dropout 20% [48], generated 10distractors for each
sample for each epoch and trained for a maximum of 25epochs
picking the parameters that achieved the maximum likelihood in a
held out validation set (the 10% of the training data). The context
size was set to K=6and subtoken context size was set to M=3.
Before the training started, parameters were initialized around 0
with uniform additive noise (scaled by 10 4). The bias parameters
bwere initialized such that P(tjc)matches the empirical (unigram)
distribution of the tokens (or subtokens for the subtoken model).
All the hyperparameters except for Dwere tuned using Bayesian
optimization on bigbluebutton for method declarations. The pa-
rameter Dis special in that as we increase it, the performance of
each model increases monotonically (assuming a good validation
set), with diminishing returns. Also, an increase in Dincreases the
computational complexity of training and testing each model. We
picked D=50that resulted in a good trade-off of the computational
complexity vs.performance.
4. IDENTIFIER REPRESENTATION
First we evaluate our model qualitatively, by visualizing its output.
All of the models that we have described assign tokens, features, and
subtokens to embeddings , which are locations in a D-dimensional
continuous space. These locations have been selected by the train-
ing procedure to explain statistical properties of tokens, but it does
not necessarily follow that the embeddings capture anything about
the semantics of names. To explore this question, we examine
qualitatively whether names that appear semantically similar to us
are assigned to similar embeddings, by visualizing the continuous
embeddings assigned to names from a few projects. This raises
the immediate difÔ¨Åculty of how to visualize vectors in a D=50
dimensional space. Fortunately, there is a rich literature in statistics
and machine learning about dimensionality reduction methods that
map high dimensional vectors to two-dimensional vectors while pre-
serving important properties of the original space.There are various
ideas behind such techniques, such as preserving distances or anglesFigure 3: A 2D non-linear projection, using t-SNE [51], of embeddings of method names appearing in method declarations in the
elasticsearch project. Similar methods have been grouped together, even though the model has no notion of the textual similarity of
the method names, for example, the assert -like methods on the left or the new array methods on the right.
between nearby points, or minimizing the distance between each
point and its image in the 2D space. Classical techniques for dimen-
sionality reduction include principal components analysis (PCA)
and multidimensional scaling. We will also employ a more modern
method called t-SNE [51].
Figure 3 displays the vectors assigned to a few method names
from a typical project ( elasticsearch ). Each point represents the
qvector of the indicated token. To interpret this, recall that the
model uses the qtvectors to predict whether token twill occur in
particular context. Therefore, tokens tandt0with similar vectors qt
andqt0are tokens that the model expects to occur in similar contexts.
These embeddings were generated from the logbilinear context
model ‚Äî that is, without using subtokens ‚Äî so the model has no
information about which tokens are textually similar. Rather, the
only information that the model can exploit is the contexts in which
the tokens are used. Despite this, we notice that many of the names
which are grouped together seem to have similar functions. For
example, there is a group of assertXXXX methods on the left hand
side. Especially striking is the clump of construction methods on the
right-hand side newDoubleArray ,newIntArray ,newLongArray ,
and so on. It is also telling that near this clump, the names grow and
resize are also close together. Analysis reveals that these names
do indeed seem to name methods of different classes that seem to
have similar functionality. Our previous work [2] indicates that
developers often prefer such entities to have consistent names.
Additionally we examine the nearest neighbors of tokens in the
D-dimensional space. This type of analysis avoids the risk, inherent
in any dimensionality reduction method, that important information
is lost in the projection from Ddimensions to 2D. Table 2 shows
some identiÔ¨Åers on a different project, clojure , for each identiÔ¨Åer
giving a list of other identiÔ¨Åers that are nearest in the continuous
space. The nearest neighbors of a token tare those tokens vsuch that
the inner product of the embeddings, that is, q>tqv;is maximized.
We choose this measure because it most closely matches the notion
of similarity in the model. Again, we are using the logbilinear
context model without subtoken information. We again see that
the nearest neighbors in the continuous space seem to have similar
semantic function such as the triple fieldName ,methodName , and
className or the names returnType ,typ, and type .
Table 3 takes this analysis a bit further for the subtoken model.
This table shows the ‚Äúnearest nearest neighbors‚Äù: those pairs of
tokens or subtokens that are closest in the embedding space out
of all possible pairs of tokens. On the left column, we see pairs
of close neighbors from the feature-based bilinear context model
without subtokens. These contain many similar pairs, such as width
andheight . It is striking how many of these pairs contain simi-
lar subtokens even though this model does not contain subtokens .
Moving to the subtoken model, the right column of Table 3 shows
pairs of subtokens that are closest in the embedding space. The
model learns that pairs like numerals, MinandMax, and HeightandWidth should be placed near to each other in the continuous
space. This is further evidence that the model is learning semantic
similarities given only statistical relationships between tokens.
We can also attempt to be a bit more speciÔ¨Åc in our analysis. In
this we are inspired by Mikolov et al. [35], who noticed that adding
together two of their embeddings of natural language words often
yielded a compositional semantics ‚Äî e.g.embedding(‚Äú Paris ‚Äù) - em-
bedding(‚Äú France ‚Äù) + embedding(‚Äú Vietnam ‚Äù) yielded a vector whose
nearest neighbor was the embedding of ‚Äú Hanoi ‚Äù. To attempt some-
thing similar for source code, we consider semantic relationships
thatpairs of identiÔ¨Åers have with each other.
For Figures 4 and 5, we project the D-dimensional embeddings
to 2D using PCA rather than t-SNE. Although a technical point, this
is important. Unlike t-SNE, PCA is a linear method, that is, the
mapping between the D-dimensional points and the 2D points is
linear. Therefore, if groups of points are separated by a plane in the
2D space, then we know that they are separated by a plane in the
higher-dimensional space as well. Figure 4 shows the embeddings
of all pairs of setter and getter methods for the project netty . The
subtoken model did not generate these models, so the model cannot
cluster these tokens based on textual similarity. Nevertheless, we
Ô¨Ånd that getter and setter tokens are reasonably well separated in
the continuous space, because they are used in similar contexts. In
Figure 5, we match pairs of variable names in the libgdx project
in which one name (the ‚Äúplural name‚Äù) equals another name (the
‚Äúsingular name‚Äù) plus the character s. The Java convention that
Collection objects are often named by plural variable names
motivates this choice. Although this mapping is more noisy than the
last, we still see that plural names tend to appear on the left side of
the Ô¨Ågure, and singular names on the right.
From this exploration we conclude that the continuous locations
of each name seem to be capturing semantic regularities. Readers
who wish to explore further can view the embeddings at http:
//groups.inf.ed.ac.uk/cup/naturalize/ .
Even though the continuous embeddings are learned from con-
text alone, these visualizations suggest that these embeddings also
contain, to some extent, semantic information about which iden-
tiÔ¨Åers are similar. This suggests that local and global context do
provide information that can be represented and exploited, that is,
semantically similar names are used in similar contexts. This is
evidence pointing towards an afÔ¨Årmative answer to RQ1 . It is espe-
cially striking that we have consistently found that nearby tokens
in the continuous space tend to share subtokens, even when the
model does not include subtoken information. The right column of
Table 3 reinforces this point since it shows that, when we do use the
subtoken model, nearby pairs of subtokens in the continuous space
seem to be meaningfully related. This provides some evidence for
an afÔ¨Årmative answer to RQ2 .
Finally, it can be objected that this type of analysis is necessarily
subjective. When backed and validated by quantitative analysis,get
setFloat
setByteset
setTimeMillis
setShortsetIntsetChar
setDoublesetLongsetAll
setBooleansetReceiveBufferSizegetOptiongetSoLinger
setSoLinger
setOptiongetReceiveBufferSize
setSendBufferSizegetSendBufferSize
setTrafÔ¨ÅcClassgetTrafÔ¨ÅcClass
setSoTimeoutsetBackloggetSoTimeout
getBytessetBytesgetBacklog
getDiscardThreshold
setDiscardThreshold
getValue
setValuegetLonggetInt
getBoolean
getByte
setMediumgetChar
getMedium
getDoublegetFloat
getShort
getRawResult
setRawResultgetTimeMillisgetAll
setCharsetgetCharset
setMaxSizegetMaxSizegetFilenamegetContentTypegetContentTransferEncoding
setContentTypesetFilename
setContentTransferEncodingFigure 4: A 2D linear projection, using PCA, of the embeddings
of setters and getters for netty method declarations. Matched
getter/setting pairs are connected with a dotted line. The em-
beddings seem to separate setters from the getters.
Table 2: Examples of nearest neighbors in the continuous space
for variable names in clojure . Ordered by higher inner product
q>t1qt2where t1is in the Ô¨Årst column and t2in the second.
IdentiÔ¨Åer Nearest Neighbors (ordered by distance)
fieldName className ,methodName ,target ,method ,
methods
returnType sb ,typ ,type ,methodName ,t
keyvals items ,seq ,form ,rest ,valOrNode
params paramType ,ctor ,methodName ,args ,arg
however, this analysis provides visual insight, gained from looking
at the embedding vectors. Thus, we complement this qualitative
analysis with a more quantitative one, in the next section.
5. EV ALUATION
In this section, we quantitatively evaluate the performance of the
neural model on the data set (Table 1) answering all the RQs.
Variable Naming Renaming variables and method invocations
has been previously shown [2] to achieve good performance using n-
gram LMs. Figure 6 shows the performance of the baseline n-gram
model along with the performance of the other neural models for
variable names. For low frequency of suggestions (high conÔ¨Ådence
decisions), the neural models overperform the n-gram-based sug-
gestions. This is expected since such models perform better than
plain n-gram models in NLP [39]. Additionally, the features give a
substantial performance increase over the models that lack features.
The subtoken model performs worse compared to the token-level
model for suggestion frequencies higher than 6%. This is to be
expected, since the subtoken model has to make a sequence of
increasingly uncertain decisions, predicting each subtoken sequen-
tially, increasing the possibility of making a mistake at some point.
For suggestion frequencies lower than 6% the performance of the
subtoken model is slightly better compared to the token-level model,
thanks to its ability to generate novel identiÔ¨Åers. Thus, we positively
answer RQ1 andRQ2 .
We computed Table 4 over only three classes because of the
cost of retraining the model one feature at a time. Looking at
Table 4 for variable names one may see how each feature affects
the performance of the models over the baseline neural model with
no features at rank k=52. First, we observe that the features
2We chose Ô¨Åve because subitizing, the ability to count at a glance,
actionvalue
values
pointsoffset
countÔ¨Çag
contactjoystickshader
texture
descriptor pathÔ¨Åle
parametername
regioncolor
argargs
vecarray
shape
pointresult
boundsbutton
actormodelid
nodemeshPartdisposable
materialattributes
testpositionconÔ¨Åg
hints
assetobjectlinebuffer
itemitemsmode
emitter
instanceinstancesassetslightsval
key
listenerlistenersrunnableÔ¨Ålesrunnablesrenderables
cache
celltilevectors
charactereffectcolors
row
emittersÔ¨Çags
handlevals
pagesglyphsparamsface
annotation
Ô¨Åeldannotationscaches
layers
controllers
spritesprites
trianglemodes
processorhintframe
touchEventskeyframes
constraintsimageregions
pagemethodtextures
controllerviewportsnames
viewporthandlescontent contacts
modelsconstraintframebuffer
paramlocation
pixelsunits
attachment
rectextensionframes
extensions
keysactorscolumns
columnpadshapesrows
results
vectorrenderableshadersattributeframebuffers
renderbuffers
buffers
positionsattachmentssamplers arrays
unitsampler
layer
nodescontents
descriptorsparts
methods
Ô¨Åeldsresourcetilesetsresourceslocations
objectscounts
buttons
keyEventsbit
effectsmeshPartskeyframe animation
materialspart animationsdisposablestiles
tokens
tokenpathsglyph
loaderpads
rects
keyEvent
touchEventparametersoffsets
testssectionvecs
sections
processors
loadersbound
imagesactions
lines
tilesetconÔ¨Ågs
pixel
lightjoystickstriangles
facesbits
charactersFigure 5: A 2D linear projection, using PCA, of the embeddings
of singular and plural names in libgdx . Pairs are connected
with a dotted line. The embeddings mostly separate singular
and plural names. We expect most of the plural variables to re-
fer to Collections of objects whose names appear in singular.
Table 3: Closely related (sub-)tokens for libgdx variables. The
top 10 pairs that have the highest q>t1qt2are shown. For the
subtoken model some numeral pairs ( e.g.9‚Äì8) are omitted.
Feature Model Subtoken
camera ‚Äìcam 6 ‚Äì5
padBottom ‚ÄìpadLeft Height ‚ÄìWidth
dataOut ‚ÄìdataIn swig ‚Äìclass
localAnchorA ‚ÄìlocalAnchorB Min ‚ÄìMax
bodyA ‚ÄìbodyB shape ‚Äìcollision
framebuffers ‚Äìbuffers Left ‚ÄìRight
worldWidth ‚ÄìworldHeight camera ‚Äìcam
padRight ‚ÄìpadLeft TOUCH ‚ÄìKEY
jarg7 ‚Äìjarg6_ end ‚Äìstart
spriteBatch ‚Äìbatch loc ‚Äìlocation
help mostly at high suggestion frequencies. This is due to the fact
that for high-conÔ¨Ådence (low suggestion frequency) decision the
models are already good at predicting those names. Additionally,
combining all the features yields a performance increase, suggesting
that for variable names, only the combination of the features gives
sufÔ¨Åciently better information about variable naming.
Method Declaration Naming Accuracy We now attempt to use
the neural model for suggesting method names, using only features
available during the declaration of a method. Surprisingly, the neural
model is exceptionally good at predicting method declaration names.
Figure 7a shows the performance of the models on all method
declarations excluding any method declarations that are method
overrides. We exclude overrides so as to avoid giving the models
credit for predicting easy names like toString . When we include
overrides, the performance of all models improves. To exclude
method overrides, we remove methods that contain the @Override
annotation as well as those methods that we can statically determine
as method overrides.
The graphs in Figures 7a show that the neural models are substan-
tially better at suggesting method names, compared to the n-gram
language model. Adding features increases the performance of the
models, indicating that the model is able to use non-local context
to make better predictions. Naturally, the performance degrades
handles 5objects and because short term memory is usually 72is
the size of human short term memory.Table 4: Absolute increase in performance for each type of feature compared to the normal and subtoken models with no features at
5% suggestion frequency and at 20% suggestion frequency for rank k=5. Averages from clojure ,elasticsearch andlibgdx , chosen
uniformly at random from our corpus. If a model does not produce suggestions at a given frequency, it is not counted in the average.
The ‚Äúvocabulary‚Äù of an identiÔ¨Åer ( e.g.method name) are all the subtokens of that identiÔ¨Åer.
FeatureAbsolute F1 Increase (%) Absolute Accuracy Increase (%)
Simple Subtoken Simple Subtoken
@5% @20% @5% @20% @5% @20% @5% @20%
Variables
AST Ancestors -0.3 -1.0 0.9 2.6 2.0 0.7 0.8 2.5
Method, Class, Superclass and Interface Subtokens -1.2 0.1 0.0 1.0 1.1 1.8 0.1 0.8
Declaration ModiÔ¨Åers -0.1 -0.1 0.7 1.9 2.2 0.7 0.2 1.4
Variable Type -0.2 -0.3 0.6 3.8 2.1 1.5 0.3 3.3
All 1.2 4.8 -0.8 5.8 2.1 5.0 -0.9 5.7
Method Declarations
AST Ancestors 3.6 1.3 1.9 -2.3 4.1 1.5 1.9 -0.9
Cyclomatic Complexity 4.0 0.2 3.7 2.3 10.9 0.5 1.8 1.4
Fields Subtokens 9.1 -0.1 -24.8 -7.6 9.8 0.6 -23.0 -6.9
Class, Superclass and Interfaces Subtokens 11.7 7.8 7.0 3.1 13.4 8.8 7.3 2.8
Method Implementation Subtokens 7.9 4.2 3.4 4.7 7.4 4.6 0.9 3.8
Declaration ModiÔ¨Åers 1.3 0.5 -1.6 0.9 6.5 0.02 -1.8 0.6
Return Type 10.0 21.7 4.0 3.2 7.0 17.5 3.6 2.6
Sibling Methods 8.8 4.9 -0.4 2.3 10.5 5.2 -1.2 1.2
Number of Arguments 7.7 0.4 4.3 6.1 6.4 0.7 3.2 4.9
Thrown Exceptions 6.4 20.8 -8.8 -2.1 5.5 16.8 -11.4 -1.1
All 17.0 5.1 5.7 7.5 19.3 5.3 5.3 6.5
Type Declarations
Field Subtokens - - -5.1 3.1 - - 5.2 0.4
Superclass and Interface Subtokens - - 5.3 8.7 - - 14.2 2.5
Contained Methods Subtokens - - -1.5 8.5 - - -4.4 2.3
All - - 13.6 10.3 - - 2.6 3.6
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
Suggestion Frequency0.00.20.40.60.81.0Suggestion F1ngram
no features
features
subtoken
Figure 6: Evaluation of single point suggestions for variables at
rank k=1averaged across all projects. The ‚Äúfeatures‚Äù and ‚Äúno-
features‚Äù models lack sufÔ¨Åcient conÔ¨Ådence to make suggestions
at the higher suggestion frequencies.
slightly as the prediction conÔ¨Ådence decreases. Interestingly, the
token-level models are unable to make any suggestions beyond a
suggestion frequency of 15%. For all other tokens, the token-level
methods return the special UNKtoken, indicating that the models
expect a neologism which they cannot predict. In contrast, the
subtoken models sustain a good F1 score, even for large sugges-
tion frequencies. This is due to the fact that the subtoken models
learn naming conventions at the subtoken level, capturing linguis-
tic patterns [5] such as that speciÔ¨Åc functions may contain various
subtokens e.g.get,set,has,is.
Table 4 shows a full list of the effect that each feature has on the
performance of the neural models at rank k=5. As expected, the
return type, the subtokens of the class where the method is declared
in and the subtokens of the variables and method invocations inside
that method provide the most substantial performance increases.
Based on these results, we conclude that we are able to suggest
accurate method names and that our suggestions are better than
previous approaches. We therefore answer RQ3 in the afÔ¨Årmative.
Class DeÔ¨Ånitions Accuracy In the previous section, the perfor-
mance of the neural model on suggesting names for method decla-
rations was shown. In this section, we evaluate the neural modelwhen making suggestions for class deÔ¨Ånitions. Figure 7b shows the
performance of the n-gram language model and the neural models
for class name deÔ¨Ånitions. In contrast to the previous models, the
token-level models cannot make any suggestions, always suggest-
ing the UNKtoken. However, the subtoken model is able to make
suggestions even at high suggestion frequencies maintaining an F1
score of more than 40% outperforming the n-gram model.
Thanks to the ability of the subtoken model to suggest neologisms
the subtoken-level model is able to suggest class deÔ¨Ånition names
that it has never seen before, with a good F1 score. Table 4 shows
that the subtokens of the superclass and interfaces that a type is
implementing are informative about the name of the class. Addition-
ally, when combining all the available features, we get a signiÔ¨Åcant
increase in F1 score. Thus, we answer yes to RQ4 as well; we are
able to suggest accurate type (class) names.
6. RELATED WORK
Naming In Software Engineering Naming in code has achieved
a fair amount of research attention. There has been prior research
into identifying poorly named artifacts. H√∏st and √òstvold [26] de-
veloped a technique for automatically inferring naming rules for
methods based on the return type, control Ô¨Çow, and parameters of
methods. Using these rules they found and reported ‚Äúnaming bugs‚Äù
by identifying methods whose names contained rule violations. Ar-
naoudova et al. presented a catalog of ‚Äúlinguistic anti-patterns‚Äù in
code that lead to developers misunderstanding code and built a
detector of such anti-patterns [5]. Binkley used part of speech tag-
ging to Ô¨Ånd Ô¨Åeld names that violate accepted patterns, e.g.the Ô¨Åeld
create_mp4 begins with a verb and implies an action which is a
common pattern for a method rather than a Ô¨Åeld [11]. Our work is
complementary, as we make suggestions for names when naming
bugs are found, anti-patterns occur, or naming rules are violated.
De Lucio et al. attempted to automatically name source code
artifacts using LSI and LDA and found that this approach doesn‚Äôt
work as well as simpler methods such as using words from class and0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
Suggestion Frequency0.00.20.40.60.81.0Suggestion F1ngram
no features
features
subtoken(a) Method declarations excluding overrides.
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
Suggestion Frequency0.00.20.40.60.81.0Suggestion F1ngram
subtoken
(b) Class (Type) Declarations.
Figure 7: Evaluation of single point suggestions for declara-
tions at k=1. Overriden method declarations are easier to
predict, so we exclude them. The ‚Äúfeatures‚Äù model achieves the
best F1 scores for method declarations, but lacks conÔ¨Ådence at
higher suggestion frequencies (where the line stops). In con-
trast, the ‚Äúsubtoken‚Äù model achieves a good F1 score for all
suggestion frequencies for method names and is the only model
to accurately suggest class names.
method names [18]. Many studies of naming have also been con-
ducted giving us insight into its importance. Butler et al. found that
‚ÄúÔ¨Çawed‚Äù identiÔ¨Åer names (those that violate naming conventions or
do not follow coding practice guidelines) are related to certain types
of defects [14]. Later they also examined the most frequent grammat-
ical structures of method names using part of speech tagging [15].
Lawrie et al. [29] and Takang et al. [50] both conducted empirical
studies and concluded that the quality of identiÔ¨Åer names in code
have a profound effect on program comprehension. Liblit et al. ex-
plored how names in code ‚Äúcombine together to form larger phrases
that convey additional meaning about the code.‚Äù [30]. Arnaoudova
et al. [6] studied identiÔ¨Åer renamings, showing that naming is an
important part of software construction. Additionally, in a survey of
94 developers, they found that about 68% of developers think that
recommending identiÔ¨Åers would be useful. These studies highlight
the importance of our work, by being able to suggest quality names
or parts of names.
As method and class names are expected to indicate their seman-
tics, they can be viewed as a special case of code summarization .
Haiduc et al. showed that NL text summarization does not work
well for code [23] and such techniques must be adapted to be ef-
fective. They later developed summaries that are used to improve
comprehension [22]. Sridhara et al. used idioms and structure in the
code of methods to generate high level abstract summaries. While
they don‚Äôt suggest method names, they discuss how their approach
may be extended to provide them [47]. Sridhara also showed how to
generate code summaries appropriate for comments within the code
(e.g.as method headers) [46, 45]. For more work in this area, Eddy
et al. provide a survey of code summarization methods [20]. We
note that most studies and approaches in this area focus on names
of variables, Ô¨Åelds, and methods. Although some examine all identi-
Ô¨Åers in the code, we are unaware of any work that focuses on type
(class) names as we do.Language Models In Software Engineering Probabilistic models
of source code have been applied in software engineering. Hindle
et al. and Ngyuen et al. [25, 41] used n-gram models to improve
code autocompletion. Allamanis and Sutton [3] present an appli-
cation of code n-gram models at scale. Maddison and Tarlow [31]
built a more sophisticated generative model of source code using
log-bilinear models that reÔ¨Çects the syntactic structure of the code.
Although the machine learning principles we use are similar, their
model differs signiÔ¨Åcantly from ours, because their purpose is to
build models that generate source code rather than improve exist-
ing code. In other words, our model is discriminative rather than
generative. Mou et al. [40] use a convolutional neural network to
classify code from programming competition problems. Karaivanov
et al. [27] combine LMs with static program analysis to suggest
method calls and Ô¨Åll-in gaps. Other applications of probabilistic
source code models are extracting code idioms [4] and code mi-
gration [27]. Closely related to this work is our previous work
where we infer formatting and naming conventions [2] using n-gram
LMs to suggest natural renamings. Raychev et al. [43] present a
discriminative probabilistic model to predict types and names of
variables in JavaScript. In contrast, our current work introduces a
log-bilinear model that greatly improves on the n-gram LM, espe-
cially on method and class naming, proposing neologisms by taking
into account subtokens and non-local context.
Other Applications of Neural Logbilinear Models Neural log-
bilinear models have been used in NLP for LMs [37, 39] and de-
scribing images with NL [28]. Log-bilinear models have been
shown in NLP to produce semantically consistent and interesting
vector space representations (embeddings). Notable systems include
word2vec [35, 36] and GloVe [42]. In contrast to these approaches,
we use a rich notion of non-local context by incorporating features
speciÔ¨Åc to source code while we produce similar vector space mod-
els for method names, variables and types. Additionally, we present
a novel sub-token model. Related to our subtoken model is the
work of Botha and Blunsom [12] that integrate compositional mor-
phological representations of words into a log-bilinear LM but the
morphological features are only used in the context of an LM.
7. CONCLUSION
We introduced the method naming problem, that of automatically
determining a functionally descriptive name of a method or class.
Previous work on automatically assigning names [2, 43] focuses
on local variables, and relies on relatively local context. Naming
methods is more difÔ¨Åcult because it requires integrating non-local
information from the body of the method or class. We presented
a Ô¨Årst solution using a log-bilinear neural language model, which
includes feature functions that capture long-distance context, and
a subtoken model that can predict neologisms, names that did not
appear in the training set. The model embeds each token into a high
dimensional continuous space.
Continuous embeddings of identiÔ¨Åers have many other potential
applications in software engineering, such as rejecting commits
whose names violate project conventions; exploration of linguistic
anti-patterns, such as a getter starting with set [5] and feature
localization. Finally, a problem similar to method naming arises in
NLP, namely the problem of generating a headline from the text of
an article [8, 19]. It is possible that models similar to ours could
shed light on that problem as well.
8. ACKNOWLEDGEMENTS
This work was supported by Microsoft Research through its PhD
Scholarship Programme. Charles Sutton was supported by the En-
gineering and Physical Sciences Research Council [grant number
EP/K024043/1].9. REFERENCES
[1] S. L. Abebe, V . Arnaoudova, P. Tonella, G. Antoniol, and
Y . Gueheneuc. Can lexicon bad smells improve fault
prediction? In WCRE , 2012.
[2] M. Allamanis, E. T. Barr, C. Bird, and C. Sutton. Learning
natural coding conventions. In FSE, 2014.
[3]M. Allamanis and C. Sutton. Mining source code repositories
at massive scale using language modeling. In MSR . IEEE
Press, 2013.
[4] M. Allamanis and C. Sutton. Mining Idioms from Source
Code. In FSE, 2014.
[5] V . Arnaoudova, M. Di Penta, G. Antoniol, and Y .-G.
Gueheneuc. A new family of software anti-patterns:
Linguistic anti-patterns. In CSMR , 2013.
[6] V . Arnaoudova, L. M. Eshkevari, M. D. Penta, R. Oliveto,
G. Antoniol, and Y . Gu√©h√©neuc. REPENT: analyzing the
nature of identiÔ¨Åer renamings. IEEE TSE , 2014.
[7] V . Arnaoudova, M. D. Penta, and G. Antoniol. Linguistic
antipatterns: What they are and how developers perceive them.
EMSE , 2015.
[8] M. Banko, V . Mittal, and M. Witbrock. Headline generation
based on statistical translation. In ACL, 2000.
[9] K. Beck. Implementation patterns . Pearson Education, 2007.
[10] Y . Bengio, R. Ducharme, P. Vincent, and C. Janvin. A neural
probabilistic language model. The Journal of Machine
Learning Research , 2003.
[11] D. Binkley, M. Hearn, and D. Lawrie. Improving identiÔ¨Åer
informativeness using part of speech information. In MSR ,
2011.
[12] J. Botha and P. Blunsom. Compositional morphology for word
representations and language modelling. In ICML , 2014.
[13] S. Butler, M. Wermelinger, Y . Yu, and H. Sharp. Relating
identiÔ¨Åer naming Ô¨Çaws and code quality: An empirical study.
InWCRE , 2009.
[14] S. Butler, M. Wermelinger, Y . Yu, and H. Sharp. Exploring the
inÔ¨Çuence of identiÔ¨Åer names on code quality: An empirical
study. In 14th European Conference on Software Maintenance
and Reengineering (CSMR‚Äô2010 , pages 156‚Äì165, 2010.
[15] S. Butler, M. Wermelinger, Y . Yu, and H. Sharp. Mining Java
class naming conventions. In ICSM , 2011.
[16] S. Chen and J. Goodman. An empirical study of smoothing
techniques for language modeling. In ACL, 1996.
[17] T. A. Corbi. Program understanding: Challenge for the 1990s.
IBM Systems Journal , 28(2):294‚Äì306, 1989.
[18] A. De Lucia, M. Di Penta, R. Oliveto, A. Panichella, and
S. Panichella. Using IR methods for labeling source code
artifacts: Is it worthwhile? In ICPC , 2012.
[19] B. Dorr, D. Zajic, and R. Schwartz. Hedge trimmer: A
parse-and-trim approach to headline generation. In
HLT-NAACL-03 , 2003.
[20] B. P. Eddy, J. A. Robinson, N. A. Kraft, and J. C. Carver.
Evaluating source code summarization techniques:
Replication and expansion. In ICPC , 2013.
[21] M. U. Gutmann and A. Hyv√§rinen. Noise-contrastive
estimation of unnormalized statistical models, with
applications to natural image statistics. The Journal of
Machine Learning Research , 2012.
[22] S. Haiduc, J. Aponte, and A. Marcus. Supporting program
comprehension with source code summarization. In ICSE ,
2010.
[23] S. Haiduc, J. Aponte, L. Moreno, and A. Marcus. On the use
of automated text summarization techniques for summarizing
source code. In WCRE , 2010.[24] D. Hendrix, J. Cross, S. Maghsoodloo, et al. The effectiveness
of control structure diagrams in source code comprehension
activities. IEEE TSE , 2002.
[25] A. Hindle, E. T. Barr, Z. Su, M. Gabel, and P. Devanbu. On
the naturalness of software. In ICSE , 2012.
[26] E. W. H√∏st and B. M. √òstvold. Debugging method names. In
ECOOP , 2009.
[27] S. Karaivanov, V . Raychev, and M. T. Vechev. Phrase-based
statistical translation of programming languages. In Onward! ,
2014.
[28] R. Kiros, R. Zemel, and R. Salakhutdinov. Multimodal neural
language models. In NIPS , 2013.
[29] D. Lawrie, C. Morrell, H. Feild, and D. Binkley. What‚Äôs in a
name? a study of identiÔ¨Åers. In ICPC , 2006.
[30] B. Liblit, A. Begel, and E. Sweetser. Cognitive perspectives on
the role of naming in computer programs. In Proceedings of
the 18th Annual Psychology of Programming Workshop , 2006.
[31] C. J. Maddison and D. Tarlow. Structured generative models
of natural source code. In ICML , 2014.
[32] C. D. Manning, P. Raghavan, and H. Sch√ºtze. Introduction to
Information Retrieval . Cambridge University Press, 2008.
[33] R. C. Martin. Clean code: a handbook of agile software
craftsmanship . Pearson Education, 2008.
[34] S. McConnell. Code Complete . Microsoft Press, 2004.
[35] T. Mikolov, K. Chen, G. Corrado, and J. Dean. EfÔ¨Åcient
estimation of word representations in vector space. In ICLR
Workshop , 2013.
[36] T. Mikolov, W.-t. Yih, and G. Zweig. Linguistic regularities in
continuous space word representations. In HLT-NAACL , 2013.
[37] A. Mnih and G. Hinton. Three new graphical models for
statistical language modelling. In ICML , 2007.
[38] A. Mnih and K. Kavukcuoglu. Learning word embeddings
efÔ¨Åciently with noise-contrastive estimation. In NIPS , 2013.
[39] A. Mnih and Y . W. Teh. A fast and simple algorithm for
training neural probabilistic language models. In ICML , 2012.
[40] L. Mou, G. Li, Z. Jin, L. Zhang, and T. Wang. TBCNN: a
tree-based convolutional neural network for programming
language processing. arXiv preprint arXiv:1409.5718 , 2014.
[41] T. T. Nguyen, A. T. Nguyen, H. A. Nguyen, and T. N. Nguyen.
A statistical semantic language model for source code. In FSE,
2013.
[42] J. Pennington, R. Socher, and C. D. Manning. Glove: Global
vectors for word representation. EMNLP , 2014.
[43] V . Raychev, M. Vechev, and A. Krause. Predicting program
properties from ‚Äúbig code‚Äù. In POPL , 2015.
[44] S. Russell and P. Norvig. ArtiÔ¨Åcial Intelligence: A Modern
Approach . Prentice Hall, 1995.
[45] G. Sridhara. Automatic generation of descriptive summary
comments for methods in object-oriented programs .
University of Delaware, 2012.
[46] G. Sridhara, E. Hill, D. Muppaneni, L. Pollock, and
K. Vijay-Shanker. Towards automatically generating summary
comments for java methods. In ASE, 2010.
[47] G. Sridhara, L. Pollock, and K. Vijay-Shanker. Automatically
detecting and describing high level actions within methods. In
ICSE , 2011.
[48] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and
R. Salakhutdinov. Dropout: A simple way to prevent neural
networks from overÔ¨Åtting. Journal of Machine Learning
Research , 2014.
[49] A. Takang, P. Grubb, and R. Macredie. The effects of
comments and identiÔ¨Åer names on program comprehensibility:
an experiential study. Journal of Program Languages ,4(3):143‚Äì167, 1996.
[50] A. A. Takang, P. A. Grubb, and R. D. Macredie. The effects of
comments and identiÔ¨Åer names on program comprehensibility:
an experimental investigation. J. Prog. Lang. , 4(3):143‚Äì167,1996.
[51] L. van der Maaten and G. Hinton. Visualizing data using
t-SNE. Journal of Machine Learning Research , 2008.