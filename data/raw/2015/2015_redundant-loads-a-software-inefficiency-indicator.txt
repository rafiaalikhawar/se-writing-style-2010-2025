Redundant Loads: A Software Inefﬁciency Indicator
Pengfei Su, Shasha Wen
College of William & Mary
{psu, swen }@email.wm.eduHailong Yang
Beihang University
hailong.yang@buaa.edu.cnMilind Chabbi
Scalable Machines Research
milind@scalablemachines.orgXu Liu
College of William & Mary
xl10@cs.wm.edu
Abstract —Modern software packages have become increas-
ingly complex with millions of lines of code and references
to many external libraries. Redundant operations are a com-
mon performance limiter in these code bases. Missed compiler
optimization opportunities, inappropriate data structure and
algorithm choices, and developers’ inattention to performance
are some common reasons for the existence of redundant op-
erations. Developers mainly depend on compilers to eliminate
redundant operations. However, compilers’ static analysis often
misses optimization opportunities due to ambiguities and limited
analysis scope; automatic optimizations to algorithmic and data
structural problems are out of scope.
We develop L OAD SPY, a whole-program proﬁler to pinpoint
redundant memory load operations, which are often a symptom
of many redundant operations. The strength of L OAD SPYexists
in identifying and quantifying redundant load operations in pro-
grams and associating the redundancies with program execution
contexts and scopes to focus developers’ attention on problematic
code. L OAD SPYworks on fully optimized binaries, adopts various
optimization techniques to reduce its overhead, and provides a
rich graphic user interface, which make it a complete developer
tool. Applying L OAD SPY showed that a large fraction of re-
dundant loads is common in modern software packages despite
highest levels of automatic compiler optimizations. Guided by
LOAD SPY, we optimize several well-known benchmarks and real-
world applications, yielding signiﬁcant speedups.
Index Terms —Whole-program proﬁling, Software optimiza-
tion, Performance measurement, Tools.
I. I NTRODUCTION
Production software packages have become increasingly
complex. They are comprised of a large amount of source
code, sophisticated control and data ﬂow, a hierarchy of
component libraries, and growing levels of abstractions. This
complexity often introduces inefﬁciencies across the software
stacks, leading to resource wastage, performance degradation,
and energy dissipation [1], [2]. Such inefﬁciencies are usu-
ally in the form of useless or redundant operations, such
as computations whose results may not be used [3], [4],
re-computation of already computed values [5], unnecessary
data movement [6]–[10], and excessive synchronization [11],
[12]. The provenance of these inefﬁciencies can be many:
rigid abstraction boundaries, missed opportunities to optimize
common cases, suboptimal algorithm design, inappropriate
data structure selection, and poor compiler code generation.
There is a long history of compiler optimizations aimed at
statically analyzing and eliminating redundant operations by
techniques such as common sub-expression elimination [13],
value numbering [14], constant propagation [15], to name a
few. However, they have a myopic view of the program, whichlimits their analysis to a small scope—individual functions
or ﬁles. Layers of abstractions, dynamically loaded libraries,
multi-lingual components, aggregate types, aliasing, sophisti-
cated ﬂows of control, input-speciﬁc path-speciﬁc redundan-
cies, and combinatorial explosion of execution paths make
it practically impossible for compilers to obtain a holistic
view of an application to eliminate all redundancies. Link-
time optimization [16] can offer better visibility, however, the
analysis is still conservative and may err on the side of being
less exhaustive to reduce prohibitive analysis cost. Whole-
program link-time optimizations [17], [18] have provided less
than 5% average speedup, although a lot more headroom exists
as we show in our work. Thus, despite their best efforts,
compilers often fall short of eliminating runtime inefﬁciencies.
Execution proﬁling aims to understand the runtime be-
havior of a program. Performance analysis tools such as
HPCToolkit [19], VTune [20], perf [21], gprof [22], OPro-
ﬁle [23], and CrayPA T [24] monitor code execution to identify
hot code regions, idle CPU cycles, arithmetic intensity, and
cache misses, etc. These tools can recognize the utilization
(saturation or underutilization) of hardware resources, but they
cannot inform whether a resource is being used in a fruitful
manner that contributes to the overall efﬁciency of a program.
A hotspot need not mean inefﬁcient code, and conversely, the
lack of a hotspot need not mean better code. Coarse-grained
proﬁlers usually cannot distinguish efﬁcient vs. inefﬁcient
code; for example, they cannot identify that repeated memory
loads of the same value or result-equivalent computations
waste both memory bandwidth and processor functional units.
Whole-program ﬁne-grained monitoring is a means to mon-
itor execution at microscopic details: it monitors each binary
instruction instance, including its operator, operands, and
runtime values in registers and memory. A key advantage of
microscopic program-wide monitoring is that it can identify re-
dundancies irrespective of the user-level program abstractions.
Prior work [5], [6], [10], [25] has shown that the ﬁne-grained
proﬁling techniques can identify many forms of software
inefﬁciencies and offer detailed guidance to tune code.
Existing ﬁne-grained proﬁlers pinpoint inefﬁciencies in
a subset of individual operations such as operations with
symbolic equivalence [5], dead memory stores [6], and op-
erations writing same values to target registers or memory
locations [10]. They have, however, overlooked an important
category temporal load redundancy —loading the same value
from the same memory location. For instance, the code on the
left of Listing 1 shows redundant operations that are invisible
9822019 IEEE/ACM 41st International Conference on Software Engineering (ICSE)
1558-1225/19/$31.00 ©2019 IEEE
DOI 10.1109/ICSE.2019.00103
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 09:39:37 UTC from IEEE Xplore.  Restrictions apply. 1while (t < threshold) {
2t=0 ;
3for( i=0 ;i<N ; i++)
4/trianglerightsld t += A[i] + B[i] *delta;
5delta -= 0.1 *t;
6}1for ( i=0 ;i<N ; i++)
2a += A[i]; b += B[i];
3while (t < threshold) {
4t=a+b *delta;
5delta -= 0.1 *t;
6}
Listing 1: An example code (on the left) with temporal inefﬁciencies
that cannot be identiﬁed by existing ﬁne-grained proﬁlers. Because
arrays Aand Bare immutable in the loop nest, computing on
these loop invariants introduces many redundancies. One can hoist
the redundant computation outside of the loop (on the right) for
optimization.
1int A[N] = {1, 1, 1, 15};
2for( i=0 ;i<N ; i++)
3{
4/trianglerightsldt += func(A[i]);
5}1int A[N] = {1, 1, 1, 15};
2a = func(A[0]);
3for( i=0 ;i<N ; i++) {
4if(A[i] != A[i-1])
5 a = func(A[i]);
6t+ =a ;}
Listing 2: An example code (on the left) with spatial inefﬁciencies
that cannot be identiﬁed by existing ﬁne-grained proﬁlers. The load
redundancy happens at line 4 where the program reads the same value
from the nearby memory locations since some adjacent elements of
array Ahave the same value. Such redundancy further results in
redundant computation involved in the function func . Because func
always returns the same value for the same input. One can compare if
the adjacent elements in array Aare equivalent to eliminate redundant
computation (on the right). If they are the same, one can reuse the
return value of func , which is generated in the previous iteration.
in existing ﬁne-grained proﬁlers. In this code, suppose all the
scalars are in registers and vectors are in memory. Because
there are no “dead store” operations (a store followed by
another store to the same location without an intervening
load), DeadSpy [6] does not identify any inefﬁciency. Since
the values written in tanddelta always change, RedSpy [10]
does not report any “silent store” operations [7]. Finally,
since there is no symbolic equivalent computation, RVN [5]
does not report any inefﬁciency. Furthermore, because the
optimization involves the mathematically equivalent transfor-
mation, as shown on the right of Listing 1, it is difﬁcult to
optimize with other compiler techniques such as polyhedral
optimization [26].
The code on the left of Listing 2 shows another kind of
load redundancy, which loads the same value from the nearby
memory locations. Even though each element of array Ais
only loaded once, adjacent elements with the same values
result in loading the same value and the subsequent redundant
computation. We refer to this type of redundancy as spatial
load redundancy . As a practical example, a sparse matrix with
a dense format can yield many spatial load redundancies.
Listing 1 and 2 show a tip of the iceberg of the inefﬁciencies
we target in this paper to complement existing tools. From
our observation, a variety of inefﬁciencies exhibit substantial
redundant loads; conversely, the presence of a large fraction of
redundant loads in an execution is a symptom of some kind of
inefﬁciency in the code regions that exhibit such redundancy.
Furthermore, the subsequent operations based on redundant
loads are potentially redundant.
We have designed and implemented a developer tool—
LOAD SPY—aimed at proﬁling an execution and quantifying
load redundancy in the execution. L OAD SPYhighlights precisesource code in its full calling contexts and the two parties
involved in a redundant load. Additionally, L OAD SPYnarrows
down the investigation scope to help developers focus on the
provenance of inefﬁciencies. A thorough evaluation on a suite
of benchmarks and real-world applications shows that looking
for redundant loads in a program offers an easy avenue for
performance enhancement in many programs.
In this paper, we make the following contributions:
•Show that redundant loads are a common indicator of
various forms of software inefﬁciencies. This ﬁnding serves
as the foundation of L OAD SPY.
•Describe the design of L OAD SPY—a whole-program ﬁne-
grained proﬁler to pinpoint redundant loads.
•Develop strategies for analyzing a large volume of proﬁling
data by attributing redundancy to runtime contexts, objects,
and scopes.
•Enable rich visualization for a large volume of proﬁling data
coming from different threads/processes with a user-friendly
GUI, which improves the usability for non-experts.
•Apply L OAD SPYto pinpoint inefﬁciencies in well-known
benchmarks and real-world applications that were the sub-
jects of study and optimization for years and eliminate
LOAD SPY-found inefﬁciencies by avoiding redundant loads,
which yield nontrivial speedups.
II. R ELA TED WORK
There exist many compiler techniques and static analysis
techniques [13], [27]–[29] to identify redundant computation.
However, these static approaches suffer from limitations re-
lated to the precision of alias information, optimization scope,
and insensitivity to inputs and execution contexts. To address
these issues, recent approaches convert the source code to spe-
ciﬁc notations for redundancy detection and removal [30], or
target speciﬁc algorithm for optimization [31]. However, these
approaches require substantial prior knowledge to identify
whether a program suffers from redundancies that are worthy
of optimization. In contrast, L OAD SPY monitors execution,
avoids inaccuracies associated with compile-time analysis, and
needs no prior knowledge of the measured programs.
There exist many hardware-based approaches [3], [7], [32]–
[37] that optimize redundant operations during program ex-
ecution. However, these approaches require hardware exten-
sion, which is unavailable in commodity processors. Instead,
LOAD SPYis a pure software approach and does not need any
hardware changes. The remaining section reviews only other
proﬁling techniques.
A. V alue proﬁling
LOAD SPYis a value-aware proﬁler; value proﬁling tech-
niques are closely related to our work. Calder et al. [38]–
[40] proposed probably the ﬁrst value proﬁler on DEC Alpha
processors. They instrumented the program code and recorded
top N values to pinpoint invariant or semi-invariant variables
stored in registers or memory. A variant of this value proﬁler
was proposed in a later research [41]. Burrows et al. [42] used
hardware performance counters to sample values in Digital
983
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 09:39:37 UTC from IEEE Xplore.  Restrictions apply. Continuous Proﬁling Infrastructure [43]. Wen et al. [44] com-
bined performance monitoring units and debug registers avail-
able in x86 to identify redundant memory operations. These
approaches do not explore whole-program load redundancy in
depth. Moreover, none of them detect spatial redundancy.
Some code specialization work depends on value proﬁling.
However, these approaches limit themselves to only analyzing
registers [45], static instructions [46], memory store opera-
tions [10], or functions [47]–[49]. They omit many optimiza-
tion opportunities and require signiﬁcant manual efforts to
reason about the root causes of inefﬁciencies.
Unlike existing value proﬁlers, L OAD SPYhas four distinct
features. First, L OAD SPYis the ﬁrst value proﬁler that tracks
thehistory of loaded values from individual memory locations ,
rather than the values produced by individual instructions .
Second, L OAD SPY identiﬁes both temporal and spatial re-
dundancies in load operations. Third, L OAD SPY provides
novel redundancy scope and metrics to guide optimization
in both contexts and semantics. Fourth, L OAD SPYnot only
identiﬁes redundancy arising due to exactly the same values
but also identiﬁes redundancy due to approximately equal
values, which offers opportunities for approximate computing .
B. V alue-agnostic proﬁling
RVN [5] assigns symbolic values to dynamic instructions
and identiﬁes redundancy on the ﬂy. DeadSpy [6] tracks
every memory operation to pinpoint a store operation that is
not loaded before a subsequent store to the same location.
MemoizeIt [50] detects Java methods that perform identical
computations. Travioli [51] detects redundant data structure
traversals. These approaches miss out on certain opportunities
that L OAD SPY can detect by explicitly inspecting values
generated at runtime.
Toddler [25] has to manually add loop events to instrument
loops in a C code base and only identiﬁes repetitive memory
loads across loop iterations. The follow-on work LDoctor [52]
reduces Toddler’s overhead using a combination of ad-hoc
sampling and static analysis techniques. LDoctor instruments
a small number of suspicious loops at compile time. This
technique can miss redundant loads in different loops. In
contrast, L OAD SPY works on fully optimized binaries, is
independent of any compiler, and performs the whole-program
proﬁling instead of limiting itself to only proﬁling loops.
III. R EDUNDANT LOADS :A NINEFFICIENCY SYMPTOM
While there are several ways to identify the inefﬁciency,
LOAD SPYfocuses on memory load operations. If two consec-
utive load operations performed on the same memory location
load the same value, the second load operation can be deemed
useless. Thus, the second load could potentially be elided. Our
study aims to quantify redundant loads and attribute them
to the code regions that cause them. A single instance of
a redundant load is uninteresting; highly frequent redundant
loads occurring in the same code location demand attention.
It is easy to imagine how redundant loads happen: re-
peatedly accessing immutable data structures or algorithmsemploying memoization. It is equally easy to see how inef-
ﬁcient code sequences show up as redundant loads: missed
inlining appears as repeatedly loading the same values in a
callee, imperfect alias information shows up as loading the
same values from the same location via two different pointers,
redundant computations show up as the same computations
being performed by loading unchanged values, algorithmic
defects, e.g., frequent linear searches or hash collisions, also
appear as repeatedly loading unchanged values from the same
locations.
Deﬁnition 1 (Temporal Load Redundancy) .A memory load
operation L2, loading value V2from location M, is redundant
iff the previous load operation L1, performed on M, loaded
a valueV1andV1=V2.I fV1≈V2, we call it approximate
temporal load redundancy.
Deﬁnition 2 (Spatial Load Redundancy) .A memory load op-
erationL2, loading a value V2from location M2, is redundant
iff the previous load operation L1, performed on location
M1, loaded a value V1andV1=V2, andM1andM2belong
to the address span of the same data object. If V1≈V2,w e
call it approximate spatial load redundancy.
Deﬁnition 3 (Redundancy Fraction) .We deﬁne the redundancy
fractionRin an execution as the ratio of bytes redundantly
loaded to the total bytes loaded in the entire execution.
We emphasize that the redundancy is deﬁned for instruc-
tion instances, not static instructions. Deleting an instruction
involved in one instance of a redundant load can be unsafe.
Observation 1. Large redundancy fraction ( R) in the execu-
tion proﬁle of a program is a symptom of some kind of software
inefﬁciency.
Redundant loads are neither a necessary condition nor a
sufﬁcient condition to capture all kinds of software inefﬁcien-
cies. However, we show, with many illustrative case studies,
that a large fraction of redundant loads in the same code
region is often a symptom of a serious inefﬁciency. We notice
frequent redundant loads across the board in many programs
irrespective of optimization levels, raising a warning alarm
of potential inefﬁciency. Although not all redundant loads
demand optimization, in our experience, investigating the top
few contributors in a proﬁle offers a high potential to tune and
optimize code. Looking for load redundancy opens potentially
an easy avenue for code optimization—manual or automatic.
We measure the redundancy fraction in a number of bench-
marks SPEC CPU2006 [53], PARSEC-2.1 [54], Rodinia-
3.1 [55], and NERSC-8 [56]. We compile these benchmarks
with gcc-4.8.5 -O3 , link-time optimization (LTO) and
proﬁle-guided optimization (PGO), which is one of the highest
optimization levels. In practice, most packages do not use this
level of optimization.
We observe that a large load redundancy fraction correlates
with some kind of inefﬁciency. Furthermore, the code that
generates many redundant loads is responsible for the inefﬁ-
ciencies in the program. We classify the causes of redundant
984
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 09:39:37 UTC from IEEE Xplore.  Restrictions apply. 1for (j = 1; j <= ndelta; j++) {
2for (k = 0; k <= nly; k++) {
3/trianglerightsld new_dw = ((ETA *delta[j] *ly[k])+(MOMENTUM *oldw[k][j]));
4 w[k][j] += new_dw;
5 oldw[k][j] = new_dw;
6}}
Listing 3: Spatial load redundancy in Rodinia-3.1 backprop. Arrays
delta andoldw are repeatedly loaded from memory whereas most
array elements are zero.
loads according to their provenance: input-sensitive redundant
loads, inefﬁcient data structure/algorithm designs, or missing
compiler optimizations. Different kinds of inefﬁciencies re-
quire different optimization strategies.
A. Input-sensitive Redundant Loads
In this section, we classify the inefﬁciency due to inputs.
Rodinia-3.1 backprop [55], a supervised machine learning
algorithm, trains the weights of connections in a neural net-
work. The redundancy fraction of this program is 64%. It
is common knowledge that as the training progresses, many
weights stabilize and do not change. Hence, their gradients
become and remain zero. Listing 3 shows the inefﬁciency at
line 3, where the majority of elements in arrays delta and
oldw are zeros. Computations at lines 3-5 can be bypassed
when delta[j] andoldw[k][j] are zeros. Repeatedly
loading the zero value from delta[j] andoldw[k][j]
shows up as spatial load redundancy. It is easy to eliminate the
input-sensitive redundant loads by predicating the subsequent
computation on the values of delta[j] andoldw[k][j]
being non-zero.
B. Redundant Loads due to Suboptimal Data Structures and
Algorithms
Inefﬁciencies of this category require semantics to identify
and optimize. These inefﬁciencies also incur a signiﬁcant
number of redundant loads. We illustrate some algorithms that
introduce inefﬁciencies in a few well-known benchmarks.
a)Linear search :Rodinia-3.1 particleﬁlter [55] is used
to estimate the location of a target object in signal pro-
cessing and neuroscience. The redundancy fraction of this
program is 99%. Listing 4 shows the inefﬁciency in function
findIndex , which performs a linear search (line 3) over
a sorted array CDF to determine the location of a given
particle. This linear search is called multiple times in a loop
to become the bottleneck of the program. The symptom of
this inefﬁciency is many redundant loads, which is caused
by the repeated loads of immutable array CDF elements in
different invocation instances of function findIndex .T oﬁ x
this problem, one can replace the linear search with a binary
search, which reduces the volume of redundant loads.
b)Hash table :Parsec-2.1 dedup [54] compresses data
via deduplication. The redundancy fraction of this program is
75%. Listing 5 shows the inefﬁciency in the program, which
searches for an item in a linked list associated with a hash
table entry. The inefﬁciency comes from the frequent execution
on the slow path due to the hash collision. We noticed that
only∼2% hash buckets are occupied, and the slow path is1int findIndex( double *CDF, int lengthCDF, double value) {
2for(x = 0; x < lengthCDF; x++) {
3/trianglerightsld if(CDF[x] >= value) {
4 index = x; break ;
5 }}
6...
7return index;
8}
9...
10for(j = 0; j < Nparticles; j++)
11 i = findIndex(CDF, Nparticles, u[j]);
Listing 4: Temporal load redundancy in Rodinia-3.1 particleﬁlter. A
linear search loads the same values from the same memory locations.
1struct hash_entry *hashtable_search( struct hashtable *h,
void *k) {
2struct hash_entry *e;
3unsigned int hashvalue, index;
4hashvalue = hash(h,k);
5index = indexFor(h->tablelength,hashvalue);
6e = h->table[index];
7while (NULL != e) {
8/trianglerightsldif((hashvalue == e->h) && (h->eqfn(k, e->k))) return e;
9 e = e->next;
10 } ...}
Listing 5: Temporal load redundancy in Parsec-2.1 dedup. Excessive
hash collisions in linear hashing result in long linked lists.
frequently taken. The linked list traversal on the slow path
loads the same values from the same locations (line 8), which
results in redundant loads. One can improve the hash function
to make hash keys uniformly distributed among buckets, which
will reduce the redundancy and hence the inefﬁciency.
C. Redundant Loads due to Missing Compiler Optimizations
Inefﬁciencies of this category occur in small scopes—loop
nests or procedure calls. One needs to either curate the code
or manually apply transformations to eliminate these inefﬁ-
ciencies. The following three examples illustrate our ﬁndings.
a)Missing scalar replacement :Rodinia-3.1 hotspot
3D [55] is a thermal simulation program that estimates pro-
cessor temperature. The redundancy fraction of this program
is 95%. Listing 6 shows a loop nest that performs a stencil
computation. At line 8, tOut_t[c] is updated with the
values in nearby tIn_t[] . Typically, w=c-1ande=
c+1. As a result, the value of tIn_t[e] in the current
iteration equals the value of tIn_t[c] in the next iteration
and further equals the value of tIn_t[w] in the iteration
after the next. However, the compiler does not perform register
promotion of tln_[e] . Hence, many redundant loads occur
in this loop nest. To ﬁx this inefﬁciency, we employ the
scalar replacement to eliminate inter-iteration redundant loads
from memory. Speciﬁcally, we store the value of tIn_t[e]
in a local variable in the current iteration to be reused by
tIn_t[c] in the next iteration and by tIn_t[w] in the
iteration after the next.
b)Missing constant propagation :NERSC-8 ms-
grate [56] measures the message passing rate via the MPI
interface. The redundancy fraction of this program is 97%.
Listing 7 shows a procedure cache_invalidate , which
sets all the elements in array cache_buf to 1. This code
adopts a suboptimal forward propagation that loads the value
ofcache_buf[i-1] and assigns it to cache_buf[i] .
985
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 09:39:37 UTC from IEEE Xplore.  Restrictions apply. 1for(y = 0; y < ny; y++) {
2for(x = 0; x < nx; x++) {
3 int c, w, e, n, s, b, t;
4 c=x+y *n x+z *nx*ny;
5 w=( x= =0 )?c:c-1 ;
6 e=( x= =n x-1 )?c:c+1 ;
7 ...
8/trianglerightsld tOut_t[c] = cc *tIn_t[c]+cw *tIn_t[w]+ce *tIn_t[e]+...
9}}
Listing 6: Temporal load redundancy in Rodinia-3.1 hotspot3D.
Array tIn_t is repeatedly loaded from memory while the values
remain unchanged.
1int *cache_buf;
2...
3static void cache_invalidate( void ){
4int i;
5cache_buf[0] = 1;
6for (i = 1; i < cache_size; ++i)
7/trianglerightsld cache_buf[i] = cache_buf[i-1];
8}
Listing 7: Temporal load redundancy in NERSC-8 msgrate. The
program repeatedly loads a constant “1” from array cache_buf .
Although there is no redundant load in one invocation of this
function, procedure cache_invalidate is called in a loop
(not shown in the listing), resulting in excessive, redundant
loads from array cache_buf . The compiler does not replace
the assignment with a constant, possibly due to its inability to
prove the safety of assigning to a global array in the presence
of concurrent threads of execution.
c)Missing inline substitution : SPEC CPU2006
464.h264ref [53] is a reference implementation of H.264,
a standard of video compression. The redundancy fraction
of this program is 84%. The compiler fails to inline the
frequently called function PelYline_11 at line 8 shown
in Listing 8. Because it is invoked via a function pointer
and the callee routines are not present in the same ﬁle. The
parameters of PelYline_11 —abs_x ,img_height , and
img_width —are unmodiﬁed across multiple successive
invocations. In each invocation, the caller pushes the same
parameters on the same stack, and then the callee loads
the same values from the same location, which show up as
redundant loads. To ﬁx the problem, we need to manually
inline the function [10].
d)Discussion :We have explored other compiler ﬂags
that enable advanced optimization such as polyhedral opti-
mization [57] in GCC . Unfortunately, the polyhedral optimiza-
tion was unsuccessful in optimizing any of the aforementioned
scenarios. Furthermore, we observed that using LTO, PGO, to-
gether with the polyhedral optimization made compilation time
extremely high for some cases. For example, it took over two
hours to compile hotspot 3D, a 30,000 ×slowdown compared
to simply using -O3 . As a result, our later evaluation section
does not use LTO and polyhedral optimization, but only uses
-O3 with PGO. We leave the effectiveness of other compilers
such as LLVM [58] and ICC [59] on the same set of programs
for a future study.
IV . L OAD SPYIMPLEMENTA TION
LOAD SPYemploys Intel Pin [60] to intercept every mem-
ory load operation. The instrumentation obtains the effective1for (pos = 0; pos < max_pos; pos++) {
2...
3if(abs_y >= 0 && abs_y <= max_height && ...)
4 PelYline_11 = FastLine16Y_11;
5else PelYline_11 = UMVLine16Y_11;
6for (blky = 0; blky < 4; blky++) {
7 for ( y=0 ;y<4 ; y++) {
8/trianglerightsld refptr = PelYline_11(ref_pic, abs_y++, abs_x,
img_height, img_width);
9 ...
10 } ...}}
Listing 8: Temporal load redundancy in SPEC CPU2006 464.h264ref
due to missing function inlining.
addressMto be accessed in the instruction, the access length
δ, and offers the pair to a runtime analysis routine. In the rest
of this section, we discuss how L OAD SPYidentiﬁes temporal
and spatial load redundancies, respectively.
A. Detecting Temporal Load Redundancy
Detecting temporal load redundancy requires two pieces of
information: the current value vnew at the target location and
the last-time loaded value voldfrom the same location. The
runtime analysis routine, run just before the execution of the
original program’s load instruction, fetches the current value
vnew at the memory range [M:M+δ).L OAD SPYemploys
a shadow memory Sfor maintaining the last-time loaded
value at the same location. S[M]maintains the value last
loaded by the program at location M.L OAD SPYutilizes the
page-table-based scheme [6] to efﬁciently manage its shadow
memory. At runtime, the analysis routine fetches vold from
S[M:M+δ)andvnew from [M:M+δ).LOAD SPYrecords
an instance of a redundant load ifvold=vnew. All bytes must
match to qualify a load as redundant. Intuitively, sub-read-
size redundancy is not actionable by the programmer. Note,
however, that vold might have been generated by multiple
shorter reads, a single longer read, or more commonly a single
read of the same size. If not redundant, L OAD SPYupdates the
shadow memory with the newly loaded value. Also, L OAD SPY
records an instance of a non-redundant load ifvold/negationslash=vnew.
LOAD SPYprovisions for approximate computation by al-
lowing the new value generated in a ﬂoating-point (FP) oper-
ation to approximately match the previously present value. If
the two values are within a threshold of difference, L OAD SPY
considers them approximately equal and records an instance
of a redundant load. The threshold is tunable; we use 1% in
our experiments. Accordingly, L OAD SPYdecomposes the load
redundancy into precise and approximate .
LOAD SPYattributes each instance of redundant loads (and
non-redundant loads) to two parties /angbracketleftCold,Cnew/angbracketrightinvolved
in two operations, where Cold is the calling context of the
previous load operation on MandCnew is the calling context
of the current load operation on M.
The following equations compute the fraction of temporal
load redundancy in an execution:
Rprecise
prog =/summationtext
i/summationtext
jRedundant non-FP bytes loaded in /angbracketleftCi,Cj/angbracketright
/summationtext
i/summationtext
jnon-FP bytes loaded in /angbracketleftCi,Cj/angbracketright
Rapprox
prog =/summationtext
i/summationtext
jRedundant FP bytes loaded in /angbracketleftCi,Cj/angbracketright
/summationtext
i/summationtext
jFP bytes loaded in /angbracketleftCi,Cj/angbracketright(1)
986
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 09:39:37 UTC from IEEE Xplore.  Restrictions apply. Load redundancy between a pair of calling contexts is given
by the following equations:
Rprecise
/angbracketleftCold,Cnew/angbracketright=Redundant non-FP bytes loaded in /angbracketleftCold,Cnew/angbracketright/summationtext
i/summationtext
jnon-FP bytes loaded in /angbracketleftCi,Cj/angbracketright
Rapprox
/angbracketleftCold,Cnew/angbracketright=Redundant FP bytes loaded in /angbracketleftCold,Cnew/angbracketright/summationtext
i/summationtext
jFP bytes loaded in /angbracketleftCi,Cj/angbracketright(2)
The metrics help identify code regions (pairs of calling con-
texts) where the highest amount of redundancy is observed.
Obtaining the Runtime Calling Context of an Instruction:
Attributing runtime statistics to a ﬂat proﬁle (just an
instruction pointer) does not offer full insights for
developers. For example, attributing redundant loads to
a common library function, e.g., strcmp , offers little
insight since strcmp can be invoked from several
places in a large code base; some invocations may not
even be obvious to the user code. A detailed attribution
demands associating proﬁles to the full calling context:
main():line->A():line->...->strcmp():line .
LOAD SPY requires obtaining the calling context on each
load operation since each load—redundant or not. L OAD SPY
employs CCTLib [61], which efﬁciently maintains calling
contexts as a calling context tree (CCT) [62] including
complex control ﬂows through longjump , tail calls, and
exceptions. The calling context, which is provided as a unique
32-bit integer, is recorded (in addition to the last-time loaded
value) in the shadow memory.
B. Detecting Spatial Load Redundancy
For arrays and aggregate objects, L OAD SPYchecks whether
two consecutive loads from any element of the same object
load (approximately) the same value. For example, if two
consecutive loads from an array a, saya[i] anda[j] , load
the same value, L OAD SPYﬂags it as an instance of spatial
load redundancy and attributes it to the same data object.
To facilitate spatial load redundancy detection, L OAD SPY
maintains a mapping from address ranges to active data objects
in a shadow memory. Associated with each data object Ois
two additional pieces of information: a singleton value vold
loaded as a result of the previous load operation performed on
Oand the calling context Coldassociated with the previous
load operation performed on O. Upon each memory load,
LOAD SPYuses the effective address of the load operation to
look up the data object it belongs to in the map. If the value of
the current load matches the one recorded with the previous
load on the same object, L OAD SPY records an instance of
spatial load redundancy. The redundancy is hierarchically
attributed ﬁrst to the data object involved and then to the two
calling contexts involved in the redundancy.
LOAD SPY provides the similar whole-program and per-
redundancy-pair metrics for spatial redundancy. Moreover,
LOAD SPYcomputes the per-data-object metrics with the fol-
lowing equations where Ois a data object.
Rprecise
O=Redundant non-FP bytes in object O/summationtext
inon-FP bytes in object i
Rapprox
O=Redundant FP bytes in object O/summationtext
iFP bytes in object i(3)Obtaining Data-object Addresses at Runtime: LOAD SPY
monitors static and dynamic data objects but ignores stack
objects from spatial redundancy detection. Data allocated in
the.bss section in a load module are static objects. Each
static object has a named entry in the symbol table that
identiﬁes the memory range for the object with an offset
from the beginning of the load module. The lifetime of static
objects begins when the enclosing load module (executable
or dynamic library) is loaded into memory and ends when the
load module is unloaded. L OAD SPYintercepts the loading and
unloading of load modules to monitor the lifetime of static data
objects and establishes a mapping from an object’s address
range to the corresponding data object.
Dynamic objects are allocated via one of malloc family
of functions ( malloc ,calloc ,realloc ) and mmap [63].
The memories for dynamic objects are reclaimed at free and
munmap .L OAD SPYintercepts these functions to establish a
mapping from an object’s address range to the corresponding
data object. Querying an address at runtime obtains a handle
to the corresponding static or dynamic object. The handle is
a unique identiﬁer representing the object name for a static
object or the allocation calling context for a dynamic object.
C. Identifying the Redundancy Scope
When the redundancy happens in the same calling context,
that isCold=Cnew, there is guaranteed to be a loop1around
the redundancy location. However, in code with nested loops, it
is unclear whether the redundancy occurred between iterations
of an inner loop or between iterations of an outer loop or some
other loop in-between. Hence, it becomes necessary to point
out the syntactic scope enclosing a redundancy pair.
We illustrate the need for scope using a real-world appli-
cation MASNUM-2.2 [65] shown on the left of Listing 9.
LOAD SPY identiﬁes 91% of memory loads are redundant
and the top contributor is at line 6. It is tempting to infer
thatx(iii+1) loaded in one iteration of the inner doloop
(line 5) is loaded again as x(iii) in the next iteration. An
obvious optimization is to perform scalar replacement to retain
x(iii+1) across iterations of the inner doloop (on the right
of Listing 9). However, this optimization does not eliminate
many redundant loads. Actually, the outer do loop at line 1
repeatedly searches for an item xx, and the inner do loop
performs a linear search. As a result, the inner loop repeatedly
loads the same set of elements across two trips of the outer
loop. Thus, the load redundancy exists not only between
iterations of the inner loop but also between iterations of the
outer loop. The load redundancy at the outer loop highlights
an algorithm-level inefﬁciency—repeated linear searches. With
this knowledge, we can replace the linear search with a binary
search. More details are shown in § VII-B.
To assist developers to focus on the scope where load
redundancy occurs, we have incorporated a redundancy scope
feature in L OAD SPY. We denote redundancy scope with the
symbolS. In Listing 9, the redundancy scope is the outer do
loop. Below we detail how redundancy scope is computed.
1We consider natural loops [64] only.
987
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 09:39:37 UTC from IEEE Xplore.  Restrictions apply. 1do500 k=1, kl
2...
3xx=x0-deltt *(cgx+ux(ia,ic)
)/rslat(ic) *180./pi
4...
5doiii = ixs, ixl-1
6/trianglerightsldif(xx >= x(iii) .and. xx
<= x(iii+1)) then
7 ixx = iii; exit
8 endif
9enddo
10 ...
11500 continue1do500 k=1, kl
2scalar = x(ixs)
3doiii = ixs, ixl-1
4 if(xx >= scalar) then
5 scalar = x(iii+1)
6 if(xx <= scalar) then
7 ixx = iii; exit
8 endif
9 else scalar = x(iii+1)
10 endif
11 enddo
12 ...
13500 continue
Listing 9: A code example (on the left) from MASNUM-2.2 [65] that
requires additional information for disambiguating the scope of load
redundancy. Many redundant loads occur at line 6 where the array
xis repeatedly loaded from memory. If we only focus on the inner
loop, we would be misled to believe the stencil computation, which
loads x(iii+1) andx(iii) , causes many redundant loads across
iterations of the inner loop. However, performing scalar replacement
(on the right) does not yield much speedup. An algorithmic-level
redundancy happens in the outer doloop, which repeatedly performs
linear searches for a sorted array of elements.
1main () {
2// loop1
3for (i=0; i<M; i++) {
4 // loop2
5 for (k=0; k<N; k++) {
6 // load from B[i]
7 t += B[i];
8}}}
Listing 10: Redundancy in the
inner loop scope.1main () {
2// loop1
3for (i=0; i<M; i++) {
4 // loop2
5 for (k=0; k<N; k++) {
6 // load from A[k]
7 t += A[k];
8}}}
Listing 11: Redundancy in the
outer loop scope.
We ﬁrst extend calling contexts to incorporate loop infor-
mation. Thus, the calling context of a load operation looks as
follows:main ()→loop 1→f()→...→loop n→load old.
Additionally, L OAD SPYmaintains a 64-bit global timestamp
counterTthat is incremented when passing through each loop
header and also through each load operation. Thus, the calling
context snapshot may appear as follows: Cold=main ()→
loop 1[T=1 ]→f()→...→loop n[T=9 ]→load old.
We extend the calling context Eto be a tuple, that is,
Eold=/angbracketleftpointer to old context, Told/angbracketright=/angbracketleftCold,10/angbracketright.
Listing 10 shows a simpliﬁed example, where the redun-
dancy happens in the inner loop (scope is loop 2). In this
setting, consider the following pair of calling context snapshot:
Eold=/angbracketleftmain ()→loop 1[T=1 ]→loop 2[T=2 ]→load old,Told=3/angbracketright
Enew =/angbracketleftmain ()→loop 1[T=1 ]→loop 2[T=4 ]→load new,Tnew =5/angbracketright
Notice that the counter associated with loop 1has remained
unchanged whereas the counter associated with loop 2has
changed. Each load maintains a pointer to the calling context,
not the entire calling context snapshot. Hence, by the time
the redundancy is detected, that is, load new is executed,
loop 2[T=2 ] would have gotten updated to loop 2[T=4 ] ;
traversing Coldwould ﬁnd Tloop 2=4 . Observe that Told<
Tloop 2<Tnew. This invariant informs that loop 2is the scope
inside which the redundancy is happening. The same invariant
does not hold for Tloop 1.
Now, consider a simpliﬁed example in Listing 11, where
redundancy happens in the outer loop (scope is loop 1). In this
setting, consider the following pair of calling context snapshot:
Eold=/angbracketleftmain ()→loop 1[T=1 ]→loop 2[T=2 ]→load old,Told=3/angbracketright
Enew =/angbracketleftmain ()→loop 1[T=8 ]→loop 2[T=9 ]→load new,Tnew =1 0/angbracketrightNotice that the counter associated with both loop 1and
loop 2have changed. Hence, by the time load new is executed,
loop 1[T=1 ] andloop 2[T=2 ] would have gotten updated
toloop 1[T=8 ] andloop 2[T=9 ] , respectively; traversing
Cold would ﬁnd Tloop 1=8 andTloop 2=9 . Observe that
Told<Tloop 1<Tloop 2<Tnew. The loop with the smallest T
value obeying this invariant, that is loop 1, is the redundancy
scope. If there was another enclosing loop, say loop 0, its
counter would not have obeyed this invariant.
Claim 1. Given a redundancy context pair
/angbracketleft/angbracketleftC,Told/angbracketright,/angbracketleftC,Tnew/angbracketright/angbracketright, the redundancy scope S is
the outermost enclosing loop iinC such that
Told<Tloop i<Tnew.
Proof. First,Tloop imust be in the range of (Told,Tnew)be-
cause loop iis the redundancy scope; otherwise, loop icannot
enclose the redundant load instances. Next, assume there exists
another loop jinCsuch that Told<Tloop j<Tloop i<Tnew
but loop jis not the redundancy scope. Loop iandjcannot
be the peer loops because they are both in the same context C.
Then one loop must enclose the other. (1) If loop iencloses
loopj,Tloop i<Tloop jbecause loop j’s counter is incremented
at least once after loop i’s counter is incremented, which
contradicts the assumption that Tloop j<Tloop i. Hence, loop
jcannot be nested inside loop i. (2) If loop jencloses
loopi, then loop iis no longer the outermost loop with
Told<Tloop i<Tnew. Hence, loop jcannot be enclosing
loopi. Since loop iand loop jare neither peer loops, nor
can they be nested within one another, the assumption is void.
Thus, Claim 1 holds. /squaresolid
Implementing Redundancy Scope: LOAD SPY combines
static and dynamic analysis to compute the redundancy scope
Sfor each redundancy pair. First, L OAD SPYinstruments each
loop header in the binary (in addition to procedures) to produce
calling contexts with augmented loop information. It identiﬁes
an instruction as a loop header by performing an interval
analysis [66] on the binary code and integrates the information
into the procedure call path. We refer to the calling contexts
with loop information as extended contexts . A runtime analysis
routine run as a part of each loop header increments the 64-
bit timestamp counter T. The analysis routine run as a part
of each load instruction also increments the counter T. Also,
the shadow memory for each byte of the original program
is extended to hold the counter T(in addition to the 32-bit
calling context handle and the 8-bit old value).
On each detected load redundancy, where Cold=Cnew,
LOAD SPYsearches the call path from root ( main) toward
the leaf (the load instruction) to look for the ﬁrst loop node
where the Claim 1 is found to be true. Such a loop is the re-
dundancy scope Sfor the current instance of load redundancy.
Each redundancy instance records the triplet /angbracketleftCold,Cnew,S/angbracketright.
IfCold/negationslash=Cnew,L OAD SPY ﬁrst ﬁnds the lowest common
ancestor (LCA) function or loop enclosing Cold andCnew,
and then searches their common call path from root (main)
toward the LCA to obtain Sbased on the Claim 1.
988
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 09:39:37 UTC from IEEE Xplore.  Restrictions apply. Computing the redundancy scope for each redundancy in-
stance introduces heavy runtime overhead. We compute the
redundancy scope for a given calling context pair only a
threshold number of times (one in our experiments), which
is good enough for most programs.
D. Handling Threaded Programs
LOAD SPYmaintains per-thread data structures: calling con-
text trees, redundancy proﬁles, T, among others and hence
needs no concurrency control for multi-threaded programs.
The runtime object map is maintained as a lock-free map
allowing concurrent lookups. L OAD SPY detects only intra-
thread redundancy and ignores inter-thread redundancy, if any.
E. Reducing Proﬁling Overhead
LOAD SPYcan introduce relatively high runtime overhead,
∼40-150×.L OAD SPY adopts a bursty sampling mecha-
nism to control its overhead [67]. Bursty sampling involves
continuous monitoring for a certain number of instructions
(WINDOW_ENABLE ) followed by not monitoring for a certain
(larger) number of instructions ( WINDOW_DISABLE ) and
repeating it over time. These two thresholds are tunable. From
our experiments, 1% sampling rate with WINDOW_ENABLE =1
million and WINDOW_DISABLE =99 million yields a good
tradeoff between overhead and accuracy.
F . Discussions
It is worth noting that there is no one-one relationship be-
tween the redundancy fraction and potential performance gains
because of pipelining, caching and prefetching in hardware.
LOAD SPYdoes not distinguish actionable vs. non-actionable
redundancies, which is a topic of our future work.
V. L OAD SPYWORKFLOW
LOAD SPY consists of three components: a runtime pro-
ﬁler (detailed previously in § IV), an analyzer, and a GUI.
LOAD SPY accepts fully optimized binary executables and
collects runtime proﬁles via its online proﬁler. The analyzer
and GUI, run in a postmortem fashion, consume the runtime
proﬁles and associate them with the application source code.
The rest of this section discusses the analyzer and GUI.
A.LOAD SPY’s Analyzer
LOAD SPY’s analyzer associates the runtime proﬁles with
source code based on the DW ARF [68] information produced
by compilers. As the proﬁler produces per-thread proﬁles, the
analyzer needs to coalesce the proﬁles for the whole execution.
The calling context proﬁles scale the analysis of program
execution to a large number of cores. The coalescing procedure
follows the rule: two redundancy pairs from different threads
are merged iff they have the same redundant loads in the
same contexts with the same redundancy scope. All the metrics
are also merged to compute uniﬁed ones across threads. The
scheme is similar for proﬁles from different processes.
It is worth noting that the proﬁle coalescing overhead grows
linearly with the number of threads and processes used by the
monitored program. L OAD SPY leverages the reduction treetechnique [69] to parallelize the merging process. Typically,
LOAD SPYtakes less than one minute to produce the aggregate
proﬁles in all of our case studies.
B.LOAD SPY’s GUI
LOAD SPY’s GUI inherits the design of an existing Java-
based graphical interface [19], which enables navigating the
calling contexts and the corresponding source code ordered
by the monitored metrics. A top-down view shows a call
pathCstarting from main to a leaf function with the
breakdown of metrics at each level. Merely attributing a metric
to two independent contexts loses the association between two
related contexts during postmortem inspection. To correlate the
source with the target, L OAD SPYallows appending a copy of
the target calling context to the source calling context. For
example, if a load in context main->A->B is redundant with
another load in context main->C->D ,L OAD SPYconstructs
a synthetic calling context: main->A->B->main->C->D .
The redundancy metrics will be attributed to the leaf of this
call chain. These synthetic call chains make it easy to visually
navigate proﬁles and focus on top redundancy pairs. Figure 2
in § VII-A shows an example of the GUI, and we postpone
the explanation of the GUI details to that section.
VI. E V ALUA TION
We evaluate L OAD SPYon a 12-core Intel Xeon E5-2650
v4 CPU (Broadwell) of 2.20GHz frequency running Linux
4.8.0. The machine has 256GB main memory. We evalu-
ate L OAD SPYwith well-known benchmarks, such as SPEC
CPU2006 [53], SPEC OMP2012 [70], SPEC CPU2017 [71],
Parsec-2.1 [54], Rodinia-3.1 [55], NERSC-8 [56], and Stamp-
0.9.10 [72], as well as several real-world applications, such as
Apache Avro-1.8.2 [73], Hoard-3.12 [74], MASNUM-2.2 [65],
Shogun-6.0 [75], USQCD Chroma-3.43 [76], Stack RNN [77],
Binutils-2.27 [78], and Kallisto-0.43 [79]. All the programs are
compiled with gcc-4.8.5 -O3 PGO except Hoard-3.12
and MASNUM-2.2. For Hoard-3.12 we use clang-5.0.0
-O3 PGO and for MASNUM-2.2 we use icc-17.0.4 -O3
PGO . We apply the ref inputs for SPEC CPU2006, OMP2012
and CPU2017 benchmarks, the native inputs for Parsec-2.1
benchmarks, and the default inputs released with the remaining
benchmarks and applications if not speciﬁed. We run all the
parallel programs with four threads with simultaneous multi-
threading (SMT) disabled.
In the rest of this section, we ﬁrst show the fraction
of temporal and spatial redundancies obtained from SPEC
CPU2006. We then evaluate the overhead of L OAD SPYwith
bursty sampling enabled. We exclude three benchmarks—
gobmk, sjeng, and xalancbmk—from monitoring because they
have deep call recursion causing L OAD SPY to run out of
memory. We post the evaluation on the accuracy of L OAD SPY
with bursty sampling enabled on arXiv [80].
a)Load redundancy in macro benchmarks :Figure 1
shows the fraction of temporal and spatial load redundan-
cies on SPEC CPU2006. We can see (1) load redundancy,
especially the temporal one, pervasively exists and (2) integer
989
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 09:39:37 UTC from IEEE Xplore.  Restrictions apply. SPEC CPU2006 INT SPEC CPU2006 FPRedundancy (%)
0 2 55 07 5 100
perlbench
bzip2
gcc
mcf
hmmer
libquantum
h264ref
omnetpp
astar
bwaves
gamess
milc
zeusmp
gromacs
cactusADM
leslie3d
named
dealII
soplex
povray
calculix
GemsFDTD
tonto
lbm
wrf
sphinx3
GeoMeanPrecise Redundancy Approximate Redundancy
(a)Temporal redundancies.SPEC CPU2006 INT SPEC CPU2006 FPRedundancy (%)
0 1 53 04 56 0
perlbench
bzip2
gcc
mcf
hmmer
libquantum
h264ref
omnetpp
astar
bwaves
gamess
milc
zeusmp
gromacs
cactusADM
leslie3d
named
dealII
soplex
povray
calculix
GemsFDTD
tonto
lbm
wrf
sphinx3
GeoMeanPrecise Redundancy Approximate Redundancy
(b) Spatial redundancies.
Fig. 1: Fraction of temporal and spatial load redundancies on SPEC CPU2006.
BenchmarksDetecting Temporal Redundancy Detecting Spatial Redundancy
Runtime Slowdown Memory Bloat Runtime Slowdown Memory Bloat
perlbench 38× 11× 51× 7×
bzip2 13× 2× 13× 1.09×
gcc 19× 26× 19× 25×
mcf 6× 14× 6× 1.04×
hmmer 12× 35× 11× 20×
libquantum 12× 18× 13× 2×
h264ref 21× 20× 21× 2×
omnetpp 10× 16× 14× 25×
astar 11× 13× 11× 18×
bwaves 17× 14× 15× 1.16×
gamess 24× 25× 24× 24×
milc 4× 10× 4× 1.18×
zeusmp 8× 14× 7× 1.42×
gromacs 10× 23× 9× 15×
cactusADM 7× 10× 7× 1.36×
leslie3d 9× 10× 8× 2×
named 10× 11× 10× 9×
dealII 21× 30× 22× 19×
sople x 13× 13× 13× 2×
povray 29× 216× 28× 70×
calculix 21× 18× 20× 19×
GemsFDTD 8× 14× 8× 1.42×
tonto 22× 49× 24× 30×
lbm 4× 14× 3× 1.15×
wrf 15× 10× 16× 3×
sphinx3 13× 16× 13× 7×
Median 12.5× 14× 13× 5×
GeoMean 13× 17× 13× 5×
TABLE I: LOAD SPY’s runtime slowdown and memory bloat over
native execution on SPEC CPU2006.
benchmarks show a high proportion of precise redundant loads
whereas ﬂoating-point benchmarks show a high proportion of
approximate redundant loads, as expected.
b)Overhead :Table I shows the runtime slowdown and
memory bloat of L OAD SPYon SPEC CPU2006. The runtime
slowdown (memory bloat) is measured as the ratio of the
runtime (peak memory usage) of a benchmark with L OAD SPY
enabled to the runtime (peak memory usage) of its native
execution. The geo-means of runtime slowdown for detecting
temporal and spatial redundancies are both 13 ×, and the geo-
means of memory bloat for detecting temporal and spatial
redundancies are 17 ×and 5×, respectively. A few benchmarks
such as tonto andpovray show excessive memory bloat
due to the following reasons: (1) tonto has a deep call stack,
which demands excessive space to maintain its calling context
tree and (2) povray has a small ( ∼6MB) memory footprint,
whereas some preallocated data structures in L OAD SPYover-
shadow this baseline memory footprint.
VII. C ASE STUDIES
We evaluate the load redundancies found in some bench-
marks and real-world applications. Table II summarizes the
inefﬁciencies found and the speedups obtained by eliminat-
ing them. We quantify the performance of all programs in
execution time except Hoard in throughput. In the rest of
this section, we exhaustively analyze the performance bugsinvolved in Apache Avro-1.8.2 and MASNUM-2.2. We detail
other newfound performance bugs on arXiv [80].
A. Apache Avro-1.8.2
Avro [73] is a remote procedure call (RPC) and data serial-
ization processing system. We apply L OAD SPY to evaluate
the C++ version of Avro with benchmarks developed by
Sorokin [81]. L OAD SPYreports a temporal redundancy frac-
tionRprecise
prog of 79% for the entire program. Figure 2 shows
the full calling contexts of the top redundancy pair visualized
through L OAD SPY’s GUI. L OAD SPY’s GUI consists of three
panes: the top pane shows the program source code, the bottom
left pane shows the full calling contexts of each redundancy
pair, and the bottom right pane shows the metrics associated
with each redundancy pair. In this ﬁgure, the GUI shows two
metrics: the number of redundant loads for a given redundancy
pair and percentage of redundant instances for a given pair,
which if 100%, means every instance of this pair is redundant.
From the ﬁgure, we can see that the redundant loads
in function doEncodeLong account for 25% of the total
redundant loads in the program. Moreover, all instances of this
pair are redundant. The redundancy scope of this pair is the
loop at lines 229-233 in the ﬁle Specific.hh enclosing the
call site of function encode . Function encode is the caller
of function doEncodeLong . With further analysis, we ﬁnd
that the epilog of function doEncodeLong consistently pops
the same values from the same stack location to restore the
register values. To eliminate redundant loads in the function
epilog, we inline doEncodeLong into its caller. L OAD SPY
further identiﬁes another problematic function (not shown)
and guides the same inlining optimization. Together, these
optimizations eliminate 31% of the memory loads and 37%
of the redundant memory loads, yielding a 1.19 ×speedup for
the whole program.
B. MASNUM-2.2
MASNUM [65], one of the 2016 ACM Gordon Bell Prize
ﬁnalists, forecasts ocean surface waves and climate change.
It is written in Fortran and parallelized with MPI. L OAD SPY
identiﬁes 91% of memory loads are redundant, of which 15%
are attributed to the array xat line 6 on the left of Listing 9.
LOAD SPYalso pinpoints the redundancy scope as the outer-
most loop at line 1. We ﬁnd that the innermost loop (line 5)
performs a linear search over the non-decreasing array xfor a
given input xx. With multiple iterations, elements of array x
990
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 09:39:37 UTC from IEEE Xplore.  Restrictions apply. Program Information LOADSPY Optimization
Programs Problematic Code Redundanc y Types Inef ﬁciencies Approaches WS∗Macro Benchmarks359.botsspar sparselu.c:loop(191) Temporal Inef ﬁcient register usage Scalar replacement 1.77×
453.po vray csg.cpp(250) Temporal Missing inline substitution Function inlining 1.05×
464.h264ref mv-search.c:loop(394) Temporal Missing inline substitution Function inlining 1.28×
470.lbm lbm.c:LBM performStreamCollide Spatial Redundant computation Approximate computing 1.25×
538.imagick r morphology .c:loop(2982) Spatial Redundant computation Conditional computation 1.25×
backprop backprop.c:loop(322) Spatial Input-sensiti ve redundancy Conditional computation 1.13×
hotspot3D 3D.c:loop(98, 166) Temporal Inef ﬁcient register usage Scalar replacement 1.13×
lavaMD kernel cpu.c(175) Temporal Redundant function calls Reusing the previous result 1.39×
srad v1 main.c:loop(256) Temporal Inef ﬁcient register usage Scalar replacement 1.11×
srad v2 srad.cpp:loop(131) Temporal Inef ﬁcient register usage Scalar replacement 1.12×
particleﬁlter exparticle OPENMP seq.c:ﬁndInde x Temporal Linear search Binary search 9.8×
vacation client.c:loop(198) Temporal Redundant function calls Reusing the previous result 1.23×
dedup hashtable.c:hashtable search Temporal Poor hashing Reducing hash collisions 1.11×
msgrate msgrate.c:cache invalidate Temporal Missing constant propagation Cop y propagation 3.03×Real ApplicationsApache Avro-1.8.2 Speciﬁc.hh(110, 117) Temporal Missing inline substitution Function inlining 1.19×
Hoard-3.12 libhoard.cpp:xxmalloc Temporal Redundant computation Reusing the previous result 1.14×
MASNUM-2.2 propag at.inc:loop(130, 140) Temporal Linear search Locality-friendly search 1.79×
USQCD Chroma-3.43 qdp random.h(56) Temporal Missing inline substitution Function inlining 1.06×
Shogun-6.0DenseFeatures.cpp(505)
Distance.cpp(185)Temporal Missing inline substitution Function inlining 1.06×
Stack RNN StackRNN.h:loop(350, 355, 363, 367)Temporal
SpatialPoor choice of algorithm
Redundant computationLoop fusion
Conditional computation1.09×
Kallisto-0.43 KmerHashT able.h(131) Temporal Poor hashing Reducing hash collisions 4.1×
Binutils-2.27 dwarf2.c:loop(2166) Temporal Linear search Binary search 3.29×
:newfound performance bugs via L OAD SPY.
WS∗: whole-program speedup after problem elimination.
TABLE II: Overview of performance improvement guided by L OAD SPY.



	



	




		
Fig. 2: The top redundancy pair in Avro with full calling contexts
reported by L OAD SPY. Along the calling contexts shown in the
bottom left pane, a procedure name following a symbol [I] means
it is inlined. We can see that most procedures on the path are inlined,
except doEncodeLong . Many redundant loads are from calling
doEncodeLong , which can be removed by function inlining.
are frequently loaded from memory for comparison, leading to
the redundancy. Changing the linear search to a binary search
reduces redundant loads and yields a 1.32 ×speedup for the
entire program. It is worth noting that the binary search still
incurs high load redundancy fraction because of the intensive
search requests in the program. To further improve the search
algorithm, we analyze the values of xxacross iterations. We
ﬁnd that xx has good value locality, that is, the values are
similar in adjacent iterations of the outermost loop. Thus,
we replace the binary search with a locality-friendly search.
We memoize the location index iii when the current search
ﬁnishes; in the next search, we begin at the recorded iii and
alternate the linear search in both directions to the array startand end. This optimization eliminates 33% of the memory
loads and 36% of the redundant memory loads, yielding a
1.79×speedup for the entire program.
VIII. T HREA TS TO V ALIDITY
The threats mainly exist in applying L OAD SPYfor code
optimization. The same optimization for one application may
show different speedups on different computer architectures.
A given load redundancy fraction may not help estimate the
potential speedup. Some optimizations are input-speciﬁc, and
a different proﬁle may demand a different optimization. Based
on the reported inefﬁciencies, programmers need to devise an
optimization that is safe in any execution.
IX. C ONCLUSIONS
In this paper, we presented a study of identifying program
inefﬁciencies by focusing on whole-program load redundancy.
We demonstrate that redundant load operations are often a
symptom of various inefﬁciencies arising from inputs, subopti-
mal data structure and algorithm choices, and missed compiler
optimizations. To pinpoint these inefﬁciencies in complex soft-
ware code bases, we have developed L OAD SPY, a ﬁne-grained
proﬁler that proﬁles load redundancy. L OAD SPY toolchain
provides valuable guidance to developers for code tuning—
calling contexts of the two parties involved in a redundancy,
narrowed-down redundancy scopes to focus on optimization,
metrics to understand relative signiﬁcance of redundancy, and
a GUI for the source code attribution. We evaluate L OAD SPY
using several benchmarks and real-world applications. Guided
by L OAD SPYwe are able to optimize prior-known and new
inefﬁciencies in several programs. Eliminating temporal and
spatial load redundancies resulted in nontrivial speedups.
ACKNOWLEDGMENT
We thank reviewers for their valuable comments. This work
is supported by Google Faculty Research Award and National
Natural Science Foundation of China (No. 61502019).
991
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 09:39:37 UTC from IEEE Xplore.  Restrictions apply. REFERENCES
[1] I. Molyneaux, The Art of Application Performance Testing: Help for
Programmers and Quality Assurance , 1st ed. O’Reilly Media, Inc.,
2009.
[2] R. E. Bryant and D. R. O’Hallaron, Computer Systems: A Programmer’s
Perspective , 2nd ed. USA: Addison-Wesley Publishing Company, 2010.
[3] J. A. Butts and G. Sohi, “Dynamic Dead-instruction Detection and
Elimination,” in Proceedings of the 10th International Conference on
Architectural Support for Programming Languages and Operating Sys-
tems , 2002, pp. 199–210.
[4] J. S. Seng and D. M. Tullsen, “Architecture-level power optimization—
what are the limits?” J. Instruction-Level Parallelism , vol. 7, 2005.
[5] S. Wen, X. Liu, and M. Chabbi, “Runtime V alue Numbering: A Proﬁling
Technique to Pinpoint Redundant Computations,” in Proceedings of the
2015 International Conference on Parallel Architecture and Compilation
(PACT) , ser. PACT ’15. Washington, DC, USA: IEEE Computer
Society, 2015, pp. 254–265.
[6] M. Chabbi and J. Mellor-Crummey, “DeadSpy: A Tool to Pinpoint
Program Inefﬁciencies,” in Proceedings of the Tenth International Sym-
posium on Code Generation and Optimization , ser. CGO ’12. New
Y ork, NY , USA: ACM, 2012, pp. 124–134.
[7] K. M. Lepak and M. H. Lipasti, “On the V alue Locality of Store Instruc-
tions,” in Proceedings of 27th International Symposium on Computer
Architecture (IEEE Cat. No.RS00201) , Jun 2000, pp. 182–191.
[8] X. Liu and J. Mellor-Crummey, “Pinpointing data locality bottlenecks
with low overhead,” in 2013 IEEE International Symposium on Per-
formance Analysis of Systems and Software (ISPASS) , April 2013, pp.
183–193.
[9] G. Marin and J. Mellor-Crummey, “Pinpointing and Exploiting Oppor-
tunities for Enhancing Data Reuse,” in IEEE Intl. Symposium on Perfor-
mance Analysis of Systems and Software , ser. ISPASS ’08. Washington,
DC, USA: IEEE Computer Society, 2008, pp. 115–126.
[10] S. Wen, M. Chabbi, and X. Liu, “Redspy: Exploring value local-
ity in software,” in Proceedings of the Twenty-Second International
Conference on Architectural Support for Programming Languages and
Operating Systems , ser. ASPLOS ’17. New Y ork, NY , USA: ACM,
2017, pp. 47–61.
[11] M. Chabbi, W. Lavrijsen, W. de Jong, K. Sen, J. Mellor-Crummey,
and C. Iancu, “Barrier Elision for Production Parallel Programs,” in
Proceedings of the 20th ACM SIGPLAN Symposium on Principles and
Practice of Parallel Programming , ser. PPoPP 2015. New Y ork, NY ,
USA: ACM, 2015, pp. 109–119.
[12] N. R. Tallent, J. M. Mellor-Crummey, and A. Porterﬁeld, “Analyz-
ing Lock Contention in Multithreaded Applications,” SIGPLAN Not. ,
vol. 45, no. 5, pp. 269–280, Jan. 2010.
[13] S. J. Deitz, B. L. Chamberlain, and L. Snyder, “Eliminating Redundan-
cies in Sum-of-product Array Computations,” in Proceedings of the 15th
International Conference on Supercomputing , ser. ICS ’01. New Y ork,
NY , USA: ACM, 2001, pp. 65–77.
[14] B. K. Rosen, M. N. Wegman, and F. K. Zadeck, “Global V alue
Numbers and Redundant Computations,” in Proceedings of the 15th
ACM SIGPLAN-SIGACT Symposium on Principles of Programming
Languages , 1988, pp. 12–27.
[15] M. N. Wegman and F. K. Zadeck, “Constant Propagation with Condi-
tional Branches,” ACM Trans. Program. Lang. Syst. , vol. 13, no. 2, pp.
181–210, Apr 1991.
[16] M. F. Fern ´andez, “Simple and Effective Link-time Optimization of
Modula-3 Programs,” in Proceedings of the ACM SIGPLAN 1995
Conference on Programming Language Design and Implementation , ser.
PLDI ’95. New Y ork, NY , USA: ACM, 1995, pp. 103–115.
[17] T. Johnson, M. Amini, and X. D. Li, “Thinlto: Scalable and incremental
lto,” in Proceedings of the 2017 International Symposium on Code
Generation and Optimization , ser. CGO ’17. Piscataway, NJ, USA:
IEEE Press, 2017, pp. 111–121.
[18] A. Srivastava and D. W. Wall, “A practical system for intermodule code
optimization at link-time,” Journal of Programming Languages , vol. 1,
no. 1, pp. 1–18, Dec. 1992.
[19] L. Adhianto, S. Banerjee, M. Fagan, M. Krentel, G. Marin, J. Mellor-
Crummey, and N. R. Tallent, “HPCToolkit: Tools for Performance
Analysis of Optimized Parallel Programs,” Concurrency Computation
: Practice Expererience , vol. 22, no. 6, pp. 685–701, Apr 2010.
[20] “Intel VTune,” https://software.intel.com/en-us/intel-vtune-ampliﬁer-xe,
2018.[21] Linux, “Linux perf tool,” https://perf.wiki.kernel.org/index.php/Main
Page, 2015.
[22] S. L. Graham, P . B. Kessler, and M. K. Mckusick, “Gprof: A Call Graph
Execution Proﬁler,” in Proceedings of the 1982 SIGPLAN Symposium
on Compiler Construction , ser. SIGPLAN ’82. New Y ork, NY , USA:
ACM, 1982, pp. 120–126.
[23] J. Levon et al. , “OProﬁle,” http://oproﬁle.sourceforge.net, 2017.
[24] L. DeRose, B. Homer, D. Johnson, S. Kaufmann, and H. Poxon, “Cray
performance analysis tools,” in Tools for High Performance Computing .
Springer Berlin Heidelberg, 2008, pp. 191–199.
[25] A. Nistor, L. Song, D. Marinov, and S. Lu, “Toddler: Detecting per-
formance problems via similar memory-access patterns,” in 2013 35th
International Conference on Software Engineering (ICSE) , May 2013,
pp. 562–571.
[26] S. Pop, A. Cohen, C. Bastoul, S. Girbal, G.-A. Silber, and N. V asi-
lache, “Graphite: Polyhedral analyses and optimizations for GCC,” in
Proceedings of the 2006 GCC Developers Summit , 2006, p. 2006.
[27] K. Cooper, J. Eckhardt, and K. Kennedy, “Redundancy elimination
revisited,” in Proceedings of the 17th International Conference on
Parallel architectures and compilation techniques , 2008, pp. 12–21.
[28] Y . Luo and G. Tan, “Optimizing Stencil Code via Locality of Computa-
tion,” in Proceedings of the 23rd International Conference on Parallel
Architectures and Compilation , 2014, pp. 477–478.
[29] R. Hundt, E. Raman, M. Thuresson, and N. V achharajani, “MAO –
An Extensible Micro-architectural Optimizer,” in Proceedings of the 9th
Annual IEEE/ACM International Symposium on Code Generation and
Optimization , ser. CGO ’11. Washington, DC, USA: IEEE Computer
Society, 2011, pp. 1–10.
[30] Y . Ding and X. Shen, “Glore: Generalized loop redundancy elimination
upon ler-notation,” Proc. ACM Program. Lang. , vol. 1, no. OOPSLA,
pp. 74:1–74:28, Oct. 2017.
[31] Y . Ding, L. Ning, H. Guan, and X. Shen, “Generalizations of the theory
and deployment of triangular inequality for compiler-based strength
reduction,” in Proceedings of the 38th ACM SIGPLAN Conference on
Programming Language Design and Implementation , ser. PLDI 2017.
New Y ork, NY , USA: ACM, 2017, pp. 33–48.
[32] M. H. Lipasti, C. B. Wilkerson, and J. P . Shen, “V alue Locality and
Load V alue Prediction,” in Proceedings of the Seventh International
Conference on Architectural Support for Programming Languages and
Operating Systems , ser. ASPLOS VII. New Y ork, NY , USA: ACM,
1996, pp. 138–147.
[33] M. H. Lipasti and J. P . Shen, “Exceeding the Dataﬂow Limit via V alue
Prediction,” in Proceedings of the 29th Annual ACM/IEEE International
Symposium on Microarchitecture , ser. MICRO 29. Washington, DC,
USA: IEEE Computer Society, 1996, pp. 226–237.
[34] K. M. Lepak and M. H. Lipasti, “Silent Stores for Free,” in Proceedings
of the 33rd Annual ACM/IEEE International Symposium on Microarchi-
tecture , ser. MICRO 33. New Y ork, NY , USA: ACM, 2000, pp. 22–31.
[35] J. S. Miguel, M. Badr, and N. E. Jerger, “Load V alue Approximation,”
inProceedings of the 47th Annual IEEE/ACM International Symposium
on Microarchitecture , ser. MICRO-47. Washington, DC, USA: IEEE
Computer Society, 2014, pp. 127–139.
[36] J. S. Miguel, J. Albericio, A. Moshovos, and N. E. Jerger, “Doppel-
ganger: A Cache for Approximate Computing,” in Proceedings of the
48th International Symposium on Microarchitecture , ser. MICRO-48.
New Y ork, NY , USA: ACM, 2015, pp. 50–61.
[37] A. Yazdanbakhsh, G. Pekhimenko, B. Thwaites, H. Esmaeilzadeh,
O. Mutlu, and T. C. Mowry, “RFVP: Rollback-free V alue Prediction
with Safe-to-approximate Loads,” ACM Transactions on Architecture
and Code Optimization (TACO) , vol. 12, no. 4, p. 62, 2016.
[38] B. Calder, P . Feller, and A. Eustace, “V alue proﬁling,” in Proceedings of
the 30th Annual ACM/IEEE International Symposium on Microarchitec-
ture , ser. MICRO 30. Washington, DC, USA: IEEE Computer Society,
1997, pp. 259–269.
[39] ——, “V alue Proﬁling and Optimization,” Journal of Instruction Level
Parallelism , vol. 1, 1999.
[40] P . T. Feller, “V alue Proﬁling for Instructions and Memory Locations,”
Master dissertation, 1998.
[41] S. A. Watterson and S. K. Debray, “Goal-Directed V alue Proﬁling,”
inProceedings of the 10th International Conference on Compiler
Construction , ser. CC ’01. London, UK, UK: Springer-V erlag, 2001,
pp. 319–333.
992
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 09:39:37 UTC from IEEE Xplore.  Restrictions apply. [42] M. Burrows, U. Erlingsson, S.-T. A. Leung, M. T. V andevoorde, C. A.
Waldspurger, K. Walker, and W. E. Weihl, “Efﬁcient and Flexible
V alue Sampling,” in Proceedings of the Ninth International Conference
on Architectural Support for Programming Languages and Operating
Systems , ser. ASPLOS IX. New Y ork, NY , USA: ACM, 2000, pp.
160–167.
[43] J. M. Anderson, L. M. Berc, J. Dean, S. Ghemawat, M. R. Henzinger,
S.-T. A. Leung, R. L. Sites, M. T. V andevoorde, C. A. Waldspurger, and
W. E. Weihl, “Continuous Proﬁling: Where Have All the Cycles Gone?”
ACM Trans. Comput. Syst. , vol. 15, no. 4, pp. 357–390, Nov. 1997.
[44] S. Wen, X. Liu, J. Byrne, and M. Chabbi, “Watching for software inef-
ﬁciencies with witch,” in Proceedings of the Twenty-Third International
Conference on Architectural Support for Programming Languages and
Operating Systems , ser. ASPLOS ’18. New Y ork, NY , USA: ACM,
2018, pp. 332–347.
[45] R. Muth, S. A. Watterson, and S. K. Debray, “Code Specialization Based
on V alue Proﬁles,” in Proceedings of the 7th International Symposium
on Static Analysis , ser. SAS ’00. London, UK, UK: Springer-V erlag,
2000, pp. 340–359.
[46] T. Oh, H. Kim, N. P . Johnson, J. W. Lee, and D. I. August, “Practi-
cal Automatic Loop Specialization,” in Proceedings of the Eighteenth
International Conference on Architectural Support for Programming
Languages and Operating Systems , ser. ASPLOS ’13. New Y ork, NY ,
USA: ACM, 2013, pp. 419–430.
[47] E.-Y . Chung, L. Benini, and G. D. Micheli, “Energy Efﬁcient Source
Code Transformation based on V alue Proﬁling,” in PROC. INTERNA-
TIONAL WORKSHOP ON COMPILERS AND OPERATING SYSTEMS
FOR LOW POWER , 2000.
[48] T. Kamio and H. Masahura, “A V alue Proﬁler for Assisting Object-
Oriented Program Specialization,” in Proceedings of Workshop on New
Approaches to Software Construction , 2004.
[49] S. Henry, H. Bollor ´e, and E. Oseret, “Towards the Generalization of
V alue Proﬁling for High-Performance Application Optimization,” http:
//sylvain-henry.info/home/ﬁles/papers/shenry 2015 vprof.pdf, 2015.
[50] L. Della Toffola, M. Pradel, and T. R. Gross, “Performance problems
you can ﬁx: A dynamic analysis of memoization opportunities,” in
Proceedings of the 2015 ACM SIGPLAN International Conference on
Object-Oriented Programming, Systems, Languages, and Applications ,
ser. OOPSLA 2015. New Y ork, NY , USA: ACM, 2015, pp. 607–622.
[51] R. Padhye and K. Sen, “Travioli: A dynamic analysis for detecting data-
structure traversals,” in Proceedings of the 39th International Conference
on Software Engineering , ser. ICSE ’17. Piscataway, NJ, USA: IEEE
Press, 2017, pp. 473–483.
[52] L. Song and S. Lu, “Performance diagnosis for inefﬁcient loops,” in Pro-
ceedings of the 39th International Conference on Software Engineering ,
ser. ICSE ’17. Piscataway, NJ, USA: IEEE Press, 2017, pp. 370–380.
[53] SPEC Corporation, “SPEC CPU2006 benchmark suite,” http://www.
spec.org/cpu2006, 2007, 3 November 2007.
[54] C. Bienia, “Benchmarking modern multiprocessors,” Ph.D. dissertation,
Princeton University, January 2011.
[55] S. Che, M. Boyer, J. Meng, D. Tarjan, J. W. Sheaffer, S. H. Lee, and
K. Skadron, “Rodinia: A benchmark suite for heterogeneous computing,”
in2009 IEEE International Symposium on Workload Characterization
(IISWC) , Oct 2009, pp. 44–54.
[56] NERSC, “NERSC-8 / Trinity Benchmarks,” http://www.nersc.gov/
users/computational-systems/cori/nersc-8-procurement/trinity-nersc-8-
rfp/nersc-8-trinity-benchmarks, 2016.
[57] GCC Wiki, “Graphite: Gimple Represented as Polyhedra,” https://gcc.
gnu.org/wiki/Graphite, 2015.
[58] C. Lattner and V . Adve, “Llvm: A compilation framework for lifelong
program analysis & transformation,” in Proceedings of the International
Symposium on Code Generation and Optimization: Feedback-directed
and Runtime Optimization , ser. CGO ’04. Washington, DC, USA:
IEEE Computer Society, 2004, pp. 75–.
[59] Intel Corp., “Intel C++ Compilers,” https://software.intel.com/en-us/c-
compilers, 2017.
[60] C.-K. Luk, R. Cohn, R. Muth, H. Patil, A. Klauser, G. Lowney,
S. Wallace, V . J. Reddi, and K. Hazelwood, “Pin: Building customized
program analysis tools with dynamic instrumentation,” in Proceedings of
the 2005 ACM SIGPLAN Conference on Programming Language Designand Implementation , ser. PLDI ’05. New Y ork, NY , USA: ACM, 2005,
pp. 190–200.
[61] M. Chabbi, X. Liu, and J. Mellor-Crummey, “Call paths for pin tools,”
inProceedings of Annual IEEE/ACM International Symposium on Code
Generation and Optimization , ser. CGO ’14. New Y ork, NY , USA:
ACM, 2014, pp. 76:76–76:86.
[62] G. Ammons, T. Ball, and J. R. Larus, “Exploiting hardware performance
counters with ﬂow and context sensitive proﬁling,” in Proceedings of the
ACM SIGPLAN 1997 Conference on Programming Language Design
and Implementation , ser. PLDI ’97. New Y ork, NY , USA: ACM, 1997,
pp. 85–96.
[63] X. Liu and J. Mellor-Crummey, “A data-centric proﬁler for parallel
programs,” in Proceedings of the International Conference on High
Performance Computing, Networking, Storage and Analysis , ser. SC ’13.
New Y ork, NY , USA: ACM, 2013, pp. 28:1–28:12.
[64] L. Torczon and K. Cooper, Engineering A Compiler , 2nd ed. San
Francisco, CA, USA: Morgan Kaufmann Publishers Inc., 2011.
[65] F. Qiao, W. Zhao, X. Yin, X. Huang, X. Liu, Q. Shu, G. Wang,
Z. Song, X. Li, H. Liu, G. Yang, and Y . Y uan, “A highly effective
global surface wave numerical simulation with ultra-high resolution,”
inProceedings of the International Conference for High Performance
Computing, Networking, Storage and Analysis , ser. SC ’16. Piscataway,
NJ, USA: IEEE Press, 2016, pp. 5:1–5:11.
[66] P . Havlak, “Nesting of reducible and irreducible loops,” ACM TOPLAS ,
vol. 19, no. 4, pp. 557–567, 1997.
[67] Y . Zhong and W. Chang, “Sampling-based program locality approxima-
tion,” in Proceedings of the 7th International Symposium on Memory
Management , ser. ISMM ’08. New Y ork, NY , USA: ACM, 2008, pp.
91–100.
[68] “The DW ARF Debugging Standard,” http://www.dwarfstd.org, 2012.
[69] N. R. Tallent, L. Adhianto, and J. M. Mellor-Crummey, “Scalable identi-
ﬁcation of load imbalance in parallel executions using call path proﬁles,”
inProceedings of the 2010 ACM/IEEE International Conference for
High Performance Computing, Networking, Storage and Analysis , ser.
SC ’10. Washington, DC, USA: IEEE Computer Society, 2010, pp.
1–11.
[70] SPEC Corporation, “SPEC OMP2012 benchmark suite,” https://www.
spec.org/omp2012/, 2015, may 2015.
[71] ——, “SPEC CPU2017 benchmark suite,” http://www.spec.org/cpu2017,
2017, november 29 2017.
[72] C. C. Minh, J. Chung, C. Kozyrakis, and K. Olukotun, “Stamp: Stanford
transactional applications for multi-processing,” in 2008 IEEE Interna-
tional Symposium on Workload Characterization , Sept 2008, pp. 35–46.
[73] Apache Software Foundation, “Apache avro,” https://avro.apache.org,
2017, 21 February 2018.
[74] E. D. Berger, K. S. McKinley, R. D. Blumofe, and P . R. Wilson, “Hoard:
A scalable memory allocator for multithreaded applications,” in Pro-
ceedings of the Ninth International Conference on Architectural Support
for Programming Languages and Operating Systems , ser. ASPLOS IX.
New Y ork, NY , USA: ACM, 2000, pp. 117–128.
[75] S. Sonnenburg, H. Strathmann, S. Lisitsyn, V . Gal, F. J. I. Garca,
W. Lin, S. De, C. Zhang, frx, tklein23, E. Andreev, JonasBehr, sploving,
P . Mazumdar, C. Widmer, P . D. . Zora, S. Mahindre, A. Kislay,
K. Hughes, R. V otyakov, khalednasr, S. Sharma, A. Novik, A. Panda,
E. Anagnostopoulos, L. Pang, A. Binder, serialhex, E. Srig, and B. Esser,
“shogun-toolbox/shogun: Shogun 6.0.0 - Baba Nobuharu,” Apr. 2017.
[76] R. G. Edwards and B. Joo, “The chroma software system for lattice
qcd,” Nucl. Phys. Proc. Suppl. , vol. 140, p. 832, 2005.
[77] A. Joulin and T. Mikolov, “Inferring Algorithmic Patterns with Stack-
Augmented Recurrent Nets,” ArXiv e-prints , Mar. 2015.
[78] GNU, “GNU Binutils,” https://www.gnu.org/software/binutils/, 2014,
september 2014.
[79] P . Melsted, H. Pimentel, and L. Pachter, “Near-optimal RNA-Seq
quantiﬁcation,” https://github.com/makaho/kallisto, 2014.
[80] P . Su, S. Wen, H. Yang, M. Chabbi, and X. Liu, “Redundant Loads: A
Software Inefﬁciency Indicator,” ArXiv e-prints , Feb. 2019. [Online].
Available: https://arxiv.org/abs/1902.05462
[81] K. Sorokin, “Benchmark comparing various data serialization libraries
(thrift, protobuf etc.) for C++,” https://github.com/thekvs/cpp-serializers,
2014.
993
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 09:39:37 UTC from IEEE Xplore.  Restrictions apply. 