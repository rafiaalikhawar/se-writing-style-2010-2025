A Statistics-BasedPerformance Testing MethodologyforCloud
Applications
Sen He
sen.he@utsa.edu
Universityof Texasat SanAntonio
USAGlennaManns
gmm6jd@virginia.edu
Universityof Virginia
USAJohnSaunders
js8ra@virginia.edu
Universityof Virginia
USA
Wei Wang
wei.wang@utsa.edu
Universityof Texasat SanAntonio
USALoriPollock
pollock@udel.edu
Universityof Delaware
USAMaryLouSoffa
soffa@virginia.edu
Universityof Virginia
USA
ABSTRACT
Thelowcostofresourceownershipandflexibilityhaveledusers
toincreasinglyporttheirapplicationstotheclouds.Tofullyrealize
the cost benefits of cloud services, users usually need to reliably
know the execution performance of their applications. However,
due to the random performance fluctuations experienced by cloud
applications, the black box nature of public clouds and the cloud
usagecosts,testingoncloudstoacquireaccurateperformancere-
sults is extremelydifficult. In thispaper, we presentanovel cloud
performance testingmethodology called PT4Cloud .By employing
non-parametric statistical approaches oflikelihood theory and the
bootstrap method, PT4Cloud provides reliable stop conditions to
obtain highly accurate performance distributions with confidence
bands. These statistical approaches also allow users to specify intu-
itive accuracy goals and easily trade between accuracy and testing
cost. We evaluated PT4Cloud with 33 benchmark configurations
onAmazonWebServiceandChameleonclouds.Whencompared
with performance data obtained from extensive performance tests,
PT4Cloud provides testing results with 95.4% accuracy on average
while reducing the number of test runs by 62%. We also propose
twotestexecutionreductiontechniquesfor PT4Cloud ,whichcan
reduce the number of test runs by 90.1% while retaining an aver-
age accuracy of 91%. We compared our technique to three other
techniques andfoundthat our results are muchmore accurate.
CCS CONCEPTS
·General and reference →Performance ;·Computer sys-
tems organization →Cloud computing ;·Software and its
engineering →Software testinganddebugging .
KEYWORDS
performancetesting,cloudcomputing,resourcecontention,non-
parametric statistics
Permissionto make digitalor hard copies of allorpart ofthis work for personalor
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACM
mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,
topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ESEC/FSE ’19, August 26ś30,2019, Tallinn,Estonia
©2019 Associationfor Computing Machinery.
ACM ISBN 978-1-4503-5572-8/19/08...$15.00
https://doi.org/10.1145/3338906.3338912ACMReference Format:
SenHe, Glenna Manns, John Saunders, WeiWang,Lori Pollock,and Mary
LouSoffa.2019.AStatistics-BasedPerformanceTestingMethodologyfor
Cloud Applications. In Proceedings of the 27th ACM Joint European Software
EngineeringConferenceandSymposiumontheFoundationsofSoftwareEn-
gineering (ESEC/FSE ’19), August 26ś30, 2019, Tallinn, Estonia. ACM, New
York, NY, USA, 12pages.https://doi.org/10.1145/3338906.3338912
1 INTRODUCTION
The low costs of ownership, flexibility, and resource elasticity have
prompted many organizations to shift applications to the cloud, in
particular, Infrastructure-as-a-Service (IaaS) clouds, to host their
applications[ 54].Forclouddeployments,itiscriticalthattheusers
have accurateknowledge of the performance of their applications
sothattheycanselecttheappropriatevirtualmachine(VM)config-
urationstosatisfytheirperformanceandcostobjectives[ 47,59,63].
The effectiveness of cloud elasticity policies also relies on the accu-
rateknowledge ofapplicationperformance [ 4,23,28,45,65].
The most reliable approach to determine the performance of an
application on the cloud is performance testing. To obtain accurate
results, performance testing usually has two requirements [ 12,14,
48,64]. First, thetest inputs should be accurately generated based
onthedesiredusecases.Second,itrequiresthattheperformance
of each test input is independently and accurately determined. For
betteraccuracy,acommonpracticeistoruntheapplication-under-
testwithatestinputmultipletimestoobtaintheaverageorcertain
percentiles ofits performance [ 3,14,48,53].
However,itextremelydifficulttodeterminewhenenoughper-
formance testruns areexecuted andaccurate resultsare obtained
on the cloud. For example, Figure 1gives the results of two perfor-
mancetestsofthesamebenchmark,YCSB,onthesameAmazon
Web Service (AWS) VM [ 6,20]. For each test, the same test input is
executedrepeatedlyforanextendedperiodof15hours.AsFigure 1
shows,theperformancedistributionsobtainedfromthetwotests
aredrasticallydifferent,andtheiraveragethroughputsaredifferent
by12%.Itisevidentthatthesetwotestresultscannotbebothaccu-
rate. In fact, as shown in Section 5, both results are inaccurate, and
more test runs need to be conducted to get reliable performance.
This difficultytogetreliableperformanceisalsoa majorobstacle
facedbysystemresearch[ 2].
The difficulty of getting accurate performance testing results
on the cloud is caused by cloud performance uncertainty , which
188
ESEC/FSE ’19, August 26ś30, 2019,Tallinn,Estonia Sen He,Glenna Manns,John Saunders,WeiWang,Lori Pollock, andMary Lou Soffa
17000
 18000
 19000
 20000
 21000
 22000
 23000
 24000
Throughput
0.0000
0.0001
0.0002
0.0003
0.0004
0.0005Density
Test 1
Test 2
Figure1:PerformancedistributionsofYCSBfromtwotests.
constitutestheconsiderableandunpredictableperformancefluc-
tuation of cloud applications [ 37]. Because of this uncertainty, a
new performance testing methodology is required for the cloud
to obtain accurate performance foreach testinput. In particular,a
good methodologyshouldaddressthefollowingchallenges posed
bycloud performance uncertainty.
The first challenge arises from the various factors that cause
performanceuncertainty,includinghardwareresourcecontentions
frommulti-tenancy (multiple VMssharing hardware) andtheran-
domness in VM scheduling [ 26,31,43,58]. To obtain accurate
results,performance testing ona cloud shouldcoverallthe uncer-
taintyfactors.Evenmoreimportantly,itshouldcovertheuneven
impactsofallthesefactorsproportionalto theirfrequenciesexpe-
riencedonthe clouds.
The second challenge is that cloud services are usually provided
to the users as black boxes and do not allow users to control the
execution environments. This black-box environment makes it
nearly impossible to know what uncertainty factors are covered
by a test run. Hence, it is also impossible to know exactly when
enough runs have been conducted to cover all uncertainty factors.
Thethirdchallengecomesfromthecloudusagecostincurred
by performance testing. Theoretically, executing a test input for
months can produce accurate results. However, such long tests
can incur a high cost. To minimize testing costs, a good perfor-
mancetestingmethodologyshouldstop testingimmediatelyafter
itdeterminesthat the results are accurate.
Inthispaper,wepresentanovelperformancetestingmethod-
ology called PT4Cloud for IaaS clouds. Our primary goal with
PT4Cloud istoprovidereliablestopconditionstoterminaterepeated
testrunsofatestinputtoobtainhighlyaccurateperformancere-
sult.Whileensuringhighaccuracy,oursecondgoalistoreducethe
numberoftestrunstocutdownonthecostofperformancetesting.
PT4Cloud is designed to obtain performance distributions, since in
manyusecases,itisimportanttoknowthebest-case,worst-case
and percentiles of the performance in addition to averages [ 14,64].
To determine the number of performance test runs required for
accurate result for one test input in black-box clouds, PT4Cloud
leveragestheobservationofstatisticalstability,whichstatesthat
thefrequenciesandaveragesconverge(i.e.,becomestable)given
a large number of samples [ 27]. Based on this observation, if a
performance distribution obtained from the test runs is stable, this
distribution may be deemed accurate. That is, performance test
runs can be stopped once the results are statistically stable with
the expectation that the results are accurate, and all uncertainty
factors are properly covered.
Moreover,astheperformancedistributionsofcloudapplications
do not always followknown distributions, common parametric sta-
tisticalapproaches(e.g.,Student’st-test)cannotbeusedforstability
Application-to-test and
a workloadStep 1 - 1: 
Execute tests for the app
continuously for a time 
interval I. Let the set of 
perf. data acquired
from these tests be S1.Step 1 - 2: 
Calculate performance 
distribution d1 from S1.
Step 2 - 2: 
Combine S1 and S2 into 
a new sample set S. 
Calculate performance
distribution d2 from S.Step 2 - 1: 
Execute the app for an- 
other time interval I. Let 
the set of perf. data from
these new tests be S2.
Yes No
Report d2 as
the perf test resultsStep 4 : 
Let  S1= S, and 
let d1 = d2
Step 3: 
Compare d1 and d2
to determine if
stable?
Figure 2:The workflow of PT4Cloud .
determination.Therefore, PT4Cloud employsnon-parametricsta-
tisticalapproachesfromlikelihoodtheory[ 42,57].Thesestatistical
approaches also allow the users to easily specify accuracy objec-
tivesandtradeaccuracyfortestingcost.Additionally,tohelpusers
interpret the performance testing results with more confidence,
we employ the bootstrap method to generate confidence bands for
the resulting performance distributions [ 21]. To further reduce the
testingcost, we alsoexploredtwotest reduction techniques.
Weevaluated PT4Cloud ontheChameleoncloud[ 1]andAWS[ 6],
usingsixbenchmarksrepresentingweb,database,machinelearning
andscientificapplicationsonsixVMconfigurations.Theevalua-
tion results show that the performance distributions acquired with
PT4Cloud always have an accuracy higher than 90% (with 95.4%
accuracyonaverage)whencomparedwiththeperformanceresults
obtainedfromextensivebenchmarkexecutionswhilereducingtest
runsby62%.Moreover,ourtestreductiontechniquescanreduce
the testrunsby90.1%whileretaining an averageaccuracyof 91%.
Our results also showed that PT4Cloud had significantly higher ac-
curacy than state-of-the-art testing and prediction methodologies
from software engineeringandsystemresearch.
The contributionsofthis paper include:
1)Thecloudperformancetestingmethodology, PT4Cloud ,which
employsnon-parametricstatisticalapproachestoprovidereliable
stoppingconditionstoobtainhighlyaccurateperformancedistribu-
tions.PT4Cloud alsoallowstheusersto specifyintuitiveaccuracy
objectives andtrade accuracyfor testingcost.
2)Twotestreductiontechniquesthatcansignificantlyreduce
the number oftest runswhileretaining ahigh level of accuracy.
3)Athoroughevaluationof PT4Cloud withsixbenchmarkson
sixVMconfigurationsonAWSandChameleoncloudtoexamine
thePT4Cloud ’saccuracyandtestreductionefficiency,anditsbenefit
over the state-of-the-artapproaches.
2 OVERVIEW OF PT4CLOUD
2.1 The PT4Cloud Methodology
PT4Cloud conductsperformancetestsoncloudapplicationsinmul-
tiple time intervals (periods) of test runs. In each time interval
(time period), the application-under-test is executed with its test
inputrepeatedlytoacquire nperformancesamples.Then PT4Cloud
determinesifaddingthesenew nsamplessignificantlychangesthe
performancedistributionacquiredfrompreviousintervals.Ifthe
189A Statistics-BasedPerformance TestingMethodologyforCloudApplications ESEC/FSE ’19, August 26ś30, 2019,Tallinn,Estonia
changeisinsignificant,thenthecurrentperformancedistribution
isconsideredstableandrepresentativeoftheactualperformanceof
theapplicationonthecloud.Otherwise,moretestrunsforanother
time interval are required. We conduct the test in time intervals
insteadofthenumberoftestrunsduetothedifficultytocontrol
thecloudexecutionenvironments.Morediscussionsonthistime
intervalare providedinSection 2.2.
Figure2shows the PT4Cloud workflow. The first step (Step 1) of
PT4Cloud istoexecuteanapplicationonthetargetcloudwithauser-
selectedVMconfiguration(i.e.,VMtypesandcount)repeatedlyfor
atimeinterval I.Letthesetofperformancedatacollectedfromthese
tests beS1. Based on S1, an initial performance distribution d1can
becalculated.Particularly, d1hererepresentsthenon-parametric
probabilitydensityfunction(PDF)of S1andcanbecalculatedusing
the kernel densityestimation(KDE) technique[ 52,55].
InStep2,theapplicationisagainexecutedforanothertimeinter-
valIusing the same VM configuration. Let the set of performance
datafromthesenewtestrunsbe S2.Combining S1andS2,weob-
tain a new set of performance data, S. That is, S=S1∪S2. Using
S, it is possible to calculate another performance distribution d2.
Intuitively, d1alwaysrepresentsthecurrentdistribution,whereas
d2alwaysrepresentsthedistributionwithnewdata.If d1andd2are
the same,then addingnewdata does not changethe distribution.
In Step 3, d1andd2are compared using non-parametric statisti-
cal approaches to determine if the performance distributions are
stable.Morespecifically,this comparisondeterminestheprobabil-
itypthatd1andd2are the same distribution. Users can choose
anobjectiveprobability pobeforehand.If p≥po,thentheperfor-
mance distributions are deemed stable, and d2is reported as the
performancedistributionofthisapplication.If p<po,thenmore
tests need to be conducted to acquire more stable results. Note
that,ausercanchooseanyobjectiveprobability.Ahigherobjec-
tive probability may require more tests, but it can also produce
moreaccurateperformanceresults.Whenreportingperformance
distribution d2,PT4Cloud alsocomputesconfidencebandswitha
user-selectedconfidence level(CL). Whileaconfidence intervalis
for a single point of estimation (e.g., mean), a confidence band is
for aseries ofestimations(e.g.,distribution).
If the test results are deemed unstable, more test runs are re-
quired.In Step 4, thenew S1isset to be Sand thenew d1to bed2.
That is,S1⇐Sandd1⇐d2. ThenPT4Cloud directs the perfor-
mancetesttogobacktoStep2totestforanothertimeinterval I
and repeat the comparison for stability. This loop repeats until the
performance results are stable.
2.2 CoverageCriteriaandTimeIntervalLength
Asstatedabove,thecoveragecriteriaforcloudperformancetesting
shouldincludeallperformanceuncertaintyfactorsandtheirpro-
portional impacts. However, as clouds are provided to the users as
black boxes, it is impossible to directly determine what factors are
coveredinonetestoraseriesoftests.Consequently,weadopted
an indirectapproach to ensure the coveragecriteriaare satisfied.
Previousstudiesobservedthatapplicationperformanceonthe
cloudroughlyexhibitsperiodic(e.g.,dailyorweekly)cycles[ 38].
Theseperiodiccyclesreflectthefactthatthemainfactorsofper-
formancefluctuationśvariousresourcecontentionsamongVMs
caused by multi-tenancy ś have a dependency on time. Motivated
25
 50
 75
 100
 125
 150
 175
Execution Time (sec.)
0.00
0.01
0.02
0.03
0.04
0.05Density
Two Weeks ( d2)
One Week ( d1)(a)Perf. Dist. for canneal
500
 525
 550
 575
 600
 625
Execution Time (sec.)
0.00
0.01
0.02
0.03
0.04
0.05
0.06
0.07Density
Two Weeks ( d2)
One Week ( d1) (b)Perf. Dist. for swaptions .
Figure 3:Testing cannealandswaptions withPT4Cloud
bythisobservation,wechoosetoconducttestsintermsofmultiple
time intervals to satisfy the coverage criteria. In this paper, we
choose the length ofeach time interval Ito be a week.Based ona
priorstudyandourexperimentalresultsforthispaper,aweekis
longenoughtoprovidegoodcoverageofuncertaintyfactorsforthe
types ofapplicationsandcloud services studiedinthis paper [ 38].
Notethat,aweekisthelongesttimeperiodrequiredforcloud
performance testingbased on our observations. Depending on the
applicationandtheVMconfiguration,thesmallestrequiredtime
intervallengthmaybelessthanaweek.Itisalsounnecessaryto
continuously execute tests for each time interval. We also explored
reducing the interval length and test runs per time interval and
reportedthe results inSection 6.
2.3 Examples ofApplying PT4Cloud
To illustrate the complete process of using PT4Cloud methodology
toconductperformancetesting,wepresenttwoexamplesoftesting
the performance of cannealandswaptions applications from the
PARSEC benchmarksuite [ 11]onAWS.
Figure3agivestheperformancedistribution d1forcannealafter
Step1,where cannealisexecutedcontinuouslyonaVMinstance
withVMtype t2.mediumforatimeintervallengthofaweek[ 5].
Figure3aalso gives the performance distribution d2forcanneal
after Step 2, which is calculated from two time intervals (weeks) of
testruns. Using the stabilitycomparisoninStep3,theprobability
thatd1andd2are the same is determined to be 92.8%. As shown
in Figure 3a,d1is very close to d2. If the user sets the objective
probability poto be 90%, then d2is reported as the final testing
result. Theshaded areainFigure 3ashows theconfidencebandof
d2with99%CL.
Figure3bgives the performance distribution d1forswaptions
afterStep1,whichincludesexecuting swaptions continuouslyon
at2.mediuminstance for a week. Figure 3balso shows the per-
formance distribution d2forswaptions after Step 2, which is cal-
culatedfromtwointervals/weeksoftestruns.Usingthestability
comparisoninStep3,theprobabilitythat d1andd2arethesame
is determined to be 80.7%. In Figure 3b,d1andd2are still close,
but they are clearly less similar than those of canneal. If the user’s
objective probability pois 80% or less, then d2is reported as the
final testing result. Otherwise, the test input has to be executed
from more time intervals for more accurateresults.
190ESEC/FSE ’19, August 26ś30, 2019,Tallinn,Estonia Sen He,Glenna Manns,John Saunders,WeiWang,Lori Pollock, andMary Lou Soffa
3 STATISTICS-BASEDSTOPCONDITIONS
Section2statesthat PT4Cloud determineswhethertheperformance
test can be stopped bydetermining if two distributions, d1andd2,
are similar (stable). That is, PT4Cloud determines if adding another
interval of tests significantly changes the performance distribution.
Akeyissuehereistoidentifytheproperstatisticalapproachfor
thisdistributioncomparison.Astatisticalapproachthatisproper
for cloud performance testing should satisfy three requirements.
First,theapproachshouldbeabletohandlenon-typicaldistribu-
tions, as performance distributions of cloud applications usually
donotfollowknowndistributions.Second,thisapproachshould
be able to handle distributionsacquired from experiments,asitis
practicallyimpossibletomakeahypothesisabouttheexacttheoret-
icaldistributionforacloudapplication’sperformance.Third,the
comparisonresult from thisapproachshouldbe intuitive so that
the ordinary user can understand. The comparison result should
also provide a quantitative definition for łsignificant change in
distributionsžto ensure testingresults are accurate.
ThemostcommonapproachfordistributioncomparisonisGood-
ness of Fit (GoF) statistical tests [ 49]. However, many GoF tests
areonlydesignedforspecifictypesofdistribution(e.g.,Shapiroś
Wilk test for normal distributions) or require one distribution to
be theoretical instead of experimental (e.g., KolmogorovśSmirnov
test) [49]. The Anderson-Darling test is a generic GoF test that can
compare two distributions acquired from experiments. However,
this test requires critical values that do not yet exist for the non-
typicalcloudperformancedistributions.TheChi-square( χ2)test
is another generic GoF test. To use χ2-test, one needs to first di-
vide the range of performance data (e.g., execution times) acquired
fromtestsintobins.Unfortunately,the χ2-testissensitivetothe
widths of the bins [ 49]. We experimented with χ2-test, but were
unabletofindabinwidththatworkedwellforthediversetypesof
distributionsobtainedfrom cloud performance tests.
Thedistributioncomparisonapproachthatweeventuallyadopted
is Kullback-Leibler (KL) divergence, which can handle any types
of distributions [ 42]. More specifically, KL-divergence measures
how one distribution diverges from a second distribution. Without
loss of generality, consider two distributions PandQover random
variablex.Theequationtocomputehow Pdivergesfrom Qwith
KL-divergenceis,
DKL(P||Q)=∫
P(x)logP(x)
Q(x)dx. (1)
The value of KL-divergence (i.e., DKL(P||Q)) ranges from 0 to
infinity. A value of 0 for KL-divergence indicates no divergence,
whereasinfinityindicatestwodistributionsarecompletelydifferent.
However,thisinterpretationisnotintuitiveforuserstounderstand
the amount of difference (divergence). To help a user interpret KL-
divergence,weemployedmultinomiallikelihood L[57]fromthe
likelihoodtheory,whichcan be computedas,
L(P||Q)=2−DKL(P||Q)(2)
Intuitively, Lrepresentstheprobabilitythat Pisdifferentfrom
Q.Asourgoalistocomparethesimilaritybetweendistributions
d1andd2, it requires considering d1andd2symmetrically. That is,
ifd1andd2are similar, then d1is not different from d2, andd2is
notdifferentfrom d1.Therefore,wedefinetheprobability pthatd1andd2are similar as,
p=L(d1||d2)×L(d2||d1) (3)
As describedin Section 2,pis theprobability used in PT4Cloud
to determine whether the performance distributions obtained from
performance tests are stable. Note that, to compute the integration
in Eq (1), we employed numerical integration by partitioning each
distributioninto1000strips.Forourexperiments,using1000strips
was sufficient, adding more strips changes the probabilities by less
than0.1%.
Notethat,KL-Divergenceiscommonlyusedasanasymmetric
metric.WeusedthesymmetricKL-Divergencefollowingitsoriginal
definition as we treat d1andd2equally in the comparison. [ 42]. A
potential variant of PT4Cloud may use asymmetric KL-Divergence.
While using asymmetric KL-Divergence does not affect the results
ofour experiments, its exact impact requires more investigation.
4 ESTABLISHINGCONFIDENCEBANDS
Tohelpusersbetterunderstandtheirapplication’sperformanceand
interprettheperformancetestingresultswithhigherconfidence,
PT4Cloud also presents each final performance distribution with
its confidence band.
Because the performance distributions of cloud applications
do not necessarily follow known distributions, we chose to com-
pute point-wise confidence bands (CB) using bootstrap [ 19,34,44].
Point-wiseconfidencebandsarecommonlyusedtodescribenon-
parametricdistributions,andbootstrapisastatisticalmethodfor
treating non-parametricdistributions.
Bootstrapisessentiallyaresamplingtechnique.Togeneratea
CB,theoriginalperformancedataset Sisresampled.Eachresample
samples a new data set with |S|data points from Swith replace-
ment, and a new probability density function (PDF) is generated
based on the new data set. Repeating the resampling for Rtimes
allowsustocalculate RPDFs.Thenapoint-wiseconfidenceband
with confidence level (CL) c% can be determined by calculating the
probabilitydensityregionthatcontains c%oftheRBootstrapped
PDFs. Figure 3also shows the confidence bands with shaded areas.
Bootstrap can produce correct results assuming re-sampling on
thedataset Sbehavessimilarlytowhen Sissampledfromthetrue
population.Thisassumptionisusuallytruewhen Siscompleteand
Raresufficientlylarge[ 19].WesetRtobe1000,followingcommon
practice[19].Sisdeemedcompleteby PT4Cloud .Therefore,aslong
asPT4Cloud methodologyisaccurate,confidencebandgenerated
withSisalsoreliable.
5 EXPERIMENTALEVALUATION
This section presents the methodology and findings from eval-
uatingPT4Cloud on two public clouds. This evaluation seeks to
answerthefollowing researchquestions :1)Howaccuratearethe
performancedistributionsacquiredwith PT4Cloud ?2)Howdoes
PT4Cloud compare withthe state of the art?
5.1 ExperimentalSetup
Benchmarks We evaluated PT4Cloud with a variety of bench-
marks thatrepresentwebapplications,high-performancecomput-
ing (HPC) applications, database (DB) applications and machine
191A Statistics-BasedPerformance TestingMethodologyforCloudApplications ESEC/FSE ’19, August 26ś30, 2019,Tallinn,Estonia
Table 1:Benchmarks andtheir application domains.
Benchmark Domain Origin
ft.C HPC NPB
ep.C HPC NPB
JPetStore (JPS) Web J2EE
YCSB DB YCSB
TPC-C DB OLTPBench
In-Memory Analytics (IMA)ML CloudSuite
Table 2:VM configurationsused forevaluation.
Config. VMCnt x VMType Core-Cnt MemSize
CHM-1 4x Small (Sm) 1/VM 2GB/VM
CHM-2 2x Medium(Md) 2/VM 4GB/VM
CHM-3 1x Large(Lg) 4/VM 8GB/VM
AWS-1 4x m5.large (M5Lg) 2/VM 8GB/VM
AWS-2 2x m5.xlarge (M5XLg) 4/VM 16GB/VM
AWS-3 1x m5.2xlarge(M52XLg) 8/VM 32GB/VM
learning (ML) applications to demonstrate that PT4Cloud can be
appliedtodiversecloudapplications.Table 1givesthebenchmarks
and their application domains that were used in our evaluation.
More specifically, we used JPetStore (JPS) from Java 2 Platform
Enterprise Edition (J2EE), ftandepfrom NAS Parallel Benchmarks
(NBP),Yahoo!CloudSystemBenchmark( YCSB)withApacheCas-
sandradatabase, TPC-CfromOTLPBenchand In-MemoryAnalytics
(IMA)fromCloudSuite[ 7,17,20,22,51].Forftandep,weusedthe
NPB’s C workloads so that the input data can fit in the memory of
amediumsizedVM.For YCSB,weuseditsworkloadAwhichhas
50%reads and50%writes.
PublicCloudsandVMConfigurations Toshowthat PT4Cloud
works properly on public clouds, we evaluated it on two public
clouds, Chameleon (CHM) [ 1], and the Amazon Web Services EC2
(AWS)[6].WeusedthreeVMconfigurationsoneachcloudthatrep-
resent use cases with both single and multiple VMs. Table 2details
theVMconfigurationsusedinourexperiments.NotethatAWShas
alarge selection ofVMtypes;inthiswork,we chose m5VMsas
they are the latest general purpose VMs. Except for ft,epandIMA,
each benchmark was tested on all configurations. Due to insuffi-
cient memory, ft,epandIMAcould not execute on Chameleon’s
smallVMs.Intherestofthispaper,wecallanexperimentwithone
benchmark on one VM configuration as a benchmark configuration .
In total, 33 benchmarkconfigurationsare evaluated.
Parameters of PT4Cloud For this evaluation, we chose the
objectprobability( po)fordistributionstabilitycomparisonto be
90%(i.e.,expectingtheaccuracyofperformancetestingresultstobe
atleast90%).Wealsosettheconfidencelevel(CL)tobe99%forthe
confidencebandsgeneration.Additionally,asstatedinSection 2,
we usedatime intervallength ofone weekinthis evaluation.
Evaluation Methodology and Metric For each benchmark
configuration,weevaluatetheaccuracyofitsperformancedistri-
butionacquiredwith PT4Cloud bycomparingthisdistributionwith
aground truth performance distribution .We then report the proba-
bility (i.e., the multinomial likelihood introduced in Section 3) that
thePT4Cloud and ground truth distributions are the same. For the
rest of this paper, we simply refer to this probability as accuracy.
Toobtainthegroundtruthdistribution,weexecutedeachbench-
markconfigurationforsixweeks(inadditiontotheperformanceTable 3: The number of intervals/weeks (W) required to ob-
tain stable performance distributions, and the accuracy of
theperformancedistributionsobtained with PT4Cloud .
łTest run Lengthž/łAccuracy(%)ž
CHM-1 CHM-2 CHM-3 AWS-1 AWS-2 AWS-3
ft.C N/A 2W/91 2W/99 2W/98 2W/99 2W/98
ep.C N/A 2W/94 2W/92 2W/96 2W/99 2W/99
JPS 2W/98 2W/99 3W/96 2W/99 3W/96 2W/96
YCSB 3W/94 3W/93 2W/90 3W/93 2W/94 2W/93
TPC-C 2W/94 2W/92 2W/90 2W/97 2W/96 2W/95
IMA N/A 2W/95 2W/95 2W/94 2W/96 2W/95
tests conducted with PT4Cloud ) and calculated the performance
distributionsusing the performance data from thesesix weeks.
Open Data Our data andsourcecode are available at
http://doi.org/10.6084/m9.figshare.7749356.
5.2 Perf.Dist.Acquiredwith PT4Cloud
Figure4presents the performance distributions acquired with
PT4Cloud for eight benchmarks configurations. Due tospace limi-
tation, only fourbenchmarks on two VMconfigurations ś CHM-2
andAWS-2śareshowninFigure 4.Therestofthebenchmarksand
configurations have similar results. Figure 4shows that the perfor-
mance distributions for these benchmarks gradually became stable
over time. That is, the changes in the distributions become smaller
aftereachnewinterval/week.ItcanalsobeseenfromFigure 4that
thedistributionsfromthefirstinterval/weekcanbedramatically
differentfromthefinalstabledistributions,suggestingthenecessity
of a performance testing methodology like PT4Cloud . The distribu-
tionsinFigure 4alsodonotalwaysfollowknowndistributionsand
varywithbenchmarkconfigurations,provingtheneedforusing
non-parametricstatisticalmethodsas employedby PT4Cloud .
Table3provides the numbers of intervals of test runs that were
conducted to obtain stable performance distributions with more
than90%stabilityprobability(i.e., pois90%)forallbenchmarkcon-
figurations.Theperformancedistributionsofthemajorityofthe
benchmarkswerestablewithintwoweeks.OncertainVMconfigu-
rations,thedistributionsofDBandwebapplicationsrequiredthree
weeks to become stable as I/O (network and disk) performance has
higher fluctuationsthanCPUandmemory performance [ 43].
5.3 Accuracyof PT4Cloud
Figure5compares the performance distributions obtained with
PT4Cloud andtheground truthperformancedistributions.Dueto
space limitation, only four benchmarks on two VM configurations
areshowninFigure 5.Asthefigureshows,thegroundtruthper-
formanceisveryclosetotheperformanceobtainedfrom PT4Cloud .
Table3alsogivestheaccuracyof PT4Cloud ’sperformancedis-
tributionswhencomparedwithgroundtruthdistributionsforall
benchmark configurations. As shown in Table 3, the accuracy of
PT4Cloud ’sperformancedistributionsisalwayshigherthan90%.
Theaverageaccuracyis95.4%.Theseresultsindicatethat PT4Cloud
methodology is highly accurate for cloud performance testing.
Moreover, PT4Cloud methodology executed considerably fewer
teststhanthegroundtruthtests.Figure 8giveshowmanyfewer
192ESEC/FSE ’19, August 26ś30, 2019,Tallinn,Estonia Sen He,Glenna Manns,John Saunders,WeiWang,Lori Pollock, andMary Lou Soffa
300
 350
 400
 450
Execution Time (sec.)
0.000
0.005
0.010
0.015
0.020
0.025
0.030
0.035Density
Week1
Week1 + 2
(a)ft.Con 2xMd(CHM-2).
200000
 300000
 400000
 500000
 600000
Successful Requests
0.000000
0.000002
0.000004
0.000006
0.000008Density
Week1
Week1 + 2 (b)JPSon 2xMd(CHM-2).
0
 2500
 5000
 7500
 10000
 12500
Overall Throughput
0.00000
0.00005
0.00010
0.00015
0.00020
0.00025
0.00030
0.00035Density
Week1
Week1 + 2
Week1 + 2 + 3 (c)YCSBon 2xMd(CHM-2).
1000000
 1200000
 1400000
 1600000
Execution Time (ms.)
0.0000000
0.0000005
0.0000010
0.0000015
0.0000020
0.0000025
0.0000030Density
Week1
Week1 + 2 (d)IMAon 2xMd(CHM-2).
100
 125
 150
 175
 200
 225
 250
Execution Time (sec.)
0.000
0.005
0.010
0.015
0.020
0.025
0.030Density
Week1
Week1 + 2
(e)ft.Con 2xM5XLg(AWS-2).
1250000
 1300000
 1350000
 1400000
Successful Requests
0.000000
0.000005
0.000010
0.000015
0.000020Density
Week1
Week1 + 2
Week1 + 2 + 3 (f)JPSon 2xM5XLg(AWS-2).
16000
 18000
 20000
 22000
 24000
Overall Throughput
0.0000
0.0001
0.0002
0.0003
0.0004
0.0005Density
Week1
Week1 + 2 (g)YCSBon 2xM5XLg(AWS-2).
650000
 750000
 850000
Execution Time (ms.)
0.000000
0.000005
0.000010
0.000015
0.000020Density
Week1
Week1 + 2 (h)IMAon 2xM5XLg(AWS-2).
Figure 4:PerformanceDistributionsacquired with PT4Cloud forfour benchmarks on configurationsofCHM-2 and AWS-2.
300
 350
 400
 450
 500
Execution Time (sec.)
0.000
0.005
0.010
0.015
0.020
0.025
0.030
0.035
0.040Density
Perf. Dist.
Grd Truth
(a)ft.Con 2xMd(CHM-2).
200000
 300000
 400000
 500000
 600000
Successful Requests
0.000000
0.000002
0.000004
0.000006
0.000008Density
Perf. Dist.
Grd Truth (b)JPSon 2xMd(CHM-2).
2000
 4000
 6000
 8000
 10000
 12000
Overall Throughput
0.0000
0.0001
0.0002
0.0003
0.0004
0.0005Density
Perf. Dist.
Grd Truth (c)YCSBon 2xMd(CHM-2).
1000000
 1200000
 1400000
 1600000
 1800000
Execution Time (ms.)
0.0000000
0.0000005
0.0000010
0.0000015
0.0000020
0.0000025
0.0000030Density
Perf. Dist.
Grd Truth (d)IMAon 2xMd(CHM-2).
125
 150
 175
 200
 225
 250
Execution Time (sec.)
0.000
0.005
0.010
0.015
0.020
0.025
0.030
0.035Density
Perf. Dist.
Grd Truth
(e)ft.Con 2xM5XLg(AWS-2).
1150000
 1250000
 1350000
 1450000
Successful Requests
0.0000000
0.0000025
0.0000050
0.0000075
0.0000100
0.0000125
0.0000150
0.0000175Density
Perf. Dist.
Grd Truth (f)JPSon 2xM5XLg(AWS-2).
16000
 18000
 20000
 22000
 24000
Overall Throughput
0.0000
0.0001
0.0002
0.0003
0.0004
0.0005
0.0006Density
Perf. Dist.
Grd Truth (g)YCSBon 2xM5XLg(AWS-2).
700000
 800000
 900000
Execution Time (ms.)
0.000000
0.000005
0.000010
0.000015
0.000020Density
Perf. Dist.
Grd Truth (h)IMAon 2xM5XLg(AWS-2).
Figure5:Performancedistributionsobtainedwith PT4Cloud andgroundtruthperformancedistributionsforfourbenchmarks
on twoVM configurationson CHMandAWS. Shaded areasare confidencebands.
test runs PT4Cloud executed compared to the ground truth tests.
Onaverage, PT4Cloud executed62%fewer test runs.
Figure5alsoshowstheconfidencebands(CB)forthefinaldis-
tributionswith99%CL.With pobeing90%,theprobabilitythatthe
true distribution falling within the CB is expected to be roughly
90%×99%=89%.Notethegroundtruthdistributionsdonotneces-
sarilyfallwithintheconfidencebandswiththissameprobability,as
theyarestillexperimental(i.e.,nottrue)distributions.Nonetheless,the majority of the ground truth distributions (including those not
showninFigure 5), fall within the confidence bands.
5.4 Comparisonwith State-of-the-Art
Todemonstratetheimportanceofanewmethodologylike PT4Cloud
for performance testing on the cloud, we compared PT4Cloud with
threeperformanceteststoppingandperformancepredictionmethod-
ologies from software engineeringandcomputer systemresearch.
193A Statistics-BasedPerformance TestingMethodologyforCloudApplications ESEC/FSE ’19, August 26ś30, 2019,Tallinn,Estonia
 0 20 40 60 80 100ft.C (CHM-2)ft.C (CHM-3)ep.C (CHM-2)ep.C (CHM-3)JPS (CHM-1)JPS (CHM-2)JPS (CHM-3)YCSB (CHM-1)YCSB (CHM-2)YCSB (CHM-3)TPC-C (CHM-1)TPC-C (CHM-2)TPC-C (CHM-3)IMA (CHM-2)IMA (CHM-3)ft.C (AWS-1)ft.C (AWS-2)ft.C (AWS-3)ep.C (AWS-1)ep.C (AWS-2)ep.C (AWS-3)JPS (AWS-1)JPS (AWS-2)JPS (AWS-3)YCSB (AWS-1)YCSB (AWS-2)YCSB (AWS-3)TPC-C (AWS-1)TPC-C (AWS-2)TPC-C (AWS-3)IMA (AWS-1)IMA (AWS-2)IMA (AWS-3)AverageAccuracy (%)PT4Cloud w. 1-week Interval SotA1 SotA2
Figure 6:Accuracyof PT4Cloud whencompared with twoother state-of-the-artperformancetestingmethodologies.
The first methodology ( SotA1) stops the performance test by
detecting the repetitiveness of the performance data obtained from
test runs [ 3]. More specifically, it picks a short period of data from
the whole performance testing results and compares this period
with other periods in the whole data set to determine if this period
of data is repeated. The methodology repeats this process for 1000
times to estimate the percentage of the performance results that
arerepeated(i.e.,repetitiveness).Iftherepetitivenessremainsun-
changed for a user-defined time span, then the test can be stopped.
Wereproducedthismethodologyusingthe18setsofparameters
given in that paper and report the accuracy of the performance
distributions obtained with this methodology in Figure 6. For each
benchmarkconfiguration,onlythebestaccuracyofthe18setsof
parameters is reported. For all of the benchmark configurations,
thismethodologystoppedthetestsrelativeearlyśalwaysinless
than 450 minutes on AWS and 250 minutes on Chameleon. Due to
spacelimitation,we cannotprovidedetailedstop times.
AsFigure 6shows,thisfirstmethodologyhasanaverageaccu-
racy of only 65.7% due to the early stop times. Many benchmark
configurationsexperiencedlowerthan50%accuracy.Weobserve
thatthis low accuracy ismainlybecause this methodologyisonly
designed to detect performance changes due to an application’s in-
ternalfactorsinsteadofexternalfactors,suchascloudperformance
uncertaintyfactors.Forinternalfactors,onlymeasuringwhether
adatumisrepeatedornotmaybeenough.However,forexternal
factors, how frequent each datum being repeated is also important.
Additionally,wefoundthattheaccuracyofthismethodologyrelied
heavily on its parameters, i.e., the length of the period and the
user-defined time span. Nonetheless, it is unclear how to select
theseparametersto maximizeaccuracy.
Thesecondmethodologywecompared( SotA2)isaclassicaltech-
niqueforperformanceanalysis[ 39,46,49].Inthismethodology,the
performancetestis stoppediftheconfidence interval(CI) forcer-
tainstatisticsłnarrowstoadesiredwidthž[ 39,46].Aswetestedfor
performancedistribution,weextendedthisideatostoprunningthe
tests ifthe 99%CIfor eachpercentileofthe performancedropped
within 10% of the observed percentile performance. This 10% is
chosen toreflect ±5% marginof error, similar toour 90%accuracy
goal (i.e., po). With this extension, we applied this methodology
to our benchmarks. The CIs were generated using the bootstrap
approach. This methodology caused the performance testing to
stopatvariabletimes,from2hourstoeven7weeks.Theextralong
tests were caused by performance outliers, which made the CIs ofcertainfringepercentileshardertoconverge.Theaccuracyofthe
performancedistributions obtainedwiththismethodology isalso
reportedinFigure 6.AsFigure 6shows,theaverageaccuracyfor
this methodology is only 66.2%, with the lowest accuracy being
8.73% (EP-CHM3).
This low accuracy shows that CI width is a poor stopping cri-
teria for cloud performance testing. That CIs failed to determine
requiredsamplesizesisalsoobservedinothersciencefields[ 29].
The main issue is that, for non-parametric distributions, CIs are
reliable only when data is complete [ 19]. That is, for cloud perfor-
mance testing results, the CIs are only reliable when the results
coversalluncertaintyfactors.AnarrowCIsimplymeansthatthere
arelargeamountofperformancedataobtainedundertheobserved
uncertainty factors, not that allfactors are observedbythe tests.
Wecomparedourtechniquewith a thirdmethodology,which
wasaperformancedistributionpredictiontechniqueforcloudusing
Monte Carlo simulation [ 10]. This methodology can be applied
to applications with multiple steps assuming the min and max
performance of each step are known. Two of our benchmarks,
YCSBandft.C, have two steps. Thus, we applied this prediction
technique on the 11 benchmark configurations involving these
two benchmarks using the min and max performance obtained
from the ground truth. For ft.C, the distributions predicted by this
techniquehaveaccuraciesof10%,11%,16%,21%,8%,onthefiveVM
configurations from CHM-2 to AWS-3. For YCSB, the accuracies
are42%,22%,20%,39%,29%,16%forCHM-1toAWS-3.Theaverage
accuracy for the 11 configurations is 21.3%. The low prediction
accuracy is primarily because this technique assumes each step
hasauniformperformancedistribution,whichisnottrueonreal
publicclouds.Infact,ifthestableperformancedistributionforeach
stepobtainedwith PT4Cloud isusedforthismethod,theaverage
prediction accuracy of this methodology can be increased to 53.9%
(with one configuration’s accuracy increased to 94%). This increase
inaccuracyshowsthatnotonlydoes PT4Cloud benefitperformance
testing, it can also benefit performance prediction techniques by
providingreliabletrainingdataset.Moreover, PT4Cloud canalso
benefit performance prediction techniques by providing reliable
ground truthto evaluate the goodness of the prediction results.
6 REDUCING THE NUMBEROFTESTS
As performance tests on clouds incur cloud usage cost, a good
performancetestingmethodologyshouldalsostrivetominimize
the testing costs. In this section, we explore the approaches to
194ESEC/FSE ’19, August 26ś30, 2019,Tallinn,Estonia Sen He,Glenna Manns,John Saunders,WeiWang,Lori Pollock, andMary Lou Soffa
Application-to-test and
a workloadStep 1 - 1: 
Execute tests for the app
continuously for a short 
interval I. Let the set of 
perf. data acquired
from these tests be S1.Step 1 - 2: 
Calculate performance 
distribution d1 from S1.
Step 2 - 2: 
Combine S1 and S2 into 
a new sample set S. 
Calculate performance
distribution d2 from S.Step 2 - 1: 
Execute the app for an- 
other short interval I. Let 
the set of perf. data from
these new tests be S2.
No
Report d2 (and
validation, if any) as
the perf test results
Step 4-1 (optional validation):
Conduct additional tests of
size |S| and calculate perfor-
mance distribution d3.
Validation is successful
Validation failsYes and choose to 
validate d2
Yes and choose 
not to validateStep 3: 
Compare d1 and d2
to determine if
stable?
Step 4-2: 
Compare d2 and d3
to validate d2.Step 5: 
Increase the length of
interval I.
Figure 7: The workflow of PT4Cloud when using intervals
shorter than oneweek.
reduce the length of the intervals and test runs per interval for
PT4Cloud ,whichinturncan reduce the test costs.
6.1 ReducingInterval Length
In Figure 4, several benchmarks’ performance distributions ob-
tained after the first week were already very similar to the final
performancedistributions(e.g., ft.CandIMA),suggestingthatsome
cloud applications can use intervals shorter than one week. In-
deed, the proper interval length depends on the application and
theVMconfigurationtobetested.Forinstance,aCPU-intensive
application is less likely to be affected by the cloud performance
uncertaintyfactorsfromI/Ocontentions.Therefore,theinterval
forthisapplicationcanpotentiallybesmallerthanaweek,asthe
performancetestonlyneedstocoverasmallernumberofperfor-
manceuncertaintyfactors.However,forapplicationswithheavy
usage of disk and network, the intervals must be longer to cover
the fluctuationsdueto alluncertainty factors.
Tohelpusersconduct testswithshorterintervals,we modified
the workflow of PT4Cloud to allow users to search for proper in-
terval lengthswhileconducting tests. Figure 7depicts a variantof
PT4Cloud designed for short intervals. In this newvariant, the user
firstconductstestsfortwoshortintervals.Thenourdistribution
comparison technique is used to determine if the performance dis-
tributionisstableafterthese twoshort intervals.Ifthe distribution
is stable, then it can be reported as the final results. Otherwise,
theintervallengthisincreased(Step5),andanewroundoftests
startsfromthebeginning(Step1)withthenewlongerinterval.The
testing interval gradually increases if the results do not stabilize.
Whentheintervallengthreachesoneweek,thestandard PT4Cloud
methodology isemployed.
In this new variant, we choose to increase the interval length
instead of conducting tests for more intervals because our primary
goal is to provide highly accurate testing results. Therefore, weaggressivelyassumethatfailuretostabilizeisduetoashortinterval
ratherthanthat insufficientintervals are tested.
A disadvantage of shorter intervals is that the resulting distri-
butions are more likelyto be inaccurate than those acquired from
longerintervals,asthe impactsofcloud performanceuncertainty
factors are more likely to remain unchanged within a short period
than a longer period. For example, an application’s performance
mayappear tobe stable within two hours, although itsactual per-
formance over a week may be quite different. Consequently, in
PT4Cloud forshortintervals,wealsoincludedanoptionalvalida-
tion step (Step 4 of Figure 7). More specifically, if the performance
distribution is stable with two short intervals of tests, then an addi-
tionaltwointervals(withsameintervallength)oftestsareexecuted.
The performance distribution from the additional test runs is com-
pared with the original distribution. If both distributions are the
samewitha probabilityhigherthantheobjectiveprobability ( po),
then the testing result is validated and can be reported. Otherwise,
the intervallength has to be increased. Based on theperformance
resultsweobtainedfromtheexperimentalevaluation,werecom-
mend the user to take the validation step if the testing result is
stablewithin aweek(i.e.,the intervallengthisless than3.5days).
6.2 ReducingTestRunswith Sampling
To further reduce the testing cost, especially for the applications
requiring1-weekintervals,wealsoexploredanhourlysampling
technique.Sofar,testrunshavebeenexecutingconsecutivelyfor
eachintervalwiththegoaltocovereverychangeinthebehavior
ofthecloud performance uncertainty factors.Similartoother sta-
tisticalpractices,thisconsecutiveexecutionoftestscanbereplaced
withsamplingtoreducecostwhileprovidingsimilaraccuracy.Here,
we employed an hour-based sampling technique. More specifically,
foreach hour within an interval,tests are only executed fora por-
tionofthishour.Wesampledourperformancetestingdataobtained
fortheevaluationinSection 5withvariousportionsizesandfound
that our benchmarks can achieve more than 90% accuracy with
portion sizes from 1/3 to 3/4. On average, a portion size of 1/2 (i.e.,
samplingonehalfofanhour)provideshighlyaccurateresultsfor
the majority ofour benchmarks.
6.3 EvaluationofTestReductionTechniques
We conducted additional evaluations to examine the effectiveness
andaccuracyofourtwotestreductiontechniques.Byeffectiveness
evaluation,wemeanevaluatingwhetherourtestreductiontech-
niquescaneffectivelyreducethenumberoftestruns.Weextracted
smaller intervals of performance data from the tests conducted for
the evaluation in Section 5and performed sampling on these data.
For interval reduction, we explored intervals of 1 hour, 12 hours, 1
day, 2 days, 3 days, etc. up to 6 days. For hourly sampling, we used
aportionsize of1/2 (i.e.,sampling one halfof an hour).
EffectivenessEvaluation Table4showsthereducedinterval
lengths to achieve at least 90% stable probability for all benchmark
configurationsusedinSection 5.Asthetableshows,theinterval
lengths ranged from 12-hours to one week. ML and HPC bench-
marksthatareCPU-and/ormemory-intensivearemorelikelyto
use intervals lessthan a week. However, DB andweb applications
thatareI/O-intensiveoftenrequireanintervalofaweekorclose
195A Statistics-BasedPerformance TestingMethodologyforCloudApplications ESEC/FSE ’19, August 26ś30, 2019,Tallinn,Estonia
Table4:Reducedintervallengthsandthenumberofintervalsittakestostabilize,forallbenchmarkconfigurationsusingthe
twotestreductiontechniques ( łDžstandsforday).
łReduced IntervalLengthž/ł# ofIntervalsž, w/o HourlySampling łReduced IntervalLengthž/ł# ofIntervalsž, w. HourlySampling
config. CHM-1 CHM-2 CHM-3 AWS-1 AWS-2 AWS-3 CHM-1 CHM-2 CHM-3 AWS-1 AWS-2 AWS-3
ft.C N/A 3D/2 3D/2 12hrs /2 12hrs /2 12hrs /2 N/A 3D/2 3D/2 12hrs /2 12hrs /2 12hrs /2
ep.C N/A 4D/2 4D/2 4D/2 1D/2 12hrs /2 N/A 4D/2 4D/2 4D/2 12hrs /2 12hrs /2
JPS 12hrs/ 2 12hrs /2 1W/3 1D/2 1W/3 3D/2 12hrs /2 12hrs /2 1W/3 1D/2 1W/3 4D/2
YCSB 4D/2 1W/3 5D/2 1W/3 2D/2 4D/2 4D/2 1W/3 5D/2 1W/3 2D/2 3D/2
TPC-C 3D/2 1D/2 4D/2 12hrs /2 12hrs /2 12hrs /2 3D/2 1D/2 4D/2 12hrs /2 12hrs /2 12hrs /2
IMA N/A 3D/2 4D/2 12hrs /2 12hrs /2 1D/2 N/A 3D/2 4D/2 2D/2 2D/2 4D/2
 0 20 40 60 80 100ft.C (CHM-2)ft.C (CHM-3)ep.C (CHM-2)ep.C (CHM-3)JPS (CHM-1)JPS (CHM-2)JPS (CHM-3)YCSB (CHM-1)YCSB (CHM-2)YCSB (CHM-3)TPCC (CHM-1)TPCC (CHM-2)TPCC (CHM-3)IMA (CHM-2)IMA (CHM-3)ft.C (AWS-1)ft.C (AWS-2)ft.C (AWS-3)ep.C (AWS-1)ep.C (AWS-2)ep.C (AWS-3)JPS (AWS-1)JPS (AWS-2)JPS (AWS-3)YCSB (AWS-1)YCSB (AWS-2)YCSB (AWS-3)TPCC (AWS-1)TPCC (AWS-2)TPCC (AWS-3)IMA (AWS-1)IMA (AWS-2)IMA (AWS-3)Reduced T ests (%)Standard PT4Cloud with 1-week Interval Reduced Interval Reduced Interval + Sampling
Figure 8:Numberoftests reduced by PT4Cloud andour testreductiontechniques, compared to the groundtruth tests.
to a week. Prior studies also observed that the performance of I/O-
bound applications depended strongly on contention [ 43]. More-
over, more benchmark configurations can use short intervals on
AWSthanonChameleon,whichreflectsthefactthatAWSprovides
betterresourcecontentionmanagementandVM placement.
Figure8shows the number of tests reduced by our test reduc-
tion techniques. On average, the interval reduction can reduce test
counts by 81.5% of the tests required for ground truth tests. Ap-
plying both test reduction techniques can reduce the test counts
by90.1%.ThisreductionintestcountcanreducebothVMusage
costs and network cost proportionally for popular public clouds,
includingAWS,MicrosoftAzure andGoogle Compute Engine.
Accuracy Evaluation Figure9gives the accuracy of the per-
formance distributions acquired with PT4Cloud methodology after
applying the two test reduction techniques. As the figure shows,
our test reduction techniques can reduce test count without sig-
nificantly scarifying accuracy. Even with the test reduction, the
performancedistributionsacquiredwith PT4Cloud canstillreach
upto99%accuracy( JPSonAWS-1).Onaverage,theperformance
distributions acquired with PT4Cloud after applying interval reduc-
tion is 92.3%. The average accuracy after applying both interval
reductionand hourly sampling is91%. It is also worth notingthat,
when the interval length is longer than 4 days, sampling half an
hour has littlenegative impact onaccuracy.
The lowest accuracy is 71.9%, which is for IMAon AWS-1 when
using both reduced intervals and sampling. For this benchmark
configuration,therewereafewperformanceoutliers.Asthetests
were only conducted for 2 days with sampling, these outliers were
observedatadifferentfrequencyduringthe PT4Cloud teststhan
the ground truth. As KL-divergence is sensitive to outliers, this
differencereducedtheaccuracy,eventhoughthemajorityofthe
PT4Cloud andground truthdistributionswere similar.7 THREATS TO VALIDITY
Execution environment changes . The performance results ob-
tainedwith PT4Cloud areonlyvalidwhentheexecutionenviron-
ment,includingtheunderlyinghardwareandmulti-tenancybehav-
ior,remainsthesame.Whentheexecutionenvironmentischanged,
newperformance tests should be conducted.
Furthermore, while our results indicate that the performance of
cloudapplicationshasweeklyordailycycles,events thathappen
only a few times a year/month may still affect overall performance
distributions. An example of such yearly events may be holiday
shopping where all websites running in the clouds exhibit high
resourcedemands.Ingeneral,webelievetheseeventsdonotsig-
nificantlychangeoverallperformancedistributionsastheyhappen
infrequently.Additionally,cloudserviceproviderscanimplement
resourcemanagementpoliciestolimittheimpactoftheseevents.
OurexperimentsonAWSactuallyoverlappedwithAmazon2018
PrimeDaywhenAmazon’sownwebsiteexperiencederrors[ 16].
However,weobservednearlynoimpactontheperformanceofour
benchmarks. Nonetheless, the potential impact of these yearly and
monthly eventsshould be acknowledged.
Otherapplications,workloadsandVMconfigurations .Al-
though we strive to evaluate PT4Cloud with all types of cloud
applications,wecanonlyevaluatealimitednumberofbenchmarks
andVMconfigurations,duetocost.Othercloudapplications,work-
loads,VMconfigurationsandcloudsmayexhibitdifferentaccuracy
withPT4Cloud orrequire differentintervallengths.
Other cloud uncertainty factors . Here, we focused on the
cloudperformanceuncertaintyfactorscausedbymulti-tenancyand
VM scheduling. Otherfactors, such asdatacenterlocation,bursty
VM types and hardware variation, may also affect performance.
The VMs used for our evaluations are not affected by these factors.
However, these factors do exist for other cloud services and VM
196ESEC/FSE ’19, August 26ś30, 2019,Tallinn,Estonia Sen He,Glenna Manns,John Saunders,WeiWang,Lori Pollock, andMary Lou Soffa
 70 75 80 85 90 95 100ft.C (CHM-2)ft.C (CHM-3)ep.C (CHM-2)ep.C (CHM-3)JPS (CHM-1)JPS (CHM-2)JPS (CHM-3)YCSB (CHM-1)YCSB (CHM-2)YCSB (CHM-3)TPC-C (CHM-1)TPC-C (CHM-2)TPC-C (CHM-3)IMA (CHM-2)IMA (CHM-3)ft.C (AWS-1)ft.C (AWS-2)ft.C (AWS-3)ep.C (AWS-1)ep.C (AWS-2)ep.C (AWS-3)JPS (AWS-1)JPS (AWS-2)JPS (AWS-3)YCSB (AWS-1)YCSB (AWS-2)YCSB (AWS-3)TPC-C (AWS-1)TPC-C (AWS-2)TPC-C (AWS-3)IMA (AWS-1)IMA (AWS-2)IMA (AWS-3)AverageAccuracy (%)Standard PT4Cloud with 1-week Interval Reduced Interval Reduced Interval + Sampling
Figure 9:Accuracyoftestreductiontechniques.
types.Although PT4Cloud methodologyisgenericenoughtohandle
thesefactors, more tests maybe requiredto cover thesefactors.
Performanceresultsotherthandistributions .Insomecases,
usersmayonlyneedtoknowthemeanoraparticularpercentile
of the performance of their applications. Although an accurate per-
formancedistributioncanprovideaccuratemeanandpercentiles,
theremaybeacheapertestingmethodologytoobtainthemdirectly.
However,asthemeansandpercentilesarestillfromthenon-typical
performance distributions of cloud applications, common paramet-
ricstatisticaltoolsstillcannotbeapplied.Wearecurrentlyworking
onaseparate testingmethodologyformeans andpercentilesthat
can further reduce test costs.
8 RELATED WORK
A large body of work in performance testing focused on the gener-
ationandprioritizationoftestinputs[ 8,13,18,60].Burnimetal.
designed an input generator that can produce worst-performing
testinputsforanyinputsizes[ 12].Zhangetal.proposedamethod-
ologytogeneratetestinputsgivenaninputsizeanddiversitybased
onsymbolicexecution[ 64].Chenetal.extendedthisideatoemploy
probabilistic symbolic execution to generate test inputs with fre-
quenciessothataperformancedistributioncanbeconstructed[ 14].
PerfLearner can help test input generation by finding important
parameters from bug reports [ 33]. Perfranker is a test input priori-
tization mechanism for performance regression testing [ 48]. There
is also work on detecting performance bugs by analyzing source
code,applicationbehaviorandtraces[ 9,15,24,32,36,40,41,50,61].
These test input generation, prioritization and performance debug-
ging studies are orthogonal to our work, as our work focuses on
determiningthe accurateperformance ofatest input.
Severalstudiesdocumentedtheimportanceofrepeatedlyexe-
cutingatestinputforperformancetesting[ 48,53].However,these
studiesdidnotprovidemeanstoproperlydeterminethenumberof
testrunsrequiredtogetreliableperformancewithlowtestingcost.
They either used extensively long test runs [ 48] or an arbitrary
numberofruns[ 53].Themostrelatedworkonperformancetest
stoppingconditionisprobablythestudydonebyAlGhmadietal.[ 3]
and the CI-based methodology [ 39,46]. However, our comparison
experiments in Section 5.4showed that these methodologies could
notprovideaccurateperformancetestingresultsonthe cloud.Guo
et al. also employed bootstrap in their work for building perfor-
mance models andcross-validation, instead oftesting[ 30].
Thereisalsoresearchonpredictingcloudapplication’sperfor-
mance. The mostly related prediction work is proposed by Luke etal.topredicttheperformancedistributionsforapplicationswith
multiple steps [ 10]. Our comparison experiments in Section 5.4
showedthatthisworkcouldprovideaccuratepredictions.Hsuetal.
investigatedpredictingthebestVMconfigurationusingBayesian
Optimizationwithlow-levelmetrics[ 35].However,thisworkdid
not aim at predicting the actual performance of cloud applica-
tions. Wang et al. predicted the performance of CPU and memory-
intensive applications [ 62]. PARIS is a model that could predict
the performance of a cloud application on a VM configuration [ 63].
However,PARISmayhaveupto50%(RMSE)error[ 63].Gambietal.
employedtheKrigingmethodtopredicttheaverageperformance
of a cloud application under different workloads and VMs [ 25].
It is worth noting that the existence of predictive work cannot
eliminate the necessity of measurement-based performance test-
ing approaches such as PT4Cloud . As shown in Section 5.4and
inpriorwork,measurement-basedperformancetestingisstillre-
quired to provide complete training set and accurate ground truths
for predictive approaches[ 56].
9 CONCLUSION
Performance testing onclouds to obtain accurate testing results is
extremelychallengingduetocloudperformanceuncertainty,the
inabilitytocontrolcloudexecutionenvironmentsandthetesting
cost.Inthis paper, we present acloud performancetestingmethod-
ology,PT4Cloud .Byemployingnon-parametricstatisticaltoolsof
likelihoodtheoryandbootstrapping, PT4Cloud canprovidereliable
stopping conditions to obtain highly accurate performance results
asperformancedistributionswithconfidencebands.Theevaluation
resultsshow that,withtwotestreductiontechniques, PT4Cloud ’s
performanceresultsareonaverage91%similarwithtestingresults
from extensive test runswith90.1% fewertests comparedtothese
extensive runs. This is considerably better than the state of the art
as evidencedinour empirical comparisons.
ACKNOWLEDGEMENT
ThisworkwassupportedbytheNationalScienceFoundationunder
grants CCF-1617390 and CCF-1618310. The views and conclusions
containedhereinarethoseoftheauthorsandshouldnotbeinter-
pretedasnecessarilyrepresentingtheofficialpoliciesorendorse-
ments, either expressed or implied of NSF. The authors would like
tothanktheanonymousreviewersfortheirinsightfulcomments.
WewouldalsoliketothankJamesSkripchuk,TianyiLiuandXin
Niefor theirvaluable inputs.
197A Statistics-BasedPerformance TestingMethodologyforCloudApplications ESEC/FSE ’19, August 26ś30, 2019,Tallinn,Estonia
REFERENCES
[1][n.d.]. AConfigurableExperimentalEnvironmentforLarge-scaleCloudResearch.
https://www.chameleoncloud.org/. [Online;accessed Aug-2018].
[2]Ali Abedi and Tim Brecht. 2017. Conducting Repeatable Experiments in Highly
VariableCloudComputingEnvironments.In Proceedingsofthe8thACM/SPEC
onInternationalConference onPerformance Engineering .
[3]H.M.Alghmadi,M.D.Syer,W.Shang,andA.E.Hassan.2016. AnAutomated
Approach for Recommending When to Stop Performance Tests. In Int’l Conf. on
SoftwareMaintenance and Evolution .
[4]OmidAlipourfard,HongqiangHarryLiu,JianshuChen,ShivaramVenkataraman,
Minlan Yu, and Ming Zhang. 2017. CherryPick: AdaptivelyUnearthing the Best
Cloud Configurations for Big Data Analytics. In USENIX Symp. on Networked
SystemsDesignand Implementation .
[5]Amazon. [n.d.]. Amazon EC2 Instance Types.
https://aws.amazon.com/ec2/instance-types/. [Online;accessed Aug-2018].
[6]Amazon. [n.d.]. Amazon Web Services. https://aws.amazon.com. [Online;
accessed Aug-2018].
[7]D.H.Bailey,E.Barszcz,J.T.Barton,D.S.Browning,R.L.Carter,L.Dagum,R.A.Fa-
toohi,P.O.Frederickson,T.A.Lasinski,R.S.Schreiber,etal .1991.TheNASParallel
BenchmarksSummaryandPreliminaryResults.In Int’lConf.onSupercomputing .
[8]Cornel Barna, Marin Litoiu, and Hamoun Ghanbari. 2011. Autonomic Load-
testingFramework. In Int’lConf.onAutonomic Computing .
[9]A. Benbachir, I. F. D. Melo, M. Dagenais, and B. Adams. 2017. Automated Perfor-
mance Deviation Detection across Software Versions Releases. In IEEE Int’l Conf.
onSoftwareQuality, Reliability and Security .
[10]Luke Bertot, Stéphane Genaud, and Julien Gossa. 2018. Improving Cloud Simula-
tionUsing the Monte-CarloMethod.In Euro-Par: ParallelProcessing .
[11]ChristianBienia.2011. BenchmarkingModernMultiprocessors . Ph.D.Dissertation.
PrincetonUniversity.
[12]JacobBurnim,SudeepJuvekar,andKoushikSen.2009. WISE:AutomatedTest
GenerationforWorst-caseComplexity.In Proceedingsofthe31stInternational
Conference onSoftwareEngineering .
[13]SudiptaChattopadhyay,LeeKeeChong,andAbhikRoychoudhury.2013.Program
Performance Spectrum. In Proc. of ACM Conf. on Languages, Compilers and Tools
for EmbeddedSystems .
[14]BihuanChen,YangLiu,andWeiLe.2016. GeneratingPerformanceDistributions
viaProbabilisticSymbolicExecution.In Proc.ofInt’lConfonSoftwareEngineering .
[15]Jane Cleland-Huang, Carl K. Chang, and Jeffrey C. Wise. 2003. Automating
Performance-related Impact Analysis through Event based Traceability. Require-
ments Engineering 8,3 (01 Aug 2003).
[16]CNBC. [n.d.]. Amazon suffers glitches at the start of Prime Day, its biggest
shopping day of the year. https://www.cnbc.com/2018/07/16/amazon-suffers-
glitches-in-opening-minutes-of-prime-day.html . [Online;accessed Aug-2018].
[17]BrianF.Cooper,AdamSilberstein,ErwinTam,RaghuRamakrishnan,andRussell
Sears.2010. BenchmarkingCloudServingSystemswithYCSB.In Proc.ofACM
SymposiumonCloudComputing .
[18]Emilio Coppa, Camil Demetrescu, and Irene Finocchi. 2012. Input-sensitive Pro-
filing.InProc.oftheConf.onProgrammingLanguageDesignandImplementation .
[19] A.C.Davison andD.V.Hinkley.2013. BootstrapMethodsandTheirApplication .
CambridgeUniversityPress,NewYork, NY, USA.
[20]Djellel Eddine Difallah, Andrew Pavlo, Carlo Curino, and Philippe Cudre-
Mauroux.2013.OLTP-Bench:AnExtensibleTestbedforBenchmarkingRelational
Databases. Proc. VLDBEndow. 7,4 (Dec. 2013).
[21]Bradley Efron. 1982. The Jackknife, the bootstrap and other resampling plans .
SIAM.
[22]Michael Ferdman, Almutaz Adileh, Onur Kocberber, Stavros Volos, Mohammad
Alisafaee, Djordje Jevdjic, Cansu Kaynak, Adrian Daniel Popescu, Anastasia
Ailamaki,andBabakFalsafi.2012.ClearingtheClouds:AStudyofEmergingScale-
out Workloads on Modern Hardware. In Proc. of the 17th Int’l Conf. Architectural
Support for ProgrammingLanguages and OperatingSystems .
[23]F.L.Ferraris,D.Franceschelli,M.P.Gioiosa,D.Lucia,D.Ardagna,E.DiNitto,
andT.Sharif.2012. EvaluatingtheAutoScalingPerformanceofFlexiscaleand
Amazon EC2 Clouds. In Int’l Symp. on Symbolic and Numeric Algorithms for
Scientific Computing .
[24]K. C. Foo, Z. M. Jiang, B. Adams, A. E. Hassan, Y. Zou, and P. Flora. 2015. An
Industrial Case Studyon the AutomatedDetectionofPerformanceRegressions
in Heterogeneous Environments. In IEEE Int’lConf.onSoftwareEngineering .
[25]A.Gambi,G.Toffetti,C.Pautasso,andM.Pezze.2013. KrigingControllersfor
Cloud Applications. IEEE Internet Computing 17(2013).
[26]DavidGesvindr andBarboraBuhnova.2016. Performance Challenges,Current
BadPractices,andHintsinPaaSCloudApplicationDesign. SIGMETRICSPerform.
Eval. Rev. 43,4 (Feb. 2016).
[27]I. I. Gorban. 2014. Phenomenon of Statistical Stability. Technical Physics 59, 3
(Mar2014).
[28]MarkGrechanik,QiLuo,DenysPoshyvanyk,andAdamPorter.2016. Enhanc-
ingRulesForCloudResourceProvisioningViaLearnedSoftwarePerformance
Models. In ACM/SPEConInt’lConf.onPerformance Engineering .[29]Greg Guest, Arwen Bunce, and Laura Johnson. 2006. How Many Interviews Are
Enough?: An Experiment with Data Saturation and Variability. Field Methods 18,
1 (2006), 59ś82.
[30]Jianmei Guo, Dingyu Yang, Norbert Siegmund, Sven Apel, Atrisha Sarkar, Pavel
Valov, Krzysztof Czarnecki, Andrzej Wasowski, and Huiqun Yu. 2018. Data-
efficient Performance Learning for Configurable Systems. Empirical Software
Engineering 23,3 (01 Jun2018).
[31]M. Hajjat, R. Liu, Y. Chang, T. S. E. Ng, and S. Rao. 2015. Application-specific
Configuration Selection in the Cloud: Impact of Provider Policy and Potential
of Systematic Testing. In 2015 IEEE Conference on Computer Communications
(INFOCOM) .
[32]S.Han,Y.Dang,S.Ge,D.Zhang,andT.Xie.2012. PerformanceDebugginginthe
LargeviaMiningMillionsofStackTraces.In Int’lConf.onSoftwareEngineering .
[33]Xue Han, Tingting Yu, and David Lo. 2018. PerfLearner: Learning from Bug
Reports to Understand and Generate Performance Test Frames. In ACM/IEEE
Int’lConf onAutomatedSoftwareEngineering .
[34]Wolfgang Karl Härdle, Marlene Müller, Stefan Sperlich, and Axel Werwatz. 2012.
Nonparametric and semiparametric models . Springer Science & Business Media.
[35]C.Hsu,V.Nair,V.W.Freeh,andT.Menzies.2018. Arrow:Low-LevelAugmented
Bayesian Optimization for Finding the Best Cloud VM. In IEEE Int’l Conf on
DistributedComputingSystems .
[36]PengHuang,XiaoMa,DongcaiShen,andYuanyuanZhou.2014. Performance
RegressionTestingTargetPrioritizationviaPerformanceRiskAnalysis.In Int’l
Conf.onSoftwareEngineering .
[37]Alexandru Iosup, Simon Ostermann, Nezih Yigitbasi, Radu Prodan, Thomas
Fahringer,andDickEpema.2011. PerformanceAnalysisofCloudComputing
Services for Many-Tasks Scientific Computing. IEEE Transcations on Parallel
DistributedSystem 22,6 (June2011),931ś945.
[38]A. Iosup, N. Yigitbasi, and D. Epema. 2011. On the Performance Variability of
ProductionCloudServices.In 201111thIEEE/ACMInternationalSymposiumon
Cluster,Cloudand Grid Computing .
[39]Raj Jain. 1990. The Art of Computer Systems Performance Analysis: Techniques
for Experimental Design,Measurement, Simulation, and Modeling . John Wiley&
Sons.
[40]MilanJovic,AndreaAdamoli,andMatthiasHauswirth.2011. CatchMeifYou
Can: Performance Bug Detection in the Wild. In Int’l Conf. on Object Oriented
ProgrammingSystemsLanguages and Applications .
[41]Charles Killian, Karthik Nagaraj, Salman Pervez, Ryan Braud, James W. An-
derson, and Ranjit Jhala. 2010. Finding Latent Performance Bugs in Systems
Implementations.In Int’lSymp. onFoundationsofSoftwareEngineering .
[42]S. Kullback and R. A. Leibler. 1951. On Information and Sufficiency. The Annals
ofMath. Statistics 22,1 (1951).
[43]Philipp LeitnerandJürgenCito.2016. Patternsinthe Chaos: AStudyofPerfor-
manceVariationandPredictabilityinPublicIaaSClouds. ACMTrans.Internet
Technol.16,3 (April2016),15:1ś15:23.
[44]Mark W. Lenhoff, Thomas J. Santner, James C. Otis, Margaret G.E. Peterson,
BrianJ.Williams,andSherryI.Backus.1999. Bootstrappredictionandconfidence
bands: a superior statistical method for analysis of gait data. Gait & Posture 9, 1
(1999), 10ś 17.
[45]Ming Mao and Marty Humphrey. 2011. Auto-scaling to Minimize Cost and
MeetApplication Deadlinesin Cloud Workflows.In Proc.of Int’lConf.forHigh
Performance Computing, Networking,Storageand Analysis .
[46]Aleksander Maricq, Dmitry Duplyakin, Ivo Jimenez, Carlos Maltzahn, Ryan
Stutsman,andRobertRicci.2018. TamingPerformanceVariability.In USENIX
Symp.onOperatingSystemsDesignand Implementation .
[47] MarissaMayer. 2009. In Searchof A better, faster, strong Web.
[48]Shaikh Mostafa, Xiaoyin Wang, and Tao Xie. 2017. PerfRanker: Prioritization of
PerformanceRegressionTestsforCollection-intensiveSoftware.In Proceedingsof
the26thACMSIGSOFTInternationalSymposiumonSoftwareTestingandAnalysis .
[49]NIST.2013. NIST/SEMATECHe-HandbookofStatisticalMethods. http://www.
itl.nist.gov/div898/handbook/ . [Online;accessed Aug-2018].
[50]AdrianNistor,LinhaiSong,DarkoMarinov,andShanLu.2013.Toddler:Detecting
Performance Problems via Similar Memory-access Patterns. In Int’l Conf on
SoftwareEngineering .
[51]Oracle. [n.d.]. Java Platform, Enterprise Edition (Java EE) 7.
https://docs.oracle.com/javaee/7/index.html. [Online;accessed Aug-2018].
[52]Emanuel Parzen. 1962. On Estimation of a Probability Density Function and
Mode.The AnnalsofMathematical Statistics 33,3 (1962), 1065ś1076.
[53]Michael Pradel, Markus Huggler, and Thomas R. Gross. 2014. Performance
Regression Testing of Concurrent Classes. In Int’l Symp. on Software Testing and
Analysis.
[54] RightScale. 2018. State of the Cloud Report.
[55]MurrayRosenblatt.1956.RemarksonSomeNonparametricEstimatesofaDensity
Function. The AnnalsofMathematical Statistics 27,3 (1956), 832ś837.
[56]Atri Sarkar, Jianmei Guo, Norbert Siegmund, Sven Apel, and Krzysztof Czar-
necki. 2015. Cost-Efficient Sampling for Performance Prediction of Configurable
Systems. In IEEE/ACMInt’lConf onAutomatedSoftwareEngineering .
198ESEC/FSE ’19, August 26ś30, 2019,Tallinn,Estonia Sen He,Glenna Manns,John Saunders,WeiWang,Lori Pollock, andMary Lou Soffa
[57]Jonathon Shlens. 2007. Notes on Kullback-Leibler Divergence and Likelihood
Theory.SystemsNeurobiology Laboratory (2007).
[58]DavidShue,MichaelJ.Freedman,andAneesShaikh.2012. PerformanceIsolation
andFairnessforMulti-tenantCloudStorage.In Proc.ofUSENIXConf.onOperating
SystemsDesignand Implementation .
[59]StoyanStefanov.2008. YSlow2.0.In CSDNSoftwareDevelopment2.0Conference .
[60]MarkD.Syer,ZhenMingJiang,MeiyappanNagappan,AhmedE.Hassan,Mo-
hamedNasser,andParminderFlora.2014. ContinuousValidationofLoadTest
Suites. In Int’lConf.onPerformance Engineering .
[61]Catia Trubiani, Antinisca Di Marco, Vittorio Cortellessa, Nariman Mani, and
Dorina Petriu. 2014. Exploring Synergies Between Bottleneck Analysis and
Performance Antipatterns. In ACM/SPEC Int’l Conf on Performance Engineering .[62]W.Wang,N.Tian,S.Huang,S.He,A.Srivastava,M.L.Soffa,andL.Pollock.2018.
Testing Cloud Applications under Cloud-Uncertainty Performance Effects. In
Int’lConf.onSoftwareTesting,Verification and Validation .
[63]Neeraja J. Yadwadkar, Bharath Hariharan, Joseph E. Gonzalez, Burton Smith,
and Randy H. Katz. 2017. Selecting the BestVM AcrossMultiple PublicClouds:
AData-drivenPerformanceModelingApproach.In ACMSymposiumonCloud
Computing .
[64]Pingyu Zhang, Sebastian Elbaum, and Matthew B. Dwyer. 2011. Automatic Gen-
eration of Load Tests. In Proc. ofInt’l Conf. on Automated Software Engineering .
[65]Timothy Zhu, Michael A. Kozuch, and Mor Harchol-Balter. 2017. WorkloadCom-
pactor: Reducing datacenter cost while providing tail latency SLO guarantees. In
ACMSymp.onCloudComputing .
199