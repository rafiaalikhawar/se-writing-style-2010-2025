Diagnosing Assumption Problems
in Safety-Critical Products
Mona Rahimi
School of Computing
Depaul University
Chicago IL, USA
m.rahimi@acm.orgWandi Xiong
Computer Science
Iowa State University
Ames, IA, USA
wdxiong@iastate.eduJane Cleland-Huang
Computer Science and Eng.
University of Notre Dame
South Bend IN, USA
JaneClelandHuang@nd.eduRobyn Lutz
Computer Science
Iowa State University
Ames, IA, USA
rlutz@iastate.edu
Abstract —Problems with the correctness and completeness
of environmental assumptions contribute to many accidents
in safety-critical systems. The problem is exacerbated whenproducts are modiﬁed in new releases or in new products of
a product line. In such cases existing sets of environmental as-
sumptions are often carried forward without suﬃciently rigorous
analysis. This paper describes a new technique that exploitsthe traceability required by many certifying bodies to reasonabout the likelihood that environmental assumptions are omitted
or incorrectly retained in new products. An analysis of over
150 examples of environmental assumptions in historical systems
informs the approach. In an evaluation on three safety-relatedproduct lines the approach caught all but one of the assumption-
related problems. It also provided clearly deﬁned steps formitigating the identiﬁed issues. The contribution of the work
is to arm the safety analyst with useful information for assessing
the validity of environmental assumptions for a new product.
Index T erms—Environmental assumptions, Safety-critical sys-
tems, Product lines, Software traceability
I. Introduction
Safety-critical systems pervade our society. In order to
reduce time-to-market and development costs, families of
safety-critical software systems increasingly are developed as
software product lines (SPL). Examples include the software in
pacemaker devices, medical infusion pumps, caretaker robots,
brake-assist systems, and ﬂight-control systems.
However, SPLs also introduce new safety-related risks,
such as the risk that an assumption about the operational
environment of one product will be inaccurate for the opera-
tional environment of a subsequent product. A new feature, a
new user, or a new adjacent system in a subsequent product
may result in the need for new or modiﬁed environmental
assumptions for safe operations. We deﬁne an environmental
assumption to be a statement about the software system’s
operational context that is accepted as true by the developers
[57]. This deﬁnition is consistent with common usage on
projects and with dictionary deﬁnitions; however, environmentalassumptions are also often referred to as contextual assumptions
in both literature and practice. Finding inaccurate or missing
environmental assumptions that may have safety-critical impact
for a new product is currently very diﬃcult. This diﬃculty
obstructs the reuse of safety case elements across the products
in a product line.Teams building safety-critical software products must typi-
cally perform a rigorous hazard analysis to identify risks and
a set of mitigating, safety-related requirements. Often these
requirements are associated with environmental assumptions
that must hold in the planned operational context. Although
it only makes sense to talk about the safety of an individual
product, not of an entire product line, prior work has shown
that certain safety analyses (including preliminary hazard
analysis [ 39], and software failure modes, eﬀects and criticality
analysis (FMECA) [ 46]) can be performed during the domain
engineering of a product line and eﬃciently pruned and/or
extended during the application engineering of each product
[17], [13]. Safety analysts often construct a formal or informal
safety case providing claims, arguments, and evidence to
demonstrate the safety of the product, typically to a government
certiﬁcation body. In practice, safety cases are produced from
scratch for each product, with only the assistance of checklists
[50], [ 20]. In this paper we focus on one aspect of reuse
pertaining to the role of assumptions.
Undetected changes in the validity of environmental assump-
tions cause many failures of safety-critical systems [ 35], [25].
Detecting and avoiding the risks associated with changes to
environmental assumptions poses a special problem in safety-
critical product lines. New products often have new operational
environments, but the documented environmental assumptions
used in previous products may be inappropriately reused rather
than updated in the new product. Perhaps the best known
example is the Ariane 5 accident, in which an assumption
made about the maximum horizontal velocity of the previous
product, Ariane 4, did not hold for the subsequent Ariane 5.
As a result, an overﬂow error occurred and both the primary
and backup guidance systems failed [37], [12].
The 2007 U.S. National Research Council report on software
for dependable systems highlighted the danger of inaccurate
domain assumptions. It cited as an example the 1993 Airbus
case in which the invalid assumption, “lack of compression
always accompanies being airborne” [ 35] was a contributing
cause to an accident. The report further stated that “construction
of a dependability case might have revealed that this assump-
tion was invalid,” noting that the dependability case “will
involve reasoning about both the code and the environmental
assumptions.” Dealing with the correctness and completeness
978-1-5386-2684-9/17/$31.00 c/circlecopyrt2017 IEEEASE 2017, Urbana-Champaign, IL, USA
T echnical Research473
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:58:07 UTC from IEEE Xplore.  Restrictions apply. of assumptions in any system, particularly an evolving or
product-line one, is extremely challenging.
We therefore set a realistic research goal of ﬂagging potential
problems in a set of environmental assumptions associated with
a new or modiﬁed product in a product line and of proposing
practical mitigations. Our approach, which we refer to as
the Assumption Diagnostics and Rationale Process (ADRP)
leverages the trace links required by many certifying bodies
for safety-critical systems [ 53], [47] augmented by information
retrieval techniques to search for additional undocumentedlinks [
29], [ 5]. We exploit these links to reason about the
likelihood that assumptions are missing or incorrectly retained
in the current product under release. ADRP was developed
through evaluating 150 documented assumptions from industrial
systems, including cases which contributed to system failure.
Examples are shown in Table I. We describe and evaluate ADRP
using three safety-related product lines for drone deliveries,search-and-rescue missions, and environmental monitoring.Each system was developed over a six month period by ateam of graduate level Software Engineering students. We
address two primary research questions:
RQ1: To what extent can ADRP detect potential risks to the
validity of environmental assumptions in a new product?
RQ2: To what extent can ADRP help a human analyst assess
assumption-related risks and reason about mitigations?
The paper is structured as follows. Section II provides
additional background about safety-critical product lines, envi-
ronmental assumptions, and safety cases. Section III describes
the ADRP diagnostic model and the approach taken to design
it. Section IV describes the experiments and analysis performed
to evaluate ADRP . Finally Sections V through VII describe
threats to validity, related work, and conclusions.
II. Background
Many system failures arise from interactions between soft-
ware and aspects of the environment in which it operates. In
safety-critical systems, these failures can contribute to accidents.
An accident is an unplanned event that results in death, injury,
illness, loss of property or damage to surroundings or habitat
[39], [ 55]. The environment is the broader context of the
software to be developed, that is, the problem world, such as
the hardware on which it runs, concurrently executing software
components, physical devices and surroundings, regulatory
dependencies, and user interactions [62], [58], [35].
The problem of ﬂawed environmental assumptions is well
documented. V an Lamsweerde describes its scope as, “Many
reported problems originate in missing, inadequate, inaccurate
or changing assumptions about the environment in which
the software operates [ 58].” Inadequately handled changes
in environmental assumptions cause many failures and, insafety-critical systems, have caused or contributed to many
accidents.
A software product line (SPL) is a set of software-intensive
systems that share a common set of features and are developedfrom a set of core assets in a prescribed way [
14], [60], [51]. A
product is typically generated by selecting a set of alternativeT ABLE I
Environmental Assumption Types with examples from historical sources.A
complete list of the historical assumptions we identified from the literature
is available online at http://tinyurl.com/ ASE2017AssumptionSamples.
Physical environment: Expected to hold invariantly regardless
of the system, e.g., “A train is moving iﬀ its physical speed is
non-null” [58].
Operational environment: Describes the operational
environment surrounding the system, e.g., “There is no interference
from other wireless devices in the vicinity” [45].
Adjacent system: Describes the behavior of adjacent systemsthat interact with the system being developed, e.g., “The Sensor
will provide the current temperature to the Thermostat with an
accuracy of±0.1
◦F” [19].
User interface: Describes users and their behavior, e.g., “Theoperator will not enter data faster than X words per minute” [39].
Regulatory: Describes how regulations aﬀect the system or
related components, e.g., “The device meets industrial standards
for electrical safety” [7].
Development process: Describes policies or procedures
impacting the development process and/or operation of the system,
e.g., “The developer knows that transient signals should be ignored
when the spacecraft lander’s legs unfold” [2].
and optional features (variabilities) and composing them with
a set of common base features (commonalities). Change
in a product line occurs when new features are introduced
for new products and also when individual products evolve
across releases [ 18]. For product lines, where the operational
environment and intended usage scenarios typically vary among
products, changes to environmental assumptions pose an
especially frequent problem [ 44]. In safety-critical product
lines the problem of ﬂawed environmental assumptions is
complicated by the desire of developers to reuse environmental
assumptions across products. Distinguishing when this reuse
of assumptions is appropriate is essential to avoid hazards but
is currently an open problem. Our work reported here aimsto provide the safety analyst with an eﬃcient way to check
the reuse of safety-related environmental assumptions in a new
product of a product line.
III. Diagnostic Model
The aim of the ADRP diagnostic model is to identify
potential problems with assumptions that are associated with
new or modiﬁed products of a product line. For example,
consider the assumptions, requirements, and classes depicted
in Table II.a which show that assumption A, is associated with
two Requirements R1 and R2 and indirectly with classes C1,
C2, and C3 in product P1. Table II.b shows the case in which
assumption Ais not present in product P2, even though all the
originally associated requirements and classes from P1 remain.
The assumption could have been correctly removed because
it is no longer needed, or incorrectly removed even though it
is still relevant. It also could be the case that the assumption
was replaced by another (possibly similar) assumption. Of
these three scenarios, the second is clearly problematic as the
assumption is still needed, while the third is also problematic
because even though a replacement assumption is provided,
its trace links to the impacted requirements are missing. In an
474
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:58:07 UTC from IEEE Xplore.  Restrictions apply. T ABLE II
Changes in trace dependencies across the baseline (P1)and a new/modified
product (P2).
Sample Trace Slice Description
a
P1: trace slice showing dependencies of
requirements R1 and R2 on assumption A,
and indirect dependencies of classes
C1-C3.
b
P2 vs. P1: Assumption Ahas been
removed in P2 but all other artifacts
dependent upon Ain P1 have been
retained. This may indicate thatassumption Ahas been incorrectly
removed. ADRP uses textual similarity
analysis and structural analysis to search
for an existing replacement assumption.
c
P3 vs. P1: Assumption A, requirements
R1 and R2, and class C2 are missing from
P2. Classes C1 and C3 are retained.
ADRP checks whether C1 and C3 are
service classes and/or implement other
retained requirements. Depending upon
this analysis ADRP ﬂags Aas a potentially
missing assumption with a medium or
high risk.
Artifacts and Links: /square--- P1 only, /square- - - Both P1 and P2
additional example, shown in Table II.c, assumption A, and
much of its original downstream trace slice has been removed
from product P3. Only two classes, C1 and C3 remain. It is
most probable that assumption Ais no longer relevant; however,
further inspection is needed to validate this.
The goal of ADRP is two-fold, ﬁrst to ﬂag errors of inclusion
and exclusion without raising false alarms; and secondly, to
provide the analyst with the information they need in orderto determine whether an assumption-related error exists ornot. Two speciﬁc types of assumption-related error that are
of interest are incorrectly included and incorrectly excluded
assumptions. ADRP leverages knowledge of assumption use in
previous product(s) as well as in the current product to identify
possible problems. However, two speciﬁc sub-cases are outside
the scope of ADRP’s current capabilities. First, ADRP is not
capable of diﬀerentiating between correct and incorrect facts.
For example, given a claim by a train manufacturer that its
model X passenger train, traveling at 50 miles per hour, can
stop in 200 feet under perfect conditions, we assume it to be
true; however, we question the relevancy of the assumption
if and when the usage context or the speciﬁc train modelchanges. Second, some missing assumptions may represent
unknown or even unknowable unknowns [ 54]. These are the set
of assumptions that neither appeared in a previous product norin the domain assets of a product line and are therefore entirely
outside ADRP’s knowledge base. While ADRP does not fully
address these problems, it does draw attention to scenarios in
which they may occur by identifying assumptions for inspectionwhich have not been included in a previously certiﬁed product.
ADRP also draws attention to new features, and to new
combinations of features introduced into a new product – with
the speciﬁc aim of encouraging a systematic analysis thatcould identify and document relevant new assumptions for
these features.
A. ADRP Design Methodology
We adapted Wieringa’s design science approach [ 61]t o
design ADRP . The process included (1) information gathering
and analysis, (2) design, (3) initial validation and reﬁnement,
(4) user evaluation, and (5) feedback based reﬁnement.
During the information gathering and analysis phase, we
reviewed literature related to the use of assumptions in safety-
critical systems (e.g., [ 58], [39], [57], [9], [7], [2], [42]) and
case studies containing assumptions and lifecycle artifacts such
as requirements, models, and safety cases. This allowed us to
reason about the impact of assumptions across the software
development lifecycle. We found evidence that diﬀerent types of
assumptions impacted multiple artifacts including source code
and requirements. During the design phase, we leveraged our
observations to design ADRP . First we identiﬁed properties of a
software product that served as indicators of assumption-related
problems and then deﬁned metrics that enabled propertiesto be measured. We also identiﬁed evidence and counter
arguments which might be used to support or refute the
diagnosis of assumption related problems. Section III describes
these metrics, evidence, and counter arguments.
In the initial validation and reﬁnement phase we tested
ADRP’s logic using examples from the literature (see Table
I) in order to improve ADRP’s design. The design, validation,
and reﬁnement steps were repeated several times until we
were satisﬁed that ADRP was able to identify the majority of
targeted assumption errors. In the fourth phase of external user
evaluation we conducted a more formal study with external
developers as reported in Section IV. We will execute the ﬁnal
step of feedback based reﬁnement in our ongoing work.
B. Product Artifacts
Certiﬁcation guidelines for safety-critical systems prescribe
traceability across a broad suite of artifact types [ 53], [28],
[4], [ 3], [ 48] providing coverage for planning, analyzing,
designing, implementing, verifying, validating, and assuring
the quality of a system. For example, the DO-178C guidelines,
adopted by the US Federal Aviation Authority, specify thattrace links must be created between speciﬁc artifact pairs
including software requirements, design, and source code [ 4].
Requirements traceability is deﬁned as “the ability to describe
and follow the life of a requirement in both a forwards andbackwards direction through periods of ongoing reﬁnement
and iteration” [23].
Safety-critical projects include diverse artifacts and associ-
ated trace links; however, ADRP utilizes a common subset of
artifacts referenced across certiﬁcation guidelines [ 4], [3], [48],
and described in literature discussing the role of assumptions
in safety-critical systems. These artifacts, depicted in Figure
1, include product line speciﬁcations, safety assets, core
implementation artifacts, and documentation. All artifacts used
in this study are available at http:// tinyurl.com/ ADRP-Data.
Assumptions for a product are often documented in the
product’s requirements speciﬁcation document, consistent
475
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:58:07 UTC from IEEE Xplore.  Restrictions apply. Commonalities
Variabilities
ParametersSource CodeActors & 
User InterfacesProduct LineFMECA
Individual Product Configurations External RegulationsOperational & 
Training ManualsAssumptions
RequirementsAdjacent Systems 
from Architectural 
Documents
Fig. 1. Software and Safety Artifacts used by ADRP are often prescribed by
certifying bodies and common across product line deﬁnitions. Arcs represent
traceability paths.
with the recommended requirements engineering practice in
ISO/IEC/IEEE 29148 [34] or the older IEEE Std-830 [33].
Requirements describe the functionality and expected be-
haviour of the system while source ﬁles represent the im-
plementation. A trace slice originates with a single artifact
and shows all of its downstream dependencies. ADRP focuses
on trace slices that originate with assumptions and include
dependent requirements and source ﬁles.Product Lines
deﬁne commonalities, variabilities, dependen-
cies, and constraints. For our study we adopted the Com-
monality and V ariability Analysis (CV A) model [ 60] which
documents commonalities, variabilities, parameters of variation,
and a decision model to conﬁgure the individual products.
Safety Assets include a Failure Mode, Eﬀects and Criticality
Analysis (FMECA) generated for the entire product line and
pruned/extended for each new product [ 17], and a safety case
for each new product requiring it. Safety-related assumptions
are linked to faults in the FMECA, while assumption-related
problems are mapped onto safety cases for analysis purposes.
Supplemental Documentation includes use case speciﬁca-
tions, architectural documents listing adjacent systems, external
regulations, and procedural documents deﬁning training and
operational processes.
C. Artifact Properties and Metrics
ADRP checks for properties in the artifacts of P1 and P2
in order to assess dependencies of artifacts upon assumption
Ain each product. It also identiﬁes the delta between the two
products with respect to assumption A. The properties and their
associated instruments of measurement are used in ADRP’s
diagnostic algorithm and summarized in Table III.
Presence of Assumptions: Diﬀerent classes of assumption
problems are possible when assumptions appear in speciﬁc com-
binations of P1 and P2 products. For example, an assumption
that is present in P1 but not present in P2 could represent an
incorrectly excluded assumption from P2 but not an incorrectly
included one. Similar logic can be applied to an assumption
that is present in P2. ADRP therefore deﬁnes attributes inP 1
and inP 2 to represent whether assumption Ais present in P1
and in P2 respectively.
We provide a concrete example for each of the four
generic Assumption Diagnostics (AD) targeted by ADRP . These
examples come from our catalog of assumptions that were
assembled from historical failures and the literature.AD1 Incorrectly removed assumption: Operational experience
with Unmanned Aerial V ehicles has shown that developers
need to assume that: “The UA V’s camera lens cap will, at
times, be accidentally left on during ﬂight." Therefore, the
assumption must be retained across all products, and software
must accommodate the error, for example by detecting it and
switching to a backup camera [45].
AD2 Assumption missing for new feature: In the Isolette (a
hospital incubator for infants), the software was developedunder the assumption that “All temperatures will be entered
and displayed in degrees Fahrenheit." [ 19]. However, if a new
version supports Celsius, then the original assumption becomes
invalidated and should be replaced by a new one [19].
AD3 Assumption is incorrectly retained. There was an oper-
ational environment assumption that was originally correctregarding the stopping distance of New Y ork subway trains
which was incorrectly retained when heavier trains with longer
stopping distances were introduced. The false assumption
contributed to several accidents [ 32]. This type of assumption
was also the cause of the Ariane 5 rocket failure [43].
AD4 Incorrect new assumption. Developers of the Therac 25
radiation therapy machine introduced a new assumption that the
independent protective circuits and mechanical interlocks used
in the previous Therac 20 system were no longer needed asthe software’s monitoring and control of the hardware wassuﬃcient. This adjacent system assumption was false and
resulted in several fatal accidents [39].
Assumption Properties: ADRP measures two properties
directly from the assumption. First, as our interest is in safety-critical assumptions we deﬁne the boolean attribute
SC (safety
critical) to indicate whether an assumption has a trace linkto a failure mode in its FMECA or not. Only assumptionslinked to the FMECA are considered safety-related. Second,
each attribute is assigned a type. As previously shown in Table
I there are six commonly recognized types of assumptions.
ADRP performs custom analysis for assumptions related
to adjacent system assumptions, user interface assumptions,
regulatory assumptions, and development process assumptions.
For example, if an assumption Ais related to an adjacent
(external) system in P1, but that adjacent system is not used in
P2, then it is unsurprising if assumption Ais not included in P2.
Similar arguments can be made for actors and user interfaces,
applicable regulations, and training and procedural policies.
Therefore, ADRP is concerned with AssumptionT ype.
Trace Slice Metrics: As previously depicted in Table II, the
downstream artifacts which depend on an assumption, can be
modeled as a trace slice. In our current version of ADRP we
deﬁne a boolean metric REM (i.e. remnant) which is set to
true if even one element of the original trace slice from P1
is retained in the new product. This overly stringent metric is
used to ﬂag potential problems, while additional information
about the number and properties of remaining artifacts is used
to populate the diagnostic report with information that enables
the analyst to refute or conﬁrm the diagnoses.
Artifact Similarity: ADRP needs to compute the similarity
between assumptions, requirements, and other artifacts, for
476
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:58:07 UTC from IEEE Xplore.  Restrictions apply. T ABLE III
Properties used by the Assumption Diagnostic and Rationale Process (ADRP)
Metric Description
SC Assumption Aor any of its directly linked requirements
are traced to a high criticality fault in the FMECA
inP1 Assumption Aexists in product P1, A∈P1
inP2 An assumption with identical ID and identical text to
assumption AinP1 exists in product P2
T1 A is associated with an Adjacent System
T2 A is associated with a user interface
T3 A is associated with an external regulatory code
T4 A is associated with a development or training process
M1 Adjacent system associated with assumption Ais retained
(as-is or modiﬁed)
M2 All features linked to actor AC are retained.
M3 All features linked to regulation Rare retained
REM At least one requirement or source ﬁle from the artifacts
linked to AinP1 has been retained in P2
T ABLE IV
Properties for Diagnosing Assumption Problems.
AD1: Incorrectly removed assumption
inPI∧¬inP2∧...
Adj T1∧SC∧(M1∨REM)
UI T2∧SC∧(M2∨REM)
Reg T3∧SC∧(M3∨REM)
Dev T4∧SC
Other¬(T1∨T2∨T3∨T4) ∧SC∧(REM)
AD2: Missing assumption for new feature
¬inPI∧¬inP2∧...
All Warning issued to analyst when new features are
introduced.
AD3: Incorrectly retained assumption
inPI∧inP2
Adj T1∧SC∧(¬M1∨¬ REM)
UI T2∧SC∧(¬M2∨¬ REM)
Reg T3∧SC∧(¬M3∨¬ REM)
Dev T4∧SC
Other¬(T1∨T2∨T3∨T4) ∧SC∧(¬REM)
AD4: Incorrectly added assumption
¬inPI∧inP2
Adj T1∧SC∧(¬M1)
UI T2∧SC∧(¬M2)
Reg T3∧SC∧(¬M3)
Dev T4∧SC
Other¬(T1∨T2∨T3∨T4) ∧SC
example to check whether a new assumption in P2 is textually
similar to one that existed in P1 and therefore could potentiallybe serving as its substitute. Such information later will be used
in the generated reports. We utilize the Vector Space Model
(VSM) to compute the similarity between two artifacts such as a
pair of assumptions, or an assumption and a requirement. VSM
is chosen because it computes quickly and has been shown to
perform consistently, to handle both large and small datasets,
and to treat each artifact as an unstructured bag of words.
This makes it appropriate for use across various artifact types.
The VSM algorithm is described in introductory information
retrieval text books [ 10] and has been broadly used for trace
retrieval purposes [29], [5].Source Code Analysis ADRP performs static analysis to
identify low-level service classes. For source code writtenin Java, C#, etc. we use standard metric analysis tools (e.g.
JHA WK) to compute aﬀerent coupling metrics (fan-in) of the
class. Service classes are not considered as the remaining
artifacts in REM metric.
D. The Diagnostic Algorithm
The properties used to diagnose each of the assumption errors
are shown in Table IV. The table was built systematically as
part of the research design process. We include subcases to
diﬀerentiate between assumptions associated with adjacent sys-
tems, user interfaces, regulatory codes, development processes,
as well as assumptions that do not ﬁt any of these categories
(i.e., other). Each row speciﬁes the properties that must be true
in order for that speciﬁc diagnosis to be made. For example,
the properties shown in the top content row of the table specify
that if an assumption is associated with an adjacent system
(T1), is safety critical (SC), was included in P1 but not P2,
and either the adjacent system referenced by Ain P1 is still
present in P2 or remnants of the A’s previous trace slice still
exist in P2, then we diagnose assumption Ato be incorrectly
excluded from P2. Similar logic is applied for each diagnosis.
The entire process for detecting these properties, given two
diﬀerent versions or products, is fully automated.
E. Implementation
The assumption of the ADRP diagnostic algorithm is that the
set of artifacts shown in Figure 1, and a set of validated trace
links, exist for both products P1 and P2. Trace links include
links between assumptions and requirements, requirements and
code, and requirements and faults. For experimental purposes
we extracted all artifacts from their project environments (i.e.,
requirements and assumptions from Jira, code from Github, and
faults from Excel), and stored them as csv formatted text ﬁles
which are readable by our ADRP tool. ADRP then imported
and parsed all artifacts for products P1 and P2, checked them
for properties deﬁned in Table III, and then automatically
generated a list of assumption diagnoses including a listing
of speciﬁc artifacts that led to the underlying properties being
detected. The entire process was automated except for the ﬁnalstep of constructing the assumption reports (e.g., Figure 2). We
created these manually based upon the listing of assumption
violations and their causes produced by ADRP . We plan to
automate this ﬁnal step in the future.
F . Rationales and Mitigating Steps
Once potential assumption-related problems are detected,
ADRP produces a report designed to aid the safety analyst in
determining whether an assumption-related problem actually
exists. Making this determination is a non-trivial task which
requires human analysis of the assumptions and all of their
dependent artifacts (i.e. requirements, design, and code) in the
original product (P1) and the subsequent one (P2). A report
is produced for all diagnosed assumption problems. These
reports are currently created manually by using the values of
metrics ADRP generates. Such reports are inherently complex,
477
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:58:07 UTC from IEEE Xplore.  Restrictions apply.  
Diagnosis:  Missing Assumption 
 
A1: Iris 3DR drones are able to carry a payload weighing up to 400 grams 
  
 
Information:  
A1 is safety-related and categorized as an  
Adjacent System  assumption . 
A1 is associated with adjacent system:  3DR Iris 
3DR Iris is  present in P1 but not  present in P2. 
A1  is linked to  FM-E3 in FMECA in  P1 which states that : 
FM-E3: Payload exceeds manufacturer’s limits. 
 
Evidence that including A1 in P2 is correct: 
 
-    Assumption A1 is missing from product P2 : - It was associated with adjacent system: 3DR Iris in product P1 
- All classes associated with A1 in P1 are still present in P2 
 
Evidence that including A1 in P2 is problematic:  
- The following  P2 assumption associated with Hexacopter is very similar to A1: A-30: 
“Hexacopter is able to carry a payload weighing up to 1000 grams .”  This suggests 
that Assumption A1 has been replaced. 
- All requirements linked to A1 in P1 have been removed. - The adjacent system, (3DR Iris ) associated with A1 is not pre sent in P2. 
- All source code files linked to A1 in P1 are now linked indire ctly to A30. 
 
Possible Remediations: 
After reading the counter argume nt, if you think A1 should be i ncluded in P2 then: 
-  Consider adding assumption A1 to product P2. If you do add A1, consider recon structing the following trace l inks: 
- From A1 to requirements SMF-188. 
- From A1 to fault FM-E3 in the FMECA 
 
 
Overview of Change scenario  
P1 only P2 only Both P1 and P2 ! 
Medium Risk 
Adjacent System referenced 
by A1 is not present in P2 
Fig. 2. Diagnosis Rationale Report generated for a low risk missing assumption.
(Note: Additional data not shown here.)
as analysts must inspect diverse information sources in order to
decide whether a problem diagnosis indicates a real assumption
problem [22].
As depicted in Figure 2, the report currently includes four
main sections: a diagnostic summary, evidence explaining the
problem diagnosis, a counter-argument providing evidence that
the observations are not indicative of a problem, and ﬁnally
suggested remediations that could be taken to remedy the
problem if the analyst concurs with the diagnosis. The report
is generated using predeﬁned templates which prescribe text tobe output when diagnostic and supporting properties are foundto be true. Sample reports are at http://tinyurl.com/ADRP-Data.
Diagnostic Summary The diagnostic summary is presented
in a header section. It uses the properties deﬁned in TableIV to describe the diagnosis as illustrated in Figure 2. Inthis example, ADRP diagnoses a missing assumption and
provides additional contextual information, for example, that
the assumption is Safety-Related and associated with the 3DR
Iris adjacent system.
Evidence The evidence sections of the report convey in-
formation associated with all detected properties. In our
example, ADRP reports that all classes associated with A1have been retained in P2 even though A1 is missing. This
type of evidence supports the “Missing Assumption” diagnosis.Similarly, ADRP reports that it has found a new assumption that
is textually similar to A1. The new assumption (A-30) statesthat “Hexacopter is able to carry a payload weighing up to 1000
grams.” This provides possible evidence that Assumption A1
was correctly removed and has been replaced.” The templates
for generating each argument were produced as a result of our
research design process.
Warning level Finally, each problem diagnosis is labeled as
Medium orHigh risk. Medium-risk diagnoses are made when
assumptions’ references (e.g., adjacent systems or regulations)
have been removed entirely even though dependent require-ments and/or source code are retained. All other cases are
marked as High Risk.
Visual Overview The report displays a visual summary of the
assumption’s role in P1 and P2 generated automatically by
ADRP using GraphVis [21].
Appendix Finally, ADRP includes supplemental data to aug-
ment each of the evidence snippets. This may include relevant
requirements, source code, and/or design artifacts referenced
by the ADRP reports. This data appears as additional pages in
the Diagnostic report and is not shown in Fig. 2.
IV . Evaluation
We evaluated our two research questions against products
for three diﬀerent cyber physical systems (CPS) as depicted
in Table V. We established the criterion that each of the
projects in our evaluation would include a hazard analysis,
requirements, assumptions, source code and/or detailed class-
level design artifacts [53] as these are common across safety-
critical products. Further, each CPS needed multiple products
with artifacts for each product. Because such datasets are
not currently available in the public domain, we recruited
Professional Masters Students enrolled in a six-month Graduate
Software Engineering Capstone course at DePaul Universityto build such systems. While the data sets are not from
industrial projects, 90% of the graduate students were currently
working full-time in the IT industry. Each CPS was developed
using diverse sensors, actuators, programming languages, and
frameworks, and the resulting products were non-trivial andfully executable. Metadata describing the artifacts for each
product’s baseline are provided in Table VI. For example, the
baseline (i.e. P1) of MedFleet, our largest data set, included 436unique source ﬁles, 78 requirements, 24 assumptions, and over
1,725 trace links. These data sets are quite large in comparison
to the 15 data sets publicly shared by the The Center of
Excellence for Software Traceabilty (CoEST) [ 1]–all of which
have been used extensively on numerous research papers (e.g.,
[26]) but lack the diversity of artifacts needed for our study.
As an additional contribution of this paper, we release the
data sets and ADRP source code for replication and reuse at
http://tinyurl.com/ADRP-Data.
A. RQ1: Diagnosing Assumption-Related Problems
The ﬁrst study addresses RQ1: “To what extent can ADRP
detect potential risks to the validity of environmental assump-
tions in a new product?”
478
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:58:07 UTC from IEEE Xplore.  Restrictions apply. T ABLE V
Products used in ADRP’s evaluation.P 1serves as a baseline, while P2-P4represent subsequent products.
MedFleet (MF) Search&Rescue (SR) Environment (ENV)
P1
Base-lineFleet of Iris 3DR drones delivers medicalsupplies. Aid requests received via a mobileapp. Mission control processes tickets and
plans routes. Ground station manages drones
in ﬂight. Interactive map shows drone locationManages search-and-rescue missions.
Rescuers monitored via health sensors.Current location shown on real-time map.
Directives transmitted to rescuers via a
wrist display.Monitors environmental pollutants
using crowd-sourced mobile sensors.Data is streamed to a central server.
Local pollution levels displayed on a
map in a mobile app.
P2 Hexacopters replace Iris 3DR for longer
ﬂights and heavier payloads, Deliveries
exceed max payload distributed across tickets.History Logging features added. V oicecommands added (for ﬁreﬁghters workingin low visibility conditionsNetwork of ﬁxed position healthsensors integrated into system
P3 Improved accuracy for identifying requester
location through use of interactive map. Drone
base stations elevated above tree line.New speed monitor feature. All terrain
vehicles (A TV) replace human-on-foot.
Impacts range and velocity of search.Network of ﬁxed position health
sensors integrated into system.
P4 Supplies ordered from medical clinic.
Bio-hazardous materials. No mobile app.Use diﬀerent types of alerts corresponding
to diﬀerent situations.A new version is released for blind
users.
T ABLE VI
Products used in the evaluation
Files per Product
Software Artifacts MF SR ENV
Requirements 78 35 20
Assumptions 24 19 18
FMECA Failures 13 18 9
Product Line Commonalities 13 15 10
(PL) V ariabilities 12 9 12
Source Code Java/C# 78 198 17
Java Script 80 18 33
XML 57 87 25
MongoDB/Waspmote 221 0 21
Trace Links Assump-Reqs 77 38 41
FMECA-Reqs 18 23 11
PL Common/V ars)-Reqs 101 30 26
Source Code-Reqs 1,529 1299 113
1) Study Design: Three products, each containing a unique
combination of features, were created for each CPS. All ADRP
properties used to diagnose assumption problems are applicable
at both the source-code and the design level. As reasoning about
environmental assumptions ideally takes place when design-
level decisions are being made, for experimental purposes
we applied ADRP at the design-level. Product P1 therefore
represents a fully-functioning, executable product (i.e. theexisting baseline), whereas Products P2, P3, and P4 include
requirements, assumptions, FMECA, and design-level artifacts
representing classes and their operations.
As a result of creating new products, several changes were
made to assumptions including removing, replacing, adding,
and modifying them. For experimental evaluation purposes, we
injected several assumption-related errors representative of the
change-related assumption problems that we had previouslycollected from real-world historical accidents and accounts
in the literature. These included (AD1) incorrectly removing
assumptions that should have been retained, (AD2) failingto add new assumptions that should have been added fornew features, (AD3) retaining assumptions that should havebeen removed, and (AD4) incorrectly adding unnecessary
assumptions to new products. For example, in the Search and
Rescue product, the assumption that “Android hardware will
have location services and cellular network turned on” wasincorrectly removed from P2 even though location services
and cellular network were still used. MedFleet’s P2 had amissing assumption because it replaced the 3DR Iris with a
HexaCopter and introduced a corresponding new requirement
that “When the total weight of payload exceeds allowedmaximum, the payload must be split into two or more
bundles and dispatched on separate drones,” but introduced noenvironmental assumption describing the maximum weight that
the HexaCopter could carry. In the Environment project, P2
replaced human users carrying mobile collection sensors with
ﬁxed-position sensors. However, the assumption “Collector
users have an Android device with properly working Bluetooth”
was incorrectly retained. Any future functionality dependent
upon this assumption would be problematic. A new assumptionstating that “The gas sensors are not physically damaged” wasincorrectly added to the Environment’s P3 product, even though
P3 used radiation sensors and not gas sensors. ADRP also
provides support for modiﬁcation errors by treating each one
as a deletion followed by an addition. As an example from the
Search & Rescue product, the assumption stating that “The
speed of personnel with an all-terrain vehicle will not exceed
5m/s” was modiﬁed to “The speed of personnel without an
all-terrain vehicle will not exceed 5 m/s”. ADRP treats this as
a deletion followed by an addition and merges both actions
into a single Incorrectly Modiﬁed diagnosis.
An answer set was systematically created for each product
as follows. All safety-critical assumptions that were added,removed, or modiﬁed correctly as part of the product devel-
opment process were marked as “not a problem”. All safety-
critical assumptions modiﬁed as a result of the error-injection
process were marked according to the type of error introduced.
ADRP was then run for products P2, P3, and P4 against
their respective P1 baselines. The diagnostic results were then
compared against the established answer set. Table VII shows
the coverage of the four assumption diagnostics for all three
products in each data set. As depicted in the table (i.e. the sum
of the true positive and false negatives in each row), the data
sets included 20 cases of incorrectly removed assumptions, 4
cases of missing assumptions for new functionality, 11 cases
ofincorrectly retained functions, and 4 cases of incorrectly
added assumptions.
479
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:58:07 UTC from IEEE Xplore.  Restrictions apply. T ABLE VII
RQ1:Results showing Actual versus Diagnosed problems across nine products
MedFleet Search & Rescue Environment
FN TP TN FP FN TP TN FP FN TP TN FP Legend
AD1: Incorrectly Removed Assumption 0 11 4 7 0 5 0 1 0 4 4 1 FN: False Negative (1)
AD2: Missing Assumption for new Feature 0 3 0 0 0 0 0 0 0 1 0 0 TP: True Positive (38)
AD3: Incorrectly Retained Assumption 0 6 29 1 0 2 26 0 0 3 28 2 TN: True Negative (106)
AD4: Incorrectly Added Assumption 1 1 4 0 0 1 5 0 0 1 6 0 FP: False Positive (12)
Totals 1 21 37 8 0 8 31 1 0 9 38 3
8.63%
0.72%14.39%76.26%
False Positives False Negatives True Positives True NegativesPercentage
Fig. 3. RQ1: 90.65% of ADRP’s classiﬁcations were correct. There was one
false negative (i.e. 0.72%). The remaining diagnoses (i.e., 8.63%) were false
positives.
2) Results: We report results for each of the four problem
diagnoses (i.e. AD1, AD2, AD3, and AD4) in Table VII.
Results show that 38 out of 39 problem cases were correctly
diagnosed; however, ADRP also diagnosed 13 additional cases
(False Positives). There was only one case in which an actual
problem was missed. This occurred for MedFleet where a new
assumption was added and associated with an adjacent system.
However, the assumption was not needed and was irrelevant.
It was not connected via trace links to requirements or code
in the product, and should have been diagnosed as incorrectly
added. As a result of this missed diagnosis, and following the
research-design methodology step of ‘feedback and reﬁnement’
we have added this scenario to ADRP’s logic so that such cases
will be classiﬁed as potential incorrectly added assumption
errors in the future. We can now address RQ1 and state that
in our experiment ADRP’s classiﬁcation system successfully
diagnosed 97% of the injected assumption errors at a precision
of 75%.
B. RQ2: User Evaluation of Diagnostic Reports
The second study addresses RQ2: “To what extent can ADRP
help a human analyst assess assumption-related risks and reason
about mitigations?”
1) Study Design: We designed a qualitative study that
approximately parallels the intended usage scenario of ADRP .
The study design was inspired in part by Holzmann’s previous
evaluation of a code review tool for safety-critical systems
[31]. In our experiment, the user, standing in for the safety
analyst, was presented with parts of the ADRP report showing
a diagnosis of an assumption-related problem, and evidence
supporting and countering the diagnosis. We constructed
eight individual reports for the MedFleet system. The reports
contained ﬁve diagnoses of missing assumptions (3 correct
and 2 false), and three diagnoses of incorrectly included
assumptions (1 correct and 2 false). We established this ground
truth by carefully examining each diagnosis and carefully
inspecting the project artifacts and features.Five study participants were recruited. One was a full-time
software engineer with safety experience, and the other four
were graduate computer science students. One of these students
had played an integral role in developing the MedFleet project
and assumed the role of internal safety assessor, while the
others assumed the role of external safety assessor. Each
session started with a 30 minute presentation on the MedFleetsystem, its interactions with adjacent systems, software artifacts
used by ADRP , and the role of assumptions in building
safety critical systems. Participants were then presented with
eight ADRP reports, asked to conﬁrm or refute the diagnoses,
state their conﬁdence in their decision, assess the quality and
completeness of the information provided in the report for
conﬁrming or refuting the diagnosis, and explain their decisions.
Users could also respond as uncertain, or claim that a diﬀerent
problem existed from the diagnosed one.
2) Results: Table VIII summarizes the diagnostic reports
and decisions that users made with respect to the ground
truth. For example, we see that three of the reports diagnosed
AD1; however, only two were correct and the other was a ‘No
Problem’. The columns shown under ‘Analysts’ decision’ depict
the response provided by the user. As there are 5 users, the
numbers in each row sum to a multiple of ﬁve (i.e. 5 ×Correct
Diagnosis Count). For AD1, ﬁve of the analysts agreed with
the AD1 diagnosis; one said there was no problem; 2 claimed
there was a diﬀerent problem (which we discuss shortly); and
2 were uncertain. In the case of AD2 and AD4, all users
successfully accepted the correct diagnoses and rejected the
incorrect ones as not being AD2 or AD4 respectively. However,there was less agreement about the four ‘No Problem’ cases, allof which were presented as speciﬁc problems (i.e. as diagnoses
of AD1-AD4). Of 20 decisions (i.e. 4 cases ×5 users), 3
were marked as AD1 errors, 1 as a AD3 error, 3 as AD4
errors, 3 as uncertain, and only half (i.e. 10) were correctly
recognized as ‘No Problem’ scenarios. We now discuss two of
thecontroversial diagnoses as they provide more insights into
the strengths and weaknesses of ADRP .
In one case ADRP diagnosed a missing assumption of
“A9: The drone will only operate in areas which allow for
unobstructed vertical takeoﬀ and landing.” The assumption was
in P1 but not present in a new product P2 in which drones
only land on elevated base stations. The assumption had been
removed because obstructions from trees and buildings were
considered no longer relevant. One participant stated that there
was no problem in removing the assumption, and three stated
that the assumption should not have been removed. All three
participants who disputed the removal of the assumption stated
480
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:58:07 UTC from IEEE Xplore.  Restrictions apply. that even though the base station was now elevated to avoid
trees, other obstacles might get in the drone’s way and that
a replacement assumption (or even Assumption A9) was still
needed. Their response highlights the kind of thought process
that ADRP makes possible by drawing attention to changes in
the system which potentially impact assumptions.
In general, we observed that raising assumption-related
issues caused our analysts to not only think about whether
the assumptions were correctly or incorrectly included in the
product, but also to consider their correctness. For example,
given a new feature that attached a locator beacon to each
drone and the associated assumption “A40: The locator beacon
can broadcast reliably over a distance of 400 feet” one of our
participants looked up the manufacturer’s product information
and discovered that the actual claim was that the beacon wouldbroadcast over a “range of
up to 400ft (122m) in clear line of
sight . As a result, he marked the assumption as inappropriate
– even though other participants accepted it.
In 50% of the reviews, users were very conﬁdent in their
decisions, while in 37% they were somewhat conﬁdent, and in
12.5% they had low conﬁdence. In 68% of the reviews they
claimed that no additional information was needed and in the
remaining 32% they stated that some additional information
was needed. In general, the cases where additional information
was needed reﬂected the users’ desire to understand more
about the context of the system and information about other
potentially relevant assumptions. We also observed that ourinternal safety assessor took almost three times as long to
complete the study as the external assessors (2 hours versus
45 minutes), inspected artifacts at a more detailed level, used
external resources to check claims on adjacent systems, and
was more likely to propose modiﬁcations to assumptions than
the external assessors. This suggests that the supplementalinformation provided in the ADRP reports is most useful to
safety analysts with more knowledge of the system.
We now address RQ2 based on the ﬁndings from this
study. We conclude that while there was relatively strong
consensus both the users and also between the users and
ADRP’s diagnoses, the primary beneﬁt from ADRP came
from presenting diagnostic information to the users which they
then used to reason about the assumptions in the new system.
3) ADRP Usage Scenarios: In Fig. 4 we illustrate a potential,
practical usage scenario for ADRP , namely guiding a safety
analyst to the parts of a new product or version’s safety case
aﬀected by a change to the assumptions. The safety analyst
here works within an organization to incrementally construct
and analyze the safety case for a new MedFleet product as the
system is developed. Fig. 4 shows part of the safety case for
P1, based on the Goal Structuring Notation (GSN) notation
[27], [36], [16]. This piece of the safety case argues that the
hazard FM-D1, “GPS coordinates fail to reﬂect actual position
of drone,” is adequately mitigated when the three subgoals
G2, G3, and G4 are met, given the assumption A1 and the
operational context C2.
We now consider the use case in which the safety analyst
is constructing the safety case for MedFleet’s new P3. P3 isA1: GPS accuracy on 
Android phone in the 
US averages 8 metersS1: Argument over 
mitigation of hazards
G1: Hazard FM-D1 is 
adequately mitigated
C2: Drone 
operates in US G2: GPS coordinates 
are acceptably close 
to actual position G3: GPS 
coordinates were 
calibrated at launchG4: Communication 
with GPS functions as intendedC1: Hazard FM-D1 means that 
GPS coordinates fail to reflect actual position of drone
Legend:
S: Strategy    G: Goal    C: Context A: Assumption! Possible incorrect 
use of Assumption.  
More info
Fig. 4. Safety Case fragment showing ADRP usage scenario
similar to P1 and also will ﬂy only in the US, but P3 has a
new feature – an interactive map. If the safety case for P3
incorrectly omits the assumption A1 (shaded in gray in Fig. 4),
ADRP will report that A1 may be missing, which guides the
analyst to investigate whether A1 should be added to P3 and
used in P3’s safety case. On the other hand, if we suppose that
P3 is being developed for sale in a country with poorer GPS
coverage than A1 asserts but that P3’s safety case mistakenly
retains A1, ADRP will catch this. It will report that A1 may
be inappropriate, which guides the analyst to consider whether
A1 should be deleted from P3 and not used in P3’s safety case.
In both cases ADRP could be used to map the problematic
assumption to the safety-case element aﬀected by it.
V. Threats to Validity
There are several potential threats to validity. First, to address
the threat that ADRP would not provide suﬃcient coverage of
assumption-related problems we conducted an in-depth study
of problems that occurred in the real-world and which were
reported in historical documents. ADRP was designed to detect
these types of problems.
Second, the CPS products we used for our study were created
by graduate students. However, the projects were non-trivially
sized, used diverse programming languages, architectural
frameworks and platforms, and the majority of team members
were currently employed in the IT industry. Environmen-
tal assumptions were identiﬁed throughout the developmentprocess and included claims made by the manufacturers of
the devices used in each application. The assumption-related
problems that were injected into our products all trace backto categories of errors which we observed in the real worldexamples. Due to time constraints, the graduate teams were
only able to deliver one viable executable project each, and sosubsequent products were constructed by researchers. All of the
ADRP techniques are extensible to larger systems. While we
cannot claim generalizability across all types of safety-critical
systems, our results suggest that ADRP can be eﬀective for
ﬂagging assumption-related errors and can provide users with
information they need to evaluate possible problems.
Third, in order to address RQ2 we engaged students as
proxies for internal and external safety analysts. Only one wasan original developer of the product and only one had external
safety experience. The behavior of the other three is likely to
diﬀer from experts experienced in performing safety analysis.
481
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:58:07 UTC from IEEE Xplore.  Restrictions apply. T ABLE VIII
Analysts’ decisions versus actual assumption problems based on the ground truth behind each diagnosis.
Number of Diagnostics Analysts’ decision
Ground Truth Generated Correct AD1 AD2 AD3 AD4 NP Diﬀ. Prob. Uncertain
Exclusion AD1:Incorrectly Removed Assumption 3 2 5 000 1 2 2
Error AD2:Missing Assumption for New Feature 2 1 0 5 00 0 0 0
Inclusion AD3:Incorrectly Retained Assumption 0 0 0 0 0 00 0 0
Error AD4:Incorrectly Added Assumption 3 1 0 0 0 5 00 0
NP: No Problem 0 4 3 0 1 3 10 03
While the study provided invaluable insights into the use of
ADRP to identify and describe assumption related problems,
such controlled experiments cannot replace actual industrial
usage. However, this type of study is a critical precursor to
designing and evaluating an industrial strength solution.
Finally, the artifacts used by ADRP are likely to be present
in most safety-critical systems; however, they may be modeled
in diﬀerent ways. For example the hazard analysis might be
conducted using fault tree analysis instead of FMECA. While
we do not envision any problems in integrating diﬀerent styles
or conﬁgurations of artifacts, we have not yet evaluated this.
VI. Related Work
While much prior work on validating environmental assump-
tions has been in the area of formal modeling of requirements
[62], [ 58], it is essential to develop better techniques to
support safety analysts’ work on projects without formal
modeling. Recent work incorporates more information about
environmental assumptions into safety cases but, as Graydon
notes, the documentation of assumptions and context is still
informal and has some ambiguity [ 24]. Handling assumptions
in new or changed products is an on-going problem in practice.
For example, Nair et al. found safety-related assumptions
across the automotive, aviation, medical, and railway domains
[49]. Weiss and Leveson presented the risks of reusing safety-
critical software in a diﬀerent environment [ 41], and Leveson
described technical and organizational approaches to not
violating assumptions as a safety-critical system and/or its
environment change [ 39], [40]. Automatically ﬂagging potential
assumption problems, as we propose here, is a necessary
next step. In closely related work De la V ara et al. surveyed
safety analysts and found that most safety-critical systems had
regular modiﬁcations requiring safety analysts to understand
the impact on safety evidence [ 15]. This conﬁrmed the need
for improved support in practice for updating safety artifacts
when software changes. Our work focuses on automatically
detecting problematic assumptions, a historically troublesome
type of safety artifacts, when change occurs.
Some recent results on the reuse of safety cases address
assumptions, although that is not their focus. Most interestingly,
Kokaly et al. described a model management framework for
reusing portions of an assurance case when a system’s design
evolves [ 38]. For product line safety cases, de Oliveira et al.
recorded information about the operational environments in
the safety case to support reuse [16].
In regard to traceability for safety-critical systems, Briand et.
al used trace slices between safety requirements and SysMLdesign models to evaluate design conformance [ 8]. Hill and
Tilley developed a database schema for tracing among safety
artifacts [ 30]. Rahimi documented patterns of changes among
artifacts in a safety-critical system and proposed work to
automate the evolution of trace links [ 52]. Sanchez et al.
described a traceability metamodel and techniques for model-
driven development of safety-critical systems [56].
Several studies have sought to improve automated support
for change management. Borg et al. conducted a series ofinterviews and identiﬁed the need for better tools to helpmaintain traceability information when software changes incomplex systems [
6]. Castro et al., identiﬁed the need for
tools that can integrate information used by both requirements
engineers and safety engineers in order to maintain traceability
links among these artifacts [ 59]. Charrada et al. proposed
tool support to highlight potentially impacted parts of a
requirements speciﬁcation when code changes [ 11]. In contrast,
we focus on automating the diagnosis of potentially problematic
environmental assumptions for a new or changed product.
VII. Conclusion
Safety analysts have diﬃculty determining when an envi-
ronmental assumption needed for one product or release is
inappropriate or missing for a subsequent, related productor release. Such assumption problems have contributed tomany accidents. The work reported here shows the beneﬁtof automating the identiﬁcation of four common risks toassumption validity in a new product. The ADRP technique
described in this paper uses trace links required by certifying
bodies for safety-critical systems, together with informationretrieval searches for additional links, to provide the safety
analyst with insights into assumptions that may be missing or
incorrectly retained in the next product. Results from evaluation
of ADRP show that it consistently diagnosed problematic
assumptions across three safety-critical product lines and that
ADRP’s results, evidence, and suggested remediations helped
users correctly assess the assumptions.
Acknowledgments
The work in this paper was partially funded by the US
National Science Foundation Grants CCF:1647342 and CCF:
1513717.
References
[1] Center of Excellence for Software and Systems Traceability. http:// coest.
org. Accessed: 2016-08-23.
482
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:58:07 UTC from IEEE Xplore.  Restrictions apply. [2] Report on the loss of the Mars Polar Lander and Deep Space 2 missions
(Casani Report). Jet Propulsion Laboratory, California Institute of
Technology, 2000.
[3] ECSS-E-40C: principles and requirements applicable to space software
engineering. 2009.
[4] RTCA/EUROCAE. DO-178C/ED-12C: Software considerations in air-
borne systems and equipment certiﬁcation. 2011.
[5] G. Antoniol, G. Canfora, G. Casazza, A. De Lucia, and E. Merlo.
Recovering traceability links between code and documentation. IEEE
Trans. Softw. Eng., 28(10):970–983, 2002.
[6] M. Borg, J. L. de la V ara, and K. Wnuk. Practitioners’ Perspectives on
Change Impact Analysis for Safety-Critical Software – A Preliminary
Analysis. In SAFECOMP Workshops, pages 346–358, 2016.
[7] Boston Scientiﬁc. Pacemaker system speciﬁcation. http:// sqrl.mcmaster.
ca/_SQRLDocuments/ P ACEMAKER.pdf, 2007.
[8] L. C. Briand, D. Falessi, S. Nejati, M. Sabetzadeh, and T. Y ue.
Traceability and SysML design slices to support safety inspections:
A controlled experiment. ACM Trans. Softw. Eng. Methodol. , 23(1):9,
2014.
[9] L. Brownsword and P . Clements. A case study in successful product
line management. Technical report, CMU/SEI-96-TR-016. Pittsburgh,
P A: SEI, Carnegie Mellon University, 1996.
[10] C.D.Manning, P . Raghavan, and H. Schütze. Introduction to Information
Retrieval. Cambridge University Press, 2008.
[11] E. B. Charrada, A. Koziolek, and M. Glinz. Supporting requirements
update during software evolution. Journal of Software: Evolution and
Process, 27(3):166–194, 2015.
[12] A. Classen. Problem-oriented feature interaction detection in software
product lines. In International Conference on Feature Interactions in
Software and Communication Systems, ICFI, pages 203–206, 2007.
[13] J. Cleland-Huang, M. P . E. Heimdahl, J. H. Hayes, R. R. Lutz, and
P . Maeder. Trace queries for safety requirements in high assurance
systems. In REFSQ, pages 179–193, 2012.
[14] P . C. Clements and L. Northrop. Software Product Lines: Practices and
Patterns. SEI Series in Software Engineering. Addison-Wesley, 2001.
[15] J. L. de la V ara, M. Borg, K. Wnuk, and L. Moonen. An industrial
survey of safety evidence change impact analysis practice. IEEE Trans.
Software Eng., 42(12):1095–1117, 2016.
[16] A. L. de Oliveira, R. T. V . Braga, P . C. Masiero, Y . Papadopoulos, I. Habli,and T. Kelly. Supporting the automated generation of modular product linesafety cases. In W . Zamojski, J. Mazurkiewicz, J. Sugier, T. Walkowiak,
and J. Kacprzyk, editors, Theory and Engineering of Complex Systems
and Dependability - Proceedings of the Tenth International Conference
on Dependability and Complex Systems DepCoS-RELCOMEX, volume
365 of Advances in Intelligent Systems and Computing, pages 319–330.
Springer, 2015.
[17] J. Dehlinger and R. R. Lutz. PLFaultCA T: A product-line software fault
tree analysis tool. Autom. Softw. Eng., 13(1):169–193, 2006.
[18] T. Devine, K. Goseva-Popstojanova, S. Krishnan, and R. R. Lutz.
Assessment and cross-product prediction of software product line quality:
accounting for reuse across products, over multiple releases. Automated
Software Engineering, 23(2):253–302, 2016.
[19] DOT/FAA/AR-08/32. Requirements Engineering Management Handbook.
2009.
[20] K. Fowler. Mission-critical and safety-critical systems handbook: Design
and development for embedded applications. Newnes, 2009.
[21] E. R. Gansner and S. C. North. An open graph visualization system
and its applications to software engineering. Software Practice and
Experience, 30(11):1203–1233, 2000.
[22] M. Goodrum, J. Cheng, R. Metoyer, J. Cleland-Huang, and R. Lutz.
What requirements knowledge do developers need to manage change in
safety-critical systems? In Requirements Engineering Conference (RE),
To appear, 2017.
[23] O. Gotel and C. Finkelstein. An analysis of the requirements traceability
problem. In Proceedings of the First International Conference on
Requirements Engineering, pages 94 –101, April 1994.
[24] P . J. Graydon. Towards a clearer understanding of context and its role inassurance argument conﬁdence. In A. Bondavalli and F. D. Giandomenico,
editors, Computer Safety, Reliability, and Security - 33rd International
Conference, SAFECOMP, volume 8666 of Lecture Notes in Computer
Science, pages 139–154. Springer, 2014.
[25] W . S. Greenwell, E. A. Strunk, and J. C. Knight. Failure analysis and the
safety-case lifecycle. In Human Error , Safety and Systems Development,IFIP 18th World Computer Congress, TC13 /WG13.5 7th Working
Conference on Human Error , Safety and Systems Development, volume
152 of IFIP, pages 163–176. Kluwer/Springer, 2004.
[26] J. Guo, M. Rahimi, J. Cleland-Huang, A. Rasin, J. H. Hayes, and
M. Vierhauser. Cold-start software analytics. In Proceedings of the 13th
International Conference on Mining Software Repositories, MSR 2016,
Austin, TX, USA, May 14-22, 2016, pages 142–153, 2016.
[27] I. Habli and T. Kelly. A safety case approach to assuring conﬁgurable
architectures of safety-critical product lines. In Architecting Critical
Systems, First International Symposium, ISARCS , pages 142–160, 2010.
[28] J. Hatcliﬀ, A. Wassyng, T. Kelly, C. Comar, and P . L. Jones. Certiﬁably
safe software-dependent systems: challenges and directions. In J. D.
Herbsleb and M. B. Dwyer, editors, Proceedings of the on Future of
Software Engineering, FOSE 2014, Hyderabad, India, May 31 - June 7,
2014, pages 182–200. ACM, 2014.
[29] J. H. Hayes, A. Dekhtyar, and S. K. Sundaram. Advancing candidate
link generation for requirements tracing: The study of methods. IEEE
Trans. Softw. Eng., 32(1):4–19, 2006.
[30] J. Hill and S. Tilley. Creating safety requirements traceability for
assuring and recertifying legacy safety-critical systems. In Requirements
Engineering Conference (RE), 2010 18th IEEE International, pages 297
–302, 2010.
[31] G. J. Holzmann. Mars code. Communications of the ACM, 57(2):64–73,
2014.
[32] E. Hull, K. Jackson, and J. Dick. Requirements engineering. Springer
Science & Business Media, 2010.
[33] IEEE. IEEE Recommended Practice for Software Requirements Speciﬁ-
cations. Institute of Electrical and Electronics Engineers, 1998.
[34] ISO/IEC/IEEE. International standard - systems and software engineer-
ing – life cycle processes –requirements engineering. ISO/IEC/IEEE
29148:2011(E), pages 1–94, 2011.
[35] D. Jackson, M. Thomas, and L. I. Millet. In Software for Dependable
Systems: Suﬃcient Evidence?, National Research Council, 2007.
[36] J. Knight. Fundamentals of Dependable Computing for Software
Engineers. Chapman Hall/CRC, 2011.
[37] J. C. Knight. Safety critical systems: challenges and directions. In
Proceedings of the 24th International Conference on Software Engineer-
ing,ICSE, pages 547–550, 2002.
[38] S. Kokaly, R. Salay, V . Cassano, T. Maibaum, and M. Chechik. A model
management approach for assurance case reuse due to system evolution.
InProceedings of the ACM/IEEE 19th International Conference on
Model Driven Engineering Languages and Systems, Saint-Malo, France,
October 2-7, 2016, pages 196–206, 2016.
[39] N. G. Leveson. Safeware, System Safety and Computers. Addison Wesley,
1995.
[40] N. G. Leveson. Engineering a Safer World: Systems Thinking Applied
to Safety. MIT Press, 2012.
[41] N. G. Leveson and K. A. Weiss. Making embedded software reuse
practical and safe. In SIGSOFT FSE, pages 171–178, 2004.
[42] G. Lewis, T. Mahatham, and L. Wrage. Assumptions managementin software development. Technical Report CMU/SEI-2004-TN-021,
Software Engineering Institute, Carnegie Mellon University, Pittsburgh,
P A, 2004.
[43] J.-L. Lions et al. Ariane 5 ﬂight 501 failure, 1996.
[44] R. Lutz, M. Lavin, J. Lux, K. Peters, and N. F. Rouquette. Mining
Requirements Knowledge from Operational Experience, pages 49–73.
Springer Berlin Heidelberg, Berlin, Heidelberg, 2013.
[45] R. Lutz, A. Patterson-Hine, S. Nelson, C. R. Frost, D. Tal, and R. Harris.
Using obstacle analysis to identify contingency requirements on an
unpiloted aerial vehicle. Requirements Engineering, 12(1):41–54, 2007.
[46] R. R. Lutz and R. M. Woodhouse. Requirements analysis using forward
and backward search. Ann. Software Eng., 3:459–475, 1997.
[47] P . Mäder, P . L. Jones, Y . Zhang, and J. Cleland-Huang. Strategic
traceability for safety-critical projects. IEEE Software, 30(3):58–66,
2013.
[48] T. members of the Task Force on Safety Critical Software. 2016:25,
Licensing of safety critical software for nuclear reactors. 2016.
[49] S. Nair, J. L. de la V ara, M. Sabetzadeh, and L. C. Briand. An
extended systematic literature review on provision of evidence for safety
certiﬁcation. Information&Software Technology, 56(7):689–717, 2014.
[50] NASA. Software Safety Standard. 2013.
[51] K. Pohl, G. Böckle, and F. van der Linden. Software Product Line
Engineering: F oundations, Principles and Techniques. Springer-V erlag
New Y ork, Inc., Secaucus, NJ, USA, 2005.
483
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:58:07 UTC from IEEE Xplore.  Restrictions apply. [52] M. Rahimi. Trace link evolution across multiple software versions
in safety-critical systems. In Proceedings of the 38th International
Conference on Software Engineering, ICSE, pages 871–874, 2016.
[53] P . Rempel, P . Mäder, T. Kuschke, and J. Cleland-Huang. Mind the gap:
assessing the conformance of software traceability to relevant guidelines.
InICSE, pages 943–954, 2014.
[54] D. Rumsfeld. Known and Unknown: A Memoir. New Y ork: Penguin
Group, 2011.
[55] J. Rushby. Critical system properties: Survey and taxonomy. Reliability
Engineering and System Safety, 43(2):189–219, 1994.
[56] P . Sánchez, D. Alonso, F. Rosique, B. Álvarez, and J. A. Pastor.
Introducing safety requirements traceability support in model-driven
development of robotic applications. IEEE Trans. Computers, 60(8):1059–
1071, 2011.[57] T. T. Tun, R. R. Lutz, B. Nakayama, Y . Y u, D. Mathur, and B. Nuseibeh.
The role of environmental assumptions in failures of DNA nanosystems.
In1st IEEE/ACM International Workshop on Complex Faults and Failures
in Large Software Systems, COUFLESS, pages 27–33, 2015.
[58] A. van Lamsweerde. Requirements Engineering: From System Goals to
UML Models to Software Speciﬁcations. Wiley, 2009.
[59] J. Vilela, J. Castro, L. E. G. Martins, and T. Gorschek. Integrationbetween requirements engineering and safety analysis: A systematic
literature review. Journal of Systems and Software, 125:68–92, 2017.
[60] D. Weiss and C. Lai. Software product-line engineering: a family-based
software development process. Addison-Wesley, 1999.
[61] R. Wieringa. Design Science Methodology for Information Systems and
Software Engineering. Springer, 2014.
[62] P . Zave and M. Jackson. Four dark corners of requirements engineering.
ACM Trans. Softw. Eng. Methodol., 6(1):1–30, 1997.
484
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:58:07 UTC from IEEE Xplore.  Restrictions apply. 