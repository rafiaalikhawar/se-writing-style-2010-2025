Combining Rule-based and Information Retrieval
Techniques to assign Software Change Requests
Yguarat√£ Cerqueira Cavalcanti
Brazilian Federal Data Processing
Service and
Federal University of Pernambuco
Center for Informatics
ycc@cin.ufpe.brIvan do Carmo Machado
Federal University of Bahia
Computer Science Department
ivanmachado@dcc.ufba.brPaulo A. da Mota S. Neto
Federal University of Pernambuco
Center for Informatics
pamsn@cin.ufpe.br
Eduardo Santana de Almeida
Federal University of Bahia
Computer Science Department
esa@dcc.ufba.brSilvio Romero de Lemos Meira
Federal University of Pernambuco
Center for Informatics
srlm@cin.ufpe.br
ABSTRACT
Change Requests (CRs) are key elements to software main-
tenance and evolution. Finding the appropriate developer
to a CR is crucial for obtaining the lowest, economically
feasible, xing time. Nevertheless, assigning CRs is a labor-
intensive and time consuming task. In this paper, we present
a semi-automated CR assignment approach which combines
rule-based and information retrieval techniques. The ap-
proach emphasizes the use of contextual information, essen-
tial to eective assignments, and puts the development team
in control of the assignment rules, toward making its adop-
tion easier. Results of an empirical evaluation showed that
the approach is up to 46,5% more accurate than approaches
which rely solely on machine learning techniques.
Categories and Subject Descriptors
D.2.7 [ Software Engineering ]: Distribution, Maintenance,
and Enhancement; D.2.9 [ Software Engineering ]: Man-
agement| Life cycle, Productivity, Software quality assur-
ance (SQA)
Keywords
Software Maintenance and Evolution; Change Request Man-
agement; Automatic Change Request Assignment; Bug Triage
1. INTRODUCTION
CRs are software artifacts that describe defects to be xed
or enhancements to be implemented in a software system [8].
CRs are managed with the support of a CR repository soft-
ware, such as Bugzilla1. A CR repository plays a funda-
1http://www.bugzilla.org
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for proÔ¨Åt or commercial advantage and that copies bear
this notice and the full citation on the Ô¨Årst page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior speciÔ¨Åc permission and/or a fee. Request
permissions from permissions@acm.org.
ASE‚Äô14, September 15-19, 2014, Vasteras, Sweden.
Copyright 2014 ACM 978-1-4503-3013-8/14/09$15.00.
http://dx.doi.org/10.1145/2642937.2642964 .mental role in the software maintenance process, being a
common place for communication and coordination among
dierent stakeholders [5 ].
The task of assigning a CR, also referred to as CR triage ,
consists of selecting the most suitable software developer
to handle a given CR. Generally, such a developer is the
one who has enough expertise to handle the issues reported
in the CR [2 ]. In addition, the assignment decision must
take into account the developer's workload, availability, and
the CR priority, in order to obtain the lowest, economically
feasible time to x [ 9]. Thus, this task requires considerable
knowledge of the project, and good communication skills to
negotiate with involved stakeholders [7 ].
Assigning CRs to developers is both labor-intensive and
time consuming, as it is usually regarded as a manual han-
dling task [ 3]. Depending on the software project, the num-
ber of new CRs can vary from dozens to hundreds in a single
day [8 ]. As a consequence, the greater the number of CRs
that are opened, the more complex the problem becomes.
Several research have proposed automated approaches to
overcome the problem of CR assignment by using Informa-
tion Retrieval (IR) techniques [8 ]. Some of these approaches
are based on the hypothesis that the most suitable developer
to handle a new CR is the one who has already solved sim-
ilar CRs [ 1,3,9,11,14]. While other approaches consider
that an appropriate developer can be found by looking at
past CRs and data from version control systems [6, 10,13]
or source code [ 12]. In general, these approaches use IR
techniques to automatically suggest a list of appropriate de-
velopers for a new incoming CR.
Despite the number of proposals, there is no empirical ev-
idence about their applicability to real-world environments.
Most practitioners are still assigning CRs manually. We be-
lieve that current approaches have not been adopted because
of two main problems, as discussed next.
Firstly, existing approaches are usually designed to be au-
tonomous, in the sense that the software analysts do not
have the control of the approach; they cannot modify the
behavior of the approach. Without such control, in turn, the
approach cannot be properly calibrated. As a consequence,
if its performance is not adequate, it is simply discarded.
Secondly, these approaches lack contextual information
necessary to properly assign CRs. Software development
325
companies might have high turnover rates, e.g., developers
move from project to project; developers can be hired/red
during project development; or they can even take a vaca-
tion or a day o. This dynamic inuences the assignment
of CRs. Thus, contextual information impacts the perfor-
mance of automated approaches.
We developed a congurable approach for semi-automated
assignment of CRs which enables software analysts control
its behavior, as well as, it provides a mean to support contex-
tual information necessary to perform eective assignments.
The approach relies on Rule-Based Expert System (RBES)
and IR techniques. It could perform up to 46,5% better than
a Support Vector Machine (SVM)-based approach.
This paper is organized as follows: Section 2 provides
some background on CR management; Section 3 presents
the proposed approach to automate the assignment of CRs;
Section 4 describes the experiment performed to evaluate
the proposed approach; and Section 5 concludes this work.
2. BACKGROUND
When a CR is reported, it follows a workow represented
as a state machine in the CR repository, as Figure 1 il-
lustrates. The workow encompasses the phases Untreated ,
Modication , and Verication, as explained next.
Figure 1: CR generic workow [7 ].
In the rst, Untreated , the CR is reported in the CR repos-
itory. Each CR stores essential information for understand-
ing and implementing the request, such as: short and long
description of the change, the change type (e.g., defect or
enhancement), the target component, and the version of the
software under analysis. Additional elds can be dened in
the CR repository conguration.
In the Modication phase, the development team can ac-
cept the CR or not, and the discussion takes place. When
a CR is accepted, a developer is assigned to the task. As
we previously discussed, he/she should be someone with the
capability of handling the request.
After the assigned developer has implemented the CR,
theVerication phase takes place. This encompasses the
verication of the CR implementation, which is commonly
held by the quality assurance team. The CR may return to
the developer if it needs additional repairs. If a developer
is not able to handle a CR, this can be reassigned to others
until it reaches the appropriate developer. When the CR is
given as properly xed, this workow reaches an end, and
the change can be released for production.
3. CR ASSIGNMENT APPROACH
To support the CR assignment task, we provide tool sup-
port for eective automation. Along this section we de-
scribe the strategy used to perform CRs assignments in
an automated way and the components used to implementsuch strategy as a software solution. Besides, we detail the
method to extract assignment rules.
3.1 Semi-automated Strategy
The approach employs an automation strategy in which
the CRs can be assigned in dierent moments, which we
callphases . Figure 2 shows this strategy as a workow dia-
gram. It integrates an RBES and an IR model with learn-
ing features. While the former is used to implement the
organization-specic rules for assignment, the latter uses his-
torical information to match developers with CRs.
Figure 2: Automation strategy.
In order to explain the workow in Figure 2, we assume
that there is an incoming CR and a list of available develop-
ers as input for the whole process. The expert system and
the IR model are executed in dierent phases of the strategy,
as we describe next.
In the rst phase, the RBES runs over these two inputs
by applying simple rules for CR assignment. Examples
of these rules could be: a)\all incoming critical CRs must
be assigned to Developer A ", and b)\all incoming CRs for
Component XYZ must be assigned to Developer B ".
Such type of rules do not require much processing re-
sources and, as reported in [7 ], it is often sucient to assign
a CR. Thus, by executing them rst, chances are high to
assign a CR in this rst phase.
However, if the simple rules are not helpful to assigning
the CR, then, as a second phase, we apply the IR model
to nd developers that have solved similar CRs in the past.
This is the main technique usually implemented in existing
approaches to reason on CR assignment [7 ]. Then, a new ex-
ecution of the RBES is performed taking as input the list of
appropriate developers. This time, the strategy encourages
the use of more complex rules , so as to combine dierent
variables, such as: availability, developers' workload and ex-
perience, CR severity, and reassignment patterns.
If neither the rst nor the second phases are able to auto-
matically assign the CR, then the third phase takes place.
326The assignment can be performed in two ways, depending
on the strategy conguration: a)it can be performed by the
analyst who manually assign the CR, or b)the CR can be
assigned automatically. In the rst case, the decision of the
analyst may be aided by the list of appropriate developers if
it was successfully retrieved. In the second case, the strat-
egy can be congured to automatically pick some developer,
possibly from the list of appropriate developers.
3.1.1 Controlling the approach‚Äôs behavior
Notably, the presence of the RBES is a fundamental mech-
anism to enable the control of the automated assignments.
That is, the approach can be tuned by changing the rules.
Otherwise, the approach could perform incorrect assignments
and no one would be able to x it.
Furthermore, the strategy of Figure 2 was described in
terms of a default workow for CR assignment. However,
the phases of the strategy could be executed separately ac-
cording to the analyst's needs. We understand that provid-
ing such a control is fundamental to increase the adoption
and eectiveness of the approach.
3.2 Components of the Approach
3.2.1 Rule-Based Expert System (RBES)
We use an RBES to deal with rules for CR assignment,
which is usually context-dependent. We implemented it us-
ing Drools2. Listing 1 shows three CR assignment rules
expressed with Drools.
The rst one is a simple rule which assigns every criti-
cal CR to the developer identied by dev03@fakedev.com ,
as well as CRs which aect component C. The second rule
veries if the textual content of the CR contains the terms
\CRUD-Orders ", and assigns it to dev01@fakedev.com .
rule " Critical CRs or CRs for module C"
when
$cr: CR( severity == CRITICAL || module == "C")
then
$cr. assignTo ( developer ( " dev03@fakedev .com" ))
end
rule "CRs that contain specific text "
when
$cr: CR( text . contains ( "CRUD - Orders " ))
then
$cr. assignTo ( " dev01@fakedev .com" )
end
Listing 1: Example of rules dened with Drools.
3.2.2 Workload Balancing
We can observe that the method availableDeveloper() re-
ceived the parameter WorkloadBalancing.ASSIGNMENTS.
This parameter informs which type of workload balancing
algorithm should be used by the RBES. In our example,
we determined the RBES to apply an algorithm based on
the amount of CRs that each developer has been assigned
to. Note that this algorithm must consider in the compu-
tation only the CRs that are not closed/xed. Other work-
load balancing algorithms could be implemented and used
in our strategy to automated CR assignment, by extending
the classes and interfaces available.
2http://www.jboss.com/drools3.2.3 Information Retrieval model with learning
Figure 3 shows the architecture of the IR model imple-
mented in our solution. It is performed in two steps: i) data
preprocessing and training and ii) recommendation. In the
rst, represented by the solid arrows, the history of CR as-
signments is retrieved from the CR repository to be used
as a training set for the classier. Each CR is preprocessed
before being indexed, in which meaningless words are re-
moved ( Stopper ) and the remaining words are reduced to
their radical ( Stemmer ). Only the xed CRs must be con-
sidered in this step, since they are classied according to the
developers who xed them.
Figure 3: IR model with learning.
Next, the Indexer receives the preprocessed CRs in or-
der to produce the matrices of terms extracted from each
CR, using the following techniques: Term Frequency-Inverse
Document Frequency (TF-IDF) and Latent Semantic Index-
ing (LSI). Both TF-IDF and LSI are further described in
this section. When the indexing is done, the Classier can
query these CRs through the Indexer .
In the second step, represented by dashed arrows, the in-
put CR is preprocessed through the Stopper and Stemmer ,
and passed to the Classier . Next, the classier matches
the CR against the Knowledge Database to retrieve a list of
similar CRs that have already been xed. As a result, the
developers that xed the retrieved CRs are potential ones
to handle the new CR.
3.2.4 Textual representation
For the process shown in Figure 3, the textual information
of each CR is represented in the Indexer using the Vector
Space Model (VSM) model [4 ]. With VSM, the CRs are
represented in a space of term vectors. Each dimension of
this space is represented by a term, which is associated with
a weight, known as TF-IDF. In the physical representation,
the TF-IDF produces a matrix that links each term with the
CRs that contains it.
In addition, the LSI is used to produce a low dimensional
representation of the TF-IDF. LSI is based on Singular
Value Decomposition (SVD). Ahsan et al. [1] identied that
LSI increases the performance of automated CR assignment.
3.2.5 Machine learning algorithm
The SVM algorithm is used to provide learning capabil-
ities to the IR model, since it has the better performance
when recommending developers to new CRs [1, 3,9].
The SVM is a supervised machine learning algorithm.
Such kind of algorithms take as input a set of labeled in-
stances to build and train a classier ; this classier is then
used to assign a label to unknown instances [3 ]. Specically,
SVM is a non-probabilistic linear binary classier based on
computational learning theory, that is used for both regres-
sion and classication tasks.
3273.3 Extraction of Assignment Rules
Currently, the extraction of assignment rules is manually
performed using two techniques: manual analysis of CRs
data; and interviews with the sta.
3.3.1 Manual analysis of CRs
Manual analysis consists of extracting the assignment rules
by sampling the available CR data. It is important to note
that the samples must contain only CRs that have already
been solved. The samples are then manually analyzed with
the objective of identifying patterns of CR assignments.
In order to nd these patterns, basically we must look for
meaningful words in the CR elds, such as title, description,
and steps to reproduce. Meaningful words are those that re-
fer to software functionalities, error messages, specic stack
traces, and names of software components.
3.3.2 Interviews with CR assignees
This technique consists of interviewing those people of
the software projects which are responsible for assigning the
CRs to developers. These can include the project leaders,
managers, developers, or anyone who performs this role.
In these interviews we must try to capture the knowledge
of these people on CR assignments. This knowledge can
be captured by asking only one simple question, such as:
\Could you describe the characteristics of the CRs that the
developer [developer name] is commonly assigned to? ".
4. EXPERIMENT
Machine learning-based approaches have proved to be the
best choice, especially when applying the SVM algorithm. In
this way, this experiment compared the proposed approach
with a pure machine learning approach which uses the SVM
algorithm.
The experiment was performed as an o-line experiment,
in which it was used historical data of CRs assignments from
one of the main software projects of the Brazilian Federal
Data Processing Service (SERPRO). Due to the use of this
historical data, it was not necessary the participation of sub-
jects, except the researcher which was responsible for per-
forming the experiment.
Next, we discuss the research questions and the related
metrics that must be collected to aid with the questions.
4.1 Research Questions
Q1. What is the accuracy of the proposed approach for
automated CR assignment?
Q2. What is the necessary eort to setup the approach in
a software development project?
Q3. Does the achieved accuracy pay the necessary eort
needed in the setup?
4.2 Metrics
M1. For the rst question, the accuracy of our approach
is computed by verifying if the developer that was auto-
matically assigned to a CR is one of the developers who
eventually worked on the CR. For instance, if we run our
approach to automatically assign a set of 100 CRs and, as a
result, 70 of these CRs were correctly assigned to developers
that worked on the respective CRs, then the approach has
an accuracy of 70%.
M2. For the second question, it is simply tracked down
the time spent with the activities to setup the approach.Examples of these activities are algorithm training, rules ex-
traction, and collecting and preparing context information.
The time for each activity was individually collected.
M3. For the third question, it is compared the time saved
with automated assignments in relation to the eort to setup
the approach. We assume that a CR assignment performed
manually takes 10 minutes on average. This time is in ac-
cordance with the results presented in [7]. Thus, whenever
our approach assigns a CR successfully, according to dened
metric for accuracy, 10 minutes are added to the total time
saved with automated assignments.
4.3 Operation
4.3.1 Instrumentation
The instruments of this experiment are the CRs and the
context information data, collected at SERPRO by querying
the available management systems, and the time-sheets used
for measurement.
The CR dataset was extracted from a private software
development project conducted at SERPRO. The software
is a Web-based application mainly developed in the Java
programming language.
As context information, we included information on devel-
opers availability, such as data about developers vacation,
developers project allocation, and developers experience.
Time-sheets were used to track down the eort spent with
the necessary activities to setup the approach, as previously
mentioned. The other instrument is the Java programming
language. Pure Java code is used to access our approach's
Application Programming Interface (API) in order to per-
form the CR assignments.
4.3.2 Testing dataset
We selected 1; 812 CRs that were with their status set to
CLOSED from the two main modules of the aforementioned
project; one module with 781 CRs and the other with 1031.
For condential reasons, we call them simply by Module A
and Module B , respectively. A total of 70 developers have
been assigned to these CRs, thus being considered for the au-
tomated assignments; note that we are counting only those
developers that changed the software in order to x the CR,
being excluded testers, requirement analysts, team leaders,
and project managers.
4.3.3 Rules extraction
In order to extract the assignment rules, we followed the
manual extraction techniques mentioned in Section 3.3. Thus,
we informally interviewed the workers which were responsi-
ble for performing the assignments. These workers consisted
of project managers and team leaders. Four workers were
interviewed. In these interviews, we asked the workers to
classify the developers according to the kind of CRs that
they were commonly assigned to. Then, we formulated some
rules according to this classication. We also analyzed sam-
ples of CRs from the dataset to discover new rules. Finally,
it was formulated 17 assignment rules.
4.3.4 ConÔ¨Åguration of the proposed approach
In Section 3, we introduced the strategy to automated
assignment in our approach, which species that each step
in the strategy could be congured, such as dening which
steps should be executed. For this experiment we congured
328the approach to execute the three possible phases, that is:
1) execute simple rules; 2) execute complex rules (includ-
ing the suggestion of appropriate developers); and, nally,
3) suggest developers for assignment. For this last phase,
the CR was simply assigned to the rst developer in the
recommendation list.
Additionally, we developed an algorithm for workload dis-
tribution to support the formulated rules, as well as the list
of recommended developers. The algorithm computes the
workload of each developer according to the amount of as-
signed CRs. Whenever two or more developers suggested
had the same amount of CRs assigned, one of them was
selected randomly.
4.3.5 Training the SVM algorithm.
We used the SVM machine learning algorithm, imple-
mented in the Weka project. The algorithm was then con-
gured encompassing stopwords and stemming. In order to
train the SVM algorithm, we used a 10-fold cross validation
with the same dataset being randomized 10 times, which
helps to increase the generalization of the results.
4.4 Analysis and Interpretation
Next, we provide the analysis and interpretation of the
experiment results, answering each research question.
4.4.1 Q1. What is the accuracy of the proposed ap-
proach for automated CR assignment?
Results with SVM. We observed a higher average ac-
curacy of 33,1% when assigning CRs from the Module A and
20% for Module B. Since SVM deals with text similarity, the
dierent values for accuracy in the two modules is possibly
due to the vocabulary used in the CRs. It is possible that
the vocabulary used in the CRs of Module A is easier for
machine learning than the one used in Module B.
Results with the proposed approach. We observed
that our approach had a higher average accuracy of 39,2%
when trying to assign the CRs from Module A. In regard-
ing to the CRs from Module B, our approach achieved an
average of 29,3% at its best. For the CRs of Module A, it
represents an improvement of 18,4% on the accuracy and, for
Module B, it represents a signicant 46,5% of improvement.
4.4.2 Q2. What is the necessary effort to setup the
approach in a software development project?
As previously mentioned, our approach needs some ex-
tra eort for conguration, such as the denition of the as-
signment rules and extraction of context information. Al-
though the retrieval of context information in SERPRO was
straightforward, extra work was necessary to insert this in-
formation in the database used by the approach. Thus, two
hours was necessary for processing and integrating the con-
text information in the approach.
Regarding rules denition, we spent about 36 hours. This
involved the manual analysis and statistics of CR data. Thus,
for the complete preparation of our approach prior to the ex-
periment's execution, we spent about 38 hours.
4.4.3 Q3. Does the achieved accuracy pay the nec-
essary effort needed in the setup?
As stated in the denition of the metric M3, for each CR
correctly assigned we considered a reduction of 10 minutes
that would be needed to perform the assignment manually.Thus, we used the Formula 1 to compute the amount of time
that would be saved with automated assignments.
hours =X
m2(A;B )Acr(m)10min
60(1)
The variable Acr(m) is the number of correctly assigned
CRs achieved with the respective approach for the module
m, given by Formula 2, where Ncr(m) is the number of CRs
for module mandAcc(m ) is the higher average accuracy for
module m.
Acr(m) =Ncr(m)Acc(m );form2(A; B ) (2)
Considering both Modules A and B, SVM correctly as-
signed 465 CRs, which would save 77,5 hours of manual
assignments. On the other hand, our approach assigned
608 CRs, that could save 101,3 hours of manual assignments.
Thus, our approach could save 23,8 hours more that pure
SVM approaches.
Thus, for the scenario that we used to perform the valida-
tion, our approach would not pay for itself, since the econ-
omy of 23,8 hours is less than the time needed to congure
the approach, which was 38 hours.
However, it is important to observe that this result is not
conclusive for a long term running experiment. This is ex-
plained by fact that, as the time passes, more CRs would be
reported and less conguration eort would be necessary. As
a consequence, the amount of manual assignments avoided
would keep increasing while the eort would be stable, thus
leveraging the payo of the approach.
4.5 Threats to Validity
External validity. Firstly, the dataset of CRs used in
the experiment may not be representative for other orga-
nizations and projects. As we used data of a large private
project from a large organization, the results may not apply
to small projects and organizations, or open source projects.
Secondly, the learning process of SVM is based on textual
similarity, and quality of the CRs's data may impact its per-
formance Matter et al. [13]. In this experiment, we included
all textual elds of the CRs in the learning process. As we
have not assessed the text quality, chances are that some of
these elds could have impacted on the SVM performance.
Internal validity. The threat regarding internal validity
concerns measuring the time spent to congure the proposed
approach, such as dening the rules. This was an iterative
process of trail and error, which made it dicult to mea-
sure the time spent. Thus, there may be some bias in the
reported values, but not sucient to invalidate the results.
4.6 Experiment Considerations
The rst point concerns to the accuracy of the SVM algo-
rithm. We understand that the SVM and the testing dataset
could be tweaked in order to produce better accuracy results;
however, it would require more preparation eort prior to
the experiment execution. Additionally, since the proposed
approach also relies on the SVM algorithm, the tweaking
would produce better results for both pure machine learning
approach and the new proposed approach. Consequently,
the extra preparation eort would not change the conclu-
sions of the experiment and, thus, was unnecessary.
The second point refers to the performance of the pro-
posed approach. In order to test it, we formulated assign-
ment rules and designed an algorithm for workload balanc-
329ing. These two aspects have a signicant impact on the
performance of the approach. As a consequence, the cre-
ation of other rules, as well as other algorithms for workload
distribution, could considerably improve the performance of
the approach. On the other hand, such a work would re-
quire more eort and could decrease the approach's payo.
We consider that the rules and the workload algorithm used
were sucient to the experiment purpose.
A third point is in respect with the time spent to formu-
late the assignment rules. We did not have previous experi-
ence with extracting such rules. It was necessary to dene
mechanisms to help us with discovering these rules from his-
torical data, such as: interviews with the management team,
statistical analysis, and manual analysis of CRs. As a con-
sequence, our inexperience could inevitably produce inac-
curate rules, thus prejudicing the results of the experiment.
However, as more experience is acquired, more eective rules
can be formulated, as well as new methods to extract them.
Finally, as a fourth point, we should consider the dier-
ence between the current assignments of CRs, that is, those
found in historical data, and the ideal assignments . This
last refers to the assignments that aim at optimizing the
CRs xing schedule. This experiment compared the auto-
mated assignments in relation to the current assignments .
As a consequence, we cannot ensure that the proposed ap-
proach produced better assignments. In order to measure
the quality of assignments, it would be necessary other kind
of experiments with more costs and eort involved.
5. CONCLUSION AND FUTURE WORK
In this paper, we proposed a novel semi-automated ap-
proach to assign CRs to software developers. It employs a
strategy to automate CR assignments based on Rule-Based
Expert System (RBES) and Information Retrieval (IR) tech-
niques. The RBES provides a means to cope with dynamic
environments and contextual information through the use of
assignment rules. The RBES is aided by the the IR model
which use historical information from assignment history.
The experiment to validate the proposed approach indi-
cated an improvement of assignment accuracy up to 46,5%
over approaches that rely solely on machine learning algo-
rithms. This means that a rule-based approach is a simple
and ecient solution that can leverage CR assignments.
Despite the observed improvement, there are open issues
regarding the approach to investigate. The rst issue con-
cerns the extraction of assignment rules from the CR repos-
itories, which is manually performed. Future work should
investigate automated methods to extract these rules.
The second issue encompasses the analysis of algorithms
for balancing the workload among developers. We have
tested a balancing algorithm that relies only on amount
of CRs assigned. However, developers have other activities
that cannot be measured by only counting CRs. Thus, we
aim at tuning our approach with other balancing algorithms.
Finally, context information is rather dynamic, as it changes
very frequently. Thus, we need to provide mechanisms that
enable to eortlessly evolve the assignment rules.
Acknowledgments
Thanks to the Brazilian Federal Data Processing Service
(SERPRO) and Center for Informatics of Federal Univer-
sity of Pernambuco (CIn/UFPE) for the support on the ex-ecution of this research and to the ASE reviewers for their
valuable feedback.
References
[1] S. N. Ahsan, J. Ferzund, and F. Wotawa. Automatic
software bug triage system (bts) based on latent semantic
indexing and support vector machine. In Proc. of the 4th
Inter. Conf. on Soft. Eng. Adv. (ICSEA) , 2009.
[2] I. Aljarah, S. Banitaan, S. Abufardeh, W. Jin, and
S. Salem. Selecting discriminating terms for bug
assignment: a formal analysis. In Proc. of the 7th Inter.
Conf. on Predictive Models in Soft. Eng. ACM, 2011.
[3] J. Anvik, L. Hiew, and G. C. Murphy. Who should x this
bug? In Proc. of the 28th Inter. Conf. on Soft. Eng.
(ICSE) , 2006.
[4] R. A. Baeza-Yates and B. Ribeiro-Neto. Modern
Information Retrieval . Addison-Wesley Longman
Publishing Co., Inc., 1999.
[5] D. Bertram, A. Voida, S. Greenberg, and R. Walker.
Communication, collaboration, and bugs: the social nature
of issue tracking in small, collocated teams. In Proc. of the
2010 ACM Conf. on Comp. Supported Coop. Work
(CSCW) . ACM, 2010.
[6] G. Canfora and L. Cerulo. Supporting change request
assignment in open source development. In Proc. of the
ACM Symp. on Applied Computing (SAC) , 2006.
[7] Y. C. Cavalcanti, P. A. M. S. Neto, I. C. Machado, E. S.
Almeida, and S. R. L. Meira. Towards Understanding
Software Change Request Assignment: A survey with
practitioners. In Proc. of the 17th Inter. Conf. on
Evaluation and Assessment in Soft. Eng. (EASE) , 2013.
[8] Y. C. Cavalcanti, P. A. M. S. Neto, I. C. Machado, T. F.
Vale, E. S. Almeida, and S. R. L. Meira. Challenges and
Opportunities for Software Change Request Repositories: a
systematic mapping study. J. of Soft.: Evolution and
Process , 2013. Online rst.
[9] G. A. Di Lucca, M. Di Penta, and S. Gradara. An
Approach to Classify Software Maintenance Requests. In
Proc. of the 18th IEEE Inter. Conf. on Soft. Maintenance
(ICSM) , 2002.
[10] H. Kagdi, M. Gethers, D. Poshyvanyk, and M. Hammad.
Assigning change requests to software developers. J. of
Soft.: Evolution and Process , 24(1):3{33, 2012.
[11] Z. Lin, F. Shu, Y. Yang, C. Hu, and Q. Wang. An
empirical study on bug assignment automation using
chinese bug data. In Proc. of the 2009 3rd Inter. Symp. on
Empirical Soft. Eng. and Measurement (ESEM) , 2009.
[12] M. Linares-V asquez, K. Hossen, H. Dang, H. Kagdi,
M. Gethers, and D. Poshyvanyk. Triaging Incoming
Change Requests: Bug or Commit History, or Code
Authorship? In Proc. of IEEE Inter. Conf. on Soft.
Maintenance (ICSM) , 2012.
[13] D. Matter, A. Kuhn, and O. Nierstrasz. Assigning bug
reports using a vocabulary-based expertise model of
developers. In Proc. of the 2009 6th IEEE Inter. Working
Conf. on Mining Soft. Repositories (MSR) , 2009.
[14] M. M. Rahman, G. Ruhe, and T. Zimmermann. Optimized
assignment of developers for xing bugs an initial
evaluation for eclipse projects. In Proc. of the 3rd Inter.
Symp. on Emp. Soft. Eng. and Meas. (ESEM) , 2009.
330