An Experience Report on Scaling Tools for Mining
Software Repositories Using MapReduce
Weiyi Shang, Bram Adams, Ahmed E. Hassan
Software Analysis and Intelligence Lab (SAIL)
School of Computing, Queen‚Äôs University
Kingston, Ontario, Canada
{swy, bram, ahmed}@cs.queensu.ca
ABSTRACT
The need for automated software engineering tools and tech-
niques continues to grow as the size and complexity of stud-ied systems and analysis techniques increase. Software engi-neering researchers often scale their analysis techniques us-
ing specialized one-oÔ¨Ä solutions, expensive infrastructures,
or heuristic techniques (e.g., search-based approaches). How-ever, such eÔ¨Äorts are not reusable and are often costly tomaintain. The need for scalable analysis is very prominent
in the Mining Software Repositories (MSR) Ô¨Åeld, which spe-cializes in the automated recovery and analysis of large data
stored in software repositories. In this paper, we explore
the scaling of automated software engineering analysis tech-niques by reusing scalable analysis platforms from the webÔ¨Åeld. We use three representative case studies from the MSRÔ¨Åeld to analyze the potential of the MapReduce platform to
scale MSR tools with minimal eÔ¨Äort. We document our
experience such that other researchers could beneÔ¨Åt fromthem. We Ô¨Ånd that many of the web Ô¨Åeld‚Äôs guidelines forusing the MapReduce platform need to be modiÔ¨Åed to betterÔ¨Åt the characteristics of software engineering problems.
Categories and Subject Descriptors
D.2.8 [Software Engineering ]: Metrics‚Äî performance mea-
sures
General Terms
Performance
Keywords
Mining Software Repositories; Cloud computing; MapRe-
duce
1. INTRODUCTION
The Mining Software Repositories (MSR) Ô¨Åeld recovers
and studies data stored in large software repositories, includ-ing source control repositories , bug repositori es, archived
communications, deployment logs, and code repositories [23].
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted w ithout fee provided that copies are
not made or distributed for pro Ô¨Åt or commercial advantage and that copies
bear this notice and the full citation on the Ô¨Årst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speci Ô¨Åc
permission and/or a fee.ASE‚Äô10, September 20‚Äì24, 2010, Antwerp, Belgium.
Copyright 2010 ACM 978-1-4503-0116-9/10/09 ...$10.00.The MSR Ô¨Åeld is one of the many Ô¨Åelds within software en-
gineering that continue to beneÔ¨Åt from the development ofautomated software engineering tools and techniques.
These automated techniques continuously need to scale as
larger systems are being analyzed and more complex tech-
niques are being used to analyze these systems. For instance,
recent studies show that the Debian Linux distribution dou-
bles in size every two years (currently at 323M SLOC [4,17]),while recent eÔ¨Äorts [2,11,30] continue to archive very largerepositories of source code based on the strong belief that thewisdom of thousands of coders can help improve the qualityof any one project. Moreover, complex model checking and
analysis techniques continue to be developed to help locate
bugs in large software (e.g., [38]).
To cope with the scale of the analyzed data and the com-
plexity of the used algorithms, researchers often make use ofone-oÔ¨Ä solutions, heuristic-based optimizations (e.g., search
based software engineering [20]), or specialized commercial
systems (e.g., [8]). However, these solutions are too expen-sive to acquire or maintain, and they often require lengthydevelopment time. The lack of oÔ¨Ä-the-shelf ways to scaleanalysis techniques hinders research progress, as researchersspend considerable time tackling side-problems that are of
limited interest to them, but which they must solve to ensure
the adoption of their research in practice. For example, D-CCFinder [28], a distributed version of the CC-Finder [26]clone detection tool, achieves a speed-up factor of 20 us-ing a custom client/server architecture consisting of 80 PCs.This specialized architecture requires substantial develop-
ment and maintenance eÔ¨Äort to keep it running correctly.
Standard platforms are needed that would enable large-
scale studies with minimal eÔ¨Äort and without the need for
continuous maintenance. Over the past decade, the webÔ¨Åeld has developed a signiÔ¨Åcant expertise in dealing withlarge-scale problems. That community has developed sev-
eral standard platforms that have been extensively reused
by its members. Hadoop [39] and Pig [33] are examples ofsuch platforms. We Ô¨Årmly believe that our community canbeneÔ¨Åt from these platforms to scale software engineeringstudies. In prior work [35], we explored the use of Hadoop, aMapReduce [15] implementation, to scale and speed-up one
particular software evolution study. We could reduce the
running time of the study tool by 60 to 70%. It is not clear,though whether Ô¨Åndings generalize to other kinds of softwareengineering analyses, and how to address the challenges weencountered with conÔ¨Åguration and designing MapReduce
strategies.
In this paper, we use the MSR Ô¨Åeld, as a sub-Ô¨Åeld within
275
software engineering, to study the beneÔ¨Åts and challenges of
scaling several software engineering analyses using the large-
scale data processing platforms. In particular, we use three
representative case studies from the MSR Ô¨Åeld to demon-strate that the MapReduce web analysis platform could be
used to successfully scale MSR tools with minimal eÔ¨Äort.
The main contributions of our paper are as follows:
1. We document our experience in scaling several MSR
problems, such that other researchers could beneÔ¨Åt
from our experience.
2. We also report the changes needed to the web Ô¨Åeld‚Äôs
guidelines for the MapReduce platform when applying
MapReduce to MSR analyses. These changes high-
light the diÔ¨Äerent characteristics of software engineer-
ing analyses compared to web analyses and must be ad-
dressed to ensure that software engineering researchers
get the most beneÔ¨Åt out of the MapReduce platform.
While we apply scalable web analysis platforms in the
context of MSR analyses, we believe that many software
engineering research problems that require automated anal-ysis would beneÔ¨Åt from these platforms. We hope that ourwork will encourage other researchers to explore the scalingof their automated techniques using such platforms.
The rest of the paper is organized as follows. Section 2
provides the background and related work of our research.
MapReduce and the expected challenges of migrating MSR
tools to MapReduce are introduced in Section 3. We presentour case study in Section 4, followed by a report about ourexperiences in addressing the challenges in Section 5. Sec-
tion 6 evaluates the ability of MapReduce to scale diÔ¨Äerent
types of MSR analyses. Section 7 discusses threats to valid-ity. Finally, Section 8 presents the conclusions of this paper.
2. BACKGROUND
Trends in MSR. In recent years, two major trends can
be observed in the MSR Ô¨Åeld that are also representative
for many other Ô¨Åelds of ASE. The Ô¨Årst trend is that the
data analyzed by software engineering researchers is explod-
ing in size. Recent empirical studies exhibit such a trend,with many researchers exploring large numbers of indepen-dent software products instead of a single software prod-
uct. Empirical studies on Debian GNU/Linux by Gonzalez-Barahona et al.[17] analyze up to 730 million lines of source
code from 6 releases of the Debian distribution, which con-
tains over 28,000 software packages. Similarly, Mockus andBajracharya et al.have been developing methods to amass
and index TBs of source code history data [11,30]. Estima-tion indicates that an entire year of processing is needed to
amass such large source code [30]. This growth of data is
not exceptional. Studies show that the Debian distributionis doubling in size approximately every two years [17].
A second trend in software engineering is the use of ever
more sophisticated automated techniques. Clone detection
techniques are examples of this trend. Text-based and token-
based techniques, such as CC-Finder [26], use raw source
code or lexical ‚Äútokens‚Äù to detect code clones in a softwareproject. However, as these clone detection techniques areonly able to detect a limited number of clone types [34],more complex techniques that require much more comput-ing power and running time are needed to detect more types
of code clones with higher precision.Approaches to scale MSR. The growth of data and
the increase in the complexity of MSR studies bring many
challenges that hinder the progress of the MSR Ô¨Åeld. Yet,there is little work that aims to address these challenges.
To enable large-scale MSR studies, researchers continue to
develop ad hoc solutions that migrate MSR studies to dis-tributed computing environments. The simplest and most
naive way is using batch scripts to split input data across
a cluster of machines, deploy the tools (unchanged) to themachines, run the tools in parallel, and Ô¨Ånally merge the out-put of every machine. However, naive approaches mostly donot support load balancing, error recovery and require ad-ditional programming eÔ¨Äort. Other approaches, such as D-
CCFinder [28], Kenyon [14] and SAGE [16], re-engineer the
original, non-distributed MSR study tools to enable themto run on a distributed environment. Distributed comput-ing libraries, such as MPI [19], can assist in developing dis-tributed MSR study tools. However, the re-engineering ofexisting tools requires additional programming eÔ¨Äort and
software engineering researchers are neither experts in dis-
tributed system programming nor willing to spend eÔ¨Äort onthe programming.
Over the past 20 years, parallel database systems, such
as Vertica [8], have been used to perform large-scale dataanalyses. Recently, work by Stonebraker et al .[37] shows
that parallel database systems are challenging to install and
conÔ¨Ågure properly and they typically do not provide eÔ¨Écientfault tolerance. MSR researchers are neither experts in in-stalling parallel databases nor can they aÔ¨Äord the time tolearn the intricacies of such systems. Moreover, MSR exper-
iments typically only extract and read large amounts of data
from software repositories, without ever updating this data.Using parallel database system is not an optimal solutionfor scaling MSR experiments.
Search-based software engineering (SBSE) [21] holds great
promise for scaling software engineering techniques by trans-
forming complex algorithms into search algorithms, which
yield approximate solutions in a shorter time span. Forexample, Kirsopp et al .[27] use various search algorithms
to Ô¨Ånd an accurate cost estimate for software projects. In
addition to optimized performance, most search algorithms
are naturally parallelizable to support even larger scale ex-
periments [20]. However, SBSE only oÔ¨Äers a set of gen-
eral techniques to solve problems, and still require consider-able application-speciÔ¨Åc customization to achieve signiÔ¨Åcantspeed-ups. Not all MSR analyses beneÔ¨Åt from approximatesolutions either.
The web Ô¨Åeld has developed large-scale data analysis plat-
forms over the years. These platforms, such as MapRe-
duce [15], are designed to run in distributed environments,and typically leverage a distributed data storage technique.
Widely and successfully used in the web Ô¨Åeld, these plat-
forms are able to analyze massive amounts of web data. Be-
cause the intensive analyses in the MSR Ô¨Åeld and the web
Ô¨Åeld are both scan-centric (no random access) and read-only,these platforms are very promising for scaling MSR analy-ses. Shang et al.[35] have presented preliminary results that
demonstrate that the analysis of large-scale software engi-
neering data could beneÔ¨Åt from such large-scale data anal-
ysis platforms, despite a number of challenges. This paper
explores whether MapReduce can successfully scale a rangeof typical MSR analyses, discusses how to address the var-ious challenges of migrating MSR analyses to MapReduce
276and analyzes the diÔ¨Äerences between guidelines from the web
Ô¨Åeld and our experience. We then show the applicability of
MapReduce in scaling other types of MSR analyses.
3. MAPREDUCE
MapReduce is a distributed platform for processing very
large data sets [15]. The platform, originally proposed by
Google, is used by Google on a daily basis to process large
amounts of web data.
MapReduce enables a distributed divide-and-conquer pro-
gramming model. The model consists of two phases: a mas-
sively parallel ‚ÄúMap‚Äù phase, followed by an aggregating ‚ÄúRe-duce‚Äùphase. The input data for MapReduce is broken down
into a list of key/value pairs. Mappers (processes assigned to
the ‚ÄúMap‚Äù phase) accept the incoming pairs, process themin parallel and generate intermediate key/value pairs. Allintermediate pairs having the same key are then passed to aspeciÔ¨Åc Reducer (process assigned to the ‚ÄúReduce phase‚Äù).Each Reducer performs computations to reduce the data to
one single key/value pair. The output of all Reducers is the
Ô¨Ånal result of a MapReduce run.An Example of MapReducing an MSR analysis.
To illustrate how MapReduce can be used to support
MSR, we consider performing a classical MSR analysis of
the evolution of the total number of lines of code (#LOC)
of a software project. The input data of this MSR analysis is
a source code repository. The repository is broken down intoa list of key/value pairs as ‚Äúversion number/source code Ô¨Åle
name‚Äù . Mappers accept every such pair, count the #LOC of
the corresponding source Ô¨Åle and generate as intermediatekey/value pair ‚Äúversion number/#LOC‚Äù . For example, for a
Ô¨Åle with 100 LOC in version 1.0, a Mapper will generate a
key/value pair of ‚Äú1.0/100‚Äù . Afterwards, each list of key/-
value pairs with the same key, i.e., version number, is sent to
the same Reducer, which sums #LOCs in the list, and gen-erates as output the key/value pair ‚Äúversion number/SUM
#LOC‚Äù . If a Reducer receives a list with key ‚Äú1.0‚Äù,a n dt h e
list consists of two values ‚Äú100‚Äù and‚Äú200‚Äù, the Reducer will
sum the values ‚Äú100‚Äù and‚Äú200‚Äù and output ‚Äú1.0/300‚Äù .
The MapReduce platform holds great promise for scaling
MSR experiments, because it is
1.a mature and proven platform. MapReduce is
widely used with great success by the web Ô¨Åeld andother communities. For example, the New York Timeshas recently used MapReduce to transform all its oldarticles into PDF format in a cost-eÔ¨Äective manner [6].
2.a simple and aÔ¨Äordable solution. MapReduce
uses a simple, distributed divide-and-conquer program-
ming model. MapReduce can be deployed on commod-ity hardware, which makes scaling MSR experimentsmore aÔ¨Äordable.
3.a read-optimized platform. MapReduce is designed
to perform large-scale read-only data analyses, such asthe scan-centric MSR analyses.
Challenges of MapReducing MSR analyses
Although MapReduce holds great promise for MSR, we
envision a number of important challenges based on ourprevious experience of using MapReduce [35]. We use theMapReduce example above to motivate and explain thesechallenges. The goal of this paper is to document our ex-periences addressing these challenges across various types
of MSR analyses and to carefully examine the guidelinesproposed by the web Ô¨Åeld regarding these challenges. By
documenting the diÔ¨Äerences in analyses and data processed
by both communities, we hope that the software engineering
Ô¨Åeld will be able to exploit the full power of MapReduce toscale software engineering analyses.Challenge 1: Migrating MSR analyses to a divide-and-conquer programming model.
The Ô¨Årst challenge is to Ô¨Ånd out how to migrate an existing
MSR analysis to a divide-and-conquer programming model.
This migration has two important aspects.
1.Locality of analysis. A Divide-and-conquer pro-
gramming model works best when the processing of
each broken data part is independent of the processing
of the other parts (i.e., a local algorithm). Countingthe number of lines of code (#LOC) for every sourceÔ¨Åle is an example of a local algorithm as this can bedone for each Ô¨Åle in isolation and the results of eachdata part can just be added up. Global algorithms
(e.g., clone detection [34]) would require each data part
(e.g., set of Ô¨Åles) to have access to the whole data set.
Semi-local algorithms (e.g., source code diÔ¨Äerencing)require more data than just local data, but not thewhole data set (e.g., only two Ô¨Åles). It is interesting tonote that an analysis might be global due to the imple-
mentation of an analysis, not due to the analysis itself.
For example, several analyses require access to the fullcode base, when robust techniques such as island pars-ing [31] could be used to overcome this implementationrequirement and would ensure local analysis.
2.Availability of source code. Having access to the
source code of an MSR study tool provides more Ô¨Çex-
ible ways to map an MSR algorithm to a divide-and-conquer programming model. However, re-engineering
a tool internally increases the risk of introducing bugs.
Challenge 2: Locating a suitable cluster.
Distributed platforms typically run on a cluster of ma-
chines. We list below a few aspects for locating clusters:
1.Private cluster versus Public cluster. A public
cluster is available and accessible to everyone, whereas
a private cluster is not.
2.Dedicated cluster versus Shared cluster. Dedi-
cated clusters ensure that only one user uses the ma-
chines at the same time, while machines in the sharedcluster may be used by many users at the same time.
3.Specialized cluster versus General-purpose clus-
ter.Specialized clusters are designed and optimized
for MapReduce (e.g., [1]), while general-purpose clus-
ters might result in sub-optimal performance.
On the one hand, private, dedicated, specialized clusters
provide the most optimal performance. On the other hand,
public, shared, general-purpose clusters require the lowest
Ô¨Ånancial cost. There are eight possible combinations of
the three aforementioned aspects. To illustrate the possi-ble types of clusters, we show four types as examples.
¬ÅMachines in a research lab (Private, Dedicated
and Specialized). Research shows that computers
are idle half of the time [10]. By bundling these com-puters together, a small cluster can be created.
¬ÅMachines in a student lab (Private, Dedicatedand General). Computers in student labs of universi-
ties can be used as medium-sized MapReduce clusters.
277¬ÅScientiÔ¨Åc clusters (Public, Shared, and Gen-
eral). Some scientiÔ¨Åc clusters, e.g., SHARCNET [7],
have hundreds or thousands of machines and are specif-
ically designed for scientiÔ¨Åc computing. The large scaleof these clusters enables running experiments on mas-
sive amounts of data.
¬ÅOptimized clusters (Public, Dedicated and Spe-
cialized). Some clusters are optimized for MapRe-
duce, e.g., the EC2 MapReduce instances oÔ¨Äered by
Amazon [1]. Optimized clusters are often too costly.
Challenge 3: Optimizing MapReduce strategy de-sign and cluster conÔ¨Åguration.
The diÔ¨Äerent implementations and conÔ¨Ågurations of the
MapReduce platform inÔ¨Çuence the performance of MapRe-
duce experiments, yet Ô¨Ånding the optimal implementationand conÔ¨Åguration is challenging.
1.Static breakdown of analysis. The optimal gran-
ularity for breaking down the analysis should be care-
fully examined. For example, counting the #LOC of a
software project can be decomposed into diÔ¨Äerent dataparts that are executed in parallel to count the #LOCof: 1) every source code Ô¨Åle (Ô¨Åne-grained) or 2) everysubsystem (coarse-grained). The Ô¨Åner the granularity,the more parallelism that can be achieved. However,
Ô¨Åner granularity leads to m ore overhead since addi-
tional ‚ÄúMap‚Äù and ‚ÄúReduce‚Äù procedures must be sched-
uled and executed. Although this granularity principle
is well known in distributed computing, choosing thebest granularity in the context of the MapReduce plat-form is still challenging.
2.Dynamic breakdown of processing. Once the
static breakdown is determined, the granularity of pro-cessing the input data can still be altered dynamically.MapReduce implementations typically allow sending anumber of ‚ÄúMap‚Äù and ‚ÄúReduce‚Äù procedures to a ma-
chine at the same time as a ‚ÄúHadoop task‚Äù. In our
#LOC example, one single source code Ô¨Åle could besent to a machine for analysis, or an ad hoc group ofÔ¨Åles could be sent together in a batch. The composi-tion of Hadoop tasks can be completely arbitrary byt h eM a p R e d u c ep l a t f o r m .
3.Determining the optimal number of machines.A third way to optimize the performance of a MapRe-duce cluster is by changing the number of machines.Adding more machines might not always lead to betterperformance or eÔ¨Äective use of resources, due to plat-form overhead. For example, adding more machines
requires more data transfer over the network, extra
computing power, and possibly additional usage fees.
Challenge 4: Managing data during analysis.
MapReduce needs a data management strategy to store
and propagate large data fast enough to avoid being a bot-
tleneck. Two data storage choices are typically available:
1.Distributed Ô¨Åle system. Input data and intermedi-
ate data are stored in one distributed Ô¨Åle system that
spreads its data to every machine of the cluster to in-crease I/O bandwidth and the total amount of storage,and to achieve fault tolerance.
2.Local Ô¨Åle system. Saving data in the local Ô¨Åle sys-
tem does not require data replication and transfer on
the network.Table 1: Eight types of MSR Analyses.
Name
 Description
 Locality
Metadata anal-
ysis
Direct analysis on the ex-tracted metadata from soft-ware repositories, e.g., [13].
 local
Static sourcecode analysis
 Static program analysis onsource code, e.g., [18] .
 local/global/semi-local
Source codediÔ¨Äerencing
and analysis
Analysis of changes between
versions of source code,
e.g., [22].
semi-local
Software met-
rics
Measuring and analyzing met-rics of software repositories,e.g., [36].
 local/global/semi-local
Visualization
 Visualizing information mined
from software repositories,
e.g., [32].
global
Clone detec-
tion methods
Detecting and analyzing sim-ilar source code fragments,
e.g. [26].
global
Data Mining
 Applying Data mining tech-
niques on software repositories,
e.g., [29].
global
Social network
analysis
Social and behavioural anal-ysis on software repositories,
e.g., [12].
semi-local
Choosing the best data storage strategy for diÔ¨Äerent types
of analyses is very important and challenging.
Challenge 5: Recovering from errors.
During the experiments, the machines in the cluster might
crash and the MSR study tools used in the experiments
might fail or throw exceptions. The MapReduce platformneeds to catch failures and exceptions from both hardwareand software during large-scale experiments. Handling and
recovering errors is import ant when migrating MSR study
tools to a MapReduce cluster.
4. CASE STUDIES
This section brieÔ¨Çy present s the three case studies that
we used to study how to address the challenges of migrating
MSR tools to the MapReduce platform.
4.1 Subject systems and input data
We chose three representative MSR case studies and asso-
ciated tools to counter potential bias. Prior research identi-
Ô¨Åes eight major types of MSR analyses [25], as shown in Ta-ble 1. Techniques across these types require time-consuming
processing and must cope with growing input data. We se-
lect three MSR tools that cover six out of the eight types
of MSR analyses. Section 6 discusses the applicability ofMapReduce to the two types of MSR analyses that are notcovered by our case studies (i.e., visualization and social net-work analysis). We summarize below our case study tools.
J-REX. CVS repositories [3] contain the historical snap-
shots of every Ô¨Åle in a software project, with a log of everychange during the history of the software project. J-REX,similar to C-REX [22], processes CVS repositories to:
¬ÅExtract information (e.g., author name and changemessage) from each CVS transaction.
¬ÅTransform source code into an XML representation.
¬ÅAbstract source code changes from the line level (‚Äúline1 has changed‚Äù) to the program entity level (‚Äúfunctionf1 no longer calls function f2‚Äù) .
¬ÅCalculate software metrics, e.g., #LOC.
278Table 2: Overview of the three subject tools.
J-REX
 CC-Finder
 JACK
Programming
Language
Java
 Python
 Perl
Source code
 available
 not available
 available
Input data
 Eclipse,
Datatools
FreeBSD
 Log Ô¨Åles No.
1&2
Input datatype
 CVS repos-itory
 source code
 execution log
Table 3: Characteristics of the input data.
Data Size
 Data Type
 # Files
Eclipse
 10.4GB
 CVS repository
 189,156
Datatools
 227MB
 CVS repository
 10,629
FreeBSD
 5.1GB
 source code
 317,740
Log Ô¨Åles No.1
 9.9GB
 execution log
 54
Log Ô¨Åles No.2
 2.1GB
 execution log
 54
J-REX performs 4 types of MSR analyses, i.e., Metadata
analysis, Static source code analysis, Source code diÔ¨Äerenc-
ing and Software metrics [25].
CC-Finder. CC-Finder is a token-based clone detection
tool [26] designed to extract code clones from systems de-
veloped in several programming languages (e.g., C++, andC). CC-Finder belongs to the clone detection analysis type.
JACK. JACK is a log analyzer that uses data mining
techniques to process system execution logs, and automat-ically identify problems in load tests [24]. JACK performsthe Metadata analysis and Data Mining MSR studies.
Source code of J-REX and JACK was available to us.
4.2 Experimental environment
To perform our evaluation, we require input data, a cluster
of machines and a MapReduce implementation.
We use the CVS repository archives of Eclipse, a widely
used Java IDE, and Datatools, a data management platform,
as J-REX‚Äôs input data. We downloaded the latest version of
these archives on September 15, 2009. FreeBSD is an opensource operating system. We use the source code distribu-tion of FreeBSD version 7.1 as the input data for CC-Finder.Finally, two groups of execution log Ô¨Åles are used as inputdata of JACK [24]. Tables 2 and 3 give an overview of the
three software engineering tools and their input data.
Our experiments are performed on two clusters: 18 ma-
chines of a student lab and 10 machines of a scientiÔ¨Åc cluster
called SHARCNET [7]. Table 4 shows the conÔ¨Åguration of
the two clusters. From previou s research [35], we also have
experience using a cluster in a research lab.
We choose Hadoop [39] as our MapReduce implementa-
tion. Hadoop is an open-source implementation of MapRe-
duce supported by Yahoo! and widely used in industry.Hadoop not only implements the MapReduce model, butalso provides a distributed Ô¨Åle system, called the Hadoop
Distributed File System (HDFS). Hadoop supplies Java in-
terfaces to implement MapReduce operations and to controlthe HDFS. Another advantage for users is that Hadoop bydefault comes with libraries of basic and widely used ‚ÄúMap‚Äùand ‚ÄúReduce‚Äù implementations, for example to break downÔ¨Åles into lines. With these libraries, users occasionally do
not have to write new code to use MapReduce.
4.3 Performance
To illustrate the scalability improvements of MapReduce
for MSR analyses, we brieÔ¨Çy discuss the performance ob-
tained using MapReduce in our experiments, compared toTable 4: ConÔ¨Åguration of MapReduce clusters.
Student Lab
 SHARCNET
# Machines
 18
 10
CPU
 Intel Q6600 (2.4GHz)
 8√óXeon(3.0GHz)
Memory
 3GB
 8GB
Network
 Gigabit
 Gigabit
OS
 Ubuntu 8 .04
 CentOS 5 .2
Disk size
 10GB
 64GB
Table 5: Best results for the migrated MSR tools.
Tool name
 Input
data
One ma-
chine
MapReduceversion
 Cluster
J-REX
 Eclipse
 755min
 80min
 SHARCNET
CC-Finder
 FreeBSD
 ‚àí
 59hours
 student lab
JACK
 Log Ô¨Åle
No.1
580min
 98min
 SHARCNET
the performance on a single machine without MapReduce.
We repeated each experiment three times, and always re-port the median value of our results. A more detailed anal-
ysis of the performance gains of MapReduce for J-REX can
be found in previous work [35]. Table 5 shows the bestperformance for each tool with the Eclipse CVS repository,FreeBSD source code and the 10GB system execution log
Ô¨Åles as input data respectively. For CC-Finder, we cannotperform code clone detection in the FreeBSD source code on
one machine because of memory limitations. From the table,
we can see that on a cluster of 10 machines (SHARCNET),the running time of J-REX and JACK is reduced by a fac-tor 9 and 6 respectively. For CC-Finder, the running time isonly 59 hours. Livieri et al.[28] claim that using CC-Finder
to detect code clones in the FreeBSD source code requires 40
days. Although the experiments are performed on diÔ¨Äerent
hardware environments, the huge diÔ¨Äerence of running timegives an idea of the scalability of MapReduce.
5. MIGRATION EXPERIENCES
While the previous section conÔ¨Årms that MapReduce can
eÔ¨Äectively scale several types of MSR analyses, it took us
several attempts and experiments to achieve such perfor-
mance results. In this section we distill our experience such
that others would beneÔ¨Åt from them. For each challengefrom Section 3, we discuss our Ô¨Åndings and provide advicebased on our experience. We also compare our Ô¨Åndings rel-
ative to common guidelines provided by the web Ô¨Åeld.Challenge 1: Migrating MSR tools to a divide-and-
conquer programming model.
We used the following strategies to map the MSR tools to
a divide-and-conquer programming model.
J-REX. Similar to the original J-REX, the history of every
single Ô¨Åle is processed in isolation. Every input key/value
pair contains the raw data of one Ô¨Åle in the CVS repository.
The Mappers pass the key/value pairs as ‚ÄúÔ¨Åle name/version
number of the Ô¨Åle‚Äù to Reducers. Reducers perform compu-
tations to analyze the evolutionary information of all therevisions of a particular Ô¨Åle. For example, if Ô¨Åle ‚Äúa.java‚Äù hasthree revisions, the mapping phase gets Ô¨Åle names and revi-
sion numbers as input, and generates every revision number
of the Ô¨Åle, such as ‚Äúa.java/a
0.java‚Äù . The Reducer gener-
ates ‚Äúa.java/evolutionary information of a.java‚Äù .T h e f u l l
implementation details are discussed in [35].
CC-Finder. Our MapReduce implementation adopts the
same computation model as D-CCFinder [28], which consists
of the following steps:
279	



	






	





	





	


	









 
 

Figure 1: Example of the typical computational
model of clone detection techniques.
1. Dividing source code into a number Nof Ô¨Åle groups.
2. Combining every two Ô¨Åle groups together, resulting
into N√ó(N+1 )/2‚ÄúÔ¨Åle group pair id/Ô¨Åle names in
both Ô¨Åle groups‚Äù pairs, which are sent to Mappers.
3. Mappers send the pair to a Reducer.
4. The Reducer invokes CC-Finder on a particular pair
to run the clone analysis.
Figure 1 shows an example of the computational model
for detecting code clones in 3 Ô¨Åles. From the Ô¨Ågure, we can
see that every Ô¨Åle needs to be compared to every other Ô¨Ålea n dt oi t s e l f ,r e s u l t i n gi n t o6p a i r s .
JACK. JACK detects system problems by analyzing log
Ô¨Åles. The Mapper receives every Ô¨Åle name as input key/-
value pair, and passes ‚ÄúÔ¨Åle name, Ô¨Åle name‚Äù to the Reducer.
Passing only the Ô¨Åle name instead of the Ô¨Åle content avoidsI/O overhead. Reducers receive the Ô¨Åle name and invokeJACK to analyze the Ô¨Åle.
In the global analyses, we have to put required data into
Reducer. For local analyses, such as evolution of SLOC and
JACK, we can use both Mapper and Reducer. Semi-localanalyses can follow the migration strategy of either globalor local analyses. Since the outputs of the diÔ¨Äerent JACKinvocations do not need to be aggregated, we only need one
MapReduce phase instead of both Mappers and Reducers.
We put all JACK functionality in Reducers, but could just
as well have put it in the Mappers. Similar migration strate-gies are found in examples of MapReduce strategies such as
‚ÄúDistributed Grep‚Äù [15].
Notable Findings.
We summarize below our main observations.
1.Locality of analysis. A majority of MapReduce uses
in the web Ô¨Åeld are local in nature, while for our case
study we Ô¨Ånd that our three tools cover three levels of
locality. The JACK tool performs local analysis, be-
cause the analysis of a Ô¨Åle does not depend on otherÔ¨Åles. CC-Finder performs global analysis because ev-ery source code Ô¨Åle must be compared to all the inputsource code Ô¨Åles. J-REX performs semi-local analy-sis, because it compares consecutive revisions of ev-
ery source code Ô¨Åle. In another perspective, both J-
REX and JACK have the algorithmic complexity of n,
which is the number of input Ô¨Åles; while CC-Finder hasthe algorithmic complexity of n
2.Y e t , a l l t o o l s s h o w
good performance after being MapReduced. For CC-
Finder, we adopted the computation model proposed
by [28] using the services provided by the MapReduceplatform instead of spending considerable time imple-
menting the platform for such a computation model.
For J-REX, we found that for each analysis we neededa subset of the data (i.e., all consecutive revisions ofa particular Ô¨Åle), hence we had to ensure that all thedata is mapped to the same machine in the cluster.
2.Availability of source code. When no source code
was available, we used a program wrapper, which cre-ates a process to call executable programs. Whenthe source code was available, we sometimes had touse a program wrapper to invoke the tool because thetool and the MapReduce implementation used diÔ¨Äer-ent programming languages (e.g., JACK is written in
Perl while developers need to use Java on Hadoop).
When the tool‚Äôs source code was available and writtenin Java, e.g., for J-REX, the source code of the toolwas modiÔ¨Åed to migrate to MapReduce.
Migrating local and semi-local analyses is much simpler
than migrating global analyses. Little design eÔ¨Äort is re-quired for migrating J-REX and JACK. CC-Finder, as aglobal analysis, required more design eÔ¨Äort than the othertools. We implemented 300 to 500 lines of Java code to
migrate each tool.
Challenge 2: Locating a suitable cluster
In previous research [35], we used a four-machine MapRe-
duce cluster in our research lab. In this paper, we used a
cluster in a student lab and a cluster in SHARCNET. We
document below our experiences using these three types of
clusters.Research lab. The heterogeneous nature of research labs
complicates the deployment of MapReduce implementations
such as Hadoop. These implementations require commonconÔ¨Åguration choices on every machine, such as a common
user name and installation location. In an eÔ¨Äort to reduce
the complexity of deployments in research labs, we exploredthe use of virtual machines instead of the actual machines.The virtual machines unify the operating system, user nameand installation location. However, virtual machines intro-duce additional overhead especially for I/O intensive anal-
ysis, while for CPU intensive analysis the overhead turned
out to be minimal.Student lab. The limited and unstable nature of stor-
age in the student lab limited the use of Hadoop. All toooften student labs provide too limited disk space for analy-sis and machines are typically conÔ¨Ågured to erase all space
when booting up. The limited storage space prevented us
from running experiments that performed global or semi-local analysis.SHARCNET. While SHARCNET (and other scientiÔ¨Åc com-
puting clusters) provide the desired disk space and homo-
geneous conÔ¨Åguration, we were not able to use the main
clusters of SHARCNET. Most scientiÔ¨Åc clusters make useof specialized schedulers to ensure fair sharing of the clus-ter, which do not support Hadoop. Fortunately, the SHAR-CNET operators gave us special access to a small testingcluster without scheduling requirements.
Notable Findings.
Heterogeneous infrastructures are not frequently used in
the web Ô¨Åeld. Hence, the support provided by MapReduce
implementations, like Hadoop, for such infrastructures islimited. In the research community, heterogeneous infras-tructures are the norm rather than the exception. We hope
that future versions of Hadoop will provide better support.
280For now, we have explored the use of virtual machines
on heterogenous infrastructures to provide a homogeneous
cluster. The virtual machine solution works well for non-
I/O intensive analysis and as a playground for analysis and
debugging before deployment on larger clusters. We have
used such a virtual playground to verify our MapReduce mi-
gration before deploying on expensive commercial Hadoop
clusters, such as the Amazon EC2 Hadoop images [1].
While scientiÔ¨Åc clusters provide an ideal homogeneous in-
frastructure, their schedulers have yet to adapt to MapRe-
duce‚Äôs model. Researchers should work closer with the ad-ministration teams of scientiÔ¨Åc clusters such that MapReduce-friendly schedulers are adopted by these clusters.
Challenge 3: Optimization of MapReduce
We now discuss our observations regarding the optimiza-
tion of MapReduce processing.
1) Static breakdown of analysis.
We explored the use of Ô¨Åne-grained (most often used in
the web Ô¨Åeld) and coarse-grained breakdown in our migra-
tion of the diÔ¨Äerent tools. For example, for the CC-Finder
tool we started to read Ô¨Åles from the input source code repos-itory and record the size of every Ô¨Åle until the total Ô¨Åle sizereached a certain threshold. The Ô¨Åne-grained breakdownprocessed 200MB of Ô¨Åles per part while the coarse-grainedbreakdown processed 1GB of data per part (the CC-Finder
version we had did not support more than 1GB of data). For
J-REX, we explored the use of single Ô¨Åles and sub-folders forbreakdown granularity. In these experiments, we found thatcoarse-grained breakdown is two to three times faster thanÔ¨Åne-grained breakdown because the processing time of each
Ô¨Åne-grained unit has a large portion wasted on communica-
tion overhead. This Ô¨Ånding conÔ¨Årms common knowledge indistributed computing.
2) Dynamic breakdown of processing.
We studied the impact of the dynamic breakdown of pro-
cessing on performance by varying the number of process-
ing tasks in Hadoop. We experimented with J-REX using
the Datatools CVS repository and JACK using the Log Ô¨ÅlesNo.2, on 10 machines in SHARCNET. We set the number ofHadoop tasks to 10 (the number of machines) and recordedthe running time of every machine in the cluster. In theviolin plots of Figure 2, the top value corresponds to the
maximum machine running time across all the machines,
which determines the running time of the whole MapRe-duce process. The taller the grey box in the violin plot, theless balanced the workload of machines.
We then increased the number of Hadoop tasks to 100 for
J-REX and 54 (the number of Ô¨Åles, see Table 3) for JACK
and compared the Ô¨Åndings for the increased Hadoop task
count to the performance of J-REX and JACK with just 10Hadoop tasks. The plots in Figure 2 show that the runningtime of every machine after increasing the number of Hadooptasks is more balanced than before (lower grey boxes in the
violin plot). However, running JACK with more Hadoop
tasks is faster than with fewer Hadoop tasks, while runningJ-REX with more Hadoop tasks is slower.
This contradictory result is caused by the diÔ¨Äerent types
of input data in the two software systems. The input data
of J-REX is a CVS repository [3]. CVS repositories store
the history of each Ô¨Åle in a separate Ô¨Åle, leading to a large
number of input Ô¨Åles. As shown in Table 2, JACK only hasa few dozen Ô¨Åles as input. The granularity of input Ô¨Åles isÔ¨Åner for J-REX than for JACK. Increasing the number of
     
 	

  	

	
	 
      
 	

  	

	
	

Figure 2: Violin plots of machine running-time for
JACK and J-REX.
Hadoop tasks, yields a more balanced workload for both J-
REX and JACK. However, this also increases the overhead
of the platform to control and monitor Hadoop tasks. As aresult, the best number of Hadoop tasks for J-REX seems
to be the number of machines, i.e., coarsest granularity. For
JACK, the best number of Hadoop tasks seems to be thenumber of input Ô¨Åles, i.e., the Ô¨Ånest granularity.
3) Determining the optimal number of machines.To determine the optimal number of machines in our case
study, we varied the number of machines from 5 to 10 on J-
REX for the Datatools CVS repository and on JACK for the
No.1 Log Ô¨Åles. The number of Hadoop tasks are 10 and 54(optimal dynamic breakdown) for J-REX and JACK respec-tively. Figure 3 shows the corresponding running times. Wenotice that the performance of J-REX grows sub-linearly,while the performance of JACK plateaus. Closer analysis
indicates that this is primarily due to two reasons:
1.Platform overhead. The platform overhead is the
time that the MapReduce platform uses to control
Hadoop tasks, while the analysis time is the actual ex-
ecution time of Mappers and Reducers. In our exper-
iments, we Ô¨Ånd that the platform overhead is around13% of the total running time with 5 machines and 23%of the total running time with 10 machines. Addingmachines into the cluster introduces additional over-
head. However, as the platform overhead is dominated
by the analysis time when doing large-scale analysis,
MapReduce performs better with larger scale analyses.
2.Unbalanced workload. An unbalanced workload
causes machines to be idle. For example, a machine
that is assigned much heavier work than others in-creases the total running time, as the whole MapRe-
duce run will have to wait for that machine. In our
experiments, unbalanced workload is the main reasonfor the un-optimal of JACK. In Figure 3, JACK doesnot improve its performance when moving from 6 ma-chines to 10 machines. We checked the system logsof the MapReduce platform and found that one of the
Hadoop tasks with the largest input log Ô¨Åle took much
longer than the other Hadoop tasks, which had to waitfor that one Hadoop task to Ô¨Ånish.
As a distributed platform, MapReduce requires transfer-
ring data over the network. Accessing a large amount of dataalso requires a large amount of I/O. Intuitively, I/O mightbe another possible source of the overhead. We observed theoutput of vmstat on every machine in the cluster and found
that the percentage of CPU time spent on I/O is less than1% on average, which means that in our experiment I/O was
not a bottleneck.
281

     	
  
	
	

Figure 3: Running time trends of J-REX and JACK
w i t h5t o1 0m a c h i n e s .
Notable Findings.
The web Ô¨Åeld often uses MapReduce to perform local anal-
ysis, with each broken-down part requiring substantial pro-
cessing. In contrast, based on our case studies we note thatmany software engineering tasks (e.g., parsing a single Ô¨Åle)require analyses that vary in locality. On the one hand,
we would suggest researchers to analyze Ô¨Åles in groups in-
stead of individually in order to reduce platform overhead.However, the grouping of Ô¨Åles might cause imbalance in therunning time of Hadoop tasks, with some parts requiringmore processing time than others. This in turn reduces the
parallelism of the platform. In short, we can conclude that
large-scale analysis on balanced input data beneÔ¨Åts more
from more machines in the cluster than small-scale analysiswith unbalanced input data.
Our studies indicate that the recommended parameter
conÔ¨Ågurations for using Hadoop on web data do not work
well for all types MSR studies. For web data, it is recom-
mended that the number of ‚ÄúMap‚Äù procedures is set to avalue in between 10 to 100 √óm, and that the number of
‚ÄúReduce‚Äù procedures is set to 0.95 or 1.75 √óm√ón,w i t h
nbeing the number of machines and mbeing the number
of processes that can run simultaneously on one machine,
which is typically the number of cores of the machines [39].
This recommendation works well for web analysis, which is
typically Ô¨Åne-grained. Fine-grained MSR tools like J-REX,
which have a large number of input key/value pairs, can still
adopt these recommendations. Coarse-grained MSR toolslike JACK, which have a small number of input key/value
pairs, should not adopt these recommendations. Instead,
such tools should set the number of ‚ÄúReduce‚Äù procedures tobe the same as the number of input key/value pairs, i.e., thenumber of input Ô¨Åles.Challenge 4: Managing data during analysis
We used both distributed and local Ô¨Åle systems.
1.Distributed Ô¨Åle system. Hadoop oÔ¨Äers a distributed
Ô¨Åle system (HDFS) to exchange data between diÔ¨Äerentmachines of a cluster. Such Ô¨Åle systems are optimizedfor reading and perform poorly for writing data [39].With many MSR tools generating a large number of
intermediate Ô¨Åles, the overhead of using HDFS is sub-
stantial. For example, if J-REX were to use HDFSwhen analyzing Eclipse, J-REX would require almost190,000 writes to HDFS (a major slowdown). There-fore, we avoided the use of HDFS whenever possible,opting instead for the local Ô¨Åle system. In the spe-
cial case where no source code is available for an MSR
tool, it might not even be possible to use HDFS, asaccessing HDFS data requires using special APIs.
2.Local Ô¨Åle system. In our experiments, we Ô¨Ånd that
using every machine‚Äôs local Ô¨Åle system provides themost optimal solution of storing intermediate and out-
put data. For example, CC-Finder and the log ana-lyzer both output results to Ô¨Åles, which we store in
local Ô¨Åle system. Since the output Ô¨Åles are spread on
diÔ¨Äerent machines, we have to retrieve the results afterthe MapReduce run is completed. However, we have totake the risk of losing output data and re-performing
the analysis when a machine crashes.
Notable Findings.
HDFS is the default data storage of Hadoop for the web
analyses, but was not designed for fast data writing, which
is necessary in saving MSR analyses result data. From our
experience, we recommend: 1) the use of the local Ô¨Åle systemif the result data consists of a large amount of Ô¨Åles; and 2)
the use of HDFS if the result data is small in size.
Challenge 5: Error recovery
Our experiments evaluated the error recovery of Hadoop.
1.Environment failure. To examine the error recovery
of Hadoop, we performed an experiment with J-REX
and the Datatools CVS repository on 10 machines.First, we killed MapReduce processes and restartedthem after 1 minute. We gradually increased the num-ber of killed processes starting from 1 until the wholeMapReduce job failed. Second, we did the same thing
as the Ô¨Årst step, but without restarting the processes.
Our experimental results show that MapReduce jobsprocess well with up to 4 out of 10 machines killed.However, the running time increases from 12 min to 22min. If we restore the working processes, the Hadoopjob can Ô¨Ånish successfully with up to half of the ma-
chines down at the same time.
2.Tool error. The strategy of addressing MSR tool
errors depends on the implementation of the ‚ÄúMap‚Äù
and ‚ÄúReduce‚Äù procedures. If the MapReduce platformcatches an exception, the platform will automaticallyre-start the Mapper or Reducer. According to our ex-
perience, if a program wrapper is used in the MapRe-
duce algorithm, the wrapper needs to take the outputof the MSR study tool, determine the running status,and throw an exception to the MapReduce platformto exploit MapReduce‚Äôs tool error recovery. Alterna-
tively, the wrapper can restart the analysis without
throwing the exception to the MapReduce platform.In both cases, tool error can be caught and recovered.
Notable Findings.
We found that Hadoop‚Äôs error recovery mechanism en-
abled us to have agile clusters with machines joining and
leaving the cluster based on need. In particular, in our re-search lab students can join and leave a cluster based ontheir location and their current needs for the machine.
Because of Hadoop, an MSR tool might be executed mil-
lions of times. Hence, better reporting is needed by MSR
tools such that any failure can be spotted easily within the
millions of executions. We are currently exploring the use oftechniques to detect anomalies in load tests (e.g., [24]) fordetecting possible failures of the execution of an MSR tool.
6. APPLICABILITY
This section discusses the applicability of MapReduce to
all the eight types of MSR analyses presented in Section 4.
For each type, we present possible migration strategies. Thesestrategies basically all depend on whether or not an analysisis local. We summarize in Table 6 the main challenges of
migration, ease of migration and existence of prior research
about scaling the analysis.
282Table 6: Applicability of performing MSR analysis
using the MapReduce platform.
Name
 Main
Challenge
Ease of
migrating
Prior re-search
Metadata analysis
 Challenge 3
 easy
 no
Static source codeanalysis
 Challenge 1&3
 easy ormedium
 no
Source code diÔ¨Äerenc-ing and analysis
 Challenge 3
 easy
 no
Software metrics
 Challenge 3
 easy or
medium
no
Visualization
 Challenge 1
 hard
 no
Clone-detection meth-
ods
Challenge 1
 hard
 yes, [28]
Data Mining
 Challenge 1
 hard
 yes, [5]
Social network analysis
 Challenge 1
 medium
 yes, [9]
Metadata analysis. In metadata analysis, data can just
be broken down by the type of the metadata. For example,
bug repository analysis can be broken down to analyzing
individual bug reports.
Static source code analysis. Local static analyses can
be migrated by breaking down the source code into several
local parts and using a program wrapper to invoke the ex-
isting tools. If the static analysis process is non-local, the
process of every source code Ô¨Åle will consist of two steps: 1)collecting the required data in the other source code Ô¨Åles; 2)performing analysis on the Ô¨Åle and its collected data.
Source code diÔ¨Äerencing and analysis. The process
can be broken down by Ô¨Åles or by consecutive revisions. J-
REX performs source code diÔ¨Äerencing.
Software metrics. The MapReduce strategies can be
designed based on the types of software metrics. Our exam-
ple of studying the evolution of #LOC of a software projectin Section 3 is an example of a software metric.
Visualization. The visualization techniques that we con-
sider consist of a regular MSR technique, followed by thegeneration of a visualization.
Clone-detection methods. Clone-detection techniques
are non-local. This is the reason why they are hard tomigrate to MapReduce. Livieri et al .[28] proposes an ap-
proach to map clone-detection to divide-and-conquer, which
we adopted in our case study as MapReduce strategy.
Data Mining. Many Data Mining techniques require the
entire data to build a model or to retrieve information, whichmakes Data Mining techniques hard to migrate to MapRe-duce. However, research has been performed to address the
challenges of Data Mining algorithms to MapReduce. As
such, some open source libraries are available for runningData Mining algorithms on Hadoop [5].
Social network analysis. Social networks can be ana-
lyzed as a graph with nodes and edges. Some of the analysesof the entire graph can be broken down to analyses of indi-
vidual nodes or edges. X-RIME [9] is a Hadoop library for
social network analysis.
B a s e do nt h ee x a m i n a t i o no ft h e8t y p e so fM S Ra n a l y s e s ,
most analyses are able to migrate to MapReduce, despitesome challenges. Moreover, previous research (e.g., [5,9,28])has addressed migrating some of the challenging analyses.
7. THREATS TO VALIDITY
We discuss the threats to validity for our Ô¨Åndings.
Generalizability. W ec h o s et os c a l et h r e eM S Rt o o l s .A l -
though we chose tools across diÔ¨Äerent types of MSR stud-ies and using diÔ¨Äerent subject systems to avoid potential
bias of our studies to any special MSR study, our results
may not generalize to other MSR studies. However, ourcase studies provide promising Ô¨Åndings and we encourageother researchers to explore MapReducing their tools. Sec-tion 6 provides a brief discussion of generalization acrossother MSR studies.
Shared hardware environment. The scientiÔ¨Åc comput-
ing environment we used is a shared cluster. The usage of
other users on the cluster may have impacted our case studyresults, which would threaten our Ô¨Åndings. To counter thisthreat, we tried to use the cluster when it was idle, we re-peated each experiment three times, and we reported the
median value of the results.
Subjectivity bias. Some Ô¨Åndings in our research can in-
clude subjectivity bias. For example, one of the MSR tools
in our experiment was developed by the author of this pa-per, while the other two are not. Using our own tools forexperimentation may cause subjectivity bias. However, in
practice one will typically only alter the source code of tools
that they know well. More case studies on other MSR toolsare needed to verify our Ô¨Åndings.
8. CONCLUSION
Automated software engineering tools continue to play
an important role in the analysis of large data sets using
sophisticated algorithms. In an eÔ¨Äort to scale such tools,
developers often opt for ad hoc, one-oÔ¨Ä solutions that are
costly to develop and maintain. In this paper, we demon-strate that standard large-scale data processing platforms,like MapReduce, could be used to eÔ¨Äectively and eÔ¨Éciently
scale MSR tools, despite several challenges. We documentour experiences such that others would beneÔ¨Åt from them.
We Ô¨Ånd that while MapReduce provides an eÔ¨Écient plat-
form, we must follow diÔ¨Äerent guidelines when conÔ¨ÅguringMapReduce runs instead of following the standard web Ô¨Åeldguidelines. In particular, software engineering analyses areoften not local and software engineering analyses require dif-ferent conÔ¨Åguration than to web analyses to achieve optimal
performance. We hope that our experiences will help others
exploring the use of large-scale data analysis platforms toscale automated software engineering tools, instead of de-veloping their own solutions.
9. REFERENCES
[1] Amazon EC2. https://aws.amazon.com/ec2/.
[2] checkmycode. http://www.checkmycode.org/.[3] CVS. http://www.cvshome.org/.[4] Debian counting. http://libresoft.es/debian-counting/.
[5] MAHOUT. http://lucene.apache.org/mahout/.
[6] Self-service, prorated super computing fun!
http://open.blogs.nytimes.com/2007/11/01/.
[7] SHARCNET. https://www.sharcnet.ca.
[8] Vertica home page. http://www.vertica.com.[9] X-RIME home page. http://xrime.sourceforge.net/.
[10] A. Acharya, G. Edjlali, and J. Saltz. The utility of
exploiting idle workstations for parallel computation.SIGMETRICS Perform. Eval. Rev. , 25(1):225‚Äì234,
1997.
[11] S. Bajracharya, J. Ossher, and C. Lopes. Sourcerer:
An internet-scale software repository. In SUITE ‚Äô09:
Proceedings of the 2009 ICSE Workshop on
283Search-Driven Development-Users, Infrastructure,
Tools and Evaluation , pages 1‚Äì4, Washington, DC,
USA, 2009. IEEE Computer Society.
[12] O. Baysal and A. J. Malton. Correlating social
interactions to release history during software
evolution. In MSR ‚Äô07: Proceedings of the Fourth
International Workshop on Mining Software
Repositories , page 7, Washington, DC, USA, 2007.
IEEE Computer Society.
[13] N. Bettenburg, R. Premraj, T. Zimmermann, and
S. Kim. Extracting structural information from bug
reports. In MSR ‚Äô08: Proceedings of the 2008
international working conference on Mining software
repositories , pages 27‚Äì30, New York, NY, USA, 2008.
ACM.
[14] J. Bevan, E. J. Whitehead, Jr., S. Kim, and
M. Godfrey. Facilitating software evolution research
with kenyon. In ESEC/FSE ‚Äô05: Proceedings of the
10th European Software Engineering Conference , 2005.
[15] J. Dean and S. Ghemawat. Mapreduce: simpliÔ¨Åed data
processing on large clusters. Commun. ACM , 51, 2008.
[16] P. Godefroid, M. Y. Levin, and D. Molnar. Automated
whitebox fuzz testing. Technical report,
MS-TR-2007-58, Microsoft, May 2007.
[17] J. M. Gonzalez-Barahona, G. Robles, M. Michlmayr,
J. J. Amor, and D. M. German. Macro-level softwareevolution: a case study of a large software compilation.Empirical Softw. Engg. , 14(3):262‚Äì285, 2009.
[18] C. G ¬®org and P. Wei√ügerber. Error detection by
refactoring reconstruction. In MSR ‚Äô05: Proceedings of
the 2005 international workshop on Mining software
repositories , pages 1‚Äì5, New York, NY, USA, 2005.
ACM.
[19] W. Gropp, E. Lusk, and A. Skjellum. Using MPI:
portable parallel programming with the message
passing interface . MIT Press, 1999.
[20] M. Harman. The current state and future of search
based software engineering. In FOSE ‚Äô07: 2007 Future
of Software Engineering , pages 342‚Äì357, Washington,
DC, USA, 2007. IEEE Computer Society.
[21] M. Harman and B. F. Jones. Search-based software
engineering. Information and Software Technology ,
43(14):833 ‚Äì 839, 2001.
[22] A. E. Hassan. Mining software repositories to assist
developers and support managers .P h Dt h e s i s ,
University of Waterloo, 2005.
[23] A. E. Hassan. The road ahead for mining software
repositories. In FoSM: Frontiers of Software
Maintenance , pages 48‚Äì57, October 2008.
[24] Z. M. Jiang, A. E. Hassan, G. Hamann, and P. Flora.
Automatic identiÔ¨Åcation of load testing problems. In
ICSM ‚Äô08: Proceedings of 24th IEEE InternationalConference on Software Maintenance , pages 307‚Äì316,
Beijing, China, 2008. IEEE.
[25] H. Kagdi, M. L. Collard, and J. I. Maletic. A survey
and taxonomy of approaches for mining softwarerepositories in the context of software evolution. J.
Softw. Maint. Evol. , 19(2):77‚Äì131, 2007.
[26] T. Kamiya, S. Kusumoto, and K. Inoue. CcÔ¨Ånder: A
multilinguistic token-based code clone detection
system for large scale source code. IEEE Transactions
on Software Engineering , 28(7):654‚Äì670, 2002.
[27] C. Kirsopp, M. J. Shepperd, and J. Hart. Searchheuristics, case-based reasoning and software project
eÔ¨Äort prediction. In GECCO ‚Äô02: Proceedings of the
Genetic and Evolutionary Computation Conference ,
pages 1367‚Äì1374, San Francisco, CA, USA, 2002.
Morgan Kaufmann Publishers Inc.
[28] S. Livieri, Y. Higo, M. Matushita, and K. Inoue.
Very-Large Scale Code Clone Analysis and
Visualization of Open Source Programs Using
Distributed CCFinder: D-CCFinder. In ICSE ‚Äô07:
Proceedings of the 29th International conference onSoftware Engineering , 2007.
[29] W. Maalej and H.-J. Happel. From work to word:
How do software developers describe their work? In
MSR ‚Äô09: Proceedings of the 2009 6th IEEE
International Working Conference on Mining Software
Repositories , pages 121‚Äì130, Washington, DC, USA,
2009. IEEE Computer Society.
[30] A. Mockus. Amassing and indexing a large sample of
version control systems: Towards the census of public
source code history. In MSR ‚Äô09: Proceedings of 6th
IEEE International Working Conference on Mining
Software Repositories , pages 11‚Äì20, 2009.
[31] L. Moonen. Generating robust parsers using island
grammars. In WCRE ‚Äô01: Proceedings of the Eighth
Working Conference on Reverse Engineering
(WCRE‚Äô01) , page 13, Washington, DC, USA, 2001.
IEEE Computer Society.
[32] A. P. Nikora and J. C. Munson. Understanding the
nature of software evolution. In ICSM ‚Äô03:
Proceedings of the International Conference on
Software Maintenance , page 83, Washington, DC,
USA, 2003. IEEE Computer Society.
[33] C. Olston, B. Reed, U. Srivastava, R. Kumar, and
A. Tomkins. Pig latin: a not-so-foreign language for
data processing. In SIGMOD ‚Äô08: Proceedings of the
2008 ACM SIGMOD international conference on
Management of data , pages 1099‚Äì1110, New York,
NY, USA, 2008. ACM.
[34] C. K. Roy, J. R. Cordy, and R. Koschke. Comparison
and evaluation of code clone detection techniques and
tools: A qualitative approach. Sci. Comput. Program. ,
74(7):470‚Äì495, 2009.
[35] W. Shang, Z. M. Jiang, B. Adams, and A. E. Hassan.
Mapreduce as a general framework to support researchin mining software re positories (MSR). In MSR ‚Äô09:
Proceedings of 6th IEEE International WorkingConference on Mining Software Repositories , pages
21‚Äì30, 2009.
[36] Y. Shin, R. Bell, T. Ostrand, and E. Weyuker. Does
calling structure information improve the accuracy of
fault prediction? In MSR ‚Äô09: Proceedings of the 2009
6th IEEE International Working Conference onMining Software Repositories , pages 61‚Äì70,
Washington, DC, USA, 2009. IEEE Computer Society.
[37] M. Stonebraker, D. Abadi, D. J. DeWitt, S. Madden,
E. Paulson, A. Pavlo, and A. Rasin. Mapreduce and
parallel dbmss: friends or foes? Commun. ACM ,
53(1):64‚Äì71, 2010.
[38] W. Visser, C. S. PÀá asÀáareanu, and S. Khurshid. Test
input generation with java pathÔ¨Ånder. SIGSOFT
Softw. Eng. Notes , 29(4):97‚Äì107, 2004.
[39] T. White. Hadoop: The DeÔ¨Ånitive Guide . Oreilly &
Associates Inc, 2009.
284