Testing Properties of DataÔ¨Çow Program Operators
Zhihong Xu
University of Nebraska
Lincoln, NE
zxu@cse.unl.eduMartin Hirzel
IBM Watson Research
Yorktown Heights, NY
hirzel@us.ibm.comGregg Rothermel
University of Nebraska
Lincoln, NE
grother@cse.unl.eduKun-Lung Wu
IBM Watson Research
Yorktown Heights, NY
klwu@us.ibm.com
Abstract ‚ÄîDataÔ¨Çow programming languages, which represent
programs as graphs of data streams and operators, are becoming
increasingly popular and being used to create a wide array of
commercial software applications. The dependability of programs
written in these languages, as well as the systems used to
compile and run these programs, hinges on the correctness of
the semantic properties associated with operators. Unfortunately,
these properties are often poorly deÔ¨Åned, and frequently are not
checked, and this can lead to a wide range of problems in the
programs that use the operators. In this paper we present an
approach for improving the dependability of dataÔ¨Çow programs
by checking operators for necessary properties. Our approach is
dynamic, and involves generating tests whose results are checked
to determine whether speciÔ¨Åc properties hold or not. We present
empirical data that shows that our approach is both effective
and efÔ¨Åcient at assessing the status of properties.
I. I NTRODUCTION
DataÔ¨Çow programming languages represent programs as
graphs of data streams and operators. This representation
facilitates parallelization, since each operator can run indepen-
dently, subject only to availability of data on its input streams.
Furthermore, these languages feel natural to programmers who
wish to process large amounts of data. DataÔ¨Çow languages
have been the subject of much research, as chronicled by
several survey papers [13], [14], [23]. Moreover, thanks to
the advantages of parallelism and a data-centric paradigm,
dataÔ¨Çow languages have received a lot of commercial attention
for their ability to analyze ‚Äúbig data‚Äù.
There are many examples of dataÔ¨Çow programming lan-
guages and systems for executing dataÔ¨Çow programs. Pig
Latin [17], FlumeJava [2], and other languages compile to
MapReduce [5], a system that parallelizes programs over
hundreds of machines and has been used in building Google‚Äôs
search index. While MapReduce provides a ‚Äúbatch‚Äù ap-
proach for executing dataÔ¨Çow programs, another execution
approach is stream processing, which continuously analyzes
data streams online as they are produced. Languages that
support stream processing include StreamIt, which empha-
sizes optimizations for media streaming kernels [24], and the
Streams Processing Language (SPL), which has use-cases in
telecommunications, healthcare, Ô¨Ånancial trading, and several
other domains [9]. Systems for executing stream processing
dataÔ¨Çow programs include Borealis [1] and IBM‚Äôs InfoSphere
Streams [11], both of which run on clusters.
The semantics of dataÔ¨Çow programs depend on the semantic
properties of the individual operators they employ. Theseproperties include determinism, selectivity, blocking, stateful-
ness, commutativity, and partition-isolation (we deÔ¨Åne these in
Section III). Unfortunately, in current research and practice,
operator properties frequently remain unchecked. This can
have serious consequences. For example:
AconÔ¨Çuent dataÔ¨Çow graph is a graph in which two paths
converge on a single operator. This operator must either be
commutative, or enforce data ordering using blocking or
state [22]. If none of these properties hold, the entire pro-
gram is non-deterministic. Furthermore, blocking operators
may cause deadlocks [15], [21].
TheMapReduce programming model assumes that the Map
operator, which is used to Ô¨Ålter data and partition it into
subsets by key, is stateless, and the Reduce operator, which
is used to aggregate data, is partition-isolated [5]. Languages
meant to run on MapReduce do not check these properties
for user-deÔ¨Åned code [2], [17]. If the properties do not hold,
MapReduce may yield unpredictable results.
Compiler optimizations for dataÔ¨Çow programs rely on op-
erator properties. For instance, synchronous dataÔ¨Çow lan-
guages use operator selectivity for scheduling and buffer
allocation [14]. Some parallelizers require stateless opera-
tors [24], while others accommodate stateful operators that
satisfy partition-isolation [20]. If properties are unknown,
the program cannot be optimized; if properties are unreli-
able, the program may be optimized incorrectly.
These examples illustrate that operator properties offer clear
beneÔ¨Åts. Further, operator properties are not overly difÔ¨Åcult
to specify, because an operator developer need only indicate
whether a property holds for an operator or not. Even so,
once speciÔ¨Åed, the correctness of operator properties should
be veriÔ¨Åed, and in the absence of speciÔ¨Åcations, it would still
be useful to have a means for determining whether particular
properties hold for given operators.
One approach that might be used to check operator proper-
ties for dataÔ¨Çow programs is static analysis. This is challenging
for several reasons. Operator code often uses pointers and
multi-threading [1], leading to a large analysis state. This
causes static analyses to be expensive and to produce overly
conservative results. Another issue is that real-world dataÔ¨Çow
programs tend to be multi-lingual. For example, SPL pro-
grams often use operators written in SPL, C++, or Java [9].
Static analyzers need to handle each language involved, as
well as cross-language interactions. A third impediment to
static analysis is code-generation. For example, operators for978-1-4799-0215-6/13/$31.00 c2013 IEEE ASE 2013, Palo Alto, USA103XML processing [16] or composite event detection [8] are
actually mini-compilers, and it is difÔ¨Åcult to statically analyze
a compiler to determine properties of the code it generates.
This paper presents a dynamic-analysis approach for verify-
ing properties of dataÔ¨Çow operators . Given a property and an
operator, our technique generates tests and checks their results
to observe whether the property is violated. The analysis
results are easy to understand; they either indicate that no
counter-evidence for the property was found, or, if the property
is violated, they include a concise concrete input as evidence
for why it does not hold.
Our approach is analogous to the ‚Äúunit testing‚Äù of individual
components that make up a ‚Äútraditional‚Äù software system. In
dataÔ¨Çow programs these components are operators, supplied
with dataÔ¨Çow processing systems or created by engineers to
enact particular operations. By applying our approach to these
operators, engineers can help ensure that the building blocks
on which their programs are founded behave appropriately,
prior to Ô¨Åelding entire programs in which failures can be more
difÔ¨Åcult to detect and faults can be more difÔ¨Åcult to localize.
In this paper we make three signiÔ¨Åcant contributions:
1) We are the Ô¨Årst to formally deÔ¨Åne core properties of op-
erators in dataÔ¨Çow languages, including the properties of
determinism, selectivity, blocking, statefulness, commuta-
tivity, and partition-isolation.
2) We describe a testing methodology for checking whether
these operator properties hold. Our methodology requires
only the operator-under-test. No prior test inputs are needed,
and the operator may be written in any language, and may
even use code-generation. Our methodology gains efÔ¨Åciency
by sequencing the consideration of properties intelligently,
in a manner that allows us to check fewer properties overall.
3) We present empirical results on a suite of 56 SPL op-
erators [9] that are widely used by engineers to create
dataÔ¨Çow programs. Our results compare several test gen-
eration techniques across our target properties, and show
that our approach is both effective and efÔ¨Åcient.
II. B ACKGROUND AND RELATED WORK
A. DataÔ¨Çow Program Terminology
DataÔ¨Çow programs involve streams and operators. We dis-
tinguish between operator instances and operator deÔ¨Ånitions .
An operator instance (at the program level) is a vertex in a
dataÔ¨Çow graph. An operator deÔ¨Ånition is a conÔ¨Ågurable blue-
print for operator instances.
Figure 1 shows a simple Ô¨Ånancial streaming application.
Operator instance sepTQ separates a market feed into trades
and quotes. The instance avgPrice of the Aggregate operator
computes, for each stock symbol, the average price over the
last 30 seconds. The instance match of the Join operator Ô¨Ånds
deals (higher-than-average quotes). The instance sumPrice
of the Average operator adds the proÔ¨Åt from all deals, not
partitioned by stock symbol.
Different instances of the same operator can have different
properties. For example, in Figure 1, the avgPrice instance of
Aggregate is partitioned, whereas the sumPrice instance is not.
sepTQ: Custom avgPrice: Aggregate match: Join sumPrice: Aggregate stream operator instance : operator definition port Feed Quotes Deals Profit Fig. 1. Sample application
Therefore, the objects of analysis in this paper are operator
instances, not operator deÔ¨Ånitions. That is to say, operator
instances constitute the basic ‚Äúunit‚Äù of dataÔ¨Çow programs that
our approach tests individually.
The point at which a stream connects to an operator is called
aport, as shown in Figure 1. We are concerned only with
operators that have at least one input and one output port.
Astream is an ordered sequence of data items and punctu-
ations. A data item is a stream element that carries payload
information of interest to the application. In this paper, we
are not concerned with the format or representation of data
items; they may be as simple as integers or as complicated as
records with deeply nested data structures. A punctuation is
a control signal that carries no further information; it is used
to separate batches of data items, for example, to indicate
window boundaries. (Because streams are conceptually inÔ¨Å-
nite, reliable, Ô¨Årst-in Ô¨Årst-out communication channels, many
operators use the window concept to limit the number of data
items in the stream. For example, avgPrice in Figure 1 uses
data items from the most recent 30 seconds.)
We assume a model in which the arrival of a data item
or a punctuation at an input port of an operator triggers an
operator Ô¨Åring [22]. When an operator Ô¨Åres, it performs some
computation, which may cause data items and punctuations to
be submitted to some or all of its output ports.
B. Related Work
In our review of prior work, we could Ô¨Ånd no attempts to
formally deÔ¨Åne the properties of dataÔ¨Çow operators. Further,
while we found some static analyses capable of checking some
properties, we found no dynamic analyses or testing techniques
for determining semantic properties of operators.
Olston et al. [17] present a technique for generating example
data for dataÔ¨Çow programs. They use a dynamic test case
generation approach, as we do. While they create example
data relevant to the entire behavior of a whole dataÔ¨Çow graph,
we focus on speciÔ¨Åc properties of single operator instances.
One drawback of their technique is that it requires sample data,
as well as elaborate speciÔ¨Åcations for each operator (‚Äúequiv-
alence classes‚Äù and ‚Äúinverting computation‚Äù). In contrast, our
technique requires only the subject operator instance.
Most of the remaining related work on checking properties
of dataÔ¨Çow operators uses static analysis. Static analysis can
produce false positives, while dynamic analysis can produce
false negatives. It is meaningless to say that one or the other is
better in general; for our task, either could be useful. We begin
with dynamic analysis because we need to analyze multiple
languages including generated code, which would be difÔ¨Åcult
with static analysis. Another advantage of dynamic analysis
in our setting is that it can easily generate concrete evidence
for a true positive. This is more difÔ¨Åcult with static analysis.104Rinard and Diniz [19] present a static analysis for commuta-
tivity in a C++ subset, which they use for auto-parallelization.
Raman et al. [18] describe an analysis that breaks the body of
a C loop into dataÔ¨Çow operators, and also discovers operator
statefulness, which they use for auto-parallelization. Jayaram
and Eugster [12] analyze EventJava to discover inter-operator
properties that help reduce communication overhead (in con-
trast, we focus on properties of individual operators.) Hueske
et al. [10] statically analyze Java code for selectivity, read-
sets, and write-sets. Schneider et al. [20] statically analyze
SPL code to discover selectivity, statefulness, and partition
isolation, for use in auto-parallelization.
An attractive alternative to static and dynamic analysis is to
establish dataÔ¨Çow operator properties by construction in the
language design. In StreamIt, selectivity and statefulness are
immediately obvious from the code, and the compiler exploits
that for scheduling, buffering, and auto-parallelization [24].
DryadLINQ can use annotations to establish a property they
call AssociativeDecomposable, used for the partial aggregation
hoisting optimization [26]. In general, however, dataÔ¨Çow appli-
cations often call out to traditional languages like C++ or Java
for some of their functionality, at which point the design of the
dataÔ¨Çow language alone cannot establish semantic properties.
III. P ROPERTIES
For this work, we selected six operator properties that
are particularly important for reliability and optimization of
dataÔ¨Çow programs. We left some less important properties
(such as idempotence, associativity, and attribute forwarding)
to future work. This section introduces the six properties.
We consider only properties for which our approach can
potentially Ô¨Ånd certain evidence via testing, so we deÔ¨Åne each
property with existential quantiÔ¨Åers ( 9).
A. Notation and Wellformedness
Before introducing the properties, we deÔ¨Åne the notation
used in their formal deÔ¨Ånitions. Square brackets []denote
lists, curly bracesfg denote sets, and angle brackets hi
denote tuples. The underscore is a don‚Äôt-care binding (a
wild-card). The remaining notation comes from Ô¨Årst-order
predicate logic. The deÔ¨Ånitions use the following domains:
Data domain D=DataItems[f;"g. Each d2Dis
either a data item, a punctuation (denoted ), or a divergence
(denoted", meaning that no more input can be sent to a
given port of an operator due to blocking).
Input domain I=DN. Each input i2Ito an operator
Ô¨Åring is a pair consisting of a stream element and an input
port number. For example, h1;0iindicates stream element
1on input port 0.
Output domain O=list(DN). Each output o2Ofrom
an operator Ô¨Åring is a list of hstream element, output port
numberipairs. For example, [h2;0i;h2;1i]indicates stream
element 2on output port 0and then output port 1.
Execution trace domain trc(op) = list(IO). Each trace
t2trc(op)for an operator opis a list ofhinput, outputi
pairs. For example, trace
hh1;0i;[ ]i;hh1;0i;[ ]i
has twooperator Ô¨Årings, each consisting of an input stream ele-
ment 1 on port 0 and an empty output [ ].
The formal deÔ¨Ånition of a property Pis speciÔ¨Åed as a predicate
P(op)4=of an operator instance op. Variables must be
bound as formal parameters, or via quantiÔ¨Åers ( 9or8), or
by appearing on the left side of a pattern match whose right
side is fully bound (for example, if tandjare bound, then
hi;i=t[j]binds i).
B. Non-determinism
A deterministic operator always generates the same output
sequences for a given input sequence. If an operator generates
two different output sequences for the same inputs, it is non-
deterministic. For example, an Aggregate operator instance
with a window based on physical machine time may generate
different outputs if it receives the same inputs with different
inter-arrival times.
DeÔ¨Ånition.
isNonDeterministicOp (op)4=
9t; t02trc(op) :sameInput (t; t0)^differentOutput (t; t0)
sameInput (t; t0)4=
8j2dom(t) : 
hi;i=t[j]^hi0;i=t0[j]
) 
i=i0
differentOutput (t; t0)4=
9j2dom(t) :h; oi=t[j]^h; o0i=t0[j]^o6=o0
For example, traces [hh1;0i;[h1;0i]i]and[hh1;0i;[h2;0i]i]for
an operator have different outputs for the same input stream;
these provide evidence that the operator is non-deterministic.
Motivation. Whether or not determinism is required de-
pends on the application. Knowing about non-determinism is
important, especially when users require an application‚Äôs out-
puts to be repeatable. If any operator in an application is non-
deterministic, the entire application may be non-deterministic.
If the outputs from the application differ for the same input
data, simply ‚ÄúdifÔ¨Ång‚Äù outputs is insufÔ¨Åcient for correctness
checking. Thus, engineers using our approach can detect
potential faults in operators and debug them more effectively.
C. Selectivity
Selectivity constrains the number of data items produced by
an operator per data items consumed. If an operator produces
>1data items for at least one Ô¨Åring, it is proliÔ¨Åc ; for example,
this is the case for a Split operator conÔ¨Ågured to duplicate
its input data items on multiple output ports. If an operator
produces1data items for all Ô¨Årings and none for at least
one Ô¨Åring it is selective ; for example, this is the case for a
Filter operator conÔ¨Ågured to output data items meeting some
criterion. If an operator produces exactly 1 data item for each
Ô¨Åring it is one-to-one ; for example, this is the case for a
Custom operator conÔ¨Ågured to count the number of input data
items received and output the current count each time it Ô¨Åres.
DeÔ¨Ånition.
isProliÔ¨ÅcOp (op)4=9t2trc(op) :isProliÔ¨Åc (t)
isProliÔ¨Åc (t)4=9j2dom(t) :h; oi=t[j]^multipleTuples (o)
multipleTuples (o)4=
9k; k02dom(o) :k6=k0^hd;i=o[k]^hd0;i=o[k0]
^d2Tuples^d02Tuples105isSelectiveOp (op)4=
:isProliÔ¨ÅcOp (op)^ 
9t2trc(op) :isSelective (t)
isSelective (t)4=9j2dom(t) :h; oi=t[j]^noTuples (o)
noTuples (o)4=8k2dom(o) :hd;i=o[k]^d =2Tuples
isOneToOneOp (op)4=:isProliÔ¨Åc (op)^:isSelective (op)
For example, since trace [hh1;0i;[h1;0i;h1;1i]i]has one input
h1;0iwith two outputs h1;0i,h1;1i, it is evidence for an
operator being deÔ¨Ånitely proliÔ¨Åc. On the other hand, trace
[hh1;0i;[h1;0i]i;hh2;0i;[ ]i]has no output for input h2;0i,
which is evidence for being potentially selective.
Motivation. Selectivity can be used for scheduling and
allocating buffers to improve performance [14], [24]. It can
also help in parallelization, since it can simplify sequence
numbers for ordering [15], [20]. Providing correct selectivity
speciÔ¨Åcations thus enables safe optimization. Operator devel-
opers can use our technique to increase their conÔ¨Ådence that
they have speciÔ¨Åed selectivity correctly.
D. Blocking
Unlike the previous two properties, blocking is speciÔ¨Åc to an
input port. Input port pis blocking if a Ô¨Åring can get stuck part-
way, so no further Ô¨Årings are possible at puntil it is unblocked.
For example, the Gate operator in SPL blocks one input port
until it receives an acknowledgment from another input port.
DeÔ¨Ånition.
isBlockingOp (op; p)4=9t2trc(op) :isBlocking (t; p)
isBlocking (t; p)4=
9j2dom(t) :
hh; pi; oi=t[j]^ 
9k2dom(o) :h";i=o[k]
For example, trace [hh1;0i;[ ]i;hh1;0i;[h";0i]i]provides evi-
dence that the operator is blocking on input port p= 0.
Motivation. Blocking can help ensure that stream elements
are placed in proper order. A stream graph with a conÔ¨Çuence,
but without a blocking or stateful operator, may behave
unpredictably. However, blocking is also the main source of
deadlock in dataÔ¨Çow applications [15]. A developer trying to
locate the root cause of a deadlock can use knowledge about
blocking operators as a starting point.
E. Statefulness
An operator is stateful if its current output is affected by
input stream elements earlier in the same trace. An example
in SPL is a Custom operator conÔ¨Ågured to count input stream
elements and output the current count each time it Ô¨Åres.
DeÔ¨Ånition.
isStatefulOp (op)4=9t2trc(op) :isStateful (t)
isStateful (t)4=
9j; j02dom(t) :
j6=j0^hi; oi=t[j]^hi; o0i=t[j0]^o6=o0
For example, trace [hh1;0i;[h1;0i]i;hh1;0i;[h2;0i]i]shows
that for the same input stream element, there are two dif-
ferent output stream elements; this provides evidence that the
operator is stateful.
Motivation. A stateless operator is easy to parallelize for
better performance, and easy to restart or migrate for betterfault-tolerance. On the other hand, a stateful operator can
buffer out-of-order stream elements in a conÔ¨Çuent graph. An
application developer can use knowledge of which operators
are stateful to help establish whether the entire program has
predictable behavior. Furthermore, many applications inher-
ently need stateful operators, for instance, for aggregation.
F . Non-commutativity
Non-commutativity, like blocking, is speciÔ¨Åc to an input
port. An input port pis commutative if a change in the order
of input data items sent to pwithin some range (such as
within a window) does not change the outputs. For example,
anAggregate operator conÔ¨Ågured to Ô¨Ånd the maximum of Ô¨Åve
input data items is commutative, since the order of the Ô¨Åve data
items does not affect the maximum. In contrast, an Aggregate
operator conÔ¨Ågured to Ô¨Ånd the last of Ô¨Åve input data items is
non-commutative.
DeÔ¨Ånition.
isNonCommutativeOp (op; p)4=
9t2trc(op) :isNonCommutative (t; p)
isNonCommutative (t; p)4=
9j; k; n2dom(t) :j < k
^endsWindow (t; j 1)^endsWindow (t; j+n 1)
^endsWindow (t; k 1)^endsWindow (t; k+n 1)
^onlyUsesPort (t; j; k j+n; p)
^sameSetOfInputs (t; j; k; n )^differentOutput (t; j; k; n )
endsWindow (t; j)4=
j= 1_h; oi=t[j]^ 
9k2dom(o) :h;i=o[k]
onlyUsesPort (t; l; n; p )4=
8j2dom(t) : 
lj^j < l +n^hh; p0i;i=t[j]
) 
p=p0
sameSetOfInputs (t; j; k; n )4=
fi:l2dom(t)^jl^l < j +n^hi;i=t[l]g=
fi:l2dom(t)^kl^l < k +n^hi;i=t[l]g
differentOutput (t; j; k; n )4=
h; oi=t[j+n 1]^h; o0i=t[k+n 1]^o6=o0
For example, [hh1;0i;[ ]i;hh2;0i;[h2;0i;h;0i]i]followed by
[hh2;0i;[ ]i;hh1;0i;[h1;0i;h;0i]i]shows that after changing
the order of two input data items, the output data items are
different before the window punctuation; this trace provides
evidence that the operator is non-commutative.
Motivation. When two paths in a dataÔ¨Çow graph converge
on a single operator, differences in the speed and scheduling
of upstream operators can cause data items to be out of order.
In general, this can lead to application-level non-determinism
even if all individual operators are deterministic. An applica-
tion developer can use commutativity to Ô¨Ånd out whether an
operator tolerates disorder.
G. Partition-interference
A partitioning keyis a part of each input data item used
for processing subsets of the stream separately. Keys are often
simply record attributes, but to keep the deÔ¨Ånition general,
we write read(d; k)for reading key kfrom data item d. In
SPL, such keys are speciÔ¨Åed by conÔ¨Åguring an operator with106Operator under test Analyzer Property of interest Property value and evidence SPL compiler and runtime environment Test program and input Test output Fig. 2. Testing framework
apartitionBy parameter. In a partition-isolated operator, output
data items for one key are not affected by input data items with
different keys. If data items with different key values can affect
each other, the operator is partition-interfering. For example,
anAggregate operator conÔ¨Ågured to compute the average price
of a stream of stock trades separately for each stock symbol is
partition-isolated. On the other hand, an Aggregate conÔ¨Ågured
to add the prices of a stream of deals irrespective of stock
symbol is partition-interfering.
DeÔ¨Ånition.
isPartitionInterferingOp (op; k)4=
9t; t02trc(op) :9c2D:
sameInputs (t; t0; k; c)^differentOutputs (t; t0; k; c)
sameInputs (t; t0; k; c)4=
[i:hi;i2t^hd;i=i^read(d; k) =c] =
[i:hi;i2t0^hd;i=i^read(d; k) =c]
differentOutputs (t; t0; k; c)4=
[o:hi; oi2t^hd;i=i^read(d; k) =c]6=
[o:hi; oi2t0^hd;i=i^read(d; k) =c]
For example, in the traces [hhh1; ci;0i;[ ]i;hh2; ci;0i;[h2;0i]i]
and[hhh1; ci;0i;[ ]i;hhh1; c0i;0i;[]i;hh2; ci;0i;[h3;0i]i], an in-
put data item whose key c0differs from the key cof the other
two input data items affected the output. This pair of traces
provides evidence that the operator is partition-interfering.
Motivation. Partition-isolation is useful for paralleliza-
tion [5], [20]. An optimizer can parallelize partition-isolated
operators by using a hash-split, thus giving each operator
replica a disjoint partition of the key domain. Operator de-
velopers can use our approach to test whether they mistakenly
created operators that are partition-interfering.
IV. T ESTING METHODOLOGY
The desired value of an operator property in the context of
a dataÔ¨Çow program depends on the intent of the program‚Äôs
developer. For example, a developer could use a stateful
operator on purpose to compute an average, or use a stateless
operator on purpose to simplify parallelization. In either case,
a mismatch between the developer‚Äôs intent and the actual fact
is a defect. This section introduces a testing framework for
determining operator properties.
A. Testing Framework
Figure 2 shows our testing framework. The main component
is the Analyzer , which takes as input the property that userswant to test and an operator-under-test (OUT). The Analyzer
generates a test program wrapping the OUT and sends it to the
SPL compiler to generate an executable program. The Analyzer
then generates inputs appropriate for the property and executes
the program using the SPL runtime environment. Execution
results are returned to the Analyzer , which checks whether
evidence is found to show that the property holds. If evidence
is found, the Analyzer presents users with that evidence. If
evidence is not found, the Analyzer generates additional tests
until it reaches a technique time limit, and on reaching that
limit, reports that the property ‚Äúpotentially‚Äù does not hold. The
Analyzer can also report statistical information that can help
users assess the extent of the evidence provided.
For example, suppose that a developer creates an operator op
that is required, due to expected compiler optimizations (e.g.,
as per [24]), to be stateless. Suppose the Analyzer generates
test cases that serve as evidence that opis stateful. Presented
with this evidence, the developer can correct their code or take
other necessary steps to ensure that incorrect optimization does
not occur. If, on the other hand, the Analyzer Ô¨Ånds no test cases
that indicate that opis stateful, the developer can have some
conÔ¨Ådence (even if not certainty) that opis stateless.
We now describe how the Analyzer wraps an OUT to form
a test program that produces output as part of test oracles.
Figure 3 shows the dataÔ¨Çow graph of a test program that
contains four operator instances: Source ,Controller , the OUT ,
andSink.Source reads in a test from a Ô¨Åle. Controller sends
input stream elements from Source to the appropriate input
port of the OUT . (Most OUT s have exactly one input port and
one output port, but an OUT can have multiple input and output
ports.) The OUT receives the stream(s) from Controller and
executes to generate output stream(s). Sink reads the stream(s)
from OUT and prints them out. The printed information from
Controller andSink is used by the Analyzer to make decisions.
Source Controller Operator under test Sink 
Fig. 3. DataÔ¨Çow graph of test program
B. Testing Order
The order in which operator properties are tested is impor-
tant: the status of certain properties can preclude the need
to check others. Thus, we have analyzed the relationships
between properties in order to identify a testing order that
maximizes testing efÔ¨Åciency. Our testing order is the partial
order shown in Figure 4. (The numbers shown in the Ô¨Ågure
are discussed later.) As the Ô¨Ågure shows, the Ô¨Årst and most
important property to test for is non-determinism. If an OUT
is non-deterministic, it is less important to test for other
properties in practice, because they are harder to exploit. From
the deÔ¨Ånition of selectivity, we can see that it focuses on the
OUT‚Äôs behavior after each single input stream element, while
other properties focus on two or more stream elements, so
selectivity is independent of other properties. Thus, we can test
for selectivity before, after, or in parallel with other properties.107Selectivity: 41 (10 prolific +     22 selective +        9 one-to-one) Non-determinism: 56 (41 yes + 15 no) Blocking: 41 (4 yes + 37 no) Statefulness: 37 (22 yes + 15 no) Non-commutativity: 26 (18 yes + 8 no) Partition-interference: 26 (2 yes + 24 no) 41 41 4 37 22 22 4 Fig. 4. Testing order
Next, we check whether the OUT has two or more input ports.
If it does, we check blocking Ô¨Årst, because blocking situations
in SPL occur only when one input port is stalled while the
operator instance waits for data on another input port. If the
OUT is blocking, it is stateful, so we can skip the check
for statefulness and directly check for non-commutativity and
partition-interference. If the OUT is non-blocking, or if it has
only one input port, we check whether it is stateful Ô¨Årst. If
it is not stateful, it is commutative and partition-isolated, and
requires no further checking. If it is stateful, we check non-
commutativity and partition-interference.
C. Testing Each Property
The formal deÔ¨Ånitions of the properties given in Section III
provide insights into ways to design testing strategies. Algo-
rithm 1 describes our general testing strategy, encapsulating
property-speciÔ¨Åc details in calls to four functions ( initia-
lize,generateTest ,checkProperty , and evidenceValue ). Table I
shows the functions for each of the six properties in detail.
Below we provide additional comments on the speciÔ¨Åc design
choices used to perform property testing for SPL.
Algorithm 1 Algorithm for checking properties
evidence =null
initialize (property )
while evidence ==null^:(techniqueTimeLimit reached )do
test=generateTest (property )
evidence =checkProperty (property ,test)
end while
return evidenceValue (property ,evidence )
1) Non-determinism: The main cause of operator non-
determinism in SPL is the inter-arrival time of stream ele-
ments, so we need to include this factor in tests. An input
for non-determinism thus includes stream elements and port
numbers, as well as delays between those stream elements.
2) Selectivity: The functions for testing selectivity in Ta-
ble I follow directly from the formal deÔ¨Ånition.
3) Blocking: In SPL, blocking occurs in operators with
more than one input port. Such operators can block one port
for synchronization until an expected stream element arrives
at another port. Therefore, our tests contain stream elements
for only the input port under test. For blocking, we compile
the test program into a single process, not distributed, so thatstream communication is a simple function call. We check
whether the submit function in the upstream operator instance
(the Controller in Figure 3) returns within blockingTimeLimit .
4) Statefulness: The functions for testing selectivity in
Table I follow directly from the formal deÔ¨Ånition, but for
simplicity, we input the same data item to each Ô¨Åring.
5) Non-commutativity: Commutativity is speciÔ¨Åc to a par-
ticular input port p. We begin by checking whether submitting
data items to this input port eventually causes punctuations to
appear on an output port. Let wbe the number of input data
items seen before an output punctuation appears. We generate
permutations of wdata items and check whether any two
permutations generate different outputs.
6) Partition-interference: In SPL, programmers declare
partitioning by conÔ¨Åguring operators with a partitionBy param-
eter, so we need only check those operators for the speciÔ¨Åed
key. We check two traces, one in which all input data items
have the same key, and the other for the same input data items,
but interspersed with data items whose keys are different.
D. Test Generation
For this work, we created test generation techniques that are
particularly appropriate given the characteristics of dataÔ¨Çow
operators and our testing methodology, along with techniques
that facilitate the empirical comparison of techniques.
To create test generation techniques we must Ô¨Årst generate
tests, and there are many approaches that could be utilized
to do this. In this work, we begin with a random test gener-
ation [6] approach, in which, for each input to an operator,
values are randomly chosen within a range appropriate for its
type. We chose this approach because it is relatively simple
to implement and provides a baseline for comparison.
Random test generation involves a huge search space, and
thus, we sought test generation approaches that reduce that
search space. The second approach that we consider leverages
the fact that in SPL code, there are many comparisons between
attributes and constants. For example, some operators will
submit data items if one attribute of an input data item equals
a number. If tests cover both sides of those comparisons, true
and false, properties may be revealed. To create such tests
we created a scanner to extract constants such as strings,
integers, and Ô¨Çoating-point numbers from the code, and place
this information in pools. We then randomly select values from
these pools. We call this approach pool-based generation.
The third approach that we consider leverages the fact that
in SPL code, there are many operators that use combinations
of predicates, e.g., x.y==5 && x.name=="Smith" . In
this case, the selection space for the pool selection approach
can still be inordinately large, so we apply an approach
widely used in combinatorial testing to further reduce the
space: pairwise testing [4]. Pairwise-generated tests cover all
combinations of two. This makes their numbers much smaller
than the number of possible random or pool-generated tests.
We call this approach pair-based generation.
To employ the foregoing approaches in the context of SPL,
we considered two orthogonal features of stream processing108TABLE I
FUNCTION IMPLEMENTATIONS FOR PROPERTIES
Property initialize generateTest checkProperty evidenceValue
Non-
determinismReturn two lists l1andl2of
triples hdelay; d; piwith the
same values for dandp, but
different delay values.Runl1;2withd; p values as
inputs, timed by the delay val-
ues. If the outputs differ, return
hl1; l2i, else return null.Ifevidence 6=null, return ‚Äúdef-
initely non-deterministic‚Äù, else
return ‚Äúpotentially determinis-
tic‚Äù.
Selectivity SetselectiveEvidence =null. Return list lof pairs hd; pi. Runl. If any Ô¨Åring has >1
output data items, return l. If
any Ô¨Åring has 0output data
items, set selectiveEvidence =l.
Return null.Ifevidence 6= null, re-
turn ‚ÄúdeÔ¨Ånitely proliÔ¨Åc‚Äù, else if
selectiveEvidence 6=null, re-
turn ‚Äúpotentially selective‚Äù, else
return ‚Äúpotentially one-to-one‚Äù.
Blocking Return list lof pairs hd; pi,
where all pvalues are the port-
under-test.Run l. If any Ô¨Åring stalls
forblockingTimeLimit , return l,
else return null.Ifevidence 6=null, return ‚Äúpo-
tentially blocking‚Äù, else return
‚Äúpotentially non-blocking‚Äù.
Statefulness Return list lof pairs hd; pi,
where all dvalues are the same
for simplicity.Runl. If any two Ô¨Årings pro-
duce different outputs, return l,
else return null.Ifevidence 6=null, return
‚ÄúdeÔ¨Ånitely stateful‚Äù, else return
‚Äúpotentially stateless‚Äù.
Non-
commutativityGenerate and run tests lof in-
creasing length until a window
punctuation is generated. Let w
be the length of such a list.Generate list lofwpairshd; pi;
return permutations of l.Run each permutation. If any
two permutations produce dif-
ferent outputs, return them, else
return null.Ifevidence 6=null, return ‚Äúdef-
initely non-commutative‚Äù, else
return ‚Äúpotentially commuta-
tive‚Äù.
Partition-
interferenceGenerate list l1of pairs hd; pi
with the same read(d; k)for
alld. Create l2containing ele-
ments from l1interspersed with
data items with different keys.Runl1andl2. If any of the
outputs for corresponding data
items differ, return hl1; l2i, else
return null.If evidence 6= null,
return ‚ÄúdeÔ¨Ånitely partition-
interfering‚Äù, else return
‚Äúpotentially partition-isolated‚Äù.
languages: dataÔ¨Çow operators and our testing methodology.
First, we often need to check output data items from the OUT;
thus, we require tests for which the output is non-empty before
we can check for properties. For example, when checking for
commutativity, we Ô¨Årst need a test that generates output data
items, and then we can try permutations of the input data
items to see whether the output changes. This suggests that
the application of mutation [3] may be helful in our context.
SpeciÔ¨Åcally, we apply mutation to existing tests that do not
generate output data items. In our approach, we add additional
input data items by mutating a single character of string
attributes, adding one and minus one for numeric attributes,
and acting similarly for enumeration constants and timestamps.
In this way, we expect to be able to trigger comparisons that
generate output data items.
The second additional feature we consider involves test
reuse . When we follow the order of property testing displayed
in Figure 4, we generate tests while checking properties earlier
in the order. Since these tests are ready to use we may save
time by reusing them, thus improving testing efÔ¨Åciency. Note
that when using this approach and when checking properties
earlier in the order we cannot save every generated test; thus,
we save only those that actually generate output data items.
The three approaches for generating test inputs that we
have described can be combined with these two orthogonal
factors to create 12 different test generation techniques. These
techniques combine the three possible test input generation
approaches (random, pool-based, and pair-based) with muta-
tion usage (with and without), and reuse of tests (with and
without). We believe that these techniques may have different
strengths across different properties and operators, and thus,
in our empirical studies, we investigate all of them.
Finally, when resources permit, test engineers may simul-
taneously apply multiple testing techniques, and in doing sotake advantage of the relative strengths and weaknesses of
individual techniques. To investigate this approach, in addition
to studying the 12 techniques individually, we created a hybrid
technique that applies all 12 techniques simultaneously in
parallel for a given OUT. As soon as any technique Ô¨Ånds
evidence to show that a tested property deÔ¨Ånitely holds, it stops
and returns the answer and evidence. If no single technique
Ô¨Ånds evidence, all 12 techniques reach the technique time
limit, and the property potentially does not hold.
V. E MPIRICAL STUDY
To evaluate our approach to verifying properties there are
several dimensions that we need to assess. The Ô¨Årst dimension
involves how effective the approach is at determining whether
properties hold or not. Here, we are interested in the precision
of the approach (how often we report no evidence when indeed
no evidence exists, on the ‚Äúpotentially‚Äù side of a property)
and the recall of the approach (how often it Ô¨Ånds evidence
when such evidence does in fact exist, on the ‚ÄúdeÔ¨Ånitely‚Äù
side of a property). The second dimension of interest involves
theefÔ¨Åciency of the approach; that is, how quickly it returns
results. The third dimension of interest involves the tradeoffs
between the different test generation techniques that we have
proposed. To investigate these dimensions of our approach we
pose the following research questions:
RQ1 : How do the test generation techniques fare, in terms of
precision and recall, in testing the six properties considered.
RQ2 : How do the test generation techniques fare, in terms of
efÔ¨Åciency, in testing the six properties considered.
Note that by addressing these questions, we also obtain data
that allows us to discuss differences that occur in assessing
the six different properties we consider.109A. Objects of Analysis
We selected 56 operator instances from the standard toolkit
of IBM‚Äôs InfoSphere Streams [11], and from examples found
in online tutorials for that product. We selected these operators
in part because InfoSphere Streams is a widely-used commer-
cial product, most of the operators are in commercial use, and
the properties we selected for study are the most helpful for
optimizations and correctness arguments in that programming
environment. Furthermore, the operator instances we selected
cover all operator deÔ¨Ånitions in the standard toolkit that have at
least one input and one output port, including 6 relational and
11 utility operator deÔ¨Ånitions. They also include 4 primitive
operators written in Java and 6 primitive operators written in
C++. The lines-of-code per operator instance are between 54
and 252, and the average is 145.7.
When applying our approach we utilized the testing order
described in Section IV-B; Figure 4 indicates the numbers
of operator instances that needed to be considered at each
step. If we did not use the testing order, we would have
needed to check all 56 instances for each property (336 checks
altogether); the use of the ordering reduced the number of
checks to 227.
B. Variables and Measures
Independent Variable. Our experiment manipulated one
independent variable: testing technique. We applied each of
the 13 test generation techniques on all properties except non-
determinism. For non-determinism, given the order in which
we considered properties, there were no existing tests for
reuse, so only seven techniques were applicable.
Ideally, we would like to compare our techniques to some
baseline. Unfortunately, there are currently no automated tech-
niques that could serve as such baselines. A second way to
compare techniques is to use a theoretical optimal approach to
judge whether properties hold. Such an approach yields correct
answers, allowing the precision and recall of techniques to
be assessed relative to those correct answers. This approach
can be approximated by using human judgment and code
inspection to determine whether properties hold; however, this
process is also expensive (a primary motivation for developing
our automated methodology). Thus, to determine whether
properties held, we began by running our techniques on all
properties. If we found evidence for a property, we were
certain about that case. We then investigated remaining cases
in which the techniques found no evidence, one by one. In
these cases, the Ô¨Årst author inspected each of the operators
relative to each remaining property, and determined whether
the property held. The second author then did the same, and
then the two authors met and came to a consensus opinion. In
this latter step, the authors found a single case in which their
judgments disagreed. (In our experiments, our techniques also
ultimately revealed the correct answer for this case).
Dependent Variables. We measured the precision, recall, and
efÔ¨Åciency of each technique as follows. Let Pbe a property,
letOpsPbe the set of operator instances on which Pis tested,
lettbe a technique, and let hbe the human expert baseline.DV1: Precision. Precision measures the extent to which
a technique does not Ô¨Ånd false evidence when indeed no
evidence exists. We calculated the precision of tas:

op2OpsP::evidence (op; P; t)^:evidence (op; P; h)	

op2OpsP::evidence (op; P; h)	
DV2: Recall. Recall measures the extent to which a technique
Ô¨Ånds evidence when indeed such evidence exists. We calcu-
lated the recall for tas:
op2OpsP:evidence (op; P; t)^evidence (op; P; h)	

op2OpsP:evidence (op; P; h)	
DV3: EfÔ¨Åciency. For any application of a technique to a
property we hope to obtain evidence as to whether that
property holds as quickly as possible. Thus, to measure the
efÔ¨Åciency of a technique t, we measured the time used by t
to Ô¨Ånd evidence for each property Pin the set of positive
operator instances, fop2OpsP:evidence (op; P; h)g.
C. Experiment Setup and Operation
In theory, we can let techniques run forever, since evidence
that a property holds can be found at any time. In practice, of
course, time limits are required. To determine what technique
runtime limits might be reasonable we conducted preliminary
trials using our techniques. We began with a technique runtime
limit of one minute and increased it by multiplying it by three.
When we failed to gain more than a 1% improvement on
effectiveness for a technique runtime limit on all operators and
properties, we stopped and chose the previous limit. Since we
reached the point of diminishing returns at nine minutes, we
chose three minutes as the techniqueTimeLimit in Algorithm 1.
We chose two seconds as the blockingTimeLimit in Table I,
because preliminary trials showed that a one second threshold
did not work well whereas two seconds allowed us to draw
correct conclusions.
To check each property, we began with a small number of
input data items, since it is easier for people to understand
shorter evidence and tests. We chose to start with two input
stream elements in l, since for checking statefulness, we need
at least two input stream elements.
Finally, because each technique that we considered can act
differently in individual runs due to non-determinism in the
test generation approach, we applied each technique 10 times
to each operator.
Our experiments were run on a RedHat Linux box with
an Intel Core2duo at 2.4GHz. We used InfoSphere Streams
2.0.0.1.
D. Threats to Validity
Where external validity is concerned, the operators we stud-
ied were all drawn from one pool of operators associated with
the InfoSphere Streams product, and different operators might
yield different results. Further, we studied only six properties.
However, the operators we used are in wide commercial use,
and thus, represent operators of particular interest.110TABLE II
RECALL
PropertyWithout Reuse With Reuse
Hybrid Without Mutation With Mutation Without Mutation With Mutation
Rand Pool Pair Rand Pool Pair Rand Pool Pair Rand Pool Pair
Non-determinism 48.7 53.3 52.0 24.0 40.7 36.7 - - - - - - 74.7
Selectivity 75.3 93.4 91.6 91.9 100.0 100.0 84.7 93.8 93.1 93.8 100.0 100.0 100.0
Blocking 83.3 83.3 83.3 83.3 83.3 83.3 83.3 83.3 83.3 83.3 83.3 83.3 83.3
Statefulness 68.2 72.7 72.7 72.7 81.8 81.8 77.3 81.8 81.8 81.8 86.4 86.4 86.4
Non-commutativity 35.0 71.3 73.8 35.0 72.5 77.5 32.5 71.3 75.0 38.8 70.0 72.5 90.0
Partition-interference 80.0 93.3 88.8 79.6 90.8 91.7 83.8 90.4 88.8 83.8 88.8 89.6 94.6
TABLE III
EFFICIENCY
PropertyWithout Reuse With Reuse
Hybrid Without Mutation With Mutation Without Mutation With Mutation
Rand Pool Pair Rand Pool Pair Rand Pool Pair Rand Pool Pair
Non-determinism 114.9 109.8 118.7 146.0 127.8 136.3 - - - - - - 78.8
Selectivity 149.2 134.9 134.8 136.5 126.2 126.8 129.8 126.3 126.7 130.0 125.1 125.4 124.3
Blocking 32.4 32.3 32.4 32.1 32.1 32.1 32.0 31.9 31.9 32.0 32.0 32.0 31.9
Statefulness 62.2 53.4 47.5 55.5 41.6 36.2 45.2 37.4 32.3 48.8 40.4 37.7 25.8
Non-commutativity 126.0 63.6 55.3 125.8 56.6 48.4 127.7 61.5 56.1 120.8 59.2 58.3 23.5
Partition-interference 58.6 39.8 43.4 53.0 38.0 38.0 55.3 47.8 51.4 56.7 53.0 56.5 18.3
Our greatest concern for internal validity involves our
implementations of properties and scripts used to gather data.
To reduce these threats we carefully evaluated our properties
and scripts on a wide range of examples. Furthermore, while
our testing order obviates the need to run all techniques on all
properties, as an additional check on our implementations, we
did also apply all techniques in those cases in which they were
not required, and veriÔ¨Åed that they returned correct results in
these cases as well. Another potential threat involves the time
limits chosen for use in testing properties. As detailed above,
however, we based our time limit choices on preliminary trials.
Where construct validity is concerned, as a baseline for
comparing our results we used property determinations per-
formed by humans, which could have been incorrect. Also,
additional measures, such as the usefulness of the property
determinations, could be valuable.
E. Results
1) RQ1: Technique effectiveness: Precision. As expected, in
our study, no cases occurred in which a technique found false
evidence. Therefore, all techniques achieved 100% precision.
Recall. Table II presents recall data. The Ô¨Årst column
indicates the property name. Columns 2-13 are divided into
two sections: the Ô¨Årst six present data on techniques without
test reuse and the next six present data on techniques with test
reuse. Within each of these sections, the Ô¨Årst three columns
present data on techniques without mutation and the last three
present data on techniques with mutation. Within each set of
three columns, the columns represent random (Rand), pool-
based (Pool), and pair-based (Pair) test generation techniques,
respectively. The rightmost column shows the data on the
hybrid technique. Each cell shows the average recall rate
percentage over all ten runs. Bold font indicates which tech-
nique(s) were most effective for a given property.
From the data, we can see that the most effective technique
was (or included) the hybrid technique; it was the most
effective on all six properties and it achieved a recall rateranging from 74.7% to 100.0%. For selectivity, using pool
and pairwise techniques with mutation achieved the same
effectiveness as the hybrid technique, 100%. For blocking, all
techniques achieved the same recall rate, 83.3%.
For all properties, pair-based or pool-based test generation
approaches were at least as effective as (and on Ô¨Åve properties
more effective than) random test generation. Also, as noted,
techniques that use mutation were at least as effective as those
that do not on three of six properties. Finally, techniques that
use test reuse were at least as effective as those that do not
on three of six properties.
2) RQ2: Technique efÔ¨Åciency: Table III presents efÔ¨Åciency
data. The table has the same structure as Table II, but here
each cell represents the average time (in seconds) required
by each technique to Ô¨Ånd evidence. To be consistent with
Table II, we report data only for operators on which there
exists evidence. As the data shows, average times for each
technique were all relatively small, ranging from 18.3 to 149.2
seconds. The hybrid technique was the most efÔ¨Åcient for all
properties. Finally, random test generation required at least
the same amount of time as the other two test generation
approaches given the same mutation and reuse approaches.
VI. D ISCUSSION AND IMPLICATIONS
We now provide additional insights into the results of our
study, and comment on implications.
Failed cases: Our techniques failed to determine correct
answers for properties on only seven operators; in other cases,
at least one technique in one run returned a correct answer.
When checking for non-determinism, there were two cases
in which none of our techniques found evidence that the
property held. One is a DeDuplicate operator using 120
seconds as its timeout. If we control the number of stream
elements arriving at the operator in 120 seconds we can Ô¨Ånd
evidence to show that it is non-deterministic. To do this, we
could use an inter-element delay period of 120 seconds. In
our implementation, however, we limited inter-element delay111periods to Ô¨Åve seconds, and we limited the number of delay
periods on an input stream to three, so that we could try more
tests in a given time. In general, any operator having a window
or timeout based on a large physical time interval will present
such challenges. To address this problem, we could ask users
to provide delay period times.
The second case in which checking for non-determinism
failed involved a Join operator that generates output only when
a concatenation of three strings equals a fourth string. In
general, this is a difÔ¨Åcult constraint to satisfy. Asking users
for sample input data could help in this case.
When checking for blocking our techniques missed one
case: a Gate operator with buffer size 1,000. As implemented,
none of our techniques generates test inputs with 1,000 stream
elements in three minutes. To Ô¨Ånd evidence for this operator,
the techniques need to run longer, or begin with a larger
number of input stream elements.
When checking for statefulness our techniques missed three
cases: two Aggregate instances and a Sort instance. These
instances are conÔ¨Ågured with a delta window in which the
difference for an attribute between the Ô¨Årst and the last stream
element in the window is speciÔ¨Åed. Our techniques are not
likely to Ô¨Ånd stream elements satisfying such a condition.
Asking users for sample input data could help.
When checking for partition-interference our techniques
missed one case; a C++ primitive operator. Except on very
speciÔ¨Åc inputs this operator is typically blocking, in which
case its outputs cannot be checked for partition-interference.
Generating appropriate inputs in this case is difÔ¨Åcult. Again,
asking users for sample data could help.
Test generation approaches: Table II shows that random
test generation was less effective than the other test generation
techniques. Because random generation performs an unguided
search, its search space is too large to let it select useful values.
Comparing the pair-based and pool-based techniques, there
were only six cases in which pair-based was more effective,
and only four in which pool-based was more effective. As
mentioned in Section IV, pair-based generation beneÔ¨Åts when
multiple conditions need to be satisÔ¨Åed to obtain an output.
On inspection, we found that only two of the operators we
tested contained this type of code.
Mutation strategy: For selectivity and statefulness, tech-
niques that used mutation performed better than those that did
not. However, when checking for non-determinism, mutation
missed operators with time-based windows greater than two
seconds. On such operators, with mutation, the time required
is too large to allow techniques to generate sufÔ¨Åciently long
input streams. Therefore, in these cases none of the techniques
generates an output, and they report that the operator is
potentially deterministic. For non-commutativity and partition-
interference, mutation did not seem to be helpful. Mutation is
used to generate a test that is able to produce some output.
If existing tests already generate outputs, mutation will waste
time and then fail to Ô¨Ånd evidence in a technique time limit.
Test reuse: For selectivity and statefulness, techniques with
reuse were more effective and efÔ¨Åcient than those without,because they can use existing tests directly. In particular, when
checking for statefulness, the testing strategy described in
Section IV uses the same input stream elements. This strategy
can miss cases in which we require different input stream
elements to reveal the property, while test reuse can help
with this. For non-commutativity and partition-interference,
however, techniques with reuse displayed no advantage over
those without. These properties have more strict requirements
for tests, so existing tests may not satisfy them. For blocking
we deleted the input stream elements for the input port that
was not under test; thus, test reuse did not provide efÔ¨Åciency
advantages in this case.
Technique comparisons: Given our data and the foregoing
discussion, it is clear that among the 12 non-hybrid techniques,
there is no consistent winner. Mutation and reuse were not
always helpful. Thus, it is difÔ¨Åcult to recommend any single
technique as appropriate in all cases. This emphasizes the
importance of the hybrid technique. The hybrid technique is
able to produce more varied test cases, which help it succeed
on operators on which single techniques could not. Essentially,
the approach achieves greater ‚Äúsearch diversity‚Äù, in a manner
similar to that discussed in [7]. Of course, in some sense, the
hybrid technique achieves its effectiveness because it allots
additional time to testing than the other techniques through
its use of additional resources (e.g., multiple processors).
Nonetheless, if a user wishes to obtain a more accurate answer
more quickly and has access to such resources, the hybrid
technique is the best choice.
VII. C ONCLUSIONS AND FUTURE WORK
DataÔ¨Çow programming is a widely-used paradigm, and with
the rise of big data it has gained signiÔ¨Åcant commercial
importance. In this work, we have presented an approach for
testing dataÔ¨Çow program operators, and provided empirical
data that shows that this approach can be effective and
efÔ¨Åcient at providing evidence that properties of operators do
or do not hold. By assisting with the veriÔ¨Åcation of operator
properties, our approach can help software engineers create
more dependable operators and dataÔ¨Çow programs.
We have focused on testing individual SPL operator in-
stances. In follow-up work, we adapted our approach to
MapReduce, and applied it to characterize semantic properties
of MapReduce workloads [25]. As future work, we intend to
consider the next stages of dataÔ¨Çow program validation, in
which testing approaches analogous to integration and system
testing are needed. We also plan to expand on the scope of
our empirical study, and to examine the potential usefulness
of the methodology in the hands of engineers.
ACKNOWLEDGEMENTS
This work was supported in part by the AFOSR through
award FA9550-09-1-0129 to the University of Nebraska - Lin-
coln. Portions of this work were performed by the Ô¨Årst author
during the course of an internship at IBM Research. We thank
Gabriela Jacques-Silva, Andy Frenkiel, and Scott Schneider
for inspiring discussions of dataÔ¨Çow operator properties.112REFERENCES
[1] Daniel J. Abadi, Yanif Ahmad, Magdalena Balazinska, U Àágur Cetintemel,
Mitch Cherniack, Jeong-Hyon Hwang, Wolfgang Lindner, Anurag S.
Maskey, Alexander Rasin, Esther Ryvkina, Nesime Tatbul, Ying Xing,
and Stan Zdonik. The design of the Borealis stream processing engine.
InConference on Innovative Data Systems Research (CIDR) , pages 277‚Äì
289, 2005.
[2] Craig Chambers, Ashish Raniwala, Frances Perry, Stephen Adams,
Robert R. Henry, Robert Bradshaw, and Nathan Weizenbaum. Flume-
Java: easy, efÔ¨Åcient data-parallel pipelines. In Conference on Program-
ming Languages Design and Implementation (PLDI) , pages 363‚Äì375,
2010.
[3] Kumar Chellapilla. Combining mutation operators in evolutionary
programming. IEEE Transactions on Evolutionary Computation (TEC) ,
2(3):91‚Äì96, September 1998.
[4] David M. Cohen, Siddhartha R. Dalal, Jesse Parelius, and Gardner C.
Patton. The combinatorial design approach to automatic test generation.
IEEE Software , 13(5):83‚Äì88, September 1996.
[5] Jeffrey Dean and Sanjay Ghemawat. MapReduce: SimpliÔ¨Åed data
processing on large clusters. In Symposium on Operating Systems Design
and Implementation (OSDI) , pages 137‚Äì150, 2004.
[6] Joe W. Duran and Simeon Ntafos. A report on random testing. In
International Conference on Software Engineering (ICSE) , pages 179‚Äì
183, 1981.
[7] Alex Groce, Chaoqiang Zhang, Eric Eide, Yang Chen, and John Regehr.
Swarm testing. In International Symposium on Software Testing and
Analysis (ISSTA) , pages 78‚Äì88, 2012.
[8] Martin Hirzel. Partition and compose: Parallel complex event processing.
InConference on Distributed Event-Based Systems (DEBS) , pages 191‚Äì
200, 2012.
[9] Martin Hirzel, Henrique Andrade, Bu Àògra Gedik, Gabriela Jacques-Silva,
Rohit Khandekar, Vibhore Kumar, Mark Mendell, Howard Nasgaard,
Scott Schneider, Robert Soul ¬¥e, and Kun-Lung Wu. IBM Streams
Processing Language: Analyzing big data in motion. IBM Journal of
Research and Development (IBMRD) , 57(3/4):7:1‚Äì7:11, 2013.
[10] Fabian Hueske, Mathias Peters, Matthias J. Sax, Astrid Rheinl ¬®ander,
Rico Bergmann, Aljoscha Krettek, and Kostas Tzoumas. Opening the
black boxes in data Ô¨Çow optimization. In Conference on Very Large
Databases (VLDB) , pages 1256‚Äì1267, 2012.
[11] IBM Infosphere Streams. http://www.ibm.com/software/data/infosphere/
streams/.
[12] K. R. Jayaram and Patrick Eugster. Program analysis for event-based
distributed systems. In Conference on Distributed Event-Based Systems
(DEBS) , pages 113‚Äì124, 2011.
[13] Westley M. Johnston, J. R. Paul Hanna, and Richard J. Millar. Advances
in dataÔ¨Çow programming languages. ACM Computing Surveys , 36(1):1‚Äì
34, 2004.[14] E.A. Lee and D.G. Messerschmitt. Synchronous data Ô¨Çow. Proceedings
of the IEEE , 75(9):1235‚Äì1245, 1987.
[15] Peng Li, Kunal Agrawal, Jeremy Buhler, and Roger D. Chamberlain.
Deadlock avoidance for streaming computations with Ô¨Åltering. In
Symposium on Parallelism in Algorithms and Architectures (SPAA) ,
pages 243‚Äì252, 2010.
[16] Mark Mendell, Howard Nasgaard, Eric Bouillet, Martin Hirzel, and
BuÀògra Gedik. Extending a general-purpose streaming system for XML.
InConference on Extending Database Technology (EDBT) , pages 534‚Äì
539, 2012.
[17] Christopher Olston, Shubham Chopra, and Utkarsh Srivastava. Gener-
ating example data for dataÔ¨Çow programs. In International Conference
on Management of Data (SIGMOD) , pages 245‚Äì256, 2009.
[18] Easwaran Raman, Guilherme Ottoni, Arun Raman, Matthew J. Bridges,
and David I. August. Parallel-stage decoupled software pipelining. In
Symposium on Code Generation and Optimization (CGO) , pages 114‚Äì
123, 2008.
[19] Martin C. Rinard and Pedro C. Diniz. Commutativity analysis: A
new analysis framework for parallelizing compilers. In Conference on
Programming Languages Design and Implementation (PLDI) , pages 54‚Äì
67, 1996.
[20] Scott Schneider, Martin Hirzel, Bu Àògra Gedik, and Kun-Lung Wu. Auto-
parallelizing stateful distributed streaming applications. In Conference
on Parallel Architectures and Compilation Techniques (PACT) , 2012.
[21] Robert Soul ¬¥e, Martin Hirzel, Bu Àògra Gedik, and Robert Grimm. From
a calculus to an execution environment for stream processing. In
Conference on Distributed Event-Based Systems (DEBS) , pages 20‚Äì31,
2012.
[22] Robert Soul ¬¥e, Martin Hirzel, Robert Grimm, Bu Àògra Gedik, Henrique
Andrade, Vibhore Kumar, and Kun-Lung Wu. A universal calculus for
stream processing languages. In European Symposium on Programming
(ESOP) , pages 507‚Äì528, 2010.
[23] Robert Stephens. A survey of stream processing. Acta Informatica ,
34(7):491‚Äì541, 1997.
[24] William Thies and Saman Amarasinghe. An empirical characterization
of stream programs and its implications for language and compiler
design. In Conference on Parallel Architectures and Compilation
Techniques (PACT) , pages 365‚Äì376, 2010.
[25] Zhihong Xu, Martin Hirzel, and Gregg Rothermel. Semantic character-
ization of mapreduce workloads. In Proceedings of the International
Symposium on Workload Characterization (IISWC) , 2013.
[26] Yuan Yu, Pradeep Kumar Gunda, and Michael Isard. Distributed
aggregation for data-parallel computing: Interfaces and implementations.
InSymposium on Operating Systems Principles (SOSP) , pages 247‚Äì260,
2009.113