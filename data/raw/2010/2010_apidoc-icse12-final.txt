Synthesizing API Usage Examples
Raymond P.L. Buse and Westley Weimer
Department of Computer Science
University of Virginia, Charlottesville, VA, USA
fbuse,weimerg@cs.virginia.edu
Abstract ‚ÄîKey program interfaces are sometimes docu-
mented with usage examples: concrete code snippets that
characterize common use cases for a particular data type.
While such documentation is known to be of great utility, it is
burdensome to create and can be incomplete, out of date, or
not representative of actual practice.
We present an automatic technique for mining and synthesiz-
ing succinct and representative human-readable documentation
of program interfaces. Our algorithm is based on a combination
of path sensitive dataÔ¨Çow analysis, clustering, and pattern
abstraction. It produces output in the form of well-typed
program snippets which document initialization, method calls,
assignments, looping constructs, and exception handling. In
a human study involving over 150 participants, 82% of our
generated examples were found to be at least as good at human-
written instances and 94% were strictly preferred to state of
the art code search.
I. I NTRODUCTION
Professional software developers spend most of their time
trying to understand code [1], [2]. Maintaining and evolving
high-quality documentation is crucial to help developers
understand and modify code [3], [4]. In reports by and
studies of developers, API use examples have been found
to be a key learning resource [5], [6], [7], [8], [9], [10].
That is, documenting how to usean API is preferable to
simply documenting the function of each of its components.
One study found that the greatest obstacle to learning an
API in practice is ‚ÄúinsufÔ¨Åcient or inadequate examples.‚Äù [11]
We present an algorithm that automatically generates API
usage examples. Given a data-type and software corpus (i.e.,
a library of programs that make use of the data-type), we
extract abstract use-models for the data-type and render them
in a form suitable for use by humans as documentation.
The state of the art in automated support for usage
examples in known as code search . Typically, the problem is
phrased as one of ranking concrete code snippets on criteria
such as ‚Äúrepresentativeness‚Äù and ‚Äúconciseness.‚Äù In 2009,
Zhong et al. described a technique called MAPO for min-
ing and recommending example code snippets [12]. More
recently, Kim et al. presented a tool called EXOADOCS
which also Ô¨Ånds and ranks code examples for the purpose
of supplementing J AVADOCembedded examples [13]. Such
examples can be useful, but they are very different from
human-written examples. Mined examples often contain
extraneous statements, even when slicing is employed. In
addition, they often lack the context required to explicate thematerial they present. In general, mined examples are long,
complex, and difÔ¨Åcult to understand and use. Good human-
written examples, on the other hand, often present only the
information needed to understand the API and are free of
superÔ¨Çuous context. Human written documentation has two
important disadvantages, however: it requires a signiÔ¨Åcant
human effort to create, and is thus often not created; and it
may not be representative of, or up-to-date with, actual use.
In this paper we present a technique for automatically
synthesizing human-readable API usage examples which are
well-typed and representative. We adapt techniques from
speciÔ¨Åcation mining [14] to model API uses as graphs de-
scribing method call sequences, annotated with control Ô¨Çow
information. We use data-Ô¨Çow analysis to extract important
details about each use beyond the sequence of method calls,
such as how the type was initialized and how return values
are used. We then abstract concrete uses into high-level
examples. Because a single data-type may have multiple
common use scenarios, we use clustering to discover and
coalesce related usage patterns before expressing them as
documentation.
Our generated examples display a number of important
advantages over both state-of-the-art code search and human
written examples, both of which we compare to in a human
study. Unlike mined examples, our generated examples con-
tain only the program statements needed to demonstrate the
target behavior. Where concrete examples can be needlessly
speciÔ¨Åc, our examples adopt the most common types and
names for identiÔ¨Åers. Unlike human-written examples, our
examples are, by construction, well-formed syntactically
and well-typed. Where previous approaches to code ranking
adopted simple heuristics based on length and a simple use
count (e.g., [15], [16]), our abstract examples are structured
and generated with a robust and well-deÔ¨Åned notion of rep-
resentativeness. Because our approach is fully automatic, the
examples are also cheap to construct and can be always up-
to-date. Additionally, their well-formedness properties make
them ideal automated tasks like for code completion [17].
The main contributions of this paper are:
A study of API usage examples. We characterize gold-
standard human-written examples from the Java SDK.
Furthermore, we present results from a large survey on
example quality and utility.
An algorithm for the automatic construction of exampledocumentation. The algorithm takes as input a software
corpus and a target data-type. It produces a ranked
list of well-typed, human-readable code snippets which
exemplify typical use of that data-type.
A prototype implementation of the algorithm, and a
comparison of its output to human-written example
documentations. A human study with over 150 partici-
pants suggests our tool could replace as many as 82%
of human-generated examples.
II. M OTIVATING EXAMPLE
In modern software development, API documentation
tools such as J AVADOChave become increasingly prevalent,
and variants exist for most languages (e.g., P YTHON DOC,
OCAML DOC, etc.). One of the principles of J AVADOCis
‚Äúincluding examples for developers‚Äù [18]. Not all examples
are created equal, however. Features such as conciseness,
representativeness, well-chosen variable names, correct con-
trol Ô¨Çow, and abstraction all relate to documentation quality.
Consider Java‚Äôs BufferedReader class, which provides
a buffering wrapper around a lower-level, non-buffered
stream. The human-written usage example included in the
ofÔ¨Åcial Java Development Kit, version 6 [19] is:
BufferedReader in =
new BufferedReader(new FileReader("foo.in"));
While this example has the merit of being concise, it shows
only how to create a BufferedReader , not how to use one.
By contrast, our algorithm produces:
FileReader f; //initialized previously
BufferedReader br = new BufferedReader(f);
while(br.ready()) {
String line = br.readLine();
//do something with line
}
br.close();
This exempliÔ¨Åes one common usage pattern for a
BufferedReader : repeatedly calling its readLine method
while it remains ready . The variable names brandfwere
selected from among the most common human choices for
BufferedReader andFileReader , and were synthesized
together here: no single usage example need exist that uses
both of those names in tandem. In addition, the example also
demonstrates the importance of control Ô¨Çow: readLine is
called repeatedly, but only after checking ready . Finally,
the//initialized previously and//do something
with line comments indicate points where different hu-
man developers would write different code and highlight
the most direct places for a developer to adapt this code
example into an existing setting.
In practice, there is more than one way to use a
BufferedReader . Our algorithm can produce a ranked
list of examples based on clusters of representative human
usages. The second example we produce is:InputStreamReader i; //initialized previously
BufferedReader reader = new BufferedReader(i);
String s;
while ((s = reader.readLine()) != null)
//do something with s
}
reader.close();
This second example shows that other concrete argu-
ment types can be used to create a BufferedReader
(e.g., a InputStreamReader can be used as well as a
FileReader ). In addition, it shows that there is a different
usage pattern that involves always calling readLine but
then checking the return value against null (rather than
calling ready ). Both of the examples produced by our
algorithm are well-formed and introduce commonly-named,
well-typed temporaries for function arguments and return
values.
It is also possible to use code search and slicing tech-
niques to produce API examples. Such a tool from Kim et
al.[13] produces an output consisting of more than 14 lines
on the same BufferedReader query (not shown).
Because they lack information related program seman-
tics, slicing based approaches have trouble distinguishing
between relevant an irrelevant details in an example. In the
output of Kim et al. ‚Äôs tool, a BufferedReader is initialized
with System.in . To a new user, it may be difÔ¨Åcult to tell if
this argument is necessary, or as in this case, coincidental.
Variable names are also often too speciÔ¨Åc to the example:
String acl_in = br.readLine() (Kim et al. ‚Äôs tool) is
less descriptive than String line = br. readLine()
for the general case. Furthermore, sliced examples do not
type-check out of context and include many irrelevant state-
ments.
We seek to produce high-quality usage example documen-
tation automatically. To do so we must Ô¨Årst understand how
humans write usage examples and what they look for in
usage documentation.
III. H UMAN -WRITTEN USAGE EXAMPLES
In this section we present a study of human-written API
examples. The purpose of this study is to establish key
guidelines for automatically creating examples that share
the best properties of human-written ones. We seek to
determine the desired size and readability of examples, and
the importance of generality and correctness. We answer
such questions by analyzing examples found in the standard
Java API docs and through the results of a brief survey on
the importance of various characteristics of examples.
A. Properties of Human-Written Examples
We explore the Java Software Development Kit (SDK)
documentation because it is authoritative, generally consid-
ered to be of high quality, and is written for a general
audience of Java developers. Furthermore, each example isFigure 1. Histogram of the sizes (in lines) of usage examples
embedded in the standard Java API documents.
tied to a speciÔ¨Åc class (and not, for example, to a coding
task or other request).
We identiÔ¨Åed examples in the Java SDK by searching for
pre-formatted text (e.g., contained in <pre> or<code> tags)
and found 234 instances. We then manually inspected each
one, selecting only those consisting of Java statements (e.g.,
ignoring lists of methods and other text). In all, we found
47 classes (3% of the total) which contain a usage example.
Length. We Ô¨Årst consider the sizes of examples measured
in lines of code. The full distribution of example sizes is
presented in Figure 1. On average, human-written examples
were 11 lines long, but the median size was 5 lines. While
a signiÔ¨Åcant number of examples are quite long, we note
that human-written examples are typically very concise. We
hypothesize that automated examples should be of a similar
length to agree with human preferences and make a suitable
supplement for existing examples.
Abstract initialization. We also note that many human-
written examples use special markers, such as ellipses, to
indicate that an input variable should be assigned a context-
speciÔ¨Åc value. For example, the glyphIndex variable in
this documentation for java.awt.font.GlyphMetrics :
int glyphIndex = ...;
GlyphMetrics metrics =
GlyphVector.getGlyphMetrics(glyphIndex);
int isStandard = metrics.isStandard();
float glyphAdvance = metrics.getAdvance();
Similarly, an example from text.MessageFormat uses
variables initialized with 7and‚Äò‚Äòa disturbance in the
Force‚Äô‚Äô , which are clearly chosen arbitrarily.
int planet = 7;
String event = "a disturbance in the Force";
String result = MessageFormat.format(
"At {1,time} on {1,date}, there was {2}" +
"on planet {0,number,integer}.",
planet, new Date(), event);
We hypothesize that automatically-constructed documen-
tation should also use such abstract initialization to highlight
‚Äúinputs‚Äù to the examples.We hypothesize that generated documentation should
employ common concrete type and variable names where
applicable, but only when they represent truly frequent usage
patterns. The algorithm presented in this paper annotates all
variable which should be initialized with additional context
with an //initialized previously comment.
Abstract use. Many examples contained an abstract
placeholder indicating where the user should insert context-
speciÔ¨Åc usage code. For example, the documentation for
java.text.CharacterIterator makes it clear that the
user should ‚Äúdo something with‚Äù or otherwise process the
character cinside the loop:
for(char c = iter.first();
c != CharacterIterator.DONE;
c = iter.next()) {
processChar(c);
}
We hypothesize that synthesized examples must similarly
and concisely indicate where context-speciÔ¨Åc user code
should be placed and what variables it should manipu-
late. The algorithm presented in this paper employs //do
something with X annotations in such cases.
Exception Handling. 16 of the 47 examples contained
some exception handling. Reasoning about programs that
use exceptions is difÔ¨Åcult for humans and also for automatic
tools and analyses (e.g., [20], [21], [22]). Often exceptions
are caught trivially (i.e., no action is taken to resolve
the underlying error [23]) or the mechanism is purposely
circumvented [24], [25]. This example for java.nio.
file.FileVisitor is indicative:
try {
file.delete();
} catch (IOException exc) {
// failed to delete, do error handling here
}
return FileVisitResult.CONTINUE;
We hypothesize that tool-generated documentation should
mention exception handling, but only when it is common
or necessary for correctness. The algorithm presented in
this paper learns common exception handling patterns in
the corpus and distills examples representative of exception
handling practice.
B. Survey Results
As part of the evaluation of the tool presented in this
paper, we conducted a human study involving over 150
participants, primarily undergraduates at The University of
Virginia enrolled in a class entitled Software Development
Methods which places a heavy focus on learning the Java
language and its standard set of APIs. Details about the study
can be found in Section VII-A. As part of the study, humans
were asked ‚ÄúWhat factors are important in a good example?‚Äù
Some common themes emerged:Multiple uses. Users wanted examples of different ways
to use the class: ‚ÄúIt shouldn‚Äôt model something extremely
speciÔ¨Åc, it should include something that the class will be
most commonly used for.‚Äù Documentation ‚Äúmust be able
to show multiple uses. If you just show one example of a
possible use, it probably won‚Äôt be the one that interests me.‚Äù
We should ‚Äúshow examples of different ways a class can be
used.‚Äù The best documentation shows ‚Äúall the different ways
to use something, so it‚Äôs helpful in all cases.‚Äù Our algorithm
uses clustering to capture multiple uses.
Readability. Users wanted examples that were easy to
read and understand: ‚Äúa good example is easy to understand
and read.‚Äù Many were explicit: ‚Äúreadable and understandable
are the most important aspects.‚Äù Slicing away unrelated code
was critical: ‚Äúless irrelevant, unrelated stuff in the example
is better.‚Äù A key component of readability is conciseness:
the example should use ‚Äúas little code as possible‚Äù and
show ‚Äúthe most basic version of the problem. You can
have additional more complicated problems if necessary.‚Äù In
Section VI-B we empirically demonstrate that our algorithm
produces readable examples.
Naming. One key aspect of readability was the choice
of identiÔ¨Åer names: ‚Äúthey should be simple and understand-
able.‚Äù Users preferred ‚Äúlogical variable names‚Äù, ‚Äúdeclaring
variable names to represent what they do/are‚Äù, ‚Äúclear naming
of variables‚Äù, and variables demonstrating ‚Äústandard nam-
ing‚Äù. The importance of identiÔ¨Åer names has been previously
studied (e.g., [26]). Our algorithm tracks common variable
naming information from concrete source code to produce
understandable variable names.
Variables. Users also prefer documentation that includes
intermediate variables and temporaries: ‚Äúshowing declara-
tions‚Äù and ‚Äúthe use of many temporary variables helps‚Äù to
make good documentation. The human-written documenta-
tion in Section II elides temporary variables; by contrast our
algorithm produces well-typed, clearly-named temporaries.
The study also included a set of Likert-scale questions
concerning the importance of several aspects of usage ex-
amples. The properties were size (‚ÄúHow concise is the
example?‚Äù), readability (‚ÄúHow easy is it to understand?‚Äù),
representativeness (‚ÄúWill it be useful for what I want to
do?‚Äù), and concreteness (‚ÄúCan I compile and use it?‚Äù).
Figure 2 shows the results. Notably, readability was the
most important of the four features, with almost every
participant ranking it as ‚Äúvery important‚Äù or ‚Äúimportant.‚Äù
Note that readability is judged even more important than
representativeness, which as been traditionally considered
most important (e.g., [13]). Representativeness and size
were the next most important, and concreteness was least
important among the four. However, all four features were
at least ‚Äúimportant‚Äù on average, and must thus inform the
design of our automatic example documentation algorithm.
IV. A LGORITHM DESCRIPTION
Figure 2. Human survey responses about the importance of various
features of example documentation.
This section describes our documentation-generation al-
gorithm. The algorithm is designed to produce documen-
tation with the qualities found in human-written examples
(Section III-A) and the qualities praised by humans in our
survey (Section III-B).
Figure 3 formalizes our algorithm in pseudo code and
is referenced throughout this section. Our algorithm has
four key phases. In Path Enumeration (Section IV-A) we
statically enumerate intra-procedural traces. In predicate
generation (Section IV-B), paths are merged into a smaller
number of concrete uses . Conceptually, each concrete use
corresponds a single (static) instantiation of the target type.
The third step applies a clustering algorithm to identify
groups of concrete uses that are similar (Section IV-C).
These clusters are then merged into a small set of abstract
uses which intuitively correspond to the ways the class is
used in the corpus. Finally, we sort the abstract uses by
their representativeness, Ô¨Çatten them into a ‚Äúbest method or-
dering‚Äù and distill them into human-readable documentation
(Section IV-D).
A. Path Enumeration
For efÔ¨Åciency, we Ô¨Årst Ô¨Ålter the entire corpus, selecting
only Ô¨Åles that include a reference to the target class. We then
process each method in each of the remaining classes of
the corpus in turn. Note, however, that example generation
does not require complete coverage of a corpus. Once a
sufÔ¨Åciently large number of examples are found such that
the model becomes stable the process can terminate.
We Ô¨Årst enumerate the loop-free control Ô¨Çow paths of
each method (Figure 3 lines 1‚Äì4): our analysis is thus path-
sensitive and potentially exponential. We obtain loop-free
paths by adding a statement to a path at most once: in effect,
we consider each loop to either be taken once, or not at
all. This decision can occasionally result in imprecise or
incorrect documentation, however we see in Section VII that
this occurs infrequently in practice.Input: Target class T.
Input: Software corpus Corp .
Input: Distance metric on concrete uses Dist .
Input: Clustering parameter k.
Input: Least-upper-bound operator on statement set t.
Input: Comparison operator on statement lattice v.
‚Äî Path Enumeration ‚Äî
1:letpaths ;
2:for all methodminCorp do
3: ifmreferencesTthen
4: paths paths[EnumerateAcyclicPaths (m)
‚Äî Symbexe & Use seeds ‚Äî
5:letseeds ;
6:letsymexe paths ;
7:for all corpus paths path inpaths do
8: letsymexe path [ ]
9: for all statementsstmt inpath in order do
10:hpathpreds;sym regi Symbexe (path;stmt )
11: for allhp;qi2symregdo
12:symexestmt stmt [p7!q]
13: symexe path 
symexe path +hsymexestmt; path preds; stmti
14: for all sub-expressions einstmt do
15: ifematches new S_(ematches obj:d^
typeof (d) =T)_(eis a function parameter
^typeof (e) =T)then
16: seeds seeds[feg
17: symexe paths symexe paths[fsymexepathg
‚Äî Concrete Uses ‚Äî
18:letconcrete uses ;
19:for all expressionsseed inseeds do
20: letseed paths ;
21: for all pathspinsymexepaths do
22: letsliced path 
fsjs2p^seed is a sub-expression of sg
23: seed paths seed paths[fslicedpathg
24: letnodes fsj 9p2seed paths: s2pg
25: letedgew(s1;s2) 
jfpjp2seed paths^p=<:::s 1:::s 2>)gj
26: concrete uses concrete uses[fhnodes;edge wig
27:for allc2concrete uses do
28: NormalizeEdgeWeights (c)
‚Äî Abstract Uses ‚Äî
29:letclusters KMedoids (k;concrete uses; Dist)
30:for all clustersC2clusters do
31: letnodes =tfabstj9hN;Ei2C : abst2Ng
32: for allhabst 1;abst 2i2nodes do
33:abstedgew(abst 1;abst 2) 0
34: for allhcon 1;con 2i2N :hN;Ei2Cdo
35: ifcon 1vabst 1^con 2vabst 2then
36: abstedgew(abst 1;abst 2) +=E(con 1;con 2)
37: letordered argmax
s1:::sn2nodes !Flow (s1:::sn;abst edge w)
38: Output: GenerateCode (ordered )
Figure 3. High-level pseudo code of our algorithm.
B. Predicate Generation
Next, we use symbolic execution [27] to compute in-
traprocedural path predicates , logical formulae that describe
conditions under which each statement can be reached [28],
[29] (Figure 3 line 10). To obtain these formulae, each con-
trol Ô¨Çow path is symbolically executed. We track conditionalstatements (e.g., ifandwhile ) and evaluate their guarding
predicates using the current symbolic values for variables.
We collect the resulting predicates; in conjunction, they form
the path predicate for a given statement in the method.
In addition to the statement itself and its path predi-
cates, we also record a version of the statement where
each subexpression is replaced by the current value of
that expression from the symbolic register Ô¨Åle (lines
11‚Äì13). For example, x.add(s) might becomefnew
LinkedList()g.add("Hello") .
Next we identify use seeds [14], expressions which rep-
resent a static instantiation of the target type: new object
allocations, Ô¨Åeld references, and function parameters (lines
15‚Äì16). Intuitively, each such expression corresponds to one
use of the class.
Seeds are the basis for the formation of concrete uses
which join together all statements that are relevant to the
object instantiation represented by the seed (lines 22‚Äì23).
Concrete uses capture method ordering in a graph data struc-
ture where edges express the happens before relationship
(cf. many speciÔ¨Åcation mining techniques [30], [31], [32]
where the edges represent method calls and an accepting
sequence corresponds to a valid ordering) (lines 24‚Äì26). Em-
ploying class Ô¨Åelds as seeds is of particular importance for
Java, where method invocations on the same dynamically-
allocated object often occur in multiple procedures. For
example, a HashTable may be allocated in a constructor,
populated in a second method and queried in a third.
Although no happens before relation can be established
between statements in separate methods in a purely intra-
procedural analysis (except that constructors must always be
called Ô¨Årst), this permits the ordering between invocations
located in the same method to be preserved. We have found
this is critical to retaining sufÔ¨Åcient ordering information to
emit representative documentation.
C. Clustering and Abstraction
Even for a single target class, there are often many
representative usage patterns to be found within a software
corpus. We desire a ranked list of such patterns. We thus
cluster the concrete uses from the previous step into ab-
stract uses , abstractions of concrete uses where nodes are
abstractions of multiple statements.
Intuitively, two concrete uses are similar and should be
viewed as examples of the same abstract use pattern if they
perform the same operations on the same data-types in the
same order. We thus propose a formal distance metric be-
tween concrete uses which captures both statement ordering
from the underlying state machines and type information.
Our distance metric is similar to Kendall‚Äôs [33], and cap-
tures both the number of sorting operations and number of
type substitutions needed to transform one concrete use into
another (see below). Our distance function is parametrized
byandwhich express the relative importance of method
ordering and type information. Our experience generatingdocumentation for popular Java classes suggests that output
quality is not highly sensitive to choice of and. However,
in languages other than Java, particularly those with weak or
dynamic type systems, selection of these parameters could
be important. For all experiments in this paper, we set
== 1, giving types and ordering equal weights.
In the following explanation, Ciis a concrete use: a
set of statements admitting a happens before relation (see
Section IV-B). The selector function Kdetermines if two
concrete instances include two statements in the same order.
The selector function Tdetermines if two concrete instances
use the same types in all relevant subexpressions of two
statements.
Dist(C1;C2) =X
fi;jg2(C1[C2)Ki;j(C1;C2) +Ti;j(C1;C2)
Ki;j(C1;C2) =(1 ifiandjarenotin the
same order in C1andC2
0 otherwise
Ti;j(C1;C2) =(1 ifi=jandiandjuse
different types in C1andC2
0 otherwise
Given this distance metric, we use the k-medoids al-
gorithm [34] to cluster concrete uses (line 29). Because
our distance metric must necessarily be computed between
concrete objects, traditional k-means cannot be used: the
k-means algorithm requires that distance can be computed
between objects and also between arbitrary points in the
metric space.
K-medoids takes a parameter: k, the number of clusters
to Ô¨Ånd.krepresents the inherent trade-off between precision
and generality that exists with any documentation system.
In our setting, a large value of kresults in the algorithm
returning multiple examples of the same basic usage pattern
(e.g., the two uses of BufferedReader in Section II). If
kis too small, however, it is possible that multiple usage
patterns could be conÔ¨Çated. An overly large kcan result is
less representative examples, especially in terms of identiÔ¨Åer
names. All of the experiments in this paper use k= 5, since
our initial study (see Section III-A) found no classes with
more than four common patterns.
While previous approaches have applied clustering to
code search (e.g., [12], [13]), to the best of our knowledge
our approach is the Ô¨Årst to leverage type information and
statement ordering. We believe that these properties are
essential to documenting many usage patterns (e.g., the best
choice of data structure often depends on the usage pattern).
After clusters are discovered, the associated concrete uses
are merged into abstract uses which are also represented as
graphs. To merge a set of concrete uses, we Ô¨Årst union all of
the states (i.e., nodes) contained in each concrete use (line
31). The merging step is conceptually akin to the least-upper-
bound lattice computation in constant-propagation dataÔ¨Çow
analysis: 5t5 = 5 , butxty=SOMETHING .For example:
A=
stmt :print (x)
pred :x:isTrue ()
B=
stmt :return x
pred :x:isTrue ()
AtB=
stmt :Do something with x
pred :x:isTrue ()
Transitions (i.e., edges) between statements in the abstract
use are then constructed and weighted according to the
number of concrete statements which include the transition
(lines 32‚Äì36). Information about the types and names of
parameters and return values is retained.
D. Emitting Documentation
The Ô¨Ånal stage of our algorithm transforms each abstract
use into a representative, well-formed, and well-typed Java
code fragment. Recall that abstract uses are graphs with edge
weights that correspond to usage counts in the corpus (e.g.,
an edge with weight 15connectingSandTindicates that
there were 15concrete uses which observed the ordering S
happens before T).
An abstract use can contain both branches and cycles.
Usage documentation, however, must be presented as linear
text. Our next step is thus to Ô¨Ånd a representative ordering
of the statements (i.e., a partial serialization) in each abstract
use. We choose to approach this problem using a weighted
topological sort. The goal is to Ô¨Ånd an ordering of state ma-
chine nodes (from start node to end node) which maximizes
the sum of the weight of all outgoing edges minus the sum
of the weight of all incoming edges. Such an ordering is
optimal in the sense that any other ordering will be less
representative of the concrete orderings in the corpus (i.e.,
more total statements must be swapped to align all of the
concrete orderings with this this ordering).
More formally, we take as input set of statements Sand
functionW, representing edge weights, which maps all pairs
of statements in Sto a value greater than or equal to 0. The
task is to Ô¨Ånd a total ordering O, deÔ¨Åned here as a function
mapping a pair of unique statements (si;sj)to1if and only
ifsicomes before sjinOand to 1otherwise. The notion
of representativeness can then be formalized by the Flow
objective function below.
Input: Statement set S:fs1;s2;:::s ng
Input: Edge weight mapping W: (si;sj)!f0:::1g
Output: OrderingO: (si;sj)!f  1;1gmaximizing:
Flow =X
fsi;sjg2SO(si;sj)W(si;sj)
Our algorithm Ô¨Ånds such an ordering by enumerating
all possible orderings and computing Flow , returning the
ordering for which Flow is largest (line 37). Note that the
time complexity of this algorithm is factorial in the number
of statements to be documented. However, this remainstractable for n< 12and we have not encountered instances
withn> 8. We implemented an unoptimized version of the
algorithm and found that it was not a performance bottleneck
in practice (see Section VI).
Once a statement ordering has been determined, the next
step is to generate code (i.e., usage example documenta-
tion) from those ordered sets of statements (line 38). We
desire code that is representative of the underlying concrete
statements which were clustered into this abstract use. We
Ô¨Årst generate a basic block for each statement, guarded by
its most common predicate (taken over the concrete uses
forming this abstract use cluster). Statements are printed
using standard Java syntax; names and types are assigned to
all return values and parameters as needed. Adjacent blocks
with the same predicate are merged (e.g., ‚Äú if (p) X; if
(p) Y; ‚Äù becomes ‚Äú if (p)fX; Y;g‚Äù).
When choosing a name, we Ô¨Årst check if any one name
is usedX%more often than all other names by human-
written code in the corpus. In our experiments we set
Xto100% , thus selecting a name if it occurs twice as
often as any other name. If no such popular name exists,
we use the declared formal parameter name for actual
arguments and the Ô¨Årst letter of the type name for other
variables. For statements that are not invocations of the
target class, we print ‚Äú //do something with Y ‚Äù where
Yis the appropriate identiÔ¨Åer. Types are chosen in the
same manner, making use of declared return types for
function return values. Declarations are inserted and marked
with ‚Äú //initialized previously ‚Äù for all otherwise-
undeÔ¨Åned (‚Äútemporary‚Äù) variables.
For each statement, we also count how often it was
found inside a try,catch , or finally clause, and the
type of the associated exception. If a statement is part of
exception handling more often than not in the corpus, we
then impose similar error handling in the generated example.
In Java, well-formed exception handling involves a try
block followed immediately by zero or more catch clauses
and possibly a finally clause, with at least one catch or
finally clause. All exception handling generated by our
algorithm is of this form; we will not to generate a catch
clause if there is no preceding try.
This construction ensures that no Java syntax rules are
violated. Furthermore, we guarantee that all assignments are
made to an identiÔ¨Åer of the proper type and that all methods
are invoked with the right number and types of arguments.
It is important to note, however, that we cannot be sure that
generated examples are free of other types of errors (e.g.,
run-time errors, unchecked exceptions, etc.).
The representativeness of our examples is well deÔ¨Åned:
if the order of any two statements were inverted (or if
some statement were added or removed), then the resulting
example would correspond to fewer real-word examples
from the corpus. We are not aware of any previous approach
to documenting APIs which offers such guarantees. Other
correctness properties are, to some degree, orthogonal. ForTable I
CORPUS FOR EXAMPLE GENERATION IN THIS STUDY .
Name Version Domain kLOC
FindBugs 1.2.1 Code Analysis 154
FreeCol 0.7.2 Game 91
hsqldb 1.8.0 Database 128
iText 2.0.8 PDF utility 145
jEdit 4.2 Word Processing 123
jFreeChart 1.0.6 Data Presenting 170
tvBrowser 2.5.3 TV Guide 138
Weka 3.5.6 Machine Learning 402
XMLUnit 1.1 Unit Testing 10
total 1361
example, we could compile our examples and run them
to check for run-time exceptions, or perhaps use model
checking to verify temporal safety properties. While the
topics are related, we are not proposing a static speciÔ¨Åcation
mining technique (where correctness is paramount); rather,
we are proposing a common-usage documentation synthesis
technique which could leverage work in speciÔ¨Åcation min-
ing. We view common behavior in the corpus as correct by
deÔ¨Ånition for this documentation task.
V. I NDICATIVE EXAMPLES
In this section we present a number of examples indicative
of the strengths and limits of our approach. We constructed
these examples using the program corpus enumerated in
Table I, which includes nine popular open-source Java pro-
grams, as input to our prototype example generation tool. We
issued example queries for each of the 47 SDK classes for
which human-written example documentation was available
(see Section III-A for details). Thirty-Ô¨Åve of the classes were
used at least once in our corpus and thus produced example
documentation. After examining these examples in detail we
performed a formal empirical evaluation and human study
in Section VI and Section VII.
A. Naming
The names of types and variables are critical to pro-
ducing readable, representative examples. The output of
our tool can be sensitive to the types and variables used
in the available corpus. Consider the following example
forjava.util.Iterator , which demonstrates how an
Iterator can be used to enumerate arbitrary objects.
Iterator iter = SOMETHING.iterator();
while(iter.hasNext()) {
Object o = iter.next();
//do something with o
}
The same query, conducted using only the FreeCol
benchmark, results in a very different example. Because
use of the EventIterator class, which is a subclass
ofjava.util.Iterator , is common in FreeCol , it is
chosen as the most representative example.
EventIterator eventIterator =
SOMETHING.eventIterator();if(eventIterator.hasNext()) {
Event e = eventIterator.nextEvent();
//do something with e
}
We hypothesize that both behaviors are useful: the former
constructs generic library API documentation while the later
crafts speciÔ¨Åc documentation for internal APIs.
B. Patterns
Our algorithm Ô¨Ånds and preserves common usage pat-
terns: frequently occurring sequences of statements. How-
ever, there are many classes, such as the generic Throwable ,
for which there is no truly common pattern. The example our
algorithms generates for Throwable shows that an instance
might be queried for its Class ,Cause , orStackTrace .
PrintWriter p; //initialized previously
Throwable e = SOMETHING.getThrown();
e.getClass();
e.getCause();
e.printStackTrace(p);
For classes such as this, more traditional API documen-
tation which lists all the methods of a class and their
functionality is clearly important. Furthermore, there are
some patterns that cannot be captured by our algorithm.
Interactions such as message passing patterns between mul-
tiple threads or client-server systems could not be discovered
without additional analyses.
C. Exceptions
Ideally, examples should contain complete and correct
exception handing. However, our tool learns actual exception
handling from real code, which may not be fully correct.
The example below of java.io.ObjectInputStream is
one of a small number of classes for which our top-ranked
example uses exception handling.
BufferedInputStream b;//initialized previously
ObjectInputStream stream =
new ObjectInputStream(b);
try {
Object o = stream.readObject();
//Do something with o
} catch(IOException e) {
} finally {
stream.close();
}
We view the correct use of exceptions and the correct
handling of resources in the presence of exceptions as an
orthogonal problem [23], [24], [25].
VI. E MPIRICAL EVALUATION
In this section we begin the evaluation of our proposed
algorithm and prototype implementation with two quantita-
tive metrics: size and readability. Both of these features are
considered very important by users of documentation (see
Section III-B). Running our prototype tool on 1,361k lines
of code to produce example documentation for 35 classes
Figure 4. Size comparison (in lines of code) of examples generated
by our tool using the corpus from Table I, written by humans, and
mined by EXOADOC. The dataset is described in Section VI.
took 73 minutes (about 2 minutes per class). About 95% of
this time is spent Ô¨Åltering the corpus and enumerating paths.
Throughout our evaluation we compare the output of our
tool to both human-written examples from the Java SDK
and also the EXOADOCtool of Kim et al. [13]. EXOADOC
works by leveraging an existing code search engine to Ô¨Ånd
examples of a class. Kim et al. then employ slicing to extract
‚Äúsemantically relevant‚Äù lines. These examples are then clus-
tered and ranked based on Representativeness ,Conciseness
andCorrectness properties. EXOADOChas been shown to
increase productivity by as much as 67% in a small study.
Our dataset consists of examples from all 35 classes from
standard Java APIs for which we have one example of each
of the three types (see Section V). Because EXOADOCs are
associated with methods rather than classes, we chose the
top example for the most popular method (by static count of
concrete uses in our benchmark set). For our tool, we chose
the top (i.e., most representative) example for each class.
A. Size Evaluation
We compared the size (in lines of code) of each doc-
umentation type. The results are presented in Figure 4.
Examples produced by our tool are slightly longer on
average as compared to human written examples, but they
follow approximately the same long-tail distribution. Much
of the size difference can be attributed to our use of separate
lines for ‚Äúpreviously initialized‚Äù variables, which are often
declared in-line by humans (see example in Section II).
Nonetheless, both human-written examples and our gener-
ated examples are shorter than EXOADOCexamples, which
are often longer than 19 lines. This difference is largely due
to the less-relevant lines present in EXOADOCexamples.
B. Readability Evaluation
Because readability was considered the most important
characteristic of examples, we chose to evaluate the read-
ability of the three documentation types directly. For this
purpose we used an automated software readability met-
ric [35]. This metric, which is based on and agrees withFigure 5. Readability comparison [35] of examples generated by
our tool, written by humans, and mined by EXOADOC. The scale
ranges from 1.0 (most readable) to 0.0 (least). The dataset, which
we breakdown into 5 Java packages, is described in Section VI.
a large set of human judgments, reports Java code snippet
readability on a scale of 0 (least readable) to 1 (most).
Figure 5 presents the results of that analysis. Overall,
examples generated by our tool are about 25% more readable
than human-written examples and over 50% more readable
than examples mined with EXOADOC(t-test signiÔ¨Åcance
level<0:002 in both cases). This difference is consistent
across the major Java package areas to which the docu-
mented classes belong. Additionally, generated documenta-
tion is more consistently readable: the standard deviation in
the readability score of our tool output is about 40% lower
than that of the other two documentation types.
VII. H UMAN STUDY
In this section we present a human study of API example
quality. The goal of this study is to quantify the desirability
of the output of our tool in comparison to both human-
written examples (from the Java SDK) and the state-of-the-
art tool EXOADOCof Kim et al. [13]. The study involved
154 participants evaluating 35 pairs of API examples.
A. Experimental Setup
For each of the 35 classes from the dataset described in
Section VI, the participant is shown the name of the target
class and is randomly shown two of the three documenta-
tion types (i.e., ours, EXOADOC, and human-written). The
participant is then required to make a preference evaluation
by selecting one option from a Ô¨Åve-element Likert scale:
‚ÄúStrong Preference‚Äù for A, ‚ÄúSome Preference‚Äù for A, ‚ÄúNeu-
tral‚Äù, ‚ÄúSome Preference‚Äù for B, ‚ÄúStrong Preference‚Äù for B. If
desired, the participant may also choose to ‚ÄúSkip‚Äù the pair.
The study was advertised to students at The University of
Virginia enrolled in a class entitled Software Development
Methods , which places a heavy focus on learning the Java
language and its standard set of APIs. For comparison, 16
computer science graduate students also participated. 179
students participated in total, however, to help preserve data
integrity, we removed from consideration the results from25 undergraduate students who completed the study in less
than Ô¨Åve minutes. The average time to complete the study
for the remaining 154 participants was 13 minutes.
Participation was voluntary and anonymous. The partic-
ipants were instructed to ‚ÄúPretend that [they] are a pro-
grammer or developer who needs to use, or understand
how to use, the class.‚Äù No additional guidance was given;
participants formed their own opinions about each example.
B. Quantitative Results
In total, 154 participants compared 35 example pairs each,
producing 5,390 distinct judgments. The aggregate results
are presented in Figure 6. Overall, the output of our tool was
judged at least as good as human written examples over 60%
of the time and strictly better than EXOADOCin about 75%
of cases. For 82% of examples, on average either humans
preferred our generated documentation to human-written
examples or had no preference. For 94% of examples, the
output of our tool was preferred to EXOADOCS.
Raw score distributions were very similar between gradu-
ate and undergraduate students. However, taken example by
example, grad students had a 20% reduced preference for
tool-generated examples when compared to human-written
examples. Nonetheless, both grads and undergrads judged
our generated documentation to be at least as good as
gold-standard human-written for over half of the examples.
Compared to EXOADOC, our tool was preferred in almost
all cases by both groups.
Finally, we asked whether ‚Äúa program that automatically
generates examples like these would be useful?‚Äù, and 81%
agreed that it would be either ‚ÄúUseful‚Äù or ‚ÄúVery Useful.‚Äù
VIII. T HREATS TOVALIDITY
Although our experiments suggest that our algorithm
produces examples that are preferred to state-of-the-art code
search techniques and as good as human-written documen-
tation over 80% of the time, our results may not generalize.
First, the benchmarks we selected may not be indicative.
We mitigated this threat by performing our human evaluation
on documentation for standard library classes ‚Äî classes used
in almost every program and by almost every programmer.
It is possible, however, that results for these classes may not
generalize to all APIs. However, the documented classes did
feature a wide range of usage patterns. In addition, the output
of our tool was shown to be consistently good over all major
packages in the Java SDK.
Our use of students for evaluation may not generalize
across all populations and to professional developers in
particular. This threat is somewhat mitigated by the ob-
servation that our graduate student population agreed quite
closely with our undergraduate one despite the fact that
the graduates had signiÔ¨Åcantly varying backgrounds and
typically had more years of programming experience.Figure 6. Aggregated results from our human study. The left charts show responses comparing our tool to human-written examples and
the charts on the right compare our tool to the EXOADOCtool of Kim et al. [13]. The charts on the top categorize all comparisons (e.g.,
in 60% of comparisons, our tool output was judged at least as good as human-written examples). The charts on the bottom reÔ¨Çect the
consensus opinion for each example (e.g., 82% of examples produced by our tool were judged to be at least as good as human-written
on average). Examples are grouped by package (e.g., java.X ) and by category of participants (138 ugrads, 16 grads).
IX. R ELATED WORK
The most closely related work is that of Kim et al. [13]
and Zhong et al. [12]. We compare to Kim et al. directly
and have discussed Zhong et al. previously. We now brieÔ¨Çy
describe other works related to mining API usage patterns
and their documentation.
Our technique is inspired in part by the work of Whaley
et al. [32] who, in 2002, used multiple Ô¨Ånite state machine
submodels to model the interface of a class. They used
dynamic instrumentation techniques to extract such models
from execution runs. Whaley et al. note that these models
can potentially serve as documentation, however they do not
evaluate the efÔ¨Åcacy of such an approach.
Wasylkowski et al. [36] adapted static speciÔ¨Åcation min-
ing techniques to learn common, but not required, method-
call sequences. Similar to our technique, they use state
machines to represent common usage patterns for an object.
Nguyen et al. [37] employ a similar approach to mine
usage patterns, but across multiple objects. Both approaches
focus on defect and anomaly detection. By contrast, our
work focuses on generating and evaluating human-readable
documentation, assuming that average behavior in the input
corpus is correct.
Yessenov et al. [38] present a tool called M ATCH MAKER
which synthesizes code suitable for use as an example. A
MATCH MAKER query consists of names of two APIs, and
produces code enabling interaction between them. Unlike
our approach which abstracts from static usage examples,
MATCH MAKER relies on a database of dynamic programtraces to reason about the evolution of the heap that connects
the two APIs.
Dekel et al. [39] decorate method invocations with rules
or caveats of which client authors must be aware. The
technique increases awareness of important documentation
written by humans. Holmes et al. [17] use structural context
to recommend source code examples. Their approach Ô¨Ånds
relevant code in an example repository by heuristically
matching the structure of the code under development.
X. C ONCLUSION
API examples are known to be a key learning resource,
however they are expensive to create and are often un-
available or out-of-date. Current tools for mining examples
produce output that is complex and difÔ¨Åcult to understand,
compromising usefulness. These observations led us to pro-
pose an algorithm for generating API usage examples. Our
technique is efÔ¨Åcient and fully automated; to the best of our
knowledge, it is the Ô¨Årst approach that synthesizes human-
readable documentation for APIs. Our output is correct-by-
construction in several important respects (e.g., syntactically
valid and type-safe) and adheres to a well-deÔ¨Åned notion
of representativeness: if the order of any two statement
in one of our examples were inverted, the result would
correspond to fewer real-world uses. In a human study
involving over 150 participants, we have shown it to produce
output judged at least as good as gold-standard human-
written documentation 82% of the time and strictly better
than a state-of-the-art code search tool 94% of the time.REFERENCES
[1] P. Hallam, ‚ÄúWhat do programmers really do anyway?‚Äù in
Microsoft Developer Network ‚Äî C# Compiler , Jan 2006.
[2] S. L. PÔ¨Çeeger, Software Engineering: Theory and Practice .
NJ, USA: Prentice Hall, 2001.
[3] S. C. B. de Souza, N. Anquetil, and K. M. de Oliveira,
‚ÄúA study of the documentation essential to software main-
tenance,‚Äù in International Conference on Design of Commu-
nication , 2005, pp. 68‚Äì75.
[4] D. G. Novick and K. Ward, ‚ÄúWhat users say they want in
documentation,‚Äù in Conference on Design of Communication ,
2006, pp. 84‚Äì91.
[5] R. Holmes, R. Cottrell, R. Walker, and J. Denzinger, ‚ÄúThe
end-to-end use of source code examples: An exploratory
study,‚Äù in International Conference on Software Maintenance ,
2009, pp. 555‚Äì558.
[6] R. Jain, ‚ÄúAPI-writing and API-documentation,‚Äù in http://
api-writing.blogspot.com/ , Apr. 2008.
[7] S. G. McLellan, A. W. Roesler, J. T. Tempest, and C. I.
Spinuzzi, ‚ÄúBuilding more usable apis,‚Äù IEEE Softw. , vol. 15,
no. 3, pp. 78‚Äì86, 1998.
[8] J. Nykaza, R. Messinger, F. Boehme, C. L. Norman, M. Mace,
and M. Gordon, ‚ÄúWhat programmers really want: results of
a needs assessment for sdk documentation,‚Äù in International
Conference on Computer documentation , 2002, pp. 133‚Äì141.
[9] F. Shull, F. Lanubile, and V . R. Basili, ‚ÄúInvestigating reading
techniques for object-oriented framework learning,‚Äù IEEE
Trans. Softw. Eng. , vol. 26, no. 11, pp. 1101‚Äì1118, 2000.
[10] J. Stylos, B. A. Myers, and Z. Yang, ‚ÄúJadeite: improving
api documentation using usage information,‚Äù in Extended
Abstracts on Human Factors in Computing Systems , 2009,
pp. 4429‚Äì4434.
[11] M. P. Robillard, ‚ÄúWhat makes apis hard to learn? answers
from developers,‚Äù IEEE Softw. , vol. 26, no. 6, pp. 27‚Äì34,
2009.
[12] H. Zhong, T. Xie, L. Zhang, J. Pei, and H. Mei, ‚ÄúMAPO:
Mining and recommending API usage patterns,‚Äù in ECOOP ,
2009, pp. 318‚Äì343.
[13] J. Kim, S. Lee, S. Hwang, and S. Kim, ‚ÄúTowards an intel-
ligent code search engine,‚Äù in AAAI Conference on ArtiÔ¨Åcial
Intelligence , 2010.
[14] G. Ammons, R. Bod ¬¥ƒ±k, and J. R. Larus, ‚ÄúMining speciÔ¨Åca-
tions,‚Äù in Principles of programming languages , 2002, pp.
4‚Äì16.
[15] D. Mandelin, L. Xu, R. Bod ¬¥ƒ±k, and D. Kimelman, ‚ÄúJungloid
mining: helping to navigate the API jungle,‚Äù in Programming
Languages Design and Implementation , 2005, pp. 48‚Äì61.
[16] S. Thummalapenta and T. Xie, ‚ÄúParseweb: a programmer
assistant for reusing open source code on the web,‚Äù in
Automated Software Engineering , 2007, pp. 204‚Äì213.
[17] R. Holmes and G. C. Murphy, ‚ÄúUsing structural context to
recommend source code examples,‚Äù in International Confer-
ence on Software Engineering , 2005, pp. 117‚Äì125.
[18] javadoctool@sun.com, ‚ÄúHow to write doc comments for the
Javadoc tool,‚Äù in http://www.oracle.com/technetwork/java/
javase/documentation/index-137868.html , 2010.
[19] Oracle, ‚ÄúJava SE 6 documentation,‚Äù in http://download.
oracle.com/javase/6/docs/ , 2010.
[20] R. Chatterjee, B. G. Ryder, and W. Landi, ‚ÄúComplexity of
points-to analysis of java in the presence of exceptions.‚Äù IEEE
Trans. Software Eng. , vol. 27, no. 6, pp. 481‚Äì512, 2001.
[21] J.-D. Choi, D. Grove, M. Hind, and V . Sarkar, ‚ÄúEfÔ¨Åcient
and precise modeling of exceptions for the analysis of javaprograms,‚Äù in Workshop on Program Analysis for Software
Tools and Engineering , 1999, pp. 21‚Äì31.
[22] S. Sinha, A. Orso, and M. J. Harrold, ‚ÄúAutomated support
for development, maintenance, and testing in the presence
of implicit control Ô¨Çow,‚Äù in International Conference on
Software Engineering , 2004, pp. 336‚Äì345.
[23] W. Weimer and G. C. Necula, ‚ÄúFinding and preventing run-
time error handling mistakes,‚Äù in Conference on Object-
oriented programming, systems, languages, and applications ,
2004, pp. 419‚Äì431.
[24] M. P. Robillard and G. C. Murphy, ‚ÄúStatic analysis to sup-
port the evolution of exception structure in object-oriented
systems,‚Äù ACM Trans. Softw. Eng. Methodol. , vol. 12, no. 2,
pp. 191‚Äì221, 2003.
[25] B. G. Ryder, D. Smith, U. Kremer, M. Gordon, and N. Shah,
‚ÄúA static study of Java exceptions using JESP,‚Äù in Int. Conf.
on Compiler Construction , 2000, pp. 67‚Äì81.
[26] P. A. Relf, ‚ÄúTool assisted identiÔ¨Åer naming for improved
software readability: an empirical study,‚Äù Empirical Software
Engineering , November 2005.
[27] M. Das, S. Lerner, and M. Seigle, ‚ÄúESP: path-sensitive
program veriÔ¨Åcation in polynomial time,‚Äù SIGPLAN Notices ,
vol. 37, no. 5, pp. 57‚Äì68, 2002.
[28] L. Carter, B. Simon, B. Calder, L. Carter, and J. Fer-
rante, ‚ÄúPath analysis and renaming for predicated instruction
scheduling,‚Äù International Journal of Parallel Programming ,
vol. 28, no. 6, pp. 563‚Äì588, 2000.
[29] T. Robschink and G. Snelting, ‚ÄúEfÔ¨Åcient path conditions in
dependence graphs,‚Äù in International Conference on Software
Engineering , 2002, pp. 478‚Äì488.
[30] R. Alur, P. Cerny, P. Madhusudan, and W. Nam, ‚ÄúSynthesis
of interface speciÔ¨Åcations for Java classes,‚Äù in Principles of
Programming Languages , 2005.
[31] W. Weimer and G. C. Necula, ‚ÄúMining temporal speciÔ¨Åca-
tions for error detection,‚Äù in Tools and Algorithms for the
Construction and Analysis of Systems , 2005, pp. 461‚Äì476.
[32] J. Whaley, M. C. Martin, and M. S. Lam, ‚ÄúAutomatic extrac-
tion of object-oriented component interfaces,‚Äù in International
Symposium of Software Testing and Analysis , 2002.
[33] S. E. Stemler, ‚ÄúA comparison of consensus, consistency, and
measurement approaches to estimating interrater reliability,‚Äù
Practical Assessment, Research and Evaluation , 2004.
[34] L. Kaufman and P. Rousseeuw, Finding Groups in Data: An
Introduction to Cluster Analysis . Wiley Interscience, 1990.
[35] R. P. L. Buse and W. R. Weimer, ‚ÄúA metric for software
readability,‚Äù in International Symposium on Software Testing
and Analysis , 2008, pp. 121‚Äì130.
[36] A. Wasylkowski, A. Zeller, and C. Lindig, ‚ÄúDetecting object
usage anomalies,‚Äù in Foundations of Software Engineering ,
2007, pp. 35‚Äì44.
[37] T. T. Nguyen, H. A. Nguyen, N. H. Pham, J. M. Al-Kofahi,
and T. N. Nguyen, ‚ÄúGraph-based mining of multiple object
usage patterns,‚Äù in Foundations of Software Engineering ,
2009, pp. 383‚Äì392.
[38] K. Yessenov, Z. Xu, and A. Solar-Lezama, ‚ÄúData-driven
synthesis for object-oriented frameworks,‚Äù in Proceedings
of the 2011 ACM international conference on Object
oriented programming systems languages and applications ,
ser. OOPSLA ‚Äô11. New York, NY , USA: ACM, 2011,
pp. 65‚Äì82. [Online]. Available: http://doi.acm.org/10.1145/
2048066.2048075
[39] U. Dekel and J. D. Herbsleb, ‚ÄúImproving api documentation
usability with knowledge pushing,‚Äù in International Confer-
ence on Software Engineering , 2009, pp. 320‚Äì330.