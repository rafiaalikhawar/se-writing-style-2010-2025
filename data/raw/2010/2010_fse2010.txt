FUSION: A Framework for Engineering  
Self-Tuning Self-Adaptive Software Systems 
Ahmed Elkhodary 
Department of Computer Science 
George Mason University 
aelkhoda@gmu.edu Naeem Esfahani 
Department of Computer Science 
George Mason University 
nesfaha2@gmu.edu Sam Malek 
Department of Computer Science 
George Mason University 
smalek@gmu.edu 
 
ABSTRACT  
Self0adaptive software systems are capable of adjus ting their 
behavior at run0time to achieve certain objectives.  Such systems 
typically employ analytical models specified at des ign0time to 
assess their characteristics at run0time and make t he appropriate 
adaptation decisions. However, prior to system’s de ployment, 
engineers often cannot foresee the changes in the e nvironment, 
requirements, and system’s operational profile.  Th erefore, any 
analytical model used in this setting relies on und erlying 
assumptions that if not held at run0time make the a nalysis and 
hence the adaptation decisions inaccurate. We prese nt and 
evaluate FeatUre0oriented Self0adaptatION (FUSION) 
framework, which aims to solve this problem by lear ning the 
impact of adaptation decisions on the system’s goal s. The 
framework (1) allows for automatic online fine0tuni ng of the 
adaptation logic to unanticipated conditions, (2) r educes the 
upfront effort required for building such systems, and (3) makes 
the run0time analysis of such systems very efficien t.    
Categories and Subject Descriptors  
D.2.10 [ Software Engineering ]: Design – Methodologies.  
General Terms  
Algorithms, Performance, Design. 
1. INTRODUCTION 
The ever0growing complexity of software systems cou pled with 
the need to maintain its quality of service (QoS) c haracteristics, 
even under adverse conditions and highly uncertain environments, 
have instigated the emergence of self-adaptive  software systems 
[13]. A self0adaptive software system is capable of  modifying 
itself at run0time to achieve certain functional or  QoS goals. The 
development of such systems has shown to be signifi cantly more 
challenging than static and predictable software sy stems [2]. 
In particular, engineering the adaptation logic pos es the greatest 
difficult. Since software engineers often cannot fo resee all of the 
changes in the environment, requirements, and syste m’s 
operational profile at design0time, they rely on an alytical models 
that given the monitoring data obtained at run0time  assess the 
system’s ability to satisfy its goals. The results produced by the 
analytical models thus serve as indicators for maki ng the adaptation decisions.  
Generally, this approach suffers from three shortco mings:  
• Unwieldy for use.  Existing state of the art self0adaptive 
frameworks require the engineer to construct and ut ilize 
complex analytical models. Unfortunately, the major ity of 
widely used analytical models (e.g., Queueing Netwo rk 
models [8] for performance analysis) have to painst akingly be 
customized to the unique characteristics of an appl ication 
domain.  Moreover, for any application0specific goa l, an 
appropriate analytical model would have to be devel oped from 
scratch; a task that is often very difficult, when one considers 
the complexity of today’s software systems. Further  
exacerbating the problem is that software engineeri ng 
practitioners are typically not savvy mathematician s and find 
it difficult to build systems that make use of such  models. 
• Wrong assumptions.  Analytical models make simplifying 
assumptions or presume certain properties of the ru nning 
system that may not bear out in practice. These mod els are 
specified at design0time and cannot cope with the r un0time 
changes that were not accounted for in their formul ation. 
These assumptions could make the analysis and hence  the 
adaptation decisions inaccurate. 
• Efficiency. Efficiency of analysis and planning is of utmost 
importance in most self0adaptive software systems t hat need to 
react quickly to situations that arise at run0time.  At the same 
time, searching for an optimal architectural config uration (i.e., 
solution) is often computationally very expensive [ 2]. 
In this paper, we present an alternative and relati vely unexplored 
method of constructing self0adaptive software syste ms aimed at 
alleviating the three problems mentioned above. Ins tead of 
manually developing an analytical model that relate s the impact of 
adaptation decisions on the system’s goals, we pres ent a learning0
based approach in which such a model is automatical ly induced 
from the monitored data. The approach not only allo ws for 
automatic online fine0tuning of the adaptation logi c to 
unanticipated conditions, but also reduces the upfr ont effort 
required for building such systems. 
We describe this research in the context of a frame work, entitled 
FeatUre-oriented Self-adaptatION (FUSION) , which by using a 
feature0oriented system model learns the impact of feature 
selection and feature interactions on the system’s competing 
(conflicting) goals. It then uses this knowledge to  efficiently adapt 
the system to satisfy as many user0defined goals as  possible.  
In this paper, we elaborate on three key contributi ons of FUSION:  
• FUSION adapts and learns in terms of features . A feature is a 
domain and platform independent method of represent ing a 
particular system capability [7,12]. This along wit h the fact 
that FUSION does not prescribe a particular analyti cal model  
Permission to make digital or hard copies of all or  part of this work for 
personal or classroom use is granted without fee pr ovided that copies are 
not made or distributed for profit or commercial ad vantage and that 
copies bear this notice and the full citation on th e first page. To copy 
otherwise, or republish, to post on servers or to r edistribute to lists, 
requires prior specific permission and/or a fee. 
FSE’10 , November 7–11, 2010, Santa Fe, New Mexico, USA. 
Copyright 2010 ACM 97801060558079102/10/11…$10.00. 
 makes the approach applicable to 
any software system with 
minimal effort. 
• FUSION copes with the changing 
dynamics of the system, even 
those that were not anticipated, 
through continuous observation 
and induction. In turn, FUSION is 
capable of learning run0time 
behaviors unforeseen at design0
time. 
• FUSION incorporates the 
engineer’s knowledge of the 
system and its capabilities in the 
form of feature relationships. It 
then uses these relationships to 
reduce the valid configuration 
space significantly, which makes 
not only the learning feasible but 
also the adaptation planning 
efficient for use at run0time. 
The rest of this paper is organized as 
follows. Section  2 motivates the 
problem using a system that also 
serves as a running example in this paper. Section  3 provides an 
overview of FUSION. Sections  4,  5, and  6 respect ively detail 
FUSION’s feature0oriented model of adaptation, lear ning method, 
and adaptation planning. Sections  7 and  8 present  the 
implementation and evaluation details of FUSION. Th e paper 
concludes with an overview of the related work and future 
avenues of research. 
2. MOTIVATION 
We illustrate and evaluate the concepts using an on line Travel 
Reservation System (TRS), which is representative o f web 
applications used by large organizations for making  travel 
reservations. Figure 1c shows a subset of its softw are architecture 
using the traditional component0and0connector view.  TRS aims to 
provide the best airline ticket prices in the marke t. To make a 
price quote for the user, TRS takes the trip inform ation from the 
user, and consequently discovers and queries variou s travel agent 
services. The travel agents reply with their itiner ary offers, which 
are then sorted and presented in ascending order of  quoted price.  
In addition to the functional goals, the system is required to attain 
a number of QoS goals, such as performance, securit y, and 
accountability. To that end, solutions for each QoS  concern were 
developed, e.g., caching for performance, authentic ation for 
security, and logging of activities for accountabil ity purposes.  
In addition, TRS needs to be self0adaptive to deal with unexpected 
situations, such as traffic spikes or security atta cks. For instance, 
enable caching to improve performance during a traf fic spike, 
increase authentication to thwart a security attack , and enable 
logging to ensure non0repudiation of transactions ( i.e., 
accountability). The adaptation logic of TRS also n eeds to balance 
tradeoffs (conflicts) when it selects from the avai lable adaptation 
choices, e.g., improving security may degrade respo nse time.  
As mentioned earlier, there are three problems asso ciated with the 
construction of adaptation logic. Consider the issu es that may 
arise in the context of TRS: • Unwieldy for use.  Consider the difficulty of accurately 
estimating the impact of enabling a particular type  of 
authentication on the price quotes in TRS. Using a heavy 
authentication protocol increases the system’s resp onse time, 
which forces more timeouts on the client0side. This  reduces 
the total number of received offers, and hence the quality of 
price quotes. Quantitatively modeling this trade0of f is 
difficult, as it depends on many dynamic parameters : available 
service providers, network characteristics, and so on.  
• Wrong assumptions.  Consider an analytical model that 
quantifies the impact of an adaptation decision on the response 
time of receiving price quotes from travel agents ( thick lines 
in Figure 1c). Such a model would inevitably make 
simplifying assumptions based on what the engineers  believe 
to be the main sources of delay in the system. For instance, if 
fast communication links are assumed, the analytica l model 
may ignore the network delay. Since accurately pred icting the 
characteristics of a dynamic system is extremely di fficult, the 
assumptions may not hold, making the analysis and h ence the 
adaptation decisions inaccurate.    
• Efficiency. To satisfy multiple goals, self0adaptation logic 
needs to search in a configuration space that is eq uivalent to 
the combined complexity of all the analytical model s 
involved. As an example, consider how TRS would mak e use 
of P authentication components for authenticating the n etwork 
traffic between its N software components, which may be 
deployed on M different hardware platforms. Analyzing the 
impact of authentication alone on the system’s goal s would 
require exploring a space of (MN possible deployments) P possible 
ways of authentication  = MNP possible configurations . Such a 
problem is computationally expensive to solve at ru n0time for 
any sizable system. This is while authentication ma y be only 
one concern out of many. 
The aforementioned difficulties have shaped our mot ivation in the 
development the FUSION framework, as described next .  
Figure 1. Travel Reservation System: (a) goals are quantified in terms of utility obtained 
for a given level of metric; (b) subset of availabl e features, where features with thick 
borders are selected; (c) software architecture cor responding to the selected features, 
where the thick lines represent an execution scenar io associated with goal G1.  
3. FUSION OVERVIEW  
Figure 2 depicts the FUSION framework as it adapts a running 
software system. The running system is variable in the sense that 
its features can be selected (i.e., enabled/disable d) on demand. 
FUSION modifies the feature selection to resolve Qo S tradeoffs 
and satisfy as many goals as possible. For example,  if the TRS 
system violates Quote Response Time  goal, it is adapted to a new 
feature selection that brings down the response tim e and keeps 
other goals satisfied. 
As depicted in Figure 2, FUSION makes such adaptati on 
decisions using a continuous loop, called adaptation cycle . The 
adaptation cycle collects metrics (measurements) an d optimizes 
the system by executing three activities in the fol lowing sequence: 
• Based on the metrics collected from the running sys tem, 
Detect  calculates the achieved utility (i.e., measure of user’s 
satisfaction) to determine if a goal violation has occurred.  
• When a goal is violated, Plan  searches for an optimal 
configuration (feature selection) that maximizes ov erall utility. 
• Given a new feature selection, Effect determines a set of 
adaptation steps (i.e., enable/disable features) to  ensure 
consistency during adaptation. 
FUSION uses learning cycle  (depicted in Figure 2) to learn the 
impact of adaptation decisions in terms of feature selection on the 
system’s goals. The first execution of learning cyc le occurs before 
the system’s initial deployment. The system is eith er simulated or 
executed in offline mode and metrics corresponding to each 
feature selection is collected. This data is used t o train FUSION to 
induce a preliminary model of the system’s behavior .  
At run0time, the learning cycle continuously execut es, and as the 
dynamics of the system and its environment change, the 
framework tunes itself. For example, when FUSION ad apts TRS 
to resolve a “ quote response time”  violation, it keeps track of the 
gap between the expected and the actual outcome of the 
adaptation. This gap is an indicator of the new beh avioral patterns 
in the system. Learning cycle collects such indicat ors and tunes 
itself by executing two activities in the following  sequence: 
• Based on the measurements collected from the system , 
Observe  detects any emerging patterns of behavior. An emergent pattern is detected when the system sets t he wrong 
expectation (i.e., inaccurate impact of adaptation on utility). 
• Induce  learns the new behavior through induction and stor es 
the refined model in the knowledge base,  which is used to 
make (more) informed adaptation decisions in future  cycles.  
In the following three sections, we describe FUSION ’s underlying 
model, learning cycle, and adaptation cycle in more  detail. 
4. FUSION MODEL 
We describe FUSION’s approach to modeling adaptatio n choices 
and goals. As detailed in Sections  5 and  6, FUSION’s model is the 
key enabler of effective learning and efficient ana lysis.  
4.1  Feature-Based Adaptation 
In FUSION, the unit of adaptation is a feature . A feature is an 
abstraction of a capability provided by the system.  A feature may 
affect either the system’s functional (e.g., abilit y to print receipts) 
or non0functional (e.g., ability to authenticate) p roperties. 
The use of features as an abstraction makes the FUS ION 
framework independent of a particular implementatio n platform 
or application domain. For example, in a rule0based  system a 
feature may correspond to a set of rules, in a serv ice0oriented 
system it may correspond to a set of services in a workflow, and 
so on. For clarity, in this paper we assume a parti cular realization 
of a feature: a feature represents an extension of the architecture at 
well0defined variation points . A feature maps to a subset of the 
system’s software architecture. For example, Figure  1b shows the 
mapping of Evidence Generation  feature to a subset of the TRS.  
Figure 1b shows a simple feature model for TRS. The re are four 
features in the system and one common core . The features in the 
example use two kinds of relationships: dependency , and mutual 
exclusion . The dependency relationship indicates that a feat ure 
requires the presence of another feature. For examp le, enabling 
the Evidence Generation  feature requires having the core  feature 
enabled as well. Mutual exclusion is another relati onship, which 
implies that if one of the features in a mutual group  is enabled, the 
others must be disabled. For example, Per-Request Authentication  
and Per-Session Authentication  cannot be enabled at the same 
time. Feature modeling supports several other types  of inter0
feature relationships (see [7]) that for brevity ar e not detailed here. 
The feature model is used to identify the current s ystem 
configuration in terms of a feature selection strin g. In a feature 
selection string, enabled features are set to “1”; disabled features 
are set to “0”. For example, one possible configura tion of TRS 
would be “1101”, which means that all features from  Figure 1b 
are enabled except Per-Request Authentication . The adaptation of 
a system is modeled as a transition from one featur e selection 
string to another, which we detail in Section  6.3.  
4.2  Goals 
A goal  represents the user’s functional or QoS objectives  for a 
particular execution scenario. A goal consists of a  metric  and a 
utility . A metric is a measurable quantity (e.g., response  time) that 
can be obtained from a running system. A utility fu nction is used 
to express the user’s preferences (satisfaction) fo r achieving a 
particular metric. For instance, goal G1 in Figure 1a specifies the 
user’s degree of satisfaction ( U) with achieving a specific value of 
Quote Response Time (M).   
Figure 2. Overview of the FUSION framework. 
Elicitation of user’s preferences, while an importa nt prerequisite 
for using the framework, is a topic that has been i nvestigated 
extensively in the existing literature (e.g., [17]) , and considered to 
be outside the focus of this paper. FUSION is indep endent of the 
type of utility functions and the approach employed  in 
extrapolating them. Arguably any user can specify h ard 
constraints, which can be trivially modeled as step 0functions (e.g., 
G4 depicted in Figure 1a). Alternatively a utility fu nction may 
take on more advanced forms (e.g., sigmoid curve), and express 
more complex preferences, such as G 1, G 2, and G 3.  
FUSION places one constraint on the range of utilit y functions: 
they need to return zero for the metric values that  are not 
acceptable to the user.  As will be discussed in Se ction  6.1, when 
a utility associated with a goal reaches zero, FUSI ON considers 
that goal violated and initiates adaptation. 
5. FUSION LEARNING CYCLE 
FUSION copes with the changing dynamics of the syst em through 
learning. Learning discovers relationships between features and 
metrics. Each relationship is represented as a func tion that 
quantifies the impact of features, including their interactions, on a 
metric. In TRS, for example, the result of learning  would be four 
functions, one function for each of the four metric s MG1  through 
MG4 . Each function takes a feature selection as input and produces 
an estimated gain/loss value for the metric as outp ut. 
Learning is typically a very computationally intens ive process. In 
particular, learning simply at the architectural0le vel is infeasible 
for any sizable system, which is the reason why its  application in 
existing architecture0based adaptation approaches h as been 
limited. FUSION’s feature0oriented model offers two  
opportunities for tackling the complexity of learni ng: 
1.  Learning operates on feature selection space , which is 
significantly smaller than the traditional architec tural0level 
configuration space. The features in FUSION encode the 
engineer’s domain knowledge of the practical variat ion 
points in a given application. For instance, the en gineer may 
only consider a small reasonable subset of MNP authentication 
driven architectural choices (recall Section  2). Figure 1b 
shows two authentication strategies modeled as feat ures in 
TRS: F3 and  F4. These two features represent what the TRS 
security engineer envisioned to be the reasonable 
applications of authentication in the system.  
2.  By using the inter0feature relationships (e.g., mut ual 
exclusions, dependencies), one can significantly re duce the 
feature selection space. For instance, Figure 1b sh ows a 
mutual exclusion relationship between F3 and F4. This 
relationship is manifestation of the domain knowled ge that 
applying two authentication protocols to the same e xecution 
scenario is not appropriate. Such relationships red uce the space of valid feature selections significantly, fu rther aiding 
FUSION to learn their trade0offs with respect to go als.  
Figure 3 is an algorithm that determines the size o f the valid 
feature selection space in a feature model recursiv ely. Applying 
this algorithm to the feature model in Figure 1b yi elds a space of 8 
valid feature selections, calculated as follows:  
2 from F 1 × 2 from F 2 × (2 from F 3 + 2 from F 4 – 2) . 
Without considering the inter0feature relationships  to prune the 
invalid selections, the space of feature selection would have been 
2number of features = 2 4 = 16 . 
Learning starts with a training process that popula tes FUSION’s 
knowledge base with an initial set of functions. Co nsequently, at 
run0time, the learning cycle fine0tunes the functio ns to 
accommodate emergent behaviors. The rest of this se ction 
describes the two activities that take place to pop ulate and fine0
tune the knowledge base. 
5.1  Observe 
Observe  is a continuous execution of two activities: (1) n ormalize 
raw metric values to make them suitable for learnin g, and (2) test 
the accuracy of learned functions. We describe each  of these 
activities below. 
Learning in terms of raw data hampers the accuracy.  For instance, 
consider the fact that the actual impact of a featu re on a metric 
may depend on the system’s workload. Therefore, the  actual 
metric data obtained from executing the same softwa re system 
(i.e., same feature selection) under different work loads may result 
in starkly different metric readings, thus making i t difficult to 
generalize in the form of a learned function.  
To address this issue,  Observe  takes raw metric data through an 
automated normalization process prior to storing th em as 
observation records. Many normalization techniques can be 
applied to transform the learning inputs into a rep resentation that 
is less sensitive to the execution context. In Tabl e 1, observation 
records were normalized using studentized residual  [1] as follows: 
/g3435/g1870/g1853/g1875 /g1874/g1853/g1864/g1873/g1857 /g3398 /g1850 /g3439//g1871, where  /g1850 and /g1871 are the mean and the 
standard deviation of the collected data, respectiv ely. 
Normalization using studentized residual does not r equire 
knowledge of population parameter, such as absolute  min0max 
values and population mean. It only requires knowle dge of mean 
and standard deviation for sample data. 
Once a preliminary set of functions are learned (de tails provided 
in the next section), Observe  continuously tests the accuracy of 
functions against the latest collected observations . Accuracy is 
defined as the difference between predicted value o f a reward 
using the learned functions and actually observed v alue. For that 
purpose, we use the learning accuracy threshold  provided by the 
learning algorithm itself. Note that the majority o f learning 
algorithms provide an error threshold that indicate s the noise in 
learned functions. On top of this, one may specify an additional 
margin of inaccuracy that can be tolerated, in case s where it is not 
desirable to run the learning algorithm frequently.  If the accuracy 
test fails, Observe  takes this as an indicator that either learning is  
incomplete or new patterns of behavior are emerging  in the 
system and, thus, notifies the Induce  activity to fine0tune the 
learned functions using the latest set of observati ons. 
5.2  Induce 
Based on the collected observations, the Induce  activity constructs 
several functions that estimate the impact of makin g a feature SelectionCounter (Feature F) :int  
int  Count = 1; 
switch  (F.Type ) 
case  “MutualGroup” :  
for  each  ( C in  F.Children ) 
Count +=  SelectionCounter (C) - 1; 
case  “LeafFeature” :  
Count = 2; 
default :  
for each  ( C in  F.Children ) 
Count *=  SelectionCounter (C); 
Count ++ ; 
return  Count;  
Figure 3. Algorithm for sizing the feature selectio n space.  selection on the corresponding metrics. Induce  executes two steps. 
The first step is a significance test  that determines the features 
with the most significant impact on each metric. Th is allows us to 
reduce the number of independent variables (recall Table 1) that 
learning needs to consider for each metric. After t he significance 
test, we apply the learning, which for each goal, g iven the 
normalized observations and the features with signi ficance, 
derives the corresponding relationships.  
While FUSION is not tied to a particular learning a lgorithm, in 
our implementation we have used the M5 model tree ( MT) 
algorithm [9], which is a machine learning techniqu e with three 
important properties: (1) ability to eliminate insi gnificant features 
automatically, (2) fast training and convergence, a nd (3) efficient 
interaction detection.  Table 2  shows the induced relationships 
among features and metrics for TRS.  
The information in this table can also be represent ed simply as a 
set of functions. For instance, a function estimati ng /g1839/g3008/g2869 
corresponds to the second column of the table  as follows: 
/g1839/g3008/g2869=1.553  F1 /g3398 0.673  F2 + 0.709  F3 + 0.163 F1F3 /g3398 0.843  (1) 
Each feature is assigned a coefficient that is effe ctive only when 
the feature is enabled (i.e., it is set to “1”). Fo r example, the 
expected value of MG1  for a feature selection where only F1 and 
F3 are enabled (“1010”) can be calculated as follows:  
/g1839/g3008/g2869=1.553× 1 + 0 +  0.709× 1+ 0.163× 1×1 /g3398 0.843 = 1.482  (2) 
When making adaptation decisions, values obtained f rom the 
induced functions (e.g., 1.482 from Eq.  2 above) a re 
denormalized by using the inverse of normalization equation 
presented in the previous section. The denormalized  value for a 
metric is then plugged into the corresponding utili ty function to 
determine the impact of feature selection on the go al.   
Note that the induction also captures the impact of  feature 
interactions on metrics. For example, Eq. 1 specifi es that enabling 
both F1 (Evidence Generation ) and F3 (Per-Request 
Authentication ) increases /g1839/g3008/g2869. This is because according to Table 
2, F1F3 increases the response time by 0.163, which decrea ses the 
utility of G 1 (utility of G 1 is shown in Figure 1a). Using Figure 1c 
we can explain this feature interaction as follows.  F1 introduces a 
delay by adding a mediator connector, called Log , that records the 
transactions with remote travel agents. At the same  time, F3 
changes the behavior of the Log, as it causes an additional delay 
in mediating the exchange of per session authentica tion 
credentials. Enabling the two features at the same time has a 
negative ramification that is beyond the individual  impact of each. 
In some cases, learning may need to incorporate som e contextual 
factors as independent variables, due to their impa ct on metrics. 
Consider a system with drastically different worklo ads at different 
times of day that cannot be dealt with effectively through normalization. In that case, the result of learning  would be a set of 
equations that estimate the impact of feature selec tion in different 
contexts. For example, the following equations esti mate the 
impact of feature selection on MG1 under different workloads ( w): 
/g1839/g3008/g2869/g3404/g3421…/g3397/g2782./g2782/g2781 /g18321/g3398/g2779./g2778/g2781 /g18322/g3397/g2779./g2781/g18323…,                          /g1875/g34091.21 
…/g3397/g2778./g2786/g2785 /g18321/g3398/g2778./g2781/g2783 /g18322/g3397/g2778./g2781/g18323…,            1.21 /g3407/g1875/g34091.29 
…/g3397/g2777./g2786/g2782 /g18321/g3397/g2777./g2783/g2783 /g18323/g3397/g2777./g2779/g2781 /g18321/g18323…,                  /g1875/g34081.29 /g1 (3)  
Where  w is the average inter0arrival time between requests  in 
milliseconds; lower inter0arrival time implies high er workload. 
Here, the generated functions indicate that TRS rea ches saturation 
when w is in the range of 1.21–1.29 milliseconds. Since t he 
impact of features on MG1  changes dramatically in that range, the 
learning algorithm produces a separate equation tar geted at that. 
Although these equations may be of any type (e.g., linear, 
sigmoid, or exponential as in [4]), for clarity we have limited the 
discussion to multi0linear equations only. 
6. FUSION ADAPTATION CYCLE 
In this section, we describe how Detect , Plan  and Effect  use the 
learned knowledge to adapt a software system in FUS ION. The 
underlying principle guiding the adaptation strateg y in FUSION is 
simple: if the system works (i.e., satisfies the user), do not change 
it; when it breaks, find the best fix for only the broken part . While 
intuitive, this approach sets FUSION apart from man y of the 
existing works that either attempt to continuously optimize the 
entire system, or solely solve the constraints (i.e ., violated goals) 
in the system. FUSION adopts a middle ground, which  we believe 
to be the most sensible, and achieves the following  objectives:  
1.  Reduce Interruption : Adaptation typically interrupts the 
system’s operation (e.g., transient unavailability of certain 
functionality). In turn, even if at run0time a solu tion with a 
higher utility is found, one may opt not to adapt t he system to 
avoid such interruptions. FUSION reduces interrupti ons by 
adapting the system only when a goal is violated.  
2.  Efficient Analysis : Often in run0time adaptation, the 
performance of analysis is crucial. FUSION uses the  learned 
knowledge to scope the analysis to only the parts o f the 
system that are affected by the adaptation, hence m aking it 
significantly more efficient than assessing the ent ire system. 
3.  Stable Fix : Given the overhead and interruption associated 
with the adaptation, effecting solutions that provi de a 
temporary fix are not desirable. We would like FUSI ON to 
minimize frequent adaptation of the system for the same 
problem. To that end, instead of simply satisfying the 
violated goals, FUSION finds a near optimal solutio n that is 
less likely to be broken due to fluctuations in the  system. 
6.1  Detect 
The adaptation cycle is initiated as soon as Detect  determines a 
goal violation. This is achieved by monitoring the utility functions Table 2. Learned metric functions. An empty cell me ans 
that the corresponding feature has no significant i mpact.  
Significant 
Variables Induc ed Functions  
MG1  MG2  MG3  MG4  ..  
Core - 0.843 - 0.161 1.332 - 0.488 .. 
F1 1.553 1.137   1.548 .. 
F2 - 0.673 - 0.938     .. 
F3 0.709   - 0.672   .. 
F4   - 0.174     .. 
F1F3 0.163       .. 
.. .. .. .. .. .. 
 Table 1. Normalized observation records 
Indep. Vars  Dependent Variables  
F1 F2 F3 F4 MG1  MG2 MG3  MG4  ..  
.. .. .. .. .. .. .. .. .. 
0 0 0 1 - 0.842 - 0.308 1.432 - 0.521 .. 
1 0 0 1 0.650 0.513 1.371 1.501 .. 
0 1 0 1 - 1.470 - 0.719 1.378 - 0.522 .. 
0 0 1 0 - 0.132 -0.103 0.740 - 0.712 .. 
0 0 0 1 - 0.736 - 1.335 1.103 - 0.117 .. 
1 0 1 0 1.574 1.951 0.550 1.566 .. 
1 1 0 1 0.153 0.513 1.090 1.501 .. 
1 1 1 0 0.804 -0.513 0.562 1.566 .. 
.. .. .. .. .. .. .. .. .. 
 (recall Section  4.2). A utility function serves two purposes in the  
adaptation cycle: (1) when the metric values are un acceptable, 
returns zero to indicate a violated goal, and (2) w hen the metrics 
satisfy the minimum, returns a positive value less than one to 
indicate the user’s preference for improvement. The refore, utility 
is not only used to initiate adaptation, but also t o perform trade0off 
analysis between competing feature selections, such  that an 
optimization of the system can be achieved.  
6.2  Plan 
To achieve the adaptation objectives, FUSION relies  on the 
knowledge base to generate a tailored problem: 
• Given a violated goal, we use the knowledge base to  eliminate 
all of the features with no significant impact on t he goal. We 
call the list of features that may affect a given g oal Shared 
Features . Consider a situation in the TRS where G2 is 
violated. By referring to column /g1839/g3008/g2870 in Table 2, we can 
eliminate feature F3, since it has no impact on G2’s metric. In 
this example Shared Features = {F 1,F2,F4}.  
• Shared Features  represent our adaptation parameters. These 
features may also affect other goals, the set of wh ich we call 
the Conflicting Goals . To detect the conflicts, again we use the 
knowledge base, except this time we backtrack the l earned 
relationships. For each feature in the Shared Features we find 
the corresponding row in Table 2, and find the othe r metrics 
that the feature affects. In the above example, we can see that 
features F1, F2, and F4 also affect metrics MG1  and MG4 , and 
hence the corresponding goals, G1 and G4.  
By using the knowledge base, FUSION generates an op timization 
problem customized to the running software. The obj ective is to 
find a selection of Shared Features , F*, that maximizes the 
system’s overall utility for the Conflicting Goals  as follows: 
/g1832/g1499/g3404/g1853/g1870/g1859/g1865/g1853/g1876 /g4666/g3007/g1488/g3020/g3035/g3028/g3045/g3032/g3031/g3007/g3032/g3028/g3047/g3048/g3045/g3032/g3046 /g4667/g3533 /g1847/g3034
/g1482/g3034/g1488/g3004/g3042/g3041/g3033/g3039/g3036/g3030/g3047/g3036/g3041/g3034  /g3008/g3042/g3028/g3039/g3046 /g4666/g1839/g3034/g4666/g1832/g4667/g4667 
where /g1847/g3034 represents the utility function associated with th e metric 
/g1839/g3034 for goal g (recall Figure 1a). Since we do not want the solut ion 
to violate any of the conflicting goals, the proble m is subject to: 
/g3537  /g1847/g3034/g4666/g1839/g3034/g4666/g1832/g4667/g4667/g34080
/g1482/g3034 /g1488 /g3004/g3042/g3041/g3033/g3039/g3036/g3030/g3047/g3036/g3041/g3034  /g3008/g3042/g3028/g3039/g3046   
Note that we do not need to include the goals that are unaffected 
by Shared Features . To prevent feature selections that violate the 
mutual exclusion, we specify the following constrai nt: 
/g1482/g1859/g1870/g1867/g1873/g1868 /g1488/g1858/g1857/g1853/g1872/g1873/g1870/g1857  /g1865/g1867/g1856/g1857/g1864 ,/g3533  /g1858/g3030/g34091
/g1482/g3033/g3278/g1488/g3034/g3045/g3042/g3048/g3043   
Here when more than one feature from the same mutua lly 
exclusive group is selected, the left hand side of the inequality 
brings the total to greater than 1 and violates the  constraint. 
Finally, we ensure the dependency relationship as f ollows: 
/g1482/g1858/g3030/g3035/g3036/g3039/g3031 /g1488/g1845/g1860/g1853/g1870/g1857/g1856  /g1832/g1857/g1853/g1872/g1873/g1870/g1857/g1871 , /g1858/g3043/g3028/g3045/g3032/g3041/g3047  /g3398/g1858/g3030/g3035/g3036/g3039/g3031  /g34100  
This inequality does not hold if a child (dependent ) feature is 
enabled without its parent being enabled. Applying this 
formulation to the TRS scenario in which G 2 is violated generates 
the following optimization problem: 
Shared features = {F 1, F 2, F 4} 
/g1853/g1870/g1859/g1865/g1853/g1876 /g4666/g3007/g4667 /g1847/g3008/g2869/g4666/g1839 /g3008/g2869/g4666/g1832/g4667/g4667 /g3397 /g1847 /g3008/g2870/g4666/g1839 /g3008/g2870/g4666/g1832/g4667/g4667 /g3397 /g1847 /g3008/g2872/g4666/g1839 /g3008/g2872/g4666/g1832/g4667/g4667 
Subject to:   /g1847/g3008/g2869/g4666/g1839 /g3008/g2869/g4666/g1832/g4667/g4667 /g3400 /g1847 /g3008/g2870/g4666/g1839 /g3008/g2870/g4666/g1832/g4667/g4667 /g3400 /g1847 /g3008/g2872/g4666/g1839 /g3008/g2872/g4666/g1832/g4667/g4667 /g3408 0  
/g1832/g2871/g3397 /g1832 /g2872/g3409 1 
Where:  MG1 =1.553 F 1- 0.673 F 2+0.709 F 3+0.163 F 1F3 - 0.843       
  M G2 =1.137 F 1 - 0.938 F 2 - 0.174 F 4 - 0.161 
 M G4  =1.548 F 1 - 0.488 Note that by eliminating /g1847/g3008/g2871 and F3 from the optimization 
problem, we obtain an optimization problem tailored  to the 
violated goals. The customized problem has less num ber of 
features and goals than the original problem. In ou r small 
example, the gain may not seem significant. However , as shown 
in Section  8, in large software systems pruning the optimizati on 
problem achieves significant performance gains. 
6.3  Effect 
Once an optimal feature selection is determined, th e Effect  activity 
is initiated to make the system transition from the  current feature 
selection to the new one. Effect  chooses a path containing several 
adaptation steps  (transitions) towards the new feature selection. 
The steps take one of the three forms: enable  and disable  an 
optional feature, or swap  two mutually exclusive features. Figure 
4 shows an adaptation path that takes the TRS syste m from 
feature selection “1010” to “0101” in three steps . 
Since there are many possible paths to reach a targ et feature 
selection, the Effect  component is responsible for picking a path 
that satisfies feature model constrains in addition  to system goals. 
In the above example, enabling F3 and F4 at the same time 
produces a feature selection that violates the mutu al exclusion 
relationship in the feature model. If two features are mutually 
exclusive, the system should never be in a state wh ere both 
features are enabled. Similarly, a dependent featur e is never 
enabled without its prerequisite (parent) being ena bled first. 
7. IMPLEMENTATION 
Figure 5 shows snapshots of a prototype implementat ion of 
FUSION. This figure closely matches the structure o f Figure 1, 
and illustrates the realization of the modeling con cepts in 
FUSION. To streamline the development of tool suppo rt for 
FUSION, we have adopted, extended, and integrated e xisting 
tools to the extent possible.  
We have provided support for FUSION’s modeling meth odology 
by extending XTEAM [3]. XTEAM supports modeling of 
software architectures using well0known Architectur al 
Description Languages (ADLs). It supports Finite St ate Processes 
(FSP) and eXtensible Architecture Description Langu age (xADL) 
for modeling the behavioral and structural properti es, 
respectively. A snapshot of xADL model for a subset  of TRS is 
shown in Figure 5c. The metrics are specified in te rms of the 
properties associated with the architectural constr ucts. For 
example, the response time of a given execution sce nario is 
modeled as a summation of the computational delay o f its 
components.  
We have enhanced XTEAM with support for modeling go als 
(Figure 5a) and features (Figure 5b). As the arrow in Figure 5b 
indicates, an engineer specifies a mapping for each  feature to the 
underlying architectural model snippet that realize s it. The model 
snippet uses references to the constructs in the co re architectural 
model to specify the variation introduced by the co rresponding 
feature. For example, Figure 5c shows the impact of  selecting the 
Caching  feature on the core architectural model, i.e., it results in 
the addition of a new 
Cache  connector in 
between 
AgentDiscovery  and 
BusinessTier . 
When FUSION selects a 
set of features, the  
Figure 4. Stepwise adaptation. 
architectural snippets are weaved with the 
base (core) architectural model to form the 
complete architecture of the system. The 
generated architectural models are used at 
run0time (i.e., kept synchronized) with an 
implementation of the system running on top 
of Prism0MW [14]. Prism0MW is a 
middleware platform with extensive support 
for monitoring and dynamic adaptation. 
FUSION and the running system are 
integrated as follows: (1) Monitoring : Prism0
MW’s monitor services provide the 
information for FUSION in terms of raw 
readings of the metrics. (2) Adaptation : 
Whenever FUSION changes the feature 
selection, a new architectural model is 
generated, an architectural diff  is performed, 
and the differences are effected through the 
dynamic adaptation services of Prism0MW. 
FUSION sends the change requests in small 
steps (recall Figure 4) to avoid feature 
violations during the adaptation. 
Finally, we have integrated the FUSION’s 
modeling environment with WEKA [20], 
which provides an open source implementation of a n umber of 
learning algorithms [9] leveraged in our work. 
8. EVALUATION 
We have evaluated a prototype implementation of FUS ION 
described in the previous section using an extended  version of 
TRS, which consisted of 78 features and 8 goals. To  evaluate 
FUSION’s ability to learn and adapt under a variety  of conditions, 
we set up a controlled environment. We used XTEAM t o simulate 
the execution context of the software (e.g., worklo ad) as well as 
the occurrence of unexpected events (e.g., database  indexing 
failure). However, note that neither the TRS softwa re nor 
FUSION was controlled,  which allowed them to behav e as they 
would in practice. FUSION was executed on a dedicat ed Intel 
Quad0Core processor machine with 5GB of RAM. We  co nducted 
the evaluation under four different execution scena rios, which 
correspond to the four possible situations FUSION m ay face:  
(NT) Similar context —the system is placed under a workload 
setting that is comparable to that used during FUSI ON’s training. 
We use a scenario, called Normal Traffic  (NT),  in which the 
system is invoked with the typical expected number of requests.  
(VT) Varying context —the system is placed under a workload 
setting that is changing at run0time and different from that used 
during FUSION’s training. We use a scenario, called  Varying 
Traffic (VT), in which the system is invoked with a continuously 
changing inter0arrival rate of price quote requests . 
(IF) Unexpected event with emerging pattern —the system 
faces an unexpected change, which results in a new behavioral 
pattern (i.e., change in the impact of features on metrics) that can 
be learned. We use a scenario, called database  Index Failure  (IF) , 
in which the index of a database table used by the Agent 
Discovery  component during the execution of the make quote 
workflow (see Figure 1c) fails, and forces a full t able scan. 
(DoS) Unexpected event with no pattern — the system faces an 
unexpected change, which results in new random beha viors that 
cannot be accurately learned.  We use a scenario, called Randomized DoS Traffic (DoS),  in which the system is flooded 
with an online denial of service attack that does n ot follow a 
pattern (e.g., does not correspond to an exponentia l distribution).  
In our evaluation, an observation  corresponds to an adaptation 
decision and its effect. It consists of (1) a new f eature selection, 
and (2) the predicted and actual impact of the feat ure selection on 
metrics. An observation error  with respect to a metric is the 
difference between predicted and actual value. In t he experiments 
reported here, learning is initiated if the average  error in 10 most 
recent observations is more than 5%.  Other learnin g initiation 
policies are also possible and would present a trad eoff between 
learning overhead and accuracy.  
8.1  Accuracy of Learning 
Figure 6 shows the observation error for the Quote Response Time  
metric in the four scenarios described earlier. The  models selected 
for comparison are: (1) offline learning, which cor responds to a 
static learning model that is based on the same obs ervations used 
to train FUSION at design0time; and (2) Queueing Ne twork (QN) 
model, which assumes that workload and service dema nd 
parameters follow an exponetial distribution.  
Note that since each feature selection may result i n a different 
architectural model, and hence a different QN model , 
incorporating QN in our experiments was challenging . In 
particular, a large number of QN models is neededwo uld have to 
be developed (we estimated a total of 26×10 12 valid feature 
selections from the total search space of 2 78 ≈ 30×10 22 ), which 
corroborates our earlier assertion about the unwiel diness of using 
analytical models. This is while performance may be  only one 
goal of interest out of many in the system. In our accuracy 
comparisons reported below we constructed a subset of QN 
models that correspond to the feature selections ma de by 
FUSION. 
Figure 6a shows the TRS system under the NT scenari o, where 
both FUSION and offline learning come to less than 5% error on 
average (i.e., average of the last 10 observations,  which as  
Figure 5. Subset of TRS in our prototype implementa tion of FUSION: (a) goals 
and metrics, (b) feature model, (c) implementations  of Core and Caching  features.  
mentioned earlier is the criteria for initiating ru n0time learning). 
As exptected, this indicates that both FUSION and o ffline 
learning achieve good level of accuracy under expec ted execution 
conditions. QN also shows relatively good level of accuracy with 
average error rate of 2.9% and some spikes of 508% errors.  
Figure 6b shows the TRS system under the VT scenari o. This 
shows that even when the workload changes significa ntly, 
FUSION’s observation error remains within 5% error rate on 
average. As a result, a new behavioral pattern suff icient for run0
time learning never emerges. On the contrary, in th e case of QN, 
the wrong assumptions about service demands exacerb ate the 
prediction errors. 
Figure 6c shows the TRS system under the IF scenari o. It shows 
that when there are unexpected events in the system , FUSION is 
capable of learning the new behavior and adjusting its model. 
FUSION’s error rate increases up to 54% at the begi nning of the 
execution scenario. This error could be attributed to the fact that 
the model did not anticipate the impact of Caching  feature when 
the table scans were taking place in the AgentDiscovery  
component. As you may recall from Figure 1, Caching  reduces 
the need for agent discovery, hence it is more effe ctive in 
reducing the response time due to a full table scan  for each 
discovery. Caching  was estimated to be responsible for 35% of 
FUSION’s prediction error. Gradually, FUSION fine0t unes the 
coefficient of Caching  and other features in the learned functions. 
As a result, the observation error rate goes down t o less than 5%  
on average and the system reaches a steady state . In constrast, the 
prediction error of QN reaches 80%, since the QN mo del 
presumes the existence of a table index (i.e., the service demand 
of the queue representing the database in the model  is different). Figure 6d shows the the DoS scenario. The random na ture of 
traffic, makes it impossible for FUSION to converge  to an 
induced model that can predict the behavior of the system within 
the on average 5% error rate goal. As soon as a new  model is 
induced, the execution conditions change, making th e prediction 
models inaccurate. As a result, FUSION’s learning c ycle is 
periodically invoked to induce. Even though FUSION does not 
reach the same level of accuracy as in the other ex ecution 
scenarios, it is still significanlty better than QN  and offline 
learning. This can be attributed to the fact that F USION is 
benefiting from the continuous tuning, although it loses accuracy 
in the absense of a stable pattern. 
8.2  Adaptation in Presence of Inaccuracy 
Clearly the quality of adaptation decisions depends  on the 
accuracy of induced models. However, when the execu tion 
context changes, the model is forced to make some a daptation 
decisions under uncertainty, which are in turn used  to fine0tune 
the induced models and account for the emerging beh avior. An 
important concern is whether the adaptation decisio ns made 
during this period of time (i.e., using an inaccura te model) could 
worsen the violated goals or not. Figure 7 shows th e normalized 
impact of enabling F3 on metrics MG1  and MG3  in the first 
observation for each of the four scenarios of Figur e 6.  Recall 
from Figure 6 that the first observation for IF and  DOS 
correspond to a situation when there is a high0leve l of inaccuracy. 
In all cases, FUSION disables F3 with the purpose of  increasing 
MG1  and reducing MG3 . While due to the inaccuracy of the induced 
model FUSION fails to predict accurately the magnit ude of 
impact on these metrics, it gets the general direct ion of impact 
(i.e., positive vs. negative) correctly. This resul t is reasonable 
since a given feature typically has a similar gener al impact on 
metrics. For instance, one would expect an authenti cation feature 
to improve the system’s security, while degrading i ts 
performance. Hence, even in the presence of inaccur ate 
knowledge, FUSION does not make decisions that wors en the 
situation. Instead it makes decisions that are good , but not 
necessarily optimal, until the knowledge base is re fined.  
8.3  Overhead of Learning 
FUSION enables adjustment of the system to changing  conditions 
by continuously incorporating observation records i n the learning 
process. An important concern is the execution over head of the 
online learning. One of the principle factors affec ting learning 
overhead is the number of observations required to make accurate 
inductions. Table 3 lists the execution time for a given number of 
observations. Simple linear regression takes insign ificant amount 
of time with large number of observations, which ma kes it an 
appealing choice when there is a large number of ob servation 
(e.g., initial training at design0time, when large number of 
observations can be obtained). In our experiments F USION 
performed online learning on a maximum of 10 observ ations, 
which from Table 3 could be verified to have presen ted an 
insignificant overhead of less than 20 milliseconds . This  
Figure 6.  Accuracy of learned functions for “Quote Response 
Time” metric: (a) Normal Traffic, (b) Varying Traff ic, (c) 
Database Indexing Failure, (d) Randomized DoS Traff ic. 
 
Figure 7. Impact of Feature “Per- Request Authentication” on 
Metrics “Quote Response Time” and “Quote Quality”. efficiency is due to the pruning of the feature spa ce and 
significance test described in Section  5.   
8.4  Quality of Feature Selection 
We evaluate the quality of solution (feature select ion) found by 
FUSION against two competing technqiues. The first technique is 
Traditional Optimization  (TO ), which maximizes the global utility 
of the system, and includes all of the feature vari ables and goals in 
the optimization problem. The second technique is Constraint 
Satisfaction  (CS ), which finds a feature combination that satisfies  
all of the goals, regardless of the quality of the solution. As you 
may recall from Section  6, FUSION adopts a middle ground with 
two objectives: (1) find solutions with comparable quality to those 
provided by TO, but at a fraction of time it takes to executing TO, 
and (2) find solutions that are significantly bette r in quality than 
CS (i.e., stable fix), but with a comparable execut ion time.  
Figure 8 plots the global utility obtained from run ning the 
optimization at 3 different points in time for each  of the 4 
evaluation scenarios discussed earlier. Each data p oint represents 
the global utility value (recall the objective func tion in Section 
 6.2) obtained for each experiment. FUSION produces solutions 
that are only slightly less in quality than TO in a ll of the 
experiments. Note that TO finds the optimal solutio n. This 
demostrates that our feature space pruning heuristi cs do not 
significnatly impact the quality of found solutions . Table 4 shows 
the average number of features that are considered for solving 
each of the experiments, which is only a small frac tion of the 
entire feature space. Figure 8 also shows that FUSI ON find 
solutions that are significantly better than CS. In  turn, this 
corroborates our assertion in Section  6.2 that FUSION produces a 
stable fix to goal violations by placing the system  in a near0
optimal configuration. On the other hand, since CS may find 
borderline solutions that barely satisfy the goals,  due to slight 
fluctuations in the system, goals may be violated a nd thus 
frequent adaptations of the system ensue.  
Finally, we should point out that the quality of so lutions found in 
all of the methods, including FUSION’s approach, de pends on the 
accuracy of induced model. In particular, FUSION’s feature space 
pruning heuristics depend on this. In fact, the spi ke in the number 
of DoS features that are considered in Table 4 demo nstrates that 
feature space pruning heuristics were not as succes sful as other 
scenarios where a more accurate knowledge base was available.    8.5  Efficiency of Optimization 
In Section  6.2 we described how FUSION achieves efficient 
anaysis by using the knowledge base to dynamically tailor an 
optimization problem to the violated goals in the s ystem. In 
comparison, TO conducts a full optimization problem  where the 
complexity of the problem is O(2F). Figure 9 shows the execution 
time for solving the optimization problem in FUSION , TO, and 
CS for the same instances of TRS as those shown in Figure 8 and 
Table 4. Note that the execution time of FUSION is comparable to 
CS and is significanly faster than TO. This in turn  along with the 
results shown in the previous section demonstrates that FUSION 
is not only able to find solutions that are compara ble in quality to 
those found by TO, but also achieves this at a spee d that is 
comparable to CS. Note that since TO runs exponenti ally in the 
number of features, for systems with slighlty large r number of 
features, TO could take several hours for completio n, which 
would make it inapplicable for use at run0time.   
9. RELATED WORK 
Over the past decade, researchers and practitioners  have 
developed a variety of methodologies, frameworks, a nd 
technologies intended to support the construction o f self0adaptive 
systems [2]. We provide an overview of the most not able research 
in this area and examine them in light of FUSION.  
Architecture-based adaptation.  IBM’s Autonomic Computing 
initiative advocates a reference model known as MAP E [10], 
which is structured as hierarchical levels of feedb ack0control loop 
consisting of the following activities: Monitor, An alyze, Plan, and 
Execute. Oreizy et al. pioneered the architecture0b ased approach 
to run0time adaptation and evolution management in their seminal 
work [16]. Garlan et al. present Rainbow framework [5], a style0
based approach for developing reusable self0adaptiv e systems. 
Rainbow monitors a running system for violation of the invariant 
imposed by the architectural model, and applies the  appropriate 
adaptation strategy to resolve such violations. All  of the above 
approaches, including many others (e.g., see [2,15] ), share three 
traits: (1) use analytical models for making adapta tion decisions, 
and (2) rely on architectural models for the analys is, and (3) effect 
a new solution through architecture0based adaptatio n. These 
works have clearly formed the foundation of our wor k and have 
guided our research as manifested by the key role o f architecture 
in FUSION. However, unlike these approaches, FUSION  adopts a 
 
Figure 8. Global utility for different scenarios. 
 
Figure 9. Optimization execution time for different  scenarios.  
Table 3. Induction execution time in milliseconds 
# of Observations 50 500 528 822 903 1227 1809 
    M5 Model Tree 60 110 130 130 130 160 230 
    Linear Regression 20 30 30 50 60 70 80 
 Table 4. Effect of feature reduction heuristics.  
# of Features 
Considered NT  
1 NT  
2 NT  
3 VT 
1 VT 
2 VT 
3 IF 
1 IF 
2 IF 
3 DoS  
1 DoS  
2 DoS  
3 
  FUSION 3 1 2 3 4 5 3 4 5 6 13 11 
  CS / TO 78 
 feature0based approach to analysis and adaptation, which not only 
makes learning feasible, but also makes the analysi s efficient and 
reduces the effort required in applying FUSION to e xisting 
systems. Moreover, unlike them, FUSION is capable o f coping 
with unanticipated changes through learning.  
Policy-based adaptation.  Related to our research are adaptation 
frameworks that employ logic and policy based metho ds of 
induction. Sykes et al. [18] present an online plan ning approach to 
architecture0based self0managed systems. Based on a  three0layer 
model for self0management [13], their work describe s plan (i.e., a 
set of condition0action rules) generation with resp ect to a change 
in the environment or a system failure. Georgas and  Taylor [6] 
present a knowledge0based approach, such that the a daptation 
polices are specified as logic rules, which are in turn leveraged to 
induce new policies. These approaches bear resembla nce to our 
work in their use of induction. While policy0based approaches 
have been shown useful in some settings (e.g., ensu ring certain 
properties hold in the system), they cannot be used  for making 
quantitative analysis of QoS trade0offs. These appr oaches may 
also suffer from conflicting rules in the knowledge  base.  
Reinforcement learning adaptation.  Finally, related to our work 
are autonomic approaches that have employed reinfor cement 
learning. Kim and Park [11] propose a reinforcement  learning0
based approach to online planning for robots. Their  work focuses 
on improving the robot’s behavior by learning from prior 
experience and by dynamically discovering adaptatio n plans in 
response to environmental changes. Tesauro et al. [ 19] have 
proposed a hybrid approach that combines queueing n etwork with 
reinforced learning to make resource allocation dec isions in data 
centers. FUSION provides a general0purpose framewor k for self0
adaption of any feature0oriented application softwa re, which is 
fundamentally different from these domain0specific solutions.  
10. CONCLUSION 
We presented FUSION, a new method of engineering se lf0
adaptive systems that combines feature0orientation,  learning, and 
dynamic optimization to alleviate some of the cruci al challenges 
in this setting. Instead of relying on analytical m odels that are 
unwieldy for use and subject to wrong assumptions, FUSION uses 
online learning to analyze and self0tune the adapti ve behavior of 
the system to unanticipated changes. Learning is en abled by a 
dynamic feature0oriented representation of the syst em that 
incorporates the engineer’s knowledge of the applic ation and its 
domain. Learning in turn enables FUSION to dynamica lly tailor 
the optimization problem to the violated goals, and  hence achieve 
efficiency of analysis without trading accuracy. Us ing a prototype 
implementation of the system and a travel reservati on system we 
have extensively validated the approach and its pro perties.  
In our future work, we intend to investigate opport unistic self0
training as a way to detect emerging behaviors befo re adaptation 
decisions are made. We are exploring a self0trainin g method that 
takes place using a shadow clone of the running sys tem during 
periods of low utilization. In addition, we intend to empirically 
compare FUSION against other self0adaptation framew orks. 
11. ACKNOWLEDGMENTS 
This work is partially supported by grant CCF008200 60 from the 
National Science Foundation. 
12. REFERENCES 
[1] Carroll, D.J. 2002. Statistics Made Simple for School Leaders: Data-Driven Decision Making . 
ScarecrowEducation. 
[2] Cheng, B., et al. 2009. Software Engineering fo r Self0
Adaptive Systems: A Research Roadmap. Software 
Engineering for Self-Adaptive Systems, LNCS . 1026. 
[3] Edwards, G., Malek, S., Medvidovic, N. 2007. Sc enario0
Driven Dynamic Analysis of Distributed Architecture s. 
Int'l Conf. on Fundamental Approaches to Software 
Engineering  (Braga, Portugal, March 2007), 125. 
[4] Friedman, J.H. and Roosen, C.B. 1995. An introd uction to 
multivariate adaptive regression splines. Statistical 
Methods in Medical Research . 4, 3 (Sep. 1995), 1970217. 
[5] Garlan, D., Cheng, S.W. et al. 2004. Rainbow: 
Architecture0Based Self0Adaptation with Reusable 
Infrastructure. IEEE Computer . 37, 10 (Oct. 2004), 46054. 
[6] Georgas, J.C. and Taylor, R.N. 2004. Towards a 
knowledge0based approach to architectural adaptatio n 
management. Workshop on Self-healing Systems  (Newport 
Beach, California, October 2004), 59063. 
[7] Gomaa, H. 2004. Designing Software Product Lines with 
UML: From Use Cases to Pattern-Based Software 
Architectures . Addison0Wesley Professional. 
[8] Gross, D. and Harris, C.M. 1985. Fundamentals of 
queueing theory (2nd ed.).  John Wiley & Sons, Inc. 
[9] Jordan, M.I. and Jacobs, R.A. 1994. Hierarchica l mixtures 
of experts and the EM algorithm. Neural Comput.  6, 2 
(1994), 1810214. 
[10] Kephart, J.O. and Chess, D.M. 2003. The Vision  of 
Autonomic Computing. IEEE Computer . 36(1), 41050. 
[11] Kim, D. and Park, S. 2009. Reinforcement learn ing0based 
dynamic adaptation planning method for architecture 0based 
self0managed software. Workshop on Softw. Eng. For 
Adaptive and Self-Managing Systems  (Vancouver, Canada, 
May 2009), 76085. 
[12] Kleppe, A., Warmer, J. et al. 2003. MDA Explained: The 
Model Driven Architecture: Practice and Promise . 
Addison0Wesley Professional. 
[13] Kramer, J. and Magee, J. 2007. Self0Managed Sy stems: an 
Architectural Challenge. Int'l Conf. on Software 
Engineering  (Minneapolis, MN, May 2007), 2590268. 
[14] Malek, S., et al. 2005. A Style0Aware Architec tural 
Middleware for Resource0Constrained, Distributed 
Systems. IEEE Trans. Softw. Eng.  31, 3 (2005), 2560272. 
[15] Menascé, D.A., Ewing, J.M. et al. 2010. A fram ework for 
utility0based service oriented design in SASSY. Joint 
WOSP/SIPEW Int'l Conf. on Performance engineering  
(San Jose, CA, January 2010), 27036. 
[16] Oreizy, P., Medvidovic, N., Taylor, R. 1998. A rchitecture0
based runtime software evolution. Int'l Conf. on Software 
Engineering  (Kyoto, Japan, April 1998), 1770186. 
[17] Poladian, V., et al. 2004. Dynamic Configurati on of 
Resource0Aware Services. Int'l Conf. on Software 
Engineering  (Scotland, UK, May 2004), 6040613. 
[18] Sykes, D., et al. 2008. From goals to componen ts: a 
combined approach to self0management. Int'l Workshop on 
Software Engineering for Adaptive and Self-Managing  
Systems  (Leipzig, Germany, May 2008), 108. 
[19] Tesauro, G., et al. 2006. A Hybrid Reinforceme nt Learning 
Approach to Autonomic Resource Allocation. Int'l Conf. 
on Autonomic Computing  (Dublin, Ireland, June 2006), 650
73. 
[20] WEKA. http://www.cs.waikato.ac.nz/ml/weka/ . 