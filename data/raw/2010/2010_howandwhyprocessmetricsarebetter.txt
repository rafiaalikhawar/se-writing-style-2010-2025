Draft!How, and Why, Process Metrics are better
Foyzur Rahman
University of California, Davis
mfrahman@ucdavis.eduPremkumar Devanbu
University of California, Davis
ptdevanbu@ucdavis.edu
Abstract ‚ÄîDefect prediction techniques could potentially help
us to focus quality-assurance efforts on the most defect-prone
Ô¨Åles. Modern statistical tools make it very easy to quickly
build and deploy prediction models. Software metrics are at the
heart of prediction models; understanding how and especially
why different types of metrics are effective is very important
for successful model deployment. In this paper we analyze the
applicability and efÔ¨Åcacy of process andcode metrics from several
different perspectives. We build many prediction models across
85 releases of 12 large open source projects to address the
performance ,stability ,portability and stasis of different sets of
metrics. Our results suggest that code metrics, despite widespread
use in the defect prediction literature, are generally less useful
than process metrics for prediction. Second, we Ô¨Ånd that code
metrics have high stasis ; they don‚Äôt change very much from
release to release. This leads to stagnation in the prediction
models, leading to the same Ô¨Åles being repeatedly predicted as
defective; unfortunately, these recurringly defective Ô¨Åles turn out
to be comparatively less defect-dense.
I. I NTRODUCTION
Software-based systems pervade and greatly enhance modern
life. As a result, customers demand very high software quality.
Finding and Ô¨Åxing defects is expensive; defect prediction mod-
els promise greater efÔ¨Åciency by prioritizing quality assurance
activities. Since defect distribution is highly skewed [ 12,29],
such models can usefully Ô¨Ånger the most defective bits of code.
Typically defect prediction models rely on supervised learn-
ers, which use a labeled training dataset to learn the association
between measured entity properties (e.g., metrics calculated on
Ô¨Åles or methods), with the defect proneness of these entities.
Careful choice of metrics can improve prediction performance.
Researchers have mostly focused on two broad classes of
metrics for defect prediction: code metrics , which measure
properties of the code ( e.g., size and complexity), and process
metrics (e.g., number of changes, number of developers).
Researchers have long been interested in which class of metrics
(process or code) are better for defect prediction.
Moser et al. [20] compared the power of code and process
metrics on Eclipse project and found that process metrics
outperform code metrics. However, Menzies et al. [18] report
that code metrics are useful for defect prediction. Arisholm
et al. found that process and code metrics perform similarly in
terms of AUC but code metrics may not be cost-effective [2].
Our work deviates from existing approaches in two important
ways. Firstly, (and most importantly) we seek to understand
how andwhy process metrics are better for defect prediction.
Secondly, our methodology is squarely based on a prediction
setting. Existing studies mostly evaluate different types ofmetrics using cross-validations on few projects. However,
the most attractive use of models is in a prediction setting ;
such models can be used to focus cost-constrained quality
control efforts. In a release oriented development process, this
means training models on earlier releases to predict defects of
latter release. We build and compare prediction models across
multiple projects and releases, using different combinations
of metrics and learning techniques, using a broad range of
settings. Our experiments lead to the following contributions:
We compare the performance of different models in terms
of both traditional measures such as AUC and F-score,
and the newer cost-effectiveness [1] measures.
We compare the stability of prediction performance of
the models across time and over multiple releases.
We compare the portability of prediction models: how do
they perform when trained and evaluated on completely
different projects.
We study stasis ,viz.., the degree of change (or lack thereof)
in the different metrics, and the corresponding models
over time. We then relate these changes with their ability
to predict defects.
We investigate whether prediction models tend to favor
recurringly defective Ô¨Åles ; we also examine whether such
Ô¨Åles are relatively more defect-dense, and thus good targets
of inspection efforts.
II. B ACKGROUND AND THEORY
Defect prediction models are mostly built using supervised
learning techniques: logistic regression, SVM, decision trees
etc.. During training, the model systematically learns how to
associate various properties of the considered software entities
(e.g., methods, Ô¨Åles and packages) with the defect proneness
of these entities. Researchers have historically hypothesized
that properties of the code, measured using code metrics , could
usefully predict defect-proneness. Code metrics could measure
size (larger Ô¨Åles may be more defect-prone), or complexity
(more complicated Ô¨Åles may be more defect-prone). The value
of code metrics in defect prediction has been well-explored [ 13,
18, 31].
However, software development processes, per se , can be
quite complex. Lately, researchers have been interested in the
impact of development process on software quality. Recent
studies [ 5,21,23,25,28] suggest that an entity‚Äôs process
properties ( e.g., developer count, code ownership, developer
experience, change frequency) may be important indicators of
defect proneness.Draft!Clearly, the relative efÔ¨Åcacy of these metrics in defect
prediction is of vital concern to the practitioner. Should she
use all types of metrics? Should she mix different types of
metrics? Moser et al. [20] compared the prediction performance
of code and process metrics in three releases of Eclipse and
found that process metrics may outperform code metrics in
defect prediction. Arisholm et al. [2] compared various metrics
and prediction techniques on several releases of a legacy
Java middleware system named COS and found that code
metrics may perform well in terms of traditional performance
measures such as AUC while it may not be cost-effective.
Menzies et al. [18] found code metrics very effective for defect
prediction.
Existing studies mostly limit themselves to a cross-validation
based model evaluation on a limited set of projects. However,
many projects align their development and quality assurance
activities with releases. Therefore, a release-based evaluation of
model performance may be more appropriate for such settings.
Research Question 1: Inrelease-based prediction settings ,
how do the process and code metrics compare to predict
defect locations?
Release-based prediction performance may, however, desta-
bilize after a major shift of activities between releases. For
example, a release comprising a burst of new features might
be followed-up by a series of incremental quality-improving
releases. The process aspects that cause defects may also
shift, thus confounding prediction performance. Ekanayake
et al. [7] found that defect prediction models destabilize, viz..,
perform poorly over time due to project phase changes. We
therefore evaluate how process and product metrics affect the
prediction stability of models. Presumably, a more stable model
would also adapt better to project phase changes; armed with
this information, a practioner can then more effectively select
prediction metrics based on speciÔ¨Åc project dynamics.
Research Question 2: Are process metrics more/less
stable than code metrics?
Another interesting way of comparing process and code
metrics would be the portability of the learned prediction
models between different projects. Portability would be use-
ful for smaller software companies, without large, available
portfolios of software for training prediction models. Even big
companies with diverse product portfolios may Ô¨Ånd it useful
to port their prediction models to a new product. Portability
(also called ‚Äúcross-project prediction‚Äù) recently attracted quite
a bit of attention [ 15,24,26,27,30]; but we have not found
any that compare the portability of different types of metrics.
Research Question 3: Are process metrics more/less
portable than code metrics?Besides exploring the practical utility of metrics in defect
prediction models, we also seek to understand why one class
of metrics outperforms another, by exploring the distribution
of metrics values in more detail. One important property of
metrics is stasis . One can reasonably expect that, as a system
evolves, the distribution of defects does not remain unchanged.
Therefore, we might expect that the values of useful metrics
(ones with defect-prediction power) would also tend to vary
with release. Metrics whose values remain unchanged would
willy-nilly tend to predict the same Ô¨Åles as defective, release
after release.
Research Question 4: Are process metrics more/less
static than code metrics?
The above question asks if metrics change (or fail to
change) as software evolves; but even a very dynamic metric
is a useful predictor only if it co-evolves signiÔ¨Åcantly with
defect occurrence. Furthermore, usually, many metrics are used
together as a group in a prediction model. Even if a single
metric isn‚Äôt very dynamic, it might synergistically work together
with other (also static) metrics to form a good prediction model.
It is therefore important to understand whether models stagnate,
viz.., they tend to repeatedly predict the same Ô¨Åles as defective.
Research Question 5: Do models built from different
sets of metrics stagnate across releases?
Of course, it is possible that even stagnant models work: if
the same Ô¨Åles are recurringly defective, across multiple releases,
then a stagnant model that selects these defect-prone Ô¨Åles will
be reasonably successful. We hypothesize that stagnant models
predict defect distributions close to those they originally learned.
Perhaps they Ô¨Çag mostly the defective entities which don‚Äôt
change much, e.g., complicated Ô¨Åles which repeatedly become
defective. We consider such defective entities (defective in
both training release and test release) as recurringly defective
and the rest as incidentally defective . We hypothesize that a
stagnant model would predict mostly the recurringly defective
entities.
Research Question 6: Do stagnant models (based on
stagnant metrics) tend to predict recurringly defective
entities?
Arisholm et al. [2] report that code metrics perform well
(as well as process metrics) when considering cost- insensitive,
entity-based measures such as AUC, but not as well when
considering cost. While Arisholm et al. don‚Äôt explore the
reasons for this result, we believe this arises from prediction
bias away from defect density . Larger Ô¨Åles are more expensive
to inspect; if such investment does not pay off in terms
of number of defects uncovered, it‚Äôs not cost-effective. WeDraft!TABLE I: Studied Projects and Release Information
Project Description Releases Avg Files Avg SLOC
CXF Services Framework 2.1, 2.2, 2.3.0, 2.4.0, 2.5.0, 2.6.0 4038 :33 358846 :67
Camel Enterprise Integration Framework 1.4.0, 1.5.0, 1.6.0, 2.0.0, 2.2.0, 2.4.0, 2.7.0, 2.9.1 4600 :38 241668 :12
Derby Relational Database 10.2.2.0, 10.3.1.4, 10.4.1.3, 10.5.1.1, 10.6.1.0, 10.7.1.1, 10.8.2.2 2497 :29 530633 :00
Felix OSGi R4 Implementation 1.0.3, 1.2.0, 1.4.0, 1.6.0, 2.0.0, 2.0.3, 3.0.0, 3.2.0, 4.0.2 2740 :56 249886 :22
HBase Distributed Scalable Data Store 0.16.0, 0.18.0, 0.19.0, 0.20.0, 0.20.6, 0.90.1, 0.90.4, 0.94.0 934 :75 187953 :38
HadoopC Common libraries for Hadoop 0.15.0, 0.16.0, 0.17.0, 0.18.0, 0.19.0, 0.20.1 1047 :17 142257 :33
Hive Data Warehouse System for Hadoop 0.3.0, 0.4.0, 0.5.0, 0.6.0, 0.7.0, 0.8.0, 0.9.0 966 :29 152079 :86
Lucene Text Search Engine Library 2.0.0, 2.1.0, 2.2.0, 2.3.0, 2.4.0, 2.9.0, 3.0.0 990 :86 122527 :00
OpenEJB Enterprise Java Beans 3.0, 3.1, 3.1.1, 3.1.2, 3.1.3, 3.1.4, 4.0.0 2895 :43 225018 :43
OpenJPA Java Persistence Framework 1.0.0, 1.0.2, 1.1.0, 1.2.1, 1.2.2, 2.0.1, 2.1.0, 2.2.0 3181 :50 321033 :50
Qpid Enterprise Messaging system 0.5, 0.6, 0.8, 0.10, 0.12, 0.14, 0.16 1724 :00 198311 :86
Wicket Web Application Framework 1.4.0, 1.4.5, 1.4.9, 1.5.0, 6.0.0b2 2295 :20 152565 :40
TABLE II: Process Metrics
Short Name Description
COMM Commit Count
ADEV Active Dev Count
DDEV Distinct Dev Count
ADD Normalized Lines Added
DEL Normalized Lines Deleted
OWN Owner‚Äôs Contributed Lines
MINOR Minor Contributor Count
SCTR Changed Code Scattering
NADEV Neighbor‚Äôs Active Dev Count
NDDEV Neighbor‚Äôs Distinct Dev Count
NCOMM Neighbor‚Äôs Commit Count
NSCTR Neighbor‚Äôs Change Scatttering
OEXP Owner‚Äôs Experience
EXP All Committer‚Äôs Experience
examine whether code metrics have a prediction bias towards
larger Ô¨Åles (and thus, ones with lower defect density), and
whether such prediction bias emanates from the stagnation of
the models as discussed earlier.
Research Question 7: Do stagnant models have a predic-
tion bias towards larger, less-defect-dense Ô¨Åles?
III. E XPERIMENTAL METHODOLOGY
A. Projects Studied
Our 12 sample projects are listed in Table I. All are Java-
based, and maintained by Apache Software Foundation (ASF);
however, they come from a very diverse range of domains.
For each project we extracted the commit history from its GIT
repository1. We also used GIT BLAME on every Ô¨Åle at each
release to get the detailed contributor information. Our BLAME
process uses copy and move detection and ignores whitespace
changes, to identify the correct provenance of each line.
All 12 projects use JIRA2issue tracking system. From JIRA,
we extracted the defect information and the Ô¨Åxing commits
for Ô¨Åxed defects. Then, for each Ô¨Åxing commit, we extract
1http://git.apache.org
2https://issues.apache.org/jira/the changed lines, author, etc.from GIT. Any Ô¨Åles modiÔ¨Åed in
these defect-Ô¨Åxing commits are considered as defective.
B. Predicting Defects
Our defect-prediction studies are at Ô¨Åle-level. We choose
Ô¨Åle-level predictive (code and process) metrics that have been
widely used in defect prediction literature. We used a wide
range of learning techniques: Logistic Regression, J48, SVM,
and Naive Bayes, all from WEKA3. This reduces the risk of
dependence on a particular learning technique. Note, in the
default setting, J48 and SVM do not provide a probability
of defect proneness, which is important for cost-effectiveness
comparison. However, WEKA provides settings for J48 and
SVM that do yield probabilities4. As did Arisholm et al. [2],
we Ô¨Ånd that the prediction performance depends mostly on
the types of metrics used, and not on the learning technique.
Therefore, for brevity, we typically just report the Ô¨Åndings
from Logistic Regression, and mention deviations in other
techniques, if any.
All of our models are binary classiÔ¨Åers (predicted defec-
tivejclean ). However, a Ô¨Åle may have zero or more defects
Ô¨Åxed during a release. As in prior work, we declare a Ô¨Åle as
defective if it had at least one defect-Ô¨Åx, or clean otherwise.
Using both code and process metrics, we build models in a
release-based prediction setting: we train the model on a given
release and evaluate its performance on the next release. So,
e.g.a model trained on the k-th release, is only tested on
thek+ 1-th release. However, as discussed in section IV, for
stability we evaluate the model on all future releases following
a training release. For sensitivity analysis of other research
questions, we also evaluated all of our models using all future
releases instead of just the immediately succeeding release and
found consistent results.
Forportability , we evaluated models trained on one project
on all releases of other projects, ignoring time-ordering.
3http://www.cs.waikato.ac.nz/ml/weka/
4For J48, an option to use unpruned decision tree with Laplace smoothing
is available to Ô¨Ånd the estimate of defect proneness of each Ô¨Åle. Similarly,
WEKA‚Äôs SVM implementation allowed us to Ô¨Åt a logistic models to SVM
output to Ô¨Ånd the probability of defect proneness.Draft!TABLE III: Code Metrics
Type Metrics Count
File CountDeclMethodPrivate, AvgLineCode, CountLine, MaxCyclomatic, CountDeclMethodDefault, AvgEssential, CountDeclClass-
Variable, SumCyclomaticStrict, AvgCyclomatic, AvgLine, CountDeclClassMethod, AvgLineComment, AvgCyclomaticModiÔ¨Åed,
CountDeclFunction, CountLineComment, CountDeclClass, CountDeclMethod, SumCyclomaticModiÔ¨Åed, CountLineCodeDecl,
CountDeclMethodProtected, CountDeclInstanceVariable, MaxCyclomaticStrict, CountDeclMethodPublic, CountLineCodeExe, Sum-
Cyclomatic, SumEssential, CountStmtDecl, CountLineCode, CountStmtExe, RatioCommentToCode, CountLineBlank, CountStmt,
MaxCyclomaticModiÔ¨Åed, CountSemicolon, AvgLineBlank, CountDeclInstanceMethod, AvgCyclomaticStrict37
Class PercentLackofCohesion, MaxInheritanceTree, CountClassDerived, CountClassCoupled, CountClassBase 5
Method CountInput, CountOutput, CountPath, MaxNesting 12
C. Process Metrics
Our Ô¨Åle-based process metrics are listed in Table II. All
process metrics are release-duration. COMM measures the
number of commits made to a Ô¨Åle. ADEV is the number of
developers who changed the Ô¨Åle. DDEV is the cumulative
number of distinct developers contributed to this Ô¨Åle up to this
release. ADD andDEL are the normalized (by the total number
of added and deleted lines) added and deleted lines in the Ô¨Åle.
OWN measures the percentage of the lines authored by the
highest contributor of a Ô¨Åle. MINOR measures the number of
contributors who authored less than 5% [ 5] of the code in that
Ô¨Åle.OEXP measures the experience of the highest contributor
of that Ô¨Åle using the percent of lines he authored in the project
at a given point in time. EXP measures the geometric mean
of the experiences of all the developers. All these metrics are
drawn from prior research [2, 5, 19, 22].
We also used a simple line based change entropy metric [ 9],
derived from the location of the changes made: SCTR measures
the scattering of changes to a Ô¨Åle; scattered changes could be
more complex to manage, and thus more likely to induce
defects. SCTR is the standard position deviation of changes
from the geographical centre thereof.
Kim et al. [11]‚Äôs celebrated BugCache is populated using
a simple co-commit history. This work suggests a range of
different process metrics based on co-commit-neighbors. For a
given Ô¨Åle Fand release R, these metrics are based on the list
of Ô¨Åles co-committed with F, weighted by the frequency of
co-commits during R.NADEV ,NDDEV ,NCOMM andNSCTR
are just the co-commit-neighbor measures of ADEV ,DDVEV ,
COMM and SCTR . We measured the usefulness of all the
neighbor based metrics, as well as SCTR using single variable
predictor models, and found them to be highly signiÔ¨Åcant
defect predictors with a median AUC of around 0.8. All of our
process metrics are cumulated and measured on a per-release
basis.
In this paper, we focus on measuring prediction performance,
rather than testing hypotheses. Therefore issues such as VIF,
goodness of Ô¨Åt, variable signiÔ¨Åcance etc. were not such a
concern and following the suggestion of Menzies et al. [18],
we simply use all the available variables.D. Code Metrics
We used UNDERSTAND from Scitools5to compute code
metrics. All 54 code metrics are listed in Table III. Our metrics
set includes complexity metrics such as Cyclomatic complexity,
essential complexity, number of distinct paths, fan in, fan
out etc.; V olume metrics such as lines of code, executable
code, comment to code ratio, declarative statement etc.; and
Object oriented metrics such as number of base classes, number
of children, depth of inheritance tree etc. Space limitations
inhibit a detailed description; however, these metrics are well
documented at the U NDERSTAND website6. Most metrics are
Ô¨Åle-level; some metrics, ( e.g.OO metrics) are class level. Since
all projects are Java-based, most Ô¨Åles contain a single class;
so we aggregate class level metrics to Ô¨Åle level using max.
Similarly some metrics are available only at method level
(such as fan-in, fan-out) and we aggregate those at Ô¨Åle level
using min, max andmean .
E. Evaluation
All of our prediction models output probabilities of defect
proneness of Ô¨Åles. To classify a Ô¨Åle as defective , one can use
varying minimum thresholds on the probability value. Most
models are fallible; thus different choices of threshold will
give varying rates of false positives/negatives (FP/FN), and
true classiÔ¨Åcations (TP/TN)
Accuracy of the model is the proportion of correct predictions,
and is deÔ¨Åned asTP+TN
TP+FP+TN+FN. Ma et al. [14] noted, that
in a highly class imbalanced data set with very few defective
entities, accuracy is not a useful performance measure: even
a poor model that declares all Ô¨Åles as clean will have high
accuracy.
Precision measures the percentage of model declared defective
entities that are actually defective. PRECISION is deÔ¨Åned as
TP
TP+FP. Low-precision models would waste precious quality-
control resources.
Recall identiÔ¨Åes the proportion of actually defective entities
that the model can successfully identify. Model with low recall
5http://www.scitools.com/
6http://www.scitools.com/documents/metricsList.php?Draft!
100755025100755025Percent of LOCPercent of Bugs FoundOPR
755025100755025Percent of LOCPercent of Bugs Found10020%P1P2RFig. 1: Cost Effectiveness Curve. On the left, O is the optimal, R
is random, and P is a possible, practical , predictor model. On the
right, we have two different models P1 and P2, with the same overall
performance, but P2 is better when inspecting 20% of the lines or
less. Figure quoted from our FSE 2011 paper [25]
would be unable to Ô¨Ånd most of the defects. Recall is identiÔ¨Åed
asTP
TP+FN.
A good model yields both high PRECISION and high
RECALL . But, it is well known that increasing one often reduces
the other; hence the F-measure .
F-Measure is the harmonic mean of PRECISION andRECALL .
All of these measures require the use of a minimum
probability threshold to declare a Ô¨Åle defective. Lessermann
et al. [13] decry the use of performance measures that require
an arbitrary threshold. Mende et al. [16] argue that threshold
based performance measures make replication difÔ¨Åcult. Ar-
isholm et al. [2] argues that in the context of SE considering
defect proneness as continuous and prioritizing resources in
the order of the predicted defect proneness is more suitable.
This brings us to measures that are threshold invariant.
ROC Receiver operating characteristic (ROC) is a curve
that plots the true positive rates ( TPR =TP
TP+FN) against
False Positive Rates for all possible thresholds between 0
and1. This threshold invariant measure gives us a 2-D curve,
which passes through (0;0)and(1;1). The best possible model
would have the curve close to y= 1, with and area under the
curve ( AUC) close to 1:0.AUC always yields an area of 0:5
under random-guessing. This enables comparing a given model
against random prediction, without worrying about arbitrary
thresholds, or the proportion of defective Ô¨Åles.
Cost-Effectiveness The above measures ignore cost-
effectiveness. Consider a model that accurately predicts defec-
tive Ô¨Åles, but orders those Ô¨Åles in increasing order of defect
density. Then we might allocate resources ( e.g., code inspectors)
to the least defect dense Ô¨Åles. Assuming cost proportional to
the size of the Ô¨Åle , this would be undesirable. In contrast, a cost-
effective model would not only accurately predict defective
Ô¨Åles, but also order those Ô¨Åles in decreasing order of defect
density.
Analogous to ROC, we can have a cost-effectiveness curve
(refer to left plot of Ô¨Ågure 1), plotting the proportion of
defects against proportion of SLOC coming from the ordered(using predicted defect proneness) set of Ô¨Åles. Unlike ROC,
however, we only consider a small portion of area under the
cost-effectiveness curve ( AUCEC ), tailored for the resource
constraints. Thus, during deadlines, we may only consider
the cost-effectiveness for at most 10% SLOC, while other
times we may consider 20% SLOC. Based on the choice of
the maximum proportion of SLOC that a manager may be
interested to consider, different models may become more or
less competitive. E.g., in the right plot of Ô¨Ågure 1, P2is a
better cost-effective model up to around 50% SLOC, after that
P1performs better.
In this paper we use AUC,AUCEC at10% (AUCEC 10) and
20% (AUCEC 20) SLOC to compare the models‚Äô performance
in a threshold invariant manner. However, to give readers a
way to compare with existing literature, we initially report
F-Measure at 0:5threshold ( F50), as used by the inÔ¨Çuential
Zimmermann et al. paper [31].
To compare process and code metrics, we evaluate 4
combinations of metrics. First, we build the model with only
process metrics. Next we build the model with just code
metrics. We then build the model with process metrics and size
(CountLineCode in table III). Size is a very important metric
by itself and there is a considerable body of evidence that
size is highly correlated with most product metrics including
McCabe and Halstead metrics [ 8,10]. This is to ensure that
we can separate the inÔ¨Çuence of the combination of size and
process metrics from the entire collection of process and code
metrics. Finally, we build the model with the entire collection
of process and code metrics. Following Menzies et al. [18]
and others, we Log transformed all of our process and code
metrics. This transformation signiÔ¨Åcantly improves prediction
performance.
IV. R ESULTS
We begin with comparing the performance of process and
code metrics in release based prediction settings using AUC,
AUCEC and F 50.
RQ 1: In release based prediction settings, how do the process
and code metrics compare to predict defect locations?
Figure 2 compares the AUC of different types of metrics
for four different classiÔ¨Åers. The metrics are marked in x-axis
as ‚ÄúP‚Äù: Process, ‚ÄúC‚Äù: Code, ‚ÄúS‚Äù: Process & Size, and ‚ÄúA‚Äù:
All (process and code metrics combined). We compared the
AUC performance for all types of metrics for a given learning
technique using Wilcoxon tests and corrected the p-values using
Benjamini-Hochberg (BH) correction. We also do the same to
compare the performance of different learning techniques for
a given set of metrics. We Ô¨Ånd that process metrics always
perform signiÔ¨Åcantly better than code metrics across all learning
techniques, with very low p value ( p < 0:001). However,
process metrics and size together may not perform any better
than process metrics alone (all p values were insigniÔ¨Åcant).
Combining process and code metrics together also doesn‚Äôt yieldDraft!
Logistic J48 SVMNaiveBayes
0.40.60.81.0
‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
PCSAPCSAPCSAPCSAAUC
Process CodeProcess+Size All(a) AUC performance
Logistic J48 SVMNaiveBayes
0.00.20.40.60.8
‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
PCSAPCSAPCSAPCSAF_50
Process CodeProcess+Size All (b)F50performance
Logistic J48 SVMNaiveBayes
0.000.050.100.15
‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
PCSAPCSAPCSAPCSAAUCEC_20
Process CodeProcess+Size All (c)AUCEC 20performance
Fig. 2: Performance of different classiÔ¨Åers and metrics
any better performance than process metrics alone. Similar to
Menzies et al. [14], we Ô¨Ånd that for code metrics, NaiveBayes
works best, while J48 and SVM may not perform as well.
However, Logistic regression performs well for all types of
metrics and is at a statistical dead heat with NaiveBayes for
code metrics ( p= 0:664).
We also present the F50and AUCEC 20in Ô¨Ågure 2. Again, we
compare different classiÔ¨Åers and set of metrics and found that
process metrics easily outperform code metrics in terms of both
F50and AUCEC (p < 0:001in both cases). Particularly, similar
to Arisholm et al. ‚Äôs Ô¨Åndings [ 2], we found code metrics are
less effective for cost-effective prediction. Interestingly, while
NaiveBayes is the best classiÔ¨Åer for code metrics [ 18] when
measured using AUC, but does worst for AUCEC (p < 0:05
after BH correction against all other classiÔ¨Åers).
In general, our results show models using code metrics
provide reasonable AUC, albeit not as good as models using
process metrics. For AUCEC 20, code metrics don‚Äôt do much
better than random: different learning techniques don‚Äôt help
much either. Therefore, for brevity, we only report results
only from Logistic regression (LR), which does well for both
process and code metrics; indeed LR yields better AUCEC than
NaiveBayes for code-metrics based models.
Next we report results on the stability of prediction models:
as discussed in [ 6], models with stable prediction performance
are more useful in practice, specially so in rapidly evolving
projects.
RQ 2: Are process metrics more/less stable than code metrics?
We evaluate stability by using all available older releases to
predict newer ones. Development activity on a project evolves
with time, and older releases may be quite different from newer
ones; this approach is a good test of the prediction stability of
a set of metrics. Therefore, for this RQ, rather than predicting
an immediately subsequent release, we predict all releases in
a project following a training release.
Process Code All
0.40.60.81.0
‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè
123451234512345
Release DifferenceAUC(a) AUC Stability
Process Code All
0.000.050.100.15
‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè ‚óè‚óè‚óè
‚óè
‚óè
123451234512345
Release DifferenceAUCEC_20 (b) AUCEC 20Stability
Fig. 3: Stability of different metrics
Figure 3a shows the stability of AUC when predicting 1 thru
5 releases in the future. For brevity, only results from LR are
presented; other learning models give similar results. We see a
perceptible downward trend in performance as we train on older
releases to predict newer releases. However, 2-sample Wilcox
tests on all pairs of boxplots ( viz., for both consecutive and
non-consecutive releases) for each types of metrics provide no
statistical signiÔ¨Åcance of this trend. We observe similar result
in terms of AUCEC 10and AUCEC 20(Ô¨Ågure 3b), for either code
or process metrics. Earlier Ô¨Åndings of instability were based
on a continuous time line [ 6]. Releases may be capturing more
wholesome encapsulations of similar activities, than do equal
time-intervals; although some releases may take longer than
others, all releases may group activities in the same way. Thus,
inherently, release-based models may be less susceptible to
concept drift [6].
Portability of prediction models across projects may be quite
important for some organizations, specially newer or rapidly
changing organizations. Cross-project defect prediction recently
attracted quite a bit of attention [ 15,26,27,30]; however, to
our knowledge, none compare the relative merits of different
types of metrics in such settings.Draft!
Process Code All
0.20.40.60.81.0
‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè ‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè
WithinCrossWithinCrossWithinCross
PortabilityAUC(a) AUC Portability
Process Code All
0.000.050.100.15
‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
WithinCrossWithinCrossWithinCross
PortabilityAUCEC_20 (b) AUCEC 20Portability
Fig. 4: Portability of different metrics
RQ 3: Are process metrics more/less portable than code metrics?
Figure 4a and 4b compares the portability of models
for different sets of metrics in terms of AUC and AUCEC 20.
Performance degrades in cross-project settings for both process
and code metrics; the degradation is statistically signiÔ¨Åcant with
low p-values (less than 0:001). It‚Äôs also clear that code metrics
show a larger decline of performance than process metrics and
the notches of code metrics based boxplots are nearly non-
overlapping. We do see a large number of outliers in process
metrics, suggesting less portability in some cases. We observed
similar pattern for AUCEC 10(p value from Wilcox test is less
than 0:001for code metrics, 0:024for process metrics, and
0:009for all metrics) and AUCEC 20(p value from Wilcox test
is less than 0:001for code metrics, 0:031for process metrics,
and0:006for all metrics), with process metrics showing more
portability than code metrics.
Our Ô¨Åndings so far indicate that code metrics are less
stable andless portable than process metrics. Why do code
metrics show such high ‚Äúresistance to change‚Äù? We guessed
that common code metrics are less responsive to development
activities, and thus less tied to factors that inÔ¨Çuence defect-
proneness. For example, a defect introduced by replacing
astrncp call with a strcp , wouldn‚Äôt affect code metrics,
but would affect process attributes such as the modiÔ¨Åcation
time, number of active developers, ownership, etc.. The joint
distribution of metrics and defects are estimated and exploited
by most learning algorithms; relatively ‚Äúchange-resistant‚Äù (more
static) metrics might do poorly at tracking changes in defect
occurrence.
RQ 4: Are process metrics more/less static than code metrics?
We use the Spearman correlation of each metric of every
Ô¨Åle between two successive releases as a measure of stasis.
We then combine all the correlations in a violin plot for each
group of metrics. This would tell us how similar a group of
metrics look in two successive releases. Figure 5a presents
the comparison of our Spearman-correlation stasis measure for
‚àí0.20.00.20.40.60.81.0
Process CodeSpearman Correlations(a)Stasis of Metrics
‚àí0.20.00.20.40.60.81.0
‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
Process Code AllSpearman Correlations (b)Stasis of Models
Fig. 5: Comparing stasis of different metrics and models
process and code metrics. As we can see from the Ô¨Ågure, code
metrics are highly correlated, and therefore changes very little
over releases. Process metrics, on the other hand, show quite
a range of stasis. While the median value of stasis in process
metrics is under 0:5, the top quartile is over 0:9!
To examine this disparity further, we ranked the process
metrics using the median value of our stasis measure for that
metric. The ranking of metrics, in decreasing order of stasis,
was OEXP, EXP, OWN, MINOR, DDEV , SCTR, COMM,
ADEV , NSCTR, NCOMM, NDDEV , NADEV , DEL, and ADD.
Interestingly, the top 5(OEXP thru DDEV) have a very high
median stasis score of over 0:93while the rest 9have a median
stasis score of under 0:32. This disparity between mean and top-
quartile prompted us to examine in more detail the effects of
process-metrics stasis on predictive power. There are 5metrics
with high stasis, and 9that show low stasis. We hypothesized
that stasis is important for predictive power and that low-stasis
measures make better predictors.
For comparison purpose, we chose all 5high-stasis process
metrics, and chose different groups of 5from the low-stasis
process metrics. We used a sliding window based approach to
select 5groups of 5low-stasis metrics: the 5groups were the
metrics ranked 1: : :5(by stasis of low to high correlations),
and then those ranked 2: : :6, all the way up to 5: : :9. We then
built prediction models from all groups of 5 and additionally
the high stasis metrics group of ranks 10: : :14. This gave
us a total of 6models, 5based on low stasis metrics and 1
based on high stasis metrics. We found that the median AUC of
models built from metrics with low correlations are over 0:9,
while the AUC of models built from highly correlated process
metrics are barely around 0:8. A paired Wilcox test of each
of the models of low stasis metrics against the model from
high stasis metrics suggests that models of low stasis metrics
are statistically signiÔ¨Åcantly better predictors than models with
high stasis metrics for all performance criteria ( AUC,AUCEC 10,
AUCEC 20, and F 50) with very low p-values (less than 0:001).
This case-study above suggests that stasis plays an important
role in predictive power. We attempted to replicate this study
for code metrics, comparing high-stasis and low-stasis code
metrics; but unfortunately, code metrics generally have very
high stasis, around 0:96. This suggests that models basedDraft!
Recurring Training Only Test Only
0.20.40.60.81.0
‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
PCAPCAPCAAverage Normalized Rank(a)Ability to rank Ô¨Åles according
to the change of defect proneness
4.04.55.05.56.06.5
0.010.020.030.04
‚óè‚óè‚óè‚óè‚óèLog SLOC Defect Density
Recurring Test Only(b)SLOC and defect density for
change of defect proneness
Fig. 6: Ranking ability, Log SLOC and defect density for change of
defect pronenes
on code metrics would also have high stasis : these models
would repeatedly predict the same Ô¨Åles over and over again as
defective.
Prediction models learn the joint distribution of metrics
and defect proneness from the training release, and use this
knowledge to predict defects in the test release. The probability
assigned by models to Ô¨Åles in the test release reÔ¨Çect this learned
defect distribution. In an active project, as different parts of
the system become foci of attention and activity, defects gets
introduced into different parts. There is thus every reason to
expect that the defect distribution in subsequent releases should
differ: Ô¨Åle fthat was buggy in release kisn‚Äôt necessarily also
buggy in k+ 1 and vice versa. We would therefore expect
that a good model wouldn‚Äôt stagnate ,viz., it would generally
indicate a change of defect probability in Ô¨Åles from release to
release.
RQ 5: Do models built from different sets of metrics stagnate
across releases?
The rank correlation between the predicted probabilities
(from test set) and the learned probabilities (from training set)
is one measure on the model‚Äôs adaptation in an active project.
A high rank correlation suggests that the model is probably
predicting the same set of Ô¨Åles as defective. Figure 5b, shows
the value range of Spearman correlations between probabilities
of defect proneness across all pairs of training-test releases.
The difference is stark: the code metrics based models are
essentially spitting out the original probabilities it learned from
the training data. This clearly indicates that the stasis of code
metrics leads to stagnant prediction models, that predict the
same Ô¨Åles as defective over and over again.
To be fair, even stagnant models might be useful in a project
where same set of Ô¨Åles become recurringly defective: identify-
ing a critical set of recurringly defective Ô¨Åles unambiguously
might still help focus testing and inspection.RQ 6: Do stagnant models (based on stagnant metrics) tend to
predict recurringly defective entities?
To evaluate the effect of stagnation, we partition the set of
Ô¨Åles in three sets, based on how much their defect-proneness
changes. Included in Set 1 are Ô¨Åles which are defective in both
training and test; these recurring Ô¨Åles should be ‚Äúeasy prey‚Äù
for stagnant models. In contrast (set 2) Ô¨Åles which are defective
in the training set but notin the test set, are bait which might
trap a stagnant model into a false positive prediction. Finally
Ô¨Åles which are defective in the test set, but not in the training
set, are decoys , which might mislead a stagnant model into a
false negative prediction.
For each type of metrics and every test release , we rank the
Ô¨Åles in that release by their predicted defect proneness. We then
normalize these ranks using the maximum possible rank (which
is the number of Ô¨Åles in the associated test release) and partition
the normalized ranks in three sets as discussed above. We can
then compare the averaged normalized ranks (averaged by the
size of the partition) across different partitions for different test
releases and metrics types. Normalization allows comparison
between releases with varying numbers of Ô¨Åles. A better model
produces a higher normalized ranks for defective Ô¨Åles.
Figure 6a compares average normalized ranks for different
types of metrics for different defect occurrences. As we can
see from the Ô¨Ågure, even for just the recurringly defective
Ô¨Åles (left plot), process metrics outperform code metrics.
Process metrics also (middle plot) avoid ranking ‚Äútraining only‚Äù
defective Ô¨Åles higher, thus doing better at avoiding such false
positives. Finally, process metrics can sniff out the Ô¨Åles with
newly introduced defects (‚Äútest only‚Äù) better. We compared
the average normalized rank of the models from each pair of
metrics using Wilcoxon test and corrected the p-values using
BH correction. The p-values conÔ¨Årm statistical signiÔ¨Åcance of
the superiority of process metrics over code metrics. Process
metrics were always better (lower normalized rank for ‚Äútraining
only‚Äù, and higher normalized rank for other two cases) than
code metrics ( p < 0:001). Process metrics also outperformed
all metrics with lower ‚Äútraining only‚Äù ranking ( p < 0:001).
This suggests that in all three defect occurrence scenarios,
process metrics are better-suited for prediction models.
All our Ô¨Åndings clearly indicate that process metrics are
superior to code metrics for building prediction models. Still
code metrics are easy to use: multiple tools support easy
gathering of code metrics. Their AUC (around 0:8) performance,
though inferior to process metrics, is not bad. If process metrics
are hard to use, should one use code metrics instead?
Prior work by Arisholm et al. [2], in a cross-validation
evaluation suggests that in a cost-constrained setting, code
metrics are not as useful, and indeed, when measured with
AUCEC , don‚Äôt do much better than random. We supplement
Arisholm et al. with evaluation in a prediction setting, to
understand why code metrics perform so poorly in terms
ofAUCEC , while giving reasonable AUC. As both AUC andDraft!AUCEC rely on the ordering of defective entities based on the
predicted defect probabilities, we conjecture that code metrics
based models are clearly prioritizing less defect dense Ô¨Åles.
Furthermore, our Ô¨Åndings suggest that code-metrics models are
fairly good at predicting recurringly defective Ô¨Åles. These two
pieces of evidence suggests that code metrics tend to predict
recurringly defective, but not very defect-dense Ô¨Åles .
Question: Are recurringly defective Ô¨Åles larger and less defect
dense, thereby rendering the models, with prediction bias
towards such Ô¨Åles, less cost- effective?
Figure 6b compares the Log SLOC and defect density of
Ô¨Åles that were recurringly defective (defective in both training
and test release), with the Ô¨Åles that only became defective in the
test release. As is evident from the Ô¨Ågure, larger Ô¨Åles are less
defect dense and more likely to stay defective in subsequent
releases. We also conÔ¨Årmed the statistical signiÔ¨Åcance of
this phenomenon using Wilcoxon test with BH correction.
Recurringly defective Ô¨Åles are statistically signiÔ¨Åcantly larger
(p < 0:001) than Ô¨Åles that are only defective in the test set. At
the same time, Ô¨Åles that are defective only in the test set, have
statistically signiÔ¨Åcantly higher defect density ( p < 0:001).
Given our observed prediction bias of code metrics based
models towards recurringly defective Ô¨Åles, such models would
be at a disadvantage to predict cost-effectively. Moreover, the
inability of code metrics based prediction models to predict
newly introduced defects, which may be more defect dense,
would only worsen the cost-effectiveness of such models.
V. T HREATS TO VALIDITY
Data Quality We use a large selection of projects from
different domains. All projects use a high-Ô¨Ådelity process to
link bug-Ô¨Åxing commits to issues in JIRA issue tracking system.
Our projects have a median defect linking rate of over 80%,
which is much higher than reported (typically under 50%) in
the literature [3].
Completeness of Code Metrics We only used code
metrics as available from Scitool‚Äôs popular UNDERSTAND tool.
UNDERSTAND does not generate all possible code metrics ever
reported or used in literature. However, it does produce a large
set of diverse code metrics. Furthermore, the AUC of our code
metrics based models are similar or better than that reported
by Arisholm et al. [2]. Our comparisons of process and code
metrics based models have large effect sizes; thus our results
appear fairly robust to the choice of code metrics.
Completeness of Process Metrics Our set of process
metrics are easily obtained, and based on a single release.
We used a diverse set of process metrics widely used in
the literature ranging from code ownership and developer
experience to Ô¨Åle activities. Our neighborhood-based metrics
are motivated by highly cited research of Kim et al. [11]. Our
location of change metric (SCTR) measures simple change
entropy [ 9]. We therefore argue that our set of process metrics
are comprehensive.
Stability Analysis We study stability in the context of
releases, instead of a continuously shifting time window likeEkanayake et al. [6]. We argue that each release is a self
contained logically deÔ¨Åned epoch, more easily comparable to
other releases; furthermore many budgeting/resource decisions
are based on a release granularity. Thus we believe this is a
suitable granuarity for evaluating prediction models.
Generalizability We use large number of projects consisting
of85releases. We observed small variances in all of our
Ô¨Åndings. To avoid ecological fallacy [ 22], we also compared
our Ô¨Åndings in a per project setting, and got very similar
results. Therefore we believe our result should be generalizable
in similar application domains and development dynamics
in a release oriented prediction context. However, all of our
projects are developed in Java, and are OSS. There is a deÔ¨Ånite
threat to generalizability based on the fact that all are Apache
projects; however, we believe this threat is ameliorated by
the diversity of the chosen projects. Also, our Ô¨Åndings may
be less generalizable for commercial projects, which have a
completely different governance style, and may demonstrate
different inÔ¨Çuence of process metrics on defect-proneness of
Ô¨Åles. Therefore we hope other researchers with access to
commercial data would replicate our Ô¨Åndings.
VI. C ONCLUSION
We studied the efÔ¨Åcacy of process and code metrics for
defect prediction in a release-oriented setting across a large
number of releases from a diverse set of projects. We compared
models from different types of metrics using both cost-
sensitive and AUC based evaluation across different objectives
of performance, stability and portability to understand ‚Äúwhen‚Äù
a set of metrics may be suitable for an organization. Our
results strongly suggest the use of process metrics instead
of code metrics. We also try to understand ‚Äúwhy‚Äù a set of
metrics may predict a type of defect occurrence by focusing
on the stasis of metrics. Our Ô¨Åndings surprisingly show that
code metrics, which is widely used in the literature, may not
evolve with the changing distribution of defects, which leads
code-metric-based prediction models stagnating , and tending
to focus on Ô¨Åles which are recurringly defective. Finally, we
observed that such recurringly defective Ô¨Åles are larger and
less defect dense, therefore these large Ô¨Åles may compromise
the cost-effectiveness of the stagnant code-metric-based models
with a prediction bias towards such Ô¨Åles.
REFERENCES
[1]E. Arisholm, L. C. Briand, and M. Fuglerud. Data mining
techniques for building fault-proneness models in telecom
java software. In ISSRE , pages 215‚Äì224. IEEE Computer
Society, 2007.
[2]E. Arisholm, L. C. Briand, and E. B. Johannessen. A
systematic and comprehensive investigation of methods to
build and evaluate fault prediction models. JSS, 83(1):2‚Äì
17, 2010.
[3]C. Bird, A. Bachmann, E. Aune, J. Duffy, A. Bernstein,
V . Filkov, and P. Devanbu. Fair and balanced?: bias in
bug-Ô¨Åx datasets. In Proceedings of the the 7th FSE , pages
121‚Äì130. ACM, 2009.Draft![4]C. Bird, N. Nagappan, H. Gall, B. Murphy, and P. T.
Devanbu. Putting it all together: Using socio-technical
networks to predict failures. In ISSRE , pages 109‚Äì119.
IEEE Computer Society, 2009.
[5]C. Bird, N. Nagappan, B. Murphy, H. Gall, and P. T.
Devanbu. Don‚Äôt touch my code!: examining the effects
of ownership on software quality. In T. Gyim ¬¥othy and
A. Zeller, editors, SIGSOFT FSE , pages 4‚Äì14. ACM,
2011.
[6]J. Ekanayake, J. Tappolet, H. C. Gall, and A. Bernstein.
Tracking concept drift of software projects using defect
prediction quality. In Proceedings of the 2009 6th IEEE
International Working Conference on Mining Software
Repositories , MSR ‚Äô09, pages 51‚Äì60, Washington, DC,
USA, 2009. IEEE Computer Society.
[7]J. Ekanayake, J. Tappolet, H. C. Gall, and A. Bernstein.
Time variance and defect prediction in software projects -
towards an exploitation of periods of stability and change
as well as a notion of concept drift in software projects.
Empirical Software Engineering , 17(4-5):348‚Äì389, 2012.
[8]K. El Emam, S. Benlarbi, N. Goel, and S. Rai. The
confounding effect of class size on the validity of object-
oriented metrics. IEEE TSE , 27(7):630‚Äì650, 2001.
[9]A. E. Hassan. Predicting faults using the complexity of
code changes. In ICSE , pages 78‚Äì88. IEEE, 2009.
[10] T. Khoshgoftaar and J. Munson. Predicting software
development errors using software complexity metrics.
Selected Areas in Communications, IEEE Journal on ,
8(2):253‚Äì261, 1990.
[11] S. Kim, T. Zimmermann, E. Whitehead Jr, and A. Zeller.
Predicting faults from cached history. In Proceedings of
the 29th ICSE , pages 489‚Äì498. IEEE Computer Society,
2007.
[12] A. G. Koru and H. Liu. Identifying and characterizing
change-prone classes in two large-scale open-source
products. Journal of Systems and Software , 80(1):63‚Äì
73, 2007.
[13] S. Lessmann, B. Baesens, C. Mues, and S. Pietsch.
Benchmarking classiÔ¨Åcation models for software defect
prediction: A proposed framework and novel Ô¨Åndings.
IEEE TSE , 34(4):485‚Äì496, July 2008.
[14] Y . Ma and B. Cukic. Adequate and precise evaluation
of quality models in software engineering studies. In
Proceedings of the 29th ICSE Workshops , ICSEW ‚Äô07,
pages 68‚Äì, Washington, DC, USA, 2007. IEEE Computer
Society.
[15] Y . Ma, G. Luo, X. Zeng, and A. Chen. Transfer learning
for cross-company software defect prediction. Information
and Software Technology , 54(3):248‚Äì256, 2012.
[16] T. Mende. Replication of defect prediction studies:
problems, pitfalls and recommendations. In T. Menzies
and G. Koru, editors, PROMISE , page 5. ACM, 2010.
[17] A. Meneely and L. A. Williams. Secure open source
collaboration: an empirical study of linus‚Äô law. In
E. Al-Shaer, S. Jha, and A. D. Keromytis, editors, ACMConference on Computer and Communications Security ,
pages 453‚Äì462. ACM, 2009.
[18] T. Menzies, J. Greenwald, and A. Frank. Data mining
static code attributes to learn defect predictors. IEEE
TSE, 33(1):2‚Äì13, 2007.
[19] A. Mockus and D. M. Weiss. Predicting risk of software
changes. Bell Labs Technical Journal , 5(2):169‚Äì180,
2000.
[20] R. Moser, W. Pedrycz, and G. Succi. A comparative
analysis of the efÔ¨Åciency of change metrics and static
code attributes for defect prediction. In W. Sch ¬®afer, M. B.
Dwyer, and V . Gruhn, editors, ICSE , pages 181‚Äì190.
ACM, 2008.
[21] N. Nagappan and T. Ball. Using software dependencies
and churn metrics to predict Ô¨Åeld failures: An empirical
case study. In ESEM , pages 364‚Äì373. IEEE Computer
Society, 2007.
[22] D. Posnett, V . Filkov, and P. Devanbu. Ecological
inference in empirical software engineering. In ASE‚Äô2011 ,
pages 362‚Äì371. IEEE, 2011.
[23] F. Rahman and P. T. Devanbu. Ownership, experience
and defects: a Ô¨Åne-grained study of authorship. In R. N.
Taylor, H. Gall, and N. Medvidovic, editors, ICSE , pages
491‚Äì500. ACM, 2011.
[24] F. Rahman, D. Posnett, and P. Devanbu. Recalling the
‚Äúimprecision‚Äù of cross-project defect prediction. In the
20th ACM SIGSOFT FSE , pages ‚Äì. ACM, 2012.
[25] F. Rahman, D. Posnett, A. Hindle, E. Barr, and P. Devanbu.
Bugcache for inspections: hit or miss? In Proceedings
of the 19th ACM SIGSOFT FSE , pages 322‚Äì331. ACM,
2011.
[26] B. Turhan, T. Menzies, A. B. Bener, and J. Di Stefano.
On the relative value of cross-company and within-
company data for defect prediction. Empirical Softw.
Engg. , 14(5):540‚Äì578, Oct. 2009.
[27] B. Turhan, A. T. Misirli, and A. B. Bener. Empirical
evaluation of mixed-project defect prediction models. In
EUROMICRO-SEAA , pages 396‚Äì403. IEEE, 2011.
[28] E. J. Weyuker, T. J. Ostrand, and R. M. Bell. Do too many
cooks spoil the broth? using the number of developers to
enhance defect prediction models. ESE, 13(5):539‚Äì559,
2008.
[29] H. Zhang. On the distribution of software faults. IEEE
TSE, 34(2):301‚Äì302, March-April 2008.
[30] T. Zimmermann, N. Nagappan, H. Gall, E. Giger, and
B. Murphy. Cross-project defect prediction: a large scale
experiment on data vs. domain vs. process. In H. van
Vliet and V . Issarny, editors, ESEC/SIGSOFT FSE , pages
91‚Äì100. ACM, 2009.
[31] T. Zimmermann, R. Premraj, and A. Zeller. Predicting de-
fects for eclipse. In Proceedings of the Third International
Workshop on Predictor Models in Software Engineering ,
PROMISE ‚Äô07, pages 9‚Äì, Washington, DC, USA, 2007.
IEEE Computer Society.