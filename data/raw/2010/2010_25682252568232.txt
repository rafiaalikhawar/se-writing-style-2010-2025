Performance Regression Testing Target Prioritization via
Performance Risk Analysis
Peng Huang, Xiao Ma†, Dongcai Shen, and Yuanyuan Zhou
Computer Science and Engineering†Computer Science
Univ. of California at San Diego Univ. of Illinois at Urbana- Champaign
La Jolla, CA, USA Urbana, IL, USA
{ryanhuang,x1ma,doshen,yyzhou}@cs.ucsd.edu
ABSTRACT
As software evolves, problematic changes can signiﬁcantly degrade
software performance, i.e., introducing performance regr ession. Per-
formance regression testing is an effective way to reveal su ch issues
in early stages. Yet because of its high overhead, this activ ity is usu-
ally performed infrequently. Consequently, when performa nce re-
gression issue is spotted at a certain point, multiple commi ts might
have been merged since last testing. Developers have to spen d extra
time and efforts narrowing down which commit caused the prob -
lem. Existing efforts try to improve performance regressio n testing
efﬁciency through test case reduction or prioritization.
In this paper, we propose a new lightweight and white-box ap-
proach, performance risk analysis ( PRA), to improve performance
regression testing efﬁciency via testing target prioritization. The
analysis statically evaluates a given source code commit’s risk in
introducing performance regression. Performance regress ion test-
ing can leverage the analysis result to test commits with hig h risks
ﬁrst while delaying or skipping testing on low-risk commits .
To validate this idea’s feasibility, we conduct a study on 10 0 real-
world performance regression issues from three widely used , open-
source software. Guided by insights from the study, we desig nPRA
and build a tool, PerfScope. Evaluation on the examined prob lem-
atic commits shows our tool can successfully alarm 91% of the m.
Moreover, on 600 randomly picked new commits from six large-
scale software, with our tool, developers just need to test o nly 14-
22% of the 600 commits and will still be able to alert 87-95% of
the commits with performance regression.
Categories and Subject Descriptors
D.2.5 [ Software Engineering ]: Testing and Debugging— Testing
tools
General Terms
Performance, Experimentation
Keywords
Performance regression, performance risk analysis, cost m odeling
Permission to make digital or hard copies of all or part of thi s work for
personal or classroom use is granted without fee provided th at copies are
not made or distributed for proﬁt or commercial advantage an d that copies
bear this notice and the full citation on the ﬁrst page. To cop y otherwise, to
republish, to post on servers or to redistribute to lists, re quires prior speciﬁc
permission and/or a fee.
ICSE ’14, May 31–June 7, 2014, Hyderabad, India
Copyright 14 ACM 978-1-4503-2756-5/14/05 ...$15.00.int bstream_rd_db_catalogue (...)
{
do {
if(bcat_add_item(cat, &ti.base.base) != BSTREAM_OK)
return BSTREAM_ERROR;
} while (ret == BSTREAM_OK);
}
int bcat_add_item (...)
{
switch (item->type) {
case BSTREAM_IT_PRIVILEGE:
Image_info::Dbobj *it1= info->add_db_object(...);
}
}
backup::Image_info::Dbobj* Backup_info::add_db_object (...)
{
}The new block calls an expensive 
function. When indirectly executed 
inside a loop, it can incur 80 times 
slowdown 
+  if (type == BSTREAM_IT_TRIGGER) {
+    obs::Obj*tbl_obj=obs:: find_table_for_trigger (...);
+ }
Figure 1: A real-world performance regression (80 times) is -
sue in MySQL: the change causes an expensive function call
find_table_for_trigger to be executed many times.
1. INTRODUCTION
1.1 Performance Regression Testing
Performance is a vital quality metric for software system. I t
can directly affect user experience and job efﬁciency. For e xam-
ple, a 500 ms latency increase could cause 20% trafﬁc loss for
Google [44]. As another example, the Colorado Beneﬁts Manag e-
ment System is designed to make social welfare accessible. B ut it
runs so slowly that the system is virtually unable to accept a ssis-
tance applications [18].
On the other hand, software today is evolving rapidly. Code
commits1for feature enhancement, bug ﬁxing or refactoring are
frequently pushed to the code repository. Some of these comm its,
while preserving the software’s functionality, may signiﬁ cantly de-
grade performance, i.e., introducing performance regression .
Figure 1 shows a real-world performance regression issue fr om
MySQL. The added code commit was written without much con-
sideration for performance. When the code gets indirectly e xecuted
inside a loop, it can cause MySQL’s backup operation to be 80
times slower. After this problem was reported, developers o pti-
mized the added code and released a patch.
To provide motivating evidence, Figure 2 shows the releases con-
taining performance regression issue(s) in a snapshot of th e evo-
1We use commit, revision and changeset interchangeably.Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a
fee. Request permissions from Permissions@acm.org.
ICSE’14 , May 31 – June 7, 2014, Hyderabad, India
Copyright 2014 ACM 978-1-4503-2756-5/14/05...$15.00
http://dx.doi.org/10.1145/2568225.2568232
60
5.1.45.1.35.1.245.1.365.1.355.1.47 5.1.42 5.1.125.1.145.1.135.1.155.1.165.1.235.1.375.1.465.1.49
 17.0.916 17.0.934 17.0.937 17.0.945 17.0.956 17.0.963MySQL (popular database system)
Chrome (popular web browser)... ... 
... ... ... ... ... Version containing performance regression 
... ... ... ... ... 
Figure 2: Performance regression in modern popular softwar e.
Data from performance regression bug reports studied in §2.
Table 1: Typical running cost for popular benchmarks.
Category Benchmark Per run cost
Web Server autobench,Web Polygraph,SPECweb 3 min–1 hr
Database pgbench,sysbench,DBT2 10 min–3 hrs
Compiler CP2K,Polyhedron,SPEC CPU 1 hr–20 hrs
OS lmbench,Phoronix Test Suite 2 hrs–24 hrs
lution history for two popular, performance-critical, ope n-source
software. These regression issues were user-reported ones , i.e.,
post-release. We can see that regressing releases are preva lent across
the evolution history of these two software packages.
Performance regression often damages software’s usabilit y con-
siderably. In one example [7], switching from MySQL 4.1 to 5. 0
in a production e-commerce website caused the loading time o f
the same web page to increase from 1 second to 20 seconds. This
made the website almost unusable. The reporter complained t hat
“MySQL 5 is no good for production until this bug is ﬁxed” . As
another example, after upgrading GCC from 4.3 to 4.5, Mozill a de-
velopers experienced an up to 19% performance regression, w hich
forced them to reconsider a complete switchover [29].
Performance regression has been a known problem for mature
and performance-conscious software projects. For example , in many
software issue tracking systems, there is a special categor y to anno-
tate issues related to performance. In the case of MySQL, a se pa-
rate severity level (S5) is used to mark performance-relate d reports.
Interestingly, among a set of 50 randomly sampled MySQL perf or-
mance regression issues (§2), almost half of them were also t agged
as S1 (Critical) or S2 (Serious).
1.2 Performance Regression Testing Challenges
An effective way to combat performance regression is to em-
ploy systematic, continuous performance regression testi ng. This
is widely advocated in academia [28, 57, 24, 46], open source com-
munity [45, 15, 36, 21] and industry [6].
Ideally, the testing should be carried out as comprehensive ly and
intensively as possible, i.e., on every source commit basis . This
can eliminate the “last-minute surprise” wherein performa nce is-
sues are exposed too late to be ﬁxed before release [16]. More im-
portantly, this would avoid the tedious and lengthy diagnos is pro-
cess to ﬁgure out which commit is responsible for the observe d
performance regression.
Unfortunately, despite the obvious beneﬁts, real practice s often
cannot afford performance regression testing for every com mit due
to the combination of two factors: the high performance test ing
overhead and the rapid software evolution pace.
Performance testing is supposed to measure systems under re p-
resentative and comprehensive workloads. It can take hours to daysTable 2: Estimated commit and performance testing frequenc y
in popular software.
SoftwareAvg. Rev.Regular Perf. Testingper Day
MySQL ∼6 every release [49]
Chrome ∼140 every 4 rev.
Linux ∼140 every week [21]
to complete even with a number of dedicated machines [3, 21, 2 3].
Table 1 lists the typical per run cost of several popular bench-
marks. A comprehensive performance testing often includes not
only these benchmarks but also load and stress testing suite s. In a
leading, public US data warehousing company that we collabo rate
with (anonymized as required), some internal test cases can take
almost one week to just load the data.
What’s more, performance testing results are subject to ext ernal
factors such as caching and testing environment load. To min imize
experimental errors, performance testing should be carrie d out in
a clean environment for a period long enough and repeated sev -
eral times until the performance becomes stable [36, 5, 1]. C on-
sequently, performance testing guide often advocates to “n ever be-
lieve any test that runs for only a few seconds” [9, 46]. In thi s sense,
performance testing is by nature time and resource consumin g.
On the other hand, the increasingly favored rapid developme nt
methodologies such as agile development [43] are catalyzin g soft-
ware revision speed. As Table 2 shows, Chrome and Linux have
more than 100 revisions per day merged into the code base. Wit h
the high revision rate and high testing cost, it is almost imp ractical
to run comprehensive performance testing on every commit.
1.3 Current Practices
The above factors often cause performance regression testi ng
frequency to be compromised. Testing is carried out on daily or per
release basis (Table 2). When performance regression is exp osed,
developers have to spend extra efforts bisecting [4] which commit
among the recently committed changes causes the problem. Fo r
example, in diagnosing a Chrome performance regression iss ue [2]
revealed during testing, there were six revisions included since last
testing. The developers had to conduct bisection to narrow d own
the problematic commit, which already took hours.
With performance test case prioritization, a technique as u sed
in feature regression testing [51, 25], the testing frequen cy can be
increased but at the cost of reduced comprehensiveness. In t his
scheme, test cases are divided into multiple levels based on their
overhead and ability to catch performance issues. Then ligh tweight
test cases with high detection rate are run more frequently w hile
costly tests are run infrequently. Our industry collaborat or adopted
this practice. Such prioritization is effective to capture easy-to-
trigger performance regression. But the detection of compl icated
issues that are manifested only under the comprehensive tes t cases
are delayed because of reduced comprehensiveness.
1.4 Our Contributions
Given that performance testing frequency and the commit spe ed
are not in synchrony, it is important to fully utilize the tes ting cost.
To this end, it is desired to devote testing on exactly the reg ressing
commits and skip non-regressing commits. But existing prac tices
as above all treat the testing target —code commits—as black-box ,
ignoring the valuable information in commit content that ma y be
exploited to direct performance testing on the right target . Conse-
quently, the testing is carried out blindly even for commits that are
unlikely to introduce performance regression.61Table 3: Studied software.
Software Description LOC # of Issues
MySQL DBMS 1.2M 50
PostgreSQL DBMS 651K 25
Chrome Web browser 6.0M 25
Our work takes this complementary approach by leveraging th e
information in each code commit to help prioritize performa nce
regression testing on risky commits. In particular, by cond ucting
a lightweight, static performance risk analysis (abbr. PRA , here-
after) on source code change, we estimate the risk of this rev ision in
introducing performance regression issues. Based on such e stima-
tion, we can prioritize performance testing to conduct more com-
prehensive tests for high-risk revisions while lowering th e testing
cost for low-risk ones.
For example, by statically analyzing the added code in Figur e 1,
we know it can be executed indirectly inside a loop. With stat ic cost
estimation and proﬁle information, our PRA will recommend this
commit to be tested heavily. On the other hand, for commits li ke
adding few arithmetic operations in a cold path, we could sug gest
skipping testing or testing it lightly.
The major contributions of this paper are:
•We conduct an empirical study on 100 real-world perfor-
mance regression issues from three widely used software to
gain insights on the feasibility and approaches for testing tar-
get prioritization with commit performance risk analysis.
•To the best of our knowledge, we are the ﬁrst to propose a
white-box approach on prioritizing performance testing tar-
gets with code change performance risk analysis ( PRA) to
make performance regression testing more efﬁcient.
•We implement a tool and release it in open source. Evalua-
tion on the studied commits as well as 600 randomly selected
newcommits shows that based on the recommendation of our
tool, developers just need to test only 14-22% of the 600 code
commits and will be able to catch 87-95% of all performance
risky commits.
2. UNDERSTANDING REAL WORLD PER-
FORMANCE REGRESSION ISSUES
To validate the feasibility of performance regression test ing tar-
get prioritization and design an effective PRA, we conduct an em-
pirical study on 100 randomly selected real-world performa nce re-
gression issues from three widely used, open source softwar e. The
study focuses on understanding what kind of code changes wou ld
cause performance regression and how they impact performan ce.
Designing in this bottom-up manner may suffer from the over-
ﬁtting problem. To address this concern, we also evaluate ou r pro-
posed PRA on 600 randomly selected new code commits that are
not used in the study described here.
2.1 Software Studied
Table 3 lists the details of studied software. We choose thes e
software packages because they are performance critical, t op-rated,
widely used and industry-backed. Chrome is developed by Goo gle;
MySQL is now owned by Oracle; and PostgreSQL’s development
team consists of employees from Red Hat. Besides, their long evo-
lution history and well-maintained issue tracking systems provide
us with many real-world performance regression issues.
2.2 Issue Collection Methodology
For each software, we ﬁrst query the tracking system or maili ng
list with a set of broad performance-related keywords like “ perfor-mance”, “hit”, “drop”, “slow”, “slower” on resolved issues . After
getting this initial collection of issues, we manually go ov er each
to prune out those issues unrelated to performance regressi on. Fi-
nally, we randomly sample 100 issues of which not only the ﬁxe s
can be found, but also the responsible change set can be track ed.
2.3 Threats to Validity
Construct Validity Our study has high construct validity because
all the issues we collect are indeed related to performance r egres-
sion issues based on issue symptom description from the repo rter
and conﬁrmation of issue ﬁx from the developer.
Internal Validity There is potential selection bias in our study. We
try to minimize it by ﬁrst covering diverse categories and re presen-
tative software. Except for Chrome which is landed in 2008, t he
other two software projects have more than 10 years of histor y. Ad-
ditionally, the issues we study for each software are randomly sam-
pled without favoring or ignoring particular type of perfor mance
regression issues. For each issue, we write a diagnosis repo rt and
have at least 2 inspectors agree on the understanding.
Another potential threat is unreported performance regres sion is-
sues. It is difﬁcult to measure quantitatively them. Howeve r, we
believe that at least the reported issues are of importance.
External Validity Although we believe our study provides inter-
esting ﬁndings on performance regression issues in studied repre-
sentative software, they may not be generalized to other sof tware
projects beyond the speciﬁc scope this study was conducted. Thus
they should be taken with limitations in mind.
Most importantly, we want to emphasize the primary purpose
of this study is for ourselves to gain insights for designing PRA,
instead of drawing any general conclusions. Therefore, the poten-
tial threats to validity in the study will only impair the efﬁ cacy of
our design. Nevertheless, as evaluation demonstrates (§5) , the PRA
guided by this study is indeed effective, even for 600 new code
commits that are not examined in our empirical study.
2.4 Code Change Categorization
To analyze what kind of code changes are likely to introduce p er-
formance regression, we develop a taxonomy. The key rationa le is
that program performance depends on how expensive an operat ion
is and how many times the operation gets executed. Therefore , per-
formance regression could be introduced by either more expe nsive
operations or regular operations but executed many times in a crit-
ical path. As a starting point, we need to look into what andwhere
are the code changes in their execution context. Moreover, w e need
to consider code changes that indirectly impact expensive opera-
tions or critical path. For example, a code change that modiﬁ es a
variable controlling the loop iteration count in a critical path may
have a big impact on performance. We further zoom into each of
the three perspective and divide it into subcategories. The subcate-
gories may not exclusive. For example, in the where perspective, a
change could lie in both a loop and a primitive function. We ch oose
the ﬁrst applicable subcategory in top to bottom order.
In our study, we consider only code changes that are the culpr its
for performance regression, ignoring innocent changes in t he same
commit. Also we focus on changes that impact the performance
of existing functionalities, ignoring changes for indepen dent, new
functionalities that do not interact with existing ones. Th e result
based on this categorization methodology is presented in Ta ble 4.
2.4.1 Where a Change Takes Place
Program scopes such as loop or primitive function are perfor -
mance sensitive places. It is because these places can be exe cuted
or called many times and magnify any overhead added inside th em.62Table 4: Categorization of issues examined from three persp ectives.
CategorySoftware
MySQL PostgreSQL Chrome
Where the change takes place
API ( Can be called a lot of times by external program ) 5 (10%) 0 (0%) 1 (4%)
Primitive(Utility) Function ( Can be called a lot of times internally, e.g. mutex_spin_wait ) 9 (18%) 4 (16%) 1 (4%)
Routine Function ( Will deﬁnitely be called under various input, e.g. MySQLParse ) 7 (14%) 4 (16%) 8 (32%)
Loop ( Can have multiple iterations ) 12 (24%) 8 (32%) 4 (16%)
Others 17 (34%) 9 (36%) 11 (44%)
What the change modiﬁes
Expensive Function Call (e.g. Figure 4) 21 (42%) 9 (36%) 16 (64%)
Performance Sensitive Condition (e.g. Figure 5) 8 (16%) 6 (24%) 4 (16%)
Performance Critical Variable (e.g. Figure 3) 6 (12%) 5 (20%) 2 (8%)
Others (e.g. overhaul) 15 (30%) 5 (20%) 3 (12%)
How the change impacts performance
Direct (e.g. Figure 1) 34 (68%) 11 (44%) 12 (48%)
Indirect, LatentThrough Function Return Value (e.g. Figure 5) 7 (14%) 7 (28%) 3 (12%)
Through Function Referential Parameter (e.g. Figure 3) 5 (10%) 4 (16%) 1 (4%)
Through Class Member 1 (2%) 1 (4%) 3 (12%)
Through Global Variable 1 (2%) 0 (0%) 1 (4%)
Others 2 (4%) 2 (8%) 5 (20%)
Total 50 25 25
Hence we ﬁrst categorize all the problematic changes based o n the
scopes that they lie in.
We also need to be context sensitive and consider call paths.
For example in Figure 1, although the added expensive functi on
callfind_table_for_trigger is not inside loop in the static
context, if the expensive call path is executed, it will be dy nami-
cally executed in a tight loop (loop that iterates many times ). There-
fore, when we design PRA, we need to be context sensitive and
examine possible call paths.
Table 4 shows more than half of the problematic changes are
located in performance sensitive scopes. However, using ch ange
scope as the sole indicator of performance regression can mi ss some
performance killers that can inﬂuence critical paths via da ta and
control ﬂow. That is why there are a signiﬁcant number of regr es-
sion issues in the Others subcategory in Table 4. For this reason,
we further categorize the change content.
Implication :PRA should pay attention to common perfor-
mance critical places such as loop and primitive function. B ut
using only place as criteria can incur high analysis inaccur acy.
2.4.2 What a Change Modiﬁes
Intuitively, for a change set to introduce performance regr ession,
it needs to add high overhead (computation or I/O). Such inﬂu ence
can happen in two ways: one case is that the change directly ad ds
signiﬁcant cost into hot path; another case is that the chang e mod-
iﬁes some “control” variables which result in an expensive c ode
region to iterate more times or take a longer execution path.
The former case is straightforward (especially if our PRA can be
context sensitive), whereas the latter would require us to c onsider
some special variables and their control and data dependenc ies. We
refer to the variable of which different values can result in dramat-
ically different performance as performance critical variable (e.g.,
loop iteration count, database table index variable as in Fi gure 3)
and the condition that controls whether a critical path will be taken
in a branch as performance sensitive condition (e.g., the branch
condition in function create_sort_index in Figure 5).
In the study, we obtain information regarding whether opera tions
are expensive or performance critical by reading the bug rep orts
and consulting with developers. In our implementation of PRA, we
obtain such information from static analysis and proﬁling.As table 4 shows, expensive function calls is the most common
problematic change content. But other types of changes such as
modifying the performance critical variable and performan ce sen-
sitive condition also cause a signiﬁcant number of performa nce re-
gression issues in the dataset.
Implication :PRA should also account for the code change
content to identify costly instructions and performance cr itical
variables or sensitive conditions.
2.4.3 How a Change Impacts Performance
As explained above, not all problematic changes affect perf or-
mance in a direct way. A change may propagate via data and con-
trol ﬂow that eventually causes a performance problem in its for-
ward slice. For example, in Figure 5, although the change its elf
is not expensive, it modiﬁes the function return value, whic h later
determines whether an expensive filesort is called.
Thus we divide all the perilous changes into two categories b ased
on how they impact performance: (1) changes that directly de -
grade performance in its execution scope; (2) changes that i ndi-
rectly cause performance problem later in other code region s.
Table 4 shows the majority of the issues belong to the ﬁrst cat -
egory. But there are also a signiﬁcant number of issues impac ting
performance indirectly. For these issues, the table also li sts the pro-
gram constructs through which the change propagates its eff ect to
critical paths and thereby leads to performance degradatio n. Most
of them are through function return values.
Implication :PRA should follow the control and data ﬂow to
factor in the indirect effect of a change. The analysis shoul d be
inter-procedural and context-sensitive.
2.5 Case Studies
We now go through three performance regression issues from
MySQL as case studies.
The issue in Figure 4 is introduced when patching a functiona l-
ity bug in MySQL Cluster. This commit changes the system call
clock_gettime argument to use CLOCK_MONOTONIC instead
of real-time clock to prevent potential hang in certain scen arios.
But in some platform like Solaris, clock_gettime call is ex-
pensive when using CLOCK_MONOTONIC . This extra overhead is
further ampliﬁed when it is called in a tight loop.63+      if (table->s->primary_key != MAX_KEY && 
+      table->file->primary_key_is_clustered()) 
+      tab->index = table->s->primary_key;
+      else 
The new logic prefers clustered 
primary index over secondary 
ones. It degrades performance 
for certain workloads.uint make_join_readinfo (JOIN * join , ulonglong options)
{
for (i=join->const_tables ; i < join->tables ; i++) {
JOIN_TAB *tab=join->join_tab+i;
tab->index =find_shortest_key(table, ...);
}
}
int join_read_first (JOIN_TAB *tab)
{
if (!table->file->inited)
table->file->ha_index_init( tab->index , tab->sorted);
}
Figure 3: MySQL performance regression issue #35850
NDB_TICKS NdbTick_CurrentMillisecond (void )
{
struct timespec tick_time;
return tick_time.tv_sec * MILLISEC_PER_SEC +
tick_time.tv_nsec / MILLISEC_PER_NANOSEC;
}
int waitClusterStatus (const char* _addr,...) 
{
while (allInState == false){ ... 
time_now = NdbTick_CurrentMillisecond ();
}
}Replacing realtime clock with monotonic for clock_g ettime sys call has 
performance loss in Solaris. When put in loop, this  loss can be up to 15%+ clock_gettime( CLOCK_MONOTONIC , &tick_time);- clock_gettime( CLOCK_REALTIME , &tick_time);
Figure 4: MySQL performance regression issue #46183
In Figure 5, function test_if_skip_sort_order decides
whether it can leverage database table index to perform the O RDER
BY query or an actual ﬁle-sort is necessary. The new optimiza tion
rule prefers ﬁle-sort with a join buffer over an index scan if possi-
ble. In such case, it returns 0. However, when an index is clus tered,
index scan can actually be much faster than the preferred ﬁle -sort.
As a result, this premature optimization code change causes perfor-
mance slowdowns for these types of data layout.
Similarly, the new optimization in Figure 3 prefers cluster ed pri-
mary key over secondary key. While this can speed up disk-bou nd
workloads, it slows down cases when the data is in disk cache.
3. PERFORMANCE RISK ANALYSIS
3.1 Overview
The objective of PRA is to examine source code commit content
and determine whether the commit is likely to introduce perf or-
mance regression. Therefore, PRA is a white-box approach. But
PRA is not meant to replace performance regression testing. On
the contrary, its main consumer is performance regression t esting.
It recommends risky commits to performance regression test ing to
test comprehensively and suggests skipping low-risk commi ts.
This role relaxes the safety and soundness requirement for PRA.
In the extreme case, if PRA recommends every commit to be tested,
it is the same situation as testing without PRA; ifPRA recommends
to skip a commit that is actually risky, it is similar as condu cting
infrequent such as per-release testing. That said, PRA should aimbool test_if_skip_sort_order (...)
{
if (select_limit >= table_records) {
}
DBUG_RETURN( 1);
}
int create_sort_index () 
{
if ((order != join->group_list || ... && 
test_if_skip_sort_order (...)) 
DBUG_RETURN(0);
table->sort.found_records= filesort (thd, 
table,join->sortorder, ...);
}+      /* filesort() and join cache are usually 
+         faster than reading in index order 
+        and not using join cache */ 
+      if (tab->type == JT_ALL && ...)
+       DBUG_RETURN( 0);
The new control flow can change 
the function return value, which 
later affects whether an expensive 
path (with firesort call) will be 
taken or not 
Figure 5: MySQL performance regression issue #50843
to ﬂag the commits’ risk accurately to be able to truly improv e per-
formance regression testing efﬁciency.
3.2 PRA Design
PRA is essentially static performance risk estimation of code
changes. The challenge is how to reason about performance im pact
without actually running the software. Guided by the real wo rld
issue study, the performance impact of a code change depends on
the cost of change operations (whether it is expensive or not ) and
the frequency of its execution (whether it lies in hot path or not).
Therefore, we design PRA as follows. First, we use a cost model
to estimate the expensiveness of a change instruction. If th e change
touches performance sensitive conditional expression, it is also con-
sidered expensive. Then, we estimate the frequency of the ch ange
instruction. With the two estimations, we index them into a r isk
matrix to assess the risk level. The risk levels for the entir e patch
can then be aggregated into a single risk score.
3.2.1 Cost Modeling
To establish a systematic and efﬁcient estimation regardin g the
expensiveness information, a static cost model for differe nt oper-
ations is necessary. The cost modeling will determine wheth er a
change instruction is expensive or not.
Since the purpose of the modeling emphasizes on relative cos t
rather than absolute cost, we express cost in abstract unit, denoted
asδ, instead of actual CPU cycles. With simple architecture mod el
and heuristics, we build a basic cost table for different ins tructions
based on its type and operands. For example, add instruction has
costδ,multiply has cost 4δandcall instruction has the cost
equal to calling convention overhead plus the callee’s cost .
A basic block’s cost is a sum of the cost of those instructions that
live inside the basic block. For control ﬂows, we adopt worst case
analysis. For example, the cost of a function or a loop body is the
maximum cost among the static paths.
Next, we assume if an operation has cost δ, and it’s executed 10
times, the aggregated cost is 10δ. While this assumption doesn’t
account for factors such as compiler optimization, it is a go od start
for the purpose of risk assessment. Then for a loop, if its trip count
(maximum number of iterations) can be statically inferred, we mul-
tiply the loop body’s cost by the trip count as the loop’s cost . Oth-
erwise, the loop is considered to be potentially expensive.64Table 5: Risk matrix of a change’s expensiveness and frequen cy
FrequencyExpensivenessFrequent Normal Rare
Expensive Extreme High Moderate
Normal High Moderate Low
Minor Moderate Low Low
With this model, we can obtain cost in terms of δ, whereas PRA
needs to gauge whether the cost is expensive or not. We use thr esh-
olds to convert the cost into level. Such thresholds can be co nﬁg-
ured or computed automatically by running the cost model on t he
whole program to obtain cost distribution for functions and basic
blocks. Then the cost of a change is ranked in the distributio n to
convert to expensiveness. Besides this static model, we als o allow
users to add dynamic proﬁle information or domain knowledge to
make PRA more accurate. But this is only optional for PRA.
The above modeling mainly deals with addcode change type.
Code change can also be deleting or replacing statements. Fo r
delete type changes, we can offset their direct cost from the total
cost. In current implementation, we do not want to make PRA ag-
gressive that miss potential performance regression issue s due to
inaccurate offsetting. Therefore, the cost of a delete chan ge is by
default 0. The theoretical cost for a replace type change would be
new.cost−old.cost . But for similar reason, the cost of a replace
change is the cost of new program elements. The exception is: if
a delete or replace change touches performance sensitive va riables
or conditions, the change cost is directly assigned to be exp ensive.
3.2.2 Performance Sensitive Condition/Variable
Condition expressions need special attention. Conditiona l branch
instruction itself is rarely costly. But as seen from the rea l world
study, a condition can signiﬁcantly inﬂuence whether an exp en-
sive path will be taken or not. We deﬁne a branch condition to
be performance sensitive if its intra-procedural paths (su ccessors)
have dramatic cost difference, which is estimated using abo ve cost
model. For example, the branch condition for the ﬁrst ifstatement
in function create_sort_index in Figure 5 is a performance
sensitive condition. Change affecting such performance se nsitive
condition is considered expensive.
For performance critical variable, we currently only consi ders
variable that can affect loop termination condition. We lea ve sys-
tematically identifying performance critical variable su ch as the ta-
ble index variable in future work.
3.2.3 Frequency Estimation
In addition to estimating the cost of change content, we shou ld
also analyze whether the change lies in hot path. To do this, w e
ﬁrst analyze the intra-procedural scope that a change lies i n. If the
change is enclosed in any loop and the trip count of this loop i n-
cluding all its parent loops can be statically determined, t he execu-
tion frequency of this change instruction is estimated by th e prod-
uct of these trip counts. Otherwise, if any of enclosing loop has
non-determined trip count, it is considered to be possibly e xecuted
frequently. Similarly code change that lies in recursive fu nctions is
also assessed to be potentially frequently executed.
Next, we examine the call path backward that could potential ly
reach the function where the change is located and perform si milar
frequency estimation for each call site. In implementation , the level
of call path length is bounded. Given the estimation, we conc lude
if a change lies in the context that may be frequently execute d using
a ranking of the frequency count.Mapper Filter --- origin 
+++ new patch 
Patch 
Parser Performance 
Risk Analyzer 
Performance Regression Testing Profile DB
Code 
Repository 
Figure 6: Architecture of our PRA implementation, PerfScope
3.2.4 Risk Matrix
Combining the above two pieces of information, we use a risk
matrix (Table 5) to assess the risk. Such matrix can be extend ed to
express more expensiveness/frequency categories and risk levels.
The output of PRA is therefore the risk level distributions. We also
calculate a simple risk score based on the distribution:
Risk Score =Nextreme ×100 + Nhigh ×10
+Nmoderate ×1
100+Nlow×1
1000(1)
Practitioners can then deﬁne testing target selection crit eria against
the risk level distribution or summary score. For example, c ommits
withNextreme >= 5||Nhigh>= 20 or whose risk score exceeds
300 require comprehensive testing. Like in choosing the cri teria
in performance testing to judge regression, for different s oftware,
these criteria may also require some initial tuning.
3.2.5 Indirect Risk
As Section 2 shows, in addition to direct performance impact ,
there are also a number of risk commits that indirectly affec t perfor-
mance via data ﬂow and control ﬂow to their forward slice, the pro-
gram subset that may be affected by a given program point. The re-
fore, we extend the basic PRA with the ability to analyze such cases,
named PRA-Slicing . We use static program slicing techniques [55]
to compute a forward slice of the change set. Then we check if a ny
element in the slice is performance sensitive or not using th e logic
as described in 3.2.2. The ﬁnal risk level is only assigned on ce
to the change instruction instead of its slice. The slicing i s inter-
procedural and bounded to a limited depth. While PRA-Slicing can
catch more complicated risky commits, it may also recommend ex-
cessive commits due to the imprecisions in the slicing. More over,
computing precise slice can be expensive. Consequently, PRA-
Slicing remains an extension to the main PRA analysis and is dis-
abled by default.
4. IMPLEMENTATION
We develop a tool called PerfScope that implements the proposed
PRA on top of the LLVM compiler infrastructure [39]. The tool is
released in open source at
http://cseweb.ucsd.edu/~peh003/perfscope .
4.1 Architecture
We ﬁrst brieﬂy describe the architecture of PerfScope. It co nsists
of ﬁve components (Figure 6).
Parser parses patch information regarding the changed ﬁles, lines
and types ( add,delete ,change ). For the change type, uniﬁed diff65ﬁle contains addanddelete but no replace . The parser will pair
delete with addand reduce them to replace .
Mapper extracts debug information of a program, build search
trees and map given lines to the program constructs, if any, w hich
are fed to the ﬁlter.
Filter prunes out insigniﬁcant change such as stylish change or
renaming. If all changes in the entire commit are ﬁltered, th e com-
mit will be considered trivial and not fed to the analyzer.
Proﬁle database (optional) allows users to incorporate proﬁle
information and domain knowledge that may make PRA more ac-
curate. Such proﬁle can be practically incorporated becaus e for
large software, the cost of many functions doesn’t change ve ry fre-
quently. Therefore, the proﬁle does not need frequent updat e.
But note that, PRA already provides a cost model (§3.2.1). The
proﬁle information is only optional. Interestingly, we obt ained the
proﬁles for several software using a popular low-overhead, system-
wide proﬁler— OProﬁle [8], and found the expensive function list
has a large portion of overlap with the list computed by PRA.
Performance risk analyzer runs PRA or PRA-Slicing on the
language constructs remained after ﬁltering. It computes t he risk
level (e.g., extreme ,high,moderate ) of a change using a risk ma-
trix like in Table 5). The output of the analyzer is a risk leve l dis-
tribution from all changes in the commit and a summary risk sc ore.
The two results can be used by performance testing practitio ners to
determine the performance testing strategy for the commit.
4.2 Challenges
There are two key challenges in implementing PerfScope.
Mapping : Raw information in patch ﬁles produced by standard
tools likediff are language agnostic and in line granularity. But
PRA works on language constructs. For example, it needs to know
the change on line 10 corresponds to an ifstatement.
LLVM provides debugging information such as line number, ﬁl e
path attached in instructions. We use this information to bu ild a
search tree. We ﬁrst ﬁnd the compile unit for the changed sour ce
ﬁles, then match the top level constructs such as functions i nside
that unit and ﬁnally the corresponding instruction(s).
Filtering : There are changes on non-source ﬁles such as the doc-
umentations, test cases. It is unlikely for them to introduc e perfor-
mance regression. We predeﬁne a set of source code sufﬁxes, e .g.,
.c, .cpp , to ﬁlter non-source changes. We also prune changes on
source ﬁles that essentially do not alter the program (e.g., add com-
ments, rename variables). We only perform safe, simple chec king
instead of trying to determine general equivalence of two pr ograms,
which is undecidable. Algorithm 1 shows the ﬁltering logic.
5. EV ALUATION
This section evaluates the effectiveness and efﬁciency of o ur
PRA implementation, PerfScope . It consists of six parts. First, we
test whether PerfScope is able to report these problematic c ommits
from our real-world issue study. Second, to address the over ﬁt-
ting concern, we also evaluate PRA using 600 new commits from
both studied software and unstudied ones. Third, we evaluat ePRA-
Slicing extension and also compare our design with random test tar-
get selection. Fourth, we estimate the practical testing co st savings
with PRA. Fifth, we show the sensitivity of parameters used in the
experiment. The last part shows the overhead of PerfScope.
5.1 Subject Software
Six large-scale (up to millions of lines of code), popular op en-
source software are used as our subjects. They range from dat abase
system (MySQL, PostgreSQL), compiler (GCC), web (caching)
server (Apache, Squid) and JavaScript Engine (V8, used in Ch rome).Algorithm 1 Determine if a change is trivial
Input: change C, source code sufﬁx set Sufﬁxes , before-revision
program OldP , after-revision program NewP
Output: True if Cis trivial, False otherwise
1:ifC.ﬁle.sufﬁx /∈Sufﬁxes then
2: return True
3:end if
4: /* map: get instructions at a line in a program */
5: new_instrs←map(C.new_line, NewP)
6: old_instrs←map(C.old_line, OldP)
7:ifnew_instrs.empty andold_instrs.empty then
8: /* change only comments, spaces, etc. */
9: return True
10:end if
11:ifnew_instrs.size/negationslash=old_instrs.size then
12: /* change added or deleted instructions */
13: return False
14:end if
15:fori←1tonew_instrs.size do
16: /* diff: compare two instructions’ opcodes and operands */
17: ifdiff(old_instrs[i], new_instrs[i]) = True then
18: return False
19: end if
20:end for
21:return True
Table 6: Subject Software.
Software LOC Studied?
MySQL 1.2M Yes
PostgreSQL 651K Yes
Apache httpd 220K No
Squid 751K No
GCC 4.6M No
V8 680K No
Among them, GCC, Apache, Squid and V8 are notused in our real-
world study. Table 6 summarizes the details.
5.2 Methodology
Table 7: Benchmarks and regression thresholds used for sub-
ject software. §5.2.1 describes how we obtain the threshold s.
Software Benchmarks Threshold
MySQL DBT2, SysBench, sql-bench 6%
PostgreSQL DBT2, SysBench, pgbench 6%
Apache httpd SPECweb2005, autobench, ab 20%
Squid Web Polygraph, autobench 10%
GCC CP2K, SPEC CPU2006 3%
V8 Octane, SunSpider 10%
5.2.1 Ground Truth for New Commits
For the studied commits, we already know they caused perfor-
mance regressions. But for the new commits, since they are ta ken
from recent code repository, few feedback on them exists. We need
to get the ground truth for each commit with respect to whether it
may introduce performance regression. Therefore, we run mu ltiple
standard, moderately intensive performance benchmarks on each
compiled revision of subject software. These benchmarks ar e often
used internally and in user-reported regression cases. Eac h bench-
marking is run multiple times.66Table 8: Coverage of studied problematic commits.
SoftwareBuggy PRA
Rev. (Ratio)
MySQL 39 36
PostgreSQL 25 23
Total 64 59 (92% )
With the benchmarking results, we can compute the performan ce
deviation of a commit from the previous commit. Then if the de via-
tion exceeds certain threshold, the commit introduces perf ormance
regression. Therefore what thresholds to use for judging pe rfor-
mance regression is crucial. However, there is inevitable b ias in
choosing the criteria because performance regression is a funda-
mentally subjective deﬁnition. Different practitioners and software
use different judgment. For example, in our real-world stud y, the
user-reported performance regression issues contain a var iety of
criteria even for the same software.
To reduce the inevitable selection bias, we refer to the comm on
practices. In particular, we sample each software’s issue t racker,
code commit comment and developer posts to see what degree of
performance deviation starts to attract practitioners’ at tention. We
then choose the minimum as our criteria. Table 7 lists the bench-
marks and thresholds we use for each software.
Sometimes the commit causing repeatable regression (i.e., the
regression is observed consistently when the testing is exe rcised
several times) may not be a performance bug but rather an expe cted
performance behavior (e.g., a patch adding authentication step to
existing work ﬂow introduces performance overhead). It is u p to
developers to decide whether a repeatable regression is exp ected
or not. PRA isnota performance bug detector. It mainly serves
performance testing and therefore only target on repeatabl e regres-
sion rather than performance bug. Therefore, if PRA recommends
a commit that can indeed manifest repeatable regression in p erfor-
mance testing, the recommendation is considered to be usefu l.
5.2.2 Setup
The benchmarking is carried out on three dedicated machines ,
each with Intel i7 Quad Core CPU 3.40 GHz, 16GB RAM. The per-
formance overhead measurement is run on the same machines. A d-
ditionally, as PRA outputs the distribution of the change set’s risk
levels and a score, we need a criterion for recommending perf or-
mance regression testing. We set it to be if risk score (Equat ion 1)
is larger than 200. §5.7 evaluates the sensitivity of this cr iterion.
5.3 Evaluation on Studied Commits
Table 8 presents the coverage on the studied problematic com -
mits. 11 commits from MySQL issue study are omitted because
these changes are for source languages other than C/C++, whi ch
our tool currently supports. Chrome is not evaluated becaus e the
compiler our analysis bases on, LLVM Clang, cannot reliably com-
pile Chrome into whole-program LLVM bitcode ﬁles for analys is
rather than because of our analysis.
As the table shows, PRA can report majority (92%) of these re-
gressing commits. Although the basic PRA does not implement
slicing, it still can capture cases in the “Indirect” catego ry in Ta-
ble 4. This is because, the categorization only focuses on th eroot
cause . It can be the case that a change impacts performance indi-
rectly but happens to lie in a loop.
A few cases are missed by PRA either because of long propaga-
tion impact or complicated domain knowledge. Listing 1 is su ch
an example. More detailed proﬁle information and deeper ana lysis
are needed to be able to alert on them.Listing 1 Commit not alarmed by PRA
voidinit_read_record (READ_RECORD *info,THD *thd,)
{
/*The patch sets mmap flag, which later causes a
function pointer to be changed to mmap. */
+
+if(table->s->tmp_table ==TMP_TABLE &&...)
+VOID(table ->file->extra(HA_EXTRA_MMAP));
+
5.4 Evaluation on New Commits
Since our PRA design is guided by a real-world issue study, it
might be tailored for these examined buggy commits. Therefo re,
we also evaluate the tool on 600 new commits from both studied
and unstudied software. Table 9 presents the result.
TheAfter Filtering column is the number of commits remaining
after pruning. The ﬁltered commits (by our tool) either only change
non-source ﬁles or only have insigniﬁcant changes on source ﬁles.
Interestingly, ﬁltering already reduces a signiﬁcant numb er of com-
mits not worth consideration for performance regression te sting.
For example, in Apache, more than one third of the commits are just
updating documentation ﬁles or code styles. We manually che cked
the ﬁltered commits are indeed trivial.
Our tool PerfScope can successfully reduce at least 81% of the
450 testing candidates (86% if no ﬁltering is conducted in ex isting
testing) and alarm 87% of the new risky commits. In other words,
with our tool, developers only now need to test 19% of the orig inal
450 commits and still be able to alert 87% of the risky commits .
This means our PRA design can signiﬁcantly reduce performance
testing overhead while preserving relatively high coverag e.
From the table, we can also read the number of commits that
are reported by PerfScope but not conﬁrmed by our benchmarki ng
from Rec. Commits−(Risky Commits−Miss ). However,
they should notbe interpreted false alarms for two reasons. First,
PerfScope is not a bug detection tool but only to reduce testi ng
overhead. These “additionally” recommended testing targe ts need
to be tested anyway in the original performance testing sche me
without using PRA. Second, the Risky Commits in the table are
lower bounds because of the limitation of our benchmarking. There-
fore, some of these additional commits might turn out to be in deed
risky if tested more comprehensively.
5.5 Extension and Alternative Solution
PerfScope also implements an extension to PRA:PRA-Slicing
(§3.2.5). In addition to the basic analysis, PRA-Slicing also per-
forms forward slicing of each change in the commit and checks if
any element in the slice is performance sensitive or not.
Table 9 also shows the evaluation of PRA-Slicing .PRA-Slicing
performs deeper analysis and as a result has higher coverage (95%)
but at the cost of lower reduction percentage (78%).
A simple alternative to PRA is random test target selection. The
probability for this approach to achieve the same or better r esult as
our tool in Table 9 is only 1.2×10−65(calculated using script2).
5.6 Practical Cost Saving
The objective of PRA is to reduce the testing target set to the
risky commits for performance regression testing. Previou s sec-
tions mainly evaluate the number of reduced testing target. How
much does the reduction translate to actual testing cost sav ing? It
depends on how comprehensive the original testing is carrie d out.
2http://ideone.com/6d70fJ67Table 9: Evaluation of PerfScope on new commits. *: the ﬁlter ing is done automatically by our tool with Algorithm 1. For re duction
rate and testing saving, larger number is based on the 600 com mits; smaller number is based on the 450 commits.
SoftwareTest Risky After PRA PRA-Slicing
Commits Commits Filtering∗Rec. Commits Miss Testing Rec. Commits Miss Testing
(Reduction) (Coverage) Savings (hrs.) (Reduction) (Coverage) Savings (hrs.)
MySQL 100 9 73 19 (74–81%) 2 324–486 22 (70–78%) 1 306–468
PostgreSQL 100 6 76 12 (84–88%) 0 384–528 16 (79–84%) 0 360–504
GCC 100 6 76 18 (76–82%) 1 870–1230 19 (75–81%) 0 855–1215
V8 100 7 85 13 (85–87%) 2 3–4 17 (80–83%) 1 3–4
Apache 100 5 60 11 (82–89%) 0 74–134 12 (80–88%) 0 72–132
Squid 100 6 80 12 (85–88%) 0 204–264 14 (83–86%) 0 198–258
Total 600 39 450 85 (81–86% ) 5 (87% ) 1859–2646 100 ( 78–83% ) 2 (95% ) 1794–2581
7075808590
100 200 300
Risk score thresholdPercentage (%)Reduction
Coverage
Figure 7: Sensitivity of PerfScope’s reduction rate and cov er-
age rate of the new commits evaluation to the recommendation
criterion (≥risk _score _threshold ).
In our experiment, we run the benchmarks using recommended s et-
tings from popular performance testing guides or the tools.
Therefore, we calculate the expected testing cost saving ac cord-
ing to our setup. As we run multiple benchmarks for each softw are,
we use the average running cost as the per iteration testing c ost.
The per commit cost is calculated by multiplying the per iter ation
cost with 3 iterations as used in our experiment. In speciﬁcs , for
MySQL and PostgreSQL, the per commit testing cost is 6 hours;
for GCC, it is 15 hours; for V8, it is 3 minutes; for Apache, it i s 1.5
hours; for Squid, it is 3 hours;
Table 9 lists the result. The lower saving is calculated assu ming
the original testing already skips all non-essential commi ts. The
higher saving assumes no ﬁltering is conducted. PerfScope i s most
useful for software whose original performance regression testing
cost is big. For example, for GCC, the saving can be almost two
months. However, for V8, the saving is limited because the be nch-
mark we use only takes minutes to test one revision. Admitted ly,
for software whose performance testing overhead is very sma ll,
PerfScope does not save much. Directly running performance test-
ing on every commit is then a better option. One possible usag e
scenario for this type of software is to run PerfScope as “cro ss ref-
erence” with the performance testing result.
5.7 Sensitivity
Since the output of PRA is risk level distribution and a summary
score, a criterion based on the output is necessary to decide whether
to recommend the given code commit to be tested. The evaluati on
in previous sections used the 200 risk score threshold as the criteria.
Figure 7 measures the reduction and coverage sensitivity to the
risk score threshold. In general, reduction rate increases as the riskTable 10: PerfScope running time (in seconds) and break-
down. The numbers in parentheses are for PRA-Slicing .
SoftwareLoading Analysis Total
Module (PRA-Slicing ) (PRA-Slicing )
MySQL 40 6 (195) 46 (235)
PostgreSQL 11 2 (183) 13 (194)
GCC 36 5 (253) 41 (289)
V8 86 10 (259) 96 (344)
Apache 5 1 (4) 6 (9)
Squid 28 5 (6) 33 (34)
score criteria increases because higher threshold would re sult in
fewer number of commits to be recommended and thus achieving
higher reduction; in contrast, coverage decreases as the cr iteria in-
creases. Such variety means, like performance regression t esting
result judgment criteria, the selection criteria requires initial tun-
ing. But as seen from the ﬁgure, the sensitivity in certain th reshold
ranges is is small because problematic and regular commits u sually
have quite different risk level distribution.
5.8 Performance
As the primary goal of PRA is to reduce performance testing
overhead through testing target selection, the analysis it self shouldn’t
become a new bottleneck.
Table 10 shows that in the evaluated software, PerfScope’s a ver-
age execution time is within 2 minutes for PRA and 6 minutes for
PRA-Slicing . The “Loading Module” breakdown is the time to load
LLVM object ﬁles for analysis using LLVM API ParseIRFile .
It is a constant cost and occupies a large portion of the total time
for both PRA andPRA-Slicing .
6. RELATED WORK
Performance Regression Testing : There are case studies and re-
search effort on performance regression testing in softwar e sys-
tems [21, 36, 60, 17, 30]. To name a few, [21] details the Linux ker-
nel performance testing project to catch kernel performanc e regres-
sion issues. [36] shares the experience in automating regre ssion
benchmarking for the Mono project. [17] proposes a model-ba sed
performance testing framework to generate appropriate wor kloads.
[30] offers a learning-based performance testing framewor k that
automatically selects test input data based on learned rule s. [61]
uses symbolic execution to generate load test suites that ex pose
program’s diverse resource consumption behaviors.
These efforts focus building better performance regressio n test-
ing infrastructure and test cases. Our work assumes the exis tence
of good performance testing infrastructure and test cases, and im-
proves the testing efﬁciency by prioritizing testing targe t.
Performance Bug Detection and Analysis : A wealth of literature
exists on performance analysis with regard to performance d ebug-68ging [11, 53, 41, 31], performance bug detection [37, 35, 34, 59],
performance regression testing result analysis [26, 27].
Similar to these work, the ultimate goal of our work is to help un-
cover performance problems. But we do notattempt to detect per-
formance regression bugs or provide forensic diagnosis but target
on recommending risky commit for performance regression te st-
ing. For the performance regression testing result analysi s work,
we complements the work by improving performance testing ef ﬁ-
ciency by better utilizing the testing resources on risky co mmit.
Regression Testing Efﬁciency : Much work has been done to re-
duce functional regression testing cost by test case selection ([22,
50]), test suite reduction([32, 19, 62]) and test cases prio ritization([51,
25, 52, 38, 42]). Test case selection realizes this through s electing a
subset of test cases in the test suite based on test case prope rty and
code modiﬁcation information. Test suite reduction works o n re-
moving redundancy in test suite. Test case prioritization o rders test
case execution in a way to meet time constraints in hope to det ect
fault faster.
Different from these work, our goal is to reduce performance re-
gression testing overhead via testing target prioritizati on. In func-
tional regression testing, work that also analyzes code mod iﬁca-
tions focuses on code coverage. But in the context of perform ance,
more important is information such as whether an operation i s ex-
pensive or lies in critical path. The analysis we propose is s peciﬁ-
cally for assessing commits’ performance risk.
There are also practices on improving performance regressi on
testing efﬁciency mainly through hierarchical test case or ganiza-
tion. We differ from them in that we take a white-box approach to
prioritize test target by analyzing commit content.
Impact Analysis : There is fruitful work on software change impact
analysis techniques to compute the set of program elements t hat
may be affected by the change ( impact set ) [14, 56, 13, 47, 10,
20, 12]. They can be broadly divided into three categories: s tatic
analysis based [14, 56, 13], dynamic execution based [40, 47 , 48,
12] and history-based [63, 54].
Our proposed method is inspired by these work. The key dif-
ference is that we focus on the performance risk implication of
change, instead of the impact set .PRA assesses the risk of a code
change to introduce performance regression in addition to t he im-
pact set.
Additionally, many impact analysis work focuses on functio n
level. But PRA needs to examine more ﬁne grained statement level
for detailed analysis. This not only poses challenges in the analy-
sis but also on mapping from textual changes to the correspon ding
programming constructs (§4.2).
Worst-Case Execution Time Analysis : Analyzing the worst case
execution time (WCET) [33, 58] of a task to be executed on a spe -
ciﬁc hardware is a necessary process for reliable real-time system
because of its stringent timing constraints. Our PRA is similar as
the static approach in WCET analysis. For example both PRA and
WCET analysis needs to use control ﬂow information and bound
calculation to determine the worst case execution path.
However, WCET analysis mainly applies to real-time systems as
they have restricted form of programming (e.g., no recursio n al-
lowed). PRA works on regular performance-critical software writ-
ten in standard C/C++ that supports generic programming con -
structs. The biggest size of tasks analyzed by WCET analysis tool
is around 50K LOC [58]. PRA can scale to millions of LOC for
regular software.
More importantly, PRA does not aim to predict the absolute per-
formance bound for the entire program. Instead, PRA calculates
the relative performance riskintroduced by given code change to
reduce testing target set. The analysis is focused in code ch angescopes. This makes PRA light-weight enough to ﬁt in the perfor-
mance testing cycle. In contrast, WCET analysis needs to obt ain
a safe bound as a worst-case guarantee for the entire program . It
therefore requires careful modeling of underlying archite ctural be-
haviors (e.g., branch prediction) and often requires user a nnotations
(e.g., loop bounds, ﬂow facts), which is very expensive to pe rform
on a per-commit basis to be used by performance testing.
7. LIMITATIONS AND DISCUSSIONS
There are limitations in the current implementation that we are
considering for future work.
First, our PRA is designed to recommend straightforward perfor-
mance regression issues. While this makes the analysis ligh tweight,
the analysis may not accurately assess the risk of sophistic ated per-
formance regression issues such as resource contention, ca ching
effect. Second, although our cost model is generic for both c om-
putation and I/O, detailed modeling and proﬁling are needed if I/O
behavior is of particular interest. Therefore our current m odeling
has limited applicability to software like OS kernel. Also o ur model
does not apply to networked software. Third, since for a comm it
that is considered to be potentially risky, PRA knows the program
points that are risky. With the risky program points and test cases’
coverage information, we are extending the analysis to not o nly
select testing target but also recommend which test case may po-
tentially expose the performance issue in the risky version .
8. CONCLUSIONS AND FUTURE WORK
In this paper, we propose a new approach, performance risk an al-
ysis ( PRA), to improve performance regression testing efﬁciency
through testing target prioritization. It analyzes the ris k of a given
code commit in introducing performance regression. To gain deep
understanding of perilous commits’ code characteristics, we con-
duct a study on 100 randomly sampled real-world performance re-
gression issues from three large popular software. Based on the
insights from the study, we propose a PRA design and implement a
tool, PerfScope. Evaluation on the studied problematic com mits
shows PerfScope can successfully recommend 92% of them for
testing. We also evaluate the tool on 600 newcommits that are not
studied. PerfScope signiﬁcantly reduces the testing overh ead by
recommending only 14-22% of the 600 commits and is still able to
cover 87-95% of the risky commits. Experiment demonstrates the
analysis is lightweight. The source code of PerfScope is rel eased at
http://cseweb.ucsd.edu/~peh003/perfscope .
9. ACKNOWLEDGEMENTS
We would like to thank the anonymous reviewers for the in-
sightful comments and suggestions. We are very grateful to D ing
Yuan, Soyeon Park, Weiwei Xiong for their constructive feed back.
Tianyin Xu generously provided dedicated machines for us to run
experiments. Rui Zhang lifted the morale with constant enco ur-
agement. This work is supported by NSF CNS-1017784, and NSF
CNS-1321006.
10. REFERENCES
[1] Approaches to performance testing. http://www.oracle.com/
technetwork/articles/entarch/
performance-testing-095962.html .
[2] Chrome bug 56752. https://code.google.com/p/
chromium/issues/detail?id=56752 .
[3] Chromium’s performance testing machines. http://build.
chromium.org/p/chromium.perf/buildslaves .
[4] Code Bisection. http://en.wikipedia.org/wiki/Code_
Bisection .69[5] Considerations for load tests. http://msdn.microsoft.com/
en-us/library/ms404664.aspx .
[6] Google performance testing. http://googletesting.
blogspot.com/2007/10/performance-testing.html .
[7] Mysql bug 16504. http://bugs.mysql.com/bug.php?
id=16504 .
[8] OProﬁle. http://oprofile.sourceforge.net .
[9] pgbench Good Practices. http://www.postgresql.org/
docs/devel/static/pgbench.html .
[10] A CHARYA , M., AND ROBINSON , B. Practical change impact
analysis based on static program slicing for industrial sof tware
systems. ICSE ’11, ACM, pp. 746–755.
[11] A GUILERA , M. K., M OGUL , J. C., W IENER , J. L., R EYNOLDS , P.,
AND MUTHITACHAROEN , A. Performance debugging for distributed
systems of black boxes. SOSP ’03, ACM, pp. 74–89.
[12] A PIWATTANAPONG , T., O RSO, A., AND HARROLD , M. J. Efﬁcient
and precise dynamic impact analysis using execute-after se quences.
ICSE ’05, ACM, pp. 432–441.
[13] A RNOLD , R. S. Software Change Impact Analysis . IEEE Computer
Society Press, Los Alamitos, CA, USA, 1996.
[14] A RNOLD , R. S., AND BOHNER , S. A. Impact analysis - towards a
framework for comparison. In ICSM (1993), pp. 292–301.
[15] B AKER , M. Mozilla performance regression policy. http://www.
mozilla.org/hacking/regression-policy.html .
[16] B ARBER , S. Life-cycle performance testing for eliminating
last-minute surprises. http://msdn.microsoft.com/
en-us/library/bb905531.aspx .
[17] B ARNA , C., L ITOIU , M., AND GHANBARI , H. Model-based
performance testing (nier track). ICSE ’11, ACM, pp. 872–87 5.
[18] B IEMAN , J. Editorial: Is anyone listening? Software Quality Journal
13, 3 (Sept. 2005), 225–226.
[19] B LACK , J., M ELACHRINOUDIS , E., AND KAELI , D. Bi-criteria
models for all-uses test suite reduction. ICSE ’04, IEEE Com puter
Society, pp. 106–115.
[20] B OHNER , S. A. Extending software change impact analysis into cots
components. SEW ’02, IEEE Computer Society, pp. 175–182.
[21] C HEN, T., A NANIEV , L. I., AND TIKHONOV , A. V. Keeping kernel
performance from regressions. vol. 1 of OLS’ 07 , pp. 93–102.
[22] C HEN, Y.-F., R OSENBLUM , D. S., AND VO, K.-P. Testtube: a
system for selective regression testing. ICSE ’94, IEEE Com puter
Society Press, pp. 211–220.
[23] C ORBET , J. Performance regression discussion in kernel summit
2010.http://lwn.net/Articles/412747/ .
[24] D ENARO , G., P OLINI , A., AND EMMERICH , W. Early performance
testing of distributed software applications. WOSP ’04, AC M,
pp. 94–103.
[25] E LBAUM , S., M ALISHEVSKY , A. G., AND ROTHERMEL , G.
Prioritizing test cases for regression testing. ISSTA ’00, ACM,
pp. 102–112.
[26] F OO, K. C., J IANG , Z. M., A DAMS , B., H ASSAN , A. E., Z OU, Y.,
AND FLORA , P. Mining performance regression testing repositories
for automated performance analysis. QSIC ’10, IEEE Compute r
Society, pp. 32–41.
[27] F OO, K. C. D. Automated discovery of performance regressions in
enterprise applications. Master’s thesis, Queen’s Univer sity, Canada,
2011.
[28] F OX, G. Performance engineering as a part of the development lif e
cycle for large-scale software systems. ICSE ’89, ACM, pp. 8 5–94.
[29] G LEK, T. Massive performance regression from switching to gcc 4. 5.
http://gcc.gnu.org/ml/gcc/2010-06/msg00715.
html .
[30] G RECHANIK , M., F U, C., AND XIE, Q. Automatically ﬁnding
performance problems with feedback-directed learning sof tware
testing. ICSE 2012, IEEE Press, pp. 156–166.
[31] H AN, S., D ANG , Y., G E, S., Z HANG , D., AND XIE, T.
Performance debugging in the large via mining millions of st ack
traces. ICSE 2012, IEEE Press, pp. 145–155.
[32] H ARROLD , M. J., G UPTA , R., AND SOFFA , M. L. A methodology
for controlling the size of a test suite. ACM Trans. Softw. Eng.
Methodol. 2 , 3 (July 1993), 270–285.[33] H ECKMANN , R., F ERDINAND , C., A NGEWANDTE , A., AND
GMBH , I. Worst-case execution time prediction by static program
analysis. IPDPS 2004, IEEE Computer Society, pp. 26–30.
[34] J IN, G., S ONG , L., S HI, X., S CHERPELZ , J., AND LU, S.
Understanding and detecting real-world performance bugs. PLDI
’12, ACM, pp. 77–88.
[35] J OVIC , M., A DAMOLI , A., AND HAUSWIRTH , M. Catch me if you
can: performance bug detection in the wild. OOPSLA ’11, ACM,
pp. 155–170.
[36] K ALIBERA , T., B ULEJ , L., AND TUMA , P. Automated detection of
performance regressions: The mono experience. In MASCOTS
(2005), pp. 183–190.
[37] K ILLIAN , C., N AGARAJ , K., P ERVEZ , S., B RAUD , R.,
ANDERSON , J. W., AND JHALA , R. Finding latent performance
bugs in systems implementations. FSE ’10, ACM, pp. 17–26.
[38] K IM, J.-M., AND PORTER , A. A history-based test prioritization
technique for regression testing in resource constrained
environments. ICSE ’02, ACM, pp. 119–129.
[39] L ATTNER , C., AND ADVE, V. LLVM: A compilation framework for
lifelong program analysis & transformation. CGO ’04, IEEE
Computer Society, pp. 75–86.
[40] L AW, J., AND ROTHERMEL , G. Whole program path-based dynamic
impact analysis. ICSE ’03, IEEE Computer Society, pp. 308–3 18.
[41] L EUNG , A. W., L ALONDE , E., T ELLEEN , J., D AVIS , J., AND
MALTZAHN , C. Using comprehensive analysis for performance
debugging in distributed storage systems. In MSST (2007),
pp. 281–286.
[42] L I, Z., H ARMAN , M., AND HIERONS , R. M. Search algorithms for
regression test case prioritization. IEEE Trans. Softw. Eng. 33 , 4
(Apr. 2007), 225–237.
[43] M ARTIN , R. C. Agile Software Development: Principles, Patterns,
and Practices . Prentice Hall PTR, Upper Saddle River, NJ, USA,
2003.
[44] M AYER , M. In search of a better, faster, stronger web. http://
goo.gl/m4fXx , 2009.
[45] M ITCHELL , M. GCC performance regression testing discussion.
http://gcc.gnu.org/ml/gcc/2005-11/msg01306.
html .
[46] M OLYNEAUX , I.The Art of Application Performance Testing: Help
for Programmers and Quality Assurance , 1st ed. O’Reilly Media,
Inc., 2009.
[47] O RSO, A., A PIWATTANAPONG , T., AND HARROLD , M. J.
Leveraging ﬁeld data for impact analysis and regression tes ting.
ESEC/FSE-11, ACM, pp. 128–137.
[48] O RSO, A., A PIWATTANAPONG , T., L AW, J. B., R OTHERMEL , G.,
AND HARROLD , M. J. An empirical comparison of dynamic impact
analysis algorithms. ICSE ’04, pp. 491–500.
[49] P ERSHAD , T., AND BARNIR, O. Software quality and testing in
mysql. MySQL Conference and Expo, 2009.
[50] R OTHERMEL , G., AND HARROLD , M. J. A safe, efﬁcient regression
test selection technique. ACM Trans. Softw. Eng. Methodol. 6 , 2 (Apr.
1997), 173–210.
[51] R OTHERMEL , G., U NTCH , R. H., C HU, C., AND HARROLD , M. J.
Test case prioritization: An empirical study. ICSM ’99, IEE E
Computer Society, pp. 179–188.
[52] R OTHERMEL , G., U NTCH , R. J., AND CHU, C. Prioritizing test
cases for regression testing. IEEE Trans. Softw. Eng. 27 (October
2001), 929–948.
[53] S HEN, K., Z HONG , M., AND LI, C. I/O system performance
debugging using model-driven anomaly characterization. F AST ’05,
USENIX Association, pp. 23–23.
[54] S HERRIFF , M., AND WILLIAMS , L. Empirical software change
impact analysis using singular value decomposition. In ICST (2008),
pp. 268–277.
[55] T IP, F. A survey of program slicing techniques. Journal of
programming languages 3 , 3 (1995), 121–189.
[56] T URVER , R. J., AND MUNRO , M. An early impact analysis
technique for software maintenance. Journal of Software
Maintenance: Research and Practice 6 , 1 (1994), 35–52.70[57] W EYUKER , E. J., AND VOKOLOS , F. I. Experience with
performance testing of software systems: Issues, an approa ch, and
case study. IEEE Trans. Softw. Eng. 26 , 12 (Dec. 2000), 1147–1156.
[58] W ILHELM , R., E NGBLOM , J., E RMEDAHL , A., H OLSTI , N.,
THESING , S., W HALLEY , D., B ERNAT , G., F ERDINAND , C.,
HECKMANN , R., M ITRA , T., M UELLER , F., P UAUT , I.,
PUSCHNER , P., S TASCHULAT , J., AND STENSTRÖM , P. The
worst-case execution-time problem – overview of methods an d
survey of tools. ACM Trans. Embed. Comput. Syst. 7 , 3 (May 2008),
36:1–36:53.
[59] Y AN, D., X U, G., AND ROUNTEV , A. Uncovering performance
problems in java applications with reference propagation p roﬁling.
ICSE 2012, IEEE Press, pp. 134–144.
[60] Y ILMAZ , C., K RISHNA , A. S., M EMON , A., P ORTER , A.,
SCHMIDT , D. C., G OKHALE , A., AND NATARAJAN , B. Maineffects screening: a distributed continuous quality assur ance process
for monitoring performance degradation in evolving softwa re
systems. ICSE ’05, ACM, pp. 293–302.
[61] Z HANG , P., E LBAUM , S., AND DWYER , M. B. Automatic
generation of load tests. In Proceedings of the 2011 26th IEEE/ACM
International Conference on Automated Software Engineeri ng
(Washington, DC, USA, 2011), ASE ’11, IEEE Computer Society ,
pp. 43–52.
[62] Z HONG , H., Z HANG , L., AND MEI, H. An experimental
comparison of four test suite reduction techniques. ICSE ’0 6, ACM,
pp. 636–640.
[63] Z IMMERMANN , T., W EISGERBER , P., D IEHL , S., AND ZELLER ,
A. Mining version histories to guide software changes. ICSE ’04,
IEEE Computer Society, pp. 563–572.71