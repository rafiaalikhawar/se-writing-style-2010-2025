Verifying Self-adaptive Applications Suffering Uncertainty
Wenhua Y ang¬ß‚Ä†, Chang Xu¬ß‚Ä†, Y epang Liu‚Ä°, Chun Cao¬ß‚Ä†, Xiaoxing Ma¬ß‚Ä†, Jian Lu¬ß‚Ä†
¬ßState Key Lab for Novel Soft. Tech., Nanjing University, Nanjing, China
‚Ä†Dept. of Comp. Sci. and Tech., Nanjing University, Nanjing, China
ihope1024@gmail.com, {changxu, caochun, xxm, lj}@nju.edu.cn
‚Ä°Dept. of Comp. Sci. and Engr., The Hong Kong Univ. of Sci. and Tech., Hong Kong, China
andrewust@cse.ust.hk
ABSTRACT
Self-adaptive applications address environmental dynamics system-
atically. They can be faulty and exhibit runtime errors when en-
vironmental dynamics are not considered adequately. It becomes
more severe when uncertainty exists in their sensing and adaptation
to environments. Existing work veriÔ¨Åes self-adaptive applications,
but does not explicitly consider environmental constraints or uncer-
tainty. This gives rise to inaccurate veriÔ¨Åcation results. In this pa-
per, we address this problem by proposing a novel approach to ver-
ifying self-adaptive applications suffering uncertainty in their envi-
ronmental interactions. It builds I nteracti
ve State
Machine
(ISM)
models for such applications and veriÔ¨Åes them with explicit con-
sideration of environmental constraints and uncertainty. It then re-
Ô¨Ånes veriÔ¨Åcation results by prioritizing counterexamples according
to their probabilities. We experimentally evaluated our approach
with real-life self-adaptive applications, and the experimental re-
sults conÔ¨Årmed its effectiveness. Our approach reported 200-660%
more counterexamples than not considering uncertainty, and elim-
inated all false counterexamples caused by ignoring environmental
constraints.
Categories and Subject Descriptors
D.2.4 [ Software Engineering ]: Software/Program VeriÔ¨Åcation; D.
2.5 [Software Engineering]: Testing and Debugging
General Terms
VeriÔ¨Åcation, Reliability, Experimentation
Keywords
Self-adaptive application; veriÔ¨Åcation; uncertainty
1. INTRODUCTION
Self-adaptive applications are gaining increasing popularity, e.g.,
Locale [1], Phone-Adapter [28, 29] and Navia [2]. These appli-
cations continually sense their environments and make adaptation
Corresponding
author
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proÔ¨Åt or commercial advantage and that copies bear this notice and the full cita-
tion on the Ô¨Årst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciÔ¨Åc permission
and/or a fee. Request permissions from permissions@acm.org.
ASE‚Äô14, September 15-19, 2014, Vasteras, Sweden.
Copyright 2014 ACM 978-1-4503-3013-8/14/09 ...$15.00.
http://dx.doi.org/10.1145/2642937.2642999 .according to their predeÔ¨Åned logics [23, 28]. This forms a reaction
loop [6], and an application‚Äôs adaptation upon certain environmen-
tal changes can then affect environmental sensing afterwards. Thus
self-adaptive applications and their environments are together cre-
ating complex and correlated interactions. This causes challenges
to building dependable self-adaptive applications, since develop-
ers have to consider every detail of such interactions in a predic-
tive way. Real-world self-adaptive applications are thus error-prone
[20, 28, 32, 34].
To assure the quality of self-adaptive applications, many research
efforts focus on sophisticated testing or debugging techniques. Some
techniques [8, 24, 34] use fault patterns to dynamically detect and
analyze faults in these applications. Others work on test case gen-
eration [30, 32] or test adequacy criteria [22] to support effective
fault detection. However, one outstanding challenge in fault de-
tection for self-adaptive applications is that these applications keep
interacting with dynamic environments. It is generally infeasible
to predict and enumerate all possible environmental conditions that
an application can encounter at runtime [23, 27]. This makes exist-
ing techniques unable to test self-adaptive applications adequately
and precisely. For example, Ramirez et al. [27] used simulation
to explore environmental conditions that may cause requirement or
behavior violation to an application, but only limited simulation
environments can be tested due to its complexity and cost.
On the other hand, formal methods such as model checking and
theorem proving rigorously verify an application‚Äôs behavior [33].
Given an application, these techniques exhaustively explore its state
space to detect potential errors (e.g., dead state or abnormal be-
havior). Recent studies have also reported promising results in
applying these techniques to verifying properties of self-adaptive
applications, e.g., safety [13, 40], reliability [7, 17], liveness and
reachability [21, 28, 29, 40], consistency [21] and stability [4, 28,
29]. However, when it comes to verifying self-adaptive applica-
tions under real-world environments, these pieces of existing work
have two major limitations:
Lack of modeling environmental constraints. First, an appli-
cation‚Äôs running environment can be subject to implicit constraints
enforced by this environment‚Äôs physical laws or assumed by the
application‚Äôs prior knowledge. Consider an example robot-car ap-
plication [38, 39] that aims to explore an unknown area without
bumping into any obstacle. The car can sense its four-directional
distances (front, back, left and right) to nearby obstacles using its
built-in ultrasonic sensors. Based on these sensed distances, it de-
cides its next movement to avoid obstacles. Suppose that the car
keeps walking forward with an obstacle detected ahead. Its sensed
distance to this obstacle would keep decreasing. One can derive
a constraint for this situation: the last sensed distance should be
equal to the summation of the new sensed distance and how far the
199
car
has walked since its last sensing. Such constraints relate en-
vironmental sensing (e.g., distance sensing) to application adapta-
tion (e.g., car walking). We name them environmental constraints.
Overlooking them can lead to inaccurate veriÔ¨Åcation results. For
example, Sama et al. [28, 29] used model checking to detect faults
for self-adaptive applications. The work does not model or use
environmental constraints in veriÔ¨Åcation, and thus many reported
faults are false positives [21].
Lack of modeling uncertainty. Second, an application‚Äôs en-
vironmental sensing and adaptation can be affected by uncertainty,
which is inevitably caused by unreliable environmental sensing and
Ô¨Çawed physical actions [26]. For environmental sensing, an ap-
plication may only be able to obtain an estimate of its environ-
mental conditions, but never know its real state. For example,
the car‚Äôs distance sensing always contains unpredictable noise, and
sensed values may not faithfully reÔ¨Çect real distances from the car
to its nearby obstacles. Similarly, for adaptation an application can
only interact with its environment as designed, but may not know
whether the interaction indeed proceeds as expected. For example,
the car can make 90‚ó¶- left or -right turns but with unpredictable er-
ror. In practice, a left turn can actually be 85‚ó¶and the car may not
be able to know this error. Such sensing and adaptation uncertainty
can cause inconsistency between an application‚Äôs understanding to
its environment and its actual environmental conditions, thus af-
fecting the application‚Äôs functionalities [27]. Similar to environ-
mental constraints, overlooking such uncertainty can also lead to
inaccurate veriÔ¨Åcation results. However, to the best of our knowl-
edge, none of existing work [4, 7, 17, 21, 28, 29, 40] has explicitly
modeled and considered such uncertainty in verifying self-adaptive
applications.
In this paper, we address these two limitations by proposing a
novel approach to verifying self-adaptive applications under uncer-
tain environmental dynamics. Our approach works in three phases:
Phase 1: Modeling adaptation logic and environmental con-
straints. We model the reaction loop of a self-adaptive application
with I nteracti
ve State
Machine
(ISM), and explicitly specify envi-
ronmental constraints for the application.
Phase 2: Verifying the model with uncertainty considered.
We consider the uncertainty in this application‚Äôs sensing and adap-
tation to its environment by specifying the error range and distribu-
tion of each related variable. We then verify the application through
its ISM model taking all error ranges into account.
Phase 3: Prioritizing counterexamples. From the veriÔ¨Åcation,
we obtain counterexamples that lead to the application‚Äôs potential
runtime errors . We reÔ¨Åne veriÔ¨Åcation results by ranking generated
counterexamples according to their occurrence probabilities.
We show that by modeling and exploiting such environmental
constraints and uncertainty, verifying self-adaptive applications can
receive results with greatly increased accuracy. Our approach re-
ported 200-660% more counterexamples than not considering un-
certainty, and eliminated all false counterexamples caused by ig-
noring environmental constraints. The experimental results consis-
tently showed that our veriÔ¨Åcation approach is more accurate and
achieves good performance as well as scalability.
We summarize our contributions in this paper below:
We propose a novel ISM model to explicitly consider envi-
ronmental constraints and uncertainty in verifying self-adapti-
ve applications. This greatly increases veriÔ¨Åcation accuracy.
We propose a prioritization technique to rank counterexam-
ples generated from veriÔ¨Åcation according to their occurrence
probabilities. This enables developers to focus on most likely
faults.We evaluate our veriÔ¨Åcation approach with self-adaptive ap-
plications by both real and simulation experiments, and vali-
date its usefulness in practice.
The remainder of this paper is organized as follows. Section 2 in-
troduces our modeling of self-adaptive applications. Section 3 uses
a motivating example to explain the inadequacy of existing work
and motivate our work. Section 4 presents our veriÔ¨Åcation approach
in detail. Section 5 validates our approach with self-adaptive appli-
cations. Section 6 discusses related work, and Ô¨Ånally Section 7
concludes this paper.
2. MODELING SELF-ADAPTIVE
APPLICATIONS
As mentioned earlier, self-adaptive applications continually sense
their environments and make adaptation upon perceived environ-
mental changes. Their functionalities are realized by the collab-
oration of three parts: environmental sensing, decision making,
and application adaptation. Thus, a self-adaptive application‚Äôs run-
ning can be described by a reaction loop, as illustrated in Figure 1.
First, the application senses its environment to capture interesting
environmental changes. Then, according to its predeÔ¨Åned logics,
the application makes a decision by selecting appropriate adapta-
tion to such environmental changes. The adaptation can change
the environment and affect the application‚Äôs environmental sens-
ing afterwards. Thus the environmental sensing is related to the
adaptation by environmental constraints as explained earlier. As
Figure 1 shows, the sensing and adaptation can also be affected
by uncertainty, which is caused by imperfect sensing technologies
and Ô¨Çawed physical actions nowadays. Since existing modeling ap-
proaches for self-adaptive applications such as A-FSM [28, 29] do
not explicitly consider how an application‚Äôs adaptation affects its
environmental sensing afterwards, we propose a new model, named
Interacti
ve State
Machine
(ISM) to meet this requirement.
Given a self-adaptive application, we deÔ¨Åne its ISM as a tuple
M:= (S;V;R;s0), in which symbols are explained below:
Sis a set of this application‚Äôs all states, and s02Sis its initial
state, with which the application starts.
Vis a set containing this application‚Äôs all variables. V=
Vs[Vn, in which VsandVnrepresent two disjointed cate-
gories. Vscontains all sensing variables , which store values
of environmental attributes interesting to this application (up-
dated by relevant sensing devices). Vncontains other normal
variables, i.e., non-sensing variables.
Ris a set containing this application‚Äôs all adaptation rules
(or rules for short). For each rule r2R,ris associated with
a state s2S, which is r‚Äôs source state. Rule rtakes a form
ofr:= (condition; actions) .condition is a logical formula
built on V, and its satisfaction would trigger the execution
of this rule. actions speciÔ¨Åes what should be done when ex-
ecuting this rule. actions can include internal actions and
interactive actions . The former takes internal adaptation by
updating values of non-sensing variables, e.g., updating the
application‚Äôs current state, and in this case the application
transits to a new state. The latter takes interactive adaptation
by interacting with the application‚Äôs environment directly,
e.g., making a robot-car move forward. interactive actions
can also specify the extents of their effects on the environ-
ment by updating certain non-sensing variables, e.g., record-
ing the distance that the car has moved forward. interactive
200ac
tions would also update values of sensing variables implic-
itly through environment constraints (the updates are explic-
itly performed upon next environmental sensing).
ISM is executable. Starting from its initial state s0, an ISM
M:= (S;V;R;s0)repeatedly reads values of its sensing variables
(automatically updated by environmental sensing), then evaluates
and decides which rule to execute, and Ô¨Ånally conducts the exe-
cuted rule‚Äôs associated actions. When a state s2Sis set as M‚Äôs cur-
rent state, rules having this state as source state are enabled, while
other rules are disabled. Only enabled rules participate in rule eval-
uation upon each environmental sensing. When an enabled rule r‚Äôs
condition r:condition is satisÔ¨Åed, the rule is triggered for execu-
tion. If multiple rules are triggered, only one of them is selected
to execute. This tie can be resolved by some priority or random
mechanisms [28, 35, 36], which are not our focus in this paper and
therefore omitted. When a rule ris selected to execute, its actions
r:actions are conducted in a sequential way. Conducting actions
concurrently might be possible for certain scenarios, but this also
is not our focus in this paper and therefore omitted. Thus, an ISM
M‚Äôs execution can be conceptually modeled by a path which is a
sequence of states and rules: s=s0r1s1:::rnsn. Similar to tradi-
tional programming, we deÔ¨Åne path condition of execution sas:
pc(s) =n‚àß
i=1ri:condition
To be representative, we adopt a quantiÔ¨Åer-free Ô¨Årst-order logic
based language for specifying a rule‚Äôs condition. With this lan-
guage, a rule‚Äôs condition can be speciÔ¨Åed by a logical formula that
is recursively constructed using the following syntax:
f:= ( f)and (f)j(f)or(f)j(f)implies (f)jnot (f)j
bfunc( v; ::: ; v):
Note that the above ‚Äúand‚Äù, ‚Äúor‚Äù, ‚Äúimplies‚Äù and ‚Äúnot‚Äù logical con-
nectives follow their traditional interpretations, i.e., representing
conjunction, disjunction, implication and negation operations, re-
spectively. Terminal bfunc refers to any user-deÔ¨Åned or domain-
speciÔ¨Åc function that returns either true or false. When a bfunc is
easy to understand, one may also use its corresponding operator
to simplify its representation, e.g., ‚Äú largerT han( v;50)‚Äù can also be
represented by ‚Äú v>50‚Äù.
3. MOTIVATING EXAMPLE
In this section, we present a motivating example using our afore-
mentioned self-adaptive robot-car application. The application con-
trols a robot-car to explore an unknown area and avoid bumping
into any obstacle. If the car bumps into an obstacle, we say that
the application fails. We Ô¨Årst use our ISM model to specify this
application‚Äôs adaptation logics. We illustrate it partially in Figure 2
due to page limit, but this sufÔ¨Åces for explaining our problem.
The ISM model for this application is: M:= (S;V;R;s0), where
S:=fA;B;:::; E;:::g,R:=fr0;r1;:::; r14;:::g,Vis the set of
variables used in this application (in particular, by R), and the ap-
plication‚Äôs initial state s0:=A. We in the following take state Aand
its associated rules for example, and other states and rules can be
explained similarly.
There are four rules associated with State A, i.e., having Aas
their source states: r0,r1,r2andr3(multiple actions are sequen-
tially separated by semicolon ‚Äú;‚Äù):
r0:= ("disF 20", "walkF").
r1:= ("( disF <20)and(disL 20)", "turnL; walkF;
EnvironmentUnreliable
FlawedEnvironmental
constraintsDecision
making
AdaptationSensingSelf-adaptiveappFigur
e 1: Reaction loop between a self-adaptive application
and its environment.
turnL; updateState(B)").
r2:= ("(disF <20)and((disL <20)and(disR 20))",
"turnR; walkF; turnR; updateState(D)").
r3:= ("(disF <20)and((disL <20)and(disR <20))",
"walkB; updateState(E)").
These rules reference four variables: disF,disB, disL anddisR.
They are all sensing variables, representing sensed distances be-
tween the car and its four-directional obstacles (front, back, left
and right), respectively. If a sensed distance is no less than 20 cm,
we consider this distance safe. These rules also involve some ac-
tions. For example, actions walkF and walkB mean driving the car
to walk forward and backward by a unit distance (say, 10cm), re-
spectively. Actions turnL and turnR represent turning the car left
and right by 90‚ó¶, respectively. At State A, the car keeps walking
forward if its sensed distance ahead is safe (Rule r0). If this dis-
tance is no longer safe but the car‚Äôs left distance is still safe, the car
would turn left, walk forward for a unit distance, and then turn left
again (Rule r1). After these interactive actions, the application also
conducts a state-transition internal action, updateState (B), which
transits the application to a new state ( B). At State B, new rules
associated with this state( r4,r8,r9) are enabled while the previous
rules (r 0,r1,r2,r3) are all disabled. Rule r2works similarly as Rule
r1except that it turns the car to right and transits the application
to State D. If the car‚Äôs sensed distances at three directions (front,
left and right) are all not safe, the application would drive the car to
walk backward for a unit distance and then transit to State E(Rule
r3).
Consider our earlier failure deÔ¨Ånition. The car should not bump
into any obstacle. Since the car can walk only forward or back-
ward, if its last action is walkF, the failure condition is " disF0",
or otherwise (i.e., walkB) the failure condition is " disB0" (the
failure condition would be different with uncertainty and environ-
ment constraints, which will be discussed later).
Now we verify this ISM model to check whether it contains any
problem. We note that if one does not model and consider envi-
ronmental constraints and uncertainty, the veriÔ¨Åcation results can
be inaccurate. For example, consider an execution: A r0A r1B.
Its path condition is " disF 020anddisF 1<20anddisL 120"
(since removing parentheses from a conjunction formula does not
change its value, all parentheses are omitted for simplicity). Here,
different subscripts represent sensed values at different time points.
Suppose that after executing action walkF in Rule r0, the car bumps
into an obstacle, i.e., the failure condition ‚Äú disF0‚Äù is satisÔ¨Åed.
Without considering uncertainty caused by unreliable sensing, disF
would equal to disF 1, which is the sensed value of disF after ex-
ecuting action walkF in Rule r0. We concatenate this failure con-
dition to the earlier path condition and try to solve the whole con-
straint. If we can successfully solve this constraint, we would ob-
201A
DE CB
r1
r3
r2r4
r5r6
r7r9r10
r13r14 r0 r12
r11"disF>=20","walkF"
"(disF<20)and ((disL<
20)and(disR<20)),
walkB";"updateState(E) ""(disF<20)and(disL>=20)",
"turnL;walkF;turnL;
updateState(B) "
"(disF<20)and((disL<20)
and(disR>=20))","turnR;
walkF;turnR;updateState(D) "r8Figur
e 2: A partial ISM model for our example self-adaptive
robot-car application (ellipses represent states and arcs repre-
sent rules).
tain a concrete answer on why this execution fails. This execution
with the answer is also known as a counterexample .
Regarding the above constraint " disF 020anddisF 1<20and
disL 120anddisF 10", one possible counterexample for solv-
ing this constraint is: disF 0=100 , disF 1=0 and disL 1=30.
This counterexample seems feasible but it violates an environmen-
tal constraint connecting disF 0anddisF 1, which is established by
action walkF in Rule r0. We observe that the distance between
disF 0anddisF 1is too large (0  100= 100 cm) such that it can-
not be accomplished by walkF (10cm). This makes the counterex-
ample unreal and thus useless (i.e., a false positive).
On the other hand, real counterexamples (i.e., true positives) may
also be missed if one does not consider uncertainty in verifying
this application. For instance, consider the same execution in the
above example: A r0A r1B. By using the approach of model-
ing environmental constraints, which will be introduced later, we
can have a constraint: disF 1=disF 0 unit, since disF 0‚Äôs value be-
comes disF 1‚Äôs value due to the effect of action walkF, which moves
the car forward by a unit distance. Suppose that one wants to know
whether after executing action walkF in the Ô¨Årst r0, the applica-
tion can fail, i.e., " disF 10"is satisÔ¨Åed. Concatenating the path
condition and the failure condition together and trying to solve the
whole constraint would return no result, since unit =10 and thus
disF 1, which equals to disF 0 unit, must be larger than 0 based on
the fact "disF 020". However, both environmental sensing and
application adaptation can contain uncertainty as explained earlier.
Suppose that disF‚Äôs value is subject to an error range of [-6, 6]
and the car‚Äôs walked unit distance falls in an error range of [-5, 5].
Then there still exists possibility that the car bumps into an obstacle
in this situation. Therefore, this calls for new effort to model such
uncertainty to enhance our constraints so that one can discover such
potential problems in a self-adaptive application in a more accurate
way.
4. VERIFYING SELF-ADAPTIVE
APPLICATIONS
In this section we present our approach to verifying self-adaptive
applications with environmental constraints and uncertainty. We
begin with an overview of the approach, followed by detailed ex-
planations.
4.1 Approach Overview
Given a self-adaptive application, we build its ISM model. As
discussed in the motivating example, the failure condition of ourAlgorithm
1VeriÔ¨Åcation algorithm.
Input:
ISM M:=
(S;R;V;s0), failure condition f c, and bound k.
Output:
SetCof counterexamples with probabilities.
1:C:= / 0
2:path :=<s0>;i:=0;
3:repeat
4: s:= the last state of path;
5: ifi<k&&shas unexplored rules then
6: r:= an unexplored rule of s;
7: s‚Ä≤:= the state that rleads to;
8: append rands‚Ä≤topath;i:=i+1;
9: extract the path‚Äôs path condition pc;
10: augment pcwith environmental constraints;
11: modify pcby introducing uncertainty;
12: check pc&& f cwith a constraint solver;
13: ifpc&& f cis satisÔ¨Åed then
14: estimate the probability of the counterexample;
15: add the counterexample with probability to C;
16: end if
17: else
18: remove s‚Ä≤andrfrom path;
19: i:=i 1;
20: end if
21:until path ==/ 0;
22:return C;
robot-car
application is that the car bumps into any obstacle. This
is veriÔ¨Åed by checking each path in the ISM to see whether the
concatenation of its path condition and the failure condition can be
satisÔ¨Åed. If so, a counterexample is found, which corresponds to
an application failure.
The overview of our veriÔ¨Åcation approach is shown in Algorithm
4.1. The algorithm takes an ISM M, a failure condition f c, and a
bound kas its inputs. The main data structure used in the algo-
rithm is a list path, which records the path that is currently being
explored. The algorithm traverses the ISM in a depth-Ô¨Årst manner
to Ô¨Ånd new paths (Lines 5-8, Lines 18-19). Since the number of op-
tional paths could be inÔ¨Ånitely many, for practical considerations,
our approach bounds the length of a path being explored with a con-
Ô¨Ågurable integer during the traversal (Line 5: i<k). Our approach
also supports setting a time budget to prevent endless traversal. The
traversal ends when there is no unexplored path within the bound
(Line 21).
For each selected path, the algorithm extracts its path condition
pc. The ISM explicitly speciÔ¨Åes the adaptation logics of an ap-
plication, but does not include its environmental constraints and
uncertainty. So we augment the path condition with environmental
constraints and uncertainty for realistic veriÔ¨Åcation. Then the path
condition is passed to Z3 [12], an efÔ¨Åcient SMT solver, to check
whether it can be satisÔ¨Åed. If so, a counterexample is found. It is
possible that many counterexamples can be reported. To make the
veriÔ¨Åcation results more actionable to users, our approach priori-
tizes the reported counterexamples according to their occurrence
probabilities. In the following three subsections, we present our
ideas of modeling environmental constraints, dealing with uncer-
tainty, and prioritizing counterexamples in detail.
4.2 Modeling Environmental Constraints
A self-adaptive application‚Äôs execution can be described by a re-
action loop. Each environmental sensing provides information for
202disFi
disLi
disBidisRi
disFi+1
disLi+1
disBi+1disRi+1 1unit
disFi
disLi
disBidisRi
disRi+1
disFi+1
disLi+1disBi+1disFi+1=disFi‚Äìunit
disBi+1=disBi+unit
disLi+1isirrelevantto disLi
disRi+1isirrelevantto disRi
disFi+1=disLi
disBi+1=disRi
disLi+1=disBi
disRi+1=disFiBeforeactionturnLBeforeactionwalkF
AfteractionturnLAfteractionwalkFFigur
e 3: Environmental constraints for actions.
the application to select an appropriate adaptation, which can fur-
ther change the environment and affect the application‚Äôs next envi-
ronmental sensing. Since the new environment is the result of the
adaptation‚Äôs effects on the previous environment, it cannot be arbi-
trary. This is because inherent constraints in the environment, such
as physical laws or the constraints predeÔ¨Åned by the application
domain, could be violated. For example, for the robot-car applica-
tion, suppose that the car keeps walking forward with an obstacle
detected ahead. If each sensed environment of the application is
treated independently and can be arbitrary, we can have that the ap-
plication‚Äôs sensed distance to its front obstacle in one environmen-
tal sensing is larger than that in the previous sensing. This clearly
contradicts physical laws and makes thus reported counterexample
useless.
The constraints related each environmental sensing to its previ-
ous sensing and adaptation are named environmental constraints, as
explained before. For a self-adaptive application, we build its ISM
M:= (S ;V;R;s0). Suppose a rule r2Rhas been just triggered, and
interactive actions ofr:actions has affected the environment. Then
an environmental constraint is a formula conenv(V;V‚Ä≤;E), where V
andV‚Ä≤are the set of sensing variables before and after the interac -
tive actions , respectively, and Eis a set of non-sensing variables
representing the effects of interactive actions. The semantics of
conenvcan be derived from inherent constraints of the environment.
Take the robot-car application as an example again. Suppose that
the car takes action walkF, which makes the car walk forward for a
distance unit. Then from physical laws we can derive an environ-
mental constraint conenv(fdisF ig;fdisF i+1g;funitg)meaning that
the last sensed distance disF ishould be equal to the summation of
the new sensed distance disF i+1and the distance unitthat the car
has walked since its last sensing, i.e., conenv(fdisF ig;fdisF i+1g;
funitg):disF i=disF i+1+unit. The environmental constraint conenv
(V;V‚Ä≤;E)requires that when values of the sensing variables in V‚Ä≤
are explicitly updated upon an environmental sensing, these up-
dated values should satisfy conenv(V;V‚Ä≤;E). Thus, through envi-
ronmental constraints, we provide a means to enable interactive
actions to update sensing variables implicitly. For conenv(fdisF ig,
fdisF i+1g;funitg)in the above example, it requires that the update
ofdisF i+1should satisfy disF i+1=disF i unit, as illustrated in
Figure 3 (top). However, for action walkF, there are no constraints
about disL i+1anddisL i, ordisR i+1anddisR i. This is because af-
ter the robot-car walks forward for a distance unit, the distances
between the car and its left and right obstacles can be arbitrary and
thus no environmental constraints are speciÔ¨Åed.Figure 3 also shows another example in the bottom about action
turnL, which is a turning-left action. Similar environmental con-
straints exist for actions walkB and turnR, which are the walking-
backward and turning-right action, respectively. They are omitted
due to page limit.
Now we can augment the path condition of a selected path with
such environmental constraints, as mentioned in the overview. For
each rule in the path, if there are environmental constraints for tak-
ing the rule‚Äôs interactive actions , we concatenate the environmen-
tal constraints to the path condition. After it is done, we get a new
formula of constraints for the path, which is called the ideal con-
dition. For example, for path s=A r0Ain the robot-car example,
its path condition is " disF 020". For Rule r0, it has an interac-
tive action walkF. Let disF 1be the distance between the car and
its front obstacle after taking the action walkF. Then as mentioned
before, there is an environmental constraint " disF 1=disF 0 unit".
After concatenating the environmental constraint to the path condi-
tion, we get the ideal condition of path s, which is "disF 020and
disF 1=disF 0 unit". For each of the selected paths, we process
the path condition in the same manner to get the ideal condition,
which will be further processed to include uncertainty, as described
in the next subsection.
4.3 Dealing with Uncertainty
Self-adaptive applications‚Äô environmental sensing and adapta-
tion can be affected by uncertainty, which is naturally caused by
unreliable sensing and Ô¨Çawed adaptation [26]. The uncertainty can
cause inconsistent understandings for an application between its
sensed environment and actual environment [38]. As suggested by
the motivating example, the application may fall into failure due
to its inaccurate understanding to the environment caused by un-
certainty. Thus, we should consider uncertainty in verifying self-
adaptive applications. Otherwise, the accuracy of veriÔ¨Åcation re-
sults cannot be guaranteed.
However, it is difÔ¨Åcult to specify uncertainty precisely in the
modeling and veriÔ¨Åcation process, because uncertainty comes from
various sources and appears in different forms. We studied uncer-
tainty in many real cases and observed that uncertainty caused by
unreliable sensing or Ô¨Çawed adaptation demonstrates regular pat-
terns. The sensed value of unreliable sensing or the effects of
Ô¨Çawed adaptation often fall into an error range, with a distribution
determined by the physical characteristics of sensing technologies
and actions. In this section, we explain how to model this kind of
uncertainty in the veriÔ¨Åcation of self-adaptive applications.
Uncertainty affects self-adaptive applications‚Äô sensing and adap-
tation, and thus affects values of sensing variables and non-sensing
variables that represent the effects of adaptation. Given an ISM,
to model uncertainty, we Ô¨Årst need to identify the variables in an
ideal condition that would be affected by uncertainty. Then for
each of these variables, we give an error range and a distribution for
its potential value, i.e., for a variable vaffected by uncertainty, let
[a;b] (a <b)be the error range of v. Then the lower bound and up-
per bound of variable varev+aandv+brespectively. Meanwhile,
we set a distribution pof the variable‚Äôs value between its lower and
upper bounds. The lower bound, upper bound and the distribution
of a variable can be obtained from Ô¨Åeld studies or experiments with
statistical analysis. In the robot-car application, the distance disF
between the car and its front obstacle is affected by uncertainty. We
found that the ultrasonic sensor used in the robot-car has an error in
sensing. Field studies show that its error range is [-6, 6] cm and its
error distribution is a Gaussian one. So, for variable disF, we give
an error range of [ 6;6]. Then the lower bound and upper bound
aredisF 6 and disF +6, respectively. The distribution of disF‚Äôs
203v
alue between its lower and upper bounds is a Gaussian distribu-
tion. Similarly, from Ô¨Åeld studies we learned that the distance unit
of the car‚Äôs walking action falls into an error range [-5, 5] cm, and
the distribution of its value is also a Gaussian distribution.
Now we can show how to augment an ideal condition with uncer-
tainty. In an ideal condition, for all its variables, there is no uncer-
tainty considered. Therefore, for each variable v, which is affected
by uncertainty in the ideal condition, since vdoes not include un-
certainty, we use a new variable v‚Ä≤to represent vwith uncertainty.
v‚Ä≤satisÔ¨Åes the constraint v+av‚Ä≤v+b, where [a;b]is the error
range of v. This means that the value of v‚Ä≤can range from v+ato
v+b. Clearly, for environmental sensing, the value of v‚Ä≤is a sensed
value of the application, and the value of vis the actual value about
the environment. For adaptation, the value of v‚Ä≤records its actual
effects on the environment, and the value of vrecords its ideal ef-
fects assumed by the application. We also set a distribution P(v‚Ä≤)
forv‚Ä≤that will be used to prioritize later reported counterexamples,
which is explained in the next subsection. Then we replace vwith
v‚Ä≤in the ideal condition, and join the constraint v+av‚Ä≤v+b
with the ideal condition. This forms a new condition named the
actual condition .
Take path s=A r0Ain the robot-car application mentioned ear-
lier for illustration. The ideal condition of the path is " disF 020
anddisF 1=disF 0 unit", in which variables disF 0,disF 1, and unit
are affected by uncertainty. The error ranges for disF 0anddisF 1
are [-6, 6], and the error range of unitis [-5, 5]. We use new vari-
ables disF‚Ä≤
0,disF‚Ä≤
1andunit‚Ä≤to replace the counterparts in the ideal
condition, respectively. The constraint between disF 0anddisF‚Ä≤
0is
"disF 0 6disF‚Ä≤
0disF 0+6". Other variables‚Äô constraints be-
tween themselves and the new variables are similar. Then we com-
bine all these new constraints into the ideal condition, and get the
actual condition of s, i.e., "disF‚Ä≤
020anddisF‚Ä≤
1=disF‚Ä≤
0 unit‚Ä≤
anddisF 0 6disF‚Ä≤
0disF 0+6anddisF 1 6disF‚Ä≤
1disF 1+
6andunit 5unit‚Ä≤unit+5".
4.4 Prioritizing Counterexamples
To check whether an execution can lead to application failure,
we concatenate a failure condition to the actual path condition as-
sociated with this execution to get a new constraint, which is then
passed to Z3 to check whether there is a satisfying solution. If yes,
the execution can fall into failure. In the solution, for each vari-
able, Z3 will provide it with a value. The values of all sensing vari-
ables represent an application‚Äôs understanding to the environment.
Based on these values, we can construct a partial environment in
which the application fails. The reason why the environment may
be partial is that the application may not sense all environmental
attributes. A path scontains a sequence of actions that the ap-
plication takes, which would lead to the application‚Äôs failure if a
solution exists. The path sand the solution together indicate an
application failure, and that is why we name them a counterexam-
ple. A counterexample is a tuple t= (s;L), where Lis a mapping
L:V!A, where Vis a set fv0;v1;:::; vngcontaining all variables
in the actual condition of sandAis a set of values fa0;a1;:::; ang
where L(vi) =ai(0in)in the solution. A counterexample rep-
resents a failed application execution in a certain environment. For
example, for the robot-car application, we already know the actual
condition of path s=A r0A(cf. the previous subsection). The
failure condition of this path sisdisF 10. Then by solving the
concatenation of the actual condition and the failure condition, we
can obtain a solution of ‚Äú disF 0=14;disF‚Ä≤
0=20;disF 1=0;disF‚Ä≤
1=
6;unit=10;unit‚Ä≤=14‚Äù. This solution corresponds to the following
failure scenario. The distance between the robot-car and its front
obstacle is initially 14 (disF 0), but due to uncertainty, the applica-tion‚Äôs sensed distance is 20 (disF‚Ä≤
0). Based on the sensed distance,
the application makes the car walk forward for a unit distance that is
assumed to be 10cm ( unit). However, the actually walked distance
is 14cm (unit‚Ä≤). As a result, although the new sensed distance of the
application is 6cm (disF‚Ä≤
1), in reality the car has already bumped
into the obstacle (disF 1=0). This counterexample would not have
been reported, if we do not consider uncertainty in environmental
sensing and application adaptation.
Self-adaptive applications are more likely to fall into failures in
an environment with uncertainty [14, 26, 27]. As a result, many
counterexamples could be reported by solving concatenation of ac-
tual conditions and failure conditions. However, for a certain coun-
terexample t= (s;L), an application may not fail by exactly fol-
lowing the actions speciÔ¨Åed in sunder the environment constructed
from the values of variables in L. This is because each time the
application runs, due to uncertainty, the application‚Äôs sensed envi-
ronment can be different from its actual environment, and its adap-
tation‚Äôs effects can be different from its assumption. The likelihood
for application making the same actions as the ones in sthat lead
to failure in the environment corresponding to the variables‚Äô values
inLis called the probability of counterexample t= (s;L). We pro-
pose to prioritize reported counterexamples to save the effort for
fault inspection and Ô¨Åxing. We believe that the more likely a coun-
terexample is to occur, the more attention it needs. Therefore, we
rank counterexamples according to their probabilities from high to
low.
For a counterexample t= (s;L)to occur in the environment con-
structed from L, it requires that the application should make the
same sequence of actions as speciÔ¨Åed in the rules of s. This means
that for each rule rins,r:condition should be satisÔ¨Åed so that
r:actions can be taken. Thus we Ô¨Årst estimate the probability of
satisfaction of r:condition. Suppose that r:condition involves one
variable vaffected by uncertainty, and the variable replacing vto
model uncertainty is v‚Ä≤. Let the error range of vbe[a;b], and the
value of vinLbed. So the lower bound and upper bound of the
value of v‚Ä≤ared+aandd+b, respectively. Then the probabil-
ity of r:condition being true true can be calculated according to
the following Equation 1. Function P(v‚Ä≤)is the probability density
function of v‚Ä≤. Function R(v‚Ä≤)is deÔ¨Åned as Equation 2. For a given
value of v‚Ä≤,R(v‚Ä≤)returns 1 if r:condition istrue, and returns 0 oth-
erwise. For other possible variables involved in r:condition , their
values are Ô¨Åxed using those values in L.
Prob =‚à´d+b
d+aP(v‚Ä≤)R(v‚Ä≤)dv‚Ä≤(1)
R(v‚Ä≤) ={
1;ifr:condition = true
0;ifr:condition = false(2)
For example, we assume that Rule r0is the Ô¨Årst rule in a coun-
terexample‚Äôs path. The rule‚Äôs condition disF20 involves one
variable disF affected by uncertainty, and we replace disF with
disF‚Ä≤. The error range of disF is[ 6;6], and the value of disF
solved in the counterexample is 17. It means that disF‚Ä≤‚Äôs value
can range from 11 to 23 due to uncertainty. The distribution of
disF‚Ä≤‚Äôs value complies with the Gaussian distribution N(17;22).
So the probability of disF‚Ä≤20 being true is calculated according
to Equation 1 as‚à´23
11P(disF‚Ä≤)R(disF‚Ä≤)ddisF‚Ä≤. Since R(disF‚Ä≤) =0
when disF‚Ä≤2[11;20), and R(disF‚Ä≤) =1 when disF‚Ä≤2[20;23], the
above equation is equivalent to‚à´23
20P(disF‚Ä≤)ddisF‚Ä≤, which results
in
‚à´23
201
2p
2pe (disF‚Ä≤ 17)2
8dd
iF‚Ä≤=0:0655:
204Note
that there can be more than one variable affected by uncer-
tainty in a rule‚Äôs condition, and there can be more than one rule in
a path. Variables in one rule‚Äôs condition may affect each other, and
variables across rules may have effects on each other as well. In
theory, if we want to calculate the probability of satisfaction of all
rules‚Äô conditions, we need to treat all the conditions in a path as
a whole, and perform a multiple integral on all variables that are
affected by uncertainty. As we know, the multiple integral can be
very inefÔ¨Åcient when its condition is complex and there are many
variables in the condition. Thus, we give an approximated solution
by treating each variable independently. First, for each rule in a
counterexample, we calculate the probability of satisfaction of the
rule‚Äôs condition according to Equation 1. If there are more than one
variable affected by uncertainty in the rule‚Äôs condition, a multiple
integral is used. Then, we multiply the probabilities of satisfaction
of each rule‚Äôs condition to get an estimated probability of whole
counterexample.
5. VALIDATING OUR APPROACH
In this section, we validate the effectiveness of our approach. We
implemented our approach as a prototype, and the validation was
carried out in the context of our robot-car application case study.
In this study, we are going to answer the following three research
questions:
RQ1: How does our modeling of environmental constraints
and uncertainty improve the accuracy of veriÔ¨Åcation of self-
adaptive applications?
RQ2: Can our approach give an accurate estimate of occur-
rence probabilities for its reported counterexamples?
RQ3: How does the bound of path length conÔ¨Ågured dur-
ing veriÔ¨Åcation affect the scalability and effectiveness of our
approach?
5.1 Experimental Setup and Design
Self-adaptive applications are different from conventional appli-
cations. They interact with their running environments, and adapt
their behavior based on their sensed environmental changes. To
conduct our evaluation, we need to carefully select our experimen-
tal application subjects. SpeciÔ¨Åcally, both the selected applications
and their running environments need to be manageable, as other-
wise it is hard for us to deploy the applications for experiments.
Guided by this requirement, we selected 12 different robot-car ap-
plications with various sizes as our experimental subjects. These
applications were independently developed by different researchers
and students in our university during the past four years. They
adopt different strategies to control a robot-car to explore unknown
areas based on collected sensory data. These applications have up
to 40 different states and rules.
To answer our research question RQ1, we conducted two experi-
ments. The Ô¨Årst experiment studies whether modeling environmen-
tal constraints can improve the veriÔ¨Åcation results by eliminating
false counterexamples. For comparison purposes, we implemented
another approach naive, which ignores environmental constraints
and uncertainty. We applied both our approach and the naive ap-
proach to the 12 robot-car applications, and checked the reported
counterexamples to see how many of them are false counterexam-
ples. Our second experiment studies whether modeling uncertainty
can help improve the veriÔ¨Åcation results. Similar to the Ô¨Årst experi-
ment, we implemented an approach ideal , which considers environ-
mental constraints but ignores uncertainty, for comparison. We ap-
plied both our approach and the ideal approach to the 12 robot-carTable 1: Experimental results without considering environ-
mental constraints
A
pplicationCounter
-
exampleF
alse
positiveF
alse positive
rate
A
pp1 120 94 78.33%
A
pp2 60 42 70.00%
A
pp3 108 90 83.33%
A
pp4 74 53 71.62%
A
pp5 120 102 85.00%
A
pp6 90 75 83.33%
A
pp7 120 90 75.00%
A
pp8 51 39 76.47%
A
pp9 62 48 77.42%
A
pp10 45 32 71.11%
A
pp11 51 43 84.31%
A
pp12 62 48 77.42%
applications,
and recorded their reported counterexamples. Then
we ran the 12 robot-car applications in both real deployments (i.e.,
real Ô¨Åeld study) and simulation to validate these counterexamples.
We then compared true counterexamples reported by the two ap-
proaches to see whether there is any counterexample reported by
actual (i.e., our approach) but not by ideal, and vice versa. In other
words, our goal is to study whether actual can report more coun-
terexamples than ideal.
To answer research question RQ2, for each counterexample, we
need to know its likelihood of occurrence in its corresponding en-
vironment in real cases. The likelihood here serves as a ground
truth to assess our predicted occurrence probability for each coun-
terexample. So we let an application run in the corresponding en-
vironment of every counterexample, and counted the number of
failures encountered by this application with respect to its corre-
sponding counterexample. The experiment was conducted for top
10 reported counterexamples of each application in both Ô¨Åeld study
and simulation. As we know, to get an accurate probability one has
to collect a fairly large number of samples. The simulation is thus
used to reÔ¨Åne our experimental results by providing more sampling
data. Then we checked calculated probabilities of counterexamples
against their observed likelihoods of occurrences, to see how close
they are.
The bound of path length is a parameter in our approach which
can affect the approach‚Äôs performance and the number of reported
counterexamples. Besides, since our approach takes one path for
veriÔ¨Åcation at a time, the maximum bound of the path, instead of
the whole size of the ISM, affects the scalability of the approach
virtually. Therefore, we need to investigate how the bound impacts
our approach. In particular, we want to know how our approach
scales and how the number of counterexamples grows as the bound
increases. So, to answer our research question RQ3, we applied
our approach to the 12 robot-car applications to measure the per-
formance of our approach, and recorded the number of reported
counterexamples as the bound increased. The results of all these
experiments are discussed in the next section.
5.2 Experimental Results
In this section, we present experimental results to answer our
earlier raised three research questions.
RQ1. First we examined the effects of modeling environmental
constraints. We implemented the naive approach, and veriÔ¨Åed the
12 robot-car applications with naive. The bound of path length was
set to 30. Table 1 gives the veriÔ¨Åcation results. Column 2 shows
205Table 2: Comparison of experimental results (reported coun-
terexamples) between actual and ideal
A
ppli-
cationActual Ideal Mor
eImpr
ovement
A
pp1 86 22 64 290.9%
A
pp2 89 29 60 206.9%
A
pp3 81 20 61 305.0%
A
pp4 112 29 83 286.2%
A
pp5 76 10 66 660.0%
A
pp6 65 16 49 306.3%
A
pp7 103 31 72 232.3%
A
pp8 114 27 87 322.2%
A
pp9 102 34 68 200.0%
A
pp10 124 25 99 396.0%
A
pp11 114 27 87 322.2%
A
pp12 102 34 68 200.0%
the
number of reported counterexamples. We examined these coun-
terexamples one by one manually, and found that many of them
violated environmental constraints. For example, in one counterex-
ample, after the car moved forward for a unit distance, the distance
between the car and its front obstacle was bigger than that before
the car took the move. These counterexamples will not happen in
real deployments, and thus are false counterexamples (false posi-
tives). Column 3 shows the number of reported counterexamples
which are false. As we can see from Column 4, the false positive
rate can vary from 70% to over 84%, which indicates the necessity
and importance of considering environmental constraints in the ver-
iÔ¨Åcation process.
Then we examined the effects of modeling uncertainty by com-
paring the veriÔ¨Åcation results of approach ideal and our approach
actual . We veriÔ¨Åed the 12 robot-car applications with both ap-
proaches, with the bound of path length set to 30. The results are
shown in Table 2. Column 2 is the number of reported counterex-
amples of actual , and Column 3 is the number of reported coun-
terexamples of ideal. Our approach actual reported clearly more
counterexamples than ideal. Furthermore, by careful examination
we found that all the counterexamples reported by ideal are also re-
ported by actual . In the meantime, we conÔ¨Årmed that all counterex-
amples reported by actual can happen in real environments. Most
conÔ¨Årmations were acquired by Ô¨Åeld study (more than 85%). The
others (less than 15%) were acquired by simulation, because these
counterexamples have a fairly low probability to occur, and there-
fore were difÔ¨Åcult to be witnessed in the Ô¨Åeld study. The robot-
car and the simulation used to do the Ô¨Åeld study and simulation
are shown in Figure 4. The experimental results show that our ap-
proach actual reported 200-660% more counterexamples than ap-
proach ideal , which demonstrates that our approach has a better
accuracy (more complete).
Based on the above discussed experimental results, we derive our
answer to research question RQ1: Modeling environmental con-
straints and uncertainty can greatly improve the accuracy of veriÔ¨Å-
cation of self-adaptive applications .
RQ2. We also ranked the counterexamples reported by our ap-
proach from the above experiment with their predicted probabili-
ties. To assess the accuracy of these probabilities, we selected the
top 10 counterexamples from each of the 12 robot-car application
(i.e., 120 counterexamples in total) for further study. SpeciÔ¨Åcally,
for each selected counterexample, we let the concerned application
to run in the environment constructed from this counterexample in
Ô¨Åeld study for 100 times, and also in simulation for 1,000 times
Figur
e 4: The robot-car and the simulation used in experi-
ments.
0.00 0.10 0.20 0.30 0.40 0.50 0.60 
0.00 0.10 0.20 0.30 0.40 0.50 0.60 Experimental likelihood 
Predicted probablity Field study Simulation 
 Field study
 Simulati on 
Figur
e 5: Comparison of predicted probabilities and experi-
mental probabilities.
(i.e., totally 1,100 runs for each counterexample). For each coun-
terexample, we counted the number of its corresponding failures,
and obtained a likelihood value for the failure‚Äôs occurrence. Figure
5 illustrates the results. The horizontal axis represents predicted
probability from our approach, and the vertical axis represents the
likelihood of occurrence observed from experiments. Each coun-
terexample corresponds to two points in the Ô¨Ågure: blue points
(solid circle) are for the Ô¨Åeld study, and red points (solid triangle)
are for the simulation. It is clear that for a counterexample, if its
predicted probability from our approach is close to its likelihood of
occurrence from experiments, its corresponding point in the Ô¨Ågure
would be close to the diagonal line. As we can observe from Fig-
ure 5, most of the points are scattered near the diagonal line. From
these results, we derive our answer to research question RQ2: Our
approach can accurately estimate occurrence probabilities for its
reported counterexamples.
RQ3. We ran our approach to verify the robot-car applications
with different bounds of path length. The experimental platform
is a Dell Desktop PC with an Intel Core 2 CPU @2.53GHz and
2GB RAM, running Windows 7. We recorded the spent time and
memory in each run, which are shown in Table 3. As we can ob-
serve from the table, when the bound is set to 50, our approach
spent about 15 minutes and consumed around 250 MB memory
on average. Clearly, given enough time, our approach is able to
handle larger bounds. However, if we focus on counterexamples
with relatively high probabilities, a bound of 50 is already sufÔ¨Å-
cient for the robot-car applications. This is because we observed
that when the bound was set to 50, the increase of the number of
counterexamples with a probability higher than 10 5, which is a
very small likelihood for a counterexample to occur, tends towards
206T able 3: Time and memory costs for veriÔ¨Åcation with different bounds
A
ppli-
cation5 10 15 20 25 30 35 40 45 50
A
pp14.0s
40MB6.8s
48MB10.5s
61MB19.8s
72MB38.5s
84MB72.3s
97MB109.3s
118MB196.3s
129MB310.8s
153MB500.3s
178MB
A
pp22.3s
55MB3.8s
70MB7.0s
87MB12.8s
102MB22.5s
116MB41.3s
134MB75.5s
153MB147s
168MB243.8s
186MB491.3s
204MB
A
pp35.0s
31MB9.3s
44MB16.5s
52MB28.8s
63MB54.0s
75MB92.8s
81MB164.3s
89MB323.5s
97MB480.8s
108MB886.3s
124MB
A
pp45.8s
44MB9.5s
51MB17.5s
59MB33.0s
68MB60.8s
81MB105.3s
103MB198.8s
117MB344.8s
132MB508.0s
155MB976.3s
196MB
A
pp53.8s
58MB7.0s
65MB11.5s
83MB19.5s
94MB38.5s
112MB71.5s
137MB137.0s
162MB243.8s
194MB419.5s
205MB529.3s
217MB
A
pp62.3s
38MB3.5s
45MB6.8s
53MB13.0s
59MB24.3s
64MB47.0s
71MB90.8s
78MB177.0s
84MB331.0s
96MB633.3s
113MB
A
pp75.3s
41MB9.0s
53MB15.3s
64MB27.5s
71MB51.5s
79MB96.3s
94MB177.5s
124MB314.3s
167MB528.8s
195MB971.8s
211MB
A
pp84.3s
47MB7.5s
55MB14.5s
67MB26.0s
84MB47.3s
95MB94.8s
110MB178.0s
135MB302.3s
157MB528.3s
198MB1000.5s
245MB
A
pp95.5s
39MB9.5s
47MB18.8s
55MB37.3s
63MB60.3s
79MB116.0s
90MB203.0s
99MB385.0s
110MB742.0s
157MB1012.5s
203MB
A
pp103.3s
29MB6.3s
43MB11.3s
60MB22.0s
75MB39.0s
91MB69.5s
105MB130.5s
119MB248.5s
168MB472.5s
211MB810.0s
264MB
A
pp114.5s
45MB8.0s
53MB15.0s
60MB24.8s
76MB45.0s
91MB92.5s
105MB180.3s
129MB313.8s
154MB525.3s
212MB998.5s
231MB
A
pp125.0s
35MB8.8s
46MB17.3s
60MB33.5s
68MB59.0s
81MB113.8s
90MB208.5s
107MB382.3s
113MB692.0s
163MB1026.5s
217MB
020 40 60 80 100 120 140 160 180 
5 10 15 20 25 30 35 40 45 50 Number of counterexamples 
Bound of  path length App 1 
App2 
App3 
App4 
App5 
App6 
Figur
e 6: The growing trend of the number of counterexamples
with the increase of the bound of path length.
stability. Figure 6 illustrates the number of counterexamples with a
probability higher than 10 5when the bound was set from 5 to 50
(for App 1 to App 6). From these discussions, we can derive our
answer to research question RQ3: Our approach can scale well to
large bounds of path length and it can detect more counterexamples
when the bound increases.
5.3 Threats to Validity
We analyze threats to the validity of our conclusions below.
Threats to construct validity. The main threat to construct va-
lidity for our study is that we may not have run our robot-car appli-
cations adequate times in the Ô¨Åeld study in order to obtain accurate
likelihoods of occurrence of the counterexamples. To reduce this
threat, we ran each application in simulation for 1,000 times to get
the simulated probabilities as complementary evidence. Another
potential threat to construct validity is the threshold of probability
used to Ô¨Ålter out low-probability counterexamples when evaluating
the impact of the bound of path length. Nevertheless, we set the
probability threshold to 10 5, which is an extremely small like-
lihood for a counterexample to occur in real cases. In fact, we
believe developers should focus mainly on counterexamples with
much higher probabilities in a realistic setting.Threats to internal validity. There are mainly two threats to
internal validity. One is that the given ranges and distributions of
variables for modeling uncertainty may not be realistic. To reduce
this threat, we have conducted a lot of Ô¨Åeld studies and performed
careful statistical analysis. The other threat to internal validity is
the selected bound of path length. It is possible to Ô¨Ånd more coun-
terexamples with a larger bound. Nevertheless, according to our
experiments, the number of counterexamples will tend to be stable
as the bound increases. So we selected a proper value for the bound
in experiments.
Threats to external validity. The main threat to external valid-
ity is that our conclusions may not generalize to other self-adaptive
applications. To reduce this threat, we selected 12 different robot-
car self-adaptive applications with various sizes as our experimen-
tal subjects. Meanwhile, we made our best efforts in selecting them
being independently developed by different developers, and vali-
dated by both simulation and Ô¨Åeld study. The results consistently
support our conclusions. Although we try to make our approach
applicable to other self-adaptive applications, there still is a strong
need for validating our approach with more realistic applications.
6. RELATED WORK
In this section, we discuss selected related work on self-adaptive
applications. Cheng et al. [10] and Lemos et al. [11] presented
a comprehensive and in-depth analysis of the current research sta-
tus including methods and challenges, to which interested readers
can refer. Here we focus on modeling, testing and verifying self-
adaptive applications.
Modeling self-adaptive applications. To model a self-adaptive
application, there are several optional methodologies, such as goal-
oriented or rule-based methodologies. Goal-based modeling offers
a means to identify and visualize different alternatives for satisfy-
ing the overall objectives of a system [9]. The goals here capture
the intentions of a stakeholder on a self-adaptive application and its
execution environment [25], and can be used to model the require-
ments of self-adaptive applications [19]. Rule-based modeling uses
rules explicitly or implicitly to model an application‚Äôs expected
reactions to monitored events [31], such as the A-FSM approach
207[29,
34]. There are also other pieces of work related to modeling
self-adaptive applications. Andersson et al. [3] deÔ¨Åned four cate-
gories of dimensions for modeling self-adaptive applications: self-
adaptive goals, causes or triggers of self-adaptation, mechanisms
used to adapt, and effects of those mechanisms on applications.
Dobson et al. [13] identiÔ¨Åed four aspects of self-adaptive applica-
tions around which decisions can be organized: collection, analy-
sis, decision and action. Brun et al. [6] discussed the importance
of making the adaptation control loops explicit during an applica-
tion‚Äôs development process, and outlined several types of control
loops that can lead to adaptation.
Verifying and testing self-adaptive applications. Designing
and deploying certiÔ¨Åable veriÔ¨Åcation and validation methods for
self-adaptive applications is one of the major research challenges
for the software engineering community in general and the self-
adaptive applications community in particular [11]. Concerned
properties for self-adaptive applications include safety [13, 40],
liveness and reachability [21, 28, 29, 40], reliability[7, 17], and
stability [4, 28, 29]. In recent years, different methods have been
used in [40, 4, 5] to verify self-adaptive applications. Zhang and
Cheng [40] introduced an approach to creating formal models for
the behavior of self-adaptive applications. They also presented a
process to construct, verify, and validate these models. Bartels et
al. used the process algebra CSP for the speciÔ¨Åcation, veriÔ¨Åcation
and implementation of self-adaptive applications [4]. Camara et al.
[7] proposed an approach for the veriÔ¨Åcation of self-adaptive ap-
plications based on stimulation and probabilistic model-checking.
It stimulates the environment and collects data about how an ap-
plication reacts environmental changes to evaluate whether impor-
tant properties are satisÔ¨Åed within certain conÔ¨Ådence levels. Model
checking is also widely used to detect some well-known fault pat-
terns in self-adaptive applications [21, 28, 29].
There are also many studies focusing on testing self-adaptive ap-
plications. Xu et al. [34] used error patterns to dynamically de-
tect and analyze responsible faults. Tse et al. [30] used isotropic
properties of contexts as metamorphic relations for testing context-
sensitive software and presented techniques for generating effec-
tive test cases. Wang et al. [32] proposed to augment existing
test cases to expose faults, by focusing on context switching points
that can affect application adaptations. Besides test case genera-
tion and augmentation, there are also efforts spent on test adequacy
criteria research. For instance, Lu et al. [22] proposed a new set
of coverage criteria to test data Ô¨Çows caused by context uses in
self-adaptive applications. These pieces of work detect faults in
self-adaptive applications, and contribute to their dependability at
design and development phases. Some other studies also tried to
improve self-adaptive applications‚Äô dependability, but from differ-
ent perspectives. For example, related studies [35, 36, 37] detected
and resolved inconsistency in contexts fed to a self-adaptive ap-
plication. Consistent context data is an important prerequisite for
dependable adaptations. Our previous work [38, 39] focused on
improving the dependability of self-adaptive applications by moni-
toring application executions and checking consistency constraints
at runtime. Kulkarni et al. [20] also introduced a runtime error-
handling framework for programming robust self-adaptive applica-
tions by adopting forward recovery strategies. Last but not least,
Ramirez et al. [27] introduced a technique for automatically dis-
covering combinations of environmental conditions that produce
requirement violations and latent behaviors in a self-adaptive ap-
plication.
Managing uncertainty. Uncertainty poses a big threat to cor-
rect and reliable self-adaptations. This problem is gaining increas-
ing attention in recent years. Ramirez et al. [26] reported a tax-onomy of uncertain factors that can affect self-adaptive applica-
tions. Their work called for a spectrum of research efforts from
requirement speciÔ¨Åcation, application design to runtime support.
There are some studies focusing on how to handle uncertainty at
design time for self-adaptive applications [15, 14, 9]. Esfahani et al.
[15] described an approach to tackling the challenge of uncertainty
by assessing both positive and negative consequences of uncer-
tainty, and proposed a framework for managing uncertainty in self-
adaptive applications [14]. Cheng et al. [9] proposed a requirement
language RELAX to explicitly address uncertainty by enabling en-
gineers to specify uncertainty in application requirements. In their
work, adaptation is achieved by relaxing non-critical requirements.
Ghezzi et al. [18] proposed a framework that supports adaptation
to non-functional manifestations of uncertainty relying on alterna-
tive or optional functionalities. The framework allows engineers to
derive a Ô¨Ånite state automaton augmented with probabilities from
an initial model of a self-adaptive application. Famelis et al. [16]
speciÔ¨Åed uncertainty using annotations with well-deÔ¨Åned seman-
tics that transforms an application model into a partial model and
presented an approach to reasoning with such models.
Our work differs from the existing work in three aspects. First,
we explicitly model environmental constraints and uncertainty cau-
sed by unreliable sensing and adaptation for self-adaptive applica-
tions. Second, we present a novel approach, which exploits the
power of SMT solvers, to verifying the correctness of self-adaptive
applications affected by uncertainty. Third, we propose to rank
counterexamples according to their probabilities, which has not yet
been considered in the above literature.
7. CONCLUSION
In this paper, we propose a novel approach to verifying self-
adaptive applications. By explicitly considering the environmental
constraints, the approach avoids reporting false counterexamples
that will not happen in real environments. At the same time, by tak-
ing error ranges of the environment-related variables into account,
the approach can Ô¨Ånd lots of potential counterexamples (faults) that
would otherwise be overlooked by methods not considering uncer-
tainty in environmental interactions. Our approach also prioritizes
its reported counterexamples according to their occurrence proba-
bilities, whose accuracy has been well validated by both simulated
experiments and Ô¨Åeld study.
This work can still be improved. For example, we need to vali-
date this work with more real-world applications. In addition to en-
vironmental interactions, we also plan to extend our consideration
of uncertainty to those from other sources, such as requirements
and adaptation decisions.
8. ACKNOWLEDGMENTS
This research was partially funded by National High-Tech Re-
search & Development Program (863 program 2013AA01A213)
and National Natural Science Foundation (61100038, 91318301,
61321491, 61361120097) of China.
9. REFERENCES
[1] Locale. http://www.twofortyfouram.com/.
[2] Navia. http://induct-technology.com/.
[3] J. Andersson, R. Lemos, S. Malek, and D. Weyns. Modeling
dimensions of self-adaptive software systems. volume 5525
ofLNCS, pages 27‚Äì47. Springer Berlin Heidelberg, 2009.
[4] B. Bartels and M. Kleine. A csp-based framework for the
speciÔ¨Åcation, veriÔ¨Åcation, and implementation of adaptive
208systems.
InProc. SEAMS‚Äô 11 , pages 158‚Äì167, Honolulu,
USA, May 2011.
[5] R. V. Borges, A. d‚ÄôAvila Garcez, and L. C. Lamb. Integrat-
ing model veriÔ¨Åcation and self-adaptation. In Proc. ASE‚Äô 10,
pages 317‚Äì320, Antwerp, Belgium, Nov 2010.
[6] Y. Brun, G. Marzo Serugendo, C. Gacek, H. Giese, H. Kienle,
and et al. Engineering self-adaptive systems through feed-
back loops. In Software Engineering for Self-Adaptive Sys-
tems, pages 48‚Äì70. Springer Berlin Heidelberg, 2009.
[7] J. Camara and R. De Lemos. Evaluation of resilience in self-
adaptive systems using probabilistic model-checking. In Proc.
SEAMS‚Äô 12 , pages 53‚Äì62, Zurich, Switzerland, June 2012.
[8] L. Capra, W. Emmerich, and C. Mascolo. Carisma: context-
aware reÔ¨Çective middleware system for mobile applications.
IEEE Trans. Softw. Eng., 29(10):929‚Äì945, 2003.
[9] B. Cheng, P. Sawyer, N. Bencomo, and J. Whittle. A goal-
based modeling approach to develop requirements of an adap-
tive system with environmental uncertainty. volume 5795 of
LNCS, pages 468‚Äì483. Springer Berlin Heidelberg.
[10] B. H. Cheng, R. Lemos, H. Giese, P. Inverardi, J. Magee, and
et al. Software engineering for self-adaptive systems: A re-
search roadmap. In Software Engineering for Self-Adaptive
Systems, pages 1‚Äì26, 2009.
[11] R. de Lemos, H. Giese, H. M ¬¥l¬¥ zller, M. Shaw, J. Anders-
son, and et al. Software engineering for self-adaptive sys-
tems: A second research roadmap. In Software Engineering
for Self-Adaptive Systems II , volume 7475 of LNCS, pages 1‚Äì
32. Springer Berlin Heidelberg, 2013.
[12] L. De Moura and N. Bj√∏rner. Z3: An efÔ¨Åcient smt solver.
InTACAS‚Äô08/ETAPS‚Äô08, pages 337‚Äì340, Budapest, Hungary,
March-Apr 2008.
[13] S. Dobson and et al. A survey of autonomic communications.
ACM Trans. Auton. Adapt. Syst. , 1(2):223‚Äì259, Dec 2006.
[14] N. Esfahani. A framework for managing uncertainty in self-
adaptive software systems. In Proc. ASE‚Äô 11 , pages 646‚Äì650,
Kansas, USA, Nov 2011.
[15] N. Esfahani, E. Kouroshfar, and S. Malek. Taming uncertainty
in self-adaptive software. In Proc. Joint ESEC/FSE‚Äô 11 , pages
234‚Äì244, Szeged, Hungary, Sept 2011.
[16] M. Famelis, R. Salay, and M. Chechik. Partial models: To-
wards modeling and reasoning with uncertainty. In Proc.
ICSE‚Äô 12 , pages 573‚Äì583, Zurich, Switzerland, June 2012.
[17] A. Filieri and G. Tamburrelli. Probabilistic veriÔ¨Åcation at run-
time for self-adaptive systems. volume 7740 of LNCS, pages
30‚Äì59. Springer Berlin Heidelberg, 2013.
[18] C. Ghezzi, L. S. Pinto, P. Spoletini, and G. Tamburrelli. Man-
aging non-functional uncertainty via model-driven adaptivity.
InProc. ICSE‚Äô 13 , pages 33‚Äì42, San Francisco, USA, May
2013.
[19] H. Goldsby, P. Sawyer, N. Bencomo, B. H. C. Cheng,
and D. Hughes. Goal-based modeling of dynamically adap-
tive system requirements. In Proc. ECBS‚Äô 08 , pages 36‚Äì45,
Belfast, Northern Ireland, March 2008.
[20] D. Kulkarni and A. Tripathi. A framework for programming
robust context-aware applications. IEEE Trans. Softw. Eng. ,
36(2):184‚Äì197, 2010.
[21] Y. Liu, C. Xu, and S. Cheung. Afchecker: Effective model
checking for context-aware adaptive applications. J. Syst.
Softw., 86(3):854 ‚Äì 867, 2013.
[22] H. Lu, W. K. Chan, and T. H. Tse. Testing context-aware
middleware-centric programs: a data Ô¨Çow approach and anrÔ¨Åd-based experimentation. In Proc. FSE‚Äô 06 , pages 242‚Äì252,
Portland, USA, 2006.
[23] P. McKinley, S. Sadjadi, E. Kasten, and B. H. C. Cheng. Com-
posing adaptive software. page 56 ¬¥lC64 Vol. 37. IEEE Com-
puter, 2004.
[24] I. Park, D. Lee, and S. Hyun. A dynamic context-conÔ¨Çict
management scheme for group-aware ubiquitous computing
environments. In Proc. COMPSAC‚Äô 05, pages 359‚Äì364 Vol.
2, Kyoto, Japan, July 2005.
[25] K. Pohl. Requirements engineering: Fundamentals, princi-
ples, and techniques. Springer Publishing Company, Incorpo-
rated, 2010.
[26] A. Ramirez, A. Jensen, and B. H. C. Cheng. A taxonomy
of uncertainty for dynamically adaptive systems. In Proc.
SEAMS‚Äô 12 , pages 99‚Äì108, Zurich, Switzerland, June 2012.
[27] A. J. Ramirez, A. C. Jensen, B. H. C. Cheng, and D. B.
Knoester. Automatically exploring how uncertainty impacts
behavior of dynamically adaptive systems. In Proc. ASE‚Äô 11 ,
pages 568‚Äì571, Kansas, USA, Nov 2011.
[28] M. Sama, S. Elbaum, F. Raimondi, D. Rosenblum, and
Z. Wang. Context-aware adaptive applications: Fault patterns
and their automated identiÔ¨Åcation. IEEE Trans. Softw. Eng. ,
36(5):644‚Äì661, 2010.
[29] M. Sama, D. S. Rosenblum, Z. Wang, and S. Elbaum. Model-
based fault detection in context-aware adaptive applications.
InProc. FSE‚Äô 08 , pages 261‚Äì271, Atlanta, USA, Feb 2008.
[30] T. H. Tse and S. Yau. Testing context-sensitive middleware-
based software applications. In Proc. COMPSAC‚Äô 04 , pages
458‚Äì466 vol.1, Hong Kong, China, Sept 2004.
[31] Q. Wang. Towards a rule model for self-adaptive software.
SIGSOFT Softw. Eng. Notes , 30(1):8‚Äì, Jan 2005.
[32] Z. Wang, S. Elbaum, and D. Rosenblum. Automated genera-
tion of context-aware tests. In Proc. ICSE‚Äô 07, pages 406‚Äì415,
Minneapolis, USA, May 2007.
[33] D. Weyns, M. U. Iftikhar, D. G. de la Iglesia, and T. Ahmad.
A survey of formal methods in self-adaptive systems. In Proc.
C3S2E‚Äô 12 , pages 67‚Äì79, Montreal, Canada, June 2012.
[34] C. Xu, S. Cheung, X. Ma, C. Cao, and J. Lu. Adam: Iden-
tifying defects in context-aware adaptation. J. Syst. Softw. ,
85(12):2812 ‚Äì 2828, 2012.
[35] C. Xu and S. C. Cheung. Inconsistency detection and reso-
lution for context-aware middleware support. In Proc. Joint
ESEC/FSE‚Äô 05 , pages 336‚Äì345, Lisbon, Portugal, Feb 2005.
[36] C. Xu, S. C. Cheung, W. K. Chan, and C. Ye. On impact-
oriented automatic resolution of pervasive context incon-
sistency. In Proc. Joint ESEC/FSE‚Äô 07, pages 569‚Äì572,
Dubrovnik, Croatia, Mar. 2007.
[37] C. Xu, S. C. Cheung, W. K. Chan, and C. Ye. Partial con-
straint checking for context consistency in pervasive comput-
ing.ACM Trans. Softw. Eng. Methodol. , 19(3):9:1‚Äì9:61, Feb
2010.
[38] C. Xu, W. Yang, X. Ma, C. Cao, and J. Lu. Environment
rematching: Toward dependability improvement for self-
adaptive applications. In Proc. ASE‚Äô 13 , pages 592‚Äì597, Palo
Alto, USA, Nov 2013.
[39] W. Yang, C. Xu, and L. Zhang. Idea: Improving dependabil-
ity for self-adaptive applications. In Proc. of the 2013 Mid-
dleware Doctoral Symposium , pages 1:1‚Äì1:6, Beijing, China,
Dec 2013.
[40] J. Zhang and B. H. C. Cheng. Model-based development of
dynamically adaptive software. In Proc. ICSE‚Äô 06 , pages 371‚Äì
380, Shanghai, China, May 2006.
209