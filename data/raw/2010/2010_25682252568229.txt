Characterizing  and Detecting Performance Bugs for 
Smartphone Applications
Yepang Liu  
Dept. of Comp. Sci. and Engr.  
The Hong Kong Univ. of Sci. and Tech.  
Hong Kong,  China  
andrewust@cse.ust.hk  Chang Xu * 
State Key Lab for Novel Soft.  Tech.  
Dept. of Comp. Sci. and Tech.  
Nanjing University, Nanjing, China  
changxu@nju.edu.cn  Shing-Chi Cheung  
Dept. of Comp. Sci. and Engr.  
The Hong Kong Univ. of S ci. and Tech.  
Hong Kong, China  
scc@cse.ust.hk  
 
 
ABSTRACT  
Smartphone applications ’ performance  has a vital impact on user 
experience . However,  many smartphone  applications suffer from 
bugs that cause significant performance degradation , thereby lo s-
ing their competitive edge . Unfortunately , people  have little u n-
derstanding of these  performance bugs . They also lack  effective 
techniques to fight with such bugs . To bridge this  gap, we co n-
ducted  a study  of 70 real-world  performance bugs collected from 
eight  large -scale and popular  Android applications. We studied  
the characteristics  (e.g., bug types and how  they manifest ed) of 
these bugs  and identified  their common  patterns. These findings  
can support follow -up research on performance bug avoidance, 
testing,  debugging  and analysis  for smartphone applications . To 
demonstrate  the usefulness of  our findings , we implement ed a 
static code analyzer , PerfChecker , to detect our identified  perfor-
mance bug patterns . We experimentally evaluated  PerfChecker  by 
applying i t to 29 popular Android applications , which comprise  
1.1 million lines of Java code . PerfChecker  successfully  detected 
126 matching instances of our performance bug patterns. Among 
them, 68 were quickly confirmed by developers  as previously -
unknown  issues  that affect  application  performance,  and 20 were 
fixed soon afterwards  by following our optimization suggestions . 
Categories and Subject Descriptors  
D.2.5 [Software Engineering ]: Testing and Debugging  
General Terms  
Experimentation, Measurement, Performanc e. 
Keywords  
Empirical study, performance bug, testing, static analysis . 
1. INTRODUCTION  
The s martphone application market is expanding  rapidly. Until 
July 2013, the one million Andro id applications i n the Google 
Play store  received 50 billion downloads  [20]. With more emer g-
ing applications of  similar functionalities  (e.g., various  web 
browsers) , perfo rmance  and user experience  has gradually become  
a dominant  facto r that affects  user loyalty  in application selection . 
However, our inspection  of 60,000 Android applications  rando m-
ly sampled  from Google Play  store using a web crawler  [11] re-vealed an alarming  fact: 11,108 of them  have suffered  or are  suf-
fering from  performance bugs of varying severity , as judged from 
their release logs or user reviews . These bugs  can significa ntly 
slow  down  application s or cause them to  consume excessive re-
sources (e.g ., memory or  battery power ). The pervasiveness of 
such performance  bugs is attributable  to two major reasons. First, 
smartphones  are resourc e-constrained  as compared to PCs, but 
their applications  often have to conduct non -trivial  tasks like  web 
browsing  and graphics rendering . Thus,  poorly  implemented a p-
plications can  easily exhibit  unsatisf actory performance. Second, 
many smartphone applications are  developed by small teams 
without dedicated quality assurance. These  developers lack viable 
techniques to  help analyze  the performance  of their applications  
[23]. As such, it is hard for them to  exercis e due diligence in a s-
suring application performance , especially when they  have  to 
push application s to market in  a short time . 
Existing studies  have fo cused on  performance bugs in  PC or ser v-
er-side applications , and proposed several interesting testing and 
analysis techniques  [27][28][35][52]. Yet, smartphone platforms 
are relatively new , and we have  limited understanding of their 
application s’ performance bugs . Whether  existing  techniques are  
applicable to smartphone applications is an open question.  Ther e-
fore, in this paper,  we aim to bridge this  gap by conducting an  
empirical study . This study focuses on performance bugs from 
real-world smartphone applications . We restrict our scope to  An-
droid applications due to their popularity and code availability .1 
Our study covered  70 real-world  performance bugs  collected  from 
eight large -scale  and popular  Android applications (e.g., Firefox)  
across  five different  categories.  The study  aims to answer  the 
following  four research questions:  
 RQ1 (Bug  types and impact s): What are  common types of  
performance bugs in Android applications? What  impacts  do 
they have on user experience?  
 RQ2 (Bug manifestation):  How  do performance  bugs man i-
fest themselves ? Does  their manifestation need special inputs?  
 RQ3 (Debugging and bug-fixing effort ): Are performance 
bugs  more difficult to debug and  fix than non -performance 
bugs? What information or tools can help with this? 
                                                                 
* Corresponding author.  
Research questions (RQ1 -4)
1.Bug types and impacts
2.Bug manifestation 
3.Debugging and fixing effort
4.Common bug patternsCombating performance bugs
•Bug avoidance techniques (RQ2, RQ4)
•Effective testing methodologies (RQ1, RQ2)
•Debugging assistance tools (RQ3)
•Bug detectors (RQ1, RQ4) 
Figure 1. Potential benefits of our empirical findings  
Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that 
copies  bear this notice and the full citation on the first page. To copy 
otherwise, to republish, to post on servers or to redistribute to lists, r e-
quires prior specific permission and/or a fee.  
ICSE’14, May 31 – June 7, 2014, Hyderabad, India  
Copyright 2014 ACM 978-1-4503 -2756 -5/14/05... $15.00  Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a
fee. Request permissions from Permissions@acm.org.
ICSE’14 , May 31 – June 7, 2014, Hyderabad, India
Copyright 2014 ACM 978-1-4503-2756-5/14/05...$15.00
http://dx.doi.org/10.1145/2568225.2568229
1013
 RQ4 (Common bug patterns):  Are there common causes of 
performance bugs? Can we d istill common bug patterns to fa-
cilitate  performance analysis  and bug detection?  
We studied  relate d work  [27][54] and formulated the above  re-
search questions . Through  answering  them, we aim to  better u n-
derstand  characteristics of  performance bugs in  smartphone appl i-
cation s. For example, we found that sma rtphone applications are 
more susceptible to performance bugs  than PC or server -side a p-
plications . Besides, m any smartphone  performance bugs  do not  
need  sophisticated data  inputs  (e.g., a database  with hundreds of 
entries ) to manifest , but instead the ir manifestation  need s special 
user interactions  (e.g., certain user interaction sequences) . Some 
of our  findings differ largely from those for perfor mance bugs in 
PC or  server -side applications [27][54], as we  will explain later.  
Besides facilitating  bug understanding , our findings  can also  sup-
port follow -up research on performance bug avoidance, testing, 
debug ging and detection  for smartphone applications , as illustra t-
ed in Figure  1. For instance , with our identifie d common bug pa t-
terns,  one can propose  guidelines for avoiding certain perfo r-
mance bugs in application  development. One can also design  and 
implement  bug detection t ools for identifying performance opt i-
mization opportunities in application  testing and maintenance.  
To evaluate the usefulness of  our empirical findings, we  further  
conducted  a case study  with 29 large -scale  and popular Android 
application s. In the  study, we implemented and tested a static code 
analyzer PerfChecker , which is bui lt on Soot, a widely -used J ava 
program analysis framework [47]. PerfChecker supports the d e-
tection of two performance bug patterns identified  in our empir i-
cal study . We applied it to analyze  the latest version of the  29 
Android  application s, which comprise 1.1 million lines of Java 
code . PerfChecker successfully detected  126 matching instances 
of the two  bug pattern s in 18  of the se applications. We report ed 
them  to the corresponding  devel opers, and  68 instances have been 
confirmed  as real issues affecting  application  performance  while 
others are pending . Among the confirmed instances , developers 
have quickly  fixed 20 of them  by following our bu g-fixing su g-
gestions . They also  expressed great interest in  our PerfChecker . 
To summarize,  we make two major  contributions  in this paper : 
 To the best of our  knowledge, we conduct ed the first  empirical 
study of real-world  performance bugs  in smartphone applic a-
tions. Our findings  can help understand  characteristics of pe r-
formance bugs in  smartp hone application s, and provi de guid-
ance to related research (e.g., performance  testing and analysis) . 
 We implemented a static  code  analyze r, PerfChecker , to detect  
our identified performance  bug pattern s in Android applic a-
tions . PerfChecker successfully identified performance optim i-
zation opportunities in 18  popular  Android applications. This  
inspiringly validated the usefu lness of our  findings.  
The rest of this paper is organized as follows. Section  2 brief s 
Android application  basic s. Section 3 presents  our empiri cal study 
of real-world  performance bugs  in Android applications . Section 4  
present s our case study , in which  we leverage our empirica l find-
ings to find performance optimization opportunities in Android 
applications . Section 5 discusses related work  from  recent years , 
and finally  Section 6 concludes this paper.  
2. BACKGROUND  
Android is a n open -source  Linux -based operating system.  It is 
now one of the most widely adopted smartphone platform s. Many 
equipment and device manufacturers  (e.g., Samsung)  customize 
their own  Android  variants by modifying the Android  software 
stack  (e.g., kernel and  libraries) . Applications running on  the An-droid  platform  are mostly written  in Java language. For perfo r-
mance considerations, developers may  write  critical parts  of their 
applications  using  native -code languages such as C and  C++.  
Application  component  and l ifecycle . Conceptually, an Android 
application cons ists of four types of component s: activity , service , 
broadcast receiver  and content provider . For example,  an applic a-
tion’s graphical user interfaces (GUIs) are defined by activities.  
Each application component is required to follow a prescribed 
lifecycle that defines how this component is created, used, and 
finally destroyed.  For example, Figure 2 gives the  lifecycle  of an 
activity . It starts with a call to onCreate() handler and ends with a 
call to onDestroy() handler. An activity’s foreground lifetime  (i.e., 
the “Ru nning” state)  starts after calling onResume() handler, and 
lasts until onPause() handler is called, when another activity 
comes to foreground. An activity can interact w ith its user  only 
when it is at foreground. When it goes to background and b e-
comes invi sible  (i.e., the “Stopped” state) , its onStop() handler 
would be called. When its user navigate s back to a paused or 
stopped activi ty, the  activity’s onResume() or onRestart () handler 
would be called, and the activity would come to foreground again. 
In exceptional cases, a paused or stopped activity may be killed 
for releasing memory to other a pplications with higher priorities . 
Single  thread policy . When  an Android  applicati on starts, An-
droid  OS creates a “main thread”  (also known as  an “UI thread”) 
to instantiate  this application ’s components. This thread dispatc h-
es system calls to  responsible  application component s, and  user 
events to appropriate UI widgets (e.g., buttons).  After dispatching, 
the corresponding components’ lifecycle handlers and UI widgets’  
GUI event handlers  will run in the main thread  to handle the sys-
tem calls or  user events . This is known as  the “single thread pol i-
cy” [1]. The policy  requires developers to control workload s of 
their applications’  main thread s (e.g., not overwhelming a main 
thread with  intensive work). Otherwise,  application s can easily  
exhibit  poor responsiveness.  
3. EMPIRICAL STUDY  
In this section, we present our  empirical study of real -world pe r-
formance bugs from  Android applications.  The study aims to  an-
swer  our earlier four research questions  RQ1 –4. In the following, 
we first describe our  application  subject s and their reported  per-
formance  bugs, and then discuss our empirical findings.  
We selected ope n-source Android applications as our  subjects for 
studying  questions  RQ1 –4 because  the study  require s application  
bug reports and  corresponding  code revisions. 29 candidate appl i-
cations that satisfy the following three criteria were randomly 
selected  from four  popular  open -source software hosting pla t-
forms,  namely,  Google Code  [18], GitHub  [17], SourceForge  
[46], and Mozilla  repositories  [33]. Firs t, a candidate  should have  
achieved  more than 10 ,000 downloads  (popularity) . Second, it 
Running
StoppedLaunch Activity
Paused
DestroyedonStop ()1.  onCreate ()
2.  onStart ()
3.  onResume ()1.  onRestart ()
2.  onStart ()
3.  onResume () onPause ()
onResume ()
onDestroy ()
<<kill>>
<<kill>> 
Figure 2. Lifecy cle of  an activity  1014should  own a public bug tracking syste m (traceability) . Third, it 
should have  at least  hundred s of code revisions  (maintainability) . 
The three criteria provide a good indicator of popular and mature 
applications.  For these 29 candidates , we tried to  identif y perfo r-
mance bugs  in their bug tracki ng system s. Due to different man-
agement  practices, some  application  developers explicitly label ed 
performance bugs using  special  tags (e.g., “perf”), while other s 
did not maintain a  clear  categorization of reported bugs . To ensure 
that we study  real performance  bugs , we refine d our application  
selection by keeping  only those contain ing clearly labeled  perfo r-
mance bugs  for our  study . As a result, eight applications were 
finally selected as  our subjects  (all 29 applications were still used 
in our later case study for validating the usefulness of our empir i-
cal findings) . From the m, we obtained a total of  70 performance  
bugs , which were clearly labeled (confirmed) and later fixed . 
Our selection process could miss some performance bugs (e.g., 
those  not performance -labeled ). Some related studies  selected  
performance  bugs  by searching keywords like “slow” or “latency” 
in bug reports  [27][54]. We found that such searches resulted in  
more than 2,800 candidate performance bugs in all 29 applic a-
tions.  We randomly sampled  and manually analyzed  140 of these  
candidate  bugs (5%) , and found that  most of  them  are inappropr i-
ate for our study . This is because  more than  70% of these cand i-
dates  are either not related to performance  (i.e., their bug reports 
accidentally contain such keywords)  or are actually  complex bugs 
that contain  both performance and functional issue s (e.g., low 
performance as  a side effect of wrongly implemented functional i-
ty). To avoid introducing such threats  or uncontrollable issues to 
our empirical  study , we refrain ed from keyword search, while  
focus ing on the 70 explicitly labeled  performance bugs . 
Table 1 lists basic information of our eight selected  Android  ap-
plications . They are  large -scale  (up to 122.9K LOC) , popularly -
downloaded  (up to 100 million  downloads) , and cover five diffe r-
ent application categories. In the following  we analyze 70 perfo r-
mance bugs  collected  from  these applications and report our fin d-
ings. The whole  empirical  study  took about 180 person -days,  
involving  three students (two  postgraduate and one  final-year 
undergraduate ) for data collection , analysis  and cross -checking.  
3.1 RQ1: Bug Types and Impact s 
We studied  the bug repor ts and related discussions (e.g. , com-
ments  and patch reviews ) of the  70 performance  bugs , and as-
signed  them  to different categories  according to their major con-
sequence s. If a bug has multiple major consequences,  we assign ed 
it to multiple categories  (so accumulated percentages can exceed 
100%) . We observed three common types  of performance bugs in 
Android applications : 
GUI  lagging . Most  performance bugs  (53 / 70 = 75.7% ) are GUI 
lagging . They can significantly  reduce responsiveness or smoot h-ness of  the concerned applications’ GUIs and  prevent user events 
from being handled in a timely way. For example, in Firefox 
browser, tab switching could take up to ten seconds  in certain 
scenarios  (Firefox bug 7194932). This may  trigger  the infamous  
“Application Not Responding  (ANR) ” error  and caus e an applic a-
tion to be  no longer  runnable , because Android OS would force  its 
user to close the application  in such circumstances . 
Energy leak. The second common  type of performance  bugs  (10 / 
70 = 14.3% ) is energy leak . With  such bugs , concerned  applic a-
tions could  quickly consume excessive battery power  with certain 
tasks , which actually bring  almost no benefit to  users . For exa m-
ple, the energy leak in Zmanim (bug 50 ) made  the application 
render invisible GUI widgets  in certa in scenarios , and this use less 
computation simply wasted  valuable battery power . If an Android 
application contains serious energy leak s, its user’s  smart phone 
battery could  be drain ed in just a few hours . For instance , My 
Tracks  has received such complaints  (bug 520) : 
“I just inst alled My Tracks on my Galaxy Note 2 and it is 
a massive batt ery drain. My battery lost 10% i n standby 
just 20 minutes after a full charge.”  
“This app is destroying my battery. I will have to uninstall 
it if there isn’t a fix soon.”  
Energy leak s in smartpho ne applications can cause  great inco n-
venience to users . Users definitely do not want their smartphones 
to power off due to low bat tery, especially  when they need  to 
make important  phone calls . As shown in the above commen ts, if 
an application drains battery  quickly , users  may switch to other  
applications that offer  similar functionalities  but are more energy -
efficient . Such  a “switch”  can be  common  since  nowadays  users  
have m any choices  in selecting smartphone  application s. 
Memo ry bloat. The third common type of performance bugs  (8 / 
70 = 11.4% ) is memory bloat , which  can incur  unnecessarily  high 
memory consumption (e.g.,  Firefox bug 723077  and Chrome bug 
245782). Such  bugs  can cause  “Out of Memory  (OOM) ” error s 
and application crash es. Even if  a concerned application does not 
crash immediately (i.e., mild memory bloat ), its performa nce can 
become unstable as  Dalvik  garbage collection would  be frequently 
invoked, leading to degraded  application  performance . 
These  three  performance bug  types have occupied  a majority of 
our studied 70 performance bugs  (94.7%; some bugs belong to 
more than one type as aforementioned) . There are also o ther types 
of bugs (e.g., those  causing  high disk  consumption  or low network 
throughput ), but we observe d them only once  for each type  in our 
dataset.  Thus, we consider them not common.  
                                                                 
2 All bugs can be retrieved in their applications’ bug  tracking system s 
using  our provided  bug ID s. We omit detailed URLs due to page limit.  Table 1. Subjects and selected bugs  
Applicatio n name  Category  Size (LOC)  Programming language  Downloads  Availability  # selected bugs  
Firefox3 Communication1 122.9K2 Java, C++, C  10M2 ~ 50M  Mozilla  Repositories  34 
Chrome3 Communication  77.3K  Java, C++, Python  50M ~ 100M  Google Code  19 
AnkiDroid  Education  44.8K  Java 500K ~ 1M  Google Code  4 
K-9 Mail  Communication  76.2K  Java 1M ~ 5M  Google Code  3 
My Tracks  Health & Fitness  27.1K  Java 10M ~ 50M  Google Code  3 
c:geo  Entertainment  44.7K  Java 1M ~ 5M  GitHub  3 
Open GPS Tracker  Travel & Local  18.1K  Java 100K ~ 500K  Google Code  2 
Zmanim  Books & Reference  5.0K  Java 10K ~ 50K  Google Code  2 
1: The application category information is obtained from Google Play store  [19]; 2: 1K = 1,000 & 1M = 1,000,000  
3: For Firefox and Chrome, we count ed only  their lines of code specific to Android.  1015 
3.2 RQ2: Bug Manifestation  
Understanding  how performance bug s manifest  in Android appl i-
cations can provide  useful  implications  on how to effectively test 
performance bugs.  Our study  reveals  some observations , which  
demonstrate unique challenges  in such performance  testing . 
Small -scale  inputs  suffice to  manifest  performance  bugs . Exist-
ing studies reported  that two thirds of  performance bugs in PC 
applications  need large -scale inp uts to manifest  [27]. However, in 
our study, we observed  only 11 performance bugs  (out of 70)  that 
require  large -scale  inputs  to manifest . Here, we consider a dat a-
base with 100 data entries  already large -scale (e.g., Firefox bug 
725914 ). Other bugs can  easily  manifest with small -scale  inputs. 
For example,  Firefox  bugs 719493  and 747419  only need  one user 
to open several  browser tabs to manifest.  Manifested bugs woul d 
significantly slow down  Firefox and make its GUI less responsive.  
We give some  comments  from  their bug reports  below:  
“I installed the nightly version and found tab switching is 
so slow that it makes using more than one tab very hard .” 
“Firefox should correctly use view holder patterns. Ot h-
erwise, it will just have pretty bad scrolling performance 
when you have more than a couple of tabs.”  
These comments suggest that  Android  applications can be  susce p-
tible to performance bugs.  If an application has issu es affecting  its 
performance, u sers can  often  have  an uncomfortable experience  
when  conducting simple daily operations  like adding a browser 
tab (Firefox bug 719493) . A few such operations can quickly  
cause performance  degradation . Due to this reason, c autious d e-
velopers should  try their best to optimize  the performance of  their 
code. For example, c:geo developers  always try  to avoid creating  
short -term objects ( c:geo bug 222), because Android document a-
tion states that  less object creation (even an array of  Integers) 
means less garbage collection  [2]. 
Special  user interaction s needed to manifest  performance  
bugs . More than one third (25 out of 70) of performance bugs 
require spe cial user interactions  to manifest. For example, 
Zmanim ’s energy  leak needs the following four steps to manifest : 
(1) switch ing on GPS, (2) configuring  Zmanim to use cu rrent 
location, (3) start ing its main activity , and (4) hit ting the “Hom e” 
button when  GPS is acquiring  a location. Such bugs  are common, 
but can easily escape  traditional  testing. They can  only manifest  
after a cer tain sequence of user interactions  happen  to the con-
cerned  application , but traditional  code -based  testing adequacy  
criteria  (e.g., statement  or branch  coverage)  do not  really  consider  
sequences of  user interaction s. A recent stud y also  show s that 
existing testing techniques  often fail to reach certain part s of An-
droid  application code  [25]. Hence, our  finding s suggest  two chal-
lenge s and corresponding  research directions  in testing  perfo r-
mance bugs for smartphone applications:  
 Effectively t esting p erformance bugs require s coverage criteria  
that explicitly  consider  sequences of user interactions in a s-
sessing  the testing adequacy . Since the validity of user intera c-
tion sequences  is essentially  defined by an application’s GUI 
structure, existing  research on  GUI testing  coverage criteria  
[32] may help in address ing this challenge.  
 Test input generation should  construct  effective  user intera c-
tion sequences  to systematically explore a n application’s state 
space.  Since  such sequences can be infinite,  research effort should focus on  effective techniques that can identify equiv a-
lence among  constructed  user interaction sequences , avoiding 
redundant sequences  and wasted test effort s. 
Automated  performance  oracle needed . Performance bugs  can 
gradually degrade an application’s performance. For example, 
Firefox becomes  progressively  slower  when its  database ’s size 
grows  (bug 785945). Such  bugs rarely cause fail -stop cons e-
quences like application crash es, thus  it is challenging to decide 
whether an application is suffering any performance bug.  Yet, our 
study  found  three co mmon judgment criteria that have been used 
in real world to  detect performance  bugs in Android applications : 
 Human oracle . More than half of  the judgments were  made  
manually  by developers or users  in our investigated Android 
applications . People  simply  made judgment s according to their 
own experience s. 
 Product comparison.  Many developers  compar ed different 
products of similar functionalities to judge  whether a partic u-
lar product contains any  performance  bugs (e.g., checking 
whether conduc ting an operation in one product  is remarkably  
slower than  in other products) . We observed ten  such cases in 
our stud y. For example, upon receiving user complaints about 
performance , K9 Mail developers check ed whether  their ap-
plication’s performance was comp arable to other email clients 
and then  decide d what to do  next (K9 Mail bugs 14 and  23). 
 Developers’ consensus.  Developers  also have  some  implicit 
consensus  for judging  performance bugs . For instance, Google 
developers  consider an application  sluggish  (i.e., GUI lagging)  
if a user event cannot be handled within 200 ms [53]. Mozilla  
developers  assume  that Firefox’s  graphics -rendering units  
should be able to produce 60 frames per second  to make  
smooth  animations (Firefox b ugs 767980 and  670930).  
Although these judgment criteria have been used in practice, they 
either re quire non -trivial manual effort  (thus not scalable)  or are  
not generally  defined  (thus not widely used) . To facilitate  perfo r-
mance testing and analysis,  automated oracles  are thus desirable . 
Even if general oracles may not be possible , application or bug  
specific  oracles  can still be  helpful . Encoura gingly, there have 
been  initial attempts toward  this end  [41][55]. Besides, our prev i-
ous work  [30] also proposed a cost -benefit analysis  to detect ene r-
gy leak s caused by improper  or ineffective  uses of smartphone 
sensors.  Still, more  effort on general automated oracle s for pe r-
formance bugs is needed to further advance rela ted research . 
Performance b ugs can be p latform -dependent.  We also ob-
serve d that a  non-negligible proportion (6 out of 70) of perfo r-
mance bugs requir e specific software or hardware  platforms  to 
manifest. For example, Chrome’s caching scheme  would  hurt 
performance on ARM -based devices, but not on x86-based devi c-
es (Chrome bugs 170344 and 245782).  Firefox’s animation works 
more smoothly  on Android 4.0 than  older systems ( Firefox bug 
767980 ). This suggests  that developers should consider  device 
varie ty during performance testing, since  Android  OS can run on 
different hardware  platforms  and has so many customized var i-
ants. This feature differs largely f rom performance bugs in PC 
applications, which are not so platform -dependent [27][54]. 
  
3.3 RQ3: Debugging and Bug-fixing Effort  
To understand the  effort  required for performance  debugging and 
bug-fixing  for Android applications , we analyzed  60 of our  70 GUI lagging, energy leak and memory bloat  are three dominant 
performance bug types in our studied Android applications.  
Research  effort can  first be devoted into  designing effective 
techniques to combat  them. 
Effective performance testing need s: (1) new coverage criteria 
to assess  testing adequacy, (2) effective techniques for genera t-
ing user interaction  sequences to manifest  performance bugs, 
and (3) automated oracle s to judge  performance degradation . 1016performance bugs . We excluded  10 remaining  bugs because we 
failed to recover  links between  their bug reports and  code rev i-
sions3. To quantify debugging and bug -fixing  effort  for each  of 
these bugs, we measured  three metrics  that were also adopted in  
related studies  [54]: (1) bug open duration , which is the amount of 
time from a bug re port is opened to the concerned bug is fixed ; (2) 
number of bug comments , which counts discussions among deve l-
opers and users  for a bug  during  its debugging and bug -fixing  
period ; (3) patch size , which is the lines of code changed  for fi x-
ing a bug . Intuitively, i f a bug is difficult to debug  and fix, its 
report would  be open for  a long time, d evelopers tend to d iscuss  it 
more , and  its patch  could  cover more lines of code  changes . 
Table 2 reports  our measurement  results.  We observe that on a v-
erage, it takes develop ers about  two month s to debug  and fix  a 
performance bug  in an Android application . During this period , 
they can ha ve tens of rounds of  discussions , resulting in  many bug 
comments  (up to 71) . Besides, on average , bug-fixing  patches can 
cover more than  182 lines of code changes , indicating non-trivial  
bug-fixing  effort. For comparison, we also randomly selected  200 
non-performance bugs ( bugs  without performance labels) from 
the bug database of Firefox and Chrome  (we selected  100 bugs for 
each) . We did not  select non -performance bugs from other appl i-
cation subjects for comparison , because each of  these subjects  
contai ns only a few performance bugs (about  two to four).  Such 
small sample sizes may l ead to unreliable  comparison results , 
leaving a weak foundation for further research on related topics  
[12]. On the other hand, the vast majority of our studied perfo r-
mance bugs come from Firefox and Chrome, and therefore  we 
select ed non-performance bugs from these two subjects for co m-
parison.  The severity level s of our selected  200 non-performance 
bugs are comparable  to those  of performance bugs in Firefox and 
                                                                 
3 Our manual  analysis of commit logs around bug -fixing dates also failed 
to find corresponding  code revisions.  Chrome . Figure 3 compares these tw o kinds  of bugs by  boxplot s. 
The results consistent ly show  that performance debugging and 
bug-fixing  require  more effort than  their non-performance cou n-
terparts . For example,  in Firefox , the median bug open duration is 
42 workdays  for performance bugs, but only 13 workdays  for 
non-performance bugs . To understand the  significance of  the dif-
ferences  between these two kinds of bugs , we conducted a Mann -
Whitney U -test [31] with the following  three  null hypotheses:  
 Performance d ebugging  and bug-fixing  do not take  a signif i-
cantly longer time tha n their non-performance counterparts . 
 Performance d ebugging and bug-fixing do not need  significan t-
ly more  discussion than  their non-performance counterparts . 
 Patches for fixing  performance bugs are not significantly larger 
than those for fixing  non-performance bugs.  
Table 3 gives  our Mann -Whitney U -test results ( p-values) . The 
results reject ed the above  three null  hypotheses  all with a conf i-
dence level over 0.95 ( i.e., p-values are  all less than 0. 05). Th us, 
we conclude that debugging and fixing  performance bug s indeed  
requires more effort  than debugging and fixing  non-performance 
bugs.  This result  can help developers  better understand and  priori-
tize bugs for  fixing in a cost -effective way, as well as estimating  
possible  manual effort  required  for fixing certain  bugs.  
We further  looked into bug comment s and bug -fixing  patches to 
understand : (1) why it is difficult to debug  and fix  performance 
bugs  in Android applications , and (2) what  support is expected in 
debugging  and bug -fixing . We found that  quite a few  (22 / 70 = 
31.4% ) performance bugs involve multiple threads or processes , 
which  may have  complicated  the debugging  and bug -fixing tasks . 
In addition,  these  performance  bugs rarely cause d fail-stop cons e-
quences such as  application  crash es. Due to this reason , traditional  
debugging  infor mation (e.g., stack trace) can  offer little help in  
performance debugging . We analyzed all 70 performance bugs, 
and found that  only four bugs have  had their debugging  and fixing 
tasks  receiving some help from such  traditional  information , as 
judged from  their bug discussions  (e.g.,  c:geo bug 949 and Firefox 
bug 721216 ). On the o ther hand , we found that debugging infor-
mation from two kinds of tools has  received more attention:  
Profiling tools.  Profiling tools  (or profilers)  monitor an a pplic a-
tion’s execution , record  its runtime information (e.g., execution 
time of  a code unit ), and trace  details of  its resource consumption 
(e.g., memory).  For example,  Firefox and Chrome developers  
often take three  steps in performance debugging:  (1) reproducing 
a performance bug  with the information  provided  in its bug report  
if any , (2) running the application  of concern  for a  long while to 
generate  a profile using their own  profi lers [9][14], and (3) per-
form ing offline  profile  analysis to identify  performance bottle-Table 2. Performance bug debugging and fixing e ffort  
Metric  Min.  Median  Max.  Mean  
Bug open duration (days)  1 47 378 59.2 
Number of  bug comments  1 14 71 16.7 
Patch size  (LOC)  2 72 2,104  182.3  
Table 3. p-values of Mann -Whitney U -tests  
Subject  p-value  
Bug open duration  # bug comments  Patch size  
Firefox  0.0008  0.0002  0.0206  
Chrome  0.0378  0.0186  0.0119  
NPerf
(a)Bug open duration (days)
Firefox(b) Number of bug comments (c) Patch size (lines of code changes)
125102050100200500Chrome Firefox Chrome Firefox Chrome
NPerf Perf NPerf Perf NPerf Perf NPerf Perf NPerf Perf Perf125102050100200500
125102050100
125102050100
5105010050010005000
15105010050010005000
142
139.518
50.5
2135
13.5
6.5
313.563 
Figure 3. Comparison of d ebugging  and bug-fixing effort (“Perf” = “perfo rmance bug” , “NPerf” = “non -performance bug”)  1017neck s/bugs  if possible . However,  profile analysis can be very 
time-consuming and painful , because current tools ( e.g., those 
from  Android SDK)  can record  tons of  runtime  information , but  
which runtime information can actually  help performance debu g-
ging is  still an open question . Firefox developers  have  designed 
some  visualization tool s (e.g., Cleopatra  [14]) to save manual 
effort in profile analysis , but these tools  are not accessible to  other  
developers  or applicable to other applications . Researcher s and 
practitioners  are thus encouraged to design  new general  tech-
niques and tools for analyzing, aggregating , simplifying  and vis u-
alizing  profiling data  to facilitate  performance  debugging . 
Performance m easurement  tools.  Performance measurement 
tools  can also ease  performance debugging. They  can directly 
report performance for a selected code unit in an  application . For 
example, Firefox ’s frame rate meter  [15] measures  the number of  
frames a graphic s-rendering unit  can produce per second ( e.g., 
when debugging  Firefox  bug 670930). This information  can help 
developer s in two ways . First, it prioritiz es the code  units that  
need  performance  optimization . Second, it  suggests whether a 
code unit has been adequately optimized . For example, Firefox 
developers could  stop further  optimizing a graphics -rendering unit 
if the frame rate meter  reports a score of  60 frames per second  
(e.g.,  when fixing  Firefox bug 767980) . Chrome  developers  also 
use similar tools  (e.g., using smoothness measurement tools for 
debugging  Chrome  bug 242976).  Such tools are useful and  wel-
come d by Android  developers. We show  some comments  about 
Firefox’s frame rate meter from  the developers’ mail ing list:  
“I found it very useful for finding performance issues  in 
Firefox UI, and web devs should find it useful too.”  
“This is fantastic stuff. It’s a must -have for people hac k-
ing on front end UI . Also for devs  tracking animation perf.”  
Besides  understanding the challenges of  performance debugging, 
we also  looked for reasons  from bug -fixing  patches why fixing  
performance bug s is so difficult . We found that such patches  are 
often  complex  and have to conduct : (1) algorithmic changes (e.g., 
Firefox bug 767980), (2) design pattern reimplementation (e.g ., 
Firefox bug 735636), or (3) data structure or  caching scheme r e-
design (e.g ., Chrome bug 245782).  Such  bug-fixing  tasks are us u-
ally complex.  This explains why fixing performance bugs took a 
longer time and incurred much large r patch sizes than fixing non -
performance bugs, as illustrated in Figure 3. 
 3.4 RQ4: Common  Bug Patterns  
To learn  the root causes of our  70 performance bugs, we studied 
their bug reports, patches, c ommit logs  and patch reviews. We 
managed to figure out root causes for 52 of these bugs. For the 
remaining 18 bugs, we failed due to the lack of info rmative mat e-
rials (e.g.,  related  bug discussions).  
Performance bugs  in Android applications  can have  complex or 
application -specific  root causes . For example, Firefox’s  “slow tab 
closing” bug was caused by heavy message communication s be-
tween  its native code and Java code  (Firefox bug 719494) , while  
AnkiDroid suffered GUI lagging because its database library was 
inefficient ( AnkiDroid bug 876) . Despite such variety , we still 
identified  three common causes for  21 out of the 52 performance 
bugs (40.4%). We  explain them  with concrete examples  below . 
Lengthy operations in  main thread s. As mentioned  earlier, An-
droid application s should not block  their main thread s with heavy 
tasks  [1]. However, when applications become  increasingly more  
complex , developers  tend to leave lengthy operations in  main 
thread s. We observed quite a few occurrences  of such bugs  (11 / 
52 = 21.2%) . Figure 4 gives  a simplified version of  Firefox bug 
721216  and its  bug-fixing  patch . This bug caused  Firefox  to suffer  
GUI lagging when its “tab strip” button was clicked. The bug 
occurred  because the  button’s click  event  handler transitively 
called a “refreshThumbnails” method , which  produced a thum b-
nail for each browser tab by  iteratively call ing heavy -weight  Bit-
map compression APIs  (Lines 3 –5). Later t o fix this  bug, deve l-
opers move d such heavy operations to a background thread  (Lines 
6–12), which  can asynchronously update  Firefox’s  GUI.  
Wasted computation for invisible  GUI . When an Android a p-
plication switches to background, it may still keep updating it s 
invisible  GUI. This bring s almost no per ceptible benefit to its 
user, and thus the performed computation  (e.g., collecting data 
and updating GUI) simply waste s resources (e.g., battery power) . 
Such bugs also form a common pattern, which covers 6 of the 52 
performance  bugs  (6 / 52 = 11.5%) . For i nstance, Figure 5 lists the 
concerned code and corresponding  bug-fixing  patch  for our 
aforementioned energy leak in  Zmanim  (bug 50) . When Zman i-
mActivity  launches, it  registers a location listener to receive loc a-
tion changes  for updating its GUI  (Lines 5 –14). The location li s-
tener  is normally  unregistered when the activity is destroyed  (Line 
27). However,  if a user launches Zmanim  and then switches it to 
background  (Android OS will call  onPause() and onStop() ha n-
dlers accordingly , but not onDestroy() ), the application will keep 
receiving location changes  to update its GUI, which is , however,  
invisible . The location sensing and GUI refreshing are  then use-
less, but still drain battery  power . This can be common for  many 
smartphone applications, because users  often  perform multiple 
tasks at the same time (e.g., play ing Facebook and Twitter while 
listening to music) and frequently switch between them . To fix 
such bugs, developers have  to carefully  monitor application state s 
and disable unnecessary tasks when an application go es to bac k-
ground. For example , Firefox developers suggest ed disabling  
timers, animations, DOM events, audio, video, flash plugins, and 
sensors  when Firefox went to background  (Firefox bug 736311 ). 
Similarly,  as Figure 5 shows , Zmanim developers disable d loca-
tion sensing  by unregistering the location li stener in ZmanimA c-
tivity’s onPause() handler (Line 23 ), and enabled it again in  onRe-
sume() handler when  necessary (Lines 17 –19). 
Frequently invoked  heavy -weight  callbacks.  Four out of the 52 
performance bugs  (4 / 52 = 7.7% ) concern frequently invoked  
callbacks. These callbacks need to  be light -weight since they  are 
frequently invoked by Android OS.  However,  many such 
callbacks in  real-world applications  are ill-implemented.  They are 
public voidrefreshThumbnails () {
//generate a thumbnail for each browser tab
-Iterator<Tab > iter= tabs.values ().iterator();
-while(iter.hasNext ())
-GeckoApp. mAppContext .genThumbnailForTab (iter.next ());
+    GeckoAppShell.getHandler ().post( newRunnable() {
+    public voidrun() {
+      Iterator<Tab > iter= tabs.values ().iterator();
+        while(iter.hasNext ())
+        GeckoApp. mAppContext .genThumbnailForTab (iter.next ());
+      }
+    });
}
Note: the method genThumbnailForTab () compresses a bitmap to 
produce a thumbnail for a browser tab.1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13. 
Figure 4. Firefox bug 721216 (simplified)  
Debugging and fixing performance bugs are generally more 
difficult than debugging and fixing non -performance bugs. I n-
formation provided by profilers and performance measurement 
tools are more helpful for debugging than traditional info r-
mation like sta ck trace. Existing profilers expect improvement 
for automatically analyzing, aggregating, simplifying and vis u-
alizing collected runtime profiles.  1018heavy -weight  and can significantly slow down  concerned applic a-
tions . We illustrate with  a list view callback example  below . 
A list view display s a list of scrollable items  and is widely used in 
Android applications.  Figure 6 gives one example , where  each 
listed item contains two elements  (i.e., two inner views of the list 
item) : an icon and a text label . When a user scroll s up a list view, 
some existing  items  will go off  the top of the  screen  while  some  
new items  will be added to the bottom . To use  a list view , deve l-opers need to  write  an adapter class and de fine its  getView() 
callback  (see Figure 7 for example ). At runtime, when  a new item 
needs to  go onto  the screen, Android OS  will invoke the 
getView( ) callback to construct  and show  this item.  This callback 
conducts two operations: (1 ) parsing the new item’s layout XML 
file and constructing a tree of its elements  (a.k.a., list item layout  
inflation) , and (2)  traversing the tree to retrieve  specific  element s 
for updating  (a.k.a., inner view  retriev al and update ). However, 
XML parsing and tree traversing can be  time-consuming  when a  
list item ’s layout is complex  (e.g., containing many  elements  or 
having hierarchical structures  as Android applications typically 
do). Screen scrolling can  thus slow down if such operations are 
common ly perf ormed . For perfor mance concern s, Android OS  
recycles each item  that goes off the screen  while users scroll  a list 
view . The  recycled items can be  reuse d to construct  new items 
that need to  appear  later. Such  “recycl e and reuse”  can be done as 
list items  often  have identical  layout s. 
We give two version s of getView() implementation  in Figure 7. 
The first ineffic ient version conducts  two aforementioned opera-
tions  (Lines 2 –9) every time the callback is invoked. The second  
version  applies a  “view holder”  design pattern suggested by  An-
droid documentation  [3]. The basic idea is  to reuse previously  
recycled list items.  It avoids  list item layout inflation  when  there 
are recycled item s for reuse  (Line s 24–25). Besides, when  a list 
item is constructed  for the first time , the references to its inner 
view objects  are identified and  stored in a special data structure  
(Lines 18 –22; data structure  defined at Lines 32 –36). Later , when 
reusing recycled i tems, these stored references can be used direc t-
ly for updating content (Lines 27–29), avoiding inner view r e-
trieval  operations . By doing so , the view holder pattern  can save 
both computation  for list item layout inflation  and inner view 
retrieval , and mem ory for constructing new list items. Frequently 
invoked callbacks  should  adopt such efficient designs . 
 
3.5 Discussions  
Our findings of performance bugs  in smartphone applications  
exhibit some unique features,  as compared with those from PC or 
server -side applications.  First,  smartphone application platforms 
are new  and quickly evolving . For example,  the current Androi d 
platform is not fully optimized and  developers  keep  improving its  
performance for better use r experience . Smartphone appli cations 
are thus susceptible to performance bugs  due to  such platform 
instability . As shown  earlier , Android users can easily manifest 
public class ZmanimActivity extends Activity {
private ZmanimLocationManager lm;
private ZmanimLocationManager.Listener locListener ;
public void onCreate () {
//get a reference to system location manager
lm= newZmanimLocationManager (ZmanimActivity. this);
locListener = newZmanimLocationManager.Listener() {
public void onLocationChanged (ZmanimLocation newLoc) {
//build UI using obtained location in a new thread
rebuildUI (newLoc);
}
};
//register location listener
lm.requestLocationUpdates (GPS, 0, 0, locListener );
}
public void onResume () {
+     //register location listener if UI still needs update
+     if(buildingUINotFinished )
+       lm.requestLocationUpdates (GPS, 0, 0, locListener );
}
public void onPause() {
+     //unregister location listener
+     lm.removeListener (locListener );
}
public void onDestory () {
-//unregister location listener
-lm.removeListener (locListener );
}
}1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.
14.
15.
16.
17.
18.
19.
20.
21.
22.
23.
24.
25.
26.
27.
28.
29. 
Figure 5. Zmanim bug 50 (simplified)  
Text 1
Text 2
Text 3
Text 4Text 1
Text 2
Text 3
Text 4
Screen
System recycler
Text 1Screen
//this callback constructs the new list item
View getView(intpos, View recycledView , ...)ScrollingOld item goes 
off screen and 
gets recycled
New item goes 
onto screen. 
Figure 6. List view example  
//inefficient version
public View getView( intpos, View recycledView , ViewGroup parent) {
//list item layout inflation
View item = mInflater .inflate (R.layout. listItem , null);
//find inner views
TextView txtView = (TextView) item.findViewById (R.id. text);
ImageView imgView = (ImageView) item.findViewById (R.id. icon);
//update inner views
txtView.setText (DATA[pos]);
imgView.setImageBitmap ((pos% 2) == 1 ? mIcon1: mIcon2);
returnitem;
}
//apply view holder pattern
public View getView(intpos, View recycledView , ViewGroup parent) {
ViewHolder holder;
if(recycledView == null) { //no recycled view to reuse
//list item layout inflation
recycledView = mInflater .inflate (R.layout. listItem , null);
holder = newViewHolder ();
//find inner views and cache their references
holder.text= (TextView) recycledView.findViewById (R.id. text);
holder.icon= (ImageView) recycledView.findViewById (R.id. icon);
recycledView.setTag (holder);
} else{ 
//reuse the recycled view, retrieve the inner view references
holder = ( ViewHolder ) recycledView.getTag ();
}
//update inner view contents
holder.text.setText (DATA[pos]);
holder.icon.setImageBitmap ((pos% 2) == 1 ? mIcon1: mIcon2);
returnrecycledView ;
}
//view holder class for caching inner view references
public class ViewHolder {
TextView text;
ImageView icon;
}1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.
14.
15.
16.
17.
18.
19.
20.
21.
22.
23.
24.
25.
26.
27.
28.
29.
30.
31.
32.
33.
34.
35.
36.
 
Figure 7. View holder pattern  Our study identified three common  performance bug patterns : 
(1) lengthy operations in main  threads, (2) wasted computation  
for invisible  GUI, and (3)  frequently -invoked  heavy -weight 
callback s. Researchers and practitioners should design effective 
techniques to prevent and detect such performance bugs.  1019performance bugs by simple daily operations. Second, smartphone 
applications  run on  devices with small -sized touch screens.  Users 
interact with these devices  in a way  that is  very different from  
what they do  with PCs.  For example,  users perform screen scrol l-
ing much more frequently  on smartphones  than on PCs. This 
makes  GUI responsivenes s and smoothness more crucial for  
smartphones.  Our study reported  that GUI-related bugs are perv a-
sive and have  become a dominant bug type that concern s Android 
applications’ performance . Third, smartphone applications run on 
devices with sm all-capacity bat teries, but can  access energy -
consuming  components like GPS sensor  and accelerometer , which 
are usually  not available  on PCs.  Cost-ineffective uses of such 
components can lead to high energy consumption . Indeed, w e 
found  energy leak s were  clearly severe in our studied Android 
applications . These comparisons help researchers and practitio n-
ers understand how performance bugs occur  in smartphone appl i-
cations , as well as expla ining why they differ  from their  counte r-
parts in traditional PC or server -side applica tions.  
The validity of our study  results may be subject to several  threats. 
The first  is the representativeness of our selected Android applic a-
tions. To minimize this threat, we selected eight large -scale  and 
popular ly-downloaded  Android applications , whic h cover five 
different  categories.  We wish to generalize our findings to more 
applications, and  we will show  in Section 4 how our findings  help 
detect performance bugs for a wider range of An droid applic a-
tions . The second threat is our manual inspec tion of the selected 
performance bugs . We under stand that  manual  inspection can be 
error -prone. To reduce this threat, we  asked participant s to co n-
duct manual inspection independently. We also re -examined and 
cross -validated all results  for consistency.  
4. DETECTING PERFORMANCE BUGS  
In this section, we conduct a case study to investigate whether our 
empirical findings  can help fight with  performance bugs in real-
world  Android applications.  This case study aims to answer the 
following research question:   RQ5:  Can we lev erage our empirical findings, e.g. , common 
bug patterns,  to help developers  identify  performanc e optim i-
zation opportunities in real-world Androi d applications ? 
To answer  research question  RQ5 , we built a static code analyzer,  
PerfChecker4, on Soot , a widely -used Java program analysis 
framework [47]. We app lied PerfChecker to 29 Android applic a-
tions discussed  earlier  to check its effectiveness of  detecting pe r-
formance bugs . We first  introduce  PerfChecker’s implementa tion 
in Section  4.1, and then report our study results  in Section 4.2. 
4.1 Performance Bug Checking Algorithms  
PerfChecker  takes as input an Android application’s Java 
bytecode , and generate s warnings if it detects  any issue s that may 
affect  application  performance . Its current implementation  sup-
ports detecting  two of our identified performance bug patterns: (1) 
lengthy operat ions in main thread s, and (2) violation of  the view 
holder pattern (as a concrete case of the “ frequently invoked  
heavy -weight  callbacks ” bug pattern) . We did not include the 
“wasted computation  for invisible  GUI” bug pattern  in this study. 
This is because our previous work  [30] has proposed a cost -
benefit analysis to detect  performance bugs of  this pattern  and 
demonstrated the effectiveness of such analys is. Therefore, w e in 
this paper focus on the other two performance bug patterns.  
Detecting lengthy operations in  main thread s. PerfChecker  first 
conduct s a class hierarchy analysis to identify  a set of chec k-
points . These checkpoints include  those lifecycle  handlers defined 
in application component  class es (e.g., those extending the Activ i-
ty class)  and GUI event handlers defined in  GUI widget class es. 
According to Android’s single thread policy, these checkpoints  
are all executed in an  Android  application’s main thread.  Then 
PerfChecker  constructs  a call graph for ea ch checkpoint, and 
traverses this  graph to check whether  the checkpoint transitively 
                                                                 
4 PerfChecker can be obtained at: http://sccpu2.cse.ust.hk/perfchecker  Table 4. Subjec ts and the detected bug pattern instances in them  
Application name  Application  
category  Revision 
no. Size 
(LOC)  Downloads  Availability  Bug pattern instances  Bug ID(s) VH (69) LM (57) 
Ushahidi  Communication  59fbb533d0  43.3K  10K ~ 50K  GitHub [48] 9* 2 146, 159 
c:geo  Entertainment  6e4a8d4ba8  37.7K 1M ~ 5M  GitHub  [8] 0 5 3054  
Omnidroid  Productivity  865 12.4K  1K ~ 5K  Google Code  [38] 9 8 182, 183 
Open GPS Tracker  Travel & Local  14ef48c15d  18.1K  100K ~ 500K  Google Code  [39] 1 0 390 
Geohash Droid  Entertainment  65bfe32755  7.0K  10K ~ 50K  Google Code  [16] 0 1 48 
Android Wifi Tether  Communication  570 9.2K  1M ~ 5M  Google Code  [4] 1 3 1829, 1856 
Osmand  Travel & Local  8a25c617b1  77.4K  500K ~ 1M  Google Code  [40] 18 17 1977 , 2025  
My Tracks  Health & Fitness  e6b9c6652f  27.1K  10M ~ 50M  Google Code  [34] 2* 0 1327  
WebSMS  Communication  1f596fbd29  7.9K  100K ~ 500K  Google Code  [49] 0 1 801 
XBMC Remote  Media & Video  594e4e5c98  53.3K  1M ~ 5M  Google Code  [50] 1 0 714 
ConnectBot  Communication  716cdaa484  33.7K  1M ~ 5M  Google Code  [10] 0 6 658 
Firefox  Communication  895a9905dd  122.9 K 10M ~ 50M  Mozilla Repositories [33] 1 0 899416  
APG  Communication  a6a371024b  98.2K  50K ~ 100K  Google Code  [6] 4 8 140, 144 
FBReaderJ  Books & References  0f02d4e923  103.4K  5M ~ 10M  GitHub  [13] 6* 6 148, 151 
Bitcoin Wallet  Finance  12ca4c71ac  35.1K  100K ~  500K  Google Code  [7] 4 0 190 
AnySoftKeyboard  Tools  04bf623ec1  26.0K  500K ~ 1M  GitHub  [5] 2* 0 190 
OI File Manager  Productivity  f513b4d0b6  7.8K  5M ~ 10M  GitHub  [37] 1* 0 39 
IMSDroid  Media & Video  553 21.9K  100K ~ 500K  Google Code  [24] 10 0 457 
1. “VH” means “ Violation  of the view Holder pattern” , and “LM” means “ Lengthy operations in Main thread s”. 
2. Underlined bug pattern instances have been confirmed by developers as real  performance  issues, and “*” marked instances have been fixed by developers  
accordingly . For more details , reader s can visit corresponding subject’s source repositories  and bug tracking systems  by our provided links  and bug ID s. 1020invoke s any heavy APIs, e.g.,  networking, database query, file IO, 
or other expen sive APIs like those for B itmap r esizing . 
PerfChecker would  report a warning  for any  of such findings . 
Detecting violation of the view holder pattern.  Similarly, 
PerfChecker first conduct s a class hierarc hy analysis to identify a 
set of  checkpoints  including  all getView() callbacks  in concerned  
list view s’ adapter classes. PerfChecker then constructs  a program 
dependency graph for ea ch checkpoint, and traverses this  graph to 
check whether  the following rule  is violated : list item layout infl a-
tion and inner view retrieval  operations  should be cond itional ly 
conducted based on  whether there are  reusable list items . 
PerfChecker would  report a warning  for any  such of violations.  
4.2 Study Results  and Developer ’s Feedback  
We applied PerfChecker to analyze  the latest version of  all 29 
Android applications, which comprise more than 1.1 million lines 
of Java code. Encouragingly, PerfChecker  finished analyzing each 
application within  a few minutes  and detect ed 126 matching i n-
stances of the two performance bug patterns  in 18 of the 29 a ppli-
cations.  Table 4 gives details of these applications.  It reports f or 
each application : (1) application name, (2)  application  category, 
(3) revision  number , (4) program size, (5) number of download s, 
(6) source code availabil ity, (7) number of bug pattern instance s 
detected , and (8) concerned bug ID (s). For example, Osmand is a 
large -scale (77.4K LOC) and popular  (500K –1M downloads)  
Android  application  for na vigation. PerfChecker detected 17  
checkpoints (i.e., handlers) invoking  file IO and da tabase query 
APIs in Osmand’s  main thread, and  18 violations of  the view 
holder pattern  throughout Osmand . We report ed our findings  as 
well as optimization suggestions  to corresponding developers and 
received  prompt and  enthusiastic feedback . Altogether,  68 detec t-
ed bug pattern instance s (54.0%; bold and underlined in Table 4) 
have been  quickly (within a few work days)  confirmed  by deve l-
opers  as real issues that affect  application  performance . These 
issues cover 9 of the 18 applications (50.0%). In addition, 20 of 
the 68 confirmed  issues  (29.4%; marked with “*” in Table 4) have 
been fixed  in a timely fashion  by following our suggestions . Some 
developers, although not imme diately fixing their  confirmed i s-
sues, promised  to optimiz e their application s according t o our 
suggestion s (e.g., My Tracks bug 1327) . Other  reported  issues are 
pending  (their concerned  applications may not be under  active  
maintenance ). We also communicated with developers  via b ug 
reports and obtained some interesting findings  as discussed bel ow. 
First,  developers show ed great interest  in our PerfChecker.  For 
example, we  received the following feedback : 
“Thanks for reporting this. I’ll take a look at it. Just cur i-
ous, where is this static code checker? Anywhere I can 
play with it as well? Thanks.”              -- Ushahidi bug 159  
“Thanks for your report. The code is only a year old . 
That’s probably the reason (why it’s not well optimized). 
Your static analy zer sounds really interesting. I wonder if 
lint5can also check this.”            -- OI File Man ager bug 39  
These  comments  suggest that  developers welcome performance  
analysis tools to help optimize their Android applications. 
PerfChecker  is helpful, especially for complex  applications . For 
example, it detected  some applications transitively calling heavy -
weight APIs in the ir main thread s, and the call chain s can last for  
tens of method calls  (e.g., c:geo bug 3054) . Currently , there are 
                                                                 
5 Lint is a static checker from  Android development tools . It can  detect 
performance threats  like using getters instead of direct field accesses 
within a class, but does not support our identified performance bug pa t-
terns. Details can be found at http://tools.android.com/tips/lint -checks . few industrial -strength  tools support ing smartphone performance 
analysis. Thus there is  a strong need for ef fective tools to  help 
developers fight with smartphone application performance bugs.  
Second,  some developers  held conservative attitudes toward  per-
formance  optimization.  They  concerned much , e.g.,  (1) whether 
optimization can bring signif icant performance gains , (2) whether 
optimization  can be done easily , and (3) whe ther optimization  
helps toward an  application’s market success . They hesitate d to 
conduct  performance  optimization when  the optimization  seem  to 
require  a lot of  effort  but bring  no immediate  benefits.  For exa m-
ple, WebSMS  and Firefox  developers  responded  as follows:  
“You are totally right. WebSMS is ported from J2ME and  
has a very  old code  base .. . If I would write it from 
scratch , I’d do things  different ly. I once started  refacto r-
ing, but gave up in the end. There were other things to do, 
and the  SMS user base is shrinking  globally … If you 
want to help, just fork it on GitHub and let me merge your 
changes. I’d be very thankful.”           -- Web SMS bug 801  
“Thanks for the report! This shoul dn't be a big concern; 
that UI is not a high -volume part. We'll keep this bug 
open, and I 'd accept a patch which improves  the code, but 
it's not a high -priority work item. ”  -- Firefox bug 899416  
Finally , some developers were  cautious  and willing  to conduc t 
code  optimization to improve  performance or maintainability for 
their applications . For example, c:geo developers  responded : 
“Such optimizations are ‘micro optimizations’, and they 
do not improve the user visible performance. Good deve l-
opers however will  still refactor code into the better ve r-
sion, mostly to make it more readable. ”    -- c:geo bug 222  
c:geo developers quickly fixed this  reported  performance  bug. 
This may explain why c:geo keeps being  highly -rated and popu-
larly-downloaded  (1M–5M downloads ) on the market.  
 
5. RELATED WORK  
Our studies in this  paper  relate to a large body of  existing  work on 
testing, debugging, bug detection  and understanding  for applic a-
tion performance . We discuss  some  representative pieces of work  
in recent years . Some of them focus on  smartph one application  
performance, while  others  are for  PC or server -side applications.   
Detecting performance  bugs. Much  research  effort  has been 
devoted to automating  performance bug /issue6 detect ion. For e x-
ample,  Xu et al. us ed cost-benefit analysis to detect  high-cost data 
structures that bring little benefit to  a program’s output [52]. Such  
data structures can cause memory bloat . Xiao et al. used a predi c-
tive approac h to detect  workload -sensitive  loops  that contain 
heavy  operations, which often cause  performance bottleneck s 
[51]. Recent work  Toddler  by Nistor et al . detect ed repetitive  
computations  that have similar memory -access patterns in loops . 
Such computations can  be unnecessary and subject to optimiz a-
tion [35]. These pieces of work focused  on performance issue s in 
PC or server -side applications , while there are also  other  pieces of 
work  particularly focusing  on smartphone application  perfo r-
mance . For example, Pathak  et al. studied  no-sleep  energy  bugs  in 
                                                                 
6 Some researchers pre fer “performance issue ” to “perfor mance bug ”. We 
do not have  a preference  and use the two terms interchangeably . Our stat ic code analyzer  detect ed quite a fe w new  issues  that 
affect performance  in a wide range o f real -world Android a p-
plication . Developers showed great interest  in such tools.  This 
validates the usefulness of our empirical findings.  1021Andro id applications  and used reaching -definition dataflow anal y-
sis to detect  such bugs  (e.g., an application forgets to unregister a 
used sensor)  [41]. Following  in this direction, Guo et al. further 
proposed a technique to detect general resource leaks, which  often  
cause  performance degradation [22]. Similar to Xu et al.’s [52] 
and Zhang et al.’s work [55], we previously leveraged cost-benefit  
analysis  to detect  whether an A ndroid application  uses sensory 
data in a cost -ineffective way  [30]. Potential energy leak bugs can 
be reported after cross -state data utilization  comparison s.  
Testing for application performance . Performance testing  is 
challenging  due to the lack of  test oracle s and effective  test input 
generation  techniques . Some  ideas have been proposed to allev i-
ate such challenges.  For example,  Jiang  et al. used performance 
baselines  extracted  from historical test runs  as tentative  oracle s for 
new test runs [26]. Grechanik et al. learn ed rules  from existing 
test runs,  e.g., what input s have  led to intensive computation s. 
They  used such rules to select  new test input s to expose  perfo r-
mance issue s [21]. These ideas work  well for PC applica tions, but  
it is un clear whether they are effective for  smartphone applic a-
tions. Our empirical study  discloses that  many  performance bugs  
in smartphone ap plications need certain  user interaction sequences 
to manifest . Besides , smartphone  applications  also have  some  
unique features , e.g., long GUI lagging can  force an  Android  ap-
plication to close.  Such requirements and features  should be co n-
sidered in order to design effective techniques to test the  perfo r-
mance of smartphone applications . We have observed  initial a t-
tempts  along this direction. For example,  Yang et al.  tried to  crash  
an Android application by  adding a long delay  after each heavy 
API call to test GUI lagging issue s [53]. Jensen et al. studied how 
to generat e user interaction sequence s to reach certain targets  in 
an Android application [25]. These  attempts  support  performance 
testing  of Android applications , but how to assess the testing ade-
quacy is still unclear.  Our work  thus motivates new attempts for  
performa nce testing  adequacy criteria , as well as effec tive tec h-
niques to  expos e performance issues  in smartphone applications.  
Debugging  and optimization  for application performance . 
Existing work on debugging and optimization for smartphone 
application performance mainly fall s into two categories. The first 
category estimates perfor mance for  smartphone applications  to aid 
debugging  and optimization  tasks  [23][29][42][57]. For example, 
Mantis  [29] estimated  the executio n time for  Android applications  
on given inputs.  This helps  identify prob lem-inducing  inputs  that 
can slow down  an application,  so that developers can conduct 
optimization  accordingly . PowerTutor [57], Eprof [42] and eLens 
[23] estimate d energy con sumption for  Android applica tions by 
differ ent energy models. They can  help debug energy leak  issue s. 
For example, eLens offered  fine-grained energy consumption 
estimation  at source  code level  (e.g., method and line level est i-
mation)  to help  locate energy bottle necks . The second category  of 
existing work  uses profiling to log performance -related info r-
mation to aid debugging and optimization  tasks  [43][44][56]. For 
example, ARO  [43] monitored  cross -layer interaction s (e.g., those 
between the applica tion layer and  the resource management layer)  
to help disclose  inefficient resource usage, which  commonly 
cause s performance degradation  to smartphone application s. Ap-
pInsight [44] instru mented  application binaries to identify  critical 
paths (e.g., slow  execution  paths) in handling user interaction  
requests , so as to disclose  root causes for performance issue s. 
Panappti con [56] share d the same goal as AppInsight, and  further  
reveal ed perfor mance issue s from  inefficient platform code or 
problematic application interac tions . There  are also performance 
debugging  techniques  [28][45] for PC applications . For example,  
LagHunter  [28] detected  user-perceivable latencies  in interactive 
applica tions (e.g., Eclipse) ; Shen et al.  construct ed a system -wide  I/O throughput model  to guide  performance  debugging  [45]. 
These techniques may not  apply  to multi -threaded  and asynchr o-
nous smartphone applications, because LagHunter tracked only  
synchronous UI e vent handling  and Shen et al.’s work  required a  
system-level performance model , which may not be available . 
Understanding p erformance  bugs. Finally, u nderstanding and 
learning characteristics of  performance bugs is  a very important 
step toward designing effective techniques to test and debug pe r-
formance issue s. Existing  characteristic  studies  have mainly  fo-
cused on PC or server -side applications [27][36][54]. For exa m-
ple, Zaman et al. [54] studied performance bug reports from Fir e-
fox and Chrome (for PCs) , and gave recommendations on how to  
better conduct  bug identification, track ing and fixing.  Jin et al.  
[27] studied  the root causes  of performance bugs  in several  selec t-
ed PC or server -side applications , and identi fied efficiency rules 
for their detection. The most recent work by Nistor  et al.  [36] 
studied  lifecycle s of performance bugs ( e.g., bug discovery, r e-
porting and fixing), and obtained s ome interesting findings. For 
instance, there is little evidence showing that fixing perfor mance 
bugs has a high  chance of introducing new  bugs. This  encourages 
developers to  conduct performance optimization  when ever possi-
ble. How ever, there  is a lack of similar studies on  performance 
bugs in smartphone applications. Our work fills  this gap by stud y-
ing 70 real-world  performance bugs from  large -scale and popu lar-
ly-downloaded  Android applications. Our study  also reveals some 
interesting  findings, which differ fr om those for performance bugs 
in PC or server -side applications. These findings can help r e-
searchers and practitioners to  better understand  performance bugs 
in smartphone applications , as well as proposing new  techniques 
to fight with these bugs  (e.g.,  as we did  in our  case study ). 
6. CONCLUSION AND FUTURE WORK  
In this paper, we conducted  an empirical study of 70  performance 
bugs from real-world Android applications.  We reported our study 
results, which revealed  some  unique features of performance bugs 
in smar tphone applications . We also identified some  common bug 
patterns , which can support related research on bug detection, 
performance testing and debugging . To validate the usefulness of 
our empirical  findings, we implemented a static code analyzer , 
PerfChecker , to detect two of our identified  performance  bug 
patterns. We applied  it to 29 real -world Android applications . It 
detected  126 matching instances of the two bug patterns , and 68 
of them  were quickly confirmed  by developers as previously -
unknow n performance  issues . Besides , developers also fixed 20 of 
the confirmed issues  accordingly . This encouraging ly confirmed  
our empirical findings’ and PerfChecker’s usefulness in detecting 
performance bugs for  smartphone applications.  
In future, w e plan to  conduct more investigations of  performance 
bugs in smar tphone applications , aiming to  identify more bug 
patterns  and build  bug taxonomies . We a lso plan  to design  effec-
tive techniques to de tect these bugs and help developers conduct 
performance optimization  in an easier way . We hope  that our 
work together with related work can help  improve the perfo r-
mance and user experience for smartphone applications , which  
will benefit millions of smartphone users.  
7. ACKNOWLEDGMENTS  
This work was partially funded by Research Grants Council 
(General Research Fund 611912) of Hong Kong, and  by National 
High -tech R&D 863 Program (2012AA011205) and National 
Natural Science Foundation (61100038, 91318301, 61321491, 
61361120097) of China. Chang Xu was also partially supporte d 
by Program for New Century Excellent Talents in University, 
China (NCET -10-0486).  10228. REFERENCES  
[1] “Android processes and threads  documentation .” URL: 
http://developer.and roid.com/guide/components/processes -
and-threads.html . 
[2] “Android programming tips for performance.” URL: 
http://developer.android.com/training/articles/perf -tips.html . 
[3] “Android lis t view programming  guidelines.” URL: 
http://developer.android.com/training/improving -layouts/  
smooth -scrolling.html . 
[4] “Android Wifi Tether repository.” URL: 
https://code.google.com/p/android -wifi-tether/ . 
[5] “AnySoftKeyboard repository.” URL: https://github.com/  
AnySoftKeyboard/AnySoftKe yboard . 
[6] “APG repository.” URL: https://code.google.com/p/android -
privacy -guard/ . 
[7] “Bitcoin Wallet repository.” URL: https://code.googl e.com / 
p/bitcoin -wallet/ . 
[8] “c:geo repository.” URL: https://github.com/cgeo/cgeo . 
[9] “Chrome testing tools.” URL: https://sites.google.com/a/  
chromium.org/dev/developers/testing . 
[10] “ConnectBot repository.” URL: https://code.google.com/p/  
connectbot/ . 
[11] “Crawler4j website.” URL: https://code.google.com/p/  
crawler4j/ . 
[12] Ellis, P. D. 2010. The essential guide to effect sizes: Statist i-
cal power, meta -analysis, and the interpretation of research 
results. Cambridge University Press . 
[13] “FBReaderJ repository.” URL: https://github.com/geometer/  
FBReaderJ . 
[14] “Firefox built -in profiler  for performance analysis.” URL: 
https://developer.mozilla.org/en -US/docs/Perform ance. 
[15] “Firefox frame rate meter  tool webpage .” URL: 
http://blog.mozilla.org/devtools/tag/framerate -monitor/ . 
[16] “Geohash Droid repository.” URL: https://code.google.com/  
p/geohashdroid/ . 
[17] “GitHub website.” URL: https://github.com/ . 
[18] “Google Code  website .” URL: https://cod e.google.com/ . 
[19] “Google Play store.” URL: https://play.google.com/store . 
[20] “Google Play Wiki Page.” URL: http://en.wikipedia.org/  
wiki/Google_Play . 
[21] Grechani k, M., Fu, C., and Xie, Q. 2012. Automatically fin d-
ing performance problems with feedback -directed learning 
software testing. In Proc . 34th Int’l Conf . Soft. Engr . ICSE 
'12. 156-166. 
[22] Guo, C., Zhang, J., Yan, J., Zhang, Z., and Zhang, Y. 2013. 
Characterizing and detecting resource leaks in Android a p-
plications. In Proc. ACM/IEEE Int’l Conf. Automated Soft. 
Engr . ASE '13, 389 -398. 
[23] Hao, S., Li, D., Halfond, W.G.J., and Govindan, R. 2013. 
Estimating mobile application energy consumption using 
program analysis.  In Proc . 35th Int’l Conf . Soft.  Engr. ICSE 
'13. 92-101. 
[24] “IMSDroid repository.” URL: https://code.google.com/  
p/imsdroid/ . [25] Jensen , C. S., Prasad , M. R., and Møller , A. 2013. Automa t-
ed testing with targeted event sequence generation.  In Proc . 
Int’l Symp . Software Testing and Analysis . ISSTA '13. 67-77. 
[26] Jiang , Z. M., Hassan, A.  E., Hamann, G., and Flora, P. 2009. 
Automated perf ormance analysis of load tests. In Proc.  Int’l 
Conf . Software Maintenance . ICSM '09. 125-134. 
[27] Jin, G., Song,  L., Shi, X., Scherpelz,  J., and Lu  S. 2012. U n-
derstanding and detecting real -world performance bugs. 
In Proc . ACM SIGPLAN Conf. Programming Language D e-
sign and Implementation .  PLDI '12. 77-88. 
[28] Jovic , M., Adamoli , A., and Hauswirth , M. 2011. Catch me if 
you can: performance bug detection in the wil d. In Proc . 
ACM Int’l Conf . Object -Oriented Programming Systems 
Languages and Applications . OOPSLA '11. 155 -170. 
[29] Kwon,  Y., Lee, S., Yi, H., Kwon,  D., Yang,  S., Chun,  B., 
Huang,  L., Maniatis,  P., Naik,  M., and Paek , Y. 2013.  Man-
tis: automatic performance prediction for smartphone appli-
cations.  In Proc . USENIX Annual Tech . Conf . USENIX '13. 
297-308. 
[30] Liu, Y., Xu, C., and Cheung, S.  C. 2013. Where has my ba t-
tery gon e? Finding sensor related energy black holes in 
smartphone applications.  In Proc . IEEE Int ’l Conf . Pervasive 
Computing and Communications.  PERCOM '13. 2 -10. 
[31] Mann , H. B.,  and Whitney,  D. R.  1947.  On a test of whether 
one of two random variables is stochastically larger than the 
other . The annals of mathematical statistics , vol. 18 , 50-60. 
[32] Memon , A. M. , Soffa,  M. L.,  and Pollack , M. E.  2001. Co v-
erage criteria for GUI testing. In  Proc. 9th ACM SIGSO FT 
International Symposium on Foundations of Software Eng i-
neering .  FSE '01. 256 -267. 
[33] “Mozilla Cross -References.” URL: https://mxr.mozilla.org/ . 
[34] “My Tracks repository.” URL: https://code.google.com/  
p/mytracks/ . 
[35] Nistor, A., Song, L., Marinov, D., and Lu, S. Toddler: detec t-
ing performance problems vi a memory -access patterns. 2013. 
In Proc . Int’l Conf . Soft. Engr. ICSE '13. 562 -571. 
[36] Nistor, A., Jiang, T., and Tan, L. 2013. Discovering, repor t-
ing, and fixing performance bugs.  In Proc . 10th Working 
Conf. Mining Software Repositories . MSR '13. 237-246. 
[37] “OI File Manager repository.” URL: https://github.com/  
openint ents/filemanager . 
[38] “Omnidroid repository.” URL: https://code.google.com/  
p/omnidroid/ . 
[39] “Open GPS Tracker repository.” URL: 
https://code.google.co m/p/open -gpstracker/ . 
[40] “Osmand repository.” URL: https://code.google.com/  
p/osmand/ . 
[41] Pathak,  A., Jindal,  A., Hu, Y. C., and Midkiff,  S. P. 2012. 
What is keeping my phone awake? Characterizing and d e-
tecting no-sleep  energy bugs in smartphone apps . In Proc . 
10th In t’l Conf . Mobile Systems, Ap plication s, and Services . 
MobiSys '12. 26 7-280. 
[42] Pathak, A., Hu, Y.  C., and Zhang, M. 2012. Where is the 
energy spent inside my app?  Fine grained energy accounting 
on smartphones with Eprof. In Proc . 7th ACM European 
Conference on Computer Systems . EuroSys '12.  29-42. 1023[43] Qian,  F., Wang,  Z., Gerber,  A., Mao,  Z., Sen, S., and 
Spatscheck , O. 2011. Profiling  resource usage for mobile a p-
plicatio ns: a cross -layer approach. In Proc . 9th Int’l Conf. 
Mobile Systems, App’s, and Services . MobiSys '11 . 321-334. 
[44] Ravindranath,  L., Padhye,  J., Agarwal,  S., Mahajan,  R., 
Obermiller,  I., and Shayandeh , S. 2012. AppInsight: mobile 
app performance monitoring in  the wild. In Proc . 10th US E-
NIX Conf. Operating Systems Design and Implementation . 
OSDI  '12. 107-120. 
[45] Shen , K., Zhong,  M., and Li, C.  2005. I/O system perfo r-
mance debugging using model -driven anomaly characteriz a-
tion. In Proc . 4th USENIX Conf . File and Storage Tech . 
FAST  '05. 309-322. 
[46] “SourceForge website.” URL: http://sourceforge.net/ . 
[47] “Soot: a Java  Program  Optimization Framework.” URL: 
http://www.sable.mc gill.ca/soot/ . 
[48] “Ushahidi repository.” URL: https://github.com/ushahidi/  
Ushahidi_Android/ . 
[49] “WebSMS repository.” URL: https://code.google.com/  
p/websmsdroid/ . 
[50] “XBMC Remote repository.” URL: 
https://code.google.com/p/android -xbmcremote/ . 
[51] Xiao , X., Han,  S., Zhang,  D., and Xie , T. 2013. Context -
sensitive delta inference for identifying workload -dependent 
performance bottlenecks. In Proc . Int’l Symp . Software Tes t-
ing and Analysis . ISSTA '13. 90 -100. [52] Xu, G., Mitchell, N., Arnold, M., Rountev, A., Schonberg, 
E., and Sevitsky, G. 2010 . Finding low -utility data structures. 
In Proc . ACM SIGPLAN Conf. Programming Language D e-
sign and Implementation . PLDI '10 . 174-186. 
[53] Yang, S., Yan, D., and Routev, A. 2013. Testing for poor 
responsiveness in Android applications. In Proc . Interna-
tional  Workshop on the  Engineering  of Mobile -Enabled Sy s-
tems. MOBS '2013.  10-20. 
[54] Zaman, S., Adams, B. , and Hassan, A.  E. 2012.  A qualitative 
study on performance bugs . In Proc . Working Conference on 
Mining Software Repository . MSR '12.  199-208. 
[55] Zhang, L., Gordon , M. S., Dick, R. P., Mao, Z., Dinda, P. A., 
and Yang, L.  2012.  ADEL: an automated detector of energy 
leaks for smartphone applications. In Proc . 10th Internatio n-
al Conference on Hardware/Software Codesign and System 
Synthesis . CODES+ISSS '12.  363-372. 
[56] Zhang, L., Bild, D.  R., Dick, R.  P., Mao, Z.  M., and Dinda, 
P. 2013. Panappticon: event -based tracing to measure mobile 
application and platform performance. In Proc.  11th Intern a-
tional Conference on Hardware/Software Codesign and Sy s-
tem Synthesis . CODES+ISSS  '13. 1-10. 
[57] Zhang, L., Tiwana, B., Qian, Z., Wang, Z., Dick, R., Mao, 
Z., and Yang, L. 2010. Accurate online power estimation and 
automatic battery behavior based power model generation for 
smartphones. In Proc. the 8th  Int’l Conf . Hardware/Software 
Codesign and System Synthesis . CODES+ISSS '1 0. 105-114.
 1024