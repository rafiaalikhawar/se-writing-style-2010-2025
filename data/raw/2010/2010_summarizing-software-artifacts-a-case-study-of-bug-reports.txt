See discussions, st ats, and author pr ofiles f or this public ation at : https://www .researchgate.ne t/public ation/221555186
Summarizing software artifacts: A case study of bug reports
Conf erence Paper  · May 2010
DOI: 10.1145/1806799.1806872  · Sour ce: DBLP
CITATIONS
202READS
1,712
3 author s, including:
Sarah R astk ar
Univ ersity of British Columbia
6 PUBLICA TIONS    316 CITATIONS    
SEE PROFILE
Gail Murphy
Univ ersity of British Columbia
197 PUBLICA TIONS    14,800  CITATIONS    
SEE PROFILE
All c ontent f ollo wing this p age was uplo aded b y Gail Murphy  on 05 June 2014.
The user has r equest ed enhanc ement of the do wnlo aded file.Summarizing Software Artifacts:
A Case Study of Bug Reports
Sarah Rastkar, Gail C. Murphy and Gabriel Murray
Department of Computer Science
University of British Columbia
{rastkar,murphy,gabrielm}@cs.ubc.ca
ABSTRACT
Many software artifacts are created, maintained and evolved
as part of a software development project. As software devel-opers work on a project, they interact with existing projectartifacts, performing such activities as reading previously
ﬁled bug reports in search of duplicate reports. These activ-
ities often require a developer to peruse a substantial amountof text. In this paper, we investigate whether it is possible tosummarize software artifacts automatically and eﬀectivelyso that developers could consult smaller summaries instead
of entire artifacts. To provide focus to our investigation,
we consider the generation of summaries for bug reports.We found that existing conversation-based generators canproduce better results than random generators and that agenerator trained speciﬁcally on bug reports can perform
statistically better than existing conversation-based genera-
tors. We demonstrate that humans also ﬁnd these generated
summaries reasonable indicating that summaries might beused eﬀectively for many tasks.
Categories and Subject Descriptors
D.2.8 [ Software Engineering ]
General Terms
Experimentation
Keywords
Machine Learning, Human-centric Software Engineering
1. INTRODUCTION
Individuals outside the profession of software development
sometimes incorrectly believe that the profession is all aboutprogramming. Those involved in software development knowthat the profession has a strong component of informationmanagement. Any successful large and complex software
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted w ithout fee provided that copies are
not made or distributed for proﬁt or co mmercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, torepublish, to post on servers or to redist ribute to lists, requires prior speciﬁc
permission and/or a fee.ICSE ’10, May 2-8 2010, Cape Town, South Africa
Copyright 2010 ACM 978-1-60558-719-6/10/ 05 ...$10.00.system requires the creation and management of many ar-
tifacts: requirements, designs, bug reports, and source code
with embedded documentation to name just a few.
To perform work on the system, a software developer must
often read and understand artifacts associated with the sys-tem development. For example, a developer attempting to
ﬁx a performance bug on a system may be told that a similar
bug was solved six months ago. Finding the bug report thatcaptured the knowledge about what was ﬁxed will likely re-quire the developer to perform searches and read several bugreports in search of the report of interest. Each report readmay contain several sentences of description as well as tens
of sentences representing discussion amongst team members.
For example, bug #491925 from the Mozilla system
1com-
prises 91 sentences: 7 in the description and 84 sentencesfrom 24 comments. If solving the bug requires an under-
standing of the Java library WeakHashMap class, another 21
sentences from documentation about that class will need to
be read. Wading through text comprising system artifactsto determine which ones matter can be time consuming andfrustrating for the developer. Sometimes, the amount of
information may be overwhelming, causing searches to be
abandoned and duplicate or non-optimized work to be per-
formed, all because the previous history of the project has
been ignored.
One way to reduce the time a developer spends getting
to the right artifacts to perform their work is to provide asummary of each artifact. An accurate summary can enable
a developer to reduce the time spent perusing artifacts that
have been returned from search es, found through browsing
or recommended by team members or tools. Perhaps opti-
mally, the authors of system artifacts would write a suitableabstract to help other developers working on the system.Given the evolving nature of artifacts and the limited timeavailable to developers, this optimal path is not likely to
occur.
Alternatively, it might be possible to generate summaries
of project artifacts, saving developers eﬀort and enabling
up-to-date summaries on-demand. In this paper, we inves-tigate the possibility of automatic summary generation, fo-
cusing on one kind of project artifact, bug reports, to make
the investigation tractable. We chose to focus on these re-
ports as there are a number of cases in which developers
may make use of existing bug reports, such as when triagingbugs (e.g., [3]) or when performing change tasks (e.g., [18]),and these reports can often be lengthy, involving discussions
amongst multiple team members.
1mozilla.org, veriﬁed 05/09/05
PREPRESS PROOF FILE CAUSAL PRODUCTIONS 1The problem of summarizing discussions amongst multi-
ple people is similar to the problem of summarizing email
and meeting discussions (Section 2). We build on existingwork in this area, investigating whether existing machine-learning based approaches (classiﬁers) for generating extrac-tive summaries can produce accu rate summaries for bug re-
ports. These approaches assign a zero or one value to each
sentence in the bug report based on features of the sentence;
sentences assigned a one value appear in a generated sum-mary.
To enable this investigation, we had human annotators
create summaries for 36 bug reports, creating a corpus of
bug report data (Section 3). These bug reports included
both reports about defects and reports about enhancements.
We then applied existing classiﬁers trained on email, and
email and meeting data to produce summaries for reports
in the bug report corpus. We also trained a classiﬁer spe-ciﬁc to bug reports and applied it to the bug report corpus(Section 4).Our bug report corpus serves as the training set
for the bug report classiﬁer and the testing set for all three
classiﬁers.
We measured the eﬀectiveness of these classiﬁers based
on several computed measures (Section 5). We found that
all three classiﬁers perform well above the chance level (arandom classiﬁer). We also found that the bug report clas-
siﬁer, having a precision of more than 62%, out-performs the
other two classiﬁers in generating summaries of bug reports.
To evaluate whether a measure of 62% produces summaries
useful for developers, we had human judges evaluate the
goodness of a subset of the summaries produced by the bug
report classiﬁer. On a scale of 1 (low) to 5 (high), the arith-
metic mean quality ranking of the generated summaries by
the human judges is 3.69 ( ±1.17). This average rating sug-
gests that the generated summaries can provide a suﬃciently
accurate summary for developers, saving them the need inall cases to read lengthy bug reports.
This paper makes three contributions.
•It demonstrates that it is possible to generate accurate
summaries for one kind of project artifact, bug reports.
•It reports on the creation of a corpus of 36 humanannotated bug report data from four diﬀerent systems.
•It demonstrates that while existing classiﬁers trained
for other conversation-based genres can work reason-
ably well, a classiﬁer trained speciﬁcally for bug re-ports scores the highest on standard measures.
2. RELATED WORK
With the growing number of electronically available infor-
mation, there is substantial interest and a substantial bodyof work in the automated generation of summaries for thatinformation. Two basic approaches have been taken to gen-erating summaries: extractive and abstractive [10]. An ex-
tractive approach selects a subset of existing sentences to
form the summary. An abstractive approach builds an in-
ternal semantic representation of the text and then applies
natural-language processing techniques to create a summary.As the current state-of-the-art in abstractive techniques hasnot yet supported meaningful application, we focus in thispaper on extractive techniques.
Extractive-based summarization approaches have been ap-
plied to many kinds of conversations, including meetings [21],telephone conversations [22] and emails [16]. Some of these
approaches have used domain-speciﬁc characteristics, such
as email header information [19]. Murray and Carenini [13]developed a summarizer for conversations in various modal-ities that uses features inherent to all multi-party conversa-tions. They applied this system to meetings and emails andfound that the general conversation system was competi-
tive with state-of-the-art domain-speciﬁc systems in both
cases. In this work, we investigate whether we can use thisgeneral conversation system to generate accurate (or good)summaries of bug reports.
While the format of bug reports vary depending upon the
system being used to store the reports, much of the informa-
tion in a bug report resembles a conversation. Beyond the
ﬁxed ﬁelds with pre-deﬁned values, such as the status ﬁeldthat records whether the bug is open or closed or some otherstate, a bug report usually involves free-form text, includ-
ing a title or summary, a description and a series of time-stamped comments that capture a conversation between de-
velopers (and sometimes users) related to the bug. In one
common system, the Bugzilla bug reporting system
2, the de-
scription and comments may be written but not edited, fur-ther adding to the conversation nature of a report. This lack
of editable text also means the descriptions do not serve as
summaries of the current report contents. Figure 1 displays
part of a bug report from the KDE Bugzilla bug repository
3
with 21 comments from 6 people; the description and each
comment can be considered a turn in a conversation.
As bug repositories contain substantial knowledge about
a software development, there has been substantial recent
interest in improving the use and management of this infor-
mation, particularly as many repositories experience a highrate of change in the information stored [1]. For instance,Anvik and colleagues [2] have shown how to provide recom-menders for whom to assign a report whereas Runeson andcolleagues [17] and Wang and colleagues [20] have demon-
strated how to detect duplicate reports. While some of this
work has applied classiﬁers to the problems they intend to
solve, none has attempted to extract a meaningful summaryfor developers.
Other eﬀorts have considered how to improve the content
of bug reports. Ko and colleagues analyzed the titles of bug
reports to inform the development of tools for both reporting
and analyzing bugs [12]. Bettenburg and colleagues surveyeda large number of open-source developers to determine whatfactors constitute a good bug report and developed a toolto assess bug report quality [4]. Developers in their studynoted that bug reports“are often used to debate the relative
importance of various issues”; it is this conversational nature
of reports that makes summaries valuable and that we hope
to exploit to produce the summaries.
3. BUG REPORT CORPUS
To be able to train, and judge the eﬀectiveness, of an
extractive summarizer on bug reports, we need a corpus of
bug reports with good summaries. Optimally, we would have
available such a corpus in which the summaries were createdby those involved with the bug report, as the knowledge ofthese individuals in the system and the bug should be thebest available. Unfortunately, such a corpus is not available
2www.bugzilla.org , veriﬁed 05/09/09
3www.kde.org , veriﬁed 05/09/05
2Bug 188311 - The applet panel should not overlap applets
Description  From mangus 2009-03-28 11:35:10
Version:           svn (using Devel) 
OS:                Linux 
Installed from:    Compiled sources 
In amarok2-svn I like the the new contextview , but I found the  
new bottom bar for managing applets annoying , as it covers parts 
of other applets sometimes , like lyrics one , so that you miss a 
part of it. Could be handy to have it appear and desappear  
onmouseover. 
thanks 
------- Comment #1 From Dan 2009-03-28 14:53:55  ------- 
The real solution is to make it not cover applets, not make it  appear/disappear on mouse over. 
------- Comment #2 From Leo 2009-03-29 14:34:53  ------- 
i dont understand your point, dan... how do we make it not cover  
applets? 
------- Comment #3 From Dan  2009-03-29 16:32:22  ------- 
Thats your problem to solve :) 
The toolbar should be like the panel in kde, it gets it's own area  
to draw in (a strut in window manager terms).  The applets should  
not consider the space the toolbar takes up to be theirs to play in,   
but rather end at the top of it. Product : amarok 
Component : ContextView 
Status : RESOLVED
Resolution : FIXED 
Target : --- Version : unspecified 
Priority : NOR 
Severity : wishlist 
Votes: 0
Figure 1: An example of the conversational struc-
ture of a bug report (The beginning part of bug
188311 from the KDE bug repository).
as developers do not spend time writing summaries once a
bug is complete, despite the fact that the bug report maybe read and referred to in the future.
To provide a suitable corpus, we recruited ten graduate
students from the Department of Computer Science at theUniversity of British Columbia to annotate a collection of
bug reports. On average, the annotators had seven years of
programming experience. Half of the annotators had experi-
ence programming in industry and four had some experience
working with bug reports.
3.1 Annotation Process
We had each individual annotate a subset of bugs from
four diﬀerent open-source software projects: Eclipse Plat-form,
4,G n o m e ,5, Mozilla and KDE. We chose a diverse
set of systems because our goal is to develop a summariza-tion framework that can produce accurate results for a widerange of bug reposito ries, not just bug reports speciﬁc to a
single project. Although the annotators did not have experi-ence with these speciﬁc systems, we believe their experiencein programming allowed them to extract the gist of the dis-cussions; no annotator reported being unable to understandthe content of the bug reports. The annotators were com-pensated for their work.
The 36 bugs reports (nine from each project) chosen for
annotation have mostly conversational content. We avoided
selecting bug reports consisting mostly of long stack traces
4www.eclipse.org , veriﬁed 04/09/09
5www.gnome.org , veriﬁed 04/09/09 0 2 4 6 8 10 12 14 16
5-9 10-14 15-19 20-25Number of Bug Reports
Number of Comments
Figure 2: The distribution of bug reports in the cor-
pus with regard to the number of comments.
and large chunks of code as this content may be used but is
not typically read by developers. The reports chosen varied
in length: 25 reports (69%) had between ﬁve and fourteencomments; the remaining eleven bugs (31%) had 15 to 25
comments each. Figure 2 shows the distribution of bug re-
ports based on the number of comments. Nine of the 36bug reports (25%) were enhancements to the target system;
the other 27 (75%) were defects. There are a total of 2361
sentences in these 36 bug reports. This corpus size is similarto that used in training an email classiﬁer; the email corpuscontains 39 email threads and 1400 sentences [13].
Each annotator was assigned a set of bug reports from
those chosen from the four systems. For each bug report,
we asked the annotator to write an abstractive summary ofthe report using their own sentences that was a maximum
of 250 words. We limited the length of the abstractive sum-
mary to motivate the annotator to abstract the given report.The annotator was then asked to specify how each sentence
in the abstractive summary maps (links) to one or more sen-
tences from the original bug report by listing the numbers
of mapped sentences from the original report.
To aid the annotators with this process, the annotators
used a version of BC3 web-based annotation software
6that
made it easier for them to manipulate the sentences of the
bug report. Figure 3 shows an example of part of an anno-
tated bug report; the summary at the top is an abstractivesummary written by an annotator with the mapping to thesentences from the original bug report marked.
The bug report corpus is publicly available.
7
3.2 Annotated Bugs
On average, the bug reports being summarized comprised
65 sentences. On average, the abstractive summaries created
by the annotators comprised just over ﬁve sentences with
each sentence in the abstractive summaries linked (on aver-
6www.cs.ubc.ca/nest/lci/bc3/framework.html , veriﬁed
04/09/09
7See www.cs.ubc.ca/labs/spl/projects/summarization.
html. The corpus contains additional annotations, including
an extractive summary for each bug report and labeling ofthe sentences.
3Summary: KDE - The applet panel should not overlap applets
Summary
Description ∇ (Collapse all )  
    From mangus
    1.1 version: svn (using Devel) 
    1.2 OS: Linux 
    1.3 Installed from: Compiled sources     1.4 In amarok2-svn I like the the new contextview , but I found the new bottom bar for managing applets annoying ,  
     as it covers parts of ot her applets sometimes, like lyrics one , so that you miss a part of it.  
    1.5 Could be handy to have it app ear and desappear onmouseover. 
    1.6 thanks 
Comment 1 ∇ 
    From Dan
    2.1 The real solution is to make it not cover appl ets, not make it apper/disapeer on mouse over. 
Comment 2 ∇ 
In the anarok2-svn contextview the bottom bar sometimes obscure applet content.
[1.4,11.1,11.2,11.3] Applets should not be larger than the viewable area, and should be 
given an appropriate sizehint.[11.2] This bug was fixed in 2.1.1[20.1]
Figure 3: A screenshot of the annotation software. The bug report has been broken down into labeled
sentences. The annotator enters the abstractive summary in the text box. The numbers in the brackets aresentence labels and serve as links between the abstractive summary and the bug report. For example, theﬁrst sentence of the abstractive summary has links to sentences 1.4, 11.1, 11.2, 11.3 form the bug report.
Table 1: Abstractive summaries generated by anno-
tators.
mean
 stdv
#sentences in the summary
 5.36
 2.43
#words in the summary
 99.2
 39.93
#linked sentences from the bug report
 16.14
 9.73
age) to three sentences in the original bug report. Table 1provides some overall statistics on the summaries producedby the annotators.
A common problem of annotation is that annotators often
do not agree on the same summary. This reﬂects the fact
that the summarization is a subjective process and there is
no single best summary for a document—a bug report in thispaper. To mitigate this problem, we assigned three annota-tors to each bug report. We use the kappa test to measure
the level of agreement between the annotators [9]. The result
of the kappa test (k value) is 0.41 for our bug report anno-tations, showing a moderate level of agreement. We askedeach annotator, at the end of annotating each bug report,
to complete a questionnaire about properties of the report.
The annotators, in the answers to the questionnaires, rated
(with 1 low and 5 high):
•the level of diﬃculty of summarizing the bug report tobe 2.68 ( ±0.86),
•the amount of irrelevant and oﬀ-topic discussion in thebug report to be 2.11 ( ±0.66), and•the level of project-speciﬁc terminology used in the bug
report to be 2.68 (± 0.83).
4. SUMMARIZING BUG REPORTS
The bug report corpus provides us a basis on which to
experiment with producing bug report summaries automat-ically. We set out to investigate two questions:
1. Can we produce good summaries with existing conver-
sation-based classiﬁers?
2. How much better can we do with a classiﬁer speciﬁcally
trained on bug reports?
The existing conversation-based classiﬁers we chose to in-
vestigate are trained on conversational data other than bug
reports. The ﬁrst classiﬁer, which we refer to as EC,w a s
trained on email threads [13]. We chose this classiﬁer asbug report conversations share similarity with email threads,such as being multi-party and having thread items added at
diﬀering intervals of time. This classiﬁer was trained on
a subset of the publicly available Enron email corpus [11],which consists of 39 annotated email threads (1400 sentences
in total).
The second classiﬁer, which we refer to as EMC ,w a s
trained on a combination of email threads and meetings [13].
We chose this classiﬁer because some of the characteristicsof bug reports might be more similar to meetings, such ashaving concluding comments at the end of the conversation.
The meetings part of the training set for EMC is a sub-
4set of the publicly available AMI meeting corpus [6], which
includes 196 meetings.
The ECand EMC classiﬁers are appealing to use be-
cause of their generality. If these classiﬁers work well for
bug reports, it oﬀers hope that general classiﬁers might beapplicable to software project artifacts without training oneach speciﬁc kind of software artifacts (which can vary be-
tween projects) or on project-speciﬁc artifacts, lowering the
cost of producing summaries.
However, unless these classiﬁers produce perfect summaries,
the question of how good of a summary can be producedfor bug reports remains open unless we consider a classiﬁertrained on bug reports. Thus, we also chose to train a third
classiﬁer, BRC, using the bug report corpus we created.
To form the training set for BRC, we combined the three
human annotations for each bug report by scoring each sen-
tence of a report based on the number of times it has beenlinked by annotators. For each sentence, the score is be-tween zero, when it has not been linked by any annotator,
and three, when all three annotators have a link to the sen-
tence in their abstractive summary. A sentence is consideredto be part of the extractive summary if it has a score of twoor more. For each bug report, the set of sentences with ascore of two or more (a positive sentence) is called the gold
standard summary . For the bug report corpus, gold stan-
dard summaries include 465 sentences, which is 19.7% of allthe sentences in the corpus, and 28.3% of all words in the
corpus.
As we have only the bug report corpus available for both
training and testing the bug report classiﬁer, we use a cross-
validation technique when evaluating this classiﬁer. Specif-
ically, we use a leave-one-out procedure so that the classi-ﬁer used to create a summary for a particular bug report istrained on the remainder of the bug report corpus.
All three classiﬁers investigated are logistic regression clas-
siﬁers. Instead of generating an output of zero or one, these
classiﬁers generate the probability of each sentence being
part of an extractive summary. To form the summary, we
sort the sentences into a list based on their probability val-ues in descending order. Starting from the beginning of thislist, we select sentences until we reach 25% of the bug re-port word count. The selected sentences form the generated
extractive summary. We chose to target summaries of 25%
of the bug report word count b ecause this value is close
to the word count percentage of gold standard summaries
(28.3%). All three classiﬁers were implemented using theLibliner toolkit.
8
4.1 Conversation Features
The classiﬁer framework used to implement EM,EMC
and BRC can learn based on 24 diﬀerent features. We hy-
pothesize that the features useful for the EMand EMC
classiﬁers are also relevant to the summarization of bug re-
ports since these reports exhibit a conversational structure.
The features are based on representing a bug report as a con-
versation comprised of turns between multiple participants.
The 24 features can be categorized into four major groups.
•Structural features are related to the conversational
structure of the bug reports. Examples include theposition of the sentence in the comment and the posi-
8www.csie.ntu.edu.tw/~cjlin/liblinear/ , veriﬁed
04/09/09tion of the sentence in the bug report.
•Participant features are directly related to the con-
versation participants. For example if the sentence is
made by the same person who ﬁled the bug report.
•Length features include the length of the sentence nor-
malized by the length of the longest sentence in the
comment and also normalized by the length of thelongest sentence in the bug report.
•Lexical features are related to the occurrence of unique
words in the sentence.
The detailed description of features can be found in [13].
5. EV ALUATION
To compare the EC,EMC and BRC classiﬁers, we have
used several measures that c ompare summaries generated
by the classiﬁers to the gold standard summaries formed
from the human annotation of the bug report corpus (Sec-
tion 4). These measures assess the quality of each classiﬁer
and enable the comparison of eﬀectiveness of the diﬀerentclassiﬁers against each other.
However, these measures do not tell us whether the in-
tended end users of the summaries—software developers—
consider the generated summaries as representative of the
original bug report. To check the performance of the classi-
ﬁers from a human perspective, we also report on an evalu-ation in which we asked human judges to evaluate the good-
ness of a set of generated summaries against the original bug
reports.
5.1 Comparing Base Effectiveness
The ﬁrst comparison we consider is whether the EC,EMC
and BRC classiﬁers are producing summaries that are bet-
ter than a random classiﬁer in which a coin toss is used to
decide which sentences to include in a summary. We performthis comparison by plotting the receiver operator character-
istic (ROC) curve and then computing the area under the
curve (AUROC) [8].
For this comparison, instead of using the 25% word count
to generate extractive summaries, we investigate diﬀerent
probability thresholds. As described in Section 4, the outputof the classiﬁer for each sentence is a value between zero and
one showing the probability of the sentence being part of the
extractive summary. To plot a point of ROC curve, we ﬁrst
choose a probability threshold. Then we form the extractivesummaries by selecting all the sentences with probability
values greater than the probability threshold.
For summaries generated in this manner, we compute the
false positive rate ( FPR) and true positive rate ( TPR),
w h i c ha r et h e np l o t t e da sap o i n ti nag r a p h .F o re a c hs u m -
mary, TPR measures how many of the sentences present in
gold standard summary ( GSS) are actually chosen by the
classiﬁer.
TPR =
#sentences selected from the GSS
#sentences in GSS
FPR computes the opposite.
FPR =#sentences selected that are not in the GSS
#sentences in the bug report that are not in the GSS
5 0 0.2 0.4 0.6 0.8 1
 0  0.2  0.4  0.6  0.8  1TPRate
FPRateBRC
Random Classifier
Figure 4: ROC plot for BRC classiﬁer.
The area under a ROC curve (AUROC) is used as a mea-
sure of the quality of a classiﬁer. A random classiﬁer has
an AUROC value of 0.5, while a perfect classiﬁer has anAUROC value of 1. Therefore, to be considered eﬀective, a
classiﬁer’s AUROC value should be somewhere in between,
preferably close to 1.
Figure 4 shows the ROC curve for the BRC classiﬁer.
The diagonal line is representative of a random classiﬁer.
The area under the curve (AUROC) for BRC is equal to
0.722, indicating that this classiﬁer performs better than arandom classiﬁer. We also computed the AUROC values for
ECand EMC . The values are 0.719 and 0.689 respectively
suggesting that each of the classiﬁers has the same level of
eﬃciency compared to a random classiﬁer.
5.2 Comparing Classiﬁers
AUROC is a measure of the general eﬀectiveness of the
classiﬁers. When the summaries are generated with a pre-
deﬁned length—25% in this paper—we need other measures
to compare the classiﬁers.
To investigate whether any of EC,EMC orBRC work
better than the other two based on our desired 25% word
count summaries, we compared them using the standard
evaluation measures of precision, recall, and f-score. Wealso used pyramid precision, which is a normalized evalua-tion measure taking into account the multiple annotations
available for each bug report.
5.2.1 F-score
F-score combines the values of two other evaluation mea-
sures: precision and recall. Precision measures how often
a classiﬁer chooses a sentence from the gold standard sum-maries ( GSS) and is computed as follows.
precision =#sentences selected from the GSS
#selected sentences
Recall measures how many of the sentences present in a
gold standard summary are actually chosen by the classiﬁer.
For a bug report summary, the recall is the same as the TPR
used in plotting ROC curves (Section 5.1).
As there is always a trade-oﬀ between precision and recall,
the F-score is used as an overall measure. 0 0.2 0.4 0.6 0.8 1
 0  5  10  15  20  25  30  35Pyramid Precision
Bug#BRC
EC
EMC
Figure 5: Pyramid precision for all classiﬁers.
Fs c o r e =2·precision ·recall
precision +recall
Figure 6 shows the values of F-score for the three classiﬁers
across all the bug reports. In this plot, the bug reports
have been sorted based on the values of the F-score for thesummaries generated by BRC. This ﬁgure shows that the
best F-score typically occurs with the BRC classiﬁer.
5.2.2 Pyramid Precision
The pyramid evaluation scheme by Nenkova and Passon-
neau [15] was developed to provide a reliable assessment of
content selection quality in summarization where there are
multiple annotations available. We used the pyramid pre-
cision scheme of Carenini et. al [5] inspired by Nenkova’spyramid scheme.
For each generated summary of a given length, we count
the total number of times the sentences in the summary
were linked by annotators. Pyramid precision is computed
by dividing this number by the maximum possible total forthat summary length. For example, if an annotated bugreport has 4 sentences with 3 links and 5 sentences with
2 links, the best possible summary of length 5 has a total
number of links equal to (4 ×3)+(1 ×2) = 14. The pyramid
precision of a generated summary of length 5 with a total of8 links is therefore computed as
Pyramid Precision =8
14≈0.57
Figure 5 shows the values of pyramid precision for the
three classiﬁers across all the bug reports in the corpus. Thebug reports have been sorted based on the values of thepyramid precision for the summaries generated by BRC.
The ﬁgure shows that BRC has better precision values for
most of the bug reports.
5.2.3 Summary
Figure 5 and Figure 6 show that BRC out-performs the
other two classiﬁers for most of the bug reports. Table 2shows the values of precision, recall, F-score, and pyramidprecision for each classiﬁer averaged over all the bug reports.
To investigate whether the bug report classiﬁer ( BRC)i s
6 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
 0  5  10  15  20  25  30  35F-score
Bug#BRC
EC
EMC
Figure 6: F-score plot for all classiﬁers.
Table 2: Evaluation measures.
Classiﬁer
 Pyramid
 Precision
 Recall
 F-Score
Precision
BRC
 .63
 .57
 .35
 .4
EC
 .54
 .43
 .3
 .32
EMC
 .53
 .47
 .23
 .29
signiﬁcantly better than the other two classiﬁers ( ECand
EMC ), we performed four paired t-tests: one to see if the
pyramid precision of BRC is signiﬁcantly better that that
ofEC, one to see if the pyramid precision of BRC is sig-
niﬁcantly better that that of EMC , and so on. The results
conﬁrm that the bug report classiﬁer (BRC )o u t - p e r f o r m s
the other two classiﬁers with statistical signiﬁcance (where
signiﬁcance occurs with p <.05).
The classiﬁer trained on emails ( EC) and the classiﬁer
trained on emails and meetings ( EMC ) have similar perfor-
mance. The paired t-test conﬁrms that there is no signiﬁcant
diﬀerence.
5.3 Feature Selection Analysis
All of the three classiﬁers used in our study, EC,EMC ,
and BRC, use a set of 24 features to generate summaries of
bug reports. The values of these features for each sentence
are used to compute the probability of the sentence being
part of the summary. To see which features are informative
for generating summaries, we perform a feature selectionanalysis.
For this analysis, we compute the F-score
9value for each
of the 24 features using the approach of [7]. This score is
commonly used to compute the discriminability of features
in supervised machine learning. The score depends only onthe set of features and the training data and is independentof the classiﬁer. Features with higher F-score are the mostinformative in discriminating between important sentences,which should be included in the summary, and other sen-
tences, which need not be included in the summary.
9This score is computed for the feautures and is diﬀer-
ent from the F-score computed for the summaries in Sec-
tion 5.2.1.Table 3: Features key.
Feature ID Description
MXS max Sprob score
MNS mean Sprob score
SMS sum of Sprob scores
MXT max Tprob score
MNT mean Tprob score
SMT sum of Tprob scores
TLOC position in turn
CLOC position in conv.
SLEN word count, globally normalized
SLEN2 word count, locally normalized
TPOS1 time from beg. of conv. to turn
TPOS2 time from turn to end of conv.
DOM participant dominance in words
COS1 cos. of conv. splits, w/ Sprob
COS2 cos. of conv. splits, w/ Tprob
PENT entro. of conv. up to sentence
SENT entro. of conv. after the sentence
THISENT entropy of current sentence
PPAU time btwn. current and prior turn
SPAU time btwn. current and next turn
BEGAUTH is ﬁrst participant (0/1)
CWS rough ClueWordScore
CENT1 cos. of sentence & conv., w/ Sprob
CENT2 cos. of sentence & conv., w/ Tprob
Table 3 provides a short description of the features con-
sidered. Some descriptions in the table refer to Sprob.I n f o r -
mally, Sprob provides the probability of a word being uttered
by a particular participant based on the intuition that cer-
tain words will tend to be associated with one conversationparticipant due to interests and expertise. Other descrip-tions refer to Tprob , which is the probability of a turn given
a word, reﬂecting the intuition that certain words will tendto cluster in a small number of turns because of shifting
topics in a conversation. Full details on the features are
provided in [13].
Figure 7 shows the values of F-score computed for the
features used by the classiﬁers to summarize bug reports.
The results show that the length features (SLEN & SLEN1)
are among the most helpful. Several lexical features are also
helpful: CWS, CENT1, CENT2, SMS, & SMT.
10These
results suggest that we may b ea b l et ot r a i nm o r ee ﬃ c i e n t
classiﬁers by combining lexical and length features of the
conversation.
5.4 Human Evaluation
The generated summaries are intended for use by software
developers. Does a classiﬁer with pyramid precision of 0.63
produce summaries that are useful for developers? To in-
vestigate whether the summaries are of suﬃcient quality forhuman use, we set up an evaluation of the generated sum-maries of the BRC classiﬁer with a group of eight human
judges. We chose to focus on the BRC classiﬁer since it had
performed the best based on the earlier measures.
Eight of our ten annotators agreed to evaluate a num-
ber of machine generated summaries. We asked the eightjudges to evaluate a set of eight summaries generated by
the BRC classiﬁer. Each summary was evaluated by three
diﬀerent judges. The human judges were instructed to read
the original bug report and the summary before starting the
evaluation process. Figure 8 provides an example of a gener-ated extractive summary the judges were asked to evaluate;
10CWS measures the cohesion of the conversation by com-
paring the sentence to other turns of the conversation.
7 0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09
SLENCWSCENT1CENT2SMSSLEN2SMTCOS1COS2TLOCTHISENTDOMMXSMNTMXTCLOCMNSPPAUSPAUSENTPENTBEGAUTTPOSE1TPOSE2F-score
FeaturesBug Report Data
Figure 7: Feature F-scores for the bug report corpus.
a portion of the original bug appears earlier in Figure 1.
We asked each judge to rank, using a ﬁve-point scale withﬁve the high value, each bug report summary based on fourstatements (mean and standard deviations are provided inparentheses following the statement):
1. The important points of the bug report are represented
in the summary. (3.54 ±1.10)
2. The summary avoids redundancy. (4.00 ±1.25)
3. The summary does not contain unnecessary informa-
tion. (3.91 ±1.10)
4. The summary is coherent. (3.29 ±1.16)
An evaluation of meeting data summarized by multiple ap-
proaches uses similar statements to evaluate the goodness of
the generated summaries [14].
We ensured in this judging process that the bug reports
were assigned to judges who had not annotated the same
reports during the annotation process. We also took care to
choose summaries with diﬀerent values of pyramid precisionand F-score so as to not choose only the best examples of
generated summaries for judging.
5.5 Threats
The primary threats to the evaluations we have conducted
are the size of the bug report corpus and the annotation by
non-experts in the projects. While the size of the corpus is
suﬃcient for initial experimentation with these approaches,
we are limited in the size of the training set that can be used
to train BRC and we are unable to use a separate set of bug
reports for training and testing.
Optimally, we would have had summaries created for the
reports by experts in the projects. Summaries created byexperts might capture the meaning of the bug reports bet-
ter than was possible by non-experts. On the other hand,
summaries created by experts might rely on knowledge that
was not in the bug reports, potentially creating a standard
that would be diﬃcult for a classiﬁer to match. This riskis mitigated by the experimental setup that requires a map-ping from abstractive to extractive sentences. By assigning
three annotators to each bug report and by using agreementbetween two to form the gold standard summaries, we haveattempted to mitigate the risk of non-expert annotators.
For the human evaluation, th e primary threat is the use of
non-experts who may not be the best judges of summaries of
the reports. Another threat is not asking the human judges
about the properties of interest through multiple questions,
which may have allowed a better determination of the judges
opinions on those properties. Finally, the judges may have
wanted to please the experimenters. In future studies, we
will consider interspersing classiﬁer-generated and human-generated summaries to reduce this risk.
6. DISCUSSION
With this work, we have shown that it is possible to gen-
erate summaries for a diverse s et of bug reports with reason-
able accuracy as judged both against gold summaries and by
human judges. The ability to produce summaries opens upthe possibility of using a summary generator to support the
work of software developers. We discuss these possibilities
as well as consider ways to fu rther improve the summaries
produced.
6.1 Using a Bug Report Summary
Automatically generated bug report summaries could aid
a software developer in several ways. During bug reporttriage activities, a developer (triager) must often consultother reports. For instance, the triager must typically de-
termine if a new incoming report is a duplicate of existing
reports. As this determination can be diﬃcult for large,fast-changing repositories, recommenders have been builtto suggest to the triager a set of existing bug reports to
consider as duplicates (e.g., [17, 20]). Assessing these rec-
ommendations to determine which are true duplicates can
be time-consuming as the determination typically involves
reading much, if not all, of the report. An automatically
produced summary of each existing report could ease this
task by greatly reducing the amount of text the triager mustread, understand and consider.
As another example, previous work in our research group
developed a recommender to suggest to a developer perform-ing an evolution task on a system, change tasks (describedby bug reports) that have been previously completed which
might guide the developer [18]. As with the duplicate bug
case, the developer must again wade through a substantial
amount of text to determine which bug report might be rel-
evant. Summaries could also ease this process.
Overall, summaries can make it possible for a developer
to use the body of knowledge about the project that is cap-tured in the repository. The ability to produce summaries
automatically opens up new possibilities in making such rec-
ommenders eﬀective and practic al to use. In future work, we
plan to investigate whether the provision of such summaries
can provide these beneﬁts.
6.2 Summarizing Other Project Artifacts
Bug reports are not the only artifacts with a substantial
amount of text in a software development project. Require-
ments and design documents can contain text describing thedomain, discussions of alternative approaches and rationale
for why decisions have been made. Even source code, which
is typically not considered to contain much text, can includemany sentences about decisions or how a piece of code works.
Given our success in producing summaries of bug reports,
8SUMMARY: The applet panel should not overlap applets
In amarok2-svn I like the the new contextview , but I found the new bottom bar for
managing applets annoying , as it covers parts of other applets sometimes , likelyrics one , so that you miss a part of it.
The real solution is to make it not cover applets, not make it appear/disappear on
mouse over.
i dont understand your point, dan... how do we make it not cover applets?Applets should not be larger than the viewable area, if there’s an applet above it,
then the lower applet should get a smaller sizehint, and resize if necessary when
it’s the active applet (and therefore the only one on the screen)
The bug that is being shown here is the fact that you cannot yet resize your ap-
plets, and as such we also don’t set default sizes sanely.
Of course :) Just thought i should point out that the feature is not yet completed
- the polish that’s gone into it lately could seem like an indication of feature
completion, and as such it would seem the prudent course to inform you that that isnot the case :)
Applets should not be larger than the viewable area, if there’s an applet above it,
then the lower applet should get a smaller sizehint, and resize if necessary when
it’s the active applet (and therefore the only one on the screen)
Figure 8: An extractive summary generated by BRC for Bug 188311 from the KDE bug repository.
we plan to investigate the generation of summaries for other
text that appears in software development projects. In par-ticular, we are interested in investigating whether textual
summarization techniques can be used to help explain fea-
tures implemented in source code.
6.3 Improving a Bug Report Summarizer
The bug report classiﬁer we trained produces reasonable
summaries. Although we have trained and evaluated the
classiﬁer across reports from four systems, we do not know
whether the classiﬁer will produce good summaries for re-ports from systems on which it was not trained. We planto investigate the generality of the summarizer to other bugreporting systems.
Another possibility for improving the accuracy of a bug
report summarizer is to augment the set of features used.
For instance, comments made by people who are more active
in the project might be more important, and thus should bemore likely included in the summary.
As part of the annotation process, we also gathered in-
formation about the intent of sentences, such as whethera sentence indicated a ’problem’, ’suggestion’, ’ﬁx’, ’agree-ment’, or ’disagreement’. These labels might also be used
to diﬀerentiate the importance of a sentence for a summary.
We leave the investigation of these intentions to future re-
search as we do not yet have the ability to automatically
label the sentences for the test data.
It is also still an open question whether extractive sum-
marization is the most appropriate choice for bug reportsor whether better results could be obtained through an ab-stractive summarizer. We leav e this determination to future
work.7. SUMMARY
Researchers rely on good summaries to be available for
papers in the literature. These summaries are used for sev-
eral purposes, including providing a quick review of a topic
within the literature and selecting the most relevant papers
for a topic to peruse in greater depth. Software develop-
ers must preform similar activities, such as understandingwhat bugs have been ﬁled against a particular componentof a system and determining why particular design decisions
have been made as recorded in design documents and else-
where. However, developers must perform these activities
without the beneﬁt of summaries, leading them to either
expend substantial eﬀort to perform the activity thoroughlyor resulting in missed information.
In this paper, we have investigated the automatic genera-
tion of one kind of software artifact, bug reports, to provide
developers with the beneﬁts others experience daily in other
domains. We found that existing conversation-based extrac-tive summary generators can p roduce summaries for reports
that are better than a random classiﬁer. We also found thatan extractive summary generato r trained on bug reports pro-
duces the best results. The human judges we asked to evalu-
ate reports produced by the bug report summary generator
agree that the generated extractive summaries contain im-
portant points from the original report and are coherent.
This work opens up new possibilities to improve the eﬀec-tiveness of existing systems for recommending duplicate bugreports and for recommending similar changes completed in
the past for a current software evolution task. It also opens
up the possibility of summarizing other software project ar-tifacts to enable developers to make better use of the richknowledge base tracked for most software developments.
9Acknowledgments
Thanks to Giuseppe Carenini and Raymond Ng for useful
discussions on summarization and to Jan Ulrich for helpingwith the annotation software. This research is supported byNSERC. Thanks also to the annotators for helping to createthe bug report corpus.
8. REFERENCES
[ 1 ]J .A n v i k ,L .H i e w ,a n dG .C .M u r p h y .C o p i n gw i t ha n
open bug repository. In Proceedings of the 2005
OOPSLA Workshop on Eclipse Technology eXchange ,
pages 35–39, New York, NY, USA, 2005. ACM.
[ 2 ]J .A n v i k ,L .H i e w ,a n dG .C .M u r p h y .W h os h o u l dﬁ x
this bug? In ICSE ’06: Proceedings of the 28th
International Conference on Software Engineering ,
pages 361–370, New York, NY, USA, 2006. ACM.
[ 3 ]J .K .A n v i k . Assisting bug report triage through
recommendation .P h Dt h e s i s ,U n i v e r s i t yo fB r i t i s h
Columbia, 2007.
[4] N. Bettenburg, S. Just, A. Schr ¨oter, C. Weiss,
R. Premraj, and T. Zimmermann. What makes a good
bug report? In SIGSOFT ’08/FSE-16: Proceedings of
the 16th ACM SIGSOFT International Symposium onthe Foundations of Software Engineering, pages
308–318, New York, NY, USA, 2008. ACM.
[5] G. Carenini, R. T. Ng, and X. Zhou. Summarizing
emails with conversational cohesion and subjectivity.InACL-08: HLT: Proceedings of the 46th Annual
Meeting of the Association for Computational
Linguistics: Human Language Technologies , pages
353–361, New York, NY, USA, 2008. ACM.
[ 6 ]J .C a r l e t t a ,S .A s h b y ,S .B o u r b a n ,M .F l y n n ,T .H a i n ,
J. Kadlec, V. Karaiskos, W. Kraaij, M. Kronenthal,G. Lathoud, M. Lincoln, A. Lisowska, and M. W.
P. D. Reidsma. The ami meeting corpus: A
pre-announcement. In MLMI ’05: Proceedings of
Machine Learning for Multimodal Interaction: Second
International Workshop , pages 28–39, 2005.
[7] Y.-W. Chen and C.-J. Lin. Combining svms with
various feature selection strategies. In Feature
extraction, foundations and applications , pages
315–324. Springer, 2006.
[8] T. Fawcett. Roc graphs: Notes and practical
considerations for researchers. Technical report, HP
Laboratories, 2004.
[9] J. Fleiss et al. Measuring nominal scale agreement
among many raters. Psychological Bulletin ,
76(5):378–382, 1971.
[10] U. Hahn and I. Mani. The challenges of automatic
summarization. Computer , 33(11):29–36, 2000.
[11] B. Klimt and Y. Yang. Introducing the enron corpus.
InCEAS ’04: Proceedings of the First Conference on
Email and Anti-Spam , 2004.
[12] A. J. Ko, B. A. Myers, and D. H. Chau. A linguistic
analysis of how people describe software problems.VL-HCC ’06: Proceedings of the 2006 IEEESymposium on Visual Languages and Human-Centric
Computing , 0:127–134, 2006.
[13] G. Murray and G. Carenini. Summarizing spoken and
written conversations. In EMNLP ’08: Proceedings of
the 2008 Conference on Empirical Methods on NaturalLanguage Processing , 2008.[14] G. Murray, S. Renals, J. Carletta, and J. Moore.
Evaluating automatic summaries of meeting
recordings. In MTSE ’05: Proceedings of the 43rd
Annual Meeting of the Association for Computational
Linguistics, Workshop on Machine Translation andSummarization Evaluation, pages 39–52. Rodopi, 2005.
[15] A. Nenkova and R. Passonneau. Evaluating content
selection in summarization: the pyramid method. In
HLT-NAACL ’04: Proceedings of the HumanLanguage Technology Conference of the North
American Chapter of the Association for
Computational Linguistics , 2004.
[16] O. Rambow, L. Shrestha, J. Chen, and C. Lauridsen.
Summarizing email threads. In HLT-NAACL ’04:
Proceedings of the Human Language Technology
Conference of the North American Chapter of the
Association for Computational Linguistics , 2004.
[17] P. Runeson, M. Alexandersson, and O. Nyholm.
Detection of duplicate defect reports using natural
language processing. In ICSE ’07: Proceedings of the
29th International Conference on Software
Engineering , pages 499–510, Washington, DC, USA,
2007. IEEE Computer Society.
[18] D. ˇCubrani´ c and G. C. Murphy. Hipikat:
recommending pertinent software development
artifacts. In ICSE ’03: Proceedings of the 25th
International Conference on Software Engineering ,
pages 408–418, Washington, DC, USA, 2003. IEEE
Computer Society.
[19] S. Wan and K. McKeown. Generating overview
summaries of ongoing email thread discussions. InCOLING ’04: Proceedings of the 20th InternationalConference on Computational Linguistics , pages
549–556, Morristown, NJ, USA, 2004. Association for
Computational Linguistics.
[20] X. Wang, L. Zhang, T. Xie, J. Anvik, and J. Sun. An
approach to detecting duplicate bug reports using
natural language and execution information. In ICSE
’08: Proceedings of the 30th International Conference
on Software Engineering, pages 461–470, New York,
NY, USA, 2008. ACM.
[21] K. Zechner. Automatic summarization of open-domain
multiparty dialogues in diverse genres. Computational
Linguistics , 28(4):447–485, 2002.
[22] X. Zhu and G. Penn. Summarization of spontaneous
conversations. In Interspeech ’06-ICSLP: Proceedings
of the 9th International Conference on Spoken
Language Processing , pages 1531–1534, 2006.
10
View publication stats