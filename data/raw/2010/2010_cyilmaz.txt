Combining Hardware and Software Instrumentation to
Classify Program Executions
Cemal Yilmaz
Sabanci University
Faculty of Engineering and Natural Sciences
Istanbul, Turkey
cyilmaz@sabanciuniv.eduAdam Porter
University of Maryland
College Park, MD 20742
aporter@cs.umd.edu
ABSTRACT
Several research e®orts have studied ways to infer properties
of software systems from program spectra gathered from the
running systems, usually with software-level instrumenta-
tion. While these e®orts appear to produce accurate classi-
¯cations, detailed understanding of their costs and potential
cost-bene¯t tradeo®s is lacking. In this work we present a
hybrid instrumentation approach which uses hardware per-
formance counters to gather program spectra at very low
cost. This underlying data is further augmented with data
captured by minimal amounts of software-level instrumen-
tation. We also evaluate this hybrid approach by comparing
it to other existing approaches. We conclude that these hy-
brid spectra can reliably distinguish failed executions from
successful executions at a fraction of the runtime overhead
cost of using software-based execution data.
Categories and Subject Descriptors
D.2.5 [ Testing and Debugging ]: Debugging aids
General Terms
Reliability, Measurement, Experimentation
Keywords
Hardware performance counters, Failure detection, Software
quality assurance.
1. INTRODUCTION
In recent years, numerous researchers have proposed data-
driven techniques to improve the quality of deployed soft-
ware. At a high level these techniques typically follow the
general approach of lightly instrumenting the software sys-
tem, monitoring its execution, analyzing the resulting data,
and then acting on the analysis results. Some example ap-
plications of this general approach include identifying likely
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
FSE ’10
Copyright 200X ACM X-XXXXX-XX-X/XX/XX ...$10.00.fault locations, anticipating resource exhaustion, and cat-
egorizing crash reports as instances of previously reported
bugs [24, 13, 23, 22, 12, 18, 15].
Another speci¯c application, which is the focus of this
paper, is determining whether execution data taken from
a deployed system comes from a failed or a successful ex-
ecution. Following the approach described above, existing
solutions to this problem typically add instrumentation to
the system source code or binaries that collect speci¯c types
of execution information, called program spectra, whenever
the system runs. This data is periodically collected and an-
alyzed. These analysis techniques attempt to identify pat-
terns in the returned program spectra that are highly corre-
lated with failed executions. The resulting models can later
be applied to new executions, taken from deployed software
instances whose failure status is unknown, as a way of clas-
sifying whether the original execution failed. Such informa-
tion can then be used, for instance, to determine whether
the deployed software system is experiencing an already re-
ported bug.
One fundamental assumption these and similar approaches
is that there are identi¯able and repeatable patterns in the
behavior of successful and failed executions and that similar-
ities and deviations from these patterns are highly correlated
with the presence or absence of failures. Previous e®orts, in
fact, appear to support this assumption, successfully apply-
ing a variety of program spectra to classify failed program
executions [7, 8, 11, 3].
Another less well-understood issue, however, is how best
to limit the total costs of implementing these approaches and
whether and how tradeo®s can be made between cost and
classi¯cation accuracy. This issue is important because these
approaches have been targeted at deployed software systems,
excessive runtime overhead is generally undesirable. There-
fore it is important to limit instrumentation overhead as
much as possible while still supporting the highest levels
of classi¯cation accuracy. In general previous e®orts have
tended to either ignore this problem or have appealed to var-
ious sampling strategies (see Section 2) for a solution. One
potential drawback of sampling however is that aggressive
sampling schemes greatly increase the numbers of observa-
tions that must be made in order to have con¯dence in the
data.
While we believe that sampling can be a powerful tool, we
also conjecture that large cost reductions may derive from
reducing the cost of the measurement instruments them-
selves. To evaluate this conjecture, we have designed and
evaluated an improved approach in which most of the datacollection work is performed by fast hardware performance
counters. The data is augmented with further data collected
by a minimal amount of software instrumentation that is
added to the system's software. We contrast this approach
with other approaches implemented purely in hardware or
purely in software. Our empirical evaluation, conducted on
four open source projects, suggests that for the systems used,
our hybrid hardware and software instrumentation approach
was as good or better than other approaches in distinguish-
ing failed executions from successful executions; and it did
so at a fraction of the cost of using purely software-based
instrumentation.
2. RELATED WORK
Classifying Program Executions. Several researchers
have studied how to predict program execution behaviors
from program spectra. To our knowledge all of these e®orts
used software spectra (program spectra collected purely by
software instrumentation) as their execution data. Podgurski
and colleagues [7, 8, 14, 19] present a set of techniques for
clustering program executions. For example, in one e®ort
they used cluster analysis to group program executions in
deployed instances. They showed that the individual clus-
ters tended to include execution pro¯les stemming from the
same failures. Bowring, Rehg, and Harrold [3] used Markov
models to classify program spectra as coming from either
successful or failed executions. Their approach uses soft-
ware instrumentation and requires probes at every branch
in a system. Brun and Ernst [4] used two machine learning
approaches to identify observed likely program invariants
that tended to be present during faulty program runs. This
approach computes very detailed abstractions at many pro-
gram points and is thus, quite expensive. Jones and Harrold
and Staska [13] use statement coverage information to iden-
tify likely causes of failures. Chen et al. [6] track the software
components used in computations and leverage that data
for problem determination. We provide a comparison of the
proposed approach to similar types of program spectra used
by Jones and Chen in the experiments section.
Reducing Data Collection Overhead. Other research
has extended earlier work with some consideration for limit-
ing instrumentation overhead. Liblit and colleagues [16, 17]
transform the source of software systems to e±ciently sam-
ple execution data collected from users and use this data
to perform fault localization. The rewriting process, how-
ever, signi¯cantly increases the size of the system code and
imposes a signi¯cant memory overhead. In addition, the
noise created by sampling must be balanced by instrument-
ing very large numbers of deployed instances, which may
not be possible for some applications. Haran et al. [11] de-
veloped several techniques for classifying execution data as
belonging to one of several classes. Each technique works by
instrumenting a set of deployed instances. Each instance in
this set, however, captures only a small subset of the pro-
gram spectra and that subset is di®erent from that captured
by all other instances. The goal is to create a combined data
set that captures enough information to allow accurate clas-
si¯cations. Yilmaz et al. present a related approach called
Time Will Tell (TWT) [24]. In this work they used min-
imal amounts of software instrumentation to capture time
spectra (i.e., traces of method execution times). Their em-
pirical evaluations suggested that coarse-grained execution
data could e®ectively capture patterns in executions.Hardware-Based Pro¯ling. Another way to cut in-
strumentation overhead is to do all data collection at the
hardware level. Anderson et al. [2] present a lightweight
pro¯ling technique done entirely with hardware performance
counters. While the program is running, they randomly in-
terrupt it and capture the value of the program counter.
Using this information they statistically estimate the per-
centage of time each instruction is executed. Assuming the
program runs long enough, they can generate a reasonably
accurate model with overheads of less than 5%. An open
question with this work is whether it can be extended to
other types of data spectra and analysis purposes. One rea-
son for the uncertainty is that their approach has no way to
associate execution data back to program entities.
In general, it is almost always possible to collect quite de-
tailed data from program executions, which is the case with
some of the existing approaches discussed above, to distin-
guish failed executions from successful executions. However,
the runtime overhead required to collect such data often
makes it impractical. In this work, we aim at ¯nding a
sweet spot between the accuracy of classi¯cations and run-
time overheads.
3. COMBINING HARDWARE AND SOFT-
WARE INSTRUMENTATION
In this article we design and evaluate an improved ap-
proach to classifying program executions that is both e®ec-
tive and inexpensive. Speci¯cally, we use hardware counters
to collect raw data, but we also use lightweight software in-
strumentation to associate subsets of the hardware-collected
data with speci¯c program entities.
Hardware performance counters are hardware-resident coun-
ters that record various events occurring on a processor. To-
day's general-purpose CPUs include a fair number of such
counters, which are capable of recording events, such as the
number of instructions executed, the number of branches
taken, the number of cache hits and misses experienced,
etc. To activate these counters, programs issue instructions
indicating the type of event to be counted and the physi-
cal counter to be used. Once activated, hardware counters
count events of interest and store the counts in a set of spe-
cial purpose registers. These registers can also be read and
reset programmatically at runtime.
One challenge we encountered when ¯rst using hardware
performance counters to collect program execution data was
that the counters do not distinguish between the instructions
issued by di®erent processes. To deal with this we used a
kernel driver, called perfctr (linux.softpedia.com ), which
implements virtual hardware counters that can track hard-
ware events on a per-process basis.
A second challenge was that hardware performance coun-
ters have limited visibility into the programs being executed,
e.g., by themselves they do not know, for example, to which
program function the current instruction belongs. As de-
scribed in more detail in Section 5.1, raw hardware spec-
tra are generally too coarse to be useful in classifying ex-
ecutions. To improve this situation, we chose to associate
hardware spectra with function invocations. That is, we use
traditional software instrumentation to indicate which func-
tion is currently executing so that di®erent subsets of the
hardware spectra are properly associated with that func-
tion. While we opted to track program execution data atthe function level, the techniques are equally applicable to
other granularity levels.
Thus, at a high level our approach in its simplest form
works as follows: 1) a hardware counter of interest is acti-
vated at the beginning of a program execution, 2) the value
of the counter is read before and after each function invoca-
tion and the di®erence is attributed to the invocation, and 3)
the counter is deactivated at the end of the execution. Note
that hardware counters are always active during executions.
Early experimentation with a prototype implementation
uncovered further issues. First, one key cost driver is the
use of software instrumentation that causes (very expen-
sive) context switching. In this work we mainly use one
instruction, called rdmpc , which enables us to read the val-
ues of physical counters from user space without causing an
OS context switch. For example, to compute the value of
a virtual counter, we record two values ( sumand start ) at
each instrumentation site for later processing, in addition
to the current value of the physical counter ( now). The sum
variable stores the total number of events observed in the
current process from the start up to the last process switch
involving the process. The start variable records the value
of the physical counter at the time of the last process switch.
Once these values are known, the value of a virtual counter
is computed as now¡start +sum. The perfctr driver used
in this work augments the OS-level context switch code to
store the start andsumvalues at suspends and restore them
at resumes. Furthermore, this driver provides a view of these
values in user space. Consequently, reading the value of a
virtual counter can be done fast without requiring any fur-
ther expensive OS context switching. To give an example,
based on performing a million counter read operations, we
observe that it takes 45 clock cycles on average to read the
value of a virtual counter on our test platforms.
Even if the cost of reading counters is low, another cost
driver is the raw number of times the counter values need to
be read. Therefore, to further reduce overhead, we in this
work chose not to pro¯le functions that are invoked more
than 50times, nor those that invoke more than 50other
functions. Note that the hardware counters are still active
during the execution of unpro¯led functions, the hardware-
spectra is just associated with function that calls these un-
pro¯led functions. The cuto® value was chosen so that func-
tions that account for the vast majority of invocations were
¯ltered. The information required for this purpose was ob-
tained by analyzing the histograms of function invocation
counts observed in our experiments.
Furthermore we chose to ignore functions that handle al-
ready detected failures, e.g., error(...) and fatal(...) .
We do this because these function invocations are the ef-
fect of failures and thus not useful in understanding failure
causes (which is often the scenario for which failure classi¯-
cation is done). In the rest of the paper these ¯ltering steps
are referred to as global function ¯ltering.
3.1 A Simple Feasibility Study
Combining hardware and software instrumentation clearly
adds some additional context to hardware spectra. Never-
theless, the underlying hardware spectra, which are based
on commonly available hardware performance counters, are
inherently less °exible than programmable software spectra.
As a result, it is unclear whether combined spectra provide
data that can be used to accurately classify executions.
| || |
||| |
|| | || |
|| || |||||| ||
||
|| |||||||||| ||| ||| ||||||||
|||||| ||||||
| || | || | | || | | | | || || |||||| | || ||| |||||||||| ||| ||| ||| ||||| ||||||||||||
2000 3000 4000 5000 6000 7000 8000Pass/Fail Info. vs. Instruction Count
number of machine instructions executedpass/fail info
data fail passFigure 1: Classi¯cation of the number of instruc-
tions executed in the socket function.
As a simple test, we therefore conducted a rudimentary
feasibility study using the socket system call as our sub-
ject. The socket function creates an endpoint (i.e., socket)
for communication. We experimented with two parameters
that this function takes as input: domain and type. The
domain parameter speci¯es a communication domain (e.g.,
INET and INET6), and the type parameter speci¯es the
communication semantics (e.g., connection-based and con-
nectionless). These parameters take a value from a set of
eleven and six discrete settings, respectively.
For this system we created 66 test cases that exhaus-
tively exercised all combinations of the parameters settings.
For each input combination, we measured the number of
machine instructions executed during the invocation of the
function. Since not all input combinations were supported
on our test platform, 51 of the test cases failed. Figure 1
depicts a simple hardware spectra we obtained. The horizon-
tal axis depicts the number of machine instructions executed
and the vertical axis denotes the entire data as well as the
data obtained from the failed and successful executions.
Using this data we asked whether this simple hardware
spectra could be used to classify executions in which the re-
quested socket failed to be created? As can be seen from
the ¯gure, even using only one simple hardware counter
the di®erence between the successful and failed invocations
is immediately apparent; failed invocations execute either
many fewer or many more machine instructions compared
to those of successful invocations. Training a classi¯cation
model for our observations (using the Weka's J48 algorithm)
and testing it on previously unseen executions provided an
F-measure of 0:98in predicting the failed invocations. With
manual analysis we determined that the failed invocations
that executed few instructions did so because of a simple
check at the very top of the function that immediately re-
turns an error code if the parameters provided are not within
the supported range. The failed invocations that executed
more instructions, on the other hand, did so because theirsubject number of number of number of total passing failing
application LOC functions versions defects tests tests tests
grep 10068 146 4 5 3050 2346 704
°ex 10459 162 4 19 11417 8399 3018
sed 14427 255 5 15 5455 4454 1001
gcc 222196 2214 1 unknown 7701 7494 207
Table 1: Subject applications used in the experiments.
failures caused the kernel to immediately release all previ-
ously allocated resources. Successful executions, in contrast,
hold on to resources until the socket is explicitly closed.
Although this simple demonstration is by no means con-
clusive, it encouraged our belief that program spectra gath-
ered from hardware performance counters can capture useful
data that can be used to reliably detect failures.
4. EXPERIMENTS
To evaluate the accuracy and e®ectiveness of hardware-
based spectra in distinguishing failed executions from suc-
cessful executions, we conducted a set of experiments using
four open source real-life applications as our subject appli-
cations. This section reports on our experiments.
4.1 Independent Variables
We manipulated two independent variables in our exper-
iments.
Program spectrum type. This variable has six dif-
ferent levels. The ¯rst three program spectra are collected
using hardware performance counters:
²TOT INS counts the number of machine instructions
executed
²BRN TKN counts the number of branches taken
²LST INS counts the number of load and store mem-
ory instructions executed
The three remaining spectra are gathered using traditional
software pro¯ling:
²CALL SWT records the functions invoked
²STMT FREQ counts the number of times the source
code statements are executed
²TIME measures execution times of functions at the
level of nanoseconds
Although numerous types of software-based spectra have
been proposed in the literature, we chose to use these four
types for several reasons. First, each has been successfully
used in the literature for related purposes [24, 13, 21, 6].
Second, each is representative of a variety of related spec-
tra in terms of the runtime overhead they impose. For
example, the overheads of collecting the CALL SWT and
STMT FREQ spectra can be viewed as lower bounds on
collecting simple function-level, and detailed statement-level
execution data, respectively. Similarly, the overhead of col-
lecting the TIME spectra can be viewed as a lower bound
on collecting simple execution data that requires OS con-
text switches; we used system calls to compute the execution
times in this work.Dynamic call tree depth to be pro¯led. We used six
di®erent dynamic call tree depths ( 0,1,2,3,4, and all) as
a cuto® point for our instrumentation. For a depth d, only
the function calls that happen when the call stack depth is
less than or equal to dare pro¯led. When d= 0, only the
main function is pro¯led, and when d=all, all the function
invocations of interests are pro¯led. Note that hardware
counters are always active during an execution. Hardware
spectra data for function invocations occurring at depth > d
are simply associated with the function invocation at depth
d.
4.2 Subject Applications
We used four open source subject applications in our ex-
periments: grep,flex,sed, and gcc. These widely-used
applications print lines matching a pattern, generate fast
lexical analyzers, ¯lter and transform text as a stream edi-
tor, and compile C programs, respectively. Table 1 provides
some descriptive statistics for the subject applications.
The ¯rst three subject applications were taken from the
Software-artifact Infrastructure Repository (SIR) [9]. The
SIR repository provided us with several versions of these
subject applications with known defects. The gccapplica-
tion used in the experiments was the o±cial release of the
GNU Compiler Collection version 2:95:2. Each subject ap-
plication came with its own test suites and oracles, which
we used in the experiments.
4.3 Operational Model
To conduct our studies we executed the test suites of our
subject applications, collected the spectra from these execu-
tions. For each run we measured the overhead incurred by
our data collection instrumentation. Next we determined
whether each execution was successful or failed using the
test oracles that came with the suites. After that, we cre-
ated classi¯cation models for each combination of subject
program version, spectrum type, and depth of dynamic call
tree being pro¯led. Once the data was collected, we cre-
ated and evaluated the classi¯cation models by using the
Weka's J48 classi¯cation algorithm with ¯ve-fold strati¯ed
cross validation [10]. That is, the evaluation of the models
was performed on previously unseen executions.
To evaluate the runtime overhead induced by collecting
various types of spectra, we compare the execution times
of programs with and without pro¯ling and compute the
overhead as follows:
overhead =exec: time w= prof: ¡exec: time w=o prof:
exec: time w=o prof:
To evaluate the accuracy of classi¯cation models we use
several standard metrics. Precision (P) and recall (R) aretwo widely used metrics to assess the performance of classi-
¯cation models. For a given testing table T, we de¯ne them
as follows:
recall =#of correctly predicted failed executions in T
total #of failed executions in T
precision =#of correctly predicted failed executions in T
total #of predicted failed executions in T
Since neither measure predominates our evaluation, we
combine these measures using the F - measure. This is de-
¯ned as:
F¡measure =(b2+ 1)PR
b2P+R
Here bcontrols the weight of importance to be given to
precision and recall: F=Pwhen b= 0andF=Rwhen
b=1. Throughout the paper, we compute F-measure with
b= 1, which gives precision and recall equal importance,
and use it to evaluate the classi¯cation models.
4.4 Experimental Setup
The software instrumentation used in associating hard-
ware spectra with function invocations was implemented via
binary instrumentation using GNU's C extension framework
for instrumentation (i.e., gcc's-finstrument-functions op-
tion). The same approach was also used to collect the TIME
and CALL SWT spectra. The STMT FREQ spectra on the
other hand were collected using GNU's gcov test coverage
tool. We modi¯ed the implementation of this tool to sup-
port selective instrumentation so that coverage information
was computed only for the functions of interests.
To compute runtime overheads, we dedicated one CPU of
a dual-CPU system to execute the test cases (using
sched_setaffinity and other related system calls). No
other user-level programs were allowed to run on the ded-
icated CPU. We furthermore measured execution times at
the level of nanoseconds in terms of the CPU allocation time
using the K-best measurement scheme [5]. All the experi-
ments were performed on a dual Intel processor machine
with 2GB of RAM, running the CentOS 5.2 operating sys-
tem.
All told, our data set is comprised of 19;922total test
cases run across 39 defective versions of our subject applica-
tions (Table 1). These 39 defective versions were culled from
an set of 166 defective versions using the criteria that the
ratio of failed to successful executions was between 0:05and
1:50. We did this because classi¯cation techniques them-
selves either perform poorly or need special enhancement
when one class is much more common than the other. Since
our goal is not to evaluate classi¯cation techniques them-
selves we ignore these cases for our analysis. For our evalu-
ations, we created a total of 5;850classi¯cation models (39
versions x6 spectra types x5 call tree depths x5-fold cross
validation).
5. DATA AND ANALYSIS
The following sections present and discuss some of our
results.
F−measure vs. Spectrum TypeF−measure
Spectrum Type0 0.2 0.4 0.6 0.8 10.58 0.83 0.78 0.46 0.44 0.48CALL_SWT
STMT_FREQ
TIME
TOT_INS
BR_TKN
LST_INSFigure 2: F-measure vs. spectrum type (hardware
and software spectra).
5.1 Hardware and Software Spectra
Our ¯rst study examined the accuracies and overheads
provided by hardware and software spectra. To collect the
hardware spectra, we activated one hardware counter of in-
terest, read the value of the counter at the beginning and at
the end of an execution, and associated the di®erence with
the execution. Note that hardware counters were read only
twice for each program execution in this study. This sce-
nario corresponds to pro¯ling with d= 0. To collect the
software spectra, we ¯rst enabled our global function ¯lters
(Section 3) and then pro¯led all the remaining functions to
collect one type of program spectra. Since this process ig-
nores the depth of the call stack, it corresponds to pro¯ling
with d=all. For these subject programs and test cases,
global function ¯ltering prevented 98% of the invocations
on average from being pro¯led.
The last three columns of Table 2 present the runtime
overheads incurred by each of the 3 hardware spectra used
in our experiments. For this study the rows marked as d= 0
are relevant. As the table indicates, the overheads ranged
from 0:0031 to about 0:0096. That is, the runtime overheads
attributable solely to hardware performance counters were
under 1%. The ¯rst three columns of Table 2 present the
overheads incurred by each of the 3 software spectra used in
the experiments. The relevant data are in the rows where
d=all. The CALL SWT spectrum, which simply records
the list of called functions, incurred overheads of about 1%
to 2%. The two remaining software counters incurred higher
overheads, ranging from about 7.6% to about 27%.
Figure 2 plots F-measures of classi¯cation accuracy we
obtained for this study. Each box in this plot illustrates the
distribution of F-measures obtained from a spectrum type.
The lower and the upper end of a box represents the ¯rst and
the third quartiles and the horizontal bar inside represents
the median value. Numbers given below the boxes indicate
the mean values and are visualized as ¯lled circles.sut CALL SWT STMT FREQ TIME TOT INS BR TKN LST INS
d overhead d overhead d overhead d overhead d overhead d overhead
grep0 n/a 0 n/a 0 n/a 0 0.0031 0 0.0036 0 0.0037
1 0.0060 1 0.0656 1 0.0425 1 0.0086 1 0.0089 1 0.0091
2 0.0072 2 0.0673 2 0.0673 2 0.0098 2 0.0107 2 0.0102
3 0.0075 3 0.0686 3 0.0650 3 0.0101 3 0.0104 3 0.0109
4 0.0076 4 0.0742 4 0.0750 4 0.0120 4 0.0114 4 0.0112
all 0.0112 all 0.0763 all 0.0821 all 0.0125 all 0.0125 all 0.0132
°ex0 n/a 0 n/a 0 n/a 0 0.0060 0 0.0058 0 0.0060
1 0.0102 1 0.0792 1 0.0847 1 0.0157 1 0.0154 1 0.0161
2 0.0105 2 0.1184 2 0.1702 2 0.0156 2 0.0166 2 0.0163
3 0.0130 3 0.1245 3 0.2170 3 0.0165 3 0.0162 3 0.0169
4 0.0130 4 0.1263 4 0.2199 4 0.0168 4 0.0162 4 0.0168
all 0.0152 all 0.1303 all 0.2448 all 0.0169 all 0.0168 all 0.0167
sed0 n/a 0 n/a 0 n/a 0 0.0075 0 0.0096 0 0.0083
1 0.0182 1 0.2441 1 0.2018 1 0.0142 1 0.0140 1 0.0141
2 0.0184 2 0.2597 2 0.2268 2 0.0150 2 0.0151 2 0.0151
3 0.0182 3 0.2551 3 0.2233 3 0.0146 3 0.0143 3 0.0141
4 0.0188 4 0.2603 4 0.2253 4 0.0144 4 0.0149 4 0.0147
all 0.0192 all 0.2701 all 0.2278 all 0.0152 all 0.0152 all 0.0161
Table 2: Runtime overheads.
F−measure vs. Spectrum Type  (depth = dall)F−measure
Spectrum Type0 0.2 0.4 0.6 0.8 10.58 0.83 0.78 0.84 0.84 0.86CALL_SWT
STMT_FREQ
TIME
TOT_INS
BR_TKN
LST_INS
Figure 3: F-measure vs. spectrum type (depth=all).
The Figure shows that even though hardware spectra were
inexpensive, they were not very e®ective in classifying exe-
cutions. In fact, the classi¯cation models created using the
hardware spectra collected in this study predicted failed ex-
ecutions with an average F-measure of only 0:46. Note: the
expected value of the F-measure varies depending on the ra-
tion of successful to failing executions, but for our studies
random assignment would give F-measures between about
0.5 and about 0.30. The Figure also shows that the F-
measures for software spectra were generally higher, ranging
from 0.58 for CALL SWT to 0.83 (STMT FREQ) and 0.78
(TIME).
Overall we see that the hardware spectra were very in-
expensive, but inaccurate. For the software spectra, one,
F−measure vs. Overhead
OverheadF−measure
0 0.016 0.05 0.1 0.15 0.17 0.190.4 0.5 0.6 0.7 0.8 0.85CALL_SWT (software)STMT_FREQ (software)    TIME (software)TOT_INS (hardware)BR_TKN (hardware)LST_INS (hardware)TOT_INS (hybrid)BR_TKN (hybrid)LST_INS (hybrid)Figure 4: F-measure vs. overhead.
CALL SWT was also very inexpensive and had moderate
accuracy. The remaining 2 software spectra were more ac-
curate, but incurred substantially more overhead.
5.2 Associating Data with Function Invocations
To improve classi¯cation accuracy while limiting instru-
mentation overhead, we decided to combine hardware and
software instrumentation. The general idea is to collect most
of the data using hardware counters, but to use software in-
strumentation to assign that data to speci¯c program enti-
ties.
To evaluate this idea, we made several changes to our
procedures and then repeated the previous study, The ¯rst
change was that we used software instrumentation to asso-ciate hardware-collected data with individual function in-
vocations. Speci¯cally, we inserted code into the programs
that read the value of the appropriate hardware counter at
the beginning and at the end of each invocation. We then
attributed the di®erence to that invocation. Since this pro-
cess ignores the depth of the call stack, it corresponds to
pro¯ling with d=all.
Table 2 presents the runtime overhead incurred by each
type of performance counter. The data relevant for this
study appear in rows marked depth=all. Here we see that
the overhead incurred by our combined hardware and soft-
ware approach ranged from about 1.25% for grep to about
1.5% for sed. These levels of overhead indicate that even
minimal software instrumentation increased overhead by a
factor of 2{4, but that the total overhead was nonetheless
substantially smaller than that incurred with more sophis-
ticated software instrumentation approaches.
Figure 3 depicts the F-measures of classi¯cation accuracy
we obtained for this study. Our ¯rst observation is that asso-
ciating hardware counters with function invocations signif-
icantly boosted classi¯cation accuracy over plain hardware
spectra. Here the average F-measure increased from 0:46to
0:85.
Figure 4 provides an integrated view of the costs and ben-
e¯ts of using hardware, software and hybrid spectra. One
observation is that for comparable overhead levels, the hy-
brid spectra were signi¯cantly better at detecting the fail-
ures than the hardware or software spectra used in the study.
For example, compared to the CALL SWT spectrum, where
the average overheads were almost the same ( 0:015), hybrid
spectra provided signi¯cantly better F-measures; an average
F-measure of 0:85vs:0:58, respectively.
Alternatively, for comparable accuracy levels, the hybrid
spectra induced signi¯cantly less overhead (see also the rows
where d=allin Table 2). For example, the hardware spec-
tra and the STMT FREQ spectra have similar F-measures;
0:85vs:0:83on average, respectively. However, the hy-
brid spectra incurred a fraction of the overhead incurred
by the STMT FREQ spectrum. The runtime overhead was
0:015on average for the hybrid spectra and 0:159for the
STMT FREQ spectrum. This corresponds to about 10-fold
reduction in the runtime overhead.
5.3 Using Structural Sampling
As discussed in Section 2 several previous research e®orts
have use sampling strategies to further reduce pro¯ling over-
head. Many of these strategies are implemented by pro¯l-
ing only a carefully chosen, small subset of all the potential
measurement locations. We call this technique structural
sampling to distinguish it from statistical sampling, where,
logically, all measurement locations are instrumented but
the instruments are enabled with certain probabilities. Note
however that even statistical approaches are sometimes im-
plemented via structural sampling. Speci¯cally, code sec-
tions are duplicated and copied into both instrumented and
uninstrumented paths [16]. While sampling certainly re-
duces overhead it may have a negative e®ect on classi¯ca-
tion accuracy, especially when the number of instrumented
systems is low and/or when failure frequencies are low.
In this study, we evaluated the overhead and accuracy of
classifying executions when execution data is acquired via
a structural sampling approach. To carry out the study,
we used the following simple sampling approach. We onlypro¯le function invocations when they occur at or below
depth din the dynamic call stack, where 1·d·4. Note
however that our hardware counters were always activate
during executions. If a function invocation occurs at depth
greater than d, the resulting data is not associated with
the current invocation, but is associated with the calling
function at depth din the dynamic call stack.
Furthermore, to focus on areas in which the loss of preci-
sion might cause a resulting loss of classi¯cation accuracy, we
considered only those faults that were exercised at a depth
greater than d. These defects were identi¯ed by manually
marking the depths at which the underlying faults were ex-
ecuted and then choosing those that satis¯ed the condition.
All other faults were ignored.
In this study, we did not observe signi¯cant overhead re-
duction by employing our structural sampling approach (Ta-
ble 2). This was because after enabling our global function
¯lters the number of functions pro¯led at di®erent depths
were close to each others.
Figure 5 depicts the classi¯cation accuracies observed in
this study. As the ¯gure indicates, the hybrid spectra were
indistinguishable from software spectra when dwas low ( d=
1andd= 2). As depth increased the hybrid spectra gener-
ally displayed greater accuracy. Referring back to Figure 3,
however, we note that when d=all, software spectra were
as accurate as hybrid spectra. Referring to Table 2, we also
note that hybrid spectra incurred a fraction of the overhead
cost compared to that of the software spectra. For example,
when d= 4, the average F-measure is 0:72with an overhead
of0:015for the combined spectra, and 0:29with an overhead
of0:154for the STMT FREQ spectrum.
To better understand these results we manually analyzed
the resulting classi¯cation models. One reason for which
the classi¯cation models created from hybrid spectra outper-
form those created from software spectra is that the hybrid
spectra summarize rather than ignore unpro¯led functions.
That is, when using hardware performance counters we as-
sociate the execution data for functions that are called at
a level greater than dwith the parent function at depth d.
In a sense, hardware counters were able to collect some in-
formation from the uninstrumented parts of the programs.
Visual inspection of the classi¯cation models con¯rmed that
the calling functions at level dbecome the key indicators
of failures. On the other hand, the software spectra, such
as the STMT FREQ spectra, su®ered greatly, because they
completely ignore uninstrumented parts of the code.
5.4 Different Defect Types
While studying the classi¯cation models obtained in the
previous study, we observed several cases in which software
spectra outperformed our hybrid spectra and vice versa. For
example, the STMT FREQ spectra when d= 1performed
better than the hybrid spectra. We discovered that our sub-
ject applications often perform certain types of error check-
ing at depth one and two, such as checking to see if the
return values of function calls indicate an internal failure.
Of course, if one of these checks fails, it is a perfect indica-
tor of a failure. In these cases STMT FREQ spectra were
faultless in predicting failures. However, many failures are
not internally detected by our subject applications. In these
cases software spectra tended to perform poorly (e.g., when
d= 4).
To further investigate potential relationships between ac-F−measure vs. Spectrum Type  (depth = d1)F−measure
Spectrum Type0 0.2 0.4 0.6 0.8 10.08 0.32 0.10 0.22 0.15 0.20CALL_SWT
STMT_FREQ
TIME
TOT_INS
BR_TKN
LST_INS(a) F-measure vs. spectrum type (d=1).
F−measure vs. Spectrum Type  (depth = d2)F−measure
Spectrum Type0 0.2 0.4 0.6 0.8 10.16 0.58 0.25 0.60 0.45 0.55CALL_SWT
STMT_FREQ
TIME
TOT_INS
BR_TKN
LST_INS (b) F-measure vs. spectrum type (d=2).
F−measure vs. Spectrum Type  (depth = d3)F−measure
Spectrum Type0 0.2 0.4 0.6 0.8 10.08 0.45 0.16 0.69 0.60 0.75CALL_SWT
STMT_FREQ
TIME
TOT_INS
BR_TKN
LST_INS
(c) F-measure vs. spectrum type (d=3).
F−measure vs. Spectrum Type  (depth = d4)F−measure
Spectrum Type0 0.2 0.4 0.6 0.8 10.04 0.29 0.34 0.72 0.75 0.69CALL_SWT
STMT_FREQ
TIME
TOT_INS
BR_TKN
LST_INS (d) F-measure vs. spectrum type (d=4).
Figure 5: Failure prediction F-measure vs. spectrum type (depth = f1, 2, 3, 4 g).
curacy and failure characteristics, we performed the follow-
ing additional study. Working with three graduate students
at Sabanci University, we asked them to examine each known
defect in our subject programs and categorize them into 1
of 3 categories. These categories are laid out by the SIR
repository that categorizes each defect into one of the three
broad defect types: Type 1, Type 2, and Type 3. Type 1
defects cover the defects that stem from the errors made on
variable assignments, such as using erroneous values for vari-
ables, and passing incorrect parameters to functions. Type
2 defects represent the errors made on the control °ow of
programs, such as missing a function call, and using in-
correct branching conditions. Type 3 defects on the other
hand depict the errors made on memory operations, such as
pointers-related errors.
We then used a voting scheme to resolve con°icting cate-
gorizations. Further inconsistencies were handled on a case-Spectrum Type 1 Type 2
CALL SWT 0.57 0.57
STMT FREQ 0.69 0.96
TIME 0.64 0.91
TOT INS 0.72 0.96
BRTKN 0.71 0.95
LST INS 0.77 0.95
Table 4: Average F-measures obtained on di®erent
types of defects.
by-case basis. Out of 39 defects used in the experiments, 19
defects were categorized into Type 1, another 19 into Type
2, and only 1 defect into Type 3. Since we happened to have
only one Type 3 defect, we leave it out of discussion in this
section.depth CALL SWT STMT FREQ TIME TOT INS
F-measure overhead F-measure overhead F-measure overhead F-measure overhead
1 0 0.0009 0 1.1291 0 0.5304 0 0.0052
2 0 0.0007 0.44 1.1801 0 0.5343 0 0.0049
3 0 0.0076 0.44 1.1891 0 0.5310 0.45 0.0047
4 0.05 0.0131 0.56 1.1901 0.02 0.5561 0.79 0.0115
all 0.59 0.0136 0.82 1.2424 0.59 0.5766 0.81 0.0138
Table 3: Results obtained from the gccexperiments.
Table 4 depicts the average F-measures obtained for dif-
ferent types of defects. Note that the F-measures used in
this table are adopted from Section 5.2. We observed that
the hybrid spectra were generally more accurate than the
software spectra across both defect types. That is, di®er-
ent types of defects did not cause any noticeable di®erences
between the relative accuracies of the hybrid and software
spectra.
Furthermore, each spectrum type was better at classifying
Type 2 failures than Type 1 failures. We observed that Type
2 defects, by directly changing the control °ow of programs,
often caused pronounced di®erences in the paths taken by
the failed and successful executions. These di®erences re-
°ected on the STMT FREQ spectrum, for example, as a set
of suspicious source code statements executed and on the
combined spectra as a set of suspicious amount of computa-
tional activities performed. It turned out that it was much
easier for the spectrum types used in the experiments (ex-
cept for CALL SWT) to detect such di®erences compared
to the ones caused by Type 1 defects.
5.5 Replication with gcc
The studies discussed so far involved medium-sized appli-
cations. In this study, we evaluated our approach on larger
GNU Compiler Collection ( gcc) version 2:95:2. For this
study we collected the CALL SWT, STMT FREQ, TIME,
and TOT INS spectra at various levels of granularity. We
then used this data to classify failing and successful execu-
tions. As we do not know the actual causes of the failures for
gcc, we could not manually verify the speci¯cs of the result-
ing classi¯cation models as we did in our previous studies.
In these experiments, we only pro¯led the cc1compo-
nent of the compiler. This is the core component of the gcc
compiler and is responsible for compiling source code into
assembly language. We executed a total of 7701 test cases
out of which 207 of them failed and 7494 of them passed. As
with the rest of the experiments we ¯rst applied our global
function ¯lters (Section 3). This prevented pro¯ling around
99% of the total function invocations. As with our previous
studies, we also did not pro¯le functions that are called only
after an internal failure has been detected.
Table 3 depicts the overheads and F-measures observed
in this study. In general these results are consistent with
those obtained on the smaller-scale applications used in our
previous studies. Looking at the overhead data, we observe
that while the cost of pro¯ling for the TOT INS spectrum
in this study was similar to the ones obtained in other stud-
ies, the STMT FREQ spectrum imposed signi¯cantly higher
costs. A manual investigation revealed that one reason for
the variation was that the gccapplication often executed
more statements per function invocation compared to the
rest of the subject applications.Compared to the STMT FREQ spectrum, when d=all,
the TOT INS spectrum provided similar F-measures at a
fraction of the cost; an average F-measure of 0:81with an
overhead cost of 0:014for the TOT INS spectrum and an
average F-measure of 0:82with an overhead cost of 1:24
for the STMT FREQ spectrum. Alternatively, compared
to the CALL SWT spectrum, where the overhead costs are
similar, the TOT INS spectrum provided signi¯cantly better
F-measures; an F-measure of 0:81vs.0:59. Furthermore,
the TOT INS spectrum was better at predicting the failures
at almost all granularity levels of pro¯ling compared to the
software spectra.
6. CONCLUDING DISCUSSION
Several research e®orts have studied how to infer prop-
erties of executing software systems from program spectra.
These e®orts appear to produce accurate results [24, 13,
23, 22, 12, 18, 15], but the issue of costs is less well un-
derstood. In this article we examined a novel approach for
instrumenting software systems to support execution classi-
¯cation. This approach combines low overhead, but coarse-
grained hardware spectra with minimal amounts of higher
overhead software instrumentation to create a hybrid spec-
tra.
We also evaluated this approach and compared it against
other representative approaches. In the experiments we used
three di®erent types of hybrid spectra and three di®erent
types of software spectra. We compared the cost and accu-
racy of the hybrid spectra to those of the software spectra.
Before discussing the results of these evaluations, we re-
mind the reader that all empirical studies su®er from threats
to their internal and external validity. For this work, we are
primarily concerned with threats to external validity since
they limit our ability to generalize our results. One threat
concerns the representativeness of the subject applications
used in the experiments. Although they are all real-life ap-
plications, they only represent four data points. A related
threat concerns the representativeness of the defects used in
the experiments. Although, the grep,flex, and sedappli-
cations were taken from an independent defect repository
which has been leveraged by many related studies in the
literature [1, 13, 24, 20, 21] and the gccapplication was
an o±cial release of a frequently-used compiler, they only
represent a subset of defects.
Keeping these limitations in mind, we believe that our
study supports our basic hypothesis: that our hybrid spectra
approach incurs low overhead, but still produces data that
generates accurate classi¯cation models.
We arrived at this conclusion by applying our approach
to several medium-sized subject systems. Based on this we
¯rst observed that for our subject systems, test cases and
choices of program spectra, hardware-only spectra had lowoverhead, but led to inaccurate classi¯cation models, while
software-only spectra had high overhead, but led to accurate
classi¯cation models.
Next, we compared the hybrid spectra to hardware-only
and to software-only spectra. Compared to hardware-only
spectra, our hybrid spectra, however, added little additional
overhead, while greatly increasing classi¯cation accuracy.
Compared to software spectra, our hybrid approach, had
equivalent accuracy, but greatly reduced overhead.
We also observed that limiting pro¯ling according to depth
of the call stack did not greatly a®ect the runtime overhead
of collecting the various program spectra. This was primar-
ily due to the fact that we had already applied global func-
tion ¯ltering, which already reduced the number of function
invocations pro¯led by around 98%. The accuracy of the
hybrid spectra appeared to degrade more slowly than did
that of the other spectra, but more studies are needed.
After this, we observed that all the program spectra-based
approaches we studied were better at classifying failed ex-
ecutions stemming from failures that changed control °ow,
rather than those that caused by incorrect data usage. From
this perspective the hybrid spectra was no di®erent from the
hardware-only or software-only spectra.
Finally, we replicated most of our initial studies on a larger
system, gcc. This study was consistent with our previous
¯ndings. This further supports our hypothesis that hybrid
spectra can be both lightweight and yield accurate execution
classi¯cations.
We believe that this line of research is novel and interest-
ing. We are therefore continuing to investigate how hybrid
hardware- and software-based instrumentation can serve as
abstraction mechanisms for program executions in various
software quality assurance approaches, such as fault localiza-
tion, failure prediction, security assurance, and in-the-¯eld
quality assurance approaches.
7. ACKNOWLEDGMENTS
This research was supported by a Marie Curie Interna-
tional Reintegration Grant within the 7th European Com-
munity Framework Programme (FP7-PEOPLE-IRG-2008).
8. REFERENCES
[1]H. Agrawal, J. Horgan, S. London, and W. Wong. Fault
localization using execution slices and data°ow tests. In ISSRE
Conference Proceedings , 1995.
[2]J. M. Anderson, L. M. Berc, J. Dean, S. Ghemawat, M. R.
Henzinger, S.-T. A. Leung, R. L. Sites, M. T. Vandevoorde,
C. A. Waldspurger, and W. E. Weihl. Continuous pro¯ling:
where have all the cycles gone? ACM Trans. Comput. Syst. ,
15(4):357{390, 1997.
[3]J. F. Bowring, J. M. Rehg, and M. J. Harrold. Active learning
for automatic classi¯cation of software behavior. In Proc. of
the Int'l Symp. on Software Testing and Analysis (ISSTA
2004) , pages 195{205, July 2004.
[4]Y. Brun and M. D. Ernst. Finding latent code errors via
machine learning over program executions. In Proc. of the 26th
Int'l Conf. on SW Eng. (ICSE 2004) , pages 480{490, May
2004.
[5]R. E. Bryant and D. R. O'Hallaron. Computer Systems: A
Programmer's Perspective . Prentice Hall, 2002.
[6]M. Y. Chen, E. Kiciman, E. Fratkin, A. Fox, and E. Brewer.
Pinpoint: Problem determination in large, dynamic internet
services. Dependable Systems and Networks, International
Conference on , 0:595{604, 2002.
[7]W. Dickinson, D. Leon, and A. Podgurski. Pursuing failure: the
distribution of program failures in a pro¯le space. In Proc. of
the 9th ACM SIGSOFT international symposium on
Foundations of SW Eng. , pages 246{255, September 2001.[8]W. Dickinson, D. Leon, and A. Podgursky. Finding failures by
cluster analysis of execution pro¯les. In Proc. of the 23rd Int'l
Conf. on SW Eng. (ICSE 2001) , pages 339{348, May 2001.
[9]H. Do, S. Elbaum, and G. Rothermel. Supporting controlled
experimentation with testing techniques: An infrastructure and
its potential impact. Empirical Soft. Eng. , 10(4):405{435, 2005.
[10]M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann,
and I. H. Witten. The weka data mining software: An update.
SIGKDD Explorations , 11(1):10{19, 2009.
[11]M. Haran, A. Karr, A. Orso, A. Porter, and A. Sanil. Applying
classi¯cation techniques to remotely-collected program
execution data. SIGSOFT Softw. Eng. Notes , 30(5):146{155,
2005.
[12]G. Hoglund and G. McGraw. Exploiting software: How to
break code . Addison-Wesley Publishing Company.
[13]J. A. Jones, M. J. Harrold, and J. Stasko. Visualization of test
information to assist fault localization. In ICSE Conference
Proceedings , pages 467{477, 2002.
[14]D. Leon, A. Podgurski, and L. J. White. Multivariate
visualization in observation-based testing. In Proc. of the 22nd
international conference on SW engineering (ICSE 2000) ,
pages 116{125, May 2000.
[15]B. Liblit, A. Aiken, A. X. Zheng, and M. I. Jordan. Bug
isolation via remote program sampling. SIGPLAN Not. ,
38(5):141{154, 2003.
[16]B. Liblit, A. Aiken, A. X. Zheng, and M. I. Jordan. Bug
isolation via remote program sampling. In Proceedings of the
Conference on Programming Language Design and
Implementation (PLDI 2003) , pages 141{154, June 2003.
[17]B. Liblit, M. Naik, A. X. Zheng, A. Aiken, and M. I. Jordan.
Scalable statistical bug isolation. In Proc. of the Conf. on
Programming Language Design and Implementation (PLDI
2005) , June 2005.
[18]A. Podgurski, D. Leon, P. Francis, W. Masri, M. Minch, J. Sun,
and B. Wang. Automated support for classifying software
failure reports. In ICSE Conference Proceedings , pages
465{475, 2003.
[19]A. Podgurski, D. Leon, P. Francis, W. Masri, M. M. Sun, and
B. Wang. Automated support for classifying sw failure reports.
InProc. of the 25th Int'l Conf. on SW Eng. (ICSE 2003) ,
pages 465{474, May 2003.
[20]M. Renieris and S. P. Reiss. Fault localization with nearest
neighbor queries. In ASE Conference Proceedings .
[21]R. Santelices, J. A. Jones, Y. Yanbing, and M. J. Harrold.
Lightweight fault-localization using multiple coverage types. In
ICSE Conference Proceedings , pages 56{66, 2009.
[22]S. Singer, K. Gross, J. Herzog, S. Wegerich, and W. King.
Model-based nuclear power plant monitoring and fault
detection: theoretical foundations. In Proceedings of the
International Conference on Intelligent Systems Applications
to Power Systems , pages 60{65, 1997.
[23]R. Vilalta and S. Ma. Predicting rare events in temporal
domains. In ICDM Proceedings , pages 474{481, 2002.
[24]C. Yilmaz, A. Paradkar, and C. Williams. Time will tell: fault
localization using time spectra. In ICSE Conference
Proceedings , pages 81{90, 2008.