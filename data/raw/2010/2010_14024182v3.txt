Learning Natural Coding Conventions
Miltiadis Allamanis†Earl T. Barr‡Christian Bird⋆Charles Sutton†
†School of Informatics‡Dept. of Computer Science⋆Microsoft Research
University of Edinburgh University College London Microsoft
Edinburgh, EH8 9AB, UK London, UK Redmond, WA, USA
{m.allamanis, csutton}@ed.ac.uk e.barr@ucl.ac.uk christian.bird@microsoft.com
ABSTRACT
Every programmer has a characteristic style, ranging from pref-
erences about identiﬁer naming to preferences about object rela-
tionships and design patterns. Coding conventions deﬁne a consis-
tent syntactic style, fostering readability and hence maintainability.
When collaborating, programmers strive to obey a project’s coding
conventions. However, one third of reviews of changes contain
feedback about coding conventions, indicating that programmers
do not always follow them and that project members care deeply
about adherence. Unfortunately, programmers are often unaware of
coding conventions because inferring them requires a global view,
one that aggregates the many local decisions programmers make
and identiﬁes emergent consensus on style. We present NATURAL -
IZE, a framework that learns the style of a codebase, and suggests
revisions to improve stylistic consistency. NATURALIZE builds on
recent work in applying statistical natural language processing to
source code. We apply NATURALIZE to suggest natural identiﬁer
names and formatting conventions. We present four tools focused
on ensuring natural code during development and release manage-
ment, including code review. NATURALIZE achieves 94% accuracy
in its top suggestions for identiﬁer names and can even transfer
knowledge about conventions across projects, leveraging a corpus
of 10,968 open source projects. We used NATURALIZE to generate
18patches for 5open source projects: 14were accepted.
1. INTRODUCTION
To program is to make a series of choices, ranging from design
decisions — like how to decompose a problem into functions — to
the choice of identiﬁer names and how to format the code. While
local and syntactic, the latter are important: names connect program
source to its problem domain [13, 43, 44, 68]; formatting decisions
usually capture control ﬂow [36]. Together, naming and formatting
decisions determine the readability of a program’s source code,
increasing a codebase’s portability, its accessibility to newcomers,
its reliability, and its maintainability [55, §1.1]. Apple’s recent,
infamous bug in its handling of SSL certiﬁcates [7, 40] exempliﬁes
the impact that formatting can have on reliability. Maintainability is
especially important since developers spend the majority ( 80%) of
their time maintaining code [2, §6].
A convention is “an equilibrium that everyone expects in inter-
actions that have more than one equilibrium” [74]. For us, coding
conventions arise out of the collision of the stylistic choices of
programmers. A coding convention is a syntactic restriction not
imposed by a programming language’s grammar. Nonetheless, these
choices are important enough that they are enforced by software
teams. Indeed, our investigations indicate that developers enforce
such coding conventions rigorously, with roughly one third of code
reviews containing feedback about following them (Section 4.1).Like the rules of society at large, coding conventions fall into
two broad categories: laws, explicitly stated and enforced rules,
andmores , unspoken common practice that emerges spontaneously.
Mores pose a particular challenge: because they arise spontaneously
from emergent consensus, they are inherently difﬁcult to codify into
a ﬁxed set of rules, so rule-based formatters cannot enforce them,
and even programmers themselves have difﬁculty adhering to all
of the implicit mores of a codebase. Furthermore, popular code
changes constantly, and these changes necessarily embody stylistic
decisions, sometimes generating new conventions and sometimes
changing existing ones. To address this, we introduce the coding
convention inference problem , the problem of automatically learning
the coding conventions consistently used in a body of source code.
Conventions are pervasive in software, ranging from preferences
about identiﬁer names to preferences about class layout, object
relationships, and design patterns. In this paper, we focus as a ﬁrst
step on local, syntactic conventions, namely, identiﬁer naming and
formatting. These are particularly active topics of concern among
developers, for example, almost one quarter of the code reviews
that we examined contained suggestions about naming.
We introduce NATURALIZE , a framework that solves the cod-
ing convention inference problem for local conventions, offering
suggestions to increase the stylistic consistency of a codebase. NAT-
URALIZE can also be applied to infer rules for existing rule-based
formatters. NATURALIZE is descriptive, not prescriptive1: it learns
what programmers actually do. When a codebase does not reﬂect
consensus on a convention, NATURALIZE recommends nothing, be-
cause it has not learned anything with sufﬁcient conﬁdence to make
recommendations. The naturalness insight of Hindle et al. [35],
building on Gabel and Su [28], is that most short code utterances,
like natural language utterances, are simple and repetitive. Large
corpus statistical inference can discover and exploit this naturalness
to improve developer productivity and code robustness. We show
that coding conventions are natural in this sense.
Learning from local context allows NATURALIZE to learn syntac-
tic restrictions, or sub-grammars, on identiﬁer names like camelcase
or underscore, and to unify names used in similar contexts, which
rule-based code formatters simply cannot do. Intuitively, NATU-
RALIZE works by identifying identiﬁer names or formatting choices
that are surprising according to a probability distribution over code
text. When surprised, NATURALIZE determines if it is sufﬁciently
conﬁdent to suggest a renaming or reformatting that is less surpris-
ing; it uniﬁes the surprising choice with one that is preferred in
similar contexts elsewhere in its training set. NATURALIZE isnot
1Prescriptivism is the attempt to specify rules for correct style in
language, e.g., Strunk and White [67]. Modern linguists studiously
avoid prescriptivist accounts, observing that many such rules are
routinely violated by noted writers.
1arXiv:1402.4182v3  [cs.SE]  7 Apr 2014automatic; it assists a developer, since its suggestions, both renam-
ing and even formatting, as in Python or Apple’s aforementioned
SSL bug [7, 40], are potentially semantically disruptive and must
be considered and approved. NATURALIZE ’s suggestions enable
a range of new tools to improve developer productivity and code
quality: 1) A pre-commit script that rejects commits that excessively
disrupt a codebase’s conventions; 2) A tool that converts the inferred
conventions into rules for use by a code formatter; 3) An Eclipse
plugin that a developer can use to check whether her changes are
unconventional; and 4) A style proﬁler that highlights the stylistic
inconsistencies of a code snippet for a code reviewer.
NATURALIZE draws upon a rich body of tools from statistical
natural language processing (NLP), but applies these techniques in
a different. NLP focuses on understanding andgenerating language,
but does not ordinarily consider the problem of improving existing
text. The closest analog is spelling correction, but that problem is
easier because we have strong prior knowledge about common types
of spelling mistakes. An important conceptual dimension of our
suggestion problems also sets our work apart from mainstream NLP.
In code, rare names often usefully signify unusual functionality,
and need to be preserved. We call this the sympathetic uniqueness
principle (SUP): unusual names should be preserved when they
appear in unusual contexts. We achieve this by exploiting a special
token UNKthat is often used to represent rare words that do not
appear in the training set. Our method incorporates SUP through a
clean, straightforward modiﬁcation to the handling of UNK. Because
of the Zipﬁan nature of language, UNKappears in unusual contexts
and identiﬁes unusual tokens that should be preserved. Section 4
demonstrates the effectiveness of this method at preserving such
names. Additionally, handling formatting requires a simple, but
novel, method of encoding formatting.
AsNATURALIZE detects identiﬁers that violate code conventions
and assists in renaming, the most common refactoring [50], it is the
ﬁrst tool we are aware of that uses NLP techniques to aid refactoring.
The techniques that underlie NATURALIZE are language indepen-
dent and require only identifying identiﬁers, keywords, and opera-
tors, a much easier task than specifying grammatical structure. Thus,
NATURALIZE is well-positioned to be useful for domain-speciﬁc or
esoteric languages for which no convention enforcing tools exist or
the increasing number of multi-language software projects such as
web applications that intermix Java, css,html , and JavaScript.
To the best of the authors’ knowledge, this work is the ﬁrst to
address the coding convention inference problem, to suggest names
and formatting to increase the stylistic coherence of code, and to
provide tooling to that end. Our contributions are:
•We built NATURALIZE , the ﬁrst framework that presents a solu-
tion to the coding convention inference problem for local conven-
tions, including identiﬁer naming and formatting, and suggests
changes to increase a codebase’s adherence to its own conven-
tions;
•We offer four tools, built on NATURALIZE , all focused on release
management, an under-tooled phase of the development process.
•NATURALIZE 1) achieves 94% accuracy in its top suggestions
for identiﬁer names and 2) never drops below a mean accuracy
of 96% when making formatting suggestions; and
•We demonstrate that coding conventions are important to soft-
ware teams, by showing that 1) empirically, programmers en-
force conventions heavily through code review feedback and cor-
rective commits, and 2) patches that were based on NATURAL -
IZEsuggestions have been incorporated into 5of the most pop-
ular open source Java projects on GitHub — of the 18patches
that we submitted, 14were accepted.Tools are available at groups.inf.ed.ac.uk/naturalize .
2. MOTIV ATING EXAMPLE
Both industrial and open source developers often submit their
code for review prior to check-in [61]. Consider the example of
the class shown in Figure 1 which is part of a change submitted for
review by a Microsoft developer on February 17th, 2014. While
there is nothing functionally wrong with the class, it violates the
coding conventions of the team. A second developer reviewed the
change and suggested that res andstr do not convey parameter
meaning well enough, the constructor line is much too long and
should be wrapped. In the checked in change, all of these were
addressed, with the parameter names changed to queryResults
andqueryStrings .
Consider a scenario in which the author had access to NATU-
RALIZE . The author might highlight the parameter names and ask
NATURALIZE to evaluate them. At that point it would have not only
have identiﬁed res andstr as names that are inconsistent with the
naming conventions of parameters in the codebase, but would also
have suggested better names. The author may have also thought
to himself “Is the constructor on line 3 too long?” or “Should the
empty constructor body be on it’s own line and should it have a
space inside?” Here again, NATURALIZE would have provided im-
mediate, valuable answers based on the the conventions of the team.
NATURALIZE would indicate that the call to the base constructor
should be moved to the next line and indented to be consonant with
team conventions and that in this codebase empty method bodies do
not need their own lines. Furthermore it would indicate that some
empty methods contain one space between the braces while others
do not, so there is no implicit convention to follow. After querying
NATURALIZE about his stylistic choices, the author can then be
conﬁdent that his change is consistent with the norms of the team
and is more likely to be approved during review. Furthermore, by
leveraging NATURALIZE , fellow project members wouldn’t need to
be bothered by questions about conventions, nor would they need
to provide feedback about conventions during review. We have
observed that such scenarios occur in open source projects as well.
2.1 Use Cases and Tools
Coding conventions are critical during release management, which
comprises committing, reviewing, and promoting (including re-
leases) changes, either patches or branches. This is when a coder’s
idiosyncratic style, isolated in her editor during code composition,
comes into contact with the styles of others. The outcome of this
interaction strongly impacts the readability, and therefore the main-
tainability, of a codebase. Compared to other phases of the develop-
ment cycle like editing, debugging, project management, and issue
tracking, release management is under-tooled. Code conventions are
particularly pertinent here, and lead us to target three use cases: 1) a
developer preparing an individual commit or branch for review or
promotion; 2) a release engineer trying to ﬁlter out needless stylistic
diversity from the ﬂood of changes; and 3) a reviewer wishing to
consider how well a patch or branch obeys community norms.
Any code modiﬁcation has a possibility of introducing bugs [3,
51]. This is certainly true of a system, like NATURALIZE , that is
based on statistical inference, even when (as we always assume) all
ofNATURALIZE ’s suggestions are approved by a human. Because of
this risk, the gain from making a change must be worth its cost. For
this reason, our use cases focus on times when the code is already
being changed. To support our use cases, we have built four tools:
devstyle Aplugin for Eclipse IDE that gives suggestions for
identiﬁer renaming and formatting both for a single identiﬁer
21public class ExecutionQueryResponse : ExecutionQueryResponseBasic<QueryResults>
2{
3 public ExecutionQueryResponse(QueryResults res, IReadOnlyCollection<string> str, ExecutionStepMetrics metrics) : base(res, str, metrics) { }
4}
Figure 1: A C# class added by a Microsoft developer that was modiﬁed due to requests by a reviewer before it was checked in.
Figure 2: A screenshot of the devstyle Eclipse plugin. The user
has requested suggestion for alternate names of the each argument.
or format point and for the identiﬁers and formatting in a
selection of code.
styleprofile Acode review assistant that produces a proﬁle
that summarizes the adherence of a code snippet to the coding
conventions of a codebase and suggests renaming and format-
ting changes to make that snippet more stylistically consistent
with a project.
genrule Arule generator for Eclipse’s code formatter that gen-
erates rules for those conventions that NATURALIZE has in-
ferred from a codebase.
stylish? A high precision pre-commit script forGit that re-
jects commits that have highly inconsistent and unnatural
naming or formatting within a project.
Thedevstyle plugin offers two types of suggestions, single
point suggestion under the mouse pointer and multiple point sugges-
tion via right-clicking a selection. A screenshot from devstyle
is shown in Figure 2. For single point suggestions, devstyle
displays a ranked list of alternatives to the selected name or format.
Ifdevstyle has no suggestions, it simply ﬂashes the current name
or selection. If the user wishes, she selects one of the suggestions.
If it is an identiﬁer renaming, devstyle renames alluses, within
scope, of that identiﬁer under its previous name. This scope traversal
is possible because our use cases assume an existing and compiled
codebase. Formatting changes occur at the suggestion point. Mul-
tiple point suggestion returns a style proﬁle , a ranked list of the
topkmost stylistically surprising naming or formatting choices in
the current selection that could beneﬁt from reconsideration. By
default,k= 5based on HCI considerations [23, 48]. To accept a
suggestion here, the user must ﬁrst select a location to modify, then
select from among its top alternatives. The styleprofile tool
outputs a style proﬁle. genrule (Section 3.5) generates settings
for the Eclipse code formatter. Finally, stylish? is a ﬁlter that
uses Eclipse code formatter with the settings from genrule to
accept or reject a commit based on its style proﬁle.
NATURALIZE uses an existing codebase, called a training corpus,
as a reference from which to learn conventions. Commonly, the
training corpus will be the current codebase, so that NATURALIZE
learns domain-speciﬁc conventions related to the projects. Alterna-
tively, NATURALIZE comes with a pre-packaged suggestion model
that has been trained on a corpus of popular, vibrant projects that
presumably embody good coding conventions. Developers can use
this engine if they wish to increase their codebase’s adherence to
a larger community’s consensus on best practice. Here, again, we
avoid normative comparison of coding conventions, and do not force
the user to specify their desired conventions explicitly. Instead, the
user speciﬁes a training corpus, and this is used as an implicit source
of desired conventions. The NATURALIZE framework and tools are
available at groups.inf.ed.ac.uk/naturalize .3. THE N ATURALIZE FRAMEWORK
In this section, we introduce the generic architecture of NATU-
RALIZE , which can be applied to a wide variety of different types
of conventions. Figure 3 illustrates its architecture. The input is a
code snippet to be naturalized. This snippet is selected based on
the user input, in a way that depends on the particular tool in ques-
tion. For example, in devstyle , if a user selects a local variable
for renaming, the input snippet would contain all AST nodes that
reference that variable (Section 3.3). The output of NATURALIZE
is a short list of suggestions, which can be ﬁltered, then presented
to the programmer. In general, a suggestion is a set of snippets that
may replace the input snippet. The list is ranked by a naturalness
score that is deﬁned below. Alternately, the system can return a
binary value indicating whether the code is natural, so as to support
applications such as stylish? . The system makes no suggestion
if it deems the input snippet to be sufﬁciently natural, or is unable to
ﬁnd good alternatives. This reduce the “Clippy effect” where users
ignore a system that makes too many bad suggestions2. In the next
section, we describe each element in the architecture in more detail.
Terminology A language model (LM) is a probability distribution
over strings. Given any string x=x0,x1...x M, where each xiis
a token, a LM assigns a probability P(x).LetGbe the grammar of
a programming language. We use xto denote a snippet, that is, a
stringxsuch thatαxβ∈L(G)for some strings α,β. We primarily
consider snippets that that are dominated by a single node in the
AST. We use xto denote the input snippet to the framework, and
y,zto denote arbitrary snippets3.
3.1 The Core of N ATURALIZE
The architecture contains two main elements: proposers and the
scoring function. The proposers modify the input code snippet to
produce a list of suggestion candidates that can replace the input
snippet. In the example from Figure 1, each candidate replaces all
occurrences of res with a different name used in similar contexts
elsewhere in the project, such as results orqueryResults . In
principle, many implausible suggestions could ensue, so, in practice,
proposers contain ﬁltering logic.
Ascoring function sorts these candidates according to a measure
of naturalness. Its input is a candidate snippet, and it returns a
real number measuring naturalness. Naturalness is measured with
respect to a training corpus that is provided to NATURALIZE — thus
allowing us to follow our guiding principle that naturalness must
be measured with respect to a particular codebase. For example,
the training corpus might be the set of source ﬁles Afrom the
current application. A powerful way to measure the naturalness
of a snippet is provided by statistical language modeling. We use
PA(y)to indicate the probability that the language model P, which
has been trained on the corpus A, assigns to the string y. The
key intuition is that an LM PAis trained so that it assigns high
probability to strings in the training corpus, i.e., snippets with higher
log probability are more like the training corpus, and presumably
more natural. There are several key reasons why statistical language
models are a powerful approach for modeling coding conventions.
2In extreme cases, such systems can be so widely mocked that they
are publicly disabled by the company’s CEO in front of a cheering
audience: http://bit.ly/pmHCwI .
3The application of NATURALIZE to academic papers in software
engineering is left to future work.
3Training Corpus  (other code from project)Code for ReviewCandidatesTop SuggestionsScoring Function(ngram language model, SVM)Proposers(rename identiﬁers, add formatting)
public void testRunReturnsResult() {  PrintStream oldOut = System.out;  System.setOut(new PrintStream(     new OutputStream() {          @Override          public void write(int arg0) throws IOException {          }        }      ));  try {    TestResult result = junit.textui.TestRunner.run(new TestSuite());    assertTrue(result.wasSuccessful());  } finally {    System.setOut(oldOut);  }}public void testRunReturnsResult() {  PrintStream oldOut = System.out;  System.setOut(new PrintStream(     new OutputStream() {          @Override          public void write(int arg0) throws IOException {          }        }      ));  try {    TestResult result = junit.textui.TestRunner.run(new TestSuite());    assertTrue(result.wasSuccessful());  } finally {    System.setOut(oldOut);  }}public void testRunReturnsResult() {  PrintStream oldOut = System.out;  System.setOut(new PrintStream(     new OutputStream() {          @Override          public void write(int arg0) throws IOException {          }        }      ));  try {    TestResult result = junit.textui.TestRunner.run(new TestSuite());    assertTrue(result.wasSuccessful());  } finally {    System.setOut(oldOut);  }}public void testRunReturnsResult() {  PrintStream oldOut = System.out;  System.setOut(new PrintStream(     new OutputStream() {          @Override          public void write(int arg0) throws IOException {          }        }      ));  try {    TestResult result = junit.textui.TestRunner.run(new TestSuite());    assertTrue(result.wasSuccessful());  } finally {    System.setOut(oldOut);  }}public void testRunReturnsResult() {  PrintStream oldOut = System.out;  System.setOut(new PrintStream(     new OutputStream() {          @Override          public void write(int arg0) throws IOException {          }        }      ));  try {    TestResult result = junit.textui.TestRunner.run(new TestSuite());    assertTrue(result.wasSuccessful());  } finally {    System.setOut(oldOut);  }}public void testRunReturnsResult() {  PrintStream oldOut = System.out;  System.setOut(new PrintStream(     new OutputStream() {          @Override          public void write(int arg0) throws IOException {          }        }      ));  try {    TestResult result = junit.textui.TestRunner.run(new TestSuite());    assertTrue(result.wasSuccessful());  } finally {    System.setOut(oldOut);  }}public void testRunReturnsResult() {  PrintStream oldOut = System.out;  System.setOut(new PrintStream(     new OutputStream() {          @Override          public void write(int arg0) throws IOException {          }        }      ));  try {    TestResult result = junit.textui.TestRunner.run(new TestSuite());    assertTrue(result.wasSuccessful());  } finally {    System.setOut(oldOut);  }}Figure 3: The architecture of NATURALIZE : a framework for learning coding conventions. A contiguous snippet of code is selected for review
through the user interface. A set of proposers returns a set of candidates, which are modiﬁed versions of the snippet, e.g., with one local
variable renamed. The candidates are ranked by a scoring function , such as an n-gram language model, which returns a small list of top
suggestions to the interface, sorted by naturalness.
First, probability distributions provide an easy way to represent soft
constraints about conventions. This allows us to avoid many of the
pitfalls of inﬂexible, rule-based approaches. Second, because they
are based on a learning approach, LMs can ﬂexibly adapt to the
conventions in a new project. Intuitively, because PAassigns high
probability to strings t∈Athat occur in the training corpus, it also
assigns high probability to strings that are similar to those in the
corpus. So the scoring function stends to favor snippets that are
stylistically consistent with the training corpus.
We score the naturalness of a snippet y=y1:Nas
s(y,PA) =1
NlogPA(y); (1)
that is, we deem snippets that are more probable under the LM as
more natural in the application A. Equation 1 is cross-entropy multi-
plied by - 1to makesa score, where s(x)>s(y)impliesxis more
natural thany. Where it creates no confusion, we write s(y), eliding
the second argument. When choosing between competing candidate
snippetsyandz, we need to know not only which candidate the LM
prefers, but how “conﬁdent” it is. We measure this by a gap function
g, which is the difference in scores g(y,z,P ) =s(y,P)−s(z,P).
Becausesis essentially a log probability, gis the log ratio of proba-
bilities between yandz. For example, when g(y,z)>0the snippet
yis more natural — i.e., less surprising according to the LM — and
thus is a better suggestion candidate than z. Ifg(y,z) = 0 then
both snippets are equally natural.
Now we deﬁne the function suggest (x,C,k,t )that returns the
top candidates according to the scoring function. This function
returns a list of top candidates, or the empty list if no candidates are
sufﬁciently natural. The function takes four parameters: the input
snippetx, the listC= (c1,c2,...c r)of candidate snippets, and
two thresholds: k∈N, the maximum number of suggestions to
return, andt∈R,a minimum conﬁdence value. The parameter
kcontrols the size of the ranked list that is returned to the user,
whiletcontrols the suggestion frequency , that is, how conﬁdent
NATURALIZE needs to be before it presents any suggestions to
the user. Appropriately setting tallows NATURALIZE to avoid the
Clippy effect by making no suggestion rather than a low quality one.
Below, we present an automated method for selecting t.
Thesuggest function ﬁrst sorts C= (c1,c2,...c r), the candi-
date list, according to s, sos(c1)≥s(c2)≥...≥s(cr). Then,
it trims the list to avoid overburdening the user: it truncates Cto
include only the top kelements, so that length (C) = min{k,r}.
and removes candidates ci∈Cthat are not sufﬁciently more natural
than the original snippet; formally, it removes all cifromCwhere
g(ci,x)<t. Finally, if the original input snippet xis the highest
ranked inC,i.e., ifc1=x,suggest ignores the other suggestions,setsC=∅to decline to make a suggestion, and returns C.
Binary Decision If an accept/reject decision on the input xis
required, e.g., as in stylish? ,NATURALIZE must collectively
consider all of the locations in xat which it could make suggestions.
We propose a score function for this binary decision that measures
how good is the best possible improvement that NATURALIZE is
able to make. Formally, let Lbe the set of locations in xat which
NATURALIZE is able to make suggestions, and for each /lscript∈L, let
C/lscriptbe the system’s set of suggestions at /lscript. In general, C/lscriptcontains
name or formatting suggestions. Recall that Pis the language model.
We deﬁne the score
G(x,P) = max
/lscript∈Lmax
c∈C/lscriptg(c,x). (2)
IfG(x,P)>T, then NATURALIZE rejects the snippet as being
excessively unnatural. The threshold Tcontrols the sensitivity of
NATURALIZE to unnatural names and formatting. As Tincreases,
fewer input snippets will be rejected, so some unnatural snippets
will slip through, but as compensation the test is less likely to reject
snippets that are in fact well-written.
Setting the Conﬁdence Threshold The thresholds in the suggest
function and the binary decision function are on log probabilities
of strings, which can be difﬁcult for users to interpret. Fortunately
these can be set automatically using the false positive rate (FPR) ,
the proportion of snippets xthat in fact follow convention but that
the system erroneously rejects. We would like the FPR to be as small
as possible, but unless we wish the system to make no suggestions
at all, we must accept some false positives. So instead we set a
maximum acceptable FPR α, and we search for a threshold Tthat
ensures that the FPR of NATURALIZE is at mostα. This is a similar
logic to statistical hypothesis testing. To make this work, we need
to estimate the FPR for a given T. To do so, we select a random set
of snippets from the training corpus, e.g., random method bodies,
and compute the proportion of the random snippets that are rejected
usingT. Again leveraging our assumption that our training corpus
contains natural code, this proportion estimates the FPR. We use
a grid search [11] to ﬁnd the greatest value of T < α , the user-
speciﬁed acceptable FPR bound.
3.2 Choices of Scoring Function
The generic framework described in Section 3.1 can, in principle,
employ a wide variety of machine learning or NLP methods for
its scoring function. Indeed, a large portion of the statistical NLP
literature focuses on probability distributions over text, including
language models, probabilistic grammars, and topic models. Very
few of these models have been applied to code; exceptions include
4[4, 35, 46, 49, 53]. We choose to build on statistical language
models, because previous work of Hindle et al. . [35] has shown that
they are particularly able to capture the naturalness of code.
The intuition behind language modeling is that since there is
an inﬁnite number of possible strings, obviously we cannot store
a probability value for every one. Different LMs make different
simplifying assumptions to make the modeling tractable, and will
determine the types of coding conventions that we are able to infer.
One of the most effective practical LMs is the n-gram language
model.N-gram models make the assumption that the next token
can be predicted using only the previous n−1tokens. Formally, the
probability of a token ym, conditioned on all of the previous tokens
y1...y m−1, is a function only of the previous n−1tokens. Under
this assumption, we can write
P(y1...y M) =M/productdisplay
m=1P(ym|ym−1...y m−n+1). (3)
To use this equation we need to know the conditional probabilities
P(ym|ym−1...y m−n+1)for each possible n-gram. This is a table
ofVnnumbers, where Vis the number of possible lexemes. These
are the parameters of the model that we learn from the training
corpus. The simplest way to estimate the model parameters is to
setP(ym|ym−1...y m−n+1)to the proportion of times that ymfol-
lowsym−1...y m−n+1. However, in practice this simple estimator
does not work well, because it assumes that n-grams that do not
occur in the training corpus have zero probability. Instead, n-gram
models are trained using smoothing methods [22]. In our work, we
use Katz smoothing.
Cross-Project Language Models As we show shortly, an n-
gram model is a good scoring function for making suggestions, but
we can do better. LMs are ﬂexible and can exploit other sources
of information beyond local context when scoring a lexeme. For
example, if we have a global set of projects Gchosen for their good
coding style, we would like to transfer information about the style of
Gto the current application. We do this by using a cross-project LM .
We train two separate n-gram models, a global model PGtrained
onG, and a local model trained only on the current application A.
Then we create a new LM PCPby averaging:
PCP(y) =λPG(y) + (1−λ)PA(y), (4)
whereλ∈[0,1]is a parameter that can be set using a development
set. The cross-project model transfers information from G, because
for ann-gramzto have high probability according to PCP, it must
have high probability according to both PGandPA.
Incorporating Syntactic Context Our approach can also ﬂexibly
incorporate higher-level information, such as syntactic context. In
Java, for example, type information is useful for predicting identiﬁer
names. A simple way to exploit this is to learn a probability model
over tokens based on context. Given an AST Tand a location
iin the snippet y, letφ(T,i)be a function that returns a feature
vector describing the current syntactic context, without including
the identity of the token yi. For example, if yinames a variable,
φ(T,i)might include the variable’s type, determined statically via
compilation. More generally, φcould include information about
the current nesting level, whether the token is within an anonymous
inner class, and so on. We learn a probabilistic classiﬁer PSYN(yi|ti)
on the same training set as for the LM. Then we can deﬁne a new
score function
s(y|Lv) = logP(y) +1
NN/summationdisplay
i=1log (PSYN(yi|φ(T,i))), (5)which combines the n-gram model Pwith the classiﬁer PSYNthat
incorporates syntactic context.
Implementation When ann-gram model is used, we can compute
the gap function g(y,z)very efﬁciently. This is because when
gis used within suggest , ordinarily the strings yandzwill be
similar, i.e., the input snippet and a candidate revision. The key
insight is that in an n-gram model, the probability P(y)of a snippet
y= (y1y2...y N)depends only on the multiset of n-grams that
occur iny, that is,
NG(y) ={yiyi+1...y i+n−1|0≤i≤N−(n−1)}.(6)
An equivalent way to write a n-gram model is
P(y) =/productdisplay
a1a2...an∈NG(y)P(an|a1,a2,...a n−1). (7)
Since the gap function is g(y,z) = log[P(y)/P(z)],anyn-grams
that are members both of NG(y)andNG(z)cancel out in the
quotient, and to compute g, we only need to consider those n-grams
that are not in NG(y)∩NG(z). Intuitively, this means that to
compute the gap function g(y,z), we need to examine the n-grams
around the locations where the snippets yandzdiffer. This is a very
useful optimization if yandzare long snippets that differ in only a
few locations.
When training an LM, we take measures to deal with rarelexemes,
since, by deﬁnition, we do not have much data about them. We use
a preprocessing step — a common strategy in language modeling
— that builds a vocabulary with all the identiﬁers that appear more
than once in the training corpus. Let count (v,b)return the number
of appearances of token vin the codebase b. Then, if a token has
count (v,b)≤1we convert it to a special token, which we denote
UNK. Then we train the n-gram model as usual. The effect is that
theUNKtoken becomes a catchall that means the model expects to
see a rare token, even though it cannot be sure which one.
3.3 Suggesting Natural Names
In this section, we instantiate the core NATURALIZE framework
for the task of suggesting natural identiﬁer names. We start by
describing the single suggestion setting. For concreteness, imagine
a user of the devstyle plugin, who selects an identiﬁer and asks
devstyle for its top suggestions. It should be easy to see how
this discussion can be generalized to the other use cases described in
Section 2.1. Let vbe the lexeme selected by the programmer. This
lexeme could denote a variable, a method call, or a type.
When a programmer binds a name to an identiﬁer and then uses
it, she implicitly links together all the locations in which that name
appears. Let Ldenote this set of locations, that is, the set of lo-
cations in the current scope in which the lexeme vis used. For
example, ifvdenotes a local variable, then Lvwould be the set
of locations in which that local is used. Now, the input snippet is
constructed by ﬁnding a snippet that subsumes all of the locations
inLv. Speciﬁcally, the input snippet is constructed by taking the
lowest common ancestor in AST of the nodes in Lv.
The proposers for this task retrieve a set of alternative names to v,
which we denote Av, by retrieving other names that have occurred
in the same contexts in the training set. To do this, for every location
/lscript∈Lvin the snippet x, we take a moving window of length n
around/lscriptand copy all the n-gramswithat contain that token. Call
this setCvthe context set, i.e., the set ofn-gramswiofxthat
contain the token v. Now we ﬁnd all n-grams in the training set
that are similar to an n-gram inCvbut that have some other lexeme
substituted for v. Formally, we set Avas the set of all lexemes
v/primefor whichαvβ∈Cvandαv/primeβoccurs in the training set. This
5guarantees that if we have seen a lexeme in at least one similar
context, we place it in the alternatives list. Additionally, we add
toAvthe special UNKtoken; the reason for this is explained in a
moment. Once we have constructed the set of alternative names, the
candidates are a list Svof snippets, one for each v/prime∈Av, in which
all occurrences of vinxare replaced with v/prime.
The scoring function can use any model PA, such as the n-gram
model (Equation 3). N-gram models work well because, intuitively,
they favors names that are common in the context of the input
snippet. As we demonstrate in Section 4, this does notreduce to
simply suggesting the most common names, such as iandj. For
example, suppose that the system is asked to propose a name for
res in line 3 of Figure 1. The n-gram model is highly unlikely to
suggest i, because even though the name iis common, the trigram
“QueryResults i , ” is rare.
An interesting subtlety involves names that actually should be
unique. Identiﬁer names have a long tail, meaning that most names
are individually uncommon. It would be undesirable to replace every
rare name with common ones, as this would violate the sympathetic
uniqueness principle. Fortunately, we can handle this issue in a
subtle way: Recall from Section 3.1 that, during training of the
n-gram language model, we convert rare names into the special
UNKtoken. When we do this, UNKexists as a token in the LM, just
like any other name. So we simply allow NATURALIZE to return
UNKas a suggestion, exactly the same as any other name. Returning
UNKas a suggestion means that the model expects that it would be
natural to use a rare name in the current context. The reason that this
preserves rare identiﬁers is that the UNKtoken occurs in the training
corpus speciﬁcally in unusual contexts where more common names
were not used. Therefore, if the input lexeme voccurs in an unusual
context, this context is more likely to match that of UNKthan of any
of the more common tokens.
Multiple Point Suggestion It is easy to adapt the system above
to the multiple point suggestion task. Recall (Section 2.1) that this
task is to consider the set of identiﬁers that occur in a region xof
code selected by the user, and highlight the lexemes that are least
natural in context. For single point suggestion, the problem is to rank
different alternatives, e.g., different variable names, for the same
code location, whereas for multiple point suggestion, the problem is
to rank different code locations against each other according to how
much they would beneﬁt from improvement. In principle, a score
function could be good at the single source problem but bad at the
multiple source problem, e.g., if the score values have a different
dynamic range when applied at different locations.
We adapt NATURALIZE slightly to address the multiple point
setting. For all identiﬁer names vthat occur in x, we ﬁrst compute
the candidate suggestions Svas in the single suggestion case. Then
the full candidate list for the multiple point suggestion is S=
∪v∈xSv; each candidate arises from proposing a change to one
name inx. For the scoring function, we need to address the fact that
some names occur more commonly in xthan others, and we do not
want to penalize names solely because they occur more often. So
we normalize the score according to how many times a name occurs.
Formally, a candidate c∈Sthat has been generated by changing a
namev, we use the score function s/prime(c) =|Cv|−1s(c).
3.4 Suggesting Natural Formatting
We apply NATURALIZE to build a language-agnostic code format-
ting suggester that automatically and adaptively learns formatting
conventions and generates rule for use by code formatters, like the
Eclipse formatter. N-gram models work over token streams; for
then-gram instantiation of NATURALIZE to provide formatting sug-
gestions, we must convert whitespace into tokens. We change the5INDENT3s
1n@SPACE0ID SPACE1spublic SPACE1svoid
6INDENT0
1nID SPACE0(SPACE0ID SPACE1sID SPACE0)
SPACE1sthrows SPACE1sID SPACE1s{
7INDENT0
1n}
8INDENT−3s
1n}
Figure 4: The formatting tokenization of lines from a snippet of
code of TextRunnerTest.java inJUnit .
tokenizer to encode contiguous whitespace into tokens using the
grammar
S::=T W S |/epsilon1
W::=SPACEspace/tab|INDENTspace/tabs
lines
T::=ID|LIT |{|}|.|(|)|<keywords> .
Figure 4 shows a sample tokenization of a code snippet drawn from
JUnit . We collapse all identiﬁers to a single IDtoken and all
literals to a single LIT token because we presume that the actual
identiﬁer and literal lexemes do not convey information about for-
matting. Whitespace at the start of a line determines the indentation
level, which usually signiﬁes nesting. We replace it with a spe-
cial tokens INDENT , along with metadata encoding the increase of
whitespace (that may be negative or zero) relative to the previous
line. We also annotate INDENT with the number of new lines before
any proceeding (non-whitespace) token. This captures code that is
visually separated with at least one empty line. In Figure 4, line 5
indents by 3 spaces in the directly next line. Whitespace within a
line, controls the appearance of operators and punctuation: “ if(
vs.if ( ” and “ x<y vs.x < y ”. We encode this whitespace into
the special token SPACE , along the number of spaces/tabs that it
contains. If between two non-whitespace tokens there is no space
we add a SPACE0. Finally, we annotate all tokens of type Twith
their size (number of characters) and the current column of that to-
ken. This helps us capture the increasing probability of an INTENT
when we reach a large line length. To reduce sparsity we quantize
these two annotations into buckets of size q.
Although hard to see, an empty exception handler is straddling
lines 6–7 in Figure 4. If a programmer asks if this is convention,
NATURALIZE considers alternatives for the underlined token. We
train the LM over the whitespace-modiﬁed tokenizer, then, since
the vocabulary of whitespace tokens (assuming bounded numbers
in the metadata) is small, we rank all whitespace token alternatives
according to the scoring function (Section 3.2).
3.5 Converting Conventions into Rules
NATURALIZE can convert the conventions it infers from a code-
base into rules for a code formatter. We formalize a code formatter’s
rule as the set of settings S={s1,s2,...,s n}andC, a set of con-
straints over the elements in S. For example sicould be a boolean
that denotes “ {must be on the same line as its function signature”
andsjmight be the number of spaces between the closing )and
the{. ThenCmight contain (sj≥0)→si∧(sj<0)→¬si.
To extract rules, we handcraft a set of minimal code snippets that
exhibit each different setting of si,∀si∈S. After training NAT-
URALIZE on a codebase, we apply it to these snippets. Whenever
NATURALIZE is conﬁdent enough to prefer one to the other, we infer
the appropriate setting, otherwise we leave the default untouched,
which may be to ignore that setting when applying the formatter.
4. EV ALUATION
We now present an evaluation of the value and effectiveness of
NATURALIZE . This evaluation ﬁrst presents two empirical studies
that show NATURALIZE solves a real world problem that program-
mers care about (Section 4.1). These studies demonstrate that 1)
6Name Forks Watchers Commit Pull Request Description
elasticsearch 1009 4448 af17ae55 #5075mergedREST Search Engine
libgdx 1218 1470 a42779e9 #1400mergedGame Development Framework
netty 683 1940 48eb73f9did not submitNetwork Application Framework
platform_frameworks_base 947 1051 a0b320a6did not submitAndroid Base Framework
junit* 509 1796 d919bb6d #834mergedTesting Framework
wildﬂy 845 885 9d184cd0did not submitJBoss Application Server
hudson 997 215 be1f8f91did not submitContinuous Integration Server
android-bootstrap 360 1446 e2cde337did not submitAndroid Application Template
k-9 583 960 d8030eaa #454mergedAndroid Email Client
android-menudrawer 422 1138 96cdcdcc #216openAndroid Menu Implementation
*Used as a validation project for tuning parameters.
Figure 5: Open-source Java projects used for evaluation. Ordered by popularity.
programmers do not always adhere to coding conventions and yet 2)
that project members care enough about them to correct such vio-
lations. Then, we move on to evaluating the suggestions produced
byNATURALIZE . We perform an extensive automatic evaluation
(Section 4.2) which veriﬁes that NATURALIZE produces natural
suggestions that matches real code. Automatic evaluation is a stan-
dard methodology in statistical NLP [56, 45], and is a vital step
when introducing a new research problem, because it allows future
researchers to test new ideas rapidly. This evaluation relies on per-
turbation: given code text, we perturb its identiﬁers or formatting,
then check if NATURALIZE suggests the original name or formatting
that was used. Furthermore, we also employ the automatic evalu-
ation to show that NATURALIZE is robust to low quality corpora
(Section 4.3).
Finally, to complement the automatic evaluation, we perform
two qualitative evaluations of the effectiveness of NATURALIZE
suggestions. First, we manually examine the output of NATURAL -
IZE, showing that even high quality projects contain many entities
for which other names can be reasonably considered (Section 4.4).
Finally, we submitted patches based on NATURALIZE suggestions
(Section 4.5) to 5of the most popular open source projects on
GitHub — of the 18patches that we submitted, 12were accepted.
Methodology Our corpus is a set of well-known open source Java
projects. From GitHub4, we obtained the list of all Java projects that
are not forks and scored them based on their number of “watchers”
and forks. The mean number of watchers and forks differ, so, to
combine them, we assumed these numbers follow the normal distri-
bution and summed their z-scores. For these evaluations reported
here, we picked the top 10 scoring projects that are notin the train-
ing set of the GitHub Java Corpus [4]. Our original intention was to
also demonstrate cross-project learning, but have no space to report
these ﬁnding. Figure 5 shows the selected projects.
Like any experimental evaluation, our results are not immune to
the standard threat to external validity that, if poorly chosen, the
evaluation corpus may not be representative of Java programs, let
alone programs in other languages. Interestingly, this threat does
not extend to the training corpus, because the whole point is to bias
NATURALIZE toward the conventions that govern the training set.
Rather, our interest is to ensure that NATURALIZE ’s performance
on our evaluation corpus matches that of projects overall, which is
why we took such care in constructing our corpus.
Our evaluations use leave-one-out cross validation. We test on
each ﬁle in the project, training our models on the remaining ﬁles.
This reﬂects the usage scenario that we recommend in practice. We
report the average performance over all test ﬁles. For an LM, we
have used a 5-gram model, chosen via calibration on the validation
project JUnit . We picked JUnit as the validation project because
4http://www.github.com , on 21 August, 2013.of its medium size.
4.1 The Importance of Coding Conventions
To assess whether obeying coding conventions, speciﬁcally fol-
lowing formatting and naming conventions, is important to software
teams today, we conducted two empirical studies that we present in
this section. But ﬁrst, we posit that coding style is both an important
and a contentious topic. The fact that many languages and projects
have style guides is a testament to this assertion. For example, we
found that the Ruby style guide has at least 167un-merged forks and
the Java GitHub Corpus [4] has 349different .xml conﬁgurations
for the Eclipse formatter.
Commit Messages We manually examined 1,000commit mes-
sages drawn randomly from the commits of eight popular open
source projects looking for mentions of renaming, changing format-
ting, and following other code conventions. We found that 2% of
changes contained formatting improvements, 1% contained renam-
ings, and 4% contained any changes to follow code conventions
(which include formatting and renaming). We observed that not all
commits that contain changes to adhere to conventions mention such
conventions in the commit messages. Thus, our percentages likely
represent lower bounds on the frequency of commits that change
code to adhere to conventions.
Code Review Discussions We also examined discussions that oc-
curred in reviews of source code changes. Code review is practiced
heavily at Microsoft in an effort to ensure that changes are free of
defects and adhere to team standards. Once an author has completed
a change, he creates a code review and sends it to other developers
for review. They then inspect the change, offer feedback, and either
sign off or wait for the author to address their feedback in a subse-
quent change. As part of this process, the reviewers can highlight
portions of the code and begin a discussion (thread) regarding parts
of the change (for more details regarding the process and tools used,
see Bacchelli et al. [10]).
We examined 169code reviews selected randomly across Mi-
crosoft product groups during 2014. Our goal was to include enough
reviews to examine at least 1,000discussion threads. In total, these
169reviews contained 1093 threads. We examined each thread to
determine if it contained feedback related to a) code conventions
in general, b) identiﬁer naming, and c) code formatting. 18% of
the threads examined provided feedback regarding coding conven-
tions of some kind. 9% suggested improvements in naming and 2%
suggested changes related to code formatting (subsets of the 18% ).
In terms of the reviews that contained feedback of each kind, the
proportions are 38%,24%, and 9%.
During February 2014, just over 126,000 reviews were com-
pleted at Microsoft. Thus, based on conﬁdence intervals of these
proportions, between 7,560and18,900reviews received feedback
7Type Reviews (CI) Commits (CI) p−val
Conventions 38% (31%–46%) 4% (3%–6%) p/lessmuch0.01
Naming 24% (17%–31%) 1% (1%–2%) p/lessmuch0.01
Formatting 9% (6%–15%) 2% (1%–3%) p/lessmuch0.01
Figure 6: Percent commits with log messages and reviews that
contained feedback regarding code conventions, identiﬁer naming,
and formatting with 95% conﬁdence intervals in parentheses.
regarding formatting changes that were needed prior to check-in and
between 21,420and39,060reviews resulted in name changes in
just one month.
Figure 6 summarizes our ﬁndings from examining commit mes-
sages and code reviews. We also present 95% conﬁdence intervals
based on the sampled results [25]. These results demonstrate that
changes, to a nontrivial degree, violate coding conventions even af-
ter the authors consider them complete and also that team members
expect that these violations be ﬁxed. We posit that, like defects,
many convention violations die off during the lifecycle of a change,
so that few survive to review and fewer still escape into the repos-
itory. This is because, like defects, programmers notice and ﬁx
many violations themselves during development, prior to review, so
reviewers must hunt for violations in a smaller set, and committed
changes contain still fewer, although this number is nontrivial, as we
show in Section 4.4. Corrections during development are unobserv-
able. However, we can compare convention corrections in review
to corrections after commit. We used a one-sided proportional test
to evaluate if more coding conventions are corrected during review
than after commit. The last column in Figure 6 contains the p-values
for our tests, indicating that the null hypothesis can be rejected with
statistically signiﬁcant support.
4.2 Suggestion
In this section we present an automatic evaluation of NATURAL -
IZE’s suggestion accuracy. First we evaluate naming suggestions
(Section 3.3). We focus on suggesting new names for (1) locals, (2)
arguments, (3) ﬁelds, (4) method calls, and (5) types (class names,
primitive types, and enums) — these are the ﬁve distinct types of
identiﬁers the Eclipse compiler recognizes [26]. Recall from Sec-
tion 3.3 that when NATURALIZE suggests a renaming, it renames
alllocations where that identiﬁer is used at once. Furthermore, as
described earlier, we always use leave-one-out cross validation, so
we never train the language model on the ﬁles for which we are
making suggestions. Therefore, NATURALIZE cannot pick up the
correct name for an identiﬁer from other occurrences in the same
ﬁle; instead, it must generalize by learning conventions from other
ﬁles in the project.
Single Point Suggestion First we evaluate NATURALIZE on the
single point suggestion task, that is, when the user has asked for
naming suggestions for a single identiﬁer. To do this, for each test
ﬁle, for each unique identiﬁer we collect all of the locations where
the identiﬁer occurs and names the same entity, and ask NATURAL -
IZEto suggest a new name, renaming all occurrences at once. We
measure accuracy, that is, the percentage of the time that NATURAL -
IZEcorrectly suggests the original name. This is designed to reﬂect
the typical usage scenario described in Section 2, in which a devel-
oper has made a good faith effort to follow a project’s conventions,
but may have made a few mistakes.
Figure 7 reports on the quality of the suggestions. Each point
on these curves corresponds to a different value of the conﬁdence
thresholdt. Thex-axis shows the suggestion frequency, i.e., at
what proportion of code locations where NATURALIZE is capable
of making a suggestion does it choose to do so. The y-axis showssuggestion accuracy, that is, the frequency at which the true name
is found in the top ksuggestions, for k= 1 andk= 5. Ast
increases, NATURALIZE makes fewer suggestions of higher qual-
ity, so frequency decreases as accuracy increases. These plots are
similar in spirit to precision-recall curves in that curves nearer the
top right corner of the graph are better. Figure 7a, Figure 7b, and
Figure 7c show that NATURALIZE performance varies with both
project and the type of identiﬁers. Figure 7a combines locals, ﬁelds,
and arguments because their performance is similar. NATURALIZE ’s
performance varies across these three categories of identiﬁers be-
cause of the data hungriness of n-grams and because local context is
an imperfect proxy for type constraints or function semantics. The
results show that NATURALIZE effectively avoids the Clippy effect,
because by allowing the system to decline to suggest in a relatively
small proportion of cases, it is possible to obtain good suggestion
accuracy. Indeed, NATURALIZE can achieve 94% suggestion accu-
racy across identiﬁer types, even when forced to make suggestions
at half of the possible opportunities.
Augmented Models Cross-project LMs raise the opportunity
to exploit information across projects to improve NATURALIZE ’s
performance. Indeed, across our corpus, we ﬁnd that on average
14.8% of variable names are shared across projects. This is an
indication that we can transfer useful information about identiﬁer
naming across projects. The cross-project model (Section 3.2) is an
approach to achieving that. Figure 8 shows the percent improvement
in suggestion accuracy at k= 5when using the cross-project scor-
ing model versus the base model. Our evaluation suggest that the
cross-project scoring model yields improvements of about 3% for
variables and types and an improvements of about 10% for method
calls at the suggestion frequency of 60%, indicating that transfer-
ring information is feasible. We note that for smaller suggestion
frequencies this improvement diminishes. We also experimented
with incorporating syntactic context into NATURALIZE , as shown
in Equation 5. Our initial results ﬁnd this strategy yields minor
performance improvements in single variable renaming suggestions
with the best improvement being 2% in the suggestion accuracy
of ﬁeld names. We believe the problem is one of ﬁnding proper
weighting for the priors, which we leave to future work.
Multiple Point Selection To evaluate NATURALIZE ’s accuracy at
multiple point suggestion, e.g., in devstyle orstyleprofile ,
we mimic code snippets in which one name violates the project’s
conventions. For each test snippet, we randomly choose one iden-
tiﬁer and perturb it to a name that does not occur in the project,
compute the style proﬁle, and measure where the perturbed name
appears in the list of suggestions. NATURALIZE ’s recall at rank
k= 7, chosen because humans can take in 7items at a glance [23,
47] is 64.2%. The mean reciprocal rank is 0.47: meaning that, on
average, we return the bad name at position 2.
Single Point Suggestion for Formatting To evaluate NATURAL -
IZE’s performance at making formatting suggestions, we follow the
same procedure as the single-point naming experiment to check if
NATURALIZE correctly recovers the original formatting from the
context. We train a 5-gram language model using the modiﬁed token
stream (q= 20 ) discussed in Section 3.4. We allow the system to
make onlyk= 1suggestions to simplify the UI. We ﬁnd that the
system is extremely effective (Figure 12) at formatting suggestions,
achieving 98% suggestion accuracy even when it is required to re-
format half of the whitespace in the test ﬁle. This is remarkable for
a system that is not provided with any hand-designed rules about
what formatting is desired. Obviously if the goal is to reformat all
the whitespace an entire ﬁle, a rule-based formatter is called for. But
this performance is more than high enough to support our use cases,
80.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
Suggestion Frequency0.60.70.80.91.0Suggestion Accuracy k=1
k=5(a) Variables
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
Suggestion Frequency0.50.60.70.80.91.0Suggestion Accuracy k=1
k=5 (b) Method Calls
0.1 0.2 0.3 0.4 0.5 0.6
Suggestion Frequency0.800.850.900.951.00Suggestion Accuracy k=1
k=5 (c) Typenames
Figure 7: Evaluation of single point suggestions of NATURALIZE , when it is allowed to suggest k= 1andk= 5alternatives. The shaded
area around each curve shows the interquartile range of the suggestion accuracy across the 10 evaluation projects.
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
Suggestion Frequency0.000.020.040.060.080.100.12Suggestion Accuracy
(% Improvement)variables
methods
types
Figure 8: Improvement of suggestion accuracy using cross-project
n-gram model at k= 5
such as providing single point formatting suggestions on demand,
rejecting snippets with unnatural formatting and extracting high
conﬁdence rules for rule-based formatters.
Binary Snippet Decisions Finally, we evaluate the ability of
stylish? to discriminate between code selections that follow
conventions well from those that do not, by mimicking commits that
contain unconventional names or formatting. Uniformly at random,
we selected a set of methods from each project ( 500in total), then
with probability uniform probability (1
3) we either made no changes
or perturbed one identiﬁer or whitespace token to a token in the
n-gram’s vocabulary V. We argue that this method for mimicking
commits is probably a worst case for our method, because the per-
turbed methods will be very similar to existing methods, which we
presume to follow conventions most of the time.
We run stylish? and record whether the perturbed snippet
is rejected based on either its names or its formatting. stylish?
is not made aware what kind of perturbation (if any), identiﬁer or
whitespace, was made to the snippet. Figure 9 reports NATURAL -
IZE’s rejection performance as ROC curves. In each curve, each
point corresponds to a different choice of threshold T, and thex-axis
shows FPR (estimated as in Equation 3.1), and the y-axis shows true
positive rate, the proportion of the perturbed snippets that we cor-
rectly rejected. NATURALIZE achieves high precision thus making it
suitable for use for a ﬁltering pre-commit script. For example, if the
FPR is allowed to be at most 0.05, then we are able to correctly re-
ject 40% of the snippets. The system is somewhat worse at rejecting
snippets whose variable names have been perturbed; in part this is
because predicting identiﬁer names is more difﬁcult than predicting
formatting. New advances in language models for code [46, 53] are
likely to improve these results further. Nonetheless, these results
are promising: stylish? still rejects enough perturbed snippets
that if deployed at 5% FPR, it would enhance convention adherence
with minimal disruption to developers.
0.0 0.2 0.4 0.6 0.8 1.0
FPR0.00.20.40.60.81.0TPR
Stylish
Formatting
IdentiﬁersFigure 9: Evaluation of stylish? tool for rejecting unnatural
changes. To generate unnatural code, we perturb one identiﬁer
or formatting point or make no changes, and evaluate whether
NATURALIZE correctly rejects or accepts the snippet. The graph
shows the receiver operating characteristic (ROC) of this process for
stylish? when using only identiﬁers, only formatting or both.
10−210−1
Percent of junk identiﬁers introduced in corpus10−310−210−1Percent of identiﬁers
renamed to junk names
Figure 10: Is NATURALIZE robust to low-quality corpora? The
x-axis shows percentage of identiﬁers perturbed to junk names to
simulate low quality corpus. The y-axis is percentage of resulting
low quality suggestions. Note log-log scale. The dotted line shows
y=x. The boxplots are across the 10 evaluation projects.
4.3 Robustness of Suggestions
We show that NATURALIZE avoids two potential pitfalls in its
identiﬁer suggestions: ﬁrst, that it does not simply rename all tokens
common “junk” names that appear in many contexts, and second,
that it retains unusual names that signify unusual functionality, ad-
hering to the SUP.
Junk Names A junk name is a semantically uninformative name
used in disparate contexts. It is difﬁcult to formalize this concept:
for instance, in almost all cases, foo andbar are junk names, while
iandj, when used as loop counters, are semantically informative
and therefore not junk. Despite this, most developers “know it when
they see it.” One might at ﬁrst be concerned that NATURALIZE
would often suggest junk names, because junk names appear in
92 4 6 8 10
Threshold t0.20.40.60.81.0% surprising names preservedvariables
methods
typesFigure 11: NATURALIZE does not cause the “heat death” of a
codebase: we evaluate the percent of single suggestions made on
UNKidentiﬁers that preserve the surprising name. The x-axis shows
the value of the threshold t, which controls the suggestion frequency
ofsuggest ; lowertmeans that suggest has less freedom to decline
to make low-quality suggestions.
many different n-grams in the training set. We argue, however,
that in fact the opposite is the case: NATURALIZE actually resists
suggesting junk names. This is because if a name appears in too
many contexts, it will be impossible to predict a unsurprising follow-
up, and so code containing junk names will have lower probability,
and therefore worse score.
To evaluate this claim, we randomly rename variables to junk
names in each project to simulate a low quality project. Notice that
we are simulating a low quality training set, which should be the
worst case for NATURALIZE . We measure how our suggestions
are affected by the proportion of junk names in the training set.
To generate junk variables we use a discrete Zipf’s law with slope
s= 1.08, the slope empirically measured for all identiﬁers in our
evaluation corpus. We veriﬁed the Zipﬁan assumption in previous
work [4]. Figure 10 shows the effect on our suggestions as the eval-
uation projects are gradually infected with more junk names. The
framework successfully avoids suggesting junk names, proposing
them at a lower frequency than they exist in the perturbed codebase.
Sympathetic Uniqueness Surprise can be good in identiﬁers,
where it signiﬁes unusual functionality. Here we show that NATU-
RALIZE preserves this sort of surprise. We ﬁnd all identiﬁers in the
test ﬁle that are unknown to the LM, i.e.are represented by an UNK.
We then plot the percentage of those identiﬁers for which suggest
does notpropose an alternative name, as a function of the threshold
t. As described in Equation 3.1, tis selected automatically, but it is
useful to explore how adherence to the SUP varies as a function of t.
Figure 11 shows that for reasonable threshold values, NATURALIZE
suggests non- UNKidentiﬁers for only a small proportion of the
UNKidentiﬁers (about 20%). This conﬁrms that NATURALIZE does
not cause the “heat death of a codebase” by renaming semantically
rich, surprising names into frequent, low-content names.
4.4 Manual Examination of Suggestions
As a qualitative evaluation of NATURALIZE ’s suggestions, three
human evaluators (three of the authors) independently evaluated the
quality of its suggestions. First, we selected two projects from our
corpus, uniformly at random, then for each we ran styleprofile
on30methods selected uniformly at random to produce suggestions
for all the identiﬁers present in that method. We assigned 20 proﬁles
to each evaluator such that each proﬁle had two evaluators whose
task was to independently determine whether any of the suggested
renamings in a proﬁle were reasonable. The evaluators attempted
to take an evidence based approach, that is, not to simply choose
names that they liked, but to choose names that were consistent
with existing practice in the project. We provided access to the full
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Suggestion Frequency0.940.950.960.970.980.991.00Suggestion AccuracyFigure 12: Evaluation of single point evaluation for formatting.
Onlyk= 1suggestions are allowed in the ranked list. The boxplots
show the variance in performance across the 10 evaluation projects.
source code of each project to the evaluators.
One special issue arose when suggesting changes to method
names. Adapting linguistic terminology to our context, homonyms
are two semantically equivalent functions with distinct names. We
deem NATURALIZE ’s suggested renaming of a method usage to
be reasonable, if the suggestion list contains a partial , not neces-
sarily perfect, homonym; i.e., if it draws the developer’s attention
to another method that is used in similar contexts. Each evaluator
had15minutes to consider each proﬁle and 30minutes to explore
each project before starting on a new project. In fact, the evaluators
required much less time than allocated, averaging about 5minutes
per example. The human evaluations can be found on our project’s
webpage. Surprisingly, given the quality of our evaluation code-
bases, 50% of the suggestions were determined to be useful by both
evaluators. Further, no suggestion works well for everyone; when
we consider NATURALIZE ’s performance in terms of whether at
least one evaluator found a suggestion useful, 63% of the sugges-
tions are useful, with an inter-rater agreement (Cohen’s kappa [21])
ofκ= 0.73. The quality of suggestions is strikingly high given that
these projects are mature, vibrant, high-proﬁle projects.
This provides evidence that the naming suggestions provided by
NATURALIZE are qualitatively reasonable. Of course, an obvious
threat to validity is that the evaluators are not developers of the
test projects, and the developers themselves may well have had
different opinions about naming. For this reason, we also provided
theNATURALIZE naming suggestions to the project developers
themselves, as described in the next section.
4.5 Suggestions Accepted by Projects
We used NATURALIZE ’sstyleprofile tool to identify high-
conﬁdence renamings and submitted 18 of them as patches to the
5evaluation projects that actively use GitHub. Figure 5 shows the
pull request ids and their current status. Four projects merged our
pull requests (14 of 15 commits), while one other ignored them
without comment. Developers in the projects that accepted NATU-
RALIZE ’s patches found the NATURALIZE useful: one said “Wow,
that’s a pretty cool tool!” [31]. JUNITdid not accept two of the
suggested renamings as-is. Instead, the patches sparked a discussion.
Its developers concluded that another name was more meaningful
in one case and that the suggested renaming of another violated
the project’s explicit naming convention: “Renaming etotis no
improvement, because we should consistently use e.” [30]. We then
pointed them to the code locations that supported NATURALIZE ’s
original suggestion. This triggered them to change all the names
that had caused the suggestion in the ﬁrst place — which shows that
NATURALIZE pointed out an inconsistency, previously unnoticed,
that improved the naming in the project. Links to all the discussions
can be found on our project webpage.
105. RELATED WORK
Coding conventions, readability, and identiﬁers have been ex-
tensively studied in the literature. Despite this, NATURALIZE is —
to our knowledge — the ﬁrst to infer coding conventions from a
codebase, spanning naming and formatting.
Coding conventions are standard practice [15, 34]. They facili-
tate consistent measurement and reduce systematic error and gen-
erate more meaningful commits by eliminating trivial convention
enforcing commits [73]. Some programming languages like Java
and Python suggest speciﬁc coding styles [55, 64], while consortia
publish guidelines for others, like C [8, 34].
High quality identiﬁer names lie at the heart of software engi-
neering [6, 16, 20, 24, 42, 66, 69, 54]; they drive code readability
and comprehension [12, 19, 20, 41, 44, 68]. According to Deißen-
böck and Pizka [17], identiﬁers represent the majority (70%) of
source code tokens. Eshkevari et al. [27] explored how identiﬁers
change in code, while Lawrie et al. [41] studied the consistency
of identiﬁer namings. Abebe et al. [1] discuss the importance of
naming to concept location [59]. Caprile and Tonella [20] propose
a framework for restructuring and renaming identiﬁers based on
custom rules and dictionaries. Gupta et al. present part-of-speech
tagging on split multi-word identiﬁers to improve software engineer-
ing tools [33]. Høst and Østvold stem and tag method names, then
learn a mapping from them to a ﬁxed set of predicates over bytecode.
Naming bugs are mismatches between the map and stemmed and
tagged method names and their predicates in a test set [37]. In
contrast, our work considers coding conventions more generally,
and takes a ﬂexible, data-driven approach. Several styles exist for
engineering consistent identiﬁers [14, 20, 24, 65]. Because longer
names are more informative [44], these styles share an agglutination
mechanism for creating multi-word names [5, 60].
Many rule-based code formatters exist but, to our knowledge,
are limited to constraining identiﬁer names to obey constraints like
CamelCase or underscore and cannot handle convention like the
use of ias a loop control variable. Pylint [58] checks if names
match a simple set of regular expressions ( e.g., variable names
must be lowercase); astyle ,aspell and GNU indent [9, 32]
only format whitespace tokens. gofmt formats the code aiming
to “eliminating an entire class of argument” among developers [57,
slide 66] but provide no guidance for naming. Wang et al. have
developed a heuristic-based method to automatically insert blank
lines into methods (vertical spacing) to improve readability [72].
NATURALIZE is unique in that it does not require upfront agreement
on hard rules but learns soft rules that are implicit in a codebase.
The soft rules about which NATURALIZE is highly conﬁdent can be
extracted for by a formatter.
API recommenders, code suggestion and completion systems
aim to help during editing, when a user may not know the name of
an API she needs [63] or that the API call she is making needs to pre-
ceded by another [29, 70, 71, 75]. Code suggestion and completion
tools [18, 35, 52, 53, 62] suggest the next token during editing, often
from a few initial characters. Essentially these methods address a
search problem, helping the developer ﬁnd an existing entity in code.
Our focus is instead on release management and improving code’s
adherence to convention. For example, code completion engines
will not suggest renaming parameters like str in Figure 1.
Language models are extensively used in natural language pro-
cessing , especially in speech recognition and machine translation
[22, 38]. Despite this extensive work, LMs have been under-explored
for non-ambiguous ( e.g.programming) languages, with only a few
recent exceptions [4, 35, 46, 53]. The probabilistic nature of lan-
guage models allows us to tackle the suggestion problem in a princi-
pled way. There is very little work on using NLP tools to suggestrevisions to improve existing text. The main exception is spelling
correction [39], to which LMs have been applied [47]. However,
spelling correction methods often rely on strong assumptions about
what errors are most common, a powerful source of information
which has no analog in our domain.
6. CONCLUSION
We have presented NATURALIZE , the ﬁrst tool that learns local
style from a codebase and provides suggestions to improve stylistic
consistency. We have taken the view that conventions are a matter
ofmores rather than laws: We suggest changes only when there
is sufﬁcient evidence of emerging consensus in the codebase. We
showed that NATURALIZE is effective at making natural suggestions,
achieving 94% accuracy in its top suggestions for identiﬁer names,
and even suggesting useful revisions to mature, high quality open
source projects.
Acknowledgements
This work was supported by Microsoft Research through its PhD
Scholarship Programme and the Engineering and Physical Sciences
Research Council [grant number EP/K024043/1]. We acknowledge
Mehrdad Afshari who ﬁrst asked us about junk variables and the
heat death of the codebase.
7. REFERENCES
[1]S. L. Abebe, S. Haiduc, P. Tonella, and A. Marcus. The effect
of lexicon bad smells on concept location in source code. In
Source Code Analysis and Manipulation (SCAM), 2011 11th
IEEE International Working Conference on , pages 125–134.
IEEE, 2011.
[2] A. Abran, P. Bourque, R. Dupuis, J. W. Moore, and L. L.
Tripp. Guide to the Software Engineering Body of Knowledge -
SWEBOK . IEEE Press, Piscataway, NJ, USA, 2004 version
edition, 2004.
[3] E. N. Adams. Optimizing preventive service of software
products. IBM Journal of Research and Development ,
28(1):2–14, Jan. 1984.
[4]M. Allamanis and C. Sutton. Mining source code repositories
at massive scale using language modeling. In Proceedings of
the Tenth International Workshop on Mining Software
Repositories , pages 207–216. IEEE Press, 2013.
[5] N. Anquetil and T. Lethbridge. Assessing the relevance of
identiﬁer names in a legacy software system. In Proceedings
of the 1998 conference of the Centre for Advanced Studies on
Collaborative Research , page 4, 1998.
[6] N. Anquetil and T. C. Lethbridge. Recovering software
architecture from the names of source ﬁles. Journal of
Software Maintenance , 11(3):201–221, 1999.
[7] C. Arthur. Apple’s SSL iPhone vulnerability: how did it
happen, and what next? bit.ly/1bJ7aSa , 2014. Visited
Mar 2014.
[8] M. I. S. R. Association et al. MISRA-C 2012: Guidelines for
the Use of the C Language in Critical Systems. ISBN
9781906400118 , 2012.
[9] astyle Contributors. Artistic style 2.03.
http://astyle.sourceforge.net/ , 2013. Visited
September 9, 2013.
[10] A. Bacchelli and C. Bird. Expectations, Outcomes, and
Challenges of Modern Code Review. In ICSE , 2013.
11[11] J. Bergstra and Y . Bengio. Random search for
hyper-parameter optimization. The Journal of Machine
Learning Research , 13:281–305, 2012.
[12] T. J. Biggerstaff, B. G. Mitbander, and D. Webster. The
concept assignment problem in program understanding. In
Proceedings of the 15th international conference on Software
Engineering , pages 482–498. IEEE Computer Society Press,
1993.
[13] D. Binkley, M. Davis, D. Lawrie, J. Maletic, C. Morrell, and
B. Sharif. The impact of identiﬁer style on effort and
comprehension. Empirical Software Engineering ,
18(2):219–276, 2013.
[14] D. Binkley, M. Davis, D. Lawrie, and C. Morrell. To
CamelCase or Under_score. In IEEE 17th International
Conference on Program Comprehension (ICPC’09) , pages
158–167, 2009.
[15] C. Boogerd and L. Moonen. Assessing the value of coding
standards: An empirical study. In H. Mei and K. Wong,
editors, Proceedings of the 24th IEEE International
Conference on Software Maintenance (ICSM 2008) , pages 277
– 286. IEEE, October 2008.
[16] F. P. Brooks. The mythical man-month . Addison-Wesley
Reading, 1975.
[17] M. Broy, F. Deißenböck, and M. Pizka. A holistic approach to
software quality at work. In Proc. 3rd World Congress for
Software Quality (3WCSQ) , 2005.
[18] M. Bruch, M. Monperrus, and M. Mezini. Learning from
examples to improve code completion systems. In
ESEC/SIGSOFT FSE , pages 213–222. ACM, 2009.
[19] R. P. Buse and W. R. Weimer. Learning a metric for code
readability. Software Engineering, IEEE Transactions on ,
36(4):546–558, 2010.
[20] B. Caprile and P. Tonella. Restructuring program identiﬁer
names. In International Conference on Software Maintenance
(ICSM’00) , pages 97–107, 2000.
[21] J. Carletta. Assessing agreement on classiﬁcation tasks: the
kappa statistic. Computational linguistics , 22(2):249–254,
1996.
[22] S. Chen and J. Goodman. An empirical study of smoothing
techniques for language modeling. In Proceedings of the 34th
annual meeting on Association for Computational Linguistics ,
pages 310–318. Association for Computational Linguistics,
1996.
[23] N. Cowan. The magical number 4 in short-term memory: A
reconsideration of mental storage capacity. Behavioral and
Brain Sciences , 24(1):87–114, 2001.
[24] F. Deißenböck and M. Pizka. Concise and consistent naming
[software system identiﬁer naming]. In Proceedings of the
13th International Workshop on Program Comprehension
(IWPC’05) , pages 97–106, 2005.
[25] S. Dowdy, S. Wearden, and D. Chilko. Statistics for research ,
volume 512. John Wiley & Sons, 2011.
[26] Eclipse-Contributors. Eclipse JDT.
http://www.eclipse.org/jdt/ , 2013. Visited
September 9, 2013.
[27] L. M. Eshkevari, V . Arnaoudova, M. Di Penta, R. Oliveto,
Y .-G. Guéhéneuc, and G. Antoniol. An exploratory study of
identiﬁer renamings. In Proceedings of the 8th Working
Conference on Mining Software Repositories , pages 33–42.
ACM, 2011.
[28] M. Gabel and Z. Su. A study of the uniqueness of source code.
InProceedings of the eighteenth ACM SIGSOFT internationalsymposium on Foundations of software engineering , FSE ’10,
pages 147–156, New York, NY , USA, 2010. ACM.
[29] M. G. Gabel. Inferring Programmer Intent and Related Errors
from Software . PhD thesis, University of California, 2011.
[30] GitHub. JUnit Pull Request #834. bit.ly/O8bmjM , 2014.
Visited Mar 2014.
[31] GitHub. libgdx Pull Request #1400. bit.ly/O8aBqV ,
2014. Visited Mar 2014.
[32] gnu-indent Contributors. GNU Indent – beautify C code.
http://www.gnu.org/software/indent/ , 2013.
Visited September 9, 2013.
[33] S. Gupta, S. Malik, L. Pollock, and K. Vijay-Shanker.
Part-of-speech tagging of program identiﬁers for improved
text-based software engineering tools. In International
Conference on Program Comprehension , pages 3–12. IEEE,
2013.
[34] L. Hatton. Safer language subsets: an overview and a case
history, MISRA C. Information and Software Technology ,
46(7):465–472, 2004.
[35] A. Hindle, E. T. Barr, Z. Su, M. Gabel, and P. Devanbu. On
the naturalness of software. In Software Engineering (ICSE),
2012 34th International Conference on , pages 837–847. IEEE,
2012.
[36] A. Hindle, M. W. Godfrey, and R. C. Holt. Reading beside the
lines: Using indentation to rank revisions by complexity.
Science of Computer Programming , 74(7):414–429, May
2009.
[37] E. W. Høst and B. M. Østvold. Debugging method names. In
In European Conference on Object-Oriented Programming
(ECOOP , pages 294–317. Springer, 2009.
[38] D. Jurafsky and J. H. Martin. Speech and Language
Processing: An Introduction to Natural Language Processing,
Computational Linguistics and Speech Recognition . Prentice
Hall, 2nd edition, 2009.
[39] K. Kukich. Techniques for automatically correcting words in
text. ACM Computing Surveys , 24(4):377–439, Dec. 1992.
[40] A. Langley. Apple’s SSL/TLS bug. bit.ly/MMvx6b , 2014.
Visited Mar 2014.
[41] D. Lawrie, H. Feild, and D. Binkley. Syntactic identiﬁer
conciseness and consistency. In Source Code Analysis and
Manipulation, 2006. SCAM’06. Sixth IEEE International
Workshop on , pages 139–148. IEEE, 2006.
[42] D. Lawrie, H. Feild, and D. Binkley. An empirical study of
rules for well-formed identiﬁers: Research articles. Journal of
Software Maintenance Evolution: Research and Practice ,
19(4):205–229, July 2007.
[43] D. Lawrie, C. Morrell, H. Feild, and D. Binkley. What’s in a
Name? A Study of Identiﬁers. In Proceedings of the 14th
IEEE International Conference on Program Comprehension
(ICPC’06) , ICPC ’06, pages 3–12, Washington, DC, USA,
2006. IEEE Computer Society.
[44] B. Liblit, A. Begel, and E. Sweetser. Cognitive perspectives
on the role of naming in computer programs. In Annual
Psychology of Programming Workshop , 2006.
[45] C.-Y . Lin. Rouge: A package for automatic evaluation of
summaries. In Text Summarization Branches Out:
Proceedings of the ACL-04 Workshop , pages 74–81, 2004.
[46] C. J. Maddison and D. Tarlow. Structured generative models
of natural source code. arXiv preprint arXiv:1401.0514 , 2014.
[47] E. Mays, F. J. Damerau, and R. L. Mercer. Context based
spelling correction. Information Processing and Management ,
1227(5):517–522, 1991.
[48] G. A. Miller. The magical number seven, plus or minus two:
some limits on our capacity for processing information.
Psychological review , 63(2):81, 1956.
[49] D. Movshovitz-Attias and W. W. Cohen. Natural language
models for predicting programming comments. In Proc of the
ACL, 2013.
[50] E. Murphy-Hill, C. Parnin, and A. P. Black. How we refactor,
and how we know it. Software Engineering, IEEE
Transactions on , 38(1):5–18, 2012.
[51] N. Nagappan and T. Ball. Using software dependencies and
churn metrics to predict ﬁeld failures: An empirical case study.
InESEM , pages 364–373, 2007.
[52] A. T. Nguyen, T. T. Nguyen, H. A. Nguyen, A. Tamrawi, H. V .
Nguyen, J. Al-Kofahi, and T. N. Nguyen. Graph-based
pattern-oriented, context-sensitive source code completion. In
Proceedings of the 34th ACM/IEEE International Conference
on Software Engineering (ACM/IEEE ICSE 2012) . IEEE,
2012.
[53] T. T. Nguyen, A. T. Nguyen, H. A. Nguyen, and T. N. Nguyen.
A statistical semantic language model for source code. In
Proceedings of the 2013 9th Joint Meeting on Foundations of
Software Engineering , pages 532–542. ACM, 2013.
[54] M. Ohba and K. Gondow. Toward mining concept keywords
from identiﬁers in large software projects. In ACM SIGSOFT
Software Engineering Notes , volume 30, pages 1–5. ACM,
2005.
[55] Oracle. Code Conventions for the Java Programming
Language. http://www.oracle.com/
technetwork/java/codeconv-138413.html ,
1999. Visited September 2, 2013.
[56] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. BLEU: a
method for automatic evaluation of machine translation. In
Association for Computational Linguistics (ACL) , pages
311–318, 2002.
[57] R. Pike. Go at Google. http:
//talks.golang.org/2012/splash.slide , 2012.
Visited September 9, 2013.
[58] Pylint-Contributors. Pylint – code analysis for Python.
http://www.pylint.org/ , 2013. Visited September 9,
2013.
[59] V . Rajlich and P. Gosavi. Incremental change in
object-oriented programming. Software, IEEE , 21(4):62–69,
2004.
[60] D. Ratiu and F. Deißenböck. From reality to programs and
(not quite) back again. In Program Comprehension, 2007.
ICPC’07. 15th IEEE International Conference on , pages
91–102. IEEE, 2007.
[61] P. C. Rigby and C. Bird. Convergent software peer review
practices. In Proceedings of the the joint meeting of theEuropean Software Engineering Conference and the ACM
SIGSOFT Symposium on The Foundations of Software
Engineering (ESEC/FSE) . ACM, 2013.
[62] R. Robbes and M. Lanza. How program history can improve
code completion. In Automated Software Engineering (ASE) ,
pages 317–326. IEEE, 2008.
[63] M. Robillard, R. Walker, and T. Zimmermann.
Recommendation systems for software engineering. Software,
IEEE , 27(4):80–86, 2010.
[64] G. v. Rossum, B. Warsaw, and N. Coghlan. PEP 8–Style
Guide for Python Code.
http://www.python.org/dev/peps/pep-0008/ ,
2013. Visited September 8, 2013.
[65] C. Simonyi. Hungarian notation.
http://msdn.microsoft.com/en-us/library/
aa260976(VS.60).aspx , 1999. Visited September 2,
2013.
[66] E. Soloway and K. Ehrlich. Empirical studies of programming
knowledge. Software Engineering, IEEE Transactions on ,
(5):595–609, 1984.
[67] W. Strunk Jr and E. White. The elements of style . Macmillan,
New York, 3rd edition, 1979.
[68] A. Takang, P. Grubb, and R. Macredie. The effects of
comments and identiﬁer names on program comprehensibility:
an experiential study. 4(3):143–167, 1996.
[69] A. A. Takang, P. A. Grubb, and R. D. Macredie. The effects of
comments and identiﬁer names on program comprehensibility:
an experimental investigation. J. Prog. Lang. , 4(3):143–167,
1996.
[70] G. Uddin, B. Dagenais, and M. P. Robillard. Analyzing
temporal API usage patterns. In Proceedings of the 2011 26th
IEEE/ACM International Conference on Automated Software
Engineering , pages 456–459. IEEE Computer Society, 2011.
[71] J. Wang, Y . Dang, H. Zhang, K. Chen, T. Xie, and D. Zhang.
Mining succinct and high-coverage API usage patterns from
source code. In Proceedings of the Tenth International
Workshop on Mining Software Repositories , pages 319–328.
IEEE Press, 2013.
[72] X. Wang, L. Pollock, and K. Vijay-Shanker. Automatic
segmentation of method code into meaningful blocks to
improve readability. In Working Conference on Reverse
Engineering , pages 35–44. IEEE, 2011.
[73] Wikipedia. Coding Conventions. http://en.
wikipedia.org/wiki/Coding_conventions .
[74] H. P. Young. The economics of convention. The Journal of
Economic Perspectives , 10(2):105–122, 1996.
[75] H. Zhong, T. Xie, L. Zhang, J. Pei, and H. Mei. MAPO:
Mining and recommending API usage patterns. In ECOOP
2009–Object-Oriented Programming , pages 318–343.
Springer, 2009.
13