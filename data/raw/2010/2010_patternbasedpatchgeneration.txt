Automatic Patch Generation Learned from
Human-Written Patches
Dongsun Kim, Jaechang Nam, Jaewoo Song, and Sunghun Kim
The Hong Kong University of Science and Technology, China
{darkrsw,jcnam,jsongab,hunkim}@cse.ust.hk
Abstract —Patch generation is an essential software mainte-
nance task because most software systems inevitably have bugs
that need to be ﬁxed. Unfortunately, human resources are often
insufﬁcient to ﬁx all reported and known bugs. To addressthis issue, several automated patch generation techniques have
been proposed. In particular, a genetic-programming-based patch
generation technique, GenProg, proposed by Weimer et al., hasshown promising results. However, these techniques can generate
nonsensical patches due to the randomness of their mutation
operations.
To address this limitation, we propose a novel patch generation
approach, Pattern-based Automatic program Repair ( P
AR), using
ﬁx patterns learned from existing human-written patches. Wemanually inspected more than 60,000 human-written patches and
found there are several common ﬁx patterns. Our approach lever-
ages these ﬁx patterns to generate program patches automatically.
We experimentally evaluated P
ARon 119 real bugs. In addition,
a user study involving 89 students and 164 developers conﬁrmedthat patches generated by our approach are more acceptable than
those generated by GenProg. P
ARsuccessfully generated patches
for27out of 119 bugs, while GenProg was successful for only
16 bugs.
I. I NTRODUCTION
Patch generation is an essential software maintenance task,
since most software systems inevitably have bugs that needto be ﬁxed [1], [2]. Unfortunately, human resources are ofteninsufﬁcient to generate patches [3], even for known bugs. Forexample, Windows 2000 was shipped with more than 63,000
known bugs, largely due to limited resources [4].
To reduce manual effort, several automatic patch generation
techniques have been proposed. Arcuri and Yao introducedthe idea of applying evolutionary algorithms to automaticpatch generation [5]. Dallmeier et al. proposed an approachleveraging object behavior model and applied this approachto real bugs from open source projects [6]. Weimer et al.
proposed a population-based technique [7], [8] leveraging
genetic programming [9]. Wei et al. provided a contract-based technique to automate patch generation and showed itsusefulness by applying it to bugs in Eiffel classes [10].
Among these, the award-winning patch generation tech-
nique, GenProg [7], and its extension [8] showed the mostpromising results. To ﬁx a bug in a given program, thistechnique generates variants of the program by using crossoveroperators and mutation operators such as statement addition,
replacement, and removal [9]. Then, it runs test cases toevaluate each variant. GenProg iterates these steps until one
of the variants passes all test cases. Any program variant1918 if (lhs == DBL_MRK) lhs = ...;
1919 if (lhs == undefined) {1920 lhs = strings[getShort(iCode, pc + 1)];1921 }1922 Scriptable calleeScope = scope;
(a) Buggy program. Line 1920 throws an Array Index Out of Bound exception when
getShort(iCode, pc + 1) is equal to or larger than strings.length
or smaller than 0.
1918 if (lhs == DBL_MRK) lhs = ...;
1919 if (lhs == undefined) {
1920+ lhs = ((Scriptable)lhs).getDefaultValue(null);
1921 }1922 Scriptable calleeScope = scope;
(b) Patch generated by GenProg.
1918 if (lhs == DBL_MRK) lhs = ...;1919 if (lhs == undefined) {1920+ i = getShort(iCode, pc + 1);
1921+ if (i != -1)
1922+ lhs = strings[i];
1923 }1924 Scriptable calleeScope = scope;
(c) Human-written patch.
1918 if (lhs == DBL_MRK) lhs = ...;1919 if (lhs == undefined) {1920+ if (getShort(iCode, pc + 1) <strings.length &&
getShort(iCode, pc + 1) >=0)
1921+ {
1922 lhs = strings[getShort(iCode, pc + 1)];
1923+ }1924 }1925 Scriptable calleeScope = scope;
(d) Patch generated by P AR.
Fig. 1: Patches created by GenProg, a human developer, and P ARfor Mozilla Bug
#114493.
passing all test cases is regarded as a successful patch.
They experimentally showed that this technique can createsuccessful patches for 55 out of 105 real bugs [8].
However, GenProg has an inherent limitation: since this
technique basically relies on random program mutations suchas statement addition, replacement, and removal, it is pos-sible to generate nonsensical patches. Figure 1(b) shows anexample of nonsensical patches generated by GenProg, for
the bug shown in Figure 1(a). Compared to the human-written
patch in Figure 1(c), GenProg’s patch completely removedthe “strings[] ” variable from the program. Note that the
program would assign an element of strings tolhs as
long as the given index is valid, while the patch generatedby GenProg does not do this. Although GenProg’s patch canactually pass all the given test cases, developers would not
accept the patch, as shown in Section IV-C.978-1-4673-3076-3/13 c2013 IEEE ICSE 2013, San Francisco, CA, USA
Accepted for publication by IEEE. c2013 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/
republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.8023561 public ITextHover getCurrentTextHover() {
3562+ if (fTextHoverManager== null)
3563+ return null;
3564 return fTextHoverManager.getCurrentTextHover();
3565 }
Fig. 2: Example of “Null Checker”, a bug-ﬁx pattern for Null Pointer Exceptionbugs. This patch ﬁxes a bug of TextViewer.java described in Eclipse JDT Bug
#26028. It inserts an ifstatement to avoid calling getCurrentTextHover() when
fTextHoverManager isnull .
To address this limitation, we propose a novel patch gen-
eration technique: Pattern-based Automatic program Repair(P
AR). This approach leverages knowledge of human-written
patches. We ﬁrst carefully inspected 62,656 human-writtenpatches of open source projects. Interestingly, we found thatthere were several common ﬁx patterns. Based on our ob-servations, we created 10 ﬁx templates, which are automaticprogram editing scripts based on the identiﬁed ﬁx patterns.
P
ARuses these ﬁx templates to generate program patches.
Although creating ﬁx templates requires manual effort, this isonly a one-time cost and these templates are highly reusablein different contexts after they are created. Figure 1(d) shows a
patch generated by our approach that is similar to the human-written patch (Figure 1(c)).
To evaluate P
AR, we applied it to 119 actual bugs collected
from open source projects including Apache log4j1, Rhino2,
and AspectJ3.
We asked 253 human subjects (89 students and 164 de-
velopers) to compare patches that they would accept, if theywere code reviewers of the anonymized patches generated by
P
ARand GenProg. The results of this study clearly showed
that patches generated by P ARare much more acceptable
than patches generated by GenProg. In addition, our approachgenerated more successful patches than GenProg: P
ARsuc-
cessfully generated 27patches out of 119 bugs, while GenProg
was successful for 16 bugs.
Overall, this paper makes the following contributions:
•Manual observations on human-written patches: Our
investigation of human-written patches reveals that thereare common ﬁx patterns in patches.
•PAR, an automatic patch generation technique lever-
aging ﬁx patterns : We propose a novel automatic patch
generation technique using ﬁx templates derived fromcommon ﬁx patterns.
•Empirical evaluation : We present the empirical evalua-
tion results by applying P ARto 119 real bugs.
The remainder of this paper is organized as follows. After
presenting common ﬁx patterns identiﬁed from human-written
patches in Section II, we propose our approach, P AR,i n
Section III. Section IV empirically evaluates our approach,and Section V discusses its limitations. After surveying therelated work in Section VI, we conclude with directions forfuture research in Section VII.
1http://logging.apache.org/log4j/
2https://developer.mozilla.org/en-US/docs/Rhino
3http://www.eclipse.org/aspectj/TABLE I: Common ﬁx patterns identiﬁed from Eclipse JDT’s patches.
Fix Patterns
Altering method parameters
Calling another method with the same parameters
Calling another overloaded method with one more parameter
Changing a branch condition
Adding a null checker
Initializing an object
Adding an array bound checker
Adding a class-cast checker
II. C OMMON FIXPATTERNS
This section presents common ﬁx patterns identiﬁed from
our manual investigation of human-written patches. We ﬁrstdescribe how we collected and examined a large number ofhuman-written patches. Then, we report a list of common ﬁxpatterns.
A. Patch Collection
For our investigation, 62,656 human-written patches were
collected from Eclipse JDT
4. We used Eclipse JDT, because
it has a long revision history (more than 10 years), and iswidely used in the literature [11], [12]. We used the Kenyonframework [13] to retrieve bug patches [14].
B. Mining Common Patches
Since our goal is to explore human knowledge in patch
generation, we focused on semantic rather than syntactic
changes [15]. First, we examined whether any semantics areadded in or removed from the patches. Second, we identiﬁedthe root cause of each bug and the resolution of the cor-responding patch. Lastly, similar patches were grouped intocommon patterns.
To reduce manual inspection time, we ﬁrst gathered similar
patches using groums [16]. A groum is a graph-based model
for representing object usage. Although not designed for patchanalysis, groums can help detect semantic rather than syntactic
differences. For each patch, we built two groums from two
consecutive program versions: before and after applying thepatch. Then, the differences of nodes and edges between thetwo groums were computed automatically. We could gatherpatches having the same differences into a group. Although
a patch group is not necessarily a ﬁx pattern, doing this can
substantially reduce manual inspection time.
To identify ﬁx patterns, we ﬁrst classiﬁed patches as addi-
tive, subtractive, or altering patches. Additive patches insertnew semantic features such as new control ﬂows, whilesubtractive patches remove semantic features. Altering patchesjust change control ﬂows by replacing semantic features.
We then examined the root causes of bugs and how the
corresponding patches speciﬁcally resolved the bugs. For ex-ample, the patch shown in Figure 2 inserts a new ifstatement
to avoid a crash when fTextHoverManager isnull .I n
this example, the root cause is “ null value ” and the patch
resolves it by adding a new control ﬂow.
Some patches address multiple causes and these are called
composite patches [17]. We divided a composite patch intomultiple independent patches and analyzed them individually.
4http://www.eclipse.org/jdt803C. Fix Patterns
After inspecting the patches, we identify many recurring
similar patches, i.e., ﬁx patterns . Table I shows common pat-
terns identiﬁed by our investigation. These top eight patterns
cover almost 30% of all patches we observed. The followingparagraphs describe the details of each pattern:
•Pattern : Altering method parameters.
Example :
obj.method(v1,v2) →obj.method(v1, v3)
Description : This pattern can ﬁx a bug since it makes the
caller give appropriate parameters to the method.
•Pattern : Calling another method with the same parameters.
Example :obj.method1(param) →obj. method2 (param)
Description : This pattern changes the callee in a method
call statement to ﬁx an inappropriate method invocation.
•Pattern : Calling another overloaded method with one more
parameter.Example :
obj.method(v1) →obj.method(v1, v2)
Description : This pattern adds one more parameter to the
existing method call, but it actually replaces the callee by
another overloaded method.
•Pattern : Changing a branch condition.
Example :if(a == b) →if(a == b && c! =0 )
Description : This pattern modiﬁes a branch condition in
conditional statements or in ternary operators. Patches inthis pattern often just add a term to a predicate or remove
a term from a predicate.
•Pattern : Initializing an object.
Example :
Type obj; →Type obj = new Type()
Description : This pattern inserts an additional initialization
for an object. This prevents an object being null .
•Pattern : Adding a “null”, “array-out-of-bound”, and “class-
cast” checker.Example :
obj.m1() →if(obj!=null) {obj.m1()}
Description : These three patterns insert a new control ﬂow
in a program. They often add a new “ if(...) ” statement
to avoid throwing exceptions due to an unexpected state ofthe program. Figure 2 shows an example of these patterns.
Overall, we found that there are common ﬁx patterns in
human-written patches. Since these major patterns are used inmany real patches (almost 30%) to ﬁx bugs, we may generatemore successful patches by leveraging them in automatic patchgeneration.
III. P
AR:PATTERN -BASED AUTOMATIC PROGRAM REPAIR
PARgenerates bug-ﬁxing patches automatically by using
ﬁx patterns described in Section II-C. Figure 3 illustrates anoverview of our approach. When a bug is reported, (a) P
AR
ﬁrst identiﬁes fault locations, i.e., suspicious statements, byusing existing fault localization techniques [8]. These fault
locations and their adjacent locations are modiﬁed to ﬁx the
bug. (b) P
ARuses ﬁx templates to generate program variants
(patch candidates) by editing the source code around the faultAlgorithm 1: Patch generation using ﬁx templates in P AR.
Input : ﬁtness function Fit: Program →R
Input :T: a set of ﬁx templates
Input :PopSize : population size
Output :Patch : a program variant that passes all test cases
1letPop←initialPopulation( PopSize );
2repeat
3 letPop←apply( Pop,T );
4 letPop←select( Pop,PopSize,Fit );
5until∃Patch inPop that passes all test cases ;
6return Patch
locations. (c) Program variants are evaluated by a ﬁtness
function that computes the number of passing test cases ofa patch candidate. If a candidate passes all given test cases,our approach assumes that it is a successful patch [7]. Other-wise, our approach repeats the patch candidate generation andevaluation steps.
Our approach leverages evolutionary computing tech-
niques [9] to generate program patches. Evolutionary comput-ing is an iterative process in which a population is reproduced,evaluated, and selected. One cycle of these three steps is calledageneration .
Algorithm 1 shows our approach following this evolutionary
computing process. Our approach ﬁrst takes a ﬁtness function,ﬁx templates, and population size as input. After creating aninitial population of program variants equal in number to thegiven population size (Line 1), it iterates two tasks: generatingnew program variants by using ﬁx templates (Line 3, repro-
duction) and selecting top variants based on the given ﬁtness
function (Line 4, evaluation and selection). This iteration stopswhen any program variant passes all given test cases (Line5) or when it meets predeﬁned termination conditions (seeSection IV-A). Algorithm 1 returns a program variant thatpasses all test cases as a successful patch for the given bug
(Line 6).
We adopt this evolutionary computing process, because it
is effective in automatic patch generation [8] by efﬁcientlyexploring a large number of program variants. Applying anevolutionary computing process to program repair was pio-neered by Weimer et al. [7] and Arcuri et al. [5].
The remainder of this section describes the details of fault
localization, ﬁx templates, and ﬁtness function used in P
AR.
A. Fault Localization
To determine fault locations, P AR uses statistical fault
localization based on test cases [8]. This technique assumesthat a statement visited by failing test cases is more likely tobe a defect than other statements. Speciﬁcally, this techniqueassigns a value to each statement in a program. This valuerepresents a degree of suspiciousness. Our approach uses thatvalue to decide whether a statement should be modiﬁed.
This localization technique ﬁrst executes two groups of test
cases: passing and failing. Then, the technique records thestatement-based coverage of both test case groups. Comparingthe coverage of each statement results in one of the followingfour outcomes: 1) covered by both groups, 2) covered only bythe passing group, 3) covered only by the failing group, and804if(lhs == DBL_MRK) lhs = ...;
if(lhs == undefined) {
lhs = strings[pc + 1];
}
Scriptable calleeScope = ...;
/g68/g68/g75/g79/g77/g68/g79/g77/g79/g68/g77/g73/g79/g30/g71/g86/g78/g79/g77/g75/g86/g71/g79/g78/g77/g73/g71/g86/g68/g73
/g36/g86/g74/g68/g71/g86/g73/g68/g86/g71/g73/g68/g86/g79/g71/g77/g74/g79/g78/g68/g77/g68/g86/g74/g68/g71/g73/g30/g79/g54/g71/g79/g77/g78/g73/g79/g78/g68/g86/g77/g71/g73/g30/g79/g77/g68/g86/g71/g79/g73/g77/g86/g71/g79
/g71/g86/g73/g86/g68/g71/g73/g86/g71/g68/g73/g86/g71/g68/g86/g71/g79/g78/g73/g77/g71/g68/g86/g79/g77
/g79/g78/g68/g86/g77/g73/g79/g78/g77/g86/g71/g68/g79/g78/g73/g77/g79/g71/g78/g86/g68/g77/g73/g79/g78/g71/g86/g68/g77Buggy
Program
/g68/g68/g75/g79/g77/g68/g79/g77/g79/g68/g77/g73/g79/g30/g71/g86/g78/g79/g77/g75/g86/g71/g79/g78/g77/g73/g71/g86/g68/g73
/g36/g86/g74/g68/g71/g86/g73/g68/g86/g71/g73/g68/g86/g79/g71/g77/g74/g79/g78/g68/g77/g68/g86/g74/g68/g71/g73/g30/g79/g54/g71/g79/g77/g78/g73/g79/g78/g68/g86/g77/g71/g73/g30/g79/g77/g68/g86/g71/g79/g73/g77/g86/g71/g79
/g71/g86/g73/g86/g68/g71/g73/g86/g71/g68/g73/g86/g71/g68/g86/g71/g79/g78/g73/g77/g71/g68/g86/g79/g77
/g79/g78/g68/g86/g77/g73/g79/g78/g77/g86/g71/g68/g79/g78/g73/g77/g79/g71/g78/g86/g68/g77/g73/g79/g78/g71/g86/g68/g77
(a) Fault 
      Localization
lhs == D
if(lhs == un
lhs = str
Scriptable 
 c
lhs 
i
}
S
/g68/g79/g77/g79/g68/g77/g73/g79/g30/g71/g86
/g36/g86/g74/g68/g71/g86/g73/g68/g86/g71/g73/g68/g86/g79/g71/g77/g74/g79/g78/g68/g77/g68/g86/g74/g68/g71/g73/g30/g79
/g54/g71/g79/g77/g78/g73/g79/g78/g68/g86/g77/g71/g73/g30/g79/g77/g68/g86/g71/g79
/g71/g86/g73/g86/g68/g71/g73/g86/g71/g68
/g79/g78/g73/g77/g79/g71/g78/g86/g68/g77/g73/g79/g78/g71/g86/g68/g77
/g71/g86/g78/g79/g77/g75/g86/g71/g79/g78/g77/g73/g71/g86/g68/g73
/g79/g73/g77/g86/g71/g79
/g68/g73/g86/g71/g68/g86/g71/g79/g78/g73/g77/g71/g68/g86/g79/g77
/g36/g86/g74/g68/g71/g86/g73/g68/g86/g71/g73/g68/g86/g79/g71/g77/g74/g79/g78/g68/g77/g68/g86/g74/g68/g71/g73/g30/g79+/g54/g71/g79/g77/g78/g73/g79/g78/g68/g86/g77/g71/g73/g30/g79/g77/g68/g86/g71/g79/g73/g77/g86/g71/g79-
/g71/g86/g73/g86/g68/g71/g73/g86/g71/g68/g73/g86/g71/g68/g86/g71/g79/g78/g73/g77/g71/g68/g86/g79/g77+
/g68/g68/g75/g79/g77/g68/g79/g77/g79/g68/g77/g73/g79/g30/g71/g86/g78/g79/g77/g75/g86/g71/g79/g78/g77/g73/g71/g86/g68/g73/g36/g86/g74/g68/g71/g86/g73/g68/g86/g71/g73/g68/g86/g79/g71/g77/g74/g79/g78/g68/g77/g68/g86/g74/g68/g71/g73/g30/g79/g54/g71/g79/g77/g78/g73/g79/g78/g68/g86/g77/g71/g73/g30/g79/g77/g68/g86/g71/g79/g73/g77/g86/g71/g79-
+
+/g68/g68/g75/g79/g77/g68/g79/g77/g79/g68/g77/g73/g79/g30/g71/g86/g78/g79/g77/g75/g86/g71/g79/g78/g77/g73/g71/g86/g68/g73
/g36/g86/g74/g68/g71/g86/g73/g68/g86/g71/g73/g68/g86/g79/g71/g77/g74/g79/g78/g68/g77/g68/g86/g74/g68/g71/g73/g30/g79
/g71/g86/g73/g86/g68/g71/g73/g86/g71/g68/g73/g86/g71/g68/g86/g71/g79/g78/g73/g77/g71/g68/g86/g79/g77
(b) Template-based
      Patch Ca ndidate Generation
FailPass
(c) Patch EvaluationT
Accept
Fix 
TemplatePatch
Candidate
/g86/g71/g68/g74/g78/g79/g77/g79/g68/g78/g90/g77/g72/g73/g85/g79/g90/g77/g72/g73/g86/g71/g77/g79/g78/g3
ifif(((f(ff((fff(((ff((f((f((f((((ififlllllllllllllllll
iiiiiiiiiiiiiiiiiiii
/g68/g68/g75/g79/g77/g68/g79/g77/g79/g68/g77/g73/g79/g30/g71/g86/g78/g79/g77/g75/g86/g71/g79/g78/g77/g73/g71/g86/g68/g73
/g36/g86/g74/g68/g71/g86/g73/g68/g86/g71/g73/g68/g86/g79/g71/g77/g74/g79/g78/g68/g77/g68/g86/g74/g68/g71/g73/g30/g79/g54/g71/g79/g77/g78/g73/g79/g78/g68/g86/g77/g71/g73/g30/g79/g77/g68/g86/g71/g79/g73/g77/g86/g71/g79
/g71/g86/g73/g86/g68/g71/g73/g86/g71/g68/g73/g86/g71/g68/g86/g71/g79/g78/g73/g77/g71/g68/g86/g79/g77
/g79/g78/g68/g86/g77/g73/g79/g78/g77/g86/g71/g68/g79/g78/g73/g77/g79/g71/g78/g86/g68/g77/g73/g79/g78/g71/g86/g68/g77/g86/g71/g68/g74/g78/g79/g77/g79/g68/g78/g90/g77/g72/g73/g85/g79/g90/g77/g72/g73/g86/g71/g77/g79/g78
 /g86
/g68/g68/g75/g79/g77/g68/g79/g77/g79/g68/g77/g73/g79/g30/g71/g86/g78/g79/g77/g75/g86/g71/g79/g78/g77/g73/g71/g86/g68/g73
/g36/g86/g74/g68/g71/g86/g73/g68/g86/g71/g73/g68/g86/g79/g71/g77/g74/g79/g78/g68/g77/g68/g86/g74/g68/g71/g73/g30/g79/g54/g71/g79/g77/g78/g73/g79/g78/g68/g86/g77/g71/g73/g30/g79/g77/g68/g86/g71/g79/g73/g77/g86/g71/g79
/g71/g86/g73/g86/g68/g71/g73/g86/g71/g68/g73/g86/g71/g68/g86/g71/g79/g78/g73/g77/g71/g68/g86/g79/g77
/g79/g78/g68/g86/g77/g73/g79/g78/g77/g86/g71/g68/g79/g78/g73/g77/g79/g71/g78/g86/g68/g77/g73/g79/g78/g71/g86/g68/g77RepairedProgram
FaultLocation
Fig. 3: Overview of our pattern-based program repair (P AR) approach. P ARﬁrst takes a buggy program and (a) identiﬁes fault locations. Then, (b) it generates program variants
by using ﬁx templates, which are program editing scripts derived from common patch patterns (Section II-C). The templates modify source code around t he fault locations. These
variants are patch candidates. Finally, (c) patch candidates are evaluated by using test cases. If a candidate passes all test cases, we assume it is a p atch for the bug. Otherwise, our
approach repeats Steps (b) and (c) to generate another patch.
1[Null Pointer Checker]
2P = program
3B = fault location
4
5<AST Analysis>
6C←collect object references (method invocations,
field accesses, and qualified names) of B in P
7
8<Context Check>
9ifthere is any object references in C ⇒continue
10otherwise ⇒stop
11
12<Program Editing>
13insert an if() statement before B
1415
loop for all objects in C {
16 insert a conditional expression that checks whether a
given object is null
17}
18concatenate conditions by using AND
19
20ifB includes return statement {
21 negate the concatenated conditional expression
22 insert a return statement that returns a default value
into THEN section of the if() statement
23 insert B after the if() statement
24}else {
25 insert B into THEN section of the if() statement
26}
Fig. 4: Null pointer checker ﬁx template. This template inserts an if() statement
checking whether objects are null .
4) not covered by either group. We assign 0.1 to statements
with the ﬁrst outcome and 1.0 to statements with the thirdoutcome. Otherwise, 0.0 is assigned.
Our approach uses the assigned values to determine the
probability of each statement to be edited. For example, 1.0implies that the statement is always edited while 0.1 impliesthat it is edited once in 10 generations.
We adopted the simple fault localization technique used
in [8], but other fault localization techniques can be used todetermine which statements to mutate.
B. Fix Templates
Fix templates are program editing scripts that rewrite a
program’s Abstract Syntax Tree (AST). Each ﬁx templatedeﬁnes three steps to edit a program: 1) AST analysis, 2)
context check, and 3) program editing. The AST analysisstep scans a given program’s AST and analyzes the givenfault location and its adjacent locations. The context check
step examines whether the given program can be edited bya template by inspecting the analyzed AST. If it is editable,
our approach rewrites the given program’s AST based on thepredeﬁned editing script in the template (the program editing
step).
In this section, we ﬁrst describe how to create and apply the
ﬁx templates. Then, we provide the list of the ﬁx templatesused in P
AR.
1) Creating Fix Templates: We ﬁrst carefully inspect
patches in each pattern (Table I). Since program patcheschange a program’s AST, we can compute AST differencesbetween before and after applying a patch. We transformthese differences into editing scripts in a ﬁx template. Notethat patches in the same pattern may have various ASTseven though their semantic changes are identical. We try togeneralize these changes in ﬁx templates by identifying themost common change set.
For the context check step, we extract context information
such as the existence of an array access from patches, which isnecessary to check whether our approach can edit a programby using a ﬁx template. Then, we add the AST analysisstep to the template, which scans a program’s AST at thefault location and extracts AST elements, such as variablenames and types. We identify these elements by looking upﬁx patterns. These elements are used in the context check andthe program editing steps.
For example, a ﬁx template, Null Pointer Checker , is shown
in Figure 4. This template is derived from the “Adding a nullchecker” pattern shown in Section II-C. To create this tem-plate, we ﬁrst generalize how patches in the pattern change the
ASTs of programs. Commonly, they insert an if() statement
containing the fault location. The detailed program editingscript is shown in Lines 13 – 26 (the program editing step).Then, we identify common context information necessary forprogram editing, which will be added to the context check
step. For this template, the context check step veriﬁes that the
fault location must have at least one object reference (Lines9–10). Finally, we add the AST analysis step (Line 6), whichcollects all object references in the given fault location.
In the same manner, we have created 10 ﬁx templates as
shown in Table II. Although creating ﬁx templates requiresmanual effort, this is only a one-time cost, and the templatescan be reused to ﬁx other similar bugs. Our evaluation in Sec-tion IV conﬁrms that the templates created from ﬁx patternsin Eclipse JDT can be successfully applied to ﬁx bugs in otherprograms such as Rhino.805TABLE II: Fix templates derived from patterns in Section II-C.
Template Name Description
Parameter Replacer For a method call, this template seeks variables or expressions whose type is compatible with a method parameter within the same scope. Then, it replac es
the selected parameter by a compatible variable or expression.
Method Replacer For a method call, this template replaces it to another method with compatible parameters and return type.
Parameter Adder and
RemoverFor a method call, this template adds or removes parameters if the method has overloaded methods. When it adds a parameter, this template search forcompatible variables and expressions in the same scope. Then, it adds one of them to the place of the new parameter.
Expression Replacer For a conditional branch such as if() or ternary operator, this template replaces its predicate by another expression collected in the same scope.
Expression Adder
and RemoverFor a conditional branch, this template inserts or removes a term of its predicate. When adding a term, the template collects predicates from the samescope.
Null Pointer Checker For a statement in a program, this template adds if() statements checking whether an object is null only if the statement has any object reference.
Object Initializer For a variable in a method call, this template inserts an initialization statement before the call. The statement uses the basic constructor which has no
parameter.
Range Checker For a statement with array references, this template adds if() statements that check whether an array index variable exceeds upper and lower bounds
before executing statements that access the array.
Collection SizeChecker For a collection type variable, this template adds if() statements that check whether an index variable exceeds the size of a given collection object.
Class Cast Checker For a class-casting statement, this template inserts an if() statement checking that the castee is an object of the casting type (using instanceof
operator).
2) Applying Fix Templates: PARapplies a template to a
fault location in each individual generation. As described inSection III-A, each fault location has a selection probability,and P
ARuses that probability to determine which locations
will be modiﬁed by a ﬁx template. Since our approach followsthe evolutionary computing process, each location can beedited by multiple templates over several generations.
When applying a ﬁx template, P
ARﬁrst takes a program and
a fault location as input values. P ARthen executes the AST
analysis step in the template to collect necessary information
such as variable types and method parameters. By using thecollected information, P
ARruns the context check step to
ﬁgure out if the program has appropriate context to applythe given template. If the context check passes, our approachexecutes the template’s program editing step to rewrite theprogram’s AST of the given fault location. AST rewritingincludes node addition, parameter replacement, and predicateremoval. The modiﬁed program is a new program variant that
P
ARregards as a patch candidate.
It is possible that several templates pass the context check-
ing for given fault locations. In this case, P ARrandomly selects
one of the passing templates.
Figure 5 shows an example of applying the null pointer
checker template (see Figure 4) to generate a new programvariant from NativeRegExp.java for ﬁxing Rhino Bug
#76683. In this example, Line 4 in Figure 5(a) is a faultlocation. Our approach checks whether the location containsany object reference to ensure the Null Pointer Checkertemplate is applicable. Since the location has two references,the template is applicable. Then, our approach generates a newprogram variant by applying the editing script in the template:inserting an if statement containing the fault location as
described in Figure 4. As a result, a new program variant (i.e.,patch candidate) has a predicate that checks whether the twoobjects are not null .
3) List of Fix Templates: Table II lists all 10 ﬁx templates
used in our approach. The Parameter Replacer template is de-
rived from the “Altering method parameters” pattern in Table I.Its editing script can change parameters for a method call. Thecandidates of a substitutive parameter are collected from thesame scope of the target method call at the fault location, andthey must have compatible types. These parameter candidates01 if (kidMatch != -1) return kidMatch;
02 for (int i = num; i < state.parenCount; i++)03 {04 state.parens[i].length = 0;
05 }06 state.parenCount = num;
(a)Buggy Program : the underlined statement is a fault location.
⇓
INPUT: state.parens[i].length = 0;<Null Pointer Checker>
1. Analyze: Extract obj refer /g198state, state.parens[i]
2. Context Check: object references?: PASS
3. Edit: INSERT 
    ...
    ... 
+i f (                                           ) {
             state.parens[i].length = 0;
+ }
    ...    ... 
                                           
tt
 [[
iii
]l]lthth000
  state != null && state.parens[i] != null 
OUTPUT:  a new program variant
⇓
01 if (kidMatch != -1) return kidMatch;
02 for ( ... )03 {04+ if( state != null && state.parens[i] != null)
05 state.parens[i].length = 0;06 }
07 state.parenCount = num;
(b)After applying a ﬁx template : a patch generated by P AR. As shown in the ﬁx template,
corresponding statements have been edited.
Fig. 5: Real example of applying a ﬁx template to NativeRegExp.java to ﬁx Rhino
Bug #76683.
are sorted according to the distance (the number of nodes)
from the given fault location in the given program’s abstractsyntax tree. Our approach selects one of them based on the
distance and replaces method parameters of the statement atthe given fault location. Parameters with a shorter distance aremore likely to be selected.
The Method Replacer template replaces the name of the
callee in method call statements. This template includes acontext check step which scans other method calls havingthe same parameters and return type in the same programscope. If there are several candidates, our approach randomlyselects one of them to replace the method name in the givenfault location. The template is derived from “Calling anothermethod with the same parameters” in Table I.
We created the Parameter Adder and Remover template
to make a method call have more or fewer parameters. This806TABLE III: Data set used in our experiments. “LOC” (Lines of code) and “# statements”
represent the size of each subject. “# test cases” is the number of test cases used forevaluating patch candidates generated by P
AR.
Subject # bugs LOC # statements # test cases Description
Rhino 17 51,001 35,161 5,578 interpreter
AspectJ 18 180,394 139,777 1,602 compiler
log4j 15 27,855 19,933 705 logger
Math 29 121,168 80,764 3,538 math utils
Lang 20 54,537 40,436 2,051 helper utils
Collections 20 48,049 35,335 11,577 data utils
Total 119 483,004 351,406 25,051
template is applicable only if there are overloaded methods for
the target method. Our approach selects one of the availableoverloaded methods randomly. When adding parameters, thetemplate’s script speciﬁes how to scan the same scope of thegiven fault location for variables compatible with the newplace. When removing parameters, our approach just ﬁltersout parameters not included in the selected overloaded method.This template is derived from the “Calling another overloadedmethod with one more parameter” pattern in Table I.
Expression Replacer and Expression Adder and Remover
are derived from patches modifying predicates in conditionalor loop statements such as if() orwhile() . Many patches
we collected change or introduce new predicates to ﬁx a bug
as described by the “Changing a branch condition” pattern inTable I. To replace or add a predicate, our approach ﬁrst scanspredicates in the same scope of the given fault location. These
are sorted according to their distances from the fault locationin the given program’s AST. Our approach selects one of thembased on the distance and replaces or adds the predicate into
the target statements at the fault location. When removing aterm from a given predicate, our approach randomly selects a
term to remove.
We created Object Initializer which inserts an initialization
statement into the fault location. The initializer of this state-ment is the basic constructor without parameter. This statementprevents the variable from being null . This template is
derived from the “Initializing an object” pattern in Table I.
Null Pointer Checker ,Range Checker ,Collection Size
Checker , and Class Cast Checker are derived from correspond-
ing patterns in Table I. By using these patterns, our approachcan insert a new if() statement to check if there is any
abnormal state such as null pointer, index out of bound, orwrong class-casting.
C. Fitness Evaluation
The ﬁtness function of our approach takes a program variant
and test cases, and then computes a value representing thenumber of passing test cases of the variant. This ﬁtnessfunction is adapted from [7], [8]. All the test cases arecollected from the corresponding code repository for a givenprogram. The resulting ﬁtness value is used for evaluating andcomparing program variants in a population: “ which variant
is better than others? ” Based on ﬁtness values of program
variants, P
ARchooses program variants by using a tournament
selection scheme [18] (Line 4 in Algorithm 1).IV . E V ALUATION
We present the experimental evaluation of our approach
in this section. Speciﬁcally, our experiments are designed toaddress the following research questions:
•RQ1 (Fixability) : How many bugs are ﬁxed success-
fully?
•RQ2 (Acceptability) : Which approach can generate more
acceptable bug patches?
A. Experimental Design
To evaluate P AR, we collected 119 real bugs from open
source projects as shown in Table III. For each bug, weapplied both P
ARand GenProg [8] to generate patches. Then,
we examined how many bugs are successfully ﬁxed by each
approach (RQ1). We also conducted a user study to compare
the patch quality of the two approaches (RQ2).
Six projects in Table III are written in Java. We chose
those projects for two main reasons. First, Java is one of themost popular programming languages
5, so there are many Java
projects. Second, thanks to JUnit6and other Java-based testing
frameworks, Java projects often include many test cases.
Many open source projects maintain their own issue tracking
systems such as Bugzilla and JIRA. For our experiment,six open source projects including Mozilla Rhino, EclipseAspectJ, Apache Log4j, and Apache Commons (Math, Lang,Collections) were selected, since they are commonly used inthe literature [6], [19], [20] and have well-maintained bugreports. We tried to search their corresponding issue trackersfor reproducible bugs. Among them, we randomly selected 15to 29 bugs per project, since some projects had too many bugs.Although we invested our best effort in bug collection, thecollected bugs did not represent the entire bugs. However, toour best knowledge, 119 was the largest number in automaticpatch generation evaluation to date.
For each bug, we collected all available test cases from
their corresponding code repositories including failing testcases that reproduce the bugs. Projects often include manytest cases since developers continuously write and maintaintest cases [21]. In our experiment, we used all test cases, asshown in Table III, to validate a candidate patch [6], [7], [10].
When applying P
ARand GenProg to each bug, we con-
ducted 100 runs. Arcuri and Briand recommended at least1,000 runs for evaluating randomized algorithms [22], but weconducted 100 runs due to time limitation. Our experiment
totaled 23,800 runs (100 ×119 bugs ×2 approaches).
Each run stopped when it took more than 10 generations
or eight hours (wall-clock time), at which time we assumedthat the run failed to generate a successful patch. We usedexactly the same termination condition as used in [8] for faircomparison.
In addition, we used the same population size (=40) as used
in GenProg. We also used the same parameters, such as themutation probability suggested in [8] for running GenProg.
5http://www.tiobe.com/index.php/content/paperinfo/tpci/index.html
6http://www.junit.org/807TABLE IV: Patch generation results. Among 119 bugs, P ARsuccessfully ﬁxed 27bugs
while GenProg was successful for only 16 bugs. Note that 5 bugs were ﬁxed by bothapproaches. We used these 5 bugs in our comparative study for acceptability evaluation.
Subject # bugs # bugs ﬁxed # bugs ﬁxed # bugs ﬁxed
by GenProg by PAR by both
Rhino 17 764
AspectJ 18 090
log4j 15 050
Math 29 531
Lang 20 100
Collections 20 340
Total 119 16 27 5
This experiment was conducted on several machines with
two hexa-core 3GHz CPUs and 16GB RAM. When runningtest cases, we executed them in parallel to accelerate theexperiment. In addition, we memoized the ﬁtness value of aprogram variant to prevent re-evaluation of the same variantagain, as described in [7].
B. RQ1: Fixability
Table IV shows patch generation results of P
ARand Gen-
Prog. P AR successfully ﬁxed 27out of 119 bugs, while
GenProg successfully generated patches for only 16 bugs.
Among ﬁxed bugs, 5 (4 of Rhino + 1 of Math) were ﬁxed
by both P ARand GenProg. Note that the two approaches
generated different patches for these ﬁve bugs. However, these
patches passed all the given test cases successfully. These
patches will be used in our comparative study for patchacceptability (Section IV-C).
Using ﬁx templates is effective for ﬁxing bugs. The Range
Checker template generated a patch for Rhino bug #114493by inserting an if() statement. This patch is shown in
Figure 1(d). AspectJ bug #131933 could be ﬁxed by the ClassCast Checker template because its buggy statements used aninvalid caster. The “Expression Replacer” and “ExpressionAdder and Remover” templates could generate successfulpatches for two Rhino bugs (#192226 and #222635) that hadinvalid conditional expressions in buggy if() statements.
Note that we identiﬁed common ﬁx patterns from Eclipse
JDT and used them to create ﬁx templates. P
ARleverages these
templates to generate patches for bugs of other projects suchas Mozilla Rhino and Apache Commons Math. This indicates
that ﬁx templates learned from one project are reusable for
other projects.
Although we have introduced only a small number of ﬁx
templates in this paper, those templates obviously expandedthe ﬁxability of patch generation. Identifying more ﬁx tem-plates from existing patches might improve ﬁxability further.This remains as future work.
GenProg ﬁxed about 10% of the bugs in our evaluation,
while a recent systematic study [8] reported that it can ﬁxalmost 50% of the bugs in their subjects written in C language.Perhaps, this is because GenProg’s mutation operators mightbe less effective in Java programs. Most Java programs tendto decompose their functionality into small-sized classes andmethods. This limits the number of statements that can beused in GenProg’s mutation operators since it collects and usesstatements in the same scope of the given fault location. On
the other hand, C programs usually have many global variablesand larger methods. This may provide more chances to use
bug-ﬁxing statements in mutation (the authors of GenProgshowed a patch for global variable accessor crashes as arepresentative example of successful cases in [8]). However,this does not imply that our subject selection was biasedagainst GenProg. In our experiments, P
AR had the same
constraints as GenProg. In addition, our approach is not limitedto Java programs.



	PAR generated patches for 27bugs, whereas
GenProg resolved 16 bugs.
C. RQ2: Acceptability
In this section, we measure the acceptability of patches
generated by P AR and GenProg. Since all the successful
patches pass the provided test cases, it is challenging to selectmore or less acceptable patches systematically. Instead, wepresented anonymized patches and asked human subjects toselect more or less acceptable patches.
To answer RQ2 , we formulated the following two null
hypotheses.
•H10: Patches generated by P ARand GenProg have no
acceptability difference from each other.
•H20: Patches generated by P ARhave no acceptability
difference from human-written patches.
The corresponding alternative hypotheses are:
•H1a:PARgenerates more acceptable patches than Gen-
Prog.
•H2a: Patches generated by P ARare more acceptable than
human-written patches.
1) Subjects: For this study, we recruited two different
participant groups: computer science (CS) students and devel-opers. The student group consisted of 17 software engineeringgraduate students who have two to ﬁve-year Java programming
experience. For the developer group, 68 developers partici-
pated in our study. We recruited these developers from bothonline developer communities such as “stackoverﬂow.com”and “coderanch.com”, and software companies such as Daum,a leading Internet software company in Korea. They wereasked to participate in this study only if they had Javaprogramming experience.
2) Study Design: Our user study had ﬁve sessions. In each
session, we showed one of the ﬁve bugs ﬁxed by both P
ARand
GenProg, shown in Table IV . Each session explained in detail
why a bug is problematic and gave a link to the corresponding
bug report. Then, the session listed three anonymized patches:a human-written patch and patches generated by P
ARand
GenProg. Each participant was asked to compare them asa patch reviewer and to report their rankings according toacceptability.
For this study, we built a web-based online survey engine
that shows ﬁve sessions in a random sequence. We gavethe engine’s hyperlink to both the student and the developergroups. At the beginning of our survey, we emphasized thatthe presented patches can pass all the given test cases collectedfrom the corresponding project. There was no time limit, so808TABLE V: Average rankings evaluated by 17 students (standard deviation is shown in
parentheses). The lower values indicate that the patch obtained higher rankings on averageby the evaluators.
Bugs Human PAR GenProg
Math #280 1.33 (0.62) 2.27 (0.59) 2.40 (0.83)
Rhino #114493 2.00 (0.54) 1.33 (0.62) 2.67 (0.72)
Rhino #192226 1.47 (0.64) 1.67 (0.62) 2.67 (0.72)
Rhino #217379 1.69 (0.70) 1.50 (0.63) 2.81 (0.40)
Rhino #76683 2.13 (0.51) 1.07 (0.26) 2.80 (0.41)
Average 1.72 (0.67) 1.57 (0.68) 2.67 (0.64)
TABLE VI: Average rankings evaluated by 68 developers (standard deviation is shown
in parentheses). The lower values indicate that the patch obtained higher rankings onaverage by the evaluators.
Bugs Human PAR GenProg
Math #280 1.92 (0.76) 2.00 (0.82) 2.08 (0.95)
Rhino #114493 1.60 (0.63) 2.40 (0.74) 2.00 (0.93)
Rhino #192226 2.00 (0.68) 1.79 (0.98) 2.21 (0.80)
Rhino #217379 1.62 (0.77) 1.69 (0.63) 2.69 (0.63)
Rhino #76683 1.92 (0.64) 1.23 (0.43) 2.85 (0.38)
Average 1.81 (0.70) 1.82 (0.80) 2.36 (0.90)
that the participants had spend enough time inspecting and
ranking the patches.
3) Result — Students: Average patch acceptability rankings
assigned by 17 student participants are shown in Table V .Consistently, patches generated by P
ARare ranked higher than
those by GenProg for all ﬁve bugs. The average rankings ofpatches generated by P
ARis 1.57, and its standard deviation is
0.68. The average ranking and standard deviation of patchesgenerated by GenProg are 2.67 and 0.64, respectively. Rankingdifferences between P
ARand GenProg are statistically sig-
niﬁcant (p-value =0.000 <0.05). The Wilcoxon signed-rank
test [23] was used for this statistical test since we comparedtwo related samples and these samples are non-parametric.Based on the results, we can reject the null hypothesis H1
0
for the student group.
Some students ranked patches generated by P ARas good
as or even higher than human-written patches. However, thismay not be necessarily indicate that patches generated by P
AR
are better than human-written patches (average: 1.72, standarddeviation: 0.67). Their ranking differences are not statistically
signiﬁcant (p-value =0.257 >0.05). Thus, we cannot reject
the null hypothesis H2
0for the student group.
4) Result — Developers: The survey results of 68 devel-
opers are shown in Table VI. Similar to Table V , patchesgenerated by P
ARare ranked higher than those by GenProg
except for Rhino Bug #114493. For this bug, the developersmight think the patch generated by GenProg was more ac-
ceptable than the patch generated by P
AR, since it assigned a
default value when the lhs variable was undefined (see
Figure 1(b)). However, this does not represent the overallresults.
The average rankings of P
ARand GenProg are 1.82 and
2.36, respectively. Their standard deviation values are 0.80 and0.90. Since ranking differences between P
ARand GenProg are
statistically signiﬁcant (p-value =0.016 <0.05), we can reject
the null hypothesis H10for the developer group.
The average ranking and standard deviation of human-
written patches are 1.81 and 0.70, respectively. Rankingdifferences between patches generated by P
ARand human-
written patches are not statistically signiﬁcant (p-value =0.411>0.05). Thus, the null hypothesis H2
0cannot be rejected.TABLE VII: Indirect patch comparison results.
Selection # response
PAR 130 (21%)
Both 175 (28%)
Human 229 (37%)
Not Sure 87 (14%)
Total 621 (100%)
(a) P ARcomparison results.Selection # response
GenProg 68 (20%)
Both 40 (12%)
Human 176 (51%)
Not Sure 60 (17%)
Total 344 (100%)
(b) GenProg comparison results.
Our two comparative studies (student and developer) con-
sistently show that patches generated by P ARhave higher
rankings than those generated by GenProg on average. Inaddition, this result is statistically signiﬁcant. Between P
AR
and human-written patches, both studies show different re-sults but ranking differences are not statistically signiﬁcant.This implies that our approach can generate more acceptablepatches than GenProg. In addition, patches generated by P
AR
are comparable to human-written patches.
5) Indirect Patch Comparison: The user studies in Sec-
tions IV-C3 and IV-C4 showed the direct patch comparisonresults, but the results are limited to ﬁve bugs ﬁxed by bothapproaches.
To address this issue, we conducted a user study to indirectly
compare the acceptability of all 43 patches generated by P
AR
(27patches) and GenProg (16 patches), by comparing them
to the corresponding human-written patches. We built a web-
based online survey engine for this study. Each survey sessionshowed a pair of anonymized patches (one from human andthe other from P
ARor GenProg for the same bug) along
with corresponding bug information. Participants were askedto select more acceptable patches if they were patch reviewers.In addition, participants were given the choice of both are
acceptable or not sure if they could not determine acceptable
patches. We randomly presented all 43 sessions to participants,and they could answer as many sessions as they wanted.
For this study, we recruited participants by posting online
survey invitations in software developer communities andpersonal twitters. We also sent invitation emails to CS un-dergraduate students who took the Software Engineering classin Spring 2012 at the Hong Kong University of Science andTechnology, since they have Java programming knowledge. Inall survey invitations, we clearly stated that only developers/s-tudents who have Java experience are invited.
The survey results are shown in Table VII. Total 168
(72 students and 96 developers) participants answered 965sessions. The session response rate (P
AR:GenProg = 621:344)
is similar to the rate of successful patches generated by eachapproach (P
AR:GenProg = 27:16). This implies that our survey
sessions were randomly presented and answered.
As shown in Table VII(a), participants chose patches gen-
erated by P ARas more acceptable in 130 (21%) out of 621
sessions and selected “ both were acceptable ” in 175 (28%)
sessions. In total, participants chose patches generated by P AR
as acceptable patches in 305 (49%) sessions (P AR: 21% +
both: 28%). On the other hand, participants selected patchesgenerated by GenProg as acceptable patches in 108 (32%) outof 344 sessions (GenProg: 20% +both: 12%), as shown in
Table VII(b).809This indirect comparison result also shows that patches
generated by P ARare more acceptable.



	PAR generates more acceptable patches than
GenProg does.
V. D ISCUSSION
This section discusses unsuccessful patches generated by
PAR, and identiﬁes threats to validity of our experiments.
A. Unsuccessful Patches
In Table IV , 92 out of 119 bugs were not ﬁxed by our
approach. We examined the main cause of patch generationfailures and identiﬁed two main challenges: branch conditions
and no matching patterns , as shown in Table VIII. We discuss
each challenge in detail.
Branch conditions indicates that P
ARcannot generate predi-
cates to satisfy branch conditions at the fault location by usingﬁx templates. P
ARcould not ﬁx 26 (28%) bugs due to this rea-
son. For example, Rhino bug #181834 occurs when the scope
variable is assigned to a NativeCall orNativeWith type
object. To ﬁx this bug, an appropriate control statement mustbe inserted before the fault location. This control statementhas a predicate checking the type of scope . Generating this
predicate from scratch is challenging.
No matching pattern indicates that P
ARcannot generate
a successful patch for a bug since no ﬁx template has
appropriate editing scripts. P AR could not ﬁx 66 (72%)
bugs due to this reason. For example, to ﬁx AspectJ Bug#109614, its human-written patch added a control statementbefore the fault location: “ if(sources[i] instanceof
ExceptionRange)... ”. However, ExceptionRange
cannot be inferred from the fault location. For P
ARto ﬁx this
bug, matching ﬁx templates must be created.
We inspected 11 bugs ﬁxed by GenProg but failed to be
ﬁxed by P AR. Our approach could not ﬁx 7 out of 11 bugs,
since there are no matching ﬁx patterns for these bugs. Thisimplies if we add more ﬁx patterns, these bugs might beﬁxable by our approach. Adding more ﬁx patterns remains asfuture work. The other 4 bugs belong to the branch conditions
category.
B. Threats to V alidity
We identify the following threats to the validity of our
experiments.
•Systems are all open source projects: We collected bugs
only from open source projects to examine our approach.
Therefore, these projects might not be representative of
closed-source projects. Patches of closed-source projectsmay have different patterns.
•Some participants of our user studies may not bethoroughly qualiﬁed. In our survey invitations for the
developer group, we clariﬁed that only developers can
participate in the survey. However, we could not fullyverify the qualiﬁcations of the survey participants.TABLE VIII: Causes of unsuccessful patches.
Cause #o fb u g s
Branch condition 26 (28%)
No matching pattern 66 (72%)
VI. R ELATED WORK
Weimer et al. [7] proposed GenProg, an automatic patch
generation technique based on genetic programming. Thisapproach randomly mutates buggy programs to generate sev-eral program variants that are possible patch candidates. The
variants are veriﬁed by running both passing and failing test
cases. If a variant passes all test cases, it is regarded as asuccessful patch of a given bug. In 2012, the authors extendedtheir previous work by adding a new mutation operation,replacement and removing the switch operation [8]. In addi-
tion, they provided systematic evaluation results with 105 real
bugs [8]. Although the evaluation showed that GenProg ﬁxed55 out of 105 bugs, GenProg can generate nonsensical patchesthat may not be accepted by developers as shown Section IV-C.
Fry et al. conducted a human study to indirectly measure
the quality of patches generated by GenProg by measuringpatch maintainability [24]. They presented patches to partic-ipants and asked maintainability related questions developedby Sillito, Murphy, and V older [25]. In addition, they pre-sented machine-generated change documents [26] along withpatches to participants. They found that machine-generatedpatches [8] with machine-generated documents [26] are com-parable to human-written patches in terms of maintainability.We also compared machine-generated patches with human-written patches. However, instead of asking the maintainabil-ity related questions, we asked participants which patch ismore/less acceptable for direct comparison. In addition, wecompared patches generated by two different patch generationapproaches, GenProg and P
AR.
Demsky et al. focused on avoiding data structure incon-
sistency [27], [28]. Their approach checks data structureconsistency by using formal speciﬁcations and inserts run-time monitoring code to avoid inconsistent states. However,this technique provides workarounds rather than actual patchessince it does not modify source code directly.
Arcuri et al. introduced an automatic patch generation
technique [5]. Although they also used genetic programming,their evaluation was limited to small programs such as bub-ble sorting and triangle classiﬁcation, while our evaluationincludes real bugs in open source software. Their approachrelies on formal speciﬁcations, which our approach does notrequire.
Wei et al. proposed a contract-based patch generation tech-
nique [10]. This technique also relies on speciﬁcations (i.e.,contracts). In addition, this technique can generate only fourkinds of program variants. These variants check only contractviolations, whereas our approach generalizes human-written
patches and generates various program variants.
PACHIKA [6] leverages object behavior models. PACHIKA
is evaluated on 26 bugs from Mozilla and Eclipse. However,
PACHIKA created successful patches for only three out of26 bugs in their evaluation since it generates only a limited810number of program variants (i.e., recorded object behavior).
Our evaluation also includes these 26 bugs (see Table III), and
PARsuccessfully ﬁxed 15 bugs including the three bugs ﬁxed
by PACHIKA.
Martinez and Monperrus identiﬁed common program repair
actions (patterns) from around 90,000 ﬁxes [15]. They claimedthat most common repair actions are semantic change patternssuch as “Additional functionality.” However, their identiﬁedpatterns are too abstract and coarse-grained to be used inautomatic patch generation. Our ﬁx templates described in
Section III-B3 are concrete enough to be used in automaticpatch generation.
SYDIT [29] automatically extracts an edit script from a
program change. Its limitation is that a user must specify aprogram change to extract an edit script and a target programto apply it. In addition, SYDIT cannot take multiple pro-gram changes to extract a (generalized) edit script. SYDIT’sauthors proposed an improved technique called LASE [30].This technique can take multiple changes to extract moregeneral edit scripts and automatically ﬁnd a target program.
P
ARcan leverage these techniques to automatically create ﬁx
templates.
VII. C ONCLUSION
In this paper, we have proposed a novel patch generation
approach, P AR, learned from human-written patches to address
a limitation of existing techniques such as GenProg [7], [8]:generating nonsensical patches. We ﬁrst manually inspected
human-written patches and identiﬁed common ﬁx patterns,which we then used in automatic patch generation. Our ex-perimental results on 119 real bugs showed that our approachsuccessfully generated patches for 27bugs, while GenProg
was successful for 16 bugs. To evaluate whether the generated
patches were acceptable for ﬁxing bugs, we conducted a userstudy of 253 participants (89 students and 164 developers).This study showed that our approach generated more accept-able patches than GenProg, and our patches were comparableto human-written patches.
Our pattern-based approach might be useful in improving
other automatic patch generation techniques. For example,contract-based techniques [10] alone can generate only fourkinds of variants for each bug, but ﬁx templates could beused to generate more program variants for such techniques.
P
ARcould also be used to generate more program variants in
model-based techniques [6].
Our future work includes automatic ﬁx template mining and
balanced test case generation . First, although the proposed ﬁx
templates are successfully used for generating patches, more
ﬁx templates are desirable to ﬁx more bugs efﬁciently. We planto investigate more human-written patches and develop auto-matic algorithms to extract ﬁx templates. Second, our approachrequires test cases to evaluate program variants, but often fewfailing test cases are available for bugs. Since the imbalance
in test cases may lead to inaccurate patch evaluation, we
will develop failing-test generation techniques by leveragingexisting automatic testing techniques [31], [32], [20].All materials used in this paper and detailed patch genera-
tion results are publicly available at
https://sites.google.com/site/autofixhkust/
REFERENCES
[1] The Guardian, “Why we all sell code with bugs,” Aug 2006.
[2] A. Zeller, Why Programs Fail: A Guide to Systematic Debugging .
Morgan Kaufmann, October 2005.
[3] Technology Review, “So many bugs, so little time,” Jul 2010.
[4] CNN, “Will bugs scare off users of new Windows 2000,” Feb 2000.
[5] A. Arcuri and X. Yao, “A novel co-evolutionary approach to automatic
software bug ﬁxing,” in CEC ’08 .
[6] V . Dallmeier, A. Zeller, and B. Meyer, “Generating ﬁxes from object
behavior anomalies,” in ASE ’09 .
[7] W. Weimer, T. Nguyen, C. Le Goues, and S. Forrest, “Automatically
ﬁnding patches using genetic programming,” in ICSE ’09 .
[8] C. Le Goues, M. Dewey-V ogt, S. Forrest, and W. Weimer, “A systematic
study of automated program repair: Fixing 55 out of 105 bugs for $8
each,” in ICSE ’12 .
[9] J. R. Koza, Genetic Programming: On the Programming of Computers
by Means of Natural Selection , 1st ed. The MIT Press, Dec. 1992.
[10] Y . Wei, Y . Pei, C. A. Furia, L. S. Silva, S. Buchholz, B. Meyer, and
A. Zeller, “Automated ﬁxing of programs with contracts,” in ISSTA ’10 .
[11] D. Babich, P. J. Clarke, J. F. Power, and B. M. G. Kibria, “Using a
class abstraction technique to predict faults in OO classes: a case study
through six releases of the eclipse JDT,” in SAC ’11 .
[12] B. Dagenais and M. P. Robillard, “Recommending adaptive changes for
framework evolution,” in ICSE ’08 .
[13] J. Bevan, E. J. Whitehead, Jr., S. Kim, and M. Godfrey, “Facilitating
software evolution research with kenyon,” in ESEC/FSE ’05 .
[14] Y . Tian, J. Lawall, and D. Lo, “Identifying linux bug ﬁxing patches,” in
ICSE ’12 .
[15] M. Martinez and M. Monperrus, “Mining repair actions for guiding
automated program ﬁxing,” INRIA, Tech. Rep., 2012.
[16] T. T. Nguyen, H. A. Nguyen, N. H. Pham, J. M. Al-Kofahi, and T. N.
Nguyen, “Graph-based mining of multiple object usage patterns,” in
ESEC/FSE ’09 .
[17] Y . Tao, Y . Dang, T. Xie, D. Zhang, and S. Kim, “How do developers
understand code changes? an exploratory study in industry,” in FSE ’12 .
[18] B. L. Miller and D. E. Goldberg, “Genetic algorithms, selection schemes,
and the varying effects of noise,” Evol. Comput. , vol. 4, no. 2.
[19] A. Nistor, Q. Luo, M. Pradel, T. R. Gross, and D. Marinov, “BALLE-
RINA: automatic generation and clustering of efﬁcient random unit tests
for multithreaded code,” in ICSE ’12 .
[20] H. Jaygarl, S. Kim, T. Xie, and C. K. Chang, “OCAT: object capture-
based automated testing,” in ISSTA ’10 .
[21] L. S. Pinto, S. Sinha, and A. Orso, “Understanding myths and realities
of test-suite evolution,” in FSE ’12 .
[22] A. Arcuri and L. Briand, “A practical guide for using statistical tests to
assess randomized algorithms in software engineering,” in ICSE ’11 .
[23] F. Wilcoxon, “Individual comparisons by ranking methods,” Biometrics
Bulletin , vol. 1, no. 6.
[24] Z. P. Fry, B. Landau, and W. Weimer, “A human study of patch
maintainability,” in ISSTA ’12 .
[25] J. Sillito, G. C. Murphy, and K. De V older, “Questions programmers
ask during software evolution tasks,” in FSE ’06 .
[26] R. P. Buse and W. R. Weimer, “Automatically documenting program
changes,” in ASE ’10 .
[27] B. Demsky and M. Rinard, “Data structure repair using goal-directed
reasoning,” in ICSE ’05 .
[28] B. Demsky, M. D. Ernst, P. J. Guo, S. McCamant, J. H. Perkins, and
M. Rinard, “Inference and enforcement of data structure consistencyspeciﬁcations,” in ISSTA ’06 .
[29] N. Meng, M. Kim, and K. S. McKinley, “Systematic editing: generating
program transformations from an example,” in PLDI ’11 .
[30] ——, “Lase: Locating and applying systematic edits by learning from
examples,” in ICSE ’13 .
[31] C. Pacheco, S. K. Lahiri, M. D. Ernst, and T. Ball, “Feedback-Directed
random test generation,” in ICSE ’07 .
[32] J. H. Andrews, T. Menzies, and F. C. Li, “Genetic algorithms for
randomized unit testing,” IEEE Trans. on Softw. Eng. , vol. 37, no. 1,
pp. 80–94, Feb. 2011.811