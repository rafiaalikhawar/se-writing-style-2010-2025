Characterizing Logging Practices in Open-Source Software
Ding Yuan ‡†, Soyeon Park †, and Yuanyuan Zhou †
†University of California, San Diego, ‡University of Illinois at Urbana-Champaign
{diyuan,soyeon,yyzhou }@cs.ucsd.edu
Abstract —Software logging is a conventional programming
practice. While its efﬁcacy is often important for users and de-
velopers to understand what have happened in the production
run, yet software logging is often done in an arbitrary manne r.
So far, there have been little study for understanding loggi ng
practices in real world software. This paper makes the ﬁrst
attempt (to the best of our knowledge) to provide a quantitat ive
characteristic study of the current log messages within fou r
pieces of large open-source software. First, we quantitati vely
show that software logging is pervasive. By examining devel -
opers’ own modiﬁcations to the logging code in the revision
history, we ﬁnd that they often do not make the log messages
right in their ﬁrst attempts, and thus need to spend a signiﬁc ant
amount of efforts to modify the log messages as after-thoughts .
Our study further provides several interesting ﬁndings on
where developers spend most of their efforts in modifying th e
log messages, which can give insights for programmers, tool
developers, and language and compiler designers to improve
the current logging practice. To demonstrate the beneﬁt of o ur
study, we built a simple checker based on one of our ﬁndings
and effectively detected 138 pieces of new problematic logging
code from studied software (24 of them are already conﬁrmed
and ﬁxed by developers).
Keywords -log message, log quality, empirical study, failure
diagnosis
I. I NTRODUCTION
Writing software log messages is a well established
programming practice to record the dynamic information
during a program’s execution. It is often used in failure
diagnosis, auditing, proﬁling, etc. Figure 1 shows how
developers write log messages in real-world open-source
software. In a log message, developers describe the logged
event using static text and optionally record variable values
related to the event. Each log message also has a verbosity
level. For instance, Figure 1 shows four types of different
verbosity levels: fatal (i.e., abort a process after logging),
error (i.e., record error events), info (i.e., record important
but normal events), debug (i.e., verbose logging only for
debugging). The verbosity levels for less critical events ( e.g.,
debug ) naturally subsume those for more critical events
(e.g.error ), meaning that all the events logged under the
latter are also recorded under the former. Using verbosity
level, users or developers can trade the beneﬁt of sufﬁcient
log messages with their cost (e.g., performance overhead).
Under the default production setting, open-source softwar e
typically only log error events in addition to a few (less
than 10% [35]) book-keeping messages (e.g., info ). Other/gid1/gid2/gid3/gid4/gid5/gid6/gid7/gid8/gid9/gid8/gid10 /gid11/gid5/gid12/gid3/gid13/gid14/gid5/gid3/gid15/gid5/gid16/gid1/gid16/gid3/gid17/gid18/gid19/gid20/gid21/gid5/gid5/gid5/gid5/gid5/gid5/gid5/gid5/gid5/gid5/gid5/gid5/gid5/gid5/gid5/gid5/gid5/gid5/gid5/gid5/gid5/gid5/gid5/gid5/gid5/gid5/gid5/gid5/gid5/gid5/gid5/gid5/gid5/gid5/gid5/gid5/gid5/gid5/gid5 /gid5/gid22/gid23/gid24/gid3/gid25/gid14/gid4/gid17/gid1/gid26/gid27/gid10/gid23/gid22
/gid28/gid29/gid30/gid2/gid3/gid4/gid30/gid1/gid17/gid17/gid3/gid17/gid5/gid6/gid31/gid32/gid32 /gid11/gid5/gid12/gid33/gid3/gid13/gid2/gid34/gid5/gid35/gid3/gid14/gid5/gid3/gid29/gid1/gid35/gid5/gid33/gid36/gid28/gid17/gid25/gid1/gid14/gid5/gid37
/gid5/gid5/gid5/gid5/gid5/gid5/gid5/gid5/gid5/gid5/gid5/gid5/gid5/gid5/gid5/gid5/gid5/gid5/gid5/gid33/gid3/gid35/gid38/gid1/gid17/gid25/gid39/gid3/gid35/gid5/gid33/gid3/gid35/gid15/gid39/gid4/gid5/gid15/gid39/gid2/gid1/gid5/gid40/gid25/gid19/gid11/gid5 /gid1/gid2/gid3/gid4/gid3/gid5/gid6/gid7 /gid20/gid21/gid5
/gid2/gid3/gid4/gid39/gid14/gid5/gid6/gid41/gid42/gid7/gid43 /gid11/gid5/gid12/gid8/gid13/gid14/gid36/gid1/gid35/gid14/gid39/gid33/gid28/gid14/gid39/gid3/gid35/gid5/gid17/gid1/gid15/gid13/gid25/gid1/gid34/gid44/gid5/gid40/gid25/gid19/gid11/gid5 /gid8/gid9/gid3/gid7 /gid20/gid21/gid5/gid5/gid5/gid5/gid5/gid5/gid5/gid5/gid5/gid5/gid5/gid5/gid5/gid5/gid22/gid23/gid5/gid5/gid43/gid29/gid1/gid35/gid26/gid26/gid45/gid5/gid5/gid23/gid22
/gid1/gid2/gid3/gid4/gid6/gid46/gid31/gid47/gid48/gid49/gid50 /gid11/gid5/gid12/gid28/gid17/gid33/gid36/gid39/gid38/gid1/gid34/gid5/gid14/gid17/gid28/gid35/gid25/gid28/gid33/gid14/gid39/gid3/gid35/gid5/gid2/gid3/gid4/gid5/gid15/gid39/gid2/gid1/gid5/gid40/gid25/gid19/gid11/gid5 /gid10/gid8/gid2/gid11 /gid20/gid21/gid5/gid22/gid23/gid24/gid3/gid25/gid14/gid4/gid17/gid1/gid26/gid27/gid10/gid23/gid22/gid5/gid22/gid23/gid5/gid5/gid5/gid5/gid8/gid29/gid28/gid33/gid36/gid1/gid5/gid5/gid5/gid5/gid23/gid22
Figure 1. Log printing code examples from open-source software.
verbose mode messages are typically used only during in-
house testing.
Logs are particularly beneﬁcial to diagnosing failures in
the production environment, which often require immediate
resolutions as they directly impact the end users. Log mes-
sages printed during the production run are often the only
data source available for the developers to diagnose such
failures. This is because it is often challenging to reprodu ce
production failures. First, end-users are often reluctant to
release their failure-triggering input due to privacy conc erns.
Second, it can be forbiddingly expensive for the vendors
to recreate the exactly same execution environment (e.g.,
the same hardware, third party library, OS, etc.) as in
the production setting. Consequently, software engineers
mostly rely on log messages for trouble-shooting productio n
failures. Besides developers, logs are also used by system
administrators to resolve failures caused by security atta cks,
hardware errors, and misconﬁgurations.
The importance of logging has been widely identi-
ﬁed [14]. Consequently, practically almost all open-sourc e
software print log messages. For example, the widely used
OpenSSH server contains 3407 log printing statements in its
81K lines of code, and 2241 of them are printed under the
default verbosity mode.
The importance of log message is also proved by its
commercial acceptance. It is an industry common practice
for vendors to request logs or even have their systems
to automatically sending the logs back periodically (i.e.,
“call-home” [6]). As a result, most modern systems today
(such as those from EMC [7], NetApp [21], Cisco [5],
Dell [6], just to name a few) are able to collect logs from at
least 50% of their customers [7], [21], [5], [6]. Once such
logs are collected, they can be either analyzed by human
or by commercial log analysis tools (e.g., Splunk [26] or
ArcSight [1]).978-1-4673-1067-3/12/$31.00 c2012 IEEE ICSE 2012, Zurich, Switzerland 102A. State of the Art of Logging Design
Despite the importance of log messages, logging is often
a subjective and arbitrary practice in reality. In general,
there is no rigorous speciﬁcation or systematic process on
software logging, partially because logging is seldom a cor e
feature provided by the vendors. Therefore, in many cases,
log messages are written as “after-thoughts” after a failur e
happens and logs are needed. There are a number of logging
libraries, such as syslog [28] and log4j [16], that provide
better logging interface (e.g., multiple verbosity levels ) than
general-purpose output functions (e.g., printf). But even
with them, it is still the developers’ arbitrary decisions o n
when, what and where to log. Recently, a few research
work proposed to systematically improve a few aspects of
software logging. LogEnhancer [36] automatically suggest s
which variable values need to be recorded in each existing
log message. However, even with LogEnhancer, developers
still need to make majority of the logging decisions.
Improving the current logging practice will signiﬁcantly
beneﬁt from a deep understanding of the real-world logging
characteristics. Speciﬁcally, we ﬁrst need to assess wheth er
the current practice is good enough, and if not, what are
the common issues and what are their consequences. Not
only it could provide programmers with useful guidelines
and motivations for better logging, but also shed lights to
new tools and programming language support for systematic
logging, better testing of logging, logging improvement, e tc.
B. Our Contributions
This paper makes the ﬁrst attempt (to the best of our
knowledge) to study the practice of software logging. Speci f-
ically, we try to answer the following questions: (i) how
pervasive is software logging? (ii) is the current logging
practice good enough? (iii) if not, how are developers
modifying logs?
To answer these questions, we study the logging practice
in four pieces of large, widely used open-source software,
including Apache httpd, OpenSSH, PostgreSQL, and Squid,
each with at least over 10 years of developing history.
To understand the pervasiveness of logging, we study the
density of log messages in source code and the churn
rate [19] from the revision history.
Answering question (ii) and (iii) is more challenging
because judging the logging efﬁcacy requires deep domain
expertise. We address this challenge by studying developers’
own modiﬁcations to their log messages. Speciﬁcally, we
systematically and automatically analyze the revision his tory
of each log message. We further separate those log modi-
ﬁcations that are indeed modifying log messages as after-
thoughts from those merely consistency updates together
with other non-logging code changes. Then we analyze each
category separately, with a focus on the former.
Table I summarizes our major ﬁndings and their implica-
tions. Overall, our study makes the following contribution s:•Our work is the ﬁrst (to the best of our knowledge)
to provide quantitative evidences that logging is an
important software development practice.
•We ﬁnd that despite the importance of effective logging,
unfortunately, developers often are not able to make
log messages right at the ﬁrst time. Therefore, many
of the log messages need to be modiﬁed as after-
thoughts . By examining developers’ own modiﬁcations,
we identify the particular aspects in logging choices
where developers spend most efforts to get them right.
Such ﬁndings can shed lights on various new tools and
program language supports to improve log messages.
•To demonstrate the potential beneﬁts of our ﬁndings,
we developed a simple checker to detect problematic
verbosity level assignment (inspired by our Finding 7).
We detected 138 new problematic cases in the latest
versions of the four pieces of the studied software.
24 of them have already been conﬁrmed and ﬁxed
by the developers as a result of our bug reports. This
result conﬁrms that our ﬁndings are indeed beneﬁcial
to tool developers to systematically help programmers
to improve their log messages.
While we believe that the open-source software we ex-
amined well represent the characteristics of current loggi ng
practice, we do not intend to draw any general conclusions.
Our ﬁndings and implications should be taken with the ex-
amined open-source software and our methodology in mind
(see our discussion about threats to validity in Section II) .
II. M ETHODOLOGY
A. Software Used in Our study
We study four large, widely used software programs
with over at least 10 years of developing history, namely
Apache httpd, OpenSSH, PostgreSQL and Squid. Table II
provides the descriptions. Each of them is popular as it
is ranked either ﬁrst or second in market share for its
product’s category. The lines of code (LOC) is measured
usingsloccount [25], which excludes non-functional code
such as comments, white-spaces, etc.
All of the software we study include server compo-
nent. We choose servers because, ﬁrst, their availability
and reliability are often important since they tend to be
used as infrastructure software providing critical servic es
(e.g., e-commerce services), and thus many users and other
applications are depending on them.
Second, runtime logs are particularly important for diag-
nosing server failures, which are hard to be reproduced due
to the privacy and environment setting issues. They typical ly
run for a long period of time, handle large amounts of data
from users, perform concurrent execution, and are some-
times deployed in a large distributed environment forming
server farms, all of which make failure reproduction and
diagnosis difﬁcult.
2103Table I
OUR MAJOR FINDINGS ON REAL WORLD LOGGING CHARACTERISTICS AND THEIR IMPLICATIONS .
Density of software logging (Section III-A) Implications
(1) On average, every 30 lines of code contains one line of log ging
code. Similar density is observed in all the software we stud ied.Logging is pervasive during software development.
Beneﬁt of log messages in failure diagnosis (Section III-B) Implications
(2) Log messages can speed up the diagnosis time of productio n-run
failures by 2.2 times [35].Logging is beneﬁcial for diagnosing production-run fail-
ures.
Churns of logging code (Section III-C) Implications
(3) The average churn rate of logging code is almost two times (1.8),
compared to the entire code.Logging code is being actively maintained by developers.
(4) In contrast to its relatively small density, logging cod e is modiﬁed
in a signiﬁcant number (18%) of all the committed revisions.Logging code takes a signiﬁcant part of software evolution
despite its relatively small presence.
(5) 33% of modiﬁcations on logging code are after-thoughts. The
remaining ones are updates together with other non-logging code
changes within the same patch to make them consistent. More t han
one third (36%) of our studied log messages have been modiﬁed at
least once as after-thoughts.The current logging practice is ad hoc, introducing problem s
to the log quality. Developers take their efforts to address
them as after-thoughts.
Overview of log modiﬁcations (Section IV) Implications
(6) Developers seldom delete or move logging code, accounti ng for
only 2% of all the log modiﬁcations. Almost all (98%) of the lo g
modiﬁcations are to the content of the log messages.We surmise there is lack of documentations explaining the
purpose of log messages so that developers are conservative
in deleting/moving log messages.
Modiﬁcation to logging content (Section V, VII, VI) Implications
(7) Developers spend signiﬁcant efforts on adjusting the verbosity
level of log messages, accounting for 26% of all the log improve-
ments. Majority (72%) of them reﬂect the changes in develope rs’
judgement about the criticality of an error event.Tools that systematically exposing error conditions would
help testing the logging behavior. Testing and code analysi s
tools need to provide more information (e.g., error condi-
tions) for developers to decide the proper verbosity.
(8) In 28% of verbosity level modiﬁcations, developers reco nsider
the trade-off between multiple verbosities. It might indic ate that de-
velopers are often confused when estimating the cost (e.g., excessive
messages, overhead) and beneﬁt afforded by each verbosity l evel.The scalar design of current verbosity level may not be a
good way to help developers with such logging decision.
More intelligent logging methods, such as adaptive logging ,
can help balancing the trade-offs.
(9) One fourth (27%) of the log modiﬁcations are to variable l ogging.
Majority (62%) of them are adding new variables that are caus ally
related to the log message. It indicates that developers oft en need
additional runtime information to understand how a failure occurred.Logging tools that automatically infer which variables to
log (e.g., LogEnhancer [36]) can help informative logging.
Given failing and passing test cases, Delta debugging [37]
can also be used to infer the relevant variables to log.
(10) 45% of log modiﬁcations are modifying static content (t ext) in
log messages. More than one third (39%) of them are ﬁxing inco n-
sistencies between logs and actual execution information i ntended
to record. Software can leverage programming language supp ort to
eliminate certain inconsistency, as Squid does.Developers should pay more attention to update the log
messages as code changes. Tools combining natural lan-
guage processing and static code analysis can be designed
to detect such inconsistencies.
Table II
OPEN-SOURCE SOFTWARE WE STUDIED . THE THIRD COLUMN SHOWS THE POPULARITY OF THE SOFTWARE IN ITS OW N PRODUCT
CATEGORY . *: A MONG ONLY OPEN -SOURCE DATABASE SERVERS ,IT HAS THE SECOND LARGEST MARKET SHARE .
Software DescriptionMarketLOCVerbose Log messages Lines of LOCRCSshare levels total default mode logging code per log
Apache httpd 2.2.16 Web server top 1 [22] 249K 8 1838 1100 (warn ) 6758 36 SVN
OpenSSH 5.8p2 Secure shell server top 1 [23] 81K 8 3407 2241 (info ) 4672 17 CVS
PostgreSQL 8.4.7 Database server top 2* [30] 614K 13 6052 5818 (warn ) 20733 30 GIT
Squid 3.1.16 Caching web proxy top 1 [31] 155K 10 3474 1268 (info ) 4103 38 Bazzar
Total 1.1M 14771 10427 36266 30
B. Study Methodology
We study various aspects of the logging practice. To study
the density of logging code, we measure both the lines of
code (LOC) of the entire program and the logging code
(shown in Table II). Note that the LOC of logging code
is larger than the number of log printing statements since a
logging statement might occupy more than one code lines.
The code churn rate is measured in Churned LOC/T otal LOC [20]. The churn rate of logging code
is thus measured in churned LOC of logging code
/LOC of logging code . We analyze each revision in the
recent ﬁve year’s history of software to measure the churned
code. We ﬁrst measure the churn rate for each of the years,
then take the average of these ﬁve one-year churn rates.
For studying the modiﬁcations on logging code, we only
focus on the modiﬁcations to the log printing behavior,
including verbosity levels, static content, variable valu es and
3104log message location. None-behavioral modiﬁcations, such
as renaming log printing functions or verbosity levels (e.g .,
fromwarn towarning ), indent changes, etc., are excluded
from our analysis.
With the collected modiﬁcations, we study the types of
modiﬁcations. In the revision history, there could be two
types of log modiﬁcations: some are merely for consistent
update with other non-logging code changes within the same
revision , and others are modifying the logging behavior
as after-thoughts. To separate the modiﬁcation types, one
possible policy is to check whether the revisions only inclu de
changes solely to logging code but not to other code.
However, this is too conservative in that developers tend
to batch multiple patches into one revision.
Instead, we use a few simple heuristics following our
observations on the common logging practice: developers
often log right after checking a certain condition (e.g., an
error condition), which is usually implemented with a branc h
statement (e.g., if, while, etc.). In a revision, if such a br anch
statement is modiﬁed together with following logging code,
it may not be introduced for logging adjustment, but for
changing program semantic together with logging. Similarl y,
if a variable or a function is renamed consistently both in th e
logging code and non-logging code within the same revision,
it is also not modifying the logging behavior.
Since our automatic analysis may not be accurate, we
further manually verify our analysis result on 400 randomly
sampled modiﬁcations (100 from each program). Our man-
ual veriﬁcation suggests the accuracy of our analysis is 94% .
We further study the details of those log modiﬁcations.
For some types of such modiﬁcations, to reason about why
the previous logging was problematic or insufﬁcient, we
randomly sample the same modiﬁcation cases and carefully
examine the relevant source code, comments, commit log,
bug reports, and discussions in mailing list (if any). If we
cannot clearly understand the reason, we always conserva-
tively classify them as the “other” category when presentin g
our results. The conﬁdence interval of our sampling is
reported together with our result whenever sampling is used .
C. Threats to Validity
As with all the previous characteristics study, our work
is also subject to a validity problem. Potential threats to t he
validity of our characteristic study are representativene ss of
the software and examination methodology.
To address the former, we selected diverse open-source
software in terms of functionality, including Web server,
database server, caching proxy, shell server, and together
with their client utilities, all of which are widely used in
their product category as shown in Table II. They have at
least 10 years of history in their code repositories and more
than 14771 static log points in source code. Overall, we
believe that they well represent large software which embed
the current logging practices. However, our study may notreﬂect the characteristics of logging practices in other ty pes
of non-server/client software, such as scientiﬁc applicat ions,
operating systems, commercial software, or software writt en
in other programming languages.
As for our examination methodology, we try to minimize
our own subjective judgement on the quality of log messages
by systematically analyzing developers’ own modiﬁcations
to the log messages. Also, we examine developers’ commit
logs, comments, related bug reports, etc., together with th e
source code to reason about the modiﬁcations. Furthermore,
our study includes all of the aspects typically considered
by developers for logging, including verbosity level, stat ic
content and variables to record, and log placement in source
code. As a limitation, for some logging problems unknown
even to the developers, our methodology may also miss
them, since the modiﬁcation is not in revision history.
However, if the problem is general enough, it should have
been ﬁxed in at least one of the program we studied.
We do not study the additions of new logging code. This is
because our goal is to reveal issues with the current logging
practices, therefore we only focus on the modiﬁcations
(including deletions) to the previously existing logging c ode.
However, adding new logging code might also reﬂect issues
with existing logs where it is a revival of the existing
logging code that has been deleted once. While we study
the deletions in such addition/deletion chains, we will mis s
the additions where they might have important meanings as
well. However, our results in Table V and Table XII suggest
that the deletions of log messages rarely occur (less than 2%
among all of the modiﬁcations). Therefore, we expect that
such deletion/addition chains are also rare.
Overall, while we cannot draw any general conclusions
that can be applied to all software logging, we believe that
our study provides insights about efﬁcacy and pitfalls of so ft-
ware logging, particularly in open source server applicati ons
written in C/C++.
III. I MPORTANCE OF LOGMESSAGES
In this section we study the pervasiveness of software
logging in reality and the beneﬁt of software logging to
production-run failure diagnosis.
A. Code Density of Sofware Logging
Finding 1: On average, every 30 lines of code contains
one line of logging code. Similar density is observed in
all the software we studied.
Implications: Logging is a pervasive practice during
software development.
The code density of software logging is shown in the
“LOC per log” column in Table II. It is calculated using
the LOCs of logging code and the entire code. Even in the
software with least log density (Squid), there is still one l ine
of logging code per 38 lines of code.
41050 %25 %50 %
apache openssh postgres squidChurn RateLogging code
Entire code base
Figure 2. Churn rates comparison between logging and entire code.
B. Beneﬁt of log messages in failure diagnosis
Finding 2 (Beneﬁt of log messages): Log messages
reduces median diagnosis time of production-run failures
between 1.4 and 3 times (on average 2.2X speedup).
Implications: Logging is beneﬁcial for failure diagnosis.
We randomly sampled 250 user reported failures from
Apache, Squid, and PostgreSQL, and compared the failure
resolution time between the set of failures where user
provided any log messages with the ones without any
log messages. The details are discussed in our previous
work [35]. Jiang et. al. [13] revealed a similar ﬁnding by
studying production failures in commercial systems.
C. Churns of Logging Code
Finding 3: As shown in Figure 2, the average churn rate
of logging code is almost two times (1.8) compared to
the entire code. Interestingly, except for PostgreSQL, all
the software show that logging code have higher churn
rates than the entire code base.
Implications: Developers are actively maintaining log-
ging code like other non-logging code for software func-
tionality. Logging is at least as important as other part of
code in the maintenance perspective.
Such churns on logging code are also scattered across
many revisions, indicating the logging code is continuously
maintained as a signiﬁcant part of software evolution:
Finding 4: In contrast to the relatively small density of
logging code (Finding(1)), a signiﬁcant number (18%) of
all the committed revisions modify logging code.
Implications: Logging code takes a signiﬁcant part of
software evolution despite its relatively small presence.
Overall, Finding 3 and 4 together implicate that logging
code is being continuously and actively modiﬁed. To under-
stand what these modiﬁcations are, we studied modiﬁcations
from the revisions of our studied software.
Table III shows the detailed classiﬁcation of modiﬁcations
to logging code, which is from our automatic analysis tool
on committed revisions (with 94% accuracy) as described
in Section II-B. Our tool identiﬁes a modiﬁcation as a
consistent update with the other changes on non-logging
code if the same patch contains one of the following three
types of changes: (i) modiﬁcation on the conditions thatTable III
MODIFICATIONS IN EACH REVISION TO THE LOG MESSAGES .
Software totalafter- following other code change
thoughts condition variable function
apache 3035 810 (27%) 1941 64 220
openssh 3459 1446 (42%) 1703 284 26
postgres 15455 5389 (35%) 9153 746 167
squid 5536 1431 (26%) 2951 930 224
Total 27485 9076 (33%) 15748 2024 637
Table IV
LOG MESSAGES (%) THAT HAVE BEEN MODIFIED .
log msgs apache openssh postgres squid total
modiﬁed 605 628 3128 1106 5367
total 1838 3407 6052 3474 14771
percentage 40% 18% 52% 30% 36%
Table V
TYPE OF LOG MODIFICATIONS AS AFTER THOUGHTS .
SoftwareLog Modiﬁcations
total location verbosity text variables
apache 810 35 (4%) 118 (15%) 429 (52%) 228 (28%)
openssh 1446 33 (2%) 550 (38%) 264 (18%) 599 (41%)
postgres 5389 17 (1%) 1148 (21%) 3000 (56%) 1224 (23%)
squid 1431 65 (5%) 573 (40%) 364 (25%) 429 (30%)
Total 9076 150 (2%) 2389 (26%) 4057 (45%) 2480 (27%)
the logging code is dependent on; (ii) re-declaration of the
logged variable that is also changed in logging code; (iii)
modiﬁcation on a function name that is also referred in the
logging code as static text. Otherwise, our tool classify th e
modiﬁcation on logging code as an after-thought. Table IV
shows the number of log messages that have been modiﬁed
at least once as after-thoughts by these 9076 modiﬁcations.
Finding 5: 33% modiﬁcations on logging code are after-
thoughts . The remaining ones are consistent updates with
the other changes on non-logging code in the same patch .
As a result, 36% of the total 14771 log messages have
been modiﬁed at least once as after-thoughts.
Implication: In current practice, logging is conducted in
a subjective and arbitrary way, introducing problems to
the log quality. Developers take efforts to improve them
as after-thoughts.
In remainder of the paper, we will use modiﬁcations to
only refer to these modiﬁcations that are notconsistent up-
dates with other non-logging code changes, unless otherwis e
speciﬁed. We focus on studying these modiﬁcations as they
are likely to reﬂect more directly developers’ concerns ove r
the previously problematic log messages.
IV. O VERVIEW OF LOGMODIFICATIONS
In Table V, we further break the 9076 modiﬁcations based
on what developers modiﬁed: the location of logging code
within the source code, verbosity level, static content of a
log message, and variables to log. For location change, we
consider the logging code’s relative location within a basi c
block, including both move and deletion.
5106Table VI
VERBOSITY -LEVEL MODIFICATION WITH ERROR EVENT
LOGGING AND WITH NON -ERROR EVENT LOGGING
software total error non-error
apache 118 84 (71%) 34 (29%)
openssh 550 398 (72%) 152 (28%)
postgres 1148 831 (72%) 317 (28%)
squid 573 399 (70%) 174 (30%)
Total 2389 1712 (72%) 677 (28%)
Finding 6: Developers seldom delete or move the logging
code, accounting for only 2% of all the log modiﬁcations.
Almost all (98%) modiﬁcations are to the content of
the log messages, namely verbosity level, static text and
variables.
Implications: Given the lack of speciﬁcations on logging
behaviors, developers would not delete/move log messages
unless they cause serious problems (Section VIII).
V. V ERBOSITY LEVELS MODIFICATION
This section analyzes the 2389 modiﬁcations to verbosity
levels (Table V), which indicate developers often do not
assign the right verbosity level at the ﬁrst time.
In Table VI, we further break down the verbosity level
modiﬁcation into those for error event logging and non-erro r
event logging. In the former (72%), at least one verbosity
level before or after the modiﬁcation is an error-level (e.g .,
error ,fatal , etc.). These indicate that developers might
have misjudged how critical the event to log is at the ﬁrst
place. Please recall that in these modiﬁcations developer
did not change the conditions (typically the error condition)
leading to the log messages, but only the verbosity level
themselves, indicating they are likely after-thoughts . In the
other 28% verbosity level modiﬁcations, developers change
between non-error (also non-fatal) levels, such as info and
debug , which are supposedly to record non-error events.
Finding 7: Majority (72%) of the verbosity-level modiﬁ-
cations reﬂect the changes in developers’ judgement about
the criticality of an error event (Table VI).
Implications: Tools that systematically exposing error
conditions would help test the logging behaviors. For
example, fault injection tools [12] can be used to inject
faults to trigger an error and consequently its error loggin g.
Similarly, software model checking [2] can be extended to
explore the execution paths reaching the logging code.
A. Reconsidering Error Criticality
Table VII breaks down the verbosity-level modiﬁcations
for error event logging. More than half (56%) of the
cases are changing levels between non-fatal and fatal. This
class is different from others in that they are introduced to
change the system’s execution behavior as well as logging
behavior. Speciﬁcally, with the modiﬁcations, developersTable VII
RECONSIDERATIONS OF ERROR CRITICALITY AND
VERBOSITY -LEVEL MODIFICATION .
Softwarenon-fatal fatal to non-error error toothersto fatal non-fatal to error non-error
apache 18 12 12 37 5
openssh 80 169 75 71 3
postgres 236 294 148 67 86
squid 29 127 42 201 0
Total363 602 277 376 94
(21%) (35%) (16%) (22%) (6%)
Figure 3. Error verbosity level modiﬁcations from PostgreSQL
changed their decision either to enforce a system to abort
after logging, or allow it to continue to run.
The modiﬁcation from a non-fatal to a fatal level is to
prevent a non-survivable error from propagating, which can
lead to serious system malfunctions or security issues. On
the other hand, the modiﬁcation from a fatal to a non-fatal
level is to avoid an unnecessary system termination on a
survivable error for better system availability.
For example, in Figure 3 (A), PostgreSQL developers
originally record an error event (i.e., an access to an unini -
tialized pointer) at ERROR level, which could potentially
introduce security holes if not aborted right away. Later th ey
provide this patch only to promote the level to a PANIC (fatal
in this software) that will abort the entire database back-
end. As an opposite example, in Figure 3 (B), PostgreSQL
developers prevent non-critical cases from taking down the
entire database by demoting the original PANIC toERROR .
In Table VII, some others (38%) are changing verbosity
levels between an error level and non-error levels. In those
cases, developers may reconsider their original judgement s
about whether the event to record is an error or not, because
recording a real error with non-default verbosity level suc h
asdebug would cause missing important error messages
for failure diagnosis, and recording a non-error event with
error level might either confuse the users and developers
or cause unnecessary production run overhead. For example,
Figure 3 (C) shows that PostgreSQL developers originally
6107missed to report a conﬁguration error by logging it with
info which is not a default verbosity level for production
run in PostgreSQL. After suffering from diagnosing it with-
out logs, they committed a patch only to change it to error .
B. Reconsidering Logging Trade-offs
As shown in Table VI, 28% of the verbosity-level mod-
iﬁcations come from non-error event logging. In general,
non-error events are logged with one of multiple verbose
levels such as debug1, debug2 ,..., or sometimes even with
a default levels, e.g. info in Squid. Table VIII decomposes
the verbosity modiﬁcations for non-error events.
Table VIII
RECONSIDERATION OF LOGGING TRADE -OFF AND
VERBOSITY -LEVEL MODIFICATION
Softwarebetween verbose default between
verbose to default to verbose default
apache 23 (67%) 3 (9%) 8 (24%) 0 (0%)
openssh 116 (76%) 11 (7%) 25 (16%) 0 (0%)
postgres 132 (42%) 108 (34%) 59 (19%) 18 (5%)
squid 115 (66%) 38 (22%) 21 (12%) 0 (0%)
Total 386 (57%) 160 (24%) 113 (17%) 18 (3%)
Finding 8: For non-error event logging, developers re-
consider the trade-off between multiple verbosity levels.
It might indicate that developers are often confused when
estimating the cost (e.g., excessive messages, logging ove r-
head) and beneﬁt afforded by each verbosity level.
Implications: The scalar design of current verbosity level
may not be a good way to help developers with such
logging decision. Adaptive logging in runtime, similar to
adaptive sampling [11], can help balancing the trade-off by
dynamically backing-off the logging rate.
In Table VIII, more than half (57%) of the non-error ver-
bosity level modiﬁcations are changing between two verbose
levels. In all the studied software, verbose levels are not
enabled by default, meaning that they are mostly used during
in-house testing. Therefore, the logging overhead may be
less of concern when developers make such adjustments.
Instead, developers probably are more concerned about
balancing the amount of logs: too excessive logging would
rather make noises for failure diagnosis, but insufﬁcient
logging would miss important runtime information.
One of the possible causes for such many adjustments
within verbose levels might be because no clear division
among multiple verbose levels is given in terms of purpose
of use, beneﬁt, and cost, resulting in confusing developers
when deciding among the verbose levels. For example, in
Squid, there are 7 debug levels out of total 10 verbosity
levels, but no guidance for which cases they should be used.
Indeed, in their header ﬁle, developers wrote a comment
saying “level 2-8 are still being discussed amongst the de-
velopers” . We surmise that developers would decide whichTable IX
MODIFICATIONS TO IMPROVE VARIABLE LOGGING (*:E.G.
FROM INTEGER FORMAT TO FLOAT FORMAT )
software. totaladd replace delete change
var. var. var. format(*)
apache 228 81 68 15 64
openssh 599 348 106 24 121
postgres 1224 839 184 102 99
squid 429 278 45 26 80
Total 24801546 403 167 364
(62%) (16%) (7%) (15%)
level to use arbitrarily at the ﬁrst place and often revisit t he
decision later.
Figure 4. Example from PostgreSQL of a verbosity level demotion
from default level (non-erroneous) to verbose level, toget her with
developers’ commit log.
For non-error logging with default level (e.g., bookkeep-
ing withinfo ), developers may need to carefully consider
more factors since it would directly affect production run.
For example, Figure 4 shows that PostgreSQL users com-
plained about the excessive log messages, and thus develop-
ers demote the previous default level ( LOG) to verbose level
(DEBUG ). Interestingly the developers originally assigned a
default level because the event was in some new code that
potentially is buggy, but it resulted in excessive logging a t
a user site. In addition, of course developers may need to
consider logging overhead in production run.
Overall, setting the verbosity level by considering all
those factors may not be easy at the ﬁrst place, or need
further adjustment as software and environment changes.
Unfortunately, the current scalar design of verbosity leve l
does not provide enough information for developers. To
help developers, systematic and dynamic logging tools or
assists, such as adaptive logging [11], are needed. Instead
of using a statically assigned verbosity, adaptive logging
exponentially decreases logging rate when a certain loggin g
statement is executed many times, only recording its 2n
dynamic occurrences. Such strategy will reduce both the
amount of logs and performance overhead, while preserving
the ﬁrst several occurrences of each log message.
VI. M ODIFICATIONS TO VARIABLE LOGGING
Table IX shows how developers improved variable log-
ging. Majority of them are adding new variables to original
logging code, which could provide more dynamic informa-
tion for failure diagnosis. For example, in Figure 5, a user o f
7108Bug Report from user: 
User: Error when setting client encoding to UTF-8, with error message : 
           failed to commit client_encoding /gid1
Dev: Cannot reproduce the bug… Asking for more details… 
… … … … 
Dev: “Fixed the bug. Motivated by this report, should always 
        include the parameter value we failed to assign . “ 
Patch:  
if (!(*conf->assign_hook) (newval, true, PGC_S_OVERRIDE)) 
-/gid1  elog(ERROR, "failed to commit %s”,  conf->gen.name); 
+ elog(ERROR, "failed to commit %s as %d”, conf->gen.name, newval); 
Figure 5. Example of adding variable values to log message.
Patch: 
ap_log_cerror(APLOG_INFO, 0, c, 
    “Connection to child %ld closed with %s shutdown” 
-   “(client %s)”, c->id, type,  c->remote_uri ); 
+   c->id, type);  
Commit log: 
“It is VERY IMPORTANT that you not log any raw data from the  
network , such as the request-URI or request header fields. Doing  
so makes the server vulnerable to a denial-of-service atta ck . “ Deleted 
Figure 6. Logging a wrong variable causing Apache vulnerable to
denial of service attack.
PostgreSQL reported a production-run failure with an error
message printed by the software. Unfortunately, developer s
could not diagnose the failure due to the lack of runtime
information. Only after a couple of rounds of back-and-fort h
discussion with users they resolved this. From the lessons,
later they committed a patch only to a new variable causally-
related to the logging point.
Finding 9: One fourth (27%) of the log modiﬁcations are
to variable logging. Majority (62%) of them are adding
new variables that are causally related to the log message.
It indicates that developers often need additional run-tim e
information to understand how a failure occurred.
Implication: Logging tools that automatically infer which
variables to log (e.g., LogEnhancer [36]) can help infor-
mative logging. Given failing and passing test cases, Delta
debugging [37] can be used to log those variable values
that are speciﬁc to a failing run.
Interestingly, once variables are introduced into logging
code, they are seldom (7%) deleted, as shown in Table IX.
Probably it is because recording unnecessary variables oft en
would not introduce serious concerns besides one or two
useless variable values in the log.
However, there can also be certain variables that should
not be logged, considering security and privacy concerns,
and developers should be careful to avoid them. Figure 6
shows that Apache developers deleted a variable including a
client’s URI from the logging code, since recording it could
“make the server vulnerable to denial-of-service attack”.
To further understand why variables are deleted or re-
placed in the logging code, we manually study 154 such
modiﬁcations that are randomly sampled. As a result, Ta-
ble X shows that (i) as the most dominant case, the original
logging code records wrong variables at the ﬁrst place, eith erTable X
VARIABLE REPLACEMENT AND DELETION . THE MARGIN OF
ERRORS ARE SHOWN AT 95% CONFIDENCE LEVEL .
wrong incon- read- redun-othervar. sistency ability dancy
46% ( ±6%)11 (% ±4%)23 (% ±5%)2 (%±2%)18 (% ±5%)
permanently_set_uid (struct passwd *pw) {       
  if (temporarily_use_uid_effective) 
-     fatal("restore_uid : temporarily_use_uid effective "); 
+    fatal("permanently_set_uid : temporarily_use_uid effective ");      Function name 
Mismatch! 
Figure 7. Example of inconsistency between log messages and the
code in OpenSSH. This patch is just to ﬁx this inconsistency.
only by mistakes or by not being aware of security or privacy
concerns; (ii) other non-logging code was evolved but the
variable logging was not updated together, becoming incon-
sistent; (iii) an error number such as errono was printed
without interpretation, requiring replacement to readabl e
description; (iv) a log message includes redundant variabl es,
preferred to be deleted. The remaining cases, where we
cannot understand the modiﬁcations from their source code,
commit logs, or comments, belong to the “other” category.
VII. M ODIFICATIONS TO STATIC CONTENT
45% of the log modiﬁcations are modifying the static
content (text) in log messages. Since it is challenging to
automatically analyze the text written in natural language ,
we randomly sampled 200 modiﬁcations and studied them,
which are shown in Table XI.
Table XI
IMPROVING STATIC CONTENT OF LOG MESSAGES . THE MARGIN
OF ERRORS ARE SHOWN AT 95% CONFIDENCE LEVEL .
incon- clariﬁ- spell/ incorrectotherssistency cation grammar content
39%( ±6.6%)36%( ±6.5%)18%( ±5.2%)5%(±2.9%)2%(±1.9%)
In some cases (39%), developers modiﬁed the out-of-
date log messages that are inconsistent with the actual
execution information, which could mislead and confuse
the developers or users (please note that those consistent
updates of both log and code in the same patch are excluded
from our analysis by our analysis tool). Majority (76%) of
them are related to function name changes. For example,
in Figure 7, OpenSSH developers changed a function name
from “restore uid” to “permanently setuid” but forgot to
update the logging code to record this name. Later, they were
confused with the out-of-date log message while trying to
resolve a failure. Finally, they committed this patch just t o
ﬁx the inconsistent logging code.
Such inconsistency can be partly avoided by using pro-
gramming language support. For example, C programming
language provides a macro “ FUNCTION ” as part of the
ANSI-C99 standard, which holds the function name within
which the code is currently executing. As a good logging
practice, as shown in Figure 8, Squid started to use this
in its logging code to automatically recognize a function
8109/* HERE is a macro that you can use like this:                  
 * debugs(1, HERE << "some message”);   */                                                                                                    
#define HERE __FILE__<<"("<<__LINE__<<") "<<__FUNCTION__<<": ” 
 Patch:  
 if (fd < 0) {        
-/gid1   debugs(3, " BlockingFile::open : got failure (" << errno <<  ")");            
+  debugs(3, HERE << ": got failure (" << errno << ")"); Fixing inconsistency  
using ‘HERE’ 
Figure 8. The use of the programming language support to hold
location information for log messages in Squid.
name and log it. This eliminates the need for developers to
manually record or update a location information, avoiding
the inconsistent update problem at the ﬁrst place.
To detect other inconsistent updates (e.g., an event to log
and its description), it would be beneﬁcial to use natural
language processing together with static source code analy -
sis, similar to iComment [29] which uses natural language
processing to automatically analyze comment and source
code in order to detect inconsistency.
Finding 10: More than one third (39%) of modiﬁcations
to static content are ﬁxing inconsistency between logs and
actual execution information intended to record. Software
can leverage programming language support to eliminate
some of the inconsistency, as Squid does.
Implication: Tools combining natural language processing
and static code analysis can be designed to detect such
inconsistency.
Bug Report from user: Confusing message in log file 
“I changed the postgresql.conf file, and see the following  messages: 
so I expect both newly enabled “ archive_command” and “shared_buffers”    
not to take effect…. But in fact, “ archive_command ” does take effect.” 
Patch:  
ereport (ERROR, 
-/gid1    “parameter \”%s\” cannot be changed after server start;”, gconf->na me 
-/gid1    “configuration file change ignored ” 
+    “attempted change of parameter \”%s\” ignored ,”, gconf->name 
+   “This parameter cannot be changed after server start” parameter “shared_buffers” cannot be changed after server start; /gid1
conﬁguration ﬁle change ignored /gid1
Figure 9. Example of a log message clariﬁcation from PostgreSQL.
In some other cases (36%), developers modiﬁed static
content of log messages to clarify the event description in
it. As an example, Figure 9 shows that a log message in
PostgreSQL was unclear and thus it misled a user to believe
that all his conﬁguration changes would lose effect, which
was not true. At the end, the modiﬁcation was made only
to clarify the content of the log message.
VIII. L OCATION CHANGE
As we discussed in Finding 6, developers seldom delete
or move logging code once it is written. To understand
under which cases developers delete/move logging code,
we randomly sampled 57 such cases from the 150 location
modiﬁcations and manually examined them. The Table XIITable XII
REASONS FOR MOVING OR DELETING LOG MESSAGES
software misleading reduceothersfailure log msg. noises
26% (±9%) 21% (±8%) 40% (±10%) 12% (±7%)
 sigusr2_handle(int sig)  { 
-  debug(1, "sigusr2_handle: SIGUSR2 received.\n"); 
+  /* no debug() here; bad things happen if 
       * the signal is delivered during debug()*/  I/O in signal  
handler can corrupt  
the system state. 
Figure 10. Deleting Logging from a signal handler in Squid.
summarizes the results with the sampling errors at the 95%
conﬁdence level.
Interestingly, 26% of the location changes were required
because the original logging code was misplaced and re-
sulted in software failures. For example, logging in sig-
nal/interrupt handlers is dangerous since the non-reentra nt
I/O operations during logging might corrupt system states
and open up vulnerabilities. Figure 3 shows a patch to
delete such a problematic logging code from a signal han-
dler in Squid. In addition, logging variables before their
initialization would result in system crash or misbehavior ;
logging non-error events with fatal verbosity level will
unnecessarily terminate the software execution. To identi fy
these problems above, in-house testing tools and static
analysis tools [8] can be extended to explore logging place.
For example, we can use static analysis to detect logging
statements within interrupt handlers and the use of unini-
tialized variables.
In some cases, developers delete some misleading log
messages (e.g, an error message printed under a non-error
situation). From several commit logs, we ﬁnd that some
developers tend to actively log certain events simply with t he
error verbosity level for the purpose of in-house testing,
then forgot to completely delete them before the production
release. In other cases, log messages are moved out of
a loop body or combined into one that can summarize
them, probably in order to avoid overhead and noises from
excessive logging. Finally, the “others” category include s the
cases that we cannot clearly understand.
IX. V ERBOSITY LEVEL CHECKER
To show the feasibility of automatic logging assistance
from our ﬁndings, we designed a simple verbosity-level
checker which helps identify certain type of problematic
verbosity-level assignment. This is motivated by the signi f-
icant number of verbosity-level adjustments (Finding 7).
Our checker is based on the observation that if the logging
code within the similar code snippets have inconsistent ver -
bosity levels, at least one of them is likely to be incorrect [ 9],
[15], [10], [33]. First, the tool identiﬁes all the code clon es
in the source code (we used CP-Miner [15] to detect code
clones). Then, it further checks each pair of clones to see
whether they contain logging code and their verbosity level s
are consistent.
9110Table XIII
VERBOSITY -LEVEL INCONSISTENCY DETECTED .
apache openssh postgres squid
Inconsistency 12 4 89 33
As a result, our checker detected 138 inconsistent pairs
of logging code, as shown in Table XIII. We reported 45
cases to the developers. 24 of them are conﬁrmed and
ﬁxed [27], 10 are conﬁrmed as false positives where the
cloned code snippets are in different contexts so they shoul d
have different verbosity levels [27], and the remaining one s
are not being responded.
This result shows that based on our ﬁnding, even a simple
checker can effectively help for better logging. It conﬁrms
that the ﬁrst important step towards systematic and automat ic
supports for better software logging is to understand the
current manual efforts for logging, which is exactly the goa l
of this work.
X. R ELATED WORK
Logging effectiveness study: Two pieces of recent
work [35], [13] studied the effectiveness of logging in fail ure
diagnosis as one part of their work. LogErr [35] character-
ized the problem of lacking of log messages for diagnosis,
and suggest to check generic error conditions and log all
detected errors (i.e., where to log). They further proposed a
tool to automatically insert error messages for those gener ic
error conditions. In contrast, we focus on many other aspect s
of logging decisions, such as verbosity levels, static cont ent,
variable values, etc. In particular, LogErr studied failur es, but
not programmer’s modiﬁcations on existing log messages.
Also it only studied the default-mode log messages, where
as we study allthe messages in this work. Therefore, all of
our ﬁndings except ﬁnding 2 are unique to this work.
Jiang et al. [13] mainly studied the correlations among
root causes of storage system failures, impact and diagnosi s
time. One of their ﬁnding conﬁrms the beneﬁt of logging,
but does not provide detailed efﬁcacy of logging practices.
Logging improvement: There have been some work
to help systematically improve logging. Some can help
developers in inserting new log messages, but only for
error conditions [35], [4], [24], [3]. SMELL [4] detects
some exception handling code which is not logged or
inappropriately logged. Some other work [24], [3] propose
to generate error checking code (e.g., assertions) from a
program speciﬁcations, which provide natural logging plac es
for error messages. LogEnhancer [36] can suggest new
variable values to be recorded in each existing log message
by analyzing the source code.
Our work is complementary to the tools for log improve-
ment as the characteristics of logging practice we reveal ca n
be used to either support the usefulness of these tools and
inspire future tools.Log analysis work: Many systems analyze the production-
run logs for post-mortem diagnosis [32], [34], [18], [17].
Some [32], [18] of them learn statistical signatures to dete ct
and diagnose anomalies. SherLog [34] infers the partial
execution paths by mapping log messages to source code.
Mariani and Pastore [17] analyze logs to learn the correct
dependency between log messages from the normal execu-
tions, and use the information to identify anomalies in fail ed
executions. The effectiveness of these work depends on the
quality of log messages, thus can potentially beneﬁt from
our study.
XI. C ONCLUSIONS
This paper presents the ﬁrst (to the best of our knowledge)
attempt to study the practice of software log messages using
four pieces of large open-source software. We ﬁrst quantify
the pervasiveness and the beneﬁt of software logging. By
further studying developers’ own modiﬁcations on their log
messages, we found they often cannot get the log messages
right after the ﬁrst attempts. In particular, developers sp end
signiﬁcant efforts in modifying the verbosity level, stati c
text, and variable values of log messages in various ways,
but rarely change the message locations. By identifying the se
common log-modiﬁcation efforts that are done manually,
we reveal many opportunities for tools, compiler and pro-
gramming language support to improve the current logging
practices. Such beneﬁt of our ﬁndings is conﬁrmed by a
simple checker we built, which is motivated by identifying
developers’ large amount of manual efforts in modifying the
verbosity level, that can effectively detect 138 new pieces of
problematic logging code .
ACKNOWLEDGEMENT
We thank the anonymous reviewers for their insight-
ful feedback. This research is supported by NSF CNS-
0720743 grant, NSF CCF-0325603 grant, NSF CNS-
0615372 grant, NSF CNS-0347854 (career award), NSF
CSR Small 1017784 grant and NetApp Gift grant.
REFERENCES
[1] ArcSight Log analysis. http://www.arcsight.com/Logg er.
[2] T. Ball and S. K. Rajamani. The SLAM project: debugging
system software via static analysis. In Proceedings of the
29th ACM SIGPLAN-SIGACT symposium on Principles of
programming languages (POPL) , pages 1–3, 2002.
[3] L. Baresi and M. Young. Toward translating design con-
straints to run-time assertions. Electron. Notes Theor. Comput.
Sci., 116:73–84, January 2005.
[4] M. Bruntink, A. v. Deursen, and T. Tourw´ e. Discovering
faults in idiom-based exception handling. In Proceedings
of the 28th international conference on Software engineeri ng
(ICSE) , pages 242–251, 2006.
[5] Cisco system log management. http://www.cisco.com/en /US/
docs/voice ipcomm/cucm/service/3 32/ccmsrvs/sssyslog.
html.
[6] Dell. Streamlined Troubleshooting with the Dell system E-
Support tool. Dell Power Solutions , 2008.
10111[7] EMC seen collecting and managing log as key driver for 94
percent of customers. http://www.rsa.com/press release.aspx?
id=7596.
[8] D. Engler, B. Chelf, and A. Chou. Checking system rules
using system-speciﬁc, programmer-written compiler exten -
sions. In Proceedings of the 4th conference on Symposium
on Operating System Design & Implementation (OSDI) , pages
1–16, 2000.
[9] D. Engler, D. Y . Chen, S. Hallem, A. Chou, and B. Chelf.
Bugs as deviant behavior: A general approach to inferring
errors in systems code. In Proceedings of the eighteenth ACM
symposium on Operating systems principles (SOSP) , pages
57–72, 2001.
[10] M. Gabel, J. Yang, Y . Yu, M. Goldszmidt, and Z. Su. Scalab le
and systematic detection of buggy inconsistencies in sourc e
code. In Proceedings of the ACM international conference
on Object oriented programming systems languages and
applications (OOPSLA) , pages 175–190, 2010.
[11] M. Hauswirth and T. M. Chilimbi. Low-overhead memory
leak detection using adaptive statistical proﬁling. In Pro-
ceedings of the 11th international conference on Architect ural
support for programming languages and operating systems ,
ASPLOS-XI, pages 156–164, 2004.
[12] M.-C. Hsueh, T. K. Tsai, and R. K. Iyer. Fault injection
techniques and tools. Computer , 30:75–82, 1997.
[13] W. Jiang, C. Hu, S. Pasupathy, A. Kanevsky, Z. Li, and
Y . Zhou. Understanding customer problem troubleshooting
from storage system logs. In Proccedings of the 7th confer-
ence on File and storage technologies (FAST) , pages 43–56,
2009.
[14] B. W. Kernighan and R. Pike. The Practice of Programming .
Addison-Wesley, 1999.
[15] Z. Li, S. Lu, S. Myagmar, and Y . Zhou. CP-Miner: A Tool for
Finding Copy-paste and Related Bugs in Operating System
Code. In Proceedings of the 6th conference on Symposium on
Opearting Systems Design & Implementation (OSDI) , pages
176–192, 2004.
[16] Apache Logging Services - Log4j. http://logging.apac he.org/
log4j.
[17] L. Mariani and F. Pastore. Automated identiﬁcation of f ailure
causes in system logs. In Proceedings of the 2008 19th
International Symposium on Software Reliability Engineer ing
(ISSRE) , pages 117–126, 2008.
[18] L. Mariani, F. Pastore, and M. Pezz` e. A toolset for auto mated
failure analysis. In Proceedings of the 31st International
Conference on Software Engineering (ICSE) , pages 563–566,
2009.
[19] J. C. Munson and S. G. Elbaum. Code churn: A measure for
estimating the impact of code change. In Proceedings of the
International Conference on Software Maintenance (ICSM) ,
pages 24–31, 1998.
[20] N. Nagappan and T. Ball. Use of relative code churn measu res
to predict system defect density. In Proceedings of the 27th
international conference on Software engineering (ICSE) ,
pages 284–292, 2005.[21] NetApp. Proactive health management with auto-suppor t.
NetApp White Paper , 2007.
[22] Netcraft report: Apache httpd is the most popular
webserver. http://news.netcraft.com/archives/categor y/web-
server-survey/.
[23] Lessons from the success of ssh. http://www.cs.virgin ia.edu/
∼drl7x/sshVsTelnetWeb3.pdf.
[24] M. Pezz` e and J. Wuttke. Automatic generation of runtim e
failure detectors from property templates. In B. H. Cheng,
R. Lemos, H. Giese, P. Inverardi, and J. Magee, editors,
Software Engineering for Self-Adaptive Systems , pages 223–
240. Springer-Verlag, Berlin, Heidelberg, 2009.
[25] SLOCCount: Source Lines Of Code Count. http://www.
dwheeler.com/sloccount/.
[26] Splunk Log management. http://www.splunk.com/view/ log-
management/SP-CAAAC6F.
[27] Squid bug report 3319. http://bugs.squid-cache.org/ show
bug.cgi?id=3319.
[28] RFC3164 – the BSD Syslog protocol. http://tools.ietf. org/
html/rfc3164.
[29] L. Tan, D. Yuan, G. Krishna, and Y . Zhou. /* iComment:
Bugs or bad comments? */. In Proceedings of twenty-ﬁrst
ACM SIGOPS symposium on Operating systems principles
(SOSP) , pages 145–158, 2007.
[30] Top 10 Enterprise database systems to consider.
http://www.serverwatch.com/trends/article.php/38834 41/Top-
10-Enterprise-Database-Systems-to-Consider.htm.
[31] D. Wessels. Squid: The Deﬁnitive Guide . O’Reilly, 2004.
[32] W. Xu, L. Huang, A. Fox, D. Patterson, and M. I. Jordan.
Detecting large-scale system problems by mining console
logs. In Proceedings of the ACM SIGOPS 22nd Symposium on
Operating Systems Principles (SOSP) , pages 117–132, 2009.
[33] A. T. T. Ying, G. C. Murphy, R. Ng, and M. C. Chu-Carroll.
Predicting source code changes by mining change history.
IEEE Trans. Softw. Eng. , 30:574–586, September 2004.
[34] D. Yuan, H. Mai, W. Xiong, L. Tan, Y . Zhou, and S. Pasupa-
thy. SherLog: Error diagnosis by connecting clues from run-
time logs. In Proceedings of the ﬁfteenth edition of ASPLOS
on Architectural support for programming languages and
operating systems (ASPLOS) , pages 143–154, 2010.
[35] D. Yuan, S. Park, P. Huang, Y . Liu, M. Lee, Y . Zhou,
and S. Savage. Did you log the error? Characterizing and
improving software error reporting. Technical Report.
[36] D. Yuan, J. Zheng, S. Park, Y . Zhou, and S. Savage. Improv -
ing software diagnosability via log enhancement. In Proceed-
ings of the sixteenth international conference on Architec tural
support for programming languages and operating systems
(ASPLOS) , pages 3–14, 2011.
[37] A. Zeller. Isolating cause-effect chains from compute r pro-
grams. In Proceedings of the 10th ACM SIGSOFT symposium
on Foundations of software engineering , SIGSOFT’02/FSE-
10, pages 1–10, 2002.
11112