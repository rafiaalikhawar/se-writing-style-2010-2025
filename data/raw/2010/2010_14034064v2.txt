arXiv:1403.4064v2  [cs.PL]  17 Sep 2014Feedback Generation forPerformance Problemsin
Introductory Programming Assignments∗
Sumit Gulwani
MicrosoftResearch,USA
sumitg@microsoft.comIvanRadi ˇcek
TUWien, Austria
radicek@forsyte.atFlorian Zuleger
TUWien, Austria
zuleger@forsyte.at
ABSTRACT
Providing feedback on programming assignments manually
is a tedious, error prone, and time-consuming task. In this
paper, we motivate and address the problem of generating
feedback on performance aspects in introductory program-
ming assignments. We studied a large number of function-
ally correct student solutions to introductory programmin g
assignments and observed: (1) There are diﬀerent algorith-
mic strategies, with varying levels of eﬃciency, for solvin g
a given problem. These diﬀerent strategies merit diﬀerent
feedback. (2) The same algorithmic strategy can be imple-
mented in countless diﬀerent ways, which are not relevant
for reporting feedback on the student program.
We propose a light-weight programming language exten-
sion that allows a teacher to deﬁne an algorithmic strategy
by specifying certain key values that should occur during
the execution of an implementation. We describe a dynamic
analysis based approach totest whether a student’sprogram
matches a teacher’s speciﬁcation. Our experimental result s
illustrate the eﬀectiveness of both our speciﬁcation langu age
and our dynamic analysis. On one of our benchmarks con-
sisting of 2316 functionally correct implementations to3 p ro-
gramming problems, we identiﬁed 16 strategies that we were
able to describe using our speciﬁcation language (in 95 min-
utes after inspecting 66, i.e., around 3%, implementations ).
Our dynamic analysis correctly matched each implementa-
tion with its corresponding speciﬁcation, thereby automat i-
cally producing the intended feedback.
Categories andSubject Descriptors
D.2.5 [SOFTWARE ENGINEERING ]: Testing and De-
bugging; I.2.2 [ ARTIFICIAL INTELLIGENCE ]: Auto-
matic Programming— Automatic analysis of algorithms
∗The second and third author were supportedby the Vienna
Science and Technology Fund (WWTF) grant ICT12-059.
Permission to make digital or hard copies of all or part of thi s work for personal or
classroom use is granted without fee provided that copies ar e not made or distributed
forproﬁt orcommercialadvantageandthatcopiesbearthisn oticeandthefull citation
on the ﬁrst page. Copyrights for components of this work owne d by others than the
author(s)mustbehonored. Abstractingwithcreditispermi tted. Tocopyotherwise,or
republish,topostonserversortoredistributetolists,re quirespriorspeciﬁcpermission
and/or a fee. Requestpermissionsfrom Permissions@acm.or g.
SIGSOFT/FSE'14, November 16 - 22 2014,HongKong, China
Copyright isheld by theowner/author(s). Publication righ ts licensedtoACM.
ACM978-1-4503-3056-5/14/11 ...$15.00.
http://dx.doi.org/10.1145/2635868.2635912.General Terms
Algorithms, Languages, Performance.
Keywords
Education, MOOCs, performance analysis, trace speciﬁca-
tion, dynamic analysis.
1. INTRODUCTION
Providing feedback on programming assignments is a very
tedious, error-prone, and time-consuming task for a human
teacher, even in a standard classroom setting. With the
rise of Massive Open Online Courses (MOOCs) [15], which
have a much larger number of students, this challenge is
even more pressing. Hence, there is a need to introduce au-
tomation around this task. Immediate feedback generation
through automation can also enable new pedagogical bene-
ﬁts such as allowing resubmission opportunity to students
who submit imperfect solutions and providing immediate
diagnosis on class performance to a teacher who can then
adapt her instruction accordingly [9].
Recent research around automation of feedback genera-
tion for programming problems has focused on guiding stu-
dents to functionally correct programs either by providing
counterexamples [25] (generated using test input generati on
tools) or generating repairs [21]. However, non-functiona l
aspects ofaprogram, especially performance, are also impo r-
tant. We studied several programming sessions of students
who submitted solutions to introductory C#programming
problems on the Pex4Fun [4] platform. In such a program-
ming session, a student submits a solution to a speciﬁed
programming problem and receives a counterexample based
feedback upon submitting a functionally incorrect attempt
(generated using Pex [24]). The student may then inspect
the counterexample and submit a revised attempt. This
process is repeated until the student submits a functionall y
correct attempt or gives up. We studied 24 diﬀerent prob-
lems, and observed that of the 3993 diﬀerent programming
sessions, 3048 led to functionally correct solutions. Howe ver,
unfortunately, on average around 60% of these functionally
correct solutions had (diﬀerent kinds of) performance prob -
lems. In this paper, we present a methodology for semi-
automatically generating appropriate performance relate d
feedback for such functionally correct solutions.
From our study, we made two observations that form the
basis of our semi-automatic feedback generation methodol-
ogy. (i) There are diﬀerent algorithmic strategies with vary-
ing levels of eﬃciency, for solving a given problem. Algo-rithmic strategies capture the global high-level insight o f
a solution to a programming problem, while also deﬁning
key performance characteristics of the solution. Diﬀerent
strategies merit diﬀerent feedback. (ii) The same algorith -
mic strategy can be implemented in countless diﬀerent ways.
These diﬀerences originate from local low-level implement a-
tion choices and are not relevant for reporting feedback on
the student program.
In order to provide meaningful feedback to a student it
is important to identify what algorithmic strategy was em-
ployed by the student program. A proﬁling based approach
that measures running time of a program or use of static
bound analysis techniques [10, 11] is not suﬃcient for our
purpose, because diﬀerent algorithmic strategies that nec es-
sitate diﬀerent feedback may have the same computational
complexity. Also, asimple patternmatchingbasedapproach
is not suﬃcient because the same algorithmic strategy can
have syntactically diﬀerent implementations.
Our key insight is that the algorithmic strategy employed
by a program can be identiﬁed by observing the values com-
puted during the execution of the program. We allow the
teacher tospecify an algorithmic strategy bysimplyannota t-
ing (at the source code level) certain key values computed
by a sample program (that implements the corresponding
algorithm strategy) using a new language construct, called
observe. Fortunately, the number of diﬀerent algorithmic
strategies for introductory programming problems is often
small (at most 7 per problem in our experiments). These
can be easily enumerated by the teacher in an iterative pro-
cess by examining any student program that does not match
any existing algorithmic strategy; we refer to each such ste p
in this iterative process as an inspection step.
Wepropose anoveldynamicanalysis thatdecides whether
the student program (also referred to as an implementation )
matchesanalgorithm strategy speciﬁedbytheteacherinthe
form of an annotated program (also referred toas a speciﬁca-
tion). Our dynamic analysis executes a student’s implemen-
tation and the teacher’s speciﬁcation to check whether the
key values computed by the speciﬁcation also occur in the
corresponding traces generated from the implementation.
We have implemented the proposed framework in C#and
evaluated our approach on 3pre-existingprogramming prob-
lems on Pex4Fun (attempted by several hundreds of stu-
dents) and on 21 new problems that we hosted on Pex4Fun
as part of a programming course (attempted by 47 students
in the course). Experimental results show that: (i) The
manual teacher eﬀort required to specify various algorith-
mic strategies is a small fraction of the overall task that
our system automates. In particular, on our MOOC style
benchmark of 2316 functionally correct implementations to
3 pre-existingprogramming problems, we speciﬁed 16 strate -
gies in 95 minutes after inspecting 66 implementations. On
our standard classroom style benchmark of 732 functionally
correct implementations to 21 programming problems, we
speciﬁed 66 strategies in 266 minutes after inspecting 149
implementations. (ii) Our methodology for specifying and
matching algorithmic strategies is both expressive and pre -
cise. In particular, we were able to specify all 82 strategie s
using our speciﬁcation language and our dynamic analysis
correctly matched each implementation with the intended
strategy.
This paper makes the following contributions:•We observe that there are diﬀerent algorithmic strate-
gies used in functionally correct attempts to introductory
programming assignments; these strategies merit diﬀer-
ent performance related feedback.
•We describe a new language construct, called observe, for
specifying an algorithmic strategy ( §3).
•We describe a dynamic analysis based approach to test
whetherastudent’simplementationmatchestheteacher’s
speciﬁcation (§4).
•Ourexperimental results illustrate theeﬀectiveness ofou r
speciﬁcation language and dynamic analysis ( §6).
2. OVERVIEW
In this section we motivate our problem deﬁnition and
various aspects of our solution by means of various examples .
2.1 Motivation
Fig. 1shows our runningexamples. Programs (a)-(i)( IM)
showsome sample implementations for the anagram problem
(which involves testing whether the two input strings can be
permuted to become equal) on the Pex4Fun platform. All
9 programs are examples of ineﬃcient implementations, be-
cause of their quadratic asymptotic complexity . An eﬃcient
solution, for example, is to ﬁrst collect (e.g. in an array) t he
number of occurrences of each character in both strings and
then compare them, leading to linear asymptotic complexity .
Algorithmic strategies . In implementations IMwe
identify three diﬀerent algorithmic strategies . Implementa-
tionsC1-C3iterate over one of the input strings and for
each character in that string count the occurrences of that
character in both strings ( counting strategy ). Implementa-
tionsS1-S3sort both input strings and check if they are
equal (sorting strategy ). Implementations R1-R3iterate
over one of the input strings and remove corresponding char-
acters from the other string ( removing strategy ).
Implementation details . An algorithmic strategy can
have several implementations. In case of counting strategy :
Implementation C1callsmanuallyimplementedmethod countChar
to count the number of characters in a string (lines 5 and 6),
while implementation C2uses a special C#construct (lines
6 and 7) and implementation C3uses the library function
Splitfor that task (lines 4 and 5). In case of the sorting
strategy: Implementation S1employs binary insertion sort,
while implementation S2employs bubble sort and imple-
mentation S3uses a library call (lines 4 and 5). We also
observe diﬀerent ways of removing a character from a string
in implementations R1-R3.
Desired feedback . Eachofthethreeidentiﬁedstrategies
requires separate feedback (independent of the underlying
implementation details), to help a student understand and
ﬁx the performance issues. For the ﬁrst strategy (implemen-
tationsC1-C3), a possible feedback might be: ”Calculate
the number of characters in each string in a preprocessing
phase, instead of each iteration of the main loop” ; for the
second strategy ( S1-S3), it might be: ”Instead of sorting
input strings, compare the number of character occurrences
in each string” ; and for the third strategy ( R1-R3):”Use a
more eﬃcient data-structure to remove characters” .
2.2 Specifying AlgorithmicStrategies
Key values . Our key insight is that diﬀerent implemen-
tations that employ the same algorithmic strategy generate
thesame key values duringtheirexecutiononthesameinput.1bool Puzzle(string s, string t) {
2if (s.Length != t.Length) return false;
3
4foreach (Char ch in s.ToCharArray()){
5if (countChars(s, ch)
6 != countChars(t, ch) ){
7return false;
8}}
9return true;}
10
11int countChars(String s, Char c){
12int number = 0;
13
14foreach (Char ch in s.ToCharArray()){
15if (ch== c){
16 number++;
17}}
18return number;}1bool Puzzle(string s, string t) {
2if (s.Length != t.Length)
3return false;
4else
5return s.All(c =>
6s.Where(c2 => c2 == c).Count() ==
7t.Where(c2 => c2 == c).Count()
8);
9}
(b) Counting/Library ( C2)
1bool Puzzle(string s, string t) {
2if(s.Length != t.Length) return false;
3foreach (var item in s) {
4if(s.Split (item).Length
5 != t.Split (item).Length )
6return false;
7}
8return true; }1int BinarySearch(List<char> xs, char y) {
2int low = 0, high = xs.Count;
3while (low < high) {
4int mid = (high - low) / 2 + low;
5if (y < xs[mid]) high = mid;
6else if (y > xs[mid]) low = mid + 1;
7else return mid;}
8return low;}
9
10char[] Sort(string xs) {
11var res = new List<char>();
12foreach (var x in xs) {
13var pos = BinarySearch(res, x);
14res.Insert(pos, x);}
15return res.ToArray(); }
16
17bool Puzzle(string s, string t) {
18return String.Join("", Sort(s))
19== String.Join("", Sort(t)); }
(a) Counting/Manual ( C1) (c) Counting/Split ( C3) (d) Sorting/Binary Insertion ( S1)
1bool Puzzle(string s, string t) {
2if (s.Length != t.Length) return false;
3char[] sa = s.ToCharArray();
4char[] ta = t.ToCharArray();
5for (int j=0; j < sa.Length; j++) {
6for (int i=0; i<sa.Length - 1;i++) {
7if (sa[i]<sa[i+1]){ char temp=sa[i];
8 sa[i]=sa[i+1]; sa[i+1]=temp ;}
9if (ta[i]<ta[i+1]){ char temp=ta[i];
10 ta[i] = ta[i+1]; ta[i+1] = temp;}
11}}
12for (int k = 0; k < sa.Length; k++) {
13if (sa[k] != ta[k]) return false; }
14return true; }1bool Puzzle(string s, string t) {
2var sa = s.ToCharArray();
3var ta = t.ToCharArray();
4Array.Sort(sa) ;
5Array.Sort(ta);
6return sa.SequenceEqual(ta);}
(f) Sorting/Library ( S3)
1bool Puzzle(string s, string t) {
2if (s.Length != t.Length) return false;
3foreach (char c in t.ToCharArray()) {
4int index = s.IndexOf(c);
5if (index < 0) return false;
6s = s.Remove(index, 1) ; }
7return true; }1bool Puzzle(string s, string t) {
2return IsPermutation(s, t);
3}
4bool IsPermutation(String s, string t) {
5if (s == t) return true;
6if (s.Length != t.Length) return false;
7int index = t.IndexOf(s[0]);
8if (index == -1) return false;
9
10s = s.Substring(1);
11t = t.Remove(index, 1) ;
12
13return IsPermutation(s, t);
14}
(e) Sorting/Bubble ( S2) (g) Removing/Library ( R1) (h) Removing/Recursive ( R2)
1bool Puzzle(string s, string t) {
2char[] sc = s.ToCharArray();
3char[] tc = t.ToCharArray();
4Char c = ’#’;
5if(sc.Length!=tc.Length) return false;
6for(int i=0;i<sc.Length;i++) {
7c = sc[i];
8for(int j=0;j<tc.Length;j++) {
9if(tc[j]==c){
10 tc[j]=’#’ ;
11 break;}
12 if(j==tc.Length-1) {
13 return false; }}}
14return true; }1Puzzle(string s, string t) {
2if (nd1) { string tt = t; t = s; s = tt; }
3for (int i = 0; i < s.Length; ++i) {
4int cnt1 = 0, cnt2 = 0;
5for (int j = 0; j < s.Length; ++j) {
6if (s[j] == s[i]) {
7 if (nd2) observe (s[j]);
8 cnt1++;
9}}
10if (!nd2 ) observeFun (Split());
11observe(nd2? cnt1 : cnt1 + 1);
12for (int j = 0; j < t.Length; ++j) {
13 if (t[j] == s[i]) {
14 if (nd2) observe (t[j]);
15 cnt2++;
16 }}
17if (!nd2 ) observeFun (Split());
18observe(nd2? cnt2 : cnt2 + 1); }}1Puzzle(string s, string t) {
2if (nd1) s = s.ToUpperInvariant();
3char[] ca = s.ToCharArray();
4Array.Sort(ca);
5if (nd2) Array.Reverse(ca);
6observe(ca);
7}
(k) Sorting Speciﬁcation ( SS)
1Puzzle(string s, string t) {
2if (nd1) {string tt = t; t = s; s = tt;}
3for (int i = 0; i < s.Length; ++i) {
4if (s.Substring(i) == t) return;
5int ni = nd2 ? i : s.Length - i - 1;
6int k = nd3 ? t.IndexOf(s[ni])
7 : t.LastIndexOf(s[ni]);
8t = t.Remove(k, 1);
9observe(t, CompareLetterString ); }}
(i) Removing/Manual ( R3) (j) Counting Speciﬁcation ( CS) (l) Removing Speciﬁcation ( RS)
1bool Puzzle(string s, string t) {
2if(s.Length != t.Length) return false;
3Char[] taux = t.ToCharArray();
4for(int i = 0; i < s.Length; i++) {
5Char sc = s[i];
6Boolean exists = false;
7for(int j = 0; j < t.Length; j++) {
8if(sc == taux[j]) {
9 exists = true; taux[j] = ’ ’ ;
10 break; }}
11if(exists == false) return false; }
12return true; }1bool CompareLetterString(string a,string b){
2var la = a.Where(x=>char.IsLetter(x));
3var lb = b.Where(x=>char.IsLetter(x));
4return la.SequenceEqual(lb);
5}1bool Puzzle(string s, string t) {
2if (s.Length !=t.Length) return false;
3int [] cs=new int [256];
4int [] ct=new int [256];
5for(int i=0;i<s.Length;i++)
6cs[(int) s[i]]++ ;
7for(int i=0;i<t.Length;++i)
8ct[(int) t[i]]++ ;
9for (int i=0;i<256;i++)
10if(cs[i] != ct[i]) return false;
11return true;
12}
(m) Removing/Manual 2 ( R4) (n) Custom Data Equality ( CDE) (o) Eﬃcient/Compare ( E1)
1bool Puzzle(string s, string t) {
2if (s.Length != t.Length)
3return false;
4char[] cs = s.ToCharArray();
5char[] ct = t.ToCharArray();
6int[] hash = new int[256];
7for (int i=0; i<255; ++i) {
8hash[i] = 0;
9}
10foreach (char ch in cs) {
11hash[(int)ch]++ ; }
12foreach (char ch in ct) {
13hash[(int)ch]-- ; }
14for (int i=0; i<255; ++i) {
15if (hash[i] < 0)
16 return false; }
17return true; }1void Puzzle(string s, string t) {
2if (nd1){string tt = t; t = s; s = tt;}
3int[] cs = new int[256],ct = new int[256];
4cover(ToCharArray());
5cover(ToCharArray());
6cover(255);
7for (int i = 0; i < s.Length; ++i) {
8cs[(int)s[i]]++;
9observe(cs); }
10for (int i = 0; i < t.Length; ++i) {
11if (nd2) { cs[(int)t[i]]--;
12 observe(cs);
13} else { ct[(int)t[i]]++;
14 observe(ct);
15}
16}
17cover(255); }1bool Puzzle(string s, string t) {
2if (s.Length != t.Length) return false;
3string cp = t;
4for(int i=0; i<s.Length; i++) {
5char k = s[i]; bool found = false;
6for(int j=0; j<cp.Length; j++) {
7if (cp[j] == k) {
8 if (j == 0) {
9 cp = (Char)0+cp.Substring(1) ;}
10 else if(j == cp.Length - 1) {
11 cp = cp.Substring(0, j)+(Char)0 ;}
12 else {
13 cp = cp.Substring(0, j) +
14 (Char) 0 + cp.Substring(j + 1) ;}
15 found = true; break; }}
16if (!found) return false; }
17return true; }
(p) Eﬃcient/Diﬀerence ( E2) (q) Eﬃcient Speciﬁcation ( ES) (r) Removing/Separate computation ( R5)
Figure 1: Running example: Implementations and Speciﬁcati ons of Anagram assignment.For example, (the underlined expressions in) the implemen-
tationsC1andC2both produce the key value sequence
(a,b,a,2,b,a,a,2,a,b,a,1,b,a,a,1,a,b,a,2,b,a,a,2) on the
input strings“aba”and“baa”.
Our framework allows a teacher to describe an algorith-
mic strategy by simply annotating certain expressions in a
sample implementation using a special language statement
observe. Our framework decides whether or not a student
implementation matches a teacher speciﬁcation by compar-
ing their execution traces on common input(s). We say that
an implementation Qmatches a speciﬁcation P, if (1) the
execution trace of Pis a subsequence of the execution trace
ofQ, and (2) for every observed expression in Pthere is
an expression in Qthat has generated the same values. We
call this matching criterion a trace embedding . The notion
of trace embedding establishes a fairly strong connection
between speciﬁcation and implementation: basically, both
programs produce the same values at corresponding loca-
tions in the same order. Our notion of trace embedding is
an adaptation of the notion of a simulation relation [16] to
dynamic analysis.
Non-deterministic choice . Because of minor diﬀer-
ences between implementations of the same strategy, key-
values can diﬀer. For example, implementation C3uses a
library function to obtain the number of characters, while
implementations C1andC2explicitly count them by ex-
plicitly iterating over the string. Moreover, counted val-
ues inC3are incremented by one compared to those in
C1andC2.C3thus yields a diﬀerent, but related, trace
(Split,3,Split,3,Split,2,Split,2,Split,3,Split,3)onin-
put strings ”aba”and ”baa”. To address variations in imple-
mentation details, we includea non-deterministic choice con-
struct in our speciﬁcation language. The non-determinism
is ﬁxed before the execution; thus such a choice is merely a
syntactic sugar tosuccinctlyrepresent multiple similar speci-
ﬁcations (nnon-deterministic variables = 2nspeciﬁcations).
Speciﬁcations .CS,SS, andRSdenote the speciﬁca-
tions for the counting strategy (used in implementations
C1-C3), sorting strategy (used in S1-S3), and removing
strategy (used in R1-R3) respectively. In CS, the teacher
observes the characters that are iterated over (lines 7 and
14), the results of counting the characters (lines 11 and 18) ,
and use of library function Split(lines 10 and 17). Also the
teacher uses non-deterministic Boolean variables: nd1(line
2) to choose the string over which the main loop iterates (as
the input strings are symmetric in the anagram problem);
andnd2to choose between manual and library function im-
plementations (which also decides on observed counted val-
ues on lines 18 and 11). In SSthe teacher observes one
of the input strings after sorting, and non-deterministica lly
allows that implementations convert input string to upper-
case (nd1online2), andsortthestringinreverseorder( nd2
online5). Noticethatitisenoughtoobserveonlyonesorted
input, as in the case that the input strings are anagrams,
the sorted strings are the same. In RSthe teacher observes
the string with removed characters and non-deterministcal ly
chooses which string is iterated ( nd1on line 2), direction of
the iteration ( nd2on line 5) and the direction in which the
remove candidate is searched for ( nd3on line 6).
3. SPECIFICATIONSANDIMPLEMENTA-
TIONSExpression e::=d|v|v1opbinv2|opunv|v1[v2]
Statement s::=v:=e|v1[v2] :=e|v:=f(v1,...,v n)
|s0;s1|whilevdos|skip
|ifvthens0elses1
|observe(v,[E])
|observeFun (f[v1,...,v n],[E])
Figure 2: The syntax of Llanguage.
In this section we introduce an imperative programming
languageLthat supports standard constructs for writing
implementations, and has some novel constructs for writing
speciﬁcations.
3.1 The Language L
The syntax of the language Lis stated in Fig. 2. We
discuss the features of the language below.
Expressions .A data value dis any value from some
data domain setD, which contains all values in the lan-
guage (e.g., in C#, all integers, characters, arrays, hash-
sets, ...). A variable vbelongs to a (ﬁnite) set of variables
Var. Anexpression is either a data value d, a variable v,
an operator applied to variables v1,v2or an array access
v1[v2]. Here,opbinrepresents a set of binary operators (e.g.,
+,·,=,<,∧) andopuna set of unary operators (e.g., ¬,|·|).
We point out that the syntax of Lensures that programs
are inthree address code : operators can only be applied
to variables, but not to arbitrary expressions. The moti-
vation for this choice is that three address code enables us
to observe any expression in the program by observing only
variables. We point out that any program can be (automat-
ically) translated into three address code by assigning eac h
subexpression toanewvariable. For example, thestatement
v1:=v2+(a+b) can be translated into three-address code
as follows: v3:=a+b;v1:=v2+v3. This enables us to
observe the subexpression a+bby observing v3.
Statements . The statements of Lallow to build simple
imperative programs: assignments (to variables and array
elements), skipstatement, composition of statements, loop-
ing andbranchingconstructs. We also allow library functio n
calls inL, denoted by v:=f(v1,...,v n), where f∈Fis a
library function name, from a set of all library functions F.
There are two special observe constructs, which are only
available to the teacher (and not to the student). We dis-
cuss the observe statements in §3.3 below. We assume that
each statement sis associated with a unique program loca-
tionℓ, and write ℓ:s.
Functions . For space reasons we do not deﬁne functions
here. We could easily extend the language to (recursive)
functions. In fact we allow (recursive) functions in our im-
plementation.
Semantics . Weassume some standardimperative seman-
tics to execute programs written in the language L(e.g., for
C#we assume the usual semantics of C#). The two ob-
servestatements have the same semantic meaning of the
skipstatement.
Computation domain . We extend the data domain D
by a special symbol ?, which we will use to represent any
data value. We deﬁne the computation domain Val asso-
ciated with our language LasVal=D∪(F×D∗). Weassume the data domain Dis equipped with some equality
relation = D⊆D×D(e.g., for C#we have ( x,y)∈=Diﬀ
aandbare of the same type and comparison by the equals
method returns true). We denote byE= 2Val×Valthe set of
all relations over Val. We deﬁne a default equality relation
Edef∈Eas follows: Wehave( x,y)∈Edefiﬀx=?ory=?or
(x,y)∈=D. We have (( f,x1,...,x n),(f′,y1,...,y n))∈Edef
iﬀf=f′and (xi,yi)∈Edeffor all 1≤i≤n.
Computation trace . A(computation) trace γoversome
ﬁnite set of (programming) locations Locis a ﬁnite sequence
of location-value pairs ( Loc×Val)∗. We use the notation
ΓLocto denote the set of all computation traces over Loc.
Given some γ∈ΓLocandLoc′⊆Loc, we denote by γ|Loc′
thesequencethatwe obtainbydeletingall pairs ( ℓ,val)from
γ, whereℓ/\e}atio\slash∈Loc′.
3.2 Student Implementation
In the following we describe how a computation trace γ
is generated for a student implementation Qon a given in-
putσ. The computation trace is initialized to the empty
sequence γ=ǫ. Then the implementation is executed on
σaccording to the semantics of L. During the execution
we append location-value pairs to γfor every assignment
statement: For ℓ:v1:=eorℓ:v1[v2] :=ewe append
(ℓ,σ(v1)) toγ(we denote by σ(v1) the current value of
v1). We point out that we add the complete array σ(v1)
to the trace for an assignment to an array variable v1. For
a library function call ℓ:v:=f(v1,...,v n) we append
(ℓ,(f,σ(v),σ(v1),...,σ(vn))) toγ. We denote the resulting
traceγby/llbracketQ/rrbracket(σ). This construction of a computation trace
can be achieved by instrumenting the implementation in an
appropriate manner.
3.3 Teacher Speciﬁcation
The teacher uses observe andobserveFun for specifying
the key values she wants to observe during the execution of
the speciﬁcation and for deﬁning an equality relation over
computation domain. As usual the rectangular brackets ‘[’
and ‘]’ enclose optional arguments.
In the following we describe how a computation trace γ
is generated for a speciﬁcation Pon a given input σ. The
computation trace is initialized to the empty sequence γ=ǫ.
Thenthespeciﬁcation isexecutedaccordingtothesemantic s
ofL. During the execution we append location-value pairs
toγonly for observe andobserveFun statements: For ℓ:
observe(v,[E]) we append ( ℓ,σ(v)) toγ(we denote by σ(v)
the currentvalue of v). Forℓ:observeFun (f[v1,...,v n],[E])
we append ( ℓ,(f,x1,...,x n)) toγ, wherexi=σ(vi), if the
ithargument to fhas been speciﬁed, and xi=?, if it has
been left out. We denote the resulting trace γby/llbracketP/rrbracket(σ).
Custom data equality . The possibility of specifying an
equality relation E∈Eat some location ℓis very useful
for the teacher. We point out that in practice the teacher
has to specify Eby an equality function ( Val×Val)→
{true,false}. The teacher can use Eto deﬁne the equality
ofsimilar computation values . We show its usage on ex-
amplesR3andR4(Fig. 1); both examples implement the
removing strategy (discussed in§2) in almost identical ways
— the only diﬀerence is on lines 10 and 9, respectively, where
implementations use diﬀerent characters to denote a charac -
ter removed from a string: ’#’and’ ’. In speciﬁcation RS
the teacher uses the equality function CompareLetterString
(deﬁned in CDE) — which compares only letters of twostrings — to deﬁne value representations of both implemen-
tations, regardless of used characters, as equal.
We call a function δ:Loc→Eacomparison function .
We deﬁne δ(ℓ) =Efor every statement ℓ:observe(v,E) or
ℓ:observeFun (f[v1,...,v n],E). For statements, where [ E]
has been left out, we set the default value δ(ℓ) =Edef.
Non-deterministic choice . We assume that the teacher
can use some ﬁnite set of non-deterministic Boolean vari-
ablesB={nd1,...,ndn}⊆Var(these are not available
to the student). Non-deterministic choice allows the teach er
to specify variations in implementations, as discussed in §2.
Non-deterministic variables are similar to the input vari-
ables, in the sense that are assigned before program is exe-
cuted. We note that this results into 2ndiﬀerent program
behaviors for a given input.
4. MATCHING
In this section, we deﬁne what it means for an imple-
mentation to (partially) match orfully match a speciﬁcation
and describe the corresponding matching algorithms. The
teacher has to determine for each speciﬁcation which def-
inition of matching has to be applied. In case of partial
matching we speak of ineﬃcient speciﬁcations and in case
of full matching of eﬃcient speciﬁcations .
4.1 Trace Embedding
We start out by discussing the problem of Trace Embed-
dingthat we use as a building block for the matching algo-
rithms.
Subsequence . We call c∈{partial,full}amatching cri-
terion. Letγ1= (ℓ1,val1)(ℓ2,val2)···(ℓn,valn) andγ2=
(ℓ′
1,val′
1)(ℓ′
2,val′
2)···(ℓ′
m,val′
m) be two computation traces
over some set of locations Loc, and let δbe some compari-
son function (as deﬁned in §3.3). We say γ1is asubsequence
ofγ2w.r.t. to δ,c, written γ1⊑δ,cγ2, if there are indices
1≤k1< k2<···< kn≤msuch that for all 1 ≤i≤nwe
haveℓi=ℓ′
kiand (vali,val′
ki)∈δ(ℓi); in case of c=fullwe
additionally require that γ1andγ2|{ℓ1,...,ℓn}have the same
length. We refer to ( vali,val′
ki)∈δ(ℓi) asequality check . If
δ(ℓi) =Id(the identity relation over Val) for all 1≤i≤n,
we obtain the usual deﬁnition of subsequence.
Since deciding subsequence, i.e., γ1⊑δ,cγ2, is a central
operation in this paper, we state complexity of this decisio n
problem. It is easy to see that deciding subsequence require s
onlyO(m) equality checks; basically one iteration over γ2is
suﬃcient.
Mapping Function . LetLoc1andLoc2be two disjoint
sets of locations. We call an injective function π:Loc1→
Loc2amapping function . We lift πto a function π: ΓLoc1→
ΓLoc2by applying it to every location, i.e., we set
π(γ) = (π(ℓ1),val1)(π(ℓ2),val2)···
forγ= (ℓ1,val1)(ℓ2,val2)···∈ΓLoc1.
Given a comparison function δ, a matching criterion c,
and computation traces γ1∈ΓLoc1andγ2∈ΓLoc2we say
thatγ1can be embedded in γ2byπ, iﬀπ(γ1)⊑δ◦π−1,cγ2,
and write γ1⊑π
δ,cγ2. We refer to πasembedding witness .
Executing a program on set of assignments Igives rise to
a set of traces, one for each assignment σ∈I. We say that
the set of traces ( γ1,σ)σ∈Ican be embedded in ( γ2,σ)σ∈Iby
πiﬀγ1,σ⊑π
δ,cγ2,σfor allσ∈I.Definition 1 (Trace Embedding). Trace Embed-
dingis the problem of deciding for given sets of traces (γ1,σ)σ∈I
and(γ2,σ)σ∈I, a comparison function δ, and a matching cri-
terionc, if there is a witness mapping function π, such that
γ1,σ⊑π
δ,cγ2,σfor allσ∈I.
Complexity . Clearly, Trace Embeddingis in NP (assum-
ing equality checks can be done in polynomial time): we ﬁrst
guess the mapping function π:Loc1→Loc2and then check
γ1,σ⊑π
δ,cγ2,σfor allσ∈I(which is cheap as discussed
above). However, it turns out that Trace Embedding is NP-
complete evenfor asingleton setI, asingleton computation
domainVal, and the full matching criterion .
Theorem 1.Trace Embedding is NP-complete (assum-
ing equality checks can be done in polynomial time).
Proof. In order to show NP-hardness we reduce Permu-
tation Pattern [6] to Trace Embedding. First, we formally
deﬁne Permutation Pattern. Let n,kbe positive integers
withk≤n. Letσbe a permutation of {1,···,n}and let
τbe a permutation of {1,···,k}. We say τoccursinσ, if
there is an injective function π:{1,···,k}→{1,···,n}
such that πis monotone, i.e., for all 1 ≤r < s≤kwe
haveπ(r)< π(s) andπ(τ(1))···π(τ(k)) is a subsequence
ofσ(1)σ(2)···σ(n).Permutation Pattern is the problem of
deciding whether τoccurs in σ.
We now give the reduction of Permutation Pattern to
Trace Embedding. We will construct two traces γ1andγ2
over a singleton computation domain Val, and over the sets
oflocations Loc1={1,...,k}andLoc2={1,...,n}. Weset
δ(i) =Id(the identity function on Val) for every i∈Loc1.
Because Valis singleton, we can ignore values in the rest
of the proof. We set γ1= 12···kτ(1)τ(2)···τ(k) andγ2=
12···nσ(1)σ(2)···σ(n). Because every i∈{1,···,k}oc-
curs exactly twice in γ1andγ2, partial and full matching cri-
teria are equivalent so we can ignore the diﬀerence. We now
show that τoccurs in σiﬀ there is an injective function π:
Loc1→Loc2withγ1⊑πγ2. We establish this equivalence
by two observations: First, because every i∈{1,···,k}oc-
curs exactly twice in γ1andγ2we have 12···k⊑π12···n
andτ(1)τ(2)···τ(k)⊑πσ(1)σ(2)···σ(n) iﬀγ1⊑πγ2. Sec-
ond, 12···k⊑π12···niﬀπ:Loc1→Loc2is monotone.
Algorithm . Fig. 3 shows our algorithm, Embed, for the
Trace Embedding problem. A straightforward algorithmic
solution for the trace embedding problem is to simply test
all possible mapping functions. However, there is an expo-
nential number of such mapping functions w.r.t. to the car-
dinality of Loc1andLoc2. This exponential blowup seems
unavoidable as the combinatorial search space is responsib le
for the NP hardness. The core element of our algorithm is
a pre-analysis that narrows down the space of possible map-
ping functions eﬀectively. We observe that if ℓ2=π(ℓ1) and
γ1⊑π
δ,cγ2, then there exists a trace embedding restricted
to locations ℓ1andℓ2, formally: γ1|{ℓ1}⊑{ℓ1/mapsto→ℓ2}
δ,c γ2|{ℓ2}.
The algorithm uses this insight to create a (bipartite) grap h
G⊆Loc1×Loc2of potential mapping pairs in lines 2-7. A
pair of locations ( ℓ1,ℓ2)∈Gis apotential mapping pair iﬀ
there exists a trace embedding restricted to locations ℓ1and
ℓ2, as described above.
The key idea in ﬁnding an embedding witness πis to con-
struct a maximum bipartite matching inG. A maximum
bipartite matching has an edge connecting every program1:Embed((γ1,σ)σ∈I,(γ2,σ)σ∈I,Loc1,Loc2,δ,c):
2:G←Loc1×Loc2
3:for allℓ1∈Loc1,ℓ2∈Loc2:
4: for allσ∈I:
5: ifγ1,σ|{ℓ1}/\e}atio\slash⊑{ℓ1/mapsto→ℓ2}
δ,cγ2,σ|{ℓ2}:
6: G←G\{(ℓ1,ℓ2)}
7: break
8:for allπ∈MaximumBipartiteMatching (G):
9: found←true
10: for allσ∈I:
11: ifγ1,σ/\e}atio\slash⊑π
δ,cγ2,σ:
12: found←false
13: break
14: iffound=true: return true
15:returnfalse
Figure 3: Algorithm for Trace Embedding problem.
location from Loc1to a distinct location in Loc2and thus
gives rise to an injective function π. We point out that such
an injective function πdoes not need to be an embedding
witness, because, by observing only a single location pair
at a time, it ignores the order of locations. Thus, for each
maximum bipartite matching [26] πthe algorithm checks (in
lines 8-14) if it is indeed an embedding witness.
The key strength of our algorithm is that it reduces the
search space for possible embedding witnesses π. The ex-
perimental evidence shows that this approach signiﬁcantly
reduces the number of possible matchings and enables avery
eﬃcient algorithm in practice, as discussed in §6.
4.2 PartialMatching
Wenowdeﬁnethenotionof partial matching (alsoreferred
to simply as matching ) which is used to check whether an
implementation involves (at least) those ineﬃciency issue s
that underlie a given ineﬃcient speciﬁcation.
Definition 2 (Partial Matching). LetPbe a spec-
iﬁcation with observed locations Loc 1, letδbe the compari-
son function speciﬁed by P, and let Qbe an implementation
whose assignment statements are labeled by Loc 2. Then im-
plementation Q(partially) matches speciﬁcation P, on a
set of inputs I, if and only if there exists a mapping function
π:Loc1→Loc2and an assignment to the non-deterministic
variables σndsuch that γ1,σ⊑π
δ,cγ2,σ, for all input val-
uesσ∈I, where γ1,σ=/llbracketP/rrbracket(σ∪σnd),γ2,σ=/llbracketQ/rrbracket(σ)and
c=partial.
Fig. 4 describes an algorithm for testing if an implementa-
tion (partially) matches a given speciﬁcation over a given s et
of input valuations I. In lines 6-7, the implementation Qis
executed on all input values σ∈I. In line 9, the algorithm
iterates throughall assignments σndtothenon-deterministic
variables BPof the speciﬁcation P. In lines 10-11, the spec-
iﬁcation Pis executed on all inputs σ∈I. With both sets
of traces available, line 12 calls subroutine Embedwhich
returnstrueiﬀ there exists a trace embedding witness.
Example . We now give an example that demonstrates
our notion of programs and that contains example applica-
tions of algorithms EmbedandMatches . In Fig. 5 we state
two implementations, (a) and (b), and one speciﬁcation (c).
These programs represent simpliﬁed versions (transformed
into three adress code) of R1(after function inlining), R31:Matches (Speciﬁcation P,Implementation Q,InputsI):
2:Loc1= observed locations in P
3:δ= comparison function speciﬁed by P
4:c= matching criterion
5:Loc2= assignment locations of Q
6:for allσ∈I:
7: γ2,σ←/llbracketQ/rrbracket(σ)
8:BP= non-deterministic variables in P
9:for allassignments σndtoBP:
10: for allσ∈I:
11: γ1,σ←/llbracketP/rrbracket(σ∪σnd)
12: ifEmbed((γ1,σ)σ∈I,(γ2,σ)σ∈I,Loc1,Loc2,δ,c):
13: returntrue
14:returnfalse
Figure 4: Matching algorithm.
1Puzzle(s, t) {
2i = 0;
3n = |s|;
4while (i < n) {
5c = s[i] ;
6j = 0;
7cnt1 = 0;
8while (j < n) {
9c2 = s[j] ;
10 if (c == c2) {
11 cnt1 = cnt1 + 1; }
12 j = j + 1; }
13j = 0;
14cnt2 = 0;
15while (j < n) {
16 c2 = t[j] ;
17 if (c == c2) {
18 cnt2 = cnt2 + 1; }
19 j = j + 1; }
20i = i + 1; }}(a)
1Puzzle(s, t) {
2i = 0;
3n = |s|;
4while (i < n) {
5c = s[i] ;
6ss =Split(s,c);
7cnt1 = |ss|;
8st =Split(t,c);
9cnt2 = |st|;
10i = i + 1; }}(b)1Puzzle(s, t) {
2i = 0;
3n = |s|;
4while (i < n) {
5c = s[i];
6observe(c);
7j = 0;
8cnt1 = 0;
9while (j < n) {
10 c2 = s[j];
11 if (nd1)
12 observe(c2);
13 j = j + 1; }
14j = 0;
15if (!nd1 )
16 observeFun (Split());
17while (j < n) {
18 c2 = t[j];
19 if (nd1)
20 observe(c2);
21 j = j + 1; }
22if (!nd1 )
23 observeFun (Split());
24i = i + 1; }}(c)
Figure 5: Implementations (a), (b) and Spec. (c).
andSC(Fig. 1). Note, that every assignment and observe
statement is on its own line; we denote line iin program x
by by location ℓx,i. The argument [ E] has been left out for
all locations in the speciﬁcation, thus we have δ(ℓ) =Edef
for all speciﬁcation locations ℓ.
Algorithm Matches runs all three programs on input val-
ues s = ”aab”and t = ”aba”. For program (a) we obtain the
following computation trace:
γa= (ℓa,2,0)(ℓa,3,3)(ℓa,5,a)(ℓa,6,0)(ℓa,7,0)(ℓa,9,a)(ℓa,11,1)
(ℓa,12,1)(ℓa,9,a)(ℓa,11,2)(ℓa,12,2)(ℓa,9,b)(ℓa,12,3)(ℓa,13,0)···
Similarly, for program (b) we obtain:
γb= (ℓb,2,0)(ℓb,3,3)(ℓb,5,a)(ℓb,6,(Split,aab,a))(ℓb,7,3)
(ℓb,8,(Split,aba,a))(ℓb,9,3)(ℓb,10,1)(ℓb,5,a)···
For speciﬁcation (c) we obtain two traces, depending on the
choice for the non-deterministic variable nd1:
γc,t= (ℓc,6,a)(ℓc,12,a)(ℓc,12,a)(ℓc,12,b)(ℓc,20,a)(ℓc,20,b)···
γc,f= (ℓc,6,a)(ℓc,16,(Split,?,?))(ℓc,23,(Split,?,?))···Algorithm Matches then calls Embedto check for trace
embedding. Algorithm Embedﬁrst constructs a potential
graphG, which contains an edge for two locations of the
speciﬁcation and the implementation that show the same
values.
Forimplementation(a), weobtainthefollowing graph: Ga=
{(ℓc,6,ℓa,5),(ℓc,6,ℓa,9),(ℓc,6,ℓa,16),(ℓc,12,ℓa,9),(ℓc,20,ℓa,16)}. No-
ticethat ℓc,6showsthesamevaluesasthelocations ℓa,5,ℓa,9,ℓa,16
in the implementation (a). However, there is only one maxi-
malmatchingin Ga,πa={(ℓc,6,ℓa,5),(ℓc,12,ℓa,9),(ℓc,20,ℓa,16)},
which is also an embeddingwitness; thus implementation (a)
matches speciﬁcation (c).
For implementation (b) and nd1=true, we obtain the
graphGb,t={(ℓc,6,ℓb,5)}, from which we cannot construct
a maximal matching. However, for nd1=false, we obtain
Gb,f={(ℓc,6,ℓb,5),(ℓc,16,ℓb,6),(ℓc,23,ℓb,8)}, which is also an
embedding witness; thus implementation (b) matches speci-
ﬁcation (c).
4.3 Full Matching
Below we will deﬁne the notion of full matching , which
is used to match implementations against eﬃcient speciﬁca-
tions. We will require that for every loop and every library
function call in the implementation there is a correspondin g
loop and library function call in the matching speciﬁcation .
In order to do so, we need some helper deﬁnitions.
Observed loop iterations . We extend the construction
of the implementation trace (deﬁnedin §3.2): For each state-
mentℓ:whilevdos, we additionally appendelement ( ℓ,⊥)
to the trace whenever the loop body sis entered. We call
(ℓ,⊥) aloop iteration . Letπbe a embedding witness s.t.,
γ1⊑πγ2. We say that πobserves all loop iterations iﬀ be-
tween every two loop iterations ( ℓ,⊥) inγ2there exists a
pair (ℓ′,val), such that∃ℓ′′.π(ℓ′′) =ℓ′. In other words, we
require that between any two iterations of the same loop,
there exists some observed location ℓ′.
Observed library function calls . We say that πob-
serves all library function calls iﬀforevery( ℓ,f(val1,...,valn))
inγ2there is a ℓ′such that π(ℓ′) =ℓ.
Definition 3 (Full Matching). LetPbe a speciﬁca-
tion with observed locations Loc 1, letδbe the comparison
function speciﬁed by P, and let Qbe an implementation
whose assignment statements are labeled by Loc 2. Then im-
plementation Qfully matches speciﬁcation P, on a set
of inputs I, if and only if there exists a mapping function
π:Loc1→Loc2and an assignment to the non-deterministic
variables σndsuch that γ1,σ⊑π
δ,cγ2,σ, for all input valuations
σ∈I, whereγ1,σ=/llbracketP/rrbracket(σ∪σnd),γ2,σ=/llbracketQ/rrbracket(σ),c=full
andπobserves all loop iterations and library function calls.
We note that procedure Embed(Fig. 3) can easily check
at line 11 whether the current mapping πobserves all loop
iterations and library function calls.
Itis tediousforateacherto exactly specify all possible loop
iterations and library function calls used in diﬀerent eﬃci ent
implementations. We add two additional constructs to the
languageLto simplify this speciﬁcation task.
Cover statement . We extendLby twocover state-
ments:ℓ:cover(f[v1,...,v m],[E]) andℓ:cover(v). The
ﬁrst statement is the same as the statement ℓ:observeFun (
f[v1,...,v m],[E]), except that we allow the embedding wit-
nessπto not map ℓto any location in the implementation.
Thisenablestheteachertospecifythatfunction f(v1,...,v m)may appear in the implementation. The second statement
allowsπto mapℓto a location ℓ′that appears at mostσ(v)
times for each appearance of ℓ, where σ(v) is the current
value of the speciﬁed variable v. Thuscover(v) enables the
teacher to cover any loop with up to σ(v) iterations.
Example . Now we present examples for eﬃcient imple-
mentations ( E1andE2) and speciﬁcation ( ES) for the Ana-
gram problem (Fig. 1). The teacher observes computed val-
ues on lines 9, 12and 14, and uses anon-deterministicchoice
(on line 11) to choose if implementations count the number
of characters in each string, or decrement one number from
another. Also the teacher allows up totwo library function
calls and two loops with at most 255 iterations, deﬁned by
coverstatements on lines 4,5,6 and 17.
5. EXTENSIONS
In this section, we discuss useful extensions to the core
material presented above. These extensions are part of our
implementation, but we discuss them separately tomake the
presentation easier to follow.
One-to-many Mapping . AccordingtodeﬁnitionofTrace
Embedding, an embedding witness πmaps one implementa-
tion location to a speciﬁcation location, i.e., it construc ts a
one-to-one mapping. However, it is possible that a student
splitsa computation of some value over multiple locations.
For example, in the implementation stated in R5(Fig. 1),
the student removes a character from a string across three
diﬀerent locations (on lines 9, 11, 13 and 14), depending on
the location of the removed character in the string. This
requires to map a singlelocation from the speciﬁcation to
multiple locations in the implementation! For this reason,
we extend the notion of trace embedding to one-to-many
mappings π:Loc1→2Loc2whereπ(ℓ′)∩π(ℓ) =∅for all
ℓ/\e}atio\slash=ℓ′. It is easy to extend procedure Embed(Fig. 3) to
this setting: the potential graph Gis also helpful to enu-
merate every possible one-to-many mapping. However, it is
costly (andunnecessary)tosearch for arbitrary one-to-ma ny
mappings. We use heuristics to consider only a few one-to-
many mappings. For example, one of the heuristics in our
implementation checks if the same variable is assigned in di f-
ferent branches of an if-statement (e.g., in example R5, for
all three locations there is an assignment to variable cp).
Although many-to-many mappings may seem more pow-
erful, we point out that the teacher can always write a spec-
iﬁcation that is more succinct than the implementation of
the student, i.e., the above described one-to-many mapping s
provide enough expressivity to the teacher.
Non-deterministic behaviour . Trace Embedding re-
quiresequal values in thesame order in the speciﬁcation and
implementationtraces. However, animplementationcanuse
alibraryfunctionwithnon-deterministicbehaviour, e.g. , the
values returned by a random generator or the iteration order
over a set data structure. For such library functions we elim -
inate non-determinism by ﬁxing one particular behaviour,
i.e, we ﬁx the values returned by a random generator or the
iteration order over a set during program instrumentation.
These ﬁxes do not impact functionally correct programs be-
cause they cannot rely on some non-deterministic behaviour
but allow us to apply our matching techniques.
6. IMPLEMENTATIONANDEXPERIMENTSWe now describe our implementation and present an ex-
perimental evaluation of our framework. More details on
our experiments can we found on the website [1].
6.1 Experimental Setup
Our implementation of algorithm Matches (Fig. 4) is in
C#and analyzes C#programs (i.e., implementations and
speciﬁcations are in C#). We used Microsoft’s Roslyn com-
piler framework [3] for instrumenting every sub-expressio n
to record value during program execution.
Data. Weused 3preexistingproblems from Pex4Fun (as
mentioned in§1): (1) the Anagram problem, where students
are asked to test if two strings could be permuted to become
equal, (2) the IsSorted problem, where students are asked to
test if the input array is sorted, and (3) the Caesarproblem,
where students are asked toapply Caesar cipher to the input
string. We have chosen these 3 speciﬁc problems because
they had a high number of student attempts, diversity in
algorithmic strategies and a problem was explicitly stated
(for many problems on Pex4Fun platform students have to
guess the problem from failing input-output examples).
We also created a new course on the Pex4Fun platform
with 21 programming problems. These problems were as-
signed as a homework to students in a second year under-
graduate course. We created this course to understand per-
formance related problems that CSstudents make, as op-
posed to regular Pex4Fun users who might not have previ-
ous programming experience. We encouraged our students
to write eﬃcient implementations by giving more points for
performance eﬃciency than for mere functional correctness .
We omit the description of the problems here, but all de-
scriptions are available on the original course page [2].
6.2 Methodology
In the following we describe the methodology by which we
envision the technique in the paper to be used.
Theteacher maintainsasetof eﬃcientandineﬃcientspec-
iﬁcations. A new student implementation is checked against
all available speciﬁcations. If the implementation matche s
some speciﬁcation, the associated feedback is automatical ly
provided to the student; otherwise the teacher is notiﬁed
that there is a new unmatched implementation. The teacher
studies the implementation and identiﬁes one of the follow-
ing reasons for its failure to match any existing speciﬁcati on:
(i) The implementation uses a new strategy not seen before.
In this case, the teacher creates a new speciﬁcation. (ii) Th e
existing speciﬁcation for the strategy used in the implemen -
tation is too speciﬁc to capture the implementation. In this
case, the teacher reﬁnes that existing speciﬁcation. This
overall process is repeated for each unmatched implementa-
tion.
New speciﬁcation . A teacher creates a new speciﬁca-
tion using the following steps: (i) Copy the code of the un-
matched implementation. (ii) Annotate certain values and
function calls with observe statements. (iii) Remove any un -
necessary code (not needed in the speciﬁcation) from the
implementation. (iv) Identify input values for the dynamic
analysis for matching. (v) Associate a feedback with the
speciﬁcation.
Speciﬁcation reﬁnement . To reﬁne a speciﬁcation, the
teacher identiﬁes one of the following reasons as to why
an implementation did not match it: (i) The implementa-
tion diﬀers in details speciﬁed in the speciﬁcation; (ii) Th e0 5 10 15 20 2505001,0001,500
# ofinspection steps# of matched implementations(a) # of required inspection steps (1/3)
0 10 20 30 4005001,0001,500
time [min]# of matched implementations(b) time required to write/reﬁne speciﬁcations (1/3)
Anagram
IsSorted
Caesar
0 5 10 1502040
# ofinspection steps# of matched implementations(c) # of required inspection steps (2/3)
0 10 20 30 40020406080
time [min]# of matched implementations(d) time required to write/reﬁne speciﬁcations (2/3)
DoubleChar LongestEqual
LongestWord RunLength
Vigenere BaseToBase
CatDog MinimalDelete
CommonElement Order3
0 2 4 6 8 10 1201020304050
# ofinspection steps# of matched implementations(c) # of required inspection steps (3/3)
0 10 20 30020406080
time [min]# of matched implementations(d) time required to write/reﬁne speciﬁcations (3/3)
2DSearch TableAggSum
Intersection ReverseList
SortingStrings MinutesBetween
MaxSum Median
DigitPermutation Coins
Seq235
Figure 6: The number of inspection steps and time required to completely specify assignments.Problem Correct IneﬃcientNSIND LS/LIOSOIMPerformance
Name Implement. Implement. Avg. Max.
Anagram 290 (37.9%) 261 (90.0%) 525131.41 118928357 0.42 7.67
IsSorted 1460 (90.1%) 139 (9.5%) 323221.45 651 13 0.33 1.31
Caesar 566 (81.2%) 343 (60.6%) 518111.10 739172 0.37 0.83
DoubleChar 46 (97.9%) 31 (67.4%) 15100.72 323 20.31 0.42
LongestEqual 37 (78.7%) 1 (2.7%) 13100.57 135 20.33 0.44
LongestWord 39 (83.0%) 13 (33.3%) 26201.31 746 15 0.35 0.47
RunLength 43 (97.7%) 32 (74.4%) 16100.90 837 54 0.33 0.44
Vigenere 41 (93.2%) 32 (78.0%) 35100.64 384 60.34 0.50
BaseToBase 15 (39.5%) 14 (93.3%) 25110.35 364 13 0.36 0.48
CatDog 41 (87.2%) 8 (19.5%) 218112.02 21531629 0.36 0.58
MinimalDelete 15 (39.5%) 8 (53.3%) 18232.21 475 10 0.86 4.36
CommonElement 43 (95.6%) 32 (74.4%) 414210.97 679107 0.36 0.53
Order3 40 (87.0%) 30 (75.0%) 612121.45 678 19 0.40 0.59
2DSearch 37 (84.1%) 36 (97.3%) 37111.09 267 10.34 0.45
TableAggSum 11 (25.0%) 10 (90.9%) 15110.80 3144 10.40 0.53
Intersection 14 (31.8%) 12 (85.7%) 37210.89 473 50.37 0.56
ReverseList 39 (97.5%) 0 (0.0%) 03100.35 434 10.34 0.44
SortingStrings 41 (91.1%) 34 (82.9%) 511111.48 13110 866 0.55 14.59
MinutesBetween 45 (100.0%) 0 (0.0%) 05100.64 8101 10.37 0.48
MaxSum 42 (95.5%) 17 (40.5%) 27111.14 251 30.35 0.47
Median 47 (100.0%) 47 (100.0%) 11100.39 1100 10.34 0.44
DigitPermutation 36 (100.0%) 1 (2.8%) 13100.26 429 40.32 0.44
Coins 27 (65.9%) 14 (51.9%) 26111.65 493175 2.41 15.44
Seq235 33 (89.2%) 30 (90.9%) 412121.79 3232 30.94 22.08
Table 1: List of all assignments with the experimental resul ts.
speciﬁcation observes more values than those that appear in
the implementation; (iii) The implementation uses diﬀeren t
data representation. In case (i) the teacher adds a new non-
deterministic choice, and, if necessary, observes new valu es
or function calls; in case (ii) the teacher observes less val -
ues; and in case (iii) the teacher creates or reﬁnes a custom
data-equality.
Input values . Our dynamic analysis approach requires
theteachertoassociate inputvalueswithspeciﬁcations. T hese
input values should cause the corresponding implementa-
tions toexhibittheirworst-case behavior; otherwise anin eﬃ-
cient implementation might behave similar toan eﬃcient im-
plementation and for this reason match the speciﬁcation of
the eﬃcient implementation. This implies that trivialinputs
should be avoided. For example, two strings with unequal
lengths constitute a trivial input for the counting strateg y
since each of its three implementations C1-C3(Fig. 1) then
exit immediately. Similarly, providing a sorted input for t he
sorting strategy is meaningless. We remark that it is easy
for a teacher (who understands the various strategies) to
provide good input values.
Granularity of feedback . We want to point out that
the granularity of a feedback depends on the teacher. For
example, in a programming problem where sorting the input
value is an ineﬃcient strategy, the teacher might not want to
distinguish between diﬀerent sorting algorithms, as they d o
not require adiﬀerent feedback. However, in a programming
problem where students are asked to implement a sorting
algorithm it makes sense to provide a diﬀerent feedback for
diﬀerent sorting algorithms.
6.3 Evaluation
We report results on the 24 problems discussed above in
Table 1.
Results from manual code study . We ﬁrst observe
that a large numberof students managed to write afunction-
ally correct implementation on most of the problems (col-umnCorrect Implementations ). This shows that Pex4Fun
succeeds in guiding students towards a correct solution.
Our second observation is that for most problems a large
fraction of implementations is ineﬃcient (column Ineﬃcient
Implementations ), especially for Anagram problem: 90%.
This shows that although students manage to achieve func-
tional correctness, eﬃciency is still an issue (recall that in
our homework the students were explicitly asked and given
extra points for eﬃciency).
We also observe that for all, except two, problems there
is at least one ineﬃcient algorithmic strategy, and for most
problems (62.5%) there are several ineﬃcient algorithmic
strategies (column N).These results highly motivate the
need for a tool that can ﬁnd ineﬃcient implementations and
also provide a meaningful feedback on how to ﬁx the problem .
Precision and Expressiveness . For each programming
assignment we used the above described methodology and
wrote a speciﬁcation for each algorithmic strategy (both ef -
ﬁcient and ineﬃcient). We then manually veriﬁed that each
speciﬁcation matches all implementations of the strategy,
hence providing desired feedback for implementations. This
shows that our approach is preciseandexpressive enough
to capture the algorithmic strategy, while ignoring low lev el
implementation details.
Teacher Eﬀort . To provide manual feedback to students
the teacher would have to go through every implementation
and look at its performance characteristics. In our approac h
the teacher has to take a look only at a few representative
implementations. In column Swe report the total num-
ber of inspection steps that we required to fully specify one
programming problem, i.e., the number of implementations
that the teacher would had to go through to provide feed-
backonallimplementations. Forthe3pre-existingproblem s
the teacher would only have to go through 66 out of 2316 (or
around 3%) implementations to provide full feedback . Fig. 6
shows the number of matched implementations with each in-
spection step, as well the time it took us to create/reﬁne allspeciﬁcations (we measured the time it takes from seeing an
unmatched implementation, until writing/reﬁning a match-
ing speciﬁcation for it).
In columnLS/LIwe report the largest ratio of speciﬁca-
tion and average matched implementation in terms of lines
of code. We observe that in half of the cases the largest spec-
iﬁcation is about the same size or smaller than the average
matched implementation. Furthermore, the number of the
input values that need to be provided by the teacher is 1-2
across all problems (column I). In all but one problem ( Is-
Sorted) one set of input values is used for all speciﬁcations.
Also, in about one third of the speciﬁcations there was no
need for non-deterministic variables, and the largest num-
ber used in one speciﬁcation is 3 (column ND).Overall, our
semi-automatic approach requires considerably less teach er
eﬀort than providing manual feedback .
Performance . We plan to integrate our framework in
a MOOC platform, so performance, as for most web ap-
plications, is critical. Our implementation consists of tw o
parts. The ﬁrst part is the execution of the implementa-
tion and the speciﬁcation (usually small programs) on rel-
atively small inputs and obtaining execution traces, which
is, in most cases, neglectable in terms of performance. The
second part is the Embedalgorithm. As discussed in §4.1
the challenge consists in ﬁnding an embedding witness π.
WithOSobserved variables in the speciﬁcation and OIob-
served variables in the implementation, there areOI!
(OI−OS)!
possible injective mapping functions. E.g., for the Sort-
ingStrings problem that gives ≈1026possible mapping func-
tions (OI= 110,OS= 13). However, our algorithm reduces
this huge search space by constructing a bipartite graph G
of potential mappings pairs. In Mwe report the number of
mapping functions that our tool had to explore. E.g., for
SortingStrings only 866 diﬀerent mapping functions had to
be explored. For all values ( OS,OIandM) we report the
maximal numberacross all speciﬁcations. Inthe last column
we state the total execution time required to decide if one
implementation matches the speciﬁcation (average and max-
imal). Note that this time includes execution of both pro-
grams, exploration of all assignments to non-deterministi c
Boolean variables and ﬁnding an embedding witness π. Our
tool runs, in most cases, under half a second per implemen-
tation.These results show that our tool is fast enough to be
used in an interactive teaching environment.
6.4 Threats to Validity
Unsoundness . Our method is unsound in general since
it uses a dynamic analysis that explores only a few possible
inputs. However, we did not observe any unsoundness in our
large scale experiments. If one desires provable soundness ,
an embedding witness could be used as a guess for a simu-
lation relation that can then be formally veriﬁed by other
techniques. Otherwise, a student who suspects an incorrect
feedback can always bring it to the attention of the teacher.
Program size . We evaluated our approach on introduc-
tory programming assignments. Although questions about
applicability to larger programs might be raised, our goal
was not to analyze arbitrary programs, but rather to de-
velop a framework to help teachers who teach introductory
programming with providing performance feedback — cur-
rently a manual, error-prone and time-consuming task.
Diﬃculty of the speciﬁcation language . Although
we did not perform case study with third-party instructors,we report our experiences with using the proposed language.
We would also like to point out that writing speciﬁcations
is a one-time investment, which could be performed by an
experienced personnel.
7. RELATED WORK
7.1 Automated Feedback
There has been a lot of work in the area of generating
automated feedback for programming assignments. This
work can be classiﬁed along three dimensions: (a) aspects
on which the feedback is provided such as functional correct -
ness, performance characteristics or modularity (b) natur e
of the feedback such as counterexamples, bug localization o r
repair suggestions, and (c) whether static or dynamic anal-
ysis is used.
Ihantola et.al. [13] present a survey of various systems de-
veloped for automated grading of programming assignments.
The majority of these eﬀorts have focussed on checking for
functional correctness. This is often done by examining
the behavior of a program on a set of test inputs. These
test inputs can be manually written or automatically gen-
erated [25]. There has only been little work in testing for
non-functional properties. The ASSYST system uses a sim-
ple form of tracing for counting execution steps to gather
performance measurements [14]. The Scheme-robo system
counts the number of evaluations done, which can be used
for very coarse complexity analysis. The authors conclude
that better error messages are the most important area of
improvement [20].
The AI community has built tutors that aim at bug lo-
calization by comparing source code of the student and the
teacher’s programs. LAURA [5] converts teacher’s and stu-
dent’s program into a graph based representation and com-
pares them heuristically by applying program transforma-
tionswhilereportingmismatchesaspotentialbugs. TALUS[ 17]
matches a student’s attempt with a collection of teacher’s a l-
gorithms. It ﬁrst tries to recognize the algorithm used and
then tentatively replaces the top-level expressions in the stu-
dent’s attempt with the recognized algorithm for generatin g
correction feedback. In contrast, we perform trace compar-
ison (instead of source code comparison), which provides
robustness to syntactic variations.
Striewe and Goedicke have proposed localizing bugs by
trace comparisons. They suggested creating full traces of
program behavior while running test cases to make the pro-
gram behavior visible to students [22]. They have also sug-
gested automatically comparing the student’s trace to that
of a sample solution [23] for generating more directed feed-
back. However, no implementation has been reported. We
also compare the student’s trace with the teacher’s trace,
but we look for similarities as opposed to diﬀerences.
Recently it was shown that automated techniques can
also provide repair based feedback for functional correctn ess.
Singh’s SAT solving based technology [21] can successfully
generate feedback (of up to 4 corrections) on around 64% of
all incorrect solutions (from an MIT introductory program-
ming course) in about 10 seconds on average. While test
inputs provide guidance on whya given solution is incorrect
and bug localization techniques provide guidance on where
the error might be, repairs provide guidance on howto ﬁx an
incorrect solution. We also provide repair suggestions tha t
are manually associated with the various teacher speciﬁca-tions, but for performance based aspects. Furthermore, our
suggestions are not necessarily restricted to small ﬁxes.
7.2 Performance Analysis
The Programming Languages and Software Engineering
communities have explored various kinds of techniques to
generate performance related feedback for programs. Sym-
bolic execution based techniques have been used for iden-
tifying non-termination related issues [12, 7]. The SPEED
project investigated use of static analysis techniques for esti-
mating symbolic computational complexity of programs [27,
10, 11]. Goldsmith et.al. used dynamic analysis techniques
for empirical computational complexity [8]. The Toddler
tool reports a speciﬁc pattern: computations with repeti-
tive and similar memory-access patterns [19]. The Cachetor
tool reports memoization opportunities by identifying ope r-
ations that generate identical values [18]. In contrast, we
are interested in not only identifying whether or not there i s
a performance issue, but also identifying its root cause and
generating repair suggestions.
8. REFERENCES
[1]http://forsyte.at/static/people/radicek/fse14 .
[2] Making Programs Eﬃcient.
http://pexforfun.com/makingprogramsefficient .
[3] Microsoft ”Roslyn”CTP.
http://msdn.microsoft.com/en-us/vstudio/roslyn.aspx .
[4] Pex for fun. http://www.pexforfun.com/ .
[5] A. Adam and J.-P. H. Laurent. LAURA, a system to
debug student programs. Artif. Intell. , 15(1-2), 1980.
[6] P. Bose, J. F. Buss, and A. Lubiw. Pattern matching
for permutations. Inf. Process. Lett. , 65(5):277–283,
1998.
[7] J. Burnim, N. Jalbert, C. Stergiou, and K. Sen.
Looper: Lightweight detection of inﬁnite loops at
runtime. In ASE, pages 161–169, 2009.
[8] S. Goldsmith, A. Aiken, and D. S. Wilkerson.
Measuring empirical computational complexity. In
ESEC/SIGSOFT FSE , 2007.
[9] S. Gulwani. Example-based learning in
computer-aided STEM education. To appear in
Commun. ACM , 2014.
[10] S. Gulwani, K. K. Mehra, and T. M. Chilimbi. Speed:
precise and eﬃcient static estimation of program
computational complexity. In POPL, pages 127–139,
2009.
[11] S. Gulwani and F. Zuleger. The reachability-bound
problem. In PLDI, pages 292–304, 2010.
[12] A. Gupta, T. A. Henzinger, R. Majumdar,
A. Rybalchenko, and R.-G. Xu. Proving
non-termination. In POPL, pages 147–158, 2008.
[13] P. Ihantola, T. Ahoniemi, V. Karavirta, and
O. Sepp¨al¨a. Review of recent systems for automaticassessment of programming assignments. In
Proceedings of the 10th Koli Calling International
Conference on Computing Education Research , Koli
Calling ’10, pages 86–93, New York, NY, USA, 2010.
ACM.
[14] D. Jackson and M. Usher. Grading student programs
using ASSYST. In SIGCSE , pages 335–339, 1997.
[15] K. Masters. A brief guide to understanding MOOCs.
The Internet Journal of Medical Education , 1(2), 2011.
[16] R. Milner. An algebraic deﬁnition of simulation
between programs. Technical report, Stanford, CA,
USA, 1971.
[17] W. R. Murray. Automatic program debugging for
intelligent tutoring systems. Computational
Intelligence , 3, 1987.
[18] K. Nguyen and G. Xu. Cachetor: Detecting cacheable
data to remove bloat. In Proceedings of the 2013 9th
Joint Meeting on Foundations of Software Engineering ,
ESEC/FSE 2013, pages 268–278, New York, NY,
USA, 2013. ACM.
[19] A. Nistor, L. Song, D. Marinov, and S. Lu. Toddler:
Detecting performance problems via similar
memory-access patterns. In Proceedings of the 2013
International Conference on Software Engineering ,
ICSE ’13, pages 562–571, Piscataway, NJ, USA, 2013.
IEEE Press.
[20] R. Saikkonen, L. Malmi, and A. Korhonen. Fully
automatic assessment of programming exercises. In
Proceedings of the 6th Annual Conference on
Innovation and Technology in Computer Science
Education , ITiCSE ’01, pages 133–136, New York, NY,
USA, 2001. ACM.
[21] R. Singh, S. Gulwani, and A. Solar-Lezama.
Automated feedback generation for introductory
programming assignments. In PLDI, pages 15–26,
2013.
[22] M. Striewe and M. Goedicke. Using run time traces in
automated programming tutoring. In ITiCSE, pages
303–307, 2011.
[23] M. Striewe and M. Goedicke. Trace alignment for
automated tutoring. In CAA, 2013.
[24] N. Tillmann and J. de Halleux. Pex-white box test
generation for .NET. In TAP, pages 134–153, 2008.
[25] N. Tillmann, J. de Halleux, T. Xie, S. Gulwani, and
J. Bishop. Teaching and learning programming and
software engineering via interactive gaming. In ICSE,
2013.
[26] T. Uno. Algorithms for enumerating all perfect,
maximum and maximal matchings in bipartite graphs.
InISAAC, pages 92–101, 1997.
[27] F. Zuleger, S. Gulwani, M. Sinn, and H. Veith. Bound
analysis of imperative programs with the size-change
abstraction. In SAS, pages 280–297, 2011.