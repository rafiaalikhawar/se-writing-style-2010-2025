 Dictionary Learning Based Software Defect Prediction  
 
Xiao -Yuan Jing1,2*, Shi Ying1, Zhi-Wu Zhang1,2, Shan -Shan Wu1,2, Jin Liu1
 
1State Key Laboratory of Software Engineering, School of Computer, Wuhan University, Wuhan, China  
2College of Automation, Nanji ng University of Posts and Telecommunications, Nanjing, China  
* Corresponding author: jingxy_2000@126.com  
  
ABSTRACT  
In order to improve the quality of a software system, s oftware 
defect prediction aims to automatically identify defective 
software module s for efficient software test. To predict software 
defect, those classification methods with static code attributes 
have attracted a great deal of attention. In recent years, machine 
learning  technique s have been applied to defect  prediction. Due 
to the fa ct that there exist s the similarit y among different 
software modules, one software module can be approximately 
represented by a small proportion of other modules.  And the 
representation coefficients  over the pre-defined dictionary, which 
consists of histor ical software module data, are generally sparse. 
In this paper, we propose to use the dictionary learning  technique  
to predict software defect . By using the characteristics of the 
metrics mined from the open source software , we learn multiple 
dictionaries (including defect ive module  and defect ive-free 
module  sub-dictionar ies and the total dictionary) and sparse 
representation  coefficients . Moreover, we take th e 
misclassification cost issue  into account  because the 
misclassification of defect ive modules gene rally incurs much 
higher risk cost than that of defect ive-free ones. We thus propose 
a cost-sensitive discriminative dictionary learning (CDDL)  
approach for software defect classification and prediction . The 
widely used datasets  from NASA projects are empl oyed as test 
data to evaluate the performance of all compared methods . 
Experimental results show that CDDL outperforms several  
representative  state-of-the-art defect prediction methods . 
Categories and Subject Descriptors  
D.2.9 [Management ]: Software qualit y assurance (SQA) , G.1.3 
[Numerical Linear Algebra ]: Sparse, structured, and very large 
systems (direct and iterative methods) , I.5.2 [Design 
Methodology ]: Classifier design and evaluation . 
General Terms  
Algorith ms  
 
Keywords  
Software defect prediction, dictionary learning, sparse 
representation, cost-sensitive discriminative dictionary learning 
(CDDL) . 
1. INTRODUCTIO N 
Software defect prediction is one of the most important 
research topics in software engineering  [1-2,57,59], which is an 
efficient means to relieve the burden  on software code inspection 
or testing.  To achieve the goal of detect ing and correct ing the 
greatest number of defects in software , software defect prediction 
enables the organization’s limited re source to be reasonably 
allocated.  It can be generally categorized into two types: static 
and dynamic  defect prediction  technology . Static defect prediction 
technology mainly refers to  defect  number  prediction or defect 
distribution prediction  based on the  defect -related metrics . 
Dynamic defect prediction technology  predict s the distribution of 
the system defects over time  by using the defect generated time . 
Static prediction  technique has been widely used , because  it can  
predict the defect proneness of new  software modules with the 
historical defect data so as to improve the quality of software [3-
4]. The key of static defect prediction technique is how to fully 
analyze and utilize the existing historical data, and then build 
more precise and effective bina ry classifiers of software modules. 
In recent years, many  popular  classification methods , such as 
support vector machine  (SVM) [5 -7], decision tree [ 8-11], neural 
networks [ 12-13], Naï ve Bayes [ 14-17], and c ost-sensitive 
learning methods [ 18-22], have been  employed to achieve this 
goal. However, in the field of software defect prediction , these 
classification methods  often encounter some difficulties, for 
example, the class -imbalance problem  [23-25] and the 
misclassification  cost issue [ 18]. Class-imbalance  problem  
indicates that a software system contains much fewer defective 
modules than defect ive-free modules , which leads to negative 
influence on decision of classifiers [ 26-29]. Classifying a 
software module as defect ive-prone implies that more testers 
should be invested in the  verification activities, thus adding to the 
development cost. Misclassifying a module as defect ive-free 
carries the risk of system failure, which is also associated with 
cost implications  [58].  
Sparse representation, a recently dev eloped technique , arouses 
much interest from researchers due to its effectiveness and 
robustness. The idea of sparse representation is that information 
of a signal can be efficiently represented or coded by a linear Permission to make digital or hard copies of all or part of this w ork for 
personal or classroom use is granted without fee provided that copies are not 
made or distributed for profit or commercial advantage and that copies bear 
this notice and the full citation on the first page. To copy otherwise, to  
republish, to post on servers or to redistribute to lists, requires prior specific 
permission and/or a fee.  
ICSE'  14, May 31 - June 7,  2014, Hyderabad, India   
Copyright 2014 ACM 978 -1-4503 -2756 -5/14/06…$15.00.   
http://dx.doi.org/10.1145/2568225.2568320  Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a
fee. Request permissions from Permissions@acm.org.
ICSE’14 , May 31 – June 7, 2014, Hyderabad, India
Copyright 2014 ACM 978-1-4503-2756-5/14/05...$15.00
http://dx.doi.org/10.1145/2568225.2568320
414
 combination of a few elementary signals called atoms  [30-31]. 
The atoms are usually chosen from a so called over-complete 
dictionary  (i.e. a collection of atoms ) that the number of atoms 
exceeds the dimension of the signal space. Sparse representation 
usually has favorable discriminative power a nd owns robustness 
to noise from signals, both of which are the advantages over other 
conventional machine learning algorithms.  
Sparse representation based classification  (SRC) scheme used  
training samples as the over -complete dictionary , while this  
dictio nary may not be the optimal one . The d ictionary learning 
technique aims to produce  an over-complete dictionary that can 
code the input signal better  [32-33]. Recently, lots of efforts [34-
36] have been made on learning a more compact and robust 
dictionary for representing the input signal well, such that the 
reconstruction error is minimized and classification accuracy can 
be improved.  
1.1 Motivation  
In software defect prediction, a test module could be similar to 
part of historical modules , which almost belong  to the same class.  
In other words, one new module  will be compactly represented or 
coded by  a liner combination of a small portion of the entire 
historical data . This representation is thus naturally sparse . 
Therefore, sparse representation can well descr ibe the natural 
characteristic s of historical data, because  it not only fully utilizes 
the information of samples mined from the software module, but 
also is not influence d by the sample distribution. Thus , it can 
achieve favorable classification effect. I n this paper, we  introduce 
the sparse coding  based  dictionary  learning  to offer a solution to 
software defect prediction.  
Usually, the dictionary can be constructed by directly using the 
original training samples , whereas the original samples have 
much re dundancy and noise , which  are adverse to prediction. For  
the purpose of further improving the classification ability , we 
hope to learn a dictionary that is capable of representing test 
module well. Although the success of some works  [37-39] implies 
that learning a dictionary shared by all classes can bring  high 
classification accuracy, it loses the correspond ing relation  
between the dictionary atoms and the class labels. To acquire a 
better solution, we employ  the supervised dictionary learning  
technique whose sub -dictionary is separately learned from the 
specific data sets, namely, defective data and defective -free data. 
The reconstruction error associated with each class can be used 
for classification.  
During the software defect prediction process, two ty pes of 
misclassification errors are encountered. The type I 
misclassification happens when a defective -free module is 
predicted as defective one while a type II misclassification is that 
a defective  module is classified as defective -free. Type I 
misclassif ication leads to increasing  the development cost , and  
Type II misclassification associate s with risk cost. Naturally, we 
take the misclassification costs of type I and II errors into 
consideration by incorporating the cost factors into process of 
dictionar y learning.  
1.2 Contribution  
In this paper, we propose a novel approach that employs  
dictionary learning for software defect prediction. And the contributions of our study are summarized as following three  
points:  
(1) Although dictionary learning technique has been effective ly 
applied  to other domains,  to the best of our knowledge, we are 
the first attempt to introduc e the dictionary learning technique  
into the field of software  defect prediction . By using the 
characteristics of the metrics mined from the open  source 
software , we learn multiple dictionaries (including defect ive 
module  sub-dictionary, defect ive-free module  sub-dictionary and 
the total dictionary) and sparse representation  coefficients . 
(2) In order to enhance the classification ability, we desig n a 
cost-sensitive  discriminative  dictionary learning  (CDDL) 
approach . It can fully exploit the class information of historical 
data to improve the discriminative power by using the supervised 
dictionary learning technique . Meanwhile , by increasing 
punishm ent on type II misclassification  (a defective module is 
misclassified as defective -free one) in the procedure of dictionary 
learning , CDDL  takes the various  misclassification cost into 
consideration , which is beneficial to the prediction performance . 
(3) CDDL employ s the p rimary component analysis  (PCA) 
technique , a widely -used dimension reduction technique, to 
initialize all t he atoms of each sub -dictionary  and simultaneously 
solve the class -imbalance problem. M oreover, e ven if defect data 
are not enough , CDDL can easily build the over-complete 
dictionary  (sub-dictionar y for each class  is complete) , due to the 
characteristic of low dimension in software module  (for example, 
the dimension of soft ware module ranges from 2 0 to 40, which is 
measured by the co mmon metric elements, McCabe [ 40] or 
Halstead metric [ 41]). 
In this paper, we conduct the experiment s on ten NASA 
datasets , which are public and widely used for software  defect 
prediction . The experimental results demonstrate  that the 
proposed approach outpe rforms several representative  methods . 
The remainder part of this paper is organized as follows.  
Section 2  introduc es the related work. Section 3 describes the 
proposed approach . Section 4 introduce s the experimental setup. 
Section 5 shows  the experimental  results  and analysis . The 
conclusion  is drawn  in Section 6. 
2. RELATED WORK  
2.1 Defect Prediction  
Software Archives Instances Metrics
Training InstancesInstance
LearnerClassifier(1) Labeling
(defective/defective-free)(2) Feature
extraction
(3) Creating a 
training corpus
(4) Building a 
prediction model(5) Prediction 
& evaluation
 
Fig. 1: Defect Prediction Process  
 415 Fig. 1 shows the typical  defect  prediction process commonly 
used in the literature  [51-54]. The fir st step is to collect and label 
module s. A software module can be labeled as defective or 
defective -free according to whether the module contains defects 
or not.  Then, defect prediction metrics , such as complexity 
metrics , are used as module  features. The module  features and 
labels are used to train prediction models by using learner . 
Finally, the  classifier predicts the new modules .  
Many traditional classification  methods have been adopted for 
software defect  prediction, including SVM, decision tree, neur al 
networks, Bayes methods, and etc. How to improve  these 
methods has recently drawn great attention.  Gray et al.  [6] 
argued that it is important to explicitly  carry out cleansing stages  
for all of the data in  NASA datasets, and then SVM can be 
successful ly considere d as a classification method for defect 
prediction, which can acquire preferable predictive power than 
directly employ ing SVM without data pre-processing . Wang  et al.  
[8] proposed  a new defect prediction model based on C4.5 model, 
which introdu ces the Spearman ’s rank correlation coefficient  into 
the process of  choosing root node of the decision tree . Tuthan et 
al. [17] showed that the prediction  performance  can gain an  
improvement  by using the weighted N aï ve Bayes classifiers that 
assign  weights  to static code attributes .  
In the  process of defect prediction,  type I misclassification  cost 
and type II misclassification cost are different.  Cost-sensitive 
learning methods  can address this issue by generating a 
classification module with  minimum misc lassification cost . 
Zheng [ 18] presented cost-sensitive boosting algorithm  to 
improve neural ne twork classifiers  for defect prediction , which 
incorporate s the misclassification costs into the weight -update 
rule of boosting , such that the classification per formance on those 
samples with higher misclassification cost s can be improved . 
Jiang  [58] pointed out that t he effectiveness  of fault prediction 
models varies with the different misclassification cost . 
Different from the  above mentioned  methods  altering th e data 
distribution, ensemble learning  [27,42-44] aims at  preserv ing 
original data distribution . Sun et al.  [27] presented a c oding -
based ensemble learning  method , which  first converts imbalanced 
binary -class data into balanced  multiclass data and then bui lds a 
defect predictor on the multiclass  data with a specific coding 
scheme . Thus, it can avoid  the loss of important information  and 
class -imbalance problem.   
The difference between our  proposed  CDDL  approach and 
other  defect prediction methods is that we  incorporate  the state-
of-the-art dictionary learning  technique into the field of software 
defect prediction . Since d ictionary learning has shown  powerful 
classification capability in many application fields , we present a 
dictionary learning approach  to im prove the defect prediction 
performance.  
2.2 Dictionary Learning  
Both  sparse representation and dictionary learning have been 
successfully applied to many application fields, including image 
clustering  [45], compressed sensing [46] as well as  image 
classifica tion tasks  [47-48]. In sparse representation based 
classification,  the dictionary for sparse coding could be 
predefined. For example, Wright et al. [ 49] directly used the 
training samples of all classes as the dictionary to code the query 
face image, and c lassified the query face image by evaluating 
which class leads to the minimal reconstruction error.  But t he 
dictionary in his method  may not be effective enough to represent the query images due to the uncertain and noisy information in 
the original traini ng images.  In addition, t he number of atoms of 
dictionary that made up of image samples  can also be very large , 
which increases the coding complexity.  
Dictionary learning (DL) aims to learn from the training 
samples’  space w here the given signal could be w ell represented 
or coded for processing.  Most DL methods attempt to learn a 
common dictionary shared by all classes as well as a classifier of 
coefficients for classification . Mairal et al. [ 39] proposed a 
discriminative DL  method by training a classifier of the coding 
coefficients,  and verified their method for digit recognition and 
texture  classification. Pham et al. [28] proposed a joint learning  
and dictionary construction method with consideration of  the 
linear classifier performance and applied their method  to object 
categorization and face recognition . Based on [ 38], Zhang et al.  
[37] proposed an algorithm called discriminative KSVD for face 
recognition.  However, the shared dictionary loses the 
correspond ing relation  between the dictionary atoms and t he class 
labels . Then , the classification results based on reconstruction 
error cannot be gained since this process is closely associated 
with the class labels. Yang et al. [ 30] learned a dictionary for 
each class and obtained better face r ecognition  resul ts than SRC.  
Then Yang et al [ 31] further employ ed the Fisher discrimination 
criterion  to learn a structured dictionary.  
The difference between CDDL and traditional DL methods is 
that we use the supervised dictionary learning  technique to solve 
the binary -class classification problem for software  defect 
prediction  application . In addition , we design a cost-sensitive 
supervised  dictionary learning solution , which  takes into account 
the problem  that misclassification cost  of binary -class modules is 
distinctly  different . 
3. OUR APPROACH  
3.1 Brief Introduction of SRC  
Wright et al.  [49] presented  the sparse representation based 
classification (SRC) method that regards a test ing sample as a 
linear combination of training samples. Suppose that we have 
c  
classes of training samples, 
 12, ,...,mn
c A A A A R  denotes 
the set of training samples, 
,1 ,2 ,n, ,...,i
imn
i i i iA s s s R  
denotes  the subset of training samples from class 
i , 
,m
ijsR  
denotes  the 
thj  sample of class 
i , and 
y  denotes  a testing 
sample . The procedure of SRC is as follows :  
1. Sparsely code 
y  over 
A via 
1l -norm minimization  
 2
21ˆarg min + yA
  
     γ
.                (1) 
2. Do classification by using  
 identity arg miniiye
,                      (2) 
where 
2iiie y A  , 
12, ,...,T
ic    , and 
i is the 
coefficient vector associated with class 
i . Finally , the test 
sample
y  is assigned to the 
thi  class  corresponding to the 
smallest  reconstruction error  
ie. 416 3.2 Cost -sensitive Discriminative D ictionary 
Learning  based Defect Prediction  
To fully exploit the discriminative information of training 
samples for improv ing the performance of classification , we 
design a  supervised dictionary learning approach , which le arns a 
dictionary that can represent  the given  software module more 
effectively . Moreover, the supervised dictionary learnin g can also 
reduce both the number of dictionary atoms  and the sparse  coding 
complexity . Instead of learning a shared dictionary for all classes, 
we learn a structured dictionary  
 1,..., ,...,ic D D D D , where 
iD
 is the class -specified sub -dictionary associated with class 
i , 
and 
c  is the total number of cla sses. We use the reconstruction 
error to do classification with such a  dictionary  
D, as the SRC 
method  does. 
Suppose that 
 1,..., ,...,ic A A A A  is the set of training 
samples  (labeled software modules) , 
iA  is the subset of the 
training samples from class 
i , 
 1,..., ,...,ic X X X X  is the 
coding coefficient matrix of 
A  over 
D , that is,  
A DX , 
where 
iX  is the sub -matrix containing the coding coefficients of 
iA
 over 
D . We requir e that 
D  should have not only powerful 
reconstruction capability of 
A  but also powerful discriminative 
capability of classe s in 
A . Thus , we propose the cost-sensitive 
discriminative dictionary learning  (CDDL ) model as follow s: 
  ( , ) 1( , )arg min ( , , )DX
DXJ r A D X X  
,                (3) 
where  
 ,,r A D X  is the discriminative fidelity term; 
1X is 
the sparsity constraint; and 
  is a balance factor.  
Let 
12, ,...,c
i i i iX X X X , where 
j
iX  is the coding 
coefficient  matrix  of 
iA  over the sub -dictionary  
jD. Denote the 
representation of  
kD  to 
iA  as 
k
k k iR D X . First of all, the 
dictionary 
D  should be able to well represent  
iA, and therefore  
1
1 ... ...ic
i i i i i c iA DX D X D X D X     
. Secondly, since 
iD 
is associated with the 
thi  class, it is expected t hat 
iA should be 
well represented by 
iD  (not by 
jD , 
ji ), which means both 
2i
i i iFA D X
 and 
2j
jiFDX  should be minimized. Thus the 
discriminative fidelity term  is 

1
22 2
11,, ,,c
ii
i
cc
ij
i i i i i j i F FFij
jir A D X
A DX A D X D Xr A D X



    

, (4) 
An intuitive explanation of three terms in 
( , , )iir A D X  is 
shown in Fig. 2. In software defect prediction , there are two kinds 
of modules:  the defective modules  and the defect ive-free modules.  
Fig. 2 (a) shows that if we only minimize the 
2
ii FA DX  on the 
total dictionary 
D , 
iR may deviate much from 
iA  so that sub-
dictionary  
iD could not well represent 
iA . In order to achieve better powerful reconstruction capability and powerful 
discriminative capability , we add another two parts  that 
2i
i i iFA D X
 (which minimizes the reconstruction error  on sub -
dictionary  of its own class ) and 
2j
jiFDX  (which minimizes the 
reconstruction term on sub -dictionary  of the other class ), both of 
them should also be minimized.  Fig. 2(b) shows that the 
proposed discriminative fidelity term could overcome the 
problem in Fig. 2(a).  
iA
iR
jR
iA
iR
jR(a) (b)
 
Fig. 2: Illustration of the discriminative fidelity term.  
As previously stated, misclassify ing defect ive-free module s 
leads  to increasing  the development cost , and  misclassif ying 
defect ive ones is related  with risk cost. Cost-sensitive learning 
can incorporat e the different misclassification costs into the 
classification process.  In this paper, we emphasize the risk cost  
such that we  add the penalty factor 
( , )cost i j  to increase the 
punishment when a defective software module is predict ed as a 
defect ive-free software module. As a result, cost -sensitive 
dictionary learning makes the p rediction incline to classify a 
module as a defective one, and generates a dictionary for 
classification  with minimum misclassification cost. The 
discriminative fidelity term with penalty factor s is 

1
22 2
11, ,
,, ,c
ii
i
cc
ij
i i i i i j i F FFijr A D X
A DX A D X cost i j DrA
XDX

    


,   (5) 
Since there are only t wo classes in software defect prediction (the 
defective class and the defect ive-free class) , that is,  
2c , the 
model of cost -sensitive discriminative dictionary learning is  
22 2
( , )
( , ) 1
22
1
1arg min
,i
D X i i i i i F FDX i
j
jiFjJ A DX A D X
cost i j D X X 
   
  

,  (6) 
where the cost matrix is shown in Table 1.  
 
Table 1. Cost matrix for CDDL  
 Predicts defect ive 
one Predicts defect ive-free 
one 
Actually 
defect ive 0 
1,2 cost  
Actually 
defect ive-free 
2,1 cost 0 417 3.3 Optimization of CDDL A lgorithm  
The CDDL objective function in Formula (6) can be divided 
into two sub -problems: updating 
X  by fixing 
D ; and updating 
D
 by fixing 
X . The optimizati on proc edure  is iteratively 
implemented for the desired discriminative dictionary 
D  and 
corresponding coefficient matri x 
X. 
At first, suppose that 
D  is fixed, the objective funct ion in 
Formula (6)  is reduced to a sparse coding problem to compute 
12, X X X
. Here 
1X  and 
2X  are calculated  one by one.  
 
We c alculate
1X with fixed 
2X , and then compute 
2X  with 
fixed 
1X . Thus, Formula (6) is rewritten  as 

2 2
()
()
22
1
1arg min
,i
ii
X i i i i i F FX
j
j i iFjJ A DX A D X
cost i j D X X 
   
  

.     (7) 
Formula  (7) can be solved by  using  the IPM algorithm in [ 50]. 
When X is fixed, we in turn update  
1D and 
2D . When we 
calculate 
1D , 
2D  is fixed, then we compute 
2D , 
1D  is fixed . 
Thus Formula (6 ) is rewritten as  
2
2
()
() 1
222
1arg min
,i
iij
D i j
D j
jiF
ij
i i i j iFFjJ A D X D X
A D X cost i j D X


  


   

, (8) 
where 
iX  is the coding coefficient matrix of 
A  over 
iD . 
Formula ( 8) is a quadratic programming problem, and we can 
solve it by using the algorithm in [ 34].  
Table 2 shows the realization  algorithm of CDDL.  We 
initialize the sub -dictionary of each class by using the PCA 
technique . Since the data dimension of software defect prediction 
is low, PCA would construct a complete initialization  sub-
dictionar y for each class , that is,  each sub -dictionar y has the same 
number of atoms (Generally the number of sub -dictionary’s atoms 
is equal to  the data dimension) .  
The algorithm of CDDL converges since its two alternative 
optimizations are both con vex. Fig. 3 illustrates the convergence 
of the algorithm.  
 
 
 
 
 
 
  
  
       (a) CM1 dataset .                                                            (b) KC1 dataset . 
    
  
                                                    (c) MW1 dataset .                                                                   (d) PC1 dataset . 
Fig. 3 : Convergence of the realization algorithm of CDDL on four NASA benchmark datasets.  418 Table 2. Algorithm of CDDL  
Step 1.  Initialize 
D . 
Initialize each atom of sub-dictionary  
iD  by 
using the PCA method.  
Step 2.  Update the sparse codin g coefficient matrix  
X. 
Fix 
D  and solve 
1X  and 
2X  one by one by 
using Formula  (7). 
Step 3.  Update dictionary 
D .  
Fix 
X  and update  
1D  and 
2D  by using 
Formula  (8). 
Step 4.  Output 
D . 
Return to Step 2 until the values of 
,DXJ  in 
adjacent iterations are close enough,  or the 
maximum number of iterations is reached.  
Step 5.  Classification prediction with SRC.  
4. EXPERIMENTAL SETUP  
In this section, we describe the experimental setup in  detail, 
including  benchmark database,  evaluation measures, and 
experiment design.   
4.1 Benc hmark Datasets   
In the experiment, ten datasets from NASA Metrics Data  
Program (MDP) are taken as the test data. NASA benchmark 
datasets are publicly available and have been widely used for 
software defect prediction. Each dataset  represents a NASA 
softwar e system or sub -system , which contains the corresponding 
defect -marking data and various static code metrics.  The 
repository  record s the number of  defects for each module by using  
a bug tracking  system.  Static code metrics of NASA datasets 
include size, re adability, complexity and etc., which are closely 
related to software quality. Here, three mentioned metrics above 
are measured by lines of code (LOC) counts, operand and 
operator counts (Halstead attributes), and McCabe complexity 
measures, respectively. For brevity,  we list the 20 common basic 
metrics selected from 40 metrics of ten NASA datasets, such as 
loc, lOCode, lOComment, lOBlank and etc. And their 
descriptions are given in Table  3. More detailed description of 
code metrics or information about the  NASA datasets can be 
obtained from [56].  
Brief properties  of ten NASA datasets are shown in Table 4. 
Among the ten datasets, the size of each one (the number of 
modules  of software)  ranges from  127 to 17001,  while the 
number of attributes range s from 20 to  40. 
4.2 Evaluation Measures  
There are four measures to evaluat e the performance of  defect 
prediction model: recall rate  (i.e. probability of detection) , false 
positive rate, precision and accuracy , which can be defined by 
using 
A , 
B, 
C, and 
D  in Table 5 . Here, 
A , 
B, 
C, and 
D  
are the number of defective modules that are predicted as 
defective , the number of defective modules that are predicted as 
defective -free, the number of defective -free modules that are predicted as defective , and the number of defective -free modules 
that are predicted as defective -free, respectively.  
Table 3.  20 metrics selected from NASA datasets  
Metrics  Description  
loc McCabe ’s line count of code for each module  
lOCode  Halstead ’s line count of code for each module  
lOComment  Halstead ’s count of lines of co mments for each 
module  
lOBlank  Halstead ’s count of blank lines for each 
module  
lOCodeAnd
Comment  Halstead ’s count of code and comments for 
each module  
v(g) Cyclomatic complexity for each module  
ev(g)  Essential complexity for each module  
iv(g)  Design co mplexity for each module  
Total_Op  Total number of operators for each module  
Total_Opnd  Total number of operands for each module  
Uniq_Op  Number of u nique operators  for each module  
Uniq_Opnd  Number of u nique operands  for each module  
n Total operators  and operands for each module  
v Volume for each module  
l Program length  for each module  
d Difficulty for each module  
i Intell igent content  for each module  
b Error estimate  for each module  
e Programming effort for each module  
t Programming time for e ach module  
 
Table 4. NASA benchmark  datasets  
dataset
s Number  of 
defective 
module  Number 
of total 
module
s Number 
of 
attribute
s Percentage 
of defective 
modules  
CM1  42 344 38 12.21%  
JM1 1759  9593  22 18.34%  
KC1 325 2096  22 15.51%  
KC3 36 200 40 18.00%  
MC2  44 127 40 34.65%  
MW1  27 264 20 10.23%  
PC1 61 759 20 8.04%  
PC3 140 1125  20 12.44%  
PC4 178 1399  38 12.72%  
PC5 503 17001  39 2.96%  
 
Table 5.  Defect prediction metric  
 Predict  as defect ive Predict  as defect ive-
free 
Defect ive 
modules  
A 
B  
Defect ive-free 
modules  
C 
D  
 
 419 Recall  rate: The ratio is the number of defective modules  
correctly classified as defect ive to the number of defect ive 
modules , which is defined as
/ ( )A A B . This ratio  is very 
important  for software defect prediction, because 
prediction  model intend s to find out  defective modules  as much 
as possible.  
False positive rate: The ratio is the number of defective -free 
modules  wrongly classified as defe ctive to the number of 
defective -free modules , which is defined as  
/ ( )C C D .  
Precision: The ratio is the number of defective modules  
correctly classified as defect to the number of modules  that are 
classified as defect ive, which is defined as
/ ( )A A C . 
This ratio evaluates the correct  degree of  prediction model.  
Accuracy: The ratio is the number of modules  that are correctly 
classified to the number of total modules , which is  defined as  
( ) / ( )A D A B C D   
. The ratio is widely used in all  data 
mining classification applications.  
A good  prediction model desires  to achieve  high value  of recall  
rate and precision. However, there exists  trade -off between 
precision and recall. Then  a comprehensive  measure of precision  
and recall rate is necessary . F-measure  is the harmonic mean of 
precision and recall  rate, which can be defined as:  
  2* * /   F measure recall precision recall precision  
 
All the above evaluation measures  range from 0 to 1 . 
Obviously, t he performance is better  with higher values  of recall, 
precision, F -measure and accuracy , and lower f alse positive rate  
is desired.  
In the experiment, we evaluate the performance of all the 
methods in terms of false positive  (Pf), recall (Pd) and F -measure.  
4.3 Experimental Design   
To evaluate our CDDL a pproach,  we conduct some 
experiments. For all selected datasets , we use the 1:1 random 
division to obtain  the training and testing sets for all compared 
methods. The random division treatment may affect the 
prediction performance. Therefore, we use the ran dom division 
and perform prediction 20 times, and report the average  
prediction results in Section 5.  
In our approach, in order to  emphasize the risk cost, the 
parameters 
1,2 cost  and 
2,1 cost  are set as 
 :11,2 , : 21 5 cost cost 
. For different project s, user can select 
different ratio [ 58].  And the parameter 
  is determined by 
searching a wide range of values and choos ing the one that yields 
the best F -measure value.  
5. EXPERIMENTAL RESULTS  
We compare the proposed CDDL approach with several 
representative methods, particularly presented  in the latest five 
years,  including support vector machine  (SVM)  [5], Compressed 
C4.5 decision tree (CC4.5)  [8], weighted Naï ve Bayes (NB)  [14], 
coding based ense mble learning (CEL)  [27], and  cost-sensitive 
boosting  neural network (CBNN)  [18]. In this section,  we present 
the detailed experimental results of our CDDL approach and 
other compared methods.  
 
 
 Table 6.  Experimental results: Pd and Pf comparisons on 
NASA’s ten datasets   
Data  
set M SVM  CC4.5  NB CEL  CBNN  CDDL  
CM1  Pd 0.15 0.26 0.44 0.43 0.59 0.74 
Pf 0.04 0.11 0.18 0.15 0.29 0.37 
JM1 Pd 0.53 0.37 0.14 0.32 0.54 0.68 
Pf 0.45 0.17 0.32 0.14 0.29 0.35 
KC1 Pd 0.19 0.40 0.31 0.37 0.69 0.81 
Pf 0.02 0.12 0.06 0.13 0.30 0.37 
KC3 Pd 0.33 0.41 0.46 0.29 0.51 0.71 
Pf 0.08 0.16 0.21 0.12 0.25 0.34 
MC2  Pd 0.51 0.64 0.35 0.56 0.79 0.83 
Pf 0.24 0.49 0.09 0.38 0.54 0.29 
MW1 Pd 0.21 0.29 0.49 0.25 0.61 0.79 
Pf 0.04 0.09 0.19 0.11 0.25 0.25 
PC1 Pd 0.66 0.38 0.36 0.46 0.54 0.86 
Pf 0.19 0.09 0.11 0.13 0.17 0.29 
PC3 Pd 0.64 0.34 0.28 0.41 0.65 0.77 
Pf 0.41 0.08 0.09 0.13 0.25 0.28 
PC4 Pd 0.72 0.49 0.39 0.48 0.66 0.89 
Pf 0.16 0.07 0.13 0.06 0.18 0.28 
PC5 Pd 0.71 0.50 0.32 0.37 0.79 0.84 
Pf 0.22 0.02 0.14 0.13 0.08 0.06 
Table 6 shows the Pd and P f values of our approach and other 
compared methods on 10 NASA datasets . For each dataset , Pd 
and Pf value s of all methods are the mean value s calculated  from 
the results of 20 runs. The results of Pf value s sugges t that in 
spite of not acquiring the best Pf values on  most datasets, CDDL 
can achieve comparatively  better results in contrast with other  
methods. We  can also observe that the Pd value s of CDDL , which 
are presented with boldface , are higher  than the corre sponding  
values of all other  methods. CDDL achieves the high est Pd 
values  on all datasets . The results indicate that the proposed 
CDDL approach takes the misclassification cost s into 
consideration, which makes the prediction tend to classify the 
defective -free modules as the defective ones in order to obtain 
higher Pd value s.  
We calculate  the average Pd va lues of 10 NASA datasets  in 
Table 7. As compared with other methods , the average Pd va lue 
of our approach  is higher in contrast with other related method s, 
and CDDL improves the average Pd value  at least by 0.15 
(=0.79-0.64).  
Table 7. Average  Pd value of 10 NASA datasets  
 SVM  CC4.5  NB CEL CBNN  CDDL  
average  0.47 0.41 0.35 0.39 0.64 0.79 
 
Table 8 shows the F -measure values of our approach and the 
compared methods on 10 NASA datasets . In Table 8, F-measure 
values  of CDDL  are better than other methods on all datasets , 
which means that our proposed approach outperforms other 
methods and achieves the ideal prediction effects . According to 
the average F -measure values shown in Table 8, CDDL improves  
the average F-measure value  at least by (0. 47-0.39=0.08 ). 
To sum up, Table 7  and 8 show that o ur approach has the best 
achievement in the P d and F-measure  values.  
 
 420 Table 8.  F-measure  values  on ten NASA datasets  
datasets  SVM  CC4.5  NB CEL CBNN  CDDL  
CM1  0.20 0.25 0.32 0.27 0.33 0.38 
JM1 0.29 0.34 0.33 0.33 0.38 0.40 
KC1 0.29 0.39 0.38 0.36 0.41 0.47 
KC3 0.38 0.38 0.38 0.33 0.38 0.44 
MC2  0.52 0.48 0.45 0.49 0.56 0.63 
MW1  0.27 0.27 0.31 0.27 0.33 0.38 
PC1 0.35 0.32 0.28 0.32 0.32 0.41 
PC3 0.28 0.29 0.29 0.36 0.38 0.42 
PC4 0.47 0.49 0.36 0.48 0.46 0.55 
PC5 0.16 0.48 0.33 0.36 0.37 0.59 
Average  0.32 0.37 0.34 0.35 0.39 0.47 
 
To statistically analyze the F-measure  results given in  Table 8, 
we conduct a statistical t est, i.e., Mcnemar ’s test [55]. This test 
can provide statistical significance between  CDDL and other 
methods. Here, the Mcnemar ’s test uses a significance level of 
0.05. If the p-value is below 0.05, the performance difference 
between two compared methods  is considered to be statistically 
significant. Table 9 shows the p -values between  CDDL and other 
compared methods  on 10 NASA datasets, where only one value is 
slightly above 0.05 . According to Table 9, the proposed  approach 
indeed makes a significant diff erence in comparison with  other 
methods for software defect prediction.  
Table 9.  P-values between CDDL and other compared 
methods on ten NASA datasets  
dataset
s  CDDL  
SVM  CC4.5  NB CEL CBNN  
CM1  1.23× 10-8 3.51×10-6 4.24×10-4 1.80×10-4 1.01×10-4 
JM1 7.51×10-18 2.33× 10-13 1.27× 10-14 1.58× 10-13 0.0564  
KC1 1.20× 10-14 1.23×10-9 8.38× 10-13 2.80× 10-11 9.69×10-6 
KC3 0.0265  0.0089  3.22× 10-4 1.61× 10-4 4.24×10-4 
MC2  1.26× 10-4 2.61×10-5 1.13× 10-8 7.58× 10-6 1.01×10-4 
MW1  1.14× 10-3 2.31×10-4 1.10× 10-3 1.84× 10-5 2.20×10-3 
PC1 2.64× 10-4 2.41×10-5 1.60× 10-8 1.69× 10-5 1.68×10-8 
PC3 7.79× 10-14 7.73×10-9 1.04× 10-8 4.03× 10-5 4.31×10-5 
PC4 7.32×10-8 7.26×10-4 2.81×10-16 4.26× 10-6 1.75×10-10 
PC5 3.01× 10-18 7.00× 10-9 1.90× 10-14 1.30× 10-12 2.13× 10-11 
6. CONCLUSION  
Although d ictionary learning has been shown effective in other 
domains, to the best of our knowledge, we are the first attempt 
towards  improv ing software prediction performance  by 
introducing the  dictionary learning technique. Aiming at the 
characteristic s of defect  data, we specifically design  a supervised  
cost-sensitive dictionary  learning  approach  to predict defect ive 
module s, i.e., cost-sensitive discriminative dictionary learning 
(CDDL). It can fully exploit the class information of historical 
data to improve th e discriminative power  and provides effective 
solutions for the major problems in the field of software defect 
prediction, which are misclassification cost  and class -imbalance 
problem s.  
As compared with several state-of-the-art representative 
software def ect prediction methods, the experiments on ten 
NASA dataset s show that the proposed CDDL  approach performs 
better. It  significantly improves both the average Pd value s and F -
measure value s on all dataset s. The Mcnemar ’s test  experiment shows  that the diffe rences between CDDL  and the compared 
methods are statistically significant . Experimental results 
demonstrate the effectiveness of our approach for the software 
defect prediction task. 
Software defect prediction techniques are mainly based on a 
sufficient a mount of historical project data. However, labeling  
software modules  is time -consuming . In practical scenario , 
software project  exits many unlabeled software modules . In order 
to fully exploit the  both labeled and unlabeled  information of 
software modules , we plan to expand  discriminative  dictionary 
learning to semi -supervised dictionary learning . 
7. ACKNOWLEDGEMENTS  
The work described in this paper was partially supported by  
the NSFC under Project No s. 61272273 , 61233011 , 61303114 , 
61373038 , 61103125 , 6107001 3, U1135005 , the 111 Programme 
of Introducing Talents of Discipline to Universities  under G rant 
No. B07037 , Great Creative Plan of Hubei Province under 
Project No. 2013AAA020 , 333 Engineering of Jiangsu Province 
under Project No.  BRA2011175 . 
8. REFERENCES   
[1] M.R. Lyu,  “Software Reliability Engineering: A Roadmap,”  
Future of Software Engineering , IEEE Computer Society , pp. 
153-170, 2007 .  
[2] J. Nam, S.J. Pany, S. Kim, “Transfer Defect Learning,” Int. 
Conf. Software Engineering , pp. 382 -391, 2013.  
[3] C. Catal,  B. Diri,  “A systematic review of software fault 
prediction studies,”  Expert Systems with Applications,  vol. 
36, pp. 7346 -7354,  2009 . 
[4] T. Hall,  S. Beecham,  D. Bowes,  D. gray, S. Counsell , “A 
Systematic Literature Review on Fault Prediction 
Performance in Software Eng ineering,”  IEEE Trans. 
Software Engineering , vol. 38, no. 6, pp. 1276 -1304 , 2011.  
[5] K. Elish,  M. Elish,  “Predicting Defect -prone Software 
Modules Using Support Vector Machines,” Journal Systems 
and Software , vol. 81,  no. 5,  pp. 649 -660, 2008.  
[6] D. Gray,  D. B owes,  N. Davey,  Y. Sun,  B. Christianson, 
“Using the support vector machine as a classi ﬁcation method 
for software defect prediction with static code metrics,”  
Engineering Applications of Neural Networks , vol. 43,  pp. 
223-234, 2009.  
[7] Z. Yan,  X.Y. Chen,  P. Gu o, “Software Defect Prediction 
Using Fuzzy Support Vector Regression,”  Advances in 
Neural Networks , pp. 17 -24, 2010.  
[8] J. Wang,  B.J. Shen,  Y.T. Chen,  “Compressed C4.5 Models 
for Software Defect Prediction,”  Int. Conf. Quality Software , 
pp.13 -16, 2012.  
[9] T.M. Khoshgoftaar,  N. Seliya,  “Tree -based software quality 
esti-mation models for fault prediction,”  IEEE Symp. 
Software Metrics , pp. 203 -214, 2002.  
[10] L. Breiman,  “Random forests,” Machine Learning , vol. 45,  
no. 1,  pp. 5 -32, 2001.  
[11] N. Gayatri, S. Nickolas,  A.V. Re ddy, “Feature Selection 
Using Decision Tree Induction in Class level Metrics 
Dataset for Software Defect Predictions,” The World 
Congress on Engineering and Computer Science , pp. 124 -
129, 2010.  
[12] M.M.T. Thwin,  T.S. Quah,  “Application of neural networks 
for s oftware quality prediction using object -oriented 421 metrics,” Journal of Systems and Software , vol. 76,  no. 2,  pp. 
147-156, 2005.   
[13] E. Paikari,  M.M. Richter,  G. Ruhe,  “Defect Prediction 
Using Case -Based Reasoning: An Attribute Weighting 
Technique Based Upon Se nsitivity Analysis In Neural 
Networks, ” Int. Journal of Software Engineering and 
Knowledge Engineering , vol. 22,  no. 5, 2012.  
[14] T. Wang , W.H. Li, “Naïve Bayes Software Defect Prediction 
Model,”  Int. Conf. Computational Intelligence and Software 
Engineering , pp. 1 -4, 2010.  
[15] S. Amasaki,  Y. Takagi,  O. Mizuno,  T. Kikuno,  “A Bayesian 
Belief Network for Assessing the Likelihood of Fault 
Content,”  Int. Symp. Software Reliability Engineering , pp. 
215-226, 2003.   
[16] B. Turhan,  A. Bener,  “Software Defect Prediction: 
Heuri stics for Weighted Naïve Bayes,” Int. Conf. Software 
and Data Technologies , pp. 244 -249, 2007.  
[17] B. Turhan,  A. Bener,  “Analysis of naïve bayes’ assumptions 
on software fault data: An empirical study,” Data Knowledge 
Engineering , vol. 68,  no. 2,  pp. 278 -290, 2009.  
[18] J. Zheng,  “Cost -sensitive boosting neural networks for 
software defect prediction,” Expert Systems With 
Applications , vol. 37,  no. 6,  pp. 4537 -4543,  2010.  
[19] Bezerra,  E. Miguel,  A.L.I. Oliveiray,  P.J.L. Adeodatoz,  
“Predicting software defects: A cost -sensitive approach,”  Int. 
Conf. Systems, Man, and Cybernetics , pp. 2515 -2522,  2011 . 
[20] N. Seliya,  T.M. Khoshgoftaar,  “The use of decision trees for 
cost-sensitive classification an empirical study in software 
quality prediction,” Wiley Interdisciplinary Reviews : Data 
Mining and Knowledge Discovery , vol. 1, no. 5,  pp. 448 -459, 
2011.  
[21] P.D. Turney,  “Types of cost in inductive concept learning,”  
Int. Conf. Machine Learning , pp. 15 -21, 2000.  
[22] P. Domingos,  “MetaCost : A general method for making 
classifiers cost -sensitiv e,” Int. Conf. Knowledge Discovery 
and Data Mining , pp. 155 -164, 1999.  
[23] T. Menzies,  J. Greenwald,  A. Frank,  “Data  mining  static 
code at -tributes to learn defect predictors,” IEEE Trans. 
Software Engineering , vol. 33,  no. 1,  pp. 2 -13, 2007.  
[24] H. He,  E.A. Garci a, “Learning from imbalanced data,” IEEE 
Trans. Knowledge Data Engineering , vol. 21,  no. 9,  pp. 
1263 -1284,  2009.  
[25] Z.H. Zhou,  X.Y. Liu,  “Training cost -sensitive neural 
networks with methods addressing the class imbalance 
problem,” IEEE Trans. Knowledge and D ata Engineering , 
vol. 18,  no. 1,  pp. 63 -77, 2006.  
[26] M.T. Khoshgoftaar, K. Gao,  N.Seliya,  “Attribute Selection 
and Imbalanced Data: Problems in Software Defect 
Prediction,”  Int. Conf. Tools with Artificial Intelligence , pp. 
137-144, 2010.  
[27] Z.B. Sun, Q.B. Song,  X.Y. Zhu,  “Using Coding Based 
Ensemble Learning to Improve Software Defect Prediction,” 
IEEE Trans. Systems,  Man,  and Cybernetics,  Part C , vol. 42,  
no. 6,  pp. 1806 -1817,  2012.  
[28] K. Gao,  T.M. Khoshgoftaar,  A. Napolitano,  “A Hybrid 
Approach to Coping with Hig h Dimensionality and Class 
Imbalance for Software Defect Prediction,” Machine 
Learning and Applications , vol. 2,  pp. 281 -288, 2012.  
[29] S. Wang,  X. Yao,  “Using Class Imbalance Learning for 
Software Defect Prediction,”  IEEE Trans. Reliability , vol. 
62, no. 2,  pp. 434 -443, 2013.  [30] S.G. Mallat,  Z.F. Zhang,  “Matching pursuits with time -
frequency dictionaries,”  IEEE Trans. Signal Processing , vol. 
41, no. 12,  pp. 3397 -3415,  1993.  
[31] M. Aharon,  M. Elad,  A. Bruckstein,  “K-SVD: An Algorithm 
for Designing Over -complete Dictio naries for Sparse 
Representation,” IEEE Trans. Signal Processing , vol. 54,  no. 
11, pp. 4311 -4322,  2006.  
[32] J. Mairal,  F. Bach,  J. Ponce, G . Sapiro, “Online learning for 
matrix factorization and sparse coding,”  Journal of Machine 
Learning Research , vol. 11,  pp. 19-60, 2010.  
[33] K. Skretting,  K. Engan,  “Recursive Least Squares Dictionary 
Learning Algorithm,” IEEE Trans. Signal Processing , vol. 
58, no. 4,  pp. 2121 -2130,  2010.  
[34] M. Yang,  L. Zhang,  J. Yang,  D. Zhang , “Metaface Learning 
for Sparse Representation based Fac e Recognition,”  Int. 
Conf. Image Processing , pp. 1601 -1604,  2010.  
[35] Z.L. Jiang,  Z. Lin,  L.S. Davis,  “Learning a Discriminative 
Dictionary for Sparse Coding via Label Consistent K -SVD,”  
IEEE Computer Society Conf . Computer Vision and Pattern 
Recognition , pp. 1697 -1704,  2011.  
[36] R. Cristian,  D. Bogdan,  “Stagewise K -SVD to Design 
Efficient Dictionaries for Sparse Representations,”  IEEE 
Signal Processing Letters , vol. 19,  no. 10,  pp. 631 -634, 2012.  
[37] Q. Zhang,  B.X. Li,  “Discriminative K -SVD for Dictionary 
Learning in Face Recognitio n,” IEEE Computer Society 
Conf . Computer Vision and Pattern Recognition , pp. 2691 -
2698,  2010.  
[38] D. Pham,  S. Venkatesh,  “Joint learning and dictionary 
construction for pattern recognition,”  IEEE Conf. Computer 
Vision and Pattern Recognition , pp. 1-8, 2008 . 
[39] J. Mairal,  F. Bach,  J. Ponce,  G. Sapiro,  A. Zisserman,  
“Supervised dictionary learning,”  IEEE Conf. Computer 
Vision and Pattern Recognition , 2008.  
[40] T.J. McCabe,  “A complexity measure,”  IEEE Trans. 
Software Engineering , vol. 4,  pp. 308 -320, 1976 . 
[41] M.H. Halstead,  “Elements of Software Science (Operating 
and programming systems series),” New York:  Elsevier 
North -Holland , 1977.  
[42] Q.B. Song,  Z.H. Jia,  M. Shepperd, S. Ying, J. Liu,“ A 
General Software Defect -Proneness Prediction Framework,” 
IEEE Trans. S oftware Engineering , vol. 37,  no. 3,  pp. 356 -
370, 2011.  
[43] T.G. Dietterich,  “Ensemble Methods in Machine Learning, ”  
Multiple Classier Systems , pp. 1-15, 2000.  
[44] G. Valentini,  F. Masulli,  “Ensembles of learning machines,”  
Neural Networks , pp. 3-20, 2002.  
[45] I. Ram í rez, P. Sprechmann,  G. Sapiro,  “Classification and 
Clustering via Dictionary  Learning with Structured 
Incoherence and Shared Features,”  IEEE Computer Society 
Conf . Computer Vision and Pattern Recognition , pp. 3501 -
3508,  2010.  
[46] B.M,  Mark. D. Plumbley,  “Dict ionary Learning with Large 
Step Gradient Descent for Sparse Representations ,” Int. 
Conf. Latent Variable Analysis and Signal Separation , pp. 
231-238, 2012.  
[47] M. Yang,  L. Zhang,  X. Feng,  D. Zhang , “Fisher 
discrimination dictionary learning for sparse represen tation,”  
Int. Conf. Computer Vision , pp. 543 -550, 2011.  
[48] J.C. Yang,  J.P. Wang,  H.S. Thomas,  “Learning the Sparse 
Representation for Classification,”  Int. Conf. Multimedia 
and Expo , pp. 1 -6, 2011.  422 [49] J. Wright,  A.Y. Yang,  A. Ganesh,  S.S. Sastry,  Y. Ma,  
“Robust Face Recognition via Sparse Representation,” IEEE 
Trans. Pattern Analysis and Machine Intelligence , vol. 31,  
no. 2,  pp. 210 –227, 2009.  
[50] L. Rosasco,  A. Verri,  M. Santoro,  S. Mosci,  S. Villa,  
“Iterative Projection Methods for Structured Sparsity 
Regularizatio n,” MIT Technical Reports,  MIT-CSAIL -TR-
2009 -050, CBCL -282, 2009.  
[51] T. Lee, J. Nam, D. Han, S. Kim, I.P. Hoh, “Micro 
interaction metrics for defect prediction,” European 
Software Engineering Conf. the Foundations of Software 
Engineering , pp. 311 -321, 2011.  
[52] R. Moser, W. Pedrycz, G. Succi, “A comparative analysis of 
the efficiency of change metrics and static code attributes for 
defect prediction,” Int. Conf. Software Engineering , pp. 181 -
190, 2008.  
[53] T.J. Ostrand, E.J. Weyuker, R.M. Bell, “Predicting the 
locatio n and number of faults in large software systems,”  
IEEE Trans. Software Engineering , vol. 31, no. 4, pp. 340 -
355, 2005.  
[54] H. Zhang, “An investigation of the relationships between 
lines of code and defects,” Int. Conf. Software Maintenance , 
pp. 274 -283, 2009.  [55] W.S. Yambor, B.A. Draper, J.R. Beveridge, “Analyzing 
PCA -based Face Recognition Algorithms: Eigenvector 
Selection and Distance Measures,” Workshop on Empirical 
Evaluation in Computer Vision, 2000.  
[56] T. Menzies, J. Greenwald, A. Frank, “Data Mining Static 
Code Attributes to Learn Defect Predictors,” IEEE Trans. 
Software Engineering , vol. 33, no. 1, pp. 2 -13, 2007.  
[57] N. Seliya,  T.M. Khoshgoftaar,  J.V. Hulse , “Predicting faults 
in high assurance software,”  IEEE Int. High Assurance 
Systems Engineering Symposium , pp. 26 -34, 2010 . 
[58] Y. Jiang, B. Cukic, T. Menzies, “Cost curve evaluation of 
fault prediction models ,” IEEE Int. 19th International 
Symposium on Software Reliability Engineering , pp. 197-
206, 2008. 
[59]  S. Lessmann, B. Baesens, C. Mues, S. Pietsch, 
“Benchmarking classification models for software defect 
prediction: A proposed framework and novel findings,” 
IEEE Trans. Software Engineering , vol. 34, no. 4, pp. 485 -
496, 2008.  
 423