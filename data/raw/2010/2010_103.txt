It's Not a Bug, It's a Feature:  
How Misclassification Impacts Bug Prediction 
Kim Herzig,  
Microsoft Research 
Cambridge United Kingdom 
kimh@microsoft.com Sascha Just§, Andreas Zeller§ 
Saarland University 
Saarbrücken, Germany 
just@st.cs.uni-saarland.de 
zeller@cs.uni-saarland.de 
 
Abstract:  This submission presents work submitted and accepted at the 
International onference on Software Engineering in 2013 [Hj2013] . In empirical 
software engineering, it has become common to mine his toric dat a to detect where 
bugs have occurred in the past, or to predict where they will o ccur in the future. 
The accuracy of such models depends on the quality of the data.  F o r  e x a m p l e ,  
defect prediction models rely on the accuracy of hist oric data,  such as bug reports. 
Bug reports that refer to any other than corrective develop ment  activities may 
cause code artefacts to be falsely marked as defective . This ma y have severe 
consequences for the resulting models and its accurac y. Earlier  studies raised 
concerns about bug reports referring to error unrelated d evelop ment activities. But 
how often does such misclassification occur? Further, doe s it a ctually impact 
analysis and prediction models? These are the questi ons we addr essed in this 
paper. In a manual examination of more than 7,000 issue reports  from five open-
source projects, we found 33.8% of all bug reports to be miscla ssified threatening 
bug prediction models, confusing bugs and features: O n average,  39% of files 
marked as defective actually never had a bug. The pres entation will cover causes 
for issue report misclassification and the result of our study  (some newer results 
not in the paper). 
1 Talk Summary 
Empirical studies are threatened by the quality of data ana lyzed and interpreted. A 
commong task in empirical software engineering is to separate d efective from defect free 
code artifacts, e.g. to build defect prediction models, which relies on historic bug data. 
The majority of issue reports are classified as bugs— that is, requests for corrective code 
maintenance—and suggest that code changes resolving these issue s should be c onsidered 
as bug fixes and that the associtaed code artifacts shoul d be considered as defective. 
However, it remains unclear how reliable issue report clas sifications are. In 2008, 
Antoniol et al. [Aa2008] raised concerns about bug reports refe rring to error unrelated 
development activities. If such mix-ups (which mostly st em from  issue reporters and 
developers interpreting “bug” differently) occurred frequently and systematically they 
would introduce bias in data mining models threatening the external validity of any 
103study that builds on such data: Predicting the most e rror-prone  files, for instance, may 
actually yield files most prone to new features. But how o ften does such  
misclassification occur? And does it actually bias analysi s and  prediction? Our study 
targeted the following research questions: 
RQ1)  Do bug databases contain data noise due to issue report misclas sification, and 
how much? 
RQ2)  Which percentage of issue reports associated with a category wa s marked as 
misclassified? Which category do these misclassified reports a ctually belong to? 
RQ3)  What is the impact of misclassified issue reports when m apping issue reports to 
source code changes? 
RQ4)  How does bug mapping bias introduced by misclassified issue r eports impact 
the TOP 5%, 10%, 15%, 20% of most  defect prone source files? 
To answer these research questio ns, we manually inspected and r e-classified more than 
7,000 issue reports from five ope n-source Java projects develop ed by the Apache and 
Mozilla foundations (we will give more details about the cl assification process in the 
talk).  
Comparing the re-classified issue categories with the orgina l issue report type as stated 
in the bug database showed that over 40% of all issue report s in the analyzed bug 
databases were associate to inaccu rate issue report types (RQ1) . Concentrating on bug 
reports, we showed that over 33% of all bug reports are miscla ssified (RQ2). During the 
talk, we show details of the analysis and discuss sources o f misclassification, many of 
which refer to the fact that bug databases and bug reports provide communication 
platforms for different stakeholder, e.g. enginners and custome rs, wich have a very 
different perception of issues and a very different level of te chnical understanding.  
Estimating the impact of these miscalssifactions on ma ppings be tween actual code fixes 
and their changed code artifacts, we show that on average 39% of all files originally 
marked as defective actually never had a bug (RQ3). This impac t on file mapping 
threatens bug count and bug prediction models. In fact, we show  that when identifying 
the top 10% most defect-prone source files, 16% to 40% of these  files do not belong in 
this category because of issue report misclassification. 
The original published paper this talk is covering can be found  on the publisher’s 
website: http://dl.acm.org/citation.cfm?id=2486788.2486840 
References 
[Hj2013]  Herzig K.; Just S. und Zeller A. (2013): It's not a Bug, It's a  Feature: How 
Misclassification Impacts Bug Prediction. In: Proceedi ngs of the 2013 
International Conference on Software Engineering . IEEE Press. S. 392-- 401. 
 
104