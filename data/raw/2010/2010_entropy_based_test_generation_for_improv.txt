Entropy-Based Test Generation for
Improved Fault Localization
Jos´e Campos*Rui Abreu*Gordon Fraser†Marcelo d’Amorim‡
*Faculty of Engineering of University of Porto, Portugal
jose.carlos.campos@fe.up.pt, rui@computer.org,
†University of Shefﬁeld, United Kindom
gordon.fraser@shefﬁeld.ac.uk,
‡Federal University of Pernambuco, Brazil
damorim@cin.ufpe.br
Abstract —Spectrum-based Bayesian reasoning can effectively
rank candidate fault locations based on passing/failing test cases,
but the diagnostic quality highly depends on the size and diversity
of the underlying test suite. As test suites in practice often do not
exhibit the necessary properties, we present a technique to extend
existing test suites with new test cases that optimize the diagnostic
quality. We apply probability theory concepts to guide test case
generation using entropy, such that the amount of uncertainty
in the diagnostic ranking is minimized. Our ENTBUGprototype
extends the search-based test generation tool EVOSUITE to use
entropy in the ﬁtness function of its underlying genetic algorithm,
and we applied it to seven real faults. Empirical results show
that our approach reduces the entropy of the diagnostic ranking
by 49% on average (compared to using the original test suite),
leading to a 91% average reduction of diagnosis candidates
needed to inspect to ﬁnd the true faulty one.
Index Terms —Fault localization; Test case generation.
I. I NTRODUCTION
Programmers make mistakes, therefore testing and debug-
ging are inevitable parts of software development. Spectrum-
based fault localization is a popular automated approach to
assist programmers in debugging [5], [6], [21], [26], [30], [35].
It takes as input code coverage information for a given test
suite, and produces a list of statements ranked in order of fault
suspiciousness. Unfortunately, the technique is fundamentally
imprecise: it is possible that a tester will need to inspect several
suspicious statements in the rank to ﬁnd the faulty one. In fact,
a recent study [30] involving 68 developers failed to establish
the usefulness of the approach as it only showed improvement
for experienced programmers on simple code. Considering this
result, is spectrum-based fault localization a dead-end avenue
of research?
We conjecture that the answer is no. Research in spectrum-
based fault localization has continuously advanced over the
past few years. Many of its notorious initial limitations are no
longer a problem:
•it can identify multiple faults [2], [27], [37], [38];
•it can aggregate faults scattered across the code [2], [27],
[37], [38];
•it can quantify conﬁdence of the diagnosis [14], [21].C Subject: TriangleT new tests
t1t2t3t4t5t6t7t8t9
class Triangle {...
static int type(int a, int b, int c) {
c1int type = SCALENE; 1 1 1 1 0 0 1 1 1
c2if ( (a==b) && (b==c) ) 1 1 1 1 0 0 1 1 1
c3type = EQUILATERAL; 1 0 0 0 0 0 0 1 0
c4else if ( (a*a) == ((b*b)+(c*c)) ) 0 1 1 1 0 0 1 0 1
c5type = RIGHT; 0 0 1 0 0 0 1 0 0
c6else if ( (a==b) ||(b==a) ) /*FAULT*/ 0 1 0 1 0 0 1 0 1
c7type = ISOSCELES; 0 1 0 0 0 0 0 0 0
c8return type;} 1 1 1 1 0 0 1 1 1
static double area(int a, int b, int c) {
c9double s = (a+b+c)/2./zero.noslash; 0 0 0 0 1 1 1 1 1
c10return Math.sqrt(s*(s-a)*(s-b)*(s-c)); }...} 0 0 0 0 1 1 1 1 1
Test case outcome (pass=0, fail=1): 0 0 0 0 0 0 1 0 1
Number of fault candidates: 6 4 2 1
Figure 1: Triangle class adapted from [4] with tests and coverage
matrix; type classiﬁes triangles based on the side lengths, and area
calculates the area of the triangle. Test suite Tproduces an unusable
ranking, but adding new test cases t7,t8,t9gradually narrows down
the candidate statements until only c6remains.
Despite all these advances, there remains one fundamental
limitation, which we argue is one of the main confounding
factors for the usefulness of spectrum-based fault localization:
the dependency on the quality of the existing test suite [23].
Figure 1 shows a variation [4] of the well-known triangle
example. There is a fault at statement c6: method type declares
the predicate b==abut the correct condition should be
b==c. Assume we have received a bug report for the
Triangle class. Using B ARINEL [2] results in a ranking with
a very low conﬁdence score, and the tester would need to
inspect six statements ( c3,c5,c6,c7,c9,c10) prior to locating
the fault. Note that none of the existing tests ( t1–t6) fails;
even with a failing test the conﬁdence score might be low, and
thus the fault localization is unreliable and not helpful for the
developer.
To overcome this limitation, this paper proposes E NTBUG,
a novel technique to generate test cases speciﬁcally guided
by diagnostic accuracy. Based on our knowledge of how the
existing test cases cover the statements of the Triangle class,
ENTBUGgenerates a new test case t7, which is the test case
that improves the conﬁdence in the ranking the most over the
existing test suite; test case t7fails. Adding t7to the original
978-1-4799-0215-6/13 c/circlecopyrt2013 IEEE ASE 2013, Palo Alto, USA
Accepted for publication by IEEE. c/circlecopyrt2013 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/
republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this wor k in other works.257suite reduces the average number of statements for inspection
to four. Generating test case t8reduces the average number of
statements for inspection to two, and ﬁnally, adding test case
t9precisely pinpoints the faulty statement c6in the top of the
rank, with high conﬁdence.
To produce these new test cases automatically, our technique
borrows ideas from probability theory. Given a diagnostic
report, ranked by probability of diagnosis candidates being
the true fault explanation, the uncertainty in the ranking can
be quantiﬁed using entropy — a measure of uncertainty in a
random variable [25]. To increase the quality of the ranking,
we need test cases that maximize the diagnostic precision of
the underlying fault localization algorithm, this way decreasing
the entropy in the ranking [24], [25], [40]. In order to achieve
this, E NTBUGuses a search-based technique to automatically
generate tests that reduce uncertainty in rank reports. This
paper makes the following contributions:
•An entropy-based strategy to optimize the quality of
ranking reports;
•A ﬁtness function based on entropy to guide search-based
test generation;
•A prototype implementation of the described approach on
top of the E VOSUITE [17] test generation tool;
•An evaluation of the approach using seven real faults;
To the best of our knowledge, this is the ﬁrst attempt to
generate tests based on the entropy of a test suite. E NTBUG
was able to pinpoint the faulty statement in all real-world
bugs we have considered in our benchmark. Compared to
the original test suite, we observed an average reduction of
49% on the uncertainty of the diagnostic ranking, delivering
an expressive reduction on the number of candidates that
one needs to inspect before ﬁnding the fault by 91% on
average. Comparing with random approach, E NTBUGreduces
the uncertainty of the diagnostic ranking in 4.2 more times,
and 2.5 times the number of candidates to explore until ﬁnding
the faulty one.
II. B ACKGROUND
ENTBUGis based on a spectrum-based reasoning approach
to multiple fault localization [2]. In this section we summarize
the background information of spectrum-based reasoning and
automated test generation.
A. Spectrum-Based Reasoning
Spectrum-based reasoning is an approach to fault local-
ization founded on probability theory. The main principles
underlying the technique are based on Model-Based Diagnosis
(MBD) [13], [15], [16], [21], [27], [39], which uses logical
reasoning to ﬁnd faults.
Acomponent is a program statement. A fault candidate
is a set of components that together explain a fault. Let the
symbolCdenote the set of program components and let
the symbol Ddenote a set of fault candidates. For instance,
D=/an}bracketle{t{c1,c3,c4}/an}bracketri}htindicates that components c1,c3, andc4are
simultaneously at fault, and no other. The approach sorts the
faulty candidates in Dby the probability of each candidate toexplain fault. Spectrum-based reasoning is comprised of two
phases: candidate generation and candidate ranking.
1) Candidate Generation: The problem of ﬁnding fault
candidates can be deﬁned in terms of the widely-known
Minimal Hitting Set (MHS) problem [13]. The precise compu-
tation of MHS is highly demanding [19], restricting its direct
usage for diagnosis. However, in practice, previous research
has found that precise computation of Dis not necessary [1].
STACCATO is a low-cost heuristic for computing a relevant
set of multiple-fault candidates. To illustrate S TACCATO , con-
sider the Triangle class from Figure 1 and the test suite
T. As all test cases in Tpass, S TACCATO yields a diag-
nostic report containing all components in the program, i.e.,
D=/an}bracketle{t{c1},{c2},{c3},{c4},{c5},{c6},{c7},{c8},{c9},{c10}/an}bracketri}ht.
But if we consider Taugmented with the failing test t7,
STACCATO reports the following diagnostic report D=
/an}bracketle{t{c1},{c2},{c4},{c5},{c6},{c8},{c9},{c10}/an}bracketri}ht. Note that com-
ponentsc3andc7are not considered valid candidates because
they have not been covered by any failing tests. For detailed
information on S TACCATO , interested readers may refer to [1].
2) Candidate Ranking: The candidate generation phase
may result in an extensive list of diagnosis candidates. As
not all candidates have the same probability of being the true
fault explanation, techniques have been devised to assign a
probability to each diagnosis candidate dk. Each candidate
dkis a subset of the system components that, when at fault,
can explain the faulty behavior. The probability a diagnosis
candidate being the true fault explanation Pr (dk|obs), given
a number of observations obs, is computed using Bayesian
probability updates. An observation obsis a tuple ( ai∗,ei).
Executing each test case tifrom the test suite T, the
probability of each candidate is updated following Bayes’ rule
Pr(dk|obs) =Pr(dk)·/productdisplay
obsi∈obsPr(obsi|dk)
Pr(obsi)(1)
Pr(dk)is the a priori probability of the candidate (i.e., the
probability before any test is executed), deﬁned as
Pr(dk) =p|dk|·(1−p)M−|dk|(2)
wherepis the a priori probability of a component being
faulty. The prior probability given no observations is an
approximation to 1 fault for every 1000 Lines of Code (LOC),
1/1000= 0.001[12].
Pr(obsi|dk)represents the conditional probability of the ob-
served outcome eiproduced by a test ti(obsi), assuming that
candidate dkis the actual diagnosis
Pr(obsi|dk) =

0ifei∧dkare inconsistent;
1ifeiis unique to dk;
εotherwise.(3)
whereεis deﬁned as
ε=

/productdisplay
j∈dk∧aij=1hj ifei= 0
1−/productdisplay
j∈dk∧aij=1hjifei= 1(4)
258andaijrepresents the coverage of the component jwhen
the testiis executed. As this information is typically not
available, the values for hj∈[0,1]are determined by maxi-
mizing Pr (obs|dk)using maximum likelihood estimation [2].
To solve the maximization problem, a simple gradient ascent
procedure [9] (bounded within the domain 0< hj<1) is
applied.
Pr(obsi)represents the probability of the observed outcome,
independently of which diagnostic explanation is the correct
one and thus needs not be computed directly. The value of
Pr(obsi)is a normalizing factor given by
Pr(obsi) =/summationdisplay
dk∈DPr(obsi|dk)·Pr(dk) (5)
The B ARINEL framework is used in our approach to com-
pute the probabilities of each diagnosis candidate dk. Further
information about the framework and underlying technique
can be found in [2]. When compared to other spectrum-based
approaches to fault localization [2], B ARINEL yields more
information rich diagnostic reports due to the fact that it also
reasons in terms of multiple faults. To illustrate this approach,
the following probabilities are computed for the example in
Figure I after executing test suite T(for detailed information
about the probabilities, see Table I)
d1={c1},→Pr(obs|d1) = 1×h3→0.099984
...
d10={c10},→Pr(obs|d10) = 2×h10→0.100004
After computing the probabilities for each dk∈D, the
candidates are ranked and shown to the user in descending
order of probability to be the true fault explanation (see
Table I).
B. Diagnostic Report Entropy
While debugging, developers can resort to the diagnostic
ranking of diagnosis candidates yielded by B ARINEL to pin-
point the root cause of observed failures quickly. This can
be done by inspecting the ranked candidates in a descending
order according to the diagnostic probabilities.
As the diagnostic ranking assigns probabilities to the di-
agnosis candidates, one may compute an entropy-based score
quantifying the reliability of the ranking; this score conveys
how conﬁdent one can be that the ranking helps ﬁnding the
fault. The entropy H(D)[22] intuitively serves to quantify
the capability to distinguish candidates in the set D. For
example, the value of H(D)is very high for the set D=
{0.3,0.5,0.5,0.5}because we have several elements in the
set with the same value. Therefore, it is difﬁcult to distinguish
which element is more relevant, i.e., which of the candidates
in the example with probability 0.5 of being faulty can explain
the fault better. The minimum value (i.e., approximate ideal)
forHis zero, in which case all elements in the set can be
distinguished from one another. In our context, the maximum
value islog2(M), whereMis the number of components. So,H(D)can be deﬁned as:
H(D) =−/summationdisplay
dk∈DPr(dk)·log2(Pr(dk)),0≤H≤log2(M)
(6)
In the Triangle example we have 10 components
(c1,...,c 10), and to represent the whole set C, in theory, we
need2Mstates
2x≥M→x= log2(M)→x= log2(10)→x= 3.322
So, the best value of His the minimum value (zero) and the
worst value is the maximum ( log2(H)),0≤H(D)≤3.322.
Therefore, the entropy for the example is
H(D) =−/parenleftBig
0.099984·log2(0.099984)+ ...
+ 0.100004·log2(0.100004)/parenrightBig
= 3.322
which corresponds to the maximum value. This means that the
ranking suffers considerably from uncertainty, and we cannot
distinguish which components are most probably at fault.
Therefore, we are not able to diagnose the faulty program.
C. Automated Test Generation
The basis for a diagnosis is a test suite, and often the
existing test suite is not optimized for producing high-quality
diagnostic reports. Hence, is important to generate tests to
improve diagnosis. There are many available test generation
techniques [7]; Search-Based Software Testing (SBST) [28]
is well suited for our scenario, as it has the distinct ability
to optimize test cases and test suites based on custom non-
functional properties. For example, SBST can generate test
suites optimized for coverage and size at the same time,
and SBST is also well suited to address test generation in
a diagnosis context [10], [33].
SBST uses meta-heuristic algorithms to generate test cases
for user-informed objectives. Global search algorithms such as
Genetic Algorithms (GAs) are popular for domains where the
neighbourhood of a candidate solution is very large, such as
for example for unit tests represented as sequences of method
calls. The E VOSUITE tool [17] uses GA to generate test suites
for Java classes with respect to a given coverage criterion. The
GA starts with an initial population of randomly-generated
candidate solutions. A ﬁtness function determines, for each
individual of the population, a numerical value estimating its
distance to the optimal solution. Individuals are selected from
the population; those with better ﬁtness value have higher
probability of being selected. Selected individuals “evolve”
according to pre-deﬁned operators, and form a new population.
This procedure continues until it either ﬁnds an optimal
solution, or runs out of resources (e.g., it reaches timeout).
Debugging is usually seen as a complementary activity to
software testing, typically done afterwards. However, there
are some test generation techniques that aim to improve
debugging [8], [10]. In addition, test generation — in par-
ticular, search-based testing — can also be helpful for failure
reproduction [34] as well as debugging [33].
259III. E NTROPY -BASED TESTGENERATION WITH ENTBUG
This section presents E NTBUG, a novel approach to improve
fault diagnosis. E NTBUGreceives a test suite as input and
produces additional test cases for that test suite, such that the
entropy in the diagnosis is reduced.
A. Calculating Entropy
Spectrum-based fault localization heavily relies on the di-
versity of coverage information across passing and failing
test cases. The variety and number of test cases are two
major factors to determine uncertainty in the ranking. Previous
work [24] has shown that variety andsizeare directly related
to¯ρ, the density of the coverage matrix [21], [24], and that
this metric can be used as a proxy for entropy. The density
of a coverage matrix is the average percentage of components
covered by test cases. It is deﬁned as follows:
¯ρ=1
N·N/summationdisplay
i=1ρ(ti)∧0≤¯ρ≤1
whereρ(ti)refers to the coverage density of a test case ti
ρ(ti) =|{j|aij= 1∧1≤j≤M}|
M(7)
whereNandMdenote the number of test cases and the
number of components, respectively. Low values of ¯ρmean
that test cases exercise small parts of the program (sparse
matrices), whereas high values mean that test cases tend to
involve most components of the program (dense matrices).
Intuitively, neither too-low nor too-high values of ¯ρare
positive. Considering the example from Figure 1, we have
ρ(t1) =4
10, ..., ρ(t6) =2
10. Consequently, the coverage
density is ¯ρ=0.4+...+0.2
6→¯ρ= 0.400, i.e., the test cases
yield a coverage matrix density of 40% ( ¯ρ= 0.400).
B. Optimal Coverage Matrix Density
Our aim is to reduce the entropy of a test suite, and
the drop in entropy is known as information gain [25]. The
information gain that a (new) test case provides is determined
by the reduction of the size of the top-ranked suspect set.
Assuming there are |D|top-ranked suspects, a test tiwith
coverage density ρ(ti)reduces the top-ranked set to |D|·ρ(ti)
components if tifails, and to|D|·(1−ρ(ti))iftipasses. Under
these conditions, it has been previously demonstrated [24] that
the information gain can be modeled as follows:
IG(¯ρ) =−¯ρ·log2(¯ρ)−(1−¯ρ)·log2(1−¯ρ) (8)
For our running example, the value of IGis equal to−0.400·
log2(0.400)−(1−0.400)·log2(1−0.400) = 0.971. The value
ofIGis optimal for ¯ρ= 0.5. Hence, a technique that is able to
generate test cases such that the coverage density of the matrix
is¯ρ= 0.5(provided there is a variety of test cases) will have
the capability of reducing the diagnostic ranking entropy, and
consequently improve the diagnostic quality of spectrum-based
reasoning. Our approach augments the existing test suite with
additional test cases with the goal of balancing the density of
the coverage matrix.Algorithm 1 ENTBUGTest Generation
Input: ProgramΠ, Test Suite T, Search Budget ∆t, Stopping
Condition C
Output: Extended Test Suite T′
1:T′←T
2:d←|0.5−DENSITY(T)|
3:δ←GETFITNESS FUNCTION (T’)
4:while¬Cdo
5:tc←EVOSUITE(Π,δ,∆t)
6: ifd >|0.5−DENSITY(T′∪{t})|−ǫthen
7: T′←T′∪{tc}
8: d←|0.5−DENSITY(T′)|
9: δ←GETFITNESS FUNCTION (T’)
10: end if
11:end while
12:returnT′
C. Generating Tests Guided by the Coverage Matrix Density
The coverage matrix density ¯ρgives us a measurable goal
to guide test generation. As we can measure the effect but
cannot construct suitable test cases systematically, this is an
ideal application for search-based software testing (SBST). In
SBST, an optimization goal is formulated as a ﬁtness function ,
and then efﬁcient meta-heuristic search algorithms are guided
by the ﬁtness function to generate tests.
A ﬁtness function takes as input a candidate solution, and
calculates a numerical value representing the quality of the
candidate, such that there is a strict ordering. In our case,
the optimal solution is a test case that leads to ¯ρ= 0.5,
consequently our ﬁtness function for test case tfor a given
test suite Tis:
ﬁtness(t) =|0.5−¯ρ(T∪{t})| (9)
This ﬁtness function turns the problem into a minimization
problem, i.e., the optimization aims to achieve a ﬁtness value
of0, which is the case if a solution is found such that ¯ρ= 0.5.
D. Entropy-Based Test Suite Extension
Algorithm 1 illustrates E NTBUG’s test-generation proce-
dure. The goal of the algorithm is to extend a potentially empty
test suite with test cases for improving the diagnosis. It takes
as input the program Π, the original test suite T, the search
budget∆tone wants to invest in generating each individual
test, and Boolean condition Cwhich evaluates to true once the
process should stop (e.g., timeout, ﬁxed number of test cases,
etc.). It produces an extension of Tas output.
The density entailed by T, which is used to guide the test
generation process, is calculated using the D ENSITY function
(Line 2) and the difference to 0.5 is stored in dto later compare
if a generated test improves ¯ρ. Then, E VOSUITE is called using
the ﬁtness function δto generate a test case using ∆tas the
search budget (Line 5). E VOSUITE returns the test case that
gives the maximum diagnostic information, i.e., the one that
minimizes the ﬁtness function (Line 5). However, it may be
the case that E VOSUITE fails to ﬁnd a solution that improves
260¯ρand in that case tcis ignored. If tcimproves ¯ρ(within an
accepted error margin ǫ), then it is added to the test suite
T′, a new ﬁtness function δis created, and the value of dis
updated (Lines 6 – 10). These steps are repeated until stopping
condition Cholds.
E. Revisiting the Triangle Example
To measure the success of a diagnosis technique we use
thediagnostic quality Cd, which is a numerical value that
estimates the number of components the tester needs to inspect
to ﬁnd the fault [2], [21]. Note that Cdcannot be computed
prior to computing the ranking: one does not know the actual
position of true-fault candidates in the ranking beforehand.
Because multiple explanations can be assigned with the same
probability, the value of Cdfor the real fault d∗is the average
of the ranks that have the same probability:
θ=|{dk|Pr(dk)>Pr(d∗)}|,1≤k≤M
φ=|{dk|Pr(dk)≥Pr(d∗)}|,1≤k≤M
Cd=θ+φ−Mf
2(10)
A value of 0 for Cdindicates an ideal diagnostic report where
allMffaulty components appear on top of the ranking, i.e.,
there is no wasted effort in inspecting other components. On
the other hand, Cd=M−Mfindicates that the user needs
to test all M−Mfhealthy components until reaching the
Mffaulty ones — this is the worst-case outcome. For the
Triangle example, the value of Cdis4.0 (=3+6−1
2)after
executing test suite T. This means that on average one needs
to inspect 4 components to ﬁnd the fault.
Table I shows the progress of the diagnostic report when
new tests are added to the test suite using E NTBUG. Each
column shows a different stage in the augmentation process.
Numbers in bold-font face under column “Pr( dk)” indicate
components which appear at the same or higher position as the
reaul faulty component d∗. The ﬁnal diagnostic report obtained
with the suite T+{t7,t8,t9}is 0 (i.e., best). At the bottom
of the ﬁgure, one can observe three metrics that, as explained
before, strongly correlate with Cd:¯ρ,IG, andH. WhileCdis
dependent on d∗, which is unknown prior to ﬁnding the faults,
the other metrics are independent. They are very important for
two reasons: (i) they provide a conﬁdence value on the quality
of the diagnostic report, which is critical for the user to build
trust on the report, and (ii) they provide a means to automate
test generation. In particular, we observed that it is possible
to derive from ¯ρthe deﬁnition of a generation.
IV. E VALUATION
We have conducted an empirical study to evaluate the
extent to which E NTBUGis capable of improving diagnostic
quality compared to a baseline technique that, to the best of
our knowledge, incorporates all recent advances in spectrum-
based fault localization [2]. As a sanity check, we have also
compared E NTBUGwith random test generation.Table I: Impact of additional test cases on fault localization for the
triangle example.
RankingT T+{t7} T+{t7,t8} T+{t7,t8,t9}
d Pr(dk)d Pr(dk)d Pr(dk)d Pr(dk)
1{c3} 0.1000140{c5} 0.2903310{c5} 0.3766646{c6} 0.3478261
2{c5} 0.1000140{c6} 0.1720448{c6} 0.2232045{c4} 0.1739130
3{c7} 0.1000140{c9} 0.1720448{c4} 0.1098354{c9} 0.1739130
4{c6} 0.1000040{c10} 0.1720448{c9} 0.1098354{c10} 0.1739130
5{c9} 0.1000040{c4} 0.08466051{c10} 0.1098354{c1} 0.04347826
6{c10} 0.1000040{c1} 0.03629137{c1} 0.02354154{c2} 0.04347826
7{c4} 0.09999400{c2} 0.03629137{c2} 0.02354154{c8} 0.04347826
8{c1} 0.09998400{c8} 0.03629137{c8} 0.02354154
9{c2} 0.09998400
10{c8} 0.09998400
Cd 4.0 2.0 1.0 /an}bracketri}ht/an}bracketri}ht0.0/an}bracketle{t/an}bracketle{t
¯ρ 0.400 0.457 0.475 0.500
IG 0.971 0.995 0.998 1.000
H 3.322 2.651 2.445 2.437
A. Experimental Setup
We have implemented E NTBUGusing E VOSUITE to drive
test generation, and evaluated it on a set of real faults. A par-
ticular difﬁculty to address in our evaluation setup is the need
to create test oracles: the test-generation procedure needs to
decide whether a test it generates passes or not. The automated
generation of test oracles is challenging [18], [29], [31]. This
has to do with the fact that the behaviour of the software has
to be known so that the right oracles are added to the test
cases. As the oracle problem is orthogonal to this paper, we
mitigated this problem by using the version of the subject after
the ﬁx was applied. Let Pto be the faulty program and P′its
ﬁxed version, a test case tpasses ifP′(t) ==P(t); itfails
otherwise. E VOSUITE adds regression oracles to the tests that
make it possible to make this comparison automatically.
To investigate how the diagnosis improves over time, we
conﬁgured the GA in E VOSUITE to run for 10 seconds
per attempt (see ∆tfrom Algorithm 1), and measured the
diagnostic accuracy of the evolving test suite at ﬁve time
intervals: 1, 2, 5, 10 and 30 minutes. To account for the
randomness of the test generation, we repeated all experiments
for 10 times, and all values reported are averaged.
As a sanity check, we also evaluated whether the test cases
produced by E NTBUGimprove the diagnosis over the same
number of random test cases. To do this, we used E VOSUITE
to produce a ﬁxed number of random tests. For example, if an
original test suite is composed by 5 test cases, and at the end
of 1 minute E NTBUGgenerated 10 additional test cases, we
also generated 10 random test cases and compared the impact
on the diagnosis.
B. Subjects
The requirements for choosing the subjects used in our eval-
uation are as follows: (1) the programs should be developed in
Java, (2) the fault must be documented, and (3) the ﬁx should
be available to validate if E NTBUGis able to identify the exact
place of the fault.
We chose the vending machine example used in previous
work [11], and selected six new faults from four large open-
source programs. For each subject, we analyzed recent bug
261Table II: Details of subject programs.
Subject Revision Classes LOCs Original Test Suite /
Test Cases UsedBug ID Affected Class
Vending Machine - 2 54 1 / 1 - VendingMachine
Apache Commons Codec 928,140 24 2,998 303 / 77 99 Base64
Apache Commons Compress 768,548 62 7,365 121 / 26 114 TarUtils
Apache Commons Math 1,244,107 537 61,921 3,541 / 197 835 Fraction
Apache Commons Math 1,416,643 588 69,520 4,318 / 26 938 Line
Apache Commons Math 1,416,643 588 69,520 4,318 / 12 939 Covariance
Joda Time 1,070 188 23,964 2,921/ 169 -BasicDayOfYearDateTimeField
reports, and selected those reports where the ﬁx represents a
change in only one statement (single-fault programs). We use
the ﬁxed version P′of a faulty version Pto evaluate whether
or not E NTBUGpinpoints the exact location of the bug in
the report, i.e., we check if E NTBUGeffectively isolates the
faulty candidate on the top of the ranking. Note that the same
subjects have also been used in previous studies (e.g., [33]),
but we use different faults to demonstrate that E NTBUGworks
regardless of whether the fault causes an undeclared exception
or a wrong output.
Table II provides details about our experimental subjects.
For each subject we only used those test cases that executed
the class containing the fault, therefore Table II shows both
the total number of test cases in the original test suite1and
the number of test cases actually used. It has also the bug
identiﬁer of each bug report used.
C. Summary of Results
Empirical results indicate that E NTBUGimproves the ability
of a test suite to diagnose a problem. In fact, E NTBUGwas
able to pinpoint the location of every bug we considered in
our study.
Figure 2 shows the evolution of ¯ρ,CdEntBug andCdrandom
over time during test generation. The y-axis at the left-hand
side denotes the domain of ¯ρwhile the y-axis at the right-
hand side denotes the domain of Cd. The x-axis represents
the number of test cases from the original test suite plus the
test cases generated on each time interval. Recall that the ideal
scenario for diagnosis is one where ¯ρapproximates to 0.5and
Cdapproximates to 0. Figure 3a shows the increase in number
of tests for each of the subjects over time, and Figure 3b
demonstrates how E NTBUGis able to reduce entropy during
evolution.
In the following we discuss each subject in detail, show
how E NTBUGhelps to locate each corresponding fault, and
compare this result with random test generation.
D. Vending Machine
Vending Machine is a small proof-of-concept example com-
prised of two classes and one test case only. Its purpose is to
check whether there is enough credit that allows a user to buy
a product. If needed, the user may chip more money in.
1The original test suite of each subject does not contain the test cases which
were submitted when a patch was committed.Listing 1: Vending Machine ﬁx.
// Class : vendingmachine.VendingMachine
@@ -45,7 +45,7 @@
public void vend()throwsException { ...
this.currValue -= COST;
-if(this.currValue == /zero.noslash) {
+if((this.currValue - COST) <= /zero.noslash) {
this.enabled = false;
} ...
Vending Machine fails when the user credit reaches a
negative value and the branch was not taken: the enabled
variable stays true, and the user has permission to buy more
products (even with a negative credit).
Using the original test suite, and despite the fact that an
observed failure is documented, the spectrum-based reasoning
approach to fault localization leads to 39% of the code to be
inspected to ﬁnd the fault. This has to do with the fact that
the original test suite has a high value ¯ρas it only contains
one passing test. Consequently, the cost Cdto diagnose this
program is relatively high, as Figure 2a indicates.
Augmenting the test suite with new test cases during 1
minute reduces the number of statements to inspect to ﬁnd the
fault (4 on average), as well as reduces the value of ¯ρfrom
0.656 to 0.500. During this time interval, E NTBUGincreased
the size of the original test suite from 1 to 15. Figure 3a shows
the increase in the number of test cases. At the same time
the information gain ( IG) obtained with the new test cases
increases and the entropy decreases from 5.000 to 3.381.
After 2 minutes and 20 test cases generated, E NTBUG
reduces the statements that need to be inspected to just 2 (on
average). After 5 minutes E NTBUGobtains a test suite that
produces a ¯ρvalue of 0.515 (0.015 away from the optimal
value) and an IGvalue of 0.999 (very close to the ideal value
of 1.0). In turn, the entropy of the ranking is 2.398representing
a reduction of 52.04% compared to that of the original test
suite, which has an entropy of 5.000. See Figure 3b.
ENTBUGpinpoints as faulty the statements at lines 44 and
45. As one can see from Listing 1, the fault localization
technique is accurate when singling the statement 45 out as
faulty. The statement 44 also appears in the top of the ranking
because it is always executed with statement 45; the underlying
technique cannot distinguish between components that have
the same execution pattern.
Comparing the results with random test generation, E NT-
BUGwas twice as precise in terms of the number of statements
that need to be explored until ﬁnding the faulty one.
262 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
1 15 21 45 52 65 2 4 6 8 10 12 14 16 18 20 22ρCd
# (suite size)|
|ρ
CdENTBUG
Cdrandom(a) Vending Machine.
 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
77 98 125 126 127 139 0 10 20 30 40 50 60ρCd
# (suite size)|
|ρ
CdENTBUG
Cdrandom (b) Apache Commons Codec #99.
 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
26 35 42 58 76 94 0 5 10 15 20 25 30 35 40ρCd
# (suite size)|
|ρ
CdENTBUG
Cdrandom (c) Apache Commons Compress #114.
 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
197 235 262 359 472 606 0 10 20 30 40 50 60ρCd
# (suite size)|
|ρ
CdENTBUG
Cdrandom (d) Apache CommonsMath #835.
 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
26 40 50 72 106 149 0 2 4 6 8 10 12 14 16 18 20ρCd
# (suite size)|
|ρ
CdENTBUG
Cdrandom
(e) Apache Commons Math #938.
 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
12 21 29 54 88 142 0 10 20 30 40 50 60ρCd
# (suite size)|
|ρ
CdENTBUG
Cdrandom (f) Apache Commons Math #939.
 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
169 191 204 232 258 287 0 2 4 6 8 10 12 14 16 18 20ρCd
# (suite size)|
|ρ
CdENTBUG
Cdrandom (g) Joda Time.
Figure 2: Evolution of ¯ρandCdfor every subject.
 0 100 200 300 400 500 600 700
0 1 2 5 10 30Size
t (minutes)Vending Machine
Apache Commons Codec #99
Apache Commons Compres #114
Apache Commons Math #835
Apache Commons Math #938
Apache Commons Math #939
Joda Time
(a) Test suite size evolution.
H(D)
 0 2 4 6 8 10
0 1 2 5 10 30
t (minutes)Vending Machine
Apache Commons Codec #99
Apache Commons Compress #114
Apache Commons Math #835
Apache Commons Math #938
Apache Commons Math #939
Joda Time
(b) Entropy value evolution.
Figure 3: Evolution of the test suites size and Hfor every subject.
E. Apache Commons Codec #99
Apache Commons Codec2provides an API of common
encoders and decoders such as Base64, Hex and URLs.
As described in the major bug 993, the method
encodeBase64String of the class Base64 fails because it
chunks the parameter binaryData . This means that the second
parameter of the method newStringUtf8 called on method
encodeBase64String should be false and nottrue .
Listing 2: Apache Commons Codec ﬁx for bug 99.
// org.apache.commons.codec.binary.Base64
@@ -667,7 +667,7 @@
public static String encodeBase64String( byte[] binaryData) {
-returnStringUtils.newStringUtf8(encodeBase64(binaryData,
true));
2Apache Commons Codec project homepage http://commons.apache.org/
proper/commons-codec/, 2013.
3Apache Commons Codec Bug 99 https://issues.apache.org/jira/browse/
CODEC-99, 2013+returnStringUtils.newStringUtf8(encodeBase64(binaryData,
false));
} ...
The original test suite requires a considerable effort to ﬁnd
the fault (44 statements on average), when using spectrum-
based reasoning. After generating 21 new test cases (1 minute)
we are able to reduce Cdto 4 statements. As can be seen
in Figure 2b, this represents a reduction of 90.91% on the
diagnostic effort. Applying E NTBUGfor an extra minute
results in more 27 test cases (see Figure 3a), and we are able
to pinpoint exactly the faulty line described in Listing 2. This
corresponds (as expected) to a value of ¯ρ= 0.480, very close
to the optimal value, a total information of 0.999, and that
the faulty statement appears in top of the ranking as the most
likely faulty component in the program.
Note that after 10 minutes there is a slight decrease in the
value of¯ρ. Recall that E NTBUG’s test generation uses an error263marginǫto calculate ﬁtness improvement (see Algorithm 1).
As such it can happen that a test case can be admitted for
inclusion in the test suite even without improving ﬁtness.
Although random test cases achieve a minimum that repre-
sents a reduction of 47.13% (when comparing to the original
test suite), the diagnostic effort is 23 times as high as on the
result achieved by E NTBUG(see Figure 2b).
F . Apache Commons Compress #114
The Apache Commons Compress4library deﬁnes an API
for working with the most popular compressed archives such
as ar, cpio, Unix dump, tar, zip, gzip, XZ, Pack200 and bzip2.
Listing 3: Apache Commons Compress ﬁx for Bug 114.
// org.apache.commons.compress.archivers.tar.TarUtils
@@ -95,11 +95,11 @@
for(inti = offset; i < end; ++i) { ...
- result.append(( char) buffer[i]);
+ result.append(( char) (b & /zero.noslashxFF)); //Allow for sign extension
} ...
The reported major bug 1145explains that the project
Apache Commons Compress fails when the class TarUtils
receive as input a tarﬁle which contains ﬁles with special
characters. A simple ﬁx to resolve the encoding problem is
to guarantee that the name of the ﬁles are treated as unsigned.
The original test suite of Apache Commons Compress
project has an IGclose to the perfect value, 0.991 (this
means a ¯ρvalue of 0.556). However, the cost to diagnose
this project in the beginning is 36 statements on average (as
seen in Figure 2c). This is explained by the diversity of the
test suite: although being able to entail a ¯ρnear to the optimal
value, the tests are not diverse.
After the ﬁrst minute of test generation the size of the
original test suite increases from 26 test cases to 35 test
cases (see Figure 3a), and the value of Cdis reduced to
only 34 statements on average. Compared to the initial effort
to diagnose the program, this represents a reduction of only
two statement on average. If we continue generating new
test cases for 1 more minute (7 new test cases) the effort to
ﬁnd the faulty statement is 27statements on average. In this
period, as we can see in Figure 2c, random generation has a
slightly better result. At the interval of 5 minutes, we have an
effort of only 7statement and at 10 minutes we pinpoint with
precision of Cd= 2 (this means a reduction of 94.37%) the
faulty statements described in Listing 3, with a best value of
¯ρ= 0.491. At the same time, random generation only achieved
a reduction in terms of Cdaround 79%.
G. Apache Commons Math #835
The Apache Commons Math6is a library that provides self-
contained mathematics and statistics functions for Java.
4Apache Commons Compress project homepage http://commons.apache.org/
proper/commons-compress/, 2013.
5Apache Commons Compress Bug 114 https://issues.apache.org/jira/browse/
COMPRESS-114, 2013.
6Apache Commons Math project homepage http://commons.apache.org/
proper/commons-math/, 2013.Listing 4: Apache Commons Math ﬁx for Bug 835.
// org.apache.commons.math3.fraction.Fraction
@@ -594,7 +594,7 @@
public double percentageValue() {
-returnmultiply(1/zero.noslash/zero.noslash).doubleValue();
+return1/zero.noslash/zero.noslash * doubleValue();
} ...
The Bug 8357reports a failure when the
percentageValue() method of the Fraction class multiplies
a fraction value by 100, and then converts the result to a
double. This causes an overﬂow when the numerator is
greater than Integer.MAX VALUE/1/zero.noslash/zero.noslash , and even when the
value of a fraction is far below this value. A change in the
order of multiplication, i.e., ﬁrst convert a fraction value to
a double and then multiply that value by 100, resolved the
overﬂow problem.
The cost to diagnose the Apache Commons Math project
with the original test suite for bug 835 is on average 34
statements (as we can see in Figure 2d).
For the bug 835, which starts with a coverage density of
0.146, ENTBUGtakes more time to achieve the perfect ¯ρ
value. After 10 minutes of random generation, the number
of candidates to inspect was reduced from 34 to 31, whereas
ENTBUGreduced the diagnostic effort in the same time by
92.24% (which represents only 3 remaining components) on
average. But, only after 30 minutes of generation, we achieve
aCdvalue of 1 (almost perfect) and a ¯ρvalue of0.365for bug
835 (Listing 4). In this period of time, the test suite increases
its size (see Figure 3a) from 197 (number of original test cases
that touch the fault class) to 606 test cases. We conjecture that
the slow performance is because E VOSUITE produces large
numbers, which are important for this scenario, with only a
low probability.
For this particular subject, as we can see in Figure 3b,
the value of entropy decreases in the ﬁrst two minutes of
generation (29.18%). However, at 5 minutes (where we are
able to reduce the cost to diagnose in 77.61%) the entropy
that characterizes the ranking increases from 5.242 to5.702.
The reason why this happened is that at the ﬁrst two minutes
almost every (71 out of 79) candidate in the ranking has the
same probability ( 0.011); at 5 minutes the probabilities still
are the same but higher ( 0.034). For these two intervals the
number of candidates in the ranking is equal, 79 in total.
H. Apache Commons Math #938
The major bug 9388explains that the method revert from
the class Line only maintains 10 digits of precision for the
ﬁeld direction. This becomes a bug when the line’s position
is evaluated far from the origin. A possible ﬁx is creating a
new instance of Line and then reverting its direction.
Using the original test suite, the cost incurred to diagnose
the Apache Commons Math project for bug 938 is 4 state-
ments.
7Apache Commons Math Bug 835 https://issues.apache.org/jira/browse/
MATH-835, 2013.
8Apache Commons Math Bug 938 project homepage https://issues.apache.
org/jira/browse/MATH-938, 2013.
264Listing 5: Apache Commons Math ﬁx for Bug 938.
// org.apache.commons.math3.geometry.euclidean.threed.Line
@@ -84,7 +84,9 @@
publicLine revert() {
-return new Line(zero, zero.subtract(direction));
+finalLine reverted = newLine(this);
+ reverted.direction = reverted.direction.negate();
+returnreverted;
} ...
Figure 2e shows that the cost increase as more tests are
added, rather than decreasing, even though ¯ρimproves. This
is because the generated test cases were all passing (and
therefore, there is no evidence for exonerating/blaming com-
ponents). After 2 minutes, random generation achieves a close
to perfect Cd– taking advantage of the fact that random
generated test cases failed, too. However, after generating test
cases for 30 minutes, and with sufﬁcient pass/fail test cases in
the suite, Cdreaches the minimum value of of 1 statement to
inspect on average and ¯ρ= 0.496. This is a 50% improvement
over the cost when using the random approach.
I. Apache Commons Math #939
Listing 6: Apache Commons Math ﬁx for Bug 939.
// org.apache.commons.math3.stat.correlation.Covariance
@@ -279,15 +279,15 @@
private void checkSufficientData( finalRealMatrix matrix)
throwsMathIllegalArgumentException {
intnRows = matrix.getRowDimension();
intnCols = matrix.getColumnDimension();
-if(nRows < 2 || nCols < 2) {
+if(nRows < 2 || nCols < 1) {
throw new MathIllegalArgumentException(
LocalizedFormats.INSUFFICIENT_ROWS_AND_COLUMNS,
nRows, nCols); ...
The speciﬁcation of the class Covariance states that it only
takes a single-column matrix (i.e., N-dimensional random
variable with N= 1) as argument and returns a 1−by−1co-
variance matrix. However, the method checkSufficientData
throws an IllegalArgumentException (see major bug 9399
for detailed information) when the constructor of the class
receives a 1−by−Mmatrix.
The cost to diagnose the Apache Commons Math project
for the bug 939 is on average 35 statements with the original
test suite (as we can see in Figure 2f).
To pinpoint the faulty location for bug 939 (described in
Listing 6) E NTBUGneeds 30 minutes. This represents a ¯ρ
value of0.510, a gain of 93.60% in terms of ranking entropies,
and 130 new test cases. On the other hand, as with the previous
subjects, random test generation performed 5.8 times worse
when comparing to the Cdvalue returned by E NTBUG. For
this subject, E NTBUGmay not achieve the perfect value of
Cd= 0because of the structure of the class Covariance . The
checkSufficientData function (more properly the line 279)
responsible for the bug 939, is a private function. This function
is only executed by the constructor of the class Covariance .
9Apache Commons Math Bug 939 https://issues.apache.org/jira/browse/
MATH-939, 2013.Therefore, those components that appears at the top of the
ranking are the statements of the Covariance constructor and
thecheckSufficientData function.
J. Joda Time
Joda Time10is a library for advanced date andtime func-
tionalities for the Java language.
Listing 7: Joda Time ﬁx.
// org.joda.time.chrono.BasicDayOfYearDateTimeField
@@ -9/zero.noslash,7 +9/zero.noslash,7 @@
protected int getMaximumValueForSet( longinstant, intvalue)
{
intmaxLessOne = iChronology.getDaysInYearMax() - 1;
-returnvalue > maxLessOne ? getMaximumValue(instant) :
maxLessOne;
+return(value > maxLessOne || value < 1) ?
getMaximumValue(instant) : maxLessOne;
} ...
The class BasicDayOfYearDateTimeField provides meth-
ods to perform time calculations for a day of a year. Joda Time
bug11was related to the method getMaximumValueForSet ,
which returns an incorrect value. The ﬁx of this bug consists
of validating if the value of the variable value is between the
maximum and the minimum value of the range or not.
Due to the nature of the Joda Time project (e.g., the source
code is structured in large hierarchies of classes consisting of
many one-line methods) the original test suite has a coverage
density of only 0.136. With the original test suite, the cost to
diagnose the fault described in Listing 7 is 19 statements on
average. However, already after two minutes and 35 new test
cases (see Figure 2g for more details), the value of Cddrops to
only one statement. At the same time, the ranking entropy was
reduced by 38.27%. After half an hour of generating new test
cases, the value of ¯ρincreases to 0.247. For this subject, and
considering the 30 minutes of experiments, random generation
achieved values of Cdbetween 14 to 16 statements on average.
So, considering the total of time (30 minutes), this means, that
ENTBUGperformed 4 times better than random generation.
K. Threats to Validity
Construct Validity: The performance of E NTBUGhas been
evaluated using the Cdmetric, which measures diagnostic ef-
fort in terms of the position of the fault in the diagnostic report.
This metric assumes that developers traverse the ranking, but
that may not be the case in practice [30]. However, we argue
that developers are more likely to traverse the ranking if the
precision is increased.
External Validity: Despite being real, large and open
source software systems, we have only considered ﬁve sub-
jects with seven (single-) faults in our empirical experiment.
Therefore, it is possible that the results for a different set of
programs with different characteristics and even with multiple-
faults may produce different results.
10Joda Time project homepage http://joda-time.sourceforge.net/, 2013.
11Joda Time bug corrected at https://joda-time.svn.sourceforge.net/svnroot/
joda-time/trunk@1071, 2013.
265Internal Validity: Eventual faults in the implementation of
ENTBUGor in the underlying test case generation E VOSUITE
may invalidate the results. To mitigate this threat, we have
not only thoroughly tested our scripts but also manually
checked a large set of results. Furthermore, all experiments
were repeated multiple times to account for the randomness
of the test generation, and we veriﬁed the results between runs
for consistency.
V. R ELATED WORK
Although there is a large body of work on automated test
generation and debugging in general, there have only been few
attempts at using test generation in the context of debugging.
Baudry et. al [10] proposed an approach to improve di-
agnostic accuracy using a bacteriological algorithm (similar
to a GA) to select test cases from a test suite. The criterion
for test selection they proposed estimates the quality of a
test for diagnosis. Their selection procedure attempts to ﬁnd
an optimal balance between the size of a test suite and its
contribution to diagnosis. The goal of their work is similar
to ours, but our contributions are complementary: One could
use E NTBUGfor test generation and the algorithm they
proposed for test selection. It remains to be evaluated if such a
combination would improve the diagnostic report’s accuracy.
Artzi et al. [8] use a specialized concrete-symbolic exe-
cution [20], [36] to improve fault localization. The principle
of their customized algorithm is highly similar to that of
the Nearest Neighbors Queries algorithm [32] proposed by
Renieris and Reiss. However, instead of selecting tests that
are similar to a given failing test, Artzi et al. generate such
tests. One important difference between our work and theirs
is in the assumptions: While we make no assumption about
the previous test suite, they assume there is at least one fault-
revealing test to seed the search. However, in practice it is
possible that no fault-revealing test exists in the test suite. An
important technical difference between our approaches is that
their algorithm uses as input a single fault-revealing test and
generates passing tests that minimizes observed differences for
that particular test. However, it has been shown that multiple
fault-revealing tests can help improve diagnostic accuracy [3].
R¨oßler et. al [33] introduced a search-based approach to
identify fault candidates. Similar to the work of Artzi et
al.[8], their B UGEX tool takes a failing test case as a starting
point, determines a set of “facts” (e.g., executed statements,
branches, program states, etc.) and then systematically tries to
generate variations of the failing test which differ in individual
facts. If a passing test differs in only one fact to the failing test,
then that fact is assumed to be relevant for diagnosis; if the
differing test also fails, then the fact is irrelevant. B UGEX is
also implemented using E VOSUITE , but our approach differs
in several aspects: First, we do not assume the existence of
a single failing test — we can optimize a test suite also
in the presence of no faults or in the presence of multiple
faults. Second, B UGEX uses a white-box testing approach
to minimize facts about structural aspects of a program. Incontrast, our approach is only guided by entropy, which means
it is applicable to any testing domain.
To the best of our knowledge, the use of entropy to guide
test generation is novel. However, entropy has been used to
prioritize test cases to improve fault localization [24], [40].
VI. C ONCLUSIONS AND FUTURE WORK
This paper presents an approach to generate test cases
automatically based on their ability to produce a reﬁned
diagnosis, rather than ﬁnding a failure as early as possible.
The results observed in our empirical evaluation using our
ENTBUGprototype show that the proposed approach can lo-
calize the faults with superior accuracy in all seven considered
scenarios. Compared to the original test suites, we observed an
average reduction of 49% on the uncertainty of the diagnostic
ranking, while also reducing the number of candidates that one
needs to inspect before ﬁnding the fault by 91% on average.
Comparison to random tests shows that this improvement does
not simply come from the increased test suite size, but that
entropy provides guidance to the test generator.
These are encouraging results, yet whether these improve-
ments are sufﬁcient to convince programmers to use spectrum-
based fault localization needs to be empirically studied. We
therefore plan to perform more experimentation and user
studies to assess the usefulness of the results for developers.
Besides studying its practical value, the presented approach
offers several avenues for future research. First, our evaluation
so far only considered single faults, and we plan to evaluate
the performance of our approach in presence of multiple faults.
Second, our approach is currently only driven by the observed
entropy, and does not take information about the source code
into account. Potentially, the efﬁciency could be increased
further by considering structural aspects during test generation,
for example by explicitly considering the coverage of existing
test cases in the ﬁtness function (e.g., [33]). Third, any
approach aiming to improve diagnostic accuracy by generating
new tests is limited by the oracle problem; future work will
therefore have to consider how to apply the presented approach
in practice, and how to best interact with the developer. Finally,
our current usage scenario is that of an existing test suite that
needs to be improved. We plan to apply and evaluate the ideas
presented in this paper also for generating test suites with high
diagnostic accuracy in the ﬁrst place.
More information on E NTBUGis available at
http://www.evosuite.org/EntBug
ACKNOWLEDGMENTS
This work is partially funded by the ERDF through the Pro-
gramme COMPETE, the Portuguese Government through FCT
- Foundation for Science and Technology, project reference
FCOMP-01-0124-FEDER-020484, and by a Google Focused
Research Award on “Test Ampliﬁcation”.
266REFERENCES
[1] R. Abreu and A. J. C. van Gemund, “A Low-Cost Approximate Minimal
Hitting Set Algorithm and its Application to Model-Based Diagnosis,”
inSymposium on Abstraction, Reformulation and Approximation , ser.
SARA ’09, 2009.
[2] R. Abreu, P. Zoeteweij, and A. J. C. v. Gemund, “Spectrum-Based
Multiple Fault Localization,” in Proceedings of the 2009 IEEE/ACM
International Conference on Automated Software Engineering , ser. ASE
’09. Washington, DC, USA: IEEE Computer Society, 2009, pp. 88–99.
[3] R. Abreu, P. Zoeteweij, R. Golsteijn, and A. J. Van Gemund, “A practical
evaluation of spectrum-based fault localization,” Journal of Systems and
Software , vol. 82, no. 11, pp. 1780–1792, 2009.
[4] H. Agrawal, J. R. Horgan, E. W. Krauser, and S. London, “Incremental
Regression Testing,” in Proceedings of the Conference on Software
Maintenance , ser. ICSM ’93. IEEE Comp. Soc., 1993, pp. 348–357.
[5] S. Ali, J. H. Andrews, T. Dhandapani, and W. Wang, “Evaluating
the Accuracy of Fault Localization Techniques,” in Proceedings of
the 2009 IEEE/ACM International Conference on Automated Software
Engineering , ser. ASE ’09. IEEE Computer Society, 2009, pp. 76–87.
[6] E. Alves, M. Gligoric, V . Jagannath, and M. d’Amorim, “Fault-
localization using dynamic slicing and change impact analysis,” in
Proceedings of the 2011 26th IEEE/ACM International Conference on
Automated Software Engineering , 2011, pp. 520–523.
[7] S. Anand, E. Burke, T. Y . Chen, J. Clark, M. B. Cohen, W. Grieskamp,
M. Harman, M. J. Harrold, and P. McMinn, “An orchestrated survey
on automated software test case generation,” Journal of Systems and
Software , vol. 86, no. 8, pp. 1978–2001, August 2013.
[8] S. Artzi, J. Dolby, F. Tip, and M. Pistoia, “Directed Test Generation for
Effective Fault Localization,” in Proceedings of the 19th international
symposium on Software testing and analysis , ser. ISSTA ’10. New
York, NY , USA: ACM, 2010, pp. 49–60.
[9] M. Avriel, Nonlinear Programming: Analysis and Methods , ser. Dover
Books on Computer Science Series. Dover Publications, 2003.
[10] B. Baudry, F. Fleurey, and Y . Le Traon, “Improving Test Suites for
Efﬁcient Fault Localization,” in Proceedings of the 28th International
Conference on Software engineering , ser. ICSE. ACM, 2006, pp. 82–91.
[11] M. Burger and A. Zeller, “Minimizing reproduction of software failures,”
inProceedings of the 2011 International Symposium on Software Testing
and Analysis , ser. ISSTA ’11. NY , USA: ACM, 2011, pp. 221–231.
[12] J. Carey, N. Gross, M. Stepanek, and O. Port, “Software hell,” in
Business Week , 1999, pp. 391–411.
[13] J. de Kleer and B. C. Williams, “Diagnosing multiple faults,” Artif.
Intell. , vol. 32, no. 1, pp. 97–130, Apr. 1987.
[14] J. De Kleer, “Diagnosing multiple persistent and intermittent faults,”
inProceedings of the 21st International Joint Conference on Artiﬁcial
Intelligence , ser. IJCAI ’09. San Francisco, CA, USA: Morgan
Kaufmann Publishers Inc., 2009, pp. 733–738.
[15] A. Feldman, G. Provan, and A. Van Gemund, “Computing minimal
diagnoses by greedy stochastic search,” in Proceedings of the 23rd
national conference on Artiﬁcial intelligence - Volume 2 , ser. AAAI’08.
AAAI Press, 2008, pp. 911–918.
[16] A. Feldman and A. van Gemund, “A two-step hierarchical algorithm for
model-based diagnosis,” in Proceedings of the 21st national conference
on Artiﬁcial intelligence , ser. AAAI. AAAI Press, 2006, pp. 827–833.
[17] G. Fraser and A. Arcuri, “EvoSuite: Automatic Test Suite Generation
for Object-Oriented Software,” in Proceedings of the 19th ACM SIG-
SOFT symposium and the 13th European conference on Foundations of
software engineering , ser. ESEC/FSE ’11. ACM, 2011, pp. 416–419.
[18] G. Fraser and A. Zeller, “Mutation-driven generation of unit tests
and oracles,” in Proceedings of the 19th international symposium on
Software testing and analysis , ser. ISSTA. ACM, 2010, pp. 147–158.
[19] M. R. Garey and D. S. Johnson, Computers and Intractability; A Guide
to the Theory of NP-Completeness . W. H. Freeman & Co., 1990.
[20] P. Godefroid, N. Klarlund, and K. Sen, “DART: Directed Automated
Random Testing,” in Proceedings of the 2005 ACM SIGPLAN conference
on Programming language design and implementation , ser. PLDI ’05.
New York, NY , USA: ACM, 2005, pp. 213–223.
[21] A. Gonzalez-Sanchez, R. Abreu, H.-G. Gross, and A. van Gemund,
“Prioritizing tests for fault localization through ambiguity group reduc-
tion,” in Proceedings of the 26th IEEE/ACM International Conference
on Automated Software Engineering , ser. ASE ’11. Washington, DC,
USA: IEEE Computer Society, 2011, pp. 83–92.[22] A. Gonzalez-Sanchez, R. Abreu, H.-G. Gross, and A. J. van Gemund,
“Spectrum-Based Sequential Diagnosis,” in Proceedings of the 25th
AAAI International Conference on Artiﬁcial Intelligence (AAAI’11) , Aug
2011, pp. 189–196.
[23] A. Gonzalez-Sanchez, H.-G. Gross, and A. J. C. van Gemund, “Model-
ing the Diagnostic Efﬁciency of Regression Test Suites,” in Proceedings
of the 2011 IEEE Fourth International Conference on Software Testing,
Veriﬁcation and Validation Workshops , ser. ICSTW ’11. Washington,
DC, USA: IEEE Computer Society, 2011, pp. 634–643.
[24] A. Gonzalez-Sanchez, E. Piel, H.-G. Gross, and A. J. C. van Gemund,
“Prioritizing Tests for Software Fault Localization,” in Proceedings of
the 10th International Conference on Quality Software , ser. QSIC ’10.
Washington, DC, USA: IEEE Computer Society, 2010, pp. 42–51.
[25] R. A. Johnson, “An information theory approach to diagnosis,” Reliabil-
ity and Quality Control, IRE Transactions on , no. 1, pp. 35–35, 1960.
[26] J. A. Jones, M. J. Harrold, and J. Stasko, “Visualization of Test
Information to Assist Fault Localization,” in Proceedings of the 24th
International Conference on Software Engineering , ser. ICSE ’02. New
York, NY , USA: ACM, 2002, pp. 467–477.
[27] W. Mayer and M. Stumptner, “Evaluating Models for Model-Based
Debugging,” in Proceedings of the 2008 23rd IEEE/ACM International
Conference on Automated Software Engineering , ser. ASE ’08. Wash-
ington, DC, USA: IEEE Computer Society, 2008, pp. 128–137.
[28] P. McMinn, “Search-Based Software Test Data Generation: A Survey,”
Softw. Test. Verif. Reliab. , vol. 14, no. 2, pp. 105–156, Jun. 2004.
[29] E. Miller and W. E. Howden, Software Testing and Validation Tech-
niques , ser. 2nd ed. New York, USA: IEEE Comp. Soc. Press, 1981.
[30] C. Parnin and A. Orso, “Are automated debugging techniques actually
helping programmers?” in Proceedings of the 2011 International Sym-
posium on Software Testing and Analysis , ser. ISSTA ’11. New York,
NY , USA: ACM, 2011, pp. 199–209.
[31] F. Pastore, L. Mariani, and G. Fraser, “CrowdOracles: Can the Crowd
Solve the Oracle Problem?” in Proceedings of the 6th International
Conference on Software Testing, Veriﬁcation and Validation , ser. ICST
’13. Washington, DC, USA: IEEE Computer Society, 2013.
[32] M. Renieris and S. P. Reiss, “Fault Localization With Nearest Neighbor
Queries,” in Proceedings of the 18th IEEE International Conference on
Automated Software Engineering , ser. ASE ’03. Washington, DC, USA:
IEEE Computer Society, 2003, pp. 30–39.
[33] J. R ¨oßler, G. Fraser, A. Zeller, and A. Orso, “Isolating failure causes
through test case generation,” in Proceedings of the 2012 International
Symposium on Software Testing and Analysis , ser. ISSTA 2012. New
York, NY , USA: ACM, 2012, pp. 309–319.
[34] J. R ¨oßler, A. Zeller, G. Fraser, C. Zamﬁr, and G. Candea, “Reconstruct-
ing Core Dumps,” in Proceedings of the 6th International Conference
on Software Testing, Veriﬁcation and Validation , ser. ICST ’13. Wash-
ington, DC, USA: IEEE Computer Society, 2013.
[35] R. Santelices, J. A. Jones, Y . Yu, and M. J. Harrold, “Lightweight
fault-localization using multiple coverage types,” in Proceedings of the
31st International Conference on Software Engineering , ser. ICSE ’09.
Washington, DC, USA: IEEE Computer Society, 2009, pp. 56–66.
[36] K. Sen, D. Marinov, and G. Agha, “CUTE: A Concolic Unit Testing
Engine for C,” in Proceedings of the 10th European software engineering
conference held jointly with 13th ACM SIGSOFT international sympo-
sium on Foundations of software engineering , ser. ESEC/FSE-13. New
York, NY , USA: ACM, 2005, pp. 263–272.
[37] W. E. Wong, V . Debroy, Y . Li, and R. Gao, “Software Fault Localization
Using DStar (D*),” in Proceedings of the 2012 IEEE Sixth International
Conference on Software Security and Reliability , ser. SERE ’12. Wash-
ington, DC, USA: IEEE Computer Society, 2012, pp. 21–30.
[38] F. Wotawa, “Bridging the Gap Between Slicing and Model-based
Diagnosis,” in Proceedings of the Twentieth International Conference
on Software Engineering & Knowledge Engineering , ser. SEKE ’08.
Knowledge Systems Institute Graduate School, 2008, pp. 836–841.
[39] F. Wotawa, M. Stumptner, and W. Mayer, “Model-Based Debugging or
How to Diagnose Programs Automatically,” in Proceedings of the 15th
international conference on Industrial and engineering applications of
artiﬁcial intelligence and expert systems , ser. IEA/AIE ’02. London,
UK, UK: Springer-Verlag, 2002, pp. 746–757.
[40] S. Yoo, M. Harman, and D. Clark, “Fault localization prioritization:
Comparing information-theoretic and coverage-based approaches,” ACM
Transactions on Software Engineering and Methodology (TOSEM) ,
vol. 22, no. 3, p. 19, July 2013.
267