DC2: A Framework for Scalable, Scope-Bounded
Software Veriﬁcation
Franjo Ivan ˇci´c1, Gogul Balakrishnan1, Aarti Gupta1, Sriram Sankaranarayanan2,
Naoto Maeda3, Hiroki Tokuoka3, Takashi Imoto3, Yoshiaki Miyazaki3
1NEC Laboratories America, Princeton, USA.2University of Colorado, Boulder, USA
3NEC Corporation, Kawasaki, Japan
{ivancic,bgogul,agupta}@nec-labs.com ,srirams@colorado.edu
{n-maeda@bp,tokuoka@ay,t-imoto@ak,y-miyazaki@bq}.jp .nec.com
Abstract —Software model checking and static analysis have
matured over the last decade, enabling their use in automate d
software veriﬁcation. However, lack of scalability makes t hese
tools hard to apply. Furthermore, approximations in the mod els
of program and environment lead to a profusion of false alarm s.
This paper proposes DC2, a veriﬁcation framework using scop e-
bounding to bridge these gaps. DC2 splits the analysis probl em
into manageable parts , relying on a combination of three auto-
mated techniques: (a) techniques to infer useful speciﬁcat ions
for functions in the form of pre- and post-conditions; (b) stub
inference techniques that infer abstractions to replace function
calls beyond the veriﬁcation scope; and (c) automatic reﬁne ment
of pre- and post-conditions from false alarms identiﬁed by a user.
DC2 enables iterative reasoning over the calling environme nt,
to help in ﬁnding non-trivial bugs and fewer false alarms. We
present an experimental evaluation that demonstrates the e ffec-
tiveness of DC2 on several open-source and industrial softw are
projects.
I. I NTRODUCTION
Software Model Checking is a promising technique for
ﬁnding subtle bugs in software [19]. The primary utility of
model checkers lies in their ability to present a witness to
help explain a bug to a developer. However, the lack of
scalability (due to fundamental hardness) and the reportin g
of false alarms (due to modeling abstractions) form a major
hindrance to the adoption of software model checking in
industry. A practically viable software model checker must
exhibit many key desirable qualities: (1) Scalability : handle
1 MLOC and beyond (for C/C++); (2) Performance : complete
veriﬁcation within the allotted time; and ﬁnally, (3) Accuracy:
yield accurate bug reports with a low rate of false alarms ,
so that human effort is not wasted in examining them. The
capabilities of model checkers have made spectacular advan ces
over the last two decades, especially with the advent of mod-
ern SAT/SMT solvers. However, in spite of these advances,
when used “ out-of-the-box ” even the most advanced software
model checking engine cannot handle large projects that are
frequently encountered in industry. In this paper, we prese nt
DC2, a veriﬁcation methodology that enables scalable softw are
model checking.
DC2—which stands for Depth- Cutoff with Design
Constraints—uses scope bounding along with automatic
speciﬁcation inference andenvironment reﬁnement techniques.
Scope bounding limits the size of the generated model byexcluding functions that are deeply nested in the call graph ,
thereby enhancing scalability. Environment constraints restrict
the environment (global variables, unknown calling contex t,
and other external inﬂuences) at function interfaces. Function
stubs capture the effect of calls to library functions, missing
source code, or functions deemed outside the scope by DC2.
The environment constraints and function stubs for availab le
code are inferred automatically using a light-weight and
scalable whole program analysis called S PECTACKLE .
SPECTACKLE can infer constraints from pointer/array in-
directions and user-deﬁned assertions. Furthermore, it pr op-
agates (hoists) these constraints across program location s
within a function body, and across function calls. As a resul t,
constraints originating from deeply-nested function call s can
be automatically hoisted many levels higher in the call graph
to provide constraints at key interface functions. Scalabi lity
is ensured in S PECTACKLE by focusing on speciﬁc syntactic
speciﬁcation templates and using a hand-crafted interpro-
cedural static analysis for these templates. Note that our
main requirement for S PECTACKLE is scalability rather than
precision. In this respect, it is different from other recen t
work on automatic generation of preconditions [14, 23], sin ce
our inferred preconditions are used only to bootstrap the
application of a bit-precise and path-sensitive model chec ker
in the next phase.
The speciﬁcations generated by S PECTACKLE may be in-
complete. Furthermore, since aliasing is not treated sound ly
by S PECTACKLE , they may also be unsound. Therefore, a
model checker (even with a precise program model) can
produce false alarms due to lack of a precise environment.
To deal with such false alarms, we use an approach we call
CEGER (CounterExample-Guided Environment Reﬁnement).
It is inspired by CEGAR [10], where counterexamples are
used to guide reﬁnement of an abstract model. In CEGER, if
the user deems a reported witness as a false alarm, the model
checker is used to generate a reﬁnement of the environment
constraint, computed using a data-sliced weakest precondi tion
backwards over the witness trace. The reﬁned environment
constraints are used in subsequent runs of the veriﬁcation t ool,
to eliminate the previously found false alarms. The applica tion
of CEGER is quite useful in practice since it helps iterative978-1-4577-1639-3/11/$26.00 c2011 IEEE ASE 2011, Lawrence, KS, USA133reasoning over the calling environment and guides the analy sis
towards non-trivial bugs and fewer false alarms.
We have implemented DC2 as part of V ARVEL , a software
veriﬁcation tool developed by NEC. We present an experimen-
tal evaluation of DC2 on several open source and industry
software projects. Our results show that use of DC2 leads
to much fewer failures, enabling the application of softwar e
veriﬁcation techniques on large projects.
Contributions. In this paper, our main contribution is the
DC2 veriﬁcation framework that combines scope bounding
with automatic inference and reﬁnement of the environment
to enhance applicability of software model checkers on larg e
projects.
ËWe present light-weight speciﬁcation inference tech-
niques to automatically infer environment constraints and
function stubs.
ËWe present a counterexample-guided environment re-
ﬁnement approach (CEGER) to further reﬁne the en-
vironment constraints, triggered by a user classifying a
reported warning as a false alarm. The CEGER approach
can potentially leverage work done in examining a set of
false alarms for eliminating others in future runs.
ËWe present an experimental evaluation of DC2 on large
open source and industry software, using an implemen-
tation in V ARVEL , a tool developed by NEC and utilized
within a perpetual veriﬁcation environment.
Overview. The rest of the paper is organized as follows.
First, we provide some background on V ARVEL and its pro-
gram modeling in section II. We describe scope bounding
in section III and the global analysis for automatic spec-
iﬁcation inference in section IV. Section V presents our
counterexample-guided reﬁnement procedure for environme nt
constraints. In section VI, we report the experimental resu lts
on application of DC2. Section VII discusses some related
work. Section VIII concludes the paper.
II. B ACKGROUND : VARVEL AND MEMORY MODELING
VARVEL is a software veriﬁcation tool based on an earlier
research prototype called F-S OFT [18]. V ARVEL uses a com-
bination of abstract interpretation and model checking to ﬁ nd
common errors in C/C++ programs including pointer usage er-
rors, buffer overruns, C string errors, API usage violation s, and
violations of user-deﬁned assertions. At its core, V ARVEL uti-
lizes a bit-precise SAT-based bounded model checker (BMC),
which operates on a model automatically extracted from a
given program. The formulas generated by BMC [8] are solved
by a SAT solver to generate witnesses that correspond to
property violations. V ARVEL has been engineered to handle
numerous low-level aspects of C programs including pointer
arithmetic, dynamic memory allocation, function pointers , and
bitwise operations.
Scalability in V ARVEL is enhanced by staging various
analyses such that cheaper methods are used ﬁrst. It uses
abstract interpretation [13] on numerical domains of incre asing
precision (e.g., intervals, octagons [22]) to statically p rove
properties. Once a property is proved, it is removed fromfurther consideration. In addition, program slicing is use d to
remove portions of the program that are irrelevant to the
unresolved properties. In practice, roughly 60-80% of the
automatically instrumented checks in a program are rapidly
proved using static analysis, before the bounded model chec ker
is deployed on the rest. Thus, the number of properties to
check, as well as the model size is reduced across the differe nt
stages.
However, it still remains prohibitively expensive to perfo rm
whole program model checking for large programs. Therefore ,
VARVEL treats every function appearing in a given program
as a possible entry function. Although this helps to handle
some functions, challenges remain for functions with large
call graphs where the extracted models may become too large.
Furthermore, when a function other than main (or an interface
function) is treated as an entry point for veriﬁcation, the i nput
parameters and global variables are assumed to hold arbitra ry
values. In reality, however, these parameters are constrai ned
based on values set by its callers. Therefore, the model chec ker
may report a witness, where it assigns a value to an input that
cannot be realized in an actual execution, thereby resultin g
in a false alarm. The aim of DC2 is to enable effective
deployment of V ARVEL on large systems for ﬁnding more
bugs and reducing false alarms with minimal user interaction .
VARVEL supports different memory models and checkers
that use the same underlying analyses. Here, we focus primar -
ily on three of these, namely (a) a ﬁne-grained array-bounds
model to check for overﬂows, (b) a light-weight validity model
that handles bugs due to malloc failures, double-free and free
of statically allocated memory, and (c) a light-weight mode l
for detecting memory leaks.
A. Array Bounds Checking
The array-bounds model tracks pointer addresses, allocate d
bounds for each pointer, and the position of the null-termin ator
sentinel for strings. This model is along the lines of the
CSSV model [15] with some key differences, to reduce model
complexity and make it easier to analyze.
The bound [ptrLo (p),ptrHi (p)]represents the range of legal
values for pointer p, such thatpmay be dereferenced in our
model without causing an out-of-bounds violation . Ifp∈
[ptrLo (p),ptrHi (p)]thenp[i]underﬂows iffp+i<ptrLo (p).
Similarly,p[i]overﬂows iffp+i>ptrHi (p). By convention,
a pointerpis regarded invalid (uninitialized, null, freed, etc.)
whenever ptrHi (p)<ptrLo (p). Addresses associated with a
pointer do not correspond to the physical layout of memory
in a program; providing an address to a pointer allows us
to model pointer arithmetic and aliasing in an easier fashio n.
Fig. 1 illustrates the modeling attributes.
For statically allocated arrays, the bounds and addresses
are set to ﬁxed values at the start. Dynamically allocated
pointers are not provided with a priori ﬁxed addresses. Inst ead,
malloc calls are instrumented to assign a new address and
bound at every invocation of malloc .
Tracking String Lengths. A majority of the buffer overﬂows
in C result from the unsafe use of the standard library134char*s=malloc(10);
if (!s) exit(-1);
strcpy(s,"ASE2011");
char*t= s+3;MM+ 5M+ 10
\0
st
ptrLo(s) ptrLo(t)strLen (t) = 4
strLen (s) = 7ptrHi(s)ptrHi(t)
Fig. 1. The memory model for the array bounds model after succ essfully executing the four statements on the left-hand sid e: The successful allocation returns
a pointer to some new address M, and the lower bound addresses ptrLo (s) =ptrLo (t) =M. The higher bound addresses are ptrHi (s) =ptrHi (t) =M+9.
Finally, the string lengths are determined using the size ab straction, namely strLen (s) = 7 andstrLen (t) = 4 .
functions. We extend our memory model to check for such
buffer overﬂows along the lines of CSSV [15].
Corresponding to each character pointer p, we associate
a variable strLen (p)to track the position of the ﬁrst null-
terminator character starting from p. The updates to strLen (p)
are derived along the same lines as those for the pointer
bounds with the exception of assignments involving pointer
indirections, where updates to a string pointer are propaga ted
to potentially overlapping strings as well.
B. Pointer Validity Checking
The pointer-validity checker handles those aspects of buff er
overﬂow checking that do not require tracking of bounds ptrHi
andptrLo for pointers. The validity checker instruments each
pointer using a seven-valued monitor ptrVal (p)to denote the
state of a pointer at runtime: (a) null: a NULL pointer; (b)
invalid : a non-null invalid pointer, whose dereference may
cause a segmentation violation; (c) static : global variables,
arrays and static variables; (d) stack : local variables, alloca
calls, local arrays, formal arguments; (e) heap : dynamically
allocated memory on the heap; (f) code : code section, e.g.,
string constants; and (g) environment : input pointer parame-
ters.
Unlike the overﬂow checker, the validity checker does not
track addresses of pointers and ignores address arithmetic . A
pointer expression p+iisassumed to have the same validity
status as its base pointer p. At a dereference ∗p, our modeling
adds an assertion check that is violated if ptrVal (p)isnullor
invalid . In case of an assignment to ∗p, we may also report
a bug if ptrVal (p) = code . Calls tofree set the validity
monitor of the argument and its aliases to invalid . Finally,
leaving a functional scope changes pointers that are set to
stack toinvalid .
C. Memory Leak Checking
We also provide a stand-alone model for detecting memory
leaks. In comparison to a standard approach of combining
memory leak detection with memory safety checkers, our
model is very light-weight. Further, we reduce the model siz e
based on the observation that, to ﬁnd memory leaks, pointer
aliasing relationships need to be tracked only between poin ters
and allocation sites. That is, rather than having to track
quadratically many aliasing predicates between all pointe rs,
we simplify the model to track only linearly many aliasing
predicates. We omit further details for sake of brevity.III. S COPE BOUNDING
The basic idea behind scope bounding in DC2 is to cut off
function calls beyond a desired call depth in the call graph o f
a given entry function. Here, call depth is deﬁned as the leng th
of the shortest path from the entry function in the call graph .
After the cut-off, the function call is replaced by a stub that
abstracts its effect in terms of preconditions, post-condi tions,
and modiﬁed variables. DC2 is supported by a three-pronged
inference methodology:
(a) Infer constraints in the form of preconditions for each
function so that the calling environment at the entry
function can be captured.
(b) Infer stubs in the form of pre-conditions, post-conditi ons,
and summaries for functions that are cut off.
(c) Support reﬁnement of the inferred pre-conditions and po st-
conditions upon demand.
In our framework, the inference of pre/post-conditions and
stubs is supported by a simple whole program analysis called
SPECTACKLE . The reﬁnement process, called CEGER, em-
ploys an analysis around a counterexample generated by a
model checker and identiﬁed as a false alarm by the user.
The algorithm for function call rewriting in DC2 is shown
in Fig. 2.φentry andφgare the constraints and stub_gis the
stub inferred by S PECTACKLE . This scope bounding instru-
mentation is done as a preprocessing step, and is independen t
of the veriﬁcation performed later. The depth cutoff scheme
is illustrated in Fig. 3. Note that for the entry function of
the analysis, the precondition inferred by S PECTACKLE is
assumed, whereas it is asserted for other called functions.
The function his deemed outside the scope. Therefore, a stub
replacement h_stub is automatically created and the inferred
preconditions, post-conditions, and stubs are used in plac e of
h. The inference of preconditions, post-conditions, and stu b
functions to support DC2 are described in the next section,
while the reﬁnement process is described in Section V.
IV. S PECIFICATION INFERENCE
Scope bounding largely alleviates the issue of scalability ,
enabling veriﬁcation to cover large parts of the code by usin g
depth cutoff on each entry function. However, cutting off de ep
function calls or replacing them by over-approximate stubs
adversely impacts accuracy, leading to false bugs. Further , we
may miss bugs that occur outside the scope. To cope with
these issues, DC2 utilizes a light-weight global analysis c alled
SPECTACKLE . It has two main roles: (1) to generate and
hoist preconditions for functions, and (2) to generate stub s135Function depthCutoff (entry ,depth )
MapdepthMap : Func /mapsto→Integer.
Insertassume(φentry)at the start of entry .
for all functionsfin callgraph do
depthMap (f) := Length of the shortest path in
call-graph from entry tof.
for allfsuch thatdepthMap(f) =depth do
for all call-sitescin function fdo
letgbe the function called at c
ifdepthMap(g)>depth then
Insertassert(φg)beforec.
// Cut off function call c.
Replacecwith call to stub_g .
Fig. 2. Function call rewriting in DC2.
fun. f(..)
assume( pref)
call g1();
...
call g2();
fun. g1(..)
assert( preg1)
...
call h();
call h_stub();
fun. g2(..)
assert( preg2)
...fun. h(..)
...
fun. h_stub(..)
assert( preh)
inferred stub
assume( posth)
Fig. 3. A schematic illustration of depth cutoff in DC2 on a ca ll graph.
that capture important side-effects of functions. Both the se are
tailored speciﬁcally to the checkers in V ARVEL .
A. Preconditions
SPECTACKLE visits functions in a reverse topological order
with respect to the call graph of the program. For each
function, it hoists error conditions within the body to the s tart
of the function. Hoisting is done by back-propagating the
conditions across the statements. In addition, S PECTACKLE
also hoists preconditions of the callees to the start of the c aller.
Hoisted conditions are used for two purposes in the scope
bounding algorithm: (1) to constrain the inputs of an intern al
functionfwhen it is used as an entry function for veriﬁcation,
and (2) to assert the correctness of inputs to a callee g1of
f. Note, in particular, that an assertion check assert(φg)
will trigger a violation during model checking and generate
a witness if the hoisted condition for gis not correct. In
other words, we do not require S PECTACKLE to generate
correct preconditions. Instead, we attempt to automatical ly
generate likely preconditions. The witnesses generated by
VARVEL can be additionally used for manual reﬁnement of
the preconditions as described in Sect. V.In the following, we brieﬂy describe the preconditions that
SPECTACKLE generates for a given function.
Pointer validity: For every pointer pthat is dereferenced,
SPECTACKLE hoists the condition (p != NULL) to the
beginning of the function, provided pis not checked for null
along all paths that lead to the dereference. If the resultin g
condition involves a formal parameter or a global variable, it
will be retained as a precondition for the function.
Array bound: For every variable ithat is used as an index
expression for accessing a static array of size n, it back-
propagates the condition (0≤i< n )to the start of the
function. If the resulting condition depends only on the inp uts
to the function, it will be used as a precondition that constr ains
the range of the respective inputs.
Allocated heap size: If a dereferenced expression is an input
variable itself or is aliased with an input, S PECTACKLE tries
to capture the constraints on the size of heap area pointed-t o
by the expression by analyzing pointer arithmetic operatio ns.
For example, consider the following function:
void f(T *t1,int k)
{T*t2; t2 = t1 + k; *t2 = 7;}
From the expression *t2and the pointer arithmetic oper-
ationt2=t1+k , SPECTACKLE generates the following pre-
condition for f:t1+k∈[ptrLo (t1),ptrHi (t1)].
Field relation inference: SPECTACKLE captures speciﬁc
patterns of ﬁeld usage. For instance, we often see struct
deﬁnitions that have two ﬁelds, one pointing to the heap and
the other indicating the length of the heap as below:
struct S{char *buf; size_t buflen;};
SPECTACKLE automatically extracts such relations and
globally constrains the type of inputs.
Assertion hoisting: SPECTACKLE hoists assertions in the pro-
gram to the beginning of functions. Currently, S PECTACKLE
can hoist simple (linear equality or inequality) expressio ns
with some simpliﬁcation for the hoisted assertions. As a
heuristic, we drop non-linear assertions and assertions in volv-
ing complex conditional expressions.
Other Properties: SPECTACKLE includes support for infer-
ring null-terminator preconditions for strings. It can als o be
easily extended to infer other properties such as type state s.
B. Hoisting of Annotations
SPECTACKLE is designed to be fast and scalable even
for large software. Therefore, we avoid computing weakest
preconditions since it is typically expensive for the whole
program. This section brieﬂy discusses how annotations are
hoisted to the beginning of functions and across calls using
purely syntactic domain operations.
Letϕlbe a precondition annotation generated at a program
pointlinside a function f. Our goal is to compute an appropri-
ate precondition ψcorresponding to the entry point of function
f. To hoist an annotation ϕlat location l, SPECTACKLE
initalizeslwithϕland every other location with the condition
false .ψis computed by means of a backwards data-ﬂow136analysis that captures the effect of various assignments an d
conditional branches between program point land the entry
point off.
The backwards transfer function is the (weakest liberal)
precondition operator. Preconditions across assignment s tate-
ments are treated by substitution of the right-hand side in p lace
of the LHS expression. The pre operator is also deﬁned to
propagate a constraint ϕbackwards across a branch condition
c. Computing this operation involves the syntactic search fo r
a conjunct in ϕthat contradicts the branch condition c. If
such a conjunct is obtained, the precondition is set to false .
If, on the other hand, ϕcontains a conjunct that is identical to
c(syntactically), the result of the precondition is given by
true. Failing this, the precondition is set to ϕ, instead of
the more general constraint c⇒ϕ. This is done in part to
keep the syntactic form of the preconditions simple so that t he
dataﬂow analysis can be performed efﬁciently. On the other
hand, in some cases, the resulting precondition tends to be
stronger, resulting in a false alarm that may need reﬁnement
using CEGER. Formally,
pre(ϕ,c) =

false ifϕ“syntactically contradicts” c
true ifϕ“is identical to” c
ϕ otherwise
A join operator ( /unionsq) is used to merge two preconditions
ϕandψobtained from the two parts of a branch. The join
operator works by matching its operands syntactically. If o ne
of the operand syntactically matches false , the result is taken
to be the other operand. If ψcan be obtained by conjoining
some assertion ψ/primetoϕthen the join chooses the weaker
assertionϕ. Finally, if the operands do not fall into any of the
categories above, the result of the join is the trivial annot ation
true. Formally,
ϕ/unionsqψ=

ϕ ifψis syntactically false
ϕ (ψ≡ϕ∧ψ/prime)
ψ ifϕis syntactically false
ψ (ϕ≡ψ∧ϕ/prime)
true otherwise
When the analysis converges, the assertion labeling the ent ry
point of the function denotes the entry precondition ψ. Ifψ
is an assertion other than true orfalse , it can be propagated
to the callers of the function. If ψ=false , then a warning is
issued to the user.
Example 4.1: Consider function f1shown in Fig. 4(a).
The pointer dereference q[4]in line 3 of f1gives rise to two
annotations ϕ3andψ3. Consider the assertion ϕ3:q/negationslash=NULL .
Becauseϕ3is identical to branch condition c:q!=NULL
at line 2, the precondition pre(ϕ3,c)istrue. Note that the
assertionϕ4labelling line 4 is initially false . Therefore,
joining the contribution across the two branches at line 2,
yieldsϕ2:true. As a result, the annotation q/negationslash=NULL does
not yield a precondition for f1.
On the other hand, hoisting the annotation ψ3:q+
4∈[ptrLo (q),ptrHi (q)]produces the precondition p+ 4∈pre:p+ 4∈[ptrLo (p),ptrHi (p)]
void f1( int *p){
1: int*q = p;
2: if(q != NULL)
ϕ3:q/negationslash=NULL
ψ3:q+ 4∈[ptrLo (q),ptrHi (q)]
3:q[4] = 0;
4: ...
}pre:r/negationslash=NULL
void f2(int *p,
int*r){
1: int*q;
ψ2:r/negationslash=NULL
true
2: if(*r > 2)
ϕ3:p/negationslash=NULL
3: q = p;
else
ϕ4:r/negationslash=NULL
4: q = r;
ϕ5:q/negationslash=NULL
5:*q = 0;
}
(a) (b)
Fig. 4. Hoisting annotations across statements and branche s.
[ptrLo (p),ptrHi (p)]. In a case like this, we generate the
following precondition: if(p){p+ 4∈[ptrLo (p),ptrHi (p)]}.
Example 4.2: Consider function f2shown in Fig. 4(b).
The pointer dereference *qat line 5 gives rise to the annota-
tionϕ5. Similarly, the pointer dereference *rat line 2 gives
rise to the annotation ψ2. Annotation ϕ5can be hoisted across
the assignments in lines 3 and 4 yielding ϕ3:p/negationslash=NULL and
ϕ4:r/negationslash=NULL , respectively. The join operation at the branch
in line 2 yields the assertion true. As a result, annotation ϕ5
does not contribute to the precondition for f2. On the other
hand, when the annotation ψ2at line 2 is hoisted to the start
off2, we generate the preconditon r/negationslash=NULL forf2./squaresolid
In practice, a sound and complete precondition is not
strictly necessary. If the precondition is overly restrict ive,
it would lead to a violation at some call site. Similarly,
if the precondition is overly relaxed, it would cause false
alarms due to an under-speciﬁed environment. In practice, o ur
implementation of S PECTACKLE sacriﬁces soundness in its
handling of pointer indirections. Nevertheless, the numbe r of
unsound preconditions generated is very few in practice, an d
such instances are detected through witnesses generated by
our model checker.
C. Stub Generation
Recall that scope bounding in DC2 removes functions that
are nested deeply in the callgraph. Another practical issue is
that veriﬁers for large projects (especially commercial sy s-
tems) often need to work with incomplete code bases, because
the source code for many third-party libraries or modules is
unavailable. For veriﬁcation purposes, it would be useful t o
have a summary of the behavior of cut-off functions (for whic h
source code is available) as well as missing functions (for
which source code is unavailable). We use S PECTACKLE to
automatically generate stubs in both situations.
SPECTACKLE uses the following analyses to generate stubs
that model the side-effects of cut-off functions:
Mod-ref analysis: A conservative update to all variables
accessible in a cut-off function may generate too many false137alarms. Thus, S PECTACKLE conducts a light-weight mod-ref
(modiﬁcation and reference) analysis to ﬁnd variables that may
be modiﬁed. Then, it generates a stub that updates only those
variables that are modiﬁed within the function and its trans itive
callees.
Key effects extraction: Library function calls are important
for veriﬁcation. For instance, if a function calls free orexit
internally, this is captured in the stub.
The stubs for missing functions are generated according
to default environment assumptions (described in the next
section), including suitable preconditions for appropria te ini-
tializations and postconditions. In general, the default s tubs
generated by S PECTACKLE are not sound. They are designed
to avoid false alarms that arise frequently in practice, whi le
still exposing problematic situations. In addition, V ARVEL
provides a mechanism for the user to provide (or reﬁne
existing) stubs through use of Stub APIs. This is described
in the next section.
Example 4.3: We applied S PECTACKLE on an H.264 video
decoder software project comprising about 25kLOC. S PEC-
TACKLE ran in a matter of seconds and generated 1473
preconditions for all the functions. Overall, a majority ( 69%)
of the generated preconditions corresponded to direct poin ter
dereferences of the form ∗pwithout any indexing. A signif-
icant number ( 22%) corresponded to preconditions generated
due to indexing of statically allocated arrays with known si zes,
while the rest corresponded to indexing of pointers of unkno wn
sizes.
Applying V ARVEL on the H.264 project without the inferred
preconditions yielded 658witnesses, almost all of them being
false alarms. However, the number of witnesses was drastica lly
reduced to 30, when we used V ARVEL with the preconditions
generated by S PECTACKLE . Of these 30witnesses, 10were
plausible buffer overﬂows. /squaresolid
V. E NVIRONMENT CONSTRAINTS AND REFINEMENT
The preconditions and stubs described in Sect. IV are
collectively referred to as the environment . We use an ap-
proach we call CEGER (CounterExample-Guided Environ-
ment Reﬁnement) for reﬁning the environment. The basic
idea behind CEGER is to ﬁrst apply the model checker using
thedefault stubs and preconditions (e.g. those generated by
SPECTACKLE ). Afterwards, the user examines the witnesses
reported and decides which are false alarms. The environmen t
is reﬁned iteratively to avoid the false alarms encountered
in the previous iterations. We start by describing the defau lt
environment assumptions.
A. Default Environment Assumptions
SPECTACKLE makes some default assumptions about func-
tions when generating the preconditions and function stubs .
These assumptions are inspired by common use scenarios.
Tab. I shows some of the default assumptions about values of
input arguments, bounds of pointers, aliasing, and functio ns
whose code is not included in the analysis.TABLE I
AUTOMATIC DEFAULT ENVIRONMENT ASSUMPTIONS
Feature Default assumption
int/char arg. Nondet. value
Pointer Aliasing No aliasing of input argument pointers
Pointer Address Nondet.: NULL orp> 0.
Pointer Bounds Nondet.: Invalid or[ptrLo (p),ptrHi (p)]
String Length Nondet.: Invalid ornullTerm
Func. missing src. ptr. args are set to nondet.,
globals are not changed
p NULL
lo 0 hi
plo0 hi
p
lo0 hi
p
p FREED
(a) (b)
Fig. 5. Possible pointer conﬁgurations of an input string p: (a) part of default
assumptions, and (b) not permitted by the default assumptio ns.
Input pointers are nondeterministically set to be NULL ,
or to a valid pointer address and bounds. If pis cho-
sen to be valid, then pis set to a unique non-NULL ad-
dress, [ptrLo (p),ptrHi (p)]forms a valid range, and p∈
[ptrLo (p),ptrHi (p)]. The allocated length of pis set to be
positive but nondeterministic. Finally, strings are assum ed
to be null-terminated. Note that these assumptions do not
lead to a strictly sound treatment of the environment. For
example, it is possible to call a function pthat has a valid
base, but points outside its allocated region at the call sit e.
However, interfaces in C are seldom designed to handle such
cases. Fig. 5 highlights some cases allowed by the default
assumptions for a string input pointer p, and some other cases
that are not allowed.
Stub API. To aid the user in supplying and reﬁning
the environment using C/C++, V ARVEL supports a special
Stub API that provides access to auxiliary (instrumented)
variables introduced in the veriﬁcation models. These incl ude
variables such as ptrHi (p),ptrLo (p),strLen (p),ptrVal (p),
and so on. The APIs also support assertions, assumptions,
preconditions and assignments involving instrumented var i-
ables (to set up an initial environment). These allow the use r
to directly control the model and veriﬁcation in V ARVEL .
Indeed, the preconditions and stubs automatically generat ed
by S PECTACKLE utilize the same APIs, providing a common
interface for modeling the environment. In addition, V ARVEL
comes equipped with about 650 pre-deﬁned stubs for standard
libraries (e.g. strings), along with embedded assertions f or
different checkers.
Tab. II shows some of the Stub APIs. The assert function
is part of the C library. It is treated as a check. The assume
function blocks the execution unless the assumed expressio n138TABLE II
STUB API TO SPECIFY ENVIRONMENT
preCond (expr) C expr. expr is a precondition.
assert (expr) expr is asserted.
assume (expr) expr is assumed.
NONDET (e1,e2) A nondet. integer in [e1,e2].
setStrLen (p,e) Insert the assignment strLen (p) :=e.
setValidity (p,e) Assignment ptrVal (p) :=e.
is valid. The precondition function preCond has two different
meanings depending on where it is encountered. It is used as
an assumption when it is encountered at the entry function
of the analysis. If encountered in a non-entry function, it i s
treated as a check. This corresponds to the natural semantic s
of a precondition in annotation checking.
B. Counterexample-Guided Environment Reﬁnement
The CEGER framework uses a software model checker in
two ways:
1) The model checker provides a concrete witness trace
showing the control ﬂow and the data values at each
program point in the execution leading up to the error.
This trace aids the user considerably in determining
whether the witness is a false alarm.
2) The model checker suggests an environment constraint
to rule out a false alarm, by propagating weakest pre-
conditions backwards along the trace, starting from the
violation and ending at the function interface. In our
experience, users ﬁnd it considerably easier to improve
upon (e.g. generalize) a suggested constraint rather than
derive a suitable constraint from scratch.
We consider reﬁnement of the preconditions for each
function. Let ϕfbe the existing environment precondition
corresponding to a function fin the code. For each witness w,
letψwbe the weakest precondition computed at the function
interface starting from the assertion violation. When a wit ness
wis marked as a false alarm by the user, there are two possible
diagnoses:
•In the witness, starting from function f, we observe an
assertion violation in the code that is dependent on the
input environment. In this case, ϕfis weak. We term
such witnesses as Type 1 witnesses.
•In the witness, starting from another function g, the
precondition ϕf(treated as an assertion) is violated when
callingf. In this case, either ϕfis strong or ϕgis weak.
We term such witnesses as Type 2 witnesses.
To eliminate a Type 1 witnessw, we reﬁne the precondition
ϕffor the entry function fby conjoining it with ¬ψw:ϕ/prime
f=
ϕf∧ ¬ψw. This sufﬁces to eliminate wfrom future iterations
of the veriﬁer. In many cases, this also rules out other relat ed
witnesses to the same assertion from alternative program pa ths.
However, note that ϕ/prime
fis presented to the user as a suggestion.
In many cases, the user may be able to rule out additional fals e
witnesses by weakening ϕ/prime
ffurther.
To handle a Type 2 witness, we re-run the analysis starting
from function gand including the code for function f, butTABLE III
DESCRIPTIONS OF BENCHMARKS .
name version LOC (analyzed) #Functions
thttpd 2.25b 6.7K 145
GenericNQS 3.50.10-pre1 15.1K 253
libupnp 1.6.6 17.9K 363
Product P1 — 54.5K 408
Product P2 — 143.6K 727
TABLE IV
RESULTS OF DC2 EXPERIMENTS . Success ratio reports the percentage of
functions successfully veriﬁed within the given timeouts. Likely Bugs
reports the number of bugs that were communicated to develop ers with
numbers in parenthesis representing the number of interpro cedural bugs.
Success Ratio #Likely Bugs
w/o DC2 w/ DC2 w/o DC2 w/ DC2
thttpd 68% 96% 0 (0) 5 (3)
GenericNQS 82% 94% 6 (2) 6 (2)
libupnp 81% 98% 8 (0) 19 (8)
Product P1 89% 96% 8 (1) 11 (6)
Product P2 88% 91% 14 (3) 22 (9)
excluding the assertion (from the precondition) for f. If the
analysis succeeds in producing a violation inside function f
that is deemed to be false by the user, then ϕgis weak.
Therefore,f’s precondition is used to reﬁne ϕgas a Type-
1witness. If the analysis fails, the user may choose to treat
the original witness was a Type-1 witness to reﬁne ϕg, or
relax the precondition of fsuch that [ [ϕ/prime
f] ]⊇[ [ϕf] ]∪s, where
sis the concrete state in the witness causing the violation of
ϕf. Such a relaxation will also eliminate the witness wfrom
future iterations of the veriﬁer.
Note that since the possibility of a witness being false is
arbitrated by the user, mistakes by the user can lead to misse d
bugs. In particular, the user may introduce a fallacious env i-
ronment assumption, such as ϕf:false , proving all properties
trivially. The static analysis can detect such cases and war n
the user. Although CEGER is not completely automatic, it is
nevertheless quite effective in our experience at enabling users
to discover real bugs on large code bases.
VI. E XPERIMENTS
This section describes experiments conducted on open
source and proprietary software projects. First, we highli ght
the usage of DC2 on several benchmarks and show the effec-
tiveness of scope-bounded model checking to ﬁnd interestin g
interprocedural bugs. Then, we present some experiments on
counterexample-guided environment reﬁnement.
A. DC2 Experiments
We applied V ARVEL to ﬁve benchmarks, including two
industry programs from NEC, with and without DC2. The
description of the benchmarks is shown in Tab.III. Due to tim e
limitations, especially for investigating results, we dec ided to
employ only a part of each benchmark for the experiments.
The column LOC shows the sizes of modules analyzed by
VARVEL . Product P1is a developer tool whose original LOC is
100kand product P2is a business application software whose
original LOC is 1400 k.
For the experiments, the DC2 depth was set to 1, i.e.,
each scope consisted of two levels of function calls in the139urlconfig.c:390-396
L1 err_code = config_description_doc( doc, ipaddr_port, & root_path );
L2if( err_code != UPNP_E_SUCCESS ) { gotoerror_handler;}
L3 err_code = calc_alias( alias, root_path, &new_alias );
urlconfig.c:203-340
intconfig_description_doc(IXML_Document *doc,const char *ip_str, char**root_path_str){
...
L4 err_code = ixmlNode_appendChild( rootNode, ( IXML_Node *) element );
L5if( err_code != IXML_SUCCESS ) { gotoerror_handler;}
L6 textNode = ixmlDocument_createTextNode( doc, ( char*)url_str.buf );
L7if( textNode == NULL ) { gotoerror_handler;}
...//*root_path_str is updated here.
error_handler:
L8if( err_code != UPNP_E_SUCCESS ) { ixmlElement_free( newElem ent );}
...
L9returnerr_code;}
Fig. 6. A subtle inter-procedural bug in libupnp-1.6.6.
call graph, and we did not change any preconditions or stubs
generated automatically by S PECTACKLE . The analysis was
performed with a time bound of 800s, and the results are
shown in Tab. IV. As expected, we observed that the success
ratio (percentage of functions successfully veriﬁed withi n an
allotted time) improved with DC2 for all benchmarks. Clearl y,
scope bounding enabled application of V ARVEL on even
those functions with large call graphs. Further, the number
of detected bugs also increased with DC2.
Interestingly, many of the bugs found without DC2 were
intraprocedural. On the other hand, V ARVEL was able to ﬁnd
deep interprocedural bugs with DC2. One such (previously
unknown) bug in libupnp is shown in Fig. 6. It was only
found when we utilized DC2. The variable root_path is
supposed to be initialized by config_description_doc
atL1. If the initialization fails, it is designed to jump
to an error handler at L2. However, V ARVEL found a
case where the initialization fails and the returned er-
ror code is UPNP_E_SUCCESS . The witness generated by
VARVEL indicates that if IXML_SUCCESS is assigned to
err_code atL4andixmlDocument_createTextNode
returnsNULL , then the execution will proceed to the er-
ror handler without changing err_code .IXML_SUCCESS
andUPNP_E_SUCCESS have the same value of 0. Thus,
uninitialized root_path will be passed as a parameter
to thecalc_alias function (L3) which checks that the
corresponding parameter is not NULL . Note this bug is pro-
duced even without analyzing ixmlNode_appendChild and
ixmlDocument_createTextNode , which are out of scope
of the veriﬁcation run.
B. CEGER Experiments
In this section, we only present V ARVEL experiments that
do not utilize S PECTACKLE . This allows us to highlight the
advantage of CEGER independently of DC2. However, this
also means that these experiments do not take full advantage
of DC2 and thus result in an increased rate of false alarms.TABLE V
DATA FROM INITIAL RUN OF THE ZITSER ET AL . BENCHMARKS .LEGEND :
AI: ABSTRACT INTERPRETATION ;MC: M ODEL CHECKING ,PRF.:
PROOFS ,WIT.: W ITNESSES ,T/O: TIMEOUT.
Code size (kLOC) Analysis Result (# Properties)
Total Avg Max Total AI-Prf. MC-Prf. Wit. T/O
46 1.9 4.5 3281 2155 4 90 500
TABLE VI
ANALYSIS OF WITNESSES FROM ZITSER ET AL . BENCHMARKS
Status #Wit(1) #Wit(2)
Plausible Bugs 7 10
Missing function getopt ,optarg , . . . 25 0
Missing function dn_skipname 12 0
Missing functions setpwent ,getpwent 43 0
Modeling limitation: Array/string elements 3 3
Zitser Benchmark Suite. We ran V ARVEL on some public
benchmarks put forth by Zitser et al. to evaluate the perfor-
mance of academic and commercial static analysis tools [28] .
The benchmark suite consists of 25programs, which form a
part of a larger open source application with known overﬂow
bugs involving arrays and strings. We allowed each instance
to run for 1800s. The programs themselves range in size
from 1−5kLOC (after preprocessing, forward slicing and
instrumentation). Table V shows the results of running the
analysis on these examples. Note that we are able to prove
a majority ( ∼65%) of the properties statically. The model
checker produces 90concrete violations. Each violation was
manually examined by one of the authors. Of the 90witnesses
found, 7were found to be real violations based on the witness.
The remaining 83were classiﬁed into 4categories. Table VI
shows an analysis of these violations ( #Wit(1) ). Note that
missing preconditions are not a problem for these benchmark s,
since they contain drivers that initialize the environment .
Indeed, a large number of false positives arose due to missin g
functions that did not have any stubs. Overall, about one man
hour was spent in analyzing all the witnesses in the ﬁrst run.140TABLE VII
RESULT DATA FOR TWO INDUSTRIAL CASE STUDIES USING CEGER
#Func- Witnesses #Pre- #Stub Bugs
kLOC tions R1 R2 conds PV AB
P3 24 188 211 19 17 6 5 2
P4 70 654 951 181 7 3 6 3⋆
Our subsequent iteration included stubs for the missing
functions. The stubs were created by consulting the descrip tion
of function behavior speciﬁed by the manuals. Table VI shows
the results of the second iteration ( #Wit(2) ). This iteration
results in a total of 13witnesses, of which 10were found to
be plausible.
Industrial Benchmarks. Next, we applied V ARVEL on a
small embedded software application (in industrial use) wi th
6kLOC. The application lacks a single “ main ” function.
Therefore, we ran the analysis on each entry function in
the source. No partitioning was required for this applicati on.
We ﬁrst used the pointer validity checker. The check ran in
60minutes and produced 15witnesses in all. One of the
authors analyzed all 15witnesses in 30minutes. It yielded one
plausible bug, where the result of a malloc was dereferenced
without a check. We formulated preconditions based on the
other witnesses. These preconditions covered both pointer and
array overﬂow checks. Overall, the entire analysis involve d
3rounds of the validity checker and 4rounds of the array
overﬂow checker, requiring 200minutes to run. The resulting
witnesses revealed 7bugs overall.
We helped in conducting case-studies on two larger em-
bedded software projects P3and P4. The study involved
the use of V ARVEL by two engineers who received tool
training and documentation, but were not veriﬁcation exper ts.
Each study was time limited: 6person days for P3and 7
person days for P4. Table VII summarizes the results. Each
analysis carried out two iterations with the pointer validi ty
checker followed by two iterations using the more complex
array bounds checker. The ﬁrst iteration used the default
environment assumptions. Subsequently, the witnesses wer e
all examined, and preconditions and stubs were written for t he
next iteration. Table VII shows the number of witnesses in th e
ﬁrst round using default environment assumptions ( R1), and
the number of witnesses after CEGER ( R2). It also classiﬁes
the environment assumptions into preconditions and missin g
stubs written by the engineers. These preconditions and stu bs
are shared by multiple entry functions. Finally, we report o n
the number of real bugs for pointer validity ( PV) and array
overﬂows ( AB) (only about a quarter of AB witnesses were
examined by the engineers).
VII. R ELATED WORK
VARVEL uses abstract interpretation and bit-precise
bounded model checking for analysis. Approaches based on
abstract interpretation [13] have been used in tools such as
PolySpace [2],Astrée [9],C Global Surveyor [27]. These tools
focus on checking embedded applications with special featu res
such as simple aliasing, no dynamic allocation, simple cont rol
ﬂow and no recursion. However, our approach is designed tobe more general purpose. The CBMC tool due to Clarke et
al. [11] also uses bit-precise bounded model checking, but d oes
not use abstract interpretation and has limited scalabilit y. The
CoVerity veriﬁer [1] has been successfully applied to large
industrial as well as open source projects, but an experimen tal
comparison is outside the scope of this paper.
Abstraction Reﬁnement. CEGAR (CounterExample Guided
Abstraction Reﬁnement) was proposed and successfully used
in many efforts [6, 10, 20]. In particular, it has been used wi th
predicate abstraction and reﬁnement in the SLAM project,
with continuing improvements in industry applications [7] .
Our CEGER approach is very much inspired by CEGAR.
Whereas other CEGAR efforts use spurious counterexamples
to reﬁne the abstract model of the program, we use them only
to reﬁne the calling environment of the function being veriﬁ ed
or to model missing functions. CEGER is not completely
automated, and relies upon the user to determine whether the
reported witness is a false alarm.
Scope Bounding. Several approaches utilize scope bounding
to achieve better scalability in veriﬁcation [5, 21, 24, 25] .
Taghdiri and Jackson proposed a CEGAR-like reﬁnement-
based method to ﬁnd bugs in Java programs [25]. It iterativel y
expands the scope of called functions by utilizing informat ion
from counterexamples, and continues until it ﬁnds a proof or
a witness that does not rely on any unconstrained value. Babi ´c
and Hu proposed structural abstraction [5] that gradually
relaxes the boundary of veriﬁcation by inserting function s um-
maries on demand for a given property. In contrast, DC2 works
for all properties within a statically-determined scope. I nstead
of expanding the scope iteratively, it reﬁnes the precondit ions
and stubs for cut-off functions, based on witness traces.
Automatic Environment Generation. Recently, numerous
formalisms have focused on automatically generating model s
of the environment, based on automatic sound abstraction [2 6],
speciﬁcation mining [4], automata learning [3], and so on. O ur
environment generation is based on a light-weight whole pro -
gram analysis, where we are occasionally unsound. The main
motivation is to suppress false alarms and to capture condit ions
spanning deeply-nested function calls to ﬁnd interprocedu ral
bugs.
Automatic Contract Inference. Techniques for synthesizing
contracts automatically from pointer indirections and ass er-
tions in the code have also been proposed by Moy and
Marche [23] and by Cousot et al. [14]. Our approach for
precondition synthesis is similar in spirit, but differs ra dically
on the choice of abstract domains: (a) our approach is mostly
syntactic in nature, based on matching expressions in the co de,
and (b) our handling of pointer indirection is simple, albei t un-
sound in theory. However, the preconditions inferred seem t o
be accurate in practice. Furthermore, a wrong precondition can
be detected by V ARVEL as a type-1 or a type-2 witness. Our
simpler approach can infer preconditions for large project s,
with 500kLOC and beyond, in a matter of minutes. This design
choice works well in combination with precise modeling in ou r
model checker.141Annotation Checking. Our work also relates to annotation
checking tools that utilize function interface speciﬁcati ons, e.g.
ESC/Java [16] and more recently VCC [12]. Although these
efforts can handle complex speciﬁcations, they require sig nif-
icant manual effort to write them, and support for automatic
annotation inference or reﬁnement is largely absent. Tools such
as SALInfer [17] provide some automated annotation inferen ce
for runtime errors. In contrast to standard annotation chec king,
note that DC2 handles multiple levels of function calls in
the analysis, thereby utilizing more precise interprocedu ral
contexts for ﬁnding bugs.
VIII. C ONCLUSIONS
We presented the DC2 framework for program analysis sup-
ported by automated speciﬁcation inference. Our experimen tal
results for DC2 in V ARVEL support our experience that a soft-
ware model checker can accommodate the requirements from
industry by carefully designing and engineering its applic ation.
We are investigating other directions to improve DC2, such a s
adaptively tuning scope-bounding based on program metrics
and prior veriﬁcation runs.
IX. A CKNOWLEDGMENTS
We would like to acknowledge the assistance and support
of Shinichi Iwasaki-san and Fusako Mitsuhashi-san from NEC
Corporation, and Mustafa Hussain and Naveen Sharma from
NEC HCL Systems Technologies during the development of
VARVEL .
REFERENCES
[1] CoVerity Inc. program veriﬁer. www.coverity.com.
[2] PolySpace program analysis tool. www.polyspace.com.
[3] R. Alur, P. ˇCerný, P. Madhusudan, and W. Nam. Synthesis of interface
speciﬁcations for java classes. In Proc. POPL , pages 98–109. ACM
Press, 2005.
[4] G. Ammons, R. Bodík, and J. R. Larus. Mining speciﬁcation s. In POPL ,
pages 4–16, 2002.
[5] D. Babi ´c and A. J. Hu. Structural abstraction of software veriﬁcati on
conditions. In CAV, 2007.
[6] T. Ball, R. Majumdar, T. Millstein, and S. Rajamani. Auto matic predicate
abstraction of C programs. In PLDI’01 , pages 203–213. ACM Press,
2001.
[7] T. Ball, E. Bounimova, R. Kumar, and V . Levin. Slam2: Stat ic driver
veriﬁcation with under 4% false alarms. In FMCAD , pages 35–42, 2010.[8] A. Biere, A. Cimatti, E. M. Clarke, and Y . Zhu. Symbolic mo del
checking without BDDs. In TACAS , pages 193–207, 1999.
[9] B. Blanchet, P. Cousot, R. Cousot, J. Feret, L. Mauborgne , A. Miné,
D. Monniaux, and X. Rival. A static analyzer for large safety -critical
software. In PLDI , volume 548030, pages 196–207. ACM, 2003.
[10] E. Clarke, O. Grumberg, S. Jha, Y . Lu, and H. Veith. Count erexample-
guided abstraction reﬁnement. In CAV, pages 154–169, 2000.
[11] E. Clarke, D. Kroening, and F. Lerda. A tool for checking ANSI-C
programs. In TACAS , 2004.
[12] E. Cohen, M. Dahlweid, M. A. Hillebrand, D. Leinenbach, M. Moskal,
T. Santen, W. Schulte, and S. Tobies. VCC: A practical system for
verifying concurrent C. In TPHOLs . Springer, 2009.
[13] P. Cousot and R. Cousot. Abstract Interpretation: A uni ﬁed lattice model
for static analysis of programs by construction or approxim ation of
ﬁxpoints. In POPL , pages 238–252, 1977.
[14] P. Cousot, R. Cousot, and F. Logozzo. Precondition infe rence from
intermittent assertions and application to contracts on co llections. In
VMCAI . Springer, 2011.
[15] N. Dor, M. Rodeh, and M. Sagiv. CSSV: Towards a realistic tool for
statically detecting all buffer overﬂows in C. In Proc. PLDI . ACM Press,
2003.
[16] C. Flanagan, K. R. M. Leino, M. Lillibridge, G. Nelson, J . B. Saxe, and
R. Stata. Extended static checking for Java. In PLDI , pages 234–245,
2002.
[17] B. Hackett, M. Das, D. Wang, and Z. Yang. Modular checkin g for buffer
overﬂows in the large. In ICSE , 2006.
[18] F. Ivan ˇci´c, I. Shlyakhter, A. Gupta, M. Ganai, V . Kahlon, C. Wang, and
Z. Yang. Model checking C programs using F-Soft. In ICCD . IEEE,
2005.
[19] R. Jhala and R. Majumdar. Software model checking. ACM Comput.
Surv. , 41(4), 2009.
[20] R. Kurshan. Computer-aided Veriﬁcation of Coordinating Processes:
the automata-theoretic approach . Princeton University Press, 1994.
[21] A. Loginov, E. Yahav, S. Chandra, S. Fink, N. Rinetzky, a nd M. G.
Nanda. Verifying dereference safety via expanding-scope a nalysis. In
ISSTA , 2008.
[22] A. Miné. The octagon abstract domain. In WCRE , 2001.
[23] Y . Moy and C. Marché. Modular inference of subprogram co ntracts for
safety checking. Symbolic Computation , 45, 2010.
[24] D. Shao, S. Khurshid, and D. E. Perry. An incremental app roach to
scope-bounded checking using a lightweight formal method. InFM,
2009.
[25] M. Taghdiri and D. Jackson. Inferring speciﬁcations to detect errors in
code. ASE, 14(1):87–121, 2007.
[26] O. Tkachuk, M. B. Dwyer, and C. Pasareanu. Automated env ironment
generation for software model checking, 2003.
[27] A. Venet and G. P. Brat. Precise and efﬁcient static arra y bound checking
for large embedded C programs. In PLDI , pages 231–242. ACM Press,
2004.
[28] M. Zitser, R. Lippmann, and T. Leek. Testing static anal ysis tools using
exploitable buffer overﬂows from open source code. In SIGSoft/FSE .
ACM, 2004.142