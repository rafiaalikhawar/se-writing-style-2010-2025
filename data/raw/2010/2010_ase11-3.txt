Towards More Accurate Retrieval of Duplicate Bug
Reports
Chengnian Sun, David Loy, Siau-Cheng Khoo, Jing Jiangy
School of Computing, National University of Singapore
ySchool of Information Systems, Singapore Management University
suncn@comp.nus.edu.sg, davidlo@smu.edu.sg, khoosc@comp.nus.edu.sg, jingjiang@smu.edu.sg
Abstract —In a bug tracking system, different testers or users
may submit multiple reports on the same bugs, referred to as
duplicates, which may cost extra maintenance efforts in triaging
and ﬁxing bugs. In order to identify such duplicates accurately,
in this paper we propose a retrieval function (REP) to measure
the similarity between two bug reports. It fully utilizes the
information available in a bug report including not only the
similarity of textual content in summary and description ﬁelds,
but also similarity of non-textual ﬁelds such as product ,com-
ponent ,version , etc. For more accurate measurement of textual
similarity, we extend BM25F – an effective similarity formula in
information retrieval community, specially for duplicate report
retrieval. Lastly we use a two-round stochastic gradient descent
to automatically optimize REP for speciﬁc bug repositories in a
supervised learning manner.
We have validated our technique on three large software
bug repositories from Mozilla, Eclipse and OpenOfﬁce. The
experiments show 10–27% relative improvement in recall rate@k
and 17–23% relative improvement in mean average precision over
our previous model. We also applied our technique to a very large
dataset consisting of 209,058 reports from Eclipse, resulting in a
recall rate@k of 37–71% and mean average precision of 47%.
I. INTRODUCTION
Due to complexities of software systems, software bugs
are prevalent. To manage and keep track of bugs and their
associated ﬁxes, bug tracking system like Bugzilla1has been
proposed and is widely adopted. With such a system, end users
and testers could report bugs that they encounter. Developers
could triage, track, and comment on the various bugs that are
reported.
Bug reporting however is an uncoordinated distributed pro-
cess. End users and testers might report the same defects many
times in the bug reporting system. This causes an issue as
different developers should not be assigned the same defect.
Figuring out which bug reports are duplicate of others is
typically done manually by a person called the triager . The
triager would detect if a bug report is a duplicate; if it is,
the triager would mark this report as a duplicate report and
the ﬁrst report as the master report. This process however is
not scalable for systems with large user base as the process
could take much time. Consider for example Mozilla; in 2005,
it was reported that “everyday almost 300 bugs appear that
need triaging. This is far too much for only the Mozilla
programmers to handle” [1].
1http://www.bugzilla.org/To address this issue there have been a number of studies
that try to partially automate the triaging process. There are
two general approaches: one is by ﬁltering duplicate reports
preventing them from reaching the triagers [2], the other is
by providing a list of top- krelated bug reports for each new
bug report under investigation [3]–[6]. In this study, we focus
on the second approach for the following reasons. The ﬁrst
approach is more difﬁcult and the state-of-the-art approach
proposed in [2] is only able to remove 8% of the duplicate
reports. The remaining 92% of the duplicate bug reports would
still require manual investigation. Furthermore, duplicate bug
reports are not necessarily bad. Bettenburg et al. notice that
one bug report might only provide a partial view of the defect,
while multiple bug reports can complement one another [7].
Thus, in this study, we focus on providing a technique that
could help in linking bug reports that are duplicate of one
another.
Unfortunately, the accuracy of proposed techniques for
duplicate bug report detection through ranking [3]–[6] is still
low. In this work, we show an alternative approach with the
goal of improving the accuracy of existing techniques. With
a more accurate technique, triagers could be presented with
better candidate duplicate bug report lists which would make
his/her job easier.
Our approach is built upon BM25F model which is studied
not long ago in the information retrieval community [8], [9].
BM25F is meant for facilitating the retrieval of documents
relevant to a short query. In duplicate bug report detection,
given a bug report (which is a rather lengthy query), we
would like to retrieve related bug reports. We extend BM25F
model to better ﬁt duplicate bug report detection problem by
extending the original model to better ﬁt longer queries.
Studies in [3], [5], [6] process only natural language text of
the bug reports to produce a ranking of related bug reports. In
this study we consider not only text but also other features that
are available in BugZilla, e:g:, the component where the bug
resides, the version of the product, the priority of the report.
Wang et al. also include execution traces as information to
predict duplicate bug reports [4]. We do not use execution
traces in this study as they are hard to get and are often
unavailable in typical bug reports.
We evaluate our approach on several large bug report
datasets from large open source projects including Mozilla, a
software platform hosting several sub-projects such as Firefoxbrowser and Thunderbird email client, Eclipse, a popular open
source integrated development environment, and OpenOfﬁce, a
well-known open source rich text editor. For Eclipse we also
consider a larger dataset that includes 209,058 bug reports.
In terms of the types of programs and the number of bug
reports considered for evaluation, to the best of our knowledge,
our study is larger than any existing duplicate bug report
detection studies in the literature. We show that our technique
could result in 10–27% relative improvement in recall rate@k
and 17-23% in mean average precision over state-of-the-art
techniques [5], [6].
We summarize our contributions as follows:
1)We propose a new duplicate bug report retrieval model
by extending BM25F . We engineer an extension of
BM25F to handle longer queries.
2)We make use of more information, that is easily avail-
able in bug tracking system (BugZilla), as compared to
existing studies that rank bug reports. We use not only
textual content of bug reports but also other categorial
information including priority ,product version ,etc.
3)We are the ﬁrst to analyze the applicability of duplicate
bug report detection techniques on a total of more than
350,000 bug reports across bug repositories of vari-
ous large open source programs including OpenOfﬁce,
Mozilla, and Eclipse.
4)We improve the accuracy of state-of-the-art automated
duplicate bug detection techniques by 10-27% in recall
rate@k(1≤k≤20) and 17-23% in mean average
precision .
The paper is organized as follows. Section II presents some
background information on duplicate bug reports, duplicate
bug reports retrieval, BM25F and the optimization of ranking
functions. Section III presents our approach to retrieve similar
bug reports for duplicate bug report detection. Section IV
describes our case study on more than 350,000 bug reports
to show the utility of our proposed approach in improving the
accuracy of existing approaches. Section V discusses related
work, and ﬁnally, Section VI concludes and describes some
potential future work.
II. B ACKGROUND
This section covers necessary background information. It
ﬁrst discusses duplicate bug reports, then describes the general
workﬂow to retrieve duplicate bug reports. Next it introduces
BM25F – an effective textual similarity measure in infor-
mation retrieval area, and lastly describes how to tune free
parameters in a similarity function in order to achieve better
performance.
A. Duplicate Bug Reports
A bug report serves multiple functions. It is used to ﬁle
defects, propose features and record maintenance tasks. A
bug report consists of multiple ﬁelds. The ﬁelds in different
projects may vary to some extent, but in general they are
similar. Table I lists the ﬁelds of interest to us in OpenOfﬁce
bug reports. Fields summary anddescription are in naturallanguage text, and we refer to them as textual features , whereas
the other ﬁelds try to characterize the report from other
perspectives and we refer to them as non-textual features or
categorial features .
TABLE I
FIELDS OF INTEREST IN AN OPENOFFICE BUGREPORT
Field Description
Summ Summary : concise description of the issue
Desc Description : detailed outline of the issue, such as what is the
issue and how it happens
Prod Product : which product the issue is about
Comp Component : which component the issue is about
Vers Version : the version of the product the issue is about
Prio Priority : the priority of the report, i:e:; P1,P2,P3,  
Type Type: the type of the report, i:e:,defect ,task,feature
In a software project, its bug tracking system is usually
accessible to testers and even to all end users. Once a bug
manifests, people can submit a bug report depicting the detail
of the bug. However multiple reports from different submitters
may correspond to the same bug, which causes the problem
of duplicate bug reports. In this situation, the triager needs to
label the duplicate reports as duplicate and add a link to the
ﬁrst report about the bug. We refer to the ﬁrst report as master
and the other duplicate ones as duplicate .
Table II shows three pairs of duplicate reports in Issue
Tracker of OpenOfﬁce with the ﬁelds in Table I. The descrip-
tion of each report is not shown as it is long. As we can
see, the two reports in each pair are similar in not only the
summary ﬁeld but also other ﬁelds. For example, the second
pair has the same component ,priority andtype, and adjacent
version in chronological order. This observation motivates us
to consider these categorial features for duplicate bug report
retrieval.
B. Workﬂow for Retrieving Duplicate Bug Reports
This section brieﬂy describes the workﬂow to retrieve
duplicate bug reports. More detail has been discussed in our
previous paper [5].
In a duplicate report retrieval system, the ﬁelds summary
and description of both existing and new bug reports are
preprocessed by standard information retrieval techniques,
i:e:; tokenization ,stemming andstop work removal . Figure 1
depicts the overall ﬂow. The bug repository is organized as
a list of buckets – a hashmap-like data structure. The key
of each bucket is a master report, and its value is a list of
duplicate reports on the same bug. Each bucket has a distinct
bug as its master report is not contained in other buckets.
When a fresh bug report Qis submitted, the system computes
the similarity between Qand each bucket, and returns K
master reports, whose buckets have the top- Ksimilarities. The
similarity of a report and a bucket is the maximum similarity
between the report and each report in the bucket computed by
the component Similarity Measure in Figure 1.
In our previous work, we used a support vector machine
(SVM) to train a model measuring the similarity between
two reports [5]. The similarity is measured based on onlyTABLE II
EXAMPLES OF DUPLICATE BUGREPORTS FROM OPENOFFICE ISSUE TRACKER
Pair ID Summary Product Component Version Priority Type
185064 [Notes2] No Scrolling of document content by use of the mouse wheel Word Code 680m240 P3 Feature
85377 [CWS notes2] unable to scroll in a note with the mouse wheel Word UI 680m240 P3 Defect
285502 Alt+<letter> does not work in dialogs None UI OOH680m4 P3 Defect
85819 Alt-<key> no longer works as expected Framework UI OOH680m5 P3 Defect
385487 connectivity: evoab2 needs to be changed to build against changed api Database None OOH680m4 P1 Defect
85496 connectivity fails to build (evoab2) in m4 Database None OOH680m4 P2 Defect
Fig. 1. Overall Workﬂow for Retrieving Duplicates
textual features: summary anddescription . While this approach
has been shown to be effective, we believe it can be fur-
ther improved. Thus in this paper we propose a new and
comprehensive similarity function to substitute the component
Similarity Measure . The other parts of the retrieval process
remain unchanged.
C. BM25F
BM25F is an effective textual similarity function for struc-
tured document retrieval [8], [9]. A structured document is
composed of several ﬁelds ( e:g:; summary anddescription in
a bug report) with possibly different degrees of importance.
Given a document corpus DofNdocuments, each document
dconsists of Kﬁelds, and the bag of terms in the f-th ﬁeld
can be denoted by d[f]where 1≤f≤K.
The deﬁnition of BM25F is based on two components. The
ﬁrst is the inverse document frequency (IDF) deﬁned in (1),
which is a global term-weighting scheme across documents,
IDF (t) =logN
Nd(1)
where Ndis the number of documents containing the term t.
The other component is the local importance measure TFD
of a term tin a document ddeﬁned in (2), which is an
aggregation of the local importance of tin each ﬁeld of d.
TFD(d; t) =K∑
f=1wf×occurrences (d[f]; t)
1−bf+bflengthf
average _lengthf(2)
For each ﬁeld f,wfis its ﬁeld weight; occurrences (d[f]; t)
is the number of occurrences of term tin ﬁeld f;length fis the size of the bag d[f],average _length fis the average
size of the bag d[f]across all documents in D, and bfis
a parameter (0≤bf≤1)that determines the scaling by
ﬁeld length: bf= 1corresponds to full length normalization,
while bf= 0corresponds to term weight not being normalized
by the length. (Length normalization of term weights is to
mitigate the advantage that long documents have in retrieval
over short documents.)
Based on the two components above, given a query qwhich
can be a bag of words or a document, the BM25F score of a
document dandqis computed as follows,
BM25F (d; q) =∑
t2d\qIDF (t)×TFD(d; t)
k1+TFD(d; t)(3)
In (3), tis the shared term occurring in both dandq, and
k1(k1≥0)is a tuning parameter to control the effect of
TFD(d; t). There is a set of free parameters to tune for BM25F
to work most effectively for a certain document corpus, i:e:,
wfandbffor each ﬁeld f, and k1. With Kﬁelds, there are
(1+2 K)parameters in total. The following section introduces
a technique available in information retrieval area to tune these
parameters.
D. Optimizing Similarity Functions by Gradient Descent
This section brieﬂy introduces an optimization technique
in [10] based on stochastic gradient descent to tune parameters
in a similarity function.
A function sim(d; q)computes a similarity score between
a document dand a query q, and has a vector of nfree
parameters ⟨x1; x2;· · ·; xn⟩. In order for sim()to perform
most effectively for a document corpus, we need a training
set to tune the nparameters in sim(). In the training set, each
instance is a triple (q; rel; irr ), where qis a query, relis a
document relevant to q, and irris an irrelevant document.
Ideally, sim(d; q)should give a higher score to (rel; q )than
to(irr; q ), namely sim(rel; q )> sim (irr; q ).
In [10], Taylor et al. use a simpliﬁed version of RankNet
cost function RNC presented in [11] to measure the cost
of the application of sim(d; q)on a training instance I=
(q; rel; irr ).
RNC (I) =log(1+eY)where Y=sim(irr; q )−sim(rel; q )
Lower RNC value means more accuracy of sim(), whereas
higher value represents less accuracy of sim(). Intuitively, the
RNC value for a training instance is large if the retrievalfunction sim()fails to rank the two documents rel,irrin the
correct order, that is, sim(irr; q )> sim (rel; q ).
The whole optimization of sim()is a process of minimizing
the cost function RNC for each training instance, and the min-
imization is achieved by iteratively adjusting free parameters
insim()via stochastic gradient descent.
Algorithm 1 Simpliﬁed Parameter Tuning Algorithm
TS: a training set
N: the times to iterate through TS
: the tuning rate – a small number such as 0.001
1:forn= 1toNdo
2: for each instance I∈TSin random order do
3: for each free parameter xinsim()do
4: x=x−×@RNC
@x(I)
5: end for
6: end for
7:end for
Algorithm 1 displays a simpliﬁed version of the tuning al-
gorithm. Generally, the algorithm iterates through the training
setNtimes, where Nis provided by the user. At each time,
for each training instance I, the algorithm adjusts each free
parameter xtox=x−×@RNC
@x(I). This adjustment is
controlled by a small coefﬁcient ( > 0)and@RNC
@x– the
partial derivative of RNC with respect to the free parameter x.
In principle, the iterative adjustment of the value of x enables
the latter to progress towards the minimum of RNC. For a
textbook treatment on gradient descent, please refer to [12].
III. A PPROACH
Our approach consists of three aspects. First we aim to
improve the accuracy of textual similarity measures specially
for bug reports. Thus we extend BM25F – a successful
measure in information retrieval area, by considering the
weight of terms in queries. Second in order to enhance the
performance of duplicate bug report retrieval, besides textual
ﬁelds we try to take better advantage of the other kinds of
information available in bug reports. Therefore we propose
a new retrieval function which is a linear combination of
similarities of textual and categorial features. Furthermore, to
enable the retrieval function to work most effectively on a
certain bug repository, we optimize it by performing gradient
descent on a training set extracted from the bug repository.
The following sections describe these aspects respectively.
A. Extending BM25F for Structured Long Queries
BM25F is designed for short queries, which usually have
no duplicate words. For example, the queries in search engines
are usually of fewer than ten distinct words. However, in
the context of duplicate bug report retrieval, each query is
a new bug report. The query is structured as it is with a short
summary and a long description, and it can sometimes be as
long as more than one hundred words. We believe these two
characteristics of bug report queries – structural and long –
can further enhance the retrieval performance of BM25F , sowe extend it into the following form by considering the term
frequencies in queries.
BM25F ext(d; q) =∑
t2d\qIDF (t)×TFD(d; t)
k1+TFD(d; t)×WQ
where WQ=(k3+ 1) ×TFQ(q; t)
k3+TFQ(q; t)
(4)
TFQ(q; t) =K∑
f=1wf×occurrences (q[f]; t) (5)
In (4), for each shared term between a document dand
a query q, its weight contains two components: one is the
product of IDF value and term weight in dinherited from
BM25F ; and the other is the term weight in query q–
WQ.WQis derived from the query term weighting scheme
ofunstructured retrieval function Okapi BM25 [13], and it
involves weight from the query computed by TFQdeﬁned
in (5). The free parameter k3(k3≥0)is to control the
contribution of the query term weighting, for example, if
k3= 0, then the query term contributes no weight as WQ
becomes always equal to 1 and BM25F extis reduced to
BM25F .WQis monotonically increasing and concave with
k3, upper-bounded by TFQ.
In (5), the weight of a term is the aggregation of the product
ofwf– the weight of ﬁeld f, and the number of occurrences
oftinq[f]. Different from TFDdeﬁned in (2), there is no
length normalization of query terms as retrieval is being done
with respect to a single ﬁxed query.
Compared to BM25F ,BM25F exthas an additional free
parameter k3, thus given documents with Kﬁelds, BM25F ext
has(2 + 2 K)free parameters.
B. Retrieval Function
From the three pairs of duplicate reports shown in Table II,
we note that duplicate reports are similar not only textually
insummary anddescription ﬁelds but also in the categorial
ﬁelds such as product ,component ,priority ,etc:To capture
this observation, given a bug report dand a query bug report
q, our retrieval function REP (d; q)is a linear combination of
seven features, in the following form where wiis the weight
for the i-th feature feature i.
REP (d; q) =7∑
i=1wi×featurei (6)
Each weight represents the degree of importance of its corre-
sponding feature. If a feature is good at distinguishing similar
reports from dissimilar ones, its weight should be larger than
those features with weaker distinguishing power. Figure 2
shows the deﬁnitions of the seven features, which can be
classiﬁed into two types:
Textual Features . The ﬁrst feature deﬁned in (7) is the textual
similarity between two bug reports over the ﬁelds summary
anddescription computed by BM25F ext. The second featuredeﬁned in (8) is the same as the ﬁrst one, except that the
ﬁelds summary anddescription are represented in bigrams.
(A bigram consists of two consecutive words.)
Categorial Features . The rest ﬁve features are categorial
features: the features 3–5 are based on the equality of the
ﬁelds product ,component and type; whereas the sixth and
seventh features are the reciprocal of the distance between
twopriorities orversions respectively.
feature1(d; q) = BM25F ext(d; q)//of unigrams (7)
feature2(d; q) = BM25F ext(d; q)//of bigrams (8)
feature3(d; q) ={
1;ifd:prod =q:prod
0;otherwise(9)
feature4(d; q) ={
1;ifd:comp =q:comp
0;otherwise(10)
feature5(d; q) ={
1;ifd:type =q:type
0;otherwise(11)
feature6(d; q) =1
1 +|d:prio −q:prio |(12)
feature7(d; q) =1
1 +|d:vers −q:vers |(13)
Fig. 2. Features in the Retrieval Function
The retrieval function REP in (6) has 19 free parameters
in total. For the ﬁrst and second features, as we compute
similarities over twoﬁelds summary anddescription , (namely
K= 2inBM25F ext), each feature has (2 + 2 ×2) = 6 free
parameters. Also, we have a weight for each of the 7 features
in (6), thus overall REP has(2×6+7) = 19 free parameters.
Table III lists all these parameters. The next section discusses
how we tune these parameters for REP to gain good retrieval
performance for a certain bug repository.
C. Optimizing REP with Gradient Descent
The parameter tuning for REP is based on gradient descent.
In this work, we do not follow our previous approach using
linear kernel SVM to optimize REP , although REP is a
linear combination of features. The reason is that SVM is
able to infer the feature weights w1–w7but cannot tune the
free parameters inside the ﬁrst and second features which are
based on BM25F ext, such as k1andk3. However, similar
to SVM, the optimization technique used in this paper is
also a discriminative approach, as parameters are tuned by
contrasting similar pairs of reports against dissimilar ones.
Performing gradient descent also needs a training set. In the
following, we ﬁrst discuss how to construct a training set in
a suitable format for our algorithm from a set of bug reports,
and then detail our training algorithm.
1) Creating Training Set: As mentioned in Section II-D,
the training set is a set of training instances of the form
(q;rel;irr), where qis a query bug report, relis a duplicate
report of q, and irris a report on a different bug.TABLE III
PARAMETERS IN REP
Param Description Init Example
w1 weight of feature1(unigram) 0.9 1.163
w2 weight of feature2(bigram) 0.2 0.013
w3 weight of feature3(product) 2 2.285
w4 weight of feature4(component) 0 0.032
w5 weight of feature5(report type) 0.7 0.772
w6 weight of feature6(priority) 0 0.381
w7 weight of feature7(version) 0 2.427
wunigram
summ weight of summary infeature1 3 2.980
wunigram
desc weight of description infeature1 1 0.287
bunigram
summ bofsummary infeature1 0.5 0.703
bunigram
desc bofdescription infeature1 1 1.000
kunigram
1 k1infeature1 2 2.000
kunigram
3 k3infeature1 0 0.382
wbigram
summ weight of summary infeature2 3 2.999
wbigram
desc weight of description infeature2 1 0.994
bbigram
summ bofsummary infeature2 0.5 0.504
bbigram
desc bofdescription infeature2 1 1.000
kbigram
1 k1infeature2 2 2.000
kbigram
3 k3infeature2 0 0.001
Algorithm 2 Constructing a Training Set from a Repository
1:TS=∅: resultant training set
2:N > 0: parameter controlling the size of TS
3:for each bucket Bin the repository do
4:R= {B:master }∪B:duplicates
5: for each report qinRdo
6: for each report relinR–{q}do
7: fori= 1toNdo
8: randomly choose a report irr s:t: irr = ∈R
9: TS=TS∪ {(q;rel;irr)}
10: end for
11: end for
12: end for
13:end for
14:return TS
Algorithm 2 lists the procedure to construct a training set
from a repository. Generally, in each bucket of the repository, it
pairs two reports as a relevant query-document pair (q;rel)at
line 5 and 6. Next in line 7–10, it randomly chooses Nreports
which are not duplicate of the current bucket as irrelevant
documents, and lastly pairs the relevant pair against each of
theNreports to form Ntraining instances. In our case studies,
we set Nto 30.
2) Parameter Tuning: To apply gradient descent, for each
parameter xinREP , we manually derive@RNC
@x– the partial
derivativeof RNC with respect to x.
Next, we initialize each parameter with a default value.
For parameters in BM25F ext, we use recommended default
values for parameters of BM25F from information retrieval
area. For example, for a lengthy ﬁeld, its bshould be big to
perform length normalization, whereas for a short ﬁled its bis supposed to be small. Thus we instantiate bsumm = 0:5
andbdesc = 1. Based on our previous experience in [5],
the terms in summary are usually three times as important
as those in description , and hence we initially set wsumm = 3
andwdesc = 1. The column Initin Table III shows these
initial values used in our case studies. Given a training set,
we start the tuning by calling the procedure in Algorithm 1.
The column Example in Table III displays a set of example
values we get after tuning for OpenOfﬁce. The tuned value
for a parameter may vary with a different initial value, but it
does not affect the retrieval performance much as the tuning
algorithm coordinates and adjusts all the free parameters
towards the optimal. Algorithm 3 lists the optimization process
in detail.
Algorithm 3 Tuning Parameters in REP
1:initialize free parameters in REP with default values
2:ﬁxk1and set k1= 2infeature1andfeature2
3:ﬁxk3and set k3= 0infeature1andfeature2
{tune unﬁxed parameters: w1–w7,b,wsumm andwdesc}
4:call Algorithm 1 with N= 24 and= 0:001
5:unﬁx k3, ﬁxb,wsumm andwdescinfeature1andfeature2
{tune unﬁxed parameters: w1–w7,k3.}
6:call Algorithm 1 with N= 24 and= 0:001again
Following the analysis in [8] [10], tuning the weights of
summary and description ﬁelds is redundant to tuning k1.
Thus, we ﬁx k1to an arbitrary value i:e:, 2 and let the tuning
algorithm tune the remaining parameters for k1= 2. Different
from BM25F ,BM25F exthas one more parameter k3that
determines the scaling of term weights in queries. Tuning
it is also redundant to tuning the weights of summary and
description to some extent. Hence instead of tuning all these
parameters together, we tune the parameters in two rounds:
1)In the ﬁrst round in line 3–4, we ﬁx k3infeature1and
feature2to 0, and tune the other parameters.
2)In the second round in line 5–6, we unﬁx k3, ﬁx
all the other parameters in feature1andfeature2to
their current values, and start tuning the rest of unﬁxed
parameters including w1–w7andk3.
In addition to avoiding redundant tuning, we enjoy another
beneﬁt from the two-round tuning. As we ﬁx k3to 0 in the ﬁrst
round, the BM25F extinfeature1andfeature2is reduced to
BM25F . All its partial derivatives except the one with respect
tok3become equal to those of BM25F respectively. Therefore
in our implementation, we use the ones of BM25F instead
ofBM25F extin the ﬁrst round, as they are faster and much
simpler.
IV. C ASESTUDIES
We have built a prototype2to validate the effectiveness
of the extension to BM25F and our new retrieval function,
2Both implementation and dataset are available online at (http://www.comp.
nus.edu.sg/~suncn/ase11/)and have applied it to the bug repositories of three large
open source projects, OpenOfﬁce, Mozilla and Eclipse. The
experiments simulate the real-world bug triaging process, that
is, for each duplicate report, we use the proposed techniques
to retrieve a list of top similar master reports from the bug
repository. The evaluation of the retrieval performance is
measured by two metrics, recall rate@k andmean average
precision (MAP). By ﬁxing the size of the top list to k,recall
rate@k deﬁned in (14) measures the fraction of duplicate
reports whose masters are successfully detected in the retrieved
top-kmasters ( Ndetected ), among all the duplicate reports
(Ntotal) used in testing the retrieval process.
recall rate @k=Ndetected
Ntotal(14)
MAP is a single-ﬁgure measure of ranked retrieval results
independent of the size of the top list. It is designed for
general ranked retrieval problem, where a query can have
multiple relevant documents. However, duplicate bug report
retrieval is special as each query (duplicate report) has only
one relevant document (master report), thus MAP in our case
studies is reduced to the simpliﬁed form in (15). For the
complete form, please refer to [14]. Given a set Qof duplicate
reports, for each duplicate, the system continually retrieves
masters in descendent order of similarity until the right master
is retrieved, and records its index in the ranked list. Then MAP
can be computed as follows,
MAP (Q) =1
|Q|jQj∑
i=11
index i(15)
where index iis the index where the right master is retrieved
for the i-th query. Higher MAP means that for each duplicate
the retrieval system can return the right master at a higher place
in the ranked result list, saving triagers’ time on checking if a
new report is a duplicate, and on ﬁnding its associated master
report.
Our case studies serve as two purposes: the ﬁrst is to
validate the effectiveness of BM25F extover BM25F ; the
second is to compare the retrieval performance of the proposed
retrieval function REP , our previous retrieval model based on
SVM [5], and the work by Sureka and Jalote in [6].
A. Experimental Setup
We used the bug repositories of three large open source
projects: OpenOfﬁce, Mozilla and Eclipse. OpenOfﬁce is an
open source counterpart of Microsoft Ofﬁce. Mozilla is a
community hosting multiple open source projects such as
the famous web browser Firefox, email client Thunderbird.
Eclipse is an extensible multi-language software development
environment written in Java. These three projects are diverse in
terms of purposes, users and implementation languages, thus
help generalizing the conclusions of the experiments on them.
We extracted four report datasets from them by choosing
reports submitted within a period of time T. In particular, we
created two datasets from Eclipse: one is for reports submitted
in 2008, whereas the other is for reports submitted from theTABLE IV
DETAILS OF DATASETS
Dataset SizePeriod Training Reports Testing Reports
From To #Duplicate #All #Duplicate #All
OpenOfﬁce 31,138 2008-01-01 2010-12-21 200 3,696 3,171 27,442
Mozilla 75,653 2010-01-01 2010-12-31 200 4,529 6,725 71,124
Eclipse 45,234 2008-01-01 2008-12-31 200 5,863 2,880 39,371
Large Eclipse 209,058 2001-10-10 2007-12-14 200 3,528 27,295 205,530
.....
0.
5.
10.
15.
20.0:4
.0:6
.
k: size of the retrieved top- klist.Recall Rate
.. ..BM25F
. ..BM25F ext
(a) OpenOfﬁce.....
0.
5.
10.
15.
20.0:4
.0:6
.
k: size of the retrieved top- klist.Recall Rate
.. ..BM25F
. ..BM25F ext
(b) Mozilla
.....
0.
5.
10.
15.
20.
0:4.0:5
.0:6
.0:7
.
k: size of the retrieved top- klist.Recall Rate
.. ..BM25F
. ..BM25F ext
(c) Eclipse.....
0.
5.
10.
15.
20.0:4
.0:5
.0:6
.0:7
.
k: size of the retrieved top- klist.Recall Rate
.. ..BM25F
. .. BM25F ext
(d) Large Eclipse
Fig. 3. Effectiveness of BM25F extCompared to BM25F inRecall Rate
beginning of Eclipse project to 2007. The reason to create the
latter larger one3is that Sureka and Jalote used this dataset
in their work [6] and we intend to compare with theirs using
the same benchmark. Furthermore, this dataset spans a long
time and is extremely large, hence it can further validate our
technique across a long period of time.
These reports include all defect reports, maintenance tasks
and feature requests. Each duplicate report has been labeled as
duplicate by triagers and linked to its master . This information
is used to test the retrieval and to measure the performance,
i.e.,extracting duplicates to construct a training set, picking a
duplicate to retrieve a top- ksimilar masters, and determining
whether a master is the correct master of a duplicate. Since our
retrieval function ( i:e:, (6)) involves the order of versions, we
manually recovered the chronological order of all the versions
for OpenOfﬁce by searching the release date of each version
on the internet. We did not do this for the other datasets due
to difﬁculty but we believe that it should be easy for project
members to get and maintain such version order. For the other
datasets, we just assume that all reports have the same version.
3We download this dataset from (http://msr.uwaterloo.ca/msr2008/
challenge/)Table IV details the four datasets. Our case studies follow
the approach used in past studies on duplicate bug report
retrieval [2]–[5]. We selected the ﬁrst Mreports in the
repository (based on chronological order) of which 200 reports
are duplicates as training set to tune the parameters in the
retrieval function REP . The column Training Reports displays
the ratio of duplicates and all reports in the training set.
Besides serving as a training set in our approach, those M
reports are also used to simulate the initial bug repository for
all experimental runs. The rest of the duplicates are used for
testing the retrieval process, shown in column Testing Reports .
At each experimental run, we iterate through the testing reports
in chronological order. Once reaching a duplicate report R, we
apply the corresponding technique to retrieve R’s potential
master reports until its right master is received. After each
retrieval is done, we record the index of R’s master in the top
list for recall rate@k and MAP calculation, and then add Rto
the repository. After the last iteration is done, the recall rate
for different sizes of the top list and MAP are calculated.
The prototype was implemented in C++, and all experiments
were carried on a Linux PC with Intel Core 2 Quad CPU
3.0GHz and 8GB memory. Since the training set is constructed
randomly ( c:f: Algorithm 2), and the gradient descent alsoinvolves randomness ( c:f: Algorithm 1), we repeated all
experiments ﬁve times and take the average values for our
analysis and conclusion.
B. Effectiveness of BM25F ext
In this experiment, we aim to validate the effectiveness
ofBM25F extover BM25F . To ensure fairness, we only
involve textual similarity on summary and description in
our experiment, and omit the use of any categorial features.
Figure 3 shows the curve of the recall rate@k of the two
similarity measures on the four datasets. On each dataset
for each different size of the top- kresult list, BM25F ext
performs constantly better than BM25F and it gains 4%–
11%, 7%–13%, 3%–6% and 3%–5% relative improvement
over BM25F on OpenOfﬁce, Mozilla, Eclipse and Large
Eclipse datasets respectively. Moreover, Table V shows the
mean average precision of the two metrics, and the last row is
the relative improvement by BM25F extover BM25F , which
is up to 10.68%. Based on the large set of testing query
duplicates – 40,071 in total, we conclude that in the context
of duplicate bug report retrieval, BM25F extis more effective
than BM25F .
TABLE V
MAP OFBM25F extANDBM25F
OpenOfﬁce Mozilla Eclipse Large Eclipse
BM25F ext 45.21% 46.22% 53.21% 44.22%
BM25F 41.92% 41.76% 51.06% 42.25%
Relative Impro. 7.86% 10.68% 4.20% 4.66%
C. Effectiveness of Retrieval Function
We evaluate the performance of our new retrieval function
REP against previous techniques, including our previous
work based on support vector machine [5] and a recent work
using character n-gram-based features by Sureka and Jalote
in [6]. We apply both our previous technique and REP on
the four datasets and compare them directly. However, due to
high overhead, our previous technique on Large Eclipse is not
able to complete despite running for two days, thus there is no
result for this experimental run. For the work by Sureka and
Jalote, they do the evaluation on the Large Eclipse dataset. We
compare our result with the accuracy results reported in their
paper.
Figure 4 shows the recall rate ofREP and our previous
technique on the four datasets. In the ﬁgure, SVM represents
our previous technique, REP-V stands for the REP retrieval
function with version information considered, and REP-NV is
theREP without version information considered. As men-
tioned before, we only recovered the chronological order of
versions for OpenOfﬁce, therefore only applied REP-V to
OpenOfﬁce. From Figure 4(a), we can see that REP-V can
improve the recall rate over REP-NV by 2–4%. In Figure 4(d),
only the performance of REP-NV is shown, as running of
SVM experiences time-out. Overall, the new retrieval function
outperforms SVM 14–27% in OpenOfﬁce dataset, 10–26%.....
0.
5.
10.
15.
20.0:4
.0:6
.
k: size of the retrieved top- klist.Recall Rate
.. ..SVM
. ..REP-V
. ..REP-NV
(a) OpenOfﬁce
.....
0.
5.
10.
15.
20.0:4
.0:6
.
k: size of the retrieved top- klist.Recall Rate
.. ..SVM
. ..REP-NV
(b) Mozilla
.....
0.
5.
10.
15.
20.0:4.0:6
.0:8
.
k: size of the retrieved top- klist.Recall Rate
.. ..SVM
. ..REP-NV
(c) Eclipse
.....
0.
5.
10.
15.
20. 0:4 .0:5
.0:6
.0:7
.Time out, no curve for SVM..
k: size of the retrieved top- klist.Recall Rate
. . ..REP-NV
(d) Large Eclipse
Fig. 4. Comparison with Our Previous Approach
in Mozilla dataset and 12–22% in Eclipse dataset. Table VI
displays the MAP of each experimental run. The last row is
the relative improvement brought by REP over SVM.
In [6], Sureka and Jalote randomly selected 1100 duplicate
reports to test their approach and reported that recall rate
@10 was 21%, @20 was 25% and @2000 was 68%. We
experimented on the same dataset, Large Eclipse, and we
tested all the 27,295 duplicate reports to avoid randomness,TABLE VI
MAP OFREP-V REP-NV AND SVM
OpenOfﬁce Mozilla Eclipse Large Eclipse
REP-V 48.63% – – –
REP-NV 47.03% 47.71% 53.96% 46.73%
SVM 39.54% 39.79% 46.09% –
Relative Impro. 22.99% 19.91% 17.20% –
which we believe can produce more reliable result. The recall
rate@1 is 37% and @20 is 71%. We can thus safely conclude
that our technique improves upon past techniques in recall
rate@k
Aside from the performance improvement, REP also sig-
niﬁcantly reduces the runtime compared to our past approach
that uses SVM [5]. Table VII displays the time needed to ﬁnish
an experiment run for each dataset.
TABLE VII
OVERHEAD OF SVM AND REP (IN SECONDS )
OpenOfﬁce Mozilla Eclipse Large Eclipse
REP 139 1603 277 7037
SVM 3408 19411 4267 > 2 days
V. R ELATED WORK
In this section, we describe the existing studies on duplicate
bug report detection and other studies that analyze bug reports.
A. Duplicate Bug Report Detection
There have been a number of past studies that detect
duplicate bug reports. We would present them in chronological
order and then elaborate the difference between our work and
theirs.
Runeson et al. perform one of the ﬁrst studies on ﬁnding
duplicate bug reports [3]. They take natural language text of
bug reports and perform standard tokenization, stemming, and
stop word removal. Each bug report is then characterized as
a bag of word tokens and is modeled as a feature vector,
where each feature corresponds to a word in the bug report.
The feature value in the vector is computed based on the
following formula: 1 +log2(TF(word )), while TF is the
term frequency of the word. Each bug report is then compared
to other bug reports by computing the similarity of their
corresponding feature vectors. Three similarity measures are
analyzed: cosine, dice, and jaccard. Bug reports with high
similarity scores are likely to be duplicates. Their study on
bug reports of Sony Ericsson Mobile Communications show
that cosine performs best and is able to detect 40% of the
duplicate bug reports.
Wang et al. use another feature vector construction approach
where each feature is computed by considering both term
frequency (TF) and inverse document frequency (IDF) of the
words in the bug reports following the formula: TF(word )∗
IDF (word )[4]. Furthermore, they consider an additional
source of information to measure the similarity between twobug reports namely program execution traces. Using cosine
similarity of the composite feature vectors, they detect top-
k similar reports as candidate duplicate bug reports. Their
approach could detect 67%–93% of the duplicate bug reports
in their case study on a small subset of Mozilla Firefox bug
reports. They only consider a relatively small case study as
bug reports which contain execution traces are scarce.
Jalbert and Weimer propose another feature vector construc-
tion approach where each feature is computed by the following
formula: 3+2 ∗log2(TF(word ))[2]. Cosine similarity is also
used to extract top-k similar reports as candidate duplicate bug
reports. They also propose a classiﬁcation technique to detect
bug reports that are duplicated.
Sun et al. propose to use a discriminative approach to detect
bug reports. A Support Vector Machine (SVM) is used to train
a model that would compute the probability of two reports
being duplicate [5]. This probability is used to detect top-
kmost similar reports as candidate duplicate bug reports.
Their case study on bug reports from OpenOfﬁce, Firefox,
and Eclipse, show that they could outperform the approaches
proposed in [2]–[4] when only the natural language text of the
bug reports is considered.
More recently, Sureka and Jalote propose an approach that
consider not word tokens but n-grams as features in a feature
vector that characterizes a bug report [6]. They show that
their approach is able to detect duplicate bug reports with
reasonable accuracy on a large bug report corpus from Eclipse.
No comparative study however is performed to compare their
approach with the other existing approaches.
Similarities and Differences. We address the same problem
of detecting duplicate bug reports or more accurately retrieving
top-krelated bug reports from a collection of bug reports.
We consider natural language text of the bug reports, similar
to the work in [2]–[6]. Aside from natural language text, we
also consider other categorial features available in Bugzilla.
The study by Jalbert and Weimer also considers categorial
features [2]. Different from the work by Wang et al. [4],
we ignore execution traces as these are often not available
in bug reports. Indeed, out of the many reports that we
study only a small proportion of them contain execution
trace information. In this study, we have extended one of the
latest textual similarity measures in information retrieval for
retrieving structured documents namely BM25F . Lastly we
show that our proposed retrieval function involving textual
and categorial similarities outperforms other state-of-the-art
measures proposed in the literature, i:e:, [5], [6].
B. Other Bug Report Related Studies
Categorization of bug reports is investigated by Anvik et
al. [15], Cubranic and Murphy [16], Pordguski et al [17],
and Francis et al. [18]. In [15], [16], this categorization is
used to assign bug reports to the right developers. A study on
predicting the severity of bug reports is performed by Menzies
and Marcus [19]. Bettenburg et al. extract stack traces, codes,
and patches from textual bug reports [20]. Ko and Myersinvestigate the differences between defect reports and feature
requests [21].
Anvik et al. perform an empirical study on the characteris-
tics of bug repositories including the number of reports that
a person submitted and the proportion of different resolution-
s [1]. Sandusky et al. study the statistics of duplicate bug re-
ports [22]. Hooimeijer and Weimer develop a model to predict
bug report quality [23]. Bettenburg et al. survey developers
of Eclipse, Mozilla, and Apache to study what developers
care most in a bug report [24]. Bettenburg et al. later show
that duplicate bug reports might be useful [7]. Duplicate bug
reports could potentially provide different perspectives to the
same defect potentially enabling developers to better ﬁx the
defect in a faster amount of time. Still there is a need to detect
bug reports that are duplicate of one another.
VI. C ONCLUSION & F UTURE WORK
In this work, we improve the accuracy of duplicate bug
retrieval in two ways. First, BM25F is an effective textual
similarity measure which is originally designed for short
unstructured queries, and we extend it to BM25F extspecially
for lengthy structured report queries by considering weight of
terms in queries. Second, we propose a new retrieval function
REP fully utilizing not only text but also other information
available in reports such as product ,component ,priority etc:A
two-round gradient descent contrasting similar pairs of reports
against dissimilar ones, is adopted to optimize REP based on
a training set.
We have investigated the utility of our technique on 4 sizable
bug datasets extracted from 3 large open-source projects, i:e:,
OpenOfﬁce, Firefox and Eclipse; and ﬁnd that both BM25F ext
andREP are indeed able to improve the retrieval perfor-
mance. Particularly, the experiments on the four datasets show
thatBM25F extimproves recall rate@k by 3–13% and MAP
by 4–11% over BM25F . For retrieval performance of REP ,
compared to our previous work based on SVM, it increases
recall rate@k by 10–27%, and MAP by 17–23%; compared
to the work by Sureka and Jalote [6], REP performs with
recall rate@k of 37–71% ( 1≤k≤20), and MAP of 46%,
which are much higher than the results reported in their paper.
In the future, we plan to build indexing structure of bug
report repository to speed up the retrieval process. Last but not
least, we plan to integrate our technique into Bugzilla tracking
system.
ACKNOWLEDGMENT
We are grateful to the reviewers for their valuable com-
ments. This work is partially supported by a research grant
R-252-000-403-112.
REFERENCES
[1]J. Anvik, L. Hiew, and G. C. Murphy, “Coping with an open bug
repository,” in eclipse ’05: Proceedings of the 2005 OOPSLA workshop
on Eclipse technology eXchange , 2005, pp. 35–39.
[2]N. Jalbert and W. Weimer, “Automated Duplicate Detection for Bug
Tracking Systems,” in proceedings of the International Conference on
Dependable Systems and Networks , 2008.[3]P. Runeson, M. Alexandersson, and O. Nyholm, “Detection of Duplicate
Defect Reports Using Natural Language Processing,” in proceedings of
the International Conference on Software Engineering , 2007.
[4]X. Wang, L. Zhang, T. Xie, J. Anvik, and J. Sun, “An Approach to
Detecting Duplicate Bug Reports using Natural Language and Execu-
tion Information,” in proceedings of the International Conference on
Software Engineering , 2008.
[5]C. Sun, D. Lo, X. Wang, J. Jiang, and S.-C. Khoo, “A discriminative
model approach for accurate duplicate bug report retrieval,” in ICSE ,
2010, pp. 45–56.
[6]A. Sureka and P. Jalote, “Detecting duplicate bug report using character
n-gram-based features,” in Proceedings of the 2010 Asia Paciﬁc Software
Engineering Conference , 2010, pp. 366–374.
[7]N. Bettenburg, R. Premraj, T. Zimmermann, and S. Kim, “Duplicate bug
reports considered harmful ... really?” in ICSM08: Proceedings of IEEE
International Conference on Software Maintenance , 2008, pp. 337–345.
[8]S. Robertson, H. Zaragoza, and M. Taylor, “Simple BM25 Extension
to Multiple Weighted Fields,” in Proceedings of the thirteenth ACM
international conference on Information and knowledge management ,
2004, pp. 42–49.
[9]H. Zaragoza, N. Craswell, M. J. Taylor, S. Saria, and S. E. Robertson,
“Microsoft cambridge at trec 13: Web and hard tracks,” in TREC , 2004.
[10] M. Taylor, H. Zaragoza, N. Craswell, S. Robertson, and C. Burges,
“Optimisation methods for ranking functions with multiple parameters,”
inProceedings of the 15th ACM international conference on Information
and knowledge management , 2006, pp. 585–593.
[11] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton,
and G. Hullender, “Learning to Rank Using Gradient Descent,” in
Proceedings of the 22nd international conference on Machine learning ,
2005, pp. 89–96.
[12] T. M. Mitchell, Machine Learning . New York: McGraw-Hill, 1997,
pp. 88–95.
[13] C. D. Manning, P. Raghavan, and H. Schtze, Introduction to Information
Retrieval . New York, NY, USA: Cambridge University Press, 2008,
pp. 232–233.
[14] ——, Introduction to Information Retrieval . New York, NY, USA:
Cambridge University Press, 2008, pp. 158–163.
[15] J. Anvik, L. Hiew, and G. Murphy, “Who should ﬁx this bug?” in
proceedings of the International Conference on Software Engineering ,
2006.
[16] D. Cubranic and G. C. Murphy, “Automatic bug triage using text
categorization,” in Proceedings of the Sixteenth International Conference
on Software Engineering & Knowledge Engineering , 2004, pp. 92–97.
[17] A. Podgurski, D. Leon, P. Francis, W. Masri, M. Minch, J. Sun, and
B. Wang, “Automated support for classifying software failure reports,”
inProceedings of the 25th International Conference on Software Engi-
neering , 2003, pp. 465–475.
[18] P. Francis, D. Leon, and M. Minch, “Tree-based methods for classifying
software failures,” in ISSRE , 2004.
[19] T. Menzies and A. Marcus, “Automated severity assessment of software
defect reports,” in ICSM08: Proceedings of IEEE International Confer-
ence on Software Maintenance , 2008, pp. 346–355.
[20] N. Bettenburg, R. Premraj, T. Zimmermann, and S. Kim, “Extracting
structural information from bug reports,” in MSR ’08: Proceedings of the
2008 international working conference on Mining software repositories ,
2008, pp. 27–30.
[21] A. Ko and B. Myers, “A linguistic analysis of how people describe
software problems,” in IEEE Symposium on Visual Languages and
Human-Centric Computing , 2006, pp. 127–134.
[22] R. J. Sandusky, L. Gasser, R. J. S, U. L. Gasser, and G. Ripoche, “Bug
report networks: Varieties, strategies, and impacts in a f/oss development
community,” in International Workshop on Mining Software Reposito-
ries, 2004, pp. 80–84.
[23] P. Hooimeijer and W. Weimer, “Modeling bug report quality,” in
ASE ’07: Proceedings of the twenty-second IEEE/ACM international
conference on Automated software engineering , 2007, pp. 34–43.
[24] N. Bettenburg, S. Just, A. Schröter, C. Weiss, R. Premraj, and T. Zim-
mermann, “What makes a good bug report?” in SIGSOFT ’08/FSE-16:
Proceedings of the 16th ACM SIGSOFT International Symposium on
Foundations of software engineering , 2008, pp. 308–318.