Scalable Test Data Generation from Multidimensional
Models 
Emina Torlak
Electrical Engineering and Computer Sciences
University of California at Berkeley
Technical Report No. UCB/EECS-2012-177
http://www.eecs.berkeley.edu/Pubs/TechRpts/2012/EECS-2012-177.html
July 13, 2012Copyright ¬© 2012, by the author(s). 
All rights reserved. 
 
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission.
 
Acknowledgement 
 
We thank Rastislav Bodik and the anonymous reviewers of this paper for
their valuable comments and feedback.Scalable Test Data Generation from
Multidimensional Models
Emina Torlak
U.C. Berkeley
emina@eecs.berkeley.edu
ABSTRACT
Multidimensional data models form the core of modern deci-
sion support software. The need for this kind of software is
signicant, and it continues to grow with the size and variety
of datasets being collected today. Yet real multidimensional
instances are often unavailable for testing and benchmarking,
and existing data generators can only produce a limited class
of such structures. In this paper, we present a new frame-
work for scalable generation of test data from a rich class of
multidimensional models. The framework provides a small,
expressive language for specifying such models, and a novel
solver for generating sample data from them. While the sat-
isability problem for the language is NP-hard, we identify a
polynomially solvable fragment that captures most practical
modeling patterns. Given a model and, optionally, a statisti-
cal specication of the desired test dataset, the solver detects
and instantiates a maximal subset of the model within this
fragment, generating data that exhibits the desired statistical
properties. We use our framework to generate a variety of
high-quality test datasets from real industrial models, which
cannot be correctly instantiated by existing data generators,
or as eectively solved by general-purpose constraint solvers.
Categories and Subject Descriptors
D.2.5 [ Software Engineering ]: Testing and Debugging|
Testing tools ; D.2.4 [ Software Engineering ]: Software /
Program Verication| Formal methods
Keywords
Test data generation, multidimensional models, specication,
constraint solving
1. INTRODUCTION
Background and Motivation. OLAP (Online Ana-
lytical Processing) technologies provide interactive decision
support in many domains, enabling users to obtain abstract
yet intuitive views of very large datasets, and to pose\what-if"
queries about the impact of hypothetical view changes on the
This research was conducted at LogicBlox, Inc.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proÔ¨Åt or commercial advantage and that copies
bear this notice and the full citation on the Ô¨Årst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciÔ¨Åc
permission and/or a fee.
SIGSOFT/FSE‚Äô12, November 10-18, 2012, Cary, NC, USA.
Copyright 2012 ACM 978-1-4503-1614-9/12/11 $15.00.underlying data. As such, they have become indispensable
tools for experts in many elds ( e.g., business, medicine, ed-
ucation, research, and government [31]), and the demand for
them is growing. With the rapid increase in the variety and
volume of data being collected, there is signicant interest in
extending decision support facilities to both non-relational
datasets (such as streams [14], sequences [29], text [27], and
networks [44]) and extremely large relational datasets [11, 6]
that exceed storage capacities of traditional data warehouses.
A key feature|and attraction|of OLAP is the use of mul-
tidimensional data models [19, 30, 31, 40], which enable both
the interactivity and intuitiveness of OLAP tools. These
models structure data in a multidimensional analysis space,
called the data cube . Dimensions themselves are graphs, and
their structure denes the levels of detail at which data can
be summarized for viewing and manipulation. For example,
Fig. 1c shows a toy person dimension, whose structure speci-
es that data about individuals|such as the tness data in
Fig. 1a|can be summarized by gender and location, which is
given at the level of cities, states or provinces, and countries.
Queries on a multidimensional dataset compute projec-
tions, or views, of the underlying data cube. OLAP-style
tools exploit this to achieve interactive response times, by
answering queries about less detailed views of the data ( e.g.,
at the level of countries) using more detailed, precomputed
views ( e.g., at the level of cities) [39]. But the application of
this core technique is complicated by several hard problems,
such as: (1) ensuring that all dimensional structures are
summarizable [32], which is a pre-condition for obtaining
correct results from precomputed views, and (2) selecting
an optimal set of views to precompute for a given analysis
space. Solving the rst problem for all but simplest struc-
tures requires intricate dimension transformations [31, 39].
The second problem is not only NP-complete [15] but it is
also inapproximable if P6=NP[23]. It is usually addressed
with sophisticated view selection heuristics ( e.g., [15, 1]).
Like most software, OLAP-style tools are validated through
testing. However, OLAP systems need large data sets with
special characteristics|evaluating view selection heuristics
at scale, for example, requires a variety of large data sets
with summarizable dimensions that have dierent (graph)
structures and statistical properties. Yet real, structurally
rich datasets are hard to come by [38] due to privacy and
proprietary concerns, and existing database synthesizers [2,
4, 5, 17, 18, 28, 41] are of limited help. They can generate
simple tree-shaped dimensions but not (valid) DAGs such as
person , since, as we will show, the data generation problem
for these structures is itself NP-hard.
1Approach. In this paper, we propose a new model-based
approach to synthesizing a rich class of multidimensional
structures, suitable for systematic testing and evaluation of
decision support software. Our approach is implemented in
an industrial-strength testing framework, called TestBlox ,
which provides a small, declarative modeling language, and
a scalable solver for generating sample data from TestBlox
specications. The language is complete [2], in the sense
that it can express every multidimensional structure, and
it captures common features of existing multidimensional
models [19, 30, 31, 40].
The solver takes as input a multidimensional model and,
optionally, a set of statistical constraints on the shape (i.e.,
distribution properties) and scale (i.e., size) of the desired
instance of that model. The model consists of a schema,
given as a DAG over abstract categories ( e.g., Fig. 1c), to-
gether with a set of integrity constraints. Its instances, if
any, are DAGs that conform to the schema and that satisfy
the integrity constraints. While deciding the satisability
of an arbitrary TestBlox model is NP-hard, most of the
models observed in practice fall within a fragment of the
modeling language that can be instantiated in polynomial
time. The TestBlox solver identies and solves constraints
in this fragment, generating a sample instance that satises
a maximal (polynomially solvable) subset of the input model,
and that exhibits the desired statistical properties (when
possible). Instance generation is parallelized across all model
components, including those related by global invariants.
Usage Scenarios. Scalable, model-based synthesis of
dimensional structures, as provided by TestBlox , is helpful
in many validation and benchmarking scenarios:
Testing and performance tuning of decision support appli-
cations . In practice, domain-specic decision support tools
are usually built on top of a generic OLAP engine. The
facts to be analyzed are stored as tuples in a relational
database. Each fact associates a point in a structured anal-
ysis space with a set of numerical values, or measures . For
example, Fig. 1 illustrates a toy tness survey application,
where the analysis space consists of two dimensional struc-
tures, personeducation , and the measures specify the height,
weight and BMI (body mass index) for a set of people. The
user of the application can view and manipulate the given
tness facts with respect to the analysis space by posing
queries such as \If the average BMI for people of all educa-
tion levels in each country were to decrease by 2 points, what
would be the corresponding decrease in the average weight
of college educated males?" (See Fig. 1b for the answer.)
As in the case of traditional database applications, the
OLAP developer uses integrity constraints on fact and di-
mension schemas to express preconditions on the inputs
accepted by the application. To test the application, how-
ever, he may need instances that are not only valid but that
also have specic statistical properties|for example, a small
dimensional instance with uniformly distributed edges for
functional testing, and many large instances with various
edge distributions for performance tuning. TestBlox can
help in either case, by quickly synthesizing valid dimensional
structures of various shapes and sizes, given only the data
model and relevant statistical constraints.1In fact, even if
1TestBlox will also generate a valid fact database with desired
statistical properties, but relational data generation is not unique
to our system (see, e.g., [2, 4, 5, 17, 18, 28, 41]).the developer had access to a real, large end-user dataset
(which is rare in practice), he would still need synthetic data
to expose corner cases, since sampling a single real dataset
would produce a biased test suite of similarly shaped inputs.
Benchmarking of decision support systems . Many common
modeling patterns lead to non-summarizable dimensional
structures [30, 31, 40], which interfere with basic OLAP
optimizations and have to be specially handled either at the
engine or the application level. The TestBlox modeling lan-
guage can express these patterns (Sec. 2), and it can be used,
together with statistical constraints, to precisely describe a
wide variety of non-summarizable dimensional structures of
dierent shapes and sizes. An application developer can use
the data generated from such specications to benchmark
competing OLAP engines, in order to determine which one
oers the best trade-o between performance and native
support for the kind of structures that arise in the targeted
application domain.
Validation of new decision support facilities . In addition
to generating non-summarizable structures, TestBlox can
also synthesize any desired summarizable structure. We
expect that such structures would be particularly useful for
validation of new kinds of decision support facilities [6, 11,
14, 27, 29, 44], as they usually assume summarizable analysis
spaces. For example, one could apply random testing to
validate a new cloud-based OLAP engine [11, 6] against a
standard relational one [34], using a DAG generator [8] to
synthesize random dimensional schemas, and TestBlox to
synthesize random summarizable instances of those schemas.
Contributions. To the best of our knowledge, Test-
Blox is the rst framework to tackle the problem of gener-
ating large-scale, realistic instances of rich multidimensional
models. Its key enabling features are also the novel contribu-
tions of this paper:
-A small but expressive modeling language, with a prac-
tical fragment that can be detected and decided in
polynomial time (relative to model size).
-A solver for the polynomial fragment, with algorithms
for computing data generation parameters that yield
instances of desired shape and scale.
-A data generation architecture that produces instances
in polynomial time (relative to instance size) and in
parallel, even in the presence of global constraints.
The modeling language and statistical (or, soft) constraints
are introduced in Sec. 2, followed by a discussion of hardness
results and the denition of the polynomially solvable frag-
ment in Sec. 3. The solver and the architecture are presented
next (Sec. 4), as is an evaluation of the framework on three
real, industrial models (Sec. 5). A discussion of related work
(Sec. 6) and future directions (Sec. 7) conclude the paper.
2. MULTIDIMENSIONAL MODELING
To illustrate the multidimensional analysis problem, con-
sider the toy application scenario in Fig. 1. The application
provides multidimensional analysis for facts collected via an
online tness survey. The goal of the survey is to investigate
the relationship between individuals' education level and
tness, as (roughly) measured by the body mass index. The
end user is a public health policy analyst. She wants to use
2person education height
(m)weight
(kg)BMI
1 H.S.D. 1.85 90 26
2 B.Sc. 1.70 83 29
3 M.Sc. 1.80 85 26
4 H.S.D. 1.64 75 26
5 B.A. 1.55 60 23
6 M.Eng. 1.72 65 22
7 Ph.D. 1.59 61 24
(a)tness factsmale female
high school 7.0 5.8
college 6.4 4.7
graduate 6.6 4.9
(b) A \what-if" view of
tness facts: decrease in
average weight (in kg) af-
ter a 2 point decrease in
the average BMI
PersonGender CityState ProvinceCountry
1   2   3   4   5   6   7  male female DCBOS LANYC TOMTLMD MA CA NY ON QCUSA Canada
(c)person schema (left) and instance (right)
Figure 1: Data for a toy tness survey application
the application to view, for example, the eect of a 2 point
decrease in the average BMI of the entire North American
population on the average weight of males and females with
dierent education levels (Fig. 1b).
Based on these analysis requirements, a developer may
design the application to provide an analysis space with two
dimensions: person andeducation . The education dimension
(not shown) classies academic degrees into \high school,"
\college" and \graduate" in the usual way. The person dimen-
sion (Fig. 1c) has a more interesting structure, classifying
individuals according to gender and location. In the rest
of this section, we use the person dimension to introduce
theTestBlox modeling language, as well as the statistical
constraints that the developer may provide to our solver in
order to obtain test instances of the person model.
2.1 Dimension Model
Like the standard relational data model, a dimension model
consists of a schema and a set of integrity constraints. A di-
mension schema represents the upper bound on the structure
of valid dimension instances ; the person schema, for example,
admits many instances, including the one shown in Fig. 1c.
Some instances that conform to the schema, however, violate
domain-specic requirements, such as Gender having exactly
two members. Dimension (integrity) constraints encode these
additional requirements, restricting the set of instances that
are valid under the model as a whole. TestBlox schema and
constraints form a complete [2] multidimensional modeling
language that naturally captures common modeling patterns.
Dimension Schema and Instances. A dimension
schema is traditionally represented as a DAG (directed acyclic
graph) over category names or, simply, categories. Each cat-
egory represents an abstract analysis concept. Schema edges
dene abstraction relations between categories, mapping
more detailed categories, such as City, to less detailed ( i.e.,
more abstract) ones, such as State .
Definition 1 (Dimension Schema). Adimension sch-
ema is a directed acyclic graph D= (C;). Schema vertices,(C1)jGenderj2 cardinality
(C2)jGenderj2
(C3)jCountryj196
(C4) Person!Gender_City rollup
(C5) City!StateProvinceCountry exclusive rollup
(C6) Person Gender drilldown
(C7) StateProvince Country exclusive drilldown
Figure 2: Dimension constraints for the person schema
called categories , are drawn from an innite set of category
names C. The expression cicjdenotes the edge hci;cji
in the schema graph. We say that ciis aparent ofcj, and
thatcjis achild ofci. The symbolstands for the reexive
transitive closure of .
A dimension schema describes a family of dimension in-
stances, which are themselves DAGs over member names (or
members) associated with each category. Each schema edge
cicjrepresents a set of instance edges between the mem-
bers of categories ciandcj. The reexive transitive closure
of an instance graph, called the rollup relation , is constrained
to be functional, or strict [19, 22, 25], between every pair
of categories. In other words, a member of any category
(such as Person ) may be abstracted by|or, equivalently, may
rollup to |at most one member of another category (such as
Country ).
Definition 2 (Dimension Instance). Adimension in-
stance is a tripled= (D;m;< ), whereD= (C;)is a
dimension schema; mis a function from each category c2C
to a non-empty, nite set of members ; and<relates mem-
bersxiandxjonly ifxi2m(ci);xj2m(cj)andcicj.
The member set of each category is drawn from an innite set
of member names M, and member sets of dierent categories
are disjoint. The rollup relation, dened as the reexive
transitive closure of <, is functional between every pair of
categories. That is, for all categories cicj, and for every
memberxi2m(ci), there is at most one member xj2m(cj)
such thatxixj.
Dimension Constraints. In addition to a schema, a di-
mension model may also specify a set of integrity constraints
that instances have to satisfy. These include cardinality ,
(exclusive) rollup , and (exclusive) drilldown constraints. Car-
dinality constraints bound the size of a category's member set
in every instance, while (exclusive) rollup and drilldown con-
straints specify which <edges must exist between adjacent
categories and how they are structured.
Figure 2 shows examples of each kind of constraint, applied
to the person schema. The cardinality constraints C1-C3
require each instance to have exactly two genders and no
more than the (current) total number of countries in the
world. The rollup constraint C4 forces every person to rollup
to a member of some specied child category, expressing the
expectation that the toy survey results may be incomplete
but will always include some gender or location information
about every person. The exclusive rollup constraint C5, on
the other hand, ensures that each city rolls-up (via <) to a
member of exactly one specied child category, expressing
the domain knowledge that North American cities belong to
either a state, or a province, or a country. These two rollup
patterns specify (non-)overlapping specialization [30, 31].
3Drilldown constraints, such as C6 and C7, provide comple-
mentary functionality to rollup constraints, and help express
(non-)overlapping generalization [25, 30, 31]. According to
C6 and C7, each gender must have members of some spec-
ied parent category (in this case, only person ) rolling-up
to it, and each country must have members of exactly one
specied parent category rolling-up to it (via <). Every
person instance therefore contains at least one person of each
gender, and all countries contain either states or provinces,
but not both. Note that C7 does not prevent countries from
having cities as children, along with either states or provinces.
All sample constraints are satised by the instance in Fig. 1c.
Definition 3 (Dimension Constraint). A constraint
is adimension constraint on a schema D= (C;)i it
takes one of the following forms:
(1)jcjkorjcjk, wherec2Candk1;
(2)c!W
i1ciorc!J
i>1ci, whereccifor alli;
(3)W
i1ci corJ
i>1ci c, wherecicfor alli.
A dimension instance d= (D;m;< )satises, written
dj=, i it fullls the conditions given below, or their
natural extension to omitted constraints:2
(1)dj=jcjkijm(c)jk;
(2)dj=c!WC0i8x2m(c);9ci2C0;9y2m(ci);x<y ;
(3)dj=c!JC0i8x2m(c);9!ci2C0;9y2m(ci);x<y .
Dimension Model. The person schema and our sample
dimension constraints collectively form a dimension model.
To simplify reasoning about dimension models, we require
that any included cardinality constraints be consistent and
minimal. In particular, we disallow models with constraints
that conict with one another, such as jcjkandjcjk+1,
or that are mutually redundant, such as jcjkandjcjk+1.
We also require that no two constraints involve the same
schema edge unless one is (exclusive) rollup and the other
(exclusive) drilldown. The person model, for example, may
not include another rollup constraint on the CityState
edge, but it could include a drilldown constraint on the same
edge. The TestBlox solver automatically rejects models
with constraints that violate these requirements.
Definition 4 (Dimension Model). Adimension mod-
elis a tupleD= (D;), whereD= (C;)is a schema, and
is a set of dimension constraints on D. Each schema edge
cicjparticipates in at most two constraints in , one
(exclusive) rollup and one (exclusive) drilldown; and each
categoryc2Cis constrained by at most two non-conicting
cardinality constraints in , one lower bound ( ) and one up-
per bound (). A dimension instance d= (D;m;< )satises
the modelD, writtendj=D, i it satises every constraint
in. That is,dj=for all2.
Expressiveness. Despite its compactness, the Test-
Blox modeling language is complete [2] in the sense that
it can be used to fully describe any dimension instance (by,
e.g., introducing a category for each member, constraining
that category's cardinality to be exactly 1, and introducing
a rollup constraint on each edge between singleton categories
with adjacent members). Our language is also practical in
2Recall that `9!' stands for `there exists exactly one.'(S1) Personfsize = 1000g scale
(S2) Person!Cityf shape
source = .85, target = .7,
distribution = normal (30, 5)g
Figure 3: Sample soft constraints for the toy retail application
that it naturally captures important families of instances|
for example, all summarizable [30, 31, 32, 40] instances of a
given schema. An instance of a schema is summarizable, and
thus directly amenable to ecient multidimensional analysis,
if its rollup relation is strict, total and surjective between ev-
ery pair of categories related by the schema. We can describe
all such instances of a schema using a model that constrains
every schema edge cicjwithci!cjandci cj.
2.2 Soft Constraints
While an application or a technique needs to work on all
valid instances of a multidimensional model, this set is, in
most cases, too large to be tested exhaustively. As a result,
testing eorts generally focus on a subset of instances that
are representative, or interesting, in some way. To guide
TestBlox toward generation of such instances, we provide
a set of soft constraints , which describe the desired scale
and distribution characteristics of generated data. We also
dene two simple metrics to quantify the degree to which
an instance satises a given set of soft constraints. The
TestBlox solver optimizes the value of these metrics when
computing data generation parameters.
Figure 3 shows a sample of supported soft constraints,
applied to the person schema. A scale constraint species
the preferred size of a category; according to S1, for exam-
ple,Person should contain roughly 1000 members. A shape
constraint characterizes the rollup relation between adja-
cent categories using three parameters: the percentage of
members in each category that participate in a rollup edge,
and the distribution of rollup edges between participating
members. For example, S2 describes a Person toCityrela-
tion that maps 85% of people to 70% of the cities according
to a normal distribution, where each participating city has,
on average, 30participating people rolling-up to it, with a
standard deviation of 5.
For ease of exposition, we dene soft constraints on a
dimension schema using functions over categories and schema
edges, as follows.
Definition 5 (Soft Constraints). Soft constraints
fon a dimension schema D= (C;)are expressed with four
functions. These are n:C!N;s;t:CC![0::1]; and
p:CC!P, where Pis the set of all probability distribu-
tions. The meaning of f= (n;s;t;p )is dened in terms of a
desired instanced= (D;m;< )as follows. For all categories
c2C,n(c)jm(c)j. For all schema edges e=cicj,
s(e)jm(ci)jj dom(<e)jandt(e)jm(cj)jj ran(<e)j,
where<eis the rollup mapping from m(ci)tom(cj), and
dom(<e)andran(<e)are its domain and range. The num-
ber of members of cithat rollup to a member of cjvaries
according to p.
Because soft constraints may not be precisely satisable,
especially in the presence of dimension constraints, we dene
two metrics to quantify the conformance of an instance to the
scaling and participation requirements encoded by n,s, and
t.Scale delity measures the squared distance between the
4desired and actual sizes of category member sets, and shape
delity measures the squared distance between the desired
and actual participations of category members in the rollup
relation. Measuring, and optimizing, the conformance of
instances to desired distributions is an area for future work.
Definition 6 (Fidelity). Given a dimension instance
d= (D;m;< )and soft constraints f= (n;s;t;p )onD, we
dene the scale andshape delity ofdwith respect to fas
f(d) =P
c2C(n(c) jm(c)j)2andf(d) =P
e=cicj(s(e)
jm(ci)j jdom(<e)j)2+ (t(e)jm(cj)j jran(<e)j)2. We
say thatdhashigher scale delity thand0with respect to a
modelD= (D;)and soft constraints fidj=D,d0j=D,
andf(d)f(d0). Similarly, dj=Dis said to have higher
shape delity thand0j=Dif(d)f(d0).
3. HARDNESS OF DATA GENERATION
To instantiate a multidimensional model, we rst need
to decide whether its constituent dimension models are
satisable| i.e., whether or not they each have an instance.
This decision problem turns out to be NP-hard, and as a
result, we cannot eciently generate arbitrary dimension
instances. Most practical models, however, tend to exhibit
regularity , which TestBlox exploits. Such models, charac-
terized by tieredness (which restricts the use of cardinality
constraints) and weak exclusivity (which restricts the use of
exclusive rollup and drilldown), can be satised in polynomial
time and space.
The hardness of the dimension satisability problem, stated
below, can be proved by reduction from 3SAT.3We do note
that, perhaps surprisingly, the proof does not use exclusivity
constraints|even a simpler version of the decision problem,
with no exclusive rollup or drilldown constraints, is hard.
Theorem 1.Deciding the satisability of dimension mod-
elsD= (D;)is NP-hard, even when is limited to con-
taining only cardinality, rollup and drilldown constraints.
The key to ensuring tractability of exclusivity-free models
is a simple and natural restriction on the use of cardinality
constraints. The restriction, called tieredness, requires that
the upper bound on the cardinality of every category be no
smaller than the lower bound on the cardinality of each of
its descendants. In other words, the model must admit a
hierarchical instance in which every category has at least
as many members as its more abstract descendants in the
dimension schema. This is, in fact, an implicit assumption
in nearly all dimensions observed in practice|more detailed
categories rollup to less detailed ones with fewer members.
Definition 7 (Tieredness). A dimension model D=
(D;)over a schema D= (C;)istiered ih(ci)l(cj)
for allcicj, wherehandlare dened as follows:
h(c)=
k9k;jcjk2
1 otherwise; and,l(c)=
k9k;jcjk2
1otherwise:
Definition 8 (Hierarchy). A dimension instance d=
(D;m;< )ishierarchical ijm(ci)jjm(cj)jfor allcicj.
Theorem 2.A tiered dimension model Dwith no exclu-
sive rollup or drilldown constraints is always satisable, and
it has hierarchical instances that can be encoded (via closures)
in polynomial time and space with respect to the size of D.
3All proofs can be found in the appendix.While tiered, exclusivity-free models are easily satised,
the problem becomes intractable if we allow unrestricted use
of either exclusive rollups or exclusive drilldowns. To main-
tain tractability, as well as practical expressiveness, we limit
the use of these constraints so that a schema edge involved
in an exclusive constraint is otherwise unconstrained|a con-
dition called weak exclusivity. Weakly exclusive models can
capture non-overlapping generalization and specialization,
which is the main use case for exclusivity constraints, but
the generalized or specialized rollup relation cannot also be
explicitly constrained to relate all members of involved cate-
gories. For the purposes of data generation, the latter can
be ameliorated by the use of soft constraints.
Theorem 3.Deciding the satisability of tiered dimen-
sion modelsD, with either exclusive rollups or exclusive
drilldowns, is NP-hard.
Definition 9 (Weak Exclusivity). A dimension mo-
delD= (D;)isweakly exclusive i, for each exclusive
rollup and drilldown constraint 2, there is no other
02that involves a schema edge constrained by .
Tiered, weakly exclusive models are said to be regular.
Many models exhibit regularity|for example, a model that
captures all summarizable instances of a schema is regular.
Another example of a regular model is the person model from
our toy survey application, which is not summarizable. Regu-
lar models always admit at least one polynomially encodable
hierarchical instance. As we will see in the next section, these
instance encodings can be executed in parallel to eciently
generate test data of desired scale and shape.
Definition 10 (Regularity). A dimension model D=
(D;)is regular i it is tiered and weakly exclusive.
Theorem 4.A regular dimension model Dis always sat-
isable, and it has hierarchical instances that can be encoded
(via closures) in polynomial time and space with respect to
the size ofD.
4. GENERATING DATA FOR MAXIMAL
REGULAR FRAGMENTS OF MODELS
Given a dimension model with soft constraints, TestBlox
rst solves the model with respect to the soft constraints,
producing a set of closures that encode the desired instance:
one closure for each edge in the model's schema. This solution
is then instantiated by invoking the closures to generate an
explicit representation of the instance (as a set of text les,
one for each schema edge). Despite having to collectively
satisfy the strictness requirement on dimension instances
(Def. 2), the closures are constructed in such a way that they
can be called in any order. As a result, instance generation
can be fully parallelized, with each schema edge instantiated
by a separate process, independently of all other edges.
Once a model is solved, the remainder of the data gen-
eration process is straightforward. This section therefore
focuses on the four steps taken to solve a dimension model
D= (D;) with respect to soft constraints f:
(1)nd(D;T[W), a maximal regular fragment of D, since
we know that such a fragment is eciently solvable;
(2)solve the cardinality constraints Tto obtain a
cardinality card(c)for each category, such that the
solution is hierarchical, with maximal scale delity f;
5Max-Regular-Fragment (D;)
1T;W fg
2forc2such that isCardinality (c)do
3ifisTiered (D;T[fcg)then
4T T[fcg
5fore2such that:isCardinality (e)do
6ifisWeaklyExclusive (D;W[feg)then
7W W[feg
8return (D;T[W)
Figure 4: Algorithm for computing a maximal regular frag-
ment of a dimension model D= (D;)
(3)solve the constraints Won schema edges, using
shape delity fas a guide, to obtain rollup parameters
dom(e) and ran(e) for each edge; and,
(4)use the rollup parameters to construct individual rollup
closures for all schema edges.
4.1 Finding a Maximal Regular Fragment
The rst step to ecient generation of dimension instances
is to check whether the input model D= (D;) is regular|
and therefore solvable in polynomial time. If so, we proceed
to the next step using D. Otherwise, we proceed using a
(locally) maximal fragment of Dthat is regular, discarding
the constraints outside of the fragment with a warning.
Figure 4 shows a greedy algorithm for detecting and en-
forcing regularity. The algorithm simply builds a regular
model (D;T[W)out of the schema D, a maximal subset
T of the cardinality constraints, and a maximal subset
Wof the constraints on schema edges. The set Tis
maximal (lines 2-4) in that it cannot be augmented with
any omitted cardinality constraints without violating the
tieredness condition| i.e.,(D;T[fcg)is not tiered for any
cardinality constraint c2nT. Similarly, Wis maximal
(lines 5-7) in that (D;W[feg)violates weak exclusivity
for any omitted edge constraint e2nW. As a result,
(D;T[W) is a maximal regular fragment of D, which may
beDitself ( e.g., the person model, Figs. 1c and 2).
The auxiliary function isCardinality tests whether its ar-
gument is a cardinality constraint. The functions isTiered
and isWeaklyExclusive return true only when applied to a
tiered or a weakly exclusive model, respectively. Both can
be implemented directly from Defs. 7 and 9 with worst case
running time of O(jDj), wherejDjis the number of nodes
and edges in the schema graph.4The fragment computation
algorithm as a whole is therefore quadratic in jDj.
4.2 Solving Cardinality Constraints
Once we have identied a regular fragment Dr=(D;T[W)
of our input model, the next step is to compute the number of
members that each category c2Cwill have in the generated
instance. These values, denoted by card :C!N, need to
satisfy two sets of constraints: the cardinality constraints T,
and the hierarchy constraints card(ci)card(cj)for each
cicjinD(Def. 8). Semantically, card is required only to
satisfyT. We impose the hierarchy constraints for eciency:
by Thm. 4, the regularity of Drensures the satisability of
both sets of constraints, while also enabling ecient encoding
of the rollup relation.
4Recall from Def. 4 that contains at most 2jDjconstraints.The cardinality and hierarchy constraints collectively form
a system of linear inequalities over integer variables, as shown
in Fig. 5. The variables represent category sizes: each vi
encodes the size of a category ci2Cin the generated instance.
Inequalities 2 and 3 bound the sizes of categories according to
the cardinality constraints in T. In particular, the functions
lTandhT(Def. 7) map each cito its lower and upper bound
specied by T, if any, or to the implicit lower bound of 1
(Def. 2) and upper bound of int1(e.g.,231 1) otherwise.
The remaining inequalities (4) are induced by the hierarchy
constraints. Figure 6 shows the corresponding inequalities
for the person model.
minimize:X
ci2C(n(ci) vi)2(1)
subject to: vilT(ci)for allci2C (2)
vihT(ci)for allci2C (3)
vivj for allcicj2D (4)
Figure 5: Quadratic program for a model ( D;T[W), with
D= (C;), and soft constraints f= (n;s;t;p ) overD
(2) (3) (4)
vPerson1vPersonint1vPersonvGender
vGender2vGender2vPersonvCity
vCity1vCityint1vCityvState
vState1vStateint1vCityvProvince
vProvince1vProvinceint1vCityvCountry
vCountry1vCountry196 vStatevCountry
vProvincevCountry
Figure 6: System of inequalities for the customer model
It is easy to see that the linear system Fig. 6 is satisable
in polynomial time. For example, we could assign vPerson and
vGender to 2, and all other variables to 1. More generally, we
can set each vitomaxcicjlT(cj), which is the maximum
lower bound over all categories reachable from ci(including
ciitself) in the schema graph. Due to the tieredness of T
(Def. 7), a solution of this form would be valid for every
system 2-4 derived from a regular model Dr. It is unlikely,
however, that such a solution would have high scale delity
(Def. 6) with respect to the provided soft constraints f. We
therefore solve the system 2-4 by forming an integer quadratic
program (IQP) that minimizes the solution with respect to
the scale metric f(Fig. 5, line 1).
In general, solving IQPs is intractable [16]. But our prob-
lem has a special structure: we can show that the matrix
corresponding to the system (2)-(4)is totally unimodular,
and that the matrix corresponding to the quadratic term of
the optimization function (1)is principally unimodular [12].
This allows us to relax the IQP to a quadratic program (QP)
over real variables, and still obtain an integral solution [26].
Moreover, since the optimization matrix is also positive semi-
denite, an optimal solution to the relaxed problem can be
obtained in polynomial time [35]. We use CPLEX [20] to
solve the relaxed problem and populate card.
4.3 Solving Edge Constraints
Given card, we next compute the domain and range sizes
of the rollup functions <ijthat will be generated for the
schema edges cicj. The computed sizes are stored in
6strengthen (D;W )
1Ds;Ws D;fg
2fors2select (W)do
3Ds;s restrict (Ds;s)
4Ws Ws[fsg
5return (Ds;Ws)restrict (Ds= (Cs;s);s)
1Es constrainedEdges (s)
2hci;cji chooseAny (Es)
3ifisExclusive (s)
4thenEomit Esnfhci;cjig
5elseEomit fg
60
s constrain (ci;op(s);cj)
7D0
s (Cs;snEomit)
8return (D0
s;0
s)
Figure 7: Strengthening algorithm for ( D;W )
c card(c)
Person 32
Gender 2
City 16
State 8
Province 8
Country 4e dom(e)ran(e)
e0: PersonGender 24 2
e1: PersonCity 24 16
e2: CityState 8 4
e3: CityProvince 0 0
e4: CityCountry 8 2
e5: StateCountry 8 4
e6: ProvinceCountry 0 0
Figure 8: Sample solutions for the person model
dom :CC!Nand ran:CC!N, respectively, and
they satisfy a system of integer (in)equalities arising from
two sources: the (exclusive) rollup and drilldown constraints
inW, and the strictness constraints on the rollup relation
as a whole (Def. 2). As before, the (in)equalities solved in
this step are stronger than those implied by Wand strictness
alone, in order to enable ecient encoding of rollup closures
in the nal step. To simplify the presentation, we show only
how to strengthen a constraint in Wwith respect to the
schemaD, and how the strengthened problem translates
into integer inequalities. The algorithms for selecting which
constraints to strengthen, and for solving the inequalities, are
described only briey, in terms of their inputs and outputs.
To determine dom andran, we begin by transforming the
schema graph Dand a subset of the constraints W, with the
help of the procedure strengthen (Fig. 7). Its key step is
the application of the function restrict , which takes two
inputs: a schema Ds, and a constraint sover two or more
edges inDs. Given these inputs, restrict outputs a schema
D0
sDsand a constraint 0
ss, which admit fewer
instances than (Ds;s). Ifsis a non-exclusive constraint
(lines 5-8), such as ci!cj__ck, the output is Ds
itself, together with a single-edge constraint 0
ss, such
asci!cj. For exclusive constraints (line 4), the output
is0
sand a schema D0
sDs, which omits the same edges
fromDsas0
sdoes froms. The auxiliary functions are
self-explanatory, and all run in constant time.
It follows easily from Defs. 3-4 that the restrict trans-
formation simply reduces the set of satisfying instances. We
can also prove (from Def. 9 and Thm. 4) that applying
strengthen toDand all ofWwill not eliminate all in-
stances ofDr. In most cases, however, TestBlox selects
only a small subset of Wto strengthen. It would not, for
example, strengthen any constraints in the person model. In
general, we strengthen all multi-edge constraints except the
following: (1) an exclusive rollup constraint over edges that
form a cut-set of D(such as C5); and, (2) any constraint over
cut-edges [10] found in Dsafter the exclusively constrained
cut-sets are removed ( e.g., C4 and C7).
Using a strengthened schema and constraints (Ds;Ws),
we obtain the system of (in)equalities in Fig. 9. The integerdij0rij0 for allcicj2Ds(5)
dijcard(ci)rijcard(cj) for allcicj2Ds(6)
dijrij sgn(dij) = sgn(rij)for allcicj2Ds(7)
X
dijcard(ci)for allci!_
cj2Ws (8)
X
dij=card(ci)for allci!K
cj2Ws (9)
X
rijcard(cj)for all_
ci cj2Ws (10)
X
rij=card(cj)for allK
ci cj2Ws (11)
Figure 9: Domain and range inequalities for ( Ds;Ws)
variablesdijandrijrepresent the size of the domain and
range of<ij, respectively. These variables are non-negative
and bounded above by the corresponding category sizes (5-
6). Due to the strictness requirement, each <ijmust be a
function, and, consequently, its domain must be at least as
large as its range (7). Moreover, the range must contain some
element whenever the domain does ( i.e.,dijandrijmust
both be 0 or both be positive), since we cannot generate a
function from a non-empty domain to an empty range. The
rest of the system (8-11) reects the meaning of (exclusive)
rollup and drilldown constraints. For example, an exclusive
rollup constraint ci!cjckconstrains the domains of the
functions<ijand<ikto partition the members of ci. As a
result, the sizes of the domains (9)are related by the equality
dij+dik=card(ci).
The system in Fig. 9 is always satisable in polynomial
time, due to the regularity of (Ds;T[Ws).TestBlox solves
it with a simple greedy algorithm, using the shape delity
metricWsto guide the assignment of values to variables.
We are unaware of results that would suggest the structure
of this problem admits an optimal solution in polynomial
time. After computing a greedy solution to the problem,
TestBlox invokes CPLEX to search for a better one, within
a user-specied time bound. The functions dom andranare
populated using the best available solution.
4.4 Constructing Instance Closures
Given the solutions from the previous steps, as well as
the strengthened model ( Ds;T[Ws), we use the procedure
generate , shown in Fig. 10, to construct a rollup closure
for each schema edge e2Ds. For simplicity, the version of
generate presented in this section omits the handling of soft
distribution constraints specied by p(Def. 5). TestBlox
uses a more sophisticated version that takes pinto account,
guaranteeing conformance to distributions provided for edges
that are exclusively constrained by Ws, and for all cut-edges
inDs. The implemented algorithm would faithfully handle
distributions on all edges of the person model, for example.
Details aside, both the shown and implemented algorithms
follow the same basic approach. They generate rollup map-
pings from citocjwith the aid of three lazy streams: X;
which produces a permutation of the domain set (line 1);
Y;which produces a permutation of the range set (line 2);
andZ;which produces the number of domain members that
map to each range member (line 3). The main loop (lines
6-9) prints zvalues drawn from Xfor each value drawn
fromY, wherezitself is obtained from the stream Z. The
function permutation-stream implements Gray's permu-
tation generator [13], which (deterministically) maps a pos-
7generate (e=cicj;Ds;Ws;card;dom;ran)
1X permutation-stream (card(ci))
2Y permutation-stream (card(cj))
3Z partition-stream (e;Ds;card;dom;ran)
4xo;yo offsets (e;Ws;card;dom;ran)
5advance-streams (X;Y;Z;x o;yo;dom(e);ran(e))
6for(r ran(e);r>0;r r 1)do
7y next(Y)
8for(z next(Z);z>0;z z 1)do
9 print next(X);y
partition-stream (e=cicj;Ds;card;dom;ran)
1(Cbcc;Ebcc) biconnected-component (Ds;e)
2ifjEbccj= 1then
3return sum-stream (ran(e);dom(e))
4a sort ([card(c)forc2Cbcc])
5m binary-search (a;card(cj))
6n binary-search (a;card(ci))
7Z sum-stream (a[n];a[n])
8for(k n;k>m ;k k 1)do
9Z compose (sum-stream (a[k 1];a[k]);Z)
10return Z
Figure 10: Generating roll-ups for cicjin (Ds;T[Ws)
itive number nto a permutation of the set f0;:::;n 1g.
Note that category members are referred to by their indices,
0;:::; card(c) 1.
We use the offsets function (line 4) to indirectly coordi-
nate instantiation of edges that are constrained by Ws. To
illustrate, consider the sample solution for the person model
in Fig. 8. According to C4, the functions <e0and<e1must
collectively map all 32 members of Person . Line 1 yields the
same permutation Xof the Person set for both e0and e1.
Theoffsets function returns xo= 0as the domain oset of
e0, andxo=card(Person ) dom(e1) = 32 24 = 8 as the
domain oset of e1. Using these osets (line 5), e0maps the
rst dom(e0) members of X, which we denote by X[0 : 24],
while e1skips the rst 8 members, mapping X[8 : 32] in-
stead. Consequently, all members of Person ,X[0 : 32] , are
mapped by at least one of the functions. offsets can be
straightforwardly implemented with O(jDsj)worst case time.
Thepartition-stream procedure fullls the global strict-
ness requirement on the rollup relation (Def. 2). It works by
maintaining strictness within each biconnected component of
the schema graph, obtained by viewing the schema as undi-
rected. Recall that a biconnected component of an undirected
graph is a maximal subgraph with no articulation vertices.
An articulation vertex is a node that cannot be removed
without disconnecting the rest of the graph. For example,
the undirected version of the person schema (Fig. 1c) has two
articulation vertices, Person andCity, and three biconnected
components:fe0g,fe1g, andfe2;e3;e4;e5;e6g. It is not
hard to see that any rollup relation for the person graph
is strict if and only if it is strict within each component.
partition-stream computesZstreams for all edges within
a biconnected component (line 1) via a deterministic sequence
(lines 4-6) of stream compositions (lines 7-9), based on the
cardinalities of the categories within the component. The
function sum-stream (k;n)returns a stream of kpositive
values that add up to n, denoted by Zk
n.compose takestwo streams, Zk
nandZn
s, and composes them via addition
to produce a stream Zk
swithkvalues that sum up to s.
To see how this construction process enforces strictness,
consider the component fe2;e3;e4;e5;e6gin Fig. 1c. Line
4 constructs the array a= [4;8;16]for all ve edges. The
arrayagoverns the construction of the resulting Zstreams
so that, for example,
partition-stream (e2;:::) =Z8
16;
partition-stream (e5;:::) =Z4
8;and
partition-stream (e4;:::) =compose (Z4
8;Z8
16):
The above means that both the rollup function <e4, and
the composition of <e2and<e5, are contained in the same
total, surjective function from CitytoCountry . Consequently,
every city must map to the same country via all paths in a
customer instance that go directly through <e4, or indirectly,
through<e2and<e5. A similar intuitive argument can be
made for all pairs of paths in a biconnected graph.
5. EVALUATION
To evaluate TestBlox , we conducted a set of experiments
on three multidimensional data models developed at a re-
tail consulting company. All three models are used in real
decision-support applications. For condentiality reasons, we
refer to the applications as A1, A2, and A3. Two of these,
A1 and A3, are deployed at major retail chains; A2 is a demo
application. Figure 11 shows the anonymized dimension
schemas for each application's analysis space. Every graph
corresponds to one dimension model, for a total of 10 models.
The experiments were designed to evaluate the scalability
ofTestBlox , and the quality of the generated data. The
experimental setup is described in Sec. 5.1, and the results
in Sec. 5.2. All experiments were conducted on an ordinary
laptop with a two core 2.8 GHz processor and 8 GB of
RAM. TestBlox is implemented in Java, using CPLEX [20]
for optimization, and the Apache Commons Mathematics
library [7] for distribution sampling.
5.1 Experimental Setup
Our applications' data models were developed using a
graphical modeling tool, with support for specifying schema
graphs but no notion of dimension constraints. As a result,
any assumptions about the expected shape of the dimension
data are hard-coded in the application logic, and dicult
to recover. We therefore evaluate TestBlox against each
application's schema using a randomly generated dimension
model (Table 1a), and four sets of randomly generated soft
constraints. The soft constraint sets describe instances of
varying sizes, ranging from .25 to 1 GB of data (Table 1b).
The generated dimension models are intended to simulate
a worst-case usage scenario, in which every element of its
schema graph participates in as many constraints as allowed
by Def. 10. We place random lower and upper bounds on
the cardinality of each category, and ensure that each edge
is involved in at least one (exclusive) rollup or drilldown
constraint. The cardinality constraints are tiered, and the
exclusivity constraints are placed only on edges leaving the
sources, or entering the sinks, of biconnected components.
The placement of exclusivity constraints approximates the
use of generalization and specialization patterns [30, 31, 32].
The generated soft constraints are similarly intended to
simulate a realistic but challenging usage scenario. We place
8(A1)
 (A2)
 (A3)
Figure 11: Dimension schemas for three retail applications
A1 A2 A3
categories 51 42 57
edges 62 63 56
cardinalities 102 84 114
rollups 46 42 23
exc. rollups 4 3 1
drilldowns 44 29 53
exc. drilldowns 2 6 0
(a) Number of categories, edges
and dimension constraintsA1 A2 A3
Soft 1 .253 .252 .249
Soft 2 .501 .496 .498
Soft 3 .750 .751 .748
Soft 4 1.03 1.02 1.06
(b) Instance size, in GB, for
each soft constraint set
Table 1: Summary of experimental parameters
a random scale constraint on each category, ensuring only
that the requested size is within the generated cardinality
bounds. Every edge is also constrained using a shape con-
straint with random source and target participations, as well
as a random distribution function. The participations are
chosen from between .85 and 1, while the distribution is ei-
ther uniform, normal, exponential, or zipan. We attempt to
choose the dening parameters of a distribution so that they
are compatible with other constraints, since TestBlox ig-
nores distributions that are unlikely to produce values within
solution parameters. For example, it would not try to create
rollup mappings for members of a category cusing a uniform
distribution over the interval ( card(c);1).
5.2 Results and Discussion
Eciency. To evaluate the scalability and parallelism
of data generation, we executed TestBlox four times on
the models and soft constraint sets for each application,
increasing the number of instance generation threads from
1 to 4. No special load balancing strategy was used; the
generator relies on the balancing provided by the standard
Java concurrency library. The timing results are grouped
by application and shown in Fig. 12. As evident from the
graphs, the time taken to generate data increases roughly
linearly with instance size and decreases linearly with the
number of threads. Solving time is negligible in all cases,
with the entire process dominated by instance generation.
Generating the largest instances, with 1 GB of data each,
takes 40 seconds or less for all benchmarks.
Data Quality. In addition to execution time, we also
tracked two sets of indicators of data quality for each gener-
ated instance:
(I1) the ratio between the desired and actual size of each
category; and,
(I2) the ratios between the desired and actual source par-
ticipation, target participation, and distribution mean
for each rollup function.The results are summarized in Figure 13 for the largest in-
stance of every benchmark. Smaller instances exhibit similar
patterns.
Since the dimension solving algorithm nds an optimal
solution to cardinality constraints, the I1 values for all three
applications remain very close to 1. Only two categories
overall had ratios that were within 2 percent of 1 rather than
1 exactly. The I2 values are less consistent for two reasons.
First, the solution to edge constraints is not guaranteed to be
optimal, and second, we can only guarantee conformance to
specied distributions for certain kinds of schema edges, as
described in Sec. 4.4. For the remaining edges, distribution
constraints are sacriced to enforce strictness. Consequently,
A3, which has the simplest structure, also has indicators
for 95% of its edges within 20 percent of 1. But even the
indicators for the more complex structures remain largely
within 20 percent of the ideal|85% of all A1 edges have
ratios between .8 and 1.2, while the same is true for 73%
percent of A2 edges.
State-of-the-Art. These results are a signicant im-
provement on the general constraint-solving approach, which
is currently the only way to generate complex multidimen-
sional structures. As discussed in Sec. 6, specialized database
generators [2, 4, 5, 17, 18, 28, 41] can be used to instantiate
tree-shaped schemas, but not general schema DAGs, such as
those in Fig. 11. We could not nd publicly available proto-
types of these tools for comparison on tree-shaped schemas.
For a rough comparison to general constraint solving, we
manually translated the person model to Alloy [21] and in-
stantiated it using the Alloy4 solver, which is based on SAT.
The largest instance we could obtain in 60 seconds consists of
0.0003 GB of data, corresponding to a dimension graph with
about 1650 edges. This instance satises the person model,
but its shape is otherwise arbitrary. Like most constraint
solvers, Alloy provides no support for statistical constraints.
TestBlox , in contrast, took 14 seconds (using 4 threads) to
produce 1 GB of data for A1, corresponding to three dimen-
sion graphs with a total of 14,084,547 edges. The generated
dimensions additionally exhibit desired statistical properties.
6. RELATED WORK
Database Generation. A variety of techniques have
been developed for generating synthetic databases with spe-
cic characteristics. Many of them [5, 13, 17, 18, 41] focus
on scalable, parallel generation of large databases, subject
to user-specied column distributions and intra-table corre-
lations. A classic use case for such tools is the generation of
realistic data for TPC benchmarks [43]. Other approaches,
such as [2, 3, 4, 9, 28, 36], are designed to produce smaller
volumes of data, subject to richer constraints. Most of them
address the following problem|given a query, a schema,
9015304560
[0, 0.8)[.8, 1.2](1.2, 1.4]I2 Indicators for A1Number of Schema EdgesRatio Rangesourcetargetmean[0, 0.8)[.8, 1.2](1.2, 1.4]I2 Indicators for A2
Ratio Range[0 0.8)[.8, 1.2](1.2, 1.4]I2 Indicators for A3
Ratio Range015304560
[.98, 1)1(1, 1.02]I1 Indicators for A1, A2 and A3Number of Schema CategoriesRatio RangeA1A2A3
01020304050
1234Time to Generate A1 InstancesGeneration Time (sec)Number of Threads0.25 GB0.5 GB0.75 GB1 GB
1234Time to Generate A2 Instances
Number of Threads1234Time to Generate A3 Instances
Number of ThreadsFigure 12: Time to generate .25 to 1 GB of data for each benchmark, using 1 to 4 threads
015304560
[0, 0.8)[.8, 1.2](1.2, 1.4]I2 Indicators for A1Number of Schema EdgesRatio Rangesourcetargetmean[0, 0.8)[.8, 1.2](1.2, 1.4]I2 Indicators for A2
Ratio Range[0 0.8)[.8, 1.2](1.2, 1.4]I2 Indicators for A3
Ratio Range015304560
[.98, 1)1(1, 1.02]I1 Indicators for A1, A2 and A3Number of Schema CategoriesRatio RangeA1A2A3
01020304050
1234Time to Generate A1 InstancesGeneration Time (sec)Number of Threads0.25 GB0.5 GB0.75 GB1 GB
1234Time to Generate A2 Instances
Number of Threads1234Time to Generate A3 Instances
Number of Threads
Figure 13: Quality of 1 GB instances, given as the number of categories and edges with I1 and I2 values in the specied ranges
and a specication of the desired output, produce an input
database that conforms to the schema, and that will elicit
an output with the specied characteristics. The speci-
cation may take the form of a table [3], a set of relation
cardinalities [2, 4, 28], or a set of coverage rules [9].
While many of the existing tools are quite general, with
some being based on general-purpose constraint solvers ( e.g.,
[4, 9, 28]), none are designed to work on multidimensional
models with rich constraints. Most could be used to generate
instances of chain and tree-like dimension schemas, but not of
general schema DAGs. For practical use, the latter requires
enforcement of global strictness constraints, as well as con-
straints implied by common modeling patterns. TestBlox
is, to our knowledge, the rst tool with this capability.
Test Data Generation. In addition to database gener-
ators, there are also many tools for producing other kinds of
test data from models ( e.g., [33, 42]) and from programs ( e.g.,
[24, 37]). Of these, the ORM generation tool by Smaragdakis
et al. [42] and McGill et al. [33] is most closely related to
TestBlox . Given a specication in the Object-Role Model-
ing (ORM) language, which is NP-hard to satisfy, the tool
identies a subset of the input that is within a commonly
used, polynomial fragment of the language. It then checks
whether the identied subset of the model is satisable, and,
if it is, generates an instance of that subset. The ORM frag-
ment recognized by the tool cannot express global strictness
constraints over relations, making it unsuitable for specifying
(and instantiating) multidimensional models. The tool itself
diers from TestBlox in that it is designed to produce any
satisfying instance; it does not have a notion of a preferred
instance, described by soft constraints.
Multidimensional Modeling. TheTestBlox mod-
eling language was inspired by much of the prior work on
multidimensional modeling [19, 22, 25, 30, 31, 40]. Schemas
and instances, as dened in Sec. 2, capture all dimension
shapes expressible in existing multidimensional models, with
the exception of instances with non-strict rollup relations [30,31, 40]. We omit this shape since it is not supported in
practice [30]. But it could be easily integrated into our
framework, by removing the strictness requirement from
Def. 2 and treating it as an explicit integrity constraint on
(a subset of) schema edges.
Like the model by Hurtado et al. [19], TestBlox sup-
ports the notion of a dimension constraint. The focus of
the two languages is dierent, however. The TestBlox
language is designed for easy encoding of common modeling
patterns, and for ecient generation of test data. The lan-
guage in [19] is designed for reasoning about a generalized
notion of summarizability. It supports a much richer super-
set of our rollup constraints, but it cannot express either
cardinality or drilldown constraints. It would be interesting
to extend TestBlox to handle a larger subset of the rollup
constraints proposed in [19].
7. CONCLUSION
TestBlox is a new framework for modeling and scalable
generation of complex multidimensional structures. The rst
of its kind, the framework provides a small but expressive
modeling language, and a solver for generating data from
specied models. While the problem of generating data from
TestBlox models is NP-hard in general, we have identied
a practical fragment of the language that can be solved in
polynomial time. Our solution approach is modular, enabling
parallel data generation in the presence of global constraints.
We have also shown how to solve constraints in this fragment
so that the resulting instance exhibits preferred scale and
shape. Plans for future work include enriching the language,
extending its polynomial fragment, and investigating ways
to incorporate reasoning about distributions into the solver.
Acknowledgement. We thank Rastislav Bodik and
the anonymous reviewers of this paper for their valuable
comments and feedback.
108. REFERENCES
[1] K. Aouiche and J. Darmont. Data mining-based
materialized view and index selection in data
warehouses. J. Intell. Inf. Syst. , 33(1):65{93, Aug. 2009.
[2]A. Arasu, R. Kaushik, and J. Li. Data generation using
declarative constraints. In SIGMOD , 2011.
[3] C. Binnig, D. Kossmann, and E. Lo. Reverse query
processing. In ICDE , 2007.
[4] C. Binnig, D. Kossmann, E. Lo, and M. T. Ozsu.
QAGen: generating query-aware test databases. In
SIGMOD , 2007.
[5] N. Bruno and S. Chaudhuri. Flexible database
generators. In VLDB , 2005.
[6] Y. Cao, C. Chen, F. Guo, D. Jiang, Y. Lin, B. C. Ooi,
H. T. Vo, S. Wu, and Q. Xu. ES2: A cloud data
storage system for supporting both OLTP and OLAP.
InICDE , 2011.
[7] Commons Math: The Apache Commons Mathematics
Library. commons.apache.org/math, 2011.
[8] D. Cordeiro, G. Mouni e, S. Perarnau, D. Trystram,
J.-M. Vincent, and F. Wagner. Random graph
generation for scheduling simulations. In SIMUTools ,
2010.
[9] C. de la Riva, M. J. Su arez-Cabal, and J. Tuya.
Constraint-based test database generation for SQL
queries. In AST, 2010.
[10] R. Diestel. Graph Theory . Springer, 2005.
[11] L. D'Orazio and S. Bimonte. Multidimensional arrays
for warehousing data on clouds. In Globe , 2010.
[12] J. F. Geelen. Matchings, Matroids and Unimodular
Matrices . PhD thesis, University of Waterloo, 1995.
[13] J. Gray, P. Sundaresan, S. Englert, K. Baclawski, and
P. J. Weinberger. Quickly generating billion-record
synthetic databases. SIGMOD Rec. , 23, 1994.
[14] J. Han, Y. Chen, G. Dong, J. Pei, B. W. Wah,
J. Wang, and Y. D. Cai. Stream cube: An architecture
for multi-dimensional analysis of data streams. Distrib.
Parallel Databases , 18(2):173{197, Sept. 2005.
[15] V. Harinarayan, A. Rajaraman, and J. D. Ullman.
Implementing data cubes eciently. In SIGMOD , 1996.
[16] R. Hemmecke, M. K oppe, J. Lee, and R. Weismantel.
Nonlinear integer programming. In 50 Years of Integer
Programming 1958{2008: The Early Years and
State-of-the-Art Surveys . Springer-Verlag, 2009.
[17] J. E. Hoag and C. W. Thompson. A parallel
general-purpose synthetic data generator. SIGMOD
Rec., 36(1), 2007.
[18] K. Houkjaer, K. Torp, and R. Wind. Simple and
realistic data generation. In VLDB , 2006.
[19] C. A. Hurtado, C. Gutierrez, and A. O. Mendelzon.
Capturing summarizability with integrity constraints in
OLAP. ACM Trans. Database Syst. , 30(3), 2005.
[20] IBM ILOG CPLEX Optimizer. www.ilog.com, 2011.
[21] D. Jackson. Software Abstractions: Logic, Language,
and Analysis . MIT Press, 2006.
[22] H. V. Jagadish, L. V. S. Lakshmanan, and
D. Srivastava. What can hierarchies do for data
warehouses? In VLDB , 1999.
[23] H. Karlo and M. Mihail. On the complexity of the
view-selection problem. In PODS , 1999.
[24] S. Khurshid and D. Marinov. TestEra: Specication-based testing of Java programs using SAT. ASE'00 ,
11(4), 2004.
[25] J. Lechtenb orger and G. Vossen. Multidimensional
normal forms for data warehouse design. Inf. Syst. ,
28(5), 2003.
[26] Z. Li, S. Zhang, Y. Wang, X.-S. Zhang, and L. Chen.
Alignment of molecular networks by integer quadratic
programming. Bioinformatics , 23(13), 2007.
[27] C. X. Lin, B. Ding, J. Han, F. Zhu, and B. Zhao. Text
cube: Computing IR measures for multidimensional
text database analysis. In ICDM , 2008.
[28]E. Lo, N. Cheng, and W.-K. Hon. Generating databases
for query workloads. Proc. VLDB Endow. , 3(1-2), 2010.
[29] E. Lo, B. Kao, W.-S. Ho, S. D. Lee, C. K. Chui, and
D. W. Cheung. OLAP on sequence data. In SIGMOD ,
2008.
[30] E. Malinowski and E. Zim anyi. Hierarchies in a
multidimensional model: from conceptual modeling to
logical representation. Data Knowl. Eng. , 59(2), 2006.
[31] S. Mansmann and M. H. Scholl. Empowering the
OLAP technology to support complex dimension
hierarchies. IJDWM , 3(4), 2007.
[32] J.-N. Maz on, J. Lechtenb orger, and J. Trujillo. A
survey on summarizability issues in multidimensional
modeling. Data Knowl. Eng. , 68(12), 2009.
[33] M. J. McGill, L. K. Dillon, and R. Stirewalt. Scalable
analysis of conceptual data models. In ISSTA , 2011.
[34] K. Morfonios, S. Konakas, Y. Ioannidis, and N. Kotsis.
ROLAP implementations of the data cube. ACM
Comput. Surv. , 39(4), 2007.
[35] K. G. Murty. Linear Complementarity, Linear and
Nonlinear Programming . Heldermann Verlag, 1988.
[36] A. Neufeld, G. Moerkotte, and P. C. Lockemann.
Generating consistent test data: restricting the search
space by a generator formula. VLDB J , 1993.
[37] C. Olston, S. Chopra, and U. Srivastava. Generating
example data for dataow programs. In SIGMOD ,
2009.
[38] T. B. Pedersen, J. Gu, A. Shoshani, and C. S. Jensen.
Object-extended OLAP querying. Data Knowl. Eng. ,
68(5), 2009.
[39] T. B. Pedersen, C. S. Jensen, and C. E. Dyreson.
Extending practical pre-aggregation in on-line
analytical processing. In VLDB , 1999.
[40] T. B. Pedersen, C. S. Jensen, and C. E. Dyreson. A
foundation for capturing and querying complex
multidimensional data. Inf. Syst. , 26(5), 2001.
[41] T. Rabl, M. Frank, H. M. Sergieh, and H. Kosch. A
data generator for cloud-scale benchmarking. In
TPCTC , 2010.
[42] Y. Smaragdakis, C. Csallner, and R. Subramanian.
Scalable satisability checking and test data generation
from modeling diagrams. ASE, 16(1), 2009.
[43] TPC benchmarks. www.tpc.org.
[44] P. Zhao, X. Li, D. Xin, and J. Han. Graph cube: on
warehousing and OLAP multidimensional networks. In
SIGMOD , 2011.
11APPENDIX
A. PROOFS
Theorem 1.Deciding the satisability of dimension mod-
elsD= (D;)is NP-hard, even when is limited to con-
taining only cardinality, rollup and drilldown constraints.
Proof (by reduction from 3SAT). LetCbe a 3CNF
formula over a set of boolean variables B. Recall that
a 3CNF formula is a conjunction of clauses of the form
`x1_x2_x3', where each xis either a variable b2B
or its negation, :b. An example of a 3CNF formula is
(:b1_b2_b3)^(b1_:b2_:b3)^(:b1_:b2_:b3). We
assume that Cis represented as a set of clauses, each of
which is a set of variables in positive or negative form. We
also assume, without loss of generality, that both positive
and negative forms of each variable appear in the clauses.5
The clause representation of our sample 3CNF would be
ff:b1;b2;b3g;fb1;:b2;:b3g;f:b1;:b2;:b3gg.
The set of clauses Ccan be encoded into a dimension
modelD= (D;) by dening D= (C;) and  as follows:
1.The setCcontains distinguished base and top categories,
c?andc>; a unique category ccfor each clause c2C;
and unique categories cbandc:bfor the positive and
negative form of each variable b2B.
C=fc?;c>g[fccjc2Cg[
fcbjb2Bg[fc:bjb2Bg:
2.The relationmapsc?to every clause category cc;
eachccis mapped to the categories that represent the
positive/negative variables comprising c; and all vari-
able categories cb;c:bare mapped to c>.
=fhc?;ccijc2Cg[
fhcc;cxijc2C^x2cg[
fhcb;c>ijb2Bg[fhc:b;c>ijb2Bg:
3.The set  constrains the cardinality of c>to be 2 and
the cardinality of all other categories to be 1. We use
jcj=kto meanjcjkandjcjk. We also impose
the following constraints on schema edges: a rollup
constraint on each edge that has c?as its source or
c>as its target; a rollup constraint from each clause
category to its constituent variable categories; and a
drilldown constraints from the top category to each pair
of categories representing the positive/negative form of
a variable.
 =fjCj= 1jC2Cnc>g[fjc>j= 2g[
fc?!ccjc2Cg[
fcb!c>jb2Bg[fc:b!c>jb2Bg[
fcc!cx1_cx2_cx3jc=fx1;x2;x3g2Cg[
fcb_c:b c>jb2Bg:
Figure 14a shows the encoding for the sample CNF.
It is easy to see that the encoding is polynomial in the
size of CandB. The dimension schema DforCconsists of
2 +jCj+ 2jBjcategories, which are connected by 4jCj+ 2jBj
edges and constrained by 2+3jCj+5jBjdimension constraints.
5IfBcontains a variable bthat appears with only one polarity
inC, we can discard all clauses that contain band get an
equivalent formula over Bnfbg.c>
c?c:b1c:b2c:b3cb1cb2cb3
cc3cc2 cc1
jcc1j= 1jcb1j= 1jc:b1j= 1jc?j= 1
jcc2j= 1jcb2j= 1jc:b2j= 1jc>j= 2
jcc3j= 1jcb3j= 1jc:b3j= 1
c?!cc1cb1!c>c:b1!c>
c?!cc2cb2!c>c:b2!c>
c?!cc3cb3!c>c:b3!c>
cc1!c:b1_cb2_cb3cb1_c:b1 c>
cc2!cb1_c:b2_c:b3cb2_c:b2 c>
cc3!c:b1_c:b2_c:b3cb3_c:b3 c>
(a) Dimension model DforC
b1=true; b2=true; b3=falsefalse
?:b1 :b2 :b3 b1 b2 b3
c3 c2 c1true
(b) A dimension instance for Dand a solution for C
Figure 14: Dimension model and instance for the 3CNF
problem C=fc1;c2;c3g, where c1=f:b1;b2;b3g,c2=
fb1;:b2;:b3g, and c3=f:b1;:b2;:b3g
It also follows directly from dimension model semantics
(Def. 4) that the model Dhas an instance if and only if C
is satisable. The top category c>has two members, which
encode the boolean values `true' and `false.' The constraints
cb!c>,c:b!c>, andcb_c:b c>ensure that the
positive and negative forms of each variable b2Brollup to
distinct boolean values. The constraint cc!cx1_cx2_cx3
forces each clause to rollup to at least one variable in that
clause, and c?!ccforces the bottom to rollup to each
clause. As a result, in every instance of the model, the single
member of c?must rollup to a member of c>through every
single clause cc. Recall that, due to the global strictness
requirement on instances (Def. 2), the bottom member must,
in fact, rollup to the same top member via all paths. If we
call this top member `true,' then we can simply read o the
solution to the original 3CNF problem|if cbrolls-up to `true'
thenbis true; otherwise c:brolls-up to `true' and bis false.
Similarly, if Chas a solution, then Dhas an instance
where each cb/c:brolls-up to `true' or `false' depending on
the value of b, and eachccrolls-up to at least one cxsuch
thatx2candxis true. Figure 14b shows an instance and
its corresponding solution for our sample CNF.
12Theorem 2.A tiered dimension model Dwith no exclu-
sive rollup or drilldown constraints is always satisable, and
it has hierarchical instances that can be encoded (via closures)
in polynomial time and space with respect to the size of D.
Proof (by construction). LetD= (D;)be a tiered
model such that contains no exclusive rollup and no ex-
clusive drilldown constraints. An example of such a model
is shown in Fig. 15a. We prove that Dis satisable by
constructing a polynomially-sized encoding of a hierarchical
instanced= (D;m;< ). Figure 15b shows an instance of the
sample model that admits such an encoding.
Letmbe the following function over ci2C, where concat
stands for string concatenation:
m(ci) =fconcat (ci;k)j1kmaxcicjl(cj)g(12)
It follows directly from Defs. 1, 2 and 7 that mis a valid
membership function for D, and that it satises all cardinality
constraints in .
Givenm, we can now dene a total function <ijfor each
cicjas follows:
<ij=fhconcat (ci;k);concat (cj;n)ij
1kjm(ci)j^n= min(k;jm(cj)j)g(13)
Note that, due to (12),jm(ci)jjm(cj)jfor allcicj. As
a result, every <ijis surjective.
Using the functions <ij, we dene the relation <to be
[
cicj<ij (14)
From (12)-(14), we have that <is strict, total, and sur-
jective between members of every pair of directly related
categoriescicj. It is not hard to see from the denition
of function composition and (13) that all three properties
also hold for every transitive edge cicj. As a result,
d= (D;m;< ) is an instance of D(by Def. 2), and because
 contains no exclusive constraints, it is also an instance of
D. In particular, any rollup or drilldown constraints in are
automatically (and conservatively) satised by d. Finally,
note that the denitions (12)-(14) comprise an encoding of
dthat is polynomial in the size of D.
Theorem 3.Deciding the satisability of tiered dimen-
sion modelsD, with either exclusive rollups or exclusive
drilldowns, is NP-hard.
Proof (by reduction from 3SAT). We prove the the-
orem by providing a reduction from 3SAT that uses exclusive
drilldown constraints, without relying on exclusive rollups. A
similar reduction can be performed with exclusive rollups and
no exclusive drilldowns. The existence of both reductions
shows that the satisability problem for tiered dimension
models becomes intractable when we allow free use of either
kind of exclusive constraint.
LetCbe a 3CNF formula over a set of boolean variables
B, as dened in the proof of Thm. 1. Let D= (D;)be the
non-tiered model encoding of C, withD= (C;), also as
dened in the proof of Thm. 1. The set of clauses Ccan be
encoded into a tiered dimension model DT= (D;T), where
T=fjcj= 2jc2Cg[fc?!ccjc2Cg[
fcb!c>jb2Bg[fc:b!c>jb2Bg[
fcc!cx1_cx2_cx3jc=fx1;x2;x3g2Cg[
fcbc:b c>jb2Bg:
PersonGender CityRegion StateCountryjPersonj5
jGenderj= 2
jStatej4
jCountryj3
Person!Gender
Person!City
City!State_Region
Person Gender
State_Region Country
(a)D= (D;)
Person1 Person2 Person3 Person4 Person5Gender1 Gender2 City1 City2 City3 City4State1 State2 State3 State4 Region1 Region2 Region3Country1 Country2 Country3
(b)d= (D;m;< )
Figure 15: A tiered dimension model with no exclusivity con-
straints, together with an eciently encodable hierarchical
instance
jcc1j= 2jcb1j= 2jc:b1j= 2jc?j= 2
jcc2j= 2jcb2j= 2jc:b2j= 2jc>j= 2
jcc3j= 2jcb3j= 2jc:b3j= 2
c?!cc1cb1!c>c:b1!c>
c?!cc2cb2!c>c:b2!c>
c?!cc3cb3!c>c:b3!c>
cc1!c:b1_cb2_cb3cb1c:b1 c>
cc2!cb1_c:b2_c:b3cb2c:b2 c>
cc3!c:b1_c:b2_c:b3cb3c:b3 c>
(a) Dimension constraints TforC
c0
2
?:b0
3
b1=true; b2=true; b3=false:b0
1 b0
2
c3 c2 c1false
:b1 :b2 :b3true
b0
1 :b0
2
c0
1 c0
3b0
3 b1 b2 b3
?0
(b) A dimension instance for DTand a solution for C
Figure 16: Dimension constraints Tand instance for the
3CNF problem Cin Fig. 14
Note thatDTis tiered, since all categories are constrained
to have the cardinality 2. Also note that in any valid instance
ofDT, there will be some bottom element and some top el-
13ement that are connected to each other through a member
of every clause category. As in the proof of Thm. 1, calling
such a top element `true' allows us to read o a solution
toC,6and given a solution to Cwe can construct a corre-
sponding instance of DT. Figure 16 shows Tfor the sample
CNF schema in Fig. 14a, as well as an instance of DTthat
represents the same solution to this CNF as in Fig. 14b.
Theorem 4.A regular dimension model Dis always sat-
isable, and it has hierarchical instances that can be encoded
(via closures) in polynomial time and space with respect to
the size ofD.
Proof (by construction). LetD= (D;)be a regu-
lar model. To prove that D= (D;)has an instance, we
construct a stronger model Ds= (Ds;s)fromDas follows:
1.DeneXto be the set of all exclusive constraints in ;
dene L()andR()to be the sets of categories on the
left and right side of a rollup/drilldown constraint ; and
dene rep()to beci!cjfor=ci!cj:::cj+k,
andci cjfor=ci:::ci+n cj.
2. LetDs= (C;s), wheresis dened as
(n[
2XL()R())[([
2XL(rep())R(rep())):
That is,sretains alledges that are not constrained
by an exclusivity constraint. But for each set of edges
constrained by an exclusivity constraint ,sretains
only the edge identied by rep().
3.Lets= (nX)[S
2Xfrep()g. In other words, sis
like , except that it replaces each exclusive constraint
in2Xwith the stronger constraint rep().
By Def. 3, any rollup relation that satises rep()also
satises the exclusive constraint , ifis the only constraint
on the edges from L() to R(). From this observation, the
construction ofDs, and the regularity of D, we know that
Dsis tiered (because Dspreserves the tiering of D); that it
is free of exclusivity constraints; and that every instance of
Dsis an instance ofD. This concludes the proof, since Ds
(and thereforeD) has an instance by Thm. 2.
6It is possible that each top element will have a bottom
rolling-up to it through all clause categories. In this case,
the schema instance is encoding 2 solutions to the original
boolean satisability problem.
14