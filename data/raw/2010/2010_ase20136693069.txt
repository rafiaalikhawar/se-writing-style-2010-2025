Identifying Execution Points for Dynamic Analyses
William N. Sumner
School of Computing Science, Simon Fraser University
wsumner@sfu.caXiangyu Zhang
Department of Computer Science, Purdue University
xyzhang@cs.purdue.edu
Abstract —Dynamic analyses rely on the ability to identify
points within or across executions. In spite of this being a
core task for dynamic analyses, new solutions are frequently
developed without an awareness of existing solutions, their
strengths, their weaknesses, or their caveats. This paper sur-
veys the existing approaches for identifying execution points
and examines their analytical and empirical properties that
researchers and developers should be aware of when using
them within an analysis. In addition, based on limitations in
precision, correctness, and e ﬃciency for techniques that identify
corresponding execution points across multiple executions, we
designed and implemented a new technique, Precise Execution
Point IDs. This technique avoids correctness and precision issues
in prior solutions, enabling analyses that use our approach to
also produce more correct results. Empirical comparison with the
surveyed techniques shows that our approach has 25% overhead
on average, several times less than existing solutions.
I. Introduction
Dynamic analyses help developers identify interesting pro -
gram behaviors at runtime. As a result, these analyses can
simplify or speed up common tasks like debugging [1] and
veriﬁcation [2]. Because these problems are core developme nt
tasks, improving them can lead to lower software developmen t
costs [3], and this, in turn, has led to great interest and gro wth
within the ﬁeld of dynamic analysis as a whole.
One fundamental task in dynamic analyses is identifying a
point within an execution of a program. Such execution points
are sometimes used to provide feedback to developers [4], [5 ].
For example, when a tool like Memcheck within Valgrind
[4] identiﬁes an invalid memory access, it can provide an
execution point showing developers where in the execution
this invalid access occurred. Developers can use this infor -
mation to help ﬁx the bug. Execution points also serve as
input to additional analyses. For instance, dual slicing uses
execution points to identify commonly executed instructio ns
across multiple executions [6]. It uses these common behavi ors
to prune out irrelevant dependences from specialized slice s that
concisely explain concurrency bugs.
In spite of this pervasiveness, dynamic analyses are incons is-
tent and imprecise in how they identify and compute executio n
points. Analyses create their own formulations of execution
point IDs (EPIDs) without understanding existing approaches,
and even among existing techniques, di ﬀerent types of EPIDs
have unexplored properties. Their strengths and weaknesse s
are poorly understood and can lead to unexpected limitation s
of precision or scalability. In addition, one deﬁnition of e xecu-
tion point may be preferable in one context but undesirable i n1defaction(x):
2print (x)
3
4defmain():
5foriin range (3):
6 x =input ()
7 ifx % 2:
8 action(x)fori = 0:
x = 2
if False :
fori = 1:
x = 4
if False :
fori = 2:
x = 5
if True :
action(5)
print (5)SIC+=1
SIC+=1
SIC+=1fori = 0:
x = 1
if True :
action(1)
print (1)
fori = 1:
x = 4
if False :
fori = 2:
x = 5
if True :
action(5)
print (5)SIC+=1
SIC+=1
SIC+=1
SIC+=1
(a) (b) (c)
Fig. 1. A program that prints odd numbers and two executions of the program.
another, yet these trade o ﬀs between the diﬀerent techniques
are presently not well understood.
Consider the program in Fig. 1a. This program reads in
three numbers from the user. If a number is odd, as checked
on line 7, then the program calls action() to print the number out.
Notice that line 2 can execute many times because it is called
from within the loop. As a result, simply using the line numbe r
to identify the execution point is ambiguous because the same
ID may appear multiple times within the same execution.
This is undesirable for many dynamic analyses, as it yields
imprecise or incorrect results [1], [7].
One approach, commonly used in the context of record and
replay techniques [7]–[10], is the Software Instruction Counter
(SIC). An SIC uses a single integer counter that increments a t
function calls and loop backedges during the execution of a
program. Combining the current counter with the current lin e
number yields a pair ( counter,line) that can uniquely identify
an execution point within one execution. For example, the
instances of the ifstatement on line 7 of execution (b) are
identiﬁed by (0,7), (1,7), and (2,7) because of the counter
increments on back edges as shown in Fig. 1b.
However, these identiﬁers only work within a single exe-
cution . The SICs used for execution (b) do not work for the
instructions of execution (c). This is because the executio ns
behave diﬀerently. The SIC is incremented at the call to action()
in the ﬁrst iteration of execution (c), so the identiﬁers for the
ifstatements are (0,7), (2,7), and (3,7). Because the SIC was
incremented at the ﬁrst call in (c) but not in (b), the SICs of
the two executions diverge and cannot be compared after this
point. This is a problem for analyses that compare informati on
across multiple executions [1], [6], [11], [12] because SICs can
only precisely identify points within one execution.
To address this problem, and enable comparison across978-1-4799-0215-6/13/$31.00 c2013 IEEE ASE 2013, Palo Alto, USA81
executions, other EPID techniques exploit program structu re
[13], [14]. Using this information, they are often able to
align the corresponding instructions across multiple exec utions.
Unfortunately, these techniques also have limitations tha t cre-
ate ambiguous or meaningless relationships when identifyi ng
the instructions that align across executions. They also ha ve
substantial limitations in usability. In particular, Stru ctural
Execution Indexing [13] has di ﬃculties scaling to longer
executions, while STAT [14] requires a program core dump
at each point that requires an EPID. In addition, SEI can fail
to identify useful relationships between EPIDs.
In this paper we survey ﬁve existing approaches used to
compute EPIDs for dynamic analyses. The surveyed tech-
niques range in their precision and purpose from only being
able to imprecisely identify points even in one execution to
uniquely identifying points across multiple executions ev en
in the presence of concurrency and nondeterminism. They
range in runtime overhead from none, using only postmortem
analysis, to several times the cost of the original executio n.
Based on the limitations of the existing techniques for cros s-
execution EPIDs, we observed a need for a new technique
that provides meaningful and unambiguous relationships in
execution alignment and without the usability and scalabil ity
limitations of existing approaches. We introduce a new tech -
nique for Precise Execution Point IDs (PEPIDs) that addresses
these goals and has a runtime overhead only slightly higher
than using calling contexts [15].
We have implemented all of these techniques, those sur-
veyed along with PEPID. We evaluated them empirically on
SPEC CINT2006 to illustrate their performance. We also
provide the ﬁrst analytical comparison of these di ﬀerent ap-
proaches, weighing their costs, their beneﬁts, and the scen arios
where one may be more desirable than another. Using this
information, a dynamic analysis designer can know in advanc e
which techniques are most appropriate for his or her purpose s
and avoid inventing or reinventing an approach with known
problems. In summary, the contributions of this paper are:
1) We surveyed and implemented existing techniques for
computing EPIDs for dynamic analyses. We analytically
examine all of the di ﬀerent techniques and compare
them along several spectra in order to weigh their
relative costs, merits, and limitations.
2) We observed problems with producing meaningful, un-
ambiguous relationships between EPIDs as well as with
usability and scalability in existing techniques for cross -
execution EPIDs. To address these problems, we intro-
duce a new cross-execution technique (PEPID) and show
that it avoids the limitations of existing work while also
having lower runtime overhead.
3) We empirically compare the runtime and space over-
heads introduced by the di ﬀerent techniques, those sur-
veyed as well as PEPID, and we show that for cross
execution EPIDs, PEPID is the most e ﬃcient with 25%
average overhead. For intra-execution techniques, SICs
are the most eﬃcient, with 9% overhead.
4) We illustrate how missing meaningful relationships be-tween inter-execution EPIDs can result in undesirable or
incorrect results for dynamic analyses.
II. Existing EPID Techniques
In this section, we review the di ﬀerent approaches for
computing EPIDs that have appeared historically in the cont ext
of dynamic analyses. We consider the intended use cases,
design, and requirements of each technique.
A. Calling Contexts
One of the traditional representations of EPIDs is the calling
context at a point within an execution. The calling context
consists of the list of active functions currently on the cal l
stack. Note, similar to using the line number or program
counter as an EPID in Fig. 1, the calling context is an am-
biguous representation. The same calling context may appea r
multiple times even within one execution. As a result, calli ng
contexts are potentially ambiguous EPIDs, but they provide
a more detailed representation of the static program behavi or
than just a line number. In spite of this, calling contexts ar e
already familiar to developers and can be easily collected b y
walking over the call stack [4], [16]. As a result, many dynam ic
analysis tools use calling contexts during analysis or whil e
generating reports for developers [4], [5], [17], [18].
In spite of their familiarity, calling contexts were tradit ion-
ally costly to collect. Walking over the call stack at every
point of interest can be costly, which has forced some dynami c
analyses to resort to sampling techniques that only analyze
portions of an execution [19]. More recently, e ﬀorts have
focused on eﬃcient means of encoding calling contexts. These
include approaches that can probabilistically encode cont exts
in constant space [20]–[22] as well as approaches that can
precisely encode calling contexts but can require a ﬂexible
encoding size and a slightly higher runtime overhead [15].
In the context of dynamic analysis, more precise informatio n
is usually preferred, so in this paper we only consider the la tter
work, Precise Calling Context Encoding (PCCE) [15]. PCCE
works on the principle that calling contexts are equivalent
to paths through the call graph of a program. The technique
examines the call graph during compilation and numbers all o f
the acyclic paths present in the graph. It then annotates eve ry
edge, or call site, with an arithmetic operation that comput es
the numerical ID of the current path in the call graph at
runtime, similar to Ball-Larus path proﬁling [23]. Combine d
with the current instruction, these comprise a calling cont ext.
PCCE handles recursion by pushing and popping the acyclic
path IDs onto a calling context stack as necessary.
main()
a() b() c()13
1defb():
...
contextID += 1
c()
contextID−= 1
...(a) (b)
Fig. 2. (a) An annotated call graph that encodes all calling c ontexts into
unique integers. (b) Example instrumentation of the function b().
For example, consider the call graph presented in Fig. 2a.
The circle annotations on the call edges denote the amount82added to the context ID before each call and subtracted from
the ID upon return. Fig. 2a illustrates this instrumentatio n
for the function b(). Using this example, the calling context
main→b→cis captured by the pair ( c,2). This reﬂects the
currently executing function, c(), and the numerical ID rep-
resenting the path in the call graph, 2.
Computing these IDs using PCCE requires that a program
be instrumented at compile time, which requires forethough t
and time not applicable for all dynamic analyses. For instan ce,
if a developer wishes to analyze an already compiled program
with a tool that uses PCCE, they have to compile the program
again to have the necessary instrumentation added. In addit ion,
the eﬃciency results achieved by PCCE, 1-3.5% runtime
overhead, exploit proﬁle guided instrumentation and addit ional
optimizations for compressing repetitive and recursive ca lling
contexts. Both of these requirements can be avoided by using
stack walking to extract the calling context, but, as mentio ned
before, they induce a high overhead [20].
B. Software Instruction Counters
Mellor-Crummey and LeBlanc introduced Software Instruc-
tion Counters (SICs) to provide a more precise notion of
execution point for proﬁling and debugging [24]. SICs have
since been used in a variety of dynamic analyses, especially
in the context of nondeterministic recording and replay [7] –
[10], [25]. SICs provided the ﬁrst representation of execut ion
points that was able to uniquely and scalably identify every
instruction within a single execution of a program. They wor k
by maintaining a monotonic counter that indicates the progr ess
through an execution. This gives SICs the advantage of only
adding a single counter and sparse increment operations to
an execution, thus yielding low overhead. While the EPIDs
deﬁned by SICs are unambiguous within a single execution,
the SIC for a point may change across di ﬀerent executions, and
the same SIC may even represent di ﬀerent execution points in
two diﬀerent executions as seen in Fig. 1 and Section I.
Computing SICs involves incrementing a counter at every
function call and back edge in the control ﬂow graph (CFG)
of a program. Fig. 1b-c show the executions of Fig. 1a
with instrumentation for computing SICs (where print,range ,
and input are built-in commands). For any point within an
execution of a program, the SIC instrumentation creates a pa ir,
(counter ,current line ) such that the pair uniquely identiﬁes that
execution point. The counter maintains a notion of forward
progress within the execution, and it is only incremented
at those features within an execution that may cause an
instruction to execute multiple times (loops and function c alls).
Accurately placing instrumentation on back edges requires
static analysis or some additional dynamic analysis to dete ct
loops within individual functions. This mandates either fo re-
thought for the static analysis, just like PCCE, or addition al
runtime, space, and complexity overhead for dynamic loop de -
tection. Instead of instrumenting back edges in the CFG, man y
analyses alternatively instrument the branch points withi n
a program [9], [10]. For executions that terminate or have
side eﬀects, these are equivalent and have the advantage that1defaction(x):
2print (x)
3
4defmain():
5foriin range (3):
6 push((5, 14))
7 x =input ()
8 ifx % 2:
9 push((8, 13))
10 push((11, 12))
11 action(x)
12 pop(12)
13 pop(13)
14 pop(14)Possible Calls to action() :
/an}bracketle{t(5,14)(8,13)(11,12),11/an}bracketri}ht
/an}bracketle{t(5,14)(5,14)(8,13)(11,12),11/an}bracketri}ht
/an}bracketle{t(5,14)(5,14)(5,14)(8,13)(11,12),11/an}bracketri}ht
(a) (b)
Fig. 3. (a) The program from Fig. 1 instrumented for computing S EIs. The
consecutive pushes at 9 and 10 are discussed in the text below . (b) EPIDs for
potential calls to action() .
branch instructions can be easily identiﬁed and instrument ed
by dynamic instrumentation or virtualization tools [4], [2 6].
C. Structural Execution Indexing
While the EPIDs provided by SIC su ﬃce for intra-execution
analyses, we saw in Section I that an EPID deﬁned by
SIC might correspond to the ﬁrst iteration of a loop in
one execution and the last iteration of a loop in another
execution. Indeed, the alignment that SICs create between t he
instructions of two di ﬀerent executions can match instructions
at the beginning of one execution with instructions at the
end of a second. For dynamic analyses that perform inter-
execution analysis, e.g. execution comparison, this can lead to
meaningless results. Intuitively, when there is no relatio nship
between instructions with the same EPIDs across the two
executions, comparing them is uninformative.
This provided the motivation for Structural Execution In-
dexing (SEI) from Xin et al [13]. They observed that some
dynamic analyses compare execution points across executio ns,
but the way that analyses identiﬁed execution points led to
meaningless correspondences, like those established by SI C in
Section I [11], [13]. They instead sought to use the semantic
structure of underlying programs to determine which progra m
points corresponded. They observed that the control struct ures
of a program along with the dynamic control dependence [27]
at runtime established a semantic identity for execution po ints
even across diﬀerent executions, so they used these to uniquely
identify instructions at runtime. The technique maintains a
stack that keeps track of the currently active control struc tures
while a program executes. This stack then acts as the EPID.
The process of computing an SEI based ID for an execution
point is similar to manually maintaining a call stack at runt ime,
except that dynamic control dependence information is also
included in the stack. At every branch (or call) instruction ,
the instruction ID is pushed onto an indexing stack along
with the ID of its postdominator (or return instruction). Th is
(ID, postdominator) pair identiﬁes the region of code that is
control dependent upon the branch (or call) instruction. Up on
encountering a postdominator (or return), all entries in th e
stack postdominated by that instruction are popped from sta ck.
Applying this process to the code from Fig. 1 yields the new831ifa || b:
2action()ifa
ifb
action()
EXITCalls to action() :
/an}bracketle{t(1a,EXIT )(2,EXIT ),2/an}bracketri}ht
/an}bracketle{t(1a,EXIT )(1b,EXIT )(2,EXIT ),2/an}bracketri}ht
(a) (b) (c)
Fig. 4. (a) A simple program where structural execution index ing is dependent
upon the execution path. (b) The CFG with two paths to action() . (c) EPIDs
for the call to action() .
program in Fig. 3a. Note, for each dynamic iteration of the
loop, an (ID, postdominator) token is pushed on the stack at
line 6. As seen in Fig. 3b, showing the EPIDs for each call
toaction() , these tokens track the monotonic progress of an
execution through a loop until the loop ﬁnishes and all itera tion
tokens are popped at line 14. The pop(x) operation removes all
tokens with the postdominator x. The push and pop on lines 9
and 13 bound a region of code control dependent upon line 8
[27], while those on lines 10 and 12 identify the call on line 1 1.
Uniquely identifying function calls is crucial because the same
function may be called multiple times, and not di ﬀerentiating
the call sites would lead to ambiguous EPIDs.
The complete algorithm also contains additional operation s
for optimizing simple loops using counters and for eliminat ing
pushes onto the stack that can be inferred based purely on
where an instruction lies within the CFG. For example, the
push for each loop iteration on line 6 can be replaced by
a counter increment, since the loop has a single conditional
guard. Also, executing the body of the ifstatement in Fig. 3
automatically implies that the ifstatement on line 8 was
executed and the Truebranch taken. Thus, the pushes and pops
on lines 9 and 13 can be safely elided.
The intuition that control dependence creates a semantic
relationship across executions had previously been used fo r
trace similarity metrics [28] and has proven e ﬀective enough
that SEI has gained traction in analyses that examine inter -
execution relationships. It has since been used for tasks ra ng-
ing from automated debugging [1] to concurrent proﬁling [29 ]
to identifying causes of vulnerabilities [12]. In spite of t his,
tracking control dependence can require O(N) space where
Nis the length of an execution, which does not scale for
some programs. The aforementioned optimization heuristic s
can mitigate this problem in practice, but they do not elimin ate
it. We explore the space and runtime overheads in Section V.
In addition SEI requires that a program be instrumented at
compile time to accurately identify postdominators.
As previously noted, SEI was designed to guarantee EPIDs
across executions could only be equal for execution points
that correspond across the executions. In some cases it is
too aggressive in achieving this goal and can create di ﬀerent
EPIDs even when execution points meaningfully correspond
across executions . Consider the code in Fig. 4a. A short-
circuiting oroperation creates the CFG in Fig. 4b with two
branches and two paths to action() on line 2. Note that the
paths through the program split based on the values of aand b,
but the paths that call action() merge together again before this
call. Intuitively, the calls to action() occur at the same execution
point even in diﬀerent executions, so their EPIDs should be the
same. In spite of this, because SEI bases EPID constructionon the control dependence of the execution point, diﬀerent
paths to the same point can have di ﬀerent EPIDs . In this case,
as Fig. 4c shows, one EPID encodes a path where aisTrue
and the call is control dependent on 1 a. The other encodes
the path where just bisTrue, and the call is depends ﬁrst
on 1 bwhich transitively depends on 1 a[27]. We show later
in Section V-C that this counterintuitive relationship lea ds to
undesirable results for dynamic analyses.
D. STAT Ordering
The techniques presented thus far all required either stati c
or dynamic program instrumentation. In some cases, such as
when analyzing a deployed program or a program whose
behavior changes when it is instrumented, it is necessary
to avoid any instrumentation whatsoever. This motivated th e
EPID technique presented by Ahn et al. as a part of their
Stack Trace Analysis Tool (STAT) [14]. STAT was designed
for debugging high performance computing applications wit h
multiple processes. In order to better classify and group
equivalent processes that represented failures, they deve loped
a technique for analyzing core dumps of programs in order
to extract the execution point where a program failed. These
core dumps are essentially snapshots of program memory
and contain not only the call stack of the execution at the
point of failure but also the values of all variables on the
stack or heap at that point. In addition to producing an EPID
from the core dumps, STAT produced a partial ordering of
execution points across di ﬀerent executions. This partial order
was particularly important in the context of analyzing para llel
code that involved multiprocess communication. STAT was
the ﬁrst EPID technique we are aware of that observed how a
partial ordering of EPIDs could be useful for analyses.
EPIDs produced by STAT are also stack based, similar to
those produced by SEI. However, STAT does not have the
control dependence or postdominance information used by SE I.
Instead, STAT infers as much as possible about an execution
point from the core dump. In particular, EPIDs from stat inte r-
leave (1) the call stack of the execution point and (2) values
of certain local variables that show the monotonic progress
of an execution through loops. The call stack of an execution
point can be extracted from the core dump using stack walking
methods mentioned in Section II-A, but ﬁnding variables tha t
show loop progress is more di ﬃcult. Such variables do not
even always exist, so STAT makes no guarantee that EPIDs it
produces are unambiguous. Pragmatically, STAT deﬁnes loop
order variable s (LOVs) that can easily be recognized and
extracted as indicators of loop progress when present. LOVs
must (1) be deﬁned at least once each iteration, (2) be given
strictly increasing or decreasing values over a loop’s life time,
and (3) be given an identical value each particular iteratio n
across all possible executions. Informally, these variabl es are
given a strictly ordered and predeﬁned sequence of values.
STAT also deﬁnes a static analysis for identifying when thes e
variables are available.
Consider the simple program in Fig. 5a. This program
contains two loops, one that iterates over a ﬁxed range of841foriin range (3):
2process_int(i)
3fornode inlinkedList:
4process_node(node)Possible Calls to
process_int() and process_node() :
/an}bracketle{t(1,i/mapsto→0)→(2,process_int )/an}bracketri}ht
/an}bracketle{t(1,i/mapsto→1)→(2,process_int )/an}bracketri}ht
/an}bracketle{t(1,i/mapsto→2)→(2,process_int )/an}bracketri}ht
/an}bracketle{t(4,process_node )/an}bracketri}ht
/an}bracketle{t(4,process_node )/an}bracketri}ht
(a) (b)
Fig. 5. Example of using STAT to identify EPIDs at the calls to process_int()
andprocess_node() .1while a:
2. . .
3ifb:break
4. . .
(d)
Fig. 6. A loop with linear SEI growth.
integers on lines 1 & 2 and another that iterates over a linked
list on lines 3 & 4. Suppose that the linked list contains
two elements. The EPIDs computed by STAT for each call
toprocess_int() orprocess_node() are shown in Fig. 5b. For the
ﬁrst loop, STAT is able to identify that iis a LOV , so its value
inside the loop is extracted and included in the EPID of each
function call. This makes the EPID for each call to process_int()
unique. However, for the second loop, there is no LOV , as the
loop iterates over a linked list. As a result, the EPID contai ns
only the call to process_node() , and the EPIDs are ambiguous.
In contrast to previous techniques, STAT does not require
program instrumentation and thus does not induce additiona l
overhead on an analyzed application. However, it can only
extract an EPID at a location where the program produced
a core dump, e.g. a crashing failure. In practice, this meant
that STAT was strictly a post-mortem technique; it could not
produce EPIDs on the ﬂy as a program was executing. While
this limitation can be worked around by explicitly producin g
core dumps, both the runtime and space overhead of producing
core dumps can be prohibitive. Also note that STAT makes
use of static analysis for identifying the LOVs whose values
it captures. Performing this analysis precisely requires a ccess
to the CFG and variable information available at compile tim e,
but it can also be approximated through binary static analys is,
thus avoiding the need for any compile time information.
E. Lightweight Execution Indexing
While SEI oﬀers an approach for computing EPIDs online at
runtime, the potential overhead can cause scalability prob lems
and interfere with the program being analyzed. This occurs
when loops have multiple guarded exits. Consider the loop
in Fig. 6. SEI pushes a token onto the stack every time
lines 1 or 3 execute because they branch the control ﬂow,
but those tokens will not be popped o ﬀthe stack until the
loop ﬁnishes because the branches are postdominated by a
statement outside the loop. In order to avoid the overhead of
SEI, some analyses instead use information about the number
of times an instruction has been seen within a particular cal ling
context, a particular function invocation, or invocations at a
certain depth of the call stack [30], [31]. A canonical examp le
of this is Lightweight Execution Indexing (LEI), which was
used to identify allocated objects in order to help expose
potential deadlocks in concurrent Java programs [30].The approach of LEI is to maintain a counter for each depth
of the call stack. This counter keeps track of how many times
a particular method has been called at that depth. For instan ce,
the ﬁrst time that the method foo()is called at a depth of 3 on
the stack, its hit counter for the depth 3 is 0. The next time
it is called at the depth of 3 on the stack, its hit counter for
that depth is 1. The counter for each method at each depth is
maintained independently. The LEI for a given execution poi nt
then comprises the current calling context along with the hi t
counts of every call site within the context as well as the hit
count and identity of the currently executing statement.
This approach bounds the size of the an EPID to twice the
size of the calling context. In addition, it maintains a noti on
of forward progress through depth counters, and this notion of
progress is structured by the call stack. As a result, each EP ID
is unique and unambiguous within one particular execution.
Unfortunately, exactly as with SIC, the values of counters
seen in one execution have no guaranteed relationship with
the counters seen in other executions. As a result, Lightwei ght
execution indexing can provide EPIDs within one execution,
but it cannot provide meaningful EPIDs across executions.
Also similar to SIC, LEI does not inherently require that a
program be analyzed or rewritten at compile time. The coun-
ters associated with each function and statement of interes t at
every depth of the call stack can be entirely constructed usi ng
dynamic instrumentation without a need for prior planning.
III. Precise Execution Point IDs
Dynamic analyses comparing multiple executions are in-
creasingly common [1], [6], [12], so having a robust, e ﬃcient
EPID technique that works across executions is important.
Such inter-execution techniques create EPIDs that are only
equal when their corresponding execution points are equiva -
lent. Prior work has called this the execution correspondence
criterion [13]. In spite of this problem’s importance, we se e
that there are only two existing techniques that can provide
EPIDs across executions: SEI and STAT. Both techniques have
limitations that can prevent them from being practical or us eful
for particular dynamic analyses. In particular, we desire a n
inter-execution EPID technique that is:
•Online - An analysis should be able to construct the EPID
for the current point in the execution and as often over
the lifetime of an execution as necessary.
•Low Overhead - An execution running with an EPID
technique should require as little additional runtime and
memory as possible.
•Scalable - Neither the duration of an execution nor the
size of its workload should signiﬁcantly a ﬀect the runtime
or space requirements of the EPID technique.
•Unambiguous - Every instruction or statement within an
execution should have a unique EPID.
•Comprehensive - As a dual to satisfying the execu-
tion correspondence criterion, equivalent execution poin ts
should also yield equal EPIDs.
Neither SEI nor STAT is able to satisfy all of these require-
ments. SEI is not low overhead, scalable, or comprehensive,851defaction(x):
2print (x)
3
4defmain():
5while notDone:
6...
7 action(x)
8action(x)defmain():
while notDone:
...
action(x)
action(x)print (x)
print (x)ifnotDone:
...
action(x)
ifnotDone:
...
action(x)
ifnotDone:
...print (x)
ifnotDone:
...
action(x)
ifnotDone:
...print (x)
ifnotDone:
...defmain():
action(x)
print (x)
(a) (b) (c)
Fig. 7. (a) A small program. (b) The program with calls logicall y inlined.
(c) The program with calls inlined and loops unrolled.
and STAT is not online, unambiguous, or comprehensive. In
this section, we introduce a new EPID technique, Precise
Execution Point ID s (PEPID), that targets all of these criteria.
We start by building an intuition about which points should cor-
respond across executions in order to provide unambiguity a nd
comprehensiveness. We then devise a technique for computin g
EPIDs that produces this correspondence e ﬃciently online.
A. Which Points Correspond?
Because we desire an inter -execution EPID technique, we
must ﬁrst decide which execution points should correspond
or align across executions. The intuition used by SEI was
that the path taken by an execution helped to determine
which execution points were equivalent, and SEI used contro l
dependence to codify this relationship. STAT, in contrast, used
the intuition that loop control variables captured a notion of
forward progress through the loop iterations of an executio n.
But, as we saw before, control dependence prevents com-
prehensiveness, and focusing on loop control variables lea ds
to ambiguity. In contrast, we base PEPID on the idea that
execution points at the same position in a su ﬃciently inlined
and unrolled CFG are equivalent .
Consider a simple program with an acyclic CFG and no
function calls. Each instruction inside the program can be
executed at most once, so an instruction’s position within
the CFG can unambiguously identify the instruction within
an execution. In addition, the same instruction will trivia lly
have the same EPID across all possible executions, thus
guaranteeing comprehensiveness. Unfortunately, this mod el is
unrealistic in general; real programs have both function ca lls
and back edges in their CFGs, both of which can cause
instructions to execute more than once and thus introduce
ambiguity. However, we can extend the intuition of equivale nt
points in the CFG to handle those cases as well.
First, consider programs that also include function calls. A
function may be called from multiple locations, thus execut ing
its body multiple times and making the CFG location an
ambiguous EPID. A simple solution to this in most cases
would be to inline every function call. If every call were
inlined, then function bodies would be duplicated at every
call site, once again ensuring uniqueness. Thus, the positi onof an instruction within this fully inlined CFG serves as an
unambiguous EPID (ignoring loops). This can be seen in
Fig. 7a-b. This simple program makes calls to action() both
inside and outside of the loop. Using the position in the CFG
alone would make these calls to print ()on line 2 ambiguous,
however, once action() is inlined, the calls from inside the loop
are clearly distinguished from those outside of the loop. Of
course, this cannot be done in practice because (1) recursiv e
calls would require an undecidable degree of inlining and (2 )
inlining every function call would simply increase a progra m’s
size too much to be pragmatic. However, we only need
to perform this operation logically for now. We shall later
show that the same correspondence can be computed without
actually inlining any functions at all.
Next, we must handle back edges in the CFGs of a pro-
gram’s functions. Back edges create loops or general cycles
in a CFG and can thus cause instructions to execute multiple
times, again making an instruction’s position in the CFG
ambiguous as an EPID. One approach used by bounded model
checkers is to unroll the loops of a program [32], [33]. Each
iteration of a loop is peeled of into the guarded body of an
ifstatement, and each successive iteration is nested within
the body of the preceding iteration. Fig. 7c illustrates thi s
unrolling in combination with the inlining of function call s.
Again, unrolling a loop su ﬃciently for all executions is not
possible in practice, but we shall show that this limitation is
irrelevant in the next section.
Using this combination of unrolling and inlining, we are
able to deﬁne how execution points relate across executions :
Deﬁnition 1 (Alignment): Given two execution points, p1
and p2from executions e1ande2of program prespectively,
letGbe CFG of psuﬃciently unrolled and inlined to contain
both execution points. Points e1ande2align iﬀthey occur at
the same instruction in G.
This alignment of execution points determines exactly
which points are equivalent and must have equal EPIDs
even across diﬀerent executions. Observe, in this transformed
program G, execution points p1and p2can each be per-
formed at most once in any execution, as guaranteed by
the acyclic structure of the unrolled and inlined CFG. Thus,
the transformed program guarantees that the position in the
control ﬂow graph of the program provides an unambiguous
EPID, and the control ﬂow graph correspondence maintains
comprehensiveness as before. This means that PEPID avoids
the problems with SEI presented in Fig. 4 and Fig. 6.
B. Eﬃciently Computing PEPIDs
As discussed in the last section, inlining all function call s
and unrolling all loops is impractical and even undecidable
in general, so we must compute this equivalence another way.
Instead of actually performing these program transformati ons,
PEPID executes the original program without any extra inlin -
ing or unrolling but at the same time keeps track of the inlini ng
and unrolling operations that would have occurred in order to
identify the current execution point. We keep track of these86Instrument (P)
Input: A program P
for each loop linPdo
insert pushLoopCounter before the loop header of l
insert incrementLoopCounter before loop latches of l
insert popLoopCounter on loop exits of l.
for each callcinPdo
insert pushCallSiteID before c
insert popCallSiteID after c
Fig. 8. Instrument takes in a program Pand modiﬁes it to maintain a PEPID
online. This is the unoptimized instrumentation.
operations on an ID stack, similar to those used in SEI and
STAT. This stack is then what PEPID uses to produce EPIDs.
In particular, we push an entry onto the stack to identify the
call site of every function invocation, popping it as the fun ction
returns (or unwinds for exceptional control ﬂow). This trac ks
the inlining operations for all function invocations. We al so
need to track all unrolling operations for backedges. We ﬁrs t
consider only natural loops, loops with a single entry node o r
loop header , but we extend this to irreducible loops in the next
section. We compactly record the unrolling of natural loops by
pushing a counter for the loop upon loop entry and popping
the counter upon loop exit. We increment the counter upon
every iteration of the loop by instrumenting the loop latche s,
or the edges in the CFG that lead back to the loop header. The
stack also naturally handles nested loops.
Fig. 8 shows a naïve instrumentation algorithm for PEPID.
It does not cover exceptional control ﬂow, but we handle
exceptions by saving the ID stack height before a call that
might throw an exception and pruning the stack to that height
if an exception was thrown. Note that the entries in the stack
related to inlining and the entries related to unrolling may be
maintained independently because they can be unambiguousl y
recombined. This stems from the fact that, given an instruct ion
i, the number of static loops containing imay be readily
identiﬁed. As a result, a PEPID can be broken down into (1)
the inlining ID stack, (2) the unrolling ID stack, and (3) the
current instruction ID. Observe, though, the inlining ID st ack is
precisely equal to the calling context! PCCE already provid es
a means of encoding the calling context that is more e ﬃcient
than explicitly pushing and popping at each call site, so we
can exploit this to make PEPID computation more e ﬃcient.
At any point during the execution, a dynamic analysis can
callgetCurrent PEPID() to yield an EPID of the form:
/an}bracketle{tPCCE context, unrolling ID stack, current instruction /an}bracketri}ht
This tuple comprises an EPID that provides comprehensive-
ness and uniqueness based on the prior construction.
Like SEI and STAT, PEPID requires compile-time knowl-
edge about a program. E ﬃciently computing PCCE calling
contexts requires the call graph, and the unrolling stack
requires loops to be identiﬁed. For programs with only natur al
loops, PEPIDs are compact. The PCCE context is bounded in
size by the calling context depth, and the loop unrolling sta ck
is bounded by the number of nested loops that may be active
at one time. We show in Section V that this instrumentation
scheme allows PEPID to scale with low overhead.A
B
CD
EA
B
CD
EA
BC
D
E
(a) (b) (c)
Fig. 9. (a) A natural loop. (b-c) Irreducible loops.
C. Handling Irreducible Loops
Counting iterations is e ﬀective for natural loops, which have
a unique headers or entry nodes. In that context, unrolling
loops is well deﬁned and corresponds to actions upon the
unrolling ID stack. Programs can also have unnatural or
irreducible loops, which have multiple entry points. Indeed,
half the SPEC CINT2006 benchmarks have such loops. Fig. 9
shows some natural and unnatural loops. With multiple head-
ers, distinguishing a loop body from a nested loop is di ﬃcult.
We use Steensgaard’s generalized loop forest recognition t o
identify irreducible loops and their bodies [34]1. Both (b) and
(c) are individual (irreducible) loops under this approach with
headers B and D for (b) and B, C, and D for (c).
Sometimes, using an iteration counter can still work for
irreducible loops. Given a loop, if there exists a header hof
the loop such that every path from each header h′through the
loop body back to h′must pass through h, then we say that
the header h naturalizes the loop. This is because there exists
a traversal of the CFG such that every backedge in the loop
hashas its destination. Thus, we can use a counter as before
and simply increment it on every loop edge that targets h. An
alternative intuition is that breaking only edges to hwould
destroy all cycles in the loop, so a counter incremented on
hwill uniquely identify instances of this acyclic subregion .
Node B in loop (b) is one such naturalizing header. Note
that this is just a generalization of natural loops, where th e
unique header always naturalizes the loop body. We identify
naturalizing headers using simple static analysis.
Without a naturalizing header, edges to multiple headers
must increment the counter to avoid ambiguity. Conservativ ely,
allheaders may need to increment. This can yield unintuitive
results. For example, the path ABDCDCDC in loop (c) would
have the EPID/an}bracketle{tEntry,{6},C/an}bracketri}htif edges to header nodes incre-
ment the loop counter, but so would the path ABCBCBDC.
Here, Entry is the calling context, and {6} is the unrolling I D
stack. Technically, there exists an unrolling of (c) that pr oduces
these IDs, but it is unclear how meaningful this is in practic e.
Alternatively, we can use the same approach as SEI for only
this small portion of the program. We push the IDs of predi-
cates in the loop that the headers are control dependent upon
and pop them upon their postdominators. This produces the
EPIDs/an}bracketle{tEntry,{(B,E)(D,E)(C,E)(D,E)(C,E)(D,E)},C/an}bracketri}htand
/an}bracketle{tEntry,{(B,E)(C,E)(C,E)(C,E)(B,E)(D,E)},C/an}bracketri}ht, which show
the diﬀerent paths. Both approaches produce unambiguous
inter-execution EPIDs. They merely use di ﬀerent approaches
for unrolling degenerate irreducible loops. In fact, an ana lysis
can correctly select either. If overhead is more important,
1Steensgaard’s approach is preferable to other loop extract ion techniques in
that it produces consistent results regardless of how a CFG i s traversed [35].87incrementing on edges to all headers is preferable. If disam -
biguating paths through these loops is important, then usin g
the localized pushing and popping from SEI is preferable.
IV . Analytical Comparison
In this section, we examine some of the analytical propertie s
of the diﬀerent techniques surveyed and how they impact
which techniques are preferable in di ﬀerent situations. Fig. I
summarizes the results, and we discuss them in detail below.
Availability- Many dynamic analyses require that EPIDs be
available online, e.g. for identifying events like allocat ion or
synchronization during an execution. Most of the technique s
provide EPIDs online, although STAT does not. However, for
analyses that are interested in execution points at the poin t a
program crashes, STAT can still be a useful choice because it
alone avoids the need for any program instrumentation.
Requirements&Instrumentation- The requirements and
time of instrumentation for the techniques can sometimes
create more work for analyses or developers that depend on
EPIDs. For example, STAT places the lowest instrumentation
burden on users and client analyses because it does not modif y
the underlying program. As a result, it is easy for STAT to be
used with an already compiled program. Because it imposes
no overhead, it could even be used on deployed software.
Techniques like SIC and LEI that use local counters can be
implemented using runtime instrumentation alone, so they a lso
impose little burden on users, but they may not be appropriat e
for deployed software. Finally, the remaining techniques a ll
require that programs are recompiled with additional stati c
instrumentation. This requires the most work and planning o n
the part of the developer or client analysis.
Independent of instrumentation, the techniques can also re -
quire additional source level information to be precise. PC CE,
SEI, and PEPID all require additional compilation informat ion,
which is expected since they also require static instrument ation.
However, STAT also requires some compile time information
in order to identify LOVs. This requirement holds in spite of
the fact that STAT performs no instrumentation.
Ambiguousness&inter-execution IDs- Ambiguous EPIDs
do not necessarily confer much information about where an
execution point occurs temporally. Thus, ambiguous tech-
niques may be useful for attaching a lightweight notion of
local execution context to an execution point, but they cann ot
be used for more ﬁne grained execution comparison based
techniques [6]. Note, though, that while both PCCE and STAT
are listed as ambiguous, STAT is unambiguous for programs
in which all loops have identiﬁable LOVs (hence the ‘*’ in
the table).
The major diﬀerentiating feature of inter-execution tech-
niques is that they are able to align loop iterations across
diﬀerent executions. As a result, techniques that do not track
the progress through each loop independently are unable to
provide inter-execution IDs. This e ﬀectively leaves only SEI
and PEPID as viable techniques for analyses requiring such
EPIDs. Note, however, that STAT can also provide this under
the same assumptions of LOVs as before (*).Comprehensiveness- One of the large limitations of SEI
was that it was not comprehensive. While its EPIDs always
established a correspondence across executions, it also cr eated
diﬀerent EPIDs for execution points that did correspond (see
Fig. 4). Note, for programs with LOVs (*), STAT actually is
comprehensive. However, in contrast to both, PEPID provide s
comprehensive inter-execution EPIDs in general, making it a
preferable choice when instrumentation is possible.
Ordering- Some analyses require that EPIDs be ordered.
For example, record and replay techniques require that EPID s
be ordered within one execution (intra) [8]. Some analyses
require stricter orders, where EPIDs are partially ordered
even across executions (inter) [1], [6], [14]. Most of the
techniques are able to provide intra-execution ordering am ong
EPIDs, except for calling contexts with PCCE. SEI, STAT,
and PEPID provide stronger inter-execution ordering as wel l
through happens-before relationships among their EPIDs [3 6].
Space overhead- The size of EPIDs is also an important
concern. The required space ranges from none or a constant
word, STAT and SIC respectively, to proportional to the leng th
of an execution in the worst case for SEI. All other technique s,
however, have EPIDs that grow roughly proportional to the
size of the call stack. We examine later how the sizes of the
EPIDs produced by these techniques compare in practice.
V . Empirical Evaluation
In order to compare these di ﬀerent EPID techniques in
practice, we implemented all of them using LLVM 3.2 as
a program instrumentation platform and compared them on
the SPEC CINT2006 benchmarks. The implementations cover
all basic program behavior covered by these benchmarks,
including exceptional control ﬂow. In this section, we look
closely at the compile time properties as well as the runtime
and space overheads induced by these techniques. We conclud e
by looking at a particular case study that illustrates why
comprehensiveness is important in practice.
Note that neither the runtime nor space overhead com-
parisons include STAT. This is because STAT performs no
instrumentation and thus has no overhead. However, the ef-
fectiveness of STAT depends heavily on the ability to produc e
core dumps and identify LOVs. To gauge whether or not these
variables can be found in practice, we compiled the SPEC
benchmarks and counted the total number of static loops as
well as the number of static loops for which a LOV could be
identiﬁed. Fig. II contains the results.
Overall, a median of 34% of loops had identiﬁable LOVs
across the diﬀerent benchmarks, and 31% of all loops had
such variables. This indicates that relying on LOVs may
not be practical in general. However, STAT was originally
designed for analyzing high performance computing program s.
For programs in that domain, the structure of the programs
may make relying on LOVs practical [14].
A. Runtime Eﬃciency
For each of the techniques except STAT, we ran the SPEC
CINT2006 benchmarks using ‘reference’ workloads 5 times88TABLE I
Analyticalpropertiesofthedifferent EPIDtechniques .
Properties PCCE SIC SEI STAT LEI PEPID
Availability online online online oﬄine online online
Requirements Call Graph NoneControl Dependence
LoopsLoop Order
VariablesNoneCall Graph
Loops
Instrumentation static dynamic static none dynamic static
Ambiguous yes no no yes* no no
Inter-execution no no yes no* no yes
Comprehensive no no no no* no yes
Ordering none intra inter inter intra inter
Space Overhead O(call stack) O(1) O(path length) none O(call stack) O(call stack+unrolling stack)
TABLE II
LOVidentificationfor SPEC CINT2006.
Program # Loops # LOVs % with LOVs
400.perlbench 2151 251 12%
401.bzip2 324 80 25%
403.gcc 7344 1816 25%
429.mcf 57 8 14%
445.gobmk 1444 1090 75%
456.hmmer 425 218 51%
458.sjeng 364 140 38%
462.libquantum 78 60 77%
464.h264ref 1526 1192 78%
471.omnetpp 913 280 31%
473.astar 101 65 64%
483.xalancbmk 8637 1938 22%
total 23364 7138 31%
and computed the median and 95% conﬁdence interval for the
mean. We ran all experiments on a 64-bit Intel i5 machine with
8GB RAM running Ubuntu 13.04. Fig. 10 presents the normal-
ized median of each technique compared to uninstrumented
trials of the benchmark suite. We also present the geometric
means of the normalized results for each technique. Error ba rs
indicate the 95% conﬁdence intervals of the means.
PCCE and SIC usually have the lowest overhead on average,
8% and 9% respectively. The next closest is PEPID with 25%,
then LEI with 70% and SEI with 314%. We immediately see
that in comparison to the other inter-execution technique, SEI,
PEPID consistently produces lower overhead. The original
SEI paper produced overhead near 42% on average, which
diﬀers the results we ﬁnd. While we used clang , SEI used
Diablo/FIT with link time optimization [37], yielding op-
timization diﬀerences. The original evaluation of SEI also
used SPEC CPU95 and CPU2000 benchmarks with smaller
workloads than those present in the 2006 benchmarks. When
we used the ‘test’ workload, the smallest that SPEC provides ,
SEI improved to 90% overhead. This illustrates that scalabi lity
was indeed a problem for SEI. One of the benchmarks, 471.om-
netpp, would not even run using SEI on the reference workload
because the stack used for EPIDs consumed all memory and
crashed the program before completion. In contrast, PEPID’ s
overhead was always closer to SIC and PCCE, in spite of the
fact that it provides a more informative form of EPID.
We also note that the original PCCE paper reports overhead
closer to 3%. The work used proﬁle guided instrumentation
to achieve low runtime overhead, but we did not use proﬁle
guided instrumentation in our LLVM based implementation.
Also, while we used clang to compile programs, PCCE used
gcc, which optimizes programs di ﬀerently. This does notTABLE III
Worstcasememoryoverheadof EPIDtechniques .
Program PCCE SIC SEI STAT LEI PEPID
400.perlbench 197KiB 8B 59.8MiB 0 110KiB 262KiB
401.bzip2 8B 8B 238MiB 0 5KiB 112B
403.gcc 165KiB 8B 885MiB 0 1.1MiB 496KiB
429.mcf 232B 8B 626MiB 0 5.3KiB 488B
445.gobmk 2.7KiB 8B 16.3MiB 0 126KiB 5.4KiB
456.hmmer 32B 8B 255KiB 0 6.1KiB 96B
458.sjeng 368B 8B 21.3KiB 0 20.4KiB 856B
462.libquantum 8B 8B 40MiB 0 5.2KiB 48B
464.h264ref 24B 8B 121KiB 0 9.7KiB 168B
471.omnetpp 1.9KiB 8B>7GiB 0 22.4KiB 1.9KiB
473.astar 8B 8B 1.3MiB 0 5.6KiB 64B
483.xalancbmk 246KiB 8B 2.86GiB 0 12.6MiB 431KiB
mean 51.1KiB 8B 436MiB 0 1.2MiB 99.9KiB
aﬀect our comparison because alltechniques in this paper
were compiled using clang . In addition, using proﬁle guided
optimizations for PCCE would just strengthen the results of
PEPID, since PEPID relies on PCCE as a subtask.
B. Space Overhead
Maintaining the current EPID consumes memory for each
technique except STAT. Table III lists the maximum memory
overhead for each benchmark and technique as well as the
mean across all benchmarks. SIC and STAT require a single
word or no overhead, respectively, which may be preferable
if memory must be conserved. Even though PCCE compactly
encodes the calling context, it still takes 51.1KiB on avera ge
because some benchmarks have deeply nested calls. For in-
stance, 403.gcc has a maximum depth of 21100 calls. Proﬁle
guided instrumentation can help reduce this. However, even the
worst case overhead of PEPID, which uses PCCE, is relatively
low, around 100 KiB on average. It is almost always smaller
than LEI and is orders of magnitude smaller than SEI in spite
of its precision. This makes PEPID a preferable technique fo r
analyses needing inter-execution EPIDs.
C. Client Impact
We now show how a comprehensive technique like PEPID
is preferred over a non-comprehensive technique like SEI fo r
a particular dynamic analysis. We consider an analysis know n
as dual slicing. Dual slicing is a backward slicing techniqu e
that contrasts two executions [6]. Instead of including all
backward dependences for a slice criterion, it includes onl y
those dependences that either (1) exist in only one of the
executions or (2) exist in both executions but deﬁne di ﬀerent
values. In this way, dual slicing produces explanations for
why two executions di ﬀer, which can be useful for debugging89400.perlbench401.bzip2403.gcc 429.mcf
445.gobmk 456.hmmer458.sjeng
462.libquantum464.h264ref 471.omnetp473.astar
483.xalancbmkgeomean1.01.21.41.61.82.0Normalized OverheadBase PCCE SIC SEI LEI PEPID
8%9%314%
70%
25%
Fig. 10. Normalized runtime median overhead of the di ﬀerent EPID techniques on SPEC CINT2006 benchmarks. Error bar s show the 95% conﬁdence
intervals for the mean of each technique.
1x =input ()
2...
3ifa || b:
4print (x)x = 5
...
if True ||...:
print (5)x = 3
...
if False ||True :
print (3)
(a) (b) (c)
1
3a
3b
4. . . . . . . . . 1
3a
3b
4
(d) (e)
Fig. 11. (a) A program that can lead to bad dual slices using SE I. (b) A trace
where aisTrue . (c) A trace where bisTrue . (d) A dual slice using SEI. (e)
A dual slice using PEPID.
[6] or for security analysis [12]. Backward slicing techniq ues
traditionally include too many dependences to be practical
[38], so dual slicing is particularly useful because it prun es
away irrelevant dependences as it contrasts two executions .
EPID techniques like SEI form the foundation of dual
slicing. EPIDs determine whether a dependence in one execu-
tion exists in another. Unfortunately, when noncomprehens ive
EPIDs are used, they can include unnecessary dependences in
the slice, defeating one of the main goals of the technique.
Consider the program in Fig. 11a. This program reads an
integer xfrom the user and prints it if either aorbisTrue.
Suppose there are two di ﬀerent executions of the program, one
where the program prints 5, and the other prints 3 as shown in
Fig. 11b-c. Note that aisTrue in one execution, but only bis
True in the other. This matches the case we considered earlier
in Section II-C, meaning that the print statements in the two
executions have diﬀerent EPIDs under SEI. Because the EPIDs
diﬀer, dual slicing considers them di ﬀerent statements and also
includes their control dependences. The dual slice include s the
diﬀerent values of aand bvia control dependence, even though
they do not actually a ﬀect the output diﬀerences. These irrele-
vant dependences get in the way and impede the user’s ability
to understand why the executions printed di ﬀerent numbers
as shown in Fig. 11d. Here, the arrows denote dependences
in the dual slice. In contrast, a comprehensive technique li ke
PEPID is able to identify that the print statements occur at
the same execution point and identify that the di ﬀering userinput for xcaused the diﬀerent output. Fig. 11e shows the dual
slice when using PEPID and clearly identiﬁes how the input
diﬀerence directly caused the output di ﬀerence.
VI. Related Work
We examined several approaches from literature that com-
pute EPIDs for dynamic analyses [13]–[15], [24], [30]. Each
of these techniques has been used to solve real problems in
dynamic analysis ranging from informing replay techniques
[8] to ﬁne-grained execution comparison [6]. The compariso n
of these techniques along with our new EPID computation
technique, PEPID, is one of the core contributions of this wo rk.
In developing PEPID, we based our system around the
notion that the position within an unwound and unrolled CFG
provides a notion of identity for execution points. This was
inspired in part by bounded model checking [33], but model
checkers do not need to consider the alternative high-level
semantics for unrolling degenerate irreducible loops. Sim ilar
notions of identifying execution points also exist within s tatic
analysis, where k-CFA provides a statically bounded approx i-
mation of execution points using a similar intuition [39].
VII. Conclusion
In this paper, we examined several techniques for computing
execution point IDs (EPIDs) and considered their strengths ,
weaknesses, and limitations. To address limitations of int er-
execution EPIDs, we introduced a new technique, PEPID, that
is able to comprehensively compute inter-execution EPIDs
with signiﬁcantly less space and runtime overhead than ex-
isting techniques. PEPID also produces more meaningful
relationships between EPIDs in di ﬀerent executions. Finally,
we show that establishing these meaningful relationships i s
useful in the context of real world dynamic analyses.
Acknowledgments
This research is supported in part by the National Sci-
ence Foundation (NSF) under grants 0917007, 0845870, and
1218993. Any opinions, ﬁndings, and conclusions or recom-
mendations in this paper are those of the authors and do not
necessarily reﬂect the views of NSF.90References
[1] W. N. Sumner and X. Zhang, “Comparative causality: Explain ing the
diﬀerences between executions,” in ICSE , 2013.
[2] B. S. Gulavani, T. A. Henzinger, Y . Kannan, A. V . Nori, and S. K. Raja-
mani, “Synergy: a new algorithm for property checking,” in Proceedings
of the 14th ACM SIGSOFT international symposium on Foundati ons of
software engineering , ser. SIGSOFT ’06 /FSE-14, 2006, pp. 117–127.
[3] G. J. Holzmann, “Economics of software veriﬁcation,” in Proceedings
of the 2001 ACM SIGPLAN-SIGSOFT workshop on Program analysi s
for software tools and engineering , ser. PASTE ’01, 2001, pp. 80–89.
[4] J. Seward and N. Nethercote, “Using Valgrind to detect un deﬁned value
errors with bit-precision,” in Proceedings of the annual conference on
USENIX Annual Technical Conference , ser. ATEC ’05, 2005, pp. 2–2.
[5] K. Serebryany, D. Bruening, A. Potapenko, and D. Vyukov, “Address-
Sanitizer: a fast address sanity checker,” in Proceedings of the 2012
USENIX conference on Annual Technical Conference , ser. USENIX
ATC’12, 2012, pp. 28–28.
[6] D. Weeratunge, X. Zhang, W. N. Sumner, and S. Jagannathan, “Ana-
lyzing concurrency bugs using dual slicing,” in Proceedings of the 19th
international symposium on Software testing and analysis , ser. ISSTA
’10, 2010, pp. 253–264.
[7] J.-D. Choi and S. L. Min, “Race frontier: reproducing dat a races in
parallel-program debugging,” in Proceedings of the third ACM SIGPLAN
symposium on Principles and practice of parallel programmi ng, ser.
PPOPP ’91, 1991, pp. 145–154.
[8] M. Ronsse, K. De Bosschere, M. Christiaens, J. C. de Kergo mmeaux,
and D. Kranzlmüller, “Record /replay for nondeterministic program exe-
cutions,” Commun. ACM , vol. 46, no. 9, pp. 62–67, Sep. 2003.
[9] K. Veeraraghavan, D. Lee, B. Wester, J. Ouyang, P. M. Chen , J. Flinn,
and S. Narayanasamy, “Doubleplay: parallelizing sequentia l logging and
replay,” in Proceedings of the sixteenth international conference on
Architectural support for programming languages and opera ting systems ,
ser. ASPLOS XVI, 2011, pp. 15–26.
[10] S. T. King, G. W. Dunlap, and P. M. Chen, “Debugging opera ting
systems with time-traveling virtual machines,” in Proceedings of the
annual conference on USENIX Annual Technical Conference , ser. ATEC
’05, 2005, pp. 1–1.
[11] W. N. Sumner and X. Zhang, “Algorithms for automatically co mputing
the causal paths of failures,” in Proceedings of the 12th International
Conference on Fundamental Approaches to Software Engineer ing, ser.
FASE ’09, 2009, pp. 355–369.
[12] N. M. Johnson, J. Caballero, K. Z. Chen, S. McCamant, P. Po osankam,
D. Reynaud, and D. Song, “Di ﬀerential slicing: Identifying causal
execution diﬀerences for security applications,” in IEEE Symposium on
Security and Privacy , 2011, pp. 347–362.
[13] B. Xin, W. N. Sumner, and X. Zhang, “E ﬃcient program execution
indexing,” in Proceedings of the 2008 ACM SIGPLAN conference on
Programming language design and implementation , ser. PLDI ’08, 2008,
pp. 238–248.
[14] D. H. Ahn, B. R. de Supinski, I. Laguna, G. L. Lee, B. Libli t, B. P.
Miller, and M. Schulz, “Scalable temporal order analysis for large scale
debugging,” in Proceedings of the Conference on High Performance
Computing Networking, Storage and Analysis , ser. SC ’09, 2009, pp.
44:1–44:11.
[15] W. N. Sumner, Y . Zheng, D. Weeratunge, and X. Zhang, “Prec ise calling
context encoding,” IEEE Trans. Softw. Eng. , vol. 38, no. 5, pp. 1160–
1177, Sep. 2012.
[16] N. Nethercote and J. Seward, “Valgrind: a framework for h eavyweight
dynamic binary instrumentation,” in Proceedings of the 2007 ACM SIG-
PLAN conference on Programming language design and impleme ntation ,
ser. PLDI ’07, 2007, pp. 89–100.
[17] S. Park, S. Lu, and Y . Zhou, “CTrigger: exposing atomicit y violation
bugs from their hiding places,” in Proceedings of the 14th international
conference on Architectural support for programming langu ages and
operating systems , ser. ASPLOS XIV , 2009, pp. 25–36.
[18] A. Zeller, “Isolating cause-e ﬀect chains from computer programs,” in
Proceedings of the 10th ACM SIGSOFT symposium on Foundation s of
software engineering , ser. SIGSOFT ’02 /FSE-10, 2002, pp. 1–10.
[19] X. Zhuang, M. J. Serrano, H. W. Cain, and J.-D. Choi, “Acc urate,
eﬃcient, and adaptive calling context proﬁling,” in Proceedings of the
2006 ACM SIGPLAN conference on Programming language design and
implementation , ser. PLDI ’06, 2006, pp. 263–271.[20] M. D. Bond and K. S. McKinley, “Probabilistic calling co ntext,” in
Proceedings of the 22nd annual ACM SIGPLAN conference on Obj ect-
oriented programming systems and applications , ser. OOPSLA ’07,
2007, pp. 97–112.
[21] M. D. Bond, G. Z. Baker, and S. Z. Guyer, “Breadcrumbs: e ﬃcient
context sensitivity for dynamic bug detection analyses,” in Proceedings
of the 2010 ACM SIGPLAN conference on Programming language
design and implementation , ser. PLDI ’10, 2010, pp. 13–24.
[22] T. Mytkowicz, D. Coughlin, and A. Diwan, “Inferred call path proﬁling,”
inProceedings of the 24th ACM SIGPLAN conference on Object ori -
ented programming systems languages and applications , ser. OOPSLA
’09, 2009, pp. 175–190.
[23] T. Ball and J. R. Larus, “E ﬃcient path proﬁling,” in Proceedings of the
29th annual ACM/IEEE international symposium on Microarchitecture ,
ser. MICRO 29, 1996, pp. 46–57.
[24] J. M. Mellor-Crummey and T. J. LeBlanc, “A software instru ction
counter,” in Proceedings of the third international conference on Ar-
chitectural support for programming languages and operati ng systems ,
ser. ASPLOS III, 1989, pp. 78–86.
[25] H. Yu and Z. Li, “Fast loop-level data dependence proﬁli ng,” in Pro-
ceedings of the 26th ACM international conference on Superc omputing ,
ser. ICS ’12, 2012, pp. 37–46.
[26] C.-K. Luk, R. Cohn, R. Muth, H. Patil, A. Klauser, G. Lown ey,
S. Wallace, V . J. Reddi, and K. Hazelwood, “Pin: building cus tomized
program analysis tools with dynamic instrumentation,” in Proceedings of
the 2005 ACM SIGPLAN conference on Programming language des ign
and implementation , ser. PLDI ’05, 2005, pp. 190–200.
[27] J. Ferrante, K. J. Ottenstein, and J. D. Warren, “The pro gram dependence
graph and its use in optimization,” ACM Trans. Program. Lang. Syst. ,
vol. 9, no. 3, pp. 319–349, Jul. 1987.
[28] L. Guo, A. Roychoudhury, and T. Wang, “Accurately choos ing execu-
tion runs for software fault localization,” in Proceedings of the 15th
international conference on Compiler Construction , ser. CC’06, 2006,
pp. 80–95.
[29] X. Zhang, A. Navabi, and S. Jagannathan, “Alchemist: A tr ansparent
dependence distance proﬁling infrastructure,” in Proceedings of the 7th
annual IEEE/ACM International Symposium on Code Generation and
Optimization , ser. CGO ’09, 2009, pp. 47–58.
[30] P. Joshi, M. Naik, K. Sen, and D. Gay, “An e ﬀective dynamic analysis
for detecting generalized deadlocks,” in Proceedings of the eighteenth
ACM SIGSOFT international symposium on Foundations of soft ware
engineering , ser. FSE ’10, 2010, pp. 327–336.
[31] H. Cleve and A. Zeller, “Locating causes of program fail ures,” in Pro-
ceedings of the 27th international conference on Software e ngineering ,
ser. ICSE ’05, 2005, pp. 342–351.
[32] A. Biere, A. Cimatti, E. M. Clarke, and Y . Zhu, “Symbolic mod el check-
ing without bdds,” in Proceedings of the 5th International Conference
on Tools and Algorithms for Construction and Analysis of Sys tems, ser.
TACAS ’99, 1999, pp. 193–207.
[33] E. Clarke, D. Kroening, and F. Lerda, “A tool for checkin g ANSI-C
programs,” in Tools and Algorithms for the Construction and Analysis
of Systems (TACAS 2004) , ser. Lecture Notes in Computer Science, vol.
2988. Springer, 2004, pp. 168–176.
[34] B. Steensgaard, “Sequentializing program dependence graphs for irre-
ducible programs,” Microsoft Research, Redmond, Wash, Tech. Rep.
MSR-TR-93-14.
[35] G. Ramalingam, “On loops, dominators, and dominance fronti ers,” ACM
Trans. Program. Lang. Syst. , vol. 24, no. 5, pp. 455–490, 2002.
[36] L. Lamport, “Time, clocks, and the ordering of events in a d istributed
system,” Commun. ACM , vol. 21, no. 7, pp. 558–565, Jul. 1978.
[37] L. Van Put, D. Chanet, B. De Bus, B. De Sutter, and K. De Bos schere,
“Diablo: a reliable, retargetable and extensible link-time rewriting frame-
work,” in 2005 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL
PROCESSING AND INFORMATION TECHNOLOGY (ISSPIT), VOLS
1 AND 2 . IEEE, 2005, pp. 7–12.
[38] X. Zhang, N. Gupta, and R. Gupta, “Pruning dynamic slices with
conﬁdence,” in Proceedings of the 2006 ACM SIGPLAN conference on
Programming language design and implementation , ser. PLDI ’06, 2006,
pp. 169–180.
[39] O. Shivers, “Control-ﬂow analysis of higher-order lan guages,” Ph.D.
dissertation, Carnegie Mellon University, May 1991.91