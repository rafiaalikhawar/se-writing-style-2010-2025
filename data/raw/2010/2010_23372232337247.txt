Bug Prediction Based on Fine-Grained Module Histories
Hideaki Hata∗, Osamu Mizuno†, and Tohru Kikuno∗
∗Osaka University, Osaka, Japan
{h-hata, kikuno }@ist.osaka-u.ac.jp
†Kyoto Institute of Technology, Kyoto, Japan
o-mizuno@kit.ac.jp
Abstract —There have been many bug prediction models
built with historical metrics, which are mined from version
histories of software modules. Many studies have reported t he
effectiveness of these historical metrics. For prediction levels,
most studies have targeted package and ﬁle levels. Predicti on
on a ﬁne-grained level, which represents the method level, i s
required because there may be interesting results compared
to coarse-grained (package and ﬁle levels) prediction. The se
results include good performance when considering quality
assurance efforts, and new ﬁndings about the correlations
between bugs and histories. However, ﬁne-grained predicti on
has been a challenge because obtaining method histories fro m
existing version control systems is a difﬁcult problem. To
tackle this problem, we have developed a ﬁne-grained versio n
control system for Java, Historage. With this system, we tar get
Java software and conduct ﬁne-grained prediction with well -
known historical metrics. The results indicate that ﬁne-gr ained
(method-level) prediction outperforms coarse-grained (p ackage
and ﬁle levels) prediction when taking the efforts necessar y to
ﬁnd bugs into account. Using a correlation analysis, we show
that past bug information does not contribute to method-lev el
bug prediction.
Keywords -bug prediction; ﬁne-grained prediction; ﬁne-
grained histories; historical metrics; effort-based eval uation
I. I NTRODUCTION
Bug prediction has been widely studied and has been one
of many hot topics among researchers. Recent ﬁndings show
the usefulness of collecting historical metrics from soft-
ware repositories for bug prediction models. Many studies
measure software development histories, such as changes
on source code [1], events of development or maintenance
processes [2]–[5], developer-related histories [6]–[13] , and
so on. It is reported that historical metrics are more effect ive
than code complexity metrics [14], [15].
In industry, there are reports of bug prediction in practice .
Microsoft Corporation built a system, CRANE, and reported
its experiences with this system [16]. Historical metrics
including code churn, regression histories, and details of
ﬁxes are collected to build failure prediction models in
CRANE. There is also a report of bug prediction in practice
at Google1. Based on research papers [4], [17], a prediction
model was built using bug-ﬁx information. In both industry
and the academy, bug prediction with historical metrics has
1Bug Prediction at Google, http://google-engtools.blogsp ot.com/2011/12/
bug-prediction-at-google.htmlbecome the focus of attention because of its effectiveness
and understandability.
In the research area of bug prediction, ﬁne-grained pre-
diction is one of the next challenges. In the ESEC/FSE 2011
conference, PhD working groups created a forum to conduct
short surveys on software engineering topics by interviewi ng
conference participants and researching the ﬁeld2. The forum
group who discussed “bug prediction models” concentrated
on the main open challenges in building bug prediction mod-
els. From 27 subjects, including ﬁve from industry and 22
from academia, ﬁne-grained prediction was selected as one
of the future directions. Studies of ﬁne-grained predictio n are
necessary because desirable results may be obtained when
compared to coarse-grained prediction. Recently, studies
take into account the effort of quality assurance activitie s
for evaluating bug prediction results [17]–[21]. Effort-b ased
evaluation considers the effort required to ﬁnd bugs, but
does not evaluate the prediction results only with predicti on
accuracy. Previous studies considered the lines of code
(LOC) of modules as efforts. If we can ﬁnd the most
bugs while investigating the small percentages of LOC
in the entire software, such prediction models would be
desirable. Recent studies reported that ﬁle-level predict ion
models are more effective than package-level prediction,
which has more coarse-grained modules than ﬁle-level, on
Java software [15], [22], [23]. From these results, we can
hypothesize that method-level prediction is more effectiv e
than package-level and ﬁle-level prediction, which means
we can ﬁnd more bugs during quality assurance activities
with method-level prediction while investigating the same
amount of LOC.
Actually, there are studies predicting ﬁne-grained buggy
modules. Kim et al. targeted buggy Java methods by a cache-
based approach [4]. Mizuno and Kikuno predicted buggy
Java methods using a spam-ﬁltering-based approach [24].
However, there have been few studies of ﬁne-grained pre-
diction using well-known historical metrics. This is becau se
of the difﬁculty of collecting method-level historical met rics
since version control systems do not control method histo-
ries. To collect detailed histories, we have proposed a ﬁne-
grained version control system, Historage [25]. Historage is
2http://pwg.sed.hu/978-1-4673-1067-3/12/$31.00 c2012 IEEE ICSE 2012, Zurich, Switzerland 200
constructed on top of Git, and can control method histories
of Java. With this system, we collect historical metrics
for methods to build prediction models, and compare such
models with package-level and ﬁle-level prediction models
based on effort-based evaluation. We empirically evaluate
the prediction models with eight open source projects writt en
in Java.
The contributions of this paper can be summarized as
follows:
•Survey and classiﬁcation of recent historical metrics
proposed in bug prediction studies.
•Study of the effectiveness of method-level prediction
compared with package-level and ﬁle-level prediction
based on effort-based evaluation, and a report of its
effectiveness.
•Analysis of the correlations between bugs and histories
of packages, ﬁles, and methods.
The reminder of this paper is structured as follows. Sec-
tion II summarizes the proposed historical metrics from our
survey. Section III discusses the problem of obtaining ﬁne-
grained module histories, and introduces our ﬁne-grained
version control system. In Section IV, we describe our study
design, including effort-based evaluation, research ques tions,
information of study projects, collected historical metri cs
in our study, and how we collect buggy modules, and the
prediction model we used. Section V reports the results and
lessons we learned, and Section VI discusses the overheads
of ﬁne-grained prediction and threats to the validity of thi s
study. Finally, we conclude in Section VII.
II. H ISTORICAL METRICS
In this Section, we classify historical metrics based on the
target of measurement. We prepare four categories each of
code-related metrics, process-related metrics, organiza tional
metrics, and geographical metrics.
A. Code-Related Metrics
Nagappan and Ball proposed code churn metrics, which
measures the changes made to a module over a development
history [1]. They measured Churned LOC /Total LOC , and
Deleted LOC /Total LOC , for example. Churned LOC is the
sum of added and changed lines of code between a baseline
version and a new version of a module. Based on code churn
metrics the authors built statistical regression models, a nd
reported that code churn metrics are highly predictive of
defect density performed on Windows Server 2003. These
code-related metrics have been basic historical metrics an d
have been used in many studies [10], [14], [15], [26]–[30].
B. Process-Related Metrics
There are many studies of historical metrics related to
development processes.
Changes, ﬁxes, past bugs, etc. Graves et al. measured the
number of changes, the number of past bugs, and the averageage of modules for predicting bugs [2]. They reported the
usefulness of such process-related metrics compared with
traditional complexity metrics from a telephone switching
system study. These process-related metrics have been used
in many studies, for example, the number of changes [3],
[4], [6], [7], [9], [10], [14], [15], [30], [31] , the number
of past bugs [27], [29], [31], [32], the number of bug ﬁx
changes [3], [4], [6], [14], [15], [30], [33], and module age s
[3], [4], [6], [15], [26], [30], [31].
Cache-based approach. Several cache-based bug pre-
diction studies exist [3], [4], [17]. Hassan and Holt, for
example, proposed a “Top Ten List,” which dynamically
updated the list of the ten most likely subsystems to have
bugs [3]. The list is updated based on heuristics including
the most recently changed, most frequently bug ﬁxed, and
the most recently bug ﬁxed as the development progresses.
Kim et al. [4] and Rahman et al. [17] discusses BugCache
andFixCache cache operations. The four heuristics used as
cache update policies in their work are as follows:
•Changed locality: recently changed modules tend to be
buggy.
•New locality: recently created modules tend to be
buggy.
•Temporal locality: recently bug ﬁxed modules tend to
be buggy.
•Spatial locality: a module recently co-changed with
bug-introduced modules tends to be buggy.
The number of co-changes with buggy modules (logical
coupling with bug-introducing modules) are also measured
in other studies [8], [14].
Process complexity metrics. Hassan proposed complex-
ity metrics of code changes [5]. These metrics are designed
to measure the complexity of change processes based on the
conjecture that a chaotic change process is a good indicator
of many project problems. The key idea is that the modules
that are modiﬁed during periods of high change complexity
will have a higher tendency to contain bugs. To measure the
change complexity of a certain period, Hassan proposed to
use Shannon’s Entropy. To measure how much a module is
modiﬁed in complex change periods, different parameters
are prepared and four history complexity metrics (HCM)
are proposed. It is reported from a study with open source
projects that history complexity metrics are better predic tors
than process-related metrics, i.e., prior modiﬁcations an d
prior bugs [5].
C. Organizational Metrics
Historical metrics related to organization are newer met-
rics and have been well studied recently.
Number of developers. Graves et al. measured the num-
ber of developers [2]. From a case study of a telephone
switching system, the authors reported that the number of
developers did not help in predicting the number of bugs.201Weyuker et al. also reported that the number of developers
is not a major inﬂuence on bug prediction models [6].
Structure of organization. To investigate a corollary
of Conway’s Law, “structure of software system closely
matches its organization’s communication structure” [34] ,
Nagappan et al. designed organizational metrics, which in-
clude the number of engineers, the number of ex-engineers,
the number of changes, the depth of master ownership,
the percentage of organizational contribution, the level o f
organizational ownership, overall organizational owners hip,
and the organization’s intersection factor [7]. They repor ted
that these organizational metrics-based failure-prone mo dule
prediction models achieved higher precision and recall val -
ues compared with models with churn, complexity, coverage,
dependencies, and pre-release bug measures from a case
study of Windows Vista.
Mockus investigated the relationship between developer-
centric metrics of organizational volatility and the proba bil-
ity of customer-reported defects [8]. From a case study of a
switching software project, Mockus reported that the numbe r
of developers leaving and the size of the organization have
an effect on software quality, but the number of newcomers
to the organization is not statistically signiﬁcant.
Network metrics. Networks between developers and
modules are analyzed for predicting failures [9]–[11]. Hu-
man factors, such as the contributions of developers, coord i-
nation, and communications are examined based on network
metrics, such as centrality, connectivity, and structural holes.
Ownership. The relationship between ownership and
quality is also investigated. Bird et al. examined the effec ts
of ownership on Windows Vista and Windows 7 [12]. They
measured the number of minor contributors, the number of
major contributors, the total number of contributors, and
the proportion of ownership for the contributor with the
highest proportion of ownership. They found a high ratio
of ownership and many major contributors, and a few minor
contributors are associated with less defects.
Rahman and Devanbu examined the effects of ownership
and experience on quality [13]. They conducted a ﬁne-
grained study about authorship and ownership of code
fragments. They measured the number of lines contributed
by an author divided by the number of lines changed to ﬁx
a bug as an authorship metric, and deﬁned the authorship
of the highest contributor as ownership. From a study of
open-source projects, they reported that a high ownership
value by a single author is associated with lines changed or
deleted to ﬁx bugs, and that lack of specialized experience
on a particular ﬁle is associated with such lines.
D. Geographical Metrics
Geographical metrics are measured for assessing the risks
of distributed development. Bird et al. investigated the
locations of engineers who developed binaries [35]. Bird
et al. classiﬁed distribution levels into buildings, cafet erias,campuses, localities, and continents. From a case study of
Windows Vista, they clariﬁed how distributed development
has little to no effect on post-release failures.
In a study of organizational volatility and its effects on
software defects, Mockus measured the number of sites that
modiﬁed the ﬁle and investigated the distribution of mentor s
and developers [8]. He also reported on a case study of large
switching software to show that geographic distribution ha s
a negative impact on software quality.
III. F INE-GRAINED HISTORIES
In Section II, we discussed various historical metrics. To
measure these historical metrics, we need to obtain version
histories of each module. For packages and ﬁles, it is easy
to collect historical metrics by using builtin commands
of ordinary version control systems. However, there is no
command to investigate the histories of methods in Java ﬁles .
To analyze ﬁne-grained module histories, some tools have
been proposed and used in research. Hassan and Holt,
for example, proposed C-REX , which is an evolutionary
extractor [36]. It records ﬁne-grained entity changes over the
development period. Although C-REX stores entire versions,
it cannot track module histories if there is renaming or
moving. BEAGLE is a research platform [37]. Using origin
analysis , it can identify rename, move, split, and merge.
However, the BEAGLE targets selected release revisions
to apply origin analysis . Bevan et al. proposed Kenyon ,
which is designed to facilitate software evolution researc h
[38]. Although Kenyon records entire versions, rename and
move are not identiﬁed. Zimmermann proposed APFEL ,
which collects ﬁne-grained changes in relational database s
[39]. Although versions are stored entirely, APFEL does not
identify rename or move.
For ﬁne-grained module histories, clarifying existing
methods in particular revisions is not difﬁcult because all
versions of the ﬁles are stored entirely. Matching every se-
quential version is required to obtain entire method histor ies,
but matching is difﬁcult when renaming and moving exist.
Because of this limitation, obtaining entire histories of J ava
methods has been difﬁcult.
To address this problem, we proposed a ﬁne-grained
version control system, Historage [25]. We make use of
the rename/move detection mechanism of Git, a version
control system. When renaming and moving exists, Git
identiﬁes matches based on the similarities of ﬁle contents .
Historage stores all Java methods independently, and contr ol
their histories3. Since Historage is created on top of a Git
version control system, every Git command can be used.
From empirical evaluation with some open source projects,
we found that Historage can identify matches practically
3A tool to create Historage is available from https://github .com/hdrky/
git2historage202Percent of LinesPercent of Bugs Found
0 20 40 60 80 1000 20 40 60 80 100LOC
20%45%
Figure 1. Cost-effectiveness curve
when renaming and moving exist4. With this system, we
can obtain the entire histories of Java methods, and collect
method-level historical metrics.
IV. S TUDY DESIGN
A. Effort-Based Evaluation
Recent studies take into account the effort of quality
assurance activities, such as inspecting and testing predi cted
modules for evaluating prediction models [17]–[21]. These
effort-based evaluations should be desirable for practica l
use of the prediction results. The key idea of effort-based
evaluation is that it discriminates the cost of inspecting a nd
testing for each module. Arisholm et al. pointed out that
the cost of such quality assurance activities on a module is
roughly proportional to the size of the module [20].
Figure 1 illustrates an example of a cost-effectiveness
curve. This curve shows that as the quality assurance cost
increases, the percentage of found bugs increases. The
quality assurance cost is represented as the percentage of
investigated LOC of software. When we inspect or test
modules, the modules are ordered by bug-proneness. If we
ﬁnd most bugs when we investigate the small percentage of
the entire LOC, it should be effective.
To compare different bug prediction results, the percent-
age values of bugs found on the same value of the percentage
of LOC should be easy to understand. For this cutoff value,
20% of LOC is used in some studies [15], [17], [20], [21].
We also choose 20% as this cutoff value because it is more
realistic than investigating the entire LOC. Inspecting 20%
of the entire LOC may be an enormous effort for large
software, or little for small software. So deciding cutoff w ith
absolute value of cumulative LOC is another possible way,
and one of future works is to discuss the results with such
effort-based evaluation.
In Figure 1, a dotted line represents this cutoff of LOC
at20%. If cost-effectiveness curves cross the upper part of
4Git detects rename/move while outputting logs with -M optio n if the
content is similar enough. The default value of this similar ity threshold is
50% . Historage makes use of this mechanism for tracking methods and we
found that it detected more than 99% correct matches in candidates with
this default threshold value.this cutoff line, it is better for the cost of inspection and
testing. In this example, when we inspect top bug-prone
modules until 20% of the entire LOC, it is revealed that
we can investigate 45% of buggy modules.
B. Research Questions
To investigate the effectiveness of ﬁne-grained predictio n,
we compare prediction models on different levels, that is,
packages, ﬁles and methods of Java software. Prediction
models are built with well-known historical metrics pro-
posed and used in previous studies, and are compared with
effort-based evaluation.
Compared with package-level and ﬁle-level prediction,
there is a difference in method-level prediction. Since pac k-
ages consist of ﬁles, the total LOC are equal in both levels.
However, this does not hold in package-level vs. method-
level and ﬁle-level vs. method-level because a ﬁle does not
consist of methods only. For fair comparison with levels
of package, ﬁle, and method, we ignore code except for
methods. This means that bugs only in methods are targeted,
and only the LOC of methods are considered as efforts.
With these settings, we investigate the following three
research questions:
RQ1: Are method-level prediction models more effective
than package-level and ﬁle-level prediction models
with effort-based evaluation?
RQ2: (When method-level prediction models are more
effective.) Why are method-level prediction models
more effective than package-level and ﬁle-level
prediction models?
RQ3: Are there differences in different module levels re-
garding the correlations between bugs and module
histories?
C. Target Projects
We selected eight open-source projects for our study:
Eclipse Communication Framework (ECF), WTP Incuba-
tor, and Xpand were chosen from the Eclipse Projects.
Ant, Cassandra, Lucene/Solr, OpenJPA, and Wicket were
chosen from the Apache Software Foundation. All projects
are written in Java and have relatively long development
histories. We chose these projects because they span varied
application domains: a building tool, a distributed databa se
management system, a text search platform, an object-
relational mapping tool, a web application framework, and
development platform plugins related to the communication
framework, web tools, and a template engine.
We obtained each Git repository5. For package-level and
ﬁle-level prediction, we mine ordinary Git repositories. F or
method-level prediction, we convert ordinary Git reposito ries
to Historage repositories, and mine them. This conversion
5Eclipse Projects from http://git.eclipse.org/ and Apache Software Foun-
dation from http://git.apache.org/203Table I
SUMMARY OF STUDIED PROJECTS
Name Initial Date Last Date # of Commits # of Developers Last L OC # of Files on Last Date
ECF 2004-12-03 2011-05-31 9,748 23 167,283 2,439
WTP Incubator 2007-11-10 2010-07-22 1,133 17 206,533 1,944
Xpand 2007-12-07 2011-05-31 1,038 21 79,589 1,126
Ant 2000-01-13 2011-08-19 12,590 46 118,969 1,194
Cassandra 2009-03-02 2011-09-20 4,423 14 99,940 712
Lucene/Solr 2010-03-17 2011-09-20 3,485 27 347,898 3,301
OpenJPA 2006-05-02 2011-09-15 4,180 26 343,191 4,229
Wicket 2004-09-21 2011-09-20 15,033 25 410,538 6,681
can be done automatically. Table I summarizes information
for each target project. The development period ranges from
18 months to 11 years, and the LOC on the last date of the
studied period ranges from 15k to 370k. Table I also presents
the number of commits (from 1k to 15k), the number of
developers (from 14 to 46), and the number of ﬁles on the
last date (from 700 to 4k). The average LOCs per one ﬁle
varies from 61.4to140.4.
D. Metrics Collection
We collected the major metrics discussed in Section II.
Historical metrics for packages can be measured by the
cumulating values of ﬁles in the packages in most cases.
Method-level historical metrics can be collected from His-
torage repositories similar to collecting ﬁle-level histo rical
metrics from Git repositories. Table II presents all histor ical
metrics collected in this study.
Code-related metrics. For code-related metrics, we mea-
sure LOC and code churn metrics (Added LOC and Deleted
LOC). As stated in Section II-A, these metrics are used
in many studies. Code churn metrics for ﬁles are easily
collected from version control repositories.
Process-related metrics. For process-related metrics, we
collect the basic metrics stated in Section II-B, such as the
number of changes, the number of past bugs, the number of
bug-ﬁx changes, and the existing period (age) of modules.
Some metrics are collected inspired by cache-based ap-
proaches [3], [4], [17]. We collect two types of logical
coupling metrics: the number of logical couplings with bug-
introduced modules and the number of logical couplings
with modules that have been buggy. To investigate the
frequency of changes, we measured average, maximum, and
minimum intervals.
In addition, we also collected one of the history complex-
ity metrics [5]. As stated in Section II-B, there are four types
of historical complexity metrics. In this paper, we select
HCM3sbecause it performed well. This metric is designed
under the assumption that modules are equally affected by
the complexity of a period. For other parameters, we follow
the paper [5].Organizational metrics. Organizational metrics and geo-
graphical metrics are relatively difﬁcult to collect from o pen-
source projects although it may be possible to measure by
integrating information from several software repositori es.
Hence, we measure ownership-related metrics designed in
[12] although there are lots of metrics, especially for orga -
nizational metrics as stated in Section II-C. Organization al
metrics in [12] can be collected only from version control
repositories.
To measure ownership-related metrics, we follow the def-
inition of proportion of ownership in [12]. The proportion o f
ownership of a developer for a particular module is the ratio
of the number of changes by the developer to the number of
total changes for that module. If ownership of an developer
is below a threshold, the developer is considered a minor
developer, otherwise, a major developer. In [12], values
ranging from 2%to10% are suggested as the threshold
based on a sensitivity analysis. Bird et al. targeted compil ed
binaries as modules for study, which tend to be developed by
many developers [12]. On the contrary, ﬁles and methods,
which are our modules for study, are a relatively small size
and are developed by relatively only a few developers. To
take into account this difference, we set the threshold valu e
at20%.
E. Bug Information
Buggy modules are collected based on the SZZ algorithm
(proposed by ´Sliwerski, Zimmermann, and Zeller), which
is designed to identify bug-introducing commits by mining
version control repositories and bug report repositories [ 40].
Buggy modules can be identiﬁed by choosing modiﬁed
modules between bug-introducing commits and bug-ﬁxing
commits. With the SZZ algorithm, bug-introducing and bug-
ﬁxing commits can be linked with each bug ID in bug
reports6.
First, we need bug reports from bug report repositories,
such as Bugzilla and JIRA. In these bug report repositories,
6Bug reports are available from https://bugs.eclipse.org/ bugs/ (Eclipse
Projects), https://issues.apache.org/bugzilla/ (Ant), and https://issues.
apache.org/jira/ (the other projects in the Apache Softwar e Foundation)204Table II
COLLECTED HISTORICAL METRICS
Name Description
Code LOC Lines of code
AddLOC Added lines of code from the initial version
DelLOC Deleted lines of code from the initial version
Process ChgNum Number of changes
FixChgNum Number of bug-ﬁx changes
PastBugNum Number of ﬁxed bug IDs
Period Existing period in days
BugIntroNum Number of logical coupling commits that introd uce more than one bug in other modules
LogCoupNum Number of logical coupling commits that change o ther modules that have been buggy
AvgInterval Period /ComNum
MaxInterval Maximum weeks between two sequential changes
MinInterval Minimum weeks between two sequential changes
HCM History complexity metric HCM3s
Organization DevTotal Total number of developers
DevMinor Number of minor developers
DevMajor Number of major developers
Ownership The highest proportion of ownership
Table III
SUMMARY OF PREDICTION MODULES
# of Packages # of Files # of Methods
Project Tag Date Method LOC Buggy / All Percent Buggy / All Per cent Buggy / All Percent
ECF Root Release 30 2009-06-02 81,324 63 / 322 19.6% 163 / 1,715 9.5% 632 / 11,121 5.7%
WTP Incubator v20090510 2009-05-10 58,407 51 / 121 42.1% 123 / 606 20.3% 318 / 5,492 5.8%
Xpand Galileo RC1 2009-05-18 68,557 55 / 213 25.8% 85 / 1,247 6.8% 270 / 8,273 3.3%
Ant ANT 180 RC1 2010-01-05 82,597 30 / 83 36.1% 87 / 912 9.5% 156 / 9,862 1.6 %
Cassandra casandra-0.6.0-rc1 2010-05-28 35,179 27 / 36 75. 0% 92 / 296 31.1% 279 / 4,419 6.3%
Lucene/Solr lucene solr 31 2011-03-30 137,747 30 / 202 14.9% 59 / 1,940 3.0% 81 / 14,478 0 .6%
OpenJPA 2.0.0 2010-04-19 119,745 24 / 50 48.0% 91 / 1,305 7.0% 162 / 21,323 0.8%
Wicket wicket-1.4.0 2009-08-04 172,277 64 / 720 8.9% 91 / 3,6 63 2.5% 192 / 25,541 0.8%
there are also reports for requesting new features or enhanc e-
ment. To ignore such reports, it is necessary to ﬁlter report s.
From Bugzilla repositories, we exclude enhancement
severity reports, and from JIRA repositories, we collect on ly
bug issue type reports. From a bug report of bug bi,
where irepresents bug ID, we obtain open date OD(bi)
and commit date CD(bi).
With collected bug reports, we then identify bug-ﬁxing
commits. Bug-ﬁxing commits and bug biare linked based
on matching bug IDs in commit messages stored in version
control repositories. While linking commits and bug bi, we
investigated whether commit dates are before CD(bi)or not
to remove improper identiﬁcation of bug-ﬁxing commits.
From each bug-ﬁxing commit, we perform the following
procedure to identify buggy modules:
1) Perform the ‘diff’ command on the same module be-
tween the bug-ﬁxing version and a preceding revision
to locate modiﬁed regions on the bug-ﬁxing commit.2) Examine the initially inserted date of the modiﬁed
regions using line tracking commands, such as ‘git
blame’ or ‘cvs annotate’. If the regions are inserted
before OD(fi), commits creating those regions are
identiﬁed as bug-introducing commits.
3) Identify a module as buggy if the module contains re-
gions that are created in the bug-introducing commits,
and are modiﬁed in the bug-ﬁxing commits.
As reported in [41], naive differencing analysis on step
1 of the procedure should yield incorrect bug-introducing
commits, such as non-behavior change commits and just
format change commits. To remove such false positives, we
ignore changes on blank lines, comment changes, and format
changes. In addition, we ignore changes not on methods to
identify bugs on methods as stated in Section IV-B. This
procedure can be performed automatically. If there is more
than or equal to one buggy ﬁle in a package, we consider
it as a buggy package. Buggy methods are identiﬁed by205mining Historage repositories. This identiﬁcation can als o
be performed automatically.
We identify buggy packages, ﬁles, and methods in one
revision for each project. For this particular revision, we
select tagged revisions or revisions that are nearby tagged
revisions. Table III shows the data of the prediction study
and the result of buggy module identiﬁcation. We obtained
bug reports from the ﬁrst report to the last one until
June 30, 2011. With these reports and entire versions in
obtained version repositories, we identify bug informatio n.
The percentages of buggy packages ranges from 8.9%to
75.0%, the percentages of buggy ﬁles ranges from 2.5%to
31.1%, and the percentage of buggy methods ranges from
0.6%to6.3%. Since we target only the code of methods,
each total LOC is accumulated with the entire method LOC
(Method LOC).
F . Prediction Model
Bug prediction models are built with the historical metrics
shown in Section IV-D. These historical metrics are mea-
sured in the period from the initial date to the tagged date
shown in Table III for each module.
We adopt the RandomForest algorithm [42] as a bug
prediction model. RandomForest is a classiﬁer with many
decision trees that outputs the class that is the mode of the
classes output by individual trees. Lessmann et al. conﬁrme d
its good performance in bug prediction [43]. There are
several other studies using the RandomForest algorithm for
bug prediction [15], [21]. We use a statistical computing
and graphics tool R [44] and a randomForest package for
our study. As shown in Table III, the percentages of buggy
methods are small in total methods. In such cases, predictio n
models tend to predict all methods as non-buggy because
there are only a small number of false positives. In our pilot
study with other prediction models like logistic regressio n,
we found such results. However, with RandomForest mod-
els, not all methods are predicted as non-buggy in every
project.
Using prepared modules in Table III, we conduct a 10-fold
cross validation analysis. Entire modules in one predictio n
level in one project are randomly divided into 10 groups. Of
the 10 groups, a single group is used for testing a model,
and the other 9 groups are used for training the model. The
cross-validation process is repeated 10 times, with each of
the 10 groups used once as test data. The 10 results are
combined into a single validation result.
V. R ESULTS
We present our results following research questions stated
in Section IV-B. Plots of the results are shown from Eclipse
Communication Framework (ECF) and Ant only, and other
results are discussed in text.Percent of LinesPercent of Bugs Found
Percent of LinesPercent of Bugs Found
Percent of LinesPercent of Bugs Found
0 20 40 60 80 1000 20 40 60 80 100PackageFileMethod
(a) ECFPercent of LinesPercent of Bugs Found
Percent of LinesPercent of Bugs Found
Percent of LinesPercent of Bugs Found
0 20 40 60 80 1000 20 40 60 80 100PackageFileMethod
(b) Ant
Figure 2. Cost-effectiveness curves of package-level, ﬁle -level and method-
level prediction
Package File MethodPercent of Bugs Found
0 20 40 60 80
(a) ECFPackage File MethodPercent of Bugs Found
0 20 40 60
(b) Ant
Figure 3. Boxplots of package-level, ﬁle-level, and method -level predic-
tion. Percentages of bugs found in 20% LOC on a 1,000 times run
A. Effort-Based Evaluation: Package, File vs. Method
RQ1: Are method-level prediction models more effective
than package-level and ﬁle-level prediction models with
effort-based evaluation?
Figure 2 shows two plots of cost-effectiveness curves.
A package-level curve (dotted), ﬁle-level curve (dashed),
and a method-level curve (solid) are plotted. We can see
that the method-level curves rise larger than the package-
level and ﬁle-level curves in a small LOC. As a result,
more bugs can be found by method-level prediction when
investigating 20% of the LOC, represented by the cutoff
lines. In all projects, method-level prediction outperfor med
package-level and ﬁle-level prediction.
As Arcuri and Briand insisted, we should collect data
from a large enough number of runs to assess the results of
randomized algorithms because we obtain different results
on every run when applied to the same problem instance
[45]. RandomForest is a randomized algorithm. Figure 2
shows the result on one run. Following the suggested value
of1,000as a very large sample [45], we conducted a 1,000
times run for all projects.
Figure 3 shows the results of the 1,000 run. In each
project, boxplots of the value of percentages of bugs found i n
20% LOC for package-level, ﬁle-level and method-level are
shown. In all projects, we observed the small distributions206Table IV
MEDIAN VALUES OF THE PERCENTAGE OF BUGS FOUND IN 20% LOC
ON1,000 TIMES RUN
Project Package File Method
ECF 19.1 42.3 69.2
WTP Incubator 29.4 37.4 61.0
Xpand 35.2 12.9 51.9
Ant 13.3 25.3 44.9
Cassandra 22.2 20.7 46.6
Lucene/Solr 17.2 52.5 59.3
OpenJPA 20.8 16.5 45.1
Wicket 60.9 65.9 82.3
of the values, and method-level prediction achieved higher
values than package-level and ﬁle-level prediction.
In Table IV, we summarize the median values of the
percentages of found bugs when investigating 20% of LOC
in all modules. The second to fourth column shows the
values of package-level, ﬁle-level, and method-level resu lts.
To detect statistical differences, the Mann-Whitney U-tes t
was used between package-level vs. method-level and ﬁle-
level vs. method-level. In both pairs in all projects, the
differences are statistically signiﬁcant ( p <0.001).
The values of the percentages of found bugs are shown
in bold if the value is more than 40%. In all projects,
method-level prediction achieved more than 40%, and out-
performed package-level and ﬁle-level prediction. Based o n
these results from eight open-source projects, we can answe r
research question RQ1. The answer is clear: method-level
prediction is more effective than both package-level and ﬁl e-
level prediction.
When comparing package-level and ﬁle-level, ﬁle-level
prediction outperformed package-level prediction in ﬁve
projects as shown in Table IV. These results are consistent
with the reports of previous studies [15], [22], [23]. How-
ever, there are opposite results in the Xpand, Cassandra, an d
OpenJPA projects. Our study is different from the previous
studies in counting LOC and targeting bugs: we limit the
LOC of methods and target buggy methods. These settings
lead to an ignorance of ﬁles that have no method, and may
improve the package-level prediction. However, these resu lts
depend on project-speciﬁc data. Therefore, analyzing thes e
project-speciﬁc features is remained as a future work.
B. Why Is Method-Level Prediction Effective?
RQ2: Why are method-level prediction models more ef-
fective than package-level and ﬁle-level prediction model s?
Intuitively, ﬁne-grained prediction may more effective
than coarse-grained prediction because ﬁnding bugs in larg e
modules is difﬁcult. Figure 4 shows boxplots of LOC for
packages, ﬁles, and methods. With the Mann-Whitney U-
test in all pairwise comparisons (packages vs. ﬁles, packag esPackage File Method0 200 400 600 800LOC
(a) ECFPackage File Method0 500 1000 1500LOC
(b) Ant
Figure 4. Size of modules: package-level, ﬁle-level and met hod-level.
All Buggy0 10 20 30 40 50 60Number of methods
(a) ECFAll Buggy0 20 40 60 80 100Number of methods
(b) Ant
Figure 5. Number of all and buggy methods in buggy ﬁles.
vs. methods, and ﬁles vs. methods), we found that the
differences in LOC are statistically signiﬁcant ( p <0.001).
Comparing the median value of the LOC, methods are nearly
ten times smaller than ﬁles, and are from thirty to three-
hundreds times smaller than packages.
Next, we investigated buggy ﬁles by considering how
many methods exist in one ﬁle, and how many buggy
methods exist in the ﬁle. The boxplots of Figure 5 present
the results. In both projects, most of the buggy ﬁles contain
nearly or more than 10methods, but there are only a few
buggy methods. From all of the projects, the median values
of the number of entire methods range from 8to22, and the
median values of the number of buggy methods range from
1to2. Although there are many methods in one buggy ﬁle,
there are only a few actual buggy methods. This indicates
that we need to investigate most of the non-buggy methods
in a ﬁle if the ﬁle is predicted to be buggy.
Similarly, we also investigated buggy packages. From all
of the projects, the median values of the number of entire
methods range from 27to579.5, and the median values of
the number of buggy methods range from 1to5.5. Because
of these non-buggy methods, method-level prediction is
more effective than package-level and ﬁle-level predictio n.207Table V
SPEARMAN CORRELATION BETWEEN THE POST BUGS AND COLLECTED MET RICS . MARKED BY *IF STATISTICALLY SIGNIFICANT (p <0.05)
ECF Xpand Ant Wicket
Metric Package File Method Package File Method Package File Method Package File Method
LOC 0.392* 0.366* 0.239* 0.303* 0.298* 0.205* 0.493* 0.362* 0.164* 0.154* 0.159* 0.094*
AddLOC 0.374* 0.063* 0.003 0.416* 0.237* 0.213* 0.515* 0.319* 0.114* 0.181* 0.089* 0.072*
DelLOC 0.259* 0.069* 0.006 0.416* 0.206* 0.210* 0.366* 0.246* 0.098* 0.075* 0.095* 0.067*
ChgNum 0.268* 0.017 -0.007 0.406* 0.212* 0.192* 0.583* 0.302* 0.112* 0.126* 0.083* 0.073*
FixChgNum 0.333* 0.138* 0.044* 0.299* 0.169* 0.051* 0.550* 0.331* 0.096* 0.337* 0.199* 0.098*
PastBugNum 0.325* 0.137* 0.044* 0.334* 0.170* 0.051* 0.565* 0.329* 0.096* 0.339* 0.200* 0.099*
Period 0.139* 0.146* 0.129* 0.316* 0.196* 0.153* 0.302* 0.139* -0.001 -0.389* -0.207* -0.104*
BugIntroNum 0.307* 0.199* 0.080* 0.126 0.228* 0.161* 0.508* 0.278* 0.088* 0.223* 0.160* 0.069*
LogCoupNum 0.312* 0.163* 0.089* 0.331* 0.146* 0.139* 0.429* 0.202* 0.040* 0.288* 0.144* 0.053*
AvgInterval -0.229* 0.059* 0.109* -0.372* -0.126* -0.035* -0.587* -0.310* -0.113* -0.261* -0.178* -0.110*
MaxInterval -0.108 0.084* 0.121* -0.329* -0.033 -0.021 -0.606* -0.297* -0.086* -0.450* -0.230* -0.118*
MinInterval -0.120* 0.095* 0.114* -0.295* -0.138* -0.035* -0.245* -0.212* -0.105* -0.108* -0.093* -0.090*
HCM 0.249* 0.200* 0.142* 0.325* 0.204* 0.174* 0.564* 0.257* 0.060* -0.210* -0.095* -0.055*
DevTotal 0.198* 0.027 -0.024* 0.571* 0.259* 0.234* 0.580* 0.290* 0.101* -0.099* 0.067* 0.070*
DevMinor 0.225* 0.101* -0.010 0.381* 0.091* -0.004 0.566* 0.305* 0.137* 0.118* 0.065* 0.022*
DevMajor 0.014 -0.040 -0.023* 0.463* 0.262* 0.234* -0.024 -0.063 0.039* -0.016 0.045* 0.069*
Ownership -0.105 -0.010 0.024* -0.547* -0.256* -0.236* -0.431* -0.178* -0.082* -0.001 -0.049* -0.068*
C. Correlation Analysis
RQ3: Are there differences in different module levels
regarding the correlations between bugs and module
histories?
The Spearman correlation values between historical met-
rics and the number of post bugs for package-level, ﬁle-leve l,
and method-level of four projects are shown in TableV. The
number of post bugs is the number of bug IDs that have
not been ﬁxed. Correlations that are statistically signiﬁc ant
(p <0.05) are marked by *. As shown in Table III, the per-
centages of buggy methods is smaller than the percentages
of buggy packages and ﬁles. Thus the correlation values
are lower in method-level. We can make the following
observations:
•Code-related metrics have relatively higher correla-
tions; that is, changes in code are related to bugs.
Although this holds in most projects, there are excep-
tional projects like the Wicket project. In this project,
Java ﬁles and methods do not change repeatedly. The
usefulness of code-related metrics depends on how the
software has evolved.
•Past bug information (FixChgNum and PastBugNum)
does not correlate with post bugs for method-level
prediction. This indicates that methods do not have bugs
repeatedly.
•Metrics about intervals have negative correlations. In
other words, short intervals between changes are related
to bugs in most projects.•Organizational metrics may not contribute to method-
level prediction. This is because many developers have
not changed the methods in the studied projects.
VI. D ISCUSSION
A. Overheads
For method-level prediction, we need additional costs for
package-level and ﬁle-level prediction. Required overhea ds
can be summarized as follows:
1)Preparing Historage: Converting Git repositories to
Historage repositories is needed only for method-level
prediction. Although it takes several hours (a one night
run), there is no need for manual efforts.
2)Mining Historage: Running the SZZ algorithm to
identify buggy modules and collecting historical met-
rics require mining repositories of version control
systems. The essential differences between Git repos-
itories and Historage repositories is the number of
storing modules. To calculate the LOC of one module,
there is no difference in processing time. When we
extract a single entire module’s history, rename/move
identiﬁcation requires an O(n2)processing time where
nis the number of candidates. As a result, to mine a
single log to collect simple historical metrics, such
as the number of changes, ﬁxes, and developers,
Historage requires more processing time than Git.
To collect the process complexity metrics, Historage
requires more processing time because it needs to
analyze multiple logs.
3)Building Models and Prediction: The processing
time of training and testing models highly depends208on the number of modules. Actually, the most time-
consuming task in this study is the 1,000 times run
of 10-fold cross validation analysis for method-level
prediction (it requires one to two days).
Although there are such overheads, we do not need
additional manual procedures for method-level prediction
compared with package-level and ﬁle-level prediction. So
we do not consider them as critical limitations.
B. Threats to Validity
Target projects are limited to open-source software
written in Java. For external validity, there is a threat of
generalization of our result. Projects we targeted are only
open-source projects written in Java. One of the good points
of targeting only open-source software projects in Java is
that there is no opposite result regarding the effectivenes s
of method-level prediction compared with package-level an d
ﬁle-level prediction.
As described in Section IV-C, the eight targeted projects
varied in sizes, domains, and development periods. For
example, the Lucene/Solr project has less than two periods,
and prediction is conducted with only a one-year history and
yields a good result. This result may promote the adoption
of historical metrics based prediction for young projects.
For future work we intend to widen our study to other
projects written in other programming languages, and work
on industrial projects.
Collection of bug information has problems. For con-
struct validity, the main threat is in the phase of collectin g
bug information. Although we adopted a well-known SZZ
algorithm discussed in Section IV-E, it has been reported th at
there is a linking bias in identifying bugs with revision log s
and bug reports [46]. Recently, a new algorithm of linking
bugs and changes has been proposed [47]. This algorithm
may mitigate this threat.
Effort-based evaluation may not reﬂect actual efforts.
In our evaluation, there are also threats to construct va-
lidity. To compare package-level, ﬁle-level, and method-
level prediction, we adopted an effort-based evaluation wi th
cost-effectiveness curves, which has been previously stud ied
[17]–[21]. This effort-based evaluation considers the cos t
of quality assurance activities to be roughly proportional
to the size of the modules, that is, to the lines of code.
For coarse-grained modules, such as packages and ﬁles, it
seems acceptable to consider the sizes of the modules as
effort. However, for methods, it may not be acceptable. For
example, although methods are small, they might require
much more effort than big methods because of the context
of the methods, such as complex call relations or other deep
dependencies.
Discussing these threats is also important for further ﬁne-
grained prediction. When we consider only the sizes of
the modules as efforts, we can hypothesize that block-
level or line-level prediction is more effective than metho d-level prediction for ﬁnding bugs. However, this hypothesiz e
should not be acceptable. Because of this threat, we need
empirical studies of the actual effort, such as times needed ,
and cumulative LOC of the code we need to inspect, by
conducting actual quality assurance activities with diffe rent
prediction levels.
VII. C ONCLUSION
This paper conducted ﬁne-grained bug prediction, which
is a method-level prediction, on Java software based on
recently proposed historical metrics. Using eight open sou rce
projects, package-level, ﬁle-level, and method-level pre dic-
tion models were compared based on effort-based evaluation .
The ﬁndings from our study are as follows. Method-level
prediction is more effective than package-level and ﬁle-
level prediction when considering efforts. This is because
predicted buggy packages and ﬁles contain many non-buggy
packages and ﬁles. From the correlation analysis, we found
that past bug information on methods does not correlate with
post bugs in methods, and organizational metrics may not
contribute to method-level prediction. Code-related metr ics
have positive correlations and interval-related metrics h ave
negative correlations.
Effort-based evaluation may not reﬂect actual efforts.
Therefore, in the future we will also use well-designed
effort calculation or an empirical study of the actual effor ts
should be required. Correlation analysis is also needed for
further study. To discuss the correlations between post bug s
and historical metrics, we need more various-type projects
to study. In addition, we want to compare ﬁne-grained
historical metrics with complexity metrics on methods.
ACKNOWLEDGMENT
This research is supported by a Grant-in-Aid for JSPS
Fellows (No.23-4335) and a Grant-in-Aid for Scientiﬁc
Research (c) (21500035) Japan.
REFERENCES
[1] N. Nagappan and T. Ball, “Use of relative code churn
measures to predict system defect density,” In ICSE ’05, pp.
284–292, 2005.
[2] T. L. Graves, A. F. Karr, J. S. Marron, and H. Siy,
“Predicting fault incidence using software change history ,”
IEEE Trans. Softw. Eng. , vol. 26, pp. 653–661, July 2000.
[3] A. E. Hassan and R. C. Holt, “The top ten list: Dynamic
fault prediction,” In ICSM ’05, pp. 263–272, 2005.
[4] S. Kim, T. Zimmermann, E. J. Whitehead Jr., and A. Zeller,
“Predicting faults from cached history,” In ICSE ’07, pp.
489–498, 2007.
[5] A. E. Hassan, “Predicting faults using the complexity of
code changes,” In ICSE ’09, pp. 78–88, 2009.
[6] E. J. Weyuker, T. J. Ostrand, and R. M. Bell, “Do too many
cooks spoil the broth? using the number of developers to
enhance defect prediction models,” Empirical Softw. Eng. ,
vol. 13, pp. 539–559, October 2008.
[7] N. Nagappan, B. Murphy, and V . Basili, “The inﬂuence of
organizational structure on software quality: an empirica l
case study,” In ICSE ’08, pp. 521–530, 2008.209[8] A. Mockus, “Organizational volatility and its effects o n
software defects,” In FSE ’10, pp. 117–126, 2010.
[9] M. Pinzger, N. Nagappan, and B. Murphy, “Can developer-
module networks predict failures?” In SIGSOFT ’08/FSE-16,
pp. 2–12, 2008.
[10] A. Meneely, L. Williams, W. Snipes, and J. Osborne,
“Predicting failures with developer networks and social
network analysis,” In SIGSOFT ’08/FSE-16, pp. 13–23,
2008.
[11] T. Wolf, A. Schroter, D. Damian, and T. Nguyen, “Predict ing
build failures using social network analysis on developer
communication,” In ICSE ’09, pp. 1–11, 2009.
[12] C. Bird, N. Nagappan, B. Murphy, H. Gall, and P. Devanbu,
“Don’t touch my code!: examining the effects of ownership
on software quality,” In ESEC/FSE ’11, pp. 4–14, 2011.
[13] F. Rahman and P. Devanbu, “Ownership, experience and
defects: a ﬁne-grained study of authorship,” In ICSE ’11,
pp. 491–500, 2011.
[14] R. Moser, W. Pedrycz, and G. Succi, “A comparative analy sis
of the efﬁciency of change metrics and static code attribute s
for defect prediction,” In ICSE ’08, pp. 181–190, 2008.
[15] Y . Kamei, S. Matsumoto, A. Monden, K. Matsumoto,
B. Adams, and A. E. Hassan, “Revisiting common bug
prediction ﬁndings using effort-aware models,” In ICSM ’10 ,
pp. 1–10, 2010.
[16] J. Czerwonka, R. Das, N. Nagappan, A. Tarvo, and
A. Teterev, “CRANE: Failure prediction, change analysis an d
test prioritization in practice – experiences from windows ,”
In ICST ’11, pp. 357–366.
[17] F. Rahman, D. Posnett, A. Hindle, E. Barr, and P. Devanbu ,
“BugCache for inspections: hit or miss?” In ESEC/FSE ’11,
pp. 322–331, 2011.
[18] A. G. Koru, K. E. Emam, D. Zhang, H. Liu, and D. Mathew,
“Theory of relative defect proneness,” Empirical Softw. Eng. ,
vol. 13, pp. 473–498, October 2008.
[19] T. Menzies, Z. Milton, B. Turhan, B. Cukic, Y . Jiang,
and A. Bener, “Defect prediction from static code features:
current results, limitations, new approaches,” Automated
Softw. Eng. , vol. 17, pp. 375–407, December 2010.
[20] E. Arisholm, L. C. Briand, and E. B. Johannessen, “A
systematic and comprehensive investigation of methods to
build and evaluate fault prediction models,” J. Syst. Softw. ,
vol. 83, pp. 2–17, January 2010.
[21] T. Mende and R. Koschke, “Effort-aware defect predicti on
models,” In CSMR ’10, pp. 107–116, 2010.
[22] D. Posnett, V . Filkov, and P. Devanbu, “Ecological infe rence
in empirical software engineering,” In ASE ’11, pp. 362–371 ,
2011.
[23] T. H. D. Nguyen, B. Adams, and A. E. Hassan, “Studying
the impact of dependency network measures on software
quality,” In ICSM ’10, pp. 1–10.
[24] O. Mizuno and T. Kikuno, “Training on errors experiment
to detect fault-prone software modules by spam ﬁlter,” In
ESEC-FSE ’07, pp. 405–414, 2007.
[25] H. Hata, O. Mizuno, and T. Kikuno, “Historage: ﬁne-grai ned
version control system for java,” In IWPSE-EVOL ’11, pp.
96–100, 2011.
[26] J. R. Ruthruff, J. Penix, J. D. Morgenthaler, S. Elbaum,
and G. Rothermel, “Predicting accurate and actionable stat ic
analysis warnings: an experimental approach,” In ICSE ’08,
pp. 341–350, 2008.
[27] M. Kl¨ as, F. Elberzhager, J. M¨ unch, K. Hartjes, and
O. von Graevemeyer, “Transparent combination of expert
and measurement data for defect prediction: an industrial
case study,” In ICSE ’10, pp. 119–128, 2010.[28] S. Kim, E. J. Whitehead, Jr., and Y . Zhang, “Classifying
software changes: Clean or buggy?” IEEE Trans. Softw.
Eng., vol. 34, pp. 181–196, March 2008.
[29] T. Zimmermann, N. Nagappan, H. Gall, E. Giger, and
B. Murphy, “Cross-project defect prediction: a large scale
experiment on data vs. domain vs. process,” In ESEC/FSE
’09, pp. 91–100, 2009.
[30] M. D’Ambros, M. Lanza, and R. Robbes, “An extensive
comparison of bug prediction approaches,” In MSR ’10, pp.
31–41, 2010.
[31] T. J. Ostrand, E. J. Weyuker, and R. M. Bell, “Predicting
the location and number of faults in large software systems, ”
IEEE Trans. Softw. Eng. , vol. 31, pp. 340–355, April 2005.
[32] N. Fenton, M. Neil, W. Marsh, P. Hearty, L. Radli´ nski,
and P. Krause, “On the effectiveness of early life cycle
defect prediction with Bayesian nets,” Empirical Softw. Eng. ,
vol. 13, pp. 499–537, October 2008.
[33] P. Knab, M. Pinzger, and A. Bernstein, “Predicting defe ct
densities in source code ﬁles with decision tree learners,” In
MSR ’06, pp. 119–125, 2006.
[34] M. Conway, “How do committees invent,” Datamation
magazine , vol. 14, no. 4, pp. 28–31, 1968.
[35] C. Bird, N. Nagappan, P. Devanbu, H. Gall, and B. Murphy,
“Does distributed development affect software quality? an
empirical case study of windows vista,” In ICSE ’09, pp.
518–528, 2009.
[36] A. E. Hassan and R. C. Holt, “C-REX: An evolutionary code
extractor for C,” CSER meeting, Montreal, Canada, 2004.
[37] M. W. Godfrey and L. Zou, “Using origin analysis to detec t
merging and splitting of source code entities,” IEEE Trans.
Softw. Eng. , vol. 31, pp. 166–181, February 2005.
[38] J. Bevan, E. J. Whitehead, Jr., S. Kim, and M. Godfrey,
“Facilitating software evolution research with kenyon,” I n
ESEC/FSE-13, pp. 177–186, 2005.
[39] T. Zimmermann, “Fine-grained processing of CVS archiv es
with APFEL,” In eclipse ’06, pp. 16–20, 2006.
[40] J. ´Sliwerski, T. Zimmermann, and A. Zeller, “When do
changes induce ﬁxes?” In MSR ’05, pp. 1–5, 2005.
[41] S. Kim, T. Zimmermann, K. Pan, and E. J. J. Whitehead,
“Automatic identiﬁcation of bug-introducing changes,” In
ASE ’06, pp. 81–90, 2006.
[42] A. Liaw and M. Wiener, “Classiﬁcation and regression by
randomforest,” R news , vol. 2, no. 3, pp. 18–22, 2002.
[43] S. Lessmann, B. Baesens, C. Mues, and S. Pietsch,
“Benchmarking classiﬁcation models for software defect
prediction: A proposed framework and novel ﬁndings,” IEEE
Trans. Softw. Eng. , vol. 34, pp. 485–496, July 2008.
[44] The R Project for Statistical Computing, “R,”
http://www.r-project.org/ .
[45] A. Arcuri and L. Briand, “A practical guide for using
statistical tests to assess randomized algorithms in softw are
engineering,” In ICSE ’11, pp. 1–10, 2011.
[46] C. Bird, A. Bachmann, E. Aune, J. Duffy, A. Bernstein,
V . Filkov, and P. Devanbu, “Fair and balanced?: bias in
bug-ﬁx datasets,” In ESEC/FSE ’09, pp. 121–130, 2009.
[47] R. Wu, H. Zhang, S. Kim, and S.-C. Cheung, “ReLink:
recovering links between bugs and changes,” In ESEC/FSE
’11, pp. 15–25, 2011.210