Singapor e Management Univ ersity Singapor e Management Univ ersity 
Institutional K nowledge at Singapor e Management Univ ersity Institutional K nowledge at Singapor e Management Univ ersity 
Resear ch Collection School Of Computing and 
Information Systems School of Computing and Information Systems 
5-2013 
Mining SQL injection and cr oss site scripting vulner abilities using Mining SQL injection and cr oss site scripting vulner abilities using 
hybrid pr ogram analysis hybrid pr ogram analysis 
Lwin Khin SH AR 
Singapor e Management Univ ersity , lkshar@smu.edu.sg 
Hee Beng K uan T AN 
Lionel C. BRI AND 
Follow this and additional works at: https:/ /ink.libr ary.smu.edu.sg/sis_r esear ch 
 Part of the Softwar e Engineering Commons 
Citation Citation 
SHAR, L win Khin; T AN, Hee Beng K uan; and BRI AND , Lionel C.. Mining SQL injection and cr oss site 
scripting vulner abilities using hybrid pr ogram analysis. (2013). Proceedings of the 35th A CM/IEEE 
International Conf erence on Softwar e Engineering (ICSE), San F rancisco, 2013 Ma y 18-26 . 1-10. 
Available at:Available at:  https:/ /ink.libr ary.smu.edu.sg/sis_r esear ch/4781 
This Conf erence Pr oceeding Ar ticle is br ought t o you for fr ee and open access b y the School of Computing and 
Information Systems at Institutional K nowledge at Singapor e Management Univ ersity . It has been accepted for 
inclusion in Resear ch Collection School Of Computing and Information Systems b y an authoriz ed administr ator of 
Institutional K nowledge at Singapor e Management Univ ersity . For mor e information, please email 
cher ylds@smu.edu.sg . Mining SQL Injection and Cross Site Scripting 
Vulnerabilities using Hybrid Program Analysis  
 
Lwin Khin Shar and Hee Beng Kuan Tan  
Block S2, School of Electrical and Electronic Engineering  
Nanyang Technological University  
Nanyang Avenue, Singapore 639798  
{shar0035, ibktan}@ntu.edu.sg  Lionel  C. Briand  
SnT Centre, University of Luxembourg  
4 rue Alphonse Weicker, L -2721, Luxembourg  
lionel.briand@uni.lu
 
 
Abstract —In previous work, we proposed a set of static  
attributes that characterize input validati on and input 
sanitization code patterns. We showed that some of the proposed 
static attributes are significant  predictors of web  application 
vulnerabilities related to SQL injection and cross site scripting. 
Static attributes have the advantage of reflecti ng general 
properties of a program. Yet, dynamic attr ibutes collected from 
execution  traces may reflect more specific code characteristics 
that are complementary to static attributes. Hence, to improve 
our initial work, in this paper, we propose the use of  dynamic 
attributes to complement static attributes in the prediction of 
vulnerabilities. Furthermore,  since existing work relies on 
supervised learning, it is dependent  on the availability of training 
data  labeled with known vulnerabilities . This paper pr esents 
prediction models that are based on both classification and 
clustering in order to predict vulnerabilities,  working  in the 
presence or absence of labeled training data, respectively. In our 
experiments  across six applications , our new supervised 
vulnerability predictors based on hybrid (static and dynamic) 
attributes achieved, on average, 90% recall and 85% precision, 
that is a sharp increase in recall when compared to static 
analysis -based predictions. Though not nearly as accurate, our 
unsupervised  predictors  based on clustering achieved, on 
average,  76% recall  and 39% precision, thus suggesting they can 
be useful in the absence of labeled training data.   
Index  Terms —Defect predi ction, vulnerability,  input validation 
and sanitization , static and dynamic analysis,  empirical  study .  
I. INTRODUCTION  
SQL injection (SQLI) and cross site scripting (XSS) are the 
two most common and serious web application vulnerabilities 
threatening the privacy and security of both clients and 
applications nowadays [1]. To mi tigate the two threats, many 
vulnerability detection approaches such as static  taint an alysis 
and concolic testing have been proposed. These approaches 
have been shown to be effective in finding many security 
vulnerabilities. Static taint analysis approach es are generally 
easy to be implement ed and adopt ed but are inefficient in 
practice due to high false positive rates  (low precision). 
Concolic testing techniques are highly precise but could be 
impractical for large systems due to state space explosion. 
Alternative or complementary vulnerability detection solutions 
that are both practical for use and precise would be beneficial 
to security teams.  Input validation and input sanitization methods are 
commonly implemented in web applications to guard against 
application -level attacks such as SQLI, XSS, path  traversal, 
and buffer overflow. Hence, intuitively, an application is 
vulnerable if the implementation of input validation and input 
sanitization methods is inadequate or incorre ct. In our initial 
work  [2, 1 6], we mined static code patterns that implement 
such methods to build vulnerability predictors based on 
supervised learning. We showed that those predictors provide 
an alternative, effective  solution for SQLI and XSS 
vulnerabilities . Although these result s were encouraging, our 
earlier work suffers from two major drawbacks —(1) though 
proposed static attribu tes are useful predictors, they  are limited 
in terms of the prediction accuracy they can yield (the 
predictive capability of these attributes is depende nt on how 
precise is the classification of the input validation and 
sanitization code patterns ); (2) being a supervised learning 
based approach, its effectiveness is dependent on the 
availability of sufficient training data labeled with manually 
checked se curity vulnerabilities . 
This paper addresses the above limitations and provides a 
more extensive empirical study than that of our previous work. 
It presents a pattern mining approach based on dynamic 
analysis that classifies  input validation and sanitizati on 
functions through the systematic  execution of these functions 
and the analysis of their execution traces . We  also use both 
supervised learning methods and unsupervised learning 
methods to build vulnerability predictors so as to determine the 
effectivene ss of the predictors with or without labeled training 
data. In existing vulnerability  prediction studies, supervised 
learning methods are generally used.  We have no knowledge of 
vulnerability prediction models built using unsupervised 
learning methods . Our  goal is to make vulnerability prediction 
both more practical and  accurate than in previous work . 
We evaluated our proposed approach based on experiments 
with a set of open source PHP -based web applications with 
known XSS and SQLI vulnerabilities. We impl emented a tool 
called PhpMinerI  for extracting relevant data from PHP 
programs.  We trained two types of vulnerability prediction 
models (for predicting SQLI vulnerabilities and XSS 
vulnerabilities ) using the extracted data. In our cross -validation  
experime nts, supervised vulnerability predictors built from hybrid attributes  achieved, on average, over 10 data sets, 90% 
recall and 85% precision on predicting XSS and SQLI 
vulnerabilities. These new predictors  improved the recall  by 
16% and the precision by 2%  compared to predictors built from 
static analysis attributes alone . Our unsupervised predictors  
also achieved, on average, over 5 data sets,  76% recall, 12% 
false alarm rate, and 39% precision.  
This paper is limited to PHP programming language  and 
focuses  on classifying PHP functio ns and operations. SQLI  and 
XSS vulnerabilities are commonly found in many PHP -based 
web applications. However, our approach could easily be 
extended to other  programming language s. 
The outline  of the paper is as follows. Section 2 discusses 
our motivation.  Section 3 defines  the static -dynamic hybrid 
attribute s and presents the two hypotheses that we shall 
formerly investigate in this paper . Section 4 presents our 
vulnerability prediction models and evaluates their accuracy 
based o n the proposed hybrid attributes. Section 5 discusses 
related work . Section 6 concludes our study.   
II. MOTIVATION  
Both SQLI and XSS vulnerabilities arise from improper 
handling of inputs in web application programs. Hence, they 
are application -level vulnerab ilities. In a typical web program, 
user inputs are accessed via forms, URLs, cookies, and XML 
files. Those inputs are often processed and propagated to 
various program points to accomplish the application ’s 
objectives. Some of those inputs may then be stor ed in the 
application ’s persistent data stores, such as databases and 
session objects, for further processing of the application ’s 
required functionalities. The operations carried out in those 
processes often include security -sensitive operations (sinks) 
such as HTML response outputs and database accesses. When 
user inputs referenced in such operations have not been 
sanitized or validated, vuln erabilities arise.  XSS vulnerabilities 
arise when an unrestricted user input is used in a HTML 
response output sta tement.  SQLI vulnerabilities occur when a 
user input is used in a SQL statement without proper checks.  
Hence, to prevent web application vulnerabilities, 
developers often em ploy input validation/ sanitization methods 
along the paths propagating the inputs t o the sinks. These 
methods can be broadly categorized in to escaping, meta -
character matching /removal , string length truncating,  and data 
type checking/ conversion  [19]. The methods typically 
employed are language -provided functions  (e.g., 
htmlentities ), tru sted t hird-party libraries (e.g., enterprise 
security APIs provided by OWASP [1 ]), or custom functions  
developed for specific security or data integrity requirements 
by developer himself or a group of security experts.  
From the analysis of many vulnerabil ity reports in security 
databases such as CVE [6], we derived the following 
observations:  
· First, many SQLI  and XSS vulnerability reports show 
that most of these vulnerabilities arise from the 
misidentification of input s. That is, developers  may 
implement adequate input validation and sanitization 
methods  but yet,  they may fail to recognize all the data that could be manipulated by external users, thereby 
missing some of the inputs  for validation . Therefore, in 
security  analysis , it is important to first id entify all the 
inputs and the sinks  that use them .  
· Second, when an input to be used in security -sensitive 
program statements is considered to be a numeric type, 
it is most effective to use a numeric -type check or 
numeric -type conversion (from string since  inputs are 
originally strings).  
· Third , we observed that individual developers often 
write their own piece of validation and sanitization 
code to protect the specific programs that they are 
responsible for. But as also observed by Jovanovic et 
al. [3] and Xie and Aiken [4], many of such 
customized functions are incorrect often due  to 
insufficient expertise in security. Thus , for most cases, 
the use of language -provided sanitization/validation 
functions, widely -accepted third -party security 
libraries, or sec urity functions developed by a group o f 
security experts is  typically  the most effective defense 
method.  
· Lastly , different defense methods  are generally 
required to prevent  differen t types of vulnerabilities. 
For example, to prevent SQLI vulnerabilities, escaping 
characters t hat have special meaning to SQL parser is 
required whereas escaping characters that have special 
meaning to client script interpreters is needed to 
prevent XSS vulnerabilities.  Thus, care m ust be taken 
to use appropriate methods.  
III. PROPO SED APPROACH  
The above observations lead us to our first hypothesis  (H1): 
Exce pt for those cases that involve  only numeric input s that can 
always be validated through simple validation, in general, it is 
not straightforward for developers to implement defe nse 
against SQLI and XSS vulnerabilities from scratch by only 
relying on basic operations provided by the programming 
language. Hence, developers should use functions pre -
developed by security experts to implement these defenses.  
From H1, we derive the fol lowing attributes to build 
vulnerability predictors.  
A. Hybrid Attributes  
Data d ependence graph : Our unit of measurement is a 
sink. A sink is a node in a control flow graph of a web program 
that may cause SQLI or XSS attacks. Basically, a sink  
represent s a p rogram statement that interacts with a database 
(denoted as SQL sink) or web client (denoted as HTML sink). 
Given a sink k, we compute its  data dependence graph  (DDG k) 
using data flow analysis.  The graph provides reachable 
definitions for the variables use d in the sink, that is, it contains 
the nodes on whi ch the sink is data dependent [8 ]. As such, any 
input validation and sanitization operations implemented for 
the sink k can be found in the nodes in DDG k.  
The first step of our method is to classify the nodes in 
DDG k according to their security -related properties, and then to  
capture these  classifications in a set of attributes on which vulnerability predictors  are to be built.  Basically, our approach 
attempts to answer the following research question: “Given the 
data dependence graph of a sink, from the number of inputs, 
and the numbers and types of input validation and sanitization 
functions found on the nodes in the graph, can we predict the 
sink’s vulnerability? ”. 
To classify nodes  in DDG k, we use a hy brid approach that 
combines static analysis and dynamic analysis techniques. 
From the language built -in functions that have specific security 
purposes (e.g., addslashes ), the language operators (e.g., 
string concatenation operator “.”), or the predefined l anguage 
parameters (e.g., $_GET ) used in a given node  n in DDG k, n is 
classified statically . But it is classified dynamically  if it invokes  
user-defined functions or some built -in functions such as string 
replacement and string matching functions. As a con trol flow 
node n may contain a variety of program operations, there may 
be multiple classifications for n (see example in Section 3.3). 
We shall address the attributes on which the classification 
schemes will rely as hybrid attributes . The attributes are l isted 
in Table 1 and presented next.  
Static analysis -based  classification : Some of the language 
built-in functions and operations can be statically and precisely 
classified from their properties or specific purposes. The 
classification can be carried out b y simply checking the 
properties of the function or operation. Attributes 1 -15 in Table 
1 characterize the functions and operators to be classified 
statically. These attributes are similar to  those proposed in our 
initial work [2, 16]. Hence, we shall only  briefly present them.  
Depending on the nature of sources, we categorize the 
inputs into five types as explained by attributes 1 -7 in Table 1.  
Attributes 8-13 basically involve language built-in functions 
and operators that could be used in input validati on and 
sanitization procedures. Attribute 8 and 9 correspond to  
language -provided SQLI  and XSS sanitization routines (e.g., 
htmlspecialchars ), respectively. Functions that invoke stored 
procedures or prepared statements (e.g., $query->prepare ) are 
also cla ssified as SQLI  sanitization routines. Attribute 10 
involves type casting built -in functions or operations (e.g ., $a 
= (double) $b/$c ) that cast the input string into a numeric 
type data. Attribute 11 corresponds to language -provided 
numeric data type chec k functions (e.g., is_numeric ). Attribute 
12 corresponds to encoding functions. An input variable may  
be properly sanitized using encoding functions  (e.g., <a href= 
‘login.php?name= ’.urlencode(input)> ). Attribute 13 
match es to functions or operations that return predefined 
information or information not extracted from the input string 
(e.g., mysql_num_rows ). We include the attribute Boolean  as a 
type of valida tion and sanitization because a Boolean value  
returned from a (user -defined or built -in) function is definitely 
safe for use in the concerned sink. And such a function can be 
classified statically by checking its  function protocol.  
Clearly, nodes in DDG k may also include ordinary 
operations that ma y or may not serve any security purpose. 
They may simply  propagate  the input . Consequently, we use 
the attribute Propagate to characterize functions and operations 
that are not classified as any of the rest  of types via either static 
analysis or dynamic analysis  (discussed in the following ). Dynamic analysis -based classification : When a node 
invokes a user -defined function or a language built -in string 
replacement/matching function (such as str_replace ), the type 
or purpose of the function cannot be easily inferred from static 
analysis. Because inputs to web app lications are naturally 
strings, string replacement/matching functions are generally 
used to implement input validation and sanitization procedures. 
A good security function generally consists of a set of string 
functions that allow only valid strings or f ilter unsafe strings. A 
filtering action entails character removal or escaping.  
In our earlier work  [2, 16], we simply characterized such 
string functions with attributes  such as Match  (e.g., strcmp ) 
and Regex -replacement  (e.g., preg_replace ). This is too 
general and thus, our earlier work  could not discriminate 
correct and incorrect string functions (e.g., it treats all 
preg_replace  functions as correct or as incorrect).  Hence, to 
improve the accuracy of classification, in this paper, dynamic 
analysis is u sed if a node in DDG k invokes a user -defined 
function or a language built -in string replacement/matching 
function. The dynamic analysis  attributes are defined as 
follows:  
1) Numeric : functions that return only numeric , 
mathemat ic, and/or dash ‘-’ charac ters (e.g., func tions 
that validate  inputs such as  mathematic equations, 
postal code, or credit card number ).    
2) LimitLength : functions that limit the length of an input 
string to a specified number .  
3) URL: functions that filter  directory paths  or URLs 
(e.g., <a href src= ‘www.hack.com/hack.js ’).  
4) EventHandler : functions that filter event handlers such 
as onload . 
5) HTMLTag : functions that filter HTML tags (e.g ., 
strings between < and the first white space or >).  
6) Delimiter : functions that filter delimiters that could  
disrupt the syntax of intended HTML documents or 
SQL queri es (e.g., string -delimiters  such as single 
quote and double quote;  comment -delimiters  such as 
/*, #, //, and  --; and some other special characters 
such as parenthesis, semi -colon , backslash, null b yte, 
and new line) . 
7) AlternateEncode : functions that filter alternate 
character encodings (e.g., char(0x27) ). 
Note that though the attribute Numeric  is similar to static 
analysis attributes 10 and 11  (Table 1), th ose two attributes 
characterize  the nodes th at invoke language -built-in-specific 
numeric type casting operations and numeric type checking 
functions, respectively.  
We believe that the above attributes reflect the types of 
input validation and sanitization methods that are commonly 
used to prevent SQ LI or XSS attacks. Clearly, a user -defined 
function or a string replacement/matching function may 
correspond to more than one attribute. I f a function corresponds 
to attribute s A and B, then, both the values of A and B are to be 
incremented  (say A and B are numeric attributes) . In detail, (1)  
we maintain seven  sets of test inputs  derived from XSS and 
SQLI cheat sheets prov ided by OWASP [1] and RSnake [10 ]. 
These two security specialists provide a comprehensive TABLE I.  STATIC -DYNAMIC  HYBRID ATTRIBUTES  
Attribute ID  Attribute Name  Description  
Static analysis attributes  
1 Client  The number of nodes that  access data from HTTP  request parameters  
2 File The number of nodes that  access data from files  
3 Database  The number of nodes that  access data from database  
4 Text-database  Boolean value ‘TRUE ’ if there is any text -based data accessed from database; ‘FALSE ’ otherwise  
5 Other -database  Boolean value ‘TRUE ’ if there is any data except text -based data accessed from database; ‘FALSE ’ otherwise  
6 Session  The number of nodes that  access data from persistent data objects  
7 Uninit  The number of nodes that  reference un -initialized program variable  
8 SQLI -sanitization  The number of nodes that apply standard sanitization functions for preventing SQLI issues  
9 XSS-sanitizat ion The number of nodes that apply standard sanitization functions for preventing XSS issues  
10 Numeric -casting  The number of nodes that  type cast data into a numeric  type data  
11 Numeric -type-check  The number of nodes that perform numeric data type chec k 
12 Encoding  The number of nodes that encode data into a certain format  
13 Un-taint The number of nodes that return predefined information or information not influenced by external users  
14 Boolean  The number of nodes which invoke functions that  return  Boolean value  
15 Propagate  The number of nodes that propagate the tainted -ness of an input string  
Dynamic analysis attributes  
16 Numeric  The number of nodes which  invoke functions that  return only numeric , mathematic, or dash  characters  
17 LimitLength  The number of nodes that invoke string -length limiting functions  
18 URL  The number of nodes that invoke path -filtering functions  
19 EventHandler  The number of nodes that invoke event handler filtering fun ctions  
20 HTMLTag  The number of nodes that invoke HTML tag filtering functions  
21 Delimiter  The number of nodes that invoke delimiter filtering functions  
22 AlternateEncode  The number of nodes that invoke alternate character encoding filtering funct ions 
Target attribute  
23 Vulnerable?  Indicates a class label —Vulnerable or Not -Vulnerable  
coverage of XSS and SQLI attack vectors that could filter 
many types of input vali dation and sanitization routines. Each 
set of test inputs ( denoted  as test-attr-set) tests for each 
dynamic analysis attribute (e.g., a test input <p>test</p>  tests 
for attribute  HTMLTag  as it could discriminate functions that 
accept or reject  HTML tags); and (2) for a test -attr-set T that 
tests for an attribute A, we execute the concerned function with 
a test input t1 from T and check if the function corresponds to A 
from the returned result. If the function cannot be classified as 
A, we choose a different  test input t2 and repeat the process 
until it is classified as A or all the test inputs from T have been 
used;  (3) step 2 is iterated for all the seven test -attr-sets, each 
set testing for each dynamic analysis attribute . 
Not all function arguments are as sociated with user inputs.  
Some arguments are assigned with literal values in the 
program.  Such literal arguments  can be  easily identified from 
the nodes in DDG k. Test inputs are only assigned to arguments 
that are derived from user inputs and literal argu ments are 
assigned with their own literal values  extracted using data flow 
analysis . More than one value is also possible for a literal 
argument if there are conditional branches. It is logical as the 
same function can be used to sanitize a variable differ ently 
depending on the path along which the variable is propagated. 
For each possible value of a literal argument, we repeat the 
above dynamic classification process. As explained, we expect 
some functions to match multiple classifications.  
Attributes 16-22 in Table 1 represent the classifications 
presented above.  We shall provide more details on the 
classification methods in our example section.  
 Target attribute : The last attribute Vulnerable?  is the 
target attribute which is used to indicate the class l abel to be 
predicted.  B. Classification and Clustering  
Our goal is to build accurate vulnerability  prediction 
models (supervised vulnerability prediction) from the hybrid 
attributes presented above. Since the proposed attributes are 
designed to reflect H1, if H1 is true, we should expect that, 
given a sufficient sample of vulnerability data, classifiers learnt 
from such data be accurate at vulnerability predictions.  
Although classifiers can be effective,  a sufficient number of 
instances with known vulnerabili ty information  is required to 
train a classifier . It is usually  tedious and labor -intensive to tag 
many instances with vulnerability labels. Sometimes, the 
vulnerability information is not even yet known. In such 
situations, supervised training (i.e., wher e training instances 
need to be labeled with vulnerability information) is simply not 
feasible.  
Cluster analysis, on the other hand, is a type of 
unsupervised learning methods in which no class labels are 
required for training with instances. Intrusion de tection studies 
[17, 18] have shown that cluster analysis could identify 
numerous anomalies  (intrusions in their context)  based on the 
two assumptions that (1) normal instances are much more 
frequent than anomalies and (2) anomalies have different 
characte ristics from normal instances. If , in our context, the 
same two assumptions hold, cluster analysis could be used for  
identifying  vulnerable sinks  as well. This leads us our second 
hypothesis  (H2): Vulnerable  sinks  can be distinguished from 
non-vulnerable sinks based on  the hybrid attributes  proposed 
above.   
If H2 is true, we would observe that cluster analysis on the 
unlabeled instances containing the data of hybrid attributes can 
predict vulnerabilities. Hence, when classification -based vulnerability pred iction models are not a feasible option, our 
approach also include making use of clustering for building 
vulnerability prediction models from our hybrid attributes  
when the above assumptions are met .  
C. Example  
In this section, we explain in detail the class ification 
methods and the attribute collection process using the program 
in Fig. 1. S tatement 1 is a class of input  because it accesses an 
HTTP session parameter. It can be statically classified via 
checking the accessed, predefined parameter ( $_SESSION ). 
Statement 2 can be classified as XSS-sanitization  because it 
invokes a standard escaping routine. Again, it can be statically 
classified via checking the invoked function name; that is, we 
predefine the function htmlspecialchars  as a XSS sanitization 
type.  Statement 3 is an Un-taint type.  
Figure 1b shows the data dependence graph of HTML  sink 
6 in Fig.  1a. Node 4 invokes a user -defined function and it is 
clear that it could not be precisely classified by just looking up 
the predefined classifications. We cl assify such nodes  via 
dynamic analysis .  
In node 4 , a customized security function PMA_backquote  is 
invoked with two arguments $trg_db  and $sqlEscape . By a 
data flow analysis, t he literal value ‘``’ for $sqlEscape  is 
extracted from no de 3. From node 1, $trg_db  is identified as  an 
input variable. It is then assigned with a value obt ained from 
test-attr-sets. And t he function is executed multiple times  (each 
time selecting a different value  from test -attr-sets) to determine 
if it can be classified with one or  more dynamic analysis 
attributes (see Dynamic analysis -based  classification ). 
Classifications are  carried out based on the types of input 
values used and the  contents of the resulting outputs. When a 
test input such as `1 or 1=1  is used, the returned resu lt shall be 
``1 or 1=1  and the function would be classified as Delimiter  
as it escapes a string -delimiter ‘`’. Nodes 9 and 10 shall not be  
 
Fig. 1.  (a) Sample vulnerable PHP code  extracted from 
PhpMyadmin \server_synchronize.php (sligh tly modified for illustration 
purpose). The code cleanses an input using standard and customized 
sanitization functions. (b) Data dependence graph of sink statement  6.  classified as the nod es are contained in the user -defined 
function th at has already been classified.  
Based on  the above classifications , the attribute vectors for 
HTML sinks 2 and 6 could be extracted  from their respective 
data dependence graphs as (1, 1, 1, 0, 0, 1, …, Not -Vulnerable) 
and (1, 1, 0, 1, 1, 1, …, Vulnerable), respectively, according to 
attribute vector ( Session , HTML , XSS-sanitization , Un-taint, 
Delimiter , Propagate ,…, Vulnerable? ). As we propose 23  
hybrid attributes, each sink would be represented by a 23 -
dimensio nal attribute vector.  
IV. EVALUATION  
We conducted two types of experiments in order to assess 
the accuracy of the predictors learnt from the proposed hybrid  
attributes in terms of vulnerability prediction. In the first 
experiment (Section 4.3), we evaluated two different types of 
classifiers on the data sets with class labels —Vulnerable or 
Not-Vulnerable. In the second experiment (Section 4.4), we 
removed the class labels and evaluated a clustering algorithm 
on the data sets without class labels.  
Performance M easures : To evaluate the vulnerability 
predictors, we computed recall or probability of detection  (pd), 
probability of false alarm ( pf), and precision ( pr). We can use 
the following contingency table to define these  standard 
measures.  
  Actual  
Vulnerabl e Not-Vulnerable  
Predicted  
 Vulnerable  True positive ( tp) False positive ( fp) 
Not-Vulnerable  False negative ( fn) True negative ( tn) 
Recall ( pd=tp/(tp+fn)) measures how good our prediction 
model is in finding actual vulnerable sinks. Precision  
(pr=tp/(tp+fp)) measures the actual vulnerable sinks that are 
correctly predicted in terms of a percentage of total number of 
sinks predicted as vulnerable. False alarms  (pf=fp/(fp+tn)) is 
generally used to measure the  cost of using the  model .  
Increasing pd by tun ing the prediction model may, in turn , 
cause more false alarms  or reduce precision . Ideally , the model 
should neither miss actual vulnerabilities ( pd~1) nor throw  
false alarms  (pf~0, pr~1). 
A. Data Collection  
For data collection, we modified the tool PhpMiner I, which 
was used in our earlier work [2]. PhpMinerI  is based on an 
open source PHP program analysis tool called Pixy [3]. For 
classification via static analysis, 300 PHP built -in functions and 
30 PHP operators  are predefined in PhpMinerI , which 
computes d ata dependence graph for each sink and collects 
static analysis attributes.   In this work, we modified the tool to 
incorporate dynamic analysis classification. Dynamic analysis 
is used when a node in DDG k invokes user -defined functions or 
language built -in string replacement/matching functions. No 
classification is made for nodes in DDG k that are contained in 
dynamically classified user -defined functions to avoid 
unnecessary or overlapp ing classifications. To identify function 
arguments (i.e., literals or i nputs), static data flow analysi s is 
used. Test inputs are generated from our predefined test suite 
which reflects the dynamic classificatio n scheme proposed in 
Section 3.1 . Function s are  executed using the APIs from a 
 1  $trg_db = $_SESSION['trg_db'];  
2  echo ‘<table><tr><th>Target database: ’ .  
htmlspecialchars($trg_db);  // HTML sink 
. . . 
3  $sqlEscap e = ‘``’; 
4  $query = “UPDATE ” . PMA_backquote($trg_db,  
$sqlEscape) . “ SET …”;   
5  if ($display == true) { 
6     echo “<p>” . $query . “</p> ”; //HTML sink 
7  $rs = mysql_query($query);  // SQL sink 
 
function PMA_backquote($a_name, $replace) {  
8     if (strlen($a_name) && $a_name !== ‘*’) { 
9       return ‘`’ .  
          str_replace( ‘`’, $replace, $a_name) . ‘`’; 
  } else { 
10       return $a_name;  
  } 
   }  
 PHP/Java Bridge Java package (provide d in http://php -java-
bridge.sourceforge.net/pjb/). F unction return results are then 
analyzed to determine the intended validation and sanitization 
scheme.  
Experiments were conducted on six  real-world PHP -based 
web applications  obtained from SourceF orge [5 ]. Table 2 
shows relevant  statistics for these test subjects. The last column 
in Table 2 shows the security advisories , such as CVE [6] , from 
which the test subjects ’ vulnerability information is obtained. 
Some of these test subjects have also been benchma rked for the 
evaluation of some vulnerability detection approaches [3, 4, 28 , 
29]. Table 3 shows the data sets collected by PhpMinerI . As 
shown in Table 3,  we extracted two types of data sets —one 
corresponds to  HTML  sinks and another corresponds to  SQL 
sinks. In total, we collected 10 data sets  (only 4 sets of SQL 
sinks were used as we have not tagged the vulnerability labels 
for SQL sinks in PhpMyAdmin  and Utopia  systems yet) . 
Column 3 in Table 3 shows the number  and percentage  of 
vulnerable  sinks  in each data set  (manually ins pected  and 
tagged by the first author ). On our web site [7], we provide 
implementation details  of PhpMinerI  and the  data sets. 
B. Data Preprocessing  
Normalization : To generalize the results, our vulnerability 
predictors must be able to handle data of arbitrary distributions. 
Excluding the target attribute, we have 22 hybrid attributes. 
Twenty  attributes take on numeric values and two attributes are 
binary . From our preliminary tests, we observed that different 
numeric attributes are defined on  different scales and most of 
the attributes ’ distributions  are highly skewed.  This may cause 
bias toward some attributes (e.g., attributes with large scale 
values), especially in the context of clustering where similarity 
measurement combines multiple attribute scales.  We use a data  
standardizing  technique called min-max normalization  to avoid 
this problem , as described in Witten and Frank  [9]. 
Min-max normalization enables our predictors to work  in a 
standardized data space instead of a raw data space. An 
attribute is normalized when its value is scaled so as to fall 
within a small specified range (we used the range of zero to 
one). As the normalized value is a linear transformation from 
the orig inal data value, the relationships among the original 
data values  are preserved . The min -max normalization is to be 
made for all the instances of every numeric attribute. This shall 
result in a set of values within the range of zero to one. The 
binary attr ibutes do not need to be transformed . 
 Principal component analysis : Principal component  
analysis (PCA) is a useful technique to identify linearly 
uncorrelated dimensions in a large datasets with possibly many 
inter-correlated attributes. Multivariate data  mining and 
statistical techniques used to build classifiers, such as  logistic 
regression , see their performance negatively impacted  in the 
presence of numerous  inter-correlat ed attributes. PCA results in 
a new set of attributes  (principal components) , each of wh ich is 
a linear combination of s ome of the original attributes. The 
number of principal components is usually much smaller.   
In our  experiments, we applied PCA to every data set (after 
min-max normalization) and used a subset of principal  
component s as attributes such that the selected explain at least  TABLE II.  STATISTICS OF THE TEST SUBJECTS  
Test Subject  Description  LOC  Security Advisories  
SchoolMate  
1.5.4  A tool for school  
administration  8145  Vulnerability  
information in  [29] 
FaqForge  
1.3.2  Document creation  
and management  2238  Bugtraq -43897  
Utopia News 
Pro 1.1.4  News management 
system  5737  Bugt raq-15027  
Phorum  
5.2.18  Message board  
software  12324  CVE -2008 -1486  
CVE -2011 -4561  
CuteSITE  
1.2.3  Content manage -
ment framework  11441  CVE -2010 -5024  
CVE -2010-5025  
PhpMyAdmin  
3.4.4  MySQL database 
management  44628  PMASA -2011 -14 –  
PMASA -2011 -20 
TABLE III.  DATA SETS 
Data Set  #HTML  sinks  #Vuln . sinks  
(%Vuln.)  Principal  
components  
schmate -html 172 138 (80%)  7 
faqforge -html 115 53 (46%)  7 
utopia -html  86 17 (20%)  9 
phorum -html  237 9 (4%)  9 
cutesite -html 239 40 (17%)  10 
myadmin -html 305 20 (7%)  9 
Data Set  #SQL  sinks  #Vuln . sinks  
(%Vuln.)  Principal  
components  
schmate -sql 189 152 (80%)  7 
faqforge -sql 42 17 (40%)  3 
phorum -sql 122 5 (4%)  6 
cutesite -sql 63 35 (56%)  7 
95% of the data variance. The last column in Table 3 shows the 
numbers of principal components selected for building 
supervised and unsupervised vulnerability predictors.  
C. Supervised Vulnera bility Prediction  
Based on H1, we evaluated two different classifiers learnt 
from our proposed hybrid attributes.  
Classifiers : Classification is a type of supervised learning 
methods because the class label of each training instance has to 
be provided.  We built Logistic Regression (LR)  and Multi -
Layer Perceptron (MLP)  models for this experiment. These 
classifiers were benchmarked as among the top classifiers in 
recent studies [14]. MLP is a type of neural networks. LR is a 
type of statistical regression models. Details about these 
classification techniques are provided by Witten and Frank [9]. 
We used two very different techniques in an attempt to 
optimize accuracy .  
Training and testing : We used a standard sampling method 
called 10-fold cross validation set up. The data is divided  into 
ten sets. A classifier is trained on nine  sets and then tested on 
the remaining set. This process is repeated ten times; each time 
testing on a different set . The order of training and test set is 
randomized. This test design o vercomes the ordering effects  
due to randomization . This is important to avoid a malignant 
increase in performance by a certain ordering of training and 
test data. Isolating a test set from the  training set also conforms 
to hold -out test design which is im portant to evaluate the 
classifier ’ capability to predict new vulnerabilities [9].  
Result : The results of the two classifiers learnt from hybrid 
attributes are shown in Fig. 2. On average, both models showed  
good performances with high vulnera bility detec tion rates (≥74%) and low false alarm rates ( ≤8%). But on some data sets 
such as phorum -html and phorum -sql, MLP could not 
discriminate vulnerabilities whereas LR is able to. Therefore, 
based on current results we advise to the use of LR to build 
vulnerabi lity prediction models.  
The significantly low false alarm rates achieved by our new 
models indicate that the models ’ precision has improve d from 
our initial work  [2, 16]. Yet, t o provide an exac t comparison 
baseline , we also  built LR models from static ana lysis attributes 
alone and evaluated them in the same way as the above models.  
Results are shown in Fig. 3. On average, our proposed LR  
models built from hybrid attributes achieved ( pd=16%, pf=3%, 
pr=2%) improvements over the LR models built from static  
analysis attributes only. As suggested by Dem šar [20], we also 
used one -tailed Wilcoxon signed -ranks tests to perform  
pairwise comparisons of the measures achieved by the two        
                Measure (%)  
Data &  
Classifier   
Pd 
  
Pf 
  
Pr 
 
schmate -html LR 99 3 98 
MLP  99 0 100 
faqforg e-html LR 89 5 94 
MLP  91 5 94 
utopia -html  LR 94 1 94 
MLP  94 2 89 
phorum -html  LR 78 1 70 
MLP  33 0 100 
cutesite -html LR 68 9 61 
MLP  78 8 67 
myadmin -html 
 LR 85 1 89 
MLP  75 1 83 
Average results on 
XSS prediction  LR 86 3 84 
MLP  78 3 89 
schmate -sql LR 97 8 98 
MLP  96 35 92 
faqforge -sql LR 88 4 94 
MLP  88 4 94 
phorum -sql LR 100 3 63 
MLP  0 1 0 
cutesite -sql LR 91 14 89 
MLP  89 18 86 
Average results on 
SQLIV prediction  LR 94 7 86 
MLP  68 15 68 
Overall ave rage  LR 90 5 85 
MLP  74 8 81 
Fig. 2.  Classification results of XSS and SQLI vulnerability predictors built 
from hybrid attributes.  
                Measure (%)  
Data &  
Classifier   
Pd 
  
Pf 
  
Pr 
 
schmate -html LR 99 9 98 
faqforg e-html LR 91 6 92 
utopia -html  LR 88 3 88 
phorum -html  LR 44 1 67 
cutesite -html LR 35 6 54 
myadmin -html LR 80 1 89 
schmate -sql LR 93 30 93 
faqforge -sql LR 88 4 94 
phorum -sql LR 40 1 67 
cutesite -sql LR 86 18 86 
Overall average  LR 74 8 83 
Fig. 3.  Classification results of XSS and SQLI vulnerab ility predictors built 
from static analysis attributes.  types of LR models over the 10 data sets. The tests show that 
the improvements of recall  and precision were statistically 
significant at a 95% level, though only the increase in recall is 
interesting from a practical standpoint.  
We can conclude that dynamic analysis attributes contribute 
to significantly improving the accuracy of vulnerabilit y 
predictors.  As these attributes are designed to store the 
information about potentially correct and incorrect input 
validation and sanitization procedures implemented in the 
program, the se results support H1. 
D. Unsupervised Vulnerability Prediction  
Regardi ng H2, we evaluated a clustering model  learnt from 
our proposed hybrid attributes.  
Cluster  analysis : Unlike classification methods, cluster 
analysis works in the absence of class labels for training 
instances. But its predictive capability would be expecte d to be 
inherently lower due to the absence of supervision. Like 
Portnoy et al. ’s unsuperv ised intrusion detection study  [17], the 
performance of our cluster analysis here should  depend on the 
following two assumptions : (1) non-vulnerable sinks  are much 
more frequent than vulnerable sinks and ( 2) vulnerable sinks  
have different characteristics from non-vulnerable sinks . If 
these two assumptions are met and H2 is true, vulnerable sinks  
would b e clustered together as outliers  in terms of hybrid 
attribute values, which could then b e detected by cluster 
analysis.  
Because there is no need to label instances, unsupervised 
learning, such as cluster analysis, is expected to be much less 
expensive tha n building  classifiers for vulnerability prediction.  
We evaluated k-means clustering algorithm  applied to  our 
proposed hybrid attributes . k-means is a simple and often 
effective partitioning algorithm. Given an input k, it partitions a 
set of instances in to k clusters in such a way that similarity 
among instances is maximized within the same clusters and 
minimized across  the different clusters.  For similarity 
measurement, standard distance functions can be used. For our 
experiments, we used  the Euclidean d istance function . Further  
details about  the algorithm  are provided in [9].  
Parameter estimation : As clustering only groups instances 
based on their similarities, some parameters must be defined to  
label the clusters as Vulnerable or Not -Vulnerable. The 
problem here is “Given a set of clusters produced by a 
clustering algorithm, what are the best rules (parameters) to 
single out clusters that contain  a large proportion of  
vulnerab le sinks ?”. In Portnoy  et al. ’s clustering -based 
intrusion detection study [17], a parameter N=15% was used as 
the percentage of the largest clusters that would be labeled as 
normal  as it was found to optimize their results . 
For our clustering -based vulnerability prediction stu dy, we 
used a parameter %Normal . It defines the minimum size (in 
terms of percentage of instances) of clusters that would be 
labeled as Not -Vulnerable. For example, if %Normal =10, the 
clusters containing more than 10% of data would be labeled as 
Not-Vulner able.  As required by k-means  algorithm , we also  
needed to determine  a parameter k that indicates the number of 
clusters to be produced  by k-means .  We determined the  two parameters by performing  
experiments that optimize results on  the test subjects used  in 
our initial work [2, 16 ]. The  resulting parameters , k=4 and 
%Normal =12, were then consistently used throughout this 
evaluation.  
Result : More than 40% of sinks in schmate -html, faqforge -
html, schmate -sql, faqforge -sql, and cutesite -sql are v ulnerable 
sinks (see %Vuln. in Table 3). These  data sets clearly  violate  
the first assumption  (stated above)  as they contain many  
vulnerabilities . We expect low  predictive power fro m our 
clustering models for such data sets. C onsequently,  we 
separated the data sets which meet our assumptions  from the 
ones that violate the  assumptions, and performed separate 
evaluations. The results on the former data sets are shown in 
Fig. 4 and the results on the latter sets are shown in Fig. 5 .  
As shown in Fig. 4, the k-mean ’s detection rate is very 
good, especially on utopia -html and phorum -sql data sets. But 
its average precision  is half that of the supervised models 
above. This is directly caused by the inherent weakness of the 
unsupervised learning scheme. It is also affected by different  
trade -offs between detection  rates and false alarm s. The trade -
offs mainly result from the parameter %Normal . With a  high 
value of %Normal  we label more clusters  as Vulnerable  and 
reduce precision . Tuning s uch a parameter must be done in 
context based on available resources for vulnerability 
detections.  
As expected, as shown in Fig. 5, cluster analyses on data 
sets which violate our first assumption result in very low 
detection rates because many or all of the vulnerab le sinks did 
not appear  as outliers (in terms of hybrid attribute values ) to 
our clustering model . Pr was also undefined for some data sets 
as both pd and pf were null.   
From the results in Fig. 4 , we c an conclude that , if certain 
assumptions are met, cluster analysis on  unlabeled instances 
using  hybrid attributes can help accurately  predict 
vulnerabilities , thus  support ing H2.   
 
                Mea sure (%)  
Data   
Pd 
  
Pf 
  
Pr 
 
utopia -html  100 13 65 
phorum -html  56 11 16 
cutesite -html 70 20 41 
myadmin -html 55 8 33 
phorum -sql 100 7 38 
Average  76 12 39 
Fig. 4.  k-means clustering analysis re sults on the data sets which meet the 
assumptions . 
                Measure (%)  
Data   
Pd 
  
Pf 
  
Pr 
 
schmate -html 9 0 100 
faqforg e-html 26 0 100 
schmate -sql 3 32 29 
faqforge -sql 0 0 undef ined 
cutesite -sql 0 0 undefined  
Average  8 6 undefined  
Fig. 5.  k-means clustering analysis re sults on the data sets which violate the 
assum ptions . E. Threats to Validity  
Our data only reflects the known vulnerabilities that are 
reported in vulnerability databases. Hence, our vulnerability 
prediction s based on classifiers do not account for 
undiscovered vulnerabilities.  
The application of clus ter analysis is limited by the two 
assumptions  stated above . In our experiments, clustering -based 
prediction models could accurately isolate vulnerabilities  in the 
data sets which satisfy those assumptions . However, it is 
unclear how frequently these assum ptions hold in practice 
across systems and types of vulnerabilities . Further, we 
estimated two parameters (k and %Normal ) driving the 
accuracy of  cluster analysis based on our experience with 
preliminary experiments. We used the same two parameters for 
all the data sets. The parameters worked well for our context  
but may not generalize well elsewhere. But as  most of our test 
subjects  such as  PhpMyAdmin  are widely -used, real-world 
applications, we believe that the above threats do not 
significantly affect ou r results although tuning the parameters 
may be required for some applications.   
The use of different or more data preprocessing activities 
may also alter our results. For example, during our preliminary 
experiments, we tested the data sets with and withou t PCA (see 
Section 4.2). Results without PCA were significantly inferior to 
results with PCA for the majority of  data sets  though  no 
significant differences were observed for some .  
Different classification and clustering algorithms could 
result in differe nt results.  In our experiments,  we used two very 
different classification algorithms which are statistical -based 
and network -based, respectively. We also tried other classifiers 
like C4.5 and naïve bayes, but the average results were similar. 
We have not  tried another algorithm for clustering -based 
prediction, but we expect similar results if similar parameters 
(i.e., k and %Normal ) are used.  
Like all other empirical studies, our results are limited to 
the applied data mining processes, the test sub jects, and the 
experimental setup used. One good solution to refute, prove, or 
improve our results is to replicate the experiments with new 
test subjects and probably with further data mining strategies . 
This can be easily done  since we have clearly define d our 
methods and setup , and we also  provide the data used in the 
experiments  and the data collection tool on our web site [7].  
V. RELATED WORK 
Our work applies data min ing for the prediction of 
vulnerabilities in web applications. Hence, its related work fal ls 
into three categories: defect prediction, vulnerability prediction, 
and vulnerability detection.  
Defect prediction : Data mining models used by our 
approach are similar to those used in many defect prediction 
studies [12, 13, 14, 15, 25]. In these studie s, defect predictors 
are generally built from static code attributes such as objec t-
oriented design attributes [12 ], LOC counts and code 
complexity attributes [14, 15] because static attributes  can be 
cheaply and consistently collected across many systems [15]. 
However, it was quickly realized that such attributes  can only 
provide limited accuracy  [13, 15, 25]. Arisholm  et al. [ 13] and Nagappan et al. [25 ] reported that process attributes (e.g., 
developer experience and fault history) could significantly 
improve prediction models.  On the other hand, as process 
attributes are difficult to measure and measurements are often  
inconsistent, Menzies et al. [15 ] showed that static code 
attributes can still be effective if predictors are tuned to user -
speci fic goals .  
In contrast to defect prediction studies, our study targets 
security vulnerabilities in web applications.  Since the se studies 
show that there is no universal set of attributes, we define 
specific  attributes targeted at predicting vulnerabilitie s based 
on automated and scalable static and dynamic analysis . 
Vulnerability prediction : Shin et al. [23] used code 
complexity, code churn, and developer activity attributes to 
predict vulnerable program s. They achieved 80 % recall  and 
25% false alarm rate . Their assumption was that,  the more 
complex the code, the higher the chances of vulnerability . But 
from our observations, many of the vulnerabilities arise from 
simple code and, if a program does not employ any input 
validation and sanitization routi nes, it would be simpler but 
nevertheless  contain  many vulnerabilities.  
Walden et al. [24] investigated correlations between the 
security resource indicator (SRI) and the numbers of 
vulnerabilities in PHP web applications. SRI is derived from 
publicly ava ilable security information such as past 
vulnerabilities , secure development guidelines , and security 
implications regarding system configurations.  Neuhaus et al. 
[26] also predicted vulnerabilities in software components from 
the past vulnerability inform ation, and  the import s and function 
calls attributes. Their work is based on the concept that 
components which contain imports and function calls that are 
similar to known vulnerable components are likely to be 
vulnerable as well. They achieved 45 % recall  and 70% 
precision .  
These existing vulnerability prediction approaches  
generally target software components. By contrast, our method 
target s specific program statements for vulnerability prediction.  
The other  difference is that  we use code attributes that 
characterize input validation and sanitization routine s. 
Vulnerability detection : Jovanovic et al. [3] and Xie and 
Aiken [4] showed that many XSS and SQLI vulnerabilities can 
be detected by static program analysis techniques. They  
identify v arious input sources and sensitive sinks, and 
determine whether any input data is used in those sinks without 
passing through sanity checks. In general, such static taint 
tracking approaches are effective but not efficient as t hey 
generate many false alarm s. 
To improve precision, Fu and Li [27]  and Wassermann and 
Su [28] approximated the string values that may appear at 
sensitive sinks by using  symbolic execution and  string analysis 
techniques.  More recent approaches incorporate dynamic 
analysis techniques,  such as concolic execution [ 11, 29] and 
model checking [30]. These approaches reason about various 
paths in the program that lead to sensitive sinks and attempt to 
generate test cases that are likely to be attack vectors. All these 
approaches reduce false  alarm rates. But symbolic, concolic, 
and model checking techniques often lead to a path explosion problem. It is difficult to reason about all the paths in the 
program when the program contains many branches and loops. 
Further, the performance of these ap proaches also depends very 
much on the capabilities of their underlying model checkers or 
string constraint solvers in handling a myriad of string 
operations offered by  programming languages.  
By contrast, although our approach also requires dynamic 
analysi s, this is done at  the function  level. It does not require 
string solving and reasoning of (potentially infinite) program 
paths like concolic execution and model checking techniques.  
However, symbolic, concolic, and model checking 
approaches could possibl y yield high precision rates which 
may never be matched by data mining methods. Thus, our 
objective is not to provide a replacement for such techniques  
but rather  to provide a complementary approach to use when 
they are not applicable or in combination wit h them . One 
could, for example,  first to gather vulnerability predictions on 
code sections using data mining  and then focus on the code 
sections  with predicted vulnerabilities  using any of the more 
precise techniques mentioned above . Thereafter, ideally, t he 
confirmed vulnerabilities should be removed by manual 
auditing or by using automated vulnerability removal 
techniqu es such as the ones proposed in  [21, 22]. 
VI. CONCLUSION  
The goal of this paper is to aid security auditing and testing 
by providing probabili stic alerts about potentially vulnerable 
code statements . We propose attributes , based on hybrid static 
and dynamic code analysis, which  characterize input validation 
and sanitization code patterns for predicting vulnerabilities  
related to SQL injectio n and cross site scripting . Given a 
security -sensitive program statement, we collect the hybrid 
attributes by classifying the nodes from its data dependency 
graph. Static analysis is used to classify nodes that have 
unambiguous security -related purposes. D ynamic analysis is 
used to classify nodes that invoke user-defined or  language 
built-in string replacement /matching functions since 
classification of such nodes by static analysis could be 
imprecise.  
We evaluated if the se hybrid attributes can be used to build 
effective vulnerability predictors , using  both supervised and 
unsupervised learning methods . The latter have, in practice, the 
advantage of not requiring labeled training data  (with known 
vulnerabilities) but may be significantly less accurate . In th e 
experiments on six PHP web applications, we first showed that 
the hybrid attributes can accurately predict vulnerabilities  (90% 
recall and 85% precision on average for logistic regression) . 
We also observed that dynamic analysis helped achieve much 
better accuracy than static analysis alone, thus justifying its 
application . Last but not least, when meeting certain 
assumptions, cluster  analysis showed to be a reasonably 
accurate, unsupervised learning method  when no labeled data is 
available for tra ining (76% recall and 39% precision on 
average). But since it is not nearly as accurate as supervised 
learning, it should be considered as a trade -off between data 
collection cost  and accuracy.   To generalize our current results, we hope that researchers  
will replicate our experiment , possibly using the data and tool 
we posted online . We also intend to conduct more experiments 
with industrial applications. While we believe that the proposed 
approach can be a useful and complementary solution to 
existing v ulnerability detection and removal approaches, 
studies should be carried out first to determine the feasibility 
and usefulness of integrating multiple approaches (i.e., 
prediction+detection+removal ).  
REFERENCES  
[1] OWASP. “The open web application security pr oject, ” 
http://www.owasp.org, accessed January 2012.  
[2] L. K. Shar and H. B. K. Tan, “Mining input sanitization patterns 
for predicting SQL injection and cross site scripting 
vulnerabilities, ” in International Conference on Software 
Engineering, 2012, pp. 129 3-1296.  
[3] N. Jovanovic, C. Kruegel, and E. Kirda, “Pixy: a static analysis 
tool for detecting web application vulnerabilities, ” in IEEE 
Symposium on Security and Privacy, 2006, pp. 258 -263. 
[4] Y. Xie and A. Aiken, “Static detection of security vulnerabilities 
in scripting languages, ” in USENIX Security Symposium, 2006, 
pp. 179 -192. 
[5] SourceF orge. http://www.sourceforge.net, accessed March 2012.  
[6] CVE. http://cve.mitre.org, accessed March 2012.  
[7] PhpMiner. http://sharlwinkhin.com/phpminer.html.  
[8] J. Ferrante, K. J. Otte nstein, and J. D. Warren, “The program 
dependence graph and its use in optimization, ” ACM 
Transactions on Programming  Languages and Systems, vol. 9, 
pp. 319 -349, 1987.  
[9] I. H. Witten and E. Frank, Data Mining, 2nd ed. , Morgan 
Kaufmann, 2005.  
[10] RSnake. http://h a.ckers.org , accessed March 2012.  
[11] K. Jayaraman, D. Harvison, V. Ganesh, and A. Kie żun, “jFuzz: 
a concolic whitebox fuzzer for Java, ” in NASA Formal Methods 
Symposium , 2009, pp. 121 -125.  
[12] L. C. Briand, J. Wüst,  J. W. Daly, and D. V. Porter, “Exploring 
the relationships between design measures and software quality 
in object -oriented syste ms,” Journal of  Systems and Software, 
vol. 51 (3),  pp. 245 -273, 2000.  
[13] E. Arisholm, L. C. Briand , and E. B. Johannessen, “A systematic 
and comprehensive investigation of methods to build and 
evalu ate fault prediction models, ” Journal of Systems and 
Software , vol. 83 (1), pp. 2–17. 2010.  
[14] S. Lessmann, B. Baesens, C. Mues, and S. Pietsch, 
“Benchmarking classification models for software defect 
prediction: a propos ed framework and novel findings, ” IEEE 
Transactions on Software Engineering, vol. 34 (4),  pp. 485 -496, 
2008.   
[15] T. Menzies, Z. Milton, B. Turhan, B. Cukic, Y. Jiang, and A. 
Bener, “Defect prediction from static code features: current 
resul ts, limitations, new approaches, ” Automated Software 
Engineering, vol. 17 (4), pp. 375-407, 2010.   [16] L. K. Shar and H. B. K. Tan, “Predicting common web 
application vulnerabilities from input validation and sanitization 
code patterns, ” in IEEE/ACM International Conference on 
Automated Software Engineering,  2012,  in press.  
[17] L. Portnoy, E. Eskin , and S. Stolfo, “Intrusion dete ction with 
unlabeled data using clustering, ” in ACM CSS Workshop on 
Data Mining Applied to Security, 2001.  
[18] S. Thamaraiselvi, R. Srivathsan, J. Imayavendhan, R. 
Muthuregunathan, and S. Siddharth, “Combining naive -bayesian 
classifier and genetic clustering f or effective anomaly based 
intrusion detection, ” Lecture Notes in Computer Science, vol. 
5908, pp. 455 -462, 2009.  
[19] S. Palmer, Web Application V ulnerabil ities: Detect, Exploit, 
Prevent,  Syngress, 2007.  
[20] J. Dem šar, “Statistical comparisons of clas sifiers over multiple 
data sets, ” Journal of Mac hine Learning Research, vol. 7, pp. 1 -
30, 2006.  
[21] S. Thomas , L. Williams, and T. Xie, “On automated prepared 
statement generation to remove SQL injection v ulnerabilities, ” 
Information and Software Technology, vol. 51 (3), p p. 589 -598, 
2009.  
[22] L. K. Shar and H. B. K. Tan, “Automated removal of cross site 
scripting vuln erabilities in web applications, ” Information and 
Software Technology, vol. 54 (5), pp. 467 -478, 2012.  
[23] Y. Shin, A. Meneely, L. Williams, and J. A. Osborne, 
“Evalu ating complexity, code churn, and developer activity 
metrics as indicators of software vulnerabilities, ” IEEE 
Transactions on Software Engineering, vol. 37 (6), pp. 772 -787, 
2011.  
[24] J. Walden, M. Doyle, G.  A. Welc h, and M. Whelan, “Security of 
open source we b applications, ” in International Symposium on 
Empirical Software Engineering and Measurement, 2009, pp. 
545-553. 
[25] N. Nagappan, T. Ball, and B. Murphy, “Using historical in -
process and product metrics for early estimation of software 
failures, ” in Internati onal Symposium on Software Reliability 
Engineering, 2006, pp. 62 -74. 
[26] S. Neuhaus, T. Zimmerma nn, C. Holler, and A. Zeller, 
“Predicting v ulnerable  software c omponents, ” in ACM 
Conference  on Comp uter and Communications  Security, 2007, 
pp. 529 -540. 
[27] X. Fu and C .-C. Li, “A string constraint solver for detectin g web 
application vulnerability, ” in International Conference on 
Software Engineering and  Knowledge Engineering , 2010, pp. 
535–542. 
[28] G. Wassermann and Z. Su, “Sound and precise analysis of web 
applications fo r injection v ulnerabilities, ” In ACM SIGPLAN 
Conference on Programming Lan guage Design and 
Implementation , 2007, pp. 32 -41. 
[29] A. Kie żun, P.  J. Guo, K. Jayaraman, and M.  D. Ernst, 
“Automatic creation of SQL injection and cross -site scripting 
attacks, ” in International Conference o n Software Engineering , 
2009, pp. 199 -209. 
[30] M. Martin and M. S. Lam, “Automatic generation of XSS and 
SQL injection attacks wi th goal -directed model checking, ” in 
USENIX Security Symposium, 2008, pp.  31-43. 