See discussions, st ats, and author pr ofiles f or this public ation at : https://www .researchgate.ne t/public ation/234125989
Supporting Swift Reaction: Automatically Uncovering Performance Problems by
Systematic Experiments
Conf erence Paper ¬†¬† in¬†¬†Proceedings - Int ernational Conf erence on Softw are Engineering  ¬∑ May 2013
DOI: 10.1109/IC SE.2013.6606601
CITATIONS
55READS
533
3 author s:
Alexander Wert
NovaTec Consulting GmbH
17 PUBLICA TIONS ¬†¬†¬†367 CITATIONS ¬†¬†¬†
SEE PROFILE
Jens Happe
Chrono24
68 PUBLICA TIONS ¬†¬†¬†1,096  CITATIONS ¬†¬†¬†
SEE PROFILE
Lucia Happe
Karlsruhe Instit ute of T echnolog y
90 PUBLICA TIONS ¬†¬†¬†795 CITATIONS ¬†¬†¬†
SEE PROFILE
All c ontent f ollo wing this p age was uplo aded b y Jens Happe  on 17 May 2014.
The user has r equest ed enhanc ement of the do wnlo aded file.Supporting Swift Reaction:
Automatically Uncovering Performance Problems
by Systematic Experiments
Alexander Wert
Karlsruhe Institute of Technology
Am Fasanengarten 5
Karlsruhe, Germany
alexander.wert@kit.eduJens Happe
SAP Research
Vincenz-Priessnitz-Str. 1
Karlsruhe, Germany
jens.happe@sap.comLucia Happe
Karlsruhe Institute of Technology
Am Fasanengarten 5
Karlsruhe, Germany
lucia.kapova@kit.edu
Abstract ‚ÄîPerformance problems pose a signiÔ¨Åcant risk to soft-
ware vendors. If left undetected, they can lead to lost customers,
increased operational costs, and damaged reputation. Despite
all efforts, software engineers cannot fully prevent performance
problems being introduced into an application. Detecting and
resolving such problems as early as possible with minimal effort
is still an open challenge in software performance engineering. In
this paper, we present a novel approach for Performance Problem
Diagnostics (PPD) that systematically searches for well-known
performance problems (also called performance antipatterns)
within an application. PPD automatically isolates the problem‚Äôs
root cause, hence facilitating problem solving. We applied PPD
to a well established transactional web e-Commerce benchmark
(TPC-W) in two deployment scenarios. PPD automatically iden-
tiÔ¨Åed four performance problems in the benchmark implemen-
tation and its deployment environment. By Ô¨Åxing the problems,
we increased the maximum throughput of the benchmark from
1800 requests per second to more than 3500.
Index Terms ‚Äîperformance; problem detection; measurement
I. I NTRODUCTION
The performance of an application is highly visible to
end-users and thus crucial for its success. Response times,
throughput and resource consumption affect conversion rates1,
user satisfaction, and operational costs. However, performance
problems are usually difÔ¨Åcult to detect and even harder to
reproduce. Applying systematic approaches (e.g. Software Per-
formance Engineering, SPE [1]) requires speciÔ¨Åc knowledge
and signiÔ¨Åcant expertise. As a consequence, most performance
and scalability questions are postponed to the end of the
development process (‚ÄúÔ¨Åx-it-later‚Äù approach [1]). Problems
introduced early in the development process are identiÔ¨Åed late
and thus expensive to Ô¨Åx as they can be disproportionally
disruptive to an application‚Äôs implementation and architecture.
As examined by Boehm [2], [3] costs escalate as problems
are discovered in later development phases. An example of
cost escalation has been given by NASA [4]: Fixing a require-
ments phase error during design increases repair costs three
to eight times. During integration, fault removal becomes 21
1Conversion rate is the fraction of visitors of a website who become
customers.to 78 times more expensive. Costs explode by 29 to 1500
times for errors found during operation. Even though these
numbers may be different in other contexts, the tendency can
be expected to be the same. Therefore, it‚Äôs vital to identify
performance problems as early as possible.
However, slow performance (low throughput, high response
times, or high resource consumption) can have various causes
in an application architecture, implementation, or deployment
environment. Without sufÔ¨Åcient expertise, it is hard to identify
the actual cause of a problem. Software engineers need to
know typical performance problems that can occur in their
application. For each problem, they must know where and how
to measure in order to get the necessary data without distorting
measurements. In many cases, the necessary performance
metrics cannot be collected. These will lead to incomplete and
noisy measurement data which in turn make it even harder to
draw the right conclusions.
Existing approaches try to identify performance problems
based on software architectures [5], [6], load tests [7]‚Äì[10] or
runtime data [11], [12]. While analyzing the architecture can
identify potential problems early, it is limited to a very high
level of abstraction. Many causes of performance problems
are not reÔ¨Çected in an early architecture and thus will be
missed in such an analysis. Load tests take into account all
effects of the implementation but are focused on speciÔ¨Åc
scenarios, like identiÔ¨Åcation of resource bottlenecks. They do
not provide a goal-driven search for performance problems
in the application‚Äôs implementation logic. Approaches using
runtime data to detect and diagnose performance problems
can operate with real data but should only be a last resort.
Problems are detected way too late to be solved efÔ¨Åciently.
In this paper, we introduce a novel Performance Problem
Diagnostics (PPD) that automatically identiÔ¨Åes performance
problems in an application and diagnoses their root causes.
Once software engineers speciÔ¨Åed a usage proÔ¨Åle for their
application and setup a test system, PPD can automatically
search for known performance problems. Since PPD encap-
sulates knowledge about typical performance problems (for
example, performance antipatterns [13]‚Äì[18]), only little per-formance engineering expertise is required for its usage. PPD
combines search techniques that narrow down the scope of
the problem based on a decision tree [12] with systematic
experiments. The combination of both allows efÔ¨Åciently un-
covering performance problems and their root causes that are
otherwise hard to tackle. In its current state, PPD is tailored
for the diagnosis of performance problems in Java-based three
tier enterprise applications. For this purpose, PPD requires a
representative usage proÔ¨Åle of the system (i.e., a load driver)
and test system that resembles the actual setup.
To validate PPD, we applied it to an established imple-
mentation of the TPC-W industry benchmark [19], a Java-
based three tier enterprise application. We deployed the bench-
mark in two different test environments. PPD identiÔ¨Åed four
performance problems in the benchmark implementation, the
web server, the database, and the infrastructure. Solving these
problems increased the maximal throughput of the benchmark
from 1800 requests per second to more than 3500.
Overall, we make the following contributions:
1) We introduce a novel approach for performance problem
detection and root cause analysis called Performance
Problem Diagnostics . PPD systematically searches for
known performance problems (cf. [13]‚Äì[18]) in three
tier enterprise applications. Once a problem has been
found, PPD isolates its root causes as far as possible.
2) We structure a large set of known performance prob-
lems [13]‚Äì[18] in a novel Performance Problem Hier-
archy . To guide PPD‚Äôs search, the hierarchy starts from
very general problems (or symptoms). Each further level
reÔ¨Ånes the problems down to root causes. The hierarchy
allows systematically excluding classes of problems and
focusing on the most relevant ones.
3) We deÔ¨Åne detection strategies for twelve performance
problems in the hierarchy. The strategies are based on
goal-oriented experiments tailored to trigger a speciÔ¨Åc
problem. Based on the results, heuristics can decide
if a problem is assumed to be present and reÔ¨Åne the
search. For each performance problem, we investigated
and compared different heuristics for detecting the prob-
lems (see Section III). We chose those heuristics that
minimize false positives and false negatives.
4) We evaluated our approach in two steps. First, we
determined the detection strategies that are most likely to
Ô¨Ånd a performance problem (see Section III). For this
purpose, we evaluated the accuracy of each detection
strategy based on ten reference scenarios . Each scenario
contains different performance problems which have
been injected into a test application.
Second, we evaluated if PPD can detect performance
problems in real enterprise applications (see Sec-
tion IV). PPD successfully identiÔ¨Åed four performance
problems in the TPC-W benchmark, which signiÔ¨Åcantly
limited the maximal throughput.
In the following section, we introduce the main concepts of
our approach.II. A UTOMATIC PERFORMANCE PROBLEM DIAGNOSTICS
The core idea of our Performance Problem Diagnostics
(PPD) is based on the observations that i) particular per-
formance problems share common symptoms and ii) many
performance problems described in the literature [13]‚Äì[18]
are deÔ¨Åned by a particular set of root causes. Based on these
observations, we create a hierarchical structure of performance
problems, their symptoms, and their root causes that simpliÔ¨Åes
the detection and diagnostics signiÔ¨Åcantly (Section II-A). The
hierarchy is based on performance antipatterns known in
the literature [13]‚Äì[18]. To detect performance problems and
diagnose their root cause, we execute a series of systematic
experiments that Ô¨Årst test for symptoms and then search for
more speciÔ¨Åc performance problems and their root cause
(Section II-B). In the following, we introduce the idea of both
concepts. A detailed description follows in Section III.
A. Performance Problem Hierarchy
Figure 1 shows an excerpt of the hierarchical structure
of performance problems. An extended performance problem
hierarchy for a large set of the performance problems known
in literature can be found on our website [20]. The hierarchy
is structured in categories, symptoms, performance problems,
and root causes. The category Occurrences of High Response
Times in Figure 1(a) groups common symptoms for the per-
formance problems High Overhead ,Varying Response Times ,
Unbalanced Processing [13], and Dispensable Computations .
Symptoms represent the starting point for the performance
problem diagnostics. They combine common characteristics
of a set of performance problems. Each symptom is reÔ¨Åned
by more speciÔ¨Åc performance problems that further limit the
set of possible root causes.
 Occurrences of 
 High Response Times
 High Overhead
 Varying Response 
 Times
 Unbalanced 
 Processing
 Dispensable 
 Computations
(a) Symptoms of known
performance problems.
 Varying Response Times
 The Ramp
 Dormant References
 Specific Data Structure
 Sisyphus DB Retrieval
 Specific Methods
 Traffic Jam
 One Lane Bridge
 Synchronization Points
 Database Locks
 Pools
 Bottleneck Resource(b) Performance problems causing Vary-
ing Response Times .
Fig. 1. Excerpt of our performance problem hierarchy.
Figure 1(b) shows the performance problem hierarchy for
Varying Response Times . We identiÔ¨Åed the performance an-
tipatterns The Ramp [13] and TrafÔ¨Åc Jam [17] as potential
causes of Varying Response Times .The Ramp occurs if re-
sponse times of an application increase during operation. Forexample, a request to an online shop takes 10 ms when the
store application has been started. After a couple of hours of
operation the same request takes more than one second. Such
a behaviour can, for example, occur if the application contains
Dormant References [21], i.e., the memory consumption of the
application is growing over time. The root cause is SpeciÔ¨Åc
Data Structure s which are growing during operation or which
are not properly disposed. Another cause of Varying Response
Times is the TrafÔ¨Åc Jam performance antipattern. A TrafÔ¨Åc Jam
occurs if many concurrent threads or processes are waiting
for the same shared resources. These can either be passive
resources (like semaphores or mutexes) or active resources
(like CPU or hard disk). In the Ô¨Årst case, we have a typical
One Lane Bridge [15] whose critical resource needs to be
identiÔ¨Åed. We focus on Synchronization Points (indicated by
semaphores and synchronized methods), Database Locks , and
Pools as potential root causes. In the case of limited physical
resources, the root cause can only be a speciÔ¨Åc Bottleneck
Resource .
Even though the presented hierarchy is not all-
encompassing, it is extensible allowing for integration
of further performance problems, symptoms, and root causes.
In this process, the deÔ¨Ånition of new and accurate heuristics
is the biggest challenge as discussed in Section V.
To detect performance problems and to identify their root
causes, the hierarchy introduced in this section serves as a
decision tree that structures the search. Starting from the
root nodes (representing symptoms of performance problems),
our algorithm looks for more and more speciÔ¨Åc performance
problems and Ô¨Ånally root causes. For example, symptoms
require top-level metrics such as end-to-end response time
or CPU utilization. If a certain symptom has been found,
the algorithm systematically investigates its associated per-
formance problems. For each problem (and root cause), we
repeat the same process. With each step the problem becomes
more speciÔ¨Åc and requires a more Ô¨Åne-grained instrumentation
of the system under test. In the following, we explain the
systematic experiments in more detail.
B. Detection Strategies
PPD provides a set of detection strategies which search
for performance problems in a System Under Test (SUT) , i.e.,
a deployed application, using goal-driven experiments. Each
detection strategy addresses a single performance problem
or root cause. It is deÔ¨Åned by i) a speciÔ¨Åc variation of
workload characteristics (such as the number of users), ii) a
speciÔ¨Åc instrumentation that allows for goal-driven collection
of performance data, and iii) a speciÔ¨Åc analysis strategy to
decide if the problem exists in the system under test.
We use systematic experimentation to observe the effect of
changes in the workload on the performance of the system
under test. Such dependencies can indicate the existence of
performance problems or can conÔ¨Årm a particular root cause.
For example, we observe changes of end-to-end response
times with respect to the number of users. If the variance of
response times increases disproportionately with the numberof users, the Varying Response Times are an indication for a
TrafÔ¨Åc Jam orThe Ramp . On a lower level, we can observe
the waiting time of threads at a synchronization point. If
the waiting times increase signiÔ¨Åcantly with the number of
users, the Synchronization Point is a potential root cause for a
One Lane Bridge . While the Ô¨Årst example decides if potential
performance problems exist in the SUT as a whole, the latter
identiÔ¨Åes the root cause of a performance problem.
All detection strategies are deÔ¨Åned once and can then be
executed against various applications fully automatically. To
achieve this, we deÔ¨Åne detection strategies so that they can be
applied to a class of applications (e.g., Java-based enterprise
applications). Each detection strategy encapsulates heuristics
for the identiÔ¨Åcation of a particular performance problem.
We deÔ¨Åne the required rules for dynamic instrumentation and
workload variation as well as analysis methods (heuristics)
that can identify the performance problem using measurement
data. Section III gives three examples for detection strategies.
C. Evaluation Method
The automation of PPD and its use of heuristics require a
thorough evaluation. We evaluate our approach in two phases.
First, we assess the accuracy of detection strategies for each
performance problem. A detection strategy is accurate if it
can identify a performance problem in various conditions with
a minimal number of false positives and false negatives (cf.
Section III-B). Second, we evaluate the applicability of our
approach to real applications. In particular, we address the
following questions:
Q1 Which detection strategies can accurately identify
performance problems (with the least number of false
positives and false negatives)?
Q2 Can PPD identify performance problems and their
root cause in real applications?
Question Q1 addresses the validity of the heuristics and
assumptions underlying our detection strategies. Each detec-
tion strategy is based on a set of heuristics and requires
assumptions about dependencies between application usage
and performance metrics. Therefore, detection strategies may
be sensitive to speciÔ¨Åc aspects of applications. For example, if
an application contains multiple performance problems, these
problems can interact in such a manner that the detection
strategies fail. In Section III, we inject different performance
problems into a test application. We create ten reference
scenarios with different combinations of performance prob-
lems and evaluate alternative detection strategies against these
scenarios.
Question Q2 addresses PPD‚Äôs detection success for real
applications which can contain various performance problems
and many additional inÔ¨Çuencing factors. PPD cannot address
all of these factors and inÔ¨Çuences. Evaluating PPD in such a
setting helps us to understand its strengths and weaknesses. In
Section IV, we apply PPD to the TPC-W benchmark in two
different deployment environments.III. D ERIVING HEURISTICS
FOR PERFORMANCE PROBLEM DETECTION
Detection strategies are heuristics that need to be carefully
chosen. In this section, we reÔ¨Åne the concept of detection
strategies, introduce a process for evaluating and comparing
different strategies, and give three examples. The description
of the twelve heuristics used in this paper can be found on
our website [20].
For each performance problem, symptom, and root cause,
we need a detection strategy that accurately identiÔ¨Åes the
problem. A detection strategy comprises a workload varia-
tion, observed metrics, and an analysis strategy all of which
contribute to its accuracy.
Workload variation : Independent workload parameters to
be varied from one experiment to the next (e.g., number
of users or data size).
Observed metrics : Performance metrics to be collected
during each experiment deÔ¨Åned by instrumentation rules
(e.g., end-to-end response times or waiting times).
Analysis strategy : Analysis of measurement data to decide
about the presence of a performance problem.
In order to compare the accuracy of different detection
strategies for a certain performance problem, we Ô¨Årst deÔ¨Åne
what we understand by accuracy in this context. A detection
strategy is a heuristic that, based on observed performance
data of a system under test, signals if a speciÔ¨Åc performance
problem is present in that system. Based on [22], we deÔ¨Åne
accuracy as a tuple (1 rfn;1 rfp), whereby rfnis the
probability that a performance problem is falsely neglected
(false negative) and rfpthe probability that a problem is
falsely identiÔ¨Åed (false positive).
In the following, we introduce a set of evaluation scenarios
(Section III-A) to quantify the accuracy of competing detec-
tion strategies and to answer research question Q1. Based
on these scenarios, we can determine the accuracy of each
detection strategy (Section III-B). To explain the process,
we describe and evaluate two detection strategies for The
Ramp (Section III-C and III-D) and one for One Lane Bridge
(Section III-E).
A. Evaluation Scenarios
For the deÔ¨Ånition of reference scenarios, we use a test
application in which we inject performance problems ( Fault
Injection [23]). We use a simple Online Banking system as a
representative three-tier enterprise application. A scenario is
created by injecting performance problems into the applica-
tion.
For instance, we inject The Ramp [13] into the applica-
tion by modifying the SQL queries in a way that complete
database tables are retrieved in order to show only a Ô¨Åxed,
limited number of entries to the user ( Sisyphus Database
Retrieval [14]). Executing the scenario leads to increasing
response times as database tables grow and results in longer
data retrieval requests. Another scenario is created by injecting
aOne Lane Bridge [15]. In this case, we limit concurrency bysynchronizing transactions in a way that only one transaction
can be executed at a time. The synchronization point is a
software bottleneck limiting the application‚Äôs performance.
Injecting different performance problems and root causes
leads to a set ~ s= (s1; : : : ; s n)of different reference scenar-
ioswhich contain either no, one or a mix of performance
problems. The reference scenarios allow evaluating detection
strategies with respect to false positives, false negatives and
interaction effects between performance problems.
B. Accuracy of Detection Strategies
In order to estimate the error probabilities rfnandrfpfor a
detection strategy we apply the strategy on a predeÔ¨Åned set of
reference scenarios (introduced in Section III-A) counting the
number of false positives andfalse negatives in the detection
result. We deÔ¨Åne an expectation vector ~ vp= (vp;1; : : : ; v p;n)
for each performance problem p.~ vpdescribes in which sce-
narios sithe problem pmust be detected and in which not:
vp;i=(
1;scenario sicontains problem p
0;otherwise(1)
Lettp=ftp;1; : : : ; t p;mgbe the set of detection strategies
for performance problem p. Applying a detection technique
tp;kon all scenarios siyields a detection vector ~dp;k=
(dp;k;1; : : : ; d p;k;n)describing in which scenarios the consid-
ered performance problem has been detected by the applied
strategy:
dp;k;i=(
1;strategy tp;kdetected pin scenario si
0;otherwise(2)
The error vector ~ ep;k= (ep;k;1; : : : ; e p;k;n)is the difference
~ ep;k = ~dp;k ~ vpand contains the frequencies of false
positives and false negatives:
ep;k;i=8
><
>: 1; tp;kfalsely neglected pinsi
1 ; tp;kfalsely identiÔ¨Åed pinsi
0 ;otherwise(3)
Counting false positives and false negatives in ~ ep;kand nor-
malizing the sums by the expected number of true positives
and true negatives yields the error rates rfn;p;k for false
negatives and rfp;p;k for false positives, respectively:
rfn;p;k = ( 1)~ vp~ ep;k
~ vp~1rfp;p;k =(~1 ~ vp)~ ep;k
(~1 ~ vp)~1(4)
We use the error rates as an accuracy metric for com-
paring different detection strategies. Considering a perfor-
mance problem p, a detection strategy tp;1with error rates
(rfn;p; 1; rfp;p; 1)is more accurate than a detection strategy tp;2
with error rate (rfn;p; 2; rfp;p; 2), if
(
rfn;p; 1< rfn;p; 2;ifrfn;p; 16=rfn;p; 2
rfp;p; 1< rfp;p; 2;ifrfn;p; 1=rfn;p; 2(5)
As neglecting a performance problem is worse than falsely
identifying one, Equation (5) implies that minimizing the ratefor false negatives has a higher priority than minimizing false
positives.
In the following, we apply the accuracy metric deÔ¨Åned
above for the identiÔ¨Åcation of a good detection strategy for
The Ramp [13]. The Ramp describes the problem of growing
response times over the operation time of an application. We
introduce two detection strategies (called ‚ÄúDirect Growth‚Äù and
‚ÄúTime Windows‚Äù) to discover this tendency. Both strategies
perform differently in our reference scenarios illustrating the
process of deriving good detection strategies.
C. Direct Growth (DG)
The ‚ÄúDirect Growth‚Äù (DG) detection strategy identiÔ¨Åes
growing response times over the measurement time of a system
under test. It compares response times measured in the begin-
ning to response times measured at the end. If the comparison
yields a signiÔ¨Åcant difference, we assume that The Ramp is
present. In the following, we describe the experiment setup, the
analysis of results and the evaluation of this strategy. We also
discuss the weaknesses of this strategy, which ultimately lead
to the introduction of the ‚ÄúTime Windows‚Äù detection strategy.
Experiment Setup: To trigger The Ramp , the load driver
executes the usage proÔ¨Åle deÔ¨Åned by software engineers
against the SUT. The detection strategy requires only one
experiment with predeÔ¨Åned duration D. During the experi-
ment, the load driver submits a Ô¨Åxed workload intensity w
to the SUT, while end-to-end response times are observed.
Additionally, for each measured response time an observation
time stamp is captured. The result of such an experiment is
an ordered series R= (r1; : : : ; r n)of response times with
corresponding time stamps T= (t1; : : : ; t n)where tiis the
time stamp of rifor all 1in.
Analysis: In order to decide if response times increase
over time, the detection strategy divides the response time
series Rinto two subsets R1= (r1; : : : ; r k)andR2=
(rk+1; : : : ; r n). The two subsets span approximately equal
time intervals so that tk t1tn tk+1.
The ‚ÄúDirect Growth‚Äù strategy detects The Ramp , if the
response times R2are signiÔ¨Åcantly larger than R1indicating
growing response times over time. For this purpose, we apply
a t-test on both subsets. Let Ribe the random variable for
subset Ri,E[Ri]its expected value and Rithe mean value
ofRi. The following null hypothesis applies for the t-test:
H0:E[R1] =E[R2]. If the t-test rejects H0andR1<R2,
the response times R2are signiÔ¨Åcantly larger than R1and we
assume The Ramp to be present in the SUT.
Evaluation: We evaluated the DG strategy using three
different workload intensities w1; w10andw50(1, 10 and 50
concurrent users). Table I shows the detection results. The
ten columns in the middle represent ten reference scenarios as
deÔ¨Åned in Section III-A. The Ô¨Årst row contains the expectation
vector ~ vramp .The Ramp occurs in the scenarios 1, 2 and 10.
The following three rows contain the detection vectors for the
DGstrategy for three different workload intensities. The last
two columns contain the error rates for false negatives rfn,
false positives rfprespectively.TABLE I
THERAMP DETECTION RESULTS FOR THE DG STRATEGY .
d:DETECTED ,n:NOT DETECTED ,:WRONG DECISION
Scenariosrfnrfp12345678910
~ vramp ddnnnnnnnd ‚Äî ‚Äî
DGw1 n n nnnnnnnn 3/3 0/7
DGw10 dn nd nnnnnd 1/3 1/7
DGw50 ddnd nd nd nd 0/3 3/7
For the DGstrategy, the error rates are high, independent
of the workload intensity w. With a low workload ( w1),The
Ramp cannot be identiÔ¨Åed as the response times grow too
slow in the scenarios 1, 2 and 10. Using a high workload
(w50) leads to many false positives . Scenarios 6 and 8 contain
software bottlenecks which lead to growing response times, if
the workload is permanently high. In these scenarios, the DG
strategy falsely identiÔ¨Åes The Ramp since the bottlenecks lead
to similar behavior. The detection results are not satisfactory
in any case. Therefore, we investigate an alternative detection
strategy for The Ramp based on time windows.
D. Time Windows (TW)
The ‚ÄúTime Windows‚Äù (TW) detection strategy is based on
the observations that i) high workload intensities push The
Ramp behaviour faster than low workload intensities and ii)
bottleneck effects have to be excluded. The TW strategy
addresses both conÔ¨Çicting requirements as described in the
following.
Experiment Setup: To deal with the conÔ¨Çicting require-
ments, we divide each experiment into two phases: A stim-
ulation phase and an observation phase. During the stimu-
lation phase, the TW strategy pushes a potential The Ramp
antipattern by submitting a high workload to the SUT. In this
phase, no measurements are taken. During the observation
phase, the TW strategy applies a closed workload with only
one user and a short think time. This workload guarantees
that requests are not processed concurrently, allowing us to
exclude synchronization problems. In this phase, we capture
a Ô¨Åxed number of end-to-end response times.
In order to observe the response time progression during
operation, we repeat this experiment increasing the duration
of the stimulation phase. In this way, we get nchronologically
sorted sets Ri(time windows ) each containing a Ô¨Åxed number
of response time measurements. Figure 2 shows the mean
response times of scenario 1 and 3 after different stimulation
phases. Scenario 1 contains The Ramp while scenario 3 does
not.
Analysis: We compare the response times of the SUT for
different stimulation times to detect increases in response time
during operation. For this purpose, we perform pairwise t-tests
on neighbouring time windows.
Two sets RiandRi+1are neighbouring if Ri+1is the time
window with the next longer stimulation time compared to
Ri. For R1; : : : ; R n, we perform n 1t-tests Tiwith null0100200300400
5 15 25 35 45 55 65Response Time [ms]  
Stimulation Time [s]  
Sc. 1: The Ramp Sc. 3: No RampFig. 2. Detecting the Ramp
hypothesis Hi
0:E[Ri] =E[Ri+1]. If t-test Tirejects Hi
0
and Ri<Ri+1,Ri+1is signiÔ¨Åcantly larger than Ri. If this
applies for all t-tests, we consider that response times grow
signiÔ¨Åcantly with the operation time and, thus, assume The
Ramp to be present in the SUT.
Evaluation: We applied the TW detection strategy on
the same ten scenarios as before. For the TW strategy, both
error rates are zero, implying that the TW strategy is more
accurate than the DGstrategy for The Ramp as it overcomes
the disadvantages of the DGstrategy. It triggers a potential The
Ramp behaviour during its stimulation phase. Measurements
are only taken during the observation phase, which avoids
any concurrency or synchronization problems. Based on these
results, we use the ‚ÄúTime Windows‚Äù strategy to detect the
antipattern The Ramp in a system under test. In the following,
we introduce the detection strategy for One Lane Bridge .
E. Detection Strategy for ‚ÄúOne Lane Bridge‚Äù
AOne Lane Bridge (OLB) [15] occurs, if a passive resource
limits the concurrency in an application. Passive resources can
be for instance mutexes, connection pools, or database locks.
In the following, we introduce the detection strategy for the
One Lane Bridge antipattern selected due to its low error rate
with respect to the reference scenarios.
Experiment Setup: Since a One Lane Bridge is a typical
scalability problem, we are interested in the performance
behaviour with respect to an increasing level of concurrency.
To detect this antipattern, we deÔ¨Åne a series of experiments
observing the end-to-end response time while increasing the
number of users for each experiment. The strategy increases
the number of users until i) a resource is fully utilized (i.e.,
its utilisation is larger than 90%), ii) response times increase
more than 10 times, or iii) the maximum number of potential
concurrent users is reached. The experiments yield nsets
of response times R1; : : : ; R nwhere nis the number of
experiments and i+ 1is the experiment with the next higher
number of users compared to experiment i(1i < n ).
In order to distinguish an OLB from a Bottleneck Resource ,
we additionally measure resource utilization during each ex-
periment. Figure 3 illustrates the measurement results for three
different scenarios: i) the SUT contains an OLB, ii) the SUT
contains no problem, and iii) the SUT contains a bottleneck
resource (BR). The graphs on the left hand side show the
average response times with respect to the number of users.
051015202530
1 6 11 16 21 26 31Response Time [s]  
Number of Users  
OLB no problem BR
020406080100
1 6 11 16 21 26 31CPU Utilization [%]  
Number of Users  
OLB no problem BRFig. 3. Detecting the One Lane Bridge
The graphs on the right hand side show the corresponding
mean CPU utilization.
If the SUT contains an OLB , its critical passive resource
leads to strongly increasing response times for an increasing
number of users. Additionally, CPU utilization is low since the
throughput is limited by the passive resource. If the CPU is a
Bottleneck Resource (BR), response times increase and CPU
utilization is high. Thus, we do not assume an OLB to be
present. Finally, if no performance problem occurs, response
times and CPU utilization increase only moderately.
Analysis: The analysis is based on the three cases de-
scribed above. It identiÔ¨Åes the number of users for which re-
sponse times increase signiÔ¨Åcantly and checks the utilisation of
resources for all subsequent experiments. Strongly increasing
response times and low resource utilisation are indicators for
an OLB.
Letnbe the number of experiments and u1; : : : ; u nthe
number of users for each experiment. The number of users
increases for each experiment, so that ui< ui+1for all 1
i < n . To detect if there is an experiment j2f1; : : : ; ngso
that response times increase for all subsequent experiments,
the analysis performs n 1pairwise t-tests Tion neighbouring
sets (RiandRi+1) of response times and compares their mean
values (cf. Section III-D). If a jexists so that all t-tests are
rejected for RjtoRn, we assume that response times increase
signiÔ¨Åcantly for each experiment following j.
If additionally the mean utilization of all resources is low
(smaller than 90%) for all subsequent experiments ( ij),
we assume that the SUT contains an OLB . Since none of the
hardware resources causes the longer response times (due to
their low utilization), the only other resonable explanation is
a software resource that limits the concurrency in the appli-
cation. Otherwise, if the utilization of at least one resource
exceeds the threshold (90%), we cannot conclude that the
SUT contains an OLB . In this case, the analysis identiÔ¨Åes the
resource with a utilisation of more than 90% as a Bottleneck
Resource .
Evaluation: The results demonstrate that the detection
strategy can successfully identify bottleneck resources in all
reference scenarios.
So far, we evaluated individual detection strategies based
on a set of reference scenarios with injected performance
problems. We chose the detection strategies that achieved the
lowest error rates for these scenarios. In such a controlled
setup, our approach works well. However, to understand ifPPD can Ô¨Ånd performance problems outside a controlled
environment (and answer research question Q2), we apply it
to a real enterprise application in the following section.
IV. S EARCHING FOR PERFORMANCE PROBLEMS
IN THE TPC-W B ENCHMARK
We use the TPC-W Benchmark [24] for evaluation of our
approach. TPC-W is an ofÔ¨Åcial benchmark to measure the
performance of web servers and databases. Thus, we expect it
to be tailored for high performance. Finding performance prob-
lems there (if any) is especially challenging (and interesting).
In the following, we explain the system under test (Section
IV-A), describe how PPD helped us to identify and solve four
performance problems (Section IV-B), and explain how PPD
identiÔ¨Åed one of these problems as an example (Section IV-C).
A. System under Test
The TPC-W Benchmark [24] emulates an online bookstore
providing twelve different request types for browsing and
ordering products and two request types for administrative
purposes. The emulated bookstore meets the requirements of
a realistic enterprise application, which is essential for the
evaluation of our approach.
Web Server / Application Server
:Apache Tomcat (6.0.35)
TPC-W
Bookstore
Database ServerMeasurement Control Node
PPD Usage 
Profile
SatelliteMySQL 5.0 
Satellite
<< JDBC >><< HTML /
 HTTP >>
<< RMI >>OS = Windows 7 x64
CPU = 2 x 3 GHz
RAM = 4 GB
OS = SUSE Linux
CPU = 16 x 2.13 GHz
RAM = 16 GBLAN
LANOS = SUSE Linux
CPU = 4 x 2,3 GHz
RAM = 4 GB
LAN
Setup A:  100 Mbit/s Ethernet
Setup B:  1 Gbit/s Ethernet
Fig. 4. Experimental Setup
Figure 4 shows the architecture and the two different setups
for evaluation. Both setups comprise three nodes which are
connected with a 100 MBit/s Ethernet in Setup A and a
1 GBit/s Ethernet in Setup B . We use a Java-Servlet implemen-
tation of TPC-W [19] which is deployed on an Apache Tom-
cat 6 Web Server . The bookstore accesses a MySQL 5.0.95
database on a separate machine through JDBC. A dedicated
Measurement Control Node executes the performance problem
diagnostics. The PPD component encapsulates our algorithm
implementing detection strategies for known performance
problems. Lightweight Satellites collect measurement data
from the SUT‚Äôs nodes. The satellites automatically instrument
the application using Javassist [25]. The instrumentation is
based on rules for pattern-matching and is thus independent of
the actual application. For example, one instrumentation rule
states ‚ÄúMeasure the response time of all calls to JDBC‚Äù. The
PPD component analyses the data as deÔ¨Åned by its detectionstrategies. Software engineers specify the SUT‚Äôs Usage ProÔ¨Åle
which encapsulates typical user behaviour. For this case study,
we use a Ô¨Åxed sequence of TPC-W requests.
B. Results
Altogether, PPD identiÔ¨Åed four performance problems in
the benchmark and its setup. The problems were located in
the benchmark itself, the web server, the database, and the
infrastructure. By resolving all problems, we increased the
throughput of the benchmark form 1800 req/s to more than
3500 req/s. In the following, we describe the results of PPD
as well as the performance problems in more detail.
First, PPD identiÔ¨Åed a One Lane Bridge (OLB) in the
TPC-W application deployed in Setup A . A pool of database
connections limits the concurrency within the application
server. It inhibits the full usage of all available resources. As a
consequence the maximal throughput of the benchmark does
not exceed 1800 req/s (Curve 1 in Figure 5(a)). The maximum
utilisation in this setup is approximately 75% for the CPU of
the web server‚Äôs CPU and 82% for the network connection
(cf. Table II).
TABLE II
1:SETUP A - A PACHE CP - PS 15 2:SETUP A - A PACHE CP - PS 60
3:SETUP B - A PACHE CP - PS 60 4:SETUP B - B ONECP - PS 60
UsersCPU Utilization Network Utilization
1 2 3 4 1 2 3 4
1 6.3 7.1 8.8 10.0 6.8 7.1 1.1 1.3
10 33.6 34.2 62.5 48.58 53.2 59.4 11.2 12.1
20 56.1 51.1 83.3 65.48 76.6 81.2 31.4 31.2
30 65.8 74.3 86.9 74.05 80.4 96.5 35.6 36.1
40 75.5 79.7 89.1 79.82 81.6 98.7 36.2 37.2
50 71.2 81.2 89.7 81.45 82.3 99.4 36.0 39.5
We could solve this problem by increasing the database
connection pool size to 60. However, PPD now identiÔ¨Åes the
bandwidth of the Ethernet connection as a Resource Bottleneck
(99% network utilisation). The additional trafÔ¨Åc leads to
collisions that even result in a declining throughput for high
load (Curve 2 in Figure 5(a)) and long response times (Bar
2 in Figure 5(b)). In order to exclude network performance
050010001500200025003000
1 10 20 30 40 50Throughput [Req/s]  
Number of Users  
1: Setup A - ApacheCP - PS 15
2: Setup A - ApacheCP - PS 60
3: Setup B - ApacheCP - PS 60
4: Setup B -   BoneCP   - PS 60
(a) Throughput
378 868 
267 215 
01002003004005006007008009001000
1 2 3 4Response Time [ms]  
1: Setup A - ApacheCP - PS 15  
2: Setup A - ApacheCP - PS 60 
3: Setup B - ApacheCP - PS 60  
4: Setup B -   BoneCP   - PS 60   (b) Response Times for complete us-
age proÔ¨Åle for 50 users
Fig. 5. Evaluation on TPC-Wproblems, we redeployed the TPC-W benchmark to Setup B
which provides a higher network bandwidth. In Setup B , the
maximal throughput of the benchmark increased to 2200 req/s
(Curve 3 in Figure 5(a)).
Despite the increased performance, TPC-W cannot fully
utilize the web server‚Äôs computational resources (cf. Column 3
in Table II). Running PPD against the benchmark in Setup B ,
yielded the database connection pool as an OLB, analogously
to its Ô¨Årst execution. In this case, an increased pool size
could not solve the problem. The connection pool remained
a software bottleneck. Therefore, we exchanged the default
implementation of the Apache Tomcat connection pool by
an alternative implementation tailored for high throughput
(BoneCP [26]). This step resolved the One Lane Bridge and
increased the maximal throughput to 2700 req/s (Curve 4 in
Figure 5(a)).
Next, PPD identiÔ¨Åed a performance problem in the database.
The CPU utilisation of the web server still does not exceed
80%, but its throughput is limited and response times increase
for a larger number of users. PPD detects that the response
times of database calls grow disproportionately, but no re-
source (including the CPU, network, and disk of the database
server) is utilized to capacity. Therefore, PPD identiÔ¨Åes the
database as a One Lane Bridge , but cannot further pinpoint its
root cause.
Since no physical resource is limiting performance, there
must be software resources in the database (e.g., table locks)
causing the problem. The default storage engine of MySQL
5.0 (MyISAM) only supports table locking as internal locking
strategy. If the engine has to process many transactions in
parallel, this can lead to unnecessary contention. Changing
the storage engine of the MySQL server to InnoDB (which
supports row locking) further increased the maximal through-
put to 3500 req/s and the CPU utilisation of the web server to
100%.
With the last improvement, we were able to solve all
performance problems such that the computing resources of
the web server are fully utilized. While solving the perfor-
mance problems had to be done manually, the identiÔ¨Åcation
and root cause diagnosis is fully automated. Executing our
diagnostics approach with the hierarchy depicted in Figure 6
took about three hours including the execution of experiments
and analyses of data. In the following, we illustrate the
diagnostics process for the Ô¨Årst case, the diagnostics of the
database connection pool as a One Lane Bridge .
C. Performance Problem Diagnostics in Action
Figure 6 shows the results for the decision tree for the initial
setup of the benchmark. PPD identiÔ¨Åes Varying Response
Times in the benchmark implementation. A further investiga-
tion excludes The Ramp and identiÔ¨Åes a potential TrafÔ¨Åc Jam
in the system. Therefore, PPD does not investigate more
speciÔ¨Åc problems of The Ramp such as Dormant References
but focuses on problems causing a TrafÔ¨Åc Jam . By applying
the strategy described in Section III-E, PPD can identify a One
Lane Bridge as the cause for the Varying Response Times . In
 Varying Response Times
 The Ramp
 Traffic Jam
 One Lane Bridge
 Synchronization Points
 Database Locks
 Pools
 Bottleneck Resource
 Temp. High DemandFig. 6. Performance problems identiÔ¨Åed in the initial setup of TPC-W.
the following, we describe how PPD diagnoses the root cause
of the One Lane Bridge in more detail.
For each individual step of the diagnostics, PPD auto-
matically instruments speciÔ¨Åc parts of the application code
and evaluates the resulting measurement data. To pinpoint
the root cause of the One Lane Bridge , PDD captures the
response times of methods that can be linked to a speciÔ¨Åc
root cause, like database calls for Database Locks . In order
to identify such methods, PPD inspects the implemented
interfaces of all Java-Classes found in the Class-Path and
compares their methods to the method signatures searched
for. It uses Javassist [25] to dynamically insert code-snippets
for response time measurements at runtime. For example,
all classes implementing JDBC are instrumented in order
to decide if Database Locks cause the problem. When the
instrumentation is in place, the workload of the application
is varied according to our heuristics. If the response time of
the JDBC-Calls increases disproportionally with the workload
while no physical resource is fully utilized, DB locks are a
potential root cause of the observed performance problem. For
the TPC-W benchmark, PPD excludes Synchronized Methods
and Database Locks as potential root causes in its Ô¨Årst
iteration. Instead, it identiÔ¨Åes the database connection pool
as limiting factor for application performance. The operation
for retrieving database connections from the connection pool
exhibits a similar increase in response times as observed
for the overall response times. Therefore, in this case the
connection pool to the database appears to be the potential
root cause for the One Lane Bridge .
D. Summary
PPD identiÔ¨Åed four performance problems and isolated their
root cause by systematically narrowing down the search space.
In the initial setup, the size of the database connection pool,
its default implementation, the network bandwidth, and the
storage engine of the database limit the maximal throughput
to 1800 req/s. Solving these problems increased TPC-W‚Äôs
maximal throughput to more than 3500 req/s.
Based on these promising results, we can state that our ap-
proach can diagnose performance problems in real applications
and detect their root cause (answering question Q2). However,
the threats to validity of the results and the assumptions and
limitations of our approach require further discussion.V. A SSUMPTIONS & L IMITATIONS
In the following, we discuss the main assumptions and
limitations of our approach: The choice of representative usage
proÔ¨Åles, the nature of performance problems, PPD‚Äôs general
applicability (threats to validity), and the usage of heuristics.
Availability of Usage ProÔ¨Åles: In order to execute exper-
iments, PPD requires a usage proÔ¨Åle generating load on the
SUT. A usage proÔ¨Åle describes the typical behaviour of the
application users. The deÔ¨Ånition of typical usage proÔ¨Åles (or
workloads) is a well-known part of load testing using tools
like LoadRunner [27]. The effort to deÔ¨Åne a usage proÔ¨Åle
can vary depending on the complexity of the application and
the required tests. The quality of the usage proÔ¨Åle can be
critical for the results. While a well-chosen usage proÔ¨Åle can
trigger rare performance problems, a badly-chosen one may
hide even simple problems [8]. In this paper, we assume that
software engineers provide a representative usage proÔ¨Åle for
their application. To provide guidance and simplify the task
for software engineers, PPD can be combined with approaches
for usage proÔ¨Åle optimization (such as [8]).
The Nature of Performance Problems: The performance
problems analysed in this paper can be tracked to a speciÔ¨Åc
part in the source code or a speciÔ¨Åc resource. However, this
may hold for all performance problems. Some are the result
of the sheer size and complexity of an application. They are
distributed over various places in the source code. In such
cases, PPD may be only able to detect the problem, but cannot
isolate the root causes.
Threats to Validity: The detection strategies presented in
Section III have been developed for typical three tier enterprise
applications. They apply heuristics based on observations for
this class of applications. Thus, the results presented here
cannot be generalized without further studies.
Usage of Heuristics: We acknowledge that PPD is based
on heuristics using best effort. Since a detection strategy
can only be falsiÔ¨Åed by a system or scenario in which
it does not correctly identify or diagnose the problem, the
detection strategies are only the best with respect to our current
knowledge. Furthermore, the examples of detection strategies
forThe Ramp illustrate that the deÔ¨Ånition of new heuristics
can require a lot of manual effort and signiÔ¨Åcant expertise in
performance engineering. Each detection strategy needs to be
evaluated in different scenarios to assess its accuracy and use-
fulness. However, the combination of goal-driven experiments,
heuristics, and systematic search signiÔ¨Åcantly eases the process
of performance problem detection and root cause isolation.
VI. R ELATED WORK
The work in the Ô¨Åeld of performance problem detection
can be categorized along design, implementation & test, and
operation within the software lifecycle.
Existing model-based approaches for performance prob-
lem detection employ performance models [28] or annotated
architectural models [5], [6] to identify performance prob-
lems during the design phase. Trubiani et al. [6] predict
the performance based on architectural models which areannotated with performance-relevant characteristics. In order
to identify performance problems, the authors analyse the
resulting predictions with predeÔ¨Åned detection rules for indi-
vidual performance problems. Such model-based approaches
can be applied early during design. However, their granularity
of detection is limited to a very high level of abstraction. In
particular, architectural models do not capture implementation
details which often are the source of performance problems.
Thus, with model-based approaches alone, many performance
problems remain undetected.
Approaches dealing with performance problem detection
during implementation & testing can overcome this disad-
vantage, as they work with implementation artifacts. Most
approaches targeting this phase apply performance testing
techniques in order to Ô¨Ånd proper test cases and input data [8]‚Äì
[10], [29], [30], identify performance regressions [31], [32] or
detect performance bottlenecks [7], [8]. Grechanik et al. [8]
introduce an approach for systematic selection of input data
for performance tests, so that performance problems can be
detected more effectively. Furthermore, the authors describe
an approach for automatically identifying performance bottle-
necks. For this purpose, they analyse methods along different
test cases and their contribution to resource consumption.
Their approach is able to Ô¨Ånd a set of potential bottleneck
methods in an application. However, the work of Grechanik
et al. is limited to one particular performance problem. We
introduce a general concept for identifying different kinds of
performance problems based on a combination of systematic
search and goal-oriented experimentation.
In order to identify performance problems during operation,
existing approaches [11], [12], [33]‚Äì[36] apply monitoring
techniques to gather performance measurement data. Parsons
et al. [11] reconstruct a run-time design model of a component-
based software system by analysing the collected data. The
design model is then examined with respect to performance
antipatterns. As monitoring actions affect the performance of
the software under observation, monitoring should be used
cautiously during operation. Therefore, some approaches [12],
[35], [36] apply dynamic instrumentation for data collection,
which maintains the ability to gather all required data. How-
ever, all approaches applied during operation share the major
drawback that their diagnoses come way to late in order to
solve performance problems efÔ¨Åciently.
Miller et al. [12] introduce the idea of dynamically in-
strumenting the system under test for a goal-oriented search
for performance problems, closely related to our work. They
report on a tool named Paradyn which is designed for perfor-
mance evaluation of parallel and distributed software systems.
Paradyn encapsulates two essential concepts. First, Miller et
al. realize dynamic instrumentation for Ô¨Çexible data collection.
Second, a hierarchical search model narrows down perfor-
mance problems to their actual place and time of occurrence.
In our work, we take up the idea of Miller et al. and extend
it by an advanced experimentation approach which makes the
search for performance problems even more effective.VII. C ONCLUSION
In this paper, we presented Performance Problem Diag-
nostics (PPD) , a novel, automated approach for performance
problem detection and root cause analysis. PPD signiÔ¨Åcantly
simpliÔ¨Åes the performance validation of enterprise applica-
tions. We combined systematic search based on a decision
tree with goal-oriented experimentation. For this purpose, we
structured performance problems known in the literature in a
Performance Problem Hierarchy , which guides the search. For
twelve of these performance problems, we developed detection
strategies based on heuristics. We evaluated each detection
strategy with ten different test scenarios and chose those that
proved to be most accurate.
In addition to the evaluation of individual detection strate-
gies, we applied PPD to a 3rd party implementation of the
well established TPC-W benchmark. We deployed the bench-
mark in two environments. PPD identiÔ¨Åed four performance
problems in the benchmark implementation, the web server,
the database, and the infrastructure. By Ô¨Åxing these problems,
we were able to increase the maximum throughput of the
benchmark from 1800 requests per second to more than 3500.
As such, the performance problems had a signiÔ¨Åcant effect on
the benchmark results.
PPD allows software engineers to automatically search for
performance problems in an application with relatively low
effort. Lowering the burden of performance validation enables
more regular and more sophisticated analyses. Performance
validation can be executed early and on a regular basis, for
example, in combination with continuous integration tests.
Based on the encouraging results presented in this paper,
we plan to integrate our approach with the development
infrastructure at SAP. This will allow us to stepwise improve
and reÔ¨Åne our approach and extend the range of performance
problems that can be identiÔ¨Åed.
ACKNOWLEDGEMENTS
We would like to thank Dennis Westermann, Roozbeh
Farahbod, and Christoph Heger for their fruitful comments
and constructive feedback. This work was partially funded by
the German Research Foundation (DFG) grant RE 1674/6-1.
REFERENCES
[1] C. Smith, ‚ÄúSoftware performance engineering,‚Äù Performance Evaluation
of Computer and Communication Systems , pp. 509‚Äì536, 1993.
[2] B. W. Boehm, Software Engineering Economics , ser. Prentice-Hall
Advances in Computing Science & Technology Series. Englewood
Cliffs, NJ: Prentice-Hall, 1981.
[3] ‚Äî‚Äî, ‚ÄúSoftware engineering economics,‚Äù Software Engineering, IEEE
Transactions on , vol. SE-10, no. 1, pp. 4 ‚Äì21, jan. 1984.
[4] J. Stecklein, ‚ÄúError cost escalation through the project life cycle,‚Äù
http://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/
20100036670 2010039922.pdf, NASA, Tech. Rep., 2004.
[5] V . Cortellessa, A. Di Marco, R. Eramo, A. Pierantonio, and C. Trubiani,
‚ÄúDigging into uml models to remove performance antipatterns,‚Äù in Proc.
ICSE . ACM, 2010, pp. 9‚Äì16.
[6] C. Trubiani and A. Koziolek, ‚ÄúDetection and solution of software
performance antipatterns in palladio architectural models,‚Äù in Proc.
ICPE . ACM, 2011, pp. 19‚Äì30.
[7] Z. Jiang, A. Hassan, G. Hamann, and P. Flora, ‚ÄúAutomated performance
analysis of load tests,‚Äù in Proc. ICSM . IEEE, 2009, pp. 125‚Äì134.[8] M. Grechanik, C. Fu, and Q. Xie, ‚ÄúAutomatically Ô¨Ånding performance
problems with feedback-directed learning software testing,‚Äù in Proc.
ICSE , 2012.
[9] A. Avritzer and E. Weyuker, ‚ÄúThe automatic generation of load test
suites and the assessment of the resulting software,‚Äù Software Engineer-
ing, IEEE Transactions on , vol. 21, no. 9, pp. 705‚Äì716, 1995.
[10] V . Garousi, L. Briand, and Y . Labiche, ‚ÄúTrafÔ¨Åc-aware stress testing of
distributed systems based on uml models,‚Äù in Proc. ICSE . ACM, 2006,
pp. 391‚Äì400.
[11] T. Parsons and J. Murphy, ‚ÄúA framework for automatically detecting and
assessing performance antipatterns in component based systems using
run-time analysis,‚Äù in Proc. WCOP , 2004.
[12] B. Miller, M. Callaghan, J. Cargille, J. Hollingsworth, R. Irvin, K. Kar-
avanic, K. Kunchithapadam, and T. Newhall, ‚ÄúThe paradyn parallel
performance measurement tool,‚Äù Computer , vol. 28, no. 11, pp. 37‚Äì46,
1995.
[13] C. Smith and L. Williams, ‚ÄúNew software performance antipatterns:
More ways to shoot yourself in the foot,‚Äù in CMG-CONFERENCE- ,
vol. 2, 2003, pp. 667‚Äì674.
[14] R. Dugan Jr, E. Glinert, and A. Shokoufandeh, ‚ÄúThe sisyphus database
retrieval software performance antipattern,‚Äù in Proc. WOSP . ACM,
2002, pp. 10‚Äì16.
[15] C. Smith and L. Williams, ‚ÄúSoftware performance antipatterns,‚Äù in Proc.
WOSP . ACM, 2000, pp. 127‚Äì136.
[16] ‚Äî‚Äî, ‚ÄúMore new software performance antipatterns: Even more ways
to shoot yourself in the foot,‚Äù in CMG-CONFERENCE- , 2003, pp. 717‚Äì
725.
[17] ‚Äî‚Äî, ‚ÄúSoftware performance antipatterns; common performance prob-
lems and their solutions,‚Äù in CMG-CONFERENCE- , vol. 2, 2002, pp.
797‚Äì806.
[18] B. Dudney, S. Asbury, J. Krozak, and K. Wittkopf, J2EE antipatterns .
Wiley, 2003.
[19] ‚ÄúObject web homepage: Java implementation of the tpc-w benchmark,‚Äù
last visited: July 2012. http://jmob.ow2.org/tpcw.html.
[20] ‚ÄúLean Performance Engineering,‚Äù http://www.lean-performance-
engineering.org/, last-visited: August 2012.
[21] D. Rayside and L. Mendel, ‚ÄúObject ownership proÔ¨Åling: a technique
for Ô¨Ånding and Ô¨Åxing memory leaks,‚Äù in Proc. ASE . ACM, 2007, pp.
194‚Äì203.
[22] J. Swets, ‚ÄúMeasuring the accuracy of diagnostic systems,‚Äù Science , vol.
240, no. 4857, pp. 1285‚Äì1293, 1988.
[23] M. Hsueh, T. Tsai, and R. Iyer, ‚ÄúFault injection techniques and tools,‚Äù
Computer , vol. 30, no. 4, pp. 75‚Äì82, 1997.
[24] D. Menasc ¬¥e, ‚ÄúTpc-w: A benchmark for e-commerce,‚Äù Internet Comput-
ing, IEEE , vol. 6, no. 3, pp. 83‚Äì87, 2002.
[25] ‚ÄúJavassist web homepage,‚Äù last visited: January 2013.
http://www.csg.is.titech.ac.jp/ chiba/javassist.
[26] ‚ÄúHomepage of BoneCP,‚Äù http://jolbox.com/, last visited: August 2012.
[27] ‚ÄúHP LoadRunner,‚Äù http://www8.hp.com/us/en/software-solutions/softwa
re.html?compURI=1175451#.UP0NO3d71Iw, last visited: Januar 2013.
[28] J. Xu, ‚ÄúRule-based automatic software performance diagnosis and im-
provement,‚Äù in Proc. WOSP . ACM, 2008, pp. 1‚Äì12.
[29] A. Avritzer and B. Larson, ‚ÄúLoad testing software using deterministic
state testing,‚Äù in ACM SIGSOFT Software Engineering Notes , vol. 18,
no. 3. ACM, 1993, pp. 82‚Äì88.
[30] A. Avritzer and E. Weyuker, ‚ÄúGenerating test suites for software load
testing,‚Äù in Proc. ISSTA . ACM, 1994, pp. 44‚Äì57.
[31] L. Bulej, T. Kalibera, and P. Tuma, ‚ÄúRepeated results analysis for
middleware regression benchmarking,‚Äù Performance Evaluation , vol. 60,
no. 1, pp. 345‚Äì358, 2005.
[32] K. Foo, Z. Jiang, B. Adams, A. Hassan, Y . Zou, and P. Flora, ‚ÄúMining
performance regression testing repositories for automated performance
analysis,‚Äù in Proc. QSIC . IEEE, 2010, pp. 32‚Äì41.
[33] D. Yan, G. Xu, and A. Rountev, ‚ÄúUncovering performance problems in
java applications with reference propagation proÔ¨Åling,‚Äù in Proc. ICSE ,
2012.
[34] M. Chen, E. Kiciman, E. Fratkin, A. Fox, and E. Brewer, ‚ÄúPinpoint:
Problem determination in large, dynamic internet services,‚Äù in Proc.
DSN . IEEE, 2002, pp. 595‚Äì604.
[35] J. Ehlers, A. van Hoorn, J. Waller, and W. Hasselbring, ‚ÄúSelf-adaptive
software system monitoring for performance anomaly localization,‚Äù in
Proc. ICAC . ACM, 2011, pp. 197‚Äì200.
[36] B. Miller and A. Mirgorodskiy, ‚ÄúDiagnosing distributed systems with
self-propelled instrumentation,‚Äù IFIP Lecture Notes in Computer Science
(LNCS) , vol. 5346, no. 5346, pp. 82‚Äì103, 2012.
View publication stats