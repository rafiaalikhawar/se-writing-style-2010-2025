Symbolic State Validation through R untime Data 
Yueqi Li  Shing  Chi Cheung  
Department of Computer Science and Engineering   
Hong Kong University of Science and Technology  
Hong Kong  Department of Computer Science and Engineering  
Hong Kong University of Science and Technology  
Hong Kong  
yueqili@cse.ust.hk  
 scc@cse.ust.hk  
ABSTRACT 
Real world programs are typically built on top of many library 
functions. Symbolic analysis  of these programs generally requires 
precise models of these functions’ Application Programming 
Interfaces (APIs), which are mostly unavailable because these 
models are costly to construct. A variant approach of symbolic 
analysis is to over-approximate the return values of those APIs 
that have not been modeled. However, such approximation can 
induce many unreachable symbolic states, which are expensive to 
validate manually. In this paper, we propose a static approach to 
automatically validating the reported anomalous symbolic states. 
The validation makes use of the available runtime data of the un-
modeled APIs collected from previous program executions. We 
show that the symbolic state validation problem can be cast as a 
MAX-SAT problem and solved by existing constraint solvers.  
Our approach is motivated by two observations. We may bind the 
symbolic parameters in un-modeled APIs based on observations 
made in former executions by other programs. The binding 
enables us to use the corresponding observed concrete return 
values of APIs to validate the symbolic states arising from the 
over-approximated return values of the un-modeled APIs. Second, 
some symbolic constraints can be accurately evaluated despite the 
imprecision of the over-approximated symbolic values.  
Our technique found 80 unreported bugs when it was applied to 
10 popular programs with a total of 1.5 million lines of code. All 
of them can be confirmed by test cases. Our technique presents a 
promising way to apply the big data paradigm to software 
engineering. It provides a mechanism to validate the symbolic 
states of a project by leveraging the many concrete input-output 
values of APIs collected from other projects. 
Categories and Subject Descriptors  
F.3.2 [ Semantics of Programming Languages ]: Program 
analysis  
Keywords  
Symbolic analysis; API Modeling; Dynamic analysis; Warning 
validation 
1. INTRODUCTION 
Existing symbolic analysis tools have shown promising results in 
checking program behavior [ 4,17,31,34]. We observe that real 
world programs often make heavy use of external library methods, 
which can be invoked through their provided APIs. For example, 
Linux kernel 2.6.38 invokes 341 system library methods and glibc-2.8 invokes 1,234 library methods [
32]. To apply symbolic 
analysis to reason about these programs, the behavior associated 
with the library methods invoked needs to be considered [
32
]. The 
state-of -the-art symbolic analysis engines like KLEE [
4
] require 
explicit modeling of each library method’s application 
programming interface (API). It is non-trivial and rarely adopted 
by developers in practice due to the multitude of library methods 
involved. For example, JDK 1.7 has over 5K native library 
methods with more than 500K SLOCs native code. Since 
modeling such scale of APIs is expensive, existing tools provide 
only the models of a small set of library APIs. Besides, the source 
code of many libraries is often not available [
32
]. Even for 
symbolic execution engines that can handle binaries, such as S2E 
[
6
], the implementations of library calls are difficult to handle as 
they are often highly optimized, extensively using caching, 
hashing, and bit level operations [
32
]. These difficulties hinder the 
wide adoption of symbolic analysis [
27,32]. 
A variant approach is not to model all library APIs but to over-
approximate the return values of those un-modeled APIs [ 5 , 9 , 28]. 
Recent study [
28
] shows that symbolic analysis can scale up to 
large programs like Hadoop by exploiting over-approximated 
variables returned by un-modeled APIs. However, this approach 
requires developers to validate the feasibility of reported warnings 
because over-approximation of return values often introduces 
infeasible behaviors in the analysis and hence generates false 
warnings [
27,32]. The amount of false warnings can be numerous 
as reported by a recent study [
3
]. Unfortunately, the false 
warnings caused by over-approximated return values can hardly 
be ruled out by constraint solvers because these warnings rarely 
give rise to constraint contradictions [
3
,
8
,
32
]. Take the code 
snippet below for instance. 
  if         (    )   
      error statement 
Symbolic analysis of the snippet requires solving the constraint 
        (    )    , which is satisfiable if the return value of 
the un-modeled API         (  ) is over-approximated to be any 
value. This reported warning requires users to validate if 
        (    )  can return a positive value greater than three in 
the program concerned. Real world programs typically comprise 
many different API calls. For example, a single execution path 
analyzed by symbolic analysis can make a few thousand different 
external API invocations in our subjects, like Hadoop or Tomcat. 
The accumulated effects arising from the over-approximation of 
these APIs generate numerous false warnings. Validating these 
warnings is expensive and a waste of effort [
3
,
8
].  
Our work is motivated by two questions. First, can the validation 
of warnings be performed mechanically by leveraging the runtime 
data of un-modeled APIs collected from a subject program’s 
previous test runs? Second , can the valida tion leverage the 
runtime data collected from other programs ’ test runs that involve 
the APIs used by a subject program? The second question is Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that 
copies bear this notice and the full citation on the first page. To copy 
otherwise, or republish, to post on servers or to redistribute to lists, 
requires prior specific permiss ion and/or a fee.  
Copyright 2014 ACM 978 -1-4503 -3013 -8/14/09…$15.00.  
http://dx.doi.org/10.1145/2642937.2642973  
187ASE’14, Septem ber 15–19,  2014, Vasteras, Sweden. 
particularly applicable to the APIs in JDK libraries which are 
commonly used by many Java programs. As such, the feasibility 
of a constraint like          (     )    due to the program in 
Figure 1  can be validated if we can find a test run of another 
program that has invoked          (  )  with a return value 
larger than 3. This feature greatly improves the applicability of 
symbolic analysis tools. For instance, it enables developers to 
perform bug detection on a partially completed program, which is 
not runnable, by using the runtime data of other programs. 
In this paper, we propose an effective approach to answering the 
two preceding questions . Our solution meets three requirements 
in order for it to be practically used on industrial scale programs. 
First, our approach is scalable that it can complement the scalable 
symbolic analysis approach to validate mechanically reported 
warnings. Second, our approach does not require extra modeling 
effort of APIs. Third, our approach is static in the sense that it 
does not require new test runs. It is particularly useful to the 
analysis of complex systems (e.g., Hadoop) whose runtime 
environments may not be easily reconfigured for running 
additional tests to validate warnings. 
We build a database for Java programs with about 200  GB 
runtime data collected from published test runs of open source 
projects. Experiments on large real world programs show that our 
approach is able to distinguish true warnings from many false 
warnings. To our best knowledge, our tool is the first tool that is 
able to perform this kind of symbolic analysis on large subjects 
like Tomcat and Hadoop. We found more than 80 unknown real 
bugs based on our validated warnings. All of these  bugs are 
confirmed by test cases. 
Validating a reported warning is essentially checking the 
feasibility of its symbolic state. Imprecision of the symbolic 
analysis is introduced by over-approximating the return values of 
un-modeled API invocations . The imprecision leads to 
uncertainties on whether such symbolic states are feasible. Our 
approach addresses the validation problem based on two insights. 
First, we observe that the actual parameters of an un-modeled 
API invocation are often evaluated to symbolic values during 
symbolic analysis . These symbolic values can be bound to some 
concrete values so that the API invocation can be validated by 
following an earlier observed execution of the API. For example, 
an API invocation        ( ) receives a symbolic value   as its 
actual parameter, where   may be evaluated to a symbolic 
expression like      . In order to leverage the information 
collected from dynamic analysis to validate  this invocation, we 
can bind symbolic actual parameter   to a concrete value, say 12 , 
observed in an earlier concrete execution of apiCall (12). This 
leads to an additional constraint           The API 
invocation can now be validated if the conjunction of this 
additional constraint and the existing path condition is satisfiable. 
Second, although over-approximated API return values can 
induce imprecision to some symbolic variables, the satisfiability 
of the constraints arising from the se variables can sometimes be 
accurately determined despite such imprecision. For example, let 
us consider a constraint        (  ), where   denotes an over-
approximated variable derived from the return of an un-modeled 
API, and      (  ) denotes a program input that is not confined to 
a particular concrete value . The constraint is satisfiable regardless 
of  ’s imprecise value because there  always exists an input value 
that differs from x’s bound value. This insight enables us to attest 
the feasibility of some constraints in symbolic states even if we 
have inadequate runtime data to validate these constraints. 
Section 3.2 shows that symbolic state validation problem can be reduced to MAX-SAT or SAT problem that can be solved by 
existing efficient constraint solvers. 
Our previous work [28 ] improves the scalability of symbolic 
analysis using a special type of symbolic state equivalence 
induced by API approximation. Like other similar techniques 
[5,9], our previous work can result in false positive warnings. We 
make three contributions in this paper. First, we develop a static 
and scalable approach to validating the warnings given by those 
symbolic analysis techniques [5 ,9,28] that do not precisely model 
all APIs. Second, we implemented our technique as a tool. Third, 
we applied our technique to 10 large real world subjects with 
totally 1.5 million lines of code, and found over 80 unreported 
bugs. 
2. ILLUSTRATIVE EXAMPLE 
We use a simple example in Figure 1  to illustrate the intuition of 
our approach. Suppose there is a previous test run of the example 
program with concrete bindings of  ,  ,  , and the return value of 
         (  ) as 8, 9, 134, and 100, respectively . The test run 
does not reach the error statement in Line 8 . Symbolic analysis of 
the program based on over-approximation of the two un-modeled 
APIs ’ return values  finds an execution path with lines 2, 3, 4, 5, 6, 
7, and 8 reaching the error statement, and reports a warning. This 
execution path generates a path condition      ⋀     
     , where    and    denote the return values of 
         (  ) and          (  ), respectively. Since the path 
condition does not have any contradiction, constraint solvers 
cannot disprove the warning. Neither can constraint solvers prove 
the feasibility of the path condition due to the over-approximated 
return values from the two un-modeled APIs. Our goal is to 
validate whether the error statement in Line 8 is truly reachable. 
In our approach, we distinguish the symbolic variables introduced 
by inputs from those introduced to denote over-approximated 
return values of un-modeled APIs. We can manipulate input 
symbolic variables without introducing imprecision in the sense 
that the se can be freely bound to arbitrary values . As a result , the 
warning validation problem is reduced to the checking of whether 
the two constraints      and       are feasible in some 
dynamic execution.  
The checking can be performed as follows. First,      can be 
validated by leveraging the previous dynamic test run. We can 
bind   to 8 so that the parameter of          (  ) is identical to 
the observed concrete actual parameter value 12 in the test run. 
1
2
3
4
5
6
7
8
9
10
11
12main ( ){
intx=input1 ( );
inty=input2 ( );
intz=input3 ( );
if( external1 (x+4) > 3){   // external1 ( ) is an un -modeled API
if( x > y) 
if( external2 ( y+x) == z){ //external2 ( ) is an un -modeled API
errorStatement ;
}
}
}
}
Runtime data: There was a test run with bindings of x, y, z, and 
the return value of external1 ( ) as 8, 9, 134, and 100.
The test run covers lines 2, 3, 4, 5, 12 
Figure 1. Illustrative example  
188As such, we know that    can bind to 100 and      is feasible. 
This corresponds to our first insight. There is no uncertainty 
in     because we can manipulate       (  ) to assume a value 
so that     is satisfiable. Now, we encounter difficulties in 
validating       because we do not have sufficient runtime 
data about          (  ). However, the satisfiability of       
is independent of   ’s binding because we can manipulate 
input3 ( ) to assume a value so that       always holds 
regardless of   ’s value. This corresponds to our second insight, 
which asserts the satisfiability of some constraints even without 
sufficient runtime data of the un-modeled APIs. As a result, we 
conclude that the warning reported by symbolic analysis is true. 
In this example, we may derive the path feasibility from the test 
runs of other programs that share the same APIs. For instance, we 
can validate      is feasible if we find that          ( )  can 
return 987 in an execution of another program with an actual 
parameter of 9. As a result, our solution can be static when we 
have pre-built a runtime data database of the un-model ed APIs. 
Our approach differs from concolic testing. Assume that we apply 
concolic testing technique on this example. Without the source 
code of the un-modeled APIs, concolic testing tools need to 
randomly try out some value of   so that          (   )   
can be satisfied. In practice, concolic testing tools often fail to 
find appropriate values after large number of executions, making 
it difficult for such tools to achieve high coverage on large 
programs [ 39]. For example, Xiao et al. [ 39] reported that it is 
hard for concolic testing tools to find a valid path string so that a 
system API like Path .GetFullPath ( ) can proceed withou t 
throwing exception. Higher-order test generation approach [ 16] is 
a complementary approach to concolic testing. It requires validity 
proof showing that          (    )     is satisfiable for 
arbitrary specifications of          ( ) . This approach has not 
yet been implemented due to the lack of efficient constraint 
solvers for the required logic.  
3. APPROACH 
We describe our approach in two subsections. We first describe 
our refined symbolic analysis rules that record the contexts of un-
model ed API’s invocation s.  Then, we introduce how we model 
the state validation problem as a MAX-SAT problem, which can 
be efficiently solved by existing constraint solvers [ 23]. 
3.1 Simple Language and Semantics 
To facilitate our discussion, w e describe symbolic analysis rules 
for a simple imperative language in Figure 2 . Our implementation 
supports the semantics of full-fledged languages, like Java. A 
program comprises a sequence of statements with expressions 
involving Boolean and Integer types. The simple language 
supports conditional, sequential, and assignment statements. 
There are three types of assignment statements, which are    
 ,       (  ), and          (  ). The first assignment 
statement     updates variable x with value of the expression e. 
The only special elements in this language are      (  ) and 
       (  ). The element      (  ) denotes reading an input. The element        (  ) denotes a returned value by an un-
modeled API invocation. We explicitly distinguish un-modeled 
API invocation s, i.e.,          ( ) , and inputs, i.e.     
     (  ), in this simple language. For example, a statement 
reading an input from a file can be modeled by        (  ) 
while a statement invoking an un-modeled math library to get 
resulting data can be modeled by          ( ) . We 
distinguish them in order to address the imprecision issue caused 
by un-modeled API invocations, i.e.,          ( ) . The idea 
is that we can arbitrarily manipulate a set of input values 
(returned from      (  )) without introducing imprecision while 
arbitrarily manipulating return values from        ( ) would 
introduce imprecision because of the ignorance of the internal 
logic of        ( ) In the simple language, each un-modeled 
API invocation accepts only one input and one return value. In 
Section 4, we show how to extend our simple language capture 
the effects of un-modeled APIs involving multiple inputs, heap 
modifications, and internal states that are invisible to symbolic 
execution engines. This language does not have function 
declarations because it is not relevant to our discussion. Our 
implementation [ 28] supports inter-procedural analysis. 
We define the semantic rules of symbolic analysis on our simple 
language in Figure 3 . Except for Rule S-UNKNOWN, these rules 
are standard. They describe the semantic s of existing symbolic 
execution engines like KLEE  [4]. There are two types of rules. 
Expression rules evaluate an expression to a symbolic value. 
Statement rules evaluate a symbolic state 〈       ⃗〉 to a set of 
symbolic states describing its possible continuations. Each 
symbolic state is a tuple 〈       ⃗〉, where   is the statement that 
represents the remaining code to be executed ,   is a state 
constraint denoting the current path conditions ,   is a symbolic 
store that maps variables to symbolic expressions, and  ⃗ is a 
vector of tuples introduced in this work to record potential 
imprecision in process of symbolic analysis. For example, the 
symbolic state is 〈                        ( )       
Program p::=  s*
Constant        c    ::=  … |-1 |0 | 1 |… |true |false 
Expression   e::=  x| c | e1ope2
Statement     s::=  x=e |x=input () | x=unknown (e) |  
s1;s2| if(e) then s1else s2
Operators op::=  |  | … |  |  |  |  |  |  | …  
Figure 2. A simple language  
      ⋀      ( ),the operator turns astore toalogical
conjunction .EXPRESSION RULE        
   (E-CONST)   (E-V AR)
               
               (E-OP)
STA TEMENT  RULES          ⃗  (       ⃗)
    
           ⃗      ,    -  ⃗(S-ASSIGN)
                           
                                        
                            ⃗ *           ⃗+(S-IF-T)
                           
                                      
                            ⃗ 
           ⃗            ⃗ (S-IF-BOTH)
                           
         ( )         
          ⃗    )(S-UNKNOWN)
                           
       ()        ⃗      ,   -  ⃗(S-INPUT)
 
Figure 3.  Semantic rules  
189         ( )            ( )         ( )   
      ( )  ⃗ 〉 after executing Line 7 of Figure 1  and taking the 
true branch. We ignore the details of  ⃗ and we will describe it in 
the next paragraph. The assignment evaluation is standard. A 
flattening operator   is used to flatten the mapping in the 
symbolic store to the corresponding constraints for feasibility 
testing. Rule (S- IF-T) states that if the path feasibility test 
determines that only the true branch is taken, the continuation is 
the statement in the true branch with the path condition updated. 
Figure 3  omits the rule for the case when only the false branch is 
taken. Rule (S-IF -BOTH) evaluates a conditional statement to 
two continuations when both branches are feasible. Both of them 
will be added to the set that tracks the states to explore and 
evaluated independently later. Rule (S-INPUT) specifies that 
when an input statement is encountered, a fresh symbolic variable 
is introduced to represent the input value.  
The semantic rules describe standard symbolic analysis with 
additional provision to record the potential imprecision caused by 
un-modeled API invocations using the vector  ⃗. We call each 
component   in  ⃗ a tuple of unknown . Each tuple of unknown 
  ⟨   ⟩ captures the potential imprecision induced by an un-
modeled API invocation. It consists of (1 ) the input parameter   
of the un-modeled API invocation, and (2) the symbolic value   
denoting the over-approximated return value of the un-modeled 
API invocation. We call u an unknown variable , which over-
approximates the return values of an un-modeled API invocation . 
The two components in ⟨   ⟩ of an un-modeled API invocation 
record its symbolic input parameter, and its return value, 
respectively. Rule (S-UNKNOWN) specifies that when an un-
modeled function is encountered, we introduce a fresh symbolic 
variable   to represent its return value and append to  ⃗ a new 
tuple of unknown   to record the potential imprecision and its 
context . For example, the tuple of unknown   derived after 
invoking the un-modeled API          (  ) in Line 5 of Figure 1  
is         , where     is the symbolic parameter of the 
invocation, and    is a fresh symbolic variable introduced to 
over-approximate the return value of external API invocation.  
Due to the approximation of some APIs’ return values , the 
symbolic analysis can introduce unrea chable execution states if 
a program contains unknown variables, and hence generate false 
warnings. Our goal is to validate whether a symbolic state with 
unknown variables is feasible in some dynamic execution. 
3.2 Feasible Symbolic States 
In this section, we first introduce a few notations and definitions 
related to feasible symbolic states. Then, we show how to reduce 
the state validation problem to a MAX-SAT problem, which can 
be efficiently solved by existing constraint solvers. 
In order to perform symbolic state validation, we need to validate 
that the path conditions involving over-approximated symbolic 
values are feasible. We introduce a function   that maps a 
conc rete parameter    of an un-modeled API to the 
corresponding concrete return value   , i.e.,     (  ). This 
function denotes the computation of un-modeled APIs. We use   
to denote a set defined by *〈     〉     (  )+, which is the set 
of feasible parameter/return bindings for a given function. Since 
one execution may involve multiple invocations of different APIs, 
we use     to denote the set of feasible parameter/return bindings 
for the API that is invoked by the ith function call in an execution 
trace under analysis. We define validate  as follows. Definition  1: A vector of sets of binary tuples,   ⃗⃗⃗         , 
validates a vector 〈     〉⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗  〈       〉    〈       〉 if 
〈       〉      〈       〉   . We denote it as 
        ( ⃗⃗⃗ 〈     〉⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗).. 
Our approach binds the symbolic parameters and return values of 
un-modeled API calls to some concrete values that have been 
observed in some dynamic execution . We need to address a 
challenge. Some of these bindings can introduce contradictions in 
the existing path conditions of a given symbolic state. For 
example, the binding of          (  )’s parameter to 12 can 
cause a contradiction in the code snippet below because     
   implies    , which contradicts the path condition    . 
  int x=input1 ( ); 
  if ( x < 0 && external1 (x+4) > 3)  
    … 
Definition 2 defines feasible symbolic states by binding some 
variables to observed concrete values while avoiding causing 
contradictions in the symbolic states concerned. 
Definition  2: A symbolic state   〈       ⃗〉 is feasible if 
  〈     〉⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗         ( ⃗⃗ 〈     〉⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗) (     ) (         
      ) (               ) where 〈     〉⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗ is a vector 
of concrete binding of parameter-return pairs of un-modeled 
APIs in the invocation order ,  ⃗⃗ is the corresponding vector of the 
sets of feasible parameter/return bindings,    is the ith unknown 
variable recorded by  ⃗, and    is the ith symbolic parameter 
recorded by  ⃗. 
Intuitively, a symbolic state is feasible if there exists a vector of 
concrete parameter-return pairs  〈     〉⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗ such that parameter-
return pairs are confined to some real execution of un-modeled 
APIs, i.e.,         ( ⃗⃗⃗ 〈     〉⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗) , and the symbolic state in the 
conjunction form with unknown variables concretized is 
satisfiable. Let us illustrate the idea using an example of the 
resulting symbolic state after executing Line 5 and taking the 
true branch in Figure 1 ,   
   〈   (   )               ( )         ( ) 
        (  ) ⟨      ⟩〉. 
We can check if this state is feasible using the runtime data in 
Figure 1 . (1) We can bind     to 12 and validate  〈      〉⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗ 
using the runtime data collected from an earlier test run. (2) The 
constraint         ( )         ( )         ( )   
                 is satisfiable.  Thus, we can 
conclude that state    is feasible.  
In fact, this validation problem can be reduced to a MAX-SAT 
problem as follows. We use          . ⃗⃗⃗ 〈     〉⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗ /  to denote the 
number of validated parameter-return pairs. 
Maximize:          . ⃗⃗⃗ 〈     〉⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗ /   
subject to 
((     ) (               ) 
 (               ))       
Intuitively, the MAX-SAT problem tries to find concrete 
bindings for 〈     〉⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗ so that the tuples of unknown can be 
validated while the bindings do not introduce contradictions .  
Definition 2 requires each un-modeled API invocation to be 
validated. However, this is not always necessary as shown in the 
190motivating example. Recall that the constraint          (  
 )    generated by the example code in Figure 1  can be 
validated without the runtime data about          ( ) . 
Definition 4 is thus given to relax the requirement of Definition 3 
by excluding those parameter/return pairs that do not need to be 
validated.  
Definition  3: A symbolic state   〈       ⃗〉 is feasible if 
  〈     〉⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗  〈     〉 ⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗         ( ⃗⃗ 〈     〉⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗ ) (     ) 
(               ) (               )  
where〈     〉 ⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗ and 〈     〉 ⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗ are two disjoint vectors partitioned 
from 〈     〉⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗ in Definition 2 ,  ⃗⃗ is a vector of the sets of feasible 
parameter/return bindings ,    is the ith unknown variable 
recorded by  ⃗, and    is the ith symbolic parameter recorded by  ⃗. 
Definition 3 differs from Definition 2 in that we partition 〈     〉⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗ 
into 〈     〉⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗  and 〈     〉 ⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗, and we only need to validate  〈     〉 ⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗ 
while 〈     〉⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗  can bind to any value. We use the universal 
quantifier   to denote that 〈     〉⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗  can bind to any value. 
However, this definition causes some difficulties in solving the 
MAX-SAT problem because (1) what belongs to partitions 
〈     〉⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗  and 〈     〉 ⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗  is unknown; and (2) constraint solvers 
cannot efficiently solve constraints with both universal and 
existential quantifiers in arbitrary domains like integer domain . 
Our approach, therefore, eliminates the universal quantifiers 
before passing the MAX-SAT problem with quantifier eliminated 
to constraint solvers . Our existing implementation uses a simple 
approach to eliminating universal quantifiers . It scans the 
constraints to identify expressions that (1) use operators like 
        , etc. and (2) involve a single-occurrence variable 
denoting input plus another variable denoting the return value of 
an un-modeled API invocation. It then marks the se identified 
expressions as validated. For example,          (   )    
in the example of Figure 1  can be identified as validated without 
using runtime data because   is only used in this constraint and 
operator    allows   to control the outcome of this expression to 
be always true regardless of the return value of          (  
 ). We leave the study of other more sophisticated quantifier 
elimination approaches [ 1] to our future work. We assume that 
inputs can bind to arbitrary values because a sound analysis of 
software applications should include the scenarios arising from 
malicious or malformed inputs. 
After quantifier elimination, the MAX-SAT becomes 
Maximize:          ( ⃗⃗⃗ 〈     〉⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗ )  
subject to 
(     ) (               )  (               )       
, where 〈     〉 ⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗ is the part of 〈     〉⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗ that still requires to be 
validated using runtime data. To apply our approach, we need to 
have the vector  ⃗⃗⃗ . Ideally, the set  ⃗⃗⃗ can be complete if we have 
formal specifications of all un-modeled APIs. In our approach, 
we obtain  ⃗⃗⃗ from the runtime data collected from earlier test runs. 
In some situations, some tuples of unknown may not be 
successfully validated using the collected runtime data. Under 
these situations, one may prioritize the warnings based on the 
amount or proportion of the validated tuples of unknown. We 
leave the study of such prioritization to our future work. In this 
paper, we focus only on the warnings whose associated symbolic 
states can be fully validated. In this setting, the problem is 
equivalent to SAT problem. 
It is easy to show that a symbolic state is feasible based on 
Definition 3 if the symbolic state is validated using the approach 
above. The proof is done by binding all symbolic values to the 
concrete values given by the constraint solver and show that there 
is no contradiction in the constraint. Our approach handles the 
computation of un-modeled APIs based on the program states 
obtained from earlier executions. In practice, some APIs may be 
subject to side effects such as thread creation and subscribing call 
back functions. Our implementation, explained in Section 5, 
adapts existing techniques to handle such APIs. The remaining 
APIs are assumed to have no side effects. Section 5 and Table 1  
show that our approach allows analysis with many un-modeled 
APIs and the manual efforts of API modeling is significantly 
reduced. We empirically show that combining modeling and 
runtime data provides precise results in practice (no false 
warnings are found in our empirical study) in Section 5.1.  
4. IMPLEMENTATION 
We have implement ed a symbolic analysis tool called Sym-JVM 
[28] to analyze Java programs. Sym-JVM uses Choco 3.0 [23 ] as 
its constraint solver and follows the design of KLEE [ 4]. For 
example, both tools use copy-on -write technique to improve state 
forking efficiency and reduce memory consumption. Unlike 
KLEE, Sym-JVM allows approximating the return values of the 
un-modeled APIs and leverages these approximated variables to 
perform state reduction. Like existing static analysis tools [30 ,37], 
Sym-JVM includes the models of a small set of basic APIs , 
including thread creating APIs, string APIs, array copy APIs, and 
container APIs. On top of these APIs, we manually model a few 
APIs where over-approximation obviously does not make sense, 
                                                                 
1 Number of warnings reported by concolic testing using inputs from 
system test cases is marked with '*'.  Table 1. Subject programs  
# Subjects  Source 
lines of 
code  Un-modeled 
APIs  Shared 
APIs  
1 Struts 2.3.15.1  151,941  1,138  90%  
2 Apache Tomcat 7.0.41  183,775  1,245  85%  
3 Apache FOP 1504760  156,135  924 84%  
4 Apache Lucene 4.4.0  432,691  367 96%  
5 Findbugs 2.0.2  121,988  1,114  80%  
6 AMCE web server 1.104  17,371  848 85%  
7 Antlr 4  56,165  374 87%  
8 Hadoop 2.0.5 alpha  266,007  1,547  85%  
9 Pmd 5.0.4  55,609  1,071  97%  
10 Proguard 4.10  62,518  253 98%  Table 2．# of w arnings  comparison  
Subjects  RAA SAVE  Concolic1 
Struts 2.3.15.1  30 2 0 
Apache Tomcat 7.0.41  28 3 0 
Apache FOP 1504760  187 3    1 * 
Apache Lucene 4.4.0  75 2 0 
Findbugs 2.0.2  206 2    0 * 
AMCE web server 1.104  24 1 0 
Antlr 4  1,896  2 0 
Hadoop 2.0.5 alpha  14,983  62 0 
Pmd 5.0.4  67 4 0 
Proguard 4.10  48 2    1 * 
 
191including the APIs for creating threads, and APIs that involve call 
back functions. Totally, we modeled 168 APIs. Other APIs are 
un-modeled. As a result, more than 80% of the APIs are un-
modeled on average.   In our study, we consider command line 
arguments, file input, and network input as program inputs. 
Although our discussion in this paper focuses on single parameter 
and return value for simplicity, Sym-JVM supports multiple 
parameters and return values. It allows variables reachable in a 
method call to be treated as its input and output. For example, if a 
method invocation          ( )  may modify a field    ,     is 
also treated as a return value (or output). 
For dynamic data collection, Sym-JVM uses Javassist [ 21] to 
instrument subject programs to collect object states before and 
after un-modeled API invocations. We also use JVM’s 
instrumentation framework [ 18], which can perform loading time 
instrumentation. The object states are stored using JSON format. 
We use Go programming language [ 19] to build a server with 
around 1,000 SLOCs to manage these JSON objects, which are 
indexed and stored in sqlite [24 ] database. 
Let us explain the handling of object returns, which often involve 
complex data structures from un-modeled APIs. Hence, simply 
introducing a fresh symbolic variable is insufficient. To address 
this issue, Sym-JVM deploys lazy initialization techniques [26 ] to 
handle returned objects from un-modeled APIs. For instance, a 
program invokes an un-modeled API that returns an object o. 
Sym-JVM first marks the object to be an unknown object. When 
the subsequent code accesses a field f in o, Sym-JVM initializes 
field f to point to a new object x and mark x to be an unknown 
object. When Sym-JVM performs symbolic state validation, it 
checks whether the lazily initialized object can be validated by 
some runtime instance. For example, suppose there is an object o 
with three fields: a, b and c. Also suppose that Sym-JVM has 
lazily initialized a, b to be some non-null objects while c remains 
uninitialized in the execution path under analysis. This only the 
bindings of a and b are needed for the analysis. If there exists a 
runtime instance r returned from the same API invocation with 
consistent precondition such that     is not null and     is not 
null, this object can be validated. Since field c remains unknown 
in symbolic analysis. Sym-JVM does not need to validate it.  
5. EVALUATION 
We conducted evaluation to study (1) whether over-
approximating the return values of un-modeled APIs cause many 
false warnings in real applications; and (2) whether our approach 
is able to effectively distinguish real warnings from false 
warnings caused by such over-approximated return values. We choose ten real world open source Java programs as our subjects. 
The information about these subjects is listed in Table 1 . These 
subjects have altogether around 1.5 million lines of code. We 
made two efforts for realistic evaluation. First, we chose popular 
industrial scale subjects instead of other benchmarks like Dacapo 
[22], which contains relatively small size subjects relying on few 
external APIs. Second, we applied Sym-JVM to the entire 
subjects without excluding those components that make heavy 
use of APIs. Table 1  shows the number of unique un-modeled 
API functions used by the chosen subjects. Each individual 
subject is built on top of a few hundred to more than one 
thousand unique un-modeled APIs. The ten subjects altogether 
rely on 3,180 unique un-modeled APIs. Modeling these APIs is 
hard because it requires domain knowledge of graphics, JVM 
execution, and operation systems etc. Fortunately, these APIs are 
commonly used by the subjects. Table 1  gives the percentage of 
the shared un-modeled APIs2 for each subject. This indicates the 
runtime data of a subject’s un -modeled APIs can be mostly 
reused for the analysis of another subject. To our knowledge, 
symbolic analysis has not been applied to these subjects because 
of the huge API modeling effort. The evaluation assesses the 
usefulness of our approach in extending static symbolic analysis 
to large real-world programs.  
We collected the API runtime data by running the documented 
test cases of the first five subjects in Table 1 . These five subjects 
were selected to collect API runtime data because they are 
accompanied by ample test cases. The database of runtime data is 
available online [ 20]. It currently stores about 200 GB runtime 
data with more than five million unique concrete pairs of input 
parameters and returns. 
To perform warning validation, we need a set of warnings 
generated for our chosen subjects. We configured our Sym-JVM 
to use Random strategy and A PI-Approximation (RAA) [ 5,9] to 
perform path exploration from main methods to detect runtime 
exceptions for each subject with maximum time budget of six 
hours3. Random path exploration strategy is commonly used by 
many symbolic analysis studies [ 4,27,34]. The warnings 
generated by RAA are available online [ 20]. The experiment was 
conducted on a Linux server with 16 cores of 2.10GHz CPUs 
running Centos 6.4 64-bit. We used the JDK 1.7 from Oracle and 
we set the maximum heap size for Sym-JVM to 22GB. The 
runtime exceptions detected by Sym-JVM include null pointer 
exceptions, array out of bound exceptions , assertion violations , 
and tho se uncaught exceptions thrown by the programs or APIs. 
The second column of Table 2  lists the number of warnings 
generated for each subject. For some large subjects like Hadoop, 
the number of generated warnings is about 15K. Validating these 
warnings manually can take non-trivial human efforts. 
5.1 Warning Validation 
We applied our Symbolic stAte Validation techniquE (SAVE) 
over Sym-JVM to automatically validate true warnings from the 
warnings generated by RAA. The number of warnings validated 
by SAVE is listed in the third column of Table 2 . All of these 
warnings have been confirmed explicitly by test cases [ 20] 4. 
Note that we did not collect any runtime data of subjects 6-10 
                                                                 
2 We consider an API is shared if the API is used by more than one 
subjects.  
3 The validation time is also included in the analysis time. 
4 The test cases were generated in a semi-automated way. We checked the 
constraints and the hints provided by Sym-JVM and then constructed the 
environments to trigger the bug. Generating complex environments for 
large subjects automatically is beyond the capability of our tool. 
(a) The number of un -modeled 
API before reaching warning.
20 50 100 500 2000
10 20 50 100 200 500
5 10 20 50 100 200(b) The number of 
constraints before reaching 
warnings.(c) The number of 
constraints involving over -
approximated symbolic 
values before reaching 
warnings. 
Figure 4. Some statistical information about warnings  
192from their test cases. The experimental results show that our 
approach is useful for the subjects with few or no documented 
test cases, like most Android apps. RAA reports more warnings 
in Antlr and Hadoop than the other subjects. This is because Antlr 
involves many recursive method invocations such that one buggy 
statement can be reached by many different stack traces while 
Hadoop has many entry methods. 
We also study the warnings that have not been validated by 
SAVE to check whether they are true warnings. Since we cannot 
afford to study all the warnings, we randomly (using random 
generator to select warnings) sample 10% (about 1.7K warnings) 
of un-validated warnings to investigate and we only found one 
true warning that was not validated by our approach. We spent 
two months on th is manual checking . The missed true warning is 
related to missing information of an exceptional case. The 
warning requires an API to throw an exception to trigger. 
However, there is no such information for this API used by the 
subject in our current database. We can enrich the database with 
such information by using more system test cases in the future. In 
our investigation, we classify a warning as a false warning by 
checking the un-validated APIs against the JDK documentation. 
If an error symbolic state requires some impossible behavior of 
APIs to reach, the warning is classified as a false warning. Our 
empirical result shows that many warnings generated by static symbolic execution with API approximation are indeed false. It 
could take huge effort to manually confirm true warnings among 
the numerous generated warnings. Our approach is able to 
automate this tedious task so that developers can prioritize their 
limited resources on the true warnings. 
In the process of analysis, we also record ed the number of un-
modeled API invocations before the Sym-JVM reaches the 
reported error states. The result is plotted in Figure 4  (a). On 
average, Sym-JVM observes 237 un-modeled API invocations 
before reaching an error state. These API invocations generate 
large numbers of over-approximated symbolic values. We also 
plot the number of constraints before Sym-JVM reaches the error 
states in Figure 4  (b) and the number of constraints with over-
approximated symbolic values before Sym-JVM reaches the error 
states in Figure 4  (c). On average, there are 275 constraints 
governing each error state. The average value is reduced to 78 
(28%) if we confine the constraints to those with over-
approximated symbolic values. Such an amount of constraints 
can be handled by existing constraint solvers effectively. Our 
approach can be practically used to validate the warnings reported 
by symbolic analysis, saving developers’ efforts.  
5.2 Bug Details 
To evaluate the applicability of our approach in identifying real 
world bugs, we applied our approach directly to the 10 selected 
subjects without artificially seeding additional bugs. Our 
evaluation focus on those bugs that cause exceptions, such as null 
pointer exceptions, array out of bounds exceptions, and 
exceptions thrown out of main functions. All validated warnings 
have been confirmed explicitly by test cases [ 20]. A large portion 
private KeepClassSpecification [] extractKeepSpecifications (
List  keepSpecifications , boolean allowShrinking , 
boolean allowObfuscation )  {
List matches = new ArrayList ();
for(intindex = 0; index < keepSpecifications. size(); index ++) // bug
{
…
private void loadBoilerplateConfiguration ()   {
try{
ConfigurationParser parser = new ConfigurationParser (
this. getClass ().getResource (BOILERPLATE_CONFIGURATION),
System.getProperties ());
Configuration configuration = new Configuration ();
try{
parser. parse (configuration );
// We're interested in the keep options.
boilerplateKeep =
extractKeepSpecifications (configuration.keep , false, false );
…1
2
3
4
5
6
7
1
2
3
4
5
6
7
8
9
10
11
12
public static void invokeAndWait (Runnable runnable )   {
try{
if(!SwingUtilities. isEventDispatchThread ()) {
SwingUtilities. invokeAndWait (runnable);
} 
…1
2
3
4
5
6
public static void main ( )
SwingUtil. invokeAndWait (new Runnable ()     {
public void run() {
try{
// transitively invoke loadBoilerplateConfiguration1
2
3
4
5
#Input Configuration snippet
-injars bin/classes
-injars libs
-outjars bin/classes -processed.jar
-dontpreverify
-repackageclasses ''
-allowaccessmodification 
Figure 5. A validated example from Proguard  
public static void main (String[] args) {
if(checkDependencies ()) 
startFOP (args);
#input arguments 
java org.apache.fop.cli.Main --dump -config -xml input.xml private intparseXSLInputOption (String[] args, inti) throws FOPException
{
setInputFormat (XSLT_INPUT);
if((i+ 1 == args.length ) || ( isOption (args[i+ 1]))) {
throw new FOPException ("you must specify the stylesheet ");
} else {
xsltfile = new File(args[i+ 1]);
return 1;
private intparseXMLInputOption (String[] args, inti) throws FOPException
{
setInputFormat (XSLT_INPUT);
if((i+ 1 == args.length )   || (isOption (args[i+ 1]))) {
throw new FOPException ("you must specify the input file ");
} else {
String filename = args[i+ 1];
if(isSystemInOutFile (filename)) {
this.useStdIn = true;
} else {
xmlfile = new File(filename );
private boolean parseOptions (String[] args) throws FOPException {
else if(args[i].equals ("-xsl")) {
i= i+ parseXSLInputOption (args, i);
} else if (args[i].equals ("-xml")) {
i= i+ parseXMLInputOption (args, i);private void dumpConfiguration () {
case XSLT_INPUT:
log.info("xsltstylesheet : " + xsltfile. toString ()); //bug1
2
3
1
2
3
4
5
6
7
8
1
2
3
4
5
6
7
8
9
10
11
1
2
3
4
5
1
2
3 
Figure 6. A validated example from Apache FOP  
193of these warnings are induced by unexpected values in the 
options specified by users. We selected 11 warnings that are 
apparently caused by logic errors and filed subsequent bug 
reports to developers. The summary of bug reports can be found 
in Table 3 . By the time of writing, six of them have been 
confirmed. Four of these six have already been fixed. The URL 
of bug reports and details of bugs can be found in [20 ]. One was 
marked as duplicated. One aroused discussion but no good 
solution was found. For severity of these responded bugs, five are 
marked as major, two are normal, and one is low. Although this 
work focuses on automatic identification of true warnings, it 
would be interesting to extend the technique to categorize 
warnings into possible failure causes in future work.  
5.3 Warning Examples  
This section discusses some examples of validated and un-
validated warnings . In Figure 5 , a null pointer exception can 
occur in Line 5 of extractKeepSpecifications ( ). The Sym-JVM 
finds that it is possible to generate an input that is shown at the 
bottom of Figure 5  so that configuration .keep is not initialized 
when the input is parsed by parse ( ) in Line 8 of 
loadBoilerplateConfiguration ( ). The field configuration .keep is 
passed to extractKeepSpecifications ( ), which dereferences the 
field without checking whether it is a null pointer. To check if 
this warning is true, Sym-JVM leverages runtime data to show 
that predicate  !SwingUtilities. isEventDispatchThread ( ) in Line 
3 of invokeAndWait ( ) is feasible. The feasibility of this predicate 
is a necessary condition to show that the reported anomalous 
symbolic state is reachable because the Runnable object that 
transitively invokes loadBoilerplateConfiguration ( ) is passed to 
invokeAndWait ( ) in Line 2 of main ( ). Before reaching 
extractKeepSpecifications ( ), 5,055 un-modeled APIs have been 
invoked in the symbolic analysis. This makes it difficult for 
developers to validate such warnings manually. In this example, 
our technique is able to conclude that the error statement is 
indeed reachable even we do not have the models of all APIs. 
Figure 6  shows another example from Apache FOP project. The 
bug is in the Line 3 of dumpConfiguration ( ), where an 
uninitialized field variable xsltfile is dereferenced. The problem 
occurs when “ -xml” option is specified in the command line . In 
parseXMLInputOption ( ), the input format is set to 
XSLT_INPUT in Line 3 of parseXMLInputOption ( ). In this case, 
variable xmlfile  is initialized instead of xsltfile . The reason why 
the developer made this mistake is that there is a method called 
parseXSLInputOption ( ) that s ets the same type of input format 
(XSLT_INPUT) and initializes xsltfile . As such, the developer 
thought that xsltfile  is always initialized when XSLT_INPUT is 
set. To check if the symbolic state is feasible, our technique 
validates that (1) checkDependencies () in Line 2 of main ( ) can 
return true, and (2) all the path conditions between startFOP ( ) and dumpConfiguration ( ) are feasible. At the bottom of Figure 6 , 
we put the command line input that can trigger the bug. 
While it is possible to extend static symbolic analysis to real-
world programs by approximating the un-modeled APIs, we can 
see from the above two examples that validating the warnings 
thus reported is non-trivial because they often involve multiple 
procedures and invocations of large numbers of complex external 
APIs. Our approach is valuable to help developers to concentrate 
on the true warnings (i.e., real bugs) when developers want to 
deploy static symbolic analysis but cannot afford to model all 
external APIs. 
According to our random sampling, un-validated ones are likely 
false warnings. Figure 7  shows an example, in which a false 
warning is un-validated by our approach. Sym-JVM reports a 
warning in Line 4 of loadCorePackage ( ).  It is suspected that 
loader in Line 4 of loadCorePackage ( ) may be a null pointer. 
The variable loader is catalinaLoader , which is passed from 
SecurityClassLoad. securityClassLoad ( ) in Line 5 of init( ) to 
loadCorePackage ( ) in Line  4 of securityClassLoad ( ). Sym-JVM 
finds that catalinaLoader , in some execution path, is an alias of 
commonLoader  that is created in Line 3 of initClassLoaders ( ). It 
is because that parent  can be returned in Line 5 of 
createClassLoader ( ), where variable parent  is commonLoader . 
In the code, there is nothing preventing commonLoader  from 
being a null pointer. We cannot even use heuristics like 
private static final void loadCorePackage (ClassLoader loader)
throws Exception {
…
loader. loadClass (basePackage + "AccessLogAdapter "); //false warning
…
public void init() {
…
initClassLoaders ();
…
SecurityClassLoad. securityClassLoad (catalinaLoader );
public static ClassLoader createClassLoader2 (
List<Repository>repositories,  final ClassLoader parent ){
…
return AccessController. doPrivileged (
new PrivilegedAction <StandardClassLoader >() {
@Override
public StandardClassLoader run() {
if(parent == null)
…..public static void securityClassLoad (ClassLoader loader )  
throws Exception {
…
loadCorePackage (loader );
…
private void initClassLoaders () {
…
commonLoader =this. getClass ().getClassLoader ();
…
catalinaLoader = createClassLoader ("server", commonLoader );
sharedLoader = createClassLoader ("shared", commonLoader );
private ClassLoader createClassLoader (String name, ClassLoader parent)
throws Exception {
…
if( somecondition )
return parent ;
…
return ClassLoaderFactory. createClassLoader 2(repositories , parent);1
2
3
4
5
1
2
3
4
5
1
2
3
4
5
1
2
3
4
5
6
1
2
3
4
5
6
7
1
2
3
4
5
6
7
8
9 
Figure 7. An un -validated false warning from Apache Tomcat  Table 3 Bug report statistics  
Rpt#  Affected Subject  Confirmed  Fixed  Severity  
1 Proguard  No No Not classified  
2 AMCE web server  Yes No Low 
3 Tomcat  Yes Yes Normal  
4 Tomcat  Yes Yes Normal  
5 FOP No 
(duplicated)  No Major  
6 FOP Pending  
(discussion)  No Major  
7 FOP Pending  No Not classified  
8 FOP Pending  No Not classified  
9 Hadoop  Yes No Major  
10 PMD  Yes Yes Major  
11 Struts Yes Yes Major  
194consistency checking [ 10] to avoid generating this warning. In 
this example, variable parent  is passed to createClassLoader2 ( ), 
in which parent  is checked against to null in Line 8 of 
createClassLoader2 ( ). The consistency checking heuristic [ 10,11]  
would report that parent is likely  to be null because the 
programmer explicitly checks it against null . However, according 
to JDK specification, getClassLoader ( ) would never return null 
for non-bootstrap boot loaders . In the case of permission denial, 
getClassLoader ( ) would throw a SecurityException instead of 
returning a null pointer. The false warning is un-validated and our 
technique reduces the efforts of developers to check these kinds 
of false warnings. 
5.4 Discussion 
We have present ed an automated and effective approach to 
identifying true warnings amongst large numbers of warnings 
generated by symbolic analysis when some APIs’ models are 
unavailable . Although our approach does not guarantee to 
identify all true warnings, the experimental results on 10 real 
industrial scale subjects show that our approach is useful to 
identify real bugs. We made several observations, which account 
for the effectiveness of our approach, in our experiments. First, 
many un-modeled API invocations are irrelevant to the 
reachability of error symbolic states. Our observation is that 
many symbolic values introduced by un-modeled APIs are not 
used in the constraints of path conditions and they are not 
required to be validated. The data in Figure 4  show that the 
number of constraints with over-approximated symbolic values is 
much smaller than the number of over-approximated variables 
generated by un-modeled API invocations before Sym-JVM 
reaches the error symbolic state. Second, there is a large set of 
concrete values that can be used to establish the feasibility of a 
constraint. For example, there are many possible values of   to 
make      satisfiable. Collecting such values from the 
dynamic execution of documented test cases is highly possible.  
Third, we found that the symbolic analysis often finds multiple 
paths leading to the same warning as shown in Figure 8 . The 
mean is 264 and median is 5 for the number of paths leading to 
the same warning. We consider a warning is validated as long as 
one of the paths can be validated. This also greatly increases the 
chance of validating a warning. 
Our approach complements other warning prioritization 
techniques [ 3] and semi-automated validation techniques [ 8]. For 
example, our technique can be first applied to identifying true 
warnings. After fixing these warnings, developers may apply 
existing prioritization [ 3] and semi-automated validation 
techniques [ 8] on the remaining un-validated warnings. 
Although existing tools DSC [25 ] and jCUTE [ 33] provide a 
prototype implementation of concolic testing, we were not able to 
apply them successfully to the real-world subjects enlisted in 
Table 1 due to the various limitations of the tools. For example, 
the DSC provides inadequate support of JDK libraries while 
jCUTE has limited support for string input s. As such, we adapt 
Sym-JVM to implement concolic testing for comparison with our 
proposed approach. We conducted two further experiments. Concolic testing is applied to each subject using the same time 
budget of six hours as our previous experiment. The time budget 
is comparable to that adopted in existing studies [ 4,25,33,34]. 
Besides the number of detected warnings, we compare the 
effectiveness using statement and branch coverages because few 
statements and branches in real subjects are unreachable [ 38]. We 
do not make comparison using path coverage because concolic 
testing is a dynamic approach while our technique SAVE is a 
static approach.  
In the first experiment, we applied concolic testing to each 
subject with 100 randomly generated test inputs as the seed test 
case. We found that concolic testing approach failed to achieve 
more than 6% of statement coverage for all of our large real 
world subjects and report no warnings. In the second experiment, 
we applied concolic testing to subjects 3, 5 and 10 with test 
inputs derived from their system test runs. We are not able to find 
documented system test runs for the remaining seven subjects. 
Note that our technique SAVE can leverage runtime data 
collected from unit test cases, which are much more widely 
available than system test cases . The use of inputs derived from 
system test cases improves the statement coverage of concolic 
testing to around 20%-30% for the three subjects . Figure 9 shows 
the branch coverage achieved by concolic testing and our SAVE 
technique. The branch coverage of concolic testing for subjects 3, 
5 and 10 is based on inputs from system test cases while the 
branch coverage of the remaining 7 subjects is based on random 
test cases. We find three reasons that explain the low coverage of 
concolic testing: (1) it cannot often find appropriate input values 
to survive initialization phases if test inputs are generated 
randomly, (2) it exhausts resources before finding appropriate 
function parameter values to achieve adequate branch coverage 
even it is able to survive initialization phases with inputs from 
system test cases , and (3) it fails to automatically setup 
environments for distributed program like Hadoop. The first and 
second reasons have been discussed in Section 2 and our result 
confirms a similar finding made by an earlier study [ 39]. Real 
world large subjects typically have complex initialization module 
and function parameters. They also require non-trivial setup to 
work correctly. The two experiments show that our approach is 
able to achieve higher test coverage than concolic testing for the 
10 real world subjects, and successfully detects more warnings 
with the same time budget as shown in Table 2 . 
Our approach requires instrumented program execution in order 
to collect runtime data . The instrumented test cases can run for a 
few hours in the collecting process. The execution time is 
acceptable for our application because we only need to collect the 
runtime data once. Although reducing instrumentation overhead 
is not a focus of our work, we plan to reduce the overhead using 
more sophisticated instrumentation techniques [ 29]. Besides, our 
runtime data collection is performed before the analysis time and 
the initial cost is amortized by many executions of symbolic 
1 100 10000 
Figure 8. Number of paths leading to the same warning  
00.20.40.60.81
0 5 10C
o
v
e
r
a
g
e
Subjects
Sym-JVM
Concolic
 
Figure 9 Branch coverage comparison  
195analysis. Currently, we have not optimized our instrumentation 
code and the instrumented test runs may incur 100 times 
overhead. 
We also studied the usefulness of utilizing the runtime data 
collected from other subjects’ test cases in validating true 
warnings of a subject. The study was conducted on subjects 1-5 
because each of them is accompanied by a set of well 
documented test cases. Eight true warnings are successfully 
validated by using only the runtime data derived from each 
subject’s own test cases. Four more true warnings are validated 
by using also the runtime data derived from the other four 
subjects’ test cases. The reason accounting for the difference is 
that other subjects ’ runtime data can sometimes trigger the corner 
cases  of the subject-under-test. In other words, the runtime data 
of other subjects can include those variable bindings that 
infrequently occur to the subject-under-test in its normal 
executions. These bindings are useful to trigger infrequent buggy 
execution paths of the subject-under-test. This shows the merit of 
cross fertilization using the runtime data from different subjects. 
6. RELATED WORK 
Promising results of symbolic analysis have been reported in the 
areas like test case generation [ 4,17,34], static bug detection 
[5,27], regression testing [ 31], security analysis [ 35], etc. To 
ensure its precision, it is often necessary to consider the APIs 
used by the programs under analysis.  
Concolic testing [15 ,17,34] is a dynamic approach to the 
symbolic analysis. It does not require prior modeling of such 
methods’ API behaviors.  However, concolic testing approach can 
only handle the APIs that are reachable by dynamic execution [27 ] 
and it often fails to figure out appropriate parameters for external 
APIs to guide execution [39 ]. Godefroid [ 16] proposed to 
leverage  parameters and return values of APIs to improve 
symbolic execution. However, there is no efficient constraint 
solver to solve the generated constraints that involve first order 
logic formula of functions [ 16]. Therefore, the technique has not 
been implemented. In contrast, our approach generates 
constraints that only involve formula in terms of variables and 
those operators supported by existing efficient constraint solvers 
as shown in Section 3.2. Our approach does not requires 
concretization of over-approximated variables until validating a 
symbolic state and therefore it is applicable to scalable static 
symbolic analysis approach that exploits over-approximated 
variables [ 28]. Our evaluation in Section 5 shows that our 
approach scales better on large subjects. Besides, it is difficult to 
perform concolic testing on those real world programs, whose 
execution requires complex environment setup, consumes 
expensive resources, or incur costly side-effects upon failures. 
For example, the execution of Hadoop or railway control systems 
often require non-trivial setup of distributed environments. Due 
to these limitations, concolic testing has not been widely adopted 
in system testing. Unlike concolic testing, our approach does not 
require further execution of the subject programs and this feature 
makes it applicable these programs whose execution 
environments are non-trivial to setup and manipulate. Concolic 
testing is also used to generate mock object for unit testing 
[25,36]. Due to scalability issue of concolic testing, existing 
approaches [25 ,36] can only be applied to unit test of small 
modules. Our approach is able to perform whole program 
analysis.  
Approaches [27 ,32] have been proposed to use runtime data 
collected from executions to automatically construct 
approximated API models or related constraints for symbolic analysis. Each constructed model is an approximation because the 
behavior covered by dynamic execution is incomplete. In contrast, 
our approach works differently by binding symbolic values to 
concrete values in the validation process. Our approach avoids 
the imprecision due to model construction. Besides, these 
approaches are limited in terms of constructed models’ 
expressiveness. For example, they can only construct models for 
those APIs based on their predefined data structures [ 32] or 
predefined linear constraints [ 27]. Our approach requires no prior 
knowledge of data structures or linear computation in the un-
modeled APIs.  
There are two categories of API specification mining. One 
focuses on temporal relation mining [2 ,13,14] of specifications on 
temporal relations like     (  ),      (  ), and      (  ). These 
specifications aim to detect program bugs instead of validating 
warnings because confining API invo cations’ temporal order 
does not imply that a warning is true. Another category focuses 
on arithmetic specifications [7 ,12]. For example, the parameter 
    of         (     )  must be a positive number. Although 
simple arithmetic specifications can describe simple program 
properties, they cannot precisely describe the complex behaviors 
of APIs. These approaches also suffer from imprecision issues 
when mining specifications from finite number of test cases. 
Since the models of external APIs are often unavailable, recent 
static symbolic analysis approach es assume these APIs can return 
arbitrary values [ 5,9]. Such approximation, however, can 
generate many false warnings. Since manual inspection of such 
warnings is tedious and time consuming, researchers proposed 
different approaches to validating warnings. For example, Dillig 
et al. [ 8] proposed a semi-automated approach to diagnosing 
warnings. Semantic consistency checking [ 10,11] is a technique 
to obtain semantic information from the usage of variables. For 
example, a check var==null  in source code would indicate that 
variable var is likely to be null. Blackshear et al. [ 3] proposed a 
more general warning prioritization approach inspired by the idea 
of program semantic consistency checking. These techniques 
reply on heuristics and still require developers to manually 
confirm warnings. Our example in Figure 7  shows that these 
techniques can prioritize false warnings. In contrast, our approach 
is precise for identifying true warnings as shown in our 
evaluation. Our approach complements existing techniques in that 
it can be first applied to identify true warnings, followed by 
further diagnosis or prioritization of the remaining warnings 
using existing techniques after these true warnings have been 
dealt with. 
7. CONCLUSION 
In this work, we have presented an approach to validating 
warnings generated by the symbolic analysis. Our approach 
provides a practical solution to distinguish true warnings from 
false warnings when developers want to apply static symbolic 
analysis to large program but cannot afford to model all the APIs 
involved. Our approach successfully extends the application of 
static symbolic analysis to ten large real programs that rely on 
external or native APIs, and automatically detects true warnings 
of these programs. We plan to apply our approach to other 
platforms, such as Android, whose applications typically rely on 
external APIs and GPU render scripts. 
Acknowledgments . 
The work is supported by the Hong Kong SAR RGC/GRF grant 
611912. 
 
1968. REFERENCES 
[1] Enderton, H. B., A Mathematical Introduction to Logic .: 
Academic Press, 2001.  
[2] Ammons, G., Bodí k, R., and Larus, J. R., "Mining 
specifications," Proc. of POPL '02, ACM, 2002.  
[3] Blackshear, S and Lahiri, S. K., "Almost -correct 
specifications: a modular semantic framework for assigning 
confidence to warnings," Proc. of conf. on PLDI '13, ACM, 
2013.  
[4] Cadar, C., Dunbar, D., and Engler, D., "KLEE: unassisted and 
automatic generation of high -coverage tests for complex 
systems programs," Proc. of O SDI'08, USENIX Association, 
2008, pp. 209 -224. 
[5] Chandra, S., Fink, S. J., and Sridharan, M., "Snugglebug: a 
powerful approach to weakest preconditions," Proc. of PLDI 
'09, ACM, 2009, pp. 363 -374. 
[6] Chipounov, V., Kuznetsov, V., and Candea, G., "The S2E 
Platform: Design, Implementation, and Applications," ACM 
Trans. Comput. Syst. , vol. 30, no. 1, p. 49, 2012.  
[7] Csallner, C., Tillmann, N., and Smaragdakis, Y., "DySy: 
dynamic symbolic execution for invariant inference," Proc. of 
the 30th int. conf. o n ICSE '08, ACM, 2008.  
[8] Dillig, I., Dillig, T., and Aiken, A., "Automated error diagnosis 
using abductive inference," Proc. of conf. on PLDI '12, ACM, 
2012.  
[9] Dillig, I., Dillig, T., and Aiken, A., "Sound, complete and 
scalable path -sensitive analysis," Proc. of PLDI '08, ACM, 
2008.  
[10] Engler, D., Chen, D. Y., Hallem, S., Chou, A., and Chelf, B., 
"Bugs as deviant behavior: a general approach to inferring 
errors in systems code," Proc. of SOSP '01, ACM, 2011.  
[11] Engler, Dawson and Dunbar, Daniel, "Under -constrained 
execution: making automatic code destruction easy and 
scalable," Proc. of ISSTA '07, 2007.  
[12] Ernst, M. D. et al., "The Daikon system for dynamic detection 
of likely invariants," Sci. Comput. Program. , pp. 35 -45, 2007.  
[13] Gabel, M. and Su, Z., "Javert: fully automatic mining of 
general temporal properties from dynamic traces," Proc. of 
FSE-16, ACM, 2008.  
[14] Gabel, M. and Su, Z., "Symbolic mining of temporal 
specifications," Proc. of ICSE '08, ACM, 2008.  
[15] Godefroid, P., "Compositional dynamic test generation," Proc. 
of POPL '07, ACM, 2007, pp. 47 -54. 
[16] Godefroid, P ., "Higher -order test generation," Proc. of PLDI' 
11, 2011.  
[17] Godefroid, P., Klarlund, N., and Sen, K., "DART: Directed 
Automated Random Testing," P roc. of PLDI 05, ACM, 2005, 
pp. 213 -223. 
[18] http://docs.oracle.com/javase/6/docs/api/java/lang/instrument/
package -summary.html Oracle.  
[19] http://golang.org/ golang.  
 
 
 
 [20] http://sccpu2.cse.ust.hk/yueqili/validation/.  
[21] http://www.csg.ci.i.u -tokyo.ac.jp/~chiba/javassist/ javassist.  
[22] http://www.dacapobench.org/Dacapo.  
[23] http://www.emn.fr/z -info/choco -solver/ choco -solve.  
[24] http://www.sqlite.org/ sqlite.  
[25] Islam, M. and Csallner, C., "Dsc+Mock: a test case + mock 
class generator in support of coding against interfaces," Proc. 
of WODA '10, 2010.  
[26] Khurshid, S., Pasareanu, C S., and Visser, V., "Generalized 
symbolic execution for model checking and testing," Proc. of 
TACAS'03, Springer -Verlag, 2003.  
[27] Le, W., "Segmented symbolic analysis," Proc. of ICSE' 13, 
ACM, 2013.  
[28] Li, Yueqi, Cheung, S.C., Zhang, Xiangyu, and Liu, Yepang, 
"Scaling Up Symbolic Analysis by Removing Z -Equivalent 
States," To appear in ACM Transactions on Software 
Engineering and M ethodology , 
http://www.cse.ust.hk/~scc/ScalingSymbolicAnalysis.pdf.  
[29] Nethercote, N. and Seward, J., "Valgrind: a framework for 
heavyweight dynamic binary instrumentation," Proc. of PLDI 
'07, 2007.  
[30] Pǎsǎreanu, C. S. et al., "Combining unit -level s ymbolic 
execution and system -level concrete execution for testing nasa 
software," Proc. of ISSTA '08, ACM, 2008.  
[31] Person, S., Yang, G., Rungta, N., and Khurshid, S., "Directed 
incremental symbolic execution," Proc. of PLDI '11, ACM, 
2011, pp. 504 -515. 
[32] Qi, D. et al., "Modeling Software Execution Environment," 
Proc. of WCRE '12, ACM, 2012.  
[33] Sen, K. and Agha, G., "CUTE and jCUTE: concolic unit 
testing and explicit path model -checking tools," Proc. of 
CAV'06, 2006.  
[34] Sen, K., Marinov, D., an d Agha, G., "CUTE: a concolic unit 
testing engine for C," Proc. of ESEC/FSE 05, ACM, 2005, pp. 
263-272. 
[35] Song, D. et al., "BitBlaze: A New Approach to Computer 
Security via Binary Analysis," Proc. of ICISS '08, 2008.  
[36] Tillmann, Nikolai and Schult e, Wolfram, "Mock -object 
generation with behavior," Proc. of ASE '06, 2006, pp. 365 -
368. 
[37] Tripp, O., Pistoia, M., Fink, S. J., Sridharan, M., and 
Weisman, O., "TAJ: effective taint analysis of web 
applications," Proc. of PLDI '09, ACM, 2009.  
[38] Wei, Y., Meyer, B., and Oriol, M., "Is branch coverage a good 
measure of testing effectiveness?," Proc. of Empirical 
Software Engineering and Verification, 2012.  
[39] Xiao, X., Xie, T., Tillmann, N., and Halleux, J., "Precise 
identification of problems fo r structural test generation," Proc. 
of ICSE '11, 2011.  
 
197