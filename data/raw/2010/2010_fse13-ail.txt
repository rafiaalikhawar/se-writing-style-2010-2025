Server Interface Descriptions for
Automated Testing of JavaScript Web Applications
Casper S. Jensen
Aarhus University
Denmark
semadk@cs.au.dkAnders Møller
Aarhus University
Denmark
amoeller@cs.au.dkZhendong Su
University of California, Davis
USA
su@cs.ucdavis.edu
ABSTRACT
Automated testing of JavaScript web applications is compli cated by
the communication with servers. Speciﬁcally, it is difﬁcul t to test
the JavaScript code in isolation from the server code and dat abase
contents. We present a practical solution to this problem. F irst, we
demonstrate that formal server interface descriptions are useful in
automated testing of JavaScript web applications for separ ating the
concerns of the client and the server. Second, to support the con-
struction of server interface descriptions for existing ap plications,
we introduce an effective inference technique that learns c ommuni-
cation patterns from sample data.
By incorporating interface descriptions into the testing t ool
Artemis, our experimental results show that we increase the level of
automation for high-coverage testing on a collection of Jav aScript
web applications that exchange JSON data between the client s and
servers. Moreover, we demonstrate that the inference techn ique can
quickly and accurately learn useful server interface descr iptions.
Categories and Subject Descriptors
D.2.5 [ Software Engineering ]: Testing and Debugging
General Terms
Algorithms, Languages
Keywords
Web applications, automated testing, interface descripti ons
1. INTRODUCTION
Many modern web applications run in browsers as HTML-
embedded JavaScript programs that communicate with a serve r. The
JavaScript code reacts to user events and asynchronously se nds
HTTP requests to the server for updating or retrieving data. The
response from the server is used for example to dynamically m od-
ify the HTML page. With this so-called Ajax style of structur ing
web applications, the server mostly acts as a central databa se seen
from the client’s point of view. The server interface compri ses a
collection of operations, identiﬁed by URLs, that accept in put and
produce output typically in XML or JSON data formats.
Some web service providers have public APIs, such as Google,
Twitter, and Facebook, that are well documented and used by m any
Permission to make digital or hard copies of all or part of thi s work for
personal or classroom use is granted without fee provided th at copies are not
made or distributed for proﬁt or commercial advantage and th at copies bear
this notice and the full citation on the ﬁrst page. Copyright s for components
of this work owned by others than the authors must be honored. Abstracting
with credit is permitted. To copy otherwise, or republish, t o post on servers
or to redistribute to lists, requires prior speciﬁc permiss ion and/or a fee.
ESEC/FSE ’13, August 18–26, 2013, Saint Petersburg, Russia
Copyright is held by the authors. Publication rights licens ed to ACM.
ACM 978-1-4503-2237-9/13/08...$15.00.1functiongoto_page(id, q) {
2jQuery.ajax(GET_PAGE_URL + ’?page=’ + id +
3 ’&query=’ + q,
4{’dataType’: ’json’,
5’success’:function(result) {
6populate_table(result);
7}});
8}
9
10functionpopulate_table(attendees) {
11vartable = $(’#attendees’)
12table.html(’’);
13for(i = 0; i < attendees.length; i++) {
14vara = attendees[i];
15varstyle = ’’;
16if(a.checkedin) {
17style = ’ style="background-color: #B6EDB8;"’;
18}
19ahtml = ’<tr id="row’ + a.id + ’"’ + style + ’>’ +
20’<td><b>’ + a.name + ’</b> - ’ +
21a.email + ’<br/>’ +
22a.department + ’</td>’ +
23’<td><a href="#" onclick="info(’ + a.id + ’)">’ +
24’[info]</a> ’ +
25’<a href="#" onclick="checkin(’ + a.id + ’)">’ +
26’[checkin]</a> ’ +
27’<a href="#" onclick="del(’ + a.id + ’)">’ +
28’[delete]</a>’ +
29’</tr>’;
30table.append(ahtml);
31}
32}
Figure 1: A typical example of Ajax in JavaScript.
client applications, for example in mashups that each use sm all
parts of different APIs. In contrast, in many other web appli ca-
tions, the server-side and the client-side are developed in conjunc-
tion within the same organization. In such web applications , the
programming interface of the server is often not described i n a
formal way, if documented at all. This can make it difﬁcult to
modify or extend the code, even for small web applications. M ore
concretely, we have observed that it limits the possibility of apply-
ing automated testing on the JavaScript code in isolation fr om the
server code and database contents.
It is well known that precisely speciﬁed interfaces can act a s con-
tracts between the server code and the client code, thus supp orting
a clean separation of concerns and providing useful documen tation
for the developers. In this work, we show that having formal d e-
scriptions of the programming interfaces of the server code in Ajax
web applications is instrumental when conducting automate d test-
ing of the JavaScript code in such applications. In addition , we
present a technique for automatically learning server inte rface de-
scriptions from sample data for pre-existing web applicati ons.
As an example, consider the JavaScript code in Figure 1, whic h
is part of a web application that manages attendance lists fo r meet-
ings. When the function goto_page is called, an Ajax request is[{"id": 6451, "name": "Fred", "email": "fred@cs.au.dk",
"department":"CS", "checkedin": true},
{"id": 4358, "name": "Joe", "email":"joe@imf.au.dk",
"department":"IMF", "checkedin": false}]
Figure 2: Example JSON response for the Ajax interaction
from Figure 1.
sent to the server via the jQuery library.1This request takes the
form of an HTTP GET request with a speciﬁc URL and the param-
eterspage andquery . ThedataType value’json’ on line 4 indi-
cates that the response data is expected to be formatted usin g JSON,
a widely used format because it integrates well with JavaScr ipt.2
When the response data arrives, the function populate_table is
called via line 6. By inspecting that function (lines 10–32) we
see that the JSON data is expected to consist of an array of ob-
jects with speciﬁc properties: id,name ,email ,department , and
checkedin . Moreover, their values cannot be arbitrary. For exam-
ple, thecheckedin property is used in a branch condition, so it
probably holds a boolean value, and the other properties app ear to
hold strings that should not contain special HTML character s, such
as<or&, since that could lead to malformed HTML when inserted
into the page on line 30. Figure 2 shows an example of an actual
JSON response that may appear.
In this example—as in many JavaScript web applications in
general—the interface between the server and the client is n ot made
explicit. As a consequence, the server code and the client co de be-
come tightly coupled, so it becomes difﬁcult to change eithe r part
without carefully checking the consequences to the other pa rt. For
instance, the server code could safely omit the checkedin prop-
erty when the value is false without breaking the client code,
sincea.checkedin on line 16 would then evaluate to undefined ,
which is coerced to false , however, the necessity for such subtle
reasoning makes the application fragile to modiﬁcations. A lso, the
client code implicitly assumes that escaping of special HTM L char-
acters has been performed on the server, but this may not have been
communicated to the server programmer.
One aim of our work is to advocate the use of formal interface d e-
scriptions as contracts between the client code and the serv er code.
In the example above, an interface description could specif y what
are valid request parameters and the details of the response data for-
mat, such that the server code and the client code to a larger e xtent
can be developed separately. Interface descriptions are th e key to
solve a substantial practical problem that we have observed in our
work related to the tool Artemis that performs automated tes ting of
JavaScript web applications [2]: It can be difﬁcult to set up servers
and populate databases to be able to test the client-side Jav aScript
code. Moreover, an automated tester, that focuses on testin g the
JavaScript code and has a black-box view on the server, is oft en
not able to produce high coverage tests within a given time bu dget.
With interface descriptions, we can automatically constru ct mock
servers that can be integrated into such an automated tester in place
of the real servers.
To illustrate this idea, consider again the example from Fig ure 1.
If we wish to apply automated testing to the JavaScript code, two
approaches could be considered at ﬁrst: (1) We could ignore t he
server and simply assume that any response is possible to any Ajax
request. Automated testing could then reveal that the JavaS cript
code will throw a runtime exception if the response data is no t an
array or if the array contains a null value (on line 13 and line 16, re-
spectively), and malformed HTML would be generated if the ob ject
properties contain special HTML characters. However, this does
not imply that there are errors in the JavaScript code—impli citly it
1http://jquery.com/
2http://json.org/may be the server’s responsibility to ensure that the Ajax re sponse
does not contain such values. (2) Alternatively, we could us e a live
server with realistic database content. This would elimina te the
problem with false positives in the ﬁrst approach. However, two
drawbacks arise: ﬁrst, it requires deep insight into the app lication
to be able to provide realistic database content [6]; second , the test-
ing capabilities become ﬁxed to that particular database co ntent,
which may limit the coverage of the client code. Interface de scrip-
tions give us another alternative: (3) With a description of what
requests the server accepts and the responses it may produce , an
automated testing tool such as Artemis becomes able to focus on
testing the JavaScript code on meaningful inputs.
To alleviate the burden of writing interface descriptions f or pre-
existing applications, we additionally propose an automat ed learn-
ing technique. Our hypothesis is that practically useful in terface
descriptions can be created using only sample request and re sponse
data. The sample data can be obtained by users exercising the func-
tionality of the application without requiring detailed kn owledge
of the server code. This makes the learning technique indepe ndent
of the speciﬁc programming languages and frameworks (PHP, J SF,
ASP.NET, Ruby, etc.) that may be used on the server and thereb y
be more generally applicable.
The idea of using interface description languages (IDLs) to spec-
ify the interfaces of software components has proven succes sful in
many other contexts. Prominent examples in related domains in-
clude Web IDL for the interface between browsers and JavaScr ipt
application code [10], WSDL for web service communication [ 8],
and OMG IDL for interprocess communication with CORBA [24].
Nevertheless, IDLs are still not widely used in the context o f client-
server interactions in Ajax web applications, despite the e xistence
of languages, such as WADL [13]. We suspect that one reason is
that writing the interface descriptions is a laborious task . To this
end, our work is the ﬁrst to propose an automatic technique to learn
interface descriptions for Ajax web applications.
In summary, our contributions are as follows:
•We ﬁrst introduce a simple Ajax server interface descriptio n lan-
guage, named AIL, inspired by WADL (Section 2). This lan-
guage can describe HTTP operations involving JSON and XML
data as commonly seen in Ajax web applications.
•We demonstrate how the interface descriptions can be incorp o-
rated into automated testing of JavaScript web application s to be
able to test client code without involving live servers. Spe ciﬁ-
cally, we extend the automated testing tool Artemis with sup port
for AIL (Section 3) by introducing a generic mock server com-
ponent that is conﬁgured using AIL descriptions.
•We provide an algorithm for learning AIL descriptions of Aja x
web applications through dynamic analysis of network trafﬁ c be-
tween clients and servers (Section 4).
•We experimentally evaluate our approach by investigating h ow
AIL descriptions affect the code coverage obtained by Artem is
with our extensions and by comparing the inferred AIL descri p-
tions with manually crafted ones (Section 5). Our results sh ow
that (1) by using the descriptions, Artemis can obtain as goo d
coverage with the mock server as with real servers and manual ly
populated databases and (2) the learning algorithm is capab le of
producing useful AIL descriptions.
Testing Ajax applications is recognized as a difﬁcult probl em [20,
22] and interface descriptions have proven useful for testi ng classi-
cal web applications [1, 15–18, 21], but no previous work has com-
bined interface descriptions and testing of Ajax applicati ons. Re-
lated work on interface description languages, learning al gorithms,
and automated testing is discussed in Section 6.URL http://www.example.org
GET news/read():@items.json
GET author(name:*):@author.json
GET users/login(user:*,pwd:*):@token.json
POST news/send(token:@token.json,items:+@item.json): void
POST users/create(token:@token.json,user:*, pwd:*):vo id
Figure 3: An example AIL description.
In this paper, we use the term Ajax [12] in a broad sense, cover -
ing different technologies for client-server communicati on in
JavaScript-based web applications. In current web applica tions this
typically involves the XMLHttpRequest3API, but our general ap-
proach in principle also encompasses the more recent WebSoc ket4
API. The data being transmitted may involve different forma ts in-
cluding XML and JSON that are supported by AIL, although our
current learning algorithm and experiments focus on JSON.
2. AN INTERFACE DESCRIPTION
LANGUAGE FOR AJAX
Our ﬁrst step is to design a formal language, AIL(Ajax server
Interface description Language), for describing the inter faces of
servers in Ajax-style web applications. The communication in such
web applications consists of HTTP client-server interacti ons where
the JavaScript code running on an HTML page in a browser sends
requests and receives responses. An HTTP request contains a
method (typically GET or POST), a URL path, and a number of
name-value parameter pairs. For simplicity, we abstract aw ay the
other information in the HTTP requests, such as the protocol and
headers. We design AIL as a simple language that concisely ca p-
tures the essence of WADL [13] and integrates with JSON.
An AIL description consists of a base URL and a collection of
operation descriptors , each of the form
request:response
where request is a pattern that describes a set of possible HTTP
requests, and response describes the possible responses that may
be generated by the server for those requests. Within an AIL d e-
scription, the request patterns must be disjoint in the sens e that ev-
ery possible HTTP request can match at most one of the pattern s,
which ensures a deterministic behavior.
An AIL description establishes a contract between the clien ts
and the server: The clients are responsible for ensuring tha t each
request matches one of the request patterns, and it is the ser ver’s re-
sponsibility that the response matches the corresponding r esponse
schema. Below we describe the syntax and matching semantics of
request patterns and response schemas.
Figure 3 shows an AIL description (without JSON Schema ﬁles)
for a simple JSON news server that makes ﬁve operations avail able
for JavaScript applications. The ﬁrst three operations pro vide ac-
cess to news items, author information, and authentication . The
last two operations can be used for submitting news items to t he
server and for registering new users. All operations use HTT P and
JSON. The description refers to external JSON Schema ﬁles th at
specify the data formats involved in the operations. Such an AIL
description evidently characterizes the structure of the o perations
that are supported by the server while abstracting away from the
actual data being transmitted at runtime.
The initial version of AIL supports two kinds of data formats :
JSON and XML. AIL simply relies on JSON Schema5and RELAX
NG6for describing the structure of data.
3http://www.w3.org/TR/XMLHttpRequest/
4http://www.w3.org/TR/websockets/
5http://json-schema.org/
6http://relaxng.org/POST comment/delete/comment_id:*(): @delete.json
POST project_id:*/issues/issue_id:*/set/title/
value/value:*(): @set_title.json
POST project_id:*/scrum/add/task/for/story/story_id: */
mode/issue(task_name:*): @add_task.json
Figure 4: Parts of the AIL description of The Bug Genie .
A request pattern consists of an HTTP method (GET,POST , etc.),
apath, and a comma-separated list of parameters :
method path(parameters)
A path consists of path fragments separated by ’/’, each being a
string or a parameter. Each parameter has the form name:cardinality
datatype , where cardinality is either absent (meaning precisely one
occurrence), ‘?’ (optional) or ‘+’ (zero or more).
Adatatype is written as a constant string (e.g. "start" ), the
wildcard ‘*’, the keywordvoid , a reference to an external JSON
Schema ﬁle (e.g. @token.json ), a reference to a RELAX NG
schema ﬁle (e.g.@person.rng ) or a type deﬁned in such a ﬁle
(e.g.@types.rng#person ), or a list of datatypes separated by ‘ |’.
The datatypes of parameters that occur in paths are restrict ed to
simple string types, such as numerals or string enumeration s, and
the special datatype void is never used in request patterns.
A datatype matches strings in the obvious way: a constant str ing
matches that string and no others, *matches any value, void is
used for describing the empty response, a reference to a sche ma
type matches according to the semantics of JSON Schema and RE -
LAX NG, respectively, and ‘ |’ represents union. An HTTP request
matches a request pattern if each constituent matches. A res ponse
pattern is simply a datatype with matching deﬁned according ly. We
omit the precise semantics of pattern matching due to the lim ited
space, but the intuition should be clear from this brief desc ription.
The example shown in Figure 4 is a part of an AIL description of
the application The Bug Genie .7This application uses REST-style
naming where some parameters appear in the path, not in the HT TP
request body or the query string. The responses use JSON in bo th
application; we omit the details of the associated JSON sche mas.
The AIL language as presented above is capable of expressing
the basic properties of server interfaces. One straightfor ward ex-
tension is to support other data formats, such as, HTML, plai n
text, or JSONP (JSON with padding), credentials for HTTP Ba-
sic authentication, and request content types (i.e. MIME ty pes). In
some situations it can also be useful both for documentation and
testing purposes to describe error responses, that is, non- “200 OK”
HTTP response codes, and HTTP content negotiation. For now,
AIL cannot describe temporal properties, for example that o pera-
tionAmust be invoked before operation B, simply because such
properties have not been signiﬁcant in any of the web applica tions
we have studied. Another possible extension is support for W eb-
Sockets, which unlike HTTP involves connection-oriented c ommu-
nication and thereby does not ﬁt directly into the simple req uest-
response model.
3. USING SERVER INTERFACE DESCRIP-
TIONS IN AUTOMATED TESTING
Server interface descriptions are not only useful for docum enting
the server interface for the client programmer; they also ma ke it
possible to test the client code in isolation from the server code,
which provides new opportunities for practical automated t esting.
In Section 3.1 we give a brief overview of the Artemis tool fro m
earlier work by Artzi et al. [2], with a focus on the complicat ions
caused by Ajax communication. In Section 3.2 we show how a new
7http://www.thebuggenie.com/mock server component can exploit AIL descriptions to impro ve
the level of automation.
3.1 Automated Testing with Artemis
A JavaScript web application is driven by events, such as the ini-
tial page load event, mouse clicks, keyboard presses, and ti meouts.
Event handlers are executed single threaded and non-preemp tively.
A test input to an application is thus given by a sequence of pa ram-
eterized events. Of particular relevance here are the event s that are
triggered by Ajax response where the event parameter contai ns the
HTTP response data from the server.
Figure 5 shows a use of the XMLHttpRequest API8, which pro-
vides low-level Ajax functionality (in contrast to the exam ple in
Figure 1 that uses the jQuery library). The call to x.send on line 45
initiates the request to the server, in this case an HTTP GET r equest
to the relative URL news/read , which matches the AIL descrip-
tion in Figure 3. An event handler for receiving the response is
set up on line 36. When the response content has ﬁnished load-
ing,x.readyState will have the value 4, and the event handler
function is called. If the response status code is 200 the res ponse
content is then available as a text string in x.responseText . For
this example, the challenge for an automated tester is how to pro-
duce meaningful server response data that will thoroughly e xercise
the response event handler including the showItems function be-
ing called on line 39.
The Artemis tool uses feedback-directed random testing of
JavaScript web applications to produce high-coverage test inputs.
That is, it executes the application on a test input and monit ors the
execution to collect interesting information that can be us ed to gen-
erate new test inputs to improve coverage. The heuristics us ed for
collecting information, producing new test inputs, and pri oritizing
the work list of test inputs are described by Artzi et al. [2], and we
here focus on the interactions with the server.
Although our goal is to test the JavaScript code, not the serv er,
we face the problem that the JavaScript code in Ajax-style ap pli-
cations closely depends on the server. As discussed in Secti on 1
it is often difﬁcult to populate the server database with use ful data
that is required to ensure high coverage of the JavaScript co de. A
simple example is line 16 in Figure 1, where both branches can
only be covered if the server database contains a nonempty li st of
attendees, whereof at least one is marked as checkedin and an-
other is not—no matter how other events, such as mouse clicks , are
being triggered in the browser. On top of this, even a well pop u-
lated database may not sufﬁce. Reaching one part of the JavaS cript
code may require certain values in the database where anothe r part
may require different values, so multiple database snapshots may
be necessary to enable high coverage of the JavaScript code, which
makes the burden even higher.
Yet another problem for automated testing appears when impo r-
tant server responses can only be triggered by request value s that
are practically impossible to guess by the testing tool. Con sider
for example the operation users/login for the server described
in Figure 3. A successful response requires the client to pro vide a
valid user name and password, which is (hopefully) impossib le to
guess, so a considerable part of the client code will remain u nex-
plored. A common workaround is to ask the user for help in such
situations [3]. The consequence of these problems is that “a uto-
mated” testing may require a considerable manual effort.
We observe that when testing client code, many execution pat hs
require data from the server that is structurally well-form ed but
not necessarily semantically valid. As an example, for test ing the
populate_table response handler function in Figure 1, we do not
8http://www.w3.org/TR/XMLHttpRequest/33functiongetNewsItems() {
34varx =newXMLHttpRequest();
35x.open("GET", "news/read");
36x.onreadystatechange = function() {
37if(x.readyState == 4)
38if(x.status == 200) {
39showItems(x.responseText);
40}else{
41alert("An error occurred :-(");
42}
43}
44};
45x.send(null);
46}
Figure 5: A simple use of XMLHttpRequest to perform an Ajax
call to get news items from the server from Figure 3.
need server response data that contains actual attendee nam es and
email addresses, but we do need JSON data with a certain struc ture.
This observation allows us to use AIL descriptions instead o f actual
servers and live data for testing client code.
3.2 Extending Artemis with an
AIL Mock Server
To alleviate the problems described above, we have extended the
Artemis tool with a mock server component that is conﬁgured b y an
AIL description. Whenever the JavaScript program under tes t ini-
tiates Ajax communication, the mock server intercepts the r equest
such that the actual server is never contacted during the tes ting.
Given an HTTP request, the mock server performs the follow-
ing steps: (1) It searches through the AIL description to ﬁnd an
operation descriptor with a request pattern that matches th e HTTP
request. If one is found, a random response that matches the c orre-
sponding response datatype is prepared; otherwise, the res ponse to
the client is a generic “404 Not Found”. (2) The response data is
then sent to the test input generator component in Artemis, w hich
will subsequently produce new test inputs that include an Aj ax re-
sponse event containing the response data.
As result, we obtain a nondeterministic model of how the serv er
may behave according to the AIL description, and the JavaScr ipt
code can be explored without the need of a real server and data base.
Now, several observations can be made. First, using the mock
server solves the problem of populating databases since it a utomat-
ically returns a wide range of possible responses, as speciﬁ ed by
the AIL description. This means that the client code is effec tively
tested on a variety of structurally meaningful inputs. The r esponse
data generated by the mock server may of course not be semanti -
cally valid, but as argued above, structurally correct resp onse data
will sufﬁce for testing many properties of client code. This ap-
proach also elegantly handles the issue with the users/login op-
eration mentioned above: the mock server component will ski p the
actual password check and automatically produce a meaningf ul re-
sponse representing successful login.
Second, our approach makes it easy to model the asynchronous
nature of Ajax, which is a source of intricate race errors [25 , 28]:
Even though the AIL mock server component only produces a sin -
gle response for each request in step 1, the Artemis test inpu t gen-
erator component may create multiple new inputs in step 2 to t est
different event orders.
Third, the construction of responses in step 1 may not be en-
tirely random. We can exploit the existing feedback mechani sm in
Artemis such that information that has been collected by Art emis in
previous executions of the JavaScript code with different t est inputs
can guide the selection of the new Ajax response data. Speciﬁ cally,
Artemis dynamically collects constants that are used in eac h event
handler [2], and these constants are often good candidates f or val-
ues in new event parameters, such as the Ajax response data.4. AUTOMATIC LEARNING OF
AIL DESCRIPTIONS
We have shown that AIL offers a simple, formal mechanism for
documenting client-server communications in Ajax-style w eb ap-
plications and that AIL descriptions are useful in automate d testing
of the client code. However, despite the many advantages of h aving
server interface descriptions, constructing such descrip tions for pre-
existing applications can be a nontrivial task. To support t he con-
struction of AIL descriptions, we show how to automatically learn
descriptions from samples of client-server communication trafﬁc
through a black-box, dynamic approach. This approach has be en
chosen for generality and independence of particular serve r tech-
nologies used. We imagine that such a learning algorithm can be
used when a development team realizes that their web applica tions
have grown from being small and manageable to become larger
and more difﬁcult to maintain without proper separation of c on-
cerns and without the ability to apply automated testing tec hniques.
Automated learning makes it easier to retroﬁt server interf ace de-
scriptions to existing applications, thereby supporting a utomated
testing for the further development of the applications.
We assume that the AIL descriptions being used in automated
testing as described in Section 3 have been written manually or with
support from the learning algorithm. The automatically gen erated
descriptions may naturally require some manual adjustment s since
they are generated on the basis of sample data.
We ﬁrst introduce our learning problem. The inputIdenotes a
ﬁnite set of concrete HTTP request and response pairs /a\}bracketle{tr,s/a\}bracketri}ht. The
outputddenotes an AIL description that expresses a possibly inﬁ-
nite relation /llbracketd/rrbracketof request and response pairs. Since we perform
black-box learning, we assume that sufﬁcient samples are av ailable
for learning.
Given a set of input samples, there are many valid AIL descrip -
tions that “generalize” it. Thus, it is important to deﬁne wh ich AIL
descriptions are the most desired. At the high level, we want a
learned AIL description to closely match the server program mer’s
view of the interface—a set of independent operations each w ith
its own meaning and purpose. The central challenge is to iden tify
these operations from the given observations without any wh ite-box
knowledge of the server and client.
To guide our learning algorithm, we specify the following de sir-
able properties that a learned description should have:
completeness: The input Iis covered by the learned AIL descrip-
tiond, i.e.I⊆/llbracketd/rrbracket.
disjointness: The request patterns of dmust be disjoint.
precision: dshould be as close to Ias possible, i.e. /llbracketd/rrbracket\Ishould
be small. We say that d1ismore precise thand2iff/llbracketd1/rrbracket\I⊆
/llbracketd2/rrbracket\I.
conciseness: dshould be small. We say that d1ismore concise
thand2iff|d1| ≤|d2|, where|d|denotes some appropriate
notion of the size of an AIL description d.
With these properties in mind, we devise an algorithm to lear n
AIL descriptions from input samples. Our algorithm has two p hases:
data clustering andpattern generation . The data clustering phase is
the key step, organizing the input samples into distinct clu sters such
that each cluster corresponds to a “likely” operation descr iptor, and
these together form an AIL description with the aforementio ned
properties. Once the appropriate clustering has been deter mined,
the pattern generation phase transforms the clusters into a ctual AIL
descriptions and JSON schemas. This last step is straightfo rward
and will not be described in this paper due to space constrain ts.
For the clustering phase we make two observations. First, id enti-
fying responses that are structurally similar can be a good s tarting
point. For example, two JSON values that have the same objectstructure but contain different strings or numbers can be co nsid-
ered “similar” and hence likely belong to the same operation . Sec-
ond, we can infer important information for clustering from the
path fragments and parameters that occur in the request data . As an
example, consider requests to the ﬁrst operation from Figur e 4:
POST comment/delete/comment_id:*(): @delete.json
A request consists of path fragments and GET/POST parameter s,
which we will denote features. The features for this operati on are
three path fragments, i.e. the constant strings comment anddelete
and a comment ID value. These can be divided into key features ,
which are characterized by having relatively few possible v alues
that together identify the operation for the request, and non-key fea-
tures , with a higher number of possible values, which do not iden-
tify operations. For this particular operation, the key fea tures are
the ﬁrst two, i.e.comment anddelete , and we can expect that our
sample data will contain a higher number of comment ID values
than the number of distinct operations.
These observations motivate us to further divide the cluste ring
phase into two steps: (1) construct an initial clustering by consid-
ering only the response data and grouping the responses into dis-
tinct clusters with respect to their response types (Sectio n 4.1); and
(2) restructure the clustering using request data by identi fying the
likely key features (Section 4.2). After the clustering pha se, we
construct AIL descriptions that satisfy the completeness p roperty
by ensuring that each sample is associated with a cluster and giv-
ing the cluster a request pattern and a response pattern that match
all samples in the cluster.
4.1 Response Data Clustering
We ﬁrst cluster the input set Iusing HTTP response data. Al-
though AIL can describe both XML and JSON data, we describe
our algorithm for JSON, which is the most widely used data int er-
change format for Ajax web applications. A JSON response is a
JavaScript data structure containing primitive values (st rings, num-
bers, booleans, and null), objects, and arrays.
For each request and response pair /a\}bracketle{tr,s/a\}bracketri}ht∈I, the response s
contains JSON data. We map sto its type abstraction :
•aprimitive value is mapped to its respective primitive type (e.g.
String ,Number ,Boolean , orNull );
•anobject value{p1:v1,...,pk:vk}is mapped to a record type
{p1:t1, ..., p k:tk}by replacing each object property value
with its type, where tidenotes the type of the value vi; and
•anarray[v1,...,vk]is mapped to a union type ∪k
i=1ti, where
tidenotes the type of vi.
We now cluster all sample pairs from Iaccording to structural
equivalence of the response type abstractions. For example , the ﬁve
sample responses shown in Figure 6 will be clustered togethe r into
three clusters. The ﬁrst two samples have the same type abstr action
{id:Number,name:String,stories :Number}and are
thus grouped together, while the next two contains an additi onal
property, resulting in the type abstraction {id:Number,name:
String,email:String,stories :Number}and their own
cluster. Similarly, the type abstraction of the last respon se{id:
Number,title:String}does not match the ﬁrst or the second
cluster, so it will be placed in a third cluster.
4.2 Request Data Clustering
Using the distinction between key and non-key features, we w ant
our learning algorithm to construct request patterns in whi ch key
features are represented using constant strings, and non-k ey fea-
tures are represented using wildcards. However, deciding o n the
division between key and non-key features may require restr ucture
of the clustering to ensure that the disjointness property i s satisﬁed./angbracketleftGET author?name=alice,
{"id": 1, "name": "Alice", "stories":10} /angbracketright
/angbracketleftGET author?name=bob,
{"id": 2, "name": "Bob", "stories":12} /angbracketright
/angbracketleftGET author?name=charlie,
{"id": 3, "name": "Charlie",
"email": "charlie@example.org","stories":1} /angbracketright
/angbracketleftGET author?name=eve,
{"id": 3, "name": "Eve",
"email": "eve@example.org","stories": 1} /angbracketright
/angbracketleftPOST news/read,
[{"id": 1, "title": "News 1"},
{"id": 2, "title": "News 2"}] /angbracketright
Figure 6: Example request and JSON response pairs, /a\}bracketle{tr,s/a\}bracketri}ht, for
two different operations.
In the example shown in Figure 6, the ﬁrst four responses are i ni-
tially put into two clusters. If the name parameter is classiﬁed as
a key feature, then we need to split the two clusters into four , one
for each sample. On the other hand, if it is classiﬁed as a non- key
feature, then we need to merge the two clusters into one to ens ure
disjointness. To generate the desired request patterns usi ng con-
stant strings and wildcards, this example shows that we may n eed
tomerge clusters together, using a wildcard, or split them into sep-
arate clusters, using constant strings.
To describe in more detail how we merge and split clusters, we
ﬁrst introduce some additional terminology. As stated, eac h path
fragment and parameter of a request is a feature . The set of fea-
tures in a request forms its signature , denoted by S. As an exam-
ple, a request with URL foo/bar and a parameterbaz=1 has the
signature{#0,#1,#baz}where path fragments are identiﬁed by
their positions in the URL and parameters are identiﬁed by th eir
names. Since we wish to construct one request pattern for eac h
cluster, we ﬁrst split clusters that contain requests with d ifferent
signatures. Request patterns that are constructed from clu sters with
different signatures are trivially disjoint. Next, we need to decide
on a suitable partition of Sinto key and non-key features, corre-
sponding to constant strings and wildcards, respectively.
There are two obvious extremes when selecting the partition : (1)
assign wildcards to all features, thereby merging all clust ers with
the same signature into a single one, which is likely to be hig hly im-
precise, and (2) assign constant strings to all features, th ereby split-
ting all clusters into singletons (i.e. simply the input set I), which
is neither concise nor very useful. These two extremes relat e to op-
eration descriptions being concise and precise respective ly, which
are conﬂicting requirements that we must reconcile.
Our algorithm is given in Figure 7. Let Dbe initial response
data clustering Dfrom Section 4.1. For each signature SinD,
the algorithm iterates over all clusters D′that match S. It then
iterates over all possible partitions ρthat divide Sinto key and
non-key features, selecting the partition with the minimal cost with
respect to a cost function C. This partition is used to restructure
the clusters D. The end result, after iterating over all signatures, is
Drestructured in accordance with its request data. What rema ins
is to deﬁne the cost function C(ρ,D′), whereρis a partition of S
into key and non-key features and D′is a set of clusters with the
same signature.
Recall our observation that clustering based on response ty pes
typically yields a good baseline clustering. Thus, we favor parti-
tions that result in the smallest number of splits and merges com-
pared to the baseline clustering. This strategy is further s upported
by the other observation that key features have few unique va lues,
so our goal is to ﬁnd a partition that leads to the smallest num ber of
splits and merges.function REQUEST DATACLUSTERING (D)
for allSinSIGNATURES (D)do
D′←FIND CLUSTERS WITHSIGNATURE (D,S)
cmin←∞
ρmin←null
for allρinPARTITIONS (S)do
c←C(ρ,D′)
ifc < cminthen
cmin←c
ρmin←ρ
end if
end for
RESTRUCTURE (D,ρmin)
end for
end function
Figure 7: The request data clustering algorithm.
We deﬁne the cost C(ρ,D′)as the total number of splits and
merges necessary to get from D′to the restructured clustering. In-
tuitively, a least cost partition helps avoid merging too mu ch, for
precision, and avoid splitting too much, for conciseness. I n case of
a tie, we choose a partition that minimizes the number of wild cards.
To illustrate the cost calculation, consider the two initia l clusters
that were created from the ﬁrst four sample responses in Figu re 6.
Those two clusters are a result of different response struct ures, how-
ever, we cannot ensure disjointness of the request patterns without a
reorganization. Both clusters have the signatures S={#0,#name}
corresponding to the author URL path fragment and the name pa-
rameter. The cost function is applied to all possible partit ionsρof
S, in this example the following four partitions:
1. Neither#0nor#name is considered a key feature, causing the
two clusters to be merged at a cost of 1 into a cluster with
request pattern*?name=* .
2. Only#0is a key feature, which also causes a single merge
operation, hence the cost is 1, but the resulting cluster now
has request pattern author?name=* .
3. Only#name is a key feature, which means that the two clus-
ters are split into four singleton clusters at a total cost of 2,
resulting in the four request patterns *?name=alice ,
*?name=bob ,*?name=charlie , and*?name=eve .
4. Both#0and#name are key features, which also results in
four singleton clusters at a total cost of 2, but the request
patterns are nowauthor?name=alice ,author?name=bob ,
author?name=charlie , andauthor?name=eve .
We choose the second partition since it has minimal cost and m ini-
mal number of wildcards.
Finally, we have constructed clusters with the desired prop erties.
Each cluster can be turned into an AIL operation descriptor, as
hinted earlier. Its request pattern is generated from the em ployed
partitionρ, and JSON schemas for the response patterns are gen-
erated from the type abstractions of the response samples in the
cluster. The close connection between JSON schemas and the t ype
abstraction we uses for response data leads to a straightfor ward con-
struction.
5. EV ALUATION
We have argued that server interface descriptions can provi de
separation of concerns, which enables testing of JavaScrip t code in
isolation from the server code. When conducting automated t esting
of the JavaScript code, the use of AIL and a mock server remove s
the burden of setting up actual servers with appropriate dat abase
contents. To ﬁnd out how this may inﬂuence other aspects of au to-
mated testing, we ﬁrst consider the following research ques tions:Client Server
Benchmark LOC Framework Language
simpleajax 79 jQuery Python (Django)
resume 244 Flapjax Python
globetrotter 347 jQuery Java (JWIG)
impresspages 558 jQuery PHP
buggenie 3,716 Prototype PHP
elﬁnder 6,724 jQuery PHP
tomatocart 8,817 Prototype PHP
eyeos 17,629 jQuery PHP
Figure 8: Benchmark applications.
Q1. How is the running time of automated testing affected whe n
replacing the real server by the mock server for a ﬁxed num-
ber of test inputs?
Q2. Does the use of AIL in place of live servers affect the code
coverage obtained by automated testing?
To evaluate how our learning algorithm from Section 4 can be u se-
ful when creating AIL descriptions for existing applicatio ns, we
consider two additional research questions:
Q3. To what extent is the learning algorithm capable of produ cing
AIL descriptions that correctly describe the servers in act ual
JavaScript web applications?
Q4. How much effort is required for producing request and re-
sponse data for the learning algorithm, and how fast is the
learning algorithm?
To answer these questions we have implemented three tools:9
(1) a web proxy for recording the HTTP communication between
clients and servers, (2) the learning algorithm from Sectio n 4, which
reads the data recorded by the web proxy and outputs AIL descr ip-
tions and JSON schemas, and (3) the AIL mock server for Artemi s.
We have collected 8 benchmark applications that use JavaScr ipt
for their client-side logic and Ajax for communicating betw een the
client and the server, and where the source code for the entir e appli-
cation has been available, including the server code: simpleajax is
a small home-built test application for event registration s;resume10
is an application management system; globetrotter11is a travel ap-
plication system; impresspages12is a CMS system; elﬁnder13is
an online ﬁle explorer; buggenie8is a project management tool
that we also used as example in Section 2; tomatocart14is an e-
commerce platform; and eyeos15is an online desktop environment.
Figure 8 contains a list of the applications together with th e num-
ber of lines of JavaScript code (excluding frameworks), the frame-
work they use for JavaScript if any, and the language or frame work
used on the server side.
Our experiments are performed on a 3.1GHz i5 machine with
4GB of memory.
5.1 Using AIL in Automated Testing
To be able to answer Q1 and Q2 we run Artemis on our bench-
mark applications using various conﬁgurations: EmptyDB with real
servers but with empty databases, FullDB with real servers where
the databases are populated with realistic data, Random with a fully
generic mock server that accepts all requests and produces r andom
JSON responses, and AILwith the mock server using the AIL de-
scription.
9Our tools are available at http://www.brics.dk/artemis/
10old version ofhttps://resume.cs.brown.edu/cs/
11https://services.brics.dk/java/globetrotter/
12http://www.impresspages.org/
13http://elrte.org/elfinder
14http://www.tomatocart.com/
15http://eyeos.org/Benchmark AIL FullDB Init EmptyDB FullDB Random AIL
simpleajax 25s 26s 22 30 62 60 62
resume 67s 77s 12 105 108 14 113
globetrotter 102s 94s 10 - 180 17 205
buggenie 104s 180s 662 - 1,322 1,138 1,308
elﬁnder 162s 152s 571 1,236 1,337 665 1,366
Figure 9: Execution time for Artemis with a budget of 100 test
input executions, and code coverage obtained by Artemis wit h
a budget of 300 test inputs.
The database contents used in the FullDB conﬁguration are se -
lected as snapshots obtained when we exercised the applicat ions
to collect sample request and response pairs. For the AIL con ﬁg-
uration, we use manually constructed AIL descriptions, or e quiva-
lently, descriptions that were produced by the automated le arning
and subsequently manually adjusted to properly model the se rvers.
Three of the larger benchmark applications are unfortunate ly be-
yond the capabilities of the latest version of Artemis for re asons
that are unrelated to AIL and Ajax communication, so our expe ri-
ments are conducted on the remaining ﬁve applications.
The execution time of Artemis depends on a number of factors,
one of course being the time budget, which is expressed as a ma x-
imum number of test input executions. Other factors are the s pe-
ciﬁc data that the JavaScript application receives from the server
in Ajax interactions and the response time of the server. Rep lac-
ing the live server with a mock server affects the two latter f actors.
Responses that are randomly generated from the AIL response pat-
terns may trigger long running loops in the JavaScript code, how-
ever, the work performed by the mock server is presumably sim pler
than that of the real server in most cases.
The ﬁrst columns in Figure 9 show the total running time of
Artemis with the two conﬁgurations AIL and FullDB using a bud -
get of 100 test input executions for each application. This g ives
an answer to Q1: for these applications, the running time is n ot
affected notably by the AIL mock server.
The remaining columns show the code coverage (measured as
number of lines of JavaScript code) for 300 test input execut ions
of each application using all four conﬁgurations. The extra col-
umn, Init, shows the coverage obtained by loading the applic ation
without triggering any events, which can be viewed as a basel ine
for the coverage comparisons. The globetrotter andbuggenie ap-
plications have not been tested with empty databases since t his did
not make sense for those cases. (Please note that the LOC colu mn
in Figure 8 should not be compared with the coverage numbers i n
Figure 9, since the latter only include lines with executabl e code.)
We observe that the use of the AIL mock server yields similar
coverage results as when using the real servers populated wi th real-
istic data, which partially answers Q2.
Forglobetrotter ,elﬁnder , and resume , coverage is slightly im-
proved when using AIL. In each case, the increased coverage i s
caused by conditions in the JavaScript code that are only tri ggered
with speciﬁc Ajax response data, for example an empty array o r a
certain boolean value somewhere in a JSON structure. These a re ex-
amples of application behavior that depend on the precise co ntents
of the server database, as discussed in Section 3. In globetrotter ,
for example, the program state describes a travel applicati on that
can be at different workﬂow stages. The mock server quickly g en-
erates JSON responses that cover all the workﬂow stages, whi le the
FullDB conﬁguration only manages to cover a single one.
The lower code coverage for buggenie is caused by an animation
not being triggered in the AIL conﬁguration due to the heuris tics
used internally by Artemis. For elﬁnder , a few lines are reached
with FullDB but not with the AIL conﬁguration. The data in thi s
application contains a tree-like structure of ﬁles and dire ctories that
are linked through hash and parent hash values that refer to e achother. This invariant cannot be expressed with the current d esign of
AIL, so the mock server is not able to produce the right respon se.
Several additional observations can be made from the covera ge
numbers. The EmptyDB, FullDB and AIL measurements show
higher coverage than Init, demonstrating that we actually t est addi-
tional functionality besides simply loading the page. Inte restingly,
the Random measurements for resume ,globetrotter , and elﬁnder
show considerably less coverage, which demonstrates that m ean-
ingful response data is important. In all cases, this is caus ed by the
initialization of the web applications depending on correc tly struc-
tured Ajax responses. As expected, populating the database (i.e.
FullDB) results in higher or equal coverage than using the em pty
database (i.e. EmptyDB).
Although we did not expect to ﬁnd bugs in the benchmark ap-
plications, the use of the AIL mock server revealed one in resume
that was not found with any of the other conﬁgurations. The bu g
is triggered by a sequence of events that involve sending an e mpty
array to the server and back to the client ending up in obj.values
at the following snippet of code where it leads to a runtime er ror:
varln =
A({href:’javascript:undefined’},
’’+obj.values[0][’number’]+’ - ’
+obj.values[obj.values.length-1][’number’]);
This example illustrates how unclear assumptions between c lient
and server developers can end up creating errors in the appli cations.
In other situations, similar unclear assumptions do not cau se er-
rors but lead to fragile code that may break in future revisio ns made
by programmers who are not aware of subtle invariants that mu st
be satisﬁed. The use of AIL in Artemis also revealed such a sit -
uation. The elﬁnder application contains the following snippet of
code wheredirandfiles originate from an Ajax response:
while(dir && dir.phash) {
dir = files[dir.phash]
}
The purpose of this code is to traverse a directory structure where
ﬁles are represented in an array indexed by ﬁle hash values. R un-
ning Artemis with the AIL conﬁguration discovered that if th is data
structure contains loops then the while loop never terminates. The
required invariant—that the data structure sent in the Ajax response
never contains such loops—is not documented in the applicat ion
source code. AIL is not expressive enough to capture such inv ari-
ants, but one could argue anyway that the application would b e
more robust if its correctness did not depend on such intrica te in-
variants involving the server state.
Concluding these experiments, our answer to Q2 is that the us e
of AIL leads to good coverage compared to using a server with
an empty database, a server with a populated database, or a mo ck
server that generates random responses. The experiments al so
pointed us to examples where correctness of the application s de-
pends on subtle, undocumented invariants.
5.2 Automated Learning of AIL Descriptions
To obtain the training data for the learning algorithm, we in stall
and exercise each application by manually clicking on visua l ele-
ments and entering data into forms for a few minutes, while th e
web proxy monitors all Ajax communication. This is done with out
detailed knowledge of each application and entirely withou t look-
ing at the server code. This gives us between 70 and 611 sample
request and response pairs, depending on the amount of infor ma-
tion exchanged.
We now run the learning algorithm on the data obtained for eac h
application, which in each case takes less than a second. The re-
quest data clustering process described in Section 4.2 perf orms al-Benchmark Samples Sampling Learning Match 1 →N N→1
simpleajax 70 3m 74ms 5 0 0
resume 128 9m 111ms 12 3 0
globetrotter 97 8m 84ms 4 1 0
impresspages 179 6m 224ms 5 1 0
buggenie 210 6m 118ms 4 3 0
elﬁnder 181 6m 124ms 11 3 0
tomatocart 370 8m 153ms 22 6 1
eyeos 611 6m 213ms 22 0 0
Total 85 17 1
Figure 10: Number of sample request and response pairs used
for AIL learning, time used for collecting sample data and
learning AIL descriptions, and results from comparing the
learned AIL descriptions with the manually written ones.
together 18 splits and 43 merges when searching for the parti tions
with the minimal cost. This results in a total of 130 AIL opera tion
descriptors and 9,550 lines of JSON schema—all generated au to-
matically.
Figure 10 shows the amount of sample data, the time used for
collecting the sample data, and the time used by the AIL learn -
ing algorithm for each application. From these numbers we ca n
give a rough answer to Q4: the effort required for using autom ated
AIL learning is clearly manageable, compared to the time oth erwise
spent developing the web applications.
Producing AIL descriptions is of course not enough; they als o
need to capture the actual patterns of the Ajax communicatio n. Re-
call from Section 4 that the constructed AIL description is c omplete
by construction, relative to the training data. However, th e training
data may not cover the entire application, which might resul t in
incomplete AIL descriptions where some operations support ed by
the server are missing in its AIL description. Another poten tial
source of mismatches between automatically constructed AI L de-
scriptions and manually written ones is that the learning al gorithm
may not be sufﬁciently precise or concise (using the termino logy
from Section 4). Furthermore, as there is no canonical “best ” AIL
description for a given Ajax server, we must settle for a subj ec-
tive baseline for comparison, which we decide to construct a s fol-
lows: For each benchmark application, we manually write an A IL
description based on an inspection of the source code for the server
part of the application. This process can take hours, but thi s work
is of course only required to be able to measure the quality of the
learning algorithm in our experiments.
Next, we need a measure of the difference between the automat -
ically constructed AIL descriptions and the manually const ructed
ones. The ﬁrst aspect of this measure is how the individual op era-
tion descriptions match between the two. Figure 10 also show s the
results of this comparison. The Match column counts the numb er
of learned operations that map directly to the actual server opera-
tions, while 1→NandN→1 count the number of server operations
that map to multiple learned operations and vice versa, whic h indi-
cate mismatches between the two descriptions. A second aspe cts is
to what extent the individual datatypes of parameters and re sponse
patterns differ between the two descriptions.
We get a total of 85 matches, 17 occurrences of 1 →N, and a
singleN→1. The high number of matches is already encouraging,
but a closer inspection of the other cases reveal that they ar e rel-
atively benign. In all the 1 →Ncases, a simple manual merging
of the relevant operation descriptors sufﬁces to resolve th e discrep-
ancy. This is acceptable since the learned AIL description i s only
intended as a starting point for the programmer, as an altern ative to
writing the AIL description from scratch. An example is delete
operation in resume , which can be called both with and without an
idparameter resulting in different responses, causing the le arning
algorithm to produce two separate AIL operation descriptor s. An-other example of a 1 →Ncase appears in buggenie . In this case,
a speciﬁc server operation runIssueRevertField performs mul-
tiple tasks and dispatches internally, based on a parameter field ,
in a way where one may argue that the AIL description produced
by the learning algorithm, where these sub-operations are d ivided
into separate descriptors, is in fact just as good as the manu ally
constructed one.
The single N→1 case appears in tomatocart and is caused by our
merging heuristic being slightly too aggressive. Two opera tions for
deleting images and setting default images, respectively, both take
anidparameter and return a trivial response, and the operations
are only distinguished by the value of an action parameter. The
similarity of the data causes the two operations to be merged by our
current heuristic.
Regarding the quality of the inferred datatypes in request a nd re-
sponse patterns, we notice that many of our benchmarks use JS ON
in responses but not in requests. For request patterns, the m ain ques-
tion then is whether wildcards are introduced appropriatel y. The
learning algorithm needs at least two distinct values of a pa th frag-
ment or parameter to conclude that it is not constant. For exa mple,
resume represents session IDs in parameters, so the training data
must involve multiple sessions. Incompleteness of our samp le data
in some cases results in missing wildcards, however, this is easy to
adjust manually after the learning phase.
Most JSON response data in the benchmark applications is bui lt
using arrays and simple objects with ﬁxed collections of pro per-
ties. For these common cases the learning algorithm is able t o
generate JSON schemas correctly. Differences between the J SON
schemas constructed by the learning algorithm and the manua lly
constructed ones are mostly due to incomplete sample data. H ow-
ever, we observed two interesting cases—in impresspages andglo-
betrotter , respectively—that could be improved. Some responses in
impresspages have a recursive structure of objects and arrays. More
speciﬁcally, the data represents a list of page and director y objects
where each directory object itself contains a list of page an d direc-
tory objects. Our current learning algorithm is not able to p roduce
the desired concise JSON schema. In globetrotter , a speciﬁc JSON
structure contains information about a list of countries. E ach coun-
try is represented by an object where the country name appear s as
a property name, not as a property value, which causes the lea rning
algorithm to view each country object as being distinct.
Based on these experiments, our answer to Q3 is that the learn ing
algorithm is able to produce AIL descriptions that are reaso nably
close to manually constructed ones. This suggests that auto mated
learning can be a good starting point for creating AIL descri ptions
for pre-existing applications, and that sufﬁcient sample d ata can be
obtained by someone who is familiar with the functionality o f the
applications but does not have knowledge of the server code.
5.3 Threats to Validity
A possible threat to validity of our experimental results is that
the manually constructed AIL descriptions that we use as bas eline
for the comparisons have been made by ourselves without expe rt
knowledge of most of the benchmark applications. More solid re-
sults could perhaps be obtained by performing user studies w ith the
developers of the applications. Also, our benchmark applic ations
do not reﬂect all possible uses of Ajax and JSON, and they may n ot
be representative of typical usage patterns although we hav e striven
toward including a wide variety of applications.
6. RELATED WORK
Our work touches on several areas of work on interface descri p-
tions, automated testing of web applications, and learning algo-
rithms.6.1 Interface Descriptions for
Separation of Concerns
The idea of design-by-contract is a fundamental principle i n mod-
ern software engineering for separating the concerns of ind ividual
software components. Even in web programming, which often
involves dynamic scripting languages both on the client and the
server, interface description languages play an important role: Sim-
ilar to AIL, WSDL [8] allows description of operations and th eir
argument and return types, however, WSDL is tailored to XML-
based web services and has no support for JSON, and we are not
aware of uses of WSDL for describing server interfaces in Aja x-
style JavaScript web applications. As mentioned in Section 2, AIL
is by design conceptually closer to the language WADL [13], a l-
though AIL has a compact non-XML syntax and supports JSON.
The Web IDL language is used for describing the API that web
browsers provide to JavaScript programs [10] for accessing the
HTML DOM and other parts of the browser state, however, unlik e
AIL, each Web IDL description is common to all JavaScript web
applications and cannot describe the interfaces of individ ual Ajax
servers. Web IDL has its roots in the OMG IDL interface deﬁnit ion
language for CORBA [24].
Interface descriptions have also been proposed for HTML for m-
based web applications without JavaScript. The WebAppSleu th
methodology by Fisher et al. [18] works by submitting reques ts to
a server and analyzing the responses to infer parts of its int erface.
The resulting interface descriptions are related to AIL des criptions
but tailored to HTML forms, not JSON or XML. Each form is de-
scribed by its set of mandatory and optional ﬁelds together w ith
simple value constraints and dependencies between the ﬁeld s.
The extensive survey by Alalﬁ et al. [1] covers many model-
ing methods used in web site veriﬁcation and testing, but wit hout
JavaScript and Ajax. To our knowledge, the only existing wor k in-
volving interface descriptions for Ajax communication is t hat by
Hallé et al. [16]. They propose a contract language based on i n-
terface grammars, linear temporal logic, and XPath express ions for
specifying the order of HTTP interactions that exchange XML data
in long-running sessions. We believe the data formats of req uests
and responses are more important in typical Ajax applicatio ns than
restrictions on the order of operations, so we have chosen to ignore
the temporal aspect in our ﬁrst version of AIL. Their paper di s-
cusses how the contracts can be used for runtime monitoring. They
also ask the important question “who should write the contracts?”
To this end, we take the approach of using machine learning on
sample execution traces, as explained in Section 4.
A range of well-documented web services that ﬁt into the desi gn
of AIL can be found at Google’s APIs Explorer website.16The in-
terface descriptions for those web services are only made av ailable
as online documentation for programmers, not using any inte rface
description language, which makes them less accessible to, for ex-
ample, automated testing tools.
6.2 Automated Testing of Web Applications
Besides the Artemis tool [2] that we discussed in Section 3.1 , we
are aware of a few other tools for automatically testing Java Script
web applications. The Kudzu tool by Saxena et al. [26] perfor ms
automated testing by a combination of symbolic execution wi th a
string constraint solver for value space exploration and ra ndom ex-
ploration of the event space, whereas Artemis uses a more lig ht-
weight feedback-directed approach. The state-based testi ng tech-
nique by Marchetto et al. [20, 21] builds ﬁnite-state machin es that
model Ajax web pages from concrete execution traces. As in ou r
approach, a subsequent manual validation or reﬁnement step is
16http://code.google.com/apis/explorer/required to ensure that the extracted model is correct befor e the
model can be used for automated testing. The key difference t o
our approach is that the models in state-based testing descr ibe the
DOM states of the JavaScript execution, not the interaction s with
the server. A closely related tool is Crawljax by Mesbah et al . [22,
23] that also aims to derive models of the user interface stat es of
Ajax applications and use these models as a foundation for te st-
ing. AJAX Crawl by Duda et al. [9] similarly performs dynamic
exploration of Ajax applications, but for the purpose of cra wling
the applications by search engines, not aiming at testing.
A common limitation of Kudzu, Crawljax, and AJAX Crawl
is that the exploration of the JavaScript applications is do ne with
little focus on the client-server communication, simply us ing live
servers, which leads to the problems discussed in Section 3. 1 about
how to properly populate the server databases. On top of this ,
most tools, with Artemis as an exception, do not restore the s erver
database state after each test input execution, which affec ts testing
reproducibility.
The JSConTest tool by Heidegger and Thiemann performs ran-
dom testing for JavaScript programs that are annotated with type
contracts [17]. These function annotations play a similar r ole as
AIL descriptions, but at the level of function calls rather t han Ajax
client-server interactions. Due to the JavaScript-orient ed design
of JSON Schema that we use in AIL, it is natural that the basic
contract language in JSConTest has similar expressiveness . How-
ever, JSConTest also supports function types, which are not rele-
vant for client-server exchange of JSON or XML data. Another
difference is that JSConTest permits user-deﬁned contract s written
in JavaScript, which might be useful to consider for a future version
of AIL to address the limitations identiﬁed in Section 5.
Several tools have been developed for automatically testin g server-
based web applications. The Apollo tool by Artzi et al. [3] an d the
tool by Wasserman et al. [27] perform directed automated ran dom
testing for PHP code, but JavaScript is not considered. With our
proposal of using a server interface description language f or sepa-
rating the concerns of server code and client code, we have so far
focused on testing the client code, but an interesting direc tion of fu-
ture work is to develop testing or analysis techniques that c an also
check whether the servers fulﬁll their part of the contracts .
Elbaum et al. [11] have proposed the use of user session data f or
black-box testing of web applications. They record concret e user
sessions and replay the sessions in various ways to test the s erver
code, not aiming for testing client code and not involving ex plicit
server interface descriptions.
The WAM tool by Halfond and Orso [14, 15] automatically dis-
covers interfaces of web applications using static analysi s or sym-
bolic execution of the server code. The interfaces are subse quently
used in automated testing, similar to our approach, althoug h WAM
considers classical web applications without Ajax and JSON . The
notion of interfaces used by WAM is similar to that in WebApp-
Sleuth. WAM is restricted to Java Servlets, unlike our appro ach,
which is in principle independent of the languages and frame works
used on the server.
6.3 Learning Algorithms
The learning algorithm presented in Section 4 has been devel -
oped speciﬁcally for AIL, but related algorithms exist for o ther do-
mains.
WebAppSleuth [18], which we also mentioned in Section 6.1,
uses learning techniques to identify interfaces of server- based web
applications that receive HTML forms. That approach does no t in-
volve learning the structure of server response data, and a s ingle
server operation is considered at a time, while our learning algo-
rithm needs to work for multiple operations.The latest version of WAM [14] likewise uses a learning algo-
rithm to produce interface descriptions. The WAM algorithm op-
erates on path constraints constructed through symbolic ex ecution
of the server code, which differs from our learning algorith m that
is based on sample request and response data and has a black-b ox
view on the server. Furthermore, WAM does not consider respo nse
types, unlike our learning algorithm.
The clustering problem that we face in Section 4 is related to
the work by Broder et al. on clustering web pages that are synt ac-
tically similar [5]. Their approach is to deﬁne a distance me asure
between two web pages, using a distance threshold to cluster sim-
ilar pages. This approach could be transferred to JSON respo nses
and our learning algorithm, but we found the results from ini tially
clustering only entirely equal structures to be sufﬁcient f or our pur-
poses.
We are not aware of existing work on JSON Schema inference.
The closest related work has been centered around DTD and XML
Schema inference. This problem is described by Chidlovskii as
being reducible to grammar inference [7]. Others improve on this
line of work [4], however, the differences between XML and JS ON
make their algorithms unsuitable for JSON Schema.
7. CONCLUSION
Server interface descriptions for Ajax-style web applicat ions en-
able separation of concerns of the server code and the client code.
The AIL language has been designed to capture the essence of t he
existing proposals WADL and allow concise description of th e ba-
sic properties of server operations, in particular involvi ng JSON
data. Our experimental validation suggests that the expres siveness
of AIL sufﬁces for typical Ajax communication patterns, but also
that it might be useful in future work to add support for user- deﬁned
contracts to specify more ﬁne-grained invariants.
One key contribution is that we demonstrate that server inte rface
descriptions are useful in automated testing. No previous w ork has
combined server interface descriptions with testing of Aja x appli-
cations. Our experimental results show that this approach c an im-
prove the level of automation by eliminating the need for car efully
populated databases on the servers, while maintaining the q uality
of the testing of the client code. Another key contribution o f our
work is the automated learning algorithm that can produce se rver
interface descriptions from sample request and response da ta. The
experiments show that AIL learning can be performed with a mo d-
est effort, and that the resulting descriptions are a good st arting
point when programmers wish to construct AIL descriptions f or
pre-existing web applications.
In addition to the suggestions about possible extensions of AIL,
several directions of future work appear. As an alternative or sup-
plement to our AIL learning approach that has a black-box vie w
on the server, it would be interesting to infer or validate AI L de-
scriptions by static or dynamic analysis of the server code f or the
most widely used server web frameworks. Additionally, AIL m ay
also be useful for static analysis of JavaScript applicatio ns to enable
more precise reasoning of Ajax interactions than currently possible.
Speciﬁcally, we wish to incorporate AIL into the JavaScript analy-
sis tool TAJS [19].
Acknowledgements
This work was supported by Google, IBM, and The Danish Re-
search Council for Technology and Production. The last auth or
acknowledges partial support from U.S. NSF grants 0917392 a nd
1117603. We thank Simon Holm Jensen and Kristoffer Just Ande r-
sen for their contributions to the Artemis tool used in the ex peri-
mental evaluation.8. REFERENCES
[1] M. H. Alalﬁ, J. R. Cordy, and T. R. Dean. Modelling
methods for web application veriﬁcation and testing: state of
the art. Software Testing, Veriﬁcation & Reliability , 19(4):
265–296, 2009.
[2] S. Artzi, J. Dolby, S. H. Jensen, A. Møller, and F. Tip. A
framework for automated testing of JavaScript web
applications. In Proc. 33rd International Conference on
Software Engineering , May 2011.
[3] S. Artzi, A. Kiezun, J. Dolby, F. Tip, D. Dig, A. M. Paradka r,
and M. D. Ernst. Finding bugs in web applications using
dynamic test generation and explicit-state model checking .
IEEE Transactions on Software Engineering , 36(4):474–494,
2010.
[4] G. J. Bex, W. Gelade, F. Neven, and S. Vansummeren.
Learning deterministic regular expressions for the infere nce
of schemas from XML data. In Proc. 17th International
Conference on World Wide Web , 2008.
[5] A. Z. Broder, S. C. Glassman, M. S. Manasse, and G. Zweig.
Syntactic clustering of the web. Computer Networks , 29
(8-13):1157–1166, 1997.
[6] D. Chays, S. Dan, P. G. Frankl, F. I. V okolos, and E. J. Webe r.
A framework for testing database applications. In Proc. ACM
SIGSOFT International Symposium on Software Testing and
Analysis , August 2000.
[7] B. Chidlovskii. Schema extraction from XML: A
grammatical inference approach. In Proc. 8th International
Workshop on Knowledge Representation meets Databases ,
2001.
[8] R. Chinnici, J.-J. Moreau, A. Ryman, and S. W. (editors).
Web services description language (WSDL) version 2.0, June
2007. W3C Recommendation.
http://www.w3.org/TR/wsdl20/ .
[9] C. Duda, G. Frey, D. Kossmann, R. Matter, and C. Zhou.
AJAX Crawl: Making AJAX applications searchable. In
Proc. 25th International Conference on Data Engineering .
IEEE, April 2009.
[10] C. M. (editor). Web IDL, February 2012. W3C Working
Draft.http://www.w3.org/TR/WebIDL/ .
[11] S. G. Elbaum, G. Rothermel, S. Karre, and M. F. II.
Leveraging user-session data to support web application
testing. IEEE Transactions on Software Engineering , 31(3):
187–202, 2005.
[12] J. J. Garrett. Ajax: A new approach to web applications,
2005.http://www.adaptivepath.com/ideas/ajax-new-
approach-web-applications .
[13] M. Hadley. Web application description language, Augu st
2009. W3C Member Submission.
http://www.w3.org/Submission/wadl/ .
[14] W. G. J. Halfond, S. Anand, and A. Orso. Precise interfac e
identiﬁcation to improve testing and analysis of web
applications. In Proc. International Symposium on Software
Testing and Analysis . ACM, July 2009.[15] W. G. J. Halfond and A. Orso. Improving test case
generation for web applications using automated interface
discovery. In Proc. 6th joint meeting of the European
Software Engineering Conference and the ACM SIGSOFT
International Symposium on Foundations of Software
Engineering , September 2007.
[16] S. Hallé, T. Bultan, G. Hughes, M. Alkhalaf, and
R. Villemaire. Runtime veriﬁcation of web service interfac e
contracts. IEEE Computer , 43(3):59–66, 2010.
[17] P. Heidegger and P. Thiemann. Contract-driven testing of
JavaScript code. In Proc. 48th International Conference on
Objects, Models, Components, Patterns , June 2010.
[18] M. F. II, S. G. Elbaum, and G. Rothermel. Dynamic
characterization of web application interfaces. In 10th
International Conference on Fundamental Approaches to
Software Engineering , March 2007.
[19] S. H. Jensen, M. Madsen, and A. Møller. Modeling the
HTML DOM and browser API in static analysis of
JavaScript web applications. In Proc. 8th joint meeting of the
European Software Engineering Conference and the ACM
SIGSOFT Symposium on the Foundations of Software
Engineering , September 2011.
[20] A. Marchetto, F. Ricca, and P. Tonella. A case study-bas ed
comparison of web testing techniques applied to AJAX web
applications. International Journal on Software Tools for
Technology Transfer , 10(6):477–492, 2008.
[21] A. Marchetto, P. Tonella, and F. Ricca. State-based tes ting of
Ajax web applications. In Proc. 1st International Conference
on Software Testing, Veriﬁcation, and Validation , April 2008.
[22] A. Mesbah, A. van Deursen, and S. Lenselink. Crawling
Ajax-based web applications through dynamic analysis of
user interface state changes. ACM Transactions on the Web ,
6(1):3:1–3:30, 2012.
[23] A. Mesbah, A. van Deursen, and D. Roest. Invariant-base d
automatic testing of modern web applications. IEEE
Transactions on Software Engineering , 38(1):35–53, 2012.
[24] Object Management Group, Inc. Common object request
broker architecture (CORBA) speciﬁcation, version 3.2,
November 2011.
http://www.omg.org/spec/CORBA/3.2/Interfaces/PDF .
[25] B. Petrov, M. Vechev, M. Sridharan, and J. Dolby. Race
detection for web applications. In Proc. ACM SIGPLAN
Conference on Programming Language Design and
Implementation , 2012.
[26] P. Saxena, D. Akhawe, S. Hanna, S. McCamant, D. Song,
and F. Mao. A symbolic execution framework for JavaScript.
InProc. 31st IEEE Symposium on Security and Privacy , May
2010.
[27] G. Wassermann, D. Yu, A. Chander, D. Dhurjati, H. Inamur a,
and Z. Su. Dynamic test input generation for web
applications. In Proc. ACM/SIGSOFT International
Symposium on Software Testing and Analysis , July 2008.
[28] Y . Zheng, T. Bao, and X. Zhang. Statically locating web
application bugs caused by asynchronous calls. In Proc. 20th
International Conference on World Wide Web , 2011.