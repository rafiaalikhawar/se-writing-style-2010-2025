Has the Bug Really Been Fixed?‚Ä†
Zhongxian Gu Earl T. Barr David J. Hamilton Zhendong Su
Department of Computer Science, University of California at Davis
{zgu,etbarr,davidjh,su}@ucdavis.edu
ABSTRACT
Software has bugs, and Ô¨Åxing those bugs pervades the software
engineering process. It is folklore that bug Ô¨Åxes are often buggy
themselves, resulting in bad Ô¨Åxes , either failing to Ô¨Åx a bug or creat-
ing new bugs. To conÔ¨Årm this folklore, we explored bug databases
of the Ant, AspectJ, and Rhino projects, and found that bad Ô¨Åxes
comprise as much as 9% of all bugs. Thus, detecting and correcting
bad Ô¨Åxes is important for improving the quality and reliability of
software. However, no prior work has systematically considered
thisbad Ô¨Åx problem , which this paper introduces and formalizes. In
particular, the paper formalizes two criteria to determine whether a
Ô¨Åx resolves a bug: coverage anddisruption . The coverage of a Ô¨Åx
measures the extent to which the Ô¨Åx correctly handles all inputs that
may trigger a bug, while disruption measures the deviations from
the program‚Äôs intended behavior after the application of a Ô¨Åx. This
paper also introduces a novel notion of distance-bounded weakest
precondition as the basis for the developed practical techniques to
compute the coverage and disruption of a Ô¨Åx.
To validate our approach, we implemented FIXATION , a proto-
type that automatically detects bad Ô¨Åxes for Java programs. When
it detects a bad Ô¨Åx, FIXATION returns an input that still triggers
the bug or reports a newly introduced bug. Programmers can then
use that bug-triggering input to reÔ¨Åne or reformulate their Ô¨Åx. We
manually extracted Ô¨Åxes drawn from real-world projects and evalu-
ated FIXATION against them: FIXATION successfully detected the
extracted bad Ô¨Åxes.
*This research was supported in part by NSF CAREER Grant No.
0546844, NSF CyberTrust Grant No. 0627749, NSF CCF Grant No.
0702622, and the U.S. Air Force under grant FA9550-07-1-0532.
The information presented here does not necessarily reÔ¨Çect the
position or the policy of the Government and no ofÔ¨Åcial endorsement
should be inferred.
‚Ä†This material is based in part upon work supported by the U.S.
Department of Homeland Security under Grant Award Number
2006-CS-001-000001, under the auspices of the Institute for In-
formation Infrastructure Protection (I3P) research program. The
I3P is managed by Dartmouth College. The views and conclusions
contained in this document are those of the authors and should not
be interpreted as necessarily representing the ofÔ¨Åcial policies, either
expressed or implied, of the U.S. Department of Homeland Security,
the I3P, or Dartmouth College.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proÔ¨Åt or commercial advantage and that copies
bear this notice and the full citation on the Ô¨Årst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciÔ¨Åc
permission and/or a fee.
ICSE ‚Äô10, May 2‚Äì8 2010, Cape Town, South Africa
Copyright 2010 ACM 978-1-60558-719-6/10/05 ...$10.00.Categories and Subject Descriptors
D.2.5 [ Software Engineering ]: Software/Program VeriÔ¨Åcation;
D.2.5 [ Software Engineering ]: Testing and Debugging; D.2.7
[Software Engineering ]: Distribution, Maintenance, and Enhance-
ment; F.3.1 [ Theory of Computation ]: Specifying and Verifying
and Reasoning about Programs
General Terms
Languages, Reliability, VeriÔ¨Åcation
Keywords
Bug Ô¨Åxes, Weakest precondition, Symbolic execution
1. INTRODUCTION
According to IDC [ 21], software maintenance cost $86 billion in
2005, accounting for as much as two-thirds of the overall cost of
software production. Developers spend 50‚Äì80% of their time look-
ing for, understanding, and Ô¨Åxing bugs [ 7]. Fixing bugs correctly
the Ô¨Årst time they are encountered will save money and improve
software quality.
Researchers have paid great attention to detecting and classifying
bugs to ease developers‚Äô work [ 2,9,12,13,14,18,30,35,37]. In
contrast, not much effort has been expended on bug Ô¨Åxes. When
testing a Ô¨Åx, programmers usually rerun a program with the bug-
triggering input. If no error occurs, programmers, hurried and
overburdened as they usually are, often move on to their next task,
thinking they have Ô¨Åxed the bug. Folklore suggests that bug Ô¨Åxes
are frequently bad, either by failing to cover ,i.e.handle, all bug-
triggering inputs, or by introducing disruptions ,i.e.new bugs. A
bad Ô¨Åx can decrease the quality of a program by concealing the
original bug, rendering it more subtle and increasing the cost of its
Ô¨Åx. Thus, detecting and correcting bad Ô¨Åxes as soon as possible is
important for the quality and reliability of software.
To gain insight into the prevalence and characteristics of bad
Ô¨Åxes, we explored the Bugzilla databases [ 5] of the Ant, AspectJ
and Rhino projects under Apache, Eclipse and Mozilla foundations.
At the time of our survey, these databases contained entries to July
2009. In Bugzilla, a bug is reopened if it fails regression testing or a
symptom of the bug recurs in the Ô¨Åeld. We hypothesized that a re-
opened bug might indicate a bad Ô¨Åx. We read the comment histories
of reopened bugs to judge whether or not the bug was reopened due
to a bad Ô¨Åx. Programmers sometimes even admit they committed a
bad Ô¨Åx: in our survey, we found statements like ‚ÄúOh, I am sorry I
didn‚Äôt consider that possibility‚Äù and ‚ÄúOops, missed one code path.‚Äù
Of all reopened bugs, we found that bad Ô¨Åxes account for 66% in
Ant, 73% in AspectJ, and 80% in Rhino. Bugs reopened because
they failed a regression test belong to the disruption dimension of
55a bad Ô¨Åx. In our preliminary Ô¨Åndings, reopened bugs comprise
4‚Äì7.25% of allbugs in these three projects1. We also found that
38‚Äì50% of bugs reopened due to a bad Ô¨Åx either had duplicates
or blocked other bugs and were therefore linked to other bugs in
Bugzilla. We found a total of 1;977bad Ô¨Åxes from reopened bugs2,
and an additional 830duplicates ( i.e.bugs marked as a duplicate of
a bad Ô¨Åx), comprising 9% of the total bugs (31 ;201) in the Apache
database. Bad Ô¨Åxes need not manifest in reopened bugs; we focused
on reopened bugs because they often make bad Ô¨Åxes easier to iden-
tify. Our manual study undercounts the prevalence of bad Ô¨Åxes in
the studied projects, although we cannot say by how much.
In this paper, we describe the Ô¨Årst systematic treatment of the bad
Ô¨Åx problem: we introduce and formalize the problem, and present
novel techniques to help developers assess the quality of a bug Ô¨Åx.
We deem a Ô¨Åx bad if it fails to cover all bug-triggering inputs or
introduces new bugs. An ideal Ô¨Åx covers all bug-triggering inputs
and introduces no new bugs. We deÔ¨Åne two criteria to determine
whether or not a Ô¨Åx resolves a bug ‚Äî coverage and disruption:
Coverage: Many inputs may trigger a bug. The coverage of a Ô¨Åx
measures the extent to which the Ô¨Åx correctly handles all
bug-triggering inputs.
Disruption: A Ô¨Åx may unexpectedly change the behavior of the
original program. Disruption counts these deviations from
the program‚Äôs intended behavior introduced by a Ô¨Åx.
Given a buggy program, a bug-triggering input that results in an
assertion failure, a test-suite, and a Ô¨Åx, the bad Ô¨Åx problem is to
determine the coverage and disruption of the Ô¨Åx.
In theory, Dijkstra‚Äôs weakest precondition (WP) [ 11] can be used
to calculate the coverage of a Ô¨Åx. We start from the manifestation
of the bug in the buggy version of the program to discover the set
predicate of bug-triggering subset of the program‚Äôs input domain.
Then we would symbolically execute the Ô¨Åxed program to learn
whether, starting from that set predicate, inputs still exist that can
trigger the bug. However, Dijkstra‚Äôs WP computation depends on
loop invariants and must contend with paths exponential in the
number of branches.
We propose distance-bounded weakest precondition ( WPd)to
perform the WP calculation over a set of paths near a concrete path.
Given a path and a distance budget, WP dproduces a set of paths and
computes the disjunction of the WP of each path. In the context of
the bad Ô¨Åx problem, the bug-triggering input induces this concrete
path. This process is sound and yields an under-approximation
of the set of bug-triggering inputs. We can improve our under-
approximation by increasing the distance budget. Indeed, in the
limit when the distance budget tends to inÔ¨Ånity, WP dis precisely
Dijkstra‚Äôs WP.
WP doffers two practical beneÔ¨Åts. First, because we operate
directly on paths, we avoid both the loop invariant requirement and
the path-explosion problem. Second, it is our intuition that paths
closer to the bug-triggering path are more likely to be related to the
same bug and more error-prone, so adding them is likely to quickly
approximate the bug-triggering input domain at low cost. Kim et al.
showed that when a bug occurs in a Ô¨Åle, more bugs are likely to
occur in that Ô¨Åle, a phenomenon they name temporal locality [ 23].
Their results can be put another way: defects are lexically clustered,
which supports our intuition since many execution paths that are
close to each other are also lexically close.
1The absolute numbers are377
5200for Ant,86
2162for AspectJ, and38
847
for Rhino. The number is2939
31201(9.4%) across all Apache projects.
2Here we restricted ourselves to bugs that had been reopened, but
not marked as duplicates.This approach may appear circuitous: why not apply WP ddirectly
to the Ô¨Åxed program to see if we can Ô¨Ånd an input that triggers the
assertion failure? The problem is that the original buggy input no
longer triggers the bug in the Ô¨Åxed program. Thus, the concrete
path that triggers the bug in the buggy version of the program no
longer reaches the assertion and may not even exist in Ô¨Åxed program.
Computing WP based on this false path may lead to spurious or
incorrect results.
Regression testing is a measure of our disruption criterion; a
project‚Äôs test suite is a parameter of the bad Ô¨Åx problem to take
advantage of this fact. We combine random and regression testing
to calculate the disruption of a Ô¨Åx.
To demonstrate the feasibility of our approach, we implemented
a prototype, FIXATION , which automatically detects bad Ô¨Åxes in
Java programs. Given the buggy and Ô¨Åxed, versions of a program,
a test-suite, and a bug-triggering input, FIXATION solves the bad
Ô¨Åx problem. Our tool currently supports Java programs with condi-
tionals in Boolean and integer domains. When it detects a coverage
failure, it outputs a counterexample that triggers the bug in the Ô¨Åxed
program (See Section 3.3 ); when it detects a disruptive Ô¨Åx, it reports
the failing test cases or inputs (See Section 3.4 ). From examining the
counterexample or the failing test cases, programmer can understand
why the Ô¨Åx did not work and improve it.
The main contributions of this paper are:
We introduce the bad Ô¨Åx problem and provide empirical ev-
idence of its importance by exploring the bug databases of
three real projects to Ô¨Ånd that bad Ô¨Åxes accounts for as much
as 9% of all bugs.
We formalize the bad Ô¨Åx problem and propose distance-
bounded weakest precondition ( WP d), a novel form of weak-
est precondition, well-suited for the bad Ô¨Åx problem, that
restricts the weakest precondition computation to a subset of
the paths in a program‚Äôs control Ô¨Çow graph.
We implemented a prototype, called FIXATION , to check the
coverage and disruption of a Ô¨Åx. We evaluated our prototype
to demonstrate the feasibility of our approach: FIXATION
detects bad Ô¨Åxes extracted from real-world programs.
The structure of this paper is as follows. In Section 2 , we illustrate
the problem with actual bad Ô¨Åxes and show how our technique
can detect them. Section 3 formalizes our criteria for a bad Ô¨Åx
and presents the detailed technique to measure them. We describe
our prototype implementation and evaluation results in Section 4 .
Finally, we survey related work ( Section 5 ) and conclude the paper
with a discussion of future work ( Section 6 ).
2. ILLUSTRATIVE EXAMPLE
This section describes an actual sequence of bad Ô¨Åxes for a bug
from the Rhino project, and how our approach would have helped.
Rhino is an open-source JavaScript interpreter written in Java.
JavaScript allows programmers to deÔ¨Åne _noSuchMethod_ , a special
method that the JavaScript interpreter invokes, instead of raising an
exception, when an undeÔ¨Åned method is called on an object. The
bug, which we name NoSuchMethod , was a lack of support for this
_noSuchMethod_ mechanism. Its Ô¨Åx is not complicated; the Ô¨Ånal
patch was less than 100 lines. However, due to bad Ô¨Åxes, the bug
was reopened twice and three Ô¨Åxes were committed in three months.
Figure 1 contains the Ô¨Årst committed Ô¨Åx. On Line 1 , the pro-
grammer admits that he was not sure whether this Ô¨Åx covered
all relevant inputs. The Ô¨Åx adds an if-block which, when an
undeÔ¨Åned method has been called, extracts noSuchMethodMethod
561// no idea what to do if it 's a TAIL_CALL
2if ( fun instanceof NoSuchMethodShim
3 && op != Icode_TAIL_CALL ){
4
5 // get the shim and the actual method
6 NoSuchMethodShim =( NoSuchMethodShim )fun ;
7 Callable noSuchMethodMethod =
8 noSuchMethodShim . noSuchMethodMethod ;
9 ...
10 }
Figure 1: First Ô¨Åx of NoSuchMethod .
1if ( fun instanceof NoSuchMethodShim ) {
2if(fun instanceof NoSuchMethodShim
3 &&op!=Icode_TAIL_CALL) {
4
5 // get the shim and the actual method
6 NoSuchMethodShim = (NoSuchMethodShim)fun;
7 Callable noSuchMethodMethod =
8 noSuchMethodShim.noSuchMethodMethod;
9 ...
10 if ( op == Icode_TAIL_CALL ) {
11 callParentFrame = frame.parentFrame;
12 exitFrame(cx, frame, null);
13 }
14 ...
15 }
Figure 2: Second Ô¨Åx of NoSuchMethod . Green, normal weight lines
indicate changes added in this Ô¨Åx; red, strikeout lines indicate those
removed; and gray lines are those left unchanged.
from NoSuchMethodShim and dispatches the undeÔ¨Åned method on
it, passing the original test case. However, the clause ‚Äú op !=
Icode_TAIL_CALL ‚Äù could be false for an undeÔ¨Åned method call. The
programmer missed this case. Under our criteria, this Ô¨Åx fails
the coverage check. Given buggy and Ô¨Åxed versions of the pro-
gram, FIXATION would compute the predicate of the bug-triggering
input domain and symbolically execute the Ô¨Åxed program with
that predicate as the initial precondition. Upon reaching the ex-
ception, FIXATION would determine the Ô¨Åx to be bad, and return
the counterexample ‚Äú fun instanceof NoSuchMethodShim ^op ==
Icode_TAIL_CALL ‚Äù to the programmer. In this example, FIXATION
can exploit the common idiom of asserting false at a code path
not expected to be reached; in general, however, the assertion that
captures a bug can be more complex.
After the bug was reopened, the programmer reÔ¨Åned the Ô¨Åx, as
shown in Figure 2 . The clause that restricted the operation mode was
dropped. Inside this if-block, the programmer added a block to deal
with the case when operation mode was set to Icode_TAIL_CALL .
The Ô¨Åx handles all inputs that triggered the original bug. However,
the Ô¨Åx failed when subjected to regression testing. It fails the dis-
ruption check of a bad Ô¨Åx: it excised the bug that motivated its
application at the cost of introducing new bugs.
Finally, the programmer committed a third version of the Ô¨Åx,
which resolved the bug and passed the regression tests. This se-
quence of Ô¨Åxes shows how easy it is to write a bad Ô¨Åx. Programmers
considering only of a subset of the bug-triggering input domain are
likely to miss conditions and execution paths. Our technique can
help programmers detect these conditions earlier and write better
Ô¨Åxes more quickly.
3. APPROACH
To begin, we formalize our problem domain, then deÔ¨Åne the cov-
erage and disruption of a Ô¨Åx. We abstract the bug bas a failure of
the assertion j. Ideally, we would directly compute whether Dijk-
ibÀÜibÀúibBfI
Figure 3: A program‚Äôs input domain I, the known input ibthat
triggers the bug b, all inputs that trigger the bug Àúib, those inputs
the Ô¨Åx fhandles ÀÜib, new bugs Bfthat fmay introduce, and their
inter-relationships.
stra‚Äôs WP from jin the Ô¨Åxed program is false. This computation
requires loop invariants and must contend with the path-explosion
problem. We have a bug-triggering input and its induced concrete
failing path. The key idea of our approach for computing whether
a Ô¨Åx covers all inputs that can trigger a bug is to leverage that
failing path to compute a sound under-approximation of the bug-
triggering input domain, then test the Ô¨Åxed program against inputs
from that domain. In Section 3.2 , we introduce WP dto compute
that sound under-approximation. Section 3.3 shows how we use
that under-approximation to symbolically execute the Ô¨Åxed program
to calculate a counterexample to the Ô¨Åx. We close, in Section 3.4 ,
by presenting our algorithm for computing Ô¨Åx disruption which
combines regression and random testing.
3.1 The Bad Fix Problem
A good bug Ô¨Åx eliminates the bug for all inputs without introduc-
ing new bugs. We deÔ¨Åne two criteria to measure these dimensions ‚Äî
coverage anddisruption .
We model a program P:I!Oas a function in terms of its
input/output behavior. We assume that the bug bcauses an assertion
failure in the buggy program Pband that we know a bug-triggering
input ibsuch that Equation 1 holds. Concretely, ibrepresents a
failing test case, i.e., the output Pb(ib)violates the assertion j.
Pb(ib)6j=j(or equivalently, Pb(ib)j=:j) (1)
Many inputs may trigger :j. DeÔ¨Ånition 3.1 speciÔ¨Åes this set.
DEFINITION 3.1 (B UG-TRIGGERING INPUT DOMAIN ).
Àúib=fi2I:Pb(i)j=:jg
A bug Ô¨Åx fcreates a new version of the program Pf. At the very
least, Pf(ib)j=j, but Pfmay not handle all of Àúib. DeÔ¨Ånition 3.2
deÔ¨Ånes the subset of ÀúibthatPfhandles.
DEFINITION 3.2 (C OVERED BUG-TRIGGERING INPUTS ).
ÀÜib=fi2Àúib:Pf(i)j=jg
Ideally, a Ô¨Åx feliminates the bug band covers all of Àúib. With
respect to Àúib, the set ÀÜibindicates the degree to which a Ô¨Åx achieves
this goal, viz.the Ô¨Årst dimension of Ô¨Åx quality, its coverage . We use
cov(f)to denote the coverage of a Ô¨Åx f.
By deÔ¨Ånition, Pfis correct, relative to b, for the inputs in ÀÜib.
Outside of Àúib, a bad Ô¨Åx fmay introduce new bugs. DeÔ¨Ånition 3.3
captures these bugs.
57DEFINITION 3.3 (I NTRODUCED BUGS).LetPobe the cor-
rect oracle for P,i.e., for all inputs, Poproduces the desired output.
Bf=fi2InÀúib:Pf(i)6=Po(i)g (2)
Thedisruption of the Ô¨Åx fis the set of new deviating input values
it introduces, viz.the set Bf. When fintroduces no new bugs,
Bf=/ 0and fis not disruptive. Since we do not have a program
oracle in general, we approximate Powith the program‚Äôs test suite
TandPb, the buggy version of the program. Section 3.4 presents
the algorithm we use to compute disruption. Along the disruption
dimension, we compare the quality of two Ô¨Åxes in terms of the Bf
sets they induce.
Anideal Ô¨Åx covers all bug-triggering inputs and introduces no
disruptions:
ÀÜib=Àúib^Bf=/ 0: (3)
Figure 3 illustrates the interrelations of the sets deÔ¨Åned in this
section. With our coverage and disruption properties in hand, we
now deÔ¨Åne the bad Ô¨Åx problem.
DEFINITION 3.4 (B ADFIXPROBLEM ).Given a buggy pro-
gram Pb, a bug-triggering input ib, a test suite T:I!O(modeled
as a partial function from ItoO), and the Ô¨Åx f, determine the
coverage and disruption of f .
We can also use our criteria to partially order Ô¨Åxes for the same
bug. The Ô¨Åx fais better than fbif and only if
cov(fb)cov(fa)^BfaBfb: (4)
3.2 Distance-Bounded Weakest Precondition
To determine whether a Ô¨Åx covers the bug-triggering, we intro-
duce the concept of distance-bounded weakest precondition ( WP d)
which generalizes Dijkstra‚Äôs weakest precondition (WP). WP dre-
stricts the weakest precondition computation to a subset of the paths
near a distinguished path in the interprocedural control Ô¨Çow graph
(ICFG) of a program. In the context of the bad Ô¨Åx problem, the dis-
tinguished path is Pib, the concrete path induced by the known bug-
triggering input ib. In this section, we explain how WP dtraverses
the ICFG of a program and uses Levenshtein edit distance [ 25] to
construct the subset of simple paths over which WP dcomputes the
weakest precondition. By considering only a subset of simple paths,
WP dmitigates the path-explosion problem and does not need loop
invariants:
WP d: ProgramsPredicatesPathsN0!Predicates :(5)
Equation 5 deÔ¨Ånes the signature of WP d, which adds a path P
and a distance dto the signature of standard WP. An application of
WP d(P;j;P;d)Ô¨Årst generates the set Cof candidate paths at most d
distance from P, then computes the standard weakest precondition
over only the candidate paths in C. Equation 6 deÔ¨Ånes the candidate
paths WP dconsiders. The metric Dcomputes the distance of two
paths. Currently, we assign a symbol to every edge in the program‚Äôs
ICFG, map every path to a string, and use Levenshtein distance as
our metric D.
C=fs2Paths : D(s;P)dg (6)
An ICFG represents loops with backedges. Thus, a concrete path
that iterates in a loop is not a simple path in an ICFG. To statically
extract paths and compute their distance, we eliminate all backedges
by inÔ¨Ånitely unrolling all loops to form an inÔ¨Ånitely unrolled ICFG,
denoted ICFG ¬•. Figure 4 shows a loop in an ICFG and its unrolled
(a) CFG with loop
 (b) ICFG ¬•
Figure 4: A CFG and its equivalent ICFG ¬•.
Algorithm 1 Generate paths Levenshtein distance dfrom P
Input: d//distance
Input: ICFG
Input: P
G=reverseArcs(ICFG)
Pr=reverse (P)
result-paths / 0
q.enqueue(hPr[0];hii) //vertex, path
while not q.empty? do
hv;pi=q.dequeue()
ifv=Pr[ 1]^l(p;Pr)dthen
result-paths result-paths[fpg
end if
e=min(jpj 1;jPj)
ifl(p;Pr[0;e])2dthen
p.append( v)
q.enqueue(hn;pi);8n2hG(v))
end if
end while
return result-paths
representation in an ICFG ¬•. All executions have simple paths in an
ICFG ¬•.ICFG n, a Ô¨Ånite subgraph of an ICFG ¬•, unrolls all loops n
times. All terminating programs iterate each loop a Ô¨Ånite number
of times, and can be captured by an ICFG n. In particular, all paths
within distance dofPibstatically exist in an ICFG nfor some n.
hG:V!2V(7)
l:SS!N0 (8)
The function hGin Equation 7 returns the neighbors of a vertex in
the graph G, and lin Equation 8 calculates the Levenshtein distance
of two strings. Algorithm 1 uses these functions to calculate C.
For the sequence s, Algorithm 1 uses the notation s[i]to denote the
ithcomponent of s,s[ 1]to denote the last component of s, and
s[i;j];for 0 <i<jjsjto denote the substring from itojins.
Algorithm 1 reverses Pand the arcs in the given ICFG, before
traversing each path until it exceeds a distance budget of 2d. Paths
that reach the entry are retained only if they pass the more stringent
distance das they can no longer become closer to P. Algorithm 1
handles either an ICFG or an ICFG ¬•, but is easier to understand
when traversing an ICFG ¬•.
The set of candidate paths Cthat Algorithm 1 outputs may contain
infeasible paths, such as ones that iterate a loop once more than its
upper bound. Such paths are discarded in WP d‚Äôs second phase. Each
path in Cis traversed. When a path reaches a node, a theorem-solver
attempts to satisfy its current predicate. A path whose predicate is
unsatisÔ¨Åable is discarded. For example, a path that iterates a loop
58beyond its upper bound generates a predicate similar to i=4^i<4.
The weakest preconditions of satisÔ¨Åable paths are computed and
combined to form a disjunction:
WP d(P;j;P;d) =_
c2CWP(c;j): (9)
Asdincreases, WP dcalculates the weakest precondition of a
larger subset of the paths over which standard WP computes; in the
limit, WP dbecomes WP:
lim
d!¬•WP d(P;j;P;d) =WP(P;j): (10)
Under standard WP, the number of paths grows exponentially
with the number of jumps in the target program. Polymorphism
exacerbates this problem in object-oriented programs. In WP d, the
distance factor dcontrols the number of paths used in the weakest
precondition computation. When d=0,WP donly computes WP
on the path P. Varying dallows one to trade off the precision of
the computed predicate against the efÔ¨Åciency of its computation.
In contrast with standard WP, moreover, WP dselects paths close
to the erroneous path Pib. Thus, WP dcan, in principle, Ô¨Ånd a
counterexample using fewer resources than standard WP.
Unlike WP, WP ddoes not need loop invariants. The difÔ¨Åculty of
deriving loop invariants has hampered the application of WP. Indeed,
current tools have adopted various heuristics, such as iterating a loop
1:5times (loop condition twice, its body once), to circumvent the
lack of loop invariants [ 6,13]. Under WP d, edit distance determines
candidate paths and therefore the number of the loop iterations of
each loop along the path. WP dsupports context-sensitivity via
cloning; the distance budget determines whether WP dexplores
a path with one more or one fewer recursive calls. Though edit
distance may construct infeasible paths, WP computation along
such a path will produce an unsatisÔ¨Åable predicate which will cause
WP dto discard the path.
3.3 Detecting Violations of Coverage
With the help of WP d, the coverage of a Ô¨Åx f,cov(f), can be
computed in three main steps, as shown in Equation 11 :
step 3z }| {
9x1xn[SE(Pf;WP d(Pb;:j;step 1z}|{
e(Pb(ib));d)| {z }
step 2^j)] (11)
1.Extract the concrete path Pibthat the buggy input ibinduces;
2. Compute WP d(Pb;:j;Pib;d) =a; and
3.Symbolically execute Pfusing ato derive Pf‚Äôs postcondition
y, and eliminate the non-input variables xifrom the clause
y^jto yield cov(f).
In the Ô¨Årst step, we run Pb(ib), then apply eto extract Pib. Re-
call that jis the failing assertion. In the second step, aunder-
approximates the set predicate of Àúib, the true bug-triggering input do-
main. In this step, we compute afor various values of d, depending
on resource constraints. The precision of our under-approximation
depends on d. Starting from the weakest precondition computed
for various din step 2 anded with the assertion j, the third step
uses symbolic execution [ 24] to compute the set of bug-triggering
inputs handled by the bug Ô¨Åx. We eliminate non-input variables,
i.e.intermediate and output variables, from the clause, as they are
irrelevant to coverage, which concerns only inputs.Algorithm 2 Compute the disruption of a Ô¨Åx
Input: I//The program‚Äôs input domain
Input: pb//The buggy version of the program
Input: pf//The Ô¨Åxed version of the program
Input: T:I!O//The program‚Äôs test suite
1:R=/ 0
2:8(i;o)2Tdo//Standard regression testing
3: ifpf(i)6=othen
4: R R[fig
5: end if
6:end for
7:8i2a random subset of Ido
8: ifpb(i))j^pb(i)6=pf(i)then
9: R R[fig
10: end if
11:end for
12:return R
To decide whether fis a bad Ô¨Åx in terms of coverage, we check
the validity of y!j. If it is valid, the Ô¨Åx does not violate the
coverage requirement. Otherwise, we deem fa bad Ô¨Åx and report
any counterexamples to the validity of y!jas new bug-triggering
inputs. Our assertion that fdoes not cover Àúibis sound because a
under-approximates Àúib,i.e.,fi2I:agÀúib.
3.4 Detecting Violations of Disruption
To measure the disruption of the Ô¨Åx f, we Ô¨Årst run Pfon the test
suite, as is conventional, because test suites are crafted to exercise
important execution paths [ 16,32]. Each test failure is a disruption.
Given a speciÔ¨Åcation of a program‚Äôs input I, we then randomly
choose an input i2InÀúib. We compare the output of PbtoPfto Ô¨Ånd
errors not anticipated by the test suite. Each time Pb(i)6=Pf(i), we
have found another disruption. Because ÀÜibunder-approximates the
actual bug-triggering input domain Àúib, we ignore inputs that trigger
:jinPb.
Algorithm 2 computes the disruptions of a Ô¨Åx, returning a set
of failing inputs. The loop at lines 7‚Äì11 samples inputs from I,
ignoring those that trigger the original bug. We use the fact that
Pbfails the assertion jto discover such inputs. In comparing Pf
andPbon those inputs, we do not assume that Pbhas only the one
bugband works correctly for all other inputs; instead, we simply
assume that we can consider each bug in isolation. Further, outside
ofÀúi;bPbusefully approximates the ideal behavior of P; when Pb
is a release, deployed version of a program, it presumably passed
regression testing.
4. EMPIRICAL EVALUATION
Our evaluation objective is two-fold: to demonstrate the feasibility
and utility of our approach, and to differentiate WP dfrom WP.
First, we describe our implementation, our computing environment
and how we selected our test suite. We then show that FIXATION
detects the bad Ô¨Åx in our motivating example, as well as Ô¨Åve others.
We compare WP dto WP by showing how WP daccumulates path
predicates, and thus subsets of the true bug-triggering input domain
Àúib, as a function of its parameter d. Finally, we close by describing
how a developer might use FIXATION to discover bad Ô¨Åxes and
instead commit good ones.
4.1 Implementation
Many tools and techniques exist for detecting the disruption of
a Ô¨Åx, so we focused our implementation on determining Ô¨Åx cover-
59age. We used the WALA framework [ 20] to extract the concrete
buggy path induced by a bug-triggering input and build a CFG, from
which we extract candidate paths using Algorithm 1 . The extracted
concrete path is the sequence of basic blocks traversed during a
particular execution of the program. ICFG nis produced by travers-
ing the CFG of each method and unrolling each loop the number
of times it iterated in the concrete path and an additional xtimes,
according to an unrolling parameter x; thus, n=x+y, where yis a
loop that iterated the most times in the concrete path. The unrolling
parameter allows FIXATION to explore paths that loop more often
than the concrete path speciÔ¨Åes. We then run our implementation of
Algorithm 1 over this ICFG nand feed each resulting path predicate
to the SMT-solver CVC3 [ 3], keeping only those that are satisÔ¨Åable.
Java PathFinder is the Swiss army knife of Java veriÔ¨Åcation [ 37].
FIXATION uses JPF‚Äôs Symbc component to symbolically execute
the Ô¨Åxed program, using the weakest precondition produced above
as the precondition. If, given this precondition, the assertion fails in
the Ô¨Åxed program, Symbc generates a concrete input that causes the
failure and returns it as a counterexample.
4.2 Experimental Setup
We built and ran our evaluations on a Dell XPS 630i with 2.4GHz
QuadCPU processors and 3.2 GB of Memory, running Ubuntu 8.04
with kernel Linux2.6.24-21-generic. FIXATION is built for and runs
on JRE 6.
Although we found many bad Ô¨Åxes in the three projects we ex-
plored, most of them were not Ô¨Åt for evaluation: either the bug
comments were unclear or no Ô¨Åx was uploaded. No convention
appears to govern the use of Bugzilla. Some programmers tend to
write detailed logs of their Ô¨Åx activity and upload their Ô¨Åx, but most
do not. We selected the Ô¨Årst six bugs we found whose comments
proved the existence of bad Ô¨Åx and that had an attached Ô¨Åx. Five of
the bad Ô¨Åxes are from Rhino and MultiTask is from Ant3.
Currently, FIXATION does not directly work on the original, un-
modiÔ¨Åed code, since both WP dand Symbc (JPF v4.1) support only
Java programs consisting of statements and expressions that use only
boolean and integer variables. Thus, we Ô¨Årst manually sliced away
all code not related to the bugs that caused the bad Ô¨Åxes. We then
transformed the code into an integer program that, given the same
inputs, traverses the same paths as the sliced version of the original
program. Since FIXATION simply ignores non-integer language con-
structs, like function calls and Ô¨Åeld or array accesses, we left them
in place to approximate the original program as closely as possible.
We manually transformed the conditionals in the original code into
integer conditionals. To rewrite fun instanceof NoSuchMethodCall ,
we introduced the integer variable fun_int and the integer constant
NoSuchMethodCall_int , then replaced the original conditional with
the conditional fun_int == NoSuchMethodCall_int . We added logic
as necessary so that fun_int correctly tracked the type of original
object fun. At each point the bug manifested itself in the original
program, we added an assertion.
Our principal goal was to faithfully retain the inherent complexity
of the program‚Äôs logic through the transformation. Table 1 presents
evidence that we succeeded; it shows the raw lines of code, the
nodes and arcs in the ICFG, and the Cyclomatic complexity (CC)
of ICFGs before ( P) and after Pitransformation. For a program
with one exit point, Cyclomatic complexity equals the number of
decision points in the program plus one [ 26]; we increase it in all
six cases.
4.3 Experimental Results
3The sliced and transformed versions of the bad Ô¨Åxes are available
athttp://wwwcsif.cs.ucdavis.edu/~gu/bugs.htm .NameLoc Nodes Arcs CC
P P i P P i P P iP P i
NoSuchMethod 60 65 60 64 70 75 12 13
MultiTask 23 36 51 62 54 68 5 8
Substring 8 17 10 16 10 18 2 4
NativeErr 9 20 10 18 10 21 2 5
Loop 34 42 42 46 46 51 6 7
PathExp 114 133 103 119 124 147 23 30
Table 1: Bad Ô¨Åx CFG complexity.
1instructionCounting ++;
2...
3stackTop -= 1 + indexReg ;
4if( fun == InterpretedFun ) {
5 return processInterFun ();
6}
7if( fun == Continuation ) {
8 return processCon ();
9}
10 if( fun == IdFunctionObject ) {
11 return processIdFunObj ();
12 }
13 ...
14 assert ( false ); // Should never execute .
Figure 5: Sliced integer version of NoSuchMethod .
Table 2 details the results of evaluating the six examples. The
second column brieÔ¨Çy describes each bad Ô¨Åx. The third column
contains the counterexample FIXATION reports. The fourth column
dpresents the edit distance used to construct the candidate paths. C
denotes the number of paths FIXATION explored before determining
the Ô¨Åx to be bad. As a point of reference for C,Pais the total number
of paths. Time records the time-to-completion.
Neglected Execution Paths The Ô¨Årst four bad Ô¨Åxes in Ta-
ble 2 are all due to a programmer‚Äôs ignorance of potential buggy
paths. FIXATION detected the buggy paths missed by each of these
bad Ô¨Åxes while exploring a limited number of paths. Once it de-
tected a bad Ô¨Åx, FIXATION reported a counterexample to help
programmers reÔ¨Åne their Ô¨Åxes. Since the bad Ô¨Åxes MultiTask ,
Substring and NativeErr all exhibit essentially the same symp-
toms as NoSuchMethod , we describe only NoSuchMethod .
Figure 5 lists the original buggy code that required three Ô¨Åxes
as chronicled in Section 2 . The original bug-triggering input fun
== NoSuchMethodShim && op != Icode_TAIL_CALL reminded the
programmer that there was no block for NoSuchMethodShim , so
the programmer committed the Ô¨Årst Ô¨Åx in Figure 1 .FIXATION
took the initial bug-triggering input and buggy code, ran with dis-
tance dset to zero, and discovered the bug-triggering input do-
main fun != InterpretedFun && fun != Continuation && fun !=
IdFunctionObject .FIXATION then symbolically executed the
Ô¨Årst Ô¨Åxed program, imposing that predicate together with the set
predicate of the program‚Äôs input domain as the initial precondition4.
Given that precondition, FIXATION reached and implied an assertion
failure, then reported the Ô¨Åx bad and returned the counterexample
fun == NoSuchMethodShim && op == Icode_TAIL_CALL .
A Bad Fix in a Loop Standard WP needs loop invariants, which
are difÔ¨Åcult to derive in general. Current tools usually adopt heuris-
4Without the predicate of the program‚Äôs input domain, Symbc may
generate nonsensical inputs. Assume that a program‚Äôs input domain
isx>0^x<10and the computed weakest precondition is x>5.
With only x>5, Symbc could generate the illegal input x=20.
60Name Description Counterexample d C P aTime (s)
NoSuchMethod Neglect an input.fun == NoSuchMethodShim
&& op == Icode_TAIL_CALL0 1 30 0.664
MultiTask Forget targets‚Äô size can be zero. type == VECTOR && size == 0 3 32 48 1.637
Substring Miss a condition. i == 1 && sT == SUB_NULL 2 3 3 0.598
NativeErr Miss handling an exception. type == NativeError 2 4 5 0.938
Loop Fail to detect bugs in loop. i_c == 6 && m_l = 3 5 354 ¬• 42.542
PathExp Miss bugs on different paths.tG == -10000 && cT == T_PRI
&& cI == T_NULL && op == T_SHNE
&& sC == T_TRUE27 234 1021 8.308
PathExplosion2 Miss bugs on different paths.tG == -10000 && cT == T_PRI
&& cI == T_NULL && op == T_SHEQ &&
sC == T_FALSE &&6 237 1021 8.518
Table 2: F IXATION results.
(a)Loop
 (b)PathExp 1
 (c)PathExp 2
Figure 6: Feasible paths as a function of edit distance.
1private static final int M_T = 2;
2private static final int I_L = 4;
3...
4public AdapterClass createAdapterClass (
5 String adapterName , Scriptable ins [],
6 int adapterType , int i_c , int m_l ) {
7 ...
8 assert ( i_c <= I_L );
9 ...
10 for( int i = 0; i < i_c; i++ ) {
11 assert ( i <= I_L && m_l <= M_T );
12 ...
13 }
14 ...
15 }
Figure 7: Sliced integer version of Loop .
tics at the cost of sacriÔ¨Åcing precision [ 6,13]. Here, we show how
WP dtackles the loop problem. Two bugs lurk in Figure 7 ‚Äî one
outside the loop and the other inside. The initial bug-triggering input
i_c == 5 && m_l == 2 exposes only the bug outside the loop. The
Ô¨Årst committed Ô¨Åx resolved only this bug. We ran FIXATION with
d=5on this Ô¨Årst Ô¨Åx. FIXATION explored 354 candidate paths, most
of which were unsatisÔ¨Åable. For example, computing WP on the
path that takes the true branch inside the for loop in its 4th iteration
generates the unsatisÔ¨Åable clause i==3 && i>4 . Disjuncting pred-
icates from feasible paths, FIXATION reported the bug-triggering
input domain (i_c == 5 && m_l > 2) || (i_c == 5 && m_l <= 2)
|| (i_c == 6 && m_l > 2) || (i_c == 6 && m_l <= 2) . Given thispredicate as its precondition, FIXATION output the counterexample
i_c == 6 && m_l == 3 .
In essence, WP dis unaware of loops. The candidate paths over
which WP dcomputes the weakest precondition are all simple paths,
drawn from an ICFG n. Figure 6a shows the number of feasible path
predicates WP ddiscovered as a function of the edit distance. It
illustrates that WP dcan produce useful results when given limited
resources. Loop has inÔ¨Ånite paths and therefore inÔ¨Ånite feasible
paths, which we cut off at d=14. By way of comparison, we ran
this example using ESC/Java2 and JPF Symbc, without our path
restriction. Using its default settings5, ESC/Java2 failed to report
a potential postcondition violation. Given the domain restriction
we inferred in our Ô¨Årst phase, our backward symbolic execution
from the assertion failure, Symbc does reach the assertion failure,
but at the cost exploring all loop iterations up to the failure, as
opposed to only those iterations within dof the failing iteration;
more importantly, Symbc continued searching until it reached loop
iteration 1 ;000, when we killed it.
Path Explosion WP dbalances scalability and coverage via
its edit distance parameter d.PathExp illustrates how FIXATION
detects potential bugs even when considering subsets of the po-
tentially feasible paths. PathExp occurred because Rhino failed to
ensure (undefined === null) evaluated to false as required by
the ECMA (JavaScript) speciÔ¨Åcation. In Figure 8 , six nested bugs
5ESC/Java2 does not support assertions, so we translated each as-
sertion into a postcondition annotation.
611if ( tG == -1 ) {
2 if ( c_T == T_PRI && c_I == T_NULL ) {
3 // bug 1
4 assert ( op != T_SHEQ && op != T_SHNE );
5 ... // assignments to op
6 // bug 2
7 assert ( op != T_SHEQ && op != T_SHNE );
8 ...
9 }
10 } else {
11 if ( c_T == T_PRI && c_I == T_NULL ) {
12 ...
13 // bug 3
14 assert ( op != T_SHEQ && op != T_SHNE );
15 ... // assignments to op
16 // bug 4
17 assert ( op != T_SHEQ && op != T_SHNE );
18 ...
19 if ( op == T_EQ || op == T_SHEQ ) {
20 if ( sC == T_FALSE ) {
21 markLabel ( popGOTO , popStack );
22 addByteCode ( ByteCode_POP );
23 // bug 5
24 assert (op != T_SHEQ && op != T_SHNE );
25 }
26 ...
27 // bug 6
28 assert ( op != T_SHEQ && op != T_SHNE );
29 }
30 }
31 }
Figure 8: Sliced integer version of PathExplosion .
are distributed in different paths. From the initial bug-triggering
input, the programmer discovered and Ô¨Åxed only bugs 1 and 2. We
ran F IXATION onPathExp , gradually increasing das shown in Fig-
ure 6b . As the Ô¨Ågure depicts, WP dÔ¨Ånds no additional paths until
d=27. Exploring Figure 8 , we Ô¨Ånd that bugs 3‚Äì6 are all in the else
block of the Ô¨Årst ifstatement, with conditional ( tG == -1 ). A path
that traverses any of bugs 3‚Äì6 shares few nodes with the original
buggy path, which traverses bugs 1 and 2. At d=27,FIXATION
Ô¨Ånds the new feasible predicate tG != -1 && c_T == T_PRI && c_I ==
T_NULL && op == T_SHNE && sC == T_TRUE after exploring 234paths.
Armed with this predicate, FIXATION can symbolically execute the
Ô¨Årst Ô¨Åxed program and report a counterexample exposing bugs 3
and 4.
Perhaps, as usual, the programmer was harried and rushing. In
any case, he committed a Ô¨Åx that corrected only bugs 1 and 2. Had
the programmer run FIXATION , he would have been aware of the
counterexample that his Ô¨Åx failed to handle. Figure 6c shows the
result of running FIXATION with increasing don the partially Ô¨Åxed
program. At d=5,FIXATION detects two new feasible weakest
preconditions after exploring 237paths. These predicates generate
two counterexamples that identify the remaining two bugs. WP d
offers a Ô¨Çexible, principled, and resource-judicious way to search
paths near a bug-triggering path.
4.4 Threats to Validity
FIXATION ‚Äôs edit distance parameter dallows its user to trade-off
performance against the completeness of FIXATION ‚Äôs approximation
of the bug-triggering input domain. Determining the optimal setting
ofdto obtain a better result is an interesting problem. The paths
WP dtraverses depends on das well as the structure of the CFG.
Gradually increasing duntil detecting interesting paths or exceeding
a resource threshold, such as time or number of paths explored,
appears to be a good heuristic.FIXATION is currently not optimized. Caching the predicates of
already explored paths would avoid redundant computation. Tabulat-
ing function summaries for reuse can also cache WP computations.
Adopting lightweight predicate on-the-Ô¨Çy feasibility checking, √† la
Snugglebug [ 6], might remove infeasible paths at an earlier stage.
We model bugs as assertion failures. Thus, FIXATION ‚Äôs applica-
bility depends on assertions being available, inferred, or written by
a programmer. In many cases, obtaining such an assertion is trivial
(as in thrown, fatal, exceptions). In others, it can be difÔ¨Åcult and is
an external threat to the validity of our approach: an empirical study
to investigate and classify those cases would help users know when
FIXATION is and is not practical. As with any manual study, our
results may exhibit selection bias. A larger empirical study would
also help in showing the technique generalizes.
We inherit our limitation to integer programs from our symbolic
execution components. This restriction makes the values of dwe
report optimistic as we operate on slightly smaller, sliced, versions
of the original program. Thus, our reliance on slicing is both a
construct and, because slicing limits the applicability and scalability
of our approach, an external, threat to validity. As the state-of-
the-art in symbolic execution improves (e.g. via techniques such
as delta-execution [ 10]), so will the applicability of our approach.
Nonetheless, the evaluation results are encouraging. FIXATION
detects bad Ô¨Åxes and reports counterexamples that can help pro-
grammers realize the limitation of particular Ô¨Åx. WP dhas shown
itself to be a promising approach to managing the path-explosion
problem and side-stepping the loop invariant problems.
5. RELATED WORK
Our work is the Ô¨Årst to offer a systematic methodology for assess-
ing the quality of a bug Ô¨Åx. It is related to the large body of work
on software testing and analysis. This section summarizes the most
closely related efforts. We divide related work into three categories:
practical computation of weakest precondition, automatic test input
generation, and studies of bug Ô¨Åxes and code changes.
5.1 Practical Computation of WP
Dijkstra‚Äôs weakest precondition [ 11] has been extended to check
the correctness of object-oriented programs. ESC/Java pioneered
its use in Java [ 13]. ESC/Java requires user-deÔ¨Åned annotations to
specify the precondition, postcondition and invariants. By checking
the validity of the veriÔ¨Åcation condition generated from guarded
commands, ESC/Java warns of potential bugs such as postcondition
violations or null pointer dereferences. ESC/Java‚Äôs checking is
modular so it relies on user annotation for procedure calls. To
handle loops, it heuristically iterates 1.5 times.
Snugglebug [ 6] presents an interprocedural WP technique. It
introduces directed call graph construction, generalization, and a
current search heuristic to improve the performance and precision.
Polymorphism means that Java programs face the dynamic-dispatch
problem when encountering function calls; directed call graph con-
struction helps Ô¨Ånd the exact callee without exhaustive search. Gen-
eralization enhances function summary reuse using tabulation. The
current search heuristic of Snugglebug prioritizes paths with less
looping or call depth.
Path-based WP computation complements WP computation over
an entire program. Applying counterexample-driven reÔ¨Ånement,
BLAST [ 19] and SLAM [ 2] check an abstract path to see if it
corresponds to a concrete trace of the program reaching an error
state. He and Gupta proposed path-based weakest precondition to
locate and correct an erroneous statement in a function [ 17]. Their
path-based weakest precondition is similar to our WP dwhen d=0.
We have assumed a Ô¨Åx at least covers the original bug-triggering
62input, so single-path approaches may not handle the bad Ô¨Åx problem.
Our approach generalizes path-based WP by parameterizing the
distance budget dand allowing arbitrary non-zero distances.
Our WP dÔ¨Ålters the paths over which standard WP computes.
Instead of computing WP on all paths, WP dapproximates the true
bug-triggering input domain by computing WP on paths that are
close to the original buggy path. Users control the performance and
coverage of WP dthrough its edit distance parameter. As dincreases,
WP dgenerates more paths and takes more time to compute. Another
insight of WP dis that computing the weakest precondition of simple
paths near a known concrete path is precise and does not rely on
heuristics: the concrete path speciÔ¨Åes how to resolve a dynamic
dispatch target or determine how often to traverse a loop.
5.2 Automatic Test Input Generation
Automatic test input generation is an active area of research.
Dozens of techniques and tools have been proposed. For brevity, we
highlight only some of the closely related work.
CUTE [ 35] and DART [ 14] combine concrete and symbolic ex-
ecution to generate test inputs for C programs. Java PathFinder
(JPF) [ 37] performs generalized symbolic execution to generate
inputs for Java programs. We use Symbc from JPF [ 30] to generate
counterexamples. We impose preconditions to guide the symbolic
execution: Given the set predicate of an approximation of the bug-
triggering input domain as the precondition, we ask whether the
Ô¨Åxed program can reach the assertion failure or not. This precon-
dition restricts the search space and enhances the performance of
Symbc. If Symbc outputs a concrete input that causes the assertion
to fail, we deem the Ô¨Åx bad and return that input as a counterexam-
ple. Csallner et al. ‚Äôs work [ 8] combines static checking and concrete
test-case generation. They perform random testing using the coun-
terexample predicate generated by ESC/Java. Random testing may
miss some paths. We symbolically execute all paths under imposed
precondition. Beyer et al. extend BLAST to generate testcases by
Ô¨Ånding inputs that satisfy predicates collected from all paths in the
program [ 4]. We also collect predicates to generate inputs, but for a
different purpose: Beyer et al. seek to exhaustively test one program,
while we use symbolic execution on a buggy and a Ô¨Åxed version of
a program to evaluate Ô¨Åxes.
5.3 Bug Fixes and Code Changes
Research on bug Ô¨Åxes mainly falls into two camps: mining soft-
ware repositories and empirical study. BugMem [ 22] mines bug
Ô¨Åx history to predict potential bugs. Kim et al. predict faults by
consulting bug and Ô¨Åx caches they build [ 23]. Their work shows
that bugs exhibit locality and inspired our design of WP d. Anvik
et al. applied machine learning to Ô¨Ånd programmers who should be
responsible for the Ô¨Åx [ 1]. Weiss et al. predict Ô¨Åxing effort needed
for a particular bug [ 38] and ¬¥Sliwerski et al. proposed a way to
locate code Ô¨Åxed using repository mining [ 36]. Most of the work in
this domain is probabilistic and may not be precise. We propose a
sound analysis for evaluating bug Ô¨Åxes: every bad Ô¨Åx we detect is
anactual bad Ô¨Åx.
Code change is fundamental to software development. Ryder
and Tip propose change impact analysis to Ô¨Ånd failure inducing
changes and thus judge the quality of change [ 33,39]. We consider
a subset of the changes they consider, speciÔ¨Åcally bug Ô¨Åxes. Change
impact analysis applies delta debugging on a sequence of atomic
changes drawn from the comparison of two versions to locate sus-
picious changes, while we combine our distance-bounded weakest
precondition with symbolic execution to judge the quality of a Ô¨Åx.
Differential symbolic execution [ 29] seeks to precisely character-
ize the differences between two program versions. DSE exploitsabstract summaries of code that is common between two program
versions, as the effects of such code do not contribute to differences.
In contrast, FIXATION seeks to identify not only issues of disruption,
but also coverage: a subset of inputs for which both versions behave
the same, viz.by manifesting a particular bug.
McCamant and Ernst compare operational abstractions of com-
ponents and their potential replacements to predict the safety of a
component upgrade [ 27]. Our approach is more Ô¨Åne-grained: as-
sertions need not refer to the modiÔ¨Åed component as operational
abstractions must. Their focus is different from ours: they seek to
verify that a newer component will behave as expected under the
conditions its predecessor was exposed to, and we seek to verify that
a component that does indeed behave differently (due to a bug Ô¨Åx)
does so safely ( i.e.without disruption) and completely ( i.e.handling
all of Àúib).
Regression testing validates modiÔ¨Åed software to ensure changed
code has not adversely affected unchanged code [ 16,28,32,34].
The cost of regression testing has been extensively studied and
shown that test suite size can be reduced without compromising
safety [ 15,31]. Currently, we combine regression and random
testing to check the disruption of a Ô¨Åx.
6. CONCLUSION AND FUTURE WORK
When run on buggy and allegedly Ô¨Åxed versions of a program,
FIXATION reports a new bug-triggering input drawn from an under-
approximation of the true bug-triggering input domain. This new
bug-triggering input is a counterexample to the implicit assertion
that the Ô¨Åx is good. FIXATION can miss bad Ô¨Åxes if it fails to
explore a buggy path, but it is sound when it asserts that a Ô¨Åx is bad:
every counterexample FIXATION reports is certain to cause the Ô¨Åxed
program fail the assertion.
We have introduced the bad Ô¨Åx problem and provided empirical
evidence of its existence in real projects. We have formalized the bad
Ô¨Åx problem and proposed an approach that combined our distance-
bounded weakest precondition with symbolic execution to evaluate
Ô¨Åxes and detect bad ones. We implemented our idea in a prototype
FIXATION and evaluated it: FIXATION was able to detect bad Ô¨Åxes
extracted from real-world programs.
In the future, we plan to extend FIXATION to support more lan-
guage features in Java and make it applicable to real code. We intend
to implement the optimizations mentioned. Unit testing is an imme-
diate application of FIXATION . A failed testcase is an ideal original
bug-triggering input for FIXATION . Self-contained assertions in a
test suite will allow FIXATION to work directly on the code and
obviate manually constructing and inserting the assertion. The fact
that the distribution of code tested by unit testing is relatively local
is likely to be a good Ô¨Åt for WP d.
Acknowledgments
We would like to thank Radu Grigore and Joseph R. Kiniry for help-
ful discussions about ESC/Java2, Stephen Fink and Manu Sridharan
for generously answering our questions about WALA, and Corina S.
PÀáasÀáareanu for promptly and concisely explaining how to run Symbc
of Java PathÔ¨Ånder. We appreciate Christian Bird‚Äôs help during our
empirical exploration on Bugzilla databases. We are also grateful to
the anonymous ICSE reviewers for their valuable comments.
References
[1]J. Anvik, L. Hiew, and G. C. Murphy. Who should Ô¨Åx this bug?
InICSE ‚Äô06: Proceedings of the 28th international conference on
Software engineering , 2006.
63[2]T. Ball and S. K. Rajamani. The SLAM project: debugging system
software via static analysis. In POPL ‚Äô02: Proceedings of the 29th
ACM SIGPLAN-SIGACT symposium on Principles of programming
languages , 2002.
[3]C. Barrett and C. Tinelli. CVC3. In CAV ‚Äô07: Proceedings of the 19th
International Conference on Computer Aided VeriÔ¨Åcation , July 2007.
[4]D. Beyer, A. J. Chlipala, and R. Majumdar. Generating tests from
counterexamples. In ICSE ‚Äô04: Proceedings of the 26th International
Conference on Software Engineering , 2004.
[5] Bugzilla. http://www.bugzilla.org/ .
[6]S. Chandra, S. J. Fink, and M. Sridharan. Snugglebug: a powerful
approach to weakest preconditions. In PLDI ‚Äô09: Proceedings of the
2009 ACM SIGPLAN conference on Programming language design
and implementation , volume 44, 2009.
[7]J. S. Collofello and S. N. WoodÔ¨Åeld. Evaluating the effectiveness of
reliability-assurance techniques. Journal of Systems and Software , 9(3),
1989.
[8]C. Csallner and Y . Smaragdakis. Check ‚Äôn‚Äô Crash: combining static
checking and testing. In ICSE ‚Äô05: Proceedings of the 27th interna-
tional conference on Software engineering , 2005.
[9]C. Csallner, Y . Smaragdakis, and T. Xie. DSD-Crasher: A hybrid
analysis tool for bug Ô¨Ånding. ACM Transactions Software Engineering
Methodology , 17(2), 2008.
[10] M. d‚ÄôAmorim, S. Lauterburg, and D. Marinov. Delta execution for
efÔ¨Åcient state-space exploration of object-oriented programs. In ISSTA
‚Äô07: Proceedings of the 2007 international symposium on Software
testing and analysis , 2007.
[11] E. W. Dijkstra. A Discipline of Programming . October 1976.
[12] J. Dolby, M. Vaziri, and F. Tip. Finding bugs efÔ¨Åciently with a SAT
solver. In ESEC-FSE ‚Äô07: Proceedings of the the 6th joint meeting of
the European software engineering conference and the ACM SIGSOFT
symposium on The foundations of software engineering , 2007.
[13] C. Flanagan, K. R. M. Leino, M. Lillibridge, G. Nelson, J. B. Saxe, and
R. Stata. Extended static checking for Java. In PLDI ‚Äô02: Proceedings
of the ACM SIGPLAN 2002 Conference on Programming language
design and implementation , 2002.
[14] P. Godefroid, N. Klarlund, and K. Sen. DART: directed automated
random testing. In PLDI ‚Äô05: Proceedings of the 2005 ACM SIGPLAN
conference on Programming language design and implementation ,
2005.
[15] T. L. Graves, M. J. Harrold, J.-M. Kim, A. Porter, and G. Rother-
mel. An empirical study of regression test selection techniques. ACM
Transactions Software Engineering Methodology , 10(2), 2001.
[16] M. J. Harrold, J. A. Jones, T. Li, D. Liang, A. Orso, M. Pennings,
S. Sinha, S. A. Spoon, and A. Gujarathi. Regression test selection
for Java software. In OOPSLA ‚Äô2001: Proceedings of the ACM Con-
ference on Object-Oriented Programming, Systems, Languages, and
Applications , 2001.
[17] H. He and N. Gupta. Automated debugging using path-based weak-
est preconditions. In FASE ‚Äô04: Proceedings of the Fundamental
Approaches to Software Engineering, 7th International Conference ,
volume 2984 of Lecture Notes in Computer Science , 2004.
[18] T. Henzinger, R. Jhala, R. Majumdar, and G. Sutre. Software ver-
iÔ¨Åcation with BLAST. In SPIN ‚Äô2003: Proceedings of the Tenth
International Workshop on Model Checking of Software, volume 2648
of Lecture Notes in Computer Science , 2003.
[19] T. A. Henzinger, R. Jhala, R. Majumdar, and G. Sutre. Lazy abstrac-
tion. In POPL ‚Äô02: Proceedings of the 29th ACM SIGPLAN-SIGACT
symposium on Principles of programming languages , 2002.
[20] IBM. T.J. Watson libraries for analysis. http://wala.sf.net .[21] IDC. A Telecom and Networks market intelligence Ô¨Årm. http://
www.idc.com .
[22] S. Kim, K. Pan, and E. E. J. Whitehead, Jr. Memories of bug Ô¨Åxes. In
SIGSOFT ‚Äô06/FSE-14: Proceedings of the 14th ACM SIGSOFT inter-
national symposium on Foundations of software engineering , 2006.
[23] S. Kim, T. Zimmermann, E. J. Whitehead Jr., and A. Zeller. Predicting
faults from cached history. In ICSE ‚Äô07: Proceedings of the 29th
international conference on Software Engineering , 2007.
[24] J. C. King. Symbolic execution and program testing. Communications
of the ACM , 19(7), 1976.
[25] V . Levenshtein. Binary codes capable of correcting deletions, inser-
tions and reversals (in Russian) . 1965.
[26] T. J. McCabe. A Complexity Measure. IEEE Transactions on Software
Engineering , 2(4), 1976.
[27] S. McCamant and M. D. Ernst. Predicting problems caused by com-
ponent upgrades. In ESEC/FSE-11: Proceedings of the 9th Euro-
pean software engineering conference held jointly with 11th ACM
SIGSOFT international symposium on Foundations of software engi-
neering , 2003.
[28] A. Orso, N. Shi, and M. J. Harrold. Scaling regression testing to large
software systems. SIGSOFT Software Engineering Notes , 29(6), 2004.
[29] S. Person, M. B. Dwyer, S. Elbaum, and C. S. P ÀáasÀáareanu. Differential
symbolic execution. In SIGSOFT ‚Äô08/FSE-16: Proceedings of the 16th
ACM SIGSOFT International Symposium on Foundations of software
engineering , 2008.
[30] C. S. P ÀáasÀáareanu, P. C. Mehlitz, D. H. Bushnell, K. Gundy-Burlet,
M. Lowry, S. Person, and M. Pape. Combining unit-level symbolic
execution and system-level concrete execution for testing nasa software.
InISSTA ‚Äô08: Proceedings of the 2008 international symposium on
Software testing and analysis , 2008.
[31] G. Rothermel and M. Harrold. Analyzing regression test selection
techniques. IEEE Transactions on Software Engineering , 22(8), Aug
1996.
[32] G. Rothermel and M. J. Harrold. A safe, efÔ¨Åcient regression test selec-
tion technique. ACM Transactions Software Engineering Methodology ,
6(2), 1997.
[33] B. G. Ryder and F. Tip. Change impact analysis for object-oriented
programs. In PASTE ‚Äô01: Proceedings of the 2001 ACM SIGPLAN-
SIGSOFT workshop on Program analysis for software tools and engi-
neering , 2001.
[34] R. Santelices, P. Chittimalli, T. Apiwattanapong, A. Orso, and M. Har-
rold. Test-suite augmentation for evolving software. In Automated
Software Engineering, 2008. ASE 2008. 23rd IEEE/ACM International
Conference on , 2008.
[35] K. Sen, D. Marinov, and G. Agha. CUTE: a concolic unit testing engine
for C. In ESEC/FSE-13: Proceedings of the 10th European software
engineering conference held jointly with 13th ACM SIGSOFT interna-
tional symposium on Foundations of software engineering , 2005.
[36] J. Sliwerski, T. Zimmermann, and A. Zeller. When do changes induce
Ô¨Åxes? In MSR ‚Äô2005: International Workshop on Mining Software
Repositories , 2005.
[37] W. Visser, C. S. P ÀáasÀáareanu, and S. Khurshid. Test input generation
with Java PathFinder. In ISSTA ‚Äô04: Proceedings of the 2004 ACM
SIGSOFT international symposium on Software testing and analysis ,
2004.
[38] C. Weiss, R. Premraj, T. Zimmermann, and A. Zeller. Predicting effort
to Ô¨Åx software bugs. In Proceedings of the 9th Workshop Software
Reengineering , May 2007.
[39] J. Wloka, E. Hoest, and B. G. Ryder. Tool support for change-centric
test development. IEEE Software , 99(PrePrints), 2009.
64