OrganizationalVolatilityand itsEffectson Software
Defects
Audris Mockus
AvayaLabs Research
233Mt Airy Rd, BaskingRidge, NJ
audris@avaya.com
ABSTRACT
The key premise of an organization is to allow more eﬃcient
production, including production of high quality software .
To achieve that, an organization deﬁnes roles and reporting
relationships. Therefore, changes in organization’s stru cture
are likely to aﬀect product’s quality. We propose and in-
vestigate a relationship between developer-centric measu res
of organizational change and the probability of customer-
reported defects in the context of a large software project.
We ﬁnd that the proximity to an organizational change is
signiﬁcantly associated with reductions in software quali ty.
We also replicate results of several prior studies of softwa re
quality supporting ﬁndings that code, change, and devel-
oper characteristics aﬀect fault-proneness. In contrast t o
prior studies we ﬁnd that distributed development decrease s
quality. Furthermore, recent departures from an organiza-
tion were associated with increased probability of custome r-
reported defects, thus demonstrating that in the observed
context the organizational change reduces product quality .
Categories andSubject Descriptors
D.2.8 [Software Engineering ]: Metrics— Product metrics ;
D.2.8 [Software Engineering ]: Metrics— Process metrics ;
D.2.8[Software Engineering ]: Metrics— Software science ;
D.2.9 [Software Engineering ]: Management— Program-
ming teams ; D.2.9[Software Engineering ]: Management—
Software quality assurance ; K.4.3[Organizational Impacts ]:
[Employment]
General Terms
Management, Measurement, Reliability
Keywords
Organizational volatility, software defects
1. INTRODUCTION
The key premise of Organization is that it allows more
eﬃcient production [20], including production of software .
Organizational design deﬁnes roles, processes, and formal
reporting relationships to improve the functioning of an or -
ganization. Ourinvestigation is focused onstudyinghowth e
Permission to make digital or hard copies of all or part of thi s work for
personal or classroom use is granted without fee provided th at copies are
not made or distributed for proﬁt or commercial advantage an d that copies
bear this notice and thefull citation on the ﬁrstpage. Tocop y otherwise, to
republish, topostonserversortoredistribute tolists,re quires priorspeciﬁc
permission and/or afee.
FSE-18,November 7–11, 2010, Santa Fe,New Mexico, USA.
Copyright 2010 ACM 978-1-60558-791-2/10/11 ...$10.00.organizational change aﬀects software development. Past
work investigating software organizations found that eﬀec -
tive organizations have very low worker turnover [7] due to
what authors call “social capital” invested in the workforc e
through culture of trust and respect, generous beneﬁts, and
recognition of importance of people’s personal lives. It is ,
therefore, of interest to quantify the beneﬁts of low turnov er
or costs of high volatility.
We have borrowed and extended measures of organiza-
tional change based on archival records as considered in,
for example, Geisler [9]. Our focus on individual develop-
ers lead us to study organization at the lowest level. We
operationalize organization for an individual developer a s
their immediate supervisor based on the formal reporting
structure1. From the individual’s perspective that we take
here, we measure three primary organizational events: the
arrival of new colleagues into an organization; the departu re
of colleagues from the organization; and the change of the
developer’s organization.
The primary positive results of organizational volatility
would be the innovation brought by incoming people [19].
The negativeside involvestheoverheadfor theexistingtea m
to train new developers, the initial lack of experience of ne w
developers, and the gaps in tacit knowledge produced by
the departure of experienced developers. While the eﬀec-
tiveness of an organization is sometimes considered throug h
the eﬀectiveness of collaborative work, we are looking at th e
eﬀects of organizational change at a personal level. After
all, collective strategies can only improve upon the indivi d-
ual performance, but if the individuals are not performing,
the collective strategies are not likely to help.
The primary eﬀect of an organizational change on prod-
uct quality is likely to come through the volatility’s impac t
on an individual developer and on the loss of expertise that
may lead to otherwise preventable mistakes. Establishing
a relationship between the organizational change and prod-
uct quality faces several challenges. First, product qual-
ity has been shown to be aﬀected by a number of factors.
Therefore, we adjust for the factors known to aﬀect soft-
ware quality before investigating the potential impact of o r-
ganizational change. Second, the organizational change an d
product quality may be simultaneously inﬂuenced by exter-
nal factors reﬂecting, for example, the business environme nt
at a particular time. To alleviate this problem we investi-
1We also considered alternative operationalizations based
on organization’s name, and they lead to similar results as
noted in the text.gate the quality of diﬀerent parts of a system over diﬀerent
time periods.
We propose three individual-centric measures of organiza-
tional volatility: proximity to organizational change; ar rival
of new colleagues; and departure of existing colleagues. We
formulate hypotheses describing the eﬀects of volatility o n
fault proneness and evaluate them using a large software
project involving several hundred developers.
The work contributes in scientiﬁc, methodological, and
practical dimensions.
1. From the scientiﬁc perspective we propose and evalu-
ate measures of organizational change, establish that
they increase the probability of defects in our con-
text, andreproduceresultsfromseveral priorempirical
studies of software quality.
2. Fromthemethodological perspectivewepropose away
to evaluate diﬀerent parts of the system at diﬀerent
moments in time to reduce or eliminate the impact the
business environment might have on a single release
and propose the need to evaluate the impact of novel
factors such as organizational change and distributed
development only after adjusting for factors that have
been shown to aﬀect software quality.
3. From the practical perspective we quantify the eﬀects
oforganizational changeontheprobabilityofcustomer-
reporteddefectsandestablishtheirrelativeimportance
in comparison to predictors derived from code, change,
social structure, and geographic distribution. This can
guide further investigations and quality improvement
eﬀorts towards areas of software and software devel-
opment that are the biggest contributors to software
defects.
We start by discussing the related work in Section 2 and
present the context of the study and data collection in Sec-
tion 3. The hypotheses are presented in Section 4, organiza-
tional change measures in Section 5, operationalizations o f
software quality predictors in Section 6, and results in Sec -
tion 7. Limitations of the study are outlined in Section 8
and conclusions are presented in Section 9
2. RELATED WORK
A substantial number of research- and practice-oriented
publications are devoted to organizational change. While
much of it is action oriented and prescriptive, little is de-
voted to methods to measure organizational volatility from
archival data, despite Webb and colleagues [21] raising the
issue as early as 1966 well summarized bya Chinese proverb:
“the palest ink is clearer than the best memory.” A notable
exception is the work of Geisler [9] that proposes a number
of organizational change measures based on archival record s.
Unlike the measures we use that are based on human re-
source systems, Geisler’s work required a manual search
through archived documents.
A particular form of volatility related to downsizing has
received substantial attention. In particular, a study of m id-
level managers and executives in Canadian civil service by
Armstrong-Stassen [3] found that the downsizing has been
associated with a reduction in productivity of the survivor s
(individuals that remain), an increase of their workload de -
mands, an increase in escape coping strategies, and an in-
crease in the incidence of health problem symptoms. The
study used four surveys — one prior to, two during, and
one after the downsizing over a three year period. Thefour survey response rates were 58, 46, 44, and 43 percent
correspondingly. The analysis distinguished control-bas ed
coping strategies that included positive thinking (recast ing
the downsizing event as a challenge), direct-action (focus ing
more energy on your job), and instrumental support (seek-
ing information from others), and escape coping strategies
that included the avoidance of the situation and the disen-
gagement (putting less eﬀort into work and spending more
time on activities outside work). Other empirical evidence
also shows that the downsizing is associated with decreased
job performance, for example, Amabile and Conti [1]. In
addition to the focus on software developers, unlike in prio r
work, we also look at more general types volatility than a
simple downsizing. Furthermore, we look at a full sample
of developers without an inconvenience of high non-respons e
rates and our observations cover the entire period without
restrictions to survey administration times.
The most relevant work on software quality is a multiple
case study by Cataldo and colleagues [6] that, unlike pre-
vious studies, has quantiﬁed relative contribution of mod-
ule size, and syntactic, logical, and social factors. It fou nd
that customer reported defects in two distinct systems (not
related to the system under study) are predominantly ex-
plained by logical coupling (deﬁned by Modiﬁcation Re-
quests (MRs) touching several ﬁles), followed by social cou -
pling (deﬁned via workﬂow on resolving MRs) of the de-
velopers making changes to that ﬁle. Little or no impact
on customer reported defects was explained by call or data
ﬂows. We adjust for predictors used by Cataldo et al. in
the model before adding factors indicating organizational
volatility. We also report the relative contributions of pr e-
dictors in explaining the observed variance in quality.
Static organizational factors have been shown to aﬀect
software qualityin, forexample, Nagappanandcolleagues [ 17].
Authors constructed eight measures related to organizatio n
forWindowsServerbinaries, ﬁtalogistic regression model to
predict failure proneness, and found that its predictions a re
similar or better than for other models involvingcode churn ,
code complexity, call and data ﬂow, code coverage, and pre-
release bugs. Unlike work by Cataldo and colleagues [6] it
does not combine information from code and organization
into a single model or even report the direction or size of
the impact of individual predictors.
Finally, we also consider factors related to distributed de -
velopment. In particular, Mockus [14] showed that work
transferred across locations halved the productivity. How -
ever, Bird and colleagues [4] found that the quality of bi-
naries in Windows Vista was not related to oﬀshore devel-
opment, though it also does not combine information from
code and organization into a single model.
3. CONTEXT AND DATASOURCES
We investigate a large switching software development
projectinAvayawithseveralhundreddevelopers. Theprod-
uct is the call processing software installed on many Avaya
telephony systems. It embodies several decades of knowl-
edge and experience in the telephony ﬁeld. In a recent re-
lease, the software contains approximately nine million li nes
ofcodemostly inCandC++. Thesoftware developmentor-
ganization deploys major releases on a ﬁxed schedule, with
subsequent minor releases that bundle patches and reﬁne-
ments to the system.
Two primary sources of data were utilized in the study.The changes to the source code were obtained from the
(custom-built) change management system that has been
used over more than eighteen years. The data were cleaned
to eliminate the administrative changes (changes made for
the purpose other than to enhance or ﬁx the product). For
example, the initial deltas for branches that do not change
the source code were excluded. The data cleaning and vali-
dation was done to support project measurement and prior
studies, for example, [15] and, therefore, only the essenti al
details are described here. In addition to version control
data we also obtained information from problem tracking
and customer support systems to ﬁnd changes related to
customer reported issues and to calculate developers’ work -
ﬂow relationships graph based on the tasks they transfer to
each other. The workﬂowgraph was constructedbytracking
the ownership history of each modiﬁcation request (MR).
The nodes of the graph represent individuals. MR’s his-
tory of ownership contains timestamp-individual-attribu te
tuples representing individuals modifying MR’s attribute s,
e.g, making status or priority changes and assignments. The
timestamp-developer-attributetuplesare chronological ly or-
dered for each MR and any two individuals immediately
subsequent in this order are represented by a link in the
workﬂow graph.
The second source of data was an organizational database
(POST) that lists individuals, their organization, and the ir
contact information. We had collected frequent snapshots o f
this data over a period of seven years. The primary purpose
of this data was to establish the identity of each developer
and to calculate measures of organizational volatility. As
with any other source of data, it had its share of anomalies
and issues. First, developer logins were not always identic al
to email handles in POST. Second, logins have changed over
time for some developers because a recent policy required lo -
gins to match email handles. Third, the email handles and
even organizational IDs have changed for some developers.
For example, oﬀshore developers who started at a US lo-
cation and later went back to their permanent oﬀshore site
got a new ID at their home site. To deal with these issues
we used a NIS database (snapshots of which we have also
collected over seven years) that mapped login to the orga-
nizational ID and the full name of the person authorized
to use the login. This extra piece of information allowed
us to establish the identities of developers over time despi te
changes in the organizational IDs, email handles, and some-
times even names (for example, as a result of a marriage).
We have used these sources of data to map logins and orga-
nizational IDstouniquenumericIDsidentifyingeachparti c-
ipant. These unique IDs were then substituted for logins in
the code change data and for organization IDs in the POST
data to normalize identity information and to provide more
privacy (some developers could be recognized from their lo-
gin). While there were more than ten thousand entries in
POST over the considered period, only around two thou-
sand of them were developers who modiﬁed the source code
in Avayaand 480 of them have modiﬁed code in the product
we investigate.
It’s worth noting the way in which we have prepared these
data sources that were obtained in June, 2009. The ver-
sion control and conﬁguration management data goes back
to 1993, and the organizational reporting structure back to
2001. Forouranalysis we reconstructthestate ofthedataat
each moment in time (at the resolution of calendar weeks).For example, the size of a ﬁle at time tis calculated as the
size of the ﬁle after the last modiﬁcation during the week
that includes time t. If the ﬁle was not modiﬁed during that
week, the size of the ﬁle after the ﬁrst change preceding that
week is used. Similarly, logical and workﬂow dependencies
for a ﬁle at time tare calculated based only on the historic
information that was available at time t. Exact calculations
are shown in Table 2.
Furthermore, to capture the concept that the activities
performed on a ﬁle may lead to introduction of defects at
a later time we separate the data into the 12-month mea-
surement period and the immediately subsequent 12-month
prediction period . We use the prediction period only to mea-
sure the customer reported defects, while the measurement
period(and, for some predictors as noted in Table 2, all prior
history) is used to obtain quality predictors.
The innovative feature of the division into prediction and
measurement periods is that these periods are chosen sep-
arately for each ﬁle to avoid being biased by peculiar cir-
cumstances of a single software release. More details are in
Sections 8.
4. HYPOTHESES
To hypothesize the eﬀects of organizational change on
software quality we borrow from a variety of theoretical
frameworks and empirical results. In general we would like
to observe if in our context the following holds:
Proposition 1.Organizational volatility reduces the qual-
ity of the software product.
This outcome would be expected based on a variety of re-
search results discussed in Section 2. More speciﬁcally,
Proposition 2.New experienced members would bring
innovations and, therefore, ﬁnd new ways to improve quality .
The manner in which the volatility aﬀects quality depends
on a variety of factors and theoretical assumptions. In par-
ticular, the arrival of new experienced members is likely to
increase quality through innovations they may bring [19].
Proposition 3.New inexperienced members would be
more likely to introduce defects.
As was shown in, for example, [16], changes done by de-
velopers with less experience were more likely to introduce
a defect. New developers would be less familiar with the
system and more likely to make mistakes leading to defects.
Proposition 4.Outgoing members would leave gaps in
the tacit knowledge, making suboptimal design and imple-
mentation decisions more likely by the remaining team. This
would increase the probability that defects will be introdu ced
or not found prior to release.
The departure of experienced members may leave gaps in
the tacit knowledge, see, for example, Nonaka [18]. Such
gaps, may lead the remaining team members to make sub-
optimal or even disastrous design and implementation deci-
sions. Furthermore, if such departures are related to unfa-
vorable business conditions and downsizing, that may exert
additional stress on the remaining employees thus reducing
their performance for reasons discussed in Section 2.
Factorsthatwereshownto affectdefects.
Even though our main focus is to establish the relation-
ship between organizational change and software defects, w e
included a variety of predictors known to aﬀect softwareTable 1: Concepts and their operationalizations.
Concept Operationalization
Proximity in time to the orga-
nizational changeTime (in years) until the next and after the last change in the organization ID
Size of the reorganization Number of employees leaving the organization over past two m onths
New recruits Number of employees entering the orga-
nization over past two months
Size of the organization Number of employees within the organization
Other factors Product, Location, Organization ID, Developer ID
quality to ensure that organizational volatility is not con -
founded with any of these predictors. Each predictor has an
associated hypothesis of how it may aﬀect software quality.
We present only the essential reasoning because the detaile d
reasoning for each may be found in prior work and is beyond
the scope of the paper. More discussion is in Section 6
Proposition 5.Larger ﬁles will have lower quality.
Size is the most commonly used predictor in quality studies
and is invariably associated with lower quality (all other
factors being equal).
Proposition 6.Files modiﬁed by diﬀused changes and
ﬁles with high logical coupling will have lower quality.
Change diﬀusion (the number of modules or ﬁles a change
modiﬁes) was found tobe associated with an increased prob-
ability that a change will contain a fault [16]. Logical cou-
pling of the ﬁle (ﬁle being changed together with other ﬁles) ,
often indicates non-explicit dependencies among ﬁles and
have been shown to be related to the increase in probability
of defects in, for example, Cataldo et al [6].
Proposition 7.Files modiﬁed by developers who have
complex workﬂow will have lower quality.
High numbers of edges in the workﬂow network of develop-
ers working on the ﬁle was associated with increased num-
ber of defects in, for example, Cataldo et al [6]. It is not
unreasonable to assume that the size of developer’s social
network (found to increase the defect prediction accuracy
by Bird et al [5]) is also positively associated with defects
in Windows binaries, even though the direction of the ef-
fects or the explanatory power of the predictors were not
presented. Finally, an association between large in-degre e
of a workﬂow network and lower productivity was reported
in, for example, [11].
Proposition 8.Files modiﬁed by developers with low project
experience will have lower quality.
Low project experience of developers working on the module
was associated with the higher probability of defects in, fo r
example, [16]. This is also related to Proposition 4.
The following propositions consider factors that have not
been found to reduce software quality but were related to
other adverse outcomes.
Proposition 9.Files modiﬁed by developers from multi-
ple development sites will have lower quality.
Distributeddevelopmentwas associated withlonger MRres-
olution times in, for example, Herbsleb and colleagues [12] .
Bird and colleagues [4] compared binaries modiﬁed from
multiple sites in Microsoft and found a nearly signiﬁcant
relationship between the oﬀshore location and higher num-
ber of defects. Often developers in oﬀshore sites have less
experience with the legacy code or system than developersin sites that have originally created it. The issues of coor-
dination may also increase the potential for defects to be
introduced in distributed development contexts.
Proposition 10.Files modiﬁed by changes that are in-
corporated into multiple releases will have lower quality.
Suchmodiﬁcationsindicatethattheissuecausingthechang e
is serious enough to aﬀect multiple releases. It also indi-
cates additional dependencies associated with changingco de
in multiple releases simultaneously. Therefore, ﬁles that
contain more such modiﬁcations may be more fault-prone.
The relationship between changes aﬀecting multiple releas es
and defects has not been investigated but Herbsleb and col-
leagues [11] found that such MRs have longer cycle times.
5. MEASURES OF ORGANIZATION AND
ITS CHANGE
WefollowedGeisler [9]inmeasuringorganizational change .
However, our data and our hypotheses were substantially
diﬀerent, therefore the operationalizations of the measur es
also have little resemblance. One of the factors that we con-
sidered was the reporting hierarchy of the organization to
capture changes in the organizational structure brought by
a reorganization. The staﬀ reductions discussed above are
also often associated with the reorganization.
The measures we propose and evaluate are calculated for
developer-time pair ( l,t) and are listed in Table 1. For ex-
ample, Armstrong-Stassen [3] found that the problems asso-
ciated with the change were transient and did not manifest
themselves either well before or well after the downsizing.
Therefore, ourﬁrstmeasure is theproximitytotheorganiza -
tional change. Denote O(l,t) to be a function that speciﬁes
the organization for Developer lat Time t. We operational-
ize proximity to the organizational change in two ways:
1. Time (in years) until the subsequent change
Pnext(l,t) = argmin
s>0O(l,t+s) :O(l,t+s)/ne}ationslash=O(l,t)
2. Time (in years) from prior change
Pprior(l,t) = argmin
s>0O(l,t−s) :O(l,t−s)/ne}ationslash=O(l,t)
Wealsoanticipate thattheextentofthereorganization mea -
sured by the number of people coming in and leaving the or-
ganization should have eﬀects described in Propositions 2,
3, and 4. The measure counting the newcomers is needed
to investigate Proposition 3, and the number of employees
leaving the organization is needed for Proposition 4. In par -
ticular, for Developer lat Time t, the number of colleagues
who left the organization is calculated as L(l,t) =ℵ{p:
O(p,t−δ) =O(l,t)∧O(p,t)/ne}ationslash=O(l,t)}, wherepdenotes a
person,ℵdenotes cardinality, and δrepresents two months.
The number of newcomers is obtained in a similar manner
N(l,t) =ℵ{p:O(p,t−δ)/ne}ationslash=O(l,t)∧O(p,t) =O(l,t)}.The organizational unit was operationalized in two ways:
through a supervisor ID and through an organizational ID.
The results were similar for both of these operationaliza-
tions. In Table 1 we present only one of the operational-
izations. Alternative operationalization may be obtained by
replacing words“supervisor ID”by“organizational ID.”
In addition to measures needed to test our proposition
we need to adjust for the systematic variations among de-
veloper organizations. Therefore, we look at the number of
employees in the organizational unit developer belongs to.
The size of the organization for a developer lat time tis
simplyS(l,t) =ℵ{p:O(p,t) =O(l,t)}.
6. QUALITY PREDICTORS
When testing a hypothesis about a phenomenon it is im-
portant to account for factors that are known to have an
impact. For example, if we study how organizational change
aﬀects software quality, the model needs to contain the soft -
warecomplexityandotherpropertiesaﬀectingsoftware qua l-
ity. Otherwise, if the organizational change happens to be
high only for the most complex parts of software, we may
end up ascribing eﬀects of software complexity to organi-
zational change. At the same time, we can not include all
possible predictors in a model, because many predictors are
strongly correlated among themselves. From the practical
perspective, we want to quantify the relative impact of an
entire class of quality predictors to understand the nature
of the quality problems and the extent to which they can be
addressed by improvements in that dimension. Therefore
we group quality predictors into several classes and pick a
subset of predictors in each class that are not strongly cor-
related among themselves.
The choice of predictor classes was based on eﬀects re-
ported in the literature and, in particular, based on the sev -
eral prior studies we sought to reproduce. In particular,
they mirror the hypotheses in Section 4. We have grouped
quality predictors into code, change, developer, organiza -
tion, and geographic distribution predictors. The predict ors
shown in Table 2 and described below were calculated at the
individual ﬁle level based on the changes made in the past
12 months ( the measurement period ). The response was an
indicator that at least one defect for that ﬁle was reported
by a customer in the subsequent 12 months ( the prediction
period). Because the measures (for example, the ﬁle size) of-
ten vary over the 12-month period, we used the largest value
observed over that period. While code-derived measures are
calculated directly at the ﬁle level, other measures were ag -
gregated over developers or changes as noted. Instead of
averaging, we used the maximum observed value over the
changes (and developers making changes) during the 12-
monthmeasurement period . The primary reason for choos-
ing extreme instead of average values was that“a rotten ap-
ple spoils the barrel.” In other words, a single risky change
or an inexperienced developer may increase the chances of
introducing a defect.
Organization-derivedmeasures.
Nagappan and colleagues [17] have used several novel pre-
dictors based on organizational structure. The basic premi se
of these predictors was the organizational ownership of the
part of a system for which defects are predicted. Owner
organization was deﬁned as the organization lead by the
lowest-levelsupervisorwho’ssubordinatesmadeatleast7 5%
of the modiﬁcations to a ﬁle. The level of that supervisor(the length of the reporting chain from the top executive to
that supervisor of the owner-organization) was referred to
as “Depth of Master Ownership (DMO)”in [17] and it was
argued that the greater depth allowed the organization to
focus on the code, speed-up the decision making, and allow
intellectual control leading to fewer defects. It also prop osed
a number of other predictors based on the concept of organi-
zational ownership. For our data none of them were statisti-
cally signiﬁcant predicting ﬁles with defects after adjust ing
for code, change, and social factors, so we omit a more de-
tailed description of these measures. It is considered in mo re
detail in Section 7.3.
More generally, the concept of organizational ownership
of a ﬁle appears to be somewhat limited. After all, individ-
ual developers are making changes to a ﬁle, and organiza-
tion aﬀects code only through these developers, not directl y.
Thus, we tried a variety of developer-centric organization al
measures. To account for the size of the organization, we
used the size of the organization deﬁned by the immediate
supervisor of a developer who made changes to the ﬁle.
The organizational change measures we chose were based
on Propositions 2 and 4 and are described in Table 1. Be-
cause there were multiple developers (each often making
multiple changes), we calculated the maximum of each mea-
sure over all developers making changes during the mea-
surement period . We made exception for the measures of
proximity to the organizational change (where we use the
smallest value) because of the evidence suggesting the tran -
sience of the eﬀects of such changes as suggested by, for
example, Armstrong-Stassen [3].
Code-derivedmeasures.
Code size and complexity, as well as syntactic dependen-
cies arising from call- and data-ﬂowgraphs are typicalexam -
ples of source code measures. Larger and more complex ﬁles
were more likely to have a defect in numerous prior studies.
There are a number of object-oriented predictors that were
shown to correlate with defects (see, e.g., [2]), but the pro d-
uct under consideration uses C language. We have chosen
the number of non-commentary source code lines (LOC) in
a ﬁle to use as the code related predictor. The other code-
derived measures, such as call- and data-ﬂow and various
code complexity measures were highly correlated with ﬁle
size, therefore we have not included them in the model to-
gether with LOC.
Change-derivedmeasures.
Logicaldependenciesarerepresentedbychangesthatmod-
ify several ﬁles simultaneously and have been found to ex-
plain the most variance in the observed defects (see, for ex-
ample, [6]). The basic assumption underlying this measure
is that a logical change requiring modiﬁcations to more than
one ﬁle implies that the decisions involving that ﬁle depend
in some way on the decisions involving other ﬁles. It is sim-
ilar to logical coupling discussed by Gall and colleagues [8 ].
We used the simplest logical dependency measure: the num-
ber of other ﬁles that have been modiﬁed together with the
ﬁle being measured. The increase in logical dependencies
has been shown to increase the fault proneness, therefore we
expect the same outcome in our study as stated in Proposi-
tion 6.
In the product under consideration, changes could be sub-
mitted to multiple releases. The product supported multipl e
releases in the ﬁeld in addition to at least two releases unde rTable 2: Quality predictors. Tuple (l,f,mr,t,r )is an abbreviation for “File fwas modiﬁed by Developer lat
Timetas a part of MR mrand that MR was submitted to Release r.” Tuple (l1,l2,t)denotes an edge in the
workﬂow graph constructed prior to t.Themeasurement period is[t−1,t].
Class Predictor Description
Size of organization The maximum size of the organization over developers modify ing the ﬁle during
themeasurement period : max l:(l,f,t−1≤to≤t)S(l,to)
Time from prior change The minimum time from the prior organizational change over d evelopers modifying
the ﬁle during the measurement period : minl:(l,f,t−1≤to≤t)Pprior(l,to)
Organization Time until next change The minimum time until the next organizational change over d evelopers modifying
the ﬁle during the measurement period : minl:(l,f,t−1≤to≤t)Pnext(l,to)
Number leaving org. The maximum number of people leaving organizations over dev elopers modifying
the ﬁle during the measurement period : max l:(l,f,t−1≤to≤t)L(l,t)
Number of newcomers The maximum number of people coming into organizations over developers modi-
fying the ﬁle during the measurement period : max l:(l,f,t−1≤to≤t)N(l,t)
File LOCLines on non-commentary source code (maximum over the measurement period )
Logical Deps. The number of other ﬁles changed by the past MRs modifying the ﬁle:LD(f,t) =
ℵ{fo:∃mr,∃t1,t2≤t,(fo,mr,t 1)∧(f,mr,t 2)}
Change Release Deps. The maximum number of releases an MR is submitted to over MRs m odifying the
ﬁle during the measurement period :R(f,t) = max mrℵ{r:∃to≤t,(r,mr,t o)}
Change Diﬀusion The maximum number of ﬁles changed by an MR modifying the ﬁle d uring the
measurement period :D(f,t) = max mrℵ{fo:∃to≤t,(fo,mr,t o)}
Workﬂow The maximum degree of the workﬂow network over developers mo difying the
ﬁle during the measurement period :W(f,t) = max lℵ{lo:∃t1≤t,∃t2∈
[t−1,t],(l,f,t2)∧(lo,l,t1)}
Social Years of prj. experience Theminimumoftheyears ofexperienceoverall developersmo difyingtheﬁleduring
themeasurement period .
Distributed development The number of sites that modiﬁed the ﬁle during the measurement period
Geography Mentor Oﬀshore The maximum of the indicator that a mentor is in another site o ver developers
modifying the ﬁle during the measurement period
development. Defect ﬁxes had to be submitted to all still-
maintained releases the defect aﬀects. It is reasonable to
assume that defects aﬀecting multiple releases may be more
important than defects aﬀecting only a single release. This
release-dependency appears to be diﬀerent from logical de-
pendencies described in the prior paragraph and, therefore ,
may serve as another measure of fault proneness. The ﬁx
implemented on a ﬁle during the measurement period that
aﬀected the largest number of releases was used to obtain
the number of releases.
The change diﬀusion (the number of ﬁles modiﬁed by a
change) was shown to be a good indicator of the probability
that a software patch will fail in [16]. Unlike logical depen -
dencies that are calculated over the entire history of the ﬁl e,
we look only at changes made during the measurement pe-
riodand pick the one with the largest diﬀusion. The change
diﬀusion is used to generate logical dependency measure,
but it captures more transient events. A particularly com-
plex change involving numerous ﬁles that was done over the
measurement period may increase the probability of a defect
during the subsequent prediction period .
Developer-derivedmeasures.
Workﬂow (social-network) measures provide information
about coordination needs (see, e.g., [11]). Workﬂow predic -
tors reﬂect the task assignment of developers working on
the code (see, for example, [6]) and are typically calculate d
through a history of individuals creating, assigning, reso lv-
ing, and conﬁrming a defect or all individuals who partic-
ipate in modifying the defect status, including comments.We use the simplest predictor: the number of other individ-
uals that a developer working on a ﬁle has interacted with
while resolving defects. The wider network of interactions
was associated with increased fault proneness in [6], there -
fore we expect a similar outcome in our study.
Increased developer experience reduced the probability
that a patch will fail in [16]. We measured developer ex-
perience by calculating the total time in years since the de-
veloper made the ﬁrst code modiﬁcation in the project. We
expect that the increase in developer experience would re-
duce the fault proneness. We used the developer with the
lowest experience who made changes to a ﬁle during the
measurement period .
Geography-distribution-derivedmeasures.
As with developer-derived measures for each source code
ﬁle we also derive geographic distribution measures based o n
developers who modiﬁed that ﬁle. We count the number of
distinct geographic locations for developers modifying a ﬁ le
during the measurement period . We expect it to increase the
fault-proneness because of potential coordination proble ms
among developers in multiple locations.
Because the 12-month measurement period may not fully
reﬂect the history of the ﬁle we also obtain mentors for each
developer making changes during the measurement period .
The mentors for each developer (follower) are obtained usin g
the method described in [14]. The basic idea is to ﬁnd devel-
opers who have modiﬁed the most similar set of ﬁles the fol-
lower has modiﬁed, butearlier. As reportedin [14], the case s
of mentorship crossing geographic boundaries have roughlyTable 3: Summaries of the model predictors. ±indicates (un)supported proposition or (un)reproduced st udy.
The eﬀects column shows increase in the modeled probability of defects from the doubling (adding 1 if 0) of
the median value for that predictor while keeping the remain ing predictors at their median values.
Class Predictor Eﬀect Propstns Reproduced Min. 1st Qu. Median Mean 3rd Qu. Max.
Size of org .38% control +[17] 1 11 22 21.53 37 225
From prior (yrs) -15% +1 new 0 0.38 1.11 1.26 2.42 7.58
Org. chng Until next (yrs) -4% +1 new 0 0.31 0.77 0.71 1.55 5.49
Left 26% +4 new 0 0 1 1.16 3 149
Newcomers N/A −3,2 new 0 0 0 0.48 1 146
File LOC 34% +5 +various 1 86 145 157.06 261 43914
Logical Deps .11% +6 +[6, 5] 0 0 1 3.42 18 2627
Change Release Deps .192% +10 +[11] 1 1 1 1.65 2 9
Change Diﬀusion 6% +6 +[16] 1 144 614 458.90 1928 4419
Social Workﬂow Deps .35% +7+[6, 5, 11] 0 34 148 73.96 262 293
Experience (yrs) 18% +8 −[16] 0 0.73 3.60 3.35 9.37 15.78
Geo. Distributed 15% +9−[4],+[12] 0 0 0 0.17 0 1
Mentor oﬀshore 69% +9 new 0 0 0 0.03 0 1
halved the productivity of the follower as compared to in-
stances where mentorship did not cross geographic bound-
aries. Itis reasonable toexpectthatthedropinproductivi ty
may have something to do with the less eﬃcient transfer of
expertise to an oﬀshore location. Therefore it is reasonabl e
to expect that this reduced expertise would also have a neg-
ative impact on quality.
7. RESULTS
First we present the model quantifyingthe relative impact
of organizational change and other factors on the probabil-
ity of customer reported software defects. Then we provide
basic summaries of the predictors used in the model and,
ﬁnally, we discuss issues we encountered while trying to re-
produce prior studies on software quality.
7.1 Customer-reported defect model
Table 4 presents the estimated logistic regression coeﬃ-
cients, their standard errors, p-values, and deviances (th e
part of the observed variance explained by the coeﬃcient)
for the model predicting defects reported by customers over
the 12 month prediction period based on changes made to a
ﬁle over the preceding 12 month measurement period . Ta-
ble 3 summarizes model predictors, the increase in proba-
bility from doubling the median value of the predictor, the
evidence for our 10 propositions, and the reproducibility o f
prior work.
Organizationalchangemeasures.
Shorter times until the next or since the last organiza-
tionalchangeareassociatedwithhigherfaultproneness(F P)
(in Table 4) supporting Proposition 1. Doubling of the me-
dian values decreases the probability of defects by 15% and
4% correspondingly (in Table 3).
The only predictor that is not statistically signiﬁcant is
the number of newcomers to the organizations, thus Propo-
sitions 2 and 3 do not get support.
It appears that the number of individuals leaving the or-
ganization may create gaps in knowledge reducing quality
as suggested in Proposition 4. This predictor also explains
a substantial amount of variance in FP and its doubling (to
two) increases FP by 26%.
The increase in the size of the organization the devel-
oper belongs to is associated with lower quality, presumabl ybecause of the diﬃculties of coordination, slower decision
making, and reduced intellectual control as hypothesized i n,
e.g., [17]. However, organizational predictors as a group e x-
plain only two percent of the variance after adjusting for
other factors. The doubling of organization’s size from the
median value (to 44) increases FP by 38%.
Because Propositions 2 and 3 provide conﬂicting predic-
tions about the impact of newcomers this result is not par-
ticularly surprising. Furthermore, the one year prediction
periodmay be insuﬃcient for newcomers to be allowed to
make signiﬁcant enough modiﬁcations that may adversely
aﬀect customers.
Changeandﬁlemeasures.
The large ﬁles are more fault-prone, supporting Proposi-
tion 5. Change-related measures have the largest impact
on fault proneness explaining 26% of the variance (more
than the remaining predictors combined) supporting Propo-
sition 6, and 10 and reproducing results in [6, 16, 11]. No-
tably, doublingthenumberofreleases anMRissubmittedto
from the median value of one almost triples the FP. The ﬁles
that are touched with many other ﬁles, that contain changes
submitted to multiple releases, and that contain changes
spanning many ﬁles are more likely to be fault-prone. The
only surprise is that the release dependencies appear to be
even more important than logical dependencies.
Workﬂowmeasures.
Increase in developers’ workﬂow complexity increased the
fault proneness supporting Proposition 7 and reproducing
results in [5, 6]. Doubling of the median workﬂow increases
the modeled FP by 35%. The workﬂow impact is somewhat
smaller (it explains less than two percent of the variance)
than observed in [6], perhaps reﬂecting the much larger size
of the project than projects considered in [6]. The code
complexity and logical coupling may take a more signiﬁcant
role for older and larger projects. Even though we expected
the developer experience to be associated with lower fault-
pronenessasstatedinProposition8(thepropositionwas no t
supportedandtheresultin [16] was notreproduced), theop-
posite result is not particularly surprising. In many proje cts
more experienced developers are assigned to more critical
and diﬃcult tasks. For example, Zhou and colleagues [22]
quantiﬁed how fast developers move toward more criticaltasks over time. Thus, the high experience of developers
who modify a ﬁle may simply reﬂect the diﬃculty, complex-
ity, and fault-proneness of that ﬁle.
Geographicdistributionmeasures.
Both, the number of geographic locations and the remote-
ness of developer’s mentor increased fault proneness, sug-
gesting that, at least in this study, the geographic distri-
bution has a negative impact on software quality. Thus we
get support for Proposition 9 and support for MR cycle-
time study [12]. However, we could not reproduce Bird and
colleagues’ [4] work reporting no impact of distributed de-
velopment on software quality. The geographic distributio n,
while signiﬁcant, explains less than one percent of the vari -
ance after adjusting for other factors (7% of the variance
before the adjustment). The work from multiple sites in-
creases fault-proneness by 15% and having a remote mentor
by 68% as shown in Table 3.
7.2 Summaries ofpredictors
Table 3 presents summaries of the predictors used in in
Table 4. All except the two indicator variables are trans-
formed via natural logarithms (by ﬁrst adding 1) in the
model. Therefore, for the transformed variables Table 3
presents geometric means: eMean(log( X+1))−1. The median
ﬁle size is 145 non-commentary source code lines and me-
dian logical dependencies are with just one other ﬁle. While
one MR is included in nine diﬀerent releases (and aﬀects
36 ﬁles), more than half of the ﬁles have been modiﬁed by
MRs included in at most one release. The median of the
maximum diﬀusion is quite high: 614. The median of the
maximum workﬂow dependencies shows that half of the ﬁles
were modiﬁed by a developer who has encountered at least
148 other developers. 17% of the ﬁles have been modiﬁed
from multiple locations and only 3% had a mentor at an-
other site. The reporting structure shows a median of 22
people in the maximum of the organization size over devel-
opers modifying the ﬁle. The median of the minimum time
from prior organizational change was 1 .1 years and to the
subsequent change was 0 .77 years. The geometric average of
the maximum number of colleagues who left over the prior
two months was 1 .16 and for newcomers was 0 .48.
Table 4: Logistic regression coeﬃcients for the ﬁle
fault-proneness. Deviances explained by each pre-
dictor are in the right column. There are 32099ﬁles,
7% of them contain defects, and 41% of the total
deviance is explained by the model.
Class Predictor Est. StdErr p-val Devnc
File log(LOC) 0.43 0.03 0.00 2450
log(Logical) 0.25 0.02 0.00 978
Chng log(Releases) 2.67 0.07 0.00 2331
log(Diﬀusion) 0.08 0.03 0.00 321
Socl log(Workﬂow) 0.43 0.05 0.00 255
log(Experience) 0.28 0.04 0.00 13
Geo Distributed 0.14 0.07 0.04 41.94
Mentor 0.53 0.12 0.00 27.97
log(OrgSize) 0.48 0.06 0.00 160
log(From) −0.40 0.07 0.00 51
Org log(Until) −0.06 0.03 0.09 6
log(Left+1) 0.33 0.04 0.00 74
log(New+1) −0.01 0.04 0.70 07.3 Reproducibility
We have reproduced the main results reported by Cataldo
and colleagues [6] on relative contribution of module size,
logical coupling, and workﬂow. We also found that the
customer reported defects were predominantly explained by
change-derived measures, though in our case the release de-
pendencies were even more important than logical depen-
dencies. Another slight diﬀerence was that the workﬂow
factors had a slightly lower impact than in the study by
Cataldo.
The measures based on the organizational ownership in-
troduced by Nagappan and colleagues [17] were not statisti-
cally signiﬁcant after code, change, workﬂow, and organiza -
tional change derived predictors were added to our model.
However, ifweincludejustthepredictorsreportedin[17], we
obtain a reasonable model with a reasonable predictive per-
formance for our project. In addition to the organizational
structure, Nagappan and colleagues [17] used the number
of past modiﬁcations and the number of developers making
modiﬁcations in the same prediction model. As discussed in
Section 8, just the number of past modiﬁcations provides an
excellent prediction results on our data (approximately 79 %
sensitivity and speciﬁcity) and in other work (see, e.g., [1 0,
13]). Therefore it is possible that the reasonable predicti on
performance in [17] also relied on these two predictors. Be-
cause the work in [17] provides no information about the
direction or the size of the eﬀects, it was not possible to
investigate reproducibility in more depth. It is worth not-
ing that measures deﬁned in [17] are quite involved, assume
static organizational structure, and appear to be somewhat
Microsoft-speciﬁc making it diﬃcult to apply them in other
contexts. For example, Depth of Master Ownership mea-
sure (described above) was deﬁned based on the reporting
distance from the supervisor responsible for the entire de-
velopment, not the company’s CEO. In Avaya, some of the
changes are made by developers in the services organiza-
tion, making the CEO the only link between them and the
developers in the product organization. Also, because our
organization was quite dynamic, we had to calculate orga-
nizational ownership measures for each week and aggregate
them for the measurement periods .
However, we feel that organizational measures do deserve
a closer look and we used some ideas proposed in [17] to con-
struct predictors that were applicable in a more dynamic
organizational structure studied by us. Instead of looking
at the level of the supervisor of the owning organization, we
looked at the longest reporting chain from that supervisor
to the leaf developer making changes to the ﬁle. The longer
that distance, the harder it would be for developers to coor-
dinate their decisions because they may need to go through
more layers of the reporting chain. The number of individu-
als in the owner organization was highly correlated ( >0.9)
to its reporting depth, so we did not consider it.
The study on prediction of patch failures [16], reported
that lower developer experience expressed as the number
of past changes to project’s code increased the probability
that a patch will fail. We observed the opposite result (even
though we use the duration of time a developer spent on
the project, not the number of past changes, both factors
were strongly correlated), perhaps reﬂecting the observat ion
that senior developers are assigned more critical tasks as
shown in [22]. However, the primary driver of patch failure
reported in [16] was the diﬀusion of code changes comprisingthe patch. We also found that the change diﬀusion increased
fault proneness.
Bird and colleagues [4] found that the quality of binaries
in Windows Vista was not related to oﬀshore development.
In contrast, we do ﬁnd that the ﬁles changed by developers
with remote mentors or by developers from multiple sites
have increased fault-proneness. It is entirely possible th at
our project was diﬀerent from Windows Vista considered by
authors. However, the basis of the claim in [4] is a fault-
proneness model which contains the number of developers
making changes to a binary and the indicator of the dis-
tributed development having a p-value of 0 .056 (not signiﬁ-
cant at 0 .05 level). If we ﬁt an identical model on our data
(we predicted for a single release to enable a direct compar-
ison to [4], otherwise the p-value was virtually zero, corre -
sponding to t-statistic of more than 9) we do ﬁnd that the
indicator of distributed development has a p-value of 0 .048
(signiﬁcant at 0 .05 level). There is no reason to assume that
such a small diﬀerence in p-values indicates contradicting
results just because of the convention that 0 .05 is a magical
number determining what is signiﬁcant. A meta-analysis of
both studies may conclude that there is a support for the
hypothesis that distributed development does increase fau lt
proneness. Furthermore, the number of developers reﬂects
multiple dimensions of software development and that may
make it diﬃcult to identify the individual contribution of
each dimension as described in the next section.
8. LIMITATIONS
While we considered the change of the organization’s ID
for a developer and the arrival and departure of other em-
ployees from that organization, there are many other types
of organizational change that may aﬀect software quality. I n
particular, we do not distinguish between a developer mov-
ing to another project versus the organization changing its
ID.
The two-month period we considered for the purpose of
counting the arrival and departure of employees, may not
be suﬃcient to assess the impact of departures as the con-
sequences may manifest over longer periods of time.
The absolute value of Spearman correlations among pre-
dictors did not exceed 0 .57 (for the correlation between the
project experience and the proximity to the prior organiza-
tional change), thus collinearity issues are not likely.
We tried to avoid release-speciﬁc eﬀects by including in-
formation about ﬁles at a variety of calendar times. There-
fore, for each ﬁle we ordered modiﬁcation times in the period
from 2004 and 2008 and chose 75-th percentile of these mod-
iﬁcation dates (to get a point in time when the ﬁle is still
beingchanged—“alive”). A release dateclosest tothat time
was then picked for the ﬁle and the measurement period was
based on the 12 months preceding the date and the response
was obtained based on the 12 months following it ( predic-
tion period ). We chose 12 month periods because most of
the work for a release is completed within a year prior to the
release date and most of the defects reported by customers
are ﬁxed during a year following the release date.
In a real prediction model we would choose the same mo-
ment in time (present) for all the ﬁles to separate the mea-
surement period and the prediction period . We, therefore,
considered how the model described in Table 4 would fare
in such a setting. To accomplish that we ﬁt it for a ﬁfty ran-
domly selected subsets of ﬁles each containing three fourth sof the entire population of ﬁles and predicting the faulty
ﬁles in the remaining one fourth of the ﬁles. Both, the aver-
age sensitivity and speciﬁcity were reasonably high (appro x-
imately 75%). It is important to note that the sensitivity
and speciﬁcity are much lower when predicting rare events
(in our case, the probability of a defect was less than 0 .07).
As we hypothesized, each release is unique in many ways.
For comparison, for the measurement period used to ﬁt the
model in Table 4, both the sensitivity and speciﬁcity aver-
aged above 84%.
It is important to note that the simplest change-derived
predictor is the number of modiﬁcations made to a ﬁle. As
was arguedbyCataldoandcolleagues [6], thenumberofpast
modiﬁcations incorporates many factors that aﬀect quality
because ﬁles that are more complex, less well designed, have
logical or other dependencies, or are more fault prone for
other reasons, are more likely to be modiﬁed. Therefore,
even though past changes can predict fault proneness [10,
17], including past changes in the model may make it im-
possible to separate individual contributions made by code ,
developer, or organizational factors. The number of devel-
opers that made modiﬁcations to a ﬁle is another obvious
predictor. The higher number of developers, the higher is
the need to coordinate among them, and that may lead to
introduction of defects. However, that number was strongly
correlated with the number of modiﬁcations to a ﬁle (0 .94
Spearman correlation). Therefore, we did not include the
number of developers making modiﬁcations to a ﬁle or the
number of past modiﬁcations in the fault proneness models,
despite their excellent predictive power. If the number of
past modiﬁcations is included as a predictor in the model,
the contributions of logical dependencies and change diﬀu-
sion become non-signiﬁcant and the estimates of the remain-
ing coeﬃcients and their p-values slightly change (but keep
their sign).
9. CONCLUSIONS
Theorganization’s volatilityas measuredbytheproximity
to an organizational change was observed to increase the
probability of customer reported defects in the context of a
large software system.
The inﬂux of new members into the organization had no
impact on software quality, possibly because the new devel-
opers are not assigned to important changes [23].
On the other hand, departures from organization were as-
sociated with the higher probability of customer reported
defects, possibly because of the gaps in knowledge and ex-
perience left by the departing members.
Thelargersizeoforganizationwasassociated withahigher
probability of defects possibly reﬂecting the increased ne ed
for coordination and reduced speed of decision making in
larger organizations.
We were able to reproduce the results from several prior
studies suggesting maturity of the empirical software en-
gineering. We found the results to be quite similar in an
exact replication (when we tried to reproduce all aspects of
the analysis) even for quite dissimilar projects. However,
some diﬀerences emerged once we assumed our perspective
of quantifying the relative size of various factors aﬀectin g
the fault proneness. For example, we found the organiza-
tional ownership measures introduced in [17] to be diﬃcult
to obtain because the organization under study was quite
dynamic and because some of the concepts did not havean obvious correspondence between the organizations in two
studies. Wealso foundthatastudyinvestigatingdistribut ed
development and fault proneness [4] reached opposite resul ts
because of a minor diﬀerence in the observed p-value.
From the research and practical perspective the main re-
sult is that organizational change is associated with lower
software quality, though the size of the eﬀects follows af-
ter change- and code-derived factors in their ability to ex-
plain fault-proneness. The dominant factors, conﬁrming
prior studies, are related to dependencies embedded in the
code through changes. It is worth noting that the project’s
abilitytoreduceorganizational volatilitymaybemuchgre ater
than its ability to reduce the complexity of the logical de-
pendencies, thusaddressingorganizational issues maybet he
easiest approach to improve quality.
It is not clear to what extent the organizational volatility
causes or is a cause of the ﬁle, change, and workﬂow proper-
ties that are associated with high fault-proneness. In part ic-
ular, the organizational change measures explain more than
20% of the variance in the fault-proneness before adjusting
for other factors. In other words, does the organizational
volatility lead to low quality code or does the low quality
code induce volatility in the organization maintaining it?
More detailed investigations of the nature of organiza-
tional change and the nature of quality reductions observed
inthisstudyarelikelytoprovidemoredetailedpractical r ec-
ommendations on ways to organize and reorganize software
development. We expect these ﬁnding to add an important
piece to a puzzle involving the understanding of how the
dynamic relationship between the product and the organi-
zation aﬀects key software engineering outcomes.
10. REFERENCES
[1] T. Amabile and R. Conti. Changes in the work
environment for creativity during downsizing.
Academy of Manegement Journal , 42:630–640, 1999.
[2] E. Arisholm and L. C. Briand. Predicting fault-prone
components in a java legacy system. In ISESE, pages
8 – 17, 2006.
[3] M. Armstrong-Stassen. Coping with downsizing: A
comparison of executive-level and middle managers.
International Journal of Stress Management ,
12(2):117–141, 2005.
[4] C. Bird, N. Nagappan, P. Devanbu, H. Gall, and
B. Murphy. Does distributed development aﬀect
software quality? an empirical case study of windows
vista? In ICSE 2009 , 2009.
[5] C. Bird, N. Nagappan, P. Devanbu, H. Gall, and
B. Murphy. Putting it all together: Using
socio-technical networks to predict failures. In ISSRE
09, Bengaluru-Mysuru, India, 2009.
[6] M. Cataldo, A. Mockus, J. A. Roberts, and J. D.
Herbsleb. Software dependencies, the structure of
work dependencies and their impact on failures. IEEE
Transactions on Software Engineering , 2009.
[7] D. J. Cohen and L. Prusak. In Good Company: How
Social Capital Makes Organizations Work . Harward
Business Press, 2001.
[8] H. Gall, K. Hajek, and M. Jazayeri. Detection of
logical coupling based on product release history. In
ICSM, pages 190–198, 1998.[9] E. Geisler. Organizational change phenomena,
managerial cognition, and archival measures:
Reconceptualization and new empirical evidence. TR
99-02, School of Business, Illinois Institute of
Technology, 1999.
[10] T. L. Graves, A. F. Karr, J. S. Marron, and H. Siy.
Predicting fault incidence using software change
history.IEEE TSE , 26(2), 2000.
[11] J. Herbsleb and A. Mockus. Formulation and
preliminary test of an empirical theory of coordination
in software engineering. In FSE’03, Helsinki, Finland,
October 2003. ACM Press.
[12] J. D. Herbsleb and A. Mockus. An empirical study of
speed and communication in globally-distributed
software development. IEEE TSE , 29(6):481–494,
June 2003.
[13] T. M. Khoshgoftaar and N. Seliya. Comparative
assessment of software quality classiﬁcation
techniques. Empirical Software Engineering ,
9(3):229–257, September 2004.
[14] A. Mockus. Succession: Measuring transfer of code
and developer productivity. In 2009 International
Conference on Software Engineering , Vancouver, CA,
May 12–22 2009. ACM Press.
[15] A. Mockus and D. Weiss. Interval quality: Relating
customer-perceived quality to process quality. In 2008
International Conference on Software Engineering ,
pages 733–740, Leipzig, Germany, May 10–18 2008.
ACM Press.
[16] A. Mockus and D. M. Weiss. Predicting risk of
software changes. Bell Labs Technical Journal ,
5(2):169–180, April–June 2000.
[17] N. Nagappan, B. Murphy, and V. R. Basili. The
inﬂuence of organizational structure on software
quality. In ICSE 2008 , pages 521–530, 2008.
[18] I. Nonaka. A dynamic theory of organizational
knwledge creation. Organizational Science , 5(1):14–37,
February 1994.
[19] J. Van Maanen and E. Schein. Towards a theory of
organizational socialization. In B. Staw, editor,
Research in organizational behavior , volume 1, pages
209–264. JAI Press, Greenwich, CT, 1979.
[20] F. W. T. F. W. The Principles of Scientiﬁc
Management . Harper & Brothers, 1911.
[21] E. Webb, D. Campbel, R. Schwartz, and L. Sechrest.
Unobtrusive Measures: Nonreactive Research in the
Social Sciences . Rand McNally Colledge Publishing
Company, Chicago, IL, 1966.
[22] M. Zhou and A. Mockus. Developer ﬂuency:
Achieving true mastery in software projects. In ACM
SIGSOFT / FSE , Santa Fe, New Mexico, 2010.
[23] M. Zhou, A. Mockus, and D. Weiss. Learning in
oﬀshored and legacy software projects: How product
structure shapes organization. In ICSE Workshop on
Socio-Technical Congruence , Vancouver, Canada, May
19 2009.