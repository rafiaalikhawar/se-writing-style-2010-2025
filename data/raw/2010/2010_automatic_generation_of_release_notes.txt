Automatic Generation of Release Notes
Laura Moreno1, Gabriele Bavota2, Massimiliano Di Penta2,
Rocco Oliveto3, Andrian Marcus1, Gerardo Canfora2
1The University of Texas at Dallas, Richardson, TX, USA
2University of Sannio, Benevento, Italy
3University of Molise, Pesche (IS), Italy
lmorenoc@utdallas.edu, gbavota@unisannio.it, dipenta@unisannio.it,
rocco.oliveto@unimol.it, amarcus@utdallas.edu, canfora@unisannio.it
ABSTRACT
This paper introduces ARENA ( Automatic REleaseNotes
generAtor), an approach for the automatic generation of
release notes. ARENA extracts changes from the source
code, summarizes them, and integrates them with informa-
tion from versioning systems and issue trackers. It was de-
signed based on the manual analysis of 1,000 existing release
notes. To evaluate the quality of the ARENA release notes,
we performed three empirical studies involving a total of
53 participants (45 professional developers and 8 students).
The results indicate that the ARENA release notes are very
good approximations of those produced by developers and
often include important information that is missing in the
manually produced release notes.
Categories and Subject Descriptors
D.2.7[Software Engineering ]: Distribution, Maintenance,
andEnhancement— Documentation, Enhancement, Restruc-
turing, Reverse Engineering, and Reengineering
General Terms
Documentation
Keywords
Release notes, Software documentation, Software evolution
1. INTRODUCTION
When a new release of a software project is issued, de-
velopment teams produce a release note , usually included in
the release package or uploaded on the project’s website. A
release note summarizes the main changes that occurred in
the software since the previous release, such as new features,
bug ﬁxes, changes to licenses under which the project is re-
leased, and, especially when the software is a library used
by others, relevant changes at code level.
Producing release notes by hand can be a demanding and
daunting task. According to a survey that we conducted
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
FSE’14 , November 16–22, 2014, Hong Kong, China
Copyright 2014 ACM 978-1-4503-3056-5/14/11 ...$15.00.among open-source and professional developers (see Section
4.2), creating a release note is a diﬃcult and eﬀort-prone
activity that can take up to eight hours. Some issue track-
ers can generate simpliﬁed release notes (e.g., the Atlassian
OnDemand release note generator1), but such notes merely
list closed issues that developers have manually associated
to a release.
This paper proposes ARENA ( Automatic REleaseNotes
generAtor), an approach for the automatic generation of re-
lease notes. ARENA identiﬁes changes occurred in the com-
mits performed between two releases of a software project,
such as structural changes to the code, upgrades of external
libraries used by the project, and changes in the licenses.
Then, ARENA summarizes the code changes through an
approach derived from code summarization [1, 2]. These
changes are linked to information that ARENA extracts
from commit notes and issue trackers, which is used to de-
scribe ﬁxed bugs, new features, and open bugs related to the
previous release. Finally, the release note is organized into
categories and presented as a hierarchical HTML document,
where details on each item can be expanded or collapsed, as
needed. It is important to point out that (i) the approach
has been designed after manually analyzing 1,000 project re-
lease notes to identify what elements they typically contain;
and (ii)ARENA produces detailed release notes, mainly in-
tended to be used by developers and integrators .
From an engineering standpoint, ARENA leverages ex-
isting approaches for summarizing code and for linking code
changes to issues; yet, it is novel and unique for two reasons:
(i) it generates summaries and descriptions of code changes
at diﬀerent levels of granularity than what was done in the
past; and (ii) it combines code analysis, summarization, and
mining approaches together to address the problem of re-
lease note generation, for the ﬁrst time .
We performed three diﬀerent empirical studies to evaluate
ARENA, having diﬀerent objectives: (i) evaluating the com-
pleteness of ARENA release notes with respect to manually
created ones; (ii) collecting from developers (internal and
external to the projects from which release notes were gen-
erated) opinions about the importance of the additional in-
formation provided by ARENA release notes, which is miss-
ing in the existing ones; and (iii) doing an “in-ﬁeld” study
with the project leader and developers of an existing soft-
ware system, to compare their release notes with the ones
generated by ARENA.
In summary, this paper makes the following contributions:
1. The ARENA approach to automatically generate re-
1http://tinyurl.com/atlassianRNPermission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a
fee. Request permissions from Permissions@acm.org.
FSE’14 , November 16–21, 2014, Hong Kong, China
Copyright 2014 ACM 978-1-4503-3056-5/14/11...$15.00
http://dx.doi.org/10.1145/2635868.2635870
484Table 1: Contents of the 1,000 release notes.
Content Type #Rel. Notes %
Fixed Bugs 898 90%
New Features 455 46%
New Code Components 428 43%
Modiﬁed Code Components 401 40%
Modiﬁed Features 264 26%
Refactoring Operations 209 21%
Changes to Documentation 204 20%
Upgraded Library Dep. 158 16%
Deprecated Code Components 100 10%
Deleted Code Components 91 9%
Changes to Conﬁg. Files 84 8%
Changes to Code Components Visibility 70 7%
Changes to Test Suites 73 7%
Known Issues 64 6%
Replaced Code Components 48 5%
Architectural Changes 30 3%
Changes to Licenses 18 2%
lease notes.
2. Results of three empirical evaluations aimed at assess-
ing the quality of the generated release notes.
3. Results of a survey analyzing the content of 1,000 re-
leasenotesfrom58industrialandopensourceprojects.
4. Results of a survey with 22 developers on the diﬃculty
of creating release notes.
Replication package. A replication package is available
online2. It includes: (i) complete results of the survey; (ii)
code summarization templates used by ARENA; (iii) all the
HTML generated release notes; and (iv) material and work-
ing data sets of the three evaluation studies.
2. WHAT DO RELEASE NOTES
CONTAIN?
To design ARENA, we performed an exploratory study
aimed at understanding the structure and content of exist-
ing release notes. We manually inspected 1,000 release notes
from 58 software projects to analyze and classify their con-
tent. The analyzed notes belong to 608 releases of 41 open-
source projects from the Apache ecosystem ( e.g.,Ant, log4j,
etc.), 382 releases of 14 open-source projects developed by
other communities ( e.g.,jEdit, Selenium, Firefox, etc.), and
ten releases of three industrial projects ( i.e.,iOS, Dropbox,
and Jama Contour). Diﬀerent communities produce release
notes according to their own guidelines, as industry-wide
common standards do not exist.
Release notes are usually presented as a list of items, each
one describing some type of change. Table 1 reports the 17
types of changes we identiﬁed as usually included in release
notes, the number of release notes containing such infor-
mation, and the corresponding percentage. Note that Code
Components (denotedbyCCinTable1)mayrefertoclasses,
methods, or instance variables.
Bug ﬁxes stand out as the most frequent item included in
the release notes (in 898 release notes—90% of our sample).
Typically, the information is reported as a simple bullet list
containing for each ﬁxed bug, its ID and a very short sum-
mary (often the bug’s title as stored in the bug tracker).
For example, in the release note of Apache Lucene 4.0.0, the
LUCENE-4310 bug ﬁx is reported as follows:
LUCENE-4310: MappingCharFilter was failing to
match input strings containing non-BMP Unicode char-
acters.
Otherfrequentlyreportedchangesinreleasenotesarenew
features (46%) and new code components (43%). These two
2http://tinyurl.com/fse14-arenatypes of changes are often found together, explaining what
code components were added to implement the new features.
Modiﬁed code components ( i.e.,classes, methods, ﬁelds,
parameters) are also frequently reported (40%). Note that
weincludehereallcaseswherethereleasenotesreportthata
code element has been changed, without specifying how. We
do not include here deprecated code components or changes
to code components’ visibility that are classiﬁed separately
(see Table 1).
Explanations of modiﬁed features are quite frequent in
release notes (26%) and are generally accompanied by the
code components that were added/modiﬁed/deleted to im-
plement the feature change. An example from the release
note of the Google Web Toolkit 2.3.0 (M1) is:
Updated GPE’s UIBinder editor (i.e., class UIBinder of
the Google Plug-in) to provide support for attribute auto-
completion based on getter/setters in the owner type.
Refactoring operations are also included in release notes
(21%), generally as simple statements specifying the im-
pacted code components, e.g.,“Refactored the WebDriver-
Js”—from Selenium 2.20.0.
Changes in documentation are present in 20% of the ana-
lyzed release notes, although, more often than not, they are
rather poorly described with general sentences like “ more
complete documentation has been added ”or“documentation
improvements ”. We also found: 158 release notes (16%) re-
porting upgrades in the libraries used by the project ( e.g.,
“The Deb Ant Task now works with Ant 1.7 ”—from Jedit
4.3pre11); 100 (10%) reporting deprecated code components
(e.g.,“The requestAccessToAccountsWithType: method of
ACAccountStore is now deprecated ”—from iOS 6.0); and 91
(9%) including deleted code components ( e.g.,“Removed
GWTShell, an obsolete way of starting dev mode ”—from
Google Web Toolkit 2.5.1 (RC1)).
Other changes performed in the new release are less com-
mon in the analyzed release notes (see Table 1). We must
note that rarely summarized types of changes are not nec-
essarily less important than the frequently reported ones.
It may be the case that some types of changes occur less
frequently than others, hence they are reported less. For ex-
ample, changes to licenses are generally rare and thus, only
18 release notes (2%) contain this information. We do not
equate frequency with importance. Future work will answer
theimportance question separately.
Based on the results of this survey and on our assessment
of what it can be automatically extracted from available
sources of information— i.e.,release archives, versioning sys-
tems, and issue trackers—we have formulated requirements
for what ARENA should include in release notes: (i) a de-
scription of ﬁxed bugs, new features, and improvement of
existing features, complemented with a description of what
was changed in the code; (ii) a summary of other source code
changes, including deprecated methods/classes and refactor-
ing operations; (iii) changes to the libraries used by the sys-
tem; (iv) changes to licenses and documentation; and (v)
open issues. Note that in the current version of ARENA we
did not consider: (i) changes to conﬁguration and build ﬁles,
because there is a plethora of diﬀerent possible conﬁguration
ﬁles; and (ii) changes to the high-level system architecture,
because existing architecture recovery approaches ( e.g.,[3,
4, 5]) might require manual eﬀort to produce usable results.
485Change 
ExtractorIssue 
Tracker
Versioning 
SystemSelect bundles for 
releases rk-1 and rk
bundle rk-1bundle rk
Code Change 
SummarizerCommits-
Issues LinkerIssue 
Extractor
Information 
AggregatorHTML
Release
NoteLegend
Information Flow
Dependency
ﬁne-grained 
structural 
changes linked 
to commits
changes to 
libraries
changes to 
documentation
changes to 
licensessummarized 
structural 
changes linked 
to commits
summarized 
structural 
changes linked 
to issues
Figure 1: Overview of the ARENA approach.
3. ARENA OVERVIEW
In a nutshell, ARENA works as depicted in Figure 1. The
process to generate release notes for a release rkis composed
of four main steps. First, the Change Extractor is used to
capture changes performed between releases rk−1andrk. In
the second step, the Code Change Summarizer describes the
ﬁne-grainedchangescapturedbythe Change Extractor , with
the goal of obtaining a higher-level view of what changed in
the code. In the third step, the Commit-Issue Linker uses
theIssue Extractor to mine information ( e.g.,ﬁxed bugs,
new features, improvements, etc.) from the issue tracker of
the project to which rkbelongs. Thus, each ﬁxed bug, im-
plemented new feature, and improvement is linked to the
code changes performed between releases rk−1andrk. The
summarized structural code changes represent whatchanged
in the new release, while the information extracted from the
issue tracker explains whythe changes were performed. Fi-
nally, the extracted information is provided as input to the
Information Aggregator , in charge of organizing it as a hi-
erarchy and creating an HTML version of the release note
forrk, where each item can be expanded/collapsed to re-
veal/hide its details. An example of a release note generated
by ARENA can be found at http://tinyurl.com/oelwef4 .
3.1 Code Changes Extraction
ARENA extracts code changes, as well as changes to
other entities, from two diﬀerent sources: the versioning
system and the source archives of the releases to be com-
pared.
Identiﬁcation of the time interval to be analyzed.
ARENA aims at identifying the subset of commits that
pertains to the release for which the release note needs to be
generated, say release rk. To identify changes between the
releasesrk−1andrk, we consider all commits occurred—in
the main trunk of the versioning system—starting from the
rk−1release date tk−1, until the rkrelease date tk. The
dates can be approximate, as developers could start working
on release rk+1even before tk,i.e.,changes committed
beforetkcould belong to release rk+1and not to release rk.
In a real usage scenario, a developer in charge of creating a
release note using ARENA could simply provide the best
time interval to analyze or, even better, tagthe commits in
the versioning system with the release number.
Analysis of Code Changes. Once the commits of in-
terest are identiﬁed, ARENA’s Change Extractor analyzes
them using a code analyzer developed in the context oftheMarkos EU project3. The analyzer parses the source
code using the srcMLtoolkit [6] and extracts a set of facts
concerning the ﬁles that have been added, removed, and
changed in each commit. Information on the commits per-
formed between the releases rk−1andrkis extracted from
the versioning system ( e.g., git, svn ). Given the set of ﬁles
in a commit, the following kinds of changes are identiﬁed:
•Files added, removed, and moved between packages;
•Classed added, removed, renamed, or moved between
ﬁles. To detect moving and renaming, classes (and
methods) are encoded by a metric-based ﬁngerprinting
[7, 8], which is used to trace the entities;
•Methods added, removed, renamed, or moved;
•Methods changed, i.e.,changes in the signature, visi-
bility, source code, or set of thrown exceptions;
•Instance variables added, removed, and visibility
changes;
•Deprecated classes and methods.
Generation of Textual Summaries from Code Ch-
anges.The ﬁne-grained structural changes are provided to
theCode Change Summarizer to obtain a higher-level view
of what changed in the code (see Figure 1). To generate
this view, ARENA’s Code Change Summarizer follows three
steps: (i) it hierarchically organizes the code changes; (ii) i t
selects the changes to be described; and (iii) it generates a
set of natural-language sentences for the selected changes.
Intheﬁrststep, ahierarchyofchangedartifactsisbuiltby
considering the kind of artifact ( i.e.,ﬁles, classes, methods,
or instance variables) aﬀected by each change. In Object-
Oriented (OO) software systems, ﬁles contain classes, which
in turn consist of methods and instance variables. There-
fore, changes are grouped based on these relationships, e.g.,
changed methods and instance variables that belong to the
same class are grouped under that class.
In the second step, ARENA analyzes the hierarchy in a
top-down fashion to prioritize and select the code changes
to be included in the summaries, in the following way:
1. If a ﬁle is associated to class-level changes, high pri-
ority is given to the class changes and low priority to
the ﬁle changes, since in OO programming languages
classes are the main decomposition unit, rather than
source ﬁles.
2. If the change associated to a class is its addition or
deletion, high priority is given to it and low prior-
ity to any other change in the class ( e.g.,the addi-
tion/deletion of its methods). Otherwise, changes on
the visibility or deprecation of the class become of high
priority. If the change associated to a class is its modi-
ﬁcation, highpriorityisgiventotheassociatedchanges
at instance variable and method level.
3. If the change associated to an instance variable or
method is its addition, deletion, renaming, or depre-
cation, the change is given a high priority. Changes
to parameters, return types, or exceptions are given a
low priority.
Finally, the Code Change Summarizer generates a natural
language description of the selected changes, presented as a
list of paragraphs. For this, ARENA deﬁnes a set of tem-
plates according to the kind of artifact and kind of change
3http://www.markosproject.eu
486New class SearcherLifetimeManager implementing Closeable. 
This boundar y class communicate s mainly with 
AlreadyClosedException, IndexSearcher, and IOException, and 
consists mostly of mutator methods. It allows getting 
record, handling release, acquirin g searcher lifetime 
manager, and closing searcher lifetime manager. This class 
declares the helper classes SearcherTracker and PruneByAge.
Figure 2: Summary for the SearcherLifetimeMan-
agerclass from Lucene 3.5.
to be described. In this way, one sentence is generated for
each change. For example, a deleted ﬁle is reported as File
<ﬁlename>has been removed .
As stated above, the focus of OO systems is on classes.
Thus for added classes, ARENA provides more detailed sen-
tences than for other changes, by adapting JSummarizer [9],
a tool that automatically generates natural-language sum-
maries of classes. Each summary consists of four parts: a
general description based on the superclass and interfaces of
the class; a description of the role of the class within the sys-
tem; a description of the class behavior based on the most
relevant methods; and a list of the inner classes, if they exis t.
We adapted JSummarizer by modifying some of the original
templates and by improving the ﬁltering process when se-
lecting the information to be included in the class summary.
Figure 2 shows part of an automatically generated summary
for theSearcherLifetimeManager class from Lucene 3.5.
Deleted classes are reported in a similar way to deleted
ﬁles. Changesregardingthevisibilityofaclassaredescribed
by providing the added or removed speciﬁer, e.g., Class
<classname>is now<addedspeciﬁer>. The description
of modiﬁed classes consists of sentences reporting methods
and instance variables added, deleted or modiﬁed. For ex-
ample, a change in a method’s name is reported as: Method
<oldmethodname>was renamed as <newmethodname>.
The generated sentences are concatenated according
to the priority previously assigned to the changes. To
avoid text redundancies, ARENA groups similar changes in
single sentences, e.g., Methods <methodname1>, ..., and
<methodnamen>were removed from <classname>,
rather than list them one at a time.
Analysis of licensing changes. To identify license
changes, ARENA’s Change Extractor analyzes the content
of the source distribution of rk−1andrk, extracting all
source ﬁles ( i.e.,.java) and all text ﬁles ( i.e.,.txt). Then,
it uses the Ninkalicense classiﬁer [10] to identify and
classify licenses contained in such ﬁles. Ninkauses a
pattern-matching based approach to identify statements
characterizing the various licenses and, given any text ﬁle
(including source code ﬁles) as input, it outputs the license
name (e.g.,GPL) and its version ( e.g.,2.0). Its precision is
around 95%.
Identiﬁcation of changes in documentation. This
analysis is done on the release archives of rk−1andrk. Al-
though release archives can contain any kind of documen-
tation, we only focus on changes to documentation describ-
ing source code entities. ARENA identiﬁes documentation
changes using the approach described below:
1. Identify text ﬁles, i.e.,.pdf,.txt,.rdf,.doc,.docx,
and.html, and extract the textual content from them
using the Apache Tika4library.
4http://tika.apache.org2. Ifatextﬁle(say doci)hasbeenaddedin rk, thenverify
whether it references code ﬁle names, class names, and
method names. We use a pattern matching approach
similar to the one proposed by Bacchelli et al.[11].
If such entities are found in a ﬁle, then check whether
these ﬁles, classes, or methods have been added, re-
moved, or changed in the source code, so that ARENA
can generate an explanation of why the documentation
was added, e.g.,if the added text ﬁle contains a ref-
erence to class Cjadded in rk, ARENA describes the
change as “ The ﬁle docihas been added to the docu-
mentation to reﬂect the addition of class Cj”.
3. If a text ﬁle (say doci) has been removed in rk, then
check if it references deleted methods, classes, or code
ﬁles. If that is the case, ARENA generates an explana-
tion“The ﬁledocihas been deleted from the documen-
tation due to the removal of <involved codecompo-
nents>from the system ”.
4. If a text ﬁle has been modiﬁed between rk−1andrk,
we use a similar approach as above, but we search for
references to code entities only in the portions of the
text ﬁle that were changed.
Identiﬁcation of changes in the used libraries.
ARENA’s Change Extractor analyzes whether: (i) rkuses
new libraries—speciﬁcally jars—compared to rk−1; (ii) li-
braries are no longer required; and (iii) libraries have been
upgraded to new releases. The analysis is performed in two
steps:
1. Parsing the ﬁles describing the inter-project depen-
dencies. In Java projects, these are usually property
ﬁles (libraries.properties ordeps.properties ) or
Maven dependency ﬁles ( pom.xml). The information
from such ﬁles allows ARENA to detect the libraries
used in both releases rkandrk−1. We detect the name
and used versions of each library, e.g., ant v.1.7.1.
2. Identifying the jarﬁles contained in the two release
archives and—by means of regular expressions—ex-
tracting name and version from each jar ﬁle name,
e.g.,ant_1.7.1.jar is mapped to library antv.1.7.1.
With the list of libraries used in both releases, ARENA
veriﬁes if: (i) new libraries have been added in rk; (ii)
libraries are no longer used in rk; or (iii) new versions of
libraries previously used in rk−1are used in rk.
Identiﬁcation of refactoring operations. ARENA’s
Change Extractor also identiﬁes refactorings performed be-
tween the releases rk−1andrk, by using two complementary
sources of information:
1. Refactorings documented in the commit notes. Such
refactorings are identiﬁed by matching regular expres-
sions in commit notes— e.g.,refact ,renam—as done in
previous work [12, 13].
2. Class/method renaming and moving (those not al-
ready identiﬁed by means of their commits using the
above heuristic). Such refactoring changes are identi-
ﬁed by means of ﬁngerprinting analysis.
In principle, ARENA could describe other kinds of refac-
torings, which could be identiﬁed using tools like RefFinder
[14]. For the time being, we prefer to keep a light-weight ap-
proach, toavoidgeneratingexcessivelyverbosereleasenotes.
4873.2 Issues Extraction
We use the versioning system to extract various kinds of
changes to source code and other system entities, i.e.,to
explainwhatin the system has been changed. In addition,
we rely on the issue tracker to extract change descriptions,
i.e.,to explain whythe system has been changed. To this
aim, the ARENA Issue Extractor (see Figure 1) extracts the
following type of issues from the issue tracker:
•Issues describing bug ﬁxes: Issues with type= “Bug”,
status=“Resolved” or“Closed” , resolution= “Fixed”,
and resolution date included in the [ tk−1,tk] period.
•Issues describing new features: Same as above, but
considering issues with type= “New Feature” .
•Issues describing improvements: Same as for bug ﬁxes,
but considering issues with type= “Improvement” .
•Open issues . Any issue with status= “Open”and open
date in the period [ tk−1,tk].
Note that open issues are collected to present in the re-
lease note rk’sKnown Issues . Based on the ﬁelds described
above, ARENA has been implemented for the Jiraissue
tracker. However, it can be extended to other issue trackers
(e.g.,Bugzilla ), using the appropriate, available ﬁelds. Note
that sometimes ﬁelds classifying issues as bug ﬁx/new fea-
ture/enhancement are not fully reliable [15, 16].
3.3 Linking Issue Descriptions to Commits
To link issues to commits we use (and complement) two
existing approaches. The ﬁrst one is the approach by Fis-
cheret al.[17], based on regular expressions matching the
issue ID in the commit note. The second one is a re-
implementation of the ReLinkapproach deﬁned by Wu et
al.[18], which considers the following constraints: (i) com-
mitter/author must match with issue tracking contributor
name/email; (ii) the time interval between the commit and
the last comment posted by the same author/contributor on
the issue tracker must be less than 7 days; and (iii) the Vec-
tor Space Model (VSM) [19] cosine similarity between the
commit note and the last comment referred above must be
greater than 0.7 (set by manually analyzing the link on two
projects, i.e.,Apache Commons Collections and JBoss-AS).
3.4 Generating the Release Note
ARENA’s Information Aggregator is in charge of building
the release note as an HTML document. The changes are
presented in a hierarchical structure consisting of the cate-
gories from the ARENA requirements deﬁned in Section 2
and items summarizing each change.
4. EMPIRICAL EV ALUATION
Thegoalof our empirical studies is to evaluate ARENA,
with the purpose of analyzing its capability to generate re-
lease notes. The quality focus is the completeness, correct-
ness and importance of the content of the generated release
notes with respect to original ones, and the perceived use-
fulness of the information in the ARENA release notes. The
perspective is of researchers, who want to evaluate the ef-
fectiveness of automatic approaches for generating release
notes, and managers and developers, who could consider us-
ing ARENA in their own company.
We aim at answering the following research questions:Table 2: System releases used in each study.
Study Name Releases KLOC# of commits
before release
Apache Cayenne 3.0.2 248 5,118
Apache Cayenne 3.1B2 232 2,550
Apache Commons Codec 1.7 17 267
Study I Lucene 3.5.0 184 2,869
Jackson-Core 2.1.0 21 170
Jackson-Core 2.1.3 22 31
Janino 2.5.16 26 612
Janino 2.6.0 31 408
Study IIApache Commons Collections 4.4.0ALPHA1 104 303
Lucene 4.0.0 192 758
Study III SMOS 2.0.0 23 109
RQ1—Completeness: How complete are the generated
release notes, compared with the original ones? In other
words, our ﬁrst objective is to check whether ARENA
is missing information that is contained in manually-
generated release notes.
RQ2—Importance: How important is the content cap-
tured by the generated release notes, compared with the
original ones? The aim is assessing developers’ percep-
tion of the various kinds of items contained in manually
and automatically-generated release notes. We are inter-
ested in the usefulness of the additional details produced
by ARENA, which are missing in the original notes.
To address our research questions, we performed three
empirical studies having diﬀerent settings and involving dif-
ferent kinds of participants. Study Iaims at assessing the
completeness of the ARENA release notes with respect to
the original ones available on the systems’ websites ( RQ1).
Toensuregoodgeneralizabilityofourﬁndings, weconducted
this study on eight open source projects. In addition, since
the goal of Study Idoes not require high experience and deep
knowledge of the application domain (as it will be clearer
later), we involved mainly students. Study II aims at eval-
uating the importance of the items present in the ARENA
release notes and missing in the original ones and vice versa
(RQ2). In this case, the task assigned to participants is
highly demanding, so we conducted the study only on two
systems. Also, we involved experts—including original de-
velopers of the analyzed projects—, since experience and
knowledge of the systems were crucial. Finally, Study III is
anin-ﬁeld study addressing both RQ1andRQ2, where we
asked the original developers of a system, named SMOS, to
evaluate a release note generated by ARENA and to com-
pare it with one produced by the development team leader.
4.1 Study I—Completeness
The goal of this study is to assess the completeness of
ARENA release notes ( RQ1) on several system releases,
hence assuring a good external validity, both in terms of
projects diversity and features to be included in the releases.
ThecontextofStudy Iconsistsof: objects,i.e.,automatically
generated and original release notes from eight releases of
ﬁve open-source projects (see Table 2); and subjects eval-
uating the release notes, i.e.,one B.Sc., ﬁve M.Sc., one
Ph.D. student, one faculty, and two industrial developers.
Before conducting the study, we proﬁled the participants
using a pre-study questionnaire, aimed at collecting infor-
mation about their programming and industrial experience.
To select releases to analyze, we used the following criteria.
A release rkwas selected if: (i) the original rkrelease note
was available, and (ii) the release bundles for rkandrk−1
were available. Additionally, we made sure that items from
each change type (see Table 1—except for conﬁguration ﬁle
488Table 3: Evaluation provided by the study partici-
pants to the items in the original release notes.
System Release AbsentLess Same More
Details Details Details
Apache Cayenne 3.0.2 15% 5% 0% 80%
Apache Cayenne 3.1B2 16% 0% 0% 84%
Apache Comm. Codec 1.7 13% 5% 5% 77%
Apache Lucene 3.5.0 37% 39% 8% 16%
Jackson-Core 2.1.0 25% 8% 33% 33%
Jackson-Core 2.1.3 0% 0% 50% 50%
Janino 2.5.16 0% 6% 0% 94%
Janino 2.6.0 12% 38% 0% 50%
Average 14% 13% 12% 61%
and architectural changes) were present in at least one of
the eight release notes.
Design and Planning. We distributed the release notes
to the evaluators in such a way that each release note was
evaluatedbytwoparticipants. Weprovidedeachparticipant
with (i) a pre-study questionnaire; (ii) the generated release
note; and (iii) the original release note.
Participantswereaskedtodetermineandindicateforeach
item in the original release note whether: (i) the item ap-
pears in the generated release note with roughly the same
level of detail; (ii) the item appears in the generated release
note but with less details; (iii) the item appears in the gen-
erated release note and it has more details; or (iv) the item
does not appear in the generated release note. In order to
avoid bias in the evaluation, we did not refer to the release
notes as“original”or“generated”. Instead, we labeled them
as“Release note A” and“Release note B” .
When all the participants completed their evaluation, a
Ph.D. student from the University of Sannio analyzed them
to verify and arbiter any conﬂict in the evaluation of the
items present in the original release notes. Out of 144 eval-
uated items, 43 (30%) exhibited some conﬂict between the
two evaluators. Only ﬁve of them (3%) showed strong con-
ﬂict between the evaluators, e.g.,“ the item appears ”vs. “the
item is missing ”. The other 38 cases had slight deviations in
the evaluation, e.g.,“ the item appears with roughly the same
level of detail ”vs. “the item appears but with less details ”.
Participants’ background. Six out of ten evaluators
have experience in industry, ranging from one to ﬁve years
(median 1.5). They reported four to 20 years (median 5.0) of
programming experience, of which two to seven are in Java
(median 4.5). Seven out of the ten evaluators declared that
they routinely check release notes when using a new release.
Results. Table 3 summarizes the answers provided by
the evaluators when asking about the presence of items from
the original release notes in the release notes generated by
ARENA. On average, ARENA correctly captures, at dif-
ferent levels of detail, 86% of the items from the original
release, missing only 14%. In particular, ARENA provides
more details for 61% of the items present in the original re-
lease notes, the same level of details for 12%, and less detail s
for 13% of the items. The following is an exemplar situation
where an item in the generated release has more details than
in the original release. In the release note of Apache Com-
mons Codec 1.7, the item “ CODEC-157 DigestUtils: Add
MD2 APIs ”describes the implementation of new APIs. In
the ARENA release note, the same item is reported as:
CODEC-157 DigestUtils: Add MD2 APIs
New methods getMd2Digest() , md2(byte) , md2
(InputStream), md2(String), md2Hex(byte), md2Hex
(InputStream), and md2Hex(String) in DigestUtils.
New methods testMd2Hex(), testMd2HexLength(), and 
testMd2Length() in DigestUtilsTest.ARENA reports the addition of new APIs to the Digest-
Utilsclass and it also explicitly includes: (i) which methods
are part of the new APIs; and (ii) the test methods added
inDigestUtilsTest to test the new APIs.
An outlier case is for Lucene 3.5.0, where ARENA missed
14 (37%) of the items present in the original release note.
Upon a closer inspection, we found that eight of the missed
items are bug reports ﬁxed in a previous release, yet (for an
unknown reason) reported in the release note of Lucene 3.5.0
(e.g.,issueLUCENE-3390 ). If we disregard such issues,
the percentage of missed items in this release drops to 20%,
almost in line with the other release notes.
We analyzed the items that ARENA missed in the other
release notes. We found that all the missed items are due
to a slight deviation between the time interval analyzed by
ARENA and the one comprising the changes considered in
the original release note. As explained in Section 3, we make
an assumption about the time period of analysis, going from
therk−1release date tk−1until the rkrelease date tk. This
problem would not occur in a real usage scenario.
Summary of Study I (RQ 1)—Completeness . The
ARENA release notes capture most of the items from the
original notes (86% on average)—many missed items can be
included by simply adjusting the considered time interval.
4.2 Study II—Importance
The goal of Study II is to evaluate the importance of the
capturedandmisseditemsintheARENArelease notesfrom
the perspective of external users/integrators as well as from
the perspective of internal developers. The contextof this
study consists of: objects,i.e.,automatically generated and
original release notes from one release of two open-source
projects (see Table 2); and subjects evaluating the release
notes,i.e.,38 professional and open-source developers, in-
cluding three developers of each object project. One release
of Apache Lucene and one release of Apache Commons Col-
lections were selected for the study. The conditions to select
the release notes were the same as of Study I.
Design and Planning. We performed Study II by using
an online survey. We emailed the survey to several open-
source developers registered in the Apache repositories and
professional developers from around the world. The ques-
tionnaire consisted of two parts on: (i) the participants’
background and their experience in using and creating re-
lease notes; and (ii) the evaluation of the ARENA release
notes and the original release notes for Apache Lucene 4.0.0
and for Apache Commons Collections 4.4.0ALPHA1.
The evaluation of each release note was divided in two
stages. In the ﬁrst one, participants were asked to indi-
cate for several kinds of items ( e.g., Major Changes ) of the
original release note that were missing in the generated re-
lease note whether each one was: (i) not at all important;
(ii) unimportant; (iii) important; or (iv) very important. A
similar process was followed in the second stage, but this
time for assessing the importance of some kinds of items of
the ARENA release note that were missing in the original
one. In both stages, we pointed out that some items present
in a release note and (apparently) missing in the other one
might simply be represented in diﬀerent ways. In the case
of Lucene 4.0.0, for example, the items under the Improve-
mentscategory in the ARENA release note are listed under
theOptimizations category in the original release note.
Participants’ background. 31 out of 38 evaluators are
4891.0 1.5 2.0 2.5 3.0 3.5 4.0How much time does it take to create a release note?Less than
one hourBetween one
and four hours
1.0 1.5 2.0 2.5 3.0 3.5 4.0How difficult is it to create a release note?
How much time does
it take to create a release note?How difﬁcult is it to create 
a release note?Between four
and eight hoursMore than
eight hours
Very 
easyEasyDifﬁcultVery
difﬁcult
Do you use any supporting tool when creating release n otes?
YES6/22 (27%)
4: Git log --  2: Issue TrackerNO16/22 (73%)
Figure 3: Diﬃculty in creating release notes.
professional developers, who reported experience in software
development ranging from two to 30 years (median 8). The
other seven participants are open-source developers, ranging
from seven to 25 years of experience in software development
(median 13). Three of the participants are developers of
Apache Commons Collections, while other three are devel-
opers of Apache Lucene. Also, 26 out of the 38 evaluators
declared that they use release notes frequently ( i.e.,more
than once a month) or occasionally ( i.e.,once a month),
mainly to check for bug ﬁxes and new features in a software
system. 16 developers declared that they check in the re-
lease notes of their project’s dependencies for compatibility
issues and changes that might arise from the new releases.
Creating release notes. Only six evaluators (16%) re-
ported never having created release notes. Of the other 32
participants, 22 (58%) reported having created release notes
many times ( i.e.,more than eight times), eight (21%) re-
ported having done it a few times ( i.e.,three to eight times),
and two (5%) declared having created a release note once or
twice. Our survey was designed to ask only highly experi-
enced developers in creating release notes ( i.e.,the 22 cited
above) details about this task. Figure 3 presents the partic-
ipants’ answers on the time and diﬃculty of creating release
notes. Speciﬁcally, 64% of the evaluators ( i.e.,14 of them)
considered this task as diﬃcult or very diﬃcult, while 36%
rated it as easy or very easy (median=diﬃcult). The par-
ticipants reported a median of between four and eight hours
to create release notes. One of the participants explained
that the time needed to create a release note depends on
the release, claiming that he worked in the past on “a major
release of a software company product for which the creation
of the release note took three days of work.” Finally, only
seven evaluators (31%) declared using a supporting tool,
such as, issue trackers or version control systems, when cre-
ating release notes. Note that ARENA is the only existing
automated tool specially designed to support the generation
of release notes. Furthermore, ARENA is able to shorten
the time devoted to this task, as less than ﬁve minutes are
needed to generate a release note for a medium-sized system.
We also asked the 22 participants with high experience in
creating release notes about the kind of content that they
usually include in these documents (see Figure 4). New Fea-
turesandBug Fixes are, by far, the most common items in
the release notes: most of the participants reported includ-
ing them often (21 and 20 evaluators, respectively). En-
hanced Features are also frequently included in the release
notes (often by 11 participants and sometimes by ten). Both
results conﬁrm the ﬁndings of our survey on 1,000 existing
release notes presented in Section 2. Other frequently in-cluded items are Deleted and Deprecated Code Components ,
Changes to Licenses ,Library Upgrades , andKnown Issues
(median=often), while items related to Refactoring Opera-
tionsandAdded,Replaced orModiﬁed Code Components are
sometimes included. On the other hand, evaluators rarely
include changes to Conﬁguration Files ,Documentation ,Ar-
chitecture , andTest Suites in the release notes.
Results. Going to the core of this study, the answers
provided by the 38 developers on the importance of diﬀer-
ent kinds of content from the original and the generated
release notes are summarized in Figures 5 and 6 for Com-
monsCollectionsandLucene, respectively. Amongtheitems
present in the original release notes and missed by ARENA,
theonesconsideredimportant/veryimportantbydevelopers
are:Major Changes from Commons Collections, summariz-
ing the most important changes in the new release; and API
Changes,Backward Compatibility , andOptimizations from
Lucene. The Major Changes section is not present in the
ARENA release notes, but future eﬀorts will be oriented to
implement an automatic prioritization of changes in the new
release, which will allows ARENA to select the most impor-
tant ones to deﬁne such a category. On the other hand,
the information present in API Changes ,Backward Com-
patibility , andOptimizations in the original release notes is
present in the ARENA release notes, but simply organized
diﬀerently. For example, the removal of the SortedVIntList
class is reported in the original release notes in the Backward
Compatibility category, while ARENA puts it under Deleted
Code Components . Thus, ARENA is not missing any im-
portant information here. As for the other items present in
the original release notes and not in the ones generated by
ARENA, they are generally classiﬁed as unimportant/not
important (see Figures 5 and 6).
Regarding the contents included in the ARENA release
notes and missing or grouped diﬀerently in the original re-
lease notes, most of them (nine out of 11 diﬀerent kinds of
content) were predominantly assessed as important or very
important (by 28 developers, in average). The Improve-
mentscategory is considered important/very important by
34 developers for Commons Collections and 33 for Lucene,
respectively. While the items contained in this category are
present in the Optimizations section of the Lucene release
note, they are absent in the Commons Collections one. Im-
portant or very important were also considered the cate-
gories covering Known Issues (by 32 developers) and Dele-
tion(29),Addition (28),Deprecation (29), and Visibility
Changes (26)of Code Components . The evaluators (24 for
CommonsCollectionsand19forLucene)alsoconsideredim-
portant the ﬁne-grained changes (i.e.,changes at code level)
providedbyARENAwhendescribingnewfeatures, bugﬁxes
and improvements, and the links to the change requests (30)
listed under such categories. Note that most of the above
categories are absent in the original release notes. For in-
stance, while in the original Lucene release note one deleted
class was listed under the Backward Compatibility section,
ARENA highlights 76 classes that were deleted in the new
release. Surprisingly, Refactoring Operations were consid-
ered as unimportant in both release notes (by 24 developers
for Commons Collections and 25 for Lucene). This might be
due to the level of details that ARENA is currently able to
provide in this matter ( i.e.,the refactored source code ﬁles,
without explaining what exactly was done to them).
Evaluation made by the original developers. As
490%vspace-5mm
●●● ●●
●●● ●
Fixed
bugsNew
featuresEnhanced
featuresNew code
comp.Modified
code comp.Deprecated
code comp.Deleted
code comp.Replaced
code comp.Changes vis.
 code comp.Changes to
conf. filesChanges to
test suitesRefactoring
operationsArchitectural
changesChanges to
docum.Changes to
licensesLibraries
UpgradesKnown
Issues1.0 2.0 3.0 4.0Often
Sometimes
Rarely
Never
Figure 4: What kind of content do you include in release notes ? (22 developers)
Figure 5: Importance reported by the evaluators for the cont ent of Commons Collections release notes.
mentioned before, six (three each) of the original Lucene and
Commons Collections developers took part in our survey. In
the case of the contents explicitly provided by the original
release note of Commons Collections, its three developers
strongly agreed on the importance of the Major Changes
category and weakly agreed on the importance of showing
theauthor of the change (marked as important by two de-
velopers and unimportant by the other one). In contrast,
having separate categories describing new classes and new
methods (as opposed to ARENA’s single New Code Compo-
nentscategory) was ranked from very unimportant to very
important, not allowing us to draw any conclusion. The
three developers strongly agreed on the importance of all
the attributes oﬀered by the ARENA release notes, except
for thenumber of items in each category (e.g.,indicating
the number of added code components near the New Code
Components category), which was considered unimportant
by two developers and very important by the other one.
In the case of the original release note of Lucene, its
three developers strongly agreed on the importance of the
API Changes andBackward Compatibility categories and
on the unimportance of the authors of the changes . For the
other contents of this release note, there was little agree-
ment. However, once again, the three developers strongly
agreed on the importance of all the contents included in the
ARENA release note, except for the Refactoring Operations
category, which was marked as important by two of the de-
velopers and unimportant by the other one. Most of the
developers’ responses go in line with the responses of all the
other evaluators presented above.
Qualitative feedback . We also allowed developers to
comment on the ARENA release notes in a free text box at
the end of the survey. Evaluators provided positive feedback
about the release notes generated by ARENA. A represen-
tative one is: “In general, they are very readable. I think
they are aimed at engineers more than at non-engineers.
[...] for something that is consumed as an API, such as
an open source library or framework, I think these kind of
notes are ideal.” Another developer commented “If it’s fully
automated (I’m not sure) ARENA is a great tool.” Note that
some of the contents provided by the generated release noteswere considered unimportant in some cases. One of the rea-
sons behind such assessments was that “Every item needs
a description to be useful. For example, the Added Com-
ponents section needs a description of each component, and
the Modiﬁed Components section needs a description of what
is the meaning of the modiﬁcation.” This happens for the
commits that cannot be linked to any bug ﬁx, improvement,
or new feature request in the issue tracker system. Since
ARENA is meant to support the creation of release notes,
theycanbeaugmentedbytheusersaccordingtotheirneeds.
Summary of Study II (RQ 2)—Importance . Most
information included in the ARENA release notes is con-
sidered important or very important by developers. Also,
most information considered as important in the original re-
lease notes is captured by ARENA (although sometimes in
a diﬀerent fashion), with the exception of the Major changes
category that we plan to include by prioritizing changes.
4.3 Study III—“In-ﬁeld” Evaluation
The goal of Study III is to allow project experts: (i) to
evaluate the generated release note on its own, including
their perceived usefulness; and (ii) to compare the gener-
ated release note with one manually produced by their team
leader. The contextofthisstudyconsistsofonereleaseofthe
SMOS system and ﬁve (out of seven) members of the orig-
inal development team of this system. SMOS is a software
developed to support the communications between schools
and the students’ parents. Its ﬁrst release ( i.e.,SMOS 1.0)
was developed in 2009 by M.Sc. students at the University
of Salerno during their industrial internship. Its code, com-
posed by almost 23 KLOC, is hosted in a Gitrepository and
has been subject to several changes over time. This led to
the second release ( i.e.,SMOS 2.0) nearly two years later.
The developers involved in our study worked on the evolu-
tion of SMOS during these two years and, since then, they
have been working for more than three years in industry.
Design and Planning. To have a baseline for compar-
ison, we asked the leader of the original development team
to generate the release note of SMOS 2.0. During the deﬁ-
nition of the release note, the leader had access to: (i) the
source code of the two releases; (ii) the list of changes (as
491Figure 6: Importance reported by the evaluators for the cont ent of Lucene release notes.
extracted from the versioning system) performed between
the two releases; and (iii) information from the issue tracker
containing the change requests implemented in the time pe-
riod between SMOS 1.0 and 2.0. Finally, we asked him to
report the time he spent on producing the release note.
We conducted this study in two stages. In the ﬁrst
stage, the developers (excluding the project leader) evalu-
ated the release note generated by ARENA based only on
their knowledge of the system. As in Study I, we did not
refer to this one as an automatically generated release note
(but asRelease note A ). We asked the developers to judge
four statements with the possible answers on a 4-points
Likert scale ( strongly agree ,weakly agree ,weakly disagree ,
strongly disagree ). We also asked them to judge the extent
to which the generated release note would be useful for de-
velopers in the context of a maintenance task and what kind
of additional information should be included in the release
note. In the second stage, we asked the SMOS developers
to evaluate—using the same questionnaire—the release note
manually produced by the team leader, calling it Release
note B.
Results. The team leader took 82 minutes to manually
summarize the 109 changes that occurred between releases
1.0 and 2.0. This resulted in a release note with 11 items,
each one grouping a subset of related changes. For example,
one of the items in this release note was:
Several changes have been applied in the system to im-
plement the Singleton design pattern in the classes ac-
cessing the SMOS database. Among the most important
changes, all constructors of classes in the storage pack-
age should now not be used in favor of the new methods
getInstance() , returning the single existing instance of
the class. This resulted in several changes to all methods
in the system using classes in storage.
In the meantime, the remaining four developers evaluated
the release note generated by ARENA. The results are re-
ported in Table 4 (see the numbers notin parenthesis).
Developers strongly agreed orweakly agreed on the fact
that the ARENA release note contains all the information
needed to understand the changes performed between the
two SMOS releases. In particular, the only developer an-
sweringweakly agree (ID 1 in Table 4) explained that “the
release note contains all what is needed. However, it would
be great to have a further level of granularity showing exact ly
what changed inside methods” . In other words, this devel-
oper would like to see, on demand, the source code lines
modiﬁed in each changed code entity. While this informa-
tion is not present in the ARENA release note, it would be
rather easy to implement. All other developers answered
with astrongly agree and one of them explained her score
with the following motivation: “I got a clear idea of what
changed in SMOS 2.0. Also, I noticed as I was not awareabout some of the reported changes” .
All developers strongly agreed on the correctness of the in-
formation reported in the release note generated by ARENA
(see Table 4). Also, they strongly disagreed about the pres-
enceofredundancyintheinformationreportedintherelease
note. In particular, one of them explained that “information
is well organized and the hierarchical view allows visualiz ing
exactly what you want, with no redundancy.” Finally, all
developers weakly agreed orstrongly agreed on the useful-
ness of the ARENA release note for a developer in charge of
evolving the software system, for example, “the release note
is very useful to get a quick idea of what changed in the sys-
tem and why.” The only developer answering weakly agree
commented: “developers are certainly able to get a clear idea
about what changed in the system. But they may still need
to look in source code for details.” Note that this developer
was the one asking for a line of code granularity level.
In the second part of this study, we asked the same four
developers to evaluate (by using the same questionnaire)
the release note manually-generated by their team leader.
Table 4 reports these results in parenthesis. In this case,
three developers weakly agreed on the completeness of the
release note, while one weakly disagreed . As comparison, on
the completeness of the release note generated by ARENA
three developers strongly agreed and one weakly agreed . The
developer answering weakly disagree motivated her choice
explainingthat “the level of granularity in this release note is
much higher as compared to the previous one. Thus, it is dif-
ﬁcult to get a clear idea of what changed in the system. Also,
information about the updated libraries is missing” . When
talking about“the granularity”of the release note, the devel-
oper refers to the fact that changes are not always reported
at method level as it happened in the ARENA release note.
Threedevelopers strongly agreed aboutthecorrectnessofthe
informationreportedinthemanually-generatedreleasenote,
while one of them answered weakly agree , reporting an error
present in the release note: “the method daysBetweenDates
has been deprecated, not deleted” . Indeed, the manually-
generated release note contained such a mistake, avoided by
ARENA that reported: Method daysBetween(Date,Date) in
Utility has been deprecated . Note that the error was rather
subtle, as only one of the developers was able to spot it.
Summary of Study III. The SMOS developers judged
the ARENA release note as more complete and precise than
theonecreatedbytheteamleader( RQ1). Moreover, theex-
tra information included in the generated release note makes
it to be considered more useful than the manual one ( RQ2).
5. THREATS TO V ALIDITY
Threats to construct validity concern the relationship be-
tween theory and observation. In this work such threats
mainly concern how the generated release notes were evalu-
492Table 4: Evaluation provided by four original developers to the release note generated by ARENA for SMOS.
In parenthesis, the evaluation provided to the manually-gene rated release note.
ClaimSubject ID
1 2 3 4
The release note contains all the information needed to understa nd what changed between the old and the new release 3 (2) 4 (3) 4 (3) 4 (3)
All the information reported in the release note is correct 4 (3) 4 (4) 4 (4) 4 (4)
There is redundancy in the information reported in the release note 1 (1) 1 (1) 1 (1) 1 (1)
The release note is useful to a developer in charge of evolving th e software system 3 (3) 4 (3) 4 (3) 4 (3)
1=strongly disagree, 2=weakly disagree, 3=weakly agree, 4=stro ngly agree
ated. For Study IandStudy II, we tried to limit the subjec-
tivenessintheanswersbyaskingrespondentstocomparethe
contents of the generated release note with that of the actual
one. In Study Iwe assigned each release note to two inde-
pendent evaluators. For Study III , although our main aim
was to collect qualitative insights, we still tried to collec t
objective information by (i) involving multiple evaluators,
and (ii) using an appropriate questionnaire with answers in
a Likert scale, complemented with additional comments.
Threats to internal validity concern factors that could
have inﬂuenced our results. In Study IandStudy III we have
tried to limit the evaluators’ bias by not telling them upfront
which were the original and automatically generated release
notes. Another possible threat is that the participants in
Study I andStudy II have diﬀerent level of knowledge of
the object projects. We must note that some of the re-
spondents in Study II were developers of the object projects
and their answers were in line with the answers of the other
participants. None of the participants in Study Iwere de-
velopers/contributors of the object projects. In Study III
we asked developers to perform the comparative evaluation
only after having provided a ﬁrst assessment of the automat-
ically generated release note. This allowed us to gain both
an absolute and a relative assessment.
Threats to external validity concern the generalization of
our ﬁndings. In terms of evaluators , the paper reports re-
sults concerning the evaluation of release notes from the
perspective of potential end-users/integrators ( Study Iand
Study II) and of developers/maintainers ( Study II andStudy
III). In terms of objects, across all studies, release notes from
11 diﬀerent releases were generated and evaluated.
6. RELATED WORK
To the best of our knowledge, no previous work has
focused on automatically extracting and describing the
system-level changes occurring between two subsequent ver-
sions of a software system. Some research has been con-
ducted to summarize changes occurring at a smaller gran-
ularity. Buse and Weimer proposed DeltaDoc [20], a tech-
nique to generate a human-readable text describing the be-
havioral changes caused by modiﬁcations to the body of a
method. Such an approach, however, does not capture why
the changes were performed. In this sense, Rastkar and
Murphy [21] proposed a machine learning-based technique
to extract the content related to a code change from a set
of documents ( e.g.,bug reports or commit messages). Dif-
ferently from our approach, these summaries focus on de-
scribing ﬁne-grained changes at method level. ARENA is
meant to summarize sets of structural changes at system
level and, where possible, relate them to their motivation.
The automatic generation of release notes relies on extract-
ing change information from software repositories and is-
sue tracking systems. More research work has been done
in this area, especially in the context of software evolution[22]. Related to our approach is the work on traceability
link recovery between the issues reported in issue trackers
and changes in versioning systems [17, 23, 24, 25]. While
ARENA employs similar techniques to extract change infor-
mation, none of these approaches attempted to produce a
natural language description of the code changes linked to
the reports.
Concerning the summarization of other software artifacts,
diﬀerent approaches have been proposed to automatically
summarize bug reports [26, 27, 28]. The focus of such ap-
proaches is on identifying and extracting the most relevant
sentences of bug reports by using supervised learning tech-
niques [26], network analysis [27, 28], and information re-
trieval [28]. These summaries are meant to reduce the con-
tent of bug reports. In a diﬀerent way, the bug report sum-
maries included in the ARENA release notes are meant to
describe the changes occurred in the code of the software
systems in response to such reports. At source code level,
the automatic summarization research has focused on OO
artifacts, such as, classes and methods [1, 2, 29]. ARENA
uses the approach proposed by Moreno et al.[2] for gener-
ating descriptions of the added classes in the new version of
the software.
7. CONCLUSION AND FUTURE WORK
We introduced ARENA, a technique that combines in a
unique way source code analysis, summarization techniques,
and information mined from software repositories to auto-
matically generate complete release notes. Three empirical
studies were aimed at answering two research questions and
concluded that: (i) the ARENA release notes provide im-
portant content that is not explicit or is missing in original
release notes, as considered by professional and open-source
developers; (ii) the ARENA release notes include more com-
plete and precise information than the original ones; and
(iii) the extra information included by ARENA makes its
release notes to be considered more useful. Based on this
work, the research on release note generation moves into a
distinct arena, where new research questions can be inves-
tigated, such as: What is the most important information
to include in the release notes and how to classify it? How
should release notes be presented to users? These are just
two items on our research agenda.
8. ACKNOWLEDGMENTS
The authors would like to thank all the participants of
our three studies. Laura Moreno and Andrian Marcus are
supported in part by grants from the National Science Foun-
dation (CCF-1017263 and CCF-0845706). Gabriele Bavota,
Gerardo Canfora, and Massimiliano Di Penta are partially
supported by the Markos project, funded by the European
Commission under Contract Number FP7-317743.
4939. REFERENCES
[1] G. Sridhara, E. Hill, D. Muppaneni, L. Pollock, and
K. Vijay-Shanker,“Towards automatically generating
summary comments for Java methods,”in Proceedings
of the IEEE/ACM international conference on
Automated software engineering , ser. ASE ’10. New
York, NY, USA: ACM, 2010, pp. 43–52.
[2] L. Moreno, J. Aponte, G. Sridhara, A. Marcus,
L. Pollock, and K. Vijay-Shanker,“Automatic
generation of natural language summaries for Java
classes,”in Proceedings of the IEEE International
Conference on Program Comprehension , ser. ICPC
’13. IEEE, 2013, pp. 23–32.
[3] R. Koschke,“Atomic architectural component recovery
for program understanding and evolution,”in 18th
International Conference on Software Maintenance
(ICSM 2002), Maintaining Distributed Heterogeneous
Systems, 3-6 October 2002, Montreal, Quebec,
Canada. IEEE Computer Society, 2002, pp. 478–481.
[4] O. Maqbool and H. A. Babri,“Hierarchical clustering
for software architecture recovery,” IEEE Transactions
on Software Engineering , vol. 33, no. 11, pp. 759–780,
2007.
[5] A. Hassan and R. Holt,“Architecture recovery of web
applications,”in Software Engineering, 2002. ICSE
2002. Proceedings of the 24rd International Conference
on, 2002, pp. 349–359.
[6] M. L. Collard, H. H. Kagdi, and J. I. Maletic,“An
XML-based lightweight C++ fact extractor,”in 11th
International Workshop on Program Comprehension
(IWPC 2003), May 10-11, 2003, Portland, Oregon,
USA. IEEE Computer Society, 2003, pp. 134–143.
[7] G. Antoniol, M. Di Penta, and E. Merlo,“An
automatic approach to identify class evolution
discontinuities,”in 7th International Workshop on
Principles of Software Evolution (IWPSE 2004), 6-7
September 2004, Kyoto, Japan . IEEE Computer
Society, 2004, pp. 31–40.
[8] M. W. Godfrey and L. Zou,“Using origin analysis to
detect merging and splitting of source code entities,”
IEEE Trans. Software Eng. , vol. 31, no. 2, pp.
166–181, 2005.
[9] L. Moreno, A. Marcus, L. Pollock, and
K. Vijay-Shanker,“Jsummarizer: An automatic
generator of natural language summaries for java
classes,”in Proceedings of the IEEE International
Conference on Program Comprehension, Formal Tool
Demonstration , ser. ICPC ’13. IEEE, 2013, pp.
230–232.
[10] D. M. Germ´ an, Y. Manabe, and K. Inoue,“A
sentence-matching method for automatic license
identiﬁcation of source code ﬁles,”in ASE 2010, 25th
IEEE/ACM International Conference on Automated
Software Engineering, Antwerp, Belgium, September
20-24, 2010 . ACM, 2010, pp. 437–446.
[11] A. Bacchelli, M. Lanza, and R. Robbes,“Linking
e-mails and source code artifacts,”in Proceedings of
the 32nd ACM/IEEE International Conference on
Software Engineering - Volume 1, ICSE 2010, Cape
Town, South Africa, 1-8 May 2010 . ACM, 2010, pp.
375–384.
[12] J. Ratzinger, T. Sigmund, and H. Gall,“On therelation of refactorings and software defect
prediction,”in Proceedings of the 2008 International
Working Conference on Mining Software Repositories,
MSR 2008, Leipzig, Germany, May 10-11, 2008 .
ACM, 2008, pp. 35–38.
[13] G. Canfora, L. Cerulo, M. Cimitile, and M. Di Penta,
“How changes aﬀect software entropy: an empirical
study,”Empirical Software Engineering , 2012.
[14] K. Prete, N. Rachatasumrit, N. Sudan, and M. Kim,
“Template-based reconstruction of complex
refactorings,”in 26th IEEE International Conference
on Software Maintenance (ICSM 2010), September
12-18, 2010, Timisoara, Romania . IEEE Computer
Society, 2010, pp. 1–10.
[15] G. Antoniol, K. Ayari, M. Di Penta, F. Khomh, and
Y.-G. Gu´ eh´ eneuc,“Is it a bug or an enhancement?: a
text-based approach to classify change requests,”in
Proceedings of the 2008 conference of the Centre for
Advanced Studies on Collaborative Research, October
27-30, 2008, Richmond Hill, Ontario, Canada , 2008,
p. 23.
[16] K. Herzig, S. Just, and A. Zeller,“It’s not a bug, it’s a
feature: how misclassiﬁcation impacts bug prediction,”
in35th International Conference on Software
Engineering, ICSE ’13, San Francisco, CA, USA, May
18-26, 2013 . IEEE Computer Society, 2013, pp.
392–401.
[17] M. Fischer, M. Pinzger, and H. Gall,“Populating a
release history database from version control and bug
tracking systems,”in 19th International Conference on
Software Maintenance (ICSM 2003), The Architecture
of Existing Systems, 22-26 September 2003,
Amsterdam, The Netherlands . IEEE Computer
Society, 2003, pp. 23–.
[18] R. Wu, H. Zhang, S. Kim, and S.-C. Cheung,“ReLink:
recovering links between bugs and changes,”in
SIGSOFT/FSE’11 19th ACM SIGSOFT Symposium
on the Foundations of Software Engineering (FSE-19)
and ESEC’11: 13rd European Software Engineering
Conference (ESEC-13), Szeged, Hungary, September
5-9, 2011 . ACM, 2011, pp. 15–25.
[19] R. Baeza-Yates and B. Ribeiro-Neto, Modern
Information Retrieval . Addison-Wesley, 1999.
[20] R. P. Buse and W. R. Weimer,“Automatically
documenting program changes,”in Proceedings of the
IEEE/ACM international conference on Automated
software engineering , ser. ASE ’10. New York, NY,
USA: ACM, 2010, pp. 33–42.
[21] S. Rastkar and G. C. Murphy,“Why did this code
change?” in Proceedings of the 2013 International
Conference on Software Engineering , ser. ICSE ’13.
Piscataway, NJ, USA: IEEE Press, 2013, pp.
1193–1196.
[22] H. Kagdi, M. L. Collard, and J. I. Maletic,“A survey
and taxonomy of approaches for mining software
repositories in the context of software evolution,” J.
Softw. Maint. Evol. , vol. 19, no. 2, pp. 77–131, Mar.
2007.
[23] J. Wu, A. E. Hassan, and R. C. Holt,“Comparison of
clustering algorithms in the context of software
evolution,”in Proceedings of 21st IEEE International
Conference on Software Maintenance . Budapest,
494Hungary: IEEE CS Press, 2005, pp. 525–535.
[24] A. Bachmann, C. Bird, F. Rahman, P. T. Devanbu,
and A. Bernstein,“The missing links: bugs and
bug-ﬁx commits,”in Proceedings of the 18th ACM
SIGSOFT International Symposium on Foundations
of Software Engineering, 2010, Santa Fe, NM, USA,
November 7-11, 2010 . ACM, 2010, pp. 97–106.
[25] C. S. Corley, N. A. Kraft, L. H. Etzkorn, and S. K.
Lukins,“Recovering traceability links between source
code and ﬁxed bugs via patch analysis,”in Proceedings
of the 6th International Workshop on Traceability in
Emerging Forms of Software Engineering , ser. TEFSE
’11. New York, NY, USA: ACM, 2011, pp. 31–37.
[26] S. Rastkar, G. C. Murphy, and G. Murray,
“Summarizing software artifacts: a case study of bug
reports,”in Proceedings of the 32nd ACM/IEEE
International Conference on Software Engineering -
Volume 1 , ser. ICSE ’10. New York, NY, USA: ACM,2010, pp. 505–514.
[27] R. Lotufo, Z. Malik, and K. Czarnecki,“Modelling the
’hurried’ bug report reading process to summarize bug
reports,”in 28th IEEE International Conference on
Software Maintenance, ICSM 2012, Riva del Garda,
Trento, Italy, September 23-28, 2012 . IEEE
Computer Society, 2012, pp. 430–439.
[28] S. Mani, R. Catherine, V. S. Sinha, and A. Dubey,
“Ausum: approach for unsupervised bug report
summarization,”in Proceedings of the ACM SIGSOFT
20th International Symposium on the Foundations of
Software Engineering , ser. FSE ’12. New York, NY,
USA: ACM, 2012, pp. 11:1–11:11.
[29] S. Haiduc, J. Aponte, L. Moreno, and A. Marcus,“On
the use of automated text summarization techniques
for summarizing source code,”in Proceedings of 17th
IEEE Working Conference on Reverese Engineering .
Beverly, MA: IEEE CS Press, 2010, pp. 35–44.
495