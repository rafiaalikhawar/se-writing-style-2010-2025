A Practical Guide for Using Statistical Tests to Assess
Randomized Algorithms in Software Engineering
Andrea Arcuri
Simula Research Laboratory
P .O. Box 134, 1325 Lysaker, Norway
arcuri@simula.noLionel Briand
Simula Research Laboratory and
University of Oslo
P .O. Box 134, 1325 Lysaker, Norway
briand@simula.no
ABSTRACT
Randomized algorithms have been used to successfully addre ss many
different types of software engineering problems. This typ e of al-
gorithms employ a degree of randomness as part of their logic .
Randomized algorithms are useful for difﬁcult problems whe re a
precise solution cannot be derived in a deterministic way wi thin
reasonable time. However, randomized algorithms produce d iffer-
ent results on every run when applied to the same problem inst ance.
It is hence important to assess the effectiveness of randomi zed algo-
rithms by collecting data from a large enough number of runs. The
use of rigorous statistical tests is then essential to provi de support
to the conclusions derived by analyzing such data. In this pa per, we
provide a systematic review of the use of randomized algorit hms in
selected software engineering venues in 2009. Its goal is no t to per-
form a complete survey but to get a representative snapshot o f cur-
rent practice in software engineering research. We show tha t ran-
domized algorithms are used in a signiﬁcant percentage of pa pers
but that, in most cases, randomness is not properly accounte d for.
This casts doubts on the validity of most empirical results a ssess-
ing randomized algorithms. There are numerous statistical tests,
based on different assumptions, and it is not always clear wh en and
how to use these tests. We hence provide practical guideline s to
support empirical research on randomized algorithms in sof tware
engineering.
Categories and Subject Descriptors
D.2.0 [ Software Engineering ]: General;
I.2.8 [ Artiﬁcial Intelligence ]: Problem Solving, Control Methods,
and Search
General Terms
Algorithms, Experimentation, Reliability, Theory
Keywords
Statistical difference, effect size, parametric test, non -parametric
test, conﬁdence interval, Bonferroni adjustment, systema tic review,
survey.
Permission to make digital or hard copies of all or part of thi s work for
personal or classroom use is granted without fee provided th at copies are
not made or distributed for proﬁt or commercial advantage an d that copies
bear this notice and the full citation on the ﬁrst page. To cop y otherwise, to
republish, to post on servers or to redistribute to lists, re quires prior speciﬁc
permission and/or a fee.
ICSE11 May 21-28 2011, Waikiki, Honolulu , HI, USA
Copyright 2011 ACM 978-1-4503-0445-0/11/05 ...$10.00.1. INTRODUCTION
Many problems in software engineering can be alleviated thr ough
automated support. For example, automated techniques exis t to
generate test cases that satisfy some desired coverage crit eria on
the system under test, such as for example branch [26] and pat h
coverage [22]. Because often these problems are undecidabl e, de-
terministic algorithms that are able to provide optimal sol utions in
reasonable time do not exist. The use of randomized algorith ms
[44] is hence necessary to address this type of problems.
The most well-known example of randomized algorithm in soft -
ware engineering is perhaps random testing [13, 6]. Techniques
that use random testing are of course randomized, as for exam ple
DART [22] (which combines random testing with symbolic exec u-
tion). Furthermore, there is a large body of work on the appli cation
ofsearch algorithms in software engineering [25], as for example
Genetic Algorithms. Since practically all search algorith ms are ran-
domized and numerous software engineering problems can be a d-
dressed with search algorithms, randomized algorithms the refore
play an increasingly important role. Applications of searc h algo-
rithms include software testing [41], requirement enginee ring [8],
project planning and cost estimation [2], bug ﬁxing [7], aut omated
maintenance [43], service-oriented software engineering [9], com-
piler optimisation [11] and quality assessment [32].
A randomized algorithm may be strongly affected by chance. I t
may ﬁnd an optimal solution in a very short time or may never
converge towards an acceptable solution. Running a randomi zed
algorithm twice on the same instance of a software engineeri ng
problem usually produces different results. Hence, resear chers in
software engineering that develop novel techniques based o n ran-
domized algorithms face the problem of how to properly evalu ate
the effectiveness of these techniques.
To analyze the effectiveness of a randomized algorithm, it i s im-
portant to study the probability distribution of its output or various
performance metrics [44]. For example, a practitioner migh t want
to know what is the execution time of those algorithms on average .
But randomized algorithms can yield very complex and high va ri-
ance probability distributions, and hence looking only at a verage
values can be misleading, as we will discuss in more details i n this
paper.
The probability distribution of a randomized algorithm can be
analyzed by running such an algorithm several times in an ind e-
pendent way, and then collecting appropriate data about its results
and performance. For example, consider the case in which we w ant
to ﬁnd failures in software by using random testing (assumin g that
an automated oracle is provided). As a way to assess its perfo r-
mance, we can sample test cases at random until the ﬁrst failu re is
detected. In the ﬁrst experiment, we might ﬁnd a failure afte r sam-
pling 24test cases (for example). We hence repeat this experimentPermission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
ICSE ’11, May 21–28, 2011, Waikiki, Honolulu, HI, USA
Copyright 2011 ACM 978-1-4503-0445-0/11/05 ...$10.00
1a second time (if a pseudo-random generator is employed, we n eed
to use a different seed for it) and then, for example, trigger the ﬁrst
failure when executing the second random test case. If in a th ird ex-
periment we obtain the ﬁrst failure after generating 274test cases,
themean value of these three experiments would be 100. Using
such a mean to characterize the performance of random testin g on
a set of programs would clearly be misleading given the exten t of
its variation.
Since such randomness might hinder the reliability of concl u-
sions when performing the empirical analysis of randomized algo-
rithms, researchers hence face two problems: (1) how many ex -
periments should be run to obtain reliable results, and (2) h ow to
assess in a rigorous way whether such results are indeed reli able.
The answer to these questions lies in the use of statistical tests [52].
There are many books on various aspects of statistics (e.g., [52, 10,
36, 24, 60]), and that research ﬁeld is still growing [60]. No tice
that though statistical testing is used in most if not all sci entiﬁc
domains (e.g., medicine and behavioral science), each ﬁeld has its
own set of constraints to work with. Even within a ﬁeld like so ft-
ware engineering the application context of statistical te sting can
vary signiﬁcantly. When human resources and factors introd uce
randomness (e.g., [15, 28]) in the phenomena under study, th e use
of statistical tests is also required but the constraints we work with
are quite different from those of randomized algorithms, su ch as
for example the size of data samples and the types of distribu tions.
Because of the widely varying situations across domains and the
overwhelming number of statistical tests, each one with its own
characteristics and assumptions, many practical guidelin es have been
provided targeting different scientiﬁc domains, such as bi ology [46]
and medicine [29]. In this paper, we intend to do the same for r an-
domized algorithms in software engineering, as they entail speciﬁc
properties and the application of statistical testing is fa r from easy,
as we will see.
To assess whether the results obtained with randomized algo -
rithms are properly analyzed in software engineering resea rch, and
therefore whether precise guidelines are required, we carr ied out a
small scale systematic review. We limited our analyses to th e year
2009 as our goal was not to perform an exhaustive systematic re-
view but to obtain a representative, recent sample on which t o draw
conclusions. We focused on research venues that deal with al l the
aspects of software engineering, such as IEEE Transactions of Soft-
ware Engineering (TSE), IEEE International Conference on S oft-
ware Engineering (ICSE) and International Symposium on Sea rch
Based Software Engineering (SSBSE). The review shows that s ta-
tistical analyses are either missing, inadequate, or incom plete. For
example, though journal guidelines in medicine require a ma nda-
tory use of standardized effect size measurements [24] to quantify
the effect of treatments, we have not found a single case in wh ich
this was used to measure the relative effectiveness of a rand omized
algorithm. Furthermore, in half of the surveyed empirical a naly-
ses, randomized algorithms were evaluated based on the resu lts of
only one run and all the empirical analyses in TSE were based o n a
maximum of ﬁve runs.
Given our survey’s results, we hence found necessary to devi se
practical guidelines for the use of statistical testing in assessing
randomized algorithms in software engineering applicatio ns. Note
that though guidelines have been provided for other scienti ﬁc do-
mains [46, 29] and for other types of empirical analyses in so ftware
engineering [15, 28], they are not necessarily applicable i n the con-
text of randomized algorithms. Our objective is therefore a ccount
for the speciﬁc properties of randomized algorithms in soft ware en-
gineering applications.
Notice that Ali et al. [3] have recently carried out a systematic re-view of search-based software testing which includes some l imited
guidelines on the use of statistical testing. This paper bui lds upon
that work by: (1) analyzing software engineering as whole an d not
just software testing, (2) considering all types of randomi zed algo-
rithms and not just search algorithms, and (3) giving precis e, prac-
tical, and complete suggestions on many aspects that were ei ther
not discussed or just brieﬂy mentioned in [3] .
The main contributions of this paper can be summarized as fol -
lows:
•We provide a systematic review of the current state of prac-
tice of the use of statistical testing to analyze randomized
algorithms in software engineering. The review shows that
randomness is not properly taken into account in the researc h
literature.
•We provide practical guidelines on the use of statistical te st-
ing that are tailored to randomized algorithms in software
engineering applications and the speciﬁc properties and co n-
straints they entail.
The paper is organized as follows. Section 2 presents the sys tem-
atic review we carried out. Section 3 presents the concept of sta-
tistical difference in the context of randomized algorithm s. Section
4 compares two kinds of statistical tests and discussed thei r impli-
cations in our context. The problem of censored data and how i t
applies to randomized algorithms is discussed in Section 5. How to
measure effect sizes and therefore the practical impact of r andom-
ized algorithms is presented in Section 6. Section 7 investi gates the
question of how many times randomized algorithms should be r un.
The problems associated with multiple tests is discussed in Section
8. Practical guidelines on how to use statistical tests in ou r context
are summarized in Section 9. The threats to validity of our wo rk are
discussed in Section 10. Finally, Section 11 concludes the p aper.
2. SYSTEMATIC REVIEW
Systematic reviews are used to gather, in an unbiased and com -
prehensive way, published research on a speciﬁc subject and ana-
lyze it [30]. Systematic reviews are a useful tool to assess g eneral
trends in published research, and they are becoming increas ingly
common in software engineering [35, 15, 28].
In our review we want to analyze: (RQ1) how often random-
ized algorithms are used in software engineering, (RQ2) how many
runs were used to collect data, and (RQ3) which types of stati stical
analyses were used to analyze those data.
To answer RQ1, we selected two of the main venues that deal
with all aspects of software engineering: IEEE Transaction s of
Software Engineering (TSE) and IEEE International Confere nce
on Software Engineering (ICSE). We also considered the Inte r-
national Symposium on Search-Based Software Engineering ( SS-
BSE), which is a specialized venue devoted to search algorit hms.
Because our goal is not to perform an exhaustive survey of exi sting
works, but simply to get an up-to-date snapshot of current pr actice
regarding the application of randomized algorithms in soft ware en-
gineering research, we only considered 2009 publications.
We only retained full length research papers and, as a result ,20
short papers at ICSE and eight at SSBSE were excluded. A total
of107papers were considered: 48in TSE, 50in ICSE and nine
in SSBSE. These papers were manually checked to verify wheth er
in their empirical analyses randomized algorithms were use d. This
left a total of 16papers using randomized algorithms: three in TSE
(6.25% of the total 48), four in ICSE ( 8%of the total 50) and all
the nine papers in SSBSE ( 100% ).2Notice that we excluded papers in which it was not clear wheth er
randomized algorithms were used. For example, the techniqu es de-
scribed in [27, 57] use external SAT solvers, and those might be
based on randomized algorithms, though we cannot say for sur e.
Furthermore, even if a paper focused on presenting a determi nistic,
novel technique, we included it when randomized algorithms were
used for comparison purposes (e.g., fuzz testing [18]). Tab le 1 sum-
marizes the results of this systematic review for the ﬁnal se lection
of16papers. The ﬁrst thing that results clear is that randomized al-
gorithms are widely used in software engineering (RQ1): We f ound
them in 6%−8%of the articles in TSE and ICSE.
To answer RQ2, the data in Table 1 show the number of times a
technique was run to collect data regarding its performance on each
artifact in the case study. Most of the time, data are collect ed from
only one run of the randomized algorithms. Only six cases out of
16show at least 30runs.
Regarding RQ3, only 5out of 16articles include empirical anal-
yses supported by some kind of statistical testing. More spe ciﬁ-
cally, we can see t-tests and U-tests for when algorithms are com-
pared, and linear regressions when prediction models are bu ilt. How-
ever, no standardized effect size measures (Section 6) are reported
in any of these articles to quantify the relative effectiven ess of al-
gorithms in an interpretable form.
Results in Table 1 clearly show that, when randomized algo-
rithms are employed, empirical analyses in software engine ering
do not properly account for their random nature. Many of the n ovel
proposed techniques may indeed be useful, but the results in Table
1 cast serious doubts on the validity of most existing result s.
Notice that some of empirical analyses in Table 1 do not use st a-
tistical tests since they do not perform any comparison of th e tech-
nique they propose with alternatives. For example, in the aw ard
winning paper at ICSE 2009, a search algorithm (i.e., Geneti c Pro-
gramming) was used and was run 100times on each artifact in the
case study [59]. However this algorithm was not compared aga inst
simpler alternatives or even random search. If we look more c losely
at the reported results in order to assess the implications o f that lack
of comparison, we see that the total number of ﬁtness evaluat ions
was400(a population size of 40individuals that is evolved for 10
generations). This is an extremely low number (for example, for
test data generation in branch coverage it is often the case o f us-
ing100,000ﬁtness evaluations for each branch [26]) and we can
conclude that there is very limited search taking place, whi ch im-
plies that a random search would have likely yielded similar results.
This is directly conﬁrmed in the reported results in [59], in which
in half of the case study the average number of ﬁtness evaluat ions
per run is at most 41, thus implying that, on average, appropriate
patches are found in the random initialization of the ﬁrst po pulation
before the actual evolutionary search even starts. This sho uld not
be surprising as the search operators were tailored to the sp eciﬁc,
small set of bugs of the case study, which then led to an easy se arch
problem. As discussed in [3], a search algorithm should alwa ys be
compared against at least random search in order to check tha t the
algorithm is not simply successful because the search probl em is
easy.
Since comparisons with simpler alternatives (at a very mini mum
random search) is a necessity when one proposes a novel rando m-
ized algorithm or addresses a new software engineering prob lem
[3], statistical testing should be part of all publications reporting
such empirical studies. In this paper we provide speciﬁc gui de-
lines on how to use statistical tests to support comparisons among
randomized algorithms.Table 1: Results of systematic review.
Reference Venue Repetitions Statistical Tests
[1] TSE 1/5 U-test
[40] TSE 1 None
[47] TSE 1 None
[42] ICSE 100 t-test, U-test
[59] ICSE 100 None
[18] ICSE 1 None
[33] ICSE 1 None
[4] SSBSE 1000 Linear regression
[21] SSBSE 30/500 None
[14] SSBSE 100 U-test
[20] SSBSE 50 None
[37] SSBSE 10 Linear regression
[31] SSBSE 10 None
[39] SSBSE 1 None
[34] SSBSE 1 None
[56] SSBSE 1 None
3. STATISTICAL DIFFERENCE
When a novel randomized algorithm Ais developed to address a
software engineering problem, it is common practice to comp are it
against existing techniques, in particular simpler altern atives. For
simplicity, let us consider just one alternative randomize d algo-
rithm, and let us call it B. For example, Bcan be random testing,
andAcan be a search algorithm such as Genetic Algorithms or an
hybrid technique that combines symbolic execution with ran dom
testing (e.g., DART [22]).
To compare Aversus B, we ﬁrst need to decide which criteria
are used in the comparisons. Many different measures ( M) can be
selected depending on the problem at hand and contextual ass ump-
tions, e.g., source code coverage, execution time. Dependi ng on
our choice, we may want to either minimize or maximize M, for
example maximize coverage and minimize execution time.
To enable statistical analysis, we should run both AandBa large
enough number ( n) of times, in an independent way, to collect in-
formation on the probability distribution of Mfor each algorithm.
Astatistical test should then be used to assess whether there is
enough empirical evidence to claim a difference between the two
algorithms (e.g., the novel technique Ais better than the current
state of the art B). A null hypothesis H0is typically deﬁned to
state that there is no difference between AandB. A statistical test
is used to verify whether we should reject the null hypothesi sH0.
However, what aspect of the probability distribution of Mis being
compared depends on the used statistical test. For example, at-test
compares the mean values of two distributions whereas other s tests
focus on the median or proportions, as discussed in Section 4 .
There are two possible types of error when performing statis tical
testing: (I) we reject the null hypothesis when it is true (we are
claiming that there is a difference between two algorithms w hen
actually there is none), and (II) we accept H0when it is false (there
is a difference but we claim the two algorithms to be equivale nt).
The p-value of a statistical test denotes the probability of a Type
I error. The signiﬁcant level αof a test is the highest p-value we
accept for rejecting H0. A typical value, inherited from widespread
practice in natural and social sciences, is α= 0.05.
Notice that the two types of error are conﬂicting; minimizin g the
probability of one of them necessarily tends to increase the prob-
ability of the other. But traditionally there is more emphas is on
not committing a Type I error, a practice inherited from natu ral sci-3ences where the goal is often to establish the existence of a n atural
phenomenon in a conservative manner. In our context we would
only conclude that an algorithm Ais better than Bwhen the prob-
ability of a Type I error is below α. The price to pay for a small
αvalue is that, when the data sample is small, the probability of
a Type II error can be high . The concept of statistical power [10]
refers to the probability of rejecting H0when it is false (i.e., the
probability of claiming statistical difference when there is actually
a difference).
Getting back to our comparison of techniques AandB, let us
assume we obtain a p-value equal to 0.06. Even if one technique
seems signiﬁcantly better than the other in terms of effect s ize (Sec-
tion 6), we would then conclude that there is no difference wh en
using the traditional α= 0.05threshold. In software engineering,
or in the context of decision-making in general, this type of reason-
ing can be counter-productive. The tradition of using α= 0.05,
discussed by Cowles [12], has been established in the early p art of
the last century, in the context of natural sciences, and is s till ap-
plied by many across scientiﬁc ﬁelds. It has, however, an inc reasing
number of detractors [23] who believe that such thresholds a re ar-
bitrary, and that researchers should simply report p-values and let
the reader decide in context.
When we need to make a choice between techniques AandB,
we would like to use the one that is more likely to outperform t he
other. Whether we get a p-value lower than αbears little conse-
quence from a practical standpoint, as in the end we must select
an alternative, e.g., we must select a testing technique to v erify the
system. However, as we will show in Section 7, obtaining p-va lues
lower thanα= 0.05should not be a problem when experimenting
with randomized algorithms. The focus of such experiments s hould
rather be on whether a given technique brings any practicall y sig-
niﬁcant advantage, usually measured in terms of an estimate d effect
size and its conﬁdence interval, an important concept addre ssed in
Section 6.
In practice, the selection of an algorithm would depend on th e p-
value of comparisons, the cost difference among algorithms (e.g.,
in terms of inputs), and the estimated effect size. Given a co ntext-
speciﬁc decision model, the reader, using such information , could
then decide which technique is more likely to maximize beneﬁ ts
and minimizes risk. In the simplest case where compared tech -
niques would have comparable costs, we would simply select t he
technique with the best performance regardless of the p-val ues of
comparisons, even if as a result there is a non-negligible pr obability
that it will bring no particular advantage.
4. PARAMETRIC VS. NON-PARAMETRIC
TESTS
The two most used statistical tests are the t-test and the Mann-
Whitney U-test. These tests are used to compare two data samp les
(e.g., the results of running ntimes algorithm Acompared to B).
Thet-test is parametric , whereas the U-test is non-parametric .
A parametric test makes assumptions on the underlining dist ri-
bution of the data. For example, the t-test assumes normality and
equal variance of the two data samples. A non-parametric tes t
makes no assumption on the distribution of the data. Why there
is the need for two different types of statistical tests? A si mple
answer is that, in general, non-parametric tests are less po werful
than parametric ones. When, due to cost or time constraints, only
small data samples can be collected, one would like to use the most
powerful test available if its assumptions are satisﬁed.
There is a large body of work regarding which of the two tests
should be used [16]. The assumptions of the t-test are in generalnot met. Considering that the variance of the two data sample s is
most of the time different, a Welch test should be used instea d of a
t-test. But the problem of the normality assumption remains.
An approach would be to use a statistical test to assess wheth er
the data is normal, and, if the test is successful, then use a W elch
test. This approach increases the probability of Type I erro r, but is
often not necessary. In fact, the Central Limit theorem tell s us that
t-test and Welch test are robust even when there is strong depa rture
from a normal distribution [52, 55]. But in general we cannot know
how many data points ( n) we need to reach reliable results. A rule
of thumb is to have at least n= 30 for each data sample [52].
There are three main problems with such an approach: (1) if we
need to have a large nfor handling departures from normality, then
it might be advisable to use a non-parametric test since, for a large
n, it might be powerful enough; (2) the rule of thumb n= 30 stems
from analyses in behavioral science, and, to the best of our k nowl-
edge, there is no supporting evidence of its efﬁcacy for rand omized
algorithms in software engineering; (3) the Central Limit t heorem
has its own set of assumptions, which are too often ignored. W e
now discuss points (2) and (3) in more details by accounting f or the
speciﬁc properties of the application of randomized algori thms in
software engineering, using software testing examples. Th is choice
was motivated by the fact that half the publications in searc h-based
software engineering are on software testing [25].
Random testing, when used to ﬁnd a test case for a speciﬁc test -
ing target (e.g., a test case that triggers a failure or cover s a partic-
ular branch/path) follows a geometric distribution. When t here is
more than one testing target, e.g., full structural coverag e, it follows
a coupon’s collector problem distribution [6]. Given θthe proba-
bility of sampling a test case that covers the desired testin g target,
then the expectation of random testing is µ= 1/θand its variance
isδ2= (1−θ)/θ2(see [17]). Figure 1 plots the density function
of a geometric distribution with θ= 0.01and a normal distribution
with sameµandδ2. In this context, the density function repre-
sents the probability that, for a given number of sampled tes t cases
l, we cover the target after sampling exactly ltest cases. For ran-
dom testing, the most likely outcome is l= 1, whereas for a normal
distribution it is l=µ. Notice that the geometric distribution is dis-
crete (i.e., it is deﬁned only on integer values), whereas a n ormal
distribution is continuous. Furthermore, the density func tion of the
normal distribution is always positive for any value, where as for the
geometric distribution it is equal to 0for negative values, where in
this context the values are the number of sampled test cases. There-
fore, a testing technique can never follow a normal distribution in
a strict way, although it might be a reasonable approximatio n.
As it is easily visible in Figure 1, the geometric distributi on has
a very strong departure from normality! Comparisons of nove l
techniques versus random testing (and this is the practice w hen
search algorithms are evaluated [25]) using t-tests are hence very
arguable. Furthermore, in contrast to many physical and beh av-
ioral phenomena, the probability distributions of search a lgorithms
are often strongly departing from normality. A common examp le is
when the search landscape of the addressed problem has trap- like
regions [48].
The Central Limit theorem states that the sumofnrandom vari-
ables converges to a normal distribution [17]. For example, con-
sider the result of throwing a dice. There are only six possib le out-
comes, each one with probability 1/6. If we consider the sum of
two dice (i.e., n= 2), we have 11possible outcomes, from value
2to12. Figure 2 shows that with n= 2, in the case of dice, we
already obtain a distribution that resembles the normal one , even
though with n= 1 it is very far from normality. In our context,
these random variables are the results of the nruns of the analyzed40 50 100 150 2000.002 0.004 0.006 0.008 0.010
Test CasesProbabilityRandom Testing
Normal Distribution
Figure 1: Density functions of random testing and normal
distribution given same mean µ= 1/θand variance σ2=
(1−θ)/θ2, whereθ= 0.01.
algorithm. This theorem has three assumptions: the nvariables
should be independent and their mean µand variance δ2should
exist (i.e., they should be different from inﬁnite). When us ing ran-
domized algorithms, having nindependent runs is usually trivial to
achieve (we just need to use different seeds for the pseudo-r andom
generators). But the existence of the mean and variance requ ires
more scrutiny. As shown before, those values µandδ2exist for
random testing. A well known “paradox” in statistics in whic h
mean and variance do not exist is the Petersburg Game [17]. Si m-
ilarly, the existence of mean and variance in search algorit hms is
not always guaranteed, as discussed next.
If the performance of a randomized algorithm is bounded with in
a predeﬁned range, then the mean and variance would always ex ist.
For example, if an algorithm is run for a preﬁx amount of time
to achieve structural coverage for software testing, and th ere arek
structural targets, then the performance of the algorithm w ould be
measured with a value between 0andk. Therefore, we would have
µ≤kandδ2≤k2, so using a t-test would be valid.
The problems arise if no bound is given on how the performance
is measured. A randomized algorithm could be run until it ﬁnd s an
optimal solution to the addressed problem. For example, ran dom
testing could be run until the ﬁrst failure is triggered (ass uming an
automated oracle is provided). In this case, the performanc e of the
algorithm would be measured in the number of test cases that a re
sampled before triggering the failure and there would be no u pper
limit for a run. If we run a search algorithm on the same proble m
ntimes, and we have nvariablesXirepresenting the number of
test cases sampled in each run before triggering the ﬁrst fai lure,
we would estimate the mean with ˆµ=1
n/summationtextn
i=1Xi, and hence
conclude that the mean exists. As Petersburg Game shows, thi s can
be wrong, because ˆµis only an estimation ofµ, which might not
exist.
For most search algorithms convergence in ﬁnite time is prov en
under some conditions (e.g., [53]), and hence mean and varia nce
exist. But in software engineering, when new problems are ad -
dressed, standard search algorithms with standard search o perators
may not be usable. For example, when testing for object-orie nted
software using search algorithms, complex non-standard se arch op-
erators are required. Without formal proofs, it is not safe t o speak
about the existence of the mean in those cases.
However, the non-existence of the mean is usually not a prob-
lem from a practical standpoint. In practice, there usually are up-per limits to the amount of computational resources a random ized
algorithm can use. For example, a search algorithm can be pre ma-
turely stopped when reaching a time limit. Random testing co uld
be stopped after 100,000sampled test cases (for example) if it has
found no failure so far. But in these cases, we are actually de aling
with censored data [36] (in particular, right-censorship) and this
requires proper care in terms of statistical testing and the interpre-
tation of results, as discussed in Section 5.
Even under proper conditions for using a parametric test, on e as-
pect that is often ignored is that t-test and U-test are two different
approaches to analyze two different properties. Let us use a random
testing example in which we identify the ﬁrst test case that t riggers
a failure. Considering a failure rate θ, the mean value of sampled
test cases done by random testing is hence µ= 1/θ. Let us assume
that a novel testing technique Ayields a normal distribution of the
required number of test cases to trigger a failure. If we furt her con-
sider the same variance as random testing and mean that is 85%
of the one of random testing, which one is better? Random test ing
with meanµorAwith mean 0.85µ? Assuming a large number
of runs (e.g., nis equal to one million), a t-test would state that A
is better, whereas a Mann-Whitney U-test would state exactl y the
opposite. How come? This is not an error but the two tests are m ea-
suring different things: The t-test measures the difference in mean
values whereas the Mann-Whitney U-test deals with their sto chas-
tic ranking, i.e., whether observations in one data sample a re more
likely to be larger than observations in the other sample. No tice that
this latter concept is technically different from detectin g difference
in the median values (which can be stated only if the two distribu-
tions have same shape). In a normal distribution, the median value
is equal to the mean, whereas in a geometric distribution the me-
dian is roughly 70% of the mean [17]. On one hand, half of the
data points for random testing would be lower than 0.7µ. On the
other hand, for Awe have half of the data points above 0.85µ, and
a signiﬁcant proportion between 0.7µand0.85µ. This explains the
apparent contradiction in results.
From a practical point of view, which statistical test shoul d be
used? Based on the discussions in this section, in contrast t o [54]
and in line with [38], we suggest to use Mann-Whitney U-test r ather
than t-test and Welch test. However, the full motivation will be-
come clear only once we discuss censored data, effect size, a nd the
choice ofnin the next sections.
In the discussion above, we have assumed that both AandBare
randomized. If one of them is deterministic (e.g., B), it is still im-
portant to use statistical testing. Consistent with the abo ve recom-
mendation, the One-Sample Wilcoxon test should be used. Given
mBthe performance measure of the deterministic algorithm, a o ne-
sample Wilcoxon test would verify whether the performance o fA
is symmetric about mB, i.e., whether by using Aone is as likely to
obtain a value lower than mBas otherwise.
5. CENSORED DATA
Assume that the result of an experiment is dichotomous: eith er
we ﬁnd a solution to solve the software engineering problem a t
hand ( success ), or we do not ( failure ). For example, in software
testing, if our goal is to cover a particular target (e.g., a s peciﬁc
branch), we can run a randomized algorithm with a time limit L.
We will stop the algorithm as soon as we ﬁnd a solution, otherw ise
we stop it after time L. The choice of Ldepends on the available
computational resources. Another example is bug ﬁxing [59] where
we ﬁnd a patch within time L, or we do not.
These types of experiments are dealing with right-censored data,
and their properties are equivalent to survival/failure ti me analysis51 2 3 4 5 6
Dice ValuesProbability
0.00 0.05 0.10 0.15 0.20
2 3 4 5 6 7 8 9 10 11 12
Sum of Two Dice ValuesProbability
0.00 0.05 0.10 0.15 0.20
Figure 2: Density functions of the outputs of one dice and the
sum of two dice.
[36]. LetXbe the random variable representing the time a ran-
domized algorithm takes to solve a software engineering pro blem,
and let us consider nexperiments in which we collect Xivalues.
We are dealing with right-censorship since, assuming a time limit
L, we will not have observations Xifor the cases X > L . There
are several ways to deal with this problem [36] and we will lim it
our discussion to simple solutions.
One interesting special case is when we cannot say for sure wh ether
we have achieved our target, e.g., generation of test cases t hat achieve
code branch coverage. Even when using a time limit L, in these
cases we are not tackling censored data. Putting aside trivi al cases,
there are usually infeasible targets (e.g., unreachable co de) and
their number is unknown. As a result, such experiments are no t
dichotomous because we cannot know whether we have covered
all feasible targets. However, if in the experiments the com parisons
are made reusing a case study from the literature, and if we wa nt
to know whether within a given time we can obtain better cover -
age than reported studies, then such experiments can be cons idered
dichotomous despite infeasible targets.
Let us consider the case in which we need to compare two ran-
domized algorithms AandBon a software engineering problem
with dichotomous outcome. Let Xbe the random variable repre-
senting the time Atakes to ﬁnd a valid solution, and let Ybe the
same type of variable for B. Let us assume that we run Antimes
collecting observations Xi, and we do the same for B. Using a time
limitL, to evaluate which of the two algorithms is better, we can
consider their success rate , i.e., the number of times out of the n
runs in which they ﬁnd a valid solution. To evaluate whether t here
is statistical difference between the success rates of AandB, a test
for differences in proportions is then appropriate, such as the Fisher
exact test [36].
If there is no statistically or practically signiﬁcant diff erence be-
tween the two success rates, from a practical standpoint, th e prac-
titioner would then be interested to know which technique yi elds
a valid solution in lesstime. This is particularly important if the
success rates are high. There can be different ways to analyz e such
cases, such as considering artiﬁcial censorships at differ ent times
beforeL. For example, we can consider censorship at L/2, i.e.,
the success rate with half the time. Note that such analysis d oes
not require to run any further experiments. Another way is to ap-
ply a Mann-Whitney U-test, recommended above, using only th e
times of successful runs, for which XiandYiare lower than L.
One more complex situation is when one algorithm shows a sig-
niﬁcantly higher success rate, but takes more time to produc e valid
solutions. Since these two variables are not necessarily co rrelated,
a careful decision must then be made in these situations.6. EFFECT SIZE
When comparing a randomized algorithm Aagainst another B,
given a large enough number of runs n, it is most of the time pos-
sible to obtain statistically signiﬁcant results with a t-test or U-test.
Indeed, two different algorithms are extremely unlikely to have ex-
actly the same probability distribution. In other words, wi th large
enoughnwe can obtain statistically difference even if that differ-
ence is so small as to be of no practical value.
Though it is important to assess whether an algorithm fares s ta-
tistically better than another, it is in addition crucial to assess the
magnitude of the improvement. To analyze such a property, effect
sizemeasures are needed [24, 28, 46]. In their systematic review of
empirical analyses in software engineering, Kampenes et al. [28]
found out that standardized effect sizes were reported in on ly29%
of the cases. In our review, we found none.
Effect sizes can be divided in two groups: standardized and u n-
standardized. Unstandardized effect sizes are dependent f rom the
unit of measurement used in the experiments. Let us consider the
difference in mean between two algorithms ∆ =µA−µB. This
value ∆has a measurement unit, that of AandB. For example, in
software testing, µcan be the expected number of test executions
to ﬁnd the ﬁrst failure. On one testing artifact we might have ∆1=
µA−µB= 100 −1 = 99 , whereas on another testing artifact we
might have ∆2=µA−µB= 100,000−200,000 = −100,000.
Deciding based on ∆1and∆2which algorithm is better is difﬁcult
to determine since the two scales of measurement are differe nt.∆1
is very low compared to ∆2, but in that case Ais100times worse
thanB, whereas it is only twice as fast in the case ∆2. Empirical
analyses of randomized algorithms, if they are to be reliabl e and
generalizable, require the use of large numbers of artifact s (e.g.,
programs). The complexity of these artifacts is likely to wi dely
vary, such as the number of test cases required to fulﬁll a cov erage
criterion on various programs. The use of standardized effe ct sizes,
that are independent from the evaluation criteria measurem ent unit,
is therefore necessary to be able to compare results across a rtifacts
and experiments.
In this section we ﬁrst describe which is the most known stan-
dardized effect size measure and why it should notbe used. We
then describe two other standardized effect sizes, and how t o ap-
ply them in practice. The most known effect size is the so call edd
family which, in the general form, it is d= (µA−µB)/σ. In other
words, the difference in mean is scaled over the standard dev ia-
tion (several corrections exists to this formula, but for mo re details
please see [24]). Though we obtain a measure that has no measu re-
ment unit, the problem is that it assumes the normality of the data,
and strong departures can make it meaningless [24]. For exam ple,
in a normal distribution, roughly 64% of the points lie within µ±σ
[17], i.e., they are at most σaway from the mean µ. But for dis-
tributions with high skewness (as in the geometric distribu tion and
as it is often the case for search algorithms), the results of scal-
ing the mean difference by the standard deviation “would not be
valid” [24], because “standard deviations can be very sensi tive to
a distribution’s shape” [24]. In this case, a non-parametri c effect
size should be preferred. Existing guidelines in [28, 46] ju st brieﬂy
discuss the use of non-parametric effect sizes.
The Vargha and Delaney’s ˆA12statistics is a non-parametric ef-
fect size measure [58, 24]. Its use has been advocated in [38] , and
one example of its use in software engineering in which rando m-
ized algorithms are involved can be found in [50]. In our cont ext,
given a performance measure M, the ˆA12statistics measures the
probability that running algorithm Ayields higher Mvalues than
running another algorithm B. If the two algorithms are equivalent,
then ˆA12= 0.5. This effect size is easier to interpret compared6to thedfamily. For example, ˆA12= 0.7entails we would obtain
higher results 70% of the time with A. Though this type of non-
parametric effect size is not common in statistical tools, i t can be
very easily computed [38, 24]. The following formula is repo rted
in [58]:
ˆA12= (R1/m−(m+ 1)/2)/n (1)
whereR1is the rank sum of the ﬁrst data group we are com-
paring. The rank sum is a basic component in the Mann-Whitney
U-test, and most statistical tools provide it. In that formu la,mis
the number of observations in the ﬁrst data sample, whereas nis the
number of observations in the second data sample. In most exp eri-
ments, we would run two randomized algorithms the same numbe r
of times:m=n.
When dealing with dichotomous results (as discussed in Sect ion
5), several types of effect size measures [24] can be conside red.
The odds ratio is the most used and “is a measure of how many
times greater the odds are that a member of a certain populati on
will fall into a certain category than the odds are that a memb er of
another population will fall into that category” [24]. Give nathe
number of times algorithm Aﬁnds an optimal solution, and bfor
algorithm B, the odds ratio is calculated as ψ=a+ρ
n+ρ−a/b+ρ
n+ρ−b,
whereρis any arbitrary positive constant (e.g., ρ= 0.5) used to
avoid problems with zero occurrences [24]. There is no diffe rence
between the two algorithms when ψ= 1. The cases in which
ψ> 1implies that algorithm Ahas higher chances of success.
Both ˆA12andψare standardized effect size measures. But be-
cause their calculation is based on a ﬁnite number of observa tions
(e.g.,nfor each algorithm, so 2nwhen we compare two algo-
rithms), they are only estimates of the real ˆA∗
12andψ∗. Ifnis
low, these estimations might be very inaccurate. One way to d eal
with this problem is to calculate conﬁdence intervals (CI) for them
[24]. A (1−α)CI is a a set of values for which there is (1−α)
probability that the value of the effect size lies in that ran ge. For
example, if we have ˆA12= 0.54and a (1−α)CI with range
[0.49,59], then with probability (1−α)the real value ˆA∗
12lies in
[0.49,59](where ˆA12= 0.54is its most likely estimation). Such
effect size conﬁdence intervals lead intuitively to decisi on making
as beneﬁts, which are directly related to effect size, can be com-
pared to the costs of using alternative algorithms while acc ounting
for uncertainty. To see how conﬁdence intervals can be calcu lated,
please see [24] and [58].
Notice that a conﬁdence interval can replace a test of statis tical
difference (e.g., t-test and U-test). If the null hypothesis H0lies
within the conﬁdence interval, then there is no enough stati stical
evidence to claim there is a statistically signiﬁcant diffe rence. In
the previous example, because 0.5is inside the (1−α)CI[0.49,59],
then there is no statistical difference at the selected sign iﬁcance
levelα. For a dichotomous result, H0would beψ= 1.
7. NUMBER OF RUNS
How many runs do we need when we analyze and compare ran-
domized algorithms? As many as necessary to show with high co n-
ﬁdence that the obtained results are statistically signiﬁc ant and to
obtain a small enough conﬁdence interval for effect size est imates.
In many ﬁelds of science (e.g., medicine and behavioral scie nce),
a common rule of thumb is to use at least n= 30 observations.
In the many ﬁelds where experiments are very expensive and ti me
consuming, it is in general not feasible to work with high val ues
forn. Several new statistical tests have been proposed and dis-
cussed to cope with the problem of lack of power and violation
of assumptions (e.g., normality of data) when smaller numbe rs ofobservations are available [60].
Empirical studies of randomized algorithms do not involve h u-
man subjects and the number of runs (i.e.,n) is only limited by
computational resources. When there is access to clusters o f com-
puters as this is the case for many research institutes and un iversi-
ties, and when there is no need for expensive, specialized ha rdware
(e.g., in hardware-in-the-loop testing), then large numbe rs of runs
can be carried out to properly analyze the behavior of random ized
algorithms. Many software engineering problems are furthe rmore
not highly computationally expensive, as for example code c over-
age at the unit testing level, and can therefore involve very large
numbers of executions. There are however exceptions, such a s the
system testing of embedded systems (e.g., [5]) where each te st case
can be very expensive to run.
Whenever possible, in most cases, it is therefore recommend ed
to use a very high number of runs. For most problems in softwar e
engineering, thousands of runs should not be a problem and wo uld
solve most of the problems related to the power and accuracy o f sta-
tistical tests. For example, as illustrated in [42, 14] in Ta ble 1, even
when 100runs are used the U-test might be not powerful enough
to conﬁrm a statistical difference at a 0.05signiﬁcance level, even
when the data seem to suggest such a difference.
Most discussions in the literature about statistical tests focus on
situations with small numbers of observations (e.g., as in [ 54]).
However, with thousands of runs, one would detect statistic ally sig-
niﬁcant differences on practically any experiment (see Sec tion 3).
It is hence essential to complement such analyses with the st udy of
the effect size as discussed in Section 6. Even when having la rge
numbers of runs may not be necessary for a set αlevel (e.g., 0.05)
if differences of practical signiﬁcance also show p-values less than
α, additional runs would help tighten the conﬁdence interval s for
effect size and would be of practical value.
In Section 3, we suggested to use U-test instead of t-test. For
very large samples, such as n= 1,000, there would be no prac-
tical difference between them regarding power and accuracy . The
choice of a non-parametric test would be driven by its effect size
measure. In Section 6 we argued that effect size measures bas ed on
the mean (i.e., the dfamily) were not appropriate for randomized
algorithms in software engineering. It would be pointless t o detect
statistical difference of mean values with a t-test if then we cannot
use a reliable measure for its effect size. In other words, it is ad-
visable to use size measures that are consistent with the dif ferences
being tested by the selected statistical test.
8. MULTIPLE TESTS
In most situations, we need to compare several alternative a l-
gorithms. Furthermore, if we are comparing different algor ithm
settings (e.g., population sizes in a Genetic Algorithm), t hen each
setting technically deﬁnes a different algorithm. This oft en leads
to a large number of statistical comparisons. It is possible to use
statistical tests that deal with multiple techniques (trea tments, ex-
periments) at the same time (e.g., Factorial ANOV A), and eff ect
size has been deﬁned for those cases [24]. However, in our app li-
cation context, we would like to know the performance of each al-
gorithm compared against all other alternatives individua lly. Given
a set of algorithms, we would not be interested to simply dete rmine
whether all of them have the same mean values. Rather, given K
algorithms, we want to perform Z=K(K−1)/2pairwise tests
and measure effect size in each case.
However, using several statistical tests inﬂates the proba bility of
Type I error. If we have only one comparison, the probability of
Type I error is equal to the obtained p-value. If we have many
comparisons, even when all the p-values are low, there is usu ally7a high probability that at least in one of the comparisons the null
hypothesis is true as all these probabilities somehow add up . In
other words, if in all the comparisons the p-values are lower than
α, then we would normally reject all the null hypotheses. But t he
probability that at least one null hypothesis is true could b e as high
as1−(1−α)ZforZcomparisons, which converges to 1asZ
increases.
One way to address this problem is to use the so called Bonfer-
roni adjustment [49, 45]. Instead of applying each test assuming
a signiﬁcance level α, we would use an adjusted level α/Z. For
example, if we want a 0.05probability of Type I error and we have
two comparisons, we would need to use two statistical tests w ith
a0.025α, and then check whether both differences are signiﬁcant
(i.e., if both p-values are lower than 0.025). However, the Bonfer-
roni adjustment has been seriously criticized in the litera ture [49,
45], and we largely agree with those critiques. For example, let us
assume that for both those tests we obtain p-values equal to 0.04.
If a Bonferroni adjustment is used, then both tests will not b e sta-
tistically signiﬁcant. A researcher could be tempted to pub lish the
results of only one of them and claiming statistical signiﬁc ance be-
cause 0.04<0.05. Such a practice can therefore hinder scientiﬁc
progress by reducing the number of published results [49, 45 ]. This
would be particularly true in our application context in whi ch many
randomized algorithms can be compared to address the same so ft-
ware engineering problem: it would be very tempting to leave out
the results of some of the poorly performing algorithms. Tho ugh
we do not recommend the Bonferroni adjustment, it is importa nt to
always report the obtained p-values, not just whether a diff erence
is signiﬁcant or not. If for some reasons the readers want to e valu-
ate the results using a Bonferroni adjustment or any of its va riants,
then it is possible to do so. For a full list of other problems r elated
to the Bonferroni adjustment, please see [49, 45]. Notice th at there
are other adjustment techniques that are equivalent to Bonf erroni
but that are less conservative [19]. However, the statistic al signiﬁ-
cance of a single comparison would still depend on the number of
performed and reported comparisons.
In Section 3 we stated that in software engineering in genera l,
and for randomized algorithms in particular, we mostly deal with
decision-making problems. For example, if we must test soft ware,
then we must choose one alternative among Kdifferent techniques.
In this case, even if the p-values are higher than α, we need to test
the software anyhow and we must make a choice. In this context ,
Bonferroni-like adjustments make even less sense. Just cho osing
one alternative at random because there is no statistically signiﬁ-
cant difference does not make much sense as it ignores availa ble
information.
9. PRACTICAL GUIDELINES
Based on the above discussions, we propose a set of practical
guidelines for the use of statistical tests in experiments c omparing
randomized algorithms. Though we expect exceptions, given the
current state of practice (Section 2 and [3, 28]), we believe that
it is important to provide practical guidance that will be va lid in
most cases and enable higher quality studies to be reported. We
recommend that practitioners follow these guidelines and j ustify
any necessary deviation.
There are many statistical tools that are available. In the f ol-
lowing we will provide examples based on R[51], because it is a
powerful tool that is freely available and supported by many statis-
ticians. But any other professional tool would provide simi lar ca-
pabilities.
Practical guidelines are summarized as follows. Notice tha t of-
ten, for reasons of space, it is not possible to report all the data ofthe statistical tests. Based on the circumstances, authors need to
make careful choices on what to report.
•On each problem instance (e.g., program) in the case study,
run each randomized algorithm at least n= 1,000times. If
this is not possible, explain the reasons and report the tota l
amount of time it took to run the entire case study. If for
example 30runs were performed and the total execution time
was just one hour, then it is rather difﬁcult to justify why a
higher number of runs was not used to gain statistical power,
lower p-values, and narrow the conﬁdence interval of effect
size estimates.
•For detecting statistical differences, use the non-parame tric
Mann-Whitney U-test for interval-scale results and the Fis her
exact test for dichotomous results (i.e., in the cases of cen -
sored data as discussed in Section 5). For the former case,
inRyou can use the function “w=wilcox.test(X,Y)” where
XandYare the data sets with the observations of the two
compared randomized algorithms. If you are comparing a
randomized algorithm against a deterministic one, use
“w=wilcox.test(X,mu=D)”, where Dis the resulting mea-
sure of the deterministic algorithm. When we have number
of successes afor the ﬁrst algorithm and bfor the second,
you can use “f=ﬁsher.test(m)”, where mis a matrix derived
in this way: “m =matrix(c(a,n-a,b,n-b),2,2)”. A ρ= 0.5
could be added to each cell of the matrix to handle the zero
occurrence cases.
•Report all the obtained p-values, whether they are smaller
thanαor not, and not just whether differences are signiﬁcant.
•Always report standardized effect size measures. For di-
chotomous results, the odds ratio ψ(and its conﬁdence inter-
val) is automatically calculated with “f=ﬁsher.test(m)”. For
interval-scale results and the ˆA12effect size, the rank sum
R1used in Equation 1 can be calculated with
“R1=sum(rank(c(X,Y))[seq_along(X)])”. It is also strong ly
advised to report effect size conﬁdence intervals (but the s up-
port for ˆA12is unfortunately limited). This is in fact a much
easier to use substitute to p-values for decision making whe re
potential beneﬁts can be compared to costs while accounting
for uncertainty.
•To help the meta-analyses of published results across studi es,
report means and standard deviations (so that effect sizes i n
thedfamily can be used). For dichotomous experiments,
always report the values aandb(so that other types of effect
sizes can be computed [24]).
•If space permits, provide full statistics for the collected data,
as for example mean, median, variance, min/max values, skew -
ness, median and absolute deviation. Box-plots are also use -
ful to visualize them.
•When analyzing more than two randomized algorithms, use
pairwise comparisons followed by pairwise statistical tes ts
and effect size measures.
10. THREATS TO VALIDITY
The systematic review in Section 2 is based on only three sour ces,
from which only 16out of 135papers were selected. A larger re-
view might lead to different results, although we can safely argue
that TSE and ICSE are representative of research trends in so ftware
engineering. Furthermore, that review is only used as a moti vation8for providing practical guidelines, and its results are in l ine with
other larger systematic reviews [3, 28]. Last, papers somet imes
lack precision and interpretation errors are always possib le.
As already discussed in Section 9, our practical guidelines may
not be applicable to all contexts. Therefore, in every speci ﬁc con-
text, one should always carefully assess them. For some spec iﬁc
cases, other statistical procedures could be preferable, e specially
when only few runs are possible.
11. CONCLUSION
In this paper we report on a systematic review to evaluate how
the results of randomized algorithms in software engineeri ng are
analyzed. This type of algorithms (e.g., Genetic Algorithm s) are
widely used to address many software engineering problems, such
as test case selection. Similar to previous systematic revi ews on re-
lated topics [3, 28], we conclude that the use of rigorous sta tistical
methodologies are somehow lacking when investigating rand om-
ized algorithms in software engineering.
To cope with this problem, we provide practical guidelines tar-
geting researchers in software engineering. In contrast to other
guidelines in the literature for other scientiﬁc ﬁelds (e.g ., [46] and
[29]), the guidelines in this paper are tailored to the speci ﬁc proper-
ties of randomized algorithms when applied to software engi neer-
ing problems. The use of these guidelines is important in ord er to
develop a reliable body of empirical results over time, whic h enable
comparisons across studies and which will converge towards gen-
eralizable results of practical importance. Otherwise, as in many
other aspects of software engineering, unreliable results would pre-
vent effective technology transfer and would limit the impa ct of
research on practice.
Acknowledgements
We would like to thanks Lydie du Bousquet and Zohaib Iqbal for
useful comments on an early draft of this paper. The work de-
scribed in this paper was supported by the Norwegian Researc h
Council. This paper was produced as part of the ITEA-2 projec t
called VERDE.
12. REFERENCES
[1] R. Abraham and M. Erwig. Mutation Operators for
Spreadsheets. IEEE Transactions on Software Engineering
(TSE) , 35(1), 2009.
[2] J. Aguilar-Ruiz, I. Ramos, J. C. Riquelme, and M. Toro. An
evolutionary approach to estimating software development
projects. Information and Software Technology , 43:875–882,
2001.
[3] S. Ali, L. Briand, H. Hemmati, and R. Panesar-Walawege. A
systematic review of the application and empirical
investigation of search-based test-case generation. IEEE
Transactions on Software Engineering (TSE) , 2009.
[4] A. Arcuri. Full theoretical runtime analysis of alterna ting
variable method on the triangle classiﬁcation problem. In
International Symposium on Search Based Software
Engineering (SSBSE) , pages 113–121, 2009.
[5] A. Arcuri, M. Z. Iqbal, and L. Briand. Black-box system
testing of real-time embedded systems using random and
search-based testing. In IFIP International Conference on
Testing Software and Systems (ICTSS) , pages 95–110, 2010.
[6] A. Arcuri, M. Z. Iqbal, and L. Briand. Formal analysis of t he
effectiveness and predictability of random testing. In ACM
International Symposium on Software Testing and Analysis
(ISSTA) , pages 219–229, 2010.[7] A. Arcuri and X. Yao. A novel co-evolutionary approach to
automatic software bug ﬁxing. In IEEE Congress on
Evolutionary Computation (CEC) , pages 162–168, 2008.
[8] A. J. Bagnall, V . J. Rayward-Smith, and I. M. Whittley. Th e
next release problem. Information and Software Technology ,
43(14):883–890, 2001.
[9] G. Canfora, M. D. Penta, R. Esposito, and M. L. Villani. An
approach for qos-aware service composition based on geneti c
algorithms. In Genetic and Evolutionary Computation
Conference (GECCO) , pages 1069–1075, 2005.
[10] J. Cohen. Statistical power analysis for the behaviora l
sciences, 1988.
[11] K. D. Cooper, P. J. Schielke, and D. Subramanian.
Optimizing for reduced code space using genetic algorithms .
InProceedings of the ACM SIGPLAN workshop on
Languages, compilers, and tools for embedded systems ,
pages 1–9, 1999.
[12] M. Cowles and C. Davis. On the origins of the .05 level of
statistical signiﬁcance. American Psychologist ,
37(5):553–558, 1982.
[13] J. W. Duran and S. C. Ntafos. An evaluation of random
testing. IEEE Transactions on Software Engineering (TSE) ,
10(4):438–444, 1984.
[14] J. Durillo, Y . Zhang, E. Alba, and A. Nebro. A Study of the
Multi-objective Next Release Problem. In International
Symposium on Search Based Software Engineering (SSBSE) ,
pages 49–58, 2009.
[15] T. Dybå, V . Kampenes, and D. Sjøberg. A systematic revie w
of statistical power in software engineering experiments.
Information and Software Technology (IST) , 48(8):745–755,
2006.
[16] M. Fay and M. Proschan. Wilcoxon-Mann-Whitney or t-tes t?
On assumptions for hypothesis tests and multiple
interpretations of decision rules. Statistics Surveys , 4:1–39,
2010.
[17] W. Feller. An Introduction to Probability Theory and Its
Applications, Vol. 1 . Wiley, 3 edition, 1968.
[18] V . Ganesh, T. Leek, and M. Rinard. Taint-based directed
whitebox fuzzing. In ACM/IEEE International Conference
on Software Engineering (ICSE) , pages 474–484, 2009.
[19] L. García. Escaping the Bonferroni iron claw in ecologi cal
studies. Oikos , 105(3):657–663, 2004.
[20] B. Garvin, M. Cohen, and M. Dwyer. An improved
meta-heuristic search for constrained interaction testin g. In
International Symposium on Search Based Software
Engineering (SSBSE) , pages 13–22, 2009.
[21] K. Ghani, J. Clark, and Y . Heslington. Widening the Goal
Posts: Program Stretching to Aid Search Based Software
Testing. In International Symposium on Search Based
Software Engineering (SSBSE) , pages 122–131, 2009.
[22] P. Godefroid, N. Klarlund, and K. Sen. Dart: directed
automated random testing. In ACM Conference on
Programming language design and implementation (PLDI) ,
pages 213–223, 2005.
[23] S. Goodman. Toward evidence-based medical statistics . 1:
The P value fallacy. Annals of Internal Medicine ,
130(12):995–1004, 1999.
[24] R. Grissom and J. Kim. Effect sizes for research: A broad
practical approach . Lawrence Erlbaum, 2005.
[25] M. Harman, S. A. Mansouri, and Y . Zhang. Search based
software engineering: A comprehensive analysis and review9of trends techniques and applications. Technical Report
TR-09-03, King’s College, 2009.
[26] M. Harman and P. McMinn. A theoretical and empirical
study of search based testing: Local, global and hybrid
search. IEEE Transactions on Software Engineering (TSE) ,
36(2):226–247, 2010.
[27] H. Hsu and A. Orso. MINTS: A general framework and tool
for supporting test-suite minimization. In ACM/IEEE
International Conference on Software Engineering (ICSE) ,
pages 419–429, 2009.
[28] V . Kampenes, T. Dybå, J. Hannay, and D. Sjøberg. A
systematic review of effect size in software engineering
experiments. Information and Software Technology (IST) ,
49(11-12):1073–1086, 2007.
[29] M. Katz. Multivariable analysis: a practical guide for
clinicians . Cambridge Univ Pr, 2006.
[30] K. Khan, R. Kunz, J. Kleijnen, and G. Antes. Systematic
reviews to support evidence-based medicine: how to review
and apply ﬁndings of healthcare research . RSM Press, 2004.
[31] U. Khan and I. Bate. WCET analysis of modern processors
using multi-criteria optimisation. In International
Symposium on Search Based Software Engineering (SSBSE) ,
pages 103–112, 2009.
[32] T. Khoshgoftaar, L. Yi, and N. Seliya. A multiobjective
module-order model for software quality enhancement. IEEE
Transactions on Evolutionary Computation (TEC) ,
8(6):593–608, 2004.
[33] A. Kieyzun, P. Guo, K. Jayaraman, and M. Ernst. Automati c
creation of SQL injection and cross-site scripting attacks . In
ACM/IEEE International Conference on Software
Engineering (ICSE) , pages 199–209, 2009.
[34] D. Kim and S. Park. Dynamic Architectural Selection: A
Genetic Algorithm Based Approach. In International
Symposium on Search Based Software Engineering (SSBSE) ,
pages 59–68, 2009.
[35] B. Kitchenham, O. Pearl Brereton, D. Budgen, M. Turner,
J. Bailey, and S. Linkman. Systematic literature reviews in
software engineering-A systematic literature review.
Information and Software Technology (IST) , 51(1):7–15,
2009.
[36] J. Klein and M. Moeschberger. Survival analysis: techniques
for censored and truncated data . Springer Verlag, 2003.
[37] S. Kpodjedo, F. Ricca, G. Antoniol, and P. Galinier.
Evolution and Search Based Metrics to Improve Defects
Prediction. In International Symposium on Search Based
Software Engineering (SSBSE) , pages 23–32, 2009.
[38] N. Leech and A. Onwuegbuzie. A Call for Greater Use of
Nonparametric Statistics. Technical report, US Dept.
Education, 2002.
[39] A. Marchetto and P. Tonella. Search-based testing of Aj ax
web applications. In International Symposium on Search
Based Software Engineering (SSBSE) , pages 3–12, 2009.
[40] A. Masood, R. Bhatti, A. Ghafoor, and A. Mathur. Scalabl e
and Effective Test Generation for Role-Based Access
Control Systems. IEEE Transactions on Software
Engineering (TSE) , pages 654–668, 2009.
[41] P. McMinn. Search-based software test data generation : A
survey. Software Testing, Veriﬁcation and Reliability ,
14(2):105–156, 2004.
[42] T. Menzies, S. Williams, B. Boehm, and J. Hihn. How to
avoid drastic software process change (using stochasticstability). In ACM/IEEE International Conference on
Software Engineering (ICSE) , pages 540–550, 2009.
[43] B. S. Mitchell and S. Mancoridis. On the automatic
modularization of software systems using the bunch tool.
IEEE Transactions on Software Engineering (TSE) ,
32(3):193–208, 2006.
[44] M. Motwani and P. Raghavan. Randomized Algorithms .
Cambridge University Press, 1995.
[45] S. Nakagawa. A farewell to Bonferroni: the problems of l ow
statistical power and publication bias. Behavioral Ecology ,
15(6):1044–1045, 2004.
[46] S. Nakagawa and I. Cuthill. Effect size, conﬁdence inte rval
and statistical signiﬁcance: a practical guide for biologi sts.
Biological Reviews , 82(4):591–605, 2007.
[47] A. Ngo-The and G. Ruhe. Optimized Resource Allocation
for Software Release Planning. IEEE Transactions on
Software Engineering (TSE) , 35(1):109–123, 2009.
[48] S. Nijssen and T. Back. An analysis of the behavior of
simpliﬁed evolutionary algorithms on trap functions. IEEE
Transactions on Evolutionary Computation (TEC) ,
7(1):11–22, 2003.
[49] T. Perneger. What’s wrong with Bonferroni adjustments .
British Medical Journal , 316:1236–1238, 1998.
[50] S. Poulding and J. Clark. Efﬁcient Software Veriﬁcatio n:
Statistical Testing Using Automated Search. IEEE
Transactions on Software Engineering (TSE) , 2010.
[51] R Development Core Team. R: A Language and Environment
for Statistical Computing . R Foundation for Statistical
Computing, Vienna, Austria, 2008. ISBN 3-900051-07-0.
[52] J. A. Rice. Mathematical Statistics and Data Analysis .
Duxbury Press, 2 edition, 1994.
[53] G. Rudolph. Convergence analysis of canonical genetic
algorithms. IEEE transactions on Neural Networks ,
5(1):96–101, 1994.
[54] G. Ruxton. The unequal variance t-test is an underused
alternative to Student’s t-test and the Mann-Whitney U test .
Behavioral Ecology , 17(4):688–690, 2006.
[55] S. Sawilowsky and R. Blair. A more realistic look at the
robustness and type II error properties of the t test to
departures from population normality. Psychological
Bulletin , 111(2):352–360, 1992.
[56] M. Shevertalov, J. Kothari, E. Stehle, and S. Mancoridi s. On
the Use of Discretized Source Code Metrics for Author
Identiﬁcation. In International Symposium on Search Based
Software Engineering (SSBSE) , pages 69–78, 2009.
[57] T. Thum, D. Batory, and C. Kastner. Reasoning about edit s to
feature models. In ACM/IEEE International Conference on
Software Engineering (ICSE) , pages 254–264, 2009.
[58] A. Vargha and H. D. Delaney. A critique and improvement o f
the CL common language effect size statistics of McGraw
and Wong. Journal of Educational and Behavioral Statistics ,
25(2):101–132, 2000.
[59] W. Weimer, T. Nguyen, C. L. Goues, and S. Forrest.
Automatically ﬁnding patches using genetic programming.
InACM/IEEE International Conference on Software
Engineering (ICSE) , pages 364–374, 2009.
[60] R. Wilcox. Fundamentals of modern statistical methods:
Substantially improving power and accuracy . Springer
Verlag, 2001.10