Symbolic Search-Based Testing
Arthur Baars
Universidad Polit ´ecnica de Valencia
Valencia, Spain
abaars@pros.upv.esMark Harman
University College London
CREST Centre, London, U.K.
mark.harman@ucl.ac.ukYoussef Hassoun
King’s College London
London, U.K.
youssef.hassoun@kcl.ac.ukKiran Lakhotia
University College London
CREST Centre, London, U.K.
k.lakhotia@ucl.ac.uk
Phil McMinn
University of Shefﬁeld
Shefﬁeld, U.K.
p.mcminn@shefﬁeld.ac.ukPaolo Tonella
Fondazione Bruno Kessler
Trento, Italy
tonella@fbk.euTanja Vos
Universidad Polit ´ecnica de Valencia
Valencia, Spain
tvos@dsic.upv.es
Abstract —We present an algorithm for constructing ﬁtness
functions that improve the efﬁciency of search-based testing when
trying to generate branch adequate test data. The algorithm
combines symbolic information with dynamic analysis and has
two key advantages: It does not require any change in the
underlying test data generation technique and it avoids many
problems traditionally associated with symbolic execution, in
particular the presence of loops. We have evaluated the algorithm
on industrial closed source and open source systems using both
local and global search-based testing techniques, demonstrating
that both are statistically signiﬁcantly more efﬁcient using our
approach. The test for signiﬁcance was done using a one-
sided, paired Wilcoxon signed rank test. On average, the local
search requires 23.41% and the global search 7.78% fewer
ﬁtness evaluations when using a symbolic execution based ﬁtness
function generated by the algorithm.
Index Terms —Search–Based Testing, Symbolic Execution, Fit-
ness Functions
I. I NTRODUCTION
Automation is essential in software testing because the
process is very slow and consequently expensive if undertaken
manually. This need to automate software testing has provided
a rich set of challenging problems for the research community
for over thirty years. One approach to software test automation
that has achieved a great deal of recent attention is Search-
Based Software Testing (SBST). SBST uses meta-heuristic
algorithms to automate the generation of test inputs that meet
a test adequacy criterion. One of the most widely-studied test
adequacy criteria in SBST is branch coverage ([1], [2], [3], [4],
[5], [6], [7]), the adequacy criterion considered in this paper.
Despite the large body of work in SBST focusing on branch
coverage, the state of the art ﬁtness function deﬁnitions used
for branch adequate testing have changed little since the early
seminal work on the Daimler Automated Software Testing
System, which has been in use for more than a decade [7].
Though there have been many developments in SBST, these
focus on changing the search algorithms and the way in which
they are used, rather than the underlying ﬁtness functions on
which all metaheuristic search relies.
This paper takes a different approach and proposes to use
static analysis. In particular, we use a form of partial symbolicexecution to statically collect information available at compile
time that can be used to deﬁne richer and more expressive
ﬁtness functions. We do not perform a complete symbolic
execution, as this would be computationally expensive. Rather,
we compute smaller amounts of symbolic information that
can be used to imbue a ﬁtness function with a much ﬁner
characterisation of the true search landscape, which deﬁnes
the location of the global optima that represent the coverage
of individual branches.
Our aim in attacking the underlying ﬁtness function is to
provide an approach that makes SBST more efﬁcient (and
possibly more effective), regardless of the particular search
algorithm used to generate the test data. We present results for
two widely used approaches to demonstrate, empirically, that
our approach does indeed make SBST more efﬁcient. There
are many search algorithms that we could have chosen to study
in our experimental work. A recent survey of Search-Based
Approaches in Software Engineering [8] listed 15 search-based
algorithms that have been used in SBST work. Clearly, it is
not possible to report results for all of them in this paper.
Rather than make an arbitrary choice of algorithms to study,
we choose to empirically study a local search (the widely
used Alternating Variable Method (A VM) of Korel [3]) and
a global search (the Genetic Algorithm approach used by
Wegener et al. [7] and widely followed by other subsequent
SBST research). Our reason for this choice was that these
two approaches characterise the two possible outcomes for
the primary choice of which algorithm to use; whether it
will be local or global. Most of the other algorithms used in
SBST are formed by a combination of local and global search.
Our results indicate that the partial symbolic information we
compute can indeed improve the efﬁciency of SBST, for both
real world production code and for open source. In the case
of a local search, the information also leads to improved
effectiveness of SBST.
The primary contributions of this paper are:
•We introduce an algorithm for improving SBST by en-
riching ﬁtness functions with statically collected symbolic
information. Because our approach targets the ﬁtness
function itself, it applies to any and every SBST techniqueand can be incorporated without change to the search-
based algorithm that uses the enriched ﬁtness function.
•We introduce an approach to overcome the problem of
loops in traditional symbolic execution that allows us
to approximate the impact of symbolic information on
ﬁtness. We introduce a new metric called the approx-
imation level1to account for uncertainty whenever we
cannot compute precise symbolic information, such as in
the presence of loops.
•We present the results of an empirical study on both open
and closed source code, the results of which indicate
that our enriched ﬁtness functions are signiﬁcantly more
efﬁcient than their traditional counterparts.
The rest of the paper is organised as follows: The next sec-
tion provides an overview of the standard ﬁtness function used
in SBST for branch coverage. Section III introduces our ﬁtness
function enhancement approach, while Section IV introduces
the code analysis algorithm based on symbolic execution. Sec-
tion V presents the empirical study, with corresponding threats
to validity discussed in Section VI. Section VII describes
related work and Section VIII draws conclusions.
II. B ACKGROUND
Meta-heuristic algorithms rely on a ﬁtness function to guide
the search towards a global optimum, i.e., the desired test
data. For branch coverage, the state of the art ﬁtness function
comprises two measures: A branch distance and an approach
level . When both these measures are 0the desired test data
has been found. The approach level records how many of a
target branch’s control dependent nodes were not executed by a
particular input. The fewer control dependent nodes executed,
the ‘further away’ the input is from executing the target in
control ﬂow terms. Consider the example from Figure 1 and
assume the target is the true branch of node (3). If an input
takes the false branch at node (1) then the approach level is 2,
and if an input takes the false branch at node (2), the approach
level is 1and so forth.
Whenever an input misses the target branch, the branch
distance measure is used to compute how close the input was
to staying on a path leading to the target. It is computed
using the condition of the last (Control Flow Graph) node
in an input’s execution trace which holds a transitive control
dependence on the target, and where execution diverged from
the target. Resuming the example from Figure 1, if an input
takes the false branch at node (1), the branch distance is
computed through |x−0|+K, where Kis a failure constant
(K=1 throughout this paper). Different branch distance
formulae exist depending on the relational predicate types used
within the condition of branching nodes on which the target
is control dependent. The interested reader is referred to the
work of Tracey et al. [9] for a complete list of branch distance
functions.
1Note that the deﬁnition of approximation level in this paper is not to be
confused with the approximation level deﬁned in [7], which is equivalent to
theapproach level metric described in Section II.void foo(int x, int y, int z) {
1 if( x ==0)
2 if( y ==z)
3 if( x ==z){
4 //TARGET
}
}
Fig. 1. Example C code used to demonstrate the standard ﬁtness function
used in Search-Based Testing.
For a target branch t, an input vector v, and a node n
where execution diverged from t, the complete ﬁtness value is
then computed by combining the branch distance and approach
level:
ﬀn(t, v)=
approach level (t, v)+norm (branch distance (t, v))
Note that the branch distance measure is normalized to a
value between [0,1], using either of the following normaliza-
tion functions [10]:
norm (d)=/braceleftbigg1−1.001−dor,
d
(d+1)(used in this paper)
This ﬁtness function can be inefﬁcient when multiple, in-
terdependent conditions need to be satisﬁed as in the example
from Figure 1. For instance, when trying to cover the true
branch at node (3), the values chosen for the inputs x,yand
zthat satisfy the ﬁrst two conditions are unlikely to traverse
the true branch at node (3). This is because the probability of
the search optimizing both yandzto0is low.
In general, optimizing each condition in ‘isolation’, as is
the case with the standard ﬁtness function, can be considered
sub-optimal. Symbolic execution on the other hand is able
to capture such constraints and interdependencies between
variables in the form of a path condition . For example, the
path condition describing the execution where all conditions
evaluate to true in Figure 1 would be /an}bracketle{tx=0∧y=z∧x=z/an}bracketri}ht,
where x, yandzdenote symbolic variables corresponding to
the three integer inputs x,y,z . For the purpose of testing,
a path condition can be fed to a constraint solver to obtain
concrete input values which can be used to execute the
program.
However, it is well known that static symbolic execution of a
program faces several challenges, arising from loops and code
constructs that cannot easily be symbolically executed such
as unknown (library) functions, complex pointer arithmetic
and functions pointers to name but a few. Loops in particular
are a common problem, because they can result in inﬁnitely
many program paths and further, when trying to cover a target
branch, it may not be possible to determine the number of loop
iterations necessary to reach the target a priori.The ﬁeld of Dynamic Symbolic Execution (DSE), also
known as concolic testing, ﬁrst introduced by Godefroid et
al.[11] tries to overcome some of the challenges faced
by static symbolic execution. In DSE, information obtained
through dynamic analysis is used to aid symbolic execution.
The work presented in this paper proposes to do the opposite,
i.e., use information gathered through symbolic execution to
aid SBST.
III. S YMBOLICALLY ENHANCED FITNESS FUNCTION
The hypothesis underlying the research work presented in
this paper assumes that incorporating information obtained
from symbolic execution into the ﬁtness function for branch
coverage reduces the number of ﬁtness evaluations required to
cover a branch. We call our approach ﬁtness function enhance-
ment because the information collected along a program path
using symbolic execution is used in place of the traditional
approach level and branch distance measures.
Before the formal presentation of the ﬁtness function en-
hancement algorithm we provide some initial intuition. We
propose to replace the branch distance measure introduced in
Section II with a path distance measure. Assume an input
follows the false branch at node (1) in Figure 1 and that our
target is the true branch of node (3). We start by computing a
path expression [12] representing all paths from node (1) to the
target. Let this path expression be abc(the edge labels a, b, c
refer to the sub-graph shown on the right in Figure 1). We
then symbolically execute this path expression to obtain a set
of (partial) path conditions. In our example this set denotes a
singleton of the form {/an}bracketle{tx=0∧y=z∧x=z/an}bracketri}ht}because there
is only one path from node (1) to the true branch of node (3).
Next we apply the branch distance measure from Section II
to each of the atomic conditions in the path condition (i.e.,
x=0,y=z,x=z), and sum the results to form a path
distance. In case we have more than one path distance, we
choose the minimum for our ﬁtness computation. The intuition
behind this choice is that the path condition with the smallest
path distance is the closest to being satisﬁed by an input.
It may not always be possible to symbolically execute a
path expression due to sources of uncertainty. To account for
this we introduce a second measure called the approximation
level . The approximation level will be deﬁned as the number
of conditions that cannot be added to a path condition, and
are thus not considered in the path distance. For example, a
condition that uses variables whose deﬁnition originates from
a statement inside a loop will be dropped. This is because in
general we do not know how often a loop is executed, thus
we also do not know the ﬁnal value of the variables that are
deﬁned in a loop. Other sources of uncertainty can include
variables deﬁned through system calls to which we do not
have access.
The next section will provide formal deﬁnitions of the ap-
proximation level andpath distance , along with the algorithm
for computing the enhanced ﬁtness function ( eﬀ).void foo(int n, int a, int b) {
1 int s =0;
2 if (n >0){
3 for (int i =0;i <n ; i++)
4 s += i;
5 if (a ==n){
6 if (s ==10){
7 if (b ==s){
8 // TARGET (t)
}
}
}
}
}
Fig. 2. Illustrative C code involving a loop with nested if-statements, used
to demonstrate the symbolically enhanced ﬁtness function.
A. Deﬁnitions
Letpbe the path expression representing all paths between
a start node nand a target node t. This path expression may
contain loops, represented as terms of the form A∗. For such
terms, we may opt for an arbitrary level of unrolling ( e.g.k
times), but we cannot handle unbounded (potentially inﬁnite)
unrollings. As a consequence, when the upper limit for the
number of unrollings is reached, we make the assumption
that variables deﬁned in any successive loop iterations are
destroyed, since in general, we cannot determine the required
number of loop iterations. Such variables will be represented
by the term D[A]. The path expression involving loops ( i.e.,
A∗) can thus be expanded as:
A∗=1+ A+A2+A3+...+AkD[A]
By replacing A∗with A∗=1 + A+A2+A3+...+AkD[A]
in the path expression pwe obtain an approximated path
expression p/primewhich contains some destroy terms of the form
D[A]. In the path condition produced by symbolic execution
ofp/prime, we drop any clauses involving variables whose deﬁnition
is inside a destroyed sub-path. Such destroyed clauses are
counted and their number deﬁnes the approximation level used
in the ﬁtness function. Path conditions built by dropping one
or more conditions are said to be partial ; the others are said
to be complete .
Deﬁnition 1. Branch distance: A quantiﬁcation in the range
[0,1]of a boolean branch condition, such that the value zero
is obtained iff the condition evaluates to true. Values close
to1indicate that the condition is far from being satisﬁed.
Intermediate values should be such as to smoothly guide the
search toward satisfying the condition.
Deﬁnition 2. Path distance: A quantiﬁcation of the (partial
or complete) path condition, given by the sum of the branch
distances computed for the conditions appearing as non-
destroyed in the path condition for the approximated path
expression. It is zero when all conjuncts in the path condition
evaluate to true.Whenever we build a partial path condition, we are dropping
a number of conditions which involve data dependencies
originating in a loop. The number of dropped conditions is
the approximation level.
Deﬁnition 3. Approximation level: The approximation level
along a path is the number of conditions that are dropped
from the path condition since they involve variables deﬁned
inside loops that are used in the condition.
An approximated path expression p/primecan always be normal-
ized into a sum of alternative paths. In fact, in p/prime, loops A∗
are replaced by alternative k-bounded loop iterations, hence p/prime
contains only sequence (multiplication) and alternative (sum)
operators, which can be normalized into a sum of products by
resorting to distribution of multiplication over sum.
Deﬁnition 4. Fitness function: Let the normalized approxi-
mated path expression p/primehave the form p1+p2+...+ph,
the ﬁtness function eﬀfor a node nis deﬁned as:
eﬀn=m i n {ﬀ1,ﬀ2,..., ﬀh}
where ﬀ1,ﬀ2,..., ﬀhare the ﬁtness functions for the atomic
paths in p/prime, each being computed as the sum of approximation
level and path distance:
ﬀi=approximation level +path distance (pi)
Consider the example code in Figure 2 and assume our
target is the true branch at node (7). If an input traverses the
false branch at node (2), the path expression representing all
paths from node (2) to the target is a(bc)∗defg . We may
distinguish paths not entering the loop, (bc)∗, from paths
which enter it one or more times ( i.e.,k=1). The ﬁrst case
(not entering the loop) is described by the path expression
adefg . Symbolically executing this path expression yields the
following path condition: /an}bracketle{tn> 0∧0≥n∧a=n∧s=
10∧b=s/an}bracketri}ht. The path described by this path condition is
clearly infeasible, because the conditions n> 0and0≥n
are mutually exclusive. Hence, we will not consider it any
further.
The second case (entering the loop one or more times) can
be described by the path expression abcD [bc]defg . The term
D[bc]indicates that all variable deﬁnitions occurring along the
path bcare to be treated as unknown , because the number of
iterations for the loop bcis unknown (it will be one or more).
For the example in Figure 2, two variables are deﬁned inside
the loop bc;sandi. Since we do not know how often the
loop will be executed, we also do not know the ﬁnal values
ofsandiwhen we exit the loop. Therefore we drop any
conditions obtained by symbolically executing the sub-path
following the term D[bc]that involve such variables, i.e., we
do not add those conditions to the path condition.
The approximation level accounts for this by being in-
cremented for each condition that is dropped. Completing
our example, symbolically executing the path expression
abcD [bc]defg yields the path condition: /an}bracketle{tn>0∧0<n∧a=
n/an}bracketri}ht. Since we dropped three conditions ( 1≥n, s= 10 ,b=s)the approximation level is 3. Thus, the approximation level
allows us to distinguish an input reaching node (2) and taking
the false branch, from an input taking the true branch at
node (2) and thus getting closer to the target. Note that the
approximation level reaches 0once we reach node (5).
IV. A LGORITHM TO COMPUTE SYMBOLICALLY
ENHANCED FITNESS FUNCTIONS
Algorithm 1 Compute symbolically enhanced ﬁtness func-
tions
Input CFG : Control ﬂow graph of the program under test; t: Target
edge to be covered
Output eﬀn: Fitness function to be used by each test case reaching
node n, for each CFG node nholding a transitive control
dependency on t
1:for each CFG node n∈N|nholds a transitive control
dependency on tdo
2: Compute the sub-graph subCFG nofCFG from ntot,i.e.,
the intersection between nodes/edges forward reachable from
nand nodes/edges backward reachable from t
3: Apply the node reduction algorithm [12] to determine the path
expression pforsubCFG n
4: Compute the approximated path expression p/primefrom pby
approximating loops A∗in the path expression pasA∗≈
1+A+A2+A3+...+AkD[A], for some ﬁxed value of k
5: Normalize the approximated path expression p/primeinto a sum of
products: p/prime=p1+p2+...+ph
6: for each path piin the normalized path expression p/primedo
7: Perform a symbolic execution along pi, keeping track of
destroyed variables and annotating destroyed conditions as
D[c]; the result is path condition pci
8: Turn the path condition pciinto a ﬁtness function ﬀiby
replacing conditions with branch distances and destroyed
conditions with 1
9: end for
10: Deﬁne the ﬁtness function eﬀnfor node nas:eﬀn=
min{ﬀ1,ﬀ2,..., ﬀh}
11:end for
Algorithm 1 shows the pseudo-code for the computation
of the enhanced ﬁtness functions introduced in the previous
section. Input to the algorithm is a program, represented as its
Control Flow Graph (CFG), and a CFG edge tthat represents
the current test target, i.e., the branch to be covered. The
output produced by the algorithm is a set of symbolically
enhanced ﬁtness functions, one for each CFG node nthat
holds a transitive control dependency on t. For each such node
that is part of the execution trace of an input, the corresponding
ﬁtness function is evaluated, with the minimum value forming
the overall ﬁtness for that input.
For all nodes nthat hold a transitive control dependency on
the target branch, Algorithm 1 determines the path expression
prepresenting all paths from nto the target t(steps 2-3).
Then, loops are approximated (typically as A∗≈1+AD[A])
and an approximated path expression p/primeis computed and
normalized into a sum of products (steps 4-5). For each nor-
malized approximated path expression picomposing the path
p/prime, symbolic execution is used to compute the corresponding
path condition pci(step 7). Whenever a destroyed sub-pathis encountered during the symbolic execution, all variables
deﬁned inside the sub-path are collected among the destroyed
variables. Successively added conditions which make use of
destroyed variables are marked as destroyed conditions. In step
8, the path condition pciis converted into a ﬁtness function
forpiby replacing conditions with branch distances, except
for destroyed conditions, which increase the approximation
level by one. The ﬁnal ﬁtness function for node nis the
minimum among the ﬁtness function values computed along
the alternative paths appearing in the normalized approximated
path expression.
It is important to note that we cannot use a constraint solver
to provide a set of input values that satisfy a path condition
pci. This is because the path expression pinot always captures
all execution paths from the entry node of a CFG to a target
edge t. It is computed using only a sub-graph of the entire
CFG (see Step 2 in Algorithm 1), i.e.the graph representing
all execution paths between a critical branching node and t.
Consequently, pcimay contain local variables, rendering the
use of a constraint solver infeasible.
V. E MPIRICAL STUDY
The aim of the empirical study in this paper is to analyse
the impact of using the enhanced ﬁtness function in SBST.
The two research questions to be addressed by the study are
as follows:
Research Question 1 - Effect of eﬀon branch coverage .
The level of branch coverage achieved, i.e., effectiveness of the
testing technique, is often the main focus when investigating
an automated test data generation approach. Our proposed
change in ﬁtness function should not negatively affect the level
of coverage achieved by a test data generation technique. Does
this hypothesis hold?
Research Question 2 - Effect of eﬀon efﬁciency . Alongside
coverage, efﬁciency is also an important factor of any testing
technique. Does the enhanced ﬁtness function make SBST
more efﬁcient, and if so, what is the performance increase?
We selected two commonly used search algorithms for
evaluation; a form of hill climbing known as the Alternating
Variable Method (A VM), ﬁrst introduced by Korel [3], and
a Genetic Algorithm (GA) based on the approach described
by Wegener et al. [7]. Details of the two algorithms can
be found in Section V-A and Section V-B respectively. The
search-based testing framework, IGUANA [13], was extended
to include the enhanced ﬁtness function proposed in this paper
and subsequently used to perform the test data searches.
The study was performed on 338 branches, drawn from
ﬁve different C programs2, two of which were provided by
Daimler, two are open source and one is the well-studied
triangle program. The input domain for each function is
composed of global variables and formal parameters. We chose
2Programs were chosen arbitrarily. However, all branches in the empirical
study have been used to evaluate search-based testing techniques in the
past [1], [2], [14]. Thus, we considered them good candidates for evaluating
a new ﬁtness function for SBSTnot to use any input domain reduction and deﬁned the domain
of each variable according to its declared type. Details of the
subjects used in the empirical study can be found in Table I.
The programs f2and defroster are industrial case
studies provided by Daimler and represent production code
for engine and rear window defroster control systems. The
code is machine generated from a design model of the desired
behaviour. To complement the industrial examples, two open-
source case studies were selected. tiff-3.8.2 is a library
for manipulating images in the Tag Image File Format (TIFF).
The functions tested comprise routines for placing images on
pages and for building ‘overview’ compressed sample images.
Finally, triangle is the well-known triangle classiﬁcation pro-
gram, often used as a benchmark program in automated test
data generation studies.
Each search for test data was performed 30times for
every combination of ﬁtness function and search algorithm.
If test data was not found to cover a branch after 100,000
ﬁtness evaluations, the search was terminated. Serendipitous
coverage, i.e., branches covered by accident during the test
data generation process, was ignored, so that a distinct search
was carried out for every branch. The success or failure of
each search was recorded, along with the number of ﬁtness
evaluations required to ﬁnd the test data. From this, the ‘suc-
cess rate’ of each branch can be calculated – the percentage
of the 30runs in which test data to execute the branch was
found. The 30runs were performed using an identical list of
ﬁxed seeds for random number generation, so as to provide
a basis for assessment with tests for statistical signiﬁcance
using a one-sided, paired Wilcoxon signed rank test. Such tests
are necessary to provide robust results in the presence of the
inherently stochastic behaviour of the search algorithms.
To facilitate replication, we will now discuss the conﬁgura-
tion of the two search algorithms used in the study.
A. Alternating Variable Method Setup
The A VM is a simple but effective optimization tech-
nique [2]. It is a form of hill climbing and works by continu-
ously changing an input parameter to a function in isolation.
Initially all (arithmetic type) inputs are initialized with
random values. Then, so called exploratory moves are made
for each input in turn. These consist of adding or subtracting
a delta from the value of an input. For integral types the delta
starts off at 1,i.e., the smallest increment (decrement).
When a change leads to an improved ﬁtness value, the
search tries to accelerate towards an optimum by increasing
the size of the neighbourhood move with every step. These
are known as pattern moves . The formula used to calculate
the delta added or subtracted from an input is: δ=2it·dir·
10−prec i, where itis the repeat iteration of the current move
(for pattern moves), direither −1or1, and prec ithe precision
of the ithinput variable. The precision applies to ﬂoating point
variables only ( i.e., it is 0for integral types). It denotes a scale
factor for the size of a neighbourhood move. For example,
setting the precision ( prec i) of an input to 1limits the smallest
possible move to ±0.1. Increasing the precision to 2limitsTABLE I
DETAILS OF THE TEST SUBJECTS .THELINES OF CODE COLUMN CONTAINS THE ansic OUTPUT OF THE SLOCCOUNT TOOL [15] USED IN ITS DEFAULT
SETTING AND APPLIED TO THE ROOT SOURCE DIRECTORY OF EACH PROG RAM .
Test Lines of Number of Number of Approximate Domain
Subject / Function Code Branches Loops Size
bibclean 10,252
check ISBN 54 1 2112
check ISSN 54 1 2112
defroster 179
Defroster main 72 0 2137
f2 305
F2 46 0 2272
tiff-3.8.2 47,794
TIFF GetSourceSamples 32 2 2135
TIFF SetSample 28 0 21102
PlaceImage 24 0 28402
triangle 53
triangle 28 0 296
Total 58,583 338 4
the smallest possible move to ±0.01, and so forth. For all
experiments carried out in this paper, the precision for ﬂoating
point variables was ﬁxed at 3.
Once no further improvements can be found for an input,
the search continues optimizing the next input parameter, and
may recommence with the ﬁrst input if necessary. In case
the search stagnates, i.e., no move leads to an improvement,
the search restarts at another randomly chosen location in the
search-space. This is known as a random restart strategy and
is designed to overcome local optima and enable the A VM to
explore a wider region of the input domain for the function
under test.
B. Genetic Algorithm Setup
A GA is a global search algorithm ﬁrst proposed by Holland
in the 1970s [16]. The conﬁguration of the GA used in this
paper is based on the approach described by Wegener et al. [7]
who used GEATbx by Hartmut Pohlheim [17].
An overall population of 300individuals is divided into six
competing sub-populations, which begin with 50individuals
each. Each sub-population evolves separately using selection,
recombination, mutation and re-insertion strategies. After eval-
uation, individuals in each sub-population are sorted using
a linear ranking method [18] with a selection pressure of
1.7. Then, individuals are selected for reproduction through
Stochastic Universal Sampling (SUS) [19]. In SUS, the prob-
ability of an individual being selected is proportionate to its
(rank-based) ﬁtness value. Selected individuals are recombined
using a discrete recombination strategy [20], whereby an
offspring receives each gene from either parent with an equal
probability.
After recombination, offspring individuals are mutated ac-
cording to the breeder genetic algorithm mutation strat-
egy [20]. The mutation operator is applied with probability
1
len, where lenis the number of genes in an individual ( i.e.,
the length of the input vector). For each gene to be mutated,
a mutation range ri=size ·dom iis deﬁned, where dom iis the domain size of the ithinput parameter and size is a
mutation step size. The mutation step size varies for each of
the six sub-populations and is deﬁned as size = 10−popwith
1≤pop≤6. The mutated value of an input parameter can
thus be computed as vi=xi±ri·η. Addition or subtraction
is chosen with an equal probability, and η=/summationtext15
x=0αx·2−x,
where αxis1with a probability of1
16and0otherwise. After
mutation, offspring are reinserted into a sub-population using
an elitist reinsertion strategy. That is, the top 10% of the
current generation is retained and the remaining individuals
are replaced by ﬁtter offspring.
A feature of the Wegener model is that the six sub-
populations of the GA compete with one another for the num-
ber of individuals each sub-population evolves. An average
ﬁtness value is computed for each sub-population and this
value is used to linearly rank the sub-populations (again using
a selection pressure of 1.7). The rank-based ﬁtness value rank
of a sub-population is then used to compute a progress value
prog for the population in generation gusing the formula
prog g+1=0.9·prog g+0.1·rank . Then, after every four
generations, the populations are ranked according to their
progress value prog, and the size of each sub-population is
updated, with weaker sub-populations transferring individuals
to stronger ones. However, no sub-population can lose its
last ﬁve individuals, preventing it from dying out. Finally, a
general migration of individuals takes place after every 20th
generation, where sub-populations randomly exchange 10% of
their individuals with one another.
C. Results
Research Question 1 - Effect of eﬀon branch coverage .
Figure 3 shows the coverage achieved by the A VM and the
GA for each test subject. A branch is counted as covered if
the search for test data succeeded in at least one out of the
thirty runs. As can be seen, using a symbolically enhanced
ﬁtness function does not negatively affect the level of branch
coverage achieved by either local or global search. Instead, theGA is able to cover a branch that it previously failed to cover.
Similarly, the local search is able to cover more branches when
using the enhanced ﬁtness functions.
To gain a better understanding of how the proposed ap-
proach affects each search algorithm, we also computed the
success rate for each search target. Table II lists the branches
for which we observed a difference in success rate when using
the enhanced ﬁtness function. The GA exhibits little variation.
For three branches, the success rate is slightly reduced when
using the enhanced ﬁtness function. However, for ﬁve branches
the success rate increases.
Compared to the GA, the enhanced ﬁtness function has a
bigger impact on the success rate of the A VM. For branches
where we observed a difference, the trend is in an increase in
success rate. Five branches stand out particularly because the
A VM failed to ﬁnd test data for these using the standard ﬁtness
function. With the enhanced ﬁtness function, the search was
able to ﬁnd the required test data in all of the 30runs. The
effect of the enhanced ﬁtness function is not always beneﬁcial
though; for example branches in the function F2from Daimler
are covered with a reduced success rate. This function is
interesting because some ifstatements check if a subtraction
operation (on operands of type short int ) resulted in an
over- or underﬂow. For example, for one branch where the
enhanced ﬁtness function performed worse than the standard
ﬁtness function, the path distance measure is computed using
the path condition /an}bracketle{tV11≥0∧V9<0∧(V11−V9)<0/an}bracketri}ht.
The conjuncts of the path condition correspond to three nested
ifstatements in the original code. When the path distance
is computed, the ﬁrst two conjuncts pull into the opposite
direction of the last conjunct. That is, as the branch distance
for the ﬁrst two conjuncts converges towards 0, the branch
distance for the third conjunct increases until an overﬂow
occurs. The standard ﬁtness function, which optimizes each
of the conjuncts in turn, does not appear to suffer from this
problem and is able to reliably ﬁnd the required test data. All
cases where the enhanced ﬁtness function did worse than the
standard ﬁtness function for F2were in code that checks for
over- or underﬂow errors.
Research Question 2 - Effect of eﬀon efﬁciency . The results
support the hypothesis that enhancing the ﬁtness function with
information gathered from symbolic execution can reduce the
number of ﬁtness evaluations required to cover a branch.
Details of the average number of ﬁtness evaluations required
by each search technique are given in Table III.
The trend for the GA is to require fewer ﬁtness evaluations
with the enhanced ﬁtness function. This difference is partic-
ularly visible for three functions, where we observed more
than a 25% reduction in ﬁtness computations. However, there
is again one case ( PlaceImage from tiff-3.8.2 ) where
we see a small increase in the number of ﬁtness evaluations.
As with the success rates, the A VM beneﬁts more from the
enhanced ﬁtness function than the GA. Four functions require
fewer than 50% of the ﬁtness evaluations compared to the
standard ﬁtness function. This is not surprising since all these
functions contain branches for which the A VM failed to ﬁnd0102030405060708090100Branch Coverage (%)Std. FFEnhanced FFBranch coverage with the Genetic Algorithm
0102030405060708090100Branch Coverage (%)Std. FFEnhanced FFBranch coverage with the Alternating Variable Method
Fig. 3. This Figure shows the branch coverage achieved by the Genetic
Algorithm (top) and the Alternating Variable Method (bottom) when using the
standard and enhanced ﬁtness functions. The graphs conﬁrm that symbolically
enhanced ﬁtness functions are equally or more effective than the standard
ﬁtness functions.
test data using the standard ﬁtness function, but for which
it achieved a 100% success rate using the enhanced ﬁtness
function. Conversely, the A VM uses more ﬁtness evaluations
with the enhanced ﬁtness function for F2, because branches
are covered with a lower success rate, and each failed search
results in 100,000ﬁtness evaluations.
To see if the differences in efﬁciency for the GA and the
A VM are statistically signiﬁcant, we used the statistical tool
R [21] to perform a paired, one-sided Wilcoxon signed rank
test with continuity correction and speciﬁed an alpha level
of0.01. For both the GA and A VM we obtained a pvalue
of2.2×10−16. This pvalue indicates that the difference
in the number of ﬁtness evaluations required by each search
algorithm is statistically signiﬁcant ( p≤α).
Finally, we also recorded the time taken to perform the up-
front static analysis required by the enhanced ﬁtness function.
To obtain a reasonable sample pool we repeated this analy-
sis30times for each function. The average analysis times,
alongside standard deviation are recorded in Table IV.
Loops often result in path explosion, even when only a
single loop unrolling is performed. Thus, the analysis takes
longer for functions containing one or more loops. Note that
the symbolic analysis is performed once per function and can
be re-used by a search algorithm for all branches containedTABLE II
DIFFERENCE IN SUCCESS RATES WITH THE STANDARD FITNESS FUNCTION AND THE ENHANCED FITNESS FUNCTION .BRANCHES ARE ONLY LISTED IF
THERE IS A DIFFERENCE FOR EITHER THE AV M ORGA. A 0% SUCCESS RATE MEANS A SEARCH ALGORITHM WAS UNABLE TO COVER A BR ANCH IN ALL
OF THE 30REPEAT RUNS .A1 0 0 % SUCCESS RATE MEANS A SEARCH WAS ABLE TO FIND THE REQUIRED TEST D ATA IN EACH OF THE 30TRIALS .
Test Subject/ A VM GA
Function (Branch ID) Standard / Enhanced Standard / Enhanced
bibclean
check ISSN (53T) 0% / 0% (0%) 43.33% / 60.00% ( +16.67% )
check ISSN (55T) 0% / 0% (0%) 16.67% / 66.67% ( +50.00% )
check ISSN (58T) 0% / 0% (0%) 100% / 96.67% ( -3.33% )
check ISSN (58F) 0% / 0% (0%) 100% / 76.67% ( -23.33% )
f2
F2 (4T) 100% / 53.33% ( -46.67% )100% / 100% (0%)
F2 (15T) 100% / 46.67% ( -53.33% )100% / 100% (0%)
F2 (35T) 100% / 100% (0%) 100% / 96.67% ( -3.33% )
F2 (43T) 3.33% / 3.33% (0%) 0% / 6.67% ( +6.67% )
tiff-3.8.2
TIFF GetSourceSamples (14T) 6.67% / 100% ( +93.33% )100% / 100% (0%)
TIFF GetSourceSamples (17T) 0% / 100% ( +100% ) 100% / 100% (0%)
TIFF GetSourceSamples (20T) 10.00% / 100% ( +90.00% )100% / 100% (0%)
TIFF GetSourceSamples (23T) 10.00% / 100% ( +90.00% )100% / 100% (0%)
TIFF GetSourceSamples (26T) 13.33% / 100% ( +86.67% )100% / 100% (0%)
TIFF GetSourceSamples (29T) 16.67% / 100% ( +83.33% )100% / 100% (0%)
TIFF GetSourceSamples (32T) 23.33% / 100% ( +76.67% )100% / 100% (0%)
TIFF SetSample (2T) 13.33% / 100% ( +86.67% )100% / 100% (0%)
TIFF SetSample (5T) 13.33% / 100% ( +86.67% )100% / 100% (0%)
TIFF SetSample (8T) 13.33% / 100% ( +86.67% )100% / 100% (0%)
TIFF SetSample (11T) 26.67% / 100% ( +73.33% )100% / 100% (0%)
TIFF SetSample (14T) 13.33% / 100% ( +86.67% )100% / 100% (0%)
TIFF SetSample (17T) 20.00% / 100% ( +80.00% )100% / 100% (0%)
TIFF SetSample (20T) 23.33% / 100% ( +76.67% )100% / 100% (0%)
triangle
triangle (14T) 0% / 100% ( +100% ) 100% / 100% (0%)
triangle (15T) 0% / 93.33% ( +93.33% )100% / 100% (0%)
triangle (15F) 0% / 100% ( +100% ) 96.67% / 100% ( +3.33% )
triangle (17T) 0% / 100% ( +100% ) 100% / 100% (0%)
triangle (17F) 0% / 100% ( +100% ) 96.67% / 100% ( +3.33% )
triangle (19T) 0% / 96.67% ( +96.67% )100% / 100% (0%)
triangle (21T) 0% / 96.67% ( +96.67% )100% / 100% (0%)
TABLE III
NORMALIZED AVERAGE FITNESS EVALUATIONS REQUIRED BY THE GA AND AV M USING THE STANDARD AND ENHANCED FITNESS FUNCTIONS .
Test Subject/ A VM GA
Function Standard / Enhanced Standard / Enhanced
bibclean
check ISBN 100% / 99.99% ( -0.01% ) 100% / 99.99% ( -0.01% )
check ISSN 100% / 99.98% ( -0.02% ) 100% / 93.71% ( -6.29% )
defroster
Defroster main 100% / 99.57% ( -0.43% ) 100% / 68.15% ( -31.85% )
f2
F2 100% / 157.44% ( +57.44% )100% / 91.61% ( -8.39% )
tiff-3.8.2
TIFF GetSourceSamples 100% / 12.41% ( -87.59% )100% / 73.05% ( -26.95% )
TIFF SetSample 100% / 8.78% ( -91.22% )100% / 73.87% ( -26.13% )
PlaceImage 100% / 99.17% ( -0.83% ) 100% / 100.53% ( +0.53% )
triangle
triangle 100% / 35.35% ( -64.65% )100% / 98.79% ( -1.21% )TABLE IV
AVERAGE TIME (IN MILLISECONDS )TAKEN OVER 30TRIALS TO PERFORM
THE UP -FRONT STATIC ANALYSIS REQUIRED BY THE ENHANCED FITNESS
FUNCTION .THE STANDARD DEVIATION IS SHOWN IN THE RIGHT MOST
COLUMN .
Test Subject/ Analysis Time(ms) (StdDev)
Function
bibclean
check ISBN 1,909,741.13 (2,172.47)
check ISSN 1,792,913.50 (2,827.38)
defroster
Defroster main 34,509.87 (345.55)
f2
F2 318,279.23 (431.68)
tiff-3.8.2
TIFF GetSourceSamples 516,514.23 (878.52)
TIFF SetSample 1,092.77 (12.22)
PlaceImage 956.43 (105.36)
triangle
triangle 716.03 (6.24)
within that function. Therefore, compared to the overall exe-
cution time of the test data generation algorithms, we consider
the analysis times reported in this paper as acceptable. Future
work might investigate how we can make the static analysis
more efﬁcient, for example by re-using symbolic information
for nested branches.
VI. T HREATS TO VALIDITY
Naturally there are threats to validity in any empirical study
such as this. The ﬁrst issue to address is the threat to the
internal validity of the experiments, i.e., whether there has
been a bias in the experimental design that could affect the
obtained results.
One potential source of bias comes from the conﬁguration of
the algorithms used in the test data generation tool IGUANA.
The settings for the GA and A VM were taken from previous
studies [1], [22], [14] that looked at generating branch ade-
quate test data. Thus, they have been shown in the past to
provide a good trade-off between effectiveness and efﬁciency.
Another potential source of bias comes from the inherent
stochastic behaviour of the meta-heuristic search algorithms.
The most reliable (and widely used) technique for overcom-
ing this source of variability is to perform statistical tests
using a sufﬁciently large sample of result data. In order to
ensure a large sample size, experiments were repeated 30
times, providing a reasonable pool of data from which to
draw observations, and ensuring sample means were normally
distributed. To show that the enhanced ﬁtness function is more
efﬁcient than the standard ﬁtness function used in SBST, a test
for a statistical signiﬁcant difference in the sample means was
performed. We used a one-sided, paired Wilcoxon signed rank
test with the conﬁdence level set at 99%.
A further source of bias includes the selection of the
functions used in the empirical study, which could potentially
affect its external validity, i.e., the extent to which it is possible
to generalize from the results obtained. The study draws
upon code from real world programs, both from industrialproduction code and from open source. While we sampled a
variety of programming styles and sources, we only considered
functions from ﬁve programs. Therefore caution is required
before making any claims as to whether these results would
be observed on other functions. Instead, the results reported
herein should only be seen to provide some initial intuition and
a larger study is required to validate or refute our ﬁndings.
VII. R ELATED WORK
The present paper is the ﬁrst to develop an amended form
of symbolic execution for SBST. Previous work on developing
symbolic execution as a practical means of improving auto-
mated testing focussed on constraint based testing techniques,
leading to the development of the very active ﬁeld now known
as ‘Dynamic Symbolic Execution’ (DSE). This ﬁeld began
with the seminal work by Godefroid et al. [11] on Directed
Automated Random Testing (DART), which combined sym-
bolic execution with random testing. Since then a number
of authors have followed this approach, which is sometimes
referred to as ‘concolic testing’ [23] as well as DSE [24], [11],
[25].
DSE and SBST have developed as separate schools of
thought in automated software testing, each with their own
advantages and disadvantages. The introduction of Dynamic
Symbolic Execution creates a signiﬁcant step forward in
the development of previous constraint based approaches to
automated test data generation, on which DSE builds.
Our introduction of partial symbolic execution as a means of
augmenting SBST seeks to provide a similar impetus to SBST
research. Like DSE, we augment an existing test automation
technique with a form of symbolic execution and like DSE,
we need to amend traditional symbolic execution to ameliorate
its problems. However, DSE performs a complete symbolic
execution, sometimes using concrete values in place of sym-
bolic values, whereas our approach does not use concrete
values, but retains the symbolic nature of symbolic execution.
Rather than performing a complete symbolic execution, we
perform a localised or ‘partial’ symbolic computation and use
approximation to overcome the problems of static symbolic
execution.
The ﬁrst authors to propose a combination of SBST and
DSE were Inkumsah and Xie [26] with the EV ACON frame-
work. Their framework targets test data generation for object
oriented code written in JA V A and uses two existing tools:
eToc [27], an evolutionary test data generation tool, and
jCUTE [28], a DSE tool. Method sequences putting the class
containing the method under test into speciﬁc states, are
constructed by eToc. Then, jCUTE is used to maximize code
coverage of a given method sequence by generating values for
the sequences’ input parameters. The method sequences with
optimized parameter values are then passed back to eToc for
further optimization.
More recently, Lakhotia et al. [29] investigated a combi-
nation of SBST and DSE in order to improve DSE’s ability
to handle constraints over ﬂoating point variables. Their workintegrated the A VM, also used in this paper, and Evolution
Strategies into Pex [25], a DSE tool for .NET.
Lakhotia et al. [30] also proposed a combination of sym-
bolic execution with search in order to improve SBST. Inspired
by the work on CUTE [23], they use symbolic execution to
extend and improve the A VM for pointer inputs.
The work presented in this paper differs from all previous
work in that it is the ﬁrst to consider symbolic execution
in order to improve a ﬁtness function used in SBST. A
beneﬁt of this approach lies in its generality; it may be used
with any search algorithm. Furthermore, the enhanced ﬁtness
function does not require a constraint solver, despite making
use of symbolic execution techniques. The path condition
generated through symbolic execution is transformed into a
ﬁtness function to guide an optimisation algorithm. This is
an advantage when testing code that contains ﬂoating point
computations or calls to system libraries.
VIII. C ONCLUSION
This paper has introduced and evaluated a symbolic search-
based software testing approach for the branch coverage test
adequacy criterion. We propose to replace the existing branch
distance andapproach level measures with two new measures:
Path distance andapproximation level . The new metrics make
use of information gathered from symbolic execution. An
empirical study, performed on 338 branches, taken from a
mix of open source and industrial programs, conﬁrmed our
hypothesis that a symbolically enhanced ﬁtness function can
make search algorithms more efﬁcient. The proposed approach
was evaluated with two commonly used algorithms in Search-
Based Software Testing: The Alternating Variable Method and
a Genetic Algorithm.
The main goal of the enhanced ﬁtness function is to make
search-based testing more efﬁcient. However, it also enables
the Alternating Variable Method, a form of hill climbing,
to cover branches for which the search failed using the
traditional ﬁtness function. Future work will investigate how
symbolic search-based testing can be further developed to not
only improve efﬁciency, but also effectiveness of a search
algorithm.
ACKNOWLEDGEMENT
Arthur Baars, Kiran Lakhotia, Paolo Tonella and Tanja Vos
are funded through the European Union project FITTEST (ICT-
2009.1.2 no 257574). Mark Harman is supported by EPSRC Grants
EP/G060525/1, EP/D050863, GR/S93684 & GR/T22872 and also by
the kind support of Daimler Berlin, BMS and Vizuri Ltd., London.
Phil McMinn is supported in part by EPSRC grants EP/G009600/1,
EP/F065825/1 and EP/I010386/1.
REFERENCES
[1] M. Harman, Y. Hassoun, K. Lakhotia, P. McMinn, and J. Wegener,
“The Impact of Input Domain Reduction on Search-Based Test Data
Generation,” in ESEC/SIGSOFT FSE , 2007, pp. 93–101.
[2] M. Harman and P. McMinn, “A theoretical and empirical study of search-
based testing: Local, global, and hybrid search,” IEEE TSE , vol. 36,
no. 2, pp. 226–247, 2010.
[3] B. Korel, “Automated software test data generation,” IEEE TSE , vol. 16,
no. 8, pp. 870–879, Aug. 1990.[4] K. Lakhotia, P. McMinn, and M. Harman, “An empirical investigation
into branch coverage for C programs using CUTE and AUSTIN,” The
J. of Systems and Software , vol. 83, no. 12, pp. 2379–2391, Dec. 2010.
[5] C. C. Michael, G. McGraw, and M. Schatz, “Generating software test
data by evolution,” IEEE TSE , vol. 27, no. 12, pp. 1085–1110, 2001.
[6] R. P. Pargas, M. J. Harrold, and R. Peck, “Test-data generation using
genetic algorithms,” Soft. Test., Ver. and Rel. , vol. 9, no. 4, pp. 263–282,
1999.
[7] J. Wegener, A. Baresel, and H. Sthamer, “Evolutionary test environment
for automatic structural testing,” Information & Software Technology ,
vol. 43, no. 14, pp. 841–854, 2001.
[8] M. Harman, A. Mansouri, and Y. Zhang, “Search based software
engineering: A comprehensive analysis and review of trends techniques
and applications,” Department of Computer Science, King’s College
London, Tech. Rep., April 2009.
[9] N. Tracey, J. A. Clark, K. Mander, and J. A. McDermid, “An automated
framework for structural test-data generation,” in ASE, 1998, pp. 285–
288.
[10] A. Arcuri, “It does matter how you normalise the branch distance in
search based software testing,” in ICST , 2010, pp. 205–214.
[11] P. Godefroid, N. Klarlund, and K. Sen, “DART: directed automated
random testing,” ACM SIGPLAN Notices , vol. 40, no. 6, pp. 213–223,
Jun. 2005.
[12] B. Beizer, Software Testing Techniques, 2nd edition . International
Thomson Computer Press, 1990.
[13] P. McMinn, “IGUANA: Input generation using automated novel algo-
rithms. A plug and play research tool.” Univ. Of Shefﬁeld, Tech. Rep.,
2007.
[14] P. McMinn, M. Harman, Y. Hassoun, K. Lakhotia, and J. Wegener,
“Input Domain Reduction through Irrelevant Variable Removal and its
Effect on Local, Global and Hybrid Search-Based Structural Test Data
Generation,” IEEE TSE , To Appear (2011).
[15] D. A. Wheeler, “More than a gigabuck: Estimating GNU/Linux’s size,”
http://www.dwheeler.com/sloc/, Jun. 2001.
[16] J. H. Holland, “Genetic algorithms and the optimal allocation of trials,”
SIAM J. of Computing , vol. 2, no. 2, pp. 88–105, Jun. 1973.
[17] H. Pohlheim, “Evolutionary algorithms: Overview, methods and oper-
ators.” documentation for: Genetic evolutionary algorithm toolbox for
use with matlab version: toolbox 1.92 documentation 1.92,” 1999.
[18] D. Whitley, “The GENITOR algorithm and selection pressure: Why
rank-based allocation of reproductive trials is best,” Computer Science
Dept., Colorado State University, Fort Collins, CO, Tech. Rep., 1989.
[19] J. E. Baker, “Reducing bias and inefﬁciency in the selection algorithm,”
inGenetic Algorithms and their Applications (ICGA’87) , J. J. Grefen-
stette, Ed., 1987, pp. 14–21.
[20] H. M ¨uhlenbein and D. Schlierkamp-Voosen, “Predictive models for
the breeder genetic algorithm, I: Continuous parameter optimization,”
Evolutionary Computation , vol. 1, no. 1, pp. 25–49, 1993.
[21] R Development Core Team, R: A Language and Environment
for Statistical Computing , R Foundation for Statistical Computing,
Vienna, Austria, 2011, ISBN 3-900051-07-0. [Online]. Available:
http://www.R-project.org
[22] M. Harman, K. Lakhotia, and P. McMinn, “A multi-objective approach
to search-based test data generation,” in GECCO , 2007, pp. 1098–1105.
[23] K. Sen, D. Marinov, and G. Agha, “CUTE: a concolic unit testing engine
for C,” in ESEC/SIGSOFT FSE , 2005, pp. 263–272.
[24] C. Cadar and D. R. Engler, “Execution generated test cases: How to
make systems code crash itself,” in Model Checking Software, 12th
International SPIN Workshop , vol. 3639, 2005, pp. 2–23.
[25] N. Tillmann and J. de Halleux, “Pex-white box test generation for.NET,”
inTAP, 2008, pp. 134–153.
[26] K. Inkumsah and T. Xie, “Evacon: A framework for integrating evo-
lutionary and concolic testing for object-oriented programs,” in ASE,
November 2007, pp. 425–428.
[27] P. Tonella, “Evolutionary testing of classes,” in ISSTA , 2004, pp. 119–
128.
[28] K. Sen and G. Agha, “CUTE and jCUTE: Concolic unit testing and
explicit path model-checking tools,” in CAV, 2006, pp. 419–423.
[29] K. Lakhotia, N. Tillman, M. Harman, and J. de Halleux, “FloPSy -
Search-Based Floating Point Constraint Solving for Symbolic Execu-
tion,” in ICTSS , 2010, pp. 142–157.
[30] K. Lakhotia, M. Harman, and P. McMinn, “Handling dynamic data
structures in search based testing.” in GECCO , 2008, pp. 1759–1766.