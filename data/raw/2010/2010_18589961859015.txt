Deviance from Perfection is a Better Criterion than
Closeness to Evil when Identifying Risky Code
Marouane Kessentini, Stéphane Vaucher, Houari Sahraoui
DIRO, Université de Montréal, CANADA
{kessentm,vauchers,sahraouh}@iro.umontreal.ca
ABSTRACT
We propose an approach for the automatic detection of po-
tential design defects in code. The detection is based on
the notion that the more code deviates from good practices,
the more likely it is bad. Taking inspiration from artiﬁ-
cial immune systems, we generated a set of detectors that
characterize diﬀerent ways that a code can diverge from
good practices. We then used these detectors to measure
how far code in assessed systems deviates from normality.
We evaluated our approach by ﬁnding potential defects in
two open-source systems (Xerces-J and Gantt). We used
the library JHotDraw as the code base representing good
design/programming practices. In both systems, we found
that 90% of the riskiest classes were defects, a precision far
superiour to state of the art rule-based approaches.
Categories and Subject Descriptors
D.2.7 [ Software Engineering ]: Maintenance— Restructur-
ing, reverse engineering, and reengineering
General Terms
Design
Keywords
Maintenance, design defects, artiﬁcial immune systems
1. INTRODUCTION
In order to limit maintenance costs and improve the qual-
ity of their software systems, companies try to both enforce
good design/development practices and prevent bad prac-
tices. As a result, these practices have been studied by pro-
fessionals and researchers alike with a special attention given
to design-level problems.
There has been much research focusing on the study of bad
design practices sometimes called defects, antipatterns [9],
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
ASE’10, September 20–24, 2010, Antwerp, Belgium.
Copyright 2010 ACM 978-1-4503-0116-9/10/09 ...$10.00.
smells [1], or anomalies [2] in the literature1. Although these
bad practices are sometimes unavoidable, in most cases, de-
velopment teams should try to prevent them and remove
them from their code base as early as possible. Hence, many
fully-automated detection techniques have been proposed [3,
4, 5].
Several problems limit the eﬀectiveness of existing tech-
niques. Indeed, the vast majority of existing work relies
on rule-based detection [6, 4]. Diﬀerent rules identify key
symptoms that characterize a defect using combinations of
mainly quantitative (metrics), structural, and/or lexical in-
formation. Therefore, to identify and remove defects in a
system, all possible defects should be known and their symp-
toms characterized with rules. Moreover, the rules must to
be applied equally to any system in any context. This is not
reasonable considering the variety of software systems and
the diﬃculty of expressing some types of symptoms. These
diﬃculties explain a large portion of the high false-positive
rates mentioned in existing research [5].
In this article, we propose an automated detection ap-
proach that is completely diﬀerent from the state of art.
Instead of characterizing each symptom of each possible de-
fect type, we apply the principle of negative selection, the
process used by biological immune systems [7] to identify
antigens. An immune system does not try to detect speciﬁc
bacteria and viruses. Rather, it starts by detecting what
is abnormal, i.e.,what is diﬀerent from the healthy cells of
the body. The more something is diﬀerent, the more it is
considered risky.
We apply the same principle to the detection of design
defects by, ﬁrst, deﬁning what is normal. Normality is de-
ﬁned using a code base containing examples of well designed
and implemented software elements. Then, we create a set
of detectors that represent diﬀerent ways that a code can
diverge from the good code. Finally, elements of assessed
systems that are similar to detectors are considered as risky.
To evaluate our approach, we used classes from the JHot-
Draw library as our examples of well-designed and imple-
mented code. Two systems, Xerces-J and Gantt, were then
analyzed using our approach. Almost all the identiﬁed riski-
est classes (with levels >90%) were found in a list of classes
tagged as defects (blobs, spaghetti code and functional de-
composition) in another project [4].
Our contributions to automation are as follows. First,
our technique is fully automatable from the creation of de-
tectors to the evaluation of classes. Second, our technique
1In the remainder of this article, we use the generic term
defect to refer to an occurrence of a bad practice in the code
113
does not require an expert to write rules for every defect
type, and adapt them to diﬀerent systems. Finally, using
only standard algorithms to measure similarity, our tech-
nique not only outperforms rule-based techniques in terms
of precision, but we are also able to ﬁnd a good mix of defects
types. The major limitation of the approach is that we re-
quire a code base representing of good design practices. Our
results indicate however that JHotdraw seems to be usable
and could serve as a starting point for a company wishing
to use our approach..
The remainder of this paper is structured as follows. Sec-
tion 2 is dedicated to the problem statement. In Section 3,
we describe the principles of the Artiﬁcial Immune System
that inspires our approach and the adaptations of these prin-
ciples to the detection of design defects. Section 4 presents
and discusses the validation results. A summary of the re-
lated work in defect detection is given in Section 5. We
conclude and suggest future research directions in Section
6.
2. PROBLEM STATEMENT
In this section, we describe the problem of defect detec-
tion. We start by deﬁning important concepts. Then, we
detail the speciﬁc problems that are addressed by our ap-
proach.
2.1 Basic Concepts
Design defects , also called design anomalies refer to
design situations that adversely aﬀect the development of a
software. In general, they make a system diﬃcult to change
which may in turn introduce bugs.
Diﬀerent types of defects presenting a variety of symptoms
have been studied with the intent of improving their detec-
tion [8] and suggesting improvements paths. The two follow-
ing types of defects are commonly mentioned. In [1], Beck
deﬁnes 22 sets of symptoms of common defects, named code
smells . These include large classes, feature envy, long para-
meter lists, and lazy classes. Each defect type is accompa-
nied by some refactoring suggestions to remove them. Brown
et al. [9] deﬁne another category of design defects named
anti-patterns , which includes blob classes, spaghetti code,
and cut & paste programming. In both books, the authors
focus on describing the symptoms to look for in order to
identify speciﬁc defects.
Regarding our detection approach, we use the following
concepts:
•Acode fragment represents a software element that
is evaluated. This could be a class, method, or package
in an object-oriented code. Although our approach
could be applied to evaluate any of these entities, in
this paper, we use code fragment to refer essentially to
a class.
•Adesign risk is a code fragment that is dissimilar
(unusual) from known good code. It could be a design
defect or simply an unusual design/developpement prac-
tice.
•The process of discovering design defects consists
of ﬁnding high-risk code fragments in the system with-
out relying on speciﬁc knowledge on the known defect
types.2.2 Problem Statement
Any technique to detect design defect should address/ cir-
cumvent many diﬃculties inherent to the nature of defects.
Here is the description of the most important diﬃculties and
how they aﬀect an automation process.
•There is no exhaustive list of all possible types of
design defects. Although there has been signiﬁcant
work to classify defect types [8, 10, 11], programming
practices, paradigms and languages evolve making un-
realistic to support the detection of all possible de-
fect types. Furthermore, there might be company or
application-speciﬁc (bad) design practices.
•For those design defects that are documented, there is
no consensual deﬁnition of symptom detections . De-
fects are generally described using natural language
and their detection relies on the interpretation of the
developers. This limits the automation of the detec-
tion.
•The majority of detection methods do not provide an
eﬃcient manner to guide the manual inspection
of the candidate list . Potential defects are generally
not listed in an order that helps developers addressing
in priority the most severe ones. There is little work,
such as the one of Khomh et al [3], where probabilities
are used to order the results.
3. AIS-BASED DETECTION ALGORITHM
Our approach is based on the metaphor of biological im-
mune systems. In this section, we present the principles of
this metaphor, and our adaptation to the problem of detect-
ing design defects.
3.1 Principles of Artiﬁcial Immune Systems
The role of a biological immune system (IS) is to protect
its host organism against foreign elements such as pathogens
(e.g.,bacteria and viruses) and/or malfunctioning cells ( e.g.,
cancerous cells). This is performed following three phases:
(1)discovery , (2) identiﬁcation , and (3) elimination of for-
eign elements. Discovery is the phase that interests us in
particular for our work. Therefore, we explain its principle
in the following paragraphs.
There is no central organ that fully controls the IS. In-
stead, detectors wander in the body searching for harmful
elements. Any element that can be recognised by the im-
mune system is called an antigen . The cells that originally
belong to our body and are harmless to its functioning are
termed self(for self antigens) while the disease causing el-
ements are named nonself (for nonself antigens). The IS
classiﬁes cells that are present in the body as self and non-
self cells.
The immune system produces a large number of randomly
created detectors. A negative selection mechanism elimi-
nates detectors that match cells present in a protected en-
vironment (bone marrow and the thymus) where only self
cells are assumed to be present. Non-eliminated ones be-
come naive detectors; they die after some time unless they
match an element assumed to be a pathogen. Detectors that
do match a pathogen are quickly cloned; this is used to ac-
celerate the response to future attacks. Since the clones are
not exact replicates (they are mutated), this provides a more
114focused response to pathogens. This process, called aﬃnity
maturation , provides an eﬃcient adaptation to a changing
non-self environment. A detailed presentation of the biolog-
ical immune system can be found in books such as [12]).
The success of immune systems at keeping a living or-
ganism healthy inspired the emergence of artiﬁcial immune
systems (AIS) as a generic solution to problems in several
domains, such as scheduling, computer security, optimiza-
tion, or robotics [13]. AIS can be adapted to the problem
of defect detection. The following mappings shows the sim-
ilarity between our problem and the AIS concepts.
•Body : the evaluated system, more precisely, its code;
•Detector : an artiﬁcial code fragment that is very dif-
ferent from a well-designed code base;
•Self Cells : well-designed code fragments in the sys-
tem to evaluate (without design defects);
•Non-Self Cells : code fragments in the system to eval-
uate that present a risk of being design defects;
•Aﬃnity : the similarity between detectors and code
fragments to evaluate.
3.2 Approach Overview
Figure 1 gives an overview of our approach. The detec-
tion process has two main steps: detector generation and
risk estimation. Detectors are generated from a collection
of code fragments coming from one or more well-designed
systems. These code fragments deﬁne the reference of what
is considered normal code. The generation process of detec-
tors is performed using a heuristic search that maximizes on
one hand, the distance between detectors and normal code
and, on the other hand, the distance between the detectors
themselves. The same set of detectors could be used to eval-
uate many systems, and it could be updated as the normal
code base or development practices evolves.
The second step of the detection process consists of com-
paring the code to evaluate to the detectors. A code frag-
ment that exhibits a similarity with a detector is considered
as a risky element. The higher the similarity, the more a
code fragment is considered risky. Both the detector gener-
ation and risk estimation steps use similarity scores. Before
detailing the two steps, we ﬁrst describe the similarity func-
tions used in this work.
Figure 1: Approach Overview
3.3 Similarity between Code Fragments
To calculate the similarity between two code fragments,
we adapted the Needleman-Wunsch alignment [14] algorithmto our context. It is a dynamic programing algorithm used
in bioinformatics to eﬃciently ﬁnd similar regions between
two sequences of DNA, RNA or protein [15]. An example of
the algorithm is presented in Figure 2.
Figure 2: Global alignment of two strings
As we are manipulating code elements and not sequences
(strings), we represent these elements by sets of predicates.
Each predicate type corresponds to a construct type of an
object-oriented system: Class (C), attribute (A) , method
(M), parameter (P), generalization (G), and method invoca-
tion relationship between classes (R). For example, in Fig-
ure 3, the sequence of predicates CGAAMPPM corresponds
to a class with a generalization link, containing two at-
tributes and two methods. The ﬁrst method has two pa-
rameters.
Figure 3: Encoding
Predicates include details about the associated constructs
(visibility, types, etc.). These details (thereafter called para-
meters) determine ways a code fragment can deviate from a
notion of normality. The example of Figure 3 is a represen-
tation of class RangeExceptionImpl from Xerces-J. The cor-
responding predicate set, extracted using our inhouse eclipse
plugin is as follows:
Class(RangeExceptionImpl,public);
Generalisation(RangeExceptionImpl,RangeException);
Attribute(RangeExceptionImpl,serialVersionUID,long,static);
Attribute(RangeExceptionImpl,serialImplUID,short,static);
Method(RangeExceptionImpl,RangeExceptionImpl,void,Y,public);
Parameter(RangeExceptionImpl,RangeExceptionImpl,code,short);
Parameter(RangeExceptionImpl,RangeExceptionImpl,message,
String);
Method(RangeExceptionImpl,implSerial,void,Y,private);
As described below, the Needleman-Wunsch global align-
ment algorithm [14] is described recursively. . When align-
ing two sequences ( a1,...,an) and ( b1,...,bm). Each position
si,jin the matrix corresponds to the best score of alignment
considering the previously aligned elements of the sequences.
The algorithm can introduce gaps (represented by ”-”) to im-
prove the matching of subsequences.
115si,j=Max8
><
>:si−1,j−g // insert gap for bj
si,j−1−g // insert gap for ai
si−1,j−1+sim i,j// match
where si,0=g∗iands0,j=g∗j
At any given point, algorithm considers two possibilities.
First, it considers the case when a gap should be inserted.
When a gap is inserted for either aorb, the algorithm ap-
plies a penalty of g. Second, it tries to match predicates.
The similarity function sim i,jreturns the reward or cost of
matching aitobj. The ﬁnal similarity is contained in sn,m.
Our adaptation of the algorithm is straightforward. We
deﬁne the gap penalty gand the similary function to match
individual predicates ( sim). We do not seek perfect matches
in terms of number of predicates. A class with 4 is not
necessarily diﬀerent from one with 6 methods if the methods
are similar. To eliminate the sensitivity of the algorithm to
size, we thus set the gap penalty to 0.
We deﬁne a predicate-speciﬁc function to measure the sim-
ilarity. First, if the types diﬀer, the similarity is 0. As we
manipulate sequences of complex predicates and not strings,
sim i,jis deﬁned as a predicate-matching function, PM ij.
PM ij, measures the similarity in terms of the elements of
the predicates associated to aiandbj. This similarity is the
ratio of common parameters in both predicates.
PM ij=∀p∈ai,q∈bi∩(p, q)
max(|ai|,|bj|)|
where aiandbjare treated as sets of predicates. The equiv-
alence between predicate parameters depends on each type
of parameter. For visibility and element types, it means
equality. Speciﬁc names are not considered. Instead, they
are used to indicate a common reference by other predicates.
For example, if a class deﬁnes an attribute and its related
getter method. They will both share the same class name.
To illustrate an example for the local alignment algorithm,
let us consider the class RangeExceptionImpl , described pre-
viously, as a code fragment C32andOptions ,C152as a second
code fragment to compare with C32. The code fragments are
sequentially numbered. C152is deﬁned as follows :
Class(Options,public);
Method(Options,isFractionalMetrics,boolean,N,public);
Method(Options,isTextAntialiased,boolean,N,private);
Parameter(Options,isTextAntialiased,id,String);
Relation(AbstractFigure;getFontRenderContext;isFractionalMetrics,
Options,N);
According to the coding mentioned previously, the pred-
icate sequence for C32is CGAAMPPM and one of C152is
CMMPR. The alignment algorithm ﬁnds the best alignment
sequence as shown in Figure 4.
There are three matched predicates between C32andC152:
one class, one method, and one method parameter. If we
consider the second matched predicates p15=
Method(RangeExceptionImpl,RangeExceptionImpl,void,Y,public) from
C32andp22=Method(Options,isFractionalMetrics,boolean,
N,public); from C152. The predicates have two common pa-
rameters out of a possible ﬁve. The resulting similarity is
consequently 40%. We normalize this absolute similarity
Figure 4: Best alignment sequence between C32and
C152
measure, sn,m, by the maximum number of predicates to
produce our similarity measure:
Sim(A, B) =sn,m
max(n, m)(1)
3.4 Detectors Generation
This section describes how a set of detectors is produced
starting from the reference code. The generation, inspired
by the work of Gonzalez and Dasgupta [16], follows a genetic
algorithm [17]. The idea is to produce a set of detectors
that best covers the possible deviations from the reference
code. As the set of possible deviations is very large, its
coverage may require a huge number of detectors, which is
infeasible in practice. For example, pure random generation
was shown to be infeasible in [18] for performance reasons.
We therefore consider the detector generation as a search
problem. A generation algorithm should seek to optimize
the following two objectives:
•Maximize the generality of the detector to cover the
non-self by minimizing the similarity with the self;
•Minimize the overlap (similarity) between detectors.
These two objectives deﬁne the cost function that evalu-
ates the quality of a solution and, then guides the search.
The cost of a solution D(set of detectors) is evaluated as the
average costs of the included detectors. We derive the cost
of a detector dias a weighted average between the scores
of respectively, the lack of generality and the overlap. For-
mally,
cost(di) =LG(di) +O(di)
2(2)
Here, we give equal weight to both scores. The lack of
generality is measured by a matching score LG(di) between
the predicate sequence of a detector diand those of all the
classes sjin the reference code (call it S). It is deﬁned as the
average value of the alignment scores Sim(di, sj) between di
and classes sjinS. Formally,
LGdi=P
sj∈SSim(di, sj)
|S|
Similarly, the overlap Oi, is measured by the average value
of the individual Sim(di, dj) between the detector diand all
the other detectors djin the solution D. Formally,
116Odi= 1−P
dj,j/negationslash=iSim(di, dj)
|D|
The cost function deﬁned above is used in our genetic-
based search algorithm. Genetic algorithms (GA) imple-
ment the principle of natural selection [17]. Roughly speak-
ing, a GA is an iterative procedure that generates a pop-
ulation of individuals from the previous generation using
two operators: crossover and mutation. Individuals having
a high ﬁtness have higher chances to reproduce themselves
(by crossover), which improves the global quality of the pop-
ulation. To avoid falling in local optima, mutation is used
to randomly change individuals. Individuals are represented
by chromosomes containing a set of genes.
For the particular case of detector generation, we reuse
the predicate sequences as chromosomes. Each predicate
represents a gene. We start by randomly generating an ini-
tial population of detectors. The size of this population is
a parameter that will be discussed later in Section 4. This
size is maintained constant during the evolution. The ﬁtness
of each detector is evaluated by the inverse function of cost.
The ﬁtness determines the probability of being selected for
the crossover. This process is called a wheel-selection strat-
egy [17].
In fact, for each crossover, two detectors are selected by
applying twice the wheel selection. Even though detector are
selected, the crossover happens only with a certain probabil-
ity. The crossover operator allows to create two oﬀspring o1
ando2from the two selected parents p1andp2. It is deﬁned
as follows:
•A random position k, is selected in the predicate se-
quences.
•The ﬁrst kelements of p1become the ﬁrst kelements
ofo1. Similarly, he ﬁrst kelements of p2become the
ﬁrstkelements of o2.
•The remaining elements of, respectively, p1andp2are
added as second parts of, respectively, o2ando1.
For instance, if k = 3 and p1 = CAMMPPP and p2 = CM-
PRMPP, then o1 = CAMRMPP and o2 = CMPMPPP.
The mutation operator operator consists of randomly chang-
ing a predicate.
3.5 Risk Estimation
The second step of our defect discovery is the assessment
of risk for the diﬀerent code fragments evaluated. These
are also represented by predicate sequences. Each sequence
is compared using the alignment algorithm to the detectors
obtained in the previous step. The risk of being a defect, as-
sociated to a code fragment eiis deﬁned as the average value
of the alignment scores Sim(ei, dj) obtained by comparing
eito respectively all the detectors of a set D. Formally,
risk ei=P
dj∈DSim l(ei, dj)
|D|
The code fragments can then be ranked according to their
risks to be inspected by the maintainers.4. EVALUATION
To test our approach, we studied its usefulness to guide
quality assurance eﬀorts on two open-source programs. In
this section, we describe our experimental setup and present
the results of an exploratory study.
4.1 Goals and Objectives
The goal of the study is to evaluate the eﬃciency of our
AIS approach for the discovery of design defects from the
perspective of a software maintainer conducting a quality
audit.
We present the results of the experiment aimed at answer-
ing the following research questions:
RQ1: To what extent can the proposed approach discover
design defects?
RQ2: What types of defects does it locate?
To answer RQ1, we used an existing corpus of known de-
sign defects to evaluate the precision of our approach. We
ranked classes in order of decreasing risk and compared re-
sults to produced by a rule-based strategy [4]. To answer
RQ2, we investigated the type of defects that were found.
4.2 System Studied
We used three open-source Java projects to perform our
experiments: GanttProject v1.10.2, Xerces v2.7.0, and JHot-
draw v7.1. Table 1 summarizes facts on these programs.
GanttProject2is a tool for creating project schedules by
means of Gantt charts and resource-load charts. GanttPro-
ject enables breaking down projects into tasks and establish-
ing dependencies between these tasks. Xerces3is a family
of software packages for parsing and manipulating XML. It
implements a number of standard APIs for XML parsing.
JHotdraw v7.14is a framework used to build graphic edi-
tors. It was ﬁrst built as an example of the use of design
patterns. JHotdraw was chosen because it contains very few
known design defects. In fact, previous work [19] could not
ﬁnd any Blob defects. In our experiments, we used all of
the classes in JHotdraw as our example set of good code.
We chose the Xerces and Gantt libraries because they are
medium sized open-source projects and were analysed in re-
lated work. The version of Gantt studied was known to be
of poor quality, which lead to a new major version. Xerces-
J on the other hand has been actively developed over the
past 10 years and its design has not been responsible for a
slowdown of its development.
In [4], Moha et al. asked three groups of students to
analyse the libraries to tag instances of speciﬁc antipat-
terns to validate their detection technique, DECOR. For
replication purposes, they provided5a corpus of describ-
ing instances of diﬀerent antipatters including: Blob classes,
Spaghetti code , and Functional Decompositions . Blobs are
classes that do or know too much. Spaghetti Code (SC) is
code that does not use appropriate structuring mechanisms.
Functional Decomposition (FD) is code that is structured
as a series of function calls. These represent diﬀerent types
of design risks. In our study, we veriﬁed the capacity of our
2http://ganttproject.biz/index.php
3http://xerces.apache.org/
4http://jhotdraw.org
5http://www.ptidej.net/research/decor/index_html
117approach to locate classes that corresponded to instances of
these antipatterns.
Systems
/sharpclasses
/sharpPredicates
KLOC
GanttProjectv1.10.2
245
16640
31
Xerces v2.7.0 991 67810 240
JHotdraw v7.1
471
40354
91
Table 1: Program statistics
4.3 Experimental Setting
For our experiment, we randomly generated 100 detectors
for JHotDraw (about a quarter of the number of examples)
with a maximum size of 256 characters. The same set of de-
tectors was used on both Xerces and Gantt. The obtained
results6were compared to those of DECOR [4], a state of
the art rule-based detection technique. For every antipat-
tern in Xerces and Gantt, they published the number of
antipatterns detected, the number of true positives, and the
precision (ratio of true positives over the number detected).
Our comparison is consequently done using precision. We
would have liked to consider recall, but they did not publish
clean, complete data describing all existing antipatterns. We
therefore could not perform systematic comparisons of the
recall of our approach. Instead, we discuss the proportion
of “known antipatterns” detected.
4.4 Results
Tables 2 and 3 summarize our ﬁndings. Each class is pre-
sented with its risk, its size, and the associated defect types.
We only presented classes with a risk level of >= 70%; this
corresponds to about 5% of the classes in the system. For
Gantt, our precision over the top 20 classes is 95% with the
eight riskiest classes being true positives. DECOR on the
other hand has a combined precision of 59% for its detec-
tion on the same set of antipatterns. For Xerces, our preci-
sion is of 90% with the top 30 classes correctly identiﬁed as
defects. For the same dataset, DECOR had a precision of
67%. In the context of this experiment, we can conclude that
our technique is able to accurately identify design anomalies
more accurately than DECOR (RQ1).
We noticed that our technique does not have a bias to-
wards the detection of speciﬁc anomaly types. In Xerces,
we had an almost equal distibution of each antipattern (14
SCs, 13 Blobs, and 13 FDs). On Gantt, the distribution is
not as balanced. This is principally due to the number of
actual antipatterns in the system. We found all four known
Blobs and all 11 SCs in the system. We found 6/17 FDs,
two more than DECOR.
Having a relatively good distribution of antipatterns is
useful for a quality engineer as he can focus on the notion
of riskiest classes regardless of the type. Furthermore, since
the results are ranked he can eﬃciently use his time unlike
DECOR.
This ability to identify diﬀerent types of antipatterns un-
derlines a key strength to our approach: the similarity func-
tion is able to abstract out the importance of size. Most
other tools and techniques rely heavily on the notion of size
to detect defects. This is reasonable considering that some
antipatterns like the Blob are associated to a notion of size.
6http://www.iro.umontreal.ca/~sahraouh/papers/
ASE2010/
Class
Risk
S.C.
Blob
F.D.
GanttOptions
0.96
/check
GanttTree
0.96
/check
GregorianTimeUnitStack
0.93
/check
GanttDialogPerson
0.92
/check
CSVSettingsPanel
0.9
/check
GanttProject
0.9
/check
/check
GanttTaskPropertiesBean
0.9
/check
NewProjectWizard
0.87
/check
TimeUnitGraph
0.87
ResourceLoadGraphicArea
0.85
/check
/check
GanttCSVExport
0.82
/check
GanttGraphicArea
0.82
/check
/check
FindPossibleDependeesAlgo...
0.82
/check
GanttXFIGSaver
0.81
/check
GanttApplet
0.79
/check
GraphicPrimitiveContainer
0.75
/check
Shape
0.75
GanttXMLSaver
0.75
/check
RecalculateTaskCompletion...
0.71
/check
TaskHierarchyManagerImpl
0.71
/check
Precision
95%
Table 2: Results for Gantt
For antipatterns like FDs however, the notion of size is ir-
relevant and this makes this type of anomaly hard to detect
using structural information. This diﬃculty is why DECOR
includes an analysis of naming conventions to perform its de-
tection. Using naming convention means that their results
depend on the coding practices of a development team. Our
results are however comparable to theirs while we do not
leverage lexical information.
4.5 Discussion
In this section, we discuss diﬀerent issues concerning the
detection of design risks.
Number of Detectors.
An important factor to our detection technique is the
number of detectors generated. In Figure 5, we present the
precision of our approach when varying the number of de-
tectors ( Nd) with Nd={50,100,150,200 }. The ﬁgure shows
that the performance of our approach improves as we con-
sider more detectors. When we use 200 detectors (50% of
the total number of cases in JHotDraw), our performance
is over 95% for both systems. Our technique requires the
comparison of every class to every detector, this improved
performance is at a negligible cost in terms of execution
time. Indeed, the execution time for applying the detection
on each system varies between 2 minutes for 50 detectors
and 15 minutes for 200.
Variability in Detector Generation.
Another issue is our selection of interesting detectors. The
detection results might vary depending on the detectors which
are generated randomly (though guided by a meta-heuristic).
To ensure that our results are relatively stable, we compared
the results of multiple executions for detector generation.
When we consider results up to 70% of risk, we observed
an average precision of 92% for Gantt and 91% for Xerces.
Furthermore, we found that the majority of defects detected
are found in every execution (54% and 60% respectively for
Gantt and Xerces). These unanimously detected defects
were systematically the riskiest classes in every execution:
in Gantt, the top 10 classes were common to all executions.
1185 10 15 200.0 0.2 0.4 0.6 0.8 1.0
Classes inspectedPrecisionl l l l l l l l
llll l l l l
l l l l
l
ll
ll l l l l
llllllllll l l l l l l l
l ll l l l l l l l l l l l l l l l l l l l l l l
l
l
l
l# detectors
50
100150200(a) Gantt
0 10 20 30 40 500.0 0.2 0.4 0.6 0.8 1.0
Classes inspectedPrecisionl l l l l l l l llllllllllllllllllllllllllllllllllllllllll l l l l l l l l lllllllllllllllllllllllllllllllllllllllll l l l l l l l l llllllllllllllllllllllllllllllllllllllll
l
l
l# detectors
50
100150200 (b) Xerces
Figure 5: Eﬀect of the number of detectors on detection precision vs. #classes inspected
In Xerces, there was only one non-unanimous class in the
top ten classes returned which was a false-positive. The av-
erage rank for a class detected by a single execution was 18
and 34 for Gantt and Xerces respectively. We consequently
believe that since the variability comes from the least risky
classes, and that our technique is stable.
Metric-based Detection vs. Similarity-based Detection.
Our approach is signiﬁcantly diﬀerent from existing work
that are rule-based. A key problem with these approaches
is that these rules simplify the diﬀerent notions that are
useful for the detection of certain antipatterns. In particular,
to detect blobs the notion of size is important. Most size
metrics are highly correlated with one another, and the best
measure of size can depend on the system itself. Our use of
predicates allows for complex structures to be detected.
For example, we correctly detected TaskHierarchyMan-
agerImpl in Gantt. It holds a reference to the root of the
hierarchy, and controls creations of new children to the root.
public class TaskHierarchyManagerImpl {
private TaskHierarchyItem myRootItem =
new TaskHierarchyItem(null ,null);
public TaskHierarchyItem getRootItem() {
return myRootItem;
}
public TaskHierarchyItem createItem(Task task) {
TaskHierarchyItem result =
new TaskHierarchyItem(task, myRootItem);
return result;
}}
public class TaskManagerImpl implements TaskManager { ...
private final TaskHierarchyManagerImpl myHierarchyManager
= new TaskHierarchyManagerImpl();
public TaskHierarchyManagerImpl getHierarchyManager() {
return myHierarchyManager;
}...}
It is detected for three reasons. First, it declares one at-
tribute type (TaskHierarchyItem) on which it never invokes
any methods. Second, it is used in a similar manner by
TaskManagerImpl. Finally, apart from creating objects, it
never uses any methods. It is consequently a datastructure.These types of relationships are hard to detect using metrics.
On the other hand, our technique produced a detector that
was almost a complete match (except the ﬁnal parameter):
Attribute(X,aaaa,AA,N,private); # myRootItem
Attribute(X,aa,X,N,private); # myHierarchyManager
Class(X,N,N,public); # TaskHierarchyManagerImpl
Method(X,z,X,Y,N,N,public); # createItem
Method(X,zzzz,AA,N,N,N,public); # getRootItem
Method(X,zxzzz,X,N,N,N,public); # getHierarchyManager
Parameter(X,z,zuwe,gsfg,declaration); # task
Parameter(X,z,xuqye,fzfgg,local); # result
Parameter(X,zzzz,jdajg,gffgs,declaration); # Match error
DECOR also successfully identiﬁed this class. However,
it did so, not because of metrics, but because the name of
the class contains the term Manager .
Building an Example Data Set.
The reliability of the proposed approach requires an exam-
ple set of good code. It can be argued that constituting such
a set might require more work than identifying and adapt-
ing rules. In our study, we showed that by using JHotdraw
directly, without any adaptation, the technique can be used
out of the box and this will produce good detection results
for the detection of antipatterns for the two systems studied.
The performance of this detection (in terms of precision)
was superiour to that of DECOR. In an industrial setting,
we could expect a company to start with JHotDraw, and
gradually migrate its set of good code examples to include
context-speciﬁc data. This might be essential if we consider
that diﬀerent languages and software infrastructures have
diﬀerent best/worst practices.
5. RELATED WORK
Several studies have recently focused on detecting design
defects in software using diﬀerent techniques. These tech-
niques range from fully automatic detection to guided man-
ual inspection. The related work can be classiﬁed into three
broad categories: metric-based detection, detection of refac-
toring opportunities, visual-based detection.
119Class
Risk
S.C.
Blob
F.D.
DFAContentModel
0.97
/check
XSFacets
0.96
/check
XMLSerializer
0.96
/check
XMLVersionDetector
0.96
/check
XML11EntityScanner
0.93
/check
XSDHandler
0.92
/check
/check
Token
0.91
/check
XMLEntityManager
0.91
/check
XSDAbstractTraverser
0.91
/check
XML11DTDValidator
0.91
/check
DOMNormalizer
0.91
/check
XMLNSDTDValidator
0.88
/check
ParserConﬁgurationSettings
0.88
/check
SAXParser
0.85
/check
DTDGrammar
0.84
/check
XML11NonValidatingConﬁguration
0.84
/check
XMLDTDValidator
0.84
/check
XMLEntityScanner
0.84
/check
XSAttributeGroupDecl
0.82
/check
AbstractDOMParser
0.81
/check
SchemaDOM
0.81
/check
XML11DTDConﬁguration
0.81
/check
XSDAttributeTraverser
0.81
/check
ObjectFactory
0.8
/check
XIncludeHandler
0.8
/check
XSDFACM
0.78
/check
NonValidatingConﬁguration
0.78
/check
XMLSchemaValidator
0.78
/check
DTDConﬁguration
0.77
/check
CoreDocumentImpl
0.77
/check
/check
XSAttributeChecker
0.77
/check
CMNodeFactory
0.77
RegexParser
0.75
/check
TimeDV
0.75
XML11Conﬁguration
0.74
/check
XMLFilterImpl
0.73
DOMSerializerImpl
0.71
/check
XSFacets
0.71
/check
BaseMarkupSerializer
0.71
ElementSchemePointer
0.71
/check
XMLParser
0.7
/check
XPathMatcher
0.7
/check
Precision
90%
Table 3: Results for Xerces
In ﬁrst category, Marinescu [6] deﬁned a list of rules re-
lying on metrics to detect what he calls design ﬂaws of OO
design at method, class and subsystem levels. Erni et al.
[20] use metrics to evaluate frameworks with the goal of im-
proving them. They introduce the concept of multi-metrics,
as an n-tuple of metrics expressing a quality criterion ( e.g.,
modularity). The main limitation of the two previous con-
tribution is the diﬃculty to deﬁne threshold values for met-
rics in the rules. To circumvent this problem, Alikacem et
al.[21] express defect detection as fuzzy rules with fuzzy
label for metrics, e.g., small, medium, large . When evaluat-
ing the rules, actual metric value are mapped to truth value
for the labels by means of membership functions. Although
no thresholds have to be deﬁned, still, it is not obvious to
decide for membership functions.
The previous approaches start from the hypothesis that
all defect symptoms could be expressed in terms of met-
rics. Actually, many defects involve notions that could not
quantiﬁed. This observation was the foudation of the work
of Moha et al. [4]. In their approach, named DECOR,
they start by describing defect symptoms using an abstract
rule language. These descriptions involve diﬀerent notions
such as class roles and structures. The descriptions are later
mapped to detection algorithms. In addition to the thresh-
old problem, this approach uses heuristics to approximate
some notions with results in an important rate of false posi-
tives. Another limitation of DECOR is that all the detected
defect candidate are listed without any rank that help the
maintainers checking/addressing in priority the most severe
ones.
Khomh et al. [3] extended DECOR to support uncertainty
and to sort the defect candidates accordingly. Uncertainty
is managed by Bayesian belief networks that implement the
detection rules of DECOR. The detection outputs are prob-
abilities that a class is an occurrence of a defect type.
In our approach, all the above mentioned problems re-
lated to the use of rules and metrics do not arise. Indeed,
the symptoms are not explicitly used, which reduces the
adaptation/calibration eﬀort.
In the second category of work, defects are not detected
explicitly. They are implicitly because, the approaches refac-
tor a system by detecting elements to change to improve he
global quality. For example, in [22], defect detection is con-
sidered as an optimization problem. The authors use a com-
bination of 12 metrics to measure the improvements achieved
when sequences of simple refactorings are applied, such as
moving methods between classes. The goal of the optimiza-
tion is to determine the sequence that maximize a function,
which captures the variations of a set of metrics [23]. The
fact that the quality in terms of metrics is improved does
not necessary means that the changes make sense. The link
between defect and correction is not obvious, which make
the inspection diﬃcult for the maintainers. In our case, we
separate the detection and correction phase.
The high rate of false positives generated by the auto-
matic approaches encouraged other teams to explore semi-
automatic solutions. These solutions took the form of visu-
alization based environments. The primary goal is to take
advantage of the human ability to integrate complex contex-
tual information in the detection. Kothari et al. [24] present
a pattern-based framework for developing tool support to
detect software anomalies by representing potentials defects
with diﬀerent colors. Later, Dhambri et al. [25] propose a
visualization-based approach to detect design anomalies by
automatically detecting some symptoms and letting others
to the human analyst. The visualization metaphor was cho-
sen speciﬁcally to reduce the complexity of dealing with a
large amount of data. Still, the visualization approach is
not obvious when evaluating large-scale systems. Moreover,
the information visualized is for the most part metric-based
meaning that complex relationships can still be diﬃcult to
detect.
In our case, the human intervention is needed for the in-
spection of the candidate only. This inspection is made eas-
ier because, the candidates are ranked by risk, and also be-
cause, by analysing the most similar detectors, it is possible
to identify what part of the element was problematic.
The work that is closest to ours is by Catal and Diri [26].
The authors use a machine-learning version of AIS called
an Artiﬁcial Immune Recognition System (AIRS) to learn a
prediction model for defect-prone modules. The AIRS used
was a generic package implemented in the machine-learning
package Weka. This package cannot handle complex struc-
tures like predicates and does not implement the negative
selection algorithm.
1206. CONCLUSION
In this article, we presented a new approach to problem of
detecting design defects. Typically, researchers and practi-
tioners try to characterize diﬀerent types of common design
defects and present symptoms to use in order to locate them
in a system. In our work, we show that we do not need this
knowledge to perform a detection. Instead, all we need is a
clear notion of what is good. What signiﬁcantly diverges is
often a defect. Interestingly enough, our study shows that
our technique outperforms an DECOR [4], a state of the art,
rule-based approach on its test corpus.
By ignoring the detection of speciﬁc defect types, we avoid
problems with existing detection techniques. First, the de-
tection of most defect is diﬃcult to automate because their
deﬁnitions are expressed informally. Second, even with a
precise deﬁnition, some symptoms are context-speciﬁc and
might or not be useful for a given system. There is conse-
quently a non-negligeable eﬀort to test and adapt a detection
process to another system. Finally, by presenting all defects,
regardless of types in order of risk, a development team can
focus on the most urgent problems ﬁrst.
This technique was tested on two open-source systems and
the results were promising. The discovery process uncovered
diﬀerent types of design defects was more eﬃciently than
DECOR. In fact, for Gantt, our precision is 95% with the
eight riskiest classes being true positives. DECOR on the
other hand has a combined precision of 59% for its detection
of the same set of antipatterns. For Xerces, our precision is
of 90% with the top 30 classes correctly identiﬁed as defects.
For the same dataset, DECOR had a precision of 67%. Fur-
thermore, as DECOR needed an expert to deﬁne rules, our
results were achieved without any expert knowledge, rely-
ing only on the good structure of JHotdraw to guide the
detection process.
In this work, we only looked at the ﬁrst step of an im-
mune systems: the discovery of risk. As part of our future
work, we plan to explore the other two steps: identiﬁcation
and correction of detected design defects (refactoring). Fur-
thermore, we need to extend our reference code base with
other well-designed code in order to take into consideration
diﬀerent programming contexts. Speciﬁcally, we plan on:
•Adapting the AIS metaphor to identify discovered de-
fects using immune memory and danger theory [27].
•Adapting the colonal selection algorithm [28] to ﬁnd
the best immune response that correspond the optimal
refactorings sequence to apply.
•Using our approach for defect prediction using the es-
timation risk score.
Acknowledgment
This work has been partly funded by the Natural Sciences
and Engineering Research Council of Canada (NSERC) and
the Tunisian Ministry of Higher Education and Scientiﬁc
Research.
7. REFERENCES
[1]M. Fowler, Refactoring – Improving the Design of
Existing Code , 1sted. Addison-Wesley, June 1999.
[2]N. Fenton and S. L. Pﬂeeger, Software Metrics: A
Rigorous and Practical Approach , 2nd ed. London,
UK: International Thomson Computer Press, 1997.
[3]F. Khomh, S. Vaucher, Y.-G. Gu´ eh´ eneuc, and
H. Sahraoui, “A Bayesian Approach for the Detection
of Code and Design Smells,” in Proceedings of the 9th
International Conference on Quality Software , D.-H.
Bae and B. Choi, Eds. IEEE Computer Society
Press, August 2009.
[4]N. Moha, Y.-G. Gu´ eh´ eneuc, L. Duchien, and A.-F. L.
Meur, “DECOR: A method for the speciﬁcation and
detection of code and design smells,” Transactions on
Software Engineering (TSE) , 2009, 16 pages. [Online].
Available: http://www-etud.iro.umontreal.ca/˜ptidej/
Publications/Documents/TSE09.doc.pdf
[5]H. Liu, L. Yang, Z. Niu, Z. Ma, and W. Shao,
“Facilitating software refactoring with appropriate
resolution order of bad smells,” in ESEC/FSE ’09:
Proceedings of the the 7th joint meeting of the
European software engineering conference and the
ACM SIGSOFT symposium on The foundations of
software engineering . New York, NY, USA: ACM,
2009, pp. 265–268.
[6]R. Marinescu, “Detection strategies: Metrics-based
rules for detecting design ﬂaws,” in Proceedings of the
International Conference on Software Maintenance ,
2004, pp. 350–359.
[7]F. Azuaje, “Review of ”artiﬁcial immune systems: a
new computational intelligence approach” by l.n. de
castro and j. timmis (eds) springer, london, 2002,”
Neural Netw. , vol. 16, no. 8, pp. 1229–1229, 2003.
[8]A. J. Riel, Object-Oriented Design Heuristics .
Addison-Wesley, 1996.
[9]W. J. Brown, R. C. Malveau, W. H. Brown, H. W.
McCormick III, and T. J. Mowbray, Anti Patterns:
Refactoring Software, Architectures, and Projects in
Crisis , 1sted. John Wiley and Sons, March 1998.
[Online]. Available: www.amazon.com/exec/obidos/
tg/detail/-/0471197130/ref=ase \
theantipatterngr/
103-4749445-6141457
[10]M. M ¨antyl ¨a, J. Vanhanen, and C. Lassenius, “A
taxonomy and an initial empirical study of bad smells
in code,” in ICSM ’03: Proceedings of the International
Conference on Software Maintenance . Washington,
DC, USA: IEEE Computer Society, 2003, p. 381.
[11]W. C. Wake, Refactoring Workbook . Boston, MA,
USA: Addison-Wesley Longman Publishing Co., Inc.,
2003.
[12]K. J., Immunology , 5th ed. by Richard A. Goldsby,
Thomas J. Kindt, Barbara A. Osborne, W.H, 2002.
[13]D. Dasgupta, Z. Ji, and F. Gonzalez, “Artiﬁcial
immune system (ais) research in the last ﬁve years.” in
IEEE Congress on Evolutionary Computation (1) .
IEEE, 2003, pp. 123–130. [Online]. Available:
http://dblp.uni-trier.de/db/conf/cec/cec2003-1.html#
DasguptaJG03
[14]L. Nanni and A. Lumini, “Generalized
needleman-wunsch algorithm for the recognition of
t-cell epitopes,” Expert Syst. Appl. , vol. 35, no. 3, pp.
1463–1467, 2008.
[15]M. Brudno, “Algorithms for comparison of dna
sequences,” Ph.D. dissertation, Stanford, CA, USA,
2004, adviser-Batzoglou, Seraﬁm.
[16]F. A. Gonz´ alez and D. Dasgupta, “Anomaly detection
using real-valued negative selection,” Genetic
121Programming and Evolvable Machines , vol. 4, no. 4,
pp. 383–403, 2003.
[17]D. E. Goldberg, Genetic Algorithms in Search,
Optimization and Machine Learning . Boston, MA,
USA: Addison-Wesley Longman Publishing Co., Inc.,
1989.
[18]H. Hou and G. Dozier, “An evaluation of negative
selection algorithm with constraint-based detectors,”
inACM-SE 44: Proceedings of the 44th annual
Southeast regional conference . New York, NY, USA:
ACM, 2006, pp. 134–139.
[19]I. G. Czibula and G. Czibula, “Clustering based
automatic refactorings identiﬁcation,” in SYNASC
’08: Proceedings of the 2008 10th International
Symposium on Symbolic and Numeric Algorithms for
Scientiﬁc Computing . Washington, DC, USA: IEEE
Computer Society, 2008, pp. 253–256.
[20]K. Erni and C. Lewerentz, “Applying design metrics
to object-oriented frameworks,” in Proc. IEEE Symp.
Software Metrics . IEEE Computer Society Press,
1996.
[21]H. Alikacem and H. Sahraoui, “D´ etection d’anomalies
utilisant un langage de description de r` egle de qualit´ e.”
inactes du 12e colloque LMO , LMO, Ed., 2006.
[22]M. O’Keeﬀe and M. . Cinn´ eide, “Search-based
refactoring: an empirical study.” Journal of Software
Maintenance , vol. 20, no. 5, pp. 345–364, 2008.
[Online]. Available: http://dblp.uni-trier.de/db/
journals/smr/smr20.html#OKeeﬀeC08
[23]M. Harman and J. A. Clark, “Metrics are ﬁtness
functions too.” in IEEE METRICS . IEEE Computer
Society, 2004, pp. 58–69. [Online]. Available:
http://dblp.uni-trier.de/db/conf/metrics/metrics2004.
html#HarmanC04
[24]S. C. Kothari, L. Bishop, J. Sauceda, and
G. Daugherty, “A pattern-based framework for
software anomaly detection,” Software Quality
Journal , vol. 12, no. 2, pp. 99–120, June 2004.
[Online]. Available:
http://springerlink.com/content/v115717r15420214/
?p=bf86b148d5d74754baec247cd0661c7c {\&}pi=53
[25]K. Dhambri, H. A. Sahraoui, and P. Poulin, “Visual
detection of design anomalies.” in CSMR . IEEE,
2008, pp. 279–283. [Online]. Available:
http://dblp.uni-trier.de/db/conf/csmr/csmr2008.
html#DhambriSP08
[26]C. Catal and B. Diri, “Software defect prediction using
artiﬁcial immune recognition system,” in SE’07:
Proceedings of the 25th conference on IASTED
International Multi-Conference . Anaheim, CA, USA:
ACTA Press, 2007, pp. 285–290.
[27]S. Rawat and A. Saxena, “Danger theory based syn
ﬂood attack detection in autonomic network,” in SIN
’09: Proceedings of the 2nd international conference
on Security of information and networks . New York,
NY, USA: ACM, 2009, pp. 213–218.
[28]W. Pang and G. M. Coghill, “Modiﬁed clonal selection
algorithm for learning qualitative compartmental
models of metabolic systems,” in GECCO ’07:
Proceedings of the 2007 GECCO conference
companion on Genetic and evolutionary computation .
New York, NY, USA: ACM, 2007, pp. 2887–2894.
122