 ReLink: Recovering Links between Bugs and ChangesRongxin Wu†, Hongyu Zhang†, Sunghun Kim§ a n d  S.C. Cheung§ †School of Software, Tsinghua University Beijing 100084, China wrx09@mails.tsinghua.edu.cn, hongyu@tsinghua.edu.cn §Department of Computer Science and Engineering The Hong Kong University of Science and Technology, Hong Kong, China {hunkim, scc}@cse.ust.hk   ABSTRACT Software defect information, including links between bugs and committed changes, p l a y s  a n  i m p o r t a n t  r o l e  i n  s o f t w a r e  maintenance such as measuring quality a n d  p r e d i c t i n g  d e f e c t s .  Usually, the links are automatically mined from change logs and bug reports using heuristics s u c h  a s s e a r c h i n g  f o r  s p e c i f i c  keywords and bug IDs in change logs. However, the accuracy of these heuristics depends on the quality of change logs. Bird et al. found that there are many missing links due to the absence of bug references in change logs. They also found that the missing links lead to biased defect information, and it affects defect prediction performance.  We manually inspected the explicit links, which have explicit bug IDs in change logs and observed t h a t  the links exhibit certain features. Based on our observation, we developed a n  a u t o m a t i c  link recovery algorithm, ReLink, which automatically learns criteria of features f r o m  e x p l i c i t  l i n k s t o  r e c o v e r  missing links. We applied ReLink to three open source projects. ReLink reliably identified links with 89% precision and 78% recall on average, while the traditional heuristics alone achieve 9 1 %  p r e c i s i o n  a n d  64% recall. We also evaluated t h e  i m p a c t  o f  r e c o v e r e d  l i n k s  o n  software maintainability measurement and defect prediction, and found t h e  results of ReLink yields significantly better accuracy than those of traditional heuristics.  Categories and Subject Descriptors D.2.7 [Software Engineering]: Distribution, Maintenance, a n d  Enhancement–Restructuring, reverse engineering, and reengineering, D.2.8 [Software Engineering]: Metrics – Product metrics General Terms Bias, Measurement, Experimentation Keywords Mining software repository, missing links, data quality, bugs, changes. 1. INTRODUCTION Software defect information, including links between bug reports in bug tracking system and committed changes i n  s o u r c e  c o d e  repository, is the key information for software maintenance such as measuring software quality and predicting defects. It is possible to understand software maintenance efforts based on quality metrics derived from the links between bugs and changes, such as the percentage of buggy files [22, 33]. Defect information is also used to train models for defect prediction [17, 18, 33, 35, 36]. To collect links between bugs and changes a u t o m a t i c a l l y ,  m a n y  researchers mine bug reports in bug tracking systems and change logs in version archives. Heuristics traditionally used include searching for keywords (such as "Fixed" or "Bug") and bug IDs (such as “#42233”) [5, 21, 27, 28, 33, 34] in change logs. Recent studies r e v e a l e d  t h a t  t h e s e  h e u r i s t i c s  likely yield biased defect data, since they primarily rely on the comments in change logs [ 6 ,  7 ,  2 5 ]. Developers often maintain high quality change logs, but it is possible that they omit bug references in change logs. For example, when a developer fixes a bug in a revision, she may not document the fixed bug ID in the change log. Then, traditional heuristics miss t h e  l i n k s  b e t w e e n  t h e  b u g  a n d  t h e  change. As a result, defect information collected by traditional heuristics includes bias, especially lots of false negatives – m i s s i n g  l i n k s .  Bird and Bachmann et al. confirmed this problem and reported that 54% of fixed b u g s  i n  t h e  b u g  d a t a b a s e  a r e  n o t  l i n k e d  t o  change logs [6, 7]. Unfortunately, these biased defect data affect software quality measurement and defect prediction performance. Bird et al. found that the BugCache algorithm [17] is sensitive to the biased data [7]. Kim et al. found that the change classification algorithm [16] is also sensitive to the biased data [18] w h e n  t h e  n u m b e r  o f  instances is small. It is desirable to collect more accurate defect information by recovering the missing links.  To explore possibilities of recovering the missing links automatically, w e  c o n d u c ted a  q u a l i t a t i v e  s t u d y  t o  i d e n t i f y  characteristics of explicit links based on the bug IDs in change logs. We found that the links between bugs and changes exhibit certain features. For example, the bug-fixing time is close to the change-commit time, the change logs and the bug reports share textual similarity, and the developers r e s p o n s i b l e  f o r  a b u g  are typically the committers of the bug-fixing change.  Based on these f i ndi ngs ,  we propose an automatic link recovery algorithm, ReLink. Relink automatically learns satisfaction criteria of features from explicit links, and by applying the learned Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.  ESEC/FSE’11, September 5–9, 2011, Szeged, Hungary.  Copyright 2011 ACM 978-1-4503-0443-6/11/09...$10.00.   criteria i t  c h e c k s  i f  the features of an unknown link satisfy t h e  criteria. If the unknown link satisfies all the feature criteria, it is considered a valid link.    We have applied ReLink to three o p e n  s o u r c e  p r o j e c t s  ( Z X i n g ,  OpenIntents, and Apache) and two simulation studies (on Apache and Eclipse MAT). We have evaluated the recovered links by comparing them to the ground truth links, which are m a n u a l l y  recovered and verified links. Our experimental results are promising: on average, for the three open source projects, ReLink recovered l i n k s  w i t h  78% recall and 89% precision, while the traditional heuristics can only identify links with 64% recall and 91% precision.  We have also evaluated the practical impact of defect information obtained b y  ReLink. We measured t h e  e f f e c t s  o f  R e L i n k  o n  several software maintainability metrics. We also built d e f e c t  prediction models using defect information with/without ReLink. Our experimental results indicate ReLink has nontrivial positive impacts on the maintenance studies. In summary, our paper makes the following contributions: • We propose ReLink, an automatic link recovery algorithm. We also report our experimental evaluation of ReLink. • We report an empirical study o n  m e a s u r i n g  t h e  i m p a c t  o f  recovered links on maintenance studies, especially on software defect prediction. The remainder of this paper is organized as follows. In Section 2, we describe the traditional heuristics for mining links and their main challenges. In Section 3, we present ReLink, the p r o p o s e d  approach to mining links. We evaluate the performance of ReLink in Section 4 and its practical effects on software maintenance studies in Section 5. Sections 6 and 7 discuss the threats to validity and the related work respectively. We conclude the paper in Section 8. 2. MINING LINKS: TRADITIONAL HEURISTICS This section reviews the use of traditional heuristics to mine the links between bugs and changes. In addition, we evaluate the quality of links mined with traditional heuristics, and discuss the involved challenges.  2.1 Traditional Heuristics During software maintenance, it is common that changes and bugs are recorded in version control systems (such as CVS and SVN) and bug tracking systems ( s uch as BugZilla), r e s p e c t i v e l y .  Developers often maintain change logs describing what they have changed and what bugs they have dealt with.  Traditional heuristics to identify bug-fixing changes r e l y  o n  t h e  premise that developers leave hints or links about bug fixes in the change logs. They look for specific keywords such as ‘fixed’, ‘bug’, and for information linking to bugs such as bug ID references in change logs. These traditional heuristics are widely used to mark bug fixes, identify bug-introducing changes, and build defect prediction models [5, 14, 16, 21, 27, 28, 33, 34].      However, the results of traditional heuristics largely rely on the change log quality. Recently, Bird et al. [7] noticed that, only a fraction of bug fixes are labeled in change logs explicitly, and this causes systematic biases in finding the links. The biases can significantly reduce the effectiveness of the follow-up studies. To overcome this challenge, Bird et al. [8] developed a manual link recovery tool, LINKSTER. The manually recovered links might be much more accurate. However, the practice d o e s  n o t  s c a l e because it requires significant manual effort. 2.2 Evaluation of Traditional Heuristics To evaluate traditional heuristics and measure the number of missing links, we performed a replication study of Bird et al.’s [7] on two Android open source projects. In t h i s  s t u d y ,  w e  a d o p ted the traditional heuristics proposed by Bachmann and Bernstein [5]: 1) Scan through the change logs for bug IDs in a given format (e.g. “issue 681”, “bug 239” and so on). 2) Exclude all false-positive bug numbers (e.g.  “r420”, “2009-05-07 10:47:39 -0400” and so on). 3) Check if there are other potential bug number formats or false positive number formats, add the new formats and scan the logs iteratively. 4) Check if potential bug numbers exist i n  t h e  b u g-tracking database with their status marked as fixed. Based on these heuristics we mined the links between change logs and bug reports. Then, we measured the number of fixed bugs successfully linked to change logs, as well as the number of links identified.  The two projects, ZXing1 a n d  O p e n I ntents2, which we studied are highly active Android projects. ZXing is a barcode image-processing l i b r a r y  f o r  A n d r o i d-based p h o n e s, a n d  OpenIntents is an open intents library. Table 1 shows the results of the replication study. For ZXing, only 40.7% (55 out of 135) of the fixed bugs were found linked to change logs and 48.2% of the links were identified. Similarly, only 54 out of 101 bugs were linked to change logs f o r  OpenIntents a n d  6 7 . 4% links were identified. Our results are consistent with Bird et al.’s results for Apache project [6]. They showed that 256 out of 559 (46%) bugs were linked a n d  there were many missing links. In the next section, we briefly discuss the reasons of having missing links and the challenges of using traditional heuristics. Table 1: Linking results of the traditional heuristics Project Revi- sions # Fixed Bugs ĊLinked Bugs # Links % Links Identified ZXing 1-1694 135 40.7% 143 48.2% OpenIntents 1-2890 101 53.5% 129 67.4% 2.3 Challenges As shown in Table 1, there is a large number of bugs that cannot be linked to the committed changes by traditional heuristics. However, the results do not necessarily mean these bugs are not associated with any committed changes. We manually investigated the unlinked bugs and i d e n t i f i e d  t h e  f o l l o w i n g  reasons that cause missing links: Missing bug reference in change logs  Since leaving explicit bug reference in change logs is optional to developers, it is possible that developers do not write related bug IDs after they have fixed bugs. In this case, traditional heuristics fail to identify links between change logs and bugs. For example,                                                                     1 http://code.google.com/p/zxing/ 2 http://www.openintents.org/  consider the change log3 a n d  t h e  bug report4 i n  F i g u r e  1, which are taken from the ZXing project. There are no bug ID references in the change log t h e r e f o r e traditional heuristics cannot i d e n t i f y  which bug the revision 148 has fixed.  However, our manual examination found that this change log can be actually linked to a bug report: we searched through all bug reports and found that the bug report #18 contained s i m i l a r  problem descriptions as the change log. Also, we noticed that there was a bug-fixing comment made for bug #18 on Jan 22, 2008, by the developer “srowen@gmail.com”. The revision #148 was also committed on Jan 22, 2008 by the author “srowen”. Therefore, it is likely that the revision #148 is linked to bug #18 (we also checked the source code and c o n f i r med t h i s  l i n k). The commonalities give an indicator of the link between the bug and the change. They provide clues to recover the missing link.  Change Log (Revision: 148; Author: srowen; Date: Jan 22, 2008) Name of midlet is "ZXingMIDlet", not "ZXingMidlet"! -------------------------------------------------------------------------------- Bug Report (Issue 18; Status: Fixed): Reported by herf...@yahoo.com, Jan 11, 2008 Issue 18: does not work on nokia 5300   Comment 1 by project member srowen@gmail.com, Jan 11, 2008 Which version, regular or basic? … …  Comment 3 by herf...@yahoo.com, Jan 12, 2008 I tested both of them (regular and basic) but in both case it just make an exception… … Comment 5 by project member srowen@gmail.com, Jan 22, 2008 …The class is called "ZXingMIDlet" but the exception mentions "ZXingMidlet" (note different capitalization). It looks like the manifest file I wrote gets this wrong. …  Comment 6 by project member srowen@gmail.com, Jan 22, 2008 I think I have fixed this particular problem by correcting… Can you try the most recent version from…  Figure 1: An example of missing links Irregular bug reference formats  The traditional heuristics, such as Bachmann and Bernstein’s approach [5], search for patterns in change logs using regular expressions (e.g. “issue 681”, “bug 239” and so on). However, our manual examination found t h a t  d e v e l o p e r s  u s ed m a n y  d i f f e r e n t  ways to give bug references in change logs, such as “solve problem 681”, “Fixed for #239”, “see #149”, “for Issue 143” (“Issue 143” is strikethrough a n d  m a r k e d-up in HTML), etc. Developers may also occasionally make typos such as “fic 239” [23]. The format may e v e n  v a r y  a c r o s s  c h a n g e  l o g s  w i t h i n  t h e  same project, as different open source contributors may have their own preferences. It is not easy to develop a single, generic tool to support all possible bug reference formats. The above analysis motivates us to develop a new link recovery algorithm. In our approach, we identify features of links between bugs and changes, and apply them to recover the missing links.                                                                     3 http://code.google.com/p/zxing/source/detail?r=148 4 h t t p : / / c o d e . g o o g l e . c o m / p / z x i n g / i s s u e s / d e t a i l ? i d = 1 8 3.  RELINK: RECOVERING LINKS BETWEEN BUGS AND CHANGES  In this section, we present ReLink, a new approach to recovering the links between bugs and changes.  3.1 Features of Links Our approach is based on the identification of features of the links between bugs and changes. First, we identify explicit links, which are links that can be discovered by traditional heuristics. We then analyze the features of these links. Through this analysis, we have identified the following features, which can be later used to recover the missing links: Time Interval: This is the interval between the time (tf) when a bug was fixed as given by its bug report and the time (tc) when the corresponding bug fix was committed at the code repository. The interval (tf – tc) is a useful feature because it leverages the knowledge that a bug should be fixed after its creation and before its closure. After developers have committed the code change for a bug fix (at time tc), they are obliged to update the bug report (at time tf) that the bug has been fixed. Therefore, tf should be greater than but close to tc f o r  a  l i n k e d  b u g  a n d  i t s  c o m m i t t e d  c o d e  change.  Bug Owner and Change Committer: F o r  linked bugs and changes, there exists a mapping between the bug owner and the change committer. Ideally, the committer who makes the bug-fixing changes should be the person who is responsible for fixing the bug. It is also possible that the change committer and the bug owner a r e  d i f f e r e n t  p ersons, b u t  the mapping b e t w e e n  them c o u l d  b e  i d e n t i f i e d  b y  m i n i n g  t h e  s o f t w a r e  r e p o s i t o r i es (which will be discussed in Section 3.2.2). Text Similarity: This is the textual similarity between bug reports and change logs. For linked bugs and changes, the natural language descriptions in the bug report are often similar to those in the change logs, as they may refer to the same issue and share similar keywords.  3.2 Mining Features of Links 3.2.1 The Interval between the Bug-fixing time and the Change-Commit Time Although the interval between bug-fixing time and the commit time is a useful feature for mining links, determining the length of such an interval is a nontrivial task. We observed from our empirical studies that developers do not always change the status of bugs to “Fixed” in the bug tracking system immediately after they have committed the bug-fixing c h a n g e s .  F o r  e x a m p l e ,  w e  investigated the explicit links of t h e  ZXing project mined by traditional heuristics, and observed when developers change the bugs’ status to “Fixed” after the bug-fixing change is committed. We found that for 20% of explicit links, their bug-fixing time and change-commit time differ by more than one day. The bug-fixing time and the change-commit time could b e  f a r  a p a r t .  O u r  f i r s t  research challenge is therefore to determine the actual bug-fixing time from each bug report.  With further investigations, we found that most bug comments are related to bug-fixing activities, as developers often post comments to the bug tracking system to report a bug fix and notify the bug reporter. Therefore, we could determine the bug-fixing time according to the time of comments in bug reports. For example, consider the change log and the bug report in Figure 1. The  revision #148 was committed on Jan 22, 2008. There was also a bug-fixing comment made for bug #18 on Jan 22, 2008, by the change committer (“srowen”).  We also empirically verified the time interval feature using the explicit links. For ZXing and OpenIntents, on average, each bug received 2-3 comments. For more than 93% of the links, the intervals between bug-comment time and the change-commit time were l e s s  t h a n 2 4  h o u r s .  F o r  m o r e  t h a n  9 6 %  o f  t h e  l i n k s ,  t h e intervals were less than one week. The results indicate that in most cases, the change-commit time is close to a b u g-comment time.  3.2.2 Mapping between Bug Owners and Change Committers When bugs are assigned to a certain developer to fix, the developer information Pb is r e c o r d e d  b y  b u g  t r a c k i n g  s y s t e m s .  When bug-fixing c h a n g e s  a r e  c o m m i t t e d ,  t h e  c o m m i t t e r  information Pc i s  r e c o r d e d  b y  v e r s i o n  c o n t r o l  s y s t e m s .  I n  o u r  empirical studies, we compared the change committers and the bug report owners for linked bugs and changes. We found that they may not have the same names. This is because developers may use different login names in different scenarios. It is also possible that there is an appointed person in the team to confirm bug fixes and change bug status. Table 2 gives some examples for the identified mappings between committers and bug owners. Table 2: Examples of the mapping between bug owners and change committers Bug  Owner Pb Change  Committer Pc Project dswitkin@gmail.com dswitkin ZXing dswitkin@google.com dswitkin@google.com ZXing srowen@gmail.com srowen ZXing peli0101@googlemail.com peli0101 Openintents Will Rowe wrowe Apache Erik Abele erikabele Apache To identify the mappings between bug owners and change committers, we again examined the comments in bug reports. Our empirical studies found that developers often actively discuss bug-related issues and announce bug fixes via the bug tracking system. Therefore, it is likely that one of t h e  commenters is the bug owner who is responsible for bug fixing. For example, in the ZXing project, for 9 9 %  o f  l i n k s  t h a t  a r e  f o u n d  b y  t r a d i t i o n a l  heuristics, the developers who committed bug-fixing changes posted bug comments in the bug tracking system. In t h e  OpenIntents project, all developers who fixed bugs posted comments in the bug reports. 3.2.3 The Similarity between Bug Reports and Change Logs Information Retrieval (IR) technology is commonly used to process textual documents in natural languages. In this project, we treat bug reports and change logs as t e x t s a n d  c o m p a r e  t h e i r  similarities. It is expected that for linked bugs and changes, bug reports and change logs exhibit certain similarity. To compute the similarity between bug reports and change logs, we first extract text features. Our approach adopts the Vector Space Model (VSM), a widely used model in IR technology [9]. In VSM, a document is represented as an n dimension vector <w1, w2, w3,…wn>, where n is the number of distinct terms and wi (1≤ wi ≤ n) represents the weight of a unique term.  One of the most important issues in text processing is to select appropriate terms to represent the entire documents. In our case, the number of terms in bug reports and change logs could be large. To select the representative terms, we use the following steps to help reduce dimensions: 1) Remove stop words. Stop words are the words that have no strong meaning, such as “a”, “an”, “the” and so on.  2) Use one term to represent all other terms that have the same stemmer. For example, the tokens “fixing”, “fixes”, and “fixed” all share the same root “fix”, thus we use “fix” to represent the others.  3) Use one term to represent all synonymous words. We apply the tool WordNet [12] to facilitate the selection of the synonymous words. For example, according to the definitions in the WordNet dictionary, the words “additional” and “extra” are synonymous words, therefore we can replace “additional” with “extra”.  After selecting the terms in bug reports and change logs, we then calculate the weight for each term. In our approach, we use Term Frequency-Inverse Document Frequency (TFIDF) metric [9] to calculate the weight. The basic idea of TFIDF is that the weight wi of a term in a document increases with its occurrence frequency in the specific document and decreases with its occurrence frequency in other documents. Formally: wi = tfi × idfi   (1) where tfi represents the occurrence frequency of the term ti in the specific document, and idfi r e p r e s e n t s  t h e  i n v e r s e  d o c u m e n t  frequency, defined as:         (2) where |D| r e p r e s e n t s  t h e  number of documents a n d |{d: ti ∈ d } | represents the number of documents that contain the i-th index term. We use the above formulas to extract features from all bug reports and change logs, and to compute the n dimension vector.  After obtaining the vector space model using TFIDF, we measure the similarity between a bug report and a change log using the Cosine similarity measure [9]: 
€ Sim=w1iw2ii=1n∑2w1ii=1n∑×2w2ii=1n∑   (3) It is expected that the linked bug reports and change logs share certain t e x t s i m i l a r i t y ,  t h u s  t h e  l a r g e r  t h e  C o s i n e  s i m i l a r i t y  measure, the more a link is likely to be valid. 3.3 Learning Criteria of Features In this section, we describe how we determine the thresholds o f  features so that these features can characterize most of the real links. Such thresholds c a n  b e  t r e a t e d  a s  c r i t e r i a  f o r  d e t e r m i n i n g  links – if the features of an unknown link satisfy the criteria, the link is likely to be valid. Otherwise it is irrelevant and should be removed.  Determining the criteria of features is nontrivial. For example, a lower threshold enables us to relate more fixed bugs and committed changes, but the links identified contain more false  positives. On the other hand, a higher threshold can reduce false positives, but less missing-links could be identified.  To determine the criteria of features, we learn from the explicit links that can be identified through traditional heuristics (Le). For the time interval feature and the text similarity feature, their values vary independently. When these two features are applied to Le, d i f f e r e n t  v a l u e s  c a n  r e s u l t  i n  s e l e c t i n g d i f f e r e n t  s e t s  o f  l i n k s  and lead to different F-measures. We propose a  s e a r c h-based algorithm, which exhaustively s e a r c h e s for the optimal combination of these two values so that the maximum F-measure can be achieved. Figure 2 shows our algorithm for determining the threshold values of these two features.  DetermineThresholds (Le: links between bugs and changes identified by the traditional heuristics) 1 Assign the time interval T with a small initial value T0  2 Assign the text similarity threshold S with a small initial       v a l u e  S0  3 Select links in Le that satisfy T and S, and compute         F-measure 4 Increase S by a small step s1 5 Repeat steps 3-4 until the maximum threshold Sm is reached 6 Increase T by a small step s2 7 Repeat steps 3-6 until T reaches the maximum threshold Tm  8 Choose the threshold values Tt and St that achieve the best        F-measure (if ties exist, choose the first occurrence of        Tt and St) 9 Return Tt and St Figure 2: Determining the thresholds of features  In our experimentation, we specify T0 =1, s2=1, a n d  Tm =  3 0 ,  which means that we try the time interval from 1 day to 30 days. We specify S0 =0, s1=0.01, and Sm = 1, which means that we try from text similarity 0 to 1, with steps of 0 . 01. In total, our algorithm executes at most 30*100 times, in which we search for the combination of S and T that maximizes F-measure. Based on the identified optimal values Tt and St, a criterion is formed: a link is considered irrelevant if its time interval and text similarity values are above the thresholds. DetermineMappings (Le: links between bugs and changes identified by the traditional heuristics) 1 Initialize the set of mappings M=Φ 2 For each link l in Le  3 For each mapping m between l’s change committer  a n d  b u g  c o m m e n t e r 4  If (m is not in M) 5   Add m to M 6     EndIf 7  EndFor 8 EndFor 9 Return M Figure 3: Determining the mappings between bug owners and change committer We also learn t h e  m a p p i n g s  b e t w e e n  b u g  o w n e r s  a n d  c h a n g e  committers from the explicit links identified by the traditional heuristics. We extract all relationships between a bug commenter and a change committer and form the mapping set. The algorithm is described in Figure 3. The criterion for this feature is: a link is considered irrelevant if none of its bug commenters is mapped to its change committer.   3.4 Recovering the Missing Links A large and evolving project tends to contain a lot of fixed bugs and committed changes. T h i s  r e s u l t s  i n  a  l a r g e  n u m b e r o f  potential links between bugs and changes. Furthermore, there are also many types of r e l a t i o n s h i ps b e t w e e n  b u g s  a n d  c h a n g e s  ( a s  illustrated in Figure 4):  • one to one: one bug could be fixed by one change;  • many to many: one bug could be fixed by multiple changes, and one change could fix multiple bugs • no relationship: a change could be a non-bug fixing change, thus it does not link to any bug.  
 Figure 4: The relationships between bugs and changes 1 Store all possible links between bugs and links in L 2 Initialize the set Lr =Φ 3 Mine links Le between bugs and changes using       t h e  t r a d i t i o n a l  h e u r i s t i c s   4 DetermineThresholds (Le) 5 DetermineMappings(Le) 6 For each link l in (L – Le) 7   I f  t h e r e  i s  m a p p i n g  b e t w e e n  l’s bug commenter and l’s          c h a n g e committer 8     I f  a n y  o f  l’s bug comment time is within the time              i n t e r v a l  t h r e s h o l d  Tt 9        I f  t h e  t e x t  s i m i l a r i t y  b e t w e e n  l’s bug report and              c h a n g e  l o g  i s  w i t h i n  t h r e s h o l d  St 10         a d d l to Lr 11    EndIf   12       EndIf 13     EndIf  14 EndFor 15 Return Lr + Le Figure 5: The ReLink algorithm  To automatically identify the links between bugs and changes, we propose a link recovery approach, ReLink. ReLink is based on the identified features. The algorithm of ReLink is described in Figure 
 5: we first determine the satisfaction criteria o f  f e a t u r e s  b y  applying the algorithms described in Figures 2 and 3 (lines 3-5). The criteria include the time interval between the bug-fixing time and the change-commit time, the text similarity threshold, and the mappings between the bug owners and the change committers. ReLink automatically learns the criteria from the explicit links that can be identified by traditional heuristics. For each unknown link that cannot be identified by traditional heuristics (line 6), ReLink checks if the link satisfies all criteria ( l i n e s  7-9). If it satisfies, we consider it a valid link (line 10). After checking all the unknown links, ReLink returns t h e  r e c o v e r e d  s e t  o f  l i n k s  between bugs and changes ( l i n e  15). The overall process of ReLink is also illustrated in Figure 6.  
  Figure 6: The overall process of ReLink 4. EVALUATION This section presents evaluation results of ReLink. 4.1 Subject Projects and Experimental Setup We investigated two p r o j e c t s  a s  s h o w n  i n  T a b l e  1 .  These two projects (ZXing and OpenIntents) are highly active Android projects hosted by Google Code. We collected their change logs from the SVN repository and bug reports from the Google Code issue tracking system. To obtain the ground truth (the “golden set”) of the links, we manually read change logs, bug reports, and the corresponding code changes to establish links between changes and bug reports. The manual annotation was performed by two people: one identifying the links and the other verifying the results. We also experimented with the Apache dataset that was provided by Bachmann and Bird et al. [6]. In the Apache dataset, the links between defects and bugs were manually annotated by an Apache core developer (Justin Erenkrantz) using the LINKSTER tool [6]. We treated this dataset as a “golden set” and used it to evaluate the performance of ReLink. To further evaluate the performance of ReLink when the bug IDs are absent from change logs, we conducted a simulation study as follows: we first collect a high quality dataset with most of the links identified, and then intentionally remove bug IDs in 50% of the change logs. Traditional heuristics fail to identify the links associated with these changes due to the absence of the bug IDs. We evaluate the performance of ReLink on recovering these missing links. We choose the Apache project and the Eclipse Memory Analyzer (MAT)5 p r o j e c t  i n  t h i s  s i m u l a t i o n  e x p e r i m e n t  as they have a high percentage of linked bugs. To overcome the randomness introduced by the 50% sampling, we perform the simulation experiment 10 times and compute the average results.  To facilitate the experiments, we have developed a tool f o r  ReLink. The tool automatically collects information from source code repository and bug tracking system, builds the links between bugs and changes, and outputs the identified links. It is developed in Java, and runs on Windows and Linux. It consists of more than 10K lines of code.  4.2 Evaluation Metrics Our experiments can lead to four kinds of results: a link we identify is a true link (TP), a link we identify is not a true link (FP), a link we miss is a true link (FN), and a link we miss is not a true link (TN). We use Recall, Precision, a n d  F-measure a s  evaluation metrics. The definitions of these metrics are as follows:  Precision = € TPTP+FP This metric indicates how accurate the experiment result is.  Recall  = € TPTP+FN This metric indicates the coverage of the experiment result.   F-measure = € 2×Precision×RecallPrecision+Recall This metric takes the precision and the recall into consideration. It is a combined metric. In this study, we adopt the F1 metric, which weights precision and recall equally [32]. 4.3 Evaluation Results 4.3.1 Comparisons to the Golden Set Table 3 shows the experimental results for the three projects we investigated.  The ZXing project contains 135 fixed bugs. For the traditional approach, we adopted the heuristics proposed by Bachmann and Bernstein [5] a n d i d e n t i f i e d  l i n k s  f o r  5 7  ( 4 2 . 2 % )  f i x e d  b u g s  without any false positives. Our approach, ReLink, identified links for 95 (70.4%) fixed bugs with 10% false positives. In ZXing, there are 143 links between bugs and changes. ReLink can successfully identify 107 links among them, leading to a Recall of 74.8%, which is much higher than that achieved by the traditional heuristics. The precision of ReLink is 0.9, which is marginally lower t h a n  that of the traditional approach. In terms of the F-measure, ReLink a l s o  o u t p e r f o r m s  t h e  t r a d i t i o n a l  a p p r o a c h  (0.820 vs. 0.651).  For OpenIntents project, the percentage of linked bugs found by ReLink is 4% more than that of the traditional heuristics. There are 129 l i n k s  b e t w e e n  b u g s  a n d  c h a n g e s  i n  t h e  p r oject. ReLink successfully recovered 9 5  o f  t h e m .  C o m p a r e d  t o  t h e  t r a d i t i o n a l  heuristics, the Recall is improved by almost 10%. Both approaches can achieve high precisions, with ReLink resulting in a better overall F-measure.                                                                      5 http://www.eclipse.org/mat/  Table 3: Evaluation results Recovery Links Project Period Revisions  #Fixed Bugs Approach % Linked Bugs Precision Recall  F-measure Traditional 42.2% 1.0 č69/69Ď 0.482 (69/143) 0.651 ZXing 11/2007- 12/2010 1-1694 135  ReLink 70.4% 0.90 (107/118) 0.748 (107/143) 0.820 Traditional 69.3% 1.0 (87/87) 0.674 (87/129) 0.805  OpenIntents 12/2007- 12/2010 1-2890  101 ReLink 73.3% 1.0 (95/95) 0.731 (95/129) 0.847 Traditional 77.1% 0.746 (791/1060) 0.764 (791/1035) 0.755 Apache 11/2004- 4/2008 76294- 899841 686 ReLink 89.8% 0.747 (904/1210) 0.873 (904/1035) 0.805 Traditional 38.2% 0.741 0.375 0.498 Apache Simulation 11/2004- 4/2008 76294- 899841 686 ReLink 53.0% 0.682 0.523 0.592 Traditional 49.1% 1 0.418 0.582 Eclipse MAT Simulation 4/2008- 2/2011 10-1070 108 ReLink 96.3% 0.858 0.623 0.718 For Apache project, the golden set identified by the Apache developer contains 1035 links between bugs and changes. ReLink successfully recovered 9 0 4  l i n k s ,  w h i l e  t h e  t r a d i t i o n a l  h e u r i s t i c s  recovered only 791 links. ReLink improves the Precision as well, leading to a better F-measure (0.805).  The experimental results confirm that ReLink can achieve better overall performance than the traditional heuristics. 4.3.2 Simulation Study For the Apache simulation study, traditional heuristics lead to an average recall of 0.375 due to the removal of bug IDs from 50% of change logs. ReLink can achieve a much higher recall of 0.523. This is because ReLink uses feature c r i t e r i a  to identify links. Overall, the traditional heuristics lead to F-measure 0.498, while ReLink achieves F-measure 0.592.  For the Eclipse MAT simulation study, traditional heuristics lead to a low recall of 0.418 due to the removal of bug IDs from 50% of change logs. ReLink can achieve a much higher recall of 0.623. Overall, the traditional heuristics lead to F-measure 0.582, while ReLink achieves F-measure 0.718, a significant improvement.  For the simulation studies, Table 3 only shows the average results of 10 simulations. We also performed the paired-sample t-test to check if the F-measures with ReLink were statistically significantly better than the traditional heuristics. The t-test results confirmed that ReLink can lead to better F-measures t h a n  t h e  traditional heuristics, at significance l e v e l  0 . 0 1 .  All simulation results confirm that ReLink can r e c o v e r  l i n k s  w i t h  r e a s o n a b l e  accuracy. 4.4 Discussion Traditional heuristics identify l i n k s  f o r  a  c e r t a i n  p e r c e n t a g e  o f  bugs, but they fail to identify links for the bugs that have no bug ID references in committed changes. As our experimental results show, ReLink is more effective to recover the links between bugs and changes than the traditional heuristics. ReLink can significantly reduce false negatives (missing links) – T a ble 3 shows that the recall values of ReLink are 6%-26% higher than those of traditional heuristics. The higher recall values are achieved because ReLink can identify links even the bug IDs are missing from the change logs, or the bug reference formats are irregular. Unlike traditional heuristics, ReLink does not use regular expressions to search for links. Instead, it identifies features of links and checks the satisfiability of unknown links. The performance of ReLink (measured in terms of F-measure) is better than that of traditional methods for all studied projects. Still ReLink may introduce false negatives. For example, Table 3 shows that ReLink missed 2 5 . 2 %  o f  l i n k s  i n  Z X i n g  a n d  2 6 . 9 %  links in OpenIntents. We found two major reasons for false negatives. One reason is that there are no similar keywords between bug reports and change logs for some links, causing very low text similarity. Thus ReLink discards these links as irrelevant. The other reason is that some bugs have long intervals between the change-commit time and the bug-fixing time. As a result, these bugs fail to satisfy the time interval threshold. Figure 7 shows such an example in OpenIntents. The change for fixing the bug 217 was committed on Aug 22, 2009. The status of the bug 217 was not updated to “Fixed” until Sep 22, 2009. Note that, all these missing links are not identifiable by traditional heuristics either. Change Log (Revision: 2293; Author: rmceoin; Date: Aug 22, 2009) OI Safe: large patch, added S e a r c h  a c t i v t y ,  c o m p l e t e l y  r e d i d  autolock, added preference 'Lock on screen lock' with default of true.… -------------------------------------------------------------------------------- Bug Report (Issue 217; Status: fixed): What steps will reproduce the problem? 1: Unlock OI Safe… 2: Leave screen on PassEdit … After step 6, screen should auto-lock.  Comment 1 by project member rmceoin@gmail.com,Mar 10,2009 …Also fails with PassView.   Comment 2 by project member rmceoin@gmail.com,Sep 22,2009 …Status: Fixed Figure 7: An example of false negative cases  Change Log (Revision: 99398; Author: trawick; Date: Apr 17, 2003) merge this fix into 2.0.46-dev:\n\n  …   PR 18649  [Justin Erenkrantz, Jeff Trawick] ------------------------------------------------------------------------------- Bug Report (bug 18649; Status: closed): …enable-layout broken in 2.0.45. In 2.0.45, --enable-layout no longer works… Figure 8: An example of false positive cases  Bachmann e t  a l .  [6] also discovered that, some bugs in Apache projects are actually “bugs incognito”, i.e., these bugs were only discussed in developer mailing list and were not stored/tracked using bug track systems. Like traditional heuristics, ReLink cannot discover links for this type of bugs either. Mining mailing lists and recovering more links remain as future work. ReLink may also introduce false positives. We manually examined these false positives in Table 3. We found that the false positives of ReLink in the Apache project overlapped largely with those identified by traditional heuristics. These links were incorrectly identified due to reasons such as merge of changes. Figure 8 s h o w s  a n  e x a m p l e  o f  f a l s e  p o s i t i v e  c a s e  i n  A p a c h e  introduced by the merge of changes. Revision 99398 was actually a merge of the bug fix (PR 18649) found in version 2.0.45 into a new version (2.0.46). There are ways to reduce ReLink’s number of false positives. For example, we can modify the ReLink algorithm (before line 15 in Figure 5) to let it double-check the links Le mined by the traditional heuristics - if a link l in Le does not satisfy the feature criteria, ReLink considers l an irrelevant link and removes it. We have compared the performance of the revised ReLink (called ReLink-), ReLink and traditional heuristics for the Apache project. The results are shown in Figure 9. ReLink- c a n  a c h i e v e  b e t t e r  precision than the other two methods. However, the recall of ReLink- i s  l o w e r  b e c a u s e  o f  t h e  t r a d e o f f  b e t w e e n  r e c a l l  a n d  precision. Overall, ReLink-’s F-measure is better than that of the traditional heuristics, but slightly worse than ReLink. Investigating techniques that can further reduce the number of false positives and at the same time improve the recall value will be our important future work. 
 Figure 9: The performance of ReLink-  5. Practical Effects of ReLink We have shown that ReLink can identify the links between bugs and changes more accurately than the traditional heuristics. The immediate next question would be the practical effects of the results. In this section, we show the implications of our results on software maintenance s t u d i e s ,  e s p e c i a l l y  o n  m a i n t a i n a b i l i t y  measurement and defect prediction.  5.1 Implications of Results on Maintainability Measurement Many maintenance metrics have been p r o p o s e d t o  understand maintenance activities and measure software maintainability. For example:  • The percentage of bug-fixing changes [15, 16, 21]: changes can be classified into many categories such as corrective changes (for bug fixing), adaptive changes (e.g., for accommodating new features), and perfective changes (e.g., for refactoring). Knowing the percentage of bug-fixing changes can help understand maintenance efforts spent on bug-fixing activities.  • The percentage of buggy files [33]: this is the percentage of defective files (files containing at least one bug). It can help measure software quality. • Mean Time to Fix [22]: this is to measure the average time a team spends on f i xing a bug. It can help measure a maintenance team’s bug-fixing ability. T h e  t i m e  spent for fixing a bug is calculated as the interval between bug report open time and bug-fixing change commit time.  Table 4. The comparisons of measurement data  Project Linking approaches %Bug-fixing changes %Buggy files Mean Time to Fix(days) Golden 8.1% (138/1694) 29.6% (118/399) 7.5 Traditional 4.0% (67/1694) 14.8% (59/399) 10.2 ZXing ReLink 6.3% (107/1694) 20.8% (83/399) 7.3 Golden 4.2% 121/2890 4.9% (36/742) 25.1 Traditional 2.9% (83/2890) 2.6% (19/742) 21.2  OpenIntents ReLink 3.3% (94/2890) 4.0% (30/742) 25.1 Golden 2.3% (976/43167) 50.5% (98/194) 159.7 Traditional 1.7% (753/43167) 47.9% (93/194) 178.9 Apache ReLink 2.0% (866/43167) 52.1% (101/194) 153.9 These metrics can be derived from the links between bugs and changes. After identifying the links, we know which bugs/changes are linked and which are not. Therefore we can collect the corresponding data and compute the metrics. As the quality of links collected by ReLink is higher than that c o l l e c t e d  b y  t h e  traditional heuristics, we can obtain better measurement data for the above-mentioned metrics. Table 4 shows the measurement data we collected for the studied projects using the traditional heuristics and ReLink. We can see that the measures derived from links obtained by ReLink are closer to the actual ones, while the measures obtained by traditional heuristics contain larger discrepancies. For example, f o r  ZXing, the actual percentage of bug-fixing changes is 8.1%. The measures derived from ReLink 
 and traditional heuristics are 6.3% and 4.0% respectively. T h e  ZXing team’s actual mean time to fix value is 7.5 days per bug. The measures derived from ReLink and traditional heuristics are 7.3 and 10.2 respectively. Clearly ReLink leads to more accurate measurement, t h u s  i t h e l ps p r o j e c t  t e a ms better understand and plan their maintenance activities. 5.2 Defect Prediction with Relink  To measure the impact of ReLink on software defect prediction, we built commonly-used file level defect prediction models using defect data collected from traditional heuristics and ReLink, and measured the performance of the models to predict ground truth defects.  5.2.1 Data Collection We collected metric data and defect data for the studied projects, and built a classification model to predict the defect-proneness of the files. The metric data includes file-level static code complexity measures collected by the Understand for Java/Understand for C++ tool6, such as lines of code, cyclomatic complexity, average lines of comments, etc. The defect data contains buggy instances (files) collected through analyzing the identified links between bugs and changes.    The buggy and clean labels are assigned based on links between change logs and bugs. If a commit is linked to a bug, we assume the files in the commit are buggy. To label files, we use the links obtained from three different approaches: traditional heuristics, ReLink, and the ground truth. Table 5 summarizes the datasets used in this experiment. The datasets contain different percentages of buggy files. For example, the ZXing project contains 399 files, among which 29.6% of the files are buggy in the “Golden” (ground truth) dataset, 14.8% files are buggy in the “Traditional” dataset, and 20.8% files are buggy in the “ReLink” dataset. Table 5. Defect Prediction Dataset Subject Version # of files # of metrics Linking approaches % of buggy Traditional 14.8% ReLink 20.8% ZXing  1.6 399 41 Golden 29.6% Traditional 10.7% ReLink 28.6% Open- Intents  Revision 1088~2073 56  41 Golden 39.3% Traditional 57.2% ReLink 46.9% Apache  2.0 194  60 Golden 50.5% 5.2.2 Prediction and Evaluation Models After collecting the metric d a t a  and buggy labels, we built a classification model using a popular machine-learning algorithm, Decision Tree (J48 in the Weka implementation [31, 32]). We adopted the 10-fold cross validation technique to train and evaluate the model. When the 9-fold is used as a training set, we used the labels from three datasets: Traditional, ReLink, and Golden. However, for the 1-fold (test set), we used the labels obtained from the Golden set, which are manually recovered                                                                     6 h t t p : / / w w w . s c i t o o l s . c o m / ground truth links ( F i g u r e  10). Note that the ultimate goal of defect prediction is to identify ground truth (actual) defects.  Since the performance may vary based on instances in each fold, we ran this 1 0-fold cross validation model 100 times (100 10-fold), and computed the average performance. To measure the prediction model’s p e r f o r m a n c e ,  w e  u s ed t h e  s t a n d a r d  m e a s u r e s  such as Precision, Recall, and F-measure [32].  5.2.3 Results Table 6 presents the prediction results using three different approaches. The prediction performance of using ReLink is much better than that of using traditional h e u r i s t i c s. For example, the F-measure with Traditional is 0.257 f o r  OpenIntents, while the F-measure with ReLink is 0.706, a s i g n i f i c a n t  i m p r o v ement. Similarly, for ZXing, the F-measure with ReLink is 0.325, while F-measure w i t h  T r a d i t i o n a l  i s  o n l y  0 . 1 7 1 .  F o r  A pache, ReLink also improved the prediction accuracy. We have also performed the paired-sample t-test to check if the improvements in F-measures are statistically significant. The t-test results confirmed that they are statistically significant, at the 99% confidence level (p-value < 0.01). 
 Figure 10: An example of false positive cases  Table 6. Prediction Results (Decision Tree) Subject Linking approaches Precision Recall F-measure Traditional 0.346 0.114 0.171 ReLink 0.432 0.261 0.325 ZXing Golden 0.476 0.435 0.454 Traditional 0.405 0.188 0.257 ReLink 0.779 0.645 0.706 Open- Intents Golden 0.742 0.683 0.711 Traditional 0.672 0.727 0.698 ReLink 0.716 0.748 0.731 Apache  Golden 0.709 0.713 0.711 These results clearly indicate that the defect information collected by ReLink improves the defect prediction performance. However, we noticed that the improvement varies across projects. For example, ReLink significantly improved the prediction accuracy of ZXing, while the improvement of Apache seems marginal. There are a few possible explanations for t h e s e  r e s u l t s .  F i r s t ,  a s  we showed in Table 3, the quality of Apache change logs is decent. As a result, traditional heuristics can mine links with reasonable accuracy. Basically, t h e  quality of Apache defect information derived by traditional heuristics and that by ReLink do not have huge differences. Note that ReLink is still able to improve the prediction accuracy because of the more accurate defect information derived. ReLink significantly improved the prediction performance of OpenIntents. As we showed in Table 3, ReLink is able to recover 
testing setReLinkXXXXXXXXXXXXXXX: buggy labelled instanceGround truth XXXXXTraditional heuristicsXXXXXXXXXXXXXXXXXXXXX
Training sets123 many links missed by traditional heuristics. In addition, the number of instances of OpenIntents is small. As we also found in a recent study [18], prediction models for a subject with a small number of instances are more sensitive to noise. Since ReLink provides much accurate defect information, the prediction performance of OpenIntents is significantly improved. 6. THREATS TO VALIDITY There are potential threats to the validity of our work: • For the two Android projects, the golden sets of links were collected by us manually. To assure their quality, two people were involved: one annotating the links and the other verifying them. However, it is difficult to guarantee that the golden sets do not contain any false negatives o r  f a l s e  positives. Even for the Apache dataset, which was manually examined by an Apache core developer, its quality is not completely assured because the developer may not recall all bug-fixing activities happened several months or years ago.  • Our approach is based on the assumption that the descriptions o f  b u g s  a n d  c o m m i t t e d  c h a n g e s  a r e  s i m i l a r .  I n  our investigation, we found that many open source proj ect s hosted by G o o g l e  Code follow the features we described. However, for projects that do not satisfy the assumption, our approach would be under threat. • All datasets used in our experiments were collected from open source projects. We need to evaluate the performance of ReLink on commercial projects. This remains as future work.  7. RELATED WORK Real-world data are often noisy, which may affect interpretations and models derived from the data. The data quality problem has also been observed by some software engineering researchers. For example, Mockus [20] noted that in many realistic scenarios the data quality is low (e.g., some change data could be missing), which could affect the outcome of an empirical study. Myrtveit et al. [24] and Strike et al. [29] also noticed the problem of missing and incomplete data in software effort estimation. In [1, 10, 11], authors analyzed the quality of bug reports a n d  c h a n g e  l o g s. Liebchen a n d  S h e p p a r d  [19] investigated the data quality issue and found that among hundreds of software engineering papers only four suggested that the data quality issue may affect their analysis results. In this paper, we focus on software process data and propose methods to improve data quality. To improve the correctness of identified links between defects and changes, Fisher et al. [13, 14] discussed the confidence of links. They took file names specified in the bug tracking databases and change logs into account. If the files exist in both of them, the confidence o f  t h e  l i n k s  w i l l  b e  h igher. Śliwerski et al. [28] proposed to verify the links by semantic analysis. A link is valid if it satisfies one of the conditions such as whether the bug has been resolved as Fixed a t  l e a s t  o n c e ,  whether the short description of the bug report is contained in the committed change log, etc. Bachmann et al. [5] improved Fischer’s approach by excluding all false-positive numbers that have a defined format. They validated the linked bug report by checking the relationship between the bug-fixing date and the change commit date. Although many researchers proposed various kinds of approaches to verify the links, the quality of the software process data is still not good enough. Bachmann and Bird et al. [6, 7] showed strong evidence that, only a fraction of bug fixes are explicitly labeled in the source code repository, and there exist systematic biases i n  finding the links. The bias could affect the effectiveness of t h e  studies based on the process data such as software defect prediction. Nguyen et al. [25] also found that biases existed in a commercial project (IBM J a z z )  t h a t  e n f o r c ed s t r i c t  d e v e l o p m e n t  guidelines. In order to find the missing links, Bird et al. [8] developed the tool “LINKSTER” to facilitate identification o f links manually. However, their tool requires manual effort, and thus is difficult to scale up. Our work is also related to the traceability analysis among software artifacts. Many researchers have proposed information retrieval (IR) based techniques to recover traceability links between source code and text documents (such as development journals, error logs, and emails) [2, 3, 4]. Runeson et al. [26] and Wang et al. [30] applied IR techniques to compare text similarity between two bug reports, thereby identifying potentially duplicate bug reports. In our work, we apply IR techniques to compare text similarity between bug reports and change logs, thereby recovering traceability links between bugs and changes. 8. CONCLUSIONS AND FUTURE WORK To automatically collect links between bugs and changes, traditional heuristics look for explicit links to bugs in change logs. Recent studies have shown t h a t  t r a d i t i o n a l  h e u r i s t i c s could b e  biased, since developers may not leave explicit links in change logs. In this paper, we have proposed ReLink, an automatic link recovery algorithm. ReLink is b a s e d  o n  a u t o m a t i c a l l y  l e a r n e d  feature criteria from explicit links. Our experimental results have shown that ReLink is able to recover links reliably. We have also shown t hat the more accurate defect information collected by ReLink has positive impacts on the follow-up software maintenance studies, including defect prediction models.  Our experimental results confirm that defect information collected using traditional heuristics should be u s e d  w i t h  c a u t i o n .  I t  i s  desirable to use link recovery algorithms and t o p e r f o r m c a r e f u l  inspection of the collected defect information. ReLink is the first step toward this direction.  In future more research on recovering links is needed in order to obtain more accurate defect data. We will also apply our approach to industrial projects to evaluate its usefulness.  Our tool and the experimental data used in this paper are available at: http://www.cse.ust.hk/~scc/ReLink.htm  ACKNOWLEDGEMENTS This research is supported by the Hong Kong RGC/GRF grant 612108, the NSFC grant 61073006, and the Tsinghua University research project 2010THZ0. We thank Christian B i r d  f o r  providing us with the annotated Apache dataset, and Lichao Liu for validating the golden set data for Android projects. REFERENCES [1] J. Aranda and G. Venolia. The secret life of bugs: Going past the errors and omissions in software repositories. In ICSE’09, pages 298–308, May 2009.  [2] G. Antoniol, G. Canfora, G. Casazza, A. De Lucia, and E. Merlo. 2002. Recovering Traceability Links between Code and Documentation, IEEE Trans. Softw. Eng. 28, 10 October 2002, 970-983. [3] A. Bacchelli, M. D'Ambros, M. Lanza, R. Robbes, Benchmarking Lightweight Techniques to Link E-Mails and Source Code. In WCRE’09, Lille, France, pp. 205-214, Oct 2009. [4] A. Bacchelli, M. Lanza, and R. Robbes, Linking e-mails and source code artifacts. In ICSE '10, Vol. 1. ACM, New York, NY, USA, 375-384.  [5] A. Bachmann and A. Bernstein. Software process data quality and characteristics - a  h i s t o r i c a l  v i e w  o n  o p e n  a n d  closed source projects. In IWPSE-Evol'09, pages 119-128, Amsterdam, The Netherlands, August 2009. [6] A. Bachmann, C. Bird, F. Rahman, P. Devanbu, and A. Bernstein, The Missing Links: Bugs and Bug-ﬁx Commits. In FSE’10, 97-106, Santa Fe, New Mexico, USA, Nov 2010.  [7] C. Bird, A. Bachmann, E.Aune, J. Duffy, A. Bernstein, V.Filkov, and P. Devanbu, Fair a n d  balanced?: bias in bug-fixing datasets. In ESEC/FSE'09, Aug. 2009, 121-130. [8] C. Bird, A. Bachmann, F. Rahman, and A. Bernstein, LINKSTER: enabling efficient manual inspection and annotation of mined data. In FSE’10, 369-370, Santa Fe, New Mexico, USA, Nov 2010.  [9] R. Baeza-Yates and B. Ribeiro-Neto, Modern Information Retrieval, Addison Wesley, 1999. [10] N. Bettenburg, R. Premraj, T. Zimmermann, and S. Kim. Duplicate bug reports considered harmful... really? In ICSM’08, pages 337–345, October 2008. [11] K. Chen, S. R. Schach, L. Yu, J. Offutt, and G. Z. Heller. Open-source change logs. Emp. Softw. Eng., 9(3):197–210, 2004. [12] C. Fellbaum, WordNet: An Electronic Lexical Database, Cambridge, MA: MIT Press, 1998. [13] M. Fischer, M. Pinzger, and H. Gall. Analyzing and relating bug report data for feature tracking. In WCRE'03, pages 90-99, Victoria, Canada, November 2003. [14] M. Fischer, M. Pinzger, and H. C. Gall. Populating a release history database from version control and bug tracking systems. In ICSM'03, pages 23-32, Amsterdam, Netherlands, September 2003. [15] A. Hindle, D. M. German, R. C. Holt: What do large commits tell us?: a taxonomical study of large commits. In MSR 2008, pp. 99-108, May 2008. [16] S. Kim, T. Zimmermann, K. Pan and E. Whitehead Jr., Automatic Identification of Bug-Introducing Changes. In ASE’06, Tokyo, Japan, September 2006. [17] S. Kim, T. Zimmermann, E. J. Whitehead Jr., and A. Zeller. Predicting faults from cached history. In ICSE’07, pages 489–498, Washington, DC, USA, 2007. [18] S. Kim, H. Zhang, R. Wu and L. Gong, Dealing with Noise in Defect Prediction. In ICSE'11, Honolulu, Hawaii, USA, May 2011.  [19] G. Liebchen and M. Shepperd. Data sets and data quality in software engineering. In PROMISE’08, 39–44, May 2008. [20] A. Mockus, Missing Data in Software Engineering, Empirical Methods in Software Engineering. The MIT Press, 2000. [21] A. Mockus and L. G. Votta, Identifying Reasons for Software Changes Using Historic Databases. In ICSM 2000, San Jose, CA, USA, 2000, pp. 120-130. [22] A. Mockus, R. T. Fielding, and J. D. Herbsleb. Two case studies of open source software development: Apache and mozilla. ACM Trans. Softw. Eng. Methodol., 11(3):309–346, 2002. [23] A. Murgia, G. Concas, M. Marchesi, R. Tonelli, A machine learning approach for text categorization of fixing-issue commits on CVS. In ESEM 2010, Bolzano-Bozen, Italy, Sep 2010. [24] I. Myrtveit, E. Stensrud, and U. H. Olsson. Analyzing Data Sets with Missing Data: An Empirical Evaluation of Imputation Methods and Likelihood-Based Methods. I E E E  Trans. on Software Engineering, 27(11), pp.999-1013, 2001. [25] T. H. D. Nguyen, B. Adams, A. E. Hassan, A Case Study of Bias in Bug-Fix Datasets. In WCRE’10, pp. 259-268. [26] P. Runeson, M. Alexanderson, O. Nyholm, Detection of Duplicate Defect Reports Using Natural Language Processing. In ICSE’07, 499-510, May 2007. [27] A. Schroter, T. Zimmermann, R. Premraj, and A. Zeller. If your bug database could talk... In ICSE’06, pages 18–20, Rio de Janeiro, Brazil, September 2006. [28] J. Sliwerski, T. Zimmermann, and A. Zeller. When do changes induce fixes? In MSR'05, pages 24-28, Saint Louis, Missouri, USA, May 2005. ACM. [29] K. Strike, K. E. Emam, and N. Madhavji. Software Cost Estimation with Incomplete Data. IEEE Trans. on Software Engineering, 27(10), pp.890-908, 2001. [30] X. Wang, L. Zhang, T. Xie, J. Anvik, and J. Sun, An approach to detecting duplicate bug reports using natural language and execution information. In ICSE'08, pages 461-470, Leipzig, Germany, 2008 [31] WEKA: http://www.cs.waikato.ac.nz/ml/weka/ [32] I.H. Witten and E. Frank, Data Mining: Practical Machine Learning Tools and Techniques with Java Implementation, second ed., Morgan Kaufmann, 2005. [33] T. Zimmermann, R. Premraj, and A. Zeller. Predicting defects for eclipse. In PROMISE'07, pages 1-9, Minneapolis, Minnesota, USA, May 2007.  [34] T. Zimmermann and P. Weissgerber. Preprocessing cvs data for Fine-grained analysis. In MSR'04, pages 2-6, Edinburgh, Scotland, UK, May 2004. [35] H. Zhang and R. W u ,  S a m p l i n g  P r o g r a m  Q u a l i t y , Proc. ICSM 2010, Timisoara, Romania, Sep 2010, pp. 1-10. [36] H. Zhang, An Investigation of the Relationships between Lines of Code and Defects. In ICSM’09, Edmonton, Canada, September 2009, pp. 274-28.  