Interrogative-Guided Re-Ranking for Question-Oriented
Software Text Retrieval
Ting Y e, Bing Xie∗, Y anzhen Zou, Xiuzhao Chen
Software Institute, School of Electronics Engineering and Computer Science,
Peking University, Beijing 100871, China
Key Laboratory of High Conﬁdence Software T echnologies, Ministry of Education,
Beijing 100871, China
{yeting12, xiebing, zouyz, chenxz12}@sei.pku.edu.cn
ABSTRACT
In many software engineering tasks, question-oriented text
retrieval is often used to help developers search for soft-ware artifacts. In this paper, we propose an interrogative-
guided re-ranking approach for question-oriented software
text retrieval. Since diﬀerent interrogatives usually indicateusers’ diﬀerent search focuses, we ﬁrstly label 9 kinds ofquestion-answer pairs according to the common interroga-
tives. Then, we train document classiﬁers by using 1,826
questions along with 2,460 answers from StackOverﬂow, ap-ply the classiﬁers to our document repository and present are-ranking approach to improve the retrieval precision. Insoftware document classiﬁcation, our classiﬁers achieve the
average precision, recall and F-measure of 56.2%, 90.9% and
69.4% respectively. Our re-ranking approach presents 9.6%improvement in nDCG@1 upon the baseline, and we also ob-tain 8.1% improvement in nDCG@10 when more candidates
are included.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Informa-tion Search and Retrieval; D.2.8 [ Software Engineering ]:
Miscellaneous
Keywords
Interrogative-Guided, Ranking, Software Text Retrieval
1. INTRODUCTION
Text Retrieval(TR) is widely used by software developers.
The performance of retrieval depends greatly on the textualquery and its relationship to the software text [5]. Consider
the following two questions about Lucene’s StandardAnalyz-
∗Corresponding Author
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for proﬁt or commercial advantage and that copies bear
this notice and the full citation on the ﬁrst page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or toredistribute to lists, requires prior speciﬁc permission and/or a fee. Request
permissions from permissions@acm.org.
ASE’14, September 15-19, 2014, V asteras, Sweden.
Copyright 2014 ACM 978-1-4503-3013-8/14/09 ...$15.00.
http://dx.doi.org/10.1145/2642937.2642953.er(“StandardAnalyzer”and“TermQuery”are class names of
Lucene1):
1. Where is StandardAnalyzer?2
2. How do I use StandardAnalyzer with TermQuery?3
IfwesimplyapplythesetwoquestiontokeywordsbasedTR,
their queries are similar. Yet they are much diﬀerent in whatthey are asking for. Question 1 wants to know the positionwhereStandardAnalyzer locates. While question 2 wants to
know how to use the StandardAnalyzer withTermQuery .I f
we use keyword-based search method, the corresponding an-swer list is coarse. In our experiments, we found question 1has 22 search results common with question 2 in top-30 TR
results. And when retrieving the question 1 by Lucene, the
answer of Question 2 ranked higher than the answer of Ques-tion 1. Thus, some new approaches are needed to improvethe performance of Question-Oriented software retrieval.
To achieve this objective, we need to answer a question
ﬁrst: is it possible to ﬁnd any diﬀerences between the an-swers of the above two questions? It is obvious that thesetwo questions have diﬀerent interrogatives (i.e. how, where,etc.). It indicates that we have an expectation of what the
answer be like when we ask a question. As a result, their
answers are diﬀerent not only in words but also in codes andlinks. Thus, we decide to build a software document classiﬁ-er, which can automatically distinguish whether a documentis suitable to answer a question. Based on the classiﬁer, we
can re-rank the search results so as to improve the precision
of software text retrieval.
Training a document classiﬁer need a lot data samples.
Fortunately, somesocialsoftwarequestion-answering(Q&A)
forums, such as StackOverﬂow, provide ideal vehicle to in-
vestigate developers’ questions and answers. We can down-load plenty of question-answer pairs from these websites.Every question is well described and its answers are votedand ranked. Therefore, we can learn from the StackOverﬂow
about what the answer would be like when given a question.
Comparing with the various existing works, our approach
provides the following beneﬁts:
1. We build six interrogative-guided software document
classiﬁers ﬁrstly in software engineering based on Q&A
pairs from StackOverﬂow. We make use of features in
1Lucene is open source software project, which oﬀers a
free/open source information retrieval technology.
http://lucene.apache.org/
2http://stackoverflow.com/questions/3080888/
3http://stackoverflow.com/questions/1390088/
115
fouraspects(i.e. text, code, linkanddocumentlength)
to train a better software text classiﬁcation model.
2. We ﬁrstly propose an interrogative-guided re-ranking
approach for software documents retrieval. When aquestionissubmitted, itsinterrogativeisidentiﬁedandthe corresponding classiﬁer is adopted to re-score thetop-k search results. If a software text is classiﬁed as
the same type with the question, it would gain higher
score, and vice verse. As a result, we could improvethe performance of software text retrieval signiﬁcantly.
3. Compared with ﬁlter policy, our re-ranking approach
demonstrates higher accuracy in the experiments. Weachieve 9.6% improvement in nDCG@1 and 8.1% im-provement in nDCG@10 upon the baseline.
2. OVERVIEW
In this paper, we proposed a re-ranking approach in soft-
ware text retrieval. As shown in Figure 1, this approach iscomposed of two parts: classiﬁer building andre-ranking .
Theclassiﬁer building part aims to build classiﬁer for each
type and the re-ranking part takes advantage of these clas-
siﬁers to re-rank the retrieved documents.
1)Classiﬁer Building: This part includes question label-
ing,document representation ,feature selection andmodel
training.T h e question labeling is to label all the Q&A pairs.
We ﬁnd that developers focus on diﬀerent information whenthey use diﬀerent interrogatives in their questions. Thus
we label the questions with diﬀerent tags based on inter-rogatives(e.g., how, why, etc.). Naturally, the answers canbe labelled the same tags as their corresponding question-s. We perform some checks on the tag of every question to
ensure the accuracy of our dataset. The document repre-
sentation component deals with these labeled answers. It
represents each answer document as a feature vector. Each
document contains a textual description along with code s-
nippers, links and document length. The feature selection
componentlearnsthemostimportantfeaturesfromdiﬀeren-
t answer vectors using machine learning technique. We usethese features to reduce the dimension of the feature vector.Next, the selected features are provided to a model training
algorithm to build classiﬁers individually.2)Re-Ranking: This part includes text retrieval andre-
ranking.T h e text retrieval engine ranks the relevant docu-
ments for a given question. Then, we use the classiﬁer to
re-rankthe documents retrieved by the TR engine. We con-
sider the ﬁnal re-rankscore as a linear combination of the
learned score(i.e. the score generated by the classiﬁer) and
thetext retrieval score. When user submitted a question,
search engine could identify its corresponding tag and re-
score every search result. For those answers belonging to
user’s question type, they possibly match some learned fea-tures so that gain a higher score; meanwhile, those softwaretexts, which do not contain those learned features, would be
scored lower.
A primary challenge in our work is to get enough amoun-
t of training data, including correctly labeled Q&A pairs
with high quality. Fortunately, social software Q&A forum-s, such as StackOverﬂow, provide a large amount of Q&A
pairs and have a voting system to point out the quality of
Q&A pairs [11]. We chose the questions and answers taggedwith “lucene” from StackOverﬂow(from Apr. 2008 to Sept.2013) and got a sample consisted of 5,587 questions with
7,872 answers. And considering there are some invalid and7[KYZOUT'TY]KX6GOXY 7[KYZOUT2GHKROTM
*UI[SKTZ8KVXKYKTZGZOUT
,KGZ[XK9KRKIZOUT
3UJKR:XGOTOTM )RGYYOLOKX2GHKRKJ'TY]KX9KZ4GZ[XGR2GTM[GMK7[KYZOUT
8KZXOK\GR:UUR
8GTQKJ*UI[SKTZ8KY[RZY
8KXGTQOTM
8KXGTQKJ
*UI[SKTZ8KY[RZY- Data
- Process
Figure 1: Approach Overview
low-quality questions and answers, we select these questions
with positive votes. By this way, we select 1,864 questionsfor labeling. These questions contain 2,502 positively votedanswers.
3. CLASSIFIER BUILDING
In this section, we describe the process of Classiﬁer Build-
ing. First, the question labeling module labels the questionswith nine tags. Second, the document representation mod-ule transforms every labeled answer document into vector.Then, the feature selection module learns the discriminativefeatures for the answers of six main question types. Finally,
the model training module builds classiﬁers for each type
individually.
3.1 Question Labeling
As shown in Table 1, we labeled the questions as how-
to,what,why, etc. And these label names (e.g. what, how,
where, why , etc), are chosen because interrogatives frequent-
ly occur in questions of the corresponding category. We con-sidered the question meaning as well.
Table 1: Question Tags
Tag Description Sum
howto Asks for instructions, which usually begins with
word “how”. e.g., How to update a Lucene.NET
index?1,134
what Asks what a variable is. e.g. What is the segment
in Lucene?343
which Asks for an advice when meeting multiple choice.e.g. Which is the best choice to indexing a
Boolean value in lucene?68
where Asks for location(path, url, directory etc). e.g.Where is StandardAnalyzer?51
why Asks for reasons. e.g. Why Lucene merge indexes? 214
when Asks about the time at which things happen. e.g.When should one stop using Lucene and graduateto Solr?5
who Asks about the name or identity of a person orgroup of people. e.g. Who created the C pro-
gramming language?2
whether Asks for yes or no. e.g. Should an index be opti-
mized after incremental indexes in Lucene?358
others The rest question which are hard to be labeled. 31
1163 PhD students, 4 mater students and 3 senior under-
graduate students are asked to do cross validation on the
labeling, i.e., if two persons have divergences on labeling
one question, a third person would discuss together untilthey have the same opinion on labeling this question. Thus,the mistakes of question labeling are minimized.
FromTable1, themostfrequenttypeis howto,f o l l o w e db y
whether, what,why,which,where.R a r e whenandwhotype
questions are labeled. Actually, the taxonomy is not orthog-onal(i.e. one question would be labeled as several tags)
4.
Thus, we want to build classiﬁers for each type individu-
ally. As the amount of whenandwhois much less than
the amount of other types, and the questions in othersdo
not contain any interrogatives at all. So, we do not build
classiﬁers for these 3 types and focus on the main 6 types,i.e.,howto,whether, what,why,which,where. Based on the
l a b e l e dq u e s t i o ns e t ,w ec a ng e tal a b e l e da n s w e rs e t .
3.2 Document Representation
We need to represent every answer document as a feature
vector. And two kinds of features (i.e. text feature andnon-text feature), are used in our approach.
A) Textual Features The text of an answer gives an expla-
nation to the question, thus some words may closely relateto the question type. To automatically extract informationfromthetext, werepresenteachdocumentasabagofwords.
For these words, we perform stop-word removal and stem-
ming [8, 10].
In our work, stop words include these four parts:
1. Common stop words, such as,“is”,“am”,“would”, etc.2. Java reserved words, such as,“public”,“static”, etc.
3. Labels deﬁned by StackOverﬂow, such as “<code>”,
“<img>”, “ <a>”etc.
4. Words related to speciﬁc projects, such as “lucene”,
“index”,“search”, etc.
Stemmingreducesawordtoitsroot; forexample,“choose”,
“chooses”,“choosing”, could all be reduced to“choos”. This
process can potentially increase the discriminative power of
root words that are good at diﬀerentiating answers of a spe-ciﬁc question type from answers of other types. We choose
“snowball” stemming algorithm
5to reduce the word to it-
s root. Note that the root of noun and verb are diﬀerent(e.g. “explains”, “explained” are reduced to the stem “ex-plain”, but“explanation”and“explanations”are reduced to
the stem“explan”).
B) Non-textual Features Theprogramming answerusually
contains code snippets and links. Empirically, we think that
the number of code snippets, links and the length are relatedwith the question types, e.g., a how-totype answer is more
likely to contain code snippets in order to explain a how-to
type question step by step than other type answers. Thereare tags presented by StackOverﬂow, e.g. tag pair (<code>,</code>) indicates the code snippers and (<a>, </a>)
indicates the links. Beneﬁt from these tags, we can extractthe code snippers and links conveniently.
These features are shown in Table 2. Features W
1toWn
are textual features; features Code-*are code features; fea-
turesLink-*are link features; features Length-* are length
features. Some features we used are similar with [2] [7] [10].
4A question post which contains several question sen-
tences would be labeled with several tags, e.g. http:
//stackoverflow.com/questions/2752612/.
5http://snowball.tartarus.org/Besides, we added more features in Table 2, such as Code-
malc,Code-clc. We add “*” to indicate the diﬀerent fea-
tures. Based on the textualandnon-textual analysis as
described above, the answer document can be represented
as− →dj={x1j,x2j,...,x Nj}, wherexijrefers to one of these
features in Table 2.
Table 2: Features for Document Representation
ID Feature Description
W1toWnEach feature represents a stemmed word in the
answer set and has a value corresponding to the
number of times the word appears in the answer.
Code-num n. of code snippets
Code-avc A v e r a g ec o d el e n g t h
Code-mac Maximum code length
Code-mic Minimum code length
Code-sdc Code length standard deviation
Code-malc* Maximum code lines
Code-milc* Minimum code lines
Code-sdlc* Code lines standard deviation
Code-clc* n. of code lines
Link-xlc n. of links to external sources
Link-ilc n. of links to other query/answer in the forum
Link-lc* n. of links
Length-wc n. of words
Length-sc n. of sentences
Length-cc n. of characters
3.3 Feature Selection
We do the feature selection for two reasons: One is that
we expect there are some features with similar values con-
tained in the same type answers but diﬀer between diﬀer-ent type answers. So, we are interested in ﬁnding the bestfeatures for discriminating the diﬀerent answer types. Theother reason is that the dimensional of our combined featurevector is more than one thousand, we only have 2,460 an-
swers available for training and testing. This makes discrim-
inative learning extremely hard since most methods tend toover ﬁt in training set. We address both issues by seeking acompact, low dimensional and readable representation thatfacilitates learning from a few samples.
Among so many feature selection algorithms, the informa-
tion gain algorithm(InfoGain)[9] suits our feature selectiontask most. InfoGain is a statistical measure of how much agiven feature contributes to discriminate the class to which
any given answer belongs. All the features are ranked ac-cording to the information gain metric. With the help of
InfoGain algorithm, we can ﬁnd the discriminative featuresfrom thousands of features. In our experiment, we selecttop-30 features for each type.
3.4 Model Training
The objective of Model Training is to build classiﬁers to
automatically classify the answers into their correspondingtypes. To achieve this objective, we consider both classiﬁca-
tion and regression method. The assumption of the latter is
that, the degree of diﬀerent documents belonging to a spe-ciﬁc type is not the same. For example, assuming there aretwo documents belonging to howtotype. If document Acon-
tains more howtotype features than document B, document
Acan get more score than B. We adopt two algorithms:
117Bayesian Logistic Regression(BLR)[3] for classiﬁcation and
Linear Regression(LR) [1] for regression.
1)Bayesian Logistic Regression: It is a good classiﬁer for
text classiﬁcation. Genkin et al.[3] ﬁrst propose this ap-
proach for text categorization. This approach uses a Laplaceprior to avoid overﬁtting and produces sparse predictivemodels for text data. Among all the classiﬁcation approach-
es, BLR performs best in our situation. We choose it to
build the classiﬁcation model.
2)Linear Regression: LR on the other hand performs re-
gression. LR is an approach to modeling the relationship
between a scalar dependent variable yand one or more ex-
planatory variables denoted X.G i v e n{y
i,xi1,...,x ip}n
i=1of
n statistical units, a LR model assumes that the relation-
ship between the dependent variable yiand the p-vector of
regressors xjis linear. The reason we adopt the LR model
is that, assuming there are two documents, if document A
is more likely to answer a positive type question than docu-mentB,Acan get a higher score.
4. RE-RANKING IN TEXT RETRIEV AL
As we mentioned above, the type of retrieved documen-
t may diﬀer from the input question type. If the answerdocument can be identiﬁed correctly and automatically, wecan pick out the right answer documents which have thesame type as the input question. This is the institution be-
hind the re-ranking approaches. We would introduce the
TR tool ﬁrst, then introduce our re-ranking approach, re-scoring, based on the text retrieval.
4.1 Text Retrieval
We use Lucene as our baseline search tool for re-ranking.
When given a natural language question, we take two stepsto search the relevant documents:1)Pre-Processing: We represent the input question as a
bag of words and perform stop-word removal and stemming
on these words. The delimiters that we used to split the
question into words are all the characters excluding digit-s(e.g. 1, 2, etc.) and letters(e.g. a, b, etc.). Then, weremove the ﬁrst three types of stop-words mentioned in Sec-tion 3.2.1 and stem the remain words by snowball algorithm.
2)Retrieval: We use Lucene engine to search the set of
words and get the documents it returns.
4.2 Re-Scoring
After getting the ranked documents, we can utilize the
classiﬁers to do re-scoring. Some former works use the clas-siﬁers to ﬁlter out the irrelevant documents(e.g. Gottipati
et al. [4] use the classiﬁer to ﬁlter out the junks). We namedthis re-ranking approach as“Filter”.
We adopt another way(i.e. re-scoring) to re-rank the doc-
uments. We combine estimated score with the original rank-ing score to obtain the re-ranking score. In particular, wemodel the re-ranking score s
Ras the linear combination
sR(x)=α·y(xI)+(1− α)·sO(x)( 1 )
where xIdenotes feature vector of each document using the
discriminative features, yis the BLR classiﬁcation function
or LR regression function for estimating the possibility ofdocument belonging to the type of the input question, ands
Ois the original ranking score. The re-ranking is obtained
by sorting sRin descending order.The score of BLR is binary, either 1.0 or 0.0 (depending
the answer was classiﬁed as the same type with the inputquestion or not). To make the LR score comparable with
BLR score, we apply the equation 2 normalization to bring
the LR regression score into range [0.0, 1.0].
y
i(xI)=yi(xI)−minm
j=1yj(xI)
maxmj=1yj(xI)−minmj=1yj(xI)(2)
wheremis the number of sample, yi(xI)i st h eithoutput
value by LR regression, and maxm
j=1yj(xI) is the maximum
value, minm
j=1yj(xI) is the minimum value.
5. EV ALUATION
Our experiments address two research questions:
RQ1:What’s the behaviour of the classiﬁers and what af-
fects the behaviour of the classiﬁers?
RQ2:Does our re-ranking approach really work? Does our
approach performs better than other re-ranking approach?
For the classiﬁer evaluation, we use 2,460 answers with
positive votes. For each type A, the positive samples are
thoseA-labeled positive-voted answers. And we randomly
selected the negative samples from the rest type answers as
the same numbers as the positive samples. Then we use a
ten-fold cross-validation method [1] to evaluate the classi-ﬁers. Thus, all samples are randomly split into ten parts,in each run, one part is used as test set, the remaining nineparts are used as training set. The split on training and
test sets is the same in all experiments. The ﬁnal results
of each experiment represent the average of ten runs. Thepositive answer numbers of six main types are howto(1,494) ,
whether(518), what(514) ,why(276), which(120), where(87) ,
respectively.
For the re-ranking evaluation, we use 1,826 questions with
positive votes. And we collect all the 7,872“lucene”-taggedanswers as the repository. We also use a ten-fold cross-validation method to evaluate the re-ranking performance.
Since the ranking evaluation metric needs a rating assess-
ment(i.e. degree of relevance) for each document of the giv-en question, we calculate the degree of relevance with thesetwostepsautomatically: (1)Foreachquestion, weregardthe
corresponding positive-vote answers as the relevant answers
while other answers are irrelevant. (2)The degree of rele-vance is measured by the score of the answer. We score theminimum positive-vote answer 1 and the second-minimumanswer 2, etc., while others are scored 0(irrelevant).
5.1 RQ1–Classiﬁer Evaluation
The feature sets and the threshold kare the two ma-
jor factors which inﬂuence the classiﬁers’ eﬀectiveness. Wewould introduce the evaluation criteria for the classiﬁersﬁrst. Then we conduct a series of experiments on diﬀer-ent feature sets and also on the varying threshold k.
Precision, Recall and F-measure. In order to evaluate
the classiﬁers’ eﬀectiveness, we use precision, recall and theF1-measure [8]. These measures have been commonly usedto evaluate the accuracy of various retrieval, classiﬁcation,and mining algorithms [4].
1)Performance on Diﬀerent Feature Sets: We extract tex-
tual and non-textual features for each question. Then we
conduct a feature selection on the all features (i.e. a com-bination of textual and non-textual features) vector. Thus,
we get four feature sets: selected feature set, all feature set,
118textual feature set and non-textual feature set. We com-
pare the best F1-score of these feature sets at the appro-
priate threshold k. The per type result is shown in Table
3.(SFS,AFS,TFS,NTFSrepresent Selected Feature Set,
All Feature Set, Textual Feature Set, Non-textual Feature
Set respectively)
Table 3: Precision, Recall and F-measure Results
for Diﬀerent Question Types with Diﬀerent FeatureSets
Ques. P,R&F1 SFS AFS TFS NTFS
howtoPrec. 0.587 0.584 0.623 0.602
Recall 0.953 0.950 0.636 0.851
F1 0.726 0.724 0.629 0.705
whetherPrec. 0.543 0.530 0.553 0.495
Recall 0.872 0.827 0.615 0.199
F1 0.669 0.646 0.582 0.284
whatPrec. 0.521 0.561 0.491 0.495
Recall 0.882 0.768 0.297 0.593
F1 0.655 0.648 0.370 0.540
whyPrec. 0.529 0.530 0.490 0.487
Recall 0.872 0.862 0.393 0.388
F1 0.659 0.656 0.436 0.432
whichPrec. 0.577 0.539 0.505 0.476
Recall 0.887 0.906 0.991 0.377
F1 0.699 0.676 0.669 0.421
wherePrec. 0.551 0.547 0.462 0.462
Recall 0.908 0.892 0.462 0.462
F1 0.686 0.678 0.462 0.462
From Table 3, it could be noticed that the Selected Fea-
tures perform best, while Text Features and Non-text Fea-
tures (i.e. using a single set features) performs worst in this
situation. As the selected feature set performs best for theBLR and LR, we will use selected features in the followingre-ranking step.
2)Performance on Varying Threshold k: BLR has a varying
parameter kranging in (0.0, 1.0). When we decrease the
parameter k, the number of tp(true positive) and fn(false
negative) increases. Table 4 reports the result of wheretype
answers with varying values kfor selected feature set and
all feature set. As we increase the parameter k, the recall
values of both features decrease and the precision values in-
crease ﬁrst then decrease when kis larger than 0.5. In table
4, whenksets 0.5, the F1-measure on both Selected Feature
Set and All Feature Set come to maximum.
Table 4: Performance on Varying Threshold kfor
Selected Feature Set and All Feature Set of Where
Classiﬁer
kSelected Feature Set All Feature Set
Prec.Recall F1Prec.Recall F1
0.480.5040.9120.6490.4960.9080.642
0.490.5180.9120.6610.5100.8920.649
0.500.5510.9080.6860.5470.8920.678
0.510.5440.6620.5970.5380.8770.667
0.520.4840.2310.3130.4630.2920.358
Summary: As an answer to RQ1, we show that the clas-
siﬁers could achieve an F1-Score of up to 72.6%. And the
selected feature set is the best feature set for BLR method.Forthevaryingthreshold k, wecanﬁndthattherecallwould
decrease and the precision may increase ﬁrst then decrease
when the kincreases.
5.2 RQ2–Re-Ranking Evaluation
To evaluate the re-ranking performance, we compare our
approach with other re-ranking approaches (including the
pure TR approach). We would introduce the evaluation cri-
teria (i.e. nDCG@k) for the re-ranking approach ﬁrst. Then
we illustrate these comparisons in detail.Normalized Discount Cumulative Gain (nDCG). The
NormalizedDiscountedCumulativeGainattopk(nDCG@k,for short) is a ranking evaluation metric ﬁrst proposed in [6].It allows us to measure how close the predicted answer rank-ing is to the ground truth ranking. More formally, NDCG@kis deﬁned as:
nDCG@k=1
IDCG@kk/summationdisplay
i=12reli−1
log2(i+1)(3)
whereriis the true rating assessment for the answer at
position i in the ranking, and IDCG@kis a normalization
factor. IDCG@k means the discounted cumulative gain (thesumpartinEquation2)oftheidealranking, i.e. therankingwhere, given a pair of answers ( a
i;aj),aiis better ranked
thanajifr/prime
iis greater than r/prime
j.
Since our work addresses the problem of ranking answers
for software text retrieval, we set the Lucene Search as thebaseline. Andwecomparewiththeﬁlterapproach(Gottipatiet al. [4]) and the LR re-scoring approach.
Figure 2,3 shows the comparison of diﬀerent methods.
We observe that almost all the re-ranking methods outper-formed the baseline (Lucene Search) in all types (except theﬁlter in wheretype). This result indicates that all the re-
ranking methods show a better performance than baseline.
And our BLR re-scoring approach obtains an improvemen-
t of 9.6% in nDCG@1 and 8.1% in nDCG@10 upon thebaseline. What we also observe is that the BLR re-scoringoutperformed the other methods for all values of k(the ND-
CG@k) in the overall improvement in our dataset.
Figure 2: nDCG@k of Overall Improvement
In the overall improvement, the ﬁlter performs worse than
BLR re-scoring. And the ﬁlter behaves unstably. It obtainsthe best performance on the whatandwhytype, while it
also obtains the worst performance on the where,e v e nw o r s e
than the Lucene Search. As for the LR re-scoring, we are a
little disappointed at its performance. After taking a look
at the score of every document using LR method, we ﬁnd
119Figure 3: nDCG@k of howto,what,where, why, which, whether Improvement for Methods of BLR Re-Scoring,
BLR Filter, LR Re-Scoring and Lucene Search
that most scores are close. Thus, the result of LR re-scoring
is comprehensible.
Summary: ForRQ2, weshowthatourre-rankingapproach
performs signiﬁcantly better than the pure TR approach,
which gains 9.6% in nDCG@1 over the TR approach. Andour approach also has an advantage over the ﬁlter approach
and the re-scoring approach using LR.
6. CONCLUSION AND FUTURE WORK
Inthispaper, wepresentaninterrogative-guidedre-ranking
approach for question-oriented software text retrieval. We
build several software document classiﬁers, which learn from
a large number of question-answer pairs on Stack Overﬂow.
Then, we apply the classiﬁers to our document repositoryand adopt a re-scoring approach, which combines the clas-siﬁcation score and the text retrieval score.
The performance of our classiﬁer still needs further im-
provement. In our approach, documents are classiﬁed auto-matically by its corresponding question types. In practice,a document may answer several types of questions and maybe not limited to the type of its original question. In the fu-ture, we will try some fuzzy learning model in our classiﬁerso as to gain higher classiﬁcation precision.
7. ACKNOWLEDGMENTS
WethankLuZhangandYangyangLufortheirdiscussions
and comments on the draft of this paper. This researchis sponsored by the High-Tech Research and DevelopmentProgram of China (863) under Grant No. 2013AA01A605,the National Basic Research Program of China (973) underGrant No. 2011CB302604, the National Natural Science
FoundationofChinaunderGrantNos. 61103024, U1201252.
8. REFERENCES
[1] C. M. Bishop et al. Pattern recognition and machine
learning, volume 1. springer New York, 2006.[ 2 ]D .H .D a l i p ,M .A .G o n ¸ c a l v e s ,M .C r i s t o ,a n d
P. Calado. Exploiting user feedback to learn to rankanswers in q&a forums: a case study with stack
overﬂow. In SIGIR, pages 543–552. ACM, 2013.
[3] A. Genkin, D. D. Lewis, and D. Madigan. Large-scale
bayesian logistic regression for text categorization.Technometrics , 49(3):291–304, 2007.
[4] S. Gottipati, D. Lo, and J. Jiang. Finding relevant
answers in software forums. In ASE, pages 323–332.
IEEE Computer Society, 2011.
[5] S. Haiduc, G. Bavota, A. Marcus, R. Oliveto,
A. De Lucia, and T. Menzies. Automatic queryreformulations for text retrieval in softwareengineering. In ICSE, pages 842–851. IEEE Press,
2013.
[6] K. J¨arvelin and J. Kek ¨al¨ainen. Ir evaluation methods
for retrieving highly relevant documents. In
Proceedings of the 23rd annual international ACMSIGIR conference on Research and development ininformation retrieval , pages 41–48. ACM, 2000.
[ 7 ]C .L i u ,Y .Z o u ,S .C a i ,B .X i e ,a n dH .M e i .F i n d i n g
the merits and drawbacks of software resources from
comments. In ASE, pages 432–435. IEEE Computer
Society, 2011.
[ 8 ]C .D .M a n n i n g ,P .R a g h a v a n ,a n dH .S c h ¨utze.
Introduction to information retrieval , volume 1. 2008.
[9] D. Roobaert, G. Karakoulas, and N. V. Chawla.
Information gain, correlation and support vector
machines. In Feature Extraction , pages 463–470.
Springer, 2006.
[10] Y. Tian, J. Lawall, and D. Lo. Identifying linux bug
ﬁxing patches. In ICSE, pages 386–396. IEEE, 2012.
[11] C. Treude, O. Barzilay, and M.-A. Storey. How do
programmers ask and answer questions on the web?:Nier track. In ICSE, pages 804–807. IEEE, 2011.
120