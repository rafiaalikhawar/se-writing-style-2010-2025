Code Coverage for Suite Evaluation by Developers
Rahul Gopinath
Oregon State University
Corvallis, OR, USA
gopinath@eecs.orst.eduCarlos Jensen
Oregon State University
Corvallis, OR, USA
cjensen@eecs.orst.eduAlex Groce
Oregon State University
Corvallis, OR, USA
agroce@gmail.com
ABSTRACT
One of the key challenges of developers testing code is deter-
mining a test suite’s quality – its ability to ﬁnd faults. The
most common approach is to use code coverage as a measure
for test suite quality, and diminishing returns in coverage or
high absolute coverage as a stopping rule. In testing re-
search, suite quality is often evaluated by a suite’s ability
to kill mutants (artiﬁcially seeded potential faults). Deter-
mining which criteria best predict mutation kills is critical
to practical estimation of test suite quality. Previous work
has only used small sets of programs, and usually compares
multiple suites for a single program. Practitioners, however,
seldom compare suites — they evaluate one suite. Using
suites (both manual and automatically generated) from a
large set of real-world open-source projects shows that eval-
uation results diﬀer from those for suite-comparison: state-
ment (not block, branch, or path) coverage predicts muta-
tion kills best.
Categories and Subject Descriptors
D.2.5[Software Engineering ]: TestingandDebuggingTest-
ing Tools
General Terms
Measurement, Veriﬁcation
Keywords
Test frameworks, evaluation of coverage criteria, statistical
analysis
1. INTRODUCTION
The purpose of software testing is to improve the quality
of software, and the primary route to this goal is the de-
tection of faults. Unfortunately, the problem of ﬁnding all
faults in a program (or proving their absence), for any mean-
ingfulprogram, isessentiallyunsolvable. Testingistherefore
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
ICSE ’14, May 31 - June 7, 2014, Hyderabad, India
Copyright 14 ACM 978-1-4503-2756-5/14/05 ...$15.00.always a trade-oﬀ between the cost of (further) testing and
the potential cost of undiscovered faults in a program. In
order to make intelligent decisions about testing, develope rs
need ways to evaluate their testing eﬀorts in terms of their
ability to detect faults. The ability, given a test suite, to
predict whether it is eﬀective at ﬁnding faults is essential to
rational testing eﬀorts.
Theidealmeasure of fault detection is, naturally, fault
detection. In retrospect, using the set of defects discovered
during a software product’s lifetime, the quality of a test
suite could be evaluated by measuring its ability to detect
those faults (faults never revealed in use might reasonably
have little impact on testing decisions). Of course, this is
not a practical method for making decisions during devel-
opment and testing. Software engineers therefore rely on
methods that predict fault detection capability based only
on the suite itself and the current version of the software un-
der test (SUT). The most popular method is the use of code
coverage criteria [1]. Code coverage describes structural as-
pects of the executions of an SUT performed by a test suite.
For example, statement coverage indicates which statements
in a program’s source code were executed, branch coverage
indicates which branches were taken, and path coverage de-
scribes (typically in a slightly more complex way, to account
for loops) the paths explored in a program’s control ﬂow
graph.
In software testing research, the gold standard for suite
evaluation is generally considered to be actual faults de-
tected, but this is, again, in practice diﬃcult to apply even
in a research setting [16]. The second most informative mea-
sure of suite quality is usually held to be mutation testing [7,
2], which measures the ability of a test suite to detect small
changes to the source code. Mutation testing subsumes
many other code coverage criteria [29], and has been shown
to predict actual fault detection better than other criteria in
some settings [13, 2, 24], but never shown to be worse than
traditional code coverage measures.
Unfortunately, mutation testing is both diﬃcult to apply
and computationally expensive, which has led to the search
for “next-best” criteria for predicting suite quality by re-
searchers [16, 19]. This eﬀort is highly relevant to real soft-
ware developers, who almost never apply mutation testing
due to its complexity, expense, and the lack of tool support
in many languages. From the point of view of actual soft-
ware developers and test engineers, rather than researchers,
however, most studies of suite evaluation are not focusing
on their actual needs.Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a
fee. Request permissions from Permissions@acm.org.
Copyright is held by the author/owner(s). Publication rights licensed to ACM.
ICSE’14 , May 31 – June 7, 2014, Hyderabad, India
ACM 978-1-4503-2756-5/14/05
http://dx.doi.org/10.1145/2568225.2568278
72
First, researchers typically consider the eﬀectiveness of
criteria for predicting which of multiple suites for a SUT
will detect the most faults. For research purposes, this is
the right focus: the primary use of coverage criteria is to
compare test-generation methods, where the fundamental
question is typically“Which of the following approaches will
detect the most faults?” In typical development settings,
however, the question is more often“Should we devote more
eﬀort to improving this test suite?” and applying completely
diﬀerent testing methods is not an option. In fact, the suites
being evaluated are usually produced manually and the only
mitigation for poor eﬀectiveness is adding more tests manu-
ally. Testing research often focuses on comparing suites for
one SUT; practitioners more often need to simply evaluate
a single suite. It is not clear that the best criteria for multi-
suite comparison, where suites are expected to be generated
by automated methods, are the most eﬀective for evaluating
a manually generated suite.
Second, researchers are far more likely to apply novel cov-
erage criteria than practitioners. Many recent papers com-
paring criteria either introduce a novel criteron [19] or im-
plement a previously proposed but never-implemented cri-
teria [16]. While most even moderately popular languages
(e.g., Java, C, C++, Python, Haskell) have (multiple) tools
for measuring statement and branch coverage, relatively few
non-research tools oﬀer any variant of path coverage, much
less data ﬂow-based [20, 19] or more esoteric measures such
as predicate-complete test coverage measures [5, 6]. This
limitation is particularly important for open source devel-
opment, where expensive commercial coverage tools are un-
likely to be applied, and developers working together may
lack a common development environment. Even criteria as
widelyadoptedastheMC/DCcoveragerequiredinaerospace
code development [9], lack open source or free tools.
This paper examines the question of coverage criteria as
suite quality predictors from the perspective of the non-
researcher audience, developers interested in suite evalua-
tion (rather than comparison), lightweight, widely availabl e
tools, and well-known coverage criteria. Given the con-
straints under which real software projects operate; which
widelyavailablecoveragecriteriaprovidethebestestimation
of fault detection? This paper draws from the evaluation
of hundreds of open source projects. While the results are
based only on open source Java programs hosted on Github,
and using the popular Maven build system, it is likely that
our ﬁndings apply at minimum to many other Java projects,
and may well apply to other languages as well. As a“silver
standard” for evaluating suite quality, mutation testing is
used, as identifying real faults of hundreds of Java projects
was clearly infeasible.
Our ﬁndings based on real-world conditions show that,
in contrast to the results of some studies conducted in a
research context, statement coverage is generally the most
eﬀective predictor of suite quality. This is not an accident
of the nature of manually-produced test suites; the same
relationship also holds for test suites generated by the Ran-
doop tool [31], which uses feedback-directed random testing
to generate suites. While branch coverage or some variant
of path coverage may be most useful for many research con-
texts, in the context of typical Java open source projects, at
least, a focus on the simple and easily understood measure
of statement coverage is probably most useful for predict-ing suite quality, even if developers are using an automated
testing tool.
Theprimarycontributionsofthispaperaretwofold: First,
the existence of popular open source repositories makes it
possible to investigate the eﬀectiveness of coverage criteria
in a more unbiased, large-scale, and systematic way than
previous studies. Such repositories provide a large body of
very diﬀerent SUTs, limiting selection eﬀects. Using actual
test suites from real projects also ensures that results are
relevant to actual testing practices. The additional avail-
ability of automated testing tools mature enough to apply
to this set of projects enables us to draw conclusions about
both human-generated and automatically-generated suites,
and to show that results do not depend on this property of
test suites. This enables our second contribution: a prac-
tical proposal to developers wishing to evaluate test suites
for open source Java projects. Finally, by showing that the
best criteria for research purposes diﬀer from those for prac-
titioners, this paper shows that the preferences and abilities
of software testing researchers may lead to less-than-optimal
advice to developers whose focus is not on evaluating testing
methods but on producing quality software.
2. RELATED WORK
A large body of work considers the relationship between
coverage criteria and fault detection. The most closely re-
lated work to ours, which considers some of the same ques-
tions from a diﬀerent perspective (that of researchers) is the
recent work of Gligoric et al. [16]. Their work uses the same
statistical approach as our paper, measuring both τβandR2
to examine correlations to mutation kill for a set of criteria,
and both studies consider realistically non-adequate suite s.
However, their workconsidersonlyasetof15Javaprograms
and 11 C programs, selected not randomly but primarily
from container classes used in previous studies and the clas-
sic Siemens/SIR subjects. Their larger projects (JodaTime,
JFreeChart, SQLLite, YAFFS2) were chosen opportunisti-
cally. Our study is on a much larger scale in terms of sub-
jects and uses a more principled selection process. Most im-
portantly, however, we consider correlation of criteria across
all SUTs, to answer the question“given a suite for an SUT,
which criteria best predicts mutation kills for that SUT?”
rather than to determine, within each SUT, which criteria
best ranks various suites for that SUT.
Gligoric et al. report that branch coverage does the best
job, overall, of predicting the best suite for a given SUT, but
thatacyclicintra-proceduralpathcoverageishighlycompet-
itive and may better address the issue of ties, which is im-
portant in their research/comparison context. Inozemtseva
et al. [21] investigates the relationship of various coverage
measures and mutation score for diﬀerent random subsets
of test suites. They found that when the test suite size
is controlled, only low to moderate correlation is present
between coverage and eﬀectiveness. This conclusion holds
for all kinds of coverage measures used. The diﬀerence in
subjects and focus yields substantially diﬀerent results than
ours, as we discuss below.
Budd et al. [7] proposed mutation testing as a stronger
criteria than other methods for evaluating test suites. Oﬀut
et al. showed that mutation coverage subsumes [29] many
othercriteria, includingthebasicsixproposedbyMyers[27].
Frankl et al. [13] compared the eﬀectiveness of mutation
testing with all-uses coverage, and found that at highest cov-73Project size in LOCFrequency
0 20 40 60 80 100 120
1 10 100 1000 10000 1e+05 1e+06
All
Selectedµ =4574
η =904
σ =18795
µ =2739
η =686
σ =8928
Cyclomatic ComplexityFrequency
0 20 40 60 80 100 120
1 2.718 7.389 20.09
All
Selected
Figure 1: ( log) project parameters distribution before and after selecti on.µis the mean, ηis the median,
andσis the standard deviation of the selected parameter.
Table 1: Symbols used
MMutation score
SStatement coverage
˜SBlock coverage
BBranch coverage
PPath coverage
KProject size in LOC
TTest suite size in LOC
CCyclomatic complexity
erage levels, mutation testing was more eﬀective. Andrews
et al. compared [2] the fault detection ratio and the muta-
tion kill ratio of a large number of test suites, ﬁnding that
the ratios were very similar, and hence the faults induced
by mutation representative of the real faults in programs. A
follow up study [3] using a large number of test suites from
a single program space.cfound that the mutation detection
ratio and the fault detection ratio are related linearly, with
similar results for other coverage criteria (0.83 to 0.9). Lin-
ear regression on the mutation kill ratio and fault detection
ratio showed a high correlation (0.9).
Li et al. [24] compared four diﬀerent criteria (mutation,
edge pair, all uses, and prime path), and showed mutation-
adequate testing was able to detect the most hand seeded
faults (85%), while other criteria were similar to each other
(in the range of 65% detection). Similarly, mutation cover-
age required the fewest test cases to satisfy the adequacy cri-
teria, while prime path coverage required the most. There-
fore, while there are no compellingly large-scale studies of
many SUTs selected in a non-biased way to support the ef-
fectiveness of mutation testing, it is at least highly plausi ble
as a better standard than other criteria.
Frankl and Weiss [12] performed a comparison of branch
coverage and def-use coverage, showing that def-use is more
eﬀective than branch coverage for fault detection and there
is stronger correlation to fault detection for def-use than
branch coverage.Gupta et al. [18] compared the eﬀectiveness and eﬃciency
of block coverage, branch coverage, and condition coverage,
with mutation kill of adequate test suites as their evalu-
ation metric. They found that branch coverage adequacy
was more eﬀective (killed more mutants) than block cov-
erage in all cases, and condition coverage was better than
branch coverage for methods having composite conditional
statements. The reverse, however, was true when consider-
ing the eﬃciency (average number of test cases required to
detect a fault) of suites.
Kakarla [23] and Inozemtseva [22] demonstrated a linear
relationship between mutation detection ratio and coverage
for individual programs. Inozemtseva’s study used machine
learning techniques to come up with a regression relation,
and found that eﬀectiveness is dependent on the number of
methods in a test suite, with a correlation coeﬃcient in the
range 0.81≤r≤0.93. The study also found a moderate-
to-high correlation, with Kendall’s τin the range 0 .61≤
τ≤0.81 between eﬀectiveness and block coverage when test
suite size was ignored, which reduced when test suite size
was accounted for. Kakarla found that statement coverage
was correlated to mutation coverage in the range of 0 .73≤
r≤0.99 and 0 .57≤τ≤0.94.
Wei et al. [35] examined branch coverage as a quality mea-
sureforsuitesfor14Eiﬀelclasses, showingthatforrandomly
generated suites, branch coverage behavior was consistent
across many runs, while fault detection varied widely. Early
in random testing, where branch coverage rises rapidly, cur-
rent branch coverage has high correlation to fault detection,
but branch coverage eventually saturates while fault detec-
tion continues to increase; the correlation at this point be-
came very weak.
Cai et al. [8] investigated correlations between coverage
criteria under diﬀerent testing proﬁles: whole test set, func-
tional test, random test, normal test, and exceptional test.
They investigated block coverage, decision coverage, C-use
and P-use criteria. Curiously, they found that the relation-
ship between block coverage and mutant kills was not always
positive. They found that block coverage and mutant kills74*
***
***
**
***
***
**
**
**
**
*
* **
*
**
****
**
*
*
* ***
****
*
****
* * **
**
* **
*
****
*
*
* * **
**
*
**
* ****
**
***
**
*
****
***
***
*
**
**
*
***
* **
*
**
**
*
* * ******
**
***
**
***
***
*
*
****
***
*
****
**
*
***
****
**
*
*
****
*
**
*
****
*****
***
*
**
**
**
**
*
*** ***
***
***
*
****
****
**
**
*
**
****
***
*
0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0Original
Statement (S)Mutation (M)●
●●●
●●●
●●
●●●
●●●
●●
●●
●●
●●
●
●●●
●
●●
●●●●
●●
●
●
●●●●
●●●●
●
●●●●
●●●●
●●
●●●
●
●●●●
●
●
●●●●
●●
●
●●
●●●●●
●●
●●●
●●
●
●●●●
●●
●
●● ●
●
●●
●●
●
●●●
●●●
●
●●
●●
●
●●●●●●●●
●●
●●●
●●
●●●
●●●
●
●
●●●●
●●●
●
●●●●
●●
●
●●●
●●
●●
●●
●
●
●●●●
●
●●
●
●●●
●
●●
●●●
●●●
●
●●
●●
●●
●●
●
●●●●●●
●●●
●●●
●
●●●●
●●●●
●●
●●
●
●●
●●●●
●●●
●●
●K≈10
K≈104
* **
* ***
**
**** ***
**
***
* * **
**
*
* **
* ***
*
**
*** **
* *****
*
**
***
**
***
***
**
* *****
**
****
**
****
**
**
*
* **
***
*
** ***
*
***
* * ****
***
* * ****
*
*
* ** * ***
*
**
**
* *****
**
***
*** **
***
**
**
**
* * ** * ******
***
***
**
**
* **
**
*
**
**
**
* ***
**
***
**
* * * * * **
* ** ***
**
0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0Generated
Statement (S)Mutation (M)
● ●●
● ●●●
●●
●● ●
●●●●
●●
●●●
● ●●●
●●
●
●●●
●●●●
●
●●
●●
● ●●
● ●●●●●
●
●●
●●●
●●
●●●
●●●
●●
● ●●●●●
●●
●●●●
●●
●●●●
●●
●●
●
● ●●
●●●
●
●● ● ●●
●
●●●
● ● ●●●●
●●●
● ● ●●●●
●
●
● ● ● ● ●●●
●
●●
●●
●●●●●●
●●
●●●
●●
● ●●
●●●
●●
●●
●●
● ● ● ●● ● ●●●
●●
●●●
●●●
●●
●●
● ●●
●●
●
●●
●●
●●
●●●●
●●
●●●
●●
●● ●● ● ●●
● ● ●●●●
●●●
●K≈10
K≈104
Figure 2: Relation between Statement Coverage and Mutation Ki lls. The circle represents the magnitude of
project size.
had a correlation of R2= 0.781 when considering the whole
test suite, but as low as 0 .045 for normal testing and as high
as 0.944 for exceptional testing. The correlation between
decision coverage and mutation kills was higher than state-
ment coverage, for the whole test suite (0 .832), ranging from
normal test (0 .368) to exceptional test (0 .952).
Namin and Andrews [28] also showed that fault detection
ratio (non-linearly) correlated well with block coverage, de-
cision coverage, and two diﬀerent data-ﬂow criteria. Their
researchsuggestedthattestsuitesizewasasigniﬁcantfactor
in the model.
In general, none of this work considered a large, represen-
tative set of open source projects, and many studies con-
sidered the within-SUT suite comparison problem, not the
problem of determining if a single suite provides eﬀective
testing for an SUT, as we do. The variety of reported rank-
ings and correlations of criteria can be highly confusing to
even a researcher wishing to compare suites, much less a
typical (open source) developer seeking to decide if current
testing for a project is eﬀective for fault detection. Many
studies do not even include all of branch, statement, and
block coverage, the most readily available criteria. Our con-
tribution over related work is a study that (1) uses a large
set of open source projects, (2) uses both manually and au-
tomatically generated tests, (3) includes all the criteria of
most interest to developers, and (4) focuses on the critical
question of single-suite evaluation correlation.
3. METHODOLOGY
Our methodology was driven by two primary concerns:
We wanted our results to be applicable to the largest set of
real-world programs possible and based on a diverse set of
actual test suites constructed by developers, not testing re-
searchers. Oursecondconcernwastostriveforastatistically
signiﬁcant result, preferring to keep as many experimental
variables constant as possible. One result of this constraint
was to restrict the study to Java programs. Java is one ofthe most widely used programming languages [14, 34], and
choosing a single language allows us to ensure a consistent
deﬁnition for coverage criteria and avoid any diﬃculties due
to variance in mutation operators. As a consequence, our re-
sults are only directly applicable to projects written in Java
(a large portion of the code written today). However, the
results are likely applicable to other programming languages
with similar structures. Previous studies [16] do not show
majordiﬀerencesbetweencriteriaeﬀectivenessbetweenJava
and C programs, despite Java’s object-oriented nature, in-
clusion of exceptions, the diﬀerent kinds of programs that
tend to be written in C and Java, etc. Inferring criteria ef-
fectiveness for projects not based on C-like languages such
as Java, C, C++, and C# — e.g. for functional languages
— would be less justiﬁed.
Projects were taken from Github [15], one of the largest
public repositories of Java projects. As a concession to ease
of analysis, only projects using the popular Maven [4] build
system were considered. Github provided an initial set of
1,733 projects meeting this criterion. While far from the
entire set of Java projects hosted by Github, there is no
reason it should be biased in terms of test suites. Note that
it is also not the full set of Maven projects, since Github
only returns 99 pages of search results. The process used by
Github to select projects is not public, but we believe it is
orthogonal to our concerns, and likely based on popularity
andrecency. Aftereliminatingprojectsaggregatingmultiple
projects (which are diﬃcult to properly analyze), a set of
1,254 projects remained.
In order to ensure that our results remain free of sys-
tematic bias, we conducted our analysis in two phases. In
the ﬁrst phase, test suites present in the projects (mostly
manually produced, though there may have been some au-
tomatically generated tests included) were used for coverage
and mutation analysis. In the second phase, we generated
test cases using Randoop [30], and performed the same sta-
tistical analysis using these suites. Finally, we compared
the results of our ﬁrst phase to that of the corresponding75*
***
***
**
* **
***
**
**
**
*
* **
*
**
**
**
*
*
* ***
****
*
****
* * **
**
* **
*
****
*
*
* * **
**
*
**
* ****
**
***
**
*
****
***
***
*
**
**
*
***
* **
*
**
**
*
* * ******
**
***
**
***
***
*
*
****
***
*
****
**
*
***
****
**
*
*
****
*
**
****
*****
***
*
***
***
*
*****
**
***
*
****
****
**
**
*
**
****
***
*
0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0Original
Branch (B)Mutation (M)●
●●●
● ●●
●●
●●●
●●●
●●
●●
●●
●
●●●
●
●●
●●
●●
●
●
●●●●
●●●●
●
●●●●
●●●●
●●
●●●
●
●●●●
●
●
●●●●
●●
●
●●
●●●●●
●●
●●●
●●
●
●●●●
●●
●
●●●
●
●●
●●
●
●●●
●●●
●
●●
●●
●
●●●●●●●●
●●
●●●
●●
●●●
●●●
●
●
●●●●
●●●
●
●●●●
●●
●
●●●
●●
●●
●●
●
●
●●●●
●
●●
●●●
●
●●
●●●
●●●
●
●●●
●●●
●
●●●●●
●●
●●●
●
●●●●
●●●●
●●
●●
●
●●
●●●●
●●●
●●
●K≈10
K≈104
* **
****
*
******
*
* * * **
**
*
* **
* ***
*
**
****
* *****
*
**
**
**
*****
**
****
**
*****
****
**
*
*
* **
**
***
*
***
* * ****
***
* * ****
*
*
* *** **
*
**
**
* *****
*
**
****
***
**
**
**
** ** * ******
***
**
***
**
**
*
**
***
***
**
***
**
* * * * **
* * * ***
**
0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0Generated
Branch (B)Mutation (M)
● ●●
●●●●
●
●● ●
●●●
●
● ● ●●●
●●
●
●●●
●●●●
●
●●
●●
●●
●●●●●●
●
●●
●●
●●
●●●
●●
●●
●●●●
●●
●●●●●
●●●●
●●
●
●
●●●
●●
●●●
●
●●●
●● ●●●●
●●●
● ● ●●●●
●
●
●●●●●●
●
●●
●●
●●●●●●
●
●●
●●
●●
●●●
●●
●●
●●
● ● ● ●● ● ●●●
●●
●●●
●●
●●
●
●●
●●
●
●●
●●●
●●●
●●
●●●
●●
●●●●●●
●●●●●●
●●●
●K≈10
K≈104
Figure 3: Relation between Branch Coverage and Mutation Kil ls. The circle represents the magnitude of
project size.
analysis in the second phase. Our aim in conducting this
cross-validation was to ensure that our results would not be
aﬀected by possible bias in manually generated suites. Au-
tomatically generating suites allowed us to have a second,
independent measure.
Inbothphases, wegatheredcoveragemetricsfromEmma[32]
(statement coverage), Cobertura [11] (statement coverage,
branchcoverage), CodeCover[33](statementcoverage, branch
coverage), JMockit [25] (statement coverage, path cover-
age), and PIT [10] (mutation kills). PIT is a tool aimed at
developers rather than researchers, actively developed and
supported, with some penetration in open source testing.
While it is not explicitly mentioned in the JMockit project
page, the path coverage provided by JMockit is similar to
the Acyclic Intra Method Path (AIMP) coverage that per-
formed well in the study by Gligoric et al. [16].
Out of our projects, only 729 projects had test suites.
These were selected for the ﬁrst phase of analysis. The
analysis included running each of the coverage tools over
the selected projects, with some eﬀort expended on ﬁxing
trivial errors. In places where the compilation did not suc-
ceed after some eﬀort, we discarded the project. Further,
we speciﬁed a maximum timeout of 1 hour per project for a
single tool. In the end, using original test cases we had 318
results from Cobertura, 286 results from Emma, 253 results
from CodeCover, 361 from JMockit, and 259 from PIT.
As an example of the selection process, consider mutation
testing. Starting from 729 projects, 273 had compilation
errors, dependency resolution problems, language version
problems, or other fundamental issues preventing a build.
Oftheremainingprojects, 37timedout, requiringmorethan
an hour for mutation testing with PIT. An additional 102
projects had test failures that prevented PIT from running,
and PIT failed to produce any output for 39 projects, pos-
sibly due to no coverage or mutants produced, even though
the build completed successfully.
For the second phase of analysis, we used Randoop, a
feedback-directedrandomtestingtool, oneachoftheprojects,and discarded those where it failed to complete successfully
or timed out. This produced test suites for 437 projects.
From these suites, following the previous procedure, we ob-
tained 314 results from Emma, 323 results from Cobertura,
287 results from CodeCover, 329 results from JMockit, and
243 results from PIT.
From the 437 projects for which we were able to gener-
ate Randoop suites, 66 again had compilation, etc. errors
preventing analysis. Of the remaining, 4 projects timed out
after over an hour with PIT, and 84 had test failures that
prevented PIT from running. PIT failed to produce output
for 51 of the Randoop suites.
The symbols used to indicate various metrics collected
are given in Table 1. The use of ˜Sfor block coverage is
motivated by the observation that block coverage is a kind of
weighted statement coverage; given source code and a CFG,
block coverage can be computed given statement coverage
details and vice-versa.
Since the coverage and mutation process resulted in a
rather drastic reduction in sample space, we compared the
distributions for code size and complexity before and after
selection to verify that our procedure did not inordinately
skew the sample space in at least these dimensions. The size
distribution histograms for both before and after selection
is provided in Figure 1.
Another important dimension in which a bias could ap-
pear is the complexity of programs; perhaps rejection is
much more common with more complex or simpler pro-
grams, which could bias results, since coverage metrics are
intimately tied to code complexity (for very simple pro-
grams, e.g., statement and path coverage are quite similar).
WemeasuredMcCabecyclomaticcomplexity[26]whichpro-
vides a measurement of program complexity by counting the
number of linearly independent execution paths through a
program. The distributions before and after selections are
given in Figure 1. These graphs suggest that selection did
not unduly bias the sample in these two key dimensions.76*
**
***
**
* **
***
**
***
**
* **
**
****
*
*
* ***
****
*
****
* * **
**
* **
*
****
*
*
* * **
**
**
* ****
**
***
**
*
****
**
*
*
**
**
*
***
* **
*
***
*
* * ******
**
** *
**
***
***
*
****
***
*
****
**
*
***
****
*
*
***
*
**
*
****
*****
***
*
***
**
**
*
*** * **
***
**
*
****
****
**
**
*
**
****
**
*
0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0Original
Path (P)Mutation (M)●
●●
●●●
●●
●●●
●●●
●●
●●●
●●
●●●
●●
●●●●
●
●
●●●●
●●●●
●
●●●●
●●●●
●●
●●●
●
●●●●
●
●
●●●●
●●
●●
●●●●●
●●
●●●
●●
●
●●●●
●●
●
●
●●
●●
●
●●●
●●●
●
●●●
●
●●●●●●●●
●●
●● ●
●●
●●●
●●●
●
●●●●
●●●
●
●●●●
●●
●
●●●
●●
●●
●
●
●●●
●
●●
●
●●●
●
●●
●●●
●●●
●
●●●
●●
●●
●
●● ● ●●●
●●●
●●
●
●●●●
●●●●
●●
●●
●
●●
●●●●
●●
●●
●K≈10
K≈104
**
* * **
*
*****
**
**
* ***
*
* **
* ***
**
****
*****
*
**
**
**
*****
**
* ****
**
***
****
**
*
* **
**
** **
*
***
* * ****
**
* * ***
*
*
* ** * ***
*
**
**
* *****
**
***
****
***
**
** *
* * * ** ***
***
***
***
* **
**
*
**
**
**
***
**
****
* * * ***
* * ***
*
0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0Generated
Path (P)Mutation (M)
●●
● ●●●
●
●● ●
●●
●●
●●
● ●●●
●
●●●
●●●●
●●
●●
●●
●●●●●
●
●●
●●
●●
●●●
●●
●●
● ●●●●
●●
●●●
●●●●
●●
●
●●●
●●
●● ●●
●
●●●
● ●●●●●
●●
● ●●●●
●
●
● ●● ● ●●●
●
●●
●●
●●●●●●
●●
●●●
●●
●●
●●●
●●
●● ●
● ● ●●● ●
●●
●●●
●●●
●●
●
● ●●
●●
●
●●
●●
●●
●●●
●●
●●●●
● ● ● ● ●●
●●●●●
●●
●K≈10
K≈104
Figure 4: Relation between Path Coverage and Mutation Kills . The circle represents the magnitude of
project size.
To account for the eﬀects of nondeterminism, we ran each
coverage measurement 10 times, and computed the average.
We also made use of multiple tools, as noted above, to ver-
ify that the coverage reported was accurate — e.g., Emma,
Cobertura, CodeCover and JMockit all give statement cov-
erage. Further, Cobertura and CodeCover provided branch
coverage, and JMockit provided path coverage. Thus, we
could compare most coverages provided by most tools and
ensure they had high correlation to other tools. Further, we
could ensure that the tools were processing all classes and
test cases by examining statement coverage results. This
was important because early on, we found that JMockit was
not including classes that were not covered by any tests in
its calculation of coverage1. Further, we have also removed a
few observations (11 in original suites, 14 in Randoop suites)
where the statement coverage reported by other tools was
zero, and mutation or path coverage was non-zero, as these
indicate some incorrect value from some tool. Our dataset,
which includes metrics for all projects before elimination of
outliers, is available for perusal in Dataverse [17].
4. ANALYSIS
Thepurposeofouranalysisistodeterminewhichcoverage
criteria that are likely to be used by real-world developers
best predict mutation kill ratios. Our analysis also considers
project and test suite size and cyclomatic complexity to de-
termine if these factors aﬀect the utility of coverage criteria.
The scatter-plots for mutation kills and statement cover-
age for both original test suites and Randoop-generated test
suites are shown in Figure 2, with 232 pairs for original test
suites, and 217 pairs for generated test suites. Similarly,
the scatter-plots for mutation kills and branch coverage for
both original test suites and generated suites is given in Fig-
ure 3, with 223 pairs for original test suites, and 191 pairs
for generated test suites. The scatter-plots for path cov-
erage in Figure 4 contain 214 pairs for original test suites,
1http://code.google.com/p/jmockit/issues/detail?id=30 5and 183 pairs for generated test suites. Finally, the scatter
plots between blockcoverage and mutation kills are given
in Figure 5. The diameter of the circles in all scatter plots
correspond to the magnitudes of the project sizes ( log(K)).
The central result of these experiments is generally visible in
these plots: statement coverage appears to give the best pre-
diction of mutation kills of all criteria developers are likely
to use, and this holds for both original and generated test
suites.
We use regression analysis and signiﬁcance testing to as-
certain the contribution of diﬀerent factors to test suite ef-
fectiveness. The correlation coeﬃcient R2indicates the ef-
fectiveness of a model, i.e how much of the variation found
in data is explainable by the parameters of the model. The
factors that were found insigniﬁcant were eliminated to ob-
tain reduced models.
4.1 Statement Coverage and Mutation Kills
In this section, we try to ﬁnd the signiﬁcant factors that,
in combination with statement coverage, predict mutation
kills with a high degree of conﬁdence.
We begin with the saturated model consisting of all vari-
ables. These are mutation score, project size in LOC, test
suite size, cyclomatic complexity, and statement coverage.
Weremovedthetestsuitesizetoavoidmulticollinearitywith
project size after noticing that it correlated very strongly
with the project size. Performing the same analysis with
test suite size in place of project size gave the same results
as below, except that the weak eﬀect for branch coverage
become a stronger eﬀect (but for other criteria, suite size
did not matter). This gives the regression relation:
µ{M|K,C,S}=β0+β1×log(K)+β2×C+β3×S
β0was set to zero since we had suﬃcient coverage data
near the zero point, and zero statement coverage should in-
dicate zero mutation coverage too. This is given in Table 2.77*
**
****
* **
***
**
**
**
**
**
*
**
***
*
*
**
***
*
***
**
*
***
*
**
**
*
**
**
**
**
**
*
****
***
***
*
**
**
*
***
*
**
**
*
*****
**
**
**
***
***
*
****
**
*
****
*
*
**
***
**
*
*
***
*
**
*
****
***
***
*
**
***
**
*
***
**
***
*
****
***
***
**
***
**
0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0Original
Block (S~)Mutation (M)●
●●
●●●●
●●●
●●●
●●
●●
●●
●●
●●
●
●●
●●●
●
●
●●
●●●
●
●●●
●●
●
●●●
●
●●
●●
●
●●
●●
●●
●●
●●
●
●●●●
●●
●
●● ●
●
●●
●●
●
●●●
●
●●
●●
●
●●●●●
●●
●●
●●
●●●
●●●
●
●●●●
●●
●
●●●●
●
●
●●
●
●●
●●
●
●
●●●
●
●●
●
●●●
●
●●●
●●●
●
●●
●●●
●●
●
●●●
●●
●●●
●
●●●●
●●●
●●●
●●
●●●
●●●
●K≈10
K≈104
* **
* * **
***** ***
**
**
* * **
**
*
**
* ***
*
**
*** **
* *****
*
**
***
**
**
***
* *****
**
****
**
****
**
**
*
**
* **
*
** **
*
***
* ****
***
* * ****
*
*
* ** ***
*
**
**
****
****
*** **
***
**
*
**
* * ** *****
**
***
*
**
* **
**
*
**
**
**
* ***
**
****
* * * * **
* ***
**
0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0Generated
Block (S~)Mutation (M)
● ●●
● ●●●
●●● ●
●●●●
●●
●●
● ●●●
●●
●
●●
●●●●
●
●●
●●
● ●●
● ●●●●●
●
●●
●●●
●●
●●
●●●
● ●●●●●
●●
●●●●
●●
●●●●
●●
●●
●
●●
● ●●
●
●
● ●●
●
●●●
● ●●●●
●●●
● ● ●●●●
●
●
●●● ●●●
●
●●
●●
●●●●
●●●●
●●
● ●●
●●●
●●
●
●●
● ● ● ●●●●
●●
●●
●●●
●
●●
● ●●
●●
●
●●
●●
●●
●●●●
●●
●●●
●
● ●● ● ●●
● ●●●
●●●
●K≈10
K≈104
Figure 5: Relation between Block Coverage and Mutation Kill s. The circle represents the magnitude of
project size.
Table 2: Saturated model (Original, Statement)
Estimate Std. Error t value Pr( >|t|)
log(K) 0.00 0.01 0.15 0.88
log(C) -0.01 0.02 -0.64 0.52
S 0.88 0.02 40.88 0.00
Further, we noticed that project size itself did not have a
signiﬁcant contribution to the response variable. Once we
removed project size, our table was updated to Table 3.
Table 3: Second model
Estimate Std. Error t value Pr( >|t|)
log(C) -0.01 0.01 -0.86 0.39
S 0.89 0.02 45.09 0.00
Since cyclomatic complexity was also clearly not signiﬁ-
cant, removing it resulted in the equation
µ{M|S}= 0+β1×S
and the result of this equation is in Table 4.
Table 4: Original: Mutation ×Statement R2=
0.9392
Estimate Std. Error t value Pr( >|t|)
S 0.87 0.01 59.88 0.00
There is no signiﬁcant eﬀect of project size or program
complexity on mutation coverage in a model based on state-
ment coverage.
4.2 Branch Coverage and Mutation Score
In this analysis (Table 5), we follow the same path we took
for statement coverage with mutation coverage. Project size
had a very weak evidence of having an eﬀect on mutation
coverage at p= 0.0672 when compared to statement cover-
age (the eﬀect for suite size here was stronger at 0.0015). β0
was set to zero since we had suﬃcient coverage data near thezero point, and zero branch coverage, again, should indicate
zero mutation kills.
µ{M|B}= 0+β1×B
Table 5: Original: Mutation X Branch R2= 0.9231
Estimate Std. Error t value Pr( >|t|)
B 0.98 0.02 51.76 0.00
4.3 Path Coverage and Mutation Score
Following the same analysis steps (Table 6), we removed
code size and cyclomatic complexity as they were not signif-
icant.β0is again set to zero for the same reasons.
µ{M|P}= 0+β1×P
Table 6: Original: Mutation X Path R2= 0.7496
Estimate Std. Error t value Pr( >|t|)
P 1.27 0.05 25.33 0.00
4.4 Comparing the Criteria
After determining that project size, suite size, and cy-
clomatic complexity were essentially irrelevant for our pur-
poses, we turned to comparing correlation statistics for all
criteria for both original and Randoop-generated tests. In
keeping with the most recent and extensive studies [16] we
report both R2and Kendall τβcorrelations. R2in our con-
text is the most useful correlation measure, since ideally
developers would like to predict the actual mutation killing
eﬀectiveness of a test suite. Kendall τβis a rank-correlation
statistic that is non-parametric, and therefore should be reli-
able even if underlying relationships are not linear — it aims
to answer the question: given that the ranking between two
coverage criteria for suites for projects is such that C(X) >78*
* ****
**
**
*
****
*
**
***
**
**
* **
***
**
****
**
*
**
* ****
***
**
**
**
**
***
**
*
**
* **
**
**
****
****
**
* * **
*
**
**
**
* ****
***
**
**
**
*
***
***
**
****
**
**
**
**
*
***
*
*
***
***
*
*****
*
****
***
***
* ** ****
*
**
*
***
***
***
**
* ****
**
**
*
*
****
**
*****
*
**
**
**
*
****
* *
****
***
***
*
*
*****
**
****
*
* *****
***
***
***
**
* **
**
***
*
**
**
***
***
**
*
*
***
**
***
*
**
*
**
***
**
** **
**
***
**
**
**
**
***
0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0Original
Statement (S)Path (P)
●
● ●●●●
●●
●●
●
●●●●
●
●●
●●●
●●
●●
●●●
●●●
●●
●●●●
●●
●
●●
●●●●●
●●●
●●
●●
●●
●●
●●●
●●
●
●●
●●●
●●
●●
●●●●
●●●●
●●
●●●●
●
●●
●●
●●
●●●●●
●●●
●●
●●
●●
●
●●●
●●●
●●
●
●●●
●●
●●
●●
●●
●
●●●
●
●
●●●
●●●
●
●●
●●●
●
●●●●
●●●
●
●●
●●●●●●●
●
●●
●
●●●
●●●
●●●
●●
● ●●●●
●●
●●
●
●
●●●●
●
●
●●●●●
●
●●
●●
●●
●
●●●●
●●
●●●●
●●●
●●●
●
●
●●●●●
●●
●●●●
●
● ●●●●●
●●●
●●●
●●●
●●
●●●
●●
●●●
●
●●
●●
●●●
●●●
●●
●
●
●
●●
●●
●●●
●
●●
●
●●
●●●
●●
●● ●●
●●
●●●
●●
●●
●●
●●
●●●●
●K≈10
K≈104
*
**
**
**
**
*
****
***
*
*
**
*
**
**
*
** *
**
*****
**
*
*
**
**
*
***
**
**
*
***
*****
*
*
****
**
****
**
**
*
***
*
****
***
*
***
*
****
*****
**
**
**
**
**
***
*
*
***
**
****
*
*
*
***
****
*
*
*
* * **
***
****
****
**
**
**
**
**
* **
*
*
***
***
**
**
*
***
**
**
**
**
*
***
*****
*
*
***
*
**
***
**
**
***
* **
* * ***
***
**
**
****
*
**
***
***
**
*
***
**
*
*
***
*
*
**
**
*
***
***
*
**
***
***
* *
0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0Generated
Statement (S)Path (P)●
●●
●●
●●
●●
●
●●●●
●●●
●
●
●●
●
●●
●●
●
●● ●
●●
●●●●●
●●
●
●
●●
●●
●
●●●
●●
●●
●
●●●
●●●●●
●
●
●●●●
●
●
●●●●
●●
●●
●
●●●
●
●●
●●
●●●
●
●●●
●
●●●●
●●●●●
●●
●●
●●
●●
●●
●●●
●
●
●●●
●●
●
●●●
●
●
●
●●●
●●●●
●
●
●
●● ●●
●●●
●●●●
●●
●●
●●
●●
●●
●●
●●
● ●●
●
●
●●●
●●●
●●
●●
●
●●●
●●
●●
●●
●●
●
●●●
●●●●●
●
●
●●●
●
●●
●
●●
●●
●●
●●●
●●●
● ● ●●●
●●●
●●
●●
●●●●
●
●●
●●●
●● ●
●●
●
●●●
●●
●
●
●●●
●
●
●●
●●
●
●●●
●●●
●
●●
●●●
●●●
● ●●
●K≈10
K≈104
Figure 6: Relation between Statement Coverage and Path Cover age. The circle represents the magnitude of
project size.
C(Y), what is the chance that the ranking of mutation kills
is in agreement with this ranking?
The results from computing R2
adjandτβfor each of the
coverage metrics with mutation coverage are given in Ta-
ble 7. (O) indicates values for original test suites, and (R)
indicates Randoop-generated suites.
Table 7: Correlation coeﬃcients (Mutation),
p <0.001
R2(O)τβ(O)R2(R)τβ(R)
M×S0.94 0.82 0.72 0.54
M×˜S0.93 0.74 0.69 0.48
M×B0.92 0.77 0.65 0.52
M×P0.75 0.67 0.62 0.49
The results are clear: across both original and generated
suites, statement coverage has the best correlation for both
R2andτβ. For predicting mutation kills for test suites in-
cluded with projects, branch, statement, and block coverage
all provide a satisfactory method; predictions for Randoop-
generated suites are more diﬃcult, but statement coverage
still performs relatively well, with suﬃcient power to be use-
ful in practice.
5. WHY STATEMENT COVERAGE?
Some previous research (e.g., as recently as 2013 [16]) sug-
gests the use of branch coverage as the best method for pre-
dicting suite quality. However, this study was conducted
on a small set of programs, a majority of which were algo-
rithms and data-structure implementations, and based on
comparing suites for the same SUT rather than predicting
the quality of testing for each SUT in isolation. Moreover,
allpreviousstudiestendtoincludesomewhatartiﬁcialsuite s
produced by testing researchers, rather than focusing on real
developer-produced test suites for a large variety of projects.
That statement coverage performs so well in fact agrees
with the conclusion of Gligoric et al. [16] that “for non-
Figure 8: Unbalanced branching
adequate suites , criteria that are stronger (in terms of sub-
sumption for adequate suites ) donot necessarily have better
ability to predict mutation scores.” The superiority of state-
ment to branch coverage, however, requires some further
examination. Figure 8 shows a simple portion of a CFG
that may explain this result. A test suite that covers either
of branches A or B would result in branch coverage of 50%.
However, there are more mutations of branch B than branch
A, and (under the assumptions that guide mutation testing)
more chances for coding errors.
Modeling the potential impact of the fact that of the pop-
ular coverages only statement coverage takes into account
the size of a code block predicts the observed results. As-
sume there are nlines of code in a SUT, and let µ(Si) be the
mutability (number of mutants) of the ithstatement. For
the SUT there are then/summationtextn
i=1µ(Si) mutants. This linear re-
lationship is also suggested by our data which shows a high
correlation — R2= 0.96 — between number of statements
andnumberofmutantsproduced. Ifweassumethatamuta-
tion is detected every time a test suite covers the statement,
and we have a constant mutability k, then we can see that of79*
* ****
**
**
*
****
*
**
***
**
* **
***
**
***
**
*
**
* ****
***
**
**
**
**
***
**
*
**
* **
**
**
****
****
**
* * **
*
**
**
**
* ****
***
**
**
**
*
***
***
**
****
**
**
**
**
*
***
*
*
***
***
*
*****
*
****
***
***
* ** ****
*
**
*
***
***
***
**
* ****
**
**
*
*
****
**
*****
*
**
**
**
*
****
**
****
***
* **
*
*
*****
**
***
*
* *****
***
***
***
**
* **
**
**
*
**
**
*****
**
*
*
***
*
***
*
**
*
**
***
**
** **
**
***
**
**
**
**
***
0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0Original
Branch (B)Path (P)
●
● ●●●●
●●
●●
●
●●●●
●
●●
●●●
●●
●●●
●●●
●●
●●●
●●
●
●●
●●● ●●
●●●
●●
●●
●●
●●
●●●
●●
●
●●
●●●
●●
●●
●●●●
●●●●
●●
●●●●
●
●●
●●
●●
●●●●●
●●●
●●
●●
●●
●
●●●
●●●
●●
●
●●●
●●
●●
●●
●●
●
●●●
●
●
●●●
●●●
●
●●
●●●
●
●●●●
●●●
●
●●
●●●●●●●
●
●●
●
●●●
●●●
●●●
●●
● ●●●●
●●
●●
●
●
●●●●
●
●
●●●●●
●
●●
●●
●●
●
●●●●
●●
●●●●
●●●
● ●●
●
●
●●●●●
●●
●●●
●
● ●●●●●
●●●
●●●
●●●
●●
●●●
●●
●●
●
●●
●●
●●●●●
●●
●
●
●
●●
●
●●●
●
●●
●
●●
●●●
●●
●● ●●
●●
●●●
●●
●●
●●
●●
●●●●
●K≈10
K≈104
*
**
**
**
**
*
****
***
**
*
**
**
*
** *
**
*****
**
*
*
**
**
*
***
**
**
*
***
*****
*
*
****
**
****
**
**
*
***
*
****
***
***
*
****
*****
**
**
**
**
**
***
*
*
***
**
***
*
*
*
* **
****
*
*
*
* * ****
****
****
**
**
**
**
**
***
*
*
***
**
**
**
*
***
**
**
**
**
*
**
****
*
*
***
*
**
***
**
**
***
* ** * ***
***
**
**** *
*
****
**
**
*
***
**
*
***
*
*
**
**
*
***
***
*
**
***
***
**
0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0Generated
Branch (B)Path (P)●
●●
●●
●●
●●
●
●●●●
●●●
●●
●
●●
●●
●
●● ●
●●
●●●●●
●●
●
●
●●
●●
●
●●●
●●
●●
●
●●●
●●●●●
●
●
●●●●
●
●
●●●●
●●
●●
●
●●●
●
●●
●●
●●●
●●●
●
●●●●
●●●●●
●●
●●
●●
●●
●●
●●●
●
●
●●●
●●
●
●●
●
●
●
● ●●
●●●●
●
●
●
●● ●●●●
●●●●
●●
●●
●●
●●
●●
●●
●●
●●●
●
●
●●●
●●
●●
●●
●
●●●
●●
●●
●●
●●
●
●●
●●●●
●
●
●●●
●
●●
●
●●
●●
●●
●●●
●●● ● ●●●
●●●
●●
●
●●● ●
●
●●●●
●●
●●
●
●●●
●●
●
●●●
●
●
●●
●●
●
●●●
●●●
●
●●
●●●
●●●
●●●
●K≈10
K≈104
Figure 7: Relation between Branch Coverage and Path Coverag e. The circle represents the magnitude of
project size.
then×kmutants produced, n×c×kwould be detected by
a test suite with cas statement coverage ratio. Under ideal
conditions, mutation kills and statement coverage share a
simple relationship.
This formulation also suggests that branch coverage alone
could not be as closely correlated with mutation kills as
statement coverage unless a model includes some way to in-
corporate the diﬀerence in mutability of program segments,
or the assumption that coverage usually results in detection
is very far from reality, and branch execution is a major fac-
tor in actual detection ratios for most mutants. For the ﬁrst
possibility we draw the readers attention to the fact that
there was a weak project size eﬀect and fairly strong suite
size eﬀect when we considered branch coverage models.
This also suggests that if we consider basic block cover-
age, which is basically statement coverage without mutabil-
ity information, the correlation should also be lower than
the coverage reported by ordinary statement coverage. This
is again borne out by the lower values of R2andτβfor block
coverage ( M×B) in Table 7. Why, though, is block cover-
age sometimes better correlated than branch coverage, when
both criteria ignore mutability of code segments? Branch
coverage can “compensate” for missing a block (which al-
ways contains at least one mutable statement) by taking a
branchthatcontains nomutablecode. Infact,“missingelse”
detection is what distinguishes block and branch coverage.
5.1 Statement Coverage and Path Coverage
Oneresultundercuttingthissimpleexplanationforthesu-
periority of statement coverage, however, is that statement
coverage also better predicts pathcoverage than branch cov-
erage.
Table 8 shows correlations between path coverage, and
statement, branch, and block coverage. Scatter plots of
statement coverage and branch coverage against path cov-
erage are provided in Figure 6 and Figure 7.
These support the conclusion that statement coverage is
better than branch for this purpose also (the winner betweenblock and statement coverage is less obvious, since block
coverage performs better for generated suites).
Table 8: Correlation coeﬃcients (Path), p <0.001
R2(O)τβ(O)R2(R)τβ(R)
P×S0.81 0.68 0.84 0.65
P×˜S0.80 0.59 0.87 0.67
P×B0.80 0.65 0.59 0.45
We do not have any explanation for this eﬀect at this
time, since path coverage, like branch and block coverage,
ignores the size of code blocks. It is possible that executing
more statements leads to producing more unusual execution
states, which results in more covered paths, but this is hard
to model or investigate.
5.2 Correlation at High Coverage Levels
An additional interesting question to consider is whether
the superiority of statement coverage for our purposes is an
artifact of the fairly weak test suites for many SUTs. While
statement coverage is most predictive for projects, actual
utility could be lower than expected, if statement coverage
works best for suites where the question “Is the testing ef-
fective for ﬁnding faults?” is fairly obviously answered with
“no.” Perhaps the better results for branch coverage in pre-
vious studies are due to considering mostly eﬀective test
suites. In order to examine this possibility, we computed
correlationsforonlythosesuiteswithatleast80%statement
coverage, a reasonable threshold for“good”testing. We have
less conﬁdence in our results for this requirement, as only
41 SUTs have original suites with this level of coverage, and
only 14 Randoop-generated suites manage it; however, this
is larger than the set of subjects in many previous studies.
Table 9 shows that even at high levels of statement cover-
age, for the original suites, statement coverage is a better
predictor of mutation kills than branch coverage. In fact,
statement coverage’s R2value is slightly increased for the
original suites. The ability to rank projects in terms of test-80ing eﬀectiveness, however, is (unsurprisingly) considerably
diminished. and block coverage becomes a better predictor
in this sense, but statement coverage remains better than
branch coverage.
Table10showscorrelationswhentherequirementischanged
to branch coverage being at least 80%, which yields only 23
original suites and 7 Randoop-generated suites. Interest-
ingly, in this case statement coverage has a very high R2
for the original suites, and branch coverage has a negative
rank correlation for Randoop suites. On the whole, given
the small number of SUTs meeting strict coverage require-
ments, the claim that statement coverage is more useful in
predicting quality than branch and block coverage, at least
fororiginaltestsuites, doesnotseemtodependonlowcover-
age. Our scatter plots show a vertical line at 80% coverage
to aid in visualizing the correlations at high coverage lev-
els. We also show the best-ﬁt lines for manual test suites in
the Randoop test suite scatter-plots as dashed lines for easy
comparison.
Table 9: Correlation coeﬃcients (S >0.8)
R2(O)τβ(O)R2(R)τβ(R)
M×S0.95 0.46 0.64 0.30
M×˜S0.93 0.51 0.69 0.30
M×B0.92 0.36 0.65 0.11
M×P0.75 0.31 0.62 -0.17
Table 10: Correlation coeﬃcients (B >0.8)
R2(O)τβ(O)R2(R)τβ(R)
M×S0.98 0.53 0.58 0.00
M×˜S0.93 0.54 0.69 0.10
M×B0.92 0.33 0.65 -0.05
M×P0.75 0.25 0.62 0.00
6. THREATS TO V ALIDITY
One of the concessions we were forced to make while mea-
suring various coverage ratios was to restrict the amount of
time each test suite was allowed to run to one hour. While
this did not result in a signiﬁcant elimination of projects,
there is a possibility that it may have biased us against
larger or more complex projects with extremely large and
thorough test suites.
We selected only those projects that compiled and passed
all tests. This may bias us against small one-time projects
where the authors did not have suﬃcient time to do a thor-
ough job of testing, or even against projects under active
development, where some tests represent open bugs.
Our ﬁndings are dependent on the metrics reported by
our tools. We have taken a number of steps, including cross
veriﬁcation and performing multiple experiment runs, to en-
sure that we are not led astray by random noise in results.
However, we do rely on a single mutation testing tool, which
is central to our results. Path coverage results are also some-
what less deﬁnite, as JMockit is less mature than the other
tools, and we do not have a second path coverage tool to
cross-validate results. However, manual examination of re-
sults on multiple simple examples seems to show that the
path coverage reported is correct. Path coverage is in any
case fairly trivial to produce, given the ability to produce
other coverages.Finally, our ﬁndings are restricted to projects using the
Javalanguage, theMavenframework, andastandardproject
layout. Further, the samples came from a single repository
(Github), and are all open source projects. While none of
these factors are obviously confounding, they may limit ap-
plicability in radically diﬀerent settings; (e.g. Haskell de vel-
opment or a commercial software project with an extremely
large user base and a dedicated QA team operating inde-
pendently of developers).
7. CONCLUSION
Mutation testing is one of the best predictors of test suite
quality, in terms of ability to detect actual faults. However,
it is also computationally expensive to run, complex to ap-
ply, and is generally not used by real-world developers with
any frequency. A long-term goal of the testing community
has therefore been to develop alternative methods for pre-
dicting suite quality, for software development purposes and
(perhaps primarily) for use in evaluating competing testing
techniques. Unfortunately, the large body of previous stud-
ies on this topic have largely considered only a small set
of programs, selected opportunistically, sometimes focused
on coverage criteria used as rarely as mutation testing in
real-world projects, and often been constructed around the
question of predicting the best among multiple suites for a
single SUT. In reality, software developers seldom have the
luxury of applying esoteric coverage criteria or choosing be-
tween competing test suites. Rather, given an existing test
suite, they want to estimate whether that suite is likely ef-
fective at detecting faults, or if more testing eﬀort may be
justiﬁed given the cost of faults.
This paper ﬁnds a correlation between lightweight, widely
available coverage criteria (statement, block, branch, and
path coverage) and mutation kills for hundreds of Java pro-
grams, for both the actual test suites included with those
projects and suites generated by the Randoop testing tool.
For both original and generated suites, statement coverage
is the best predictor for mutation kills, and in fact does a
relatively good ( R2= 0.94 for original tests and 0.72 for
generated tests) job of predicting suite quality. SUT size,
code complexity, and suite size do not turn out to be im-
portant. A simple model of mutation and mutation detec-
tion predicts the higher eﬀectiveness of statement coverage,
but does not explain why statement coverage even predicts
path coverage better than branch coverage does, a highly
counter-intuitive result. Moreover, while block coverage be-
came more competitive at high statement coverage levels,
statement coverage still appeared to be the best method for
evaluating high-coverage (80%+ statement coverage) suites,
with an R2of 0.98 for suites with branch coverage ≥80%.
The lesson for software developers is somewhat comfort-
ing: statementcoverageisthemostwidelyavailableandeas-
ily interpreted coverage criteria, and is also the best cover-
age criteria for predicting test suite quality in their context.
The lesson for software testing researchers is that the ques-
tion of how coverage correlates to suite eﬀectiveness likely
has no single correct answer, but must pay careful attention
to the context of application, and the selection of a proper
population of subjects and suites to examine.
8. REFERENCES
[1] P. Ammann and J. Oﬀutt. Introduction to software
testing. Cambridge University Press, 2008.81[2] J. H. Andrews, L. C. Briand, and Y. Labiche. Is
mutation an appropriate tool for testing experiments?
InProceedings of the 27th International Conference on
Software Engineering , pages 402–411. IEEE, 2005.
[3] J. H. Andrews, L. C. Briand, Y. Labiche, and A. S.
Namin. Using mutation analysis for assessing and
comparing testing coverage criteria. IEEE
Transactions on Software Engineering , 32(8):608–624,
2006.
[4] Apache Software Foundation. Apache maven project.
http://maven.apache.org .
[5] T. Ball. A theory of predicate-complete test coverage
and generation, 2004. Technical report, Microsoft
Research Technical Report MSR-TR-2004-28.
[6] T. Ball. A theory of predicate-complete test coverage
and generation. In Formal Methods for Components
and Objects , pages 1–22. Springer, 2005.
[7] T. A. Budd, R. J. Lipton, R. A. DeMillo, and F. G.
Sayward. Mutation analysis . Yale University,
Department of Computer Science, 1979.
[8] X. Cai and M. R. Lyu. The eﬀect of code coverage on
fault detection under diﬀerent testing proﬁles. In ACM
SIGSOFT Software Engineering Notes , volume 30,
pages 1–7. ACM, 2005.
[9] J. J. Chilenski. An investigation of three forms of the
modiﬁed condition decision coverage (MCDC)
criterion. Technical report, DTIC Document, 2001.
[10] H. Coles. Pit mutation testing. http://pittest.org/ .
[11] M. Doliner and Others. Cobertura - a code coverage
utility for java.
http://cobertura.github.io/cobertura .
[12] P. G. Frankl and S. N. Weiss. An experimental
comparison of the eﬀectiveness of branch testing and
data ﬂow testing. IEEE Transactions on Software
Engineering , 19:774–787, 1993.
[13] P. G. Frankl, S. N. Weiss, and C. Hu. All-uses vs
mutation testing: an experimental comparison of
eﬀectiveness. Journal of Systems and Software ,
38(3):235–253, 1997.
[14] GitHub Inc. Github languages.
http://www.github.com/languages .
[15] GitHub Inc. Software repository.
http://www.github.com .
[16] M. Gligoric, A. Groce, C. Zhang, R. Sharma, M. A.
Alipour, and D. Marinov. Comparing non-adequate
test suites using coverage criteria. In ACM
International Symposium on Software Testing and
Analysis. ACM, 2013.
[17] R. Gopinath. Replication data for: Code coverage for
suite evaluation by developers. In
http://dx.doi.org/10.7910/DVN/24574 . Harvard
Dataverse Network V1, 2014-01.
[18] A. Gupta and P. Jalote. An approach for
experimentally evaluating eﬀectiveness and eﬃciency
of coverage criteria for software testing. International
Journal on Software Tools for Technology Transfer ,
10(2):145–160, 2008.[19] M. M. Hassan and J. H. Andrews. Comparing
multi-point stride coverage and dataﬂow coverage. In
Proceedings of the 2013 International Conference on
Software Engineering , pages 172–181. IEEE Press,
2013.
[20] M. Hutchins, H. Foster, T. Goradia, and T. Ostrand.
Experiments of the eﬀectiveness of dataﬂow-and
controlﬂow-based test adequacy criteria. In
Proceedings of the 16th international conference on
Software engineering , pages 191–200. IEEE Computer
Society Press, 1994.
[21] L. Inozemtseva and R. Holmes. Coverage is not
strongly correlated with test suite eﬀectiveness. In
Proceedings of the 2014 International Conference on
Software Engineering , 2014.
[22] L. M. M. Inozemtseva. Predicting test suite
eﬀectiveness for java programs. Master’s thesis,
University of Waterloo, 2012.
[23] S. Kakarla. An analysis of parameters inﬂuencing test
suite eﬀectiveness. Master’s thesis, Texas Tech
University, 2010.
[24] N. Li, U. Praphamontripong, and J. Oﬀutt. An
experimental comparison of four unit test criteria:
Mutation, edge-pair, all-uses and prime path coverage.
InInternational Conference on Software Testing,
Veriﬁcation and Validation Workshops, 2009.
ICSTW’09. , pages 220–229. IEEE, 2009.
[25] R. Liesenfeld. JMockit - A developer testing toolkit
for Java. http://code.google.com/p/jmockit/ .
[26] T. J. McCabe. A complexity measure. IEEE
Transactions on Software Engineering , (4):308–320,
1976.
[27] G. J. Myers. The art of software testing. A
Willy-Interscience Pub , 1979.
[28] A. S. Namin and J. H. Andrews. The inﬂuence of size
and coverage on test suite eﬀectiveness. In Proceedings
of the eighteenth international symposium on Software
testing and analysis , pages 57–68. ACM, 2009.
[29] A. J. Oﬀutt and J. M. Voas. Subsumption of condition
coverage techniques by mutation testing. Technical
report, 1996.
[30] C. Pacheco and M. D. Ernst. Randoop random test
generation. http://code.google.com/p/randoop .
[31] C. Pacheco, S. K. Lahiri, M. D. Ernst, and T. Ball.
Feedback-directed random test generation. In
Proceedings of the 29th International Conference on
Software Engineering , pages 75–84. IEEE, 2007.
[32] V. Roubtsov and Others. Emma - a free java code
coverage tool. http://emma.sourceforge.net/ .
[33] R. Schmidberger and Others. Codecover - an
open-source glass-box testing tool.
http://codecover.org/ .
[34] TIOBE. Tiobe index. http://www.tiobe.com/index.
php/content/paperinfo/tpci/index.html .
[35] Y. Wei, B. Meyer, and M. Oriol. Is branch coverage a
good measure of testing eﬀectiveness? In Empirical
Software Engineering and Veriﬁcation , pages 194–212.
Springer, 2012.82