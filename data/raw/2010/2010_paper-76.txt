An Empirical Study of Optimizations in YOGI
Aditya V. Nori
Microsoft Research India
adityan@microsoft.comSriram K. Rajamani
Microsoft Research India
sriram@microsoft.com
ABSTRACT
Though verication tools are nding industrial use, the util-
ity of engineering optimizations that make them scalable and
usable is not widely known. Despite the fact that several
optimizations are part of folklore in the communities that
develop these tools, no rigorous evaluation of these optimiza-
tions has been done before. We describe and evaluate several
engineering optimizations implemented in the Yogi prop-
erty checking tool, including techniques to pick an initial
abstraction, heuristics to pick predicates for renement, op-
timizations for interprocedural analysis, and optimizations
for testing. We believe that our empirical evaluation gives
the verication community useful information about which
optimizations they could implement in their tools, and what
gains they can realistically expect from these optimizations.
Categories and Subject Descriptors
D.2.4 [ Software Engineering ]: Software/Program Veri-
cation| Correctness proofs, Model checking ; D.2.5 [ Software
Engineering ]: Testing Tools| Symbolic execution
General Terms
Testing, Verication
Keywords
Software model checking; Directed testing; Abstraction re-
nement
1. INTRODUCTION
Over the past decade we have seen several program ver-
iers and static analysis tools [5, 2, 1, 6] used in industrial
practice. The algorithms implemented in these tools are
available in published literature, and the principles behind
these algorithms are well understood. However, the scala-
bility and usability of these tools relies as much on careful
engineering of optimization techniques as on the algorithms.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proï¬t or commercial advantage and that copies
bear this notice and the full citation on the ï¬rst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciï¬c
permission and/or a fee.
ICSE â€™10, May 2-8 2010, Cape Town, South Africa
Copyright 2010 ACM 978-1-60558-719-6/10/05 ...$10.00.Thus, it is important for the research community to under-
stand which optimizations work well in practice, and how
much benet each optimization brings.
Knowledge about which optimizations work in practice is
less widely known, since papers about engineering optimiza-
tions are less fashionable than papers about new algorithms.
In addition, several of the tools mentioned above are com-
mercial tools and source code for these tools is not available
to the research community at large. As a result, it is very dif-
cult for the community to replicate these verication tools.
Thorough evaluation of optimizations will help a researcher
building a new verication tool decide which optimizations
to implement.
Part of the diculty in evaluating optimizations is that
it is very dicult to design verication tools that are bug
free! For example, a bug in an optimization technique for a
model checker could lead to the state space being pruned in
an unsound manner. As a result, the model checker could
show impressive performance gains, but the bug in the opti-
mization could just remain unnoticed. Thus, measurements
about the eciency of optimizations can be trusted only
after the verication tool itself has been tested extensively.
The goal of this paper is to empirically measure the op-
timizations implemented in the tool Yogi [13]. Yogi is a
program verier that checks safety properties of sequential C
programs. Yogi has the capability to certify whether a pro-
gram satises a property, and it can generate a test case to
demonstrate that a program violates a property. Over the
past year, Yogi has been extensively tested using bench-
mark suites for the Static Driver Verier toolkit ( Sdv) [1].
The testing team for Sdvhas created several test suites over
the years. One test suite consists of 619 small examples that
exemplies tool errors that have surfaced in the past. An-
other test suite contains 30 device drivers, and 83 properties,
and hence a total of 2490 runs. The testing team has thor-
oughly analyzed the test suite, and determined the correct
answer to each of these runs. Since Yogi has been exten-
sively tested using these test suites, and the results inspected
by several people, we have sucient condence about the
correctness of Yogi , and believe that these test suites can
be used to empirically measure the eect of optimizations.
The optimizations themselves are not novel. Several of the
optimizations are part of the folklore in the software model
checking community, and have been implemented before in
tools such as Slam [2] and Blast [10]. However, they have
never really been thoroughly evaluated before, which is the
main contribution of this paper.
The core premise behind Yogi is that static vericationcan be combined with testing to improve eciency. The al-
gorithms in Yogi have evolved over time. We rst presented
Synergy [9] which is the core algorithm, but it worked only
for single-procedure Cprograms without pointers. Next,
we discovered a new way to deal with pointers using the
Dash algorithm [4]. More recently, we have added a way
to perform interprocedural analysis such that information
is reused from both tests and proofs using the Smash algo-
rithm [8]. Orthogonal to the algorithmic development, we
have implemented a number of optimizations to make Yogi
scale to large and realistic programs. In this paper, we de-
scribe these optimizations and evaluate each of these opti-
mizations empirically. In particular, we evaluate the eect
of the following optimizations:
Initial abstraction using predicates from the property.
Relevance heuristics to help pick suitable predicates
for renement.
Optimizations for interprocedural analysis, including
global modication analysis, and summaries for proce-
dures.
Using thresholding to limit the number of steps to run
tests.
Fine tuning stubs to make them executable so that
Yogi can work correctly.
We present detailed empirical results for the eect of each of
these optimizations over a large number of sample programs.
We believe that these optimizations represent a bulk of the
engineering we had to do in order to make Yogi scale and
work on large programs.
2. OVERVIEW
A detailed description of Yogi 's algorithms can be found
in [9, 4, 8]. Here, we give an overview of the algorithm with
just enough detail to allow us to describe the optimizations,
which are the subject of this paper.
The goal of Yogi is to solve the property checking prob-
lem. An instance of the property checking problem consists
of a sequential program Pand an assertion '.1An answer
to the property checking problem is \pass" if every run of P
satises the property '. The answer is \fail" if there exists
some run of Pthat violates '. In the latter case, the goal of
Yogi is to come up with a test input tsuch that the run of
Pobtained by giving input tviolates'. Since the property
checking problem is undecidable in general, it is not possi-
ble for Yogi to give correct \pass" or \fail" answers to all
instances of this problem. Specically, Yogi might fail to
terminate for certain problem instances.2
Static analysis and testing are complementary approaches
to the property checking problem. With static analysis, we
can obtain very good coverage and analyze program paths
that are hard to exercise using testing, but we are forced
1Yogi can also check safety properties expressed in the
Slic [3] language. However, these can be compiled down
to assertions, so there is no loss of generality by considering
assertions.
2In practice, we stop every run of Yogi after 1000 seconds
and say that Yogi is unable to give a conclusive answer to
the property checking instance.
Figure 1: The Yogi algorithm.
to deal with scalability issues and false errors. With run-
time testing, we can obtain only partial coverage, but the
approach scales to large programs and every error that is
reported is indeed realizable.
Yogi 's algorithm combines static analysis (based on ab-
stractions, which are region graphs) together with testing
(based on light-weight symbolic execution). Yogi simul-
taneously maintains a nite set of test runs and a nite
abstraction of the program. A test (or test run) is a -
nite sequence of program states starting from some initial
program state. An abstraction is a nite graph whose ver-
tices are regions, and whose edges are over-approximations
of transitions between states. Each region represents an in-
nite set of states. An edge exists in the abstraction from
region1to region2whenever there exists states s21
andt22such that there is a transition from stotin
the program. The set of initial states are grouped together
to form an \initial" region, and the set of states that vio-
late an assertion 'are grouped together to form an \error"
region in the abstraction. A set of tests forms an under-
approximation of the set of all runs of the program, since
every test is a run, and not all runs are represented in the
set of tests. A region graph forms an over-approximation of
the set of all runs of the program, since every run has a cor-
responding path in the abstraction, whereas every path in
the region graph need not necessarily have a corresponding
run. The goal of Yogi is to either nd a test that starts
from the initial region and ends in the error region, or nd
an abstraction that is strong enough to show that no such
test exists.
A high-level view of Yogi 's algorithm is shown in Fig-
ure 1. Yogi starts with an initial abstraction (which can
either be the control ow graph of the program or its rene-
ment that is based on some predicates from the property, as
described below), and a set of randomly selected tests. Dur-
ing every iteration, if a test has managed to reach the error
region, Yogi declares \test succeeded" and outputs the bug
found. If no path in the abstract region graph exists fromthe initial region to the error region, Yogi declares \ab-
straction succeeded" and outputs the current abstraction as
the proof that the program satises the property. If neither
of the above two cases are true, then Yogi nds a path 
in the abstraction from the initial region to the error re-
gion, along which a test can be potentially driven to reveal
a bug. Yogi crucially relies on the notion of a frontier [9,
4], which is the boundary between tested and untested re-
gions along the abstract counterexample that a test has
managed to reach. In every iteration, the algorithm rst
attempts to extend the frontier using test case generation
techniques similar to Dart [7]. If test case generation fails,
then the algorithm renes the abstract region graph so as
to eliminate the abstract counterexample. For performing
renement, the Dash algorithm uses a new renement oper-
ator WPwhich is the usual weakest precondition operator
that uses information from executed tests in order to com-
pute sound and succinct renement predicate [4].
We have now provided enough background to describe the
optimizations studied in this paper.
The rst set of optimizations are concerned with creating
an initial abstraction that is better than the control ow
graph of the program. We use predicates from the Slic
property to split regions of the control ow graph and create
an initial abstraction. We can also choose other predicates
to split based on dataow dependencies on these predicates.
However, the more the initial predicates, the greater is the
cost (in terms of both time and space) to create the initial
abstraction. This tradeo is evaluated in Section 4.
The second set of optimizations are concerned with choos-
ing relevant predicates for rening the abstraction. Here
we study two optimizations, one that performs cheap in-
terpolant computation, and another that uses domination
relationships in the control ow graph to eliminate predi-
cates from the program using a pre-pass (with the caveat
that some of these removed predicates might be replaced
later if necessary). These optimizations are described and
evaluated in Section 5.
The third set of optimizations are concerned with inter-
procedural analysis. Techniques such as global modication
analysis, as well as storing and re-using summaries from each
analysis of a procedure can greatly improve the eciency of
interprocedural analysis. These optimizations are described
and evaluated in Section 6.
The fourth set of optimizations are concerned with how
long tests need to be run to be eective. Every transition
computed using a test can potentially save a theorem prover
call in the future. However, running tests also costs in terms
of time and space. Section 7 evaluates the best parameter
setting for how long to run tests.
The nal set of optimizations are concerned with writing
accurate stub functions. We painfully discovered that writ-
ing stub functions for Yogi is more challenging than writing
stub functions for tools based on purely static analysis such
asSlam orBlast . The main reason is that the stub func-
tions for Yogi need to be precise enough to be executable for
the purpose of running tests. Section 8 measures the amount
of eort needed to produce executable stub functions on our
benchmark test suites.
3. EVALUATION SETUP
Currently, the Windows driver group is evaluating Yogi
as an engine to be used within Microsoft's Static Driver Ver-ier toolkit [1]. As a part of this evaluation, Yogi has been
fully integrated as the property checking engine inside Sdv
(Static Driver Verier). Slam is another property checking
engine for Sdv. One of the signicant advantages of this
integration is that it is possible to run Yogi over large test
suites that are maintained and evolved by the Windows test
team. As a part of this evaluation, the Windows team has
extensively compared results produced by Slam andYogi
over this test suite. Discrepancies between the outputs of
the tools have been tracked and xed.
For the purposes of this paper, we consider two benchmark
test suites from the Sdv toolkit:
1.C test suite: The rst test suite consists of 619 Cpro-
grams that were created to test tool errors that have
surfaced in SDV over the past 10 years. This suite
(called SmallC suite) consists of 386 positive tests and
233 negative tests. Yogi takes approximately 20 min-
utes to run on this suite.
2.WDM Drivers: The second suite (called WDM suite)
consists of 30 device drivers written for the Windows
Driver Model ( WDM ) and 83 properties, a total of 2490
checks, also developed over the past 10 years. The
total number of lines of code analyzed is 250 KLOC
(but each driver is analyzed repeatedly 83 times for
each of the properties), and Yogi takes approximately
36 hours to run on this suite on a single processor.3
For every run in these suites, the expected result (that
is, the property should pass, or property should fail) has
been manually established by the Windows group over the
years. Sdv itself is routinely run over several millions of
lines of driver code, including drivers for the new Kernel
Mode Driver Framework and User Mode Driver Framework
(called KMDF andUMDF respectively), networking drivers
that use the NDIS driver model, etc. The anecdotal belief
in the Windows group is that any tool bugs that surface
in these bigger runs are usually caught in the above two
test suites [11]. Our own experience with these test suites
corroborates this belief |whenever we make a mistake in
implementing an optimization, these tests usually nd them
(it manifests as some run where the buggy optimization says
that the run passes, but the property check is expected to
fail on the run).
In this paper, we measure the eectiveness of an optimiza-
tion by evaluating Yogi with and without the optimization
on the WDM benchmark suite. We do not report perfor-
mance numbers on the SmallC suite because the runs in this
suite nish quickly even without any optimizations, and the
optimizations have no eect on the runtime. However, the
SmallC suite is very eective in catching bugs in the opti-
mizations. We have tested each optimization we report in
the paper on SmallC suite to conrm that we have not im-
plemented the optimization erroneously.
Our earlier papers report comparisons between Yogi and
Slam [4, 13]. As a result of valuations over several hun-
dreds of thousands of lines of code, extensive comparison of
results between Yogi andSlam , and extensive comparison
between results produced by Yogi and results expected by
theSdv team in these test suites, we believe that Yogi is
3Since each check is independent we run this on an 8-core
machine usually, to get the runs to complete quickly.1 state {
2 enum {Locked = 0, Unlocked = 1} state = Unlocked;
3 }
4
5 KeAcquireCancelSpinLock.Entry{
6 if (state == Unlocked) {
7 state = Locked;
8 }
9 else
10 abort;
11 }
12
13 KeReleaseCancelSpinLock.Entry{
14 if (state == Locked) {
15 state = Unocked;
16 }
17 else
18 abort;
19 }
Figure 2: Slic specication for CancelSpinLock
property.
currently a stable and robust tool, and we have condence in
the correctness of all the optimizations we report in this pa-
per. Without such extensive testing, empirical evaluations
of optimizations could be awed, since buggy optimizations
can boost performance due to unsound pruning of the state
space of the program being analyzed.
3.1 Presentation methodology
All evaluations are done using 2490 runs of Yogi on the
WDM suite. We make a few remarks about how we present
the results of our evaluations. We group the optimizations
logically so that interrelated optimizations are in the same
group. Each of the subsequent sections presents a descrip-
tion and evaluation for one group of optimizations. For each
group, we present two sets of empirical data. We present
aggregate numbers for total time taken, total number of de-
fects found, and total number of defects found for every pos-
sible choice of enabling/disabling each optimization in the
group. For instance, Table 2 presents aggregate data for rel-
evance heuristics. Since there are two relevance heuristics,
we consider 4 possible choices, with or without each rele-
vance heuristic. Thus, there are 4 rows of data in Table 2.
In addition, we present scatter plots for comparing runtimes
of individual runs between these 4 possible choices. As a
convention, the x-axis of all scatter plots represent runtimes
with all optimizations present . In the y-axis of each plot, we
consider various choices where at least one optimization is
absent . For example, in Section 5, we evaluate two optimiza-
tions SPandCDand show the eect of these optimization in
3 scatter plots (Figure 6, Figure 7 and Figure 8). The x-axis
represents Yogi 's runtimes with both the SPandCDopti-
mizations, and the y-axis in the each of the gures represents
Yogi 's runtimes for the other three possible combinations
of these optimizations { (without SP, with CD), (with SP,
without CD) and (without SP, without CD). If most points
in the scatter plots lie above the line ( x=y), then this is
an indication that the optimization is eective.
4. INITIAL ABSTRACTION
One possible choice for the initial abstraction used by
Yogi is the control ow graph of the program. Often,
we nd that splitting the abstraction based on predicates
present in the conditionals of the Slic property greatly helpsAbstraction using total time no. no.
Slicpredicates (minutes) defects timeouts
yes 2160 241 77
no 2580 241 86
Table 1: Empirical evaluation of initial abstraction
using Slicpredicates using the WDM suite.
avoid exploring false counterexamples in the abstraction.
For example, consider the Slic property shown in Fig-
ure 2, which species constraints on sequences of opera-
tions on cancel-spin-locks in the Windows kernel. At a high
level, the property states that calls to KeAcquireCancel-
SpinLock and KeReleaseCancelSpinLock need to be per-
formed in strict alternation. A detailed description of the
syntax and semantics of Slic can be found in [3]. For the
purposes of this paper, think of a Slic property as an ob-
ject with some state (here the state is an enum type which
can take one of two values Locked orUnlocked ), and in-
strumentation added at appropriate points in the program
to methods that manipulate the state (here instrumentation
is added to the entry of calls to KeAcquireCancelSpinLock
and KeReleaseCancelSpinLock ).
Suppose we use Yogi to check this property on a large pro-
gram. Suppose further that the program indeed satises the
property. We nd that an abstraction to establish this prop-
erty needs to split all regions with the predicates (state ==
Locked) and (state == Unlocked) . These predicates are
typically discovered by Yogi through several iterations of
counterexample driven renement. Thus, it makes sense to
juts add them upfront and compute an initial abstraction
in which each region in the abstraction is split with these
predicates, motivating this optimization. However, there
are several subtle choices in implementing this optimization.
Sometimes, predicates in conditionals of Slic properties de-
pend on argument or return values to function calls that
they are instrumenting. In such cases, it is sometimes useful
to propagate the predicates to the actual parameters or the
variables that gets the return value assigned at the call site.
We experimented with adding such predicates as well, and
we found that this resulted in actually slowing down Yogi
substantially. Thus, the \sweet spot" we have arrived at is
to introduce predicates based on Slic predicates that do not
depend on parameters or return values in the initial abstrac-
tion. We present empirical results to show this \sweet-spot"
produces performance benets.
4.1 Empirical Results
Table 1 shows aggregate metrics for running Yogi over
theWDM suite with and without initial abstraction using
predicates from Slic. The rst row of the table shows data
from the run with the initial abstraction optimization, and
the second row of the table shows data from the run with-
out this optimization. As the results show, the time taken
byYogi improves 16% due to this optimization, and the
number of timeouts (with timeout threshold equal to 1000
seconds) reduces considerably as well.
To drill down into these aggregate results further, Figure 3
shows comparisons of runtimes of individual runs of Yogi
with and without this optimization. This plot shows that
while most runtimes improve due to the optimization (most
points are above the straight line x=y), there are severalFigure 3: Evaluating the impact of Slicabstraction
onYogi.
cases that degrade in runtime as well. But, the degradation
never leads to a timeout, and there are several runs that
timeout without the optimization that indeed complete due
to the optimization. Thus, the overall eect of the optimiza-
tion is very positive, and we have included it in Yogi .
5. RELEVANCE HEURISTICS
Yogi performs a renement of the abstraction whenever
a test cannot be extended across a frontier. Figure 4 il-
lustrates how Yogi performs renement. The top picture
in the gure shows the frontier between regions Sk 1and
Sk, where a test has reached the region Sk 1along a path
through region Sk 2. The state reached by the test is de-
noted by \" in the gure. Let denote the set of states
obtained by performing forward symbolic execution along
this test. Note that the set includes the state \ ". Sup-
pose that this test cannot be extended along the frontier.
Then, Yogi performs a renement as shown in the bottom
picture of the gure. In particular, Yogi splits the region
Sk 1using a predicate such that)Sk 1^:, and
there is no edge in the abstraction from Sk 1^:toSk.
Such a predicate is called a suitable predicate . The qual-
ity of suitable predicates inferred during verication (also
related to the problem of computing invariants in program
verication) is important as it can critically aect both the
eciency as well as the precision of Yogi [9, 4].
5.1 SP heuristic
As discussed in [4], Yogi uses the WPoperator to com-
pute the suitable predicate .WPcan be used to avoid con-
sidering an exponential number of aliasing conditions while
performing weakest preconditions over assignments. Thus,
if the operator opin Figure 4 on the frontier is an assign-
ment statement, WPis very helpful in producing a sim-
ple predicate . However, if the operator opis of the form
assume ('), we have that WP(op;Sk) ='^Sk. Thus,
theWPoperator accumulates conditions from all assume
statements along the entire path. Several of the condition-
als along the path may be irrelevant to the property we are
trying to prove.
In general, any interpolant [12] between and'^Sk
would do as a suitable predicate. Our theorem prover Z3
does not yet support computation of interpolants. We have
discovered a simple relevance heuristic, called the SPheuris-
tic, to avoid accumulating irrelevant conjuncts in the rele-
Figure 4: Renement using suitable predicates.
vant predicate. The heuristic is as follows. We rst check if
Sk 1is a suitable predicate. If it is, we use Sk. Otherwise,
we use'^Skas a suitable predicate.
5.2 CD heuristic
Though the SPheuristic is useful, it still does not fully
avoid irrelevant conjuncts in suitable predicates used for re-
nement. We have found another orthogonal heuristic based
on control-dependencies. We pre-process the input program
Pas follows. For each assume statement in the program4,
we mark the assume statement as potentially relevant if
some Slic function is control dependent on the assume
statement. Then, we make one pass over the input pro-
gramPand abstract every assume statement that is not
potentially relevant using a skip statement, producing an
abstract program P0. Next, we give the abstract program
P0toYogi for checking. If Yogi is able to prove that P0
satises the desired property, then we have that Psatises
the property as well. However, if Yogi establishes that P0
does not satisfy the desired property, and produces a test
tthat executes along some control path to demonstrate
this, we are still not sure if Pviolates the property. Thus,
we execute the path along the original program Pto check
if it is feasible. If it is indeed feasible, we report a bug in P.
If not, we add all assume statements that occur along the
pathtoP0and rerun Yogi onP0. Note that it is possible
that only a subset of the assume statements might suce
to explain the infeasibility of the path , but this subset is
hard to compute in general. However, we nd that adding
4A preprocessing step in Yogi converts all if-then-else, and
looping constructs to assume statements.1 int x;
2 void foo() {
3 bool protect = true;
4 ...
5 if(x>0)
6 protect = false;
7 ...
8 if(protect)
9 KeAcquireCancelSpinLock();
10
11 for(i = 0; i < 1000; i++){
12 // do stuff, but no calls to
13 // acquire or release CancelSpinLock
14 a[i] = readByte(i);
15 }
16
17 if(protect)
18 KeReleaseCancelSpinLock();
19 }
Figure 5: Example to illustrate relevance heuristics.
SP CD total time no. no.
heuristic heuristic (minutes) defects timeouts
yes yes 2160 241 77
yes no 2580 239 91
no yes 2400 238 87
no no 2894 235 174
Table 2: Empirical evaluation of relevance heuristics
inYogi using the WDM suite.
all assume statements along an infeasible path works well in
practice.
5.3 Example
Figure 5 shows an example to illustrate the SPand CD
heuristics. Assume that the property we are interested in
is the one about proper acquisition and release of Cancel-
SpinLock from Figure 2. In this example, the call to KeAc-
quireCancelSpinLock at line 9, and the call to KeRelease-
CancelSpinlock are guarded by conditionals which checks
if the boolean protect is true. The other conditionals in
the program, such as the check (x > 0) in line 5, and the
check (i < 1000) in line 11 are irrelevant for proving the
property. The SPheuristic is able to abstract the condition-
als in lines 5 and 11 because there is no call to any of the
Slic functions (which, in this case are KeAcquireCancel-
SpinLock and KeReleaseCancelSpinLock ) that are control
dependent on these conditionals. The SPheuristic preserves
the conditional at lines 8 and 17 since the calls at lines 9 and
lines 18 to the Slic functions are control dependent on the
conditionals at line 8 and line 17 respectively.
To illustrate the SPheuristic, suppose Yogi needs to per-
form a renement at line 11. Referring to Figure 4, sup-
poseSkis the region (state = Locked) and opis the op-
eration assume(i > 1000) , which is from the loop check at
line 11. The weakest precondition operator WPin this case
will yield the conjunction of the two predicates (state =
Locked) and (i > 1000) . The SPheuristic is able to elimi-
nate the conjunct (i > 1000) from the renement predicate
and simplify the abstraction constructed.
5.4 Empirical Results
Table 2 gives aggregate metrics obtained by running Yogi
over the WDM suite. As seen by comparing the rst and sec-
ond rows of Table 2, we see that SPheuristic improves overall
Figure 6: Comparison of Yogi runtimes with and
without the SPrelevance heuristic.
Figure 7: Comparison of Yogi runtimes with and
without the CDrelevance heuristic.
runtime of Yogi on the WDM suite by 16% . By comparing
the rst and third rows, we see that CDheuristic improves
overall runtime of Yogi on the WDM suite by 10% . By
comparing the rst and fourth rows, we see that the com-
bination of SPand CDheuristics improves the runtime of
Yogi on the WDM suite by 25% .
To drill down into the individual runs better, Figure 6,
Figure 7 and Figure 8 compares runtimes from individual
runs of Yogi with and without the SPandCDheuristics.
As seen in Figure 6 most points are close to the ( x=
y) line, which shows that the SPheuristic does not really
aect a large number of runs. This is actually due to the
fact that the runtime reported for the x-axis includes the
CDheuristic, which mitigates the eect of accumulation of
predicates. However, there are still several runs, where the
SPheuristic helps, and there are very few runs where the
SPheuristic increases the runtime.
As seen in Figure 7 most points are well above the ( x=y)
line, and there are some points below the line. We investi-
gated the points below the line, and realized that these are
because the CDheuristic sometimes aggressively abstracts
the predicates. Sometimes, even if a predicate in a condi-
tional does not directly inuence the execution of a Slic
function, it could aect it indirectly by controlling assign-
ments to some other variable, which could later inuence the
Slic function. In such cases, we nd that more iterations
need to be spent by putting these predicates back into the
program, and Yogi runs slower.Figure 8: Comparison of Yogi runtimes with and
without both SPand CDrelevance heuristics.
However, as Figure 8 shows the combination of the two
heuristics works very well, and there are only a negligible
number of points below the ( x=y) line. As a result, both
these optimizations are enabled by default in Yogi .
6. INTERPROCEDURAL ANALYSIS
Yogi performs modular analysis, one procedure at a time.
Every query to Yogi is of the formh'1;P;' 2iwhich means
\Is it possible to execute Pstarting with some state from '1
and reach a state in '2?".Yogi performs optimizations to
reduce the cost of interprocedural analysis, and we describe
these below.
Interprocedural analysis in Yogi arises in the case when
the frontier is at a procedure call. Referring back to Figure 4,
suppose opis a call to a function, say foo. Then, Yogi
attempts to extend the test across the frontier (which is the
function call), or attempts to perform a renement of the
regionSk 1. Recall that is the subset of states in region
Sk 1that are obtained by performing symbolic execution
along the test that reaches Sk 1. As before, we desire to
pick some state from such that it crosses the frontier,
or establish that none of the states from can cross the
frontier and perform a renement. Yogi approaches this
problem by performing a sub-query h;foo;Ski. To answer
this sub-query, Yogi performs the following steps:
1. First check if the query can be answered using global
modication analysis. Using alias analysis, Yogi pre-
computes for each procedure Qan over-approximation
to the set of all locations that Qcan modify. Using
this information Yogi nds the weakest precondition
ofSkwith respect to the procedure foo. In particular,
it analyzes every l-value in the formula representing Sk
and nds out which of these l-values can be modied
byfoo.Yogi then constructs an over-approximation
to the call foowhere it assumes that every l-value that
can be modied can take an arbitrary (nondetermin-
istic) value. Using this over-approximation, suppose
Yogi is able to prove that from the set of states ex-
ecution of foocan never reach any state in Sk. Then,
this is a sound answer, and Yogi uses this to rene
the regionSk 1. In particular, a suitable predicate is
found by computing WPof the region Skwith respect
to this over-approximation, just as in the interprocedu-
ral case. If the over-approximation fails to produce aModication Summaries total time no. no.
analysis (minutes) defects timeouts
yes yes 2160 241 77
yes no 2760 239 109
no yes 3180 237 134
no no 3780 236 165
Table 3: Empirical evaluation of interprocedural
analysis optimizations with Yogi.
suitable predicate for renement, then Yogi generates
a test that crosses the frontier using the above over-
approximation for the call (using modication analy-
sis). However, this test is not guaranteed to reach Sk
at the end of the call, since the formula used for mod-
eling the call is an over-approximation.
2. If the rst step fails, then Yogi attempts to answer
the query using must summaries and not-may sum-
maries. A must summary consists of triples of the form
h;Q; iinterpreted to mean that every state in can
reach some state in  by executing Q. A not-may sum-
mary also consists of triples of the form h;Q; ibut
is interpreted to mean that no execution of Qcan start
in a state from and end in a state in  .Yogi caches
must summaries and not-may-summaries from every
query. Recall that the current query of interest with
procedure fooish;foo;Ski. Suppose there is a must
summaryh;foo; isuch that, andSk\ 6=fg.
Then, Yogi uses this to deduce that any test in  can
extend the frontier. Dually, suppose there is a not-may
summary of the form h;foo; isuch thatand
Sk . Then, Yogi uses this to deduce that no state
incan reach a state in Skby executing foo, and
thatcan be used as a suitable predicate to perform
renement of the region Sk 1.
3. Finally, if both the above steps fail, then Yogi analyzes
the body of footo answer the query h;foo;Ski.Yogi
then caches the results in the form of summaries, so
that these summaries can be used to answer future
queries at call sites to procedure foo.
6.1 Example
Consider an example where a procedure foodoes notmod-
ify a global variable x. Referring to Figure 4, suppose Yogi
encounters a situation where the operation opat the frontier
is a call to procedure foo. SupposeSkis the region ( x>5),
andis the region ( x2). Due to modication analysis,
Yogi is able to conclude that there is no way for any ex-
ecution in footo start in a state from region ( x2) and
end in a state in the region ( x>5). Further, since foodoes
not modify x, the weakest precondition of ( x>5) over the
entire body of foois (x >5) which is a suitable predicate
to reneSk 1.
6.2 Empirical Results
Table 3 shows the aggregate metrics for running Yogi over
theWDM suite, for various combinations of the interproce-
dural optimizations. For each combination, the table shows
total time taken for all the runs in the WDM suite, the num-
ber of correct defects detected by Yogi , and the total num-
ber of cases where Yogi results in a timeout. The rst row
of the table (Modication analysis|yes, Summaries|yes)Figure 9: Comparison of Yogi runtimes with and
without modication analysis.
Figure 10: Comparison of Yogi runtimes with and
without summaries.
gives empirical results with both optimizations are enabled,
the second row gives results with only modication analysis
was enabled (and summaries was disabled), the third row
gives results with only summaries were enabled (and modi-
cation reference is not used), and the last row gives results
with both optimizations disabled.
We note that both summaries and modication analysis
greatly help with reducing the runtime and decreasing the
number of timeouts in the runs. The total improvement due
to modication analysis alone in terms of total runtime is
32% (considering improvement from 3180 minutes to 2160
minutes), whereas the the total improvement due to sum-
maries alone is 28% (considering improvement from 2760
minutes to 2160 minutes). The total improvement due to
both modication analysis and summaries is 42% (consider-
ing improvement from 3780 minutes to 2160 minutes). Also,
the number of runs where Yogi runs out of time or space
decreases with each one of these optimizations, whereas the
number of defects that Yogi nds does not change much
with or without these optimizations.
To drill down into these aggregate results further, Fig-
ure 9, Figure 10 and Figure 11 show comparisons of runtimes
of individual runs of Yogi with and without these two op-
timizations. These gures again show how useful these two
optimizations are. In all the gures, the runtime of Yogi
with the optimization is given in the x-axis, and the runtime
ofYogi without the optimization is given in the y-axis. In
all the scatter plots, almost all points above the straight
Figure 11: Comparison of Yogi runtimes with and
without both modication analysis and summaries.
Test threshold total time no. no.
(minutes) defects timeouts
250 2600 236 92
500 2160 241 77
1000 2359 240 88
1500 2400 239 89
Table 4: Empirical evaluation of test thresholds us-
ing the WDM suite.
linex=y, which says that except for very few runs, there
was a runtime improvement in every run due to each of
these optimizations. The gains from modication analysis
seem bigger than those from summaries, consistent with the
aggregate numbers in Table 3. As seen in Figure 11, the
combined improvement due to both the modication analy-
sis and summaries is very signicant. The row of points at
the top in each of the gures represent cases where the run
times out (since we have set the value of timeout to be 1000
seconds) without the optimization, whereas completes with
the optimization.
7. TESTING
In every iteration of Yogi , testing takes precedence over
verication. Yogi rst checks if a test can be extended
across a frontier. If this is possible, Yogi generates a test
that crosses the frontier and runs the test. One parame-
ter that needs to be tuned is the number of steps that the
test should run beyond the frontier. We call this param-
eter as test threshold . The advantage in running a \long"
test is that several potential future renement queries can
be avoided, since every state transition made by the test po-
tentially avoids reasoning between the actual existence of a
transition between the corresponding regions using the the-
orem prover. However, running the test for too long creates
too many states and consumes memory for storing states.
Therefore, one has to make a time vs. space tradeo in
order to choose this parameter for optimal performance of
Yogi .
We have evaluated the performance of Yogi empirically
for various values of test thershold and arrived at a value
of500. Table 4 and Figure 12 compares results for test
thresholds 250, 500, 1000 and 1500 respectively.
Another optimization we have implemented is to save mem-
ory while storing all the states generated from tests. For ev-Figure 12: Evaluating the impact of test thresholds
onYogi.
ery test, Yogi generates a concrete program state at every
program location in the control ow graph of the program.
In order to be memory ecient, the whole concrete pro-
gram state is not stored at every program location. Instead,
only the delta state , that is, only the dierence between two
consecutive concrete states is stored. This poses several en-
gineering challenges { for instance, looking up the value of
a variablexin a concrete state requires a series of lookup
operations over a set of delta states5. If the value of the
variablexis not present in the current delta state (this can
happen ifxis not modied at the corresponding program
location), its parent is queried for the value. In the worst
case, a lookup can result in traversing the test all the way
up to its start state (linear time in the length of the test per
lookup). Therefore, we have implemented a scheme that
caches the results of lookup queries so that the amortized
cost of the lookup operation is ecient (almost constant
time per lookup) over all lookups.
We have found delta state optimization to be indispens-
able for running Yogi on large programs. Without this op-
timization Yogi runs out of time or space on a very large
number of runs in the WDM suite. Consequently, we do not
present these results. We believe it is not possible to build
a tool like Yogi and make it scale without an optimization
such as delta state to reduce storage at each state. An alter-
native is to not store states at all, and only store test inputs
and rerun tests from the initial state as done in Dart . How-
ever, we nd that this option is also too expensive for Yogi
since, unlike Dart it also maintains an abstraction, and
needs values from the concrete state every time a region is
partitioned, so that each concrete state is assigned to the
correct partition containing it.
8. MODELING
One of the most challenging issues we encountered in the
development of Yogi is to keep the testing runs and sym-
bolic execution runs consistent with each other. Suppose we
have a test tthat starts at some initial state and runs along
a control path and results in a state s. Further, suppose
we perform symbolic execution along the same path and
produce a symbolic state . For consistency, we require that
s2. However, such a consistency requirement is dicult
5Yogi performs this operation in order to decide which ab-
stract region a concrete state belongs to [9, 4].1 if (DestinationString)
2 {
3 DestinationString->Buffer = SourceString;
4
5 // DestinationString->Length should be set to the
6 // length of SourceString. The line below is missing
7 // from the original stub SDV function.
8 DestinationString->Length = strlen(SourceString);
9 }
10 if (SourceString == NULL)
11 {
12 DestinationString->Length = 0;
13 DestinationString->MaximumLength = 0;
14 }
Figure 13: A sample code fragment from an Sdvstub
function.
Issue type Number of issues
Integers used as pointers 8
Uninitialized variables 15
Type inconsistencies 9
Table 5: Summary of changes made to Sdvstub
functions.
to achieve in practice, in particular because the harnesses
and stub functions used in our benchmarks are not type safe.
Part of the reason for this is historical. Our benchmarks are
obtained from Static Driver verier and Slam . In this con-
text, the driver code is analyzed as is, but stub functions are
written for all the OScalls that the driver makes, and a stub
harness is written for how the OSloads and calls the driver.
Since Slam performs only static analysis, it was sucient
to write\rough"non-deterministic functions to model all the
non-determinism in a driver's environment. For instance, if
the environment returned a pointer, it was sucient (for the
purposes of doing analysis with Slam ) to write a stub that
returned an integer, and later type cast it into a pointer.
This had the advantage that using rough models of the en-
vironment, Slam could still analyze the driver. However,
the disadvantage was that error reports produced by Slam
could have false errors (though a lot of eort was put in to
minimize this possibility). With Yogi , since bugs are estab-
lished by nding test cases that lead to the error, we need
higher quality OSmodels. Stub functions for Yogi need to
be executable in the sense that running the program with
the stubs should not result in memory crashes. As a result,
we had to \ne-tune" all the stub functions of Static Driver
Verier so that they are executable. For example, consider
the code fragment from an Sdv stub function shown in Fig-
ure 13. The eld DestinationString->Length should be
set to the length of the string SourceString when SourceS-
tring is not equal to NULL, but is left uninitialized. This
causes Yogi to miss bugs for some checks and xing this
enables it to recover these bugs.
Table 5 summarizes some of the changes that we made to
the stub functions for drivers (there are a total of 456 func-
tions for the WDM OS model). Each of these changes im-
proved the quality of results produced by Yogi (eliminated
false positives as well as false negatives). Indeed, integers
used as pointers, uninitialized variables and type inconsis-
tencies are issues due to which a program may crash thus
rendering paths containing these issues infeasible.
While some of these issues (such as making sure that in-tegers are not type cast to pointers, and all pointers are
appropriately allocated) can be identied by doing a code-
review of the stub functions, some others such as the issue in
Figure 13 where we have missed assigning the Length eld of
the struct pointed by DestinationString under some condi-
tions are very hard to identify by reading through the code.
As a result, we discovered several of these by debugging cases
with unexpected results from our test suites.
9. THREATS TO VALIDITY
There are two main threats to the validity of our results.
Though our test suites are representative of device driver
code, and we have experiential and anecdotal evidence to
this eect, we do not know if these empirical results gener-
alize to other domains. As Yogi gets used as part of Sdv
more inside Microsoft, we expect to be able to experiment
with a wider variety of test suites, to evaluate and ne-tune
these optimizations. Though our experience is that bugs in
optimizations are usually caught by the test suites we have,
we cannot be 100% certain about the absence of bugs in our
implementation.
10. CONCLUSION
We have described several optimizations for Yogi and em-
pirically evaluated them over a large number of runs from
theWDM test suite. We have used empirical data to drive
decisions as to what optimizations to include in Yogi . There
are other optimizations that we have tried, but are absent
from this paper, because they did not give good results even
though they intuitively sound appealing. For instance, we
had another heuristic to choose relevant predicates (see Sec-
tion 5) where we favored choosing \simple" predicates over
\complex" predicates, where simplicity had to do with how
small the syntax tree of the predicate is, how many oper-
ators it has, etc. However, this yielded bad results, and
we realized that choosing relevant predicates is more impor-
tant than choosing simple predicates and this led to the SP
heuristic described in Section 5.1. In another instance, we
designed some sophisticated decision trees to store and re-
trieve not-may and must summaries, with the property that
storing the summaries would be expensive and lookup would
be cheap. But the performance benet was not compelling
enough.
For the optimizations that were successful, the empirical
data enabled us to make decisions on ne tuning the opti-
mization. For choosing an initial abstraction, we decided to
use predicates that are present in conditionals of the Slic
specication, but do not contain references to parameters
or return values. We also decided not to generate any ex-
tra predicates by propagating these predicates across assign-
ments, by looking at empirical data. In the case of relevance
heuristics, we learned that there are some cases where the
CDheuristic performs worse than not having the optimiza-
tion as described in Section 6. However, once we added both
theCDandSPheuristics, the performance was much better.
In the interprocedural case, the empirical data showed that
modication analysis and summaries were not only useful
individually, but their combination is very eective as well.
Empirical data also enabled us to choose parameters such
as test threshold.
We believe that detailed empirical study of optimizations
in verication tools will enable tool builders in the commu-nity to decide on which optimizations to implement in their
tools, and also have a realistic expectation on how much
benets each optimization is likely to provide.
11. ACKNOWLEDGEMENTS
We thank Venkatesh Prasad Ranganath and the anony-
mous reviewers for their insightful comments on earlier drafts
of this paper.
12. REFERENCES
[1] T. Ball, B. Cook, V. Levin, and S. K. Rajamani.
SLAM and Static Driver Verier: Technology transfer
of formal methods inside Microsoft. In IFM '04:
Integrated Formal Methods , pages 1{20, 2004.
[2] T. Ball and S. K. Rajamani. Automatically validating
temporal safety properties of interfaces. In SPIN '01:
SPIN workshop on Model checking of Software , pages
103{122, 2001.
[3] T. Ball and S. K. Rajamani. Slic: A specication
language for interface checking of C. Technical Report
MSR-TR-2001-21, Microsoft Research, 2001.
[4] N. E. Beckman, A. V. Nori, S. K. Rajamani, and R. J.
Simmons. Proofs from tests. In ISSTA'08:
International Symposium on Software Testing and
Analysis , pages 3{14, 2008.
[5] P. Cousot, R. Cousot, J. Feret, L. Mauborgne,
A. Min e, D. Monniaux, and X. Rival. The ASTRE E
analyzer. In ESOP '05: European Symposium on
Programming , pages 21{30, 2005.
[6] M. Das, S. Lerner, and M. Seigle. ESP: Path-sensitive
program verication in polynomial time. In PLDI
'02:Programming Language Design and
Implementation , pages 57{68, 2002.
[7] P. Godefroid, N. Klarlund, and K. Sen. DART:
Directed Automated Random Testing. In PLDI '05:
Programming Language Design and Implementation ,
pages 213{223, 2005.
[8] P. Godefroid, A. V. Nori, S. K. Rajamani, and S. D.
Tetali. Compositional may-must program analysis:
Unleashing the power of alternation. In POPL '10:
Principles of Programming Languages , pages 43{56,
2010.
[9] B. S. Gulavani, T. A. Henzinger, Y. Kannan, A. V.
Nori, and S. K. Rajamani. Synergy : A new algorithm
for property checking. In FSE '06: Foundations of
Software Engineering , pages 117{127, 2006.
[10] T. A. Henzinger, R. Jhala, R. Majumdar, and
G. Sutre. Lazy abstraction. In POPL '02: Principles
of Programming Languages , pages 58{70, 2002.
[11] V. Levin. Personal communication, August 2009.
[12] K. L. McMillan. Interpolation and SAT-based model
checking. In CAV '03: Computer-Aid Verication ,
pages 1{13, 2003.
[13] A. V. Nori, S. K. Rajamani, S. Tetali, and A. V.
Thakur. The Yogi Project: Software property checking
via static analysis and testing. In TACAS '09: Tools
and Algorithms for the Construction and Analysis of
Systems , pages 178{181, 2009.