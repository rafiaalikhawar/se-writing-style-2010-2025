Zurich Open Repository and
Archive
University of Zurich
University Library
Strickhofstrasse 39
CH-8057 Zurich
www.zora.uzh.ch
Year: 2010
The Missing Links: Bugs and Bug-fix Commits
Bachmann, Adrian ; Bird, Christian ; Rahman, Foyzur ; Devanbu, Premkumar ; Bernstein, Abraham
DOI: https://doi.org/10.1145/1882291.1882308
Posted at the Zurich Open Repository and Archive, University of Zurich
ZORA URL: https://doi.org/10.5167/uzh-44841
Conference or Workshop Item
Originally published at:
Bachmann, Adrian; Bird, Christian; Rahman, Foyzur; Devanbu, Premkumar; Bernstein, Abraham (2010). The
Missing Links: Bugs and Bug-fix Commits. In: ACM SIGSOFT / FSE ’10: eighteenth International Symposium
on the Foundations of Software Engineering, CHECK Santa Fe, USA, 2010, 97-106.
DOI: https://doi.org/10.1145/1882291.1882308The Missing Links: Bugs and Bug-ﬁx Commits
Adrian Bachmann1, Christian Bird2, Foyzur Rahman2,
Premkumar Devanbu2and Abraham Bernstein1
1Department of Informatics, University of Zurich, Switzerland
2Computer Science Department, University of California, Davis, USA
{bachmann,bernstein}@iﬁ.uzh.ch
{cabird,mfrahman,ptdevanbu}@ucdavis.edu
ABSTRACT
Empirical studies of software defects rely on links between
bug databases and program code repositories. This linkage
istypicallybasedonbug-ﬁxesidentiﬁedindeveloper-entered
commitlogs. Unfortunately, developersdonotalwaysreport
which commits perform bug-ﬁxes. Prior work suggests that
such links can be a biased sample of the entire population
of ﬁxed bugs. The validity of statistical hypotheses-testi ng
based on linked data could well be aﬀected by bias. Given
the wide use of linked defect data, it is vital to gauge the
nature and extent of the bias, and try to develop testable
theoriesandmodelsofthebias. Todothis, we must establish
ground truth : manually analyze a complete version history
corpus, and nail down those commits that ﬁx defects, and
those that do not. This is a diﬃcult task, requiring an ex-
pert to compare versions, analyze changes, ﬁnd related bugs
in the bug database, reverse-engineer missing links, and ﬁ-
nally record their work for use later. This eﬀort must be
repeated for hundreds of commits to obtain a useful sam-
ple of reported and unreported bug-ﬁx commits. We make
several contributions. First, we present Linkster , a tool to
facilitate link reverse-engineering. Second, we evaluate this
tool, engaging a core developer of the Apache HTTP web
serverproject to exhaustively annotate 493 commits that
occurred during a six week period. Finally, we analyze this
comprehensive data set, showing that there are serious and
consequential problems in the data.
Categories and Subject Descriptors
D.2.8 [Software Engineering ]: Metrics— Product Metrics,
Process Metrics
General Terms
Experimentation; Measurement; Veriﬁcation
Keywords
case study; apache; bias; tool; manual annotation
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
FSE-18, November 7–11, 2010, Santa Fe, New Mexico, USA.
Copyright 2010 ACM 978-1-60558-791-2/10/11 ...$10.00.1. INTRODUCTION
Software process data, especially bug reports andcommit
logs, are widely used in software engineering research. The
integration of these two provides valuable information on the
history and evolution of a software project. It is used, e.g.,
to predict the number and locale of bugs in future software
releases (e.g., [27, 31, 17, 6]). The two data sources are nor-
mally integrated by scanning through the version control
log messages for potential bug report numbers; conscien-
tious developers enter this information when they check-in
bug ﬁxes (e.g., see [14]). We used similar techniques in our
previous work, and, in fact, improved current practice by
adding heuristics to check the results [3, 4]. Even so, the
links (between program code commits and bug reports) thus
extracted cannot be guaranteed to be correct, as they are
reliant on voluntary developer annotations in commit logs.
In prior work, we have shown that such data sets are
plagued by quality issues [4]; furthermore, these issues
(e.g., incompleteness, bias, etc.) adversely aﬀect applica-
tions and algorithms which rely on such data [10]. We de-
ﬁned two types of bias: bug-feature bias , where only the ﬁxes
of certain types of defects are linked, and commit-feature
bias, where only the certain kinds of ﬁxes, or ﬁxes to certain
kinds of ﬁles, are linked. In addition to these data quality
issues, many researchers make questionable process assump-
tions: for instance they assume that all the relevant bugs of
a software product are actually reported in the bug track-
ing database of the project. To truly understand defect-
reportingbiasandverifysuchassumptions, wemustuncover
theground truth : we must analyze completely (at least a
time-window of) the commit version history of a project,
and precisely identify allthe commits that are defect ﬁxes,
and those that are not.
To get at ground truth requires skill, knowledge and ef-
fort: one must compare successive versions, understand the
changes, identify any relevant reported bugs in the repo,
and establish a link when possible. This process must be
repeated until we have a large enough sample for statistical
analysis. This is costly, diﬃcult, and time-consuming.
Linkster is a convenient, interactive tool, integrating multi-
ple queryable, browseable, time-series views of version con-
trol history and bug report history. Linkster enables an
expert to quickly ﬁnd and examine relevant changes, and an-
notate them as desired; speciﬁcally, Linkster makes it easy
to ﬁnd defect-ﬁx commits. We engaged an expert Apache
core developer, Dr. Justin Erenkrantz, to use Linkster to
manually annotate 6 full weeks (including 493 commit mes-
sages) of the Apache history. This case study helped us toimprove the tool, and yielded a trove of data to examine
three research questions.
Traditionally, researchers have made several assumptions
about the bug ﬁxing, reporting, and linking phenomena.
The ﬁrst two research questions reﬂect general internal va-
lidity concerns that arise when using linked bug data for
software engineering research.
RQ 1:Do the bug reporting and ﬁxing practices of
developers correspond to the assumptions commonly made
by researchers?
Second, researchers have tended to gloss over the issue of
whether automated tools that ﬁnd links between commits
and bug reports have false-positives or false-negatives.
RQ 2:How well does the automated approach of ﬁnding
links between commits and bug reports work?
Finally, the linked set of bug-ﬁxing commits are a sample of
the full set of bug-ﬁx commits. We can check and see if this
sample is biased in any detectible way.
RQ 3:Is there any evidence of systematic bias in the
linking of bug-ﬁx commits to bug reports [10]?
To our knowledge, the only published study on this question
is by Aranda and Venolia [1]: they analyzed the complete-
ness and degree of truth in software engineering datasets
and provided a partial answer to RQ 1 (see Sub-Section 2.2).
Most studies do not even address data quality issues [23].
In addition, we were able to qualitatively explore how
theApache project actually uses software engineering tools
such as bug tracker and version control systems, yielding
some rather surprising observations.
We begin with a discussion of related work (Section 2),
followed by an overview of the tools and processes (Sec-
tion 3) used in Apache HTTP web server project. We
then present (Section 4) a description of Linkster , and de-
tails of the case study procedure evolving an Apache core
developer (Section 5). In Sections 6 and 7 we present our
ﬁndings, which we summarize brieﬂy below:
Finding 1: A so-called “bug” is not always a bug; neither
is a“commit”always a commit. In other words: in Apache,
the most important bugs are not handled in the bug tracker
but mentioned in the mailing list system; and only a fraction
of commits actually pertain to program changes (RQ 1).
Finding 2: Wecomparedthemanualannotationswithdata
produced by automated linking ( viz.,for false-positives or
false-negatives); the automated approach ﬁnds virtually all
the commit log messages which contain a link to the bug
tracking database (RQ 2). Sadly, however, many defect-ﬁx
commits are un-identiﬁed in the commit logs, and thus are
invisible to automated approaches.
Finding 3: In the manually annotated sample, we ﬁnd
strong statistical evidence that diﬀerent bug-ﬁxers vary in
their linking behavior. Investigating further, we ﬁnd anec-
dotal evidence suggesting that factors such as experience,
ownership and the size (number of ﬁles) of the commit aﬀect
linking behaviour. We also ﬁnd that reporting bias aﬀects
theperformanceofabugpredictionalgorithm( BugCache ).
Given the small size of the manually annotated sample, the
evidence here is mostly suggestive rather than statistically
signiﬁcant; however, it points out the strong need for further
studies—for if this type of reporting bias is conﬁrmed as a
widespread problem, this is of serious, fundamental concernto all empirical research that uses this type of linked bug-ﬁx
data.
2. RELATED WORK
Areas closely related to this research include data extrac-
tion and integration, data quality in software engineering,
data veriﬁcation in software repositories, and our own pre-
vious work on data quality eﬀects on empirical software en-
gineering.
2.1 Data Extraction and Integration
Software engineering process data such as bug reports and
version control log ﬁles are widely used in empirical software
engineering. Therefore, the extraction and integration of
this data is critical.
Fischeret al.[14] presented a Release History Database
(RHDB) which contains the version control log and the bug
report information. To link the change log and the bug
tracking database, Fischer et al.searched for change log
messages which match to a given regular expression. Later,
they improved the linking algorithm and built in a ﬁle-
module veriﬁcation [13]. A similar approach to link the
change log with the bug tracking database was chosen by
other researchers. All of them used regular expressions to
ﬁndbugreportlinkcandidatesinthechangelogﬁle(e.g.,[32,
31, 30, 33, 34, 30]).
In [3], we presented a step-by-step approach to retrieve,
parse, convert and link the data sources. We improved the
well-established prior art, enhancing both the quality and
quantity of links extracted.
2.2 Data Quality in Software Engineering
As discussed in [10], empirical software engineering rese-
archers have considered data quality issues. Space limita-
tions inhibit a full survey, we present a few representative
papers.
KoruandTian[21]surveyedmembersof52diﬀerentmedi-
um to large size Open Source projects with regards to defect
handling practices. They found that defect-handling pro-
cesses varied among projects. Some projects are disciplined
and require recording of all bugs found; others are more lax.
Some projects explicitly mark whether a bug is pre-release
or post-release. Some record defects only in source code;
others also record defects in documents. This variation in
bug datasets requires a cautious approach to their use in em-
pirical work. Liebchen et al.[22] examined noise, a distinct,
equally important issue.
Liebchen and Shepperd [23] surveyed hundreds of empiri-
calsoftwareengineeringpaperstoassesshowstudiesmanage
data quality issues. They found only 23 that explicitly ref-
erenced data quality. Four of the 23 suggested that data
quality might impact analysis, but made no suggestion of
how to deal with it. They conclude that there is very lit-
tle work to assess the quality of data sets and point to the
extreme challenge of knowing the“true”values and popula-
tions. They suggest that simulation-based approaches might
help.
Bettenburg et al.[7, 8, 9] provided ﬁrst analysis of bug
report quality. They investigated the attributes of a good
bug report surveying developers and used it to develop a
computational model of a bug report quality. The resulting
model allowed to display the current quality of a defect re-
port whilst typing. Hooimeijer et al.[16] also analyzed thequality of defect reports and tried to predict whether the
defect report will be closed within a given amount of time.
Chenet al.[12] studied the change logs of three Open
Source projects and analyzed the quality of these log ﬁles.
In [4] we surveyed ﬁve Open Source and one Closed Source
project in order to provide a deeper insight into the quality
and characteristics of these often-used process data. Specif-
ically, we deﬁned quality and characteristics measures, com-
puted them and discussed the issues arose from these obser-
vation. We showed that there are vast diﬀerences between
the projects, particularly with respect to the quality of the
link rate between bugs and commits.
Aranda and Venolia [1] provided a ﬁeld study of coordi-
nation activities around bug ﬁxing, based on a survey of
software professionals at Microsoft. Speciﬁcally, they stud-
ied 10 bugs in detail and showed that (i) electronic reposi-
tories often hold incomplete or incorrect data, and (ii) the
histories of even simple bugs are strongly dependent on so-
cial, organizational, and technical knowledge that cannot be
solely extracted through the automated analysis of software
repositories. They report that software repositories show an
incompletepictureofthesocialprocessesinaproject. While
they studied 10 bugs in detail, we focus on commit history:
we employed an expert, supported by a specially-designed
tool to fully annotate a sample of 493 commits. This data
helped us uncover a) some of the weaknesses of software
repositories as well as b) anecdotal evidence of systematic
bias in bug-ﬁx reporting.
2.3 Studying Bias
Papersinempiricalsoftwareengineeringrarelytackledata
quality issues directly (see discussion earlier in this sectio n);
our earlier work is an exception. In [2] and [10] we inves-
tigated historical data from several software projects, and
found strong evidence of systematic bias. We then investi-
gated potential eﬀects of “unfair, imbalanced” datasets on
the performance of prediction techniques.
Ideally, all bug-ﬁxing commits are linked to bug reports;
then empirical research would consider all type of ﬁxed bug
reports. However only some of the ﬁxed bugs have links to
the bug-ﬁxing commits. This raises the possibility of two
types of bias: bug feature bias , where only certain types of
bugs are linked, or commit feature bias , whereby only certain
types bug-ﬁxing repairs are linked. Either type of bias is
highly undesirable. With access to all the ﬁxed bugs, and
the linked bugs, we could check for bug feature bias. Our
study [10] suggested that bug feature bias does exist, and
also that it aﬀects the performance of the award-winning
BugCache defect prediction algorithm [19]. In this work,
we have a fully annotated list of commits for the ﬁrst time,
thus achieving “ground truth” for a subset of the Apache
dataset, andthuswecananalyzethedataforcommitfeature
bias.
In summary: a few studies explicitly consider the qual-
ity of systematic bias in the data. This study, in contrast,
explores the implications of this behavior by attempting to
unearth the ground truth by enlisting a core developer to
annotate all commits, and thus seek out quality and bias
issues.
3. CASE STUDY: APACHE
TheApache HTTP web server is an Open Source soft-
ware system developed under the auspices of the Apache
Software Foundation. Apache is the most popular webserver on the Internet, serving over 55% of all websites [26].
Apache isalsooneofthemostpopularOpenSourceprojects
among researchers. It is widely used in current empirical
software engineering research (e.g., [25, 28, 20, 8, 18]), and
thus a good subject for an in-depth examination of data
quality.
3.1 Project Tools
Like many other Open Source projects, Apache uses the
BugZilla1bug tracker and the SVN2version control system.
In addition, the Apache Software Foundation provides oﬃ-
cially maintained git3mirrors for all projects. The Apache
project allows free access to the contents of all these tools.
Apache also maintains a public mailing list for developers
andApache users to discuss issues of concern.
3.2 Data Gathering and Integration
We retrieved, processed and linked the Apache HTTP
web server process data as presented in [3]. Basically, we
downloaded all BugZilla bug reports and SVN version con-
trol log ﬁles. Then, we scanned each commit log message for
indications of ﬁxing a bug using a set of heuristics; typically
we look for bug report numbers in log messages. This leads
to a set of automatically extracted links between program
code commits and bug reports. This set of links is validated
using another set of heuristics ( op cit).
3.3 Apache Dataset
With our own (rather modest) resources, we could only
completely evaluate and manually verify a subset of the orig-
inalApache dataset. Therefore, we had to sample the orig-
inal dataset. There were two choices: random sampling or
temporal sampling.
Random sampling requires some rationale for selecting a
sample— e.g.,prior knowledge of the distribution of the rele-
vant co-variates to the study, so that a sample representative
of the population could be chosen. It is diﬃcult to decide a
prioriwhat such co-variates might be, let alone their distri-
bution. So, we chose to perform temporal sampling .
Table 1: Apache Datasets: Details
Dataset Original Evaluation
Dataset Sample
Considered time period 2004-06-18 – 2005-09-23 –
2008-04-25 2005-11-18
#Bug reports 2,409 (100%) 103 (100%)
#Fixed bug reports4559 (23.20%) 23 (22.33%)
#Linked bug reports 256 (10.63%) 10 (9.71%)
#Duplicate bug reports 364 (15.11%) 8 (7.77%)
#Invalid bug reports 766 (31.80%) 38 (36.89%)
#Diﬀerent bug reporters 1,827 98
#Commit messages 8,580 (100%) 493 (100%)
(transactions)
#Empty commit messages 0 (0.00%) 0 (0.00%)
#Linked commit messages 472 (5.50%) 29 (5.88%)
#Diﬀerent developers 63 23
1Seehttp://www.bugzilla.org/
2Seehttp://subversion.tigris.org/
3Seehttp://git-scm.com/
4We deﬁne “ﬁxed” bug reports as bug reports that have at least o ne
associated ﬁxing activity (which means a status change to “ﬁ xed”)
within the considered time period.a
b
c
Drag & Drop
Double Click
to view diff
1
2
3
4
5
6
7
8
9
10
11
12
13
 14
15
Figure 1: Linkster (Screenshot)
With this approach, we chose to verify all the commits in a
given period. With complete results for that period, we can
then revisit our earlier results and judge the quality against
this limited but complete and accurate temporal sample.
To ﬁnd a“typical”period for our evaluation dataset we ana-
lyzed the whole original Apache dataset based on week-long
epochs. Then, we chose a period of 6 consecutive weeks
that was as representative as possible to the overall original
Apache dataset in terms of its descriptive process statistics
(e.g., similar proportions of bugs and commits). Table 1 lists
some basic software process statistics for both—the original
and the evaluation— Apache datasets including the ﬁnally
deﬁned time-frames.
4. LINKSTER
The use of Linkster simpliﬁed our domain expert’s task,
greatly accelerating an otherwise tedious, repetitive and in-
convenient sequence of invocations of multiple tools.
Figure 1 shows a screenshot of Linkster , showing win-
dows containing three kinds of information: commit trans-
actions including all the changed ﬁles (a), bug reports (b),
and diﬀ & blame information for all of the lines in a ﬁle
before and after a particular commit (c).
Linkster requires access to a version control system for
ﬁle content and a database (local or remote), containing the
raw mined repository and bug tracking information. We use
git as our backend repository format, given its increasing
popularity [11], and ready availability of tools supporting
conversion from competitors such as CVS, SVN, etc.How-
ever, for convenience, Linkster displays the revision IDs
from the original repository. All notes, links, and annota-
tions (explained below) made by the user are also recordedin the database to facilitate use and analysis thereof after
annotation. Linkster eﬃciently displays, integrates, and
allows inspection and annotation of information from all
data sources. Linkster is written in Python, using the
PyQt widget toolset and has been written with portability
in mind. We have successfully run it on Linux, OS X, and
Windows.
To our knowledge, no other tool provides integrated pro-
ject information in combination with functionality to an-
notate / link commits. Hipikat [32], which was developed
at UBC, is similar in that it creates links between diﬀerent
types of software artifacts. However, these links are based
purely on heuristics and Hipikat functions as a recommender
system rather than a browsing and annotation system.
Other tools such as EvoLens, softChange, or Shrimp pro-
vide only part of the functionality, but all existing tools hav e
goals other than expert commit annotation.
SoftChange [15] is a tool to aid software engineering re-
searchbyvisualizingdata. Similarto Linkster , SoftChange
integrates data from multiple sources such as version control
systems, releases, and bug databases. However, softChange
uses visualizations (usually plots) to answer questions,( e.g.,
how many bugs are closed in each time period?) and does
not allow annotation of data as Linkster does.
EvoLens [29] helps developers to understand the evolution
of a piece of software by visualizing the software as well as
metrics of the software over time. The visual nature across
time facilitates identifying design erosion and hot spots of
activity. Linkster does not leverage advanced visualization
techniques and integrates multiple types of data rather than
just source code information.
Shrimp [24] integrates and visualizes source code, docu-mentation (Javadoc), and architectural information to aid
source code exploration. Linkster is more concerned with
processrelated artifacts, ( e.g.,changes, discussions, bug re-
ports, and ﬁxes) than understanding the source code itself.
4.1 Commit Information
Figure 1-a shows the Commit Information Window of
Linkster . The top ( 1) contains a list of commits that
satisfy some query, e.g.,commits within a time window or
changes made by a particular author. Each line shows the
revision identiﬁer (as used in the original repository), com-
mit time, author, and the ﬁrst line of the commit message.
The entire commit message is shown in a tooltip when the
mouse hovers over an entry.
When a commit entry in the list is selected, the meta-
data is updated in the bottom half ( 2). The list of ﬁles
modiﬁedinthecommit( 3)isalsodisplayed. Doubleclicking
a ﬁle brings up the Blame & Diﬀ Information for the ﬁle
allowing the user to examine the exact changes that were
made. For annotation purposes, the user may select the
reason(s) for the commit by checking boxes ( 4) or drag and
drop (or remove) a bug record from the Bug Information
Window into the list of bug IDs (5), which is populated
with the set of automatically identiﬁed links between the
commit and bug records. Finally, the user may enter free
form notes for the commit ( 6).
4.2 Bug Information
Figure 1-b contains the Bug Information Window . The
top portion ( 7) is a scrollable list of bugs from the bug
database. Each entry contains the bug ID, the date of cre-
ation, and a one line summary of the bug. Hovering over
an entry shows the bug severity in a tooltip. Any of these
entries may be dragged to the bug IDslist (5) in the commit
information window to indicate a commit that is associated
with the bug.
Selecting a bug entry populates the bottom half of the
window with detailed information. The left side ( 8) con-
tains short attributes of the bug, while the right side ( 9)
displays the full bug description followed by all of the com-
ments in chronological order with author and date. Clicking
on theBug Activity tab (10) displays a list (not shown) of
all changes to the bug record, such as assigning the bug to a
developer or marking a bug as closed. Each entry indicates
when the change was made and who made it along with old
and new values for the changed ﬁeld as appropriate. Fi-
nally, clicking on the Fixing Files tab (11) presents a list
(not shown) of all of the commits to ﬁles that are associated
with the ﬁx of the bug. This list is comprised of ﬁles auto-
matically or manually linked to the bug. Double clicking on
any ﬁle in this list will bring up a blame & diﬀ window for
the commit.
4.3 Blame & Diff Information
Figure 1-c shows the Blame & Diﬀ Information Window
for the changes to a ﬁle in a particular commit. The left
view (13) shows the content of the ﬁle prior to the change,
and the right view ( 14) shows the content after the change.
Removed lines are preﬁxed with“ −”and are highlighted red,
and added lines are in green with a “+” preﬁx. Each line
is also preﬁxed with revision identiﬁer of the commit that
introduced the line. Selecting a line highlights all other li nes
introduced in the same commit, and also updates the meta-
data area ( 12) with information about that commit. Thiscan help the user learn why, when, and by whom, the line
was originally added. If additional information is desired,
double clicking a line will bring up a new Blame & Diﬀ
window for the commit which introduced the line (if, for
example, one desires to see why a line that was removed in
one revision was originally added in a prior revision). An
annotator can, thus, gradually step back through version
history.
The views are synchronized such that scrolling up, down,
left, or right in one view causes the other to change ac-
cordingly. The thumbnail view ( 15) graphically shows the
diﬀerences for the entire ﬁle with red indicating removed
lines and green, added lines. Clicking on a location in the
thumbnail view will cause the pre and post views to jump
to that location, making it easier to identify and examine
changes in larger ﬁles.
5. APACHE DATA EV ALUATION
To address our research questions, we began our evalua-
tion with the creation of an evaluation dataset, as deﬁned
in Section 3.3. Armed with Linkster to facilitate browsing
and annotation, we engaged the services of an informant:
an experienced Apache developer, Dr. Justin Erenkrantz,
to manually annotate a temporal sample of commits using
Linkster . Clearly, the quality of this completely annotated
evaluationdatasetispredicatedontheexpertiseoftheanno-
tator. Justin is a core developer of the Apache HTTP web
serverproject (since January 2001), the President of the
Apache Foundation and serves on the Foundation’s Board of
Directors. He also develops for Apache Portable Runtime,
Apache ﬂood and Subversion5.
UsingLinkster , Justin annotated each commit, to ﬂag
it as abug ﬁx, an implemented feature request , amainte-
nancetask orother. With this information, we obtain fully
annotated commit data, providing a complete picture of all
the changes during the given period and how/why/by whom
these changes were made. This data can be used to verify
our automated linking approach (which includes mainly bug
ﬁxes and some feature requests). Indeed, annotating pro-
gram code commits dating back months or years in the past
is a challenge, even for an experienced core developer like
Justin.Linkster was very helpful, providing an integrated
view of all the relevant information. Based on the log mes-
sage, the changed ﬁles and the ﬁle diﬀs of the changed ﬁles,
Justin was able to annotate all commits, and, in most cases,
provided additional information about the commits.
Justin’s familiarity with the Apache project gives us con-
ﬁdence that the results of our evaluation can be trusted.
In addition, detailed discussions and interviews with him
revealed facts about the tools and processes used in the
Apache HTTP web server project, and also ideas for im-
provingLinkster .
6. RESULTS
All 493 commits in our selected temporal sample were
annotated. In addition to the annotation into the four cate-
gories above: bug ﬁx, feature request, maintenance/refactor-
ing,andother,our informant helped us further sub-classify
the commits. Table 2 summarizes the annotation results
including the sub-classiﬁcation. Note, a single commit can
have many annotations, e.g.,a commit may be annotated
as both a“bug ﬁx”and a“feature request”.
5Seehttp://www.erenkrantz.com/ for more details.Table 2: Linkster Commit Categorization
(non-exclusive)
Category Sub-Category #Commits
Bug ﬁx – 82
Bug ﬁx Bug report 32
Bug ﬁx Bug report (merge) 7
Bug ﬁx Mailing list 13
Bug ﬁx Backport 13
Bug ﬁx Other 17
Feature request – 54
Feature request Documentation 7
Feature request Backport 14
Feature request Other 33
Maintenance – 49
Maintenance Documentation 5
Maintenance Backport 5
Maintenance Other 39
Other – 356
Other Documentation 156
Other Backport 49
Other Non-functional 30
Other Release 44
Other Voting 26
Other Other 51
Based on Justin’s insights into the Apache development
process, we developed a second, orthogonal categorization
that was more consistent with the procedures within the
project (Table 3). In contrast to our categorization, this
one assigns each commit exclusively to one of its process-
speciﬁc categories: backport/forward port, security ﬁx, bug
ﬁx, documentation, voting, release, orother.
Table 3: Process Speciﬁc Commit Categorization
(exclusive)
Category #Commits
Backport / Forward port 79
Security ﬁx 7
Bug ﬁx 69
Documentation 158
Voting 26
Release 44
Other 110
In the following sub-sections, we present our ﬁndings rela-
tivetotheresearchquestionspresentedinSection1. Wealso
present additional ﬁndings based on interviews with Justin.
6.1 Bugs Incognito
Contrary to conventional wisdom, participants of the
Apache projectdo not report all the bugs solely through
BugZilla . We found that developers and professional users
also make use of the Apache mailing list to report bugs and
provide bug ﬁxes (sometimes at the same time)
without reporting them in the bug tracker.
Finding 1. Not all ﬁxed bugs are mentioned in the bug
tracking database. Some are discussed (only) on the
mailing list.
As shown in Table 2, we have 82 bug ﬁx related com-
mits in our evaluation dataset. 32 of them (bug report)
are directly related to the bug tracking database. 7 othercommits contain a bug-ﬁx, but are not the initial bug ﬁx
commit rather than a merge of versions which contain bug
ﬁxes indirectly (bug report (merge)). This means, that only
47.6% of bug ﬁx related commits (32+7
82) are documented
in the bug tracking database. For 13 other commits (16%
of total) identiﬁed by Justin as bug ﬁxes, there are related
discussions in the Apache mailing list. This leads to the
discouraging observation that many bugs never appear in
the bug tracking database, but rather are onlydiscussed on
the mailing list. Such a discussion often includes the bug
ﬁx provided by a non Apache core developer. According to
Justin, these bugs are often the very important bugs espe-
cially because of the high attention by Apache developers
and the core community on the mailing list. Note also that
reporting some types of bugs (e.g., security related ones)
on the mailing list is a practice explicitly requested by the
Apache Foundation6.
Unfortunately, even knowing about the mailing list bugs,
it is hard to i) identify and ii) automatically mine them or
extract information similar to a bug report stored in the bug
tracking database (such as status changes, priority, severity,
etc.).Apache SVN revision #291558 (see Figure 2), for
instance, is related to a bug discussed on the mailing list7.
If one were to inspect the mailing list message, one would
ﬁnd almost no evidence that this was a bug ﬁx.
Finally, Justin found 17 other bug-ﬁxing commits (21%)
which have neither an associated bug report or mailing list
message. This phenomenon, of under-reporting of bugs, is
a big problem. If important bugs are excluded from experi-
mental data (i.e., many bugs are left out) then the eﬀective-
ness of defect prediction models and the validity of statis-
tical studies (which rely on them being in the bug tracking
database) may be threatened. This leads to the conclusion,
that not all ﬁxed bugs are reported as bugs in the bug track-
ing database, or in other words: bugs go“incognito”.
6.2 Backport Incognito
In theApache HTTP web server project only a few
developers are allowed to commit to an Apache release ver-
sion: thus a bug-ﬁx on one release may actually have to be
committed by someone else to an older or diﬀerent release.
Typically, this process works as follows. First, a developer
ﬁxes a given bug and commits the new version to the cur-
rent version under active development (also known as the
“trunk”). Ideally s/he also refers to the related bug report
in the commit log. Next, at least two other developers re-
view the changed code, verify the changes and vote either for
or against the ﬁx (this step is related to the voting commits
as shown in Table 2 and 3). Finally, if the votes are positive,
the ﬁx is committed (or merged) to Apache release versions,
which is called a backport. As a result of this process, we
might ﬁnd several diﬀerent commits in the version history,
that ﬁx the same bug.
Finding 2. To ﬁx a bug in an Apache release, multiple
similar commits by diﬀerent developers are needed.
Unfortunately, backport commits are not that easy to
identify by existing linking algorithms and heuristics; fre-
quently, while the log message for original commit to the
trunk refers to the bug report, the backport commit log does
not. To worsen matters, after the bug is actually closed,
6Seehttp://httpd.apache.org/security_report.html
7Seehttp://mail-archives.apache.org/mod_mbox/httpd-docs/
200509.mbox/%3c200509260627.33737@news.perlig.de%3ethere is a rigorous review, veriﬁcation and voting process
before the backport is accepted and committed. Therefore,
the time diﬀerence between the backport commit and the
status change (to ﬁxed) on the bug report may rise to sev-
eral days, which again, makes it diﬃcult to link the bug with
the commit. As a result, automated linking algorithms will
largely ignore backport ﬁxes. Arguably, these are ﬁxes are
very important: often they are involved in post-release fail-
ures. They should not be ignored by researchers engaged in
hypothesis testing or defect prediction work. Alas, ﬁnding
themmayrequireextensive, high-expertisecombingthrough
commit histories.
Figure 2: Commit message of Apache HTTP web
server revision #291558
6.3 Impact-of-Defect vs. Cause-of-Defect
This is a thorny issue: a defect in one project’s code base
might actually manifest as a failure in a diﬀerent project.
Thus, some of the reported bugs in Apache HTTP web
server have their root-cause outside of theApache pro-
gramcode. Apache usesexternallibraries, aswellasApache
Commonsmodules. Therefore, failuresinthe Apache HTTP
web server , evenifdulyreportedinthe Apache bugtrack-
ing database, may actually have to be ﬁxed elsewhere. The
reverse is also possible.
The mod-python8sub-project maintains its own version
control system repository and an Apache project’s main
bug tracker independent Jira issue tracker9. Mod-python is-
sue 8310, for instance, was reported in the Jira issue tracker
but ﬁxed in the Apache program code.
Finding 3. Developers sometimes ﬁx bugs that are only
reported in some other projects’ bug tracker, rather than in
their own; and vice-versa.
Ideally, we have a complete, integrated source of all the
bugs in the bug repository, and all the ﬁxes in the version
control system. Our ﬁndings, and indeed, the widespread
prevalence of cross-project module reuse, we can expect that
this type of separation between causes and eﬀects of defects
is quite common. Given this, it would be helpful if a re-
port of a bug impacting one system would be transferred to
the bug repository of the causingsystem, and linked to ﬁx
in the version control of that system. However, given the
poor linking behaviour when the cause and eﬀect are in the
samesystem, we might expect that this type of cross-system
linking is pretty unlikely to occur.
6.4 Commits Incognito
In earlier work [4], we encountered the problem of unex-
plained commits , e.g.,due to empty commit log messages.
Sadly, even an experienced developer would ﬁnd it diﬃcult
to retrospectively reconstruct the explanation of an unex-
plained commit.
8http://www.modpython.org/
9http://www.atlassian.com/software/jira/ and
https://issues.apache.org/jira/browse/MODPYTHON
10https://issues.apache.org/jira/browse/MODPYTHON-83Finding 4. Even if we annotate all commits, the cause of
a commit still remains unspeciﬁed in some cases.
Table 2 and 3 show the annotation, sub-classiﬁcation and
process-oriented classiﬁcation of all the commits in our eval -
uation dataset. Based on the values in Table 3, for 110 com-
mits (22.3%) we have a process speciﬁc annotation of other.
The reason for these commits, therefore, is not justiﬁed by
one of the Apache software engineering core tasks.
In addition, most of the commits are not justiﬁed by a bug
ﬁx or feature request rather than for documentation (32%),
voting (5.3%) or releases (8.9%). Only 37.1% of all commits
have a functional impact on the software product (feature
requests and bug ﬁxes including all backport), which leads
us to the conclusion that not all commits are commits that
actually change the software.
For additional information to the quality and character-
istics of the version control data, we refer to our previous
work presented in [4].
6.5 Performance of the Linking Algorithm
Inearlierwork[3, 4, 10, 5], wereportedalinkingalgorithm
whose performance was found to be best-in-class. The fully
annotated data provided the ﬁrst known oracle to evaluate
linking algorithms, and so we evaluated ours.
Finding 5. The algorithm ( op cit) ﬁnds most of the
commit log messages that the developers linked to bugs
reported in the bug tracker, subject to the time constraints
used by our algorithm.
Inthechosentemporalsample, ourlinkingalgorithmfound
29 links between the commit messages and the bug tracking
database. Justin also identiﬁed all these links; we thus foun d
no false-positive links in our evaluation dataset. In additio n
to these, Justin found 10 additional links. Seven did not
satisfy our heuristic for valid links (time constraint of ±7
days between commit and status change on the bug report),
and so our algorithm rejected them as invalid links. Hence,
we found three false-negative links in our evaluation dataset.
The seven invalid links resulted from backport commits (as
explained earlier, Sub-Section 6.2). These backports corre-
sponded to bug-ﬁx links in the original trunk which in fact,
were successfully discovered by our algorithm.
Unfortunately, as we elaborated before, even with a high
linkingratebetweenthecommitmessagesandthebugtrack-
er, only a subset of the ﬁxed bugs are considered. Hence,
bugs discussed on the mail discussion system are often left
out by automated linking approaches.
6.6 Performance of LINKSTER
Linkster performed mostly as expected and Justin was
able to annotate all the commits (493) of our evaluation
sample dataset in one working day. In the discussions with
Justin, we found some minor issues, which were promptly
remedied. In addition, we found that the most important
bugs are discussed in the mailing list system only. There-
fore,Linkster has been extended to support browsing of
messages from development mailing lists and also enables
linking them to both bug reports and repository commits.6.7 Threats to Validity
This sub-section discusses external and internal threats to
validity that can aﬀect the results reported in this section.
Threats to external validity. Can we generalize from
theresultsbasedonthe Apache HTTP web server dataset
to other datasets? Software engineering tools and processes
vary in diﬀerent projects and, therefore, our ﬁndings based
onApache may not generalize. However, our ﬁndings indi-
cate that developers may use software process support tools
for various goals not envisioned by its original developers
(such as version control systems for voting or mailing list
systems for bug reporting). It seems prudent to assume that
theApache project is not a complete exception and that,
therefore, the data used in studies of other projects may also
lack important information. Another threat is the use of a
single annotator (Justin). Getting the same data annotated
by other developers, and checking agreement, would have
been better; we hope to do this in future work.
Threats to internal validity. Did we choose our eval-
uation dataset well, and properly analyze it? We chose our
time-frame carefully; however, it may not properly represent
the original Apache dataset. The annotation and classiﬁca-
tion were performed carefully by a very experienced Apache
core developer. Still, there may be errors. Nonetheless, ac-
cording to Justin, the interesting practices of the Apache
developers are by no means exceptional to this time period.
7. COMMIT-FEATURE BIAS, REVISITED
The manual annotation eﬀort indicates that many bug
ﬁxes are not identiﬁed in the commit logs, and thus are
completely invisible to the automated linking tools used to
extract bug-ﬁx data. Thus the linked bug-ﬁx commits are
asampleof the entire group of commits. However, samples
thus extracted have been central to many research eﬀorts.
The natural question is: is this sample representative, or bi-
ased? We seek to test for the two kinds of bias: bug feature
bias, whereby only ﬁxes to certain kinds of bugs are linked,
andcommit feature bias whereby only certain types of com-
mits are linked [10]. Earlier, with access to the entire set
of ﬁxed bugs, and the subset of linked bugs, we could check
for (and did ﬁnd) bug feature bias; lacking access to a fully
annotated set of commits that tells us which commits are
bug ﬁxes, we were previously unable to check for commit
feature bias.
Now, with a fully annotated temporal sample of commits,
we can indeed check for commit feature bias. Commit fea-
turesarepropertiesoftheﬁleanditsrevisionhistory, suchas
size, complexity, authorship, etc.. These are critical proper-
ties that have been studied in dozens of papers that test the-
ories of bug introductions; they are also the features used for
bug prediction. So it is important to test for commit feature
bias, and evaluate its impact. In this section, we describe
some ﬁndings related to commit feature bias, and its eﬀect
on a well-known bug-prediction algorithm ( BugCache ).
We remind the reader that our sample size (despite the
time and eﬀort required to gather even that much) is not
big enough to realistically expect to ﬁnd statistically sign if-
icant support for answers to the questions discussed in this
section. However, there are some takeaways: we do ﬁnd sta-
tistical support for the answer to one question, and we do
ﬁnd some anecdotal answers for the other questions. Fur-
thermore, actual bias along any of the lines discussed here
would have a highly deleterious eﬀect on the external va-
lidity of theories tested using only the linked data. Mostimportantly, we hope to convince the reader that such stud-
ies are important and need to be repeated and conducted at
larger scales .
7.1 Sources and Extent of Commit Feature Bias
The ﬁrst question arises naturally from the fact that there
are diﬀerent individual developers, who may have diﬀerent
attitudes towards linking. The simplest and most obvious
question is as follows:
Do diﬀerent developers show signiﬁcantly diﬀerent
linking behaviour? The anonymized table of developers’
linking behavior indicates that this is the case: ( p≃0.002).
Name Linked Not Linked Name Linked Not Linked
a 0 6 b 10 5
c 1 1 d 11 8
e 0 3 f 0 1
g 0 3 h 0 5
i 2 7 j 0 3
k 0 2 l 0 1
m 0 2 n 0 1
o 0 1 p 1 0
q 4 0 Total 26 52
We now hypothesize several diﬀerent speciﬁc possible mo-
tivationaltheoriesoflinkingbehavior. Inseveralcases, there
was a visually apparent signal, in boxplots, albeit none that
were statistically signiﬁcant. The results are shown in Fig-
ure 3. We list them below, but we caution the reader to
interpret all these ﬁndings as at best anecdotal . However, it
is important to bear in mind that actual bias inﬂuenced by
any of the processes hypothesize below would be very dam-
aging to the external validity of theories tested solely on t he
linked data .
Does the experience of the author(s) whose code is
being ﬁxed inﬂuence linking behaviour? We hypothe-
sized that the quest for greater reputation might incentivize
people to link ﬁxes when the code under repair belonged to
an experienced (and thus more reputable) person. We mea-
suredtheﬁxedcode’s“authorreputation”asthegeometricof
the prior commit experience of everyone who contributed to
the ﬁxed code. The left most boxplot in Figure 3 is weakly
suggestive that ﬁxes made to code with more experienced
authorship are more likely to be linked.
Does the number of ﬁles involved in the bug ﬁx mat-
ter?If more ﬁles are repaired in a bug ﬁx, perhaps the ﬁx
is more “impactful”; this might motivate the ﬁxer to more
carefully document the change. In fact, the boxplot (second
from left in Figure 3) is suggestive that this might be the
case, with all the unlinked ﬁxes being single-ﬁle ﬁxes.
Are more experienced bug ﬁxers more likely to link?
We might expect that more experienced developers behave
more responsibly. We measure experience as the number of
prior commits. The boxplot (second from right) suggests
support for this theory, with a noticeably higher median for
the linked case.
Are developers who “own” a ﬁle more likely to link
bug-ﬁxes in that ﬁle? One might expect that people
ﬁxing bugs in their own ﬁles are more likely to behave re-
sponsibly and link; on the other hand, there is a anti-social
reputation-preserving instinct that suggests that they may
belesslikely to link. We measure ownership as the propor-
tion of lines in the ﬁle authored by the bug ﬁxer. Indeed,
the boxplot visually supports the“anti-social”theory.
We created plots to evaluate two other theories: Are bug●●●
● ●●
●●●
Linked Not Linked0 500 1000 1500Apache − Weighted Experience of RevisionWeighted Experience (Commit Count)●●●
●
●
Linked Not Linked1 2 3 4 5 6Apache − Number of Files CommittedNumber of Files Committed●●
Linked Not Linked0 500 1000 1500 2000Apache − Experience of the Fixing AuthorExperience (Commit Count)●●●
●●●
Linked Not Linked0.0 0.2 0.4 0.6 0.8 1.0Apache − File Ownership of Fixing AuthorFile Ownership
Figure 3: Commit feature bias (reading left to right) weighted experi ence of the original authors of the ﬁx-inducing
code; number of ﬁles changed in the bug ﬁx; experience of the a uthor committing the bug ﬁx; proportion of ﬁxed ﬁle
owned by bug ﬁx author at the time of the bug ﬁx.
ﬁxes to bigger ﬁles more likely to be linked? andDoes
the prior experience of the ﬁle owner inﬂuence linking be-
haviour? and found no informal visual evidence supportive
of these theories.
7.2 Practical Effects: BugCache Revisited
The above analysis shows that the extent of bias in the
data is signiﬁcant and that the eﬀort of ﬁnding the ground
truth (e.g.,through manual annotation with Linkster )
leads to important insights. But do those insights translate
to practical impact? In this sub-section we investigate the
impact of approaching ground truth in terms of changes in
the accuracy of the award-winning BugCache algorithm
[19]. To that end, we repeated our experiment showing
the impact of bias using Apache data [10]. Speciﬁcally,
we departed from two diﬀerent datasets: The ﬁrst dataset
(calledAbelow) contained all 1576 bugs introduced in the
Apache 2.0 branch. The second one contained the addi-
tional 65 bugs found by Justin (called J). Table 4 shows
the resulting accuracies for training and predicting on each
combination of these two datasets.
Consider training on the extracted data Aand predicting
onthesamedata. Thisprovidesabaselineaccuracyof0 .875.
If the prediction is, however, performed on the dataset rep-
resenting ground truth for the period of manual annotation
A∪Jthentheaccuracyfallsto0 .870. Weaccedethatdueto
the limited manually annotated period the diﬀerence—like
all the diﬀerences in the table—is not signiﬁcant. But as the
following shows we can recognize a tendency. Alternatively,
consider adding the manually annotated bugs to the train-
ing set (i.e., training on A∪J). In each possible prediction
target (i.e., A,J, andA∪J) we ﬁnd that the availability
of the additional information actually leads to an improve-
ment in prediction accuracy. This is especially impressive
where the prediction target is Aas it shows that the man-
ually annotated bugs actually contain information relevant
to the automatically extracted ones helping BugCache to
ﬁnd four additional bugs.
Table 4: BugCache Prediction Quality
Learning Set Test Set Accuracy 95% Conﬁdence Interval
A A 0.875 0.858, 0.890
A A ∪J 0.870 0.852, 0.885
A J 0.738 0.620, 0.830
A∪J A 0.878 0.860, 0.893
A∪J A ∪J 0.874 0.857, 0.889
A∪J J 0.785 0.670, 0.8678. DISCUSSION AND CONCLUSIONS
In this paper, we analyzed three main research questions
andtriedtoﬁnd“groundtruth”inthecommitannotationsof
a very popular software engineering dataset. We used tem-
poral sampling to deﬁne an evaluation subset of the original
Apache dataset and manually annotated all commits, with
the assistance of an Apache core developer and the use of
Linkster .
As presented in our previous work, bias in empirical soft-
ware engineering datasets may aﬀect results of applications
which rely on such data [10]. Unfortunately, based on our
data veriﬁcation, we found that things are even worse: our
ﬁndings cast doubt on some of the core assumptions made
in empirical research. Speciﬁcally:
1. Bugsoftengoincognitoastheyarenotalwaysreported
as a bug in the bug tracker but, e.g., in mailing lists,
and
2. commits not always clearly change the functionality of
the program.
Speciﬁcally, weshowedthatnotallﬁxedbugsarereportedin
the bug tracking database and most of the commits (62.9%)
are not related to a bug ﬁx or feature request (which would
introduce a program change) rather than for documenta-
tion (32%), voting (5.3%), or releases (8.9%). In addition,
we presented the curious case of backport commits and the
challenging impact-of-defect vs. cause-of-defect problem.
Bothissueshaveanimpactonsoftwareengineeringdatasets.
Consequently, even though automated linkage tools are able
to connect a remarkable number of commits to bugs reports,
many bugs—sometimes the most critical ones—never show
up in the bug tracker and are, therefore, not linked. This
raises new issues concerning the validity of studies that rely
on version control and bug report data only—beyond what
we reported earlier [10]. We presented a detailed examina-
tion of the bias in automatically linked set, when compared
to the manually linked set. Especially notable is the sig-
niﬁcant variation in linking behavior among developers, and
the anecdotal evidence suggestingthat bug-ﬁxing experienc e
and code ownership play a role in linking behaviour. We also
showed that BugCache has a strong tendency to miss pre-
dictions if it is not trained on ground truth.
Another implication of the work presented here is that
empirical software engineering studies will need to take the
whole software development social eco-system (revision con-
trol system, bug tracking database, mailing list systems,
email discussions, discussion boards, chats, etc. as well as
thesedatafromother, relatedprojects)intoaccountinorderto elicit a more complete picture of the underlying develop-
ment process. This would allow to capture the nature of
some of the bugs and commits that our informant tediously
collected manually.
Nonetheless, this study is only a ﬁrst step towards quality-
approved datasets and we acknowledge that we were only
able to verify a small subset of the overall Apache dataset.
Therefore, we hope to inﬂuence the community to seek more
ground truth for more software engineering datasets.
Granted, such work would entail a signiﬁcant manual la-
bor, but, undoubtedly, the resulting valuable improvements
in data ﬁdelity will serve the community well in years to
come. We seek mechanisms for fostering this community
eﬀort, and welcome suggestions from readers to this end.
Acknowledgment
Many thanks to Dr. Justin Erenkrantz for the time he spent
in Zurich annotating commits and providing feedback to
theApache dataset and Linkster . This work was sup-
ported by Zurich Cantonal Bank (Bachmann), U.S. NSF
SoD-TEAM 0613949 and an IBM Faculty Fellowship (Bird,
Rahman, and Devanbu), and Swiss National Science Foun-
dation award number 200021-112330 (Bernstein).
9. REFERENCES
[1] J. Aranda and G. Venolia. The secret life of bugs: Going
past the errors and omissions in software repositories. In
ICSE’09 , pages 298–308, May 2009.
[2] E. Aune, A. Bachmann, A. Bernstein, C. Bird, and
P. Devanbu. Looking back on prediction: A retrospective
evaluation of bug-prediction techniques. Student Researc h
Forum at SIGSOFT 2008/FSE 16, November 2008.
[3] A. Bachmann and A. Bernstein. Data retrieval, processin g
and linking for software process data analysis. Technical
Report IFI-2009.0003, Department of Informatics,
University of Zurich, May 2009.
[4] A. Bachmann and A. Bernstein. Software process data
quality and characteristics - a historical view on open and
closed source projects. In IWPSE-Evol’09 , pages 119–128,
Amsterdam, The Netherlands, August 2009.
[5] A. Bachmann and A. Bernstein. When process data quality
aﬀects the number of bugs: Correlations in software
engineering datasets. In MSR’10 , pages 62–71, Cape Town,
South Africa, May 2010. IEEE Computer Society.
[6] A. Bernstein, J. Ekanayake, and M. Pinzger. Improving
defect prediction using temporal features and non linear
models. In IWPSE’07 , pages 11–18, Dubrovnik, Croatia,
September 2007.
[7] N. Bettenburg, S. Just, A. Schroeter, C. Weiss, R. Premra j,
and T. Zimmermann. Quality of bug reports in eclipse. In
eTX’07, pages 21–25, Montreal, Canada, October 2007.
[8] N. Bettenburg, S. Just, A. Schroeter, C. Weiss, R. Premra j,
and T. Zimmermann. What makes a good bug report?
Technical report, Saarland University, Saarbr ¨ucken,
Germany, September 2007.
[9] N. Bettenburg, R. Premraj, T. Zimmermann, and S. Kim.
Duplicate bug reports considered harmful... really? In
ICSM’08 , pages 337–345, October 2008.
[10] C. Bird, A. Bachmann, E. Aune, J. Duﬀy, A. Bernstein,
V. Filkov, and P. Devanbu. Fair and balanced? bias in
bug-ﬁx datasets. In ESEC/FSE’09 , pages 121–130,
Amsterdam, The Netherlands, August 2009.
[11] C. Bird, P. C. Rigby, E. T. Barr, D. J. Hamilton, D. M.
German, and P. Devanbu. The promises and perils of
mining git. In MSR’09 , pages 1–10, Vancouver, Canada,
May 2009. IEEE Computer Society.
[12] K. Chen, S. R. Schach, L. Yu, J. Oﬀutt, and G. Z. Heller.
Open-source change logs. Emp. Softw. Eng. , 9(3):197–210,
2004.[13] M. Fischer, M. Pinzger, and H. Gall. Analyzing and
relating bug report data for feature tracking. In WCRE’03 ,
pages 90–99, Victoria, B.C., Canada, November 2003.
[14] M. Fischer, M. Pinzger, and H. C. Gall. Populating a
release history database from version control and bug
tracking systems. In ICSM’03 , pages 23–32, Amsterdam,
Netherlands, September 2003.
[15] D. M. German. Mining cvs repositories, the softchange
experience. In MSR’04 , pages 17–21, Edinburgh, Scotland,
UK, May 2004. ACM.
[16] P. Hooimeijer and W. Weimer. Modeling bug report
quality. In ASE’07, pages 34–43, Atlanta, Georgia, USA,
November 2007.
[17] H. Joshi, C. Zhang, S. Ramaswamy, and C. Bayrak. Local
and global recency weighting approach to bug prediction.
InMSR’07 , pages 33–34, Minneapolis, Minnesota, USA,
May 2007. IEEE Computer Society.
[18] S. Just, R. Premraj, and T. Zimmermann. Towards the
next generation of bug tracking systems. In VL/HCC’08 ,
pages 82–85, September 2008.
[19] S. Kim, T. Zimmermann, E. J. Whitehead Jr., and
A. Zeller. Predicting faults from cached history. In
ICSE’07 , pages 489–498, Washington, DC, USA, 2007.
[20] A. J. Ko, B. A. Myers, and D. H. Chau. A linguistic
analysis of how people describe software problems. In
VL/HCC’06 , pages 127–134, Washington, DC, USA, 2006.
[21] A. G. Koru and J. Tian. Defect handling in medium and
large open source projects. IEEE Softw. , 21(4):54–61, 2004.
[22] G. Liebchen, B. Twala, M. Shepperd, M. Cartwright, and
M. Stephens. Filtering, robust ﬁltering, polishing:
Techniques for addressing quality in software data. In
ESEM’07 , pages 99–106, Washington, DC, USA, 2007.
[23] G. A. Liebchen and M. Shepperd. Data sets and data
quality in software engineering. In PROMISE’08 , pages
39–44, New York, NY, USA, 2008.
[24] J. Michaud, M.-A. Storey, and H. Muller. Integrating
information sources for visualizing java programs. In
ICSM’01 , pages 250–258, Florence, Italy, November 2001.
IEEE Computer Society.
[25] A. Mockus, R. T. Fielding, and J. D. Herbsleb. Two case
studies of open source software development: Apache and
mozilla. ACM Trans. Softw. Eng. Methodol. ,
11(3):309–346, 2002.
[26] Netcraft Ltd. May 2010 web server survey.
http://news.netcraft.com/archives/2010/05/14/
may_2010_web_server_survey.html , May 2010.
[27] M.-T. J. Ostrand, F.-E. J. Weyuker, and R. M. Bell.
Predicting the location and number of faults in large
software systems. IEEE Trans. Softw. Eng. , 31(4):340–355,
2005.
[28] J. W. Paulson, G. Succi, and A. Eberlein. An empirical
study of open-source and closed-source software products.
IEEE Trans. Softw. Eng. , 30(4):246–256, 2004.
[29] J. Ratzinger, M. Fischer, and H. C. Gall. Evolens:
Lens-view visualizations of evolution data. In IWPSE’05 ,
pages 103–112, Lisbon, Portugal, September 2005.
[30] A. Schr ¨oter, T. Zimmermann, R. Premraj, and A. Zeller. If
your bug database could talk... In ICSE’06 , pages 18–20,
Rio de Janeiro, Brazil, September 2006.
[31] J.´Sliwerski, T. Zimmermann, and A. Zeller. When do
changes induce ﬁxes? In MSR’05 , pages 24–28, Saint Louis,
Missouri, USA, May 2005. ACM.
[32] D. ˇCubrani´ c and G. C. Murphy. Hipikat: Recommending
pertinent software development artifacts. In ICSE’03 , pages
408–418, Washington, DC, USA, 2003.
[33] T. Zimmermann, R. Premraj, and A. Zeller. Predicting
defects for eclipse. In PROMISE’07 , pages 1–9,
Minneapolis, Minnesota, USA, May 2007. IEEE Computer
Society.
[34] T. Zimmermann and P. Weissgerber. Preprocessing cvs
data for ﬁne-grained analysis. In MSR’04 , pages 2–6,
Edinburgh, Scotland, UK, May 2004. ACM.