See discussions, st ats, and author pr ofiles f or this public ation at : https://www .researchgate.ne t/public ation/221560736
Golden implementation driven software debugging
Conf erence Paper  ¬∑ No vember 2010
DOI: 10.1145/1882291.1882319 ¬†¬∑¬†Sour ce: DBLP
CITATIONS
33READS
137
4 author s, including:
Ansuman Baner jee
Indian St atistic al Instit ute
167 PUBLICA TIONS ¬†¬†¬†983 CITATIONS ¬†¬†¬†
SEE PROFILE
Abhik R oychoudhur y
National Univ ersity of Sing apor e
256 PUBLICA TIONS ¬†¬†¬†10,608  CITATIONS ¬†¬†¬†
SEE PROFILE
Zhenk ai Liang
National Univ ersity of Sing apor e
24 PUBLICA TIONS ¬†¬†¬†2,203  CITATIONS ¬†¬†¬†
SEE PROFILE
All c ontent f ollo wing this p age was uplo aded b y Abhik R oychoudhur y on 20 May 2014.
The user has r equest ed enhanc ement of the do wnlo aded file.Golden Implementation Driven Software Debugging
Ansuman Banerjee, Abhik Roychoudhury, Johannes A. Harlie, Zhenkai Liang
National University of Singapore
{banerjee,abhik,johannes,liangzk}@comp.nus.edu.sg
ABSTRACT
The presence of a functionally correct golden implementation has a
signiÔ¨Åcant advantage in the software development life cycle. Such
a golden implementation is exploited for software development in
several domains, including embedded software ‚Äî a low resource-
consuming version of the golden implementation. The golden im-
plementation gives the functionality that the program is supposed
to implement, and is used as a guide during the software develop-
ment process. In this paper, we investigate the possibility of using
the golden implementation as a reference model in software debug-
ging. We perform a substantial case study involving the Busybox
embedded Linux utilities while treating the GNU Core Utilities as
the golden or reference implementation. Our debugging method
consists of dynamic slicing with respect to the observable error in
both the implementations (the golden implementation as well as the
buggy software). During dynamic slicing we also perform a step-
by-step weakest precondition computation of the observable error
with respect to the statements in the dynamic slice. The formulae
computed as weakest pre-condition in the two implementations are
then compared to accurately locate the root cause of a given observ-
able error. Experimental results obtained from Busybox suggest
that our method performs well in practice and is able to pinpoint
all the bugs recently published in [8] that could be reproduced on
Busybox version 1.4.2. The bug report produced by our approach
is concise and pinpoints the program locations inside the Busybox
source that contribute to the difference in behavior.
Categories and Subject Descriptors
D.2.5 [ Software Engineering ]: Testing and Debugging‚Äî Debug-
ging aids, Symbolic execution ; D.3.4 [ Programming Languages ]:
Processors‚Äî Debuggers
General Terms
Experimentation, Reliability
Keywords
Embedded Linux, Software Evolution, Symbolic Execution
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proÔ¨Åt or commercial advantage and that copies
bear this notice and the full citation on the Ô¨Årst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciÔ¨Åc
permission and/or a fee.
FSE-18, November 7‚Äì11, 2010, Santa Fe, New Mexico, USA.
Copyright 2010 ACM 978-1-60558-791-2/10/11 ...$5.00.1. INTRODUCTION
Software debugging can be an extremely time-consuming activ-
ity. It seeks to answer the following question - given a program P
and a test input twhich fails in P, how to explain the failure of tin
P? Debugging usually returns a bug report - a fragment of Pthat
is highlighted to the programmer.
We observe that debugging of a given test input in a buggy pro-
gram can be aided by the presence of a golden implementation.
This golden implementation captures how the buggy program is
‚Äúsupposed‚Äù to behave. The presence of such golden implementa-
tions are common in many domains, such as for embedded software
(where the embedded software is derived as a low-resource con-
suming equivalent of a golden implementation) or for web-servers
(where both the buggy and the golden implementations implement
the HTTP protocol). It should be noted that the golden implemen-
tation often employs different data structures as compared to the
program being debugged. To exploit the golden implementation in
software debugging, the debugging method should be based on se-
mantic analysis. Evolving programs (where the code of a program
evolves from one version to another1) can be viewed as a special
case of golden implementation ‚Äî where the older version of an
evolving program can be treated as a golden implementation. For
evolving programs, the older version or golden implementation is
quite similar (in terms of code) to the buggy implementation.
In this paper, we study how the golden implementation can be
effectively exploited for software debugging. Our method involves
simultaneously performing dynamic slicing and symbolic execu-
tion in both the programs - the golden implementation as well as
the buggy implementation. The slicing is performed with respect
to the observable error, which is set as the slicing criteria. More-
over, given the speciÔ¨Åcation of the observable error, we perform a
step-by-step weakest pre-condition calculation along the dynamic
slice. The resultant weakest pre-conditions accumulated in the two
programs are then compared to Ô¨Ånd new / missing constraints accu-
mulated in the buggy implementation. The lines of code contribut-
ing to these constraints form our bug report.
We employ our methods on Busybox [2] ‚Äî the de-facto standard
for Embedded Linux devices. It provides many of the standard
Linux utilities, but has a smaller code size (size of the executable)
than the GNU Core Utilities, net-tools and procps. We choose two
versions of Busybox, namely, version 1.4.2 and the latest version
1.16.0 and employ our methods to Ô¨Ånd the root causes of errors
that have previously been reported in very recent literature [8]. In
1Here we consider software evolution where the new program ver-
sion may have additional functionality, but the requirements for
the functionality which are common across the versions are not
changed. In other words, the code for the common functionality
may have evolved, but not the requirements.particular, [8] reports a test generation method which is used to
Ô¨Ånd problematic test inputs ( i.e.failing test cases) for the Busybox
tool suite. In this paper, we seek to Ô¨Ånd an accurate bug report
explaining the failure of these test cases. It should be noted that
all of these errors are realfaults done by programmers, rather than
being deliberately seeded faults.
At this point, let us step back and study whether any existing
debugging method can pinpoint the errors in Busybox. This will
also inspire and drive us to develop a new debugging method.
Will Delta Debugging [21] work. Note that the golden im-
plementation (in this case, the Coreutils, net-tools and procps) and
the buggy implementation (Busybox) are supposed to be function-
ally equivalent. A natural question that arises is as follows ‚Äî can
we simply treat Coreutils and Busybox as two program versions?
Unfortunately, the answer is no! Due to the demands of low re-
source consumption and less code-size, Busybox often employs
completely different algorithms and data structures to perform the
same tasks as in Coreutils. For example, the implementation of the
trutility is quite different in terms of source level implementation.
Moreover, Busybox implements wrapper functions over many of
the system function implementations (e.g. getOpt ) that correspond
to a lightweight variant of the original utilities. As a result, we can-
not use debugging methods such as delta-debugging [21], which
enumerates and analyzes the changes across program versions ‚Äî
the code is greatly changed in going from Coreutils to Busybox!
Will DARWIN [18] work. In a recent work, we have devel-
oped the DARWIN approach [18], which can simultaneously ana-
lyze the behavior of a test input in two programs that are supposed
to be semantically equivalent. Thus, the method can be applied
to two versions of a program, or two completely different imple-
mentations (possibly with different algorithms and data structures)
of the same speciÔ¨Åcation. However, while employing the DAR-
WIN method for explaining the Busybox bugs, we came across a
technical issue. The DARWIN method is most suited for explain-
ing errors that are exposed by a difference in control Ô¨Çow ‚Äî er-
rors in branch statements, or errors due to missing code containing
branches. In the Busybox toolkit, the failing test inputs often fol-
lowed ‚Äúequivalent" paths ‚Äî that is, for Coreutils and Busybox -
the path conditions of the failing test input (which is computed by
symbolic execution) on the two are found to be logically equivalent.
This necessitated looking beyond the DARWIN method.
A New Debugging Method. Our proposed method works as
follows. Given the failing test input t, the golden implementation
Pand the buggy implementation P0- we Ô¨Årst characterize the ob-
servable error in terms of an output variable getting an unexpected
value at the end of the program. If the observable error is a pro-
gram crash, we cast it as a variable getting an unexpected value at
the crash site. In either case, debugging now boils down to answer-
ing a query of the form ‚Äî why is variable out> 0at (the last visit
of) line number LwhereLis the site of output (or a site of program
crash). We answer such a question as follows.
1. We perform (backwards) dynamic slicing on the execution
trace oftin programs PandP0. The slicing criteria is (L;out ),
taken from the query we are seeking to answer.
2. While traversing the execution trace during dynamic slicing,
we also compute the weakest precondition (WP) of the formula
out> 0. Note that the weakest pre-condition is only computed for
the statements that are in the dynamic slice. It terminates when the
slicing terminates.
Having performed the two steps given in the preceding, we com-
pute two formulae 'and'0as the weakest pre-condition (WP) in
programsPandP0. Since we have computed WP on a single exe-cution trace, these formulae are conjunctions of the form
'= 1^:::^ mand'0= 0
1^:::^ 0
n
We Ô¨Ånd a constraint  0
jwhich is not logically implied by '(or
symmetrically a constraint  iwhich is not logically implied by
'0). Once such a constraint  0
j(or i) is found, we locate the lines
in the program P0(orP) which contributed to  0
j(or i) thereby
constructing our bug report.
Contributions. Concretely, the contributions of this paper are
as follows. We study the problem of debugging software with re-
spect to its golden implementation. Our new debugging method
combines dynamic slicing with weakest pre-condition computa-
tion. Symbolic execution along the dynamic slice is performed in
both golden and buggy implementations to compute the weakest
pre-condition with respect to the observable error. This gives us a
logical explanation of why the error appears in the buggy imple-
mentation. Our debugging method can also be used to root cause
code errors resulting from software evolution where a test case
passes in an old stable program and fails in a new buggy program.
To stress our method, we perform a very substantial case study in-
volving the Busybox Embedded Linux utilities, root-causing bugs
in Busybox utilities as compared to GNU Core Utilities. Our exper-
imental results suggest that our method performs well in practice
and is able to pinpoint all the bugs published in [8] that could be
reproduced on Busybox version 1.4.2 and also on the latest version
of Busybox.
2. OVERVIEW
In this section, we Ô¨Årst present an overview of our approach via
an illustrative example. The ARP (Address Resolution Protocol)
utility is used as an example to demonstrate the working of our
methodology.
2.1 A Motivating Example
Program P: Golden Implementation
1 #include <stdio.h>
2
3 const char *get_hwtype (const char *name) {
4 return name;
5 }
6
7
8 int main(int argc, char **argv) {
9 const char *hw = NULL;
10 const char *ap = NULL;
11 int hw_set = 0;
12 switch ( *argv[1]) {
13 case ‚ÄôA‚Äô: case ‚Äôp‚Äô: {
14 ap = argv[2];
15 break;
16 }
17 case ‚ÄôH‚Äô: case ‚Äôt‚Äô: {
18 hw = get_hwtype(argv[3]);
19 hw_set = 1;
20 break;
21 }
22 default : break;
23 }
24 if ((hw_set == 0) && ( *argv[1] != ‚ÄôH‚Äô))
25 hw = get_hwtype ("DFLT_HW");
26 printf ("%s %s\n", ap, hw);
27 }
Figure 1: SimpliÔ¨Åed fragment of ARP in Coreutils/Net-tools
We create a simpliÔ¨Åed version of the actual ARP implementation
in Busybox and its corresponding variant in Coreutils/net-tools for
the purpose of illustration of our philosophy. Consider the pro-
gram fragment Pin Figure 1. We can consider this as the goldenimplementation. The variant of the Busybox ARP implementation
implementing the same functionality is given as the program P0in
Figure 2. Program P0is the buggy program version.
Program P‚Äô: Buggy Implementation
1 #include <stdio.h>
2
3 const char *get_hwtype (const char *name) {
4 return name;
5 }
6
7
8 int main(int argc, char **argv) {
9 const char *hw = NULL;
10 const char *ap = NULL;
11 int hw_set = 0;
12 if ( *argv[1] == ‚ÄôA‚Äô || *argv[1] == ‚Äôp‚Äô) {
13 ap = argv[2];
14 if ((hw_set == 0) && ( *argv[1] != ‚ÄôH‚Äô))
15 hw = get_hwtype(argv[3]);
16 }
17 printf ("%s %s\n", ap, hw);
18 }
Figure 2: SimpliÔ¨Åed fragment of ARP utility in Busybox
Consider an execution of the two programs as cuArp A inet
andbbArp A inet , where bbArp and cuArp are respectively the
names of the executables resulting out of P and P‚Äô. Figure 3 shows
a source-level view of the execution traces of the two programs.
Evidently, the output of cuArp is as expected inetDFLT _HW ,
while the output of bbArp is inet NULL (sinceargv[3]is NULL)
which is undesirable. As an objective of investigating the incorrect
value of hardware type, we set out to Ô¨Ånd the root cause as to why
the variablehwis set to a NULL value at the end of the program
execution.
2.2 Will other methods work?
We now show the working of existing debugging methods.
Trace comparison. In our example, the two programs are quite
different from the implementation perspective, the only resemblance
being the structure of the get_hwtype function. Hence, an attempt
to establish a mapping of the execution traces of the two programs
may be quite an arduous task. Consider the traces shown in Fig-
ure 3. The only similarities they share are in lines {9, 10, 11}
and the rest of the traces are different and exhibit different execu-
tion Ô¨Çow. Any difference metric we choose to compare the traces
(statements executed, set/sequence of branches executed and so on)
will report a large difference between the traces, and will be unsuc-
cessful in pinpointing the root cause.
DARWIN. We now employ the DARWIN method [18] on the ex-
ample program. For the sake of completeness, we brieÔ¨Çy describe
the basic DARWIN methodology here. Given the programs P and
P‚Äô, DARWIN will proceed as follows:
1. Run program Pfor test input A inet , and calculate the re-
sultant path condition f, a formula representing set of inputs
that exercise the same path as that of A inet in programP.
In our example, the path condition fis as follows:
(argv[1] == ‚ÄôA‚Äôjjargv[1] == ‚Äôp‚Äô)^(hw_set==
0)^(argv[1]6=‚ÄôH‚Äô).
2. Run program P0for test input A inet , and calculate the re-
sultant path condition f0, a formula representing set of inputs
which exercise the same path as that of A inet in programP0.
In our example, the path condition f0is as follows:
9c o n s t ¬†char¬†*hw¬†=¬†NULL;
10 const ¬†char¬†*ap =¬†NULL;9c o n s t ¬†char¬†*hw¬†=¬†NULL;
10 const ¬†char¬†*ap =¬†NULL;
1 1 i n th w _ s e t= ¬†0;
12 if¬†(*argv[1] ¬†==¬†'A'¬†||¬†*argv[1]¬†==¬†'p‚Äò)¬†{
13¬†¬†¬†¬†¬†¬†¬†¬†¬† ap =¬†argv[2];
14 if ¬†((hw_set == ¬†0)¬†¬†&&11¬†¬†¬†¬†i n th w _ s e t= ¬†0;
12¬†¬†¬†¬†switch¬†(*argv[1]) ¬†{
13¬†¬†¬†¬†¬†¬†¬†¬†¬† case¬†'A':¬†case¬†'p':¬†{
14¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬† ap =¬†argv[2];
(*argv[1] ¬†!=¬†'H'))¬†
15 hw¬†=¬†¬†get_hwtype (argv[3]);
3¬†¬†¬†¬†¬†const¬†char*¬†get_hwtype (const ¬†char¬†*name)
4¬†¬†¬†¬†¬†¬†¬†¬†¬† return¬†name;15¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬† break;
16¬†¬†¬†¬†¬†¬†¬†¬†¬† }
23¬†¬†¬†¬†¬†}
24¬†¬†¬†¬†¬†if¬†((hw_set == ¬†0)¬†&&¬†
16¬†¬†¬†¬†¬†}
17¬†¬†¬†¬†¬†printf ("%s ¬†%s\n",¬†ap,¬†hw);
18¬†}(*argv[1] ¬†!=¬†'H'))
25¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬† hw¬†=¬†get_hwtype ("DFLT_HW");
3¬†¬†¬†¬†¬†const¬†char*¬†get_hwtype (const ¬†char¬†*name)
4¬†¬†¬†¬†¬†return¬†name
26¬†¬†¬†¬†¬†printf ("%s ¬†%s\n",¬†ap,¬†hw);
27¬†}
Trace¬†from¬†Program¬†P¬†shown¬†in¬†Figure¬†1 Trace¬†from¬†Program¬†P‚Äô¬†shown¬†in¬†Figure¬†2 f g gFigure 3: Execution Traces of the programs from Figure 1 and 2
(argv[1] == ‚ÄôA‚Äôjjargv[1] == ‚Äôp‚Äô)^(hw_set==
0)^(argv[1]6=‚ÄôH‚Äô).
3. Here we Ô¨Ånd that fandf0are equivalent, that is, f,f0.
Thus, no solutions can be found for f^:f0orf0^:f. The
DARWIN method fails to create a bug report!
The DARWIN method requires the path conditions of the test
inputtbeing debugged to be different in the two implementations.
In other words, the effect of the bug should appear via a difference
in control Ô¨Çow. As a result, assignment errors can only be handled
by making the effect of a wrong assignment observable, usually
through explicit instrumentation of predicates into the code, which
was not done in this case.
2.3 Our method at work
We now explain the working of our debugging method. We
perform backward dynamic slicing and simultaneous weakest pre-
condition computation on the execution traces produced by P and
P‚Äô. We chose the slicing criteria by analyzing the observable error
or the crash location. The resulting slice includes all the statements
in the program that inÔ¨Çuence the slicing criteria through data or
control dependencies on the entire execution trace.
The backward dynamic slice computation is inter-procedural and
attempts to compute the transitive closure of the data and control
dependencies encountered in the path from the slicing point (in our
case, the bug location) to the start of the execution trace.
The weakest pre-condition computed is simply a conjunction of
constraints on the input variables (that contribute to the satisfaction
of the post-condition attributed to the slicing criteria at the slicing
point). A comparison of the slices (more speciÔ¨Åcally, the weakest
pre-conditions) reveals the difference in transformation induced in
the two program executions. This helps us pinpoint the root cause
of the error. Our core method works as follows.
1. We run the program Pfor test input A inet , and record the
execution trace (at the binary level). The execution trace at
the source level is shown in Fig 3.
2. We run the program P0for test input A inet , and record the
execution trace 0(at the binary level).
3. We identify hwas the slicing variable, and the observable
error involves hwbeing output as NULL in program P0.
4. We perform backwards dynamic slicing on the execution trace
inP. The slicing criteria is (26; hw), since 26is the line
wherehwis printed. While computing the dynamic slice, we
compute the weakest pre-condition (henceforth called WP)
for the post-condition (hw6=NULL ). Recall that theobservable error was in hwbeingNULL in the buggy pro-
gramP0. The execution of the same test input in the correct
programPproduces the non-NULL value DFLT _HW for
hw. Hence, we compute the weakest pre-condition of the
formulahw6=NULL along the dynamic slice. The dy-
namic slice consists of the statements (at the source level) {3,
4, 9, 11, 24, 25, 26 }. The WP computed, '1for the post-
condition (hw6=NULL )is as follows:
(argv[1]6=‚ÄôH‚Äô)^
(get_hwtype (\DFLT _HW")6=NULL ).
5. We perform backwards dynamic slicing on the execution trace
0in the buggy program P0. The slicing criteria is (17; hw)
since 17is the line where hwis printed. While computing the
backward slice, we also compute the weakest pre-condition
for the post-condition (hw==NULL ). This is because the
observable error lies in hwbeingNULL in the buggy pro-
gramP0. The slice consists of the statements (at the source
level) {3, 4, 9, 11, 12, 14, 17}. The WP computed, '2for
the post-condition (hw==NULL )is as follows:
(get_hwtype (argv[3]) ==NULL )^(argv[1]6=‚ÄôH‚Äô)^
(argv[1] == ‚ÄôA‚Äôjjargv[1] == ‚Äôp‚Äô).
6. We compare '1and'2to deduce the difference in program
executions. It is worth noting that since the WP is computed
on a trace, the resulting WP is a conjunction of constraints.
We proceed as follows.
We check if there is any unexplained constraint in '2that
is not implied by '1. This is a straightforward implication
check, for each conjunct  iin'2, where'2= 1^ 2:::^
 k, we check if '1) i. If'16) ifor some i, we pro-
duce it as an unexplained WP constraint. In our example, the
constraint (*argv[1] == ‚ÄôA‚Äô || *argv[1] == ‚Äôp‚Äô) in the WP is
unexplained and this was contributed by the incorrect func-
tional check in line number 12. This helps us identify func-
tionality errors. In fact, this revealed the bug in the Busybox
ARP utility and also 4 of the 6 published Busybox bugs we
discuss in this paper.
Similarly, we look for an unexplained constraint in '1that
which is not implied by '2. In fact, for the printf andlsbugs
in Busybox, such unexplained constraints in '1pointed to
code missing errors ‚Äî some necessary program fragment
that is present in Coreutils, but is missing from Busybox.
It is worth noting that both the above steps are performed at the
binary level and we map them back to the source using compiler
level information (about symbol table). Thus the bug report re-
turned by our method is at the source code level.
3. DETAILED METHODOLOGY
In this section, we elaborate on the different steps of our method.
3.1 Slice computation
In this phase, we need to execute the test input tunder examina-
tion in both the programs. We Ô¨Årst concretely execute tand record
the trace for both the programs. This is followed by identifying the
slicing criteria and computing the backward slice with respect to
the slicing criteria.
Identifying the Slicing criteria. Given two programs, one of
which is the golden implementation (let us call it Phenceforth),
and the other, a lightweight implementation (we call it P0hence-
forth), we intend to characterize the difference in behavior betweenthe executions of PandP0on the test input t. To this effect, our
Ô¨Årst step is to set up a slicing criteria that can be used in both the
programs. Since both the programs are expected to implement the
same speciÔ¨Åcation, it is not difÔ¨Åcult to establish a mapping of the
output signatures of the two programs obtained by executing them.
The difference in output between the two programs leads us to the
slicing criteria (the output variable vwhose value differs in the two
programs) and the slicing point (the line number where variable v
is output). Our choice of the slicing criteria is found by analyz-
ing the program P0and analyzing the manifestation of the wrong
behavior. The wrong behavior may simply be an incorrect / unex-
pected value of a program variable at a location Lof the program,
or de-referencing of NULL pointers at some location L, or over-
writing of critical data such as return addresses. In all these cases,
the slicing criteria is set on the value of a variable outgetting an
unexpected value at line number L(the last visit of line number L
if it is visited many times). In our example given in Section 2.1,
the slicing criteria on the program Pin Figure 1 is set as h26; hwi
andh17; hwifor program P0in Figure 2.
Backward Dynamic Slicing. The backward dynamic slicing
step takes in the following: (a) a trace obtained by executing a
programPon test input t, and (b) a slicing criteria of the form
hL;outi. The output of the method is a fragment of (andP)
that is likely to be responsible for the observed error. The dynamic
slice routine computes a chain of dynamic data and control depen-
dencies. Formally, given a slicing criteria hL;oution a trace
obtained by executing the program Pon a test input t, a dynamic
slice contains all statement instances (or statements) that have af-
fected the value of variable outreferenced at Lin the trace. The
dynamic slicing algorithm is standard. We provide below a brief
discussion of this algorithm. During the trace traversal that starts
from the statement in the slicing criterion, a dynamic slicing al-
gorithm maintains the following quantities: (a) the dynamic slice
, (b) a set of variables whose dynamic data dependencies need
to be explained, and (c) a set of statement instances whose dy-
namic control dependencies need to be explained. Initially, we set
the following: == last instance of Lin the traceofP, and
=foutg. For each statement instance stmt encountered during
the backward traversal, the algorithm performs the following two
checks. The algorithm terminates when we reach the beginning of
the trace.
Check dynamic data dependencies: Letvstmt
def be the vari-
able deÔ¨Åned by stmt. If vstmt
def2, we have found the deÔ¨Å-
nition ofvstmt
def which the slicing algorithm was looking for.
Sovstmt
def is removed from and variables used by stmt are
inserted into . In addition, stmt is added toand.
Check dynamic control dependencies: If any statement in-
stance inis dynamically control dependent on stmt , all
statement instances that are dynamically control dependent
onstmt are removed from . Variables used by stmt are
inserted into andstmt is inserted into and.
The resultant slice (the set ) is reported as the Ô¨Ånal dynamic slice.
In our example program Pin Figure 1, we Ô¨Ånd the following de-
pendencies given in terms of line numbers shown on the execution
trace in Figure 3: 26 is data dependent on 25, 25 is data dependent
on 9 and 4, and control dependent on 24, 24 is data dependent on
11, 4 is data dependent on 3. Therefore, the dynamic slice com-
puted by our algorithm is h3;4;9;11;24;25;26i. In a similar fash-
ion, the dynamic slice with respect to the slicing criteria h17;hwi
obtained for program P0in Figure 2 ish3;4;9;11;12;14;17i.3.2 Weakest pre-condition (WP) along a slice
Weakest pre-condition (of a given post-condition c) along a pro-
gram path can be deÔ¨Åned as follows. Let be a sequence of in-
structionshi1;:::i niin programP, whereinis our post-condition
point wherecshould hold true. Inductively we calculate wp(in;c)
=cond n 1, thenwp(in 1;cond n 1)=cond n 2and so on. The
weakest pre-condition of calongis then the formula cond 0ob-
tained when we reach the beginning of the trace. The resulting
formula is satisÔ¨Åed by all program inputs that follow the given ex-
ecution trace in programPand satisfycin control location in.
We now elaborate on the weakest pre-condition (WP) computa-
tion method that is at the core of our approach. The WP compu-
tation proceeds along with the slice computation. At each step of
the slice computation, the WP is calculated. The WP computation
Ô¨Ånishes once the slice computation completes.
Weakest pre-condition (WP) computation along a trace usually
proceeds by computing the weakest pre-condition for every state-
ment. However, in our case we are computing WP along a dynamic
slice, which makes the computation much more goal-directed. Con-
sider the following schematic example where inp1 andinp2 are
inputs and x, y are outputs.
1. ... // input inp1, inp2
2. if (inp1 > 0)
3. x = f1(inp1)
4. else x = g1(inp1);
5. if (inp2 > 0)
6. y = f2(inp2)
7. else y = g2(inp2);
8. ... // output x, y
Suppose we now perform slicing w.r.t the criteria h8;xifor the
program input inp1 == inp2 == 1 , since the value of xthat
is observed is unexpected: it was expected to be positive but it is
observed to be negative. The execution trace for inp1 == inp2
== 1 is the sequence of statements h1;2;3;5;6;8iand the dy-
namic slice is the set f1;2;3;8g. If we computed WP of x < 0
w.r.t. the execution trace we get
inp2>0^inp1>0^f1(inp1)<0
whereas the WP along the dynamic slice simply gives us inp1>
0^f1(inp1)<0. Since the value of inp2is irrelevant to the
computation of x(and hence for explaining why x<0at the end of
the program) the WP along the slice does not include the constraint
inp2>0.
WP computation rules. To compute the weakest pre-condition
(WP), we need to Ô¨Årst set a post-condition c, with respect to which
the weakest pre-condition is to be computed. This is straightfor-
ward in our case, and we use the slicing criteria hL;outito gener-
ate the post-condition. For our example program Pin Figure 1, we
choose the post-condition as hw6=NULL and for program P0,
we set it ashw==NULL .
We now discuss the technical issues in WP computation along a
dynamic slice. Since only two kinds of statements, namely data de-
pendencies and control dependencies, show up when we compute
the WP simultaneously with the backward dynamic slice, our WP
computation algorithm is simple, as enunciated in the following.
During the trace traversal that starts from the statement in the
slicing criterion, the WP computation algorithm maintains the fol-
lowing (a) the current WP Qcomputed so far (this is a conjunc-
tion of constraints), and (b) an array Aof structures, where each
element is a tuple hlP;Picontaining the source line number lP
for each conjunctPinQ. IntuitivelyAcaptures a mapping of
constraints to line numbers, that is, for every conjunct  in WP, it
captures the source code line numbers contributing to  .Initially,Q=candA=hL;ciwhereLis the line number from
the slicing criterion. This helps us in tracing back each WP con-
straint to the source and is useful for the bug report construction.
For each statement instance stmt encountered during the backward
slicing, the algorithm updates the current WP as follows.
1.Data dependency (Assignment statement): For a statement
stmt of the formx=e, we use the following rule:
wp(x=e;Q) :Q[e=x]
The above rule essentially substitutes efor all occurrences of
variablexin the currently computed WP Q.
2.Control dependency (branch statement): For a control state-
mentstmt involving the condition R, we use the rule:
wp(R;Q) :R^Q
The above rule essentially conjoins the branch condition with
the currently maintained WP.
In both the cases, we add hlstmt;stmtitoAwherelstmt is
the line number corresponding to statement stmt .
The algorithm terminates when we reach the end of the trace and
the slicing routine completes. The resultant WP is reported.
The WPs are computed for both the programs PandP0. Let
us examine the application of the rules given above in computing
the WP for the example program Pgiven in Figure 1. Initially,
the WP is set as the post-condition hw6=NULL . For the pur-
pose of easy correlation, we refer to the statements with respect
to the execution trace shown in Figure 3. While computing the
backward dynamic slice with the slicing criteria h26;hwi, we en-
counter statement 25 (hw=get_hwtype (\DFLT _HW"))as a
data dependency. Therefore, Rule 1 above applies, and we have
the updated WP as get_hwtype (\DFLT _HW")6=NULL .
Proceeding, we encounter statement 24 (if(hw_set== 0)) as
a control dependency. Hence, Rule 2 above applies, and we have
the updated WP as get_hwtype (\DFLT _HW")6=NULL^
(hw_set== 0) . Proceeding in this fashion, we compute the
WP as: (argv[1]6=‚ÄôH‚Äô)^(get_hwtype (\DFLT _HW")6=
NULL ).
In a similar fashion, the WP computed for the program P0with
respect to the post-condition (hw==NULL )is :
(argv[1]6=‚ÄôH‚Äô)^(get_hwtype (argv[3]) =NULL )
^(argv[1] == ‚ÄôA‚Äôjjargv[1] == ‚Äôp‚Äô).
3.3 Putting it All Together
Having performed the WP computation on the programs Pand
P0, we obtain two formulae 'and'0as the respective weakest pre-
condition (WP) in programs PandP0. Since we have computed
WP on (a dynamic slice of) an execution trace, these formulae are
conjunction of constraints, that is
'= 1^:::^ mand'0= 0
1^:::^ 0
n
Our goal is now to Ô¨Ånd a constraint  0
jwhich is not logically im-
plied by'(or symmetrically a constraint  iwhich is not logi-
cally implied by '0). Once such a constraint  0
jis found, we lo-
cate the lines in the program P0fromAwhich contributed to  0
j
thereby constructing our bug report. Similarly we Ô¨Ånd a constraint
 iwhich is not logically implied by '0. Again, we locate the lines
in programPthat contributed to  i‚Äî producing our bug report.
So far, we have presented the execution traces, slices and bug re-
port at the level of statements. In our implementation however, the
traces, slices and bug reports are computed at the level of instruc-
tions. The instruction-level bug report is reverse translated to the
source code level using standard compiler level debug information.4. IMPLEMENTATION
We implemented our approach using the BitBlaze [20] binary
analysis framework as the underlying platform, and using STP [12]
as the constraint solver. In this section, we describe the imple-
mentation of our approach, including the system architecture and
optimizations made in our implementation.
4.1 System architecture
Figure 4 illustrates the system architecture of our implementa-
tion, which consists of four components: trace collection ,slic-
ing and WP computation ,WP comparison , and mapping to source
code . Our implementation takes a golden implementation P, the
implementation to debug P0, and an input t, which demonstrates
the error inP0. It outputs a bug report consisting of locations in
the source code of PorP0that contributes to the error. We now
describe each component in details.
4.2 Trace collection
GivenP,P0, and the input t, the trace collection component uses
the TEMU component of BitBlaze to execute the programs using
tas the input. TEMU is a virtual machine based on QEMU [6],
emulating a full PC platform. TEMU records all instructions exe-
cuted by a program, and indicates the instruction operands that are
derived from program inputs. For each program, TEMU outputs an
execution trace containing all the user-mode x86 instructions exe-
cuted in the program. The trace also contains information about the
instruction operands that are dependent on program inputs. Next,
the instruction traces are converted into traces in the intermediate
representation (IR) used by BitBlaze. An IR trace is semantically
equivalent to its corresponding instruction trace. Based on the user
input information recorded in the instruction trace, we also convert
the variables directly dependent on user inputs into symbolic vari-
ables. Finally, the trace collection component identiÔ¨Åes the x86-
level slicing criteria in both traces, and maps them into IR-level
slicing criteria.
4.3 Slicing and WP computation
The slicing and WP computation component implements our so-
lution described in Section 3. Using two IR traces T,T0, and the
slicing criteria generated by the previous step, it computes the slice-
based WP of each trace. This step is implemented on top of the
BitBlaze‚Äôs VINE [7] module. VINE is the static analysis compo-
nent of BitBlaze [20] that provides an infrastructure for manipu-
lating and performing automated analysis on the IR. Recall that our
dynamic slicing algorithm needs control dependency information,
which is not available in a trace. In our implementation, we use the
ERESI [3] utility to compute the static control-Ô¨Çow graph (CFG)
on program binaries PandP0. The ERESI tool produces CFG for
all the functions deÔ¨Åned in the binary. From the CFG, we compute
the dominator tree using Lengauer-Tarjan Algorithm [16]. From
the dominator tree, we compute the dominance frontier and then
the CDG using the algorithm described in [11]. The CDG is then
fed to our VINE-based slicing algorithm.
4.4 WP comparison
The outputs of the previous components are two WPs, each as a
conjunction of constraints:
'= 1^:::^ mand'0= 0
1^:::^ 0
n:
The goal of the WP comparison component is to Ô¨Ånd the individual
constraint 0
ithat is not logically implied by '(or ithat is not log-
ically implied by '0). To this end, this component breaks each WP
into a set of individual constraints. For each constraint  0
i(or j),we check whether it is logically implied by '(or'0). However, the
WP of large programs may be inefÔ¨Åcient or impossible for the STP
solver to handle due to limits in memory or processing power. To
address this issue, instead of checking ') 0
ior'0) i, we do
pairwise constraint comparison as follows:
1.8 0
iin'0, we check if there is any corresponding constraint
 kin'such that ( k) 0
i)holds, (in;km). All
constraints 0
i2'0for which a corresponding constraint is
not found in 'are reported as unexplained.
2.8 iin', we check if there is any corresponding constraint
 0
kin'0such that ( 0
k) i)holds, (im;kn). All
constraints i2'for which such a corresponding constraint
is not found in '0are reported as unexplained.
This is an approximation step but worked quite well for us in re-
vealing all the relevant bugs in Busybox.
Optimization in WP comparison. For two WPs that have n
andmconstraints respectively, our approach needs to make mn
queries to STP. This step is time-consuming for WPs with large
numbers of constraints. Since the WPs are symbolic formula over
program input variables, if a constraint  ior 0
jis not affected
by program inputs, it will always be evaluated to TRUE, i.e., it is
atautology . To eliminate unnecessary queries, before we check
the implication relationship among individual constraints, we Ô¨Årst
use STP to check whether  ior 0
jis a tautology and remove the
tautologies from the list of constraints. This step will cost m+n
additional queries, but if a signiÔ¨Åcant portion of the constraints are
tautologies, the unnecessary queries we avoid is much larger than
m+n. In Section 5, we will show that this optimization achieves
huge reduction in the number of queries in some programs.
4.5 Mapping to source code
The unexplained constraints generated by the previous compo-
nent indicate the difference in behavior between the implementa-
tion to debug and the golden implementation, but they do not di-
rectly point to the source of bugs. To associate the constraints to
program locations, our approach maintains the connection between
constraints in a WP and its corresponding instructions in the trace.
Therefore, after the WP comparison component outputs a list of
unexplained constraints, this component can map them to the in-
structions and program locations that contribute to the unexplained
constraints. With the help of compiler debug symbols, we further
map the program locations in binaries to source code lines and out-
put them as our Ô¨Ånal bug report.
5. EV ALUATION
We now report our experience in using our method for locating
error causes in real-life case studies.
5.1 Experience with Busybox
We describe our experience in debugging the Busybox utilities.
KLEE [8] detected 21 bugs in Busybox, and 6 of them can be repro-
duced using the BusyBox version 1.4.2, namely, arp -Ainet, tr
[, top d, printf %Lu, ls -co, install -m .2We also
tested the latest version of BusyBox, version 1.16.0, and found
that 3 of these bugs ( tr, printf, ls ) still persist. As the
golden implementation, we used CoreUtils 5.97 for the install,
2We contacted the authors of KLEE [8]. The bugs they reported
were on the development branch of Busybox, and not all of them
can be reproduced on the released version of the Busybox utility.Observable Error
Bug
Report Collection ComparisonTrace WP
Source CodeMapping to
P‚ÄôPt
T T‚Äô WP  WP‚Äôunexplained
constraintsSlicing and WP
Computation/Slicing criteriaFigure 4: Architecture of our approach.
1 const struct hwtype *get_hwtype (const char *name) {
2 const struct hwtype *const *hwp;
3 hwp = hwtypes;
4 while ( *hwp != NULL) {
5 if (!strcmp(( *hwp)->name, name))
6 return ( *hwp);
7 hwp++;
8 }
9 return NULL;
10 }
446 int arp_main(int argc, char **argv)
447 {
....
469 option_mask32 =
getopt32(argc, argv, "A:p:H:t:i:adnDsv",
&protocol, &protocol,
&hw_type, &hw_type, &device);
471 argv += optind;
472 if (option_mask32 & ARP_OPT_A ||
option_mask32 & ARP_OPT_p) {
473 ap = get_aftype(protocol);
474 if (ap == NULL)
475 bb_error_msg_and_die();
476 }
477 if (option_mask32 & ARP_OPT_A ||
option_mask32 & ARP_OPT_p) {
478 hw = get_hwtype(hw_type);
479 if (hw == NULL)
480 bb_error_msg_and_die();
481 hw_set = 1;
482 }
....
Figure 5: Source code fragment of the arp utility in Busybox
ls, printf, tr utilities, net-tools 1.60 for the arp utility, and
procps-3.2.6 for the top utility.
Scale of Busybox. The Busybox bundle functions as a single
executable where the different utilities are actually passed on at the
command line for separate invocation. It is not possible to build
the individual utilities separately and run them stand alone. For
example, for running the arp utility, we need to invoke Busybox as
busybox arp -Ainet and record the execution trace. Since
we work on the binary level, the buggy implementation for us is
the Busybox binary, which has a large code base ( about 121000
lines of code ).
Locating the arpcrash bug in Busybox. Thearp utility
manages the kernel‚Äôs network neighbor cache. It can add or delete
entries to the cache, or display the cache‚Äôs current content. There
is a bug in the BusyBox arp implementation: running arp with
the command-line option -Ainet results in a segmentation fault .
However, with the same command-line option, the net-tools variant
ofarp executes successfully and displays the list of neighboring
computers known to the host computer through the inet address
family. We now explain our experience in localizing this bug.
Figure 5 shows a fragment of the source code of arp in Busy-
box. With the command-line argument -Ainet , line 469 sets the
ARP_OPT_A mask in the variable option_mask32 . Because no1 int main(int argc, char **argv)
2 {
3 int i, lop, what;
4 while ((i = getopt_long(argc,argv,
5 "A:H:adfp:nsei:t:vh?DNV",
6 longopts, &lop)) != EOF) {
7 switch (i) {
8 ...
9 case ‚ÄôA‚Äô:
10 case ‚Äôp‚Äô:
11 ap = get_aftype(optarg);
12 // Error check and exit
13 }
14 break;
15 case ‚ÄôH‚Äô:
16 case ‚Äôt‚Äô:
17 hw = get_hwtype(optarg);
18 // Error check and exit
19 }
20 hw_set = 1;
21 break;
22 case ‚Äôi‚Äô: ...; break;
23 case ‚ÄôV‚Äô: ...; break;
24 case ‚Äô?‚Äô:
25 case ‚Äôh‚Äô:
26 default : ...; break;
27 }
28 }
29 if (hw_set==0)
30 if ((hw = get_hwtype(DFLT_HW)) == NULL) {
31 // Error check and exit
32 }
33 ...
34 } //
Figure 6: Source code fragment of arp utility in net-tools
Hortoption was given in the command line, hw_type was set
to NULL. The bug is at line 477: instead of checking the mask of
hardware type, the program checks for the mask of address family,
ARP_OPT_A . As a result, control Ô¨Çows to line 478, which passes
the NULL hw_type intoget_hwtype function, and causes a
segmentation fault at line 5 due to a NULL argument being used in
the string comparison function strcmp . To isolate the root cause
of this error, we set the slicing criteria as name at the crash site
and the post-condition as name ==null.
Figure 6 shows a fragment of the implementation of the arp
utility of the net-tools, where the incorrect condition check is not
present. Therefore, the arp -Ainet invocation is successful. To
Ô¨Ånd the root cause of the bug, we did an execution of both the pro-
gram versions on the same input -Ainet and generated the WPs.
After tautology elimination and WP comparison, we were left with
one unexplained WP constraint from Busybox. The following ex-
ample is a snapshot of our intermediate result on the bug.
busybox_stp : UNMATCHED CONSTRAINT
0002366 :: 8052d75 arp_main
0002367 :: 8052d78 arp_main
0002368 :: 8052d7a arp_main
The Ô¨Årst number in each line (excluding the Ô¨Årst line) is an id
in the BitBlaze Intermediate Representation (IR), followed by the
corresponding instruction address and the name of the functioncontaining the bug. Here our approach found three instructions in
arp_main that are related to the bug. With the help of compiler
level debug symbols, our approach associated all three instructions
to the same line in the source code, line 477 in Figure 5. We il-
lustrate the association using the output of the objdump utility,
which disassembled the Busybox binary with symbol information:
/root/coreutils/busybox-1.4.2/networking/arp.c:477
8052d70: a1 d0 71 0d 08 mov 0x80d71d0,%eax
8052d75: 83 e0 01 and $0x1,%eax
8052d78: 84 c0 test %al,%al
8052d7a: 75 0c jne 8052d88 <arp_main+0x11f>
There were eight unexplained constraints from net-tools (point-
ing to three lines of source ), which is the result of additional options
implemented in net-tools arp and not implemented in Busybox.
Code missing error in Busybox printf .Theprintf
utility prints data according to a format argument. The Busybox‚Äôs
printf , when run as printf %Lu 0 incorrectly outputs a large
number. However, if we run Coreutils‚Äô printf with the same
arguments, the output is 0. Using our approach, we produced
four unexplained constraints from Busybox and 14 unexplained
constraints from Coreutils. The unexplained constraints in Busy-
box point to a single line of source, but it was due to code differ-
ences in the print_formatted function: Busybox implements
this function using strchr , while Coreutils‚Äô version is imple-
mented by switch-case statements. We proceeded to look for unex-
plained constraints contributed by Coreutils. The 14 unexplained
constraints in Coreutils point to three lines of code , out of which
two were due to the implementation difference described above.
The remaining unexplained constraint pointed to the bug, which is
at the address of 0x8048c29 . Using compiler-level information,
we found that the above instruction is compiled from line 349 of
the Ô¨Åle printf.c.
349 switch (conversion)
350 {
351 case ‚Äôd‚Äô: case ‚Äôi‚Äô: case ‚Äôo‚Äô:
case ‚Äôu‚Äô: case ‚Äôx‚Äô: case ‚ÄôX‚Äô:
352 length_modifier = PRIdMAX;
353 length_modifier_len = sizeof PRIdMAX - 2;
354 break;
355
356 case ‚Äôa‚Äô: case ‚Äôe‚Äô: case ‚Äôf‚Äô: case ‚Äôg‚Äô:
357 case ‚ÄôA‚Äô: case ‚ÄôE‚Äô: case ‚ÄôF‚Äô: case ‚ÄôG‚Äô:
358 length_modifier = "L";
359 length_modifier_len = 1;
360 break;
361
362 default:
363 length_modifier = start;
364 length_modifier_len = 0;
365 break;
366 }
....
This code fragment does a check on the formatting character
supplied to printf before it is used in the printf function in
C standard library. If ‚Äôu‚Äô,‚Äôd,‚Äôi‚Äô,‚Äôo‚Äô,‚Äôx‚Äô or‚ÄôX‚Äô conver-
sion speciÔ¨Åer is found, it sets the length modiÔ¨Åer to ll(through
the PRIdMAX variable deÔ¨Åned in header Ô¨Åle system.h) and if Ô¨Çoat
conversion character (‚Äôa‚Äô,‚Äôe‚Äô,‚Äôf‚Äô,‚Äôg‚Äô,‚ÄôA‚Äô,‚ÄôE‚Äô,‚ÄôF‚Äô or
‚ÄôG‚Äô) is found, it sets the length modiÔ¨Åer to L. The format we
speciÔ¨Åed on the command line (%Lu) was changed to %llu and
passed on to the print routine. Therefore, the Coreutils‚Äô printf
produces a correct output, whereas the Busybox‚Äôs printf which
passes the %Lu format speciÔ¨Åer directly to the print routine, pro-
ducing a buggy output.Results on all six Busybox bugs. Our approach successfully
identiÔ¨Åed the root cause of each bug in our bug report. The Ô¨Åndings
on all the six utilities and the corresponding data produced and an-
alyzed by our tool are summarized in Table 1, which presents com-
parative data obtained by us on both Busybox and CoreUtils/net-
tools/procps. The Ô¨Årst column of Table 1 is marked ‚ÄúUtility" ‚Äî
this represents the utility whose observable error is being diag-
nosed. Each entry in the table is a tuple, where the Ô¨Årst entity
is from Busybox and the second from Coreutils/net-tools/procps.
Trace Size is the size of the trace in terms of number of instructions
obtained from TEMU. IR size refers to the number of statements in
the intermediate representation (IR) obtained from VINE. IR Slice
refers to the size of the slice obtained in IR form. Columns 5 and
6 respectively present the number of WP constraints obtained by
our method and the number of WP constraints remaining after the
tautology elimination optimization. Column 7 presents the number
of lines of source code present in the Ô¨Ånal bug report obtained after
comparing the WPs produced and mapping the unexplained terms
to the source. Columns 8 and 9 present the time and memory usage
requirements of our tool.
It is worth noting a few important facts on Table 1. First, the size
of the traces and the size of the IR produced are usually comparable
or orders of magnitude smaller in the Busybox variant, as expected
since Busybox is a much lightweight implementation. Secondly, a
signiÔ¨Åcant fraction of the WPs are eliminated using the tautology
optimization. For example, the number of WP constraints of Core-
utils‚Äô lsutility is reduced from 30752 to 97, which signiÔ¨Åcantly
reduces the time of WP comparison. Last but not the least, the bug
report produced by our approach is small, which is very useful for
the programmer. For each bug in Busybox, we have at most four
lines reported as the bug report!
DARWIN on Busybox. We tried running the DARWIN setup
on the Busybox utilities to see if we can pinpoint any of the six
bugs. Since all the six bugs we encounter here involved an incor-
rect assignment statement, the current DARWIN setup could not
produce a bug report without predicate instrumentation. In all the
six cases, the path conditions produced by DARWIN from Busybox
and Coreutils (or net-tools/procps) were equivalent.
5.2 Experience with Program Versions
In this section, we describe our experience with an evolving
program benchmark, namely libPNG , which was used by DAR-
WIN [18]. Table 2 summarizes the results. Each entry for libPNG
is a tuple, where the Ô¨Årst entity is from libPNG 1.0.7 and the second
is from libPNG 1.2.21. Thus, in this case, the golden implementa-
tion is a stable version of the program.
We now describe our experience with the libPNG open source
library [4], a library for reading and writing PNG images. We used
a previous version of the library (1.0.7) as the buggy version. This
version contains a known security vulnerability, which was subse-
quently identiÔ¨Åed and Ô¨Åxed in later releases. We used the version
1.2.21 as the golden implementation, which has Ô¨Åxed the vulner-
if (!(png_ptr->mode & PNG_HAVE_PLTE))
{
png_warning(png_ptr, "Missing PLTE before tRNS");
}
else if (length > (png_uint_32)png_ptr->num_palette)
{
png_warning(png_ptr, "Incorrect tRNS chunk length");
png_crc_finish(png_ptr, length);
return;
}
Figure 7: Buggy code fragment from libPNGUtility Trace size IR Size Slice Size #WP #WP Constraints #LOC in Time Mem.
(# instructions) Constraints after elimination Bug Report (min:sec) Usage (MB)
arp <5039, 4764> <251906, 232365> <56524, 51448> <722, 434> <27, 34> <1, 3> 1:40 463.09
top <1637, 13921> <85247, 676525> <34523, 332281> <566, 2501> <8, 6> <2, 0> 1:42 665.74
install <3129, 179120> <158513, 9369897> <74030, 3868082> <872, 30221> <29, 49> <0, 4> 133:33 3720.64
ls <102666, 179120> <2033702, 6315442> <1134397, 3859712> <8806, 30752> <14, 97> <1, 1> 22:42 1172.98
printf <3702, 3633> <181652, 176942> <27781, 40403> <241, 414> <21, 35> <1, 3> 1:20 471.14
tr <5474, 138538> <303684, 7235883> <85047, 29375> <445, 280> <9, 9> <1, 0> 4:30 2529.67
Table 1: Experimental Results on BusyBox bugs
Programs Trace size IR Size Slice Size #WP #WP Constraints #LOC in Time Mem.
(# instructions) Constraints after elimination Bug Report (min:sec) Usage (MB)
libPNG <22196, 10313> <1088817, 558079> <393060, 242227> <12978, 3011> <836, 796> <8, 9> 112:34 805.19
(1.0.7 / 1.2.21)
Miniweb / Apache <25517, 19613> <1312106, 947508> <49984, 5344> <317, 37> <34, 25> <4, 11> 2:45 934.99
Table 2: Experimental Results on two versions of libPNG, and the Miniweb web-server
ability. The code base of both the versions are signiÔ¨Åcantly large,
31164 lines for libPNG-1.0.7 and 36776 for libPNG-1.2.21.
The vulnerability is shown in Figure 7. If !(png_ptr->mode
& PNG_HAVE_PLTE) is true, the length check is missed, leading
to a buffer overrun error. The error is Ô¨Åxed by converting else if
in the code fragment to an if. After applying our approach to two
libPNG versions, we identiÔ¨Åed 17 unexplained constraints (point-
ing to only 8 lines in source ) from libPNG 1.0.7 and 28 unexplained
constraints (pointing to only 9 lines of source ) from libpng v1.2.21.
The constraints from libPNG 1.0.7 show the execution difference,
but do not help us identify the root cause of the speciÔ¨Åc bug. A
careful examination revealed one unexplained WP constraint (the
remaining showed the execution difference) from libpng v1.2.21,
which leads to line 1306 of pngrutil.c. This is a check of the sec-
ond condition, which was missing in the buggy version.
5.3 Experience with Webservers
We studied the web-server miniweb [5], a simple HTTP server
implementation. The input query whose behavior we debugged
was a simple HTTP GET request for a Ô¨Åle, the speciÔ¨Åc query being
‚ÄúGET x ‚Äù. Ideally, we would expect miniWeb to report an error as
xis not a valid request URI (a valid request URI should start with
‚Äò/‚Äô). However, miniweb did not report any errors, and returned
index.html . We then attempted to localize the root cause of
this observable error. Since the latest version of miniweb also
contains the error, we chose another HTTP server Apache [1] as
the golden implementation. The Apache is a well-known HTTP
server for Unix and Windows. Apache does not exhibit the bug
we are investigating. Apache is a signiÔ¨Åcantly complex implemen-
tation with about 358379 lines in the code base , while miniweb is
comparatively a much light-weight variant with about 2838 lines
of code. Thus, in this case the golden and the buggy implementa-
tion are both implementations of the same protocol ‚Äî HTTP. The
results appear in the second row of Table 2.
The unexplained WP constraint contributing to our bug report
pointed to function apr_uri_parse which shows that apache
checks for /in GET queries and reports accordingly (line 276 of
apr_uri.c ). The miniweb server missed this check and treated
the query GET x similar to GET /. Because of this missing check,
the string ‚Äú x‚Äù in the GET query is thought to be an HTTP header.
6. RELATED WORK
Debugging with respect to a golden implementation is related
to the problem of debugging evolving programs. In evolving pro-
gram debugging, a buggy program version is simultaneously ana-lyzed along with an older stable program version. This analysis is
done with the goal of explaining an observed error for a particular
test input in the buggy program version. One of the Ô¨Årst efforts
for evolving program debugging is [21]. This work identiÔ¨Åes the
changes across program versions and searches among subsets of
this change set to identify which changes could be responsible for
the given observable error. In contrast, we employ a semantic anal-
ysis of the test input‚Äôs execution in the two program versions.
Recently, we proposed the DARWIN approach [18] for debug-
ging evolving programs. DARWIN performs a dynamic symbolic
execution along the execution of the given test input in two pro-
grams. Thus, it is applicable for debugging a buggy implementa-
tion with respect to a golden implementation. However, the DAR-
WIN approach depends on the path condition of the buggy input
being different in the stable program and buggy program. In other
words, the observable error must be reÔ¨Çected by a difference in path
conditions in the two programs. For the Busybox case study, this
was often not the case. The main issue here is that the DARWIN
method is most suited for debugging branch errors (or code missing
errors where the missing code contains branches). In contrast, the
method in this paper aims to pinpoint both branch and assignment
errors; code missing errors are handled by examining the weakest
pre-condition from the golden implementation.
Dynamic slicing (see e.g., [15]) has long been studied as an aid
for program debugging and comprehension. A recent work [9] also
uses dynamic program dependencies to Ô¨Ånd the relevant parts of an
input that are responsible for a given failed output. Our work aug-
ments dynamic slicing with symbolic execution (along a path) and
employs the augmented method for debugging two different pro-
grams. We perform symbolic execution along a dynamic slice by
following the dynamic data and control dependencies. Works such
as [17, 19] combine symbolic execution and dependency analysis
for test-suite augmentation. In particular, [19] uses symbolic exe-
cution (of programs, not paths) along static data and control depen-
dencies to generate criteria for additional test cases for the purpose
of test-suite augmentation. In [17], the authors use dynamic sym-
bolic execution ( i.e., along a path) to generate additional test cases
which stress a program change and reÔ¨Çect the effect of the change
in the program output. The problem we tackle is different ‚Äî in-
stead of trying to Ô¨Ånd test cases that stress a given program change,
we are trying to Ô¨Ånd the root cause of failure of a given test case in
a changed program.
In this work, we focus on debugging a given failing test case.
These are several directions of work in Ô¨Ånding failing tests (which
demonstrate an observable error), such as ‚Äî the DSD-crasher ap-
proach which combines static and dynamic analysis [10], and bug-Ô¨Ånding approaches based on software model checking ( e.g., [14]).
Symbolic execution has also been used for generating problematic
or failing tests. The works on Directed Automated Random Testing
(DART) ( e.g., see [13]) combine symbolic and concrete execution
to explore different paths of a program. A recent work [8] uses
symbolic execution on GNU Coreutilities as well as Busybox to
compute test-suites with high path coverage. All of these works are
complementary to our work ‚Äî our method can try to Ô¨Ånd the root
cause of the error in the failing test cases generated by these works.
7. LIMITATIONS
The WP comparison and bug localization scheme achieves rea-
sonably good performance for real world programs as recorded by
our experiments. Our approach assumes the availability of a sta-
ble golden implementation to which the buggy implementation can
be compared and the bug can be isolated. This is a reasonable as-
sumption to make, considering the fact that such stable versions are
almost always available, in any project, as the product goes through
version-driven evolutions and a bug is introduced in reÔ¨Åning a sta-
ble version to incorporate some changed needs.
Another important assumption is the deÔ¨Ånition of the observable
error and the error location in terms of a variable on which the error
is manifested. For our approach to work, a similar variable should
be present in the golden implementation, but this is also usually the
case when programs move between versions.
We note that there are various threats to validity to our approach.
One important factor that deserves mention is scalability. First, the
WP computation approach is linearly dependent on the size of the
dynamic slice. For programs with little inherent parallelism, and
having long chains of control/data dependencies, the sizes of the
dynamic slice may be substantially large. This may lead to signif-
icantly large number of WP constraints that need to be analyzed.
This was the case for the lsandinstall experiments. Moreover, our
method also requires an off-line computation of the static CDG.
Another element of concern that deserves discussion is the issue
of false positives in our bug report. Due to limits in processing
power of the SMT solver, we resorted to an element-by-element
comparison measure for comparing the non-trivial WP constraints
obtained after tautology elimination. In some cases, our approach
will incorrectly report two sets of WP constraints as different though
they are semantically same. Consider the example where the WP
constraints obtained from the golden program are '=y5^x>
yand those obtained from the buggy program are '0=x6for
integer program variables x,y. A pairwise comparison approxima-
tion will incorrectly generate unexplained constraints. However,
such a situation was not encountered in our experiments.
A second limitation arises due to the comparison method adopted
by us. Our method generates some unexplained constraints as bug
reports though they are actually due to implementation differences.
Since the programs are compiled without jump tables, syntactic dif-
ferences like if-else and switch-case are automatically Ô¨Åltered out.
However, some implementation differences still remain in our bug
reports as witnessed in a couple of our experiments. For example,
out of the 4 unexplained elements in the bug report for install , 2
were due to additional case checks in the command line processing
unit in Coreutils. This was due to the fact that there are two addi-
tional command line features implemented in Coreutils install and
the case enumerations for these features (v for verbose and b for
backup) are placed before the check for m (install -m is the bug).
Differences in implementation style also cropped into our bug re-
port for printf and were not Ô¨Åltered out. In this case, the command
line processing is implemented using strchr in Coreutils, where a
longword is tested at a time. The Busybox variant employs thetraditional loop, which tests each character at a time. Thus, 1 un-
explained LOC from Busybox and 2 unexplained ones from Core-
utils point to this implementation difference, but we report these as
bugs. In summary, our method can report false positives whenever
there are implementation differences that enter the dynamic slice
and thereby, participate as WP constraints. However, we noticed
that the ability to point to the bug with a few additional lines of
false positives is tolerable in practice, since our main goal is aid the
debugging / comprehension of the program versions.
8. DISCUSSION
In this paper, we have presented a debugging methodology and
tool for root-causing errors in programs with a golden program as
a reference. Our toolkit takes in a given buggy implementation
and the reference golden implementation and combines slicing and
symbolic execution to explain the behavior of a test input which
passes in the golden one, while failing in the buggy program. Our
experience with real-life case studies (including the Busybox Em-
bedded Linux distribution) demonstrates the utility of our method
for localizing real bugs. The bug report generated by our method is
concise, thereby, aiding the programmer to localize the root cause
of a given observable error.
Acknowledgments. This work was partially supported by a De-
fence Innovative Research Programme (DIRP) grant (R-252-000-
393-422) from Defence Research and Technology OfÔ¨Åce (DRTech)
Singapore, and a grant (R-252-000-385-112) from Academic Re-
search Fund (ARF).
9. REFERENCES
[1] Apache webserver. http://httpd.apache.org/ .
[2] Busybox. http://busybox.net/ .
[3] ERESI: the ERESI reverse engineering software interface.
http://www.eresi-project.org .
[4] libPNG library. http://www.libpng.org .
[5] Miniweb webserver. http://miniweb.sourceforge.net/ .
[6] QEMU emulator. http://www.qemu.org .
[7] Vine. http://bitblaze.cs.berkeley.edu/vine.html .
[8] C. Cadar, D. Dunbar, and D. Engler. Klee: Unassisted and automatic generation
of high-coverage tests for complex systems programs. In OSDI , 2009.
[9] J. Clause and A. Orso. Penumbra: Automatically identifying failure-relevant
inputs using dynamic tainting. In ISSTA , 2009.
[10] C. Csallner and Y . Smaragdakis. DSD-Crasher: a hybrid analysis tool for bug
Ô¨Ånding. In ISSTA , 2006.
[11] R. Cytron et al. EfÔ¨Åciently computing static single assignment form and the
control dependence graph. ACM Trans. Program. Lang. Syst. , 13(4), 1991.
[12] V . Ganesh and D. Dill. A decision procedure for bit-vectors and arrays. In CAV,
2007.
[13] P. Godefroid, N. Klarlund, and K. Sen. DART: Directed automated random
testing. In ESEC-FSE , 2005.
[14] B. Gulavani, T. Henzinger, Y . Kannan, A. Nori, and S. Rajamani. SYNERGY:
A new algorithm for property checking. In FSE, 2006.
[15] B. Korel and J. W. Laski. Dynamic program slicing. Information Processing
Letters , 29(3):155‚Äì163, 1988.
[16] T. Lengauer and R. Tarjan. A fast algorithm for Ô¨Ånding dominators in a
Ô¨Çowgraph. ACM Trans. Program. Lang. Syst. , 1(1):121‚Äì141, 1979.
[17] D. Qi, A. Roychoudhury, and Z. Liang. Test generation to expose changes in
evolving programs. In ASE, 2010.
[18] D. Qi, A. Roychoudhury, Z. Liang, and K. Vaswani. DARWIN: An approach
for debugging evolving programs. In ESEC-FSE , 2009.
[19] R. Santelices, P. Chittimalli, T. Apiwattanapong, A. Orso, and M. Harrold.
Test-suite augmentation for evolving software. In ASE, 2008.
[20] D. Song et al. Bitblaze: A new approach to computer security via binary
analysis. In ICISS (Keynote) , 2008.
http://bitblaze.cs.berkeley.edu .
[21] A. Zeller. Yesterday my program worked, today it does not. Why? In
ESEC-FSE , 1999.
View publication stats