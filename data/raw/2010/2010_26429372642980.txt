Automated Synthesis and Deployment
of Cloud Applications
Roberto Di Cosmo
roberto@dicosmo.orgMichael Lienhardt
michael.lienhardt@inria.frRalf Treinen
treinen@pps.univ-paris-
diderot.fr
Stefano Zacchiroli
zack@pps.univ-paris-
diderot.frJakub Zwolakowski
jakub.zwolakowski@inria.fr
Univ Paris Diderot, Sorbonne Paris Cit√©, PPS, UMR 7126, CNRS, F-75205 Paris, France
Antoine Eiche
Mandriva, FR
aeiche@mandriva.comAlexis Agahi
Kyriba Corporation, USA
alexis.agahi@kyriba.com
ABSTRACT
Complex networked applications are assembled by connect-
ing software components distributed across multiple ma-
chines. Building and deploying such systems is a challenging
problem which requires a signicant amount of expertise:
the system architect must ensure that all component depen-
dencies are satised, avoid conicting components, and add
the right amount of component replicas to account for qual-
ity of service and fault-tolerance. In a cloud environment,
one also needs to minimize the virtual resources provisioned
upfront, to reduce the cost of operation. Once the full archi-
tecture is designed, it is necessary to correctly orchestrate
the deployment phase, to ensure all components are started
and connected in the right order.
We present a toolchain that automates the assembly and
deployment of such complex distributed applications. Given
as input a high-level specication of the desired system,
the set of available components together with their require-
ments, and the maximal amount of virtual resources to be
committed, it synthesizes the full architecture of the sys-
tem, placing components in an optimal manner using the
minimal number of available machines, and automatically
deploys the complete system in a cloud environment.
This work was supported by the French ANR project ANR-
2010-SEGI-013-01 Aeolus and partially performed at IRILL,
center for Free Software Research and Innovation in Paris,
France, http://www.irill.org
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proÔ¨Åt or commercial advantage and that copies bear this notice and the full citation
on the Ô¨Årst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciÔ¨Åc permission
and/or a fee. Request permissions from permissions@acm.org.
ASE‚Äô14, September 15-19, 2014, Vasteras, Sweden.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-3013-8/14/09 ...$15.00.
http://dx.doi.org/10.1145/2642937.2642980.1. INTRODUCTION
In contrast to classic, monolithic software that runs lo-
cally on a single machine, large distributed systems are built
from many running services executing on (possibly hetero-
geneous) virtual machines (orlocations ) and collaborating
to provide the expected functionality to nal users.
The system architect must choose which services to use
and how to congure them, knowing that services may de-
pend on, and/or be in conict with, each other; consider
fault tolerance and quality of service issues, and provide
enough instances of each service; design the physical archi-
tecture on which to run the system, trying to keep its cost
reasonable with nonetheless enough locations with enough
resources (e.g. RAM, disk space, bandwidth) to allow the in-
stallation and the good execution of the services they host;
choose which implementation of each service to install on
which location, knowing that implementations (usually in
the form of packages ) have dependencies and conicts too.
Once all this planning is done, the deployment phase must
provision the required virtual machines, install the right
packages on each of them, and nally start and intercon-
nect services in the right order.
This is a daunting task, not unlike building a puzzle|each
running service, package, and machine being a piece|where
one only knows the overall expected functionality.
To reduce the complexity of this process, many industrial
initiatives develop tools [32, 5] that allow to select, congure,
and push on the Cloud some well dened services. However,
these tools are only useful once the puzzle is solved, i.e. when
the right services and packages have been selected, the loca-
tions on which they must be deployed have been chosen, and
the way of conguring them in a manner that satises all the
requirements has been found. Solving this puzzle currently
requires a signicant amount of human expertise, so that in
practice large software stacks are often managed using cus-
tom scripts and manual techniques, which are error-prone
and fragile [23].
In this article, we present a toolchain developed in the
framework of the Aeolus project [6] that provides a generic,
automatic and sound alternative to these scripts and tech-
niques. The toolchain is composed of two tools:
211
Zephyrus automatically generates an abstract represen-
tation (or conguration ) of the target system according to
a concise specication of the expected functionalities. It
takes into account the set of available services, which can
serve as building blocks, with their requirements, replication
policies, and resource consumption characteristics; informa-
tion concerning the implementation of the services (e.g. that
theApache service is provided by the www-servers/apache
package on Gentoo Linux, by the apache2 package on De-
bian, etc.); and the maximum amount of (virtual) machines
available, together with their characteristics. It is also able
to minimize the amount of needed resources.
Armonic takes the full system conguration produced by
Zephyrus and deploys it, by provisioning the required vir-
tual machines on a cloud computing platform (such as Open-
Stack), installing the needed packages on each machine; it
congures the dierent services to establish the required con-
nections; and nally starts the services in the right order,
relying on precise metadata that describe the internal state
machines and runtime dependencies of each service.
This toolchain decouples system design from system de-
ployment. It builds upon the sound formal foundations of
the Aeolus component model [9], which describes each avail-
able service as a component type, using ports tagged with an
arity to encode requirements, provides, conicts and repli-
cation policy, as well as an internal state machine to capture
component life-cycles.
Zephyrus uses a stateless version of the Aeolus model, ex-
tended to take into account locations, repositories, packages,
and resources, as detailed in [7]; packages and repositories
are encoded following the now standard approach originated
from [21]. The specications accepted by Zephyrus are given
in a rigorous syntax whose semantics denes when a cong-
uration satises a specication. Based on this formalization,
Zephyrus can be proven correct and complete: it will always
nd a conguration that is optimal w.r.t. the chosen crite-
rion if one exists. Furthermore, the generated conguration
is guaranteed to provide the expected functionalities, and
satises the constraints dened by the replication policies,
as well as the dependencies and conicts between services.
Armonic then takes over and uses the information about
the internal state machine of the components to determine
a correct service activation sequence, under the assump-
tion that the dependency relation among services is acyclic,
which is usually the case.
Paper structure. Section 2 presents the toolchain through
a realistic running example. Section 3 discusses the inter-
nals of the various tools and presents the theoretical results
establishing soundness, completeness and complexity of the
architecture synthesis phase. Section 4 reports on experi-
mentation with the toolchain, as well as its adoption in an
industrial context at Kyriba Corporation. Before conclud-
ing, Section 5 discusses related work.
2. WALKTHROUGH
In this section we describe Zephyrus andArmonic by show-
ing them at work on an example that is simple enough to
be fully presented, and yet realistic as it corresponds to a
common use case of application deployment in the cloud.
Zephyrus takes several inputs:1. a description of all the existing components and their
constraints, which come in various formats due to their
dierent origins (e.g. package database, architectural
choices, machine physical resources, etc.); this is called
auniverse.
2. a description of the current system conguration (ex-
isting machines, which services are currently deployed
where, etc.)
3. a high level specication of the desired system. As
part of the specication, architects can include objec-
tive functions that they would like to optimize for, such
as the desire of minimizing the number of virtual ma-
chines that will be used for the deployment (and hence
the system cost).
2.1 Use case: deploying a WordPress farm
The task we want to perform is deploying the popular
Wordpress blog platform on a private OpenStack cloud. In
addition to being realistic, this use case is often used as a
\benchmark" to showcase the characteristics of cloud provi-
sioning platforms. Wordpress is written in PHP and as such
is executed within Web server software like Apache or nginx.
Additionally, Wordpress needs a connection to a MySQL in-
stance, in order to store user data. Simple Wordpress de-
ployments can therefore be obtained on a single machine
where both Wordpress and MySQL get installed.
\Serious" Wordpress deployments, however|that sustain
high load and are fault tolerant|are more complex and rely
on some form of load balancing. One possibility is to balance
load at the DNS level using servers like Bind: multiple DNS
requests to resolve the website name will result in dierent
IPs from a given pool of machines, on each of which a sep-
arate Wordpress instance is running. Alternatively one can
use as website entry point an HTTP reverse proxy capable
of load balancing (and caching, for added benet) such as
Varnish. Either way, Wordpress instances will need to be
congured to contact the same MySQL database, to avoid
delivering inconsistent results to users. Also, having redun-
dancy and balancing at the front-end level, one usually ex-
pects to have them also at the DBMS level. One way to
achieve that is to use a MySQL cluster , and congure the
Wordpress instances with multiple entry points to it.
Constraints. Several design constraints should be taken into
account when designing such a system. Some constraints
come from package providers and cannot be easily changed.
For instance, Wordpress, Varnish, etc. usually come from
software distribution packages and have their own dependen-
cies and conicts which must be respected on each machine
when installing the software.
On the other hand, \house" requirements are dened by
system architects to capture some ad-hoc policy. For this
use case, we assume given the following requirements:
at least 3 replicas of Wordpress behind Varnish or, al-
ternatively, at least 7 replicas with DNS-based load
balancing (since DNS-based load balancing is not capa-
ble of caching, the expected load on individual Word-
press instances is higher);
at least 2 dierent entry points to the MySQL cluster;
each MySQL instance shouldn't serve the needs of more
than 3 Wordpress instances;
212Figure 1: Zephyrus used to design a scalable, fault-tolerant Wordpress deployment
no more than 1 DNS server deployed in the adminis-
trative domain;
dierent Wordpress (and MySQL) instances are de-
ployed at dierent locations.1
Similar constraints might exist on machine resources, e.g.
we expect Varnish to consume 2Gb of RAM and we don't
want to deploy it to a smaller machine, especially if in com-
bination with other RAM-consuming services. Note that
\house"requirements are not intrinsically related to the soft-
ware components we are using, but are rather an encoding
of explicit architectural choices.
2.2 Architecture synthesis
Figure 1 shows the application of our toolchain to the
design of a complex Wordpress deployment like the one we
have discussed.
On the left of the black arrow is a schematic representation
ofZephyrus ' input, on the right its output. Available ser-
vices are depicted in the gure using a graphical syntax in-
spired by the Aeolus model [9], each one with its own require-
ments, conicts, and (house) replication policy. Component
requirements are exposed as required ports that should be
connected, via bindings, to matching provided ports oered
by other service instances, respecting port replication con-
straints: an upper bound (or 1) on the amount of incoming
bindings for provided ports; a lower bound on the amount of
outgoing bindings to dierent service instances for required
ports. For example, the fact that the HTTP load balancer
requires 3 Wordpress replicas is indicated by the 3 anno-
tation on its wordpress backend required port, and the fact
that the DNS load balancer is incompatible with other DNS
services is indicated by the dnsconict port.
Note that our ports result in a very exible notion of de-
pendency, with choice: any requirement can be satised by
anycomponent providing the right port. For instance, if we
require the port wordpress frontend , we allow Zephyrus to
choose which of DNS load balancer orHTTP load balancer is
the best to use. To our knowledge, Zephyrus is the only tool
to manage such exibility in dependencies.
1It is technically possible to co-locate multiple, say, MySQL
instances on the same machine, but it would be pointless to
do so when we are seeking fault tolerance and load balancing.Services and implementations. Zephyrus takes as input a
description of the available service types, and an implemen-
tation relation that maps each service to the set of packages
implementing it.2These two parts of the universe are given
as input to Zephyrus as a JSON le that in our running
example looks like this:
{ "component_types": [
{ "name" : "DNS-load-balancer",
"provide" : [["@wordpress-frontend"], ["@dns"]],
"require" : [["@wordpress-backend", 7]],
"conflict": ["@dns"],
"consume" : [["ram", 128]] },
{ "name" : "HTTP-load-balancer",
"provide" : [["@wordpress-frontend"]],
"require" : [["@wordpress-backend", 3]],
"consume" : [["ram", 2048]] },
{ "name" : "Wordpress",
"provide" : [["@wordpress-backend"]],
"require" : [["@sql", 2]],
"consume" : [["ram", 512]] },
{ "name" : "MySQL",
"provide" : [["@sql", 3]],
"consume" : [["ram", 512]] } ],
"implementation": [
[ "DNS-load-balancer", ["bind9"] ],
[ "HTTP-load-balancer", ["varnish"] ],
[ "Wordpress", ["wordpress"] ],
[ "MySQL", ["mysql-server"] ] ] }
The component_types section describes the available com-
ponent types with their ports, as well as their non func-
tional requirements like memory or bandwidth. Port names
are distinguished from components or packages by a simple
syntactic convention: ports start with @. The implemen-
tation section maps services to the software packages that
should be installed to realize them on actual machines.
Package repositories. Unlike other tools, Zephyrus is fully
aware of available package repositories, with their dependen-
cies and conicts, and uses such information to ensure that
package-level conicts and dependencies are respected on all
machines. It is possible to associate dierent package repos-
2In the example we have kept things simple, but Zephyrus
is capable of handling complex situations where the same
service can be implemented by dierent packages on dierent
machines, according to the locally installed OS.
213Table 1 Specication Syntax
S::= trueje op e Specication
jSandSjSorS
jS=>SjnotS
e::= nj#`jneExpression
je+eje e
`::= kjtjp Elements
j(J)fJr:Slg
Sl::= truejelop el Local Specication
jSlandSljSlorSl
jSl=>SljnotSl
el::= nj#`ljnelLocal Expression
jel+eljel el
`l::= kjtjp Local Elements
J::=jo op n; J Resource Constraint
Jr::=jrjr_Jr Repository Constraint
op::= j =j  Operators
itories to dierent locations, allowing to handle deployment
of heterogeneous systems. The size of a repository may be
huge (the Debian Squeeze repository contains 30'000 pack-
ages), so Zephyrus uses coinst [8] to abstract packages into
a set of much smaller equivalence classes, and yet sucient
to capture all package incompatibilities.
Available (virtual) machines. Another essential part of
Zephyrus input is the description of the initial conguration,
i.e. the set of available machines with information on their
resources: memory, package repository, existing services and
packages. In our example, we start with an initial congura-
tion consisting of 6 bare locations with 2Gb of RAM. Such
conguration is fed to Zephyrus in JSON format, e.g.:
{ "locations" : [
{ "name" : "loc1",
"repository" : "debian-squeeze",
"provide_resources" : [["ram", 2048]] },
{ "name" : "loc2",
"repository" : "debian-squeeze",
"provide_resources" : [["ram", 2048]] },
[...] }
Target system speciÔ¨Åcation. Zephyrus accepts a specica-
tion of the desired target system. Specications are dened
according the abstract syntax presented in Table 1.
A specication Sis a set of basic constraints e op e, com-
bined using the usual logical connectors. These basic con-
straints specify how many elements (packages, component
types, etc) should be in the generated conguration, using
terms of the form # `that correspond to the number of in-
stances of element `in the system. For instance, one might
state that we want at least 3 instances of the component
type apache : \#apache3", where #apache represents the
number of apache instances in the conguration.
Moreover, it is possible to express constraints on loca-
tions. Locations can be specied in our syntax with the
term ( J)fJr:Slgwhere Jis the constraint on the re-
source available on that machine; Jris the set of repositories
that can be installed on that machine (` ' standing for any
repository); and Slis a constraint specifying the contents
of the machine (basically, SlisSwithout locations). For
instance, we can specify that no location with less than 2Gbof RAM and redhat installed should have a MySQL running:
\#(mem < 2G){redhat: #MySQL 1} = 0 ".
For our running example we need exactly one Wordpress
frontend (i.e., exactly one service oering a wordpress-frontend
port), and that no machine is deployed with more than one
instance of either MySQL/Wordpress services on it.
(#@wordpress-frontend = 1)
and #(_){_ : #MySQL > 1} = 0
and #(_){_ : #Wordpress > 1} = 0
Note that no constraint is imposed on the co-location of
dierent services on the same machine.
Optimization criteria. InZephyrus , one may request a so-
lution that is optimal w.r.t. a specic objective function.
Currently, Zephyrus supports two built-in optimization cri-
teria, namely compact andconservative , which respectively
minimize the number of components and locations used, or
their dierence with respect to the initial conguration.
Running Zephyrus. We are now ready to ask Zephyrus to
compute the nal conguration:
$ zephyrus -u univ.json -opt compact \
-ic conf.json -spec sp.spec \
-repo debian-squeeze ds.coinst
In addition to the obvious parameters (universe, optimiza-
tion function, conguration, specication), we pass an extra
one: the -repo option tells Zephyrus that all the informa-
tion about the packages contained in the repository named
debian-squeeze is available in the le ds.coinst.
The actual output of Zephyrus contains a complete de-
scription of the system to be deployed; it is too long to be
listed here in full, so we only highlight some excerpts of it.
The format is the same as for congurations, and starts with
the description of the locations:
{ "locations": [
{ "name": "loc1",
"provide_resources": [ [ "ram", 2048 ] ],
"repository": "debian-squeeze",
"packages_installed": [ "wordpress" ] },
{ "name": "loc2",
"provide_resources": [ [ "ram", 2048 ] ],
"repository": "debian-squeeze",
"packages_installed": ["mysql-server",
"wordpress" ] } [...]
We see that each location is associated to a list of packages
that should be installed there. Only the root packages are
listed, and Zephyrus has already checked that they can be
co-installed, satisfying dependencies and conicts.
The second part of the output is the list of service in-
stances present in the system, mapped to their locations:
"components": [
{ "name": "Wordpress-1",
"type": "Wordpress",
"location": "loc1" },
{ "name": "Wordpress-2",
"type": "Wordpress",
"location": "loc2" },
{ "name": "MySQL-1", "type": "MySQL",
"location": "loc2" }, [...]
Finally, the third part of the output lists the bindings that
connect (ports of) service instances together:
214Figure 2: Screenshot of Armonic web interface.
"bindings": [
{ "port": "@wordpress-backend",
"requirer": "HTTP-load-balancer-1",
"provider": "Wordpress-1" },
{ "port": "@wordpress-backend",
"requirer": "HTTP-load-balancer-1",
"provider": "Wordpress-2" },
{ "port": "@sql",
"requirer": "Wordpress-1",
"provider": "MySQL-1" } [...]
The conguration corresponding to Zephyrus output is
depicted on the right of Figure 1, where shaded boxes denote
locations; we omit installed packages for the sake of read-
ability. All choices there|load balancer, mapping between
services and machines, bindings, etc.|have been made by
Zephyrus . Note how services have been co-located where
possible, minimizing the number of used machines: only 4
out of the 6 available machines have been used: the proposed
solution is optimal w.r.t. the desired metric.
2.3 ConÔ¨Åguration deployment
Once we have the conguration generated by Zephyrus as
output, we are left with the task of turning it into a running
system. While Zephyrus output is agnostic as to the nal
deployment tool, we have developed our own tool|called
Armonic |to work closely in conjunction with Zephyrus .
Starting from the conguration generated by Zephyrus ,
Armonic rst provisions the needed virtual machines (VMs)
on a target private OpenStack cloud, creating new instances
as needed. The resource information (CPU, storage, mem-
ory) contained in Zephyrus output are used by Armonic to
choose appropriately-sized VMs (AKA \avors").
After provisioning, Armonic takes care of conguring VMs
using an agent-orchestrator architecture: each VM comes
with an agent which receives conguration instructions from
a central orchestrator. During VM conguration, Armonic
installs on each VMs the packages dictated by Zephyrus ;
tunes service conguration les to implement bindings (e.g.
to \connect" a Wordpress to a MySQL, Armonic will patch
a Wordpress conguration le to point to a given VM IP
address and port); and starts services in the right order as
it goes. In our example, a Wordpress instance is deployed
at location loc1. However, a MySQL database must be
available before the deployment of Wordpress in order to
properly start the service. To address this, Armonic devisesadeployment plan , using bindings to determine a suitable
deployment ordering, and follows it during component de-
ployment. In the example Armonic will deploy Mysql right
away (as it has no further component dependencies), then
Wordpress, and nally the load balancer.
The Armonic orchestrator is equipped with a web inter-
face, shown in Figure 2 at work on the deployment of a
simplied version (using a single MySQL database, instead
of a cluster) of the conguration of Figure 1. To deploy
an application, users can simply feed Zephyrus output into
theArmonic web interface and then monitor live the state
of deployment. In Figure 2 the deployment is almost n-
ished: all components are deployed and connected, except
the last Wordpress instance and the load balancer which are
shown grayed-out, as they are only partially deployed. The
deployment of this use case takes about 7 minutes, including
building and booting virtual machines, package installation,
and service conguration.
3. TOOLCHAIN INTERNALS
3.1 Minimizing input
Figure 3 presents a simple schema of the architecture of
Zephyrus , which is basically structured into ve blocks. The
input phase of Zephyrus collects all the data provided by the
user.
For each location, we take into account not only the avail-
able services, but also all the possible ways to deploy them
(i.e. packages that must be installed to realize them, to-
gether with their dependencies): this can amount to han-
dle tens of thousands of packages for each location, and a
nave approach would simply be unfeasible. We believe this
is one of the fundamental reasons why competing tools do
not represent package relationships explicitly or completely,
with the consequence of potentially producing congurations
which are not deployable due to package incompatibilities
unknown to the tool. We compare this aspect of Zephyrus
with alternative approaches in Section 5.
To render the problem tractable, Zephyrus performs sev-
eral simplication passes on the input data that greatly re-
duce its size: the universe is trimmed by removing all ser-
vices that are not in the transitive closure of the services
present in the initial conguration or the request; package
215Figure 3: Overall architecture of Zephyrus
repositories are pruned by keeping only packages that im-
plement services which were not removed by the previous
simplication phase; lower and upper bounds on the needed
resources and components are computed, and only the min-
imum estimated number of available locations is kept. All
these operations are safe, as we can prove that they do not
exclude any correct solution.
A second important simplication is achieved by using
a slightly modied version of the coinst tool [8], which re-
duces by several orders of magnitude the data present in soft-
ware package repositories, like those oered by the Debian
or RedHat distributions, while retaining all the coinstallabil-
ityinformation needed to determine if a set of packages can
or cannot installed together. We refer the interested reader
to [8] for precise gures and detailed proofs; we just recall
here that this simplication is safe, and preserves all correct
solutions.
3.2 Constraint generation
The second phase of Zephyrus translates the (trimmed)
input into a set of constraints over non-negative integers.
These constraints use dierent variables for the number of
instances to create on each of the locations for each of the
types in the universe, and also variables representing the
packages that must be installed on each location. The con-
straints impose that the instances respect the denition of
their type in the universe, the way how instances are im-
plemented by packages, the (compacted) dependencies and
conicts between packages, and the problem specication.
The most interesting of these constraints ensure that it is
possible to create the bindings between all instances accord-
ing to the capacity constraints. These constraints distin-
guish our approach from others [13, 12, 14], they are neces-
sary due to the exible dependencies we have on ports. Con-
straints are constructed using auxiliary variables B(p; tr; tp)
for the number of bindings on port pbetween requesting
instances of type trand providing instances of type tp.
On the example of Section 2.2, these particular constraints
for the bindings on port sqllook like this:
B(sql;wp;mysql )#mysql3 (1)
B(sql;wp;mysql )#wp2 (2)
B(sql;wp;mysql )#wp#mysql (3)
Here, (1) expresses that the number of bindings on port sql
between instances of the two types is at most the number
of instances of the providing type mysql times 3 (since any
component of that type can bind to at most 3 instances),
(2) that the number of bindings on port sqlis at least the
number of instances of the requesting type wptimes 2 (thenumber of binding each component of type wprequires), and
nally (3) states that the number of bindings is at most the
number of pairs of instances of type wpwith instances of type
sql. This last restriction expresses that no two bindings may
exist between the same pair of instances.
The ability to capture as simple integer constraints the
existence of a complete architecture corresponding to a spec-
ication is the cornerstone of our approach. It allows us to
deal with the many facets of a system design as a whole, and
thus ensures the completeness of our tool and the optimality
of the generated conguration. In particular, if the output of
Zephyrus indicates that several services are to be installed
on the same machine, we know that no conict between the
packages that realize them will arise on actual machines.
3.3 External solvers
The generated constraints, as well as the optimization
function, are expressed using the MiniZinc constraint mod-
elling language [24, 22]. This allows us to employ any of
the many existing constraint solvers that support MiniZinc .
Zephyrus can currently use GeCode [29] (an ecient open
source solver) or several of the solvers provided in the G12
suite [30]. The tool exploits this exibility by implementing
asolver portfolio approach [2, 3] that reduces execution time
by running several solvers in parallel, and stops as soon as
one of the solvers nds a solution.
3.4 ConÔ¨Åguration generation
When the external solver nds a solution for the generated
constraint, the next part of Zephyrus proceeds to transform
that solution, which is a simple mapping from variables to
integers, into an actual conguration. The two main chal-
lenges of that generation are: i) to reuse as many existing
parts of the initial conguration as possible in order to min-
imize the impact on the existing system; and ii) to correctly
generate bindings between the instances taking into account
that any two instances can be bound on a given port at most
once. The algorithms employed in this generation phase are
presented in detail in [7].
Once the conguration has been generated, it can be writ-
ten to a le in two dierent formats: either (a) the same
JSON format used for the input conguration, which pre-
cisely describes all the conguration features and is used by
Armonic as input; or (b) the dotformat that encodes the
conguration into a graph that can be viewed using the dot
program to visualize the synthesized architecture.
If no conguration satises the given input constraints,
Zephyrus will exit with an error message and produce no
output les.
3.5 Synthesis soundness and completeness
An important property of Zephyrus is that all its parts
have been formalized. In particular, the translation into
constraints and the generation of the conguration, two very
complex and important pieces of Zephyrus , have been pre-
cisely described and proven correct in [7], where the follow-
ing results have been shown for Zephyrus :
Theorem 1 (Soundness) .The conguration generated by
Zephyrus is correct w.r.t. the input universe and specica-
tion.
Theorem 2 (Completeness) .If there exists a congura-
tion that validates the input universe and specication, then
Zephyrus will successfully generate a correct conguration.
216Figure 4: Armonic representation of Mysql and Word-
press component types.
Theorem 3 (Optimality) .The conguration generated by
Zephyrus is optimal w.r.t. the input optimization function.
3.6 Deployment planning
Virtual machine selection. In this paper, Armonic uses
the popular OpenStack platform to provision virtual ma-
chines, starting from the locations entries found in Zephyrus
output to determine machine names and resources (com-
pute, storage, and memory capacity). In order to map re-
sources to the available VM\avors", a correspondence table
is dened between provide_resources and Openstack a-
vors, for instance:
[["ram", 2048]] -> m1.small
Using this table, Armonic can create VMs using the Open-
stack API, e.g.:
nova boot --flavor m1.small --image debian-squeeze loc1
Component life-cycle. Armonic associates each component
type to an acyclic state diagram that captures the compo-
nent life-cycle. As an example, Figure 4 shows the life-cycles
for the Worpdress and Mysql component types that we have
used in the walkthrough of Section 2.
Dierent states may require and provide dierent ports.
For instance, MySQL's active state provides a port @sql,
denoting that such port can be depended upon only when
that state in the MySQL life-cycle has been reached. Given
that Wordpress' congured (and subsequent) state(s) re-
quire that port, Wordpress cannot enter such state before
MySQL has entered its active state.
Determining deployment order. Section 2.3 briey intro-
duces the need of a component deployment plan. Com-
puting such a plan can be quite challenging (even unde-
cidable [9]), depending on the expressivity of component
constraints. Hence, Armonic makes several simplifying as-
sumptions to keep the problem tractable.
First, Armonic currently assumes that all VMs are\empty",
with no services initially deployed on them. This assumption
simplies deployment of services because it ignores recong-
uration needs. Second, we suppose there is only one path to
reach a given state, while Armonic life-cycle representation
allows for multiple paths. Finally, we suppose that there
are no circular dependencies between components. We are
working on an improved planning algorithm which will ad-dress these limitations [18] and also allow for optimizations
such as maximally parallel component deployment.
3.7 Service conÔ¨Åguration
To connect components according to Zephyrus bindings,
Armonic has to generate service conguration les and may
have to create resources. For instance, connecting MySQL to
Wordpress consists of creating a MySQL database and user
with the appropriate permissions, and making a Wordpress
conguration le point to the right IP address, port, DB
name, and user name.
To automate this process, Armonic allows to attach addi-
tional information to provide and require ports. Thus, the
provide port @sql of MySQL exposes three required vari-
ables, a database name, a user name, and a user password.
The require port @sql of Wordpress exposes these variables
with predened values. Since the component MySQL is the
provider of the bindings @sql which has Wordpress as re-
quirer, Armonic uses this information to bind requirer and
provider conguration variables. In this case, Armonic will
call the provide port @sql of MySQL with values of require
port @sql of Wordpress. This action will create the database
and the MySQL user. These values will then be used by the
Armonic agent to patch the Wordpress conguration le.
4. EXPERIMENTATION
The complete toolchain presented in this paper is available
as free software, released under the GPL license. Zephyrus
amounts to about 10.000 lines of OCaml [20] and is avail-
able at https://github.com/aeolus-project/zephyrus/ ;
Armonic is about 5.000 lines of Python, plus glue code for
component life-cycles written in shell script or Augeas [28],
and is available at https://github.com/armonic/armonic .
We have experimented the complete toolchain in both arti-
cial and industrial settings; in this section we present some
of our ndings. As the gures for Armonic are dominated
by the deployment time used by system-level tools (package
managers, service startup, etc.), and also because we have
already briey presented them at the end of Section 2.3, we
focus here on Zephyrus .
4.1 Synthesis efÔ¨Åciency
Given that the architecture synthesis part of our toolchain
implemented by Zephyrus has a daunting complexity in the-
ory one may ask whether this part could be a bottleneck of
our approach. In order to answer this question we have con-
ducted several architecture synthesis benchmarks on both
realistic and extreme use cases. The ones we illustrate here
are variants of the WordPress example described in Sec-
tion 2. There are, however, three important changes needed
to properly benchmark it:
The use case is parameterized to scale it up and to
demonstrate how Zephyrus handles problems which
require more and more components and locations: (i)
the rst parameter is the minimum replication con-
straint on the wordpress backend port (required by
the load balancer); (ii) the second parameter is the
minimum replication constraint on the sqlport (re-
quired by WordPress components).
The resources associated to available locations are in-
spired by Amazon's EC2 VM oering. We took what
217Figure 5: WordPress synthesis benchmarks, for
increasing values of the replication constraints on
wordpress backends (x-axis) and mysql (y-axis)
Amazon calls \old" (previous generation) general pur-
pose machines. So there are four types of locations
available, corresponding (by cost and capacity) to Ama-
zon instance types: m1.small ,m1.medium ,m1.large
and m1.xlarge . We have provided Zephyrus with a
nite, but large enough number of machines (250 for
each instance type).
We use a single package repository associated to each
machine, and a single package implementing all the
available component types. This simplication does
not aect the test results, as all component types in
this benchmark are co-installable anyhow.
We have used a portfolio of solvers, as discussed in Sec-
tion 3.3, consisting of the following 3 solvers: Gecode [29],
the standard nite domain constraint solver from the G12
suite [30], and the G12/CPX (Constraint Programming with
eXplanations) solver from the G12 suite. As these solvers
are optimized for dierent goals, each of them works better
in some situations and worse in others. It is very dicult to
guess beforehand which solver is more adapted to a specic
constraint problem instance. The portfolio approach per-
mits us to work around this obstacle by trying these three
approaches at the same time.
We have varied the two use case parameters from 1 to 16.
Execution times are obtained as average of 5 runs on a com-
modity desktop machine (Intel i7 3.40 GHz, 8 GB of RAM).
The diagram in Figure 5 shows that a vast majority of cases
are solved very quickly in less than one minute. Only the
larger ones can take more than 20 minutes, e.g. the (14,16)
case, which is the highest peak in the chart. To put this
worst-case solving time into perspective, please note that
the largest use case ( (16,16) ) consists of 103 components,
interconnected by 272 bindings, and distributed over 86 ma-
chines. This surpasses by a signicant margin the size of
most professional WordPress deployments.
4.2 Application to continuous integration
Zephyrus has been deployed in a large industrial use case
at Kyriba Corporation3, a large software editor providing
Software-as-a-Service treasury management solutions. In
3http://www.kyriba.com/
Figure 6: local qualication process at Kyriba
Figure 7: remote qualication process at Kyriba
the following we oer a return on that experience, validating
the usefulness of the proposed approach in an industrial set-
ting. This use case highlights the importance of considering
all system design constraints together and the benets of
statically detecting when they are not satisable|in which
case Zephyrus will exit with an error. It also shows the exi-
bility of our toolchain, by relying on a (in-house) deployment
back-end other than Armonic for the actual deployment.
Kyriba solution is a complex software platform composed
of more than 150 components deployed on multi-tier archi-
tectures, with many dierent versions running at the same
time. Maintaining the consistency of the system as a whole
is a major undertaking. To address this challenge, Kyriba
has invested in completely automating the build, integra-
tion, and deployment processes.
Kyriba distinguishes two software qualication processes:
a local one run by individual developers on their machines;
and another, more thorough one run on a remote contin-
uous integration (CI) service. Heavy, exhaustive tests are
performed remotely, whereas individual developers only run
a subset of available tests on their machines.
Successful completion of the local qualication process,
detailed in Figure 6, is required in order to be able to com-
mit code changes to the source version control system. After
each commit the process depicted in Figure 7 is triggered:
rst CI runs the same process that has been run on develop-
ers machine; automatic deployment is then performed on the
cloud infrastructure with the latest component version, and
more extensive tests|UI, deep functional scenarios, stress
tests|are executed.
4.2.1 A Case for automation
Kyriba follows the continuous integration recommenda-
tions [10] and implements acceptance and stress tests. These
tests are very time consuming: while local tests take less
than 4 minutes to complete, global ones might take 4{8
hours. Furthermore, as Kyriba solution is an assembly of
multiple components, integration tests involve many inter-
dependent components that should all be deployed before
testing. When deploying on a single machine, maintaining
218consistency (e.g. version alignment) is rather easy and can
be enforced using package dependencies; when components
are distributed as services on multiple physical/virtual ma-
chines, consistency is harder to maintain.
In the past, test deployment was done using custom tools
involving a manual setup, and component/protocol incom-
patibilities were only detected at runtime. Short feedback
loops discipline helps developers with error diagnostic re-
lated to small code changes [16], so Kyriba has been looking
for a tool that could anticipate error detection.
Zephyrus turned out to be a perfect t for this need, as
a deployment validation tool for both the local and remote
qualication processes. Zephyrus is now used in distributed
component consistency validation and deployment cong-
uration scenarios. Zephyrus helps to get feedback before
launching local deployments tests and has led to a signif-
icant reduction of the number of failures occurring during
automated deployment in comparison to the previous, more
manual, test setup.
4.2.2 Zephyrus adoption
For developers. Developers dene relationships between
components in partial Zephyrus les when they create pack-
ages for their project. These les are then merged with
Zephyrus les containing the full infrastucture description
provided by engineering operations team before being pro-
cessed by the solver.
Two kinds of such relationships need to be dened:
Dependencies between packages with specic version
requirements, e.g. the application 1.0 requires a web
server of version at least 3.2.4 to be installed on the
same machine to run properly;
Service binding relationship with API level require-
ment, e.g. the application 1.0 requires a service API
version 1 exposed by another application on some ma-
chine, not necessarily where the application is deployed.
Developers can declare these requirements using ports spec-
ied in the Zephyrus universe denition:
{ "component_types": [
{ "name" : "fa-accounting-engine-0.1",
"provide": [["@fa-accounting-engine-v1", ["
FiniteProvide", 1]]],
"require": [["@graphite-v3", 1]] } ],
"implementation": [
[ "fa-accounting-engine-0.1",
[["debian-kyriba",
"kyriba-fa-accounting-engine (= 0.1)"]] ] ] }
The component type kyriba-fa-accounting-engine provides
a \fa-accounting-engine-v1" service with API level 1 and re-
quires a \graphite-v3" service. In the implementation sec-
tion, the component_type is linked to the concrete package
implementation kyriba-fa-accounting-engine (= 0.1) . This
partial universe denition is merged with the full universe
description (containing the denitions of all Kyriba compo-
nents) and the default specication in order to verify that
at least one nal conguration exists. This ensures that no
dependency problem can arise during deployment.
Figure 8: local qualication process with Zephyrus
Figure 9: global qualication process with Zephyrus
Developers may also override the default specification
with their own specication to validate dierent deployment
scenarios during the local qualication process.
The local qualication process is modied by adding a
Zephyrus validation stage before (local) deployment, see
Figure 8. Using Zephyrus metadata developers simply de-
clare component interfaces and the way they are exposed.
Moreover, using Zephyrus , developers know beforehand if
the components they are working on can be deployed to-
gether with other components.
For continuous integration and deployment environment.
Similarly to the local one, the global qualication process
has been instrumented with an extra Zephyrus validation
stage as illustrated in Figure 9.
Zephyrus automatically checks application consistency be-
fore actually engaging in a deployment process. If the solver
nds a solution, the related conguration is used by infras-
tructure management scripts based on the Fabric library4
in order to orchestrate an integration test deployment on
the cloud infrastructure (such as the Amazon Elastic Cloud
Computing service). The newly created platform is then
checked against all acceptance, stress and UI test cases.
To upgrade the production platform. Zephyrus is also
used to plan platform upgrades on the infrastructure cur-
rently in production. According to the software road map,
product managers dene the component versions needed to
be shipped to production for a milestone release. All those
values are set in Zephyrus les, and based on the current
production deployment, Zephyrus computes an output le
containing the dierent application packages that should be
installed with the related conguration parameters that need
to be set for application binding. This guideline le is then
used by engineering teams to write orchestration scripts and
pinpoint manual upgrade tasks.
4.2.3 Outcome
Summing up, Kyriba's experience with Zephyrus is that,
instead of managing deployment scenarios manually using
spreadsheets and at documents, with ad hoc semantics
leading to complex, time-consuming and error-prone deploy-
ments, Zephyrus brings precise semantics and simplies the
4http://www.fabfile.org
219automation of software qualication processes. Zephyrus
provides static validation before actually performing expen-
sive and very long dynamic validation at runtime. As most
\compiler-like" tools, Zephyrus improves engineering quality
and reduces building cost with less failures at deployment,
integration test stage and platform upgrade.
5. RELATED WORK
The problem of managing networks of interconnected ma-
chines has attracted signicant attention in the area of sys-
tem administration. Many popular system management tools
exist to that end: CFEngine [4], Puppet [17], MCollec-
tive [27], and Chef [26] are just a few among the most popu-
lar ones. Despite their dierences, such tools allow to declare
the components that should be installed on each machine,
together with their conguration les. Then, they employ
various mechanisms to deploy components accordingly. The
burden of specifying where components should be deployed,
and how to interconnect them is left to the sysadmin, let
alone the dicult problem of optimal resource allocation.
As an extra complication, system management tools stop at
the package management abstraction, and therefore have no
way of knowing in advance whether deployment will succeed
or not. If the sysadmin requests to install two incompati-
ble web servers on the same machine, the incompatibility
will only be discovered by the package manager at deploy
time, when one of the two services fails to get installed (or
started). At that point, it is up to the admin to go back
to the planning stage and work around the incompatibil-
ity. In our approach all package relationships are known to
Zephyrus which can therefore plan around component in-
compatibilities.
System management tools can be used as an alternative
toArmonic , though. Once optimal resource allocation is
done by Zephyrus , the actual deployment can be delegated
to them, now with the guarantee that no deployment error
due to incompatibilities will arise; an interesting candidate
could be [31].
CloudFoundry [32] specically targets application deploy-
ment in the \cloud", but suers from the same limitations
as described above. ConfSolve [15] improves on the system
management approach: it relies on a constraint solver to
propose an optimal allocation of virtual machines to servers,
and of applications to virtual machines, but it does not han-
dle connections among services, nor capacity or replication
constraints, and is unaware of package incompatibilities.
In Juju [5], each service is deployed on a single machine
(or, more recently, in a virtual container on a machine).
That avoids the issue of component incompatibilities, but
does so at the price of wasting resources. In our Word-
press example Zephyrus proposes a solution that needs 4
machines, whereas Juju would have required 6.
Two recent eorts, Feinerer's work on UML [12] and En-
gage [14], are more similar to our approach as they both rely
on a solver to plan deployments. Feinerer's work is based on
the UML component model, which includes conicts and de-
pendencies with capacity constraints, but uses dependencies
only between components, which greatly restricts the expres-
siveness of the model (choices are not possible). Moreover,
no tool for actually building the computed conguration is
provided. Engage, on the other hand, oers no support for
conicts in the specication language: one can only indi-
cate that a service can be realized by exactly one out of alist of components. Neither Feinerer's work nor Engage, or
any other tool that we are aware of, allows to nd a deploy-
ment that uses resources in an optimal way, minimizing the
number of needed (virtual) machines.
Another approach to automated deployment is proposed
in [11], which uses an Architecture Description Language
with user-provided information about relationships among
software services, and implements a decentralized protocol
to perform automatic conguration. This work may also be
used as a backend for Zephyrus .
Finally, we would like to put Zephyrus in perspective. In
our view, automated management of cloud applications is
best realized by a 2-phase approach. In the rst phase ( ar-
chitecture synthesis )Zephyrus or similar tools are used to
devise an optimal system architecture. Then, in a second
phase ( planning ), the obtained conguration is compared
with that of the existing system to produce a detailed de-
ployment plan that migrates the existing system to the de-
sired one. To implement planning, the actual state of ser-
vices and their life cycles (e.g. how do they pass from an
inert \installed" state to an \up and running" one? do de-
pendencies and conicts change in the meantime?), ignored
for the purpose of this paper, become relevant again. Even
though planning has been shown to be undecidable in the
most general case [18], promising progress has been made
on automated planning for restricted cases, like planning in
the presence of complex activation requirements that include
circular dependencies, whereas giving up the possibility of
expressing component conicts [19].
6. CONCLUSION
We have introduced an automated approach to the design
and deployment of complex distributed applications com-
posed of interconnected services, as typically found in mod-
ern \cloud" environments. The system architect can specify
the components needed to obtain the required functional-
ities, add non-functional constraints|e.g. maximum num-
ber of client components connected to a given service, or
minimum number of replicas|as well as available physical
resources|e.g. memory or bandwidth|and declare compo-
nent incompatibilities. The architect can also choose an op-
timization goal, allowing to specify whether she prefers a
conservative solution that changes the current conguration
as little as possible, or a minimum-cost solution.
The approach is realized by two complementary tools:
Zephyrus will synthesize an optimal architecture, including
precise information about service interconnections. Such an
architecture is then fed to the second tool, Armonic , which
is able to deploy it on state-of-the art cloud infrastructures
such as OpenStack, taking care of all deployment aspects
from machine provisioning to service conguration and ini-
tialization. A major advantage of the proposed approach
w.r.t. the state of the art is that all existing constraints,
including software package-level incompatibilities, are taken
into account to prevent deploy-time errors. We have val-
idated the complete toolchain both theoretically, showing
soundness and completeness of the approach, and practi-
cally by applying it to relevant industrial use cases.
To the best of our knowledge this toolchain is the rst
that allows to consistently handle capacity and replication
constraints, conicts, and service co-location, thus nally
providing an instrument able to handle the stringent require-
ments of cloud applications in the real world.
2207. REFERENCES
[1] A. Aleti, B. Buhnova, L. Grunske, A. Koziolek, and
I. Meedeniya. Software architecture optimization
methods: A systematic literature review. IEEE TSE ,
39(5):658{683, 2013.
[2] R. Amadini. Evaluation and application of portfolio
approaches in constraint programming. Theory and
Practice of Logic Programming (TPLP),
13(4-5-Online-Supplement), 2013.
[3] R. Amadini, M. Gabbrielli, and J. Mauro. An
empirical evaluation of portfolios approaches for
solving CSPs. In C. P. Gomes and M. Sellmann,
editors, Constraint Programming for Combinatorial
Optimization Problems (CPAIOR) , volume 7874 of
LNCS , pages 316{324, 2013.
[4] M. Burgess. A site conguration engine. Computing
Systems , 8(2):309{337, 1995.
[5] Canonical Ltd. Juju, devops distilled.
https://juju.ubuntu.com/ . Retrieved October 2013.
[6] M. Catan, R. Di Cosmo, A. Eiche, T. A. Lascu,
M. Lienhardt, J. Mauro, R. Treinen, S. Zacchiroli,
G. Zavattaro, and J. Zwolakowski. Aeolus: Mastering
the complexity of cloud application deployment. In
ESOCC 2013: Service-Oriented and Cloud Computing ,
volume 8135 of LNCS , pages 1{3, 2013.
[7] R. Di Cosmo, M. Lienhardt, R. Treinen, S. Zacchiroli,
and J. Zwolakowski. Optimal provisioning in the
cloud. Technical report, Universit e Paris Diderot,
2013. Available at
http://hal.archives-ouvertes.fr/hal-00831455/ .
[8] R. Di Cosmo and J. Vouillon. On software component
co-installability. In Foundations of Software
Engineering (FSE) , pages 256{266. ACM, 2011.
[9] R. Di Cosmo, S. Zacchiroli, and G. Zavattaro.
Towards a formal component model for the cloud. In
Software Engineering and Formal Methods (SEFM)
2012, volume 7504 of LNCS , pages 156{171, 2012.
[10] P. M. Duvall, S. Matyas, and A. Glover. Continuous
integration: improving software quality and reducing
risk. Pearson Education, 2007.
[11] X. Etchevers, T. Coupaye, F. Boyer, and N. de Palma.
Self-conguration of distributed applications in the
cloud. In International Conference on Cloud
Computing, pages 668{675. IEEE, 2011.
[12] I. Feinerer. Ecient large-scale conguration via
integer linear programming. Articial Intelligence for
Engineering, Design, Analysis and Manufacturing (AI
EDAM), 27(1):37{49, 2013.
[13] I. Feinerer and G. Salzer. Consistency and minimality
of UML class specications with multiplicities and
uniqueness constraints. In Theoretical Aspects of
Software Engineering (TASE) , pages 411{420, 2007.
[14] J. Fischer, R. Majumdar, and S. Esmaeilsabzali.
Engage: a deployment management system. In
PLDI'12: Programming Language Design and
Implementation, pages 263{274. ACM, 2012.
[15] J. A. Hewson, P. Anderson, and A. D. Gordon. A
declarative approach to automated conguration. In
LISA '12: Large Installation System Administration
Conference, pages 51{66, 2012.[16] J. Humble and D. Farley. Continuous delivery: reliable
software releases through build, test, and deployment
automation . Pearson Education, 2010.
[17] L. Kanies. Puppet: Next-generation conguration
management. ;login: the USENIX magazine ,
31(1):19{25, 2006.
[18] T. A. Lascu, J. Mauro, and G. Zavattaro. Automatic
component deployment in the presence of circular
dependencies. In Formal Aspects of Component
Software (FACS) 2013 , volume 8348 of LNCS , 2013.
[19] T. A. Lascu, J. Mauro, and G. Zavattaro. A planning
tool supporting the deployment of cloud applications.
InInternational Conference on Tools with Articial
Intelligence (ICTAI) , pages 213{220. IEEE, 2013.
[20] X. Leroy, D. Doligez, J. Garrigue, and D. R emy. The
Objective Caml system release 4.01; Documentation
and user's manual . INRIA, Rocquencourt, Paris, 2013.
[21] F. Mancinelli, J. Boender, R. D. Cosmo, J. Vouillon,
B. Durak, X. Leroy, and R. Treinen. Managing the
complexity of large free and open source
package-based software distributions. In International
Conference on Automated Software Engineering
(ASE) , pages 199{208. IEEE, 2006.
[22] K. Marriott, N. Nethercote, R. Rafeh, P. J. Stuckey,
M. G. de la Banda, and M. Wallace. The design of the
zinc modelling language. Constraints , 13(3):229{267,
2008.
[23] I. Neamtiu and T. Dumitras. Cloud software upgrades:
Challenges and opportunities. In Maintenance and
Evolution of Service-Oriented and Cloud-Based
Systems (MESOCA), 2011 International Workshop on
the, pages 1{10, Sept. 2011.
[24] N. Nethercote, P. J. Stuckey, R. Becket, S. Brand,
G. J. Duck, and G. Tack. Minizinc: Towards a
standard CP modelling language. In Principles and
Practice of Constraint Programming (CP) , pages
529{543, 2007.
[25] Normation. Rudder { open source automation &
compliance. https://www.rudder-project.org/ .
Retrieved April 2014.
[26] Opscode. Chef. http://www.opscode.com/chef/ .
Retrieved October 2013.
[27] Puppet Labs. Marionette collective.
http://docs.puppetlabs.com/mcollective/ .
Retrieved October 2013.
[28] RedHat. Augeas { a conguration API.
http://augeas.net/ . Retrieved April 2014.
[29] C. Schulte, M. Lagerkvist, and G. Tack. Gecode.
http://www.gecode.org/ . Retrieved October 2013.
[30] P. J. Stuckey, M. G. de la Banda, M. Maher,
J. Slaney, Z. Somogyi, M. Wallace, and T. Walsh. The
G12 project: Mapping solver independent models to
ecient solutions. In International Conference on
Logic Programming (ICLP), volume 3668 of LNCS ,
pages 9{13, 2005.
[31] S. van der Burg, E. Dolstra, and M. de Jonge. Atomic
upgrading of distributed systems. In Hot Topics in
Software Upgrades , pages 1{5. ACM, 2008.
[32] VMWare. Cloud Foundry, deploy & scale your
applications in seconds. Retrieved October 2013,
http://www.cloudfoundry.com/ .
221