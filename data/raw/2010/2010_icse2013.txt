Published in Proceedings of the 35thInternational Conference on Software Engineering (ICSE‚Äô13), San Francisco, California, May 2013.
Inferring Likely Mappings Between APIs
Amruta Gokhale Vinod Ganapathy Yogesh Padmanaban
amrutag@cs.rutgers.edu vinodg@cs.rutgers.edu ypadmana@cs.rutgers.edu
Department of Computer Science, Rutgers University, Piscataway, NJ, USA
Abstract ‚ÄîSoftware developers often need to port applications
written for a source platform to a target platform. In doing so,
a key task is to replace an application‚Äôs use of methods from
the source platform API with corresponding methods from the
target platform API. However, this task is challenging because
developers must manually identify mappings between methods
in the source and target APIs, e.g., using API documentation.
We develop a novel approach to the problem of inferring
mappings between the APIs of a source and target platform.
Our approach is tailored to the case where the source and
target platform each have independently-developed applications
that implement similar functionality. We observe that in building
these applications, developers exercised knowledge of the corre-
sponding APIs. We develop a technique to systematically harvest
this knowledge and infer likely mappings between the APIs of
the source and target platform. The output of our approach is a
ranked list of target API methods or method sequences that likely
map to each source API method or method sequence. We have
implemented this approach in a prototype tool called Rosetta,
and have applied it to infer likely mappings between the Java2
Platform Mobile Edition and Android graphics APIs.
I. Introduction
Software developers often wish to make their applications
available on a wide variety of computing platforms. The
developer of a gaming app, for instance, may wish to make
his app available on smart phones and tablets manufactured
by various vendors, on desktops, and via the cloud. The key
hurdle that he faces in doing so is to port his app to these
software and hardware platforms.
Why is porting software a di cult problem? Consider an
example: suppose that we wish to port a Java2 Platform Mobile
Edition (JavaME)-based game to an Android-powered device.
Among other tasks, we must modify the game to use Android‚Äôs
API [10] (the target platform) instead of JavaME‚Äôs API [21]
(thesource platform). Unfortunately, the process of identifying
the API methods in the target platform that implement the
same functionality corresponding to that of a source platform
API method is cumbersome. We must manually examine the
SDKs of the source and target APIs to determine the right
method (or sequence of methods) to use. To add complexity,
there could be multiple ways in which a source API method
can be implemented using the target‚Äôs API methods. For exam-
ple, the fillRect() method in JavaME‚Äôs graphics API, which
Ô¨Ålls a speciÔ¨Åed rectangle with color, can be implemented using
either one of these two sequences of methods in Android‚Äôs
graphics API: setStyle();drawRect() or as moveTo();lineTo();
lineTo();lineTo();lineTo();drawPath() (we have omitted class
names and the parameters to these method calls).
One way to address this problem is to populate a database
ofmappings between the APIs of the source and target plat-forms. In this database, each source API method (or method
sequence) is mapped to a target API method (or method
sequence) that implements its functionality. The database
could contain multiple mappings (possibly ranked) for each
source API method in cases where its functionality can be
implemented in di erent ways by the target API. The mapping
database signiÔ¨Åcantly eases our task. We need only consider
the mappings in this database to Ô¨Ånd suitable target API meth-
ods to replace a source API method, instead of painstakingly
poring over the SDKs and their documentation. Such mapping
databases do exist, but only for a few source /target API pairs
(e.g., Android, iOS and Symbian Qt to Windows 7 [30] and
iOS to Qt [25]), and they are populated by domain experts
well-versed in the source and target APIs.
Our contribution .We present an approach to automate the
creation of mapping databases for any given source /target API
pair. To bootstrap our approach, we rely on the availability
of a few similar application pairs on the source and tar-
get platform. A source platform application Sand a target
platform application T, possibly developed independently by
dierent sets of programmers, constitute a similar application
pair if they implement similar high-level functionality. For
example, bothSandTcould implement the TicTacToe game
on JavaME and Android, respectively. This situation is not
uncommon in modern app markets, where independently-
developed versions of popular apps are available in markets
hosted by di erent vendors. Our approach builds upon the
observation that in implementingSandT, their developers
exercised knowledge about the APIs of the corresponding
platforms . We provide a systematic way to harvest this knowl-
edge into a mapping database, which can then beneÔ¨Åt other
developers porting applications from the source to the target
platform. Our approach works by recording traces of Sand
Texecuting similar tasks, structurally analyzing these traces,
and extracting likely mappings using probabilistic inference.
Each mapping output by our approach is associated with a
probability, which indicates the likelihood of the mapping
being true. The intuition is that the more evidence we see of a
mapping, e.g., the same pair of API methods being used across
many traces to implement similar functionality, the higher the
likelihood of the mapping. The output of our approach is a
ranked list of mappings inferred for each source API method.
We demonstrate our approach by building a prototype tool
called Rosetta (in reference to the legendary Rosetta Stone)
to infer likely mappings between the JavaME and Android
graphics APIs. We chose JavaME and Android because both
platforms use the same language for application development.
However, this requirement is not germane to our approach,(Sn, Tn) Src Tgt Prb A()	 ¬†A()	 ¬†B()	 ¬†B()	 ¬†a()	 ¬†b()	 ¬†a()	 ¬†b()	 ¬†0.9 0.1 0.2 0.8 	 ¬†	 ¬†	 ¬†	 ¬†	 ¬†	 ¬†	 ¬†	 ¬†
T1 T2 T3              Inference (S1, T1) Combine Step 1:  Collect apps S1             
T1             (S1, T1) ‚Ä¶.. A()!B() ...!a()!b()!...!S1 S2 S3              Source API apps Target API apps ‚Ä¶.. 
	 ¬†	 ¬†	 ¬†	 ¬†	 ¬†	 ¬†	 ¬†	 ¬†Src Tgt Prb A()	 ¬†A()	 ¬†B()	 ¬†B()	 ¬†a()	 ¬†b()	 ¬†a()	 ¬†b()	 ¬†0.8 0.2 0.4 0.4 Step 2:  Get trace pairs Step 3:  Infer mappings for each trace pair Step 4:  Combine inferences across traces Fig. 1. WorkÔ¨Çow of our approach to inferring likely API mappings.JavaME Trace Android Trace
Graphics.setColor() Paint.setStyle()
Graphics.fillRect() Color.parseColor()
Graphics.setColor() Paint.setStyle()
Graphics.fillRect() Color.parseColor()
Graphics.fillRect() Canvas.drawLine()
Graphics.fillRect() Canvas.drawLine()
Canvas.drawLine()
Canvas.drawLine()
Canvas.drawLine()
Note: Each trace snippet shows the method in-
voked, and the class in which the method is
implemented. JavaME and Android classes are
preÔ¨Åxed with javax/microedition/lcdui and
android/graphics , respectively. For brevity, we
only refer to method names and not their classes.
Fig. 2. Snippets from traces of similar executions
of JavaME and Android-based TicTacToe games.
and it may be possible to adapt Rosetta to work with source
and target APIs that use di erent languages for application
development. We evaluated Rosetta with a set of twenty-one
independently-developed JavaME /Android application pairs.
Rosetta was able to Ô¨Ånd at least one valid mapping within the
top ten ranked results for 70% of the JavaME API methods
observed in our traces. Further, for 40% of JavaME API
methods, the top-ranked result was a valid mapping.
II. A pproach Overview
We present the workÔ¨Çow of our approach (Figure 1), tailored
to JavaME and Android as the source and target platforms,
respectively. We only provide informal intuitions here, and
defer the details to xIV. The workÔ¨Çow has four steps.
STEP 1: Collection of application pairs. The Ô¨Årst step is
to gather a database of applications in both the source and
target platform. For each source application in the database,
we require a target application that implements the same high-
level functionality. For example, if we have a TicTacToe game
for JavaME, we should locate a TicTacToe game for Android
that is as functionally and visually (GUI-wise) similar to the
JavaME game as possible.
Given the popularity of modern mobile platforms, and the
desire of end-users to use similar applications across platforms,
such application pairs are relatively easy to come by. Of
course, given that the games were independently developed
for these two platforms, there may be minor di erences in
functionality. For example, the Android game may o er menu
options that are di erent from those of the JavaME game.
However, as we discuss in Step 2, we can restrict ourselves to
inducing functionally similar execution paths in these appli-
cations. Any functional di erences that still make their way
into these execution paths will manifest as inaccuracies during
probabilistic inference in Step 3. However, the e ect of these
inaccuracies can be mitigated by combining inferences across
multiple applications pairs and their executions in Step 4.
STEP 2: Execution and collection of trace pairs. In this
step, we take each application pair, and execute them in similar
ways, i.e.,we provide inputs to exercise similar functionality
in these applications. As we do so, we also log a trace of
API calls invoked by the applications. This gives a trace
pair, consisting of one trace each for the source and targetapplications. Figure 2 presents a snippet from a trace pair that
we gathered for TicTacToe games on the JavaME and Android
platforms. They were collected by starting the game, waiting
for the screen to display a grid of squares, and exiting.
Since the traces in each pair were obtained by exercising
similar functionality in the source and target applications, these
traces must contain some API calls that can be mapped to each
other. This is the key intuition underlying our approach. Of
course, an application can be invoked in many ways, and in
this step, we collect several trace pairs for each application
pair. The output of this step is a database of functionally
equivalent trace pairs ( TraceS,TraceT) across all of the
application pairs collected in Step 1. Steps 1 and 2 involve
manual e ort, while steps 3 and 4 are automated.
STEP 3: Trace analysis and inference. In this step we
analyze each trace pair to infer likely API mappings implied by
the pair. Our inference algorithm relies on various attributes
that are determined by the structure of the traces. We cast
the attributes as inputs to a probabilistic inference algorithm
(xIV). The output of this step is a probability for each pair
of source and target API calls A()/a()that indicates the like-
lihood of A()mapping to a()(denoted here as A()!a()). Our
algorithm can also infer mappings between method sequences
(e.g., A()!a();b() orA();B()!a()).
We now discuss the attributes used by our approach using
our running example. Our aim is to Ô¨Ånd likely mappings for
the JavaME calls setColor() and fillRect() . For simplicity, we
restrict our discussion to the snippets of the traces shown in
Figure 2. In reality, our analysis considers the entire trace.
(1)Call frequency. IfA()in the source API maps to the
a()in the target API, then the frequency with which these
method calls occur in functionally-similar trace pairs must also
match. The trace pairs may di er in the absolute number of
method calls that they contain, so we focus on the relative
count of each method call, which is the raw count of the
number of times that a method is called, normalized by
trace length. The table below shows the raw and relative
counts of various method calls based upon the snippets in
Figure 2. Using this attribute, the following API mappings ap-
pear likely, setColor()!parseColor() ,setColor()!setStyle() ,
fillRect()!drawLine() , while the others appear unlikely.
In fact, our approach works on method sequences aswell using the same reasoning as above, and infers that
setColor()!setStyle();parseColor() is a mapping.
API Call Raw Count Relative Count
setColor() 2 0.33
fillRect() 4 0.67
setStyle() 2 0.22
parseColor() 2 0.22
drawLine() 5 0.56
(2)Call position. The location of method calls in a trace pair
provides further information to determine likely mappings.
Since the traces exercise the same application functionality,
method calls that map to each other should appear at ‚Äúsimilar‚Äù
positions in the trace pair. The table below shows the position
of each of the method calls in our running example, roughly
categorized as belonging to the Ô¨Årst half or the second half
of the corresponding traces. We can therefore reinforce the
beliefs about API mappings inferred using call frequencies.
API Call First Half Second Half
setColor() X
fillRect()  X
setStyle() X
parseColor() X
drawLine()  X
(3)Call context. The context in which a method call A()
appears is deÔ¨Åned to be the set of API methods that appears
in its vicinity in the execution trace (e.g., within a pre-set
threshold distance, preceding or following A()in the trace). For
example, both setColor() calls appear in the preceding context
offillRect() calls in the JavaME trace (using a threshold
distance of 2 calls). Likewise, parseColor() and setStyle()
appear in the preceding context of drawLine() in the Android
trace. In fact, the sequence setStyle(); parseColor() appears in
the preceding context of the Ô¨Årst drawLine() . From this, we can
infer that iffillRect()!drawLine() holds, then the mapping
setColor()!setStyle();parseColor() is likely to hold.
(4)Method names. While trace structure, as captured by the
attributes above, contributes to inference, method (and class)
names also contain useful information about likely mappings,
and we leverage this attribute as well. We use Levenshtein edit
distance to compute the similarity of method names. Using this
attribute, for instance, we can lend credence to the belief that
setColor() maps to parseColor() .
Our approach combines these attributes to output likely
mappings, each associated with a probability. We can rank
the results using the corresponding probabilities, and use
thresholds to limit the number of results that are output.
STEP 4: Combining inferences across traces. The inference
algorithm of Step 3 works by analyzing a single trace pair. In
the Ô¨Ånal step, we combine inferences across multiple traces.
During combination, we weight inferences obtained from the
analysis of individual trace pairs using the conÔ¨Ådence of each
inference. Intuitively, more data we have about an inferred
mapping from a trace pair, the stronger our conÔ¨Ådence in that
inference. Thus, our conÔ¨Ådence in mapping A()!a()obtained
from a trace pair where these calls occur several times is
stronger than the same mapping obtained from a trace pair
where these calls occur infrequently. We use this heuristic to
combine inferences across trace pairs.III. F ramework to Represent and Infer Mappings
We now describe the framework used to represent and
infer likely mappings between APIs. We restrict ourselves to
identifying likely mappings between methods of the source
and target APIs. We do not currently consider the problem of
determining mappings between arguments to these methods;
that is a related problem that can be addressed using parameter
recommendation systems ( e.g., [33]). LetS=fA,B,C,: : :gand
T=fa,b,c,: : :gdenote the sets of methods in the source
and target APIs, respectively. Our goal is to determine which
methods inTimplement the same functionality on the target
platform as each method in Son the source platform.
To denote mappings, our framework considers a set of
Boolean variables Xs
t, where s2S and t2T. The Boolean
variable XA
ais set to true if the method call A()maps to the
method call a(), and false otherwise.
This framework extends naturally to the case where a
method (or sequence of methods) in Scan be implemented
with a method sequence in T. For example, suppose that the
method A()is implemented using the sequence a();b() in the
target. We can denote this by assigning true to the Boolean
variable XA
ab. Likewise, we can also denote cases where the
functionality of a sequence of methods A();B() in the source
platform is implemented using a method a()on the target
platform by setting the Boolean variable XAB
atotrue. Although
our framework theoretically supports inference of mappings
between arbitrary length method sequences ( e.g., XABC :::
abc :::=
true), for performance reasons we conÔ¨Ågured Rosetta to only
infer mappings between method sequences of length two.
We approach the problem of inferring API mappings by
studying the structure of execution traces of similar applica-
tions on the source and target platforms. We use the trace
attributes informally discussed in xII to deduce API mappings.
Our approach is inherently probabilistic. It cannot conclude
whether a call A()deÔ¨Ånitively maps to a call a(); rather
it determines the likelihood of the mapping. Thus, it infers
Pr[XA
a=true] for each Boolean variable XA
a. As it observes
more evidence of the mapping XA
abeing true, it assigns a
higher value to this probability. Therefore, our framework
treats each Boolean variable XA
aas a random variable, and our
probabilistic inference algorithm determines the probability
distributions of these random variables.
Each application trace has several method calls, and the
inference algorithm must leverage the structure of these traces
to determine likely mappings. The inference algorithm draws
conclusions not just about individual random variables, but
also about how they are related . For example, consider a
trace snippet TraceS=(: : :;A();A();B();B(); : : :) of an ap-
plication from the source API, and a snippet TraceT=(: : :;
a();a();b();b(); : : :) from the corresponding execution of a
similar application on the target. Suppose also that these are
the only occurrences of A(),B(),a()and b()inTraceSand
TraceT, respectively, and that these execution traces have
approximately the same number of method calls in total. By
just observing these snippets, and relying on the frequency ofmethod calls, each of the following cases is a possibility: XA
a
=true,XA
b=true,XB
a=true,XB
b=true. However, if XA
b=
true, then because of the relative placement of method calls
in these traces ( i.e.,call context), it is unlikely that XB
a=true.
Now suppose that by observing more execution traces, we are
able to obtain more evidence that XA
b=true, then we can
leverage the structure of this pair of traces to deduce that XB
a
is unlikely to be a mapping. Intuitively, the structure of the
trace allows us to propagate the belief that if XA
bistrue, then
XB
aisfalse . Our probabilistic inference algorithm uses factor
graphs [13], [32] for belief propagation.
Factor Graphs. LetX=fx1,x2,: : :,xngbe a set of Boolean
random variables and J(x1,x2,: : :,xn) be a joint probability
distribution over these random variables. Jassigns a probabil-
ity to each assignment of truth values to the random variables
inX. Given such a joint probability distribution, it is natural
to ask what the values of the marginal probabilities Mk(xk) of
each random variable xkinXare. Marginal probabilities are
deÔ¨Åned asMk(xk)=P
i,k;i2[1::n];xi2ftrue ;falsegJ(x1;x2; : : : ; xn).
That is, they calculate the probability distribution of xkalone
by summing upJ(: : :) over all possible assignments to the
other random variables. Mk(xk) allows us to compute Pr[ xk=
true]. Factor graphs allow e cient computation of marginal
probabilities from joint probability distributions when the
joint distribution can be expressed as a product of factors ,
i.e.,J(x1;x2; : : : ; xn)=Q
ifi(Xi). Each fiis a factor, and is a
function of some subset of variables XiX.
For example, consider a probability distribution J(x,y,z).
Let this distribution depend on three factors f(x;y),g(y;z) and
h(z),i.e.,J(x,y,z)=f(x,y).g(y,z).h(z), deÔ¨Åned as follows:1
f(x,y)=0.9 if x_y=false , 0.1 otherwise.
g(y,z)=0.9 if y_z=true, 0.1 otherwise.
h(z)=0.9 if z=true, 0.1 otherwise.
f g h x y z 
Fig. 3. Factor graph of J(x;y;z).A factor graph de-
notes such joint proba-
bilities pictorially as a
bipartite graph with two
kinds of nodes, func-
tion nodes and vari-
able nodes. Each func-
tion node corresponds to
a factor, while each vari-
able node corresponds to a random variable. A function node
has outgoing edges to each of the variable nodes over which
it operates. Figure 3 depicts the factor graph of J. The AI
community has devised e cient solvers [13], [32] that operate
over such graphical models to determine marginal probabilities
of individual random variables. We do not discuss the details
of these solvers, since we just use them in a black box fashion.
Factor graphs cleanly represent how the random variables
are related to each other, and can inÔ¨Çuence the overall prob-
ability distribution. One of the key characteristics of factor
1BecauseJis a probability distribution, the product J(x;y;z) is in fact
Z:f(x;y):g(y;z):h(z), where Zis a normalization constant introduced to ensure that the
probabilities sum up to 1. Henceforth, Zis implied, and will not be shown explicitly.graphs, which led us to use them in our work, is that they were
designed for belief propagation, i.e., in transmitting beliefs
about the probability distribution of one random variable to
determine the distribution of another.
To illustrate this, consider the probability distribution
J(x;y;z), discussed above. Jcan be interpreted as denoting
the probabilities of the outcomes of an underlying Boolean
formula for various assignments to x,y, and z. Under this
interpretation, we could say that the Boolean formula evaluates
totrue for those assignments to x,yandzfor which the value
ofJ(x;y;z) is above a certain threshold, e.g., if the threshold
is 0.6, andJ(true ;true ;false ) is 0.7, we say that the formula
istrue under this assignment. Now suppose that yis likely to
befalse ,i.e.,Pr[y=false ] is above a threshold. We are asked
to Ô¨Ånd under what conditions on xandzthe Boolean formula
still evaluates to true,i.e.,J(x;y;z) is above the threshold.
From the deÔ¨Ånitions of the factors f,g, and h, we know that
Jobtains values that are likely to exceed the threshold if x_y
=false ,y_z=true andzistrue. Given that yis likely to be
false false , these factors lead us to deduce that xis also likely
to be false (giving f(x;y) a high value) and zis likely to be
true (giving g(y;z) and h(z) high values), thereby pushing the
value ofJabove the threshold. Intuitively, the factor graph
allows us to propagate the belief about the value of yinto
beliefs about the value of xandz.xIV shows how we cast the
problem of inferring likely API mappings using factor graphs,
thereby allowing us to transmit beliefs about one mapping into
beliefs about others.
IV . D esign and Implementation of Rosetta
We now describe in detail the Rosetta prototype, which
currently infers mappings between the JavaME and Android
graphics APIs. SpeciÔ¨Åcally, we focus on the machinery that
enables Steps 2-4 discussed in xII.
A. Infrastructure for Trace Collection
To record execution traces, we instrumented JavaME pro-
grams via bytecode rewriting. We used the ASM toolkit [4]
to insert logging functionality that records the name of each
method call (and class name), prior to invocation. During
runtime, this results in a trace of all methods invoked. We
then Ô¨Ålter out just those methods that derive from the class
javax/microedition/lcdui ‚Äîthe JavaME graphics API. In all,
this API has 281 distinct methods. Rosetta infers mappings
for those methods that appear in application traces.
We did not employ bytecode rewriting for Android applica-
tions because of the lack of publicly-available tools to rewrite
Android‚Äôs dexbytecode. Instead, we leveraged the Dalvik
virtual machine (v2.1r1) to record the names of all methods
invoked by an application. We record all method and class
names, and then Ô¨Ålter methods in the following classes (preÔ¨Åx:
android/ )graphics ,text,view,widget ,content/DialogInterface ,
app/Dialog ,app/AlertDialog ,app/ActionBar . This API has 3,837
distinct methods. The di erence in the sizes of these APIs
illustrates in part the di culties that a programmer manually
porting an application would face.With this infrastructure, a Rosetta user can collect traces for
a pair of similar applications on both platforms. As discussed
inxII, the user must exercise similar functionality in both
applications, thereby collecting a pair of traces that record
this functionality. This process can be repeated multiple times
for the same application, exercising di erent functionalities,
thereby resulting in a database of trace pairs. Although it
is hard to provide concrete guidelines to ‚Äúexercise similar
functionality,‚Äù we found that it was relatively easy to do
so for gaming applications. Given similar games, they will
likely have the same logic and similar GUIs on both appli-
cations. We simply performed the same moves on games in
both platforms, avoiding situations that involve randomness
where possible ( e.g., choosing the two-user mode to avoid the
computer picking moves at random). However, randomness
is not always avoidable, e.g., some games only support user
versus computer modes, and this randomness may manifest
in the corresponding portions of the traces as well. Despite
this, Rosetta infers high-quality mappings because its inference
algorithm prioritizes method mappings that persist both across
the entirety of each trace pair, and across multiple trace pairs.
B. Trace Analysis and Inference
In this step, Rosetta analyzes each trace pair collected in the
previous step and draws inferences about likely API mappings
implied by that trace pair. Recall from xIII that we use a
Boolean random variable XA
ato denote a mapping between
A()and a()(likewise XA
abetc., for method sequences). In this
step, Rosetta uses factor graphs to compute Pr[ XM
m=true], for
each such random variable, where Mandmdenote individual
methods or method sequences from the source and target
APIs, respectively. The value of this probability determines
the likelihood that the corresponding mapping holds.
The intuition behind Rosetta‚Äôs use of factor graphs is as
follows. The set of random variables XM
mimplicitly deÔ¨Ånes a
joint probability distribution Jover these random variables:
J(XA
a,XA
b,: : :,XB
a,XB
b,: : :XA
ab,XB
ab,: : :). As inxIII, we
can assignJa Boolean interpretation. That is, we treat J
as a probability distribution that estimates the likelihood of
an underlying Boolean formula being true, under various
truth assignments to the random variables XA
a,XA
b,etc. From
this joint distribution, our goal is to Ô¨Ånd the probability
distributions of the individual random variables. Under this
Boolean interpretation, if Pr[ XA
a=true] acquires a high value,
it means that the Boolean formula underlying Jis likely to
betrue ifXA
aistrue, thereby leading us to conclude that A()
is likely to map to a(). Likewise, if Pr[ XA
a=true] acquires a
low value, A()is unlikely to map to a().
The main challenge in directly realizing this intuition within
an inference tool is that the Boolean formula underlying
Jis unknown (for if it were known, then any satisfying
assignment to it would directly yield an API mapping!).
As a result, the joint probability distribution Jcannot be
explicitly computed. However, the attributes described in xII
determine the conditions that are likely to inÔ¨Çuence J. Thus,
we formalize each of these attributes as factors, and estimateAlgorithm 1 :Inferring likely mappings.
Input : A trace pair ( TraceS,TraceT).
Output : Pr[XM
m=true] for each XM
m, where Mandmare methods and
sequences in TraceS,TraceT, respectively.
MethSeqS=set of methods and method sequences that appear in TraceS
(sequences up to length 2 in our prototype)
MethSeqT=set of methods and method sequences that appear in TraceT
foreach (M2MethSeqSandminMethSeqT)do
ffreq(XM
m)=simCount (M,m,TraceS,TraceT)
fpos(XM
m)=simPos (M,m,TraceS,TraceT)
fname (XM
m)=simName (M,m)
foreach (M,N2MethSeqSandm,ninMethSeqT)do
fctxt(XM
m,XN
n)=simCtxt (M,N,m,n,TraceS,TraceT)
fctxt(XM
n,XN
m)=simCtxt (M,N,n,m,TraceS,TraceT)
Let the setFdenote the factors gathered above.
LetJ(XA
a,XA
b,: : :,XB
a,XB
b,: : :XA
ab,XB
ab,: : :)=Q
f2Ff.
Use factor graphs to obtain marginal probabilities for each XM
mfromJ.
Algorithm 2 :Subroutines invoked by Algorithm 1.
simCount (M,m,TraceS,TraceT)
begin
relCount (M)=#Occurences of MinTraceS
Length (TraceS)
relCount (m)=#Occurences of minTraceT
Length (TraceT)
Return min[relCount (M)
relCount (m),relCount (m)
relCount (M)]
end
simPos (M,m,TraceS,TraceT)
begin
relPos( M)[.]=relative positions of MinTraceS
relPos( m)[.]=relative positions of minTraceT
avgRP (M)=average of values in relPos (M)[.]
avgRP (m)=average of values in relPos (m)[.]
if(the values in the array relPos (M)[.] are within a threshold (1% of the
trace length) of avgRP (M) and likewise for relPos (m)[.] and avgRP (m))
then Return min[avgRP (M)
avgRP (m),avgRP (m)
avgRP (M)]
else Return undecided (the value of undecided is 0.5)
end
simName (M,m)
begin
if(Mandmare individual methods) then Return levenshtein .ratio (M,m)
else Return undecided
end
simCtxt (M,N,m,n,TraceS,TraceT)
begin
relCount (M,N)=#Occurences of Min preceding context of NinTraceS
Length (TraceS)
relCount (m,n)=#Occurences of min preceding context of ninTraceT
Length (TraceT)
relCount (N,M)=#Occurences of Nin preceding context of MinTraceS
Length (TraceS)
relCount (n,m)=#Occurences of nin preceding context of minTraceT
Length (TraceT)
ifApproxMatch (relCount (M,N),relCount (m,n)) and
ApproxMatch (relCount (N,M),relCount (n,m))then Return high (we
conÔ¨Ågued high to be 0.7), else Return (1 - high).
end
the joint probability distribution as the product of these factors.
Of course, these factors are not comprehensive, i.e., there
may be other factors that inÔ¨Çuence the value of J. Rosetta
can naturally accommodate any new factors; they are simply
treated as additional factors in the product.
Rosetta‚Äôs trace analysis computes four families of factors,
one each for the four attributes. It combines them and uses
them for probabilistic inference of likely mappings as shown
in Algorithm 1.
(1) Call frequency ( ffreq).The intuition underlying this factor
is that if Mmaps to m(where Mandmare individual methods
or method sequences), then the frequency with which they
appear in functionally similar traces must match. Thus, wecompute the relative count of Mand mas the number of
times that they appear, normalized by the corresponding trace
length. We then use the ratio of relative counts of Mandmto
compute ffreq. This is described in the subroutine simCount ,
shown in Algorithm 2.
(2) Call position ( fpos).We observed in our experiments
that certain API methods and sequences appear only at speciÔ¨Åc
positions in the trace. For example, API methods that initialize
the screen or game state appear only at the beginning of
the trace. To identify such methods and sequences, we use
a similarity metric that determines the relative position of the
appearance of the method call or call sequence in the trace,
i.e.,its oset from the beginning of the trace, normalized by
the trace length. Of course, there may be multiple appearances
of the method call or sequence in the trace, so we average their
relative positions.
In this factor, we restrict ourselves only to calls and se-
quences that are localized in a certain portion of the trace,
i.e.,if the relative positions are not within a threshold (1% of
the trace length) of the average, this factor does not contribute
positively or negatively to the likelihood of the mapping
(undecided is a probability of 0.5). This is described in the
subroutine simPos in Algorithm 2.
(3) Method names ( fname ).We use the names of methods
in the source and target APIs to determine likely mappings.
Unlike the other factors, which are determined by trace struc-
ture ( i.e.,program behavior), this is factor relies on a syntactic
feature. The simName subroutine in Algorithm 2 uses a ratio
based upon the Levenshtein edit distance, computed using a
standard Python library [24]. This ratio ranges from 1 for
identical strings, to 0 for strings that do not have a common
substring. simName only returns a valid ratio for individual
methods; for sequences, it returns undecided .
(4) Call context ( fctxt).We deÔ¨Åne the context of a method
call A()in a trace as the set of method calls that appear
in the vicinity of A(). Likewise, the context of a sequence
A();B() is the set of method calls that appear in the vicinity
of this sequence (if it exists in the trace). Considering context
allows us to propagate beliefs about likely mappings. Recall
the example presented in xIII, where considering the frequency
of the method calls A(),B(),a(), and b()alone does not allow
precise inference of mappings. In that example, the context
of the calls allows us to infer that if XA
bistrue, then XB
ais
unlikely to be true. Of the four factors that we consider, fctxt
is the only one that relates pairs of random variables; the others
assign probabilities to individual random variables.
We deÔ¨Åne the context of a method call or sequence M
in the trace as the set of method calls and sequences that
appear within a Ô¨Åxed distance kofMin the trace; in our
prototype, k=4. When computing the context of M, we also
consider whether the entities its context precede Mor follow
M. To compute context as a factor, we use the function
simCtxt , which considers all pairs of methods and method
sequences ( M,N) that appear in the source trace, and all
pairs ( m,n) that appear in the target trace. We then count
the number of times Mappears in the preceding contextofNin the source trace ( i.e., within k=4 calls preceding
each occurence of N) and normalize this using the trace
length ( relCount (M,N)); likewise we compute relCount (N,M),
and the corresponding metrics for the target trace. We then
check whether relCount (M,N) ‚Äúmatches‚Äù relCount (m,n), and
relCount (N,M) ‚Äúmatches‚Äù relCount (n,m) We do not require
the relative counts to match exactly; rather their di erence
should be below a certain threshold (10% in our prototype);
ApproxMatch encodes this matching function.
If both the counts match, then the factor fctxt(XM
m,XN
n)
positively inÔ¨Çuences the inference that if XM
mistrue, then XN
n
is also true, and vice-versa. The simCtxt function ensures this
by returning a high probability value. Likewise, if the counts
do not match, fctxt(XM
m,XN
n) would indicate that XM
mandXN
n
are unlikely to be true simultaneously. Note that this does not
preclude XM
morXN
nfrom being true individually. Intuitively,
the Boolean interpretation of fctxt(XM
m,XN
n) is ( XM
m^XN
n). In
our prototype, we set the value of high as 0.7. We conducted
a sensitivity study by varying the value of high between 0.6
and 0.8, and observed that it did not signiÔ¨Åcantly change the
set of likely mappings output by Rosetta.
To illustrate the context factor, consider again the example
fromxIII. There, simCtxt (A,B,a,b) would be high, while
simCtxt (A,B,b,a) would be 1- high (using exact matches
forrelCount instead of ApproxMatch , to ease illustration).
Therefore, we can infer that XA
aandXB
bcould both be true,
but that XA
bandXB
aare unlikely to be true simultaneously.
We implemented Rosetta‚Äôs trace analysis and factor gener-
ation algorithms in about 1300 lines of Python code. We used
the implementation of factor graphs in the Bayes Net Toolbox
(BNT) [19] for probabilistic inference. Rosetta generates one
factor for each Boolean XM
mfor each of the three factor families
ffreq,fpos,fname . Letting Sand Tdenote the number of
unique source and target API calls observed in the trace, there
areO(S2T2) such Boolean variables (because Mandminclude
individual methods and method sequences of length two).
Likewise, Rosetta generates two fctxt factors for each pair
(M,N) and ( m,n), resulting in a total of O(S4T4) factors. We
restricted Rosetta to work with method sequences of lengths
one and two because of the rapid growth in the number of
factors. Future work could consider optimizations to prune the
number of factors, thereby allowing inference of mappings for
longer length method sequences.
C. Combining Inferences Across Traces
As discussed so far, we apply probabilistic inference to
each trace pair, which results in di erent values of Pr[ XM
m=
true] for each Boolean variable XM
m. In this step, we combine
these inferences across the entire database of trace pairs. One
way to combine these probabilities is to simply average them.
However, if we do so, we ignore the conÔ¨Ådence that we have in
our inferences from each trace pair. The more occurrences we
see of a method call or sequence Min a source trace, the more
conÔ¨Ådence we have in the values of Pr[ XM
=true]. Therefore,
we compute a weighted average of these probabilities, with the
relative count of each source call as the weight.Game (#Traces) JavaME Traces Android Traces Factor graphs
AvgLen MaxLen AvgLen MaxLen MaxNodes MaxEdges
Backgammon (1) 33,733 33,733 197,073 197,073 15,230 13,435
Blackjack (3) 690 731 272 369 11,408 10,256
Bubblebreaker (4) 858 2,033 2,366 7,377 2,844 2,358
Checkers (1) 111,790 111,790 1,014 1,014 4,923 4,191
Chess (4) 80,483 175,018 1,377 1,594 7,562 9,664
Four in a Row (3) 4,828 8,751 12,757 23,450 19,868 16,021
FreeCell (4) 9,159 15,271 330 746 128,128 96,096
Hangman (3) 3,715 4,282 3,477 3,491 10,319 11,263
Mahjongg V1 (5) 8,035 18,321 20,198 49,887 8,891 7,062
Mahjongg V2 (4) 93,739 150,206 17,241 22,025 4,703 4,806
Memory (3) 153,282 190,199 26,796 35,369 15,387 16,399
Minesweeper (4) 2,091 5,396 1,161 1,939 14,105 12,025
Roulette (5) 5,003 6,232 185 227 26,580 19,935
Rubics Cube (3) 16,521 19,343 159 255 30,660 22,995
Scrabble (4) 7,834 14,358 207 497 105,300 78,975
SimpleDice (5) 654 965 581 593 37,152 27,864
Snake (2) 33,399 63,104 11,681 20,372 1,528 1,356
Soltaire (3) 8,016 12,471 10,860 20,803 21,714 20,228
Sudoku (5) 3,897 9,968 17,347 42,567 16,306 24,317
Tetris (2) 486 916 6,116 11,991 13,105 14,520
TicTacToe (4) 154 475 183 418 3,840 4,690
Fig. 4. Statistics of traces and factor graphs for various JavaME and Android games.
The actual runtime traces of the games are Ô¨Åltered to leave only JavaME and Android
graphics API calls. We report the average and maximum lengths of these Ô¨Åltered traces.
For each game, we also show the size of the largest factor graph across all its traces.JavaME class #Methods #Top-10 (#TotValid ) #Top-1
Alert 4 3 (11) 3
Canvas 8 5 (18) 4
Command 2 2 (3) 0
Display 7 3 (5) 1
Displayable 6 4 (12) 3
Font 5 3 (9) 1
Form 4 2 (6) 1
game.GameCanvas 5 5 (13) 2
game.Layer 3 3 (7) 3
game.Sprite 4 4 (8) 1
Graphics 21 18 (61) 11
Image 4 4 (8) 2
List 1 0 (0) 0
TextField 6 0 (0) 0
Total 80 56 (161) 32
Fig. 5. Results of applying Rosetta to the traces obtained
from the games shown in Figure 4. We have shown the number
of unique JavaME methods (categorized by class) for which
Rosetta inferred at least one valid mapping in the top ten (#Top-
10), and the total number of valid mappings found in the top
ten (#TotValid). Also shown is the number of JavaME calls
for which Rosetta‚Äôs top-ranked inference was a valid mapping
(#Top-1). See also Figure 6 and Figure 7 for a more detailed
rank distribution of mappings.
Pr[XM
m=true]combined=P
Traces relCount (M)Pr[XM
m=true]P
Traces relCount (M)
We chose this approach because of its modularity. As we
collect more trace pairs and inferences from them, we can
combine them with mappings inferred from other traces in
a straightforward way using the weighted average approach.
Alternatively, we could have chosen to concatenate individual
traces (in the same order for both components of each trace
pair) to produce a ‚Äúsuper-trace,‚Äù and perform probabilistic
inference over this super-trace. However, if we do so, then
we would have to reproduce the super-trace after each new
trace pair that is collected, and execute the inference algorithm
over the ever-growing super-trace. This approach is neither
memory-e cient nor time-e cient. In contrast, our weighted
average approach provides more modular support to add
inferences from new trace pairs as they become available.
This weighted average is presented to the user as the output
from Rosetta. We present the output of Rosetta to the user in
terms of the inferred mappings for each source API call. For
each source API method or method sequence, we present a
list of mappings inferred for it, ranked in decreasing order of
the likelihood of the mapping.
V . E valuation
A. Methodology
To evaluate Rosetta, we collected a set of 21 JavaME
applications for which we could Ô¨Ånd functionally-equivalent
counterparts in the Android market. In particular, we chose
board games, for two reasons. First, many popular board
games are available for both the JavaME and Android plat-
forms. Checking the functional equivalence of two games
is as simple as playing the games and ensuring that the
moves of the game are implemented in the same way on
both versions. Second, the use of board games also eases
the task of collecting trace pairs. Moves in board games are
easy to remember and can be repeated on both game versions
to produce functionally-equivalent traces on both platforms.Note that a pair of ‚Äúfunctionally-equivalent‚Äù games on JavaME
and Android could di er in the features that they implement.
However, when tracing these games, we restrict ourselves to
the subsets of features that are common to both games.
To collect traces, we ran JavaME games using the emu-
lator distributed with the Sun Java Wireless Toolkit, version
2.5.1 [22]. For Android games, we used the emulator that is
distributed with the Android 2.1 SDK. Figure 4 shows the
games that we used, the number of trace pairs that we collected
for each game, and the sizes of the traces for the JavaME and
Android versions.
We ran Rosetta on these traces to obtain a set of likely
mappings. This set is presented to the user as a ranked list
of mappings inferred for each JavaME method (or method
sequence). To evaluate the list associated with each JavaME
method, we consulted the documentation of the JavaME and
Android graphics APIs to determine which members of the
list are valid mappings.
B. Quality of Inferred Mappings
Figure 5 presents the results of running Rosetta on the traces
that we collected. This Ô¨Ågure shows the number of distinct
JavaME methods that we observed in the trace, grouped by the
parent JavaME class to which they belong. Thus, for example,
there were four unique JavaME methods belonging to the
Alert class in our traces, namely, Alert.setCommandListener ,
Alert.setString ,Alert.setType , and Alert.setTimeout . In all,
there were 80 unique JavaME methods in our traces.
For each of these JavaME methods, we determined whether
the top ten ranked mappings reported for that method con-
tained a method (or method sequence) from the Android
API that would implement its functionality. When interpreting
Rosetta‚Äôs output for a JavaME method, we mark a method
sequence from the Android API as a valid mapping even if
it is only a subsequence of a longer method sequence that
implements the JavaME method‚Äôs functionality. We did so
because Rosetta currently only supports inference of mappings
with method sequences up to length two. As reported in
Figure 5, we found such valid mappings for 56 of the observed0 5 10 15 20 25 30 35 
1 2 3 4 5 6 7 8 9 10 Number of JavaME Methods Rank of First Valid Mapping Fig. 6. This Ô¨Ågure shows the rank distribution of the Ô¨Årst valid mapping
found for each JavaME method. In all, for each of 56 JavaME methods, a
valid Android mapping appeared within the top ten results reported for that
method. For 32 JavaME methods, the top-ranked mapping was a valid one.
0 5 10 15 20 25 30 35 
1 2 3 4 5 6 7 8 9 10 Number of Valid Mappings Found Rank of Valid Mappings Fig. 7. This Ô¨Ågure shows the rank distribution of all valid mappings
found by Rosetta. For each rank, it shows the number of valid mappings
that appear at that rank across all JavaME API methods observed in our
traces. In all, we found 161 valid mappings ranking in the top ten.
JavaME methods (70%). Figure 6 depicts in more detail the
rank distribution of the Ô¨Årst valid mapping found for each of
these 56 JavaME methods. The top-ranked mapping for 32 of
these methods (40%) was a valid one.
Recall that a JavaME method can possibly be implemented
in multiple ways using the Android API. Thus, the ranked list
associated with that method could possibly contain multiple
valid mappings. Rosetta‚Äôs output contained a total of 161 valid
mappings within the top ten results of the 56 JavaME methods.
Figure 7 depicts the rank distribution of all the valid mappings
found by Rosetta. Below, we illustrate a few examples of
mappings inferred by Rosetta:
(1) The Graphics.clipRect() method in JavaME intersects
the current clip with a speciÔ¨Åed rectangle. Rosetta correctly
inferred the Android method Canvas.clipRect() as its top-
ranked mapping.
(2) In JavaME Graphics.drawChar() draws a speciÔ¨Åed character
using the current font and color. In Rosetta‚Äôs output, the
sequence Paint.setColor();Canvas.drawText() , which Ô¨Årst sets
the color and then draws text, was the second-ranked mapping
forGraphics.drawChar() .
(3) The Graphics.drawRect() JavaME method was mapped
to the Android methods Canvas.drawRect() (rank 1) and
Canvas.drawLines() (rank 7), all of which can draw rectangles.
C. Impact of Individual Factors
Variant %Valid
ffreq62.2%
fname 44.0%
fpos 44.0%
ffreqfctxt 95.6%
ffreqfname 69.8%
ffreqfpos 77.0%
Fig. 8. Studying the impact of
various combinations of factors.Rosetta‚Äôs output is the result
of combining all the four factors
discussed inxIV-B. We also eval-
uated the extent to which each
of these factors contributes to the
output. To do so, we constructed
factor graphs (using the traces in
Figure 4) individually with ffreq,
fpos, and fname . Because fctxt
correlates two mappings, we did not consider it in isolation.
We also considered a few combinations of factors, to study
how the mappings change as factors are added.
We inferred mappings with these factor graphs, and com-
pared the resulting mappings with those obtained by Rosetta.For each factor graph variant in Figure 8, we report the fraction
of Rosetta‚Äôs 161 valid mappings that correspond to a valid
mapping inferred by the variant. As we did with Rosetta, we
only report valid mappings that rank in the top ten in the
output of these factor graph variants. As Figure 8 shows, the
addition of more factors generally improves the accuracy of
the reported mappings. This is because the addition of more
factors incorporates more trace features into the probabilistic
inference algorithm, thereby removing spurious mappings.
D. Runtime Performance
We ran Rosetta on a PC with an Intel Core2 Duo CPU,
running at 2.80Ghz and 4GB memory, running Ubuntu 10.04.
For each of the traces reported in Figure 4, Rosetta took
between 2‚Äì55 minutes to analyze traces and output mappings.
On average, approximately 80% of this time was consumed
by the algorithms in xIV-B which produce factor graphs, and
20% was consumed by the factor graph solver. The solver
consumed 2GB memory for the largest of our factor graphs.
As discussed, Rosetta currently supports inference on
method sequences of length up to two. We also conÔ¨Ågured
Rosetta to work with longer method sequences, which would
produce larger factor graphs. However, the factor graph solver
ran out of memory in these cases.
E. Experiments with MicroEmulator
There have been some recent e orts [1], [5], [18] to allow
the execution of legacy JavaME applications on the Android
platform. MicroEmulator [18] is one such open-source tool
that works on JavaME applications. It rewrites JavaME API
calls in the input application with the corresponding Android
API calls. The implementation of MicroEmulator therefore
implicitly deÔ¨Ånes a mapping between the JavaME and Android
graphics APIs. However, MicroEmulator does not translate
the entire JavaME graphics API, and only allows JavaME
applications with rudimentary GUIs to execute on Android
devices. Nevertheless, the mappings that it does implement
provide a basis to evaluate Rosetta.
We considered a subset of Ô¨Åve JavaME games from
Figure 4 that MicroEmulator could successfully translate
(Chess, Minesweeper, Snake, Sudoku, TicTacToe), executedthem, and collected the corresponding traces of JavaME API
calls. We then repeated the same set of experiments on
the MicroEmulator-converted versions of these applications
and collected the corresponding traces of Android API calls,
and fed these trace pairs to Rosetta. In our evaluation, we
determined whether the API mappings inferred by Rosetta
contained the mappings implemented in MicroEmulator.
Across the JavaME traces for these Ô¨Åve games, we ob-
served 18 distinct JavaME graphics API methods translated
by MicroEmulator. In Rosetta‚Äôs output, we found at least one
valid mapping for 17 of these JavaME methods within the
top ten ranked results for the corresponding JavaME method.
Rosetta only failed to discover an API mapping for the JavaME
method Alert.setString() . Out of 17 JavaME methods with
valid mappings, 8 methods had valid top-ranked mappings to
their Android counterparts.
F . Threats to Validity
There are a number of threats to the validity of our results,
which we discuss now. First, although we attempted to Ô¨Ånd
JavaME and Android games that are functionally equivalent,
dierences do exist in their implementations. This is because
JavaME is an older mobile platform that does not support
as rich an API as Android; many Android calls do not even
have equivalents in JavaME. Together with randomness that is
inherent in certain board games ( xIV-A), this could result in
traces which contain Android calls implementing functionality
unseen in the source application. They may mislead the
attributes used by our inference algorithm ( e.g., frequency
of calls), leading to both invalid mappings as well as valid
mappings being suppressed in the output.
Second, there is no ‚Äúground truth‚Äù of JavaME to Android
mappings available to evaluate Rosetta‚Äôs output (the mappings
in MicroEmulator aside), as a result of which we are unable
to report standard metrics, such as precision and recall. We
interpreted Rosetta‚Äôs results by consulting API documentation.
Such natural language documentation is inherently ambigious
and prone to misinterpretation. We mitigated this threat by
having two authors independently cross-validate the results.
Finally, a threat to external validity, i.e.,the extent to which
our results can be generalized, comes from the fact that we
only inferred mappings for a pair of graphics APIs using
board games. It is unclear how many mappings we would
have inferred using other graphical applications ( e.g., oce
applications) or for other families of APIs.
VI. R elated Work
An upcoming survey article by Robillard et al. [26,x6]
provides a good overview of prior work on mining API
mappings. Among these, the work most directly related to
ours is the MAM project [34]. MAM‚Äôs goal is the same as
Rosetta‚Äôs, i.e., to mine software repositories to infer how a
source API maps to a target API. The MAM prototype was
targeted towards Java as the source API and C# as the target
API. Despite sharing the same goal, MAM and Rosetta di er
signiÔ¨Åcantly in the approaches that they use, each with itsadvantages and drawbacks. To mine API mappings between
a source and a target API, MAM relies on the existence of
software packages that have been ported manually from the
source to the target platform. For each such software package,
MAM then uses static analysis and name similarity to ‚Äúalign‚Äù
methods and classes in the source platform implementation
with those of the target platform implementation. Aligned
methods are assumed to implement the same functionality.
MAM‚Äôs use of static analysis allows it to infer a large
number of API mappings (about 25,800 mappings between
Java and C#). It also allows MAM to infer likely mappings
between arguments to methods in the source and target APIs,
which Rosetta does not currently do. However, unlike Rosetta,
MAM requires that the same source application be available on
the source and target platforms. MAM‚Äôs approach of aligning
classes and methods across two implementations of a software
package does not allow the inference of likely API mappings
if there are similar, but independently-developed applications
for the source and target platforms. MAM‚Äôs approach is also
limited in that it uses name similarity as the only heuristic to
bootstrap its API mapping algorithm. In contrast, Rosetta uses
a number of attributes combined together as factors, and can
easily be extended to accommodate new similarity attributes
as they are designed. Most importantly, while MAM uses a
purely syntactic approach to discover likely API mappings,
Rosetta‚Äôs approach uses similarities in application behavior .
Nita and Notkin [20] develop techniques to allow developers
to adapt their applications to alternative APIs. They provide a
way for developers to specify a mapping between a source and
a target API, following which a source to source transforma-
tion automatically completes the transformations necessary to
adapt the application to the target API. Rosetta can potentially
complement this work by inferring likely mappings.
Androider [28] is a tool to reverse-engineer application
GUIs. Androider uses aspect-oriented programming tech-
niques to extract a platform-independent GUI model, which
can then be used to port GUIs across di erent platforms. While
Androider provides a GUI for a target platform by analyzing
GUIs of a source platform at runtime, Rosetta instead infers
API mappings, and is not restricted to GUI-related APIs.
Bartolomei et al. [2] analyzed wrappers between two Java
GUI APIs and extracted common design patterns used by
wrapper developers. They focused on mapping object types
and identiÔ¨Åed the challenges faced by wrapper developers.
Method mappings given by Rosetta can possibly be used along
with their design patterns to ease the job of writing wrappers.
A number of prior projects provide tool support to assist
programmers working with large, evolving APIs ( e.g., [6], [8],
[17], [27], [31], [33], [35]). The programmer‚Äôs time is often
spent in determining which API method to use to accomplish
a particular task. These projects use myriad techniques to
develop programming assistants that ease the task of working
with complex APIs. Rosetta is complementary to these e orts,
in that it works with cross-platform APIs .
A related line of research is on resources for API learning
(e.g., [3], [7], [11], [23], [29]). These projects attempt to easethe task of a programmer by synthesizing API usage examples,
evolution of API usage, and extracting knowledge from API
documentation. Again, most of these techniques work on APIs
on a single platform. Rosetta‚Äôs cross-platform approach can
possibly be used in conjunction with these techniques to
facilitate cross-platform API learning.
Finally, Rosetta‚Äôs probabilistic inference approach was in-
spired by other uses of factor graphs in the software engi-
neering literature. Merlin [15] uses factor graphs to classify
methods in Web applications as sources, sinks and sanitizers.
Such speciÔ¨Åcations are useful for veriÔ¨Åcation tools, which
attempt to determine whether there is a path from a source to
a sink that does not traverse through a sanitizer. To infer such
speciÔ¨Åcations, Merlin casts a number of heuristics as factors,
and uses belief propagation. Kremenek et al. [12] also made
similar use of factor graphs to infer speciÔ¨Åcations of allocator
and deallocator methods in systems code. Factor graphs are
just one approach to mining speciÔ¨Åcations; a number of prior
software engineering projects have considered other mining
techniques e.g., [9], [14], [16]. Future work could consider
the use of such techniques in Rosetta as well.
VII. S ummary and Future Work
We presented a generic approach to infer mappings between
the APIs of a source and target platform. Our Rosetta prototype
used this approach to infer likely mappings between the
JavaME and Android graphics APIs. While we are encouraged
thus far by the success of Rosetta, future work could extend
it in a number of ways to overcome its current shortcomings.
(1)Mapping method arguments. Rosetta could be extended to
also discover mappings between method call arguments. To do
so, the tracing infrastructure must be extended to log method
arguments and their types, and new factors and inference
techniques must be developed to map arguments.
(2)Mapping longer method sequences. Although our frame-
work supports the inference of mappings between method se-
quences of arbitrary length, we restricted the Rosetta prototype
to sequences of length two to limit the number of factors
produced by our algorithms in xIV-B. Future work could
investigate optimizations to prune the number of factors, so
that Rosetta can scale to longer method sequences.
(3)Hybrid approaches. Rosetta‚Äôs use of runtime techniques
fundamentally limits the mappings inferred to only those
source and target API methods that appear in the trace. It
may be possible to increase coverage using a hybrid technique
that combines static analysis ( e.g., MAM [34]) with Rosetta‚Äôs
trace-based approach.
(4)More platforms. Finally, the Rosetta prototype can be
extended to work on larger subsets of the JavaME and Android
APIs, and also to infer mappings between other API pairs
(e.g., iOS and Android).
Acknowledgments
We thank Shakeel Butt, Liviu Iftode, Pratyusa Manadhata,
and Tina Eliassi-Rad for their contributions to the project. This
work was funded by NSF grants 0931992 and 1117711.References
[1] Assembla. J2ME Android bridge. http: //www.assembla.com /spaces /
j2ab/wiki.
[2] T.T. Bartolomei, K. Czarnecki, and R. Laandmmel. Swing to SWT and
back: Patterns for API migration by wrapping. In ICSM , 2010.
[3] R. Buse and W. Weimer. Synthesizing API usage examples. In ICSE ,
2012.
[4] OW2 Consortium. ASM toolkit. http: //asm.ow2.org.
[5] Netmite Corporation. App runner. http: //www.netmite.com /android /.
[6] B. Dagenais and M. P. Robillard. Recommending adaptive changes for
framework evolution. ACM TOSEM , 20(4), 2011.
[7] B. Dagenais and M. P. Robillard. Recovering traceability links between
an API and its learning resources. In ICSE , 2012.
[8] D. Dig, C. Comertoglu, D. Marinov, R. Johnson, and D. Thomas. Au-
tomated detection of refactorings in evolving components. In ECOOP ,
2006.
[9] D. Engler et al. Bugs as deviant behavior: a general approach to inferring
errors in systems code. In SOSP , 2001.
[10] Google. Android API reference. http: //developer.android.com /reference /
packages.html.
[11] S. Hens, M. Monperrus, and M. Mezini. Semi-automatically extracting
FAQs to improve accessibility of software development knowledge. In
ICSE , 2012.
[12] T. Kremenek, P. Twohey, G. Back, A. Ng, and D. Engler. From
uncertainty to belief: Inferring the speciÔ¨Åcation within. In OSDI , 2006.
[13] F.R. Kschischang, B.J. Frey, and H.-A. Loeliger. Factor graphs and the
sum-product algorithm. IEEE Trans. Inf. Theory , 47(2), 2001.
[14] Z. Li and Y . Zhou. PR-Miner: Automatically extracting implicit
programming rules and detecting violations in large software code. In
FSE, 2005.
[15] B. Livshits, A. Nori, S. Rajamani, and A. Banerjee. Merlin: Inferring
speciÔ¨Åcations for explicit information Ô¨Çow problems. In PLDI , 2009.
[16] B. Livshits and T. Zimmermann. Dynamine: Finding common error
patterns by mining software revision histories. In FSE, 2005.
[17] D. Mandelin, L. Xu, R. Bodik, and D. Kimelman. Jungloid mining:
Helping to navigate the API jungle. In PLDI , 2005.
[18] Microemulator. http: //www.microemu.org /.
[19] K. Murphy. Bayes Net toolbox for Matlab, October 2007. http: //code.
google.com /p/bnt/.
[20] M. Nita and D. Notkin. Using twinning to adapt programs to alternative
APIs. In ICSE , May 2010.
[21] Oracle. Java ME API reference. http: //docs.oracle.com /javame /conÔ¨Åg /
cldc/ref-impl /midp2.0 /jsr118 /index.html.
[22] Oracle. Sun Java wireless toolkit for CLDC, 2.5.1. http: //java.sun.com /
products /sjwtoolkit /download-2 51.html.
[23] R. Pandita, X. Xiao, H. Zhong, T. Xie, S. Oney, and A. M. Paradkar.
Inferring method speciÔ¨Åcations from natural language API descriptions.
InICSE , 2012.
[24] python-levenshtein-0.10.2. pypi.python.org /pypi/python-Levenshtein.
[25] Qt API mapping for iOS developers. http: //www.developer.nokia.com /
Develop /Porting /API Mapping.
[26] M. P. Robillard, E. Bodden, D. Kawrykow, M. Mezini, and T. Ratchford.
Automated API property inference techniques. IEEE TSE . To appear.
[27] T. Schafer, J. Jonas, and M. Mezini. Mining framework usage changes
from instantiation code. In ICSE , 2008.
[28] E. Shah and E. Tilevich. Reverse-engineering user interfaces to facilitate
porting to and across mobile devices and platforms. In Workshop on
Next-generation Applications of Smartphones , 2011.
[29] G. Uddin, B. Dagenais, and M. P. Robillard. Temporal analysis of API
usage concepts. In ICSE , 2012.
[30] Windows phone interoperability: Windows phone API mapping. http:
//windowsphone.interoperabilitybridges.com /porting.
[31] Z. Xing and E. Stroulia. API-evolution support with Di CatchUp. IEEE
TSE, 33(12), 2007.
[32] J. S. Yedidia, W. T. Freeman, and Y . Weiss. Understanding belief
propagation and its generalizations. Exploring artiÔ¨Åcial intelligence in
the new millennium , 2003.
[33] C. Zhang, J. Yang, Y . Zhang, J. Fan, X. Zhang, and J. Zhao. Automatic
parameter recommendation for practical API usage. In ICSE , 2012.
[34] H. Zhong, S. Thummalapenta, T. Xie, L. Zhang, and Q. Wang. Mining
API mapping for language migration. In ICSE , 2010.
[35] H. Zhong, T. Xie, L. Zhang, J. Pei, and H. Mei. Mapo: Mining and
recommending API usage patterns. In ECOOP , 2009.