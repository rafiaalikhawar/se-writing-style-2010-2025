Sound Empirical Evidence in Software Testing
Gordon Fraser
Saarland University
Saarbr ¨ucken, Germany
fraser@cs.uni-saarland.deAndrea Arcuri
Certus Software V&V Center, Simula Research Laboratory
P .O. Box 134, 1325 Lysaker, Norway
arcuri@simula.no
Abstract —Several promising techniques have been proposed
to automate different tasks in software testing, such as test da ta
generation for object-oriented software. However, reported
studies in the literature only show the feasibility of the proposed
techniques, because the choice of the employed artifacts in
the case studies (e.g., software applications) is usually done
in a non-systematic way. The chosen case study might be
biased, and so it might not be a valid representative of the
addressed type of software (e.g., internet applications and
embedded systems). The common trend seems to be to accept
this fact and get over it by simply discussing it in a threats
to validity section. In this paper, we evaluate search-based
software testing (in particular the EVOSUITE tool) when applied
to test data generation for open source projects. To achieve
sound empirical results, we randomly selected 100 Java projects
from SourceForge, which is the most popular open source
repository (more than 300,000 projects with more than two
million registered users). The resulting case study not only is
very large (8,784 public classes for a total of 291,639 bytecode
level branches), but more importantly it is statistically sound
and representative for open source projects. Results show th at
while high coverage on commonly used types of classes is
achievable, in practice environmental dependencies prohibit
such high coverage, which clearly points out essential future
research directions. To support this future research, our SF100
case study can serve as a much needed corpus of classes for
test generation.
Keywords -test case generation; unit testing; search-based
software engineering; class corpus; security exception; envi-
ronment
I. I NTRODUCTION
Software testing is an essential yet expensive activity in
software development, therefore much research effort has
been put into the question of how to automate it as much as
possible. In this paper, we focus on test data generation for
code coverage, in particular branch coverage in the context
of object-oriented software. The simplest automated testi ng
technique in this context is perhaps random testing [6],
but during the years different sophisticated techniques ha ve
also been proposed. At a high level, the current state of
the art can roughly be divided into three main groups:
variants of random testing (e.g., Randoop [30]), dynamic
symbolic execution (e.g., CUTE [35]) and search-based
software testing (e.g., [27]). A recent trend also goes towards
combining the individual techniques (e.g., [25]).For “simple” techniques such as random testing, it is
possible to provide rigorous answers based on theoreti-
cal analyses (e.g., see [6]). For more complex techniques
where mathematical proofs become infeasible or too hard,
researchers have to rely on empirical analyses. There are
several challenges when carrying out empirical studies,
among which there is the choice of the case study. If a
technique works well in the lab on a speciﬁc case study,
will it also work well in the real-world when it is applied
by practitioners on their software? It might be that a novel
technique works well in the lab just because the case study
is too simple or small, but then it might fail on real-world
instances. Even if real-world instances are used in a case
study, the proposed technique might be too speciﬁc/biased
toward those instances, and still fail when applied on new
instances by practitioners.
How are case studies chosen in the literature? In most
cases, this choice is not made in a systematic way, i.e.,
researchers choose software artifacts without providing a ny
speciﬁc and unbiased motivation. Notice that, in many
software testing contexts, this is the only viable option. T his
is a typical example in the context of testing techniques
targeted for industrial systems. Obtaining real data from
industry is a very difﬁcult and time consuming activity,
and so case studies tend to be either “small” or biased
toward a speciﬁc kind of software (e.g., software in the
automotive industry [47], seismic acquisition systems [5] ,
video-conference and safety-critical control systems [21 ]).
The case of test data generation for open source software
is very different from industrial software. The world wide
web hosts a huge amount of open source projects, and there
are specialized repositories that are freely accessible (e .g.,
SourceForge1or Google Code2). A researcher can easily
download open source software and use those programs as
case study. But how to choose them? For example, it is
quite common that empirical studies only involve container
classes (e.g., lists and vectors, see [4], [44]). It is quite hard
to generalize the conclusions from such empirical studies
to any other kind of software. Even when case studies are
large andvariegated (e.g., several hundreds of classes from
1http://sourceforge.net/, accessed September 2011.
2http://code.google.com/, accessed September 2011.978-1-4673-1067-3/12/$31.00 c2012 IEEE ICSE 2012, Zurich, Switzerland 178different kinds of software [17], [30]), still a manual choi ce
of software artifacts might introduce bias in the results. F or
example, if a proposed testing technique does not support
ﬁle system I/O, then that kind of software might have been
excluded from the case study, although programs with I/O
may be very common in practice.
To the best of our knowledge, we are not aware of any
empirical study in the literature in which this kind of threats
to external validity has been addressed. To cope with this
problem, in this paper we present what is perhaps the ﬁrst
empirical study where the choice of the case study is statis-
tically sound, as far as open source software is concerned.
We randomly selected 100 Java projects from SourceForge,
which is the most popular open source repository. Currently ,
it hosts more than 300,000 projects in several programming
languages and it has more than two million registered users.
The resulting case study is very large, consisting of 8784
classes for a total of 291,639 bytecode level branches.
Because the case study is randomly selected from an open
source repository, the proportions of kinds of software (e. g.,
numerical applications and video games) are statistically
representative for open source software (a more precise
deﬁnition will be presented later in the paper).
On this large case study we applied E VOSUITE [17], [18],
which is a search-based test data generation tool for object -
oriented software written in Java. E VOSUITE is an advanced
research prototype that can efﬁciently handle all the diffe rent
kinds of programming structures in Java (e.g., it has speciﬁ c
operators to handle string objects and arrays). Furthermor e,
it uses a sandbox where potentially unsafe operations (e.g.,
class methods that take as input the name of a ﬁle to delete)
are caught and properly taken care of. This feature was
essential for the chosen case study, as 100 real-world open
source programs likely have at least one unsafe operation.
The results of our empirical analysis show that, as demon-
strated by previous empirical studies, test generation can
indeed achieve high coverage – but only on a certain type of
classes. In practice, dependencies on the environment inhi bit
high coverage, and thus clearly point out directions into
which future research needs to investigate more.
In many research disciplines, common benchmarks allow
tool comparisons and exploration of novel ideas – in the
ﬁeld of software testing there is no such common bench-
mark, despite recent community efforts to provide one. Our
selection of 100 SourceForge projects (which we provide to
the research community) can serve as a corpus of classes
for the ﬁeld of test generation for object-oriented softwar e.
We call this corpus SF100.
The paper is organized as follows. Section II surveys the
literature on test generation for object-oriented softwar e to
gain insights into the current practice in performing exper i-
ments. Section III then describes the ﬁrst sound experiment
in software testing, which allows us to draw conclusions
about where the actual problems in this domain are. BasedTable I
EVALUATION SETTINGS IN THE LITERATURE . THE CONTAINER COLUMN
DENOTES HOW MANY OF THE CLASSES ARE CONTAINER DATA
STRUCTURES ,IN THOSE CASES WHERE THIS WAS DETERMINABLE . THE
SOURCE COLUMN DESCRIBES WHETHER CASE STUDIES WERE CHOSEN
FROM AVAILABLE OPEN SOURCE PROJECTS (OS), INDUSTRY PROJECTS ,
TAKEN FROM THE LITERATURE ,OR CREATED BY THE AUTHORS .
Tool Reference Projects Classes Container Source
Artoo [11] 1 8 8 Open Source
AutoTest [12] 1 27 17 Open Source
Check’n’Crash [14] 2 ? 1 OS / Literature
Covana [49] 2 388 - Open Source
DiffGen [39] 1 21 8 Literature
DSDCrasher [15] 2 24 - Open Source
DyGen [40] 10 5,757 - Industrial
Eclat [29] 7 631 16 OS/Lit./Constr.
eCrash [33] 1 2 2 Open Source
eCrash [32] 1 2 2 Open Source
eToc [43] 1 6 6 Open Source
EvaCon [22] 1 6 6 Open Source
EvoSuite [17] 6 727 - OS + Industrial
Jartege [28] 1 1 - Constructed
JAUT [10] 3 7 - Constructed
JCrasher [13] 1 8 2 Literature
JCute [34] 1 6 6 Open Source
jFuzz [24] 1 ? - Open Source
JPF [45] 1 1 1 Open Source
JPF [46] 1 4 4 Constructed
JTest+Daikon [52] 1 9 9 Constructed / Lit.
JWalk [37] 6 13 - Constructed
Korat [9] 1 6 6 Literature
MSeqGen [41] 2 450 - Open Source
MuTest [19] 10 952 - Open Source
NightHawk [3] 2 20 20 Literature
NightHawk [4] 1 34 34 Open Source
OCAT [23] 3 529 - Open Source
Palus [54] 6 4,664 - OS + Industrial
Pex [42] 2 8 - Constructed
PexMutator [53] 1 5 1 Open Source
Randoop [30] 14 4,576 - OS / Industrial
Rostra [50] 1 11 9 Constructed / Lit.
RuteJ [2] 1 1 1 Open Source
Symclat [16] 5 16 12 Constructed / Lit.
Symstra [51] 1 7 7 Literature
Symbolic JPF [31] 1 1 - Industrial
Symbolic JPF [38] 6 6 4 Industrial/OS
TACO [20] 6 6 6 OS/Lit.
Testera [26] 4 4 2 Open Source
TestFul [8] 4 15 12 OS + Literature
N/A [7] 1 7 7 Open Source
N/A [48] 2 4 4 Open Source
N/A [1] 2 2 1 Open Source
on these results, Section IV discusses the threats of choosi ng
an unsuitable case study, and Section V concludes the paper.
II. S OFTWARE ENGINEERING EXPERIMENTATION
To get a better picture of the current practice in evaluation s
in software engineering research, we surveyed the literatu re
on test generation for object-oriented software. This is no t
meant to be an exhaustive and systematic survey, but rather
a representative sample of the literature to motivate the wo rk
presented in this paper. Table I lists the inspected papers a nd
tools, together with statistics on their experiments.179We explicitly list how many out of the considered classes
are container classes, if this was clearly speciﬁed. This is
of interest as container classes represent a particular typ e
of classes that avoids many problems such as environment
interaction, and recent studies have shown that even “sim-
ple” random testing can achieve high coverage on such
classes [36]. Interestingly, 17 papers exclusively focus o n
container classes, and many other papers include container
classes.
We also list how the evaluation classes were selected;
interestingly, not a single paper out of those considered
justiﬁes why this particular set of classes was selected, an d
how this selection was done. In principle, this could mean
that the presented set represents the entire set of classes
on which the particular tool was ever tried on, but it could
also mean that it is a subset on which the tool performs
particularly well. An exception is industrial code, where
often there is no choice, because the case study is selected
by an industrial partner.
Out of 44 evaluations we considered in our literature
survey, 29 selected their case study programs from open
source programs, while only six evaluations included indus -
trial code. This is to be expected, as it is difﬁcult to get
access to industrial code, and even if one gets access it is
not always easy to publish results achieved on this code due
to privacy and conﬁdentiality issues. We also include the
.NET libraries as industrial code here, although the byteco de
is available freely. On the other hand, 17 evaluations used
artiﬁcially created examples, either by generating them or
by reusing them from the literature.
Xiao et al. [49] evaluated problems in structural test
generation, concluding that the main problems in structura l
testing are related to object creation and external method
calls. In related work, Jaygarl et al. [23] performed an
experiment on open source libraries to determine the main
reasons why branches were not covered by random testing.
In their experiment, the main reason was also the problem of
generating complex objects, followed by string comparison s
and container object access. Out of the analyzed branches,
only 3.1% were not covered because of environmental
dependencies that were not satisﬁed. However, the results
that we will present later in this paper lead to different
conclusions.
III. A S TATISTICALLY SOUND EXPERIMENT
Section II illustrated that the choice of case studies in
software engineering experiments is often unclear, result ing
in a threat to the external validity of these experiments. In
this section, we describe a sound experiment on software
testing which does not suffer from this threat to external
validity. Given these data, we perform a reality check on the
research ﬁeld of test generation for object-oriented softw are:
How good is the state of the art really, and what are the real
problems?A. Objectives
The performance of test generation tools is commonly
evaluated in terms of the achieved code coverage. High code
coverage by itself is not sufﬁcient in order to ﬁnd defects
as there are further major obstacles, most prominently the
oracle problem: Except for special kinds of defects, such
as program crashes or undeclared exceptions, the tester has
to provide an oracle that decides whether a given test run
detected an error or not. This oracle could be anything
from a formal speciﬁcation, test assertions, up to manual
assessment. The oracle problem entails further problems; f or
example, in order to be able to come up with a test assertion
a generated test case needs to be easily understandable and
preferably short. However, in all cases a prerequisite to th e
oracle problem is to ﬁnd an input that takes the program
to a desired state. Therefore, in our experiment we compare
the results in terms of the achieved branch coverage.
In Section II we saw that many case studies focus on
container classes, which are often chosen simply because
they are “nice” to test: There is no I/O, no interaction with
the environment, no multi-threading, etc. In practice, one
often uses existing libraries of container classes but want s to
apply testing tools to other types of classes, which may very
well try to interact with their environment. Test generatio n
for such code is unsafe as the tested code might interact
with its environment in undesired ways, for example by
creating or deleting ﬁles. To evaluate to what extent this is
the case, we want to ﬁnd out how many unsafe operations
are attempted during test generation. This results in the
following two research questions:
RQ1: What is the probability distribution of achievable
branch coverage on open source software?
RQ2: How often do classes execute unsafe operations?
B. The EvoSuite Tool for Search-Based Test Generation
As context of our experiment we chose the E VO-
SUITE [17], [18] tool, which automatically generates test
suites for Java classes, targeting branch coverage. E VO-
SUITE uses an evolutionary approach to derive these test
suites: A genetic algorithm evolves candidate individuals
(chromosomes) using operators inspired by natural evoluti on
(e.g., selection, crossover and mutation), such that itera tively
better solutions with respect to the optimization target (e .g.
branch coverage) are produced.
Chromosomes in E VOSUITE are test suites, and each test
suite consists of a variable number of test cases, which are
sequences of method calls. Crossover produces offspring te st
suites by exchanging test cases from two parent individuals ,
and mutation either adds new randomly generated test cases,
or mutates individual test cases. Mutation of test cases may
add, remove, or change the method calls in a sequence.
Fitness is calculated with respect to branch coverage, usin g
a calculation based on the well-established branch distanc e180measurement [27]. The branch distance estimates how close
a branch is to evaluating to true or false for a particular run .
For each branch we consider the minimum branch distance
over all test cases of a test suite. The overall ﬁtness of a
test suite is the sum of these minimal values, such that an
individual with 100% branch coverage has ﬁtness 0.
Through its use of method sequences, E VOSUITE can
handle any datatype, and can be applied out of the box to
any Java program. It only requires the bytecode to produce
test suites, which it outputs in JUnit format.
Calculating the ﬁtness value requires executing code, and
if this code interacts with its environment then unexpected or
undesirable side-effects might occur. For example, the cod e
might access the ﬁlesystem or network, causing damage to
data or affecting other users on the network. To overcome
this problem, E VOSUITE provides its own custom security
manager : The Java language is designed with a permission
system, such that potentially undesired actions ﬁrst ask
a security manager for permission. E VOSUITE uses its
own security manager that can be activated to restrict test
execution.
When running test generation on unknown code, using
a sandbox in which permissions are restricted is essential.
We therefore enabled the custom security manager for our
experiment. With respect to RQ2, we are interested in
ﬁnding out to what extent these unsafe operations are a
problem for test generation. Consequently, we kept track of
which kinds of permissions were requested from the code
under test. However, no permissions were granted, except
for three permissions which we determined necessary to run
most code in the ﬁrst place in our earlier experiments [17]:
(1) Reading from properties, (2) Loading classes, and (3)
Reﬂection. Except for these permissions, all other permis-
sions were denied. This might be overly strict, and indeed
ﬁnding a suitable set of permissions for test generation is a
future research question.
In our previous experiments [17], we applied E VOSUITE
with a timeout of 10 minutes per class. As we apply the
technique to a larger set of classes in this experiment, and a
developer might not be willing to wait for 10 minutes to see
a result, we chose a timeout of two minutes per class, after
which the search always ended, except if 100% coverage
was already achieved earlier. For all other settings, we use d
EVOSUITE with its default parameter settings.
C. Case Study Selection
To select an unbiased sample of Java software, we con-
sider the SourceForge open source development platform.
SourceForge provides infrastructure for open source devel -
opers, ranging from source code repositories, webspace,
discussion forums, to bug tracking systems. There are other
similar services on the web, for example Google Code,
GitHub, or Assembla. We chose SourceForge because it isTable II
DETAILS OF THE SF100 CASE STUDY . FOR EACH PROJECT ,WE REPORT
HOW MANY CLASSES IT IS COMPOSED OF ,AND THE TOTAL NUMBER OF
BYTECODE BRANCHES .
Name # Classes # Branches Name # Classes # Branches
ifx-framework 2189 93307 mygrid 35 1266
jcvi-javacommon 565 7347 jigen 35 631
caloriecount 524 12064 shop 32 1035
openjms 486 11744 dsachat 31 951
summa 428 13711 jaw-br 29 811
lilith 311 17063 gangup 29 991
corina 310 10731 inspirento 26 571
heal 186 6070 rif 25 488
at-robots2-j 174 2201 ext4j 23 525
lhamacaw 168 4973 ﬁxsuite 22 519
xbus 168 4422 xisemele 21 343
jiggler 140 6325 biblestudy 21 630
dom4j 136 5702 imsmart 21 183
jnfe 128 2428 jgaap 19 222
hft-bomberman 125 1956 templateit 19 692
jiprof 101 5222 javaviewcontrol 18 3071
wheelwebtool 100 7246 tullibee 17 1185
sbmlreader2 85 4841 httpanalyzer 17 499
jdbacl 84 5188 asphodel 16 137
db-everywhere 84 1786 noen 16 138
quickserver 78 3648 diebierse 15 352
beanbin 75 986 cards24 14 323
echodep 74 3606 gsftp 14 614
jsecurity 72 998 jni-inchi 12 178
objectexplorer 70 1516 io-project 12 129
jhandballmoves 68 1507 fps370 12 325
schemaspy 67 3493 battlecry 11 705
twfbplayer 61 1178 celwars2009 11 964
nutzenportfolio 59 1835 ipcalculator 10 644
openhre 58 1468 sugar 9 135
apbsmem 52 1641 dvd-homevideo 9 332
geo-google 52 1344 bpmail 8 108
petsoar 52 523 byuic 8 703
lotus 52 228 jclo 8 143
follow 52 814 omjstate 8 80
jwbf 50 1371 saxpath 8 1064
lagoon 49 1140 sfmis 8 90
gfarcegestionfa 46 797 falselight 8 40
a4j 45 952 difﬁ 8 130
dash-framework 45 425 nekomud 7 57
javathena 44 2412 biff 6 825
lavalamp 43 306 classviewer 6 524
jtailgui 42 430 gae-app-manager 6 88
javabullboard 42 2197 resources4j 6 381
ﬁm1 41 1194 dcparseargs 6 100
water-simulator 41 1074 trans-locator 5 74
jopenchart 38 693 shp2kml 4 51
newzgrabber 37 1354 jipa 2 34
feudalismgame 36 1454 templatedetails 2 125
jmca 35 2521 greencow 1 1
the dominant site of this type, having more than 300,000
registered projects at the time of our experiment.
We based our selection on the dataset of all projects
tagged as being written in the Java programming language.
In total there were 48,109 such projects at the time of
our experiment, and applying E VOSUITE to all of them
would not be possible in reasonable time. Therefore, we
sampled the dataset, picking one randomly chosen project
out of this data set at a time. For each chosen project we
downloaded the most recent sources from the corresponding
source repository and tried to build the program. It turned
out that many projects on SourceForge have no ﬁles (i.e.,
they were created but then no ﬁles were ever added). A
small number of projects was also misclassiﬁed by their
developers as Java project although in fact they were writte n
in a different programming language. Finally, we did not
succeed in compiling all of the projects, sometimes because
they were too old and relying on particular Java APIs that are
no longer available. Where available, we downloaded binary
releases for projects we could not build, as E VOSUITE does
not actually require the source code for test generation. In181total, we therefore had to consider 321 projects until we had
a set of 100 projects in binary format.3
We call this case study SF100 corpus of classes. Table II
shows the number of classes and branches per each of the
100 projects, whereas Table III presents the summarized
statistics (e.g, mean and standard deviation). These numbe rs
were derived using E VOSUITE , which only lists top-level
classes; E VOSUITE attempts to cover member or anony-
mous classes together with their parent classes. Furthermo re,
EVOSUITE might exclude certain classes it determines that
it cannot handle, such as private classes. In total, there ar e
8784 classes and 291,639 bytecode branches reported by
EVOSUITE in this case study. Both in terms of the number
of classes and branches, what stands out is the large variati on
in the data; e.g., the number of classes in a project ranges
from 1 to 2189, and the number of branches in a class ranges
from 0 to 2480. Furthermore, these distributions present
infrequent extreme deviations, which is represented by hig h
kurtosis values, and are highly skewed (skewness represent s
the asymmetry of a distribution between its left and right
probability tails). Notice that, in the normal distribution ,
skewness is equal to zero whereas kurtosis is equal to three,
regardless of the variance.
Tables II and III report data only for the classes for which
EVOSUITE run without problems. However, there were a
further 87 classes in these projects for which E VOSUITE
“crashed” without outputting any result. The reasons behin d
these crashes are still under investigation. At any rate,
because these special cases represent only a tiny fraction
of the case study, i.e. 87/8871 <1%of the case study, they
do not pose any particularly serious threat to the validity o f
this study.
D. Results
To account for the randomness of the evolutionary search,
we applied E VOSUITE to each of the selected case study
objects 10 times with different random seeds and then
averaged the values. In each run, we left E VOSUITE running
up to two minutes. In total, running the experiment took
up to (8784×10×2)/(60×24) = 122 days (recall that,
when 100% coverage was achieved, we stopped the search).
Figure 1 shows the distribution of the coverage results per
project.
EVOSUITE produces test suites per class, and each project
might have some more difﬁcult classes and some easier
classes. Figure 2 therefore illustrates the distribution o f
coverage across the classes (RQ1). This shows that there is a
large number of classes which can easily be fully covered by
EVOSUITE (coverage 90%-100%), and also a large number
of classes with problems (coverage 0%-10%), while the rest
is evenly distributed across the 10%-90% range.
3The details of this selection process and the case study are a vailable
online at http://www.evosuite.org/SF10010% 20% 30% 40% 50% 60% 70% 80% 90% 100%
Coverage IntervalsPercentual of Projects
0.00 0.05 0.10 0.15
Figure 1. For each 10% code coverage interval, we report the p roportion of
projects that have an average coverage (averaged out of 10 ru ns on all their
classes) within that interval. Labels show the upper limit (i nclusive). For
example, the group 40% represent all the projects with averag e coverage
greater than 30% and lower or equal to 40%.
10% 20% 30% 40% 50% 60% 70% 80% 90% 100%
Coverage IntervalsPercentual of Classes
0.00 0.05 0.10 0.15 0.20 0.25 0.30
Figure 2. For each 10% code coverage interval, we report the p roportion of
classes that have an average coverage (averaged out of 10 run s) within that
interval. Labels show the upper limit (inclusive). For exampl e, the group
40% represent all the classes with average coverage greater than 30% and
lower or equal to 40%.
The large number of classes with full coverage suggests
that there are many classes that are trivially covered by
EVOSUITE . To analyze this further, Figure 3 illustrates, for
each 10% code coverage interval, the average number of
branches of the classes within this interval. The 90%-100%
interval contains on average the smallest classes, suggest ing
that a large number of classes are indeed easily coverable.
On the other hand, the large number of classes that
apparently have problems (0%-10% coverage) is very large.
Such low coverage may result if the security manager blocks
execution of unsafe code in the tested classes. To see to what
extent this is indeed the case, Table IV lists the average
coverage achieved for classes for each of the possible182Table III
SUMMARIZED STATISTICS OF THE SF100 THE CASE STUDY . FOR MEDIAN ,AVERAGE ,SKEWNESS AND KURTOSIS ,WE ALSO REPORT A 95%
CONFIDENCE INTERVAL (CI) CALCULATED WITH A 10,000 RUN BOOTSTRAPPING .
Min Median CI Average CI Std. Dev. Max Skewness CI Kurtosis CI Tot al
# of Classes per Project 1 35 [26, 48] 87.84 [34.55, 123.96] 23 7.00 2189 7.30 [6.13, 12.14] 63.46 [48.75, 118.66] 8784
# of Branches per Class 0 18 [18, 19] 33.20 [31.60, 34.69] 75.7 9 2480 16.66 [14.50, 21.85] 429.14 [324.24, 636.62] 291639
Table IV
FOR EACH TYPE OF PERMISSION EXCEPTION ,WE REPORT IN HOW MANY CLASSES IT IS THROWN AT LEAST ONCE ,AND THE AVERAGE COVERAGE FOR
THOSE CLASSES . W E ALSO SHOW HOW MANY PROJECTS HAVE AT LEAST ONE CLASS IN WHICH S UCH EXCEPTION IS THROWN ,AND THE AVERAGE
COVERAGE FOR THOSE PROJECTS (INCLUDING ALSO THE CLASSES IN THOSE PROJECTS FOR WHICH THAT K IND OF EXCEPTION IS THROWN ).
Type Per Class Per Project
Occurrence Mean Coverage Occurence Mean Coverage
No Exception 0.093 0.90 0.03 0.91
AllPermission 0.00 - 0.00 -
SecurityPermission 0.11 0.54 0.36 0.51
UnresolvedPermission 0.00 - 0.00 -
AWTPermission 0.00 - 0.00 -
FilePermission 0.71 0.41 0.87 0.54
SerializablePermission 0.00020 0.79 0.01 0.44
ReﬂectPermission 0.00 - 0.00 -
RuntimePermission 0.52 0.49 0.85 0.55
NetPermission 0.49 0.51 0.79 0.56
SocketPermission 0.061 0.39 0.22 0.56
SQLPermission 0.00 - 0.00 -
PropertyPermission 0.074 0.50 0.16 0.59
LoggingPermission 0.00 - 0.00 -
SSLPermission 0.00 - 0.00 -
AuthPermission 0.00012 0.20 0.01 0.25
AudioPermission 0.00 - 0.00 -
OtherPermission 0.00022 0.73 0.01 0.25
10% 20% 30% 40% 50% 60% 70% 80% 90% 100%
Coverage IntervalsAverage Number of Branches
0 10 20 30 40 50 60
Figure 3. Average number of branches of classes within each 10 % code
coverage interval. Classes in the 90%-100% coverage range a re the smallest,
and thus potentially “easiest” classes.
permissions that the security manager may deny. Classes
that raise no exceptions achieve an average coverage of
90%, whereas all classes that require some permission that i s
not granted have lower coverage. Consequently, interactio ns
with the environment are a prime source of problems in10% 20% 30% 40% 50% 60% 70% 80% 90% 100%
Coverage IntervalsRatio of Classes that Spawn Threads
0.00 0.02 0.04 0.06 0.08 0.10 0.12
Figure 4. Average number of threads for classes within each 10 % code
coverage interval: Multi-threaded code does not per se inhi bit coverage.
achieving coverage (RQ2). It is striking that 71% of all
classes lead to some kind of FilePermission – in other
words, almost three quarters of all classes tried to access t he
ﬁlesystem in some way! It is important to note that this I/O
might not come directly from the class under test but one of
its parameters: When testing object-oriented code one needs183sequences of method calls, and as part of the evolutionary
search E VOSUITE attempts to create various different types
and calls many different methods on them. This means that
just the existence of a denied FilePermission does not yet
indicate a problem as there might be other ways to cover the
target code that do not cause ﬁle access; indeed even classes
that achieve high coverage often lead to some kind of denied
permission check. However, the fact that classes with ﬁle
access achieved signiﬁcantly lower average coverage (41%)
is a clear indication that ﬁle access isa real problem.
The other two dominant types of permissions we observed
were RuntimePermissions and NetPermissions. RuntimePer-
missions can have various reasons, such as for example
attempts to shut down the virtual machine or to access
environment variables. On closer inspection, many cases
of RuntimePermissions turned out to be attempts to load
GUI toolkit libraries, which are not Java bytecode librarie s
but platform-dependent libraries. Therefore, a large part
of the classes causing RuntimePermission checks (52% of
the classes) are classes related to GUIs. The number of
classes causing NetPermission checks is also surprisingly
large (49%). Again, a NetPermission check does not auto-
matically mean that the code under test immediately tries
to access the network, but it might happen through the
parameter generation sequences, and NetPermission checks
are also caused for example by generation of an invalid
URL. However, the Java language is by construction well-
suited for web applications and several of the 100 projects
are indeed web applications.
Finally, a common assumption for test generation tools
is that the code under test is single-threaded, as multi-
threaded code adds an additional level of difﬁculty to the
testing problem. Creating a new thread does not require any
permissions in Java, only terminating or changing running
threads leads to permission checks. We therefore observed
the number of running threads each time any permission
check was performed, and each time a test execution timed
out (E VOSUITE by default uses a timeout of ﬁve seconds
for execution of one test case). Figure 4 illustrates the
relation of code coverage to the frequency of cases where
we observed more than one thread: Classes that achieved
90%-100% coverage had the fewest cases of additional
threads, but in general the existence of threads does not per
se seem to have a big impact on coverage, as the largest
number of multi-threaded classes was observed in the 80%-
90% range. However, in the case of multi-threaded code
simply covering the code is usually not sufﬁcient as test
cases might become nondeterministic. Furthermore, multi-
threading introduces new types of faults (deadlocks etc),
and using a randomized algorithm (like E VOSUITE uses) on
code that spawns new threads may cause problems, as Java
offers no way to forcefully stop running threads. However,
on average we observed problems with multi-threading in
only 6.4%of all projects.E. Manual Inspection
On a high level view, the results of the experiment gives
us a clear message: Test generation works well as long as
the environment is not involved – but usually it is involved.
To understand the problems in test generation better, we
manually inspected 10 classes that had low coverage but
no permission problems, 10 classes that had ﬁle permission
problems, 10 classes that had network problems, and 10
classes with runtime permission problems. For this selecti on,
we sorted the classes by coverage, and then chose the classes
with the lowest coverage for each of the categories, but only
one per project per category.
1) Classes without Permission Problems: Classes with
low coverage but no permission problems are of particular
interest with respect to improving E VOSUITE , but might not
generalize to other tools. For example, in the 10 classes we
investigated we identiﬁed the following main reasons for
low coverage: 1) Complex string handling, 2) Java generics
and dynamic type handling, and 3) branches in exception
handling code.
EVOSUITE has basic support for string handling; for ex-
ample, it replaces calls to String.equals with a custom
method that calculates the Levenshtein distance, which can
then serve in branches to give better guidance to the search.
However, this by itself is not sufﬁcient to properly exercis e
complex parsers and string handling functions – at least
in the two minute limit given for test generation in our
experiment. However, there are dedicated string solvers an d
techniques to handle regular expressions, so these classes
might not be problematic for other tools.
The second problem is largely due to Java’s handling of
generics – all type information is erased during compilatio n.
For example, for the constructor StateMachine(List
<Transition> transitions) EVOSUITE only sees
the parameter of type List , but not that this is supposed
to be a list of Transition objects. When generating
List objects E VOSUITE only sees that it can add instances
of type Object to these lists, and thus the chances of
putting Transition objects into the list are small. These
problems could be overcome by incorporating static analysi s
or support for type constraints. Finally, E VOSUITE usually
has no guidance in reaching exception handling blocks,
unless there is an explicit branch in the target class that
leads to a throw statement. Consequently, E VOSUITE only
covers such statements by chance.
Note that other tools might have other problems. For
example, tools based on dynamic symbolic execution have
more problems related to object creation [23], [49].
2) Classes with File Permission Checks: File handling is
very common in Java classes, both in reading as well as in
writing mode. Branches do not necessarily depend on ﬁle
contents, but sometimes just depend on ﬁle existence or ﬁle
names. However, even though these example branches do184not depend on the ﬁle content, usually such branches are
followed by code that manipulates these ﬁles.
Another ﬁle permission we frequently observed is when
code tries to read custom property ﬁles. Even though grant-
ing read access to property ﬁles might not pose an immediate
danger, such ﬁles still need to exist and contain appropriat e
content in order to allow testing.
Consequently, automatically setting up a suitable ﬁle
environment for testing classes is a major technical obstac le.
Besides the difﬁculty in covering branches, there is also
always the danger that code manipulating the ﬁlesystem can
cause unwanted effects; for example, whenever new ﬁles are
generated it is highly undesirable to let the genetic algori thm
pass random strings as ﬁlenames, as that way the ﬁlesystem
will be cluttered with ﬁles with random names – which is
something we observed for several classes in the SF100
corpus when deactivating the custom security manager.
3) Classes with Runtime Permission Checks: As in-
dicated in the previous section, a large share of the
runtime permission checks we observed were due to
code trying to set up a graphical user interface. To
do so, Java ﬁrst tries to read the environment variable
DISPLAY , and then attempts to read a custom GUI toolkit
(e.g., jre/lib/amd64/xawt/libmawt.so ). Further-
more, most GUI applications try to access ﬁles (e.g.,
.accessibility.properties ). Java has its own class
ofAWTPermission that are related to GUI events; as
loading of GUI toolkits was prohibited, we did not observe
any such permission checks.
We tried to see what happens when granting permis-
sions to load libraries. However, even with these permis-
sions the coverage does not increase, as it opens up a
range of other permissions that GUI programs require:
AWTPermissions to access the mouse pointer, a large
amount of thread manipulation, special exception handlers ,
permissions to open windows, etc.
Besides GUI related runtime checks, there are other
common permissions that are undesirable during test gener-
ation, most prominent probably the permission exitVM.0
which is required to shut down the running virtual machine.
Other instances of runtime permission checks include actio ns
on running threads (modifyThread, stopThread), loading of
libraries, queuing of printer jobs, or changing the securit y
manager – none of these actions are desirable during test
execution.
4) Classes with Network Permission Checks:
Only few classes directly attempted to open sockets
(SocketPermission ), although dependent classes or
parameters did this more frequently (in total for 6% of
all classes). NetPermission s were more frequent,
and the most common type of such network permission
that we observed was due to invalid URL generation
(specifyStreamHandler ). This particular permission
does not immediately signify network access, but creationof a URL for a resource to which the program would
normally not have access to (like ﬁle:/foo/fum/). It will
require further experimentation to determine how many of
these permissions were caused by the test generation itself
(e.g., random strings propagating to URL generation), and
how many were real attempts to access resources through
URLs. In general, our observations suggest that in many
cases the NetPermission checks are in fact very similar
toFilePermission checks, which would mean that I/O
remains the most important issue.
In general, the question of ﬁnding a perfect setting of
permissions for test execution is a research question on its
own, and it might be possible to increase coverage by being
more gratuitous with permissions for tests.
F . Threats to Validity
Threats to internal validity come from how experiments
were carried out. We used the E VOSUITE tool for our ex-
periment, which is an advanced research prototype for Java
test data generation. Although E VOSUITE has been carefully
tested, it might have internal faults that compromised the
validity of the results. Furthermore, because E VOSUITE is
based on randomized algorithms, we repeated each experi-
ment on each class 10 times to take this randomness into
account. However, because our study was focused on ob-
taining insights on the challenges of applying test generat ion
tools in realistic settings, our research questions did not deal
with comparisons of algorithms, and so statistical tests we re
not required.
A possible threat to the construct validity is how we
evaluated whether there are unsafe operations when testing
a class. We considered the security exceptions thrown by all
method calls in a test case, even when those methods do not
belong to the class under test. Potentially, E VOSUITE might
have tried to satisfy parameters of the class under test usin g
classes that lead to actions blocked by the security manager ,
even if these parameters could also have been satisﬁed with
other classes that do not result in any security exceptions
(e.g., when a method is declared to take an Object as
parameter, E VOSUITE considers every known class as a
potential input).
The main goal of this paper was to deal with the threats to
external validity that afﬂict current research in software test-
ing. The SF100 corpus is a statistically sound representati ve
of open source projects, and our results are also statistica lly
valid for the other Java projects stored in SourceForge.
For example, even if we encountered high kurtosis in the
number of classes per project and branches per class, median
values are not particularly affected by extreme outliers. T o
reduce such threat to validity, we used bootstrapping to
create conﬁdence intervals for some of the statistics (medi an,
average, skewness and kurtosis).
Our results might not extend to all open source projects,
as other repositories (e.g., Google Code) might contain185software with statistically different distribution prope rties
(e.g., number of classes per project, difﬁculty of the softw are
from the point of view of test data generation). Furthermore ,
there might be a signiﬁcant percentage of open source
projects that are not stored in any repository. Furthermore ,
results on open source projects might not extend to software
that is developed in industry, as for example ﬁnancial and
embedded systems might be under represented in open
source repositories. At any rate, considering the two milli on
subscribers of SourceForge, even if our results would be
valid only for SourceForge projects, still they would be
of practical value and important for a large number of
practitioners (both developers and ﬁnal users).
IV. I MPLICATIONS FOR SOFTWARE ENGINEERING
EXPERIMENTATION
In the previous section we described and analyzed a sound
empirical study in software testing. Given the insights fro m
this experiment, we now discuss the potential implications
of the choice of case studies. In other words, we can answer
the following research question:
RQ3: What are the consequences of choosing a small
case study in a biased way?
An analysis of the literature in test data generation has
shown, in Section II, that a large portion of research body
has practically ignored the issues of test data generation
when the system under test interacts with its environment
(e.g., ﬁle systems and networks). But our empirical analysi s
(Section III) has shown that 90.7%of classes may lead
to interactions with their environment. When there are no
interactions with the environment (i.e., in the 9.3%of cases),
a research prototype such as E VOSUITE can achieve an
average coverage as high as 90% (see Table IV). On the
other hand, when we apply E VOSUITE on a statistically
valid sample of open source projects, the average coverage
is only 48%. Therefore, our analysis casts serious doubts
about the external validity of many empirical analyses that
reported successful results on only a small number of classe s
with no interaction with their environment (e.g., containe r
classes are a typical example).
Does using a large and variegated case study solve this
problem of external validity? The answer is unfortunately no.
If we look at Figure 1, we can see that there are 23 projects
for which E VOSUITE achieves on average a coverage higher
than 80%. If we wanted to boast and promote our research
prototype E VOSUITE , we could have carried out an empir-
ical analysis with only those 23 projects. That would have
resulted in a variegated and large empirical analysis. In ot her
words, any case study, in which the selection of artifacts is
not justiﬁed and not done in systematic way, tells very littl e
about the actual performance of the analyzed techniques.
Our empirical analysis on the SF100 corpus clearly
pointed out which are the real main problems in test datageneration for object-oriented software. For a successful
technology transfer from academic research to industrial
practice, it will be essential that the research community
will solve all of these problems. Therefore, we can provide
the SF100 corpus of classes and pose this challenge to the
research community:
As a research community, can we develop novel techniques
that achieve on average at least 80% of bytecode branch
coverage on this SF100 corpus?
V. C ONCLUSIONS
Experimentation in software engineering research inher-
ently suffers from a common threat to external validity,
caused by the choice of case studies for experimentation.
In this paper, we have presented the SF100 corpus, which
is a statistically sound representative of open source proj ects.
It is composed of 100 Java projects that were randomly
selected from SourceForge, which, given that it has more
than 300 thousand projects and two millions subscribers,
is perhaps the most used open source project repository
on the web. The SF100 corpus consists of 8784 classes,
for a total of 291,639 bytecode branches. To the best of
our knowledge (see Section II), this corpus does not only
represent the largest case study in the literature of test da ta
generation for object-oriented software to date, but most
importantly it is the only one that is not negatively affecte d
by threats to external validity. External validity is one of the
main barriers for a successful transfer of research results to
software development practices.
On this statistically valid corpus, we applied our research
prototype E VOSUITE . EVOSUITE uses many of the most
advanced techniques from the literature on search-based
software testing. Our analysis shows that the large majorit y
of classes (i.e., 90.3%) may lead to execution of “unsafe”
operations, which can potentially harm the execution envi-
ronment (e.g., by deleting ﬁles at random in the ﬁle systems) .
On classes without unsafe operations, E VOSUITE achieves
on average an impressive 90% branch coverage, while on
the entire SF100 corpus it “only” achieves 48% of coverage
on average. As most of the research body in the software
testing literature seems to ignore these issues (e.g., how t o
safely write/read on ﬁle systems and open/close network
connections without negative side effects), our empirical
analysis is a valuable source of statistically valid inform ation
to understand which are the real problems that need to be
solved by the software testing research community.
With this paper, we challenge the research community to
develop novel testing techniques to achieve at least 80% of
bytecode branch coverage on this SF100 corpus, because
it is a valid representative of open source projects, and
our E VOSUITE prototype only achieved 48% coverage on
average. To help the community in this regard, we provide186the SF100 corpus. For more information on E VOSUITE and
the SF100 corpus of classes, please visit our website at:
http://www.evosuite.org/
ACKNOWLEDGEMENTS
This project has been funded by Deutsche Forschungs-
gemeinschaft (DFG), grant Ze509/5-1, and by a Google
Focused Research Award on “Test Ampliﬁcation”. Andrea
Arcuri is funded by the Norwegian Research Council. We
thank Jeremias R ¨oßler for his help in acquiring the Source-
Forge data, and Valentin Dallmeier and Yana Mileva for
feedback on earlier versions of this paper.
REFERENCES
[1] J. H. Andrews, A. Groce, M. Weston, and R. G. Xu. Random
test run length and effectiveness. In IEEE/ACM Int. Confer-
ence on Automated Software Engineering (ASE) , pages 19–
28, 2008.
[2] J. H. Andrews, S. Haldar, Y . Lei, and F. C. H. Li. Tool
support for randomized unit testing. In Proceedings of the
1st International Workshop on Random Testing , RT ’06, pages
36–45, New York, NY , USA, 2006. ACM.
[3] J. H. Andrews, F. C. H. Li, and T. Menzies. Nighthawk: a two-
level genetic-random unit test data generator. In Proceedings
of the 22nd IEEE/ACM Int. Conference on Automated Soft-
ware Engineering , ASE ’07, pages 144–153, New York, NY ,
USA, 2007. ACM.
[4] J. H. Andrews, T. Menzies, and F. C. Li. Genetic algorithms
for randomized unit testing. IEEE Transactions on Software
Engineering (TSE) , 37(1):80–94, 2011.
[5] A. Arcuri, M. Z. Iqbal, and L. Briand. Black-box system
testing of real-time embedded systems using random and
search-based testing. In IFIP International Conference on
Testing Software and Systems (ICTSS) , pages 95–110, 2010.
[6] A. Arcuri, M. Z. Iqbal, and L. Briand. Random
testing: Theoretical results and practical implications.
IEEE Transactions on Software Engineering (TSE) , 2011.
doi:10.1109/TSE.2011.121.
[7] A. Arcuri and X. Yao. Search based software testing of object-
oriented containers. Information Sciences , 178(15):3075–
3095, 2008.
[8] L. Baresi, P. L. Lanzi, and M. Miraz. Testful: an evolutionary
test approach for java. In IEEE International Conference on
Software Testing, Veriﬁcation and Validation (ICST) , pages
185–194, 2010.
[9] C. Boyapati, S. Khurshid, and D. Marinov. Korat: automated
testing based on java predicates. In Proceedings of the 2002
ACM SIGSOFT International Symposium on Software Testing
and Analysis , ISSTA ’02, pages 123–133, New York, NY ,
USA, 2002. ACM.
[10] F. Charreteur and A. Gotlieb. Constraint-based test input
generation for java bytecode. In Proceedings of the 2010
IEEE 21st International Symposium on Software Reliability
Engineering , ISSRE ’10, pages 131–140, Washington, DC,
USA, 2010. IEEE Computer Society.
[11] I. Ciupa, A. Leitner, M. Oriol, and B. Meyer. Artoo: adaptive
random testing for object-oriented software. In ACM/IEEE
International Conference on Software Engineering (ICSE) ,
pages 71–80, 2008.[12] I. Ciupa, A. Pretschner, A. Leitner, M. Oriol, and B. Meyer.
On the predictability of random tests for object-oriented
software. In IEEE International Conference on Software
Testing, Veriﬁcation and Validation (ICST) , pages 72–81,
2008.
[13] C. Csallner and Y . Smaragdakis. JCrasher: an automatic
robustness tester for Java. Softw. Pract. Exper. , 34:1025–1050,
2004.
[14] C. Csallner and Y . Smaragdakis. Check ’n’ crash: combining
static checking and testing. In Proceedings of the 27th
international conference on Software engineering , ICSE ’05,
pages 422–431, New York, NY , USA, 2005. ACM.
[15] C. Csallner, Y . Smaragdakis, and T. Xie. DSD-Crasher: A
hybrid analysis tool for bug ﬁnding. ACM Trans. Softw. Eng.
Methodol. , 17:8:1–8:37, May 2008.
[16] M. d’Amorim, C. Pacheco, T. Xie, D. Marinov, and M. D.
Ernst. An empirical comparison of automated generation and
classiﬁcation techniques for object-oriented unit testing. In
IEEE/ACM Int. Conference on Automated Software Engineer-
ing (ASE) , pages 59–68, 2006.
[17] G. Fraser and A. Arcuri. Evolutionary generation of whole
test suites. In International Conference On Quality Software
(QSIC) , pages 31–40, Los Alamitos, CA, USA, 2011. IEEE
Computer Society.
[18] G. Fraser and A. Arcuri. Evosuite: Automatic test suite
generation for object-oriented software. In ACM Symposium
on the Foundations of Software Engineering (FSE) , 2011.
[19] G. Fraser and A. Zeller. Mutation-driven generation of unit
tests and oracles. IEEE Transactions on Software Engineer-
ing, 99(PrePrints), 2011.
[20] J. Galeotti, N. Rosner, C. L ´opez Pombo, and M. Frias. Anal-
ysis of invariants for efﬁcient bounded veriﬁcation. In ACM
Int. Symposium on Software Testing and Analysis (ISSTA) ,
pages 25–36, 2010.
[21] H. Hemmati, A. Arcuri, and L. Briand. Empirical investi-
gation of the effects of test suite properties on similarity-
based test case selection. In IEEE International Conference
on Software Testing, Veriﬁcation and Validation (ICST) , pages
327–336, 2011.
[22] K. Inkumsah and T. Xie. Improving structural testing of
object-oriented programs via integrating evolutionary testing
and symbolic execution. In IEEE/ACM Int. Conference
on Automated Software Engineering (ASE) , pages 297–306,
2008.
[23] H. Jaygarl, S. Kim, T. Xie, and C. K. Chang. Ocat: object
capture-based automated testing. In Proceedings of the 19th
International Symposium on Software Testing and Analysis ,
ISSTA ’10, pages 159–170, New York, NY , USA, 2010.
ACM.
[24] V . G. Karthick Jayaraman, David Harvison and A. Kiezun.
jfuzz: A concolic whitebox fuzzer for Java. In Proceedings
of NASA Formal Methods Workshop (NFM 2009) , 2009.
[25] J. Malburg and G. Fraser. Combining search-based and
constraint-based testing. In IEEE/ACM Int. Conference on
Automated Software Engineering (ASE) , 2011.
[26] D. Marinov and S. Khurshid. Testera: A novel framework
for testing java programs. In IEEE/ACM Int. Conference on
Automated Software Engineering (ASE) , 2001.
[27] P. McMinn. Search-based software test data generation:
A survey. Software Testing, Veriﬁcation and Reliability ,
14(2):105–156, 2004.187[28] C. Oriat. Jartege: A Tool for Random Generation of Unit
Tests for Java Classes. In Quality of Software Architectures
and Software Quality , volume 3712/2005 of Lecture Notes
in Computer Science , pages 242–256, Heidelberg, 2005.
Springer Berlin.
[29] C. Pacheco and M. D. Ernst. Eclat: Automatic generation
and classiﬁcation of test inputs. In ECOOP 2005 — Object-
Oriented Programming, 19th European Conference , pages
504–527, Glasgow, Scotland, July 27–29, 2005.
[30] C. Pacheco, S. K. Lahiri, M. D. Ernst, and T. Ball. Feedback-
directed random test generation. In ACM/IEEE International
Conference on Software Engineering (ICSE) , pages 75–84,
2007.
[31] C. S. P ˇasˇareanu, P. C. Mehlitz, D. H. Bushnell, K. Gundy-
Burlet, M. Lowry, S. Person, and M. Pape. Combining unit-
level symbolic execution and system-level concrete execution
for testing nasa software. In Proceedings of the 2008 Int.
Symposium on Software Testing and Analysis , ISSTA ’08,
pages 15–26, New York, NY , USA, 2008. ACM.
[32] J. C. B. Ribeiro, M. A. Zenha-Rela, and F. F. de Vega. Test
case evaluation and input domain reduction strategies for the
evolutionary testing of object-oriented software. Information
and Software Technology , 51(11):1534–1548, 2009.
[33] J. C. B. Ribeiro, M. A. Zenha-Rela, and F. F. de Vega. En-
abling object reuse on genetic programming-based approaches
to object-oriented evolutionary testing. In Proceedings of the
European Conference on Genetic Programming (EuroGP) ,
pages 220–231, 2010.
[34] K. Sen and G. Agha. Cute and jcute: Concolic unit testing and
explicit path model-checking tools. In T. Ball and R. Jones,
editors, Computer Aided Veriﬁcation , volume 4144 of Lecture
Notes in Computer Science , pages 419–423. Springer Berlin
/ Heidelberg, 2006.
[35] K. Sen, D. Marinov, and G. Agha. CUTE: a concolic unit
testing engine for C. In ESEC/FSE-13: Proc. of the 10th
European Software Engineering Conf. held jointly with 13th
ACM SIGSOFT Int. Symposium on Foundations of Software
Engineering , pages 263–272. ACM, 2005.
[36] R. Sharma, M. Gligoric, A. Arcuri, G. Fraser, and D. Mari-
nov. Testing container classes: Random or systematic? In
Fundamental Approaches to Software Engineering (FASE) ,
2011.
[37] A. J. Simons. JWalk: a tool for lazy, systematic testing
of java classes by design introspection and user interaction.
Automated Software Engg. , 14:369–418, December 2007.
[38] M. Staats and C. Pasareanu. Parallel symbolic execution for
structural test generation. In ACM Int. Symposium on Software
Testing and Analysis (ISSTA) , pages 183–194, 2010.
[39] K. Taneja and T. Xie. Diffgen: Automated regression unit-
test generation. In Proceedings of the 2008 23rd IEEE/ACM
International Conference on Automated Software Engineer-
ing, ASE ’08, pages 407–410, Washington, DC, USA, 2008.
IEEE Computer Society.
[40] S. Thummalapenta, J. de Halleux, N. Tillmann, and
S. Wadsworth. Dygen: automatic generation of high-coverage
tests via mining gigabytes of dynamic traces. In Proceedings
of the 4th international conference on Tests and proofs ,
TAP’10, pages 77–93, Berlin, Heidelberg, 2010. Springer-
Verlag.
[41] S. Thummalapenta, T. Xie, N. Tillmann, J. de Halleux, and
W. Schulte. MSeqGen: object-oriented unit-test generation via
mining source code. In Proceedings 7th European SoftwareEngineering Conference and ACM SIGSOFT Symposium on
the Foundations of Software Engineering , ESEC/FSE ’09,
pages 193–202. ACM, 2009.
[42] N. Tillmann and W. Schulte. Parameterized unit tests. In
Proceedings of the 10th European Software Engineering
Conference Held Jointly with 13th ACM SIGSOFT Interna-
tional Symposium on Foundations of Software Engineering ,
ESEC/FSE-13, pages 253–262, New York, NY , USA, 2005.
ACM.
[43] P. Tonella. Evolutionary testing of classes. In ACM Int.
Symposium on Software Testing and Analysis (ISSTA) , pages
119–128, 2004.
[44] W. Visser, C. S. Pasareanu, and R. Pel `anek. Test input
generation for java containers using state matching. In ACM
Int. Symposium on Software Testing and Analysis (ISSTA) ,
pages 37–48, 2006.
[45] W. Visser, C. S. P ˇasˇareanu, and S. Khurshid. Test input
generation with java pathﬁnder. In Proceedings of the 2004
ACM SIGSOFT International Symposium on Software Testing
and Analysis , ISSTA ’04, pages 97–107, New York, NY , USA,
2004. ACM.
[46] W. Visser, C. S. P ˇasˇareanu, and R. Pel ´anek. Test input gener-
ation for java containers using state matching. In Proceedings
of the 2006 International Symposium on Software Testing and
Analysis , ISSTA ’06, pages 37–48, New York, NY , USA,
2006. ACM.
[47] T. V os, A. Baars, F. Lindlar, P. Kruse, A. Windisch, and
J. Wegener. Industrial Scaled Automated Structural Testing
with the Evolutionary Testing Tool. In IEEE International
Conference on Software Testing, Veriﬁcation and Validation
(ICST) , pages 175–184, 2010.
[48] S. Wappler and J. Wegener. Evolutionary unit testing of
object-oriented software using strongly-typed genetic pro-
gramming. In Genetic and Evolutionary Computation Con-
ference (GECCO) , pages 1925–1932, 2006.
[49] X. Xiao, T. Xie, N. Tillmann, and J. de Halleux. Precise
identiﬁcation of problems for structural test generation. In
Proceeding of the 33rd International Conference on Software
Engineering , ICSE ’11, pages 611–620, New York, NY , USA,
2011. ACM.
[50] T. Xie, D. Marinov, and D. Notkin. Rostra: A framework for
detecting redundant object-oriented unit tests. In IEEE/ACM
Int. Conference on Automated Software Engineering (ASE) ,
pages 196–205, 2004.
[51] T. Xie, D. Marinov, W. Schulte, and D. Notkin. Symstra:
A framework for generating object-oriented unit tests using
symbolic execution. In Proceedings of the 11th Int. Con-
ference on Tools and Algorithms for the Construction and
Analysis of Systems , pages 365–381, 2005.
[52] T. Xie and D. Notkin. Tool-assisted unit-test generation
and selection based on operational abstractions. Automated
Software Engg. , 13:345–371, July 2006.
[53] L. Zhang, T. Xie, L. Zhang, N. Tillmann, J. de Halleux, and
H. Mei. Test generation via dynamic symbolic execution for
mutation testing. In Proceedings of the 2010 IEEE Interna-
tional Conference on Software Maintenance , ICSM ’10, pages
1–10, Washington, DC, USA, 2010. IEEE Computer Society.
[54] S. Zhang, D. Saff, Y . Bu, and M. D. Ernst. Combined
static and dynamic automated test generation. In ISSTA
2011, Proceedings of the 2011 International Symposium on
Software Testing and Analysis , Toronto, Canada, July 19–21,
2011.188