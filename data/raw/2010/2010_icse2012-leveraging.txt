Leveraging Test Generation and SpeciÔ¨Åcation Mining for
Automated Bug Detection without False Positives
Michael Pradel
Department of Computer Science
ETH ZurichThomas R. Gross
Department of Computer Science
ETH Zurich
Abstract ‚ÄîMining speciÔ¨Åcations and using them for bug
detection is a promising way to reveal bugs in programs.
Existing approaches suffer from two problems. First, dynamic
speciÔ¨Åcation miners require input that drives a program to
generate common usage patterns. Second, existing approaches
report false positives, that is, spurious warnings that mislead
developers and reduce the practicability of the approach. We
present a novel technique for dynamically mining and checking
speciÔ¨Åcations without relying on existing input to drive a
program and without reporting false positives. Our technique
leverages automatically generated tests in two ways: Passing
tests drive the program during speciÔ¨Åcation mining, and failing
test executions are checked against the mined speciÔ¨Åcations.
The output are warnings that show with concrete test cases how
the program violates commonly accepted speciÔ¨Åcations. Our
implementation reports no false positives and 54 true positives
in ten well-tested Java programs.
Keywords -Bug detection; SpeciÔ¨Åcation mining; False posi-
tives
I. I NTRODUCTION
Programs often access libraries and frameworks via appli-
cation programming interfaces (APIs). While this approach
reduces development effort by reusing existing functionality,
it entails challenges for ensuring the program‚Äôs correctness.
Many APIs impose protocols , that is, temporal constraints
regarding the order of the calls of API methods. For ex-
ample, calling peek() onjava.util.Stack without a
preceding push() gives an EmptyStackException , and
calling next() onjava.util.Iterator without check-
ing whether there is a next element with hasNext() can
result in a NoSuchElementException . API clients that
violate such protocols do not obtain the desired behavior
and may even crash the program.
This paper presents a fully automatic approach to reveal
violations of API protocols. Our work builds upon three
existing techniques that‚Äîon their own‚Äîdo not fully address
the problem. First, we build upon automatic test genera-
tion [1]‚Äì[3]. While test generators achieve good coverage,
their usefulness is bounded by the test oracle that decides
when to report a potential bug. For example, the Randoop
test generator [2] comes with a small set of manually
speciÔ¨Åed, generic test oracles that do not detect all bugs
triggered by the generated tests.Second, we build upon dynamic protocol mining [4], [5].
Protocol miners infer likely API protocols from executions
of API clients. The usefulness of a dynamic protocol miner is
bounded by the executions it analyzes [6]. A protocol miner
can infer a protocol only if there is input that exercises paths
of an API client that expose the protocol.
Third, we build upon dynamic protocol checking [7]‚Äì[9],
which veriÔ¨Åes whether a program execution violates a given
protocol. Similar to a protocol miner, the usefulness of a
protocol checker is bounded by the executions it analyzes.
The checker can report a protocol violation only if some
input triggers the illegal behavior.
As a motivating example, consider a simpliÔ¨Åed version of
a problem our analysis found in xalan , a program shipped
with the DaCapo benchmarks [10]: A class Coffers a public
method report() . Calling report() on a fresh instance of
Cgives an EmptyStackException , which is not declared
in the public interface of report() . The problem is that C
uses the Stack API in an unsafe way that exposes users of
Cto exceptions caused by violations Stack ‚Äôs API protocol.
A class using an API should ensure that the API is used
safely, that is, without the risk to throw an exception due to
a protocol violation. A user of Ccannot foresee that Cmay
crash the program, unless she inspects C‚Äôs implementation,
which however, contradicts the idea of modular, object-
oriented programming.
Can test generation, protocol mining, and protocol check-
ing help Ô¨Ånding the problem in class C? A test generator can
produce tests that cover many paths through C, including
a path that triggers the EmptyStackException . Unfortu-
nately, an exception alone is an unreliable indicator for a
bug: Generated tests produce many exceptions due to illegal
usage, for example, by passing illegal method arguments.
A protocol miner can infer the protocol for using Stack s
but requires input that drives clients of the Stack class.
Finally, a runtime protocol checker can detect that Cviolates
theStack protocol but requires the protocol and input that
drives Cinto the protocol violation.
We present a fully automatic approach to reveal API
protocol violations by combining test generation, protocol
mining, and protocol checking. Our approach consists of
three steps. First, we generate tests for the classes of aprogram. Some tests fail, because they result in an uncaught
exception, while other tests pass. Second, we analyze exe-
cutions of passing tests with a protocol miner, giving likely
speciÔ¨Åcations of how to use API classes. Third, we verify
executions of failing tests against the mined protocols and
report a warning if a test fails because of a protocol violation.
The approach is easily applicable in practice because
it has a high beneÔ¨Åt-cost ratio. All the input required by
the analysis is the program itself. The approach does not
require speciÔ¨Åcations, as these are extracted by the protocol
miner. Furthermore, it does not require any input to run the
program, because all the input is automatically generated.
The output of the analysis is precise and concrete. It is
precise, because each warning reveals a protocol violation
that certainly leads the program to an exception. It is
concrete, because each warning comes with a test case
demonstrating how to trigger the problem.
A main beneÔ¨Åt of the approach is that all reported
warnings are true positives. The analysis reports a warning
only if a protocol is violated and if the protocol violation
causes an uncaught exception. As a result, each warning
exposes an unsafe API usage where a class uses an API
in a way that exposes clients of the class to the risk of
unintentionally violating the API‚Äôs protocol. In principle, the
approach might report a false warning when two conditions
hold. First, an API method mmust be implemented incor-
rectly and throw an exception despite being called legally.
Second, a mined protocol must forbid calling mat a state
where a permissive protocol [11] would allow it. One of
the two conditions alone does not lead to false warnings.
In particular, incomplete protocols alone, which may result
from dynamic protocol mining, do not cause false positives.
In practice, both conditions together are very unlikely to
occur and we did not encounter this situation during our
experiments.
We implement the approach based on two existing tools: a
dynamic protocol miner to extract API protocols [5], [6] and
a random test generator, Randoop [2]. To evaluate the ap-
proach, we apply it to ten real-world Java programs [10]. The
analysis reports no false positives and 54 true bugs, all of
which are illegal API usages that can be triggered via public
methods of the programs. Applied to the same programs, the
best existing approach to dynamically Ô¨Ånd violations of API
protocols without a priori known speciÔ¨Åcations has reported
three warnings, two of which are false positives [12].
As an optional, heuristic optimization of our approach, we
modify Randoop by adding a static analysis that guides the
random test generation towards those methods that use the
API in question. Without this guidance, the test generator
randomly chooses the methods to call from all methods of a
program. If only a small part of the program uses the API,
most of the testing effort is wasted. Our analysis statically
prioritizes methods that are likely to contribute to calls from
the program to the API, so that API usage bugs are revealed(1)
Method
Priori-
tization(2)
Random
Test Gen-
eration(3)
Protocol
Mining
(4)
Protocol
CheckingProgram
& API
PrioritiesPassing
tests
Failing
testsProtocols
Unsafe
API usages
Figure 1. Overview.
faster. Compared to standard Randoop, this optimization
increases the number of API calls by a factor of 57 and
reveals bugs with Ô¨Åve times less testing effort, on average.
In summary, this paper makes the following contributions:
No false positives. An analysis to detect bugs based on
mined speciÔ¨Åcations without reporting false positives.
Mining and checking driven by generated tests. Our
approach drives both speciÔ¨Åcation mining and program
checking with automatically generated input.
Guided random test generation. We reduce the time
to trigger an API usage bug by guiding random test
generation towards a set of API methods.
II. L EARNING FROM THE GOOD TO FIND THE BAD
In this section, we describe our approach to leverage
generated tests to reveal API protocol violations without
reporting false positives. Figure 1 illustrates the major steps.
Given a program and an API as input, we use a random
test generator (2) to generate tests for the program. The
generated tests can be classiÔ¨Åed into two groups: failing
tests that lead the program to an exception or that violate an
assertion, and passing tests that execute without any obvious
error. We analyze executions of passing tests with a dynamic
protocol miner (3), which produces protocols describing
common API usage patterns (Section II-B). Then we analyze
executions of failing tests with a dynamic protocol checker
(4) (Section II-C). This last step Ô¨Ålters the various failing
tests by selecting those that fail because the program violates
a protocol, removing all tests that fail for other reasons, such
as illegal input (Section II-D). Finally, the protocol violations
are reported to the developer.
Figure 1 also shows method prioritization (1), a static
analysis that heuristically computes how relevant each of
the program‚Äôs methods is for triggering calls into the API.
Method prioritization is an optional optimization and can
be ignored for the description of our general approach.
Section III explains how method prioritization makes our
technique even more useful in practice.
A. Background
1) Protocol Mining: SpeciÔ¨Åcation mining extracts formal
speciÔ¨Åcations from existing software. This work focuses
on protocols that describe in which order API methods1 2 3Stack()!ss.push()s.push()
s.pop()
s.peek()s.removeAllElements()
Figure 2. Mined protocol for java.util.Stack . States with gray
background are liable states.
are commonly called. Several techniques for mining such
protocols from an API [13] or from programs using an
API [4], [14]‚Äì[18] have been proposed. We use a scalable
protocol miner that infers protocols involving one or more
interacting objects [5], [6]. Figure 2 shows an example
of a mined protocol for Stack . An empty stack allows
only calls to push() . Once elements have been pushed
on the stack, one can call push() ,pop() , and peek()
in an arbitrary order, until calls to removeAllElements()
empty the stack again. This protocol is a regular language
approximation of the more precise property that push()
andpop() are balanced. Despite this approximation, the
protocol can reveal illegal uses of stacks, as we show
subsequently.
A protocol consists of a Ô¨Ånite state machine and a set
of typed protocol parameters that represent the objects
involved in the protocol. The states of a protocol represent
states of the involved objects. The transitions represent
method calls, where receiver and, optionally, method param-
eters and the return value are protocol parameters. A call
r:m(a1; : : : ; a n)!oencodes the method identiÔ¨Åer m, the
receiver r, the arguments a1; : : : ; a n, and the return value o.
The protocols created by the protocol miner distinguish two
kinds of states: setup states andliable states . The setup states
establish which objects interact in the protocol by binding
objects to protocol parameters. The liable states describe
constraints on method calls that a programmer ought to
respect. A protocol must be followed only when the current
state is a liable state.
The protocol miner requires an execution trace , that is, a
sequence of method calls observed during an execution. The
protocol miner splits the execution trace into small subtraces
that contain calls to one API object or to a set of interacting
API objects. For a subtrace sub,types (sub)denotes the set
of receiver types occurring in sub. The miner summarizes
all subtraces with the same types (sub), for example all
subtraces for Stack s, into a protocol. Whenever a subtrace
contains a method call sequence not yet accepted by the
protocol, the protocol is extended to accept the subtrace.
The resulting protocol accepts all observed subtraces.
2) Random Test Generation: There are various
approaches to automatically generate tests (Section VI-B).
This work builds upon feedback-directed random test
generation, implemented in Randoop [2]. We use Randoop
because it requires no input except the program under test,
scales to large programs, and generates tests with highcoverage. Randoop randomly selects methods and chains
them into call sequences. If executing a call sequence
neither produces an exception nor violates an assertion, the
sequence is used to construct further call sequences. The
output are passing and failing JUnit tests.
B. Driving Protocol Mining with Passing Tests
Dynamic protocol mining is inherently limited by the
execution traces given to the miner, which in turn is limited
by the input used to execute the program. Most programs
come with a Ô¨Ånite set of inputs, for example, given as unit
tests. Producing more input requires manual effort, reducing
the overall usefulness of protocol mining.
We address this limitation by combining protocol mining
with test generation. The idea is to analyze an execution of
a generated test in the same way as traditional speciÔ¨Åcation
mining analyzes executions driven by otherwise available
input. A beneÔ¨Åt of this approach is that generated tests are a
large and diverse source of inputs. Random test generation
provides fresh inputs for different random seeds, which
can trigger behavior not yet triggered with other random
seeds [19].
To mine the API usage of a program driven by generated
tests we proceed as follows. At Ô¨Årst, we use AspectJ to
instrument the program so that it writes an execution trace
with all API calls to a Ô¨Åle. We create one Ô¨Åle per generated
sequence of method calls. Then, the protocol miner analyzes
the execution traces and extracts protocols describing in
which order the program calls API methods. Call sequences
of different tests that trigger the same API class contribute
to the same protocol. The main difference to traditional
speciÔ¨Åcation mining is that we drive the program with
generated tests instead of other input.
A potential problem of generating tests for protocol
mining is that artiÔ¨Åcial input may trigger illegal API call
sequences that can lead to incorrect protocols. We address
this problem in two ways. First, we feed only execution
traces of passing tests into the protocol miner, not execution
traces of failing tests. While a passing test does not guarantee
to use the program in the intended way, we found that
in practice, most illegal inputs are Ô¨Åltered, because they
would lead to an exception. Second, the execution trace
contains only API calls from the program, not calls from
the generated tests (a generated test can call API methods,
for example, to create an API object before passing it as
a parameter). That is, each API call the miner learns from
is part of the program and not a call that was randomly
generated by Randoop.
C. Checking Failing Tests against Mined Protocols
A dynamic protocol checker can verify whether a program
violates commonly accepted constraints by checking the
program against mined protocols. For a given programexecution, a checker guarantees soundness and complete-
ness: All reported protocol violations provably occur in
the execution, and all protocol violations in the execution
are reported. Unfortunately, this approach can only reveal
a problem if the problem occurs in the analyzed program
execution, that is, if the problem is triggered by the given
input. For example, checking the execution of a manually
created unit test suite against a set of protocols is unlikely
to reveal many protocol violations, because the test suite
exercises well-tested paths of the program.
We address this limitation by driving the program with
generated tests. In contrast to the mining step of our ap-
proach, we now use generated tests that fail. These tests are
classiÔ¨Åed as failing because they trigger a problem in the
program that leads to an exception or an assertion violation.
Many failing tests do not expose any bug but fail be-
cause the generated test uses the program incorrectly. For
example, a test may fail with a NullPointerException or
anIllegalArgumentException , because the test passes
null or another illegal value as a method argument. Such
tests are not relevant for programmers, as they do not reveal
bugs in the program. To focus on tests that fail because of a
bug in the program, we check whether the program violates
a protocol during test execution, as those tests expose an
unsafe API usage in the program and therefore are relevant
for programmers. That is, we use protocol checking as a
Ô¨Ålter that identiÔ¨Åes true bugs among all failing tests.
The checker veriÔ¨Åes the execution trace of each failing
test against the set of mined protocols. Conceptually, the ap-
proach is similar to existing runtime veriÔ¨Åcation tools, such
as JavaMOP [9]. Our implementation uses the infrastructure
built for mining protocols. At Ô¨Årst, the execution trace is split
into subtraces in the same manner as for protocol mining [6].
Then, each subtrace subis veriÔ¨Åed against all protocols that
have protocol parameters with types equal to types (sub).
The checker assigns each receiver object in the subtrace
to a protocol parameter of matching type. For subtraces
with multiple receiver objects of the same type, the checker
creates multiple protocol instances, one for each possible
assignment of objects to protocol parameters.
At the beginning of a subtrace, the protocol is in the initial
state. The checker goes through all calls in the subtrace
and analyzes all calls that appear in the alphabet of the
protocol. Each such call is matched against the outgoing
transitions of the current state. As all mined protocols are
deterministic, each call matches at most one transition. If a
transition matches, the target state of the transition becomes
the current state. If no transition matches and the current
state is a liable state, the checker reports a protocol violation.
Non-matching transitions in setup states are ignored.
For example, consider the source code in Figure 3a, which
is the problem described in Section I. The class Cuses a
stack in an unsafe way, because a client of Ccan trigger
a violation of the stack protocol via C‚Äôs public interface.class C {
private Stack s = newStack ();
public String report () {
return get (). toString ();
}
private Object get() {
s.peek ();
}
public void fill() {
s.push (..);
}
}
(a) Source codenew Stack() !s
s.peek()
# EmptyStackException
# at Stack.peek()
# at C.get()
# at C.report()
# . . .
(b) Execution trace
Figure 3. Example code that can violate the Stack protocol and
execution trace exposing the violation.
We consider this to be an API usage bug, because C‚Äôs
public interface does not reveal this risk, and understanding
it requires knowledge of C‚Äôs implementation.
Our approach Ô¨Ånds this problem because Randoop gen-
erates a test that triggers an API protocol violation by
calling C‚Äôs methods. The test calls report() without a
preceding call to fill() and therefore triggers a call to
peek() while the stack is empty. This call causes an
EmptyStackException and fails the test. Figure 3b shows
the execution trace of the failing test. The trace contains all
calls to Stack that were observed during the test execution.
The checker Ô¨Ånds a violation of the protocol in Figure 2
because the second call in the trace tries to call peek() at
state 2, but peek() is only allowed at state 3.
D. Warnings without False Positives
Reporting few (ideally no) false positives is crucial to
make a bug Ô¨Ånding technique applicable in practice [20].
By using runtime checking, our approach avoids a problem
typically encountered with static checkers, namely false
positives due to approximations of execution paths. Another
potential source of false positives are mined protocols that
disallow legal behavior because this behavior was not ob-
served during protocol mining. Our approach eliminates this
problem by only reporting those protocol violations that
certainly lead to an exception thrown by the API, that is,
to undesired behavior. For example, the call to peek()
in Figure 3 results in an EmptyStackException , a clear
indication that the API is used incorrectly.
To Ô¨Ålter protocol violations that cause an exception, we
need several pieces of information. For each failing test, the
analysis logs the reason for the failure, which consists of two
parts: the type Texcof the thrown exception and the stack
trace S= (loc1; loc 2; : : : ; loc n)when throwing it. Each
stack trace element lociindicates a source code location.
For each protocol violation, the protocol checker reports the
source code location locviol where the violating API call
occurs. Furthermore, the analysis statically extracts the set
of declared exception types DAPI of the API method that
is called at locvioland the set of declared exception typesDprogram of the method containing locviol. An exception
can be declared using the throws keyword or using the
@throws or@exception Javadoc tags.
Our analysis reports a protocol violation to the user only
if the following three conditions are true:
1)Texc2DAPI
2)locviol2S
3)Texc=2Dprogram
The Ô¨Årst two conditions ensure that the call that violates
the protocol is responsible for failing the test. The third
condition avoids warnings for methods that deliberately pass
on exceptions thrown by the API to their own clients. In
this case, the protocol-violating class is implemented safely,
because it explicitly passes on responsibility for using the
API correctly to its clients. For Figure 3, all three conditions
hold and our analysis reports an unsafe API usage, along
with a test case that illustrates how to trigger the problem.
III. API-G UIDED TESTGENERATION
Section II describes how randomly generated tests can
drive a program for mining API protocols and for checking
the program against mined protocols. With a random test
generation tool, such as Randoop, many generated tests
do not trigger API calls. The reason is that even though
only a subset of all methods of a program uses an API,
Randoop chooses which method to call randomly from
all available methods. This section presents a heuristic to
optimize our approach by guiding random test generation
towards methods that trigger API calls.
Given inÔ¨Ånite time, standard Randoop triggers the same
behavior as a guided approach focusing on methods relevant
for calling the API. In practice, increasing the number and
variety of API calls triggered by generated tests within a
Ô¨Ånite amount of time is important for two reasons. First, ran-
dom test generators require a stopping criterion‚Äîtypically,
wall clock time or the number of generated method call
sequences. Second, random test generators execute arbitrary
code, making them susceptible to crashes that are hard to
avoid in general [21]. Whatever stops the test generator,
triggering API calls earlier than a purely random approach
not only saves time, but also is likely to trigger more API
usage bugs before the test generator terminates.
To guide Randoop towards methods relevant for triggering
API calls, we statically analyze the program and prioritize
its methods. The priorities inÔ¨Çuence the random decisions
of the test generator so that calling a method with higher
priority is more likely. Methods that do not contribute at
all to calling the API get a priority of zero and are ignored
altogether by the test generator.
A. Computing Method Priorities
We build upon two well-known graph representations
of the program under test: an inverted, context-insensitive
call graph and a parameter graph [1]. The inverted callgraph is a directed graph where vertices represent methods
and edges represent a ‚Äúmay be called by‚Äù relationship
between methods. We extract call graphs with Soot [22].
The parameter graph is a directed graph where vertices also
represent methods and where edges represent a ‚Äúcan use
the result of‚Äù relationship between methods. There is an
edge from m1tom2ifm1requires an object of type T(as
receiver or as argument) and if m2‚Äôs return type is equal to
Tor a subtype of T.
Test generators call methods for three reasons. First,
because a method is among the methods under test . In our
case, these are all methods that call the API. Second, because
the method returns an object that can be passed as an argu-
ment to another method, that is, the method is a parameter
provider . Third, because the method may change the internal
state of an object that is used for another call afterwards, that
is, the method is a state changer . Our prioritization technique
considers these reasons and computes for each method three
priority values that indicate how relevant the method is for
each reason. Afterwards, the priorities are combined into a
single priority value per method.
Algorithm 1 describes how we compute the priority of
each method. The algorithm takes as input the call graph
Gc, the parameter graph Gp, the set of all methods M, and
the set MAPI of API methods. There are four main steps.
Algorithm 1 Compute priorities for calling methods during
random test generation
Input: Inverted call graph Gc, parameter graph Gp, meth-
odsM, API methods MAPI
Output: Map p:method!Rassigning priorities to
methods
/*priorities for methods under test */
1:initialize pmut :method!Rto zero
2:for all m2Mdo
3: for all mAPI2MAPI withdc(mAPI; m)dmax
c
do
4: pmut(m) +=relevance (mAPI)
dc(mAPI; m)
/*priorities for parameter providers */
5:initialize pparam :method!Rto zero
6:for all m2Mwithpmut(m)>0do
7: for all m02reachable (Gp; m)do
8: pparam (m0)+ =pmut(m)
nbProviders (retType (m0))
/*priorities for state changers */
9:initialize pstate :method!Rto zero
10:for all m2Mwithpmut(m)>0do
11: for all t2inputTypes (m)do
12: for all m02methods (t)do
13: pstate +=pmut(m)
jmethods (t)j
14:p=merge (pmut; pparam ; pstate )1) Priorities for Methods Under Test: At Ô¨Årst, we com-
pute how relevant a method is depending on whether it calls
any of the API methods. Methods calling an API method
directly are the most relevant ones. In addition, methods
calling API methods indirectly, that is, via other methods, are
given a lower priority. The reason for considering indirect
calls is that some methods should be called at a state built
up by another method. For instance, a method m1may
create a Ô¨Åle and call another method m2that writes into the
Ô¨Åle. By calling m1, the test generator can trigger successful
writes into the Ô¨Åle, even though m1calls the Ô¨Åle API only
indirectly. We limit the level of indirection up to which to
consider indirect callers of API methods to dmax
c, which
is set to three in our experiments. The priority gain of a
method calling an API method depends on two factors. First,
it increases with the relevance of the called API method:
relevance (mAPI) =1
jfmj(mAPI; m)2Gcgj
Less frequently called methods are more relevant so that all
API methods get an equal chance of being triggered. Sec-
ond, the priority gain decreases with the minimal distance
dc(mAPI; m)inGcbetween the API method mAPI and the
calling method m. The rationale for this decrease is that the
call graph contains may-call and not must-call edges; fewer
indirections to an API call increase the chance that the API
is actually called during an execution.
2) Priorities for Parameter Providers: We compute how
relevant a method is for providing parameters for calling
methods under test. Therefore, we consider all methods
reachable (Gp; m)that are reachable from a method under
testmin the parameter graph. These are all methods that
directly or indirectly contribute parameters for calling the
method under test. The gain of priority for such a parameter
provider depends in two factors. First, it increases with the
priority of the method it provides parameters for. Second,
the gain of priority decreases with the overall number of
providers for the type required by the method under test.
The rationale for this decrease is to give higher priority
to methods providing an uncommon type than to methods
providing a type returned by many other methods.
3) Priorities for State Changers: The third kind of prior-
ity is for methods changing the internal state of objects used
for calling methods under test. The types inputTypes (m)
calling a method mare the receiver type and all argument
types of m. We consider all methods of input types for
methods under test as potentially state-changing and increase
their priority. The priority gain depends on two factors. First,
it increases with the priority of the method under test that
the state changer may inÔ¨Çuence. Second, the priority gain
decreases with the overall number of methods of the input
type. The rationale for this decrease is to give objects of each
input type an equal chance of being changed, independently
of the number of methods of the type.class A {
API api;
A(B b) { .. }
void init() { api = APIPool.some; }
void doIt() { api.use (); }
}
class B {
B(C c) { .. }
void m(D d) { .. }
}
class C {}
class D {}Method Priority
A.doIt() 107
A() 33
B(C) 27
C() 27
A.init() 7
B.m(D) 0
D() 0
Figure 4. Example for prioritizing methods towards using the API.
4) Merging: The Ô¨Ånal step is to normalize the three
kinds of priorities and to combine the three priorities of
each method in a weighted sum. The weights indicate
how important calling methods under test, calling parameter
providers, and calling state changers are, respectively. In
our experiments, the test generator devotes 20% of all calls
to calling state changers and 40% to each calling methods
under test and to calling parameter providers.
B. Example
Figure 4 shows a minimal example to illustrate how
prioritizing methods guides random test generation towards
calling API methods. The example contains seven methods
and constructors, of which one, doIt() , calls the API.
Algorithm 1 Ô¨Ånds this method as the only method under test
and assigns a high priority to it. To call doIt() , the test
generator requires an object of type A, which in turn requires
objects of types BandC. Our algorithm Ô¨Ånds the constructors
of these classes as parameter providers and gives them
a medium priority. The API call in doIt() results in a
NullPointerException unless init() is called beforehand.
This method is found to be a state changer and also gets a
non-zero priority. The remaining two methods, B.m() and
D(), do not contribute to calling the API and get a priority of
zero. Given the priorities, the random test generator selects
only methods that eventually contribute to calling the API
and probabilistically leads to more calls to the API than a
purely random approach. We show this claim to be valid for
real-world programs and APIs in Section IV-C.
IV. E VALUATION
Our evaluation was driven by two main questions.
1)How effective is our approach in Ô¨Ånding unsafe API
usages? We Ô¨Ånd 54 unsafe API usages in ten well-
tested Java programs. Manual inspection shows all of
them to be true positives.
2)How much does the testing effort reduce compared to a
purely random approach by guiding test generation to-
wards an API? Our heuristic optimization reveals bugs
with Ô¨Åve times fewer generated call sequences than the
unguided approach, on average. This improvement isTable I
ANALYZED PROGRAMS ,NUMBER OF FAILING TESTS (OUT OF 10,000),
AND API USAGE BUGS FOUND .
Program Classes Failing tests Bugs found
(Average per run) Average per run Total
Coll/Iter Vector/Enum Coll/Iter Vector/Enum
antlr 199 1,429 1,695 0 0 0
bloat 331 4,098 4,042 0 0 0
chart 481 2,336 2,396 1.4 1 4
eclipse 384 2,817 3,375 0 0 0
fop 969 3,148 2,948 3.2 2.8 9
hsqldb 383 2,246 2,264 0 0.1 1
jython 890 4,314 5,169 1.4 0 2
lucene 316 3,573 3,372 1.9 1.1 5
pmd 524 2,155 4,158 12.8 5.9 16
xalan 535 4,440 4,395 6.6 5.9 17
Sum 5,012 3,056 3,381 27.3 16.8 54
Table II
ANALYZED API S.
API Classes
Coll/Iter All public methods of java.util.Collection ,
java.util.Iterator , and their subclasses.
Vector/Enum All public methods of java.util.Vector ,
java.util.Enumeration , and their subclasses.
possible because method prioritization increases the
number of API calls by a factor of 57 and decreases
the sequences required to call a particular API method
by a factor of seven.
A. Setup
We run our analysis on all programs from the DaCapo
benchmarks, version 2006-10-MR2 (Table I) [10]. The anal-
ysis focuses on API classes of the Java library (Table II).
We run the analysis in two modes: unguided andguided
towards an API. Unguided runs use Randoop‚Äôs standard test
generation technique. Guided runs use method prioritization
(Section III). To evaluate guided runs, we run Randoop
for each program-API pair. To evaluate unguided runs, we
run Randoop on each program. Since Randoop is based on
random decisions, we perform each experiment with ten
different random seeds [2]. Unless otherwise mentioned, the
reported results are average values of all ten runs.
Randoop requires a stopping criterion. We use a maximum
number of generated method call sequences of 10,000. We
prefer this criterion over a maximum running time, because
it is easily reproducible, whereas Randoop‚Äôs execution time
depends on the environment used for the experiments.
B. Detection of Protocol Violations
The analysis Ô¨Ånds a total of 54 protocol violations. Each
static source code location with a protocol violation counts
only once, even if it occurs in multiple failing tests. Table Ipublic class MIFDocument {
protected BookComponent bookComponent;
public MIFDocument () {
bookComponent = newBookComponent ();
}
public void setTextRectProp( intleft , inttop ,
intwidth , intheight) {
(bookComponent.curPage ()). curTextRect ()
.setTextRectProp(left , top , width , height );
}
public void createPage () {
bookComponent.pages.add( newPage ());
}
class BookComponent {
ArrayList pages = newArrayList ();
private Page curPage () {
return (Page) pages .get(pages .size()-1);
}
}
}
(a) Incorrect usage of ArrayList infop.
public class InternalTables2 {
protected Iterator iter;
public void _beginCanonical () {
iter = (( TableProvid2)classes ). values (). iterator ();
}
public Object _next () {
if(iter.hasNext ()) {
cur = iter.next ();
return (PyJavaClass)cur;
}
return null ;
}
public void _flushCurrent () {
iter.remove ();
classesDec ((( PyJavaClass)cur). __name__ );
}
}
(b) Incorrect usage of Iterator injython .
Figure 5. Examples of bugs found.
shows the number of violations detected for each program-
API pair, along with the total number of violations found
over all runs. The table also shows the number of failing tests
in which the protocol checker Ô¨Ånds the bugs. On average,
3,219 of 10,000 tests fail‚Äîtoo much for a programmer to
inspect. The protocol checker Ô¨Ålters these tests and presents
only those that expose a protocol bug.
We inspect all protocol violations manually to verify that
no false positives are reported. Indeed, all reported violations
show an illegal API usage that can be triggered via the
public methods of a class in the program. Furthermore, we
manually check whether the comments of the buggy methods
contain any natural language warnings about the exceptions
that may result from these methods. None of the reported
methods has such a comment. That is, all reported bugs are
real problems in the programs.
Figure 5 shows two examples of bugs found. The
Ô¨Årst example, Figure 5a, was found in fop. The class
MIFDocument contains an inner class BookComponent
that stores pages in a list, which initially is empty. Theinner class provides a method curPage() that tries to
return the last page in the list. This method is called in
the public method setTextRectProp() , which raises an
IndexOutOfBoundsException when the list of pages is
empty. This risk is not apparent to clients of MIFDocument ,
and hence, the class uses ArrayList in an unsafe way.
Our analysis Ô¨Ånds this bug because the mined protocol for
ArrayList disallows get() directly after creating the list.
The second example, Figure 5b, is taken from jython .
The class has an Iterator Ô¨Åeld that is accessed by
multiple public methods. One of them, _flushCurrent() ,
calls remove() on the iterator. The iterator protocol
allows this call exactly once after each call to next() .
This constraint is not respected in the class, causing
a potential IllegalStateException when calling
_flushCurrent() . Similar to the Ô¨Årst example, the class
uses an API in an unsafe way and does not make the
potential problem apparent to its clients. Our analysis infers
the iterator protocol and Ô¨Ånds this violation of it.
Descriptions of all detected bugs are available at http:
//mp.binaervarianz.de/icse2012-leveraging.
An obvious question about bug reports is how to Ô¨Åx the
bugs. During our experiments, we have seen two strategies
to ensure safe usage of the API and conformance to its
protocols. First, an API-using method m() can ensure to
follow the protocols of all API objects that m() uses and
not to propagate any exception from the API to callers of
m(). Often, such methods return a special value, for example
null , when they cannot return another value. Second, an
API-using method m() can propagate API exceptions and
declare this in its signature. This approach indicates to
callers of m() that calling the method at an illegal state
or with illegal input may result in an exception, and hence,
m() explicitly passes the responsibility on to its callers.
Interestingly, many of the detected bugs are surrounded by
attempts to shield users of a class from protocol violations.
For example in Figure 5b, the _next() method checks by
calling hasNext() whether a call to next() is legal and
returns null otherwise. That is, the programmers protect
clients of the class from some violations of the iterator
protocol but, unfortunately, forgot about another violation.
Comparison to Existing Approaches: We directly com-
pare our analysis to OCD [12], a combined dynamic pro-
tocol miner and checker. OCD also searches for protocol
violations by mining and checking protocols at runtime. Two
main differences are that OCD relies on existing input to
drive the program and that the analysis combines mining and
checking into a single run of the program under test. Gabel
and Su used OCD to analyze the usage of the Java standard
library in the DaCapo programs. Their technique reports
three warnings, matching Gabel and Su‚Äôs expectation to
Ô¨Ånd few problems in these well-tested benchmarks. Manual
inspection showed two warnings to be clear false positives
and one warning to be a potential problem.Applied to the same programs, our analysis reports 54
warnings that are all true positives. What are the reasons
for these differences? First, our analysis does not report
false positives by construction, as explained in Section II-D.
Second, OCD is limited to the program paths triggered
by available input (here, DaCapo‚Äôs benchmark input). In
contrast, the generated tests used by our analysis exercise
less frequently tested parts of the programs and trigger API
usage bugs not exposed with the benchmark inputs.
C. API-Guided Test Generation
The goal of guiding test generation towards an API is
to reduce the overall time required to reveal API usage
bugs. The most important factor for reducing this time is
the number of generated call sequences, which controls the
running times of test generator, protocol miner, and protocol
checker. Therefore, we use the number of generated call
sequences as a metric for testing effort.
We compare guided runs of our analysis to unguided
runs (Table III). Do guided runs trigger more API calls
than unguided runs within a Ô¨Åxed number of generated call
sequences? The Ô¨Årst block of columns in Table III compares
the number of calls to an API. Each value is the average over
ten runs of Randoop with different random seeds. For all but
one program-API pair, guided runs trigger more API calls.
For example, the number of calls to Vector/Enum triggered
in tests for antlr increases from 6,146 to 58,438. On average
over all programs, APIs, and runs, guidance improves the
number of API calls by 56.7x (median: 5.2x).
Increasing the number of API calls is important to give
the miner different examples to learn from and to increase
the chance to hit a bug. It is also important to trigger
calls to different API methods to expose a broad variety
of API protocols and their violations. The second block
of columns in Table III compares the number of distinct
API methods called. In most cases, there is no signiÔ¨Åcant
difference between guided and unguided runs. The reason
is that our stopping criterion is large enough to give even
unguided runs a realistic chance to hit each API method.
How long does it take the test generator to hit an API
method? The third block of columns in Table III shows after
how many generated sequences an API method is called the
Ô¨Årst time. For all but one program-API pair, API methods
are called earlier in guided runs than in unguided runs.
On average over all programs, APIs, and runs, the guided
approach calls a method 6.9 times faster than the unguided
approach. That is, even though after enough time the same
set of API methods is called, guided runs achieve this goal
much faster than unguided runs.
Figure 6a illustrates an unguided and a guided run. The
graphs show the cumulative number of API calls and the
number of distinct called API methods depending on how
many sequences Randoop has generated, respectively. The
Ô¨Årst graph shows that the number of API calls increasesTable III
COMPARISON OF UNGUIDED AND GUIDED TEST GENERATION . THE LAST ROW SUMMARIZES THE RESULTS FROM ALL RUNS .
Program API calls Called API methods Sequences to Ô¨Årst call
Coll/Iter Vector/Enum Coll/Iter Vector/Enum Coll/Iter Vector/Enum
Ung.. Guid. Ung.. Guid. Ung.. Guid. Ung.. Guid. Ung.. Guid. Ung.. Guid.
antlr 3,068 7,915 6,146 58,438 3 3 11 11 930 510 651 188
bloat 33,363 47,269 0 0 62 62 0 0 686 470 0 0
chart 1,564,161 2,356,751 278 19,761 77 75 5 6 624 236 5,472 720
eclipse 33,334 164,051 29,285 152,103 45 40 20 20 506 348 804 512
fop 12,291 59,220 1,163 22,078 46 46 12 12 1,016 601 1400 260
hsqldb 1,549 50,730 37,479 656,172 12 28 12 14 2,503 275 1,238 105
jython 261,276 236,718 36,474 122,400 105 89 18 18 444 802 1,414 575
lucene 70,884 136,490 66,169 97,585 38 37 12 12 561 537 942 1,861
pmd 16,371 134,074 2,934 450,228 91 93 19 21 1,264 638 6,051 2,167
xalan 22,289 116,225 38,230 106,600 9 9 17 18 940 459 802 376
Avg./Med. Improvement: x56.7 / x5.2 Improvement: x1.0 / x1.0 Reduction: x6.9 / x1.9
 0 10000 20000 30000 40000 50000 60000 70000
 0  2500  5000  7500  10000API calls triggered
Generated call sequencesUnguided Guided
 0 2 4 6 8 10 12 14 16 18
 0  2500  5000  7500  10000API methods called
Generated call sequences
(a) Effects of guiding test generation using xalan and Vector/Enum as an example.
x1x10x20x30
 0 10 20 30 40 50 60 70 80 90Improvemt. of seqs. to bug
Runs (sorted by improvement)x74x109
Average: x5.0
Median: x1.2 (b) Improvement of guided runs over unguided runs in
the number of call sequences required to trigger a bug.
Figure 6. Graphical comparison of guided and unguided test generation.
much faster with guidance than without guidance. The
second graph illustrates that the guided run triggers API
methods much faster than the unguided run, even though
both arrive roughly at the same number of API methods
after about 4,500 sequences.
Finally, does guided test generation trigger bugs earlier
than unguided test generation? Figure 6b compares the
number of generated sequences required to trigger a bug.
The graph shows the improvement through guidance for each
run where a bug was found both with and without guidance.
In most of the runs, bugs are triggered faster with guidance,
with up to 109x improvement. For some runs, our heuristic
triggers bugs slower, with a slowdown up to 8x. On average,
guided runs improve upon unguided runs by 5x.
There is one outlier in Table III: For jython and the Coll/-
Iter API, guidance decreases the number of API calls and
increases the sequences needed to trigger an API method.
The reason is that almost all methods (91%) in jython are
relevant for using the Coll/Iter API, which skews the method
priorities. After all, our guidance technique is a heuristic
that, even though successful in most cases, cannot guarantee
improvements for all programs and APIs.D. Scalability and Performance
The total execution time of our analysis is the sum of three
components: the time for generating tests and executing
them, the time for mining execution traces, and the time
for checking execution traces. All three scale well to large
programs. Randoop‚Äôs execution time is the product of the
number of sequences to generate and some program-speciÔ¨Åc
factor (because Randoop executes code from the program).
The performance of the protocol miner scales linearly with
the size of the execution traces [5], which is also true for the
checker because it builds upon the same infrastructure. On
a standard PC, analyzing a program-API pair takes between
less than a minute and several minutes, which we consider
to be acceptable for an automatic testing tool.
V. D ISCUSSION
Our analysis reveals unsafe API usages in classes of a
program but does not guarantee that a crash is feasible
when using the program as a whole, for example, via its
graphical user interface. Nevertheless, all reported bugs are
real problems because a programmer can trigger them via the
public interface of a class. That is, even if a bug is infeasiblein the current program, changes in using the buggy class
can make it feasible. Programmers can avoid this risk by
ensuring safe API usage.
Currently, the analysis is limited to detecting violations
of API protocols. Future work may apply the approach to
other kinds of dynamically inferred speciÔ¨Åcations, such as
invariants [23], [24]. Another limitation is that the analysis
focuses on bugs that manifest through an exception. While
this approach guarantees all bugs to be severe, we might
miss more subtle problems that lead to incorrect but not
obviously wrong behavior.
VI. R ELATED WORK
A. Bug Finding with Mined SpeciÔ¨Åcations
Mined speciÔ¨Åcations can be used for bug Ô¨Ånding
by searching for anomalies within the extracted
speciÔ¨Åcations [25]‚Äì[28] or by feeding the speciÔ¨Åcations into
program checkers [14], [29]. In contrast to our work, these
approaches either require input to run a program, produce
false positives, or both.
We present a static bug Ô¨Ånding technique based on mined
protocols in [29]. It can analyze those parts of a program that
are not covered by generated tests but reports false positives
and does not address the problem of exercising programs
for protocol mining.
B. Test Generation
This work builds upon random test generation in gen-
eral [1], [3], and the feedback-directed random test generator
Randoop [2] in particular. The bugs found by our analysis
complement the bugs reported by Randoop based on built-
in, generic test oracles. Our approach extends the built-in
oracles with inferred protocols that reveal problems Randoop
is oblivious of otherwise.
Jaygarl et al. [30] modify Randoop‚Äôs random method
selection by creating parameters before calling a method and
by favoring methods with low coverage in the existing tests.
Zheng et al. [31] propose to select methods that are likely to
affect the method under test, where inÔ¨Çuence between two
methods is estimated based on whether the methods access
a common Ô¨Åeld. These approaches increase the coverage
of methods under test, whereas our guidance technique
intensiÔ¨Åes the API usage of the tested program. In contrast to
them, our guidance technique is more general, as it considers
both parameters needed to call a method and methods that
may inÔ¨Çuence the state of objects. A technique for regression
testing by Jin et al. [32] guides Randoop towards methods
that have been modiÔ¨Åed between two versions.
Test generation can be enhanced by mining call sequence
models [33]‚Äì[36]. Xie and Notkin [37] propose to com-
bine test generation and speciÔ¨Åcation mining for iteratively
enhancing both activities. Dallmeier et al. [38] concretize
this idea by generating tests that explore not yet speciÔ¨Åed
sequences of a protocol and by using the result of executingthese tests to reÔ¨Åne the protocol. The main goal of these
approaches is to enhance test generation with speciÔ¨Åcation
mining, or vice versa, whereas we combine test generation
and speciÔ¨Åcation mining into an automatic bug Ô¨Ånding
technique.
Xie and Notkin integrate invariant inference with test
generation to guide the test generator and to select generated
tests that are more likely to expose a bug [39]. In contrast
to their work, our approach does not require an existing test
suite and reports bugs without false positives.
C. Comparing Passing and Failing Tests
Two kinds of approaches leverage the differences between
passing and failing tests. The Ô¨Årst group of approaches com-
pares features of passing and failing tests to identify classes
and methods that are likely to cause a known failure [40]‚Äì
[42]. The second group of approaches compares passing and
failing tests to generate Ô¨Åxes for a known problem [43], [44].
In contrast to both approaches, our work discovers unknown
bugs and leverages test generation instead of relying on
existing input, such as manually written tests.
D. Runtime Protocol Checking
The checking step of our analysis is closely related to
runtime veriÔ¨Åcation of protocols [7]‚Äì[9]. Our checker is
conceptually similar to the online checker JavaMOP [9]
but works on execution traces and thus avoids some of
the challenges faced by JavaMOP, for example, because we
know which objects interact during an execution.
VII. C ONCLUSIONS
This paper shows how to integrate test generation, speci-
Ô¨Åcation mining, and runtime checking into a fully automatic
bug Ô¨Ånding technique. We draw several conclusions. First,
generated tests are not only useful to reveal bugs, but they
can also serve as input to drive other dynamic analyses. This
allows our approach to beneÔ¨Åt from the precision of dynamic
analysis without requiring any input to run a program.
Second, leveraging passing and failing tests for mining and
checking speciÔ¨Åcations, respectively, allows for detecting
bugs without reporting false positives. Third, focusing the
efforts spent during random test generation on those parts
of a program relevant for the problem at hand increases the
effectiveness of the overall approach, compared to purely
random test generation.
ACKNOWLEDGMENTS
Thanks to Eric Bodden, Mark Gabel, Patrick Meredith,
Albert Noll, Grigore Rosu, and the anonymous reviewers
for their valuable comments on this work, as well as to
Carlos Pacheco and Michael Ernst for providing Randoop.
This work was partially supported by the Swiss National
Science Foundation under grant number 200021-134453.REFERENCES
[1] C. Csallner and Y . Smaragdakis, ‚ÄúJCrasher: an automatic
robustness tester for Java,‚Äù Software Pract Exper , vol. 34,
no. 11, pp. 1025‚Äì1050, Sep. 2004.
[2] C. Pacheco, S. K. Lahiri, M. D. Ernst, and T. Ball, ‚ÄúFeedback-
directed random test generation,‚Äù in ICSE , 2007, pp. 75‚Äì84.
[3] I. Ciupa, A. Leitner, M. Oriol, and B. Meyer, ‚ÄúARTOO:
adaptive random testing for object-oriented software,‚Äù in
ICSE , 2008, pp. 71‚Äì80.
[4] G. Ammons, R. Bod ¬¥ƒ±k, and J. R. Larus, ‚ÄúMining speciÔ¨Åca-
tions,‚Äù in POPL , 2002, pp. 4‚Äì16.
[5] M. Pradel and T. R. Gross, ‚ÄúAutomatic generation of object
usage speciÔ¨Åcations from large method traces,‚Äù in ASE, 2009,
pp. 371‚Äì382.
[6] M. Pradel, P. Bichsel, and T. R. Gross, ‚ÄúA framework for
the evaluation of speciÔ¨Åcation miners based on Ô¨Ånite state
machines,‚Äù in ICSM , 2010, pp. 1‚Äì10.
[7] C. Allan, P. Avgustinov, A. S. Christensen, L. Hendren,
S. Kuzins, O. Lhot ¬¥ak, O. de Moor, D. Sereni, G. Sittampalam,
and J. Tibble, ‚ÄúAdding trace matching with free variables to
AspectJ,‚Äù in OOPSLA , 2005, pp. 345‚Äì364.
[8] M. C. Martin, V . B. Livshits, and M. S. Lam, ‚ÄúFinding
application errors and security Ô¨Çaws using PQL: A program
query language,‚Äù in OOPSLA , 2005, pp. 365‚Äì383.
[9] F. Chen and G. Rosu, ‚ÄúMOP: An efÔ¨Åcient and generic runtime
veriÔ¨Åcation framework,‚Äù in OOPSLA , 2007, pp. 569‚Äì588.
[10] S. M. Blackburn et al. , ‚ÄúThe DaCapo benchmarks: Java
benchmarking development and analysis,‚Äù in OOPSLA , 2006,
pp. 169‚Äì190.
[11] T. A. Henzinger, R. Jhala, and R. Majumdar, ‚ÄúPermissive
interfaces,‚Äù in ESEC/FSE , 2005, pp. 31‚Äì40.
[12] M. Gabel and Z. Su, ‚ÄúOnline inference and enforcement of
temporal properties,‚Äù in ICSE , 2010, pp. 15‚Äì24.
[13] H. Zhong, L. Zhang, T. Xie, and H. Mei, ‚ÄúInferring resource
speciÔ¨Åcations from natural language API documentation,‚Äù in
ASE, 2009, pp. 307‚Äì318.
[14] J. Whaley, M. C. Martin, and M. S. Lam, ‚ÄúAutomatic ex-
traction of object-oriented component interfaces,‚Äù in ISSTA ,
2002, pp. 218‚Äì228.
[15] S. Shoham, E. Yahav, S. Fink, and M. Pistoia, ‚ÄúStatic speciÔ¨Å-
cation mining using automata-based abstractions,‚Äù in ISSTA ,
2007, pp. 174‚Äì184.
[16] M. Gabel and Z. Su, ‚ÄúJavert: Fully automatic mining of
general temporal properties from dynamic traces,‚Äù in FSE,
2008, pp. 339‚Äì349.
[17] D. Lorenzoli, L. Mariani, and M. Pezz `e, ‚ÄúAutomatic gen-
eration of software behavioral models,‚Äù in ICSE , 2008, pp.
501‚Äì510.
[18] C. Lee, F. Chen, and G. Rosu, ‚ÄúMining parametric speciÔ¨Åca-
tions,‚Äù in ICSE , 2011, pp. 591‚Äì600.
[19] I. Ciupa, A. Pretschner, A. Leitner, M. Oriol, and B. Meyer,
‚ÄúOn the predictability of random tests for object-oriented
software,‚Äù in ICST , 2008, pp. 72‚Äì81.
[20] A. Bessey, K. Block, B. Chelf, A. Chou, B. Fulton, S. Hallem,
C. Henri-Gros, A. Kamsky, S. McPeak, and D. R. Engler, ‚ÄúA
few billion lines of code later: Using static analysis to Ô¨Ånd
bugs in the real world,‚Äù Commun ACM , vol. 53, no. 2, pp.
66‚Äì75, 2010.
[21] C. Pacheco, S. K. Lahiri, and T. Ball, ‚ÄúFinding errors in .NET
with feedback-directed random testing,‚Äù in ISSTA , 2008, pp.
87‚Äì96.
[22] R. Vall ¬¥ee-Rai, P. Co, E. Gagnon, L. J. Hendren, P. Lam,
and V . Sundaresan, ‚ÄúSoot - a Java bytecode optimization
framework,‚Äù in CASCON , 1999, pp. 125‚Äì135.[23] M. D. Ernst, J. Cockrell, W. G. Griswold, and D. Notkin,
‚ÄúDynamically discovering likely program invariants to sup-
port program evolution,‚Äù IEEE T Software Eng , vol. 27, no. 2,
pp. 213‚Äì224, 2001.
[24] S. Hangal and M. S. Lam, ‚ÄúTracking down software bugs
using automatic anomaly detection,‚Äù in ICSE , 2002, pp. 291‚Äì
301.
[25] A. Wasylkowski and A. Zeller, ‚ÄúMining temporal speciÔ¨Åca-
tions from object usage,‚Äù in ASE, 2009, pp. 295‚Äì306.
[26] S. Thummalapenta and T. Xie, ‚ÄúMining exception-handling
rules as sequence association rules,‚Äù in ICSE , 2009, pp. 496‚Äì
506.
[27] T. T. Nguyen, H. A. Nguyen, N. H. Pham, J. M. Al-Kofahi,
and T. N. Nguyen, ‚ÄúGraph-based mining of multiple object
usage patterns,‚Äù in ESEC/FSE , 2009, pp. 383‚Äì392.
[28] M. Monperrus, M. Bruch, and M. Mezini, ‚ÄúDetecting missing
method calls in object-oriented software,‚Äù in ECOOP , 2010,
pp. 2‚Äì25.
[29] M. Pradel, C. Jaspan, J. Aldrich, and T. R. Gross, ‚ÄúStatically
checking API protocol conformance with mined multi-object
speciÔ¨Åcations,‚Äù in International Conference on Software En-
gineering (ICSE) , 2012.
[30] H. Jaygarl, K.-S. Lu, and C. K. Chang, ‚ÄúGenRed: A tool
for generating and reducing object-oriented test cases,‚Äù in
COMPSAC , 2010, pp. 127‚Äì136.
[31] W. Zheng, Q. Zhang, M. R. Lyu, and T. Xie, ‚ÄúRandom unit-
test generation with MUT-aware sequence recommendation,‚Äù
inASE, 2010, pp. 293‚Äì296.
[32] W. Jin, A. Orso, and T. Xie, ‚ÄúAutomated behavioral regression
testing,‚Äù in ICST , 2010, pp. 137‚Äì146.
[33] S. Thummalapenta, T. Xie, N. Tillmann, J. de Halleux, and
W. Schulte, ‚ÄúMSeqGen: Object-oriented unit-test generation
via mining source code,‚Äù in ESEC/FSE , 2009, pp. 193‚Äì202.
[34] T. Xie and D. Notkin, ‚ÄúAutomatic extraction of object-
oriented observer abstractions from unit-test executions,‚Äù in
ICFEM , 2004, pp. 290‚Äì305.
[35] S. Thummalapenta, J. de Halleux, N. Tillmann, and
S. Wadsworth, ‚ÄúDyGen: Automatic generation of high-
coverage tests via mining gigabytes of dynamic traces,‚Äù in
TAP, 2010, pp. 77‚Äì93.
[36] S. Zhang, D. Saff, Y . Bu, and M. D. Ernst, ‚ÄúCombined static
and dynamic automated test generation,‚Äù in ISSTA , 2011, pp.
353‚Äì363.
[37] T. Xie and D. Notkin, ‚ÄúMutually enhancing test generation
and speciÔ¨Åcation inference,‚Äù in FATES , 2003, pp. 60‚Äì69.
[38] V . Dallmeier, N. Knopp, C. Mallon, S. Hack, and A. Zeller,
‚ÄúGenerating test cases for speciÔ¨Åcation mining,‚Äù in ISSTA ,
2010, pp. 85‚Äì96.
[39] T. Xie and D. Notkin, ‚ÄúTool-assisted unit test selection based
on operational violations,‚Äù in ASE, 2003, pp. 40‚Äì48.
[40] M. Renieris and S. P. Reiss, ‚ÄúFault localization with nearest
neighbor queries,‚Äù in ASE, 2003, pp. 30‚Äì39.
[41] V . Dallmeier, C. Lindig, and A. Zeller, ‚ÄúLightweight defect
localization for Java,‚Äù in ECOOP , 2005, pp. 528‚Äì550.
[42] L. Mariani, F. Pastore, and M. Pezz `e, ‚ÄúDynamic analysis for
diagnosing integration faults,‚Äù IEEE Trans Sw Eng , vol. 37,
no. 4, pp. 486‚Äì508, 2011.
[43] W. Weimer, T. Nguyen, C. Le Goues, and S. Forrest, ‚ÄúAu-
tomatically Ô¨Ånding patches using genetic programming,‚Äù in
ICSE , 2009, pp. 363‚Äì374.
[44] Y . Wei, Y . Pei, C. A. Furia, L. S. Silva, S. Buchholz, B. Meyer,
and A. Zeller, ‚ÄúAutomated Ô¨Åxing of programs with contracts,‚Äù
inISSTA , 2010, pp. 61‚Äì72.