Potential Biases in Bug Localization: Do They Matter?
Pavneet Singh Kochhar, Yuan Tian and David Lo
School of Information Systems
Singapore Management University
{kochharps.2012, yuan.tian.2012, davidlo}@smu.edu.sg
ABSTRACT
Issue tracking systems are valuable resources during software
maintenance activities and contain information about the
issues faced during the development of a project as well as
after its release. Many projects receive many reports of bugs
and it is challenging for developers to manually debug and x
them. To mitigate this problem, past studies have proposed
information retrieval (IR)-based bug localization techniques,
which takes as input a textual description of a bug stored
in an issue tracking system, and returns a list of potentially
buggy source code les.
These studies often evaluate their eectiveness on issue
reports marked as bugs in issue tracking systems, using as
ground truth the set of les that are modied in commits
that x each bug. However, there are a number of potential
biases that can impact the validity of the results reported in
these studies. First, issue reports marked as bugs might not
be reports of bugs due to error in the reporting and classi-
cation process. Many issue reports are about documentation
update, request for improvement, refactoring, code cleanups,
etc. Second, bug reports might already explicitly specify the
buggy program les and for these reports bug localization
techniques are not needed. Third, les that get modied in
commits that x the bugs might not contain the bug.
This study investigates the extent these potential biases
aect the results of a bug localization technique and whether
bug localization researchers need to consider these potential
biases when evaluating their solutions. In this paper, we
analyse issue reports from three dierent projects: HTTP-
Client, Jackrabbit, and Lucene-Java to examine the impact
of above three biases on bug localization. Our results show
that one of these biases signicantly and substantially im-
pacts bug localization results, while the other two biases
have negligible or minor impact.
Categories and Subject Descriptors
D.2.7 [ Software ]: Software Engineering| Distribution, Main-
tenance, and Enhancement
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proÔ¨Åt or commercial advantage and that copies bear this notice and the full cita-
tion on the Ô¨Årst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciÔ¨Åc permission
and/or a fee. Request permissions from permissions@acm.org.
ASE‚Äô14, September 15-19, 2014, Vasteras, Sweden.
Copyright 2014 ACM 978-1-4503-3013-8/14/09 ...$15.00.
http://dx.doi.org/10.1145/2642937.2642997.General Terms
Experimentation
Keywords
Issue Reports, Bug Localization, Bias, Empirical Study
1. INTRODUCTION
Issue tracking systems, which contains information related
to issues faced during the development as well as after the
release of a software project, is an integral part of software
development activity. Issue tracking systems such as JIRA
or Bugzilla can help reporters report various kinds of is-
sues such as bug reports, documentation update, refactor-
ing request, addition of new feature and so on. Well-known
projects often receive large number of issue reports which
might be dicult for developers to handle. A mozilla de-
veloper accepted that the project receives over 300 bugs per
day which needs triaging [5]. Therefore, it is important to
have techniques which can help developers nd buggy les
quickly, which can help them resolve the bug faster.
To overcome the above issue, researchers have proposed
techniques which use information given in the bug report
to identify source code les that contain the bug [20, 21,
26]. These techniques often use standard information re-
trieval (IR) techniques to compute the similarity between
the textual description of bug report and textual descrip-
tion of source code. Based on the similarity scores, these
IR-based bug localization techniques return a ranked list
of source code les which are likely to be buggy for that
bug report. These techniques are evaluated using closed
and xed issue reports marked as bugs collected from issue
tracking systems. The evaluation involves comparison of
les returned by bug localization techniques with the actual
les changed to x the bug. Past studies indicate that the
performance of these techniques are promising { up to 80%
of bug reports can be localized by just inspecting 5 source
code les [21].
Despite the promising results of IR-based bug localization
approaches, a number of potential biases can aect the va-
lidity of results reported in prior studies. If these biases
signicantly aect the results of bug localization studies, fu-
ture researchers need to put more care in cleaning their eval-
uation datasets when evaluating the performances of their
techniques. In this work, we focus on investigating three
potential biases:
1.Wrongly Classied Reports. Herzig et al. reported
that many issue reports in issue tracking systems are
803
wrongly classied [12]. About one third of all issue
reports marked as bugs are not really bugs. Herzig et
al. have shown that this potential bias signicantly af-
fects bug prediction studies that predict whether a le
is potentially buggy or not based on the history of prior
bugs. This potential bias might aect bug localization
studies too as the characteristics of bug reports and
other issues, e.g., refactoring requests, can be very dif-
ferent. Refactoring can touch a large number of les,
while bug xes are often more localized [16, 24]. Thus,
there is a need to investigate whether wrongly classi-
ed reports signicantly skew eectiveness results of
bug localization approaches.
2.Already Localized Reports. Our manual investi-
gation of a number of bug reports nd that the textual
descriptions of many reports have already specied the
les that contain the bug. These localized reports do
not require bug localization approaches. The buggy
les are already localized and only need to be xed.
Evaluating bug localization approaches with these lo-
calized reports will unfairly inate the eectiveness re-
sults. Thus, there is a need to investigate how common
are localized reports and whether their presence in the
evaluation data set can signicantly skew the eective-
ness results of bug localization approaches.
3.Incorrect Ground Truth Files. Kawrykow and Ro-
billard reported that many changes made to source
code les are non-essential changes [14]. These non-
essential changes include cosmetic changes made to
source code which do not aect the behavior of sys-
tems. Past fault localization studies often use as ground
truth source code les that are touched by commits
that x the bugs [21, 26]. However, no manual inves-
tigation was done to check if these les are aected
by essential or non-essential changes. Files that are
aected by non-essential changes should be excluded
from the ground truth les as they do not contain the
bug. Including these non-essential changes as ground
truth les can unfairly inate the eectiveness results
{ with more ground truth les, there is a higher chance
that one of them will be identied by a bug localization
tool. Thus, there is a need to investigate how common
are the incorrect ground truth les and whether their
presence in the evaluation data set can signicantly
skew the eectiveness results of bug localization ap-
proaches.
To investigate the impact of the above mentioned possi-
ble biases, in this study, we analyse the following research
questions:
RQ1: What are the eects of wrongly classied issue
reports on bug localization?
RQ2: What are the eects of localized bug reports on
bug localization?
RQ3: What are the eects of wrongly identied ground
truth les on bug localization?
The rst research question has been partly answered in
our short paper in MSR 2014 [15]. We extend our short
paper by performing a per-project analysis instead of an all-
project analysis; we also employ an additional test to shedmore light on the research question. The other two research
questions have not been investigated in prior work.
To answer the above research questions, we reuse the man-
ually categorized issue reports dataset of Herzig et al. [12].
Herzig et al.'s dataset consists of issue reports from Bugzilla
and JIRA issue tracking systems. Several studies in the
past have shown that the bug reports in Bugzilla are poorly
linked [6, 8, 25], whereas bug reports in JIRA are well linked
as JIRA provides add-ons to help connect issues to commits
in the version control systems [9]. Therefore, we only use
projects from Herzig et al.'s dataset that use JIRA. We in-
vestigate 5,591 issue reports stored in issue tracking systems
of three projects: HTTPClient, Jackrabbit and Lucene-Java.
HTTPClient is a Java library for implementing the client
side of the most recent HTTP standards and recommen-
dations [1]. Jackrabbit is a content repository written in
Java [2]. Lucene-Java is a high-performance search engine
library written in Java [3]. Table 1 shows the dataset we use
for this study.
Table 1: Project Details
Project Issue Tracker #Issue Reports
HTTPClient JIRA 746
Jackrabbit JIRA 2,402
Lucene-Java JIRA 2,443
These reports have been manually classied by Herzig
et al. into bug reports and other kinds of issues [12]. We
perform suitable statistical tests (Mann-Whitney-Wilcoxon
test [17] and Fisher exact test [11]) and compute eect size
measure (Cohen's d [10]) to investigate whether the skew
introduced by the above potential biases matters. We nd
that some biases do not signicantly or substantially aect
bug localization techniques performance. However, one of
them signicantly and substantially aect bug localization
techniques performance and researchers need to consider this
bias by cleaning their evaluation dataset to correctly mea-
sure the eectiveness of bug localization solutions.
The contributions of this paper are as follows:
1. We extend our preliminary study [15], to analyze the
eect of wrongly classied issue reports on the eec-
tiveness of bug localization tools.
2. We analyze the eect of localized bug reports on the
eectiveness of bug localization tools. To do this, we
manually investigate bug reports and categorize them
as localized, partially localized, or non-localized. We
also build an automated technique that can catego-
rize bug reports into these three categories with high
accuracy.
3. We analyze the eect of incorrect ground truth les
on the eectiveness of bug localization tools. To do
this, we manually investigate source code les that are
touched by commits to x various bugs and detect ir-
relevant les that do not contain bugs.
4. We release a clean dataset that researchers can use to
evaluate future bug localization techniques.
The structure of this paper is as follows. In Section 2,
we give a brief summary of IR-based bug localization tech-
niques. In Sections 3, 4 and 5, we investigate RQ1, RQ2,
804and RQ3 respectively. We discuss other interesting ndings
and threats to validity in Section 6. Related work is dis-
cussed in Section 7. We nally conclude and mention future
work in Section 8.
2. BUG LOCALIZATION: A PRIMER
In Recent years, a number of studies have proposed infor-
mation retrieval based approaches to automatically locate
bugs in source code les. They take bug reports as input
queries and source code les as input documents and out-
put a list of relevant source code les to a particular bug
report. The major goal is to nd a well-designed metrics
to capture the similarity between bug reports and source
code les. Many techniques have been proposed to compute
such similarity based on textual information stored in bug
reports and source code, e.g., words appear in bug reports
and source code les.
In Section 2.1 we describe a general bug localization frame-
work and highlight one popular technique based on Vector
Space Model (VSM). In Section 2.2, we describe how bug
localization studies often obtain ground truths to evaluate
the eectiveness of their techniques when applied on bug re-
ports. In Section 2.3, we describe a popular metric to mea-
sure the eectiveness of bug localization techniques namely
Mean Average Precision (MAP).
2.1 General Framework
The general framework contains three major steps: code
corpus pre-processing and indexing, bug report pre-processing,
and retrieval and ranking. We describe the details of each
step as follows.
Step 1: Code Corpus Pre-processing and Indexing.
We apply three preprocessing steps to process code corpus
(i.e., source code les): normalization, stop word removal,
and stemming. In the normalization step, we rst extract
comments, identiers, and string literals from source code
les. Tools like Eclipse JDT1can be used to extract Ab-
stract Syntax Trees (ASTs) that can then be traversed to ex-
tract these textual contents from source code les. Next, we
remove punctuation marks, special symbols, and number lit-
erals from the extracted text. We also convert all words into
lower case. In the stop word removal step, we rst remove
commonly occurring English words (e.g., \I", \you", \we",
\are", etc.). We use the stop word list from: http://dev.
mysql.com/doc/refman/5.6/en/fulltext-stopwords.html.
We also remove programming language keywords, e.g., pub-
lic,class,if,for, etc. In the stemming step, we apply the
famous Porter Stemming Algorithm2to reduce a word to its
root form. For example, we reduce \mapping", \mapped",
and \maps" to \map".
At the end of the above steps, each of the les in the
corpus are represented by a bag of words. We then index
these bags of words so that one can locate les containing a
particular word eciently.
Step 2: Bug Report Pre-Processing. IR-based bug
localization approaches regard one bug report as a query.
Textual information inside a bug report such as the content
of the summary and description elds are extracted. Ta-
1http://www.eclipse.org/jdt/
2http://tartarus.org/martin/PorterStemmer/ble 2 shows an example bug report with ID JCR-2718 from
Jackrabbit Project along with the contents of its summary
and description elds. After the text of the bug reports
are extracted, we perform several pre-processing steps: to-
kenization, stop-word removal, and stemming. In the tok-
enization step, we convert the textual content of a bug report
into a multi-set (bag) of words that appear in it. This bag
of words are then input to the stop-word removal and stem-
ming steps which are are the same ones that are applied to
pre-process the code corpus.
Table 2: Issue Report JCR-2718 from Jackrabbit
Summary: Incorrect results from joins on multi-
valued properties
Description: It looks like join conditions on multi-
valued properties only use one of the
multiple values for the comparison.
Step 3: Retrieval and Ranking. In this step, given
a bug report, we want to retrieve and rank relevant source
code les. Retrieval and ranking of relevant source code les
is based on the similarity between the bug report and each
of the les in the code corpus. In the IR domain, various
models have been proposed to measure the similarities of a
query (in our case: bug report) with a document (in our
case: a source code le). These include Vector Space Model
(VSM), Smoothed Unigram Model (SUM), Latent Dirichlet
Allocation (LDA), etc.
In this work, we focus on VSM, which is the foundation of
many state-of-the-art IR-based bug localization approaches,
e.g., [21, 26], and has been shown to outperform many other
models [20]. VSM takes the pre-processed bug report as a
query and pre-processed source code les as documents. In
VSM, each query or document is expressed as a vector of
weights, where each weight corresponds to a word that ap-
pears in the query or document. The weight is usually com-
puted using the tf-idf weighting scheme. The tf-idf weight of
word win document dgiven a set of documents D, denoted
astf id f(w; d; D ), is computed as:
tf id f(w; d; D ) =log(f(w; d ) + 1)logjDj
jdi2D:w2dij
(1)
In the above equation, f(w; d ) is the number of times word
woccurs in document d, and di2D:w2direpresents
documents that contain word w. After converting queries
and documents into vectors of weights, VSM computes the
similarity between a query and a document as the cosine
similarity between the two corresponding vectors. Equation
2 shows the function to calculate the cosine similarity be-
tween a query qand a document d.
Similarity (q; d) =cos(q; d ) =VqVd
jVqjjVdj(2)
where VqandVdare vector of term weights for query q
and document d, respectively. VqVdrepresents the inner
product of the two vectors.
2.2 Ground Truth IdentiÔ¨Åcation
IR-based bug localization approaches are often evaluated
on bug reports submitted to various issue tracking systems.
805For each bug report, there is a need to get the ground truth
leswhich are the source code les that contain the bug.
Manual identication of ground truth les would takes a
lot of eort, therefore, researchers have come up with an au-
tomated yet imperfect approach to identify these les. The
underlying idea is to map a bug report to commits in version
control systems that x the bug. Often the identier of the
bug report appears in the logs of the corresponding commits.
After these commits are identied, les that are touched by
one or more of these commits are considered as ground truth
les. Bug localization techniques are then evaluated based
on their ability to recover these ground truth les from the
bug report.
2.3 Evaluation Metric
A bug localization technique outputs a ranked list of les
for every bug report. Given a set of bug reports, the tech-
nique will output a set of ranked lists. A number of metrics
can be used to evaluate the eectiveness of the technique
based on the position of the buggy les in the ranked lists.
One of the most popular metrics is Mean Average Preci-
sion (MAP) which has been used to evaluate many recent
studies [20, 21, 26].
To compute MAP, for each ranked list, we need to com-
pute Average Precision (AP), which is dened as follows:
AP=MX
i=1P(i)rel(i)
#All buggy les(3)
where Mis the number of retrieved source code les, rel(i)
is a binary value that represents whether the ith retrieved
source le is buggy or not. P(i) is the precision at position
iof the ranked list, which is dened as:
P(i) =#Buggy les retrieved in top i positions
i(4)
MAP is then the mean of the average precisions over all
ranked lists produced for the bug reports. The higher the
MAP of a bug localization technique, the more eective is
the technique.
3. BIAS 1: REPORT MISCLASSIFICATION
In this section, we investigate the rst research question:
What are the eects of wrongly classied issue reports on bug
localization? We describe the motivation of answering this
question in Section 3.1, the methodology of our experiments
in Section 3.2, and the results of our experiments which
answer the question in Section 3.3.
3.1 Motivation
Issue tracking systems contain reports of several types of
issues such as bugs, requests for improvement, documenta-
tion, refactoring, etc. Herzig et al. report that a substantial
number of issue reports marked as bugs, are not bugs but
other kinds of issues. Their results show that these misclas-
sications have a signicant impact on bug prediction. In
this question, we want to analyse the consequences of mis-
classication on bug localization.
3.2 Methodology
Step 1: Data Acquisition. We use Herzig et al.'s dataset of
manually analyzed issue reports (www.st.cs.unisaarland.de/-
softevo/bugclassify). We download the issue reports fromthe associated JIRA repositories and extract the textual con-
tents of the summary and description of the reports. After
downloading, we perform the preprocessing steps described
previously. In JIRA, each issue report has a unique identi-
er represented by the project name and a unique number.
For example, HTTPCLIENT-974 represents issue number
974 of project HTTPClient. We use the gitversion control
system of the projects to get the commit log les, which are
used to map issue reports to their corresponding commits.
Commit logs contain unique identier of the issue report as
part of the commit message. We use these mapped commits
to check out the source code les prior to the commits that
address the issue and the source code les when the issue
is resolved. For each source code le, we perform a similar
preprocessing step to represent a le as a bag-of-words.
Step 2: Bug Localization. After the data acquisition, we
have the textual content of the issue reports, the textual
content of each source code le in the revision prior to the
x, and a set of ground truth les that are changed to x
the issue report. We give the textual content of the issue
reports and the revision's source code les as input to the
bug localization technique, which outputs a ranked list of
les sorted based on the similarity to the bug report.
Step 3: Eectiveness Measurement & Statistical Analysis.
After Step 2, for each issue report, we have a ranked list of
source code les and a list of supposed ground truth les.
We compare these two lists to compute the average precision
score.
We divide the issue reports into two categories: issue re-
ports marked as bugs in the tracking system (Reported) and
issue reports that are actual bugs i.e., manually labeled by
Herzig et al. (Actual). In Herzig et al.'s dataset, the set Ac-
tual is a subset of Reported. We compute the MAP scores
and use Mann-Whitney U test to examine the dierence be-
tween these two categories at 0.05 signicance level. We use
Cohen's d to measure the eect size, which is the standard-
ised dierence between two means. To interpret the eect
size, we use the interpretation given by Cohen [10], i.e., d
<0.2 means trivial, 0.20 d<0.5 means small, 0.5 d
<0.8 means medium, 0.80 d<1.3 means large, and d 
1.3 means very large.
Table 3: Mean Average Precision (MAP) Scores for Re-
ported and Actual
Project Reported Actual Dierence d
HTTPClient 0.429 0.419 -2.33% 0.13
Jackrabbit 0.302 0.339 12.25% 0.06
Lucene-Java 0.301 0.322 6.98% 0.04
3.3 Results
Eect of Misclassication on Bug Localization. Ta-
ble 3 shows the MAP scores for the two categories: reports
marked as bugs (Reported) and manually classied bug re-
ports (Actual). We observe that there are dierences of
-2.33%, 12.25% and 6.98% in the MAP scores for HTTP-
Client, Jackrabbit and Lucene-Java, respectively. We per-
form the Mann-Whitney Wilcoxon test and compute Co-
hen's d to examine the dierences between the two cate-
gories. The results are also presented in Table 3. From the
results, we observe that, for HTTPClient and Lucene-Java,
806the dierences are statistically insignicant and the eect
sizes are trivial (i.e., less than 0.2). For Jackrabbit, the
eect size is trivial, however, the dierence is statistically
signicant.
Eect of Dierent Misclassication Types. We now
analyse the misclassication type that has the most im-
pact on the dierence of MAP scores between Reported and
Actual. Herzig et al. classify issue reports into 13 cate-
gories: BUG,RFE,IMPROVEMENT ,DOCUMENTATION ,REFACTOR-
ING,BACKPORT ,CLEANUP ,SPEC,TASK,TEST,BUILD_SYSTEM ,
DESIGN_DEFECT , and OTHERS . We omit issue reports that
are misclassied one category at a time and recalculate the
MAP score. For example, RFEtoBUGrepresents issue reports
which are RFE (Actual) but are misclassied as BUG (Re-
ported). Table 4 shows the MAP scores when we remove
issue reports of particular misclassication types one at a
time. Each row corresponds to a subset of reports where
reports of a misclassication type is removed. We observe
that TEST toBUGhas the largest dierence in the MAP score
followed by misclassication from IMPROVEMENT toBUG.
Table 4: Mean Average Precision (MAP) Scores when
Issue Reports of a Particular Misclassication Type are
Omitted. Omit. = Omitted, Misclass. = Misclassica-
tion, HC = HTTPClient, JB = Jackrabbit, LJ = Lucene-
Java. The last column is the MAP of all three projects.
Omi
t. Misclass. Type HC JB LJOv
erall
(Ac
tual to Reported)
No
ne 0.4
29 0.3
02 0.3
01 0.3
12
RF
EtoBUG 0.4
27 0.3
03 0.3
04 0.3
13
DO
CUMENTATION toBUG 0.4
30.3
04 0.3
05 0.3
15
IM
PROVEMENT toBUG 0.4
16 0.2
99 0.2
95 0.3
07
RE
FACTORING toBUG 0.4
28 0.3
01 0.3
01 0.3
11
BA
CKPORT toBUG 0.4
30.3
03 0.3
00 0.3
13
CL
EANUP toBUG 0.4
29 0.3
03 0.3
03 0.3
14
SP
ECtoBUG 0.4
35 0.3
02 0.3
03 0.3
12
TA
SKtoBUG 0.4
32 0.3
02 0.3
01 0.3
12
TE
STtoBUG 0.4
29 0.3
28 0.3
13 0.3
34
BU
ILD_SYSTEM toBUG 0.4
29 0.3
06 0.3
03 0.3
15
DE
SIGN_DEFECT toBUG 0.4
24 0.3
01 0.3
01 0.3
11
OT
HERS toBUG 0.4
39 0.3
03 0.3
01 0.3
13
Bias 1, which is wrongly classied issue reports, signi-
cantly impacts bug localization result for one out of the
three projects. However, the eect of this bias is negligi-
ble{ the eect sizes are less than 0.2.
4. BIAS 2: LOCALIZED BUG REPORTS
In this section, we investigate the second research ques-
tion: What are the eects of localized bug reports on bug
localization? We describe the motivation of answering this
question in Section 4.1, the methodology of our experiments
in Section 4.2, and the results of our experiments which an-
swer the question in Section 4.3.
4.1 Motivation
Localized bug reports are those whose buggy les have
been identied in the report itself. For these reports, the
remaining task to resolve the bug is simply to x the buggy
les. These bug reports do not benet or require bug lo-
calization solutions. Past studies on bug localization do notseparate localized from non-localized bug reports. In this
research question, we want to investigate the number of lo-
calized bug reports and the impact of including localized
bug reports in the evaluation of bug localization tools. If
bias exists, then future bug localization solutions need to
be careful to perform a data cleaning step to remove these
localized bug reports from their evaluation dataset.
4.2 Methodology
To investigate this research question, we rst need to iden-
tify localized bug reports. We start by manual investigat-
ing of a smaller subset of bug reports and identify localized
ones. We then developed an automated means to nd local-
ized bug reports so that our analysis can scale to a larger
number of bug reports. Finally, we input these reports to
a number of IR-based bug localization tools to investigate
whether localized reports skew the results of bug localization
tools.
Table 5: Fully Localized, Partially Localized, and
Not Localized Reports
Category Description
Fully Bug reports where all the les containing the
bugs are explicitly specied in the report.
Partially Bug reports where some of the les contain-
ing the bugs are explicitly mentioned in the
report.
Not Bug reports which do not explicitly specify
any of the buggy les.
Step 1: Manually Identifying Localized Bug Reports. We
manually analysed 350 issue reports that Herzig et al. la-
beled as bug reports. Out of the 5,591 issue reports from the
three projects, Herzig et al. labeled 1,191 of them as bug
reports. We randomly selected these 350 from the pool of
bug reports from the three software projects. For our man-
ual analysis, we read the summary and description elds of
each bug report. We also collected the corresponding les
changed to x each bug. We classied each bug report into
one the three categories shown in Table 5. Table 6, 7, and 8
show example bug reports that are fully localized, partially
localized, and not localized.
Table 6: Fully Localized Report: HTTPCLIENT-
1078
Summary: DecompressingEntity not calling
close on InputStream retrieved by get-
Content
Description: The method DecompressingEn-
tity.writeTo(OutputStream out-
stream) does not close the InputStream
retrieved by getContent(). According
to the documentation of HttpEn-
tity.writeTo: IMPORTANT: Please
note all entity implementations must
ensure that all allocated resources are
properly deallocated when this method
returns. - >imho this is not satised
inDecompressingEntity .writeTo
Buggy Files: DecompressingEntity.java
807Table 7: Partially Localized Report: JCR-814
Summary: Oracle bundle PM fails checking
schema if 2 users use the same database
Description: When using the OracleBundlePersis-
tenceManager there is an issue when
two users use the same database
for persistence. In that case, the
checkSchema() method of the Bun-
dleDbPersistenceManager does not
work like it should. More precisely,
the call "metaData.getTables(null, null,
tableName, null);" will also includes
table names of other schemas/users.
Eectively, only the rst user of a
database is able to create the schema.
probably same issue as here: JCR-582
Buggy Files: BundleDbPersistenceManager.java ,
OraclePersistenceManager.java
Table 8: Not Localized Report: LUCENE-3721
Summary: CharFilters not being invoked in Solr
Description: On Solr trunk, all CharFilters have
been non-functional since LUCENE-
3396 was committed in r1175297 on 25
Sept 2011, until Yonik's x today in
r1235810; Solr 3.x was not aected -
CharFilters have been working there all
along.
Buggy Files: TokenizerChain.java
Step 2: Automatic Identication of Localized Reports. In
this step, we build an algorithm that takes in a set of les
that are changed in bug xing commits and a bug report,
and outputs one of the three categories described in Ta-
ble 5. Our algorithm rst extracts the text that appear in
the summary and description elds of bug reports. Next,
it tokenizes this text into a set of word tokens. Finally, it
checks whether the name of each buggy le (ignoring its le-
name extension) appears as a word token in the set. If all
names appear in the set, our algorithm categorizes the re-
port as fully localized . If only some of the names appears in
the set, it categorizes the bug report as partially localized .
Otherwise, it categorizes the bug report as not localized . We
have evaluated our algorithm on the 350 manually labeled
bug reports and nd that its accuracy is close to 100%.
Step 3: Application of IR-Based Bug Localization Techniques.
After localized, partially localized, and not localized reports
are identied, we create three groups of bug reports. We
feed each of them into the VSM-based bug localization tool
described in Section 2. We then evaluate the eectiveness
of these tools for each of the three groups of reports.
Step 4: Statistical Analysis. We perform two statistical anal-
yses. First, we compare the average precision scores achieved
by VSM-based bug localization tool for the set of fully lo-
calized, partially localized, and not localized reports using
Mann-Whitney-Wilcoxon test at 5% signicance level. We
also compute Cohen's d on the average precision scores to
see if the eect size is small, medium or large.Second, we compare a subset of bug reports where the
VSM-based bug localization technique performs the best and
another subset where the VSM-based bug localization tech-
niques performs the worst. We then compare the distribu-
tion of fully, partially, and not localized bugs in these two
subsets. We employ Fisher exact test [11] to see if the dis-
tribution for the rst subset signicantly diers with the
distribution for the second subset.
4.3 Results
Number of Fully Localized, Partially Localized, and
Not Localized Reports. The numbers of bug reports that
are identied as fully, partially, and not localized are shown
in Table 9. We can observe that out of 1,191 bug reports,
398 (33.41%) bug reports are fully localized i.e., the bug
reports contains the name of all the class les changed to
x the bug. Over 50% of the bug reports are either fully or
partially localized. This shows that a signicant number of
bug reports are already localized, and do not benet from
a bug localization algorithm. On the other hand, 546 bug
reports (45.84%) are not localized at all.
Table 9: Fully, Partially, and Not Localized Reports
Project Category Number Proportion
HTTPClientFully 36 3.02%
Partially 28 2.35%
Not 35 2.93%
JackrabbitFully 299 25.10%
Partially 132 11.08%
Not 402 33.75%
Lucene-JavaFully 63 5.28%
Partially 87 7.30%
Not 109 9.15%
Average Precision Scores of Fully vs. Partially vs.
Not Localized Reports. Table 10 shows the Mean Av-
erage Precision (MAP) of the VSM-based bug localization
technique when applied to the set of fully, partially, and not-
localized reports. We can note that the MAP score dier-
ences between fully localized and not localized bug reports
for HTTPClient, Jackrabbit, and Lucene-Java are 84.39%,
99.86% and 91.16% respectively. Also, the MAP score dif-
ferences between partially localized and not localized bug
reports for HTTPClient, Jackrabbit, and Lucene-Java are
33.05%, 66.42% and 52.71% respectively.
Table 10: MAP Scores: Fully vs. Partially vs. Not
Project Fully Partially Not
HTTPClient 0.615 0.349 0.250
Jackrabbit 0.560 0.373 0.187
Lucene-Java 0.527 0.338 0.197
We also perform Mann-Whitney Wilcoxon test to exam-
ine the dierence between the following categories: fully &
partially, partially & not and fully & not. Table 11 shows
the p-values between dierent categories. The results show
that there are signicant dierences between average pre-
cision scores of fully localized and partially localized bug
reports, fully localized and partially localized bug reports,
808Table 11: Comparison: Fully vs. Partially vs. Not
ProjectFully-Partially Partially-Not Fully-Not
p-value d Eect Size p-value d Eect Size p-value d Eect Size
HTTPClient 0.007 0.94 Large 0.007 0.53 Medium 3.094 e 051.27 Large
Jackrabbit 4.544 e 050.56 Medium <2.2e 160.55 Medium <2.2e 161.14 Large
Lucene-Java 0.010 0.53 Medium 1.851 e 050.41 Small 3.183 e 091.04 Large
and partially localized and not localized bug reports, i.e., all
the p-values are less than 0.05. We also compute Cohen's d
to measure an eect size and nd that the eect sizes are
small to large. The eect sizes between average precision
scores of fully localized and not localized bug reports are
large for all three projects. This shows that there is a large
substantial dierence in the eectiveness of a bug localiza-
tion tool when applied to bug reports that are fully localized
and those that are not localized.
Best vs. Worst Bug Reports. We want to examine
the dierence between the proportion of bug reports that
are fully, partially, and not localized in the upper and lower
quartile of the bug reports based on the ability of the VSM-
based bug localization tool to localize them. We simply
sort the bug reports based on their average precision scores
and identify the subset that appear in the top 25% of the
list (upper quartile) and another subset that appear in the
bottom 25% of the list (lower quartile). For Jackrabbit and
Lucene-Java, we randomly select 50 bug reports from the
upper quartile and another 50 from the lower quartile. For
HTTPClient, we randomly select 25 bug reports from the
upper quartile and another 25 from the lower quartile { since
in our dataset, HTTPClient has less than 100 bug reports.
Table 12 shows the number of fully, partially and not local-
ized bugs for each of the projects. We use Fisher exact test
to examine the dierence between the distribution of fully
localized, partially localized, and not localized bug reports
in the upper and lower quartiles. The null hypothesis is that
there is no dierence between the distribution of fully, par-
tially, and not localized bug reports in the upper and lower
quartiles. The alternate hypothesis is that there is a signif-
icant dierence between the distribution of bug reports in
the upper and lower quartiles. We nd that the p-values for
all the projects are very small, which shows that there is a
signicance dierence in the distribution of fully localized,
partially localized, and not localized bug reports between
the best and worst bug reports.
Bias 2, which is localized bug reports, signicantly and
substantially impacts bug localization results. More
than 50% of the bugs are (already) localized either fully
or partially; these reports explicitly mention some or all
of the les that are buggy and thus do not require a bug
localization algorithm. The mean average precision scores
for fully and partially localized bug reports are much
higher (i.e., signicantly and substantially higher) than
those for not localized bug reports. The eect sizes of av-
erage precision scores between fully and not localized bug
reports are large for all three projects.5. BIAS 3: NON-BUGGY FILESTable 12: Fisher Exact Test: Best vs. Worst Re-
ports
Project Fully Partially Not p-value
HTTPClientUpper 16 5 40.0041Lower 6 4 15
JackrabbitUpper 35 9 62.807 e 13
Lower 7 1 42
Lucene-JavaUpper 22 18 108.724 e 05
Lower 5 18 27
In this section, we investigate the third research question:
What are the eects of wrongly identied ground truth les
on bug localization? We describe the motivation of answer-
ing this question in Section 5.1, the methodology of our ex-
periments in Section 5.2, and the results of our experiments
which answer the question in Section 5.3.
5.1 Motivation
Another issue which can bias the result is wrongly iden-
tied ground truth les. In past studies, wrongly identied
ground truth les have not been removed since they require
additional analysis. These wrongly identied ground truth
les can potentially skew the result of existing bug localiza-
tion solutions. In this research question, we want to investi-
gate to what extent do wrongly identied ground truth les
aect bug localization.
5.2 Methodology
Step 1: Manually Identifying Wrong Ground Truth Files.
We randomly select 100 bug reports that are not (already)
localized (i.e., these reports do not explicitly mention any of
the buggy les) and investigate the les that are modied
in the bug xing commits. We manually perform a dithat
gives us the dierences between the modied le and the
original le. Based on these dierences we manually decide
if a le contains a bug or not. Files that are only aected
by cosmetic changes, refactorings, etc. are considered as
non-buggy les. Based on this manual analysis, for each
bug report we have the set of clean ground truth les and
another set of dirty ground truth les.
Thung et al. have extended Kawrykow and Robillard
work [14] to automatically identify real ground truth les [22].
However the accuracy of their proposed technique is still rel-
atively low (i.e., precision and recall scores of 76.42% and
71.88%). Hence, we do not employ any automated tool to
identify wrong ground truth les. We also cannot extend the
study to investigate a large number of bug reports since the
identication of wrong ground truth les is time consuming.
Step 2: Application of IR-Based Bug Localization Techniques.
After the set of clean and dirty ground truth les are iden-
tied for each of the 100 bug reports, we input the 100 bug
809reports to a VSM-based bug localization tool described in
Section 2. We evaluate the results of the tool on dirty and
clean ground truth les.
Step 3: Statistical Analysis. We compare the average preci-
sion scores achieved by the VSM-based bug localization tool
for the 100 bug reports with clean and dirty ground truth
les using Mann-Whitney-Wilcoxon test at 5% signicance
level. We also compute Cohen's d on the average precision
scores to see if the eect size is small, medium or large.
5.3 Results
Number of Wrong Ground Truth Files. We found that
out of 498 les changed to x the 100 bugs, only 358 les are
really buggy. The other 140 les (28.11 %) do not contain
any of the bugs but are changed because of refactorings,
modications to program comments, due to changes made
to the buggy les, etc. Figure 1 shows the di of a le that
is changed in a commit that x bug report LUCENE-2616.
The content of the bug report with ID LUCENE-2616 is
shown in Table 13.
Figure 1: Example Di of a File that is Changed to
Fix a Bug in Lucene-Java Project with ID LUCENE-
2616. Note: (1) The name of the le: Seg-
mentInfo.java; (2) An empty line and an import
statement are deleted; (3) An empty line is deleted
and another one is added.
Table 13: Bug Report: LUCENE-2616
Summary: FastVectorHighlighter: out of align-
ment when the rst value is empty
in multiValued eld
Description: -
Non-Buggy File: SegmentInfo.java
Table 14: MAP Scores: Dirty vs. Clean Ground
Truths
Project Dirty Clean Dierence d
HTTPClient 0.207 0.171 0.036 0.08
Jackrabbit 0.115 0.115 0.000 0.08
Lucene-Java 0.271 0.239 0.032 0.17
MAP Scores: Dirty vs. Clean. We compare the Mean
Average Precision (MAP) scores of these 100 bug reports
when evaluated on dirty and clean ground truths. Table 14
shows that the dierences in the MAP scores are between
0 to 0.036. We also ran Mann-Whitney Wilcoxon test andcompute Cohen's d to check if each dierence is signicant
or substantial. We nd that the dierence is not statistically
signicant and the eect size is trivial ( <0.2).
Bias 3, which is incorrect ground truth les, neither sig-
nicantly nor substantially aects bug localization re-
sults. We notice that 28.11% of the les present in the
ground truth (i.e., they are changed in a commit that x a
bug) are non-buggy. Also, there is a dierence of 0-0.036
between the MAP scores when a bug localization tool is
evaluated on dirty and clean ground truth. However, this
dierence is neither statistically signicant nor substan-
tial.
6. OTHER FINDINGS AND THREATS
In this section, we rst describe the eects of the biases
measured by several other popular evaluation metrics. Next,
we describe some threats to validity.
6.1 Other Evaluation Metrics
Beside Mean Average Precision (MAP) which we used in
the previous sections, HIT@N and MRR have also been used
to evaluate bug localization studies [20, 21, 26]. HIT@N and
MRR are presented below:
HIT@N: This metric counts the percentage of bug
reports with at least one buggy le found in the top N
(e.g., 1) ranked results.
MRR (Mean Reciprocal Rank): The reciprocal
rank of a bug report is the inverse of the rank of the
rst buggy le in the ranked results. The mean recip-
rocal rank takes the average of the reciprocal ranks of
all bug reports. For a set of bug reports Q, MRR is
dened as:
MRR =1
jQjQX
i=11
rank i(5)
where rank iis the rank of the rst buggy le in the
output ranked list.
Figure 2: Before and After Removing Bias 1
The eect of bias 1, bias 2, and bias 3 measured by HIT@1
and MRR are shown in Figures 2 to 4. Figure 2 shows that
for bias 1, its eect in terms of HIT@1 and MRR scores is
810Figure 3: Before and After Removing Bias 2
Figure 4: Before and After Removing Bias 3
minimal. Figure 3 shows that for bias 2, its eect in terms
of HIT@1 and MRR score is substantial. Figure 4 shows
that for bias 3, for Jackrabbit, its eect is minimal. For
HTTPClient and Lucene-Java, its eect is more apparent
albeit not as substantial as the eect of bias 2.
For MRR since it is a mean of a distribution, we also run
Mann-Whitney-Wilcoxon test and compute Cohen's d val-
ues. The results are shown in Table 15. We nd that for
bias 1, its eect is not statistically signicant for all projects.
For bias 2, its eect is both statistically signicant and sub-
stantial when comparing the results of (fully or partially)
localized bug reports with results of not localized bug re-
ports. For bias 3, its eect is not statistically signicant for
all projects.
To conclude, the above results show that bias 2 has sub-
stantial eect on the performance of bug localization tech-
niques. The eects of bias 1 and 3 are more minor or even
negligible. These results are in line with the ndings of Sec-
tions 3, 4, and 5.
6.2 Threats to Validity
Threats to internal validity corresponds to errors in our
experiments and our labeling. In this work, these threats are
coming from human classication of bug reports. For bias 1,
we consider the same issue reports that were manually cat-
egorized by Herzig et al. [12]. For bias 2, we manually cate-
gorize 350 bug reports as fully localized, partially localized,
or not localized. Also, we design an automated algorithm
to identify localized reports and nd that it performs very
well on the manually labeled bug reports. However, it is notTable 15: Results of Mann-Whitney-Wilcoxon Test
and Cohen'd Computation for MRR. (F-P) = Fully
Localized vs. Partially Localized. (P-N) = Partially
Localized vs. Not Localized. (F-N) = Fully Local-
ized vs. Not Localized.
Bias Type Project p-value d
Bias 1HTTPClient 0.6667 0.241
Jackrabbit 0.7855 0.050
Lucene-Java 0.7336 0.043
Bias 2HTTPClient(F-P) 0.5465 0.142
(P-N) 0.0008925 0.364
(F-N) 0.0003381 0.634
Jackrabbit(F-P) 0.075 0.128
(P-N) <2.2e-16 1.421
(F-N) <2.2e-16 0.962
Lucene-Java(F-P) 0.2024 0.097
(P-N) 8.201e-08 0.944
(F-N) 3.805e-06 0.775
Bias 3HTTPClient 0.6464 0.163
Jackrabbit 0.9404 0.088
Lucene-Java 0.7449 0.137
clear if performs as well on other bug reports in our dataset.
For bias 3, we manually categorize les that are changed
in commits that x 100 bug reports. To reduce bias, two
PhD students majoring in software engineering analyze the
bug reports and agree on the labels. Threats to external va-
lidity relates to the generalizability of our ndings. In this
work, we consider three open source projects: HTTPClient,
Jackrabbit, and Lucene-Java. We analyze 5,591 bug reports
for RQ1 (bias 1), 1,191 bug reports for RQ2 (bias 2), and
100 bug reports for RQ3 (bias 3). We plan to include more
projects and analyze more bug reports in a future work.
We have also only analyzed VSM-based bug localization ap-
proach. There are many other techniques proposed in the lit-
erature and we plan to analyze them in a future work. Many
of these techniques are based on VSM, e.g., [21, 23, 26], and
they are likely to be aected by the biases in a similar way
as plain VSM. Threats to construct validity relates to the
suitability of our evaluation metrics. We make use of MAP,
HIT@N, and MRR which are popular metrics that have been
used in many past bug localization studies [20, 21, 26]. We
also perform two widely used statistical tests (i.e., Mann-
Whitney-Wilcoxon test [17] and Fisher exact test [11]), and
compute one widely used eect size measure (i.e., Cohen's
d [10]).
7. RELATED WORK
In this section, we describe studies that analyze bias in
software engineering and IR-based bug localization studies.
Our survey here is by no means complete.
7.1 Bias in Software Engineering
Many software engineering studies are highly dependant
on data stored in software repositories. However, the dataset
is not always clean, which means it might contain bias. A
set of research work have shown that such bias in a dataset
might impact software engineering studies, e.g., [7, 12, 13,
14, 15, 19]. We highlight some of them especially closely
related ones below.
811Antoniol et al. noted that bug tracking system not only
maintains reports of bugs, but also other issue reports, such
as reports that contain feature enhancement requests [4].
Later, Herzig et al. studied misclassied bug reports of mul-
tiple projects by manually checking around 7,000 issue re-
ports [12]. They found that around 33.8% of their sampled
bug reports are misclassied and such bias aects bug pre-
diction techniques. Bird et al. investigated whether the bugs
sampled based on commit logs are fair representation of the
full set of xed bugs or not [7].
Kawrykow and Robillard observed that many changes to
source code corpus contain non-essential modications, such
as renaming variable and adding comments, and thus might
cause bias to techniques that analyze version control reposi-
tories [14]. Thung et al. extended Kawrykow and Robillard's
approach to detect root causes of bugs from commits [22].
Recently, Herzig and Zeller investigated the impact of tan-
gled code changes, which are multiple changes for dierent
tasks inside a single commit [13]. Nguyen et al. analyzed
1,296 bug xing commits and found that more than 10% of
the changed les are non-buggy les [19].
This work, especially the rst research question, is an ex-
tension of our previous short paper that analyzes how issue
report misclassication aects bug localization [15]. For our
rst research question, we extend the previous work by per-
forming a per-project analysis instead of all-project analysis,
and by computing Cohen's d to check if the impact of mis-
classication is substantial or not. In the previous work, we
nd that bias 1 signicantly impacts bug localization results.
In this study, we nd that bias 1 only signicantly impacts
bug localization results for one out of the three projects.
The other two research questions are newly proposed in this
work.
7.2 IR-based Bug Localization Approaches
There are many IR-based bug localization approaches that
retrieve source code les that are relevant to an input bug
report [18, 20, 21, 23, 26]. Rao and Kak conducted a com-
parative study on the performance of a number of general
IR models on bug localization task [20]. They found that
simple text models such as Vector Space Model (VSM) and
Smoothed Unigram Model (SUM) perform better than more
sophisticated models such as Latent Dirichlet Allocation (LDA).
Since then, a number of works have been done to improve the
eectiveness of standard IR models by considering more in-
formation, applying advanced techniques, and rening queried
bug reports.
Zhou et al. proposed an extended vector space model named
rSVM to localize bug reports by leveraging information from
similar bug reports [26]. Saha et al. made use of code struc-
ture information retrieved from source code (e.g, whether
a word is used as a class name or a variable name), and
bug report structure (e.g., whether a word is appeared in
the title or description led of a bug report) to improve
the eectiveness of IR-based bug localization [21]. Based on
the assumption that a bug report and its relevant code les
share several latent technical aspects, Nguyen et al. devel-
oped a customized topic model approach named BugScout
to localize bug reports [18]. Recently, Wang et al. proposed
an integrated bug localization approach by considering mul-
tiple resources (i.e, version history, similar bug reports, and
structure information) [23].
In this work, we focus on potential biases that might im-pact bug localization techniques. Our study highlights sev-
eral steps that researchers need to take to clean up datasets
used to evaluate the performance of bug localization tech-
niques.
8. CONCLUSION AND FUTURE WORK
Many studies have proposed IR-based bug localization
techniques to aid developers in nding buggy les given a
bug report. These studies often evaluate their eectiveness
on issue reports marked as bugs in issue tracking systems,
using as ground truth the set of les that are modied in
commits that x each bug. However, there are several po-
tential biases that can impact the results of these bug lo-
calization studies. Firstly, issue reports marked as bugs in
issue tracking systems might not be bugs due to errors in the
reporting and classication process. Secondly, bug reports
might already be localized, i.e., they might explicitly men-
tion the buggy les, which obviates the need to run localiza-
tion on these bug reports. Thirdly, les modied as part of a
bug x commit might not be buggy, i.e., their modications
only involve cosmetic changes such as declaring empty vari-
able, changing comments and so on. Our study analyzes the
impact of these potential biases on bug localization results.
Our empirical study highlights the following results:
1. Wrongly classied issue reports do not statistically sig-
nicantly impact bug localization results on two out
of the three projects. They also do not substantially
impact bug localization results on all three projects
(eect size <0.2).
2. (Already) localized bug reports statistically signicantly
and substantially impact bug localization results (p-
value <0.05 and eect size >0.8).
3. Existence of non-buggy les in the ground truth does
not statistically signicantly or substantially impact
bug localization results (eect size <0.2).
Our ndings suggest that future bug localization researchers
need to at least remove (already) localized bug reports from
their evaluation dataset since such reports have signicant
and substantial impact on the performance of bug localiza-
tion techniques.
As a future work, we plan to investigate more bug reports
from additional systems to reduce the threats to external
validity. We also plan to investigate additional biases that
might aect bug localization studies.
Acknowledgement
We would like to thank Kim Herzig, Sascha Just, and An-
dreas Zeller for making the dataset used in their work [12]
publicly available.
Dataset
Our dataset is made publicly available at https://github.
com/smusis/buglocalizationbiases .
8129. REFERENCES
[1] HTTPClient. http://hc.apache.org/
httpcomponents-client-ga/index.html.
[2] Jackrabbit. https://jackrabbit.apache.org/ .
[3] Lucene-java. http://lucene.apache.org/ .
[4] G. Antoniol, K. Ayari, M. Di Penta, F. Khomh, and
Y.-G. Gu eh eneuc. Is it a bug or an enhancement?: a
text-based approach to classify change requests. In
Proceedings of the 2008 conference of the center for
advanced studies on collaborative research: meeting of
minds , page 23. ACM, 2008.
[5] J. Anvik, L. Hiew, and G. C. Murphy. Coping with an
open bug repository. In ETX , pages 35{39, 2005.
[6] A. Bachmann, C. Bird, F. Rahman, P. T. Devanbu,
and A. Bernstein. The missing links: bugs and bug-x
commits. In FSE, 2010.
[7] C. Bird, A. Bachmann, E. Aune, J. Duy,
A. Bernstein, V. Filkov, and P. Devanbu. Fair and
balanced?: bias in bug-x datasets. In Proceedings of
the the 7th joint meeting of the European software
engineering conference and the ACM SIGSOFT
symposium on The foundations of software
engineering, pages 121{130. ACM, 2009.
[8] C. Bird, A. Bachmann, E. Aune, J. Duy,
A. Bernstein, V. Filkov, and P. T. Devanbu. Fair and
balanced?: bias in bug-x datasets. In ESEC/FSE ,
2009.
[9] T. F. Bissyand e, F. Thung, S. Wang, D. Lo, L. Jiang,
and L. R eveill ere. Empirical evaluation of bug linking.
InCSMR , 2013.
[10] J. Cohen. Statistical Power Analysis for the Behavioral
Sciences. Hillsdale: Lawrence Erlbaum, 1988.
[11] R. A. Fisher. On the Interpretation of IG2 from
Contingency Tables, and the Calculation of P. Journal
of the Royal Statistical Society , 85:87{94, 1922.
[12] K. Herzig, S. Just, and A. Zeller. It's not a bug, it's a
feature: How misclassication impacts bug prediction.
InProceedings of the 2013 International Conference
on Software Engineering, pages 392{401. IEEE Press,
2013.
[13] K. Herzig and A. Zeller. The impact of tangled code
changes. In Mining Software Repositories (MSR),
2013 10th IEEE Working Conference on, pages
121{130. IEEE, 2013.
[14] D. Kawrykow and M. P. Robillard. Non-essential
changes in version histories. In Proceedings of the 33rd
International Conference on Software Engineering ,
pages 351{360. ACM, 2011.
[15] P. S. Kochhar, T.-D. Le, and D. Lo. It's not a bug, it'sa feature: Does misclassication aect bug
localization? In MSR , 2014.
[16] Lucia, F. Thung, D. Lo, and L. Jiang. Are faults
localizable? In MSR , pages 74{77, 2012.
[17] H. B. Mann, D. R. Whitney, et al. On a test of
whether one of two random variables is stochastically
larger than the other. The annals of mathematical
statistics , 18(1):50{60, 1947.
[18] A. T. Nguyen, T. T. Nguyen, J. Al-Kofahi, H. V.
Nguyen, and T. N. Nguyen. A topic-based approach
for narrowing the search space of buggy les from a
bug report. In Automated Software Engineering
(ASE), 2011 26th IEEE/ACM International
Conference on, pages 263{272. IEEE, 2011.
[19] H. A. Nguyen, A. T. Nguyen, and T. N. Nguyen.
Filtering noise in mixed-purpose xing commits to
improve defect prediction and localization. In Software
Reliability Engineering (ISSRE), 2013 IEEE 24th
International Symposium on , pages 138{147. IEEE,
2013.
[20] S. Rao and A. Kak. Retrieval from software libraries
for bug localization: a comparative study of generic
and composite text models. In Proceedings of the 8th
Working Conference on Mining Software Repositories ,
pages 43{52. ACM, 2011.
[21] R. K. Saha, M. Lease, S. Khurshid, and D. E. Perry.
Improving bug localization using structured
information retrieval. In Automated Software
Engineering (ASE), 2013 IEEE/ACM 28th
International Conference on , pages 345{355. IEEE,
2013.
[22] F. Thung, D. Lo, and L. Jiang. Automatic recovery of
root causes from bug-xing changes. In Reverse
Engineering (WCRE), 2013 20th Working Conference
on, pages 92{101. IEEE, 2013.
[23] S. Wang and D. Lo. Version history, similar report,
and structure: Putting them together for improved
bug localization. In ICPC , 2014.
[24] S. Wang, D. Lo, and L. Jiang. Understanding
widespread changes: A taxonomic study. In CSMR ,
pages 5{14, 2013.
[25] R. Wu, H. Zhang, S. Kim, and S.-C. Cheung. Relink:
recovering links between bugs and changes. In FSE,
2011.
[26] J. Zhou, H. Zhang, and D. Lo. Where should the bugs
be xed? more accurate information retrieval-based
bug localization based on bug reports. In Software
Engineering (ICSE), 2012 34th International
Conference on, pages 14{24. IEEE, 2012.
813