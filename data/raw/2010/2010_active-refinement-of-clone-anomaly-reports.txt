Singapor e Management Univ ersity Singapor e Management Univ ersity 
Institutional K nowledge at Singapor e Management Univ ersity Institutional K nowledge at Singapor e Management Univ ersity 
Resear ch Collection School Of Computing and 
Information Systems School of Computing and Information Systems 
6-2012 
Activ e refinement of clone anomaly r epor ts Activ e refinement of clone anomaly r epor ts 
LUCIA 
Singapor e Management Univ ersity , lucia.2009@smu.edu.sg 
David L O 
Singapor e Management Univ ersity , davidlo@smu.edu.sg 
Lingxiao JI ANG 
Singapor e Management Univ ersity , lxjiang@smu.edu.sg 
Adity a BUDI 
Singapor e Management Univ ersity , adity abudi@smu.edu.sg 
Follow this and additional works at: https:/ /ink.libr ary.smu.edu.sg/sis_r esear ch 
 Part of the Softwar e Engineering Commons 
Citation Citation 
LUCIA; LO, David; JI ANG, Lingxiao; and BUDI, Adity a. Activ e refinement of clone anomaly r epor ts. (2012). 
ICSE'12: 34th International Conf erence on Softwar e Engineering: Pr oceedings: June 2-9, 2012, Z urich, 
Switz erland . 397-407. 
Available at:Available at:  https:/ /ink.libr ary.smu.edu.sg/sis_r esear ch/1530 
This Conf erence Pr oceeding Ar ticle is br ought t o you for fr ee and open access b y the School of Computing and 
Information Systems at Institutional K nowledge at Singapor e Management Univ ersity . It has been accepted for 
inclusion in Resear ch Collection School Of Computing and Information Systems b y an authoriz ed administr ator of 
Institutional K nowledge at Singapor e Management Univ ersity . For mor e information, please email 
cher ylds@smu.edu.sg . Active Reﬁnement of Clone Anomaly Reports
Lucia, David Lo, Lingxiao Jiang, and Aditya Budi
School of Information Systems
Singapore Management University
{lucia.2009,davidlo,lxjiang,adityabudi }@smu.edu.sg
Abstract —Software clones have been widely studied in the
recent literature and shown useful for ﬁnding bugs because
inconsistent changes among clones in a clone group may
indicate potential bugs. However, many inconsistent clone
groups are not real bugs (true positives). The excessive num ber
of false positives could easily impede broad adoption of clo ne-
based bug detection approaches.
In this work, we aim to improve the usability of clone-based
bug detection tools by increasing the rate of true positives
found when a developer analyzes anomaly reports. Our idea
is to control the number of anomaly reports a user can see
at a time and actively incorporate incremental user feedbac k
to continually reﬁne the anomaly reports. Our system ﬁrst
presents top few anomaly reports from the list of reports
generated by a tool in its default ordering. Users then eithe r
accept or reject each of the reports. Based on the feedback,
our system automatically and iteratively reﬁnes a classiﬁc ation
model for anomalies and re-sorts the rest of the reports. Our
goal is to present the true positives to the users earlier tha n
the default ordering. The rationale of the idea is based on
our observation that false positives among the inconsisten t
clone groups could share common features (in terms of code
structure, programming patterns, etc.), and these feature s can
be learned from the incremental user feedback.
We evaluate our reﬁnement process on three sets of clone-
based anomaly reports from three large real programs: the
Linux Kernel (C), Eclipse, and ArgoUML (Java), extracted by
a clone-based anomaly detection tool. The results show that
compared to the original ordering of bug reports, we can
improve the rate of true positives found (i.e., true positiv es
are found faster) by 11%, 87%, and 86% for Linux kernel,
Eclipse, and ArgoUML, respectively.
I. I NTRODUCTION
Code clones, or pieces of similar code, commonly occur in
large software systems [1], [2] due to various reasons, whic h
range from improper code reuse via the prevalent copy-and-
paste practice, to the introduction of redundant code to im-
prove runtime efﬁciency and/or reliability of systems. The y
have attracted many research interest and various studies o n
detecting code clones [2]–[5], tracking and managing code
clones [6]–[8], and examining the harmfulness or usefulnes s
of code clones [9]–[11].
One important use of code clones is their applicability
in detecting bugs [11]–[16]. These clone-based anomaly
detection tools look for inconsistencies among code clones
in every clone group ( i.e., a group of code fragments similar
to each other) and report them as anomalies ( i.e., potential
bugs). For example, Li et al. [14] look for different identiﬁernames among clones and check whether all names are
changed consistently ; Jiang et al. [12] look at syntactical
structures of the code surrounding every clone, in addition
to the identiﬁer names in clones, and report differences as
anomalies. Tens of true positives of diverse characteristi cs
from large systems, such as the Linux kernel and Eclipse,
have been found by these tools. Figure 1 shows a true
positive from the Linux kernel: there is a missing null-chec k
on thetmp variable in the code fragment 2. Such a detection
is possible because most parts of the two code fragments
(after theifcondition) are detected as clones, and the code
surrounding the clones (which are the variable declaration
and theifcondition) shows some structural differences (no
ifin code fragment 2).
However, the set of reported anomalies can be huge, con-
taining hundreds or even thousands of reports. Among these
anomalies, only a small proportion are true positives; othe rs
arebenign variations among clones in a clone group, which
are intended changes rather than mistakes. The process of
verifying whether these anomalies are true or not can be
painstaking and time-consuming. Developers tend to give
up if many of the ﬁrst set of anomaly reports that they
check are false positives. For example, Jiang et al. [12]
reported that among more than 800 reports generated by
their tool for the Linux kernel, only 57 are true bugs or
bad programming styles. Gabel et al. [15] applied more
advanced ﬁltering techniques based on textual similarity a nd
sequence alignment on inconsistent clones detected from a
large commercial code base. They reported that among 500
manually checked anomaly reports (out of 8103 in total), 149
may be true bugs and 109 may be code smells, while the
rest is unsure. Hence, reducing the manual effort in locatin g
true positives in clone-based anomaly reports remains an
important task for wide adoption of such tools.
In this paper, we propose an active-learning and user-
feedback directed approach to help alleviate the problem
of false positives. The task is challenging as there are
only a few true positives embedded in a mass of false
positives. Our idea is to actively, iteratively incorporat e user
feedbacks to reﬁne anomaly reports. Users are presented
anomaly reports one by one; as a user labels the report
as a false positive or true positive, our system actively
updates the remaining set of anomaly reports. In so doing,
we aim to make true positives appear earlier in the list978-1-4673-1067-3/12/$31.00 c2012 IEEE ICSE 2012, Zurich, Switzerland 397ICSE '12: 34th International Conference on Software Engineering: Proceedings: June 2-9, 2012, Zurich, Switzerland, Pages 397-407.
http://dx.doi.org/10.1109/ICSE.2012.6227175Code Fragment 1 Code Fragment 2 
File: linux-2.6.19/fs/sysfs/inode.c 
219: struct dentry * dentry = sd->s_dentry; 
220: 
221: if (dentry) { 
/* the following parts are detected as clones */ 
222: spin_lock(&dcache_lock); 
223: spin_lock(&dentry->d_lock); 
224: if (!(d_unhashed(dentry) && dentry->d_inode)) { 
225: dget_locked(dentry); 
226: __d_drop(dentry); 
227: spin_unlock(&dentry->d_lock); 
228: spin_unlock(&dcache_lock); 
229: ...... File: linux-2.6.19/drivers/infiniband/hw/ipath/ipat h_fs.c 
456: struct dentry *tmp; 
457: 
458: tmp = lookup_one_len(name, parent, strlen(name )); 
459: 
460: spin_lock(&dcache_lock); 
461: spin_lock(&tmp->d_lock); 
462: if (!(d_unhashed(tmp) && tmp->d_inode)) { 
463: dget_locked(tmp); 
464: __d_drop(tmp); 
465: spin_unlock(&tmp->d_lock); 
467: spin_unlock(&dcache_lock); 
468: ...... Code Fragment 1 Code Fragment 2 
File: linux-2.6.19/fs/sysfs/inode.c 
219: struct dentry * dentry = sd->s_dentry; 
220: 
221: if (dentry) { 
/* the following parts are detected as clones */ 
222: spin_lock(&dcache_lock); 
223: spin_lock(&dentry->d_lock); 
224: if (!(d_unhashed(dentry) && dentry->d_inode)) { 
225: dget_locked(dentry); 
226: __d_drop(dentry); 
227: spin_unlock(&dentry->d_lock); 
228: spin_unlock(&dcache_lock); 
229: ...... File: linux-2.6.19/drivers/infiniband/hw/ipath/ipat h_fs.c 
456: struct dentry *tmp; 
457: 
458: tmp = lookup_one_len(name, parent, strlen(name )); 
459: 
460: spin_lock(&dcache_lock); 
461: spin_lock(&tmp->d_lock); 
462: if (!(d_unhashed(tmp) && tmp->d_inode)) { 
463: dget_locked(tmp); 
464: __d_drop(tmp); 
465: spin_unlock(&tmp->d_lock); 
467: spin_unlock(&dcache_lock); 
468: ...... 
Figure 1. A sample bug (missing null-check) revealed by cont extual inconsistency among clones in a clone group from the L inux kernel – compare
lines 221 & 224 in code fragment 1 with lines 459 & 462 in code fr agment 2.
G# Code Clone 1 Code Clone 2 
1 File: linux-2.6.19/fs/nfsd/nfs3xdr.c 
423: if (!(p = decode_fh(p, &args->fh)) 
424:  || !(p = decode_filename(p, &args->name, &arg s->len)) 
425:  || !(p = decode_sattr3(p, &args->attrs))) 
426: return 0; File: linux-2.6.19/fs/nfsd/nfsxdr.c 
344: if (!(p = decode_fh(p, &args->ffh)) 
345:  || !(p = decode_fh(p, &args->tfh)) 
346:  || !(p = decode_filename(p, &args->tname, &ar gs->tlen)))
347: return 0; 
2 File: linux-2.6.19/drivers/hwmon/lm87.c 
688: if ((err = device_create_file(&new_client->dev , 
689: &dev_attr_in6_input)) 
690:  || (err = device_create_file(&new_client->dev , 
691: &dev_attr_in6_min)) 
692:  || (err = device_create_file(&new_client->dev , 
693: &dev_attr_in6_max))) 
694: goto exit_remove; File: linux-2.6.19/drivers/hwmon/gl520sm.c 
615: if ((err = device_create_file(&new_client->dev , 
616: &dev_attr_in4_input)) 
617:  || (err = device_create_file(&new_client->dev , 
618: &dev_attr_in4_min)) 
619:  || (err = device_create_file(&new_client->dev , 
620: &dev_attr_in4_max))) 
621: goto exit_remove_files; G# Code Clone 1 Code Clone 2 
1 File: linux-2.6.19/fs/nfsd/nfs3xdr.c 
423: if (!(p = decode_fh(p, &args->fh)) 
424:  || !(p = decode_filename(p, &args->name, &arg s->len)) 
425:  || !(p = decode_sattr3(p, &args->attrs))) 
426: return 0; File: linux-2.6.19/fs/nfsd/nfsxdr.c 
344: if (!(p = decode_fh(p, &args->ffh)) 
345:  || !(p = decode_fh(p, &args->tfh)) 
346:  || !(p = decode_filename(p, &args->tname, &ar gs->tlen)))
347: return 0; 
2 File: linux-2.6.19/drivers/hwmon/lm87.c 
688: if ((err = device_create_file(&new_client->dev , 
689: &dev_attr_in6_input)) 
690:  || (err = device_create_file(&new_client->dev , 
691: &dev_attr_in6_min)) 
692:  || (err = device_create_file(&new_client->dev , 
693: &dev_attr_in6_max))) 
694: goto exit_remove; File: linux-2.6.19/drivers/hwmon/gl520sm.c 
615: if ((err = device_create_file(&new_client->dev , 
616: &dev_attr_in4_input)) 
617:  || (err = device_create_file(&new_client->dev , 
618: &dev_attr_in4_min)) 
619:  || (err = device_create_file(&new_client->dev , 
620: &dev_attr_in4_max))) 
621: goto exit_remove_files; 
Figure 2. False positive clone groups in Linux Kernel. Each r ow is a pair of inconsistent clones which do not correspond to bugs. Each pair of clones
involve the same numbers of ifstatements, ||operators, function calls, and assignments.
Table I
INFORMAL ILLUSTRATION : REFINEMENT PROCESS
Case 1: Without reﬁnement
Jack is presented with 500 bug reports. He investigates the ﬁ rst 100, and can ﬁnd
ﬁve true positives. If the bugs are mission critical, it’s wo rth the effort.
Case 2: With reﬁnement
Jack is presented with 500 bug reports. As he navigates throu gh the bug reports
and labels each of them as true bugs or false positives, the sy stem automatically
reorders the remaining unlabeled bug reports. He can now ﬁnd ten true positives
after investigating 100 reports. Jack’s productivity in ﬁn ding bugs is doubled.
of all anomaly reports. Thus, we provide a feedback loop
between bug detection tools and developers, and help to
improve the quality of anomaly reports and reduce the effort
of manual investigation. As an informal illustration consi der
two scenarios in Table I.
Now we describe how this active reﬁnement of anomaly
reports could be performed. Conceptually, we divide the
space of possible clone groups into four quadrants as shown
in Figure 3. The columns separate clone groups that have
inconsistencies from those that do not; The rows separate
clone groups that allow variations ( i.e.,ﬂexible ) from those
that do not ( i.e.,rigid ). A rigid clone group is a set of
clones where variations among clones are harmful; a ﬂexible
clone group is a set of clones where variations are benign1.
Current clone-based anomaly detection tools would separat e
clone groups in the two quadrants on the left from those
in the two quadrants on the right. However, clone groups
in the bottom left quadrant would be all false positives
since the inconsistent changes in those clones are allowed
or intentional and should not be reported as anomalies. The
1Both notions allow gapped clones, and are orthogonal to the c oncept of
gapped clones.Rigid 
Flexible Inconsistent Consistent 
True Positive 
Figure 3. Clone Group’s Four Quadrants
goal of our approach is to learn a discriminative model to
provide the likelihood of a clone group belonging to the top
left quadrant ( i.e., rigid but inconsistent) versus belonging
to the bottom left quadrant ( i.e., ﬂexible and inconsistent).
Then, this model can be used to re-sort the list of anomaly
reports and make true positives appear earlier in the list.
We observe that false positives could be similar to one
another in certain ways. For example, consider the code
snippets in Figure 2 containing two clone groups with two
clones each. All of code snippets involve the same number
ofifstatements, ||operators, function calls, and assign-
ments, but they are quite different from the true positive
shown in Figure 1. Thus, the intuition of our approach is
that false positives may have similar characteristics amon g
themselves, but they have different characteristics from t rue
positives, and differences between false and true positive s
could be leveraged to build a discriminative model.
In order to characterize the similarities and differences
among clones, we convert each clone group into a set of
features. These features are built from the various syntact ical
patterns in the clones of each group. A discriminative model
is a composition of features that collectively capture the d if-
ferences between false and true positives. Often such model s
are built from a ﬁxed static training dataset, e.g., [17],
[18]. However, in our bug reﬁnement process, the training398dataset is incrementally updated as a new anomaly report
is inspected and marked by a developer as either a false or
true positive, and we would like to build our discriminative
model based on such a dynamic training dataset.
We propose a framework consisting of a reﬁnement engine
that leverages user feedbacks and is iteratively invoked.
People need to take action on anomaly reports, to either get
bugs ﬁxed or discard them. Our feedbacks are from such
actions and no extra effort is needed. The reﬁnement engine
is composed of a feature extractor, a pre-processor, and a
classiﬁer arranged in a pipeline. It takes in the feedbacks
given so far to build and reﬁne discriminative models. The
resultant discriminative model in each iteration is used to
reﬁne the remaining uninvestigated anomaly reports.
We evaluate our framework on three sets of clone anomaly
reports for three large programs: the Linux kernel (C),
Eclipse, and ArgoUML (Java) [12] extracted by a clone-
based anomaly detection tool. Our evaluation shows that
compared to the original ordering of bug reports, we can
improve the average percentage of true positives found , an
evaluation metric adopted from the test case prioritizatio n
community [19], by 11%, 87%, and 86% for the Linux
kernel, Eclipse, and ArgoUML respectively.
The main contributions of this work are as follows:
1. We present the topic of reﬁning clone anomaly reports.
2. We propose an active learning approach to incremen-
tally reﬁne anomaly reports with user feedbacks.
3. We present an engine that learns discriminative models
that can assign the likelihood of each anomaly report
being a false positive.
4. We evaluate our proposed approach on three large
systems—the Linux Kernel, Eclipse, and ArgoUML—
with promising results.
This paper is organized as follows. We describe related
work in Section II. In Section III, we review the concept of
clone-based anomaly detection. In Section IV, we describe
our overall active reﬁnement framework which iteratively
invokes a reﬁnement engine. We elaborate this engine in
Section V. Our evaluation metric is described in Section VI.
In Section VII, we describe our evaluation results on three
large software systems. We conclude and mention potential
future work in Section VIII.
II. R ELATED WORK
There are many studies in software engineering that are
related to our work. We summarize them in the following.
1) Code Clone Analysis and Clone-Based Bug Detection:
Code clones have been widely studied in the literature.
Some studies focus on detection of code clones, based on
similarities among strings, tokens, syntax trees, depende ncy
graphs, and even functionalities [2]–[4], [14], [20]–[22] .
Clones are traditionally thought as harmful, and technique s
have been proposed to reduce clones [23], [24]. On the
other hand, some studies show that clones can be usefuland necessary [9], [10]. Then, instead of reducing clones,
some studies investigate techniques to track and manage
code clones [7], [8], [25].
One important use of code clones is to detect bugs. A
number of studies detect bugs by detecting inconsistencies
among clones [11], [12], [14]–[16]. Such inconsistence-
based detection of clone-related bugs often produces many
false positives, and uses various ﬁltering rules to reduce
false positives. However, even with the most recent ﬁlterin g
techniques, such as ones based on textual similarity and
sequence alignment [15], false positive rates remain high.
Compared with these studies that use ﬁltering-based ap-
proaches to remove reports which may cause false negatives,
our approach actively and incrementally reﬁnes and re-
ranks anomaly reports based on user feedbacks without
removing any report. The code features used in our re-
ranking are also different from those papers. Our work is not
an alternative, but rather a complement of others. Filterin g-
based approaches (which still leave many false positives
behind) may be applied ﬁrst, then our work reﬁnes the
ﬁltered reports as users take actions, e.g., to ﬁx an anomaly
if it is a true positive.
2) Bug Prediction and Triage: Many studies aim to predict
whether certain code changes or ﬁles may contain faults.
Kim et al. [26], [27] use bug history to predict faults.
Ruthruff et al. [28] use logistic regression models from
historical data to predict whether a warning generated by
FindBugs is actionable. Zimmermann et al. [29] have studied
the accuracies of bug prediction models that may be used
across various projects in various domains.
Other studies aim to reduce similar bug reports or priori-
tize bug reports. Podgurski et al. [30] group software failures
with similar symptoms together. Kremenek and Engler [31]
propose z-ranking to order bug reports produced by a static
program checking analysis tool. Heckman and William [32]
propose FAULTBENCH, a benchmark for evaluating alert
prioritization and classiﬁcation techniques. These ranki ng
models only perform reordering of bug reports once.
Our approach is different from the above studies in several
aspects. We focus on anomaly reports generated by a clone-
based anomaly detection tool, instead of reports from users .
Wereorder anomaly reports, while most other studies ﬁlter
reports. Filtering anomaly reports carries a risk of removi ng
true positives. Filtering and reordering are complementar y as
we could ﬁrst ﬁlter and then re-order anomaly reports. Some
studies leverage historical data to prioritize anomaly reports,
while we leverage immediate user feedbacks to iteratively
prioritize clone-based anomaly reports.
III. C LONE -BASED ANOMALY DETECTION
Clone-based bug detection techniques [12], [14] are based
on code clone detection and the concept of contextual
consistency . The intuition behind the technique is that code399clones should be inherently similar to each other, and incon -
sistent changes to the clones themselves or their surroundi ng
code (which are called contexts ) may indicate unintentional
changes, bad programming styles, and bugs.
The technique in [12] is summarized as follows:
1) It uses a code clone detection tool, D ECKARD [5], to
detect code clones in programs. The output of this step
is a set of clone groups, where each clone group is a
set of code pieces that are syntactically similar to each
other ( a.k.a. clones);
2) Then, it locates the locations of every clone in the
source code and generates parse (sub)trees for them;
3) Next, it detects inconsistencies among the parse trees of
the clones and their contexts, e.g., whether the clones
contain different numbers of unique identiﬁers, and how
the language constructs of the contexts are different.
The inconsistencies are then ranked heuristically based
on their potential relationship with bugs. Inconsistent
clones unlike to be buggy are also ﬁltered out.
4) Finally, it outputs a list of anomaly reports, each of
which indicates the location of a potential bug in the
source code, for developers to inspect.
It has been reported that this technique has high false
positive rates, even though it can ﬁnd true bugs of diverse
characteristics that are difﬁcult to detect by other techni ques.
For example, among more than 800 reported bugs for the
Linux Kernel, only 41are true bugs and another 17are bad
programming styles; among more than 400 reported bugs
for the Eclipse, only 21are true bugs and 17are issues with
bad programming styles [12].
IV. O VERALL REFINEMENT FRAMEWORK
A typical clone-based anomaly detection system performs
a single batch analysis where a static set of anomaly or bug
reports (ordered or unordered) are produced. It requires no
or little user intervention (e.g., setting some parameters ),but
may produce many false positives. To alleviate this problem ,
we propose an active learning approach that can dynamically
and continually reﬁne anomaly reports based on incremental
user feedbacks; each feedback is immediately incorporated
by our approach into the ordering of anomaly reports to
move possible true positive reports up in the list while
moving likely false positives towards the end of the list.
Our proposed active reﬁnement process supporting user
feedbacks is shown in Figure 4. It is composed of ﬁve
parts corresponding to the boxes in the ﬁgure.2Let us
refer to them as Block 1 to 5 (counter-clockwise from left
to right). Block 1 represents a typical batch-mode clone-
based anomaly detection system. Given a program, the
system identiﬁes parts of the program that are different
from the norm, where the norm corresponds to the common
2A square, a trapeze, and a parallelogram represent a process , a manual
operation, and data respectively.Anomaly Detection 
System 
Refinement 
Engine 
<<Refinement Loop>> 1
5
User 
Feedback 
4Sorted 
Bug Reports 2
First Few Bug 
Reports 3
Figure 4. Active Reﬁnement Process
characteristics in a clone group. Then, the set of anomalies or
bugs ( i.e., Block 2) is presented for manual user inspection.
We extend such typical clone-based anomaly detec-
tion systems by incorporating incremental user feedbacks
through the feedback and reﬁnement loop starting at Block
2 followed by Blocks 3, 4, and 5, and back to Block 2. At
Blocks 3 and 4, a user is presented with a few bug reports
and is asked to provide feedbacks on whether the reports he
or she sees are false or true positives. These feedbacks are
then fed into our reﬁnement engine ( i.e., Block 5) to update
the original or intermediate lists of bug reports.
With user feedbacks, the reﬁnement engine analyzes the
characteristics of both false positives and true positives
labeled by users so far and hypothesizes about other false
positives and true positives in the list based on various cla s-
siﬁcation and machine learning techniques. This hypothesi s
is then used to rearrange the remaining bug reports. It is
possible that a true positive, that is originally ranked low , is
moved up the list; a false positive, that is originally ranke d
high, is “downgraded” or pushed down the list.
The active reﬁnement process repeats and users are asked
for more feedbacks. With more iterations, more feedbacks
are received, and a better hypothesis can be made for the
remaining unlabeled reports.
The ultimate goal of our reﬁnement process is to produce
abetter ordering of bug reports so that true positive reports
are listed ﬁrst ahead of false positives, which we refer to
as the bug report ordering problem . With better ordering,
true positives can be identiﬁed earlier without the need to
investigate the entire report list. With less false positiv es
earlier in the list, a debugger can be encouraged to continue
investigating the rest of the reports and ﬁnd more bugs in a
ﬁxed period of time. If all (or most) of the true positives can
appear early, a debugger may stop analyzing the anomaly
reports once he or she ﬁnds many false positives.
V. R EFINEMENT ENGINE
This section elaborates our reﬁnement engine further. Our
reﬁnement engine takes in a list of anomaly reports and
reﬁnes it by reordering the reports. Each anomaly report
is a set of code clones ( i.e., a clone group) which contain
inconsistencies among the clones. Given a list of anomaly
reports, ordered either arbitrarily or with some ad-hoc cri -
teria, and user-provided labels ( i.e., true positives or false400Input: Bug Report List 
Feature Extraction From Source Code 
Classification Pre-Processing 
Feature 
Selection Balancing 
Reordered Input Discriminative Model 2
3
41
5
6
Figure 5. Reﬁnement Engine
positives) for some of the reports, our reﬁnement process
reorders unlabeled anomaly reports based on the predicted
likelihood of each of them being true positives. Figure 5
shows our reﬁnement engine that is composed of mainly
three blocks: feature extraction, pre-processing (includ ing
feature selection and data balancing), and classiﬁcation.
The feature extraction is meant to transform each clone
group into a set of features , where each feature is simply
a quantitative value that represents a certain property of t he
code. The set of features is also referred to as a data point .
In our case, we apply feature extraction to each inconsisten t
clone group reported by the clone anomaly detection tool
and collect a set of data points ( a.k.a. a dataset ) for all
clone groups in the reported list.
This feature set is then fed to the preprocessor, which
analyzes the data points, and may remove some features or
data points from the dataset. Its goal is to smooth over data
“noise” as much as possible before classiﬁcation.
The classiﬁer then takes a preprocessed dataset to mine a
classiﬁcation model that can discriminate features belong ing
to one class from the other. In our setting, the two classes
are false positive class and true positive class. We use class
labels (False andTrue ) to indicate whether a clone group
is a false or true positive. This mined model in turn is used to
predict the class labels of the reported clone groups that ha ve
received no user feedback. We also make our classiﬁcation
engine to provide the degree of likelihood for a clone group
to be in each of the two classes, which is used as a key to
rank and sort unlabeled clone groups.
A. Feature Extraction
Our feature extraction block analyzes parse trees which
are commonly used to represent programs written in various
languages. As a beneﬁt, it is easier to adopt our reﬁnement
engine to code written in different programming languages.
A parse tree node is labeled with different information to
represent various program constructs e.g., for, switch, et c.
Each clone is reported as a sub-tree rooted in a particular
node in a parse tree. The feature extraction would constructparse trees for every reported inconsistent clone group and
traverse the trees to collect features. More speciﬁcally, i t
performs the following two steps:
1) Tree Construction: For each clone in the anomaly
reports, we invoke a parser on the source ﬁle containing
the clone to construct a parse tree for the ﬁle; each node
in the tree contains a label indicating its type ( e.g.,for,
if,assignment , etc.). Then, we locate the root node of
the subtree that corresponds to the clone. We refer to this
subtree as a clone tree . We also locate the ﬁrst ancestor node
of this subtree that corresponds to the containing scope of
the clone in the source ﬁle, and refer to the subtree rooted
at this ancestor node as a clone ancestor tree .
Clone ancestor trees correspond to more code than clone
trees. They may contain more information that could help
decide whether an anomaly report is false or true positive.
Thus, we extract features from clone ancestor trees.
2) Representing Clone Ancestor Trees as Features: We
deﬁne ﬁve sets of features that could be extracted from
a clone ancestor tree, namely: basic, pair, proportional-
basic, proportional-pair, and rich. Consider a clone group
CG containing a set of clones corresponding to a set of
clone ancestor trees, the ﬁve sets of features are deﬁned in
Deﬁnitions 5.1, 5.2, 5.3, 5.4, and 5.5. Rich features belong
to the most comprehensive feature set that is a superset of
the other four feature sets. Our engine would convert the
clone ancestor trees into rich features.
Deﬁnition 5.1 (Basic Features): The basic feature set
(Basic ) of a clone group CG is the set of type-count pairs
in which each pair contains a node type and the number of
parse trees in CG having that particular type. For example,
considering the trees in Figure 6, the basic feature set
contains the following pairs: /an}bracketle{tCall, 2/an}bracketri}ht,/an}bracketle{tName, 2/an}bracketri}ht,/an}bracketle{tExpr -
list,2/an}bracketri}ht, and /an}bracketle{tExpr, 2/an}bracketri}ht. Mathematically, Basic (CG)=/vextendsingle/vextendsingle/vextendsingle/vextendsingle(t,|CS|), where
CS={c∈CG|c has a node of type t } ∧ |CS|>0/vextendsingle/vextendsingle/vextendsingle/vextendsingle
Deﬁnition 5.2 (Pair Features): The pair feature set ( Pair)
of a clone group CG is the set of type-count pairs in which
each pair contains a pair of node types and the number of
parse trees in CG having that particular pair. For example,
considering the trees in Figure 6, the pair feature set conta ins
the following pairs: /an}bracketle{tCall/Name, 2/an}bracketri}ht,/an}bracketle{tCall/Expr -list,2/an}bracketri}ht,
and/an}bracketle{tExpr -list/Expr, 2/an}bracketri}ht. Mathematically, Pair (CG)=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle((t1, t2),|CS|), where
CS={c∈CG| ∃n1,n2∈c.n1&n2are connected ∧
n1is of type t 1∧n2is of type t 2} ∧ |CS|>0/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
Deﬁnition 5.3 (Proportional Features—Basic):
The proportional-basic feature set ( Prop-Basic ) of a clone
group CG is the set of type-count pairs in which each
pair contains a node type and the proportion of parse
trees in CG having that particular type. For example, con-
sidering the trees in Figure 6, the Prop-Basic feature set401Code clone 1: 
... 
decode_sattr3(p, &args->attrs)
... Code clone 2: 
... 
decode_filename(p, &args->tname, &args->tlen)
... 
Type Count 
Call 2
Name 2
Expr-list 2
Expr 2Basic 
Type Count 
Call / Name 2
Call / Expr-
list 2
Expr-list / 
Expr 2Pair 
Type Proportion 
Call / Name 100% 
Call / Expr-list 100% 
Expr-list / Expr 100% Prop-Pair 
Type Proportion 
Call 100% 
Name 100% 
Expr-list 100% 
Expr 100% Prop-Basic 
Type Value
Num 2
Avg 5.5 Other Call 
Name Expr-
list 
Expr Expr Call 
Name Expr-
list 
Expr Expr Expr 
Rich Clone Group 
Figure 6. Feature Extraction
contains the following pairs: /an}bracketle{tCall, 100% /an}bracketri}ht,/an}bracketle{tName, 100% /an}bracketri}ht,
/an}bracketle{tExpr -list,100% /an}bracketri}ht, and /an}bracketle{tExpr, 100% /an}bracketri}ht. Mathematically,
PrBasic (CG)=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle(t,|CS|
|CG|), where
CS={c∈CG|c has a node of type t } ∧ |CS|>0/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
Deﬁnition 5.4 (Proportional Features—Pair):
The proportional-pair feature set ( Prop-Pair ) of a clone
group CG is the set of type-count pairs in which each
pair contains a pair of node types and the proportion of
parse trees of CG having that particular pair. For exam-
ple, considering the trees in Figure 6, the Prop-Pair fea-
ture set contains the following pairs: /an}bracketle{tCall/Name, 100% /an}bracketri}ht,
/an}bracketle{tCall/Expr -list,100% /an}bracketri}ht, and /an}bracketle{tExpr -List/Expr, 100% /an}bracketri}ht.
Mathematically, PrPair (CG)=
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle((t1, t2),|CS|
|CG|), where
CS={c∈CG|∃n1,n2∈c.n1&n2are connected ∧
n1is of type t 1∧n2is of type t 2} ∧ | CS|>0/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
Deﬁnition 5.5 (Rich Features): The rich feature set
(Rich) of a clone group CG is the union of the Basic,
Pair, Prop-Basic, Prop-Pair feature sets, plus two additio nal
features: the number of clones in CG (Num), and the
average size of the clones in CG (Avg). For example,
considering the trees in Figure 6, the Rich feature set is the
union of other features sets plus two additional features:
/an}bracketle{tNum, 2/an}bracketri}ht, and /an}bracketle{tAvg, 5.5/an}bracketri}ht. Mathematically, Rich (CG)=
Basic (CG)∪Pair (CG)∪PrBasic (CG)∪
PrPair (CG)∪ {(Num, |CG|),(Avg,Σc∈CG.|c|
|CG|)}
B. Preprocessing
We consider two pre-processing options: feature selection
and dataset re-balancing. Feature selection is to reducethe number of features by removing unimportant ones.
Unimportant features are noise and are good to be removed.
Also, as our data contains much more false positives than
true positives, we need to re-balance the dataset; otherwis e
the discriminative model would be biased to always label
unknown reports as false positives.
1) Feature Selection: Various approaches have been pro-
posed to select important features. Information gain has be en
widely used to evaluate the usefulness of a feature, e.g., [3 3].
If we use cto denote the class labels (true positive [ +ve
class] vs. false positive [ −veclass]), and use fto represent
a feature, then information gain of fis deﬁned as in Eq.(1).
IG(c|f) =H(c)−H(c|f) (1)
where H(c) =−/summationtext
ci∈{±ve}P(ci) logP(ci)is the entropy
andH(c|f) =−/summationtextP(f)/summationtext
ci∈{±ve}P(ci|f) logP(ci|f)is
the conditional entropy given the feature f.
We select important features based information gain and
the Weka toolkit [34] with its default conﬁguration.
2) Dataset Re-balancing: To re-balance the dataset, we
reduce the number of data points in the larger class. We
retain all data points in the smaller class. For each data poi nt
in the smaller class, we ﬁnd the most similar data points in
the other class and retain it—cosine similarity [35] betwee n
two feature sets corresponding to the two data points is
used as the similarity measure. Other data points in the
larger class are dropped. This is motivated by the nearest
neighbor approach by Renieris and Reiss that localizes bugs
by comparing the nearest faulty and correct executions [36] .
In their setting, they also have the issue of imbalanced
dataset: correct executions are many more than faulty ones.
C. Classiﬁcation
The classiﬁcation block takes preprocessed datasets and
learns a discriminative model that discriminates true posi -402tives from false ones. We refer to the true and false positive s
as class labels. The purpose of a discriminative model
is to take an unlabeled datapoint ( i.e., a datapoint or an
anomaly report that is not known to be a true positive or
a false positive) and assign a class label to it. To produce
a discriminative model, the classiﬁer learns from a given
labeled training data points. In our case, the training data
points are the clone reports that have been investigated by
developers to be true positives or false positives.
In this paper, we use a variant of nearest neighbor clas-
siﬁcation scheme, namely nearest neighbor with non-nested
generalization (NNGe) [37]. Nearest neighbor classiﬁcati on
has been proved successful for various tasks, e.g., [38]. Al so,
this technique matches our intuition: an instance similar t o
known false positives is likely to be a false positive. Our
initial study showed that NNGe performs no worse than
other common classiﬁcation approaches. We describe the
technique in the following.
1) Nearest Neighbor with Non-Nested Generalization:
As its name suggests, in nearest neighbor classiﬁcation,
unknown data would be assigned with the same label as its
nearest neighbor. The time needed to build a model would
be little as it only involves index building and distance
calculation [39].
The nearest neighbor approach can not generalize or group
several data points together, which potentially reduces it s
classiﬁcation accuracy. Thus, it has been extended with
generalization [40]. Rather than loading all data points in to
the memory, the training phase constructs multi-dimension al
rectangles ( i.e., hyper-rectangles) that generalize a few data
points in a multi-dimensional space. This approach has
poor performance on some settings due to nested gener-
alization ( i.e., hyper-rectangles are contained inside other
hyper-rectangles or overlap with one another) [41]. Martin
addresses this issue by proposing nearest neighbor with non -
nested generalization ( NNGe ) [37] which we use in this
work. In particular, we use the implementation available in
Weka [34] with its default distance function.
In this work, we extend NNGe to output the likelihood
for a data point dpto belong to each of the two classes
(i.e., true positives (T) and false positives (F)). Let’s refer
to the set of exemplars, the set of exemplars with label
T, and the set of exemplars with label F as D,DT, and
DF, respectively. Also, considering an exemplar d, let
sim(dp, d) = 1 −dist(dp, d), where dist(dp, d)is the
distance between dpto the exemplar dwhich ranges from
0 to 1. Our likelihood measure to re-sort the bug reports is
given by the following formula:
LH(dp) = 0 .5 +RS(dp)
2
RS(dp) =|/summationtext
dT∈DTsim(dp, d T)|
|DT|−|/summationtext
dF∈DFsim(dp, d F)|
|DF|The LH measure corresponds to the normalized relative
similarity of a datapoint dpto the datapoints in DTas
compared to those in DF. Bug reports with higher LH are
more likely to be true positives and would be listed ﬁrst.
D. Concrete Reﬁnement Process
Algorithm 1 is the pseudo-code of our reﬁnement process.
It takes in several parameters: the list of bug reports (BR)
from a bug detection tool, the initial number of bug reports
to be labeled ( k), and the feedback pool size ( p). The
process would be bootstrapped by manually labeling the
ﬁrstkbug reports which are used to train an initial model
(Lines 1–5). The classiﬁcation model is then used to re-
sort the unlabeled bug reports (Line 6). The next top p
reports are presented for user feedback (Lines 7–8). We
only repeat the reﬁnement process after pnew feedback are
obtained. Then, the feedback are incorporated by learning a
new discriminative model and applying it to the remaining
unlabeled bug reports in the reﬁnement loop (Lines 3–14).
When the false positive rate goes too high, a user can choose
to stop the reﬁnement process (Lines 10–11).
With accumulated user feedback (Lines 8 and 13), the
reﬁnement process can incrementally improve the classiﬁ-
cation and ranking accuracy of the discriminative models so
that true positives can be ranked higher.
Algorithm 1 Clone Report Reﬁnement Process
Input : BR: Bug Reports
k: Initial set of bug reports to be labeled
p: Feedback pool size
Output : Re-ordered Bug Reports
1: Let BK = Select the ﬁrst kbug reports
2: Label all bug reports in BK (manual)
3: Let FK = Features extracted from BK
4: Perform pre-processing on FK
5: Let M = Classiﬁcation model created from FK
6: Reﬁne BR using M
7: Let BP = Select the new top punlabeled bug reports
8: Ask for user feedback on bug reports in BP
9: Let FPRate = Compute false positive rate
10: If FPRate is too high (based on user feedback)
11: Stop
12: Else
13: Set BK = BK ∪BP
14: Goto 3
VI. E VALUATION CRITERIA
In this section we deﬁne a suitable metric to measure
the quality of the re-sorted bug reports to evaluate the
effectiveness of our active reﬁnement process.
Our reﬁnement process is effective if it could re-sort the
reports such that all reports corresponding to true positiv es
are listed ﬁrst. As an illustration, consider a scenario whe re
our reﬁnement process starts with a set of klabeled bug
reports and there are mtrue positive reports among the
remaining unlabeled reports. The ideal situation happens
when all mother true positives are listed in the (k+ 1)thto
(k+m)thpositions after the reﬁnement process. The worst403case happens when the true positives are listed as the last
mreports after reﬁnement.
To measure the quality of the reﬁnement process, we
adapt a measure proposed in test case prioritization area—
average percentage faults detected (APFD) [19]. There are
a number of similarities between test case prioritization a nd
our problem. In test case prioritization, test cases need to
be sorted ( i.e., prioritized) in the order of their likelihood
to reveal program failures. Also, there is a need to measure
and compare the quality of different test case orderings.
In [19], a graph capturing the cumulative proportion of
faults captured as more test cases are run is plotted. APFD
deﬁned as the area under this curve measures the rate of fault
detection . In our work, we use the same concept and plot a
graph capturing the cumulative proportion of true positive s
found as more anomaly reports are inspected by users. We
refer to this graph as the cumulative true positives curve .
In the cumulative true positives curve , a larger area under
the curve indicates that more true positives are found by
developers early, which means that the reﬁnement process
effectively re-sorts the anomaly reports. Consider the sam ple
graphs in Figure 7 and assume that there are ﬁve true
positives among 10 reports. Each of the ﬁve increments
in each of the two cumulative curves corresponds to when
each of the ﬁve true positives is found. The left curve
shows that true positives are found at positions 1, 2, 3,
7, and 8. The right curve shows that true positives are
found at positions 1, 2, 3, 4, and 5. Using the original
list of reports (left), the developer could only ﬁnd three
true positives by investigating the ﬁrst six reports. Using
the reﬁned list produced by the reﬁnement process (right),
ﬁve true positives are found in the ﬁrst six reports. Hence,
by performing the same number of inspections (which may
correspond to the time budget a developer has in real-world
situations), the developer could ﬁnd more true positives
using the reﬁned report list as compared to the original one.
In this case, the reﬁned report list has a better true positiv e
detection rate. In the graphs, this improvement is indicate d
by a larger area under the curve for the reﬁned report list.
The idea of using APFD as the evaluation criteria for
bug ﬁnding is also used by Kremenek and Engler [31].
Following the same idea, we deﬁne average percentage true
positives found (APPF) as the area under the cumulative
true positives curve . Our goal is to improve the APPF score
which measures the rate of true positives found. We illustra te
APPF improvement computation in Figure 7.
VII. E MPIRICAL EVALUATION
In this section, we describe our experimental settings, our
evaluation results, and the threats to validity.
A. Settings and Results
1) Settings: We evaluate our approach on clone-based
anomaly reports for three real programs written in differen t020 40 60 80 100 
10 
20 
30 
40 
50 
60 
70 
80 
90 
100 Cumulative True Positives Curve % True Positives Found 
% Bug Reports Investigated APPF=62% 
020 40 60 80 100 
10 
20 
30 
40 
50 
60 
70 
80 
90 
100Cumulative True Positives Curve% True Positives Found 
% Bug Reports Investigated APPF=72% 
Original 
APPF Improvement = (70-62)/62 = 12.9% Refined 
Figure 7. Computing and Comparing APPF
programming languages: the Linux kernel (C), Eclipse and
ArgoUML (Java). We analyze the reports generated by Jiang
et al. [12]. We choose these reports as they contain a large
number of false positives. There are more than 800anomaly
reports ( i.e., clone groups) for the Linux kernel, and only
57of them are true positives or programming style issues.
There are more than 400 anomaly reports for Eclipse, and
only 38of them are true positive. There are more than 50
anomaly reports for ArgoUML, and only 15of them are true
positive. Finding a few true positives on the large number
of false positives is a challenging task that would stress te st
the usability of our approach. The authors from [12] have
manually labeled all the reported inconsistent clone group s
from the Linux kernel and Eclipse as either true positives or
false positives. We manually label the reported inconsiste nt
clone groups from ArgoUML. We use these clone groups
and their labels to simulate initial and incremental user
feedbacks as inputs to our reﬁnement engine.
The tool in [12] returns the list of anomalies in a particular
order. We take the ordering returned by the tool and reﬁne it.
Following the steps in Section V, we initially take the ﬁrst k
labeled clone groups. We set kto be 50 since there is only
one true positive among the ﬁrst 50bug reports for Eclipse.
We also use k= 50 for the Linux kernel. Since there are
only 50 inconsistent clone groups reported for ArgoUML,
we set kto be 10 for ArgoUML. We set the feedback pool
size (i.e., p) to be 1. We thus iteratively reﬁne the bug reports
as each feedback is received. In this paper, we repeat the
reﬁnement process until all anomaly reports are inspected.
2) Evaluation Results: We improve APPF by 11%, 87%,
and 86% for the Linux kernel, Eclipse, and ArgoUML bug
reports respectively. These measures mean that within a
limit period of time, a developer investigating the anomaly
reports may ﬁnd more true positives in the Linux kernel,
Eclipse, and ArgoUML. The improvement for the Linux
kernel is not as much as Eclipse and ArgoUML. The bugs
in the Linux kernel often involves identiﬁer changes (e.g.,
variations in variable names, function names, type names,
etc.) which are not captured well by our feature sets which
are mostly based on syntactical node types, while the bugs404Table II
TOP-5 R E-ORDERINGS .x/mapsto→yMEANS THAT A REPORT OF A TRUE
POSITIVE AT POSITION xIS REORDERED TO POSITION y.
System Top-5 Re-orderings
Linux 694/mapsto→18, 672/mapsto→64, 760/mapsto→131, 770/mapsto→179,
792/mapsto→206
Eclipse 373/mapsto→4, 348/mapsto→11, 394/mapsto→29, 388/mapsto→43, 370/mapsto→49
ArgoUML 40/mapsto→12, 35/mapsto→15, 34/mapsto→11, 29/mapsto→9, 23/mapsto→8
Table III
TOP3 FEATURES BASED ON THEIR INFORMATION GAIN: LINUX
KERNEL . THE##SYMBOL IS THE SEPARATOR BETWEEN TWO FEATURES
FOR A PAIR FEATURE SET . THEPSUPERSCRIPT DENOTES A
PROPORTIONAL FEATURE .
Top Feature Info. Gain
1 extdefP0.015941
2 extdef 1P0.015941
3 program##extdefsP0.015941
in Eclipse and ArgoUML often involve conditionals which
may be better captured by our features. In future, we plan to
add more features to construct better discriminative model s
for Linux and more programs.
The top-5 successful re-orderings for the Linux kernel,
Eclipse and ArgoUML are shown in Table II. We highlight
sample bugs that are successfully reordered. Figure 8 shows
a buggy clone group in Linux that is reordered from position
694 to 18. The bug is related to an early unlock of a variable.
Figure 9 shows a bug from Eclipse that is successfully
reordered from position 373 to 4. The bug is similar to the
bug in Figure 1; it misses a null-check in code fragment 2,
and was reported to developers and ﬁxed. For ArgoUML, a
bug shown in Figure 10 is reordered from position 40 to 12.
This bug is related to a missing validation before a variable
is used in the next statement.
To further investigate which program elements ( i.e., fea-
tures as described in Section V-A) may be better bug
indicators than others, we compute the information gain [42]
of each feature in Linux and Eclipse bug reports. Informa-
tion gain is frequently used to ﬁnd important features that
differentiate two contrasting datasets ( i.e., in our case, true
positives and false positives), e.g., [33].
The top 3 features for Linux kernel, Eclipse, and Ar-
goUML are shown in Table III, IV, and V. We notice that
the individual features have low information gain. Thus,
individually they are not able to distinguish true positive s
from false positives. However, composing them into a dis-
criminative model is more effective in improving the rate of
true positives found. From the list, one could intuitively i nfer
that if a clone from Eclipse involves inconsistent changes
related to conditionals, it is more likely to be buggy. For
ArgoUML, the inconsistent changes that involve variable
declaration and initialization are more likely to be buggy,
e.g., a declared variable being used without further valida tion
or checking (null checking), a variable needs to be converte d
to an appropriate type, etc. As discriminative features us-
inginformation gain could mean that the features are eitherTable IV
TOP3 FEATURES BASED ON THEIR INFORMATION GAIN: ECLIPSE . THE
BSUPERSCRIPT DENOTES A BASIC FEATURE WHILEPSUPERSCRIPT
DENOTES A PROPORTIONAL FEATURE
Top Feature Info. Gain
1 BOOL OR TKP0.01898
2 conditional orexpression ## 0.01898
conditional orexpressionP
3 BOOL OR TKB0.01898
Table V
TOP3 FEATURES BASED ON THEIR INFORMATION GAIN: ARGOUML
Top Feature Info. Gain
1 local variable declaration statementB0.145772
2 variable initializerB0.145772
3 block statement ## 0.145772
local variable declaration statementB
highly related to buggy clone or highly related to non-
buggy clone, in Linux kernel, if a clone involves inconsiste nt
changes related to global deﬁnitions ( e.g., extdef), it is
more likely not a bug. Overall, our approach helps to better
separate false positives from true bugs, making ﬁrst listed
anomaly reports closer towards the top left cell of the ideal
four quadrants of Figure 3.
B. Threats to Validity
Threat to construct validity corresponds to the suitablene ss
of our evaluation metric. In this study we adapt a measure
commonly used in test case prioritization which also needs
to re-sorts ( i.e., prioritize) test cases. Their goal is to ﬁnd
an optimal ordering of test cases that would identify the
failures early. They measure the quality of an ordering usin g
average percentage faults detected (APFD). We propose a
similar measure referred to as average percentage of true
positives found (APPF). Similar like APFD that measures
the rate of fault detection, APPF measures the rate of true
positives found. We believe this measure is relevant in
measuring the performance of a reﬁnement framework. A
higher APPF score indicates that within the same period of
time a developer can ﬁnd more true positives.
Threat of internal validity corresponds to the ability of ou r
experiments to link the independent and dependent variable s.
The threat could be manifested due to experimental or
human errors. The labels of the bug reports are decided
manually by the authors of [12]. The labeling might be prone
to errors. Still, the authors of [12] and us have taken some
precautions to prevent these to happen – at least two people
are assigned to label each of the inconsistent clone group;
for any discrepancy, a third person would break the tie.
Threat of external validity corresponds to the generaliz-
ability of our result. We have performed a study on three
large real systems that are written in two most popular
programming languages: C and Java. Although these help,
there is still a threat to external validity. In the future,
we plan to investigate more systems written in various
programming languages.405Code Fragment 1 Code Fragment 2
File: linux-2.6.19/drivers/net/wireless/bcm43xx/bcm 43xx_sysfs.c 
347: struct bcm43xx_private *bcm = dev_to_bcm(dev); 
…
351: mutex_lock(&(bcm)->mutex); 
352: switch (bcm43xx_current_phy(bcm)->type) { 
353:   case BCM43xx_PHYTYPE_A: 
…      
362:   default: 
363:     assert(0); 
364: }
365: mutex_unlock(&(bcm)->mutex); File: linux-2.6.19/drivers/net/wireless/bcm43xx/bcm 43xx_wx.c 
615: struct bcm43xx_private *bcm = bcm43xx_priv(net_d ev); 
…
618: mutex_lock(&bcm->mutex); 
619: mode = bcm43xx_current_radio(bcm)->interfmode;
620: mutex_unlock(&bcm->mutex); 
621: switch (mode) { 
622:   case BCM43xx_RADIO_INTERFMODE_NONE: 
…
632:   default: 
633:     assert(0); 
634: .. 
Figure 8. Report of a true positive in Linux that is reordered from position 694 to 18
Code Fragment 1 Code Fragment 2 
File: eclipse-cvs20070108/org.eclipse.debug.core/co re/org/ 
eclipse/debug/internal/core/LaunchConfiguration.jav a 
253: if (file != null) { 
254: // validate edit 
255: if (file.isReadOnly()) { 
256: IStatus status =  ResourcesPlugin. 
getWorkspace().validateEdit(new IFile[] {file}, nul l); 
257: if (!status.isOK()) { 
258: throw new CoreException(status); 
259: ...... File: eclipse-cvs20070108/org.eclipse.debug.core/co re/org/ 
eclipse/debug/internal/core/LaunchConfigurationWork ingCopy.java
310: ...... 
311: // validate edit 
312: if (file.isReadOnly()) { 
313: IStatus status = ResourcesPlugin. 
getWorkspace().validateEdit(new IFile[] {file}, nul l);
314: if (!status.isOK()) { 
315: throw new CoreException(status); 
316: ...... Code Fragment 1 Code Fragment 2 
File: eclipse-cvs20070108/org.eclipse.debug.core/co re/org/ 
eclipse/debug/internal/core/LaunchConfiguration.jav a 
253: if (file != null) { 
254: // validate edit 
255: if (file.isReadOnly()) { 
256: IStatus status =  ResourcesPlugin. 
getWorkspace().validateEdit(new IFile[] {file}, nul l); 
257: if (!status.isOK()) { 
258: throw new CoreException(status); 
259: ...... File: eclipse-cvs20070108/org.eclipse.debug.core/co re/org/ 
eclipse/debug/internal/core/LaunchConfigurationWork ingCopy.java
310: ...... 
311: // validate edit 
312: if (file.isReadOnly()) { 
313: IStatus status = ResourcesPlugin. 
getWorkspace().validateEdit(new IFile[] {file}, nul l);
314: if (!status.isOK()) { 
315: throw new CoreException(status); 
316: ...... 
Figure 9. Report of a true positive in Eclipse that is reorder ed from position 373 to 4
Code Fragment 1 Code Fragment2 
File: /argouml/src/argouml-
app/src/org/argouml/uml/diagram/UMLMutableGraphSupp ort.java 
331: if (edge instanceof CommentEdge) {            
332:     …       
333:  } else if (Model.getFacade().isARelationship (edge)                  
334:          || Model.getFacade().isATransition (edge)                
335:          || Model.getFacade().isAAssociationEn d (edge)){            
336:     return Model.getUmlHelper().getDestination (edge);      
337  } else if (Model.getFacade().isALink(edge)) {          
338:  ..                 
339: }File: /argouml/src/argouml-
app/src/org/argouml/uml/diagram/UMLMutableGraphSupp ort.java 
360: if (edge instanceof CommentEdge) {            
361:     … 
362:  } else if (Model.getFacade().isAAssociation (edge)) {       
363:     List conns = new  
ArrayList(Model.getFacade().getConnections (edge));      
364:     return conns.get(1);        
365:  } else if (Model.getFacade().isARelationship (edge)         
366:          || Model.getFacade().isATransition (edge)           
367:          || Model.getFacade().isAAssociationEn d (edge)){     
368:     return Model.getUmlHelper().getDestination (edge);      
369  } else if (Model.getFacade().isALink(edge)) {          
370:  ..                 
371: }
Figure 10. Report of a true positive in ArgoUML that is reorde red from position 40 to 12
VIII. C ONCLUSION AND FUTURE WORK
Code clones have been widely studied in the literature.
Various techniques have been proposed to recover clones
from a code base. One important usage of clones is to ﬁnd
bugs by detecting inconsistencies among members of the
same clone group. These correspond to bugs that might arise
due to inconsistent updates among parallel code fragments
or violation of a common programming practice. Past tech-
niques, e.g., [12], have demonstrated the ability of clone-
based bug detection tools to detect true positives in large
systems. However, often the number of false positives are
too many. This could affect the usability of such a system
as a developer could spend a lot of time in performing a
debugging activity, which at the end might be futile, as the
reported anomaly is a false alarm.
Our work tries to address this issue by proposing an
approach to automatically reﬁne bug reports by the incorpo-
ration of user feedback. Rather than having a static sorted list
of bug reports, our bug reports are dynamic . As a user inves-
tigates the top few bug reports and feedback to the system,
the system automatically re-sorts the remaining uninvesti -
gated bug reports, and thus reﬁnes it. This reﬁnement processis performed multiple times as more feedback is available.
For each reﬁnement, we perform feature extraction, pre-
processing (feature selection and dataset re-balancing), and
discriminative model learning. To evaluate the quality of a
list of ordered bug reports we use average percentage of
true positives found (APPF) which measure the rate true
positives are found. We evaluate our reﬁnement process on
three sets of clone-based anomaly reports from three large
real programs: the Linux kernel (C), Eclipse, and ArgoUML
(Java), extracted by a clone-based anomaly detection tool.
The results show that, compared to the original ordering of
bug reports, we can improve APPF by 11%, 87%, and 86%
for Linux kernel, Eclipse, and ArgoUML, respectively.
As future work, we plan to extend our approach to reﬁne
not only clone-based anomaly reports but also other types of
anomaly reports. We also plan to investigate the applicabil ity
of models learned from one or more software systems to
reﬁne bug reports of other software systems.
ACKNOWLEDGMENTS
We appreciate the valuable feedbacks from anonymous
reviewers for earlier versions of this paper.406REFERENCES
[1] S. Livieri, Y . Higo, M. Matsushita, and K. Inoue, “Very-l arge
scale code clone analysis and visualization of open source
programs using distributed ccﬁnder,” in ICSE , 2007.
[2] I. D. Baxter, C. Pidgeon, and M. Mehlich, “DMS c/circlecopyrt: Program
transformations for practical scalable software evolutio n,” in
ICSE , 2004.
[3] B. S. Baker, “Finding clones with Dup: Analysis of an
experiment,” IEEE TSE , 2007.
[4] T. Kamiya, S. Kusumoto, and K. Inoue, “CCFinder: A
multilinguistic token-based code clone detection system f or
large scale source code,” IEEE TSE , 2002.
[5] L. Jiang, G. Misherghi, Z. Su, and S. Glondu, “D ECKARD :
Scalable and accurate tree-based detection of code clones, ” in
ICSE , 2007.
[6] Y . Higo, S. Kusumoto, and K. Inoue, “A metric-based ap-
proach to identifying refactoring opportunities for mergi ng
code clones in a java software system,” Journal of Software
Maintenance , vol. 20, no. 6, pp. 435–461, 2008.
[7] E. Duala-Ekoko and M. P. Robillard, “Tracking code clone s
in evolving software,” in ICSE , 2007.
[8] T. T. Nguyen, H. A. Nguyen, N. H. Pham, J. M. Al-Kofahi,
and T. N. Nguyen, “Clone-aware conﬁguration management,”
inASE, 2009.
[9] M. Kim, V . Sazawal, D. Notkin, and G. Murphy, “An empir-
ical study of code clone genealogies,” in ESEC/FSE , 2005.
[10] C. Kapser and M. W. Godfrey, ““cloning considered harmf ul”
considered harmful,” in WCRE , 2006.
[11] E. Juergens, F. Deissenboeck, B. Hummel, and S. Wagner,
“Do code clones matter?” in ICSE , 2009.
[12] L. Jiang, Z. Su, and E. Chiu, “Context-based detection o f
clone-related bugs,” in ESEC/SIGSOFT FSE , 2007.
[13] J. Krinke, “A study of consistent and inconsistent chan ges to
code clones,” in WCRE , 2007.
[14] Z. Li, S. Lu, S. Myagmar, and Y . Zhou, “CP-Miner: A tool
for ﬁnding copy-paste and related bugs in operating system
code,” in OSDI , 2004.
[15] M. Gabel, J. Yang, Y . Yu, M. Goldszmidt, and Z. Su,
“Scalable and systematic detection of buggy inconsistenci es
in source code,” in OOPSLA , 2010.
[16] Y . Hayase, Y . Y . Lee, and K. Inoue, “A criterion for ﬁlter ing
code clone related bugs,” in DEFECTS , 2008.
[17] J.-G. Lee, J. Han, X.Li, and H. Cheng, “Mining discrimin ative
patterns for classifying trajectories on road networks.” IEEE
Trans. Knowl. Data Eng. , 2011.
[18] A. Bosch, A. Zisserman, and X. Munoz, “Scene classiﬁcat ion
using a hybrid generative/discriminative approach.” IEEE
Trans. Pattern Anal. Mach. Intell. , 2008.
[19] S. Elbaum, A. Malishevsky, and G. Rothermel, “Test case
prioritization: A family of empirical studies.” IEEE Trans.
Software Eng. , vol. 28, pp. 159–182, 2002.
[20] V . Wahler, D. Seipel, J. W. von Gudenberg, and G. Fischer ,
“Clone detection in source code by frequent itemset tech-
niques,” in SCAM , 2004.
[21] R. Komondoor and S. Horwitz, “Using slicing to identify
duplication in source code,” in SAS, 2001.[22] A. Podgurski and L. Pierce, “Retrieving reusable softw are by
sampling behavior,” ACM TOSEM , 1993.
[23] S. Jarzabek and S. Li, “Eliminating redundancies with
a “composition with adaptation” meta-programming tech-
nique,” in ESEC/FSE , 2003.
[24] D. C. Rajapakse and S. Jarzabek, “Using server pages to u nify
clones in web applications: A trade-off analysis,” in ICSE ,
2007.
[25] P. Jablonski and D. Hou, “CReN: A tool for tracking copy-
and-paste code clones and renaming identiﬁers consistentl y
in the IDE,” in ETX, 2007.
[26] S. Kim, T. Zimmermann, E. J. W. Jr., and A. Zeller, “Pre-
dicting faults from cached history,” in ICSE , 2007.
[27] S. Kim and M. D. Ernst, “Which warnings should i ﬁx ﬁrst?”
inESEC-FSE , 2007.
[28] J. R. Ruthruff, J. Penix, J. D. Morgenthaler, S. Elbaum,
and G. Rothermel, “Predicting accurate and actionable stat ic
analysis warnings: an experimental approach,” in ICSE , 2008.
[29] T. Zimmermann, N. Nagappan, H. Gall, E. Giger, and B. Mur -
phy, “Cross-project defect prediction: a large scale exper iment
on data vs. domain vs. process,” in ESEC/FSE , 2009.
[30] A. Podgurski, D. Leon, P. Francis, W. Masri, M. Minch,
J. Sun, and B. Wang, “Automated support for classifying
software failure reports,” in ICSE , 2003, pp. 465–477.
[31] T. Kremenek and D. Engler, “Z-ranking: Using statistic al
analysis to counter the impact of static analysis approxima -
tion,” in SAS, 2003, pp. 295–315.
[32] S. Heckman and L. Williams, “On establishing a benchmar k
for evaluating static analysis alert prioritization and cl assiﬁ-
cation techniques,” in ESEM , 2008, pp. 41–50.
[33] J. Quinlan, C4.5: Programs for Machine Learning . Morgan
Kaufmann, 1993.
[34] G. Holmes, A. Donkin, and I. Witten, “Weka: A machine
learning workbench.” in Proc Second Australia and New
Zealand Conference on Intelligent Information Systems , 1994.
[35] J. Han and M. Kamber, Data Mining Concepts and Tech-
niques , 2nd ed. Morgan Kaufmann, 2006.
[36] M. Renieris and S. Reiss, “Fault localization with near est
neighbor queries,” in ASE, 2003, pp. 30–39.
[37] B. Martin, “Instance-based learning : Nearest neighbo r with
generalization.” in Thesis, University of Waikato, Hamilton,
New Zealand , 1995.
[38] S. Tan, “An effective reﬁnement strategy for knn text classi-
ﬁer,” Expert Syst. Appl. , 2006.
[39] A. Moore, “An introductory tutorial on KD-Trees,” Comp uter
Laboratory, University of Cambridge, Tech. Rep. 209, 1991,
extract from Andrew Moore PhD Thesis: Efﬁcient Memory-
based Learning for Robot Control.
[40] S. Salzberg, “A nearest hyperrectangle learning metho d,”
Machine Learning , 1991.
[41] D. Wettschereck and G. Dietterich, “An experimental co m-
parison of the nearest-neighbour and nearest-hyperrectan gle
algorithms.” Machine Learning , 1994.
[42] T. M. Mitchell, Machine Learning . New York, USA:
McGraw-Hill, March 1997.407