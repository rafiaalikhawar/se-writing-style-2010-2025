Unleashing Concurrency for Irregular Data Structures
Peng Liu*y
lpxz@ust.hkCharles Zhangy
charlesz@cse.ust.hk
*State Key Laboratory of Software EngineeringyHong Kong University of Science and Technology
Wuhan University
China China
ABSTRACT
To implement the atomicity in accessing the irregular data
structure, developers often use the coarse-grained locking
because the hierarchical nature of the data structure makes
the reasoning of ne-grained locking dicult and error-prone
for the update of an ancestor eld in the data structure may
aect its descendants. The coarse-grained locking disallows
the concurrent accesses to the entire data structure and leads
to a low degree of concurrency. We propose an approach,
built upon the Multiple Granularity Lock (MGL), that re-
places the coarse-grained locks to unleash more concurrency
for irregular data structures. Our approach is widely applica-
ble and does not require the data structures to have special
shapes. We produce the MGL locks through reasoning about
the hierarchy of the data structure and the accesses to it.
According to the evaluation results on widely used applica-
tions, our optimization brings the signicant speedup, e.g.,
at least 7%-20% speedup and up to 2X speedup.
Categories and Subject Descriptors
D.3.4 [ Processors ]: Optimization
General Terms
Reliability, Performance, Design
Keywords
Synchronization, ne-grained locking, data structure, li-
brary
1. INTRODUCTION
Irregular data structures [17, 22, 25], such as graphs, sets,
and lists, use pointers to dynamically adapt to the computa-
tion needs of many applications. They are typically provided
as libraries and hidden behind APIs. When they are used in
multi-threaded programs, developers commonly use locks to
achieve the correctness properties such as atomicity, which re-
quires a group of accesses to the data structure be completed
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proÔ¨Åt or commercial advantage and that copies
bear this notice and the full citation on the Ô¨Årst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciÔ¨Åc
permission and/or a fee.
ICSE ‚Äô14, May 31 ‚Äì June 7, 2014, Hyderabad, India
Copyright 2014 ACM 978-1-4503-2756-5/14/05 ...$15.00.
Figure 1. Concurrent update of disjoint parts of the
data structure from the STMBench7 benchmark. In
the object graph, each circle stands for an object
(labeled with Type @Identifier ) and each edge stands
for a eld reference (labeled with the eld name).
without interruption. At the same time, the developers also
face tough challenges when specifying the locks in balancing
between correctness and performance. We illustrate this
problem with an example.
Figure 1 depicts the object graph of a tree-like data struc-
ture, where the objects connect to each other via the eld
reference. To guarantee the atomicity of the methods that
access the data structure, developers often [6] specify coarse-
grained locks that disallow the concurrent accesses to the
data structure. For example, with the coarse-grained locking,
all threads that access the object graph of the data structure
in Figure 1 are synchronized on the same object, e.g., the
root of the graph. The coarse-grained locking drastically
limits the concurrency as it denies the concurrent updates
of the disjoint parts of the object graph (highlighted as grey
in Figure 1) and, therefore, leads to the poor scalability
with respect to the number of threads and the unsatisfactory
performance.
In Figure 1, we could allow the concurrent updates of the
disjoint parts if we use the ne-grained locking, e.g., one
lock for synchronizing the accesses to the subgraph rooted
atManual @2and the other for the accesses to the sub-
graph rooted at ComplexAssembly @3. However, it is gen-
erally challenging [6] to reason about and implement the
ne-grained locking for the data structure due to its hier-
archical nature. On the one hand, the insucient synchro-
nization would cause the atomicity violation. Consider the
data structure in Figure 2b, the update ( ancestor update ) of
an ancestor eld aects the accesses ( descendant accesses ) to
its descendant elds as they belong to the subgraph rooted
at the ancestor. Following the atomicity, the accesses to
the descendant eld title by a thread, as shown in Fig-Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proÔ¨Åt or commercial advantage and that copies bear this notice and the full citation
on the Ô¨Årst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speciÔ¨Åc permission and/or a
fee. Request permissions from Permissions@acm.org.
ICSE‚Äô14 , May 31 ‚Äì June 7, 2014, Hyderabad, India
Copyright 2014 ACM 978-1-4503-2756-5/14/05...$15.00
http://dx.doi.org/10.1145/2568225.2568277
480
ure 2a, should return the consistent values. However, they
may lead to inconsistent values if they are interleaved by
another thread that misses the synchronization and updates
the ancestor eld man, leading to atomicity violation. On
the other hand, the overly restrictive synchronization may
drastically limit the concurrency. Specically, one way to
achieve the atomicity in the above case is to ask the threads
to acquire the ne-grained locks for all elds involved, which
include both the eld that contains the intended value, e.g.,
the eld title , and the elds on the reference path from the
root to it, e.g., the eld man. However, the concurrency is
limited in this way. If two threads update two elds, e.g.,
the elds title and text (highlighted in Figure 3a), respec-
tively, of which the reference paths share some common eld,
e.g., the eld man, the threads cannot run concurrently as
they contend for the lock protecting the common eld. To
summarize, we encounter the conundrum, i.e., the insu-
cient synchronization causes the atomicity violation while
the overly restrictive synchronization limits the concurrency.
Ideally, we want to simultaneously guarantee the atomicity
and allow a high degree of concurrency, e.g., enabling the
concurrent updates in Figure 3a.
In this paper, we present an automated approach that
replaces the coarse-grained locks to allow the concurrent
accesses to disjoint parts of data structures, as illustrated
in Figure 1 and Figure 3a, while preserving the atomicity,
as shown Figure 2a. Dierent from the techniques that take
advantage of the special properties [6, 7] of the data structure
(as explained in Section 5), our approach is generally applica-
ble, e.g., it is even applicable to irregular data structures that
contain cycles or collections. Our optimization is built upon
theMultiple Granularity Lock (MGL), of which the high-level
idea is borrowed from the database community [8].
Technique Summary The input to our technique is
the concurrent library that encapsulates the data structure
with the atomic methods (APIs), implemented using the
coarse-grained locks; the output is the optimized library
that uses the ne-grained locks (i.e., MGL locks). We rst
reason about the data structure with the abstract object
graph (AOG), built by leveraging the pointer analysis. Given
each atomic method, we collect the object elds of the data
structure that it accesses using the side eect analysis. Our
technique then tags each eld in the AOG with the MGL
locks according to the access type of the eld.
Under the hood, the MGL supports multiple kinds of locks,
which correspond to dierent levels of restriction. Our opti-
mization chooses the least restrictive lock whenever possible.
Specically, if a eld is accessed, each of its ancestor elds is
protected by the intention lock in the MGL, which prevents
the concurrent update of the ancestor eld that leads to the
atomicity violation. Meanwhile, the intention lock is least re-
strictive and can be granted to multiple threads concurrently,
which allows the concurrent updates of the object graph in
Figure 3a.
Challenges There are many challenges in generating the
good placement of MGL. First, the static analyses cannot
distinguish dierent objects created at the same allocation
site due to the conservativeness nature [31] and, consequently,
produce a single lock to protect all of them. Existing static
lock inference approaches [2] [21] suer from this problem.
We observe that the library developers usually treat the root
of the object graph of data structure as its \owner" and apply
the lock onto the owner object to enforce atomicity. Dierentowner objects distinguish dierent data structure instances.
Our optimization starts with such owner locks, which encode
the developers' knowledge, and ne tunes them by pinpoint-
ing the parts accessed by each atomic method using the static
analysis. Second, the MGL from the database community
does not support cycles, while the data structures in general
programs often contain cycles. Third, naively applying our
technique to the collection would produce the conservative
locking that denies the concurrent accesses to it because
the static analyses cannot distinguish dierent entries of the
collection. Instead, we propose an ecient MGL locking
design for collections, that leverages the commutativity prop-
erty [13]. Finally, we apply a set of reduction rules when the
approach introduces too many locks.
We implement our approach and evaluate it against ve
widely used concurrent applications, which are built upon
the cache, the database, and the graph libraries. The re-
sults show that our optimization produces the libraries that
are more scalable, more concurrent, and more performant.
Specically, we observe the consistent speedup when the
number of threads increases. The optimized version outper-
forms the original version by at least 7%-20% and up to 2X,
which we believe is a signicant improvement. We make the
following contributions in this paper.
1.We propose an automated and systematic optimization
that unleashes more concurrency for irregular data
structures, by leveraging both the concept of Multiple
Granularity Locks (MGL) and the developers' knowledge
encoded in the original locking.
2.We design the MGL framework that is applicable to
general-purpose programs. The framework allows the
data structures to contain the cycles or use the collec-
tions. Besides, it provides the systematic techniques
for reducing the number of locks.
3.We implement our approach and conduct the in-depth
case studies on the widely used applications.
2. MOTIVATION EXAMPLE
We demonstrate how our optimization simultaneously
guarantees the atomicity in the case in Figure 2a and allows
the concurrent updates of disjoint elds in Figure 3a and
Figure 1. The data structure and the operations are adapted
from the benchmark STMBench7 , a well designed bench-
mark [11] for studying various synchronization mechanisms.
We rst briey introduce the Multiple Granularity Lock
(MGL), which is designed by Gray et al. [8] and originally
applied to the database systems.
MGL Primer Each eld f(i.e., the edge) in the object
graph of the data structure is associated with the locks in
dierent modes: Xlock, Slock, ISlock and IXlock. These
modes correspond to dierent levels of restriction, according
to the MGL design. For example, the Xlock exclusively
protects the subgraph rooted at the eld f(including f),
disallowing any concurrent accesses to the subgraph; the S
lock, similar to the Xlock, disallows any concurrent updates
of the subgraph; the ISlock and the IXlock, which are in
general called the intention lock , are designed to facilitate
the restriction of the XorSlock. Two locks are compatible
if they can be held by two threads concurrently. The locks
associated with dierent elds are by default compatible,
while the compatibility between any two locks associated481T
1
T
2
@atomic
@atomic
@
@
atomic
atomic
o
perationA
()
operationB
()
{
{
t1=
man.title
;
t1=
man.title
;
man
= new Manual();
t2=
man.title
;
assert(t1==t2);
}
}(a)
 (b)
 (c)
Figure 2. The update of a eld aects the accesses to the descendant elds. (a) The code (b) The object
graph (c) The tagged object graph
with the same eld is listed in Figure 4a. For instance, the
Xlock and the intention lock for the same eld cannot be
concurrently held.
Once a thread Tholds the Xlock for a eld f, it should
prevent the access to any eld f0in the subgraph (referenced
byf) by another thread T0. The synchronization is realized
in the cooperative way. The thread T0that intends to access
the descendant eld f0needs to acquire the intention lock
for each of the ancestor elds, e.g., the eld f. The lock will
not be granted to the thread T0because the thread Tholds
the incompatible Xlock for f. In short, the intention lock
for the ancestor eld indicates the intended access to some
descendant eld, which coordinates the threads accessing at
dierent levels.
The core of our optimization is to tag each eld in the
object graph with some MGL locks according to the type of
the eld access. As the runtime object graph is not available
during the static analysis, we approximate (Section 3.1) it
with the abstract object graph (AOG). If the access to a
eld is the read operation, we tag the eld with the ISlock.
If a eld is updated, we tag it with the Xlock. The X
lock associated with the updated eld fshould disallow
any concurrent access to the subgraph (referenced by f), as
the update attaches a new object subgraph to the eld f,
implicitly aecting all descendant elds. Besides, we tag each
eld on the reference path to a eld fwith the intention lock,
i.e., with the IXlock, if the eld fis tagged with the Xlock,
or with ISlock, if the eld fis tagged with the ISlock. We
defer the explanation of Slock to Section 3.2.2. The locks in
theTagged AOG (TAOG ) guarantee the atomicity and allow
a high degree of concurrency.
Preventing the Atomicity Violation Given the exam-
ple in Figure 2a, the update of the ancestor eld manis
protected by the Xlock for man, while the accesses to the
descendant eld title by another thread are protected by
both the ISlock for title and the ISlock for the ancestor
eld man(manis on the reference path from root to title ).
Specically, the former ISlock is used to protect the eld
read, and the latter ISlock is used as the intention lock. The
ancestor update and the descendant accesses are not allowed
to run concurrently due to the incompatibility between the
Xlock and the intention lock for the eld man. Hence, the
atomicity violation is avoided.
Allowing the Concurrent Update The locks in the
TAOG allow the concurrent updates of the elds title and
text in Figure 3a. Both updates are protected by the IX
lock for the common ancestor eld man, as well as by the X
lock for each updated eld. The updates can be executed
(a)
 (b)
Figure 3. The accesses to the elds which share the
common eld on the reference paths. (a) The object
graph (b) The tagged object graph
concurrently because the IXlock can be held by two threads
concurrently. Besides, our optimization allows the concurrent
updates of the elds title and subAssemblies in Figure 1.
Comparison with the Fine-Grained Locking The de-
velopers of the benchmark STMBench7 also build the ne-
grained locking by leveraging their domain expertise to allow
more concurrency. The ne-grained locking assigns a (readers-
writer) lock to protect all the ComplexAssembly objects and
a dierent lock to protect all the Manual objects, therefore,
it allows the concurrent updates of the elds title and id,
which are protected by dierent locks. However, as compared
to our approach, their ne-grained locking allows much less
concurrency in the presence of the structural modication,
i.e., when the non-primitive elds (e.g., manorsubAssem-
blies ) that form the structure are updated. Specically, to
guarantee the atomicity, the ne-grained locking introduces
a global readers-writer lock to protect the structure, and re-
quires the structural modication to be protected by the lock
in the writer mode and other operations in the reader mode.
Therefore, the locking disallows any concurrent operations
if a thread conducts the structural modication. For exam-
ple, it disallows the concurrent updates of the non-primitive
elds manand subAssemblies , even if they refer to disjoint
parts of the structure. Comparatively, our approach lever-
ages the static analysis to nd that the concurrent updates
involve disjoint parts, and produces the locking that allows
the concurrent updates.
In the rest of the paper, Section 3 presents our technique
in details. Section 4, and Section 5 present the evaluation
and related work respectively.
3. OPTIMIZATION OF LOCKS
At the high level, our optimization rst inspects the side
eects of the eld accesses in the atomic method and maps482them to the object graph of the data structure. Then, de-
pending on what parts of the data structure are accessed, it
decides what locks to use. Specically, our optimization stati-
cally constructs (Section 3.1) the abstract object graph (AOG)
to approximate the runtime object graph. We then map the
side eects to it and and apply the MGL tags (Section 3.2).
Given the Tagged AOG (TAOG ), we produce (Section 3.3)
the optimized locking.
3.1 Construction of Abstract Object Graph
Abstract object graph (AOG) is statically constructed to
approximate the runtime object graph. An object graph is a
graph with a single root, where each vertex represents the
object or the primitive value and each edge, labeled with
the eld name, represents the eld reference between the
vertices. The root object that we are interested in is the
object referenced by the thisvariable of the library class that
encapsulates the data structure. The object graph, as shown
in Figure 1, represents the layout of the root object in the
heap, that complies to the type denition.
In the rest of the paper, we do not distinguish the vertex
and the object that it represents and, similarly, the edge and
the eld. In addition, we refer to the path from the root to
an edge, that consists of the edges, as the reference path of
the edge. Multiple reference paths for an edge may exist,
indicating the object graph is not a tree.
Denition 1 (Abstract Object Graph) .The abstract object
graph (AOG) is a graph with a single root, where each edge,
similar to the edge in the object graph, represents the eld
reference between two vertices, and each vertex is a points-to
set, computed by the pointer analysis to represent the possible
objects referenced by the eld.
The points-to set is a set of allocation nodes (anodes ), each
of which represents a unique allocation site where the runtime
objects are created. We construct the AOG recursively using
the pointer analysis [31], by rst computing the root points-
to set S, i.e., pointto (this), where this isthisvariable of
the library class, and then computing each of its children
points-to sets, i.e., pointto (S; f), where the eld fis a eld
possibly referenced by anodes in the set S. The constructed
AOG is a static summary of the runtime object graphs
during the program execution. The main dierence with
the object graph is that the objects created at the same site,
represented by dierent vertices in the runtime object graph,
are represented by the same vertex in the AOG. They are
represented as an anode in a points-to set.
3.2 Tagging the Abstract Object Graph
Tagging the AOG is the core of our approach. Given an
atomic method, we tag each eld (edge) in the AOG with
dierent MGL locks based on how the eld is accessed in
the atomic method. The MGL locks need to coordinate the
ancestor update and the descendant accesses to avoid the
atomicity violation (Figure 2a). Meanwhile, the MGL locks
need to allow the concurrent updates of the disjoint parts
(Figure 1 and Figure 3a), even if they share the common
ancestor eld. In the following, we present the tagging rules,
Rule 1 and Rule 2, which we design to achieve the above
goals.
We rst compute how the elds are accessed in the atomic
method using the standard side eect analysis, which is well
implemented in open source compiler frameworks such as
IS
IX
X
S
IS
ÔÉº
ÔÉº
ÔÉª
ÔÉº
IX
ÔÉº
ÔÉº
ÔÉª
ÔÉª
X
ÔÉª
ÔÉª
ÔÉª
ÔÉª
S
ÔÉº
ÔÉª
ÔÉª
ÔÉº(a)
IS
IX
X
S
IS
0
0
1
0
IX
0
0
1
1
X
1
1
1
1
S
0
1
1
0 (b)
Figure 4. (a) Compatibility matrix and (b) its nu-
meric form
Soot1. Given a statement o.f=i , the analysis records that
the eld fof the objects (or anodes, we may use the two
terms interchangeably) in pointto (o) are accessed2and the
access type is update . We are only interested in the shared
objects in pointto (o), identied by the thread-escape analysis
inSoot. Note that the side eect analysis does not record
the elds accessed within the collection invocations, which
we handle separately (Section 3.4).
Rule 1. If a eld is updated, the edge that represents it
in the AOG is tagged with the Xlock, and each edge on the
reference paths is tagged with the IXlock. If a eld is read,
the corresponding edge is tagged with the ISlock, and each
edge on the reference paths is tagged with the ISlock.
The edge that represents a eld o:fin the AOG is an edge
that satises the following conditions: (1) it is labeled with
the eld name f, and (2) the source of it is a points-to set
which contains the object o.
Besides, the compatibility between the locks, as explained
in Section 2, is specied in Figure 4a and implemented by
the locking primitives (Section 3.5).
Following Rule 1, the intention lock, i.e., the IXor the IS
lock, placed on the ancestor eld o:fon the reference paths,
indicates that the thread intends to access a descendant
eldod:fd. As explained in Section 3.3, to access the eld
od:fd, a thread needs to acquire both the associated lock and
the intention lock for the ancestor eld o:f. Therefore, the
thread that updates the ancestor eld o:fneeds to acquire
theXlock for the eld o:f, and the thread that accesses the
descendant eld od:fdneeds to acquire the intention lock for
o:f. The two threads are coordinated because they acquire
the incompatible locks, i.e., the Xlock and the intention
lock for o:f, which cannot be granted concurrently. With
the intention lock, we avoid the large amount of runtime
searching and analysis [8] needed to decide whether the
thread is eligible for holding the lock.
Let us apply the tagging rule to the example in Figure 2a
to see how it prevents the atomicity violation. In the atomic
method operationB , the eld manis updated. Therefore,
we tag the eld in the AOG with the Xlock, as shown in
Figure 2c. In the atomic method operationA , the eld title
is read. Therefore, we tag the eld with the ISlock and the
ancestor eld manwith the intention lock IS, as also shown
1Soot: http://www.sable.mcgill.ca/soot/
2There is a special eld ArrayElement for denoting all ele-
ments in the array and there is a special anode for denoting
each class.483in Figure 2c. Note that, in practice, we tag the AOG for
dierent methods independently. Here, due to the space
limit, we put all tags for the methods in one TAOG. As a
result, the threads T1andT2are synchronized as T1acquires
the intention lock ISfor the eld man, in addition to the IS
lock for the eld title , and T2acquires the Xlock for the
same eld man, which are incompatible with each other.
For the example in Figure 3a, the thread that updates
the eld title acquires its Xlock and the intention lock
for the ancestor eld man, the other thread that updates
the eld text acquires its Xlock and the intention lock
for the ancestor eld man. Therefore, both threads can run
concurrently as the locks that they acquire are compatible
with each other.
In addition, we reduce the number of tags, i.e., the number
of locks, associated with each eld. The underlying intuition
is that, if a thread acquires a lock for the eld, e.g., the X
lock, it does not need to acquire the less restrictive lock for
the eld, e.g., the ISlock, as the former lock is more general
in the thread coordination. Intuitively, the less restrictive
lock is incompatible with fewer locks. Formally, we say the
lockl1is less restrictive than the lock l2(denoted as l1l2),
if and only if, M(l1)M(l2). Here, Mis the incompatibility
matrix in Figure 4b, where \1" denotes the incompatibility
and \0" denotes the compatibility, and M(l1)is a row vector.
Specically, we have ISIXXandISSX,
where SandIXare incomparable (The usage of the Slock
is explained later in Section 3.2.2). Rule 2 formalizes the
reduction based on the relation.
Rule 2. Given an edge tagged with the a set of locks in the
TAOG, the lock lcan be removed if it is less restrictive than
some other lock in the set, without aecting the concurrency
allowed by the set of locks.
3.2.1 AOGs that Contain Cycles
The AOGs constructed for real world programs are likely
to be complex, e.g., they contain the cycles, in which case
we cannot apply Rule 1 since innite number of reference
paths may exist.
Innite Number of Reference Paths Consider the
AOG in Figure 5, which contains the cycle, a!b!root,
there are innite number of reference paths for the edge b,
in which the cycle appears dierent times. According to the
reduction in Rule 2, tagging an edge with multiple locks in
the same mode has the same eect as tagging it with only
one lock in that mode. Therefore, we do not consider the
reference paths where the cycle appears more than once.
Superuous Reference Paths In Figure 5, a reference
path of the edge bis,c!d!root!a. Following Rule 1,
if the eld bis updated, each ancestor edge (e.g., the edge d)
on the reference path needs to be tagged with the IXlock,
in order to prevent the concurrent update of the ancestor
edge. However, in this example, the update of the eld d
does not aect the eld b, because the path from dtob
includes the root object, e.g., the thisobject of the library
class, which remains unchanged throughout the execution
of library methods. As the root object remains unchanged,
the elds on the left cycle are not aected by the update
of the eld d. In other words, the eects of the update of
a eld cannot propagate beyond the root object, as long as
the root is unchanged. Therefore, when applying Rule 1,
we do not consider the superuous reference path, i.e., the
reference path that includes the thisobject of the library
Figure 5. AOG that contains the cycles
class. Back to our example, when the eld bis updated, we
only need to tag the elds on the reference path aonly, we do
not need to tag the elds on the superuous reference path
c!d!root!aora!b!root!a, both including the
object this.
3.2.2 Reduction of the Number of Tags
In addition to the reduction rule (Rule 2), we apply two
reduction techniques to further reduce the number of tags,
in order to reduce the number of locks to be acquired.
Cost Benet Model for Reduction A basic reduction
step is, given an edge e, we remove all the tags in the sub-
graph referenced by eand tag the edge ewith the Slock or
theXlock. The Slock, similar to the Xlock, protects all
edges in the subgraph in the shared mode and disallows any
concurrent update of them. Its compatibility with other lock
modes is shown in Figure 4a. We tag the edge ewith the
Slock only if all the tags in the subgraph are ISorSlocks,
otherwise, we tag it with the Xlock.
However, we need to decide which subgraph we apply the
basic reduction step to. We propose the cost benet model
to decide that. The cost is computed as the number of tags
in the subgraph, i.e., the number of locks to be acquired; the
benet is computed as the number of elds in the subgraph
that are not tagged and not reachable from any eld with
theX/Slock tag. As these elds are not accessed and not
protected, some other threads that access them are allowed to
run concurrently. After the reduction, these elds would be
protected, which disables the concurrency. We then compute
the benet/cost ratio ( utility ) for the subgraph referenced by
each edge. We repeatedly nd the subgraph with the lowest
utility and apply the above basic reduction step, until the
total number of tags is less than a threshold N(We set N
as 10 in our evaluation) or no subgraph can be chosen for
the reduction, e.g., all tagged elds are directly referenced
by the root object.
Representative Lock If multiple locks are used together
or abandoned together in each atomic method, we select one
representative from them and replace the use of other locks to
the use of the representative lock. This reduces the number
of locks to be used. The mode of the representative lock is
the strictest mode among the multiple locks. This reduction
method applies well in the presence of a cycle, as all the
edges from the cycle often appear together on the reference
path and the locks protecting them are used together.
3.3 Producing the Lock Operations
After we construct the TAOG for an atomic method, we
remove the original synchronization from the source code
and insert the lock acquisitions (or releases) that acquire484(or release) the locks according to the tags in the TAOG.
We follow the non-strict two phase locking [5] which allows
the lazy acquisition and early release, i.e., the acquisition
of a lock is placed just before the shared access that the
lock protects, and the release is placed immediately after the
shared access. Specically, our static analysis keeps track of
the accesses that each lock protects. The analysis rst nds
the minimal code block that is not in any loop body to enclose
the accesses3. It then places the acquisition at the beginning
of the block and the release at the end. The code block
should not be in any loop body, otherwise, we may break
the atomicity across iterations. The lock regions computed
in a method may violate the two phase locking, e.g., one
lock region is placed after the other so that the acquisition
may occur after the release. We resolve the problem by
merging4the lock regions as a new lock region and moving
the acquisitions (or releases) accordingly.
After computing the lock regions, we synthesize the lock
operations. The library developers often refer to the root of
the irregular data structure as \ this" and use it to protect
the data structure. The underlying assumption or rationale
is that all accessed elds are descendants of the root of
runtime data structure instance. The assumption allows us
to distinguish dierent data structure instances created at the
same site. Specically, we maintain a unique AOG in each
runtime data structure, which coordinates only the accesses
to that runtime data structure instance. The lock operation
is in the form of g:lock (e):X():acquire (), where lock(e):X()
returns the Xlock associated with the edge ein the AOG g
andeis an edge tagged in the TAOG, which is constructed
during the static analysis. More implementation details
are presented in Section 3.5. Our generated locking can be
viewed as the ne tuned version of the original lock, which
relies on the TAOG to pinpoint the parts to be protected.
For the cases the assumption does not hold, e.g., the library
developers use a static lock (the lock referenced by a static
eld) to protect the entire data structure, our above approach
does not apply. However, we do not nd such cases in our
evaluation.
Deadlock-freeness The acquisitions may lead to dead-
locks when the locks are acquired in the reversed order or
cyclic order. To avoid the deadlocks, we make sure the
locks are acquired following the xed orders. First, the locks
for the same runtime data structure are acquired following
the topological order among the edges, which is determined
statically and embeded in the code. Second, the locks for
dierent runtime data structure instances or the locks for
dierent keys (Section 3.4) need to be sorted at runtime
before they are acquired. In practice, each library method
typically involves only one runtime data structure instance
and one key during each execution, for which the sorting
overhead vanishes.
3.4 Collection
The collections, such as Map and Set, are heavily used in
real world applications and impose special challenges to our
optimization. Consider a Map, our static analysis cannot
distinguish its dierent entries and, therefore, produce a
single lock to protect the atomic methods such as putand
3Finding the minimal block is a standard compiler analysis,
of which the details can be found in our technical report:
http://www.cse.ust.hk/prism/MGL/TR.pdf.
4The details are also in our technical report.get. Therefore, given two atomic methods that contain
the invocations upon the same collection, they cannot be
executed concurrently.
Herlihy et al. [13] exploit the linearizability [1] and commu-
tativity [13] properties of the collection to draw the conclusion
in Lemma 1. They prove Lemma 1 and design the software
transactional memory mechanism to increase the concurrency
degree for collections. Instead, we design the MGL locking
for collections to achieve the same goal. In the following, we
rst briey explain the linearizability and the commutativity.
Lemma 1. If the collection APIs are linearizable and in-
voked in two atomic methods, the two atomic methods can be
executed concurrently without breaking the atomicity if and
only if the invocations in one atomic method are commutative
with those in the other.
Linearizability The linearizability species the atomicity
in terms of the return values. Specically, the APIs from
a linearizable collection implementation, if invoked concur-
rently, return the values equivalent to those returned in some
serial execution of the invocations. The linearizable collec-
tion is often implemented in the non-blocking way [26] and
allows high concurrency. In the following, we assume the
collection is linearizable by default, unless otherwise speci-
ed. In our evaluation, we replace5the collection with the
linearizable version from the java.util.concurrent package in
JDK by default.
Commutativity Two invocations, e.g., the invocations
ofmap.put(5,5) andmap.put(6,6) , are commutative [13] if
applying them in either order returns the same response and
leaves the collection in the same abstract state , e.g., the state
in which the map contains both 5 and 6. Dierent from the
concrete heap state, the abstract state, specied by develop-
ers, captures only the information that the developers are
interested in. Herlihy et al. [13] have specied the abstract
state for the collections, such as Map, Set, Queue and others,
that have the standard semantics. They have also summa-
rized the commutativity among the invocations. In our work,
we only optimize the uses of Map and Set, which are heavily
used in real world code. Specically, the invocations upon a
collection are commutative if they operate on dierent keys,
e.g., the invocations of put(5,5) andput(6,6) ; they are not
commutative if they operate on the same key or either of
them does not require a key, e.g., the invocation of keySet() .
Lemma 1 states that the interleavings by the commutative
invocations do not break the atomicity as the interleaved
execution can be transformed to some sequential execution
through the equivalence-preserving commutation. Therefore,
to ensure the atomicity of an atomic method, we only need
to prevent the interleaving by non-commutative invocations.
One natural way is to apply the key-based locking. That
is, one unique lock protects each key. In this way, the non-
commutative invocations that share the common keys are
synchronized as they are protected by the same lock for
the same key. However, the key-based locking requires the
key or its equivalent6to be available at the beginning of
the lock region (We compute the lock region following the
lock placement in Section 3.3.), where the lock acquisition
happens. However, the key may not be available at the
beginning of the lock region, or, the invocation simply does
5The replacement can be realized automatically [4].
6The local must-alias analysis in Soot computes the variables
that are the must-alias of the key.485(a)
 (b)
Figure 6. (a) TAOG for the collection (b) A new
level of edges for representing the partitions
not require any key. In either case, the key-based locking is
not applicable. The coarse locking should be applied instead
to prevent any concurrent invocations upon the collection.
It is important to coordinate the threads that use the ne
key-based locking and the threads that use the coarse locking.
We design a variant of MGL for the collection to achieve the
goal. Figure 6a shows the TAOG for a collection, where the
second level edge represents each key and the rst level edge
allKeys represents the whole collection (or all the keys). If
a thread resolves, at the beginning of the lock region, the
key to be involved in the invocation, it acquires the Xlock
for the key and the IXlock for the edge allKeys . Otherwise,
the thread simply locks the whole collection by acquiring
theXlock for the edge allKeys . In this way, the invocations
with common keys are coordinated by the Xlocks for the
common keys, while the invocations with dierent keys can
run concurrently. Besides, the thread that carries out the
invocations without a key (or the invocations of which the
keys cannot be resolved at the lock acquisition) prevents any
concurrent invocations by acquiring the Xlock for the edge
allKeys and locking the whole collection.
Partition As expected, the above MGL protocol would
involve many locks at runtime, which may cause the memory
problems in practice. We reduce the number of locks needed
by rst mapping the keys to Npartitions7and then replacing
the locks for keys to the locks for partitions. In particular,
we insert an additional level of edges, each representing a
partition, as shown in Figure 6b. At runtime, instead of
acquiring the lock for a key, the thread acquires the lock for
the partition that contains the key, which is determined based
on the hashing function hashcode (key)%N. In addition, the
thread also needs to acquire the lock for the edge allKeys , in
the same way as discussed above. As a result, we reduce the
total number of locks to a small number. Note that we reduce
the concurrency at the same time, e.g., the invocations upon
two dierent keys may be unnecessarily synchronized because
the keys are mapped to the same partition. In practice, we
do not maintain the level of keys (dotted edges in Figure 6b)
in the TAOG for the collection as we can nd the partition
that contains the key through the calculation of the hashing
value.
7Similar to the implementation of the class Concurren-
tHashMap in JDK, we use 16 partitions by default.3.5 Implementation
Our implementation is built upon the static analysis, e.g.,
theSpark pointer analysis and the side eect analysis, pro-
vided by the Soot compiler framework. Our approach does
not support the atomicity among the atomic set [29] cur-
rently. However, it is not hard to extend our approach to
support the atomic set: we just need to treat the elds in
the same atomic set as one eld, and protect them with the
same lock.
The implementation of the MGL Locking primitives, e.g.,
lock.X().acquire() orlock.IX().acquire() , is similar to
the implementation of the readers-writer primitives in the
java.util.concurrent package, which is built with the extensible
Synchronizer framework [18] in the same package. The main
dierence is that the MGL supports four modes while the
readers-writer locking supports only two modes. We build
the MGL locking on the same framework by supporting four
modes. Besides, the compatibility among the four modes is
hardcoded as the code logic.
4. EVALUATION
In our evaluation, we focus on the following aspects: (1)
The metrics of the static analysis. (2) The performance
comparison between the optimized version and the origi-
nal version. Our evaluation uses ve applications: Cache4j ,
Jgrapht ,Tomcat ,Tuplesoup and STMBench7 , of which the
rst four are widely used in practice, while STMBench7 is de-
signed [11] for evaluating various synchronization approaches.
All studies are performed on a x86 64 Dell workstation with
8 cores. The server has 16GB of RAM and runs on a Linux
2.6.22 kernel.
4.1 Cache4j
Cache4j implements the memory-based caching capabil-
ity for Java objects. The SynchronizedCacheTest class in
the benchmark measures the performance of the concurrent
cache library, i.e., the SynchronizedCache class. The per-
formance test code starts multiple threads, each of which
invokes the putand getmethods from the Synchronized-
Cache class. The methods are originally atomic as their
bodies are wrapped by the synchronized block, synchro-
nized(this){} . Specically, each thread executes 1,000,000
invocations, of which 2/3 invoke the getmethod and the
rest invoke the putmethod.
Our analysis takes 35267 milliseconds. The AOG, con-
structed to represent the data structure, is acyclic and con-
tains 47 nodes and 51 edges in total. The AOG is tagged
dierently in the methods. Figure 7a and Figure 7b show
the Tagged AOGs (TAOGs) for the methods getand put,
respectively. Due to the limit of space, we omit the edges
involving collections and those never tagged. Some eld
names are also abbreviated.
We observe that the Xlock for the eld memorySize in
Figure 7a is incompatible with the Xlock for the same eld
in Figure 7b, while the locks for each of the other elds in
both TAOGs are mutually compatible and can be held by
two threads concurrently, e.g., the IXlock associated with the
eld cacheInfo in Figure 7a is compatible with the IXlock
for the same eld in Figure 7b. As for the eld memorySize ,
it is updated by a single statement in both methods. Our lock
placement (Section 3.3) places the lock operations closely
around the statement, leading to a very small lock region.
Given the threads that invoke the methods getand put486X
X
X
X
IX
IS
IS
IS
(a)
X
X
IX
IS
IS
IS
IS
(b)
Figure 7. (a) Tagged AOG produced for the method
get, the number in each node denotes the hashcode
of the points-to set represented by the node. (b)
Tagged AOG for the method put.
concurrently, they are coordinated by the Xlock for the eld
memorySize only when both of them want to enter the small
lock regions.
The reduction using the cost-benet model (Section 3.2.2)
does not apply because the number of tags is less than the
threshold (10), while the reduction using the representative
lock (Section 3.2.2) applies. Given the elds miss,remove
andhit, which are all used in the method getand all unused
in the method put, we select the lock for the eld miss as the
representative and remove the locks for the other two elds
by replacing the use of them to the use of the representative
lock.
Besides, the root object has two elds of the Map type,
_map and _tmap . Following the optimization in Section 3.4,
we replace the map with the linearizable implementation
in JDK, e.g., the class ConcurrentHashmap , and synchronize
the operations on the keys whenever possible. Specically, we
synchronize the operations on _map using the key-based lock
because the key is resolved as a method parameter. However,
we have to synchronize the operations on _tmap using the
coarse lock that protects the whole collection because the key
cannot be resolved at the lock acquisition as it is computed
after the lock acquisition.
Figure 8 shows the performance comparison between our
optimized version optand the original version orig. Our
optimized version scales well when the number of threads
increases, e.g., the optimized version outperforms the original
version by 30%-60% when the number of threads is greater
than 4. However, when the number of threads is smaller
than 4, e.g., when only one thread is used, the optimized
version (94 msecs) is 20% slower than the original version (77
msecs). The result is consistent with our expectation: The
original version performs well when the number of threads
is small because it incurs fewer locking operations and the
optimized version performs well when the number of threads
is large because it allows greater concurrency. We believe
2000
3000
4000
5000
6000
orig
opt
0
1000
2000
1
2
4
8
12
16Figure 8. Performance comparison for Cache4j . The
X axis denotes the number of threads and the Y axis
denotes the running time (msec).
allowing greater concurrency is of critical importance as
multiple (or even many) threads are often used in the real
world applications to exploit the power of the multi-core
machine.
4.2 Jgrapht
The benchmark Jgrapht is a commonly used graph li-
brary. The class PerformanceDemo , shipped with the library,
measures the performance of the library. Specically, the per-
formance test code runs several threads, each manipulating
the graph gby invoking the methods addVertex oraddEdge .
To achieve the atomicity of each method, the developers wrap
the method body with the synchronized block that uses the
graph object gas the lock. As a result, all the invocations
are executed sequentially.
The Jgrapht library heavily uses the Map collection, e.g.,
it stores the vertices in a map and the edges in the other
map. Our optimization replaces the maps with the Concur-
rentHashMap implementation and successfully synchronizes
each of the methods on the key passed as the method param-
eter. With our optimization, the invocations of the methods,
addVertex oraddEdge , can be executed concurrently if they
involve dierent keys, i.e., the dierent vertices or dierent
edges.
Figure 9a shows the performance comparison between the
optimized version optand the original version orig. The
optimized version nishes faster when the number of threads
increases, because the total workload (i.e., adding 10,000 ver-
tices and 200 edges for each vertex) is shared among threads
and these threads can run concurrently. Comparatively, the
original version runs the invocations sequentially even in the
presence of multiple threads and the synchronization incurs
extra overhead, which becomes very high when the contention
becomes erce as the interaction with OS is involved [27],
therefore, it requires even longer execution time when the
number of threads increases. Our optimized version is 2X
faster as compared to the original version when there are
more than 8 threads. We also notice that our optimized ver-
sion does not perform well in the single-threaded setting, e.g.,
it is 70% slower than the original version. The possible expla-
nation is that, the original map implementation introduces
only one instruction (compare-and-set instruction) while the
ConcurrentHashMap implementation adopts the non-trivial
synchronization which introduces more instructions.
4.3 STMBench7
STMBench7 is an application specially designed to measure
the eects of dierent synchronizations on the performance.487600
800
1000
1200
1400
1600
1800
2000
orig
opt
0
200
400
600
1
2
4
8
12
16(a)
200
250
300
350
Throughput: transactions per second
coarse
manual
opt
100
150
1
2
4
8
10
12
14
16
Throughput: transactions per second (b)
400
600
800
1000
1200
orig
opt
0
200
400
1
2
4
8
12
16 (c)
10000
15000
20000
25000
30000
35000
orig
opt
0
5000
10000
1
2
4
8
12
16 (d)
Figure 9. Performance comparison for (a) Jgrapht (b)STMBench7 (c)Tomcat (d)Tuplesoup . The X axis represents
the number of threads and the Y axis represents the running time (msec). Specially, the Y axis for STMBench7
represents the throughput (transactions per second).
The core data structure is encapsulated in the library class
Module , which references many other objects transitively.
STMBench7 supports various kinds of atomic transactions,
which include the traversal of the core data structure, the
local update of the primitive elds in the data structure, the
structural modication that updates the non-primitive elds
that form the structure. The xed number of transactions
are completed by multiple threads together.
Our analysis run for 46325 milliseconds, the constructed
AOG contains 62 nodes and 71 edges, where two cycles are
present. We cannot show the TAOG for each transaction
as there are 45 transactions in the benchmark in total. Fig-
ure 2c shows a part of the TAOG. As shown in Figure 9b, we
compare our optimized locking optwith the coarse-grained
locking coarse provided in the benchmark, which synchro-
nizes all transactions using a global lock, and the ne-grained
locking manual , which is manually crafted by the expert.
Note that the Y axis represents the throughput, i.e., the num-
ber of transactions completed per second. Higher throughput
corresponds to higher performance.
Both the optversion and the manual version scale well
when the number of threads increases, while the coarse
version does not scale well, e.g., it even slows down when
the number of threads increases. Specically, when there
are more than 8 threads, the optandmanual versions are
10%-40% faster than the coarse version. Besides, when there
are less than 4 threads, the optandmanual versions lead to
slightly lower throughput, because the ne-grained locking
typically incurs more locking operations.
Surprisingly, our automatically optimized locking (the opt
version) even outperforms the manual ne-grained locking
(themanual version). The optversion is 7%-16% faster when
there are more than 8 threads. After investigation, we nd
that the performance advantage manifests itself in the pres-
ence of structural modication transaction, which changes
the non-primitive elds that dene the structure. If we set
the ratio of structural modication transactions in the total
workload as 1%, the two versions produce similar through-
puts. However, if we set the ratio to be 20%, as used in our
experiment, our optimized locking outperforms the original
version by 7%-16%. As also explained in Section 2, our opti-
mized locking allows the structural modication transaction
to be executed concurrently with other transactions that
involve disjoint parts. Comparatively, the manual locking
disallows the concurrency between a structural modication
transaction and any other transactions.4.4 Tomcat
The class that we optimize in the benchmark Tomcat
is the library class ApplicationContext , of which the eld
attributes is of the type ConcurrentHashMap . The perfor-
mance test driver rst stores many key-value pairs into the
map, then starts multiple threads, each repeatedly invoking
the method removeAttribute from the class Application-
Context to remove the entry for the key that is randomly
selected.
Listing 1. Code snippet from Tomcat application
1removeAttribute (key){
synchronized ( attributes ){
3 found = attributes . containsKey (key);
if( found )
5 {
value = attributes .get(key);
7 attributes . remove (key);
}
9 }
}
Listing 1 shows the details about the method. Note that
the synchronization is newly added in Tomcat 7.0.30 version
after the atomicity violation bug (Bug 53498) is reported
to the developers [19]. Our optimization safely replaces the
synchronization on the map to the synchronization on each
key, which leads to the 60% performance speedup in the
multi-threaded settings, as shown in Figure 9c.
4.5 Tuplesoup
Tuplesoup implements the easily embedable database
(in-memory database), which also provides the class Par-
allelPerformanceTest as the performance test driver. In
the driver, multiple threads invoke the methods (Listing 2)
scanIndex andaddEntry from the library class MemoryIndex
to scan the index or to add an entry to the index. Each
method is atomic as it is a synchronized method. An im-
portant eld in the class MemoryIndex is the eld cache of
theHashMap type (The eld is changed to a eld of the
ConcurrentHashMap type by our optimization), which stores
the mapping between the elements and their entries in the
database. We apply our optimization to the class MemoryIn-
dex. Specically, the analysis nish within 23787 msecs.
The AOG constructed has 24 vertices and 23 edges, with no
cycles.488One interesting observation is related to the eld cache .
For the method scanIndex , our optimized locking adopts the
Xlock for the whole map because the analysis cannot gure
out the key to be used by the invocations at the acquisition
point, i.e., the point just before the loop at line 3 (Listing 2).
However, for the method addEntry , our optimized locking
adopts the Xlock for the key (more precisely, for the partition
that contains the key) which is resolved as dbEntry.getID()
at the acquisition point, i.e., the point just before line 12.
Therefore, the invocations of the method addEntry can be
executed concurrently once they operate on dierent keys
(dierent partitions). Interestingly, our optimized locking
also requires the thread that executes the method addEntry
to acquire the IXlock for the whole collection. The IXlock
is important as it prevents the concurrent invocations of the
addEntry method and the scanIndex method. Without the
IXlock, the two threads, synchronized on dierent Xlocks
only, would invoke the methods concurrently, which may lead
to the missing update.
Listing 2. Code snippet from Tuplesoup application
synchronized scanIndex ()
2{
for(key: cache . keySet ())
4 {
dbEntry = cache .get(key);
6 list .add( dbEntry );
}
8}
10synchronized addEntry ()
{
12 cache .put( dbEntry . getID () , dbEntry );
}
The performance comparison is shown in Figure 9d. Our
optimized version outperforms the original version by 7%-
20% when there are multiple ( >=8) threads.
5. RELATED WORK
Automatic Lock Allocation The automatic lock allo-
cation [21, 12, 10, 2, 28] is targeted at freeing the developers
from the challenging and error-prone reasoning of locks by
producing them automatically. However, it often suers from
the conservativeness of the static analysis [31, 3, 28, 6]. For
example, it often produces a lock to protect multiple data
structures created at the same site at runtime because the
static analysis cannot distinguish them. In contrast, the de-
velopers adopt the lock that distinguishes dierent runtime
data structure instances. Our optimization starts with such
manually specied lock, which encodes the developers' knowl-
edge, and ne tunes the lock to lower down its restriction
level.
Fixing Concurrency Bugs Automatic xing of concur-
rency bugs [20, 30, 9, 16] introduces new locks to eliminate
the buggy interleavings, without changing existing locks in
the program. The bug xing work shares the same spirit
with our work, i.e., both want to inherit the developers' do-
main knowledge encoded in the original locks, rather than
allocating all the locks from scratch.
Fine-Grained Locking Several approaches [6, 7] are
recently proposed to implement the ne-grained locking,which are however not general. For example, the approach
of Golan-Gueta et al. [6] requires the shape of the data
structure to satisfy the special domination property. The
other approach of Golan-Gueta et al. [7] is applicable to the
collection only and is not applicable in the presence of the
accesses to common elds. Our approach does not suer
from the limitations and is generally applicable.
Parallelization of Irregular Applications The Galois
project [17, 22] considers the parallelization of irregular appli-
cations, but applies only to the applications that exhibit the
amorphous data-parallelism, e.g., the computation centers
around the distributed active elements and updates/reads
some local neighbours. Many applications do not exhibit the
property, e.g., in the benchmark STMBench7 , the structural
modication operation aects the whole subgraph and some
long traversal operations involve almost all nodes. Besides,
Galois and other work [25] rely on the special runtime sup-
port such as the scheduler or rollback [24, 14]. The parallel
graph traversal on GPUs has also been investigated [23, 15]
recently.
6. CONCLUSION
We present a general approach that unleashes more con-
currency for the irregular data structures through the use of
the Multiple Granularity Lock (MGL). Specically, the MGL
coordinates correctly the ancestor update and the descendant
accesses to the data structure, and it is lightweight in the
sense that it limits the concurrent accesses to the data struc-
ture as little as possible. Compared to the original locking,
our optimized locking consistently allows higher degree of
concurrency. The evaluation shows our optimization leads
to higher performance and better scalability.
7. ACKNOWLEDGMENTS
We thank the anonymous reviewers for their valuable com-
ments and suggestions. This research is supported by RGC
GRF grant RGC622909 and RGC621912 and also by the
HKUST RFID Center.
8. REFERENCES
[1] S. Burckhardt, C. Dern, M. Musuvathi, and R. Tan.
Line-up: a complete and automatic linearizability
checker. In PLDI , 2010.
[2] S. Cherem, T. M. Chilimbi, and S. Gulwani. Inferring
locks for atomic sections. In PLDI , 2008.
[3]J.-D. Choi, M. Gupta, M. Serrano, V. C. Sreedhar, and
S. Midki. Escape analysis for java. In OOPSLA , 1999.
[4] D. Dig, J. Marrero, and M. D. Ernst. Refactoring
sequential java code for concurrency via concurrent
libraries. In ICSE '09 .
[5] K. P. Eswaran, J. N. Gray, R. A. Lorie, and I. L.
Traiger. The notions of consistency and predicate locks
in a database system. CACM , 19(11), Nov. 1976.
[6] G. Golan-Gueta, N. Bronson, A. Aiken,
G. Ramalingam, M. Sagiv, and E. Yahav. Automatic
ne-grain locking using shape properties. In OOPSLA ,
2011.
[7] G. Golan-Gueta, G. Ramalingam, M. Sagiv, and
E. Yahav. Concurrent libraries with foresight. In PLDI ,
2013.
[8]J. N. Gray, R. A. Lorie, and G. R. Putzolu. Granularity
of locks in a shared data base. In VLDB , 1975.489[9]M. Grechanik, B. M. M. Hossain, U. Buy, and H. Wang.
Preventing database deadlocks in applications. In
ESEC/FSE , 2013.
[10] K. Gudka, T. Harris, and S. Eisenbach. Lock inference
in the presence of large libraries. In ECOOP , 2012.
[11]R. Guerraoui, M. Kapalka, and J. Vitek. Stmbench7: a
benchmark for software transactional memory. In
EuroSys , 2007.
[12] R. L. Halpert, C. J. F. Pickett, and C. Verbrugge.
Component-based lock allocation. In PACT , 2007.
[13]M. Herlihy and E. Koskinen. Transactional boosting: a
methodology for highly-concurrent transactional
objects. In PPoPP , 2008.
[14] M. Herlihy, V. Luchangco, and M. Moir. A exible
framework for implementing software transactional
memory. In OOPSLA , 2006.
[15] S. Hong, T. Oguntebi, and K. Olukotun. Ecient
parallel graph exploration for multi-core cpu and gpu.
InPACT , 2011.
[16] G. Jin, L. Song, W. Zhang, S. Lu, and B. Liblit.
Automated atomicity-violation xing. In PLDI , 2011.
[17] M. Kulkarni, M. Burtscher, R. Inkulu, and K. Pingali.
How much parallelism is there in irregular applications.
InPPoPP , 2009.
[18] D. Lea. The java.util.concurrent synchronizer
framework. Sci. Comput. Program. , 2005.
[19] P. Liu, J. Dolby, and C. Zhang. Finding incorrect
compositions of atomicity. In ESEC/FSE , 2013.
[20] P. Liu and C. Zhang. Axis: automatically xing
atomicity violations through solving control constraints.
InICSE , 2012.[21] B. McCloskey, F. Zhou, D. Gay, and E. Brewer.
Autolocker: synchronization inference for atomic
sections. In POPL , 2006.
[22] M. M endez-Lojo, D. Nguyen, D. Prountzos, X. Sui,
M. A. Hassaan, M. Kulkarni, M. Burtscher, and
K. Pingali. Structure-driven optimizations for
amorphous data-parallel programs. In PPoPP , 2010.
[23]D. Merrill, M. Garland, and A. Grimshaw. Scalable gpu
graph traversal. In PPoPP , 2012.
[24] V. Pankratius and A.-R. Adl-Tabatabai. A study of
transactional memory vs. locks in practice. In SPAA ,
2011.
[25] B. Ren, G. Agrawal, J. Larus, T. Mytkowicz,
T. Poutanen, and W. Schulte. Simd parallelization of
applications that traverse irregular data structures. In
CGO , 2013.
[26] M. Rinard. Parallel synchronization-free approximate
data structure construction. In HotPar , 2013.
[27] M. Thompson. Java lock implementations.
http://www.javacodegeeks.com/2012/07/
java-lock-implementations.html .
[28]G. Upadhyaya, S. P. Midki, and V. S. Pai. Using data
structure knowledge for ecient lock generation and
strong atomicity. In PPoPP , 2010.
[29] M. Vaziri, F. Tip, and J. Dolby. Associating
synchronization constraints with data in an
object-oriented language. In POPL , 2006.
[30] Y. Wang, S. Lafortune, T. Kelly, M. Kudlur, and
S. Mahlke. The theory of deadlock avoidance via
discrete control. In POPL , 2009.
[31] J. Whaley and M. S. Lam. Cloning-based
context-sensitive pointer alias analysis using binary
decision diagrams. In PLDI , 2004.490