TightÔ¨Åt: Adaptive Parallelization with Foresight
Omer Tripp
Tel-Aviv University
omertrip@post.tau.ac.ilNoam Rinetzkyy
Tel-Aviv University
maon@cs.tau.ac.il
ABSTRACT
Irregular applications often exhibit data-dependent paral-
lelism: Dierent inputs , and sometimes also dierent exe-
cution phases , enable dierent levels of parallelism. These
changes in available parallelism have motivated work on
adaptive concurrency control mechanisms. Existing adap-
tation techniques mostly learn about available parallelism
indirectly , through runtime monitors that detect pathologies
(e.g.excessive retries in speculation or high lock contention
in mutual exclusion).
We present a novel approach to adaptive parallelization,
whereby the eective level of parallelism is predicted directly
based on input features, rather than through circumstan-
tial indicators over the execution environment (such as retry
rate). This enables adaptation with foresight , based on the
input data and not the run prex. For this, the user spec-
ies input features, which our system then correlates with
the amount of available parallelism through oine learning.
The resulting prediction rule serves in deployment runs to
foresee the available parallelism for a given workload and
tune the parallelization system accordingly.
We have implemented our approach in Tightfit , a gen-
eral framework for input-centric oine adaptation. Our ex-
perimental evaluation of Tightfit over two adaptive run-
time systems and eight benchmarks provides positive evi-
dence regarding Tightfit 's ecacy and accuracy.
Categories and Subject Descriptors
D.1.3 [ Software Engineering ]: Concurrent Programming
General Terms
Algorithms, Performance, Experimentation, Measurement
Supported by the European Research Council under the Euro-
pean Union's Seventh Framework Programme (FP7/2007-2013)
/ ERC grant agreement no[321174-VSSC]
ySupported by the EU project ADVENT, grant number: 308830
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proÔ¨Åt or commercial advantage and that copies
bear this notice and the full citation on the Ô¨Årst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciÔ¨Åc
permission and/or a fee.
ESEC/FSE ‚Äô13, August 18√¢ ÀòA¬∏ S26, 2013, Saint Petersburg, Russia
Copyright 2013 ACM 978-1-4503-2237-9/13/08 ...$15.00.Keywords
oine learning, irregular applications, data-dependent par-
allelism, adaptive software parallelization, STM
1. INTRODUCTION
This paper addresses the problem of parallelizing irregular
applications, whose available parallelism is data dependent.
Contrary to regular applications ( e.g. scientic programs
that manipulate dense arrays), where dependencies between
computations are identical for all inputs, an irregular ap-
plication has a dierent dependence structure per dierent
data inputs, which leads to uctuating parallelism across
inputs, and sometimes also execution phases. As an illus-
trative example, minimum spanning tree (MST) algorithms
typically have more parallelism over dense graphs, where it
is less likely for tasks to work on common nodes or edges.
Irregular applications are widespread [12]. Common
examples include graph algorithms, such as Boruvka's
and Kruskal's MST algorithms and Dijkstra's single-source
shortest path algorithm; scientic applications like the
Barnes-Hut and Discrete Event Simulation algorithms;
machine-learning and data-mining algorithms, such as Ag-
glomerative Clustering and Survey Propagation; and appli-
cations in the area of computational geometry, including De-
launay's mesh-renement and triangulation algorithms.
Eective parallelization of irregular applications is chal-
lenging because the available parallelism is input sensitive.
Fixing a single synchronization scheme, treating all inputs
uniformly, might either (i) exploit the available parallelism
poorly for high-parallelism inputs if synchronization is con-
servative ( e.g. coarse-grained locking), or (ii) yield high
overheads for low-parallelism inputs if synchronization is
permissive ( e.g. many retries in speculative execution of
conicting transactions).
Adaptive concurrency control. A natural means of ad-
dressing data-dependent parallelism is adaptation, whereby
the parallelization system changes its behavior in response
to changes in available parallelism. Recent studies have pro-
posed a variety of techniques for identifying such changes,
and more specically, estimating available parallelism on
the y. Examples include monitoring abort/commit ratio
in software transactional memory (STM) [5, 34], or track-
ing access patterns to shared data structures at the early
stages of execution [7]. (See Section 7 for a comprehensive
discussion.)
These as well as other existing techniques estimate the
parallelism enabled by the input workload indirectly , based
on properties of the execution environment rather than prop-erties of the input. The motivation is clear: Deriving useful
input characteristics require semantics understanding of the
application at hand, whereas proling system behaviors (like
abort/commit ratio or lock contention) provides a general
and tractable basis for automatic adaptation. The disad-
vantage, however, is that adaptation is guided by hindsight
judgments over the behavior of the execution environment
during the run prex. This can potentially lead to poor per-
formance at the beginning of the run, as well as misguided
adaptation if parallelism changes across phases ( i.e.when
the future is not predictable from the past).
Ideally, the runtime system would make adaptation de-
cisions directly , with foresight : Given an input workload,
the system would know in advance, based on inspection of
the input itself, how much parallelism that workload per-
mits, and tune its behavior accordingly [14, 11]. Design-
ing foresight-guided adaptation mechanisms is challenging:
It requires uncovering the relationship between inputs and
available parallelism at runtime, and with low overhead [27].
Our approach. We present a novel approach for enabling
foresight-based adaptation. The main idea is to utilize of-
ine analysis|and more specically, a heavyweight learning
algorithm|to characterize the parallelism permitted by dif-
ferent inputs. The runtime system then makes use of the
artifacts from oine analysis to derive adaptation decisions
from the current state of the workload at hand.
What complicates learning is that the input data is of-
ten a complex data structure, like a graph. To address this
challenge, we ask the user to provide a feature extraction
function , mapping the concrete input to a set of features
that are amenable to learning ( e.g.the number of nodes in
the graph, the number of edges, the graph's density, etc).
We require this specication because feature extraction is
a challenging problem for an automated algorithm. It is
our impression and experience that providing the specica-
tion places low burden on a user intimate with the target
application, as the entailed reasoning is relatively simple.
Furthermore, the specication is concise and, most impor-
tantly, the oine learning system is robust to user errors.
(See Section 2 for further discussion.)
Based on the specication of input features, our approach
decomposes into the following three phases:
1.Per-system learning. For the given adaptive paral-
lelization system, converge (once per all applications)
on the most favorable execution mode of the system
per dierent proles of available parallelism.
2.Oine learning. For every application, build a pre-
diction function from input features to available paral-
lelism using oine learning over representative work-
loads.
3.Runtime parallelization. At runtime, the system
periodically calls the user-provided feature extraction
function to obtain the features of the current data set.
It then performs a lookup to nd which mode of the
parallelization system best matches these features, and
sets the system to that mode.
For the second phase of estimating available parallelism,
which is the main focus of this paper, we analyze data depen-
dencies. A data dependency arises between two statements
during (sequential) execution if both access a common mem-
ory location, and at least one of them writes it. Intuitively,
this is an indication that the two statements might interfere
with each other during concurrent execution.Traditionally, compile-time parallelization transforma-
tions have been governed by qualitative analysis of data de-
pendencies, requiring absence of data dependencies between
code blocks as the precondition for their parallel execution.
A fundamental observation of this paper is that dynamic
data dependencies expose ne-grained information about
the available parallelism. This includes not only the volume
of data dependencies between tasks that can potentially run
in parallel ( i.e.the density of the dependence graph), but
also the structure of such dependencies, which discloses e.g.
whether two tasks have cyclic dependencies. This provides a
principled basis for oine estimation of available parallelism
that abstracts away low-level deployment details (such as
the size of the cache, the number of cores, the number of
executing threads, etc).
An important emphasis of our approach is on eective user
interaction: The application designer, who is best aware of
pertinent workload features, communicates this information
through a concise and lightweight specication. The adapta-
tion algorithm, in turn, leverages this information to build a
bridge|using statistical analysis|between workload char-
acteristics and available parallelism, which is known as a
challenging task for fully automated systems [27].
Scope and limitations. We expect the application designer
to provide candidate features as well as representative work-
loads for proling. We assume that the program is already
parallel, or at least contains atomicity annotations, and so
the notion of atomic tasks is specied over the program's
code. We further assume that the criterion for correct par-
allel execution is serializable execution of atomic blocks (or
transactions), which Tightfit guarantees.
A dependent, and more subtle, assumption is that parallel
tasks communicate only by modifying the shared memory,
as is the case e.g. with applications using STM, and thus
data dependencies are a reliable model of runtime conicts.
A main source of complexity in Tightfit is the learning
process. This process entails oine analysis, as well as user
involvement, but in return it enables low-overhead input-
centric adaptation. At present, our prototype has limited
inference capabilities, and so the user has to choose which
statistical learning algorithm to apply (though this, again,
is a question of performance/accuracy and not correctness).
We intend to reduce this conguration burden in the future
by introducing inference capabilities into Tightfit .
Contributions. The principal contributions of this paper
are the following:
Adaptation with foresight. We present a novel solu-
tion for the problem of adaptive parallelization, focus-
ing on the direct connection between input features
and parallelism. This is achieved via expensive of-
ine analysis of training runs (backed by user-provided
input features), alongside low-overhead, input-centric
runtime adaptation.
Adaptation based on input features. We make the
relationship between the input data and available par-
allelism amenable to learning by abstracting the data
as a set of features according to a user-provided speci-
cation. (We believe that this novel idea can be reused
in other contexts.)
Parallelism characterization via analysis of de-
pendencies. We derive quantitative as well as struc-
tural characteristics of data dependencies to estimate
the available parallelism, thereby abstracting awayGraph g = /* read input graph */;
Graph mst = g.getNodes();
List worklist = g.getNodes();
@atomic foreach (Node nd in worklist) {
 Node nbr = minWeight(nd, g.getAdjacent(nd));
 Node nnd = edgeContract(nd, nbr);
 mst.addEdge(nbr, nnd);
 worklist.add(nnd); }
Figure 2: The Boruvka MST algorithm
n1 n2 
n5 n3 
n6 n4 
n7 3 4 2 6 
5 7 1 (a) 
(b) (c) 
(d) n2 
c1 n3 
n6 c2 
4 2 6 
5 7 n2 
c1 c3 
n6 4 2 
5 
c1 c4 
4 
n6 5 
Figure 3: Dierent revisions of an input graph under
the Boruvka MST algorithm
implementation-specic details. This goes beyond the
standard approach of treating data dependencies qual-
itatively as a clear-cutting criterion for disjoint paral-
lelism.
Implementation and evaluation. We have imple-
mented our approach in Tightfit . We describe our
experiments with Tightfit over two dierent adap-
tive parallelization systems and a suite of eight bench-
marks. The results provide solid evidence in support
of our approach. ( Tightfit is available online at [3].)
2. OVERVIEW
In this section, we motivate our approach with reference
to Boruvka's MST algorithm. We then walk the reader
through Figure 1, which summarizes the entire ow of the
Tightfit system.
2.1 Boruvka‚Äôs algorithm
Figure 2 shows Boruvka's algorithm. The algorithm com-
putes a minimum spanning tree by reducing the input graph
to a single node through successive application of edge con-
traction. In each iteration, a graph node ndis selected non-
deterministically, a minimum-weight neighbor nndofndis
obtained, and the edge between ndand nndundergoes con-
traction (becoming part of the MST).
For a sparse input graph, this permits a high level of par-
allel work at the beginning, where dierent contractions are
applied to disjoint regions of the graph, but ultimately, as
the graph grows smaller, the available parallelism gradually
decays [17]. These two cases are illustrated in Figure 3: The
transition from state (a) of the graph to state (b) is via two
parallel, non-overlapping edge contractions: ( n1;n5)!c1
and ( n4;n7)!c2. However, the application of contraction
(c2;n3)!c3, shown as state (c), cannot run in parallel with
its succeeding contraction, ( n2;c3)!c4, which appears in
(d). Both access a common region in the graph.features Graph:g {
 "nnodes": { g.nnodes(); }
 "density": { (2./zero.noslash * g.nedges()) /
                g.nnodes() * (g.nedges()-1); }
 "avgdeg": { (2./zero.noslash * g.nedges()) / g.nnodes(); } .... }
Figure 4: Fragment of the Tightfit specication for
the Graph type ( cf.Figure 2)
2.2 Flow
TheTightfit system breaks the parallelization process
into several phases, which we discuss in turn.
Parallelization modes. For a given adaptive paralleliza-
tion system, Tightfit needs to establish a mapping from
parallelism levels to eective execution modes of the adap-
tive system. This is done once per each system. The
parallelism-to-mode mapping, shown as the \Sys.-level para.
7!mode" box in Figure 1, can either be specied by the user
or it can be learned automatically.
In this paper, we place more emphasis on the problem of
estimating application-specic per-input parallelism, and so
to compute the parallelism-to-mode mapping, we resorted
to the standard solution of running the system on a syn-
thetic benchmark whose available parallelism is parametric
to nd eective threshold values for switching between par-
allelization modes [33]. (The underlying assumption is that
dierent modes correspond to dierent parallelism levels.)
We expand on this step in Section 4.
Oine adaptation. The main focus of this paper is
on learning an input-centric application-specic adaptation
rule. To make the learning setting induced by oine adap-
tation feasible (especially in the case of complex inputs, like
a graph), we ask the user to \simplify" the input description
by providing a feature extraction function that reduces the
input to a vector of simple features. This is summarized as
the double-framed \Input 7!features" box in Figure 1. Fig-
ure 4 illustrates the specication format for the example of
aGraph data structure.
Oine learning of adaptation rules. (Input7!fea-
tures ) For every application, we learn an input-centric
adaptation rule. To make the learning procedure feasible,
especially in the case of complex inputs, like a graph, we
ask the user to \simplify" the input description by provid-
ing a feature extraction function that reduces the input to a
vector of simple features. This is summarized as the double-
framed \Input7!features" box in Figure 1. Figure 4 illus-
trates the specication format for the example of a Graph
data structure.
The required specication puts little responsibility on the
user's shoulders, in that even if the specication is incom-
plete or simply irrelevant, correctness ( i.e.serializability) of
parallel execution is guaranteed. The user can only aect the
ecacy of adaptation decisions, at most degrading perfor-
mance if providing a bad specication. Moreover, the speci-
cation is not application specic. All graph algorithms, for
example, can share the specication in Figure 4.
Tightfit favors a comprehensive specication, including
features that may prove useless or irrelevant rather than
excluding them. This is because Tightfit 's learning algo-
rithm applies regression, seeking correlations between input
features and (estimated) parallelism levels, which automat-
ically prunes irrelevant ( i.e.weakly correlated) features, as-Input7!features //,,
Features7!prof.11Feature sampling //Mode selection Sys.-level prof.7!mode oo
Oine (per app.) Sensor Runtime  Actor Oine (per sys.)
Figure 1: Outline of the complete Tightfit ow
signing them low weight in the adaptation rule. This makes
such features harmless, and thus the more exhaustive the
specication is, the better the adaptation rule becomes.
Since feature extraction is done not only oine, but also
online, during parallel runs, features should be eciently
computable. If at runtime feature computation requires too
many cycles ( e.g.counting how many cycles or simple paths
a graph contains), then the performance benets of adapta-
tion are obviated. As an example, the cost of methods nnodes
and nedges in Figure 4 should be low.
In addition, to ensure the statistical signicance of the
regression algorithm correlating input features with paral-
lelism, we ask the user to characterize the application's in-
put space by providing legal ranges for input parameters.
Tightfit features a built-in harness for sampling inputs at
random based on the user's characterization of parameter
ranges.
(Features7!parallelism prole ) The next step in oine
adaptation, given the availability of input features, is to esti-
mate available parallelism over the training runs. Our main
observation here is that prole-guided analysis of data de-
pendencies between tasks designated for concurrent execu-
tion permits eective measurement of available parallelism.
This is because at runtime, data dependencies translate into
conicting accesses to shared memory, which mandate syn-
chronization, thereby limiting parallelism. Moreover, com-
pared to more concrete measures of available parallelism
(like direct measurement of running time over dierent in-
puts), data dependencies are less coupled with low-level de-
ployment details (like the hardware architecture, number of
threads, caching, etc), which makes the learned input-to-
parallelism correlations more signicant as well as robust
to changes in the parallelization system and/or deployment
conguration.
For the example of Boruvka, there are indeed no data de-
pendencies between the rst two loop iterations, but the
next two iterations, performing overlapping contractions,
generate data dependencies over c3. That is, there is an
increase in the number of data dependencies as the com-
putation evolves, which is compatible with the observation
that the available parallelism in Boruvka gradually decays.
Quantitative abstraction of data dependencies enables of-
ine generation of input-centric parallelism proles: The
target application is run on dierent inputs, a dependence
graph is computed for each trace, and the density and shape
of the graph are analyzed to derive a general (rather than
deployment-sensitive) measure of available parallelism. This
exposes the relationship between input properties and paral-
lelism level, allowing prediction of the available parallelism
for new inputs, as summarized in the \Features 7!prof."
box in Figure 1. This analysis is the subject of Sections 3
and 4.1.
(Parallelism prole 7!mode ) The remaining task is to
correlate between parallelism proles with dierent execu-tion modes of a particular adaptive parallelization system.
This correlation is achieved via a synthetic benchmark suite
that enables parametric control over the amount of paral-
lelism in a run. This task is summarized in the \Sys.-level
prof.7!mode" box in Figure 1, and is the subject of Sec-
tion 4.2.
Runtime parallelization. The nal stage in the Tight-
fitow is online parallelization. Here the oine adapta-
tion stragey is utilized by sampling|at the beginning of the
run, and possibly also throughout the run (when starting
an atomic block) if there are phase transitions|the feature
values for the input at hand. This is done, based on the
same feature extraction function serving for oine analy-
sis, by the \Sensor" module, which ows this information to
the \Actor" component. The \Actor" module then makes a
mode recommendation (the \Mode selection" box) based on
the composition of the \Features 7!prof." and \Sys.-level
prof.7!mode" functions computed oine. In Section 5, we
discuss two adaptive parallelization systems with which we
evaluated Tightfit .
3. PREDICTING A VAILABLE PARAL-
LELISM FROM DATA DEPENDENCIES
In this section, we explain how the available parallelism in
a given (sequential) execution trace is estimated by analysis
of data dependencies.
3.1 Atomicity-aware dependence graphs
A sequential trace is a sequence [ :::(;s;0):::] of tran-
sitions, where sis a primitive statement and and0are
the prestate and poststate, respectively. Abstracting a trace
as a dependence graph is standard [31]. For the purpose of
this paper, we dene a slight variant, which we refer to as
anatomicity-aware dependence graph (AADG). We assume
that the program is annotated with atomicf:::gsections,
and that the semantics records entry and exit events corre-
sponding to entering and leaving atomic sections.
This yields, for a given sequential run of the program, a
partitioning of the transitions coming from atomic sections
into distinct atomic blocks . (Note that dierent executions,
or instances, of the same atomic section correspond to dif-
ferent atomic blocks.)
Traceis abstracted as an AADG as shown in Figure 5:
1. Every transition t= (;s;0) occurring within an
atomic block bis mapped to a node ( t;b).
2. For each memory location lread byt, we draw edges to
all nodes (t0;b0), such that block b0diers from bandl
is written by t0(i.e.there is ow dependence between
tandt0, and these transitions occur in dierent atomic
blocks).
3. For each location lwritten by t, we draw edges to all
nodes (t0;b0), such that block b0diers from bandlis
either read or written by t0(i.e. there is either anti
dependence or output dependence between tandt0,Oine Estimation of Parallelism by Analysis of Dependencies
Input:
 : execution trace with atomicity annotations
OfflineEstimate : [returns estimated contention, cyclic dep.s]
 letG=;[initialize empty dependence graph]
 for each transition toccurring in atomic block bin:
  insert node ( t;b) intoG
  for each mem. loc. l2r(t): [r(:) denotes readset]
    for each node ( t0;b0)2Gs.t.l2w(t0),b6=b0:
     insert edge ( t0;b0) (t;b) intoG
  for each mem. loc. l2w(t): [w(:) denotes writeset]
    for each node ( t0;b0)2Gs.t.l2r(t0)[w(t0),b6=b0:
     insert edge ( t0;b0) (t;b) intoG
 return ( densityG;cdepG)
Figure 5: Oine alg. for parallelism estimation
which occur in dierent atomic blocks).
Note that the algorithm is parametric in the specication of
readsets and writesets ( randw), as well as in the grain of
trace transitions [31]. (We explain the algorithm's output
soon.)
The above steps yield a standard dependence graph mod-
ulo two adjustments: First, nodes point to their enclosing
atomic block, and second, there are no intra-block depen-
dence edges. Intuitively, this enables qualitative checking , as
well as quantitative measurement , of dependencies between
statements belonging in distinct atomic blocks designated
for parallel execution.
Note, importantly, that Tightfit computes AADGs
solely over sequential traces (and not parallel ones). How-
ever, assuming the parallelization system guarantees serial-
izability, there is no loss of generality here. This is because
a parallel trace can be serialized, and thus its AADG is the
same as that of its corresponding sequential run, allowing
oine analysis to consider only sequential executions.
3.2 Quantifying dependencies
Given an AADG G, we measure two aspects of the paral-
lelism it permits:
Contention. Contention is interpreted as the level of (po-
tential) interference between tasks designated for parallel
execution. This corresponds naturally to the density of the
AADG (when viewed as an undirected graph):
densityG= 2jG:Ej=(jG:Vj(jG:Vj 1)):
We obtain a normalized measure of the intensity (or pro-
portion) of dependencies between atomic blocks. (Data de-
pendencies between transitions in the same atomic block are
suppressed to concentrate on inter-task dependencies.)
Cyclic dependence. Another measure of parallelism is the
level to which tasks exhibit cyclic dependencies. This sit-
uation, illustrated in Figure 6, arises when distinct atomic
blocks access two or more memory locations in opposing or-
ders. In Figure 6, this is shown for four transitions from two
atomic blocks. The upper-left and bottom-right transitions
are dependent due to memory location l1(output depen-
dence), whereas the other two transitions are dependent due
tol2(ow dependence). Measuring cyclic dependencies|
beyond contention|is useful for synchronization, because
some protocols work well under high contention, but cope
poorly with cyclic dependencies [20, 21].((1;x:=10;0
1);b1) ((3;y:=2;0
3);b2)
((2;y:=5;0
2);b1)55
((4;z:=x;0
4);b2)ll
Figure 6: Illustration of a sitaution where two
atomic blocks, b1andb2, are cyclically dependent
To dene this measure, we need an auxiliary denition:
We say that atomic blocks bandb0arecyclically depen-
dent in AADGGover trace, denoted by bG
b0, if there
are two memory locations landl0, and four transitions
(t1;b)(t2;b)(t3;b0)(t4;b0) (whereis the order
of appearance of transitions in ), such that
t1andt4are the rst transitions in bandb0, respec-
tively, to access l;
t2andt3are the rst transitions in bandb0, respec-
tively, to access l0; and
(t1;b) (t4;b0) and (t2;b) (t3;b0)2G:E.
At runtime, this translates into a cyclic conict if the exe-
cutions ofbandb0are interleaved ( i.e.t1andt3are rst
executed, which forces cyclic dependence between bandb0).
Based on the above denition, we quantify cyclic depen-
dence as the following normalized measure:
cdepG= 2fbG
b0g=(nblksG(nblksG 1));
where nblksGis the number of distinct atomic blocks in G.
The algorithm in Figure 5 outputs a pair of real numbers
in the range [0 ;1], which denote estimates of contention and
cyclic dependencies. Together, these estimates provide a
characterization of the available parallelism in trace , which
we refer to as the parallelism prole of.
4. ADAPTATION VIA OFFLINE
LEARNING
Given the mechanisms of Section 3 to estimate parallelism
for an execution trace, we now describe an oine learning
system that synthesizes|based on a nite number of \rep-
resentative" traces for an application|a specialized adapta-
tion strategy for that application. The learning process is
independent of the parallelization system.
4.1 Learning per-input parallelism
A high-level view of the input-to-parallelism learning algo-
rithm is given in Figure 7. This algorithm accepts as input
a collection of sequential execution traces ( 1:::m), along
with their respective input workloads ( i1:::im).Tightfit
derives a parallelism prole ^ pjfromjusing quantitative
analysis (by applying the oalgorithm explained in Section 3
toj), and records the relationship between the features of
inputijand ^pj. (Recall that ^ pjestimates the available par-
allelism inj.)
The connection between inputs and their respective par-
allelism prole is lifted|via regression analysis|into a pre-
dictor,of, from input features to an estimated parallelism
prole, as shown in Figure 7: The free variables are the fea-
ture values, and the dependent variables are the estimates
of contention and cyclic dependencies. By default, Tight-
fitapplies linear regression analysis, though the choice of
regression model is parametric and congurable ( e.g.cubicInputFeatures Trace Para:profile
i1 //55 [f1
1:::f1
n]
&&1o//^p1
vv:::
im //55 [fm
1:::fm
n]
,,mo//^pm
rrRegression

i //[f
1:::f
n]of//^p
Figure 7: Abstract view of the oine input-to-
parallelism learning alg.
or quartic regression). To make the mapping from inputs to
parallelism prole amenable to learning, inputs are modeled
according to the user-provided features.
Regression analysis makes statistical assumptions over its
inputs. Specically, the training set's size should be propor-
tional to the number of independent variables, and the in-
puts should be sampled independently at random. To meet
these requirements, Tightfit provides a learning harness to
the user. The user is asked to specify an admissible range
of values for each application parameter. The harness then
picks parameter values at random to craft training inputs.
By default, Tightfit samples maxf150;50mginputs,
wheremis the number of features. This is a conserva-
tive approximation of several popular guidelines, including
N104 +m,N40mandN50 + 8m[8]. The user
can set other bounds if desirable. Our experiments conrm
these bounds as eective ( cf.Section 6).
4.2 Threshold values for mode transitions
A remaining task after learning the predictor, of, is to
correlate between parallelism estimates and modes of given
adaptive parallelization systems. Building this additional
bridge yields a direct relationship between input features
and parallelization modes, which concludes the oine adap-
tation process with an adaptation strategy that chooses be-
tween system behaviors according to input features. We
assume that the runtime adaptation system has several dis-
tinct modes, totally ordered according to parallelism level.
Thus, correlating between parallelism proles and paral-
lelization modes is essentially the problem of setting thresh-
olds for transitioning between modes.
Tightfit computes the thresholds for every paralleliza-
tion system, irrespectively of the particular application at
hand. Similarly to other adaptive systems [33], this is
achieved via a synthetic benchmark suite that enables para-
metric control over the amount of parallelism in a run. The
benchmarks manipulate a shared ConcurrentMap object using
a random client loop, where each iteration is an atomic task.
The proportion of read/write operations, range of keys and
number of operations are all parametric.
Within this setting, Tightfit induces (a large number
of) dierent parallelism proles, checking for each which
mode works best ( i.e.results in shortest execution time).
This yields empirical cuto values for transitioning between
modes as a function of the parallelism prole ( Tightfit 's
contention and cyclicity estimates).Oine Learning of Adaptation Strategy
Inputs:
 figm
i=1: execution traces with atomicity annotations
 features : feature extraction function
Parameters:
 : adaptive para. system with modes m1:::mk
 mode : mapping from para. estimate (( D;C)) to mode ( mi)
 regress : regression algorithm
OfflineAdapt : [returns actor function: ff7!fmigk
i=1g]
 let=;[mapping from feature val.s to para. proles]
 for each trace i:
  letfi=features0[0is the initial state in i]
  let (Di;Ci) =OfflineEstimate (i)
  insertfi7!(Di;Ci) into
 letof=ff7!(eD;eC)g=regress[avail. para. predictor]
 returnff7!(modeof)fg[actor: feature val.s to mode]
Figure 8: Oine alg. for synthesizing the \Actor"
module for an adaptation scheme
4.3 Putting it all together
The complete learning system is shown in pseudocode
in Figure 8, where mode is the thresholding algorithm. Note
that in Figure 8, the oine learning algorithm is param-
eterized by the (system-specic) mapping from parallelism
proles to modes of the parallelization system . The Offli-
neEstimate algorithm developed in Section 3 is a subrou-
tine of OfflineAdapt . The output is the \Actor" module
for parallelization system , formed as the composition of
mode and the parallelism predictor of.
This enables the runtime system outlined in Figure 1. The
\Sensor" module realizes the reading of feature values o the
application's state (initial state in general, and intermedi-
ate states to account for phase transitions), and the \Actor"
module issues (if needed) a mode selection request based
on the feature values sampled by the \Sensor": First, the
\Actor" applies ofto obtain a parallelism prole, thus es-
timating the available parallelism, and then it invokes the
mode function to obtain the recommended mode.
5. ADAPTIVE RUNTIME SYSTEMS
Research on data-dependent parallelism has resulted in a
wide range of adaptive concurrency control mechanisms. We
have realized two such mechanisms to evaluate the ecacy of
Tightfit , which we describe in this section. In both cases,
the initial adaptation decision is made at the beginning of
a run. The user can congure Tightfit to perform addi-
tional adaptation steps during the run to account for phase
transitions. The user then also congures the frequency of
adaptation decisions. A value nmeans that once every n
transaction starts an adaptation decision will be taken.
5.1 Switching between STM protocols
Dierent STM protocols have dierent strengths and
weaknesses. Some protocols work better under high con-
tention, whereas others are specialized for high-parallelism
workloads [19]. This leads to an adaptation method,
whereby the runtime system switches between protocols ac-
cording to the (estimated) available parallelism. We have
designed such a system with three underlying protocols:
The retry protocol ( Retry ) applies eager specula-tion [25], aborting a transaction immediately as it
attempts conicting access to a memory location
\owned" by another live transaction ( i.e.at least one
of the transactions needs write access to the location).
This works well for low-contention proles.
Dependence-aware transactional memory ( DATM-
FG) [20, 21] is eective for high-contention proles
with scarce cyclic dependencies between transactions.
DATM-FG lets a transaction depend on another
transaction \owning" a needed memory location, in-
stead of aborting and retrying, eectively stalling the
dependent transaction as long as no cyclic dependen-
cies arise. This reduces concurrency, as well as retries,
which is desirable for high-contention proles.
Finally, for high-contention proles with a high pro-
portion of cyclic dependencies, a coarse-grained variant
ofDATM-FG , dubbed DATM-CG , is reserved. This
variant eectively simulates a global lock by viewing
all memory locations as one and the same, which obvi-
ates the threat of rollbacks due to cyclic dependencies.
The resulting \Actor" module has the following form:
Actor (f) =8
<
:Retry ifgcon(f)tcon
DATM-FG ifgcon(f)>tcon;fcyc(f)tcyc
DATM-CG otherwise
Because Retry andDATM-FG are friendly protocols [20],
switching between these protocols is permitted even when
there are live transactions synchronizing according to the
old protocol. To switch to/from DATM-CG , however, a
barrier is required. Otherwise serializability is not guaran-
teed. Our implementation instead makes an opportunistic
attempt to enforce the selection, but gives up if the attempt
fails due to live transactions. A backo mechanism improves
the probability that the next attempt will succeed.
5.2 Active threads
Another adaptation strategy is to set concurrency level|
as dictated by the number of active threads|acrroding to
the available parallelism [18, 13]. This is facilitated by par-
allelization systems with a congurable thread pool, whose
size can be adjusted during the run, such as clients of
the Java ThreadPoolExecutor library. The number of active
threads can be adapted at any point in the run, without
any constraints due to live transactions, and without aect-
ing the correctness of the run.
In our implementation, the \Actor" follows the template
Actor (f) =ceil(ncoresgcon(f) +fcyc(f)
2);
which yields an integral value in the range [1 ;ncores ] for real
coecients ;2[0;1]. That is, contention and cyclicity are
given distinct weights in deciding the concurrency level, up
to the number of available cores.
6. IMPLEMENTATION & EVALUATION
In this section, we describe our protoype implementation
of the Tightfit system, and report on experiments mea-
suring the eect of (oine) adaptation according to our ap-
proach in comparison with several other alternatives.
6.1 Prototype implementation
Tightfit is implemented as a Java library. Our prototype
loads user specications at runtime from a designated loca-
tion. It additionally inserts instrumentation hooks at thebeginning of code blocks designated for atomic execution
(as specied by an annotation) to perform feature sampling
and enforce adaptation requests. For instrumentation, we
use the Javassist bytecode instrumentation library [6].
For oine analysis, our prototype exposes a congurable
policy prescribing which regression algorithm to apply. We
use algorithms from the Weka library [4] for regression anal-
ysis, the default being the LinearRegression class. The user
can also control sampling parameters. For sampling, the
default interval is 50 atomic blocks. We used these default
settings in our experiments.
6.2 Experiments
Benchmarks. Our experimental suite included eight
benchmarks. These are described in Table 1. All
benchmarks, excluding Boruvka and Elevator, were taken
from JSTAMP|the Java port of the STAMP benchmark
suite [15]|which is distributed as part of the Deuce STM
implementation [1]. The Boruvka benchmark is based on
a sequential implementation of Boruvka's algorithm that
is available as part of the Java version of the LoneStar
suite [12]. Elevator is taken from the pjbench suite of paral-
lel Java benchmarks [2]. Elevator and Bank are lock-based
parallel applications. MatrixMultiply is embarrassingly par-
allel, admitting zero conicts on any input matrix, but is
included in the evaluation for sanity checking and overhead
assessment. The remaining ve benchmarks are irregular,
and thus use STM.
Our specication of workload features over these bench-
marks was straightforward: For all benchmarks except Boru-
vka, we set the command-line arguments as the candidate
features. This is because the benchmarks are all paramet-
ric per command-line values. For Boruvka, we specied the
three features in Figure 4; namely: number of nodes, density
and average node degree.
Experimental setup. We designed two experiments, cor-
responding to the adaptive systems described in Section 5.
In the rst experiment, we implemented an adaptive system
that can switch between STM protocols ( cf.Section 5.1),
and used the Boruvka algorithm as well as the ve JSTAMP
STM applications as benchmarks. In the second experiment,
we investigated adaptation by means of changing concur-
rency levels ( cf.Section 5.2). This mechanism works uni-
formly both for STM and for lock-based synchronization,
and thus we considered all eight benchmarks. (The STM
benchmarks used the \DATM-FG" protocol.)
In the rst experiment, we compared Tightfit with three
non-adaptive STMs, each using one of the protocols underly-
ing the adaptive system (\Retry", \DATM-FG"and\DATM-
CG"). In the second experiment, we compared Tightfit
against three xed concurrency levels (\2 thr.s", \4 thr.s"
and \8 thr.s", where \1 thr" serves as a reference), as well
as an online adaptation algorithm (\Online") that switches
between the protocols by tracking per-thread abort/commit
ratio [5, 34].
In both experiments, we also compared the ecacy of
the two-step oine learning technique featured in Tight-
fit(where the application-specic predictor is computed
separately from the system-specic thresholding function)
with two oine adaptation algorithms (\O-4" and \O-8")
that learn a predictor from input features to system modes
directly. This is achieved by correlating between input fea-
tures and the mode yielding the shortest execution time forBenchmark Domain/Description Inputs
Boruvka Scientic: Computes MST 5000{10000 nodes, avg. degree between 1{5
Genome Bioinformatics: Performs gene sequencing 256  g1024, 8  s64, 32768  n131072
Intruder Security: Detects network intrusions 1  a25, 2  l32, 1024  n65536, 1  s256
KMeans Data mining: Implements K-means clustring 5  m; n320, 100 1  s4 1
MatrixMultiply Scientic: Performs matrix multiplication NNmatrices, where 100 N1000
Vacation Online tx. processing: Emulates a travel reservation sys. 1  n10, 1  q; u100,213  r215, 210  t213
Bank Online tx. processing: Emulates a banking sys. 1000  n10000, 0  i50000,1000  m100000
Elevator Discrete event simulator: Simulates a sys. of elevators 10{10000 oors, random bias over from/to oor numbers
Table 1: Benchmarks, including parameter ranges for random workload generation
that input, where \O-4" utilizes four threads for this task
and \O-8" trains with eight threads.
We conducted the experiments on a 64-bit Linux machine
with an Intel Core i7-870 processor combining four dual
cores (at 2.93GHz), each multiplexing 2 hardware threads,
for a total of 8 threads. The host VM was Java SE Devel-
opment Kit 6 Update 25 (JDK6u25). The input ranges we
used, both for oine analysis and for the deployment runs,
are listed in column three of Table 1. The number of traces
we considered for training is discussed in Section 4.1. For
the production runs, we selected at random 20 inputs per
benchmark. For each input and concurrency level, ranging
from 1 to 8 threads, we ran the benchmark 4 times and
recorded the average across the last 3 runs, excluding the
rst (cold) run. The measurements reported in Section 6.3
represent the average across all 20 of the selected inputs.
6.3 Performance results
Speedup and retry trends for the rst experiment (the
system of Section 5.1), going from one to eight threads,
are shown in Figures 9{14 and Figures 15{19, respectively.
We omit retry statistics for MatrixMultiply, which does not
admit any conicts ( cf.Section 6.2). We also omit retry
statistics for the \DATM-CG" protocol, since this protocol
simulates a global lock, and thus prevents retries completely.
Average speedup and retries for eight threads are summa-
rized below:
Speedup Retries
all wo. MMul all wo. MMul
Retry 3.75 3.04 1.53 1.84
DATM-FG 4.38 3.77 0.32 0.38
DATM-CG 3.96 3.28 | |
Tightfit 4.91 4.43 0.21 0.25
Online 4.18 3.54 0.52 0.62
O-4 4.92 4.44 0.22 0.26
O-8 5.27 4.83 0.19 0.22
Tightfit achieves better speedup, and less retries, than the
xed alternatives ( i.e.\Retry", \DATM-FG" and \DATM-
CG") as well as online adaptation (\Online"). As for direct
oine learning (\O-4" and \O-8"), at rst glance the re-
sults seem to suggest that these are comparable if not su-
perior to Tightfit : \O-4" has similar speedup and retry
statistics to Tightfit over eight threads, and \O-8" is
approximately 8% faster (more accurately: 7% with Ma-
trixMultiply and 9% without MatrixMultiply). However,
more careful analysis of the results|and in particular, the
speedup and retry trends visualized in Figures 9{14 and Fig-
ures 15{19, respectively|reveals that on average, Tightfit
is as eective as \O-4" and \O-8" on benchmarks with
high variance in parallelism, such as Genome and KMeans,
across the entire range of concurrency levels . We return to
this point in Section 6.4.The speedup results we obtained in the second experi-
ment (the system of Section 5.2) are listed in Figure 20.
For some of the benchmarks (including Elevator, KMeans
and Genome), Tightfit is superior to alternatives xing
the number of active threads throughout all inputs and the
entire duration of the run. Retries and memory usage statis-
tics for the lock-based benchmarks as well as nonzero-retry
STM benchmarks under \DATM-FG" are reported below:
ModeRetries Memory
Genome Boruvka Vacation Bank Elevator
1 thread 0 0 0 1 1
2 threads 0.18 0.07 0.19 0.98 0.99
4 threads 0.22 0.2 0.48 0.95 0.96
8 threads 0.56 0.46 0.99 0.92 0.94
Tightfit 0.47 0.31 0.76 0.93 0.94
O-4 0.53 0.36 0.70 0.94 0.95
O-8 0.51 0.33 0.72 0.96 0.96
Again here, \O-4" and \O-8" appear slightly better
thanTightfit on some benchmarks and concurrency levels,
though consistently with the results of the rst experiment,
Tightfit achieves comparable speedups across all concur-
rency levels on benchmarks with highly dynamic parallelism
(namely, Boruvka, Genome and KMeans).
6.4 Discussion
Overall, the experimental results provide support for the
main thesis of this paper, which puts forward the direct con-
nection between input features and available parallelism as
the basis for adaptation. For the adaptive STM system (Sec-
tion 5.1), Tightfit 's oine adaptation algorithm is measur-
ably better than its online alternative, also yielding better
012345678
1 thr
2 thr.s
4 thr.s
8 thr.s
Tightfit
Off-4
Off-8
Figure 20: Speedup as function of active threads11.522.533.544.555.566.577.5
1 2 3 4 5 6 7 8Retry
DATM-FG
DATM-CG
Tightfit
Online
Off-4
Off-8Figure 9: Speedup: Intruder
11.522.533.544.555.566.577.5
1 2 3 4 5 6 7 8Retry
DATM-FG
DATM-CG
Tightfit
Online
Off-4
Off-8 Figure 10: Speedup: Genome
11.522.533.544.555.566.577.5
1 2 3 4 5 6 7 8Retry
DATM-FG
DATM-CG
Tightfit
Online
Off-4
Off-8 Figure 11: Speedup: Boruvka
11.522.533.544.555.566.577.5
1 2 3 4 5 6 7 8Retry
DATM-FG
DATM-CG
Tightfit
Online
Off-4
Off-8
Figure 12: Speedup: Vacation
11.522.533.544.555.566.577.5
1 2 3 4 5 6 7 8Retry
DATM-FG
DATM-CG
Tightfit
Online
Off-4
Off-8 Figure 13: Speedup: KMeans
11.522.533.544.555.566.577.5
1 2 3 4 5 6 7 8Retry
DATM-FG
DATM-CG
Tightfit
Online
Off-4
Off-8 Figure 14: Speedup: MMul
results than the three baseline STM algorithms. A similar
trend is seen with the system of Section 5.2, where adapta-
tion is achieved by controlling concurrency level.
Moreover, our analysis of the oine artifacts conrms that
the learning algorithm is able to converge on the features
that eectively control parallelism. As an example, Tight-
fitwas able to converge on -qand -nas the signicant
features in Vacation. These two parameters control transac-
tion duration and query range, respectively, and are indeed
the determining factors of available parallelism in Vacation
(where -rand -tare global parameters aecting the overall
duration of the run), as the documentation for this bench-
mark conrms.
The improvement thanks to Tightfit is more noticeable
on benchmarks whose available parallelism is highly dy-
namic, such as KMeans and Genome, and to a lesser extent
also Boruvka. Benchmarks with less variance in parallelism
proles, such as Vacation, provide less room for optimization
thanks to (oine) adaptation, leaving Tightfit similar to
the \Online" adaptation algorithm in performance.
This observation also connects to the comparison between
Tightfit and the two other oine adaptation techniques:
\O-4" and \O-8". While benchmarks whose available par-
allelism is more stable across dierent workloads seem to fa-
vor \O-4" and \O-8", Tightfit is competitive with both
of these alternatives on the benchmarks whose parallelism
changes signicantly across inputs throughout the entire
spectrum of concurrency levels. In particular, while \O-
4" is more eective in the neighborhood of four threads,
and\O-8" performs better when concurrency level is closer
to eight threads, Tightfit is more stable than both of these
alternatives, and thus achieves comparable speedups on av-
erage across all concurrency levels.
The experiments indicate the superiority of the oine
adaptation technique suggested in this paper. However, it
appears that if the deployment setup ( i.e.concurrency level,
cache sizes, processor architecture, etc) is known a priori ,
then direct learning (in the style of \O-4" and \O-8") is
potentially preferable. If, on the other hand, the deployment
setup is subject to variation ( e.g.due to usage of several dif-ferent hardware congurations), then we expect Tightfit 's
learning algorithm to provide better results as it abstracts
away all deployment details.
Another advantage of the oine learning algorithm of
Tightfit over direct learning is that it allows for separa-
tion of concerns: The application designer provides useful
input features, and the designer of the adaptive system sep-
arately decides which execution mode best ts every par-
allelism prole. Recall that Tightfit automatically learns
the mapping from parallelism proles to system modes. We
believe that if this mapping could be specied by an expert,
then Tightfit would achieve even better results. (Asking
for such a specication is reasonable, as it is done once per
system.) We leave research on this topic for future work.
7. RELATED WORK
For space constraints, we restrict our discussion to closely
related research on adaptive parallelization. A more detailed
review of the related work appears in [30]. In the broader
scope of prole-guided specialization, we refer the reader
to [22] for adaptive garbage collection, [28] for prole-guided
compile-time parallelization, and [16] for prole-based spe-
cialization of static heap abstractions.
The transactional concurrency tuning system [5] uses
a control-theoretic model to adapt the number of active
threads to the available parallelism, where the percentage
of committed transactions out of all executed transactions
in a sample period provides a measure of available paral-
lelism. A closely related approach, presented in [34], is to
stall a thread if its abort/commit history indicates low par-
allelism. A similar heuristic is implemented in the Galois
system [13]. The main distinction is that Tightfit esti-
mates available parallelism directly, based on input features,
instead of tracking indirect monitors related to the execution
system's behavior.
The Shrink system [7] utilizes the run prex to predict
transactional memory accesses. Prediction is based on the
access patterns of past transactions from the same thread.
The scheduler then tries to prevent transactions whose pre-00.511.522.533.54
1 2 3 4 5 6 7 8Retry
DATM-FG
Tightfit
Online
Off-4
Off-8Figure 15: Retries: Intruder
00.511.522.533.54
1 2 3 4 5 6 7 8Retry
DATM-FG
Tightfit
Online
Off-4
Off-8 Figure 16: Retries: Genome
00.511.522.533.54
1 2 3 4 5 6 7 8Retry
DATM-FG
Tightfit
Online
Off-4
Off-8 Figure 17: Retries: Boruvka
00.511.522.533.54
1 2 3 4 5 6 7 8Retry
DATM-FG
Tightfit
Online
Off-4
Off-8
Figure 18: Retries: Vacation
00.511.522.533.54
1 2 3 4 5 6 7 8Retry
DATM-FG
Tightfit
Online
Off-4
Off-8 Figure 19: Retries: KMeans
dicted access sets overlap from running in parallel. Tracking
memory acccesses is reminiscent of our analysis of data de-
pendencies, though it is not clear how to cast Shrink into
our setting of oine learning.
Another form of adaptation, proposed in [32], is adap-
tive locking: The parallelization algorithm monitors the
execution, and|based on the collected statistics|decides
whether to execute a critical section (CS) speculatively or
using mutual exclusion. In [23], \hot" variables, which cause
large numbers of transactions to abort, are identied at run-
time; for these variables, the transactional manager selec-
tively switches to pessimistic concurrency control, where
reader/writer locks mediate access to locations. adapt-
STM [19] collects STM-internal statistics, like the number of
collisions in the hashtable mapping memory to locks, to tune
STM-internal data structures. The system of [24] goes be-
yond such ne-grained adaptivity (that optimizes a specic
STM implementation) to also support coarse-grained adap-
tivity, where a system-wide policy species when to switch
between STM implementations. The Sambamba system [26]
monitors the application's behavior and specializes functions
on-the-y for actual usage proles. One of the supported
optimizations is automatic parallelization using speculation,
where deciding whether to apply parallelization depends on
the available compute resources.
All of these systems adapt their behavior online, and could
benet from integration with Tightfit , so that the choice
of parallelization mode ( e.g. conguration of internal data
structures in adaptSTM or CS execution mode in adaptive
locking) is made oine based on input characteristics.
A mechanism for dynamic proling of a running trans-
actional program is presented in [33]. The obtained prole
is then used, in conjunction with a machine-learning (ML)
algorithm, to select an \optimal" STM implementation at
runtime. The ML algorithm is trained oine by measuring
the running time of parameterized microbenchmarks for all
available STM algorithms at multiple thread levels. Then,
during program execution, a xed number of transactions
is proled to guide the ML algorithm's choice of STM algo-
rithm. Oine learning records certain code-level character-istics of the microbenchmarks, such as whether transactions
contain loops. The online prediction rule is parameterized
by these distinctions. Recall that Tightfit estimates the
available parallelism as a function of user-specied proper-
ties of the data. In contrast, the predictor in [33] relies on
predened syntactic features.
In [27], the authors utilize oine analysis to discover sem-
inal behaviors . These are behaviors that typically manifest
at an early stage of execution, and are correlated with many
other behaviors of the program, permitting eective predic-
tion of the program's behavior. Seminal behaviors are ex-
tracted automatically to support proactive optimizations in
the Jikes VM. A similar approach, developed in [10], trades
training for incremental adaptation across production runs.
The authors of [10] apply this idea to predict the likelihood
of successful speculation, where predictions account for in-
put properties indirectly using classication trees. Tight-
fitshares the motivation of these works, but supports di-
rect learning of the relationship between input features and
parallelism, rather than passing through seminal behaviors,
thanks to oine learning.
The Janus algorithm [29], built on top of the Hawkeye
algorithm for abstract-level dependence tracking [31], uses
oine training to improve the precision of conict detection
during speculative parallelization. The key idea is to enforce
sequence-based detection, where sequences of operations|
rather than individual operations|are tested for commuta-
tivity. The training phase is used to observe which sequences
occur in the client application and check oine whether
they commute, so that the runtime overhead of sequence-
based detection becomes negligible. Tightfit is similar to
Janus in applying oine learning, but the learning process|
including its scope, ow and techniques|is quite dierent.
8. CONCLUSION AND FUTURE WORK
We have presented a novel approach for foresight-guided
adaptation, which permits low-overhead, input-centric run-
time adaptation by shifting most of the cost of predicting
per-input parallelism to an expensive oine analysis. Twoaspects of our approach are of particular interest: (i) speci-
cation of workloads in terms of useful features, which per-
mits direct learning of the connection between input char-
acteristics and available parallelism, and (ii) quantitative
and structural analysis of data dependencies as a means
of estimating available parallelism while abstracting away
deployment-specic details. Our approach is implemented
in the Tightfit system, which is publicly available [3], and
shows promising results in our experiments.
In the future, we intend to make Tightfit more au-
tomatic by introducing auto-tuning capabilities, similarly
to [9] ( e.g.to decide on eective threshold values for mode
transitions). We also plan to develop inference capabilities
to automatically converge on useful workload features.
9. REFERENCES
[1] Deuce stm. http://www.deucestm.org .
[2] The pjbench suite. http://code.google.com/p/pjbench/ .
[3] Tightt.
http://www.cs.tau.ac.il/~omertrip/software/tightfit/tightfit.html .
[4] The weka library. http://sourceforge.net/projects/weka .
[5] Mohammad Ansari, Mikel Luj an, Christos Kotselidis,
Kim Jarvis, Chris C. Kirkham, and Ian Watson.
Robust adaptation to available parallelism in
transactional memory applications. T. HiPEAC , 3,
2011.
[6] S. Chiba and M. Nishizawa. An easy-to-use toolkit for
ecient java bytecode translators. In GPCE , 2003.
[7] A. Dragojevi c, R. Guerraoui, A. V. Singh, and
V. Singh. Preventing versus curing: avoiding conicts
in transactional memories. In PODC , 2009.
[8] P. I. Good and J. W. Hardin. Common errors in
statistics (and how to avoid them). The American
Statistician , 58, 2004.
[9] P. Hawkins, A. Aiken, K. Fisher, M. C. Rinard, and
M. Sagiv. Concurrent data representation synthesis. In
PLDI , 2012.
[10] Y. Jiang, F. Mao, and X. Shen. Speculation with little
wasting: Saving cost in software speculation through
transparent learning. In ICPDS , 2009.
[11] Y. Jiang, E. Z. Zhang, K. Tian, F. Mao, M. Gethers,
X. Shen, and Y. Gao. Exploiting statistical
correlations for proactive prediction of program
behaviors. In CGO , 2010.
[12] M. Kulkarni, M. Burtscher, C. Cascaval, and
K. Pingali. Lonestar: A suite of parallel irregular
programs. In ISPASS , 2009.
[13] M. Kulkarni, K. Pingali, B. Walter,
G. Ramanarayanan, K. Bala, and L. P. Chew.
Optimistic parallelism requires abstractions. In PLDI ,
2007.
[14] F. Mao and X. Shen. Cross-input learning and
discriminative prediction in evolvable virtual
machines. In CGO , 2009.
[15] C. C. Minh, J. Chung, C. Kozyrakis, and
K. Olukotun. STAMP: Stanford transactional
applications for multi-processing. In IISWC , 2008.
[16] M. Naik, H. Yang, G. Castelnuovo, and M. Sagiv.
Abstractions from tests. In POPL , 2012.
[17] K. Pingali, D. Nguyen, M. Kulkarni, M. Burtscher,
M. A. Hassaan, R. Kaleem, T.-H. Lee, A. Lenharth,R. Manevich, M. M endez-Lojo, D. Prountzos, and
X. Sui. The tao of parallelism in algorithms. In PLDI ,
2011.
[18] K. K. Pusukuri, R. Gupta, and L. N. Bhuyan. Thread
reinforcer: Dynamically determining number of
threads via os level monitoring. In IISWC , 2011.
[19] M. Payer T. R. and Gross. Performance evaluation of
adaptivity in software transactional memory. In
ISPASS , 2011.
[20] H. E. Ramadan, C. J. Rossbach, and E. Witchel.
Dependence-aware transactional memory for increased
concurrency. In proc.s of the 41st annual IEEE/ACM
intl. Symp. on Microarchitecture , 2008.
[21] H. E. Ramadan, I. Roy, M. Herlihy, and E. Witchel.
Committing conicting transactions in an stm. In
PPOPP , 2009.
[22] S. Soman, C. Krintz, and D. F. Bacon. Dynamic
selection of application-specic garbage collectors. In
ISMM , 2004.
[23] N. Sonmez, T. Harris, A. Cristal, O. S. Unsal, and
M. Valero. Taking the heat o transactions: Dynamic
selection of pessimistic concurrency control. In
IPDPS , 2009.
[24] M. F. Spear. Lightweight, robust adaptivity for
software transactional memory. In SPAA , 2010.
[25] M. F. Spear, V. J. Marathe, W. N. Scherer, and M. L.
Scott. Conict detection and validation strategies for
software transactional memory. In ICDC , 2006.
[26] K. Streit, C. Hammacher, A. Zeller, and S. Hack.
Sambamba: A runtime system for online adaptive
parallelization. In Compiler Construction , 2012.
[27] K. Tian, Y. Jiang, E. Z. Zhang, and X. Shen. An
input-centric paradigm for program dynamic
optimizations. In OOPSLA , 2010.
[28] G. Tournavitis, Z. Wang, B. Franke, and M. F.
O'Boyle. Towards a holistic approach to
auto-parallelization: integrating prole-driven
parallelism detection and machine-learning based
mapping. In PLDI , 2009.
[29] O. Tripp, M. Manevich, J. Field, and M. Sagiv. Janus:
Exploiting parallelism via hindsight. In PLDI , 2012.
[30] O. Tripp and N. Rinetzky. Tightt: Adaptive
parallelization with foresight. Technical report.
[31] O. Tripp, G. Yorsh, J. Field, and M. Sagiv. Hawkeye:
eective discovery of dataow impediments to
parallelization. In OOPSLA , 2011.
[32] T. Usui, R. Behrends, J. Evans, and Y. Smaragdakis.
Adaptive locks: Combining transactions and locks for
ecient concurrency. In PACT , 2009.
[33] Q. Wang, S. Kulkarni, J. Cavazos, and M. Spear. A
transactional memory with automatic performance
tuning. ACM Transactions on Architecutre and Code
Optimization , 8, 2012.
[34] R. M. Yoo and H. H. Lee. Adaptive transaction
scheduling for transactional memory systems. In proc.s
of the twentieth annual Symp. on Parallelism in
algorithms and architectures , 2008.