Automated Support for Repairing Input-Model Faults
Senthil Mani, Vibha Singhal Sinha, Pankaj Dhoolia, Saurabh Sinha
IBM Research – India
{sentmani,vibha.sinha,pdhoolia,saurabhsinha}@in.ibm.com
ABSTRACT
Model transforms are a class of applications that convert a model
to another model or text. The inputs to such transforms are of-
ten large and complex; therefore, faults in the models that cause a
transformation to generate incorrect output can be difﬁcult to iden-
tify and ﬁx. In previous work, we presented an approach that uses
dynamic tainting to help locate input-model faults. In this paper, we
present techniques to assist with repairing input-model faults. Our
approach collects runtime information for the failing transforma-
tion, and computes repair actions that are targeted toward ﬁxing the
immediate cause of the failure. In many cases, these repair actions
result in the generation of the correct output. In other cases, the
initial ﬁx can be incomplete, with the input model requiring further
repairs. To address this, we present a pattern-analysis technique
that identiﬁes correct output fragments that are similar to the incor-
rect fragment and, based on the taint information associated with
such fragments, computes additional repair actions. We present the
results of empirical studies, conducted using real model transforms,
which illustrate the applicability and effectiveness of our approach
for repairing different types of faults.
Categories and Subject Descriptors
D.2.5 [ Software Engineering ]: Testing and Debugging— Debug-
ging aids, Tracing
General Terms
Algorithms, Experimentation, Measurement
Keywords
Model repair, repair actions, execution tracing, pattern analysis,
model-driven engineering
1. INTRODUCTION
Model-Driven Engineering (MDE) [13] is the paradigm of soft-
ware development that uses formal models, at different abstrac-
tion levels, to represent the system under development, and uses
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
ASE’10, September 20–24, 2010, Antwerp, Belgium.
Copyright 2010 ACM 978-1-4503-0116-9/10/09 ...$10.00.automated transforms to convert one model to another model or
text.1A signiﬁcant class of model transforms, called model-to-text
transforms, generate text output ( e.g., code, conﬁguration ﬁles, or
HTML/JSP ﬁles) from an input model ( e.g., XML or UML). The
inputs to the transforms are often large and complex, containing
tens of thousands of elements. Therefore, faults in an input model
that cause a transformation to fail by generating an incorrect output
can be difﬁcult to identify and ﬁx.
Although model-transformation failures can be caused by faults
in the transform, the goal of our work is to develop techniques for
investigating failures that are caused by faults in the input models.
This is particularly relevant in MDE, where it is a common prac-
tice for transform users to use transforms that are not written by
them, e.g., standard built-in transforms provided by a tool. There-
fore, when an execution of such a transform fails, the pertinent task
for the user is to locate and ﬁx faults in the failure-inducing in-
put models; investigating the transform code is irrelevant from the
user’s perspective.
In recent work [6], we presented an approach for assisting with
fault localization —identifying the input-model faults—for model-
to-text transforms. That approach uses dynamic tainting [3] to trace
the ﬂow of data from the transform input to the transform output.
Using the generated taint logs and a location in the output where a
missing or incorrect string occurs, the approach identiﬁes the fault
space that the user can examine incrementally to locate the fault.
Fault localization is only the ﬁrst step in debugging an input
model; the next step is fault repair : ﬁxing the fault so that the ex-
ecution of the transform on the ﬁxed model generates the correct
output. Although fault localization may identify the problematic
parts of the input with high accuracy, ﬁxing the fault may not be
obvious. To illustrate, consider a case in which an input-model at-
tribute isGen determines the outcome of a conditional statement in
the transform. An incorrect value of isGen causes a wrong path
to be taken and, consequently, an incorrect output to be generated.
Even though isGen may be identiﬁed precisely during fault local-
ization, ﬁxing the value of isGen may not be obvious. This is es-
pecially true when the user has no access to the transform code,
which, as mentioned, is often the case. Thus, in addition to fault
localization, a technique for supporting fault repair is essential for
developing efﬁcient model debugging.
Existing work on fault repair for models ( e.g., [8, 9, 14]) con-
siders only those faults that violate consistency constraints; such
faults can be detected using consistency checkers. However, in the
context of automated transforms, a large class of faults may violate
no (model or transform) constraints, and yet cause a transformation
1Atransform is a function, or a program, that maps one model to another
model or text; a transformation is the application, or the execution, of a
transform on a model instance [2].
195
failure. Such faults cannot be detected using automated checkers;
therefore, no support for fault repair can be provided.
To extend our work on fault localization [6] and address these
limitations, in this paper, we present techniques to support fault
repair—ﬁxing the input-model faults. Our approach for fault re-
pair collects runtime information for the failing transformation, and
uses the information to compute repair actions to be performed
on the failure-inducing input model. For example, a repair action
could suggest the addition of an element to the model, or a change
in the value of a model attribute.
We deﬁne different types of runtime information (or metadata )
about the failing transformation that need to be collected to enable
the computation of accurate repair actions. The metadata contain
information about conditional statements, looping constructs, and
how input-model elements and attributes are accessed during the
transformation. For example, for a conditional statement, we cap-
ture information about the comparison made on an input model el-
ement: the model element that is compared, the comparison opera-
tor, and the value compared with. To collect the runtime informa-
tion, we leverage and extend the dynamic-taint analysis developed
in our previous work [6].
We present two techniques that leverage the metadata to compute
repair actions. The ﬁrst technique, fault-index analysis, computes
repair actions that are targeted toward ﬁxing the immediate cause
of the failure. This initial set of repair actions can, in many cases,
ﬁx the fault completely so that the transform generates the correct
output on the repaired input model. However, in other cases, the
application of the repair actions may not generate the correct output
because the input model requires further repairs.
To address this, we present a second technique, which performs
pattern analysis on the output of the failing transformation to ﬁnd
instances of correct output string fragments that are similar to the
incorrect output string fragment. The intuitive idea underlying this
technique is that by analyzing correct samples of similar output
fragments, appropriate repair actions can be computed. For in-
stance, the correct samples can highlight omissions—missing el-
ements and attributes—that need to be added to the input model.
In addition to ﬁnding similar fragments in the output of the fail-
ing transformation, pattern analysis can be performed on the output
generated from exemplar input models.
We implemented our fault-repair approach for XSL-based trans-
forms that convert input models to text (Java code, properties ﬁles,
and conﬁguration ﬁles). Using the implementation, we conducted
empirical studies on a set of real transforms to evaluate the use-
fulness of our approach. Our results indicate that, for the subjects
considered, different types of metadata are necessary to enable the
computation of accurate repair actions. Moreover, the fault-index-
based repair actions sufﬁce for completely ﬁxing many types of
faults. For the types of faults for which the initial repair actions
are insufﬁcient, the pattern-analysis technique computed meaning-
ful repair actions in each instance.
The main contributions of the work are
•The deﬁnition of metadata about failing transformations that
enable the computation of accurate fault-repair actions
•The development of two complementary techniques, fault-
index analysis and pattern analysis, for supporting repairs of
input model faults
•The implementation of the techniques for XSL-based model-
to-text transforms
•Empirical results that illustrate the applicability of the tech-
niques in supporting model repairs for real model transforms2. BACKGROUND
In this section, we introduce an example to illustrate the problem,
and discuss our tainting-based fault-localization technique [6].
2.1 Example Transform and Input Model
Model-to-text (M2T) transforms are a class of applications that
transform input models to text ﬁles. Examples of such transforms
include UML-to-Java code generators and XML-to-HTML con-
verters. An M2T transform can be implemented using a general-
purpose programming language, such as Java. Such a transform
reads content from input ﬁles, performs the transformation logic,
and writes strings to output ﬁles. Alternatively, a transform can
be implemented using specialized templating languages, such as
XSLT (eXtensible Stylesheet Language Transformation), that let
developers code the transform logic as a template. The associ-
ated Xalan library ( http://xml.apache.org/xalan-j ) provides the
functionality to read the input and write the output to a text ﬁle.
Figure 1(a) presents PrintUser , an example M2T transform
written in XSLT. The transform takes as input an XML ﬁle con-
taining user information and prints the information in a plain-text
format. The XML shown in Part (b) of the ﬁgure illustrates a sam-
ple input to the transform. The input model contains two <user>
elements (lines 2–15 and 16–24), each of which has a <family> el-
ement. Each <family> element, in turn, contains a list of <user>
elements that specify the spouse and dependents of the containing
<user> element. As illustrated, a <user> element can have ﬁve at-
tributes: first ,last,hasMiddle ,middle , and gender . A<user> el-
ement also has a <relationship> element contained within it. Each
element has a unique node ID, speciﬁed as the tagattribute, which
is added by our infrastructure to identify input-model elements.
PrintUser iterates over the top-level <user> elements using the
XSL for-each construct (line 2). Within the loop, it ﬁrst calls func-
tion printUserName (shown in lines 44–52) to print the user name
(lines 4–6). Then, it processes <family> elements (lines 7–40): it
prints spouse details (lines 8–29) and dependent details (lines 30–
39). To print the spouse information, the transform uses the choose
construct at line 9 to test the gender attribute of <user> (lines 10,
19). If the value of gender isF, the transform selects the contained
<user> element whose <relationship> sub-element has type at-
tribute with value husband (line 11) and prints the husband’s name
(lines 13–16). Lines 20–26 perform a similar processing for the
case where the value of gender isM.
The output of the transform on the sample input is shown in Fig-
ure 1(c). Lines 1–9 of the output correspond to the data printed
for the ﬁrst <user> element (lines 2–15 of the input). Transform
lines 3–6 print output lines 1–2; transform lines 13–16 print output
line 5; and transform lines 30–39 print output lines 7–9.
The input model contains three faults that, which cause incorrect
output to be generated. The incorrect parts of the output, which
have missing substrings, are highlighted in the output as E1, E2,
and E3. E1 illustrates a missing middle name on output line 12.
This occurs because of a fault in line 17 of the input: the <user>
element is missing attributes hasMiddle andmiddle . E2 highlights
missing text for the spouse name, which occurs because of the fault
in input line 21, where element <relationship> is missing attribute
type. Finally, E3 highlights missing dependent details; this oc-
curs because element <family> in input line 18 is missing an entire
<user> sub-element.
2.2 Fault Localization
In previous work [6], we presented a dynamic-tainting-based ap-
proach that can assist in locating such input-model faults. As men-
tioned earlier, the goal of our work is to assist with locating and
196Figure 1: Illustrative example: (a) XSL transform PrintUser ; (b) a failure-inducing input; and (c) the corresponding incorrect
output (the error markers E1, E2, and E3 in the output point to locations where substrings are missing).
Figure 2: Association of taint marks with the input (left) and
propagation of the marks to the output (right).
ﬁxing input-model faults (and not transform faults). Therefore, we
assume that the transform is correct. Moreover, we assume that
the input model satisﬁes all consistency constraints imposed by the
metamodel or required by the transform.
The fault-localization approach associates taints marks with input-
model elements and adds probes to the transform that propagate the
taint marks to the output string. A key feature of our approach is
that it distinguishes different types of taint marks: data taints, con-
trol taints, and loop taints, which are propagated at different trans-
form statements. A data taint is propagated at assignments and
statements that directly, or indirectly, construct the output string.
Acontrol taint is propagated at conditional statements to the parts
of the output that are constructed along the branches of the condi-
tional. A loop taint is propagated at looping constructs to the out-
put strings that are constructed within the loops. The taint marks
associated with substrings of the output are used to construct a fault
space, which the user can examine incrementally to locate the fault.
To illustrate, consider a smaller fragment of the input model of
Figure 1(b), shown in a tree form in Figure 2 (left). The model el-ements and attributes have taint marks, labeled t1tot8, associated
with them. During the execution of the transform, these taint marks
are propagated to the output string to generate a taint log. The right
side of the ﬁgure shows an intuitive representation of the taint log.2
Different substrings in the output have data, control, and loop taints
associated with them. For example, the output string “Jane” has
data taint t3,dassociated with it because the string is constructed
from the input-model attribute first , which was initialized with
taintt3. For another example, the output string “P.” is generated
under a condition (line 47 of PrintUser ) whose outcome is deter-
mined by attribute hasMiddle , which was initialized with taint t5;
therefore, t5is propagated to “P.” as control taint t5,c. A data or
a control taint can be empty— i.e.,it may enclose no string. This
occurs when an input-model attribute is empty or no string is gen-
erated along the taken-branch of a conditional whose outcome is
determined by an input-model attribute.
Given a taint log and an error marker (i.e.,a location in the out-
put text where a missing or incorrect string occurs) identiﬁed by the
user, the fault-localization technique identiﬁes enclosing taints, in
reverse order of nesting, and constructs the fault space incremen-
tally. To illustrate, for failure E1, line 47 of PrintUser attempts
to access attribute hasMiddle for the second user. Because this at-
tribute does not exist, no control-taint information is generated for
the access. Starting at the error marker, our fault-localization tech-
nique would search for the enclosing control taints. It would iden-
tify the control taint that was propagated at line 2: the loop that lex-
2Figure 2 shows an abstract representation of the taint log. In our imple-
mentation, the taint log is an XML ﬁle in which different elements, and
their nesting structure, capture the taint information. We show samples of
taint logs in the discussion of our approach (Section 3).
197Figure 3: Overview of our approach, which performs fault-
index analysis and pattern analysis to compute repair actions.
ically encloses line 47 via the call in line 4. Thus, the fault would
be localized to the second <user> element in the input model.
Although our fault-localization technique is able to localize such
faults effectively, it provides no guidance on how the fault should be
ﬁxed. Next, we present techniques that provide automated support
for repairing such input-model faults.
3. AUTOMATED FAULT REPAIR
The goal of our approach for fault repair is to offer recommen-
dations to the user on how to ﬁx a faulty input model that causes
a transformation failure. A ﬁx recommendation Rconsists of a set
of repair actions on a faulty input model. A repair action ron an
input model is a unit change to be made to the model. We deﬁne a
repair action as follows
r::= ADD attr toelem t
ADD elem toelem t
SET attr tovalue | ¬value | ∗
The ﬁrst action adds an attribute with an arbitrary, possibly empty,
value to an element. The second action adds an element to another
element. The ﬁnal action sets an attribute to the speciﬁed value,
any value other than the speciﬁed value, or to an arbitrary value
(indicated by ‘ ∗’).elem is an element name, whereas elem tcan be
an element name (for a newly added element by a repair action) or
a node ID (for an existing element in the model).
Figure 3 presents the overall approach for debugging input-model
faults: it includes both fault localization and fault repair. Given
a transform program Tand a failure-inducing input model Ifaulty
for which Tgenerates an incorrect output, fault localization assists
in identifying the input-model fault. Given a user-provided error
marker and the taint log for the failing transformation, the fault lo-
calizer computes the fault space and the fault index Lﬁin the taint
log: the relevant taint mark for a given error marker.
To compute repair actions, we have extended the taint-propagation
mechanism to collect additional information (or metadata) about
the failing transformation. The fault-repair approach takes as input
the fault index in the taint log. It computes a set of alternative rec-
ommendations R. The user can select a recommendation R∈ R
and apply the repair actions in Rto the faulty input.
Next, we deﬁne the metadata required for computing accurate
repair actions (Section 3.1). Following that, we present the two
fault-repair techniques: fault-index analysis (Section 3.2) and pat-
tern analysis (Section 3.3). Both these techniques leverage a com-
mon algorithm for computing repair actions, which we present in
Section 3.4. Finally, we present an analytical discussion of the ap-
proach (Section 3.5).
3.1 Metadata Collection for Fault Repair
For fault localization, information about the ﬂow of input data to
output text in the failing transformation is sufﬁcient. However, tosupport fault repair, extra information that indicates how the input
data was used to create the output is required. This metadata essen-
tially allows us to recreate, to some degree, the transform logic. Our
approach collects three types of metadata for the failing transfor-
mation: (1) accesses of input-model entities, (2) conditional state-
ments, and (3) selection of model elements from a collection.
Metadata about entity access.
We collect metadata about transform statements that, directly or
indirectly, attempt to access input-model entities (elements or at-
tributes). For entities that exist in the input model, this information
is naturally captured via taint propagation. However, for a missing
entity, no taint initialization occurs; therefore, no taint propagation
is performed even though the transform attempts to access the at-
tribute. To handle such cases, we augment the transform instrumen-
tation to collect runtime information about accesses to input-model
entities, irrespective of whether the access succeeds.
The metadata is captured in a <get> element in the taint log. This
element has two attributes: xpath , which records the path in the in-
put model along which a model entity is accessed, and startNode ,
which records the input-model element on which the path is ex-
ecuted. Figure 4(a) shows the metadata collected at line 47 of
PrintUser when executed on the faulty input. The xpath at-
tribute of the <get> element indicates the access of hasMiddle .3
The startNode attribute of <get> indicates that the hasMiddle is
being accessed on element 1.4. Thus, although hasMiddle for the
second <user> element is missing, the collected information indi-
cates that the attribute was attempted to be accessed at line 47.
Metadata about conditional statements.
For a conditional statement ( i.e.,anifor a case statement), our
technique collects information about the comparison that occurs
at the statement: the left-hand and right-hand expressions (which
could represent an input-model entity or a string literal), and the
comparison operator. For example, for line 47 of PrintUser , we
collect information that attribute hasMiddle (for a particular <user>
element) is compared to “true” using the equality operator.
In the taint log, the information is represented in a <condition>
element, with attributes: (1) leftNode and lvalue to capture the
left-hand expression, (2) opto capture the comparison operator, and
(3)rightNode andrvalue to capture the right-hand expression. If
an expression is a string literal, the node attribute does not occur. If
the accessed input-model attribute does not exist, neither the node
attribute nor the value attribute exist. The <condition> element in
the taint-log fragment for line 47, shown in Figure 4(a), illustrates
this. In this case, <condition> is missing leftNode and lvalue
attributes because the accessed attribute hasMiddle does not exist
in the input model.
Metadata about selection in loops.
Transforms often select speciﬁc elements from a collection of
elements based on some selection criteria. For such statements,
we collect data about the input-model elements being iterated over
and the selection criterion being applied to each element. The taint
log in Figure 4(b) illustrates the metadata collected at lines 20–
21. The <get> sub-element of the <loop> element (which contains
information about loop taints) records the selection criterion (an
XPath expression) in the <xpath> attribute. The XPath captures
the fact that the type attribute of the <relationship> element is
compared, using the equality operator, to string “wife.”
3Thexpath attribute can capture a general XPath expression ( http://
www.w3.org/TR/xpath20 ). For XSLT-based transforms, this expression
is readily available during the transform execution.
198Figure 4: Illustration of fault repair: (a) for E1, using metadata about attribute access and conditional statements; (b) for E2, using
metadata about selection in loops; and (c) for E3, using metadata and pattern analysis.
3.2 Fault-Index Analysis
The fault-index analysis uses the metadata associated with the
fault index to compute repair actions. Consider the taint-log frag-
ment shown in Figure 4(a), in which the fault index for failure E1 is
highlighted as I1. The fault index is an empty control taint, which
is preceded by a <condition> node. This control taint is propagated
during the execution of line 47 of the transform on the faulty input.
Thus, the empty control taint indicates that the fault caused the con-
ditional in line 47 of PrintUser to evaluate false. The metadata
associated with the <condition> element lets our technique recon-
struct the expected evaluation as leftNode = true . However, the
leftNode attribute is not present in the taint log, suggesting that
the input-model entity that was accessed as leftNode did not exist.
Next, the <get> element preceding the <condition> element pro-
vides information that attribute hasMiddle was accessed on input-
model element 1.4. Putting together all this information, the tech-
nique computes the following repair actions
ADD hasMiddle TO1.4 [r1(E1)]
SET hasMiddle TOtrue [r2(E1)]
Figure 4(b) illustrates the fault index for E2, highlighted by the
ellipse. Similar to E1, the fault index is an empty control taint.
The control taint is preceded by a <next> element whose parent
element is a <loop> element. This indicates that the fault causes
a loop iteration to not occur. The metadata associated with the
<get> sub-element of <loop> captures the criterion that was used
to select the elements to be iterated over. Thus, starting at element
1.4, the transform expected the path family/user/relationship to
exist in the input such that the type attribute of <relationship>
was wife. Moreover, the analysis is able to resolve the access of
<relationship> to ID 1.4.2.2.2(see line 21 in Figure 1(b)); we
discuss the resolution algorithm in Section 3.4 Thus, the fault-index
analysis issues two repair actionsADD type TO1.4.2.2.2 [r1(E2)]
SET type TOwife [r2(E2)]
Part (c) of Figure 4 illustrates the fault index for E3. In this
case, the problem is traced to a missing loop iteration, which occurs
because the selection criterion is not satisﬁed. Following a similar
approach as for E2, the analysis computes four repair actions
ADD user TO1.4.2 [r1(E3)]
ADD relationship TOuser [r2(E3)]
ADD type TOrelationship [r3(E3)]
SET type TOdependent [r4(E3)]
The repair actions computed by the fault-index analysis target
ﬁxing the immediate cause of the failure. However, some faults
can require multiple ﬁxes in the input model. For such faults, the
repair actions computed by fault-index analysis ﬁx the model only
partially. In our example, both E1 and E3 illustrate partial ﬁxes.
For E1, after repair actions r1(E1)and r2(E1)are applied and the
transform is rerun, the desired output (the middle name for user
“Amit Vyas”) is still not generated. Although the repair actions
ﬁxed attribute hasMiddle , attribute middle was still missing in the
ﬁxed model. On this execution, the fault index is the empty data
taint marked I2in Figure 4(a). The fault-index analyzer computes
the following additional repair actions for E1
ADD middle TO1.4 [r3(E1)]
SET middle TO∗ [r4(E1)]
In this manner, the user could iteratively perform fault localiza-
tion and repair and, in each iteration, ﬁx the model partially based
on the repair actions computed by the fault-index analysis, until
the ﬁx is complete. Clearly, such an approach can be cumbersome.
Our pattern-analysis technique, which we discuss next, addresses
this problem by identifying all possible repair actions that may be
required to generate the correct output.
199Figure 5: Intuitive illustration of the pattern analysis per-
formed on the hierarchy of taint marks.
3.3 Pattern Analysis
Pattern analysis attempts to ﬁnd output string fragments that are
similar to the incorrect string fragment. It does this based on the
structure of taint marks in the taint log. Intuitively, the taint marks
in the log have a hierarchical tree-like structure. Figure 5 presents
an illustrative example. Each taint mark in the log is represented
as a node. The edges in the tree represent the nesting among the
taint marks. The sibling nodes are ordered, from left to right, based
on their sequence of occurrence in the log. Thus, each node has
anindex associated with it, which speciﬁes the position of the node
among its siblings. In the ﬁgure, the index of a node is speciﬁed as
a subscript [x]. A loop taint has a control-taint child per iteration
of the loop, and one control-taint child for the last loop condition
evaluation (after which execution exits the loop). In addition to the
three types of taints, nodes also represent string literals, which have
no taints associated with them. A data or a literal node can occur
only as a leaf node.
The fault index of a taint log maps to a taint node in the taint-
log tree. In Figure 5, node 16[1]is the fault index. Intuitively,
starting at the fault index, the pattern analysis traverses up in the
tree to identify the (unique) path to the root, which we refer to as
thefault path . This path is highlighted in the ﬁgure with shaded
nodes. Next, the analysis traverses down in the tree, in a depth-ﬁrst
manner guided by the fault path, to ﬁnd a matching path that is at
least as long as the fault path and that ends at a data node. The fault
path guides the downward traversal in the following manner. If a
loop node is encountered, the traversal explores each child of the
node (except the child that occurs in the fault path). At any non-
loop node, the traversal continues at the child node whose index
matches the index of the corresponding node ( i.e.,the node at the
same depth) in the fault path.
For the fault path (1,2[1],6[3],11[1],16[1]), the algorithm tra-
verses node 2[1]—it does not traverse 3[2]because its index, 2, does
not match the index, 1, of the fault node. Because 2[1]is a loop
node, the analysis explores all its child nodes except for 6[3]be-
cause it occurs on the fault path. The analysis traverses four paths,
which are emphasized with thick lines in the ﬁgure. Of these, the
subpaths in dotted lines illustrate the traversals along which the
analysis ﬁnds no relevant paths for computing repair actions. For
example, path (1,2[1],4[1],9[1])is irrelevant because its length is
less than the length of the fault path. Path (1,2[1],5[2],10[1],14[2])
is irrelevant because it does not end at a data or string literal node;
Path (1,2[1],5[2],10[1],13[1],17[1])is also irrelevant of same rea-
son. The highlighted path (1,2[1],5[2],10[1],15[3],18[1],19[1]), is
the only matching path.
The search for matching paths is guided by the fault path to en-
sure that a similar string is explored by the pattern analysis. The
constraints on the path length ( i.e.,path must be at least as long asalgorithm ComputeRepairActions
input LTaint log
tTaint mark in L
output RFix recommendation for t
begin
1.iftis a data taint then
2. ift’s preceding element e=<next> ∧ ∃e.node then
3. add [ SETe.node TO∗] toR
4. elseﬁnd preceding <get> element e// access of missing entity
5. add [ ADDe.xpath TOe.startNode ] toR
6. add [ SETe.xpath TO∗] toR
7.else if tis a control taint then
8. ift’s preceding element e1=<next> then // missing loop iteration
9. ﬁnd e1’s preceding <get> element e2
10. add [ ADDe2.xpath TOe2.startNode ] toR
11. else if t’s preceding element e1=<condition> then
12. if∃e1.leftNode ∧ ∃e1.rvalue then
13. add [ SETe1.leftNode TO(e1.rvalue | ¬e1.rvalue )] toR
14. else if /negationslash ∃e1.leftNode ∧ ∃e1.rvalue then
15. ﬁnd e1’s preceding <get> element e2
16. add [ ADDe2.xpath TOe2.startNode ] toR
17. add [ SETe2.xpath TO(e1.rvalue | ¬e1.rvalue )] toR
end
Figure 6: The algorithm for computing intermediate repair ac-
tions.
the fault path) and on the last node ( i.e.,the node must be a data
node) ensure that the repair actions computed based on a matching
path will lead to a change in the output text at the error marker. The
repair actions are generated based on the metadata associated with
the taints that are encountered starting at the node where the length
of the matching path is equal to the length of the fault path.
To illustrate the pattern analysis, consider the taint log shown in
Figure 4(c), which is used to compute repair actions for E3. We
illustrate paths using the line numbers in the log. The fault path in
the log is (1, 2, 26, 28, 31, 35). To ﬁnd a similar output instance,
the algorithm traverses the matching path (1, 2, 5, 7, 10, 14, 17). In
the matching path, it skips the ﬁrst six nodes, because the length of
the fault path is six, and then repair actions are computed based on
the metadata information associated with node 17 ( i.e.,at lines 15
and 16). This part of the taint log is highlighted with a bounding
box. The <next> element at line 16 indicates that attribute first of
1.2.2.4 is printed. Generalizing the metadata information to model
entity types, we identify that attribute first is required for model
entity of type user (1.2.2.4is a model entity of type user). There-
fore, the pattern analysis suggests the following additional repair
actions for E3
ADD first TOuser [r5(E3)]
SET first TO∗ [r6(E3)]
Combined with the four repairs actions r1−4(E3)computed by the
fault-index analysis, our approach, thus, computes a recommenda-
tion with six repair actions. In general, for each matching path, an
alternative recommendation is computed.
3.4 Algorithm for Computing Repair Actions
The fault-index and pattern analyses compute repair actions start-
ing at a taint mark t. In the case of fault-index analysis, tis the
fault index, whereas in the case of pattern analysis, tis a taint
encountered during the traversal of a matching path in the taint
tree-structure. Both these analyses leverage a common algorithm
that computes repairs actions for a given taint mark. The algorithm
consists of three steps.
The ﬁrst step computes intermediate repair actions. An interme-
diate repair action can contain an XPath expression. For example,
for E2, the intermediate repair actions corresponding to r1(E2)and
r2(E2)are computed as4
ADD family /user /relationship /@type=wife TO1.4
4The “@” symbol in an XPath expression represents access a node attribute.
200Figure 6 presents the algorithm that, given a taint mark t, com-
putes repair actions for t. For a data taint, the algorithm checks
whether a preceding <next> element with a node attribute exists; if
it does, the created repair action suggests a change in the value of
the named attribute (lines 1–3). If no such element exists, the algo-
rithm looks for a preceding <get> element (line 4), which captures
information about access of a missing entity; in this case, the al-
gorithm computes two repair actions to suggest the addition of the
missing entity with some value (lines 5–6). For a missing loop iter-
ation (lines 8–10), a similar repair action is generated by accessing
information about the loop selection criterion (captured in a <get>
element). For a <condition> element, if leftNode andrvalue ex-
ist, a repair action is generated that sets leftNode to either rvalue
or the negation of rvalue depending on the comparison operator
(lines 12–13). Finally, if leftNode does not exist (indicating a com-
parison with a missing entity) and rvalue exists, appropriate ADD
and SETactions are created (lines 14–17).
The intermediate repair actions can contain complex XPath ex-
pressions, parts of which may already exist in the input model; in
the second step, we reﬁne the intermediate repair actions based
on such existing parts. This step executes each XPath on the in-
put model to identify parts that already exist in the input. For the
non-matching/non-existent XPath parts, we generate repair actions.
To illustrate, consider the intermediate repair action for E2, which
has the complex XPath family/user/relationship/@type . Any
of the direct or indirect sub-elements of 1.4(<family> ,<user> ,
<relationship> ) or the attribute type of<relationship> could be
missing. We execute this XPath on the input model, starting at 1.4.
This returns node 1.4.2.2.2, up to which the match has occured, but
beyond that type is missing. Thus, we generate the repair actions
r1(E2)and r2(E2), which suggest that a type attribute be added to
1.4.2.2.2and its value be set to wife.
In the last step, after all the recommendations have been com-
puted, we prune the recommendation set to ﬁlter out duplicate rec-
ommendations and invalid recommendations. Two recommenda-
tions are equal if they consist of the same set of actions. An invalid
recommendation is one whose application can alter the output prior
to the error marker. The goal of fault repair is to ﬁx the input in such
a way that the output at the error marker changes to the expected
output; all text preceding the error marker is assumed to be cor-
rect. Therefore, we classify a recommendation as invalid if it can
potentially change any output text that precedes the error marker.
To illustrate consider the scenario in which the fault for E2 has
been ﬁxed: i.e.,an appropriate type attribute has been added to
line 21 of the input (Figure 1(b)). In this case, the analysis can rec-
ommend two sets of repair actions for E3. One of them requires the
addition of a new (dependent) <user> sub-element to the <family>
element in line 18 of the input. The second one suggests modifying
the<user> element in line 19 as follows
SET 1.4.2.2.type TOdependent
However, because 1.4.2.2.type is used to generate “Spouse De-
tails,” applying this repair action would change the part of output
that is assumed to be correct. Therefore, this recommendation is
classiﬁed as invalid and ﬁltered out.
3.5 Discussion
Although we have illustrated our approach using XSLT-based
transforms, it is applicable more generally to other transform im-
plementations, e.g.,in Java. For such implementations, the runtime
metadata would need to be collected appropriately to generate the
taint log. For example, collecting the metadata for loop selections,
which is fairly straightforward for XSLT transforms, would require
analysis to identify the code that performs the selection. Similarly,Table 1: Subjects used in the empirical studies.
XSLT Constructs used in the Subjects
Select
Subject
Templates
Loop
Condition
Data
Variables
Simple
Complex
Total
ClsGen1
4
8
14
147
27
125
31
156
ClsGen2
19
16
94
519
103
446
95
541
IntGen1
2
2
5
16
9
16
3
19
IntGen2
6
3
40
101
15
87
20
107
PluginGen
7
9
48
107
52
102
22
122
PropGen
3
0
1
2
0
3
1
4
Total
41
28
202
892
206
779
170
2949
identifying the metadata for conditionals would require code anal-
ysis and appropriate instrumentation. However, the fault-index and
pattern analyses, which operate on the taint log, remain unchanged.
The pattern analysis computes repair actions based on instances
of correct strings that are similar to the incorrect string. It presumes
that all matching paths identiﬁed in the taint tree-structure generate
correct strings. Therefore, if a model contains multiple faults, the
analysis could use an incorrect string to compute repair actions. In
our approach, we assume that all output before the error marker is
correct and, if there are multiple faults, the user identiﬁes the error
markers in order from the beginning of the output. With these as-
sumptions, the pattern analysis could compute repair actions based
on only those strings that precede the error marker; alternatively,
it could rank lower the repair actions that are computed based on
strings that follow the error marker.
4. EMPIRICAL EVALUATION
We implemented our fault-repair approach and conducted em-
pirical studies. The goals of the studies were to investigate (1) the
effectiveness of the fault-index analysis, and (2) the characteristics
of the recommendation sets computed by the pattern analysis. Af-
ter describing the experimental setup, we present the results of the
two studies.
4.1 Experimental Setup
4.1.1 Subjects and Faulty Inputs
We used six XSL transforms, listed in Table 1, as our experimen-
tal subjects. The transforms are real programs that have been de-
veloped in projects at IBM Research. Each transform takes as input
a domain-speciﬁc Ecore EMF model, and generates different types
of text output: ClsGen1 andClsGen2 generate Java classes; IntGen1
andIntGen2 generate Java interfaces; PluginGen generates an XML
conﬁguration ﬁle; and PropGen generates a properties ﬁle consist-
ing of name-value pairs. Table 1 lists different XSLT features used
in the subjects, which indicates the complexity of the subjects.
Columns 2–6 list, respectively, the counts of template (Column 2),
for-each (Column 3), if,when,otherwise (Column 4), value-of
(Column 5), and variable ,param (Column 6). Columns 7–9 present
data about the select construct. We classiﬁed a select as simple
if it contained no condition or axes ( e.g., child:: ,parent:: ), and
complex otherwise. As the data indicate, ClsGen2 is the most com-
plex subject, whereas PropGen is the least complex.
To generate failure-inducing input models, we used the tech-
nique of generating test inputs by applying mutation operators to
a valid input ( e.g., [7]). Speciﬁcally, we deﬁned four mutation op-
erators: (1) DeleteElement deletes a model element, (2) DeleteAt-
tribute deletes a model attribute; (3) EnumerateAttribute modiﬁes
the value of an attribute based on a predeﬁned enumeration of val-
ues for the attribute; and (4) EmptyAttribute sets an attribute to
empty. We used two valid input models that contain approximately
38,100 and 40,500 entities, respectively. We applied the mutation
201Table 2: Faulty inputs generated using the mutation operators.
Delete
Delete
Empty
Enumerate
Subject
Element
Attribute
Attribute
Attribute
Total
ClsGen1
33
94
111
96
334
ClsGen2
190
296
306
307
1099
IntGen1
25
16
29
20
90
IntGen2
162
344
327
347
1180
PluginGen
42
27
29
53
151
PropGen
8
27
22
38
95
Total
460 (16%)
804 (27%)
824 (28%)
861 (29%)
2949
operators to the two valid inputs to generate faulty inputs. Given
the large sizes of our input models, we used an upper limit of 1000
for each input to bound the number of mutants. From the initial set
of mutants, we eliminated equivalent mutants : mutants that caused
no change in the transform output.
Table 2 lists the number of faults of different types that were
generated for the subjects. Overall, the different types of faults are
fairly well-represented; deleted-element faults have a lower repre-
sentation but, nonetheless, are not insigniﬁcant in number. A fair
distribution lowers the bias that predominant numbers of some fault
types might have introduced in our studies.
4.1.2 Implementation
We leveraged and extended the implementation infrastructure,
which had we developed in our previous work on fault localization
for XSLT-based transforms [6]. Our implementation converts an
XSLT program to a Java translet, on which all the analyses oper-
ate. The infrastructure consists of ﬁve main components. (1) A
taint API , which contains taint-initialization and taint-propagation
methods. (2) An instrumentation component , implemented using
BCEL ( http://jakarta.apache.org/bcel ), which adds probes to
the translet bytecode to invoke control-tainting and loop-tainting
method. (3) An aspect-weaver component , implemented using As-
pectJ, which weaves in aspects to the instrumented bytecode to in-
voke taint-initialization and data-tainting methods. (4) An indexer
component , which sanitizes and indexes the raw taint log to make
it appropriate for querying. (5) A differencing component , which
compares the faulty output with the correct output and identiﬁes the
ﬁrst point at which the output differs (speciﬁed as the error marker).
Reference [6] provides the implementation details.
As part of the extensions for this work, we bound additional as-
pects over the translet class, speciﬁcally, at calls to iterator and
compare methods of the Xalan API to extract metadata information.
We implemented the fault-index analyzer and the pattern analyzer
in Java to compute all possible recommendations. We automated
the execution of all the steps using a Ant build script.
4.2 Effectiveness of Fault-Index Analysis
4.2.1 Goals and Methods
The goals of the ﬁrst study were to evaluate the applicability and
effectiveness of the fault-index analysis. We measure applicability
as the percentage of faults for which different types of metadata can
be used for computing repair actions. (In the discussion, we refer
to the entity-access, condition, and select metadata as Me,Mc, and
Ms, respectively.) Thus, applicability indicates the importance of
a particular type of metadata in terms of how often it can be used
in practice. We measure effectiveness as the percentage of faults
for which the repair actions computed by the fault-index analysis
completely ﬁx the faulty model.
To collect the data, we executed each subject on each of its faulty
input model. For each execution, we identiﬁed the error marker in
the incorrect output using a ﬁle-differencing utility, and mappedthe marker to the fault index in the taint log. Next, we used the
fault-index analyzer to compute repair actions. We determined ap-
plicability by identifying the mutation operator and the fault index.
For example, for operators DeleteElement and DeleteAttribute, if
the fault index points to a control taint, MeandMcare applicable;
if the fault index points to a loop taint, MeandMsare applicable;
ﬁnally, if the fault index points to a data taint, Meis applicable.
Thus, metadata applicability depends on both the type of fault and
the structure of the program.
4.2.2 Results and Analysis
Figure 7 presents data about applicability and effectiveness. The
chart on the left shows the applicability data. It contains a seg-
mented bar for each subject. The height of a bar represents 100%
of the faulty inputs for the subject for which we computed repair ac-
tions. Note that these numbers are less than the faulty inputs shown
in column 6 of Table 2. This occurs because for some of the faults,
no repair actions are required; and no meaningful repairs actions
can be computed. For example, consider a fault where the value
of an attribute name is incorrect, and suppose that name is simply
written out by the transform— i.e.,it is not used at a condition or
a select. In such cases, no suggestion can be made on how the in-
correct value of name should be ﬁxed. The fault localization would
point the user to name, and it would be up to the user to decide how
to change the value.
The segments within the bars represent the percentage of faults
for which Me,Mc, and Msare applicable individually. In some
cases, (1) MeandMcare applicable together, and (2) MeandMs
are applicable together; the bar chart contains segments for these
combinations as well. For example, for ClsGen1 , of the total 264
faults, 12 were ﬁxed using the Merecommendation, 51 were ﬁxed
using both MeandMc, 57 were ﬁxed using both MeandMs, 66
were ﬁxed using Mc, and the remaining 78 required Ms.
Overall, the data indicate that Meis applicable in many cases,
typically in combination with McandMs. This conﬁrms our be-
lief that Mecollects valuable metadata about accesses of model
entities. This information is especially useful when the accessed
entity is missing in the model. The percentage of faults for which
Mewas required varied from 45% for ClsGen1 to 51% for IntGen2 .
On average, Mewas required for 46% of the faults. Another obser-
vation we make is that Msis applicable quite frequently as well,
both individually and in conjunction with Me. On average, Msis
applicable for 58.1% of the faults.
Figure 7(b) presents the completeness data. In pretty much all
cases, for the faulty inputs in which elements were missing ( i.e.,
the inputs created using the DeleteElement mutation operator), the
application of repair actions computed by the fault-index analysis
resulted in incomplete ﬁxes. In general, repairs performed based on
the fault-index analysis would be complete for a missing-element
fault only if (2) the element is a leaf element, or (2) none of its
attributes or sub-elements are accessed by the transform. We did
not observe such cases in our subjects. On average, for 81% of the
faults, the metadata-based repairs were complete, whereas for the
remaining 19%, the repairs were incomplete.
4.3 Characteristics of Recommendation Sets
4.3.1 Goals and Methods
The goal of this study was to evaluate our approach in terms of
the number and sizes of the computed recommendations. For each
subject (except ClsGen2 ) and each faulty input for the subject, we
ran the pattern analyzer on the taint logs (with the fault index identi-
ﬁed in it) and computed the sizes of the recommendation sets. One
202Figure 7: Applicability of different types of metadata for computing repair actions using the fault-index analysis (left) and the
completeness of ﬁxes resulting from the application of those repair actions to the faulty inputs (right).
Figure 8: Sizes of recommendations computed using the pat-
tern analysis.
of the steps of our approach is the removal of duplicate recommen-
dations. To assess the beneﬁt of this step, we collected data about
the reduction in the number of recommendations after the removal
of duplicate recommendations.
4.3.2 Results and Analysis
Figure 8 shows the distribution of the sizes of the recommen-
dation sets. The height of the segmented bar for a subject repre-
sents 100% of the faulty inputs for the subject. The segments in a
bar depict different ranges of sizes. For example, for IntGen2 , for
45% of the faulty inputs, our approach computed recommendations
consisting of only one repair action; for 11% of the faulty inputs, it
computed recommendations containing two repair actions; for 26%
of the inputs, the recommendations contained between three and
ﬁve repair actions; and for the remaining 18%, the recommenda-
tions contained between six and nine repair actions. For PropGen ,
all the recommendations contained only two repair actions. For
none of the subjects, except PluginGen , more than 10 repair actions
were computed; the largest recommendation set for PluginGen con-
sisted of 13 repair actions; the average recommendation set size
was two. Thus, for our subjects and faulty inputs, the pattern anal-
ysis computed recommendations that were not very large.
Table 3 presents data about the reduction achieved by the re-
moval of duplicate recommendations. Maximum reduction occurs
when the pruned recommendation set contains only one recommen-
dation; over all subjects, this was achieved for 31% of the faulty
inputs. For another 33% of the faulty inputs, at least 95% reduc-
tion occurred. For PropGen , after the removal of duplicates, all the
recommendation sets contained only one recommendation. On av-
erage, 32% of the recommendation sets contained no duplicates;
therefore, no reduction occurred for these sets. To summarize, the
data indicate that the pattern analysis can often compute duplicate
recommendations and that the removal of duplicates can achieve
signiﬁcant reductions. Thus, the removal of duplicates is an impor-
tant, and effective, step in our approach.Table 3: Reduction in the sizes of the recommendation sets.
Suject
% Recommenation set reduction
100%
95-99.9%
50-94.9%
1-49.9%
0%
ClsGen1
140
56
0
9
129
IntGen1
80
6
2
0
2
IntGen2
299
470
50
0
361
PluginGen
0
75
2
0
74
PropGen
61
0
0
0
34
4.4 Threats to Validity
The most signiﬁcant threats to the validity of our results are
threats to external validity, which arise when the observed results
cannot be generalized to other experimental setups. In our studies,
we used six XSLT-based subjects and, therefore, we can draw lim-
ited conclusions about how our results might hold for transforms
written using other frameworks. However, our transforms vary in
complexity and generate different types of outputs (code, proper-
ties, and conﬁguration ﬁles), which gives us conﬁdence that the re-
sults might apply to other XSLT transforms. For transforms written
using other frameworks, the effectiveness of the approach would
vary depending on the quality of the metadata that is collected.
Another threat to validity is the representativeness of the faulty
input models. We used mutation analysis to generate the faulty
models, which may not represent the types of faults that occur fre-
quently in practice. However, based on our experience with devel-
oping transforms, we designed the mutation operators to capture
commonly occurring faults. This threat is further mitigated by the
fact that different types of faults were well-represented in our set of
faulty models. A related threat is that, in our studies, each faulty in-
put contained a single fault. Further experimentation using inputs
that contain multiple faults is required to provide evidence of the
usefulness of our approach in more general settings.
For this study, we performed the pattern analysis on the taint log
for the faulty input model only—we did not use additional exem-
plar models. Because our input models are large and contain in-
stances of different elements, pattern analysis using only the faulty
input worked well. If the faulty input model was not as comprehen-
sive as the inputs used in our studies, or if exemplar models were
used to perform the pattern analysis, the effectiveness of pattern
analysis might vary. In the extreme case, when no instance of cor-
rect pattern is available, only the fault-index analysis (which can
compute partial ﬁxes) would be applicable.
5. RELATED WORK
Fault localization and repair is an active research area that has
a rich body of work. However, most of the research addresses
the problems of locating and ﬁxing faults in programs. Unlike
this research, the goal of our work is to provide support for re-
pairing faults in large and complex inputs. From the perspective
203of this goal, existing research that is most closely related to ours
is the work on automated repair of model inconsistencies ( e.g.,
[8, 9, 12, 14]) and automated repair of data structures ( e.g.,[5, 10]).
Although the overall goal of this work and our work is similar, the
types of faults targeted and the techniques developed for supporting
fault repair differ signiﬁcantly.
Speciﬁcally, in the context of UML models, existing research
has developed approaches for detecting design inconsistencies in
linked models and suggesting repairs to ﬁx the inconsistencies af-
ter changes are made to the models [8, 9]. In these approaches,
the patterns for design inconsistencies are identiﬁed manually; they
include both generic patterns that apply to all UML models and
model-speciﬁc constraints. The ﬁxes could be speciﬁed statically
along with the pattern, e.g.,by using ﬁrst-order logic formulas [12].
Xiong and colleagues [14] present a language that can be used
for writing both consistency relations and ﬁxing behavior. Thus,
the existing techniques for supporting model repairs focus on only
those faults that can be speciﬁed as consistency relations. Our goal
is not to ﬁx statically identiﬁable inconsistencies, but to ﬁx condi-
tions in the input that do not violate consistency constraints, and
yet cause a transformation failure.
Existing research in data-structure repair attempts to ﬁx incon-
sistencies at runtime to prevent unexpected program termination.
Demsky and Rinard [5] present a speciﬁcation-based approach for
dynamic detection and repair of data-structure inconsistencies. Elka-
rablieh and colleagues [10] leverage program assertions to identify
consistency constraints expected on the input data structures. Their
approach uses static analysis and symbolic execution to identify the
repair actions at the assertion point. Unlike this research, the goal
of our work is to assist the user in ﬁxing faults in input models.
Moreover, our work focuses on supporting fault repair after a fail-
ure is observed—its goal is not to detect and ﬁx faults on-the-ﬂy
during transform execution.
Other techniques for providing automated support for bug ﬁxes
focus on faults in programs. Kim and colleagues [11] present an
approach for building a catalog of project-speciﬁc bug patterns and
bug-ﬁx patterns. By analyzing the code changes made to ﬁx bugs,
their approach creates a normalized representation of a bug (code
that is deleted in a bug ﬁx) and the bug ﬁx (code that is added in
a bug ﬁx). Based on pattern matching on such a catalog, their ap-
proach can suggest ﬁxes. DebugAdvisor [1] and Hipikat [4] create
a project memory by mining relationships between various soft-
ware artifacts present in different project repositories. For any new
bugs, users can query these project memories to understand how
similar bugs have been ﬁxed in the past.
6. SUMMARY AND FUTURE WORK
We presented an approach for assisting transform users in ﬁx-
ing faulty input models. We deﬁned different types of information
about the failing transformation that are essential for computing
accurate repair actions; we presented two techniques that analyze
the metadata and compute repair actions. Finally, we demonstrated
the feasibility of our approach by implementing it for XSLT trans-
forms.
There are several directions for future work. Although the meta-
data that our approach requires are generic in nature, the ease with
which they can be collected can vary with different transform im-
plementations. Our current implementation has explored the tech-
nique for XSLT transforms. Future work could investigate how
such metadata can be collected for transforms written using other
frameworks or general-purpose languages, such as Java.
To suggest complete ﬁxes for an incorrect output, our approach
performs pattern analysis to identify correct output string fragmentsthat are similar to the incorrect or missing output string fragment.
To be successful, pattern analysis requires samples of correct out-
put to be available. When such samples are unavailable, which can
occur in practical scenarios, pattern analysis would not be applica-
ble. Moreover, a sample input model may not exercise all paths in
the transform to generate a particular output. Therefore, the alter-
native sets of recommendations we compute may be incomplete.
To overcome these limitations, predicate switching [15] may be a
promising technique, which we plan to investigate.
Finally, user studies are required to evaluate how it can help users
locate and ﬁx faults efﬁciently and effectively.
References
[1] B. Ashok, J. Joy, H. Liang, S. K. Rajamani, G. Srinivasa, and V . Van-
gala. Debugadvisior: A recommender system for debugging. In Proc.
of the 7th joint meeting of the European Softw. Eng. Conf. and the
ACM SIGSOFT Symp. on the Foundations of Softw. Eng. , pages 373–
382, August 2009.
[2] I. D. Baxter. Design maintenance systems. Communications of the
ACM , 35(4):73–89, April 1992.
[3] J. Clause, W. Li, and A. Orso. Dytan: A generic dynamic taint analysis
framework. In Proc. of the Intl. Symp. on Softw. Testing and Analysis ,
pages 196–206, July 2007.
[4] D. ˇCubrani ´c, G. C. Murphy, J. Singer, and K. S. Booth. Hipikat:
A project memory for software development. IEEE Transactions on
Software Engineering , 31(6):446–465, June 2005.
[5] B. Demsky and M. Rinard. Automatic detection and repair of errors in
data structures. In Proc. of the 18th ACM SIGPLAN Conf. on Object-
Oriented Prog. Syst. Lang. and Applications , pages 78–95, October
2003.
[6] P. Dhoolia, S. Mani, V . S. Sinha, and S. Sinha. Debugging model-
transformation failures using dynamic tainting. In Proc. of the 24th
European Conf. on Object-Oriented Prog. , pages 26–51, June 2010.
[7] T. Dinh-Trong, S. Ghosh, R. France, B. Baudry, and F. Fleury. A
taxonomy of faults for UML models. In Proc. of the 2nd Workshop on
Model Design and Validation , October 2005.
[8] A. Egyed. Fixing inconsistencies in UML design models. In Proc. of
the 29th Intl. Conf. on Softw. Eng. , pages 292–301, May 2007.
[9] A. Egyed, E. Letier, and A. Finkelstein. Generating and evaluating
choices for ﬁxing inconsistencies in UML design models. In Proc. of
the 23rd Intl. Conf. on Automated Softw. Eng. , pages 99–108, Septem-
ber 2008.
[10] B. Elkarablieh, I. Garcia, Y . L. Suen, and S. Khurshid. Assertion-
based repair of complex data structures. In Proc. of the 22nd Intl.
Conf. on Automated Softw. Eng. , pages 64–73, November 2007.
[11] S. Kim, K. Pan, and E. J. Whitehead, Jr. Memories of bug ﬁxes. In
Proc. of the 14th ACM SIGSOFT Symp. on the Foundations of Softw.
Eng., pages 35–45, November 2006.
[12] C. Nentwich, W. Emmerich, and A. Finkelstein. Consistency man-
agement with repair actions. In Proc. of the 25th Intl. Conf. on Softw.
Eng., pages 455–464, May 2003.
[13] D. C. Schmidt. Model-driven engineering. IEEE Computer , 39(2):
25–31, February 2006.
[14] Y . Xiong, Z. Hu, H. Zhao, H. Song, M. Takeichi, and H. Mei. Sup-
porting automatic model inconsistency ﬁxing. In Proc. of the the 7th
joint meeting of the European Softw. Eng. Conf. and the ACM SIG-
SOFT Symp. on the Foundations of Softw. Eng. , pages 315–324, Au-
gust 2009.
[15] X. Zhang, N. Gupta, and R. Gupta. Locating faults through automated
predicate switching. In Proc. of the 28th Intl. Conf. on Softw. Eng. ,
pages 272–281, May 2006.
204