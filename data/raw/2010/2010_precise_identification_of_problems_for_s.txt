Precise Identiﬁcation of Problems for
Structural Test Generation
Xusheng Xiao1Tao Xie1Nikolai Tillmann2Jonathan de Halleux2
1Dept. of Computer Science, North Carolina State University , Raleigh, NC
2Microsoft Research, One Microsoft Way, Redmond, WA
1{xxiao2,txie}@ncsu.edu ,2{nikolait,jhalleux}@microsoft.com
ABSTRACT
An important goal of software testing is to achieve at least
high structural coverage. To reduce the manual eﬀorts of
producing such high-covering test inputs, testers or devel -
opers can employ tools built based on automated structural
test-generation approaches. Although these tools can easi ly
achieve high structural coverage for simple programs, when
they are applied on complex programs in practice, these
tools face variousproblems, suchas(1)theexternal-metho d-
call problem (EMCP), where tools cannot deal with method
calls to external libraries; (2) the object-creation probl em
(OCP), where tools fails to generate method-call sequences
to produce desirable object states. Since these tools cur-
rently could not be powerful enough to deal with these prob-
lems in testingcomplex programs in practice, we propose co-
operative developer testing, where developers provide gui d-
ance to help tools achieve higher structural coverage. To re -
duce the eﬀorts of developers in providing guidance to tools ,
in this paper, we propose a novel approach, called Covana,
which precisely identiﬁes and reports problems that preven t
the tools from achieving high structural coverage primaril y
by determining whether branch statements containing not-
covered branches have data dependencies on problem can-
didates. We provide two techniques to instantiate Covana
to identify EMCPs and OCPs. Finally, we conduct evalua-
tions on two open source projects to show the eﬀectiveness
of Covana in identifying EMCPs and OCPs.
Categories and Subject Descriptors
D.2.5 [Software Engineering ]: Testing and Debugging
General Terms
Measurement, Reliability
Keywords
Structuraltestgeneration, dynamicsymbolicexecution,d ata
dependency, problem identiﬁcation
Permission to make digital or hard copies of all or part of thi s work for
personal or classroom use is granted without fee provided th at copies are
not made or distributed for proﬁt or commercial advantage an d that copies
bear this notice and the full citation on the ﬁrst page. To cop y otherwise, to
republish, to post on servers or to redistribute to lists, re quires prior speciﬁc
permission and/or a fee.
ICSE ’11, May 21–28, 2011, Honolulu, Hawaii, USA
Copyright 2011 ACM 978-1-4503-0445-0/11/05 ...$10.00.1. INTRODUCTION
Software testing is by far the most widely used technique
for improvingsoftware reliability. An important goal of so ft-
ware testing is to achieve full or at least high structural
coverage, such as statement or block coverage and branch
coverage of the program under test. However, manually
producing test inputs for achieving high structural covera ge
is labor-intensive. To address the issue, testers or devel-
opers can employ tools built based on state-of-the-art au-
tomated structural test-generation approaches to automat i-
cally generate test inputs, such as Dynamic Symbolic Exe-
cution (DSE) [3,7] (also called concolic testing [7]).
Althoughthese automated test-generation tools can easily
achieve high structural coverage for simple programs, thes e
tools face challenges ingeneratingtest inputstoachieve h igh
structural coverage when they are applied on complex pro-
grams in practice. To better understand how automated
test-generation tools perform for complex programs, we car -
ried out a preliminary study of applying Pex [10], a DSE
tool, on four popular open source projects, which have high
download counts (study details are described in Section 2).
The results show that the total block coverage achieved is
49.87%, with the lowest coverage being 15.54%. Among the
problems that we empirically obervered, many statements
or branches are not covered due to two major types of prob-
lems: (1) the external-method-call problem (EMCP), where
method calls to external libraries1throw exceptions to abort
test executions, or their returnvalues are used todecide su b-
sequent branches, causing the branches not to be covered;
(2) the object-creation problem (OCP), where tools fail to
generate sequences of method calls to construct desired ob-
ject states for non-primitive method arguments or receiver
objects to cover certain branches.
Since these automated tools could not be powerful enough
to deal with various complicated situations in real-world
codebasesautomaticallywithouthumaninterventionorgui d-
ance, we propose a new methodology of cooperative devel-
oper testing, where tools and developers cooperate to ef-
fectively carry out software testing as follows. Automated
test-generation tools are ﬁrst applied to generate test in-
puts and achieve coverage without human guidance. After
the tools reach the pre-deﬁned limits of resource consump-
tion, the tools stop and report the achieved coverage and the
problemsthatpreventthemfromachievinghigherstructura l
coverage back to developers, such as which external-method
1External libraries include native system libraries, such a s
ﬁle system andnetwork socket libraries, andthird-partypr e-
compiled libraries, where source code is not available.call causes branches not to be covered or the state of which
object is required to cover certain branches. By looking int o
the reported problems, the developers provide correspond-
ing guidance to help the tools address the problems. For
example, to deal with OCPs, developers can specify factory
classes [10] that encode desired method sequences for non-
primitive object types. To deal with EMCPs, developers can
instruct the tools to instrument the external-method calls
or provide mock objects [12] to simulate irrelevant environ -
ment dependencies. With the provided guidance, the tools
are reapplied to generate test inputs for achieving higher
structural coverage.
To achieve this cooperative developer testing, the tools
need to report the encountered problems and narrow down
the investigation scope, thus reducing the required eﬀorts
from the developers. Given the generated test inputs from
tools and the achieved coverage, it is not diﬃcult to iden-
tify the problem candidates for the developers to analyze.
For example, locating all the external-method calls in the
program under test can be easily achieved by static or dy-
namic program analysis, and reporting the object types of
the program inputs and all their ﬁelds to the developers is
fairly easy as well. However, the number of such problem
candidates could be high, and quite some of these problem
candidates (referred to as irrelevant problem candidates)
are not causes for the tools not to achieve higher struc-
tural coverage. For example, the external-method call Con-
sole.WriteLine onlyprintstheargumentstringvalue. There-
fore, instrumenting or mocking this method call cannot re-
sult in any increase in coverage. Similarly, some branches
may require only the speciﬁc object state of a ﬁeld of the
program inputs, and thus there is no need to spend eﬀorts
in providing sequences of method calls for all the ﬁelds of
the program inputs.
Since the number of problem candidates could be large,
the tools need to prune irrelevant problem candidates for
reducing the eﬀorts of developers in terms of investigation
scope. Simple pruning techniques, such as pruning external -
methodcalls bymethodnames orbelonginglibraries, canre-
sult in high false positives andfalse negatives. In our prel im-
inary study, we observed that if branch statements are data
dependent on external-method calls for their return values
(i.e., return values are used to decide which branches of the
statements totake), thebranchesofthese branchstatement s
are verylikelynotcoveredbythegenerated testinputs,sin ce
automatedtest-generationtoolsnormallycannotinstrume nt
or analyze the external-method calls. Hence, we can use
compute data dependencies of branch statements contain-
ing not-covered branches (referred to as partially-covere d
branch statements) on external-method calls for their re-
turn values, and use the computed data dependencies to
eﬀectively identify such external-method calls and prune i r-
relevant ones. Similarly, we can compute data dependencies
of partially-covered branch statements on program inputs
and their ﬁelds, and use the computed data dependencies
to help identify which ﬁelds of the program inputs require
desired object states to cover certain not-covered branche s.
To address the need of precisely identifying problems for
developers to provide guidance, in this paper, we propose
a novel approach called Covana that precisely identiﬁes the
problems that prevent the tools from achieving high struc-
tural coverage and prunes the irrelevant problem candidate s
usingthedatadependenciesofpartially-coveredbranchst ate-mentson problemcandidates. Covanaconsists ofthreemain
steps: (1) identify problem candidates based on the types of
problems, (2)assign symbolic values toelements ofthe prob -
lem candidates (including return values of external-metho d
calls or program inputs as well as their ﬁelds) and perform
forward symbolic execution [10] using test inputs generate d
by the tools as program inputs, (3) compute data depen-
dencies of partially-covered branch statements on program
candidates, and prune the candidates that none of partially -
coveredbranchstatementshavedatadependencieson. Since
EMCPs and OCPs are the two major types of the problems
observed in our preliminary study, we provide two speciﬁc
techniques to instantiate Covana for identifying these two
types of problems.
To show the eﬀectiveness of Covana, in this paper, we
use Dynamic Symbolic Execution (DSE) [3,7,10] as an il-
lustrative example of automated structural-test-generat ion
approaches. The primary reason why we choose DSE is
that DSE is the most recent state-of-the-art in test genera-
tion. We concretize Covana as an extensible framework that
collects information from DSE to identify diﬀerent types of
problem candidatesandperform forward symbolic execution
to compute data dependencies of partially-covered branch
statements on problem candidates.
This paper makes the following major contributions:
•The ﬁrst attempt to precisely identify problems faced
bytoolsbuiltforstructural-test-generationapproaches ,
achievingtheﬁrst stepofcooperative developertesting
between tools and developers.
•A novel approach, called Covana, that identiﬁes diﬀer-
ent types of problem candidates and prunes irrelevant
candidates by computing data dependencies using for-
ward symbolic execution.
•A concretization of Covana as a framework to identify
problems that prevent DSE from achieving high struc-
tural coverage. Two analysis techniques are provided
to instantiate Covana for precisely identifying EMCPs
and OCPs.
•TwoevaluationsofCovanaontwoopensourceprojects,
including xUnit [14] and QuickGraph [6]. The results
show that Covana eﬀectively identiﬁes 43 EMCPs out
of 1610 EMCP candidates with only 1 false positive
and 2 false negative, and 155 OCPs out of 451 OCP
candidates with 20 false positives and 30 false nega-
tives.
2. PRELIMINARY STUDY
In this section, we discuss the diﬀerent types of problems
that we empirically observed by applying a state-of-the-ar t
DSE tool, Pex [10], on four open source projects for achiev-
ingstructuralcoverage. Theanalysisoftheseproblemshel ps
motivate our approach. We choose Pex as the DSE tool in
our empirical study and later we implement our approach
upon it for two reasons: (1) Pex can explore all public meth-
ods of any real-world .NET code bases and generate test
inputs automatically; (2) Pex has been applied internally
in Microsoft to test core components of the .NET runtime
infrastructure and found serious defects [10].
We apply Pex on the core libraries of the four open source
projects until all the methods have been explored by Pex orProject LOC Cov % OCP EMCP Boundary Limitation
SvnBridge 17.1K 56.26 11 (42.31%) 15 (57.69%) 0 (0%) 0 (0%)
xUnit 11.4K 15.54 8 (72.73%) 3 (27.27%) 0 (0%) 0 (0%)
Math.Net 3.5K 62.84 17 (70.83%) 1 (4.17%) 4 (16.67%) 2 (8.33%)
QuickGraph 8.3K 53.21 10 (100%) 0 (0%) 0 (0%) 0 (0%)
Total 40.3K 49.87 46 (64.79%) 19 (26.76%) 4 (5.63%) 2 (2.82%)
Table 1: Main problems for not-covered branches in 10 ﬁles fr om core libraries of four open source projects
Pexrunsoutofmemoryandcannotcontinuetogeneratetest
inputs. These four open sources projects are SvnBridge [8],
xUnit [14], Math.NET [4], and QuickGraph [6], which are
quite popular and have high download counts. After Pex
generates test inputs and produces coverage ﬁles, we select
10 source ﬁles that achieve low coverage in each project,
and manually investigate the problems that contribute to
the not-covered statements and branches. The details of the
subjects and results can be found in our project web2.
Table 1 shows the distribution of the problems that pre-
vent DSE from achieving high structural coverage. Column
“Project” lists the name of each project, Column “LOC”
shows the number of lines of codes for each project, and
Column“Cov %”shows the block coverage achieved by Pex.
The other four columns give the number and the percent-
age of the not-covered branches caused by diﬀerent types of
problems.
The top major type of problems is object-creation prob-
lems (64.79%), shown in Column “OCP”, since desired ob-
ject states cannot be generated. In unit testing of object-
oriented code, achieving high structural coverage require s
desired object states for the receiver or non-primitive arg u-
ments of the method under test (MUT). These desired ob-
ject states help cover various branches. However, automate d
approaches are often ineﬀective in generating method-call
sequences that produce desired object states to achieve hig h
structural coverage [9], facing object-creation problems that
prevent DSE from achieving high structural coverage.
The second major type of problems is external-method-
call problems (26.76%), shown in Column “EMCP”, since
external-method calls cause to lose track of computed sym-
bolic valuespassedastheirargumentsor throwexceptionst o
hinder the exploration. In our study, we encountered many
external-method calls, 405 in 40 ﬁles, but only 4.7% (19 in
405) are causes for DSE not to achieve high structural cover-
age. If we simply report every encountered external-method
call as an EMCP, we can get many irrelevant problems that
are not cause for any not-covered statment or branch.
The third main type of problems is boundary problems
(5.63%), shown in Column “Boundary”, mostly caused by
loops in the program under test. Some programs under test
have loops whose number of iterations depends on symbolic
values, and DSE keeps increasing the number of iterations
of the loops during path exploration, preventing DSE from
exploring other paths in the remaining parts of the program.
The last main type of problems is limitations of the used
constraint solver (2.82%), shown in Column “Limitation”,
since the used constraint solver cannot compute exact so-
lutions to ﬂoating-point arithmetics. The reason why we
did not have so many not-covered branches due to this type
of problems is that the used constraint solver generates ap-
proximateintegersforconstraintsthatcontainﬂoating-p oint
arithmetics andtheseapproximateintegers cancover certa in
branches.
2http://research.csc.ncsu.edu/ase/projects/covana/static string GetDefaultConfigFile(string assembly-
File) {
00: string configFilename = assemblyFile + ".config";
01: if (File.Exists(configFilename))
02: return configFilename;
03: return null;
04: }
...
public ExecutorWrapper(string assemblyFilename, ...) {
05: ...
06: assemblyFilename = Path.GetFullPath(assemblyFilena me);
07: ...
}
public AssertActualExpectedException
(object expected, object actual, ...) {
08: ...
09: this.actual += String.Format("(0)",
actual.GetType().FullName);
10: this.expected += String.Format("(0)",
expected.GetType().FullName);
11: ...
}
Figure 1: Three simpliﬁed methods from xUnit [14].
3. EXAMPLE
We next explain how Covana, instantiated with two spe-
ciﬁc techniques, identiﬁes EMCPs and OCPs with two illus-
trative examples.
3.1 External-Method-Call Problem (EMCP)
During the execution of the automatically generated test
inputs, external-method calls may prevent the generated
test inputs from achieving high structural coverage if the
return values of external-method calls are used to decide
subsequent branches to take or throw exceptions to termi-
nate test executions. As a real example, the return value of
File.Exists in Figure 1 is used in deciding which branch at
Line 1 to take. If the generated test inputs do not contain a
ﬁle name that exists in the test environment, being mostly
the case, the statement at Line 2 cannot be covered by the
test inputs. The method Path.GetFullPath at Line 6 is an-
otherexampleexternal-methodcall thatpreventstestinpu ts
from achieving higher structural coverage. Path.GetFullPath
throwsexceptionswhenaninvalidofanassemblyﬁleisgiven
as the argument. Therefore, if none of the generated in-
puts includes a valid name, the lines after Line 6 remain
not-covered. However, not all the external-method calls ca n
cause problems for achieving high structural coverage. For
instance, String.Format at Line 9 and 10 in Figure 1 only
formats the string value of the input and does not aﬀect the
coverage achieved by the generated test inputs.
Covana ﬁrst identiﬁes as problem candidates the external-
method calls whose arguments have data dependencies on
program inputs. In Figure 1, Covana identiﬁes File.Exists
at Line 1, Path.GetFullPath at Line 6, and String.Format at
Lines 9 and 10 as candidates, since they all have data depen-
dencies on the program inputs. By assigning symbolic val-
ues to return values of the candidates and applying forward
symbolic execution [3,7], Covana collects symbolic expres -public class FixedSizeStack {
00: private Stack stack;
01: public FixedSizeStack(Stack stack) {
02: this.stack = stack;
03: }
04: public void Push(object item) {
05: if(stack.Count() == 10) {
06: throw new Exception("full");
07: }
08: stack.Push(item);
09: }
10: ...
}
11: public void TestPush(FixedSizeStack stack,
object item){
12: stack.Push(item);
13: }
Figure 2: FixedSizeStack implemented using Stack
sions in the predicates of branch statements. From the sym-
bolic expressions collected from branch statements, Covan a
extracts elements of problem candidates and considers the
branch statements that have data dependencies on problem
candidates. If one of the branches at Line 1 is not covered
(i.e., Line 1 is a partially-covered branch statement), Cov -
ana identiﬁes FileExists as an EMCP. On the other hand,
there are no partially-covered branch statements that are
data dependent on the return values of String.Format at
Lines 9 and 10, causing String.Format at Lines 9 and 10
to be pruned. For Path.GetFullPath , if all of its executions
throw exceptions, the remaining part of the program, start-
ing at Line 7, remains not-covered. Covana detects the ex-
ceptions that cause to abort the test executions and iden-
tiﬁesPath.GetFullPath as an EMCP that causes the area
starting at Line 7 not to be covered.
3.2 Object-Creation Problem (OCP)
Figure2showsaclass FixedSizeStack thathasaﬁeld stack
of typeStack. Invoking the method Pushto push objects is
required for increasing the size. Stackhas a ﬁeld itemsthat
stores the pushed objects, and the method stack.Count()
returns the number of objects stored in the Stack.items3.
FixedSizeStack has an upper bound of the number of ob-
jects that can be pushed into the stack. To bound the size,
the method FixedSizeStack.Push throws an exception when
the size of the stack has reached the bound (10 in the exam-
ple). The method TestPush receives a FixedSizeStack object
and an object to be pushed as its arguments and invokes
the method FixedSizeStack.Push to push the object to the
stack for testing. To cover the true branch at Line 5 of the
method FixedSizeStack.Push , the generated test inputs need
to include method-call sequences to create a full FixedSizeS-
tackwhose size is 10.
Since the ﬁeld FixedSizeStack.stack can be assigned di-
rectly by invoking the constructor of FixedSizeStack and
passing an object of Stackas an argument (i.e., the ﬁeld
FixedSizeStack.stack isassignable for the declaring class
FixedSizeStack ), the diﬃculty of generating an object state
ofFixedSizeStack whosesizeis10liesingeneratinganobject
ofStackwhosesize is10. Letusassumethatautomatedtest-
generation tools cannot produce the required object state o f
Stack.
Covana ﬁrst assigns symbolic values to program inputs
and their ﬁelds, i.e., FixedSizeStack ,FixedSizeStack.stack ,
3Assume that Stack.items is implemented using the object
typeList<object>
Figure 3: Overview of Covana
andStack.items , and performs forward symbolic execution
to compute data dependencies. By computing data depen-
dencies of partially-covered branch statements, Covana ﬁg -
ures out that the branch statement at Line 5 (with the true
branch not-covered) has data dependencies on Stack.items .
However, reporting the object type of Stack.items , being
List<object> , results in a false warning. Since by providing
method-call sequences for List<object> , the tools cannot as-
sign it totheﬁeld Stack.items sinceStack.items is assignable
forStack.
Based onthisobservation, Covanaconstructsaﬁelddecla-
ration hierarchyfrom theﬁeldthatthebranchstatementhas
data dependencies on up to the program input and identiﬁes
the declaring class whose ﬁeld is not assignable as the cause
of the OCP. In this example, the constructed hierarchy is
FixedSizeStack ,FixedSizeStack.stack , andStack.items . Co-
vana analyzes the ﬁeld declaration hierarchy starting from
the program input. By analyzing FixedSizeStack andFixed-
SizeStack.stack , Covana knows that FixedSizeStack.stack
is assignable for FixedSizeStack . Then Covana continues to
checkFixedSizeStack.stack andStack.items . Since the ﬁeld
Stack.items can be changed only by invoking the method
Stack.Push (i.e., not assignable for Stack), Covana identiﬁes
the object type StackofFixedSizeStack.stack as an OCP
that causes the true branch at Line 5 not to be covered.
4. APPROACH
In this section, we describe how Covana identiﬁes problem
candidates of structural test generation and prunes irrele -
vant problem candidates by computing data dependencies.
Covana consists of three main steps: Problem-Candidate
Identiﬁcation, Forward Symbolic Execution, and Data De-
pendence Analysis. In the following part of the section, we
introduces the overview of Covana and describe these three
main steps in detail.
4.1 Overview of Covana
Inthispaper, weconcretizeCovanaasanextensibleframe-
work for identifyingproblemsthatpreventDSEfrom achiev-
ing high structural coverage. DSE [3,7] executes the pro-
gram symbolically, starting with arbitrary inputs. Along
the execution path, DSE collects symbolic constraints on
program inputs in branch nodes (being runtime instances of
branch statements) to form an expression, called the path
condition. To obtain a new path that takes a diﬀerent
branch, one of the branch nodes in the path condition is
negated to create a new path condition that shares the pre-ﬁx up till the node being negated with the original path.
Then a constraint solver is used to compute test inputs that
satisfy the new path condition. These generated test inputs
again are executed on the program to explore diﬀerent paths
of the program. Ideally, all feasible paths can be exercised
eventually through such iterations of path variations. How -
ever, as we discussed in the introduction, various problems
cause DSE not to achieve high structural coverage.
Figure 3 shows a high-level overview of Covana. Covana
accepts as inputaprogram undertestor Parameterized Unit
Test (PUT) [11], and generated test inputs from automated
test-generation tools (such as a DSE-based tool). Covana
then leverages the DSE engine to perform forward symbolic
execution on the program or PUT using the test inputs as
program inputs (program inputs are assigned with symbolic
values). During execution, Covana monitors runtime events
triggered by the DSE engine for identifying diﬀerent types
of problem candidates. After identifying problem candi-
dates, Covana assigns symbolic values to elements of these
problem candidates, performs forward symbolic execution
on these symbolic values, and collects runtime information ,
such as symbolic expressions and exceptions. Covana then
uses the collected structural coverage and runtime informa -
tion to compute the data dependencies of partially-covered
branch statements on problem candidates, and prunes ir-
relevant problem candidates that none of partially-covere d
branch statements have data dependencies on. In our cur-
rent prototype, we instantiate this general approach with
two techniques to identify the top two main types of prob-
lems: EMCPs and OCPs. We next discuss each step of
Covana in detail.
4.2 Problem-Candidate Identiﬁcation
Covana collects necessary information from the DSE en-
gine for identifying diﬀerent types of problem candidates.
The DSE engine executes the program under test or PUT
with the generated test inputs symbolically. During execu-
tion, Covana monitors diﬀerent events triggered by the DSE
engine and exposes these events as interfaces for specifyin g
diﬀerent types of problem candidates. There are many kinds
of events that can be monitored, such as events of method
entry and method exit. We next discuss how these events
can be used to identify the problem candidates of EMCPs
and OCPs.
4.2.1 Identifying EMCP Candidates
A method exit event is triggered by DSE engine when the
execution ofamethod call is ﬁnished. This eventcomes with
detailed method information, including method arguments,
method instrumentation information, and so on.
IfthemethodisnotinstrumentedbyDSE,themethodcall
is considered as an external-method call, method calls to ei -
ther system libraries or third-partypre-compiledlibrari es. If
Covanaconsidersallexternal-methodcalls asproblemcand i-
dates of EMCP, then the number of problem candidates can
be very large for complex programs. Hence, Covana con-
siders as candidates only the external-method calls whose
arguments have data dependencies on program inputs. In
this way, the external-method calls that have constant ar-
guments are not considered as problem candidates and are
pruned without computing data dependencies. Normally,
such external-method calls are method calls that print con-
stant strings or puta threadtosleep for some time, which donot cause DSE not to achieve higher structural coverage and
can be safely pruned. Since DSE typically assigns symbolic
values to program inputs, to know whether method argu-
ments have data dependencies on program inputs can be
achieved easily by checking whether the method arguments
contain symbolic expressions of program inputs.
In our preliminary study described in Section 2, we ob-
served that many branches are not covered since the con-
ditions of these branches use the return values of external-
method calls (i.e., data dependent on these external-metho d
calls). The reason is that DSE tools and other automated
test-generation tools are unlikely to generate diﬀerent te st
inputs to cause external-method calls to return desired val -
ues since these tools have not instrumented or analyzed
external-method calls. Therefore, for the external-metho d
calls whose arguments have data dependencies on program
inputs, Covana considers them as EMCP candidates. To il-
lustrate the analysis, we use the example shown in Figure 1.
During the test execution of the method GetDefaultConfig-
File, Covana identiﬁes the external-method call File.Exists
as an EMCP candidate, since its argument configFilename
has data dependency with assemblyFile , which is the pro-
gram input. The return value of File.Exists , which is used
in the branch statement at Line 1, is assigned with a sym-
bolic value for computing data dependencies.
4.2.2 Identifying OCP Candidates
Whenever test inputs are used as the arguments to exe-
cute the program under test, the method entry event of the
method under test is triggered. In this exposed event, the
details of the generated program inputs are collected. Sinc e
OCP requires objects of a non-primitive type as program in-
puts, Covanaignores program inputswhose typeis primitive
type, such as int,double, andboolean. Covana considers the
program inputs of non-primitive types themselves and their
ﬁelds of non-primitive types as OCP candidates.
4.3 Forward Symbolic Execution
Covana performs forward symbolic execution using the
test inputs generated by automated test-generation tools a s
program inputs, and collects runtime information for com-
puting data dependencies. Covana assigns symbolic values
to elements of the identiﬁed problem candidates (such as re-
turn values of external-method calls) and leverages the DSE
engine to perform forward symbolic execution for collect-
ing constraints on elements of problem candidates in branch
statements. We next discuss how Covana uses this run-
time information to compute data dependencies of partially -
covered branch statements on problem candidates.
4.3.1 Collecting Symbolic Expressions in Branches
Since elements of problem candidates are assigned with
symbolic values, if a branch statement has data dependency
on problem candidates, we can ﬁnd symbolic expressions
(on elements of the problem candidates) in the predicates
of the branch statement. Such information is later used to
compute data dependencies on problem candidates.
4.3.2 Collecting Uncaught Exception
After assigning symbolic values to elements of problem
candidates, Covanamonitors theprogram execution. When-
ever an uncaught exception is thrown, Covana collects the
exception including its stack trace of the exception. As weobserved in the preliminary study, if an external-method
call throws an exception for the executions of all the gener-
ated test inputs, the remaining parts of the program after
the call site of the external-method call cannot be covered.
Thus, Covana uses the stack trace of an exception thrown
at runtime for the analysis of EMCP described in Section
4.4.1.
4.4 Data Dependence Analysis
Covana consumes the collected runtime information from
the forward symbolic execution to compute data dependen-
cies. For each collected symbolic expression symfound in
the predicates of a branch statement b, Covana extracts el-
ements of the problem candidates elemfromsym. From
elem, Covana extracts the corresponding problem candi-
datesPand considers bhas data dependency on P. Using
the collected structural coverage, Covana further compute s
data dependencies of partially-covered branch statement o n
problem candidates. With these data dependencies, diﬀer-
ent analyses further prune irrelevant problem candidates.
4.4.1 EMCP Analysis
Covana ﬁrst identiﬁes EMCP using the data dependen-
cies of partially-covered branch statements on EMCP can-
didates. If these exist some partially-covered branch stat e-
ments that have data dependencies on EMCP candidates for
their return values, Covana directly reports such external -
method calls as EMCPs. To identify external-method calls
that throw exceptions to abort test executions, Covana fur-
theranalyzesthemethodcallsfromthecollectedstacktrac es
of exceptions thrown during runtime. If these method calls
contain any external-method call and the remaining parts of
the program after the call site of the external-method call
are not covered, Covana identiﬁes the extracted external-
method call as an EMCP that causes the remaining parts of
the program not to be covered.
Algorithm 1 Object Creation Problem (OCP) Analysis
Require: Fieldsfor ﬁeld declaration hierarchy, Bfor not-
covered branches
Ensure: OCP
1:ifLength(Fields) == 1then
2:OCP=CreateOCP (TypeOf(Fields[0]),B)
3:return OCP
4:else
5: Set current =NULL
6:fori= 1toLength(Fields)−1do
7:current =Fields[i]
8:dc=TypeOf(Fields[i−1])
9:assg=IsAssignable (current,dc )
10: if!assgthen
11: OCP=CreateOCP (dc,B)
12: return OCP
13: end if
14:end for
15:OCP=CreateOCP (TypeOf(current),B)
16:return OCP
17:end if
4.4.2 OCP Analysis
Covana identiﬁes OCPs using the data dependencies of
partially-covered branch statements on program inputs andtheir ﬁelds. If a partially-covered branch statement is dat a
dependent on only program inputs, Covana directly reports
theprogram inputsas OCPs. However, ifapartially-covered
branch statement is data dependent on ﬁelds of program in-
puts, Covana constructs a ﬁeld declaration hierarchy up to
a program input and performs further analysis to identify
which ﬁeld causes tools not to achieve high structural cov-
erage.
To construct a ﬁeld declaration hierarchy of the ﬁeld f
that a partially-covered branch statement is data dependen t
on, we can use reﬂection to obtain the class structure of a
program input pand search the ﬁelds for ﬁnding f(i.e.,f
is one of the ﬁelds of p). If we fail to ﬁnd f, we continue to
search the class structures of the ﬁelds of p, similar to graph
searching. The search continues until we ﬁnd f. The ﬁelds
along the path from ptofare used to construct the ﬁeld
declaration hierarchy. Another way, which Covana adopts,
is to use path conditions that lead to not-covered branches
and extract ﬁelds directly from path conditions, since the
symbolic expressions in the path conditions already contai n
program inputs and their ﬁelds. To illustrate the extractio n,
we use the example shown in Figure 2. The ﬁgure below
shows the path condition that leads to the true branch at
Line 5 and the ﬁeld declaration hierarchy constructed from
the path condition.
Algorithm 1 shows our algorithm that identiﬁes OCPs by
analyzing the ﬁeld declaration hierarchy (Fields) and the
not-covered branches (B). If the extracted ﬁeld declaratio n
hierarchy contains only one ﬁeld (satisfying Line 1 of the
algorithm), which should be the program input itself, our
algorithm reports the program input as an OCP directly.
To identify which ﬁeld in the ﬁeld declaration hierarchy
causes the OCP, our algorithm analyzes the ﬁeld declaration
hierarchy level by level, starting from Level 2 (i.e., the ﬁe ld
ofthe program input). Ifaﬁeldis assignable for its declari ng
class (not satisfying Line 10), DSE or other automated test-
generation tools can easily create an object of the ﬁeld’s
class type and assign the object to the ﬁeld by invoking
the corresponding constructor or public setter method. In
this case, it is the object type of the ﬁeld or a ﬁeld in the
next level(s), not its declaring class, that causes an OCP.
To further decide whether it is the current ﬁeld or the ﬁeld
in the next level that causes the OCP, our algorithm then
continues to check the ﬁeld in the next level (back to Line
7).
If a ﬁeld is not assignable for its declaring class (satisfy-
ing Line 10), the object state of the ﬁeld can be changed
only by invoking other public state-modifying methods of
its declaring class. Hence, Covana reports the type of its
declaring class as an OCP. In our example shown in Figure
2, the ﬁeld Stack.items cannot be assigned with an object
ofList<object> by invoking any constructor or public setter
method of Stack. To change the object state of Stack.items ,
DSE needs speciﬁc sequences of method calls for Stackin-
stead of List<object> . As a result, Covana identiﬁes the
object type Stackas an OCP (Line 11).5. IMPLEMENTATION
The prototype implementation of Covana includes three
parts: (1) the extension to Pex [10], an automatic white-box
test generation tool built for DSE. This extension identiﬁe s
problem candidates, assigns symbolic values to elements of
problem candidates, and collects runtime information; (2)
a data-dependence analyzer that analyzes the information
produced by these two components and identiﬁes EMCPs
and OCPs; (3) a Graphic User Interface (GUI) component
that shows the identiﬁed problems with detailed analysis
information.
6. EV ALUATIONS
In this section, we discuss the two evaluations conducted
to show the eﬀectiveness of Covana. In our evaluations, we
use two popular .NET applications: xUnit [14] and Quick-
Graph [6], and answer the following research questions:
•RQ1: How eﬀective is Covana in identifying the two
main types of problems, EMCPs and OCPs?
•RQ2: How eﬀective is Covana in pruning irrelevant
problem candidates of EMCPs and OCPs?
We next provide details of the metrics that we collect in our
evaluations. To measure the eﬀectiveness of our approach in
identifying EMCPs and OCPs (addressing RQ1), we mea-
sure the number of problems that Covana ﬁnds for the not-
covered branches or statements of the subject applications .
To measure the eﬀectiveness of our approach in pruning ir-
relevant problem candidates (addressing RQ2), we compare
the number of identiﬁed problem candidates with the num-
ber of identiﬁed problem by our approach in applications
under test and measure the number of problem candidates
pruned by our approach. To address both RQ1 and RQ2,
we measure the false positives, i.e., the number of irreleva nt
problem candidates that are not pruned by Covana, and the
false negatives, i.e., the number of real problems that are
identiﬁed as irrelevant problem candidates and pruned.
We next provide details on the subject applications and
evaluation setup, and the results of the two evaluations.
6.1 Subjects and Evaluation Setup
We usedtwopopular .NET applications for evaluatingour
Covana approach: xUnit [14] and QuickGraph [6]. xUnit is
a unit testing framework for .NET program development.
xUnit includes 223 classes and interfaces with 11.4 KLOC.
QuickGraph is a C# graph library that provides various
directed and undirected data structures of graphs. Quick-
Graph also provides graph algorithms such as depth-ﬁrst
search, topological sort, and shortest path [2]. QuickGrap h
includes 165 classes and interfaces with 8.3 KLOC.
In our evaluations, we use Pex with the implemented ex-
tensions as our DSE test-generation tool. The Pex version
usedfor ourevaluationis 0.24.50222.1. We ﬁrstapplyPex to
explore the applications under test and generate test input s.
After test generation and execution, which is automated by
Pex, the coverage and the collected runtime information are
fed into our stand-alone analysis tool for identifying EMCP s
and OCPs.
We next discuss the results of our evaluations in terms of
theeﬀectivenessof CovanainidentifyingEMCPs andOCPs,
and in reducing the irrelevant problem candidates.6.2 RQ1: Problem Identiﬁcation
In this section, we address the research question RQ1 of
how eﬀectively Covana identiﬁes EMCPs and OCPs. To
address this question, we measure the number of identiﬁed
problems, the number of false positives, and the number
of false negatives generated by Covana. To measure values
for these metrics, we executed the stand-alone analysis too l
implemented for our approach with the output information
from Pex as inputs, and manually classiﬁed the problems
reported by our tool as real problems, false positives, and
false negatives. To verify EMCP candidates, we either in-
strument or provide mock objects for the external-method
calls identiﬁed as EMCP candidates, and reapplied Pex to
check whether the not-covered branches can be covered. If
so, we classify the EMCP candidates as real problems, or
irrelevant problem candidates otherwise. Similarly, to ve rify
OCP candidates, we provide sequences of method calls for
the object types of the OCP candidates, and reapplied Pex
to check whether the not-covered branches can be covered.
If so, we classify the OCP candidates as real problems, or
irrelevant problem candidates otherwise.
Table 2 shows the results for all the assemblies in both
subject applications. Column “# File” lists the number of
source ﬁles in each application assembly. Columns“Object-
CreationProblem(OCP)”and“External-Method-CallProb-
lem (EMCP)”show the statistics of EMCPs and OCPs iden-
tiﬁed by Covana. Subcolumn“# Real”gives the number of
real problems identiﬁed by us manually. Subcolumn“Identi-
ﬁed”gives the numberof problems identiﬁedbyCovana, and
subcolumn“#FP”and“#FN”givethenumberoffalse posi-
tives and false negatives, respectively. The results show t hat
our approach identiﬁes 43 EMCPs with only 1 false positive
and 2 false negatives. In addition, our approach identiﬁes
155 OCPs with 20 false positives and 30 as false negatives.
The reason why we have 30 false negatives is that in our
prototype analysis tool, we did not implement the logics re-
quired to handle Dictionay objects (C# version of HashMap)
and static ﬁelds of classes. In our future work, we plan to
address these issues by identifyingthe ﬁelds of Dictionay ob-
jects and static ﬁelds of classes as candidates and computin g
datadependencies of partially-covered branchstatements on
them.
We next provide examples to describe scenarios where our
approach eﬀectively identiﬁes EMCPs and OCPs. We also
describe scenarios where our approach produces false posi-
tives and false negatives.
Figure 4 shows the class TestClassCommand of theXunit.Sdk
namespace. When we applied Pex to generate test inputs
for the method TestClassCommand.ClassStart , Pex generated
only one test input and achieved low block coverage of 2/27
(7.14%). In TestClassCommand.ClassStart , the loop at Line 8
requires the ﬁeld TestClassCommand.typeUnderTest to be not
null. Since Pex cannot ﬁnd in the application any public
class that implements the interface ITypeInfo to create such
an object for TestClassCommand.typeUnderTest , Pex cannot
generate more useful test inputs. Thus, we need to report
an OCP of the interface type ITypeInfo . By analyzing the
data dependencies of the entry branch of the loop at Line 8,
our approach extracts the argument object TestClassCommand
and its ﬁeld TestClassCommand.typeUnderTest . By analyzing
TestClassCommand andTestClassCommand.typeUnderTest , our
approach ﬁgures out that TestClassCommand.typeUnderTest
can be assigned by using the public constructor of the classApplication Assembly # File Object-Creation Problem (OCP) External-Method-Call Problem (EMCP)
# Identiﬁed # Real # FP # FN # Identiﬁed # Real # FP # FN
xUnit 71 68 67 13 12 24 24 0 0
xUnit.Extensions 17 7 5 3 1 2 2 0 0
xUnit.Console 7 2 2 0 0 2 2 0 0
xUnit.Gui 12 3 3 0 0 1 3 0 2
xUnit.Runner.Msbuild 6 15 14 1 0 0 0 0 0
xUnit.Runner.Tdnet 3 5 5 0 0 1 1 0 0
xUnit.Runner.Utility 28 7 12 0 5 9 9 0 0
Quickgraph 3 0 0 0 0 0 0 0 0
Quickgraph.Algorithms 12 7 11 0 4 0 0 0 0
Quickgraph.Algorithms.Graphviz 14 20 20 2 2 4 3 1 0
Quickgraph.Collections 19 6 11 1 6 0 0 0 0
Quickgraph.Concepts 35 5 5 0 0 0 0 0 0
Quickgraph.Exceptions 3 0 0 0 0 0 0 0 0
Quickgraph.Predicates 9 8 8 0 0 0 0 0 0
Quickgraph.Representations 3 2 2 0 0 0 0 0 0
Total 242 155 163 20 30 43 44 1 2
Table 2: Evaluation results showing the eﬀectiveness of Cov ana in identifying EMCP and OCP
public class TestClassCommand : ITestClassCommand {
00: readonly Dictionary<MethodInfo, object> fixtures
= new Dictionary<MethodInfo, object>();
01: Random randomizer = new Random();
02: ITypeInfo typeUnderTest;
03: ...
04: public TestClassCommand(ITypeInfo typeUnderTest) {
05: this.typeUnderTest = typeUnderTest; }
06: public Exception ClassStart() {
07: try {
08: foreach (Type @interface in
typeUnderTest.Type.GetInterfaces()) {
09: ...
10: }
11: }
12: ...
13: }
14: public Exception ClassFinish() {
15: foreach (object fixtureData in fixtures.Values) {
16: ...
17: }
18: }
}
Figure 4: TestClassCommand class of xUnit
TestClassCommand , and correctly reports an OCP of IType-
Info.
Similarly, Pex achieves low coverage block coverage of
6/16 (37.50%) when generating test inputs for the method
TestClassCommand.ClassFinish . The reason is that the loop
at Line 15 requires the ﬁeld TestClassCommand.fixtures to
hold at least one item. Since there is no constructor or pub-
lic setter method to assign an external object to TestClass-
Command.fixtures , other public methods of TestClassCommand
need to be invoked to change the value of TestClassCom-
mand.fixtures . Therefore, we need to report the program
inputTestClassCommand as an OCP. However, our approach
cannot detect such situation since the object type of the
ﬁeldfixtures isDictionary and we did not implement the
logics to handle such type. Hence, our approach did not
identify the object type of the fixtures as an OCP for the
not-covered branch at Line 15.
Figure 5shows twomethods: (1) themethod ParseComman-
dLineof classProgramin the namespace Xunit.ConsoleClient
and (2) the constructor of the class Executor in the names-
paceXunit.Sdk . ForParseCommandLine , Pex achieved low
block coverage of 44/154 (28.57%), because it cannot gener-
ate test inputstocause theexternal-methodcall File.Existsstatic bool ParseCommandLine(string[] args,
out string assemblyFile, ...) {
00: assemblyFile = args[0];
...
01: if (!File.Exists(assemblyFile)) {
02: Console.WriteLine("error: assem-
bly file not found: {0}", assemblyFile);
03: return false;
04: }
...
public Executor(string fileName) {
05: this.assemblyFilename = Path.GetFullPath(fileName) ;
06: ...
}
Figure 5: Two methods that have EMCPs in xUnit
to return true. Since the outvariable assemblyFile is as-
signed with the value of args[0] (and thus has data de-
pendencies on the program input args[0]), our approach as-
signedasymbolicvaluetothereturnvalueof File.Exist and
found that the branch statement at Line 1 (the false branch
not-covered) has data dependency on File.Exist for its re-
turnvalue. Thus, ourapproachcorrectlyreportedanEMCP
ofFile.Exists . For the constructor of the class Executor,
Pex achieved low block coverage of 2/5 (40%), because Pex
generated a nullobject as the argument for the constructor,
which caused the external-method call Path.GetFullPath to
throw an exception. Our approach collected this exception
thrown from Path.GetFullPath during runtime. By check-
ing the coverage of the remaining parts of the program af-
ter the call site of Path.GetFullPath , our approach found
that none of them was covered. As a result, our approach
reported Path.GetFullPath as an EMCP. Although another
external method Console.WriteLine at Line 2 receives assem-
blyFileas argument and is marked as an EMCP candidate
by our approach, this external method did not have any
return value, and thus no branch statements have data de-
pendencies on Console.WriteLine . As a result, our approach
correctly pruned Console.WriteLine .
6.3 RQ2: Irrelevant-Problem-Candidate Prun-
ing
In this section, we address the research question RQ2 of
how eﬀectively our approach prunes irrelevant problem can-
didates. To address this question, we compare the number
of identiﬁed problem candidates with the number of prob-
lems reported by our approach, and measure the number ofApplication Object-Creation Problem (OCP) External-Method-Call Problem (EMCP)
#Cand #Ident #Pruned #FP#FN#Cand #Ident #Pruned #FP#FN
xUnit 335 107 228 (68.06%) 17181313 391274 (97.03%) 02
QuickGraph 116 48 68 (58.62%) 312 297 4293 (98.65%) 10
Total 451 155 296 (65.63%) 20301610 431567( 97.33%) 12
Table 3: Evaluation results showing the eﬀectiveness of Cov ana in reducing irrelevant problem candidates
// Lines 1, 2, 4 are external-method calls
public static List<RecentlyUsedAssembly> LoadAssem-
blyList() {
00: ...
01: using (var xunitKey = Registry.CurrentUser.
CreateSubKey (XUNIT_KEY_NAME))
02: using (var recentKey = xunitKey.
CreateSubKey (RECENT_ASSEMBLIES_KEY_NAME)){
03: for (int index = 0; ; ++index)
04: using (var itemKey = recentKey.
OpenSubKey (index.ToString())) {
05: if (itemKey == null) {
06: break;
07: }
08: if (itemKey != null) {
09: ...
10: }
11: }
12: }
}
Figure 6: The method LoadAssemblyList in the class
RecentlyUsedAssemblyList of xUnit
problem candidates pruned by our approach. In addition,
we measure the false positives, i.e., the irrelevant proble m
candidates not pruned by Covana, and the false negatives,
i.e., the real problems prunedby Covana. To measure values
for these metrics, we executed the stand-alone analysis too l
implemented for our approach with the output information
from Pexas inputs, andmanually classiﬁed theproblemcan-
didates reduced by our tool as real problems, false positive s,
and false negatives in the same way as in addressing RQ1.
Table 3 shows the results of both subject applications.
Column “Application” lists the names of the subject ap-
plications. Columns “External-Method-Call Problem” and
“Object-Creation Problem” show the statistics of EMCPs
and OCPs, respectively. Here, the EMCP candidates are all
the encountered external-method calls during the test exe-
cution, and the OCP candidates are all the non-primitive
object types of program inputs and their ﬁelds that DSE
assigns symbolic values to. In Table 3, subcolumn“#Cand”
givesthenumberofproblemcandidates, subcolumn“#Ident”
gives the number of problems identiﬁed by Covana, and sub-
columns “#FP” and “#FN” give the number of false posi-
tives and false negatives, respectively. The results show t hat
our approach prunes 97.33% (1567 in 1610) EMCP candi-
dates with only 1 false positive and 2 false negatvies and
prunes 65.63% (296 in 451) OCP candidates with 20 false
positives and 30 false negatives. These results show that
our approach eﬀectively reduces the irrelevant problem can -
didates with low false positives and false negatives.
7. DISCUSSION AND FUTURE WORK
Covana identiﬁes problems faced by tools built for struc-
tural test-generation approaches and prunes irrelevant pr ob-
lemcandidatestoreducetheproblemspaceforinvestigatio n.
Covana serves as the ﬁrst step towards problem solving. In
fact, identifying problems for developers to investigate i sanalogical to fault localization before fault ﬁxing. Below ,
we discuss how Covana can be used to assist other auto-
mated test-generation approaches or manual test-generati on
approaches, and thendiscuss some issues including those en -
countered in our evaluations.
Assisting Other Structural Test-Generation Ap-
proaches . Given test inputs, no matter whether they are
generated by other automated test-generation approaches,
such as a random approach, or are generated manually, Co-
vana can be used to identify problems of speciﬁc types, such
as EMCPs and OCPs. The analysis result of Covana not
only can reduce the eﬀorts of developers in providing guid-
ance to tools, but also can reduce the cost of tools built for
other test-generation approaches. The ﬁrst example is to
automatically generate mock objects for only the external-
method calls identiﬁed as EMCPs by Covana. Since Covana
greatly reduces the number of irrelevant problem candidate s
of EMCP, it becomes possible to generate mock objects for
the external-method calls identiﬁed as EMCPs. As another
example, random approach can assign more probabilities on
exploring the object types reported as OCPs by Covana, in-
creasing the chances to achieve higher structural coverage
in shorter time. Advanced method-sequence-generation ap-
proaches [9] can also be used to address OCPs for increasing
coverage.
Static Field . In our evaluations, we observed that a few
classes contained static ﬁelds that were initialized insid e the
classes. These static ﬁelds were later used by some branches
and some of these branches were not covered by DSE. Since
DSE did not automatically assign symbolic values to static
ﬁelds, DSE was not able to collect symbolic constraints on
these static ﬁelds. In future work, we plan to assign sym-
bolic values to these static ﬁelds, so that our approach can
collect the symbolic constraints on these static ﬁelds for o ur
analysis.
Concrete Arguments for External-Method Calls .
Our current Covana implementation identiﬁes the return
values of external methods as candidates if the method ar-
guments have data dependencies on program inputs. How-
ever, in our evaluations, there were a few external-method
calls received concrete values as arguments, and resulted i n
some not-covered branches. The external-method call re-
centKey.OpenSubKey , shown in Figure 6, received a concrete
value returned by index.ToString() . Since its return value
itemKeyofrecentKey.OpenSubKey isnull, the false branch at
Line 5 is not covered. In this case, our approach cannot
detect the problem, since our approach does not mark as a
candidate the return value of any external-method call that
does not receive any symbolic values as an argument. By
assigning symbolic values to all external-method calls, ou r
approach can be easily extended to compute data depen-
dencies on every external-method call, no matter whether
its arguments have data dependencies on program inputs.
However, computing data dependencies on every external-
method call may incur many false positives and increase
the performance overhead signiﬁcantly, since the number ofexternal-method calls encountered during the program ex-
ecutions is not trivial. In future work, we plan to conduct
experiments to measure the eﬀectiveness and performance
overhead when every external-method call is considered as
a candidate.
Other Potential Issues . Besides the issues encoun-
tered in our evaluations, there are still some potential iss ues
that may aﬀect the eﬀectiveness of our approach: (1) ar-
gument side eﬀect : some external-method calls may have
side eﬀects on the receiver objects or method arguments
that have data dependencies on program inputs, causing
some subsequent branches not to be covered; (2) control
dependency : extending our approach to consider control
dependency may improve the eﬀectiveness of our approach
in some cases; (3) static analysis : our approach currently
computes dynamic data dependencies based on the executed
paths, and may miss some data dependencies on unexecuted
paths. Employing static analysis to analyze all the paths is
one option to solve the problem. Nevertheless, due to the
complexity of programs, static analysis may produce false
positives on detected data dependencies, which would com-
promise the eﬀectiveness of our approach. We plan to con-
duct experiments to evaluate the eﬀectiveness of incorpo-
rating argument side eﬀect, control dependency, and static
analysis.
8. RELATED WORK
Coverage Analysis . Pavlopoulou and Young [5] devel-
oped a residual coverage monitoring tool for Java, which
provides richer feedback from actual use of deployed soft-
ware. Since their approach aims to reduce the performance
overhead for gathering structural coverage from deployed
software, their approach does not provide a way to ana-
lyze the coverage, while our approach analyzes the residual
structural coverage gathered from DSE to ﬁlter out irrele-
vant problem candidates.
Explaining Failures of Program Analysis . Dinck-
lage and Diwan [13] propose an analysis language and build
a system to produce reasons when program analyses fail to
produce desirable results. The objective of their approach is
to express arbitrary data ﬂow analyses using their analysis
language andcomputereasons for thefailures. Althoughour
approach is remotely related to their approach in terms of
helping explain causes of residual structural coverage in t he
form of problems, our approach focuses on a quite diﬀer-
ent problem and includes signiﬁcantly diﬀerent techniques
needed for addressing unique challenges in identifying pro b-
lems that prevent test-generation tools from achieving hig h
structural coverage.
Symbolic Execution . Anand et al. [1] propose type-
dependence analysis, which performs a context- and ﬁeld-
sensitive interprocedural static analysis to identify the parts
of the program under test that may be unsuitable for sym-
bolic execution, such as third-party libraries. Their ap-
proach identiﬁes external-method calls that are problemat ic
in symbolic execution by carrying out static analysis to de-
termine whether an external-method call receives symbolic
values as arguments. To identify EMCPs, our approach con-
siders not only data dependencies of arguments of external-
method calls on program inputs, but also data dependencies
of partially-covered branch statements on external-metho d
calls for their return values.9. CONCLUSION
In this paper, we propose cooperative developer testing,
where developers provide guidance to help structural test-
generation tools achieve high structural coverage. To redu ce
the eﬀorts of developers in providing guidance, we propose
a novel approach, called Covana, which precisely identiﬁes
and reports problems that cause structural test-generatio n
tools not to achieve high structural coverage. Covana iden-
tiﬁes these problems by computing data dependencies of
partially-covered branch statements on problem candidate s.
We concretize Covana to identify problems faced by DSE
and present two techniques to identify EMCPs and OCPs,
the top two major types of problems. We also evaluate Co-
vana on two open source projects and the results show that
Covana eﬀectively identiﬁes EMCPs and OCPs.
Acknowledgments. This work is supported in part by
NSFgrantsCNS-0716579, CCF-0725190, CCF-0845272, CCF-
0915400, CNS-0958235, an NCSU CACC grant, ARO grant
W911NF-08-1-0443, andAROgrantW911NF-08-1-0105man-
aged by NCSU SOSI.
10. REFERENCES
[1] S. Anand, A. Orso, and M. J. Harrold.
Type-Dependence Analysis and Program
Transformation for Symbolic Execution. In Proc.
TACAS, pages 117–133, 2007.
[2] T. H. Cormen, C. Stein, R. L. Rivest, and C. E.
Leiserson. Introduction to Algorithms . McGraw-Hill
Higher Education, 2001.
[3] P. Godefroid, N. Klarlund, and K. Sen. DART:
Directed Automated Random Testing. In Proc. PLDI ,
pages 213–223, 2005.
[4] Math.NET, 2008. http://www.mathdotnet.com/ .
[5] C. Pavlopoulou and M. Young. Residual Test Coverage
Monitoring. In Proc. ICSE , pages 277–284, 1999.
[6] QuickGraph, 2008. http://www.codeproject.com/KB
/miscctrl/quickgraph.aspx.
[7] K. Sen, D. Marinov, and G. Agha. CUTE: a Concolic
Unit Testing Engine for C. In Proc. ESEC/FSE , pages
263–272, 2005.
[8] SvnBridge: Use TortoiseSVN with Team Foundation
Server, 2009. http://www.codeplex.com/SvnBridge .
[9] S. Thummalapenta, T. Xie, N. Tillmann,
P. de Halleux, and W. Schulte. MSeqGen:
Object-Oriented Unit-Test Generation via Mining
Source Code. In Proc. ESEC/FSE , pages 193–202,
2009.
[10] N. Tillmann and J. de Halleux. Pex-White Box Test
Generation for .NET. In Proc. TAP , pages 134–153,
2008.
[11] N. Tillmann and W. Schulte. Parameterized Unit
Tests. In Proc. ESEC/FSE , pages 253–262, 2005.
[12] N. Tillmann and W. Schulte. Mock-object Generation
with Behavior. In Proc. ASE , pages 365–368, 2006.
[13] D. von Dincklage and A. Diwan. Explaining failures of
program analyses. In Proc. PLDI , pages 260–269,
2008.
[14] xUnit, 2007. http://www.codeplex.com/xunit .