BugCache for Inspections : Hit or Miss?
Foyzur Rahman Daryl Posnett Abram Hindle Earl Barr Premkumar Devanbu
Department of Computer Science
University of California Davis, Davis, CA., USA
{mfrahman,dpposnett,ajhindle,etbarr,ptdevanbu}@ucdavis.edu
ABSTRACT
Inspection is a highly eective but costly technique for quality
control. Most companies do not have the resources to inspect
all the code; thus accurate defect prediction can help focus
available inspection resources. BugCache is a simple, elegant,
award-winning prediction scheme that \caches" les that are
likely to contain defects [ 12]. In this paper, we evaluate
the utility of BugCache as a tool for focusing inspection,
we examine the assumptions underlying BugCache with the
aim of improving it, and nally we compare it with a simple,
standard bug-prediction technique. We nd that BugCache is,
in fact, useful for focusing inspection eort; but surprisingly,
we nd that its performance, when used for inspections, is
not much better than a naive prediction model | viz., a
model that orders les in the system by their count of closed
bugs and chooses enough les to capture 20% of the lines in
the system.
Categories and Subject Descriptors
D.2.5 [ Software Engineering ]: Testing and Debugging|
Code Inspections and Walk-Throughs
General Terms
Experimentation; Measurement; Reliability; Verication
Keywords
Empirical Software Engineering, Fault Prediction, Inspection
1. INTRODUCTION
The later a bug is discovered, the more expensive and dif-
cult it is to resolve [1]. Deployed bugs are the worst of all,
requiring costly, embarrassing, and labor-intensive software
updates in the eld and/or re-deployments. Thus stakehold-
ers, such as managers and quality assurance people, worry
most about faults that escape into released and deployed
products. Quality control methods such as inspection and
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proÔ¨Åt or commercial advantage and that copies
bear this notice and the full citation on the Ô¨Årst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciÔ¨Åc
permission and/or a fee.
ESEC/FSE‚Äô11, September 5‚Äì9, 2011, Szeged, Hungary.
Copyright 2011 ACM 978-1-4503-0443-6/11/09 ...$10.00.testing aim to detect faults prior to release. Unfortunately,
code inspection and testing are costly in terms of time and
manpower, so managers seek to optimize their eectiveness.
Bug prediction has been suggested as a means to this end.
If one could predict which entities | modules, les or lines
of code | are bug-prone, quality control teams might focus
their eort on inspecting those entities.
In their award-winning paper on BugCache [12], Kim et al.
hypothesized that past knowledge of fault occurrence can
optimize testing and inspection eort. Their proposed cache-
inspired models depend on the exploitable \locality" of faults.
They hypothesized three types of locality. First, faults are
more likely to occur in recently added/changed entities (churn
locality). Second, if an entity contains a fault, then it is likely
to contain more faults (temporal locality). Finally, entities
that are logically coupled (by co-changes) with other faulty
entities are more likely to contain faults (spatial locality).
To exploit fault locality, Kim et al. uses two types of
changes, bug-introducing changes and bug-xing changes,
along with change history, to identify a subset of entities
that are highly likely to contain defects. They implemented
two types of caches: BugCache, a theoretical model, and
FixCache , a practical prediction model. BugCache contains
faulty entities and is updated when a bug, or fault, is in-
troduced into a le. In reality, bug-introductions usually
occur unnoticed, (and hopefully discovered later!) so Bug-
Cache is a theoretical model that can't be used until the full
project history is known. FixCache , on the other hand, is a
practical realization of BugCache that doesn't need oracular
knowledge.
The evaluation of FixCache drew our attention: we
wanted to replicate it from a dierent perspective, with
an emphasis on inspection. FixCache was originally evalu-
ated using the measure of \hit rate", which is the cumulative
percentage of the successful cache probes through a project
history. For regression testing , this measure is eective: if
FixCache accurately identies the defect-prone les, one
can run tests only on defect-prone, modied les. However,
our concern is inspection eciency . Kim et al. wrote:
\A manager then can use the list for quality as-
surance | for example, she can test or review
the entities in the bug cache with increased prior-
ity" [12,x6] (our italics).
Thus we evaluate FixCache from the point of view of a
stakeholder involved in code inspection | \how much eort
does FixCache actually save inspectors?"
Code inspectors would prefer to focus their eort on bug-
giest of entities in order to achieve a greatest possible payoof bug detection for a given investment in inspection time.
Suppose, hypothetically, that FixCache simply loaded the
largest les into the cache. In this case, a high hit rate would
not necessarily translate into an eective inspection process:
a great many lines of code might have to be inspected to nd
bugs. Likewise, loading smaller les might lead to fewer hits,
but might result in ecient inspection that discovers more
bugs per unit of work.
To this end, we rstevaluate FixCache , not against hit
rate (cache hits), but against defect density and the cost
eectiveness of inspection. Second , we address the questions
\What makes FixCache work?" and \Of the dierent cache
update policies it uses, which is most eective?" Third ,
we investigate whether certain modications to these cache
update policies would improve FixCache 's performance with
respect to the inspection cost-eectiveness. Finally , we study
whether FixCache outperforms, in terms of inspection cost
eectiveness, a very simple, standard bug prediction model.
This paper mounts a replicated study of FixCache and
makes the following contributions:
We evaluate FixCache using defect density. We nd
that, in fact, FixCache does indeed nd more bug-
dense entities.
We study how the dierent update and replacement
policies applicable contribute to FixCache 's bug pre-
diction and inspection eciency, using an inspection
cost-eectiveness measure. We nd that there is not
much dierence in the performance of dierent update
and replacement policies.
We compare FixCache with a simple prediction model,
using the same inspection measure and nd that there
is not much dierence .
2. THEORY
FixCache is easily described. To start, the cache is primed
with a set of les, typically the largest les. Then, the
FixCache algorithm monitors the commits to the system
and updates its cache using a set of policies. A hitoccurs
when a le in the cache undergoes a bug x; a miss occurs
when a le outside the cache receives a bug x. The cache's
contents, at any one point in time, contains the target set
of buggy les for testing or inspection. FixCache uses 3
update policies.
When a bug-x is committed, FixCache immediately adds
the modied les to its cache; this is the temporal locality
policy. Next it retroactively nds and adds the le(s) from
the commit(s) in which the bug was introduced, as well as
the les that were most often committed together with these
les at that time. This is the spatial locality policy. Finally,
FixCache adds recently added/changed les to the cache.
We call this the churn locality policy. All these les are added
to the cache, if they are not already present, when a bug-x
is observed in the anticipatory belief that they will require
xes soon.
Kim et al. empirically evaluated FixCache using various
cache sizes (most often 10% of the entities in the system) and
found it very accurate ( 73%{95%) at identifying faulty les.
They also evaluated FixCache 's accuracy at method granu-
larity and found its accuracy to fall within 46%{72%. In this
paper we only evaluate FixCache at le granularity. Sev-
eral papers [ 7,18,21,23] have reported re-implementationsof FixCache, for various applications; our implementation
achieved performance levels similar to those reported in these
earlier papers, roughly 60%{80% hit rate at le granularity,
as we discuss below.
2.1 Cost-Effectiveness
Evidently, if we could inspect just 10% of the les in a
project and nd most of the project's defects, that would
be terric for the software industry. Unfortunately, unlike
a hardware cache that contains xed size cache lines, Fix-
Cache contains source code entities, such as les, that can
vary greatly in size. Thus, if the les in the FixCache tend
to be large and contain a disproportionately large portion
of the project code, then focusing on the les in FixCache
may not be very helpful for code inspection. So, rst, we ask
what proportion of the lines of code are actually contained
in the les that reside in the cache.
Research Question 1: What proportion of the total
lines of code are in the FixCache ?
Even if the entities in FixCache include a very large pro-
portion of the lines of code in the project, they might still be
well worth inspecting, if those lines contain proportionately
more bugs ( viz., higher defect density) than the lines not in
the cache. When this happens, prioritizing the cached les
increases inspection eectiveness. If, however, cached les
turn out to have lower defect density than les that are not
cached, then prioritizing les in FixCache for inspection is
unwarranted. This leads us to our next research question.
Research Question 2: Do the cached les have higher
defect density (defects per lines of code) than les not
in the cache?
Answering this question will allow us to measure the ef-
cacy of FixCache . While this gives us a general idea of
the possible benets of using FixCache , it does not tell us
why it actually works (if it does). A good understanding of
the mechanics of FixCache may enable us to customize or
enhance it. Thus arises the imperative to desconstruct the
various \locality-based" update policies used by FixCache ,
and evaluate their merits.
2.2 What Makes FixCache Tick?
The operation of FixCache is predicated upon several
update policies. How do these update policies aect the
performance of FixCache ? There are four sources of cache
updates:
1.\Initial pre-fetch": Files added to the cache during
cache initialization (following Kim et al. [12]).
2.\Temporal locality": Files added to the cache because
they were changed to x bugs.
3.\Spatial locality": Files co-changed with a buggy le,
viz., les that co-occur in many transactions where the
buggy le was changed.
4.\Churn locality": Files that have recently been changed
or added.Of these four, we consider the rst two to reect core
aspects of any cache, including FixCache . First, we must
initialize the cache with some les, or else the initial hit rate
would be low. Second, temporal locality adds a le to the
cache, if it is not already present, when a defect is found in
it. The other two sources, spatial locality and churn locality,
provide additional policies for including les into the cache;
it is these two policies that we evaluate empirically.
Research Question 3: How do temporal and spatial
locality policies aect FixCache performance?
With a detailed understanding of the relative inuence
of dierent FixCache update policies, future research may
explore customizing FixCache to attain a dierent set of per-
formance goals. For example, while Kim et al. used the \hit
rate" measure to evaluate performance, one might, as we do,
choose other measures, such as density or cost-eectiveness,
a measure developed by Arisholm, Briand & Johannessen [ 3].
Research Question 4: Can we improve FixCache to
achieve better defect density?
This question raises additional interesting questions. What
is the right way to optimize cache performance? Surely it will
depend on a project's requirements and resource availability.
Kim et al. measured FixCache performance in terms of
predictive accuracy. Is this sucient?
2.3 Evaluating Prediction Performance
Kim et al. use the measure\hit rate"to evaluate FixCache .
\Hit rate" is the cumulative percentage of successful cache
probes at the end of a run through a project's history. At
any point in time, the cache may contain both defective and
non-defective les. Only les that actually contain defects
score hits, so the number of defective les in the cache bears
some relation to the hit rate. FixCache considers les in
the cache to be defective and those not in the cache to be
non-defective. Consequently, the cache contains both true
positives (TP), les that actually contain defects, and false
positives (FP). The les not in the cache comprise the true
negatives (TN), non-defective les, and false negatives (FN),
defective les that are incorrectly classied. Recall measures
the ratio of retrieved to relevant les:
Recall =TP
TP+FN=TP
Defective:
No matter how many false positives are in the cache, recall
remains unaected. In fact, we can achieve perfect recall sim-
ply by enlarging the cache to hold all les. Recall, however,
only tells half the story of a successful prediction algorithm.
We are also interested in the ratio of relevant les to those re-
trieved. If only 10% of the les are defective, then retrieving
100% of the les, which would naively yield perfect recall, is
an inecient way to identify the defective les and demon-
strates the limitations of using recall in isolation. Thus,
recall is usually paired with another measure that penalizes
an algorithm for falsely predicting too many entities as de-
fective. Precision is the ratio of correct predictions to the
total predicted as defective
Precision =TP
TP+FP=TP
InCache:When applied in this setting, however, both precision and
recall treat all les equally. Eectively, they are indierent to
the number of lines of code (LOC) in the les being agged
as defective. When using a prediction tool as a guide for
inspection, LOC is critical; smaller, buggy les may well
have higher defect density, and consequently may yield a
better return on inspection investment. When comparing
dierent cache update policies, or dierent prediction models
that select dierent numbers of les (and varying amounts of
code) for inspection, we adopt a cost-eectiveness measure,
specically the area under the cost-eectiveness curve, or
aucec .Readers unfamiliar with this rather subtle measure
may wish to read Section 2.4 below, before returning here.
We apply these concepts to the evaluation of FixCache in
order to build a more complete picture of its performance in
an inspection setting. The extent to which FixCache exceeds
the performance of standard models could suggest approaches
for improving standard prediction models intended for use in
inspection; perhaps an update policy, such as spatial locality,
can be fruitfully incorporated into a traditional model. For
this reason and to asses its ecacy, we use aucec to compare
FixCache against a vanilla prediction model.
TheFixCache algorithm imposes only a gross ordering
of the les to be inspected, viz., those les within the cache
are to be inspected prior to those without. The calculation
of cost eectiveness requires that we impose an order on the
les within the cache so that we do not articially penalize
theFixCache algorithm in our evaluation.
To evaluate FixCache using aucec , we impose the same
order on the les within the cache and those not in the
cache | rst by decreasing number of closed bugs, then by
increasing size. This means that the smaller les with many
closed bugs bubble up to the top.
This approach evaluates the cost-eectiveness of FixCache
fairly with our naive algorithms: we found that this ordering
provided the best performance for FixCache in every setting
when compared with other simple orderings, e.g.density, le
size, or previous defects.
Research Question 5: Does FixCache outperform
standard, simple prediction models, when used for in-
spection?
2.4 Cost-Effectiveness Curve
Here, we briey dene cost-eectiveness, then the area
under the CE curve (AUCEC), measures which we use to
investigate RQ5. If you are familiar with these measures,
please feel free to skip this section.
Suppose defects are uniformly distributed throughout the
source code. Then, if an inspection budget allows inspection
of10% of the source code, we might choose 10% of the lines
at random and reasonably expect to nd about 10% of the
defects. This scheme requires no work or expertise in data
gathering and defect modeling. It is therefore reasonable to
expect that any useful defect prediction method should be
able to improve on this result. This is the basis for the cost
eectiveness (CE) metric dened by Arisholm et al. [2] to
investigate defects in telecommunications software.
Suppose that source code is selected for inspection using a
prediction algorithm that orders code entities ( e.g., les) in
terms of predicted defectiveness. If the algorithm is good and
the system has only a few defects, or well-clustered defects,100755025100755025Percent of LOCPercent of Bugs FoundOPR
755025100755025Percent of LOCPercent of Bugs Found10020%P1P2RFigure 1: Cost Eectiveness Curve. On the left, O is
the optimal, R is random, and P is a possible, practical ,
predictor model. On the right, we have two dierent
models P1 and P2, with the same overall performance,
but P2 is better when inspecting 20% of the lines or less.
.
this procedure would allow us to inspect just a few lines of
code to nd most of the defects. If the algorithm performs
poorly and/or the defects are uniformly distributed in the
code, we would expect to inspect most of the lines before we
nd all the defects. The CE curve (see Figure 1, left side)
is a harsh but meticulous judge of prediction algorithms; it
plots the proportion of identied faults (a number between
0 and 1) found against the proportion of the lines of code
(also between 0 and 1) inspected. It is a way to evaluate a
proposed inspection ordering. With a random ordering (the
curve labeled Ron the left side of Figure 1) and/or defects
uniformly scattered throughout the code, the CE curve is the
diagonal y=xline. At le-granularity, the optimal ordering
(labeled Oin the Figure 1 left) places the smallest, most
defective les rst, and the curve climbs rapidly, quickly
rising well above the y=xline. A practical algorithm has a
CE curve (labeled Pin the Figure 1 left) that falls below the
optimal ordering, but remains usefully above the y=xline.
The CE curve represents a variety of operating choices:
one can choose an operating point along this curve, inspect
more or fewer lines of code and nd more or fewer defects.
Thus, to jointly capture the entire set of choices aorded
by a particular prediction algorithm, one typically uses the
area under the CE curve , oraucec , which is also a number
between 0and1. An imaginary, utterly fantastical, prediction
algorithm will have an area very close to 1,viz., by ordering
thelines so that one can discover all the defects by inspecting
a single line; a superb algorithm will have a aucec value
close to the optimal. Values of aucec below 1 =2 indicate a
poor algorithm. Thus, useful values of aucec lie between
1=2 and the optimum.
Consider two dierent prediction models with nearly iden-
ticalaucec . On the right side of Figure 1, the two curves
labeled P1 and P2 have very similar aucec values. How-
ever, if one were inspecting 20% of the lines or less, P2
oers a better set of operating points. This line budget is
indeed quite realistic: inspecting 20% of the LOC in the
system is denitely more realistic than inspecting all of it.
Thus, the aucec , cut o at 20%, is a useful measure of
the cost-eectiveness of a prediction model; this is what we
use. Arisholm et al. [3] refer to a similar notion of aucec ,
conditioned on choosing 20% of the system. As it turns out,
the preferred FixCache cache size setting of 10% of the
overall number of les in the system, typically results in acache that contains close to 20% of the LOC in the system .
So we adopt aucec at20% of the LOC, which we refer to
asaucec 20.
3. EXPERIMENTAL FRAMEWORK
In this section, we dene some terminology and present
our experimental setup.
3.1 Revision
Source code management (SCM) systems provide a rich
version history of software projects. This history includes all
commits to every le. These commits have various attributes
such as a timestamp, authorship, change content, and commit
log message. In our study, a commit, or a revision, consists of
an author, a timestamp and a set of les changed. We chose
Gitas our version control system, because of its excellent
ability to track changes and nd the origin of each line of
code.
3.2 Bug-Ô¨Åxing Revision
Bugs are discovered and usually recorded into an issue-
tracking system such as Bugzilla and subsequently rejected
or xed by the developers. Each bug report records its
opening date, the date of the bug x, a free-form textual bug
description, and the nal, triaged Bugzilla severity. For this
research, we consider any severity other than an enhancement
to be a bug.
Our study begins with links between Bugzilla bugs and the
specic revision that xes the bug | we call this a bug-xing
revision . We employed various heuristics to derive our data.
We scan for keywords, such as\bug", \xed", etc., in the SCM
commit log to ag bug-xing revisions [ 15]. Also, numerical
bug identiers, mentioned in the commit log, are linked back
to the issue tracking system's identiers [ 8,20]. Then these
identiers are crosschecked against the issue tracking system
to see whether such an issue identier exists and whether
its status changed after the bug-xing commit. Finally, we
manually inspect the links to remove as many spurious links
as possible. Each remaining linked bug has a bug-xing
revision. We gratefully acknowledge bug data we obtained
from Bachmann et al. [4].
3.3 Bug-Introducing Change
We call the lines of code that are associated with the
changes that trigger a bug x \x-inducing code", following
Sliwerski et al. [19] (also see [ 11]), as it is the code that
needed repair. For example, if strcpy (str2,str1); were
changed to strncpy(str2,str1,n); then the original line is
considered x-inducing code. New lines may also be added
in the bug-xing revision, but we do not consider these
\implicated", since they are part of the treatment, not the
symptom. Kim et al. uses the term bug-introducing change
to describe such x-inducing code.
We use the popular Sliwerski-Zimmerman-Zeller SZZ [19]
approach to identify bug-introducing changes. We start
with data that links a bug to the revision where that bug
was xed. If a bug x is linked to revision n+ 1, then n,
the immediately preceding revision, contains the relevant
buggy code. The diff of revision nandn+ 1of each le
changed in revision n+ 1gives us the potentially buggy
code. We call these lines the x-inducing code. We then use
thegit blame command on the x-inducing code; git blame
produces accurate provenance annotations (author, date,revision number where they were last changed) for each line.
With this information, we use the SZZ approach to rule out
lines that could not have been involved in the defect, because
of their time of introduction. For the rest, this approach
identies the commit that introduced the x-inducing line.
3.4 Defect Density
We calculate two types of defect density: defect density
based on open bugs and defect density based on closed bugs.
At time t, we identify number of closed bugs of a le up to
time tand divide that by the size of the le to calculate
closed bug density . At time t, we identify open bugs as those
reported before tand closed after t.Open bug density is the
count of open bugs divided by le size. Note we use only
the bugs that are linked to a le, since we cannot in general
identify the culpable le of an unlinked bug. By using this
density measure, we are assuming that les with higher open
bug density are more protable to inspect.
3.5FixCache
We faithfully re-implemented FixCache as described in [ 12].
Following earlier evaluation studies [ 18,21,23] ofFixCache ,
we implemented le-level FixCache ,i.e.our implementation
aims to predict defect-prone les.
Before running FixCache , for eciency, we rst identify
all the bug-xing revisions and chronologically order them.
For each of these bug-xing revisions, we also use the SZZ
technique to identify the revision of the bug-introducing
change. Now we are ready to run FixCache . We \prime"
the cache with the largest les of a project during the initial
pre-fetch , then scan its history. At every bug-x revision, we
probe the cache to see if the le being xed is in the cache;
if so, we score a hit, otherwise, we score a miss. If a probe
results in a cache miss, we \fetch" that buggy le into the
cache, and additionally determine all the spatially related
les (les that were modied frequently with the buggy le
in question). As described by Kim et al. , we go back in time
to when a bug was introduced using SZZ, and determine
spatially local les, les that were co-committed with the
buggy le being probed at that time. We limit the fetch
size of spatially local les to the block size parameter [ 12],
prioritized by the frequency of co-commit (more often co-
committed les get higher priority).
Regardless of cache hit or miss, we also fetch the recently
changed/added les (les that were changed/added between
two bug-x revisions), subject to the pre-fetch size parame-
ter [12].
As new les are added into the cache, we may need to
evict old les. Following Kim et al. , we used two eviction
policies | least recently used (LRU), to retain les recently
involved in a bug-x, and closed bugs, to retain les with the
highest number of closed bugs. We did not implement the
CHANGE [ 12] eviction policy as Kim et al. reported that it
performs poorly.
The original FixCache presented results for 10% cache
size. Prior evaluations [ 18,21,23] ofFixCache also rely on
10% cache size. To ensure the sensitivity of our analysis, we
experimented with several dierent cache sizes. We worked
with 6,10,18and25% cache size. For these cache sizes,
we set the block size to 2,5,9and13% respectively and
pre-fetch size to 1, 1, 2 and 3% respectively.3.6 Snapshot
As we described earlier, after each bug-xing revision,
we update the cached les. After updating the cache, we
determine the state of the system, in particular the cached
and uncached les. We also capture other system details such
as the cache update source, the last hit time, etc.For each
cached le, we determine its number of closed bugs, its size,
etc.We call the overall system state after each bug-xing
revision a snapshot of the system.
4. THE DATA SETS
We chose 5 dierent medium to large-sized open-source
projects for our study. All have long development histories,
but are otherwise quite dierent. Apache Httpd is a widely
used open source web server. Gimp is a popular open source
image manipulation program. Nautilus is the default le man-
ager for the Gnome desktop. Evolution is the default email
client for the Gnome desktop with support for integrated
mail, address book and calender functionality. Lucene is the
most popular text search engine library. Except Lucene, all
of our projects are written in C. Lucene is written in Java.
We converted the Apache subversion repository to gitand
used the other projects' gitrepositories directly.
These systems span varied application domains: clients
and servers, GUI applications, a le browsers, and libraries
for text searching. All are of substantial size and complex-
ity. Table 1 summarizes some descriptive statistics for each
project studied. Project size ranges from 191k lines to about
800k lines. The table also presents the study period, the
number of buggy les (which represents the number of cache
probes made), the average number of source les (.c, .cpp, .h,
or .java les) in bug xing revisions, the average lines of code
(as measured by wc -l Unix command) in bug xing revi-
sions, the number of commits made during the study period,
the number of bug xing commit made during this period
and the number of bug-introducing commits (commits that
introduced/changed some code that was changed/deleted
later to x some bug).
We determine all linked bugs and their associated bug-
xing revisions for all the subject projects. The correspond-
ing bug-xing change is found by using diff between a bug-
xing revision and its immediate preceding revision. After
collecting all bug-xing changes, we use git blame on each
line of the bug-xing change to identify its lastmodication
time. This gives us all of the bug-introducing changes.
5. EV ALUATION
We now present our results per each research question
presented in Section 2. We only present plots from Apache
and Lucene. Data from other projects is discussed in the
text.
RQ1: What is the line coverage of FixCache ?
Does FixCache achieve good hit-rates by simply caching
a disproportionate fraction of the total LOC in the system?
To check this, we plot the cached les' total LOC count in
Figure 2 for 10% cache size and LRU based replacement
policy. The time-series line plot shows that the FixCache
caches a higher proportion of total line count when compared
to the proportion of total le count within the cache. We
found similar results for all projects. Moreover, we ran a
sensitivity analysis by choosing the cache size to be 6,10,18Start End Number of Avg. Avg. Number Number Number of
Date Date Buggy Number Number of of Fix Fix Inducing
Name Files of Files of LOC Commits Commits Commits
Apache 1996-07-03 2009-04-21 610 313 191303 18456 372 1108
Gimp 1997-01-01 2009-08-08 5291 2108 784529 25583 1727 6042
Nautilus 1997-01-02 2009-08-06 1235 312 136736 13462 656 1608
Evolution 1998-01-12 2009-08-08 3980 1146 432498 30528 1363 3733
Lucene 2001-09-11 2010-06-17 2453 1181 236672 5182 683 2369
Table 1: Summary of study subjects
2002 2003 2004 2005 2006 2007 20080 10 20 30 40 50
Apache
FixTime%ofTotalFileSize
2006 2007 2008 2009 20100 10 20 30 40 50
Lucene
FixTime%ofTotalFileSize
Figure 2: LOC in cached les for Apache and Lucene. If
FixCache does not favor any particular size of le, then
the dashed at-line represents the expected fraction of
total project line count in the cache for a 10% cache
size. Time-varying line is the fraction of total project
line count in the cache
and25% of the overall les and using both LRU and Closed
Bugs replacement policy. In all cases the FixCache retained
a signicantly greater (double to triple) proportion of lines,
in comparison to the proportion of the total number of les
held in the cache. This fact could undermine the ecacy of
FixCache , when used to prioritize les for inspection.
These results suggest that the performance of FixCache ,
when used as an aid for targeting inspections, is not neces-
sarily as good as suggested by the original hit rate measure.
For inspection use, we need to consider le size to build a
clearer picture of FixCache performance. We do this by
comparing the defect density of cached les with that of the
un-cached les.
Cached0.000 0.002 0.004 0.006ApacheDefectDensity
Uncached
Cached Uncached0.000 0.005 0.010 0.015 0.020LuceneDefectDensity
Figure 3: Defect Density boxplot in cached and un-
cached les (10% cache size, LRU replacement) for
Apache and Lucene.
RQ2a: Do the cached les have higher defect density than
les not in cache?
Figure 3 shows the boxplot of defect density in cached
2002 2003 2004 2005 2006 2007 20080.000 0.001 0.002 0.003 0.004
Apache
FixTimeDefectDensity
CachedFile
UncachedFile
2006 2007 2008 2009 20100.000 0.004 0.008 0.012
Lucene
FixTimeDefectDensity
CachedFile
UncachedFileFigure 4: Defect Density in cached and un-cached
les (10% cache size, LRU replacement) for Apache and
Lucene.
and un-cached les for 10% cache size and LRU replacement
policy. Clearly, les in the FixCache have higher defect
density. We also ran an one sided paired Wilcoxon signed
rank test with a null hypothesis : defect density in cached les
is no greater than defect density in un-cached les. We found
that the FixCache does indeed contain les with greater
density (all the p-values were very small ( <<0:001)).
We also did a sensitivity analysis for dierent cache sizes.
We found that the above result holds for all projects, for all
cache sizes. Moreover, we found that the eect size varies
little over all cache sizes for a given project.
Figure 4 shows the time-series line plot of density in
cached and un-cached les (again for 10% cache size and
LRU replacement policy). This gure highlights the right-
censoring [ 13] eect in our data. We use bug x information
to determine opened/closed bugs for a given sets of les. We
use our ( a posteriori ) knowledge of bug xes to calculate
the density of open bugs in the code at any given point in
time. As we near the end of our data collection phase, we
have fewer xes; this gradually reduces the number of known,
open bugs to zero. The cache, consequently, suers from a
diminishing defect density at the end of the run, which is
really an artifact of right-censoring, not an indication that
the performance of FixCache is getting worse.
There is a noteworthy, sudden spike in FixCache defect-
density and hit rate in July, 2007 for Apache. We did a case
study to understand this event. It turned out that on July 20,
2007, 15distinct les were linked to 7distinct bugs, resulting
in a total of 157 = 105 cache probes. This rapid succession
of defects in the same les induced a a dramatic boost in
the hit rate. Moreover, since each of these les had a large
number of opened bugs, we also observed a higher cache
defect density at that point. We observed similar (though
less pronounced) sudden hit rate spikes for other projects
as well. This suggests that FixCache may yield a higherreturn in response to certain events, while performing less
well at other times.
Though les resident in FixCache have higher density,
one might argue that inspecting the entire set of les in the
cache during each bug x commit would induce redundant in-
spection eort, since some les would be inspected repeatedly.
Rather, we could prefer to inspect les that are added or
changed (we refer to such les as churned les ) since the last
inspection (the immediate prior bug x). Thus we can ask,
how well does FixCache work if we only inspect the churned
les? This immediately raises the question of FixCache
ecacy when considering only churned les. We therefore
compare defect density of churned cached les against the
defect density of churned un-cached les.
Cached Uncached0.000 0.002 0.004 0.006 0.008 0.010ApacheDefectDensity
Cached Uncached0.00 0.01 0.02 0.03 0.04LuceneDefectDensity
Figure 5: Defect Density boxplot in cached and un-
cached churned (newly added or changed) les (10%
cache size, LRU replacement) for Apache and Lucene.
RQ2b: Do the churned les in the cache have higher defect
density than churned les not in the cache?
Figure 5 shows the boxplot of the calculated defect density
of cached and un-cached churned les at 10% cache size
for the LRU replacement policy. Not only the eect size
dramatically diminished, it actually ipped for some projects,
under some parameter settings. The Evolution project shows
a lower defect density of cached les even at the typical
10% cache size. Moreover, Nautilus also shows lower defect
density of cached les at all but the 10 and 18% cache size.
Eect size is very small across the board; even so, some
projects, for some parameter settings, show a statistically
signicant greater defect density for churned les.
Next, we evaluate the eect of each cache-replacement
policy on the defect density achieved in FixCache .
Preload Churned Spatial Temporal0.000 0.002 0.004 0.006ApacheDefectDensity
Preload Churned Spatial Temporal0.000 0.005 0.010 0.015 0.020LuceneDefectDensity
Figure 6: Defect Density by loading reason in cached
and un-cached les for Apache and Lucene.
RQ3: Do dierent sources of cache updates have dierent
levels of inuence on cache performance?Figure 6 shows the defect density as contributed by dier-
ent sources of cache updates.
As is apparent from the gure, temporal locality is the
highest contributor of defect density. We disregard the initial
preloading of large les as we observe that, across all projects,
they are evicted from the cache early in the run. Wilcoxon
signed rank tests were used to check whether temporal local-
ity consistently yields greater defect density across all cache
sizes. We did nd that temporal locality, for various cache
sizes, and across all projects, exhibits higher defect density
than spatial locality with only a few exceptions: Apache at 18
and 25% and Nautilus at 25% cache sizes. When compared
to churn locality, however, temporal locality may not yield
better performance.
This immediately raises a question: are temporally local
entities are competing for cache space with spatially local
or churn-local entities? In terms of defect density, can we
do better if we selectively enable/disable the other sources
of cache updates? This is a challenging question to answer
as enabling and disabling dierent policies will change both
the number of les and the number of lines in the cache.
Consequently, we have to take into account the total number
of lines to be inspected, as well as the open bug density. We
use the aforementioned cost eectiveness measure aucec 20
to consider this question.
To use aucec 20, we require a prediction model to provide
an ordering in which the les should be inspected. As speci-
ed, the FixCache algorithm provides only a gross ordering
of les, i.e., les in the cache should be inspected prior to
les not in the cache. To compare FixCache to other al-
gorithms we tried various orderings to nd the one most
favorable to FixCache , but, that only used data available
to practitioners at the time of prediction. Specically, we
order the les within the cache and those not in the cache
independently using the same ordering: order les rst by
decreasing number of closed bugs, then by increasing size.
We found that this ordering provided the best performance
forFixCache in every setting. Given this ordering, we can
then pick o les from the top of the ordering until we reach
just over 20% of the overall line count in the system.
DSDC DSEC ESDC ESEC0.00 0.05 0.10 0.15 0.20 0.25Apache Cost Effectiveness
DSDC DSEC ESDC ESEC0.00 0.05 0.10 0.15 0.20 0.25LuceneCost Effectiveness
Figure 7: aucec 20cost eectiveness after enabling/dis-
abling spatial/changed le locality ( 10%cache size, LRU
replacement, 20% of lines inspected) for Apache and
Lucene. EC/DC refers to enable/disable churn locality;
ES/DS refers to enable/disable spatial locality. DSDC
means disable both; all other combinations are labeled
similarlyRQ4a: Do spatial and churn locality update policies improve
the cost-eectiveness of FixCache ?
Figure 7 shows the aucec 20measure after enabling and
disabling spatial and churn locality.
In Figure 7, we use DS/ES to denote \Disable/Enable Spa-
tial locality"and DC/EC for\Disable/Enable Churn locality".
DSDC denotes \both disabled", while DSEC denotes \dis-
able spatial, enable churn". This gure makes it abundantly
clear that from the perspective of aucec 20cost-eectiveness,
adding spatial and churn locality to the FixCache update
policies provides little benet.
We want to show that there is little to no practical dier-
ence in cost eectiveness across the dierent locality settings.
The Kruskal-Wallis test is a non-parametric test which assess
whether nindependent samples are drawn from the same
population. It is the non-parametric counterpart to a one-
way Anova[ ?]. This test shows that for every project, there
is no signicant dierence in cost-eectiveness observed when
enabling or disabling churn and/or spatial locality. Thus,
although Figure 6 suggests that churn and spatial locality
potentially have some defect density to contribute, and may
in fact cache additional les, they do not provide any ben-
et for inspection when inspecting 20% or fewer lines, (as
peraucec 20). This clearly indicates that temporal locality
(which caches les when defects are found) underlies most of
the predictive power in FixCache .
Finally, we focus on dierent cache replacement policies
inFixCache and compare their performance.
LRU Density Clo sed Bugs Naive0.00 0.05 0.10 0. 15 0.20 0.25 0.30ApacheCost Effectiveness
LRU Density Clo sed Bugs Naive0.00 0.05 0.10 0. 15 0.20 0.25 0.30LuceneCost Effectiveness
Figure 8: aucec 20values for Apache and Lucene. The
left 3 are FixCache with dierent replacement policies,
and right most is a naive prediction model, which simply
order les by closed bug count. Only in Apache (of all)
are any of the FixCache version statistically better than
the naive model, and even then the eect size is quite
small
RQ4b: Can we improve FixCache aucec 20performance
by using a dierent cache replacement policy?
The original FixCache used two dierent policies. The
rst is an LRU based replacement policy, which keeps fre-
quently hit les in the cache. The principle here is that les
that haven't had bug xes in a while are probably unlikely
to have latent bugs. The second was a \closed bugs" policy,
which replaced les with the least number of closed bugs
rst. For our evaluation, we added a third policy based on
defect density,which replaced the le with the least density of
closed bugs. The latter two policies are based on the theory
that les with fewer bugs, or lower defect density, are likely
to have fewer latent bugs. All of these policies prima facie
appear to have merit, so it is reasonable to compare them to
see which policies yield better aucec 20performance.The results are shown as the left most 3 boxplots in the
two plots in Figure 8
For this question, we ran FixCache with cache size set to
6%,10%,18% and25% of the total le count, and consis-
tently obtained results with all projects similar to those seen
in Figure 8. The closed bugs policy generally showed the best
performance, while the density policy was consistently the
worst performer. This nding again, is strongly indicative
that the raw count of closed bugs in a le is an excellent
indicator of future latent bugs.
Our nal question is in some sense the most critical evalu-
ation of FixCache . Does FixCache perform better than a
naive prediction model?
RQ5: When used for inspections, is FixCache doing better
than a naive model that simply picks previously buggy les?
For this question we chose a very simple, naive prediction
model: the model simply orders all the les in the system by
the number of closed bugs , and chooses enough les in order
to capture 20% of the total line count. The results are shown
as the rightmost boxplot in Figure 8.
Testing the hypothesis that the best FixCache replace-
ment policy ( vizclosed bugs) is better than the naive model,
we nd that, after Benjamini-Hochberg correction, only the
Apache project data rejects the null hypothesis. However, as
can be seen in Figure 8, even in this case, the eect size is
fairly small.
This leads to a rather surprising, conclusion:
The naive model is actually about the same utility for
inspections, under the aucec 20measure, as the best
possible setting for FixCache .
6. THREATS TO V ALIDITY
Perry et al. [17] identify three forms of validity that must
be addressed in research studies. We now examine threats
to each form of validity in our study and the methods used
to mitigate these threats where possible.
Construct validity refers to the degree to which the mea-
surements are actually related to the concepts they are be-
lieved to represent. The main threat to construct validity
come from bias in bug reporting. The bugs that are being
reported might not be representative of actual bugs encoun-
tered in the system. Linking bias is certainly extant in the
datasets, and can aect [ 5] the performance of FixCache ,
and aect our evaluations of FixCache and the merits of dif-
ferent update policies (RQ1-4). However, it is less likely that
the result of comparing FixCache and the simple standard
prediction model (RQ5) is aected by bias.
Internal validity is the ability of a study to establish a
causal link between independent and dependent variables
regardless of what the variables are believed to represent.
Our study is focused on examining a link between variables
such as closed bugs per le, the FixCache prediction and the
actually open bugs in a le. Threats to internal validity come
from our choice of evaluation. We attempted to address this
threat by using dierent, specically tailored measures for
dierent RQs: lines for RQ1, density for RQs 2a, 2b, and
3, and the very demanding aucec for the rest. We chose
aucec at 20% of the lines, because it closely approximates
the linecount of the les in FixCache running with a cache
size of 10%.External validity refers to how these results generalize. Our
study includes multiple projects, belonging to dierent soft-
ware domains, and we nd similar results among all projects
studied. However, the sample size is only ve projects. While
this gives evidence of the ability of our results to generalize,
further study on more projects will increase our condence
in these ndings as answers to the research questions on a
broad level. In particular we are concerned about projects
that do not follow development processes similar to those of
the projects that we studied.
7. RELATED WORK
Kim et al. took their inspiration from Hassan et al. who
proposed the cache metaphor drawing on operating systems
research [ 10]. BugCache principally diers from Hassan
et al. 's \Top Ten List" in granularity: The top ten list con-
tains modules, while BugCache contains les. BugCache
performed better, even at the ner granularity. Our work
mirrors this dierence; we empirically evaluate the perfor-
mance of FixCache, at line granularity , for prioritizing en-
tities for inspection. In the course of this investigation, we
compare FixCache to simple prediction models with an eye
toward improving FixCache. Thus, there are two strands of
work related to ours | FixCache evaluations and prediction
models.
FixCache Evaluations Wikrstrand et al. used FixCache
to dynamically select regression tests that run over modied,
fault-prone (in cache) les. They found that in an embedded
industrial system, the FixCache reached a weekly hit rate of
50{80% [23]. The study found that FixCache pre-population
did not aect its hit rate and observed that daily updates
quickly negate any pre-population eects. Engstr omet al.
follow up Wikrstrand et al. 's and evaluate the eectiveness
of Wikrstrand et al. 's algorithm at selecting regression tests.
They found the x-cache approach to be more ecient than
its predecessor at selecting regression tests in four case stud-
ies [ 7]. Their work principally diers from ours in their
focus on regression testing. Sadowski et al. investigated how
FixCache's prediction performance varies over time and how
much the set of buggy les changes [ 18]. They report that the
hit rate is relatively stable and that, Apache httpd , the initial
exception, stabilizes as the number of les approached the
current project size. They nd that most les that remain in
the cache are correlated with a high hit rate, but that there
is room for improvement. This evaluation diers from ours
in the questions posed and our focus on cost-eectiveness,
which is line-granular.
Prediction Models A large body of research into predic-
tion models exits. Catal and Diri [ 6] have published a cur-
rent survey. The Predictor Models in Software Engineering
(PROMISE) series of events at ICSE are a well-known venue
in this sub-discipline. The core idea is to build a statistical
model, usually some type of linear, logistic, or count model,
with either defect-proneness (binary response) or number
of defects (count or continuous response) as the dependent
variable. Predictors are measures such as size, number of
authors [ 22], churn [ 16], social network measures [ 14], com-
plexity [ 9], and a range of other measures. A full survey of
this prolic sub-discipline would consume many pages; for
our purposes here, it is sucient to note that we chose a
simple model as a straw-man for comparison, one that simply
ordered les by number of closed defects found in that le.8. CONCLUSION
In this paper, we evaluated the usefulness of FixCache
as a tool for focusing code inspection eort. We found that
FixCache tends to pull larger les into the cache; however,
the bug density of les in FixCache is generally higher. We
found, in addition, that of all the update policies in Fix-
Cache , temporal locality (recently closed bugs) contributed
the most to increase bug density in the cache; indeed, when
measured in terms of the demanding cost-eectiveness mea-
sureaucec 20, the other two policies, spatial and churn lo-
cality, did not contribute much. Furthermore, we compared
three dierent cache replacement policies, viz., based on LRU,
closed bug density, and closed bugs, and found that closed
bugs based replacement policy generally performs better in
terms of cost-eectiveness.
Perhaps most surprisingly, we found that a very simple
scheme | using the number of closed defects in a le to order
the les | provides a aucec 20performance level that is in a
statistical dead heat with the best performing conguration of
FixCache . While a priori this might seem like a discouraging
nding, it is in fact quite informative: this suggests that the
most important aspect of FixCache , if one were to use it for
inspection, is, in fact its temporal locality. Thus, the major
conclusion of our paper is this:
When used for inspection, FixCache gets most of its
power by predicting that les that recently experienced
a bug x, do in fact contain additional, latent bugs.
Finally, we emphasize that our evaluation targeted the
utility of FixCache for inspections . While the aucec 20
measures the eectiveness of FixCache or other ordering
algorithms when choosing 20% of the lines to inspect, it does
not really say much about other possible uses of FixCache ,
for inspections, static analysis, formal verication, etc.We
believe these are fruitful directions for further research.
References
[1]The economic impacts of inadequate infrastructure for
software testing,
www.nist.gov/director/planning/upload/report02-
3.pdf.
[2]E. Arisholm, L. Briand, and M. Fuglerud. Data min-
ing techniques for building fault-proneness models in
telecom java software. 2007.
[3]E. Arisholm, L. C. Briand, and E. B. Johannessen. A
systematic and comprehensive investigation of methods
to build and evaluate fault prediction models. Journal
of Systems and Software , 83(1):2{17, 2010.
[4]A. Bachmann and A. Bernstein. Software process data
quality and characteristics: a historical view on open
and closed source projects. In IWPSE-Evol '09 , 2009.
[5]C. Bird, A. Bachmann, E. Aune, J. Duy, A. Bernstein,
V. Filkov, and P. Devanbu. Fair and balanced?: bias
in bug-x datasets. In Proceedings of the the 7th joint
meeting of the European software engineering conference
and the ACM SIGSOFT symposium on The foundations
of software engineering , pages 121{130. ACM, 2009.[6]C. Catal and B. Diri. A systematic review of software
fault prediction studies. Expert Systems with Applica-
tions , 36(4):7346{7354, 2009.
[7]E. Engstr om, P. Runeson, and G. Wikstrand. An empir-
ical evaluation of regression testing based on x-cache
recommendations. In Software Testing, Verication and
Validation (ICST), 2010 Third International Conference
on, pages 75{78. IEEE, 2010.
[8]M. Fischer, M. Pinzger, and H. Gall. Populating a
release history database from version control and bug
tracking systems. In ICSM '03: Proceedings of the In-
ternational Conference on Software Maintenance , pages
23+, Washington, DC, USA, 2003. IEEE Computer
Society.
[9]A. E. Hassan. Predicting faults using the complexity of
code changes. In Proceedings of the 31st International
Conference on Software Engineering , ICSE '09, pages
78{88, Washington, DC, USA, 2009. IEEE Computer
Society.
[10]A. E. Hassan and R. C. Holt. The top ten list: Dynamic
fault prediction. In Proceedings of the 21st IEEE In-
ternational Conference on Software Maintenance , pages
263{272, Washington, DC, USA, 2005. IEEE Computer
Society.
[11]S. Kim, T. Zimmermann, K. Pan, and J. Jr. Auto-
matic identication of bug-introducing changes. In ASE
'06: Proceedings of the 21st IEEE/ACM International
Conference on Automated Software Engineering , pages
81{90, Washington, DC, USA, 2006. IEEE Computer
Society.
[12]S. Kim, T. Zimmermann, E. Whitehead Jr, and A. Zeller.
Predicting faults from cached history. In Proceedings of
the 29th international conference on Software Engineer-
ing, pages 489{498. IEEE Computer Society, 2007.
[13]J. Lawless and J. Lawless. Statistical models and meth-
ods for lifetime data. 1982.
[14]A. Meneely and L. Williams. Secure open source collab-
oration: an empirical study of linus' law. In Proceedings
of the 16th ACM conference on Computer and commu-
nications security , pages 453{462. ACM, 2009.[15]A. Mockus and L. Votta. Identifying reasons for software
changes using historic databases. In Software Mainte-
nance, 2000. Proceedings. International Conference on ,
pages 120{130. IEEE, 2002.
[16]N. Nagappan and T. Ball. Use of relative code churn
measures to predict system defect density. In Software
Engineering, 2005. ICSE 2005. Proceedings. 27th Inter-
national Conference on , pages 284{292. IEEE, 2005.
[17]D. Perry, A. Porter, and L. Votta. Empirical studies of
software engineering: a roadmap. In Proceedings of the
conference on The future of Software engineering , pages
345{355. ACM New York, NY, USA, 2000.
[18]C. Sadowski, C. Lewis, Z. Lin, Z. X., and E. J. White-
head. An empirical analysis of the xcache algorithm.
InProceedings of the Eigth MSR Conference (to appear) ,
2011.
[19]J.Sliwerski, T. Zimmermann, and A. Zeller. When do
changes induce xes? In MSR '05: Proceedings of the
2005 international workshop on Mining software reposi-
tories , pages 1{5, New York, NY, USA, 2005. ACM.
[20]D.Cubrani c and G. C. Murphy. Hipikat: recommending
pertinent software development artifacts. In ICSE '03:
Proceedings of the 25th International Conference on
Software Engineering , pages 408{418, Washington, DC,
USA, 2003. IEEE Computer Society.
[21]Z. Wang. Fix Cache Based Regression Test Selection.
Master's thesis, Computer Science, Chalmers University
of Technology, Sweden, 2010.
[22]E. Weyuker, T. Ostrand, and R. Bell. Do too many
cooks spoil the broth? Using the number of developers
to enhance defect prediction models. Empirical Software
Engineering , 13(5):539{559, 2008.
[23]G. Wikstrand, R. Feldt, J. Gorantla, W. Zhe, and
C. White. Dynamic regression test selection based on a
le cache an industrial evaluation. In Software Testing
Verication and Validation, 2009. ICST'09. Interna-
tional Conference on , pages 299{302. IEEE, 2009.