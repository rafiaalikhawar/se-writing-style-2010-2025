Angelic Debugging
Satish Chandra
IBM Research
satishchandra@us.ibm.comEmina Torlak
IBM Research
etorlak@us.ibm.comShaon Barman
UC Berkeley
sbarman@cs.berkeley.edu
Rastislav Bodik
UC Berkeley
bodik@cs.berkeley.edu
ABSTRACT
Software ships with known bugs because it is expensive to pinpoint
and ﬁx the bug exposed by a failing test. To reduce the cost of bug
identiﬁcation, we locate expressions that are likely causes of bugs
and thus candidates for repair. Our symbolic method approximates
an ideal approach to ﬁxing bugs mechanically, which is to search
the space of all edits to the program for one that repairs the failing
test without breaking any passing test. We approximate the expen-
sive ideal of exploring syntactic edits by instead computing the set
of values whose substitution for the expression corrects the execu-
tion. We observe that an expression is a repair candidate if it can
be replaced with a value that ﬁxes a failing test and in each passing
test, its value can be changed to another value without breaking the
test. The latter condition makes the expression ﬂexible in that it
permits multiple values. The key observation is that the repair of a
ﬂexible expression is less likely to break a passing test. The method
is called angelic debugging because the values are computed by an-
gelically nondeterministic statements. We implemented the method
on top of the Java PathFinder model checker. Our experiments with
this technique show promise of its applicability in speeding up pro-
gram debugging.
Categories and Subject Descriptors
D.2.5 [ Software Engineering ]: Testing and Debugging
General Terms
Testing, Debugging, Validation
Keywords
Angelic non-determinism, debugging, symbolic execution, tests
1. INTRODUCTION
Software producers often ship code with known faults that are
deemed too expensive to ﬁx and unlikely to occur in practice. Iso-
lating causes of these faults from failing tests is a labor-intensive
process, with considerable costs. Given that compute cycles are
abundant and inexpensive, judicious use of automation for fault
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
ICSE ’11, May 21–28, 2011, Waikiki, Honolulu, HI, USA
Copyright 2011 ACM 978-1-4503-0445-0/11/05 ...$10.00.isolation could greatly improve software quality, as well as reduce
costs.
In this paper, we propose a computer- assisted debugging method-
ology, in which value-based reasoning is used to identify plausi-
blerepair candidates in a faulty program. We consider only those
faulty programs in which the defect can be repaired by altering at
most one expression. A repair candidate is an expression that can
be modiﬁed to correct the failing test(s) without breaking any pass-
ing tests. In principle, we could identify repair candidates by ex-
ploring (a subset of) valid syntactic variations of all expressions in
the program. This, however, tends to be too computationally de-
manding in practice.
Our idea is to approximate syntactic changes with value changes.
Instead of checking if the program can be ﬁxed by replacing an
expressionewith some expression e0, we check whether a failing
test can be repaired by replacing the value vofewith some value
v0. Ifv0ﬁxes the test, we assume it is possible to ﬁnd an expression
e0that evaluates to v0. If no suchv0exists, we can ﬁlter out efrom
the set of repair candidates.
Of course, the value-replacement check based only on a failing
test may not be able to ﬁlter out repair candidates that could break
some of the passing tests. Our approach takes into account the
effects of repairing einpassing tests as follows: if eevaluates tow
in a passing test, but the same test continues to pass if wis replaced
with a value w06=w, then we have a level of conﬁdence that ecan
be replaced with a syntactically different expression e0(as we shall
see later, this is an approximate test).
In summary, if replacing an expression ewith a value vﬁxes the
failing test, then repairing ehas the potential to ﬁx the failing test.
If replacing ewith a value vdifferent than ecomputes does not
break a passing test, then we have some freedom to change e; we
say that it is ﬂexible and a good repair candidate.
Our methodology is called angelic debugging and comprises the
following steps:
1. The programmer—based on intuition, observation of runtime
logs, or feedback from fault localization tools (e.g., Taran-
tula [14])—demarcates a scope in the code where he believes
the defect might lie. By selecting a scope, the programmer
expresses the hypothesis that altering some expression within
the scope can resolve the bug.
2. The computer tries to validate this hypothesis. It automat-
ically examines all expressions in the scope to ﬁnd plausi-
ble repair candidates. For each such candidate, the com-
puter demonstrates the existence of one alternative value that
would have rescued the failing test case.
3. The computer then factors in information from any passing
test cases to automatically eliminate inﬂexible repair candi-1public int classify( inta,intb,intc) {
2 intretval;
3 if(a >= b && b >= c) {
4 if(a == c || b == c) {
5 if(a == b && a == c)
6 retval = EQUILATERAL;
7 else
8 retval = ISOSCELES;
9 }else {
10 if(aa != b b + cc) {
11 if(aa < bb + cc) {
12 retval = ACUTE;
13 }else {
14 retval = OBTUSE;
15 }
16 }else {
17 retval = RIGHT;
18 }
19 }
20 }else {
21 retval = ILLEGAL;
22 }
23 return retval;
24 }
Figure 1: Triangles example from DeMillo [8].
dates. A repair candidate is inﬂexible if changing the candi-
date expression can rescue a particular failing test, but any
change would break one of the previously passing tests. If
all repair candidates are eliminated in this step, then the pro-
grammer has either not chosen the scope correctly, or there
is indeed no ﬁx to the program that can pass all test cases.
4. The programmer attempts to devise a suitable replacement
expression for each of the remaining repair candidates, one at
a time, until one works. A successful replacement expression
must give passing results on the hitherto failing input, as well
as on the previously passing inputs. The search for such a
replacement expression is not computer assisted, as the space
of possible syntactic expressions is unbounded. However, the
programmer can draw inspiration from the sample alternate
value demonstrated by the second step.
Example .Consider the code in Figure 1. It is a slightly modiﬁed
version of an example from the paper by DeMillo et al. [8] on mu-
tation testing. The program takes as input the lengths of three sides
of a triangle, presented in non-decreasing order, and it is supposed
to classify the triangle as one of equilateral, isosceles, right angled,
acute, or obtuse.
Table 1 shows eight sample inputs to the program, also taken
from the DeMillo paper. The code works correctly for inputs T1-
T7, but not for T8.
Suppose the program is known to be ﬁxable, in the speciﬁc sense
that there is a single expression that, if altered, will let all of the
test inputs pass. It is the task of the programmer to ﬁnd the faulty
expression and repair it.
In the ﬁrst step, the programmer has to hypothesize a scope. We
assume that the entire program is in scope. In the second step, the
computer ﬁgures out the repair candidates for the failing test T8.
These are shown in Table 2. Each repair candidate could have sal-
vaged this particular execution by evaluating to a different value
than it did. The column titled “alt value” lists the alternate value
that would have made T8 pass. For example, the ﬁrst row in the ta-
ble says that if the value 7 were to be produced in place of the high-lighted expression a, whose runtime value is 26, T8 would pass.
For brevity, we have considered only reads of local variables and
constants, but there is no conceptual reason to exclude arithmetic
and logical expressions (e.g., the result of a == c ). Among the
kinds of expressions considered, these are the only possible repair
candidates.
The third step uses the passing test cases to ﬁlter out the in-
ﬂexible repair candidates, leaving the right choice and one wrong
choice. This is shown in the last column of Table 2. For exam-
ple, the last row says it is no good changing the value of ACUTE at
line 12 to ISOSCELES ; doing so will rescue T8, but break test T6,
whose only correct outcome is in fact ACUTE . This much would be
obvious to a programmer. The other two choices that our technique
eliminates from consideration on the basis of test T3 may not be
immediately obvious.
Given the information in Table 2, a programmer would consider
each of the remaining two candidates in turn, and try to ﬁnd a syn-
tactic replacement that produces an acceptable value for T8, and
moreover, works for all the other test cases (T1-T7) as well. (Note
that the concrete values that our technique suggests are speciﬁc to
T8.) Of these two candidates, the ﬁrst choice is in fact futile, but
we cannot get that information from the passing tests. The second
choice is the real bug: it should be bin place of c.
We call our methodology angelic debugging, because it relies on
angelic nondeterminism. Imagine that an expression is replaced
with a call to an oracle that returns a value, chosen nondeterminis-
tically. The oracle is both omniscient and co-operative, in the sense
that if a suitable value exists—one that makes the failing input now
pass—the oracle is guaranteed to return it. While supporting an-
gelic nondeterminism is computationally expensive, it is becoming
increasingly viable for selective use, thanks to recent advancements
in decision procedures. In our case, angelic non-determinism is
used only on one expression at a time. This limits the kinds of
defects on which our technique is effective, but enhances computa-
tional feasibility. Nondeteminism has long been used in testing to
create an adversary (a demonic oracle), but its use as an ally is less
common.
A key novelty in this work is in how we use information from
passing tests to ﬁlter out repair candidates that will not work out.
While the programmer still needs to go through the laborious task
of ﬁnding replacement expressions for multiple repair candidates,
he does not need to carry out the Sisyphean effort in looking for
replacement expressions for repair candidates that are “doomed” by
another test case. Without ﬁrst positing a replacement expression,
he would not know whether it would work out or not, and the space
of replacement expressions can be huge.
Previous work on fault localization that has leveraged informa-
tion from a suite of passing tests has relied on indirect information.
The basic idea is that a failing run has an anomalous behavior in
Test a b c Expected outcome
T1 212 27 ILLEGAL
T2 5 4 3 RIGHT
T3 26 14 14 ISOSCELES
T4 19 19 19 EQUILATERAL
T5 9 6 4 OBTUSE
T6 24 23 21 ACUTE
T7 7 5 6 ILLEGAL
T8 26 26 7 ISOSCELES
actual outcome: ACUTE
Table 1: Inputs for Fig. 1line# code alt value (T8) inhibitor
4if (a== c || b == c) { 7
4if (a == c|| b == c) { 26
4if (a == c || b== c) { 7 T3
4if (a == c || b == c) { 26 T3
12 retval = ACUTE ISOSCELES T6
Table 2: Repair candidates for Fig. 1.
some regard, when compared to passing runs. The anomaly in be-
havior could be executing statements that are rarely, if ever, exe-
cuted by passing tests; or it could be violating invariants that have
been discovered to hold during passing executions. This informa-
tion, while useful in fault localization, does not directly answer the
question “Can I change a speciﬁc expression?” We believe that for
debugging applications, this is the more important question.
We have built a tool, A NGELINA , that supports angelic debug-
ging. It has been built on top of the J AVA PATHFINDER model
checker [15]. A NGELINA accepts as input a Java program, a fail-
ing test case, and a suite of passing test cases. The default scope is
the entire program, but a programmer can specify scopes at various
granularities: a class ﬁle, a method, or a range of lines. A NGELINA
outputs information of the kind shown in Table 2. A NGELINA takes
no speciﬁcation or annotation; it bases its ﬁndings only on the input
mentioned above.
Remarkably, A NGELINA not only ﬁnds faulty arithmetic and log-
ical expressions, but also faulty heap references for which it sug-
gests alternative heap values: this feature is crucial for handling
typical bugs in Java programs.
We have used A NGELINA on several small examples, and one
large Java application taken from the SIR repository [24].
Contributions .This paper makes the following contributions:
1. We propose the angelic debugging methodology, in which
we take advantage of plentiful compute cycles—instead of
the programmer’s time—to answer questions about where to
ﬁx a bug in a program.
2. We present a novel way of leveraging the information in
passing test inputs to eliminate non-productive repair can-
didates.
3. We have implemented the automated parts of this method-
ology in a tool, and have evaluated the tool on a number of
small and one large Java application.
Outline .The rest of the paper is organized as follows. Section 2
presents technical details of computing a ﬁltered list of repair can-
didates. Section 3 describes how angelic execution is implemented
in A NGELINA . Section 4 presents an evaluation of the tool, and
Section 5 presents related work.
2. OVERALL APPROACH
In this section, we present a technical overview of angelic de-
bugging. For expository reasons, we assume that each expression
has a single dynamic occurrence; later sections will clarify how we
deal with loops and procedure calls in practice. Hereafter, when we
refer to an expression e, we mean a speciﬁc occurrence of ein the
program.
Our approach uses angelic execution of a program. In angelic
execution, a selected expression in the program is interpreted as a
&&>=>=abbc1234567||====acbc891011121314
line 3line 4ACUTE27line 12(Not shown expressions for  lines 10 and 11)Figure 2: Portion of the AST from Figure 1
query to an angel, whose job is to return a value—if one exists—
that would make the program pass. We denote P[=e]as the pro-
gram obtained by replacing the occurrence of einPby a symbolic
variable. For a given expression eand inputI,AngelicTest is
deﬁned as follows:
AngelicTest (P;I;e ) =9:Test(P[=e];I)
The expression Test(P;I)above is true if interpreting PonIpro-
duces the expected outcome (provided with I) within a bounded
amount of time. It is false otherwise.
Recall that a repair candidate is an expression that, if replaced by
another expression, can rescue a failing run. An initial set of repair
candidates can be speciﬁed as follows:
fejAngelicTest (P;If;e)g
To obtain an initial list of repair candidates, our technique carries
out angelic execution separately for each expression evaluated on
the path taken by the failing execution triggered by input If. (Of
course, once an angelic value is obtained, the program may proceed
along a different, successful path if one exists.) For compound ex-
pressions, the subexpressions are tested for being repair candidates
as well. Figure 2 shows the various expressions that are on the path
executed by the failing input for the program in Figure 1. For each
ei, whereiranges from 1 to 27, we invoke AngelicTest (P;If;ei)
to compute the set of repair candidates. This process identiﬁes the
ﬁve repair candidates mentioned in Table 2, as the other expressions
cannot rescue the failing run no matter what value they return.
Since each expression on the faulty path is considered for angelic
valuation, all potential repair candidates will indeed be found. (Var-
ious static analyses can be applied to optimize this process.) Ap-
plying AngelTest will also yield a concrete value for each repair
candidate that would rescue the given test input.
Next, we leverage the passing test inputs, if any. Given a repair
candidatee, and a passing test input Ip, could the program still pass
if you substitute ewith a different value than what eevaluates to on
this input? If the answer is no, it is not a good repair candidate.
This question can also be answered by angelic execution, via the
ﬂexibility check:
FlexTest (P;I;e ) =9:(Test(P[=e];I)^6=Eval(P;I;e ))
We deﬁne Eval(P;I;e )as the value obtained by evaluating the
expressionein the course of interpreting the program Pon theinputI. Ifeis not visited while executing PonI,Eval(P;I;e )is
undeﬁned and FlexTest (P;I;e )is trivially true.
Given a failing input Ifand passing inputs Ip1:::I pk, the good
repair candidates are:
fejAngelicTest (P;If;e)^ 8i2[1::k]:FlexTest (P;Ipi;e)g
For example, consider the last row of Table 2. The only passing
test case in which line 12 is visited is T6, so it is pertinent to ask
the question only for T6. T6 can pass only when the value of retval
at line 12 is ACUTE ; no other value would let T6 pass. Hence, T6
rules out that repair candidate. Line 12 sets the (constant) return
value, which obviously is not the right ﬁx and, the ﬂexibility test
discovers that it is not the right ﬁx. In other cases, inﬂexibility
might not be immediately obvious, e.g., the repair candidates ruled
out by T3.
We note that there is no guarantee that for every “good” repair
candidates that our technique produces, there exists a suitable re-
placement expression (i.e., one that would work out for the failing
as well as all the passing tests.) The value of our technique is in
eliminating from consideration those repair candidates for which
no suitable replacement expression is likely to exist.
Rationale for FlexTest .If an expression ein the program is the
right repair candidate, it must be ﬁne to replace ewith some as-
yet-undetermined expression e0. The expression eis not the right
repair candidate if replacing it with anyexpressione0will make
one or more of the passing test cases fail. Can we take advantage
of passing test cases without knowinge0?
The ideal ﬂexibility question that we want to ask is this:
(Ideal ) Given a repair candidate—i.e., an expression
e—and given a passing test case T, is it possible for
Tto continue to pass if we replace eby a different
expressione0?
If the answer is no, then we do not consider eto be the place to
ﬁx, because it can break T(and we are allowed only one ﬁx to the
program.)
Since the ideal question is too expensive to answer, FlexTest car-
ries out a more readily answerable, but approximate check. It asks
the following question on values :
(Approximate ) Given a repair candidate e, and given a
passing test case T, suppose that eproduces the value
wwhenTis run. Is is possible for Tto continue to
pass ifeis replaced by some w0wherew6=w0? IfT
does not encounter e, the check returns true by default.
From the perspective of a passing test, if the answer to the ap-
proximate question is yes, then the answer to the ideal question is
also yes. On the other hand, if the answer to approximate question
is no, then the answer to the ideal question is probably no; but it is
not guaranteed to be negative. Consider the following example:
t = x + y
Suppose that eisy;e0is 1; and tmust be x+1later in the execution.
Then, on a passing test case for which yhappens to be exactly 1
at this point in the code, the approximate check would say it is
inﬂexibile. The ideal check, however, would have allowed e0.
We believe that in practice this is not a problem; indeed, we have
not found this to be a problem in using an implementation of this
idea on various examples. Since the error is “one sided”, we can
reduce the mathematical likelihood of being wrong—eliminating arepair candidate unnecessarily—by relying on many passing tests,
together with making the notion of ﬂexibility from a Boolean to
a quantitative property that would estimate the likelihood that an
expression is ﬂexible.
Limitations .If the desired ﬁx to a program is a single expression
that needs to be altered—or any one of a set of single expressions—
our technique will eventually ﬁnd it. We call such programs 1-
ﬁxable .
If a program is not 1-ﬁxable, then our technique can suggest a
repair candidate that is not the desired ﬁx, but that can often in-
spire the actual ﬁx. For example, if the program is missing a cer-
tain assignment to a ﬁeld, then a ﬁx might be suggested at a point
downstream where the ﬁeld is used (assuming there is just one such
use.)
Our technique also inherits the limitations of constraint solvers
used in the implementation of the angelic nondeterminism. It is
well known that real programs can defeat the capabilities of cur-
rent generation constraint solvers. Supplementing the solver with
dynamic execution will sidestep these problems, potentially at the
cost of reducing scalability.
3. ANGELIC EXECUTION
There are many ways to realize the angelic execution environ-
ment described in the previous section (e.g., [5]). Our tool, A N-
GELINA , employs a symbolic execution [18] engine that is based
on an explicit-state model checker [15]. It implements the func-
tions AngelicTest andFlexTest in three steps.
In the ﬁrst step, we execute the failing test concretely, collect-
ing the labels of all executed statements in a trace f. Each label
infgives a unique identity to a dynamically occurring bytecode
instruction. It can be thought of as the bytecode index that an ex-
ecuted instruction would have if all loops in the program were un-
wound and all method calls inlined.
In the second step, the failing test is re-executed jfjtimes to
ﬁnd all statements in fthat are potential repair candidates . The
ithexecution in this phase is concrete until it reaches the ithstate-
mentf[i], at which point we replace the concrete value returned
byf[i]with a symbolic value si. From then on, the values of all
branch predicates that depend on the output of f[i]are encoded
as constraints over si. Section 3.1 describes this process in more
detail. If there is a path through the program in which all branch
constraints are satisﬁed, and which does not lead to a failure, then
we have found a value that ﬁxes the failing test and f[i]is placed
in the repair candidate set R.
The third step of the analysis uses the available passing tests to
pruneR. Each passing test is ﬁrst executed concretely to collect
a tracep. Next, for each statement p[i]that is inR, we execute
the preﬁxp[0::i]ofpconcretely. This includes the statement p[i],
whose concrete execution produces the concrete value ci. We then
replaceciwith a symbolic value siand search for a failure-free
path through the program in which siis different from ci. If such
a path is found, we say that p[i]isﬂexible and keep it as a repair
candidate. Otherwise, p[i]isinﬂexible , and we remove it from R.
3.1 Symbolic execution
Symbolic execution of integer programs .To execute a program
in which an integer-valued expression is treated symbolically, our
tool employs the classic approach [18] of searching for a failure-
free path in the program’s symbolic execution tree . An example of
such a tree is shown in Fig. 3. The nodes in the tree represent pro-x = 1;
1if(x > 0)
2sgn =  1;
3else if (x < 0)
4sgn =  1;
5else
6sgn = 0;
7assert xsgn >= 0;
x: XPC: truex: XPC: X > 0x: X, sgn: -1PC: X < 0x: X, sgn: 0PC: X = 0x: XPC: X ! 0
x: X, sgn: -1PC: X < 0 !        X"-1 " 0x: X, sgn: -1PC: X < 0 !        X"-1 < 0x: X, sgn: 0PC: X = 0 !        X"0 " 0x: X, sgn: 0PC: X = 0 !        X"0 < 0falsefalse777746
x: X, sgn: -1PC: X > 0 !        X"-1 " 0x: X, sgn: -1PC: X > 0 !        X"-1 < 0falsesatisﬁablesatisﬁablesatisﬁable11
77x: X, sgn: -1PC: X > 02
x: 1PC: truex: XPC: X > 0x: 1, sgn: -1PC: X ! 0 ! 1 < 0x: 1, sgn: 0PC: X ! 0 ! 1 # 0x: XPC: X ! 0
x: 1, sgn: 0PC: X ! 0 !        1"0 " 0x: 1, sgn: 0PC: X ! 0 !        1"0 < 0infeasible7746
x: 1, sgn: -1PC: X > 0 !        1"-1 " 0x: 1, sgn: -1PC: X > 0 !        1"-1 < 0infeasibleAssertionErrornormal termination11
77x: 1, sgn: -1PC: X > 02infeasible
Figure 3: A symbolic execution tree for an integer program. The tree represents a symbolic execution of the sample program in
which the concrete value of xon line 1 is replaced with the symbolic value X.
gram states , and the edges are transitions between states. Each state
includes the (symbolic or concrete) values of program variables; a
path condition (PC); and a program counter. The path condition
is a boolean formula over symbolic values that encodes the con-
ditions under which a given execution path can be taken; i.e., the
path condition is satisﬁable only along feasible paths. The program
counter, shown as the edge label, speciﬁes the next statement to be
executed from the given state.
The symbolic execution tree on the right of Fig. 3 corresponds
to the buggy implementation of the signum function shown on the
left. The tree illustrates an execution of the program in which the
concrete value of the variable xon line 1 is replaced with the sym-
bolic valueX. Note that all other occurrences of xremain concrete.
The root of the tree represents the initial state of the execution, in
which xhas the concrete value 1 and the path condition is true.
At each branch point, the PC is updated to reﬂect which path is
taken. For example, evaluating the conditional on line 1 and taking
the true-branch leads to the left child of the root, in which xhas
the symbolic value Xand the path condition is X > 0. Whenever
the path condition becomes false, or we reach an error state, the
execution along that path terminates, and the search backtracks. In
this example, A NGELINA explores three different paths (two infea-
sible and one erroneous) before reaching the highlighted passing
state. The remaining infeasible path is shown for completeness;
the search terminates as soon as the highlighted state is discovered.
Symbolic execution of heap-manipulating programs .To ana-
lyze a program in which a reference value is symbolic, we employ
a variant of the generalized symbolic execution algorithm by Khur-
shid et al. [17]. The algorithm works like the basic symbolic exe-
cution described above, except when it encounters a ﬁeld access to
a symbolic reference s. In this case, we perform a lazy concretiza-
tion ofsand proceed as before. The lazy concretization process
involves choosing a concrete value for s; recording that choice in
the path condition; and, if the updated path condition is satisﬁable,
replacing all occurrences of swith the chosen concrete value. In
our framework, the possible concrete values for sinclude null; an
existing concrete reference of the same type as s; and a reference to
a new object (also of the same type as s) with all of its ﬁelds initial-
ized to fresh symbolic values. In contrast to previous work [17, 21]
where only certain aliasing relationships are considered, our choice
of concrete values accounts for all aliasing that smay exhibit.Figure 4 shows the generalized symbolic execution tree for the
buggy heap-manipulating program displayed on the left. The tree
characterizes the execution of the program in which the reference
produced by the new expression on line 2 is treated symbolically.
Each program state (i.e., tree node) is shown together with its cor-
responding heap, as indicated by the gray shading. An object in
the heap is represented by a table that maps ﬁeld names to values.
Symbolic values are italicized, and concrete references are repre-
sented by arrows.
Executing the ﬁrst two lines of the program results in the cre-
ation of concrete objects N1andN2, whose ﬁelds are initialized to
default values. To avoid cluttering the ﬁgure, we pretend that the
execution of line 2 has no effect on the heap; it simply introduces a
new symbolic reference N2to which the variable n2is bound. Line
3 sets the “next” ﬁeld of N1to the symbolic reference N2. Line 4
dereferences n2, which triggers the concretization of N2. Setting
N2tonullleads to a null pointer exception, causing the search to
backtrack. The next choice, N1, leads to an infeasible path and an
assertion failure. (Note that the conditional on line 4 evaluates to
true in this case.) The last choice—setting N2to a fresh object
N3with symbolic ﬁeld values—results in a failure-free path. The
remaining erroneous paths are shown for completeness.
3.2 Implementation
ANGELINA is implemented on top of the J AVA PATHFINDER
(JPF) model checker [15]. JPF is a Java Virtual Machine that is
capable of bactracking to a given point in the execution and pro-
ceeding along a different path. Our implementation is patterned
after the S YMBOLIC JPF (SJPF) tool [21]. Like SJPF, we use the
extension mechanisms provided by JPF to force non-standard in-
terpretation of instructions that use or deﬁne symbolic values. Path
conditions are checked for satisﬁability using the Choco constraint
solver [6].
We avoid potentially inﬁnite symbolic executions of loops and
recursion by bounding the number of terms in the path condition.
As soon as the path condition reaches a certain (user-determined)
size, the corresponding path is abandoned. The current prototype
also abandons paths on which a symbolic value is used to index
into an array.
Procedure calls are handled automatically by JPF’s dispatch mech-
anism. We simply ensure that calls to symbolic receivers can be
resolved by keeping track of the runtime type of each symbolic ref-
erence.class Node {
intval = 0;
Node next = null;
}
1Node n1 = new Node();
2Node n2 = new Node();
3n1.next = n2;
4if(n2.next==n1) {
5assert n1.val < n2.val;
6}else {
7assert false ;
8}
PC: truen1: N1PC: truen1: N1, n2: N2PC: true21
n1: N1, n2: N2PC: true3N1val0nextnullN1val0nextN2
n1: N1, n2: N1PC: N2 = N1 !        0 < 0n1: N1, n2: N1PC: N2 = N1 !        0 ! 0AssertionError55n1: N1, n2: N1PC: N2 = N1
infeasibleN1val0nextn1: N1, n2: N3PC: N2 = N3n1: N1, n2: N3PC: N2 = N3 !        N3.next = N1 !        0 < N3.valAssertionErrornormal terminationn1: N1, n2: N3PC: N2 = N3 !        N3.next " N1AssertionErrorn1: N1, n2: N3PC: N2 = N3 !        N3.next = N1 !        0 ! N3.valN1val0nextN3valN3.valnextN3.nextn1: N1, n2: nullPC: N2 = null  NullPointerExceptionN1val0nextnull444557
Figure 4: A symbolic execution tree for a heap-manipulating program. The tree represents a symbolic execution of the sample
program in which the reference value produced by the newexpression on line 2 is treated symbolically.
One subtle issue arises when FlexTest (see Sec. 2) is performed
on a reference. Since our solution to heap constraints allows manu-
facture of new objects, the requirement of dis-equality ( 6=) between
theEval value and the newly constructed value ( ) is satisﬁed triv-
ially. If a programmer’s notion of equality is not based on refer-
ence equality, but rather on an equals predicate, then the ﬂexibility
check is not meaningful. To implement a meaningful equality test,
we preserve a snapshot of the ﬁeld values as observed in the default
execution—i.e., as computed by Eval . In principle, we would then
have JPF execute the programmer-provided equals method to com-
pare the snapshot to the replacement value . This is somewhat
intricate to implement, however, so our current prototype carries
out a shallow comparison of the top-level ﬁelds.
4. EVALUATION
To assess the feasibility and usefulness of angelic debugging,
we applied A NGELINA to several programs with known defects.
This section presents the results we obtained on two representative
samples: a small integer program and a large heap-manipulating
application.
4.1 Zunebug example
Our integer example, shown in Figure 5, is taken from Weimer et
al. [28]. The program takes as input a number of days as input, and
returns the year in which that many days would elapse starting from
beginning of 1980. (Table 3 shows samples of input and output of
this program.) We have changed it slightly from Weimer’s version
to make it 1-ﬁxable. The original buggy program required both
a statement insertion and a statement deletion to ﬁx it. We have1public static int zunebug( intdays) {
2 intyear = 1980;
3 while (days > 365) {
4 boolean leap = isLeapY ear(year);
5 if(leap) {
6 if(days > 366) {
7 days  = 366;
8 year += 1;
9 }else {
10 }
11 days  = 366;
12 }else {
13 days  = 365;
14 year += 1;
15 }
16 }
17 return year;
18 }
Figure 5: Zunebug code.
carried out the insertion, so the only bug is a superﬂuous statement
that needs to be deleted.
The program was tested on inputs shown in Table 3; T6 is the
failing input. The repair candidates, and the inﬂexibilities found are
reported in Table 4. The return statement was excluded from scope.
Note that in this program, alternate values are one of potentially
many valid possibilities. The code inside isLeapY ear was excluded
from scope.
Of the 7 repair candidates, 2 are ruled out as inﬂexible by pass-
ing tests. The real ﬁx is at line 7: the line should be omitted, orTest days Expected outcome
T1 1 1980
T2 365 1980
T3 366 1980
T4 367 1981
T5 731 1981
T6 732 1982
actual outcome: 1981
Table 3: Tests for Zunebug.
line# code alt value (T6) inhibitor
3 (2nd) while ( days >365) { 366, -1
11 days -= 366 0
8 year += 1 2 T4, T5
7 days -= 366 0
5 if (leap ) { false T3
4leap = isLeapY ear( year ) 1981
Table 4: Repair candidates for Zunebug.
equivalently days should be decremented by 0. This ﬁx causes the
while loop to be executed two times, as required for the input value
of 732. The repair candidates also show 4 alternate ways in which
this can be achieved from the perspective of the failing run. Specif-
ically, one option is to get the loop condition to pass one more time
as shown by the ﬁrst line in the table. Although it might seem as
if the loop condition—the second instance of it—is ﬂexible, in fact
its operands are the ones that are ﬂexible, not the condition itself.
That means that for each passing run, either of those operands could
have had at least one other value without impacting the outcome of
the run; this is easy to happen with inequalities.
4.2 JTopas case study
Our main case study focused on JT OPAS [16], an open source
Java library for parsing arbitrary text. We analyzed version 0.4
of the library from the Software-artifact Infrastructure Repository
(SIR) [24], which comes with 10 seeded faults. A NGELINA can
help diagnose four of these faults. The remaining faults are either
not 1-ﬁxable (and there is no obvious way make them so) or they
could not be analyzed due to our limited handling of arrays. Ta-
ble 7 shows a short description of each fault; why it could not be
analyzed by the tool; or, if analyzed, what scope and code modiﬁ-
cations were applied.
Faults 1, 2 and 6 .Table 5 shows the results of applying A N-
GELINA to JT OPAS faults 1, 2 and 6. The “repair cand.” column
displays the total number of repair candidates for each fault. The
“time” column speciﬁes the running time in seconds; “loaded code”
shows the number of classes and methods loaded by JPF; “instruc-
tions” is the number of executed bytecode instructions; and “heap”
shows the number of new objects created by JPF’s virtual machine
while executing the system under test. Multiple rows of data for
fault 6 indicate that its detection required repeated application of
the tool. We show the results for each application separately, start-
ing with the top-level scope.
Faults 1 and 2 were minor, and their details are fully covered
by the description in Table 7. Fault 6, however, was intricate, and
it involved a large amount of code. We therefore analyzed it in a
modular fashion—that is, one method at a time—starting with the
top-level test method shown in Fig. 6.
The top-level method tests the JT OPAS Java and Javadoc tok-
enizers by checking that their outputs satisfy two properties: (1)faultrepair time loaded code executedheapcand. (sec) classes methods instructions
1 2 2 98 1259 6,063 940
2 2 1 98 1,258 4,052 539
6 13 45 140 2,094 300,219 2,864
4 29 140 2,093 312,430 4,955
1 24 139 2,091 257,847 4,778
Table 5: The results of applying A NGELINA to three known
faults in JT OPAS .
the ‘@’ token is followed by a keyword inside of a Javadoc com-
ment1(line 39), and (2) the number of open braces is the same as
the number of closed braces (line 44). The tokenization process
has two modes: the Java parsing mode, handled javaTokenizer (line
2), and the Javadoc parsing mode, handled by docTokenizer (line
4). We start in the Java mode (line 6) and then switch (lines 27-32)
between the two modes depending on the consumed tokens. The
‘/**’ token triggers the switch from Java to Javadoc (line 15), and
‘*/’ does the opposite (line 18).
When testJavaTokenizer is executed on the Java snippet2in Fig.
7(a), it fails with an assertion violation on line 39. In particular, the
‘author’ token that follows the ‘@’ character is not interpreted as a
keyword, against expectation. Executing the test on the snippet in
Fig. 7(b) results in normal termination. Given these snippets as in-
puts, and the while loop in the test method as the scope, A NGELINA
produces the repair candidates shown in Table 6. The candidates
inhibited by the passing test are shaded in gray.
According to the results in Table 6, the failing test can be res-
cued in essentially one of three ways: (1) pretend that the ﬁrst read
is not ‘/**’ but some other (non-special) string; (2) pretend that
the second token is not the ‘@’ character; and (3) pretend that the
third token (‘author’) has the type KEYWORD . The ﬁrst ﬁx could
be applied on lines 25, 26, 28 and 30 of the ﬁrst iteration, causing
the tokenization process to remain in the Java mode. Since the ‘@’
character has no meaning in the Java mode, it is not associated with
theatSign object, and the conditional on line 37 would evaluate to
false. This ﬁx, however, is ruled out by the passing test. If the pass-
ing test remains in the Java mode, by interpreting the ﬁrst token
as anything other than ’/**’ in the passing test, the execution will
always violate the balanced braces property on line 44.
The second ﬁx—pretending that ‘@’ is something else—can be
applied in the second iteration, on lines 25, 26, 28, 30 and 37. It
rescues the test case in the same way as the ﬁrst ﬁx, by ensuring that
the conditional on line 37 evaluates to false. Like the ﬁrst ﬁx, the
second ﬁx is essentially equivalent to changing the input, which is
not an acceptable solution. The third ﬁx suggests that the program
could be rescued by changing the output of nextToken on line 38 to
produce a token of type KEYWORD . This is, in fact, the real ﬁx.
Given the buggy call to nextToken as the scope, A NGELINA re-
turns four repair candidates. (For reference, nextToken consists of
80 lines of code.) Three of the candidates simply replace the return
value of the method with an angelic token that has the right type,
essentially ignoring the computation of the next token. The re-
maining candidate suggests calling the method test4Normal with a
different input token. When the analysis is scoped to test4Normal ,
the tool pinpoints the seeded faulty expression as the only repair
candidate.
1Recall that a Javadoc comment is a block comment that starts with
the sequence ‘/**”.
2The test input we retrieved from SIR was too large for our in-
frastructure to handle, so we manually minimized it to the snippet
shown here.Fault number Description of the fault ANGELINA ’s diagnosis Scope and code modiﬁcations
Fault 1 In a constructor to class ExtIOEx-
ception , call super(fmt) is com-
mented out. fmtinitializes a for-
mat ﬁeldANGELINA offers to match up a
read of format ﬁeld to the ex-
pected value in the test harness.
This is not real ﬁx, but since the
program is not 1-ﬁxable, this is
the best A NGELINA could offer.Changed type of format to Ob-
ject, as A NGELINA cannot handle
strings (arrays). The scope was
the test harness code.
Fault 2 In a constructor of class Ex-
tIOException , the boolean ﬁeld
isWrapper is assigned the oppo-
site valueANGELINA can ﬁnd the exact ex-
pression to be ﬁxedNo modiﬁcations. Scope was the
class ExtIOException
Fault 3 The virtual method getMessage
is commented out, causing the
program to use the base class’s
implementationANGELINA cannot ﬁnd the fault.
The test harness is heavily depen-
dent on string manipulation
Fault 4 Off-by-one error in string copy ANGELINA cannot ﬁnd the fault
as it does not handle strings (ar-
rays)
Fault 5 Similar to Fault 4
Fault 6 Missing setter of type of aToken As discussed in the text, A N-
GELINA is able to assist in diag-
nosing the faultReinstanted a setter, but changed
to argument to an incorrect token
type. Scope was used hierarchi-
cally (see text)
Fault 7 Multiple integer constants
changedANGELINA cannot diagnose the
fault
Fault 8 Similar to Fault 7
Fault 9 Similar to Fault 3
Fault 10 Similar to Fault 1
Table 7: A table of seeded faults in jtopas 0.4. SIR.
5. RELATED WORK
Fault Localization by altering program states .Cleve and Zeller [7]
propose cause transitions as being indicators of potentially faulty
statements. A cause is deﬁned as the minimal part of the program
state, which when changed, can change a passing execution into a
failing one. To ﬁnd the cause, their technique compares program
states of a failing execution with those of a neighboring passing ex-
ecution at comparable stages in the two executions. At each such
comparable stage, the technique ﬁnds the cause by using ideas from
delta minimization algorithm [30]. That is, it ﬁnds the smallest
state that can be spliced from the state of the failing execution into
the state of the passing execution such that the passing execution no
longer passes. Causes thus identiﬁed can give valuable debugging
clues. More importantly, program points at which cause changes
from one variable (or a set of variables) to another—cause transi-
tion points—are indicative of faulty code.
Jeffrey et al. [12] present a somewhat similar idea, in that they
too splice in interesting values into an execution to impact its out-
come. However, they splice values from a “value proﬁle” into one
statement of a failing execution to see if the failing run can change
into a passing one. In contrast to Cleve and Zeller [7], the value
proﬁle need not arise from state of any single nearby passing run,
rather, it can be based on proﬁling a mix of executions of the pro-
gram. Statements that admit successful value replacements are in-
dicative of faulty code, but are not necessarily the actual fault. A
ranking heuristic is therefore used to order this set of statements
on suspiciousness. Statements that are associated with successful
value replacements in more faulty runs are deemed to be more sus-
picious that those associated with successful value replacements in
fewer faulty runs. This work is a generalization of earlier work
by Zhang et al. [32], and Wang and Roychoudhury [26], in which
only predicate statements were considered for alteration. We ex-
tend this line of work by adding the crucial test of ﬂexibility, whichdetermines whether an expression permits multiple value in passing
tests.
We can think of obtaining values from one run to splice into the
other as a heuristic implementation of the angel. By contrast, our
implementation of the angel is based on symbolic execution, but
the underlying purpose is to ﬁnd values that can lead to successful
execution. Our technique does not rely on another execution as a
source of values. However, a symbolic implementation of an angel
is also more expensive computationally. Also, our approach of im-
plementing an angel lets us ﬁnd repair candidates in heap manip-
ulating programs. Since addresses cannot be spliced across runs,
none of the previous techniques has been shown to work with heap
bugs. This is one of the key advantages of our approach, and one
that is very pertinent to Java bugs.
Mutation Testing .The goal of mutation testing [8] is to enhance
the adequacy of a passing test suite for a program. The basic idea
is this: suppose we change (“mutate”) some expression in the pro-
gram. These mutations are drawn from a language-speciﬁc set of
commonly occurring errors. Do we see any failures in the work-
ing test suite? If yes, the test suite is powerful enough to kill that
“mutant”. If not, the programmer tries to create a new test input
that can kill that mutant; though, it can sometimes happen that no
test input can kill a mutant because that mutant happens to result
in a semantically equivalent program. By carrying out mutations at
different points in a program, and then trying to kill those mutatnts,
a programmer can increase conﬁdence in the test suite.
Debugging is a closely related problem. In testing, the goal is
to ﬁnd a test input that kills a mutant. In debugging, the goal in-
stead is to ﬁnd a mutant that passes a new test input that is failing
before mutation, and just as importantly, that is notkilled by ex-
isting test cases. In the terminology of this paper, the mutation we
seek must be on a ﬂexible expression, because otherwise it would
be killed by an existing test case. This is the role of the ﬂexibil-
ity check. The more comprehensive a set of passing test cases, the1public void testJavaTokenizer() throws Throwable {
2InputStreamTokenizer javaTokenizer =
3 new InputStreamTokenizer(m_reader);
4InputStreamTokenizer docTokenizer =
5 new InputStreamTokenizer(m_reader);
6AbstractTokenizer currTokenizer = javaTokenizer;
7Object openBlock = new Object();
8Object closeBlock = new Object();
9Object atSign = new Object();
10
11 intblockBalance = 0;
12 Token token;
13 javaTokenizer.addTokenizer(docTokenizer);
14
15 javaTokenizer.addSpecialSequence("/ ", docTokenizer);
16 javaTokenizer.addSpecialSequence("{", openBlock);
17 javaTokenizer.addSpecialSequence("}", closeBlock);
18 docTokenizer.addSpecialSequence(" /", javaTokenizer);
19 docTokenizer.addSpecialSequence("@", atSign);
20 // ...
21 docTokenizer.addKeyword("author");
22 // ...
23
24 while (currTokenizer.hasMoreToken()) {
25 Token token = currTokenizer.nextToken();
26 switch (token.getType()) {
27 case Token.SPECIAL_SEQUENCE:
28 if(token.getCompanion() instanceof AbstractTokenizer) {
29 AbstractTokenizer tokenizer =
30 (AbstractTokenizer)token.getCompanion();
31 currTokenizer.switchTo(tokenizer);
32 currTokenizer = tokenizer;
33 }else if (token.getCompanion() == openBlock) {
34 blockBalance++;
35 }else if (token.getCompanion() == closeBlock) {
36 blockBalance   ;
37 }else if (token.getCompanion() == atSign) {
38 Token token = currTokenizer.nextToken();
39 assert token.getType() == Token.KEYWORD;
40 }
41 break ;
42 }
43 }
44 assert blockBalance == 0;
45 // ...
46}
Figure 6: JT OPAS test that exposes Fault 6.
more expressions can be found to be inﬂexible, leaving aside only
the expressions at which the program can be ﬁxed.
Note that our current focus in angelic debugging is only on the
location of the desired mutant, and not on how to create the mutant
in terms of mutation operations. Indeed, not all bugs can be cor-
rected by a standard set of mutations, so we are unable to simply
try all mutations of a repair candidate on all passing test cases.
Using Slicing For Debugging .A large body of work exists on
debugging based on static and dynamic slicing (e.g. [2, 33, 29]).
The basic intuition is that the backward slice with respect to the
statement that manifests the bug often contains the cause of the
bug. Two ideas in the literature on slicing bear close relation to our
work.
/@author / / {/
(a) Failing input (b) Passing input
Figure 7: Inputs for the test in Fig. 6.Dicing is among the ﬁrst ideas to leverage information from
passing tests to help with debugging of failing tests [1]. In fault
localization based on dices, one computes portions of slices that
appear in failing inputs but not on passing inputs.
Critical slicing [9], which also relates to mutation testing, uses a
speciﬁc kind of mutation—statement deletion—to determine which
statements have a bearing on the ﬁnal outcome in terms of values
of selected output variables. If the value of those output variables
does not change even after the statement is deleted, the statement
is not critical. For assignment statements, critical slicing checks
whether changing t=etot=tcan possibly impact execution
of a failing test case. Our technique performs (at higher cost) a
more general, existential check: could there exist an e0such that
replacing the assignment with t=e0can ﬁx the program.
Fault Localization by Comparing Executions .Several researchers
have proposed techniques to use passing runs to localize faults in
failing runs. The Tarantula technique [14, 13] compares statement
coverage spectrum taken from a number of passing runs, to the
coverage obtained from a failing run. This works on the empirical
observation that fault locations are correlated with statements that
are much more likely to appear in the failing run that in a pass-
ing one. In other words, the statement coverage proﬁle of the fail-
ing run is an anomaly [31]. Anomalies can be detected on other
kinds of “signatures” of program executions. For example, Liblit
et al. [19] show excellent correlation between faults and anoma-
lous return values of functions, whereas Hangal and Lam [11] de-
tects anomalies on discovered invariants. Reps et al. [23] solves the
Y2K problem by comparing executions executed before and after
the critical date, both summarized with path proﬁle spectra.
Others have focused on ﬁnding “nearby” passing runs to com-
pare to a failing run, with the intention of getting more precise fault
localization. For example, Renieris and Reiss [22] select a passing
run based on minimizing a nearest-neighbor similarity metric with
respect to a failing run; and Artzi et al. [3] use mixed symbolic and
concrete execution to generate nearby passing test cases.
Comparing passing and failing executions have also been used in
explaining counterexamples in model checking. In works of Ball et
al. [4], and Groce and Visser [10], the idea is to compare a single
passing trace with a failing trace, and ﬁnd program points at which
the two begin to diverge.
Our work shares with these techniques the intention of getting
useful information from passing runs. However, the goal or our
work is to pinpoint expressions that are good repair candidates. Our
line itr. code
25 1 Token token = currTokenizer.nextToken()
25 2 Token token = currTokenizer.nextToken()
26 1 switch ( token .getType())
26 2 switch ( token .getType())
28 1 if (token .getCompanion() . . . )
28 2 if (token .getCompanion() . . . )
30 1 (AbstractTokenizer) token .getCompanion()
37 2 else if ( token .getCompanion() == atSign)
37 2 else if (token.getCompanion() == atSign )
38 2 Token token = currTokenizer.nextToken()
39 2 token .getType() == Token.KEYWORD
39 2 token.getType() == Token.KEYWORD
39 2 token.getType() == Token.KEYWORD
Table 6: Repair candidates for testJavaTokenizer . Gray shad-
ing highlights the candidates that were inhibited by the passing
input.technique is geared to answering the question: can this expression
be changed to ﬁx the program, given the failing test input as well
as the passing ones? In that, the information that we compute is
at a much ﬁner granularity. Even if anomaly is perfectly correlated
with the real defect, it does not tell us what should be changed in the
program. On the other hand, these techniques do have an advantage
of being applicable easily to large applications.
Automated Repair .Although the work presented in this paper
is about ﬁnding repair candidates, and not about ﬁnding replace-
ment expressions, there is promising recent work that raises hope
that the latter can be automated to some extent. We mention four
recent lines of research in automated repair. Weimer et al. [28]
have shown promising results on automating bug ﬁxing. They use
a novel genetic programming algorithm to ﬁnd suitable replace-
ment expressions to repair bugs. The search for replacement ex-
pressions is limited to syntactic constructs found elsewhere in the
same program. Program sketching [25] presents another approach
to repairing programs in certain domains, when the range of re-
placement expressions that need to be explored is predetermined,
and the computer can simply search for one that passes all test in-
puts. (Sketching was developed originally for automatically com-
pleting partially speciﬁed programs, but its applicability to repair
is clear.) Malik et al. [20] have presented interesting initial results
in repairing heap manipulating programs, for which a representa-
tional invariant (“repOK”) can be given. The replacement expres-
sions are drawn from a small set of heap manipulating instructions.
Finally, Wei at al. [27] automatically generate bug ﬁxes based on
deviation from invariants discovered from passing tests; they also
use contracts present in Eiffel programs to validate automatically
identiﬁed program repairs.
6. CONCLUSION
The objective of angelic debugging is to automatically identify
expressions in a buggy program that, if changed, can remedy the
bug. We observe that an expression is a repair candidate if it can
be replaced with a value that ﬁxes a failing test and if, crucially, in
each passing test, its value can be changed to another value without
breaking the test. We implemented a tool based on this observation
on top of the J AVA PATHFINDER model checker. While more ex-
perience with angelic debugging is needed, our early experiments
suggest that the technique can be useful in speeding up debugging.
7. REFERENCES
[1] H. Agarwal, J. R. Horgan, S. London, and W. Wong. Fault
localization using execution slices and dataﬂow tests. In SRE
’95, 1995.
[2] H. Agrawal and J. R. Horgan. Dynamic program slicing. In
PLDI ’90 , pages 246–256, 1990.
[3] S. Artzi, J. Dolby, F. Tip, and M. Pistoia. Directed test
generation for effective fault localization. In ISSTA ’10 ,
pages 49–60, 2010.
[4] T. Ball, M. Naik, and S. K. Rajamani. From symptom to
cause: localizing errors in counterexample traces. In POPL
’03, pages 97–105, 2003.
[5] R. Bodik, S. Chandra, J. Galenson, D. Kimelman, N. Tung,
S. Barman, and C. Rodarmor. Programming with angelic
nondeterminism. In POPL ’10 , pages 339–352, 2010.
[6] Choco Solver. http://www.emn.fr/z-info/choco-solver,
August 2010.
[7] H. Cleve and A. Zeller. Locating causes of program failures.
InICSE ’05 , 2005.[8] R. A. DeMillo, R. J. Lipton, and F. G. Sayward. Hints on test
data selection: Help for the practicing programmer.
Computer , 11(4):34–41, 1978.
[9] R. A. DeMillo, H. Pan, and E. H. Spafford. Critical slicing
for software fault localization. In ISSTA ’96 , 1996.
[10] A. Groce and W. Visser. What went wrong: explaining
counterexamples. In SPIN’03 , pages 121–136, 2003.
[11] S. Hangal, S. Microsystems, D. C. Shantinagar, and M. S.
Lam. Tracking down software bugs using automatic anomaly
detection. In ICSE ’02 , 2002.
[12] D. Jeffrey, N. Gupta, and R. Gupta. Fault localization using
value replacement. In ISSTA ’08 , pages 167–178, 2008.
[13] J. A. Jones and M. J. Harrold. Empirical evaluation of the
tarantula automatic fault-localization technique. In ASE ’05 ,
pages 273–282, New York, NY , USA, 2005. ACM.
[14] J. A. Jones, M. J. Harrold, and J. Stasko. Visualization of test
information to assist fault localization. In ICSE ’02 , pages
467–477, 2002.
[15] Java PathFinder. http://babelﬁsh.arc.nasa.gov/trac/jpf,
August 2010.
[16] Java tokenizer and parser tools (JTopas).
http://jtopas.sourceforge.net/jtopas/index.html, August 2010.
[17] S. Khurshid, C. S. P ˘as˘areanu, and W. Visser. Generalized
symbolic execution for model checking and testing. In
TACAS’03 , pages 553–568, 2003.
[18] J. C. King. Symbolic execution and program testing.
Commun. ACM , 19(7):385–394, 1976.
[19] B. Liblit, A. Aiken, M. Naik, and A. X. Zheng. Scalable
statistical bug isolation. In PLDI ’05 , pages 15–26, 2005.
[20] M. Z. Malik, K. Ghori, B. Elkarablieh, and S. Khurshid. A
case for automated debugging using data structure repair. In
ASE ’09 , 2009.
[21] C. S. P ˇasˇareanu, P. C. Mehlitz, D. H. Bushnell,
K. Gundy-Burlet, M. Lowry, S. Person, and M. Pape.
Combining unit-level symbolic execution and system-level
concrete execution for testing NASA software. In ISSTA ’08 ,
pages 15–26, 2008.
[22] M. Renieris and S. P. Reiss. Fault localization with nearest
neighbor queries. ASE ’03 , page 30, 2003.
[23] T. W. Reps, T. Ball, M. Das, and J. R. Larus. The use of
program proﬁling for software maintenance with
applications to the year 2000 problem. In M. Jazayeri and
H. Schauer, editors, FSE ’97 , volume 1301 of LNCS , pages
432–449, 1997.
[24] Software-artifact infrastructure repository (SIR).
http://sir.unl.edu/portal/index.html, August 2010.
[25] A. Solar-Lezama, L. Tancau, R. Bodik, S. Seshia, and
V . Saraswat. Combinatorial sketching for ﬁnite programs. In
ASPLOS-XII , pages 404–415, New York, NY , USA, 2006.
ACM.
[26] T. Wang and A. Roychoudhury. Automated path generation
for software fault localization. In ASE ’05 , pages 347–351,
2005.
[27] Y . Wei, Y . Pei, C. A. Furia, L. S. Silva, S. Buchholz,
B. Meyer, and A. Zeller. Automated ﬁxing of programs with
contracts. In Proceedings of the 19th international
symposium on Software testing and analysis , ISSTA ’10,
pages 61–72, New York, NY , USA, 2010. ACM.
[28] W. Weimer, S. Forrest, C. Le Goues, and T. Nguyen.
Automatic program repair with evolutionary computation.
CACM , 53(5), 2010.[29] M. Weiser. Program slicing. In ICSE ’81 , pages 439–449,
1981.
[30] A. Zeller. Isolating cause-effect chains from computer
programs. In FSE ’02 , pages 1–10, 2002.
[31] A. Zeller. Why Programs Fail: A Guide to Systematic
Debugging . Morgan Kaufmann Publishers Inc., San
Francisco, CA, USA, 2005.
[32] X. Zhang, N. Gupta, and R. Gupta. Locating faults through
automated predicate switching. In ICSE ’06 , pages 272–281,
2006.
[33] X. Zhang, H. He, N. Gupta, and R. Gupta. Experimental
evaluation of using dynamic slices for fault location. In
AADEBUG’05 , pages 33–42, 2005.