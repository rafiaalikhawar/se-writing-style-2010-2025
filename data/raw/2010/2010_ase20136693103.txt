Measuring the Structural Complexity of Feature
Models
Richard Pohl, Vanessa Stricker, and Klaus Pohl
paluno ‚Äì The Ruhr Institute for Software Technology
University of Duisburg-Essen
Gerlingstr. 16, 45127 Essen, Germany
{richard.pohl,vanessa.stricker,klaus.pohl}@paluno.uni-due.de
Abstract ‚ÄîThe automated analysis of feature models (FM) is
based on SAT, BDD, and CSP ‚Äì known NP-complete problems.
Therefore, the analysis could have an exponential worst-case
execution time. However, for many practical relevant analysis
cases, state-of-the-art (SOTA) analysis tools quite successfully
master the problem of exponential worst-case execution time
based on heuristics. So far, however, very little is known about
the structure of FMs that cause the cases in which the execution
time (hardness) for analyzing a given FM increases unpredictably
for SOTA analysis tools. In this paper, we propose to use
width measures from graph theory to characterize the structural
complexity of FMs as a basis for an estimation of the hardness of
analysis operations on FMs with SOTA analysis tools. We present
an experiment that we use to analyze the reasonability of graph
width measures as metric for the structural complexity of FMs
and the hardness of FM analysis. Such a complexity metric can
be used as a basis for a uniÔ¨Åed method to systematically improve
SOTA analysis tools.
Index Terms ‚Äîsoftware product line, feature model, automated
analysis, performance measurement
I. I NTRODUCTION
Software product line engineering (SPLE) [ 25] leverages
systematic and proactive reuse across several similar software
products. Feature models (FM) [ 17] are a common way to
document commonality and variability between products of
software product lines (SPL) [ 9]. The automated analysis of
FMs is concerned with extracting information and detecting
inconsistencies of these FMs. Examples for analysis operations
on FMs are determining whether valid products can be
conÔ¨Ågured from an FM or Ô¨Ånding dead features [3].
A. Problem Statement
The complexity of analyzing FMs for validity is generally
known as an NP-complete problem [ 30]. Current analysis
techniques are mostly based on reducing the problem of
automated analysis of FMs to a more general computational
problem, e.g. SAT or CSP. This implies exponential worst-case
execution time that is, however, mastered quite successfully
for many practical relevant analysis cases by state-of-the-art
(SOTA) analysis tools based on the application of heuristics.
Current research focuses on maturing and improving these
heuristics [ 18]. Since heuristics used to solve NP-hard problems
are highly dependent on the problem structure, this calls for an
understanding of the relation between the modeling constructs
of FMs and the complexity of analysis operations on them.Despite a constant evolution of the heuristics, experiments
have shown that for some models, the execution time of the
analysis particularly increases unpredictably on some models
[26], i.e. there are models that are hard to analyze. While
hardness can have different causes in practice (e.g. memory
consumption, required user interactions), we limit our work in
this paper to hardness in terms of execution time. Even with
this restriction, apart from knowledge about the theoretical
worst-case complexity, little is known about the structures
and the respective modeling constructs in FMs that cause a
model to be hard to analyze for SOTA analysis tools [ 18].
Thus, improving the efÔ¨Åciency of FM analysis is an open
research issue [ 32]. Related work on the hardness of FM
analysis operations addresses the relation between the structural
complexity of the models and the time needed for the analysis
with two different approaches.
On the one hand, theoretical considerations reveal more
approximate estimations of the worst-case time complexity for
certain structural characteristics. These theoretical considera-
tions are made for general SAT and CSP problems, but also
on the level of FM analysis. It is for example known that an
FM containing only mandatory and optional features is always
satisÔ¨Åable and thus can be searched in linear time.
On the other hand, several empirical experiments have
been conducted collecting data about the performance of
SOTA analysis tools beyond the knowledge of the worst-case
complexity. These studies analyze the execution time with
respect to different complexity measures. These measures all
have in common that they consider only one characteristic or a
subset of characteristics of the analyzed models, e.g. the size of
the FM or the ratio of FM constraints. These measures, however,
are not able to cover all cases in which the execution time of
a solver increases unpredictably. Instead, the complexity of the
analysis of FMs depends on the combination of all modeling
concepts of the analyzed FM, i.e. considering the full semantics
of the structure.
Thus, the current situation is lacking a comprehensive theory
of the structural complexity of FMs that allows drawing
conclusions about the hardness of analysis operations. Filling
this gap would contribute to build heuristics that achieve a
reliable tool performance. A solid understanding of the structure
causing the hardness ‚Äì called the structural complexity of FMs
in the following ‚Äì will allow developing improved heuristics.978-1-4799-0215-6/13/$31.00 c2013 IEEE ASE 2013, Palo Alto, USA454
B. Contribution of this Paper
In this paper, we propose to use width measures from graph
theory as metrics for the structural complexity of and thus for
the hardness of analysis operations on FMs.
Graph width measures were originally deÔ¨Åned with the goal
of determining the hardness of analysis operations on graphs.
While some successful general applications of graph width
measures to SAT [ 6] and CSP [ 20] exist, currently there is no
application for the special case of FM analysis. Graph width
measures usually provide an upper bound for the worst-case
time complexity of a graph operation. Thus, while they are
generally reasonable for SAT and CSP, there are particular
problem instances that are much easier than indicated by the
measure. Moreover, there are sets of models for which a graph
width measure does not differentiate, i.e. it delivers the same
value for all problem instances. According to this, the primary
objective of this paper is to analyze whether graph width
measures are reasonable metrics to characterize the structural
complexity of FMs. This boils down to the following research
question:
RQ:Are graph width measures reasonable metrics to
differentiate easy from hard FM analysis cases?
C. Approach
According to the deÔ¨Åned problem, we aim at analyzing
the reasonability of graph width measures as metrics for the
complexity of the automated analysis of FMs. In general, a
complexity metric can be considered to be reasonable if it
differentiates hard from easy cases [ 12]. We deÔ¨Åne two quality
criteria in order to evaluate the reasonability:
1)As the graph width measure is an upper bound for the
complexity of the analysis, the metric should differentiate
between models of different hardness classes, i.e. it does
not deliver an equal value for all models to be analyzed.
2)The metric should not deliver values that are unreasonably
above the actual complexity for many models, i.e. the
majority of models is not signiÔ¨Åcantly easier than
predicted by the metric.
We prove criterion (1) by computing the measures on a test
set of models from the BeTTy generator [ 31]. Criterion (2)
is addressed by an experiment that aims at Ô¨Ånding signiÔ¨Åcant
correlations between the execution time of SOTA analysis tools
for the valid operation and the structural complexity metric.
We consider the validity check or valid operation as the most
fundamental analysis operation. It checks whether at least
one product can be conÔ¨Ågured according to a feature model
without violating any feature model constraints. A variety of
other operations on FMs can be implemented by decomposing
them into several calls to the valid operation (see e.g. [3]).
The structure of this paper is as follows. Section II presents
related work on the complexity of FM analysis. Section III
summarizes the foundations of the automated analysis of feature
models, graph width measures, and their relation. In Section IV,
an experimental study of applying graph width measures to
FM analysis is described. Section V presents conclusions from
our work.II. R ELATED WORK
The related work on the complexity of FM analysis based on
their structure can be divided into theoretical and experimental
studies.
Theoretical Complexity Analysis: A survey on different
representations of FMs, including theoretical complexity results
is presented by Schobbens et al. [ 30]. They reveal that checking
FM validity in general is NP-complete. Mendon√ßa et al. [ 21]
map FM constructs to their representation in PL in order to
evaluate the complexity of solving the SAT instance. They
conduct an experimental analysis of the complexity of the
solving process based on published, realistic FMs. Despite
potential hardness, they found that SAT based analysis is easy
on most models that occur in practice. Gil et al. [ 13] analyze
the performance of several analysis operations on tree-like
FMs, i.e. models without cross-tree-constraints, showing that
these models are comparably easy to analyze. For the rare
hard cases that were revealed on realistic FMs [26], however,
the origin of hardness was not yet traced back to concrete
structural elements of the model.
≈†tuikys and Dama≈°evi Àácius [ 36] propose three structural
complexity measures for FMs based on model complexity
research. Their aim is to use the complexity in order to improve
the quality of the design and the productivity of the design
process. Their measures cover the cognitive complexity of
an FM that is the complexity of understanding the model
from a human perspective and the structural complexity as a
high-level measure for the complexity of verifying a model,
without stating analysis operations to which this measure
applies. Bagheri and Gasevic [ 1] propose the use of structural
complexity measures for assessing the maintainability of feature
models. Their measures are based on properties of feature
models (e.g. percentage of features involved in constraints) and
do not necessarily reÔ¨Çect the hardness of analysis operations.
Experimental Performance Studies: Benavides et al. [ 4]
present a comparison between Java-based SAT, BDD, and CSP
solvers focusing on two sample operations. Their experiment
used three different solvers and was run on randomly generated
models with up to 300 features. They conclude that the BDD-
based solver shows a lower execution time increase on larger
models on the more complex operation. Furthermore, they
revealed an exponential growth in memory usage for the BDD-
based solver on larger models. In [ 5], all products of an FM
are computed using a CSP solver. The authors describe an
experiment with models containing up to 25 features. The
results show an exponential growth in time, in particular on
highly variable models, i.e. models with few constraints.
Mendon√ßa [ 22] examines the execution time of SAT-, BDD-,
and CSP-based tools on basic operations and proposes heuristics
to reduce BDD size based on randomly generated models and
models from literature. The studies are designed to ensure that
the models have certain structural properties (e.g. a Ô¨Åxed ratio
of constraints and features). The result is that SAT solvers
handle FM problems easily, while BDD-based solvers suffer
from intractability issues on large models. Segura et al. [ 31]455present BeTTy, a framework for Benchmarking and Testing
in the auTomated analYsis of FMs. Their framework provides
a generator for FMs that can be driven either by random or
by an evolutionary algorithm that maximizes properties of the
analysis, e.g. execution time. Thus, deliberately hard models
can be generated. Moreover, test data for functional tests of
analysis tools can be generated. In their later work, Segura et
al. [32] propose the use of hardness metrics for the underlying
solvers to measure the complexity of feature models. These
metrics include e.g. the number of backtracks of a CSP solver
or the number of decisions of a SAT solver. While these metrics
are deterministic and eliminate threats to validity induced by
time measurements, they currently do not consider structural
properties of FMs.
Pohl et al. [ 26] describe an experiment on FMs from
SPLOT. An analysis on the effect of the choice of the solver
and operation on the execution time is presented, proving
statistically signiÔ¨Åcant differences between different solvers,
operations, models with a different number of products in the
modeled product line, and models with a different number of
features. An analysis of correlations between graph parameters
(e.g. cyclomatic number) and the execution time of the analysis
is described. While the work prior to [ 26] was performed on
Java solvers, Pohl et al. introduce C solvers and show that
there is a considerable difference between Java and C solvers
regarding their execution time. These are closer to the current
state of the practice in solver technology, as shown by their
participation and awards in solver competitions.
Sayyad et al. [ 29] discuss using multi-objective optimization
techniques for the automated analysis of feature models. This
allows choosing correct models that are optimal with respect
to speciÔ¨Åc goals (e.g. minimum number of features). This adds
more complexity to the original problem of Ô¨Ånding correct
models. Their paper describes an experimental performance
study that compares several techniques for multi-objective
optimization. The multi-objective scenario in their work is
a generalization of the consistency check analyzed in this
paper, so it could be basically used to Ô¨Ånd consistent solutions
although the techniques used there are not optimized to perform
this task.
III. F EATURE MODEL ANALYSIS AND GRAPH WIDTH
We brieÔ¨Çy outline the foundations of the automated analysis
of feature models and present graph width measures from graph
theory. We propose using graph width measures as metrics for
the structural complexity of FMs. The measures are based on
structural characteristics of graphs [11].
A. Automated Analysis of Feature Models
Technically, the automated analysis of FMs follows a two-
step approach. Figure 1 shows the dependencies between both
steps on the example of our experiment described in Section IV.
In the Ô¨Årst step, the FM is encoded in a formal representation.
To ease automated reasoning, general purpose formal encodings
(e.g. CNF or CSP) are used [ 3]. In the second step, tools for
reasoning on this formal encoding (e.g. a SAT solver) are
STEP 1  
a 
b c 
libTW  
Parallel  
Binary  
Counter  
Encoding  
CSP 
(direct ) 
Encoding  CNF 
CNF 
CSP 
ùíïùíòùëÆ(ùúûùüè), ùíïùíòùëÆ(ùúûùüê), 
ùíïùíòùëÆ(ùúûùüë), ùíïùíòùë∞(ùúûùüè), 
ùíïùíòùë∞(ùúûùüê), ùíïùíòùë∞(ùúûùüë) 
lingeling ùùã 
clasp ùùã 
SAT4J ùùã 
PicoSAT ùùã 
buddy ùùã 
CUDD ùùã 
JavaBDD ùùã 
Choco ùùã 
Cream ùùã 
JaCoP ùùã 
Mistral ùùã 
Order  
Encoding  
STEP 2  
GRAPH -BASED  
COMPLEXITY  
ANALYSIS  
result  tool output  TREE -WIDTH MEASURES  
ùùã FM 
tool input  BDD  
SAT 
CSP Fig. 1. Overview of the experiment
used to compute properties of the FM. Thus, different analysis
alternatives can be used by combining the different formalisms
for encoding FMs and tools for reasoning on them. Depending
on the structure of the model and the property to be computed,
the alternatives differ in the efÔ¨Åciency of the computation.
To realize the Ô¨Årst step, features are usually represented as
variables over a two-valued domain so that one value of the
domain corresponds to feature selection and the other value to
feature exclusion. Constraints on these variables are deÔ¨Åned in
a way that they are satisÔ¨Åed only for all valid products. Two
most common techniques are [3]:
The encoding of an FM as a formula in propositional
logic (PL): For all FM constructs except for cardinality
constraints over feature groups, a Ô¨Åxed set of clauses in
conjunctive normal form (CNF) can be used to encode the
modeling construct. CNF facilitates automated processing,
since most tools expect their input in CNF [ 6]. Encoding
group cardinalities in PL is not trivial [ 34]. The size
of the formula for a cardinality constraint depends on
the minimum and maximum values. The propositional
formula can be used to form an instance of the Boolean
satisÔ¨Åability problem (SAT).
The encoding of an FM using constraint programming
(CP) over integer domains: Integer domains facilitate
the encoding of cardinality constraints over feature
groups. Minimum and maximum values of the cardinality
constraint can be encoded in a straightforward way.
Constraints over the sum of variables corresponding to the
features in the group are deÔ¨Åned. The set of constraints456corresponding to the FM can be used to form an instance
of the constraint satisfaction problem (CSP).
Solving the problems SAT and CSP realizes step 2 of an
implementation of the valid operation on an FM. For this, a
variety of tools can be used. Examples for these include SAT
and CSP solvers, but also BDD-based tools or SatisÔ¨Åability
Modulo Theory (SMT) solvers. SMT solvers are extended SAT
solvers that can be used to solve problems in PL and problems
with integer variables. For the experiment in this paper, we
focus on the most common tools for feature model analysis,
which are BDD-, SAT- and CSP-based tools [ 3]. While SMT
solvers could show a superior performance to other tools, to
achieve their optimal performance on feature models, feature-
speciÔ¨Åc theories would have to be deÔ¨Åned and evaluated Ô¨Årst.
Despite the difÔ¨Åculty of encoding cardinality constraints, the
Ô¨Årst step is tractable [ 2]. The complexity of the second step
depends on the choice of analysis operations.
B. Application of Graph Width Measures to SAT and CSP
Several studies propose the use of graph width measures to
measure the complexity of SAT [ 11] or CSP [ 28] instances.
They discuss the reasonability on a theoretical level by proving
that problems with a bounded value in these measures can
be solved efÔ¨Åciently, while other problems remain potentially
hard. Because of their general applicability, we suggest to apply
graph width measures to FM formalisms. Tree width, clique
width, and rank width provide appropriate measures for FMs,
since SAT, CSP, and the problem of counting the number of
satisfying assignments to a SAT instance (#SAT) are tractable
on bounded measures [ 11], [28]. The counting version of CSP
(#CSP) is also tractable on bounded tree width [10].
While clique width [8] and rank-width [23] are deÔ¨Ånitely
promising from a theoretical point of view, tree width is the only
measure for which current SOTA algorithms are sufÔ¨Åciently
efÔ¨Åcient for practical use. Thus, for our experimental study,
we are limited to using tree width.
Informally, tree width [27] is a metric to describe how far
the structure of a graph resembles a tree. It is applied to many
computationally hard, e.g. NP-complete, problems that are
tractable on graphs with a bounded tree width. Tree width is a
measure for the complexity of analysis operations on undirected
graphs. It has applications to the most basic and generic graph
problems that are problems in searching and traversing a graph
[7]. The widest class of efÔ¨Åciently solvable problems on graphs
with bounded tree width is given by expressions in monadic
second order logic over vertices, sets of vertices, edges, and
sets of edges ( MSO 2-expressions) [15].
C. Application of Graph Width Measures to FMs
A feature model encoded as a SAT instance in conjunctive
normal form (CNF) or as a CSP can be represented as a graph.
More speciÔ¨Åcally, the formal encodings (CNF and CSP) of an
FM have a hypergraph representation. As graph width measures
were originally deÔ¨Åned for graphs instead of hypergraphs,
this hypergraph needs to be represented as a graph. We use
two common forms to represent the hypergraph of the FMformalization (i.e. CNF or CSP) as a graph that are called the
Gaifman graph and the incidence graph (see e.g. [ 6] for a
deÔ¨Ånition) of this hypergraph. This way of applying tree width
deÔ¨Ånes a structural complexity metric for an FM depending
on the encoding produced in step 1 of the analysis process .
The idea of using these graph representations to extend the
application of tree width for hypergraphs is not new. Marx
et al. [ 20] refer to hypertree width as the tree width of the
Gaifman graph of a hypergraph. They also describe applications
of hypertree width to the structure of SAT and CSP instances.
According to the results from the work in the SAT [ 6] and
CSP [ 14] communities, graph width measures in general can
be applied to CNF and CSP hypergraphs. This way, we can
apply graph width measures to FMs.
To describe our experiments on tree width , we use the
following short notation: twGdenotes the tree width of the
Gaifman graph (hypertree width), twIdenotes the tree width of
the incidence graph. For a feature model M, we use P(M)for
its PL (CNF) formalization and C(M)for its CSP formalization.
Thus, for example, twI(C(M))denotes the tree width of the
incidence graph of the CNF formalization of an FM M.
IV. E XPERIMENTAL STUDY
We propose a systematic experimental study on tools that can
be used for the automated analysis of FM. For this purpose, we
implemented our Comparison Framework for Feature Model
Analysis Performance (CoFFeMAP1) that provides these SOTA
analysis tools with a uniÔ¨Åed infrastructure for standard tasks
like model parsing, displaying the results, etc. As input for the
experiment, we use FMs generated from the BeTTy generator
created by Segura et al. [ 31]. These samples constitute models
with constant size but different structural complexity in terms
of the proposed tree width measures. According to the research
question, we deÔ¨Åne the following hypothesis.
H: There is a statistically signiÔ¨Åcant dependency between
the structural complexity in terms of tree width of a
formally encoded FM and the time needed to analyze
the model for validity.
Figure 1 gives an overview of the experimental setup that is
described in the following subsections in detail.
A. Tool Selection
We selected the following representative set of encodings
and analysis tools for the experiment.
Step 1: Formal Encoding of Feature Models: Benavides
et al. [ 3] make a straightforward proposal for a direct CP
encoding of FMs that is used in our framework. The CNF
case is more difÔ¨Åcult because expressing cardinality constraints
within PL is not trivial. In the literature, different encodings
are proposed with different implications [ 2]. Some encodings
produce CNF instances that are easy to solve, that is, at the
cost of potentially making these instances very large. Other
encodings produce small CNF instances, but, at the cost of
potential computational hardness. As both space consumption
1http://www.sse.uni-due.de/en/278457and computational hardness can lead to intractability, we chose
two CNF encodings for our experimental study. First, we
consider the order encoding [35] known from the Sugar solver.
While we use the order encoding for cardinality constraints,
the order encoding can also be used to encode general CSP
instances to CNF. In this context, it has proven to preserve many
structural properties of tractable problems (see [ 24] for details).
Moreover, in connection with a SAT solver (either PicoSAT
or MiniSat), it won about half of the categories in several
years of the CPAI CSP competitions2. Thus, we consider it to
produce easy-to-solve problem instances. Second, we consider
theParallel binary counter (LTn;k
PAR ) encoding [ 34], as it has
proven to produce CNF instances of minimal size [2].
While we provide the direct CSP encoding and the parallel
binary counter encoding as part of our framework, we use an
existing implementation for the order encoding from the Sugar
solver. This implementation is fed with a CP representation
of the FM and encodes it to CNF. Consequently, we actually
conduct two encoding steps in sequence to realize the order
encoding. As all of the encodings we use run in linear time,
we decide not to measure the time needed for the encoding of
the model, so the use of several encodings in sequence has no
inÔ¨Çuence on the measured execution time.
Step 2: Solving Computational Problems: This step is
conducted using several SOTA solvers. To Ô¨Ånd the best
performing solver implementations, we referred to the last
years of the SAT Competition and CPAI CSP competition3.
For practical reasons, we include solvers from the Ô¨Årst three
ranks of at least one track of the competition that provide
either a Java or a C/C++ interface. Moreover, we also include
the solvers from the FaMa framework4for the analysis of FMs
because of their common use in the SPL community. Table I
gives an overview of all solvers.
This leads to 15 combinations of encodings and analysis
tools in our experiment: The CSP solvers can only be used
on the direct CSP encoding while the SAT solvers can be
used on the ( LTn;k
PAR ) encoding and on the order encoding. As
Sugar was designed for the use with SAT solvers [ 35], we do
not include the theoretically possible combination of Sugar
with BDD solvers in the experiment. As indicators for the
solving performance of the valid operation, we measure the
execution time of the solver (step 2 of the analysis process).
Figure 1 shows the basic experimental setup with the encoders
and solvers used. For our data analysis, we implemented an
application to compute the tree width of the formal encodings of
an FM based on LibTW5. Due to the computational hardness of
determining an exact tree width value, we used approximation
algorithms that delivered lower and upper bounds for each tree
width measure.
2http://cpai.ucc.ie
3Competitions from 2012 and 2013 were not reviewed, because experiments
were started in 2012
4http://www.isa.us.es/fama/
5http://www.treewidth.comTABLE I
OVERVIEW OF SOLVERS FOR STEP 2OF THE EXPERIMENT
Name Reason for inclusion
buddy1BDD literature [16]
CUDD2BDD literature [16]
JavaBDD3FaMa
lingeling4SAT comp. (2011)
clasp5SAT comp. (2009)
SAT4J6FaMa
PicoSAT7SAT comp. (2007), CPAI comp. (Sugar, 2009)
CHOCO8FaMa
Cream9CPAI comp. (Sugar, 2009)
JaCoP10FaMa
Mistral11CPAI comp. (2009)
1http://buddy.sourceforge.net/manual/main.html
2http://vlsi.colorado.edu/~fabio/CUDD/
3http://JavaBDD.sourceforge.net
4http://fmv.jku.at/lingeling/
5http://www.cs.uni-potsdam.de/clasp/
6http://JavaBDD.sourceforge.net
7http://fmv.jku.at/PicoSAT/
8http://www.emn.fr/z-info/CHOCO-solver/
9http://bach.istc.kobe-u.ac.jp/Cream/
10http://www.JaCoP.eu/
11http://homepages.laas.fr/ehebrard/Software.html
B. Experiment Design
According to the research question, the goal of the experi-
ment is to reveal dependencies between the different tree width
measures of an FM and the time needed to analyze the FM
for validity. Since it is known that for the valid operation
there is a relationship between the model size (number of
features) and the execution time of the analysis [ 26], we keep
the number of features in all models constant. Our previous
experiments [ 26] ran on models of up to about 300 features.
These previous experiments showed that these models were
easy to check for validity. Furthermore, industry-scale models
can easily contain several thousand features [ 25]. Due to these
two reasons we decided to increase the number of features and
use models with 500 and 1000 features. The models generated
by BeTTy contain only groups with [1;1]and[1;]cardinalities.
This is a common practice in existing performance studies on
feature model analysis [ 3]. We modiÔ¨Åed the set of models
by introducing random cardinalities of type [m; n]that are
established in current FM languages, e.g. OMG‚Äôs Common
Variability Language (CVL)6.
The independent variable that is varied throughout the
experiment is the FM structure, measured in terms of graph
width measures on the FM encodings. More precisely, these
are the metrics twG(PP(M)),twI(PP(M)),twG(PO(M)),
twI(PO(M)),twG(C(M)), and twI(C(M)), where PPde-
notes the parallel binary counter CNF encoding and POdenotes
the order encoding.
The dependent variable we measure that represents the
performance of the analysis is time consumed by the solver .
We conducted three experiments using different sets of
models in the following conÔ¨Ågurations:
6http://www.variabilitymodeling.org4581)Number of features per model jFj= 500 , groups with
[m; n]cardinalities
2)jFj= 500 , groups with [1;1]and[1;]cardinalities
3)jFj= 1000 , groups with [m; n]cardinalities
These conÔ¨Ågurations allow us to reveal differences between
different model sizes but also between models with different
types of cardinality constraints.
C. Experiment Execution
Our experimental environment included four state-of-the-art
computers with an Intel Core i7-3770 CPU and 16 GB RAM.
As an operating system, we used Ubuntu Linux 12.04 LTS 64
bit. To minimize confounding factors and to ease replication
with the same environment on all computers, we used a live
CD to boot the PCs. We set a timeout of 1000 seconds and a
memory limit of 12 GB. To avoid slowing down computations
of the Java applications by extensive garbage collection before
running out of memory, we set the JVM heap size to 1.2
times the memory limit. The application (including the JVM)
restarted for each tool combination and model. When reaching
either the memory limit or the timeout, the type of error was
saved. Otherwise, on completion, the time for solving the FM
and the peak memory were saved.
D. Threats to Validity
Concerning construct validity , there is a risk that better
measures than the graph width measures might exist. Although
this would impair the quality of our results, the results of the
current experiment provide a contribution to the community
as it is the Ô¨Årst to consider the full semantics of FMs in a
structural complexity measure.
Correlations between graph width measures and execution
time with no true causal relationship could furthermore threaten
theinternal validity . In particular, confounders that could cause
such a false conclusion are all factors that increase the measured
execution time beyond the time needed by a SOTA analysis
tool for solving the FM. Most critical is the decision of using
Sugar as an encoder, since the tools using Sugar contain several
steps that increase the execution time due to its implementation.
First, a CNF Ô¨Åle (in DIMACS format) is written and one of the
participating SAT solvers is called for solving the CNF. The
time for solving the CNF is measured. The other encoders are
directly linked to our framework, allowing to call the solver
directly via the corresponding (C or Java) API and pass them
a data structure containing the formalized FM as parameter.
Obviously, writing and reading a Ô¨Åle, as well as the initialization
of the new process demand additional computational resources.
Moreover, Sugar uses solvers that are implemented in C that
are known to perform signiÔ¨Åcantly faster than comparable Java
implementations [26].
Generally, these factors infer the ability to trace back a
possible correlation to the solution procedure rather than to
implementation aspects. Nevertheless, we are conÔ¨Ådent that
the confounders are not too strong since Sugar participated
in the CPAI competitions in the same conÔ¨Åguration with verygood results, outperforming solvers with a more traditional con-
Ô¨Åguration in several categories. To avoid differences between
the used computers and random performance deviations, we
distributed equal numbers of models within a speciÔ¨Åc range of
tree width of the formalizations across the different machines
and executed them in random order. Our sample includes
several FMs with approximately equal graph width that further
minimizes the effect of outliers on the results.
Concerning conclusion validity , in particular the time mea-
surement, measurements were done before and after calls to
solvers. We measured wall clock time that is common for time
measurement (e.g. in the SAT competition). As discussed by
Luque et al. [ 19], wall clock time increases with the number
of tasks in a system. We addressed this issue by using a
minimal conÔ¨Åguration of the operating system that keeps the
number of additional tasks run by the system constant and as
low as possible. For the time measurement, we used an own
implementation as part of our framework instead of solver-
internal time measurements to ensure that the measurement is
done equally for all solvers.
As far as our sample of models is concerned, we used
artiÔ¨Åcially generated models that source from BeTTy [ 31], a
generator that is in common use in research work in the area.
We used the original models as well as an enhanced version
of these models that contain random cardinality constraints to
represent realistic models.
Another aspect that needs to be discussed carefully is the
external validity . For easy integration within our framework,
we call the participating BDD solvers in the same way as the
SAT solvers, neglecting feature-model speciÔ¨Åc advances like
the BDD heuristics from Mendon√ßa [ 21]. This lead to out of
memory problems with our solvers. Thus, our results are not
necessarily generalizable for the BDD-based analysis of FM.
Despite this, we cover a wide variety of models and a wide
set of SOTA analysis tools. Thus, further evaluation studies in
various environments have to be conducted to determine their
effect in different practical situations, e.g. in applications that
use the automated analysis of FMs in modeling environments
in the context of industrial projects.
E. Data Analysis
Quality criterion 1 (from Section I-C) is fulÔ¨Ålled by the
distribution of the tree width measures in our experiments,
as shown in Tables II‚ÄìIV. The tables show the number of
models with a tree width value in a given interval. For example,
Table II shows in row 1 that for twG(PP)+‚Äì short for the
upper bound of the Gaifman tree-width of the parallel binary
counter encoding PP‚Äì there are 40 models for which the
computed value is within [0;1](column 1), 28 models with a
tree width in [2;4], etc.
Because the lower and upper bound approximations delivered
by the tree width approximation algorithm differ slightly, we
computed all correlations for the set of lower and the set of
upper bounds separately. We use x to refer to the lower bound,
x+to refer to the upper bound. Because of the complexity
of computing the incidence graph for the models with 1000459TABLE II
DISTRIBUTION OF twGMEASURES OVER jFj= 500 MODELS
[0;1] [2;4] [5;9] [10;19] [20;39] [40;59] [60;79] [80;99] [100;149] >149
twG(PP)+40 28 52 0 0 80 7 143 10 0
twG(PP)‚Äì40 28 52 0 0 120 0 120 0 0
twG(PO)+54 14 2 0 22 55 44 83 16 70
twG(PO)‚Äì120 14 2 0 135 52 33 4 0 0
twG(C)+40 28 52 0 0 80 14 146 4 0
twG(C)‚Äì40 28 52 0 0 120 0 116 4 0
TABLE III
DISTRIBUTION OF twIMEASURES OVER jFj= 500 MODELS
[0;1] [2;3] [4;5] [6;8] [9;11] [12;15] [16;19] [20;29] [30;49] [50;69] [70;99] >99
twI(PP)+0 46 42 32 0 0 35 33 35 32 52 50
twI(PP)‚Äì0 46 52 35 40 19 22 129 14 0 0 0
twI(PO)+12 52 6 0 0 0 0 0 0 0 0268
twI(PO)‚Äì78 52 4 78 126 0 0 0 0 0 0 0
twI(C)+40 86 42 60 56 40 32 4 0 0 0 0
twI(C)‚Äì40 86 44 58 66 50 16 0 0 0 0 0
features, we were restricted to conduct the analysis on the
Gaifman graph for these models.
To analyze quality criterion 2, we used the solver-speciÔ¨Åc
execution times in addition to the tree width measures to
conduct a Spearman‚Äôs rank correlation analysis (see [ 37] for
details). We applied a post-hoc signiÔ¨Åcance test to the resulting
correlation coefÔ¨Åcient. Spearman‚Äôs rank correlation is non-
parametric and thus we do not need to make assumptions on
the distribution of the base data (e.g. normality).
A general observation on the solver execution times of the
BDD-based solvers is that these solvers were incapable of
processing models with cardinality constraints as they ran out
of memory. This might be due to our decision to connect
BDD solvers directly to the LTn;k
PAR encoder without using
BDD-speciÔ¨Åc heuristics, as described in the previous section.
Tables V‚ÄìXIII describe signiÔ¨Åcant correlations between
graph width measures and solver execution time revealed by
the post-hoc signiÔ¨Åcance test at the 5%signiÔ¨Åcance level. In
the four columns, correlations are shown for the lower (-) and
upper (+) bound heuristics of the tree width applied to the
Gaifman ( twG) and the incidence graph ( twI). The results
of each experiment are grouped according to the analyzed
encoding. We describe the correlation between the tree width
of the analyzed encoding and the execution time of the solvers
that are run on this particular type of encoding. These are
highlighted grey in the following tables. In addition, for each
encoding, we describe correlations between the tree width of the
encoding and the execution time of the solvers on the according
models that are not being run on the encoding (but that were
run on the other encodings). This allows us to evaluate the
suitability of measures to approximate the structural complexity
of more than one type of encoding.
1) Experiment 1:jFj= 500 ,[m; n]Cardinalities:
a) Results for the Parallel Binary Counter ( PP) Encoding
(Table V): The correlation for twGis highest (between 0.61
and 0.76) for all solvers that process the LTn;k
PAR encoding,
but is also within the same interval for Cream. For twI, lowerTABLE IV
DISTRIBUTION OF twGMEASURES OVER jFj= 1000 MODELS
[0;1] [2;4] [5;9] [10;19] [20;39] [40;59] [60;79] [80;99] [100;149] >149
twG(PP)+20 0 23 16 0 20 0 49 11 40
twG(PP)‚Äì20 0 24 15 0 60 0 60 0 0
twG(PO)+22 3 5 2 0 3 7 32 4 101
twG(PO)‚Äì50 3 6 1 20 34 45 20 0 0
twG(C)+20 0 23 16 0 20 0 50 10 39
twG(C)‚Äì20 0 24 15 0 40 20 57 3 0
and upper bounds differ slightly on some solvers and are at
about 0:7for BDD-based solvers and for Cream.
b) Results for the Order ( PO) Encoding (Table VI):
FortwG, only the lower bound shows useful correlations.
The correlation is 0.7 or higher on 2=3of all solvers. For
lingeling and Cream, the correlation is above 0:8. FortwI, the
correlations on the lower bound are higher than on the upper
bound with 1/3 of all solvers above 0.8 and 2=3above 0:7.
c) Results for the CP ( C) Encoding (Table VII): FortwG,
the lower bound shows stronger correlations. The correlation
is 0.7 or higher on 2=3of all solvers. For lingeling and Cream,
the correlation is above 0:8. For twI, the correlations on the
lower bound are higher than on the upper bound with 1/3 of
all solvers above 0.8 and 2=3above 0:7.
2) Experiment 2:jFj= 500 ,[1;1]&[1;]Cardinalities:
a) Results for the Parallel Binary Counter Encoding ( PP)
(Table VIII): Noticeable differences to the Ô¨Årst experiment
are that on some solvers, the correlations for most solvers are
weaker, in particular for twI.
b) Results for the Order ( PO) Encoding (Table IX):
It is noticeable that, while tw(PO(M))+shows relatively
strong correlations on both graph representations in the Ô¨Årst
experiment for CHOCO, Cream, and JaCoP, the correlations
for these solvers decrease in the second experiment.
c) Results for the CP ( C) Encoding (Table X): It is
noticeable that the correlations for most solvers remain at
the same level or slightly decrease compared to the Ô¨Årst
experiment. However, the correlations for Cream and JaCoP
show a noticeable drop.
3) Experiment 3:jFj= 1000 ,[m; n]Cardinalities: Tables
showing the results of this experiment show only the two
columns with correlations for tw+
Gandtw 
G, because, as
mentioned before, the incidence graph twIwas not computed
on thejFj= 1000 models.
a) Results for the Parallel Binary Counter ( PP) Encoding
(Table XI): The correlations are at approximately the same
level as in the Ô¨Årst experiment with a slight decrease on some
solvers, in particular on the lower bound of twG. For some
solvers, in particular PicoSAT, there is a noticeable decrease
of the correlation compared to the Ô¨Årst experiment.
b) Results for the Order ( PO) Encoding (Table XII):
While for buddy, CUDD, JavaBDD, and SAT4J there are
only slight differences, the correlation for the other solvers is
noticeably weaker on the third experiment.
c) Results for the CP ( C) Encoding (Table XIII): It
is noticeable that correlations drop compared to the Ô¨Årst460TABLE V
SIGNIFICANT CORRELATIONS FOR EXPERIMENT 1,ENCODING PP(M)
corr(tw G‚Äì,t) corr(tw G+,t)corr(tw I‚Äì,t)corr(tw I+,t)
buddy 0.739 0.726 0.691 0.722
CUDD 0.741 0.730 0.709 0.725
JavaBDD 0.745 0.732 0.712 0.728
clasp (LT PAR(n,k) enc.) 0.756 0.706 0.573 0.661
lingeling (LTPAR(n,k) enc.) 0.690 0.661 0.660 0.645
PicoSAT (LTPAR(n,k) enc.) 0.664 0.610 0.519 0.580
SAT4J (LTPAR(n,k) enc.) 0.739 0.713 0.685 0.694
clasp (order enc.) 0.524 0.477 0.465 0.432
lingeling (order enc.) 0.493 0.458 0.467 0.424
PicoSAT (order enc.) 0.482 0.440 0.442 0.412
SAT4J (order enc.) 0.592 0.533 0.410 0.453
CHOCO 0.368 0.328 0.292 0.318
Cream 0.722 0.708 0.710 0.724
JaCoP 0.572 0.520 0.524 0.505
Mistral 0.209 0.203 ‚Äì 0.212
TABLE VI
SIGNIFICANT CORRELATIONS FOR EXPERIMENT 1,ENCODING PO(M)
corr(tw G‚Äì,t)corr(tw G+,t)corr(tw I‚Äì,t)corr(tw I+,t)
buddy 0.710 ‚Äì 0.668 0.414
CUDD 0.772 ‚Äì 0.770 0.349
JavaBDD 0.721 ‚Äì 0.693 0.443
clasp (LT PAR(n,k) enc.) 0.759 ‚Äì 0.725 0.440
lingeling (LTPAR(n,k) enc.) 0.821 ‚Äì 0.801 0.346
PicoSAT (LTPAR(n,k) enc.) 0.757 -0.166 0.801 0.329
SAT4J (LTPAR(n,k) enc.) 0.795 ‚Äì 0.779 0.479
clasp (order enc.) 0.677 -0.167 0.836 0.364
lingeling (order enc.) 0.672 -0.150 0.822 0.363
PicoSAT (order enc.) 0.663 -0.172 0.821 0.364
SAT4J (order enc.) 0.716 -0.208 0.773 0.365
CHOCO 0.609 -0.213 0.552 ‚Äì
Cream 0.890 ‚Äì 0.789 0.361
JaCoP 0.753 ‚Äì 0.766 0.231
Mistral 0.518 -0.359 0.442 -0.193
TABLE VII
SIGNIFICANT CORRELATIONS FOR EXPERIMENT 1,ENCODING C(M)
corr(tw G‚Äì,t)corr(tw G+,t)corr(tw I‚Äì,t)corr(tw I+,t)
buddy 0.728 0.721 0.398 0.387
CUDD 0.740 0.730 0.358 0.339
JavaBDD 0.735 0.728 0.385 0.377
clasp (LT PAR(n,k) enc.) 0.762 0.709 ‚Äì ‚Äì
lingeling (LTPAR(n,k) enc.) 0.689 0.660 0.343 0.284
PicoSAT (LTPAR(n,k) enc.) 0.649 0.602 0.216 ‚Äì
SAT4J (LTPAR(n,k) enc.) 0.730 0.713 0.362 0.343
clasp (order enc.) 0.553 0.510 ‚Äì 0.182
lingeling (order enc.) 0.530 0.499 0.235 0.241
PicoSAT (order enc.) 0.512 0.475 ‚Äì 0.193
SAT4J (order enc.) 0.587 0.530 ‚Äì ‚Äì
CHOCO 0.368 0.333 ‚Äì ‚Äì
Cream 0.727 0.708 0.468 0.432
JaCoP 0.572 0.526 0.302 0.168
Mistral 0.225 0.217 ‚Äì ‚Äì
TABLE VIII
SIGNIFICANT CORRELATIONS FOR EXPERIMENT 2,ENCODING PP(M)
corr(tw G‚Äì,t)corr(tw G+,t)corr(tw I‚Äì,t)corr(tw I+,t)
buddy 0.744 0.728 0.612 0.719
CUDD 0.734 0.722 0.625 0.715
JavaBDD 0.752 0.735 0.628 0.726
clasp (LT PAR(n,k) enc.) 0.721 0.687 0.535 0.627
lingeling (LTPAR(n,k) enc.) 0.745 0.707 0.510 0.615
PicoSAT (LTPAR(n,k) enc.) 0.672 0.609 0.445 0.498
SAT4J (LTPAR(n,k) enc.) 0.713 0.679 0.566 0.684
clasp (order enc.) 0.529 0.464 0.422 0.425
lingeling (order enc.) 0.500 0.458 0.450 0.469
PicoSAT (order enc.) 0.519 0.466 0.406 0.451
SAT4J (order enc.) 0.534 0.476 0.431 0.435
CHOCO 0.247 0.187 ‚Äì ‚Äì
Cream 0.672 0.638 0.520 0.599
JaCoP 0.347 0.271 ‚Äì ‚Äì
Mistral ‚Äì ‚Äì ‚Äì ‚ÄìTABLE IX
SIGNIFICANT CORRELATIONS FOR EXPERIMENT 2,ENCODING PO(M)
corr(tw G‚Äì,t)corr(tw G+,t)corr(tw I‚Äì,t)corr(tw I+,t)
buddy 0.711 ‚Äì 0.670 0.404
CUDD 0.774 ‚Äì 0.767 0.324
JavaBDD 0.724 ‚Äì 0.695 0.423
clasp (LT PAR(n,k) enc.) 0.837 ‚Äì 0.784 0.349
lingeling (LTPAR(n,k) enc.) 0.806 ‚Äì 0.721 0.421
PicoSAT (LTPAR(n,k) enc.) 0.774 -0.172 0.834 0.323
SAT4J (LTPAR(n,k) enc.) 0.757 ‚Äì 0.742 0.470
clasp (order enc.) 0.683 -0.200 0.913 0.348
lingeling (order enc.) 0.685 -0.176 0.898 0.348
PicoSAT (order enc.) 0.698 -0.172 0.893 0.350
SAT4J (order enc.) 0.703 -0.175 0.864 0.353
CHOCO 0.441 -0.309 0.486 ‚Äì
Cream 0.819 ‚Äì 0.803 0.330
JaCoP 0.485 -0.276 0.607 ‚Äì
Mistral 0.327 -0.465 0.344 -0.318
TABLE X
SIGNIFICANT CORRELATIONS FOR EXPERIMENT 2,ENCODING C(M)
corr(tw G‚Äì,t)corr(tw G+,t)corr(tw I‚Äì,t)corr(tw I+,t)
buddy 0.728 0.721 0.397 0.386
CUDD 0.729 0.722 0.381 0.363
JavaBDD 0.736 0.728 0.386 0.377
clasp (LT PAR(n,k) enc.) 0.704 0.672 0.279 0.280
lingeling (LTPAR(n,k) enc.) 0.740 0.711 ‚Äì 0.218
PicoSAT (LTPAR(n,k) enc.) 0.660 0.603 0.216 ‚Äì
SAT4J (LTPAR(n,k) enc.) 0.694 0.669 0.360 0.357
clasp (order enc.) 0.548 0.497 ‚Äì ‚Äì
lingeling (order enc.) 0.529 0.501 0.224 0.234
PicoSAT (order enc.) 0.547 0.507 ‚Äì 0.180
SAT4J (order enc.) 0.553 0.502 ‚Äì ‚Äì
CHOCO 0.271 0.203 ‚Äì -0.203
Cream 0.682 0.644 0.385 0.318
JaCoP 0.364 0.287 ‚Äì -0.218
Mistral ‚Äì ‚Äì ‚Äì -0.282
TABLE XI
SIGNIFICANT CORRELATIONS FOR EXPERIMENT 3,ENCODING PP(M)
corr(tw G‚Äì,t)corr(tw G+,t)
buddy 0.744 0.716
CUDD 0.764 0.740
JavaBDD 0.752 0.723
clasp (LT PAR(n,k) enc.) 0.647 0.565
lingeling (LTPAR(n,k) enc.) 0.722 0.512
PicoSAT (LTPAR(n,k) enc.) 0.554 0.487
SAT4J (LTPAR(n,k) enc.) 0.736 0.717
clasp (order enc.) 0.584 0.523
lingeling (order enc.) 0.554 0.572
PicoSAT (order enc.) 0.571 0.549
SAT4J (order enc.) 0.671 0.532
CHOCO ‚Äì ‚Äì
Cream 0.710 0.704
JaCoP 0.404 0.324
Mistral 0.184 ‚Äì
TABLE XII
SIGNIFICANT CORRELATIONS FOR EXPERIMENT 3,ENCODING PO(M)
corr(tw G‚Äì,t)corr(tw G+,t)
buddy 0.754 ‚Äì
CUDD 0.801 ‚Äì
JavaBDD 0.763 ‚Äì
clasp (LT PAR(n,k) enc.) 0.702 ‚Äì
lingeling (LTPAR(n,k) enc.) 0.670 0.167
PicoSAT (LTPAR(n,k) enc.) 0.594 ‚Äì
SAT4J (LTPAR(n,k) enc.) 0.777 0.192
clasp (order enc.) 0.602 ‚Äì
lingeling (order enc.) 0.617 ‚Äì
PicoSAT (order enc.) 0.622 ‚Äì
SAT4J (order enc.) 0.650 ‚Äì
CHOCO 0.264 -0.261
Cream 0.808 0.172
JaCoP 0.480 ‚Äì
Mistral 0.358 -0.249461TABLE XIII
SIGNIFICANT CORRELATIONS FOR EXPERIMENT 3,ENCODING C(M)
corr(tw G‚Äì,t)corr(tw G+,t)
buddy 0.719 0.716
CUDD 0.744 0.740
JavaBDD 0.727 0.723
clasp (LT PAR(n,k) enc.) 0.620 0.558
lingeling (LTPAR(n,k) enc.) 0.681 0.517
PicoSAT (LTPAR(n,k) enc.) 0.499 0.473
SAT4J (LTPAR(n,k) enc.) 0.720 0.707
clasp (order enc.) 0.553 0.524
lingeling (order enc.) 0.542 0.574
PicoSAT (order enc.) 0.544 0.549
SAT4J (order enc.) 0.635 0.539
CHOCO ‚Äì ‚Äì
Cream 0.685 0.702
JaCoP 0.384 0.324
Mistral 0.170 ‚Äì
experiment for clasp, lingeling, PicoSAT, and JaCoP. For the
Ô¨Årst two, the drop is especially on the upper bound.
F . Interpretation
Summarizing the results of experiment 1, there is a signiÔ¨Åcant
correlation between at least one tree width measure ( twGor
twI) and execution time in all cases. Unexpectedly, for tools
processing the LTn;k
PAR encoding (thus actually belonging to the
PP(M)category) the correlation for tw(PO(M))(referring
to both twGandtwI) is higher than the correlation for
tw(PP(M))on some solvers. However, the correlation for
the Sugar-based solvers and tw(PP(M))is not signiÔ¨Åcant.
Comparing the correlations for the tree width of the encod-
ings that were used on a particular solver in the experiment
(highlighted values in Tables V‚ÄìXIII) with the tree width of
the encodings that were not used on this solver, the latter are
in most cases weaker than the former. The difference between
these correlations is in most cases higher on twIthan on
twG. There are some cases (see e.g. Table VI), where only
correlations for twIshow this difference. From this, it can
be concluded that using the incidence graph (instead of the
Gaifman graph) is more useful for analyzing the correlation
for the encoding that is particular to the analysis tool that
was used (e.g. the correlation for tw(PP(M))onLTn;k
PAR-
based solvers) compared to using it as a general complexity
metric. This conforms to graph theory, as the incidence graph
is a more structure-preserving representation of the original
hypergraph than the Gaifman graph. This is because the
Gaifman graph abstracts from the hyperedges, leading to more
similar hypergraphs for the different encodings.
Another noticeable observation is the great difference be-
tween the correlations for tw(PO(M)) andtw(PO(M))+,
see Tables VI, IX, and XII. Compared to this, the difference
between the lower and upper bound values for PP(M)and
C(M)(see other tables) is very small. In particular, while
twI(PO(M)) produces signiÔ¨Åcant correlations for LTn;k
PAR-
based tools (see Tables VI, where the correlations for lingeling
and picosat are above 0.8, but also Tables IX and XII), the
correlations for tw(PO(M))+are much weaker in all of the
cases. The difference in the correlations is caused by deviations
in the computed tree width values due to the computationalhardness of computing tw(PO). Interestingly, the revealed
correlations indicate that in particular the lower bound approx-
imation for twG(PO(M)) constitutes a good measure for
structural complexity. The correlations for twG(PO(M)) are
strong and signiÔ¨Åcant for most solvers.
FortwG(PO(M)) (see e.g. the value for Cream in Table VI
that is 0.89) and twI(PO(M)) , signiÔ¨Åcant correlations can be
observed on all CP-based solvers. However, these correlations
are exceptionally low for Mistral, while Cream shows signiÔ¨Å-
cant correlations on all experiments. There are correlations for
Cream and JaCoP on the twG(C(M))measure. The JaCoP
solver shows correlations for twG(PO(M)) andtwI(C(M))
in the Ô¨Årst experiment that disappears on the larger and the
cardinality-restricted models. The Mistral solver does not show
any signiÔ¨Åcant correlations for these measures.
On experiment 2 with only [1;1]and[1;]cardinality
constraints, the distribution of the correlations for LTn;k
PAR
and Sugar-based solvers is similar to the Ô¨Årst experiment.
For JaCoP and CHOCO, the correlations on twG(C(M))and
twG(PO(M)) drop from between 0:526and0:89in the Ô¨Årst
experiment to at most 0:485. The correlation for Mistral even
drops below signiÔ¨Åcance. Moreover, for Sugar-based solvers,
the correlations for twI(PP(M))drop, while correlations for
twI(PO(M))increase. In general, experiment 2 indicates that
there are some differences between arbitrary and restricted
cardinalities on at least some solvers.
Experiment 3 with jFj= 1000 and[m; n]cardinality
constraints conÔ¨Årms the previous Ô¨Åndings. The correlations
for the Sugar encoding are the strongest on some LTn;k
PAR-
based solvers. The highest correlation for Sugar-based solvers
can be observed for tw(PO(M)). A noticeable difference to
other experiments is that the values of the correlations dropped
signiÔ¨Åcantly or even disappeared for PicoSAT, CHOCO, and
JaCoP. The correlations for JaCoP on twIcould not be veriÔ¨Åed
since twIcould not be computed on the size 1000 models.
In summary, the results of the experiments conÔ¨Årm the
hypothesis, as there are signiÔ¨Åcant and strong correlations
for all CNF-based and the majority of CP-based solvers.
Throughout all experiments, as the Gaifman graph is concerned,
our experiment suggests twG(PO(M)) as the best structural
complexity measure. The experiments indicate that it can
satisfy our quality criteria for a metric for the hardness of
the analysis (see Section I-C) on the participating CNF-based
solvers and on CP-based solvers, since the correlations between
solver execution time (what we consider as hardness) and
twG(PO(M)) are present throughout all experiments.
G. Tree Width Based Tool Performance Characterization
The measures revealed by the experiment allow us to
characterize the performance of SOTA analysis tools based
on the structure of the analyzed models. For evaluation, we
repeated our experiment on all models from SPLOT7, a
common benchmark for the FM community. Figure 2 shows an
example of a tree width based performance characterization for
7www.splot-research.org462t(s)
twG(C(M)) 
Fig. 2. Example of applying tree width to SPLOT
the Cream solver operating on the CP encoding. It is based on
the lower bound of the tree width of the Gaifman graph of the
CP encoding. The example shows that the solution procedure is
fast (low execution time) and reliable (low standard deviation)
on models with low Gaifman tree width, while models with
higher tree width are increasingly hard to solve for Cream.
There are few complex models in SPLOT. To improve the
visualization of these models in the Ô¨Ågure, we group models
with a tree width value within the shown intervals for all models
withtwG7. The number of models with twG7per single
twGvalue in SPLOT is very low ( 7models per value), and
it remains even relatively low ( 10) for the depicted interval
groups. Accordingly, conclusions drawn from the last three
boxplots might not be valid. The correlation for most solvers
on SPLOT models using tw 
Gis signiÔ¨Åcant. In the example, it
is0:67(on the raw data, without the grouping). The full data
from our experiments and the complete SPLOT example is
available for further analysis from our website8.
This type of performance characterization ‚Äì as well as the
basic experiments ‚Äì can be used for evaluation and further
improvement of tools running on a particular formalization
by comparing the results on models with a formalization of
particular structural complexity. Though, this is not intended
to build an automated procedure based on tree width to select
tools dynamically in particular veriÔ¨Åcation scenarios. For this,
one would have to compute the tree width value for the
encoding used by several tools before tool selection, which
would probably not be efÔ¨Åcient.
V. C ONCLUSION
In this paper, we contribute to measuring the complexity of
analysis operations on FM by proposing and evaluating the use
of graph width measures as metrics for the structural complexity
of FMs. Answering our research question, we can state that our
experiment shows the general reasonability of these metrics
and reveals two interesting conclusions. First, we can conclude
that the incidence graph of the formalization is most useful for
8http://www.sse.uni-due.de/images/Ô¨Åles/PohlEtAl2013-data.pdfmeasuring the structural complexity of a model with respect
to a particular formalization (e.g. an order-encoded CNF). A
possible explanation for this is that the preservation of the
hypergraph structure by the Gaifman graph is not sufÔ¨Åcient for
evaluating differences between individual encodings. Second,
based on our correlation analysis, we are able to propose the
lower bound heuristic of the tree width of the order encoding
(twG(PO) ) as a reasonable metric for the hardness of the
analysis run on most SOTA analysis tools, even for those that
do not apply order encoding. Such a complexity metric can be
used as a basis for a uniÔ¨Åed method to systematically improve
SOTA analysis tools.
Moreover, we implemented CoFFeMAP, a framework to
conduct systematic experiments with measures that can be
used to characterize the performance of solvers. Results from
these experiments can be used to reveal classes of models that
are easy to solve using a particular tool. Future work can use
this as a basis to systematically examine the impact of tool
improvements on the analysis of particular models. Our long-
term vision is to encourage the collection of knowledge about
the complexity of FM analysis apart from worst-case time
complexity. Accordingly, we expect our presented framework
to contribute to overcoming the lack of knowledge about the
behavior of different automated FM analysis tools.
In the future, we plan a rigorous analysis including the
different heuristics within one tool as it was done in [ 18] in
order to reveal their performance in analyzing FMs with a
constant structural complexity. This analysis should include
existing case studies, e.g. the Linux kernel feature model
[33]. Moreover, we plan to extend our analyses conducted
on generated models to realistic handcrafted models (e.g. from
industrial case studies) in order to improve the validity of our
results. In particular, it is still questionable why, in the SPLOT
case, models are more easy to process for analysis tools than
generated models. Models from industrial case studies might
help to reveal the reason for this difference ‚Äì whether it lies in
the nature of SPLOT that consists mostly of academic models
or in the possible artiÔ¨Åcial hardness of our generated models.
We further envision considering more tools that use ad-
vanced FM analysis techniques, e.g. the BDD heuristics from
Mendon√ßa [ 22] and SMT solvers, to cover the full state-of-
the-art in FM analysis. Applying the presented systematic
characterization to this wide range of SOTA analysis tools
will yield new opportunities for the construction of a new
generation of reliable and scalable FM analysis tools. In a
second step, to evaluate this knowledge, we would like to
extend our experiments to reÔ¨Çect several typical usage scenarios
of FM validation tools that will reveal knowledge on more
complex analysis operations than the validity check.
ACKNOWLEDGMENT
We would like to thank Sergio Segura and Ana B. S√°nchez
for providing us with models from BeTTy [ 31] as well as
helpful comments on this paper. This work has been funded
by the DFG under grant PO 607/4-1 KOPI.463REFERENCES
[1]E. Bagheri and D. Gasevic. Assessing the maintainability of software
product line feature models using structural metrics. Software Quality
Journal , 19, 9 2011.
[2]Y . Ben-Haim, A. Ivrii, O. Margalit, and A. Matsliah. Perfect hashing and
CNF encodings of cardinality constraints. In Proc. 15th Int. Conf. on
Theory and Application of SatisÔ¨Åability Testing , pages 397‚Äì409. Springer,
2012.
[3]D. Benavides, S. Segura, and A. Ruiz-Cort√©s. Automated analysis of
feature models 20 years later: A literature review. Inf. Syst. , 35(6):615‚Äì
636, 2010.
[4]D. Benavides, S. Segura, P. Trinidad, and A. Ruiz-Cort√©s. A Ô¨Årst step
towards a framework for the automated analysis of feature models.
InManaging Variability for Software Product Lines: Working With
Variability Mechanisms , 2006.
[5]D. Benavides, P. Trinidad, and A. Ruiz-Cort√©s. Automated reasoning
on feature models. In Proc. 17th Int. Conf. on Advanced Information
Systems Engineering , pages 381‚Äì390. Springer, 2005.
[6]A. Biere, M. Heule, H. van Maaren, and T. Walsh. Handbook
of SatisÔ¨Åability: Volume 185 Frontiers in ArtiÔ¨Åcial Intelligence and
Applications . IOS Press, Amsterdam, The Netherlands, The Netherlands,
2009.
[7]H. L. Bodlaender. Treewidth: Characterizations, applications, and
computations. In Proc. 32nd Int. WS on Graph-Theoretic Concepts
in Computer Science , pages 1‚Äì14. Springer, 2006.
[8]B. Courcelle and S. Olariu. Upper bounds to the clique width of graphs.
Discrete Appl. Math. , 101(1-3):77‚Äì114, 2000.
[9]K. Czarnecki, P. Gr√ºnbacher, R. Rabiser, K. Schmid, and A. Wasowski.
Cool features and tough decisions: a comparison of variability modeling
approaches. In Proc. 6th Int. WS on Variability Modeling of Software-
Intensive Systems , pages 173‚Äì182, New York, NY , USA, 2012. ACM.
[10] A. Favier, S. De Givry, and P. J√©gou. Exploiting problem structure for
solution counting. In Proc. 15th Int. Conf. on Principles and Practice
of Constraint Programming , pages 335‚Äì343, Berlin/Heidelberg, 2009.
Springer.
[11] R. Ganian, P. Hlin Àáen√Ω, and J. Obdr≈æ√°lek. Better algorithms for
satisÔ¨Åability problems for formulas of bounded rank-width. ArXiv e-
prints , 2010.
[12] R. Ganian, P. Hlin Àáen√Ω, J. Kneis, D. Meister, J. Obdr≈æ√°lek, P. Rossmanith,
and S. Sikdar. Are there any good digraph width measures? In Proc.
5th Int. WS on Parameterized and Exact Computation , pages 135‚Äì146.
Springer, 2010.
[13] Y . Gil, S. Kremer-Davidson, and I. Maman. Sans constraints? feature
diagrams vs. feature models. In Proc. 14th Int. Conf. on Software Product
Lines , pages 271‚Äì285, Berlin/Heidelberg, 2010. Springer.
[14] M. Grohe. The structure of tractable constraint satisfaction problems. In
Proc. 31st Int. Conf. on Mathematical Foundations of Computer Science ,
pages 58‚Äì72, Berlin/Heidelberg, 2006. Springer.
[15] P. Hlin Àáen√Ω, S.-I. Oum, D. Seese, and G. Gottlob. Width parameters
beyond tree-width and their applications. Computer Journal , 51(3):326‚Äì
362, 2008.
[16] G. Janssen. A consumer report on BDD packages. In Proc. 16th Symp.
on Integrated Circuits and Systems Design , pages 217‚Äì222, 2003.
[17] K. Kang, S. Cohen, J. Hess, W. Novak, and A. S. Peterson. Feature-
Oriented Domain Analysis (FODA) Feasibility Study. Tech. rep., CMU
Software Engineering Institute, 1990.
[18] H. Katebi, K. A. Sakallah, and J. a. P. Marques-Silva. Empirical study
of the anatomy of modern SAT solvers. In Proc. 14th Int. Conf. on
Theory and Application of SatisÔ¨Åability Testing , pages 343‚Äì356, Berlin,
Heidelberg, 2011. Springer-Verlag.[19] C. Luque, M. Moreto, F. Cazorla, R. Gioiosa, A. Buyuktosunoglu, and
M. Valero. CPU accounting for multicore processors. IEEE Transactions
on Computers , 61(2):251‚Äì264, 2012.
[20] D. Marx. Approximating fractional hypertree width. In Proc. Annual
ACM-SIAM Symposium on Discrete Algorithms , pages 902‚Äì911, 2009.
[21] M. Mendonca, A. Wasowski, and K. Czarnecki. SAT-based analysis
of feature models is easy. In Proceedings of the 13th Int. Software
Product Line Conf. , SPLC ‚Äô09, pages 231‚Äì240, Pittsburgh, PA, USA,
2009. Carnegie Mellon University.
[22] M. Mendon√ßa. EfÔ¨Åcient Reasoning Techniques for Large Scale Feature
Models . PhD thesis, University of Waterloo, 2008.
[23] S.-i. Oum. Rank-width and vertex-minors. J. Comb. Theory Ser. B ,
95(1):79‚Äì100, 2005.
[24] J. Petke and P. Jeavons. The order encoding: From tractable CSP to
tractable SAT. In Proc. 14th Int. Conf. on Theory and Application of
SatisÔ¨Åability Testing , pages 371‚Äì372. Springer, 2011.
[25] K. Pohl, G. B√∂ckle, and F. J. v. d. Linden. Software Product Line
Engineering: Foundations, Principles and Techniques . Springer New
York, Inc., Secaucus, NJ, USA, 2005.
[26] R. Pohl, K. Lauenroth, and K. Pohl. A performance comparison of
contemporary algorithmic approaches for automated analysis operations
on feature models. In Proc. 26th IEEE/ACM Int. Conf. on Automated
Software Engineering , pages 313‚Äì322, Washington, DC, USA, 2011.
IEEE Computer Society.
[27] N. Robertson and P. Seymour. Graph minors. III. planar tree-width. J.
Combinatorial Theory, Series B , 36(1):49 ‚Äì 64, 1984.
[28] M. Samer and S. Szeider. Constraint satisfaction with bounded treewidth
revisited. J. Computer and System Sciences , 76(2):103 ‚Äì 114, 2010.
[29] A. S. Sayyad, T. Menzies, and H. Ammar. On the value of user
preferences in search-based software engineering: a case study in software
product lines. In Proceedings of the 2013 International Conference on
Software Engineering , ICSE ‚Äô13, pages 492‚Äì501, Piscataway, NJ, USA,
2013. IEEE Press.
[30] P.-Y . Schobbens, P. Heymans, and J.-C. Trigaux. Feature diagrams:
A survey and a formal semantics. In Proc. 14th IEEE Int. Conf. on
Requirements Engineering , pages 139 ‚Äì148, 2006.
[31] S. Segura, J. Galindo, D. Benavides, J. Parejo, and A. Ruiz-Cort√©s. BeTTy:
Benchmarking and testing on the automated analysis of feature models.
In U. Eisenecker, S. Apel, and S. Gnesi, editors, Sixth International WS
on Variability Modelling of Software-intensive Systems (VaMoS‚Äô12) , page
63‚Äì71, Leipzig, Germany, 2012. ACM, ACM.
[32] S. Segura, J. Parejo, R. Hierons, and A. Ruiz-Cort√©s. ETHOM: An
evolutionary algorithm for optimized feature models generation (v. 1.1).
Technical report, 2012.
[33] S. She, R. Lotufo, T. Berger, A. Wasowski, and K. Czarnecki. Variability
model of the Linux kernel. In Fourth International Workshop on
Variability Modeling of Software-intensive Systems (VaMoS 2010) , Linz,
Austria, 2010.
[34] C. Sinz. Towards an optimal CNF encoding of boolean cardinality
constraints. In Proc. 11th Int. Conf. on Principles and Practice of
Constraint Programming , pages 827‚Äì831. Springer, 2005.
[35] N. Tamura, A. Taga, S. Kitagawa, and M. Banbara. Compiling Ô¨Ånite
linear CSP into SAT. Constraints , 14:254‚Äì272, 2009.
[36] V . ≈†tuikys and R. Dama≈°evi Àácius. Measuring complexity of domain
models represented by feature diagrams. Information Technology and
Control , 38(3):179‚Äì187, 2009.
[37] C. Wohlin, P. Runeson, M. H√∂st, M. C. Ohlsson, B. Regnell, and
A. Wessl√©n. Experimentation in software engineering: an introduction .
Kluwer Academic Publishers, Norwell, MA, USA, 2000.464