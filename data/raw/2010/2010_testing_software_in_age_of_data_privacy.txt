Testing Software In Age Of Data Privacy: A Balancing Act
Kunal Taneja
North Carolina
State University
Raleigh, NC 27695
ktaneja@ncsu.eduMark Grechanik
Accenture Technology Labs
Chicago, IL 60601
mark.grechanik@
accenture.comRayid Ghani
Accenture Technology Labs
Chicago, IL 60601
rayid.ghani@
accenture.comTao Xie
North Carolina
State University
Raleigh, NC 27695
xie@csc.ncsu.edu
ABSTRACT
Database-centric applications (DCAs) are common in enterprise
computing, and they use nontrivial databases. Testing of DCAs is
increasingly outsourced to test centers in order to achieve lower
cost and higher quality. When proprietary DCAs are released, their
databases should also be made available to test engineers. How-
ever, different data privacy laws prevent organizations from shar-
ing this data with test centers because databases contain sensitive
information. Currently, testing is performed with anonymized data,
which often leads to worse test coverage (such as code coverage)
and fewer uncovered faults, thereby reducing the quality of DCAs
and obliterating beneﬁts of test outsourcing.
To address this issue, we offer a novel approach that combines
program analysis with a new data privacy framework that we design
to address constraints of software testing. With our approach, or-
ganizations can balance the level of privacy with needs of testing.
We have built a tool for our approach and applied it to nontrivial
Java DCAs. Our results show that test coverage can be preserved
at a higher level by anonymizing data based on their effect on cor-
responding DCAs.
Categories and Subject Descriptors
D.2.5 [ Software Engineering, Testing and Debugging ]: Testing
tools; D.4.6 [ Software Engineering, Security and Protection ]:
Information ﬂow controls; K.4.1 [ Computers and Society, Public
Policy Issues ]: Privacy
General Terms
Security, Veriﬁcation
Keywords
Data anonymity, software testing, privacy framework, utility, PRIEST
1. INTRODUCTION
Large organizations today face many challenges when engineer-
ing software applications. Particularly challenging is the fact that
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
ESEC/FSE’11, September 5–9, 2011, Szeged, Hungary.
Copyright 2011 ACM 978-1-4503-0443-6/11/09 ...$10.00.many applications work with existing databases that contain conﬁ-
dential data. A large organization, such as a bank, insurance com-
pany, or government agency, typically hires an external company to
develop or test a new custom software application. However, recent
data protection laws and regulations [35] around the world prohibit
data owners to easily share conﬁdential data with external software
service providers.
Database-centric applications (DCAs) are common in enterprise
computing, and they use nontrivial databases [26]. When releasing
these proprietary DCAs to external test centers, it is desirable for
DCA owners to make their databases available to test engineers, so
that they can perform testing using original data . However, since
sensitive information cannot be disclosed to external organizations,
testing is often performed with synthetic input data . For instance,
if values of the ﬁeld Nationality are replaced with the generic value
“Human ,” DCAs may execute some paths that result in exceptions
or miss certain paths [23]. As a result, test centers report worse
test coverage (such as code coverage) and fewer uncovered faults,
thereby reducing the quality of applications and obliterating bene-
ﬁts of test outsourcing [30].
Automatic approaches for test data generation [12, 17, 22, 29,
37] partially address this problem by generating synthetic input
data that lead program execution toward untested statements. How-
ever, one of the main issues for these approaches is how to gener-
ate synthetic input data with which test engineers can achieve good
code coverage. Using original data enables different approaches in
testing and privacy to produce higher-quality synthetic input data
[3][21, page 42], thus making original data important for test out-
sourcing.
A fundamental problem in test outsourcing is how to allow a
DCA owner to release its private data with guarantees that the enti-
ties in this data (e.g., people, organizations) are protected at a cer-
tain level while retaining testing efﬁcacy. Ideally, sanitized data
(sanitized data or anonymized data is the original data after anony-
mization. We use the two terms interchangeably throughout this pa-
per) should induce execution paths that are similar to the ones that
are induced by the original data. In other words, when data is san-
itized, information about how DCAs use this data should be taken
into consideration. In practice, this consideration rarely happens;
our previous work [23] showed that a popular data anonymization
algorithm, called k–anonymity, seriously degrades test coverage of
DCAs.
Naturally, different DCAs have different privacy goals and lev-
els of data sensitivity – privacy goals are more relaxed for a DCA
that manages a movie ticket database than for a DCA that is used
within banks or government security agencies. Applying more re-
laxed protection to databases is likely to result in greater test cover-
age since a small part of the databases is anonymized; conversely,
201stricter protection makes it more difﬁcult to outsource testing. The
latter is the result of two conﬂicting goals: making testing as real-
istic as possible and hiding the original data from testers who need
this data to make testing effective. Balancing these goals, i.e., to
anonymize data while preserving test coverage of DCAs that use
this data is emerging to be an important problem.
Given the importance of this problem, it may be surprising that
there exists little prior research on this topic. There may be two
main reasons for the lack of prior research. First, elaborate data pri-
vacy laws are a new phenomenon, and many of these laws [35, 38]
have been introduced after the year 2000. Second, it is only in the
past decade that applications are increasingly being tested by third-
party specialized software service providers, which are also called
test centers. Numerous test centers have emerged and often offer
lower cost and higher quality when compared to in-house testing.
In 2007, the test outsourcing market was worth more than USD 25
billion and growing at 20% annually, making test outsourcing the
fastest growing segment of the application services market [7, 16].
To address this issue, we offer a novel approach, PRIvacy Equal-
izer for Software Testing (PRIEST) that combines a new data pri-
vacy framework with program analysis enabling business analysts
to determine how anonymizing values of database attributes affects
test coverage. With PRIEST, organizations can balance the goals
of preserving test coverage, while releasing DCAs to external test
centers with a controlled disclosure of sensitive information. The
source code for PRIEST as well as its illustration video are publicly
available1. Our work is unique; to the best of our knowledge, there
exists no prior approach that addresses the problem that we pose in
this paper. In summary, we make the following main contributions
in this paper:
•We create a new privacy framework (described in Section 4)
that includes a novel combination of guessing anonymity-
based privacy metrics and a technique of data swapping ano-
nymization to enable organizations to keep original values in
sanitized data; keeping such values is important for improv-
ing the effectiveness of testing.
•We design and implement a technique using program anal-
ysis for determining how values of database attributes affect
test coverage of DCAs that use this data (see Section 3.3).
•We combine our privacy framework with this technique in
PRIEST to enable business analysts to balance data privacy
with test coverage.
•We evaluate PRIEST using three open-source Java DCAs
and one large Java DCA that handles logistics of one of
the largest supermarket chains in Spain. We show that with
PRIEST, test coverage can be preserved at a higher level by
pinpointing database attributes that should be anonymized
based on their effect on corresponding DCAs.
2. THE PROBLEM
In this section, we provide the necessary background on how
DCAs interact with databases, show how sanitizing data affects test
coverage of DCAs, describe the state of the art and practice, and
formulate the problem statement.
2.1 Background
Majority of enterprise-level DCAs use programs that are written
in general-purpose programming languages and relational databases
1http://www.privacytesting.orgr e c o r d s e t = db . e x e c S q l ( "SELECT ∗FROM T b l P a t i e n t " ) ;
n a t i o n a l i t y = r e c o r d s e t . G e t A t t r i b u t e ( i ) ;
age = r e c o r d s e t . G e t A t t r i b u t e ( j ) ;
d i s e a s e = = r e c o r d s e t . G e t A t t r i b u t e ( " D i s e a s e " ) ;
i f( n a t i o n a l i t y ==" P a l a u a n " && age >60) f ( d i s e a s e ) ;
Figure 1: An illustrative example that shows how replacing val-
ues of database attributes Nationality andAge of patients
can make the function funreachable.
to maintain large amounts of data. A primary way for these pro-
grams to communicate with databases is to use call-level interfaces ,
which allow DCAs to access database engines through standardized
application programming interfaces (APIs) , e.g., Java DataBase
Connectivity (JDBC). Using JDBC, programs pass SQL queries
as strings to corresponding API calls to be sent to databases for
execution.
Once these queries are executed, values of attributes of database
tables are returned to DCAs using JDBC RecordSet objects and
these values are stored in variables of the DCAs, which in turn use
these variables as part of their application logic. In this way, values
from a database may be used in branch decisions, and these values
may therefore affect the subsequent execution of the DCAs. De-
pending on returned values, different paths can be taken in DCAs,
and subsequently these values affect test coverage. Hence remov-
ing certain classes of values in database attributes may make some
branches and statements in DCAs unreachable.
To see how anonymization affects test coverage of DCAs, con-
sider a fragment of code shown in Figure 1. After executing JDBC
API calls that submit an SQL query to a database and obtain the ob-
jectrecordset , the values of the attributes Age,Nationality ,
andDisease are put in the corresponding variables of the DCA
nationality ,age, anddisease , respectively. If the val-
ues of the attribute nationality are replaced with the generic value
“Human ,”, the function call f() becomes unreachable.
Certain classes of values of database attributes that contain non-
sensitive information should be anonymized. These attributes, also
called quasi-identiﬁers (QI) often contain information that can be
linked with other data to infer sensitive information about entities
(i.e., people, objects). For example, given the values of the QIs
Race ,Sex,Height ,ZipCode and the attribute that contains
sensitive data about diseases, it is possible to link a person to spe-
ciﬁc diseases, provided that the values of these QIs uniquely iden-
tify this person.
Existing data anonymization approaches are centered on creating
models for privacy goals and developing algorithms for achieving
these goals using particular anonymization techniques [15]. One of
the most popular privacy goals is k−anonymity [34], where each
entity in the database must be indistinguishable from k−1 oth-
ers. Anonymization approaches use different anonymization tech-
niques including suppression, where information (e.g., nationality)
is removed from the data, and generalization, where information
(e.g., age) is coarsened into sets (e.g., into age ranges) [15]. These
and other techniques modify or suppress values of attributes, and
a common side-effect of these modiﬁcations is non-covered state-
ments in DCAs that are otherwise covered with the original data.
2.2 State of the Art and Practice
After interviewing professionals at IBM, Accenture, two large
health insurance companies, a biopharmaceutical company, two
large supermarket chains, and three major banks, we found that
current test data anonymization is manual, laborious, and error-
202prone. In a few cases, client companies outsource testing using
an especially expensive and cumbersome testing procedure called
cleanroom testing , where DCAs and databases are kept on com-
pany premises in a physically secured environment [23]. Typically,
business analysts and test managers from outsourcing companies
come to the cleanrooms of their client companies to evaluate their
clients’ DCAs and to plan work. However, when better options
to access data are not available, testers from outsourcing compa-
nies are also allowed in these cleanrooms to test software on their
clients’ premises. Actions of these test engineers are tightly mon-
itored; network connections, phone calls, cameras, and USB keys
are forbidden. Cleanroom testing requires signiﬁcant resources and
physical proximity of test outsourcing companies to their clients.
A more commonly used approach is to use tools that anonymize
databases indiscriminately, by generalizing or suppressing all data.
Even though this procedure is computationally intensive, it is ap-
pealing since it does not require sophisticated reasoning about pri-
vacy goals and protects all data.
But in many real-world settings, protecting all data blindly makes
testing very difﬁcult. When large databases are repopulated with
fake data, it is likely that many implicit dependencies and patterns
among data elements are missing, thereby reducing testing efﬁcacy.
Moreover, fake data is likely to trigger exceptions in DCAs, leading
test engineers to ﬂood bug-tracking systems with false bug reports.
In addition, testers often cannot use such anonymized data because
DCAs may throw exceptions that would not occur when the DCAs
are tested with original data or other real data in ﬁeld.
A more sophisticated approach is selective anonymization , where
a team is assembled comprising of business analysts and database
and security experts [25, page 134]. After the team sets privacy
goals, identiﬁes sensitive data, and marks database attributes that
may help attackers to reveal this sensitive data (i.e., QIs), anony-
mization techniques are applied to these QIs to protect sensitive
data, resulting in a sanitized database. A goal of all anonymization
approaches is to make it impossible to deduce certain facts about
entities with high conﬁdence from the anonymized data [4, pages
137-156]. Unfortunately, this approach is subjective, manual, and
laborious. In addition, it involves highly trained professionals and
therefore this approach is very expensive. Currently, there exists
no solution that enables these professionals to accomplish this task
efﬁciently and effectively with metrics that clearly explain the cost
and beneﬁts of selective anonymization decisions.
2.3 Balancing Utility And Privacy
Utility of anonymized data is measured in terms of usefulness of
this data for computational tasks when compared with how useful
the original data is for the same tasks. Consider an example of
the utility of calculating average salaries of employees. Adding
random noise to protect salary information will most likely result in
computing incorrect values of average salaries, thereby destroying
utility of this computation. Recent cases with U.S. Census show
that applying data privacy leads to incorrect results [5, 8]. Multiple
studies demonstrate that even modest privacy gains require almost
complete destruction of the data-mining utility [1, 9, 18, 19, 28].
Data swapping is an anonymization technique that is based on
exchanging values of attributes among individual records while
maintaining certain distribution properties [33, 32]. Data swap-
ping is more effective in preserving utility than data suppression
and generalization privacy algorithms since data swapping allows
users to better preserve statistical information [21, page 42]. In
this paper, we develop a data swapping privacy algorithm that al-
lows analysts to balance privacy and utility goals, i.e., to chooseappropriate levels of privacy that will guarantee certain basic test
coverage.
2.4 The Problem Statement
The problem statement that we address in this paper is how to en-
able stakeholders to balance a level of test coverage for DCAs (i.e.,
utility) with privacy for databases that these DCAs use. The prob-
lem space is restricted by three fundamental constraints of software
development and privacy-preserving data publishing as follows.
First, a chosen anonymization approach should preserve original
data as much as possible since it is important for preserving the
testing utility of DCAs. Anonymizing databases using data sup-
pression techniques may result in a different behavior of the DCAs.
Suppose that No⊆Nis the set of all covered nodes in the control
ﬂow graph of a DCA when the DCA is tested with the original data,
andNa⊆Nis the set of all covered nodes when the same DCA is
tested with the anonymized data. In general, testing with anony-
mized data makes DCAs behave in ways that are different from
speciﬁcations, and as a result new execution paths in DCAs with
anonymized data may lead to exceptions or not-covered branches
being covered originally. For example, the DCA logic handles
suppressed values of Nationality by not covering the body of
theifstatement shown in Figure 1. Therefore, in the worst case
NT=No∩Na=⊘, where NTis the set of nodes of the preserved
statement coverage. A problem is how to keep a good extent of
the original data in databases for testing DCAs while guaranteeing
certain levels of privacy.
Our goal is to ensure that all statements (i.e., nodes in the control
ﬂow graph) that are executed with original data will also be exe-
cuted with anonymized data. Our goal is difﬁcult to achieve since
it is an undecidable problem to determine precisely how values of
QIs are used in DCAs [27]. In addition, anonymization algorithms
present a dilemma: suppressing attribute data with different val-
ues results in loss of test coverage, and keeping original data in
the database results in loss of privacy. Balancing these conﬂicting
outcomes is the problem that we address in this paper.
Second, as a result of software evolution [20, page 163], the code
of the DCA is modiﬁed. In consecutive releases, the DCA may use
different database attributes in different ways, thereby requiring the
DCA owner to reanonymize data to ensure that the balance between
privacy and utility is maintained. However, re-anonymization intro-
duces a problem – if an attacker keeps the previous version of the
anonymized data where values of some attributes are not sanitized,
then this attacker can link original values in different releases, thus
inferring sensitive information. A solution should enable stake-
holders to repeatedly release anonymized data in such a way that
both privacy and utility are guaranteed at certain levels.
Finally, a privacy metric should be linked directly to test cover-
age and vice versa, i.e., guaranteeing a certain level of test cover-
age should allow stakeholders to calculate bounds of the privacy
level. For example, if some path is controlled by branch conditions
that use values of database attributes, then it is possible to predict
the effect of anonymization of these values on this path. In other
words, the problem is to present business analysts with choices of
predicted coverage levels for different anonymization goals.
3. OUR SOLUTION
In this section, we present core ideas behind our approach that
we call PRIvacy Equalizer for Software Testing (PRIEST) and we
describe the PRIEST architecture and its workﬂow. In this section,
we concentrate on the overall architecture of PRIEST that uses our
privacy framework that we describe in Section 4.
2033.1 Core Ideas
At the core of our work are three major ideas. The ﬁrst one is a
privacy framework that enables organizations to keep original val-
ues in sanitized data. The framework is based on a data swapping
anonymization technique to preserve original values of database at-
tributes. As a result, test coverage is not affected so negatively as it
happens when data suppression and generalization techniques are
used. The technique swaps data based on a probability value pro-
vided by a user. Hence different levels of privacy can be achieved.
The second idea is our guessing anonymity privacy metric that
allows stakeholders to quantify the level of privacy achieved in an
anonymized database. In particular, the metric provides measure-
ment of difﬁculty for an attacker to relate a sanitized record to the
original record.
The third idea is an idea to statically determine how different da-
tabase values affect the behavior of a DCA. This idea uniﬁes DCAs
and their databases in a novel way – database attributes are tied
to the source code of the DCAs, and depending on how the DCAs
use values of these attributes, business analysts and security experts
can determine what anonymization strategy should be used to pro-
tect data without sacriﬁcing much of test coverage. A key point is
that often not all of the database attributes have to be anonymized
to achieve a given level of data protection. Therefore, it is impor-
tant to extend data protection strategies with information about how
DCAs use their databases to which these strategies are applied.
As it often happens, control-ﬂow decisions in DCAs are affected
by a smaller subset of database attributes. In an extreme case, if
some data in the database is not used in any control-ﬂow decision
of any DCAs, then anonymizing this data will have no effect on
these DCAs. A more subtle point is that values of some attribute
may not affect most branch conditions in DCAs, and therefore test
coverage will not be affected much if this attribute is anonymized.
Thus it is beneﬁcial to focus anonymization on those aspects of
the data that have minimal inﬂuence on deeply nested control-ﬂow
decisions.
3.2 PRIEST Architecture and Process
We propose a novel process for using PRIEST that partially in-
volves the cleanroom testing that we described in Section 2.2. Re-
call that business analysts and test managers from outsourcing com-
panies come to the cleanroom of their client company to evaluate
their client’s DCAs and to plan work. As part of their evaluation,
these analysts and managers determine different sets of attributes of
the database (i.e., QIs) that can be used by attackers to re-identify
entities. In general, only a few subsets of these QIs should be ano-
nymized, thus creating favorable conditions for preserving test cov-
erage. With PRIEST, this QI selection procedure can be improved
by pointing out the QIs that affect test coverage the least. These
analysts and managers can use PRIEST as part of their evaluation
and planning in order to determine how to maximize test coverage
for DCAs while achieving desired privacy goals for databases that
these DCAs use.
As the ﬁrst step of the PRIEST process, programmers link pro-
gram variables to database attributes using annotations, so that
these annotations can be traced statically using control- and data-
ﬂow analyses. Tracing these attribute annotations is required to
determine how the values of these attributes are used in conditional
expressions to make branching decisions, thereby inﬂuencing the
execution ﬂow of the DCAs. Our goal is to quantify the effect of
replacing values of database attributes on reachability of program
statements.
At ﬁrst glance, it appears to be tedious and laborious work for
programmers to annotate program variables with the names of data-DBODCA
1
DBAAnonymi-
zation
Algorithm
Schema
ExtractorStatic 
Analysis
EngineAttribute 
Rankings4
24
Schema and 
relations5
3Quasi 
Identifiers 
(QIs)
Privacy 
Goals6
67
89
10
1112
Figure 2: PRIEST architecture and workﬂow. Solid arrows de-
pict the ﬂow of command and data between components, num-
bers indicate the sequence of operations in the workﬂow.
base attributes from which these variables obtain values. In reality,
it is a practical and modest exercise that takes little time. Program-
mers annotate selected program variables only once, where these
variables are ﬁrst assigned values that are retrieved from recordset
objects using speciﬁc database-related API calls. We observe that
in many projects it is a small fraction of code that deals with obtain-
ing values from databases, and most code is written to implement
application logic that processes these values. This observation is
conﬁrmed by our previous study [24] that shows that out of 2,080
randomly chosen Java programs in Sourceforge, there is approxi-
mately one JDBC-related API call per 2,200 LOC on average that
retrieves a value from a recordset object and assigns it to a program
variable. Extrapolating this result means that programmers may
have to annotate approximately 450 variables for a project with 1
Million LOC and such expense is acceptable.
In addition, a variety of object-relational mapper (O/R mapping)
tools and frameworks bridge the gap between an application’s ob-
ject model and the relational schema of the underlying database by
generating classes that represent objects in relational databases [6].
Since O/R mapping is done automatically, links between program
variables and database attributes are recovered as a by-product of
using O/R mapping tools. For example, one of our subject appli-
cations, a logistics application for handling one of the largest su-
permarket chains in Spain, is written using iBatis2, an open-source
O/R mapper.
Figure 2 shows the architecture of PRIEST. The inputs to PRIEST
are the DCA bytecode and the original database DBOthat this DCA
uses(1). With PRIEST, business analysts, test managers, and se-
curity experts connect to DBO(2) to obtain its schema using the
Schema Extractor that uses JDBC metadata services to obtain (3)
database schema including all relations among different attributes.
Next, PRIEST performs control and dataﬂow analyses (4) using
the Soot toolkit3to establish how the DCA uses values of differ-
ent database attributes. Values of some attributes are used in ex-
pressions to compute other values, which in turn are used in other
expressions and statements. In some cases, these values are used
in conditional statements, and they affect control ﬂows of DCAs
using control-ﬂow dependencies. Ideally, attributes whose values
affect many other expressions and statements in DCAs (in terms of
statement coverage) should not be picked as QIs. The output (5)
of this procedure is a list of attribute rankings that show how many
statements are approximately encapsulated by branches whose con-
ditions contain program variables that receive their values from da-
tabase attributes.
At this point, PRIEST displays to the user (6) the list of ranked
2http://ibatis.apache.org
3http://www.sable.mcgill.ca/soot
204attributes and their relations with other attributes in the database
schema. The user selects (7) a subset of database attributes as
QIs whose values should be anonymized to protect sensitive data
based on certain privacy goals (8) that are required by the DCA
owner. The selected QIs are supplied (9) to the Anonymization
Algorithm along with (10) the required privacy level. In addition,
the algorithm takes (11) DBOas its input and outputs (12) the
anonymized database DBA.
3.3 Ranking Attributes
To understand which attributes affect DCAs the most, we rank
these attributes by counting the numbers of statements that their
values affect. To ﬁnd the preceding information, our approach uses
taint analysis to track the annotated variables (as described in Sec-
tion 3.2) corresponding to each attribute. In particular, for each
attribute, our approach uses control- and data-ﬂow taint propaga-
tion [13] to ﬁnd out branch conditions that are tainted by an an-
notated variable corresponding to the attribute. We then count the
number of statements that are control-dependent on these branch
conditions. We perform virtual-call resolution using static class hi-
erarchy analysis, and we take a conservative approach by counting
all the statements in all the target methods that can potentially be
invoked.
Our experiments on subject applications show that this approach
can predict the effect of anonymization on coverage within less than
8.3% error (see Section 5 and Table 6 for further details). Improv-
ing the precision to compute how many statements are affected by
these attributes is a subject of future work.
4. PRIEST PRIV ACY FRAMEWORK
In this section, we describe the PRIEST privacy framework. We
discuss constraints, show how to connect different aspects of data
privacy with the testing utility, and present a privacy metric and our
anonymization algorithm.
4.1 Constraints and Goals
After discussions with a number of experts from different or-
ganizations who are involved in testing DCAs, we identiﬁed two
main constraints for a successful anonymization solution: simplic-
ity and consistency. A simple anonymization solution should not
impose signiﬁcant laborious manual or intellectual effort on stake-
holders who are involved in protecting privacy. A case at hand
is when using data suppression and generalization algorithms to
achieve k-anonymity, a popular data privacy approach, users must
specify generalization hierarchies that guide corresponding anony-
mization algorithms to replace data values with generalized sub-
stitutes to protect sensitive information. For example, a branch of
the generalization hierarchy for the attribute Nationality could
be{Canadian ,Mexican,USA}→ NorthAmerican→American→
Human . In general, identifying and properly using these hierar-
chies involves deep domain expertise and signiﬁcant effort. Stake-
holders want a high degree of automation where they are presented
with choices for privacy levels and corresponding test coverages,
making it easy for them to balance privacy and utility.
Consistency of data is the other constraint. Since data used in
typical DCAs can span multiple tables in databases, any ﬁeld that
is used as a key to link entities across tables needs to be anonymized
consistently so that the data can be linked correctly after anonymi-
zation. Thus, anonymization techniques should take into consider-
ation different constraints among attributes that are imposed by the
database schema for describing this data.
Preserving original data values is a goal of our anonymization
framework – no new values for a data ﬁeld should be introducedand the unique set of values in a ﬁeld should be preserved after
anonymization. This constraint is important because changing the
values (such as generalizing a ﬁve digit zip code to ﬁrst three digits,
or replacing the city name with the state in the City ﬁeld would
result in new values for a ﬁeld that the DCA may not be able to
handle properly. Having new data values often results in extra effort
to modify the DCA and may cause unintended consequences in
production with real data; for example, exceptions that are thrown
would not be thrown if the original data is used.
Random data generation would typically preserve only univari-
ate properties (marginal distributions) of each attribute. Our ap-
proach can preserve distributions over conjunctions of attributes
(Gender=Female, Diagnosis=Ovarian Cancer) that are useful for
preserving test coverage when conditional statements containing
variables linked to multiple attributes are concerned, and such cases
are common in real-world DCAs.
Finally, in this paper, we assume that the developer or data owner
cannot be an attacker. While other attack models are possible, we
assume that since developers have direct access to all information
that is needed to create software, it is reasonable to assume that
developers enjoy a high level of trust.
4.2 Guessing Anonymity
Guessing anonymity [31] is a privacy metric that enables stake-
holders to quantify the level of privacy using a guessing strategy
of the attacker as a sequence of questions of the form “Are these
the original values of quasi-identiﬁers that are used to generate a
sanitized record?”
Deﬁnition: The guessing anonymity of the sanitized record is the
number of guesses that the optimal guessing strategy of the attacker
requires in order to correctly guess the record used to generate the
sanitized record.
To illustrate this deﬁnition, consider an attacker with knowledge
that Alice and Chris are in the database, and knowledge of their
true ages shown in Table 1 that contains original data. Sanitized
data is shown in Table 2 where names are replaced with “***” and
the values of the attribute Age are perturbed with random noise.
The guessing anonymity of each record is shown in the fourth col-
umn of Table 2. While the record that corresponds to Alice has
a guessing anonymity of two, the sanitized record corresponding
to Chris has a guessing anonymity of one due to the fact that his
age is signiﬁcantly higher than the ages in the other records. The
distribution of guessing anonymity of the different records in a da-
tabase can be used to deﬁne a variety of privacy metrics. Mean
guessing anonymity ,Minimum guessing anonymity , and Fraction
of records with guessing anonymity greater than a certain value m
are some metrics that we discuss later in this section. For our toy
database shown in Table 1, those values would be 1.75, 1, and 0.75
(form=1), respectively.
To further illustrate our deﬁnition of guessing anonymity, we link
our deﬁnition to the deﬁnition of k-anonymity. Recall that in k-
anonymity, a database is considered private if every record has at
least kother records in the original database with which the re-
leased record is indistinguishable. k-anonymity can be achieved
by different anonymization operators but typically suppression and
generalization operators are the most commonly used ones.
Consider a released record that is anonymized such that there
are exactly krecords in the original database with which the re-
leased record is consistent. Without any further information, the
optimal guessing strategy would choose among these krecords with
uniform probability for its ﬁrst guess. The probability of the ﬁrst
205Table 1: Original Database
Name Age Procedure or Prescription
Alice 19 Antidepressants
Bob 15 Antibiotics
Chris 52 Chemotherapy
Diana 25 Abortion
Table 2: Sanitized Database
Name Age Procedure Guessing
or Prescription Anonymity
*** 23.1Antidepressants 2
*** 19.4 Antibiotics 2
*** 49.3 Chemotherapy 1
*** 21.1 Abortion 2
Algorithm 1 ThePriestPrivacy algorithm.
1:PriestPrivacy ({QI},p) {{QI}is the set of QIs, and pis the
probability that the original data will remain unchanged in the
anonymized set, T}
2:/bardblT/bardbl←NULL {Initialize values of the anonymized matrix.}
3:forj←1 to # of attributes in /bardblQI/bardbldo
4: DistinctValues (/bardblQI/bardblj)/mapsto→{V}j{Returns the set of distinct
values for a given attribute.}
5: fori←1 to # of rows in/bardblQI/bardbldo
6: Randomize (QIi
j,p)/mapsto→(∃v∈{V}js.t.Ti
j←v)
7: end for
8:end for
9:return/bardblT/bardbl
guess being correct is1
k. If the ﬁrst guess is incorrect, the second
guess is chosen with uniform probability from among the remain-
ingk−1 records. The probability of the second guess being correct
is1
k−1(1−1
k). The expected number of guesses simpliﬁes tok+1
2,
so the expected guessing anonymity of a k-anonymized record is
k+1
2.
4.3 The PRIEST Anonymization Algorithm
The algorithm PriestPrivacy is shown in Algorithm 1. We
use a data swapping anonymization technique to preserve original
values of database attributes. The goal is to preserve test coverage
better than when data suppression and generalization techniques
are used, while keeping the data usable by the DCA. To protect pri-
vacy, each value for each row for each attribute is swapped with
some probability with a different value for the same attribute. For
example, for the attribute Gender that contains two distinct values
MandF, the user may choose to replace the value of a given cell
with the other value with probability 0 .5, i.e., an unbiased coin ﬂip.
Excluding an attribute from anonymization means that the proba-
bility of value replacement for this attribute is zero.
This algorithm takes as its inputs the matrix of QIs and their val-
ues,/bardblQI/bardbl, whose columns include QIs and rows include different
tuples for these QIs from the original database DBO, and the value
of the probability, p, that the original data will remain unchanged in
the anonymized matrix, /bardblT/bardbl. The matrix/bardblT/bardblhas the same dimen-
sions and semantics as the matrix /bardblQI/bardbl, and it contains anonymized
values of QIs./bardblT/bardbl’s values are initialized to NULL in Line2and
this matrix is returned in Line 9.
Thefor-loop in Lines 3-8 iterates through attributes that are in
/bardblQI/bardbl, and the procedure DistinctValues is called to compute
the set of distinct values for each QI, {V}. Next, in Lines 5-7 the
nestedfor-loop iterates through all rows for the given QI and theAlgorithm 2 Guessing anonymity metric calculation algorithm.
1:PrivacyMetric (/bardblQI/bardbl,/bardblT/bardbl)
2:/bardblD/bardbl← 0{Initialize values of the distance matrix, D.}
3:fori←1 to # of rows in/bardblT/bardbldo
4: fork←1 to # of rows in/bardblQI/bardbldo
5: forj←1 to # of attributes in /bardblQI/bardbldo
6: ifTj
j=QIi
jthen
7: Di
k←Di
k+1
8: end if
9: end for
10: Di
k←Di
k
#o f attributesinQI
11: end for
12:end for
13: {Compute privacy metrics PM1andPM2.}
14:R←0
15:di f f Records←0
16:d×d←/bardblD/bardbl{Dimensions of the similarity matrix.}
17:fori←1 toddo
18: ifD(i,i)<1then
19: di f f Records←di f f Records +1{For metric PM2.}
20: end if
21: forj←1 toddo
22: ifi/ne}ationslash=j∧D(i,j)≥D(i,i)then
23: R←R+1
24: end if
25: end for
26:end for
27:return PM1=R
d,PM2=di f f Records
d
procedureRandomized is invoked to replace the original value in
a cell with one of the original distinct values of the given QI, and
the replaced value is written in the corresponding location in the
matrix/bardblT/bardbl. Once the algorithm iterates over all QIs and all rows
for each QI, it terminates and returns /bardblT/bardblin Line9.
An example of application of the algorithm PriestPrivacy
is shown in Table 3 that contains (for illustrative purposes) three
attributes:Age,Gender , andRace . The ﬁrst column of this ta-
ble holds the record number. Original values are shown in each cell
with their sanitized replacements shown as a superscript for each
value. For example, record 1contains original value 30for the
attributeAge that is anonymized and replaced with the value 40,
which is one of three distinct values for this attribute. The super-
scripted values populate the matrix /bardblT/bardbl. Interestingly, the sanitized
record1matches the original record 2; however, since the attacker
sees only the sanitized data, it is not possible for the attacker to
know with certainty that the sanitized record matches some origi-
nal record.
4.4 Privacy Metrics
We ﬁrst calculate the probability distribution of guessing anonymity
over all the database records. Speciﬁc privacy metrics are then de-
ﬁned as a function of that distribution. In this paper, we deﬁne three
of them but the optimal metric may vary based on the task at hand.
A privacy metric measures how identiﬁable records in the san-
itized table are with respect to the original table [21, page 43].
The privacy metrics that we propose in this paper are all moti-
vated by the notion of guessing anonymity. We calculate guessing
anonymity using the algorithm shown in Algorithm 2. We begin
by creating a similarity matrix that shows the similarity between
sanitized and original records. For each record Roin the original
table, the similarity of Roto a record Rsin the sanitized table is
206Table 3: A table with original and anonymized (superscript)
data. For example, the original value of the attribute Age is 30 and the
sanitized value is 40.
Record Age Gender Race
Rec 1 3040FMWB
Rec 2 4040MMBH
Rec 3 4530MFHW
Rec 4 3040FMWH
Table 4: A similarity matrix for the table shown in Table 3.
Each cell contains the value that shows the fraction of attributes values
that are the same between original and sanitized records.❵❵❵❵❵❵❵❵❵❵❵AnonymizedOriginalRec 1 Rec 2 Rec 3 Rec 4
Rec 1 0 1 0.33 0
Rec 2 0 0.66 0.66 0
Rec 3 1 0 0 1
Rec 4 0 0.66 0.66 0
measured by the fraction of attributes whose values are the same
between RoandRs. This computation is performed in Lines 3-12
of the privacy metric algorithm. This similarity matrix, /bardblD/bardbl, shows
similarities between rows in the original matrix, /bardblQI/bardbl, and the ano-
nymized matrix,/bardblT/bardbl.
Table 4 shows an illustrative example for computing the simi-
larity matrix/bardblD/bardblfor the data in Table 3. The similarity matrix
/bardblD/bardblhas dimensions d×d, where dis the number of records in the
matrices/bardblQI/bardbland/bardblT/bardbl. Rows correspond to anonymized records
and columns correspond to the original records in /bardblD/bardbl. The values
of the attributes Age,Gender , andRace are30,F, andWfor
the original record 1, respectively, and the values for the sanitized
record1are40,M, andH, respectively. Therefore, the similarity
score is zero for the original and sanitized record 1. In contrast, the
similarity score is one for original record 2and sanitized record 1
since all attribute values of the original record 2 match the values
of their corresponding attributes for the sanitized record 1.
We use the similarity matrix /bardblD/bardblto compute how difﬁcult it is
for attackers to guess original records given sanitized records. Con-
sider an extreme case where /bardblD/bardblis a diagonal matrix, i.e., all en-
tries outside the main diagonal are zero. In this case, each record is
similar to itself only, that is, all records are unique and easily iden-
tiﬁable by attackers. The privacy level for this sanitized table is
zero. The other extreme case is when all sanitized records are sim-
ilar to all other records. In this case, it is very difﬁcult for attackers
to guess original data since all sanitized records are highly similar
to one another. Correspondingly, the privacy level for this sani-
tized table is close to one. In practice, the privacy level is between
zero and one, and our goal is to help analysts ﬁnd the right balance
between a privacy level and the utility of the sanitized database.
The distribution of guessing anonymity of the different records
in a database can be used to deﬁne a variety of privacy metrics. In
this paper, we propose three different privacy metrics that are de-
rived from the guessing anonymity distribution, but do not focus
on providing the bestmetric. We believe that different applications
and risk tolerances of users would require the use of different de-
rived metrics and leave the optimal metric determination as future
work.
In this paper, we propose three metrics based on guessing anonymity:
PM1:Mean guessing anonymity ,PM2:Fraction of records with
guessing anonymity greater than a certain value m (where we set
m to one in this paper) , and Unique Records . The computation ofthese privacy metrics PM1andPM2is shown in Lines 14-27 of
Algorithm 2, and their values are returned in Line 27.
PM1: The Mean guessing anonymity of a database is the arith-
metic mean of the individual guessing anonymities for each record
in the database. This metric gives us measurement of the overall
privacy of the sanitized database and helps us compare different
versions of sanitized databases. Naturally, a database with higher
Mean guessing anonymity would be more difﬁcult to attack and
hence have higher privacy. Suppose that there is a record ro∈To,
where Tois the table with original records. After Tois sanitized,
the table Tswith sanitized records is obtained, rs∈Ts, and we com-
pute the similarity matrix, /bardblD/bardbl, for records in these tables. Then,
for each ro∈To, we increase the counter, R, by one if we ﬁnd a
record in the table Tsthat has the similarity score in /bardblD/bardblequal to
or higher than the similarity score between roand its sanitized ver-
sion, rs, not counting the similarity score between roandrs. The
formula for the privacy metric is PM1=R
d, where dis the number
of records.
PM2: The fraction of records with guessing anonymity greater
than mis our second metric. We set mto one for the work in this
paper; such setting corresponds to measuring the fraction of orig-
inal records that have been modiﬁed at all by the anonymization
algorithm. PM2is calculated as the ratio of records that were san-
itized and that differ from their original record in at least one at-
tribute value to the total number of records. This metric is useful
for a variety of reasons. If the fraction of records that have guessing
anonymity greater than mis too low and the users are unsatisﬁed,
they have the option to increase the level of privacy or remove the
records that fall below the threshold from the sanitized database,
thus making the database more private.
UniqueRecords is measured as part of PM2and it is deﬁned as
follows. Suppose that there is a record ro∈To, where Tois the
table with original records. After Tois sanitized, the table Tswith
sanitized records is obtained. Let rs∈Tsbe the sanitized record
forro∈To.rsis a unique record if there exists r′o∈To, such that
all attributes of r′ohave the same value as the attributes of rs. A
key idea of guessing anonymity is that although some records after
applying data swapping may match some original records, the at-
tacker still cannot know with certainty which ones do and whether
sensitive information (that these records identify) was not changed.
Ideally, the percentage of unique records in the anonymized data-
base will be zero - that is what we would like to achieve. If the
number of unique records is greater than zero and the desired goal
is to have complete anonymity, the users will have to delete these
unique records before releasing the database.
5. EXPERIMENTAL EV ALUATION
In this section, we describe the results of the experimental evalu-
ation of PRIEST on three open-source Java programs and one large
commercial application that is used to manage logistics of one of
the largest supermarket chains in Spain.
5.1 Research Questions
In this paper, we make one meta-claim – our privacy framework
for managing the tradeoff between data privacy and test coverage
for DCAs is “better” than other frameworks. We deﬁne “better”
in two ways: coverage andﬂexibility . Achieving higher coverage
with PRIEST means that for a given level of privacy, we can pro-
vide higher test coverage. We measure it using the area under the
privacy-coverage curve. Flexibility means that we can give more
choices to the business analyst to make informed decisions. If
more data points in that curve can be created using PRIEST for
207a given range of privacy, we have a better framework for managing
the tradeoff.
PRIEST does not compete with other anonymization techniques,
such as k-anonymity since the latter is a metric, not a framework.
Our claim is that our framework (that includes guessing anonymity)
is a better framework to achieve coverage and ﬂexibility. We seek
to answer the following research questions.
RQ1 How much test coverage does PRIEST help achieve at given
levels of privacy for subject applications?
RQ2 How effective is PRIEST in achieving different levels of data
privacy while preserving original data?
RQ3 How effective is PRIEST in releasing different versions of
anonymized databases for the same level of privacy and test
coverage without enabling the attacker to link sensitive data?
With RQ1, we address our claim that we designed and imple-
mented a technique using program analysis for determining how
values of database attributes affect test coverage of DCAs that use
this data. Our goal is to show that with PRIEST, the privacy metric
is linked directly to test coverage and vice versa; in other words,
guaranteeing a certain level of test coverage should allow stake-
holders to calculate bounds of the privacy level.
With RQ2, we address our claim that using different levels of
privacy enables business analysts to make trade-off decisions about
privacy and utility.
With RQ3, we address our claim that PRIEST enables stakehold-
ers to repeatedly release anonymized data in such a way that both
privacy and utility are guaranteed at certain levels. Suppose that a
technique of data suppression anonymization is applied to protect
data and release the anonymized data with the DCA to testers. After
some time, programmers produce the next release of this DCA that
uses different attributes differently in its database. Suppose that the
DCA does not use the attribute Nationality any more, and it
uses the attribute Race instead. At this point, the database should
be reanonymized, since the original values of the attribute Race
should be left intact to preserve test coverage. However, in the pre-
vious anonymization, the values of the attribute Nationality
are left unprotected. It means that the testers (the attackers) know
original values of the attribute Nationality and now the testers
will know the original values of the attribute Race . Together, these
attributes enable the attackers to infer sensitive information from
the released database.
5.2 Subject Programs
We evaluate PRIEST with three open-source and one large com-
mercial Java programs that belong to different domains. Our selec-
tion of subject programs is inﬂuenced by several factors: sizes of
the databases, size of the source code, presence of unit, system, and
integration tests, and the presence of embedded SQL queries that
these programs use.
We selected four subject programs that come with test cases.
N2A is a program for handling logistics of one of the largest super-
market chains in Spain. N2A has a total of 77,828 LOC. RiskIt is
an insurance quote program4.DurboDax enables customer sup-
port centers to manage customer data5. Finally,UnixUsage is
a program for obtaining statistics on how users interact with Unix
systems using their commands6.
4https://riskitinsurance.svn.sourceforge.net as of March 10, 2011.
5http://se547-durbodax.svn.sourceforge.net as of March 10, 2011.
6http://sourceforge.net/projects/se549unixusage as of March 10,
2011.DCA App Test DB Tbl Att IC ETC
[kLOC] [MB] % %
N2A 77.8 19.3 20 506 2514 53 50
DurboDax 2.8 2.0 49 27 114 77 59
UnixUsage 2.8 .9 21 8 31 61 53
RiskIt 4.3 2.6 628 13 57 62 48
Table 5: Characteristics of the subject DCAs. App = applica-
tion code, Test = test cases, DB = database, Tbl = tables, Att =
attributes in all tables, IC = initial test coverage with the orig-
inal database, ETC = estimated worst test coverage with sani-
tized data using the approach described in Section 3.
Table 5 contains characteristics of the subject programs, their
databases, and test cases. The ﬁrst column shows the names of the
subject programs, followed by the number of lines of code, LOC
for the program code and accompanying test cases. The source
code of the project ranges from 2.8 to 77.8 kLOC. The test cases
range from 0.9 to 19.3 kLOC. Other columns show the size of the
database, number of tables and attributes in the database, test cov-
erage (statement coverage) that is obtained with the original data-
base, and the estimated worst test coverage (statement coverage)
with sanitized data using our approach described in Section 3.
5.3 Methodology
To evaluate PRIEST, we carry out experiments to explore its
effectiveness in enabling users to determine how to balance test
coverage while achieving different levels of data privacy (RQ1 and
RQ2), and to show that it is possible to apply PRIEST to get pri-
vacy guarantees while releasing different sanitized versions of the
database for the same privacy levels thereby supporting software
evolution (RQ3).
5.3.1 Variables
The main independent variable is the value of p. Two main re-
sponse variables are the time that it takes to anonymize data in the
database to achieve the desired level of kto answer RQ2 and the
test coverage in percentage of program statements to answer RQ1.
5.3.2 The Structure of the Experiments
For the experiments, we select as QIs all attributes whose val-
ues affect execution paths in the corresponding DCAs. It is physi-
cally not possible to carry out an experiment using all subsets of the
powerset of attributes as QIs where we measure test coverage for
subject DCAs while achieving anonymity, since it would require
us to consider the powerset of all attributes. Given that databases
of the subject DCAs contain between 31 and 2,514 attributes, it is
challenging to select a subset of them as QIs to enable achieve the
higher possible level of test coverage.
Our goal is to run experiments for different values of the inde-
pendent variable pand report the effect of varying the values on p
on dependent variables. To address RQ3, we anonymize databases
for the subject DCAs for a given level of p, and we report and an-
alyze privacy metrics between different sanitized versions of the
same original database.
5.4 Threats to Validity
A threat to the validity of this experimental evaluation is that
our subject programs are of small to moderate size because it is
difﬁcult to ﬁnd a large number of open-source programs that use
nontrivial databases. Large DCAs that have millions of lines of
code and use databases whose sizes are measured in thousands of
208Figure 3: Experimental results for the subject DCAs. All graphs show the dependence on the probability of replacing original data value that
is assigned to the horizontal axis. The vertical axis designates dependent variables that are given in the captions to these ﬁgures.
tables and attributes may have different characteristics compared
to our small to medium size subject programs. Increasing the size
of applications to millions of lines of code may lead to a nonlinear
increase in the analysis time and space demand for PRIEST. Future
work could focus on making PRIEST more scalable.
5.5 Results
The results of the experiments conducted to address RQ1 and
RQ2 are shown in Figure 3 and Table 6. Dependency of test cov-
erage on the probability pof replacing original values is shown
in Figure 3 (a). The maximum reduction in test coverage is close
to 26% from the initial test coverage with p=1 for application
DurboDax. In our previous work [23], we showed that the maxi-
mum reduction in test coverage for the same applications reaches
80% from the initial test coverage when using a popular algorithm
Dataﬂy that is based on data suppression and generalization tech-
niques [36]. We observed the biggest drop in test coverage with
Dataﬂy when k=7, while much smaller maximum drop is ob-
served with PRIEST for p=1, which is the maximum value for
the anonymization parameter.
The least drop in test coverage is observed for the application
N2A, while the biggest drop is attributed to the application Dur-
boDax. Our explanation is that N2A is least sensitive to values
of the database attributes since it uses these values to compute re-
sults rather than in path conditions. We explain the sharper drop for
DurboDax as a result of branch conditions that use conjunctions of
multiple QIs, thereby making the application sensitive to joint dis-
tributions of values of different attributes that may be destroyed by
anonymization. We also observe that the shape of the test cover-
age curves show gradual decline in test coverage rather than abrupt
changes. The former makes it easier for stakeholders to balance
privacy and coverage on a continuous scale.
Dependencies of privacy metrics PM1andPM2on the proba-
bility pof replacing original values are shown in Figure 3 (b)and
Figure 3(c). Combined with the test coverage curve shown in Fig-
ure 3(a), these dependencies enable business analysts to select a
level of protection that also ensures a certain level of test cover-
age. While values of PM1are monotonically increasing for N2A,
RiskIt, and DurboDax with the increase in p, the values of PM1
for UnixUsage slightly drop when the values of pare increasing
from 0.2 to 0.6. This result can be explained as an effect of the
variations of random value replacement that may result in small
changes in the numbers of similar records. The values for PM1
are relatively small for N2A since many of the selected QIs were
involved in some primary or unique key. As a result, the sanitizedTable 7: Results for privacy metrics to compare two sanitized
databases for p=0.6for subject DCAs
Subject PM1PM2 Total Unique UR,%
DCA Records Records
N2A 0.20 0.78 586801 265400 38.7
DurboDax 0.60 0.77 88450 20687 23.4
UnixUsage 0.61 0.60 250570 100422 40.1
RiskIt 0.47 0.76 1270792 512051 40.3
records contain distinct values for these QIs and fewer records (in
the original database) are likely to be similar to a sanitized record.
Opposite to the dependencies of privacy metrics PM1andPM2,
the graph of the dependency of the percentage of unique records,
URshows corresponding decline in Figure 3 (d). It means that the
percentage of unique records between the original and sanitized
databases is monotonically declining as the values of the probabil-
itypare increasing thereby making it more difﬁcult for attackers to
use these unique records to guess the original data. While values of
URare monotonically decreasing for UnixUsage, RiskIt, and Dur-
boDax with the increase in p, the values of URfor N2A slightly
increase when the values of pare increasing from 0 .2 to 0.6. This
result can be explained as an effect of the variations of random
value replacement that may result in small changes in the numbers
of unique records.
To address RQ3, we generated two sanitized databases for p=
0.6 and computed the privacy metrics PM1,PM2, and number of
unique records between the two databases. These metrics quantify
the difﬁculty of an attacker to relate records from the two sanitized
databases. Since the same database is anonymized independently
and the probabilities of replacing cell values are independent from
one another, then resulting databases cannot be deterministically
correlated since they are not used to produce each other. Thus, the
attacker has to guess original data independently, meaning that the
lowest guessing anonymity score can be used to quantify the difﬁ-
culty of an attacker to guess records from both sanitized databases.
The values for PM1 are slightly lower to the values of PM1 for
p=0.6 in Table 6, while the values of PM2 are slightly higher to
the values of PM1 for p=0.6 in Table 6. This phenomenon is
expected since the variation between the records of the two ano-
nymized databases is expected to be more as compared to varia-
tion between the records of original and an anonymized database.
The number of unique records is similar to the number of unique
records for p=0.6 in Table 6. It is also important to note that the
overall privacy of multiple sanitized databases is only as good as
the privacy of the least private version. If the ﬁrst released ver-
sion has PM2= 3 and the second version has PM2= 5, PM2for
209Subject QIs Total Initial Worst Max Dependent The Probability of Value Replacement, p
Records Cov Exp Cov Err Cov Variable 0.01 0.1 0.2 0.4 0.6 0.8 1
N2A 61 586801 53% 50% 2.0%PM1 0.17 0.18 0.19 0.19 0.20 0.21 0.21
PM2 0.70 0.72 0.74 0.77 0.80 0.81 0.83
TC,% 49.00 49.00 49.00 50.00 51.00 49.00 49.00
UR,% 41.80 40.30 39.00 48.60 36.10 36.00 31.30
DurboDax 20 88450 77% 59% 3.6%PM1 0.05 0.26 0.40 0.61 0.77 0.89 1.00
PM2 0.01 0.10 0.20 0.40 0.59 0.80 1.00
TC,% 72.00 67.00 67.00 65.00 65.00 59.00 57.00
UR,% 95.20 74.40 60.00 38.90 23.30 10.60 0.00
UnixUsage 16 250570 61% 53% 5.7%PM1 0.05 0.27 0.80 0.82 0.76 0.92 1.00
PM2 0.08 0.16 0.25 0.44 0.63 0.81 1.00
TC,% 60.00 59.00 59.00 59.00 59.00 58.00 56.00
UR,% 98.30 90.00 80.00 60.20 39.90 20.00 0.01
RiskIt 28 1270792 62% 48% 8.3%PM1 0.07 0.37 0.66 0.75 0.81 0.95 1.00
PM2 0.07 0.15 0.22 0.34 0.45 0.55 0.64
TC,% 57.00 56.00 55.00 54.00 53.00 52.00 53.00
UR,% 96.90 81.70 69.90 51.60 36.70 23.80 12.30
Table 6: Results of experiments with subject DCAs for different values of the independent variable pthat is shown in the last column
header that spans seven subcolumns. The second column, QI, shows the number of QI whose values affect control-ﬂow decisions in DCAs, the next
two columns show the initial test coverage (statement coverage) with the original database and the estimated worst test coverage with sanitized data
using the approach described in Section 3. The next column shows the error in the estimated test coverage that varies from 2.0% to 8.3%. Finally,
the next column lists four dependent variables, privacy metrics PM 1and PM 2, test coverage, TC, and the percentage of unique records, UR. Finally,
the last seven subcolumns show values of these dependent variables for different values of p.
the two databases combined will be 3. In general, for two released
databases DiandDj, the overall PM2= Min( PM2(Di),PM2(Dj)).
Result summary. These results strongly suggest that PRIEST
helps achieve higher test coverage for given levels of privacy for
subject applications when compared with Dataﬂy that is based on
data suppression and generalization techniques, thereby address-
ing RQ1. PRIEST can also achieve different levels of data privacy
while preserving original data as it is seen from Table 6, thereby
addressing RQ2. Finally, the results shown in Table 7 strongly sug-
gest that PRIEST is effective in releasing different versions of ano-
nymized databases for the same level of privacy and test coverage,
thereby addressing RQ3.
6. RELATED WORK
Our work is related to regression testing [39] since PRIEST is
used to assess the impact of data anonymization on testing. Nu-
merous techniques have been proposed to automate regression test-
ing. These techniques usually rely on information obtained from
the modiﬁcations made to the source code. These techniques are
not directly applicable to preserving test coverage while achieving
data anonymity for test outsourcing, since regression information
is derived from changes made to the source code and not to how
this code uses databases.
Closely related to PRIEST is kb−anonymity model that enables
stakeholders to release private data for testing and debugging by
combining the k−anonymity with the concept of program behavior
preservation [10]. Unlike PRIEST, kb−anonymity replaces some
information in the original data to ensure privacy preservation so
that the replaced data can be released to third-party developers.
PRIEST and kb−anonymity are complementary in using different
privacy mechanisms to preserve original data thereby improving its
testing utility.
Recently proposed is an anonymization technique for protect-
ing private information in bug reports that are delivered to ven-
dors when programs crash on computers of customers [11] and the
follow-up work on this technique by Clause and Orso [14]. Thistechnique provides software vendors with new input values that sat-
isfy the conditions required to make the software follow the same
execution path until it fails, but are otherwise unrelated with the
original inputs. This technique uses symbolic execution to create
new inputs that allow vendors to reproduce the bug while revealing
less private information than existing techniques. The technique re-
quires test cases, which are not present in our situation. In contrast,
PRIEST does not require any test case.
There has been a lot of recent work to achieve general purpose
(task-independent) data anonymization. We choose the guessing
anonymity approach in this paper because guessing anonymity can
be used to provide privacy guarantees for data swapping algorithms
and can also provide an optimal noise parameter when implement-
ing data swapping algorithms for anonymization. In contrast, ap-
proaches that aim to achieve k-anonymity do not allow the user
to explicitly control how much each record is altered. Empiri-
cal results reported by Rachlin et al. [31] show that Guessing
anonymity outperforms DataFly, a well-known k-Anonymity al-
gorithm on speciﬁc data mining tasks, namely classiﬁcation and
regression, while at the same time providing a higher degree of
control over how much the data is distorted.
Recent work on privacy introduced a similar deﬁnition of pri-
vacy for noise perturbation methods, known as k-randomization
[2]. This work deﬁnes a record as k-randomized if the number
of records that are a more likely match to the original is at least
k. Although this notion is similar to the deﬁnition of guessing
anonymity, the deﬁnition differs by not providing a lower limit on
the number of records that provide a more likely match, and by
explicitly establishing a connection between privacy and guessing
functions.
7. CONCLUSION
We offer a novel and effective approach called PRIEST that helps
organizations to remove an obstacle to effective DCA test outsourc-
ing. With PRIEST, DCAs can be released to external testing orga-
nizations without disclosing sensitive information while retaining
210testing efﬁcacy. We built a tool and applied it to nontrivial DCAs.
The results show that PRIEST is effective in enabling users to de-
termine how to balance test coverage while achieving different lev-
els of data privacy, and that with PRIEST, users can release differ-
ent sanitized versions of the database for the same privacy levels,
thereby supporting software evolution.
Acknowledgments
We thank anonymous reviewers whose comments helped us to im-
prove the quality of this paper. This work is supported in part by
NSF CCF-0916139, CCF-1017633, CCF-0725190, CCF-0845272,
CCF-0915400, CNS-0958235, an NCSU CACC grant, and ARO
grant W911NF-08-1-0443, and Accenture. Any opinions, ﬁndings,
and conclusions expressed herein are the authors’ and do not nec-
essarily reﬂect those of the sponsors.
8. REFERENCES
[1] C. C. Aggarwal. On k-anonymity and the curse of
dimensionality. In VLDB , pages 901–909, 2005.
[2] C. C. Aggarwal. On randomization, public information and
the curse of dimensionality. In ICDE , pages 136–145, 2007.
[3] C. C. Aggarwal and P. S. Yu. On static and dynamic methods
for condensation-based privacy-preserving data mining.
ACM Trans. Database Syst. , 33:2:1–2:39, March 2008.
[4] C. C. Aggarwal and P. S. Yu. Privacy-Preserving Data
Mining: Models and Algorithms . Springer, 2008.
[5] J. T. Alexander, M. Davern, and B. Stevenson. Inaccurate age
and sex data in the census pums ﬁles: Evidence and
implications. Working Paper 15703, National Bureau of
Economic Research, January 2010.
[6] S. Ambler. Robust persistence layers. Softw. Dev. ,
6(2):73–75, 1998.
[7] W. Aspray, F. Mayades, and M. Vardi. Globalization and
Offshoring of Software . ACM, 2006.
[8] C. Bialik. Census bureau obscured personal data – too well,
some say. The Wall Street Journal , Feb. 2010.
[9] J. Brickell and V . Shmatikov. The cost of privacy:
destruction of data-mining utility in anonymized data
publishing. In KDD , pages 70–78, 2008.
[10] A. Budi, D. Lo, L. Jiang, and Lucia. b-anonymity: a model
for anonymized behaviour-preserving test and debugging
data. In PLDI , pages 447–457, 2011.
[11] M. Castro, M. Costa, and J.-P. Martin. Better bug reporting
with better privacy. In ASPLOS , pages 319–328, 2008.
[12] L. A. Clarke. A system to generate test data and symbolically
execute programs. IEEE Trans. Softw. Eng. , 2(3):215–222,
1976.
[13] J. Clause and A. Orso. Penumbra: Automatically identifying
failure-relevant inputs using dynamic tainting. In ISSTA ,
pages 249–260, 2009.
[14] J. A. Clause and A. Orso. Camouﬂage: automated
anonymization of ﬁeld data. In ICSE , pages 21–30, 2011.
[15] G. Cormode and D. Srivastava. Anonymized data:
generation, models, usage. In SIGMOD , pages 1015–1018,
2009.
[16] Datamonitor. Application testing services: global market
forecast model. Datamonitor Research Store , Aug. 2007.
[17] R. A. DeMillo and A. J. Offutt. Constraint-based automatic
test data generation. IEEE Trans. Softw. Eng. ,
17(9):900–910, 1991.[18] I. Dinur and K. Nissim. Revealing information while
preserving privacy. In PODS , pages 202–210, 2003.
[19] J. Domingo-Ferrer and D. Rebollo-Monedero. Measuring
risk and utility of anonymized data using information theory.
InEDBT/ICDT , pages 126–130, 2009.
[20] A. Endres and D. Rombach. A Handbook of Software and
Systems Engineering . Pearson Addison-Wesley, 2003.
[21] B. C. M. Fung, K. Wang, A. W.-C. Fu, and P. S. Yu.
Introduction to Privacy-Preserving Data Publishing:
Concepts and Techniques . Data Mining and Knowledge
Discovery. Chapman & Hall/CRC, August 2010.
[22] P. Godefroid. Compositional dynamic test generation. In
POPL , pages 47–54, 2007.
[23] M. Grechanik, C. Csallner, C. Fu, and Q. Xie. Is data privacy
always good for software testing? In ISSRE , pages 368–377,
2010.
[24] M. Grechanik, C. McMillan, L. DeFerrari, M. Comi,
S. Crespi, D. Poshyvanyk, C. Fu, Q. Xie, and C. Ghezzi. An
empirical investigation into a large-scale Java open source
code repository. In ESEM , pages 11:1–11:10, 2010.
[25] C. Jones. Software Engineering Best Practices .
McGraw-Hill, Inc., New York, NY , USA, 1 edition, 2010.
[26] G. M. Kapfhammer and M. L. Soffa. A family of test
adequacy criteria for database-driven applications. In
ESEC/FSE , pages 98–107, 2003.
[27] W. Landi. Undecidability of static analysis. ACM Lett.
Program. Lang. Syst. , 1(4):323–337, 1992.
[28] T. Li and N. Li. On the tradeoff between privacy and utility
in data publishing. In KDD , pages 517–526, 2009.
[29] R. Majumdar and K. Sen. Hybrid concolic testing. In ICSE ,
pages 416–426, 2007.
[30] T. E. Murphy. Managing test data for maximum productivity.
http://www.gartner.com/DisplayDocument
?doc_cd=163662&ref=g_economy_2reduce , Dec. 2008.
[31] Y . Rachlin, K. Probst, and R. Ghani. Maximizing privacy
under data distortion constraints in noise perturbation
methods. In PinKDD , pages 92–110, 2008.
[32] S. P. Reiss. Practical data-swapping: the ﬁrst steps. ACM
Trans. Database Syst. , 9:20–37, March 1984.
[33] S. P. Reiss, M. J. Post, and T. Dalenius. Non-reversible
privacy transformations. In PODS , pages 139–146, 1982.
[34] P. Samarati. Protecting respondents’ identities in microdata
release. IEEE Trans. Knowl. Data Eng. , 13(6):1010–1027,
2001.
[35] I. Shield. International data privacy laws.
http://www.informationshield.com/intprivacylaws.html ,
2010.
[36] L. Sweeney. k-anonymity: A model for protecting privacy.
International Journal of Uncertainty, Fuzziness and
Knowledge-Based Systems , 10(5):557–570, 2002.
[37] K. Taneja, Y . Zhang, and T. Xie. MODA: Automated test
generation for database applications via mock objects. In
ASE, pages 289–292, 2010.
[38] B. G. Thompson. H.R.6423: Homeland Security Cyber and
Physical Infrastructure Protection Act of 2010 . U.S.House,
111th Congress, Nov. 2010.
[39] S. Yoo and M. Harman. Regression testing minimisation,
selection and prioritisation: A survey. STVR ,to appear ,
2011.
211