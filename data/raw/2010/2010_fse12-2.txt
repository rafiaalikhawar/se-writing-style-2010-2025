Improving Software Developers‚Äô Fluency by
Recommending Development Environment Commands
Emerson Murphy-Hill
Department of Computer Science
North Carolina State University
Raleigh, North Carolina
emerson@csc.ncsu.eduRahul Jiresal and Gail C. Murphy
Department of Computer Science
University of British Columbia
Vancouver, Canada
jiresal,murphy@cs.ubc.ca
ABSTRACT
Software developers interact with the development environ-
ments they use by issuing commands that execute various
programming tools, from source code formatters to build
tools. However, developers often only use a small subset
of the commands oered by modern development environ-
ments, reducing their overall development uency. In this
paper, we use several existing command recommender al-
gorithms to suggest new commands to developers based on
their existing command usage history, and also introduce
several new algorithms. By running these algorithms on
data submitted by several thousand Eclipse users, we de-
scribe two studies that explore the feasibility of automati-
cally recommending commands to software developers. The
results suggest that, while recommendation is more di-
cult in development environments than in other domains,
it is still feasible to automatically recommend commands
to developers based on their usage history, and that using
patterns of past discovery is a useful way to do so.
Categories and Subject Descriptors
D.2.6 [ Software Engineering ]: Coding Tools and Tech-
niques
Keywords
discovery, software developers, commands, IDEs
1. INTRODUCTION
Software tools can help developers meet users' increasing
demands for better software. For instance, research suggests
that documentation tools can help developers use unfamiliar
application programming interfaces to create code several
times faster [19], code smell tools can help developers make
more informed judgments about design [15], and debugging
tools can help developers x bugs more successfully [7].
To use these tools, developers issue commands to their
development environment, whether that environment is an
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proÔ¨Åt or commercial advantage and that copies
bear this notice and the full citation on the Ô¨Årst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciÔ¨Åc
permission and/or a fee.
SIGSOFT‚Äô12/FSE-20, November 11‚Äì16, 2012, Cary, North Carolina, USA.
Copyright 2012 ACM 978-1-4503-1614-9/12/11 ...$10.00.editor like emacs [29] or an integrated development environ-
ment like Eclipse [28]. Commands may range from simple
text editing functions, such as copying text, to sophisticated
programming actions, such as generating code or synchro-
nizing with a repository. In 2009, logs collected from over 1
million users interacting with the Eclipse development envi-
ronment indicate the use of over 1100 distinct commands.
Although the community of all software developers use
a wide range of commands, any given developer tends to
use only a small number. After reaching their own personal
xpoint, developers tend to face diculties discovering and
becoming aware of other helpful commands. Consider the
open resource command in Eclipse that allows a developer to
simply type the rst few characters of a le, press return and
the desired le is opened almost instantly. Several bloggers
have praised open resource , including \10 Eclipse Naviga-
tion Shortcuts Every Java Programmer Should Know" [23].
Other bloggers have noted it is the \biggest time-saver I've
stumbled upon" [24], and a \command I use religiously" [25].
However, despite the acclaim given to this command, based
on Eclipse Usage Data Collector [26] data for May 2009, of
the more than 120,000 people who used Eclipse, more than
88% do not use open resource .
This command discovery problem can be addressed in
many ways. One way is through searchable documentation,
yet the developer must know of a command's existence in
order to search for it. Another way is to improve the visi-
bility of a command, such as through a special toolbar, yet
this comes at the expense of the discoverability of other com-
mands that are not on the toolbar. Another way is through
tip-of-the-day messages, which recommend a random com-
mand at regular intervals, yet the suggested command may
have no bearing on what a developer actually needs.
In this paper, we address the command discovery prob-
lem by making custom command recommendations to a de-
veloper by analyzing the developer's past history of com-
mand usage and comparing it to a larger community's use
of development commands. This approach builds on a sim-
ilar approach taken by Matejka and colleagues [9, 12] to
recommend commands useful in a computer-aided drawing
application, AutoCAD. Since Eclipse includes about 1100
distinct commands while AutoCAD contains about 750 [9],
recommending commands in development environments ap-
pears to be more challenging; indeed, our results in Sec-
tion 5 suggest that existing algorithms do not perform as
well with Eclipse data as they do with AutoCAD data. We
extend Matejka and colleagues' work by introducing four
novel algorithms for recommending relevant commands tosoftware users. We evaluated six algorithms in two evalu-
ations: one automated evaluation and the second with live
software developers. In the automated evaluation, we found
that our best novel algorithm made recommendations with
about 30% accuracy, which is about 20% higher than recom-
mendations produced by the best previous algorithm. Al-
though 30% accuracy may seem low, we believe adopting a
small number of commands at a continual rate is an appro-
priate way to learn new commands and that delivery mech-
anisms are available to make it possible for developers to
learn those recommendations that are relevant. We discuss
these points in further detail in Section 8.
We rst review previous work on feature awareness and
command recommendations (Section 2). We then present
a proof-of-concept study that explores discovery trends in
Eclipse to help explain the motivation behind our novel al-
gorithms (Section 3). We then describe the existing algo-
rithms we applied and the novel algorithms we developed
(Section 4) before presenting the results of an automated ex-
ploration of the eectiveness of the algorithms (Section 5).
We also performed a study with developers to gain human
insight into the value of the recommendations (Section 6).
We discuss various aspects of our approach (Section 7) and
future work (Section 8) before summarizing the paper (Sec-
tion 9).
2. RELATED WORK
Several pieces of previous research have attempted to in-
vestigate the problem of command awareness and ways to
recommend commands to users of complex software. We di-
vide the latter work into three categories: human-to-human,
ineciency-based, and history-based recommendation.
2.1 Software Feature Awareness
The challenge of making software learnable has been a
topic of considerable study among the human-computer in-
teraction community. One aspect of learnability is aware-
ness of what functionality is available in a user interface.
When Grossman and colleagues conducted a study of learn-
ability issues for users of AutoCAD, a computer-aided draft-
ing application, they found that a \typical problem was that
users were not aware of a specic tool or operation which
was available for use" [6]. Although less well-documented,
the awareness problem has been described in integrated soft-
ware development environments as well, such as Campbell
and Miller's discussion of discoverability as a barrier to refac-
toring tool use [2].
2.2 Human-to-Human Recommendation
Perhaps one of the most natural, most overlooked, and
most eective ways to learn is from a peer. In the general ed-
ucational literature, situated learning , where a person learns
something new from another person during routine activi-
ties, has been suggested as a mechanism by which knowledge
may spread \exceedingly rapidly and eectively" [8, p. 93].
Learning software from peers appears to be eective as
well. Suppose Hilaria needs to submit a bug report, but
she does not know the command to use to attach a patch
to the bug report. Hilaria asks a colleague, who shows Hi-
laria the menu item and hotkey for attaching a patch. This
scenario is known as Over the Shoulder Learning , or OTSL
for short, where a software user learns from another user at
the same computer [20]. In the context of software develop-ment, Cockburn and Williams as well as M uller and Tichy
have suggested that software developers learn eectively in
this way during pair-programming [3, 13]. For example, Hi-
laria might be programming with a peer, notice that the
peer is refactoring without using the refactoring commands,
and may then recommend to the peer that they try a refac-
toring command. Our research has suggested that although
discovering tools from peers in such a way is eective, it is
relatively rare [16]. Moreover, a software developer cannot
discover a new command using OTSL nor pair programming
when she is working without other developers or she is the
most uent command-user in her peer group. The approach
that we propose in this paper, using a recommender system,
does not depend on a developer's peer group.
2.3 InefÔ¨Åciency-based Recommendation
One approach that does not rely on peers is a recom-
mender system that makes command recommendations based
on inecient behavior. The Spyglass system, for example,
works in a manner quite similar to the way that Hilaria
recommended a refactoring command during pair program-
ming [21, 22]. For example, when Hilaria is using Eclipse,
Spyglass might notice that she often uses the nd references
command repeatedly on methods. Spyglass would then rec-
ommend that Hilaria instead use the show call hierarchy com-
mand, reasoning that the task that Hilaria is performing is
walking up a call tree, a task that can be accomplished in
fewer steps using the recommended command.
A problem with this approach is that the recommender
system must be pre-programmed to recognize inecient be-
havior as a precondition for each and every command that
it want to recommends. For example, how such a systems
identies inecient behavior prior to recommending the nd
references command is entirely dierent from how it recog-
nizes inecient behavior prior to recommending the open
type hierarchy command. On the other hand, a history-
based recommender system like the one in this paper, can
recommend one command as easily as the next, without any
special command-specic programming.
2.4 History-Based Recommendation
Other researchers have used history-based recommenders,
the approach taken in this paper, for software systems in
other domains. We briey outline previous research here,
and more extensively discuss the advantages and disadvan-
tages of the algorithms in Section 4.
The Toolbox system created by Maltzahn and Vollmar
creates a database of commands invoked by Unix users in
order to identify commands that users have discovered re-
cently [11]. Using the information in the database, Toolbox
sends an email interview to a user who discovers a new com-
mand. In the email, Toolbox solicits information about how
the new command works. The response is then distributed
via an online newsletter, so that other members of the com-
munity can discover the command as well. This system relies
on users completing the interview, whereas the algorithms
described in this paper do not require user interaction for a
recommendation to be made.
Linton and colleagues created OWL, a system that makes
command recommendations to Microsoft Word users [10].
OWL observes what commands the community is using, and
makes recommendations to individuals to change their com-
mand usage behavior. For example, if OWL observes thatcommit
update
compareWithRemote
showHistorycompareWithTag
replace compareWithRevisionmerge
syncFigure 1: A discovery graph for Eclipse CVS com-
mands.
the community of Word users use the ndcommand more
than any other command, yet you do not use it at all, OWL
would recommend that you use ndas well.
More recently, Matejka and colleagues proposed using a
history-based recommender called CommunityCommands to
recommend commands to users of AutoCAD, a computer-
aided drafting system [12]. Matejka and colleagues used two
collaborative-ltering algorithms to compare a user's com-
mand history with the history of many other users of Au-
toCAD, and used the comparison to make a recommenda-
tion. The researchers showed that CommunityCommands'
best algorithm could predict the next command that an
AutoCAD user would discover next about 27% more accu-
rately than Linton's algorithm. The researchers also showed
that their algorithms could make command recommenda-
tions that AutoCAD users preferred over the recommenda-
tions made by Linton.
In this paper, we build on this work in two ways. First,
we extend their recommender systems into a new domain,
software development environments. Second, we explicitly
build upon each of their algorithms to create several new
algorithms, which we discuss in Section 4.
3. DISCOVERY TRENDS IN ECLIPSE
Several of the algorithms we investigate in this paper model
how users discover commands as a means to predict new,
potentially relevant commands. Why might this be a fruit-
ful approach? To answer this question, in this section we
present a small proof-of-concept study of how Eclipse users
discover two sets of commands.
To perform this study, we use data from the Eclipse Usage
Data Collector (UDC). This data set contains information
submitted voluntarily by users of Eclipse, information in-
cluding events that capture which Eclipse plugins are loaded,
which user interface views and perspectives are shown, and
which commands users invoke. Events in this data set are
tagged by timestamps and anonymous user identiers. Weexamined data from 2009, which includes a total of about
1.3 billion events from more than a million users.
We ltered this UDC data in two ways. First, we only
included command events, such as invocations of the open
resource command. Second, we included events only from
users who reported data every month in 2009 because we
wanted to improve our chances of exploring real, long-term
discovery trends. This reduced the data set to about 22
million command uses from about 4300 Eclipse users.
To examine discovery trends, we chose to focus our study
on two sets of commands in Eclipse and make two specic
hypotheses:
CVS Commands. CVS commands enable develop-
ers to interact with their CVS source revision reposi-
tories. Commands include update and commit , which
enable developers to get the latest version from and
commit to their repositories, respectively. We chose
to study CVS commands because we view one com-
mand in particular | synchronize | as an advanced
command that users would be more likely to discover
after discovering other CVS commands rst. We hy-
pothesized this discovery ordering because synchronize
is not strictly a necessary command to use CVS, but
it may improve productivity.
Java Editing Commands. Java editing commands
enable software developers to more quickly edit their
code. Examples of such commands include tools to
toggle comments and perform refactorings. We chose
to study editing commands because our previous ex-
perience with refactoring tools [17] suggests that one
command in particular | rename | invokes a very ba-
sic refactoring tool, and thus we hypothesize that it is
likely discovered before other refactoring commands.
We tested these hypotheses by building discovery graphs
for both of these sets of commands. Figures 1 and 2 show
these graphs, as produced by GraphViz [5]. In each graph,
each node represents a command that users can discover. We
dene a command discovery as occurring only when two con-
ditions are met. First, a command discovery can only occur
after the user's base command usage window, which repre-
sents a period of time where we assume that all commands
used in this period are tools that the developers know, and
any commands used after this period are commands that
the developer has discovered. In this paper, we x the base
command usage window to the rst 40 sessions for which
we have data. We divide a user's command history into ses-
sions by grouping two commands into the same session if
they were invoked within 60 minutes of one another. The
motivation for using sessions is to remove spurious discovery
preconditions { for instance, if two commands are discov-
ered around the same time, we assume that learning about
one command was not a precondition for learning about the
other. Based on a manual exploration of the UDC data,
we judged that the 60 minute tolerance and the 40 session
windows were reasonable values to group working periods
together, although we did not perform full sensitivity anal-
ysis. Second, a command discovery occurs the rst time a
user uses a new command only if she uses that command a
second time at some point later in the data set. We add this
second condition to attempt to rule out false-positive dis-
coveries where the user inadvertently used a command but
then never used it again.create_getter_setter
rename_element
toggle_comment
move_elementopen_type_hierarchy
extract_methodopen_hierarchyformat correction_assist_proposals search_references_in_project open_editor override_methods
extract_constantsearch_references_in_workspace organize_importsFigure 2: A discovery graph for Eclipse Java editing commands.
In Figures 1 and 2, an edge from node ato node brep-
resents users who discovered a command arst and then
discovered bin a later session. The thickness of a line rep-
resents the number of users who discovered aand then b.
Thus, thicker lines means that more people discovered athen
b. The lightness of the line indicates the number of users
who rst discovered b, then a. Thus, perfectly black lines
indicate that everyone who discovered aand bdiscovered
arst, whereas very light lines indicate that just as many
people discovered arst as discovered brst. To reduce the
visual complexity of the graphs, we eliminated edges which
represent the discovery of only a few people; for Figure 1,
we excluded edges with fewer than 7 people (per direction),
and in Figure 2, fewer than 14.
For example, in Figure 2, notice a fairly dark line from
rename element toextract method . The line thickness in-
dicates that 18 people rst discovered the rename element
command, then discovered the extract method command.
The line lightness indicates that 6 people rst discovered ex-
tract method , then discovered rename element . As another
example, Figure 1 shows a light, thin line from showHistory
tocompareWithRemote . For this line, 8 people discovered
theshowHistory command rst, and 6 people discovered the
compareWithRemote command rst.
In the whole data set, a large number of people use CVS
(n=41,734) and Java editing commands (n=351,657), but in
the ltered data set, most trends in the graphs are supported
by typically fewer than 20 people. This is partly because
many people only reported data for a short period of time,
and partly because some people used the same tools both
before and after the base command usage window.
By inspecting the two discovery graphs, we can evalu-
ate our hypotheses with regard to the rename refactoring
and CVS synchronize commands. Looking at Figure 1, sync
does not actually seem to be discovered after other CVS
commands, but is actually a predecessor of update and com-
pareWithRemote . Looking at Figure 2, we can see that re-
name element appears as a predecessor to the three other
refactoring commands shown ( move element ,extract method ,
and extract constant ).Overall, the graphs reveal that there are discovery trends
common between dierence Eclipse users, although we did
not consistently predict those trends. Certain commands
do tend to be discovered earlier than other commands, sug-
gesting that usage data may be a valuable predictor of the
order of command recommendations, and that developers
with dierent levels of command maturity may benet from
being recommended dierent commands. For example, a de-
veloper who is unfamiliar with refactoring may nd it more
useful to rst be recommended the rename command before
theextract method command.
4. DESCRIPTION OF THE ALGORITHMS
In this section, we describe two existing algorithms (as
indicated by the sub-section headings) and four new algo-
rithms for recommending commands to users. Through-
out this section, we will use a simple hypothetical exam-
ple, shown in Table 1. In the table, each row represents a
developer, Hilaria, Ignacio, and Chuy, each using the same
development environment on dierent machines. Each user
happens to use only two commands per day on Monday,
Tuesday, and Wednesday; these commands are called Copy ,
Rename ,Cut,Commit ,Paste ,Format , and Update . For exam-
ple, on Monday, Hilaria uses Copy once, then Rename once;
on Tuesday she uses Copy once, then Paste once; and on
Wednesday she uses Copy once, and then Format once.
Using dierent techniques, each algorithm provides a sug-
gestion for which command we should recommend to a user
on Thursday. For example, based on Table 1, we can see
that Chuy does not use command Paste nor command For-
mat, so we could reasonably recommend either command to
him. Which command is more useful?
Monday Tuesday Wednesday
Hilaria Copy Rename Copy Paste Copy Format
Ignacio Copy Cut Copy Cut Copy Paste
Chuy Cut Commit Cut Commit Cut Update
Table 1: Developers' command use over three days.4.1 Existing: Most Popular
Linton and colleagues' [10] algorithm recommends the com-
mands that are the most-used by the community. This al-
gorithm is executed by counting the number of times each
command has been used in the entire user community. This
count represents the algorithm's condence level () that
this recommendation will be useful. To make a recommen-
dation to a specic user, the algorithm recommends com-
mands with the highest condence values, excluding com-
mands that the user already uses. Suppose that we want
to recommend a command to Hilaria (Table 1). This algo-
rithm would recommend command Cut, since Cuthas the
most uses (and thus highest condence level) of any com-
mand that she does not already use.
The intuition behind this algorithm is that the commands
that are used the most are also the most useful. A main ad-
vantage of this algorithm is that it can make recommenda-
tions to any user, including users whose history is unknown.
One disadvantage is that its recommendations are not tai-
lored; all users are assumed to be equal. This assumption
may largely hold in some kinds of applications, but it is
unlikely to hold in more complex software where dierent
users perform dierent tasks using dierent workows in
dierent roles. As we alluded to in the introduction, devel-
opment environments are likely to be one of these complex
pieces of software due to diering languages (for example,
python versus ruby), developer roles (programmer versus
manager), and diering tasks (xing bugs versus adding fea-
tures). These dierences may make recommendations made
by this algorithm less relevant.
4.2 Existing: Collaborative Filtering
Collaborative ltering algorithms can be divided into two
categories. The rst is user-based collaborative ltering,
where the command history of the user that we would like
to make a recommendation for is compared to the histories
of other users, and recommendations are made based solely
on the commands used by users with similar histories. The
second category is item-based collaborative ltering, where
similarities between commands are calculated based on the
community's usage history, and a recommendation for a sin-
gle user can be made by looking for commands similar to the
ones he already uses. The condence level for collaborative
ltering algorithms is based on similarities between users or
similarities of commands. We will not explain these algo-
rithms in any more depth here; instead, we refer to reader
to Matejka and colleagues for a technical explanation on
command recommendation using collaborative ltering [12].
Looking at our example (Table 1), suppose we use user-
based collaborative ltering to recommend a command to
Hilaria. The algorithm would nd similarities in the histo-
ries of Hilaria and Ignacio because they both use commands
Copy and Paste , and thus would recommend the command
Cut. If we used item-based collaborative ltering to rec-
ommend a command to Hilaria, the algorithm would notice
that, for example, someone who uses Copy also uses Cut, and
since Hilaria uses Copy , it would thus recommend Cut.
The intuition behind this family of algorithms is that users
and commands with similar histories are good sources of rel-
evant recommendations. The main advantage of this family
of algorithms is that recommendations are tailored to spe-
cic users. One disadvantage is that the algorithms ignore
sequential and temporal aspects of command usage. For in-stance, suppose that Table 1 displays only the rst week
on the job for Hilaria and Ignacio, but Chuy been working
with this software for 10 years. Also suppose that when
Chuy started, his history looked very much like Hilaria's,
but changed into a more \expert" pattern of command us-
age within his rst few weeks on the job. If we ran a user-
based collaborative ltering algorithm, very little similarity
would be found between Hilaria and Chuy, compared to Hi-
laria and Ignacio. This is a problem because Chuy's history
could be an excellent indicator of what command Hilaria
should discover next, because Chuy was once a novice like
Hilaria and successfully transitioned into an expert. In other
words, traditional collaborative ltering ignores how usage
patterns change over time.
4.3 Novel: Collaborative Filtering with Dis-
covery
In the next family of algorithms, we model how usage
patterns change over time with what we call \collaborative
ltering with discovery," which are based in part on sequen-
tial pattern mining [18]. With these algorithms, we model
how users discover new commands, then run standard col-
laborative ltering on their discovery patterns.
The algorithm operates as follows. For each user in a
community, given each user's command history, determine
their base command usage ,B, that is, all of the commands
that they know up to a certain point in time t(a tuning
parameter). Then, initialize a ruleset D= fg. Then starting
at time t+1:
1. For each used command xwhere x62B;D=D[fy!
x:y2Bg.
2. UpdateBto reect the newly discovered commands:
B=B[fxg
Continue these two steps until the end of the user's com-
mand history is reached. Next, we apply either user-based
collaborative ltering or item-based collaborative ltering to
the rulesetDfor each user in the community to obtain a rec-
ommendation for the desired user. The result will be a list
of command recommendations, each in the form x!yat
condence level , wherepropagates from the underlying
collaborative ltering algorithm. Ideally, the desired user
will already know command x, and not know command y.
In that case, we add yto the set of recommendations at
condence level . If the user does not know xory, then we
recommend only xat condence level . Finally, after the
list of recommended commands has been produced, the com-
mands recommended at the highest condence levels will be
recommended rst.
For example, let us apply this algorithm to our example
from Table 1. Suppose we want to make a command rec-
ommendation for Ignacio. First, let base command usage be
determined by a single day (for any real data set, using a sin-
gle day may yield inaccurate results; it is unlikely that a user
will use all the commands that she knows over the course of
just one day). Thus, Bfor Hilaria isfCopy ,Renameg,Bfor
Ignacio =fCopy ,Cutg, andBfor Chuy isfCut,Commitg.
Looking at Tuesday and Wednesday, we see that: Hilaria
rst discovers Paste , then Format ; Ignacio discovers Paste ,
and Chuy discovers Update . This leads to the rule sets dis-
played in Table 2.
Using the rulesets in the table, we can then apply collabo-
rative ltering; we will use user-based collaborative lteringHilariafCopy!Paste;Rename!Paste;
Copy!Format;Rename!Format;
Paste!Formatg
IgnaciofCopy!Paste;Cut!Pasteg
ChuyfCut!Update;Commit!Updateg
Table 2: Rulesets for developers in Table 1.
in this example. Since we want to make a recommendation
for Ignacio, user-based collaborative ltering will notice a
similarity between the discovery rulesets for Hilaria and Ig-
nacio; they both share Copy!Paste . Because of this simi-
larity, the algorithm will suggest discovery rules that may be
applicable to Ignacio fRename!Paste ,Copy!Format ,
Rename!Format ,Paste!Formatg, resulting in recom-
mendations for commands Rename and Format . Notice that
this algorithm did notrecommend Update , because there is
no evidence that Ignacio and Chuy discover in the same way.
The intuition behind this family of algorithms is that it
makes command recommendations based on discovery styles.
One major limitation of this approach is that it requires a
long enough usage history to capture discovery patterns. For
example, if we would like to make a recommendation to a
user who only has a week's worth of history, we are faced
with two diculties when picking the tparameter. If we
pick atthat is early (say, two days into the week), any com-
mands that we observe him \discovering" later in the week
may not be discoveries at all; they may in fact be commands
that he only needed later on in the week. However, if we pick
atthat is late (say, just before the last day of the week), we
risk not observing anynew commands being used, and thus
the algorithm could make no recommendations at all.
4.4 Novel: Most Widely Used
A slight extension of the Most Popular algorithm is what
we call the \most widely used" algorithm. This algorithm
ignores repeated uses of a command from a user, and instead
each command's condence level is based on the number
of people who use that command. In our example (Table 1),
suppose that we want to recommend a command to Hilaria
using this algorithm. Because commands Copy ,Cut, and
Paste are the most widely used commands, each used by
two users in the community, and Hilaria does not use Cut,
the algorithm would recommend Cutto Hilaria.
The intuition behind this algorithm is that a tool used
by many people is a useful tool. The advantage is that it
does not give undue weight to commands that are used a
lot, but by few people. One disadvantage is that commands
which few people use, even if the users nd them exceedingly
useful, are unlikely to be recommended using this algorithm.
4.5 Novel: Advanced Discovery
Another algorithm we call \advanced discovery" is a sim-
pler version of the collaborative ltering with discovery algo-
rithms above. The algorithm starts by producing a ruleset
for each user in the community. Then, given a user who we
would like to make recommendations for and the set of com-
mands that she has used B, we create a set of all discovery
rulesDfrom the community in the form x!ysuch that x
2Band y62B. The algorithm then recommends the com-
mands with the highest condence level , whereis dened
as the number of occurrences of x!yin the setDsuch that
the user under consideration has not used y.Let us return to our example in Table 1, which gener-
ates the rulesets shown in Table 2. Suppose we want to
make a recommendation for Ignacio. Ignacio's set Bis
fCopy;Cut;Pastegand setDis
fCopy!Format;Paste!Format;Cut!Updateg
Counting commands from Dthat Ignacio does not know,
we haveis 2 for Format andis 1 for Update . Thus, this
algorithm would recommend Format to Ignacio.
The intuition behind this algorithm is that it makes rec-
ommendations based on what a person already knows by
looking for commands that other people have discovered who
knew the same commands. The advantage of this algorithm
is that it recommends commands for which the user has the
\prerequisites" that other users had when they learned those
commands.
4.6 Novel: Most Popular Discovery
This algorithm counts the number of times a a command is
discovered in the community. A command is recommended
when it has not already been used by the developer and has
the highest , dened as the number of users who have used
the command after the base command usage window.
For example, take the users in Table 1. The command
Paste was discovered by 2 users, Format by 1 user, and Up-
date by 1 user. With this algorithm, if we wanted to make
a recommendation to Chuy, we would recommend Paste .
The intuition behind this algorithm is that it recommends
commands that many other people have discovered in the
past. The main disadvantage is the same as with the \Most
Popular" algorithm; it may recommend commands that are
not relevant to the tasks or style of the user.
5. AUTOMATED EV ALUATION
We conducted an automated comparison of algorithms to
investigate three research questions:
1. How feasible is automated recommendation of devel-
opment environment commands?
2. How does automated recommendation of development
environment commands compare to automated recom-
mendation of commands in other software?
3. How do our novel algorithms compare to existing al-
gorithms?
5.1 Implementation
We implemented each algorithm in Java. Our implemen-
tation uses data structures from the Sequential Pattern Min-
ing Framework [30], an open-source data mining framework.
Our implementation also uses collaborative ltering algo-
rithms from Apache Mahout [27], an open source machine
learning library. However, we modied Mahout's machine
learning algorithms to align with the algorithms described
by Matejka and colleagues [12].
5.2 Methodology
To investigate our research questions, we conducted a k-
tail evaluation as described by Matejka and colleagues [12].
A k-tail evaluation rst divides each user's command his-
tory into a training set and a testing set. The testing set
contains the last ktools that the developer discovered, andthe training set contains the entire usage history up until
the rst command in the testing set was discovered. Fi-
nally, the evaluation runs each algorithm on the training
set, and compares the recommendations made by each al-
gorithm to the commands in the testing set. To illustrate
the evaluation, consider again Table 1, which shows how
three imaginary users use commands. First, setting k=1,
we create the training set for each user (for example, Hilaria
=fCopy;Rename;Copy;Paste;Copyg) and the corresponding
testing sets (for example, Hilaria = fFormatg). Finally, we
train each algorithm on the data in the training sets, and
use the output of the algorithm to predict the command(s)
in the testing set. In our example, a successful outcome for
an algorithm is that, given the command usage for Hilaria,
the algorithm recommends Format . In simple terms, a more
successful algorithm will be one that can more often predict
the last commands discovered by a set of users.
This type of evaluation has several advantages but also
limitations, when compared to a live evaluation on real users
(see Section 6). One advantage is that it allows the algo-
rithm to make predictions for a very large number of users,
reducing any biasing eect a single user might have. It also
allows us to evaluate the algorithm with minimal distur-
bance to users, allowing us to avoid possible Hawthorne
eects [4]. On the other hand, the evaluation only ap-
proximates the quality of a recommendation because it may
falsely classify a use of a command as a discovery because
the user may actually be using the command again after us-
ing it before data collection began, rather than discovering
the command for the rst time. Moreover, this evaluation
only measures whether the algorithms correctly predict what
command that users naturally discovered, not what com-
mand would have been most useful for them to discover.
We created two variants of the k-tail evaluation because
the standard k-tail described by Matejka and colleagues makes
a key assumption, namely that the last command used by
a user is actually a useful discovery of a command by that
user. However, an analysis of the UDC data suggests that
some Eclipse users only used certain commands once; rather
than being useful discoveries, these may instead have been
instances where the user tried a command and did not nd it
useful. To mitigate this risk, we created two variants: k-tail
multi-use andk-tail multi-session . With k-tail multi-use, we
only include commands in the test set which have been used
multiple times. With k-tail multi-session, we only include
commands that have been used in multiple sessions. The
original k-tail evaluation, where a single use constitutes a
useful discovery, we will refer to as k-tail standard .
In order to attain comparable results, we attempted to
use the same algorithm and evaluation parameters used by
Matejka and colleagues [12]. In the Appendix to this paper,
we list those parameters.
5.3 Dataset
We used the dataset with 4309 Eclipse users, described
in Section 3. To fairly compare the algorithms, we only
include users for which every algorithm can make a recom-
mendation. For example, this excluded some users, such as
three who only appeared to use one command repeatedly in
12 months; we suspect that these were automated Eclipse
test systems that did little more than start and stop an
Eclipse instance at regular intervals. As another example,
some users appear to not discover commands after a rea-sonable base command usage window, so algorithms such as
Advanced Discovery are unable to make recommendations
for those users. Again, this illustrates the main limitation of
our discovery algorithms. When reporting results, we report
the total number of users for which every algorithm was able
to make a recommendation.
5.4 Results
Figure 3 displays the total number of correct recommen-
dations for each algorithm under each k-tail variant. The
number of Eclipse users for which all recommender algo-
rithms produced results is shown in each column after \n=".
Algorithms introduced in previous work are shown with grey
bars while our novel algorithms are shown in black.
With regard to our rst research question, the results in
Figure 3 suggest that it is feasible to recommend develop-
ment environment commands to software developers based
on previous usage history. Overall, the algorithms correctly
predicted between 1 in 10 to 3 in 10 commands, similar to
previous results that correctly predicted around 2 in 10 [12].
One might ask if such a low percentage of useful recommen-
dations is worthwhile. We argue that providing a developer
with one useful recommendation may make it worthwhile to
wade through nine not so useful ones, as long as it is easy
to determine which recommendations are useful.
With regard to our second research question, the results
suggest that recommendations for Eclipse commands are
more dicult to make correctly than for AutoCAD. For com-
parison, on the graph we mark the three results obtained
by Matejka and colleagues with an asterisk (*) [12]. For
the three comparable algorithms, Most Popular, User-Based
Collaborative Filtering, and Item-Based Collaborative Fil-
tering, results for Eclipse were about 24% lower than the
recommendations for AutoCAD. In Section 7, we discuss
why such a dierence may exist.
With regard to our third research question, overall our
novel algorithms faired favorably compared to the existing
algorithms. Linton's original Most Popular algorithm [10]
had the lowest percentage of correct recommendations. User-
Based Collaborative Filtering using Discovery, a novel al-
gorithm we introduced, faired the best in two out of three
evaluations, being beaten in the Standard evaluation by Ad-
vanced Discovery. We were surprised how often Most Widely
Used was correct, considering that it is such a straightfor-
ward extension of Linton's original algorithm. Overall, this
evaluation suggests that the User-Based Collaborative Fil-
tering using Discovery algorithm provides the most useful
recommendations of the algorithms studied.
The algorithms' performance varied signicantly. The
most ecient, Most Popular, took a few milliseconds to
make each recommendation, whereas User-Based Collabora-
tive Filtering with Discovery took up to 10 minutes to make
recommendations for a user. We view even 10 minutes as ac-
ceptable because, in a practical implementation, each user's
recommendations would be computed on her own computer,
in the background. Such distributed recommender system
algorithms have been explored by Ali and van Stam [1].
6. LIVE EV ALUATION
We performed a second study to evaluate the quality of
our recommendations. In this study, we again borrow from
Matejka and colleagues [12] and try to make recommenda-
tions to real developers, and then ask for their feedback/uni00A0
11.6% 
22.4%
17.2%
11.6%
23.6%
23.2%
22.3%
23.3%
0%                10%                 20% 0%                10%                 20% 0%                10%                 20%               30% Most /uni00A0Popula r
Most /uni00A0Widely/uni00A0Used
Item/uni2010Based
User /uni2010Based
Advanced
Popula r
Item/uni2010Based /uni00A0
User /uni2010Based /uni00A0/uni00A0/uni00A0/uni00A0/uni00A0Collabor ative/uni00A0Filtering
Disco very
Collabor ativeFilter ing/uni00A0
withDisco very15.4%
27.5%
24.0%
16.8%
22.8%
22.6%
28.6%
29.8%15.2%
26.9%
22.2%
15.3%
22.8%
22.8%
27.5%
28.2%Multi-Session (n=2471) Multi-Use (n=2855) Standard (n=3165)
*
**Figure 3: % recommendations correct in each k-tail evaluation for each algorithm.
about the quality of recommendations. This study comple-
ments the automated study in that it signicantly alleviates
the automated study's main limitation, that is, that it was
unclear whether the recommendations were truly useful.
6.1 Participants and Dataset
While the Matejka and colleagues study was able to con-
tact and recruit AutoCAD users because their own company
developed AutoCAD, we did not have any direct access to
Eclipse users through the Eclipse Foundation, due to pri-
vacy concerns. Therefore, we took two small convenience
samples of software developers; we treat each sample of de-
velopers separately in the remainder of this paper. We felt
that taking a relatively small sample was appropriate at this
stage in the research, before building a tool appropriate for
eld deployment and evaluation. We also felt that a conve-
nience sample was acceptable, given that our methodology
was double-blind, as we explain in Section 6.2. We again
used the 2009 dataset to train the algorithms.
The rst set of developers we call the \experts": these 4
people had extensive software development experience and
5 to 8 years of Eclipse experience. All were located in
Vancouver, Canada. The second set of developers we call
\novices": these participants were a mix of software devel-
opers and students, all with some Eclipse development ex-
perience. Each used Eclipse for Java development, and had
between 1 month and 6 years of experience with Eclipse,
with a median of 2 years of Eclipse experience.
6.2 Methodology
Matejka and colleagues used Linton's Most Popular, User-
Based Collaborative Filtering, and Item-Based Collabora-
tive Filtering to generate 8 recommendations per algorithm
for a total of 17 users of AutoCAD. Without knowing which
algorithm recommended which command, users responded
to a survey containing questions about novelty and useful-ness of the command recommendations. For novelty, users
were asked to use a 5-point Likert scale to rate their agree-
ment with the statement\I was familiar with the command,"
from \strongly agree" to \strongly disagree". For usefulness,
users rated on a 5-point Likert scale their agreement with
the statement \I will use this command."
We delivered the recommendations slightly dierently to
the two dierent participant sets. Whereas Matejka and col-
leagues sent their questions via survey, we asked the experts
to rate the commands during a face-to-face or phone-based
interview. This allowed us to infer whythe commands were
or were not novel and useful. For the novices, we evolved our
methodology slightly; the second author showed the partici-
pant each command using screencasts or live screen sharing,
told them what keyboard shortcut (if any) invoked the com-
mand, then asked participants to rate the novelty and use-
fulness of the command over the phone. This dierence be-
tween the delivery methods for novices and experts presents
a threat to the validity of our study, specically, that novices
may have rated novelty and usefuless dierently than ex-
perts due in part to dierences in delivery methods.
To reduce bias for both novices and experts, the rst au-
thor of this paper produced the recommendations from each
algorithm, combined and randomized the commands, then
gave the list to the second or third author to use to interview
the participants. In this way, the study was double-blind;
neither the interviewer nor the participant knew which al-
gorithm produced which command.
6.3 Results
Some algorithms were not able to recommend commands
for every user; we did not have long enough command us-
age histories for several participants to make recommenda-
tions with the Collaborative Filtering with Discovery algo-
rithms. This was the case for 1 of the 4 experts and 7 of
the 9 novices. Participants were recommended a mean of 31commands each because there was overlap in the command
recommendation set produced by each algorithm.
Overall, how many recommendations were rated novel by
participants depended on the user group. For experts, par-
ticipants reported that only about 26% of commands were
novel. For the novices, participants reported that about 80%
of recommended commands were novel. In both cases, it was
surprising that many commands were not considered novel
by participants. In discussing why those commands were not
novel, participants felt that either (1) they use an alterna-
tive command to the recommended one that they feel suits
them better or (2) that they already use the command. We
suspect that when developers reported that they already use
the recommended command, the reason is that the Eclipse
Usage Data Collector does not always record a command
usage in the data or records the command in a dierent way
whether invoked via a key binding or a tool menu. These re-
sults suggests that further work on the data collection side of
UDC to capture all command invocation events will improve
the usefulness of all algorithms' recommendations.
For the commands that participants rated as novel, Fig-
ure 4 displays how many commands each algorithm pro-
duced that participants rated as either useful or not useful.
The left column lists the name of the algorithm. Asterisks
indicate that, as we mentioned, these algorithms could not
produce recommendations for some users. The middle ma-
jor column indicates the number of useful and not useful
recommendations for novices, while the right major column
indicates the same for experts.
For experts, most algorithms produced fewer useful rec-
ommendations than not useful ones. Two exceptions that
produced just as many useful as not useful recommendations
were Item-Based Collaborative Filtering and User-Based Col-
laborative Filtering using Discovery. Item-Based Collabo-
rative Filtering using Discovery was the only algorithm to
produce more useful recommendations than not useful ones.
This result is consistent with our automated evaluation that
Collaborative Filtering with Discovery algorithms do well,
as compared to the other algorithms.
For novices, all algorithms produced more useful recom-
mendations than non-useful ones. The best performers were
Linton's Most Popular and the Item-Based Collaborative
Filtering with Discovery algorithms, both of which produced
zero not-useful recommendations. The Item-Based Collabo-
rative Filtering and User-Based Collaborative Filtering with
Discovery algorithms also performed well, having only one
not-useful recommendation each. User-Based Collaborative
Filtering produced the most useful recommendations, but
also produced the most not-useful recommendations. Over-
all, the results where Collaborative Filtering with Discovery
performing relatively well are consistent with our automated
evaluation, although the results are more dicult to draw
conclusions about because the Collaborative Filtering with
Discovery algorithms were not able to produce recommen-
dations for many novice users. The surprise from this eval-
uation is that Linton's Most Popular did so well, when it
performed so poorly on the automated evaluation. We dis-
cuss why this might be in the next section.
Despite these promising results, they likely do not reect
actual acceptance rates of recommendations if those recom-
mendations were delivered by a fully automated system.
Specically, these results may overestimate the likelihood
of a successful recommendation because a person deliveredthe recommendation. However, we are currently in the pro-
cess of building a recommender system that accompanies
tool recommendations with social endorsements from a de-
veloper's peers [14]. Such endorsements may improve the
likelihood that a recommendation is accepted. Nonetheless,
a recommender system algorithm, such as the ones evaluated
here, remain crucial to the success of this proposed system.
7. DISCUSSION
Overall, the results suggest that our Collaborative Filter-
ing using Discovery algorithms provide relatively good com-
mand recommendations. However, our Collaborative Filter-
ing using Discovery algorithms also require a longer-term
usage history to make accurate recommendations than do
simpler algorithms. Perhaps a reasonable compromise is a
hybrid-based approach, where simpler algorithms are used
to recommend commands to developers with little or no his-
tory, and more advanced discovery algorithms are used for
developers with a more extensive history.
Our results suggest that good recommendations can be
obtained by modeling short discovery patterns, such as that
the rename command is usual discovered before the move
element command. Our technique is based on sequential
pattern mining [18], which in the past has been used to de-
termine customer buying habits over time. Our technique
could be expanded to more fully implement sequential pat-
tern mining by modeling longer and more sophisticated pat-
terns of discovery. Other data mining algorithms may also
improve the quality of command recommendations further.
We were surprised that command recommendations for
software developers were signicantly (about 24%) less ac-
curate than recommendations for AutoCAD users in k-tail
evaluations. Although we are uncertain of the exact cause
of this dierence, we believe that it may be due to how
commands are invoked in Eclipse versus in AutoCAD. In
Eclipse, actions (such as switching editors) may or may
not be invoked through commands, depending on how they
are implemented and how well the implementors adhere to
command-invocation conventions. Strict, uniform adhesion
is especially unlikely because Eclipse is open-source and de-
veloped by many individual contributors, in contrast to Au-
toCAD, which is closed-source. Thus, the command pre-
dictability dierences may not be as much related to dif-
ferences in domain as to dierences in command framework
implementation. We hope to test this hypothesis by running
our recommendations over data produced by closed-source
development environments, such as Visual Studio.
In the live evaluation, novices were more readily accept-
ing of recommendations than experts. There may be several
reasons for this dierence. First, novices simply know fewer
commands than experts. Second, with novices we included
the keyboard shortcut for each recommendation, whereas
with experts we did not. Perhaps the fact that the recom-
mendations were more concrete made them more appealing.
Third, novices may be more exible and open to more possi-
bilities in the development environments they use. In other
words, as developers gain an experience with an IDE, they
may become more prone to \tool inertia" [16]. If that is the
case, it may be benecial for IDE command recommender
systems to make recommendations early in a developer's ca-
reer, so that the system can build a track record of success
by the time the developer becomes more experienced and it
becomes harder to make useful recommendations.0 246 8 1012141618200 2 4 6 8Most /uni00A0Widely/uni00A0Used
Item/uni2010Based
User/uni2010Based
Advanced
Popula r
Item/uni2010Based */uni00A0
User/uni2010Based */uni00A0/uni00A0/uni00A0/uni00A0/uni00A0Collabor ative/uni00A0Filtering
Disco very
Collabor ativeFilter ing/uni00A0
withDisco veryExperts Novices
11667246
1432223
155612
237918157Most /uni00A0Popula r17
2Figure 4: The number of commands rated as useful or not useful by participants. Useful recommendations
are solid bars; not useful recommendations are white bars.
We were surprised that Linton's Most Popular algorithm
performed so well for novices, yet did so poorly in the live ex-
pert evaluation, the automated evaluation, and in the evalu-
ations performed by Matejka and colleagues [12]. This sug-
gests that which recommendation algorithm will be the most
useful may depend in part on how mature the software user
is to whom the recommendation is made.
Both evaluations were limited in that we were unable to
measure recall, that is, the fraction of total relevant com-
mands that an algorithm recommended. Calculating recall
would require, for each developer, that we determine which
of the hundreds of commands that they did not use were
relevant. This would have been impossible in the k-tail eval-
uation and impractical in the live evaluation.
8. FUTURE WORK
Building on the results presented here, we feel that there
are several promising areas for future work.
One area for future work is improved detection of mean-
ingful discovery and adoption events. Our live participants
were surprised at the occasional poor quality of the rec-
ommendations, such as a Subversion user who was recom-
mended three CVS commands by the User-Based Collab-
orative Filtering Algorithm. The problem could be that
our current discovery algorithms assume that any time a
new command is used (or used repeatedly), that it is an
indicator of a useful command. Alternatively, future algo-
rithms could recognize discovery as some other data pat-
tern. For instance, an algorithm could detect replacement
behavior , where a developer frequently uses a command for
a period of time, then switches to using some other com-
mand frequently. For example, a developer may frequently
use Eclipse's nd references command, but may later switch
to using the open call hierarchy command, because the latter
is a more ecient way to complete her tasks.
While the evaluations that we performed roughly estimate
how well our algorithms make useful command recommen-
dations, future studies could evaluate usefulness in more
meaningful ways. Rather than making predictions about
what command users would do naturally or ask developers
whether they think that they would use the recommendedcommands, we could instead perform a longer term study
where we evaluate whether the recommended commands
are actually adopted. A clever study might be able to de-
termine more objectively whether the recommendation of a
command actually results in improved productivity, higher
software quality, or novel software artifacts.
Finally, future command recommender systems may bene-
t from including richer information than simply which com-
mand is recommended. Specically, one participant in our
live evaluation wanted to know how much time a command
would save him compared to the commands he already uses.
Moreover, that developer not only wanted to know whether
other developers nd commands useful, but whothose devel-
opers were, so he could assess his trust in those commands.
In previous work, we speculated that leveraging trust in this
way is essential for recommending commands that the de-
veloper will fully embrace [16].
9. CONCLUSION
In this paper, we introduced the notion of improving the
uency of software developers by automatically recommend-
ing development environment commands. Building on pre-
vious work, we introduced several novel recommendation al-
gorithms, including three that model patterns of command
discovery. In a proof of concept analysis and two studies
drawing on data from several thousand Eclipse users, we
demonstrated the utility of discovery patterns, compared
several algorithms' ability to predict command usage, and
evaluated the novelty and usefulness of the algorithms' rec-
ommendations with real developers. Our results suggest
that it is feasible to recommend integrated development en-
vironment commands to software developers, but research
remains to determine whether the recommended commands
result in an improvement to the development process.
Acknowledgments
We thank our study participants and Giuseppe Carenini for
their help. Thanks to Wayne Beaton at the Eclipse Foun-
dation for access to data from the Usage Data Collector.10. REFERENCES
[1] K. Ali and W. van Stam. TiVo: making show
recommendations using a distributed collaborative
ltering architecture. In Proceedings of the
International Conference on Knowledge Discovery and
Data Mining , KDD '04, pages 394{401. ACM, 2004.
[2] D. Campbell and M. Miller. Designing refactoring
tools for developers. In Proceedings of the 2008
Workshop on Refactoring Tools , pages 1{2, 2008.
[3] A. Cockburn and L. Williams. The costs and benets
of pair programming. In In eXtreme Programming and
Flexible Processes in Software Engineering , pages
223{247, 2000.
[4] G. Diaper. The hawthorne eect - a fresh examination.
Educational Studies , 16(3):261{267, 1990.
[5] J. Ellson, E. Gansner, L. Koutsoos, S. North, and
G. Woodhull. Graphviz - open source graph drawing
tools. In Graph Drawing , volume 2265 of Lecture
Notes in Computer Science , pages 594{597. Springer
Berlin / Heidelberg, 2002.
[6] T. Grossman, G. Fitzmaurice, and R. Attar. A survey
of software learnability: metrics, methodologies and
guidelines. In Proceedings of the 27th International
Conference on Human Factors in Computing Systems ,
CHI '09, pages 649{658, New York, NY, USA, 2009.
ACM.
[7] A. J. Ko and B. A. Myers. Finding causes of program
output with the java whyline. In Proceedings of the
27th International Conference on Human Factors in
Computing Systems , CHI '09, pages 1569{1578, New
York, NY, USA, 2009. ACM.
[8] J. Lave and E. Wenger. Situated Learning: Legitimate
Peripheral Participation . Cambridge University Press,
1st edition, September 1991.
[9] W. Li, J. Matejka, T. Grossman, J. A. Konstan, and
G. Fitzmaurice. Design and evaluation of a command
recommendation system for software applications.
ACM Trans. Comput.-Hum. Interact. , 18(2):6:1{6:35,
July 2011.
[10] F. Linton, D. Joy, H. Schaefer, and A. Charron. OWL:
A recommender system for organization-wide learning.
Educational Technology & Society , 3(1):62{76, 2000.
[11] C. Maltzahn. Community help: discovering tools and
locating experts in a dynamic environment. In
CHI'07: Proceedings of the SIGCHI Conference on
Human Factors in Computing Systems , pages 260{261,
New York, NY, USA, 1995. ACM.
[12] J. Matejka, W. Li, T. Grossman, and G. Fitzmaurice.
CommunityCommands: Command recommendations
for software applications. In Proceedings of the 22nd
Annual ACM Symposium on User Interface Software
and Technology , UIST '09, pages 193{202, 2009.
[13] M. M. M uller and W. F. Tichy. Case study: Extreme
programming in a university environment. In
Proceedings of the 23rd International Conference on
Software Engineering , ICSE '01, pages 537{544,
Washington, DC, USA, 2001. IEEE Computer Society.
[14] E. Murphy-Hill. Continuous social screencasting to
facilitate software tool discovery. In The Proceedings
of the International Conference on Software
Engineering , pages 1317{1320. IEEE, 2012.[15] E. Murphy-Hill and A. P. Black. An interactive
ambient visualization for code smells. In Proceedings
of the 5th International Symposium on Software
Visualization , SOFTVIS '10, pages 5{14, 2010.
[16] E. Murphy-Hill and G. C. Murphy. Peer interaction
eectively, yet infrequently, enables programmers to
discover new tools. In Proceedings of the ACM 2011
Conference on Computer Supported Cooperative Work ,
CSCW '11, pages 405{414, 2011.
[17] E. Murphy-Hill, C. Parnin, and A. P. Black. How we
refactor, and how we know it. In Proceedings of the
31st International Conference on Software
Engineering , ICSE '09, pages 287{297, Washington,
DC, USA, 2009. IEEE Computer Society.
[18] R. Srikant and R. Agrawal. Mining sequential
patterns: Generalizations and performance
improvements. In P. Apers, M. Bouzeghoub, and
G. Gardarin, editors, Advances in Database
Technology U EDBT '96 , volume 1057 of Lecture
Notes in Computer Science , pages 1{17. Springer
Berlin / Heidelberg, 1996. 10.1007/BFb0014140.
[19] J. Stylos, A. Faulring, Z. Yang, and B. Myers.
Improving api documentation using api usage
information. In Visual Languages and Human-Centric
Computing, 2009. VL/HCC 2009. IEEE Symposium
on, pages 119 {126, sept. 2009.
[20] M. B. Twidale. Over the shoulder learning:
Supporting brief informal learning. Proceedings of the
ACM Conference on Computer Supported Cooperative
Work , 14(6):505{547, 2005.
[21] P. Viriyakattiyaporn and G. C. Murphy. Challenges in
the user interface design of an IDE tool recommender.
InProceedings of the 2009 ICSE Workshop on
Cooperative and Human Aspects on Software
Engineering , CHASE '09, pages 104{107, 2009.
[22] P. Viriyakattiyaporn and G. C. Murphy. Improving
program navigation with an active help system. In
Proceedings of the 2010 Conference of the Center for
Advanced Studies on Collaborative Research , CASCON
'10, pages 27{41, New York, NY, USA, 2010. ACM.
[23] 10 Eclipse navigation shortcuts every Java
programmer should know. http://bit.ly/PVgPZl ,
May 2007.
[24] Eclipse - open resource shortcut - ctrl+shift+r.
http://greatwebguy.com/programming/eclipse/
eclipse-open-resource-shortcut-ctrlshiftr/ ,
Mar. 2007.
[25] Keyboard shortcuts - Eclipse.
http://parenthetical-thoughts.blogspot.com/
2008/08/keyboard-shortcuts-eclipse.html , Aug.
2008.
[26] Usage Data Collector Results, 2009. Website,
http://www.eclipse.org/org/usagedata/reports/
data/commands.csv .
[27] Apache mahout. http://mahout.apache.org/ , Mar.
2012.
[28] Eclipse. http://www.eclipse.org , Mar. 2012.
[29] emacs. http://www.gnu.org/software/emacs/ , Mar.
2012.
[30] SPMF - a sequential pattern mining framework.
http://www.philippe-fournier-viger.com/spmf/
index.php , Mar. 2012.AppendixParameter Value Description Rationale
Similarity Function Cosine
SimilarityUsed to calculate the similarity between
two users or two commands.Same as used by Matejka and col-
leagues [12].
User Neighborhood
Size32 The number of similar users from which
to draw commands to recommend for
user-based collaborative ltering algo-
rithms.Left unspecied by Matejka and col-
leagues [12]; in initial experiments with
user-based collaborative ltering, we found
that a value of 32 provided good results.
Base Command Us-
age Window40 sessions See Section 4.3 Based on observations about UDC data;
appears short enough to enable recommen-
dations but long enough to include most
commands the user knows.
k 1 See Section 5.2 Same as used by Matejka and col-
leagues [12].
Number of Recom-
mendations10 Maximum number of recommendations
that each algorithm is allowed to pro-
duce.Same as used by Matejka and col-
leagues [12].
 1 Tuning parameter that serves as a scal-
ing factor for condence level ( ).Left unspecied by Matejka and col-
leagues [12]; any positive value appears to
produce the same results as any other pos-
itive value.
Table 3: Parameters used in algorithm comparison.