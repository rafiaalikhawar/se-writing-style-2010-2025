High-Impact Defects: A Study of Breakage and Surprise
Defects
Emad Shihab
Software Analysis and
Intelligence Lab (SAIL)
Queen‚Äôs University, Canada
emads@cs.queensu.caAudris Mockus
Avaya Labs Research
233 Mt Airy Rd, Basking
Ridge, NJ
audris@avaya.comY asutaka Kamei, Bram
Adams and Ahmed E.
Hassan
Software Analysis and
Intelligence Lab (SAIL)
Queen‚Äôs University, Canada
{kamei, bram, ahmed}
@cs.queensu.ca
ABSTRACT
The relationship between various software-related phenomena (e.g.,
code complexity) and post-release software defects has been thor-
oughly examined. However, to date these predictions have a lim-
ited adoption in practice. The most commonly cited reason is that
the prediction identiÔ¨Åes too much code to review without distin-
guishing the impact of these defects. Our aim is to address this
drawback by focusing on high-impact defects for customers and
practitioners. Customers are highly impacted by defects that break
pre-existing functionality (breakage defects), whereas practition-
ersare caught off-guard by defects in Ô¨Åles that had relatively few
pre-release changes (surprise defects). The large commercial soft-
ware system that we study already had an established concept of
breakages as the highest-impact defects, however, the concept of
surprises is novel and not as well established. We Ô¨Ånd that surprise
defects are related to incomplete requirements and that the common
assumption that a Ô¨Åx is caused by a previous change does not hold
in this project. We then Ô¨Åt prediction models that are effective at
identifying Ô¨Åles containing breakages and surprises. The number
of pre-release defects and Ô¨Åle size are good indicators of break-
ages, whereas the number of co-changed Ô¨Åles and the amount of
time between the latest pre-release change and the release date are
good indicators of surprises. Although our prediction models are
effective at identifying Ô¨Åles that have breakages and surprises, we
learn that the prediction should also identify the nature or type of
defects, with each type being speciÔ¨Åc enough to be easily identiÔ¨Åed
and repaired.
Categories and Subject Descriptors
D.2.8 [ Software Engineering ]: Metrics‚Äî Complexity Measures,
Performance Measures
General Terms
Software Quality Assurance
Keywords
High-Impact, Process Metrics, Defect Prediction
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proÔ¨Åt or commercial advantage and that copies
bear this notice and the full citation on the Ô¨Årst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciÔ¨Åc
permission and/or a fee.
ESEC/FSE‚Äô11, September 5‚Äì9, 2011, Szeged, Hungary.
Copyright 2011 ACM 978-1-4503-0443-6/11/09 ...$10.00.1. INTRODUCTION
Work on defect prediction aims to assist practitioners in prior-
itizing software quality assurance efforts [6]. Most of this work
uses code measures (e.g., [25, 35]), process measures (e.g., [11]),
and social structure measures (e.g., [4]) to predict source code ar-
eas (i.e., Ô¨Åles) where the post-release defects are most likely to be
found. The number of pre-release changes or defects and the size of
an artifact are commonly found to be the best indicators of a Ô¨Åle‚Äôs
post-release defect potential.
Even though some studies showed promising performance re-
sults in terms of predictive power, the adoption of defect prediction
models in practice remains low [10, 13, 29]. One of the main rea-
sons is that the amount of code predicted to be defect-prone far ex-
ceeds the resources of the development teams considering inspec-
tion of that code. For example, Ostrand et al. [27] found that 80%
of the defects were in 20% of the Ô¨Åles. However, these 20% of the
Ô¨Åles accounted for 50% of the source code lines. At the same time,
all defects are considered to have the same negative impact, which
is not realistic, because, for example, documentation defects tend
to be far less impacting than security defects. At the same time,
model-based prediction tends to indicate the largest and the most
changed Ô¨Åles as defect-prone: areas already known to practitioners
to be the most problematic.
In this work, we address the problem of predicting too many
defects by focusing the prediction on the limited subset of high-
impact defects . Since impact has a different meaning for different
stakeholders, we consider two of the possible deÔ¨Ånitions in this pa-
per:breakages andsurprises . Breakages are a commonly accepted
concept in industry that refer to defects that break functionality de-
livered in earlier releases of the product on which customers heav-
ily rely in their daily operations. Such defects are more disruptive
because 1) customers are more sensitive to defects that occur in
functionality that they are used to than to defects in a new feature
and 2) breakages are more likely to hurt the quality image of a pro-
ducer, thus directly affecting its business. To ensure that we focus
only on the highest impact defects, we only consider breakages that
have severities ‚Äúcritical‚Äù (the product is rendered non-operational
for a period of time) and ‚Äúhigh‚Äù (the product operation has signiÔ¨Å-
cant limitations negatively impacting the customer‚Äôs business).
Whereas breakages have a high impact on customers, surprise
defects are a novel concept representing a kind of defects that highly
impact practitioners. Surprises are defects that appear in unex-
pected locations or locations that have a high ratio of post-to-pre
release defects, catching practitioners off-guard, disrupting their
already-tight quality assurance schedules. For example, post-releasedefects in Ô¨Åles that are heavily changed or have many defects prior
to the release are expected and scheduled for. However, when a
defect appears in an unexpected Ô¨Åle, the workÔ¨Çow of developers is
disrupted, causing them to vacate their current work and shift focus
to addressing these surprises.
The investigated project has used the concept of breakages for
several decades but surprise defects are a new concept. Hence,
we start by comparing the properties of breakage and surprise de-
fects and qualifying the impact of surprises. We then build pre-
diction models for breakages and surprise defects (RQ1), identify
which factors are the best predictors for each type of defect (RQ2)
and quantify the relative effect of each factor on the breakage and
surprise-proneness (RQ3). Finally, we calculate the effort savings
of using specialized defect prediction models and perform a quali-
tative evaluation of the usability of the prediction models in practice
and propose ways to make such prediction more relevant.
We make the following contributions:
Identify and explore breakage and surprise defects. We
Ô¨Ånd that breakage and surprise defects are approximately
one-Ô¨Åfth of the post-release defects. Only 6% of the Ô¨Åles
have both types of defects. For example, breakages tend to
occur in locations that have experienced more defect Ô¨Åxing
changes in the past and contain functionality that was imple-
mented less recently than the functionality in locations with
surprise defects.
Develop effective prediction models for breakage and sur-
prise defects. Our models can identify future breakage and
surprise Ô¨Åles with more than 70% recall and a two to three
fold increase of precision over random prediction.
Identify and quantify the major factors predicting break-
age and surprise defects. Traditional defect prediction fac-
tors (i.e., pre-release defects and size) have a strong posi-
tive effect on the likelihood of a Ô¨Åle containing a breakage,
whereas the co-changed Ô¨Åles and time-related factors have a
negative effect on the likelihood of a Ô¨Åle containing a surprise
defect.
Measure the effort savings of specialized prediction mod-
els.Our custom models reduce the amount of inspected Ô¨Åles
by 41 - 55%, which represents a 42 - 50% reduction in the
number of inspected lines of code.
Propose areas and methods to make defect prediction more
practical. A qualitative study suggests that an important bar-
rier to the use of prediction in practice is lack of indications
about the nature of the problem or the ways to solve it. The
method to detect surprise defects may be able to highlight
areas of the code that have incorrect requirements. We pro-
pose that an essential part of defect prediction should include
prediction of the nature of the defect or ways to Ô¨Åx it.
The rest of the paper is organized as follows. Section 2 highlights
the related work. Section 3 compares the properties of breakages
and surprise defects. Section 4 outlines the case study setup and
Section 5 presents our case study results. Section 6 discusses the
effort savings provided by the specialized models built in our study.
Section 7 highlights the limitations of the study. Section 8 reÔ¨Çects
on the lessons learned about the practicality of defect prediction
and details our future work. We conclude the paper in Section 9.2. RELATED WORK
The majority of the related work comes from the area of defect
prediction. Previous work typically builds multivariate logistic re-
gression models to predict defect-prone locations (e.g., Ô¨Åles or di-
rectories). A large number of previous studies use complexity met-
rics (e.g., McCabe‚Äôs cyclomatic complexity metric [19] and Chi-
damber and Kemerer (CK) metrics suite [5]) to predict defect-prone
locations [3,12,24,26,32,35]. However, Graves et al. [11], Leszak
et al. [18] and Herraiz et al. [15] showed that complexity metrics
highly correlate with the much simpler lines of code (LOC) mea-
sure. Graves et al. [11] argued that change data is a better predictor
of defects than code metrics in general and showed that the number
of prior changes to a Ô¨Åle is a good predictor of defects. A number
of other studies supported the Ô¨Ånding that prior changes are a good
defect predictor and additionally showed that prior defect is also a
good predictor of post release defects [1,14,17,18,23,33]. To sum
up, this previous work showed that complexity metrics and hence
size measured in LOC, prior change and prior defects are a good
predictor of defect-proneness. The focus of the aforementioned
work was to improve the prediction performance by enriching the
set of metrics used in the prediction model. In this work, we incor-
porate the Ô¨Åndings of previous work by using traditional defect pre-
diction metrics/factors, in addition to other more specialized fac-
tors related to co-change and time properties to predict the location
of highly impacting defects. However, we would like to note that
our focus here is capturing high-impact defects rather than adding
metrics to improve the performance of defect prediction models in
general.
Although it has been shown that defect prediction can yield ben-
eÔ¨Åt in practice, its adoption remains low [10, 13]. As Ostrand et
al.[27, 28] showed, 20% of the Ô¨Åles with the highest number of
predicted defects contain between 71-92% of the defects, however
these 20% of the Ô¨Åles make up 50% of the code. To make de-
fect prediction more appealing to practice, recent work examined
the performance of software prediction models when the effort re-
quired to address the identiÔ¨Åed defect is considered (i.e., effort-
aware defect prediction) [2, 16, 20]. Although effort is taken into
account, these models still predict the entire set of post-release de-
fects, giving each defect equal impact potential.
Other work focused on reducing the set of predicted defects based
on the semantics of the defect. For example, Shin et al. [31] and
Zimmermann et al. [34] focused on predicting software vulnerabil-
ities since they have high priority. Instead of narrowing down the
set of defects vertically based on the semantics of the defects, we
narrow down the defects horizontally across domains. For example,
our models can predict high impact defects across many domains,
whereas a model focused on vulnerability defects is only useful for
one domain.
3. BREAKAGE AND SURPRISE DEFECTS
In this section, we provide background of the software project
under study, and deÔ¨Åne breakages and surprise defects. We then
characterize and compare breakage and surprise defects.
3.1 Background
Software Project: The data used in our study comes from a well-
established telephony system with many tens of thousands of cus-
tomers that was in active development for almost 30 years and,
thus, has highly mature development and quality assurance proce-
dures. The present size of the system is approximately seven mil-
lion non-comment LOC, primarily in C and C++. The data used in
our study covers Ô¨Åve different releases of the software system.Change Data: There are two primary sources of data used. Sablime,
a conÔ¨Åguration management system, is used to track ModiÔ¨Åcation
Requests (MR), which we use to identify and measure the size of
a software release and the number of defects (pre-release and post-
release). When a change to the software is needed, a work item
(MR) is created. MRs are created for any modiÔ¨Åcation to the code:
new features, enhancements, and Ô¨Åxes. The project uses internally
developed tools on top of the Source Code Control System (SCCS)
to keep track of changes to the code. Every change to the code has
to have an associated MR and a separate MR is created for differ-
ent tasks. We call an individual modiÔ¨Åcation to a single Ô¨Åle a delta.
Each MR may have zero or more deltas associated with it. Since
the development culture is very mature, these norms are strictly
enforced by peers.
For each MR, we extracted a number of attributes from Sablime
and the SCCS: the Ô¨Åles the MR touches, the release in which the
MR was discovered, the date the MR was reported, the software
build where the code was submitted, the resolution date (i.e., when
the MR was Ô¨Åxed/implemented), resolution status for each release
the MR was submitted to, the severity and priority of the MR, the
MR type (e.g., enhancement or problem) and the general availabil-
ity date of the release that includes the MR.
For each release, we classify all MRs into two types: pre-release
changes (or defects if they are type problem), and post-release de-
fects. MRs that are submitted to a release before the General Avail-
ability date (we refer to it as GA), are considered to be pre-release
defects. Fixes reported for a particular release after the GA date,
are considered to be post-release defects.
3.2 DeÔ¨Åning Breakages and Surprise Defects
Breakage Defects: Defects are introduced into the product because
the source code is modiÔ¨Åed to add new features or to Ô¨Åx existing
defects. When such defects break, i.e., cause a fault or change ex-
isting functionality that has been introduced in prior releases, we
call these defects breakages. The concept of breakages typically
is familiar in most companies. For example, in the project studied
in this paper, all severity one and two defects of established func-
tionality are carefully investigated by a small team of experts. In
addition to a root-cause analysis and suggestions to improve quality
assurance efforts, the team also determines the originating MR that
introduced the breakage. While other companies may use a dif-
ferent terminology than ‚Äúbreakages‚Äù, most investigate such high-
impact problems just as carefully, therefore similar data is likely to
be available in other projects with mature quality practices.
Surprise Defects: Previous research has shown that the number
of pre-release defects is a good predictor of post-release defects
(e.g., [23, 35]). Therefore, it is a common practice for software
practitioners to thoroughly test Ô¨Åles with a large number of pre-
release defects. However, in some cases, Ô¨Åles that rarely change
also have post-release defects. Such defects catch the software
practitioners off-guard, disrupting their already-tight schedules. To
the best of our knowledge, surprise defects have not been stud-
ied yet, and therefore are not recorded in issue tracking systems.
Hence, for the purpose of our study, we use one possible deÔ¨Ånition
of surprise. We deÔ¨Åne the degree to which a Ô¨Åle contains surprise
defects as:
Surprise (file) =No: of post release defects (file)
No: of pre release defects (file);
whereas in Ô¨Åles without pre-release defects we deÔ¨Åne it as:
Surprise (file) =No: of post release defects (file)2:
Because our deÔ¨Ånition of surprise is given in terms of the ra-
tio of post-to-pre release defects, we need to determine from whatthreshold the ratio should be considered signiÔ¨Åcant. For example,
if a Ô¨Åle has one post-release defect and 10 pre-release defects, i.e.,
the deÔ¨Åned surprise value is1
10= 0:1, should this Ô¨Åle be Ô¨Çagged
as being a surprise?
To calculate the surprise threshold, we examine all the Ô¨Åles that
had a defect reported within one month of the GA date of the pre-
vious release. The intuition behind this rule is that high impact
defects are likely to be reported immediately after the software
is released. Hence, we calculate the median of the ratio of all
post-release defects and all pre-release defects for all Ô¨Åles changed
within one month of the previous GA date, then use this value as
the surprise threshold for the next release. For example, the sur-
prise threshold for release 2.1 is determined by the median ratio of
post-to-pre-release defects of all Ô¨Åles that had a post-release defect
reported against them within a month after release 1.1.
3.3 Occurrence of Breakage and Surprise De-
fects
Before analyzing the characteristics of breakage and surprise de-
fects, we examine the percentage of Ô¨Åles that have breakages and
surprise defects. To put things in context, we also show the percent-
age of Ô¨Åles with one or more post-release defects. Table 1 shows
that on average, only 2% of the Ô¨Åles have breakages or surprise
defects. That is approximately one Ô¨Åfth of the Ô¨Åles that have post-
release defects.
Having a small percentage of Ô¨Åles does not necessarily mean less
code, since some Ô¨Åles are larger than others. Therefore, we also ex-
amine the amount of LOC that these Ô¨Åles represent. Table 1 shows
that on average, the amount of LOC that these breakage and sur-
prise Ô¨Åles make up is 3.2% and 3.8%, respectively. This is approx-
imately one fourth the LOC of Ô¨Åles with the post-release defects.
The reduction is both promising, because it narrows down the set of
Ô¨Åles to be Ô¨Çagged for review, and challenging, because predicting
such unusual Ô¨Åles is much harder.
Table 1 also compares the percentages of MRs representing post-
release, breakage and surprise defects. On average, the percentage
of breakage and surprise MRs is much smaller than that of post-
release MRs. Since we use the surprise threshold from the previous
release to determine the surprise threshold, we are not able to cal-
culate surprise defects for the Ô¨Årst release (R1.1).


Breakage and surprise defects are difÔ¨Åcult to pinpoint, since
they only appear in 2% of the Ô¨Åles.
3.4 Breakages vs Surprise Defects
As mentioned earlier, breakage defects are high severity defects
that break existing functionality. They are a common concept in
industry, and their impact is understood to be quite high. How-
ever, surprise defects are a concept that we deÔ¨Åned based on our
own industrial experience. In this section, we would like to learn
more about the characteristics of surprise defects and verify our
assumption that surprise defects highly impact the development or-
ganization, by addressing the following questions:
Are surprise defects different than breakage defects? If so,
what are the differences?
Are surprise defects impactful?
Such veriÔ¨Åcation helps us appreciate the value of studying sur-
prise defects and to understand the implications of our Ô¨Åndings for
future research in the Ô¨Åeld.Table 1: Percentage of Files, LOC and MRs containing Post-release, Breakage and Surprise defects.
Release Post-Release Breakage Surprise
Files LOC MRs Files LOC MRs Files LOC MRs
R1.1 21.8 27 78.8 1.6 2.6 29.4 - - -
R2.1 6.5 8.4 48.6 2.1 2.6 27.1 0.2 0.4 1.5
R3.0 7.8 12.7 54.5 2.1 3.8 28.4 3.3 6.4 13.6
R4.0 11 18 79.7 1.4 1.3 25.6 3.0 5.2 11.5
R4.1 5.0 6.8 46.1 2.5 2.8 22.4 1.5 3.2 6.1
Average 10.4 14.6 61.5 2.0 3.2 26.6 2.0 3.8 8.2
R2.1 R3.0 R4.0 R4.1 % of Files 
0 2 4 6 8 10 
Figure 1: Percentage of Files Containing both, Surprise and
Breakage Defects
Are surprise defects different than breakage defects? If
so, what are the differences?
First, we look at the locations where breakage and surprise defects
occur. If we, for example, Ô¨Ånd that breakage and surprise defects
occur in the same location, then we can assume that the predic-
tion models for breakage-prone Ô¨Åles will sufÔ¨Åce for surprise defect
prediction.
To quantify the percentage of Ô¨Åles that contain both types of de-
fects, we divide the Ô¨Åles into two sets: Ô¨Åles with breakages and Ô¨Åles
with surprise defects. Then, we measure the intersection of the Ô¨Åles
in the two sets divided by the union of the Ô¨Åles in the two sets:
Breakages‚à©Surprise
Breakages‚à™Surprise:
Figure 1 shows the percentage of Ô¨Åles that have both breakages
and surprise defects. At most 6% of the breakage and surprise Ô¨Åles
overlap. This low percentage of overlap shows that breakages and
surprises are two very different types of defects. However, further
examination of their speciÔ¨Åc characteristics is needed to better un-
derstand the potential overlap.
Therefore, we investigate the characteristics of breakage and sur-
prise defects along the different defect prediction dimensions. For
example, previous work showed that the amount of activity (i.e.,
number of pre-release changes) is a good indicator of defect-proneness,
therefore, we look at the activity of breakage and surprise defects
to compare and contrast. We summarize our Ô¨Åndings, which are
statistically signiÔ¨Åcant with a p-value < 0.05, as follows:Activity. Comparing the activity, measured in number of MRs, of
breakage and surprise Ô¨Åles to non-breakage and non-surprise Ô¨Åles
shows that breakage and surprise Ô¨Åles have less activity. This could
indicate that perhaps breakage and surprise Ô¨Åles were inspected
less.
MR size. Breakage and surprise Ô¨Åles are modiÔ¨Åed by larger MRs.
On average, a breakage MR touches 47 Ô¨Åles as compared to 8 Ô¨Åles
touched for a non-breakage MRs. Surprise MRs touch 71 Ô¨Åles on
average as compared to 13 Ô¨Åles touched by non-surprise MRs.
Time. The start times of MRs that touch breakage and surprise Ô¨Åles
show that for a particular release, breakage Ô¨Åles are modiÔ¨Åed earlier
than average, whereas surprise defect Ô¨Åles are on average, worked
on closer to the release date. This suggests that considerably less
time was available for the development and testing of surprise Ô¨Åles.
Functionality. By deÔ¨Ånition, breakage Ô¨Åles are involved with legacy
features (e.g., core features of the communication software), whereas
surprise defect Ô¨Åles are involved with more recent features (e.g., the
porting of the studied software system to work in virtual environ-
ments).
Maintenance efforts. Breakage and surprise defect Ô¨Åles are in-
volved in more code addition MRs than the average non-breakage
or non-surprise Ô¨Åles. However, breakage Ô¨Åles were involved in
more Ô¨Åxes than surprise defect Ô¨Åles.
Are surprise defects impactful?
The above comparisons show that breakage and surprise defects are
different. To assess the impact of surprise defects we selected the
Ô¨Åve Ô¨Åles with the highest surprise ratios in the last studied release
of the software for an in-depth evaluation.
To evaluate the Ô¨Åve Ô¨Åles with the highest surprise score, we se-
lected the latest changes prior to GA and the changes (defects)
within one month after the GA for each of these Ô¨Åles and inves-
tigated the nature and the origin of these defects by reading defect
descriptions, resolution history and notes, inspection notes, as well
as changes to the code. The Ô¨Åve Ô¨Åles were created eight, six, Ô¨Åve,
and, (for two Ô¨Åles), one year(s) prior to GA. There were seven Ô¨Åxes
within one month after the GA in these Ô¨Åve Ô¨Åles. Five defects had
high severity and only two had medium severity, indicating that all
of them were important defects. At least one Ô¨Åx in each of the Ô¨Åve
Ô¨Åles was related to incomplete or inadequate requirements, i.e., in-
troduced at the time of Ô¨Åle creation or during the last enhancement
of functionality.
For each post-GA defect in these Ô¨Åve Ô¨Åles, we traced back through
changes to determine the Ô¨Årst change that introduced the defect.
Our intuition was that if the prediction can point to the change in-
troducing the defect, it would limit the scope of the developers in-
specting the Ô¨Çagged area to the kind of functionality changed and to
the particular lines of code, thus simplifying the task of determin-
ing the nature of the potential defect or, perhaps, even highlighting
the avenues for Ô¨Åxing it.The results were surprising. In none of the cases the last pre-GA
change could have been the cause of the defect. In the Ô¨Åve defects
related to inadequate requirements, we had to trace all the way to
the initial implementation of the Ô¨Åle. In the case of the two more
recent Ô¨Åles, at least we found a relationship between the pre-GA
change (that was Ô¨Åxing inadequate requirements) and the post-GA
Ô¨Åx that was Ô¨Åxing another shortcoming of the same requirements
that was not addressed by the prior Ô¨Åx. For the two remaining de-
fects introduced during coding or design phases, we had to trace
back at least three changes to Ô¨Ånd the cause.
This qualitative investigation supports our intuition about the na-
ture of surprise defects and our Ô¨Åndings of the differences between
breakages and surprise defects. The surprise defects appear to be
an important kind of unexpected defect that appears to lurk in the
code for long periods of time before surfacing. It also suggests
a possible mechanism by which the prediction of surprise defects
might work. As the usage proÔ¨Åle or the intensity of usage changes
over the years, early warnings are given by customers wanting to
adjust a feature (one of the defects), or hard-to reproduce defects
are starting to surface. Two of the defects appear to be caused by
the increased use of multicore architecture that resulted in hard-to-
reproduce timer errors and interactions with the system clock. At
some point a major fault (surprise) is discovered (system restarting
because of full message buffer, or because of data corruption) and
the shortcomings of the requirements are Ô¨Ånally addressed by the
Ô¨Åx to the surprise defect.


Breakage and surprise defects are unique and different. Sur-
prise defects also have high severity and appear to indicate
problems in the requirements.
4. CASE STUDY SETUP
Now that we have analyzed the characteristics of breakage and
surprise defects, we want to examine the effectiveness of predict-
ing the locations of breakage and surprise defects. We address the
following research questions:
RQ1. Can we effectively predict which Ô¨Åles will have break-
age/surprise defects?
RQ2. Which factors are important for the breakage/surprise
defect prediction models?
RQ3. What effect does each factor have on the likelihood of
Ô¨Ånding a breakage/surprise defect in a Ô¨Åle?
In this section, we outline and discuss the case study setup. First,
we present the various factors used to predict breakage and sur-
prise defects. Then, we outline the prediction modeling technique
used. Finally, we outline how we evaluate the performance of our
prediction models.
4.1 Factors Used to Predict Breakage and Sur-
prise Defects
The factors that we use to predict Ô¨Åles with breakage or surprise
defects belong to three different categories: 1) traditional factors
found in previous defect prediction work, 2) factors associated to
co-changed Ô¨Åles and 3) time-related factors (i.e., the age of a Ô¨Åle
and the time since the last change to the Ô¨Åle).
These factors are based on previous post-release defect predic-
tion work and on our Ô¨Åndings in Section 3.4.
Traditional Factors: Previous work shows that the number of pre-
vious changes, the number of pre-release defects and the size of
a Ô¨Åle are good indicators of post-release defects [11, 35]. Sincebreakage and surprise defects are a special case of post-release de-
fects, we believe that they can help us predict breakage and surprise
defects as well.
Co-change Factors: By deÔ¨Ånition, breakage and surprise defects
are not expected to happen. One possible reason for their occur-
rence could be hidden dependencies (e.g., logical coupling [9]).
The intuition here is that a change to a co-changed Ô¨Åle may cause a
defect. Since more recent changes are more relevant, we also cal-
culate for each factor its value in the three months prior to release,
labeled as ‚Äúrecent‚Äù.
Time Factors: To take into account the fact that breakage and sur-
prise defects start earlier and later than average, respectively, we
consider factors related to how close to a release a Ô¨Åle is changed,
as well as the Ô¨Åle‚Äôs age.
Each category comprises several factors, as listed in Table 2. For
each factor, we provide a description, motivation and any related
work.
4.2 Prediction Models
In this work, we are interested in predicting whether or not a
Ô¨Åle has a breakage or surprise defect. Similar to previous work on
defect prediction [35], we use a logistic regression model. A lo-
gistic regression model correlates the independent variables in Ta-
ble 2) with the dependent variable (probability of the Ô¨Åle containing
a breakage or surprise defect).
Initially, we built the logistic regression model using all of the
factors. However, to avoid collinearity problems and to assure that
all of the variables in the model are statistically signiÔ¨Åcant, we re-
moved highly correlated variables (i.e., any variables with corre-
lation higher than 0.5). In the end, we were left with Ô¨Åve factors
that all had a Variance InÔ¨Çation Factor (VIF) below 2.5, as recom-
mended by previous work [4]. To test for statistical signiÔ¨Åcance,
we measure the p-value of the independent variables in the model
to make sure that this is less than 0.1.
Altogether, we extracted a total of 15 factors. After removing the
highly correlated variables, Ô¨Åve factors were left that covered all
three categories, i.e., pre_defects, Ô¨Åle_size, num_co-changed_Ô¨Åles,
modiÔ¨Åcation_size_co-changed_Ô¨Åles and latest_change_before_release.
4.3 Performance Evaluation of the Prediction
Models
After building the logistic regression model, we verify its perfor-
mance using two criteria: Explanatory Power and Predictive Power.
These measures are widely used to measure the performance of lo-
gistic regression models in defect prediction [6,35].
Explanatory Power . Ranges between 0-100%, and quantiÔ¨Åes the
variability in the data explained by the model. We also report and
compare the variability explained by each independent variable in
the model. Examining the explained variability of each indepen-
dent variable allows us to quantify the relative importance of the
independent variables in the model.
Predictive Power . Measures the accuracy of the model in predict-
ing the Ô¨Åles that have one or more breakage/surprise defects. The
accuracy measures that we use (precision and recall) are based on
the classiÔ¨Åcation results in the confusion matrix (shown in Table 3).
1.Precision: the percentage of correctly classiÔ¨Åed breakage/surprise
Ô¨Åles over all of the Ô¨Åles classiÔ¨Åed as having breakage/surprise
defects: Precision =TP
TP+FP.
2.Recall: the percentage of correctly classiÔ¨Åed breakage/surprise
Ô¨Åles relative to all of the Ô¨Åles that actually have breakage/surprise
defects: Recall =TP
TP+FN.Table 2: List of Factors Used to Predict Breakage and Surprise Defects. Factors in italics are factors used in the Ô¨Ånal prediction
models, i.e., after removing highly correlated factors.
Category Factor Description Rationale Related Work
Traditional Factorspre_defectsNumber of pre-
release defectsTraditionally performs
well for post-release de-
fect prediction.Prior defects are a good indicator of future
defects [33].
pre_changesNumber of pre-
release changesTraditionally performs
well for post-release de-
fect prediction.The number of prior modiÔ¨Åcations to a Ô¨Åle
is a good predictor of future defects [1, 11,
18].
Ô¨Åle_sizeTotal number of
lines in the Ô¨ÅleTraditionally performs
well for post-release de-
fect prediction.The lines of code metric correlates well
with most complexity metrics (e.g., Mc-
Cabe complexity) [11,15,18,27].
Co-change Factors(recent)
num_co-
changed_Ô¨ÅlesNumber of Ô¨Åles
a Ô¨Åle co-changed
withThe higher the num-
ber of Ô¨Åles a Ô¨Åle co-
changes with, the higher
the chance of missing to
propagate a change.The number of Ô¨Åles touched by a change
is a good indicator of its risk to introduce
a defect [22]. We apply this factor to the
number of Ô¨Åles co-changing with a Ô¨Åle.
(recent)
size_co-
changed_Ô¨ÅlesCumulative size of
the co-changed Ô¨ÅlesThe larger the co-
changed Ô¨Åles are, the
harder they are to main-
tain and understand.The simple lines of code metric correlates
well with most complexity metrics (e.g.,
McCabe complexity) [11, 15, 18, 27]. We
apply this factor to co-changing Ô¨Åles.
(recent) mod-
iÔ¨Åcation_
size_co-
changed_Ô¨ÅlesThe number of lines
changed in the co-
changed Ô¨ÅlesThe larger the changes
are to the co-changed
Ô¨Åles, the larger the
chance of introducing a
defect.Larger changes have a higher risk of intro-
ducing a defect [22]. We apply this factor
to a Ô¨Åle‚Äôs co-changing Ô¨Åles.
(recent)
num_changes_
co-
changed_Ô¨ÅlesNumber of changes
to the co-changed
Ô¨ÅlesThe higher the number
of changes to the co-
changed Ô¨Åle, the higher
the chance of introduc-
ing defects.The number of prior modiÔ¨Åcations to a Ô¨Åle
is a good predictor of its future defects [1,
11, 18]. We apply this factor to a Ô¨Åle‚Äôs co-
changing Ô¨Åles.
(recent)
pre_defects_
co-
changed_Ô¨ÅlesNumber of pre-
release defects in
co-changed Ô¨ÅlesThe higher the number
of pre-release defects in
the co-changed Ô¨Åles, the
higher the chance of a
defect.Prior defects are a good indicator of future
defects [33]. We apply this factor to a Ô¨Åle‚Äôs
co-changing Ô¨Åles.
Time Factorslatest_change_
be-
fore_releaseThe time from the
latest change to the
release (in days)Changes made close to
the release date do not
have time to get tested
properly.More recent changes contribute more de-
fects than older changes [11].
ageThe age of the Ô¨Åle
(from Ô¨Årst change
until the release GA
date)The older the Ô¨Åle, the
harder it becomes to
change and maintain.Code becomes harder to change over
time [8].
Table 3: Confusion Matrix
True class
ClassiÔ¨Åed as Breakage No Breakage
Breakage TP FP
No Breakage FN TN
A precision value of 100% would indicate that every Ô¨Åle we
classify as having a breakage/surprise defect, actually has a break-
age/surprise defect. A recall value of 100% would indicate that
every Ô¨Åle that actually has a breakage was classiÔ¨Åed as having a
breakage/surprise.
We employ 10-fold cross-validation [7]. The data set is divided
into two parts, a testing data set that contains 10% of the originaldata set and a training data set that contains 90% of the original data
set. The model is trained using the training data and its accuracy is
tested using the testing data. We repeat the 10-fold cross validation
10 times by randomly changing the fold. We report the average of
the 10 runs.
Determining the logistic regression model threshold value: The
output of a logistic regression model is a probability (between 0
and 1) of the likelihood that a Ô¨Åle belongs to the true class (e.g., a
Ô¨Åle is buggy). Then, it is up to the user of the output of the logis-
tic regression model to determine a threshold at which she/he will
consider a Ô¨Åle as belonging to the true class. Generally speaking, a
threshold of 0.5 is used. For example, if a Ô¨Åle has a likelihood of
0.5 or higher, then it is considered buggy, otherwise it is not.
However, the threshold is different for different data sets and the
value of the threshold affects the precision and recall values of theprediction models. In this paper, we determine the threshold for
each model using an approach that examines the tradeoff between
type I and type II errors [22]. Type I errors are Ô¨Åles that are identi-
Ô¨Åed as belonging to the true class, while they are not. Having a low
logistic regression threshold (e.g., 0.01) increases type I errors: a
higher fraction of identiÔ¨Åed Ô¨Åles will not belong to the true class.
A high type I error leads to a waste of resources since many non-
faulty Ô¨Åles may be wrongly classiÔ¨Åed. On the other hand, type II
error is the fraction of Ô¨Åles in the true class that are not identiÔ¨Åed
as being true when they should be. Having a high threshold can
lead to large type II errors, and thus missing many Ô¨Åles that may be
defective.
To determine the optimal threshold for our models, we perform a
cost-beneÔ¨Åt analysis between the type I and type II errors. Similar
to previous work [22], we vary the threshold value between 0 to 1
and use the threshold where the type I and type II errors are equal.
4.4 Measuring the Effect of Factors on the Pre-
dicted Probability
In addition to evaluating the accuracy and explanatory power of
our prediction models, we need to understand the effect of a factor
on the likelihood of Ô¨Ånding a breakage or surprise defect. Quanti-
fying this effect helps practitioners gain an in-depth understanding
of how the various factors relate to breakage and surprise defects.
To quantify this effect, we set all of the factors to their median
value and record the predicted probabilities, which we call the Stan-
dard Median Model (SMM). Then, to measure the individual effect
of each factor, we set all of the factors to their median value, ex-
cept for the factor whose effect we want to measure. We double the
median value of that factor and re-calculate the predicted values,
which we call the Doubled Median Model (DMM).
We then subtract the predicted probability of the SMM from the
predicted output of the DMM and divide by the predicted probabil-
ity of the SMM. Doing so provides us with a way to quantify the
effect a factor has on the likelihood of a Ô¨Åle containing a breakage
or surprise defect.
The effect of a factor can be positive or negative. A positive ef-
fect means that a higher value of the factor increases the likelihood,
whereas a negative effect value means that a higher value of the fac-
tor decreases the likelihood of a Ô¨Åle containing a breakage/surprise
defect.
5. CASE STUDY RESULTS
In this section, we address the research questions posted earlier.
First, we examine the accuracy (in terms of predictive and explana-
tory power) of our prediction models. Then, we examine the contri-
bution of each factor on the models in terms of explanatory power.
Lastly, we examine the effect of the factors on the breakage- and
surprise-proneness.
RQ1. Can we effectively predict which Ô¨Åles will
have breakage/surprise defects?
Using the extracted factors, we build logistic regression models
that aim to predict whether or not a Ô¨Åle will have a breakage or
surprise defect. The prediction was performed for Ô¨Åve different
releases of the large commercial software system. To measure pre-
dictive power, we present the precision, recall and the threshold
(Th.) value used in the logistic regression model, for each release.
The last row in the tables presents the average across all releases.
Predictive power: Tables 4 and 5 show the results of the breakage
and surprise defect prediction models, respectively. On average,
the precision for breakage and surprise defects is low, i.e., 4.7% for
breakages and 6.7% for surprise defects.Table 4: Performance of Breakage Prediction Models
Predictive Power Explanatory Power
Release Precision Recall Th. (pred) Deviance Explained
R1.1 3.7 70.4 0.45 17.48%
R2.1 4.8 69.2 0.48 7.99%
R3 4.8 69.6 0.49 14.49%
R4 3.9 73.1 0.48 11.68%
R4.1 6.5 73.8 0.49 13.98%
Average 4.7 71.2 0.48 13.12%
Table 5: Performance of Surprise Prediction Models
Predictive Power Explanatory Power
Release Precision Recall Th.
(pred)Th.
(sup)Deviance Explained
R1.1 - - - - -
R2.1 3.9 71.6 0.55 2.4 4.39 %
R3 7.3 69.0 0.44 1.7 10.79 %
R4 9.8 77.7 0.39 1.6 30.88 %
R4.1 5.7 78.0 0.38 1.5 25.27 %
Average 6.7 74.1 0.44 1.8 17.83 %
It is important to note that the low precision value is due to the
low percentage of breakage and surprise defects in the data set (as
shown earlier in Table 1). As noted by Menzies et al. [21], in cases
where the number of instances of an occurrence is so low (i.e., 2%),
achieving a high precision is extremely difÔ¨Åcult, yet not that impor-
tant. In fact, a random prediction would be correct 2.0% of the time,
on average, whereas our prediction model more than doubles that
precision for breakages and more than triples that for surprise de-
fects. The more important measure of the prediction model‚Äôs per-
formance is recall [21], which, on average is 71.2% for breakage
defects and 74.1% for surprise defects.
Explanatory power: The explanatory power of the models ranges
between 8 - 17.5% (average of 13.1%) for breakage defects and
between 4.4 - 30.9% (average of 17.8%) for surprise defects. The
values that we achieve here are comparable to those achieved in
previous work predicting post-release defects [4,30].
The explanatory power may be improved if more (or better) fac-
tors are used in the prediction model. We view the factors used in
our study as a starting point for breakage and surprise defect pre-
diction and plan to (and encourage others to) further investigate in
order to improve the explanatory power of these models.
Other considerations: For each prediction model, we explicitly
report the threshold for the logistic regression models as Th. (pred)
in Tables 4 and 5. In addition, the surprise threshold, which we
use to identify Ô¨Åles that had surprise defects, is given under the Th.
(sup) column in Table 5.
Since the number of pre-release defects is used in the dependent
variable of the surprise model ( Surprise (file) =post defects
pre defects),
we did not use it as part of the independent variables in the pre-
diction model. This makes our results even more signiÔ¨Åcant, since
previous work [23,35] showed that pre-release defects traditionally
are the largest contributor to post-release defect prediction models.

Our prediction models predict breakage and surprise de-
fects with a precision that is at least double that of a random
prediction and a recall above 71%.
RQ2. Which factors are important for the break-
age/surprise defect prediction models?
In addition to knowing the predictive and explanatory power of our
models, we would like to know which factors contribute the most to
these predictions. To answer this question, we perform an ANOV A
analysis to determine the contribution of the three factor categories.
We summarize the Ô¨Åndings in Tables 6 and 7 as follows:
Traditional Defect Prediction Factors: are major contributors for
the breakage defect prediction models, however, they only
have a small contribution in predicting surprise defects.
Co-Change Factors provide a small contribution to breakage de-
fect prediction models, however, they make a major contri-
bution to predicting surprise defects.
Time Factors provide a minor contribution to breakage defect pre-
diction models, however, they make a major contribution in
predicting surprise defects.
Although there are exceptions to the above mentioned observa-
tions (e.g., traditional defect prediction factors make a major con-
tribution to the surprise defect model in R2.1), our observations are
based on the trends observed in the majority of the releases.
As mentioned earlier in Section 3.4, breakage Ô¨Åles were mainly
involved with defect Ô¨Åxing efforts and are, by deÔ¨Ånition, defects
that are found in the Ô¨Åeld. Perhaps these characteristics of breakage
defects help explain why the traditional post-release defect predic-
tion factors perform well in predicting breakages.
Furthermore, earlier observations in Section 3.4 showed that sur-
prise defect Ô¨Åles were worked on later than other Ô¨Åles (i.e., MRs
that touched surprise defect Ô¨Åles were started later than other MRs).
Our Ô¨Åndings show that being changed close to a release is one of the
best indicators of whether or not a Ô¨Åle will have a surprise defect.


Traditional defect prediction factors are good indicators of
breakage defects. The factors related to co-changed Ô¨Åles and
time-related factors are good indicators of surprise defects.
RQ3. What effect does each factor have on the
likelihood of Ô¨Ånding a breakage/surprise defect
in a Ô¨Åle?
Thus far we examined the prediction accuracy and the importance
of the factors to these prediction models. Now, we study the effect
of each factor on the likelihood of Ô¨Ånding a breakage or surprise
defect in a Ô¨Åle. In addition to measuring the effect, we also con-
sider stability and explanatory impact. If an effect has the same
sign/direction for all releases (i.e., positive or negative effect in all
releases), then we label it as highly stable. If the effect of a factor
has the same sign in all releases except for one, then we label it as
being mainly stable. A factor having a different sign in more than
two of the Ô¨Åve releases is labeled as being unstable. The explana-
tory impact column is derived from the values in Tables 6 and 7. If
a factor belongs to a category that had a strong explanatory power,
then we label it as having high impact, otherwise we consider it as
having low impact.We use the stability and impact measure to help explain the strength
of our Ô¨Åndings. For example, if we Ô¨Ånd that a factor has a positive
effect and has high impact to the explanatory power of the model,
then we believe this effect to be strong.
Breakage Defects: Table 8 shows the effect of the different fac-
tors on the likelihood of predicting a breakage defect in a Ô¨Åle. We
observe that in all releases, pre-release defects have a positive ef-
fect on the likelihood of a breakage defect existing in a Ô¨Åle (i.e.,
highly stable). In addition, Table 6 showed that the traditional
defect prediction factors contributed the most to the explanatory
power of the model (i.e., high impact). File size and the number of
co-changed Ô¨Åles generally have a positive and negative effect, re-
spectively. However, the impact of the number of co-changed Ô¨Åles
and recent modiÔ¨Åcation size co-changed Ô¨Åles is low (as shown in
Table 6). Similarly, the latest change before release factor had low
impact.
As stated earlier, our manual examination of the breakage Ô¨Åles
showed that breakage Ô¨Åles were especially involved with Ô¨Åxing ef-
forts. Therefore, the fact that pre-release defects and size have a
strong positive effect was expected (since these factors are posi-
tively correlated with post-release defects). In fact, we found that
the average Ô¨Åle size of breakage Ô¨Åles is 50% larger than non-breakage
Ô¨Åles. The rest of the factors had low impact on the explanatory
power of the prediction model, therefore we cannot conclude any
meaningful results from their effect.
Release 4 (R4) in our results seems to be an outlier. For example,
contrary to the other releases, Ô¨Åle size shows a negative effect in
R4. After closer examination, we found that R4 was a large major
release that added a large amount of new functionality. This could
be the reason why the effect values for R4 are so different from the
effect results of the remaining releases.
Surprise Defects: Table 9 shows the effect values for the surprise
defect prediction model. In this model, Ô¨Åle size has a large stable
positive effect, however as shown earlier in Table 7 this factor cat-
egory has very little contribution to the explanatory power of the
model (i.e., low impact).
The number of co-changed Ô¨Åles has a negative effect that is highly
stable and highly impacting. This is counter-intuitive, especially
since manual inspection showed that MRs that contained surprise
Ô¨Åles are larger than average. After closer examination, we found
that although surprise Ô¨Åles were part of large MRs, the number of
surprise Ô¨Åles in those MRs was low. In fact, the percentage of sur-
prise Ô¨Åles in those large MRs was 16.5% (15% in R2.1 and 17% in
the remaining 3 releases). To sum up, we Ô¨Ånd that surprise defect
Ô¨Åles exist in large MRs, however these surprise Ô¨Åles make up only
a small portion of those MRs.
We Ô¨Ånd that the recent modiÔ¨Åcation size of co-changed Ô¨Åles has a
negative effect that is mainly stable and highly impacting. Further-
more, as expected, making a change last minute increases the like-
lihood of a surprise defect (i.e., the negative effect of latest change
before release). As shown in Section 3.4, in contrast to breakages,
surprise defect Ô¨Åles were worked on later than usual. We conjecture
that starting late means less time for testing, hence the much higher
effect of these late changes on surprise defect Ô¨Åles compared to the
breakage Ô¨Åles.


Pre-release defects and Ô¨Åle size have a positive effect on
the likelihood of a Ô¨Åle containing a breakage defect. The
number of co-changed Ô¨Åles, the modiÔ¨Åcation size of recently
co-changed Ô¨Åles and the time since last change before re-
lease have a negative effect on the likelihood of a Ô¨Åle having
a surprise defect.Table 6: Contribution of Factor Categories to the Explanatory Power of Breakage Prediction Models
R1.1 R2.1 R3 R4 R4.1
Traditional Defect Prediction Factors 15.60% 6.80% 11.16% 8.92% 10.75%
Co-Change Factors 1.47% 1.09% 2.93% 0.62% 3.22%
Time Factors 0.39% 0.09% 0.39% 2.12% 0.00%
Overall Deviance Explained 17.47% 7.98% 14.48% 11.67% 13.98%
Table 7: Contribution of Factor Categories to the Explanatory Power of Surprise Prediction Models
R1.1 R2.1 R3 R4 R4.1
Traditional Defect Prediction Factors -% 2.94% 2.76% 0.68% 0.69%
Co-Change Factors -% 1.11% 4.56% 9.80% 14.46%
Time Factors -% 0.33% 3.46% 20.38% 10.11%
Overall Deviance Explained -% 4.39% 10.79% 30.87% 25.27%
Table 8: Effect of factors on the likelihood of predicting a Ô¨Åle with a breakage defect. Effect is measured by setting a factor to double
its median value (1 if the median is 0), while the rest of the factors are set to their median value.
R1.1 R2.1 R3 R4 R4.1 Stability Explanatory Impact
pre_defects 136% 106% 85% 121% 154% Highly Stable High Impact
Ô¨Åle_size 157% 68% 575% -47% 39% Mainly Stable High Impact
num_co_changed_Ô¨Åles_per_mr -76% -79% -91% 629% -89% Mainly Stable Low Impact
recent_modiÔ¨Åcation_size_co_changed_Ô¨Åles 3% 2% 13% -4% 3% Mainly Stable Low Impact
latest_change_before_release -47% 55% 85% -88% 5% Not Stable Low Impact
Table 9: Effect of factors on the likelihood of predicting a Ô¨Åle with a surprise defect. Effect is measured by setting a factor to double
its median value (1 if the median is 0), while the rest of the factors are set to their median value.
R1.1 R2.1 R3 R4 R4.1 Stability Explanatory Impact
Ô¨Åle_size - 1113% 260% 184% 126% Highly Stable Low Impact
num_co_changed_Ô¨Åles_per_mr - -57% -75% -71% -85% Highly Stable High Impact
recent_modiÔ¨Åcation_size_co_changed_Ô¨Åles - 6% -3% -28% -19% Mainly Stable High Impact
latest_change_before_release - -50% -65% -97% -92% Highly Stable High Impact
6. EFFORT SA VINGS BY FOCUSING ON SUR-
PRISE AND BREAKAGE FILES
Thus far, we have shown that we are able to build prediction
models to predict Ô¨Åles that contain breakage and surprise defects.
However, one question still lingers: what if we used the traditional
post-release defect prediction model to predict breakage and sur-
prise defects? Is it really worth the effort to build these specialized
models?
To investigate whether building specialized prediction models
for breakage and surprise defects is beneÔ¨Åcial, we use defect pre-
diction models that are trained to predict post-release defects, to
predict Ô¨Åles with breakages and surprise defects. Due to the fact
that post-release defects are much more common than breakages or
surprise defects, post-release defect models are more likely to say
that most Ô¨Åles have breakages or surprise defects. That will lead to
a large amount of unnecessary work. Therefore, we use the number
of false negatives to compare the performance of the post-release
models and the specialized models. To make a meaningful compar-
ison of effort (which is related to Type I error), we Ô¨Åx Type II error
to be the same in both models.
Breakage Defects. Table 10 shows the results of the specialized
prediction model (Breakage -> Breakage) and the post-release pre-
diction model (Post -> Breakage) for release 4.1. Both of theseTable 10: Breakages in Release 4.1
Breakage -> Breakage Post -> Breakage
Predicted Predicted
Actual 0 1 Actual 0 1
0 1093 320 0 875 538
1 7 26 1 7 26
models predict Ô¨Åles that have breakage defects. The false positives
are highlighted (in grey) in Table 10. We observe that the special-
ized prediction model has approximately 41% (i.e.,538 320
538) less
false positives than the post-release model. This means that using
the specialized model would reduce the inspection effort of Ô¨Åles by
41%. We also convert this effort saving into LOC, which is approx-
imately 42% of the total LOC.
Surprise Defects. Table 11 shows the results of the specialized
model (Surprise -> Surprise) and the post-release prediction model
(Post -> Surprise) for Ô¨Åles that have surprise defects. In this case,
the specialized models lead to approximately 55% (i.e.,525 235
525)
effort savings (i.e., less false positives). Comparing the savings in
terms of LOC, we Ô¨Ånd that using the specialized prediction modelTable 11: Surprise in R4.1
Surprise -> Surprise Post -> Surprise
Predicted Predicted
Actual 0 1 Actual 0 1
0 1193 235 0 903 525
1 4 14 1 4 14
leads to approximately 50% effort savings compared to using a tra-
ditional post-release defect prediction model. This is a consider-
able amount of effort savings and shows the beneÔ¨Åts of building a
specialized prediction model of Ô¨Åles with surprise defects.


Using our custom prediction models reduces the amount of
Ô¨Åles inspected by practitioners by 41% for breakages and
55% for surprise defects.
7. LIMITATIONS
Threats to Construct Validity consider the relationship between
theory and observation, in case the measured variables do not mea-
sure the actual factors.
Breakage MRs were manually identiÔ¨Åed in our data by project
experts. Although this manual linking was done by these project
experts, some MRs may have been missed or incorrectly linked.
We used Ô¨Åles in defects reported within one month after release
to determine the surprise defect threshold. The assumption here is
that defects reported within one month involve important function-
ality that is widely used after release. Defects that affect important
functionality may be reported later than one month, however.
Threats to External Validity consider the generalization of our
Ô¨Åndings. The studied project was a commercial project written
mainly in C/C++, therefore, our results may not generalize to other
commercial or open source projects.
8. LESSONS LEARNED AND FUTURE WORK
After performing our study, we asked the opinions of the highly
experienced quality manager in the project about the prediction re-
sults. The manager has a theory about the reported effect of our
last_change _before _release factor. The theory is that the so
called ‚Äúlate Ô¨Åx frenzies‚Äù that go on in organizations to bring down
the number of open defects in a software system before the release,
might have compromised the quality of inspections and other qual-
ity assurance activities. This suggests that prediction may help to
quantify and conÔ¨Årm intuition about the relationships between the
aspects of the development process and the high-impact defects.
However, when the quality manager considered the merits of our
prediction by their utility for system veriÔ¨Åcation she argued that
identifying locations of defects is of limited use to system testers
because they test system features or behaviors, not individual Ô¨Åles.
In addition, she doubted that the predicted location could be helpful
even to developers doing inspections or unit tests without the addi-
tional information about the nature of the problem or how it should
be Ô¨Åxed.
Despite the positive Ô¨Åndings related to prediction quality, nar-
rowing the scope, and effort savings, we still appear to be far from
the state where the prediction results could be used in practice.
Based on our quantitative and qualitative Ô¨Åndings and experience
we hypothesize that for defect prediction to become a practical tool,
each predicted location has to also contain a clear suggestion on
why the defect might be there and how it may be Ô¨Åxed.To achieve this, we propose a procedure similar to the one we
conducted to identify surprise defects, to classify defects into a va-
riety of classes according to their nature, the ways they may have
been introduced, and to the ways they may need to be Ô¨Åxed. We
expect that each type of high-impact defect would have a different
prediction signature, which, in turn, can be used to provide devel-
opers with a recommendation on where the defect may be, what
nature it may have, and how it may be Ô¨Åxed. We can see an ex-
ample of such a classiÔ¨Åcation in the static analysis tools that not
only provide a warning, but also give a reason why a particular pat-
tern might be a defect and a clear suggestion on how the potential
defect can be Ô¨Åxed. We are not aware of any similar patterns for
defect prediction.
For example, our investigation of surprise defects could be used
to provide a warning of the kind: ‚ÄúThis Ô¨Åle might contain a defect
that has been introduced a while ago, perhaps because of incor-
rect requirements. Change patterns to this Ô¨Åle suggest that the us-
age proÔ¨Åle might have changed recently and the requirements may
need to be reviewed to make sure they are accurate and complete.‚Äù
Obviously, a more extensive investigation may be needed to pro-
vide more speciÔ¨Åc recommendations and we believe that the defect
prediction methods should be tailored not simply to predict defect
locations, but, like basic static analysis tools such as lint , should
also detect patterns of changes that are suggestive of a particular
type of defect, and recommend appropriate remedies.
9. CONCLUSION
The majority of defect prediction work focuses on predicting
post-release defects, yet the adoption of this work in practice re-
mains relatively low [10, 13]. One of the main reasons for this is
that defect prediction techniques generally identify too many Ô¨Åles
as having post-release defects, requiring signiÔ¨Åcant inspection ef-
fort from developers.
Instead of considering all defects as equal, this paper focuses on
predicting a small subset of defects that are highly impacting. Since
there are many different interpretations of ‚Äúhigh impact‚Äù, we focus
on one interpretation from the perspective of customers (breakage
defects) and one from the perspective of practitioners (surprise de-
fects). We Ô¨Ånd that:
Both kinds of defects are different and that surprise defects,
similar to the more established concept of breakage defects,
have a high impact.
Specialized defect prediction models can predict breakage
and surprise defects effectively, yielding sizeable effort sav-
ings over using simple post-release defect prediction models.
Traditional defect prediction factors (i.e., the number of pre-
release defects and Ô¨Åle size) are good predictors of breakage
defects, whereas the number of co-changed Ô¨Åles, the size of
recently co-changed Ô¨Åles and the time since the last change
are good predictors of surprise defects.
Our Ô¨Åndings suggest that building specialized prediction models
is valuable to bring defect prediction techniques closer to adoption
in practice.
However, our qualitative analysis of surprise defects and the feed-
back from the quality assurance manager clearly indicate that fur-
ther work is needed to develop defect prediction into a practical
tool. In particular, we found support for the idea of building spe-
cialized models that identify not only a defect‚Äôs location, but also
its nature, thus greatly simplifying the process of determining what
the defect is and how it needs to be Ô¨Åxed.10. REFERENCES
[1]E. Arisholm and L. C. Briand. Predicting fault-prone
components in a java legacy system. In Proc. Int‚Äôl
Symposium on Empirical Softw. Eng. (ISESE‚Äô06) , pages
8‚Äì17, 2006.
[2]E. Arisholm, L. C. Briand, and E. B. Johannessen. A
systematic and comprehensive investigation of methods to
build and evaluate fault prediction models. J. Syst. Softw. ,
83(1):2‚Äì17, 2010.
[3]V. R. Basili, L. C. Briand, and W. L. Melo. A validation of
object-oriented design metrics as quality indicators. IEEE
Trans. Softw. Eng. , 22(10):751‚Äì761, 1996.
[4]M. Cataldo, A. Mockus, J. A. Roberts, and J. D. Herbsleb.
Software dependencies, work dependencies, and their impact
on failures. IEEE Trans. Softw. Eng. , 99(6):864‚Äì878, 2009.
[5]S. R. Chidamber and C. F. Kemerer. A metrics suite for
object oriented design. IEEE Trans. Softw. Eng. ,
20(6):476‚Äì493, 1994.
[6]M. D‚ÄôAmbros, M. Lanza, and R. Robbes. An extensive
comparison of bug prediction approaches. In Proc. Int‚Äôl
Working Conf. on Mining Software Repositories (MSR‚Äô10) ,
pages 31‚Äì41, 2010.
[7]B. Efron. Estimating the error rate of a prediction rule:
Improvement on Cross-Validation. Journal of the American
Statistical Association , 78(382):316‚Äì331, 1983.
[8]S. G. Eick, T. L. Graves, A. F. Karr, J. S. Marron, and
A. Mockus. Does code decay? assessing the evidence from
change management data. IEEE Trans. Softw. Eng. , 27:1‚Äì12,
2001.
[9]H. Gall, K. Hajek, and M. Jazayeri. Detection of logical
coupling based on product release history. In Proc. Int‚Äôl
Conf. on Software Maintenance (ICSM‚Äô98) , pages 190‚Äì198,
1998.
[10] M. W. Godfrey, A. E. Hassan, J. Herbsleb, G. C. Murphy,
M. Robillard, P. Devanbu, A. Mockus, D. E. Perry, and
D. Notkin. Future of mining software archives: A
roundtable. IEEE Software , 26(1):67 ‚Äì70, 2009.
[11] T. L. Graves, A. F. Karr, J. S. Marron, and H. Siy. Predicting
fault incidence using software change history. IEEE Trans.
Softw. Eng. , 26(7):653‚Äì661, 2000.
[12] T. Gyimothy, R. Ferenc, and I. Siket. Empirical validation of
object-oriented metrics on open source software for fault
prediction. IEEE Trans. Softw. Eng. , 31(10):897‚Äì910, 2005.
[13] A. Hassan. The road ahead for mining software repositories.
InFrontiers of Software Maintenance (FoSM‚Äô08) , pages
48‚Äì57, 2008.
[14] A. E. Hassan. Predicting faults using the complexity of code
changes. In Proc. Int‚Äôl Conf. on Softw. Eng. (ICSE‚Äô09) , pages
78‚Äì88, 2009.
[15] I. Herraiz, J. M. Gonzalez-Barahona, and G. Robles.
Towards a theoretical model for software growth. In Proc.
Int‚Äôl Workshop on Mining Software Repositories (MSR‚Äô07) ,
pages 21‚Äì29, 2007.
[16] Y. Kamei, S. Matsumoto, A. Monden, K. Matsumoto,
B. Adams, and A. E. Hassan. Revisiting common bug
prediction Ô¨Åndings using effort aware models. In Proc. Int‚Äôl
Conf. on Software Maintenance (ICSM‚Äô10) , pages 1‚Äì10.
[17] T. M. Khoshgoftaar, E. B. Allen, W. D. Jones, and J. P.
Hudepohl. Data mining for predictors of software quality.
International Journal of Software Engineering and
Knowledge Engineering , 9(5):547‚Äì564, 1999.
[18] M. Leszak, D. E. Perry, and D. Stoll. ClassiÔ¨Åcation andevaluation of defects in a project retrospective. J. Syst. Softw. ,
61(3), 2002.
[19] T. J. McCabe. A complexity measure. In Proc. Int‚Äôl Conf. on
Softw. Eng. (ICSE‚Äô76) , page 407, 1976.
[20] T. Mende and R. Koschke. Revisiting the evaluation of defect
prediction models. In Proc. Int‚Äôl Conf. on Predictor Models
in Software Engineering (PROMISE‚Äô09) , pages 7:1‚Äì7:10,
2009.
[21] T. Menzies, A. Dekhtyar, J. Distefano, and J. Greenwald.
Problems with precision: A response to "comments on ‚Äôdata
mining static code attributes to learn defect predictors‚Äô".
IEEE Trans. Softw. Eng. , 33:637‚Äì640, September 2007.
[22] A. Mockus and D. M. Weiss. Predicting risk of software
changes. Bell Labs Technical Journal , 5(2):169‚Äì180, 2000.
[23] R. Moser, W. Pedrycz, and G. Succi. A comparative analysis
of the efÔ¨Åciency of change metrics and static code attributes
for defect prediction. In Proc. Int‚Äôl Conf. on Softw. Eng.
(ICSE‚Äô08) , pages 181‚Äì190, 2008.
[24] N. Nagappan and T. Ball. Use of relative code churn
measures to predict system defect density. In Proc. Int‚Äôl
Conf. on Softw. Eng. (ICSE‚Äô05) , pages 284‚Äì292, 2005.
[25] N. Nagappan, T. Ball, and A. Zeller. Mining metrics to
predict component failures. In Proc. Int‚Äôl Conf. on Softw.
Eng. (ICSE‚Äô06) , pages 452‚Äì461, 2006.
[26] N. Ohlsson and H. Alberg. Predicting fault-prone software
modules in telephone switches. IEEE Trans. Softw. Eng. ,
22(12):886‚Äì894, 1996.
[27] T. J. Ostrand, E. J. Weyuker, and R. M. Bell. Where the bugs
are. In Proc. Int‚Äôl Symposium on Software Testing and
Analysis (ISSTA‚Äô04) .
[28] T. J. Ostrand, E. J. Weyuker, and R. M. Bell. Predicting the
location and number of faults in large software systems.
IEEE Trans. Softw. Eng. , 31(4):340‚Äì355, 2005.
[29] E. Shihab. Pragmatic prioritization of software quality
assurance efforts. In Proceeding of the 33rd international
conference on Software engineering , ICSE ‚Äô11, pages
1106‚Äì1109, 2011.
[30] E. Shihab, Z. M. Jiang, W. M. Ibrahim, B. Adams, and A. E.
Hassan. Understanding the impact of code and process
metrics on post-release defects: A case study on the Eclipse
project. In Proc. Int‚Äôl Symposium on Empirical Softw. Eng.
and Measurement (ESEM‚Äô10) , pages 1‚Äì10, 2010.
[31] Y. Shin, A. Meneely, L. Williams, and J. Osborne.
Evaluating complexity, code churn, and developer activity
metrics as indicators of software vulnerabilities. IEEE Trans.
Softw. Eng. , PP(99):1, 2010.
[32] R. Subramanyam and M. S. Krishnan. Empirical analysis of
ck metrics for object-oriented design complexity:
Implications for software defects. IEEE Trans. Softw. Eng. ,
29(4):297‚Äì310, 2003.
[33] T.-J. Yu, V. Y. Shen, and H. E. Dunsmore. An analysis of
several software defect models. IEEE Trans. Softw. Eng. ,
14(9), 1988.
[34] T. Zimmermann, N. Nagappan, and L. Williams. Searching
for a needle in a haystack: Predicting security vulnerabilities
for windows vista. In Proc. Int‚Äôl Conf. on Software Testing,
VeriÔ¨Åcation and Validation (ICST‚Äô10) , pages 421‚Äì428, 2010.
[35] T. Zimmermann, R. Premraj, and A. Zeller. Predicting
defects for Eclipse. In Proc. Int‚Äôl Workshop on Predictor
Models in Software Engineering (PROMISE‚Äô07) , pages 1‚Äì7,
2007.