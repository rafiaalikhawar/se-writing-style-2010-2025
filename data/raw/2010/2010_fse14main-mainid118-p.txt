How Should We Measure Functional Sameness
from Program Source Code?
â€“ An Exploratory Study on Java Methods â€“
Y oshiki Higo
Osaka University
1-5 Y amadaoka, Suita, Osaka, Japan
higo@ist.osaka-u.ac.jpShinji Kusumoto
Osaka University
1-5 Y amadaoka, Suita, Osaka, Japan
kusumoto@ist.osaka-u.ac.jp
ABSTRACT
Program source code is one of the main targets of software engi-
neering research. A wide variety of research has been conducted
on source code, and many studies have leveraged structural, vo-
cabulary, and method signature similarities to measure the func-
tional sameness of source code. In this research, we conducted
an empirical study to ascertain how we should use three similari-
ties to measure functional sameness. We used two large datasets
and measured the three similarities between all the method pairs in
the datasets, each of which included approximately 15 million Java
method pairs. The relationships between the three similarities were
analyzed to determine how we should use each to detect function-
ally similar code. The results of our study revealed the following.
(1) Method names are not always useful for detecting functionally
similar code. Only if there are a small number of methods having a
given name, the methods are likely to include functionally similar
code. (2) Existing ï¬le-level, method-level, and block-level clone
detection techniques often miss functionally similar code generated
by copy-and-paste operations between different projects. (3) In the
cases we use structural similarity for detecting functionally similar
code, we obtained many false positives. However, we can avoid
detecting most false positives by using a vocabulary similarity in
addition to a structural one. (4) Using a vocabulary similarity to
detect functionally similar code is not suitable for method pairs in
the same ï¬le because such method pairs use many of the same pro-
gram elements such as private methods or private ï¬elds.
Categories and Subject Descriptors
D.2.2 [ Design Tools and Techniques ]: Computer-aided software
engineering; D.2.7 [ Distribution, Maintenance, and Enhance-
ment ]: Restructuring, reverse engineering, and reengineering
General Terms
Experimentation, Measurement
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proï¬t or commercial advantage and that copies
bear this notice and the full citation on the ï¬rst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciï¬c
permission and/or a fee.
FSE â€™14, November 16â€“22, 2014, Hong Kong, China
Copyright 2014 ACM 978-1-4503-3056-5/14/11 ...$15.00.Keywords
Functionally similar code, Clone Detection, Structural similarity,
Vocabulary similarity, Method name similarity
1. INTRODUCTION
In software engineering, program source code is one of the main
research targets. Various studies have been conducted on source
code, and these studies often utilize similarity of structure and/or
vocabulary to measure the functional sameness of source code. For
example, clone detection is a family of research studies that utilizes
the similarities of code structure such as token sequences or abstract
syntax trees [36, 38]. Detected clones are generally code instances
implementing the same functions. They are sometimes merged
as new modules [14, 15, 25]. On the other hand, keyword-based
code searching is a representative study that utilizes the similarity
of code vocabulary [18, 32]. Developers can obtain reusable code
from keyword-based code searching systems by inputting keywords
related to the function that they want.
Clone detection research assumes that, if the structures of two
code units are identical or similar to each other, their functions are
also identical or similar to each other. On the other hand, keyword-
based code searching research assumes that, if the vocabulary in a
code unit is similar to that of another unit, their functions are also
similar.
However, of course, those assumptions do not always make sense.
For example, herein, we consider two code units: one is an im-
plementation of quicksort and the other is bubblesort. Both code
units have the same function, which sorts numerical values stored
in an array in ascending/descending order. Thus, both code units
should include the same words such as â€œsortâ€ or â€œarrayâ€. In this
case, the functional sameness appears in their vocabulary but not
in their structures because the two code units implement different
algorithms.
In addition, in some program languages such as C or Java, for
statements are often used to perform an action for each element in a
given array iteratively. Thus, many code units using forstatements
have a common operation that iteratively does something. How-
ever, such an iterative operation with a forstatement is a stylized
implementation. Such stylized implementations do not necessarily
include the same words.
If a code unit is a declarative unit in programming language, the
code has its own name. For example, in Java language, classes
and methods have names. Corazza et al. reported that the vocabu-
lary appearing in the signature of a method is the most informative
one in Java language [6]. On the other hand, there are many Java
methods whose names are not at all informative such as main or
actionPerformed .The purpose of this research is to reveal how we should lever-
age the three similarities, structural similarity, vocabulary similar-
ity, and method name similarity, for measure functional sameness
from program source code. In this research, we have investigated
the relationships between functional sameness and the three sim-
ilarities. In the investigation, we selected the Java method as the
target code unit. To reduce bias in the investigation results, we
conducted the same investigation on two different datasets. We in-
vestigated the relationships for approximately 14 million method
pairs in the two datasets.
The following are the main ï¬ndings of this research.
The name of a method does not always reï¬‚ect its function.
In cases where a given name is used by a small number of
methods, the degree of the functional sameness of the meth-
ods is likely to be high. However, if there are many methods
that have the same given name, the degree of their functional
sameness is low. Checking the number of methods having a
given name is a way to learn whether the name well repre-
sents its function.
There are hash-based clone detection techniques at the ï¬le
level, method level, and block level. However, such tech-
niques often miss functionally similar code generated by copy-
and-paste operations between different projects.
If we use structural similarity to detect functionally similar
code in Java, we obtain many false positives such as con-
secutive switch-case or consecutive else-if statements. Such
consecutive instructions are dependent on Java. If we use vo-
cabulary similarity in addition to structural similarity, we can
avoid the detection of most false positives.
Method pairs in the same class share the same private ï¬elds
an private methods. As a result, such method pairs tend to
have a high vocabulary similarity. Consequently, using a
vocabulary similarity is not suitable for method pairs in the
same class.
The remainder of this paper is organized as follows: Section 2
describes the experimental design used in this study; Section 3 ex-
plains how we measure the three similarities; in Section 4, we de-
scribe how we prepared the datasets for the experiments; Section
5 shows the experimental results; and Section 6 discusses future
research based on these results; Section 7 describes some threats
to the validity of these experiments; Section 8 introduces existing
works related to our experiment; lastly, in Section 9, we conclude
this paper.
2. EXPERIMENTAL DESIGN
In this research, we investigate how the following three types of
similarity should be used in detecting functionally similar code.
Structural similarity
Vocabulary similarity
Method name similarity
Although all of the above similarities have been leveraged in ex-
isting research studies, they are not complete measures. Code pairs
regarded as similar by these measures are occasionally recognized
as false positives by humans. For example, in code clone detection,
where structural similarity is generally leveraged, code fragments
including repeated instructions tend to be detected as clones, but
they are generally regarded as false positives [3, 12, 44]. Conse-
quently, the authors made the following hypotheses.
A	
B	C	D	E	F	G	Structure Similarity	
Vocabulary 	Similarity	Method Name Similarity	Figure 1: Three types of similarities used in this research
Code pairs regarded as similar by two measures are more
likely to be recognized as similar by humans than ones re-
garded by only one measure.
Code pairs regarded as similar by all three measures are more
likely to be recognized as similar by humans than ones re-
garded as similar by two measures.
Figure 1 shows the relationships between the three similarities.
By using this ï¬gure, the probabilities that code pairs are function-
ally similar code are presented as follows.
A;B;CD;E;FG
In this research, we determine: (1) whether code pairs in each re-
gion (from AtoG) are truly functionally similar code; and, (2) the
characteristics of code pairs in each region. We conducted experi-
ments on two large sets of open source projects written in the Java
language. The details of the experimental targets are described in
Section 4.
Java is an object-oriented language, and multiple classes operate
by cooperating with one another. Each class should have its own re-
sponsibility, so that two methods in the same class tend to be more
closely related to each other than two methods in different classes.
In addition, Java has the notion of package , which includes a set
of classes that closely cooperate with each other. Thus, two meth-
ods in the same package should tend to be more closely related to
each other than two methods in different packages. To summarize
the above-mentioned assumption, the degree of similarity between
two methods depends on their distance apart in source code. In this
research, we took into account the distance between a given code
pair when investigating it. More concretely, we used four distance
categories:
Within-File (WF) two methods are in the same ï¬le,
Within-Directory (WD) two methods are in different ï¬les, but in
the same directory,
Within-Project (WP) two methods belong to different directories,
but to the same project, and
Across-Project (AP) two methods are deï¬ned in different projects.3. SIMILARITY MEASURES
In this section, we explain how we measured structural, vocabu-
lary, and method name similarities from a given pair of Java meth-
ods.
3.1 Structural Similarity
The measurement procedure for structural similarity (SS) con-
sists of the following steps, which are based on the clone detection
procedure of Nicad [37].
STEP 1 The source code of each target method is transformed into
a token sequence. In this step, all white spaces, tabs, and new
line characters are deleted.
STEP 2 All the tokens representing variable names, method names,
and type names are replaced with special tokens. The three
types of special tokens are all different from each other.
STEP 3 The longest common subsequence between the two nor-
malized token sequences is identiï¬ed.
STEP 4 A quantiï¬ed value of SSis calculated using the following
formula.
SS(TA;TB) = min(jLCS(TA;TB)j
jTAj;jLCS(TA;TB)j
jTBj)
where â€œ TAâ€ and â€œ TBâ€ are normalized token sequences ob-
tained from methods â€œ Aâ€ and â€œ Bâ€, and â€œ jTAjâ€ represents the
number of tokens included in â€œ TAâ€. â€œLCS(TA;TB)â€ shows the
longest common subsequence between â€œ TAâ€ and â€œ TBâ€.
An identiï¬ed longest common subsequence is not necessarily a
consecutive subsequence of the original sequence. In other words,
the longest common subsequence algorithm considers additional
shorter equal subsequences among two sequences as common se-
quence in addition to the longest equal consecutive subsequence.
The experiment in this paper, we used 0.7 as a threshold for de-
termining whether given two methods are structurally similar to
each other or not.
3.2 Vocabulary Similarity
We use Jaccard similarity [43] as vocabulary similarity (VS).
The steps for measuring Jaccard similarity in this research are as
follows.
STEP 1 Variable names and method names are extracted from the
source code of each method by performing syntax analysis.
STEP 2 Nouns and verbs are obtained from the extracted names
with their dictionary forms by performing camel/snake case
splitting and stemming. Note that stop words are ignored.
STEP 3 A quantiï¬ed value of VSis calculated using the following
formula.
VS(VA;VB) =jVA\VBj
jVA[VBj
where â€œ VAâ€ and â€œ VBâ€ show sets of words in two methods â€œ Aâ€
and â€œ Bâ€, respectively. â€œ jVAjâ€ is the number of words included
in â€œVAâ€.
The experiment in this paper, we used 0.7 as a threshold for de-
termining whether given two methods have vocabulary similarity
or not.3.3 Method Name Similarity
In this research, the unit of investigation is the Java method.
Each Java method has its own signature, so we need to quantify
the similarity between two given signatures. However, quantifying
signature similarity appropriately as a single value is very difï¬cult
because a signature includes multiple elements that need to be con-
sidered. For example, we need to take into account the method
name, number of parameters, type of each parameter, and name of
each parameter. In this research, instead of quantifying the simi-
larity of a whole signature, we use the simplest way to determine
whether signatures of two given methods are similar; that is, if their
method names are exactly the same, their signatures are regarded
as similar. If not, they are regarded as not being similar.
Readers may think why method name similarity is not measured
in an analogous way with vocabulary similarity . Generally, a method
name consists of a few English words. Measuring Jaccard similar-
ityfrom such a small number of words is meaningless. Conse-
quently, in this research, we chose a binary similarity for method
name similarity .
4. DATASETS
In this research, we conducted experiments on the following two
datasets in order to reduce bias due to the datasets used1.
APACHE The entire set of Java projects included in the Apache
Software Foundation2. The SVN repositories are open to
the public. In the experiment, we used a snapshot taken on
2013/Oct/31.
UCI A large set of Java software projects that includes approxi-
mately 13,000 projects and 20 million methods3. If we were
to use the entire UCI dataset, we would need to measure sim-
ilarity between 200 trillion method pairs. In this research, we
used 500 projects in the dataset, which were extracted by us-
ing the following steps.
STEP 1 13,000 projects were sorted in the order of the num-
ber of methods they included.
STEP 2 The sorted list was divided equally into 10 sections.
STEP 3 50 projects were randomly extracted from each of
the 10 sections.
TheAPACHE dataset consists of directories and ï¬les that were
checked out from SVN repositories. SVN repositories often include
branches andtagsdirectories. The former directories include ï¬les
that belong to branches, and the latter ones include ï¬les of tagged
versions. In order to exclude source ï¬les under such directories, we
used only source ï¬les under trunk directories, which are used for
storing mainstream development.
1The two datasets are open to the public on our website, http:
//sdl.ist.osaka-u.ac.jp/~higo/fse2014/
2http://www.apache.org
3http://www.ics.uci.edu/~lopes/datasets/
Table 1: Overview of Datasets
APACHE UCI
No. of projects 84 500
No. of ï¬les 66,724 60,548
No. of methods 628,219 532,556
Total LOC 11,545,556 10,073,635In addition, we expended considerable effort to eliminate test
cases from the datasets. We obtained a list of source ï¬les whose
paths included â€œtestâ€. Then, we checked every source ï¬le in the list
manually to identify whether it was a test case.
Our elimination targets were not only test cases but also source
code generated by tools. Here we used the same strategy as for
generated code. That is, ï¬rst we obtained source ï¬les whose paths
include â€œgeneratedâ€. Then, we checked each of them manually. We
also obtained a list of source ï¬les where code comments include
â€œ@generatedâ€, â€œantlrâ€, â€œjavaccâ€, â€œsableccâ€, or the names of other
compiler compilers. Then, each of them was interactively checked,
and eliminated if it was regarded as generated code.
The same data cleansing was performed on the UCI dataset be-
cause it included projects that had been checked out from SVN
repositories. Table 1 shows numerical data of the two datasets such
as the number of source ï¬les, the number of methods, and LOC.
However, some of the methods should not be targets even if they
are neither test cases nor generated code. When we make programs
using the Java language, we generally deï¬ne many small methods
such as getters and setters. Measuring the similarity between such
small methods does not make sense. In addition, Merlo et al. re-
ported that small methods tend to have similar metric values even if
their contents are different [31]. Consequently, we removed small
methods from our measurement targets. In this research, a given
method was regarded as small and ignored if it satisï¬ed either of
the following conditions.
It included 50 or fewer tokens.
It included 10 or fewer words that appeared in user-deï¬ned
identiï¬ers.
As mentioned above, method pairs were classiï¬ed into four cat-
egories based on the distance between the two methods in the pairs.
Table 2 shows the number of method pairs in each category. The
within-ï¬le category has the least number of method pairs, and the
across-project category has the largest number of method pairs.
5. INVESTIGATION RESULTS
We investigated method pairs in each region shown in Fig. 1
by browsing their source code manually. In this investigation, we
used 0.7 for the thresholds of structural similarity andvocabulary
similarity . Table 3 shows the number of method pairs in each of
the regions. If 100 or more method pairs were included in a given
region, we investigated at least 100 pairs. If fewer than 100 pairs
were included in a given region, we investigated all the pairs. In
the reminder of this section, we describe the result for each of the
regions.
In this paper, we describe only the results for the APACHE dataset
due to space limitations. However, we would like to note that we
obtained the same result from both the datasets. Some of the graphs
for the UCI dataset can be seen on our website4.
4http://sdl.ist.osaka-u.ac.jp/~higo/fse2014/
Table 2: Number of Method Pairs in Each Category
Category APACHE UCI
within-ï¬le 4,974 18,617
within-directory 13,162 24,722
within-project 559,592 147,181
across-project 14,162,007 14,723,451
total 14,739,735 14,913,9715.1 Region â€œCâ€
Method pairs in Region â€œCâ€ had the same name, but their struc-
tural similarity andvocabulary similarity were low. Manual in-
vestigation revealed that none of the selected pairs contains related
methods. Hence, it does not seem worthwhile to detect them as
functionally similar code. They quite often had highly abstract
names such as â€œ getâ€ or â€œ execute â€ or language-dependent names
such as â€œ main â€ or â€œ addActionListener â€.
In the graph for across-project in Fig. 2, many method pairs hav-
ing the same name are located near the bottom left corner. That
is, their structural similarity andvocabulary similarity are low.
On the other hand, some of the same-name method pairs are lo-
cated near the top right corner. In order to ascertain the differences
in characteristics between bottom-left method pairs and top-right
method pairs, we analyzed the relation between the abstractness of
their names, their structural similarity , and their vocabulary simi-
larity . Figure 3 shows the result. For example, in Fig. 3(a), the
left-most boxplot shows the distribution of structural similarity of
same-name method pairs where there are ï¬ve or fewer methods
having the same name. This ï¬gure shows that the lower the num-
ber of methods that have the same name, the higher their strucutral
similarity andvocabulary similarity .
Besides, Fig. 4 shows histogram representing frequency of struc-
tural and vocabulary similarities for the same-name anddifferent-
name method pairs for the category across-project . We can see that
even most the same-name method pairs have low structural and vo-
cabulary similarity. This result shows that the method name same-
ness of a given method pair does not necessarilly indicate its high
structural similarity or high vocabulary similarity .
5.2 Regions â€œAâ€ and â€œEâ€
Method pairs in Region â€œAâ€ have a high structural similarity , but
they have low vocabulary similarity and different method names.
Method pairs in Region â€œEâ€ have high structural similarity and the
same method names, but have a low vocabulary similarity .
In the category across-project , many method pairs included con-
secutive switch-case statements and consecutive if-else statements.
In Java language, such implementations are often used in cases
where we need to bifurcate a procedure into multiple branches. In
other words, the reason their structural similarity was high was that
they included language-dependent implementations. We were not
able to ï¬nd any other reason, such as that they had been created
by copy-and-paste operations. Such code (repeated instructions)
is occasionally regarded as false positives in clone detection [12].
There is even a clone detection technique that has a special function
to avoid detecting repeated instructions as clones [34].
In the categories within-project andwithin-directory , many meth-
ods had similar procedure logic for different object types. For ex-
ample, in project qpid, the following two ï¬les had methods whose
names were â€œ construct â€ (see Fig. 5):
Table 3: Number of Method Pairs in Each Region
Region APACHE UCI
A 45 161
B 149 355
C 29,591 37,047
D 229 598
E 82 80
F 98 176
G 472 1,918Figure 2: Overview of Three Types of Similarities for APACHE (Each dot represents a method pair. Each black dot is a method pair
whose names are different and each red dot is a method pair whose names are the same)
messeging/codec/PropertiesConstructor.java , and
transport/codec/AttachConstructor.java .
Their structural andvocabulary similarities were 0.83 and 0.22,
respectively. Such method pairs are latent refactoring opportuni-
ties. However, refactoring them is not an easy task because they
generally include small code fragments that are different from each
other. Complicated operations such as the Form Template Method
[7] are required to refactor them. We also found method pairs thatincluded the language-dependent repeated code mentioned in the
previous paragraph.
In the category within-ï¬le , there were only 14 method pairs whose
structural similarity was high but whose vocabulary similarity was
low. The lowest value of vocabulary similarity was 0.39. If two
methods are deï¬ned in the same class, they can use the same pri-
vate methods and private ï¬elds. That is, method pairs in the same
class tend to have a higher vocabulary similarity . Figure 6 supports
this conclusion.2--5 6--10 11--100 101--1000 1001--0 . 00 . 20 . 40 . 60 . 81 . 0(a) across-project structural similarity
2--5 6--10 11--100 101--1000 1001--0.0 0.2 0.4 0.6 0.8 1.0
(b) across-project vocabulary similarity
Figure 3: Boxplot representing relationship struc-
tural/vocabulary similarity distribution and name abstractness
for the category across-project . X-axis represents the degree of
name abstractness. There are 5 levels of name abstractness.
5.3 Regions â€œBâ€ and â€œFâ€
Method pairs in Region â€œBâ€ have a high vocabulary similarity ,
but a low structural similarity and different method names. Pairs
in Region â€œFâ€ have a high vocabulary , but low structural similarity
and the same method names.
In the category across-project , we found many cases of code
reuse between different projects. After copying and pasting a code
fragment, it was modiï¬ed extensively (in many cases, new state-
ments had been added to the pasted code). Such large modiï¬cations
lowered the structural similarity between the original code and the
structure similarityfrequency
0.0 0.2 0.4 0.6 0.8 1.00 1000 2000 3000 4000(a) structural similarity
vocabulary similarityfrequency
0.0 0.2 0.4 0.6 0.8 1.00 1000 2000 3000 4000 5000 6000
(b) vocabulary similarity
Figure 4: Histogram representing frequency of struc-
tural/vocabulary similarity for same-name/differnet-name
method pairs for the category across-project . The black bars
represent frequency of different-name method pairs and the
red bars represent same-name ones.
copied code. However, user-deï¬ned names were not changed in
such modiï¬cations. Consequently, their vocabulary similarity had
been kept high. We could not ï¬nd any different characteristics be-
tween pairs having the same names and different names.
In the categories within-project andwithin-directory , the struc-
tures of method pairs were partially similar to each other. They
should be detected by token-based or string-based clone detection
techniques such as CCFinder [20]. Some method pairs having the	 Â 53ã€€public	 Â Properties	 Â construct(Object	 Â 	 Â 	 Â 54	 Â {	 Â 	 Â 	 Â 58	 Â 	 Â 	 Â Properties	 Â obj	 Â =	 Â new	 Â Properties()	 Â 	 Â 	 Â 72	 Â 	 Â 	 Â 	 Â 	 Â obj.setMessageId(	 Â val	 Â );	 Â 	 Â 	 Â 99	 Â 	 Â 	 Â 	 Â 	 Â obj.setUserId(	 Â (Binary)	 Â val	 Â );	 Â 	 Â 126	 Â 	 Â 	 Â 	 Â 	 Â obj.setTo(	 Â (String)	 Â val	 Â );	 Â 	 Â 153	 Â 	 Â 	 Â 	 Â 	 Â obj.setSubject(	 Â (String)	 Â val	 Â );	 Â 	 Â 180	 Â 	 Â 	 Â 	 Â 	 Â obj.setReplyTo(	 Â (String)	 Â val	 Â );	 Â 	 Â 207	 Â 	 Â 	 Â 	 Â 	 Â obj.setCorrelationId(	 Â val	 Â );	 Â 	 Â 234	 Â 	 Â 	 Â 	 Â 	 Â obj.setContentType(	 Â (Symbol)	 Â va	 Â 	 Â 261	 Â 	 Â 	 Â 	 Â 	 Â obj.setContentEncoding(	 Â (Symbol	 Â 	 Â 288	 Â 	 Â 	 Â 	 Â 	 Â obj.setAbsoluteExpiryTime(	 Â (Dat	 Â 	 Â 315	 Â 	 Â 	 Â 	 Â 	 Â obj.setCreationTime(	 Â (Date)	 Â val	 Â 	 Â 	 Â 342	 Â 	 Â 	 Â 	 Â 	 Â obj.setGroupId(	 Â (String)	 Â val	 Â );	 Â 	 Â 369	 Â 	 Â 	 Â 	 Â 	 Â obj.setGroupSequence(	 Â (Unsigned	 Â 	 Â 396	 Â 	 Â 	 Â 	 Â 	 Â obj.setReplyToGroupId(	 Â (String)	 Â 	 Â 	 Â 424	 Â }	 Â 
	 Â 53	 Â public	 Â Attach	 Â construct(Object	 Â und	 Â 	 Â 54	 Â {	 Â 	 Â 	 Â 58	 Â 	 Â 	 Â Attach	 Â obj	 Â =	 Â new	 Â Attach();	 Â 	 Â 	 Â 72	 Â 	 Â 	 Â 	 Â 	 Â obj.setName(	 Â (String)	 Â val	 Â );	 Â 	 Â 	 Â 99	 Â 	 Â 	 Â 	 Â 	 Â obj.setHandle(	 Â (UnsignedIntege	 Â 	 Â 126	 Â 	 Â 	 Â 	 Â 	 Â obj.setRole(	 Â Role.valueOf(	 Â val	 Â 	 Â 	 Â 153	 Â 	 Â 	 Â 	 Â 	 Â obj.setSndSettleMode(	 Â SenderSe	 Â 	 Â 180	 Â 	 Â 	 Â 	 Â 	 Â obj.setRcvSettleMode(	 Â Receiver	 Â 	 Â 207	 Â 	 Â 	 Â 	 Â 	 Â obj.setSource(	 Â (Source)	 Â val	 Â );	 Â 	 Â 234	 Â 	 Â 	 Â 	 Â 	 Â obj.setTarget(	 Â (Target)	 Â val	 Â );	 Â 	 Â 261	 Â 	 Â 	 Â 	 Â 	 Â obj.setUnsettled(	 Â (Map)	 Â val	 Â );	 Â 	 Â 288	 Â 	 Â 	 Â 	 Â 	 Â obj.setIncompleteUnsettled(	 Â (B	 Â 	 Â 	 Â 315	 Â 	 Â 	 Â 	 Â 	 Â obj.setInitialDeliveryCount(	 Â (	 Â 	 Â 342	 Â 	 Â 	 Â 	 Â 	 Â obj.setMaxMessageSize(	 Â (Unsign	 Â 	 Â 368	 Â 	 Â 	 Â 	 Â 	 Â if	 Â (val	 Â instanceof	 Â Symbol[]	 Â )	 Â 369	 Â 	 Â 	 Â 	 Â 	 Â {	 Â 370	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â obj.setOfferedCapabilities(	 Â 	 Â 371	 Â 	 Â 	 Â 	 Â 	 Â }	 Â 	 Â 376	 Â 	 Â 	 Â 	 Â 	 Â obj.setOfferedCapabilities(	 Â ne	 Â 	 Â 402	 Â 	 Â 	 Â 	 Â 	 Â if	 Â (val	 Â instanceof	 Â Symbol[]	 Â )	 Â 403	 Â 	 Â 	 Â 	 Â 	 Â {	 Â 404	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â obj.setDesiredCapabilities(	 Â 	 Â 405	 Â 	 Â 	 Â 	 Â 	 Â }	 Â 	 Â 410	 Â 	 Â 	 Â 	 Â 	 Â obj.setDesiredCapabilities(	 Â ne	 Â 	 Â 437	 Â 	 Â 	 Â 	 Â 	 Â obj.setProperties(	 Â (Map)	 Â val	 Â )	 Â 	 Â 462	 Â }	 Â Figure 5: Method pair whose structural similarity is high
but whose vocabulary similarity is low (Different variables are
underlined. Bidirectional arrows show statement correspon-
dences. Identical statements are omitted due to space limita-
tions.)
same names are semantically the same procedure, even if their im-
plementation ways are different. Some of them were overriding the
same method in a common parent class. Some method pairs having
different names implemented opposite procedures such as â€œ uncom-
press â€ and â€œ compress â€ or implemented related procedures such as
logical AND andORoperators.
In the category within-ï¬le , methods can use the same resources
such as private methods or private ï¬elds. This is because their vo-
cabulary similarity tends to be higher. Figure 7 shows the distri-
butions of structural similarity andvocabulary similarity in each
category. This ï¬gure shows that vocabulary similarity in the cat-
egory within-ï¬le stay higher than the other categories even if their
structural similarity is not high.
5.4 Regions â€œDâ€ and â€œGâ€
In none of the categories did we ï¬nd false positives, regardless of
the methodsâ€™ name sameness. We also found that if both structural
similarity andvocabulary similarity were 1.0, their method names
were always the same.
In the category across-project , we found many examples of code
reuses between different projects. In the category within-ï¬le , the
method pairs seemed to be good opportunities for performing the
Extract Method refactoring pattern. If both the similarities of a
given method pair are 1.0, their difference exists only in data types;
for example, one is a quicksort implementation for an â€œ intâ€ array
and the other is also a quicksort implementation for a â€œ byteâ€ array.
In the categories within-project andwithin-directory , such method
pairs can be latent opportunities that similar procedures are pulled
up to common parent classes. However, methods that are exactly
Figure 6: Vocabulary similarity in each category
the same are a minority. If a given method pair includes different
statements, we need to use complicated modiï¬cations such as the
Form Template Method to treat the differences. In addition, if two
classes that include methods of a given pair do not have a common
parent class, we ï¬rst need to create it. If either of the two classes
has an explicit parent class, which means it has an â€œ extends â€ clause,
the class hierarchy must be changed to create a new common parent
class. However, creating a new class and changing an existing class
hierarchy make up a large task and may be a design-level modiï¬-
cation. Thus, we need to give careful consideration to it. In other
words, if two classes already have a common parent class, we need
less effort to refactor the method pair.
We investigated to what extent method pairs had common par-
ent classes. In this investigation, â€œ java.lang.Object â€ and the other
classes in JDK were not treated as a common parent class. Table 4
shows the result. We can see that a considerable number of method
pairs have common parent classes. The category within-directory
has a higher rate of method pairs having a common parent class
than the category within-project . Interestingly, however, if we con-
sider only method pairs whose structural similarity andvocabulary
similarity are 1.0, the category within-project has a higher rate. It
is not a hard task to pull up method pairs if they are completely the
same and have a common parent class.
6. TOWARD FUTURE RESEARCH
In this section, we discuss some directions for future research
based on the investigation results.
6.1 Pulling up Similar Methods
In the categories within-directory andwithin-project , we found
many pairs of functionally similar methods. However, most of them
included different statements from each other. To promote refactor-
ing of such method pairs, we need techniques to help refactoring.
Hotta et al. proposed a technique to identify the differences be-
tween a given pair of Java methods [15]. They leveraged program
dependence graphs to detect non-duplicated statements, which should
be kept in child classes in a case where we apply the Form Template
Method refactoring pattern to a given pair of similar methods.
Krishnan and Tsantalis proposed a technique to identify a set of
statements that should be extracted as a new method [25]. Clone50-59 60-69 70-79 80-89 90-1000.2 0.4 0.6 0.8 1.0Within-file
structure similarityvocabulary similarity
50-59 60-69 70-79 80-89 90-1000.2 0.4 0.6 0.8 1.0Within-directory
structure similarityvocabulary similarity
50-59 60-69 70-79 80-89 90-1000.0 0.2 0.4 0.6 0.8 1.0Within-project
structure similarityvocabulary similarity
50-59 60-69 70-79 80-89 90-1000.0 0.2 0.4 0.6 0.8 1.0Across-project
structure similarityvocabulary similarity
50-59 60-69 70-79 80-89 90-1000 . 00 . 20 . 40 . 60 . 81 . 0Within-file
vocabulary similaritystructure similarity
50-59 60-69 70-79 80-89 90-1000.0 0.2 0.4 0.6 0.8 1.0Within-directory
vocabulary similaritystructure similarity
50-59 60-69 70-79 80-89 90-1000.0 0.2 0.4 0.6 0.8 1.0Within-projet
vocabulary similaritystructure similarity
50-59 60-69 70-79 80-89 90-1000 . 20 . 40 . 60 . 81 . 0Across-projet
vocabulary similaritystructure similarityFigure 7: Structural-similarity-based and vocabulary-similarity-based boxplots
detection tools generally identify maximum duplications in source
code as clones. However, maximum duplications often include var-
ious differences such as different variables, different literals, or dif-
ferent statements. Their technique identiï¬es duplications that in-
clude a small number of differences. Clones detected by their tech-
nique are suited to refactoring, and developers can create easily
reusable methods from detected clones.
Hottaâ€™s technique suggests method pairs where the Form Tem-
plate Method pattern can be applied. However, humans may not
think suggested method pairs should be refactored. If removing
duplicate code is the primary goal of refactoring, it is worth sup-
porting the deletion of duplicate code as far as possible, even by
using complicated operations. On the other hand, there are many
Table 4: Number of method pairs where common parent
classes exist. The numbers in parentheses are method pairs
whose structural and vocabulary similarities are 1.
(a)APACHE
within-directory within-project
(A) all in D and G 124 257
(B) common parent 43 (5) 47 (19)
rate of (B) against (A) 0.34 (0.04) 0.18 (0.07)
(b)UCI
within-directory within-project
(A) all in D and G 443 1,375
(B) common parent 285 (53) 785 (336)
rate of (B) against (A) 0.64 (0.12) 0.57 (0.24)cases where developers do not want to change a class hierarchy or
do not want to perform complicated refactoring operations.
Krishnanâ€™s technique suggests code fragments that can be refac-
tored easily with some duplicate code remaining. His technique is
intended for the Extract Method pattern. However, the same strat-
egy should prove useful for other refactoring patterns such as Pull
Up Method . There are probably many cases where developers want
to perform simple refactorings that leave some duplicate code re-
maining, rather than complicated refactorings designed for remov-
ing all duplications. We need techniques for pulling up a chunk of
duplicated code to a parent class with a small amount of effort.
6.2 Detecting Semantic Clones
Detecting semantic clones (type-4 clones) is a challenging re-
search topic. Existing graph-based detection techniques can detect
a part of semantic clones, such as a pair of iterative procedures: one
is implemented using a forloop, and the other is implemented using
awhile loop [13, 22, 24]. However, their detection capabilities are
not adequate. Kim et al. proposed a technique that leverages states
of memory while a target program is executing [21]. This technique
can detect semantic clones that are not detected by graph-based de-
tection techniques. However, we need to prepare many test cases
to use this technique. In addition, such a dynamic analysis suffers
from scalability issues.
Within-directory andwithin-project method pairs in Region â€œFâ€
told us that using vocabulary and method name is a good way to
detect semantic clones. Such method pairs were often semantic
clones in the experiment. However, within-ï¬le method pairs in two
regions were often false positives because they tended to have a
higher vocabulary similarity (see Fig. 6).	 Â â€¦	 Â 	 Â 17	 Â package	 Â org.apache.activemq.filter;	 Â 	 Â â€¦	 Â 	 Â 31	 Â public	 Â abstract	 Â class	 Â ComparisonExpression	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â extends	 Â BinaryExpression	 Â implements	 Â BooleanExpression	 Â {	 Â 	 Â â€¦	 Â 355	 Â 	 Â 	 Â @SuppressWarnings({	 Â "rawtypes",	 Â "unchecked"	 Â })	 Â 356	 Â 	 Â 	 Â protected	 Â Boolean	 Â compare(Comparable	 Â lv,	 Â Comparable	 Â rv)	 Â {	 Â 357	 Â 	 Â 	 Â 	 Â 	 Â Class<?	 Â extends	 Â Comparable>	 Â lc	 Â =	 Â lv.getClass();	 Â 358	 Â 	 Â 	 Â 	 Â 	 Â Class<?	 Â extends	 Â Comparable>	 Â rc	 Â =	 Â rv.getClass();	 Â 359	 Â 	 Â 	 Â 	 Â 	 Â //	 Â If	 Â the	 Â the	 Â objects	 Â are	 Â not	 Â of	 Â the	 Â same	 Â type,	 Â 360	 Â 	 Â 	 Â 	 Â 	 Â //	 Â try	 Â to	 Â convert	 Â up	 Â to	 Â allow	 Â the	 Â comparison.	 Â 361	 Â 	 Â 	 Â 	 Â 	 Â if	 Â (lc	 Â !=	 Â rc)	 Â {	 Â 362	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â try	 Â {	 Â 363	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â if	 Â (lc	 Â ==	 Â Boolean.class)	 Â {	 Â 364	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â if	 Â (convertStringExpressions	 Â &&	 Â rc	 Â ==	 Â String.class)	 Â {	 Â 365	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â lv	 Â =	 Â Boolean.valueOf((String)lv).booleanValue();	 Â 366	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â }	 Â else	 Â {	 Â 367	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â return	 Â Boolean.FALSE;	 Â 368	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â }	 Â 369	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â }	 Â else	 Â if	 Â (lc	 Â ==	 Â Byte.class)	 Â {	 Â 370	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â if	 Â (rc	 Â ==	 Â Short.class)	 Â {	 Â 371	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â lv	 Â =	 Â Short.valueOf(((Number)lv).shortValue());	 Â 372	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â }	 Â else	 Â if	 Â (rc	 Â ==	 Â Integer.class)	 Â {	 Â 373	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â lv	 Â =	 Â Integer.valueOf(((Number)lv).intValue());	 Â 374	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â }	 Â else	 Â if	 Â (rc	 Â ==	 Â Long.class)	 Â {	 Â 375	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â lv	 Â =	 Â Long.valueOf(((Number)lv).longValue());	 Â 376	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â }	 Â else	 Â if	 Â (rc	 Â ==	 Â Float.class)	 Â {	 Â 377	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â lv	 Â =	 Â new	 Â Float(((Number)lv).floatValue());	 Â 378	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â }	 Â else	 Â if	 Â (rc	 Â ==	 Â Double.class)	 Â {	 Â 378	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â lv	 Â =	 Â new	 Â Double(((Number)lv).doubleValue());	 Â 379	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â }	 Â else	 Â if	 Â (convertStringExpressions	 Â &&	 Â rc	 Â ==	 Â String.class)	 Â {	 Â 380	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â rv	 Â =	 Â Byte.valueOf((String)rv);	 Â 381	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â }	 Â else	 Â {	 Â 382	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â return	 Â Boolean.FALSE;	 Â 383â€¯	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â }	 Â 	 Â â€¦	 Â (a) method â€œcompareâ€ in project â€œactivemqâ€
	 Â â€¦	 Â 	 Â 21	 Â package	 Â org.apache.qpid.filter;	 Â 	 Â â€¦	 Â 	 Â 33	 Â public	 Â abstract	 Â class	 Â ComparisonExpression	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â extends	 Â BinaryExpression	 Â implements	 Â BooleanExpression	 Â 	 Â 34	 Â {	 Â 	 Â â€¦	 Â 403	 Â 	 Â 	 Â protected	 Â Boolean	 Â compare(Comparable	 Â lv,	 Â Comparable	 Â rv)	 Â 404	 Â 	 Â 	 Â {	 Â 405	 Â 	 Â 	 Â 	 Â 	 Â Class	 Â lc	 Â =	 Â lv.getClass();	 Â 406	 Â 	 Â 	 Â 	 Â 	 Â Class	 Â rc	 Â =	 Â rv.getClass();	 Â 407	 Â 	 Â 	 Â 	 Â 	 Â //	 Â If	 Â the	 Â the	 Â objects	 Â are	 Â not	 Â of	 Â the	 Â same	 Â type,	 Â 408	 Â 	 Â 	 Â 	 Â 	 Â //	 Â try	 Â to	 Â convert	 Â up	 Â to	 Â allow	 Â the	 Â comparison.	 Â 409	 Â 	 Â 	 Â 	 Â 	 Â if	 Â (lc	 Â !=	 Â rc)	 Â 410	 Â 	 Â 	 Â 	 Â 	 Â {	 Â 411	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â if	 Â (lc	 Â ==	 Â Byte.class)	 Â 412	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â {	 Â 413	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â if	 Â (rc	 Â ==	 Â Short.class)	 Â 414	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â {	 Â 415	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â lv	 Â =	 Â ((Number)	 Â lv).shortValue();	 Â 416	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â }	 Â 417	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â else	 Â if	 Â (rc	 Â ==	 Â Integer.class)	 Â 418	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â {	 Â 419	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â lv	 Â =	 Â ((Number)	 Â lv).intValue();	 Â 420	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â }	 Â 421	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â else	 Â if	 Â (rc	 Â ==	 Â Long.class)	 Â 422	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â {	 Â 423	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â lv	 Â =	 Â ((Number)	 Â lv).longValue();	 Â 424	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â }	 Â 425	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â else	 Â if	 Â (rc	 Â ==	 Â Float.class)	 Â 426	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â {	 Â 426	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â lv	 Â =	 Â ((Number)	 Â lv).floatValue();	 Â 427	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â }	 Â 428	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â else	 Â if	 Â (rc	 Â ==	 Â Double.class)	 Â 429	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â {	 Â 430	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â lv	 Â =	 Â ((Number)	 Â lv).doubleValue();	 Â 431	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â }	 Â 432	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â else	 Â 433	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â {	 Â 434	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â return	 Â Boolean.FALSE;	 Â 435	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â }	 Â 436â€¯	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â }	 Â 	 Â â€¦	 Â 
(b) method â€œcompareâ€ in project â€œqpidâ€
Figure 8: Source Code of the Vocabulary-Similar Method Pair
whose Structural Similarity is the Lowest
6.3 Identifying Code Reuse between Different
Projects
We found many instances of code reuse between different projects
in Regions â€œBâ€ and â€œDâ€. After copying and pasting code from a dif-
ferent project, reused code was modiï¬ed. If the modiï¬cations were
small, the pair of original and reused code fell into Region â€œDâ€.
If large modiï¬cations were performed, the structural similarity de-
creased and it fell into Region â€œBâ€. There were 126 and 283 methodpairs whose vocabulary similarity was larger than 0.7 in APACHE
andUCI, and all of them seemed to be examples of code reuse by
copy-and-paste operations. Figure 8 shows a method pair whose
vocabulary similarity is greater than 0.7 and whose structural sim-
ilarity is the lowest (0.47). Although structural similarity is low, it
is obvious that the pair was made by copying and pasting because
the head parts of the two methods are quite similar to each other.
There are several approaches to detect clones between different
projects. They can be classiï¬ed into two categories, ï¬ne-grained
detection [17, 23, 27, 39, 41] and unit-level detection[19, 35, 40].
Fine-grained detections can identify duplications even if they
are only small code chunks in source ï¬les. However, their
scalability is inferior to unit-level detections. For example,
Livieri et al. took two days to complete clone detection from
700 million lines of code using 80 personal computers [27].
Unit-level detections can identify duplications only if whole
units such as ï¬le, class, or method are duplicated. However,
they have high scalability. For example, Ishihara et al. took
less than two hours to complete clone detection from 360
million lines of code by using a single workstation [19].
To date, there has been no empirical study that has compared
across-project clone detection results between ï¬ne-grained and unit-
level detection. This research shows there is a risk of missing
clones even if we use both ï¬ne-grained and unit-level detection
techniques. Consequently, we need to develop new methodologies
for detecting across-project code reuse by using code characteris-
tics other than its structure.
7. THREATS TO VALIDITY
In order to relieve bias due to the dataset being used, we used
two different datasets in this experiment. The experimental results,
which are described in Section 5, were almost the same for the two
datasets. Consequently, we can say that if we use another dataset
in the future, the result will be almost the same. However, we used
only a set of parameters where the minimum length of token se-
quences was 50 and the minimum number of words was 10 and
both of the structural similarity and the vocabulary similarity were
0.7. If we use another set of parameters, we may obtain different
tendencies in the result.
The category within-directory means that given two methods are
in different ï¬les but in the same directory. Directories are gener-
ally nested, but the top-most directories in the source folder should
be the deciding factor. Consequently, if we had counted for only
the top-most directories for the category within-directory , we might
have obtained a different result.
We classiï¬ed method pairs into eight categories based on binary
determinations on the three similarities. Then, we sampled 100
method pairs from each of the categories. In such a way, the degree
of similarities on the structural similarity andvocabulary similar-
itymay not be considered appropriately. A random sampling of
method pairs based on the similarity values should be an appropri-
ate way.
We used the longest common subsequence algorithm to measure
thestructural similarity among methods because it is popular and
its computational complexity is not so high. However, there are
various ways to detect structurally similar code [36, 38]. If we had
used another way to measure the structural similarity , we would
have obtain a different distribution of structural similarity among
methods. Bellon et al. compared some techniques that detect struc-
turally similar code [3].TheUCI dataset includes the projects of Apache Software Foun-
dation . Consequently, The 500 projects extracted from UCI dataset
may include projects included in Apache dataset.
We split camel/snake cases, performed stemming, and removed
stop words in extracting vocabulary from source code. However,
some of the words were not extracted appropriately due to reasons
such as they were short names. To extract vocabulary more appro-
priately, it would be better to use Lawrie et al.â€™s method [26].
8. RELATED WORK
Tiark et al. conducted an experiment on type-3 clones5[44].
Their concern was what kinds of code characteristics contributed
to type-3 clone detection. They revealed that, if a given clone pair
had a similar word set in their identiï¬ers, humans were not likely
to reject it. Their result is similar to our result described in Section
5.4. However, they investigated only code that had been detected
as clones. They investigated neither code where the vocabulary
similarity was high nor code having the same signature.
Abebe et al. proposed using not only the structure of code but
also its vocabulary for predicting fault-prone modules [1]. They
conï¬rmed that using vocabulary had improved the accuracy of pre-
diction. In their experiment, predictions using a CK metrics suite
were compared with ones using CK metrics and bad smell informa-
tion of a vocabulary. The majority of cases using vocabulary with
CK metrics improved prediction.
Bigger et al. investigated the vocabulary relationship between
comments, identiï¬er, and literal on 125 projects [4]. They found
that 75% of words in the vocabulary appeared in identiï¬ers. On the
other hand, only a few words appeared only in comment or literal.
Their investigation is a comparison between comment, identiï¬er,
and literal. They did not compare vocabulary between projects.
Marcus et al. proposed a class cohesion metric based on code
comments and identiï¬ers in code [30]. In their evaluation, they
made two bug-prediction models: one was made from the pro-
posed metric and existing structure-based cohesion metrics such as
LCOM1[5]; the other was made only from existing metrics. Then,
they compared the bug prediction accuracy of both the models and
they conï¬rmed that the proposed metric was useful for bug predic-
tion.
Haiduc and Marcus investigated how many words in source code
were domain terms [11]. Their investigation targets were six graph
theory libraries, and they found that that 62% of words were do-
main terms. The result indicates that methods within a project or
domain have the same words in their code. We did not investi-
gate vocabulary similarity of same-domain software. However, our
investigation result showed that methods within a project have a
higher vocabulary similarity than ones across projects. Our inves-
tigation result showed the same trend as their investigation result.
Source code clustering is a promising technique for maintain-
ing legacy code or software evolution. For example, clustering can
be used for detecting source code that should be re-modularized
[33, 45] or identifying abstract data types [9]. Maletic and Marcus
showed that identifying similar modules in a software system was
helpful in understanding it [28, 29]. They utilized Latent Semantic
Indexing techniques to make similar module clusters. Such support
reduces the developerâ€™s cost for ï¬nishing a given task when devel-
oping or maintaining systems, and a developer can ï¬nish the task
better than without support.
Different software programs use different words even if they in-
clude the same processing [10]. This is known as the vocabulary
problem, which states that â€œno single word can be chosen to de-
5type-3 clones are duplicate code that include gapped lines.scribe a programming concept in the best wayâ€ [8]. Bajracharya
et al. developed a system to automatically learn how APIs can be
used [2]. They assumed that source code using the same APIs im-
plemented similar processing contents even if the source code used
different user identiï¬ers such as variable names. They then devel-
oped a technique called Structural Semantic Indexing . In addition,
there are methods that automatically identify a set of words related
to one another even if they are not related as English words [42,
46].
Corazza et al. proposed a software clustering technique using vo-
cabulary information [6]. They classiï¬ed vocabulary into six cat-
egories: class name, ï¬eld name, method name, parameter name,
comments, and statements. They gave different weights to dif-
ferent categories, and clusters are made by weighted vocabulary
similarities. They showed that vocabulary-based clustering was
more accurate than structure-based clustering in the context of re-
modularization in their experiment.
Hotta et al. compared ï¬ne-grained and unit-level clone detec-
tions [16]. They developed a unit-level detection tool that detects
similar blocks such as an ifstatement or forstatements in Java
source code. They evaluated the tool by using the four Java soft-
ware projects included in Bellonâ€™s benchmark [3]. They revealed
the unit-level detection had enough accuracy, but did not have high
recall compared to ï¬ne-grained detectors. They conducted experi-
ments for each of the projects and they did not target across-project
clones.
9. CONCLUSION
In this paper, we investigated the relationships between struc-
tural similarity ,vocabulary similarity , and method name similarity
of Java methods with consideration of their positional relationship,
which has four categories: within-ï¬le ,within-directory ,within-project ,
andacross-project . Our experimental targets were two different
sets of open source projects. For each of the datasets, we measured
the three similarities on approximately 14 million method pairs.
As a result, we found the following. (1) Method names do not
always reï¬‚ect functional code similarity. If there are a small num-
ber of methods that have a given name, the methods are likely to
include functionally similar code. (2) Existing hash-based clone
detection techniques at the ï¬le-level, method-level and block-level
miss many instances of copy-and-pasted code between different
projects. (3) In cases where we use structural similarity for detect-
ing similar code, we obtain many false positives. However, most
of the false positives are avoidable by using vocabulary similarity
in addition to structural similarity . (4) Using vocabulary similar-
ityfor detecting similar code is not suitable for method pairs in the
same ï¬le because such method pairs use many of the same program
elements such as private methods or private ï¬elds. Their high vo-
cabulary similarity is due to using the same program elements, not
due to using the same words.
We also showed some directions for future research based on the
experimental results. They include the following: (A) techniques
for pulling up similar methods to the common parent classes, (B)
detecting semantic clones, and (C) identifying code reuse between
different projects.
10. ACKNOWLEDGMENTS
This study was supported by a Grant-in-Aid for Scientiï¬c Re-
search (S) (25220003), a Grant-in-Aid for Exploratory Research
(24650011) from the Japan Society for the Promotion of Science,
and a Grant-in-Aid for Young Scientists (A) (24680002) from the
Ministry of Education, Culture, Sports, Science and Technology.11. REFERENCES
[1] S. L. Abebe, V. Arnaoudova, G. Antoniol, and Y. Gueheneuc.
Can Lexicon Bad Smells Improve Fault Prediction. In
Proceedings of the 19th Working Conference on Reverse
Engineering , pages 235â€“244, 2012.
[2] S. K. Bajracharya, J. Ossher, and C. V. Lopes. Leveraging
Usage Similarity for Effective Retrieval of Examples in Code
Repositories. In Proceedings of the 18th International
Symposium on Foundations of Software Engineering , pages
157â€“166, 2010.
[3] S. Bellon, R. Koschke, G. Antoniol, J. Krinke, and E. Merlo.
Comparison and Evaluation of Clone Detection Tools. IEEE
Transactions on Software Engineering , 33(9):577â€“591, 2007.
[4] L. R. Biggers, B. P. Eddy, N. A. Kraft, and L. H. Etzkorn.
Toward a Metrics Suite for Source Code Lexicons. In
Proceedings of the 27th International Conference on
Software Maintenance , pages 492â€“495, 2011.
[5] S. R. Chidamber and C. F. Kemerer. A Metrics Suite for
Object Oriented Design. IEEE Transactions on Software
Engineering , 20(6):476â€“493, 1994.
[6] A. Corazza, S. D. Martino, V. Maggio, and G. Scanniello.
Investigating the Use of Lexical Information for Software
System Clustering. In Proceedings of the 15th European
Conference on Software Maintenance and Reengineering ,
pages 35â€“44, 2011.
[7] M. Fowler. Refactoring: Improving the Design of Existing
Code . Addison-Wesley Longman Publishing Co., Inc.,
Boston, MA, USA, 1999.
[8] G. W. Furnas, T. K. Landauer, L. M. Gomez, and S. T.
Dumais. The Vocabulary Problem in Human-system
Communication. Communications of the ACM ,
30(11):964â€“971, 1987.
[9] J. Girard and R. Koschke. A Comparison of Abstract Data
Types and Objects Recovery Techniques. Science of
Computer Programming , 36(2â€“3):149â€“181, 2000.
[10] M. Grechanik, C. Fu, Q. Xie, C. McMillan, D. Poshyvanyk,
and C. Cumby. A Search Engine for Finding Highly Relevant
Applications. In Proceedings of the 32nd International
Conference on Software Engineering , pages 475â€“484, 2010.
[11] S. Haiduc and A. Marcus. On the Use of Domain Terms in
Source Code. In Proceedings of the 16th International
Conference on Program Comprehension , pages 113â€“122,
2008.
[12] Y. Higo, T. Kamiya, S. Kusumoto, and K. Inoue. Method and
Implementation for Investigating Code Clones in a Software
System. Information and Software Technology ,
49(9-10):985â€“998, 2007.
[13] Y. Higo and S. Kusumoto. Code Clone Detection on
Specialized PDGs with Heuristics. In Proceedings of the
15th European Conference on Software Maintenance and
Reengineering , pages 75â€“84, 2011.
[14] Y. Higo, S. Kusumoto, and K. Inoue. A Metric-based
Approach to Identifying Refactoring Opportunities for
Merging Code Clones in a Java Software System. Journal of
Software: Maintenance and Evolution , 20(6):435â€“461, 2008.
[15] K. Hotta, Y. Higo, and S. Kusumoto. Identifying, Tailoring,
and Suggesting Form Template Method Refactoring
Opportunities with Program Dependence Graph. In
Proceedings of the 16th European Conference on Software
Maintenance and Reengineering , pages 53â€“62, 2012.
[16] K. Hotta, Y. Higo, and S. Kusumoto. How Accurate Is
Coarse-grained Clone Detection?: Comparison withFine-grained Detectors. In Proceedings of the 8th
International Workshop on Software Clones , pages 1â€“18,
2014.
[17] B. Hummel, E. Juergens, L. Heinemann, and M. Conradt.
Index-based Code Clone Detection: Incremental,
Distributed, Scalable. In Proceedings of the International
Conference on Software Maintenance , pages 1â€“9, 2010.
[18] K. Inoue, R. Yokomori, T. Yamamoto, M. Matsushita, and
S. Kusumoto. Ranking Signiï¬cance of Software Components
Based on Use Relations. IEEE Transactions on Software
Engineering , 31(3):213â€“225, 2005.
[19] T. Ishihara, K. Hotta, Y. Higo, H. Igaki, and S. Kusumoto.
Inter-Project Functional Clone Detection Toward Building
Libraries - An Empirical Study on 13,000 Projects. In
Proceedings of the 19th Working Conference on Reverse
Engineering , pages 387â€“391, 2012.
[20] T. Kamiya, S. Kusumoto, and K. Inoue. CCFinder: A
Multilinguistic Token-based Code Clone Detection System
for Large Scale Source Code. IEEE Transactions on
Software Engineering , 28(7):654â€“670, 2002.
[21] H. Kim, Y. Jung, S. Kim, and K. Yi. MeCC: Memory
Comparison-based Clone Detector. In Proceedings of the
33rd International Conference on Software Engineering ,
pages 301â€“310, 2011.
[22] R. Komondoor and S. Horwitz. Using Slicing to Identify
Duplication in Source Code. In Proceedings of the 8th
International Symposium on Static Analysis , pages 40â€“56,
2001.
[23] R. Koschke. Large-scale Inter-system Clone Detection Using
Sufï¬x Trees and Hashing. Journal of Software: Evolution
and Process , pages n/aâ€“n/a, 2013.
[24] J. Krinke. Identifying Similar Code with Program
Dependence Graphs. In Proceedings of the 8th Working
Conference on Reverse Engineering , pages 301â€“309, 2001.
[25] G. P. Krishnan and N. Tsantalis. Uniï¬cation and Refactoring
of Clones. In Proceedings of the International Conference on
Software Maintenace, Reengineering and Reverse
Engineering , pages 104â€“113, 2014.
[26] D. Lawrie, D. Binkley, and C. Morrell. Normalizing Source
Code Vocabulary. In Proceedings of the 17th Working
Conference on Reverse Engineering , pages 3â€“12, 2010.
[27] S. Livieri, Y. Higo, M. Matushita, and K. Inoue. Very-Large
Scale Code Clone Analysis and Visualization of Open
Source Programs Using Distributed CCFinder: D-CCFinder.
InProceedings of the 29th international conference on
Software Engineering , pages 106â€“115, 2007.
[28] J. I. Maletic and A. Marcus. Supporting Program
Comprehension Using Semantic and Structural information.
InProceedings of the 23rd International Conference on
Software Engineering , pages 103â€“112, 2001.
[29] A. Marcus and J. I. Maletic. Identiï¬cation of High-Level
Concept Clones in Source Code. In Proceedings of the 16th
international conference on Automated software
engineering , pages 107â€“114, 2001.
[30] A. Marcus, D. Poshyvanyk, and R. Ferenc. Using the
Conceptual Cohesion of Classes for Fault Prediction in
Object-Oriented Systems. IEEE Transactions on Software
Engineering , 34(2):287â€“300, 2008.
[31] J. Mayrand, C. Leblanc, and E. Merlo. Experiment on the
Automatic Detection of Function Clones in a Software
System Using Metrics. In Proceedings of the 1996
International Conference on Software Maintenance , pages244â€“253, 1996.
[32] C. McMillan, M. Grechanik, D. Poshyvanyk, Q. Xie, and
C. Fu. Portfolio: Finding Relevant Functions and Their
Usage. In Proceedings of the 33rd International Conference
on Software Engineering , pages 111â€“120, 2011.
[33] H. A. MÃ¼ller, M. A. Orgun, S. R. Tilley, and J. S. Uhl. A
Reverse-engineering Approach to Subsystem Structure
Identiï¬cation. Journal of Software Maintenance: Research
and Practice , 5(4):181â€“204, 1993.
[34] H. Murakami, K. Hotta, Y. Higo, H. Igaki, and S. Kusumoto.
Folding Repeated Instructions for Improving Token-Based
Code Clone Detection. In Proceedings of the 12th
International Working Conference on Source Code Analysis
and Manipulation , pages 64â€“73, 2012.
[35] J. Ossher, H. Sajnani, and C. Lopes. File Cloning in Open
Source Java Projects: The Good, The Bad, and The Ugly. In
Proceedings of the 27th International Conference on
Software Maintenance , pages 283â€“292, 2011.
[36] D. Rattan, R. Bhatia, and M. Singh. Software Clone
Detection: A Systematic Review. Information and Software
Technology , 55(7):1165â€“1199, 2013.
[37] C. K. Roy and J. R. Cordy. NICAD: Accurate Detection of
Near-Miss Intentional Clones Using Flexible Pretty-Printing
and Code Normalization. In Proceedings of the 2008 The
16th IEEE International Conference on Program
Comprehension , pages 172â€“181, 2008.
[38] C. K. Roy, J. R. Cordy, and R. Koschke. Comparison and
Evaluation of Code Clone Detection Techniques and Tools:
A Qualitative Approach. Science of Computer Programming ,
74(7):470â€“495, 2009.
[39] H. Sajnani and C. Lopes. A Parallel and Efï¬cient Approachto Large Scale Clone Detection. In Proceedings of the 7th
International Workshop on Software Clones , pages 46â€“52,
May 2013.
[40] Y. Sasaki, T. Yamamoto, Y. Hayase, and K. Inoue. Finding
File Clones in FreeBSD Ports Collection. In Proceedings of
the 7th Working Conference on Mingin Software
Repositories , pages 102â€“105, 2010.
[41] W. Shang, B. Adams, and A. E. Hassan. An Experience
Report on Scaling Tools for Mining Software Repositories
using MapReduce. In Proceedings of the international
conference on Automated software engineering , pages
275â€“284, 2010.
[42] L. Tan, Y. Zhou, and Y. Padioleau. aComment: Mining
Annotations from Comments and Code to Detect Interrupt
Related Concurrency Bugs. In Proceedings of the 33rd
International Conference on Software Engineering , pages
11â€“20, 2011.
[43] P. Tan, M. Steinbach, and V. Kumar. Introduction to Data
Mining, (First Edition) . Addison-Wesley Longman
Publishing Co., Inc., 2005.
[44] R. Tiarks, R. Koschke, and R. Falke. An Extended
Assessment of Type-3 Clones As Detected by
State-of-the-art Tools. Software Quality Control ,
19(2):295â€“331, 2011.
[45] T. A. Wiggerts. Using Clustering Algorithms in Legacy
Systems Remodularization. In Proceedings of the 4th
Working Conference on Reverse Engineering , pages 33â€“43,
1997.
[46] J. Yang and L. Tan. Inferring Semantically Related Words
from Software Context. In Proceedings of the Working
Conference on Mining Software Repositories , pages
161â€“170, 2012.