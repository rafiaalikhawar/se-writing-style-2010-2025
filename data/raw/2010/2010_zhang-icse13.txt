Bridging the Gap between the Total andAdditional
Test-Case Prioritization Strategies
Lingming Zhang∗†, Dan Hao∗, Lu Zhang∗, Gregg Rothermel‡, Hong Mei∗
∗Key Laboratory of High Conﬁdence Software Technologies (Pe king University), MoE, Beijing, 100871, China
{zhanglm07,haod,zhanglu,meih }@sei.pku.edu.cn
†Department of Electrical and Computer Engineering, Univer sity of Texas, Austin, 78712, USA
zhanglm@utexas.edu
‡Department of Computer Science and Engineering, Universit y of Nebraska, Lincoln, 68588, USA
grother@cse.unl.edu
Abstract —In recent years, researchers have intensively inves-
tigated various topics in test-case prioritization, which aims to
re-order test cases to increase the rate of fault detection d uring
regression testing. The total and additional prioritization strate-
gies, which prioritize based on total numbers of elements co vered
per test, and numbers of additional (not-yet-covered) elem ents
covered per test, are two widely-adopted generic strategie s used
for such prioritization. This paper proposes a basic model a nd
an extended model that unify the total strategy and the additional
strategy. Our models yield a spectrum of generic strategies
ranging between the total and additional strategies, depending
on a parameter referred to as the pvalue. We also propose
four heuristics to obtain differentiated pvalues for different
methods under test. We performed an empirical study on 19
versions of four Java programs to explore our results. Our
results demonstrate that wide ranges of strategies in our ba sic
and extended models with uniform pvalues can signiﬁcantly
outperform both the total and additional strategies. In addition,
our results also demonstrate that using differentiated pvalues for
both the basic and extended models with method coverage can
even outperform the additional strategy using statement coverage.
I. I NTRODUCTION
Software engineers usually maintain a large number of test
cases, which can be reused in regression testing to test soft ware
changes. Due to the large number of test cases, regression
testing can be very time consuming. For example, Elbaum et
al. [7], [9] reported one industrial case in which the execut ion
time of the entire regression test suite for one product was
seven weeks. Test-case prioritization [7]–[9], [24], [26] , [30],
which attempts to re-order regression test cases to detect f aults
as early as possible, has been intensively investigated as a way
to deal with lengthy regression testing cycles.
In test-case prioritization, a fundamental topic involves
prioritization strategies. In previous work, researchers have
studied two greedy strategies (the total andadditional strate-
gies), which are generic for different coverage criteria. G iven
a coverage criterion, the total strategy sorts test cases in
descending order of coverage, whereas the additional strategy
always picks a next test case having the maximal coverage
of items not yet covered by previously prioritized test case s.
In addition to these two strategies, researchers have also
investigated other generic strategies. Li et al. [16] inves tigated
the 2-optimal greedy strategy [17], a hill-climbing strate gy, anda genetic programming strategy. Jiang et al. [12] investiga ted
adaptive random prioritization. Their empirical results s how
that the additional strategy remains the most effective generic
strategy on average in terms of rate of fault detection.
There is also, however, a weakness in the additional
strategy. Consider statement coverage for instance. In the
additional strategy, after a test case tis chosen, no statement
covered by tis explicitly considered again until all coverable
statements are covered at least one time. As a result, when
there is a fault fin one statement covered by tbut not covered
by any test case chosen before t, iftcannot detect f, the
detection of fmay be greatly postponed. In contrast, the total
strategy does not have this weakness, because when choosing a
next test case, the total strategy always considers all statements
no matter whether or not previously chosen test cases have
covered the statements. This said, as the total strategy counts
only the numbers of statements covered by each test case, it
may be more inclined to choose test cases to cover statements
previously covered many times than to choose test cases to
cover previously not (intensively) covered statements. Th us,
thetotal strategy may postpone the detection of faults in rarely
covered statements. As a result, some strategy that has the
ﬂavor of both the additional strategy and the total strategy
may be more advantageous.
To understand the situation in which a test case covers a
statement but does not reveal a fault in the statement, consi der
the following piece of code with a fault in line 5. The code is a
method returning the larger of xandy. A test case in which the
value ofxis smaller than that of ycovers the faulty statement
and detects the fault. However, a test case in which the value
ofxis equal to that of yalso covers the faulty statement but
does not detect the fault.
1:intmax(intx,inty) {
2: if(x>y)
3: returnx;
4: else
5: returnx;//should be "return y".
6: }
In fact, research on test-suite reduction [11], [25], [31]
has demonstrated that re-covering already covered stateme nts
may enhance fault-detection capability. Furthermore, whe n we978-1-4673-3076-3/13 c2013 IEEE ICSE 2013, San Francisco, CA, USA
Accepted for publication by IEEE. c2013 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/
republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.192consider test-case prioritization based on coverage infor mation
at a coarser level (e.g., the method level), it may be more
common for a test case to miss a fault in a method covered
by the test case, because that test case may fail to cover the
faulty statement in the method.
In this paper, we propose a uniﬁed view (including a basic
model and an extended model) for generic strategies in test-
case prioritization. In our models, the total and additional
strategies are extreme instances, and the models also deﬁne
various generic strategies that lie between the total strategy
and the additional strategy depending on the value of fault
detection probability (referred to as the pvalue). In addition,
we further extend the models by using differentiated pvalues.
We view our models as an initial framework to control the un-
certainty of fault detection during test-case prioritizat ion, and
believe more techniques can be derived based on our models.
We performed an empirical study to compare our strategies
with the total strategy and the additional strategy. Our results
demonstrate that many of our strategies can outperform both
thetotal andadditional strategies.
The main contributions of this paper are as follows.
•A new approach that creates better prioritization tech-
niques by controlling the uncertainty of fault detection
capability in test-case prioritization.
•Two models that unify the total andadditional strategies
and can also yield a spectrum of generic strategies having
ﬂavors of both the total andadditional strategies.
•Empirical evidence that many strategies between the total
andadditional strategies are more effective than either of
those strategies.
•Empirical evidence that our strategies using differentiat ed
pvalues with method coverage can signiﬁcantly outper-
form the additional strategy with statement coverage.
II. U NIFYING THE TOTAL AND ADDITIONAL STRATEGIES
With the additional strategy, the primary concern is to cover
units not yet covered by previous test cases. This strategy
should be well suited for circumstances in which the probabi l-
ity of a test case detecting faults in units it covers is high. On
the other hand, the primary concern for the total strategy is to
cover the most units with each test case. This strategy shoul d
be well suited for circumstances in which the probability of a
test case detecting faults in units it covers is low. Thus, if we
explicitly consider the probability for a test case to detec t faults
in units it covers, we may devise strategies to take advantag e of
the strengths of both the total andadditional strategies. More
speciﬁcally, our models initially assign probability valu es for
each program unit1. Then, each time a unit is covered by
a test case (that could potentially detect some fault(s) in t he
unit ), the probability that the unit contains undetected faults
is reduced by some ratio between 0% (as in the total strategy)
and 100% (as in the additional strategy). In this way, we build
1As our approach is intended to work with different coverage c riteria, we
useunit as a generic term to denote different structural elements us ed in
different coverage criteria. For example, a unit represents a statement for
statement coverage but a method for method coverage.a spectrum of generic prioritization strategies between th etotal
andadditional strategies.
Algorithm 1 Prioritization in the basic model with p
1:foreachj(1≤j≤m)do
2:Prob[j]←1
3:end for
4:foreachi(1≤i≤n)do
5:Selected[i]←false
6:end for
7:foreachi(1≤i≤n)do
8:k←1
9: whileSelected[k]do
10:k←k+1
11: end while
12:sum←0
13: foreachj(1≤j≤m)do
14: ifCover[k,j]then
15: sum←sum+Prob[j]
16: end if
17: end for
18: foreachl(k+1≤l≤n)do
19: if notSelected[l]then
20: s←0
21: foreachj(1≤j≤m)do
22: ifCover[l,j]then
23: s←s+Prob[j]
24: end if
25: end for
26: ifs > sum then
27: sum←s
28: k←l
29: end if
30: end if
31: end for
32:Priority [i]←k
33:Selected[k]←true
34: foreachj(1≤j≤m)do
35: ifCover[k,j]then
36: Prob[j]←Prob[j]∗(1−p)
37: end if
38: end for
39:end for
A. Basic Model
In our basic model, when a test case tcovers a unit u,
we refer to the probability that tcan detect faults in uas
p. Consider a test suite T={t1,t2,...,tn}containing ntest
cases and a program U={u1,u2,...,um}containing munits.
Algorithm 1 depicts test-case prioritization in our basic m odel,
in which we use a Boolean array Cover[i,j] (1≤i≤n,1≤
j≤m)to denote whether test case ticovers unit uj.
In Algorithm 1, we use an array Prob[j] (1≤j≤m)to
store the probability that unit ujcontains undetected faults.
Initially, we set the value of Prob[j] (1≤j≤m)to
be1. We use a Boolean array Selected[i] (1≤i≤n)to
2193store whether test case tihas been selected for prioritization.
Initially, we set the value of Selected[i] (1≤i≤n)to be
false . Furthermore, we use an array Priority [i] (1≤i≤n)
to store the prioritized test cases. If Priority [i]is equal to k
(1≤i,k≤n), test case tkis ordered in the ith position.
In Algorithm 1, lines 1-6 perform initialization. In the mai n
loop from lines 7 to 39, each iteration determines which test
case to place in the prioritized test suite. Lines 8-31 ﬁnd a
test casetksuch that tkis previously not chosen and the sum
of probabilities that units covered by tkcontain undetected
faults is the highest among test cases not yet chosen. Note th at,
as the basic model utilizes a uniform probability pfor fault
detection in covered units, lines 8-31 actually ﬁnd a test ca se
with the highest probability of detecting previously undet ected
faults. In particular, lines 8-17 ﬁnd the ﬁrst test case tknot
previously chosen for prioritization and calculate the sum of
the probabilities that units covered by tkcontain undetected
faults, and lines 18-31 check whether there is another uncho sen
test case tlfor which the sum of the probabilities that the
covered units contain undetected faults is higher than that
fortk. Line 32 sets the ith position in the prioritized test
suite to tk, and line 33 marks tkas already chosen for
prioritization. Lines 34-38 update the probability that un its
contain undetected faults for each unit covered by tk.
Algorithm 1 is in fact a variant of the algorithm for the
additional strategy. The main difference is that this algorithm
tries to ﬁnd the test case covering units with the maximal sum
of probabilities of containing undetected faults. Due to th e
similarity between this algorithm and the additional strategy,
its worst case time cost is the same as that of the additional
strategy (i.e., O(mn2), wherenis the number of test cases
andmis the number of units [26]).
With Algorithm 1, an optimistic tester who believes that a
test case is likely to detect faults in covered units may set
the value of pto1. In such a circumstance, this algorithm
is equivalent to the additional strategy. The reason for this is
that lines 34-38 set the probability for any previously cove red
unit to contain undetected faults to 0. In contrast, a pessimistic
tester who is concerned with the situation in which a test cas e
may not detect faults in units covered by the test case may set
the value of pto0. In such a circumstance, this algorithm is
equivalent to the total strategy. The reason is that lines 34-38
do not change the probability that any previously covered un it
contains undetected faults. Note that setting pto0does not
render the algorithm as efﬁcient as the original total strategy,
whose worst case time cost is O(mn)[26]. Finally, if a tester
sets the value of pto a number between 0and1, this algorithm
is a strategy between the total strategy and the additional
strategy. The closer pis to0, the closer this algorithm is to the
total strategy; and the closer pis to1, the closer this algorithm
is to the additional strategy.
B. Extended Model
In our basic model and previously proposed strategies for
test-case prioritization, when a test case tcovers a unit u, the
number of times tcoversuis not further considered. That is,no matter how many times tcoversu, the algorithm treats
uas having been covered once. Intuitively, the more times t
coversu, the more probable it may be for tto detect faults in
u. Therefore, considering the ability of a test case to cover a
unit multiple times may result in higher effectiveness.
We now extend our basic model to consider multiple cov-
erage of units by given test cases. In our extended model, the
main body of the algorithm is the same as the algorithm in
our basic model, but the extended algorithm uses a different
method for calculating the probability for a test case to
detect previously undetected faults. We extend Cover[i,j]
(1≤i≤n,1≤j≤m)to denote the number of times test
caseticovers unit uj. We now present the main differences
between the two algorithms.
First, as the number of times test case tkcovers unit ujis
Cover[k,j], the probability for unit ujto contain undetected
faults changes from Prob[j]toProb[j]∗(1−p)Cover[k,j]after
executing tkif we consider each instance of coverage to have
an equal probability pof detecting faults. That is to say, for
unitujalone, execution of tkdecreases the probability that uj
contains undetected faults by Prob[j]∗(1−(1−p)Cover[k,j]).
Thus, in the extended algorithm, we change lines 15 and 23 of
Algorithm 1 to sum←sum+Prob[j]∗(1−(1−p)Cover[k,j])
ands←s+Prob[j]∗(1−(1−p)Cover[l,j]), respectively.
Second, after we select test case tkfor prioritization at the
ith place, the probability for unit ujto contain undetected
faults changes from Prob[j]toProb[j]∗(1−p)Cover[k,j].
Thus, in the extended algorithm, we change line 36 of Algo-
rithm 1 to Prob[j]←Prob[j]∗(1−p)Cover[k,j]. The worst
case time cost of the extended algorithm is also O(mn2), the
same as that of Algorithm 1.
In the extended algorithm, if we set pto1, the algorithm is
the same as the additional strategy, because (1−p)Cover[k,j]
is equal to 0whenpis equal to 1. However, if we set pto0,
the extended algorithm cannot distinguish any test cases fr om
each other,2because1−(1−p)Cover[k,j]is always equal to 0
whenpis equal to 0. If we set pto a number between 0and1,
the extended algorithm also represents a strategy between t he
total andadditional strategies, considering multiple coverage
for each test case.
C. Differentiating p Values
In Section II-A, in our basic model, whenever a test case t
covers a unit u, we consider the probability for tto detect
faults in uto be uniformly p. In Section II-B, using our
extended model, we reason that when tcoversumultiple
times, the probability for tto detect faults in umay not be
uniform, but each instance of coverage also implies a unifor m
probability of fault detection. In reality, however, fault s in
some units may be easier to detect than faults in other units.
In this section, we further extend our models to account
for the situation in which the probability of fault detectio n
is differentiated. To deal with this situation, we need to
2This limitation is due to the speciﬁc algorithm, but concept ually our
extended model implementation yields the total strategy when p= 0.
3194assign different probability values for test cases to detec t
faults in different units. The challenge in performing such an
assignment, however, lies in obtaining effective estimate s of
the probability of fault detection. In this paper, we furthe r
estimate differentiated pvalues at the method level based
on two widely used static metrics: MLoC , which stands for
Method Line of Code, and McCabe , which stands for the well-
known McCabe Cyclomatic Complexity [19]. Our approach
is based on the intuition that methods with larger volume
(e.g., higher MLoC values) or greater complexity (e.g., hig her
McCabe values) need to be covered more times to reveal the
faults within them, i.e., they should have lower pvalues. We
believe that test cases should be good at detecting faults,
and thus we calculate the pvalue for each method in the
range[0.5,1.0]. Formally, we use both linear normalization
(Formula (1)) and log normalization (Formula (2)) to calcul ate
thepvalue for the jth method (i.e., p[j]) as follows:
1−0.5∗Metric[j]−Metric min
Metric max−Metric min(1)
1−0.5∗log10(Metric[j] +1)−log10(Metric min+1)
log10(Metric max+1)−log10(Metric min+1)(2)
whereMetric[j]denotes the MLoC or McCabe metric values
for thejth method, and Metric min/Metric max denotes the
minimum/maximum metric value among all methods of the
program under test.3
Based on the two metrics and the two pcalculation formu-
las, we thus have four heuristics for generating a different iated
pvalue for each method. For both models, we change all ref-
erences to the uniform pinto the differentiated p[j]generated
for the speciﬁc jth method. For the basic model, we change
line 36 of Algorithm 1 into Prob[j]←Prob[j]∗(1−p[j]).
Similarly, for the extended model, we change lines 15, 23,
and 36 of Algorithm 1 to sum←sum+Prob[j]∗(1−(1−
p[j])Cover[k,j]),s←s+Prob[j]∗(1−(1−p[j])Cover[l,j]),
andProb[j]←Prob[j]∗(1−p[j])Cover[k,j], respectively.
Note that the worst case time costs of the basic and extended
models with differentiated pvalues are still O(mn2).
III. E MPIRICAL STUDY
To evaluate our strategies with uniform and differentiated
pvalues in the basic and extended models, we performed an
empirical study to investigate the following research ques tions:
•RQ1 : How do prioritization strategies generated by the
basic and extended models with uniform pvalues com-
pare with the total andadditional strategies?
•RQ2 : How do the granularity of coverage and the gran-
ularity of test cases impact the comparative effectiveness
of strategies generated by our models?
•RQ3 : How does the use of differentiated pvalues
compare, in terms of effectiveness, with the total and
additional strategies?
3Note that all metric values are increased by 1 in the log norma lization to
avoid the log100exception.A. Independent Variables
We consider three independent variables:
Prioritization Strategy . We use the following 48 strategies
for test-case prioritization. First, as control strategie s, we use
thetotal andadditional strategies. Second, for our basic model
we use values of pranging from 0.05 to 0.95 with increments
of 0.05, i.e., 19 pvalues. Third, for our extended model we use
the same 19 values of pas those used for our basic model.
Fourth, for differentiated pvalues we use the four pvalue
generation heuristics for both the basic and extended model s.
Coverage Granularity . In prior research on test-case pri-
oritization, researchers treated coverage granularity as a con-
stituent part of prioritization techniques. As our aim is to
investigate various generic prioritization strategies, w e separate
coverage granularity from prioritization techniques. As i n prior
research, we use structural coverage criteria at both the me thod
level and the statement level. Note that we used differentia ted
pvalues only at the method level.
Test-Case Granularity . We consider test-case granularity
as an additional factor, at two levels: the test-class level and
the test-method level. For the test-class level we treat eac h
JUnit TestCase class as a test case. For the test-method leve l
we treat each test method in a JUnit TestCase class as a test
case. That is to say, a test case at the test-class level typic ally
consists of a number of test cases at the test-method level.
Section III-C provides a detailed description.
B. Dependent Variable
Our dependent variable tracks technique effectiveness. We
adopt the well-known APFD (Average Percentage Faults De-
tected) metric [26]. Let Tbe a test suite and T′be a
permutation of T, the APFD for T′is deﬁned as follows.
APFD=/summationtextn−1
i=1Fi
n∗l+1
2n(3)
Here,nis the number of test cases in T,lis the number of
faults, and Fiis the number of faults detected by at least one
test case among the ﬁrst itest cases in T′.
C. Object Programs, Test Suites, and Faults
As objects of study we consider 19 versions of four pro-
grams written in Java, including three versions of jtopas ,
three versions of xml-security , ﬁve versions of jmeter , and
eight versions of ant. We obtained the programs from the
Software-artifact Infrastructure Repository (SIR)4[3], which
provides Java and C programs for controlled experimentatio n
on program analysis and testing. The sizes of the programs
range from 1.8 to 80 KLoC. Table I depicts statistics on the
objects. In Table I, for each object program, Columns 3 and
4 present the number of classes (including interfaces) and t he
number of methods, respectively.
In SIR, each version of each program has a JUnit test
suite that was developed during program evolution. Due to th e
features of JUnit, there are two levels of test-case granula rity in
4http://sir.unl.edu/portal/index.html, accessed in Febr uary 2013.
4195TABLE I
STATISTICS ON OBJECTS OF STUDY
Object KLoC #Class #Meth #TClass #TMeth
jtopas -v1 1.89 19 284 10 (8) 126 (24)
jtopas -v2 2.03 21 302 11 (10) 128 (27)
jtopas -v3 5.36 50 748 18 (8) 209 (25)
xmlsec -v1 18.3 179 1627 15 (3) 92 (10)
xmlsec -v2 19.0 180 1629 15 (1) 94 (7)
xmlsec -v3 16.9 145 1398 13 (8) 84 (41)
jmeter -v1 33.7 334 2919 26 (7) 78 (18)
jmeter -v2 33.1 319 2838 29 (8) 80 (31)
jmeter -v3 37.3 373 3445 33 (16) 78 (43)
jmeter -v4 38.4 380 3536 33 (16) 78 (55)
jmeter -v5 41.1 389 3613 37 (20) 97 (57)
ant-v1 25.8 228 2511 34 (17) 137 (45)
ant-v2 39.7 342 3836 51 (42) 219 (118)
ant-v3 39.8 342 3845 51 (44) 219 (148)
ant-v4 61.9 532 5684 102 (47) 521 (135)
ant-v5 63.5 536 5802 105 (53) 557 (133)
ant-v6 63.6 536 5808 105 (52) 559 (230)
ant-v7 80.4 649 7520 149 (122) 877 (599)
ant-v8 80.4 650 7524 149 (51) 878 (197)
these test suites: the test-class level and the test-method level.
Column 5 of Table I depicts the number of all test cases and
the number of test cases that detect at least one studied faul t
for each program at the test-class level. Similarly, Column
6 depicts the test case statistics at the test-method level. As
previous research [1], [2], [5] has conﬁrmed that it is suita ble
to use faults produced via mutation for experimentation in
test-case prioritization, we followed a similar procedure to
produce faulty versions for each of the 19 object programs.
In particular, we used MuJava5[18] to generate faults and
followed the procedure used by Do et al. [5] to select speciﬁc
mutants to use (as detailed below).
D. Implementation
To collect coverage information, we used on-the-ﬂy byte-
code instrumentation which dynamically instruments class es
loaded into the JVM through a Java agent without any
modiﬁcation of the target program. We implemented code
instrumentation based on the ASM byte-code manipulation
and analysis framework .6To compute Java metrics for each
method, we implemented our tool based on the abstract syntax
tree (AST) analysis provided by the Eclipse JDT toolkit .7We
extended the Eclipse AST parsing tool to calculate method
lines of code (MLoC) and McCabe Cyclomatic complexity
(McCabe) metrics.
E. Experiment Procedure
In actual testing scenarios, a speciﬁc program version usu-
ally does not contain a large number of faults [5]. Therefore ,
similar to Do et al. [5], we used the mutant pool for each
object program to create a set of small mutant groups. To
form a mutant group, we randomly selected ﬁve mutants that
can be killed by one or more test cases in the test suite for
the program. For each program, we randomly produced up to
20 mutant groups for each program ensuring that no mutant is
5http://cs.gmu.edu/ ∼offutt/mujava, accessed in February 2013.
6http://asm.ow2.org, accessed in February 2013.
7http://www.eclipse.org/jdt/, accessed in February 2013.used in more than one mutant group. In fact, as there are only
35 mutants of jmeter-v1 that can be killed by one or more test
cases in its test suite, we produced only seven mutant groups
for this program. In all other circumstances we produced 20
mutant groups for each program.
Next, we used each of the mutant groups produced for
each of the 19 program versions as possible subsequent
versions. That is to say, given a program version Vand a
generic prioritization strategy Swith a coverage-granularity
levelCland a test-case-granularity level Tl, we obtained the
effectiveness of strategy SonVforClandTlas follows.
First, we used Sto obtain a prioritized sequence of test cases
forVatClandTl. Then, we calculated the APFD values of
the prioritized sequence of test cases for each mutant group
ofV. These values serve as our data sets for analysis.
F . Threats to Validity
Our object programs, test cases, and seeded faults may
all pose threats to external validity. First, although we us ed
19 Java program versions of various sizes, the differences
seen in our study may be difﬁcult to generalize to other
Java programs. Furthermore, our results may not generalize
to programs written in languages other than Java. Second,
our results based on programs with seeded faults may not be
generalizable to programs with real faults. Third, the resu lts
may not be generalizable to other test cases. Further reduct ion
of these threats requires additional studies involving add itional
object programs, test suites, and faults.
The main threat to internal validity for our study is that
there may be faults in our implementation of the strategies
and the calculation of APFD values. To reduce this threat, we
reviewed all the code that we produced for our experiments
before conducting the experiments.
To assess technique effectiveness, we used the APFD
metric [26] that is widely used for test-case prioritizatio n.
However, the APFD metric does have limitations [5], [26],
and we did not consider efﬁciency or other cost and savings
factors. Reducing this threat requires additional studies using
more sophisticated cost-beneﬁt models [8].
G. Results and Analysis
Due to the large number of strategies, various test-case gra n-
ularities, coverage granularities, objects, and mutant gr oups
studied, box-plots across all objects are the most suitable way
to show our results.
1) RQ1: Existence of Better Strategies Between the Total
and Additional Strategies: Figures 1 to 4 depict the results of
the comparision of the 19 strategies in our basic model and
the 19 strategies in our extended model with the total and
additional strategies using test suites at the test-method/test-
class level and coverage information at the method/stateme nt
level. We denote the total strategy as Tot.and the additional
strategy as Add. . For a strategy in our basic model, we use
Band the value of pto denote the strategy. For example, we
useB05 to denote the strategy in our basic model with the
pvalue 0.05. Similarly, for a strategy in our extended model,
5196Tot.B05B10B15B20B25B30B35B40B45B50B55B60B65B70B75B8 0B85B90B95Add.404550556065707580859095100105
  
 APFD
Tot.E05E10E15E20E25E30E35E40E45E50E55E60E65E70E75E8 0E85E90E95Add.404550556065707580859095100105 APFD
Fig. 1. Results for test suites at the test-method level with method coverage
Tot.B05B10B15B20B25B30B35B40B45B50B55B60B65B70B75B8 0B85B90B95Add.404550556065707580859095100105 APFD
Tot.E05E10E15E20E25E30E35E40E45E50E55E60E65E70E75E8 0E85E90E95Add.404550556065707580859095100105 APFD
Fig. 2. Results for test suites at the test-method level with statement coverage
Tot.B05B10B15B20B25B30B35B40B45B50B55B60B65B70B75B8 0B85B90B95Add.404550556065707580859095100105 APFD
Tot.E05E10E15E20E25E30E35E40E45E50E55E60E65E70E75E8 0E85E90E95Add.404550556065707580859095100105 APFD
Fig. 3. Results for test suites at the test-class level with m ethod coverage
Tot.B05B10B15B20B25B30B35B40B45B50B55B60B65B70B75B8 0B85B90B95Add.404550556065707580859095100105 APFD
Tot.E05E10E15E20E25E30E35E40E45E50E55E60E65E70E75E8 0E85E90E95Add.404550556065707580859095100105 APFD
Fig. 4. Results for test suites at the test-class level with s tatement coverage
we useEand the value of pto denote the strategy. Thus,
the strategy in our extended model with the value of pset
to 0.05 is denoted as E05. In each plot, the X-axis shows
various strategies compared, and the Y-axis shows the APFD
values measured. Each box plot shows the average (dot in
the box), median (line in the box), upper/lower quartile, an d
90th/10th percentile APFD values achieved by a strategy ove r
all mutant groups of all 19 versions. For ease of understandi ng,
we mark the strategies with higher average APFD values over
the corresponding additional strategies as shadowed box plots.
Based on the results, we make the following observations.
First, when comparing strategies in our approach with the
additional strategy, strategies with pvalues between 0.95 and
0.50 in both our basic and extended models typically achievehigher average APFD values. The only exceptions to this are
the strategies in our basic model based on statement coverag e
for test suites at the test-class level with pvalues between 0.90
and 0.50, and for test suites at the test-method level with p
values between 0.65 and 0.50. This observation indicates th at
there is a wide range of pvalues that can be used for our
models. It should also be noted that the average increases in
APFD of our strategies over the additional strategy are usually
not large. However, considering that the additional strategy is
widely accepted as the most effective prioritization strat egy
and is as expensive as our strategies, the increases in APFD
are valuable and are actually achieved with almost no extra
cost.
6197TABLE II
FISHER ’SLSD TEST FOR COMPARING STRATEGIES IN THE BASIC MODEL TO THE additional STRATEGY
TCG CG B95B90B85B80B75B70B65B60B55B50B45B40B35B30B25B20B15B10B05
Test- Method 1 1 1 1 1 1 1 0 0 0 0 0 0 0 -1 -1 -1 -1 -1
Method Statement 0 0 0 0 0 0 0 0 0 0 0 -1 -1 -1 -1 -1 -1 -1 -1
Test- Method 0 0 0 0 0 0 0 0 0 0 0 0 0 -1 -1 -1 -1 -1 -1
Class Statement 0 0 0 0 0 0 0 0 0 0 -1 -1 -1 -1 -1 -1 -1 -1 -1
TABLE III
FISHER ’SLSD TEST FOR COMPARING STRATEGIES IN THE EXTENDED MODEL TO THE additional STRATEGY
TCG CG E95E90E85E80E75E70E65E60E55E50E45E40E35E30E25E20E15E10E05
Test- Method 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 -1
Method Statement 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -1 -1
Test- Method 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0
Class Statement 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
Second, when comparing strategies in our approach with the
total strategy (denoted as Tot. in the ﬁgures), our strategies
with all pvalues in both our basic and extended models
achieve higher average APFD values. One interesting point i s
that, even when the pvalue is 0.05 (which results in strategies
similar to the total strategy), strategies in both our basic and
extended models are substantially more effective than the total
strategy. This observation indicates that adding a little ﬂ avor
of the additional strategy into the total strategy could improve
thetotal strategy substantially.
Third, when comparing strategies in our basic model and
strategies in our extended model, the strategies perform si m-
ilarly with pvalues close to 1 and differently with pvalues
close to 0. When pis close to 1, strategies in both models
achieve comparable and even higher APFD values than the
additional strategy. However, when pis close to 0, strategies
in our extended model remain competitive but strategies in o ur
basic model become much less competitive. In other words,
strategies with a small pvalue in our basic model perform
more like the total strategy, but strategies in our extended
model always perform like the additional strategy with any
pvalues. In fact, almost all strategies of our extended model
withp≥0.15outperform the additional strategies, except
those prioritizing tests at the test-method level using sta tement
coverage with p∈[0.15,0.30].
As strategies in our models and the additional strategy
typically achieve similar APFD values, for each coverage-
granularity level and each test-case-granularity level, w e used
Origin8to perform a one-way ANOV A analysis of the strate-
gies. The results indicate that there are signiﬁcant differ ences
among the strategies at the 0.05 signiﬁcance level. We then
used Origin to perform Fisher’s LSD test [29] of the strategi es.
Tables II and III (in which TCG stands for test-case granular -
ity, CG stands for coverage granularity, 1 indicates statis tically
signiﬁcantly better, 0 indicates no signiﬁcant difference , and
-1 indicates statistically signiﬁcantly worse) list the re sults of
Fisher’s LSD test for comparing strategies in our models to
theadditional strategy.
According to Tables II and III, when prioritizing test cases
at the test-method level using method coverage, strategies with
pvalues between 0.65 and 0.95 in our basic model and with
anypvalues between 0.30 and 0.95 in our extended model
8http://www.originlab.com/, accessed in February 2013.achieve signiﬁcantly better APFD values than the additional
strategy. When prioritizing test cases at the test-class le vel
using method coverage, strategies with pvalues between
0.20 and 0.75 in our extended model signiﬁcantly outperform
theadditional strategy. Furthermore, the additional strategy
cannot signiﬁcantly outperform any strategies in our basic
model with pvalues between 0.50 and 0.95 and any strategies
in our extended model with pvalues between 0.15 and 0.95 in
any circumstance. This observation further conﬁrms that ou r
models can achieve clear beneﬁts.
2) RQ2: Impact of Coverage and Test-Case Granularities:
Based on Figures 1 to 4, we make the following observations.
Impact of coverage granularity. Our models seem to be more
beneﬁcial when using coverage information at the method
level than at the statement level. According to comparisons
between Figure 1 and Figure 2, and between Figure 3 and
Figure 4, in both our basic and extended models, the ranges
in which our strategies outperform the additional strategy on
average are much broader using coverage information at the
method level than at the statement level. We suspect the reas on
for this to be that, when a test case covers a statement, the
probability for the test case to detect faults in the stateme nt
is very high. Thus, the additional strategy is already a good
enough strategy for this situation. However, when a test cas e
covers a method, the probability for the test case to detect
faults in the covered method is not very high. Thus, we should
typically consider that the method may still contain some
undetected faults after being covered by some test cases.
Our extended model seems to be applicable for both method
coverage and statement coverage. In fact, for all combinati ons
of coverage granularity and test-case granularity, the ran ges of
strategies in our extended model that outperform the additional
strategy on average are all very wide (i.e., for any p >0.30).
As our empirical results indicate that our models are more
beneﬁcial with method coverage, we further compare our
strategies using method coverage with the additional strategy
using statement coverage. When prioritizing test cases at t he
test-method level, the average APFD values of wide ranges
of strategies in our models (i.e., strategies in the basic mo del
withpvalues between 0.75 and 0.90, and strategies in the
extended model with pvalues between 0.45 and 0.75) using
method coverage are very close to the average APFD values
of the additional strategy using statement coverage. When
prioritizing test cases at the test-class level, the averag e
7198BP1 BP2 BP3 BP4 Add.6065707580859095100105 APFD
(a) Test-method, basic modelEP1 EP2 EP3 EP4 Add.6065707580859095100105 APFD
(b) Test-method, extended modelBP1 BP2 BP3 BP4 Add.6065707580859095100105 APFD
(c) Test-class, basic modelEP1 EP2 EP3 EP4 Add.6065707580859095100105 APFD
(d) Test-class, extended model
Fig. 5. Prioritization results for models embodied with dif ferentiated pvalues for each method
APFD values of wide ranges of strategies in our models
(i.e., strategies in the basic model with pvalues between
0.70 and 0.80, and strategies in the extended model with p
values between 0.20 and 0.80) using method coverage are
as competitive as or even better than the average APFD
values of the additional strategy using statement coverage. We
also performed an ANOV A analysis (at the 0.05 level) and
Fisher’s LSD test to compare our strategies and the additional
strategy using method coverage to the additional strategy
using statement coverage. The ANOV A analysis and Fisher’s
LSD test demonstrate that there is no statistically signiﬁc ant
difference between the additional strategy using statement
coverage and any strategy in our basic model with any p
value between 0.50 and 0.95 or any strategy in our extended
model with any pvalue between 0.20 and 0.95 using method
coverage. However, the additional strategy using statement
coverage is signiﬁcantly better than the additional strategy
using method coverage. As coverage information at the metho d
level is usually much less expensive to acquire than coverag e
information at the statement level, this result indicates t hat
wide ranges of strategies in our models using method coverag e
can serve as cheap alternatives for the additional strategy using
statement coverage.
Impact of test-case granularity. Our extended model seems
to be more beneﬁcial than our basic model for prioritizing
test cases at the test-class level. First, when using the ext ended
model instead of the basic model, the number of strategies th at
outperform the additional strategies increases more dramati-
cally at the test-class level than at the test-method level ( shown
in Figures 1 to 4). Second, when prioritizing test cases at th e
test-method level, the largest average APFD values achieve d
by our extended model are larger than those achieved by our
basic model by 0.15 (statement coverage) and 0.25 (method
coverage), respectively. However, when prioritizing test cases
at the test-class level, the differences are 0.69 (statemen t
coverage) and 1.03 (method coverage), respectively. Third ,
results of our statistical analysis shown in Tables II and II I
also conﬁrm this observation. We suspect the reason for this to
be that it is more common for a test case at the test-class leve l
than a test case at the test-method level to cover a method or a
statement more than once. In such a circumstance, it is more
beneﬁcial to consider multiple coverage information.
All the strategies that we considered achieve signiﬁcantly
higher average APFD values for prioritizing test cases at th eTABLE IV
FISHER ’SLSD TEST FOR COMPARING p-DIFFERENTIATED TECHNIQUES
WITHp-UNIFORM TECHNIQUES AT THE TEST -METHOD LEVEL
Tech. BP1 BP2 BP3 BP4 EP1 EP2 EP3 EP4
M-B75 0 0 0 0 0 0 0 0
M-E60 0 0 0 0 0 0 0 0
M-Add. 1 1 1 1 1 1 1 1
S-B85 0 0 0 0 0 0 0 0
S-E65 0 0 0 0 0 0 0 0
S-Add. 0 0 0 0 0 0 0 0
TABLE V
FISHER ’SLSD TEST FOR COMPARING p-DIFFERENTIATED TECHNIQUES
WITHp-UNIFORM TECHNIQUES AT THE TEST -CLASS LEVEL
Tech. BP1 BP2 BP3 BP4 EP1 EP2 EP3 EP4
M-B70 1 0 1 0 1 1 1 1
M-E55 0 0 0 0 0 0 0 1
M-Add. 1 1 1 1 1 1 1 1
S-B95 1 0 1 0 1 1 1 1
S-E70 0 0 0 0 0 1 0 1
S-Add. 1 0 1 0 1 1 1 1
test-method level than for prioritizing test cases at the te st-class
level. In fact, for any object and strategy, using either sta tement
coverage or method coverage, the average APFD value for
prioritizing test cases at the test-method level is uniform ly
higher than that for prioritizing test cases at the test-cla ss level.
We suspect the reason for this to be that, as a test case at the
test-class level consists of a number of test cases at the tes t-
method level, it is more ﬂexible to prioritize test cases at t he
test-method level.
3) RQ3: Using Differentiated pValues: Figure 5 depicts
results obtained by comparing the p-differentiated strategies
with the corresponding additional strategies. We use BP to
denote the four strategies in the basic model, and EP to
denote the four strategies in the extended model. For the
basic model, BP1 denotes the use of the MLoC metric and
linear normalization, BP2 denotes the use of the MLoC metric
and log normalization, BP3 denotes the use of the McCabe
metric and linear normalization, and BP4 denotes the use of
the McCabe metric and log normalization. The naming of
strategies in the extended model follows the same manner.
In the box plots, the X-axis denotes the studied strategies,
the Y-axis denotes the APFD values achieved by compared
strategies, and each box denotes the results of a strategy
on all mutant groups of all objects. We also performed an
ANOV A analysis (at the 0.05 level) and Fisher’s LSD test to
compare the eight strategies with differentiated pvalues to the
best strategies in our basic/extended models and additional
8199strategies using method and statement coverage. Tables IV
and V show the Fisher’s LSD test result, where “M-” denotes
the strategies using method coverage, and “S-” denotes the
strategies using statement coverage. For example, “M- B70”
denote the B70 strategy using method coverage. We make the
following observations.
First, all strategies with differentiated pvalues outperform
the corresponding additional strategies based on method cov-
erage substantially. Figure 5 shows that all the strategies with
differentiated pvalues achieve higher APFD values over corre-
sponding addtional strategies on average. For example, whe n
prioritizing test-class-level tests using method coverag e, the
additional strategy achieves an APFD value of 76.88 on aver-
age, while the four strategies from the extended model achie ve
APFD values from 81.10 to 81.92. In addition, Table IV shows
that all eight strategies are statistically signiﬁcantly b etter than
theadditional strategy based on method coverage under test-
method granularity, and Table V shows that all eight strateg ies
are statistically signiﬁcantly better than the additional strategy
based on method coverage under test-class granularity.
Second, all strategies with differentiated pvalues using
method coverage are comparable to the best strategies in
our basic and extended models (including strategies using
method and statement coverage) and the additional strategies
using statement coverage, and even outperform some of those
techniques. At both test-class and test-method granularit ies,
the eight strategies are not statistically inferior to any b est
strategies within our basic/extended models or additional
strategies using statement coverage. At the test-class gra nu-
larity, six of the eight strategies are statistically signi ﬁcantly
better than the additional strategy using statement coverage
and the best strategies of the basic model using method
coverage and statement coverage. This indicates that strat egies
with differentiated pvalues using method coverage can even
be a cheaper but better alternative choice for prioritization
techniques using statement coverage.
H. Summary and Implications
We summarize the main ﬁndings of our experimental study:
•For a wide range of pvalues (i.e., between 0.95 and
0.50), strategies in both our basic and extended models
(on average) outperform or are at least competitive with
theadditional strategy using any combination of test-case
and coverage granularities.
•Strategies in the extended model are generally more
effective than strategies in the basic model, especially
when the values of pare close to 0.
•Strategies in the basic and extended models are more
beneﬁcial for method coverage than statement coverage.
•Our extended model is more beneﬁcial for test suites at
the test-class level, while our basic model is more suitable
for test suites at the test-method level.
•All our strategies using differentiated pvalues statistically
signiﬁcantly outperform the additional strategies using
method coverage. Some of our strategies using differen-
tiatedpvalues with method coverage even statisticallysigniﬁcantly outperform the additional strategies using
statement coverage.
The experimental ﬁndings provide implications for prac-
titioners. The need for more and better blended approaches
provides implications for researchers.
IV. R ELATED WORK
Since there is a considerable amount of research focusing
on various issues in test-case prioritization, we partitio n and
discuss the investigated issues into the following categor ies.
Prioritization Strategies . The total andadditional strate-
gies are the most widely-used prioritization strategies [2 6].
As neither can always achieve the optimal ordering of test
cases [26], researchers have also investigated various oth er
generic strategies. Li et al. [16] present the 2-optimal str ategy
(a greedy algorithm based on the k-optimal algorithm [17]),
a strategy based on hill-climbing, and a strategy based on
genetic programming. Jiang et al. [12] present the adaptive
random strategy. According to the reported empirical resul ts,
theadditional strategy is more effective than the total strategy
on average, and other strategies falls between the total and
additional strategies in terms of effectiveness. In this paper,
we investigate strategies with ﬂavors of both the total and
additional strategies. Most of our strategies are more effective
than either the total or the additional strategy. Theoretically,
our strategies are as expensive as the additional strategy.
Coverage Criteria . In principle, test-case prioritization can
use any test adequacy criterion as the underlying coverage
criterion. In fact, many criteria have been investigated in
previous research on test-case prioritization. The most wi dely
used criteria include basic code-based coverage criteria, such
as statement and branch coverage [26], function coverage [7 ],
[9], block coverage [6], modiﬁed condition/decision cover -
age [13], method coverage [6] and statically-estimated met hod
coverage [20], [32]. There has also been work on incorporati ng
information on the probability of exposing faults into crit e-
ria [9]. There is research [15] on test-case prioritization using
coverage of system models, which can be acquired before the
coding phase. Mei et al. [22] investigate criteria based on
dataﬂow coverage [21] for testing service-oriented softwa re.
In this paper, we investigate generic strategies that can wo rk
with any coverage criteria.
Constraints . In practice, there are many constraints af-
fecting test-case prioritization. Elbaum et al. [8] and Par k et
al. [23] investigate the constraints of test cost and fault s everity.
Hou et al. [10] investigate the quota constraint on test-cas e
prioritization. Kim and Porter [14] investigate the resour ce
constraint that may not allow the execution of the entire tes t
suite. Walcott et al. [28] and Zhang et al. [33] investigate
time constraints that require the selection of a subset of te st
cases for prioritization. Do et al. [4] investigate the use o f
techniques not speciﬁc to time constraints in the presence o f
those constraints. For constraints that impact only the sel ection
of test cases, our strategies may also be applicable.
Usage Scenarios . Prioritized regression test cases can be
used for either a speciﬁc subsequent version or a number of
9200subsequent versions. Elbaum et al. [7], [9] refer to the form er
as version-speciﬁc prioritization and the latter as genera l
prioritization. Although the majority of research on test- case
prioritization focuses on techniques for general prioriti zation,
some researchers (such as Srivastava and Thiagarajan [27])
have investigated techniques for only version-speciﬁc pri oriti-
zation. There are also researchers (such as Elbaum et al. [7] ,
[9]) investigating the use of general test-case prioritiza tion
techniques in version-speciﬁc prioritization. Like other general
prioritization techniques, our generic test-case priorit ization
strategies may also be applicable in version-speciﬁc prior i-
tization. Furthermore, it should also be possible to develo p a
version-speciﬁc technique similar to Srivastava and Thiag ara-
jan’s technique by using our strategies on coverage of chang ed
code instead of all the code.
V. C ONCLUSION AND FUTURE WORK
In this paper, we show how the total andadditional strate-
gies can be seen as two extreme instances in models of
generic prioritization strategies. Naturally, there is a s pectrum
of generic strategies between the total andadditional strategies
in our models. We also proposed extensions to enable the
use of differentiated pvalues for methods. Empirical results
demonstrate that wide ranges of strategies in both our basic
and extended models are more effective than either the total or
theadditional strategies. Also, wide ranges of our strategies
using method coverage can be as effective as or more effectiv e
than the additional strategy using statement coverage. In a
broader sense, we view our results as a fundamental step
toward controlling the uncertainty of fault detection in te st-
case prioritization. In this sense, our models provide a new
dimension for creating better prioritization techniques.
In future work, we plan to address the difference between
recorded coverage information and actual coverage informa -
tion in test-case prioritization. We also plan to investiga te
adaptive test-case prioritization, which changes the prob ability
value of each unit based on actual fault revealing behavior
during test case prioritization in regression testing.
ACKNOWLEDGEMENTS
This research is sponsored in part by the National 973
Program of China No. 2009CB320703, the Science Fund for
Creative Research Groups of China No. 61121063, and the
National Natural Science Foundation of China under Grant
Nos. 91118004, 61228203, and 61272157. This research is
also supported by the US NSF through awards CCF-0845628,
CNS-0958231, CNS-0720757, and the Air Force Ofﬁce of
Scientiﬁc Research through award FA9550-10-1-0406.
REFERENCES
[1] J. H. Andrews, L. C. Briand, and Y . Labiche, “Is mutation a n appropriate
tool for testing experiments?” in ICSE , 2005, pp. 402–411.
[2] J. H. Andrews, L. C. Briand, Y . Labiche, and A. Siami Namin , “Using
mutation analysis for assessing and comparing testing cove rage criteria,”
TSE, vol. 32, no. 8, pp. 608–624, 2006.
[3] H. Do, S. Elbaum, and G. Rothermel, “Supporting controll ed experi-
mentation with testing techniques: An infrastructure and i ts potential
impact,” ESE, vol. 10, no. 4, pp. 405–435, 2005.[4] H. Do, S. Mirarab, L. Tahvildari, and G. Rothermel, “An em pirical
study of the effect of time constraints on the cost-beneﬁts o f regression
testing,” in FSE, 2008, pp. 71–82.
[5] H. Do and G. Rothermel, “On the use of mutation faults in em pirical
assessments of test case prioritization techniques,” TSE, vol. 32, no. 9,
pp. 733–752, 2006.
[6] H. Do, G. Rothermel, and A. Kinneer, “Empirical studies o f test case
prioritization in a JUnit testing environment,” in ISSRE , 2004, pp. 113–
124.
[7] S. Elbaum, A. Malishevsky, and G. Rothermel, “Prioritiz ing test cases
for regression testing,” in ISSTA , 2000, pp. 102–112.
[8] S. Elbaum, A. Malishevsky, and G. Rothermel, “Incorpora ting varying
test costs and fault severities into test case prioritizati on,” in ICSE , 2001,
pp. 329–338.
[9] S. Elbaum, A. Malishevsky, and G. Rothermel, “Test case p rioritization:
A family of empirical studies,” TSE, vol. 28, no. 2, pp. 159–182, 2002.
[10] S.-S. Hou, L. Zhang, T. Xie, and J. Sun, “Quota-constrai ned test-case
prioritization for regression testing of service-centric systems,” in ICSM ,
2008, pp. 257–266.
[11] D. Jeffrey and N. Gupta, “Improving fault detection cap ability by
selectively retaining test cases during test suite reducti on,” TSE, vol. 32,
no. 2, pp. 108–123, 2007.
[12] B. Jiang, Z. Zhang, W. K. Chan, and T. H. Tse, “Adaptive ra ndom test
case prioritization,” in ASE, 2009, pp. 257–266.
[13] J. A. Jones and M. J. Harrold, “Test-suite reduction and prioritization
for modiﬁed condition/decision coverage,” in ICSM , 2001, pp. 92–101.
[14] J. M. Kim and A. Porter, “A history-based test prioritiz ation technique
for regression testing in resource constrained environmen ts,” in ICSE ,
2002, pp. 119–129.
[15] B. Korel, L. Tahat, and M. Harman, “Test prioritization using system
models,” in ICSM , 2005, pp. 559–568.
[16] Z. Li, M. Harman, and R. Hierons, “Search algorithms for regression
test case prioritisation,” TSE, vol. 33, no. 4, pp. 225–237, 2007.
[17] S. Lin, “Computer solutions of the travelling salesman problem,” Bell
System Technical Journal , vol. 44, no. 5, pp. 2245–2269, 1965.
[18] Y .-S. Ma, J. Offutt, and Y . R. Kwon, “MuJava : An automate d class
mutation system,” STVR , vol. 15, no. 2, pp. 97–133, 2005.
[19] T. McCabe, “A complexity measure,” TSE, no. 4, pp. 308–320, 1976.
[20] H. Mei, D. Hao, L. Zhang, L. Zhang, J. Zhou, and G. Rotherm el, “A
static approach to prioritizing junit test cases,” TSE, vol. 38, no. 6, pp.
1258–1275, 2012.
[21] L. Mei, W. K. Chan, and T. H. Tse, “Data ﬂow testing of serv ice-oriented
workﬂow applications,” in ICSE , 2008, pp. 371–380.
[22] L. Mei, Z. Zhang, W. K. Chan, and T. H. Tse, “Test case prio ritization for
regression testing of service-oriented business applicat ions,” in WWW ,
2009, pp. 901–910.
[23] H. Park, H. Ryu, and J. Baik, “Historical value-based ap proach for
cost-cognizant test case prioritization to improve the eff ectiveness of
regression testing,” in SSIRI , 2008, pp. 39–46.
[24] X. Qu, M. B. Cohen, and G. Rothermel, “Conﬁguration-awa re regression
testing: an empirical study of sampling and prioritization ,” in ISSTA ,
2008, pp. 75–86.
[25] G. Rothermel, M. J. Harrold, J. von Ronne, and C. Hong, “E mpirical
studies of test-suite reduction,” STVR , vol. 12, no. 4, pp. 219–249, 2002.
[26] G. Rothermel, R. H. Untch, C. Chu, and M. J. Harrold, “Tes t case
prioritization: an empirical study,” in ICSM , 1999, pp. 179–188.
[27] A. Srivastava and J. Thiagarajan, “Effectively priori tizing tests in devel-
opment environment,” in ISSTA , 2002, pp. 97–106.
[28] K. R. Walcott, M. L. Soffa, G. M. Kapfhammer, and R. S. Roo s, “Time
aware test suite prioritization,” in ISSTA , 2006, pp. 1–11.
[29] L. J. Williams and H. Abdi, “Fisher’s least signiﬁcance difference (LSD)
test,” in Encyclopedia of Research Design . Thousand Oaks, 2010, pp.
491–494.
[30] W. Wong, J. Horgan, S. London, and H. Agrawal, “A study of effective
regression testing in practice,” in ISSRE , 1997, pp. 230–238.
[31] S. Yoo, M. Harman, and S. Ur, “Measuring and improving la tency to
avoid test suite wear out,” in ICSTW , 2009, pp. 101–110.
[32] L. Zhang, J. Zhou, D. Hao, L. Zhang, and H. Mei, “Prioriti zing JUnit test
cases in absence of coverage information,” in ICSM , 2009, pp. 19–28.
[33] L. Zhang, S.-S. Hou, C. Guo, T. Xie, and H. Mei, “Time-awa re test-
case prioritization using integer linear programming,” in ISSTA , 2009,
pp. 213–224.
10201