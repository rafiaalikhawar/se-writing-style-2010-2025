Bayesian Inference using Data Flow Analysis
Guillaume Claret
INRIA, France
guillaume@claret.meSriram K. Rajamani
Microsoft Research India
sriram@microsoft.comAditya V. Nori
Microsoft Research India
adityan@microsoft.com
Andrew D. Gordon
Microsoft Research
Cambridge, UK
adg@microsoft.comJohannes BorgstrÃ¶m
Uppsala University, Sweden
johannes.borgstrom@it.uu.se
ABSTRACT
We present a new algorithm for Bayesian inference over
probabilistic programs, based on data ow analysis tech-
niques from the program analysis community. Unlike exist-
ing techniques for Bayesian inference on probabilistic pro-
grams, our data ow analysis algorithm is able to perform in-
ference directly on probabilistic programs with loops. Even
for loop-free programs, we show that data ow analysis of-
fers better precision and better performance benets over
existing techniques. We also describe heuristics that are
crucial for our inference to scale, and present an empirical
evaluation of our algorithm over a range of benchmarks.
Categories and Subject Descriptors
D.2.4 [ Software Engineering ]: Software/Program Veri-
cation| Statistical methods
General Terms
Algorithms, Verication
Keywords
probabilistic programming, algebraic decision diagrams,
data ow analysis
1. INTRODUCTION
We present a data ow analysis for probabilistic programs,
which can be used to perform Bayesian inference. Before
delving into details of the analysis, we rst give the reader
some background on probabilistic programs and Bayesian
inference.
Probabilistic programs are \usual" programs (written in
languages like C or Java or LISP or ML) with two added con-
structs: (1) the ability to draw values at random from distri-
butions, and (2) the ability to condition values of variables
in a program through observations. A variety of probabilis-
tic programming languages and systems have been proposed
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proï¬t or commercial advantage and that copies
bear this notice and the full citation on the ï¬rst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciï¬c
permission and/or a fee.
ESEC/FSE â€™13, August 18-26, 2013, Saint Petersburg, Russia
Copyright 13 ACM 978-1-4503-2237-9/13/08 ...$15.00.[13,14,19,21,26,28]. However, unlike\usual"programs which
are written for the purpose of being executed, the purpose of
a probabilistic program is to implicitly specify a probability
distribution. Bayesian inference is the problem of comput-
ing an explicit representation of the probability distribution
implicitly specied by a probabilistic program.
Probabilistic programs can be used to represent proba-
bilistic graphical models [20], which use graphs to denote
conditional dependences between random variables. Prob-
abilistic graphical models are widely used in statistics and
machine learning, with diverse application areas including
information extraction, speech recognition, computer vision,
coding theory, biology and reliability analysis. They allow
specication of dependences between random variables via
generative models, as well as conditioning of random vari-
ables using phenomena or data observed in the real world. A
variety of ecient inference algorithms have been developed
to analyze and query probabilistic graphical models.
Inference algorithms for probabilistic programs are
broadly classied into: (1) dynamic methods such as the
Gibbs sampling algorithm, the Metropolis-Hastings (MH)
algorithm, which involve executing the program with ran-
dom draws and computing statistics on the resulting data,
and (2) static methods such as message-passing and be-
lief propagation. Current approaches to performing static
inference on probabilistic programs involve compiling such
programs to graphical models such as Bayesian networks
or factor graphs, and using known inference techniques on
such models. For instance, in [3], a functional probabilis-
tic program is rst translated into a factor graph, and In-
fer.NET [26] is used to analyze the resulting factor graph
and perform inference.
We propose a new direction for ecient static inference of
probabilistic programs based on techniques from data ow
analysis. Our \data ow facts" are probability distributions,
and our analysis merges data ow facts at join points, and
computes xpoints in the presence of loops. However, we
show that our data ow analysis does not lose precision,
and performs exact Bayesian inference (see Theorem 1).
Our approach is fundamentally dierent from sampling
algorithms, which use multiple concrete executions to rep-
resent distributions approximately using a set of samples,
and from message-passing algorithms, which maintain rep-
resentations of approximate distributions. Performing prob-
abilistic inference using data ow analysis oers several ad-
vantages. Prior techniques for static inference are restricted
to loop-free programs [3, 11]. We are able to statically an-
alyze probabilistic programs with loops using the idea ofPermission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proï¬t or commercial advantage and that copies bear this notice and the full citation
on the ï¬rst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speciï¬c permission and/or a
fee. Request permissions from Permissions@acm.org.
ESEC/FSEâ€™13 , August 18â€“26, 2013, Saint Petersburg, Russia
Copyright 2013 ACM 978-1-4503-2237-9/13/08...$15.00
http://dx.doi.org/10.1145/2491411.2491423
92
bool c1, c2;
c1 := Bernoulli(0.5);
c2 := Bernoulli(0.5);bool c1, c2;
c1 := Bernoulli(0.5);
c2 := Bernoulli(0.5);
observe(c1 || c2);
Example 1. Example 2.
Figure 1: Two probabilistic programs.
bool b, c;
b := true;
c := Bernoulli(0.5);
while (c){
b := !b;
c := Bernoulli(0.5);
}bool c1, c2;
c1 := Bernoulli(0.5);
c2 := Bernoulli(0.5);
while !(c1 || c2) {
c1 := Bernoulli(0.5);
c2 := Bernoulli(0.5);
}
Example 3. Example 4.
Figure 2: Probabilistic programs with loops.
xpoints from the program analysis and verication com-
munities. Even for loop-free programs, we show that data
ow analysis oers performance benets over existing tech-
niques. We are able to perform exact inference, and hence
compute an answer with better precision than current static
techniques which use approximate distributions to scale.
1.1 Probabilistic Programs
We motivate probabilistic programs and inference using
a series of examples. Consider Example 1 in Figure 1. In-
tuitively, this program tosses two fair coins (simulated by
drawing from a Bernoulli random variable with mean 0.5),
assigns the outcomes of these coin tosses to c1and c2re-
spectively, and returns the values of the two variables c1and
c2. The program represents a probability distribution over
Bernoulli variables c1and c2, where:
Pr(c1=false ,c2=false ) =Pr(c1=false ,c2=true) =
Pr(c1=true,c2=false ) =Pr(c1=true,c2=true) = 1/4.
Next, consider Example 2 in Figure 1. In this pro-
gram, in addition to tossing the two coins and assign-
ing the outcomes to c1and c2, we have the statement
observe (c1jjc2). The semantics of the observe statement
classies runs which satisfy the boolean expression c1jjc2
asvalid runs. Runs that do not satisfy c1jjc2are classied
asinvalid runs. The program species the generated distri-
bution over the values of the variables ( c1;c2) conditioned
over valid runs, which is given by: Pr(c1=false ,c2=false )
= 0, and Pr(c1=false ,c2=true) =Pr(c1=true,c2=false )
=Pr(c1=true,c2=true) = 1/3.
Next, we consider probabilistic programs with loops. Con-
sider Example 3 in Figure 2. This program initializes b
totrue and cto the outcome of tossing a coin. Then,
it loops until cbecomes false , toggling band assigning
tocthe result from a fresh coin-toss in every iteration of
the loop. The while-loop terminates with probability 1,
since for the loop to not terminate, cshould be always as-
signed true from the coin toss, the probability of which de-
creases exponentially with the number of iterations. The
program species the generated distribution over the values
of the variables ( b;c) given by: Pr(b=true,c=true) = 0,
andPr(b=false ,c=true) = 0, and Pr(b=true,c=false ) =
2/3, and Pr(b=false ,c=false ) = 1/3.
The probability Pr(b=true;c=false ) is the proba-
bility that the program executes the while loop an even
number of times, and is given by the summation (1 =2) +float skillA, skillB,skillC;
float perfA1,perfB1,perfB2,
perfC2,perfA3,perfC3;
skillA := Gaussian(100,10);
skillB := Gaussian(100,10);
skillC := Gaussian(100,10);
// first game:A vs B, A won
perfA1 := Gaussian(skillA,15);
perfB1 := Gaussian(skillB,15);
observe(perfA1 > perfB1);
// second game:B vs C, B won
perfB2 := Gaussian(skillB,15);
perfC2 := Gaussian(skillC,15);
observe(perfB2 > perfC2);
// third game:A vs C, A won
perfA3 := Gaussian(skillA,15);
perfC3 := Gaussian(skillC,15);
observe(perfA3> perfC3);
Figure 3: (Example 5) TrueSkill skill rating.
(1=8) + (1=32) +, which equals 2 =3. The probability
Pr(b=false;c=false ) is the probability that the pro-
gram executes the while loop an odd number of times, and
is given by the summation (1 =4) + (1=16) + (1=64) +,
which equals 1 =3.
Consider Example 4 in Figure 2. This program repeat-
edly assigns to c1and c2outcomes of fair coin tosses, in
a loop, until the condition ( c1jjc2) becomes true. Thus,
this program species the generated distribution over the
variables ( c1;c2) given by: Pr(c1=false ,c2=false ) = 0,
and Pr(c1=false ,c2=true) = Pr(c1=true,c2=false ) =
Pr(c1=true,c2=true) = 1/3. The alert reader would no-
tice that this distribution is identical to the distribution
specied by Example 2. Though observe statements can
be equivalently represented using while loops using a sim-
ple program transformation illustrated in this example, our
inference algorithm handles observe statements more e-
ciently, when compared to loops. Also, there is no simple
transformation that converts any while loop to an observe
statement. Consequently, we have both observe statements
and while loops in our language.
The nal example we use in this introduction is a simpli-
ed version of the TrueSkill [16] skill rating system used by
Microsoft's Xbox Live to rate the relative skills of players
playing online games. In Example 5 (Figure 3), we have
3 players, A, B and C, whose skills are given by variables
skillA ,skillB and skillC respectively, which are initial-
ized by drawing from a Gaussian distribution with mean
100, and variance 10. Based on the outcomes of some num-
ber of played games (which is 3 games in this example),
we condition the skills thus generated. The rst game was
played between A and B, and A won the game. This is
modeled by assigning to the two random variables perfA1
and perfB1 denoting the performance of the two players in
the rst game, and constraining that perfA1 is greater than
perfB1 using an observe statement. Note that the per-
formance of a player (such as player A) is a function of her
skill, but with additional Gaussian noise introduced in order
to model the variation in performance we may see because of
incompleteness in our model (such as the amount of sleep the
player got the previous night). The second game was played
between B and C, and B won the game. The third game was
played between A and C, and A won the game. Using this93model, we want to calculate the joint probability distribu-
tion of these random variables, and use this to estimate the
relative skills of the players. Note that each observe state-
ment constrains performances in a game, and implicitly the
skills of the players, since performances depend on skills.
Such a rating can be used to give points, or match players
having comparable skill for improved gaming experience. In
this example, the skills skillA ,skillB , and skillC inferred
by our tool are: skillA =Gaussian (102:1;7:8),skillB =
Gaussian (100:0;7:6),skillC =Gaussian (97:9;7:8). Note
that since A won against both B and C, and B won against
C, the resulting distribution agrees with our intuitive assess-
ment of their relative skills.
1.2 Inference
Calculating the distribution specied by a probabilistic
program is called inference . The inferred probability dis-
tribution is called posterior probability distribution, and the
initial guess made by the program is called the prior prob-
ability distribution. For instance, in Example 5, the prior
distribution for skillA isGaussian (100,10), whereas the
posterior distribution is Gaussian (102.1, 7.8). One way to
perform inference is runtime execution. We can execute the
program several times using sampling to execute probabilis-
tic statements, and observe the values of the desired vari-
ables in valid runs [14], and compute statistics on the data
to infer an approximation to the desired distribution. Al-
ternatively, a probabilistic program can be compiled to a
graphical model [3,21] over which inference is performed us-
ing message passing algorithms such as belief propagation
and its variants [27].
Data ow analysis, invented by Kildall [18], uses a lattice
of data ow facts, merging at join points, and xpoints for
loops to compute solutions to several \meet-over-all-paths"
(MOP) analysis problems. The main contribution of this pa-
per is a new technique to perform inference on probabilistic
programs using data ow analysis.
We consider Boolean Probabilistic Programs, and in par-
ticular the programming language BernoulliProb (see
Section 3), where all variables are boolean and the only
distribution allowed is the Bernoulli distribution. This lan-
guage is similar to the pWhile language [2], which in turn
is based on the probabilistic while language in [22]. Our
main formal result, Theorem 1, is the correctness of our in-
ference algorithm with respect to the formal semantics of
our probabilistic language.
Probabilistic programs with discrete variables over a -
nite domain can be directly encoded with BernoulliProb ,
without any approximation. Further, probabilistic programs
with continuous variables can be approximated to boolean
programs by approximating continuous distributions with
discrete distributions (see Section 4). Using such transfor-
mations, any probabilistic program can be approximately
represented in BernoulliProb and analyzed using our
technique.
We have implemented our approach and we nd that in
several examples, we are able to perform exact inference. If
the probabilistic program has a large number of variables,
explicit representation of joint probability distributions is
expensive. Our implementation uses Algebraic Decision Di-
agram (ADD) [1], a graphical data structure for compactly
representing nite functions, to represent probability distri-
butions, and perform data ow analysis symbolically. Forr2R
x2Vars
T ::= bool types
uop ::= not unary operators
bop ::= andjor binary operators
D ::=jTx1;x2;:::;xn declaration
E ::= expressions
x variable
jc constant
jE1bopE2 binary operation
juopE1 unary operation
S ::= statements
x:=E deterministic assignment
jx:=Bernoulli (r) Bernoulli assignment
jobserve (E) observe
jskip skip
jS1;S2 sequential composition
jifEthenS1elseS2conditional composition
jwhileEdoS1 loop
P ::=DS programs
Figure 4: Syntax of BernoulliProb.
large examples, where exact inference is infeasible, we pro-
pose a batching technique where we periodically project the
joint distribution to marginal distributions over individual
variables, to save space at the cost of some approximation.
In our experiments, our approximate inference produces re-
sults with better precision than other state-of-the-art infer-
ence approaches.
This work is related thematically to our other recent work
in the intersection of program analysis and Bayesian infer-
ence. In particular, related recent eorts include using weak-
est preconditions to perform ecient sampling [6] and using
a framework of model-learners to do Bayesian reasoning [15].
2. PROBABILISTIC PROGRAMS
We start by formally dening Boolean probabilistic pro-
grams. Figure 4 shows the syntax of BernoulliProb . The
only type T allowed in the language is the boolean type,
with values true and false . A program has a declaration
of variables x1;x2;:::;xnfollowed by statements. We use
V(P) to denote the variables of program P, andS(P) to
denote the statement body of program P. Primitive state-
ments include deterministic assignments, Bernoulli assign-
ment, observe and skip statements. A deterministic assign-
ment is of the form x:=E, whereEis an expression. Expres-
sions are formed from variables and constants using binary
and unary operators. A Bernoulli assignment is of the form
x:=Bernoulli (r), whereris a real number. An observe
statement is of the form observe (E), whereEis an expres-
sion. A skip statement is of the form skip. Compound
statements are formed from primitive statements using se-
quential composition, conditional composition and looping.
The operational semantics of BernoulliProb is given in
Figure 5 as a probabilistic transition system. A stateof
the program with variables x1;x2;:::;xnis a valuation to
all the variables. The domain of all possible states is  . A
conguration !is a pairh;Si, whereis a state, andSis a
statement. Intuitively, a run of a program returns the nal
state0, which is the rst component of the conguration on
termination. Since programs are probabilistic, each time we94h;x:=Ei!1h[x (E)];skipi
h;x:=Bernoulli (r)i!rh[x true];skipi
h;x:=Bernoulli (r)i!1 rh[x false ];skipi
h;observe (E)i!1h;skipi;if(E) =true
h;skip;Si!1h;Si
h;S1;S2i!ph0;S0;S2i;ifh;S1i!ph0;S0i
h;ifEthenS1elseS2i!1h;S1i;if(E) =true
h;ifEthenS1elseS2i!1h;S2i;if(E) =false
h;whileEdoSi!1h;skipi;if(E) =false
h;whileEdoSi!1h;S;whileEdoSi;if(E) =true
Figure 5: Semantics of BernoulliProb.
run a program Pwe might get a dierent nal state. The
semantics of the program is the distribution over nal states
returned by the program. We formalize this below.
Given a state , we use the notation (xi) to denote the
value of variable xiin, and the notation (E) to denote
the value of the expression Ein.
The probabilistic transition system shown in Figure 5 has
transition rules of the form form !!p!0, meaning cong-
uration!takes a step to conguration !0with probability
p, inspired by the transition system for the functional lan-
guage Fun [3]. We use the notation [x v] for the state
obtained by updating the value of xinwithvand leav-
ing the values of all other variables in unchanged. The
only transition in Figure 5 whose probability is not 1, is the
one for Bernoulli assignment x:=Bernoulli (r). This state-
ment assigns true toxwith probability rand, false tox
with probability 1  r. The conguration h;observe (E)i
transitions to the conguration h;skipiwith probability 1
if(E) is equal to true. In this case, we say that the obser-
vation succeeds . Otherwise, if (E) is equal to false , then
the conguration h;observe (E)igets stuck with no outgo-
ing transitions. Thus, implicitly, the resulting distribution
of a program is conditioned on all observations succeeding.
The sequential composition S1;S2transitions depending
on the transition of the rst statement S1. The conditional
composition ifEthenS1elseS2transitions according to
how the current state evaluates the expression E. The
while loop whileEdoSalso transitions according to how the
current state evaluates the expression E. If the expression
evaluates to false the loop exits, otherwise it executes the
bodySand loops.
Arunof a statementSstarting from state and end-
ing in state 0is a sequence != (!1;!2;:::;!n+1) for
n0, where the following conditions are satised: (1)
!1!p1!2!n!pn!n+1, (2) the initial congura-
tion!1=h;Si, and (3) the nal conguration !n+1=
h0;skipi, where the statement part is equal to skip, sig-
nifying termination of execution. Given such a run !, we
say that the statement Sevaluates to state 0starting at
statewith probability Pr( !) =p1pn. The set of all
runs of the statement Sstarting from state and ending
with state 0is denoted by 
( ;S;0). Note that we are
only concerned with runs of nite length in 
( ;S;0) that
end in a conguration with a skip statement. Since the lan-
guage BernoulliProb has loops, the number of such runs
is potentially innite.
Given a statement S, the probability Pr( ;S;0) of exe-
cuting statement Sstarting at state and ending at state
0is given by
Pr(;S;0) = !2
(;S;0)Pr(!)Algorithm 1 TheInfer algorithm.
Algorithm Infer
Input: ABernoulliProb programP.
Output: A posterior distribution overP's output.
1:0:=:ite(8xi2V(P):(xi) =false;1;0);
2::=Post(0;S(P))
3:N:=Normalize ()
4:returnN
Note that even though the summation is potentially over an
innite number of runs, we are interested in its limit which is
always lesser than one. Given a distribution over program
states  , we dene
Pr(;S;0) = 2 ()Pr(;S;0)
Consider a program Pwith variables x1;x2;:::;xnand
statementS. Let0denote the state where all variables
x1;x2;:::;xnare assigned false , and0denote the Dirich-
let distribution over states :ite(=0;1;0), where the
expression ite(e;x;y ) evaluates to xifeistrue, andyife
isfalse . Intuitively, the semantics of the program Pis the
probability distribution obtained by starting the execution
ofSfrom initial state o. Formally, the semantics of Pis
the distribution :Pr(0;S;).
We note that the semantics given here sums over all the
runs of the program. In Section 3, we present our main
result of the paper, which shows that this summation can
be computed by a data ow analysis which merges data ow
facts at join points.
We end this section by noting two facts about the seman-
tics of BernoulliProb programs. First, if a program P
has non-trivial observe statements, the resulting distribu-
tion:Pr(0;S;) is not necessarily normalized (i.e., the
sum of the probabilities over all states can be strictly less
than 1). If we wanted a distribution, we can calculate the
sum of probabilities Sof all the states and appropriately
normalize each of the probabilities by S. Second, we can
write pathological programs containing statements such as
observe (false ) or non-terminating while loops with no ter-
minating executions. The output distribution of such a pro-
gram maps all states to 0 probability, which is equivalent to
saying that the semantics of such a program is undened.
3. ALGORITHM
Algorithm 1 describes the inference algorithm Infer for
BernoulliProb programs, which is based on data ow
analysis. Infer takes a BernoulliProb programPas in-
put and returns the joint distribution over the output states
ofP(see Section 2). Line 1 constructs an initial distribu-
tion0, which is a Dirichlet distribution mapping the state
with every variable set to false as having probability 1,
and all other states as having probability 0. Recall that
the function ite(e;x;y ) evaluates to xife=true andyif
e=false . The procedure Post (line 2) returns the poste-
rior sub-distribution over the output of statements S(P) of
programP, starting with 0as the input distribution. Post
is a recursive procedure shown in Algorithm 2.
The function Normalize takes the output of Post , which
is a function from output values of Pto [0;1], and if
range()6=f0git returns a probability distribution Nover
output values, given by (1 =v2 (v))(recall that   is the
domain of all possible states).95Algorithm 2 ThePost computation.
Algorithm Post(;S)
Input: An input distribution over the states of the program P, and
a statementS
Output: Output distribution over the states of the program P
1:switch (S)
2:casex:=E:
3: return:f0j0[x 0(E)]=g(0)
4:casex:=Bernoulli (r):
5: return:(rf0j0[x true]=g(0)+
6: (1 r)f0j0[x false]=g(0))
7:case observe (E):
8: return:ite((E);();0)
9:case skip:
10: return
11:caseS1;S2:
12:0=Post(;S1);
13: return Post(0;S2)
14:case ifEthenS1elseS2:
15:t=:ite((E);();0);
16:f=:ite((E);0;());
17: return:(Post(t;S1)() +Post(f;S2)())
18:case whileEdoS1:
19:p:=?;c:=
20: whilep6=cdo
21:p:=c22:c:=Post(p;ifEthenS1else gskip )
23: end while
24: return:ite((E);0;c())
25:end switch
Algorithm 2 relies on notations introduced in Section 2 for
valuations,[x v], and values (x),(E). Distributions
,c, etc (normalized or un-normalized) map valuations to
[0;1]. Let?denote the null map which maps every valuation
to 0.
Post operates recursively over the syntax of an input
BernoulliProb statementS(as dened in Figure 5). In
lines 2{3, Post handles the case when the statement is a
deterministic assignment. In this case, the output distribu-
tion maps a state to the sum over all the input densities of
states0that equalon executing the deterministic assign-
ment. Lines 4{6 handle Bernoulli assignment. The output
distribution for this statement is a convex combination of
the result of the deterministic assignment x:=true scaled
byr, and the deterministic assignment x:=false scaled
by 1 r. Lines 7{8 handle the observe statement. The out-
put distribution (which is unnormalized) maps a state to
the density over the input distribution if the expression E
evaluates to true and 0 otherwise. Note that the output
distribution here is unnormalized, unless Eistrue for all
states. Lines 9{10 handle the skip statement, which is an
identity for Post .
Lines 11{12 handle sequential composition by rst com-
puting Post over the rst statement, and using the resulting
distribution to compute Post over the second statement.
Lines 14{17 handle conditional statements. The output dis-
tribution is the pointwise sum of the distribution obtained
from the \if-part" and the \else-part". The \if-part" and
\else-part" are recursively computed by applying Post on
their bodies, after splitting the input distribution depend-
ing on the condition predicate Etotandf.
The nal case (lines 18{24) handles a while loop by com-
puting a xpoint. It uses two scratch variables pand
cto represent the \previous" distribution and \current"
distribution respectively. Iteratively, Post is repeatedly
applied on the input distribution pwith the statement
ifEthenS1else skip until the output distribution cis the same as the input p. We note that this xpoint is
potentially nonterminating, even though pandcmay con-
verge in the limit. In our implementation we terminate the
xpoint when the KL-divergence between candpgoes
below a certain threshold.
Readers familiar with data ow analysis will note that
our data ow facts are probability distributions, and our
algorithm merges these distributions at join points (lines 14{
17), and computes xpoints for loops (lines 18{24). Next,
we prove that even with such merging of data ow facts
at join points, this algorithm computes the exact posterior
distribution given by the \sum over all paths" denition in
Section 2.
Lemma 1.For any statement Sand any distribution ,
if the Post algorithm terminates, then:
:Pr(;S;) =Post (;S)
Proof: We show that for any statement Sover variables
x1;x2;:::;xn, any input distribution over the program
states, and any output state , we have that Pr( ;S;) =
Post (;S)() when the Post algorithm terminates. The
proof is by induction over the structure of S, and we carry
it out by performing a case analysis of all types of statements
supported in BernoulliProb .
Case 1(deterministic assignment):
IfS=x:=E, we have that
Pr(;S;) = 12 (1)Pr(1;S;)
= f12 j=1[x 1(E)]g(1)Pr(1;S;)+
f12 j6=1[x 1(E)]g(1)Pr(1;S;)
= f12 j=1[x 1(E)]g(1)1+
f12 j6=1[x 1(E)]g(1)0
(from Figure 5)
= f12 j=1[x 1(E)]g(1)
=Post (;x:=E)()
(from line 3 of Algorithm 2)
Case 2(Bernoulli assignment):
IfS=x:=Bernoulli (r), we have that
Pr(;S;) = 12 (1)Pr(1;S;)
= f12 j=1[x true]g(1)Pr(1;S;)+
f12 j6=1[x false ]g(1)Pr(1;S;)
= f12 j=1[x true]g(1)r+
f12 j6=1[x false ]g(1)(1 r)
(from Figure 5)
=Post (;x:=Bernoulli (r))()
(from lines 5{6 of Algorithm 2)
Case 3(Observe statement):
IfS=observe (E), we have that
Pr(;S;) = 12 (1)Pr(1;S;)
= f12 jE(1)=trueg(1)Pr(1;S;) +
f12 jE(1)=falseg(1)Pr(1;S;)
= f12 jE(1)=trueg(1)ite(=1;1;0) +
f12 jE(1)=falseg(1)0
(from Figure 5)
=ite(E();();0)
=Post (;observe (E))()
(from line 8 of Algorithm 2)96Case 4(Skip statement):
ifS=skip, we have that
Pr(;S;) = 12 (1)Pr(1;S;)
=()Pr(;S;) +
f12 j16=g(1)Pr(1;S;)
=()1 + f12 j16=g(1)0
(from Figure 5)
=()
=Post (;skip)()
(from line 10 of Algorithm 2)
Case 5(Sequential composition):
IfS=S1;S2, we have that
Pr(;S;) = 12 (1)Pr(1;S;)
= 12 (1)22 Pr(1;S1;2)Pr(2;S2;)
(from Figure 5)
= 12 22 (1)Pr(1;S1;2)Pr(2;S2;)
= 22 12 (1)Pr(1;S1;2))Pr(2;S1;)
= 22 Pr(;S1;2)Pr(2;S1;)
= 22 Post (;S1)(2)Pr(2;S1;)
(by induction)
= Pr( Post (;S1);S2;)
=Post (;S1;S2)()
(from lines 12{13 of Algorithm 2)
Case 6(Conditional composition):
IfS=ifEthenS1elseS2we have that
Pr(;S;) = 12 (1)Pr(1;S;)
= f12 jE(1)=trueg(1)Pr(1;S1;) +
f12 jE(1)=falseg(1)Pr(1;S2;)
(from Figure 5)
= 12 ite(E(1);(1);0)Pr(1;S1;) +
12 ite(E(1);0;(1))Pr(1;S2;)
=Post (1:ite(E(1);(1);0);S1)() +
Post (1:ite(E(1);0;(1));S2)()
(by induction)
=Post (;ifEthenS1elseS2)()
(from lines 15{17 of Algorithm 2)
Case 7(While loop):
IfS=whileEdoS1we have that
Pr(;S;) = 12 (1)Pr(1;S;)
= f12 jE(1)=falseg(1)+
f12 jE(1)=trueg(1)Pr(1;S1;S;)
(from Figure 5)
= Pr(;(ifEthenS1;Selse skip );)
=Post (;whileEdoS1)()
(from lines 19{24 of Algorithm 2)
Our main theorem, which states that the data ow anal-
ysis algorithm computes the exact posterior distribution as
specied by the \sum over all paths" semantics is stated be-
low, and follows directly from the above lemma.
Theorem 1.LetPbe aBernoulliProb program with
nvariablesx1;x2;:::;xnand a statement S. Let obe a
state which assigns all nvariables the value false , and let
obe the Dirichlet distribution which maps such a state oto
probability 1 and all other states to the probability 0. Then, if
thePost algorithm terminates, then the output distribution
ofPis given by the Infer algorithm.It is important to note that the Post algorithm might not
always terminate. Consider the following example program:
bool x := true;
while (true)
x := !x
return x;
In this program xis initialized to true, and toggles in every
iteration of the loop. Thus, the probability distribution c
computed by the Post algorithm while analyzing the while
loop is given by :ite((x);1;0) at the end of even number
of iterations of the loop, and :ite((x);0;1) at the end
of odd number of iterations of the loop. Hence, the Post
algorithm does not terminate on this example. This example
is reminiscent of Markov chains with periodic cycles. In
Section 6, we show that in several real-world examples where
such oscillating behaviors do not occur, the Post algorithm
is indeed able to terminate and produce useful results.
4. DISCRETIZATION
We extend BernoulliProb to handle continuous distri-
butions. There are two phases. First, we extend Bernoul-
liProb to support discrete distributions over nite sets.
Next, we show how we can approximate a continuous distri-
bution as a discrete distribution over a nite set, which eec-
tively shows that a probabilistic program dened over con-
tinuous distributions can be approximated by a Bernoul-
liProb program.
LetDbe a distribution over elements of a nite set Sof
cardinalityjSj>0. Then, we can encode elements of Susing
tuples of logjSjboolean variables in the standard way (that
is, using boolean tuples in f0;1glogjSj. Thus, using this en-
coding, we are able to model Das a distribution over tuples
of boolean variables, thus reducing a program dened over
arbitrary nite distributions to a BernoulliProb program.
Now consider a continuous distribution N. For ease of
exposition, we assume that Nis a Gaussian distribution
whose probability density function is dened as follows
f(x;;2) =1p
2e (x )2
22
whereand2are the mean and variance parameters re-
spectively. For a suitable choice i2N, dene an interval
[a;b] such that a= iandb=+i. For some w>0,
dene the following set.
S=
a+kb a
wj0k<w
We dene a discrete distribution Dover the elements of
Swhich is a discrete probability mass function that approx-
imates the Gaussian probability density function f(x;;2)
as follows.
D(x2S;i;w) =1
NZx+b a
w
xf(x;;2)dx
whereiandware parameters that control the degree of
approximation, and
N=Zb
af(x;;2)dx
a normalization constant ensuring that Dis a probability
distribution.97It is easy to see that the degree of approximation is con-
trolled by the parameters iandw. In particular, the ap-
proximation improves with i!1 andw!0.
Therefore, with the two reductions described above, we
are able to reduce probabilistic programs dened over con-
tinuous and nite distributions to BernoulliProb pro-
grams.
5. IMPLEMENTATION
Algebraic Decision Diagrams. Recall that our algorithm
(in Section 3) maintains joint probability distributions at ev-
ery program point. A probability distribution is a real val-
ued function over variable values. For example, after execut-
ing the fragment x=Bernoulli (0:5), we obtain the function
x:0:5 representing the distribution which maps both values
ofx(true and false ) to 0.5. As a second example, after
symbolically executing Example 2 (Figure 1), we obtain the
function(c1;c2):ite((c1jjc2);1=3;0).
These are functions from tuples of boolean values to real
numbers. One way to represent such functions is using ta-
bles (similar to truth tables, but having probabilities as the
range). However, with nboolean variables a table represen-
tation has 2nrows, and this is infeasible for large n. Alge-
braic Decision Diagrams (ADDs) can compactly represent
such functions as directed acyclic graphs. ADDs [1] are gen-
eralizations of Binary Decision Diagrams (BDDs), invented
by Bryant [5]. An ADD is a directed acyclic graph. Each
internal node of the graph is a decision node, labeled with a
variable name. Each leaf node is labeled with a real value.
Each internal node nhas two outgoing edges labeled with 0
and 1 respectively, and the target of these edges are called 0-
successor and 1-successor of nrespectively. Each ADD xes
a total order among its variables, and the ordering of vari-
ables in every path respects this total order. Furthermore,
each variable occurs at most once on a path from the root to
the leaf. ADDs can be compactly constructed from decision
trees by performing the following two reductions until sat-
uration: (1) merging isomorphic nodes, and (2) eliminating
nodes whose 0-successor and 1-successor are identical. Once
we x a total ordering of variables, and apply the above 2
reductions in any order till convergence, the resulting ADD
is canonical (regardless of the order in which the reductions
were applied).
Functions can be manipulated using graph algorithms on
their ADD representations. For example, if f1andf2are
two functions represented as ADDs with sizes jf1jandjf2j
respectively, the ADD for operations such as f1+f2orf1f2
can be obtained using graph algorithms on the ADDs of f1
andf2with a worst-case complexity of O(jf1jjf2j). Given
a functionfwith a free variable xrepresented as an ADD,
we can also obtain the ADD for 9x:fby eliminating xfrom
the ADD using graph operations, and potentially doubling
the size of the ADD in the worst case. We refer the reader
to [1] for details of these algorithms.
For instance, Figure 6 shows the ADD for the distribution
of Example 2 from Figure 1: part (a) shows the distribution
represented as a decision tree, and part (b) shows the ADD
obtained by applying the two reductions above until none
applies.
Our implementation of the inference algorithm uses ADDs
for a compact representation over distributions. Each of the
operations in the Algorithm 2 can be implemented using
ð’„ðŸ 
ð’„ðŸ 
ðŸðŸ‘  ðŸŽ ðŸŽ ðŸ 
0 1 ð’„ðŸ 
ð’„ðŸ 
ðŸðŸ‘  ðŸŽ 0 
ðŸ 
0 ðŸ ð’„ðŸ  
ðŸðŸ‘  ðŸðŸ‘  0 1 
(a)  Decision tree  (b)  ADD  Figure 6: ADD representation of Example 2.
ADD operations as described below. Consider the opera-
tion for processing the deterministic assignment statement
in line 3. Let fbe the ADD representation of and let
fx0=Ebe the ADD representing the relation x0=E, where
x0is a fresh variable. We implement the summation in line
3 as follows: (1) First we compute g=f^fx0=E. (2)
Next we existentially quantify xfromgto geth=9x:g.
(3) Finally, we rename x0toxinhto get the result of
Post ash[x0=x] and return the resulting ADD (we note that
^, existential quantication and renaming are implemented
by ADD packages using graph operations). The implemen-
tation for Bernoulli assignment can be done by separately
processing the two deterministic assignments x:=true and
x:=false as above, and scaling the two resulting ADDs by
rand (1 r) respectively, and adding them (we note that
scaling and adding operations provided by ADD packages).
The operations in lines 8, 15 and 16 can be directly imple-
mented on ADDs since iteoperation is supported by ADD
packages. The xpoint computation in lines 20{23 can be
implemented using the techniques described above, and in
addition we terminate the xpoint when the KL-divergence
betweencandpgoes below a certain threshold.
Each of these operations ^,ite, scaling, summation,
equality check are directly supported by ADD packages such
as CUDD [32] and takes time proportional to the product
of the sizes of the arguments in the worst case. The normal-
ization operation in Algorithm 1 is not directly supported
by ADD packages. We implement this operation using a
bottom-up scan of the ADD in time proportional to the size
of the ADD.
Figure 7 illustrates how the Infer algorithm proceeds on
Example 2 from Figure 1. In particular, the Post com-
putation on the rst two statements c1:=Bernoulli (0:5)
and c2:=Bernoulli (0:5) results in uniform distributions
over c1and ( c1;c2) respectively. Both these distributions
are compactly represented by ADDs with a single leaf node.
Next, Post processes the statement observe (c1jjc2) which
results in a subdistribution represented by the ADD shown
in the gure. Finally, Infer normalizes this subdistribution
(via the call to Normalize ) in order to obtain the nal ADD
representing the posterior distribution of ( c1;c2).98ð’„ðŸ 
ð’„ðŸ 
ðŸðŸ‘  ðŸŽ 0 
ðŸ 
0 ðŸ ðŸðŸ  
ð’„ðŸ 
ð’„ðŸ 
ðŸðŸ’  ðŸŽ 0 
ðŸ 
0 ðŸ 
 c1 = Bernoulli(0.5)  
observe(c1 || c2)  c2 = Bernoulli(0.5)  
Normalize  ðŸðŸ’  Figure 7: Sequence of ADDs obtained by applying
Post and Normalize to Example 2.
Our implementation uses the ADD library from the
CUDD package [32]. Our implementation supports a much
richer language than BernoulliProb , including continu-
ous distributions, oating point variables, multidimensional
array with statically determined sizes, for-loops, etc. Con-
tinuous distributions are automatically discretized using the
technique in Section 4, and Algorithm 2 is easily extended
to for-loops and static sized arrays.
Heuristics. We have implemented the following heuristics
for scaling and optimizing our implementation:
Fast exponentiation : To speed up the Post com-
putation over while loops we employ exponentia-
tion. We describe exponentiation by way of an
example. Consider the program whileEdoS
and an input distribution 0. Dene f() =
Post (;(ifEthenSelse skip )). Then the Post
algorithm computes the posterior distribution by ap-
plyingfto0until a xpoint is reached (in practice,
a xed constant number of times). If we can symboli-
cally represent f, then we can eciently compute f2n
by computing f2=ff,f4=f2f2;:::, and nally
evaluatef2n(0). This can be done as follows:
f() =0:X
()t(;0)
where
t(;0) := Pr(;ifEthenSelse skip;0)
With this denition of f, it is easy to compute f2as
follows:
f2() =0:X
()t2(;0)
wheret2(;0) :=P
00t(;00)t(00;0) Since the
functiontmaps program states to real numbers, it
can be compactly represented as an ADD. As a con-
sequence,t2can be eciently computed and it follows
thatf2nis also eciently computable.
Variable ordering : The size of an ADD crucially de-
pends on the ordering of variables used to construct it.
Our compiler implements well-known algorithms [24]using the program's variable dependency graph to de-
termine ADD variable ordering.
Batch processing : Given a joint probability distribu-
tionp(x1;x2;:::;xn) overnvariables, the marginal
distribution overxiis the projection of the joint dis-
tribution to that variable. Formally, if each of the
xiis a continuous variable ranging over values with a
lowerbound `and an upper bound u, the marginal dis-
tribution over a particular variable xi, denotedpi(xi)
is dened as:
Zu
`Zu
`p(x1;x2;:::;xn)dx1:::dxi 1dxi+1:::dxn
Computation of marginals for discrete distributions is
done by replacing the integrals with discrete summa-
tion. With ADDs, such summation is easily imple-
mented by existential quantication:
pi(xi) =9x1x2:::xi 1xi+1:::xn:p(x1;x2;:::;xn)
When the space required by the ADD for the
joint distribution p(x1;x2;:::;xn) becomes large,
we approximate the distribution using its projec-
tion to the tuple of marginal distributions as:
hp1(x1);p2(x2);:::;pn(xn)i. The latter representa-
tion, though more concise, loses information about cor-
relations between the variables, and is therefore less
precise. However, when exact inference runs out of
memory, this technique allows us to perform ecient
and approximate inference, with a very compact mem-
ory representation. We use marginalization to imple-
ment a heuristic called batch processing as follows. We
periodically replace the ADD for the joint distribution
by the component marginal distributions for batches
of data (for example, every nnames in TrueSkill , for
some value of n), and perform approximate inference.
6. EVALUATION
Benchmarks. We present empirical results from running
the inference algorithm on the following benchmarks1:
Students : This example is adapted from the advisor-
student examples in Markov Logic Networks [19]. We
have an array of mstudents,nteachers, and kcourses,
and we have information about which teacher is teach-
ing which course, and which student attends which
course. The probabilistic program for this example
models our belief that if a student sattends a course
ctaught by teacher t, then we can infer that slikest
(represented by a Bernoulli variable Likes(s;t)) is true
with a certain probability p. The goal is to infer the
posterior probability of every random variable in the
two-dimensional array Likes.
Friends : This benchmark performs probabilistic tran-
sitive closure. Given nstudents and an input friend-
ship matrix (this is an nnsymmetric matrix F
withF(a;b) = 1 if student ais a friend of student
b, and ? or unknown otherwise), we wish to compute
the set of all friends for each student. The proba-
bilistic program in this case encodes a probabilistic
1The source code for all benchmarks is available in [8].99unfairCoin(p) {
x := p;
b := true;
while (b) {
b := random(Bernoulli 0.5);
if (b)
x := 2 * x;
if (x >= 1.0)
x := x - 1;
else if (x >= 0.5)
x := 1;
else
x := 0;
}
return x;
}uniform(N) {
g := N;
while (g >= N) {
n := 1;
g := 0;
while (n < N) {
n := 2 * n;
if (random(Bernoulli(0.5)))
g := 2 * g;
else
g := 2 * g + 1;
}
}
return g;
}
(a) (b)
Figure 8: The unfairCoin and uniform benchmarks.
transitive closure constraint { that is, the constraint
(F(a;b) = 1)^(F(b;c) = 1))(F(a;c) = 1) holds
with a certain probability. The objective is to com-
plete the friends matrix Fconditioned on the above
constraint.
Compare : We have two n-bit numbers, where each bit
is drawn from a Bernoulli distribution. We want to
compute posterior probabilities of these distributions
conditioned on the observation that the two numbers
are unequal.
TrueSkill : This is the TrueSkill model [16] (the sim-
plied version of TrueSkill is shown in Figure 3). We
have an array of nplayers, each of whose skill is drawn
from a Gaussian distribution. When player iplays
with player j, we observe that the performance of
playeriin that game (another Gaussian with mean
given by the skill of player i), is greater than the per-
formance of player jin that game. Using observations
frommsuch games, we desire to infer the posterior
probability distributions for the skills of each player.
Loopy programs : The benchmarks OneCoin (Example
3 in Section 1), Dice,unfairCoin (Figure 8(a)) and
uniform (Figure 8(b)) are benchmarks that contain
loops. The benchmark Dice: Figure 8(a) and 8(b) are
examples with complex loops [17] that are beyond the
scope of existing probabilistic inference solvers. The
program unfairCoin(p) simulates a biased coin with
meanp. Its parameter is the number dof binary digits
used in the discretization of real numbers. For the
experiments we took an arbitrary value of p= 0:6. The
program uniform(N) simulates a uniform distribution
over the interval [0 ;N 1]. In order to demonstrate
the generality of our approach, we also consider the
benchmark MCthat is a program representing a Markov
chain dened by the following transition matrix:
2
40;9 0;05 0;05
0;7 0 0 ;3
0;8 0 0 ;23
5
Results. All experiments were performed on an 2.00GHz
Intel i7 processor system with 4GB RAM running Microsoft
Windows 7. The maximum memory consumed by ADD in-
ference in any of the benchmarks is less than 200MB.Table 3: Comparing runtimes of ADD inference
with Expectation Propagation (EP) for loopy bench-
marks.
Benchmark Parameters EP ADD
(seconds) (seconds)
OneCoin 0.25 0.56
Dice 183 0.57
unfairCoin d = 5? 0.95
d = 10? 179
uniform N = 100? 1.09
N = 800? 7.54
N = 2000? 21.50
MC ? 0.71
Table 1 compares exact ADD inference with a number
of probabilistic inference tools. Each benchmark is associ-
ated with a set of parameters that dene the size of the
problem. The parameters for the Student ,Friends and
Compare benchmarks are (#students, #courses, #teachers)
and (#people), (#width) respectively. We compared our
tool with SamIam [11], an inference engine for discrete mod-
els (implementing the algorithms Shenoy-Shafer ,Hugin ,
ZC-Hugin andRecursive Conditioning ) and OpenBugs
an inference engine that employs MCMC sampling. SamIam
performs almost as well as the ADD algorithm for the dis-
crete benchmarks. Exact ADD inference runs of out memory
for the Friends benchmark with p>6. This motivates the
need for a scalability heuristic like batch processing at the
cost of approximation. OpenBugs can quickly give approx-
imate answers on small examples, but is very slow for exact
answers. This is due to the fact that with a lot of observe
statements, it is hard to nd valid paths to compute a rele-
vant answer { this is a standard issue with all rejection sam-
pling based techniques. We give the computation times with
a number of iterations set to get a posterior probability with
a precision of 0 :01. Gibbs Sampling (GS) and Expectation
Propagation (EP) are inference algorithms available with
the Infer.NET [26] toolkit. As seen from the table, ADD is
signicantly more performant than GS and EP (even though
GS and EP compute approximate answers).
Table 2 reports the results for ADD inference with batch
processing and discretization. For both the benchmark pro-
grams, we use batch processing in order to get an approx-
imate solution whose precision is comparable to the solu-
tion obtained by EP (which is promising as EP is used by
TrueSkill in Xbox live). For example, in TrueSkill , we
marginalize the skill variables after every set of ngames. For
theTrueSkill benchmark, ADD also performs discretiza-
tion in order to handle continuous distributions.
Finally, Table 3 shows the results of ADD inference over
the loopy benchmarks. We compare ADD inference with EP.
Since EP does not support xpoints, we unroll the loops in
the benchmarks a xed number of times and then feed the
resulting programs to EP. Except for the simple examples,
OneCoin and Dice, the EP algorithm was not able to give
the expected distributions. On the other hand, ADD in-
ference converges in a small number of iterations for the
loops (generally 2 or 3), thanks to the fast exponentiation
method. For the MCexample, we are essentially computing
an ADD representation of the transition matrix, which can
be compact in presence of sparse data, and then perform fast
exponentiation until convergence (we use KL divergence [10]
of distributions across iterations to detect convergence).100Table 1: Comparing runtimes of exact ADD inference with other approximate inference algorithms. The ?
entry represents a \did not complete".
Benchmark Parameters shenoy-shafer hugin zc-hugin rec-cond OpenBugs GS EP ADD
(seconds) (seconds) (seconds) (seconds) (seconds) (seconds) (seconds) (seconds)
Students s=10, c=10, t=4 0.38 0.40 0.41 0.53? 0.88 1.57 0.11
Friendsp=4 0.40 0.41 0.41 0.50 4 7.7 4.09 0.19
p=5 2.75 2.66 3.37 9.62 18? 6.06 0.42
p =6 ? ????? 27.3 4.66
Comparen=10 0.28 0.26 0.29 0.33 3? 1.58 0.15
n=20 0.33 0.31 0.30 0.37 2? 2.34 0.16
n=100 0.53 0.55 0.52 0.92 6? 12.58 2.15
Table 2: Comparing runtimes of ADD inference (with batch processing and discretization) with Expectation
Propagation (EP).
Benchmark Parameters EP ADD
(seconds) (seconds)
Friendsp=6 27.3 4.66
p=7 28.0 10.11
p =8 30.34 20.4
TrueSkill matches=100, n=1 2.86 2.42
matches=100, n=2 2.86 2.82
matches=100, n=3 2.86 3.05
matches=100, n=4 2.86 4.54
matches=100, n=5 2.86 5.94
matches=100, n=7 2.86 9.79
matches=100, n=10 2.86 25.98
matches=5000, n=1 33.17 123
matches=5000, n=2 33.17 153
7. RELATED WORK
There are a variety of probabilistic programming lan-
guages and systems [3, 14, 15, 19, 20, 26, 28]. They perform
either dynamic inference either by running the program and
performing sampling [6,14], or static inference by rst trans-
forming the program to a probabilistic model such as a
Bayesian network and then using well known inference al-
gorithms over the transformed model [3,26]. Our technique
is static, and in contrast to previous work, perform infer-
ence directly over the probabilistic program. Our technique
merges data ow facts at join points and hence does not
suer from explosion due to a large number of paths in the
program.
Data ow analysis for frequency counting has been ex-
plored before by Ramalingam [29]. Geldenhuys et al. [12]
use symbolic execution to estimate the probability of exe-
cuting parts of a program. Both [29] and [12] are frequen-
tist in nature where the probability of a path is obtained via
explicit counting. On the other hand, our work is Bayesian
in nature where we consider the richer class of probabilistic
programs that include sample statements as well as observe
statements and conditional distributions. Recent work has
explored probabilistic abstract interpretation in the domain
of numeric programs [25,30].
The idea of using ADDs for probabilistic inference has
been explored before. Sannar and McAllester [31] dene
Ane Algebraic Decision Diagrams to perform inference
over Bayesian networks and Markov Decision Processes.
Kwiatkowska et al. have used a variants of ADDs to per-
form probabilistic model checking in the PRISM project [23].
Bolzga and Maler have used ADDs to symbolically simulate
Markov chains [4]. All these papers study the problem of
computing the steady state distribution of Markov chains.
Markov chains do not support observe statements, and it is
not clear how to encode posterior probability inference ef-ciently in the framework of Markov chains. Chavira and
Darwiche [7] use ADDs to compactly represent factors in a
Bayesian network and thereby perform ecient inference via
variable elimination. In contrast, we avoid factor graphs al-
together and use ADDs to represent symbolic program states
(which are distributions) at every program point, much like
a data ow analysis or an abstract interpreter [9]. Further-
more, in contrast to graphical models such as Bayesian net-
works, our technique can handle probabilistic programs with
loops.
8. CONCLUSION
We proposed a technique to perform probabilistic infer-
ence using data ow analysis. We have also implemented
the technique using ADDs as a data structure. We showed
that our algorithm indeed computes the posterior probabil-
ity of a probabilistic program. We have also presented an
implementation which shows promising results. We believe
this approach opens a door to applying other ideas from pro-
gram analysis and verication (such as slicing, and abstract
interpretation) for doing probabilistic inference.
9. ACKNOWLEDGEMENTS
We thank Andreas Podelski and G. Ramalingam for very
helpful comments on earlier drafts of this paper.
10. REFERENCES
[1] R. I. Bahar, E. A. Frohm, C. M. Gaona, G. D.
Hachtel, E. Macii, A. Pardo, and F. Somenzi.
Algebraic decision diagrams and their applications.
Formal Methods in System Design , 10(2/3):171{206,
1997.101[2] G. Barthe, B. Gr egoire, and S. Zanella B eguelin.
Formal certication of code-based cryptographic
proofs. In Principles of Programming Languages
(POPL) , pages 90{101, 2009.
[3] J. Borgstr om, A. D. Gordon, M. Greenberg,
J. Margetson, and J. Van Gael. Measure transformer
semantics for Bayesian machine learning. In European
Symposium on Programming (ESOP) , pages 77{96,
2011. Extended version available as Technical Report
MSR{TR{2011{18.
[4] M. Bozga and O. Maler. On the representation of
probabilities over structured domains. In Computer
Aided Verication (CAV) , pages 261{273, 1999.
[5] R. E. Bryant. Symbolic boolean manipulation with
ordered binary-decision diagrams. ACM Computing
Surveys , 24(3):293{318, September 1992.
[6] A. T. Chaganty, A. V. Nori, and S. K. Rajamani.
Eciently sampling probabilistic programs via
program analysis. In Articial Intelligence and
Statistics (AISTATS) , 2013.
[7] M. Chavira and A. Darwiche. Compiling bayesian
networks using variable elimination. In International
Joint Conference on on Articial Intelligence (IJCAI) ,
pages 2443{2449, 2007.
[8] G. Claret, S. K. Rajamani, A. V. Nori, A. D. Gordon,
and J. Borgstr om. Bayesian inference for probabilistic
programs via symbolic execution. Technical Report
MSR-TR-2012-86, Microsoft Research, 2012.
[9] P. Cousot and R. Cousot. Abstract interpretation: a
unied lattice model for the static analysis of
programs by construction or approximation of
xpoints. In Principles of Programming Languages
(POPL) , pages 238{252, 1977.
[10] T. M. Cover and J. A. Thomas. Elements of
Information Theory (Wiley Series in
Telecommunications and Signal Processing) .
Wiley-Interscience, 2006.
[11] A. Darwiche. SamIam. Software available from
http://reasoning.cs.ucla.edu/samiam .
[12] J. Geldenhuys, W. Visser, and M. B. Dwyer.
Probabilistic symbolic execution. In International
Symposium on Software Testing and Analysis
(ISSTA) , pages 166{176, 2012.
[13] W. R. Gilks, A. Thomas, and D. J. Spiegelhalter. A
language and program for complex Bayesian
modelling. The Statistician , 43(1):169{177, 1994.
[14] N. D. Goodman, V. K. Mansinghka, D. M. Roy,
K. Bonawitz, and J. B. Tenenbaum. Church: a
language for generative models. In Uncertainty in
Articial Intelligence (UAI) , pages 220{229, 2008.
[15] A. D. Gordon, M. Aizatulin, J. Borgstr om, G. Claret,
T. Graepel, A. V. Nori, S. K. Rajamani, and
C. Russo. A model-learner pattern for Bayesian
reasoning. In Principles of Programming Languages
(POPL) , pages 403{416, 2013.
[16] R. Herbrich, T. Minka, and T. Graepel.
TrueSkill(TM): A Bayesian skill rating system. In
Advances in Neural Information Processing Systems
(NIPS) , pages 569{576, 2007.
[17] J.-P. Katoen, A. McIver, L. Meinicke, and C. C.
Morgan. Linear-invariant generation for probabilistic
programs: - automated support for proof-basedmethods. In Static Analysis Symposium (SAS) , pages
390{406, 2010.
[18] G. A. Kildall. A unied approach to global program
optimization. In Principles of Programming Languages
(POPL) , pages 194{206, 1973.
[19] S. Kok, M. Sumner, M. Richardson, P. Singla,
H. Poon, D. Lowd, and P. Domingos. The Alchemy
system for statistical relational AI. Technical report,
University of Washington, 2007.
http://alchemy.cs.washington.edu.
[20] D. Koller and N. Friedman. Probabilistic Graphical
Models: Principles and Techniques . MIT Press, 2009.
[21] D. Koller, D. A. McAllester, and A. Pfeer. Eective
Bayesian inference for stochastic programs. In
National Conference on Articial Intelligence (AAAI) ,
pages 740{747, 1997.
[22] D. Kozen. Semantics of probabilistic programs.
Journal of Computer and System Sciences (JCSS) ,
22(3):328{350, 1981.
[23] M. Z. Kwiatkowska, G. Norman, and D. Parker.
Probabilistic symbolic model checking with prism: a
hybrid approach. International Journal on Software
Tools for Technology Transfer (STTT) , 6(2):128{142,
2004.
[24] S. Malik, A. R. Wang, R. K. Brayton, and A. S.
Vincentelli. Logic verication using binary decision
diagrams in a logic synthesis environment. In
International Conference on Computer-Aided Design
(ICCAD) , pages 6{9, 1988.
[25] P. Mardziel, S. Magill, M. Hicks, and M. Srivatsa.
Dynamic enforcement of knowledge-based security
policies using probabilistic abstract interpretation.
Journal of Computer Security , January 2013.
[26] T. Minka, J. Winn, J. Guiver, and A. Kannan.
Infer.NET 2.3, Nov. 2009. Software available from
http://research.microsoft.com/infernet .
[27] J. Pearl. Probabilistic reasoning in intelligent systems
{ networks of plausible inference . I-XIX. Morgan
Kaufmann, 1989.
[28] A. Pfeer. Statistical Relational Learning , chapter The
design and implementation of IBAL: A
General-Purpose Probabilistic Language. MIT Press,
2007.
[29] G. Ramalingam. Data ow frequency analysis. In
Programming Languages Design and Implementation
(PLDI) , pages 267{277, 1996.
[30] S. Sankaranarayanan, A. Chakarov, and S. Gulwani.
Static analysis of probabilistic programs: Inferring
whole program properties from nitely many
executions. In Programming Languages Design and
Implementation (PLDI) , 2013.
[31] S. Sanner and D. A. McAllester. Ane Algebraic
Decision Diagrams (AADDs) and their application to
structured probabilistic inference. In International
Joint Conference on on Articial Intelligence (IJCAI) ,
pages 1384{1390, 2005.
[32] F. Somenzi. CUDD: CU decision diagram package,
release 2.5.0. Software available from
http://vlsi.colorado.edu .102