Ownership, Experience and Defects:
A Fine-Grained Study of Authorship
Foyzur Rahman and Premkumar Devanbu
Department of Computer Science
University of California, Davis, USA
{mfrahman,ptdevanbu}@ucdavis.edu
ABSTRACT
Recent research indicates that “people” factors such as own-
ership, experience, organizational structure, and geographic
distribution have a big impact on software quality. Un-
derstanding these factors, and properly deploying people
resources can help managers improve quality outcomes. This
paper considers the impact of code ownership and developer
experience on software quality. In a large project, a ﬁle
might be entirely owned by a single developer, or worked
on by many. Some previous research indicates that more
developers working on a ﬁle might lead to more defects.
Prior research considered this phenomenon at the level of
modules or ﬁles, and thus does not tease apart and study
the eﬀect of contributions of diﬀerent developers to each
module or ﬁle. We exploit a modern version control system
toexamine this issue at a ﬁne-grained level . Using version
history, we examine contributions to code fragments that
are actually repaired to ﬁx bugs. Are these code fragments
“implicated” in bugs the result of contributions from many?
or from one? Does experience matter? What type of ex-
perience? We ﬁnd that implicated code is more strongly
associated with a single developer’s contribution; our ﬁnd-
ings also indicate that an author’s specialized experience in
the target ﬁle is more important than general experience.
Our ﬁndings suggest that quality control eﬀorts could be
proﬁtably targeted at changes made by single developers
with limited prior experience on that ﬁle.
Categories and Subject Descriptors
D.2.8 [ Software Engineering ]: Metrics— Process Metrics
General Terms
Experimentation; Measurement; Veriﬁcation
Keywords
Ownership; Experience; Collaboration; Software Quality
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
ICSE ’11, May 21–28, 2011, Waikiki, Honolulu, HI, USA
Copyright 2011 ACM 978-1-4503-0445-0/11/05 ...$10.00.1. INTRODUCTION
Software defects cost the US economy billions each year [1].
For several decades now, studies of the causes of defects have
led to both prediction models and “actionable” approaches
to change processes to improve quality. Of late, there has
been increasing attention to human factors (such as organi-
zational structures [33], code ownership [34], and developer
experience [13]) as an inﬂuence on software quality.
Code ownership can be a good thing. Strong code owner-
ship might give rise to pride of workmanship, and thus yield
higher-quality results. In industry, code ownership makes it
easier for managers to hold the right people accountable, and
also can help ﬁnd experts to whom tasks may be delegated,
and questions addressed. On the other hand, excessive levels
of ownership might lead to “in-bred” code whose quality suf-
fers from a lack of wider scrutiny. Experience is a related, but
distinct phenomenon. As a developer contributes more to a
project, her experience increases, thereby it is likely that the
code she writes would be of higher quality and thereby less
defect prone. On the other hand, an experienced developer
might be overconﬁdent in her approach to unfamiliar code,
and thus inadvertently lower its quality.
Researchers have previously studied [11, 13, 16, 30, 46] the
eﬀects of ownership and experience; but this research largely
considers ownership and experience at the level of ﬁles and
modules. However, several developers may contribute to each
ﬁle or module; thus the eﬀects of many developers’ varying
ownership and experience, on quality, can be conﬂated to-
gether, and give anomalous, noisy, and/or confusing results,
arising from the ecological fallacy [22]. We adopt a much
more precise, ﬁne-grained approach.
Our approach is to zero-in on the lines of code that were
actually changed or deleted to ﬁx bugs. We refer to these lines
as“implicated code1”. Using the provenance facilities (based
on modern, more precise diﬀerencing algorithms) aﬀorded by
theGitversion control system, we can track down the origin
and authorship of individual lines of code, and thus more
precisely identify the authors of implicated code. We can
then contrast the experience and ownership characteristics of
implicated code with“normal”(randomly chosen background)
code, as we describe in detail below.
In this paper, we study the relationship of ownership and
experience (at a ﬁne-grained level) on the quality of code.
We make the following contributions.
1.We ﬁnd that stronger ownership by a single author is
associated with implicated code.
1We use “implicated” in the sense described in WordNet, http:
//wordnetweb.princeton.edu/perl/webwn?s=implicatedPermission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
ICSE ’11, May 21–28, 2011, Waikiki, Honolulu, HI, USA
Copyright 2011 ACM 978-1-4503-0445-0/11/05 ...$10.00
491
2.We ﬁnd lack of specialized experience on a particular
ﬁle is associated with implicated code in that ﬁle.
3.We ﬁnd that lack of general (non-specialized) expe-
rience is not consistently associated with implicated
code.
4.Methodology: For our study, we use a novel, ﬁne-grained,
sampling method. We use the blame feature of Gitto
track the provenance of code fragments implicated in
bugs, and compare the provenance of this “implicated”
code with randomly chosen “unimplicated” fragments of
the same size, drawn from the same ﬁle, and same point
in the system’s history. We argue that this approach is
general, and can be used to study a range of questions
as to why certain code fragments are involved in bugs,
and others are not; the resulting knowledge can drive
new approaches to quality control and defect detection.
These results are actionable : at the simplest level, our
results indicate that specialized experience matters more
than general experience: rather than throwing a random
developer, at crunch time, to ﬁx problems in an unfamiliar
part of a system, we should use someone with experience on
that part of the system. We discuss further implications in
Section 7.
2. BACKGROUND & THEORY
Ownership : Large software projects must divide the work
between members of the development team. The question
ofownership arises here: should each member of the team
own their piece of the system? Or should everyone collec-
tively own everything? This is controversial. The Agile
movement [8] argues for collective ownership, as does Ray-
mond [37], who claims that increasing the number of collab-
orators will accelerate defect diagnosis. Others counter with
evidence suggesting that “too many cooks” working on the
software leads to unfocused, defect-prone contributions [34].
Also, communication and coordination overheads worsen as
the team size increases [14, 17]. Researchers have also found
that these overheads can slow down development [15, 20, 26]
and increase defects [18].
Seifert et al. [41] found that an increasing number of dis-
tinct authors making change to a ﬁle may lead to more
defects. Meneely et al. [30] used social network based mea-
sures to analyze the eﬀect of focus and ownership on security
related errors. They found that contributions by less-focused
developers were associated with more security-related errors.
They also found that when more than nine developers con-
tribute to a source ﬁle, it is sixteen times more likely to
include a security vulnerability. Bird et al. [11] ﬁnd evidence
that small contributions by minor developers are associated
with defects, specially in more commercially-oriented devel-
opment processes. There are, however, some contradicting
results: Weyuker et al. [46] and Graves et al. [25] both found
that the number of contributors didn’t necessarily relate to
fault-proneness.
The ﬁndings reported above are generally at the ﬁle or
module level. However, code authorship happens at a much
more ﬁne-grained level. Indeed, diﬀerent bits of code in the
same ﬁle may have been contributed by diﬀerent authors;
version control systems can track authorship at the line level.
Studies of ownership eﬀects at a ﬁle or module level aggregate
the eﬀects of individual contributors together, thus risking
the “ecological fallacy” [22] of transferring ﬁndings from theaggregate to the constituent element; there are both internal
and construct validity issues with this type of ecological in fer-
ence. With the ﬁne-grained provenance information available
from git blame , we address the question, is ownership at a
ﬁne-grained level indicative of defect-proneness? If one be-
lieves the “many eyeballs” theory of Raymond [37], shouldn’t
fragments of code with multiple authors be less likely to have
defects?
RQ1: Is implicated code less likely to involve contributions
from multiple authors?
Experience: A related, but distinct issue is experience . Em-
ployee experience and its impact on productivity and quality
of output has been the subject of extensive study in many
ﬁelds and several industries [4, 5, 35, 45]. In manufacturing,
when a worker performs a task repetitively, her eﬃciency im-
proves and quality of output increases [19]. Similar ﬁndings
were reported for service industries [35]. Unlike manufac-
turing or service industries, software development is not
particularly repetitive, and requires signiﬁcant creativity;
however, studies indicate that a developer’s knowledge of the
system increases as she works more on various components
of the system [7, 39]. Knowledge gained from experience
matters: it can lead to better ability to answer questions
about previous work [23], and better quality work [31]. These
ﬁndings are also supported by Robillard [38], who suggests
that lack of such knowledge could hurt software quality. We
contribute to this literature in two ways. First, we pursue a
ﬁne-grained approach, considering the contributions of each
developer within a ﬁle, and studying the relationship of deve l-
oper experience to quality at the level of these fragmentary
contributions. Second, we conceptualize two distinct types
of experience that can aﬀect the quality of a developer’s
work: specialized experience in a ﬁle under consideration, and
general experience in the entire project. We deﬁne precise
measures later, but intuitively, we view general experience
of an individual as a measure of the total contributions of
that individual to a project, and specialized experience of an
individual with respect to a ﬁle as a measure of the number
of contributions of that individual to that ﬁle.
The distinction between specialized and general experi-
ence matters. Given an urgent task, a manager might have
diﬃculty ﬁnding a person with specialized experience rele-
vant to a given task, and so might be tempted to deputize
a developer with a lot of general (not necessarily relevant)
experience. Is this a temptation to which one should yield?
Boh et al. [13] studied the impact of such specialized and
general experience on developer productivity. Their ﬁndings
indicate that specialized experience has much larger impact
on developer productivity than diverse experience in unrelated
systems. We consider the impact of general and specialized
experience on quality . While Mockus and Weiss [31] con-
sidered the quality eﬀect of both types of experience, they
study the quality eﬀect of experience at the relatively coarser
granularity of a “maintenance request” (MR); a single MR
may involve changes to multiple ﬁles by multiple authors,
and [31] conﬂates the specialized & general experience of
all contributing authors together using a geometric mean.
We study the quality eﬀect of experience at a ﬁner granular-
ity, considering the authorship of lines of code within each
ﬁle, and comparing the experience of authors of lines impli-
cated for bugs, with that of “background code” (randomly
selected code with similar properties) authors. We consider
specialized experience (RQ2) and general experience (RQ3)
separately.492In particular, when a developer authors some lines in a ﬁle,
one can reasonably assume that her prior experience on that
selfsame ﬁle is the most relevant experience. We hypothesize
that the more a developer contributes to a ﬁle, the less likely
it is that a change authored in that ﬁle by him/her would
be implicated later.
RQ2: Is a developer with higher ﬁle level authorship less
likely to be associated with implicated code?
Next, we consider general experience. The more work
a developer has done on every ﬁle of the system, perhaps
the less likely they are to make mistakes. Paradoxically,
prior work suggests that experienced developers are more
defect prone (e.g. Erich Gamma was identiﬁed as second
most defect prone programmer in Eclipse). Zeller makes
the argument that this is because the most experienced
developers work on the most often exercised, most complex
part of the system [47]. We examine this issue at a ﬁne-
grained level, by measuring the level of experience of the
authors associated with implicated code:
RQ3: Is general experience more likely to be associated with
implicated code?
The above questions consider defect-proneness without
addressing the type of defects. However, in most projects
faults are associated with a criticality factor, e.g., Apache
etc. uses a notion of Severity . Unfortunately, fault-proneness
generally considered in the research (with few exceptions
[36, 42, 48]) without attention to severity. However, when
considering the eﬀect of experience on fault-proneness, one
can naturally believe that experienced developers are more
likely to be working on critical code, and thus more likely
to be associated with critical faults. Thus when we inspect
the authorship of implicated code, we might expect that
implicated code associated with severe defects is more likely
to be associated with senior developers, viz.developers with
more general experience.
We therefore examine whether experienced developers are
writing more severe bugs.
RQ4: Is general experience associated with implicated code
for more severe defects?
Overview of Methodology Existing research addresses
ownership and experience questions mostly at ﬁle or module
level. We instead try to study the ownership and experience
in the context of implicated code. We identify lines of code
that are associated with changes that are identiﬁed as de-
fect repairs. This code was originally called “ﬁx-inducing
code” by Sliwerski et al. [43] (also see [29]), as it is the
code that needed repair; for example if the code line strcpy
(str2,str1); was changed to strncpy(str2,str1,n); then the
original line is considered ﬁx-inducing code. We have found
that the term “ﬁx-inducing code” causes confusion, we prefer
the term “implicated code” :
Implicated code is code that is modiﬁed to ﬁx a defect.
Figure 1 (adapted from [43]) shows how we ﬁnd implicated
code. We start with data linking a given bug to the revision
where that bug was ﬁxed. If a bug ﬁx is linked to revision
n+ 1, then the immediately preceding revision ( viz., n)
should contain the relevant “unﬁxed” code. A diff between
revision nand n+ 1 for each of the ﬁles that was changed
in revision n+ 1 gives us the potential buggy code. We
call these lines the implicated code . We then use the git
1 2 i j
 k ndiffBug b ﬁ xed
at revision n+1Link 
Database Bug b was 
reportedImplicated
Implicated
Innocent
Figure 1: Finding Implicated Code
blame command on all such implicated code; this produces
provenance annotations (author, date, revision number where
they were last changed) on each line (we note here that git
uses a sophisticated diﬀerencing algorithm that is sensitive
to whitespace changes, line breaking, and even (to some
extent) code movement). In the ﬁgure, versions i, jand k
contributed lines that were changed from version nton+ 1.
Versions iand joccurred before bug bwas reported; version
koccurred after. Following [43] we consider code attributed
to version kas innocent, and not part of the implicated code
associated with bug b.
foo.c: n foo.c: j foo.c: i
Bob on
09-19-2008
at 12:15pmGopal on 
10-07-2008
at 3:29pm
Figure 2: Implicated Code author information
Figure 2 shows how we ﬁnd the author and modiﬁcation
time. In the ﬁgure, three of the seven implicated code lines
were attributed to revision iand jwhich were originally
committed by Bob, on 09-19-2008, and Gopal, at 10-07-
2008. These details can be used to study the ownership and
experience in implicated code, when compared to the rest
of the code. Does the implicated code itself involve more
authors? Are there any signiﬁcant diﬀerences in experience
level associated with implicated code?
To answer these questions, we draw upon the existing
literature on ownership and experience. Girba et al. [24]
visualize a project’s evolution based on ownership changes.
They deﬁne a developer’s ownership of a ﬁle as the percentage
of lines attributed to him/her in a ﬁle. The overall owner is
the author with the highest ownership. Mockus et al. [32]
report that developer experience is negatively correlated with
failure proneness. They measure experience as the number of
changes that a developer made to a system upto a given time.
We deﬁne general experience as the cumulative developer
contribution to the entire system at a given time. Our study
of implicated code goes further, to consider the inﬂuence of
specialized experience , as discussed below.
3. EXPERIMENTAL FRAMEWORK
In this section we will deﬁne all the terminologies and
background of our experiment.4933.1 Revision
Source code management systems (SCM) provide a rich
version history of software projects. This information in-
cludes the full history of commits to each ﬁle: timestamps,
authorship, change content, and the commit log. In our
study we identify each of these commits as a revision, where
a revision consists of an author, a timestamp and a set of
ﬁles changed in that revision. We chose Gitas our version
control system, thanks to its excellent provenance facilities .
3.2 Bug-ﬁxing Revision
Typically bugs are discovered and reported to an issue
tracking system such as Bugzilla and later on ﬁxed by the
developers. Each bug report records the opening date, the
ﬁxing date, a free text bug decription, and the ﬁnal, triaged
Bugzilla severity. A typical bug severity in Bugzilla may
have one of the following values: “blocker”, “critical”, “major” ,
“normal”, “minor”, “trivial”, “enhancement”. We consider any
change associated with a report in the Bugzilla database that
is not labeled as an “enhancement” to be a bug.
Our study begins with links between Bugzilla bugs and the
speciﬁc revision that ﬁxes the bug—-we call this a bug-ﬁxing
revision . Our data is derived using several diﬀerent heuristics.
Various key words such as “bug”, “ﬁxed” etc. in the SCM
commit log are used to ﬂag bug-ﬁxing revisions [2]. Also,
numerical bug ids mentioned in the commit log, are linked
back to the issue tracking system’s identiﬁers [21, 44]. Then
the data is crosschecked with the issue tracking system to
see whether such issue identiﬁer exists and whether its stat us
changes after the bug-ﬁxing commit. Finally manual inspec-
tion is used to remove spurious linking as much as possible.
Each of the remaining linked bugs can be associated with a
particular bug-ﬁxing revision. We gratefully acknowledge
the direct use of linked bugs data derived by Bachmann [6].
These bug-ﬁxing revisions change (or delete) certain lines
in the source base in order to ﬁx bugs. These changed or
deleted lines are the implicated code lines, which are the
central focus of our research. New lines may also be added
in the bug ﬁxing revision, but we do not consider these
“implicated”, since they are part of the treatment, not the
symptom.
Implicated code for a single bug may not be contiguous,
in fact it may occur as several spatially separated groups
of contiguous lines. Each group of contiguous lines in the
implicated code is referred to as a hunk.
3.3 Implicated Author
For each line of implicated code, we seek to identify the
author of that line of code. We use the blame functionality
ofgitto identify the revision, time, author that introduced
the line. githas a very accurate blame facility [12], which
properly handles whitespace changes, and tracks code copying
and movement within and even between ﬁles2. We then
identify, at the resolution of individual lines, all the revisi ons,
and revision authors contributing to the implicated code.
3.4 Authorship, Ownership
Based on our partitions for implicated code, we compute
the proportion of contribution for each author. So, if Nblines
are changed to ﬁx a bug b, and there are a total of mdistinct
authors, and the number of lines contributed by author ato
the implicated code of bug bisNb
a, then contribution ratio
2See http://www.kernel.org/pub/software/scm/git/
docs/git-blame.html , last checked July 25, 2010for author aisra=Nb
a
Nb. We call this authorship for author a.
We also determine the author with the highest authorship,
i.e. highest contribution ratio, and ﬂag this author as the
primary implicated code contributor for this bug.
In this paper, we use the term ownership of a code fragment
to refer to the authorship of the highest contributor. Thus,
implicated code ownership would give us the authorship of
the highest contributor to that implicated code. In a similar
fashion, we determine ownership of a ﬁle by considering
highest contributor of a ﬁle instead of a bug.
3.5 Background Code
When making claims about properties of implicated code
(such as number of contributors, experience of owner, etc), it
is important to have a reference point. For example, if we
claim that implicated code has more authors: the question
arises, “when compared to what”? Clearly, one needs to
compare implicated code with some non implicated code, to
see if there is something diﬀerent about the implicated code.
Our approach is to compare each hunk of implicated code
to a randomly chosen hunk of non-implicated code. We call
these hunks chosen for comparison background code .
Obviously, the background code chosen for comparison to
a given hunk of implicated code, should be selected both
carefully and fairly. For example, we should choose hunks
of the same size. A background code hunk smaller than
a corresponding implicated hunk will tend to have fewer
contributors than a larger hunk. Furthermore, newer code
might have fewer authors than older code. Therefore, we
compare each hunk of implicated code to a randomly chosen
hunk of the same length, from the same ﬁle, from the same
revision as the implicated code was considered. Furthermore,
comments and blank lines can be expected to have diﬀerent
patterns of authorship and evolution than non-commentary
code. For this reason, we disregard comments and blank
lines in both implicated code and background code.
We use the following procedure to choose background
hunks. First, we pick locationally similar background code,
from the same ﬁle of its corresponding implicated code. Next,
we choose temporally similar background hunks, by choosing
a hunk from the same revision that the corresponding im-
plicated code was changed to ﬁx the bug. Next, we ensure
length similarity by picking random contiguous hunks [28] of
the same length as each such hunk in the implicated code.
During the selection process, we ensure structural integrity
by picking code hunks that do not cross function bound-
aries3. That is, we do not pick code hunks which may span
multiple functions. Our algorithm tries to ﬁnd background
code that doesn’t overlap with the corresponding implicated
code. However, it may occasionally return overlapping back-
ground code when it couldn’t ﬁnd a non-overlapping one.
Following [43] (also explained in section 2), we discard any
implicated code lines that were introduced after the bug
was reported. To ensure spatial and structural similarity of
background code in such cases, we run a post processing on
the selected background code hunks. The post processing
discards i-th line of h-th hunk of background code, if the
corresponding line is discarded from the implicated code (i.e.
i-th line of h-th hunk of implicated code was changed after
the bug was reported and was identiﬁed as innocent for that
reason).
3We use Understand from SciToolsTMto ﬁnd function
boundaries494Thus, we choose background code hunks matched as closely
as possible to the implicated code . Since the background
code hunks come from the same time, from the same ﬁle,
and are of the same length as the implicated hunks, we have
a greater chance of discerning any signiﬁcant dissimilarities
arising in the ownership level of the implicated hunks, and
the experience of the authors contributing to them.
This methodology of comparing implicated code with care-
fully chosen background code is an important contribution
of this paper, and we argue that this is a novel experimen-
tal technique that has applications beyond this work . It can
allow study of a range of questions concerning ﬁne-grained
phenomena that can give rise to defects—such as language
features, APIs, programming styles, static properties, and so
on.
3.6 Experience
To measure the general experience of an author, we use the
total number of deltas committed to the source code reposi-
tory by an author up to a particular point in time. E.g. if
she committed a total of 500 lines of deltas until January 1st,
2002, her general experience would be 500 at that point, but
if we look at January 1st, 2010, she might have contributed
10000 lines by that time, thereby changing her general expe-
rience to 10000. We derive general experience for implicated
and background hunks using the weighted geometric mean
of the experience of all contributing authors, as described by
Mockus [32]. In our case, weight is the proportion of either
the implicated code or the background code written by a
person and experience is her general experience at that point
in time.
We also use a developer’s authorship of a ﬁle at a point in
time to measure her specialized experience .
4. THE DATA SETS
We chose 4 diﬀerent medium- to large-sized open-source
projects for our study. All have long development history,
but hail from diﬀerent domains. Apache Httpd is a widely
used open source web server. Nautilus is the default ﬁle
manager for the Gnome desktop. Evolution is the default
email client for the Gnome desktop with support for inte-
grated mail, address book and calender functionality. Gimp
is a popular open source image manipulation program. All
of our projects are written in C. We converted the Apache
subversion repository to gitand used the other projects’ git
repositories directly.
Max. Linked Implicated
Name Size bug hunk Author
(LOC) count count count
Apache 208388 478 1687 96
Evolution 531342 1022 9836 685
Gimp 947073 1601 31184 400
Nautilus 366894 534 3025 483
Table 1: Summary of study subjects
These projects represent a diverse set of application do-
mains: a server, a client, a ﬁle browser, and a GUI application.
All are non-trivial. A summary of descriptive statistics of the
projects studied is presented in table 1. They range in size
from 200k lines to about a million lines. The table presents
details about the number of LOC, the total number of linked
bugs (over the entire period), total number of implicated
code fragments and number of distinct authors.We determine all linked bugs and their associated bug-
ﬁxing revision for all the subject projects. The corresponding
implicated code is found by using diﬀ between a bug-ﬁxing
revision and its immediate preceding revision. Such impli-
cated code may consist of multiple hunks and for each hunk
we used our careful background-code-ﬁnding procedures to
select similar, random non-implicated code. We also experi-
mented with several alternative, less stringent methods to
select background code. Our alternative background code
selection procedure includes: relaxing function boundary
requirement, keeping comments, picking one contiguous code
section for all the composing hunks of a bug etc. In all those
cases we found results similar to our stringent background
selection procedure.
After collecting all implicated code and the correspond-
ing background code, we use git blame on each line of the
implicated code and background code to identify its lastmod-
ifying author and the modiﬁcation time. We then proceed
to compare their properties to answer various research ques-
tions concerning the impact of code ownership and developer
experience on software defect proneness.
5. RESULTS
We now present our results, broken down by the research
questions presented earlier in Section 2. In some cases, a
single research question was subject to detailed investigatio n
using a few diﬀerent measures, and we present them as
RQ1a, RQ1b ,etc. For illustrative purposes, we present the
boxplot charts only for Apache and Evolution. Data from
other projects is discussed in the text.
GG
GG G GG
G G
GG
GG
GG
G
G GG
G
GG
GG
G G GG
G G G GG
GG GG
G G GG G
GG
G G G GG G GG G G
GG G
GG
G G GG
GGG
GGGG
GG
G GG
GGG G
GG
GG
GG
GGG
G GG
Implicated Code Background2 4 6 8 10 12 14Apache − Number of AuthorsNumber of AuthorsG
G GG G
G G G GGGG
G GG
G GG G G GG
G
GGG
G
GG
G GG
G G G GGG
G GGG
G
GG GGG
GG
G
GG
G
G GGG
GG
GG
GG
G
GG
GG
GG
GGGG
GG
G
G
G G GG
GG
G GG
G
G
GG
G G G GG
G GG
G GG
G
G GG
GG G G
GG
G GG
G G GG GG
G
GG
GGG G
GG
G GG
G
GG
G
GG G
GG
G
G GG
GGG
G
GG GG
G
GGG
G GG
G
G
GG
G GG
GGG
G
GG G
G G
GGG
GG
GG
G
G GG
GG
GGG
G
Implicated Code Background2 4 6 8 10 12 14Evolution − Number of AuthorsNumber of Authors
Figure 3: Number of distinct authors in implicated code
and background for (a) Apache (b) Evolution.
RQ1a: Is implicated code less likely to involve contributions
from multiple authors?
To evaluate this “more eyeballs ⇒better code” theory,
we compare the number of authors in the implicated code
against the number of authors in comparable background
code. Figure 3 shows the boxplot of number of authors in
implicated code and background code. The boxplot suggests
that the number of author is actually higher in background
than implicated code. A Wilcoxon paired test (alternative
hypothesis set to “number of author is greater in background
code”) are shown in table 2. All the p-values for RQ1a in
table 2 are very low, rejecting the null hypothesis conclusively .
Despite the evidence to reject the null hypothesis in this
case, if we look at the boxplots, we can notice both small
range and strong positive skew, due to the limited range of
number of authors (mostly 1, 2, or 3) in implicated code
and comparable background code: most implicated code
hunks are small, they are largely single authored. As a
more sensitive test, we consider the primary ownership of495Name RQ1a RQ1b RQ1c RQ2a RQ2b RQ3 RQ4
Apache p≪0.001 p≪0.001 0 .625 0 .275 0 .0009 0 .159 0 .361
Evolution p≪0.001 p≪0.001 0 .002 p≪0.001 p≪0.001 0 .999 0 .304
Gimp p≪0.001 p≪0.001 0 .004 p≪0.001 p≪0.001 p≪0.001 0 .076
Nautilus p≪0.001 p≪0.001 0 .002 0 .0008 p≪0.001 0 .0002 0 .632
Table 2: The Alternative hypothesis is that the answers to the research question is “yes” in all cases.
Wilcoxon test p-values for RQ1, RQ2, RQ3 and RQ4. All p-values h ave been adjusted using Benjamini-Hoch-
berg procedure. p-values are the probability of the sample if the null hypothesis were true. Except RQ4, all
arepaired Wilcoxon test.
implicated code and background code, viz., the proportion of
the hunks contributed by the largest contributor, thus giving
us range of values upto 1.0.
G GG G G G G
Implicated Code Background0.2 0.4 0.6 0.8 1.0Apache − OwnershipOwnership
G
Implicated Code Background0.2 0.4 0.6 0.8 1.0Evolution − OwnershipOwnership
Figure 4: Ownership in implicated code and Back-
ground for (a) Apache (b) Evolution.
RQ1b: Does implicated code have higher primary ownership?
Figure 4 shows the boxplot of primary ownership in impli-
cated code and background code. The boxplots are consistent
with RQ1a boxplots and the p-values are shown in table 2.
All the p values are again very low, thereby we reject the
null hypothesis. This result provides additional evidence
indicating that the implicated code tends to more frequently
originate from fewer authors, when compared to similarly
chosen background code.
Noticing that many of the implicated code fragments had
only one single author, (and the resulting negative skew in
the data) we decided to perform one bit of sensitivity analysis
on the data. We discarded all singly-authored implicated
code fragments, and considered just the ownership of the
remaining implicated code. It’s important to notice that this
is a particularly stringent test: by omitting this data from
implicated code, we are reducing the likelihood of detecting
a diﬀerence between implicated code and background.
Implicated Code Background0.2 0.4 0.6 0.8 1.0Apache − Ownership (Excluding 1.0)Ownership
Implicated Code Background0.2 0.4 0.6 0.8 1.0Evolution − Ownership (Excluding 1.0)Ownership
Figure 5: Ownership after discarding single author im-
plicated code and their corresponding background code
for (a) Apache (b) Evolution.RQ1c: Does implicated code with more than one author
demonstrate higher ownership?
We discard all the implicated code that has a single author;
for fairness, we also discard the background code chosen to
match this implicated code. It’s important to note that it
is only the background code chosen to match this buggy
code that is discarded. We then compare the ownership of
the remaining implicated code and background code. The
boxplot of our comparison is shown in ﬁgure 5. Three out of
four projects (except Apache) still give us consistent ﬁndings
with RQ1a and RQ1b. The p-values are shown in table 2.
This is a conservative analysis, as we selectively discarded
only the singly-authored implicated code (and its correspond-
ing background code), without selectively discarding the
singly-authored background code (we of course discarded the
singly-authored background code if that corresponds to a
discarded implicated code). The Apache contains relatively
more singly-authored implicated code (63% of linked bugs
have singly-authored implicated code, while Evolution, Gimp
and Nautilus have 57%, 46%, 55% respectively), and so a
relatively large number of multi-authored background code
got discarded in the process.
Our results from RQ1a, RQ1b and RQ1c all shows evi-
dence of fewer authors and corresponding higher ownership
in implicated code.
Implicated code is less likely to involve contribu-
tions from multiple developers.
Implicated Code Background0.0 0.2 0.4 0.6 0.8 1.0Apache − File Owner's ContributionContribution
Implicated Code Background0.0 0.2 0.4 0.6 0.8 1.0Evolution − File Owner's ContributionContribution
Figure 6: File owner’s contribution in implicated code
and background code for (a) Apache (b) Evolution.
RQ2a: Is higher authorship by the ﬁle owner less likely to
be associated with implicated code?
One would expect that a developer’s expertise on a ﬁle’s
code would increase signiﬁcantly as she writes more of the
code in the ﬁle. Does this expertise translate into better
quality code? If so, we would expect to see a ﬁle’s owner
writing less implicated code.496Implicated Code Background0.0 0.2 0.4 0.6 0.8 1.0Apache − Implicated Owner's File Level ContributionContribution
Implicated Code Background0.0 0.2 0.4 0.6 0.8 1.0Evolution − Implicated Owner's File Level ContributionContribution
Figure 7: Implicated and background code owner’s con-
tribution at ﬁle level for (a) Apache (b) Evolution.
We deﬁne the ﬁle’s owner (at the time the bug ﬁx was
performed) to be the developer with the most contributions
to the ﬁle. We then compare a ﬁle’s owner’s contribution
to the implicated code against background code. Figure 6
compares ﬁle owners’ contribution in implicated code and
background code. If the ﬁle owner writes less implicated
code in that ﬁle, then her contribution in implicated code of
that ﬁle would be less than her contribution in background
code of that ﬁle (alternative hypothesis). In 3 of 4 cases (see
table 2, RQ2a column) we can reject the null hypothesis, and
ﬁnd support for the alternative hypothesis that ﬁle owners
indeed write signiﬁcantly less implicated code in that ﬁle.
Figure 6 shows how RQ2a is supported by the Evolution
dataset, but not the Apache dataset. However, left and
right side boxplots in each dataset in the ﬁgure are skewed,
and do look rather similar. This is because ﬁle owners ipso
facto write most of the code in the ﬁles they own, including
both implicated code and background code. We therefore
consider a more sensitive measure, by reversing the sense of
ownership, and asking how much the primary contributor
of the implicated code (or corresponding random fragment)
contributes to the ﬁle.
RQ2b: Do implicated code owners have lower contribution
at ﬁle level?
In RQ2a we found, in 3 of 4 cases, that ﬁle owners are less
likely to introduce implicated code. We intend to investigat e
this trend further by asking the converse question. If the
RQ2a trend holds, i.e. specialized experience in a ﬁle is
associated with writing less implicated code in the same ﬁle,
then we would see that the implicated code owners would
have lower contribution to the ﬁle.
To investigate this, we compare the contributions of im-
plicated code owners and background code owners to the
containing ﬁle. If prior experience on the ﬁle matters, then
implicated code owners should tend to have less ownership
at the ﬁle level. Figure 7 presents the boxplots comparing
these two samples. The ﬁgure indicates that implicated code
owners have lower ﬁle level contribution than background
code owners. The p-values are shown in table 2 with alter-
native hypothesis set to “implicated code owners have lower
ﬁle level contribution than background code owners”. All
the p-values are highly signiﬁcant, thereby rejecting the null
hypothesis conclusively. The ﬁndings from RQ2a and RQ2b
are largely consistent and suggest that specialized experienc e
plays an important role to write less defective code.Implicated code owner has lower contribution at
ﬁle level.
GG
GG
GGG GG
G
G
GGGG
GG
G
GG
GGGG
G
GG
GG
GG G
GGGG
G
Implicated Code Background Code1 2 3 4 5Apache − Weighted Experience (Overall)Log10(Weighted Experience)G
GGG
G
GGG GG
G
GGG
GG
G
GG
G
Implicated Code Background Code1 2 3 4 5 6Evolution − Weighted Experience (Overall)Log10(Weighted Experience)
Figure 8: Weighted author experience in implicated
code and background code for (a) Apache (b) Evolution.
RQ3: Is general experience more likely to be associated with
implicated code?
Next we examine whether general experience is associated
with implicated code. We compare general experience of
implicated code against the general experience of background
code. We measure general experience of a developer as the
number of deltas she has contributed until a point in time.
The weighted experience of all the hunks of the implicated
code of a bug is computed by multiplying each contributing
developer’s experience by the number of lines she contributes
to those hunks. Following Mockus [3] we use the geometric
mean of these weighted values; geometric means are used
because the distribution of experience is very right-skewed.
In ﬁgure 8, we compare general experience of implicated
code and background code. From boxplots we observe a lower
weighted experience of implicated code only for Evolution.
Also, for Apache, the diﬀerence is almost indistinguishable .
For other two projects (Gimp and Nautilus) we observe
a higher weighted experience for implicated code. Table 2
presents p-values for Wilcoxon rank sum test with alternative
hypothesis set to “Implicated code associated with more
general experience than background code”. We reject the
null hypothesis for the Gimp and Nautilus while fail to do so
for Apache and Evolution. Further study is clearly needed
to shed light on this question.
General experience of author has no clear asso-
ciation with implicated code
RQ4: Is general experience associated with implicated code
for more severe defects?
If experienced authors always take the brunt of program-
ming the most risky parts of the system, it can happen,
oddly enough, that they are responsible for more implicated
code [47]. We come at this question somewhat indirectly,
asking if experienced developers are more responsible for
implicated code corresponding to severe bugs. We therefore
compare weighted experience as is calculated in RQ3 but
for diﬀerent bug severities. Thus, we compare weighted ex-
perience of implicated code after breaking the data down
by severity. We use two partitions. The severe partition
contains “blocker” and “critical” and rest of the categories
are partitioned as not severe . We then compare these two497G
GG
GG
GGG G
Severe Bugs Non−Severe Bugs1 2 3 4 5Apache − Weighted Experience (Severity)Log10(Weighted Experience)
G
GGGG
G
G
Severe Bugs Non−Severe Bugs1 2 3 4 5 6Evolution − Weighted Experience (Severity)Log10(Weighted Experience)
Figure 9: Weighted Author Experience for Severe Bugs
and Other Bugs in (a) Apache (b) Evolution.
distributions’ weighted experience using boxplots to see if
the general experience is more associated with implicated
code for more severe bugs (alternative hypothesis).
Figure 9 depicts the boxplot of weighted experience of
corresponding implicated code for severe and non severe
bugs. As is apparent from the ﬁgure and supported by
the Wilcoxon rank test (p-values are presented in table 2)
experienced authors may not be introducing signiﬁcantly
more severe defects than non experienced developers. This
suggests a lack of association of experience and the authorshi p
of code involving severe defects.
Experienced developers are not clearly associ-
ated with code implicated in severe defects.
6. DISCUSSION
Our ﬁndings have several important implications on exist-
ing software development practice. We found that implicated
code is mostly contributed by a single author. This ﬁnding
provides empirical support for Linus’ Law (as asserted by
Raymond [37]). Perhaps, at this ﬁne-grained level, program-
mers could eﬃciently review any contributions by their peers,
and perhaps serendipitously ﬁnd and ﬁx errors before they
are exposed and reported into the issue tracking system.
Interestingly, this appears to conﬂict with the ﬁndings
by [11, 41] which suggest that more contributors lead to
more defects However, it should be noted these 2 results
were both based on aggregated studies at the module and ﬁle
levels, whereas ours is a ﬁne-grained study at the line level
of the implicated code. Perhaps at a coarse-grained level,
it’s more likely that contributors aren’t as aware of each
other’s work; this mutual “long-distance” ignorance could
lead to cross-purposes and thus errors. However, Weyuker
et al. [46] and Graves et al. [25], in another coarse-grained
study, found that number of contributors were not a factor
in defect-proneness. Clearly, the number of authors plays a
complex role in defect-proneness, and further study is needed.
We speculate e.g.,that a code fragment with multiple authors
in its backward slice is more likely to show defects if authors’
contributions are pairwise distant in the same slice.
We also found that specialized experience consistently
helps in writing less defective code. This suggests a very
targeted inspection approach, based on prior history of the
contributions to a given ﬁle; rather than directed inspection
eﬀorts at a coarse level to an entire ﬁle, this approach would
direct inspectors to a speciﬁc set of changed or new lines, viz.,
those written by developers into a ﬁle, when those developershad less prior experience in that ﬁle. Given the isolated
nature of implicated code, written by mostly one developer,
even a review of only newly introduced code may yield fewer
defects. Moreover, instead of making changes themselves,
less familiar developers might be required to collaborate with
more familiar developers, subject to resource availability.
On the other hand, the importance of general experience
on implicated code is not clearly evident. More research
is needed to assess the precise impact. But the ﬁndings
of RQ3 and RQ4 suggest that general experience may not
be very eﬀective to ensure code quality. However, it is not
our intention to downplay the value of general experience.
Earlier ﬁndings suggest that general experience could cut
down integration time and foster productivity [13]. Also,
experience of one domain may provide an analogous solution
for another domain [40]. Moreover, repetitive use of partic-
ular APIs or working on a particular system could create
episodic knowledge . Such episodic knowledge could help in
writing succinct code or designing a better software [38].
7. THREATS TO VALIDITY
7.1 Construct Validity
There are questions concerning whether we are truly mea-
suring just and all of the implicated code.
Bugs were collected from the Bugzilla databases for each
project, and thus may not represent the complete set of
all bugs. However, in all cases, Bugzilla is the designated
repository of bug reports; we also focus on the ﬁxed bugs,
and thus the ones the community deemed worthy of repair.
We used an automated bug linking process which may
be inaccurate. There may be both false positives and false
negatives in the linked set. In a prior study [10] we evaluated
the false positive and false negative rates and found them to
be quite low. Bias in ﬁx reporting [10] could have resulted in
some ﬁxes being unreport, and might have aﬀected our results.
In addition, our implicated code identiﬁcation algorithm us es
thediff tool. It is entirely possible that some of the changes
in a revision marked as a bug ﬁx are not, in fact, ﬁxing lines
which caused the bug. In lieu of this problem we use an
approach used by well known prior studies [43]. Accuracy
in identifying bug introducing changes may be increased by
using advanced algorithms [29]. We use git blame which
we have found produces highly accurate results, even in the
presence of white-space changes and code movement [12].
We also ignore comments and blank lines in implicated code.
Labeling of bug severity may not be entirely accurate and
depends on many factors including external factors such as
bug reporter and internal factors such as the criticality of
impact. However, we only focus on the ﬁxed bugs, which the
community deemed worthy of repair, and thereby we believe
they are more accurate in their severity labeling. Availability
of large number of bugs also provides better resilience against
possible bias.
We compared properties of implicated code against ran-
domly chosen background code. Our chosen random code
distribution may not be representative of the implicated code
distribution. We did take several measures, as described in
Section 3.5 to mitigate this threat.
Sometimes “implicated code ” may not really be buggy
code, but rather a kind of “innocent bystander” that gets
changed to remedy a ﬂaw elsewhere, or even an unrelated
change that just happened in a bug-ﬁxing commit. However,
one can reasonably expect that usually implicated code is
indeed buggy code, and given the large number of ﬁx inducing498hunks (ranging from 1,687 for Apache to 31,184 for Gimp)
one can reasonably expect that the signals we see in the data
are robust. Another issue is that bugs may have arisen due
tomissing code, such as an omitted case in a switch . We
haven’t addressed this issue.
For measuring general experience we consider only the
contribution in that particular project. However, developers
may contribute in several other projects, thereby garnering
experience from those too. Measuring external experience
would be diﬃcult. However, given the long history of our
studied projects and the participation of a large number
of developers, it is reasonable to assume that our method
reasonably estimates general experience of the developers.
7.2 Internal Validity
One issue (raised by one of the reviewers) is that implicated
code is more likely to be “logically cohesive”, and thus edite d
by a single author. This requires further study, beginning
with a clear deﬁnition of“logically cohesive”code. Meanwh ile,
as seen with RQ1c above, even multiply authored implicated
code generally shows higher ownership than background code.
We present evidence that implicated code tends to be
associated with fewer authors. We also ﬁnd that authors
with prior experience of a ﬁle tend to be associated with
less implicated code in that ﬁle. While strong correlation
exists, the stringent requirements for causality have not been
shown [27]. Despite this, our results do indeed show strong
evidence that buggy code is diﬀerent than other background
code, and provide support for further research examining why
it is diﬀerent and whether these properties can be utilized to
develop better process model or to predict software failures.
7.3 External Validity
In an attempt to address the generalizability of our ﬁndings,
we have studied four real software projects that represent
varying software processes and governance styles [9]. How-
ever, while it is reasonable to believe that our results are
representative of open source software, it is unclear how
well they generalize to commercial software, which may have
diﬀerent ownership policies and behavior. Again, we have
provided evidence that bugs are diﬀerent than background
code, they have fewer number of authors and they are less
likely to be introduced by the primary contributor of the
target ﬁle.
All of our study subjects were written in C. While C is
a very popular language, it is remarkably diﬀerent from
Object Oriented languages like Java. Given the rich encap-
sulation support of OOP, it is entirely possible that topic
knowledge [38] would play a larger role and episodic knowl-
edge [38] would be more reusable. It would be an interesting
future work to study such possibilities.
8. CONCLUSION
We have studied 4 open source projects to understand
impact of ownership and developer experience on software
defects. Using version control history, we examine contri-
butions to code fragments that are actually repaired to ﬁx
bugs. Are these “troubled” code fragments the result of
contributions from many? or from one? Does experience
matter? What type of experience? We ﬁnd that implicated
code is more strongly associated with a single developer’s
contribution; our ﬁndings also indicate that author’s spe-
cialized experience in the target ﬁle is more important than
general experience. Our ﬁndings suggest that quality controleﬀorts could be proﬁtably targeted at changes made by single
developers with limited prior experience on that ﬁle.
Acknowledgments
We thank A. Bachmann and A. Bernstein for the bug linking
data. We were supported by an IBM Faculty Fellowship, a
gift from Microsoft Research and NSF grants SoD-TEAM
0613949 and SHF-Medium 0964703. Any opinions, ﬁndings,
and conclusions or recommendations expressed in this mate-
rial are those of the authors and do not necessarily reﬂect
the views of the National Science Foundation. We grate-
fully acknowledge helpful feedback from anonymous ICSE
reviewers.
9. REFERENCES
[1]Software errors cost u.s. economy $59.5 billion annually,
http://www.nist.gov/public aﬀairs/releases/n02-
10.htm.
[2]Identifying reasons for software changes using historic
databases , volume 0, Los Alamitos, CA, USA, August
2002. IEEE Computer Society.
[3]Succession: Measuring transfer of code and developer
productivity , Washington, DC, USA, June 2009. IEEE
Computer Society.
[4] L. Argote. Organizational Learning: Creating,
Retaining, and Transferring Knowledge . Kluwer
Academic Publishers, Norwell, MA, USA, 1999.
[5] L. Argote, S. L. Beckman, and D. Epple. The
persistence and transfer of learning in industrial
settings. Manage. Sci. , 36(2):140–154, 1990.
[6] A. Bachmann and A. Bernstein. Software process data
quality and characteristics: a historical view on open
and closed source projects. In IWPSE-Evol ’09 , 2009.
[7] R. D. Banker, G. B. Davis, and S. A. Slaughter.
Software development practices, software complexity,
and software maintenance performance: A ﬁeld study.
MANAGEMENT SCIENCE , 44(4):433–450, April 1998.
[8]K. Beck. Embracing change with extreme programming.
IEEE computer , 32(10):70–77, 1999.
[9] J. Berkus. The 5 types of open source projects, 2007.
March 20, 2007
http://www.powerpostgresql.com/5 types.
[10]C. Bird, A. Bachmann, E. Aune, J. Duﬀy, A. Bernstein,
V. Filkov, and P. Devanbu. Fair and balanced?: bias in
bug-ﬁx datasets. In ESEC/FSE ’09 , pages 121–130,
New York, NY, USA, 2009. ACM.
[11] C. Bird, N. Nagappan, B. Murphy, H. Gall, and
P. Devanbu. An analysis of the eﬀect of code ownership
on software quality across windows, eclipse, and ﬁrefox.
Technical report, University of California, Davis, 2010.
Submitted to FSE 2010.
http://wwwcsif.cs.ucdavis.edu/ bird/.
[12] C. Bird, P. Rigby, E. Barr, D. Hamilton, D. German,
and P. Devanbu. The Promises and Perils of Mining
Git. In MSR 2009 , 2009.
[13] W. Boh, S. Slaughter, and J. Espinosa. Learning from
experience in software development: A multilevel
analysis. Management Science , 53(8):1315–1331, 2007.
[14] F. Brooks. The mythical man-month . Addison-Wesley,
1995.
[15] M. Cataldo, P. Wagstrom, J. Herbsleb, and K. Carley.
Identiﬁcation of coordination requirements:
implications for the Design of collaboration and499awareness tools. Proceedings of the 2006 20th
anniversary conference on Computer supported
cooperative work , pages 353–362, 2006.
[16] B. Curtis and H. Iscoe. N.(1988). A ﬁeld study of the
software design process for large systems.
Communications of the ACM , 31(11):1268–1287.
[17] B. Curtis, H. Krasner, and N. Iscoe. A ﬁeld study of
the software design process for large systems.
Communication of the ACM , 31(11):1268–1287, 1988.
[18] B. Curtis, E. M. Soloway, R. E. Brooks, J. B. Black,
K. Ehrlich, and H. R. Ramsey. Software psychology:
the need for an interdisciplinary program. pages
150–164, 1987.
[19] E. Darr, L. Argote, and D. Epple. The acquisition,
transfer, and depreciation of knowledge in service
organizations: Productivity in franchises. Management
Science , 41(11):1750–1762, 1995.
[20]J. A. Espinosa. Shared mental models and coordination
in large-scale, distributed software development . PhD
thesis, Pittsburgh, PA, USA, 2002.
[21] M. Fischer, M. Pinzger, and H. Gall. Populating a
release history database from version control and bug
tracking systems. In ICSM ’03 , pages 23+, Washington,
DC, USA, 2003. IEEE Computer Society.
[22] D. Freedman. Ecological inference and the ecological
fallacy. International encyclopedia of the social and
behavioral sciences , pages 4027–4030, 2004.
[23] T. Fritz, G. C. Murphy, and E. Hill. Does a
programmer’s activity indicate knowledge of code? In
ESEC-FSE ’07 , pages 341–350, New York, NY, USA,
2007. ACM.
[24]T. Girba, A. Kuhn, M. Seeberger, and S. Ducasse. How
developers drive software evolution. In Eighth
International Workshop on Principles of Software
Evolution (IWPSE’05) , pages 113–122, Washington,
DC, USA, 2005. IEEE.
[25] T. L. Graves, A. F. Karr, J. S. Marron, and H. Siy.
Predicting fault incidence using software change history.
IEEE Trans. Softw. Eng. , 26(7):653–661, 2000.
[26] J. D. Herbsleb and A. Mockus. An empirical study of
speed and communication in globally distributed
software development. IEEE Trans. Softw. Eng. ,
29(6):481–494, 2003.
[27] S. Kan. Metrics and models in software quality
engineering . Addison-Wesley Longman Publishing Co.,
Inc. Boston, MA, USA, 2002.
[28] S. Kim, K. Pan, and E. E. J. Whitehead. Memories of
bug ﬁxes. In SIGSOFT ’06/FSE-14 , pages 35–45, New
York, NY, USA, 2006. ACM.
[29]S. Kim, T. Zimmermann, K. Pan, and J. Jr. Automatic
identiﬁcation of bug-introducing changes. In ASE ’06 ,
pages 81–90, Washington, DC, USA, 2006. IEEE
Computer Society.
[30] A. Meneely and L. Williams. Secure open source
collaboration: an empirical study of linus’ law. In CCS
’09, pages 453–462, New York, NY, USA, 2009. ACM.
[31]A. Mockus and D. M. Weiss. Predicting risk of software
changes. Bell Labs Technical Journal , 5(2):169–180,
2000.
[32]A. Mockus and D. M. Weiss. Predicting risk of software
changes. Bell Labs Technical Journal , 5(2):169–180,
2000.[33]N. Nagappan, B. Murphy, and V. Basili. The inﬂuence
of organizational structure on software quality: an
empirical case study. In ICSE ’08 , pages 521–530.
ACM, 2008.
[34] M. Pinzger, N. Nagappan, and B. Murphy. Can
developer-module networks predict failures? In
SIGSOFT ’08/FSE-16 , pages 2–12, New York, NY,
USA, 2008. ACM.
[35]G. P. Pisano, R. M. J. Bohmer, and A. C. Edmondson.
Organizational diﬀerences in rates of learning:
Evidence from the adoption of minimally invasive
cardiac surgery. Manage. Sci. , 47(6):752–768, 2001.
[36] J. Ratzinger, M. Pinzger, and H. Gall. Eq-mine:
Predicting short-term defects for software evolution. In
M. B. Dwyer and A. Lopes, editors, Fundamental
Approaches to Software Engineering , volume 4422 of
Lecture Notes in Computer Science , chapter 3, pages
12–26. Springer Berlin Heidelberg, Berlin, Heidelberg,
2007.
[37]E. Raymond. The cathedral and the bazaar: musings on
Linux and open source by an accidental revolutionary .
O’Reilly & Associates, Inc. Sebastopol, CA, USA, 2001.
[38] P. N. Robillard. The role of knowledge in software
development. Communications of the ACM ,
42(1):87–92, January 1999.
[39] M. Sacks. On-the-Job Learning in the Software
Industry. Corporate Culture and the Acquisition of
Knowledge. Quorum Books, 88 Post Road West,
Westport, CT 06881., 1994.
[40] M. A. Schilling, P. Vidal, R. E. Ployhart, and
A. Marangoni. Learning by doing something else:
Variation, relatedness, and the learning curve.
MANAGEMENT SCIENCE , 49(1):39–56, January
2003.
[41]T. I. Seifert and B. Paech. Exploring the relationship of
history characteristics and defect count: an empirical
study. In DEFECTS ’08 , pages 11–15, New York, NY,
USA, 2008. ACM.
[42] R. Shatnawi and W. Li. The eﬀectiveness of software
metrics in identifying error-prone classes in post-release
software evolution process. J. Syst. Softw. ,
81(11):1868–1882, 2008.
[43] J. ´Sliwerski, T. Zimmermann, and A. Zeller. When do
changes induce ﬁxes? In MSR ’05 , pages 1–5. ACM,
2005.
[44] D. ˇCubrani´ c and G. C. Murphy. Hipikat:
recommending pertinent software development artifacts.
InICSE ’03 , pages 408–418, Washington, DC, USA,
2003. IEEE Computer Society.
[45] D. G. Wastell. Learning dysfunctions in information
systems development: overcoming the social defenses
with transitional objects. MIS Q. , 23(4):581–600, 1999.
[46] E. Weyuker, T. Ostrand, and R. Bell. Do too many
cooks spoil the broth? using the number of developers
to enhance defect prediction models. Empirical
Software Engineering , 13(5):539–559, October 2008.
[47]A. Zeller. Why Programs Fail, Second Edition: A Guide
to Systematic Debugging . Morgan Kaufmann, 2 edition,
June 2009.
[48] Y. Zhou and H. Leung. Empirical analysis of
object-oriented design metrics for predicting high and
low severity faults. IEEE Trans. Softw. Eng. ,
32(10):771–789, 2006.500