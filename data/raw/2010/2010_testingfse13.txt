Boosting Concolic Testing via Interpolation
Joxan Jaffar, Vijayaraghavan Murali
National University of Singapore
{joxan, m.vijay}@comp.nus.edu.sgJorge A. Navas
The University of Melbourne
jorge.navas@unimelb.edu.au
Abstract . Concolic testing has been very successful in au-
tomatically generating test inputs for programs. However
one of its major limitations is path-explosion that limits the
generation of high coverage inputs. Since its inception sev-
eral ideas have been proposed to attack this problem from
various angles: dening search heuristics that increase cov-
erage, caching of function summaries, pruning of paths us-
ing static/dynamic information etc. We propose a new and
complementary method based on interpolation , that greatly
mitigates path-explosion by subsuming paths that can be
guaranteed to not hit a bug. We discuss new challenges in
using interpolation that arise specically in the context of
concolic testing. We experimentally evaluate our method
with dierent search heuristics using Crest, a publicly avail-
able concolic tester.
Categories and Subject Descriptors
D.2.5 [ Software Engineering ]: Testing and Debugging|
symbolic execution, testing tools
General Terms
Algorithms, Reliability
Keywords
Concolic testing, interpolation, symbolic execution
1. INTRODUCTION
Testing is the most commonly used method to ensure soft-
ware quality. It executes a given program with some inputs
and the objective is then to nd bugs or validate the program
with respect to the given inputs. Traditionally, testing was
carried out using manually generated inputs which became
cumbersome and ineective. Random testing alleviates this
problem by generating random test inputs, but suers from
poor code coverage. Recently, more intelligent methods [15,
9, 5, 3] based on concolic testing , a variant of symbolic execu-
tion, have emerged, that generate inputs by systematically
exploring program paths attempting to increase coverage.The main idea of concolic testing is to execute the program
simultaneously with concrete values and symbolic values.
When the program is executed, symbolic constraints along
the executed path are collected in a formula called path con-
dition . Then, a branch is picked and negated from the path
condition resulting in a new formula which is then fed to a
constraint solver to check for satisability. If it is satisable,
concrete test inputs are generated to follow the new feasible
path. If it is unsatisable, the new path is infeasible and
another branch has to be picked to be negated. This way
concolic testing attempts to improve the poor code coverage
of random testing. A key characteristic of concolic testing is
that path conditions can be simplied using concrete values
whenever the decidability of their symbolic constraints goes
beyond the capabilities of the underlying constraint solver.
One major problem with concolic testing is that there are
in general an exponential number of paths in the program
to explore, resulting in the so-called path-explosion prob-
lem. Recently, several methods have been proposed to at-
tack this problem from various angles: using heuristics fo-
cused on branch coverage [3], function summaries [8], using
static/dynamic program analysis [2] and so on. We pro-
pose a new method based on interpolation , largely comple-
mentary to existing approaches, that signicantly mitigates
path-explosion by pruning a potentially exponential number
of paths that can be guaranteed to not encounter a bug.
Our method, inspired by [12], aims at assisting concolic test-
ing by making use of the concept of interpolation [7] in-
terleaved with the concolic execution process. The use of
interpolation for pruning paths in the context of symbolic
execution is well-known (see e.g., [12, 14]). The idea is as
follows: rst, assume that the program is annotated with
certain bug conditions of the form \ ifCthenbug", where
if the condition Cevaluates to true along a path, the path
is buggy. Then, whenever an unsatisable path condition is
fed to the solver, an interpolant is generated at each program
point along the path. The interpolant at a given program
point can be seen as a formula that succinctly captures the
reason of infeasibility of paths at the program point. In other
words it succinctly captures the reason why paths through
the program point are not buggy. As a result, if the pro-
gram point is encountered again through a dierent path
such that the interpolant is implied, the new path can be
subsumed , because it can be guaranteed to not be buggy.
The exponential savings are due to the fact that not only is
the new path subsumed, but also the paths that this newPermission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proï¬t or commercial advantage and that copies bear this notice and the full citation
on the ï¬rst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speciï¬c permission and/or a
fee. Request permissions from Permissions@acm.org.
ESEC/FSEâ€™13 , August 18â€“26, 2013, Saint Petersburg, Russia
Copyright 2013 ACM 978-1-4503-2237-9/13/08...$15.00
http://dx.doi.org/10.1145/2491411.2491425
23path would spawn by negating its branches.
Unfortunately, methods such as [12, 14] cannot be used di-
rectly for concolic testing due to several challenges. First,
the soundness of these methods relies on the assumption
that an interpolant at a node has been computed after ex-
ploring the entire \tree" of paths that arise from the node.
In concolic testing, this assumption is invalid as the tester
can impose an arbitrary search order. For example, concolic
testers such as Crest [3] and KLEE [4] use often many heuris-
tics that may follow a random walk through the search space,
thus making this method unsound. To address this problem,
we need to keep track of nodes whose trees have been ex-
plored fully (in which case we say the node is annotated with
afull-interpolant ) or partially (similarly, a half-interpolant ).
Under this new setting, only nodes with full-interpolants are
capable of subsumption in a sound manner. As a result, the
amount of subsumption depends on how often nodes get an-
notated with full-interpolants from the paths explored by the
concolic tester. Unfortunately our benchmarks in Section 6
showed that the above method by itself results in very few
nodes with full-interpolants, thereby providing poor bene-
t to the concolic tester, because the tester rarely explores
the entire tree of paths arising from a node. Hence, an im-
portant challenge now is to \accelerate" the formation of
full-interpolants in order to increase subsumption. For this,
we introduce a novel technique called greedy conrmation
that performs limited path exploration (i.e., execution of a
few extra paths) by itself, guided by subsumption, with an
aim to produce a full-interpolant at nodes currently anno-
tated with a half-interpolant. It is worth mentioning that
this execution of few paths is done without interfering with
the search order of the concolic tester. This technique ul-
timately resulted in a signicant increase in subsumption
for our benchmarks, and is vital for the eectiveness of our
method.
We implemented our method and compared it with a pub-
licly available concolic tester, Crest [3]. We found that for
the price of a reasonable overhead to compute interpolants, a
large percentage of paths executed by those heuristics can be
subsumed thereby increasing their coverage substantially.
2. RELATED WORK
The main innovation introduced by concolic testing (orig-
inally presented in DART [9] and Cute [15]) was the fact
that concrete inputs can be generated based on some intel-
ligent decision by symbolically negating one of the executed
branches. Since the seminal papers of [9, 15] many works
have been published improving concolic testing in dierent
ways. We limit our discussion to related works that attempt
at mitigating the scalability issues in concolic testing due to
the exponential numbers of paths.
Recent extensions (e.g., EXE [5], Crest [3], Sage [10] and
KLEE [4]) have tried to address this challenge by using novel
heuristics to guide the exploration of paths improving the
naive depth-rst search strategy originally used in DART.
They target branch coverage (i.e., number of branches evalu-
ated to true and false) rather than path coverage . Although
branch coverage does not suer from path-explosion it is un-
derstood that the ultimate goal of the concolic tester is pathcoverage. Branch coverage is just an inexpensive measure of
the quality of a test suite, and a good branch coverage is
more of a necessary requirement for quality than sucient.
The main dierence with the above methods is that we focus
on path coverage rather than branch coverage while respect-
ing those search strategies.
Another interesting line of research has addressed the caching
offunction summaries as a way of reducing the exponential
number of paths (e.g., SMART [8] and [1]). Our method
performs function inlining (not intelligent interprocedurally)
while these methods execute the DART algorithm to gen-
erate reusable summaries (not intelligent intraprocedurally).
This suggests that both approaches are orthogonal and could
work together to benet from each other.
The closest related tester to ours in the line of pruning paths
is [2]. This method eliminates redundant paths by analysing
the values read and written in memory by the program. The
main idea is to prune paths that only dier in program vari-
ables that are not subsequently read (i.e., dead variables ).
We share with them the high-level idea of removing irrel-
evant information in order to increase the chances of sub-
sumption, but the similarity ends there. The most impor-
tant dierence is that [2] denes \irrelevant information" us-
ing live (dead) variables so the subsumption test is simply
a subset operation. We use interpolation to prune paths
so our subsumption test involves logical entailment checks,
which are more expensive. However, interpolants are much
more powerful for subsumption, which can result often in a
greater amount of pruned paths. We exemplify these dier-
ences with [2] through an example in Section 3.
Finally, as mentioned before, we have been clearly inspired
by [12] in the idea of interpolating infeasible paths which
has been also applied in [14]. However, [12, 14] assume full
control of the search space by performing a DFS-traversal
to compute their interpolants, which ensures an interpolant
at a node always represents the entire tree of paths below a
node. This is the key to making sound subsumption. This
assumption is no longer valid in concolic testing since the
tester controls the search space using some heuristic which
may not be DFS. Interestingly, Crest [3] compares dier-
ent heuristics for concolic testing and concluded that DFS
is actually the worst in terms of branch coverage. Thus, [12,
14] are not readily suitable for concolic testing. More im-
portantly, as we will see in Section 6, even if somehow the
DFS-restriction is lifted in these methods (e.g., by augment-
ing them with half and full-interpolants), it is not enough
to provide a reasonable benet to concolic testing, as they
scale poorly without greedy conrmation.
3. RUNNING EXAMPLE
Consider the program in Fig. 1, where a read() call signals
the concolic tester to generate a concrete input. In this
case, the inputs are used to decide the program's control
ow. Assume initially the concolic tester generates a positive
value for both inputs. This drives the tester down the path
`1`2`3`4`5`6`8, which is shown by the left-most
path in the program's symbolic execution tree. A symbolic
execution tree represents the set of paths traversed by the
program where each node corresponds to a program location
and an edge between two nodes corresponds to the program24`1int s=0;
`2if (read())
`3 s=s+1;
`4if (read())
`5 s=s+2;
`6if (s>3)
`7 bug;
`8

:{â‰¤0}

=0
read() â‰ 0read() =0
:{â‰¤1}
read() â‰ 0+=1
+=2
â‰¤3
:{}  :{}â€²
read() =0
:{â‰¤3}
â€²
Figure 1: A program and its symbolic execution tree
transition.
Now, the concolic tester provides us this path, on which
we perform symbolic execution and annotate it with inter-
polants in a backward manner. At `8the path is not buggy
and there is no infeasibility, therefore the interpolant true is
stored there (denoted by `8:ftrueg). Propagating this to
`6we note that there is another branch (to `7) that has not
been explored, so we annotate the interpolant true at`6as
ahalf-interpolant to denote this fact. Once a node has been
annotated with a half-interpolant, we stop and give control
back to the concolic tester.
Assume now the concolic tester attempts to \turn-around"
at`6into the path `1`2`3`4`5`6`7. This path is
infeasible as its path condition s= 0^s0=s+ 1^s00=s0+
2^s00>3 is unsatisable (for simplicity we omitted the read
constraints). Now we generate interpolants for this path by
rst annotating `7:ffalsegbecause it is an unreachable
node. Propagating this back to `6through the label s>3,
we obtain the interpolant s31. We now note all paths
from`6have been explored, so we conjoin all interpolants
at`6(true^s3) annotating it with the full-interpolant
s3, denoting that the entire tree of paths under `6has
been explored.
Now, when the concolic tester generates a zero for the sec-
ondread() and executes the path `1`2`3`4`0
6, we check
if the path condition at `0
6,s= 0^s0=s+1 implies the full-
interpolant at `6,s03 (after proper renaming). It does, so
we can guarantee the entire tree of paths below `0
6to not be
buggy and subsume it. The exponential savings is because
we saved the concolic tester from executing a potentially ex-
ponential number of paths in the tree under `0
6(in this case,
two paths, but in general exponential). Importantly, note
that only full-interpolants are capable of subsumption and
the amount of savings provided by our method is directly
proportional to the number of full-interpolants formed from
exploring entire trees of paths.
Unfortunately, the method discussed so far has a catch. We
1Note that for this example we use interpolants based on
weakest preconditions, but our method is not limited to
them and any interpolation method can be used.conveniently assumed the concolic tester would execute the
paths in that specic order, which resembles a depth-rst
search (DFS). If after executing the rst path the tester
did not attempt to turn-around at `6(thus leaving `6with a
half-interpolant), `6would not have been able to subsume `0
6.
Even worse is the fact that because of the half-interpolant
at`6all its ancestors along the path also become incapable
of subsumption, thereby losing a great amount of savings.
Thus, we need to\accelerate"the formation of full-interpolants
instead of simply relying on the concolic tester to explore
the paths. However a challenge in concolic testing is that
whatever technique is employed for this purpose, it must al-
ways stay proportional to the executed path, that is, not be-
come intractable. For this, we introduce a technique called
greedy conrmation that is both tractable (proportional to
the path length in the worst-case) and accelerates the for-
mation of full-interpolants resulting in a substantial increase
in subsumption.
The basic idea is while backtracking along a path, once we
encounter a node with another branch that has not been ex-
plored by the concolic tester, instead of simply annotating
the node with a half-interpolant and halting the process, we
explore the other branch ourselves (while the concolic tester
is waiting to generate the next path) and attempt to conrm
whether a full-interpolant can be generated from it, so that
we can annotate the node as full and continue the backward
propagation. However, the sub-tree under the other branch
could be exponentially large, in which case we must avoid
exploring it to remain tractable. Hence we introduce a re-
striction: while exploring the sub-tree, each program point
is allowed to be explored at most once. If a program point
is visited along more than one path, we demand that it be
subsumed. If not, we declare that the greedy conrmation
process failed at the original branch node, which will remain
annotated with a half-interpolant.
The reasoning behind the restriction is as follows. During
greedy conrmation, we want to give each program point at
least one chance to become subsumed, so we must allow at
least one visit to each program point. However, we do not
want to resort to a full search within the sub-tree, which
could become intractable. Thus, this restriction ensures that
the \search" is linearly bounded by the longest path in the
program2. The motivation behind this technique is that the
dierence between the two branching sub-trees might not
aect the bug condition and so one tree can quickly subsume
the other.
Let us see how greedy conrmation works on the example.
When backtracking along the rst path, `1`2`3`4`5`6`8,
once we reach `6with the interpolant true, we trigger greedy
conrmation and attempt to explore the other tree under `6.
We immediately notice the other branch to `7is infeasible,
and annotate it with false . Propagating this back to `6we
get the full-interpolant s3, which can now be propagated
back further. Hence at `5we get the full-interpolant s
1. Propagating this to `4makes it a half-interpolant since
there is another branch from `4to`0
6. Triggering greedy
2We tried restricting to two, three and other constant in-
stances of each program point, but we found no dierence
in our benchmarks.25conrmation, this time at `4, we notice that `0
6is subsumed
by`6with the full-interpolant s3. Propagating this to `4
and conjoining it with the existing half-interpolant, we get
s3^s1, or simply s1 at`4, which is now a full-
interpolant that can be propagated back. Similar reasoning
at`2subsumes a large tree of paths under `0
4resulting in
the full-interpolant s0 at`2.
Now, in this contrived example, if the concolic tester ex-
ecutes any path in the program, we are able to subsume
it immediately at `2. Essentially, greedy conrmation has
pushed subsumption from happening at lower levels in the
symbolic execution tree to upper levels. A simple but eec-
tive optimisation is if greedy conrmation failed at a node,
thereby leaving it with a half-interpolant, we can simply halt
the annotation process because there is no use propagating
a half-interpolant to the node's parents.
Finally, note importantly that this example does not contain
any dead variables, so techniques like [2] will not be able to
prune any path, whereas with interpolation we are able to.
Remark. The correctness of our method relies on the fact
that whenever we subsume a path (i.e., we skip its execu-
tion) we can ensure that the path will not be buggy (see
Theorem 5.1 in Sec. 5). This guarantee is only feasible if
the program is annotated with assertions. Without them
we have no basis to make such a guarantee and subsume the
path. A key observation is that without assertions each path
is unique because at least it will dier in one branch from the
rest of paths, and hence, there is no hope for boosting the
concolic execution. However, with an assertion many paths
can be considered equivalent and this allows our method re-
porting savings. In conclusion, our method is only eective
if we consider programs annotated with assertions. We be-
lieve that this step can be done automatically and it is not
a big hassle in practice.
4. PRELIMINARIES
Syntax . Most of the formalism used here has been borrowed
from [11, 13]. We restrict our presentation to a simple im-
perative programming language where all basic operations
are either assignments or assume operations, and the domain
of all variables are integers. The set of all program variables
is denoted by Vars. An assignment x := e corresponds to
assign the evaluation of the expression eto the variable x. In
theassume operator, assume (c), if the Boolean expression
cevaluates to true, then the program continues, otherwise
it halts. The set of operations is denoted by Ops. We then
model a program by a transition system . A transition sys-
tem is a quadruple h;I; !;Oiwhere  is the set of states
andI is the set of initial states.  ! Ops
is the transition relation that relates a state to its (possi-
ble) successors executing operations. This transition rela-
tion models the operations that are executed when control
ows from one program location to another. We shall use
`op   !`0to denote a transition relation from `2 to`02
executing the operation op2Ops. Finally,O is the set
of nal states.
Symbolic Execution . Asymbolic state is a tripleh`;s;i.
The symbol `2 corresponds to the current program loca-tion. For clarity of presentation in our algorithm, we will use
special symbols for initial location, `start2I, nal location,
`end2O, and bug location `bug2O(if any). W.l.o.g we
assume that there is only one initial, nal, and bug location
in the transition system.
The symbolic store sis a function from program variables
to terms over input symbolic variables. Each program vari-
able is initialised to a fresh input symbolic variable. This is
done by the procedure initstore() . The evaluation JcK(s) of
a constraint expression cin a storesis dened recursively
as usual: JvK(s) =s(v) (ifcvis a variable), JnK(s) =n
(ifcnis an integer), Jeopre0K(s) = JeK(s)oprJe0K(s) (if
ceopre0wheree;e0are expressions and opris a rela-
tional operator <;>; =;! =;>=;<=), and Jeopae0K(s) =
JeK(s)opaJe0K(s) (ifceopae0wheree;e0are expressions
and opais an arithmetic operator + ; ;;:::).
Finally,  is called path condition , a rst-order formula over
the symbolic inputs that accumulates constraints which the
inputs must satisfy in order for an execution to follow the
particular corresponding path. The set of rst-order formu-
las and symbolic states are denoted by FOand SymStates ,
respectively. Given a transition system h;I; !;Oiand a
stateh`;s;i2SymStates , the symbolic execution of
`op   !`0returns another symbolic state 0dened as:
0,8
<
:h`0;s;^JcK(s)i ifopassume (c) and
^JcK(s) is satisable
h`0;s[x7!JeK(s)];iifopx := e(1)
Note that Equation (1) queries a constraint solver for sat-
isability checking on the path condition. We assume the
solver is sound but not necessarily complete. That is, the
solver must say a formula is unsatisable only if it is indeed
so.
Abusing notation, given a symbolic state h`;s;iwe de-
ne JK:SymStates!FOas the formula (V
v2VarsJvK(s))^
 where Vars is the set of program variables.
Asymbolic path 01:::nis a sequence of symbolic
states such that8i1inthe stateiis a successor
ofi 1, denoted as SUCC (i 1,i)3. A path 0
1:::nisfeasible ifnh`;s;isuch that JK(s) is
satisable. If `2Oandnis feasible then nis called
terminal state. Otherwise, if JK(s) is unsatisable the path
is called infeasible andnis called an infeasible state. If
there exists a feasible path 01:::nthen we
sayk(0kn) is reachable from0ink steps . We
say00is reachable from if it is reachable from in some
number of steps. A symbolic execution tree contains all the
execution paths explored during the symbolic execution of
a transition system by triggering Equation (1). The nodes
represent symbolic states and the arcs represent transitions
between states.
Concolic Testing . We rst dene a concolic path p as
the path executed by the concolic tester represented as a
sequence of program locations `1:::`nand dene the
3W.l.o.g, we assume each state has at most two successors.26GenericConcolic (programP, path p)
1:while termination conditions are not met do
2:bi:= pick a branch from p
3: p0:= construct a path passing through
the branches b0;:::;b i 1;bi
4: if9satisfying assignment IforcingPthrough p0then
5:q ConcretePath (P,I)
6: process qby either p qor
GenericConcolic (P,q)
7: endif
8:endwhile
Figure 2: A Generic Concolic Tester
transition relation `op   !`0as before. In order to manipu-
late concolic paths we also dene prex (p,`) of a path pwrt
a location`as the prex up to `without including `. We
also dene constraints (p) that maps a concolic path into a
formula representing the conjunction of the symbolic con-
straints along the path. Of course, this formula is properly
renamed (i.e., SSA form) to take into account variables that
are redened more than once.
We now show in Fig. 2 a generic algorithm that performs
concolic testing as described in [3]. The algorithm is generic
in the sense that it can be parameterised with dierent
search strategies by simply choosing dierent heuristics to
pick a branch at line 2. The algorithm is instantiated with
the program Pand an initial concolic path p, usually chosen
by running the program with random inputs. In line 2, a
branchbiis chosen from the path pbased on the heuristic
used. In line 3, a new path p0is built by keeping the prex
up tobiand negating the constraints at the branch bi. In
line 4 the symbolic constraints associated with p0are then
fed to a constraint solver to check for satisability. If they
are unsatisable, the algorithm returns to line 2 and picks
another branch to negate. Otherwise, an assignment I(con-
crete inputs) is extracted from the solver which is used to
guide the next concrete path along the negated branch. This
is done using the call to ConcretePath( P,I)in line 5. Once
the new path, say q, is executed, depending on the heuris-
tic, it is processed by either replacing the old path pwithq
or by making a recursive call to GenericConcolic withq
in line 6. The entire process continues until some termina-
tion condition is met (usually a xed number of iterations
or recursive call depth) shown in line 1.
5. CONCOLIC TESTING WITH INTERPO-
LATION
We now present our symbolic execution based algorithm that
runs synchronised with GenericConcolic and helps the
concolic testing strategy mitigate the path-explosion prob-
lem. First, we give few denitions critical to our algorithm:
Definition 1 (Craig Interpolant). Given two for-
mulasAandBsuch thatA^Bis unsatisable, a Craig in-
terpolant [7] (INTP (A;B)) is another formula 	such that
(a)Aj= 	, (b) 	^Bis unsatisable, and (c) all variables
in	are common to AandB.An interpolant allows us to remove irrelevant information
inAthat is not needed to maintain the unsatisability of
A^B. That is, the interpolant captures the essence of the
reason of unsatisability of the two formulas. Ecient in-
terpolation algorithms exist for quantier-free fragments of
theories such as linear real/integer arithmetic, uninterpreted
functions, pointers and arrays, and bitvectors (e.g., see [6]
for details) where interpolants can be extracted from the
refutation proof in linear time on the size of the proof.
Definition 2 (Full and half interpolants). An in-
terpolant annotated at a symbolic state is afull-interpolant
if either:
(a)is a leaf node (terminal, infeasible or subsumed) in
the symbolic tree, or
(b)80such that SUCC (;0),0is annotated with a full-
interpolant.
An interpolant that is not full is called a half-interpolant .
Definition 3 (Subsumption check). Given a current
symbolic state h`;s;iand an already explored symbolic
state0h`;;iannotated with the interpolant 	, we say
issubsumed by0(SUBSUME (;h0;	i)) if	is a full-
interpolant and JK(s)j= 	.
To understand the intuition behind the subsumption check,
it helps to know what a full-interpolant at a node actually
represents. A full-interpolant 	 at a node 0succinctly cap-
tures the reason of infeasibility of all infeasible paths in the
symbolic tree rooted at 0(let us call this tree T1). Then, if
another state at`implies 	, it means the tree rooted at
(say,T2) has exactly the same, or more, infeasible paths
compared to T1. In other words, T2has exactly the same,
orless feasible paths compared to T1. SinceT1did not con-
tain any feasible path that was buggy, we can guarantee the
same forT2as well, thus subsuming it. Note that if 	 was
a half-interpolant, we cannot guarantee this because T1has
not been fully explored.
5.1 Symbolic execution of paths with
interpolants
We rst present the algorithm SymExec (Figure 3) that takes
a path chosen by the concolic tester and executes it symbol-
ically in order to annotate each program point in it with
interpolants. A collection of such annotated paths implic-
itly represents the symbolic execution tree. Then, we will
present GenericConcolicWithPruning , a modied ver-
sion of GenericConcolic that can prune paths using the
annotated symbolic execution tree.
SymExec takes as arguments a symbolic state and a path
p. Typically, the state refers initially to the rst location
inp. It is also assumed that all procedures have access
to the program's original transition system P, which can
considered a global variable to the algorithm. The actual
object of interest is the annotation done by the procedure
which is persistent across multiple calls to it. For the sake of
simplicity, ignore the gray box which we will come to later.27SymExec (h`;;i,p)
1: if TERMINAL ()thenh	;fi:=htrue;fulli
2: else if INFEASIBLE ()thenh	;fi:=hfalse; fulli
3: else if90h`;;iannotated withh	0;fullisuch that SUBSUME (;h0;	0i)then
4:h	;fi:=h	0;fulli
5: elseh	;fi:=UnwindPath (;p)
6: endif
7: annotate withh	;fi
8: if f6=full then halt
UnwindPath (h`;s;i,p)
1: 	 := existing interpolant at 
2:
0,(
h`0;s;^JcK(s)iif`op   !`02p^opassume (c)
h`0;s[x7!Sx];iif`op   !`02p^opx := e andSxfresh variable
3: SymExec (0;p) and let the resulting annotation at 0beh	0;i
4: 	 := 	^INTP (constraints (prex (p;`));constraints (``0)^:	0)
5: if900h`00;;isuch that00is not annotatedh;fulliand SUCC (;00)then
6: GreedyConrmation (00) and let the resulting annotation at 00beh	00;f00i
7: if f00=full then
8: 	 := 	 ^INTP (constraints (prex (p;`)`);constraints (``00)^:	00)
9: endif
10: endif
11: f
full if8000s.t.SUCC (;000);000is annotated with h;fulli
half otherwise
12: returnh	;fi
Figure 3: Symbolic execution with interpolation along a path
First, lines 1-4 handle the three possible base cases for the
symbolic execution of a path: terminal, infeasible and sub-
sumed. In line 1, the function TERMINAL checks if`=`end.
If yes, the path can be fully generalised returning the full-
interpolant true, since it is feasible. In line 2, the function
INFEASIBLE checks if the path condition  of is unsatis-
able. If yes, again the path can be fully generalised, but
this time to false since it is infeasible. Finally, line 3 checks
if there is another state 0that can subsume , using the
function SUBSUME which implements Denition 3. If yes,
we can simply annotate with the (full) interpolant of 0
(line 4).
If the three base cases described above are not applicable,
the algorithm executes symbolically the next location of the
path by calling the procedure UnwindPath . This procedure,
at line 1, obtains any interpolant that may be annotated
at(it can be assumed that initially all symbolic states
are annotated with the default interpolant htrue; halfi). In
line 2, it executes one symbolic step along the path and
calls the main procedure SymExec with the new successor
state0(line 3). This mutually recursive call will in the
end annotate 0with an interpolant, say 	0. In line 4,
it computes the interpolant for the current state , using
	0and the constraints along the transition `op   !`0and
then conjoining the result with any existing interpolant at
. Finally, in line 11, it computes whether 	 is a full
or half-interpolant by implementing Denition 2. For ex-
ample, consider the case where constraints (prex (p;`))
x0= 1^x1=x0+ 3, constraints (``0)x2=x1+ 2,and 	0=x210 (after proper renaming). The call at
line 11 will generate an interpolant whose variables must
be common to constraints (prex (p;`)) and constraints (``0)
together with 	0. Thus, it can only include x1. An in-
terpolant generated by MathSAT [6] would be x14. A
weaker interpolant is x18 which can be computed by
weakest preconditions.
Returning back the tuple h	;fito the caller SymExec , at
line 7, it performs the actual annotation of . Finally, in
line 8, SymExec checks if it just annotated the state with
a full interpolant. If not, it halts in a normal manner and
returns control to the concolic tester (this can be seen as a
system-wide halt and is also notied to UnwindPath )4. The
reason for halting is that there is no use propagating the half
interpolants to parent nodes, because they will still remain
as half interpolants and be of no use for subsumption. In
other words, the algorithm only propagates full interpolants
to parent nodes. This is a simple but very eective optimi-
sation. Note however, that the annotation of states done so
far is still persistent. More importantly, a half interpolant
at a node now could get converted to a full interpolant later
when all successors of the node get full interpolants.
We now present a modied concolic tester GenericConcol-
icWithPruning , shown in Fig. 4, with the gray boxes high-
lighting the changes made in order to prune paths using
our method. First, after picking a branch bito negate
4This halting may make the algorithm seem \impure" but
we believe it makes it much easier to understand.28GenericConcolicWithPruning (programP, path p)
1:while termination conditions are not met do
2:bi:= pick a branch from p
3: p0:= construct a path passing through
the branches b0;:::;b i 1;bi
4: if IsSubsumedPath (p0)then continue
5: endif
6: if9satisfying assignment IforcingPthrough p0then
7:q ConcretePath (P,I)
8: SymExec (0h`start;initstore();truei;q)
9: process qby either p qor
GenericConcolicWithPruning (P,q)
10: endif
11:endwhile
Figure 4: A Generic Concolic Tester with Pruning
(line 2) and constructing the corresponding path p0that
goes through it (line 3), the concolic tester makes a call to
IsSubsumedPath with p0at line 4. This call queries the per-
sistent annotated symbolic execution tree to check whether
the path p0has been subsumed already. Note that this is just
a look-up and hence does not involve symbolic execution. If
yes, the tester can simply skip p0from being executed and
continue, thus pruning a potentially exponential number of
recursive calls at line 9 and all those paths it would have
generated. The second change is that once a path has been
concolically executed in line 7, the tester has to invoke our
method so that we can annotate it with interpolants. Thus,
line 8 makes a call to SymExec with the initial state of the
path0and the path qthat was executed.
5.2 Greedy Conï¬rmation to accelerate forma-
tion of full-interpolants
The method discussed so far has laid out the framework to
bring interpolation to concolic testing with the help of half
and full-interpolants. The benet provided by our technique
relies heavily on the formation of full-interpolants from paths
explored by the concolic tester (i.e., the amount of times f
is assigned fullinUnwindPath , line 11). Unfortunately the
method explained so far does not perform well in practice (as
we will see in Section 6) because relying only on the tester to
explore paths results in poor formation of full-interpolants.
This is because the tester's arbitrary search strategy seldom
explores the entire tree of paths arising from a node, a re-
quirement to annotate the node with a full-interpolant and
enable it to subsume other nodes.
Hence, an important challenge to deal with in concolic test-
ing is to \accelerate" the formation of full-interpolants. For
this, we introduce a new concept called greedy conrmation .
The basic idea was described in Section 3, here we explain it
in technical terms (gray box in Fig. 3). If the call to SymExec
(line 3 of UnwindPath ) returned successfully, it means 0, the
successor of , was annotated with a full-interpolant, say 	0.
Now, we check (at line 5) if the other successor of (say,
00) is also annotated with a full-interpolant. If not, then
this half-interpolant, say 	00, is the one preventing 	 from
becoming a full-interpolant. Now, we greedily explore 00by
ourselves in an attempt to conrm whether we can make 	00a full-interpolant, so that we can immediately upgrade 	 to
full, thus enabling it to perform subsumption. The intuition
behind this technique is that the dierence between the trees
below the two \siblings" 0and00might have no eect on
the bug condition, and since the tree below 0has been fully
explored, thereby annotated with full-interpolants, it opens
up opportunities to subsume nodes in the tree below 00.
Thus, we make the call to GreedyConrmation in line 6.
GreedyConrmation essentially performs symbolic execution
of00similar to SymExec . However, a key impediment is
that the symbolic execution of 00must not become expen-
sive and it should have a proportional cost to the length
of path, otherwise we consider it intractable. However the
tree under 00might be arbitrarily large and so we impose
an important restriction to maintain tractability: each pro-
gram point is allowed to be explored at most once during
greedy conrmation of 00. If a program point `kis vis-
ited in two states k1andk2then we demand that one of
them be subsumed (not necessarily by the other). If it is
not possible, then we declare that the invocation of greedy
conrmation failed at 00, which will not be annotated with
a full-interpolant. This restriction ensures that in the worst
case, greedy conrmation at any symbolic state is bounded
linearly by the length of the longest path in the program.
Note that GreedyConrmation does not need the help of the
concolic tester to explore its paths. In fact, this is the whole
point. Thus, it does not take as argument the path p, but
instead explores paths on its own. Since GreedyConrma-
tionis almost identical to SymExec , for lack of space, we do
not show any pseudo-code here. The only modication to
SymExec is to keep track of counters for each visited program
location that is not subsumed and stop if a counter becomes
greater than one. Finally, to perform the symbolic step
(line 3, UnwindPath ),GreedyConrmation must pick`op   !`0
from the program's transition system Pinstead of the path
p.
Coming back to the description of UnwindPath , consider the
call to GreedyConrmation at line 6 produced the annota-
tionh	00;f00iat00. If suppose 	00was a full-interpolant
(i.e., greedy conrmation succeeded), then at line 8 we aug-
ment the interpolant 	 at with this full-interpolant. More
importantly, at line 11, fwill be assigned fullbecause both
successors of (0and00) have been annotated with full-
interpolants (	0and 	00respectively). This directly accel-
erates the formation of full-interpolants, resulting in more
subsumption (line 3 of SymExec succeeding more often). In
Section 6 we will see this greatly increases the savings pro-
vided for concolic testing, while still maintaining tractabil-
ity.
Theorem 5.1. When a symbolic state is subsumed by
another state 0, the error location `bugis unreachable from
.
The proof follows trivially from the correctness of the algo-
rithms described in [12, 14] and the fact we only subsume if
the interpolant is full.29Corollary 5.2. IfGenericConcolic will execute a path
pthat leads to the bug location `bug, then GenericConcol-
icWithPruning will also execute p.
Theorem 5.1 states that we never subsume a node from
which the error location `bugwould be reachable. Thus,
it can be shown that we never prevent GenericConcol-
icWithPruning from executing a path that will reach `bug.
Finally, note that it is entirely possible that the concolic
tester can detect infeasible paths after simplifying a com-
plex constraint using concrete values that we cannot detect
symbolically. Therefore if unsatisability cannot be deter-
mined symbolically, we cannot compute interpolants. As a
result, the pruning of paths is limited only to those where
unsatisability does not require reasoning about complex
constraints. Moreover, it is possible for GreedyConrmation
to reach`bugduring the exploration of a path in the presence
of constraints whose unsatisability cannot be determined.
In this case, we also declare that greedy conrmation failed
and return the control to the concolic tester, since only the
tester can decide the reachability of `bugin order to ensure
zero tolerance for false alarms.
6. EXPERIMENTAL EV ALUATION
We implemented our algorithm on the tracer [13] frame-
work for symbolic execution and modied the concolic tester
Crest [3] to communicate with tracer during its testing
process. To achieve this, the algorithm in Fig. 3 was im-
plemented in tracer while Crest was slightly modied to
implement the procedure in Fig. 4.
We conducted experiments on three strategies: Depth-First
Search (DFS), Uniform Random Search (URS) and Control
Flow Graph (CFG) directed search. DFS explores the paths
in a depth-rst order naturally forming full-interpolants in
a bottom-up manner (even without the need for greedy con-
rmation) and maximising the benet of subsumption, so it
is the best-case strategy for our method. However according
to [3], it is the worst strategy in terms of branch coverage, so
we mainly provide it for completeness. CFG is a heuristic-
based strategy that rst computes the shortest distances be-
tween every basic block using the CFG of the program and
decides on a \target" basic block to cover. Then, when a
path is executed it chooses the next path by turning around
at a branch along the path with the least distance to the
target block. If it is unable to do so (due to infeasibility of
constraints), it chooses the next closest branch to the tar-
get that is along the path, and so on. Once the target is
reached, it randomly chooses a new target block to cover.
It is worth noting that CFG-directed search was shown to
be best strategy in terms of branch coverage by [3], so we
consider it an illustrative concolic testing strategy. Finally,
URS explores paths in a random order by choosing to turn-
around at a random branch in a currently executed path.
We included URS just to provide another example of a typ-
ical random concolic testing strategy. This random element
in both CFG and URS hinders greatly with the formation of
full-interpolants and is the main challenge for our algorithm
to overcome.
As benchmarks, we used four programs from the ntdrivers-simplied category of SV-COMP'12: cdaudio ,diskperf ,oppy
and kbltr . These programs range from 1000 to 2000 lines
of code. In spite of their relative small sizes these programs
have a large number of paths due to very complex control
ows. Thus, we believe these programs can stress more the
computation of interpolants and subsumption checks since
symbolic paths contain a high number of constraints and it
is often harder to reason about the infeasibility of these con-
straints. One of the consequences is that interpolants are
stronger (in the logical sense) since they must consider mul-
tiple reasons of infeasibility and hence, they are less likely
to be reusable. We ran three main experiments with them
on an Intel 3.2Ghz with 2Gb memory.
A technical problem with the experiments is that if our al-
gorithm prevented Crest from exploring subsumed trees, we
would never know how many paths Crest would have exe-
cuted in those trees (and subsequently the time taken for the
same) had we not prevented it. Also, we do not want to med-
dle with the random number-based sequence in CFG and
URS by forbidding Crest to execute certain paths. Hence
for measurement purposes, we do not forbid Crest from ex-
ploring subsumed trees in our experiments, but instead take
note when Crest is in subsumed trees and discard any mea-
surements taken during that time.
First experiment { Performance: At the outset we
would like to measure the actual performance improvement
provided by our method. For this we measure the time taken
by each strategy of naive Crest (i.e., original version) and
Crest aided by our method (i.e., algorithms in Figs. 3 and 4)
to execute a target number of paths, chosen based on the
size of the benchmark: 200k for cdaudio and oppy , 500k
fordiskperf and around 20k for kbltr (due to its smaller
size). The results are shown in Fig. 5, where the number of
executed paths is shown on the X-axis and the time taken
(in seconds) to execute those paths is shown on the Y-axis.
For naive Crest, we noticed that the time taken to execute
a path is almost constant across all three strategies, so for
simplicity we took the average time of the three strategies
(shown as Naive in Fig. 5). When running with our method
(and with greedy conrmation), we show the strategies sep-
arately labelled as CFG+GC ,DFS+GC and URS+GC .
It can be seen that in each benchmark, naive Crest takes
almost linear time to execute the target number of paths
because the generation of each path involves simply one
constraint solving, which takes almost constant time, and
possibly a heuristic to choose the branch to negate, which
is negligible. When aided by our method, Crest starts out a
bit slower due to the overhead of interpolation, however after
a certain time the benets of interpolation (i.e., subsump-
tion) start to payo, ultimately reaching the target number
of paths much faster. The magnitude of improvement de-
pends on the strategy. As expected, our method works best
with DFS, providing more than 10-20 times improvement,
followed by CFG and URS with about 3-5 times improve-
ment. For example, in diskperf , naive Crest takes on average
1700s to complete, whereas with our aid, CFG and URS take
about 600s and DFS takes 50s. Note that every path sub-
sumed in this experiment is a path executed by Crest that
need not have been executed. More importantly, the trend
of the graphs of Crest running with our algorithm indicates300200 400 600 800 1000 1200 
0k 50k 100k 150k 200k NaÃ¯ve 
CFG+GC 
DFS+GC 
URS+GC 
CFG-GC 
0400 800 1200 1600 2000 
0k 100k 200k 300k 400k 500k NaÃ¯ve 
CFG+GC 
DFS+GC 
URS+GC 
CFG-GC 
0200 400 600 800 
0k 50k 100k 150k 200k NaÃ¯ve 
CFG+GC 
DFS+GC 
URS+GC 
CFG-GC 
010 20 30 40 50 60 70 
0k 2k 4k 6k 8k 10k 12k 14k 16k NaÃ¯ve 
CFG+GC 
DFS+GC 
URS+GC 
CFG-GC (a) (b) (c) (d)
Figure 5: Timing for (a) cdaudio (b) diskperf (c)oppy (d) kbltr . X-axis: Paths, Y-axis: time in seconds
020 40 60 80 100 
0 500 1000 1500 2000 2500 3000 CFG 
DFS 
URS 
020 40 60 80 100 
0 750 1500 2250 3000 3750 4500 CFG 
DFS 
URS 
020 40 60 80 100 
0 1000 2000 3000 4000 CFG 
DFS 
URS 
020 40 60 80 100 
0 100 200 300 400 500 600 700 800 CFG 
DFS 
URS 
(a) (b) (c) (d)
Figure 6: Subsumption for (a) cdaudio (b) diskperf (c)oppy (d) kbltr . X-axis: Paths, Y-axis: % subsumption
potentially exponential benet in time over naive Crest.
To measure the eectiveness of greedy conrmation in our
algorithm, we measured the timing of Crest with our al-
gorithm but with greedy conrmation turned o (i.e., the
gray box in Fig. 3 removed). Hence, the full-interpolants
would be generated only by Crest exploring full sub-trees in
the symbolic execution tree. We chose the canonical CFG
strategy for this experiment, and its result is shown as CFG-
GC(double-dotted line) in the graphs in Fig. 5. It can be
seen immediately that the timing without greedy conrma-
tion is much worse than otherwise. Especially for diskperf
in Fig. 5(b), the timing is even slower than running naive
Crest, because the overhead incurred due to interpolation
does not payo in subsuming other paths. In other bench-
marks it pays o resulting in a benet to Crest, but the
amount of benet is small compared to running CFG with
greedy conrmation( CFG+GC ). More importantly, the trend
ofCFG-GC does not appear to provide exponential benet
to Crest, if at all. This experiment shows that greedy con-
rmation is a indeed major contribution to the eectiveness
of our algorithm and interpolation-based methods without
it [14, 12] are not readily suitable for concolic testing.
Second experiment { Subsumption: Now we would like
to understand the basis on which the rst experiment pro-
vided benet. A good measure for this is the amount of
subsumption that our method provides, i.e., the percentage
of Crest paths that are subsumed. A thing to take note
is that CFG and URS may repeat execution of some paths
due to their random element which we cannot control. Forthis and the subsequent experiment, we do not include such
paths in the calculations because a repeated path does not
contribute to the percentage of subsumption. We included
them in the previous experiment because they indeed con-
tribute to execution time.
The results are shown in Fig. 6. In the graph of each bench-
mark, the X-axis represents the number of paths executed
by Crest and the Y-axis represents the percentage of those
paths subsumed. It can be clearly seen that DFS very
quickly reaches almost 100% subsumption whereas the other
two strategies CFG and URS show an increasing trend to-
wards it, with DFS as their asymptote. This is in line with
the rst experiment where the magnitude of performance
benet was the greatest in DFS, followed by the other two.
Indiskperf we notice some noise, as CFG and URS uctuate
between 50-65% subsumption. The next experiment below
suggests that this is because even though we subsume trees
during this period, CFG and URS are not interested in ex-
ecuting paths in those trees. This is possibly why the mag-
nitude of improvement was low (only around 2) for diskperf
in Fig. 5(b). Theoretically, the rate of subsumption could
decrease (even be zero) if Crest constantly avoids execut-
ing paths in subsumed trees, but we expect this to seldom
happen in reality.
Third experiment { Extra path-coverage: Now we
present a dierent view of the provided benet. When we
subsume a tree, we not only provide the direct benet of
saving the paths that Crest indeed executes in the tree (the
rst experiment), but also the indirect benet of covering31010000 20000 30000 40000 50000 
0 500 1000 1500 2000 2500 3000 CFG 
URS 
110 100 1000 10000 100000 1000000 10000000 
0 750 1500 2250 3000 3750 4500 CFG 
URS 
050000 100000 150000 200000 250000 300000 
0 1000 2000 3000 4000 CFG 
URS 
04000 8000 12000 16000 20000 24000 28000 
0 150 300 450 600 750 900 CFG 
URS (a) (b) (c) (d)
Figure 7: Extra coverage provided for (a) cdaudio (b) diskperf (c) oppy (d) kbltr by our method. X-axis:
Crest path coverage, Y-axis: Additional path coverage from subsumption.
paths in the tree that Crest cannot cover within its budget.
Although such paths are of low priority to the strategy's
heuristics, they are provided \free of charge" by our method
because the time we spend to subsume the whole tree is in-
clusive of these paths as well. This can be considered extra
path coverage because Crest has no hope of executing them
in its budget, but if its budget were longer it may execute
them in future. Note that this experiment does not make
sense for DFS, because when we subsume a tree, DFS would
immediately proceed to execute allpaths in the tree anyway.
In Fig. 7, we measure the number of such \free" paths per
path executed by Crest. Note that we again do not include
repeated paths in this experiment since they do not con-
tribute to path coverage. In each graph, the X-axis shows
the actual path coverage of Crest, and the Y-axis shows the
extra path coverage obtained due to subsumption. The ratio
of extra path coverage to Crest's actual path coverage varies
greatly depending on the benchmark, from a magnitude of
10 in cdaudio to about 1000 in diskperf (note the logarith-
mic scale). Specically, for the CFG strategy in diskperf ,
we subsume a huge number of trees around 1500 paths, but
the previous experiment showed yet uctuating percentage
of subsumption around that time, indicating that CFG is
not interested to execute paths in those subsumed trees.
In this experiment, by \taking credit" for entire subsumed
trees, we measured the upper bound on the magnitude of
benet in path coverage that we can provide, a mean of
100. The lower bound, around 3 to 5, is dictated by the
rst experiment (although we did not explicitly measure it
there, we can extract it from the timing), where we took
credit only for the paths that Crest actually executed in
the subsumed trees, within its budget. In general, one can
expect the benet we provide to lie somewhere in between,
depending on the budget.
Fourth experiment { Terminating testing: Finally, we
discuss a small but important experiment. We wanted to
make a \pure" comparison of concolic testing with and with-
out our method, notwithstanding the complications with
measuring the number of subsumed paths, random number
sequences and repeated paths we encountered in the pre-
vious experiments. In other words, we wanted to actively
forbid Crest from exploring subsumed trees instead of let-ting it run and discarding measurements like before. The
problem in doing this is that when terminating with a bud-
get, the sequence of paths executed by naive Crest and our
method would be dierent and hence, incomparable. How-
ever, if the testing process terminates having explored the
entire search space (i.e., veried the program), the sequence
of paths it took to do so does not matter.
We obtained a smaller non-buggy version of kbltr from the
same benchmark suite and ran Crest's CFG strategy on it
with and without our method, this time actively forbidding
Crest from exploring subsumed trees. With our aid, Crest
was able to completely verify the program in 20 seconds,
whereas naive Crest was able to complete only after 256
seconds. This experiment shows concrete evidence that our
method indeed accelerates a typical concolic testing strat-
egy such as CFG towards more path coverage, in this case
verifying the program much faster than otherwise. Unfor-
tunately, we could not run this experiment on other bench-
marks as they contain a prohibitive number of paths and
concolic testing could not explore all of them in a reason-
able amount of time making it not possible to make such an
interesting comparison.
Remark. Although we focus on path coverage in this paper
it is worthwhile to note that our method can be also used to
improve branch coverage . In fact, in some of our preliminary
experiments we targeted branch coverage and observed that
we were achieving the same branch coverage but sometimes
faster with our method than without. However, the problem
of branch coverage is simpler than path coverage and hence,
pruning is not always vital. In these cases, the overhead of
interpolation and subsumption may not pay o.
7. CONCLUSION
We attacked the path-explosion problem of concolic test-
ing by pruning redundant paths using interpolation. The
challenge for interpolation in concolic testing is the lack of
control of search order. To solve this, we presented the con-
cept of half and full interpolants that makes the use of in-
terpolants sound, and greedy conrmation that accelerates
the formation of full-interpolants thereby increasing the like-
lihood of subsuming paths. We implemented our method
and empirically presented its performance and path cover-
age gains.328. REFERENCES
[1] S. Anand, P. Godefroid, and N. Tillmann.
Demand-Driven Compositional Symbolic Execution.
InTACAS , pages 367{381, 2008.
[2] P. Boonstoppel, C. Cadar, and D. R. Engler. RWset:
Attacking Path Explosion in Constraint-Based Test
Generation. In TACAS , pages 351{366, 2008.
[3] J. Burnim and K. Sen. Heuristics for Scalable
Dynamic Test Generation. In ASE, pages 443{446,
2008.
[4] C. Cadar, D. Dunbar, and D. R. Engler. KLEE:
Unassisted and Automatic Generation of
High-Coverage Tests for Complex Systems Programs.
InOSDI , pages 209{224, 2008.
[5] C. Cadar, V. Ganesh, P. M. Pawlowski, D. L. Dill,
and D. R. Engler. EXE: Automatically Generating
Inputs of Death. In CCS, pages 322{335, 2006.
[6] A. Cimatti, A. Griggio, and R. Sebastiani. Ecient
interpolant generation in satisability modulo
theories. In TACAS'08 , pages 397{412, 2008.
[7] W. Craig. Three uses of Herbrand-Gentzen theorem in
relating model theory and proof theory. Journal of
Symbolic Computation , 22, 1955.
[8] P. Godefroid. Compositional dynamic test generation.In M. Hofmann and M. Felleisen, editors, 34th POPL ,
pages 47{54. ACM Press, 2007.
[9] P. Godefroid, N. Klarlund, and K. Sen. DART:
Directed Automated Random Testing. In PLDI , pages
213{223, 2005.
[10] P. Godefroid, M. Y. Levin, and D. A. Molnar.
Automated Whitebox Fuzz Testing. In NDSS , 2008.
[11] J. Jaar, , J. Navas, and A. Santosa. Unbounded
Symbolic Execution for Program Verication. In RV
2011, pages 396{411, 2011.
[12] J. Jaar, A. E. Santosa, and R. Voicu. An
interpolation method for CLP traversal. In 15th CP ,
volume 5732 of LNCS . Springer, 2009.
[13] J. Jaar, Vijayaraghavan Murali, J. Navas, and
A. Santosa. TRACER: A Symbolic Execution Tool for
Verication. In CAV 2012 , pages 758{766, 2012.
[14] K. L. McMillan. Lazy annotation for program testing
and verication. In T. Touili, B. Cook, and
P. Jackson, editors, 22nd CAV , volume 6174 of LNCS ,
pages 104{118. Springer, 2010.
[15] K. Sen, D. Marinov, and G. Agha. Cute: a concolic
unit testing engine for c. In ESEC/FSE-13 , pages
263{272, 2005.33