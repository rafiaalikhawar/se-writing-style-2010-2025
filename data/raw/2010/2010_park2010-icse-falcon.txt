Falcon: Fault Localization in Concurrent Programs∗
ABSTRACT
Concurrency fault are diﬃcult to ﬁnd because they usually
occur under speciﬁc thread interleavings. Fault-detection
tools in this area ﬁnd data-access patterns among thread
interleavings, but they report benign patterns as well as
actual faulty patterns. Traditional fault-localization tech-
niques have been successful in identifying faults in sequen-
tial, deterministic programs, but they cannot detect faulty
data-access patterns among threads. This paper presents
a new dynamic fault-localization technique that can pin-
point faulty data-access patterns in multi-threaded concur-
rent programs. The technique monitors memory-access se-
quences among threads, detects data-access patterns asso-
ciated with a program’s pass/fail results, and reports data-
access patterns with suspiciousness scores. The paper also
presents the description of a prototype implementation of
the technique in Java, and the results of an empirical study
we performed with the prototype on several Java bench-
marks. The empirical study shows that the technique can
eﬀectively and eﬃciently localize the faults for our subjects.
Categories and Subject Descriptors
D.1.3 [Programming Techniques ]: Concurrent Program-
ming— Parallel programming ; D.2.5 [ Software Engineer-
ing]: Testing and Debugging— Debugging aids, Diagnostics,
Monitors, and Tracing
General Terms
Algorithms, Reliability
Keywords
Concurrency, Debugging, Fault Localization, Atomicity Vi-
olation, Order Violation
1. INTRODUCTION
The widespread deployment of parallel systems based on
multicore processors [2] is already having a tangible impact
∗Patent pending.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted w ithout fee provided that copies are
not made or distributed for proﬁt or co mmercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redist ribute to lists, requires prior speciﬁc
permission and/or a fee.
ICSE ’10, May 2-8 2010, Cape Town, South Africa
Copyright 2010 ACM 978-1- 60558-719-6/10/ 05 ...$10.00.on development and testing for major software vendors. For
example, a 2007 survey of concurrency usage among nearly
700 developers and testers at Microsoft revealed that con-
currency is prevalent, with nearly two-thirds of respondents
reporting that they have had to deal with concurrency is-
sues [11]; and of these people, over half have had to detect,
debug, and/or ﬁx concurrency bugs1on at least a monthly
basis. Not only were these bugs reported as being among
themost diﬃcult toﬁnd, reproduce, and ﬁx, butover60% of
these bugs took at least a few days to repair, accounting for
“thousands of days of work” in aggregate. Besides produc-
tivity loss, the ultimate consequences of shipping concurrent
software with defects can be devastating; the oft-cited 2003
Northeastern U.S. electricity blackout, which left 10 million
people without power, has been attributed in part to a race
condition in multi-million line monitoring software [27].
Testing and debugging concurrent software can be even
more challenging than for sequential programs, for at least
two reasons [19]. First, concurrent programs exhibit more
nondeterministic behavior, which can make it diﬃcult to
even reproduce the faults. Second, concurrent faults typi-
cally involve changes in program state due to particular in-
terleavings of multiple threads of execution, making them
diﬃcult to ﬁnd and understand. These faults most fre-
quently manifest as data races, atomicity violations, and
order violations, which are consistently ranked as the most
common and diﬃcult source of concurrency faults [4, 16, 31].
Prior work. Numerous eﬀorts have discovered principles
and methods to pinpoint these most frequent concurrent
faults. The earliest work applies static and dynamic analy-
sis to detect data races , in which multiple threads perform
unsynchronized accesses to shared data [7, 20, 22, 28, 29,
30]. However, benign data races are common and can yield
high false-positive rates [16].
More recent work has instead tried to identify and de-
tect constructs that correspond directly to faults. Such con-
structsinclude atomicity violations ,i nw hi c hare g i o no fcode
that should execute atomically does not, and order viola-
tions, in which an unintended sequence of operations among
threads occurs. Early work in this area focused on atomic-
ity violations, relying on the developer to explicitly annotate
atomic regions for subsequent static or dynamic veriﬁcation
by a tool [8, 10].
To relieve this annotation burden, a recent trend is to ap-
plydynamic pattern analysis [13, 17]. Thetechniquecharac-
terizes faults by likely interleaved sequences of operations,
1We use fault and bug interchangeably.Sangmin Park, Richard W. Vuduc, and Mary Jean Harrold
College of Computing
Georgia Institute of Technology
E-mail: {sangminp |richie |harrold }@cc.gatech.edu
245and then searches for these patterns in an execution. A
pattern-based approach can in principle be applied to both
atomicity and order violations, although existing methods
have thus far considered only atomicity [16]. Furthermore,
current methods may report many patterns, only some ofwhich might directly identify the fault. These methods donotpresentlyhaveanywaytorankorprioritize thepatterns.
Many existing eﬀective ranking techniques for fault local-
ization are based on code coverage. Examples include prior
work on Tarantula for sequential programs [14], and recentwork for statement and expression (predicate) ranking forconcurrent programs [18, 32]. These methods work by as-sociating the number of occurrences of a target coverage
criterion with passing and failing executions, and use these
data to compute suspiciousness scores. However, thus far
this approach has not been applied to concurrency patterns.
Approach and contributions. Weproposeanewpattern-
based dynamic analysis technique for fault localization in
concurrent programs that combines pattern identiﬁcation
with statistical rankings of suspiciousness of those patterns.
We apply our technique to both atomicity and order viola-
tions. During testing, our technique detects access patterns
from actual program executions, which either pass or fail.
For each pattern, the technique uses the pass/fail statistics
to compute a measure of suspiciousness that is used to rankall occurring patterns, in the spirit of Tarantula in the se-
quential case [14]. We also describe Falcon, a prototype
implementation of the technique in Java, that is designed
to have reasonable storage and execution time overheads, sothat it may be deployed in realistic testing environments.We used Falcon to perform an empirical study on several
Java benchmarks. The empirical study shows that the tech-
niquecan eﬀectively andeﬃciently localize thebuglocationsfor our subjects.
The technique has several advantages over existing tools.
First, the technique captures not only order violations butalso atomicity violations: existing tools focus only on either
of the bugs. Second, the technique reports the real violation
patterns with high priority, unlike other techniques that re-port benign and real violations without priority. Third, thetechniqueis more eﬃcient than previous techniques in termsof time and space. In short, our approach provides the same
beneﬁtsof priordynamicpattern analysis methods, andcon-
tributes an explicit prioritized ranking of those patterns to
guide the developer toward the most likely cause of a fault.
The main contributions of the paper are
•To the best of our knowledge, our approach is the ﬁrst
to localize malicious interleaving patterns in concur-rent programs. The aim is to help the developer morequickly identify the source of a concurrency fault.
•Our technique detects both atomicity and order viola-
tions. In particular, we believe our work is the ﬁrst to
eﬀectively identify order violations.
•We have implemented this technique in a Java-basedtoolset, called Falcon, which can be applied to any
concurrent program with test cases.
•We evaluate Falcon experimentally, and show that it
is both eﬀective and eﬃcient in localizing concurrency
faults in our subjects. We compare Falcon to the
most closely related tools [13, 17, 32].Table 1: Conﬂicting interleaving patterns.
Interleaving
 Description
1
R1-W2
 Unexpected value is written.
2
W1-R2
 Unexpected value is read.
3
W1-W2
 The result of remote write is lost.
Table 2: Unserializable interleaving patterns.
Interleaving
 Description
1
R1-W2-R1
Two local reads expect to get the
same value.
2
W1-W2-R1
A local read expects to get the resultof a local write.
3
W1-R2-W1
A temporary result between local
writes (in one thread) is not sup-
posed to be seen to other thread.
4
R1-W2-W1
The result of remote write is lost.
5
W1-W2-W1
The result of remote write is lost.
2. MOTIVATING EXAMPLE
Inthis section, we deﬁneourtarget concurrency violations
and present an example that we use throughout the paper.
We then use this example to illustrate the limitations ofexisting techniques.
2.1 Concurrency Violations
We begin by introducing our formal notation and then
deﬁning the key concurrency violations of interest in this
paper: atomicity violations and order violations.
Notation. We denote a memory access to a shared variable
bybt,s,w h e r e bis the memory access type, either read (R)
or write (W); tis the thread that performs the access; and s
is the program statement containing s. For example, R 1,S2
indicates a read access to a s hared variable in statement S2
of thread 1. We can often abbreviate this notation (e.g., by
R1if the statement is understood from the context).
Data race. Adata race occurs when two or more threads
access a shared memory location, where at least one of the
accesses is a write, and there is no locking to synchronizethe accesses. For example, any of the pairs R
1-W2,W 1-R2,
W1-W2are, in the absence of synchronization, data races.
As is well-known, a data race does not always imply a fault.
For example, barriers, ﬂag synchronization, and producer-
consumer queues are common concurrency constructs that
are implemented with deliberate data races [17]. Therefore,we do not focus on data race detection in this paper, thoughour method can include it.
Order violation. Two sequential thread accesses to a
shared memory location is a conﬂicting interleaving pattern
if at least one of the accesses is a write, as shown in Ta-
ble 1. An order violation occurs when a conﬂicting inter-
leaving pattern appears that leads to unintended program
behavior. The following is an example of this violation:
Thread 1
1: lock (L);
2: A = new MyObject();
3: if (A) { /* ... */ }
4: unlock (L);Thread 2
1: lock (L);
2: x = A.getValue();
3: unlock (L);
246Initially x=0; y=0;
Thread1
1: if(x==0) x=1;
2: if(y==0) y=1;
3: if(x==2 and y==2) assert(false);
Thread2
4: if(x==1) x=2;
5: if(y==1) y=2;
Thread3
6: if(x==1) x=3;
7: if(y==1) y=3;
(a)(b)
Trace
 Trace
 Trace
 Trace (sim-
 Trace (sim-
(statement)
 (x-access)
 (y-access)
 pliﬁed x)
pliﬁed y)
1
1-6-4-2-7-5-3
 R1,1-W1,1-R3,6-
W3,6-R2,4-R1,3
R1,2-W1,2-R3,7-
W3,7-R2,5-R1,3
W1,1-W3,6-
R2,4-R1,3
W1,2-W3,7-
R2,5-R1,3
2
1-6-4-2-5-7-3
 R1,1-W1,1-R3,6-
W3,6-R2,4-R1,3
R1,2-W1,2-R2,5-
W2,5-R3,7-R1,3
W1,1-W3,6-
R2,4-R1,3
W1,2-W2,5-
R3,7-R1,3
3
1-4-6-2-7-5-3
 R1,1-W1,1-R2,4-
W2,4-R3,6-R1,3
R1,2-W1,2-R3,7-
W3,7-R2,5-R1,3
W1,1-W2,4-
R3,6-R1,3
W1,2-W3,7-
R2,5-R1,3
4
1-4-6-2-5-7-3
 R1,1-W1,1-R2,4-
W2,4-R3,6-R1,3
R1,2-W1,2-R2,5-
W2,5-R3,7-R1,3
W1,1-W2,4-
R3,6-R1,3
W1,2-W2,5-
R3,7-R1,3
(c)
Run1
 Run2
 Run3
 Run4
 Suspciousness
Pattern for x:W 1-W3-R1
 ∗
 ∗
 0
Pattern for x:W 1-W2-R1
 ∗
 ∗
 0.5
Pattern for y:W 1-W3-R1
 ∗
 ∗
 0
Pattern for y:W 1-W2-R1
 ∗
 ∗
 0.5
Result:
 P
 P
 P
 F
Figure 1: Motivating example: (a) a three-threaded buggy program example, (b) three passing and one
failing executions of the program with diﬀerent level of traces, and (c) unserializable interleaving patternsassociated with suspiciousness scores computed by a statistical method.
The correct order of the execution is executing the code in
Thread 1 and then Thread 2. However, variable xgets an
unexpected value when the code in Thread 2 is executed
before Thread 1. Here, R
2,S2-W1,S2is an order violation.
Note that the example is synchronized by a lock L, so it is
neither a data race nor an atomicity violation (see below).
Atomicity violation. Anatomicmethodoracodeblockis
a sequence of statements whose execution is not interrupted
by the execution of statements in diﬀerent threads. Atom-
icityis often referred to as serializability , and is satisﬁed if
the resulting state of data in a concurrent execution is the
same as that of a serialized execution. In Table 2, we show
unserializable interleaving patterns . For example, R 1-W2-
R1is an unserializable interleaving pattern where Thread 1
ﬁrst reads a shared-memory variable, then another thread,
Thread 2, writes to the shared-memory variable, and ﬁnally
Thread 1 reads the shared-memory variable. The pattern
is unserializable because a serial execution R 1-R1-W2has a
d i ﬀ e r e n tv a l u ef o rt h es e c o n dr e a da c c e s s .
Anatomicity violation occurs when an unserializable in-
terleaving pattern leads to unintended program behavior.
Note that, as with order violations, an atomicity violation
can occur without a data race [16, 25]. We give an exampleof an atomicity violation in the following section.
2.2 A Running Example
Asourexample, we usethethree-threadedprogram shown
in Figure 1(a). The program has shared variables xand y,
which are initialized to 0. Thread 1 sets the values of xand
yto 1 if their values are 0, and checks whether the values of
both xandyare2. Ifthevaluesof xandyare2, theprogram
fails with an assertion violation. Thread 2 and Thread 3 setthe values of xandyto 2 and 3, respectively, if the values of
xandyare1. To raise theassertion violation, two conditions
forxand ymust be satisﬁed: for x, statements 1 and 4
consecutively executed without intervention of statement 6;
fory, statements 2 and 5 consecutively executed without
intervention of statement 7.
Figure 1(b) shows the results of four executions. The
ﬁrst column numbers each execution. The second column
shows statement traces for the four executions. The third
and fourth columns show the data accesses for variable xandythat occur during each execution. The ﬁfth and sixth
columns show the simpliﬁed data accesses of variables xand
yof the third and fourth columns. The simpliﬁcation will
be discussed in Section 3.1. Columns 3 through 6 use the
notation of Section 2.1.
Figure 1(c) shows the ranking of identiﬁed unserializable
interleaving patterns. The second to the ﬁfth rows show
all four unserializable interleaving patterns identiﬁed from
the four executions. In this example, we consider only un-serializable interleaving patterns (as opposed to conﬂictinginterleaving patterns)because theprogram has atomicity vi-olations that consists of two unserializable interleaving pat-terns. The second to the ﬁfth columns show the four runs
from Figure 1(b). If a pattern appears in a run, the over-
lapped cell is marked with *. For example, W
1-W3-R1pat-
tern for x appears in runs 1 and 2. The sixth row shows the
passing/failing result of each run; runs 1–3 are passing and
run 4 is failing. The sixth column shows the suspiciousness
scores for the four patterns. The suspiciousness scores of
W1-W2-R1patterns for xand yare 0.5, and suspiciousness
scores for other two patterns are 0.
Existingatomicity violationdetectorsfailtoprecisely iden-
tify the patterns that cause the fault. Most detectors will
ﬁnd all or some of the four distinct unserializable interleav-
ing patterns shown in the ﬁrst column of the table in Fig-
ure 1(b) after four executions [8, 13]. AVIO reports patterns
that appear only in failing executions [17]; it fails to reportany pattern in our example because all four patterns appearin at least one passing execution. Only Falcon reports the
two W
1-W2-R1patterns for xand y, with the highest sus-
piciousness scores. We discuss the technique in detail in
Section 3.
3. OUR TECHNIQUE
Our technique for identifying concurrency bugs in Java
threads consists of two main steps. In Step 1, our tech-
nique monitors shared-memory accesses online, detecting
and recording patterns of such accesses that correspond to
unserializable and conﬂicting interleaving patterns.2Step 1
associates these patternswith test cases and pass/fail results
2Recall that these two classes of patterns are associated with
atomicity violations and order violations, respectively.
247(a): xaccess: R 1,1-W1,1-R3,6-W3,6-R2,4-R1,3
(b): yaccess: R 1,2-W1,2-R2,5-W2,5-R3,7-R1,3
Figure 2: Memory access and sliding windows—(a)
and (b) are shown in the trace of the second execu-tion (third row) in Figure 1 (b).
of the executions. In Step 2, our techniqueapplies statistical
analysis to the results of Step 1 to compute a suspiciousness
valuefor each detected pattern, as described in Section 3.2.
Using these suspiciousness values, Step 2 ranks the patterns
from most to least suspicious, and presents this ordered list
to the user. The remainder of this section describes thesesteps in detail.
3.1 Step 1: Online Pattern Identiﬁcation
Step 1 of the technique identiﬁes unserializable and con-
ﬂicting interleaving patterns (described in Section 2.1) dur-
ing the program’s execution. Like other fault-localization
techniques, our technique records program entities and sub-
sequently associates them with passing and failing runs.
However, ourtechniquediﬀersfrompriorfault-localization
work in two ways. First, instead of running the program
with many test cases, our technique runs the program manytimes (i.e., ktimes) with the same test case. The program is
non-deterministic; thus, diﬀerent and possibly faulty inter-
leavings of access to shared variables can occur in diﬀerent
executions of the same test cases. (We also apply random
delays(or irritators ) toincreasethelikelihood of diﬀerentin-
terleavings [5, 34].) Second, instead of gathering coverage of
simple entities, such as statements, branches, or predicates,
our technique tracks patterns (i.e., sequences of shared vari-
able references).
An instrumented version of the program, which we call
P
/prime, executes each test case ktimes. Duringthese executions,
our technique uses a ﬁxed-sized sliding-window mechanism
to identify patterns. For each execution, our technique as-
sociates patterns with program-execution behavior: passing
(i.e., the program behaved as expected) or failing (i.e., the
program exhibited unexpected behavior). After all kexe-
cutions, the set of suspicious patterns and the number of
passing and failing executions associated with each pattern
is passed to Step 2 of the algorithm.
Windowing scheme and update policy. AsP/primeexecutes
with a test case, it maintains a set of ﬁxed-size windows
that store memory-access information. There is one window
for each shared-memory location. When tracking patterns,
using a ﬁxed-size data structure for each memory location
reduces the time and storage overheads compared to record-
ingall shared-memoryaccesses. Forexample, inFigure1(b),
the trace (column 3) is tracked by two windows—one for
variable xand the other for variable y. With ﬁxed-size win-
dows, the storage overhead grows with the number of sharedvariables rather than the number of memory accesses.
We illustrate how our scheme maintains the window using
the example in Figure 2, which corresponds to the secondInput :m: shared memory location
b: memory access type
t:t h r e a dI D
s: memory access location
Pt: current set of patterns (initially null)
Output :Pt: updated set of patterns
ifmdoes not yet have any window then 1
w=createWindow (); 2
w.insert( b,t,s); 3
registerWindow (w,m); 4
else5
w=getWindow (m); 6
(b2,t2,s2)=w.getLastAccess(); 7
ift=t2then 8
w.update( b,s); 9
else 10
ifwi sf u l l then 11
Pt+=getPatterns (w); 12
w=slideWindow (w); 13
end 14
w.insert( b,t,s); 15
end 16
end17
return Pt; 18
Algorithm 1 : GatherPatterns.
execution of our running example in Figure 1(b). We showthe two reference streams for the two shared variables, x
and y. We represent a reference to a shared variable with a
circle, labeled by b
t,s(see Section 2.1). The window size is
the maximum number of references tracked for each sharedvariable.
When any thread references the variable, our technique
updatesitsassociated window. Initially,thewindowisempty
so our technique always records the ﬁrst reference. If a new
reference occurs in a diﬀerent thread from the previously
recorded reference—a thread-remote access—our technique
records the new reference in the next slot. Otherwise, the
threads are the same—a thread-local access—and our tech-
nique replaces the previous reference. One exception to this
replacement is when the new reference is a read and the
last reference was a write, in which case we keep the write.That is, we heuristically prefer writes, largely because we
know that both order and atomicity violations require at
least one write.
For example, Figure 2(a) shows the xreferences and their
sliding window for the second execution in Figure 1(b). Thec o m p l e t ea c c e s s e sa r es h o w ni nt h et h i r dc o l u m na n dt h e
simpliﬁed accesses used in the sliding window are shown inthe ﬁfth column of Figure 1(b). Consider the ﬁrst two con-secutive accesses R
1,1-W1,1from Thread 1. When Thread
1p e r f o r m sR 1,1, our technique records it because it is the
ﬁrst xreference of Thread 1. Our technique then observes
W1,1with no interleaved access from another thread, and
it overwrites the R 1,1slot with W 1,1. When the window
becomes full, our technique evicts the oldest reference. In
Figure 2(a), the window of size 3 becomes full once R 2,4
occurs; the subsequent R 1,3causes the window to “slide,”
thereby evicting W 1,1.
This scheme is approximate in the sense that it may pre-
maturely evict references that are part of some pattern, ow-
ing to the limited capacity of the window. Tuning the win-
dow size allows our techniqueto trade accuracy for time and
storage overhead.
248The online pattern gathering algorithm. Our overall
pattern-collection algorithm, shown in Algorithm 1, is in-
voked whenever there is a new reference ( b,t,s) to shared-
memory location m, and that also includes the window up-
date policy described above. Because this algorithm is gath-
ering patterns online, it assumes there is some current set ofpatterns, Pt, and updates this set.
The algorithm ﬁrst checks whether a window exists for
m(line 1). If not, it creates one (lines 2–4). Otherwise,
it retrieves the window wfrom a global table, extracts the
last access (lines 6–7), and updates the window (lines 8–17)
using the window-update policy described previously.
If, during the window update, the algorithm discovers
that the window is full (line 11), then it scans the window
for patterns (lines 12), and ﬁnally slides the window (line
13). When extracting patterns, the algorithm checks the
window for all of the unserializable interleaving patterns inTable 2, where the ﬁrst access in the pattern indicates theoldest slot. If there is no unserializable interleaving pattern,
the algorithm checks the window for conﬂicting interleaving
patterns in Table 1. That is, the algorithm does not doublycount a pair (conﬂicting interleavingpattern) that is alreadydetected as a triplet (unserializable interleaving pattern).
Toseehowpatternextractionworks, recallexecutiontrace
2 of Figure 1(b). This trace contains two reference streams:
one each for variables xand yas shown in Figures 2(a)
and 2(b), respectively. In Figure 2(a), recall that the win-
dow becomes full at the ﬁfth reference, R
2,4.A tt h e s i x t h
reference, R 1,3, lines 12–13 of Algorithm 1 are executed as
follows. In the window, there are no unserializable inter-
leaving patterns because the ﬁrst and third slots of the win-
dow (W 1,2and R 3,7circles) do not have matching thread
IDs. However, there is a conﬂicting interleaving pattern,
W1-W3, where the ﬁrst element is the oldest slot. Thus, the
algorithm slides the window, adds the R 1,3reference, and
emits a pattern.
Figure2(b)showsthissameprocessrepeatedfortheshared
variable y. Importantly, observe that a window size of 4 is
required to detect the unseria lizable interleaving pattern in
thiscase. The threadof theoldest slot matchesthe threadof
the fourth slot, so in the window, the algorithm can detect
the W 1-W2-R1pattern with the ﬁrst, second, and fourth
slots. It is in this sense that windowing may not capture all
patterns, and so tuning the window size is a critical engi-
neering consideration.
Theoretically, if we want to guarantee that our technique
does not miss any patterns, we can argue bounds on the
necessary window size as follows. A trivial lower bound on
the window size for detecting the unserializable interleaving
patterns is the maximum length of any pattern. For thepatterns in this study (Table 2), the longest pattern has 3
references, so a lower bound on window size for our patterns
is 3.
However, the upper bound should also be proportional to
the number of threads (without any compression). To see
this bound, suppose we wish only to gather patterns of theform R
i-W j-Ri, and that there are nthreads of execution.
Consider an actual execution with the reference stream, R 1-
W2-W3-···-W n-R1. Clearly, we need at least O(n) slots to
capture all n−1 patterns of the form R 1-W j-R1.T h u s ,
w em i g h te x p e c tt h a ta s nincreases, we need to increase
the window size accordingly. Moreover, we might expect
that this window size might need to grow by as much asO(n2) in the worst possible case, since the O(n) bound ap-
plies to just a single thread. We investigate the relationship
between window size and number of threads empirically in
Section 4.3.
3.2 Step 2: Pattern Suspiciousness Ranking
Step 2 of the technique uses the results of Step 1—the
gathered patterns and their association with passing and
failing executions—and computes a suspiciousness score us-
ing statistical analysis.
Basic approach: Suspiciousness scores for patterns.
There is a body of research on statistical analysis for fault
localization for sequential or deterministic programs [1, 14,
15]. Theseapproachesassumethatentities(e.g., statements,
branches, and predicates) executed more often by failing ex-
ecutions than passing executions are more suspect. Thus,
they associate each entity with a suspiciousness score thatreﬂects this hypothesis. For example, Tarantula uses the
following formula, where sis a statement, % passed(s)i st h e
percentage of the passing test executions that execute s,a n d
%failed(s) is the percentage of the failing test executions
that execute s[14]:
suspiciousness
T(s)=%failed( s)
%failed( s)+%passed( s)(1)
For concurrent programs, we can apply the same methodol-
ogy, includingthe Tarantula formula, to score patterns.T h i s
approach works in a reasonable way most of the time, but
sometimes produces unexpected suspiciousness values.
The problem arises from the non-determinism inherent in
concurrent programs. It is possible that a pattern occurs in
only one failing execution and no passing executions, but is
not related to the real fault in the program. In this case,
the Tarantula formula (1) gives this pattern a suspiciousness
value of 1—the highest suspiciousness value. To accountfor this case, the formula should assign a higher score topatterns that appear more frequently in failing cases.
Our scoring approach: Jaccard Index .T h e Jaccard in-
dexaddresses this weighting issue by comparing the similar-
ity of the passing and failing sets [1]. We use this measure in
our technique. For a pattern s,w h e r e passed(s)i st h en u m -
ber of passing executions in which we observe s,failed(s)
is the number of failing executions, and totalfailed is the
number of total failures, we use the following score:
suspiciousness
J(s)=failed( s)
totalfailed +passed( s)(2)
Again, consider the example in Figure 1. There are three
passing and one failing executions in Figure 1(b), making
totalfailed = 1. The suspiciousness scores for the four pat-
terns computed by (2) are presented in Figure 1(c). The twoW
1-W2-R1patterns for xand yare ranked highly. Indeed,
they are the main causes of the bug in this example.
4. EMPIRICAL STUDY
Weimplementedaprototypeofourfault-localization tech-
nique in a tool, called Falcon. In this section, we empiri-
cally evaluate Falcon, includingcomparisonstoexistingap-
proaches, on a suite of faulty multithreaded Java programs.In particular, we conducted four evaluation studies, whose
key ﬁndings are summarized as follows.
2491. We study how the window size needed to ﬁnd a faulty
pattern varies as a function of the number of threads
(Section 4.3). We ﬁnd that a constant window size,
rather than one linear or quadratic in the number of
threads, works well. Thus, windowing is space-eﬃcientand does not miss critical patterns in practice.
2. We assess the eﬀectiveness of our ranking algorithm
(Section 4.4). On our benchmark suite, we ﬁnd that
the patterns directly corresponding to a fault always
rank at or near the top of the observed patterns. Thisresult implies that a programmer can quickly hone inon the source of a fault.
3. We measure the run-time overheads of our approach,
showing a median slowdown of 2 .6×across our bench-
marks (Section 4.5). Thus, it is feasible to incorporate
Falcon into realistic testing environments.
4. We compare to the most closely related approach—
Cooperative Crug Isolation (CCI) (Section 4.6). CCIgathers predicate entities rather than patterns, so it
only ranks individual accesses. Moreover, in CCI’srankings, the accesses involved in a faulty pattern of-t e nd on o te n du pa tt h et o p .
4.1 Implementation
We implemented Falcon in Java. Not counting other
software that it uses, Falcon consists of 7224 lines of code.
The ﬁrst main component of Falcon is its instrumenta-
tion and monitoring capabilities. For the instrumentation
component of Falcon, we used the Soot Analysis Frame-
work,3which analyzes programs in Java bytecode format.
Falcon performs a static thread-escape analysis [12] to de-
termine which variables might be shared among multiple
threads, and instruments the program to observe and record
shared accesses at runtime.
Falcon also instruments methods, to provide detailed
stack-trace information in subsequent bug analysis. More-
over, Falcon provides an option to inject artiﬁcial-delays
that can increase the number of interleavings that occur,
therebyincreasingthechanceofelicitingconcurrencybugs[5,34]. We use this option in our experiments. The Falcon
dynamic monitor executes in a separate thread as the in-
strumented program executes. This monitor dynamicallyreceives memory-access information generatedfrom multiple
threadsinanon-blockingconcurrentqueue,whichmaintains
memory accesses in a sequential order. The accesses are ob-tained from this queue to construct windows for extracting
patterns (Section 3.1).
The second main component of Falcon computes sus-
piciousness values for each pattern (Section 3.2), and re-
ports the list of ranked suspicious patterns in a text format.
Within the Falcon toolset, each suspicious pattern can be
represented in dotty
4graphical format.
4.2 Empirical Setup
Table 3 describes the set of subject programs we used in
our study. The ﬁrst and second columnslist thesubject pro-
grams, classiﬁed into three categories: Contest benchmarks,
JavaCollection Library, andMiscellaneous (Misc) programs.
The third column shows the size of the subject program inlines of code. The fourth column displays the empirically
3http://www.sable.mcgill.ca/soot/
4http://www.graphviz.org/Table 3: This table describes the subject, lines of
code, failure rate, and violation type.
Program
Lines
 Failure
 Violation
of Code
 Rate
 Type
Contest
Benchmarks
Account
 155
 3%
Atomicity
AirlinesTickets
 95
54%
Atomicity
BubbleSort2
 130
 69%
Atomicity
BufWriter
 255
 14%
Atomicity
Lottery
 359
 43%
Atomicity
MergeSort
 375
 84%
Atomicity
Shop
 273
 2%
Atomicity
Java
Collection
ArrayList
 5866
 2%
Atomicity
HashSet
 7086
 3%
Atomicity
StringBuﬀer
 1320
 3%
Atomicity
TreeSet
 7532
 3%
Atomicity
Vector
 709
 2%
Atomicity
Miscellaneous
Cache4j
 3897
 3%
Order
Hedc
 29947
 1%
Atomicity
Philo
 110
 0%
Atomicity
RayTracer
 1924
 14%
Atomicity
Tsp
 720
 0%
Atomicity
observed failure rate, to give a rough sense of the diﬃculty
of eliciting a fault. The ﬁfth column classiﬁes concurrencyviolation type for the known bug as either an atomicity or
an order violation.
The Contest benchmark suite is a set of small programs
(fewer than 1KLOC) created by students as a homework as-
signment in a class on concurrency [6]. The programs con-
tain various kinds of concurrency bugs, such as data races,
atomicity violations, and deadlock. We selected the subset
knowntohaveatomicityviolations[13]. Althoughthesepro-
grams are small, we include them because they have beenused in other studies of concurrency bugs [13]. Each pro-gram comes with a test harness that determines whether an
execution passed or failed.
The Java collection library classes , fromJDK1.4.2.,
include ArrayList ,LinkedHashSet ,StringBuffer ,TreeSet,
andVector. Theseclasses haveatomicity violations thatcan
result in a program crash or memory corruption [13, 25, 35].
We wrote multi-threaded test cases that can yield buggy in-terleavings. The test cases create two or more objects of the
collection, pass the objects to multiple threads, and concur-
rently call methods of the objects. If any child thread halts,the main thread catches an exception and memory corrup-tion to determine whether the program passes or fails.
The set of Miscellaneous programs contains additional
programs: Cache4j, a simple and fast cache implementa-
tion for Java objects; Hedc, a Web crawler application de-
veloped at ETH [33]; Philo, a dining philosophers’ problem
solver; RayTracer , a benchmark program from Java Grande
Forum
5;a n d Tsp, a Traveling Salesman Problem solver [33].
Every program has concurrency violations: Cache4j has an
order violation [30] and the other programs have atomicity
violations [3, 35, 36]. Philo and Tsp actually never failed,
even though we executed the programs for six hours; though
conﬂict interleaving and unserializable interleaving patterns
are present, they appear to be benign.
We ran our experiments on a desktop computer with a
2.66 GHz Intel Core 2 Duo processor and 4GB RAM, using
Windows Vista and Sun’s Java 1.5.
5http://www.javagrande.org/
250/;#23/;#23/;#23#23#23#23#23#23/;#23/;#23#23/;#23 /;#23/;#23#23/;#23#23#23#23#23#23 /;#23/;#23#23#23/;#23 /;#23/;#23#23#23/;#23#23#23#23#23#23 /;#23/;#23#23#23#23/;#23 /;#23/;#23#23#23#23/;#23#23#23#23#23#23 /;#23/;#23#23#23#23#23/;#23 /;#23/;#23#23#23#23#23/;#23#23#23#23#23#23 /;#23/;#23#23#23#23#23#23/;#23 /;#23
/;#23#23#23#23#23/;#23 /;#23#23#23#23#23#23#23#23/;#23 /;#23#23/;#23#23#23#23#23#23#23 /;#23 /;#23#23#23#23/;#23#23#23 /;#23 /;#23#23#23#23#23#23#23/;#23#23#23#23#23 /;#23/;#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23/;#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23/;#23
/;#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23/;#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23/;#23/;#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23
Figure 3: Results of Study 1: Relationship between
window size and number of threads.
4.3 Study 1: Window Size
The goal of this study is to investigate the relationship be-
tween thenumberofthreadsandthewindowsizerequiredto
capturethetargetunserializableinterleavingpatternsshownin Table 2. To do so, we computed a histogram of thewindow size for each pattern detected. For example, for
the pattern, R
1-W2-R1, within the reference stream, R 1-R3-
W2-R3-R1, we would increment the frequency for minimum
window size 5 in the histogram. We built this histogram for
each number of threads p,w h e r e p∈{4,8,16,32,64}.W e
set thewindow size to 50 for this experiment, so we compute
a histogram of the window size for patterns detected within
50 reference streams. We executed the benchmark programsk=100 times for each pwith artiﬁcial delays, and aggregate
the data over all benchmarks.
Figure 3 shows the results as a boxplot.
6The horizontal
axis is the number of threads, ranging from 4 to 64, and the
verticalaxisshowstheobserveddistributionofwindowsizes.
For example, for p= 8 threads, Figure 3 shows that the
window sizes ranged from 3 to 38, with 75% of the window
sizes no more than 6 (top of the 3rd quartile, which is thetop of the solid box).
Interestingly, we observe in Figure 3 that the minimum
and median window sizes are constant and always coincide
at the value 3, independent of the number of threads. Thismedian value indicates that a window size of 3 was suﬃcient
half the time. Moreover, the 3rd quartile is always at most
9, meaning that 75% of the time, a window size of at most9 is suﬃcient to capture the target patterns.
Thus, we may conclude that even though the worst case
windowsizeshouldgrowlinearlywiththenumberofthreads,as discussed in Section 3.1, in practice it does not. In partic-ular, there is 50% chance that we will catch the patterns of
interest even with the minimum window size of 3, and with
a window of no more than 10 we will capture them roughly
75% of thetime. Thus, we can expectonly amodest increase
in the number of test runs required to accurately rank thesepatterns with a small window size. Moreover, a small win-dow size implies that monitoring the patterns should incurrelatively small storage overhead.
6A boxplot consists of ﬁve numbers: the smallest observation, the
lower quartile, the median, the upper quartile, and the largest ob-
servation. The lower quartile, median, and upper quartile consist
of the box, and the minimum and maximum observations arelinked to the box by lines.4.4 Study 2: Effectiveness
The goal of this study is to investigate how well our tech-
nique ranks patterns by determining whether highly ranked
patterns correspond to true bugs. To do this, we used the
Falcon prototype with artiﬁcial delays and window size 5
to get the ranked patterns. We used the benchmark pro-
grams with their default number of threads, and executedeach program k=100 times.
Columns 2–6 in Table 4 summarize the results of this
study for the programs listed in the ﬁrst column. Column 2
reports the highest observed suspiciousness value; column 3
reports the number of patterns identiﬁed; column 4 reports
the number of patterns appearing in at least one failing ex-ecution; and column 5 reports the number of patterns ap-
pearing only in failing executions. We only report either the
number of conﬂicting interleaving patterns or the number
of unserializable interleaving patterns, according to the pro-gram’s violation type(seeTable3). Forinstance, inCache4j,the number of patterns indicates the number of conﬂictinginterleaving patterns since it contains an order violation; in
the other programs, the number of patterns indicates the
number of unserializable interleaving patterns. Column 6shows the highest rank of the ﬁrst pattern found by Fal-
conthat corresponds to a true violation. For example, the
Account program has 11 patterns typical of atomicity vio-
lations, among which 10 patterns appeared in at least one
failing execution, seven patterns appeared only in failing ex-
ecutions, and the highest rank assigned by Falcon to any
pattern corresponding to a true bug was 2.
We observe that Falcon is eﬀective for our subjects be-
cause it identiﬁes a true bug as either its ﬁrst or its second
ranked pattern. This result implies that a programmer need
only look at the ﬁrst or second pattern reported by Falcon
to ﬁnd an actual bug.
By contrast, other atomicity violation detectors that do
not rank and report more patterns, implying potentially
more programmer eﬀort to examine the report. Several of
these approaches [8, 13] will report the number of patterns
shown in Column 3, without additional ﬁltering. The AVIO
technique [17] would reduce these patterns, instead report-
ing the number of patterns shown in Column 5. However,
this number of patterns still implies more programmer’s ef-
fort than with Falcon.
Falcon alsoworkseﬀectivelyeveniftheprogramhasmul-
tiple concurrency bugs, which is the case with the Contestbenchmarks. For the Contest benchmarks, Falcon reports
diﬀerent patterns as the most suspicious pattern from diﬀer-
ent experiments, but themost suspicious patternwas alwaysa real bug.
There are three special cases in our data. Philo and Tsp
did not fail at all during our many runs of the programs,
andthus, wecannot report anysuspiciouspatternsfor them.
(That is, we detect all patterns but all suspiciousness scoreswill be 0 if there are no failing cases.) Hedc is the only casein which we cannot pinpoint the real bug, because the bug
is hidden in the library code. The bug is triggered when a
shared object concurrently calls a library method from mul-tiple threads. Because thebuglocation is in uninstrumentedlibrary code, bug detection tools including Falcon cannot
pinpoint the bug location. We are planning to address the
problem by adding probes before and after library method
calls and monitoring the method call interruption.
251Table 4: Results of Study 2 (eﬀectiveness) and Study 3 (eﬃciency).
Program
Study 2 (Eﬀectiveness)
 Study 3 (Eﬃciency)
Highest sus-
piciousness
score
Number of
patterns
identiﬁed
Number of
patterns
appearing
in at least
one failure
Number of
patterns ap-
pearingonly
in failure
Ranking of
bug by Fa l-
con
Slowdown
 Number ofaccesses
 Number ofmemory lo-
cations
Account
 0.8
 11
 10
 7
 2
 2
 79
 12
AirlinesTickets
 0.33
 4
 1
 0
 1
 1.1
 44
 3
BubbleSort2
 1
 4
 4
 1
 1
 1
 136270
 402
BufWriter
 0.33
 105
 71
 25
 1
 6.6
 20830
 1007
Lottery
 0.1
 4
 4
 4
 1
 4.9
 94
 14
MergeSort
 0.9
 76
 63
 42
 1
 2.6
 70
 23
Shop
 0.16
 10
 2
 0
 2
 3.8
 298
 34
ArrayList
 1
 1
 1
 1
 1
 8.3
 162
 49
HashSet
 1
 7
 3
 3
 1
 1.4
 202
 68
StringBuﬀer
 1
 2
 1
 1
 1
 8.2
 25
 11
TreeSet
 1
 9
 5
 3
 2
 1.4
 651
 129
Vector
 1
 1
 1
 1
 1
 3.2
 132
 44
Cache4j
 0.51
 23
 12
 2
 1
 1.4
 210835
 8059
Hedc
 0.01
 2
 2
 0
 0
 1
 7556
 706
Philo
 0
 9
 0
 0
 0(*)
 2.5
 87
 7
RayTracer
 1
 14
 14
 14
 2
 58.1
 147724
 1142
Tsp
 0
 24
 0
 0
 0(*)
 61.4
 451264
 15059
4.5 Study 3: Efﬁciency
The goal of this study is to investigate the eﬃciency of
theFalcon toolset by analyzing the slowdown caused by
instrumentation and dynamic analysis. For this study, we
implemented two counters to collect the number of shared
memory accesses and shared memory locations. We com-
puted the slowdown factor by comparing normal programexecution and Falcon-instrumented program execution.
Columns 7–9 in Table 4 summarize the results, showing
the slowdown factor, the numberofshared memory accesses,
and the number of shared memory locations, respectively.
For example, when executed with the Falcon toolset, Ac-
count experiences 2 ×slowdown over execution without any
instrumentation. Account has 12 shared memory locations,
which are accessed 79 times during program execution.
In terms of slowdown, Falcon matches or slightly im-
proves on prior work. The average slowdown of Falcon is
9.9×, whereas AVIO reports 25 ×[17], while other atomicity
violation detectors report 14 ×[13] and 10 .3×[9]. Given im-
provements in machines over time, we consider these slow-downs to be comparable. In fact, Falcon may have less
overhead because its sliding window data structure should
result in less memory usage; by contrast, Hammer, et al. [13]
construct data structures tha t are proportional to the num-
ber of shared memory accesses. RayTracer and Tsp show
much greater slowdown than the other programs, whereother techniques also experience exceptionally huge slow-down for the same programs [9, 35, 36]. We believe that a
large number of small method calls makes the instrumenta-
tion eﬀect worse than other programs.
4.6 Study 4: Comparison to CCI
Oneof the most closely related studies of fault localization
in concurrent programs with a ranking scheme is CCI [32].7
Thus, the goal of this study is to compare the eﬀectiveness
ofFalcon to CCI [32].
7Bugaboo [18] is a study applying statistical ranking to concur-
rent programs. Because the paper was published after the sub-
mission of this paper, we do not empirically compare to Bugaboo.For this study, we implemented the CCI technique in the
Falcon toolset (basedonourunderstandingofCCI)andex-
ecuted it for our benchmarks. The CCI toolset is similar to
Falcon in that both collect dynamic information from con-
current programs and report lists of entities with suspicious-
ness scores. However, CCI diﬀers in that it collects shared
variable accesses (predicates) and Falcon collects patterns.
CCI designs predicates to check whether a previous access
is thread-local or thread-remote. Using the predicates withpass/fail results, CCI computes suspiciousness scores using
statistical analysis [15] to report suspicious predicates (i.e.,
suspicious memory access locations).
Table 5 compares what the two techniques report, for the
benchmarks listed in Column 1. Column 2 shows the mostsuspicious bug pattern that Falcon reports. The notation
is“Bug:Rank”, where Bugrepresents the pattern of the true
bug and RankisFalcon’s rank of that pattern. Columns
3–5 show CCI’s ranking of the same accesses identiﬁed by
Falcon. For CCI, the notation is “Access:Rank”. For ex-
ample, for Account Falcon reports a real bug as an R-W-R
patternwith rank2; CCI’s rankingof theindividualaccessesfrom this pattern are 6th, 4th, and 6th, respectively.
We make two observations from this study. First, the re-
sults show that CCI tends to rank individual accesses of a
real bug much lower than Falcon ranks its patterns. This
observation suggests that Falcon may more eﬀectively pin-
point the real bug. Second, CCI gives less context for un-derstanding the actual currency bug than Falcon.T h a t
is, order and atomicity violations involve multiple accesses,
which means a programmer using CCI must be able to man-ually construct the right pattern, whereas Falcon reports
it directly.
5. RELATED WORK
There is a large and growing body of work on fault anal-
ysis and detection for concurrent programs. In this section,
we discuss the most relevant work to our own and contrast
existing methods to the Falcon approach.
252Table 5: Results of Study 4 (comparison to CCI).
Program
Fa lcon
 CCI
Bug:Rank
 A1:Rank
 A2:Rank
 A3:Rank
Account
 R1-W2-R1:2
R1:6
W2:4
R1:6
AirlinesTickets
 R1-W2-R1:1
R1:2
W2:5
R1:4
BubbleSor2
 W1-R2-W1:1
W1:1
R2:7
W1:2
BufWriter
 R1-W2-R1:1
R1:5
W2:1
R1:5
Lottery
 R1-W2-R1:1
R1:1
W2:7
R1:4
MergeSort
 W1-R2-W1:1
W1:1
R2:3
W1:12
Shop
 R1-W2-R1:2
R1:15
W2:12
R1:15
ArrayList
 R1-W2-R1:1
R1:4
W2:4
R1:4
HashSet
 R1-W2-R1:1
R1:15
W2:15
 R1:1
StringBuﬀer
 R1-W2-R1:1
R1:14
 W2:2
R1:1
TreeSet
 R1-W2-R1:2
R1:10
 W2:5
R1:10
Vector
 R1-W2-R1:1
R1:2
W2:2
R1:2
Cache4j
 W1-R2:1
W1:79
 R2:2
 -
Hedc
 -
 -
 -
 -
Philo
 -
 -
 -
 -
RayTracer
 R1-W2-W1:2
R1:4
W2:4
W1:3
Tsp
 -
 -
 -
 -
Data race detection. Early work focused onstaticanddy-
namic approaches to detecting data races , which occur when
multiple threads perform unsynchronized access to a shared
memory location (with at least one write). Static techniques
include those based on type systems [7], model checking[20],
and general program analysis [22]. Dynamic techniques in-clude happens-before (RecPlay [28]) and lockset algorithms
(Eraser [29]). There are additional approaches [23, 30], in-cluding hybrid analysis [24], that feature improved over-
heads and reduce the number of false positive reports.
The main drawback of data race detectors is that some
races—like those used in barriers, ﬂag synchronization, and
producer-consumer queues—are common parallel patternsthat relyon deliberate but benign races [17]. Programmers
are left to sort out benign and problematic cases on theirown. In Falcon, we focus on atomicity and order violations
(though we can also handle data races) and provide addi-tional information (suspiciousness scores) to help pinpoint afault’s root cause.
Atomicity violation detection. Researchers have sug-
gested that atomicity (orserializability )i sa na l t e r n a t i v e
higher-levelpropertythatcouldbecheckedtodetectconcur-
rency faults. Atomicity violation detectors were ﬁrst advo-
cated by Flanagan and Qadeer [10]. Atomicity checkers rely
on programmer annotations of atomic regions or other con-
structs, which the checkerscan then verify or use to do addi-
tional inference. There are numerous static [10], dynamic [8,9, 35, 36], and hybrid schemes [3]. The main practical draw-
back of atomicity violation detectors is the need for inves-
tigating synchronization keywords to infer atomic regions,which Falcon avoids by using pattern-analysis techniques.
Pattern analysis. Falcon is most closely related to the
class of pattern-analysis techniques. These include AVIO,which learns benign data access patterns from“training”ex-
ecutions; during testing executions, AVIO reports as “ma-
licious” any data access patterns not part of the training
set [17]. As discussed in Section 4.4, Falcon improves on
AVIO by computing suspiciousness scores. This approachprioritizes patterns and mitigates false positive cases. More-
over, it can reduce false negatives, since in AVIO a faultypatternscould appear in bothpassing and failing executions;
inFalcon, our weighted ranking mitigates this eﬀect.Hammer et al. [13] develop the notion of atomic-set-seri-
alizability, an extension of conﬂict-serializability [35], which
can capture atomicity violations with more precision by con-sidering atomic regions. Their tool records data access se-quences at runtime. As with AVIO, Falcon improves on
the Hammer, et al., technique by ranking patterns. More-
over, Falcon reduces space-time requirements, as discussed
in Section 4.5.
Bug eliciting techniques for concurrent programs. A
drawback of any testing-basedapproach is thatprogram fail-
ures may occur infrequently (if at all). Introducing randomdelays using irritators can increase the likelihood of a buggyinterleaving [5, 34]. More focused (non-random) schemes
exist as well. These include schemes that control the sched-
uler to elicit speciﬁc interleavings [30]; run-time monitor-
ing and control of synchronization [25]; and analysis-basedmethods [26].
Musuvathi et al. published several papers [20, 21] for
CHESS model checker, which reduces the interleaving space
by bounding the number of preempting context switches.
The technique is based on a theorem that limiting context
switches only at synchronization points is suﬃcient to detectalldataracesin theprogram[20]. Thus, thetoolinvestigates
polynomial time interleaving space, while checking assertion
violations, deadlock, livelock, and data races.
InFalcon, we provide the option of introducing random
delays, though we did not evaluate it experimentally. In
general, we believe bug eliciting methods complement our
approach, and combined schemes are possible.
Faultlocalization. Thereareanumberoffault-localization
techniques based on code coverage, particularly for sequen-
tial programs [1, 14, 15]. These methods instrument code
predicates (e.g., statements or branches) and check cover-
age by counting the number of occurrences of the predicates
in a number of passing and failing executions. These pred-
icates are then assigned some suspiciousness score. Asidefrom Falcon, CCI [32] and Bugaboo [18] apply to fault lo-
calization. The main distinction of Falcon is that it ranks
patterns, which can provide more contextual informationthan statement or predicate expression ranking as done inCCI or Bugaboo.
6. CONCLUSION
Our technique for fault localization in concurrent pro-
grams combines two promising approaches: (1) dynamic
analysis to identify shared memory access patterns associ-
ated with order and atomicity violations, and (2) ranking
these patterns statistically, using pass/fail test case data.We believe ours to be the ﬁrst technique to both report
and rank patterns. Our empirical study shows that our
implementation, Falcon, is eﬀective in ranking true fault
patterns highly and eﬃcient in storage and time overheads.
Taken together, these observations suggest that a Falcon-
like approach could be deployed in real test environments.
Though our initial study is promising, we need additional
studies on more diverse subjects to determine whether our
technique generalizes. In addition, our evaluation relied on
only one test input and multiple runs. We plan additional
studies considering multiple test inputs, input fuzzing, andmore analysis of irritator techniques, to see if we can reducethe overall number of runs required to isolate a fault.
Anotherarea for future work involves the number of faults
253that are identiﬁed in the program. For deterministic pro-
grams, a debugging approach would locate the ﬁrst fault,
ﬁx it, and rerun the test cases again to determine whether
the program still failed. For nondeterministic programs, this
approach may not work because, after one fault is found andﬁxed, rerunning the test cases may not be suﬃcient to de-termine whether any faults remain. Thus, for concurrent
programs, it might be useful to search for several faulty pat-
terns at once. We believe that if several faulty patterns existin the program, their suspiciousness values may all be high.We plan to investigate whether our hypothesis holds andwhat additional information we can provide to the devel-
oper to assist in locating multiple faults in the program.
A third area for future work concerns the faulty patterns
that our technique detects. There are additional types of
atomicity violations, such as deadlock that can be investi-gated. Additionally, multiple variable related atomicity vi-olations patterns may identify diﬃcult-to-detect bugs. Weplan to investigate extensions to our technique to handle
these additional patterns.
7. ACKNOWLEDGEMENTS
This work was supported in part by awards from NSF
under CCF- 0429117, CCF-0541049, and CCF-0725202, and
Tata Consultancy Services, Ltd. to Georgia Tech.
8. REFERENCES
[1] R. Abreu, P. Zoeteweij, and A. J. C. van Gemund. On the
accuracy of spectrum-based fault localization. In TAIC
PART, pages 89–98, 2007.
[2] K. Asanovic, R. Bodik, B. C. Catanzaro, J. J. Gebis,
P .H u s b a n d s ,K .K e u t z e r ,D .A .P a t t e r s o n ,W .L .P l i s h k e r ,
J. Shalf, S. W. Williams, and K. A. Yelick. The landscapeof parallel computing research: A view from Berkeley.
Technical Report UCB/EECS-2006-183, UC Berkeley, 2006.
[ 3 ]Q .C h e n ,L .W a n g ,Z .Y a n g ,a n dS .D .S t o l l e r .H A V E :
detecting atomicity violations via integrated dynamic and
static analysis. In ETAPS, pages 425–439, 2009.
[4] J. DeSouza, B. Kuhn, B. R. de Supinski, V. Samofalov,
S. Zheltov, and S. Bratanov. Automated, scalabledebugging of MPI programs with the Intel ®Message
Checker. In Proc. Int’l. Wkshp. Software Eng. for HPC
System Applications , IEEE Int’l. Conf. Software Eng.
(ICSE), pages 78–82, 2005.
[5] O. Edelstein, E. Farchi, Y. Nir, G. Ratsaby, and S. Ur.
Multithreaded java program test generation. In Java
Grande, page 181, 2001.
[6] Y. Eytani, K. Havelund, S. D. Stoller, and S. Ur. Towards a
framework and a benchmark for testing tools for
multi-threaded programs. Concurr. Comput. : Pract.
Exper., 19(3):267–279, 2007.
[7] C. Flanagan and S. N. Freund. Type-based race detection
for java. In PLDI, pages 219–232, June 2000.
[8] C. Flanagan and S. N. Freund. Atomizer: a dynamic
atomicity checker for multithreaded programs. In POPL,
pages 256–267, 2003.
[9] C. Flanagan, S. N. Freund, and J. Yi. Velodrome: a sound
and complete dynamic atomicity checker for multithreaded
programs. In PLDI, pages 293–303, 2008.
[10] C. Flanagan and S. Qadeer. A type and eﬀect system for
atomicity. In PLDI, pages 338–349, 2003.
[11] P. Godefroid and N. Nagappan. Concurrency at Microsoft:
An exploratory survey. In Workshop on Exploiting
Concurrency Eﬃciently and Correctly , 2008.
[12] R. L. Halpert. Static lock allocation. Master’s thesis,
McGill University, 2008.[13] C. Hammer, J. Dolby, M. Vaziri, and F. Tip. Dynamic
detection of atomic-set-serializability violations. In ICSE,
pages 231–240, 2008.
[14] J. A. Jones and M. J. Harrold. Empirical evaluation of the
tarantula automatic fault-localization technique. In ASE,
pages 273–282, 2005.
[15] B. Liblit, M. Naik, A. X. Zheng, A. Aiken, and M. I.
Jordan. Scalable statistical bug isolation. In PLDI, pages
15–26, 2005.
[16] S. Lu, S. Park, E. Seo, and Y. Zhou. Learning from
mistakes: A comprehensive study on real world
concurrency bug characteristics. In ASPLOS , pages
329–339, March 2008.
[ 1 7 ]S .L u ,J .T u c e k ,F .Q i n ,a n dY .Z h o u .A V I O :d e t e c t i n g
atomicity violations via access interleaving invariants. In
ASPLOS , pages 37–48, 2006.
[18] B. Lucia and L. Ceze. Finding concurrency bugs with
context-aware communication graphs. In MICRO, pages
553–563, 2009.
[19] C. E. McDowell and D. P. Helmbold. Debugging concurrent
programs. ACM Computing Surveys (CSUR) ,
21(4):593–622, 1989.
[20] M. Musuvathi and S. Qadeer. Iterative context bounding
for systematic testing of multithreaded programs. In PLDI,
pages 446–455, June 2007.
[ 2 1 ]M .M u s u v a t h i ,S .Q a d e e r ,T .B a l l ,G .B a s l e r ,P .N a i n a r ,
and I. Neamtiu. Finding and reproducing heisenbugs in
concurrent programs. In OSDI
, pages 267–280, 2008.
[22] M. Naik and A. Aiken. Conditional must not aliasing for
static race detection. In POPL, pages 327–338, 2007.
[23] S. Narayanasamy, Z. Wang, J. Tigani, A. Edwards, and
B. Calder. Automatically classifying benign and harmful
data racesallusing replay analysis. In PLDI, pages 22–31,
2007.
[24] R. O’Callahan and J. Choi. Hybrid dynamic data race
detection. In PPoPP, pages 167–178, 2003.
[25] C. Park and K. Sen. Randomized active atomicity violation
detection in concurrent programs. In FSE, pages 135–145,
2008.
[26] S. Park, S. Lu, and Y. Zhou. CTrigger: exposing atomicity
violation bugs from their hiding places. In ASPLOS , pages
25–36, 2009.
[27] K. Poulsen. Tracking the blackout bug. SecurityFocus ,
February 2004. http://www.securityfocus.com/news/8412 .
[28] M. Ronsse and K. D. Bosschere. RecPlay: a fully integrated
practical record/replay system. Trans. Comput. Syst. ,
17(2):133–152, 1999.
[29] S. Savage, M. Burrows, G. Nelson, P. Sobalvarro, and
T. Anderson. Eraser: a dynamic data race detector formultithreaded programs. Trans. Comput. Syst. ,
15(4):391–411, 1997.
[30] K. Sen. Race directed random testing of concurrent
programs. In PLDI, pages 11–21, 2008.
[31] M. S ¨uß and C. Leopold. Common mistakes in OpenMP and
how to avoid them. In OpenMP Shared Memory Parallel
Programming , volume LNCS 4315, pages 312–323, 2008.
[ 3 2 ]A .T h a k u r ,R .S e n ,B .L i b l i t ,a n dS .L u .C o o p e r a t i v ec r u g
isolation. In WODA, pages 35–41, 2009.
[33] C. von Praun and T. R. Gross. Object race detection. In
OOPSLA , pages 70–82, 2001.
[34] R. Vuduc, M. Schulz, D. Quinlan, B. de Supinski, and
A. Sæbjørnsen. Improving distributed memory applicationstesting by message perturbation. In PADTAD , pages
27–36, 2006.
[35] L. Wang and S. D. Stoller. Accurate and eﬃcient runtime
detection of atomicity errors in concurrent programs. InPPoPP, pages 137–146, 2006.
[36] J. Yi, C. Sadowski, and C. Flanagan. Sidetrack:
Generalizing dynamic atomicity analysis. In PADTAD ,
2009.
254