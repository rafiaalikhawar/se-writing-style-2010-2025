Comparing Static Bug Finders and Statistical Prediction
Foyzur Rahman†Sameer Khatri†Earl T. Barr‡Premkumar Devanbu†
†Department of Computer Science
University of California Davis
Davis, CA 95616, USA
{mfrahman, sakhatri, ptdevanbu}@ucdavis.edu‡Department of Computer Science
University College London
London, WC1E 6BT, UK
e.barr@ucl.ac.uk
ABSTRACT
The all-important goal of delivering better software at lower
cost has led to a vital, enduring quest for ways to ﬁnd and
remove defects eﬃciently and accurately. To this end, two
parallel lines of research have emerged over the last years.
Static analysis seeks to ﬁnd defects using algorithms that
process well-deﬁned semantic abstractions of code. Statisti-
cal defect prediction uses historical data to estimate parame-
ters of statistical formulae modeling the phenomena thought
to govern defect occurrence and predict where defects are
likely to occur. These two approaches have emerged from
distinct intellectual traditions and have largely evolved in de-
pendently, in“splendid isolation”. In this paper, we evaluat e
thesetwo(largely)disparateapproachesonasimilarfooting.
We use historical defect data to apprise the two approaches,
compare them, and seek synergies. We ﬁnd that under some
accounting principles, they provide comparable beneﬁts; we
also ﬁnd that in some settings, the performance of certain
static bug-ﬁnders can be enhanced using information pro-
vided by statistical defect prediction.
Categories and Subject Descriptors
D.2.5 [Software Engineering ]: Testing and Debugging—
Code Inspections and Walk-Throughs
General Terms
Experimentation; Measurement; Reliability; Veriﬁcation
Keywords
Software Quality; Fault Prediction; Inspection; Empirical
Software Engineering; Empirical Research
1. INTRODUCTION
Given the centrality of software in modern life and the
fallibility of human programmers, better software quality is
a never-ending quest. Software quality-control involves sev-
eral distinct approaches, including testing, inspection, an d
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
ICSE ’14, May 31–June 7, 2014, Hyderabad, India
Copyright 14 ACM 978-1-4503-2756-5/14/05 ...$15.00.formal veriﬁcation. In this paper, we use historical defect
data to comparatively evaluate and search for synergies be-
tween two approaches that have, of late, been of tremendous
interest both in academia and industry: static bug-ﬁnding
andstatistical defect prediction
Static Bug-Finding ( SBF):Theseapproachesrangefrom
simple code pattern-matching techniques to rigorous static
analyses that process carefully designed semantic abstrac-
tions of code; all SBFtools ﬁnd and report likely defect
locations in code. These likely locations are reported to pro-
grammers (typically) at coding-time. The pattern-matching
techniques, such as PMD, are unsound but scale well and
have been eﬀectively employed in industry. Hybrid tools
likeFindBugs incorporate both static data-ﬂow analysis,
and pattern matching. Tools like ESC-Java [8] and CodeS-
onar1areslower, buttheiranalyzeshavedesirable soundness
properties (See Section 2.2). In practice, however, reported
warnings are infested with both false positives and/or false
negatives. False positives can waste a developer’s time, and
false negatives can allow defects to escape and cause cus-
tomer grief. Developers usually cannot determine the truth
or falsity of a given warning without examining code.
Defect Prediction ( DP):Extensiveandwidespreaddata-
gathering in modern software processes stimulates this ap-
proach. Animated by theories of the human and techni-
cal factors that durably inﬂuence errors, researchers have
channeled this deluge of data to estimate rich statistical
(and machine-learning) models that predictwhere defects
can occur. Given the relative immutability of human na-
ture, models are expected to be quite stable over relatively
long time intervals. Thus, it has been believed statistical
methods can predict where defects are likely to occur in the
future. Empirical evaluations using fairly sophisticated eco-
nomic models suggest that these methods are indeed likely
to be eﬀective in helping to locate defects [2, 5, 12]
Motivation/Barriers: These approaches have emerged
fromparallelanddisparatetraditionsofintellectualthought:
one driven by algorithm and abstraction over code, and
otherbystatisticalmethodsoverlargedefectdatasets. The se
diﬀerences are perhaps analogous to the Chomsky/Norvig
debate in computational linguistics (see Section 2).
Although the“holy grail”of SBFis automatically proving
a correctness property of a program, i.e.certifying the pro-
gram free of a certain class of bugs, undecidability means
that, in practice, SBFproduces warnings that identify lines
for developer inspection. For its part, DPidentiﬁes ﬁlesfor
1http://www.grammatech.com/codesonarPermission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a
fee. Request permissions from Permissions@acm.org.
ICSE’14 , May 31 – June 7, 2014, Hyderabad, India
Copyright 2014 ACM 978-1-4503-2756-5/14/05...$15.00
http://dx.doi.org/10.1145/2568225.2568269
424
inspection. In practice, then, these approaches tackle the
same problem: improving inspection eﬃciency , the problem
of ﬁnding minimal, potentially defective regions in source
for careful inspection. However, neither is infallible.
Although, in practice, these two approaches share the
same goal, their performance is diﬃcult to compare. We
call this the comparability of bugginess identiﬁcation (CBI)
problem; This problem comprises two subproblems: zona-
tionandmeasurement . The zonation problem concerns the
granularity at which a tool reports bugginess: intra-line,
line, statement, scope, block, method, ﬁle, etc.. While most
defects are local and contiguous, a single defect can be arbi-
trarily scattered in the sense that ﬁxing it requires changes
to lines throughout a codebase. Defect scatter causes the
measurement problem: given a particular zonation, how
should one measure bug identiﬁcation? Choices for a bi-
nary score include 1) the reported zone must subsume the
defect or 2) it must simply intersect the defect; choices for a
fractional score include 1) the ratio of the size of intersection
to the size of the defect or 2) their Jaccard index.
The IEEE Computer Society deﬁnes software engineering
as “(1) The application of a systematic, disciplined, quan-
tiﬁable approach to the development, operation, and main-
tenance of software; that is, the application of engineering
to software. (2) The study of approaches as in (1).” [24];
its goal is to ﬁnd ways to produce better, cheaper software
faster. The CBI problem, therefore, cuts to the core of soft-
ware engineering: neglecting it violates this imperative be-
cause it not only leaves unanswered which approach is bet-
ter than the other under what circumstances, but it also
forgoes the exploration of their synergies in service of this
goal. Both approaches have attracted considerable invest-
ment, which could be better guided by a good answer to the
former question; failure to address the latter has stymied
cross-pollination between the two traditions. We take the
ﬁrst steps toward solving it and building a bridge between
SBFandDP.
The two approaches oﬀer very diﬀerent trade-oﬀs. DP
tools are very easy to implement: once adequate history is
available, programming language, build environment, plat-
form evolution, etc.are immaterial; indeed, process meta-
data is very easily extracted from bug repositories and ver-
sion control has proven to be much more valuable than met-
rics based on source code content [19]. On the other hand,
DPgenerally operates at a coarse granularity, typically sug-
gesting entire ﬁles for inspection. SBFtools, including bug-
ﬁnders, are ﬁne-grained, suggesting individual lines to in-
spect; but they are language- and platform-speciﬁc; they
require a variety of compilation, thus carrying the baggage
of build procedures that vary with language, platform and
even time. SBFtools can be very diﬃcult to deploy2. In-
deed, our dataset of warnings from bug-ﬁnding tools is very
hard-won, and we claim the public availability of our dataset
is itself an important stepping-stone for future work on com-
paring the two and searching for synergies.
In current state-of-the-art SBFandDPtechniques and
tools, the CBI problem ﬁrst manifests itself in diﬀerences
in how the two traditions have done their evaluation. In
terms of zonation, DPworks at ﬁle granularity while SBF
works at line granularity; this zonation mismatch compli-
cates the choice of a bug-identiﬁcation measure. Solving the
2See [4] for vivid testimonials on deployment diﬃculties.CBI problem, and comparing the two approaches is vital,
given the investments in using them and the high human
and economic cost of software defects. Section 3.2 presents
the choices we made and the techniques we deployed to over-
come these challenges and present a ﬁrst solution to the CBI
problem.
We empirically study the value of three diﬀerent static
bug-ﬁnding tools, FindBugs ,Jlint, andpmd, in conjunc-
tion with statistical defect prediction, using a very large set
of historical defects.
•We formulate and present two cost-eﬀectiveness-based
accounting approaches to the comparability of buggi-
ness identiﬁcation problem (CBI).
•We ﬁnd no signiﬁcant diﬀerences in the cost-eﬀect-
iveness of the DPand the two SBFtechniques —
FindBugs andpmd— we could evaluate.
•We ﬁnd SBFandDPdo sometimes ﬁnd diﬀerent de-
fects.
•We ﬁnd the incorporation of metrics derived from SBF
does not signiﬁcantly improve the performance of DP
techniques.
•Weﬁndthat SBFtoolsfrequentlyperformbetterwhen
ordering their warnings using priorities produced by
DPthan when using their native priorities.
•Finally, we have (with a great deal of eﬀort) created a
multi-release repository of warnings from three tools,
Jlint,pmd, andFindBugs , which we shall make pub-
licly available for future research.
These ﬁndings are actionable ; as explained earlier, DPare
independent of language and build procedures, and work
very well with process meta data [19]; if they could pro-
vide comparable beneﬁts, this is good news for projects
that have abundant process data, but have complex, multi-
lingual source bases and build procedures that (see [4]) in-
hibit the adoption of SBFtools. It is also interesting that
they do ﬁnd diﬀerent defects in some cases, so that if bud-
gets allow, it might be worthwhile to use both. Finally,
under one measurement regime, DPappears to be almost
always a better way to order SBFwarnings, compared to
their native priority-ordering.
2. BACKGROUND
The contrast between SBFandDPhas parallels in the
Chomsky-vs-Norvig debate about statistical vs.ﬁrst princi-
ples approaches to natural language processing3. Chomsky
tends to favor ﬁrst-principles approaches to NLP, whereas
Norvig argues that NLP should be based on the statistics
of human linguistic behavior. Defects in software, likewise,
arise from the interaction of formal semantics of PL and
human behavioural imperfections.
SBFresearchers observe a pattern in defect occurrence,
use PL semantics to theorize how such defects could be
found, and then engineer abstractions and algorithms to ﬁnd
these defects. DPresearchers observe patterns in human be-
haviors that cause defects, and (because formal theories of
3Norvig-vs-Chomksy and the ﬁght for the future of AI at
http://www.tor.com , June 2011.425humans do not exist), pursue the ML approach of extract-
ing features and letting learning algorithms do the work of
ﬁtting the data. Perhaps because of diﬀerent intellectual
traditions, the two have never been properly compared and
synthesized. In this paper, we ﬁrst attack the problem of
comparing the two on an equal footing.
(Prelude) Defect data: Most modern software projects
follow software processes that require extensive data gath-
ering. Version control tools such as gittrack every change
made to the code. Tools such as BugZilla andjiraare
widely used to record bug reports, track the associated dis-
cussion and task allocation, and eventually relate the ﬁx to
a speciﬁc commit in a speciﬁc set of ﬁles by a speciﬁc in-
dividual. It is thus possible to ﬁnd where (which ﬁles and
line numbers) defect repairs occurred. Because of the careful
tracking of changes by version control systems such as git,
it is actually possible to precisely track the provenance of
each line of code; it is thus possible to track, for every line
of code that is changed to ﬁx a defect, exactly where that
line came from. A large body of research has been animated
by the availability of this data, and a steady stream of re-
sults in defect prediction and the etiologies of defects has
been coming out in recent years. In this paper, we use this
data to evaluate static analysis tools and statistical defec t
prediction, and to compare the two and seek synergies.
2.1 Statistical Defect Prediction
Statistical defect prediction ( DP) employs historical data
on reported (and repaired) defects to predict the location of
previously unknown defects that lurk in the code.
Speciﬁcally, let us assume that we are developing the code
for release Rn. When working on Rn, we are also repair-
ing defects reported on the previous release Rn−1. In the
process of working on Rn, we also, invariably, introduce de-
fects, someofwhichwillbediscoveredandreportedafterthe
oﬃcial release of Rn, and presumably, ﬁxed during the de-
velopment of Rn+1. In modern development, one typically
gathers not just the details of all defect repairs during R1
··· R n, but also other process and product metrics associ-
ated with ﬁles (or packages, or directories) such as complex-
ity metrics, number of developers, number of lines changed,
number of commits, etc. The task of statistical defect pre-
diction is to learn a per-ﬁle prediction function bugsof the
form
defectcount (f) =bugs(m1(f),m2(f),···,mn(f)),
wherem1,m2,···are process or product metrics over the
ﬁlef, as of release of Rnand the predicted defect count is
the number of defects bugspredicts will be discovered in
the ﬁlef. In some settings bugsis a binary, indicating only
whether somedefect is predicted to occur in f.
Defect prediction is a fairly mature ﬁeld of research. Pre-
vious research indicates that process metrics [19] and orga-
nizational metrics [17] are quite eﬀective in predicting the
loci of defects. Defect prediction models have been reported
to have been used at Google [13].
Evaluating Defect Prediction: DPtools are intended
to be used to focus quality control eﬀorts. Good predic-
tion performance is thus vital, to ensure that limited money
and time are spent eﬀectively. Several diﬀerent approaches
to evaluating DPtool performance have been reported. IR
methods such as precision, recall and accuracy are easily
calculated, but are diﬃcult to interpret in the highly class-imbalanced settings of defect prediction; a simple guesser
that predicts all ﬁles to be defect-free achieves high-levels
of accuracy. In response, the non-parametric AUROC (area
underthereceiver-operatingcharacteristiccurve, alsoknown
as AUC) has become increasingly popular. However, as
noted by Arisholm & Briand [2], measures of DPperfor-
mance should be sensitive to the costof inspecting ﬁles;
in particular larger ﬁles are often more costly to inspect.
They proposed AUCEC (area under the cost-eﬀectiveness
curve), a “lift-chart” measure, non-parametric (like AUC),
but diﬀerent in that it is sensitive to the cost of inspection,
essentially based on the number of lines inspected. It is by
nowde rigueur for papers on DPtools to report a variety
of parametric, non-parametric, and cost-sensitive measures
of performance. The scholarly literature on well-evaluated
DPtools is extensive, as a simple search will reveal; we have
just presented a few highlights above, for lack of space.
2.2 Static Bug Finders
Staticbugﬁnding( SBF)arguablybeginswithtypecheck-
ing built into the compiler. We focus here on supplemen-
tary tools, which are typically targeted at speciﬁc kinds
of coding errors, such as buﬀer overﬂow, race conditions,
SQL injection vulnerabilities, or cross-site scripting attacks
[11, 7, 28, 9]. Static analysis has typically developed oppo r-
tunistically, as researchers discover new classes of defects,
and seek to invent abstraction techniques and algorithms
that can detect these classes of defects eﬃciently. Analy-
sis tools that can detect various classes of defects, includin g
memory allocation errors, race conditions, buﬀer overﬂows,
and taint-related errors, have been reported in the litera-
ture. The veriﬁcation imperative is to never falsely certify
a program to be free of some class of bugs. Since bug-
freeness is undecidable in general, veriﬁcation techniques
over-approximate program behavior to include infeasible be-
havior so long as no feasible behavior is lost. Bug-freeness is
then proven with respect to the over-approximation. A tool
is sound, i.e.meets the veriﬁcation imperative, when this
proof goes through and the over-approximation is proven
not to lose any feasible behavior. Imprecision is the degree
to which an over-approximation admits infeasible behavior.
Sound static analysis generates false positives when it warns
aboutinfeasiblebehaviorintroducedbyover-approximation.
Undecidability of nontrivial program properties thus forces
a Hobson’s choice: sound tools have false positives, whereas
unsound tools can have false negatives. And so, without
foreknowledge of which warnings are false and which are
true, developers must examine all the lines of code ﬂagged
in the warnings and hope to ﬁnd some actual defects.
Evaluating Static Analysis Tools: The evaluation crite-
ria here have parallels with those discussed above for defect
prediction. SBFtools pay oﬀ when warnings lead to true
positives, viz., actual defects, and waste eﬀort when warn-
ings are false positives; they incur potential subsequent cos ts
for missed defects that“leak”out as ﬁeld defects. Formerly,
papers on static analysis tools reported successful experi-
ences with ﬁnding actual defects with their tools. When
open-source developers agreed that reported warnings were
actually bugs, and agreed to ﬁx them, that was considered
a success. Often the sample of test subjects is chosen by
theSBFdevelopers to illustrate the power of the tool in
ﬁnding the speciﬁc coding errors targeted by the SBFtool,
rather than improving the overall quality of the test subject.426Clearly, not all coding errors lead to defects that actually get
exposed and reported; this issue is typically not of concern
in these evaluations. In terms of CBI, their zonation is line,
and their measure, developer conﬁrmation of a warning. In
addition, these papers typically describe one speciﬁc tool,
and evaluate it, rather than comparing the overall power of
several diﬀerent tools in ﬁnding defects.
Our evaluation here is comparative, retrospective and cen-
ters onreported defects : we are asking,
“If developers had actually carefully inspected the
lines warned by SBFtools, how many of the de-
fects subsequently reported (and ﬁxed) in the sys-
tem could potentially have been discovered?” .
We also retrospectively evaluate SBFandDPon an equal
footing, and explore synergies. We focus exclusively on
pattern-matching static bug-ﬁnding tools for three reasons:
1) commercial static analysis tools are encumbered by li-
censes that prohibit their study; 2) unencumbered static
analysis tools are prototypes that usually suﬀer from bit-rot
and are diﬃcult to acquire, build, and run; 3) in particular,
running these tools retrospectively on systems with long his-
tories is especially diﬃcult; and 4) the fact that static anal y-
sis tools do not scale well to large systems, which would have
hampered a fair comparison with prediction models, which
do scale, and, in fact, are hungry for masses of data. In
particular, our work focuses on Jlint,FindBugs andpmd.
Closely Related Work: We now brieﬂy review prior work
in evaluating SBFtools. Kim & Ernst [10] analyze history
to determine which warnings programmers tend to ﬁx, to
help prioritize future warnings; they focused on warnings,
not associated defects (if any). Wagner et al[27] evaluate
FindBugs, PMD and QJ Pro with 4 small Java projects (3K-
58K NCSL) by manually inspecting warnings to determine
how many of them were true positives. Our interest is in
much larger systems; we also focus on ﬁeld defects, and false
negatives with respect to ﬁeld defects. Rutar et al[23] eval-
uate the overlap between diﬀerent Java bug ﬁnding tools,
without considering their relationship to reported defects.
Thunget al[26] actually do retrospectively evaluate with
ﬁeld bugs with 3 widely used open-source systems; they
use a case study methodology, manually examining the code
to evaluate the precision and recall of static analysis tools.
Nandaet al[18] also do a retrospective evaluation with re-
spect to actual defects, but their retrospective evaluation
is concerned primarily with null pointer defects; we con-
sider all kinds of reported ﬁeld defects. Zheng [30] evaluated
the eﬀectiveness of SBFtools in “live” use in an industrial
testing, evaluating what kinds of errors the tools detected.
Marchenko & Abrahamsson [14] report that warning counts
are sometimes correlated (and sometimes anti-correlated!)
with defects. Nagappan & Ball [16] report that static analy-
sis warning density is well-correlated with prerelease defect
density; weareinterestedtousemoretypicalprocess-metric-
basedpredictionmodels, ratherthanstaticanalysistools per
se. Ayewah [3] et alreport the results of a survey at Google
on how users and projects use FindBugs. Bessy [4] oﬀers en-
gaging anecdotes on the experience of running CoverityTM
at scale. The defect-ﬁnding rate is not reported. None of the
above approaches speciﬁcally compare or look for synergies
between DPandSBF.
Our central experimental conceit is evaluation based on
actual, reported and ﬁxed defects; we take the position thatthese defects (which after all, are the ones that developers
actually chose to ﬁx) are the most important ones. While
otherdefectsmaylurkundetectedinthesystemforyears, we
argue that the defects that were actually reported and ﬁxed
are the ones most likely to have inﬂuenced the perceived
quality of the system. So we assess the potential value of
DPandSBFbased on their potential to guide developers
towards locating these defects as early as possible.
An immediate consequence of this conceit is that our anal-
ysis ispost facto ; as such, we have to retrospectively predi-
cate how the DPandSBFtoolsshouldhave been used, to
locate and remove these defects beforethey were released
into the wild and base our analysis on this predication. We
chose a simple approach. We stipulate that:
1. The tools (either DPorSBF) are run very close to
release date;
2. Lines indicated with warnings by SBFand ﬁles pre-
dicted as defective by DPare carefully inspected by
competent personnel; and
3.All defects associated with those lines are discovered
during inspection.
Whilethesesimpliﬁcationsarenotentirelyrealistic, wear-
gue that they are justiﬁable, for a retrospective experiment.
First (step 1 above), many processes do indicate that code
inspections occur fairly close to release candidate status; fo r
simplicity, we run the analysis tools and defect prediction
on the code of a released version. Second (step 2 above),
the cost of inspecting the entire system make it quite rea-
sonable to target inspections at those portions of the system
that are considered highest risk. Finally (step 3), as a ﬁrst
step toward solving the CBI measurement problem, we as-
sume that all defects in the inspected code are discovered,
even ones unrelated to SBFwarnings.
Thiseﬃcacy assumption is made in published literature
on the evaluation of DPtools, since DPtools indicate the
likely locations of defects based on previous patterns, rather
than on speciﬁc etiologies. However, eﬃcacy is typically
notassumed in the evaluation of SBFtools: a line of code
that ﬂagged as containing a race condition may not, in fact,
contain a race condition, but contain instead an unrelated
error. By convention, SBFtool researchers typically give
theSBFtool credit only if the actual error corresponds
to the reported error. To measure the two approaches on
equal footing, we must adopt either DP’s eﬃcacy assump-
tion orSBF’s more stringent measure. We lack an oracle
for matching warnings to bugs and are operating at a scale
where manually checking warning to bug correspondence is
infeasible, so we make DP’s eﬃcacy assumption. In other
words, we assume that a careful code inspector would in fact
detect any and all errors in the warned lines.
In conclusion, we make a fairness assumption that applies
equally to SBFandDP: if some code is ﬂagged for inspec-
tion, regardless of the reason why, we assume that a compe-
tent and conscientious developer would detect anydefect in
that region of code.
2.3 Research Questions
Armed with our solution to CBI, which we explicate in
Section 3.2, we are now ready to compare SBFtoDP. First,
we ask how well do the two approaches perform in terms of
eﬀectively identifying code for inspection.427Research Question 1: How do static bug-ﬁnding
tools compare with statistical defect prediction with
partial and full credit?
Here, partial and full credit refers to our solution to the
CBImeasurementproblem, againasdescribedinSection3.2.
To our knowledge, we are the ﬁrst to eﬀect this comparison.
Having established a means for comparing the two ap-
proaches we now turn our attention to the search for syner-
gies between the two approaches. First we ask;
Research Question 2: Can statistical defect pre-
diction improve the performance of static bug-ﬁnding
tools?
then,“vice versa”, we ask:
Research Question 3: Can static bug-ﬁnding tools
improvetheperformanceofstatisticaldefectprediction?
3. EXPERIMENTAL METHODOLOGY
We study ﬁve open-source projects from the Apache Soft-
ware Foundation: Lucene, Derby, Wicket, OpenJPA, and
QpidTM, as shown in Table 1. They range in size from 68-
630K NCSL. All are Java projects. There are varying num-
bers of releases for each project. We studied the occurrence
of bugs, and the performance DPand and SBFapproaches
at release level intervals. Speciﬁcally, as described above,
we assess how well inspections just prior to a release Rn,
guided by DPand/orSBF, would have helped developers
avoid defects discovered after Rnis made available to users.
3.1 Data Gathered
Foreveryproject, wegatheredversion-controlinformation
fromgit; all projects also use the jiraissue tracking sys-
tem. From the jiradata, we identiﬁed commits that were
bug ﬁxing. We considered any ﬁle associated with a bug ﬁx
to be buggy. We used gitblameto identify the provenance
of the lines where defects occurred with options for detecting
changes and moves, and whitespace insensitivity. We used
theSZZ[25]approachwiththisdatatoidentifythesourceof
the buggy lines. For each release of each project, we collect
the warnings reported by FindBugs ,pmd, andJlint.SBF
tools have diﬀerent settings that could potentially produce
diﬀerent sets of warnings. In practice, we found that the
warnings produced FindBugs andJlintshowed no varia-
tion with diﬀerent settings. For pmdwe used all the Java
language rulesets, except for rules having to do with com-
ments, coupling & design (we felt these were higher level
than we wanted to target) and API rules that were not rel-
evant to the systems under evaluation. While pmd, being
source-based, did not require a huge eﬀort to run, Jlintand
FindBugs required builds of multiple versions of our large
subject systems, which required six person-months of eﬀort
to compile all the systems, run the tools over the resulting
class ﬁles, and gather warnings.
Perhaps because of the need to map warnings from class
ﬁles to source ﬁles, we found that Jlintwarnings had erro-
neous line numbers. Upon inspection of a sizable randomly
chosen sample, we found too many warnings pointed into
Figure 1: The timeline shows a system version re-
lease date (dashed vertical line) with released versions
of all ﬁles in a dashed box. For every bug that is
ﬁxed, post-release, we use git blame to identify the
lines in the released version of the ﬁle; we then exam-
ine whether DPpredictions or SBFwarnings would have
indicated, at release time , that those lines should be in-
spected.
commented regions of code, speciﬁcally in the license disclo -
sure region in the ﬁrst several lines. Because of poor data
quality issues, and because our analysis is at the line-level ,
wediscarded Jlintfrom further consideration .
We gathered a wide range of process and product met-
rics and used them all for DP. As reported earlier [2] the
precise learning algorithm is not vital; for simplicity, we just
used logistic regression with this collection of metrics to pre-
dict defect occurrence likelihood. Since the objective here
is to maximize prediction performance, we were not con-
cerned with issues, such as multi-collinearity, and just used
all available process and product metrics at the prediction
problem. For process metrics, we used a large collection, in-
cluding number of committers, number of changes, number
of minor committers, experience of owners, etc.; for product
metrics, we used a large collection of complexity, size, and
object-oriented metrics. We have described and used this
set of metrics in earlier work [20, 19, 21].
To classify ﬁles as defective or clean, we use a logistic
regression classiﬁer with these metrics as its input. The
logistic regression classiﬁer uses both code metrics and pro-
cess metrics as gathered from the jiraissue tracking system.
To consider a ﬁle truly defective, there must be some bug-
ﬁxing commit that includes that ﬁle. We train our logistic
regression-based prediction models on the k-th release of a
project, and we test the model on any successive release.
Test Data: We study ﬁve open-source projects from the
ApacheSoftwareFoundation: Lucene, Derby, Wicket, Open-
JPA, and QpidTM. For each release of each project, we col-
lect the warnings reported by FindBugs ,pmd, andJlint.
All three tools produce warnings on the functionality of
code.Jlintis the only one not to report on style. However,
FindBugs andpmdwarn on style. For example, FindBugs
has a warning category speciﬁcally for style, and pmdwarns
on empty code. While pmdandJlintreport warnings
at a line-level granularity, FindBugs operates diﬀerently.
FindBugs reports warnings using a combination of line-
level, method-level, and class-level granularity. For this re a-
son,FindBugs reportssigniﬁcantlymorelines, butfarfewer
warnings, as compared to pmdandJlint.
Defect Data: For our test to scale, we need to map defects
to lines so we can then ask when SBFwarned lines or ﬁles428Table 1: Summary data on projects, ranges of values are shown in colum ns when applicable, from smallest to largest.
Extremal values may occur in diﬀerent releases for diﬀerent covariates.
Project Releases Files NCSLFindBugs
WarningsPMD
WarningsDefectsFindBugs PMD
Rec. Prec. Rec. Prec.
Lucene 7 0.5–1.4K 68–178K 137–300 12–31K 24–83 0.036 0.022 0 .17 0.015
Qpid 7 2.3–3.3K 212–342K 32–66 69–80K 74–127 0.00061 0.0017 0.10 0.0056
Wicket 5 2.1–2.7K 138–178K 45–86 23–30K 47–194 0.0070 0.017 0.10 0.0080
Derby 7 2–2.9K 420–630K 1527–1688 140–192K 89–147 0.067 0.004 3 0.17 0.0041
OpenJPA 8 1.2–4.7K 152–454K 51–340 62–171K 36–104 0.0034 0. 0038 0.26 0.0024
identiﬁedby DPintersectthoselines. Figure1showsourso-
lution to this problem. First, we identify (using jira) all the
big-ﬁxing commits that occurred subsequent to the release
date (note commit marked with X-ed out bug). The lines
that were changed in these commits are considered defective
lines. We use git blame to identify the provenance of the
defective lines. Now any blamed lines that were present in
the released system, shown in the Figure 1, could potentially
have been identiﬁed for inspection by SBFwarnings, or by
anDPprediction. Blamed lines that are actually identiﬁed
withSBFwarnings or DPpredictions are considered true
positives (or“hits”, to use IR terminology).
3.2 Solving the CBI Problem
In general, we measure the value of both methods on an
equal footing, using area under the cost-eﬀectiveness curve,
AUCEC[2]. Thismeasurementistheareaunderalift-chart,
x-axis being proportion of SLOC, and y-axis the proportion
of defects. This is a cost-sensitive and, unlike precision and
recall, non-parametric measure. For these reasons, it has be-
come quite popular recently. However, several complications
arise when applying these measures in our setting, which we
discuss below, along with our solution to the CBI problem.
Zonation: The fact that DPpredictions are ﬁle-granular,
whileSBFwarns at a line level makes their direct compar-
ison challenging. A fair comparison dictates that we com-
pare the performance of the two approaches on the same
number of lines. To solve this problem, we introduce some
terminology. Each warning a SBFtool emits warns a pos-
sibly empty set of lines. For each project, we sum all of the
unique, warned lines a SBFtool emits. This sum is our“in-
spection budget”for that project. To measure the“hit”rate
of the warned lines, we inspect the emitting tool’s native
priority order (high priority warnings in smaller ﬁles ﬁrst, in
line order within ﬁles) and calculate the AUCEC value. We
call this value AUCECL (AUCEC for warning Lines).
We then train logistic regression normally, using all avail-
able data but restrict the ﬁles for which we measure its suc-
cess at defect prediction to be only over a set of ﬁles whose
line sum is within a small error tolerance of the inspection
budget. We then ask: “What is logistic regression’s pay-oﬀ
given this inspection budget?”. Now the question becomes
choosing on which subset of a project’s ﬁles to“spend”this
inspection budget, based on DPpredictions. First, we order
all the ﬁles by defect likelihood (as predicted by DP), then
select as many ﬁles as we can given the inspection budget.
Most of the time, we waste some of the inspection budget
because it is insuﬃcient to allow us to “buy” the next ﬁle;
sometimes, this means we return no ﬁle. The AUCEC mea-
sure over this collection of ﬁles can then be compared with
the AUCECL above on an equal footing.
One could argue that this approach is unfair to DP;sometimes SBFtools warn on as little as 0.4% of the SLOC
inaproject; withsuchalimited line budget, acandidate DP
might only be allowed to recommend a handful of ﬁles. On
the other hand, one could argue that, when inspecting SBF
warnings, developers rarelylook atjustthe warned lines and
examine surrounding code, with the result that SBF’s bud-
get is unfairly low. We acknowledge these arguments, but
claim our procedure deﬁnes a reasonable baseline for a long-
overdue comparison; we hope that our dataset will enable
other types of future analysis of the CBI problem.
IMPORTANT AUCEC and AUCECL are the same mea-
sure, calculated the same way (proportional pay oﬀ in de-
fects for proportion of lines inspected). We use the diﬀerent
spellings to remind the reader of the zonation issue at play
here, betweentheﬁle-levelfor DPandtheline-levelfor SBF.
CBI Measurement: Defective code sometimes is a few
consecutive lines in a ﬁle; sometimes defective code is widel y
scattered. What we have access to in the process metadata
is the bug-ﬁxing commit and the lines changed to repair
the defect. Current best-practice in the mining community
is the celebrated “SZZ” [25] approach, which ﬂags the lines
changed in the bug-ﬁx commit as defective lines. We adopt
this approach, while acknowledging its imperfections, and
consider this code to be defective in our analysis.
Theoverlap, intersection, of defective code with either a
ﬁle ﬂagged by DP, or lines warned by SBFmay be partial or
complete. Consider a null-pointer warning by an SBFtool,
sayFindBugs . Assume there are mlines warned for this
null-pointer a ﬁle fin release r. Let’s assume that this same
ﬁlefafter the same release rhas a bug ﬁx, (that changes n
lines in ﬁle f) between release randr+1. Assume further
thatllines overlap between the mwarning lines and the n
defective lines. The CBI measurement problem arises here:
“How much credit should be given to FindBugs ?”
An optimistic view, the “optimistic credit” or Credit F
view, is that, if even a singlewarning line overlaps with a de-
fectivelineassociatedwithabugﬁx( i.e.thereisanonempty
intersection), there is a strong possibility that bug would
have been noticed during inspection and caught before re-
lease. This view suggests that with a single line of overlap,
the warning tool should be given full credit for the bug. A
less optimistic view, “scaled, or partial, credit”, ( Credit P)
is that, if x% of the defective lines associated with a bug
overlap a warning tool, then the warning tool gets credit
equivalent tox
100for that bug.
These measures are not perfect. For example, one can ar-
gue that Credit Fwill lower the AUCEC scores for DP, since
defective lines occur together in ﬁles; DPwould get credit
for only full defects, whereas SBF, which can give scattered
warnings at diﬀerent locations in ﬁles, has a greater chance
of“hitting”defective lines. One could also criticize Credit P
as lowering the scores for SBF: as when a single warning429Table 2: Recall between various methods of ﬁnding de-
fects. O/L refers to number of defects found in that
category that overlap defects found by logistic.
ProjectFindBugs PMD Logistic
Total O/L Total O/L FB PMD
Lucene 41 11 147 87 36 129
Qpid 5 0 218 113 5 167
Wicket 10 0 160 40 0 82
Derby 171 78 461 319 251 476
OpenJPA 8 0 321 264 14 383
on one line ( e.g., a failure to check a return value) generates
10 bug-ﬁx lines, thus giving the warning only 0 .1 credit for
that bug. This seems unfairly low, if the warning on that
one line was suﬃcient to incite the programmer to ﬁx that
bug. In this work, we compare DPandSBFusing both
the optimistic and scaled approaches, to give two diﬀerent
perspectives on their relative eﬀectiveness.
Surely, other measures could be deﬁned, with other at-
tendant compromises. In this work, we have highlighted the
importance of the CBI problem, explicated the diﬃculties
of devising an experiment to solve it, and present the re-
sults of our solution. We have spent a great deal of eﬀort
constructing the dataset; we hope that follow-on work will
leverage our dataset to more readily tackle CBI, bringing
new measures to bear.
4. RESULTS
We now present the results of our comparative study of
twoSBFtools,pmdandFindBugs , andDPbased on a
logistic model.
Table 1 summarizes our 5 projects. The number of re-
leasesvaryineachproject, fromalowof5forWicketto8for
OpenJPA. We generally discarded the last (current during
our analysis) release, because defect data is incomplete, and
subject to right-censorship. The systems are of moderate
size, ranging from 68K lines to 630K lines for Derby. The
ﬁle count varies from The defect counts per release range
from 24 (Lucene) to almost 200 (Wicket).
Once we gathered the warnings, we removed comment
lines from our partial- and full-credit calculations. The re-
maining lines that are warned in each release are shown Ta-
ble 1. The diﬀerence between FindBugs andpmdis in
some cases a couple of orders of magnitude. The sparsity
of warnings from FindBugs is particularly noteworthy in
Qpid and Wicket, and is in fact troublesome for our pur-
poses — with such a limited number of warned lines, our
approach of selecting ﬁles from DPto meet this budget is
severely constrained, and thus we might reasonably expect
very poor performance, as we shall see.
The recall and precision numbers are shown in the last
column. These numbers are cumulative over all releases; we
simply count the total number of defects and see how many
are indicated in the warning lines using Credit F. While the
recall numbers and precision numbers are very low, this is
not unexpected. In particular, note that the recall numbers
are as a proportion of actual ﬁeld defects: it is worthwhile to
avoid even a single ﬁeld defect, so ﬁnding any at all is a good
thing. The low precision numbers are more troublesome —
this indicates that the vast majority of the warnings (as
many as 99.5 in some cases) are perhaps false positives that
havenobearingondefectsreportedafterrelease. Aspointedout by Zhang & Cheung [29], however, sometimes avoinding
the high cost of defects might compensate for the cost of
inspecting a large number of false positives.
In Table 2, we show the defect counts actually found
byFindBugs (e.g., 41 total across all releases in Lucene)
and the defects found by Logistic regression DPusing the
number of lines warned by FindBugs (a total of 36) and
number of lines warned by PMD (total of 129). This ta-
ble also shows the overlap between FindBugs defects and
those found by logistic DP(11 of 41 defects) and the over-
lap between PMD-found defects and logistic DPdefects (86
defects). It’s noteworthy that the overlap with DPis gen-
erally higher for FindBugs than for PMD, thus suggesting
thatFindBugs and logistic DPare more complementary
thanpmdand logistic DP.
Certainly, DPalso produce a lot of false alarms, suggest-
ing ﬁles for inspection that have no defects. We compare
the two next.
4.1 Inspection Cost Comparison
Figure 2 compares the performance of the FindBugs tool
against performance of logistic DP, for all 5 of our test
projects, over all releases. Each plot point represents the
performance of a given approach, for a given project for a
given release. The lines are not really meaningful or indica-
tive of any real trends, and are just shown for clarity and
aesthetics. TheperformancehereismeasuredusingAUCEC
(for logic DP) and AUCECL (for FindBugs ). As explained
in Section 3.2, the measures are fair: they allow the two
approaches an equal inspection budget, and measure their
eﬀectiveness in capturing ﬁeld defects. We use the diﬀer-
ent names to emphasize that they arise from diﬀerent tool
granularities. This plot is made using full credit, Credit F
as discussed in Section 3.2: for defects that require mul-
tiple line changes, even a single line overlapping with an
inspected line or ﬁle is considered a “hit”. The random is
the AUCECL value (in expectation) of choosing lines uni-
formly at random, and assume that defects are uniformly at
random associated with the lines. In the case of Derby, DP
outperforms FindBugs , while in all the other, FindBugs
generally does better, except for a couple of early releases in
Lucene, release 5 in Qpid, and release 2 in OpenJPA. All in
all, in 21 (out of 29) releases across 5 projects, FindBugs
outperforms logistic DP. In the case of PMD, the situation
is almost reversed (Figure 3: logistic DPdominates uni-
formly in Qpid, Derby, and OpenJPA, in half the releases
in Lucene, and uniformly loses in Wicket. All in all, logistic
DPdominates in 19 out of 29 releases.
In the case of Partial Credit accounting (not shown for
reasons of space) for PMD, Logistic DPalmost uniformly
dominates PMD in 27 out of the 29 releases, except for the
ﬁrst and last release in Qpid. Turning to FindBugs for
Partial Credit accounting, logistic DPagain dominates for
Derby, and dominates FindBugs for two releases each in
OpenJPA and Lucene, and one release in Qpid, overall doing
better in only 11 releases out of 29.
For 3 of the projects, Wicket, Qpid, and OpenJPA, only
a very small portion of the system is selected for inspection
by the warning tools (and thus the same line count by DP)
so the performance of both is very low. In some cases a few
defects are “hit” by both tools, but there is no noticeable
consistent diﬀerence between the approaches. The core ob-
servationhereisthatthereisnosigniﬁcantdiscerniblediﬀer-430●
●
● ● ● ● ● ● ● ●●
● ● ● ●●
●
● ● ●●
● ●●●●
●●
●Lucene Wicket OpenJPA Qpid Derby
0.00000.00050.00100.00150.0020
0e+005e−051e−04
0e+001e−042e−043e−04
0e+001e−042e−043e−04
0.0050.0100.0150.020
2 3 4 5 6 7 2 3 4 5 2 4 6 8 2 3 4 5 6 7 2 3 4 5 6 7
ReleasesAUCEC/AUCECL●Logistic
FB
RandomFull Credit
Figure 2: Comparing SBF(FindBugs ) andDP(logistic regression) prediction using AUCEC as a performa nce measure
forDPand AUCECL for warnings. In general the performance is not sig niﬁcantly diﬀerent in our dataset. The two
measures are calculated in a commensurate manner, using Credit F(See Section 3.2).
●
●
●●
●
●●
●●●
●●●
●
●●●●
●●
●
●
●●●●
●●
●Lucene Wicket OpenJPA Qpid Derby
0.010.020.030.040.05
0.0100.0150.0200.0250.030
0.050.100.150.20
0.050.100.150.20
0.0250.0500.0750.1000.125
2 3 4 5 6 7 2 3 4 5 2 4 6 8 2 3 4 5 6 7 2 3 4 5 6 7
ReleasesAUCEC/AUCECL●Logistic
PMD
RandomFull Credit
Figure 3: Isomorphic to Figure 2, but for pmd.
ence in our dataset between the performance of FindBugs
and logistic DP
Figure 3 shows the same data as Figure 2, but for PMD,
again, we can observe that there is no discernible consistent
diﬀerence either way. The above plots are calculated using
the full credit accounting methods, Credit F. We have also
generated plots using partial credit method, Credit P; how-
ever, we see the same lack of a clear, consistent diﬀerence
between the two approaches.
This is a rather unexpected ﬁnding. As noted above, SBF
toolssuchas FindBugs andpmdoperateataline-levelgran-
ularity, and only require speciﬁc lines that are warned to
be inspected. Specially given partial credit accounting, one
might reasonably expect that DPtools would have the ben-
eﬁt, and thus show a clear advantage over SBF. Subject to
the warnings given in Section 5, this ﬁnding emphasizes the
importance of comparing these two important, disparate ap-
proaches to software inspecting targeting on an equal foot-
ing. It also suggests licensing regimes imposed by certain
very successful commercial vendors of SBFtools that inhibit
thepublicationofsuchevaluationresultsareanunjustiﬁable
barrier to the development of more eﬀective software inspec-
tion practices; certainly, Vendors such as GrammaTech, who
actively support such evaluations, are to be commended.
If this result holds up on replication, it’s rather cheer-
ing.DPtools work well with meta data, and do not require
build integration! WithDPtools, we do not need to get
SBFtools for each programming language, and integrate
with build procedures. The barriers to large-scale use of
SBFtools are documented in Bessey et al[4]; however SBF
tools that analyze byte codes do not require separate build
integration, andcanbeeasiertouse. Infact, manyorganiza-tions, becauseofprocessmaturityimperatives, haveengaged
in substantial metrics-gathering, and have good databases of
bugreportsandversioncontrolrepositories; insuchsettings,
DPcan be readily utilized, without need for expensive soft-
ware licenses or patent royalties: in fact, we just used the
open-source R system in our work.
4.2 Enhancing DPwithSBF
WenowturntoResearchQuestion2: cantheperformance
ofDPbe improved by using static analysis results? (See
Figure 4.) There is a simple rationale for this pursuit: Prior
work on DPpredictions has indicated that process metrics
work really well [19], and in general beat measure of prop-
erties of the product. Generally speaking product metrics,
which measure various properties of code such as coupling,
inheritance etc. are strongly correlated with size (the larger
a module, the more it is coupled, etc.), have been found by
others [6] and us [19]. In a sense, one can view static analy-
sis warnings as assessing a novel kind of property of source
code, perhaps one more strongly allied with defects.
AllSBFtools are intentionally designed to locate regions
of code that appear to have properties that prior experience
(or theory) directlyindicate the presence of defects. This is
diﬀerent than a property like coupling, which is thought to
cause defects indirectly , perhaps mediated by HCI phenom-
ena such as the diﬃculty of code comprehension. Indeed
prior work by Nagappan & Ball [16] suggests that there is a
direct, positive correlation between static analysis warning
density and defect density (although Marchenko & Abra-
hamsson [14] found a negative correlation in some cases). In
conclusion, motivated by prior research and experience, and
our available data, we sought to improve the performance of
DP, using metrics based static analysis warnings.
Our metrics are quite simple: we simply followed the suc-
cessful experience of [16], and added warning counts from
bothpmdandFindBugs as an additional metrics into lo-
gisticDP. Whether the correlation or earnings with de-
fects was positive, or negative (!) logistic regression would
discover the relationship, and train a model that can ex-
ploit this information. The results can be seen in Figure 4,
which uses the same general scheme as the earlier ﬁgures;
so it is compacted to save space. The y-axis shows the per-
formance measure using the AUCEC measure. The per-431●●
●●
●
●●●
●
●
●●●
●
●●
●
●
●●
●
●
●●●●
●●
●Lucene Wicket OpenJPA Qpid Derby
0.070.080.090.100.11
0.0570.0580.0590.0600.061
0.080.090.100.110.12
0.090.100.110.120.130.14
0.070.080.090.100.11
2 3 4 5 6 7 2 3 4 5 2 4 6 8 2 3 4 5 6 7 2 3 4 5 6 7
ReleasesAUCEC●Process
Process+WarningFull Credit
Figure 4: The eﬀect of including SBFwarnings as pre-
dictive metrics for DPfor20%NCSL inspection budget.
There appears to be no discernible trend. This ﬁgure is
isomorphic to Figure 2, hence its compact form here.
formance of “baseline” process-metrics based DP(logistic
regression in this case) are shown in red. The blue line
shows an “enhanced” DPexploiting a combination of the
traditional process metrics and the warnings counts from
pmdandFindBugs as an additional metric. The speciﬁc
plot here is computed using the Optimistic Credit account-
ing method, Credit F. However, accounting under scaled
credit gives similar results. The basic conclusion that can be
drawn here is that there is no clear evidence that warning
counts improve the performance of logistic DP. Sometimes
the enhanced DPperforms better, sometimes the baseline
performs better; this phenomenon does not change under
Credit Paccounting.
Other approaches to synergizing DPandSBFmay pro-
vide better performance. For example traditional product
metrics thus far have only taken limited advantage of se-
mantic properties of code, such as control ﬂow (McCabe
metrics) or data ﬂow (certain cohesion metrics). We be-
lieve that more complex properties, such as the cardinality
of points-to sets or a count of the number of times widening
was applied during an abstract interpretation, that relate
semantic properties to potential diﬃculties in human com-
prehension or maintainability might yield better results.
4.3 Enhancing SBFwithDP
Our ﬁnal research question concerns whether DPcan im-
prove the performance of SBFtools. The intuition here
arises from the abundant evidence that human/process fac-
tors (such as ownership, organizational structure, and geo-
graphical distribution [15, 17, 22]) inﬂuence quality. Thus, i t
is entirely possible that static bug ﬁnders could beneﬁt from
paying attention to such factors when prioritizing warnings
for developers. As an example, warnings on code recently
produced by an inexperienced programmer who is changing
code that he is unfamiliar with are probably likely to be as-
sociated with defects. By contrast, warnings produced on
mature code written by one of the initial creators of a sys-
tem, that has remained unchanged for several releases, are
unlikely to be of any great concern. These phenomena are
very well accounted for in the typical process metrics used
in theDPcommunity in recent years to predict defective
ﬁles, with very good reported results. There is a zonation
problem here: DPprioritizes ﬁlesfor inspection; how is to
be used for prioritizing line-level warnings?
Our approach to enhancing SBFwithDP, wasto or-
der the warnings by the DP-predicted probability of the ﬁles
within which the warned lines occur . This gives us a partic-
ular ordering of the warned lines. Using both our optimisticand scaled accounting, we can scan these lines in this or-
der, assigning credit for any defective lines that are encoun-
tered. This gives us an AUCECL score for this enhanced
SBFwarning order. Now what should we compare this
with? Clearly, one good candidate the the AUCECL score
of a standard, priority-based ordering that is produced by
FindBugs , that we use in the ﬁrst research question above.
However, there are many possible based orderings of defects;
in fact, programmers may have diﬀerent preferred orders on
how they choose to inspect the warnings. It would be in-
teresting to get a robust estimate of how well our ordering
enhanced by DP, and the“native”ordering produced by the
built-in prioritization of the tool compares with the entire
population of such orderings.
For this purpose, we generate 100 random re-orderings of
the warnings produced by pmdandFindBugs , and for each
one, we calculate the AUCECL. This gives an empirical dis-
tribution over a large sample of orderings, which then allows
us to estimate how well the DP-enhanced ordering compares
with the overall population of orderings. Figure 5 shows the
results under partial credit accounting. There are separate
panels for each projects, and each is broken up by release.
For each release, we show a box plot of the AUCECL val-
ues for 100 random orderings, a blue triangle for the native
priority warning, and a red dot for the logistic (p-val, or
predicted probability of defects value) based ordering. The
lines are shown for visual clarity, and have no semantics.
Upper row is FindBugs , and lower row is pmd. Under this
accounting, (after correcting the p-values for false discov-
ery using Benjami-Hochberg procedure) the logistic (p-val)
ordering dominates ( p <0.01) in 25 of the 29 releases for
PMD (over all diﬀerent projects), and 19 of the 29 releases
forFindBugs . It also dominates the native priority order-
ing most cases. In fact, logistic-based ordering dominates in
virtually every release of virtually every project except for
Wicket, just because wicket has very few warning lines, thus
constraining the line-budget for DP. Thus yielding a simple
lesson: under partial accounting, best to order the warnings
using logistic DPpredicted probability of defects!
Under full accounting (which we omit for space reasons),
p-val based ordering doesn’t fare well at all. It signiﬁcantly
beats the random orderings ( p <0.01) after correction only
in 5 releases (2 in OpenJPA and 4 in Qpid). As explained
earlier, thisisnotsurprising—whenbugsspanmultiplelines,
all lines tend to occur together in a ﬁle, and scattershot SBF
warnings that happen to hit even a single one of those lines
will get full credit for that bug; whereas DPwill select full
ﬁles and get only a single credit for a multi-line bug only
after the entire set of lines for that ﬁle is accounted for.
Finally, there is a very interesting observation: the logis-
tic based ordering frequently outperforms the native prior -
ity tool-based ordering, across project and release, for bo th
tools. In the case of pmdonly for 2 out of the 29 releases
under Full Credit (1 for the partial) the native ordering is
better than logistic ordering; for FindBugs , the native or-
dering dominates only 8 cases under partial credit (7 full). A
two-sample Wilcoxon test rejects the null hypothesis (with
alternative hypothesis set to logistic > native ) after correc-
tion (p <0.01) in all cases, except for partial credit under
FindBugs (p= 0.02). This suggests that one could prefer
logistic-based ordering to the native ordering. However, one
should interpret the p-values prudently; an abundance of
caution suggests that releases are not necessarily indepen-432Derby
● ● ● ●● ● ● ●●
●●
●●●
● ● ● ●● ● ● ●
● ● ● ●● ● ● ●● ● ● ●● ● ● ●●●●
●
●●0.0020.0040.006
0.0150.0200.0250.030FB PMD
2 4 6Lucene
● ● ● ●
● ● ● ●
● ● ● ●●
●●●
●●
● ● ● ●
● ● ● ●● ● ● ●
● ● ● ●● ● ● ●
● ● ● ●●●
●●
●
●0.000250.000500.00075
0.0050.0100.0150.020FB PMD
2 4 6OpenJPA
●●●
●●●
●
● ● ● ●● ● ● ●●
●
●●
●●●0e+001e−052e−053e−054e−05
0.020.030.040.050.06FB PMD
2 4 6 8Qpid
● ● ● ●
●●
●
●●
●
● ● ● ●● ● ● ●
● ● ● ●●●
●
●●
●0e+002e−064e−066e−068e−06
0.0050.0100.0150.0200.025FB PMD
2 4 6Wicket
● ● ● ●
●●●
●
● ● ● ●● ● ● ●● ● ● ●
●●
●●0e+001e−052e−053e−054e−055e−05
0.0040.0060.0080.010FB PMD
2 4Partial Credit
ReleaseAUCECL●Pval
Priority
Figure 5: The eﬀect of ordering SBFwarnings using DPpredictions, compared to 100 random orderings of SBF
warnings. The boxplot shows the 100 orderings. The red line ( with circles) is warnings ordered by logistic DP
prediction of defect probability (p-val) and the blue trian gles are the native priority ordering. Partial credit scoring is
used.
dent samples.
5. THREATS TO V ALIDITY
If developers were in fact using SBFtools during de-
velopment, then it is possible that warnings (and perhaps
associated bugs) were ﬁxed before release, and thus fewer
post-release defects would be associated with warnings. In
Lucene and Wicket, we found no evidence of systematic
SBFuse. In the case of Derby, a developer list message re-
ported that contributors from Oracle may have been using
FindBugs. In the case of Qpid, we found that a FindBugs
task was added Sept 17, 2010, and in OpenJPA, on Jun 14,
2010. However, when we examined the history of warning
counts in Derby, Wicket, and OpenJPA, we found no sig-
niﬁcant evidence of warning repair; nor did we ﬁnd any sig-
niﬁcant preferential reduction in high-priority warnings, or
ones more ominous-sounding ( e.g.,MALICIOUS CODE or
CORRECTNESS being). In addition, we found no evidence
in the email archives of any these projects suggesting a sys-
tematic adoption of SBFtools. These observations mitigate
this particular threat to our ﬁndings.
Our work may not be generalizable . We have chosen 5
distinct projects of diﬀerent sizes and application domains.
We However, all are Java-based. Our two tools, pmdand
FindBugs are also therefore Java based. Both are heuristic
bug-ﬁnders, although FindBugs does employ some static
analysis. FindBugs requires compiled Java code, and thus
entailed tremendous eﬀort, to compile our 5 systems; older
versions presented special challenges, such as ﬁnding older
Java SDK versions. pmdwas relatively easier, as it works on
source code. Ideally, this experiment should be repeated for
more projects in more diﬀerent languages, with other tools.
In this case, since we making the defect data publicly avail-
able, itmayhelpotherresearcherstryotherJava-based SBF
tools. Although we use one DPmethod (logistic regression)
using primarily process metrics, prior reports [1, 19] sug-
gest that this approach a) would be diﬃcult to beat and
b) easy to repeat in a new setting. Our measures may be
mis-targeted . As explained, we report performance under
the full credit and partial credit assumptions. As discussedin Section 3.2, there are arguments against these measures,
and others could be deﬁned. In addition, AUCEC per sehas
beencriticizedforignoringthecostoffalsenegatives(missed
defects [29]). We hope that our making this hard-won warn-
ings & defect data available will encourage repetition of this
trial with other measures.
6. CONCLUSION AND FUTURE WORK
Defect prediction and bug ﬁnders target the same prob-
lem: selecting a subset of source code on which to expend
limited quality control budgets. We are the ﬁrst to compare
their performance. We address the comparability of buggi-
ness identiﬁcation problem, whose tackling cuts to the core
of software engineering: knowing when one outperforms the
other optimizes resource allocation and promises to guide
the search for useful synergies in service of software engi-
neering’s core aim to produce better cheaper software faster.
Our comparison is based on the AUCEC cost-beneﬁt met-
ric. We ﬁnd that statistical defect prediction appears to do
better than pmd, a widely used tool, under both partial and
full credit accounting in most cases. However, DPdoes not
fare as well against FindBugs , generally doing worse until
full credit accounting, and not as badly under partial credit
accounting. Second, we ﬁnd that using SBFwarnings as an
additional metric does not signiﬁcantly improve statistical
prediction. Last but not least, we ﬁnd that ordering SBF
warnings based on DPappears to improve upon the native
SBFpriority levels in a majority of cases. While this re-
sult appears signiﬁcant on a two-sample statistical test, we
urge caution, since releases are not necessarily independent
samples. Comparisons such as these are key to promoting
engineering discipline in the selection of quality control te ch-
niques, and we invite others use our dataset for further ex-
periments. This material is based upon work supported by
the National Science Fondation under Grant No. 0974703.
References
[1] E. Arisholm, L. C. Briand, and M. Fuglerud. Data
mining techniques for building fault-proneness models433in telecom java software. In ISSRE, pages 215–224.
IEEE Computer Society, 2007.
[2] E. Arisholm, L. C. Briand, and E. B. Johannessen. A
systematic and comprehensive investigation of meth-
ods to build and evaluate fault prediction models. JSS,
83(1):2–17, 2010.
[3] N. Ayewah, D. Hovemeyer, J. D. Morgenthaler,
J. Penix, and W. Pugh. Using static analysis to ﬁnd
bugs.IEEE Software , 25(5):22–29, 2008.
[4] A. Bessey, K. Block, B. Chelf, A. Chou, B. Fulton,
S. Hallem, C. Henri-Gros, A. Kamsky, S. McPeak, and
D. Engler. A few billion lines of code later: using static
analysis to ﬁnd bugs in the real world. Communications
of the ACM , 53(2):66–75, 2010.
[5] M. D’Ambros, M. Lanza, and R. Robbes. Evaluating
defect prediction approaches: a benchmark and an ex-
tensive comparison. Empirical Software Engineering ,
17(4-5):531–577, 2012.
[6] K. El Emam, S. Benlarbi, N. Goel, and S. Rai. The
confounding eﬀect of class size on the validity of object-
oriented metrics. TSE, 27(7):630–650, 2001.
[7] D. Engler and K. Ashcraft. Racerx: eﬀective, static
detection of race conditions and deadlocks. In SOSP,
volume 37, pages 237–252. ACM, 2003.
[8] C. Flanagan, K. R. M. Leino, M. Lillibridge, G. Nelson,
J. B. Saxe, and R. Stata. Extended static checking for
java. In ACM Sigplan Notices , volume 37, pages 234–
245. ACM, 2002.
[9] N. Jovanovic, C. Kruegel, and E. Kirda. Pixy: A static
analysis tool for detecting web application vulnerabili-
ties. InSP, pages 6–pp. IEEE, 2006.
[10] S. Kim and M. D. Ernst. Which warnings should i ﬁx
ﬁrst? In FSE, pages 45–54. ACM, 2007.
[11] D. Larochelle and D. Evans. Statically detecting likely
buﬀer overﬂow vulnerabilities. In USENIX Security
Symposium , pages 177–190. Washington DC, 2001.
[12] S. Lessmann, B. Baesens, C. Mues, and S. Pietsch.
Benchmarking classiﬁcation models for software defect
prediction: A proposed framework and novel ﬁndings.
TSE, 34(4):485–496, July 2008.
[13] C. Lewis, Z. Lin, C. Sadowski, X. Zhu, R. Ou, and E. J.
Whitehead Jr. Does bug prediction support human de-
velopers? ﬁndings from a google case study. In ICSE,
pages 372–381. IEEE Press, 2013.
[14] A. Marchenko and P. Abrahamsson. Predicting soft-
ware defect density: a case study on automated static
code analysis. In Agile Processes in Software En-
gineering and Extreme Programming , pages 137–140.
Springer, 2007.
[15] A. Meneely and L. A. Williams. Secure open source
collaboration: anempiricalstudyoflinus’law. InE.Al-
Shaer, S.Jha, andA.D.Keromytis, editors, CCS,pages453–462. ACM, 2009.
[16] N. Nagappan and T. Ball. Static analysis tools as early
indicators of pre-release defect density. In ICSE, pages
580–586. ACM, 2005.
[17] N. Nagappan, B. Murphy, and V. Basili. The inﬂu-
ence of organizational structure on software quality: an
empirical case study. In ICSE, pages 521–530. ACM,
2008.
[18] M. G. Nanda, M. Gupta, S. Sinha, S. Chandra,
D. Schmidt, and P. Balachandran. Making defect-
ﬁnding tools work for you. In ICSE, pages 99–108.
ACM, 2010.
[19] F. Rahman and P. Devanbu. How, and why, process
metrics are better. In ICSE, pages 432–441. IEEE
Press, 2013.
[20] F. Rahman, D. Posnett, and P. Devanbu. Recalling the
“imprecision”of cross-project defect prediction. In FSE.
ACM, 2012.
[21] F. Rahman, D. Posnett, I. Herraiz, and P. Devanbu.
Sample size vs. bias in defect prediction. In FSE, 2013.
[22] N. Ramasubbu, M. Cataldo, R. K. Balan, and J. D.
Herbsleb. Conﬁguring global software teams: a multi-
company analysis of project productivity, quality, and
proﬁts. In ICSE, pages 261–270. ACM, 2011.
[23] N. Rutar, C. B. Almazan, and J. S. Foster. A com-
parison of bug ﬁnding tools for java. In ISSRE, pages
245–256. IEEE, 2004.
[24] A. September. IEEE standard glossary of software en-
gineering terminology, 1990.
[25] J.´Sliwerski, T. Zimmermann, and A. Zeller. When do
changes induce ﬁxes? In ACM sigsoft software engi-
neering notes , volume 30, pages 1–5. ACM, 2005.
[26] F. Thung, D. Lo, L. Jiang, F. Rahman, P. T. Devanbu,
et al. To what extent could we detect ﬁeld defects? an
empirical study of false negatives in static bug ﬁnding
tools. In ASE, pages 50–59. ACM, 2012.
[27] S. Wagner, J. J ¨urjens, C. Koller, and P. Trischberger.
Comparing bug ﬁnding tools with reviews and tests.
InTesting of Communicating Systems , pages 40–55.
Springer, 2005.
[28] G. Wassermann and Z. Su. Sound and precise analy-
sis of web applications for injection vulnerabilities. In
ACM Sigplan Notices , volume 42, pages 32–41. ACM,
2007.
[29] H. Zhang and S. Cheung. A cost-eﬀectiveness criterion
for applying software defect prediction models. In FSE,
pages 643–646. ACM, 2013.
[30] J. Zheng, L. Williams, N. Nagappan, W. Snipes, J. P.
Hudepohl, and M. A. Vouk. On the value of static
analysis for fault detection in software. TSE, 32(4):240–
253, 2006.434