Using Unfoldings in Automated Testing of Multithreaded
Programs
Kari Kähkönen, Olli Saarikivi, Keijo Heljanko
Department of Information and Computer Science, School of S cience, Aalto University
PO Box 15400, FI-00076 AALTO, Finland
{kari.kahkonen, olli.saarikivi, keijo.heljanko}@aalto .ﬁ
ABSTRACT
In multithreaded programs both environment input data
and the nondeterministic interleavings of concurrent even ts
can aﬀect the behavior of the program. One approach to
systematically explore the nondeterminism caused by in-
put data is dynamic symbolic execution. For testing mul-
tithreaded programs we present a new approach that com-
bines dynamic symbolic execution with unfoldings, a method
originally developed for Petri nets but also applied to many
other models of concurrency. We provide an experimental
comparison of our new approach with existing algorithms
combining dynamic symbolic execution and partial-order re -
ductions and show that the new algorithm can explore the
reachable control states of each thread with a signiﬁcantly
smaller number of test runs. In some cases the reduction
to the number of test runs can be even exponential allow-
ing programs with long test executions or hard-to-solve con -
strains generated by symbolic execution to be tested more
eﬃciently.
Categories andSubject Descriptors
D.2.4 [ Software Engineering ]: Software/Program Veriﬁ-
cation; D.2.5 [ Testing and Debugging ]: Symbolic execu-
tion
General Terms
Veriﬁcation, Algorithms, Reliability
Keywords
Dynamic symbolic execution, unfoldings
1. INTRODUCTION
Designing correct multithreaded programs is a very chal-
lenging task, mostly due to the large number of ways how
diﬀerent threads can interleave their execution. For exam-
ple, if there are nindependent operations being executed
Permission to make digital or hard copies of all or part of thi s work for
personal or classroom use is granted without fee provided th at copies are
not made or distributed for proﬁt or commercial advantage an d that copies
bear this notice and the full citation on the ﬁrst page. To cop y otherwise, to
republish, to post on servers or to redistribute to lists, re quires prior speciﬁc
permission and/or a fee.
ASE ’12, September 3-7, 2012, Essen, Germany
Copyright 2012 ACM 978-1-4503-1204-2/12/09 ...$10.00.concurrently, there are n! possible interleavings. Diﬀerent
interleavings can lead to possibly diﬀerent states in the pr o-
gram and therefore a programmer needs to make sure that
no interleaving leads to an erroneous state. It is, however,
easy for the programmer to erroneously miss some of the
possible interleavings.
It is easy to generate an execution tree that represents
all the possible interleavings of a program. This execution
tree can be ﬁnite or inﬁnite depending whether there is a
cycle in the state space of the program. A large number
of the interleavings can be irrelevant for checking propert ies
like the reachability of a control state in the program. This
is because quite often some of the operations executed by a
thread are independent with operations in other threads and
therefore the set of possible interleavings can be partitio ned
into equivalence classes that are often called Mazurkiewic z
traces [3]. A sequence of executed operations is one lineari za-
tion of a trace and the rest of the linearizations belonging t o
the same Mazurkiewicz trace can be obtained by swapping
adjacent independent operations in the sequence.
As the number of possible interleavings can grow very
quickly, ways to ﬁght the state explosion caused by this
are needed. Partial order reduction methods (e.g., persis-
tent sets [8]) can be seen as reducing the execution tree
representation containing all possible interleavings by i gnor-
ing provably irrelevant parts of the tree such that at least
one representative from each Mazurkiewicz trace equivalen ce
class gets explored. An alternative way to ﬁght state ex-
plosion is to use a “compression approach” by constructing
a symbolic representation of the possible interleavings th at
is more compact than the full execution tree. Unfoldings,
ﬁrst introduced in the context of veriﬁcation algorithms by
McMillan [12], is an example of such a representation (see [4 ]
for an extensive survey on the topic).
In this work we present an unfolding approach to con-
struct a compact representation of the interleavings of mul -
tithreaded programs. This allows our new testing algorithm
to avoid irrelevant interleavings. In particular, we can so me-
times cover all of the local control states of the system us-
ing less test runs than there are Mazurkiewicz traces of the
system. We will also show how this approach can be com-
bined with dynamic symbolic execution (DSE) to test mul-
tithreaded programs that use input values as part of the
execution.
Symbolic execution of sequential programs can also be
seen as a compression approach. It is easy to consider all
possible combinations of input values, however, this becom es
quickly infeasible because just two 32-bit integer input va l-Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
ASE’12, September 3–7, 2012, Essen, Germany
Copyright 2012 ACM 978-1-4503-1204-2/12/09 ...$15.00
150
ues would generate (232)2possible combinations. Symbolic
execution constructs a symbolic representation that parti -
tions the input values to equivalence classes such that all
the possible input values are still presented by the symboli c
structure. Given this similarity, we feel that the unfoldin g
approach integrates naturally to DSE in an intuitive way.
It is also possible to combine partial order reduction meth-
ods with DSE. Persistent set based dynamic partial order
reduction [7] and race detection and ﬂipping [16] are exam-
ples of algorithms presented recently that are suitable for
this. In this work we compare our approach with these algo-
rithms and discuss the advantages and disadvantages of us-
ing a compression approach over reduction methods in test-
ing multithreaded programs. Especially, we show that our
algorithm allows better reduction in the number of needed
test runs than the current dynamic partial order reduction
based approaches. The main contributions are the following :
(i) a new algorithm combining dynamic symbolic execution
and unfolding methods that allows multithreaded programs
containing input values to be tested, (ii) an approach to un-
fold a multithreaded program using only the information col -
lected while executing the program, and (iii) a comparison o f
the new approach with dynamic partial-order reduction and
race detection and ﬂipping that are similar to algorithms
based on partial-order reductions.
The rest of this paper is organized as follows. Section 2
provides the necessary background deﬁnitions and concepts
used in this work. Section 3 presents the new approach
to explore multithreaded programs. A comparison of this
approach with related approaches is given in Section 4. An
experimental evaluation of the new algorithm is given in
Section 5 and Section 6 concludes the paper.
2. BACKGROUND
This section introduces background technical deﬁnitions
and concepts needed for understanding the new algorithm.
2.1 BasicDeﬁnitions
2.1.1 PetriNets
Anetis a triple ( P, T, F ), where PandTare disjoint sets
of places and transitions, respectively, and F⊆(P×T)∪
(T×P) is a ﬂow relation. Places and transitions are called
nodes and elements of Fare called arcs. The preset of a
node x, denoted by•x, is the set {y∈P∪T|F(y, x) = 1}.
The postset of a node x, denoted by x•, is the set {y∈P∪
T|F(x, y) = 1}. A marking of a net is a mapping P/mapsto→N.
A marking Mis identiﬁed with the multiset which contains
M(p) copies of p. Graphically markings are represented by
putting tokens on circles that represent the places of a net.
A Petri net is a tuple Σ = ( P, T, F, M 0), where ( P, T, F ) is
a net and M0is an initial marking of ( P, T, F ).
2.1.2 Causality,ConﬂictandConcurrency
Causality, conﬂict and concurrency between two nodes x
andyin a net are deﬁned as follows:
•Nodes xandyare in causal relation, denoted as x < y ,
if there is a non-empty path of arcs from xtoy. In
this case we say that xcausally precedes y.
•Node xis in conﬂict with y, denoted as x#y, if there
is a place zdiﬀerent form xandysuch that z < x ,UnfoldNet (Σ: a Petri net)
1:Unf := empty net
2:pe:=PossibleExtensions (Unf, Σ)
3:while pe/\e}atio\slash=∅do
4: add an event e∈peand its output conditions to Unf
5: pe:=PossibleExtensions (Unf, Σ)
6:end while
Figure 1: Basic unfolding algorithm
z < y and the paths from ztoxand from ztoytake
diﬀerent arcs out of z.
•Node xis concurrent with y, denoted as xcoy, if nodes
are neither causally related ( x < y ory < x ) nor in
conﬂict.
Any two nodes xandy, such that x/ne}ationslash=yare either causally
related, are in conﬂict or are concurrent. A co-set is a set
of nodes where all nodes in the set are pairwise concurrent.
2.1.3 OccurrenceNets andBranchingProcesses
A directed graph can be unwinded into a (possibly inﬁ-
nite) tree when starting from a root node such that each
node in the tree is labeled by a corresponding node in the
graph. For nondeterministic sequential programs this un-
winding would be the computation tree of the program,
where the computations maximally share their preﬁxes. In
a similar way, Petri nets can be unfolded into labeled occur-
rence nets . These nets have a simple DAG-like structure. A
multithreaded program can be unwinded to a computation
tree but it can also be represented as an unfolding that al-
lows the preﬁxes of computations to be shared even more
succinctly. Intuitively these unfoldings represent both t he
causality of events as well as the conﬂicts between them.
Formally, an occurrence net Ois net ( B, E, G ) such that O
is acyclic; for every binB,|•b| ≤1; for every x∈B∪Ethere
is a ﬁnite number of nodes y∈B∪Esuch that y < x ; and
no node is in conﬂict with itself. To avoid confusion when
talking about Petri nets and their occurrence nets, the node s
BandEare called conditions andevents , respectively.
Labeled occurrence net (also called branching process ) is
a tuple ( O, l) = ( B, E, G, l ) where l:B∪E/mapsto→P∪Tis a
labeling function such that (adapted from [10]):
•l(B)∈Pandl(E)∈P;
•for all e∈E, the restriction of lto•eis a bijection
between e•andl(e)•;
•the restriction of ltoMin(O) is a bijection between
Min(O) and M0, where Min(O) denotes the set of min-
imal elements with respect to the causal relation;
•for all e, f∈E, if•e=•fandl(e) =l(f) then e=f.
It is possible to obtain diﬀerent branching processes by
stopping the unfolding process at diﬀerent times. The max-
imal branching process (possibly inﬁnite) is called the un-
folding of a Petri net.
2.2 Constructing Unfoldings
Here we ﬁrst consider a simple algorithm for unfolding
Petri nets, and later adopt it for testing multithreaded pro -
grams. The approach we take is based on truncating an151Global variables: Thread 1: Thread 2:
int x = 0; local int a = x; x = 5;
2Thread 1 Thread 2
3 1 4R
R W WThread 1 Thread 2x1,1x2,1
N0
Thread 1 Thread 2x1,1x2,1
N1
1
x1,2
x2,2
Thread 1 Thread 2x1,1x2,1
N2
1
x1,2
x2,2
N42
x1,3
x
1,1x
2,1
x
2,2x2,3
x1,2
x1,3x1,4
x
1,5N3... (Omitted for brevity)
Figure 2: Unfolding of the simple example
inﬁnite unfolding, similar to dynamic symbolic execution
approaches that truncate the inﬁnite computation tree to
guarantee termination. Using advanced techniques such as
cut-oﬀ events, see [4], is non-trivial and left for further s tudy.
Algorithm in Fig. 1 shows the basic approach to generate
unfoldings of Petri nets. The algorithm starts with an empty
net and in each iteration computes a set of events that can
be added to the current copy of the unfolding and extends
the unfolding with an event from this set. As explained
in [5], to improve the performance of the algorithm it is not
necessary to compute the the whole set of possible extension
in each iteration but instead update the set with events that
are enabled by the last event added to the unfolding. Com-
puting the possible extensions is computationally the most
expensive part of unfolding algorithms and in fact a decisio n
version of the problem can be shown to be NP-complete, for
a more detailed analysis, see [4].
As a ﬁrst example of unfolding multithreaded programs,
consider the simple program in Fig. 2 with two threads where
the ﬁrst thread simply reads a shared variable and the sec-
ond thread writes to the shared variable. There are two pos-
sible ways to execute this program: either the read operatio nis executed before the write or the other way around. To un-
fold this program, we start with an initial set of conditions
that contains a condition for the initial program counter va l-
ues for each thread and a copy for the shared variable xfor
each thread. This is shown in Fig. 2 as the net N0. From
this initial state the net can be extended by either adding
an event corresponding to the read or the write operation.
N1is the resulting net after adding the write event. Note
that the write accesses a copy of the variable xof all threads.
For this new net, there are again two possible ways to extend
it. Either by a read event from the initial state or by a read
event that reads the value written by the write event already
added to the net. In N2the latter read is added to the net.
Finally, the net N4is the resulting unfolding of the program
after all events have been added. The construction of such
unfoldings follows the general idea of the algorithm in Fig. 1
and is formalized in Section 3. Also note that the only race
in the program is the race in the initial state between write
in event 1 and read in event 3. This can be seen in the ﬁnal
unfolding where the condition x1,1has two outgoing edges
such that the following events belong to diﬀerent threads.
2.3 DynamicSymbolicExecution
Dynamic symbolic execution [9, 14, 1] (also known as con-
colic testing) is a method where a given program is executed
both concretely and symbolically at the same time in order
to explore the diﬀerent behaviors of the program. The main
idea behind this approach is to, at runtime, collect symboli c
constraints at each branch point that specify the input val-
ues causing the program to take a speciﬁc branch. As an
example, a program x = x + 1; if (x > 0); would gen-
erate constraints input1+ 1>0 and input1+ 1≤0 at the
if-statement given that the symbolic value input1is assigned
initially to x.
A path constraint is a conjunction of the symbolic con-
straints corresponding to each branch point in a given exe-
cution. All input values that satisfy a path constraint will
explore the same execution path for sequential programs.
For multithreaded programs the schedule aﬀects the execu-
tion path as well. The nondeterminism caused by the thread
interleavings can be handled in dynamic symbolic execution
by taking control of the scheduler and considering the sched -
ule as an input to the system. For more details, see e.g., [14] .
2.4 Representing Multi-threaded Programs
To formalize our algorithm, we use a simple language to
describe the programs that can be tested. To keep the pre-
sentation simple, this language does not not contain dy-
namic thread creation or dynamically varying number of
shared variables, instead these are ﬁxed at program start
time. Handling these features in the context of Java pro-
grams is discussed in Section 3.4. The syntax of the language
for describing threads in the programs is shown in Fig. 3 and
can be seen as a subset of programming languages such as
C, C++ or Java.
We assume that the only nondeterminism in threads is
caused by concurrent access of shared variables or by input
data from the environment. We also assume that the mem-
ory model is sequentially consistent. Operations that acce ss
shared variables are called visible operations as they can b e
used to share information between the threads. The state of
a multithreaded program consists of the local state of each
of the threads and the shared state consisting of the shared152T ::= Stmt* (thread)
Stmt ::= l: S (labeled stmt.)
S ::= lv := e |sv := lv | (statement)
ifb goto l′|lock (sv)|
unlock (sv)
e ::= lv |sv|c|lv op lv |input (expression)
b ::= true |false | (boolean expr.)
lv = lv |lv/\e}atio\slash= lv |
lv<lv|lv>lv|lv≥lv|
lv≤lv
op∈ {+, -, *, /, %, ... },
lv is a local variable,
sv is a shared variable and
c is a constant
Figure 3: Simpliﬁed language syntax
...
...read global variable write global variable
acquire lock l release lock lsymbolic branching
true falseX1,1
X1,2X1,1
X1,2Xn,1
Xn,2
lxlypckpc
ipcipci
pcipci
pcjpcjpcjpcjpcj
Figure 4: Modeling execution events
variables. The visible operations considered in this work a re
read and write of shared variables and acquire and release of
locks. We assume that a read operation reads a value from
a shared variable and assigns it to a variable in the local
state of the thread performing the operation. Write assigns
either a constant or a value from a local variable to a shared
variable. Non-visible operations, such as if-statements, are
evaluated solely on the values in the local state of the exe-
cuting thread and therefore cannot access shared variables
directly. In real programs the statements can be modiﬁed
automatically to satisfy these assumptions.
3. COMBININGDYNAMICSYMBOLICEX-
ECUTION ANDUNFOLDINGS
In this section we describe how a multithreaded program
can be tested using DSE and unfolding techniques without
constructing an explicit model of the program ( e.g., a very
large high-level Petri net) that would be unfolded. Instead ,
the approach constructs an unfolding of the program execu-
tions based solely on the information collected dynamicall y
during test runs.
To build an unfolding to capture the control and data
ﬂow of a multithreaded program, we model the visible op-
erations executed during testing with nets shown in Fig. 4.
These constructs are the ones employed in [6]. When testing
a program with nthreads, the unfolding is initially set to
contain one condition for each thread and nconditions for
each shared variable. The conditions for shared variablesUnfoldProgram (P: program)
1:Unf := initial unfolding
2:pe:= set of events enabled in the initial state of P
3:while pe/\e}atio\slash=∅do
4: choose target ∈pe
5: event sequence :=Execute (P,target ,k)
6: for all e∈event sequence do
7: ife /∈Unfthen
8: addeand its output conditions to Unf
9: pe:=pe\ {e}
10: UpdatePotExt (pe,Unf,e)
11: end if
12: end for
13:end while
Figure 5: Main loop of the new testing algorithm
can be seen as local copies of the shared variable for each
thread (labeled xn,ifor thread nin Fig. 4). Having local
copies of shared variables allows, for example, two reads of
a same variable by diﬀerent threads to be independent by
construction. Each event added to the unfolding has one
condition that corresponds to a control location of a thread
at preset and another such condition in the postset. Sim-
ilarly, events corresponding to visible operations have co n-
ditions for shared variables in their pre- and postsets. For
example, reading a shared variable accesses the local copy o f
the variable of the reading thread, whereas writing accesse s
the copies of all threads.
The conditions in Fig. 4 can be divided into three cate-
gories. One set of conditions represent the control locatio n
(or a program counter value) of a thread, the second set
represents shared variables and the third represents locks .
To simplify the discussion, we refer to these types of con-
ditions as thread conditions, shared variable conditions a nd
lock conditions, respectively. As any program has a unique
next operation for each program counter value, it is easy
to see that for each thread condition all the events in the
postset of the condition must be of the same type (e.g., if a
thread condition has a read event in the post set, all other
events in the post set must be read events as well).
When an event is added to the unfolding, we need to com-
pute its possible extensions. By considering the combina-
tion of each thread condition and its following event type
as a result of unfolding a transition in a Petri net, we could
use existing standard Petri net possible extension algorit hms
in the construction of the unfolding. However, these algo-
rithms are designed for arbitrary Petri nets in mind and are
computationally expensive. In our case the structure of the
unfolding we want to generate is quite restricted and we will
show in the following that these restrictions allow the pos-
sible extensions to be computed more eﬃciently in practice.
3.1 The Algorithm
The new algorithm, shown in Fig. 5, starts by adding
events that are enabled in the initial state of the program
to a set of possible extensions (denoted as pein line 2 of
the algorithm). The algorithm then uses the standard DSE
approach where an unvisited location in the execution struc -
ture is selected (in this case an event from the set of possi-
ble extensions in line 4) and the program is executed with a
schedule and input values obtained by solving the path con-
straints collected during earlier executions to explore th e
target location. The information obtained from this exe-153cution is then converted into a sequence of events (line 5)
and any errors during the execution, such as uncaught ex-
ceptions, are reported. This sequence can be seen as a net
containing events corresponding to the operations observe d
during the execution. A net containing only events 1 and 2
and their pre- and postsets in Fig. 2 would be an example of
an event sequence. The construction of event sequences is
further illustrated in the example at the end of this section .
Notice that the events will not be added to the unfolding
when an event sequence is constructed. Instead, the events
in the sequence are processed one by one in their execution
order (the for-loop in line 6). In this loop an event will
be added to the unfolding unless it was already added by
an earlier execution. When a new event is added, the set
of possible extensions is updated (line 10) with new events
that could be enabled after ﬁring the event just added. To
guarantee termination, the execution depth of each thread
is limited by bound kand the test run is terminated if the
number of executed operations exceeds this limit.
It should be noted that even if the event sequence is com-
puted in line 5 by re-executing the program, the algorithm
works also when the event sequence is obtained by back-
tracking the execution to the point where the target event
is executed and continuing from there. This could be done,
for example, by forking the execution at each point where
new possible extensions are discovered.
Example 1. To illustrate the algorithm further, consider
the program in Fig. 6. The unfolding shown in the ﬁgure has
been generated by executing ﬁrst the thread 1 fully, followe d
by thread 2, in the ﬁrst test run. The event sequence for this
execution corresponds to the events 1, 2, and 3. Notice that
the if-statements of the threads do not generate events as
they do not depend on input values in this particular exe-
cution (i.e., they are not evaluated after the x = input();
statement has been executed). Adding event 1 to the net
does not result in any new possible extensions. However,
when event 2 is added, the possible extensions algorithm
notices that the write event of xcan be performed either
before or after the event 1. Therefore two possible exten-
sion events are generated that correspond to events 3 and
4. The possible extension corresponding to event 3 will al-
ready be explored by the current execution and therefore is
taken out of the set of possible extensions when the event 3
is added to the unfolding.
For the second test run, an event from the set of pos-
sible extensions is selected as a new test target. In this
case the set contains only the event 4. This event can be
reached by a schedule that forces the test run to ﬁrst exe-
cute two visible operations of thread 2 (and then allows the
test run to follow an arbitrary schedule). With this sched-
ule (after adding the event 5) the test run evaluates the
if-statement of thread 1 which now depends on the input
value assigned to xby thread 2. Assuming that the test run
follows the false branch, an event corresponding to the true
branch along with a path constraint is added as a possible
extension (event 7). For the ﬁnal test run the path con-
straint input >0 is solved and the resulting concrete value
is used as the input. This allows the error location of the
program to be reached.
If the part of an unfolding that belongs to a single thread
is considered in isolation, it can be seen as a symbolic ex-
ecution tree constructed by DSE together with additionalGlobal variables: Thread 1: Thread 2:
int x = 0; local int a = x; local int b = x;
if (a > 0) if (b == 0)
error(); x = input();
error5 1
3 4Thread 1 Thread 2 x
1,1x2,1
2
6 7x
1,2
x
1,3
x
1,4
x
1,5x2,2
x2,3
x2,4
Figure 6: Complete unfolding of the example
branching for scheduling related operations. Note also tha t
if some operation in the program can be reached by multiple
execution paths or non-equivalent schedules, the respecti ve
events in the unfolding are distinct as the set of events that
causally preceed a given event exactly describe an executio n
path through the control ﬂow graph of the program and a set
of schedules belonging to the same Mazurkiewicz trace. We
next describe the UpdatePotExt subroutine in Section 3.2
andExecute in Section 3.3.
3.2 Computing PossibleExtensions
As discussed before, we could use the standard possible
extension algorithms to implement UpdatePotExt . For
example, to compute possible extensions for a thread con-
dition that has read as the next operation we coukd check
every shared variable condition of the thread executing the
read operation to see if it is concurrent with the thread con-
dition. If it is, then there is a read event as a possible exten -
sion that has the shared variable condition and the thread
condition in its preset. The number of shared variable con-
ditions that need to be considered using this approach can
be large and it might be the case that only a small subset of
them are actually concurrent with the thread condition in
question. In this section we describe properties of the type
of unfoldings that are generated by our algorithm that al-
lows us in many cases reduce the number of conditions that
need to be considered when computing possible extensions.
The approach presented here is inspired by the DPOR algo-
rithm [7] that looks for transitions that are in race and can
be seen as being adjacent in some test execution. To make
the discussion precise, we use the following deﬁnitions.
Deﬁnition 1. If a place pin a Petri net is marked initially
and every transition that has pin its preset has it also in its
postset, the place pispermanently marked .
Deﬁnition 2. LetSbe a set of places. A set Cof con-
ditions represents Sif|S|=|C|and for each x∈Sthere
exists y∈Csuch that l(y) =x. For single conditions we
write that a condition crepresents a place pifl(c) =p.
Deﬁnition 3. LetC1andC2be co-sets of conditions that
represent the same set of places S. The co-sets C1and154C2areadjacent if there exists a sequence of events e0...en
and a reachable marking Mcontaining all the conditions in
C1such that ﬁring the event sequence from Mleads to a
marking where all conditions in C2are marked and there is
no preﬁx of the event sequence e0...ei<nleading to a marking
that contains a co-set representing S.
Example 2. Let us consider the co-sets a={x1,2, x2,2},
b={x1,1, x2,1}andc={x1,2, x2,1}in Fig. 6. The sets a
andbare not adjacent as there are two events connecting
them. However, bis adjacent with candcis adjacent with
a.
Deﬁnition 4. Two co-sets C1andC2that represent the
same set of places are alternatives if there exists a co-set C3
that is adjacent to both C1andC2and there exists con-
ditions a∈C1,b∈C2andx∈C3such that x < a and
x < b .
Lemma 1.Letcbe a condition in an unfolding and Sbe
a set of initially marked places. Let Gbe a graph constructed
as follows: there is a vertex in Gfor every co-set represent-
ingSsuch that all conditions in the co-set are concurrent
withcand there is an edge between two vertices if the respec-
tive co-sets are either adjacent or alternatives. The graph G
is connected.
Proof. See Appendix.
Once we have found one possible extension for some event,
Lemma 1 shows that the graph Ggives the search space from
where the other possible extensions can be found. We can
then do a backtrack search in this space to compute the
possible extensions. By restricting the search to graph G
instead of searching the whole unfolding, the search space
for possible extensions is in practice in many cases greatly
reduced. In worst case, however, the search spaces in both
cases are the same. It will be shown in the following, that
one possible extension that acts as the starting point for th e
search can always be found eﬃciently.
In order to do a backtrack search in the graph G, we
need to be able to compute adjacent and alternative co-sets
eﬃciently. For a co-set Ccontaining only shared variable
conditions it is easy to determine the adjacent co-sets as
the shared variables are modeled with permanently marked
places. Speciﬁcally, the event sequences in Deﬁnition 3 con -
tain only a single read or write event as ﬁring any such event
transforms Cto a new co-set representing the same shared
variable conditions. All these events can therefore be foun d
in the pre- and postset of the conditions in C. Checking
if two conditions are concurrent can be done in linear time
in the size of the unfolding. Adjacent lock conditions can
be eﬃciently computed by by maintaining a mapping from
each lock event to the set of following unlock events and a
mapping from unlock event to the causally preceding lock
event. The only case where there are alternative co-sets is
when there are multiple execution paths from a lock event
to diﬀerent unlock events. These can again eﬃciently found
by maintaining the mapping described above.
3.2.1 PossibleExtensionsfromThreadConditions
Computing possible extensions for thread conditions that
have either unlock or symbolic branching as their next op-
eration is straightforward. For unlocks there can be onlyone extension. The handling of symbolic branching is iden-
tical to normal dynamic symbolic execution: a branch with
a symbolic constraint and its negation are added to the un-
folding. For the other cases we need to compute a set of
shared variable conditions or lock conditions that are con-
current with the given thread condition such that the set
forms the preset of the possible extension event together
with the thread condition. Assuming that one such set of
conditions is known, we can use Lemma 1 to restrict the
number of conditions that need to be considered in order to
ﬁnd the the rest of the sets.
If the next operation of the thread condition is either read
or write, then one such set can be found by considering the
marking obtained by ﬁring the event sequence that is cur-
rently being explored by the algorithm in Fig. 5 up to the
point of reaching the thread condition. The shared variable
conditions in this this marking are then concurrent with the
thread condition and because shared variables are modeled
with permanently marked places, there is a marked condi-
tion for every shared variable copy in every marking. There-
fore, the initial set of conditions needed by Lemma 1 can be
directly obtained from the computed marking.
For a thread condition tthat has lock as the next op-
eration, it is not necessarily the case that in the marking
obtained as above there is a lock condition marked that is
concurrent with the thread condition. In this case, there
either is no suitable lock condition that is concurrent with
the thread condition or one such lock condition can be found
either in the preset of the last lock acquire event executed i n
the event sequence currently being explored or in one of the
unlock events that release the lock acquired by this event.
This is because the lock conditions in the postset of the un-
lock events are guaranteed to be concurrent with the thread
condition t(i.e., the lock operation following tbecomes en-
abled when the the current lock is released). If such unlock
event exists in the unfolding, we have found one suitable
lock condition. In the case that no such unlock events yet
exist in the unfolding, they will be added to it later when the
lock in question is unlocked. In this case, the only conditio n
adjacent or alternate to these future conditions is the lock
condition in the preset of the last lock acquire event that
has been executed and therefore by Lemma 1 it is the only
possible place to ﬁnd a suitable concurrent lock condition.
3.2.2 Possible Extensions from Shared Variable and
LockConditions
There can also be possible extensions from shared variable
or lock conditions that are not part of the extensions com-
puted from the thread conditions. For this we need to ﬁnd
all thread conditions having a next operation that accesses
the shared variable or lock condition and that are concurren t
with the conditions Cin the postset of the event for which
we are computing the possible extensions. For each thread
condition tfound we can then use Lemma 1 to restrict the
search space to ﬁnd possible extensions that all have tand
Cas part of their presets. In order to ﬁnd the thread con-
ditions, there are two types of thread conditions that need
to be considered: thread conditions for which no possible
extensions have yet been computed and thread conditions
with known possible extensions.
By Lemma 1 we know that if a suitable thread condition
has known possible extensions, one of the extensions must
have a co-set that is alternative or adjacent to the condi-155tions in C. Therefore such thread conditions can be found
by searching the events connected to the adjacent or alter-
native co-sets of C; more speciﬁcally the thread conditions
preceding these events and checking if they are concurrent
with C. As not all possible extensions are necessarily yet
added to the unfolding, we need to do this search in a net
that is the union of the current unfolding and the currently
known possible extensions.
For thread conditions that have no known possible exten-
sions yet, a list of them need to be maintained and each
thread condition in the list must be checked in turn to
see if it is concurrent with the conditions in C. Notice
that the only thread conditions that can be in the list are
thread conditions that have lock as their next operation.
For all other operation types one possible extension is al-
ways known. Therefore the size of the list that need to be
checked is small in practice in most cases.
3.3 ComputingSchedulesandInputValuesto
Execute Tests
TheExecute subroutine performs a single test run to
generate an event sequence. For this a schedule and input
values need to be computed. This can be done by collect-
ing all the events that causally precede the target event.
For each event we maintain the information in which order
they were added to the unfolding ( i.e., the labeling of events
with numbers in our examples) and which thread executed
the event. This allows a schedule for a test run to be con-
structed simply by ordering the collected events in the orde r
they were added and requiring the scheduler to schedule
threads in this order. We also store the symbolic constraint
information obtained by symbolic execution to these events ,
the path constraint describing the set of possible input val -
ues can be constructed and given to a constraint solver in
the same way as in standard dynamic symbolic execution.
That is, we take a conjunction of all the symbolic constraint s
in the collected events. If the path constrain is unsatisﬁab le,
the target event is unreachable and it is removed from the set
of possible extensions. A satisﬁable path constraint gives in-
put values that together with the computed schedule forces
a test run to reach the target event.
3.4 HandlingDynamicNumberofSharedVari-
ables and Threads
The algorithm described above assumes that all shared
variables are explicitly deﬁned in the beginning and the
number of threads is ﬁxed. It is possible to extend the
algorithm to support dynamically changing sets of shared
variables. In order to do this each such variable needs an
unique identiﬁer across all possible executions. In the con -
text of Java programs it is possible to obtain such identiﬁer s
by adding an event to the unfolding when a new object is
created and then identifying a shared variable (a ﬁeld of thi s
object) by combination of the object creation event and the
ﬁeld name of the variable. Static variables can be identiﬁed
by their class and ﬁeld names. Notice also that the con-
crete execution in dynamic symbolic execution gives precis e
aliasing information for the variables. Therefore, it is al ways
known if two references point to the same shared variable.
Handling dynamic number of threads is more challeng-
ing. The main problem here is that when a thread performs
a write operation, the corresponding write event accesses
shared variable conditions from all threads in the program.r1
w1
r2
w2w2
r2w1
r1
r2
w2w2
r2
Figure 7: Exponential example
It is not enough to consider only those threads that are run-
ning at the time the operation is executed as it is possible
that there exists an execution of the program where there are
additional threads running and the write aﬀects the behav-
ior of those threads as well. One simple way to address this
problem is to update each write operation in the unfolding
to access the conditions of a new thread when the thread
is added to the unfolding. This, however, is an expensive
operation. An alternative way is to model the thread cre-
ation with an event that reads all the shared variables of the
parent thread and writes the shared variables to the child
thread. The problem with this approach is that thread cre-
ations can now be considered to be in race with some write
operations and this can lead to unnecessary test runs. Cur-
rently dynamic thread creation is not supported and ﬁnding
an eﬃcient solution to the problem is part of future work.
3.5 Further Observations
In the set of possible extensions from which the target
event for the next test run is selected there can be multiple
events that are concurrent. It is therefore possible that on e
test run covers more than one test target. This property can
provide further reduction to the number of needed test runs
especially in situations where there are independent compo -
nents in the program under test and the random scheduler
and random inputs have a good chance of exploring diﬀerent
execution paths. Naturally it would be possible to compute
the exact sets of possible extensions that could be covered
with a single test run. This computation, however, is poten-
tially expensive (NP-hard) and it is left for future work to
study diﬀerent heuristics and approaches to take advantage
of this property and improve the runtime of the algorithm.
To illustrate this property further, consider a program
with 2 nthreads and nshared variables such that for all
the shared variables there is one thread that reads the vari-
able and one that writes to it. Therefore there are npairs
of threads that are independent to each other. For testing
approaches based on partial-order reduction, the number of
needed test executions to cover the program starts to grow
exponentially as the number of threads increases. This is il -
lustrated in Fig. 7. With unfoldings the possible extension s
from each pair are concurrent and therefore it is possible
with to explore the whole program with only two test runs
regardless of the number of threads. This can lead to an
exponential reduction compared to DPOR-like approaches.
4. RELATED APPROACHES
Dynamic partial-order reduction [7] and race detection
and ﬂipping [16] are algorithms that can be combined with
dynamic symbolic execution to reduce the number of test156r1
r2
l2l2l1
l1r1
r2
l2l2l1
l1r2
l2r1
l1
Figure 8: Order example
runs needed to test multithreaded programs. The intuition
behind these algorithms is to ﬁnd transitions that are in rac e
in the current execution and then introduce backtracking
points to the execution tree such that the diﬀerent order-
ings of the transitions in race will eventually be explored.
The main diﬀerence between these two algorithms is that
DPOR computes persistent sets and race detection and ﬂip-
ping uses postponed sets ( i.e., when a race is detected, in
another test run the ﬁrst transition participating in the ra ce
is postponed as much as possible, causing the transitions to
swap places in the trace). Another diﬀerence is that race de-
tection and ﬂipping remembers one operation participating
in the race for the next test run and in some cases is able to
use this information to avoid unnecessary backtracks.
Both of these algorithms reduce the execution tree of the
program in question to avoid exploration of irrelevant in-
terleavings of transitions. However, the obtained reducti on
depends on the order in which the events are added to the
execution tree. In [11] multiple diﬀerent heuristics are co n-
sidered for choosing the order in which the transitions are
explored and it is shown that the number of execution paths
that need to be tested can vary greatly between the heuris-
tics. As a simple example, consider a program with two
threads where both threads read a shared variable and then
lock and unlock the same lock. Figure 8 shows two possi-
ble execution trees that the algorithms explore depending
on whether the both reads are added to the tree ﬁrst or
not. The unfolding algorithm presented in this paper does
not have this problem. In fact, the new algorithms gives a
canonical representation of the program under test regard-
less of the order in which transitions are explored whereas
in dynamic partial-order reduction based approaches the ex -
ecution trees and the number of required test runs can be
diﬀerent as in the example above.
One additional diﬀerence between the partial-order reduc-
tion algorithms and the new approach is that the reduc-
tion algorithms are guaranteed to ﬁnd not only safety vi-
olations but all deadlocks as well. This is due to the fact
that these algorithms explore all Mazurkiewicz traces of a
program whereas the new algorithm constructs a represen-
tation that contains all the traces but does not necessarily
perform a test execution on each of them. This can some-
times result in exponential reduction in the number of test
cases. However, the new algorithm needs a separate post
processing step with the generated unfolding as an input if
all deadlocks (possible global ﬁnal states) of the program
need to be enumerated. There is, to our knowledge, no dy-
namic partial-order reduction like approach that can furth er
restrict the search space when only local reachability prop -
erties of each thread are to be checked.The operations in partial-order reduction based algorithm s
are also computationally less expensive than in the new al-
gorithm. Therefore, in cases where both approaches explore
the same number of execution paths, the existing algorithms
are likely to be faster, but, on the other hand, the new algo-
rithm can signiﬁcantly reduce the number of execution paths
that need to be tested and thus make the overall testing pro-
cess more eﬃcient especially if runtime of the test runs are
long, for example, due to calls to a constraint solver.
5. EXPERIMENTAL EVALUATION
This section provides preliminary experiments to compare
our algorithm with DPOR and race detection and ﬂipping.
5.1 Implementation
We have implemented the algorithm described in this pa-
per and the DPOR algorithm extended with a support for
commutativity of reads and writes as described in [13] as
well as sleep sets in a prototype tool that currently allows
testing a small subset of Java programs. Both of these algo-
rithms have been set to select the next test target randomly
from the set of all known test targets. The tool uses Soot
framework [17] to instrument the program under test with
additional code that enables symbolic execution alongside
concrete execution and allows the schedule of the threads to
be controlled. As a constraint solver the tool uses Z3 [2]. To
compare our algorithm with race detection and ﬂipping we
also use jCUTE [15], which is a tool that combines race de-
tection and ﬂipping with dynamic symbolic execution to test
Java programs. For jCUTE we do not report runtimes as
jCUTE restarts JVM for each test run which our tool does
not and also uses a diﬀerent constraint solver. Therefore
the runtimes are not directly comparable and the runtimes
of jCUTE would be noticeably longer. However, runtime
overhead per test execution of race detection and ﬂipping
is expected be close to the DPOR implementation used in
these experiments.
5.2 Benchmarks
The indexer and ﬁle system benchmarks are from [7] where
they are used for experimental evaluation of the DPOR algo-
rithm. Both of them are examples of programs where static
partial-order reduction algorithms cannot accurately pre dict
the use of the shared memory and therefore conservatively
explore more states than dynamic approaches. Parallel pi-
algorithm is an example of a typical program where a task
is divided to multiple threads and the results of each com-
putation are merged in a block protected by locks. The test
selector benchmarks represent an abstracted and faulty im-
plementation of a modiﬁcation to a small part of our tool
that allows multiple test executions to be run concurrently .
The pairs program is an artiﬁcial example that is similar to
the example discussed in Section 3.5. This could be consid-
ered as a close to optimal case for the unfolding approach
when compared to partial-order reduction approaches. The
single lock benchmark is a program where multiple threads
access various shared variables in a way that all accesses ar e
protected by the same lock. In this case all the approaches
explore the same amount of execution paths (the interleav-
ings of lock operations) and there are no such operations
that can cause the DPOR algorithm to explore unnecessary
paths due to scheduling choices (as explained in Section 4).
The example also forces the unfolding algorithm to make157large number of checks whether two conditions are concur-
rent. This illustrates a case where the DPOR algorithm has
an advantage when compared to the unfolding approach.
The synthetic examples are handmade examples where the
threads perform an arbitrarily generated sequences of oper -
ations, including branching operations on input values. Th e
benchmarks used in the evaluation are available online1.
5.3 Discussion
Table 1 shows the results of our experimental evaluation.
To obtain the results, each benchmark was tested 25 times
with both the unfolding algorithm and the DPOR algorithm.
For these tests we report the best, worst and median runtime
and number of execution paths explored by the algorithms.
To get an idea of the memory requirements of the new al-
gorithm, the results also show the number of events in the
unfolding that is generated by the algorithm. The size of
the pre- and post sets of any event is at most the number
of threads plus one. Each event and condition requires a
constant amount of memory. The memory requirements for
for the lock-to-unlock mappings described in Section 3.2 ar e
negligible in these experiments.
In the ﬁle system example there is only one execution
path up to using 13 threads. After this the threads start to
interact in pairwise manner such that the pairs do not aﬀect
each other. For the partial-order reduction based algorith ms
the number of test cases starts to grow rapidly whereas the
new algorithm improves the situation signiﬁcantly. In the
single lock and synthetic-1 benchmarks there are no input
variables and therefore no constraint solver calls and DPOR
is able to take full advantage of the fact that it is more
lightweight than our approach. For the optimal case for
unfoldings, our algorithm expectedly performs very well.
Based on these results the unfolding approach typically
requires less test executions but is slower per test executi on.
However, the reduction to the number of executions is in
many case enough to make the new algorithm faster than
DPOR. In the cases where jCUTE executes the same num-
ber of tests as our algorithm, race detection and ﬂipping is
expected to be faster than the unfolding algorithm.
6. CONCLUSIONS
We have presented a new approach combining dynamic
symbolic execution and unfoldings for testing multithread ed
programs. We have shown that this approach can result in
some cases even in an exponential reduction to the needed
test runs when checking reachability of local states of thre ads
when compared with existing dynamic partial-order reduc-
tion based approaches. In particular, we can sometimes
cover all of the local states of threads using less test runs
than there are Mazurkiewicz traces of the system. The ap-
proach also does not need any ordering heuristics to improve
the partial-order reduction eﬀectiveness, unlike previou s ap-
proaches that work on execution trees. We have discussed
the advantages and disadvantages of the new testing ap-
proach over existing algorithms. Basically our approach is
more eﬃcient in the number of test cases but relies on more
expensive algorithms (possible extensions calculation) t o do
so. Unfoldings naturally capture the causality and conﬂict s
of the events in multithreaded programs in a way that al-
lows test cases to be eﬃciently generated. Given this, we
1http://users.ics.tkk.fi/ktkahkon/ASE2012believe that unfolding based techniques are promising alte r-
natives to dynamic partial-order reduction based approach es
in the context of automated testing and provides interestin g
avenues for further research.
7. ACKNOWLEDGMENTS
This work has been ﬁnancially supported by Tekes - Finnish
Agency for Technology and Innovation, ARTEMIS-JU and
Academy of Finland (projects 128050 and 139402).
8. REFERENCES
[1] C. Cadar, V. Ganesh, P. M. Pawlowski, D. L. Dill,
and D. R. Engler. EXE: automatically generating
inputs of death. In Proceedings of the 13th ACM
conference on Computer and communications security
(CCS 2006) , pages 322–335. ACM, 2006.
[2] L. M. de Moura and N. Bjørner. Z3: An eﬃcient SMT
solver. In Proceedings of the 14th International
Conference on Tools and Algorithms for the
Construction and Analysis of Systems (TACAS 2008) ,
volume 4963 of Lecture Notes in Computer Science ,
pages 337–340. Springer, 2008.
[3] V. Diekert. The Book of Traces . World Scientiﬁc
Publishing Co., Inc., River Edge, NJ, USA, 1995.
[4] J. Esparza and K. Heljanko. Unfoldings – A
Partial-Order Approach to Model Checking . EATCS
Monographs in Theoretical Computer Science.
Springer-Verlag, 2008.
[5] J. Esparza and S. R ¨omer. An unfolding algorithm for
synchronous products of transition systems. In
J. C. M. Baeten and S. Mauw, editors, CONCUR ,
volume 1664 of Lecture Notes in Computer Science ,
pages 2–20. Springer, 1999.
[6] A. Farzan and P. Madhusudan. Causal atomicity. In
T. Ball and R. B. Jones, editors, CAV , volume 4144 of
Lecture Notes in Computer Science , pages 315–328.
Springer, 2006.
[7] C. Flanagan and P. Godefroid. Dynamic partial-order
reduction for model checking software. In J. Palsberg
and M. Abadi, editors, POPL , pages 110–121. ACM,
2005.
[8] P. Godefroid. Partial-Order Methods for the
Veriﬁcation of Concurrent Systems: An Approach to
the State-Explosion Problem . Springer-Verlag New
York, Inc., Secaucus, NJ, USA, 1996.
[9] P. Godefroid, N. Klarlund, and K. Sen. DART:
Directed automated random testing. In Proceedings of
the ACM SIGPLAN 2005 Conference on Programming
Language Design and Implementation (PLDI 2005) ,
pages 213–223. ACM, 2005.
[10] V. Khomenko and M. Koutny. Towards an eﬃcient
algorithm for unfolding Petri nets. In K. G. Larsen
and M. Nielsen, editors, CONCUR , volume 2154 of
Lecture Notes in Computer Science , pages 366–380.
Springer, 2001.
[11] S. Lauterburg, R. K. Karmani, D. Marinov, and
G. Agha. Evaluating ordering heuristics for dynamic
partial-order reduction techniques. In 13th
International Conference of Fundamental Approaches
to Software Engineering , pages 308–322, 2010.
[12] K. L. McMillan. Using unfoldings to avoid the state
explosion problem in the veriﬁcation of asynchronous158Table 1: Comparison of the new algorithm with dynamic partia l-order reduction
Unfolding DPOR jCUTE
number of paths time (seconds) size number of paths time (seconds) paths
Benchmark min med max min med max min med max min med max
indexer (12) 8 8 8 2 2 2 544 51 85 138 6 10 14 8
ﬁlesystem (14) 2 2 2 0 0 0 130 2 3 5 0 0 0 2
ﬁlesystem (16) 2 3 4 0 0 0 178 9 16 35 1 2 2 31
ﬁlesystem (18) 2 4 5 0 0 0 226 49 97 217 4 6 10 2026
parallel π(4) 24 24 24 1 1 1 294 36 95 925 1 2 7 24
parallel π(5) 120 120 120 3 3 3 1346 882 2698 10086 8 17 53 120
test selector (3) 65 65 65 2 2 2 955 65 87 191 1 2 4 65
test selector (4) 2576 2576 2576 62 70 76 33677 5667 8042 13163 71 97 158 2576
pairs (6) 6 7 10 0 0 2 228 512 512 512 7 8 10 580
single lock (4) 2520 2520 2520 40 42 44 36834 2520 2520 2520 13 13 14 2520
synthetic-1 (3) 984 984 984 15 15 15 3888 2562 3716 5624 7 10 13 2430
synthetic-2 (3) 1940 1943 1947 52 54 55 7590 6183 7768 10365 46 56 72 4860
synthetic-3 (4) 682 682 682 13 14 14 3959 2677 8550 16372 19 52 96 1757
circuits. In G. von Bochmann and D. K. Probst,
editors, CAV , volume 663 of Lecture Notes in
Computer Science , pages 164–177. Springer, 1992.
[13] O. Saarikivi, K. K ¨ahk¨onen, and K. Heljanko.
Improving dynamic partial order reductions for
concolic testing. In Proceedings of the 12th
International Conference on Application of
Concurrency to System Design , to appear.
[14] K. Sen. Scalable automated methods for dynamic
program analysis . Doctoral thesis, University of
Illinois, 2006.
[15] K. Sen and G. Agha. CUTE and jCUTE: Concolic
unit testing and explicit path model-checking tools. In
Proceedings of the 18th International Conference on
Computer Aided Veriﬁcation (CAV 2006) , volume
4144 of Lecture Notes in Computer Science , pages
419–423. Springer, 2006. (Tool Paper).
[16] K. Sen and G. Agha. A race-detection and ﬂipping
algorithm for automated testing of multi-threaded
programs. In Haifa Veriﬁcation Conference , volume
4383 of Lecture Notes in Computer Science , pages
166–182. Springer, 2006.
[17] R. Vall´ ee-Rai, P. Co, E. Gagnon, L. J. Hendren,
P. Lam, and V. Sundaresan. Soot - a Java bytecode
optimization framework. In Proceedings of the 1999
conference of the Centre for Advanced Studies on
Collaborative Research (CASCON 1999) , page 13.
IBM, 1999.
APPENDIX
A. PROOFOF LEMMA 1
Let us denote a vertex in Gcorresponding to a co-set C
with v(C) and let us assume that it does not hold that Gis
connected. This means that there are co-sets AandBsuch
thatv(A) and v(B) belong to diﬀerent subgraphs of G.
LetG′be graph that has a vertex for every co-set repre-
senting Sand an edge between two vertices if the respective
co-sets are adjacent. The vertices of Gare then a subset of
the vertices in G′. Let v(I) be the vertex in G′such that all
the conditions in Iare in the initial marking. For any vertex
vthere exists at least one path in G′from v(I) tov. In the
shortest path it holds that for any vertices v(N) and v(N′)
that are next to each other in the path, any condition n∈Nis either the same or causally related to the corresponding
condition in N′.
Let us now consider such paths from v(I) to both v(A)
andv(B). In one of these paths there must exist a vertex
representing a co-set that is not concurrent with c; otherwise
AandBwould belong to the same subgraph in G. Such co-
set contains a condition zthat is not concurrent with cand
causally precedes the condition a∈Aorb∈Brepresenting
the same place. If we select AandBsuch that both paths
contain only one vertex corresponding to a co-set concurren t
with cthen zis causally related to both aandb. There are
now three cases how zcan be in relation to aandb: (i)
a < z < b , (ii) b < z < a , or (iii) z < a andz < b . Note that
z=aandz=bcannot hold as aandbare concurrent with
c. Because zis causally related to both aandb, it cannot
be that zis in conﬂict with either of them.
Let us consider the ﬁrst and second cases. Since zis not
concurrent with cit holds that either c < z ,z < c orc#z.
These alternatives imply c < b ,a < c andc#b, respectively,
for the ﬁrst case and c < a ,b < c andc#a, for the second
case. Each of these contradicts the assumption that aandb
are concurrent with c. In the third case c < z andc#zimply
c < a andc#a, respectively, leading to a contradiction. We
still need to consider the case z < c . If there exists a co-
set containing zthat is adjacent to both AandB, then
AandBare alternatives by deﬁnition and therefore in the
same subgraph in G. Therefore there must be a co-set Y
adjacent to either AorBcontaining a condition z′that
represents that same place as zandz < z′. We can also
assume that in Ythere is a condition ythat is not concurrent
with c. Otherwise the co-set Ywould be concurrent with c
which is against our selection of AandBto be only such
sets in the path. Let us consider the case where the set
Yis adjacent to Aand let a′andb′be conditions in A
andB, respectively, that represent the the same place as y.
Since yis not concurrent with c, we know that either y < c ,
c < y ory#c. The ﬁrst alternative implies that b′is not
concurrent with cbecause it must hold that either y#b′or
b′< y. This is in contradiction with the fact that every
condition in Bis concurrent with c. The second and third
alternatives imply c < a′anda′#c, respectively, leading
again to a contradiction. The case where Yis adjacent to
Bis symmetric to the case above and therefore leads to a
contradiction. Since all the cases lead to contradictions, it
must hold that Gis connected.159