Improving the Accuracy of Oracle Verdicts Through
Automated Model Steering
Gregory Gay, Sanjai Rayadurgam, Mats P .E. Heimdahl
Department of Computer Science & Engineering
University of Minnesota, USA
greg@greggay.com, [rsanjai,heimdahl]@cs.umn.edu
ABSTRACT
The oracle‚Äîa judge of the correctness of the system under test
(SUT)‚Äîis a major component of the testing process. Specifying
test oracles is challenging for some domains, such as real-time em-
bedded systems, where small changes in timing or sensory input
may cause large behavioral differences. Models of such systems,
often built for analysis and simulation, are appealing for reuse as
oracles. These models, however, typically represent an idealized
system, abstracting away certain issues such as non-deterministic
timing behavior and sensor noise. Thus, even with the same inputs,
the model‚Äôs behavior may fail to match an acceptable behavior of
the SUT, leading to many false positives reported by the oracle.
We propose an automated steering framework that can adjust the
behavior of the model to better match the behavior of the SUT to re-
duce the rate of false positives. This model steering is limited by a
set of constraints (deÔ¨Åning acceptable differences in behavior) and
is based on a search process attempting to minimize a dissimilar-
ity metric. This framework allows non-deterministic, but bounded,
behavior differences, while preventing future mismatches, by guid-
ing the oracle‚Äîwithin limits‚Äîto match the execution of the SUT.
Results show that steering signiÔ¨Åcantly increases SUT-oracle con-
formance with minimal masking of real faults and, thus, has signif-
icant potential for reducing false positives and, consequently, de-
velopment costs.
Categories and Subject Descriptors
D.2.5 [Software Engineering]: Testing and Debugging
General Terms
VeriÔ¨Åcation
Keywords
Software Testing, Test Oracles, Model-Based Testing
This work has been partially supported by NSF grants CNS-
0931931 and CNS-1035715.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proÔ¨Åt or commercial advantage and that copies bear this notice and the full cita-
tion on the Ô¨Årst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciÔ¨Åc permission
and/or a fee. Request permissions from permissions@acm.org.
ASE‚Äô14, September 15-19, 2014, V√§ster√•s, Sweden.
Copyright 2014 ACM 978-1-4503-3013-8/14/09 ...$15.00.
http://dx.doi.org/10.1145/2642937.2642989.1. INTRODUCTION
When running a suite of tests, the test oracle is the judge that
determines the correctness of the execution of a given system un-
der test (SUT). Despite increased attention in recent years, the test
oracle problem [16]‚Äîa set of challenges related to the construc-
tion of efÔ¨Åcient and robust oracles‚Äîremains a major problem in
many domains. One such domain is that of real-time process con-
trol systems‚Äîembedded systems that interact with physical pro-
cesses such as implanted medical devices or power management
systems. Systems in this domain are particularly challenging since
their behavior depends not only on the values of inputs and outputs,
but also on their time of occurrence [9]. In addition, minor behav-
ioral distinctions may have signiÔ¨Åcant consequences [18]. When
executing the software on an embedded hardware platform, several
sources of non-determinism, such as input processing delays, exe-
cution time Ô¨Çuctuation, and hardware inaccuracy, can result in the
SUT non-deterministically exhibiting varying‚Äîbut acceptable‚Äî
behaviors for the same test case.
Behavioral models [21], typically expressed as state-transition
systems, represent the system requirements by prescribing the be-
havior (the system state) to be exhibited in response to given input.
Common modeling tools in this category are StateÔ¨Çow [22], State-
mate [15], and Rhapsody [1]. Models built using these tools are
used for many purposes in industrial software development and,
thus, their reuse as a test oracle is highly desirable. These mod-
els, however, provide an abstract view of the system that typically
simpliÔ¨Åes the actual conditions in the execution environment. For
example, communication delays, processing delays, and sensor and
actuator inaccuracies may be omitted. Therefore, on a real hard-
ware platform, the SUT may exhibit behavior that is acceptable
with respect to the system requirements, but differs from what the
model prescribes for a given input; the system under test is ‚Äúclose
enough‚Äù to the behavior described by the model. Over time, these
differences can build to the point where the execution paths of the
model and the SUT diverge enough to Ô¨Çag the test as a ‚Äúfailure,‚Äù
even if the system is still operating within the boundaries set by the
requirements. In a rigorous testing effort, this may lead to tens of
thousands of false reports of test failures that have to be inspected
and dismissed‚Äîa costly process.
One solution would be to Ô¨Ålter the test results on a step-by-step
basis‚Äîchecking the state of the SUT against a set of constraints
and overriding the oracle verdict as needed. However, Ô¨Ålter-based
approaches are inÔ¨Çexible. While a Ô¨Ålter may be able to handle iso-
lated non-conformance between the SUT and the oracle model,
it will likely fail to account for behavioral divergence that builds
over time, growing with each round of input. Instead, we take in-
spiration for addressing this model-SUT mismatch problem from
program steering, the process of adjusting the execution of live
527
programs in order to improve performance, stability, or correct-
ness [23]. We hypothesize that behavioral models can be adapted
for use as oracles for real-time systems through the use of steering
actions that override the current execution of the model [10, 31].
By comparing the state of the model-based oracle (MBO) with that
of the SUT following an output event, we can guide the model to
match the state of the SUT, as long as a set of constraints deÔ¨Åning
acceptable deviation are met. Unlike a Ô¨Ålter, steering is adaptable,
adjusting the live execution of the model‚Äîwithin the space of le-
gal behaviors‚Äîto match the execution of the SUT. The result of
steering is a widening of the behaviors accepted by the oracle, thus
compensating for allowable non-determinism, without unaccept-
ably impairing the ability of the model-based oracle to correctly
judge the behavior of the SUT.
We present an automated framework for comparing and steer-
ing the model-based oracle with respect to the SUT, building on
ideas Ô¨Årst proposed in [10]. We detail the implementation of the
framework, and assess its capabilities on a model for the control
software of an patient controlled analgesia pump (a medical infu-
sion pump)‚Äîa real-world systems with complex, time-based be-
haviors. Case study results indicate that steering improves the accu-
racy of the Ô¨Ånal oracle verdicts‚Äîoutperforming both default test-
ing practice and step-wise Ô¨Åltering. Oracle steering successfully
accounts for within-tolerance behavioral differences between the
model-based oracle and the SUT‚Äîeliminating a large number of
spurious ‚Äúfailure‚Äù verdicts‚Äîwith minimal masking of real faults.
By pointing the developer towards behavior differences more likely
to be indicative of real faults, this approach has the potential to
lower testing costs and reduce development effort.
2. BACKGROUND
There are two key artifacts necessary to test software, the test
data‚Äîinputs given to the system under test‚Äîand the test oracle‚Äî
a judge on the resulting execution [17, 32]. A test oracle can be
deÔ¨Åned as a predicate on a sequence of stimuli to and reactions
from the SUT that judges the resulting behavior according to some
speciÔ¨Åcation of correctness [16].
The most common form of test oracle is a speciÔ¨Åed oracle‚Äî
one that judges behavioral aspects of the system under test with
respect to some formal speciÔ¨Åcation [16]. Commonly, such an or-
acle checks the behavior of the system against a set of concrete
expected values [30] or behavioral constraints (such as assertions,
contracts, or invariants) [7]. However, speciÔ¨Åed oracles can be de-
rived from many other sources of information; we are particularly
interested in using behavioral models, such as those often built for
purposes of simulation, analysis and testing [21].
A common practice during the development and testing of soft-
ware is to build models of the intended behavior of the Ô¨Ånal system.
Although such models are useful at all stages of the development
process‚Äîparticularly during requirements analysis‚Äîthey are par-
ticularly useful for addressing two problems in testing: (1) models
allow analysis and testing activities to begin before the actual im-
plementation is constructed, and (2) models are suited to the appli-
cation of veriÔ¨Åcation and automated test generation techniques that
allow us to cover a larger class of scenarios than we can cover man-
ually [26]. As such, models are often executable; thus, in addition
to serving as the basis of test generation [21], models can be used
as a source of expected behavior, that is, used as a test oracle.
Executable behavioral models can be divided into declarative
models created in formal speciÔ¨Åcation languages, and construc-
tivemodels built with state-transition languages such as Simulink
and StateÔ¨Çow, Statemate, Ô¨Ånite state machines, or other automata
structures [21]. Presently, we focus our work on constructive state-
Model 
SUTInput 
Device Output 
Device Environment Environment 
Input1: 
(analog) 
Input2: 
T Input1: 
72
Input2: 
T+dOutput: 
Mode 3 
Time: 
T+d+d2 
Input 
Device 
Input1: 
(analog) 
Input2: 
T Input1: 
72
Input2: 
TOutput 
Device Output: 
Mode 1 
Time: T 
Figure 1: Illustration of abstraction induced behavioral differ-
ences between the model and the system under test.
transition systems since these are frequently used to model real-
time control systems‚Äîsoftware that monitors and interacts with a
physical environment [18].
Non-determinism is a major concern in embedded real-time sys-
tems. The task of monitoring the environment and pushing signals
through multiple layers of sensors, software, and actuators can in-
troduce points of failure, delay, and unpredictability. Input and ob-
served output values may be skewed by noise in the physical hard-
ware, timing constraints may not be met with precision, or inputs
may arrive faster than the system can process them. Often, the sys-
tem behavior may be acceptable, even if the system behavior is not
exactly what was captured in the model‚Äîa model that, by its very
nature, incorporates a simpliÔ¨Åed view of the problem domain. A
common abstraction when modeling is to omit any details that dis-
tract from the core system behavior in order to ensure that analysis
of the models is feasible and useful. Yet these omitted details may
manifest themselves as differences between the behavior deÔ¨Åned in
the model and the behavior observed in the implementation.
To give an example of how these differences manifest them-
selves, consider the model and a corresponding system depicted
in Figure 1. A common abstraction when designing a model is
to assume that all actions take place instantly, ignoring the reality
ofcomputation time. Both the model and the actual implementa-
tion receive the same stimuli from the physical environment, both
process that input through the system, and then both issue output
back to the environment. In the model, all actions are considered
to have taken place at time step T. However, in the system, com-
putations take time to be completed, and the processing of environ-
mental stimulus through hardware layers and software subsystems
adds an additional delay to the time at which the system receives
that input. By the time that the system Ô¨Ånishes responding to the
stimulus, at time T+ (2 delay periods), it is unlikely that the
output fed to the environment matches the output that the model is-
sued (in this case, not only do the output timestamps not match, but
the SUT made a mode selection that is entirely different from that
of the model). In systems such as pacemakers or infusion pumps,
time is a crucially important piece of data, and delays can lead to
a behavior that is very different from the one produced by a model
that abstracts such delays. Furthermore, such delays are commonly
non-deterministic. Repeated application of the same stimulus may
not result in the same output if processing time varies.
This raises the question‚Äîwhy use models as oracles? Alterna-
tive approaches could be to turn to an oracle based on explicit be-
havioral constraints‚Äîassertions or invariants‚Äîor to build declar-
528ative behavioral models in a formal notation such as Modelica.
These solutions, however, have their limitations. Assertion-based
approaches only ensure that a limited set of properties hold at par-
ticular points in the program [7]. Further, such oracles may not
be able to account for the same range of testing scenarios as a
model that prescribes behavior for all inputs. Declarative models
that express the computation as a theory in a formal logic allow for
more sophisticated forms of veriÔ¨Åcation and can better account for
time-constrained behaviors [11]. However, Miller et al. have found
that developers are more comfortable building constructive models
than formal declarative models [24]. Constructive models are visu-
ally appealing, easy to analyze without specialized knowledge, and
suitable for analyzing failure conditions and events in an isolated
manner [11]. The complexity of declarative models and the knowl-
edge needed to design and interpret such models make widespread
industrial adoption of the paradigm unlikely.
While there are challenges in using constructive model-based or-
acles, it is a widely held view that such models are indispensable in
other areas of development and testing, such as requirements analy-
sis or automated test generation [26, 20]. From this standpoint, the
motivational case for models as oracles is clear‚Äîif these models
are already being built, their reuse as test oracles could save signif-
icant amounts of time and money, and allow developers to automate
the execution and analysis of a large volume of test cases. Practi-
tioners are well-versed in these models and so these are likely to
be less error-prone compared to building a new unfamiliar type of
oracle. Therefore, we seek a way to use constructive model-based
oracles that can handle the non-determinism introduced during sys-
tem execution on the target hardware.
3. ORACLE STEERING
In a typical model-based testing framework, the test suite is ex-
ecuted against both the SUT and the model-based oracle, and the
values of certain variables are recorded to a trace Ô¨Åle after each ex-
ecution step. The oracle‚Äôs comparison procedure examines those
traces and issues a verdict for each test (fail if test reveals discrep-
ancies, pass otherwise). When testing a real-time system, we would
expect non-determinism to lead to behavioral differences between
the SUT and the model-based oracle during test execution. The
actual behaviors witnessed in the SUT may not be incorrect‚Äîthey
may still meet the system requirements‚Äîthey just do not match
what the model produced. We would like the oracle to distinguish
between correct, but non-conforming behaviors introduced by non-
determinism and behaviors that are indicative of a fault.
One approach to address this would be to augment the compar-
ison procedure with a Ô¨Åltering mechanism to detect and discard
acceptable differences on a per-step basis. For example, to address
the simple computation-time abstraction in Figure 1, a Ô¨Ålter could
simply allow any timestamp within a certain range. However, the
issue with Ô¨Åltering on a per-step basis is that the effect of non-
determinism may linger on for several steps, leading to irrecon-
cilable differences between the SUT and the model-based oracle.
Filters may not be effective at handling growing behavioral diver-
gence. If the time of input or output impacts behavior, such as in
the case of a pacemaker‚Äîa system where even a single delayed in-
put may impact all future commanded paces‚Äîa Ô¨Ålter is unlikely to
make an accurate judgement after the Ô¨Årst few comparisons.
To address this problem, we take inspiration from program steer-
ing‚Äîthe process of adjusting the execution of live programs in or-
der to improve performance, stability, or behavioral correctness [12].
Instead of steering the behavior of the SUT, however, we steer the
oracle to see if the model is capable of matching the SUT‚Äôs be-
havior. When the two behaviors differ, we backtrack and apply a
Output = Mode2 
Time = (T-1) 
...Output = Mode1 
Time = T 
...Input1: 72 
Input2: T 
Original transition taken 
Backtrack 
Consider possible transitions 
Output = Mode3 
Time = (T+d+d2) 
...Chose steering 
target and transition 
to it. Figure 2: Illustration of steering process.
steering action‚Äîe.g., adjust timer values, apply different inputs, or
delay or withhold an input‚Äîthat changes the state of the model-
based oracle to a state more similar to the SUT (as judged by a
dissimilarity metric). Oracle steering, unlike Ô¨Ålters, is adaptable.
Such actions provide Ô¨Çexibility to handle non-determinism, while
still retaining the power of the oracle as an arbiter. Of course, im-
proper steering can bias the behavior of the model-based oracle,
masking both acceptable deviations and actual indications of fail-
ures. Nevertheless, we believe that by using a series of constraints
it is possible to sufÔ¨Åciently bound steering so that the ability to de-
tect faults is still retained.
First, a set of tolerance constraints governing the allowable
changes to certain variables (input, internal, or output) in the model
that can be affected by steering. These constraints deÔ¨Åne bounds on
the non-determinism or behavioral deviation that can be accounted
for with steering. For example, a pacemaker takes as input time-
stamped sensed cardiac events and responds by setting the time of
the next commanded pace (the pulse delivered to the heart). Then,
if there are no additional sensed events within that time frame, the
pacemaker will issue an electrical pulse to the appropriate cham-
ber of the heart. If the model acts on the input after a slight delay
and the model acts instantly, we might allow steering to adjust the
time stamp on an input by up to four milliseconds. However, we
might forbid steering from changing the pacemaker‚Äôs response to
that input (i.e., if the pacemaker responds with a status ‚Äúevent ac-
knowledged and acted on‚Äù, we might not allow the response to be
changed to ‚Äúpremature sensed event‚Äù). If a pace command occurs at
a different time in the system than it does in the model, we might‚Äî
within a similar time window‚Äîallow steering to attempt to adjust
the time that the pace command occurs.
Second, a dissimilarity function Dis(model state; SUT state ),
that compares the state of the model to the observable state of the
SUT. We seek a minimization of Dis(snew
m; ssut)< Dis (sm; ssut).
That is, within the bounds on the search space set by the tolerance
constraints, we seek the candidate solution with the lowest dissim-
ilarity score to the state of the SUT. There are many different func-
tions that can be used to calculate dissimilarity. Cha provides a
good primer on the calculation of dissimilarity [6].
Third, a further set of general policy decisions on when to steer.
For example, one might decide not to steer unless Dis(snew
m; ssut)
= 0‚Äîthat is, one might decide not to steer at all unless there ex-
5291. for test in Tests
2. for step in test
3. initialVerdict = Dis(Sm; Ssut)
4. if initialVerdict > 0
5. oldState = Sm
6. targetState=searchForNewState(Model,S m,Ssut,Constraints,Dis())
7. while Dis(targetState; S sut)< Dis(oldState; S sut)
8. oldState = targetState
9. targetState=searchForNewState(Model,S m,Ssut,Constraints,Dis())
10. transitionModel(Model,targetState)
Figure 3: Steps in the Steering Process
ists a steering action that results in a model state identical to that
observed in the SUT.
In other words, the new state of the model-based oracle following
the application of a steering action must be a state that is possible to
reach within a limited number of transitions from the current state
of the model, must fall within the boundaries set by the tolerance
constraints, and must minimize the dissimilarity function.
We can illustrate steering using the system depicted in Figure 1.
This system takes as input some environmental factor and a time
stamp on when the reading was taken. It then outputs a mode
choice and a time stamp on when the mode choice is issued. The
model has abstracted computation time, and as a result, the model
issues a mode choice and time stamp that differs from the SUT (the
SUT is subjected to the time delay of the initial input to pass from
the environment to the software and the time to perform compu-
tations). Although there is clearly non-conformance between the
model-based oracle and the SUT, the SUT may still be operating
within the bounds of the system requirements. Thus, as depicted in
Figure 2, we can attempt to steer the model-based oracle.
As outlined in Figure 3, after obtaining our initial verdict by
calculating the dissimilarity function (i.e., the Euclidean distance
between the state of the SUT and the state of the model-based or-
acle), the steering procedure would backtrack to the state of the
model before receiving the input Input 1 = 72, Input 2 =Tand
evaluate the set of possible transitions from that state through the
use of a search algorithm. The set of candidate solutions is limited
by the set of tolerance constraints‚Äîfor example, we might allow
Input 1<=Input 1new<= (Input 1+5) (the value Input 1new
may fall up to 5 seconds after the original value of Input 1). Then,
the search selects from the remaining candidates through the use of
the dissimilarity function. Finally, the candidate solution that re-
sults in the lowest observed value of Dis(snew
m; ssut)is chosen as
the new state of the model-based oracle. Execution then resumes
from the chosen state.
We have implemented the basic steering approach outlined in
Figure 3. Our search process is based on SMT-based bounded
model checking [13], which is a natural choice for this problem.
We have a series of constraints that govern steering actions and
seek to locate a model state reachable in a limited number of tran-
sitions that satisÔ¨Åes those constraints and minimizes a dissimilarity
metric. SpeciÔ¨Åcally, we make use of the Kind model checker [13]
and the Z3 constraint solver [8].
A solution to the constraint Dis(snew
m; ssut)< Dis (sm; ssut)
would give us a model state that is more similar to the behav-
ior of the SUT than the original transition taken by the model-
based oracle, but carries no guarantee that the satisfying state min-
imizes the dissimilarity metric. Thus, we Ô¨Årst check the constraint
Dis(snew
m; ssut) = 0; then, if an exact match cannot be found and
if the steering policy allows inexact matches, we apply the modelchecker with this constraint in order to get an initial threshold, then
iteratively reapply the model checker with new thresholds until we
can no longer Ô¨Ånd a better solution. The best solution found then
becomes the new state of the model-based oracle.
3.1 Selecting Constraints
The efÔ¨Åcacy of the steering process depends heavily on the tol-
erance constraints and policies employed. If the constraints are too
strict, steering will be ineffective‚Äîleaving as many ‚Äúfalse failure‚Äù
verdicts as not steering at all. On the other hand, if the constraints
are too loose, steering runs the risk of covering up real faults in the
system. Therefore, it is crucially important that the constraints to
be employed are carefully considered.
Often, constraints can be inferred from the system requirements
and speciÔ¨Åcations. For example, when designing an embedded sys-
tem, it is common for the requirements documents to specify a de-
sired accuracy range on physical sensors. If the potential exists for
a model-system mismatch to occur due to a mistake in reading in-
put from a sensor, than it would make sense to take that range as
a tolerance constraint on that sensor input and allow the steering
algorithm to try values within that range of the canonical test input.
We recommend that users err toward strict constraints. While
it is undesirable to spend time investigating failures that turn out
to be acceptable, that outcome is preferable to masking real faults.
Steering will not fully account for a model that produces incorrect
behavior, so steering should start with a mature, veriÔ¨Åed model.
To give an example, consider a pacemaker. The pacemaker might
take as input a set of prescription values, event indicators from sen-
sors in the patient‚Äôs heart, and timer values. We would recommend
that steering be prohibited from altering the prescription values at
all, as manipulation of those values might cover faults that could
threaten the life of a patient. However, as electrical noise or compu-
tation delays might lead to issues, steering should be allowed to al-
ter the values of the other inputs (within certain limits). The system
requirements might offer guidance on those limits‚Äîfor instance,
speciÔ¨Åying a time range from when a pace command is supposed
to be issued to when it must have been issued to be acceptable.
This boundary can be used as to constrain the manipulation of timer
variables. Furthermore, given the critical nature of a pacemaker, a
tester might also want to employ a policy where steering can only
intervene if a solution can be found that identically matches the
state of the SUT.
Unlike approaches that build nondeterminism into the model,
steering decouples the speciÔ¨Åcation of nondeterminism from the
model. This decoupling allows testers more freedom to experi-
ment with different sets of constraints and policies. If the initial
set of constraints leaves false failure verdicts or if testers lack con-
Ô¨Ådence in their chosen constraints, alternative options can easily be
explored by swapping in a new constraint Ô¨Åle and executing the test
suite again. Using the dissimilarity function to rate the set of Ô¨Ånal
test verdicts, testers can evaluate the severity and number of failure
verdicts remaining after steering with each set of constraints and
gain conÔ¨Ådence in their approach.
3.2 Automated Testing Framework
In a typical testing scenario that makes use of model-based ora-
cles, a test suite is executed against both the system under test and
the behavioral model. The values of the input, output, and select in-
ternal variables are recorded to a trace Ô¨Åle at certain intervals, such
as after each discrete cycle of input and output. Some comparison
mechanism examines those trace Ô¨Åles and issues a verdict for each
test case (generally a failure if any discrepancies are detected and
apass if a test executes without revealing any differences between
530Implementation 
Model Issues input to Output Trace Writes output to 
Comparator Test Suite 
Steerer Issues output to Issues output to 
Informs of 
scores. Informs of current 
state 
Issues state 
transition 
Result Log Writes results to 
Steering 
Constraints 
Limits steering actions Polls for 
new scores. Issues input to Figure 4: An automated testing framework employing steering.
the model and SUT). As illustrated in Figure 4, such a framework
can be modiÔ¨Åed to incorporate automated oracle steering.
No matter the size of the model, steering will add additional
overhead to the time required to execute tests. In practice, depend-
ing on the strictness of the constraints employed and the complexity
of the model, execution with steering can take anywhere between
a few additional seconds of execution to a few additional minutes.
Therefore, we recommend that testing take place ofÔ¨Çine, rather than
attempting to run the model and system in lockstep.
A framework would follow these steps:
1. Execute each test against the system under test, logging the
values of select variables at each test step.
2. Execute each test against the model-based oracle, and for
each step of the test:
(a) Feed input to the oracle model.
(b) Compare the model output to the SUT output.
(c) If the output does not match, the steering algorithm will
interface with the model, backtracking execution and
attempting to steer the model‚Äôs execution within the
speciÔ¨Åed constraints (perhaps consulting dissimilarity
metrics used by the comparison module)
(d) Compare the new output of the model to the SUT out-
put and log the new dissimilarity score.
3. Issue a Ô¨Ånal verdict for the test.
4. RELATED WORK
Several authors have examined the use of behavioral models as
test-generation targers for real-time systems [4, 20, 9, 29, 5]. If
such models are used to generate tests, they can implicitly serve as
a test oracle. For example, Larsen et al. [20] model a system as
a non-deterministic timed automata that is constrained by an en-
vironment model. The combined model serves as an oracle during
test execution. Arcuri et al. also examine the impact of the environ-
ment when testing process-control systems [4]. Their framework,
which only models the environment that the system interacts with
and eschews the system itself, allows limited non-determinism in
the time that an action can take place, as well as certain forms of
hardware-related non-determinism (through tester-provided proba-
bilities of hardware failure). Savor and Seviora proposed a frame-
work where the behavior of the SUT is compared to expected be-
haviors produced by a Ô¨Ånite state machine derived from the systemrequirements [29]. Their framework can handle non-determinism
in process communication by appending signals with an interval on
the time of occurence. This interval is used to construct alternative
legal event orderings. Briones and Brinksma treat quiescence‚Äî
the lack of system output‚Äîas a special form of system output, and
thus, offer a behavioral model that is able to capture a range of
non-deterministic response times from the system under test [5].
While the above approaches consider forms of non-determinism,
there are a few key differences with our proposed approach since
it decouples the model from the rules governing steering. This de-
coupling makes non-determinism implicit and the approach more
generally applicable. Explicitly speciÔ¨Åed non-deterministic behav-
ior would limit the scope of non-determinism handled by the oracle
to what has been planned for by the developer and subsequently
modeled. It is difÔ¨Åcult to anticipate the non-determinism resulting
from deploying software on a hardware platform, and, thus, such
models will likely undergo several revisions during development.
Steering instead relies on a set of rule-based constraints that may
be easier to revise over time. Additionally, by not relying on a
speciÔ¨Åc model format, steering can be made to work with models
created for a variety of purposes.
Oracle steering is conceptually similar to dynamic program steer-
ing, the automatic guidance of program execution [12, 23]. Much
of the research in dynamic program steering is concerned with au-
tomatic adaptation to maintain consistent performance or a cer-
tain reliability level when faced with depleted computational re-
sources [12]. However, Kannan et al. have proposed a framework
to assure the correctness of software execution at runtime through
corrective steering actions [19]. Their framework serves as a super-
visor on program execution, checking observed behavior at runtime
against a set of assertions and adjusting the behavior of the program
whenever an assertion is violated. This is done under the assump-
tion that the SUT is mostly correct, and that only minimal control
should be exercised. Although their system bears similarities to
what we are proposing, our goals are very different‚Äîrather than
adjusting the behavior of the live system, we apply steering to the
test oracle in order to better identify fault-indicitive behaviors.
More relevant to our work is Microsoft Research‚Äôs Spec Ex-
plorer [31] test generation framework. Their framework explores
the possible runs of the executable model by applying steering ac-
tions to the model and guiding it through various execution sce-
narios with the goal of systematically generating test suites. It can
then use the model as an oracle for the generated test by checking
whether the SUT produces the same behaviors, steering through
different execution scenarios for test generation as opposed to ad-
judging system execution. Although Spec Explorer also steers the
actions of a behavioral model, their application and goals greatly
differ from ours. They use steering to create tests, and do not apply
it when checking conformance. Therefore, their framework can-
not be used to address the instances of non-conformance related to
non-deterministic execution of interest in this report.
5. CASE STUDY
We aim to gain an understanding of the capabilities of oracle
steering and the impact it has on the testing process‚Äîboth positive
and negative. Thus, we pose the following research questions:
1. To what degree does steering lessen behavioral differences
that are legal under the system requirements?
2. To what degree does steering mask behavioral differences
that fail to conform to the requirements?
3. Are there situations where a Ô¨Åltering mechanism is more ap-
propriate than actively steering the oracle, and vice-versa?
5315.1 Experimental Setup Overview
We have based our model-based oracle on the management sub-
system of a generic Patient-Controlled Analgesia (GPCA) infusion
pump [25]. This subsystem takes in a prescription for a drug‚Äî
as well as several sensor values‚Äîand determines the appropriate
dosage of the drug to be administered to a patient each second
over a given period of time. This case example, developed in the
Simulink and StateÔ¨Çow notations and translated into the Lustre syn-
chronous programming language [14], is a complex real-time sys-
tem of the type common in the medical device domain. Details on
the size of the Simulink model and the number of lines of code in
the translated Lustre code are provided in Table 1.
Table 1: Case Example Information
# States # Transitions Lustre LOC
Infusion 23 50 6299
To evaluate oracle steering, we performed the following:
1.Generated system implementations: We approximated the
behavioral differences expected from systems running on real
embedded hardware by creating alternate versions of the model
with non-deterministic timing elements. We also generated
50 mutated versions of both the oracle and each ‚ÄùSUT‚Äù with
seeded faults (Section 5.2).
2.Generated tests: We randomly generated 100 tests for each
case example, each 30 test steps in length (Section 5.2).
3.Set steering constraints: We constrained the variables that
could be adjusted through steering and the values that those
variables could take on, and established dissimilarity metrics
to be minimized (Section 5.3).
4.Assessed impact of steering: For each combination of SUT,
test, and dissimilarity metric, we attempted to steer the ora-
cle to match the behavior of the SUT. We compare the test
results before and after steering and evaluate the precison
and recall of our steering framework, contrasted against the
general practice of not steering and a step-by-step Ô¨Åltering
mechanism (Section 5.4).
5.2 System and Test Generation
To produce ‚Äúimplementations‚Äù of the example system, we cre-
ated alternative versions of the model, introducing realistic non-
deterministic timing changes to the systems. We built (1) a version
of the system where the exit of the patient-requested dosage pe-
riod may be delayed by a short period of time, and (2) a version
of the system where the exit of an intermittent increased dosage
period (known as a square bolus dose) may be delayed. These
changes are intended to mimic situations where, due to hardware-
introduced computation delays, the system remains in a particular
dosage mode for longer than expected.
For the original model and the ‚Äúsystem under test‚Äù variants, we
have also generated 50 mutants (faulty implementations) by intro-
ducing a single fault into each model. This ultimately results in
a total of 152 SUT versions‚Äîtwo versions with non-deterministic
timing behavior, Ô¨Åfty versions with faults, and one hundred ver-
sions with both non-deterministic timing and seeded faults (Ô¨Åfty
per timing variation).
The mutation testing operators used in this experiment include
changing an arithmetic operator, changing a relational operator,
changing a boolean operator, introducing the boolean :operator,
using the stored value of a variable from the previous computa-
tional cycle, changing a constant expression by adding or subtract-
ing 1 from int and real constants (or by negating boolean constants),and substituting a variable occurring in an equation with another
variable of the same type. The mutation operators used are dis-
cussed at length in a previous report on empirical software testing
research [28], and are similar to those used by Andrews et.al, where
the authors found that generated mutants are a reasonable substitute
for actual failures in testing experiments [3].
Using a random testing algorithm, we generated 100 tests, each
thirty test steps long. Each test represents thirty seconds of sys-
tem activity‚Äîa length appropriate to capture a relevant range of
time-sensitive behaviors, but still short enough to yield a reason-
able experiment cost. These tests were then executed against each
model in order to collect traces. In the models with timing Ô¨Çuc-
tuations, we controlled those Ô¨Çuctuations through the use of an
additional input variable. The value for that variable was gener-
ated non-deterministically, but we used the same value across all
systems with the same timing Ô¨Çuctuation. As a result, we know
whether a resulting behavioral mismatch is due to a seeded timing
Ô¨Çuctuation or a seeded fault in the system. Using this knowledge,
we labeled each test that fails pre-steering as failing due to an ‚Äúac-
ceptable timing deviation‚Äù, an ‚Äúunacceptable timing deviation‚Äù, or
a ‚Äúseeded fault.‚Äù
5.3 Steering Constraints
For this particular case study, we have speciÔ¨Åed the tolerance
constraints on steering in terms of limits on the adjustment of the
input variables of the system (our model-based oracle steering frame-
work allows constraints to be placed on internal or output variables
as well). The chosen tolerance constraints include:
Five of the input variables relate to timers within the system‚Äî
the duration of the patient-requested bolus dose period, the
duration of the intermittent square bolus dosage period, the
lockout period between patient-requested bolus dosages, the
interval between intermittent square bolus dosages, and the
total duration of the infusion period. For each of those, we
placed an allowance of
(CurrV al 1)<=NewV al < = (CurrV al + 2).
E.g., following steering, a dosage duration is allowed to fall
within a three second period‚Äîbetween one second shorter
and two seconds longer than the original prescribed duration.
The remaining 15 input variables are not allowed to be steered.
These constraints reÔ¨Çect what we consider a realistic application
of steering‚Äîwe expect issues related to non-deterministic timing,
and, thus, allow a small acceptable window around the behaviors
that are related to timing. In this study, we do not expect any sensor
inaccuracy, so we do not allow freedom in adjusting sensor-based
inputs. Similarly, as these are medical devices that could harm a
patient if misused, we do not allow any changes to the inputs related
to prescription values.
In this experiment, we have made use of two different dissimi-
larity metrics when comparing a candidate state of the model-based
oracle to the state of the SUT. The Ô¨Årst is the Manhattan (or City
Block) distance. Given vectors representing the state of the SUT
and the model-based oracle‚Äîwhere each member of the vector
represents the value of a variable‚Äîthe dissimilarity between the
two vectors can be measured as the sum of the absolute numerical
distance between the state of the SUT and the model-based oracle:
Dis(sm; ssut) =nX
i=1jsm;i ssut;ij (1)
The second is the Squared Euclidean distance. Given vectors rep-
resenting the state, the dissimilarity between the vectors can be
532measured as the ‚Äústraight-line‚Äù numerical distance between the two
vectors. The squared variant was chosen because it places greater
weight on states that are further apart in terms of variable values.
Dis(sm; ssut) =nX
i=1(sm;i ssut;i )2(2)
A constant difference of 1 is used for differences between boolean
variables or values of an enumerated variable. All numerical values
are normalized to a 0-1 scale using predetermined constants for the
minimum and maximum values of each variable.
We must choose a set of variables to compare when calculat-
ing a dissimilarity score (or oracle verdict). As we cannot assume
that the internal variables of the SUT and the model are the same,
we calculate similarity using the Ô¨Åve output variables of the infu-
sion pump: the commanded Ô¨Çow rate, the current system mode,
the duration of active infusion, a log message indicator, and a Ô¨Çag
indicating that a new infusion has been requested.
5.4 Evaluation
Using the generated artifacts‚Äîwithout steering‚Äîwe monitored
the outputs during each test, compared the results to the values of
the same variables in the model-based oracle to calculate the dis-
similarity score, and issued an initial verdict. Then, if the verdict
was a failure (Dis(s m:ssut)>0), we steered the model-based
oracle, and recorded a new verdict post-steering. As mentioned
above, the variables used in establishing a verdict are the Ô¨Åve out-
put variables of the system.
In Section 3, we stated that an alternative approach to steering
would be to simply apply a Ô¨Ålter on a step-by-step basis. We have
implemented such a Ô¨Ålter for the purposes of establishing a base-
line to which we can compare the performance of steering. This
Ô¨Ålter compares the values of the output variables of the SUT to
the values of those variables in the model-based oracle and, if they
do not match, checks those values against a set of constraints. If
the output‚Äîdespite non-conformance to the model-based oracle‚Äî
meets these constraints, the Ô¨Ålter will still issue a ‚Äúpass‚Äù verdict
for the test. The Ô¨Ålter will allow a test to pass if (despite non-
conformance) values of the output variables in the SUT satisfy the
following constraints:
The current mode of the SUT is either ‚Äúpatient dosage‚Äù mode
or ‚Äúintermittent dosage‚Äù mode, and has not remained in that
mode for longer than prescribed duration + 2seconds.
If the above is true, the commanded Ô¨Çow rate should match
the prescribed value for the appropriate mode.
All other output variables should match their corresponding
variables in the oracle
As we expect a non-deterministic duration for the patient dosage
and intermittent dosage modes (corresponding to the seeded issues
in the SUT variants), this Ô¨Ålter should be able to correctly classify
many of the same tests that we expect steering to handle.
We compare the performance of the steering approach to both
the Ô¨Ålter and the default practice of accepting the initial test verdict.
We can assess the impact of steering or Ô¨Åltering using the verdicts
made before and after steering by calculating:
The number of true positives‚Äîsteps where an approach does
not mask incorrect behavior;
The number of false positives‚Äîsteps where an approach fails
to account for an acceptable behavioral difference;
And the number of false negatives‚Äîsteps where an approach
does mask an incorrect behavior.Table 2: Verdicts: T(true)/F(false), P(positive)/N(negative).
Initial Verdict Pass (Post-Steering) Fail (Post-Steering)
Pass TN FP
Fail (Due to Timing,
Within Tolerance)TN FP
Fail (Due to Timing,
Not in Tolerance)FN TP
Fail (Due to Fault) FN TP
Table 3: Initial test results when performing no steering or Ô¨Åltering.
Raw number of test results, followed by percent of total.
Verdict Number of Tests
Pass 11364 (74.8%)
Fail (Due to Timing,
Within Tolerance)1438 (9.4%)
Fail (Due to Timing,
Not in Tolerance)268 (1.7%)
Fail (Due to Fault) 2230 (14.7%)
The testing outcomes in terms of true/false positives/negatives
are listed in Table 2. Using these measures, we calculate the pre-
cision‚Äîthe ratio of true positives to all positive verdicts‚Äîand re-
call‚Äîthe ratio of true positives to true positives and false negatives:
Precision =TP
TP +FP(3)
Recall =TP
TP +FN(4)
We also calculate the F-measure‚Äîthe harmonic mean of precision
and recall‚Äîin order to judge the accuracy of oracle verdicts:
F-measure = 2precisionrecall
precision +recall(5)
6. RESULTS AND DISCUSSION
When running all tests over the various implementations (con-
taining either timing deviations or seeded faults as discussed in
Section 5.2) using a standard test oracle comparing the outputs
from the SUT with the outputs predicted by the model-based or-
acle (15,200 test runs), 11,364 runs indicated that the system under
test passed the test (the SUT and model-based oracle agreed on the
outputs) and 3,936 runs indicated that the test failed (the SUT and
model-based oracle had mismatched outputs). In an industry appli-
cation of a model-based oracle, the 3,936 failed test would have to
be examined to determine if the failure was due to an actual fault in
the implementation, an unacceptable timing deviation from the ex-
pected timing behavior, or an acceptable timing deviation that, al-
though it did not match the behavior predicted by the model-based
oracle, was within acceptable tolerances‚Äîa costly process. Given
our experimental setup, however, we can classify the failed tests as
to the cause of the failure: failure due to timing within tolerances,
failure due to timing not in tolerance, and failure due to a fault in
the SUT. This breakdown is provided in Table 3. As can be seen,
1,438 tests failed even though the timing deviation was within what
would be acceptable‚Äîthese can be viewed as false positives and a
Ô¨Åltering or steering approach that would have passed these test runs
would provide cost savings. On the other hand, the steering or Ô¨Ål-
tering should not pass any of the 268 tests where timing behavior
falls outside of tolerance or the 2,230 tests that indicated real faults.
Results obtained from the case study showing the effect of steer-
ing on oracle verdicts are summarized in Tables 4 and 5 respec-
tively, for the two different distance metrics studied. The numbers
533Table 4: Distribution of results for steering (Squared Euclidean dis-
similarity). Raw number of test results, followed by percent of total.
Initial Verdict Pass (Post-Steering) Fail (Post-Steering)
Pass 11364 (74.8%) 0 (0.0%)
Fail (Due to Timing,
Within Tolerance)1286 (8.4%) 152 (1.0%)
Fail (Due to Timing,
Not in Tolerance)0 (0.0%) 268 (1.7%)
Fail (Due to Fault) 43 (0.3%) 2187 (14.4%)
Table 5: Distribution of results for steering (Manhattan dissimilarity).
Raw number of test results, followed by percent of total.
Initial Verdict Pass (Post-Steering) Fail (Post-Steering)
Pass 11364 (74.8%) 0 (0.0%)
Fail (Due to Timing,
Within Tolerance)1198 (7.9%) 240 (1.6%)
Fail (Due to Timing,
Not in Tolerance)0 (0.0%) 268 (1.7%)
Fail (Due to Fault) 43 (0.3%) 2187 (14.4%)
Table 6: Distribution of results for step-wise Ô¨Åltering. Raw number of
test results, followed by percent of total.
Initial Verdict Pass (Post-Filtering) Fail (Post-Filtering)
Pass 11364 (74.8%) 0 (0.0%)
Fail (Due to Timing,
Within Tolerance)1286 (8.4%) 152 (1.0%)
Fail (Due to Timing,
Not in Tolerance)0 (0.0%) 268 (1.7%)
Fail (Due to Fault) 1253 (8.2%) 977 (6.4%)
Table 7: Precision, recall, and F-measure values.
Technique Precision Recall F-measure
No Steering 0.63 1.00 0.77
Filtering 0.89 0.50 0.64
Steering - Euclidean 0.94 0.98 0.96
Steering - Manhattan 0.91 0.98 0.94
are presented as laid out in Table 2. Testing outcomes are Ô¨Årst
categorized according to the initial verdicts as determined by the
model-based oracle before steering; a ‚Äúfail‚Äù verdict is further de-
lineated according to its reason‚Äîa mismatch that is attributable to
either an allowable timing Ô¨Çuctuation, or an unacceptable timing
Ô¨Çuctuation or a fault. For each category, the post-steering verdict is
presented as both a raw number of test outcomes and as a percent-
age of total test outcomes. Table 6 shows the corresponding data
for the step-by-step Ô¨Åltering approach. Data from these tables lead
to the precision, recall, and F-measure values, shown in Table 7,
for the default testing scenario (accepting the initial oracle verdict),
steering utilizing two different dissimilarity metrics, and Ô¨Åltering.
6.1 Allowing Tolerable Non-Conformance
According to Table 3, 11.1% of the tests (1,706 tests) initially
fail due to timing-related non-conformance. Of those, 1438 tests
(9.4% of the total) fall within the tolerances set in the requirements.
Steering should result in a pass verdict for all those tests.
As the tables show, for both dissimilarity metrics, steering is able
to account for almost all of the situations where non-deterministic
timing affects conformance while both the model-based oracle and
the implementation remain within the bounds set in the system
speciÔ¨Åcation. For example, in Table 4 we see that steering usingthe square Euclidian distance would have correctly passed 1,286
tests where the timing deviation was acceptable‚Äîtests that with-
out steering failed. Therefore, we see a sharp increase in precision
over the default situation where no steering is employed (from 0.63
when not steering, to 0.94 for the Squared Euclidean metric, and to
0.91 for the Manhattan metric, see Table 7).
Where previously developers would have had to manually in-
spect the more than 25% of all the tests (the sum of all ‚ÄúFail‚Äù Ta-
ble 4) to determine the causes for their failures (system faults or
otherwise), they could now narrow their focus to the roughly 17%
that still result in failure post-steering. Particularly given the large
number of tests in this study, this reduction represents a signiÔ¨Åcant
savings in time and effort, removing between 1,198 and 1,286 tests
that the developer would have needed to inspect manually. Still,
there were a small number of tests that steering should have been
able to account for (152 for the Squared Euclidean metric and 240
for the Manhattan metric). The reason for the failure of steering to
account for allowable differences can be attributed to a combination
of three factors: the tolerance constraints employed, the dissimilar-
ity metric employed, and design differences between the SUT and
the model-based oracle.
First, it may be that the tolerance constraints were too strict to
allow for situations that should have been considered legal. Con-
straints should be relatively strict‚Äîafter all, we are overriding the
nominal behavior of the oracle while simultaneously wishing to
retain the oracle‚Äôs power to identify faults. Yet, the constraints
we apply should be carefully designed to allow steering to han-
dle these allowed non-conformance events. In this case, the chosen
constraints may have prevented steering from acting.
Second, the dissimilarity metric does appear to play a small role
in the effectiveness of steering. The Squared Euclidean metric re-
sulted in higher precision than the Manhattan metric (0.94 vs 0.91),
indicating that the former was better able to account for tolerable
non-conformance. As there is no difference in recall between the
two metrics, it appears that‚Äîfor this case example‚Äîthe use of the
Manhattan metric results in a slightly more conservative steering
process. With the Manhattan metric, the steering approach is more
likely to refuse to steer. However, given the similarity in the per-
formance of the two metrics, it appears that the set of constraints
employed plays a more dominant role in determining the Ô¨Ånal out-
come of steering.
The tolerance constraints applied reduce the space of candidate
targets to which the oracle may be steered. We then use the dissimi-
larity metric to choose a ‚Äúnearest‚Äù target from that set of candidates.
Thus, the relationship between the constraints and the metric ulti-
mately determines the power of the steering process. However, no
matter how powerful steering is, there may be situations where dif-
ferences in the internal design of the systems render steering either
ineffective or incorrect. We base steering decisions on state-based
comparisons, but those comparisons can only be made on the por-
tion of the state variables common between the SUT and oracle
model. For this experiment, we only compared the output vari-
ables. As a result, there may be situations where we should have
steered, but could not, as the state of the SUT depended on internal
factors not in common with the oracle. In general, as the oracle and
SUT are both ultimately based on the same set of requirements, we
believe that some kind of relationship can be established between
the internal variables of both realizations. However, in some cases,
the model and SUT may be too different to allow for steering in all
allowable situations. The inability of steering to account for tolera-
ble differences for at least some tests in this case study can likely be
attributed to the changes made to the SUT versions of the models.
5346.2 Masking of Faults
As steering changes the behavior of the oracle and can result in
a new test verdict, the danger of steering is that it will mask actual
faults in the system. Such a danger is concerning, but with the
proper choice of steering policies and constraints, we hypothesize
that such a risk can be reduced to an acceptable level.
In this case study, steering changed a fault-induced ‚Äúfail‚Äù verdict
to ‚Äúpass‚Äù in forty-three tests. This is a relatively small number‚Äî
only 0.3% of the 15,200 tests. Although loss in recall is cause for
concern when working with safety-critical systems, given the small
number of incorrectly adjusted test verdicts, we hypothesize that it
is unlikely for an actual fault to be entirely masked by steering on
every test in which the fault would otherwise lead to a failure.
Just as the choice of tolerance constraints can explain cases where
steering is unable to account for an allowable non-conformance,
the choice of constraints has a large impact on the risk of fault-
masking. At any given execution step, steering, as we have deÔ¨Åned
here, considers only those oracle post-states as candidate targets
that are reachable from the the given oracle pre-state. However, this
by itself is not sufÔ¨Åciently restrictive to rule out truly deviant behav-
iors. Therefore, the constraints applied to reduce that search space
must be strong enough to prevent steering from forcing the oracle
into an otherwise impermissible state for that execution step. It is,
therefore, crucial that proper consideration goes into the choice of
constraints. In some cases, the use of additional policies‚Äîsuch as
not steering the oracle model at all if it does not result in an ex-
act match with the system‚Äîcan also lower the risk of tolerating
behaviors that would otherwise indicate faults.
Note that a seeded fault could cause a timing deviation (or the
same behavior that would result from a timing deviation). In those
cases, the failure is still labeled as being induced by a fault for our
experiment. However, if the fault-induced deviation falls within
the tolerances, steering will be able to correct it. In such cases,
it is unlikely that even a human oracle would label the outcome
differently, as they are working from the same tolerance limits.
If care is taken when deriving the tolerance constraints from the
system requirements, steering should not cover any behaviors that
would not be permissable under those same requirements. Still, as
steering carries the risk of masking faults, we recommend that it
be applied as a focusing tool ‚Äîto point the developer toward test
failures likely to indicate faults so that they do not spend as much
time investigating non-conformance reports that turn out to be al-
lowable. The Ô¨Ånal verdict on a test should come from a run of the
oracle model with no steering, but during development, steering
can be effective at streamlining the testing process by concentrat-
ing resources on those failures that are more likely to point to faults.
6.3 Steering vs Filtering
In some cases, allowable non-conformance events could simply
be dealt with by applying a Ô¨Ålter that, in the case of a failing test
verdict, checks the resulting state of the SUT against a set of con-
straints and overrides the initial oracle verdict if those constraints
are met. The use of a Ô¨Ålter is tempting‚Äîif the Ô¨Ålter is effective, it is
likely to be easier to build and faster to execute than a full steering
process. Indeed, the results in Table 6 appear promising. The Ô¨Ål-
ter performs identically to steering for the initial failures that result
from non-deterministic timing differences. It does not issue a pass
verdict for timing issues outside of the tolerance limits, and it does
issue a pass for almost all of the tests where non-conformance is
within the tolerance bounds.
However, when the results for tests that fail due to faults are
considered, a Ô¨Ålter appears much less attractive. The Ô¨Ålter issues a
passing verdict for 1,253 tests that should have failed‚Äî1,210 morethan steering. This is because a Ô¨Ålter is a blunt instrument. It simply
checks whether the state of the SUT meets certain constraints when
non-conformance occurs. This allowed the Ô¨Ålter to account for the
allowed non-conforming behaviors, but these same constraints also
allowed a large selection of fault-indicating tests to pass.
This makes the choice of constraints even more important for
Ô¨Åltering than it is in steering. The steering process, by backtracking
the state of the system, is able to ensure that the resulting behavior
of the SUT is even possible (that is, if the new state is reachable
from the previous state). The Ô¨Ålter does not check the possibility
of reaching a state; it just checks whether the new state is globally
acceptable under the given constraints. As a result, steering is far
more accurate. A Ô¨Ålter could, of course, incorporate a reachability
analysis. However, as the complexity of the Ô¨Ålter increases, the
reasons for Ô¨Åltering instead of steering disappear.
In fact, the success of steering at accounting for allowable non-
conformance is somewhat misleading for this case example. Both
Ô¨Åltering and steering base their decisions on the output variables
of the SUT and oracle, on the basis that the internal state vari-
ables may differ between the two. For this case study, all of the
output variables reÔ¨Çect current conditions of the infusion pump‚Äî
how much drug volume to infuse now, the current system mode,
and so forth. Internally, these factors depend on both the current
inputs and a number of cumulative factors, such as the total vol-
ume infused and the remaining drug volume. Over the long term,
non-conformance events between the SUT and model will build,
eventually leading to wider divergence. For example, the SUT or
the model-based oracle may eventually cut off infusion if the drug
reservoir empties.
As the output variables reÔ¨Çect current conditions, mounting in-
ternal differences may be missed, and the Ô¨Ålter may not be able
to cope with large-scale behavior differences that result from this
steady divergence. Steering is able to prevent and account for these
long-term divergences by actually changing the state of the oracle
throughout the execution of the test. A Ô¨Ålter simply overrides the
oracle verdict. It does not change the state of the oracle, and as a
result, a Ô¨Ålter cannot predict or handle behavioral divergences once
they build beyond the set of constraints that the Ô¨Ålter applies.
We can illustrate this effect by adding a single internal variable
to the set of variables considered when making Ô¨Åltering or steer-
ing conditions‚Äîa variable tracking the total drug volume infused.
Adding this variable causes no change to the results of steering seen
in Tables 4 and 5. However, the addition of this internal variable
dramatically changes the results of Ô¨Åltering. The new results can
be seen in Tables 8 and 9.
Table 8: Distribution of results for step-wise Ô¨Åltering, (outputs + vol-
ume infused oracle). Raw number of test results, followed by percent
of total.
Initial Verdict Pass (Post-Filtering) Fail (Post-Filtering)
Pass 11311 (74.4%) 0 (0.0%)
Fail (Due to Timing,
Within Tolerance)312 (2.1%) 1123 (7.4%)
Fail (Due to Timing,
Not in Tolerance)0 (0.0%) 268 (1.7%)
Fail (Due to Fault) 598 (3.9%) 1688 (11.1%)
Because the total volume infused increases over the execution of
the test, it will reÔ¨Çect any divergence between the model-based or-
acle and the SUT. As steering actually adjusts the execution of the
model-based oracle, this volume counter also adjusts to reÔ¨Çect the
changes induced by steering. Thus, steering is able to account for
the growing difference in the volume infused by the model-based
535Table 9: Precision, recall, and F-measure values for Ô¨Åltering (outputs
+ volume infused oracle).
Technique Precision Recall F-measure
Filtering 0.64 0.76 0.70
oracle and the volume infused by the SUT. However, as the Ô¨Ål-
ter makes no such adjustment, it is unable to handle the mounting
difference in this variable (or any other considered variable that re-
Ô¨Çects change over time). The Ô¨Ålter, even if initially effective, will
fail to account for a large number of acceptable non-conformance
events‚Äîultimately resulting in a precision value barely more effec-
tive than not doing anything at all.
6.4 Summary of Results
The precision, recall, and F-measure (a measure of accuracy) for
each method‚Äîaccepting the initial verdict, steering (using two dif-
ferent dissimilarity metrics), and Ô¨Åltering‚Äîare shown in Table 7.
The default situation, accepting the initial verdict, results in the
lowest precision value. Intuitively, not doing anything to account
for allowed non-conformance will result in a large number of in-
correct ‚Äúfail‚Äù verdicts. However, the default practice does have the
largest recall value. Again, not adjusting your results will prevent
incorrect masking of faults. Filtering on a step-by-step basis results
in higher precision, but due to the lack of reachability analysis and
state adaptation‚Äîboth of which used by the steering approach‚Äî
the Ô¨Ålter masks an unacceptably large number of faults.
Steering performs similarly for both of the dissimilarity metrics
used in this study. It is able to adapt the oracle to handle almost
every situation where non-conforming behaviors are allowed by the
system requirements, while masking only a few faults in a small
number of tests. The Squared Euclidean metric results in a higher
precision, while both metrics obtain an equal recall value.
Ultimately, we Ô¨Ånd that steering‚Äîemploying the Squared Eu-
clidean dissimilarity metric‚Äîresults in the highest accuracy for the
Ô¨Ånal test results. Steering is able to automatically adjust the execu-
tion of the oracle to handle non-deterministic, but acceptable, be-
havioral divergence without covering up most fault-indicative be-
haviors. We, therefore, recommend the use of steering as a tool for
focusing and streamlining the testing process.
7. THREATS TO V ALIDITY
External Validity: Our study is limited to one case example.
We are actively working with domain experts to produce additional
systems for future studies. We believe this systems to be represen-
tative of the real-time embedded systems that we are interested in,
and that our results will generalize to other systems in this domain.
We have used Simulink and Lustre as our modeling and imple-
mentation languages rather than more common languages such as
C or C++. However, systems written in Lustre are similar in style
to traditional imperative code produced by code generators used in
embedded systems development. A simple syntactic transforma-
tion is sufÔ¨Åcient to translate Lustre code to C code.
We have limited our study to Ô¨Åfty mutants for each version of the
case example, resulting in a total of 150 mutants. These values are
chosen to yield a reasonable cost for the study, particularly given
the length of each test. It is possible the number of mutants is
too low. Nevertheless, we have found results using less than 250
mutants to be representative [27, 28], and pilot studies have shown
that the results plateau when using more than 100 mutants.
Internal Validity: Rather than develop full-featured system im-
plementations for our study, we instead created alternative versionsof the model‚Äîintroducing various non-deterministic behaviors‚Äî
and used these models and the versions with seeded faults as our
‚Äúsystems under test.‚Äù We believe that these models are representa-
tive approximations of the behavioral differences we would see in
systems running on embedded hardware. In future work, we plan
to generate code from these models and execute the software on
actual hardware platforms.
In our experiments, we used a default testing scenario (accepting
the oracle verdict) and stepwise Ô¨Åltering as baseline methods for
comparison. There may be other techniques‚Äîparticularly, other
Ô¨Ålters‚Äîthat we could compare against. Still, we believe that the
Ô¨Ålter chosen was an acceptable comparison point, and was designed
as such a Ô¨Ålter would be in practice.
Construct Validity: We measure the fault Ô¨Ånding of oracles and
test suites over seeded faults, rather than real faults encountered
during development of the software. Given that our approach to
selecting oracle data is also based on the mutation testing, it is pos-
sible that using real faults would lead to different results. This is
especially likely if the fault model used in mutation testing is sig-
niÔ¨Åcantly different than the faults we encounter in practice. Nev-
ertheless, as mentioned earlier, Andrews et al. have shown that the
use of seeded faults leads to conclusions similar to those obtained
using real faults in similar fault Ô¨Ånding experiments [2].
8. CONCLUSION AND FUTURE WORK
Specifying test oracles is still a major challenge for many do-
mains, particularly those‚Äîsuch as real-time embedded systems‚Äî
where issues related to timing, sensor inaccuracy, or the limited
computation power of the embedded platform may result in non-
deterministic behaviors for multiple applications of the same input.
Behavioral models of systems, often built for analysis and simula-
tion, are appealing for reuse as oracles. However, these models typ-
ically provide an idealized view of the system, and may struggle to
differentiate unexpected‚Äîbut still acceptable‚Äîbehavior from be-
haviors indicitive of a fault.
To address this challenge, we have proposed an automated model-
based oracle steering framework that, upon detecting a behavioral
difference, backtracks and transitions the model-based oracle, throu-
gh a search process, to a state that satisÔ¨Åes certain constraints and
minimizes a dissimilarity metric. This framework allows non-deter-
ministic, but bounded, behavior differences while preventing future
mismatches by guiding the model-based oracle‚Äîwithin limits‚Äîto
match the execution of the SUT. Experiments, conducted over an
infusion pump system, have yielded promising results and indicate
that steering signiÔ¨Åcantly increases SUT-oracle conformance with
minimal masking of real faults and, thus, has signiÔ¨Åcant potential
for reducing development costs. The use of our steering framework
can allow developers to focus on behavioral difference indicative of
real faults, rather than spending time examining test failure verdicts
that can be blamed on a rigid oracle model.
There is still much room for future work. We plan to expand on
the selection of case examples in terms of both program size and
scope of non-determinism, as well as examining:
The impact of different dissimilarity metrics, tolerance con-
straints, and steering policies on oracle verdict accuracy;
Improvements to the speed and scalability of the steering
framework, including the use of alternative search algorithms;
The use of steering and dissimilarity metrics as methods of
quantifying non-conformance and their utility in fault identi-
Ô¨Åcation and location.
5369. REFERENCES
[1] IBM Rational Rhapsody.
http://www.ibm.com/developerworks/rational/
products/rhapsody/, 2014.
[2] J. Andrews, L. Briand, and Y . Labiche. Is mutation an
appropriate tool for testing experiments? Proc of the 27th
Int‚Äôl Conf on Software Engineering (ICSE), pages 402‚Äì411,
2005.
[3] J. Andrews, L. Briand, Y . Labiche, and A. Namin. Using
mutation analysis for assessing and comparing testing
coverage criteria. Software Engineering, IEEE Transactions
on, 32(8):608 ‚Äì624, aug. 2006.
[4] A. Arcuri, M. Z. Iqbal, and L. Briand. Black-box system
testing of real-time embedded systems using random and
search-based testing. In Proceedings of the 22nd IFIP WG
6.1 Int‚Äôl Conf. on Testing software and systems, pages
95‚Äì110. Springer-Verlag, 2010.
[5] L. B. Briones and E. Brinksma. A test generation framework
for quiescent real-time systems. In IN FATES‚Äô04, pages
64‚Äì78. Springer-Verlag GmbH, 2004.
[6] S.-H. Cha. Comprehensive survey on distance/similarity
measures between probability density functions.
International Journal of Mathematical Models and Methods
in Applied Sciences, 1(4):300‚Äì307, 2007.
[7] D. Coppit and J. Haddox-Schatz. On the use of
speciÔ¨Åcation-based assertions as test oracles. In Proceedings
of the 29th Annual IEEE/NASA on Software Engineering
Workshop, SEW ‚Äô05, pages 305‚Äì314, Washington, DC,
USA, 2005. IEEE Computer Society.
[8] L. De Moura and N. Bj√∏rner. Z3: An efÔ¨Åcient smt solver. In
Tools and Algorithms for the Construction and Analysis of
Systems, pages 337‚Äì340. Springer, 2008.
[9] A. En-Nouaary, R. Dssouli, and F. Khendek. Timed
wp-method: testing real-time systems. Software Engineering,
IEEE Transactions on, 28(11):1023‚Äì1038, Nov.
[10] G. Gay, S. Rayadurgam, and M. P. Heimdahl. Steering
model-based oracles to admit real program behaviors. In
Proceedings of the 36th International Conference on
Software Engineering ‚Äì NIER Track, ICSE ‚Äô14, New York,
NY , USA, 2014. ACM.
[11] A. Gomes, A. Mota, A. Sampaio, F. Ferri, and E. Watanabe.
Constructive model-based analysis for safety assessment.
Int‚Äôl Journal on Software Tools for Technology Transfer,
14(6):673‚Äì702, 2012.
[12] W. Gu, J. Vetter, and K. Schwan. An annotated bibliography
of interactive program steering. ACM SIGPLAN Notices, 29,
1994.
[13] G. Hagen. Verifying safety properties of Lustre programs: an
SMT-based approach. PhD thesis, University of Iowa,
December 2008.
[14] N. Halbwachs. Synchronous Programming of Reactive
Systems. Klower Academic Press, 1993.
[15] D. Harel, H. Lachover, A. Naamad, A. Pnueli, M. Politi,
R. Sherman, A. Shtull-Trauring, and M. Trakhtenbrot.
Statemate: A working environment for the development of
complex reactive systems. IEEE Transactions on Software
Engineering, 16(4):403‚Äì414, April 1990.
[16] M. Harman, P. McMinn, M. Shahbaz, and S. Yoo. A
comprehensive survey of trends in oracles for software
testing. Technical Report CS-13-01, University of ShefÔ¨Åeld,
Department of Computer Science, 2013.[17] W. Howden. Theoretical and empirical studies of program
testing. IEEE Transactions on Software Engineering,
4(4):293‚Äì298, 1978.
[18] M. S. Jaffe, N. G. Leveson, M. P. Heimdahl, and B. E.
Melhart. Software requirements analysis for real-time
process-control systems. IEEE Transactions on Software
Engineering, 17(3):241‚Äì258, March 1991.
[19] S. Kannan, M. Kim, I. Lee, O. Sokolsky, and
M. Viswanathan. Run-time monitoring and steering based on
formal speciÔ¨Åcations. In Workshop on Modeling Software
System Structures in a Fastly Moving Scenario, 2000.
[20] K. G. Larsen, M. Mikucionis, and B. Nielsen. Online testing
of real-time systems using UPPAAL. In Int‚Äôl workshop on
formal approaches to testing of software (FATES 04).
Springer, 2004.
[21] D. Lee and M. Yannakakis. Principles and methods of testing
Ô¨Ånite state machines-a survey. Proceedings of the IEEE,
84(8):1090‚Äì1123, 1996.
[22] MathWorks Inc. StateÔ¨Çow.
http://www.mathworks.com/stateÔ¨Çow.
[23] D. Miller, J. Guo, E. Kraemer, and Y . Xiong. On-the-Ô¨Çy
calculation and veriÔ¨Åcation of consistent steering
transactions. In Supercomputing, ACM/IEEE 2001 Conf.,
pages 8‚Äì8, 2001.
[24] S. P. Miller, A. C. Tribble, M. W. Whalen, and M. P. E.
Heimdahl. Proving the shalls: Early validation of
requirements through formal methods. Int. J. Softw. Tools
Technol. Transf., 8(4):303‚Äì319, 2006.
[25] A. Murugesan, S. Rayadurgam, and M. Heimdahl. Modes,
features, and state-based modeling for clarity and Ô¨Çexibility.
InProceedings of the 2013 Workshop on Modeling in
Software Engineering, 2013.
[26] M. Pezze and M. Young. Software Test and Analysis:
Process, Principles, and Techniques. John Wiley and Sons,
October 2006.
[27] A. Rajan, M. Whalen, and M. Heimdahl. The effect of
program and model structure on MC/DC test adequacy
coverage. In Proc. of the 30th Int‚Äôl Conf. on Software
engineering, pages 161‚Äì170. ACM, 2008.
[28] A. Rajan, M. Whalen, M. Staats, and M. Heimdahl.
Requirements coverage as an adequacy measure for
conformance testing, 2008.
[29] T. Savor and R. Seviora. An approach to automatic detection
of software failures in real-time systems. In Real-Time
Technology and Applications Symposium, 1997.
Proceedings., Third IEEE, pages 136‚Äì146, 1997.
[30] M. Staats, G. Gay, and M. Heimdahl. Automated oracle
creation support, or: how I learned to stop worrying about
fault propagation and love mutation testing. In Proceedings
of the 2012 Int‚Äôl Conf. on Software Engineering, pages
870‚Äì880. IEEE Press, 2012.
[31] M. Veanes, C. Campbell, W. Grieskamp, W. Schulte,
N. Tillmann, and L. Nachmanson. Model-based testing of
object-oriented reactive systems with spec explorer. In R. M.
Hierons, J. P. Bowen, and M. Harman, editors, Formal
Methods and Testing, volume 4949 of Lecture Notes in
Computer Science, pages 39‚Äì76. Springer, 2008.
[32] E. Weyuker. The oracle assumption of program testing.
537