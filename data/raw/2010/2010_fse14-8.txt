An Empirical Analysis of Flaky Tests
Qingzhou Luo, Farah Hariri, Lamyaa Eloussi, Darko Marinov
Department of Computer Science, University of Illinois at U rbana-Champaign
Urbana, IL 61801, USA
{qluo2, hariri2, eloussi2, marinov}@illinois.edu
ABSTRACT
Regression testing is a crucial part of software developmen t.
It checks that software changes do not break existing func-
tionality. An important assumption of regression testing i s
that test outcomes are deterministic: an unmodiﬁed test is
expected to either always pass or always fail for the same
code under test. Unfortunately, in practice, some tests—
often called ﬂaky tests —have non-deterministic outcomes.
Such tests undermine the regression testing as they make it
diﬃcult to rely on test results.
We present the ﬁrst extensive study of ﬂaky tests. We
study in detail a total of 201 commits that likely ﬁx ﬂaky
tests in 51 open-source projects. We classify the most com-
monrootcausesofﬂakytests, identifyapproachesthatcoul d
manifest ﬂaky behavior, and describe common strategies
that developers use to ﬁx ﬂaky tests. We believe that our
insights and implications can help guide future research on
the important topic of (avoiding) ﬂaky tests.
Categories and Subject Descriptors: D.2.5 [Software
Engineering ]: Testing and Debugging
General Terms: Measurement, Reliability
Keywords: Empirical study, ﬂaky tests, non-determinism
1. INTRODUCTION
Regression testing is a crucial part of software develop-
ment. Developers use regression test suites to check that
software changes do not break existing functionality. The
result of running a regression test suite is a set of test out-
comes for the tests in the suite. The outcomes are important
for developers to take actions. If all the tests pass, devel-
opers typically do not inspect the test runs further. If any
test fails, developers reason about the cause of failure to
understand whether the recent changes introduced a fault
in the code under test (CUT) or whether the test code it-
self needs to be changed [9]. The key assumption behind
this process is that a test failure indicates that the recent
changes introduced a problem in the CUT or the test code.
Permission to make digital or hard copies of all or part of thi s work for
personal or classroom use is granted without fee provided th at copies are
not made or distributed for proﬁt or commercial advantage an d that copies
bear this notice and the full citation on the ﬁrst page. To cop y otherwise, to
republish, to post on servers or to redistribute to lists, re quires prior speciﬁc
permission and/or a fee.
FSE’14 , November 16–22, 2014, Hong Kong, China
Copyright 2014 ACM 978-1-4503-3056-5/14/11 ...$10.00.Unfortunately, test outcomes are not reliable for tests tha t
canintermittentlypassorfailevenforthesame codeversio n.
Following practitioners [12,13,20,32,34], we call such te sts
ﬂaky1: their outcome is non-deterministic with respect to a
given software version. Flaky tests create several problem s
during regression testing. First, test failures caused by ﬂ aky
testscanbehardtoreproduceduetotheirnon-determinism.
Second, ﬂakytests waste timewhen theyfail evenunaﬀected
by the recent changes: the developer can spend substantial
time debugging only to ﬁnd out that the failure is not due to
the recent changes but due to a ﬂaky test [20]. Third, ﬂaky
tests may also hidereal bugs: if a ﬂaky test fails frequently,
developers tend to ignore its failures and, thus, could miss
real bugs.
Flaky tests are not only problematic but also relatively
common in large codebases. Many practitioners and re-
searchers have pointed out that ﬂaky tests can be a big and
frequent problem in general [7,12,13,21,26,29,32,34,37] , but
the only speciﬁc numbers we could obtain2are that the TAP
system at Google had 1.6M test failures on average each day
in the past 15 months, and 73K out of 1.6M (4.56%) test
failures were caused by ﬂaky tests.
The current approaches to combat ﬂaky tests are rather
unsatisfactory. The most common approach is to run a ﬂaky
test multiple times, and if it passes in any run, declare it
passing, even if it fails in several other runs. For example, at
Google, a failing test is rerun 10 times against the same code
version on which it previously failed, and if it passes in any
ofthose 10reruns, itis labeled as aﬂakytest[15,27]. Sever al
open-source testing frameworks also have annotations (e.g .,
Android has @FlakyTest [2], Jenkins has @RandomFail [17],
and Springhas @Repeat [31]) to label ﬂakytests that require
a few reruns upon failure.
Another approach would be to remove ﬂaky tests from
the test suite, or to mentally ignore their results most of th e
time (in the limit, ignoring the failure every time is equiva -
lent toremovingthetest). InJUnit, the @Ignore annotation
is used to exclude a test from the test suite to be run. How-
ever, developers are reluctant to use this approach, becaus e
ﬂaky tests may still provide some coverage and could help
ﬁnd regression bugs. Although the current approaches used
to deal with ﬂaky tests may alleviate their impact, they are
more“workarounds”rather than solutions. They do not ad-
dress the root causes of ﬂaky tests and can potentially waste
a lot of machine resources (with test reruns) or reduce the
eﬀectiveness of the test suite (with ﬂaky test exclusion).
1“Flaky”(sometimes spelled“ﬂakey”) means“unreliable”.
2Personal communication with John Micco.Findings about ﬂaky test causes Implications
F.1The top three categories of ﬂaky tests are Async Wait ,Concur-
rency, andTest Order Dependency .I.1Techniques for detecting and ﬁxing ﬂaky tests should focus o n
these three categories.
F.2Most ﬂaky tests (78%) are ﬂaky the ﬁrst time they are written. I.2Techniquesthat extensivelycheck tests when they are ﬁrst a dded
can detect most ﬂaky tests.
Findings about ﬂaky test manifestation Implications
F.3Almost all ﬂaky tests (96%) are independent of the platform
(i.e., could fail on diﬀerent operating systems or hardware ) even if
they depend on the environment (e.g., the content of the ﬁle s ystem).I.3Techniques for manifesting ﬂaky tests can check platform de pen-
dence lower in priority than checking environment dependen ce (e.g.
event ordering or time), especially when resources are limi ted.
F.4About thirdof Async Wait ﬂaky tests (34%) use a simple method
call with time delays to enforce orderings.I.4ManyAsync Wait ﬂaky tests can be simply manifestedby chang-
ing time delays of order-enforcing methods.
F.5MostAsync Wait ﬂaky tests (85%) do not wait for external
resources and involve only one ordering.I.5MostAsync Wait ﬂaky tests can be detected by adding one time
delay in a certain part of the code without the need of control ling
the external environment.
F.6Almost all Concurrency ﬂaky tests contain only two threads or
their failures can be simpliﬁed to only two threads, and 97% o f their
failures are due to concurrent accesses only on memory objec ts.I.6Existing techniques of increasing context switch probabil ity, such
as [10], could in principle manifest most Concurrency ﬂaky tests.
F.7ManyTest Order Dependency ﬂaky tests (47%) are caused by
dependency on external resources.I.7Not all Test Order Dependency ﬂaky tests can be detected by
recording and comparing internal memory object states. Man y tests
require modeling external environment or explicit reruns w ith diﬀer-
ent orders [37].
Findings about ﬂaky test ﬁxes Implications
F.8ManyAsync Wait ﬂaky tests (54%) are ﬁxed using waitFor .
which often completely removes the ﬂakiness rather than jus t reduc-
ing its chance.I.8 For developers: Explicitly express the dependencies between
chunks of code by inserting waitFor to synchronize the code.
For researchers: Comparing the order of events between correct
runs and failing runs, techniques could automatically inse rt order-
enforcing methods such as waitFor to ﬁx the code.
F.9Various Concurrency ﬂaky tests are ﬁxed in diﬀerent ways: 31%
are ﬁxedby addinglocks, 25% are ﬁxedby makingcode determin istic,
and 9% are ﬁxed by changing conditions. Our results are consi stent
with a study on concurrency bugs [24].I.9There is no one common strategy that can be used to ﬁx all
Concurrency ﬂaky tests. Developers need to carefully investigate
the root causes of ﬂakiness to ﬁx such tests.
F.10MostTest Order Dependency ﬂaky tests (74%) are ﬁxed by
cleaning the shared state between test runs.I.10 For developers: Identify the shared state and maintain it
clean before and after test runs.
For researchers: Automated techniques can help by recording the
program state before the test starts execution and comparin g it with
the state after the test ﬁnishes. Automatically generating code in
setUp/tearDown methods to restore shared program state, such as
static ﬁelds [7], could ﬁx many Test Order Dependency ﬂaky tests.
F.11Fixing ﬂaky tests in other categories varies from case to cas e.I.11There is no silver bullet for ﬁxing arbitrary types of ﬂaky te sts.
The general principle is to carefully use API methods with no n-
deterministic output or external dependency (e.g., time or network).
F.12Some ﬁxes to ﬂaky tests (24%) modify the CUT, and most of
these cases (94%) ﬁx a bug in the CUT.I.12Flaky tests should not simply be removed or disabled because
they can help uncover bugs in the CUT.
Table 1: Summary of ﬁndings and implications
Despite the pervasiveness of ﬂaky tests in practice, they
have not drawn much attention from the research commu-
nity. The few recent eﬀorts focus on only one category of
ﬂaky tests due to test-order dependency [7,29,37]. In this
paper, we present the ﬁrst extensive study of ﬂaky tests. We
analyze in detail 201 commits that likely ﬁx ﬂaky tests from
51 open-source projects from the Apache Software Founda-
tion. For each ﬂakytest, we inspect thecommit log message,
thecorrespondingbugreport(ifany), andthecorrespondin g
patch to determine the root cause of the non-deterministic
outcome and the way it was ﬁxed. We also examine how the
ﬂakiness was introduced in the test suite and how it could
be manifested.
We focus our study on the following questions that we be-
lieve could provide insights for practitioners and researc hers:
(1) What are the common causes of ﬂakiness? By
studying the root causes of ﬂaky tests, we reveal the most
prominent categories of ﬂaky tests for developers and re-
searchers to focus on. Our study of how ﬂakiness is intro-
duced also suggests the best stage to identify ﬂaky tests.
(2) How to manifest ﬂaky test failures? By studying
the possible ways to manifest ﬂaky test failures, we suggest
howautomatedtechniquescoulddetectunknownﬂakytests.
(3) What are the common ﬁxing strategies for
ﬂaky tests? By studying how developers ﬁx ﬂaky tests in
practice, we provide insights for both developers about som e
principled ways for avoiding certain kinds of ﬂaky tests andfor researchers about the potential techniques that could a u-
tomatically ﬁx ﬂaky tests.
Table 1 shows the summary of our ﬁndings and implica-
tions. The remaining sections discuss these in more detail.
2. METHODOLOGY
Our goal is to provideactionable information about avoid-
ing, detecting, and ﬁxing ﬂaky tests. To that end, we focus
on identifying and analyzing version-control commits that
likely ﬁx ﬂaky tests . One can view each ﬂaky test as a bug
in the test code whereas it can produce a non-deterministic
rather than deterministic outcome. Most empirical studies
of bugs start from bug reports [8,14,22,24,28]. However, we
start from commit logs, because we are mostly interested in
ﬂaky tests that are ﬁxed. Starting from commits gives us a
larger dataset than we would get starting from bug-report
databases [5]. First, some ﬁxes are made without ever be-
ing reported in bug-report databases. Second, some reports
from bug-report databases are open and not ﬁxed. In brief,
every ﬁxed ﬂaky test is reﬂected in the version-control sys-
tem but may not be reﬂected in its bug-report database.
To identify commits that likely ﬁx ﬂaky tests, we choose
to search through the central SVN repository of the Apache
Software Foundation [3]. This repository hosts a diverse se t
of over 150 top-level projects, written in various program-
ming languages, having varying sizes, and being actively de -“intermit” “ﬂak” TotalTotal w.
Bug ReportsHBase ActiveMQ Hadoop DerbyOther
Projects
All commits 859 270 1,129 615 134 86 90 118 701
Commits about ﬂaky tests 708 147 855 545 132 83 83 102 455
LDFFT commits 399 87 486 298 72 68 56 49 241
Inspected commits 167 34 201 124 23 20 29 12 117
Async Wait 62 12 74 43 10 11 7 3 42
Concurrency 26 6 32 19 2 3 3 1 23
Test Order Dependency 14 5 19 16 3 0 10 2 4
Resource Leak 9 2 11 8 2 2 0 1 6
Network 10 0 10 6 1 1 2 0 6
Time 5 0 5 2 0 1 1 0 3
IO 4 0 4 3 0 0 1 1 2
Randomness 2 2 4 4 1 0 3 0 0
Floating Point Operations 2 1 3 2 0 0 1 1 1
Unordered Collections 1 0 1 1 0 0 0 0 1
Hard to classify 34 6 40 21 4 2 2 3 29
Table 2: Summary of commit info and ﬂaky test categories
veloped for varying amount of time (from months to years)
by a large open-source community.
Weﬁrstextractthecompletecommithistoryofallprojects
from the Apache Software Foundation. To identify commit
messages that may indicate a ﬁx of a ﬂaky test, we search
for the keywords“intermit”and“ﬂak”. One could search for
more keywords, but these two already ﬁnd enough commits
for several months of inspection. More precisely, the searc h
yielded 1,129 commit messages. Table 2 shows the distri-
bution of these between the two keywords. After collecting
these commit messages, our study proceeds in two phases.
Filtering Phase. The ﬁrst phase in studyingthese 1,129
commits is to identify those that are likely about ﬁxing ﬂaky
tests. We manually inspect each commit, and if needed, the
bug report(s) associated with this commit. To increase con-
ﬁdenceinourinspection, twoofthepaperauthorsseparatel y
inspect each commit and then merge their results. The up-
per partofTable 2shows thesummaryofthis initial labeling
phase. (We discuss the lower part of Table 2 in Section 3.)
Ourgoal is todetermine for each commit whether: (1) itis
likely about (reporting or ﬁxing) a ﬂaky test (labeled ‘Com-
mits about ﬂaky tests’ in Table 2), and (2) it attempts to ﬁx
a distinct ﬂaky test (labeled ‘LDFFT commits’ in Table 2).
We ﬁnd 855 commits that are likely about ﬂaky tests; the
other 274 commits match“intermit”or“ﬂak”but are either
about the CUT not about the test code or just incidental
matches (e.g., ausername that contains“ﬂak”). Ofthese 855
commits, 486 are likely distinct ﬁxed ﬂaky tests ( LDFFT
commits ); the other 369 commits either report that certain
tests are ﬂaky but do not provide ﬁxes for those tests, or are
duplicates (e.g., multiple attempts to ﬁx the same ﬂaky test
or multiple copies of commits from one development branch
to another). For duplicates, we keep only the latestcommit
about the distinct ﬂaky test, because we want to study the
most recent change related to the ﬂaky test (e.g., the most
eﬀective ﬁx is usually the latest ﬁx).
Comparing the results across the two keywords, we ﬁnd
that “intermit”is used more often than “ﬂak”to indicate a
ﬂaky test. The numbers of commits for“intermit”are larger
than the corresponding numbers for “ﬂak”both in absolute
terms (399 vs. 87) and in relative terms (46% vs. 32%).
Comparing the column for all commits and the column
for the commits that have a bug report, we can see that a
large fraction of ﬁxes for ﬂaky tests have no bug report. For
example, 188 LDFFT commits have no bug reports, while
298 such commits have bug reports. Hence, we could have
missed a large number of commits related to ﬂaky tests if
our methodology relied solely on bug reports.The key result of this phase is a set of 486 LDFFT com-
mits. Table 3 shows the number of these LDFFT commits
across various projects. At least 51 projects out of the 153
projects in Apache likely have at least one ﬂaky test. For
each project, we tabulate the programming language(s) that
it uses, the number of LDFFT commits, the number of com-
mits that have at least one associated bug report, and the
total number of lines of code in the project (computed by
the“cloc”script). We can see that ﬂaky tests occur in a di-
verse set of projects, using various languages, ranging ove r
various sizes, and implementing code for various domains.
Analysis Phase. We study in more depth a subset of
the 486 LDFFT commits, selected as follows. First we sort
the projects according to their number of LDFFT commits
and split them into two groups: small (less than 6 LDFFT
commits) and large (greater than or equal to 6 LDFFT com-
mits). Table 3 shows a line separating these two groups.
Then we select to inspect all the commits from the small
groupandsampleonethirdofthecommitsfromeachproject
from the large group. We are thus covering all the projects
from the Apache Software Foundation where we identiﬁed
some LDFFT commits, and our results are not overly bi-
ased toward the projects with the largest number of LDFFT
commits. This sampling gives us a total of 201 out of 486
LDFFT commits to inspect.
For each of these 201 LDFFT commits, we ﬁrst have one
of the authors examine the commit in detail. Our goal is
to answer the following set of questions: (1) Is the commit
indeed ﬁxing a ﬂaky test? (2) What is the root cause of the
ﬂakiness for the test? (3) How can the ﬂakiness be man-
ifested? (4) How is the test ﬁxed? The answers are then
inspected and conﬁrmed by another author. The following
sections discuss these answers.
3. CAUSES OF FLAKINESS
We ﬁrst analyze the root causes of test ﬂakiness and clas-
sify them into 10 categories. We then study when ﬂakiness
is introduced in tests.
3.1 Categories of Flakiness Root Causes
We analyze in detail 201 LDFFT commits to classify the
root causes of the ﬂakiness likely ﬁxed by these commits.
We precisely classify the root causes for 161 commits, while
the remaining 40 commits are hard to classify for various
reasons as described later.
We split the root causes of ﬂakiness into 10 categories.
Some of these categories have been previously described byProject Language
LDFFT
commits
Bug
reports LOC
HBase Java 72 61 3199326
ActiveMQ C++/Java/Scala 68 17 323638
Hadoop Java 56 56 1348117
Derby Java 49 42 719324
Harmony C/C++/Java 30 27 1155268
Lucene Java 27 12 384730
Tomcat Java 16 0 316092
ServiceMix Java 15 12 156316
ZooKeeper Java 14 14 119139
Qpid C++/Java 14 8 359968
CXF Java 11 3 441783
Web Services C++/Java 9 1 859462
Tuscany Java 7 1 181669
Flume Java 7 6 60640
Maven Java 7 0 23000
OpenJPA Java 7 4 484054
Oozie Java 6 6 149315
Aries Java 6 4 125563
Continuum Java 5 0 130765
Subversion C/Python 4 0 562555
Tapestry Java 4 0 233382
Mesos C++ 4 0 219485
Flex Java 4 1 2909635
HttpComponents Java 3 0 52001
Accumulo Java/Python 3 1 309216
Kafka Scala 3 2 139736
Hive Java 3 3 715967
Ambari Java 2 2 115725
Jena Java 2 0 349531
APR C 2 0 64279
Jackrabbit Java 2 2 295961
Sling Java 2 2 226613
OpenEJB Java 2 1 534262
Mahout Java 2 1 121312
Avro Java 2 2 131384
NPanday C# 1 0 41947
Cassandra Java 1 1 130244
UIMA Java 1 1 218881
Roller Java 1 0 75560
Portals Java 1 0 228907
ODE Java 1 1 114807
Buildr Ruby 1 0 31062
Pig Java 1 1 378181
Camel Scala 1 0 580015
Archiva Java 1 1 107994
XMLBeans Java 1 0 211425
SpamAssassin C 1 0 65026
Shindig Java 1 0 151743
MINA Java 1 0 417571
Karaf Java 1 1 360403
Commons Java 1 1 21343
Total Multiple 486 298 20654322
Table 3: Many projects contain ﬂaky tests
practitioners [13,33] and others we have identiﬁed by study -
inganumberof similar ﬂakytests. The lower part ofTable 2
shows the summary of our analysis. The ﬁrst column lists
the 10 categories, and the last row shows the 40 commits
that are hard to classify. The remaining columns tabulate
thedistributionofthesecategories overvarioustypesofc om-
mits and various projects. We highlight the number of ﬂaky
tests for the top four projects, and the last column sums up
the numbers for the remaining 47 projects.
We next discuss in more detail the top three categories
that represent 77% of the 161 studied commits. We ﬁnally
brieﬂy summarize the other seven categories.
3.1.1Async Wait
74 out of 161 (45%) commits are from the Async Wait
category. We classify a commit into the Async Wait cat-
egory when the test execution makes an asynchronous call
and does not properly wait for the result of the call to be-come available before using it. For example, a test (or the
CUT) can spawn a separate service, e.g., a remote server or
another thread, and there is no proper synchronization to
wait for that service to be available before proceeding with
the execution. Based on whether the result becomes avail-
ablebefore(orafter)itisused, thetestcannon-determini sti-
cally pass (or fail).
While our focus is on understanding ﬂaky tests, we point
out that lack of synchronization can also lead to bugs in the
CUT even if it does not manifest in ﬂaky tests. In fact, bugs
caused by asynchronous wait fall in a subcategory of concur-
rency bugs that Lu et al. [24] call “order violation”because
the desired order of actions between multiple threads is not
enforced. We classify Async Wait as a separate category
of root causes from Concurrency because it represents a
large percentage of ﬂaky tests with some common charac-
teristics that are not shared among all tests that are ﬂaky
due toConcurrency .
We next describe an example Async Wait ﬂaky test.
This snippet is from the HBase project:
1@Test
2public void testRsReportsWrongServerName() throwsException {
3MiniHBaseCluster cluster = TEST_UTIL.getHBaseCluster() ;
4MiniHBaseClusterRegionServer firstServer =
5(MiniHBaseClusterRegionServer)cluster.getRegionServ er(0);
6HServerInfo hsi = firstServer.getServerInfo();
7firstServer.setHServerInfo(...);
8
9// Sleep while the region server pings back
10Thread.sleep(2000);
11assertTrue(firstServer.isOnline());
12assertEquals(2,cluster.getLiveRegionServerThreads() .size());
13...// similarly for secondServer
14}
The test uses a cluster to start a server firstServer and
then uses Thread.sleep(2000) to wait for it to ping back. If
theserver doesnot respondinatimelymanner, e.g., because
of threadschedulingor network delay, thetest will fail. (T he
test has similar code for secondServer .) Basically the test
intermittently fails based on how fast the server responds.
So far we have described the root cause of this ﬂaky test,
but as a preview of our overall analysis, we describe how
we found this commit and how the developers ﬁxed the
ﬂakiness. This commit matches the keyword “ﬂak” as its
commit message reads “HBASE-2684 TestMasterWrongRS
ﬂaky in trunk”, which refers to the bug report ID HBASE-
2684 [4]. The test was added in revision3948632 and was
ﬂaky ever since. The developers ﬁxed this test in revision
952837 by making two changes. First, they replaced each
Thread.sleep(2000) statement (for firstServer andsecond-
Server) with a call to cluster.waitOnRegionServer(0) that
waits until the server responds back and then removes its
corresponding thread. Second, to ensure that the test does
not run indeﬁnitely in case that the server cannot start
for some reason, the developers added (timeout=180000) to
the@Testannotation, which fails the test after 180 seconds.
These changes completely removed the ﬂakiness of this test:
the developers removed the assumption that the server will
respondwithin twoseconds andexplicitly expressedthecon -
dition to wait for before resuming the execution. Table 4
furthercategorizes howmanyﬁxescompletelyremovedﬂaki-
ness and howmany just decrease the probability of ﬂakiness.
3All the revision numbers refer to the Apache Software
Foundation SVN repository [3].3.1.2Concurrency
32 out of 161 (20%) commits are from the Concurrency
category. We classify a commit in this category when the
test non-determinism is due to diﬀerent threads interactin g
inanon-desirablemanner(butnotduetoasynchronouscalls
from the Async Wait category), e.g., due to data races,
atomicity violations, or deadlocks.
The source of non-determinism can be either in the CUT
or in the test code itself. 10 out of 32 (30%) cases are due
to non-determinism in the CUT and manifest by the in-
termittent failure of a corresponding test. Note that non-
determinism in the test (or code) execution may or may not
be a bug: the code could indeed have several correct diﬀer-
ent behaviors. However, if the test incorrectly accepts onl y
a subset of these as passing behaviors, then the test has
non-deterministic outcome and is deﬁnitely ﬂaky.
We next describe an example Concurrency ﬂaky test,
where the cause is non-determinism in the CUT. This snip-
pet is from the Hive project:
1if(conf != newConf) {
2for(Map.Entry<String, String> entry : conf) {
3if((entry.getKey().matches( "hcat.*" )) &&
4 (newConf.get(entry.getKey()) == null)) {
5newConf.set(entry.getKey(), entry.getValue());
6}
7}
8conf = newConf;
9}
The code iterates over a map shared by multiple threads;
if the threads modify the map concurrently, a Concurrent-
ModificationException is thrown [23]. This code led to ﬂaky
failures in several tests. The developers ﬁxed this by enclo s-
ing the lines 3 to 6 in a synchronized block, making them
execute atomically. We classify this case in the atomicity
violation subcategory of the Concurrency category.
Our study ﬁnds the main subcategories of Concurrency
ﬂaky tests to match the common bugs from concurrent pro-
gramming [24]: data races (9 out of 32), atomicity violation s
(10 out of 32), and deadlocks (2 out of 32). But we also
identify a new prominent subcategory that we call “ bug in
condition ”(6 out of 32)4. We classify a commit in this sub-
category whensomemultithreadedcodehasaconditionthat
inaccurately guards what threads can execute the guarded
code. The problem is when this condition is too tight or
too permissive. An example is in the project Lucene with
bug report LUCENE-1950. The test is ﬂaky because a code
portion should be executed only by the thread named main,
but the condition does not guard for that, so when another
thread executes the code, the test fails. The ﬁx was to
strengthen the condition to check if the thread name is main.
3.1.3Test Order Dependency
19 out of 161 (12%) commits are from the Test Order
Dependency category. We classify a commit into this cat-
egory when the test outcome depends on the order in which
the tests are run. In principle, all tests in a test suite shou ld
be properly isolated and independent of one another; then,
the order in which the tests are run should not aﬀect their
outcomes. In practice, however, it is not the case.
This problem arises when the tests depend on a shared
state that is not properly setup or cleaned. The shared state
can be either in the main memory (e.g., the static ﬁelds in
45 out of 32 cases are hard to classify in any subcategory.Java) or some external resource (e.g., ﬁles or databases). E i-
ther a test expects to ﬁnd the state as it was initialized but
meanwhile another test changed that state (i.e., running on e
“polluter”test before another test fails the latter test), or a
test expects the state to be set by the execution of another
test that was not run (i.e., notrunning one“setup”test be-
fore another test fails the latter test). Hence, dependenci es
among tests result in unpredictable behavior when the test
order changes. For example, the update from Java 6 to Java
7 changed the order in which JUnit ﬁnds the tests in a test
class (due to the change in the reﬂection library) [19].
We next describe an example Test Order Dependency
ﬂaky test. This snippet is from the Hadoop project:
1@BeforeClass
2public static void beforeClass() throwsException {
3bench = newTestDFSIO();
4...
5cluster = newMiniDFSCluster.Builder(...).build()
6FileSystem fs = cluster.getFileSystem();
7bench.createControlFile(fs, ...);
8
9/* Check write here, as it is required for other tests */
10testWrite();
11}
The snippet is from a test class where one test ( testWrite )
writes to a ﬁle via fspreparing data to be read by several
other tests. The developers incorrectly assumed that test-
Writewould always run ﬁrst, but JUnit does not guarantee
any particular test ordering. If JUnit runs some read test
beforetestWrite , the test fails. The developers ﬁxed this
by removing the original testWrite test and adding a call to
testWrite in the@BeforeClass as shown in line 10.
3.1.4 Other Root Causes
Webrieﬂydiscuss the otherseven categories of root causes
of ﬂakiness. Due to space limitation, we do not give a de-
tailed example for each category. The relative ratio of ﬂaky
tests in these categories can be computed from Table 2.
Resource Leak. A resource leak occurs whenever the ap-
plication does not properly manage (acquire or release) one
ormore ofitsresources, e.g., memoryallocations ordataba se
connections, leading to intermittent test failures.
Network. Tests whose execution depends on network can
be ﬂaky because the network is a resource that is hard to
control. In such cases, the test failure does not necessaril y
mean that the CUT itself is buggy, but rather the developer
does not account for network uncertainties. From the result s
of our inspection, we distinguish two subcategories of ﬂaky
tests whose root cause is the network. The ﬁrst subcategory
is due to remoteconnection failures (60%), and the second
subcategory is due to localbad socket management (40%).
Time. Relying on the system time introduces non-deter-
ministic failures, e.g., a test may fail when the midnight
changes in the UTC time zone. Some tests also fail due to
the precision by which time is reported as it can vary from
one platform to another. We discuss platform-dependent
tests later (Sec. 4), and while theyare not frequent, it is ea sy
for developers to overlook the diﬀerences among platforms.
IO.I/O operations (in addition to those for networks) may
also cause ﬂakiness. One example we encountered was in the
project Archiva where the code would open a ﬁle and read
from it but not close it until the fileReader gets garbage col-
lected. So, a test that would try to open the same ﬁle would
either pass or fail depending on whether the fileReader was
already garbage collected or not.Randomness. The use of random numbers can also make
some tests ﬂaky. In the cases that we analyzed, tests are
ﬂaky because they use a random number generator without
accounting for all the possible values that may be generated .
For example, one test fails only when a one-byte random
number that is generated is exactly 0.
Floating Point Operations. Dealing with ﬂoating point
operationsisknowntoleadtotrickynon-deterministiccas es,
especiallyinthehigh-performancecomputingcommunity[6 ].
Even simple operations like calculating the average of an ar -
ray require thorough coding to avoid overﬂows, underﬂows,
problems with non-associative addition, etc. Such problem s
can also be the root cause of ﬂaky tests.
Unordered Collections. In general, when iterating over
unorderedcollections (e.g., sets), thecodeshouldnotass ume
that the elements are returned in a particular order. If it
doesassume, thetestoutcomecanbecomenon-deterministic
as diﬀerent executions may have a diﬀerent order.
Other Cases. We did not classify the root causes for 40
commits; of those, 7 do not ﬁx a (non-deterministic) ﬂaky
test but rather ﬁx some deterministic bug, and 6 do not ac-
tually ﬁx a ﬂaky test. The other 27 are hard to understand.
We inspected each commit for several hours, reading the
commit message and associated bug reports if any, reason-
ing about the patched code, and occasionally even trying to
compile and run the tests. However, certain commits turned
out to be hard to understand even after several hours. If the
test or code change proven to be too complex, and there is
not much information in the commit message or bug report
to help us understand the root cause of ﬂakiness, we mark it
as unknown. We exclude all 40 cases from our further study.
See Finding F.1 and Implication I.1 in Table 1.
3.2 Flaky Test Introduction
We also study when the tests became ﬂaky. From the
161 tests we categorized, 126 are ﬂaky from the ﬁrst time
they were written, 23 became ﬂaky at a later revision, and
12 others are hard to determine (mostly because tests were
removed or relocated to other ﬁles). We analyze in more
detail the 23 cases in which the tests became ﬂaky later and
identify two main causes.
The ﬁrst reason is the addition of new tests that violate
the isolation between tests. For example, consider a test t1
that requires a shared variable to be zero but relies on the
initialization phase to set that value rather than explicit ly
setting the value before beginning the execution. A newly
added test t2sets the value of that shared variable to 1,
without cleaning the state afterwards. If t2is run before t1,
the latter would fail.
The second reason for introducing ﬂakiness after a test
is ﬁrst written is due to test code changes such patching
a bug, changing the test functionality, refactoring a test, or
incompletelypatchingsomeﬂakinessitself. Forthe152ﬂak y
tests for which we have evolution information, we calculate
the average numberof days it takes toﬁx a test tobe 388.46.
In sum, tools should extensively check tests when they are
ﬁrst written, but some changes can also introduce ﬂakiness.
See Finding F.2 and Implication I.2 in Table 1.
4. MANIFESTATION
Manifesting the ﬂakiness of ﬂaky tests is the ﬁrst step in
ﬁxingthem. Inpractice, givenatest failure thatissuspect ed
to be from a ﬂaky test, the most common approach is torerun the failing test multiple times on the same code to
ﬁnd whether it will pass (and thus is deﬁnitely ﬂaky) or will
not pass (and thus may be a real deterministic failure or
might be still a ﬂaky test that did not manifest in a pass in
those multiple runs). While such rerunning can be useful in
some cases, it has disadvantages. When the probability for
a ﬂaky test to change its outcome is low, rerunning it a few
times may not be enough to manifest that it can change the
outcome. Also, rerunning the tests multiple times is time
consuming, especially if there are multiple test failures.
While inspecting each ﬂaky test in our study, we have
considered possible ways to automatically trigger the fail ure
oftheﬂakytest, eithertodeﬁnitelyshowthatitcanbothfai l
and pass or at least to increase the probability of its failur e
(under the assumption that it passes majority of the time).
Our analysis leads to ﬁndings that could help in developing
automatic techniques for manifesting ﬂaky tests.
4.1 Platform (In)dependency
To understand how ﬂaky tests manifest in failures, the
ﬁrst question we want to answer is how many of those ﬂaky
failures only manifest on a particular platform. By a“plat-
form”, we refer to the underlying system the test is running
on, including the hardware, operating system, JVM/JRE,
etc. It diﬀers from the environment (mentioned later), as it
is not provided or controlled by the test or the application.
An example of platform dependence is a test failing due to
a diﬀerent order of ﬁles on a speciﬁc ﬁle system.
From the ﬂaky tests we categorized, we ﬁnd that 154 out
of161(96%)haveoutcomethatdoesnotdependontheplat-
form. Namely, the test failures only depend on the events
in the application code and not on any system call to the
underlying platform. Of the 7 cases where the ﬂaky failures
can only be reproduced on a certain platform, 4 tests require
a particular operating system to manifest the failure, 2 re-
quire a particular browser to manifest the failure, and only
1 requires a speciﬁc buggy JRE to manifest the failure.
See Finding F.3 and Implication I.3 in Table 1.
4.2 Flakiness Manifestation Strategies
For each of the top three categories of root causes of ﬂaky
tests, we next discuss how one could modify the tests from
that category to manifest the ﬂaky failures.
4.2.1Async Wait
How many Async Wait ﬂaky tests can be manifested
bychanging an existing time delay? To enforce a cer-
tain ordering in test execution, in many cases, developers
use asleeporwaitFormethod with a time delay. A sleep
pauses the current thread for a ﬁxed amount of time and
then resumes its execution. With waitFor, we refer to a set
ofmethodsusedtoeitherletthecurrentthreadbusywait for
some condition to become true or block the current thread
until being explicitly notiﬁed. We ﬁnd 25 out of 74 (34%)
Async Wait ﬂaky tests use sleeporwaitForwith a time
delay to enforce ordering. Their ﬂaky failures can be simply
manifested by changing the time delay of such method calls,
e.g., decreasing the sleeping time in the test. For the other
Async Wait ﬂaky tests, developers do not use a common
method call with a time delay to enforce certain ordering
but either do not enforce any ordering at all or use some
application-speciﬁc APIs to enforce the desired ordering.
See Finding F.4 and Implication I.4 in Table 1.How many Async Wait ﬂaky tests can be manifested
by adding onenew time delay? We ﬁnd two factors
that determine the diﬃculty of manifesting Async Wait
ﬂaky tests. The ﬁrst is whether a test depends on exter-
nal resources or not, because it is harder to control externa l
resources. The second is whether the ﬂaky failure involves
only one ordering (where one thread/process is supposed to
wait for an action from another thread/process to happen)
or more orderings. Manifesting failures for multiple order -
ings would require carefully orchestrating several action s,
which is much harder than just delaying one action.
Our study ﬁnds that the majority of Async Wait ﬂaky
tests, 67 out of 74 (91%), do not “wait” for external re-
sources. The few that do wait include a test that waits for
a speciﬁc process to be started by the underlying OS and a
test that waits for a response from the network.
Our study also ﬁnds that most Async Wait ﬂaky tests
involve only one ordering. In fact, we ﬁnd only 5 out of 74
(7%) cases with multiple orderings, e.g., a client waits for
multiple servers to be started in a certain order.
Overall, 63 out of 74 (85%) Async Wait ﬂaky tests do
not depend on external resources andinvolve only one or-
dering. Their ﬂaky failures can be manifested by adding
only one time delay in the code, without the need of con-
trolling the external environment. (Note that several test s
can be manifested as ﬂaky byeither addinga newtime delay
or changing an existing time delay.) While ﬁnding the ap-
propriate place to add the delay could be quite challenging,
researchers can attempt to build on the heuristic or random-
izationapproachesthatweresuccessfullyusedinmanifest ing
concurrency bugs [10,35].
See Finding F.5 and Implication I.5 in Table 1.
4.2.2Concurrency
How many threads are involved? For all the 32 ﬂaky
tests caused by Concurrency , we also study how many
threads are involved in the test failure. We ﬁnd out that
13 cases involve more than two threads. Seemingly con-
tradictory, Lu et al. [24] found out that most concurrency
bugsrequireonly two threads to manifest. However, our re-
sult does not contradict their ﬁnding because we study real
tests that already contain multiple threads, whereas they
study bugs in the code and reason about tests that could
have been written to expose those bugs. To reproduce Con-
currency ﬂaky failures, all the existing tests with multiple
threads that we studied could be simpliﬁed into at most two
threads. Interestingly enough, we even ﬁnd one ﬂaky test
for which only one thread suﬃces to trigger a deadlock bug.
How many Concurrency ﬂaky tests do not depend
on external resources? We also ﬁnd out that 31 out of 32
(97%)Concurrency ﬂaky tests do not depend on external
resources. In other words, their failures are only caused by
concurrent accesses to the objects in the main memory.
See Finding F.6 and Implication I.6 in Table 1.
4.2.3Test Order Dependency
We further study the source of dependency for each Test
Order Dependency ﬂaky test. We identify three sources
of dependency. We call the ﬁrst one“Static ﬁeld in TEST”
(3 out of 19), which means that several tests access the same
static ﬁeld declared in the test code, without restoring the
state of that ﬁeld in a setUportearDown method. We call the
second one“Static ﬁeld in CUT”(6 out of 19), which meansCategory Fix type TotalRem-
oveDecr-
ease
Async WaitAdd/modify waitFor 42 23 19
Add/modify sleep 20 0 20
Reorder execution 2 0 2
Other 10 9 1
ConcurrencyLock atomic operation 10 10 0
Make deterministic 8 8 0
Change condition 3 3 0
Change assertion 3 3 0
Other 8 8 0
test order
dependencySetup/cleanup state 14 14 0
Remove dependency 3 3 0
Merge tests 2 2 0
Table 4: Flaky test ﬁxes per category
that the shared static ﬁeld is declared in the CUT rather
than in the test code itself. Test-order dependencies of the
ﬁrst two kinds can be exposed by recording and comparing
object states. We call the third one“External dependency”
(10 out of 19), which means that the dependency is caused
bysome external environment, suchas shared ﬁle or network
port, and not by a static ﬁeld.
Of the 19 Test Order Dependency ﬂaky tests, we ﬁnd
that more than half are caused by an external dependency.
Those tests cannot be easily manifested by recording and
comparinginternalmemoryobject states butinsteadrequir e
more sophisticated techniques to model the state of externa l
environment or to rerun tests with diﬀerent order [37].
See Finding F.7 and Implication I.7 in Table 1.
5. FIXING STRATEGIES
Developers ﬁx various root causes of ﬂakiness in diﬀerent
ways. We identify the main strategies that they use and
extract insights for practitioners to use in manually ﬁxing
ﬂaky tests and for researchers to use as starting points to
develop tools to automate this process. For the top three
categories of ﬂaky tests, we give a detailed description of t he
common ﬁxes and discuss their eﬀectiveness in removing the
ﬂakiness. For the other categories, we only brieﬂy discuss
the ﬁxes. Some of the ﬁxes for ﬂaky tests change the CUT,
so we also study those cases.
5.1 Common Fixes and Effectiveness
We describe the main strategies that developers use to
ﬁx ﬂaky tests of diﬀerent categories. Table 4 summarizes
the ﬁxes for the top three categories. An interesting prop-
erty of these ﬁxes is that they do not always completely
eliminate the root cause, namely they do not turn a (non-
deterministic)ﬂakytestintoafullydeterministictest. R ather,
some ﬁxes change the code such that the test is less likely
to fail, although it could still fail. The column “Remove”
shows the number of ﬁxes that completely remove the ﬂaki-
ness, while the column“Decrease”shows the number of ﬁxes
that only decrease the chance of failures in the ﬂaky tests.
5.1.1Async Wait
Common ﬁxes. The key to ﬁxing Async Wait ﬂaky tests
is to address the order violation between diﬀerent threads o r
processes. We describe the strategies used in practice and
evaluate their eﬀectiveness.
Fixes using waitFor calls:42 out of 74 (57%) Async
Waitﬂaky tests are ﬁxed via some call to waitFor. Recall
that those calls block the current thread until a certain con -
dition is satisﬁed or a timeout is reached. The ﬁxes add anew call, modify an existing call (either the condition or th e
timeout), or replace an already existing sleep(36% of these
ﬁxes replace a sleepwith awaitForcall). The latter shows
thatwaitForisapreferredmechanismthatdevelopersshould
use whenever possible. Also, outof all the waitForﬁxes, 46%
have a time bound, while the others are unbounded.
Fixes using sleepcalls:20outof74(27%) Async Wait
ﬂaky tests are ﬁxed by stalling some part of the code for a
pre-speciﬁed time delay using sleep. 60% of these cases
increase the waiting time of an already existing sleep, whil e
the other 40% add a sleep that was missing (conceptually
increasing the time delay from 0 up to the speciﬁed bound
in the added sleep). This shows the eﬀect that machine
speed variation can have on ﬂaky tests, especially for those
60% of the cases where the sleepwas already there but the
waiting time was not long enough on some slower (or faster)
machines, leading to intermittent failures.
Fixes by reordering code: 2 out of 74 (3%) Async
Waitﬂaky tests are ﬁxed by reordering code. Instead of
simply using sleeptowait for some time, the developer ﬁnds
a piece of code that can be executed such that the execution
of that code achieves the delay and hopefully achieves the
particular event ordering. A beneﬁt of executing some code
rather than simply using sleepis that useful computation
gets done, but the problem remains that the developer can-
not precisely control the time taken for that computation.
Other: 10 out of 74 (14%) Async Wait ﬂaky tests are
very speciﬁc to the code and hard to generalize. For ex-
ample, some of these ﬁxes use an application-speciﬁc API
method to trigger an event so the ordering is not violated.
Eﬀectiveness of ﬁxes in alleviating ﬂakiness. Using a
sleepis rarely a good idea when writing tests. As analyzed
in our earlier work that focused on writing multithreaded
tests [16], the use of sleepmakes the test unintuitive, un-
reliable, and ineﬃcient. It is hard to reason from the sleep
calls what ordering among what events they try to enforce.
Moreover, sleepcalls cannot provide the guarantee that the
ordering that the developer wants to enforce among events
will indeedhappenwithin the amountof time given in sleep.
For that reason, developers tend to over-estimate the time
needed in sleepcalls, which makes the tests rather ineﬃ-
cient, because most of the time, the event can ﬁnish way
before the time bound (and yet occasionally it does not ﬁn-
ish before the time bound, thus intermittently failing the
test). For all these reasons, we ﬁnd that the ﬁxes where
sleepcalls are used to ﬁx Async Wait ﬂaky tests are only
decreasing the chance of a ﬂaky failure: running tests on
diﬀerent machines may make the sleepcalls time out and
trigger the ﬂaky failures again.
Using a waitForis the most eﬃcient and eﬀective way to
ﬁxAsync Wait ﬂaky tests. Because waitFormakes explicit
the condition that has to be satisﬁed before the execution
can proceed, it becomes much easier to understand what
ordering the test expects. Moreover, the execution is more
eﬃcient because it can proceed as soon as the condition is
satisﬁed, rather than waiting for some time bound when
the condition may or may not be satisﬁed. In fact, Table 4
shows that 23 out of 42 (55%) cases with waitForcompletely
remove the ﬂakiness. The remaining cases only decrease
the ﬂakiness because those waitForcalls are bounded with
a timeout. Even when there is no explicit timeout on some
waitFor, the test itself can have a timeout. Such a timeout
is practically useful for preventing a single test from hang -ing the entire test suite forever. We ﬁnd that developers set
much higher timeouts when using waitForrather than sleep;
in particular, the average waiting time for waitForcalls in
our cases is 13.04 seconds, while the average waiting time
forsleepcalls is 1.52 seconds. The higher upper bound on
waitFormakes them more robust against ﬂakiness when the
condition that is being waited for gets delayed; at the same
time,waitForis more eﬃcient than sleepwhen the condi-
tion gets available earlier than expected. While the tests
withwaitFormay still fail when run on an extremely slow
machine, using waitForis much more eﬃcient and reliable
than using a sleep.
Reordering code is ineﬀective as using sleepcalls, for the
similar reasons. The only advantage of reordering code over
sleeps is that the waiting time is not purely idle time but
rather some useful computation happens. However, the de-
veloper does not have a precise control over the amount of
time taken for that computation. Hence, the ﬂaky tests can
still fail after the code is reordered.
See Finding F.8 and Implication I.8 in Table 1.
5.1.2Concurrency
Common Fixes. Concurrency bugs are caused by unde-
sired interleavings among diﬀerent threads. The ﬂaky tests
from the Concurrency category in our study are similar
to common concurrency bugs in the CUT [24]. We ﬁnd four
main strategies of ﬁxes for Concurrency ﬂaky tests.
Fixes by adding locks: 10 out of 32 (31%) Concur-
rencyﬂaky tests are ﬁxed by adding a lock to ensure mu-
tual exclusion for code that is supposed to be accessed by
one thread at a time. 8 of these ﬁxes address atomicity
violation, and 1 each addresses deadlock and race condition .
Fixes by making code deterministic: 8 out of 32
(25%)Concurrency ﬂaky tests in the study are ﬁxed by
making the execution deterministic. The speciﬁc changes
include modifying code to eliminate concurrency, enforcin g
certain deterministic orders between thread executions, e tc.
Fixes by changing concurrency guard conditions:
3 out of 32 (9%) Concurrency ﬂaky tests in our study are
ﬁxed by changing the guard conditions in the test code or
the CUT. For example, developers use a condition check to
onlyallow certain threads toentercertain partof thecodea t
the same time. If that condition does not take into account
all the possible scenarios, the test may become ﬂaky.
Fixes by changing assertions: 3 out of 32 (9%) Con-
currency ﬂaky tests in our study are ﬁxed by changing
assertions in the test code. Although non-determinism is
permitted bythe concurrentprogram, thetest assertion fai ls
to accept all valid behaviors. The ﬁx is to account for all
valid behaviors in the assertion.
Others: The remaining ﬁxes for Concurrency ﬂaky
tests vary from case to case, and they are usually speciﬁc to
theapplication. Forinstance, developers mayﬁxa Concur-
rencyﬂaky test due to race condition by making a speciﬁc
shared variable to be thread local.
Eﬀectiveness of ﬁxes in alleviating ﬂakiness. In our
study we ﬁndthat all the ﬁxes for Concurrency ﬂakytests
completely remove ﬂakiness in the test. As long as the root
cause of the Concurrency ﬂaky test is correctly identiﬁed
and understood by developers, the committed ﬁx always re-
solves the problem completely.
Our ﬁnding seemingly contradicts a previous study [24]
by Lu et al. who found a number of concurrency bugs to behard to ﬁx and typically have at least one incomplete patch
attempting to ﬁx the bug but not completely ﬁxing it. The
diﬀerence is likely due to the diﬀerent methodologies we use .
In particular, we analyze one committed ﬁx per a ﬂaky test
in the repository, and if the same ﬂaky test has multiple
commits, we pick the last commit to fully understand the
ﬂakiness and how it was resolved. It is quite possible that
our study missed some incomplete ﬁxes. Also, it is possi-
ble that some incomplete ﬁxes were proposed with the bug
reports, but developers rejected these incomplete ﬁxes wit h-
out committing them to the repository at all. In contrast,
Lu et al. study concurrency bugs from bug reports and not
from commits. Also, one reason that Concurrency ﬂaky
tests can be easier to ﬁx than general concurrency bugs is
that general bugs may have no tests, making it harder to
debug than when a speciﬁc test is present.
See Finding F.9 and Implication I.9 in Table 1.
5.1.3Test Order Dependency
Common Fixes. Test Order Dependency ﬂaky tests
are not easy to debug because it may be diﬃcult to ﬁnd out
which other test is (or tests are) interdependent with the
intermittently failing test. However, once developers ﬁgu re
out the dependency, the ﬁx is usually simple and obvious.
We classify the common ﬁxes into three main strategies.
Fixes by setting up/cleaning up states: 14 out of
19 (74%) Test Order Dependency ﬂaky tests are ﬁxed
by setting up or cleaning up the state shared among the
tests. Basically, the test needs to set up the state before it
executes, clean up the state after it ﬁnishes, or both.
Fixes by removing dependency: 3 out of 19 (16%)
Test Order Dependency ﬂaky tests are ﬁxed by making
local copies of the shared variable and removing the depen-
dency on it.
Fixesby merging tests: 2outof19(10%) Test Order
Dependency ﬂaky tests are ﬁxed by merging dependent
tests. For example, developers copy the code in one test
into another one and remove the ﬁrst test.
Eﬀectiveness of ﬁxes in alleviating ﬂakiness. All the
ﬁxes we ﬁnd in our study for Test Order Dependency
ﬂaky tests completely remove the ﬂakiness. However, the
ﬁrst twostrategies of ﬁxes(Setup/cleanupstate andRemove
dependency) are better than the last strategy. Setting up
or cleaning up state ensures that even if tests do depend
on the shared state, that state is always found as expected,
independent of the order in which the tests run. The second
strategy remedies the case where two or more tests access
and modify a common ﬁeld that is expected to be local;
this strategy is somewhat related to concurrency, even when
tests are executed on one thread.
Merging dependenttests makes tests larger and thushurts
their readability and maintainability, although it does re -
move the ﬂakiness. Moreover, merging smaller tests into
large tests limits the opportunities for parallelizing the test
suite or applyingtest selection andprioritization techni ques.
See Finding F.10 and Implication I.10 in Table 1.
5.1.4 Others
We next discuss brieﬂy the common ways of ﬁxing the
other categories of ﬂaky tests.
Resource Leak. This category is one of the hardest for
generalizing the speciﬁc ﬁxes. Fowler suggests to manage
the relevant resources through resource pools [13]. When aclient needs a resource, the pool provides it. The pool can
be conﬁgured to either throw an exception if all resources
are in use, or to grow. When the client is done with the
resource, it should return it to the pool. A resource leak
occurs if a client does not return resources. These leaky
clients can be detected by reducing the pool size so that
requesting a resource triggers an exception, and ﬁxed by
properly returning the resources.
Network. Whether for internal communication between
processes on the same machine through sockets, or getting
data and services from remote servers, software relies ex-
tensively on the network. This dependency results in non-
determinism in test and code execution. One of the best
ﬁxes for this category is to use mocks. Whenever the use
of mocks is non practical, the ﬂakiness can be remedied by
usingwaitFor.
Time.Time precision diﬀers from one system to another.
In general, tests should avoid using platform dependent val -
ues like time.
IO.Because they deal with external resources, I/O opera-
tions can cause intermittent test failures. Therefore, the de-
veloper should make sure to close any opened resource (ﬁle,
database, etc.) and to use proper synchronization between
diﬀerent threads sharing the same resource.
Randomness. This category is associated with random
number generation. To avoid/ﬁx ﬂakiness due to random-
ness, the developers should control the seed of the random
generator such that each individual run can be reproduced
yet the seed can be varied across runs. The developer should
also handle the boundary values that the random number
can return, e.g., zero can be a problem in some scenarios.
Floating Point Operations. Floating-point operations
are non-deterministic bynature, and can cause a lot of prob-
lems if not handled correctly. In general, one has to be care-
ful when dealing with ﬂoating point operations. Imprecisio n
is not avoidable, but one should aim for determinism, and
it is good practice to have test assertions as independent as
possible from ﬂoating-point results.
Unordered Collections. Flakiness due to unordered col-
lections arises whenever the developer assumes that the API
guarantees a certain order that it does not. In general, a
good programming practice is to write tests that do not as-
sume any speciﬁc ordering on collections unless an explicit
convention is enforced on the used data structure.
See Finding F.11 and Implication I.11 in Table 1.
5.2 Changes to the code under test
38 out of 161 (24%) of the analyzed commits ﬁx the ﬂak-
iness by changing both the tests and the CUT. We classify
the changes to the CUT as follows.
Deterministic Bug. In some cases, the source code is
deterministic and contains a bug. The ﬂakiness is in the
test code, but the test, by failing even intermittently, hel ps
in uncovering the real bug in the CUT.
Non-Deterministic Bug. Thesourcecodeisnon-determi-
nistic and contains a bug (e.g., a race condition) that cause s
ﬂakiness. The ﬂaky failures again help to uncover the bug.
Non-Deterministic No Bug. The code is non-determi-
nistic but contains no bug. However, the developers decide
to make it more deterministic to avoid some cases. Such
change can help in writing tests because the ﬂaky test need
not consider all the possible correct results.
See Finding F.12 and Implication I.12 in Table 1.6. THREATS TO V ALIDITY
Our study is empirical and has the common threats of
internal, external, and construct validity as any empirica l
study. We focus on more speciﬁc issues.
Choice of projects. We study only a subset of all soft-
ware projects, so our results may not generalize. To address
this threat, we consider allthe projects from the Apache
Software Foundation. We examine a diverse set of projects
with more than 20 million LOC (just in the projects with
ﬂaky tests) and 1.5 million commits. The projects use diﬀer-
ent languages and various types of applications (web server ,
databases, cloud, graphics, mail, build-management, etc. ).
However, Apachedoes notcontain manymobile applications
(such as Android Apps) or GUI tests [26], so some of our
ﬁndings and implications, such as platform independence,
may not apply to those cases.
Selection Criteria. In selecting the cases to study, we
(1) rely only on the commit log, (2) use speciﬁc keywords to
search it, and (3) study only the ﬁxed tests. Section 2 ex-
plains in detail the beneﬁts of choices (1) and (3), in partic -
ular ﬁnding ﬁxed ﬂaky tests that other approaches (such as
relying on bug reports) could miss. However, we could still
miss many ﬂaky tests that are only reported in bug reports
and never got ﬁxed. Concerning choice (2), we search the
commit messages using only two keywords, “intermit” and
“ﬂak”, so there is no guarantee on the recall of our search. In
fact, we believe oursearch could miss manyﬂakytests whose
ﬁxescouldusewords like“concurrency”,“race”,“stall”,“ fail”,
etc. In our future work we intend to expand our search for
likely ﬁxes of ﬂaky tests. The large number of commits we
already ﬁnd with just two keywords shows that ﬂaky tests
are an important problem to study.
Manual inspection. Labeling and characterizing commits
manually can lead to incorrect labeling, e.g., our analysis
phase found some false positives from our ﬁltering phase.
However, the number of false positives is relatively low. Fu r-
ther, to minimize the probability of error, two authors inde -
pendently inspect every commit and then merge the results.
7. RELATED WORK
Severalresearchersandpractitionershavepointedoutpro b-
lems with non-deterministic tests [1,7,12,13,20,21,26,2 9,32,
34,37]. Forexample, Fowler [13]describednon-determinis tic
test outcomes as a recurring problem in regression testing
and outlined some ways for avoiding and manually ﬁxing
ﬂaky tests. Memon and Cohen [26] pointed out a few pos-
sible reasons that make GUI tests ﬂaky. Lacoste [20] also
described some of the unfortunate side-eﬀects of ﬂaky tests
in automated regression testing, e.g., some features may
miss a release deadline because of intermittent test failur es.
More recently, Marinescu et al. revealed a number of non-
deterministic test suites in their study that analyzed evol u-
tion of test suite coverage [25].
Non-deterministic bugs and tests. Most existing work
on non-deterministic bugs (either in the CUT or in the test
code) focuses on onespeciﬁc category of non-determinism
causes. For example, several researchers focus on Test Or-
der Dependency . Zhang et al. formalized the test depen-
dency problem, studied real-world test suites with test de-
pendency problems, and implemented several techniques to
identify these tests by reordering test runs [37]. Mu¸ slu et
al. found that isolating unit tests can be helpful in detecti ngfaults, butenforcingisolation canbecomputationallyexp en-
sive [29]. Bell and Kaiser proposed an automatic approach
for isolating unit tests in Java applications by tracking al l
side-eﬀects on shared memory objects and undoing these ef-
fects between tests [7].
Concurrency was also well studied. For example, Farchi
et al. summarized common bug patterns in concurrent code
and employed static analysis to detect some of them [11].
Lu et al. published a comprehensive characteristic study [2 4]
examining bug patterns, manifestation, and ﬁxes of concur-
rency bugs. Compared to prior work, our study focuses on
characterizing ﬂaky tests across allcategories, and we start
from commit logs rather than just bug reports. Our results
showthatsomecategories ofﬂakytests suchas Async Wait
andConcurrency are more prevalent than Test Order
Dependency and should likely get more attention.
While we focus on ﬂaky tests, some causes that we ﬁnd
are general non-determinism bugs in the CUT. However, we
ignore the cases of bugs in the CUT that do not result in
test ﬂakiness, but we do include the cases of test ﬂakiness
that may have no bug in the CUT (e.g., Test Order De-
pendency ). We believe that the causes and ﬁxes for ﬂaky
tests diﬀer enough from general bugs in the CUT to warrant
more special focus.
Bug-ﬁxes study. Researchers have also studied diﬀerent
characteristics of bug ﬁxes. For example, Murphy-Hill et al .
conducted a large study to ﬁnd factors that inﬂuence how
bugs get ﬁxed [30]. Bachman et al. found that many bug-ﬁx
commits do not have corresponding bug reports [5]; their
results motivate us to start from commits instead of bug re-
ports. Automated techniques have also been proposed to ﬁx
concurrency bugs by Jin et al. [18]. Our study revealed a
number of diﬀerent strategies for ﬁxing ﬂaky tests in diﬀer-
ent categories, and we believe that our ﬁndings can help in
developing more automated techniques for ﬁxing bugs.
Fixing tests. Daniel et al. proposed an automated tech-
nique for ﬁxing broken tests [9]. Yang et al. proposed a dif-
ferenttechniqueusingAlloyspeciﬁcationstorepairtests [36].
However, existing test repair only focuses on broken tests
that fail deterministically, while we study ﬂaky tests that
fail non-deterministically.
8. CONCLUSIONS
Regression testing is important but can be greatly under-
mined by ﬂaky tests. We have studied a number of ﬁxes to
ﬂaky tests to understand the common root causes, identify
approaches that could manifest ﬂaky behavior, and describe
common strategies thatdevelopers usetoﬁxﬂakytests. Our
analysis provides some hope for combating ﬂaky tests: while
there is no silver bullet solution that can address all cate-
gories of ﬂaky tests, there are broad enough categories for
which it should be feasible to develop automated solutions
to manifest, debug, and ﬁx ﬂaky tests.
9. ACKNOWLEDGMENTS
We thank John Micco for sharing personal experience
about ﬂaky tests at Google, and Sebastian Elbaum and Sai
Zhang for the valuable discussions about this work. This re-
search was partially supported by the NSF Grant Nos. CNS-
0958199 and CCF-1012759, and the DARPA grant FA8750-
12-C-0284. Farah Hariri was also supported by the Saburo
Muroga Endowed Fellowship.10. REFERENCES
[1] API design wiki - OrderOfElements.
http://wiki.apidesign.org/wiki/OrderOfElements .
[2] Android FlakyTest annotation.
http://goo.gl/e8PILv .
[3] Apache Software Foundation SVN Repository.
http://svn.apache.org/repos/asf/ .
[4] Apache Software Foundation. HBASE-2684.
https://issues.apache.org/jira/browse/HBASE-2684 .
[5] A. Bachmann, C. Bird, F. Rahman, P. T. Devanbu,
and A. Bernstein. The missing links: bugs and bug-ﬁx
commits. In FSE, 2010.
[6] E. T. Barr, T. Vo, V. Le, and Z. Su. Automatic
detection of ﬂoating-point exceptions. In POPL, 2013.
[7] J. Bell and G. Kaiser. Unit test virtualization with
VMVM. In ICSE, 2014.
[8] N. Bettenburg, W. Shang, W. M. Ibrahim, B. Adams,
Y. Zou, and A. E. Hassan. An empirical study on
inconsistent changes to code clones at the release level.
SCP, 2012.
[9] B. Daniel, V. Jagannath, D. Dig, and D. Marinov.
ReAssert: Suggesting repairs for broken unit tests. In
ASE, 2009.
[10] O. Edelstein, E. Farchi, E. Goldin, Y. Nir, G. Ratsaby,
and S. Ur. Framework for Testing Multi-Threaded
Java Programs. CCPE, 2003.
[11] E. Farchi, Y. Nir, and S. Ur. Concurrent bug patterns
and how to test them. In IPDPS, 2003.
[12] Flakiness dashboard HOWTO.
http://goo.gl/JRZ1J8 .
[13] M. Fowler. Eradicating non-determinism in tests.
http://goo.gl/cDDGmm .
[14] P. Guo, T. Zimmermann, N. Nagappan, and
B. Murphy. Characterizing and predicting which bugs
get ﬁxed: an empirical study of Microsoft Windows. In
ICSE, 2010.
[15] P. Gupta, M. Ivey, and J. Penix. Testing at the speed
and scale of Google, 2011. http://goo.gl/2B5cyl .
[16] V. Jagannath, M. Gligoric, D. Jin, Q. Luo, G. Rosu,
and D. Marinov. Improved multithreaded unit testing.
InFSE, 2011.
[17] Jenkins RandomFail annotation.
http://goo.gl/tzyC0W .
[18] G. Jin, L. Song, W. Zhang, S. Lu, and B. Liblit.Automated atomicity-violation ﬁxing. In PLDI, 2011.
[19] JUnit and Java7. http://goo.gl/g4crZL .
[20] F. Lacoste. Killing the gatekeeper: Introducing a
continuous integration system. In Agile, 2009.
[21] T. Lavers and L. Peters. Swing Extreme Testing . 2008.
[22] Z. Li, L. Tan, X. Wang, S. Lu, Y. Zhou, and C. Zhai.
Have things changed now?: An empirical study of bug
characteristics in modern open source software. In
ASID, 2006.
[23] Y. Lin and D. Dig. CHECK-THEN-ACT misuse of
Java concurrent collections. In ICST, 2013.
[24] S. Lu, S. Park, E. Seo, and Y. Zhou. Learning from
mistakes: A comprehensive study on real world
concurrency bug characteristics. In ASPLOS , 2008.
[25] P. Marinescu, P. Hosek, and C. Cadar. Covrig: A
framework for the analysis of code, test, and coverage
evolution in real software. In ISSTA, 2014.
[26] A. M. Memon and M. B. Cohen. Automated testing of
GUI applications: models, tools, and controlling
ﬂakiness. In ICSE, 2013.
[27] J. Micco. Continuous integration at Google scale,
2013.http://goo.gl/0qnzGj .
[28] B. P. Miller, L. Fredriksen, and B. So. An empirical
study of the reliability of Unix utilities. CACM, 1990.
[29] K. Mu¸ slu, B. Soran, and J. Wuttke. Finding bugs by
isolating unit tests. ESEC/FSE, 2011.
[30] E. R. Murphy-Hill, T. Zimmermann, C. Bird, and
N. Nagappan. The design of bug ﬁxes. In ICSE, 2013.
[31] Spring Repeat Annotation. http://goo.gl/vnfU3Y .
[32] P. Sudarshan. No more ﬂaky tests on the Go team.
http://goo.gl/BiWaE1 .
[33] 6 tips for writing robust, maintainable unit tests.
http://blog.melski.net/tag/unit-tests .
[34] TotT: Avoiding ﬂakey tests. http://goo.gl/vHE47r .
[35] R. Tzoref, S. Ur, and E. Yom-Tov. Instrumenting
where it hurts: An automatic concurrent debugging
technique. In ISSTA, 2007.
[36] G. Yang, S. Khurshid, and M. Kim.
Speciﬁcation-based test repair using a lightweight
formal method. In FM, 2012.
[37] S. Zhang, D. Jalali, J. Wuttke, K. Muslu, M. Ernst,
and D. Notkin. Empirically revisiting the test
independence assumption. In ISSTA, 2014.