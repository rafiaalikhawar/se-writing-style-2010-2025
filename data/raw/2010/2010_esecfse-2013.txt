Second-Order Constraints in Dynamic Invariant Inference
Kaituo Li
Computer Science Dept.
University of Massachusetts
Amherst, MA 01003, USAChristoph Reichenbach
Institut fÃ¼r Informatik
Goethe University Frankfurt
Frankfurt am Main 60054,
GermanyY annis Smaragdakis
Dept. of Informatics
University of Athens
Athens 15784, Greece
Michal Y oung
Computer Science Dept.
University of Oregon
Eugene, OR 97403, USA
ABSTRACT
The current generation of dynamic invariant detectors often pro-
duce invariants that are inconsistent with program semantics or
programmer knowledge. We improve the consistency of dynami-
cally discovered invariants by taking into account higher-level con-
straints. These constraints encode knowledge about invariants,
even when the invariants themselves are unknown. For instance,
even though the invariants describing the behavior of two functions
f1andf2may be unknown, we may know that any valid input for
f1is also valid for f2, i.e., the precondition of f1implies that of f2.
We explore techniques for expressing and employing such consis-
tency constraints to improve the quality of produced invariants. We
further introduce techniques for dynamically discovering potential
second-order constraints that the programmer can subsequently ap-
prove or reject.
Our implementation builds on the Daikon tool, with a vocab-
ulary of constraints that the programmer can use to enhance and
constrain Daikonâ€™s inference. We show that dynamic inference of
second-order constraints together with minimal human effort can
signiï¬cantly inï¬‚uence the produced (ï¬rst-order) invariants even in
systems of substantial size, such as the Apache Commons Collec-
tions and the AspectJ compiler. We also ï¬nd that 99% of the dy-
namically inferred second-order constraints we sampled are true.
Categories and Subject Descriptors
D.2.4 [ Software Engineering ]: Software/Program Veriï¬cationâ€”
Class invariants ; D.2.5 [ Software Engineering ]: Testing and De-
buggingâ€” testing tools
General Terms
Reliability,Veriï¬cation
Keywords
Daikon, dynamic invariant inference, second-order constraints
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proï¬t or commercial advantage and that copies
bear this notice and the full citation on the ï¬rst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciï¬c
permission and/or a fee.
ESEC/FSE â€™13, August 18-26, 2013, Saint Petersburg, Russia
Copyright 2013 ACM 978-1-4503-2237-9/13/08 ...$10.00.1. INTRODUCTION AND MOTIV ATION
Systematically understanding program properties is a task at the
core of many software engineering activities. Software testing,
software documentation, and software maintenance beneï¬t directly
from any signiï¬cant advance in automatically inferring program
behavior. Dynamic invariant detection is an intriguing behavior
inference approach that has received considerable attention in the
recent research literature. Tools such as Daikon [6], DIDUCE [14],
and DySy [5] monitor a large number of program executions and
heuristically infer abstract logical properties of the program, ex-
pressed as invariants.
Dynamic invariant inference is fundamentally a search in the
space of propositions over program variables for the tiny subset of
propositions that are both true (on all executions) and relevant (to
the programmer, or to some client analysis such as a theorem prover
or test data generator). Any such search requires a way of generat-
ing candidate propositions and a way of evaluating them. Even lim-
iting consideration to propositions of ï¬xed length, over a particular
vocabulary of operations and relations among variables, the space
is enormous, and for a long time it seemed folly to even attempt
such a search based on dynamic program execution. Daikon [6]
was the ï¬rst demonstration that a generate-and-test tactic could be
feasible, generating a moderately large set of candidate proposi-
tions from syntactic templates and quickly pruning the vast major-
ity as they were refuted by program execution.
In this work we enhance the dynamic invariant detection ap-
proach by including second-order constraints. The invariants in-
ferred should be consistent with these constraints. More specif-
ically, we identify a vocabulary of constraints on inferred invari-
ants. We call these constraints â€œsecond-orderâ€ because they are
constraints over constraints: they relate classes of invariants (ï¬rst-
order constraints). Such second-order constraints can be known
even though the invariants are unknown. (To avoid confusion, we
try to consistently use the term â€œconstraintsâ€ for second-order con-
straints, as opposed to the ï¬rst-order â€œinvariantsâ€, although the
term â€œsecond-order invariantsâ€ would have been equally applica-
ble.) Our vocabulary includes concepts such as Subdomain (one
methodâ€™s precondition implies that of another); Subrange (one
methodâ€™s postcondition implies that of another); CanFollow (one
methodâ€™s postcondition implies the precondition of another); Con-
cord (two methods satisfy a common property for the common part
of their domains, although some inputs for one may be invalid for
the other); and more.
Such constraints arise fairly naturally. Two methods may be ap-
plicable at exactly the same state of an input. For instance, the top
andpopmethods of a stack data structure will have constraints:Subdomain(top, pop)
Subdomain(pop, top)
This signiï¬es that their preconditions are equivalent. As another
example, a method may be a specialized implementation of an-
other (e.g., a matrix multiplication with a faster algorithm, appli-
cable to upper triangular matrices only, relative to a general ma-
trix multiplier). This induces a Subdomain(triangMatMul(),
matMul()) constraint. Similarly, a method may be preparing an
objectâ€™s state for other operations, as in the common case of open
andread :CanFollow(open(), read()) .
The purpose served by second-order constraints is dual:
First, when the constraints are discovered dynamically, without
any human effort expended, they can serve as a concise and deeper
documentation of program behaviorâ€”e.g., a single second-order
constraint can distill the meaning of many Daikon invariants. This
can be seen as an effort to generalize the observations that the
Daikon system already makes via sophisticated post-processing.
For instance, state-machine behavior (e.g., calls to method foo
can follow calls to method bar) can be documented in the vocab-
ulary of second-order constraints.
Second, when the (second-order) constraints are either supplied
by the programmer or vetted by the programmer (among candi-
date constraints suggested automatically) they can help enhance
dynamically inferred (ï¬rst-order) invariants by effectively cross-
checking invariants against others. In this way, the noisy infer-
ences caused by having a limited number of dynamic observations
are reduced: the second-order constraint links the invariant at a
certain program point with a larger set of observed values from
different program points. In exchange for modest speciï¬cation
effort, second-order constraints can eliminate inconsistencies and
produce more relevant and more likely-correct invariants.
In more detail, the main contributions of our paper are as follows:
We identify and describe the idea of second-order constraints that
help improve the consistency and relevance of dynamically in-
ferred invariants.
We deï¬ne a vocabulary of common second-order constraints.
We present a generalization of Daikonâ€™s dynamic inference ap-
proach to second-order constraints, by appropriately deï¬ning the
Daikon â€œconï¬denceâ€ metric for second-order properties.
We discuss how second-order constraints can be implemented,
how their enforcement is affected by properties of the dynamic
invariant inference system (we identify monotonicity as a partic-
ularly important property). We provide an implementation of the
constraints in our vocabulary in the context of the Daikon system.
We provide experimental evidence for the value of second-order
constraints and their dynamic inference. We evaluate our ap-
proach both in controlled micro-benchmarks and in realistic eval-
uations with the Apache Commons Collections and the AspectJ
compiler. A low-cost effort to write constraints (e.g., a few hours
spent to produce 26 constraints) results in signiï¬cant differences
in the dynamically inferred invariants. Furthermore, the vast ma-
jority (99%) of a random sample among our dynamically inferred
second-order constraints are found to be true.
2. INV ARIANTS AND CONSTRAINTS
We begin with a brief background on dynamic invariant infer-
ence, and subsequently present our vocabulary of second-order
constraints. For illustration purposes, our discussion is in the con-
text of Java and the Daikon invariant inference tool. This is themost mainstream combination of programming language and dy-
namic invariant inference tool, but similar observations apply to
different contexts and tools.
2.1 Daikon and Dynamic Invariant Inference
Daikon [6,24] tracks a programâ€™s variables during execution and
generalizes their observed behavior to invariants. Daikon instru-
ments the program, executes it (for example, on an existing test
suite or during production use), and analyzes the produced exe-
cution traces. At each method entry and exit, Daikon instantiates
some three dozen invariant templates, including unary, binary, and
ternary relations over scalars, and relations over arrays (relations
include linear equations, orderings, implication, and disjunction).
For each invariant template, Daikon tries several combinations of
method parameters, method results, and object state. For example,
it might propose that some method mnever returns null. It later dis-
cards those invariants that are refuted by an execution traceâ€”for
example, it might process a situation where mreturned null and it
would therefore discard the above invariant. Daikon summarizes
the behavior observed in the execution traces as invariants and gen-
eralizes it by proposing that the invariants hold in all other execu-
tions as well. Daikon can annotate the programâ€™s source code with
the inferred invariants as preconditions, postconditions, and class
invariants in the JML [18] speciï¬cation language for Java.
2.2 A Vocabulary of Constraints
To express rich constraints on invariants, we use a vocabulary in-
cluding constraints such as Subdomain andSubrange . These con-
straints are speciï¬ed explicitly by the user of the invariant infer-
ence system by employing an annotation language. Furthermore,
we give the user an opportunity to express one more useful kind
of information regarding inferred constraints: a set of â€œcare-aboutâ€
program variables and ï¬elds that are allowed in inferred invariants.
Our vocabulary includes the following concepts:
OnlyCareAboutVariable( hvari)and OnlyCareAboutField( hï¬‚di):
These annotations instruct the invariant inference system that the
derived invariants of a method should only contain the listed
method formal argument variables or class ï¬elds. This prevents
the system from deriving facts irrelevant to the property under
examination, which would also be more likely to be erroneous.
Additionally, it allows restricting invariants so that complex con-
straints can be expressed. The OnlyCareAbout... constraints are
not consistency constraints on invariants, but they are essential
background for use together with the consistency constraints that
follow.
Subdomain(m1,m2) : The precondition of method m1implies the
precondition of method m2. For instance, the author of a matrix
library may specify the constraint:
Subdomain(processDiagonal, processUpperTriangular)
This means that method processDiagonal , which works on di-
agonal matrices, has a precondition that implies that of method
processUpperTriangular , which works on upper triangular
matrices: the latter method is applicable wherever the former is.
(Matrix libraries often have a large number of related operations,
in terms of interface and applicability, but with different imple-
mentations for performance reasons [13]. In this section, we draw
several examples from this domain for illustration purposes.)
Preconditions are not only expressible on method parameters but
also on member ï¬elds of an object. E.g., one may specify that the
preconditions of method append imply those of method write :
If the object is in an appendable state, it can also be written to.In order to specify useful Subdomain constraints, one may need
to limit the inferred invariants by employing the OnlyCareAbout-
Variable andOnlyCareAboutField annotations presented earlier.
(This also holds for many of the later constraints.) For instance,
methods m1andm2may be on separate objects. Thus, their in-
variants would normally include distinct sets of variables. Yet if
we specify their common variables with OnlyCareAboutVariable ,
then the Subdomain constraint can be applicable.1
Subrange(m1,m2) : This constraint is analogous to Subdomain but
for postconditions. It means that the postcondition of method m1
is stronger than (i.e., implies) that of m2. Again, the condition
may apply to parameter or return valuesâ€”e.g., a method with
postcondition isDiagonal(return) is related with Subrange to
one with postcondition isUpperTriangular(return) . (Where
return stands for the return value of the method.) Yet the con-
dition can also be on object state. For instance, we may have
constraints such as:
Subrange(listTail,listRange)
This means that the state of the return value and overall object
after execution of the method listTail is also a valid state after
listRange : the former method produces a subset of the possible
results and effects of the latter.
CanFollow(m1,m2) : This constraint means that the postcondition
of method m1implies the precondition of method m2. Execution
of method m2is enabled by execution of m1in this sense, and it
is a natural way to capture temporal sequencing rules, e.g.,:
CanFollow(open,write)
Such speciï¬cations are common and are often expressed in the
form of state machines. Note that the above constraint means that
â€œanopen can be followed by a write â€, not â€œa write has to be
preceded by an open â€. (The latter is inexpressible, since our vo-
cabulary is only concerned with the relationship between condi-
tions at two program points, but there is nothing to prohibit other
program points from establishing the same conditions.)
Follows(m1,m2) : This constraint means that the precondition of
method m2implies the postcondition of method m1. Informally,
this means that the only states that m2can start from are states
thatm1 may result in. m1does not guarantee such a state, though:
m1â€™s postcondition is necessary but not sufï¬cient for preparing to
execute m2. In practice, this constraint prevents the postcondition
ofm1from being more speciï¬c than necessary. For instance, we
may have:
Follows(add,remove)
This does not signify that addneeds to execute before remove (a
different method may be reaching the same states as add). But it
means that if we are in a state where the last operation cannot have
been an add(e.g., an empty structure), then we cannot remove .
Concord(m1,m2) :Concord means that the postcondition of
method m1implies that of method m2but only for the values that
satisfy the preconditions of both m1andm2(or, put another way,
1Our implementation provides some default restrictions on which
variables are considered when second-order constraints are used, so
that common cases are covered without explicit OnlyCareAbout...
annotations. Namely, when two methods are linked by a second-
order constraint, their invariants are deï¬ned over â€œmatchingâ€ argu-
ments and ï¬elds. Roughly, argument variables match if they are in
the same position and the methods have the same arity, and class
ï¬elds match if they have the same name and type.only for the values for which the precondition of m1implies that
ofm2). This is a valuable constraint for methods that specialize
other methods. For instance, there can be a fully general matrix
multiply routine, a more efï¬cient one for when the ï¬rst argument
is an upper triangular matrix, one for when the ï¬rst argument is a
diagonal matrix, etc. The operationsâ€™ invariants are linked with a
Concord constraint:
Concord(triangularMultiply, matrixMultiply)
Thus, the postcondition of the specialized operation
(triangularMultiply ) should imply that of the more general
operation (indeed, in this case the conditions should be equivalent)
for all their common inputs. Nevertheless, the general operation,
matrixMultiply , is applicable in more cases and, thus, has a
weaker precondition than the specialized one. For inputs valid for
matrixMultiply but not triangularMultiply , the Concord
constraint imposes no restriction.
2.3 Meaning and Completeness of Second-
Order Constraints
We intend for second-order constraints to be devices for express-
ing relationships between program elements at a high level. This
means that the precise interpretation of the constraints will depend
on the speciï¬cs of the invariant inference system we use and on its
capabilities. As a guideline, we present the intended meaning of
ï¬ve of our constraints in Figure 1, column Pre/Post . (We omit the
â€˜OnlyCareAboutâ€™ predicates as they are straightforward.) In that
column we summarize the meanings of each constraint as logical
relations over the preconditions Premand postconditions Postmof
all involved methods.
We assume in this presentation that all preconditions use the
same names for parameters in the same position, and all postcon-
ditions use the same names for their return value. In other words,
Prem1=)Prem2means that any constraints that Prem1places
on the ï¬rst (second, third, . . . ) parameter of m1,Prem2also places
on the ï¬rst (second, third, . . . ) parameter of m2.
As we can see, the ï¬rst four predicates capture all possible
â€˜straightforwardâ€™ implications between the preconditions and post-
conditions of two methods. Predicate Concord meanwhile captures
a more complex but practically important second-order constraint.
We will return to this ï¬gure later, when we describe our imple-
mentation.
2.4 Discussion
Second-order constraints can serve as documentation of program
behavior in much the same way that ï¬rst-order invariants can (i.e.,
for program understanding purposes). Furthermore, second-order
constraints can offer external knowledge that helps steer invariant
inference in the right direction. Certainly one reason to do this is to
avoid erroneous invariants: second-order constraints serve to cross-
check invariants and thus enhance the dynamic opportunities to in-
validate them, even with a limited number of observations. Another
important use of constraints, however, is in deriving more relevant
invariants. Clearly, a simple use of OnlyCareAbout... can prevent
some irrelevant invariants. Greater beneï¬t can be obtained by more
abstract constraints, however. Consider, for example, a constraint
on the constructor of a class Cand on one of Câ€™s methods, m. If
we have the constraint Follows (C,m) then we have a hint that the
constructor establishes part of the precondition for m. Without the
constraint, we may observe the constructorâ€™s execution and derive
for a certain ï¬eld fld of the class the postcondition â€œ this.fld
== 100 â€. This inference would certainly be reasonable, if, when-
ever the constructor is executed, fldis assigned the value 100. YetConstraint Pre/Post Dataï¬‚ow
Subdomain(m1,m2)Prem1=)Prem2 int(m1)int(m2)
Subrange(m1,m2) Postm1=)Postm2 outt(m1)outt(m2)
CanFollow(m1,m2)Postm1=)Prem2 outt(m1)int(m2)
Follows(m1,m2) Prem2=)Postm1 int(m2)outt(m1)
Concord(m1,m2) Prem1^Prem2=)(Postm1=)Postm2)Prem1(t)^Prem2(t) =)outt(m1)outt(m2)
Figure 1: Meanings of second-order constraints.
the invariant could be overly speciï¬c. Given that we know that the
constructorâ€™s postcondition is implied by mâ€™s precondition, we can
observe all the different circumstances when mis called (not neces-
sarily right after construction). This may yield the broader precon-
dition â€œ this.fld > 0 â€ for m. Because of the Follows constraint,
the constructor needs to also register all the same values and, hence,
changes its postcondition to â€œ this.fld > 0 â€ instead of the more
speciï¬c â€œ this.fld == 100 â€. The generality can mean that the
new postcondition is more meaningful from a program understand-
ing standpoint. The speciï¬c value 100 may be just an artifact of
the speciï¬c test cases used (i.e., the postcondition â€œ this.fld ==
100â€ may be erroneously over-speciï¬c) but, even if it is correct, it
may be the result of an arbitrary technicality or a detail likely to
change in the next version of the program. Certainly, the Follows
constraint steers the invariant inference to the kind of invariant the
user wants to see in this case: the condition that (partly) enables m
to run.
It is worth noting that the principle of behavioral subtyping [18]
can be viewed as a combination of constraints from our vocabu-
lary. Informally, behavioral subtyping is the requirement that an
overriding method should be usable everywhere the method it over-
rides can be used. This is a common concept, employed also in
program analyzers (e.g., ESC/Java2 [3]) and design methodologies
(e.g., â€œsubcontractingâ€ in Design by Contract [23, p.576]). When a
method C.moverrides another, S.m, the behavioral subtyping con-
straints consist of a combination of Subdomain (S.m,C.m) and Con-
cord(C.m,S.m). (Implicitly there are also OnlyCareAboutVariable
andOnlyCareAboutField constraints that restrict both the precon-
ditions and postconditions to be over parameters of method mand
member variables of the superclass S.) In past work [4] we dis-
cussed how consistent behavioral subtyping can be supported in the
context of dynamic invariant inference systems, and similar ideas
have informed the present work.
3. DESIGN AND IMPLEMENTATION
We next discuss the crucial design questions regarding a second-
order constraints facility, as well as our own implementation de-
cisions. Before this, however, we introduce the concept of mono-
tonicity for a dynamic invariant inference system. This concept
plays a central role in our discussion and understanding.
3.1 Background: Monotonicity
An important consideration for our later implementation argu-
ments is whether our invariant inference system is (mostly) mono-
tonic : If supplied more values to observe, the conditions it will out-
put will be logically weaker (i.e., broader). Note that monotonicity
applies to the logical predicates and not to the number of invariants.
There may be fewer invariants produced when more values are ob-
served, but these will be implied by the invariants produced for any
subset of the values.
Most dynamic invariant inference tools are in principle mono-
tonic. Tools like DySy [5] and Krystal [17] use a combination
of dynamic and symbolic execution. The invariants are computedfrom the boolean conditions that occur inside the program text,
when these are symbolically evaluated. The more executions are
observed, the more the invariants of a program point are weakened
by the addition of extra symbolic predicates (in a disjunction). Sim-
ilarly, tools like Daikon or DIDUCE have invariant templates that
are (multiply) instantiated to produce concrete candidate invariants.
The candidate invariants are conceptually organized in hierarchies
from more to less speciï¬c. Values observed during execution are
used to falsify invariants and remove them from the candidate set.
Themost speciï¬c non-falsiï¬ed candidate invariant of each hierar-
chy is a reported invariant. For instance, if two candidate invariants
â€œx == 0 â€ and â€œ x >= 0 â€ are satisï¬ed by all observed values of x,
then the former will be reported, as it is more speciï¬c. This is a
monotonic approach: It means that as candidate invariants are fal-
siï¬ed, they are replaced by more general conditions.
Nevertheless, invariant detection tools often include non-
monotonicities in their inference logic. For instance, Daikon uses a
conï¬dence threshold for invariants: An invariant is not output even
when consistent with all observations, if the number of pertinent
observations is small. Similarly, Daikon treats some boundary val-
ues and invariant forms specially. For instance, if the maximum
value for a variable is -2, Daikon will not output an invariant of
the form x <= -2 , but if it additionally observes the value 0 for
the variable, it will output x <= 0 (an invariant that would have
been true even without observing the 0 value, but would not have
been reported). Furthermore, Daikon produces the weakest predi-
cate ( true) when no invariants can be inferred. These features entail
non-monotonicity: Observing more inputs will produce a stricter
invariant.
In general, some non-monotonicity is unavoidable in practically
useful dynamic invariant inference tools, even if just in corner
cases. For instance, if a tool is strictly monotonic, then it has to in-
fer a precondition and postcondition of false for every method that
happens to not be exercised by the test suite. (This is the only log-
ical condition that is guaranteed to be weakenedâ€”as monotonic-
ity prescribesâ€”when we add an execution that does exercise the
method.) Computing invariants of false is a sound approach (accu-
rately captures observations) but largely useless. Any further use
of the produced invariants cannot employ the method in any way,
as it can never establish the preconditions for calling it or know
anything about its results. Effectively, monotonic invariant infer-
ence would have to report that a method is forever unusable if it
was not observed to be used in the test run. This and other similar
examples in practically signiï¬cant cases preclude the use of strictly
monotonic dynamic invariant inference.
3.2 Implementation
We implemented second-order constraints in the Daikon system.
(Daikon is not only the mainstream option for dynamic invariant
inference, but also convenient for engineering purposes. The DySy
tool [5], on which we have worked in the past, was another inter-
esting prospect, but requires C#/.NET and a specialized, closed-
source, dynamic-symbolic execution infrastructure.) There are two
aspects to the implementation. First, we infer second-order auto-matically through dynamic observations of program behavior. Sec-
ond, we allow treating second-order constraints as â€œground truthâ€
and use them to inï¬‚uence regular invariant inference.
Dynamically inferring second-order constraints :The key con-
struct in most of our second-order constraint is an implication be-
tween pre/post conditions PandQat two different program points
(with each one being either the entry or the exit of a program
procedure). Our strategy for proving the second-order constraint
P=)Qis to ï¬rst use Daikon (Rev. 2eded4baf181) to produce
PandQand then employ the Simplify theorem prover to check
whether Pcan imply Q. The full implication check may fail, even
if the implication holds in principle. One reason for this is that an
inadequate test suite may lead Daikon to produce a slightly inac-
curate PorQ. Furthermore, the theorem prover may be unable to
decide whether an implication is satisï¬able or not. Therefore, a
more realistic check, since PandQare expressed as conjunctions
of invariants, is to see if a particularly large number of invariants
inQare subsumed by invariants of P(i.e., implied by the full con-
junction P).
In order to ï¬nd whether a sufï¬ciently large number of implica-
tions between Pand the constituent invariants of Qare true, we
deï¬ne the success rate (SA) of the implication as SA=N
M, where
Nrepresents the number of invariants in Qthat can be implied by
PandMdenotes the number of invariants of Q.
The success rate on its own is often not enough for comparing
the likelihood of a second order constraint being true. In our deï¬-
nition of success rate, all invariants are treated equally, so in a case
with lots of junk invariants in Pbut not in Qthat ratio may get over-
whelmed by the junk. Daikon uses a conï¬dence metric to approxi-
mate the probability that an invariant cannot hold by chance alone.
If we sum up the conï¬dence values for all invariant implications
between Pand each constituent invariant of Q, then the weight of
the junk invariants in the ratio will decrease. Thus, we extend the
deï¬nition of conï¬dence to second-order constraints in order to ap-
proximately measure the probability that a second-order constraint
could not hold by chance alone. For regular invariants, Daikon de-
ï¬nes the conï¬dence of an implication relation ( Inv1 =)Inv2) to
be the product of the conï¬dence of the guard ( Inv1) and the con-
ï¬dence of the consequent ( Inv2). Similarly, the second-order con-
straint conï¬dence of P=)Qcan be deï¬ned as the mean of the
conï¬dences of the implication relations between Pand each invari-
ant of Qthat is implied (according to Simplify). Representing the
conï¬dences of invariants in Pbypc1,pc2, ...,pcZ, and denoting
the conï¬dences of invariants of Qthat can be implied by Pbyqc1,
qc2, ...,qcN, we calculate the second-order constraint conï¬dence,
MA, asMA=pc1pc2:::pcZ(qc1+qc2+:::+qcN)
N.
We ï¬lter out second-order constraints whose success rate is be-
low a threshold (by default 0:75) and rank the unï¬ltered second-
order constraints according to conï¬dence. It is meaningful for the
user to change the success rate threshold, possibly based on the ac-
curacy of invariants produced by Daikon for the program at hand.
The fewer redundant and irrelevant invariants a program has, the
higher the programâ€™s success rate ï¬lter can be.
The time complexity of our algorithm is quadratic over the num-
ber of methods. We consider all combinations of program points
inside a class as candidate second-order constraints. If there are n
methods in a class, there are 2nprogram points, i.e., 2n(2n 1)po-
tential second-order constraints. To verify a candidate second-order
constraint, we invoke the Simplify prover on implications between
invariants. Removing irrelevant and redundant invariants before
applying our tool can help reduce theorem proving time. (Standard
avenues for doing so with Daikon include using more represen-tative test suites and employing the DynComp tool [12] for more
accurate invariants.)
We use a few heuristics to improve the accuracy of derived
second-order constraints. One heuristic is to normalize parameters:
a parameterâ€™s name is based on its position in the parameter list.
For example, if we examine the second-order constraint Subdo-
main (foo(int i, int j) ,bar(int x, int y) ) then we do
not use the names iandjwhen we compare to bar(int x, int
y). Instead, we substitute arg1 foriandx, and arg2 forjandy.
We implement this normalization via regular expression transfor-
mation on the data trace ï¬le generated by Chicory (the Java instru-
menter inside Daikon). In addition, if a variable only exists in the
consequent-side program point of a second-order constraint, it of-
ten gets in the way of veriï¬cation for the second-order constraint.
We ignore invariants that contain such variables in the consequent-
side program point. Another heuristic is that invariants over vari-
ables that are likely to be meaningless for cross-method compar-
isons are ignored. For instance, in the case of Follows (bar,foo)
andSubrange (foo,bar), we ignore invariants over the orig(x) vari-
able (which refers to the value of variable xupon entry to a pro-
cedure) at the exit of bar; for Subrange (foo,bar) and CanFol-
low(foo,bar), we ignore procedure parameters at both the entry
and exit point of bar; for Follows (bar,foo), we ignore the return
variable at the exit of bar.
Using second-order constraints for better invariants :We ex-
tended Daikon with an annotation mechanism for second-order
constraints. Users can pass a separate conï¬guration ï¬le with
second-order constraints to Daikon. This changes the Daikon pro-
cessing of the low-level observations and allows reï¬ning the in-
ferred invariants without having to re-run any test suites.
The main element of our approach is that when the precondition
(or postcondition) of a method m1is constrained to imply the pre-
condition (resp. postcondition) of another method m2(as shown in
our earlier Figure 1, column Dataï¬‚ow ), we propagate the values ob-
served at entry (resp. exit) of m1tom2. This ensures that producing
m2â€™s invariants takes into account all the behavior observed for m1.
Note that m2need not be executed during invariant inferenceâ€”
the conditions observed/established for m1are simply registered
as if m2had also observed/established them. These observations
are suitably adapted to be over common variables, as dictated by
OnlyCareAbout... constraints (including implicit assumptions, as
mentioned in Footnote 1).
For the ï¬rst four second-order constraints of Figure 1 the imple-
mentation of the above value propagation is straightforward, as it
can leverage existing Daikon facilities. Speciï¬cally, the implemen-
tation extensively hijacks the Daikon dataï¬‚ow hierarchy mecha-
nism. This Daikon internal facility propagates primitive invariants
between different program points (a program point is a line of code
or the entry or exit from a method). The machinery is therefore
quite suitable (with some minor adjustment to suppress ï¬ltering in
some cases) for implementing our value propagation.
However, the Concord second-order constraint is more com-
plex: we only want to ï¬‚ow primitive invariants from outt(m1)to
outt(m2)if at execution point tthe preconditions of both m1and
m2hold. However, at this point in Daikonâ€™s execution we do not
yet know what the preconditions of m1andm2should be, since
we have not even seen all primitive invariants yet. We thus evalu-
ateConcord constraints in two passes. The ï¬rst pass stores all data
from int(m1),int(m2), and outt(m1). At the end of this pass we
ask Daikon to compute the preconditions Ptom1andm2using
Daikonâ€™s usual heuristics (and any other second-order constraints).
In the second pass, we now use these preconditions on the data
from int(m1)andint(m2):P(mi;t)holds iff int(mi)satisï¬esthe precondition to miat pointt. We thus iterate over tone more
time to copy all primitive invariants from outt(m1)tooutt(m2)
and update Daikonâ€™s results as needed.
For example, assume that we are examining two methods int
m1(int x) andint m2(int y) . We make the following observations
about m1:
m1:t int(m1) outt(m1)
t1 arg1 = 3return = 1
t2 arg1 = 0 return = 0
t3 arg1 = 3 return = 1
t4 arg1 = 6 return = 2
That is, the method is invoked at time t1with its ï¬rst (and only)
parameter xbound to 3, and returns 1, etc. From the table
above, Daikon might plausibly infer the precondition Pre m1true
and postcondition Post m1return =arg1=3.
Now assume that we make the following observations about m2:
m1:tint(m2) outt(m2)
t5 arg1 = 0 return = 0
t6 arg1 = 1 return = 0
t7 arg1 = 2 return = 0
Here, Daikon sees no negative inputs and a constant output, so
it might infer precondition Pre m2arg10and postcondition
Post m2return = 0.
If the user intended m2to be a specialized version of m1for
nonnegative integers, the precondition for m2would be correct
but the postcondition would be wrong. The user can address this
by adding the constraint Concord(m1, m2) . This constraint will
make our system ï¬rst compute preconditions and postconditions as
above, then compute all the times tat which Pre m1(t)agrees with
Pre m2(t).Pre m1is always true, so we will only ï¬lter out t1via
Pre m2. Consequently, Daikon will get to see all inputs and outputs
fromt2,t3andt4in addition to the ones it was already consid-
ering for method m2. With this additional data, Daikon can no
longer infer Post m2return = 0 but might instead conclude
Post m2return =arg1=3^return0, which matches the
userâ€™s intention.
3.3 Discussion
It is evident from the previous section that our two mechanisms
(that of dynamically inferring second-order constraints and that of
taking them into account when inferring invariants) operate differ-
ently. The former follows a static approach for checking invari-
ant implication: the Simplify system is used as a symbolic prover.
However, the mechanism of enforcing invariant implications (to
produce different invariants by taking second-order constraints into
account) eschews symbolic reasoning in favor of propagating more
dynamic observations. Why do we not just take the conjunction
of the produced invariants and declared second-order constraints,
simplify it symbolically, and report it as the new produced invari-
ants? The reason is that there is noise introduced when generalizing
from observed executions to invariants (e.g., because the invariant
patterns are uneven), and this carries over to the combined invari-
ants. It is better to generalize (i.e., compute invariants) from more
observations than to generalize from fewer ones and then combine
the generalizations symbolically.
The above insight is evident in the case of non-monotonic in-
variant inferences, which are virtually unavoidable, as mentioned
in Section 3.1. For instance, consider a Subdomain(m1,m2) con-
straint. IfPis the precondition of m1andQis the precondition
ofm2, then we can statically satisfy the constraint by considering
the real precondition of m2to beP_Q, so that it is always im-
plied byP. If, however, method m1is never called in our test suite,
Daikon will infer trueas its precondition. This will make truealsobe the precondition of m2, even though we have actual observations
for that method! This result is counter-intuitive and so common in
practice as to signiï¬cantly reduce the value of produced invariants.
In contrast, in our chosen approach, a second-order constraint
just causes more observations to register. These observations
are then generalized using the same approach as the base in-
ference processâ€”i.e., just as if the system had really registered
these observations. This is particularly beneï¬cial in cases of non-
monotonicity. Consider again our example of an m1and m2
with preconditions PandQ, respectively, and a constraint Sub-
domain(m1,m2) . If m2observes the exact values that led to the
inference of P, then these values, combined with the ones that led
to the inference of Q, may induce higher conï¬dence for an invari-
ant more speciï¬c than either PorQ, which will now be reported
(because of crossing a conï¬dence threshold).
Caveats :Note that our approach (of propagating observations
from one program point to another) does not strictly guarantee that
the dynamically inferred invariants satisfy the second-order con-
straints. Interestingly, if the inference process is monotonic, cor-
rectness is guaranteed : under monotonic invariant inference, tak-
ing into account the union of two sets of observations should pro-
duce a condition that is weaker than either individual condition.
This observation argues for why our approach is expected to be
correct: as long as there are enough observations, dynamic invari-
ant inference is typically monotonic, as discussed in Section 3.1.
Additionally, any implementation of dynamic invariant inference
under second-order constraints suffers from the possibility of a dis-
connect between observed executions and values reï¬‚ected in an in-
variant. The source of the problem is that we are allowing an in-
variant to be inï¬‚uenced by values not really seen at that program
point during execution. These values will be reï¬‚ected in a reported
invariant but will not be reï¬‚ected in other dependent invariants. For
instance, using second-order constraints we may infer a more gen-
eral precondition, but not the corresponding postcondition. Thus,
readers who consider the two invariants together may misinterpret
their meaning even when the invariants are individually correct.
4. EV ALUATION
There are two questions that our evaluation seeks to answer:
Do second-order constraints aid the inference of better ï¬rst-order
invariants?
Can correct second-order constraints be inferred dynamically?
The next two sections address these questions in order.
4.1 Impact of Second-Order Constraints
We explored the utility of second-order constraints in three case
studies: one of a manageable, small example application with a rel-
atively thorough test suite, and two of large, unfamiliar programs,
with their actual test suites. In all studies, we ï¬rst took existing
classes and examined their APIs. We then added second-order con-
straints to manifest implicit relationships between API methods in
the same class or in different classes. We ran a series of experi-
ments to determine the effects that adding these constraints had on
the observed pre- and postconditions reported by Daikon.
In these three case studies, second-order constraints were writ-
ten by hand and their correctness was veriï¬ed by inspection. This
is a feasible approach in several settings since second-order con-
straints are much sparser/coarse-grained than ï¬rst-order invariants:
one needs to write few second-order constraints to affect a large
number of ï¬rst-order invariants.p u b l i c c l a s s StackAr {
p u b l i c boolean isEmpty ( ) ;
p u b l i c boolean i s F u l l ( ) ;
p u b l i c void makeEmpty ( ) ;
p u b l i c void push ( O b j e c t ) ;
p u b l i c void pop ( ) ;
p u b l i c O b j e c t t o p ( ) ;
p u b l i c O b j e c t topAndPop ( ) ;
}
Figure 2: The array-based stack StackAr , shipped with Daikon
4.1.1 StackAr
Our ï¬rst case study is StackAr , an array-based ï¬xed-size stack
implementation that ships with Daikon and is perhaps the most
common Daikon benchmark and demonstration example. StackAr
is interesting because it is the most controlled of our case studies
(due to test suite coverage and small size).
Figure 2 lists the class and its methods. Methods isEmpty
andisFull are straightforward. makeEmpty clears the stack.
push(o) pushes element oto the top of the stack. pop() removes
the top stack entry, but does not return a value. Instead, top()
peeks at the top of the stack and returns the most recently pushed
value; while topAndPop() returns the top of the stack before re-
moving it. Operations top andpop raise an exception if the stack
is empty, while topAndPop returns null in that case.
To explore our invariants, we examined this API and determined
second-order constraints that we considered to be meaningful for
this class. The process took one of the authors negligible time (less
than a minute, although there was previous discussion of interesting
invariants, hence the exact effort cost is unclear). We split these
constraints into separate experiments and explored the effect they
had on Daikonâ€™s invariant detection mechanism:
Experiment 1 ( Ex1):Subdomain ontopAndPop ,top, and pop.
The operations top,pop, and topAndPop all require a nonempty
stack, so we instructed the system to treat all of them as having
identical subdomains.
Experiment 2 ( Ex2): Any push sets up the stack for a top,pop, or
topAndPop . We experimented with setting up CanFollow relations
between push and the three top/pop operations.
To ensure the highest-quality invariants, we ran these experi-
ments with the DynComp tool [12] enabled.
Experiment 1: We instructed the system to treat all of top,pop,
andtopAndPop as having the same subdomains, using speciï¬ca-
tions such as
Subdomain(StackAr.topAndPop(), StackAr.pop())
Subdomain(StackAr.pop(), StackAr.top())
Subdomain(StackAr.top(), StackAr.topAndPop())
The above speciï¬cation captures a circular subrange depen-
dence, and hence equality: it speciï¬es that all three operations
should have effectively the same preconditions. There are other
ways to express this equality: we experimented with reversing
the circular dependencies and with establishing mutual Subdomain
constraints between all three interesting pairs of the three opera-
tions (for a total of six constraints). We found these approaches to
be equivalent.
The experiment results in the elimination of ï¬ve spurious invari-
ants from pop:
â€˜this has only one valueâ€™
â€˜this.theArray has only one valueâ€™â€˜size(this.theArray[]) == 100 â€™
â€˜this.theArray[this.topOfStack] != null â€™
â€˜this.topOfStack < size(this.theArray[])-1 â€™
Also, â€œ this.topOfStack < size(this.theArray[])-1 â€ is
replaced by the weaker (and correct) â€œ this.topOfStack <=
size(this.theArray[])-1 â€. Our constraints further helped in-
fer two new correct preconditions: one establishing an inequality
between a stackâ€™s default capacity and the size of this.theArray ,
and one establishing that the top of the stack does not exceed the
size of the internal array. As a result, the preconditions between the
three methods were identical.
Experiment 2: For this experiment we speciï¬ed that the push
operation sets up a stack for using topand similar operations:
CanFollow(StackAr.push(Object), StackAr.top())
CanFollow(StackAr.push(Object), StackAr.pop())
CanFollow(StackAr.push(Object),
StackAr.topAndPop())
This speciï¬cation improved the inferred invariants, removing four
incorrect preconditions (the same as for experiment #1, except for
â€˜this has only one valueâ€™) and adding three invariants:
One states that topOfStack cannot exceed the internal array size
(as in experiment #1).
One establishes an inequality between stack default capacity and
array size (as in experiment #1).
One establishes that the topOfStack is nonnegative.
Again the changes affected method â€˜ popâ€™, while â€˜ topâ€™ and
â€˜topAndPop â€™ remained unaffected, as they already had high test
coverage.
In summary, the impact of second-order constraints on StackAr ,
under the standard test suite was overwhelmingly positive. All
of the additional invariants were correct and most were insightful,
while spurious invariants were eliminated.
4.1.2 Apache Commons Collections
Our second case study is the Apache Commons Collections li-
brary,2version 3.2.1. This library contains 356 classes, of which
we used a total of 18 explicitly3for our experiments.
For our experiments, one of the authors (unfamiliar with the
library) examined the above classes and constructed a speciï¬ca-
tion (while consulting the API documentation) with a total of 27
second-order constraints, comprising 22 experiments. This pro-
cess took approximately 3.5h. The author spent another 2h double-
checking and ï¬xing the invariants after speciï¬cation. This amount
of effort is in the noise level, compared to the development effort
for a library of this size.
We then examined the utility of our speciï¬cations by running
Daikon ï¬rst with and then without our constraints. Since running
the entire test suite with Daikonâ€™s instrumentation tool would have
consumed harddisk space in excess of 10 GB (compressed), we in-
structed Daikonâ€™s instrumentation tool to only report invariants for
the methods we were interested in. We only ran these experiments
with DynComp disabled, as using the DynComp-instrumented JDK
caused errors during execution.
We recorded the invariants of all affected methods and analyzed
the generated differences. Figure 3 summarizes our results. We
ï¬rst list the affected class or classes, then the concrete second-
order constraints we introduced (slightly compressed for space).
2http://commons.apache.org/collections/
3Some classes may use other classes from the library internally.# Classes second-order constraints First-order invariants
pre postprepost
1 ArrayStack CanFollow(push, peek) 1 3 -1
2 ArrayStack Subrange(peek, get) 0 5 -2
3 ListUtils Subdomain(removeAll, retainAll) 12 0 -2
4 ListUtils Subdomain(sum, subtract) 0 0
5 CollectionUtils Subdomain(subtract, disjunction) 8 0 +2
6 CollectionUtils Subdomain(subtract, intersection) 14 0
7 TreeBidiMap Subdomain(containsKey, get) 101 0 +6-7
8 TreeBidiMap Subdomain(nextKey, previousKey) 108 0
9 Unmodiï¬ableSortedBidiMap Subrange(tailMap, subMap) 0 16
10 TreeBidiMapSubdomain(remove, get) ,99 0 +2-7Subdomain(get, remove)
11 TreeBidiMapSubdomain(getKey, removeValue) ,93 0 +2-7Subdomain(removeValue, getKey)
12 DualTreeBidiMap Subrange(tailMap, subMap) 0 22
13 UnboundedFifoBuffer Follows(add, remove) 11 34 +1-1
14 SetUniqueList Follows(addAll, remove) 3 8 -1
15 AbstractBidiMapDecorator Subrange(getKey, removeValue) 0 10 +3-1
16 ReverseListIteratorSubrange(previousIndex, nextIndex) ,0 20 +5-1Subrange(nextIndex, previousIndex)
17 CursorableLinkedList CanFollow(addNode, removeNode) 3 6
18 CursorableLinkedList CanFollow(addNode, updateNode) 5 6
19 LinkedMapSubdomain(get, remove) ,4 6 +1-1 +1-1Subrange(get, remove)
20 ListOrderedMap Subrange(put, remove) 0 16 -3
21 CompositeMap CanFollow(addComposited, removeComposited) 8 19 +6-3
22 CompositeCollection Concord(addComposited(Collection), addComposited(Collection, Collection)) 12 28 +2-2
Figure 3: Summary of our experiments on the Apache Commons Collections.
Next, we considered the invariants inferred by Daikon, restricted
to invariants of the particular methods occurring in our constraints.
For example, in experiment #1, we considered methods push and
peek in class ArrayStack only. The ï¬nal four columns summa-
rize these invariants: ï¬rst, we give the number of invariants in the
absence of any second-order annotations, separated into pre- and
postconditions ( preandpost). Finally we give the differences over
any preconditions ( pre) and postconditions we observed ( post).
We use the notation +x yto indicate that we added xnew invari-
ants and removed yexisting ones.
Qualitatively, the use of second-order constraints on the Apache
Commons Collections was a clear win. All 35 invariants re-
moved were false, to the best of our understanding. We
added 26 invariants, of which our manual inspection found 25
to be true (i.e., expected to hold for all executions, not just
the ones observed) and 1 to be false. The added invariants
arise due to non-monotonicity. In experiment 16, for exam-
ple, the augmented observations in the presence of second-
order constraints enable two additional true invariants (i.e., a
stronger inference) in the precondition of method nextIndex :
this.list != null and this.iterator != null . The
invariant â€œ orig(value) != null â€ was incorrectly added to
AbstractBidiMapDecorator.removeValue(Object) in ex-
periment 15, where parameter value does not have to be non-null.
Furthermore, we replaced 5 invariants with more general invari-
ants. Consider, for example, experiment 13, with second-order
constraint Follows(add,remove) . The postcondition of method
UnboundedFifoBuffer.add(Object) originally contained in-
variants such as â€œ this.head one of {0, 1, 2} â€. Such false
invariants are replaced with a more insightful â€œ this.head >= 0 â€.
4.1.3 AspectJ Compiler
Our third case study is the AspectJ compiler. We followed the
same approach as for the Apache Commons Collections, collectinginvariants from unit tests and integration tests. Since AspectJ lacks
detailed API documentation, one of the authors (unfamiliar with the
library) directly inspected the source code of AspectJ and derived
a total of 27 second-order constraints. The combined process of
understanding the foreign code base and writing invariants cost the
author approximately 10h. Again DynCompâ€™s instrumented JDK
caused problems during execution, so we only tested with Dyn-
Comp disabled.
We summarize our results in Figure 4. We removed 12 in-
variants. All of the 12 invariants were false. For instance, most
â€œvariable has only one valueâ€ and â€œvariable is one of f:::gâ€
invariants (largely due to limitations in the test suite) were re-
moved or replaced by more accurate invariants. We also added 1
false invariant in experiment 16 (again, due to non-monotonicity)
by assuming a variable is always less than a constant. Mean-
while, we added 12 true invariants and replaced 3 invariants
with more general ones. For instance, without the second-
order constraint Subrange(getAjType, getDeclaringType)
in experiment 1, Daikon reports no invariants for the exit
of the getDeclaringType method. Our Subrange con-
straint yields two new postconditions, â€œ return != null â€ and
â€œreturn.getClass() == AjTypeImpl.class â€, due to obser-
vations on getAjType .
4.2 Inferring Second-Order Constraints
We next evaluate the success of our dynamic process of inferring
second-order constraints. Note that dynamically inferred second-
order constraints are useful for many reasons:
As documentation of program behavior, on their own, i.e., as
deeper invariants than typical Daikon invariants.
For ï¬nding bugs in manually stated second-order constraints.
For offering the programmer a set of mostly-correct second-order
constraints to choose from.# Classes second-order constraints First-order invariants
pre postprepost
1 AjTypeSystem, AdviceImpl Subrange(getAjType, getDeclaringType) 0 9 +2
2 ProgramElement Subdomain(toSignatureString(boolean), toLabelString(boolean)) 76 0
3 ProgramElement Subdomain(toLabelString(boolean), toSignatureString(boolean)) 76 0
4 ProgramElement Subrange(toLabelString(), toLabelString(boolean)) 0 142
5 ProgramElement Subdomain(getParent, getChildren) 76 0
6 ProgramElement Subrange(genModiï¬ers(), getModiï¬ers(int)) 0 80
7 ProgramElement Subdomain(toLabelString(), toSignatureString()) 81 0 +1-5
8 ProgramElement Subdomain(toSignatureString(), toLabelString()) 81 0
9 ProgramElement Subdomain(getChildren, getParent) 76 0 -1
10 FieldSignatureImpl Subdomain(getDeclaringTypeName, getDeclaringType) 0 0
11 FieldSignatureImpl Subdomain(getDeclaringType, getDeclaringTypeName) 0 0
12 MethodSignatureImpl Subdomain(getField, getFieldType) 0 0
13 MethodSignatureImpl Subdomain(toShortString, toLongString) 0 0
14 MethodSignatureImpl Subdomain(getDeclaringType, getDeclaringTypeName) 0 0
15 SignatureImpl Subdomain(getDeclaringTypeName, getDeclaringType) 66 0
16 SignatureImpl Subdomain(getDeclaringType, getDeclaringTypeName) 66 0 +4-4
17 SignatureImpl Subdomain(toLongString, toShortString) 71 0 +3
18 SignatureImpl Subdomain(toShortString, toLongString) 71 0 +3-1
19 BcelWeaver CanFollow(prepareForWeave, weave(UnwovenClassFile, BcelObjectType)) 52 80 +3-4
20 BcelWeaver CanFollow(prepareForWeave, weave(UnwovenClassFile, BcelObjectType, bool.)) 41 80
21 BcelWeaver CanFollow(prepareForWeave, weaveAndNotify) 54 80
22 BcelWeaver CanFollow(prepareForWeave, weaveNormalTypeMungers) 53 80
23 BcelWeaver CanFollow(prepareForWeave, weaveParentTypeMungers) 53 80
24 BcelWeaver CanFollow(prepareForWeave, weave(IClassFileProvider)) 52 80
25 BcelWeaver CanFollow(prepareForWeave, weaveParentsFor) 57 80
26 BcelWeaver CanFollow(prepareForWeave, weaveWithoutDump) 51 80
Figure 4: Summary of our experiments on the AspectJ Compiler.
Although the ï¬rst beneï¬t is very important, it is hard to quantify
experimentally. Therefore, we focus on the second and third bene-
ï¬ts, and speciï¬cally on evaluating the correctness of automatically
inferred second-order constraints.
Correctness of Inferred Constraints :We inspected the results
of our automatic mechanism for inferring second-order constraints
(Section 3.2) on four randomly selected classes from ACC and As-
pectJ (two each). One of the authors manually veriï¬ed all of the
generated second-order constraints. For this experiment, we con-
sidered a second-order constraint to be correct whenever it did not
disagree with the implementation of the given class. Figure 5 lists
our precision results. We note that overall precision exceeds 99% ,
suggesting that our conï¬dence metric is highly effective at identi-
fying low-quality constraint candidates.
Our ï¬ve losses in LstBuildConfigManager were three Fol-
lows and two Subrange constraints involving methods with low-
quality invariants. For example, four of the ï¬ve constraints involved
anaddListener method for which Daikon had failed to observe
the methodâ€™s effect on the internal object state.
Although these constraints are true, they are not necessarily all
interesting. Many of them just reï¬‚ect implementation artifacts and
would not arise if the methods in question had interesting invariants
to begin with. For instance, for two methods that have a very small
number of invariants in their preconditions, it is easy to ï¬nd mean-
ingless agreementâ€”e.g., on the fact that their argument is never
null .
ForSingletonMap , we observed more than 800 proposed high-
conï¬dence constraints that claimed that most methods were in
some relationship to each other. We found that SingletonMap
is an immutable class, meaning that methods do not inï¬‚uence each
other on subsequent callsâ€”therefore many invariants remained the
same across methods, resulting in second-order constraints being
derived. (These include the two second-order constraints we man-
ually wrote for SingletonMap but also many others of muchless value.) This suggests the existence of other useful higher-
order constraints beyond the catalog we have proposed herein; for
SingletonMap , a meta-constraint Immutable would be the most
concise way to express the properties that we observed.
In summary, we found that dynamic second-order constraint
inference is highly effective at identifying high-quality sets of
second-order constraints, even though we allow some margin of
error over already-erroneous or imprecise axioms.
Inferred vs. Manual Constraints :Our manually derived
second-order constraints of the previous section were never in-
tended as a full or ideal set, but as a set of constraints that take
low effort to produce and have signiï¬cant effect over ï¬rst-order in-
variants. Still, it is instructive to compare them with automatically
inferred constraints. For this purpose, we ran our inference mecha-
nism on all classes that we had manually written second-order con-
straints for.
Indeed, our manual effort to produce constraints for ACC and
the AspectJ compiler originally yielded 64constraints and not just
the52shown in Figures 3 and 4. 12manually derived second-order
invariants were removed exactly because their absence from the set
of automatically inferred constraints caused us to re-inspect them
and discover they were erroneous! This pattern is likely to also
occur in practice when dealing with unfamiliar code (in fact, 8of
the12were in AspectJ, which lacks documentation). Note that in
a scenario in which software authors write their own second-order
constraints, such disagreements between hand-written and inferred
second-order constraints would point to more serious issues; likely
to poor test suites or to implementation bugs.
Of the 52constraints in Figures 3 and 4 our automatic infer-
ence mechanism produced 37and missed 15. On closer inspection
we found that 6of those missing hand-written second-order con-
straints are rejected (although true) due to low success rate. This
is caused by â€œnoiseâ€ invariants participating in the corresponding
preconditions and postconditions, due to very few actual observa-Second-Order Constraints #
Library Class methods # Program points # Daikon invariants # total correct incorrect
ACC AbstractMapBag 25 7 85 2 2 0
ACC SingletonMap 34 54 635 806 806 0
AspectJ Reï¬‚ection 17 10 192 30 30 0
AspectJ LstBuildConï¬gManager 18 23 778 112 107 5
Figure 5: Results of inferring second-order constraints. In the above, â€˜program pointsâ€™ lists only program points that our inference
considers, i.e., :::ENTER and:::EXIT program points.
tions for the relevant methods. 7more second-order constraints are
missing due to having no data samples at all for the methods. The
last2second-order constraints are missing since our current im-
plementation does not support detecting the Concord constraint or
constraints relating methods in two different classes.
Thus, overall the automatic inference facility produces quite
high-quality results on its own, and is found to be a strong com-
plement for manual derivation of second-order constraints.
5. RELATED WORK
There is a wealth of other work on invariant inference. We next
selectively focus on some recent approaches that were not covered
in the body of the paper (typically because they focus on static in-
variant inference techniques).
For reverse engineering, Gannod and Cheng [10] proposed to
infer detailed speciï¬cations statically by computing the strongest
postconditions. Nevertheless, pre/postconditions obtained from an-
alyzing the implementation are usually too detailed to understand
and too speciï¬c to support program evolution. Gannod and Cheng
[11] addressed this deï¬ciency by generalizing the inferred speci-
ï¬cation, for instance by deleting conjuncts, or adding disjuncts or
implications. Their approach requires loop bounds and invariants,
both of which must be added manually.
There has been some recent progress in inferring invariants us-
ing abstract interpretation. Logozzo [20, 21] infers loop invariants
while inferring class invariants. The limitation of his approach are
the available abstract domains; numerical domains are best studied.
Resulting speciï¬cations are expressed in terms of ï¬elds of classes.
Flanagan and Leino [7] propose a lightweight veriï¬cation-based
tool, named Houdini, to statically infer ESC/Java [8] annotations
from unannotated Java programs. Based on pre-set property pat-
terns, Houdini conjectures a large number of possible annotations
and then uses ESC/Java to verify or refute each of them. The ability
of this approach is limited by the patterns used. In fact, only sim-
ple patterns are feasible, otherwise too many candidate annotations
will be generated, and, consequently, it will take a long time for
ESC/Java to verify complicated properties.
Taghdiri [25] uses a counterexample-guided reï¬nement process
to infer over-approximate speciï¬cations for procedures called in the
function being veriï¬ed. In contrast to our approach, Taghdiri aims
to approximate the behaviors for the procedures within the callerâ€™s
context instead of inferring speciï¬cations of the procedure.
Henkel and Diwan [15] have built a tool to dynamically discover
algebraic speciï¬cations for interfaces of Java classes. Their speciï¬-
cations relate sequences of method invocations. The tool generates
many terms as test cases from the class signature. The results of
these tests are generalized to algebraic speciï¬cations. They use
a second tool to dynamically compare their speciï¬cations against
implementations by executing both simultaneously and comparing
their behavior [16].
Much of the work on speciï¬cation mining is targeted at infer-
ring API protocols dynamically. Whaley et al. [26] create a ï¬nitestate machine into which a transition from method A to method B
is added if the post-condition of method A is not mutually exclu-
sive with the pre-condition of method B. Meghani and Ernst [22]
build upon Whaleyâ€™s work by using Daikon to determine the likely
pre/post-condition of each method. Other approaches use data min-
ing techniques. For instance Ammons et al. [1] use a learner to in-
fer nondeterministic state machines from traces; similarly, Yang
and Evans [27] built Terracotta, a tool to generate regular pat-
terns of method invocations from observed runs of the program.
Li and Zhou [19] apply data mining in the source code to infer pro-
gramming rules, i.e., usage of related methods and variables, and
then detect potential bugs by locating the violation of these rules.
Gabel and Su [9] dynamically infer and verify method call order-
ing constraints, and report the constraints only if they are violated.
Beschastnikh et al. [2] use mined temporal invariants from logs to
derive a reï¬ned ï¬nite state machine. Although not explicitly our
goal, some of our second-order constraints can be thought of as
a way to express temporal API protocols. For example, we can
ï¬nd the general has , nexttype speciï¬cation [9] by checking if
CanFollow(has(return==true),next ), or that the postcondition of
haswhen hasreturns true implies the precondition of next . It is
interesting future work to see how to integrate existing techniques
on speciï¬cation mining with our approach to derive automata that
describe correct behavior.
6. CONCLUSIONS
Second-order constraints can steer dynamic invariant inference
to avoid erroneous invariants and to derive more relevant invariants
while reducing noise. We have deï¬ned a vocabulary of second-
order constraints and described how each of them encodes infor-
mation that is typically known by programmers and useful to a
dynamic invariant detector. We have taken an approach in which
the second-order constraints control the propagation of the observa-
tions on which invariant detection is based. We have also extended
the Daikon system so as to infer second-order constraints.
Overall, we consider second-order constraints to be a particularly
promising idea not just as a meaningful documentation concept but
also for improving the consistency and quality of dynamically in-
ferred invariantsâ€”a major challenge in this area.
7. ACKNOWLEDGMENTS
We gratefully acknowledge funding by the European Union un-
der a Marie Curie International Reintegration Grant and a European
Research Council Starting/Consolidator grant; by the Greek Sec-
retariat for Research and Technology under an Excellence (Aris-
teia) award; and by by the U.S. National Science Foundation under
grants CCF-0917391 and CCF-0916569.
Our tools are available at:
http://code.google.com/p/getmetainv/
http://code.google.com/p/usemetainv4daikon/ .8. REFERENCES
[1] Glenn Ammons, Rastislav BodÃ­k, and James R. Larus.
Mining speciï¬cations. In POPL â€™02: Proceedings of the 29th
ACM SIGPLAN-SIGACT symposium on Principles of
Programming Languages , pages 4â€“16, New York, NY , USA,
2002. ACM Press.
[2] Ivan Beschastnikh, Yuriy Brun, Sigurd Schneider, Michael
Sloan, and Michael D. Ernst. Leveraging existing
instrumentation to automatically infer invariant-constrained
models. In Proceedings of the 19th ACM SIGSOFT
symposium and the 13th European conference on
Foundations of software engineering , ESEC/FSE â€™11, pages
267â€“277, New York, NY , USA, 2011. ACM.
[3] David R. Cok and Joseph R. Kiniry. ESC/Java2: Uniting
ESC/Java and JML: Progress and issues in building and
using ESC/Java2. Technical Report NIII-R0413, Nijmegen
Institute for Computing and Information Science, May 2004.
[4] Christoph Csallner and Yannis Smaragdakis. Dynamically
discovering likely interface invariants. In Proc. International
Conference on Software Engineering (Emerging Results
Track) , pages 861â€“864, May 2006.
[5] Christoph Csallner, Nikolai Tillmann, and Yannis
Smaragdakis. DySy: Dynamic symbolic execution for
invariant inference. In International Conference on Software
Engineering (ICSE) , May 2008.
[6] Michael D. Ernst, Jake Cockrell, William G. Griswold, and
David Notkin. Dynamically discovering likely program
invariants to support program evolution. IEEE Transactions
on Software Engineering , 27(2):99â€“123, February 2001.
[7] Cormac Flanagan and K. Rustan M. Leino. Houdini, an
annotation assistant for ESC/Java. In JosÃ© Nuno Oliveira and
Pamela Zave, editors, FME 2001: Formal Methods for
Increasing Software Productivity: International Symposium
of Formal Methods Europe , pages 500â€“517. Springer, March
2001.
[8] Cormac Flanagan, K. Rustan M. Leino, Mark Lillibridge,
Greg Nelson, James B. Saxe, and Raymie Stata. Extended
static checking for Java. In Proc. ACM SIGPLAN 2002
Conference on Programming Language Design and
Implementation , pages 234â€“245, June 2002.
[9] Mark Gabel and Zhendong Su. Online inference and
enforcement of temporal properties. In Proceedings of the
32nd ACM/IEEE International Conference on Software
Engineering - Volume 1 , ICSE â€™10, pages 15â€“24, New York,
NY , USA, 2010. ACM.
[10] Gerald C. Gannod and Betty H. C. Cheng. Strongest
postcondition semantics as the formal basis for reverse
engineering. In Proc. Second Working Conference on
Reverse Engineering (WCRE) , pages 188â€“197. IEEE, July
1995.
[11] Gerald C. Gannod and Betty H. C. Cheng. A speciï¬cation
matching based approach to reverse engineering. In Proc.
International Conference on Software Engineering , pages
389â€“398. ACM, May 1999.
[12] Philip J. Guo, Jeff H. Perkins, Stephen McCamant, and
Michael D. Ernst. Dynamic inference of abstract types. In
Proceedings of the 2006 international symposium on
Software testing and analysis , ISSTA â€™06, pages 255â€“265,
New York, NY , USA, 2006. ACM.[13] Samuel Z. Guyer and Calvin Lin. An annotation language for
optimizing software libraries. In Second Conference on
Domain-Speciï¬c Languages (DSL) , pages 39â€“52, 1999.
[14] Sudheendra Hangal and Monica S. Lam. Tracking down
software bugs using automatic anomaly detection. In
Proceedings of the 24th International Conference on
Software Engineering , pages 291â€“301, May 2002.
[15] Johannes Henkel, Christoph Reichenbach, and Amer Diwan.
Discovering documentation for java container classes. IEEE
Trans. Software Eng. , 33(8):526â€“543, 2007.
[16] Johannes Henkel, Christoph Reichenbach, and Amer Diwan.
Developing and Debugging Algebraic Speciï¬cations for Java
classes. ACM Trans. Softw. Eng. Methodol. , 17(3), 2008.
[17] Yamini Kannan and Koushik Sen. Universal symbolic
execution and its application to likely data structure invariant
generation. In ISSTA â€™08: Proceedings of the 2008 Int.
Symposium on Software Testing and Analysis , pages
283â€“294, New York, NY , USA, 2008. ACM.
[18] Gary T. Leavens, Albert L. Baker, and Clyde Ruby.
Preliminary design of JML: A behavioral interface
speciï¬cation language for Java. Technical Report TR98-06y,
Department of Computer Science, Iowa State University,
June 1998.
[19] Zhenmin Li and Yuanyuan Zhou. PR-miner: automatically
extracting implicit programming rules and detecting
violations in large software code. In Proc. 13th International
Symposium on Foundations of Software Engineering (FSE) ,
pages 306â€“315. ACM, September 2005.
[20] Francesco Logozzo. Automatic inference of class invariants.
InVeriï¬cation, Model Checking, and Abstract Interpretation
(VMCAI) , number 2937 in LNCS. Springer-Verlag, January
2004.
[21] Francesco Logozzo. Modular Static Analysis of
Object-Oriented Languages . PhD thesis, Ecole
Polytechnique, June 2004.
[22] Samir V . Meghani and Michael D. Ernst. Determining legal
method call sequences in object interfaces, May 2003.
[23] Bertrand Meyer. Object-Oriented Software Construction .
Prentice Hall PTR, 2nd edition, 1997.
[24] Jeff H. Perkins and Michael D. Ernst. Efï¬cient incremental
algorithms for dynamic detection of likely invariants. In
Proceedings of the ACM SIGSOFT 12th Symposium on the
Foundations of Software Engineering (FSE 2004) , pages
23â€“32, November 2004.
[25] Mana Taghdiri. Inferring speciï¬cations to detect errors in
code. In Proc. 19th IEEE International Conference on
Automated Software Engineering (ASE) , pages 144â€“153,
September 2004.
[26] John Whaley, Michael C. Martin, and Monica S. Lam.
Automatic extraction of object-oriented component
interfaces. In Proc. International Symposium on Software
Testing and Analysis (ISSTA) , pages 218â€“228. ACM, July
2002.
[27] Jinlin Yang and David Evans. Dynamically inferring
temporal properties. In Proc. 5th ACM SIGPLAN-SIGSOFT
Workshop on Program Analysis for Software Tools and
Engineering (PASTE) , pages 23â€“28. ACM, June 2004.