Variability-Aware Performance Prediction:
A Statistical Learning Approach
Jianmei Guo, Krzysztof Czarnecki, Sven Apely, Norbert Siegmundy, and Andrzej W Ë› asowskiz
University of Waterloo, Canada
yUniversity of Passau, Germany
zIT University of Copenhagen, Denmark
Abstract â€”Conï¬gurable software systems allow stakeholders to
derive program variants by selecting features. Understanding
the correlation between feature selections and performance is
important for stakeholders to be able to derive a program variant
that meets their requirements. A major challenge in practice is
to accurately predict performance based on a small sample of
measured variants, especially when features interact. We propose
a variability-aware approach to performance prediction via sta-
tistical learning. The approach works progressively with random
samples, without additional effort to detect feature interactions.
Empirical results on six real-world case studies demonstrate an
average of 94 % prediction accuracy based on small random
samples. Furthermore, we investigate why the approach works
by a comparative analysis of performance distributions. Finally,
we compare our approach to an existing technique and guide
users to choose one or the other in practice.
I. I NTRODUCTION
Many software systems provide conï¬guration options for
users to tailor their functional behavior as well as non-
functional properties (e.g., performance, cost, and energy
consumption). Conï¬guration options relevant to users are often
called features [2], [3], [7], [8], [13], [16]. Each variant de-
rived from a conï¬gurable software system can be represented
as a selection of features, called a conï¬guration .
Performance (e.g., response time or throughput) is one
of the most important non-functional properties, because it
directly affects user perception and cost [25]. Finding an
optimal conï¬guration to meet a speciï¬c performance goal is
a fundamental task for developers and system administrators.
Performance of a software system is often subject to a wide
variety of inï¬‚uencing factors [24]. Understanding trade-offs
between inï¬‚uencing factors and performance is non-trivial. In
this paper, we focus on how to determine the inï¬‚uence of
feature selections on performance. Considering a conï¬gurable
software system as a black box, we investigate and exploit
the correlation between feature selections and performance for
performance prediction.
A straightforward approach to reveal such correlation is
to measure the performance of all conï¬gurations of a soft-
ware system and then provide direct answers (e.g., which
conï¬guration is the fastest). However, such a brute-force
approach is usually infeasible, because even a small-scale
conï¬gurable system can give rise to an exponential number
of conï¬gurations, due to feature combinatorics, and the cost
of measurement may be high (e.g., executing a complexbenchmark). Therefore, in practice, often only a limited set
of conï¬gurations can be measured, either by simulation or by
monitoring in the ï¬eld [24]. We denote these conï¬gurations
along with their performance measurements as a sample ,
and all conï¬gurations of a software system along with their
performance as the whole population . The challenge is how to
use a small (e.g., linear in the number of features) sample to
predict the performance of other conï¬gurations in the whole
population, with a high accuracy (e.g., above 90 % ).
Quantifying the performance inï¬‚uence of each individual
feature is not sufï¬cient in most cases, as feature interactions
may cause unpredictable performance anomalies [19]. That is,
the performance inï¬‚uence of two features, both appearing in
a conï¬guration, may not be easily deducible from the perfor-
mance inï¬‚uence of each feature without the other. Siegmund
et al. [19] addressed this issue by introducing a measurement-
based prediction approach, called SPLC ONQUEROR , which
detects performance-relevant feature interactions using spe-
ciï¬c sampling heuristics that meet different feature-coverage
criteria. However, in practice, the conï¬gurations that we can
measure or that we already have at our disposal may not meet
any feature-coverage criterion. Thus, we pose the following
research question: Is it feasible to use small random samples
as a basis for accurate performance prediction?
To answer this question, we formalize the problem of
variability-aware performance prediction and reduce it to a
non-linear regression problem. We use a statistical learning
technique, Classiï¬cation And Regression Trees (CART) [5],
to address the problem and to model the correlation between
feature selections and performance.
Compared to existing methods [19], [20], [22]â€“[25], our
approach works automatically andprogressively with random
samples, such that one can use it to produce predictions,
starting with a small random sample, and subsequently extend
it when further measurements are available; it considers all
features of a system and identiï¬es the performance-relevant
ones; it treats the selected and deselected features in a con-
ï¬guration equally , to describe the correlation between feature
selections and performance; and it can be easily implemented
and deployed in practice, without additional effort to detect
feature interactionsâ€”an inherently challenging task [19].
In summary, we make the following contributions:
We propose a progressive and variability-aware approach
that predicts a conï¬gurationâ€™s performance based on978-1-4799-0214-9/13/$31.00 c2013 IEEE ASE 2013, Palo Alto, USA301random samples. The approach builds an explicit perfor-
mance model to specify the correlation between feature
selections and performance, to be used for performance
prediction.
We implement the approach and demonstrate its practi-
cality and generality by experiments on six real-world
conï¬gurable software systems. The results show that the
approach produces an average prediction accuracy of
94 % , based on only small random samples. Moreover,
we observe a desirable increasing trend of prediction
accuracy when the sample size increases.
We conduct a comparative analysis of performance dis-
tributions on the evaluated case studies and empirically
explore why the approach works with small random
samples.1A key ï¬nding is that it works well when the
sample it uses has a performance distribution similar to
the whole population.
We compare our method with an existing technique
that relies on heuristics and feature coverage [19]. In
particular, we discuss the strengths and weaknesses of
the two approaches and guide users to choose one or the
other in practice.
The implementation of the approach and all experimental
data are available at http://cpm.googlecode.com .
II. M OTIVATING EXAMPLE
We use the conï¬gurable tool X264 as an example to
motivate our approach. X264 is a command-line tool to
encode video streams into the H.264/MPEG-4 A VC format.2
In this example, we consider 16encoder features of X264,
such as parallel encoding on multiple processors or encoding
with multiple reference frames. Users can conï¬gure X264 by
selecting different features to encode a video. We use the
encoding time to indicate the performance of X264 in different
conï¬gurations. Even such a simple case with only 16features
gives rise to 1;152conï¬gurations. Given that we measure the
performance of a limited set of conï¬gurations as a sample, how
can we determine the performance of other conï¬gurations?
To address this issue, previous work on SPLC ONQUEROR
[19] focuses on selecting a speciï¬c sample to detect
performance-relevant feature interactions. That is, following
a certain feature-coverage criterion, SPLC ONQUEROR selects
a ï¬xed set of speciï¬c conï¬gurations and then measures
their performance, which is then the input for predicting
the performance of other conï¬gurations. Two fundamental
feature-coverage criteria are feature-wise andpair-wise . The
feature-wise measurement quantiï¬es an individual featureâ€™s
performance inï¬‚uence by calculating the performance delta of
two minimal conï¬gurations with and without the feature. The
pair-wise heuristic selects and measures additionally a speciï¬c
set of conï¬gurations to detect all pair-wise feature interactions.
SPLC ONQUEROR also provides heuristics for the detection of
higher-order feature interactions.
1A performance distribution denotes the frequency distribution of all
performance values in a sample or in the whole population.
2http://www.videolan.org/developers/x264.htmlAn important point is that, in practice, the conï¬gurations
that we can measure or that we already have are often arbitrari-
ly selected; they may not meet any feature-coverage criterion.
Moreover, the number of available conï¬gurations may vary
and is usually very limited due to the high cost of performance
measurement. For example, Table I lists a sample of 16
randomly-selected conï¬gurations of X264 and corresponding
performance measurements. The question is, can we predict
performance of all other conï¬gurations accurately with such
a limited number of measurements? Next, we formally reduce
this question to a non-linear regression problem, and then we
present our approach to address the problem.
III. P ROBLEM FORMALIZATION AND REDUCTION
In this section, we formalize the problem of variability-
aware performance prediction and reduce it to a non-linear
regression problem.
We represent all features of a conï¬gurable software system
as a set Xof binary decision variables. If a feature is selected
in a conï¬guration, then the corresponding variable xis equal
to1, and 0otherwise. We denote the number of all features in
a system as N, i.e., X=fx1; x2; :::; x Ng. Then, we represent
each conï¬guration of a system as an N-tuple, assigning value
1or0to each variable in X. For example, X264 has 16
features in total, as listed in Columns x1tox16in Table I;
thus, each conï¬guration of X264 is represented by a 16-tuple,
e.g.,x1= (x1= 1; x2= 1; x3= 0; :::; x 16= 1) . We denote
all valid conï¬gurations in a system as set X.
Each conï¬guration xof a system has an actual perfor-
mance value y. We indicate the actual performance of all
conï¬gurations of a system as Y. Suppose that we acquire
a set of conï¬gurations XSXand measure their actual
performance YS, together forming sample S. For example,
Table I lists a sample of 16randomly-selected conï¬gurations
ofX264 (Rows x1tox16) and their performance values
(the rightmost column). Thus, the problem of variability-aware
performance prediction is how to predict the performance of
other unmeasured conï¬gurations in XnXSbased on the
measured sample S.
Since we focus on the inï¬‚uence of feature selections on
performance, we consider all variables in Xaspredictors and
a conï¬gurationâ€™s actual performance value yas the response .
In essence, we try to ï¬nd a function to relate the tuple xof
predictors to the quantitative response y, which is a typical
regression problem [11]. Given a sample S, the problem is to
ï¬nd a function fthat reveals the correlation between XSand
YSand that makes each conï¬gurationâ€™s predicted performance
f(x)as close as possible to its actual performance y, i.e.:
f:X!Rsuch thatX
x;y2SL(y; f(x))is minimal (1)
where Lis a loss function to penalize errors in prediction.
An assumption of our approach is that the sample Sand
the whole population exhibit the same or similar correlation
between feature selections and performance. Thus, we can use302TABLE I
ASAMPLE OF 16RANDOMLY -SELECTED CONFIGURATIONS OF X 264 AND CORRESPONDING PERFORMANCE MEASUREMENTS (SECONDS )
Conf. Features Perf. (s)
xix1x2x3x4x5x6x7x8x9x10x11x12x13x14x15x16 yi
x1 1 1 0 1 1 1 1 0 1 0 0 1 1 0 0 1 651
x2 1 1 1 1 1 1 0 1 1 1 0 0 1 0 1 0 536
x3 1 1 1 1 0 0 0 0 1 1 0 0 1 0 0 1 581
x4 1 0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 381
x5 1 1 0 1 0 0 0 1 1 1 0 0 1 0 1 0 424
x6 1 1 0 0 1 0 1 1 1 1 0 0 1 0 0 1 615
x7 1 0 1 0 1 1 1 0 1 1 0 0 1 0 1 0 477
x8 1 0 1 0 0 0 0 1 1 0 0 1 1 1 0 0 263
x9 1 0 0 0 0 0 1 1 1 0 0 1 1 1 0 0 272
x10 1 1 1 1 0 0 0 1 1 0 0 1 1 1 0 0 247
x11 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 612
x12 1 0 1 1 1 0 0 0 1 0 0 1 1 0 1 0 510
x13 1 1 1 1 0 1 1 0 1 0 1 0 1 0 0 1 555
x14 1 1 0 0 1 0 1 1 1 0 0 1 1 1 0 0 264
x15 1 0 1 0 0 1 1 1 1 0 0 1 1 0 0 1 576
x16 1 0 1 0 1 0 1 1 1 0 1 0 1 1 0 0 268
the function f, built on the sample S, to predict performance
of other conï¬gurations in XnXS.
Note that we do not presume which features are actually
relevant to performance, but we consider all features of a
system. Moreover, we cannot linearly deduce the performance
of a conï¬guration from the performance inï¬‚uence of each
individual feature in separation due to feature interactions [19].
Therefore, the problem of variability-aware performance pre-
diction is a non-linear regression problem, i.e., the prediction
function depends non-linearly on one or more predictors [11].
IV. V ARIABILITY -AWARE PERFORMANCE PREDICTION
This section presents our progressive and variability-aware
approach to performance prediction via statistical learning.
A. Overview of the Approach
Figure 1 illustrates our approach, which consists of two
iterative processes. The ï¬rst process predicts performance
based on a set of rules. After conï¬guring a system Aand
deriving a new, previously unmeasured conï¬guration x, users
want to know the performance yif system Auses the con-
ï¬guration x. Our approach returns the quantitative prediction
(i.e., f(x)) after retrieving the corresponding decision rule
for conï¬guration x. Each decision rule speciï¬es the predicted
performance value of a conï¬guration when the conï¬guration
has the same feature selections (i.e., selected and deselected
features) as the rule deï¬nes.
The second process includes performance modeling and val-
idation. As shown in the dotted box in Figure 1, performance
modeling starts with a random sample. We use the sample to
build a performance model automatically by statistical learn-
ing. From the performance model, we derive a set of decision
rules to enable fast, direct question answering for performance
prediction. To validate the current performance model, users
can measure the actual performance of conï¬guration xand
then compare its actual performance measurement with its
A new
conï¬guration xsystem AConï¬gure
Decision rules
Performance
modelRandom sampleif system Auses conï¬guration x?What is performance y
& validationMeasurement
Learning DerivingpredictionQuantitative
Figure 1. Overview of the approach
performance prediction. Next, conï¬guration xand its actual
performance measurement can be reused to expand the sample
and then to rebuild the performance model. Thus, the approach
works in a progressive way and improves performance predic-
tions based on updated samples.
B. CART-Based Performance Modeling
In this section, we explain the process of variability-aware
performance modeling via statistical learning in detail, as
illustrated in the dotted box in Figure 1. As explained in
Section III, the problem is to ï¬nd a function fthat predicts the
performance value yfor a conï¬guration xbased on a sample
S. We use CART [5], [11] to address this problem. The basic
idea is as follows. We recursively partition the sample into
smaller segments until we can ï¬t a simple local prediction
model into each segment; and ï¬nally we organize all the local
models into a global prediction model, which is represented
as a binary decision tree.
Figure 2 shows a performance model generated by CART
based on the X264 sample in Table I. CART starts with the
sample Sthat contains 16conï¬gurations x1;x2; :::;x16and
their performance measurements y1; y2; :::; y 16. Then, CART
partitions the sample Sinto two segments SLandSRby303{x1,x2, ...,x16}S
SL
x7= 1?
{x8,x10}SLL
â„“SLL= 255{x9,x14,x16}SLR
â„“SLR= 268no yes
SRL
x3= 1?
{x4,x5}SRLL
â„“SRLL= 402{x2,x7,x12}SRLR
â„“SRLR= 508no yesSRR
x3= 1?
{x3,x13,x15}SRRL
â„“SRRL= 571{x1,x6,x11}SRRR
â„“SRRR = 626yes noSR
x15= 1?
yes nox14= 1?
yesnoFigure 2. Example performance model of X264 generated by CART based on the random sample of Table I
exhaustively searching over all feature-selection variables in X
for the best split that minimizes the total prediction errors in its
two resulting segments. For example, as shown in Figure 2, the
ï¬rst best split for the X264 sample Sis the feature-selection
variable x14, because choosing x14to partition Sproduces the
minimal total prediction errors in the two resulting segments
SLandSR. After partitioning, conï¬gurations with x14= 1go
to the left segment SL, and conï¬gurations with x14= 0go to
the right segment SR. Each segment is partitioned recursively
by further splits, such as the variables x7,x15, and x3.
For each segment Si, we use the sample mean of the actual
performance measurements as the local prediction model of
the segment to make prediction fast [4]:
`Si=1
jSijX
yj2Siyj (2)
The local model of each segment identiï¬es the common
feature selections (the corresponding branch from the ï¬rst
split to the current split) and the average performance of
the conï¬gurations contained in the segment. For example,
the local model of the leftmost leaf in Figure 2 indicates
the common feature selections (x14= 1; x7= 0) and the
average performance `SLL=1
2(y8+y10) = 255 s for the two
conï¬gurations x8;x10.
To penalize the prediction errors in each segment Sithat
uses the corresponding local model `Si, we adopt the most
common and convenient loss function, the sum of squared
error loss [11]:
X
yj2SiL(yj; `Si) =X
yj2Si(yj `Si)2(3)Thus, the best split for each segment Siis determined to
partition Siinto two segments SiLandSiRsuch that:
X
yj2SiLL(yj; `SiL) +X
yj2SiRL(yj; `SiR)is minimal (4)
To prevent underï¬tting the input sample, we may expect
that each ï¬nal segment (i.e., leaf) is small enough to produce
as small prediction errors as possible; but excessive partition-
ing may give rise to overï¬tting the input sample and thus
compromise prediction accuracy for other conï¬gurations [4],
[11].3Hence, determining when is the best time to stop the
recursive partitioning process is an empirical activity to trade-
off underï¬tting and overï¬tting, and it often involves a manual,
iterative process of parameter tuning [4], [24]. For our case
studies, we use two important parameters and deï¬ne a set
of empirically-determined parameter settings to automatically
control the termination of the recursive partition process, as
we explain in Section V.
Suppose that there are qleaves in the tree structure of a
performance model; we organize all the local models of these
leaves into a global model as follows:
f(x) =qX
i=1`SiI(x2Si) (5)
where I(x2Si)is an indicator function to denote if
conï¬guration xbelongs to a leaf Si. To determine to which
leaf a conï¬guration xbelongs, we match the feature selections
of a conï¬guration with the corresponding branch in the tree,
from the ï¬rst split to a leaf. For example, in the tree shown
in Figure 2, if a conï¬guration xsatisï¬es (x14= 1; x7= 0) ,
3If an algorithm works poorly even with the existing data, then the algorithm
underï¬ts the existing data. If an algorithm works well with the existing data,
but not with new data, then the algorithm overï¬ts the existing data.304which is consistent with the feature selections of the leftmost
branch, then this conï¬guration falls into the leftmost leaf SLL.
The global model for the tree shown in Figure 2 is speciï¬ed
as follows:
f(x) = 255I(x14= 1; x7= 0)
+ 268I(x14= 1; x7= 1)
+ 402I(x14= 0; x15= 1; x3= 0)
+ 508I(x14= 0; x15= 1; x3= 1)
+ 571I(x14= 0; x15= 0; x3= 1)
+ 626I(x14= 0; x15= 0; x3= 0)
We can derive a set of decision rules from a global perfor-
mance model to provide direct performance predictions for
users. Each branch in the tree structure of a performance
model indicates a decision rule. For example, we can derive
the following if-then decision rule from the leftmost branch
of the tree shown in Figure 2: if a conï¬guration satisï¬es
(x14= 1; x7= 0) , then its predicted performance is 255s.
V. I MPLEMENTATION
We implemented our approach using R 2.15.1 and J AVA
(ECLIPSE 4.2 with JVM 1.7). R is a language and environ-
ment for statistical computing and graphics.4We used the R
packages R ATTLE and RPART to implement CART and to
generate the performance models [26]. We developed a rule
generator to parse the built performance models and generate
decision rules. We also experimented with two CART variants,
Random Forests andBoosting [26], which try to enhance the
prediction effects of CART, but we observed similar prediction
improvements on our evaluated case studies through parameter
tuning. Thus, we choose a simple solution that uses only
CART for our case studies.
As mentioned in Section IV-B, we use two important
parameters to control the recursive partitioning process of
CART: minbucket is the minimum sample size for any leaf
of the tree structure of a performance model; and minsplit is
the minimum sample size for any segment in the tree before
the segment is considered for further partitioning. A segment
is not considered for partitioning if its sample size is less than
minsplit . We performed a set of preliminary experimental
tests to identify parameter settings that trade-off underï¬tting
and overï¬tting for our case studies. Moreover, to implement a
fully-automated process of performance modeling by CART,
we aim at setting the two parameters automatically in terms of
the size of the input sample, i.e., jSj. Since the size of most of
the samples used in our case studies is less than 100, we set
the threshold of 100to distinguish random samples of different
sizes. Finally, we use the following empirically-determined pa-
rameter settings to achieve automated performance modeling
and reasonable prediction accuracy for our case studies: if
jSj 100, then minbucket =bjSj
10+1
2candminsplit =
2minbucket ; ifjSj>100, then minsplit =bjSj
10+1
2cand
minbucket =bminsplit
2c; the minimum of minbucket is2;
and the minimum of minsplit is4.5
4http://www.r-project.org/
5bcindicates rounding down, i.e., bxc=maxfn2Zjnxg.VI. E VALUATION
We conducted a series of case studies to evaluate our ap-
proach. We aim at answering the following research questions:
RQ 1: How accurate is the approach of variability-aware
performance prediction? (Section VI-C)
RQ 2: Can the prediction process be progressive? (Sec-
tion VI-C)
RQ 3: How fast is the prediction process? (Section VI-D)
RQ 4: Is it possible to make accurate predictions using only
small random samples? (Section VI-E)
RQ 5: What are the strengths and weaknesses of our
approach compared to existing techniques? (Sec-
tion VI-G)
A. Subject Systems
We performed our case studies on a publicly-available
dataset, deployed with the SPLC ONQUEROR tool.6The
dataset covers a reasonable spectrum of practical applica-
tion scenarios. As shown in Table II, there are six existing
real-world conï¬gurable systems with different characteristics:
different sizes ( 42thousand to 300 thousand lines of code,
192 to millions of conï¬gurations), different implementation
languages (C, C++, and J AVA), and different conï¬guration
mechanisms (conditional compilation, conï¬guration ï¬les, and
command-line options). Moreover, the dataset contains the
whole population of each system, i.e., all conï¬gurations of
each system and their performance measurements (the ex-
ception is SQL ITE, for which the dataset contains 4;553
conï¬gurations for prediction modeling and 100 additional
random conï¬gurations for prediction evaluation [19]). For each
system, the performance has been measured using a standard
benchmark, either delivered by its vendor (e.g., O RACLE â€™s
standard benchmark for B ERKELEY DB) or used widely in
its application domain (e.g., AUTOBENCH and HTTPERF for
the A PACHE Web Server).
B. Experimental Setup
In our experiments, the independent variables are the sub-
ject system and the size of the input sample. The prediction
fault rate and the time cost of building a performance model
are measured as the dependent variables . The prediction fault
rate is the relative difference between the actual performance
and the predicted performance, i.e., FR=jactual predictedj
actual.
Correspondingly, the prediction accuracy is 1 FR.
To reduce the ï¬‚uctuations of the dependent variables caused
by random generation, we performed ï¬ve repetitions for each
combination of the independent variables. That is, for each
subject system, we repeated ï¬ve times generating a random
sample of a certain size and subsequently measured the depen-
dent variables after applying our approach to the sample. We
took only the average of these measurements for analysis. We
performed all measurements on the same Windows 7 machine
with Intel Core i5 CPU 2.5 GHz and 8 GB RAM.
6The dataset is available at http://fosd.de/SPLConqueror.305TABLE II
OVERVIEW OF THE SIX SUBJECT SYSTEMS ; LANG .â€”L ANGUAGE ;
LOCâ€”L INES OF CODE ;jXjâ€”N UMBER OF ALL VALID CONFIGURATIONS ;
Nâ€”N UMBER OF ALL FEATURES ;Mâ€”N UMBER OF CONFIGURATIONS
REQUIRED BY THE PAIR -WISE HEURISTIC OF SPLC ONQUEROR
System Domain Lang. LOC jXjN M
1 A PACHE Web Server C 230,277 192 9 29
2 LLVM Compiler C++ 47,549 1,024 11 62
3 X264 Encoder C 45,743 1,152 16 81
4 B ERKELEY DB Database C 219,811 2,560 18 139
5 B ERKELEY DB Database J AVA 42,596 400 26 48
6 SQL ITE Database C 312,625 3,932,160 39 566
For each subject system, we randomly selected a certain
number of conï¬gurations from the whole population as the
training sample for prediction modeling and all remaining
as the test sample for prediction evaluation. Take the X264
system as an example, if we select 16conï¬gurations as the
training sample, then the remaining 1;136conï¬gurations form
the test sample.
To assess the effectiveness of our approach working with
random samples of different sizes, we use four sizes for the
training sample of each subject system: N,2N,3N, and M,
where Nis the number of all features of each system, and
Mis the number of all speciï¬c conï¬gurations required by the
pair-wise heuristic of SPLC ONQUEROR . We list the concrete
values of NandMfor each system in the rightmost two
columns in Table II. We choose size N,2N, and 3N, because
measuring a sample whose size is linear in the number of all
features is likely feasible and reasonable in practice, given
the high cost of performance measurement. For example, the
number of even one percent of all conï¬gurations of X264 (i.e.,
115) is still much more than the triple fold of the number of all
features (i.e., 48). We choose size M, so that we can compare
our approach to SPLC ONQUEROR .
C. Experiment on Prediction Fault Rate
CART has been proved effective for many non-linear re-
gression problems [4], [5]. Moreover, most statistical learning
techniques can make more accurate predictions when more da-
ta are available [11]. Hence, the hypotheses of this experiment
are as follows.
1) Hypotheses: Our CART-based approach is effective for
variability-aware performance prediction (for RQ 1). Further-
more, it works progressively and improves the prediction
accuracy when a larger sample is available (for RQ 2).
2) Results: We measured the prediction fault rate for the
six systems listed in Table II and the four sample sizes ( N,
2N,3N, and M). We present the experimental results using
different statistical measures. Figure 3a shows the boxplots
of the results excluding outliers , such that other statistical
measures such as the median and quartiles can be shown
clearly.7Figure 3b includes all outliers. Table III (Column
7A boxplot represents statistical data on a plot, in which a rectangle is
drawn to represent the second and third quartiles, usually with a vertical line
inside to indicate the median value. The lower and upper quartiles are shown
as horizontal lines either side of the rectangle. An outlier is one that appears
to deviate markedly from other members of the sample in which it occurs.
4 4 4 40.0 0.2 0.4 0.6 0.8 
5 5 5 50.0 0.2 0.4 0.6 0.8 
6 6 6 60.0 0.2 0.4 0.6 0.8 
1 1 1 10.0 0.2 0.4 0.6 0.8 
2 2 2 20.0 0.2 0.4 0.6 0.8 
3 3 3 30.0 0.2 0.4 0.6 0.8 Fault Rate (x100%) 
N 2N 3N M(a) Excluding outliers
â—
â—â—â—â—â—
â—â—
â—â—â—â—
â—
â—â—
â—â—â—â—â—â—â—â—
â—â—â—â—â—
â—â—â—
â—â—
â—
â—â—
â—â—â—
â—â—â—
â—â—â—â—
â—â—
â—â—â—â—â—
â—â—â—â—â—
â—â—â—â—â—
â—â—
â—â—â—â—â—
â—â—â—â—
â—â—â—â—
â—
â—â—â—â—
â—â—â—â—â—
â—â—â—
â—â—â—
â—â—â—
â—â—
â—â—
â—â—
â—
â—â—
â—â—â—
â—â—
â—â—â—
â—â—â—â—
â—
â—â—
â—â—â—
â—
â—â—â—â—â—
â—â—â—â—â—â—
â—â—â—
â—â—â—
â—â—â—â—
â—â—â—
â—â—
â—â—
â—â—
â—â—
â—â—
â—â— â—
â—â—â—â—
â—â—â—â—â—â—â—â—
â—
â—â—â—â—
â—â—â—â—â—â—
â—â—â—â—
â—â—
â—â—â—â—â—
â—â—
â—â—â—
â—â—â—â—
â—â—â—â—â—â—â—
â—â—â—â—â—â—
â—â—
â—â—â—
â—
â—â—â—
â—â—â—â—â—â—â—â—â—
â—â—â—â—
â—â—
â—â—â—
â—â—
â—â—
â—â—â—
â—â—â—
â—â—
â—â—â—â—
â—â—
â—â—
â—â—â—
â—â—
â—â—â—
â—â—â—
â—
â—â— â—â—
â—â—â—
â—â—â—â—
â—â—â—â—
â—â—
â—
â—â—
â—â—â—â—
â—â—â—â—â—
â—â—
â—â—â—â—
â—â—â—â—â—â—
â—â—â—
â—â—â—â—
â—
â—â—â—â—â—â—
â—â—
â—â—
â—â—â—â—â—
â—â—â—
â—â—â—
â—â—â—â—
â—â—â—â—
â—â—â—â—â—
â—
â—â—â—â—
â—â—â—
â—â—â—
â—â—â—
â—â—
â—â—â—â—â—â—â—â—â—â—â—â—
â—â—â—â—â—
â—â—
â—â—â—â—
â—â—â—â—â—
â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—
â—â—â—â—â—
â—â—â—â—â—â—â—
â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—
â—â—â—â—â—â—â—â—
â—â—
â—â—â—
â—â—â—
â—â—â—â—â—
â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—
â—â—â—
â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—
â—â—â—
â—â—
â—â—
â—â—â—â—â—â—â—â—â—â—â—â—
â—â—â—â—
â—â—
â—
â—
â—â—
â—â—â—â—
â—
â—â—
â—â—â—
â—â—
â—â—â—
â—â—
â—â—â—
â—â—
â—â—
â—
â—â—
â—â—
â—
â—
â—â—â—â—
â—â—â—â—â—
â—â—
â—â—
â—â—
â—â—
â—â—â—
â—â—â—
â—
â—â—
â—â—â—â—
â—â—â—
â—
â—â—â—
â—
â—â—
â—
â—
â—â—â—â—
â—
â—â—
â—â—â—â—â—â—
â—
â—â—â—
â—â—â—
â—
â—â—â—â—â—
â—
â—
â—â—â—
â—â—
â—â—
â—â—
â—â—â—â—â—â—â—
â—â—
â—â—
â—
â—â—â—
â—â—
â—
â—â—â—
â—â—
â—â—â—
â—
â—â—
â—
â—â—â—
â—â—â—â—
â—
â—
â—â—
â—â—
â—â—
â—â—â—â—
â—
â—â—â—
â—â—â—
â—
â—â—
â—â—
â—â—
â—â—
â—â—
â—
â—â—
â—
â—â—â—
â—
â—â—â—
â—â—â—
â—
â—â—
â—â—â—
â—â—â—
â—â—
â—
â—
â—â—
â—â—
â—
â—â—
â—â—â—
â—â—
â—â—
â—â—â—â—
â—â—â—â—â—
â—
â—â—
â—â—â—â—
â—â—
â—â—
â—â—â—
â—â—
â—
â—â—
â—â—â—
â—â—â—â—
â—â—
â—
â—
â—â—â—
â—
â—â—â—â—
â—â—
â—â—â—
â—â—
â—â—â—
â—â—â—
â—
â—
â—â—â—â—â—
â—â—
â—â—â—
â—â—
â—â—
â—
â—â—â—
â—â—â—
â—
â—â—
â—â—â—
â—â—â—
â—
â—â—â—
â—
â—
â—â—â—
â—â—
â—â—
â—â—
â—â—
â—â—â—
â—â—
â—â—
â—â—â—
â—
â—â—â—â—
â—â—â—
â—â—â—â—â—
â—â—â—â—
â—â—
â—â—
â—â—
â—
â—â—â—â—â—
â—
â—â—
â—â—â—
â—â—â—â—â—â—
â—â—â—
â—â—â—
â—â—â—
â—â—â—â—
â—â—
â—â—â—â—â—
â—
â—â—
â—â—
â—â—â—â—
â—â—
â—â—
â—â—â—
â—â—â—
â—â—â—
â—â—â—â—
â—â—
â—â—â—â—â—â—â—â—
â—â—â—â—â—
â—â—
â—â—
â—â—â—â—â—
â—â—â—â—â—â—â—â—
â—â—
â—â—
â—â—
â—â—â—
â—â—â—
â—â—â—
â—â—â—â—â—â—â—â—â—
â—â—â—â—â—
â—â—â—
â—â—
â—â—â—
â—
â—â—â—â—â—â—
â—â—â—
â—â—â—
â—â—
â—
â—â—â—
â—â—
â—â—
â—â—â—â—
â—â—â—â—â—â—â—â—â—
â—â—â—
â—â—â—
â—â—
â—â—
â—â—â—â—
â—â—â—
â—â—â—
â—â—
â—â—
â—â—
â—â—â—â—â—
â—â—â—â—
â—â—
â—â—
â—â—â—â—
â—â—â—
â—â—â—
â—â—
â—â—â—â—â—â—
â—
â—â—â—â—â—â—
â—
â—â—â—â—â—
â—
â—â—
â—â—â—â—â—
â—â—
â—â—â—â—
â—â—â—
â—â—
â—â—
â—â—
â—â—
â—â—â—â—
â—â—
â—â—
â—â—â—â—â—â—
â—â—
â—â—â—
â—â—â—â—
â—â—â—
â—
â—â—â—
â—â—â—â—â—â—â—
â—â—â—
â—â—â—
â—â—â—â—â—
â—â—
â—
â—â—â—
â—â—â—â—â—
â—â—â—â—
â—â—
â—â—â—
â—â—
â—â—â—
â—â—
â—â—â—â—â—
â—â—
â—â—
â—â—â—â—
â—â—â—â—â—â—
â—â—â—â—
â—â—â—â—â—â—â—
â—â—â—â—â—â—â—
â—â—â—
â—â—â—
â—â—â—â—â—
â—â—â—â—â—â—
â—â—â—
â—â—â—â—
â—â—â—â—â—â—â—
â—â—â—â—â—â—â—â—
â—â—â—â—â—â—â—
â—â—â—â—â—â—â—â—â—â—
â—â—â—
â—â—
â—â—â—
â—â—â—
â—â—
â—â—â—â—â—â—â—
â—â—
â—â—
â—â—â—
â—â—â—â—â—â—
â—â—â—â—â—â—
â—â—â—â—â—
â—â—
â—
â—â—â—
â—â—â—â—
â—â—â—â—
â—â—â—
â—â—â—â—
â—â—â—
â—â—
â—â—
â—â—
â—â—â—â—
â—â—
â—â—â—â—
â—â—â—â—â—
â—â—
â—
â—â—â—â—â—â—â—
â—
â—â—â—â—â—â—â—
â—â—â—
â—â—â—â—â—
â—
â—â—â—â—â—
â—â—
â—â—â—â—
â—â—â—
â—â—â—
â—â—â—â—
â—â—
â—â—
â—â—â—â—
â—â—â—
â—â—â—
â—
â—â—â—â—
â—
â—â—â—
â—â—â—â—â—â—
â—â—â—â—
â—
â—â—
â—â—â—â—
â—â—â—
â—â—
â—â—â—
â—â—â—â—
â—â—â—â—â—â—
â—â—
â—â—
â—â—
â—â—â—â—
â—â—â—â—â—â—â—
â—â—â—
â—â—â—â—
â—â—
â—â—â—â—â—
â—
â—â—
â—â—
â—
â—â—
â—â—â—
â—â—â—â—
â—â—
â—â—
â—â—â—â—â—
â—â—â—â—
â—â—
â—â—â—â—â—
â—â—
â—â—â—
â—â—â—
â—â—
â—â—â—â—â—â—â—â—â—
â—â—â—â—â—
â—â—â—â—â—
â—â—â—â—
â—â—â—â—â—â—
â—â—â—
â—â—
â—â—
â—â—
â—â—â—â— â—â—â—â—â—â—â—â—â—
â—â—â—
â—â—â—â—â—â—
â—â—
â—â—â—â—â—â—â—â—â—â—â—â—
â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—
â—â—â—â—â—â—â—â—
â—â—â—â—â—â—â—â—â—â—
â—â—â—â—â—â—â—
â—â—â—
â—â—
â—â—â—â—â—â—â—â—
â—â—
â—â—â—â—
â—â—â—â—
â—â—â—â—â—
â—â—â—
â—â—â—â—â—â—â—â—â—â—
â—â—â—â—â—â—â—â—â—
â—â—â—â—â—
â—â—â—â—
â—â—â—â—â—â—â—
â—â—â—â—â—
â—â—
â—â—
â—â—â—â—
â—â—
â—â—
â—â—
â—â—â—
â—â—â—
â—â—â—â—â—
â—â—â—â—â—â—â—â—â—â—
â—â—â—â—â—
â—â—â—â—â—â—â—â—
â—â—â—â—â—â—
â—â—â—â—
â—â—â—â—â—â—
â—â—â—
â—â—
â—â—â—â—â—â—
â—â—â—â—â—â—â—â—â—â—â—
â—â—â—â—â—
â—â—â—
â—â—â—â—â—
â—â—â—
â—â—â—â—â—â—â—â—â—
â—â—â—â—â—
â—â—â—â—â—
â—â—â—â—â—â—â—â—â—
â—â—â—â—â—â—â—
â—â—â—â—â—
â—â—â—â—â—â—
â—â—
â—â—
â—â—
â—â—
â—â—â—
â—â—â—â—â—â—
â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—
â—â—â—â—â—â—
â—â—â—â—â—â—â—â—â—â—
â—â—
â—â—
â—â—â—
â—â—â—â—
â—â—â—â—
â—â—â—â—â—â—â—
â—â—â—â—â—
â—â—
â—â—â—
â—â—â—â—
â—â—â—â—â—â—â—â—â—â—
â—â—
â—â—
â—â—
â—â—
â—â—â—â—
â—
â—â—â—
â—
â—â—â—â—â—
â—â—â—
â—â—
â—â—â—â—
â—â—
â—â—
â—â—â—â—â—
â—â—â—â—
â—â—
â—
â—â—â—â—â—
â—â—â—â—
â—â—â—
â—
â—â—â—
â—â—
â—â—â—â—â—â—â—â—â—
â—â—â—
â—â—
â—
â—â—
â—
â—â—â—â—
â—â—â—â—â—â—
â—â—â—
â—â—â—â—â—
â—â—â—â—
â—
â—â—â—â—
â—
â—â—
â—â—â—
â—â—â—â—
â—â—â—â—
â—â—
â—â—â—
â—
â—â—
â—â—â—
â—â—â—
â—â—
â—â—â—
â—â—
â—â—â—â—â—
â—â—â—â—â—
â—â—
â—â—â—
â—â—â—
â—â—â—â—â—
â—â—
â—
â—â—â—
â—â—
â—â—â—â—
â—â—
â—â—â—â—â—â—
â—
â—â—â—
â—
â—â—
â—â—â—â—â—
â—â—â—â—â—
â—â—â—â—
â—â—
â—
â—â—â—â—â—
â—â—â—â—â—â—â—â—
â—â—
â—â—â—â—â—
â—â—
â—â—â—â—
â—â—â—â—â—â—
â—
â—â—â—â—â—â—
â—â—
â—
â—â—â—
â—â—
â—â—â—â—â—
â—â—
â—â—â—â—
â—â—
â—â—
â—â—â—â—â—â—
â—â—â—
â—â—â—â—
â—â—
â—â—
â—â—â—
â—â—â—
â—â—
â—â—â—â—
â—â—â—
â—
â—â—â—
â—â—â—
â—â—â—
â—â—â—
â—â—â—â—â—
â—â—
â—â—â—â—â—â—
â—â—
â—â—â—
â—â—
â—â—â—
â—â—â—
â—
â—â—
â—â—â—
â—â—â—
â—â—â—â—â—â—â—â—
â—â—â—â—â—â—â—
â—â—â—â—â—
â—
â—â—â—
â—â—
â—â—
â—â—â—â—
â—â—
â—â—
â—â—â—
â—â—â—â—â—â—â—
â—â—â—â—â—
â—â—
â—
â—â—â—â—
â—â—â—
â—â—â—
â—
â—â—
â—â—
â—
â—â—â—
â—â—â—
â—â—â—
â—â—
â—â—
â—â—
â—â—â—â—
â—â—â—
â—
â—â—â—â—â—
â—â—
â—â—
â—â—â—â—â—
â—â—â—
â—â—
â—â—
â—â—â—
â—â—â—â—
â—â—
â—â—â—
â—â—â—
â—â—â—â—
â—â—â—
â—
â—â—
â—â—â—
â—â—
â—â—â—â—â—â—â—â—â—â—
â—â—â—
â—
â—â—
â—
â—â—
â—â—
â—â—â—
â—
â—â—â—
â—â—â—â—
â—â—â—â—
â—â—
â—â—
â—â—â—
â—
â—â—â—â—â—â—
â—â—
â—
â—â—â—â—
â—â—
â—â—
â—â—â—â—â—
â—â—
â—â—â—â—â—â—â—
â—â—â—â—
â—â—â—
â—
â—â—
â—â—
â—â—â—â—â—â—
â—
â—â—
â—
â—â—â—
â—
â—â—
â—â—â—â—â—
â—â—
â—â—
â—â—â—
â—â—â—
â—â—â—â—
â—â—
â—â—
â—â—â—
â—â—â—â—
â—â—â—â—â—
â—â—â—â—
â—â—â—
â—â—
â—
â—â—
â—â—
â—â—
â—
â—â—â—â—
â—â—â—â—
â—â—
â—â—â—â—â—
â—
â—â—â—â—
â—â—
â—
â—â—â—â—
â—â—â—
â—â—â—
â—â—â—â—
â—â—
â—â—â—â—
â—â—â—
â—â—
â—â—
â—
â—â—â—â—â—
â—â—â—
â—
â—â—â—â—â—â—
â—
â—â—
â—â—
â—
â—
â—â—
â—â—â—â—
â—â—â—
â—â—â—
â—â—
â—
â—â—
â—â—â—
â—
â—â—
â—â—â—â—
â—
â—â—â—
â—â—â—
â—â—â—
â—â—â—â—â—
â—â—â—â—â—â—
â—â—â—
â—â—
â—â—
â—
â—â—â—
â—â—â—
â—
â—â—â—â—
â—â—â—
â—â—â—â—
â—
â—â—â—â—
â—â—
â—â—â—â—â—â—â—â— â—â—â—â—â—â—â—â—â—
â—â—â—â—â—â—
â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—
â—â—â—â—â—â—â—â—â—â—â—â—
â—â—â—â—â—â—â—
â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—
â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—
â—â—â—â—â—â—â—
â—â—â—
â—â—
â—â—â—â—â—â—â—â—â—â—â—â—
â—â—â—â—â—â—â—â—â—
â—â—
â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—
â—â—â—â—â—â—
â—â—â—â—â—â—
â—â—
â—â—â—â—â—â—â—â—â—â—â—â—
â—â—â—â—â—â—
â—â—â—â—â—â—â—â—â— â—â— â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—
â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—
â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—
â—â—â—â—â—â—
â—â—â—â—â—â—â—
â—â—â—â—â—â—â—
â—â—
â—â—â—â—â—
â—â—â—
â—
â—â—â—
â—â—â—
â—â—
â—
â—â—â—â—â—
â—â—â—â—
â—â—
â—
â—â—â—â—
â—â—
â—
â—â—â—â—â—
â—â—â—
â—â—
â—â—â—
â—â—â—â—â—
â—â—â—â—
â—â—â—â—
â—â—
â—â—
â—â—â—â—
â—â—
â—â—â—â—â—
â—â—
â—â—â—
â—â—â—â—
â—â—
â—â—â—â—â—
â—â—
â—â—â—
â—â—â—â—
â—â—â—â—
â—â—â—â—â—
â—â—
â—â—â—
â—â—
â—
â—â—â—â—
â—â—
â—
â—â—
â—â—
â—
â—â—â—â—â—â—â—
â—
â—â—â—
â—â— â—
â—â—
â—â—
â—â—
â—â—
â—â—
â—â—â—â—
â—â—â—
â—â—
â—â—â—
â—â—
â—â—â—
â—â—â—â—â—
â—â—â—â—â—â—â—â—â—
â—â—â—â—
â—â—â—â—
â—â—
â—â—â—
â—â—â—â—
â—â—
â—â—â—
â—â—
â—â—â—
â—â—â—
â—â—â—â—â—â—â—
â—â—â—â—â—
â—â—
â—â—â—â—â—â—
â—â—
â—â—
â—â—â—
â—â—â—
â—â—
â—â—â—
â—â—
â—â—
â—â—â—â—â—
â—â—â—â—
â—â—â—â—â—
â—â—â—â—â—
â—â—â—â—â—â—
â—â—â—
â—â—â—â—
â—â—â—â—
â—â—
â—â—â—
â—â—â—â—â—â—â—â—â—â—
â—â—â—â—
â—â—â—â—â—
â—â—
â—â—â—â—
â—â—â—
â—â—â—
â—â—
â—â—â—
â—â—
â—â—â—â—
â—â—â—â—â—â—
â—â—â—
â—â—â—â—â—
â—â—â—
â—â—
â—â—â—
â—â—â—
â—â—
â—â—
â—â—â—â—
â—â—â—
â—â—
â—
â—â—
â—â—â—
â—â—â—
â—â—â—â—â—â—
â—â—â—â—â—
â—â—â—â—â—
â—
â—â—â—â—
â—â—â—
â—â—
â—â—â—â—
â—â—â—â—â—
â—â—â—â—â—
â—
â—â—â—â—â—â—â—â—
â—
â—â—â—
â—â—
â—â—â—
â—â—â—â—â—â—
â—
â—â—
â—â—
â—â—â—
â—â—
â—â—
â—â—â—â—
â—â—â—â—â—â—
â—â—
â—â—
â—â—
â—
â—â—â—
â—
â—â—â—â—
â—â—â—
â—
â—â—â—
â—â—â—
â—â—
â—â—â—â—â—
â—â—â—â—â—â—â—
â—â—â—â—â—â—â—â—
â—â—
â—â—
â—â—â—
â—â—â—
â—â—â—
â—â—â—â—â—
â—â—â—
â—â—â—
â—
â—â—â—â—â—
â—â—â—â—â—â—
â—â—â—â—
â—â—
â—â—â—
â—â—â—â—
â—â—
â—â—â—
â—
â—â—â—â—
â—â—
â—â—
â—â—â—
â—â—â—â—â—
â—
â—â—â—â—â—â—â—â—â—
â—â—
â—â—â—
â—â—â—
â—â—
â—â—â—â—
â—â—
â—â—â—
â—â—
â—â—
â—â—â—â—â—
â—â—
â—â—â—â—â—â—
â—â—â—â—â—â—â—
â—â—â—â—
â—â—â—â—â—â—â—
â—â—â—â—â—â—
â—â—â—â—â—
â—â—â—â—â—
â—â—
â—â—â—
â—â—â—
â—â—â—
â—â—
â—â—â—
â—â—â—â—â—
â—â—â—â—â—â—â—
â—â—â—
â—â—â—â—
â—â—â—â—â—â—
â—â—â—â—â—
â—â—â—â—â—
â—â—â—â—
â—â—
â—â—â—â—â—â—
â—â—â—â—â—â—â—
â—â—â—â—â—â—â—â—â—â—
â—â—â—
â—â—
â—â—â—â—â—â—â—â—â—
â—â—â—â—â—â—â—â—â—
â—â—â—
â—â—â—
â—â—â—â—
â—â—â—â—
â—â—
â—â—
â—â—
â—â—
â—â—
â—â—â—â—â—
â—â—â—â—â—â—â—â—â—
â—â—â—
â—â—â—
â—â—â—
â—â—
â—â—â—â—â— â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â— â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â— â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â— â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â— â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â— â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â— â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â— â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â— â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â— â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â— â—â—â—â—â— â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—
4 4 4 40 10 20 30 40 â—â—â—â—â—â— â—â—â—â—â—â—â—â—â—â—â— â—â—â—â—â— â—â—â—â—â—â—â—â—â—â—â—â—â—â— â—â—â—â—â—â—â— â—â—â—â—â—â—â—â—â—â—â—â—â—â— â—â—â—â—â—â—â—â—
5 5 5 50 10 20 30 40 
6 6 6 60 10 20 30 40 â—â—â—â— â—â— â—â—â—â— â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â— â—â—â—â—
â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â— â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â— â—â—â—â—â— â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—
1 1 1 10 10 20 30 40 â—â—â—â—â—â—â—â—â— â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â— â—â—â—â—â—â—â—â—â—â—â—â—â— â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â— â—â—â—â— â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â— â—â—â—â—â— â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â— â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â— â—â—â—â—â—â—â—â—â—â—â—â—â— â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â— â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â— â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â— â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â— â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â— â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â— â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â— â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â— â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â— â—â—â—â—â—â— â—â—â—â—â—â—â—â—â— â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â— â—â—â—â—â—â—â—â—â— â—â—â— â—â—â—â—â—â—â—â—â—â—â— â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—
2 2 2 20 10 20 30 40 â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—
â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â— â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â— â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â— â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â— â— â—â—â—â— â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â— â— â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â— â— â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â— â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â— â—â— â—â—â—â—â—â—â—â—â—â—â—â—â—â— â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â— â— â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â— â—â—â—â— â—â—â—â—â—â—â—â—â— â— â—â—â—â—â— â—â—â—â—â—â—â—â—â—â— â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â— â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â— â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â— â—â—â—â—â—â—â—â—â—â—â—â—â—â— â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â— â—â—â—â—â—â—â—â—â—â—â—â—â— â—â—â—â—â—â— â—â—â—â—â—â—â—â—â—â— â—â—â—
3 3 3 30 10 20 30 40 Fault Rate (x100%) 
N 2N 3N M
(b) Including outliers
Figure 3. Boxplots of the prediction fault rates for the six systems ( 1to6
listed in Table II) and the four sample sizes ( NtoMlisted in Table II)
â€œFault Rateâ€) lists the mean and standard deviation of the
prediction fault rate for each system and each sample size.
As shown in Figure 3 and Table III, for any statistical
measure (the mean, standard deviation, median, or outlier),
we observe a robust decreasing trend of the prediction fault
rate when the sample size increases from NtoMfor each
system.8As listed in Table III, based on a random sample of
sizeN(Column â€œFault Rate â€“ Nâ€), the fault rate is 8 % or
8Note that Mdepends on the number of features and on the conï¬guration
constraints among features in a case study. In most cases, Mis greater than
3N; the exception is B ERKELEY DB J AVA where M= 48 and3N= 78 .306TABLE III
MEANSTANDARD DEVIATION OF THE PREDICTION FAULT RATE (%)AND TIME COST (ms)FOR THE SIX SYSTEMS (1TO6LISTED IN TABLE II)AND
THE FOUR SAMPLE SIZES (NTOMLISTED IN TABLE II)
Fault Rate (%) Time Cost (ms)
N 2N 3N M N 2N 3N M
1 26.928.4 11.614.4 8.46.7 9.710.8 265 3012 245 245
2 5.74.9 4.54.2 4.03.6 3.32.4 345 4213 369 349
3 15.118.1 8.57.5 7.26.4 6.45.7 245 200 265 224
4112.4354.6 98.3243.1 46.870.7 7.813.2 265 428 265 345
5 3.22.6 2.22.3 2.62.5 2.72.5 349 369 345 324
6 8.04.5 8.14.4 7.64.4 7.24.2 445 424 449 900
less for three subject systems (LLVM, B ERKELEY DB J AVA,
and SQL ITE, in Rows 2, 5 and 6). Based on a random sample
of size M, we achieve a mean prediction fault rate of 6:2 %,
on average, for all six subject systems, i.e., the average of all
mean prediction fault rates listed in Column â€œFault Rate â€“ Mâ€.
3) Discussion: An average prediction accuracy of 93:8 %
on small random samples of size Mfor six real-world conï¬g-
urable systems demonstrates the effectiveness of our approach
for variability-aware performance prediction. For three subject
systems, the approach even produces a prediction accuracy
of92 % or higher, based on small random samples of size
N. Moreover, our approach does show a robust increasing
trend of prediction accuracy with the increasing sample size.
Thus, the experiment conï¬rms that our approach can work
progressively with random samples of any user-deï¬ned size
(i.e.,Nor larger) and improve the prediction accuracy when
more data are available.
D. Experiment on Time Cost
For RQ 3, the time consumed by the prediction process of
our approach mainly stems from performance modeling using
CART. Once the performance model is built, as explained
in Section IV, deriving a decision rule and providing the
prediction result for users are instantaneous. Furthermore, as
CART has been widely used in statistics and data-mining
applications, and it has been shown fast and reliable [4], [11],
the hypothesis of this experiment is as follows.
1) Hypothesis: The time of building a performance model
by CART is reasonable.
2) Results: We measured the time of building a perfor-
mance model in the same experimental context as the ex-
periment of Section VI-C. The results are listed in Table III
(Column â€œTime Costâ€). For ï¬ve of the six systems, the time of
building a performance model on any random sample of size
fromNtoMis approximately 42milliseconds (ms). Only for
SQL ITEand the sample size M= 566 , the time cost reaches
a high of 90ms, which is still a reasonable amount of time.
3) Discussion: Although we perform CART with an ex-
haustive search over all feature-selection variables for the best
split that minimizes the total prediction errors, as explained in
Section IV-B, the constant local model deï¬ned in Equation 2
makes the search process fast, because there is no complicated
calculation for the total prediction errors deï¬ned in Equation 4.Moreover, the time of at most 90ms needed for all six subject
systems and for any sample size from NtoMdemonstrates
that our approach is highly efï¬cient for variability-aware
performance prediction.
E. Comparative Analysis of Performance Distributions
The previous experiments demonstrate the effectiveness of
our approach, however, we still want to give evidence why
the approach works with small random samples (for RQ 4).
Since the approach depends on CART to address a non-linear
regression problem, a general explanation from the statistical-
learning theory is that CART works well when the problem it
addresses or the data it evaluates does ï¬t the regressive pattern
it builds [4]. Moreover, as explained in Section IV-B, CART
builds a tree-like prediction model that recursively partitions a
sample and renders the total prediction errors in each partition
minimal; this way, the prediction model always ï¬ts the sample
well. If the sample can represent the whole population or
reï¬‚ect the important characteristics of the whole population,
then the prediction model built on the sample also ï¬ts the
whole population well and makes accurate predictions.
Since our prediction targets numeric performance values,
the performance distribution is an important characteristic for
performance prediction. Thus, we conducted a comparative
analysis of performance distributions between random sam-
ples and the corresponding whole populations, guided by the
following hypothesis.
1) Hypothesis: Our approach works well with a small
random sample when the sample has a similar performance
distribution as the whole population.
2) Results: For each subject system, we collected all the
random samples generated in the previous experiments. Then,
we visualized the average performance distribution for each
sample size from NtoM, to mitigate the inï¬‚uence of a
speciï¬c performance distribution of a certain random sample.
Furthermore, we visualized the performance distribution of the
whole population of each system, to be able to compare it to
the performance distribution of each random sample of size
from NtoM.
Due to the space limit, we present the experimental results
for only one subject system here.9Figure 4 shows the his-
9The experimental results of all six subject systems ( 1to6listed in Table II)
are available in a technical report at http://gsd.uwaterloo.ca/node/527.307tograms of the performance distributions for the four sample
sizes ( NtoM) and the whole population of X264.10
As shown in Figure 4e, the performance distribution of the
whole population of X264 is roughly a distribution with two
peaks. The performance distribution of the random sample of
sizeN(Figure 4a) identiï¬es the two peaks, but misses the
correct locations of these peaks. The performance distributions
for the sample size from 2N(Figure 4b), 3N(Figure 4c), to M
(Figure 4d) gradually move and form the two peaks approxi-
mate to the precise locations, as shown in Figure 4e. With such
a gradual process that generates a more similar performance
distribution as the whole population, the prediction fault rate
shows a robust decreasing trend from 15:1 %,8:5 %,7:2 %to
6:4 %when the sample size increases from N,2N,3NtoM.
Similarly, for each of the other ï¬ve subject systems, the
random sample of size Malways exhibits a very similar
performance distribution as the whole population. For the three
systems (LLVM, B ERKELEY DB J AVA, and SQL ITE) with
92 % or higher prediction accuracy based on a random sample
of size N, a similar performance distribution as the whole
population can be found on the random sample of size N.
3) Discussion: The comparative analysis of performance
distributions between random samples and the whole popula-
tions reveals that our approach works well with a small random
sample when the sample has a similar performance distribution
as the whole population. In fact, we found explicit evidence
that a sample does reï¬‚ect some important characteristics of the
whole population when we can produce accurate predictions
based on it. However, we are aware that the performance
distribution may be just one of the relevant characteristics
for performance prediction. These characteristics may involve,
for example, the number and dispersion of distinct values
as well as the feature coverage. A quantitative study on the
similarity between a random sample and the whole population,
involving more characteristics for performance prediction,
shall be conducted in future work.
F . Threats to Validity
To enhance internal validity, we performed automated ran-
dom sampling avoiding misleading effects of speciï¬c-selected
training samples and test samples. We randomly selected
samples of four sizes ( NtoM) respectively from the whole
population of each subject system as the training sample, and
all of the rest as the test sample. We repeated each random
sampling ï¬ve times with freshly generated training samples
and test samples of the same size. The exception is the test
sample of SQL ITE, in which the original authors could not
measure all valid conï¬gurations in reasonable time; thus, to
mitigate the possible effects of missing some important conï¬g-
urations, they sampled 100 additional random conï¬gurations
for prediction evaluation [19].
10A histogram provides a quick and intuitive visualization of the distribution
of the data [26]; it consists of two parts: the vertical bars , each of which
displays the frequency of each value range; and the density estimate curve ,
which shows a more accurate display of the distribution of the data.
(a) Random sample of size N
(b) Random sample of size 2N
(c) Random sample of size 3N
(d) Random sample of size M
(e) Whole population
Figure 4. Histograms of the performance distributions of the random samples
of four sizes ( NtoMlisted in Table II) and of the whole population of X264
(X-axis: Performance (seconds); Y-axis: Relative Frequency)
To automate the process of performance modeling by
CART, we use two important parameters ( minbucket and
minsplit ) and ï¬x others provided by the R packages to control
the recursive partitioning process of CART. For each case308study, we followed the same parameter settings to generate
the performance models automatically. We cannot guarantee
that the prediction fault rate and the time cost of performance
modeling obtained in our experiments depend on certain
shapes of the performance models built by CART. However,
to avoid misleading effects of specially-shaped performance
models, we generated all performance models automatically,
repeated each measurement (the prediction fault rate or the
time cost) for each system and each sample size ï¬ve times,
and took only the average of these measurements for analysis.
To increase external validity, we used a public dataset with
six systems spanning different domains, with different sizes,
different conï¬guration mechanisms, and different implemen-
tation languages. All systems have been deployed and used in
real-world scenarios. Moreover, the performance is measured
by standard benchmarks in the respective application domain.
However, we are aware that the results of our experiments
are not automatically transferable to all other conï¬gurable
systems, but we are conï¬dent that we controlled this threat
sufï¬ciently.
G. Strengths and Weaknesses of the Approach
To answer RQ 5, we compared our approach to SPLC ON-
QUEROR , a most recent approach to performance prediction
for conï¬gurable software systems [19]. We summarize the
strengths and weaknesses of the two approaches, which guides
users to choose one or the other in practice.
1) Prediction Fault Rate: In our experimental setup, N
is the number of all features of a system, and Mis the
number of conï¬gurations required by the pair-wise heuristic
of SPLC ONQUEROR . Moreover, Nis similar to the number
of conï¬gurations required by the feature-wise measurement of
SPLC ONQUEROR . Thus, we can compare the two approaches
according to these sample sizes. Table IV lists the prediction
fault rates produced by the two approaches when we apply
them to the (random or speciï¬c) samples of size NandM
for the six subject systems listed in Table II.
Our approach produces a prediction fault rate of 8 % or
less based on a random sample of size Nfor three systems
(Rows 2, 5, and 6 in Table IV); and it produces an average
of6:2 % prediction fault rate for all six systems, when the
sample size reaches M. By comparison, SPLC ONQUEROR
produces a prediction fault rate of 7:8 %using the feature-wise
measurement on a speciï¬c sample of size Nfor two systems
(Rows 2 and 6); and it produces an average of 9:1 %prediction
fault rate using the pair-wise heuristic on a speciï¬c sample of
sizeMfor all six systems. When other heuristics for higher-
order feature interactions are considered, SPLC ONQUEROR
can produce an average of 5 %prediction fault rate for all six
systems (not listed in Table IV) [19], which is more accurate
than our approach, but requires additional measurements.
2) Prediction Effort: The higher prediction accuracy of
SPLC ONQUEROR comes at a cost: SPLC ONQUEROR needs
additional effort to select speciï¬c conï¬gurations and to detect
performance-relevant feature interactions. When we encounter
a large-scale system with a great number of features, suchTABLE IV
MEANSTANDARD DEVIATION OF THE PREDICTION FAULT RATE (%)FOR
THE SIX SYSTEMS (1TO6LISTED IN TABLE II)AND THE TWO SAMPLE
SIZES (NANDMLISTED IN TABLE II)USING OUR APPROACH AND
SPLC ONQUEROR ; THE NUMBER IN BOLD INDICATES THE BEST CASE IN
EACH ROW
Our approach SPLConqueror
N M N M
1 26.928.4 9.710.8 14.924.8 7.711.2
2 5.74.9 3.32.4 7.89.0 7.410.2
3 15.118.1 6.45.7 29.622.0 17.927.2
4 112.4354.6 7.813.2 44.142.3 3.95.3
5 3.22.6 2.72.5 17.719.6 8.59.6
6 8.04.5 7.24.2 7.89.2 9.312.5
Avg. 28.668.9 6.26.5 20.321.2 9.112.7
effort can be quite expensive [14]. In contrast, our approach
supports random sampling and progressive performance pre-
diction, which makes it more pragmatic. That is, our approach
can work with random samples and make more accurate
predictions progressively when more data are available. Fur-
thermore, as shown in Section VI-D, our approach is highly
efï¬cient and the prediction process often takes only little time.
3) Sample Dependence: SPLC ONQUEROR works because
it relies on speciï¬c samples selected by heuristics to detect
performance-relevant feature interactions; our approach works
because (1) the evaluated dataset ï¬ts well in the non-linear
regression model we use, and (2) the random sample we use
reï¬‚ects the important characteristics of the whole population.
As demonstrated in Section VI-E, our approach works well
with a small random sample, provided it has a similar perfor-
mance distribution as the whole population. However, if such
a random sample happens to be skewed to some undesirable
characteristics, the prediction effects of our approach might
be affected. For example, users may prefer some speciï¬c
conï¬gurations with certain features and always miss some
other features; based on such a sample, our approach may not
produce accurate predictions for the conï¬gurations selecting
the missed features.11
4) Application Scope: A clear advantage of a regression
technique is that it can support not only Boolean (e.g., a
â€œselectedâ€ or â€œdeselectedâ€ feature), but also numeric feature
selections (e.g., the heap size or the CPU speed [24], [25]),
which makes our approach applicable in wider domains. In
contrast, the existing techniques for feature-interaction detec-
tion support only Boolean feature selections.
5) Summary: Both approaches have strengths and weak-
nesses. We expect that a combination of both approaches is
beneï¬cial to further increase prediction accuracy and reduce
prediction effort in wider application domains (e.g., by com-
bining heuristics used in SPLC ONQUEROR to identify suitable
samples for CART). In particular, we provide the following
guidance for users to choose between the two approaches
in practice. (1) If a software system has a small number of
features, such that the cost of detecting feature interactions is
11An exploratory experiment on missing features and skewed conï¬gurations
is available in a technical report at http://gsd.uwaterloo.ca/node/484.309acceptable, users shall choose SPLC ONQUEROR , due to its
higher prediction accuracy; otherwise, our proposed approach
is superior, due to its reduced prediction effort and reasonable
prediction accuracy. (2) If there is already a sample available,
one has to check whether the sample satisï¬es certain feature-
coverage criteria, before using SPLC ONQUEROR ; whereas one
can produce predictions directly based on the sample using our
approach.
VII. R ELATED WORK
A. Model-Based Prediction
CART and its variants, such as Random Forests and Boost-
ing, have been widely used in statistics and data mining,
because CARTâ€™s algorithm is fast and reliable, and its tree
structure can provide insight into the relevant input variables
for prediction [4], [11].
Thereska et al. [24] proposed a practical performance model
based on CART for interactive client applications, such as
Microsoft Ofï¬ce and Mozilla. They focused on a range of de-
ployment parameters from the usersâ€™ application environment,
such as CPU speed and memory size; instead, we consider
the conï¬guration options of a software system. Moreover, our
approach targets all kinds of conï¬gurable software systems,
as long as the valid conï¬gurations can be derived.
Westermann et al. [25] presented an approach to the
automated improvement of performance-prediction functions
by three measurement-point-selection strategies based on the
prediction accuracy. They constructed the prediction functions
by statistical inference techniques, including CART. This
approach, however, assumes that all input variables of the
prediction function are already relevant to performance; while
our approach does not have such a restriction, but considers
all features of a software system.
Even though the above studies have demonstrated the effects
of CART on performance prediction for different case studies
[24], [25], they did not explicitly provide evidence for why
CART does or does not work. We found that our approach
works well with a small random sample when the sample has
a similar performance distribution as the whole population.
Happe et al. [10] proposed a compositional reasoning
approach based on component speciï¬cations with resource
demands and predicted execution time. Their approach is re-
stricted to component-based systems, whereas our approach is
applicable to all conï¬gurable systems, once their conï¬gurable
options are abstracted as features.
Tawhid and Petriu [23] presented a model-driven approach
to deriving a performance model from an extended feature
model with performance-analysis information. The approach
requires detailed up-front knowledge from a domain-speciï¬c
performance analysis, which makes tuning prediction for accu-
racy difï¬cult. Our approach avoids these problems by directly
working with performance measurements.
Ramirez and Cheng [17] presented an approach that lever-
ages goal-based models to facilitate the automatic derivation of
utility functions at the requirements level; our approach works
at the level of actual program variants.B. Measurement-Based Prediction
A most recent measurement-based prediction technique is
SPLC ONQUEROR [19], [21]. We have compared it to our
approach and discussed the strengths and weaknesses of both
approaches in Section VI-G.
Sincero et al. [22] used existing conï¬gurations and measure-
ments to predict a conï¬gurationâ€™s non-functional properties.
They designed the Feedback approach to ï¬nd the correlation
between feature selections and measurements and to provide
qualitative information about how a feature inï¬‚uences a non-
functional property during the conï¬guration process. In con-
trast to our approach, their approach does not actually predict
a performance value quantitatively.
Chen et al. [6] combined benchmarking and proï¬ling to
predict the performance of component-based applications. In
contrast, our approach correlates performance measurements
with conï¬gurations and can work with any set of conï¬gura-
tions measured by simulation or by monitoring in the ï¬eld.
VIII. C ONCLUSION
We proposed a progressive and variability-aware approach
to performance prediction for conï¬gurable software systems
based on random samples. The approach uses the statistical
learning technique CART to build an explicit performance
model that represents the correlation between feature se-
lections and performance. We demonstrated the feasibility
and effectiveness of our approach on six real-world systems,
spanning different domains, implementation languages, and
conï¬guration mechanisms. Our empirical results show that the
approach produces a prediction accuracy of 94 % , on average,
based on small random samples. Moreover, our approach
shows a robust increasing trend of prediction accuracy as the
sample size increases. A comparative analysis of performance
distributions revealed that our approach works well when the
corresponding sample has a similar performance distribution as
the whole population. We compared our approach to a state-
of-the-art technique, called SPLC ONQUEROR , and explored
the strengths and weaknesses of the two approaches, to guide
users to choose one or the other in practice.
Our approach has the potential of wide application to
help users make trade-offs between feature selections and
performance and to guide the conï¬guration process [15]. In
future work, we aim at performing systematic parameter tuning
for CART and trying other regression techniques (e.g., Sup-
port Vector Machines [4]). Moreover, we aim at quantifying
the similarity between a sample and the whole population,
involving several characteristics for performance prediction. In
addition, we will explore the potential of using our approach
for conï¬guration optimization [1], [9], [18], test generation
[27], and bug prediction [12].
ACKNOWLEDGMENTS
This work has been partially supported by NECSIS, Ontario
Research Fund - Research Excellence Project on Model-Based
Development, the German Research Foundation (AP 206/4 and
AP 206/5), and MT-LAB (a VKR Centre of Excellence).310REFERENCES
[1] A. Aleti, B. Buhnova, L. Grunske, A. Koziolek, and I. Meedeniya,
â€œSoftware Architecture Optimization Methods: A Systematic Literature
Review,â€ IEEE Transactions on Software Engineering , vol. 39, no. 5,
pp. 658â€“683, 2013.
[2] S. Apel and C. KÃ¤stner, â€œAn Overview of Feature-Oriented Software
Development,â€ Journal of Object Technology , vol. 8, no. 5, pp. 49â€“84,
2009.
[3] D. Batory, D. Benavides, and A. Ruiz-Cortes, â€œAutomated Analyses
of Feature Models: Challenges Ahead,â€ Communications of the ACM ,
vol. 49, no. 12, pp. 45â€“47, 2006.
[4] R. Berk, Statistical Learning from a Regression Perspective . Springer,
2008.
[5] L. Breiman, J. Friedman, C. Stone, and R. Olshen, Classication and
Regression Trees . Wadsworth and Brooks, 1984.
[6] S. Chen, Y . Liu, I. Gorton, and A. Liu, â€œPerformance Prediction
of Component-Based Applications,â€ Journal of Systems and Software ,
vol. 74, no. 1, pp. 35â€“43, 2005.
[7] P. Clements and L. Northrop, Software Product Lines: Practices and
Patterns . Addison-Wesley, 2001.
[8] K. Czarnecki and U. Eisenecker, Generative Programming: Methods,
Tools, and Applications . Addison-Wesley, 2000.
[9] J. Guo, J. White, G. Wang, J. Li, and Y . Wang, â€œA Genetic Algorithm
for Optimized Feature Selection with Resource Constraints in Software
Product Lines,â€ Journal of Systems and Software , vol. 84, no. 12, pp.
2208â€“2221, 2011.
[10] J. Happe, H. Koziolek, and R. Reussner, â€œFacilitating Performance
Predictions Using Software Components,â€ IEEE Software , vol. 28, no. 3,
pp. 27â€“33, 2011.
[11] T. Hastie, R. Tibshirani, and J. Friedman, The Elements of Statistical
Learning: Data Mining, Inference, and Prediction , 2nd ed. Springer,
2009.
[12] K. Herzig, S. Just, and A. Zeller, â€œItâ€™s not a Bug, itâ€™s a Feature: How
Misclassiï¬cation Impacts Bug Prediction,â€ in Proc. ICSE . IEEE, 2013.
[13] K. Kang, S. Cohen, J. Hess, W. Novak, and A. Peterson, â€œFeature-
Oriented Domain Analysis (FODA) Feasibility Study,â€ SEI, CMU,
Tech. Rep. SEI-90-TR-021, 1990.
[14] J. Liebig, A. von Rhein, C. KÃ¤stner, S. Apel, J. DÃ¶rre, and C. Lengauer,
â€œScalable Analysis of Variable Software,â€ in Proc. ESEC/FSE . ACM,
2013.[15] A. Murashkin, M. Antkiewicz, D. Rayside, and K. Czarnecki, â€œVisual-
ization and Exploration of Optimal Variants in Product Line Engineer-
ing,â€ in Proc. SPLC . ACM, 2013.
[16] K. Pohl, G. Bockle, and F. van der Linden, Software Product Line
Engineering: Foundations, Principles, and Techniques . Springer-Verlag,
2005.
[17] A. Ramirez and B. Cheng, â€œAutomatic Derivation of Utility Functions
for Monitoring Software Requirements,â€ in Proc. MODELS . IEEE,
2011.
[18] A. Sayyad, T. Menzies, and H. Ammar, â€œOn the Value of User
Preferences in Search-Based Software Engineering: A Case Study in
Software Product Lines,â€ in Proc. ICSE . IEEE, 2013.
[19] N. Siegmund, S. Kolesnikov, C. KÃ¤stner, S. Apel, D. Batory, M. Rosen-
mÃ¼ller, and G. Saake, â€œPredicting Performance via Automated Feature-
Interaction Detection,â€ in Proc. ICSE . IEEE, 2012.
[20] N. Siegmund, M. RosenmÃ¼ller, C. KÃ¤stner, P. Giarrusso, S. Apel, and
S. Kolesnikov, â€œScalable Prediction of Non-functional Properties in Soft-
ware Product Lines: Footprint and Memory Consumption,â€ Information
and Software Technology , vol. 55, no. 3, pp. 491â€“507, 2013.
[21] N. Siegmund, M. RosenmÃ¼ller, M. Kuhlemann, C. KÃ¤stner, S. Apel,
and G. Saake, â€œSPL Conqueror: Toward Optimization of Non-Functional
Properties in Software Product Lines,â€ Software Quality Journal , vol. 20,
no. 3-4, pp. 487â€“517, 2012.
[22] J. Sincero, W. SchrÃ¶der-Preikschat, and O. Spinczyk, â€œApproaching
Non-functional Properties of Software Product Lines: Learning from
Products,â€ in Proc. APSEC . IEEE, 2010.
[23] R. Tawhid and D. Petriu, â€œAutomatic Derivation of a Product Perfor-
mance Model from a Software Product Line Model,â€ in Proc. SPLC .
IEEE, 2011.
[24] E. Thereska, B. Doebel, A. Zheng, and P. Nobel, â€œPractical Performance
Models for Complex, Popular Applications,â€ in Proc. SIGMETRICS .
ACM, 2010.
[25] D. Westermann, J. Happe, R. Krebs, and R. Farahbod, â€œAutomated
Inference of Goal-Oriented Performance Prediction Functions,â€ in Proc.
ASE. ACM, 2012.
[26] G. Williams, Data Mining with Rattle and R: The Art of Excavating
Data for Knowledge Discovery . Springer, 2011.
[27] S. Yoo and M. Harman, â€œRegression Testing Minimization, Selection and
Prioritization: A Survey,â€ Software Testing, Veriï¬cation & Reliability ,
vol. 22, no. 2, pp. 67â€“120, 2012.311