ConvergentContemporar ySoftware PeerReviewPractices
PeterC.Rigby
Concordia University
Montreal, QC,Canada
peter .rigby@concordia.caChristian Bird
Microsoft Research
Redmond, WA,USA
cbird@microsoft.com
ABSTRA CT
Softwarepeerreviewispracticedonadiversesetofsoft-
wareprojectsthathavedrasticallydiﬀerentsettings,cultures,
incentivesystems,andtimepressures.Inaneﬀorttochar-
acterizeand understandthesediﬀerencesweexaminetwo
Google-ledprojects,AndroidandChromiumOS,threeMi-
crosoftprojects,Bing,Oﬃce,andMS SQL,and projects
internaltoAMD.Wecontrastourﬁndingswithdatataken
fromtraditionalsoftwareinspectionconductedonaLucent
projectandfromopensourcesoftwarepeerreviewonsix
projects, includingApache,Linux,andKDE.Ourmeasures
ofinterestincludethereviewinterval,thenumberofdevelop-
ersinvolvedinreview,and proxymeasuresforthenumberof
defectsfound duringreview.Weﬁndthatdespitediﬀerences
amongprojects,manyofthecharacteristicsofthereviewpro-
cesshaveindependentlyconvergedtosimilarvalueswhichwe
thinkindicategeneralprinciplesofcodereviewpractice.We
alsointroduceameasureofthedegreetowhichknowledge
issharedduringreview.Thisisanaspectofreviewpractice
thathastraditionallyonlyhadexperientialsupport.Our
knowledgesharingmeasureshowsthatconductingpeerre-
viewincreasesthenumberofdistinctﬁlesadeveloperknows
aboutby66%to 150%depending ontheproject.Thispaper
isoneoftheﬁrststudiesofcontemporaryreviewinsoftware
ﬁrmsandthemostdiversestudyofpeerreviewtodate.
Categories andSubject Descriptors
D.2.8[SoftwareEngineering]: [Metrics];K.6.3[Software
Management]: [Softwaredevelopment;Softwareprocess]
General Terms
Management,Measurement
Keywords
Peercodereview,EmpiricalSoftwareEngineering,Inspection,
Softwareﬁrms,Opensourcesoftware
Permission tomakedigital orhard copies ofallorpart ofthisworkfor
personal orclassroom useisgranted without feeprovided thatcopies are
notmade ordistrib uted forproﬁt orcommercial advantage andthatcopies
bear thisnotice andthefullcitation ontheﬁrstpage. Tocopyotherwise, to
republish, topost onserversortoredistrib utetolists, requires prior speciﬁc
permission and/or afee.
ESEC/FSE ’13,August 18 – 26,2013, Saint Petersb urg,Russia
Copyright 2013 ACM978-1-4503-2237-9/13/08 ...$15.00.1.INTR ODUCTION
Softwarepeerreview,inwhichanindependentevaluator
examinessoftwareartifactsforproblems,hasbeenanengi-
neeringbestpracticeforover35years[9,10].Whileeﬀective
inidentifyingdefects,therigidityoftraditionalformalre-
viewpracticeshasbeenshowntolimitadoptionandreview
eﬃciency[12,29].Incontrast,contemporaryormodern
peerreviewencompassesaseriesoflessrigidpractices[6,
22].Theselightweightpracticesallowpeerreviewtobe
adaptedtoﬁt theneedsofthedevelopmentteam.Forexam-
ple,peerreviewiswidelypracticedonopensourcesoftware
(OSS)projects.Rigbyetal.[23]describedaminimalistOSS
processthateﬃcientlyﬁt thedevelopment team.However,
therewasalackoftraceabilityandtoolstosupportreview
thatmadeitdiﬃcult toexternallymonitortheprogressand
qualityofanOSSsystem.Despitealargebodyofresearch
on peerreviewinthesoftwareengineeringliterature,little
workfocusesoncontemporarypeerreviewinsoftwareﬁrms.
Therearepractitionerreports,but theseareexperiential
[20]orbiasedbyacommercialinterestinthereviewtool
beingexamined[6].Todate,practitionershavedriventhe
developmentofcontemporarypeerreviewandthetoolsthat
supportit[26,6].Theproliferationofreviewingtools(e.g.,
CodeCollaborator,ReviewBoard,Gerrit,Crucible)andthe
growingnumberofcompaniesusinglightweightreviewin-
dicatessuccessintermsofadoption(e.g.,Google,Cisco,
Microsoft),butthereisnosystematicexaminationofthe
eﬃcacyofcontemporarypeerreviewinsoftwareﬁrms.
Wepositthatcontemporarypeerreview(reviewpracticed
todaybymanycommercialandOSS projects)evolvedfrom
themoretraditionalpracticeofformal inspectionsofadecade
ormoreago.Inthispaper,wepresentanexplorationof
aspectsofcontemporarypeerreviewinsoftwareprojects
thatspanvaryingdomains,organizations,and development
processesinanattempt to aggregateandsynthesizemore
generalresults.Ourprimaryconjectureisthatifthepeer
reviewpracticesandcharacteristicsinmultipledisparate
projects(SeeTable2)havebecomesimilarastheyhave
naturallyororganicallyevolved,thensuchcharacteristics
maybeindicativeofconvergentpracticesthatrepresent
generallysuccessfulandeﬃcientmethodsofreview.Assuch,
thesecan beprescriptiveto otherprojectschoosingto add
peerreviewtotheirdevelopmentprocess.
Ouroverarchingresearchquestionishowdotheparame-
tersofpeerreviewdiﬀerinmultipledisparateprojects?We
operationalizethisquestionforeach parameterofreview:Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a
fee. Request permissions from Permissions@acm.org.
Copyright is held by the author/owner(s). Publication rights licensed to ACM.
ESEC/FSE’13 , August 18–26, 2013, Saint Petersburg, Russia
ACM 978-1-4503-2237-9/13/08
http://dx.doi.org/10.1145/2491411.2491444
202Table1:Projectdatasets:Thetimeperiodwe ex-
aminedinyearsandthenumberofreviews
Project PeriodYearsReviews
Lucent 1994–1995 1 .5 88
Apache 1996–2005 9 .8 5.9K
Subversion 2003–2008 5 .6 4.9K
Linux 2005–2008 3 .528K
FreeBSD 1995–2006 12 .0 47 K
KDE 2002–2008 5 .6 23 K
Gnome 2002–2007 5 .88K
AMD 2008–2011 3 .2 7 K
MSBing 2010–2013 3 .7 102 K
MS SqlServer2011–2013 2 .8 80 K
MSOﬃce2013 2011–2013 2 .7 96K
Android 2008–2013 4 .016K
ChromeOS 2011–2013 2.1K39K
1.Whatpeerreviewprocess(e.g.,Faganinspectionvs
Commit-then-review)doestheprojectuse?
2.Howlongdoreviewstakeand howoftenare reviews
performed?
3.Whatisthesizeofartifactunderreview?
4.Howmanypeopleareinvolvedinreview?
5.Howeﬀectiveisreviewintermsofproblemsdiscussed?
6.Doesreviewspreadknowledgeaboutthesystemacross
thedevelopmentteam?
Withthe exceptionofthelastquestion,theseparameters
ofreviewhavebeenstudiedinmanyexperimentsoverthe
last35years[9,19,24].Ourcontributionistocomparea
largediversesetofprojectsontheseparameters.
Thispaperisorganizedasfollows.In Section2,weprovide
abriefoverviewofthesoftwarepeerreviewliteratureand
describethereviewpracticesoftheprojectswestudyin
thispaper.In Section3,wedescribethedatathatwemine
andourmultiplecasestudymethodology.In Section4,
wepresentourcasestudyﬁndingsand describeconvergent
and divergentpractices.In Section5,weprovideaﬁrst
measurementoftheimpactofpeerreviewonknowledge
sharinginadevelopmentteam.Whilewediscussthreatsto
validitythroughout thepaper,weprovideafullerdiscussion
ofthemin Section6.In Section7,weconcludethepaper.
2.BACKGR OUND AND PROJECT INFOR-
MATION
Inthissectionweintroducethreetypesofpeer review:
traditionalinspection,OSSemail-basedpeerreview,and
lightweight toolsupportedreview.Wealsodescribethe
projectsand datawehaveforeachreviewtype.Thenovel
datainthispapercomesfromAdvancedMicroDevices
(AMD),Microsoft,andGoogle-led projects.Table2isin-
tendedtoshowthetimeperiodsandsizeofdatasetwehave
foreach project,andisnotintendedforcomparisonsamong
projects.Intheremainderofthispaper,wenormalizeand
converttherawdatatoperformmeaningfulcomparisons.2.1 Softwar eInspection
Softwareinspectionsarethemostformaltypeofreview.
Theyareconductedafterasoftwareartifactmeetspredeﬁned
exitcriteria(e.g.,aparticularrequirementisimplemented).
Theprocess,originallydeﬁned byFagan[9], involvessome
variationofthefollowingsteps:planning,overview,prepa-
ration, inspection,reworking,andfollow-up.Intheﬁrst
threesteps,theauthorcreatesaninspection package(i.e.,
determineswhatistobeinspected),rolesareassigned(e.g.,
moderator),meetingsarescheduled,andtheinspectorsex-
aminetheinspection package.Theinspectionisconducted,
and defectsarerecordedbutnotﬁxed.Intheﬁnalsteps,
theauthorﬁxesthedefectsandthemediatorensuresthat
theﬁxesareappropriate.Althoughtherearemany varia-
tionsonformal inspections,“theirsimilaritiesoutweightheir
diﬀerences”[31].
Comparisondata:WeusedatathatPorteretal.collected
ininspectionexperimentsatLucent [19]tocompareour
ﬁndingsforcontemporaryreviewwithtraditionalsoftware
inspection.Theirstudywasconductedinasemi-controlled
industrialsetting.Eachconditionintheirstudywasde-
signedtoemulateaparticularvariationininspection process.
However,theyfoundthatvariationininspection processes
accountedforverylittlevariationinthenumberofdefects
found duringreview.Peopleand productmeasures,suchas
theexpertiseofthereview,accountedformuchmoreofthe
variance.
2.2 Open Sour ceSoftwar ePeerReview
PeerreviewisanaturalwayforOSS developers,who
rarelymeetinperson,toensurethat thecommunityagrees
onwhatconstitutesa goodcodecontribution.Mostlarge,
successfulOSS projects seepeerreviewasoneoftheirmost
importantqualityassurancepractices[23,18,1].OnOSS
projects,areviewbeginswithadevelopercreating apatch.
Apatchisadevelopmentartifact,usuallycode,that the
developerfeelswilladdvaluetotheproject.Althoughthe
levelofformalityofthereviewprocessesvariesamongOSS
projects,thegeneralstepsareconsistentacrossmostprojects:
1)theauthorsubmitsacontributionbyemailingittothe
developermailinglistorpostingtothebug orreviewtracking
system,2)oneormorepeoplereviewthecontribution,3)itis
modiﬁed untilitreachesthestandardsofthecommunity,and
4)itiscommittedtothecodebase.Manycontributionsare
ignoredorrejectedand nevermakeitintothecodebase[3].
Thisstyleofreviewiscalledreview-then-commit (RTC).In
contrast toRTC,someprojectsallowtrusted developersto
commitcontributions(i.e.addtheircontributionstothe
sharedcoderepository)beforetheyarereviewed.Themain
orcoredevelopersfortheprojectarethenexpectedtoreview
allcommits.Thisstyleofreviewiscalledcommit-then-
review(CTR).AllprojectsuseRTC,butsomealsouseCTR
depending onthestatusofthecommitterandthenatureof
thepatch[23].
Comparisondata:Wehavedatafromsixlarge,successful
OSS projects(whichwewillrefertoasthe“OSS projects”in
thispaper1):theApachehttpdserver,theSubversionversion
controlsystem,theLinuxoperatingsystem,theFreeBSD
1AlthoughChromeandAndroid haveopensourcelicenses,
theyareledbyGoogle.203operatingsystem,KDEdesktopenvironment,andGnome
desktopenvironment.Theresultsfromtheseprojectshave
beenpublishedbyRigbyetal.[24,25]andareusedhere
only forcomparison purposes.
2.3 PeerReview atMicr osoft
Microsofthas slowlybeenevolvingitscodereviewprocess.
WhilecodereviewhasbeenanintegralpracticeatMicrosoft
formany years,themethod usedhasshiftedfromsending
patchesand discussionthemonlargeemail liststousing
centralizedtools.
Afewyearsago,Microsoftdevelopedaninternaltool,
CodeFlow,to aidinthereviewprocess.Inmanyprojects
(includingtheMicrosoftprojectsthatweexaminedforthis
study),codereviewisprimarilyaccomplishedviaCodeFlow
andoccursonceadeveloperhascompletedachange,but
priortocheckinintotheversioncontrolsystem.Adeveloper
willcreateareviewbyindicatingwhichchangedﬁlesshould
beincluded,providing adescriptionofthechange(similarto
acommitmessage),andspecifyingwhoshould beincluded
inthereview.Thoseincludedreceiveemailnotiﬁcations
andthenopenthereviewtoolwhich displaysthechangesto
theﬁlesandallowsthereviewerstoannotatethechanges
withtheirowncommentsandquestions.Theauthorcan
respondtothecommentswithinthereviewandcanalso
submitanewsetofchangesthataddressesissuesthat the
reviewershavebroughtup.Onceareviewerissatisﬁedwith
thechanges,hecan“signoﬀ” onthereviewinCodeFlow.
Whilemanyteamshavepoliciesregardingcodereviewsign
oﬀ,thereisnoexplicitconnection betweenthereviewsystem
andversioncontrolthatdisablescheckin untilareviewhas
beensignedoﬀ.Teams’reviewpolicymay varyinmany
ways.Forexample,somerequirejustonereviewertosign
oﬀwhileothersrequiremore;somespecifywhoshouldsign
oﬀforchangesin diﬀerentcomponentsandsomeleaveitup
tothedeveloper;etc.Formoredetails,wereferthereader
toanearlierempiricalstudy[2] inwhichweinvestigated
thepurposesforcodereview(e.g.,ﬁndingdefects,sharing
knowledge)alongwiththeactualoutcomes(e.g.,creating
awarenessandgainingcodeunderstanding)atMicrosoft.
NewData:Inthispaper,wepresenttheresultsofanalyz-
ingreviewdatadrawnfromthreelargeprojectsatMicrosoft
thatuseCodeFlowastheprimarymechanismforcodereview
andthatdiﬀerintheirdomainand developmentmethodol-
ogy–Bing,MicrosoftOﬃce2013,andMicrosoftSQLServer.
BingisanInternetsearchengine;itiscontinuouslybeing
developedand deployedand undergoesconstantdevelopment.
Oﬃceisasuiteofbusinessapplicationsthatshipsasaboxed
productandfollowsamoretraditionalprocesswith phases
forplanning,implementation,stabilization,andshipping.
SQLServerisadatabasemanagementand businessintelli-
genceapplicationthatfollowsadevelopmentcyclesimilar
toOﬃce.Wepresentdetailsofdata gatheringforthese
projectsinsubsection3.1.
2.4 Google-based Gerrit PeerReview
WhentheAndroid projectwasreleasedasOSS,theGoogle
Engineersworking onAndroidwantedtocontinueusingthe
internalMondriancodereviewtoolusedatGoogle[28].
GerritisanOSS,gitspeciﬁcimplementationofthecode
reviewpracticesusedinternallyatGoogle,createdbyGoogleEngineers[11].Gerritcentralizesgitacting asabarrier
betweenadeveloper’sprivaterepositoryandtheshared
centralizedrepository.Developersmakelocalchangesin
theirprivategitrepositoriesandthensubmit thesechanges
forreview.ReviewersmakecommentsviatheGerritweb
interface.Forachangetobemergedintothecentralized
sourcetree,itmustbeapprovedandveriﬁedbyanother
developer.Thereviewprocesshasthefollowingstages:
1.”Veriﬁed”-Beforeareviewbeings,someonemustverify
that thechangemergeswiththecurrentmasterbranch
and doesnotbreakthebuild.Inmanycases,thisstep
isdoneautomatically.
2.”Approved”-Whileanyonecancommentonthechange,
someonewithappropriateprivilegesandexpertisemust
approvethechange.
3.”Submitted/Merged”-Oncethechangehasbeenap-
proveditismergedintoGoogle’smasterbranchso
thatotherdeveloperscanget thelatestversionofthe
system.
NewData:Inthispaper,wepresentresultsfromtwo
Google-led,OSS projectsthatusetheGerritpeerreviewtool:
AndroidandChromiumOS.Androidisanoperatingsystem
developedformobileandtabletdevices.ItisOpenSource
Softwareandwasinitiated bya groupofcompaniesknown
asOpenHandsetAlliance,whichisledbyGoogle.2Google
ChromiumOS,referredtoasChrome,isanoperatingsystem
whichrunsonlywebappsandrevolvesaroundtheChromium
webbrowser.3Wepresentdetailsofdata gatheringforthese
projectsinsubsection3.1.
2.5 AMD andCodeCollaborator
Ratcliﬀe[20]presentedapractitionersreportontheadop-
tionofaCodeCollaboratorbasedpeerreviewpracticeonan
internalAMDproject,whichservedasthemodelforother
projectsatAMD.ThepracticeusedatAMDinvolvesthe
followingsteps:1) theauthoruploadsthesoftwareartifacts
forreviewinthewebinterface,2)reviewersareassignedto
thereview,3)areviewdiscussionoccursand problemsare
ﬁxed,4)onceareviewisapproveditiscommitted.The
CodeCollaboratortoolallowsforassignmentofrulesandthe
speciﬁcationandenforcementofbusinessrules(e.g.,areview
mustbeapprovedby2reviewersbeforeitcan becommitted).
Whileasynchronousdiscussioncanoccur,achatinterface
canalsobeused.Cohen,thefounderofthecompanythat
sellsCodeCollaborator,performedadetailedevaluationof
thetoolatCisco[6].
Newdata:Ratcliﬀe’spractitionersreportwasmainlyqual-
itative.Inthiswork,wepresent thequantitativeresultsfrom
theuseofCodeCollaboratoratAMD.TheAMDdataset
islimited,soweindicatebelow whenweareunablepresent
resultsforAMD.
2.6 Contemporary PeerReview Process
Comparingtheabovereviewprocesses,weﬁndthatcon-
temporarypeerreviewischaracterizedbybeinglightweight
2http://source.android.com/
3http://www.chromium.org/chromium-os204andoccurringbeforethecodeisaddedto aversioncontrol
repositorythatmanydevelopersdepend upon(e.g.,themas-
terbranch).Thisprocesscontrastssharplywithtraditional
softwareinspectionwherelargecompletedartifactsare re-
viewedinco-locatedmeetingwithrigidlydeﬁnedgoalsand
participantroles.ContemporaryOSSreviewislightweight
and ﬁtsthedevelopment team,butwhenitisconducted
onamailinglistitisdiﬃculttotrack.SomeOSS projects
andallthesoftwareﬁrmsweexamineuseareviewtool,
whichmakestheprocesstraceablethought thecollection
ofreviewmetrics.Contemporaryreviewsaretypicallycon-
ductedasynchronouslyandmeasuresofreviewarerecorded
automatically.
Convergent Practice1:Contemporarypeerreviewfol-
lowsalightweight,ﬂexibleprocess
Ingeneral,contemporaryreviewinvolvesthefollowing
steps.
1.Theauthorcreatesachangeandsubmitsitforreview.
2.Developersdiscussthechangeandsuggestﬁxes.The
changecan bere-submittedmultipletimestodealwith
thesuggestedchanges.
3.Oneormorereviewersapprovethechangeanditis
addedtothe“main”versioncontrolrepository.The
changemayalsoberejected.
3.METHODOLOGY AND DA TA
WeuseYin’smultiplecasesstudymethodology[32].Case
studyﬁndings‘generalize’oraretransferablethroughanalyt-
icalgeneralizations.Unlikestatisticalgeneralization,which
derivessamplesfromandgeneralizesto adeﬁned popula-
tion,analyticgeneralizationrequiresresearcherstodevelop
atheoryorframeworkofﬁndingsrelatedto aparticular
phenomenon.Weusetheoreticalsamplingtoselectadi-
versesetofcasesandthencontrastourﬁndingsdeveloping
aframeworkthatdescribestheconvergentand divergent
practicesofcontemporarypeerreview.
Webegan bycollectingdata onMicrosoftreviewpractices
andweresurprisedtoseeconvergenceswiththepracticesob-
servedbyRigbyonOSS projects[23].Thesepracticestended
tocoincidewiththoseseenatAMD[20]andCisco[6].We
collected data ontheGoogle-ledOSS projects,ChromiumOS
andAndroid,tounderstandthepracticesofhybridprojects.
Wealsohavedata onthetraditional inspection practicesat
Lucent[19]thatweuseforcomparison purposes.
Wequantifyhowlightweight,tool-supportedreviewis
conducted.Since eachcasestudyhasdiﬀerentdatapoints
andmeasures,afurthercontributionofthisworkisthe
conversionofrawandsummarydatafrompastandcurrent
casestoreportcomparablemeasures.Wecontributeauniﬁed
setofﬁndingsacrossalarge,diversesampleofprojects.
Inthis section,wegiveanoverviewofthedatawehavefor
each project.Ineachsubsequentsection,wediscussin detail
thepertinentdataandmeasures.Wealsodiscusslimitations
inourdata andconstructvalidityissues.3.1 Data Extraction
Thedataextractionforthefollowingprojectsisdescribed
inotherwork:Lucent [19],OSS projects[23],andAMD[20].
Theformertwodatasetsareusedforcomparison purposes,
whiletheAMDdatahadnotbeenquantitativelyreportedin
previouswork.In previouswork,wedescribedtheextraction
processandresultingdataforGoogleChromeandAndroid;
thedataisalsoavailableforotherresearchers[17].This
workdid notinvolveanalysisofthedata.Intheremainder
ofthissection,wediscusswhatconstitutesareviewforeach
projectand brieﬂydescribehow we extractedpeerreview
data.
Microsoft:TheMicrosoftdataforthisstudywascollected
fromtheCodeFlowtool.Thistoolstoresalldataregarding
codereviewsinacentral location.Webuiltaservicetomine
theinformationfromthislocationandkeepadatabaseup
todatefortoolstoleverageandforempiricalanalysis.For
eachreview,werecordinformationincludingwhocreatedthe
review,whatﬁlesweremodiﬁed,howmanysetsofchanges
weresubmitted,thecommentsthatreviewersadded,and
whosignedoﬀ.
Onediﬃcultywiththisdataisknowingwhenareview
iscomplete.Thereareanumberofstatesthatareview
can bein,oneofwhichis“Closed”.However,tobeinthe
“Closed”state,someonemustexplicitlyset thereviewtothat
state.Weobservedthatinpractice,adevelopermaycheck
in hischangesoncereviewershadsignedoﬀwithoutﬁrst
changingthereviewto“Closed”.Inothercases,therewas
evidencethatamemberofaprojectclosedreviewsasaform
ofmaintenance(onepersonclosedthousandsofreviewsina
matterofminutes).Todealwiththis,weusetheheuristic
thatareviewisconsideredcompletedatthetimeofthelast
activitybyaparticipantinthereview(i.e.thedateofthe
lastcommentorthelastsignoﬀ,whicheverislater).For
allthecasesstudiesinthiswork,reviewswith nocomments
orsignoﬀswereexcludedfromthedatasetasnoreview
discussionoccurs.
GoogleChromeandAndroid:Weconsiderreviewsinthe
mergedandabandonedstates,openreviewsarenotconsid-
eredinthiswork.Reviewsmustalsohaveonecommentfrom
ahumanreviewerwhoisnot theauthor(Veriﬁcationsby
botsareremoved).Tocollectpeer reviewdatafromthese
projects,wereverse engineeredtheGerritJSON APIand
queriedtheGerritserversfordataregardingeachreviewfor
both projects,gatheringinformationsuchastheauthor’s
andreviewers’activity,ﬁleschanged,commentsmade,and
datesofsubmissionandcompletion.Westoredthisinforma-
tiononpeerreviewsinadatabaseforfurtheranalysis.The
extracteddataand detailsofourtechniqueareavailableto
otherresearchers[17].
AMD:Weattainedasummaryofthedatadumpfromthe
CodeCollaboratortool[20].Unfortunately,thisdatasetdoes
nothavealltheparametersofreviewwewishtomeasure,
suchasthenumberofcommentsperreview.Inthisdata
set,weonlyincludereviewdiscussionsthathaveatleastone
reviewer.
Lucent:Siyattendedinspectionmeetingsandcollected
self-reportdatafromreviewersonacompilerprojectat
Lucent [19].Therolesand numberofparticipantswere205speciﬁedinadvance.Sincethisiscomparisondata,we
discussdiﬀerences,butdonotpresent thisdatain ﬁgures.
OSSproject:Rigbyetal.’sworkconsideredsixOSSproject,
Apache,Subversion,Linux,FreeBSD,KDE,andGnome[24,
25,23].Thereviewdatawasextractedfromdevelopermail-
inglists.Forareviewtobeconsideredvalidithadtocontain
thefollowing:1)achangeor‘diﬀ’and2)oneormoreemails
fromreviewers(i.e.not theauthorofthechange).Both
acceptedandrejectedchangesthatwerereviewedareinthe
dataset.LiketheLucentdata,wedonotreportthisdata
inourﬁgures.
PlottingtheData:Weusetwotypesofplots:beanplots
and boxplots.Beanplotsshowthedistributiondensityfor
multiplesamplesalongthey-axis(ratherthanmorecom-
monlyalongthex-axis)toenableeasy visualcomparison
andinthisworkcontainahorizontal linethatrepresents
themedian[13].Beanplotsarebestforalargerangeof
non-normaldata astheyshowthe entiredistribution(they
essentiallyshowthefulldistribution drawnvertically,and
showwhethertherearepeaksandvalleysinadistribution)
whileboxplotsarebetterforsmallerranges.Whenwehave
countdatathatishighlyconcentrated,weuseaboxplot.
Foralltheboxplotsinthiswork,thebottomandtopof
theboxrepresent theﬁrstandthirdquartiles,respectively.
Eachwhiskerextends1.5timestheinterquartilerange.The
medianisrepresentedbytheboldlineinsidethebox.Since
ourdataarenotnormallydistributed,regardlessofthestyle
ofplot,wereportand discussmedianvalues.
4.MUL TIPLE CASE STUD YFINDINGS
Inthis section,wepresentourconvergentand divergent
ﬁndingsinthecontextofiterativedevelopment,reviewers
selection practices,reviewdiscussionsand defects,andknowl-
edgesharingthroughreview.Foreach ﬁnding,weplaceitin
thecontextoftheSoftwareEngineeringliteratureonpeer
review,summarizeitina “ConvergentPractice”box,and
thendiscusstheevidencethatwehaveforeach practice.
4.1 Iterati veDevelopment
Theconceptofiterativedevelopmentisnotnewandcan
betraced backtothemanysuccessfulprojectsintheearly
daysofsoftwaredevelopment[14].However,progressive
generationsofsoftwaredevelopershaveworkedinshorter
andshorterintervals.Forexample,“continuousbuilds”[8]
and“releaseearly,releaseoften”[21].Peerreviewisno
exception.
Anoriginalgoalofsoftwareinspectionwastoﬁndsoftware
defectsbyexposing artifactstocriticismearlyinthedevelop-
mentcycle.Forexample,Faganinspectionintroducedearly
andregularcheckpoints(e.g.,afterﬁnishing amajorcompo-
nent) thatwould ﬁnd defectsbeforethesoftware’srelease.
However,thetimefromwhenthereviewstartedtowhenthe
discussionended(i.e.thereviewinterval)wasontheorderof
weeks[9].In1998,Porter[19]reportedinspectionintervals
atLucent tohaveamedianof10days.OSS projectslike
ApacheandLinuxhavereviewintervalsontheorderofa
fewhoursto aday[23].Android Chrome AMD Bing Office SQL
Projectsinterval in da ys (log)
1 min 1 hr .25 1 7 30 365
Figure1:FirstResponseonleft(wedo nothave
ﬁrstresponsedataforAMD)andFull intervalon
right
Convergent Practice2:Reviewshappenearly(beforea
changeiscommitted),quickly,andfrequently
AMD,Microsoft,andtheGoogle-ledprojectsexemplify
theconvergentpracticeoffrequentreviews,Figure2,that
happenquickly,Figure1.Thereviewsarealwaysdone
early(i.e.beforethecodeischeckedintotheversioncontrol
system).
AMD:AMDhasshortreviewintervals,withthemedian
reviewtaking 17.5hours.Thenumberofreviewspermonth
isalsohighandincreasesfromafewreviewspermonthwhen
thetooland practicewasintroduced,to over500reviews
permonth.
Microsoft:Bing,SQL,andOﬃcealsoshowshortintervals
forreviewswithmediancompletiontimesof14.7,19.8,and
18.9hoursrespectively.Intermsofreviewspermonth,all
threeprojectsareveryactive,butshowdiﬀerent trends.
SQLhasamedianof3739reviewspermonthandisfairly
consistentmonthtomonth.Incontrast,Binghasamedian
of2290,buthasshownasteadyincreaseovertimesinceits
initialadoptionofCodeFlow.Oﬃcehasthehighestmedian
at4384,anditfollowsatypicalreleasecyclewithaninitial
rampupofreviewsandafall-oﬀnearrelease.
GoogleChromeandAndroid:Themedianfrequencyis
1576and310forChromeandAndroid,respectively.The
mediancompletiontimeis15.7and20.8hours,forChrome
andAndroid,respectively.
ProjectComparisons:Thereviewinterval,whichison
theorderofhoursandwithamedianaroundaday,shows2061 10 100 1000 10000
Android Chrome AMD Bing Office SQL
ProjectsReviews per month (log)
Figure2:Thenumberofreviewspermonth
remarkableconsistencyacrossallprojects.Figure1 also
showstheamountoftimeittakesfortheﬁrstresponseto
areview.Wecanseeforallprojectsthatmostreviewsare
pickedupwithanfewhours, indicatingthatreviewersare
regularlywatching and performingreview.
Thenumberofreviewspermonth,orthereviewfrequency,
isveryhighincomparisontotraditionalinspection practices,
but tendstovarywith thestage,developmentstyle,and
sizeoftheproject(adivergentﬁnding).InFigure2,wecan
seethreedistinct typesofprojects:adoption(e.g.,Bing),
cyclic(e.g.,Oﬃce),andstable(e.g.,Chrome).Thelongtails
ineach beanplotshowthatadoptiontookplace,andwith
AMDandBingtheamountofreviewisstill increasingwith
eachmonth.Thistrendcan beseeninFigure3,which plots
Bingdata asatimeseries.Incontrasttothismonotonic
trend,cyclicprojects,likeAndroid,FreeBSD,Oﬃceshowan
irregularconeshape,withgradualﬂuctuationsintheamount
ofdevelopmentandreview(SeeOﬃceinFigure2).Finally,
Chromeand SQLshowarelativelystablenumberofreviews.
LinuxandKDEexhibitsimilartrends.
Convergent Practice3:Changesizesaresmall
Having ashortintervalcannotbeachievedwithoutchanges
to otheraspectsofsoftwaredevelopment.Bycreatingsmaller
changes,developerscanworkinshorterintervals.Forexam-
ple,Mockusetal.notedthatApacheandMozillahadmuch
smallerchangesizesthantheindustrialprojectstheyused
forcomparison,butdidnotunderstandwhy[15,22].On
theOSS projects studiedbyRigbyetal.,themedianchange
onOSS projectsvariesfrom11to 32lineschanged.They
arguedthat thesmallchangeonOSS projectsfacilitates
frequentreviewofsmall independentchanges.Reviews per Month
Bing
Office
MonthReviews
Figure3:NumberofreviewspermonthinBingand
Oﬃce.Wewererequestedtokeeprawnumbersand
datesconﬁdential,butthisplotshowsthetrendsin
codereviewasatoolisadopted(Bing)andoverthe
courseofareleasecycle(Oﬃce).
FromFigure4,bothAndroidandAMDhaveamedian
changesizeof44lines.Thismedianchangesizeislargerthan
Apache,25lines,andLinux,32lines,butmuchsmallerthan
Lucentwherethenumberofnon-commentlineschangedis
263lines.Bing,Oﬃce,SQL,andChromehavelargermedian
changesthantheotherprojectsexamined,butarestillmuch
smallerthanLucent.Forexample,Chrome’smedianchange
is78linesandincludes5ﬁles.However,forChrome,only
23%ofchangesarethesamesizeorlargerthanamedian
Lucentchange.Furthermore,thedistributionofchanges
onGoogle-ledandtheotherOSS projectareleftskewed
indicatingthat themajorityofchangesaresmall.Whilethe
distributionforthecommercialﬁrmsisalsoleftskewed,itis
almostlognormal.
4.2 Selecting Reviewers
Traditionally,developersareassignedtoreviewanartifact.
OnOSS projects,developersselectthechangesthat theyare
interestedinreviewing and noreviewsareassigned.Many
reviewtoolsallowforassignmentaswellasself-selection
incorporating apositivemixofbothtechniques[26,11,6].
Theself-selection usedinreviewtools isaccomplishedby
adding a group(e.g.,amailinglist) tothereviewerlist,then
individualsfromthisgroupcan ﬁndthereview[20,2].In
thissection,wediscusstheoptimalnumberofreviewersas
wellasdiﬀerentreviewerselectiontechniques.
Theoptimalnumberofinspectorsinvolvedinameeting
haslongbeencontentious(e.g.,[5,30]).Reviewsareex-
pensivebecausetheyrequirereviewerstoread,understand,
andcritiqueanartifact.Anyreductioninthenumberof
reviewersthatdoesnotleadto areductioninthenumber
ofdefectsfoundwillresultincostsavings.Buck[5]found
nodiﬀerenceinthenumberofdefectsfound byteamsof
three,four,and ﬁveindividuals.BisantandLyle[4]proposed
twopersoninspectionteamsthateliminatedthemoderator.
Inexaminingthesourcesofvariationin inspections,Porter207Android Chrome AMD Bing Office SQL
ProjectsLines changed (log)
5 15 45 150 1000 10000
Figure4:Churn:Linesaddedandremoved.Note:
wedo notshowproportionofchangesover10 000
lines.
etal.[19]foundthat tworeviewersdiscoveredasmanyde-
fectsasfourreviewers.Theconsensusseemstobethat two
inspectorsﬁndanoptimalnumberofdefects[27].InOSS
review,themedian numberofreviewersistwo;however,
sincethepatchandreviewdiscussionsarebroadcastto a
largegroupofstakeholders,thereisthepotentialtoinvolve
alargenumberofinterestedreviewersifnecessary[25].
Convergent Practice4:Tworeviewersﬁndanoptimal
numberofdefects
AtAMDthemedian numberofreviewsis2.Whilereview-
ersaretypicallyinvited,RatcliﬀedescribeshowCodeCollab-
oratorallowsinvitestobebroadcastto a groupofdevelop-
ers[20].HefurtherdescribeshowCodeCollaboratorsuggests
potentialreviewersbasedonwhohasworkedontheﬁlein
therecentpast.
ForGoogleChromeandAndroid,thereisamedianof
tworeviewers,seeFigure5.Gerritallowsdevelopersto
subscribetonotiﬁcationswhenareviewincludeschangesto
aparticularpartofthesystem[28].Reviewerscanalsobe
invitedwhentheauthorincludestheiremailaddressinthe
reviewsubmissionsenttoGerrit.
AtMicrosoftthemedian numberofreviewersinvitedto
eachreviewinBing,Oﬃce,and SQLrespectivelyare3,3,
and4.AsFigure5shows,themedian numberofpeoplethat
actuallytakepartinareview(otherthantheauthor)is2.
Interestingly,wefoundthattherewasonlyaminimalincrease
inthenumberofcommentsabout thechangewhenmore
reviewerswereactiveandtherewasnoincreaseinthenumber
ofchangesetssubmitted(i.e.,thesamenumberof“rounds
ofreviewing”).Wealsoinvestigated,bothqualitativelyandAndroid Chrome AMD Bing Office SQL1 2 5 10 20 50
ProjectsReviewers per re view (log)
Figure5:Tworeviewersinvolvedinreviewinthe
mediancase
quantitatively,reviewsthathadmanymorereviewersthan
themedianandfoundthat theauthororthereviewerswill
inviteadditionaldevelopersafteraroundofreviewinghas
takenplace.Thiscanbetheresultofadeveloperrealizing
thatsomeoneelseisbetterﬁt toexaminethechangeor
concludingthat thechangecarriesahighriskandshouldbe
reviewedby“moreeyes”.Thegeneralpracticeappearsto
involveinvitingthreetofourreviewersandthenlettingthe
reviewtakeitscoursewhichmayleadtoinvolving additional
participants.
4.3 Defects vsDiscussion
Therigidtimeconstraintsofsynchronousreviewmeet-
ingsforcedtraditional inspectionstofocusexclusivelyon
ﬁndingdefects,discussionsofothertopics,suchas solutions
tothedefect,werestrictlyforbidden[9].Inspection used
explicitroles,suchasreaderandsecretary,toensurethat
defectswereaccuratelyrecordedandthatdeveloperswere
notdistractedfromﬁndingsdefects[9].AtLucentthereisa
medianof3truedefectsfound perreview[19].Anadditional
4defectsperreviewwerefoundtobefalsepositives.Inspec-
tionsalsofoundalargenumberofsoftmaintenanceissues,
median13perreview,whichincludedcodingconventions,
andtheadditionofcomments.Thistypeofsoftmaintenance
codeimprovementswasalso observedatMicrosoftandin
OSSreview[2,25].Incontrasttosoftwareinspection,asyn-
chronousreviewshavelessrigidtimeconstraintsallowing
fordetaileddiscussionsofsoftwareartifacts.Forexample,
onOSS projects,thediscoveryofthedefectisnot thefocal
point.Instead developersdiscusspotentialdefectsandso-
lutionstothesedefects.Theseteamdiscussionsmeanthat
theauthorisnolongerisolatedfromhisorherpeerswhen208Android Chrome Bing Office SQLComments per re view (log)
1 2 5 15 45 150 1000
Figure6:Numberofcommentsperreview
ﬁxingthedefectsfound duringreview[25].Inthissection,
wealsoprovideproxymeasuresforthenumberofdefects
foundandshowthat theyarecomparabletothosefound
duringinspection.
Convergent Practice5:Reviewhaschangedfroma
defectﬁnding activitytoa group problemsolving activity
Examiningcontemporarypracticesinsoftwareﬁrms,we
ﬁndconvergencewithOSS:defectsarenotexplicitlyrecorded.
AMDusesCodeCollaborator,which hasaﬁeldtorecordthe
numberofdefectsfound;however,87%ofreviewshaveno
recordeddefects,andonly7%havetwo ormoredefectsfound.
Measuresofreviewactivityindicateamedianoftwopartic-
ipantsperreviewandqualitativeanalysisbyRatcliﬀe[20]
foundthatdiscussionsdidoccurbutfocusedon ﬁxingdefects
insteadofrecordingtheexistenceofadefect.Thedisconnect
betweenexplicitlyrecordeddefectsandactivityonareview
indicatesthatreviewersareexaminingthesystem,butthat
developersarenotrecordingthenumberofdefectsfound.
Microsoft’sCodeFlowreviewtoolprovidesfurtherevidence
–itdoesnotprovideawayfordeveloperstorecordthe
defectsfound duringreview.Thisdesign decisionresults
fromthewaythatcodereviewispracticedatMicrosoft.
Whenanauthorsubmitsachangeforreview,theauthor
andotherreviewershaveajointgoalofhelpingthecode
reachasatisfactorylevelbeforeitischeckedin.Wehave
observedthatreviewerswillcommentonstyle,adherence
toconventions,documentation,defects,missedcornercases,
andwillalsoaskquestionsabout thechanges[2] inaneﬀort
tohelptheauthormakethecodeacceptable.Itisunclear
whichoftheserepresentdefectsandwhich donot (e.g.wouldAndroid Chrome Bing Office SQL1 2 5 10 20 50
ProjectsNumber of resubmissions per re view (log)
Figure7:Numberofsubmissionsperreview
thecomment“Areyousureyoudon’tneedtocheckagainst
NULLhere?”beadefect?).Inaddition,recordingthedefects
found duringreviewwouldnotaid intheaforementionedgoal.
CodeFlowdoesprovidetheabilityforanauthortomark
anythreadofconversationwithastatusof“open”,“ignored”,
“resolved”,or“closed”enablingparticipantstotrackthe
variousdiscussionswithinthechanges.Forourpurposes,the
closestartifact to adefectisathreadofdiscussionthathas
beenmarkedasresolved,asaproblemfoundwithinthecode
wouldneedtoberesolvedbytheauthorpriortocheckin.
Thecaveatisthatareviewermightmakecommentsorask
questionsthatleadtodiscussionandareeventuallymarked
asresolved,but thatdon’trepresentadefectfoundorresult
inanycodebeingchanged.
OntheGoogleChromeandAndroidprojects,theGerrit
reviewtooldoesnotprovideanyﬁeldtoexplicitlyrecord
thenumberofdefectsfound.However,aswediscussedin
section2,reviewspassthroughthreestages:veriﬁedto
notbreakthebuild,reviewed,andmerged.Thegoalof
thesestagesisnot tosimplyidentifydefects,buttoremove
anydefectsbeforemergingthecodeintoacentral,shared
repository.AswecanseefromFigure6,thereisamedian
of4 and3commentsperreviewforChromeandAndroid
respectively–discussionoccursontheseprojectsatsimilar
levelsto otherOSS projects.OntheIndustrialside,the
mediansarethesame,with4,3,and3commentsforBing,
Oﬃce,and SQLrespectively.
“Youcan’tcontr olwhat youcan’tmeasur e”[7]
Thecontemporarysoftwareprojectswestudieddonotrecord
thenumberofdefectsfound duringreview, in partbecauseit
distractsdevelopersfromtheirprimarytaskofimmediately
ﬁxingthedefectsfoundinreview.However,without this209Table2:Descriptivestatisticsforthenumberof
comments,threadsofdiscussionandthreadsmarked
asresolvedinBing,Oﬃce,andSQL.
Project CommentsThreadsResolved
1stQuartile 2 1 0
BingMedian 4 2 0
3rdQuartile 9 6 1
1stQuartile 2 1 0
OﬃceMedian 3 2 0
3rdQuartile 8 5 1
1stQuartile 2 1 0
SQL Median 3 3 0
3rdQuartile 8 7 2
measurecansoftwareprojectsimprovetheirprocessand
productinthelongterm?Aretherealternativemeasuresof
revieweﬀectiveness?
Wesuggestthreealternativemeasuresthatwhentaken
togetherprovideanapproximationofrevieweﬀectiveness.
First,thenumberofcommentsduringreviewisan upper
boundonthenumberofdefectsfound perreview(SeeFig-
ure6).Theunderlying assumptionisthateachcomment
representsadistinctdefect.Thisassumptionisoftenin-
validasmanycommentswillberelatedtothediscussionof
asingledefect.Inourmanualanalyses,wefoundthatit
wasextremelyrareforacomment toincludemorethanone
substantivedefect;however,trivialformattingissueswere
oftenreportedinasinglecomment.Second,abetteresti-
mateisthenumberofcomment threads(SeeTable2).The
assumptionisthateachthreadcontainsasingledefect,how-
ever,sometimesacommentthreadwillcontaindiscussions
ofmultiplerelateddefects,othertimesitwillcontainfalse
positives,suchasdeveloperquestionsthatdonotuncover
adefect.Third,alowerboundonthenumberofdefects
foundinareviewisthenumberofartifactresubmissions
(SeeFigure7).Fornon-trivialdefects,arevisedartifactmay
besubmittedforre-review.However,arevisionwillcover
alltheﬁxeddefectsidentiﬁedduring areviewsession.Since
CodeFlowistheonlytoolthat tracksthreadsofconversation,
wereportthesummarystatisticsofthenumberofcomments,
threads,andthreadsmarkedasresolvedinTable2.
Thesesmeasuresprovidenon-intrusivetechniques(i.e.the
dataisimplicitlyrecordedduringtheprimaryactivityof
discussingthesoftware)to approximaterevieweﬀectiveness.
Wedonotwant tomakestrongclaimsaboutrevieweﬀec-
tivenessbecausethesemeasuresareproxiesofthenumber
ofdefectsfoundandartifactsizestendtobesmallerthan
intraditional inspection.However,wefeelthat thelevelof
discussion duringreviewand patchresubmissions suggests
thatcontemporaryreviewdoesﬁnd defectsatacomparable
leveltotraditionalinspection.Thesemeasuresandreview
practicesoncontemporaryprojectsraisealargerphilosophi-
calquestionthatdeservesfuturework:isitmoreimportant
tohaveadiscussionabout thesystemortoﬁndandreport
defects?5.SHARING KNO WLEDGE THR OUGH
REVIEW
Thenumberofdefectsfound duringreviewisknowntobe
alimitedmeasureofrevieweﬀectivenessbecauseitignores
manyoftheotherbeneﬁtsofreview,suchasthesharing of
knowledgeamongdevelopers[12].Someofthebeneﬁtsof
spreadingknowledgeacrossthedevelopmentteaminclude
havingco-developerswhocan doeachother’sworkifa
developerleavesaprojectandinvolvingnewdevelopersin
reviewstofamiliarizethemwithaproject’scodebase.While
qualitative evidencefrompractitionersindicatesthatreview
doesindeedspreadknowledgeacrossthedevelopmentteam[2,
20],weareunawareofanyempiricalstudiesthatmeasure
thisphenomenon.
Toprovideapreliminarymeasurementoftheknowledge
spreadingeﬀectofpeerreview,weextendtheexpertise
measuredeveloped byMockusandHerbsleb[16].Theymea-
suredthenumberofﬁlesadeveloperhasmodiﬁed(submitted
changesto).Wealsomeasurethenumberofﬁlesadeveloper
hasreviewedandthetotalnumberofﬁlesheknowsabout
(submitted∪reviewed).Figure8showthat thenumberof
ﬁlesadeveloperhasmodiﬁed(ontheleft)comparedtothe
totalnumberofﬁlesheorsheknowsabout (ontheright).4
Forexample, inthemediancase,aGoogleChromedeveloper
submitschangesto24distinctﬁles,reviews38distinctﬁles,
andknowsaboutatotalof43distinctﬁles.Withoutreview,
inthemediancase,aChromedeveloperwouldknowabout
19fewerﬁles,adecreaseof44%.Similarly, inthemedian
caseforBing,Oﬃce,and SQL,reviewincreasesthenumber
ofﬁlesadeveloperknowsaboutby100,122,and150%,
respectively.
BothGoogleChromeandAndroidappeartohavealarger
numberofdeveloperswhohavesubmittedto andreviewed
fewﬁles.OSS projectareknowntohave,whatoneinter-
vieweecalled“drive-by”developers,whosubmitasingle
change[25](e.g.,abugﬁxthateﬀectsthedeveloper).Fig-
ure8showsthatthiseﬀectisespeciallypronouncedon
theAndroidprojectwhere54%ofdevelopershavemodiﬁed
fewerthan ﬁveﬁles.Theincreaseinthenumberofﬁles seen
throughreviewisalsolowerforAndroid,a 66%increasein
themediancase.Ifwe excludedeveloperswhohavemodiﬁed
ﬁveorfewerﬁles,weseethemediannumberofﬁlesmodiﬁed
jumpsfrom6to16 andthetotalnumberofﬁlesgoesfrom
10to 25.
Ourmeasureofknowledgesharingthough peerreview
hasshownasubstantial increaseinthenumberofﬁlesa
developerknowsaboutexclusivelybyconductingreviews.
Thismeasuredeservesfuturestudy.Enhancementstothe
measurecouldalsobeusedto gaugethediversityofthe
knowledgeofdevelopersassignedto areview.Ifareview
hasdevelopersfromdiversepartsofthesystemreviewing
thecodeand discussingit, itislesslikelythattherewillbe
downstreamintegrationproblems.
4Weconservativelyexcludesubmissionsandreviewsthat
containmorethan10ﬁles.210Android Chrome Bing Office SQL
ProjectsFiles Seen (log)
5 15 45 150 1000 10000
Figure8:Onleft,thenumberofﬁlessubmittedfor
review,Onright,thetotalnumberofﬁleseither
submittedforrevieworreviewed
6.THREA TSTOVALIDITY
Westudiedalarge,diversesampleofprojects;however,
each projecthasdiﬀerenttools,processes, incentives,etc,
sothedatawecollectedisnotascontrolledandfreefrom
confoundsasitwould beinanexperimentalsetting.Wehave
attemptedtocleanandreportthedatausingsimilarmeasures
andmethodsand havediscussedlimitationsthroughout the
paper.
Whenaﬁndingwasunusualwewouldreadtheassociated
anomalousreviewsand discussthemwith developers.For
example,wehaveremovedreviewsthatreceivedno activity
fromtheMicrosoftdatasets(reviewsthathad nocomments,
nosignoﬀs,andonlyonesubmittedchangeset).Upon
initiallyﬁndingtheseinourdataset,weinquiredofthe
developerswhocreatedthereviews.Theyindicatedthat
sometimesreviewsaremoreforawareness(e.g.,alerting
amanagerto afeaturebeingimplementedorshowing a
testercodethat theyshouldwriteatestfor)thanactually
reviewingthechangesandthat thecompletelackofactivity
representsreviewsthatareusedforawarenesspurposesonly.
Itispossiblethataportionofthesereviewswereactually
intendedasactualcodereviewsratherthanforawareness
andsimplyreceivedno attentionfromtheinvitedreviewers.
ThistypeofreviewhasalsobeenremovedfromtheGoogle-
led projectsandfromtheOSS projects studiedbyRigby
etal.[24]and usedforcomparison purposesinthispaper.
Insomecases,were-ranmeasuresonRigby’srawdatasets.
TheAMDandLucentdatasetspresentedfurtherdiﬃculties
becausewehavesummaryinsteadofrawdata.Wehave
convertedtherawdatatomakeitcomparablewiththeother
datawecollected;however,weareoftenmissingtherequired
data.7.CONCLUSION
Thevariationsintraditional,formalsoftwareinspections
werefoundtohavelittleimpactonitseﬀectivenessasa
peerreviewprocess,withproductand processfactorsbeing
betterpredictorsofthenumberofdefectsfoundinreview[19,
27].Furthermore,asWiegerspointsinhispracticalguideto
peerreview,the“similarities[informalinspection process]
outweightheirdiﬀerences”[31].
Contemporarypeerreviewrepresentsalightweight,“stripped-
down”versionofsoftwareinspectionthatremovestherigidity
oftheformalinspection processes,whileleavingtheeﬀective
defectﬁndingtechniqueofhaving anexpertpeerexamine
softwareartifactsbeforetheyareaddedtothesharedversion
controlrepository.
Contemporarypeerreviewhasevolvedfromtheneedsof
practitionersandthesepractitionershavedriventhedevel-
opmentofreviewtools[6].Thelargebodyofliteratureon
softwareinspection haslargelyignoredthesecontemporary
practices.Inthispaper,wehavepresentedﬁndingsonthe
peerreviewpracticesusedonthreeprojectsrepresentativeof
developmentatMicrosoft,AMDprojects,andtwoGoogle-
ledOSS projects.Wehavecomparedparametersofreview,
suchasreviewintervalandthenumberofcommentsinre-
viewdiscussions,ofthesesixprojectswiththedatafrom
Rigby’sstudyofsixOSS projects[23].Wealsousedata
frominspectionatLucentasacontrast.Wefoundthat
whilethereweresomeminordivergencesincontemporary
practice“theirsimilaritiesoutweighedtheirdiﬀerences,”i.e.
theﬁndingsconverged.
Theconvergentcontemporarypeerreviewpracticescan
can bedescribedasthefollowing.
1.Contemporaryreviewisperformedregularlyandquickly
justbeforethecodeiscommittedinsteadofwhena
largerworkproductiscompleteasininspection.
2.Contemporaryreviewusuallyinvolvestworeviewers.
However,thenumberofreviewersisnotﬁxedand
canvaryto accommodateotherfactors,suchasthe
complexityofachange.
3.Contemporaryreviewersprefersdiscussionand ﬁxing
codeoverreportingdefects.
4.Toolsupportedreviewprovidesthebeneﬁtsoftrace-
ability,whencomparedtoemailbasedreview,and
canrecordimplicitmeasures,whencomparedtotradi-
tionalinspection.Theriseinadoptionofreviewtools
providesanindicatorofsuccess.
Aﬁnalcontributionofthispaperwasanovelmeasureof
thedegreetowhichreviewsspreadsknowledgeacrossthe
development team.Thismeasureprovidesaquantiﬁcationof
knowledgespreadthathaspreviouslyonlyhadexperiential
support.Weﬁndthatreviewincreasesthenumberofdistinct
ﬁlesadeveloperknowsaboutby66%to 150%depending
ontheproject.Wefeelthatfutureworkisnecessaryto
determinewhetherlightweightmeasures,suchasthediversity
andamountofdiscussionduringreview,can beusedasnew
releasequalitymeasures.Insteadofcountingthenumber
ofdefectsfoundinamodule,amanagermightask,“have
developerswithsuﬃcientlydiversebackgroundsdiscussed
thisnewsectionofcodeenoughforittobereleased?”2118.REFERENCES
[1]J.AsundiandR.Jayant.Patchreviewprocessesin
opensourcesoftwaredevelopmentcommunities:A
comparativecasestudy.InHICSS:Proceedingsofthe
40thAnnualHawaiiInternationalConferenceon
SystemSciences,page10,2007.
[2]A.BacchelliandC.Bird.Expectations,outcomes,and
challengesofmoderncodereview.InProceedingsofthe
InternationalConferenceonSoftwareEngineering.
IEEE,2013.
[3]C.Bird,A.Gourley,andP.Devanbu.Detectingpatch
submissionandacceptanceinossprojects.InMSR:
ProceedingsoftheFourthInternationalWorkshop on
MiningSoftwareRepositories,page4.IEEEComputer
Society,2007.
[4]D.BisantandJ.Lyle.Atwo-personinspectionmethod
toimproveprogrammingproductivity.IEEE
Transactionson SoftwareEngineering,
15(10):1294–1304 ,1989.
[5]F.Buck.Indicatorsofqualityinspections.IBMSyst.
Commun.Division,Tech.Rep.TR,21,1981.
[6]J.Cohen.BestKeptSecretsofPeerCodeReview.
SmartBearInc.,2006.
[7]T.DeMarco.ContollingSoftwareProjects:
Management,Measurement,andEstimation.Prentice
Hall,1986.
[8]P.M.Duvall,S.Matyas,andA.Glover.Continuous
integration:improvingsoftwarequalityandreducing
risk.Addison-WesleyProfessional,2007.
[9]M.Fagan.DesignandCodeInspectionstoReduce
ErrorsinProgramDevelopment.IBMSystemsJournal,
15(3):182–211,1976.
[10]M.Fagan.Ahistoryofsoftwareinspections.Software
pioneers:contributionstosoftwareengineering,
Springer-Verlag,Inc.,pages562–573,2002.
[11]Gerrit.Webbasedcodereviewand project
managementforgitbasedprojects.
http://code.google.com/p/gerrit/ .
[12]P.M.Johnson.Reengineeringinspection.ACM
Communications,41(2):49–52,1998.
[13]P.Kampstra.Beanplot:Aboxplotalternativefor
visualcomparisonofdistributions.Journalof
StatisticalSoftware,CodeSnippets1,28:1–9,2008.
[14]C.LarmanandV.Basili.Iterativeandincremental
developments:abriefhistory.Computer,36(6):47 – 56,
June2003.
[15]A.Mockus,R.T.Fielding,andJ.Herbsleb.Twocase
studiesofopensourcesoftwaredevelopment:Apache
andMozilla.ACMTransactionson Software
EngineeringandMethodology,11(3):1–38,2002.
[16]A.MockusandJ.D.Herbsleb.Expertisebrowser:a
quantitativeapproachtoidentifyingexpertise.InICSE:
Proceedingsofthe24thInternationalConferenceon
SoftwareEngineering,pages503–512.ACMPress,2002.
[17]M.Mukadam,C.Bird,andP.C.Rigby.Gerrit
softwarecodereviewdatafromandroid.InProceedingsofthe10thWorkingConferenceonMiningSoftware
Repositories,MSR’13,pages45–48,Piscataway,NJ,
USA,2013.IEEE Press.
[18]M.Nurolahzade,S.M.Nasehi,S.H.Khandkar,and
S.Rawal.Theroleofpatchreviewinsoftware
evolution:ananalysisofthemozillaﬁrefox.In
InternationalWorkshop onPrinciplesofSoftware
Evolution,pages9–18,2009.
[19]A.Porter,H.Siy,A.Mockus,andL.Votta.
Understandingthesourcesofvariationinsoftware
inspections.ACMTransactionsSoftwareEngineering
Methodology,7(1):41–79,1998.
[20]J.Ratcliﬀe.Movingsoftwarequalityupstream:The
positiveimpactoflightweightpeercodereview.In
PaciﬁcNWSoftwareQualityConference,2009.
[21]E.S.Raymond.TheCathedralandtheBazaar.
O’ReillyandAssociates,1999.
[22]P.Rigby,B.Cleary,F.Painchaud,M.Storey,and
D.German.Contemporarypeerreviewinaction:
Lessonsfromopensourcedevelopment.Software,
IEEE,29(6):56 –61,nov.-dec.2012.
[23]P.C.Rigby.UnderstandingOpenSourceSoftwarePeer
Review:ReviewProcesses,Parametersand Statistical
Models,andUnderlyingBehavioursandMechanisms.
http://hdl.handle.net/1828/3258 Universityof
Victoria,Canada,Dissertation,2011.
[24]P.C.Rigby,D.M.German,andM.-A.Storey.Open
sourcesoftwarepeerreviewpractices:Acasestudyof
theapacheserver.InICSE:Proceedingsofthe30th
internationalconferenceonSoftwareEngineering,
pages541–550,2008.
[25]P.C.RigbyandM.-A.Storey.Understanding
broadcastbasedpeerreviewonopensourcesoftware
projects.InProceedingofthe33rdinternational
conferenceonSoftwareengineering,ICSE’11,pages
541–550,NewYork,NY,USA,2011.ACM.
[26]G.V.Rossum.Anopensourceapp:Rietveldcode
reviewtool.https://developers.google.com/
appengine/articles/rietveld .
[27]C.Sauer,D.R.Jeﬀery,L.Land,andP.Yetton.The
EﬀectivenessofSoftwareDevelopmentTechnical
Reviews:ABehaviorallyMotivatedProgramof
Research.IEEETransactionsSoftwareEngineering,
26(1):1–14,2000.
[28]R.Schwartz.Interviewwith ShawnPearce,Google
Engineer,onFLOSSWeekly.
http://www.youtube.com/watch?v=C3MvAQMhC_M .
[29]L.G.Votta.Doeseveryinspection needameeting?
SIGSOFT Softw.Eng.Notes,18(5):107–114,1993.
[30]E.Weller.Lessonsfromthreeyearsofinspection data.
IEEESoftware,10(5):38–45,1993.
[31]K.E.Wiegers.PeerReviewsinSoftware:APractical
Guide.Addison-WesleyInformationTechnologySeries.
Addison-Wesley,2001.
[32]R.K.Yin.CaseStudyResearch:DesignandMethods,
volume5ofAppliedSocialResearchMethodsSeries.
SagePublicationsInc.,3edition,2003.212