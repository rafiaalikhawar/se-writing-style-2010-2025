Learning Revised Models for Planning
in Adaptive Systems
Daniel Sykes, Domenico Corapi, Jeff Magee,
Jeff Kramer, Alessandra Russo
Imperial College London, UK
Ô¨Årst.last@imperial.ac.ukKatsumi Inoue
National Institute of Informatics, Tokyo, Japan
inoue@nii.ac.jp
Abstract ‚ÄîEnvironment domain models are a key part of
the information used by adaptive systems to determine their
behaviour. These models can be incomplete or inaccurate. In
addition, since adaptive systems generally operate in environ-
ments which are subject to change, these models are often also
out of date. To update and correct these models, the system
should observe how the environment responds to its actions, and
compare these responses to those predicted by the model. In this
paper, we use a probabilistic rule learning approach, NoMPRoL,
to update models using feedback from the running system in
the form of execution traces. NoMPRoL is a technique for non-
monotonic probabilistic rule learning based on a transformation
of an inductive logic programming task into an equivalent
abductive one. In essence, it exploits consistent observations by
Ô¨Ånding general rules which explain observations in terms of the
conditions under which they occur. The updated models are then
used to generate new behaviour with a greater chance of success
in the actual environment encountered.
Index Terms ‚Äîadaptive systems, software architecture, runtime
model, machine learning, feedback.
I. I NTRODUCTION
One of the chief challenges of (and motivations for) adaptive
systems is to recognise that the information traditionally
provided by an engineer at design time is, to a greater or
lesser degree, incomplete and inaccurate. Indeed, as the system
runs, its environment evolves such that incompleteness and
inaccuracy grow with time, and failures become ever more
likely. An adaptive system should adapt in line with the
environment‚Äôs evolution, but it cannot reliably do so on the
basis of outdated information.
This information typically comes in the form of architectural
designs [12], [4], [18], behavioural models [1], requirements
[3], [20], [13], or plans [25], [23], which can be used to direct
or verify the running system. One solution to the problem is
to keep the system designer in the loop by providing updated
models, or to minimise inaccuracy by providing abstract,
strictly declarative models [3]. A more dynamic and cheaper
solution is to use techniques such as those used in machine
learning which can automatically update models so that the
predictions of the model correspond to the situations that are
actually observed at runtime.
In our previous work [23], [24], we separated the high-
level behavioural model of the system from the architectural
model, and proposed mechanisms to update the architectural
model according to observed events such as component failure.What was lacking was a comprehensive automated approach
to change the behaviour of the system to respond meaningfully
to changes in the execution environment.
In this paper we describe a solution which uses non-
monotonic (probabilistic) rule learning [6], [5] to revise a
behavioural model following observations of real system
execution in the environment. The behavioural model in
question comes from our previous work involving reactive
planning [23]. The model ( domain model hereafter) describes
the expected behaviour of the environment and what effect
actions available to the system have on the state of the
environment. From the domain model a reactive plan is
extracted that, for every state of the environment, speciÔ¨Åes
a system action that will lead towards a goal [19], [21].
Reactive plans go some way towards handling discrepan-
cies between the expected and the actual behaviour of the
environment by checking the current state before performing
every action. If the environment has moved to an unexpected
state, the system simply performs the action associated with
that state. However, if the unexpected state transition occurs
consistently (under certain conditions), there is no way to make
use of the fact by feeding the observation back into the domain
model. More importantly, if some sequence of actions can no
longer lead to the goal (because some assumption has been
broken), the reactive plan will nevertheless continue to perform
them. In other words, the reactive plan is generated under the
assumption that the speciÔ¨Åed transitions are nearly certain and
any unspeciÔ¨Åed transitions are very unlikely.
Probabilistic rule learning allows us to exploit consistent ob-
servations by Ô¨Ånding rules (in a logic programming language)
which explain each observation in terms of the conditions
under which it occurs. This technique can discover previously-
unmodelled dependencies between states and actions, even
pertaining to historical conditions. The technique also consid-
ers observations as inherently probabilistic, which can capture
the effect of unmodelled conditions and evolution of the
environment. The new rules are automatically combined with
the original domain model so that they are considered when a
new reactive plan is generated. Furthermore, the rules remain
human-readable so that the system designer has the oppor-
tunity to review them whenever convenient. In this way, the
adaptive system is able to overcome the incompleteness and978-1-4673-3076-3/13/$31.00 c2013 IEEE ICSE 2013, San Francisco, CA, USA63
inaccuracy in its original model (sometimes called uncertainty
[11]) and adapt to an evolving execution environment.
The paper is organised as follows. In Section II we set out
a motivating problem scenario in which the domain model
differs substantially from the runtime environment. Sections
III and IV describe the technical details of our approach,
while Section V demonstrates the approach on a case study
and Section VI provides a performance evaluation. Section
VII compares the approach to existing work in the area, and
Section VIII concludes.
II. T HEPROBLEM
Our previous work has concentrated on elaborating aspects
of the three-layer conceptual model for adaptive systems
shown in Figure 1, which aims to stratify adaptive processes
into three layers of increasing abstraction [16], [23], [24].
In the lower component layer reactive domain-speciÔ¨Åc be-
haviours that require little or no computation overhead are
applied. In the middle change management layer adaptations
with a wider scope can be applied by manipulating the
conÔ¨Åguration of components running in the layer below. In
the upper goal management layer reactive plans are generated
from a domain model describing the environment and the
actions the system can perform therein. Having three layers
means that the computational cost of adaptations involving
the upper layers, particularly planning, is avoided whenever
possible.
Fig. 1. Three-layer conceptual model.
The work described in this paper elaborates one function of
the goal management layer, namely, how to use feedback from
the running system to improve the domain model. From the
improved model, reactive plans are generated that, by having
a better understanding of the environment, will be more likely
to achieve the goal. To illustrate our motivation we show how
the approach in [23] would work in the following scenario.
Figure 2 depicts the domain model for a robot operating in a
factory-Ô¨Çoor setting. The task of the robot is to carry fragile
objects from one side of the factory (state 1) to the other side
of the factory (state 6) via number of intermediate locations.
One possible run has the robot picking up the object (state
2), driving to an intermediate location (state 3), driving to the
destination (state 5), and placing the object at the destination.An alternative run would have the robot driving via a different
intermediate location (state 4).
Supposing the goal of the system was to reach state 6, the
reactive planning technique in [23] could produce the plan
shown in Figure 3 (there is one alternative plan). The reactive
plan associates a single action with every state. In order to
execute the reactive plan, the robot must check the current
state (which can be distinguished by checking where it is in the
factory and whether an object has been picked up) and perform
the associated action. If executing the action does not lead to
the expected state, the reactive plan can still continue from the
new state. Provided that every performed action eventually has
the expected outcome, the goal will be reached.
Fig. 2. Robot domain model.
Fig. 3. Robot plan.
What if the environment never behaves as expected? Let
us now consider some situations in which the plan will fail.
Suppose that the sensors used to detect the location of the
robot are correct only with some probability pwhen the robot
attempts to move to state 3, and otherwise report that the
robot is in state 4. This may cause the entire plan to fail with
probability 1 p, or succeed, albeit under the belief that it
went via state 41.
Let us further suppose that when the sensors fail in this
way it disrupts the motion of the robot and the fragile object
is smashed with a different probability 1 q. When the object
is smashed, the act of putting it down fails, and so the plan
fails. The probability of this plan succeeding is now pq. Note
that there is still an alternative plan, in which the robot moves
from state 2 to 4, that avoids these difÔ¨Åculties.
Finally, let us suppose that picking up the object fails with a
small probability 1 r(trapping the robot in state 1) because
there is insufÔ¨Åcient illumination in the factory (perhaps there
was a power cut). Then the probability of success is rpq
1The authors experienced this very problem in previous experiments with
mobile robots.64under the current plan, while the alternative plan has a greater
success rate of r.
Unfortunately, the designer of the above domain model did
not foresee these failure scenarios. Even if it were possible to
predict them, it would be extremely difÔ¨Åcult to provide exact
values forr,pandqwithout running tests in situ . We propose
to use trace executions of the running system to perform, in
effect, such testing at runtime, and to update the domain model
appropriately. What is needed is an approach that handles:
Unmodelled conditions Had the designer foreseen the
effect of illumination on pickup , it could have been
added as an explicit pre-condition for pickup , and extra
behaviour could have been added to deal with the case
when there is no illumination, such as turnOnLights .
By observing that pickup sometimes fails, the planner
can choose a more reliable action, where possible.
Probabilistic factors The sensor failure in state 3 and
the subsequent chance of smashing the object represent
probabilistic factors for which there is no action that
could be applied to rectify the problem (even if the
problem could be reliably sensed). The only solution is
to avoid the situation occurring in the Ô¨Årst place by not
moving to state 3.
Unmodelled transitions If attempting to move to state 3
in fact reliably takes the robot to state 4 (meaning p= 0),
then a new transition should be added to the model so that
it can be taken account of when generating a new plan.
Also, when p= 0, then the probability of the transition
from state 2 to 3 is 0 and the transition can be ignored.
Unmodelled historical dependencies The success of the
putdown action is dependent on which path was taken
to arrive at state 5. In the case of the path through state
3, success is dependent on the chance of sensor failure.
Making the path taken a pre-condition of performing
putdown would enable the planner to produce plans
avoiding the unreliable route.
Evolution in the environment will over time exhibit a
combination of these issues. In this paper we propose an
approach that takes advantage of existing work on probabilistic
rule learning [6] to handle environmental evolution and model
inaccuracies through a process of model updating.
III. A PPROACH
Figure 4 gives an overview of our approach for elaborating
the goal management layer of the three-layer model. Reactive
plans are produced from the domain model provided by the
system designer and passed to the lower layers for execution.
The change management layer keeps a record of the traces
actually executed along with an indication of whether the
goal was reached or not (a time-out is sometimes necessary
to detect failure in practical cases). Since the environment
may behave differently from the expectations encoded in the
domain model, such traces may exist in neither the plan nor the
domain model. Once a certain number of runs (or speciÔ¨Åcally
failing runs) have been executed, the traces are passed back
to the goal management layer in order to update the domain
Fig. 4. Overview of the approach.
1c o n n e c t e d ( loc1 , l o c 3 ) .
2c o n n e c t e d ( loc1 , l o c 4 ) .
3c o n n e c t e d ( loc3 , l o c 4 ) .
4c o n n e c t e d ( loc4 , l o c 3 ) .
5c o n n e c t e d ( loc3 , l o c 5 ) .
6c o n n e c t e d ( loc4 , l o c 5 ) .
7
8p o s s i b l e ( pickup , T ) :  
9 n o t h o l d s A t ( h o l d i n g O b j e c t , T ) ,
10 h o l d s A t ( a t ( l o c 1 ) , T ) .
11p o s s i b l e ( putdown , T ) :  
12 h o l d s A t ( h o l d i n g O b j e c t , T ) ,
13 h o l d s A t ( a t ( l o c 5 ) , T ) .
14p o s s i b l e ( move ( L1 , L2 ) , T ) :  
15 h o l d s A t ( a t ( L1 ) , T ) ,
16 c o n n e c t e d ( L1 , L2 ) .
17 . . .
18 i n i t i a t e s ( pickup , h o l d i n g O b j e c t , T ) .
19t e r m i n a t e s ( putdown , h o l d i n g O b j e c t , T ) .
20 i n i t i a t e s ( move ( L1 , L2 ) , a t ( L2 ) , T ) .
21t e r m i n a t e s ( move ( L1 , L2 ) , a t ( L1 ) , T ) .
22 . . .
Fig. 5. Part of a domain model. In each rule, Tindicates a time point.
model and then generate a reÔ¨Åned plan. The planner may also
conclude that it is no longer possible to achieve the goal.
This feedback cycle continues for the lifetime of the adaptive
system.
A. Domain Model
In order to make use of probabilistic rule learning (explained
in the next section), it is necessary to encode the domain model
as a logic program in terms of conditions and actions occurring
at time points T(similar to event calculus [15] as used in
[1]). In previous work we encoded the domain model as LTL
(linear temporal logic) constraints, which are straightforward
to transform into a logic program.
Figure 5 shows part of the domain model corresponding
to the example in Figure 2. Lines 8-16 describe what actions
are (according to the designer) possible in the domain, and
under what pre-conditions they can take place. For example,
theputdown action cannot occur (at a time point T) unless65holdingObject andat(loc5)are true. The actions such as
putdown and conditions such as at(loc5)have a domain-
speciÔ¨Åc meaning evaluated at runtime during plan execution,
which takes place in the lower layers. Lines 18-21 show the
effect of executing actions, that is, their post-conditions. For
instance,putdown makes theholdingObject predicate false.
Other lines such as 1-6 express domain-speciÔ¨Åc constraints
(in this case how factory locations are connected). We have
omitted general parts of the model which are not domain-
speciÔ¨Åc and facilitate model update and planning.
Planning, in the absence of probabilities, is exactly as
previously described [23], which, when the domain model is
given as a logic program, corresponds to abductive reasoning,
as in [22]. Planning with probabilities is described in Section
IV-C.
B. Execution Traces
Execution traces record what conditions held and what
actions were performed at each time point. In addition, success
or failure is recorded. For example, the trace
1h o l d s A t ( a t ( l o c 1 ) , 0 ) .
2do ( pickup , 0 ) .
3h o l d s A t ( a t ( l o c 1 ) , 1 ) .
4h o l d s A t ( h o l d i n g O b j e c t , 1 ) .
5do ( move ( loc1 , l o c 3 ) , 1 ) .
6h o l d s A t ( a t ( l o c 3 ) , 2 ) .
7h o l d s A t ( h o l d i n g O b j e c t , 2 ) .
8do ( move ( loc3 , l o c 5 ) , 2 ) .
9h o l d s A t ( a t ( l o c 5 ) , 3 ) .
10h o l d s A t ( h o l d i n g O b j e c t , 3 ) .
11do ( putdown , 3 ) .
represents a successful execution (over time points 0 to 3) of
the path via state 3 of Figure 2. In contrast, the trace
1h o l d s A t ( a t ( l o c 1 ) , 0 ) .
2do ( pickup , 0 ) .
3h o l d s A t ( a t ( l o c 1 ) , 1 ) .
4h o l d s A t ( h o l d i n g O b j e c t , 1 ) .
5do ( move ( loc1 , l o c 3 ) , 1 ) .
6h o l d s A t ( a t ( l o c 4 ) , 2 ) .
7h o l d s A t ( h o l d i n g O b j e c t , 2 ) .
8do ( move ( loc4 , l o c 5 ) , 2 ) .
9h o l d s A t ( a t ( l o c 5 ) , 3 ) .
10h o l d s A t ( h o l d i n g O b j e c t , 3 ) .
11do ( putdown , 3 ) .
12h o l d s A t ( a t ( l o c 5 ) , 4 ) .
13h o l d s A t ( h o l d i n g O b j e c t , 4 ) .
14do ( putdown , 4 ) .
15 . . .
represents a failing trace caused by the location sensor giving
an incorrect reading on line 6, resulting in the object being
broken and putdown failing at time points 3 and 4. Traces
acquired by executing different plans for a given goal or
altogether different goals can be used for model updating. The
use of traces from different plans also reduces the risk of the
system entering cycles of unsuccessful adaptations. Note that
traces, and consequently states and transitions within those
traces, have a probability by virtue of their frequency within
the overall set of traces.IV. P ROBABILISTIC RULE LEARNING
Here we give an overview of NoMPRoL, which we use for
model updating, omitting technical details that the interested
reader can Ô¨Ånd in [6]. NoMPRoL is a technique for non-
monotonic probabilistic rule learning based on a transfor-
mation of an inductive logic programming (ILP) task into
an equivalent abductive one, which is solved using efÔ¨Åcient
answer set programming (ASP) tools. More speciÔ¨Åcally, given
some background knowledge, and a set of observed facts, the
task is to Ô¨Ånd a hypothesis ‚Äîa set of new or revised rules‚Äî
that explains the occurrence of the observations. In general
there are many such explanations and so they are ranked by
the number of observations they are capable of explaining.
In this case, the background knowledge (also called the
theory ) is provided by the domain model, and the observations
are provided by the set of execution traces. The learned rules
are added to the domain model so that it better represents
the environment as observed, thus leading to more successful
plans.
A. Rule Learning
The Ô¨Årst stage consists of transforming the ILP task into one
solvable using ASP. ASP produces answer sets , that is, assign-
ments of true or false to all atoms (predicates) that satisfy the
background theory in conjunction with the observations. In
NoMPRoL, the answer sets contain atoms encoding hypothe-
ses (sets of rules) that explain the observations with respect to
the background knowledge. The set of all answer sets encodes
all possible hypotheses. For example, consider the observation
of two factsfrain(yesterday );london (yesterday )g. The
possible hypotheses include:
h1 :frain(X) :-london (X): london (X):g
h2 :flondon (X) :-rain(X): rain (X):g
which can be interpreted as ‚Äúif I am in London, it is raining
(and I am always in London)‚Äù and ‚Äúif it is raining, I am
in London (and it is always raining)‚Äù. In any realistic ILP
problem there are of course a huge number of possible
hypotheses. The space of hypotheses can be controlled by
using mode declarations . Each mode declaration states that
an atom is permitted to appear in the head (the consequent)
or body (the antecedent) of a rule in the hypothesis.
Mode declarations are speciÔ¨Åed as m:mode (t;r(m);s)
wheremis the label; the Ô¨Årst argument speciÔ¨Åes whether it is
a head (h) or body (b) mode declaration; the second argument
speciÔ¨Åes the maximum number of occurrences of m; and the
third argument speciÔ¨Åes the schema . A schema sis an atom
(possibly negated) that contains one or more placemarkers for
arguments. A placemarker consists of one of the three symbols
‚Äò+‚Äô (for input placemarkers), ‚Äò-‚Äô (for output placemarkers), and
‚Äò#‚Äô (for constant placemarkers), and a constant called the type.
For example, the mode declarations:
m1 :mode (h;1;rain (+date)):
m2 :mode (h;1;london (+date)):
m3 :mode (b;1;rain (+date)):66state that the london andrain atoms are permitted to appear
in the head of rules, and the rain atom is permitted to appear
in the body. Each has an input of type date . This restricts the
space of hypotheses so that the only solution is now:
h2 :flondon (X) :-rain(X): rain (X):g
The rules we are interested in learning to update the domain
model state the conditions under which actions can be per-
formed. Therefore we restrict the hypothesis space Ato rules
containing an action in the head and environment conditions
in the body by generating mode declarations
mode (h;2;succeeds (act;+time))
for all actions actand, for all conditions cond ,
mode (b;2;holdsAt (cond; +time))
mode (b;2;notholdsAt (cond; +time))
It is also sometimes necessary to have body mode declarations
for related domain-speciÔ¨Åc predicates such as connected from
Figure 5. We also restrict the number of occurrences of a
condition in a rule (to two occurrences in this case) since
this results in more general rules (avoiding over-Ô¨Åtting) and
improves the performance of the learning process. An added
beneÔ¨Åt is that fewer, more general rules are more readable,
should the system designer wish to review them.
B. Probability Estimation
Once a set of hypotheses hhave been found, the next
step is to Ô¨Ånd the probabilities associated with each rule, and
thus to Ô¨Ånd the maximum likelihood hypothesis, that is to
say, the set of rules (from the space of hypotheses deÔ¨Åned
by the mode declarations) which explains, with the maximal
probability, the observations. Since the absolute value of the
time points included in the execution traces is normally not
relevant2, we divide the traces into a historyand an observed
statex, for every observed state in the trace. An observed state
consists of a number of holdsAt atoms at a particular time
point, and the associated history consists of all holdsAt and
doatoms up to that time point. For example, the succeeding
trace from Section III-B can be divided in four ways (one for
each time point), one of which is:
x2=holdsAt (at(loc3);2);
holdsAt (holdingObject; 2):
2=8
>>>><
>>>>:holdsAt (at(loc1);0);
do(pickup; 0);
holdsAt (at(loc1);1);
holdsAt (holdingObject; 1);
do(move (loc1;loc3);1):
Following this procedure, the maximum likelihood hypothesis
is the set of rules which explains the most observations x,
given the preceding history .
2Although it could become relevant with long-running systems, if, for
instance, the robot‚Äôs battery becomes progressively unreliable.Each condition a2Ain each rule of a hypothesis has an
associated estimated probability . From these probabilities,
the probability of a hypothesis can be calculated:
P
0(h) =Y
a2haY
a2Anh(1 a) (1)
is estimated by minimising the mean squared error function:
MSE () =1
jXjX
i(1 P(xij[i))2
wherePindicates the ratio of the summed probabilities of
hypotheses that predict the observation to those that do not,
given the current values:
P(xij[i) =P
fh2;h[ij=xigP
0(h)
P
fh2gP
0(h)
The ideal values of will ensure all the hypotheses that predict
the observation will have higher probabilities compared to
those that do not.
For example, consider the two hypotheses from the previous
section:
h1 :frain(X) :-london (X): london (X):g
h2 :flondon (X) :-rain(X): rain (X):g
Suppose the probabilities associated with the heads and bodies
are initially: h1h= 0:8,h1b= 0:7,h2h= 0:1,h2b= 0:2.
ThenP
0(h1) = 0:4032 andP
0(h2) = 0:0012 . This gives an
MSE of 0.997 since hypothesis h2that predicts the observation
oflondon (yesterday )has lowvalues. Increasing h2hand
h2btowards 1improves the MSE.
Once the minimisation of the MSE has converged (using
gradient descent in our current implementation), the maximum
likelihood hypothesis hcan be selected by choosing the one
with the largest P
0.
The probabilities obtained for each hypothesis through
minimising the MSE indicate how well they predict the
observations, given the situations in which their conditions
hold. For example, in a set of 20 observations in which
rain(X)is true in 10 cases, while london (X)is true in 6
of those 10 cases, the rule london (X) :-rain(X)should
have a probability close to 0.6 (provided there are no other
rules in the hypothesis).
C. Applying Learned Rules
In order to use the learned rules for planning, a probability
for the rule as a whole, which represents the probability that
the performing the action will have the speciÔ¨Åed successful
outcome, is required. However, the estimated probabilities 
do not relate directly to rules, but rather to the presence of
conditions in the body of rules3. In other words, each rule
in the hypothesis hrepresents a set of possible applicable
rules , where each applicable rule contains a subset of the
conditions mentioned in the original rule. The probability r
of an applicable rule is calculated by multiplying the , for
3This representation can be used directly for probabilistic inference.67each condition kept from the original rule, by (1 ), for each
omitted condition (analogous to Equation 1). We then select
the applicable rule with the highest probability. For example,
the rule below with estimated probabilities for each condition
can give rise to several possible applicable rules:
s u c c e e d s ( move ( L1 , L2 ) , T ) :  
h o l d s A t ( a t ( L1 ) , T ) , 0 . 9
c o n n e c t e d ( L1 , L2 ) , 0 . 9
L2 != loc3 , 0 . 7
L2 != l o c 5 . 0 . 2 5
The applicable rule omitting the last condition L26=loc5has
the greatest probability (0.42525), while the rule including the
last condition has a lower probability (0.14175). The chosen
applicable rules (hereafter we simply call them rules) are
combined with the original domain model so that a new plan
can be generated.
Suppose the scenario in Section II resulted in the following
learned rules:
r1 : 0 . 7 : s u c c e e d s ( pickup , T ) .
r2 : 0 . 9 : s u c c e e d s ( move ( L1 , L2 ) , T ) :  
h o l d s A t ( a t ( L1 ) , T ) ,
c o n n e c t e d ( L1 , L2 ) ,
L2 != l o c 3 .
r3 : 0 . 9 : s u c c e e d s ( putdown , T ) :  
n o t happened ( move ( loc2 , l o c 3 ) , T  2).
r4 : 0 . 1 : s u c c e e d s ( putdown , T ) :  
happened ( move ( loc2 , l o c 3 ) , T  2).
Ruler1with probability 0.7 reÔ¨Çects the fact that picking up
the object fails some of the time, with no other dependencies.
Ruler2states that moving to a location succeeds only if the
destination is not loc3, reÔ¨Çecting the unreliable sensing in that
location. Rule r3states thatputdown will succeed if the robot
has not attempted to move to loc3previously. Rule r4states
thatputdown can succeed with a small probability if the robot
did move via loc3.
The updated domain model can be visualised as the labelled
transition system (LTS) shown in Figure 6, where each state
represents a particular conjunction of conditions. A transition
from a source state to a target state is shown where an
applicable rule states that an action from the source state
can succeed (with an estimated probability) and where the
target state is speciÔ¨Åed (in the domain model) as the outcome
of performing that action. Notice that two new states 5a
and 5b have been introduced as a result of the happened
condition distinguishing between the two possible paths into
loc5. The grey self-loops indicate the correspondence between
this model and a Markov decision process: when an action
does not succeed, the process is expected to remain in the
same state with the residual probability4.
Figure 7 shows the plan generated from the updated domain
model, which, in contrast to Figure 3, avoids the risky move
toloc3. In fact, the domain model does not permit such a
move by virtue of rule r2. If, however, the learned hypothesis
4Though in general there is a residual probability of moving to any other
state.
Fig. 6. Updated domain model.
Fig. 7. New plan.
stated that the move was possible (with a low probability),
then the planner would have to choose between the paths
vialoc3andloc4. To handle this circumstance, a small
extension to the planning algorithm is required. The planner
constructs a reactive plan by starting at the goal states and
backtracking along transitions, adding states, until no more
states can be added. When each state is added, the planner
must choose a transition to include in the plan for that state.
Previously, when faced with a choice of transitions from the
same state, the planner would make an arbitrary choice. Our
extended planner uses the transition probabilities in order to
choose the most reliable path. To achieve this, the planner
backtracks from the goal state recording the probabilities of
paths (the product of transition probabilities) leading from
previous states. When all transitions from a state have been
considered, the maximum-probability path is recorded for that
state, and the planner can consider transitions leading into
that state. When no more transitions can be considered, the
plan is complete and prescribes for every state the transition
that maximises the chance of success over the whole path5.
Standard techniques for solving Markov decision processes
can also be applied, given an appropriate reward function. In
Figure 7, the transition selected for state 3 goes to state 4
because this path has a higher probability of success (0.729)
than the path via state 5a (probability 0.09).
V. F EASIBILITY CASE STUDY
We now apply the approach to a case study based on a
variant of the production cell taken from [7]. The scenario
comprises a tray of products, either processed or unprocessed,
to which are applied various processing operations such as
drilling, pressing, cooking and so on. A robot arm moves the
5Assuming that the environment behaves as the updated model predicts,
and the optimisation of MSE did not get stuck in a local minimum.681p o s s i b l e ( p u t ( d r i l l , P ) , T ) :  
2 n o t h o l d s A t ( i n ( d r i l l , P ) , T ) ,
3 n o t h o l d s A t ( d r i l l e d ( P ) , T ) ,
4 h o l d s A t ( p i c k e d u p ( P ) , T ) .
5p o s s i b l e ( d r i l l ( P ) , T ) :  
6 h o l d s A t ( i n ( d r i l l , P ) , T ) ,
7 n o t h o l d s A t ( d r i l l e d ( P ) , T ) .
8p o s s i b l e ( g e t ( d r i l l , P ) , T ) :  
9 h o l d s A t ( i n ( d r i l l , P ) , T ) ,
10 h o l d s A t ( d r i l l e d ( P ) , T ) ,
11 n o t h o l d s A t ( p i c k e d u p ( P ) , T ) .
12
13p o s s i b l e ( p u t ( oven , P ) , T ) :  
14 n o t h o l d s A t ( i n ( oven , P ) , T ) ,
15 n o t h o l d s A t ( cooked ( P ) , T ) ,
16 h o l d s A t ( p i c k e d u p ( P ) , T ) .
17p o s s i b l e ( cook ( P ) , T ) :  
18 h o l d s A t ( i n ( oven , P ) , T ) ,
19 n o t h o l d s A t ( cooked ( P ) , T ) .
20p o s s i b l e ( g e t ( oven , P ) , T ) :  
21 h o l d s A t ( i n ( oven , P ) , T ) ,
22 h o l d s A t ( cooked ( P ) , T ) ,
23 n o t h o l d s A t ( p i c k e d u p ( P ) , T ) .
24
25p o s s i b l e ( g e t ( t r a y , P ) , T ) :  
26 n o t h o l d s A t ( p i c k e d u p ( P ) , T ) .
27p o s s i b l e ( p u t ( t r a y , P ) , T ) :  
28 h o l d s A t ( p i c k e d u p ( P ) , T ) .
29
30: d r i l l e d ( a ) , n o t cooked ( a ) .
31: cooked ( b ) , n o t d r i l l e d ( b ) .
Fig. 8. Production cell domain model.
products from the tray to each processing unit and back again.
Following [7] we restrict the scenario to two products of type
aandband the processing units to a drill and an oven. Figure
8 shows our initial domain model. The rules have the intuitive
meanings, such as ‚Äúa product can be placed in the drill unit
if it is not already there, not already drilled, and being held
by the robot arm‚Äù (lines 1-4). Lines 30-31 state constraints on
the processing of products aandb:amust be drilled before
being cooked, and bmust be cooked before being drilled.
The goal is to have both products aandbcooked, drilled
and placed on the tray. Unfortunately the LTS representations
of both the domain and the plan are too large to show here.
We now suppose that the oven develops a fault that, with
some probability, breaks product aif the drill is in use at the
same time. The observable consequence of breaking product
ais that the robot arm can no longer successfully pick up the
product after a cook action. After executing the production
cell a number of times, the domain model should be updated
with a new rule to reÔ¨Çect the faulty oven, with a body that
captures the context in which faults occur.
The maximum likelihood hypothesis produced includes
rules distinguishing the two cases when get(oven;P )suc-
ceeds. In the Ô¨Årst case when the product is not a, the action
always succeeds. The second case states that when the product
isaandin(drill;b )holds success is much less likely (due to
the fault).r1 : 1 . 0 : s u c c e e d s ( g e t ( oven , P ) , T ) :  
h o l d s A t ( i n ( oven , P ) , T ) ,
h o l d s A t ( cooked ( P ) , T ) ,
n o t h o l d s A t ( p i c k e d u p ( P ) , T ) ,
P != a .
r2 : 0 . 4 : s u c c e e d s ( g e t ( oven , P ) , T ) :  
h o l d s A t ( i n ( oven , P ) , T ) ,
h o l d s A t ( cooked ( P ) , T ) ,
n o t h o l d s A t ( p i c k e d u p ( P ) , T ) ,
h o l d s A t ( i n ( d r i l l , b ) , T ) ,
P = a .
The updated model provides feedback to the designer and
enables the planner to produce an alternative plan that avoids
putting ainto the faulty oven when the drill is already in
use, making overall success far more likely. While this case
study has given us an indication of the feasibility and promise
of the approach, we recognise that further work is necessary
to explore practical constraints on its utility. The next section
discusses the performance of the model updating approach and
its impact on the adaptive system as a whole.
VI. P ERFORMANCE EVALUATION
A key test of the approach is whether its computational cost
is prohibitive for application inside the adaptive feedback loop.
The rule learning part of the approach has a time complexity
proportional to the space of possible hypotheses A. This space
is dependent on the number of mode declarations employed
and is exponential in the permitted number of occurrences of
each condition6.
However, we found that the rule learning step has reasonable
performance in comparison to the probability estimation step
using gradient descent in our prototype implementation (see
Figure 9). To give an indication of the time taken, Ô¨Ånding
the set of hypotheses for an example of similar size to those
described herein took 23 seconds, while probability estimation
took 345 seconds to converge.
While such performance might ordinarily be a cause for
concern inside a classical sense-plan-act loop, the three-layer
conceptual model means that the costly deliberations of the
goal management layer are avoided whenever possible. In
addition:
Since the rule learning can make use of any number of
traces (and indeed incomplete traces), model updates can
take place concurrently with normal execution, which, in
some domains, may take place over hours or even days7.
When a failure eventually occurs, the goal management
layer is already prepared with an updated model and a
new plan.
Since plan execution in the change management layer
can tolerate an inaccurate domain model (such as the one
provided by the system designer), it may be acceptable
to use suboptimal probabilities resulting from limiting the
running time of gradient descent. Indeed, the marginally
6Hence why we limit the occurrences to two in the motivating example.
7For instance, generating the execution traces for the factory Ô¨Çoor example
took several hours.69updated model may provide further execution traces that
improve the performance and accuracy of the next round
of learning.
Fig. 9. Graph showing probabilities of conditions as estimated during gradient
descent.
VII. R ELATED WORK
The paramount importance of maintaining a model of the
environment at runtime and updating it to match observed
behaviour as the environment evolves has been recognised
in a number of works, notably in KAMI [10]. In that work,
a discrete-time Markov chain (DTMC) is used to analyse
the non-functional properties (such as reliability) of a system
with respect to its behaviour. The probabilities associated with
transitions in the model are then updated through Bayesian
estimation. While this ensures that the probabilities in the
model are progressively more accurate, it does not account
for the possibility that the structure of the model is incorrect.
Moreover, the choice of formalism means that the updated
probabilities are assumed to be independent (the Markov
property), whereas in Section II we showed the possibility
of probabilities being dependent upon the path by which the
state was reached. Finally, the updated information is not
explanatory (as an inferred requirement) to the designer in
the way that a learned rule is, in the approach described here.
Another approach that uses learning as part of a feedback
loop is described in [14]. In that work, the model being
updated is a mapping from states to architectural conÔ¨Ågu-
rations, where the selected conÔ¨Ågurations are those with the
highest expected reward. Feedback from execution provides
updated reward values. A similar approach [9] learns the model
relating feature conÔ¨Ågurations to the expected values of non-
functional properties. Our approach is appropriate for more
expressive LTS models (and feedback), and architectural issues
are handled separately.
ILP has been used previously by Alrajeh et al. [1] to
reÔ¨Åne system requirements at design time using user-provided
scenarios. Our approach moves the process to runtime, using
feedback from the running system as counter-examples (traces
that violate the goal). The work in [2] likewise uses machinelearning to reÔ¨Åne speciÔ¨Åcations, in this case using neural-
symbolic networks [17] that handle noise. In addition feedback
from the running system is used to learn speciÔ¨Åcations. This
work is likely applicable in an adaptive context.
D‚ÄôIppolito et al. [8] provide an alternative technique for
generating controllers (plans) for domains in which the suc-
cessful outcome of actions is not guaranteed (in a single
execution). This could be employed in place of reactive
planning in the three-layer model, although the approach only
permits a single failing outcome, while rule learning may
provide several possible outcomes.
VIII. C ONCLUSIONS AND FUTURE WORK
In this paper we have presented an approach for updating
process models used for planning the high-level behaviour of
adaptive systems. Updating the model is performed at runtime
on the basis of execution traces, which‚Äîsince the reactive plan
can often continue when an action has an unexpected (unmod-
elled) outcome‚Äîneed not have been described in the original
domain model. Such updates enable the adaptive system to
cope with incomplete knowledge (given by the designer) and
a noisy, ever-evolving environment. We use a non-monotonic
probabilistic rule learning technique, NoMPRoL, that Ô¨Ånds
hypotheses consisting of new rules specifying the probability
of an action achieving its speciÔ¨Åed outcome under particular
conditions. The same technique can be used to learn different
action outcomes (shown in the initiates andterminates
clauses in Figure 5), which we intend to demonstrate in future
work.
NoMPRoL is sound and complete with respect to logic
programs and consequently the Ô¨Ånite domain models we
employ herein. However, the fundamental limitations of this
kind of learning for adaptive systems come from the set of
conditions which can be sensed from the environment and
the actions that can be performed within it. In other words,
a rule cannot be learned for a condition that is not sensed,
or an action that is not performed, in the execution traces.
Traditionally the designer only models the aspects of the
domain thought relevant to the achievement of the system‚Äôs
goals‚Äîthat being the essence of abstraction‚Äîbut in a dynamic
environment the designer‚Äôs assumptions about what is relevant
(in addition to how they are relevant) are likely to be broken.
There is therefore an incentive to guard against future change
by modelling ‚Äúirrelevant‚Äù conditions and actions in case they
ever become important for the achievement of goals. This
leads to a trade-off between adaptability and cost, as the
extra modelling increases development time and reduces the
efÔ¨Åciency of learning (by enlarging the hypothesis space). One
solution to be examined would be to keep the system designer
in the loop to manage the ‚Äúmacro‚Äù adaptations that require new
actions, while the system handles ‚Äúmicro‚Äù adaptations (within
the existing alphabet of actions) autonomously.
As indicated in Section VI, it is desirable, though not
critical, to have an efÔ¨Åcient goal management layer so that new,
more successful plans are available when required. In order
to keep the examples tractable, we currently enforce a strict70limit on the length of learned rules. We aim in future work to
make efÔ¨Åciency gains by exploiting the statistical nature of the
environment so that the entire space of hypotheses need not
be explored every time. Indeed, several questions arise when
considering the behaviour of the system over several rounds
of learning, rather than the single round on which we have
focused in this paper. For instance, it is possible that, in one
round of learning, the environment behaves in a highly atypical
manner, and that the rules learned are unsuccessful when
subsequently applied in planning. However, we expect that
using many execution traces from a long period (many rounds
of learning) will both reduce its susceptibility to anomalous
rounds and improve the learning by avoiding over-Ô¨Åtting. The
length of period (i.e. the window) from which traces are used
is likely a domain-speciÔ¨Åc property. Consider, for example,
an environment that is stable and predictable for a long period
and then changes to be highly dynamic or stable for only
a few rounds. In this case, a large trace window would be
inappropriate, as it would make the system very resistant to
change.
Another potentially fruitful avenue for future investigation
would be to allow the system to explore the environment by
executing actions outside the strictures of the current plan.
This provides extra information that may become valuable
for future adaptations. Indeed, an exploratory approach would
enable to system to start with an empty domain model and
no plan, and gradually construct a domain model, alternating
between exploration to reÔ¨Åne the model and exploitation of
it to achieve goals. Evidently, this is not feasible in some
domains (e.g. safety critical systems), but, where it can be
applied safely, it presents a relatively new software design
process ‚Äì automated prototyping.
Finally, we make no pretence to have exhausted the Ô¨Åeld
of machine learning. There is a wide variety of alternative
techniques that may be applicable, including non-symbolic
methods. However our chosen approach has the beneÔ¨Åt of
producing human-readable logical rules that naturally Ô¨Åt our
existing planning formalism, while enabling us to restructure
the model, learn historical conditions and deal with a proba-
bilistic environment.
REFERENCES
[1] D. Alrajeh, J. Kramer, A. Russo, and S. Uchitel. Learning operational
requirements from goal models. In Proceedings of the 31st International
Conference on Software Engineering , ICSE ‚Äô09, pages 265‚Äì275,
Washington, DC, USA, 2009. IEEE Computer Society.
[2] R. V . Borges, A. d‚ÄôAvila Garcez, L. C. Lamb, and B. Nuseibeh.
Learning to adapt requirements speciÔ¨Åcations of evolving systems (NIER
track). In Proceedings of the 33rd international conference on Software
engineering , ICSE ‚Äô11, pages 856‚Äì859, New York, NY , USA, 2011.
ACM.
[3] B. H. Cheng, P. Sawyer, N. Bencomo, and J. Whittle. A goal-based
modeling approach to develop requirements of an adaptive system with
environmental uncertainty. In Proceedings of the 12th International
Conference on Model Driven Engineering Languages and Systems ,
MODELS‚Äô09, pages 468‚Äì483, Berlin, Heidelberg, 2009. Springer-
Verlag.
[4] S.-W. Cheng. Rainbow: Cost-Effective Software Architecture-Based Self-
Adaptation . PhD thesis, 2008.[5] D. Corapi, A. Russo, and E. Lupu. Inductive Logic Programming
as Abductive Search. In Technical Communications of the 26th
International Conference on Logic Programming , volume 7, pages 54‚Äì
63, Dagstuhl, Germany, 2010. Schloss Dagstuhl‚ÄìLeibniz-Zentrum fuer
Informatik.
[6] D. Corapi, D. Sykes, K. Inoue, and A. Russo. Probabilistic Rule
Learning in Nonmonotonic Domains. Proceedings of International
Workshop on Computational Logic in Multi-Agent Systems , 2011.
[7] N. D‚ÄôIppolito, V . Braberman, N. Piterman, and S. Uchitel. Synthesis of
live behaviour models. In Proceedings of the eighteenth ACM SIGSOFT
international symposium on Foundations of software engineering , pages
77‚Äì86. ACM, 2010.
[8] N. D‚ÄôIppolito, V . Braberman, N. Piterman, and S. Uchitel. Synthesis
of live behaviour models for fallible domains. In Proceedings of the
International Conference on Software Engineering , ICSE‚Äô11, pages 211‚Äì
220. IEEE, 2011.
[9] A. Elkhodary, N. Esfahani, and S. Malek. Fusion: a framework
for engineering self-tuning self-adaptive software systems. In
Proceedings of the eighteenth ACM SIGSOFT international symposium
on Foundations of software engineering , pages 7‚Äì16. ACM, 2010.
[10] I. Epifani, C. Ghezzi, R. Mirandola, and G. Tamburrelli. Model
evolution by run-time parameter adaptation. In Proceedings of the 2009
International Conference on Software Engineering , ICSE‚Äô09, pages 111‚Äì
121. IEEE Computer Society, 2009.
[11] N. Esfahani and S. Malek. Uncertainty in self-adaptive software systems.
2013.
[12] D. Garlan and B. Schmerl. Model-based adaptation for self-healing
systems. In Proceedings of the Ô¨Årst workshop on Self-healing systems ,
WOSS‚Äô02, pages 27‚Äì32, New York, NY , USA, 2002. ACM.
[13] P. Inverardi and M. Mori. Requirements models at run-time to support
consistent system evolutions. In Proceedings of the International
Workshop on Requirements@Run.Time , Req@Run‚Äô11, pages 1 ‚Äì8, aug.
2011.
[14] D. Kim and S. Park. Reinforcement learning-based dynamic adaptation
planning method for architecture-based self-managed software. In
Proceedings of the ICSE workshop on Software Engineering for Adaptive
and Self-Managing Systemsn , SEAMS‚Äô09, pages 76‚Äì85. IEEE, 2009.
[15] R. Kowalski and M. Sergot. A logic-based calculus of events. New Gen.
Comput. , 4(1):67‚Äì95, 1986.
[16] J. Kramer and J. Magee. Self-managed systems: an architectural
challenge. Future of Software Engineering, 2007. FOSE‚Äô07 , 0:259‚Äì268,
2007.
[17] L. Lamb, R. Borges, and A. Garcez. A connectionist cognitive model for
temporal synchronisation and learning. In Proceedings of the national
conference on artiÔ¨Åcial intelligence , volume 22, page 827. Menlo Park,
CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999, 2007.
[18] P. Oreizy, M. M. Gorlick, R. N. Taylor, D. Heimbigner, G. Johnson,
N. Medvidovic, A. Quilici, D. S. Rosenblum, and A. L. Wolf. An
architecture-based approach to self-adaptive software. IEEE Intelligent
Systems , 14(3):54‚Äì62, May 1999.
[19] P. Bertoli, A. Cimatti, M. Pistore, M. Roveri, P. Traverso. MBP: A
Model-Based Planner. Proceedings of IJCAI‚Äô01 Workshop on Planning
Under Uncertainty and Incomplete Information , 2001.
[20] N. Qureshi, A. Perini, N. Ernst, and J. Mylopoulos. Towards
a continuous requirements engineering framework for self-adaptive
systems. In Proceedings of the First International Workshop on
Requirements@Run.Time , Req@Run‚Äô10, pages 9 ‚Äì16, sept. 2010.
[21] M. Schoppers. Universal plans for reactive robots in unpredictable
environments. In IJCAI , volume 87, pages 1039‚Äì1046, 1987.
[22] M. Shanahan. An abductive event calculus planner. The Journal of
Logic Programming , 44(1-3):207‚Äì240, 2000.
[23] D. Sykes, W. Heaven, J. Magee, and J. Kramer. From Goals
to Components: A Combined Approach to Self-Management. In
Proceedings of the ICSE Workshop on Software Engineering for
Adaptive and Self-Managing Systems , SEAMS‚Äô08, 2008.
[24] D. Sykes, J. Magee, and J. Kramer. FlashMob: Distributed Adaptive
Self-Assembly. In Proceedings of the ICSE Symposium on Software
Engineering for Adaptive and Self-Managing Systems , SEAMS‚Äô11,
2011.
[25] H. Tajalli, J. Garcia, G. Edwards, and N. Medvidovic. Plasma: a plan-
based layered architecture for software model-driven adaptation. In
Proceedings of the IEEE/ACM international conference on Automated
software engineering , ASE ‚Äô10, pages 467‚Äì476, New York, NY , USA,
2010. ACM.71