Recurring Bug Fixes in Object-Oriented Programs
T ung Thanh Nguyen, Hoan Anh Nguyen, Nam H. Pham, Jafar Al-Kofahi, Tien N. Nguyen
Electrical and Computer Engineering Department
Iowa State University
{tung,hoan,nampham,jafar,tien}@iastate.edu
ABSTRACT
Previous research conﬁrms the existence of recurring bug
ﬁxesin software systems. Analyzing such ﬁxes manually,
we found that a large percentage of them occurs in code
peers, the classes/methods having the similar roles in the
systems, such as providing similar functions and/or partici-
pating in similar object interactions. Based on graph-based
representation of object usages, we have developed several
techniques to identify code peers, recognize recurring bug
ﬁxes, and recommend changes for code units from the bug
ﬁxes of their peers. The empirical evaluation on several
open-source projects shows that our prototype, FixWizard,
is able to identify recurring bug ﬁxes and provide ﬁxing rec-ommendations with acceptable accuracy.
Categories and Subject Descriptors
D.2.7 [ Software Engineering ]: Distribution, Maintenance,
and Enhancement
General Terms
Algorithms, Design, Reliability, Measurement
1. INTRODUCTION
A bug-ﬁxing change is considered recurring if it is repeated
identically or with relevant, slight modiﬁcations on several
code fragments at one or multiple revisions. Previous re-
search conﬁrms the existence of recurring bug ﬁxes [13, 23].
Such existence inspires us with many research questions:
Why, where, and how often do such changes recur? How
could they be characterized and recognized? And, impor-
tantly, how could we use them to help the developers to
ﬁx recurring bugs more eﬀectively? This paper aims to an-
swer such questions. Our ultimate goal is to build a semi-
automated tool that helps the developers in ﬁxing bugs by
recommending relevant, useful code changes based on theknowledge about the program and its recurring bug ﬁxes.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
ICSE ’10, May 2–8 2010, Cape Town, South Africa
Copyright 2010 ACM 978-1-60558-719-6/10/05 ...$10.00.We have conducted an empirical study in which 7 experi-
enced programmers manually examined all the bug ﬁxes oc-
curring in about a thousand of ﬁxing revisions in ﬁve popular
open-source projects containing several thousands of ﬁxing
changes. Some projects were also used in previous research
in recurring bug ﬁxes [13]. The result shows that in those
systems, there are about 17-45% of total ﬁxing changes that
could be considered as recurring. While many recurring ﬁxes
occur on multiple ﬁles at the same revisions (i.e. in space),fewer ﬁxes recur on diﬀerent revisions (i.e. in time). Some
of them recur in both time and space. Some recurring ﬁxing
changes are identical or have slight lexical modiﬁcations. A
few others are complicated and scattered, e.g., the ﬁxes in-
volving algorithmic modiﬁcations, transformations to a new
design pattern, or project-speciﬁc requirements. However,
they generally have similar syntactical changes to the code,
such as adding new statements or modifying current expres-
sions. Since we focus only on object-oriented code, we found
that most of recurring ﬁxes aﬀect object usages similarly, e.g.
adding the same/similar method invocations.
More interestingly, recurring changes often occur on the
parts of code that have certain interrelations such as meth-
ods being code clones, classes extending the same parentclass, classes implementing the same interface, methods over-
riding the same parent method, or classes belonging to the
same design pattern. Investigating such code units, we found
they are the representatives of a broader and more interest-
ing phenomenon. That is, in a large object-oriented pro-
gram, there tends to exist multiple objects that play similar
roles, provide similar functions , or perform similar inter-
actions with the other objects in the system. Those func-
tions/interactions are realized with similar code and/or in
similar object usage scenarios. Then, when the functions/in-
teractions need to be changed (e.g. due to bug ﬁxing), the
corresponding code/usages are changed in the similar man-
ner, resulting in recurring ﬁxes. In this paper, such meth-
ods/classes are called code peers . The term peeris used to
denote the similar roles of such code units.
Here is a simple example. In a graphic editor, diﬀerent
shape objects (e.g. Rectangle, Circle, Triangle) have similar
behaviors (e.g. drawing, moving, checking whether a point
lies in its area, etc). Some behaviors might be implementedsimilarly such as the drawing functions of Rectangle and
Triangle objects because one draws four lines and the otherdraws only three. In contrast, the drawings of Rectangle
and Circle could have diﬀerent concrete implementations.
Nevertheless, the interactions between each shape and the
editor as well as other graphical objects in drawing scenarios
315
would be similar. Assume that the drawing procedure of
the shapes in the editor is required to record the drawingoperations and parameters into a log ﬁle. Then, the drawingfunctions of all shapes will be added with similar code for
the logging task, resulting in recurring changes.
Using knowledge of code peers and recurring bug ﬁxes
learnt from the empirical study, we developed several tech-
niques to 1) identify code peers in object-oriented programs;2) recognize recurring bug ﬁxes; and 3) recommend ﬁxing
changes to code units from the ﬁxes of their peers. Those
techniques are realized in a prototype tool, FixWizard.
In FixWizard, code peers are formulated as the code units
(e.g. methods, classes) involving similar object interactions.Such interactions are represented by groum, a graph-based
representation for object usages in our previous work [21].To avoid pairwise checking for all pairs of code units, weuse several heuristics to ﬁnd the peer candidates. Generally,peer candidates are code units that 1) are similar in im-
plementation code or naming scheme, or2) share the same
ancestor method/class or implement the same interface(s),
i.e. promising the same set of functions, or3) belong to the
classes that have other code peers or recurring ﬁxes. Can-didates with suﬃcient similarity in functions/interactions
measured based on the object usage models extracted from
their implementation code and client code are considered aspeers and used for later recommendation of bug ﬁxing.
To characterize ﬁxing changes and recognize recurring ones,
FixWizard represents the change to a code unit as the changes
of the corresponding object usage models (i.e. groums). Us-
ing our previous tree edit scripting algorithm in [20], FixWiz-ard derives the changed nodes in the abstract syntax tree(AST) representing the code fragment. Then, it maps suchchanged AST nodes to the corresponding nodes of the groum,
connects such nodes into sub-groum(s), and considers them
belonging to the impact usage of the change. Eventually,
two changes are considered as recurring if their correspond-ing impact usages are suﬃciently similar.
Fixing recommendation is useful in two cases: 1) when
a fragment is ﬁxed, the tool recommends similar ﬁxes toits not-yet ﬁxed peers; and 2) while a developer is ﬁxing abuggy code fragment, it recommends the ﬁx derived from thesimilar ﬁxes of its peers in the past. To derive such recom-
mended ﬁxes, FixWizard ﬁrst determines the code elements
(e.g. methods, statements, expressions, variables) involvedin the change of the source peer X. Then, it maps them tothe corresponding code elements of the target peer Y basedon their similarity in structure and object usages. Even-
tually, for each changed element of X, the tool derives and
recommends the relevant editing operations and parametersfor its mapped elements in Y.
We have conducted an empirical experiment to evaluate
the correctness and usefulness of our approach. The resultsshow that, in the recognition of recurring bug ﬁxes, FixWiz-ard achieves a high accuracy level with average precision of81% and recall of 74%. In ﬁxing recommendation, FixWiz-ard suggests correct locations and necessary coarse-grained
operations such as adding/deleting/modifying methods and
statements with 49% precision and 71% recall on average.
The key contributions of this paper include1. An empirical study on recurring bug ﬁxes that provide
insight observations on such changes. It provides the evi-
dence conﬁrming that a large percentage of recurring bug
ﬁxes occurs at code peers, i.e. methods/classes having sim-Project App. Type Revision Range Fixes
ArgoUML Graphic Modeling 2 - 1130 2318
Columba Mail Client 4 - 370 829
ZK Ajax Framework 2400 - 6200 490
FlashRecruit Job Listings 100 - 600 1007
gEclipse Dev Environment 4 0 0-1 0 3 0 0 1126
Table 1: Subject Systems
Project RBF Percentage In Space In Time Both
ArgoUML 390 16.8% 96.9% 17.2% 14.1%
Columba 377 45.4% 88.8% 17.7% 6.5%
ZK 188 38.4% 91.5% 13.3% 4.8%
FlashRecruit 244 24.2% 85.3% 22.1% 7.4%
gEclipse 215 19.1% 89.1% 27.7% 16.8%
Table 2: Manually Identiﬁed Recurring Fixes
ilar functions and interactions in the system.
2. New concepts, formulations, and algorithms to identify
code peers, to characterize and recognize recurring ﬁxes, andto recommend such changes to the code peers.
3. An empirical evaluation shows the correctness and use-
fulness of our approach.
Section 2 describes our empirical study on recurring ﬁxes.
Sections 3, 4 present our approach. Evaluation is in Section
5. Related work is in Section 6. Conclusions appear last.
2. EMPIRICAL STUDY
2.1 Hypotheses
In object-oriented programming, a software system is mod-
eled via objects and their interactions, which are realized in
the classes/methods providing the abstraction to the objectsand their behaviors. The interactions of an object Otoward
other objects are expressed in the implementation code of itsclass/methods, in which it uses the other objects (internalusage). In contrast, the interactions of other objects towardOare expressed in its client code within other classes/meth-
ods in which it is used by other objects (external usage). Ineither case, the interactions of the objects could be realizedvia object usages, i.e. method invocations, ﬁeld accesses,their usage orders, and the relevant control structures.
In a large-scale system, there tends to exist several objects
having similar functions and/or interactions with other ob-
jects. Thus, in the program, such similar functions/interac-tions are implemented by classes and methods having sim-ilar object usages, which we call code peers . Bug ﬁxing is
to change the functions and/or interactions of objects. Be-
cause similar functions and/or interactions usually need to
be changed in the similar ways, we hypothesize that similar
ﬁxing changes would often occur on code peers (H1).
As conventional in object-oriented programming, the ob-
jects with similar functions will often be abstracted into par-ent classes. The speciﬁc behaviors are implemented in thechildren classes. In other cases, the methods/classes mightnot be implemented in the similar ways, but they implementthe same interface, i.e., promise the similar functions. The
other objects could interact in the same way with the ob-
jects in such classes via their promised methods. The class-es/methods having similar functions and/or being relatedvia inheritance/interface will often be named similarly bythe developers to help themselves in better understanding
the roles of such classes/methods. In other cases, to im-
plement the methods/classes having similar functions, de-
316public void setColspan( intcolspan) throws WrongValueException {
if(colspan <=0 ) throw new WrongValueException(...);
if(colspan != colspan) {
colspan = colspan;
ﬁnal Execution exec = Executions.getCurrent();
if (exec != null && exec.isExplorer()) invalidate() ;
smartUpdate(” colspan” , Integer.toString( colspan));...
public void setRowspan( introwspan) throws WrongValueException {
if(rowspan <=0 ) throw new WrongValueException(...);
if(rowspan != rowspan) {
rowspan = rowspan;
ﬁnal Execution exec = Executions.getCurrent();
if (exec != null && exec.isExplorer()) invalidate();
smartUpdate(” rowspan” , Integer.toString( rowspan));...
Figure 1: Bug Fixes at v5088-v5089 in ZK
Usage in method colSpan Usage in method rowSpan 
Usage in changed codeExecutions.getCurrent
Execution.isExplorerIF
WrongValueException .< init > IF
Auxheader.smartUpdateAuxheader.invalidateIFExecutions.getCurrent
Execution.isExplorerIF
WrongValueException .< init > IF
Auxheader.smartUpdateAuxheader.invalidateIF
Figure 2: Graph-based Object Usages for Figure 1
velopers tend to copy-and-paste the implementation code,
thus creating similar code fragments. Therefore, we hypoth-esize that code peers, i.e. classes/methods having similarfunctions/interactions, tend to have similar implementation
code,similar naming schemes, inherit from the same class,
or implement the same interface (H2).
2.2 Manual Analysis of Recurring Fixes
We conducted a manual analysis of recurring bug ﬁxes in
a two-phase experiment. First, a group of experienced pro-grammers examined all ﬁxing changes of the subject systems
and manually identiﬁed the similar ones. Then, we analyzed
their reports to characterize such recurring ﬁxes and theirenclosing code units in order to verify the main hypothesisH1: similar ﬁxes tend to occur on code units having similarroles, i.e. providing similar functions and/or participating
in similar interactions, in term of object usages.
We represented object usages in such code units by graph-
based object usage model, a technique in our previous work
GrouMiner [21]. In general, each usage scenario is modeled
as a labeled, directed, acyclic graph, called a groum, in which
nodes represent method invocations/ﬁeld accesses of objects,
as well as control structures (e.g. if, while) and edges repre-sent the usage orders and data dependencies among them.
Table 1 shows subject systems used in our study. Two of
them were also used by Kim et al. [13] in previous research
on bug ﬁxes. The ﬁxes are considered at the method level,i.e. all ﬁxing changes to a method at a revision of a sys-tem are considered as an atomic ﬁx. Seven Ph.D. studentsin Software Engineering at Iowa State University with the
average of 5-year experience in Java manually examined all
those ﬁxes and identiﬁed the groups of recurring bug ﬁxespublic class UMLOperationsListModel extends
UMLModelElementCachedListModel {
public void add( intindex) {
Object target=getTarget();
if(target instanceof MClassiﬁer) {
MClassiﬁer classiﬁer=(MClassiﬁer)target;
Collection oldFeatures=classiﬁer.getFeatures();
MOperation newOp=MMUtil.SINGLETON.buildOperation(classiﬁer);
classiﬁer.setFeatures(addElement(oldFeatures,index,newOp,
operations.isEmpty()?null: operations.get(index)));
public class UMLAttributesListModel extends
UMLModelElementCachedListModel {
public void add( intindex) {
Object target=getTarget();
if(target instanceof MClassiﬁer) {
MClassiﬁer classiﬁer=(MClassiﬁer)target;
Collection oldFeatures=classiﬁer.getFeatures();
MAttribute newAt=MMUtil.SINGLETON.buildAttribute(classiﬁer);
classiﬁer.setFeatures(addElement(oldFeatures,index,newAt,
attributes.isEmpty()?null: attributes.get(index)));
Figure 3: Bug Fixes at v0459-v0460 in ArgoUML
Usage in UMLOperationsListModel.addElement 
IF
MClassifier.getFeatures
MMUtil.buildOperation
MClassifier.setFeaturesUMLOperationsListModel.addElementList.getList.isEmptyUsage in UMLAttributesListModel.addElement 
IF
MClassifier.getFeatures
MMUtil.buildAttribute
MClassifier.setFeaturesUMLAttributesListModel.addElementList.getList.isEmpty
Figure 4: Graph-based Object Usages for Figure 3
(RBFs). Conﬂicting identiﬁcations were resolved by the ma-jority vote among them. There were only 2 disputed groups.
Table 2 shows the collective reports. Columns
RBFand
Percentage show the total numbers and the percentage of re-
curring bug ﬁxes in all ﬁxing ones. We can see that RBFsare between 17-45% of all ﬁxing changes. This is consistent
with the previous report [13]. While many RBFs (85%-
97%) occur at the same revisions on diﬀerent code units(column
In Space ), less RBFs occur in diﬀerent revisions (col-
umn In Time ). Analyzing such recurring ﬁxes, we found that
most of them (95%-98%) involve object usages (e.g. method
calls and ﬁeld accesses). This is understandable because the
study is focused on object-oriented programs.
2.3 Representative Examples
Example 1 . Figure 1 shows two recurring ﬁxes taken from
ZK system with added code shown in boxes . Two methods
setColspan and setRowspan are very similar in structure and
function, thus, are considered as cloned code. When theirfunctions need to be changed, they are changed in the sameway. Figure 2 shows the object usage models of those twomethods with the changed parts shown in the boxes. The
nodes such as
Executions.getCurrent and Auxheader.smartUpdate
represent the invocations of the corresponding methods. An
edge such as the one from Executions.getCurrent toExecution.is-
Explorer shows the usage order, i.e. the former is called before
the latter. As we could see, both methods are implemented
with the same object usage. Then, they are also modiﬁed in
the same way as shown in the boxes.
317public class TableController implements TreeSelectionListener {
public TableController(MailFrameController mailFrameController) {
this.mailFrameController=mailFrameController;
headerTableItem=(TableItem)MailConﬁg.getMainFrameOptionsConﬁg().
getTableItem();
headerTableModel= new HeaderTableModel(headerTableItem);
view=new TableView(headerTableModel);
tableSelectionManager= new TableSelectionManager();
mailFrameController.getSelectionManager()
.addSelectionHandler(new TableSelectionHandler(view));
tableChangedListenerList= new Vector();
actionListener= new HeaderTableActionListener( this); ...
public class TreeController implements TreeSelectionListener {
public TreeController(MailFrameController mailFrameController, TreeModel
model){
this.model=model;
this.mailFrameController=mailFrameController;
view=new TreeView(model);
actionListener= new FolderTreeActionListener( this);
treeSelectionManager= new TreeSelectionManager();
mailFrameController.getSelectionManager()
.addSelectionHandler(new TreeSelectionHandler(view));
view.addTreeWillExpandListener( this); ...
Figure 5: Bug Fixes at v0224-v0225 in Columba
Example 2 . Figure 3 shows another example of recurring
ﬁxes. The two methods also have similar code. In fact, they
override the same method in the common parent of their
classes. Analyzing two classes, we found that the majorityof their methods are similar. In other words, they could beconsidered as clones in class design, i.e. having the similarroles, both in function and interaction. The groums rep-
resenting the interactions of those two methods with other
classes/methods are shown in Figure 4. They have identicalstructures, and if we consider
buildOperation and buildAttribute
ofMMUtil ,a sw e l la st w o addElement so f UMLOperationsList-
Model and UMLAttributesListModel having the same role, the
two usages could be considered representing the same rou-tines. Since two
Listobjects operations and attributes are
used in the same way (as a caching mechanism), their us-ages are changed in the same manner, i.e
isEmpty should be
checked before using method geton the Listobject.
Example 3 . Figure 5 shows a more interesting case. The
changes (in the boxes) are very similar although the enclos-ing methods are not much similar to each other as in theprevious examples. However, analyzing the usages of two
enclosing classes
TableController and TreeController , we found
that they are used only once, and used together, in class
ThreePaneMailFrameController . Figures 6 and 7 show the code
and the groums representing their usage scenarios. It couldbe seen that such two classes are used in the similar ways in
ThreePaneMailFrameController (and thus, in the whole system).
This explains why their constructors are changed similarly,
resulting in recurring ﬁxes. That is, they need to inter-act to their respective
MailFrameController object in the same
manner (i.e. adding to its SelectionManager a relevant Selec-
tionHandler object for their corresponding views).
Another interesting point is that, the interaction of Table-
Controller toTableView ,TableSelectionManager ,a n d TableSelec-
tionHandler isidentical to that of TreeController toTreeView ,
TreeSelectionManager ,a n d TreeSelectionHandler . Examining such
classes, we found that they follow the Model-View-Controllerpublic ThreePaneMailFrameController(ViewItem viewItem) {
...
trCtrl= new TreeController( this, FolderTreeModel.getInstance());
tbCtrl=new TableController( this);
TableSelectionHandler tbHdl= new TableSelectionHandler(tbCtrl);
getSelectionManager().addSelectionHandler(tbHdl);
TreeSelectionHandler trHdl= new TreeSelectionHandler(trCtrl.getView());
getSelectionManager().addSelectionHandler(trHdl);
tbCtrl.getView().addMouseListener( new TableMouseListener());
trCtrl.getView().addMouseListener(new TreeMouseListener());...
Figure 6: External Usages of Code Units in Figure 5
TableController .< init > 
TableSelectionHandler .< init > 
SelectionManager.addSellectionHandlerTreeController .< init > FolderTreeModel.getInstance
TreeSelectionHandler .< init > 
SelectionManager.addSellectionHandlerThreePaneMailFrameController .getSelectionManager ThreePaneMailFrameController .getSelectionManager 
TableController.getView TreeController.getView
TableView.addMouseListenerTableMouseListener .< init > TreeMouseListener .< init > TreeController.getView
TreeView.addMouseListener
Figure 7: Graph-based Object Usages for Figure 6
(MVC) design pattern. Therefore, they in pairs have the
identical roles. We also found many similar cases in which
two methods/classes have similar interactions with other ob-jects in one or multiple usage scenarios, although they arenot implemented in the similar ways or do not belong to thesame class hierarchy/interface.
Example 4 . This example is taken from two classes
Prop-
PanelClass and PropPanelNode at the revision 0569 of ArgoUML
project. We found many instances of the modiﬁcation from
addCaption(” Extends:” ) toaddCaption(” Specializes:” ) .
Observations . The code units in ﬁrst three examples share
the common nature that they have similar object interac-
tions, in term of object usages, as represented by groums.
Such similarity could be seen in their implementation code
(Examples 1 and 2) or in their client code (Example 3).
Since they all have similar roles in the system, we call them
code peers . In contrast, the ﬁxes as in Example 4 are due
to system requirements, and occur in several classes andmethods having no peer relations.
2.4 Analysis of Enclosing Code Units
Table 3 shows the result of our semi-automatic analysis
on the locations of recurring ﬁxes. Column RBFshows the
total number of recurring ﬁxing changes. Column IdUsage
shows the percentage of ﬁxes recurring in methods that haveidentical object usages (Example 1). Column
SimInt refers
to the code units which do not have exact object usages,
Project RBF IdUsage SimInt SimExt Others
ArgoUML 390 24% 56% 8% 12%
Columba 377 15% 70% 6% 9%
ZK 188 22% 62% 8% 8%
FlashRecruit 244 27% 54% 5% 14%
gEclipse 215 22% 60% 7% 11%
Table 3: Locations of Recurring Fixes
318but have similar object interactions (Example 2). Column
SimExt refers to the ﬁxes to code peers that are used sim-
ilarly, but are not implemented with similar object usages
(Example 3). Column Others refers to other recurring ﬁxes.
The numbers show that a large percentage (86%-92%) of
recurring ﬁxes are at code peers (the sum of three columns
IdUsage ,SimInt ,a n d SimExt ). Many of code peers with re-
curring ﬁxes are closely related in the inheritance hierarchy,or share the same interface(s), or have similar implemen-
tations, object usages, and naming schemes. This result
conﬁrms both of our hypotheses H1 and H2.
2.5 Implications
The empirical study gives us the following observations:1. A considerable portion of ﬁxing changes (17%-45%)
are actually recurring. Most of them (85%-97%) are madeat the same revision.
2. A large percentage of recurring ﬁxes (86%-92%) occurs
at code peers, i.e. methods/classes having similar functionsand/or object interactions.
3. Code peers and their recurring changes involve similar
object usages (e.g. method invocations and usage orders).
Peer classes tend to have several peer methods.
4. Code peers often have similar implementations or nam-
ing schemes, and are related via inheritance or interface.
Observation 1 implies the necessity of a recommendation
tool helping developers with recurring ﬁxes. Observation 2suggests that the tool could be based on code peers, i.e.
identifying code peers and recommending the ﬁxes for them.Observation 3 implies that the code peers and the recurringﬁxing changes to them could be identiﬁed based on objectinteractions. Observation 4 provides more information for
ﬁnding the candidates of code peers.
We have built such a recommendation tool, FixWizard,
based on those implications. In general, its main task is
toidentify code peers and when one peer changes, it rec-
ommends the ﬁx to other peers . FixWizard identiﬁes code
peers and the recurring ﬁxes via object usages. That is, codepeers are code units (methods/classes) having similar objectusages, internally (i.e. in their implementation code) and/orexternally (i.e. in the code using them). Recurring ﬁxes at
code peers are also the changes involving in similar object
usages. The formulation and algorithms of our approach willbe discussed in the next sections.
3. FORMULATION
3.1 Code Peer and Usage Similarity
Definition 1 (Internal/External Usage). Internal
usage of a method A.m, denoted by UI(A.m), is the set of
all groums and sub-groums (subgraphs) in its implementa-
tion code. External usage of A.m, denoted by UE(A.m),i s
the set of all groums and sub-groums in the implementationcode in the system, that could have an invocation of A.m.
This deﬁnition also takes into account dynamic binding
in object-oriented programming. That is, an invocation ofA
0.morI.mmight actually be an invocation of A.mifAis
a subclass of class A0orAimplements interface I.
Definition 2 (Peer). Two methods are peers if and
only if (iﬀ) the usage similarity, measured by a functionSim, of their respective internal or external usages exceeds
a pre-deﬁned threshold. Two classes are peers iﬀ the numberof their peer methods exceeds a chosen threshold.
Peer relation between methods/classes is denoted by ≡.
It is reﬂexive (a method/class is a peer of itself), symmetric(i.e. if xis a peer of y,t h e n yis also a peer of x), and not
transitive. Deﬁnition 2 could be written as: A.m≡B.niﬀ
Sim(U
I(A.m),U I(B.n))≥σ1orSim(U E(A.m),U E(B.n))≥
σ2, in which σ1andσ2are chosen thresholds. The formula-
tion of Sim, the usage similarity measure between any two
sets of graph-based usages, will be presented next.
Definition 3 (Peer-isomorphic Usage). Two groums
arepeer-isomorphic , if there exists a bijective (one-to-one)
mapping for their nodes such that the mapped nodes repre-sent the invocations of the same or peer methods, and their
usage orders are the same.
Figure 4 shows an illustrated example for peer-isomorphic
usages. Assume that
buildOperation and buildAttribute ofMMU-
til,a sw e l la st w o addElement so f UMLOperationsListModel and
UMLAttributesListModel are peer methods. Then, all the nodes
between two groums in Figure 4 could be mapped while the
usage orders are still preserved. Thus, the two usages arepeer-isomorphic. Note that peer-isomorphism for 2 groums
subsumes label-isomorphism (peer relation is reﬂexive).
However, the usages could not always be peer-isomorphic.
They could be similar as in Figure 7. Therefore, we deﬁne
the similarity of two object usages as follows.
Definition 4 (Usage Similarity). Given two groums
GandH. Assume that G
oandHoare their largest peer-
isomorphic sub-groums, respectively (the size of groum ismeasured by the number of nodes). Then, the usage simi-
larity of GandHis deﬁned as gsim(G, H)=
|G o|+|H o|
|G|+|H |.
Let us revisit Figure 7. Assume that all corresponding
methods of TableXXX classes and TreeXXX classes are peer
methods. Then, two graphs could be mapped such that two
peer-isomorphic subgraphs have their sizes up to 7 nodes.(Two methods
FolderTreeModel.getInstance and TreeController.
getView could not be mapped). Thus, the similarity of two
usages is (7 + 7) /( 7+9 )=0 .88.
Using the usage similarity gsimfor any pair of groums, we
could deﬁne function Simused in Deﬁnition 2 that measures
the usage similarity of two methods as in the following.
Definition 5 (Similarity of Two Usage Sets). Given
two sets of groums UandV. Their usage similarity, Sim(U, V),
is the ratio between the total usage similarity of the maxi-mum weighted matching between the members of UandV
and their average size.
This deﬁnition could formally written as
Sim(U, V)=max
M/summationtext
(G,H )∈Mgsim(G, H )
(|U|+|V|)/2
for all M={(G, H )|G∈U, H∈V}such that
∀(G, H ),(G/prime,H/prime)∈M:G=G/prime⇔H=H/prime
For example, assume that two methods have the external
usage sets {G1,G2}and{H 1,H2}, respectively. The usage
similarity of each pair is sim(G1,H1)=0.84,sim(G1,H2)=
3190.36,sim(G2,H1)=0.54, and sim(G2,H2)=0.78. Then,
the maximum matching of two sets is ( G1,H1)a n d( G2,H2),
with the total similarity is 0.84 + 0.78 = 1.62. Since their
average size is 2, the usage similarity is 1 .62/2=0.81.
3.2 Recurring Changes in Code Peers
If code peers are modiﬁed, their corresponding object us-
ages (represented by groums) tend to be changed. Thechange of a groum might include the added, deleted, re-
labeled, or edge-changed nodes. For example, in Figure 4,
the node
List.isEmpty is added. Then, three nodes ( List.get ,
UMLOperationsListModel.addElement ,a n d MClassiﬁer.setFeature )
are edge-changed, because they have the added edges due tothe addition of
List.isEmpty . In this case, we could say that
the change aﬀects all four nodes, and such impact could berepresented by the sub-groum containing them.
Of course, the change of a groum could aﬀect several
nodes, and they might belong to diﬀerent usages, i.e. thechange might aﬀect diﬀerent sub-groums. Two disconnected
sub-groums are considered belonging to diﬀerent usages, since
if they had dependency, they would have been connected.Therefore, the impact of a change is modeled as a set ofconnected sub-groums.
Definition 6 (Impact Usage). Impact usage of a change
to a code peer is the set of connected sub-groums of thechanged nodes in the groum of that code peer.
Since code peers have similar object usages, if their object
usages are changed in the similar ways, i.e. having simi-lar impacts on the corresponding groums, we could considersuch changes as recurring.
Definition 7 (Recurring Changes). Two changes are
recurring, if their impact usages are suﬃciently similar.
For example, in Figure 1 and Figure 3, the changes are
recurring since their respective impact usages are identical(as in Figure 2) or peer-isomorphic (as in Figure 4).
4. ALGORITHMIC SOLUTION
In this section, we will discuss three algorithms to 1) iden-
tify code peers, 2) recognize recurring bug ﬁxes, and 3) de-rive the recommended ﬁx for a code peer from the ﬁx of its
peer. Algorithms 1 and 3 are used in the recommendation
task. Algorithm 2 is needed to record/recognize the recur-ring ﬁxes, which helps in verifying and improving the cor-rectness of code peer identiﬁcation, which in turn improvesthe ﬁxing recommendation.
4.1 Code Peers Identiﬁcation
If we use pair-wise comparison between all methods to
identify all code peers as in Deﬁnition 2 in Section 3, thecomputational cost could be expensive because:
1) In large systems, the number of methods could be huge,
which makes pair-wise comparison too expensive.
2) Finding maximum peer-isomorphic subgraphs to calcu-
late the similarity between two usage sets is hard (ﬁndingmaximum isomorphic subgraphs is already NP-hard).
3) There is a possibility that the computation of peer-
isomorphic subgraphs would result in an inﬁnite loop dueto the recursive nature of the deﬁnition of code peers. For
example, in Figure 5, to calculate the similarity of the exter-
n a lu s a g eo f
TableController. <init>and TreeController.<init >inThreePaneMailFrameController (Figure 7), we need to check the
peer relation of TableSelectionHandler.<init >and TreeSelection-
Handler.<init >. However, both internal and external usages
of two XXXSelectionHandler ’s constructors use two XXXCon-
troller’s constructors, respectively. Thus, the peer checking
for two XXXSelectionHandler ’s constructors requires the peer
checking for two XXXController ones. This recursive checking
could cause inﬁnite computation.
We design an approximate algorithm for code peer iden-
tiﬁcation using the following ideas:
1. Instead of pair-wise comparison for all methods, FixWiz-
ard uses Observation 4 (Section 2.5) to ﬁnd candidates foridentiﬁcation of code peers. Generally, it checks the peerrelation for only the methods/classes that 1) are similar in
their implementation code or naming scheme, or2) share
the same ancestor method/class or implement the same in-
terface(s), i.e. promising the same set of functions, or3)
belong to the classes that have other peers or recurring ﬁxes.
2. Graph-based usage similarity is computed approxi-
mately. Instead of ﬁnding maximum peer-isomorphic sub-graphs of two groums to calculate their similarity, FixWiz-ard extracts their characteristic features. If such features aresimilar, the corresponding groums are considered as similar.
3. To avoid the possibility of recursive calculation of peer
relations, FixWizard iteratively calculates the usage similar-ity of candidates using only already-identiﬁed peers. Whenany candidates are identiﬁed as peers, they will be used toupdate the usage similarity of the remaining candidates.
4.1.1 Feature-based Usage Similarity
In [21], we proposed the use of structural features to com-
pare the similarity of groums (labeled, directed, acyclic graphs).We extend that technique to support peer-based similarity.
Definition 8.A feature, extracted from a path within a
groum, is the sequence of method names represented by thenodes along that path.
For example, in Figure 4, the extracted features could be
[List.isEmpty] (path of size 1), [List.isEmpty]-[List.get] (path of
size 2), [List.isEmpty]-[List.get]-[UMLOperationsListModel.addElement]
(path of size 3), etc. Such features could describe an objectusage approximately, such as the method invocations, theirusage orders, and the interactions between objects (by se-quences of method calls) in the usage.
Definition 9 (Similar Feature). Two features x=
x
1−x2−...−xnandy=y1−y2−...−ynare considered
similar, denoted as x≈y,i fx i≡yifor all i.
For example, if addElement methods of UMLOperationsList-
Model and UMLAttributesListModel are peers, then two features
[List.isEmpty]-[List.get]-[UMLOperationsListModel.addElement] and
[List.isEmpty]-[List.get]-[UMLAttributesListModel.addElement] are
considered similar. The similarity of two feature sets is de-ﬁned using the following deﬁnition:
Definition 10 (Similarity of Two Feature Sets).
Similarity of two feature sets XandY, denoted as fsim(X,Y),
is the ratio between the size of their maximum matching
based on the similar feature relation and their average size.
This could be written formally as fsim(X,Y)=
max F|F|
(|X|+|Y|)/2
for all F={(x, y)|x∈X,y∈Y,x≈y}such that
∀(x, y),(x/prime,y/prime)∈F:x=x/prime⇔y=y/prime.
320
1 IdentifyCodePeer( Prog )
2M.add(SimilarMethod( Prog ))//add cloned methods as candidates
3C.add(SimilarClass( Prog ))//find similar classes and
4C.add(SimilarFixedClass( Prog ))//classes with recurring fixes
5M.add(SimilarNamedMethod( C))//match methods as candidates
6 do
7( A.m, B.n )=M.next() //repeatedly process candidates
8 ifSim(U I(A.m),U I(B.n))≥σ1or
Sim(U E(A.m),U E(B.n))≥σ2//if similar enough
9 move(A.m, B.n )from MtoPM//add as peers
10 C.add((A, B))//and check enclosing classes
11 M.add(SimilarNamedMethod( (A, B))) // for new candidates
12 while new peers are still identified
13 PC.add(PeerClass( C))//find peer classes

 
Figure 8: Code Peer Identiﬁcation
The similarity of two groums is measured by the similarity
of its two feature sets, i.e. function fsim is used, instead of
gsim in the calculation of function Simin Deﬁnition 5.
4.1.2 Code Peer Identiﬁcation Algorithm
Figure 8 shows the algorithm for identifying code peers.
As any time, we have the persistent lists of identiﬁed code
peers and candidates: PCandCfor classes and PMandM
for methods. Each element of such list is a pair of classes
or methods. The algorithm works by iteratively updatingthe elements of those lists. It is run incrementally for eachrevision, i.e. whenever new code is added.
Step 1. Find candidates . Structural clones are de-
tected by our incremental clone detection algorithm [20].Pairs of cloned methods are added to the candidate list M
(line 2,
function SimilarMethod ).
Candidates are also scanned from the classes that have
similar interface, or inheritance, or names (line 3, function
SimilarClass ), or used to have recurring ﬁxes reported from
the previous revisions (line 4, function SimilarFixedClass ).
In function SimilarClass , for each class, the extracted fea-
tures include its name, its parent name, the interface(s) itimplements, the names of its methods/ﬁelds. Then, theclasses are compared pair-wise to ﬁnd the ones having sim-ilar features. Classes/methods’ names are compared as fol-lows. First, each name is separated into words. For ex-
ample,
UMLOperationsListModel is separated into UML,Oper-
ations,List,a n d Model. Then, the similarity of two names,
as two sequences of words, are calculated based on their
largest common subsequence. For example, UML-Operations-
List-Model and UML-Attributes-List-Model will be matched re-
spectively. Their largest common subsequence has the sizeof 3. Thus, the overall similarity is (3+3)/(4+4)=0.75.
For each pair of candidate classes in C, their methods are
matched based on the similarity of their names (function
SimilarNamedMethod ), and are added to the list M.
Step 2. Evaluate candidates . Peer candidates (pairs
of methods) in Mare stored as a descending sorted list based
on their current usage similarity (either of internal usage orexternal usage, whichever higher). Such usage similarity is
calculated via features (i.e. using fsim in Deﬁnition 10),
and the features are compared using identiﬁed peers in P
M
only. That is, features having the names of the methods
that are not determined as peers yet will not be consideredas similar to any other feature (Deﬁnition 9).
Each pair of candidates having usage feature similarity
(internal or external) larger than chosen thresholds (line 8)
1 RecognizeRecurringFixes( Fixes )
2 for each Δ∈Fixes
3 IU(Δ) = ImpactUsage( Δ)//extract impact usage
4 for each pair of changes Δ,Δ/prime//pair-wise comparison
5 ifSim(IU(Δ),IU(Δ/prime))≥σ3//if impacts are similar
6 RBF .add(Δ,Δ/prime)//report as recurring fixes

 
Figure 9: Recurring Fixes Recognition
will be moved from MtoPM(line 9). Their corresponding
classes are considered as candidate classes (line 10). Othermethods are then matched (function
SimilarNamedMethod )t o
get new candidates for adding into M(line 11). This step
repeats until all candidates are evaluated and no new peer is
detected. Finally, after all peer methods are identiﬁed, the
candidate classes are evaluated to ﬁnd peer classes (line 13).
4.2 Recurring Fixes Recognition
Figure 9 shows the algorithm to recognize recurring ﬁxes.
Generally, it ﬁrst extracts the impact usages of all ﬁxingchanges, and then compares them to identify the ﬁxes withsuﬃcient similar impact usages as the recurring ones.
Step 1. Extract Impact Usages . For each ﬁx Δ in the
setFixes of all ﬁxes, the algorithm parses two versions of
the corresponding code unit into two ASTs and builds twocorresponding groums. Then, it uses Treed [20], a tree edit-
ing algorithm, to detect all AST node-level tree edit oper-
ations, i.e. inserting ,deleting ,updating (relabeling), and
moving an AST node. Using the mapping between AST
nodes and groums nodes, it determines the changed nodesof the groums corresponding to the changed nodes in the
AST. From those nodes, it traverses the groums to ﬁnd edge-
changed nodes and connects all the changed nodes into con-nected sub-groums to form the impact usage of the change.
Let us illustrate this via the example in Figure 3. Based
on Treed, the algorithm knows that the AST node of type
Method Invocation
operations.isEmpty() isadded. Thus, the
corresponding node List.isEmpty in the groum is determined
asadded. This addition also adds new edges from List.isEmpty
to three other nodes ( List.get ,MClassiﬁer.setFeature ,a n d UML-
OperationsListModel.addElement ) due to the changes in usage
orders (Figure 4). From List.isEmpty , the algorithm traverses
through such edges and detects those three edge-changed
nodes. Then, the sub-groum of those four nodes is extractedas the impact usage of the change.
Step 2. Clustering. After the impact usages of all ﬁxes
are extracted, FixWizard compares them pair-wise. Pairswith suﬃciently similar impact usages are added to the listof detected recurring bug ﬁxes RBF. Similar to the peer
detection algorithm, this comparison uses the usage feature
similarity, i.e. function fsim, in calculating the total simi-
larity Simbetween two impact usages of any pair of ﬁxes.
4.3 Fixing Recommendation for Code Peers
Because recurring ﬁxes are modeled by the impact us-
ages, the recommendation should be also represented as the
change operations to the groums (i.e. inserting, deleting, re-labeling, changing nodes/edges in groums, etc). However,to make the ﬁxing changes more readable and instructiveto developers, FixWizard recommends bug ﬁxes via change
operations at the syntactic level.
Figure 10 shows the recommendation algorithm. It is used
321
1 RecommendFix( X,ΔX)
2 for each Y∈PeerOf( X)//for each peer of X
3 X∗= Affect( X,ΔX)//detect affected sub-trees of X
4 M= Map( X∗,Y)//map them and other code elements to Y
5 for each mapped pair (x, y)∈M//for the mapped elements
6 O= DeriveOperation( x,y)//derive the relevant operation
7 Recommend( O)//to recommend

 
Figure 10: Fixing Recommendation for Code Peers
w h e nac o d eu n i t Xis ﬁxed by a ﬁxing change Δ X, and the
tool derives the recommended changes for every code peer Y
ofX(Xmight have no peer or multiple peers). To do that,
the algorithm ﬁrst determines the impact usage of Δ XtoX
as in Section 4.2. Via the mappings between changed nodes
of the groum and the AST, it identiﬁes the changed sub-trees
in AST relevant to the change in object usage of X(line 3).
Then, those sub-trees are mapped into the corresponding
sub-trees in Y(line 4). Each pair of sub-trees is mapped
based on their structural similarity and usage similarity of
the sub-groums extracted from those two sub-trees. Then,
the mapped sub-trees are used to map other code elements,such as ﬁelds, variables, types, method invocations. Finally,for each mapped element xinX, if it is aﬀected by a tree
edit operation, we will suggest the corresponding operation
to its mapped element yinY(lines 5-7).
Let us revisit Figure 3. Assume that the above method
Xchanges, and the tool needs to recommend for the below
method Y. First, the algorithm determines the change to the
expression containing
operations.isEmpty() . It knows that the
corresponding groum node List.isEmpty is added. It could de-
termine the statement containing MClassiﬁer.setFeatures and
the ifstatement as the relevant sub-trees. It then maps those
subtrees to the corresponding ones of Y.
From the mapped expressions, the algorithm is able to
map two variables operations and attributes . Thus, the ad-
dition of operations.isEmpty() is used to derive the addition
of attributes.isEmpty() forY. Other added nodes of the ex-
pressions are the same. After applying such operations intothe corresponding sub-trees in Y, it outputs them as texts
for the recommendation.
Similarly, for the example in Figure 5, the algorithm de-
rives the correct addition of the statement. However, since
it could not map two types
TableSelectionController and TreeS-
electionController , the recommendation for the below method
contain the incorrect class name TableSelectionController .T h u s ,
in the current implementation, we stop at the recommenda-tion of the locations of code peers at the method and state-
ment levels, with the ﬁrst change operation. From there,
developers could be able to complete the changes.
5. EV ALUATION
We implemented those three algorithms in a prototype
tool named FixWizard. Its main function is to identify thecode peers in the program, and when a code unit Xis ﬁxed,
it recommends the similar ﬁxes to all the code peers of X(if
any). Currently, FixWizard could recommend the locations
(e.g. class, method, statement) and operations (e.g. add or
delete a method, a statement, a method invocation) for theﬁxes. It is also able to analyze the history of bug ﬁxes in
any period, then, recognize and report the recurring ones.
We performed an empirical study to evaluate FixWizardSystem Class Method RBF Prec. Rec. Fscore
ArgoUML 1063 2318 390 65% 70% 67%
Columba 1161 829 377 87% 73% 79%
ZK 295 490 188 91% 80% 85%
FlashRecruit 665 1007 244 84% 75% 79%
gEclipse 672 1126 215 78% 70% 74%
Table 4: Recognition Accuracy
Accuracy from ZK
8090100Accuracy from ZK
Precision
60708090100Accuracy from ZK
Precision
Recall
30405060708090100Accuracy from ZK
Precision
Recall
2030405060708090100Accuracy from ZK
Precision
Recall
2030405060708090100
RevisionAccuracy from ZK
Precision
Recall
2030405060708090100
RevisionAccuracy from ZK
Precision
Recall
Figure 11: Recognition Accuracy on ZK
regarding those two functions. In the evaluation, we speci-
ﬁed the parameters for FixWizard with the feature size be-tween 1-4 and threshold of 0.75 for all similarity measure-
ments. All experiments were carried out in a computer with
Intel Core 2 Duo 2Ghz, 3GB RAM, and Windows XP.
5.1 Recurring Fixes Recognition
To evaluate the recognition of recurring ﬁxes in FixWiz-
ard, we execute it on the subject systems whose recurring
ﬁxes were manually veriﬁed as described in Section 2. Met-
rics for performance evaluation are precision, recall, and fs-core. The precision value is deﬁned as the number of cor-
rectly detected recurring ﬁxing changes over the total num-
ber of detected ones. The recall value is deﬁned as the
number of correctly detected recurring ﬁxing changes over
the total number of recurring ﬁxing ones. We also use fs-
core, a metric combining both precision and recall by the
formula 2 /fscore =1/precision +1/recall .
For each system, we selected the range of ﬁxing revisions
r
1tornas exactly as the one in the experiment in Section 2.
We incrementally executed FixWizard for each ﬁxing revi-sionr
kin the range from r1torn. At each rk, based on the
recognized recurring ﬁxes and detected code peers in the
past from r1tork−1, FixWizard examines the current ﬁx
and detect if those ﬁxing changes are recurring. Note that
all ﬁxing changes to the same method at a ﬁxing revisionare considered as an atomic ﬁx. We compared the detected
result with the human-veriﬁed data. The accumulated pre-
cision and recall values are recorded at each ﬁxing revisions.
An accumulated value at r
krefers to the value measured
for all ﬁxing revisions from r1tork. The accumulated val-
ues reﬂect better the accuracy of FixWizard over time than
instant precision/recall at a single revision.
Figure 11 shows the accumulated precision and recall val-
ues, respectively, for the project ZK in the ﬁxing revisions.
For example, among 124 ﬁxing revisions in ZK project, wecould see that after the initial phase of about 6-8 revisions,
the accumulated precision and recall values reach the ranges
between 87-92% and 78-83%, respectively. In the initial
322phase, because the number of accumulated recurring ﬁxes is
still small, precision and recall are aﬀected much by a coupleof miss or incorrectly detected recurring ﬁxes. The similargraph results are also achieved for other systems in which
after the initial phases the average precision and recall val-
ues are 81% and 74%, respectively (see Table 4). Due to thespace limit, we could not show the graphs for all subjects.
The values are quite consistent and stable after the initial
phase for all projects. Moreover, comparing with the per-
centage of the recurring ﬁxes that occur in code peers (see
the empirical study in Section 2), we could see that FixWiz-ard is able to detect the majority of such recurring ﬁxes(74% vs 88% on average). This shows that our formulationand algorithms for detecting code peers and recurring ﬁxes
are quite accurate in object-oriented programs.
Other experiments . To conﬁrm that our formulation and
algorithms are still valid on other systems than the onesused in the empirical study (Section 2), we also run FixWiz-
ard on two other Java application server systems Jetty and
Apache Tomcat. We manually checked the results reportedby FixWizard for precision values. On Jetty, FixWizard re-ported 13 recurring ﬁxes and 11 were veriﬁed to be correct(85% precision). On Tomcat, 99 out of 136 reported ﬁxes
were truly recurring (73% precision).
5.2 Fixing Recommendation
This section describes our evaluation for the accuracy of
the recommendation algorithm. We used the same set of
subject systems as in the previous experiment. We also per-formed a similar process as in the detection experiment inwhich FixWizard was incrementally executed for each ﬁxing
revision in the chosen range from r
1torn. At each ﬁxing
revision rk, we executed the tool for each ﬁxing change and
recorded the recommendations for its code peers. Then,
we compared with the actual ﬁxes. The number of pro-duced recommendations and the number of correct ones are
counted. A recommendation is considered as correct if it
correctly suggests the ﬁxing location and the ﬁrst change op-eration. We recorded the correctness of recommended loca-tions at both method and statement levels (e.g. add/delete
methods, add/delete/modify statements). Only the ﬁrst op-
eration is considered because if the correct location is sug-gested, it would already save much eﬀort for developers.
Because the human-veriﬁed oracle (Section 2) did not con-
tain the detailed change operations, we had to check the
recommendation results manually. Therefore, we chose a
smaller range of ﬁxing revisions to check. Table 5 showsthe recommendation results. Column
Check is the number
of recurring ﬁxes we manually checked, which is less thanthe total number of recurring ﬁxes in the subject systems.
Columns
Recom. and Correct are the number of changes that
FixWizard recommended and that of changes that we con-
sidered to be correct. Two columns Prec. and Rec.are the
precision and recall values, respectively. It could be seenthat, on average for all subject systems, FixWizard has 71%
recall and 49% precision. This result shows that using code
peers to suggest the locations for recurring ﬁxes are accept-ably accurate. The mapping task between the nodes in codepeers and the detected editing operations need to improve.
5.3 Threats to Validity
Open-source bug databases used in our research could be
incomplete because there exist bug ﬁxes that might not beSystem Check Recom. Correct Prec. Rec. Fscore
ArgoUML 283 515 217 42% 77% 54%
Columba 199 293 139 47% 70% 56%
ZK 69 103 44 43% 64% 51%
FlashRecuit 65 77 39 51% 60% 55%
gEclipse 152 206 127 62% 84% 71%
Table 5: Recommendation Accuracy
reported in such repositories. Since our empirical study isbased on the reported bug ﬁxes in those databases, its results
might be aﬀected. Moreover, recurring bug ﬁxes in our study
are examined and identiﬁed by human beings, thus, theycould be biased due to human subjective views.
6. RELATED WORK
There exist other research that aims to record and recom-
mend recurring bug ﬁxes [13, 23, 27]. Sun et al.[27] present
a template- and rule-based approach to automatically prop-agate bug ﬁxes. Their approach supports some pre-deﬁnedtemplates/rules (e.g. orders of pairs of method calls, con-dition checking around the calls) and requires a ﬁx to beextracted/expressed as one or more rules in order to prop-
agate it. In contrast, our approach is peer-based, thus, if a
new ﬁx occurs in a code unit, FixWizard is still able to rec-ommend the similar ﬁx to its code peer(s) without requiringany predeﬁned templates or rules. Moreover, their approachrelies on rule matching via label-isomorphism that does not
take the peer relation into account.
Another research close to our work is BugMem [13]. Bug-
Mem uses the line-based textual diﬀerencing approach to
identify changed textual areas (called hunks ). BugMem’s
atomic ﬁx is a pair of textual hunks: bug hunk (in the olderrevision) and ﬁx hunk (in the new one). For each line in a
hunk, BugMem extracts program units (with type informa-tion) and uses them as the features of the hunk. A ﬁx ischaracterized by the features existing in the bug hunk but
not in the ﬁx hunk. First, FixWizard diﬀers from BugMem
on the program context used to extract the features of a ﬁx.
Because examining onlythe changed area, BugMem misses
the recurring ﬁxes that involve the addition of new code.In these cases, BugMem faces empty bug hunk. FixWiz-
ard could detect these recurring ﬁxes because it examines
also the impact area of the change. Moreover, it handles
global recurring ﬁxes in code peers with the impact areaslying outside of the changed regions. Importantly, FixWiz-ard performs program/data analyses from the changed area
in its enclosing method (even on code peers ), and extracts
features based on object usages. Finally, FixWizard can
recommend in both ﬁxing locations andoperations .
In Patch Miner [23], after making a ﬁx, developers could
use the tool to create a patch. It ﬁnds all code snapshotswith similar snippets (i.e. cloned code) to the fragment thatwas ﬁxed. It uses largest common subsequences of programtokens to ﬁnd such cloned code. Since the level of abstrac-tion of features is at program tokens, it could not handle the
cases requiring complex program analysis (e.g. code peers).
Moreover, Patch Miner detects code clones to suggest a ﬁx,
while FixWizard can also detect similar ﬁxes. Thus, Patch
Miner could not suggest a ﬁx even though a similar ﬁx oc-curred in the past to a peer of the current fragment.
Many approaches for code clone detection have been pro-
posed [5]. The key diﬀerence of FixWizard with this research
323is that FixWizard is able to detect not only similar code, but
alsosimilar changes . A cloned fragment is a candidate for
code peers because it could have the similar object usageswith another fragment. CP-Miner [15] mines frequent sub-
sequences of tokens to detect bugs caused by inconsistent
editing to cloned code. Jiang et al. [12] detect clone-related
bugs via formulating context-based inconsistencies. Exist-ing supports for consistent editing of cloned code are limited
to interactive synchronization in code editors such as Clone-
Tracker [8] and Codelink editor [29]. Libra [10] searches code
fragments for simultaneous changes using token-based cloneanalysis. FixWizard could support oﬀ-line clone synchro-nization because it is extended from Clever [20].
Several bug ﬁnding approaches are based on mining of
code patterns in the project’s history [11, 16, 28, 30, 31].Some tools detect pre-deﬁned , common bug patterns us-
ing syntactic pattern matching [6, 11]. They do not detecthigh-level, project-speciﬁc bugs. In contrast, JADET [30]
performs mining the object usage patterns and detects the
violations as potential bugs. Hipikat [7] extracts lexical in-
formation while building the project’s memories and rec-ommends related artifacts including documentation. Sev-eral approaches mine usage patterns in term of the orders
of method calls (Dynamine [16], MAPO [1], Williams and
Hollingsworth’s [31]), or association rules [26, 28]. However,they focus only on a small set of patterns and bugs (e.g. codeusages [16, 30], error-handling [28], condition checking [4]).In FixWizard, a new ﬁx pattern can be characterized, rec-
ognized, and used in recommendations. Some of such ap-
proaches [1, 16] ﬁnd the methods that are used/changed to-gether. However, they analyze only the client code of meth-ods. FixWizard looks at their internal usages as well.
Several approaches have been proposed to help users lo-
calize buggy code areas [3, 9, 14, 17, 19]. Some leveragethe project’s historical information: the amount of changedLOC over the total in a time period [19], frequently/recentlymodiﬁed/ﬁxed modules [9], code co-changes and bug local-
ity [14], change and complexity metrics [18, 22, 25], social
network among developers [2, 3, 24, 32], etc. Although theyhave achieved the good level of accuracy (60%-80%), thegranularity levels of buggy area are still coarse, ranging frommodules to ﬁles, or methods [13]. With slightly lower accu-
racy, FixWizard is able to recommend ﬁxes at the statement
level and provide useful semantics operations.
7. CONCLUSIONS
This paper investigated recurring bug-ﬁxing changes, both
theoretically and empirically, and found that a high percent-age of such recurring ﬁxes occur at code peers, i.e. certainclasses and methods having similar roles in a system such asproviding similar functions and/or participating in similar
usage scenarios. To aim for that high-percentage portion
of recurring ﬁxes, we developed several novel techniques torecognize code peers and the similar ﬁxing changes made tothem, as well as to recommend the ﬁxing changes for a codepeer from the ﬁxes made to one of its peers. Our empirical
evaluation showed that our approach achieves high level of
accuracy in both recognizing recurring ﬁxes and recommend-ing the bug-ﬁxing changes. In future, we will investigate theapproaches to deal with similar usages involving design pat-terns to improve the quality of our recommendations.
Acknowledgment. This project was partially funded by a
Vietnam Education Foundation grant for the ﬁrst author.8. REFERENCES
[1] M. Acharya, T. Xie, J. Pei, and J. Xu. Mining API patterns as
partial orders from source code: from usage scenarios to
speciﬁcations. In ESEC-FSE’07 , pages 25–34. ACM, 2007.
[2] E. Arisholm and L. Briand. Predicting fault-prone components
in a Java legacy system. In ISESE’06 , pages 8–17, ACM, 2006.
[3] C. Bird, D. Pattison, R. D’Souza, V. Filkov, and P. Devanbu.
Latent social structure in open source projects. In
SIGSOFT’08/FSE-16 , pages 24–35. ACM, 2008.
[4] R.-Y. Chang, A. Podgurski, and J. Yang. Discovering neglected
conditions in software by mining dependence graphs. IEEE
Transactions on Software Engineering , 34(5):579–596, 2008.
[5] R. Tairas - Bibliography of code detection literature.
http://www.cis.uab.edu/tairasr/clones/literature/.
[6] T. Copeland. PMD Applied. Centennial Books, 2005.
[7] D. Cubranic, G. C. Murphy, J. Singer, and K. S. Booth.
Hipikat: A project memory for software development. IEEE
Transactions on Software Engineering , 31(6):446–465, 2005.
[8] E. Duala-Ekoko and M. Robillard. Tracking code clones in
evolving software. In ICSE’07 , pages 158-167. IEEE CS, 2007.
[9] A. E. Hassan and R. C. Holt. The top ten list: Dynamic fault
prediction. In ICSM’05 , pages 263–272. IEEE CS, 2005.
[10] Y. Higo, Y. Ueda, S. Kusumoto, and K. Inoue. Simultaneous
modiﬁcation support based on code clone analysis. In
APSEC’07 , pages 262-269. IEEE CS, 2007.
[11] D. Hovemeyer and W. Pugh. Finding bugs is easy. ACM
SIGPLAN Notices , 39(12):92–106, 2004.
[12] L. Jiang, Z. Su, and E. Chiu. Context-based detection of
clone-related bugs. In ESEC/FSE’07 , pages 55-64. ACM, 2007.
[13] S. Kim, K. Pan, and E. E. J. Whitehead, Jr. Memories of bug
ﬁxes. In FSE’06 , pages 35–45. ACM, 2006.
[14] S. Kim, T. Zimmermann, E. E. J. Whitehead, Jr., and
A. Zeller. Predicting faults from cached history. In ICSE’07 ,
pages 489–498. IEEE CS, 2007.
[15] Z. Li, S. Lu, and S. Myagmar. CP-Miner: Finding copy-paste
and related bugs in large-scale software code. IEEE
Transactions on Software Engineering , 32(3):176–192, 2006.
[16] B. Livshits and T. Zimmermann. Dynamine: ﬁnding common
error patterns by mining software revision histories.
ESEC/FSE’05 , pages 296–305, ACM Press, 2005.
[17] T. Menzies, J. Greenwald, and A. Frank. Data mining static
code attributes to learn defect predictors. IEEE Transactions
on Software Engineering , 33(1):2–13, 2007.
[18] R. Moser, W. Pedrycz, G. Succi. A comparative analysis of the
eﬃciency of change metrics and static code attributes for defectprediction. In ICSE’08 , pages 181–190. ACM, 2008.
[19] N. Nagappan and T. Ball. Use of relative code churn measures
to predict system defect density. In ICSE’05 . ACM, 2005.
[20] T.T. Nguyen, H.A. Nguyen, N. H. Pham, J. M. Al-Kofahi, and
T. N. Nguyen. Clone-aware Conﬁguration Management. InASE’09 , pages 123–134. IEEE CS, 2009.
[21] T.T. Nguyen, H.A. Nguyen, N. H. Pham, J. M. Al-Kofahi, and
T. N. Nguyen. Graph-based mining of multiple object usagepatterns. In ESEC/FSE’09 , pages 383–392. ACM Press, 2009.
[22] T. Ostrand, E. Weyuker, and R. Bell. Predicting the location
and number of faults in large software systems. IEEE
Transactions on Software Engineering , 31(4):340–355, 2005.
[23] Patch Miner: patterninsight.com/solutions/ﬁnd-once.php.
[24] M. Pinzger, N. Nagappan, and B. Murphy. Can developer-
module networks predict failures? In FSE’08 .A C M ,2 0 0 8 .
[25] J. ´Sliwerski, T. Zimmermann, and A. Zeller. Hatari: raising
risk awareness. In ESEC/FSE’05 ,
 pages 107–110. ACM, 2005.
[26] Q. Song, M. Shepperd, M. Cartwright, C. Mair. Software defect
association mining and defect correction eﬀort prediction. IEEE
Transactions on Software Engineering , 32(2):69–82, 2006.
[27] B. Sun, R. Chang, X. Chen, and A. Podgurski. Automated
Support for Propagating Bug Fixes. ISSRE’08 .I E E EC S ,2 0 0 8 .
[28] S. Thummalapenta and T. Xie. Mining exception-handling rules
as sequence association rules. In ICSE’09 , IEEE CS, 2009.
[29] M. Toomim, A. Begel, S. Graham. Managing duplicated code
with linked editing. In VLHCC’04 , p. 173-180. IEEE CS, 2004.
[30] A. Wasylkowski, A. Zeller, and C. Lindig. Detecting object
usage anomalies. In ESEC/FSE’07 , pages 35–44. ACM, 2007.
[31] C. C. Williams and J. K. Hollingsworth. Automatic mining of
source code repositories to improve bug ﬁnding techniques.
IEEE Trans. on Software Engineering , 31(6):466–480, 2005.
[32] T. Wolf, A. Schroter, D. Damian, T. Nguyen. Predicting build
failures using social network analysis on developercommunication. In ICSE’09, pp. 1-11. IEEE CS, 2009.
324