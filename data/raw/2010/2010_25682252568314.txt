An Analysis of the Relationship between Conditional
Entropy and Failed Error Propagation in Software Testing
Kelly Androutsopoulos
Middlesex University, UKDavid Clark
University College London, UKHaitao Dan
University College London, UK
Robert M. Hierons
Brunel University, UKMark Harman
University College London, UK
ABSTRACT
Failed error propagation (FEP) is known to hamper soft-
ware testing, yet it remains poorly understood. We intro-
duce an information theoretic formulation of FEP that is
based on measures of conditional entropy. This formula-
tion considers the situation in which we are interested in
the potential for an incorrect program state at statement s
to fail to propagate to incorrect output. We dene ve met-
rics that dier in two ways: whether we only consider parts
of the program that can be reached after executing sand
whether we restrict attention to a single program path of
interest. We give the results of experiments in which it was
found that on average one in 10 tests suered from FEP,
earlier studies having shown that this gure can vary sig-
nicantly between programs. The experiments also showed
that our metrics are well-correlated with FEP. Our empir-
ical study involved 30 programs, for which we executed a
total of 7,140,000 test cases. The results reveal that the
metrics dier in their performance but the Spearman rank
correlation with failed error propagation is close to 0.95 for
two of the metrics. These strong correlations in an exper-
imental setting, in which all information about both FEP
and conditional entropy is known, open up the possibility in
the longer term of devising inexpensive information theory
based metrics that allow us to minimise the eect of FEP.
Categories and Subject Descriptors
D.2.5 [ Software Engineering ]: Testing and Debugging|
Testing tools and code inspections
General Terms
Information Theory, Experimentation, Verication.
Keywords
Program Analysis, Information Theory.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proï¬t or commercial advantage and that copies
bear this notice and the full citation on the ï¬rst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciï¬c
permission and/or a fee.
ICSE â€™14, May 31 â€“ June 7, 2014, Hyderabad, India
Copyright 14 ACM 978-1-4503-2756-5/14/05 ...$15.00.1. INTRODUCTION
Coincidental correctness occurs when the program hap-
pens to produce the correct output for some input even
though it has executed a fault; the program is coinciden-
tally correct rather than actually correct. One of the causes
of coincidental correctness is known as Failed Error Propaga-
tion (FEP) [17, 28, 31]. In this situation, a faulty statement
is executed and the resulting internal computational state
becomes faulty, but the dierences between the faulty and
correct state fail to be observed at output. We say that the
error (the faulty state) has `failed to propagate'.
Empirical studies have revealed that FEP inhibits eective
software testing [4, 18, 22, 23, 24, 29] but it remains unclear
how software testing could be better designed to ameliorate
the problems it causes. In order to improve software testing,
we need to reduce the probability that test cases will suer
from FEP. However, in order to do that, we need metrics
that can help us to identify parts of a program that are
more likely to lead to FEP.
Failed error propagation can occur for a number of rea-
sons. For example, it might be that the faulty state is simply
never inspected by the test oracle. In this case, the failure
to propagate the error is caused by an inadequate oracle
rather than by any inherent property of the program under
test. Such failures of error propagation could be addressed
by oracle improvement [26, 30].
A more interesting class of FEP occurs when the program
itself removes traces of an error before it has had a chance
to propagate to a point at which it can be observed. For this
to occur, faulty state changes must become `lost' along some
paths through the program because state update functions
along these paths `squeeze out' the faulty information [7].
One obvious way in which this could occur is when a path
contains a `killing assignment', which overwrites the value
of the variable with a constant.
A killing assignment is the most extreme example of a
state update function squeezing out information. In gen-
eral, anycomputation that reduces entropy of inputs will
have the potential to `squeeze out' error information and
thereby lead to failed error propagation. This loss of error
information suggests a connection between FEP and infor-
mation theory, but this relationship has been little explored
in the literature.
This paper introduces an information theoretic formula-
tion of FEP. We introduce ve dierent metrics, based on
the computation of conditional entropy in a program, with
these diering in the parts of the program considered. When
considering a statement s, one natural approach is to exam-Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proï¬t or commercial advantage and that copies bear this notice and the full citation
on the ï¬rst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speciï¬c permission and/or a
fee. Request permissions from Permissions@acm.org.
ICSEâ€™14 , May 31 â€“ June 7, 2014, Hyderabad, India
Copyright 2014 ACM 978-1-4503-2756-5/14/05...$15.00
http://dx.doi.org/10.1145/2568225.2568314
573
ine a single path since a test case will lead to a path being
followed. However, a test case might lead to dierent paths
in some idealised correct program Pand the program under
testP0; by only considering a single path we do not recog-
nise the potential for FEP associated with this pair of paths
to occur. We therefore also consider approaches that look
at the part of the program that `follows' s(the statements
that can be reached from s).
In total, we introduce and evaluate ve dierent metrics,
experimentally evaluating each on 30 dierent programs.
Our results are based on the execution of 7,140,000 test
cases over 1,428 original and fault-seeded versions of the pro-
grams. In these experiments one in 10 test inputs suered
from FEP; these results are in line with those of Masri et alia
[18]. Masri et alia also found that the potential for FEP dif-
fers signicantly between programs, with over 60% of tests
being aected by FEP in 13% of the programs studied [18].
For each of the ve metrics, we performed an experimental
evaluation of its relationship to FEP. We report Spearman
rank correlations between each of the metrics and FEP.
The results of these experiments are promising, indicating
strong Spearman rank correlations between several of our
conditional entropy metrics and failed error propagation.
The primary contributions of this paper are as follows.
1. We introduce an information theoretic approach to
FEP, dening associated metrics.
2. We experimentally investigate the correlation between
the metrics and FEP, using 30 programs, including
open source programs and laboratory benchmarks. The
results reveal a Spearman rank correlation of over 0.95
for one metric and just under 0.95 for another.
The main motivation for this work is that a better under-
standing of FEP has the potential to lead to more eective
testing. In particular, the strong correlations in an exper-
imental setting in which all information about both FEP
and conditional entropy is known opens up the possibility
in the longer term of devising inexpensive information the-
ory based metrics that allow us to minimise the eect of
FEP when choosing test cases.
The rest of this paper is organised as follows. In Section
2 we briey describe related work and in Section 3 we pro-
vide background information regarding program semantics
and information theory. In Section 4 we outline how FEP
and conditional entropy relate conceptually, with this feed-
ing into research questions and associated hypotheses that
are given in Section 5. Section 6 outlines the experiments
and in Section 7 we give the results of these experiments.
Finally, in Section 8, conclusions are drawn and potential
lines of future work described.
2. RELATED WORK
Voas introduced the PIE framework and so explicitly recog-
nised the need for an incorrect program state to propagate
to output in order for a failure to occur [28]. The notion of
FEP was also explored by Laski et alia [17], who called this
error masking. They explored the concept and proposed the
use of a mutation approach to estimate the sensitivity of a
given test suite and program component C: this is the like-
lihood of an incorrect value produced by Cbeing masked
through FEP. The approach taken to mutation was to di-rectly mutate the program state (change it to some randomly
generated state).
Masri and Podgurski used experiments to explore variable
dependence within a program and how this relates to (an
estimate of) information ow [20]. They found that many
cases where there is a dependence (through a mixture of con-
trol and data dependence), there was negligible information
ow. This helps motivate our work, since the lack of true
information ow could be one source of FEP (although FEP
can occur even when there is a signicant amount of infor-
mation ow). It also suggests that if we just use dependence
when, for example, choosing test cases to exercise program
elements then we are likely to encounter FEP and so there
is a need for alternatives.
Masri et alia explored factors that adversely aect cov-
erage based fault localisation methods [18]. Such methods
assign a `suspiciousness' value to a program element s(such
as a statement) in order to allow the developer to focus on
those elements that are most likely to be responsible for ob-
served failures. They do this based on how many failing tests
execute an element sand how many passing tests execute
s. In their study, which used 148 versions of ten Java pro-
grams seeded with faults, they found that FEP (which they
called coincidental correctness) was relatively common but
also the rates varied with program: in 13% of programs over
60% of tests that led to a corrupted state did not produce a
failure while in 28% this eect was not observed.
Wang et alia [29] also considered the eect of FEP on fault
localisation. However, their approach was quite dierent
and involved producing multiple versions of each program
element by adding in `program patterns'. Each context pat-
tern describes aspects of the control ow and data ow after
this element. The results of experiments suggested that this
approach reduces the eect of FEP on fault localisation. Po-
tentially there would be value in adopting a similar approach
to dene coverage metrics.
In mutation testing we judge the adequacy of test suite T
for program pby executing Ton mutants, which are pro-
duced by making small changes to p. A mutant mofpis
weakly killed by test case tif the executions of mandp
produced dierent sequences of program states. In contrast,
mis strongly killed by tif the execution of mandpwitht
lead to dierent outputs. Thus, FEP corresponds to the dif-
ference between weak mutation testing and strong mutation
testing; experiments have shown that there are signicant
dierences between weak and strong mutation testing [22,
23], again indicating that FEP is relatively common.
Masri and Assi considered how test suites can be cleansed
of coincidental correctness (FEP) [19]. Their approach as-
sumes that the test cases have already been applied (and so
are suitable for regression testing) and identies program el-
ements that appear in all failing runs but also in a percentage
of runs that do not fail (the percentage must meet a thresh-
old value): these are considered to be a potential source of
coincidental correctness. Then test cases are removed from
consideration in fault localisation based on which identied
elements they contain.
Chen et alia [6] dened a measure that aims to approxi-
mate the probability of coincidental correctness using a syn-
tax based metric. The results of experiments with ve small
programs (for sorting arrays) were positive and this suggests
that such an approach is worth exploring further.5743. BACKGROUND
3.1 Control Flow Graphs (CFG)
A CFG is an alternative representation of the syntax of a
program which makes explicit the execution paths that may
be travelled in a program. We assume for simplicity that all
nodes in the graph are of two types: nodes corresponding to
state updates which have a single output edge, and nodes
corresponding to control ow decision points which have two
output edges, one labelled Tand the other labelled F. Some-
times the convention is employed that state update nodes in
CFGs are dened in terms of blocks of straight line code but
here we assume that each individual statement or call in the
program has its own node and outgoing edge.
Definition 1.(CFG). For a given programming language,
a CFG is a pair,hN;Ei, whereNis a set of nodes that con-
tains a unique start node and two virtual nodes, nsa virtual
start node which has a single edge, esto the unique start
node,nea virtual exit node with a unique virtual exit edge,
ee, with no successor node, and Eis a set of labelled, di-
rected edges, so that
Nu=fnjnis an assignment or a function call g
Nc=fnjnis a control (Boolean) expression g
N=Nu[Nc[fns;neg
L=fT;Fg
Ea=f(n1;n2)jn12Nu;n22Nand an empty label g
Ec=f(n1;n2)jn12Nc;n22Nand a label in Lg
E=Ea[Eu[fes;eeg
The virtual nodes, fns;neg, are merely for convenience
and allow the inclusion of edges esandeein the CFG that
are associated with the initial and exit program points.
Definition 2.(CFG paths). A path in the CFG is a
sequence of nodes such that any two successive nodes form
an edge in the CFG. An execution path in the CFG is a
path whose rst node is the virtual start node and either the
path is innite or it is nite and the nal node is the exit
node. A prex path is any nite path whose rst node is the
virtual start node.
Note that the set of prex paths includes the set of nite
execution paths.
We will use the notion of a well-formed subgraph of a con-
trol ow graph , i.e. a subgraph of a CFG which is itself a
CFG. Not every subgraph of a CFG is well formed. For ex-
ample, the subgraph reachable from a control node is not
well formed in general as it may lack a unique start node.
3.2 Program Semantics
In what follows we informally set out some concepts from
program semantics that occur in the course of our explana-
tions of what we are measuring and why.
We assume a deterministic, imperative language such as
C, C++, Java, et cetera. We assume a small step structured
operational semantics (SOS) [21] for the programming lan-
guage. An SOS formally describes, for a given input state,
the sequence of state updates and branching decisions that
occur along the execution path for the input. This sequence
corresponds to a sequence of edges in the CFG that startswith the start node and is either innite or is nite and ter-
minates in the exit node, i.e. an execution path. On this
basis we can associate each state in the SOS sequence with
an edge in the CFG and each update instruction or branch-
ing decision with a node in the CFG. The SOS for a given
program and a given input corresponds to an execution path
in the CFG in which each edge is labelled with the states
that occur after the execution or evaluation of a node in the
CFG. We will refer to this set of CFG paths as the Execu-
tion Semantics of the CFG.
To dene some notation for this idea we put the emphasis
on prex paths rather than execution paths in the CFG and
informally dene a property that relates a program, a path,
an input, an edge on the path and the state reached at that
edge using the input.  P(;t;e;x ) is the property that there
exists a (possibly incomplete) SOS sequence for input tto
programPthat corresponds to the prex path in the CFG
forPandeis an edge in andxis a state that occurs at e.
Definition 3.(Execution semantics for a path). Let 
be the set of prex paths in the CFG of program Pand let
be the set of all possible states that could occur in the
executions of P. The execution semantics for a path in 
has type:
[ [ ] ]ex: !E!!2
and is dened as
[ [] ]ex
e(t) =fxjP(;t;e;x )g
The set of states is non-empty in the case that eis an edge
inandis a prex of the execution path for t; otherwise
it is empty.
We dene three abstractions of the execution semantics:
aCollecting Semantics forprex paths , forControl
Flow Graphs , and for Programs . These simply collect up
sets of states, arising from the execution semantics, which
occur at edges in the CFG.
The collecting semantics for a prex path in a CFG is
given by the set of states that can occur at each edge in the
path over all runs of the program.
Definition 4.(Collecting Semantics for a prex path).
The collecting semantics for a path in and an edge in E
has type:
[ [ ] ]pa: !E!2
and is dened as
[ [] ]pa
e=[
t2[ [] ]ex
e(t)
The collecting semantics for a CFG is also dened in terms
of edges that occur in the CFG and collects the sets of states
for all paths that pass through an edge.
Definition 5.(Collecting Semantics for a CFG).
The collecting semantics for a CFG (at an edge in E) has
type:
[ [ ] ]cfg:E!2
and is dened as
[ [e] ]cfg=[
2[ [] ]pa
e575To dene the collecting semantics of a program we employ
the concept of a program point . In a formal semantics of a
programming language these are taken to be points in the
program syntax before or after the execution of a program
construct or statement dened in the grammar of the lan-
guage. In what follows we dene them in terms of nodes
in the CFG: a program point is a node in the CFG and
the set of states that can occur at that program point is
the collection of the CFG semantics of the edges that exit
that node, that is, the program point occurs immediately
after any node in the CFG and collects up all states that
the program may be in, once control passes from this node.
This is consistent with Cousot's reachability semantics
[11], an alternative way to present these semantic concepts,
but for a xed language.
Definition 6.(Collecting Semantics for a Program).
The collecting semantics for a program (at a node in N) has
type:
[ [ ] ]pr:N!2
and is dened as
[ [n] ]pr=[
fe2Eje=(n;)g[ [e] ]cfg
We dene various semantic concepts in terms of these def-
initions. The set of reachable states for a prex path is
[ [] ]pa
!()where!() is the edge following the nal state up-
date node of .
The set of input states for a program is the collecting se-
mantics for the program at the unique virtual start node, i.e.
[ [ns] ]prforCFG (P). The set of output states for a program
Pis the collecting semantics at the virtual edge eefollowing
the virtual exit node, i.e. [ [ ne] ]pr.
Let  be the set of all possible states for a program. The
weakest precondition for program Pto terminate and satisfy
is the largest   so that restricting the set of inputs to
 produces a subset of  as the set of output states. We
write = [P]. This can be extended to paths and CFGs
in the obvious way.
3.3 Information Theory
We consider total, onto functions with xed, nite, dis-
crete domains. That is, a function fis equipped with a xed
input domain, I, and an output range, O, so thatf:I!O
and (overloading f)O=fIandI=f 1O.
We take a probabilistic view of the behaviour of f. We
overloadIandOto also represent random variables equipped
with probability distributions, IandOrespectively. Shan-
non [25] measured the information content, or entropy, of a
random variable Xwith probability distribution pas follows.
Definition 7.Entropy of a random variable.
H(X) = X
x2Xp(x)log2p(x)
Since random variable Ois completely determined by f's
action onIall the information in Ostems from I[9] so
H(I) H(O) is the amount of information destroyed by f.
This is conditional entropy, the entropy of Iconditional on
knowledge of O, in the deterministic case. To emphasise the
role that this loss of information is playing we also call this
quantity Squeeziness . If the function is one to one there areno collisions and the Squeeziness of the function is 0 since
H(I) =H(O).
Definition 8.Squeeziness. The Squeeziness of total func-
tionf:I!O,Sq(f), is dened as the loss of information
after applying ftoI
Sq(f;I) =H(I) H(O)
Note that squeeziness considered as a function takes two
arguments, a domain (actually a random variable in the val-
ues of the domain) and a function applied to that domain.
The partition property of entropy [12] allows us to refor-
mulate Squeeziness in a more useful way. Let f 1obe the
random variable in the inverse image of o2O. The in-
verse images of elements of OpartitionI. For each o2O,
O(o) =P
i2f 1oI(i) soOis the probability distribution
for the random variable in the partitions induced by the
inverse images. These inverse images partition the input
space. By the partition property
H(I) =H(O) +X
o2Op(o)H(f 1o)
hence
Sq(f;I) =X
o2Op(o)H(f 1o)
The RHS of the equation is a weighted sum of terms and
H(f 1o) is the amount of information contained in (the ran-
dom variable in) the set of elements mapped to a single out-
put.
f. . . . . . . . . . . . . . . . . . . . . op(o)fâˆ’1oH(fâˆ’1o)
Friday, 13 September 2013
Figure 1: Loss of information: squeezing inverse im-
ages of outputs.
In what follows whenever the distribution on a set is not an
induced one, e.g. it is a distribution on inputs, we always use
a uniform distribution. This corresponds to the Maximum
Entropy Principle and can be viewed as producing metrics
in which inputs have equal weight.
4. FAILED ERROR PROPAGATION (FEP)
AND CONDITIONAL ENTROPY
We argue that a very useful way of looking at FEP is to see
it in terms of loss of information. This has been hinted at in
studies by Masri, Woodward, and others without any strong
conclusions [20, 31]. The nature of testing software, i.e. lack
of knowledge of the error-free program, necessarily makes
any analysis approximate rather than formal. In this case576the proof of our analytical pudding will be experiment rather
than proof based. To overcome our ignorance of the ideal,
error free program, from now on referred to as the \ghost"
program, we make the following two strong assumptions.
Assumption 1.There is a single error in the program
under test.
This is a fairly common, simplifying assumption. In or-
der to state the second assumption we must rst compare
execution of a test input using the \ghost" program with ex-
ecution of the same input using the Implementation Under
Test (IUT).
Suppose the program we intended to write, the \ghost"
program, is program Pbut the program we actually wrote,
the IUT, is another program which we will call P0.Pis
a perfect oracle for P0which exhibits not only the desired
input-output behaviour of P0but allows us to examine the
desired internal states of P0. The dierence between Pand
P0is thatP0may contain faults. Let us assume that there is
only one fault in P0and that it occurs in a single structural
component, C0which corresponds to its fault-free version,
C, inP. This is a scenario similar to mutation testing.
Now consider a test input, t, and the execution of each
program on t. This is the situation illustrated by Figure 2.
In the CFG corresponding to each program we can break
the execution path into three parts: the upper path that
precedes entry to the structural component, the structural
component, and the lower path that succeeds the compo-
nent. Since we assume that there is a single fault the re-
spective upper paths will be the same, i.e. AandA0in
Figure 2 are the same. Clearly CandC0are not the same
and, in general, the succeeding or lower paths, BandB0
are not the same. Let us assume that cis the path through
Cand thatChas a nal node, n, which is a state update
node. In this case there is a single outgoing edge, e, and we
expectc0,C0,n0ande0to play the corresponding roles in
P0. For a covering path =A0:c0:B0inP0when describing
theupper path we will mean u=A0:c0and by the lower
path we mean l=B0.
The execution semantics in Pis then [ [A:c:B ] ]ex
e(t) and in
P0is [ [A0:c0:B0] ]ex
e0(t). The path semantics in Pis [ [A:c:B ] ]pa
e
and inP0is [ [A0:c0:B0] ]pa
e0. Finally, under the assumption
that the nal node of C(andC0) is a state update node,
the CFG collecting semantics at nandn0respectively and
the program collecting semantics at eande0respectively are
the same (respectively). That is [ [ e] ]cfg= [ [n] ]prand [ [e0] ]cfg=
[ [n0] ]pr. In Figure 2, eis identied with program point pp
ande0with program point pp0.
We will use these notations to frame hypotheses and de-
scribe the experiments in Sections 5 and 6.
To return to Figure 2, it may be possible that the states
associated with each edge in the execution semantics are
the same, i.e. [ [ A:c] ]ex
e(t) = [ [A:c0] ]ex
e0(t), but in general these
will be dierent, corresponding to the Infection phase of the
PIE scenario. However when FEP occurs we have that the
output,o, is the same in each case.
Laski et alia observed that it is the behaviour of the sub-
graph whose input state corresponds to the edge e0in Figure
2 and which may be described as the subgraph reachable
from the target node of e0which is somehow failing to Prop-
agate the Infection to the output [17]. In PandP0these
respective subgraphs are labelled QandQ0. It is the joint
behaviour of these that causes FEP. Laski and others fur-ther noted that these subgraphs are exactly the same except
in the case that the component C0is within a loop. Then C
may occur many times in Qcorresponding to C0occurring
many times in Q0but the CFG context in which they occur
will be the same. In the case that the fault occurs in a node,
n, which is a control expression there is no unique outward
edge as per our earlier assumption and the subgraph is not
a well formed CFG. This case did not occur in the paper by
Laski et alia as they assumed that the fault was within a
well formed program construct.
Suppose that the two subgraphs are the same ( Q=Q0),
then the collision where two dierent inputs produce the
same output is an example of loss of information (condi-
tional entropy) between inputs and outputs as discussed in
a previous paper [7] and above in Section 3.3, so even if
they are not the same but their information ow behaviour
is very similar we can use the information ow behaviour of
Q0to estimate how likely it is that FEP occurs. This is the
key idea in this paper and our second assumption.
Assumption 2.The sub programs QandQ0following the
error point in the \ghost" program and the IUT are essen-
tially the same from an information ow quantity perspec-
tive.
nâ€™t
CFG(Pâ€™)
A Aâ€™
C Câ€™
Q
o oB
Bâ€™Qâ€™t
CFG(P)
pp ppâ€™
n
Figure 2: FEP scenario.
5. THE RESEARCH QUESTION
Our long term aim (beyond the scope of this paper) is to
produce a set of lightweight, information ow based metrics
which can support both coverage based testing and muta-
tion based testing in generating test suites that minimally
suer from FEP. As Masri et alia have observed, dierent
programs suer from coincidental correctness to dierent ex-
tents [19]. However, we can attempt to optimise for a par-
ticular program. In what follows we consider the question
of useful correlations from a coverage testing perspective.
There is only one research question:
Research Question 1.What are the useful correlations
between the conditional entropies of dierent information
ow channels in the IUT and the probability of FEP for a
given, erroneous, program construct?577In what follows we propose six answers to this research
question in the form of hypotheses. In subsequent sections
we will evaluate these answers experimentally. It will be
useful to refer to Figure 2 when interpreting the hypothe-
ses. The statements of the hypotheses refer to the following
conventions:
5.1 Hypothesis 1
Consider an incorrect program construct, C0in an IUT
P0containing program point pp0corresponding to edge e0as
above, immediately following C0. Let Ibe the set of inputs
toP0and letQ0be the sub program of P0reachable from
pp0and the collecting semantics at pp0be the set of states
pp0= [ [e0] ]cfg. Let [ [Q0] ] be the functional semantics of Q0.
Hypothesis 1.There is a correlation between the prob-
ability of FEP for all input states whose execution path in-
cludese0and
sq([ [Q0] ];[Q0]([ [Q0] ]pp0))
We unpack and motivate the metric. Laski et alia have ob-
served that failed error propagation is due to the behaviour
ofQ0. We propose that, given our assumptions, the more
squeezy [ [Q0] ] is on states that get mapped to the output of
Q0applied to  pp0, i.e. the higher the conditional entropy of
[ [Q0] ] on that domain, the higher the probability that any in-
ternal state chosen at pp0suers from FEP. But what should
we estimate as the domain for the function [ [ Q0] ]? It has to be
larger than  pp0as it needs to contain states that could oc-
cur atppinP, the \ghost" program, i.e. states in  pp. One
way of estimating  pp[pp0is to consider the weakest pre-
condition of the outputs of the IUT for all execution paths
throughpp0under [ [Q0] ]. This is the domain [ Q0]([ [Q0] ]pp0).
The consequence of a strong correlation here is that the
squeeziness based metric would tell the tester whether or
not it is necessary to use optimisation to minimise the prob-
ability of FEP.
5.2 Hypothesis 2
Consider the same setup as for the previous hypothesis.
Similarly, let R0be the sub program of P0which is back-
wardly reachable from pp0so that  pp0is also the set of
outputs from applying [ [ R0] ] to I.
Hypothesis 2.There is a correlation between the proba-
bility of FEP for all input states that reach pp0via the exe-
cution ofR0and
sq([ [R0] ];[R0]pp0) +sq([ [Q0] ];[Q0]([ [Q0] ]pp0))
except when sq([ [Q0] ];[Q0]([ [Q0] ]pp0)) = 0 .
This is the same as hypothesis 1 but in addition we con-
sider the sub program R0. This may potentially squeeze
multiple input states in  Ionto states in  pp0that in turn
suer from FEP, multiplying the eect of Q0. To account
for this we add the conditional entropy of [ [ R0] ] on the input
states that get mapped to  pp0. If [ [Q0] ] has zero squeeziness
on [Q0]([ [Q0] ]pp0) there should be no possibility of FEP and
no need to consider the multiplier eect.
The consequences of a strong correlation are the same as
for Hypothesis 1. In this case we are merely interested in
which correlation is the stronger. In the next two hypotheses
we examine how we can provide an optimisation when the
squeeziness metric is relatively high.5.3 Hypothesis 3
Consider an execution path, , inP0that covers the faulty
component C0. In general there may be many inputs to P0
that follow and correspond to states at both pp0and the
exit point for P0. Let the outputs that reach the exit point
forP0alongbe [ [] ]I.
Hypothesis 3.There is a correlation between the proba-
bility of FEP for all states that reach pp0via execution along
, i.e. [ [] ]pa
e0, and
sq([ [Q0] ];[Q0]([ [] ]I))
Again, we unpack and motivate the conditional entropy
metric. Previously we considered all states that reach the
program point pp0immediately after executing the poten-
tially faulty program component C0. Here we consider the
states that reach the program point only via a single cover-
ing path for C0,. Examining Figure 2, an input to both P
andP0will reachppandpp0respectively via the same up-
per path (modulo C=C0) but may have dierent lower paths.
Under Assumption 2, both of the lower paths are paths in
Q0so the degree of FEP for inputs to P0that reach [ [ ] ]pa
e0
depends on the degree to which Q0is colliding states at pp
andpp0to produce states in [ [ ] ]I. We estimate the states
in pp[pp0that are mapped onto [ [ ] ]Iby considering
the weakest precondition for Q0but this time with a post
condition of [ [ ] ]I.
The consequence of a strong correlation in this case is
that it gives us a means to rank covering paths for C0using
squeeziness. We can choose the least squeezy path and have
condence that, by using that path to generate a test input
that covers C0, we have maximised, or at least improved,
the probability that our choice of test input will produce a
failing test output in the case that C0has a bug.
5.4 Hypothesis 4
Let a path covering a construct C0be expressed as the
concatenation of two paths so that =ulas above in
Section 4 , where uis the upper path that terminates at pp0
(e0) andlis the lower path that follows pp0(the part of 
beginning at the target node of e0). There is the possibility
thatusqueezes inputs onto states at pp0which magnies
the degree of FEP caused by the squeeziness of Q0on pp0[
pp. In a way similar to Hypothesis 2 we add the conditional
entropy of the upper path to improve the correlation.
Hypothesis 4.There is a correlation between the proba-
bility of FEP for all states that reach pp0via execution along
, i.e. [ [] ]pa
e0, and
sq([ [u] ];[u][ [] ]pa
e0) +sq([ [Q0] ];[Q0]([ [] ]I))
As in Hypothesis 2, our aim is to test whether the ad-
dition of this upper path conditional entropy improves the
correlation.
5.5 Hypothesis 5
In a previous paper we speculated that the squeeziness of
lon pp0would be correlated with the probability of fault
masking for inputs that reach pp0[7].
Hypothesis 5.There is a correlation between the proba-
bility of FEP for all input states that reach pp0via execution
alongand
sq([ [l] ];[ [] ]pa
e0))578The consequences of this correlation are that we would
only need to rank covering paths for a construct on the basis
of the squeeziness of the lower path on the states that occur
atpp0along path . In comparison to metrics in earlier
hypotheses this would be comparatively cheap to estimate.
5.6 Hypothesis 6
We know that for a function fand domain Dwhere
sq(f;D) = 0 that fmust be one to one. So we expect
that if a squeeziness metric is zero then fmust propagate
error states in Dto the output of the function without any
collisions, i.e. that the probability of FEP is zero. That is
the theory. However Assumption 2 may weaken this so we
need to test the hypothesis experimentally.
Hypothesis 6.Let > 0be a small number arbitrar-
ily close to 0. Whenever sq([ [Q0] ];[Q0]([ [] ]I))then
p(FEP ).
The consequence of this hypothesis being validated is that
a squeeziness close to zero for a path means that we don't
need to rank the covering paths but can simply use that path
to generate a test input to cover the program construct.
6. EXPERIMENTAL SETUP
6.1 Subjects and Mutants
Three sources of subject programs are shown in Table
1 where we aggregate lines of code for each project, with
all programs being written in C. The toyproject contains
17 small programs that we implemented based on designs
that aimed to demonstrate squeeziness. The other subject
programs were taken from two real-world projects: the R
project [14] for statistical computing and graphics, and the
open source statistical package GRETL (Gnu Regression,
Econometrics and Time-series Library) [10].
Table 1: Projects under investigation
Project Function total LoC Mutants
Toy 17 810 383
R 10 221k 953
GRETL 3 286k 72
The two real-world projects contain many functions and so
we chose entrance functions: those directly called by a user
or a program from outside of the project. For R, as shown
in Column 2 of Table 2, we selected the 10 functions with
the most Lines of Code (LoCs) from the nmath library of
R, a C Library of special mathematical functions. However,
to simplify the experiments, we did not use functions that
contained array variables. To add variety, another three
subject programs were chosen from the cephes library of
GRETL (see Table 2).
The functions from RandGRETL require other functions
from their libraries. We therefore formed subject programs
by (recursively) including the required functions, with Col-
umn 3 of Table 2 giving the numbers of functions involved.
Columns 4 and 5 give the LoC and physically executable
lines of code (SLoC) of the subject programs.
We generated dierent versions of programs by seeding a
fault into each original subject program as done in muta-
tion testing [16]. The faults were introduced by using theC mutation operator OAAN [3] that replaces an arithmetic
operator with another, for example, - with +, or / with +.
The mutation tool SMT-C was used to generate the mu-
tants (mutated programs) and to run the mutation analysis
[13]. Some of the mutants could not be compiled and so
were not used in the experiments. To calculate FEP, both
strong and weak mutation analysis were applied1. Finally,
several subject programs ( rhyper ,ptukey ,qgamma ,psiand
qt) led to too many mutants and in these cases we randomly
selected 100 mutants. The numbers of mutants used in the
experiment are given in Table 1.
Table 2: Real world statistical subject programs
Project Function C Files LoC SLoC
Rbratio 41667 1573
rhyper 2 338 260
gamma cody 2 137 116
ptukey 71360 674
qgamma 14 2397 1312
psi 2 261 119
pnorm both 1 315 178
pnchisq raw 1 275 181
gammafn 5 527 264
GRETLqt 1 234 124
i0 2 270 108
k0 5 688 267
unity 3 335 147
6.2 Experimental Design
In the experiments, randomly chosen inputs were sampled
from a uniform distribution. This allowed us to estimate
quantities. For signicance we have relied on a sample size
which is relatively large over all programs and can be viewed
as \all inputs" for a restricted input domain.
As all subject programs have numeric inputs, the Rng-
Pack library was used to generate the random numbers [2].
Initially, we generated inputs from the complete ranges of
the input parameters but we found that most inputs led to
special numbers (0, NAN and INFINITE ) as output. To
address this problem, for each program we limited the in-
put domain from which inputs were chosen based on the
rst level guards of that program. This led to smaller in-
put domains but useful test cases. For example, inputs for
gamma cody were drawn from [  200;200].
We then ran strong/weak mutation analysis using SMT-C
and the randomly generated inputs. As the weak mutation
analysis functionality of SMT-C is implemented based on
the GNU Debugger (GDB) [1], it was possible to use GDB
commands to extract the runtime program states, and this
allowed us to extend SMT-C to support information ow
analysis. We executed each subject program and each of its
mutants with the same 5000 inputs and recorded the state
after the mutation point (at program point ppin the original
programPandpp0in the mutant P0in Figure 2), the state
at the end of the program ( oin the original program Pando0
in the mutant P0in Figure 2), and the execution path taken
by the mutant. We ignored any inputs that led to invalid
1See section 2 for a description of weak and strong mutation
testing579outputs, such as those that are not a number, are innite or
that cause an exception. The executions of the more than
7 million test cases generated more than 7 gigabytes of raw
result les. Given mutant P0, the following shows how we
calculated the probability of fault masking p(FEP) on the
inputs used.
p(FEP) =# of tests that weakly kill P0
but do not strongly kill P0
# of tests that weakly kill P0
Note that the way we counted the total number of tests in
the denominator varied between experiments. In EXP1 and
EXP2 we counted the tests that reach pp0via any execution
path through C0. In experiments EXP3, EXP4 and EXP5
we counted the number of tests that reach pp0by following
a single execution path to output.
For a test input to weakly kill a mutant, it must lead
to dierent program states for the mutant and the original
program after the mutation point (in Figure 2 the states
atppinPandpp0inP0should be dierent). A test input
strongly kills a mutant if it leads to this dierence in program
state propagating to the program's output (in Figure 2 oin
PandoinP0are dierent). Therefore, p(FEP) computes
the proportion of tests that cause a dierent program state
after the mutation point at ppinPandpp0inP0but do not
lead to a dierent output.
We classied tests according to whether they are able to
propagate the seeded fault to the output. The tests that
successfully propagate faults have the property EP (error
propagation). The rest of the tests suer from failed er-
ror propagation (FEP) and other coincidental correctness
(CC1). Tests classied as CC2 suered from anomalies. Ta-
ble 3 lists the proportion of tests that have these properties.
The signicant statistic is that approximately 10% of tests
across all program-mutant pairs suered from FEP.
Table 3: The proportion of randomly generated
tests for all subject programs that are weakly and
strongly killed.
Weakly Killed Strongly Killed Proportion
EP Yes Yes 84.73 %
FEP Yes No 9.85 %
CC1 No No 4.89 %
CC2 No Yes 0.44 %
Given our complete knowledge in the mutation testing
scenario of the experiments we can replace [ Q0]([ [Q0] ]pp0)
with the quantity it estimates, namely  pp[pp0in EXP1
and EXP2. Similarly we can use the appropriate subset of
pp[pp0, calculated by direct examination of the data, to
replace its estimation, [ Q0](I) in EXP3, EXP4 and EXP5.
This gives a maximal correlation, useful when seeking to
compare correlations and rate the strength of correlations.
We now outline the experiments performed.
6.2.1 Experiment 1 (EXP1)
This experiment explored the strength of the correlation
suggested in Hypothesis 1 by estimating the correlation be-
tween the probability of FEP for inputs reaching pp0and
sq([ [Q0] ];pp[pp0).For each mutant, sq([ [Q0] ];pp[pp0) can be calculated
as follows. First, we set
s=1
jpp[pp0j
Then we set S=jpp[pp0jslog2(s) = log2(s). Given
outputo, the probability for ogiven pp[pp0is
t(o) =# of states in  pp[pp0that lead to o
jpp[pp0j
and consequently T=P
ot(o)log2(t(o)) and the squeezi-
ness forQ0on pp[pp0issq([ [Q0] ];pp[pp0) = S+T
6.2.2 Experiment 2 (EXP2)
This experiment explored the strength of the correlation
suggested in Hypothesis 2. The experiment is the same as
EXP1 with the dierence being that if sq([ [Q0] ];pp[pp0)6=
0 we add to it sq([ [R0] ];[R0]pp0), i.e. the squeeziness of R0
on the inputs that R0maps to states at pp0.
For each mutant, the squeeziness of R0on the domain can
be calculated as follows. The probability assigned to the
inputs isI= log2(jIj) (recall that  Iis the set of inputs
used). The probability assigned to a state 2pp0is:
u() =# of inputs that get mapped to byR0
jIj
and we letU=P
u()log2(u()). Then the squeeziness
ofR0is:sq([ [R0] ];[R0]pp0) = I+U.
If the squeeziness of Q0is non-zero, then the squeeziness
ofR0is added to it.
6.2.3 Experiment 3 (EXP3)
For a given path , which executes the component of inter-
est, p(FEP) for states occurring on the path at pp0correlates
withsq([ [Q0] ];[Q0]()). Given path we will let 
pp0and

ppbe the sets of states occurring on the path at pp0and
pprespectively when following inP0and its equivalent in
Prespectively.
For each path in a mutant, associated values of p(FEP)
and the squeeziness of Q0,sq(Q0;[Q0]()), can be calcu-
lated using an approach similar to that in the description of
experiment EXP1 except that the values used are specic to
. Thus, we assign the probability
s=1
jpp[
pp0j
Then we set S=Pslog2(s). The probability for the
corresponding outputs for is given by:
t(o) =# of states in 
pp[
pp0that lead to o
jpp[
pp0j
and consequently, T=P
ot(o)log2(t(o)). The squeeziness
forQ0with respect to at 
pp[
pp0is then given by:
sq([ [Q0] ];
pp[
pp0) = S+T.
6.2.4 Experiment 4 (EXP4)
This experiment explored the strength of the correlation
suggested in Hypothesis 4. The experiment is the same
as EXP3 except that we add to sq([ [Q0] ];
pp[
pp0)) the
squeeziness of the upper path, u, on the inputs to the pro-
gram that follow execution path . The latter can be ex-
pressed assq([ [u] ];[]([ [] ])) since [ u][ [] ]pa
e0= []([ [] ]).580Let = []([ [] ]). The information content of the in-
puts that travel down path isI= log2(jj). The proba-
bility assigned at pp0is:
r() =# of times that9s2:[ [] ](s) =
jj
and this leads to the term U= P
r()log2(r()). Then
the squeeziness of uis given by sq([ [u] ];) =I U.
6.2.5 Experiment 5 (EXP5)
This experiment assessed the correlation between the prob-
ability of FEP on states in [ [ ] ]pa
e0and the squeeziness of l
on these states.
For each path in mutant P0, p(FEP) for states that
occur onatpp0, [ [] ]pa
e0, can be calculated as in Experiments
EXP3 and EXP4. To calculate squeeziness sq([ [l] ];[l]())
we have to calculate the probability distributions on [ [ ] ]pa
e0
atpp0and [ [] ]Iat the end of the program. To calculate
the latter, let  = []([ [] ]I). It is sucient to determine,
for each output o, how many inputs that follow lead too.
The probability assigned to oin [ [] ]Ifor a given path is:
m(o) =# of inputs from  that lead to o
jj
and we setM= P
om(o)log2(m(o)). The probability
assigned to state atpp0foris:
l() =# of inputs from  that lead to 
jj
and we set L= P
l()log2(l()). Then the squeezi-
ness is calculated as: sq([ [l] ];[l]()) =L M.
6.2.6 Experiment 6 (EXP6)
We examined various upper bounds \near zero" and found
the maximum observed value for sq([ [Q0] ];pp[pp0)) less
than the bound and its corresponding value for p(FEP) using
the pairs calculated for EXP1. We looked at a small sample
of three bounds: 0 :1, 0:01 and 0:001, but examined a large
number of pairs for each bound.
7. RESULTS
The experiments, inevitably, have limitations. We did not
use a variety of mutation operators that, for example, delete
statements, replace boolean relations with others, or replace
boolean subexpressions with true or false. However, our
approach is independent of types of faults as it is semantics
based and considers program points and incorrect states. We
only generated mutants with a single fault while in practice
programs may contain multiple faults but this is a common
assumption in testing. We aim to address this in future
work.
We sampled inputs rather than considering all inputs.
This was the only way to make the experiments practical.
This necessity may serendipitously be the foundation of a
sampling approach to estimating squeeziness metrics in the
future. Although we did not consider formal statistical guar-
antees, the sample size across all programs was large.
Information from both ppfromPandpp0fromP0was
used, when in practice we would only have pp0fromP0.
When testing a program P0, p(FEP) can never be known
in practice as we don't have P. Since our objective in thisTable 4: Spearman's Rank Correlation Coecient
for all programs.
Experiment Correlation
EXP1 0.715267
EXP2 0.699165
EXP3 0.955647
EXP4 0.948299
EXP5 0.031510
Table 5: Spearman's Rank Correlation Coecient
for statistical programs.
Experiment Correlation
EXP1 0.974459
EXP2 0.974459
EXP3 0.998526
EXP4 0.998526
EXP5 -0.001361
paper was to use strength of correlation to nd the most
suitable metrics using knowledge of ppwas not a drawback.
Consider the correlations found in Experiments 1-4. The
experiments computed the dierent metrics and Table 4
gives the Spearman's Rank Correlation Coecient for all
programs. In order to understand the contributions from
the small set of toy programs we wrote ourselves and the
real world programs we looked at the correlations for these
two groups separately. Table 5 gives the results for the sta-
tistical programs, and Table 6 gives the results for the small
programs. Interestingly, we have very strong correlations
for Experiments 1-4 and these are particularly strong for
the larger, real world, programs. Experiments 2 and 4 had
lower correlation values than Experiments 1 and 3, suggest-
ing that the important correlations are with squeeziness of
Q0on dierent domains and that contributions from the
upper program are not signicant, in fact retrograde. In
contrast, Experiment 5 did not reveal a correlation, rather
invalidating the suggestion of this metric in our IPL paper
[7]. These results give a very strong correlation for between
information theoretic metrics and both FEP and FEP along
a particular path.
We reproduce here the three plots of rank correlations
corresponding to the table entries for Experiment 2. Fig-
ure 3 plots the ranks given to squeeziness and p(FEP) when
calculating Spearman's Rank Correlation Coecient. Fig-
ure 4 plots the ranks given to squeeziness and p(FEP) for
the statistical programs. Figure 5 plots the ranks given to
squeeziness and p(FEP) for small programs.
Table 6: Spearman's Rank Correlation Coecient
for small programs.
Experiment Correlation
EXP1 0.705367
EXP2 0.686284
EXP3 0.761889
EXP4 0.666140
EXP5 0.005787581Figure 3: The rank correlation of p(FEP) and
Squeeziness for all programs (EXP2).
Figure 4: The rank correlation of p(FEP) and
Squeeziness for statistical programs (EXP2).
Figure 5: The rank correlation of p(FEP) and
Squeeziness for small programs (EXP2).
Table 7: Maximum p(FEP) for all programs
sq(Q0) Range Max sq(Q)Max p(FEP)
0.1 0.090683 0.090683
0.01 0.001120 0.001120
0.001 0.000800 0.000200
The plots show that the strong correlation for EXP2 is in
the most part derived from the larger, real world statistical
programs.
Table 7 gives the maximum values for squeeziness of Q
and p(FEP) for a given \small" bound on sq(Q) values over
all programs using information from EXP1. The results vali-
date theory and could be used to identify program constructs
in an implementation under test that are highly unlikely to
suer from FEP.
8. CONCLUSIONS AND FURTHER WORK
Our analysis and the results of our experiments have shown
that we can interpret Failed Error Propagation during soft-
ware testing using conditional entropy based metrics on the
Implementation Under Test. This is a novel use of Quanti-
ed Information Flow, a concept whose applications have to
date been in the area of secure information ow [8, 15].
An enormous vista of possible future work now beckons.
Having identied useful metrics, the next task is to repeat
the experiments using estimates of the weakest precondi-
tions that appear in the metrics. We expect that the rank
correlations will be weaker but still highly signicant. The
success of these experiments would put the utility of the
approach beyond doubt. Beyond that lies the problem of
estimating the metrics. Estimating entropy for the absolute
values used in the nal experiment will be more dicult but
not impossible [5].
An interesting challenge in the long run would be applica-
tion in a concolic testing scenario such as that oered by Pex
[27]. We believe this paper lays the foundation for signicant
future improvement in test suite eectiveness.5829. REFERENCES
[1] GDB: The GNU Project Debugger.
http://www.gnu.org/software/gdb/, Accessed in 2013.
[2] RngPack 1.1a.
http://www.honeylocust.com/RngPack/, Accessed in
2013.
[3] H. Agrawal, R. DeMillo, R. Hathaway, W. Hsu,
W. Hsu, E. Krauser, R. Martin, A. Mathur, and
E. Spaord. Design of mutant operators for the C
programming language. Technical report, Department
of Computer Sciences, Purdue University, 1989.
[4] T. Apiwattanapong, R. A. Santelices, P. K.
Chittimalli, A. Orso, and M. J. Harrold. Matrix:
Maintenance-oriented testing requirements identier
and examiner. In Testing: Academia and Industry
Conference - Practice And Research Techniques
(TAIC PART 2006) , pages 137{146, Windsor, UK,
2006. IEEE.
[5] K. Chatzikokolakis, T. Chothia, and A. Guha.
Statistical measurement of information leakage. In
Tools and Algorithms for the Construction and
Analysis of Systems , pages 390{404. Springer, 2010.
[6] J. Chen, Q. Li, J. Zhao, and X. Li. Test adequacy
criterion based on coincidental correctness probability.
Inthe Second Asia-Pacic Symposium on
Internetware , Internetware '10, pages 20:1{20:4,
Suzhou, China, 2010. ACM.
[7] D. Clark and R. Hierons. Squeeziness: An information
theoretic measure for avoiding fault masking.
Information Processing Letters , 112(8 { 9):335 { 340,
2012.
[8] D. Clark, S. Hunt, and P. Malacaria. Quantitative
analysis of the leakage of condential data. In A. D.
Pierro and H. Wiklicky, editors, Electronic Notes in
Theoretical Computer Science , volume 59. Elsevier,
2002.
[9] D. Clark, S. Hunt, and P. Malacaria. A static analysis
for quantifying information ow in a simple imperative
language. Journal of Computer Security , 15(3):321 {
372, 2007.
[10] A. Cottrell, R. Lucchetti, et al. GNU regression,
econometrics and time-series library.
http://gretl.sourceforge.net, Accessed in 2013.
[11] P. Cousot and R. Cousot. Building the Information
Society , chapter Basic Concepts of Abstract
Interpretation. Springer US, 2004.
[12] T. M. Cover and J. A. Thomas. Elements of
Information Theory . Wiley Interscience, 1991.
[13] H. Dan and R. M. Hierons. SMT-C: A Semantic
Mutation Testing Tools for C. In the Fifth
International Conference on Software Testing,
Verication and Validation , pages 654{663, Montreal,
Canada, Apr. 2012. IEEE.
[14] R. Gentleman, R. Ihaka, et al. The R project for
statistical computing. http://www.r-project.org,
Accessed in 2013.
[15] J. Heusser and P. Malacaria. Quantifying information
leaks in software. In the Twenty-Sixth Annual
Computer Security Applications Conference , pages
261{269, Austin, Texas, USA, 2010. ACM.
[16] Y. Jia and M. Harman. An analysis and survey of the
development of mutation testing. IEEE Transactionson Software Engineering , 37(5):649{678, 2011.
[17] J. W. Laski, W. Szermer, and P. Luczycki. Error
masking in computer programs. Software Testing,
Verication and Reliability , 5(2):81{105, 1995.
[18] W. Masri, R. Abou-Assi, M. El-Ghali, and
N. Al-Fatairi. An empirical study of the factors that
reduce the eectiveness of coverage-based fault
localization. In the 2nd International Workshop on
Defects in Large Software Systems , DEFECTS '09,
pages 1{5, Chicago, Illinois, USA, 2009. ACM.
[19] W. Masri and R. A. Assi. Cleansing test suites from
coincidental correctness to enhance fault-localization.
Inthe Third International Conference on Software
Testing, Verication and Validation , pages 165{174,
Paris, France, 2010. IEEE.
[20] W. Masri and A. Podgurski. Measuring the strength of
information ows in programs. ACM Transactions on
Software Engineering and Methodology , 19(2), 2009.
[21] H. R. Nielson and F. Nielson. Semantics with
Applications . Wiley Professional Computing, 1993.
[22] A. J. Outt and S. D. Lee. How strong is weak
mutation? In the Fourth Symposium on Testing,
Analysis, and Verication , pages 200{213, Victoria,
British Columbia, Canada, 1991. ACM.
[23] A. J. Outt and S. D. Lee. An empirical evaluation of
weak mutation. IEEE Transactions on Software
Engineering , 20(5):337{344, 1994.
[24] R. A. Santelices and M. J. Harrold. Applying
aggressive propagation-based strategies for testing
changes. In the Fourth International Conference on
Software Testing, Verication and Validation , pages
11{20, Berlin, Germany, 2011. IEEE.
[25] C. Shannon. A mathematical theory of
communication. The Bell System Technical Journal ,
27:379{423 and 623{656, July and October 1948.
Available on-line at http://cm.bell-
labs.com/cm/ms/what/shannonday/paper.html .
[26] M. Staats. The inuence of multiple artifacts on the
eectiveness of software testing. In Proceedings of the
IEEE/ACM international conference on Automated
software engineering , ASE '10. ACM, 2010.
[27] N. Tillmann and J. de Halleux. Pex{White Box Test
Generation for .NET. Tests and Proofs , pages
134{153, 2008.
[28] J. M. Voas. PIE: A dynamic failure-based technique.
IEEE Transactions on Software Engineering ,
18(8):717{727, 1992.
[29] X. Wang, S.-C. Cheung, W. K. Chan, and Z. Zhang.
Taming coincidental correctness: Coverage renement
with context patterns to improve fault localization. In
the 31st International Conference on Software
Engineering , pages 45{55, Vancouver, British
Columbia, Canada, 2009. IEEE.
[30] M. Whalen, G. Gay, D. You, M. Heimdahl, and
M. Staats. Observable modied condition/decision
coverage. In Proceedings of the 2013 International
Conference on Software Engineering . ACM, May 2013.
[31] M. R. Woodward and Z. A. Al-Khanjari. Testability,
fault size and the domain-to-range ratio: An eternal
triangle. In the international symposium on Software
testing and analysis , ISSTA '00, pages 168{172,
Portland, Oregon, USA, 2000. ACM.583