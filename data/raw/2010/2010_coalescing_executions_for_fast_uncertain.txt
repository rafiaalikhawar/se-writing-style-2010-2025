CoalescingExecutions forFastUncertaintyAnalysis
WilliamN. Sumner TaoBao XiangyuZhang SunilPrabhakar
Department of Computer Science, Purdue University
{wsumer,tbao,xyzhang,sunil}@cs.purdue.edu
ABSTRACT
Uncertain data processing is critical in a wide range of ap-
plications such as scientiÔ¨Åc computation handling data with
inevitableerrorsandÔ¨Ånancialdecisionmakingrelyingonhu-
man provided parameters. While increasingly studied in the
area of databases, uncertain data processing is often carried
out by software, and thus software based solutions are at-
tractive. In particular, Monte Carlo (MC) methods execute
software with many samples from the uncertain inputs and
observe the statistical behavior of the output. In this paper,
we propose a technique to improve the cost-eÔ¨Äectiveness of
MC methods. Assuming only part of the input is uncer-
tain, the certain part of the input always leads to the same
execution across multiple sample runs. We remove such re-
dundancy by coalescing multiple sample runs in a single run.
In the coalesced run, the program operates on a vector of
values if uncertainty is present and a single value otherwise.
We handle cases where control Ô¨Çow and pointers are uncer-
tain. Our results show that we can speed up the execution
time of 30 sample runs by an average factor of 2.3 without
precision lost or by up to 3.4 with negligible precision lost.
Categories andSubjectDescriptors
D.1.2 [Programming Techniques ]: Automatic Program-
ming; D.2.5 [ Software Engineering ]: Testing and Debug-
ging; D.3.3 [ Programming Languages ]: Language Con-
structs and Features
GeneralTerms
Languages, Experimentation, Performance
Keywords
uncertainty, sensitivity, monte carlo, coalescing
1. INTRODUCTION
Uncertain data processing is becoming more and more
important. In scientiÔ¨Åc computation, data are collected
Permission to make digital or hard copies of all or part of this w ork for
personal or classroom use is granted without fee provided th at copies are
not made or distributed for proÔ¨Åt or commercial advantage and th at copies
bear this notice and the full citation on the Ô¨Årst page. To cop y otherwise, to
republish, to post on servers or to redistribute to lists, re quires prior speciÔ¨Åc
permission and/or a fee.
ICSE ‚Äô11, May 21‚Äì28, 2011, Waikiki, Honolulu, HI, USA
Copyright 2011 ACM 978-1-4503-0445-0/11/05 ...$10.00.throughinstrumentsorsensorsthatmaybeexposedtorough
environmental conditions, leading to errors. Computational
processing of these data may hence draw faulty conclusions.
For example, it was shown that a protein was mistakenly
classiÔ¨Åed as a cancer indicator by slightly altering a param-
eter of the program used to process experimental data [29].
These parameters are uncertain because they are provided
by biologists based on their experience. Such mistakes could
be highly costly because expensive follow-up wet-bench ex-
periments could be further conducted based on the faulty
protein. One of the most widely used sources for protein
data is Uniprot [8], in which proteins are annotated with
their functions. The annotations may come from real exper-
iments (accurate) or computation based on protein similar-
ity (uncertain). Software operating on these data must be
aware of the uncertainty [16]. In modern combat, soldiers
rely on data collected by sensors deployed on harsh battle-
Ô¨Åelds. Processing unreliable data and making proper deci-
sions is key to their survival. Software facilitating Ô¨Ånancial
decision making is often required to model uncertainty [19].
Traditionally, uncertaintyanalysisis conductedon the un-
derlying mathematical models [28]. However, modern data
processing uses more complex models and relies on com-
puters and programs, rendering mathematical analysis diÔ¨É-
cult. Realizing the importance of uncertain data processing,
recently, researchers have proposed database techniques to
store, maintain and query uncertain data [25, 13]. However,
more sophisticated data processing is often performed out-
side a database. Addressing uncertainty from the software
engineering and program analysis perspective becomes nat-
ural. Continuity analysis [6] is a static technique analyzing
if a given program output varies in a continuous way as in-
put changes. While the analysis has been shown to work on
simple programs like sorting algorithms, applying it to pro-
gramswithcomplexcomputationremainsachallenge. Taint
analysis [21, 7] tracks the set of inputs used to compute indi-
vidualoutputsthroughprogramdependencetracking. Users
can thus focus their attention on the relevant inputs when
analyzing uncertainty. However, it does not provide direct
help as it cannot identify whether one input in the lineage
set is more important than another.
Monte Carlo (MC) methods provide a simple and eÔ¨Äective
means of studying uncertainty [13, 5, 26]. They randomly
select input values from predeÔ¨Åned distributions and aggre-
gatethecomputedoutputstoyieldstatisticalinsightsonthe
output space. They are increasingly studied by researchers
in software engineering. For instance, MC methods are used
to classify input values critical to the output [5] and test	





	



	



 	




!
"
#
 
$
%
&

!
"
#
 
$	
 	

$ 	
	
# 

&	"

'
!
!!
!"
!#
! 
!$
!%
!&
!(
)*+ ,	-+.  +/+,0- +.	
Figure 1: Program Specialization. Assume A[0]andA[3]are uncertain; other A[i]=i.
stability of numerical implementations [26].
In this paper, we improve the eÔ¨Éciency of MC methods.
MC methods require computing independent solutions, or
trials, for many samples. In the context of uncertain data
processing, often only part of the data are uncertain. Hence,
the sample runs have a lot redundancy, dictated by the cer-
tain part of the input, i.e. the part that remains the same
across sample runs.
Eliminating redundant execution caused by a certain part
of the input is not a new challenge. Classic solutions in-
clude partial evaluation (program specialization) [15, 17,
24] and memoization. However, they fall short for our pur-
pose. In particular, partial evaluation generates specialized
versions of a program that can run faster. For example, sup-
pose a function has multiple input parameters, and some of
the parameters often take the same values. Specialized ver-
sions of the function can be generated by replacing those
parameters and computation related to them with concrete
values at compile or binding time. Consider the program
in Fig. 1 (a). Suppose variable xoften takes the value 5.0.
Specialization creates the version in (b). Specialization
is harder where only part of an array is uncertain. Recent
work specializes execution traces [24], working at the array
element level, but such techniques unroll loops and generate
linear code [14]. Supposing A[0]andA[3]are uncertain in
our example, Ô¨Ågure (c) shows the program after trace based
specialization . Observe that statements 2, 3, 4, and part of
5 are concretized as their outcomes are certain. Trace based
specialization is intended for use on functions, whereas we
need to specialize whole programs. Section 6 shows that
when only one input element is uncertain, up to 98.84% of
the executed instructions for some programs operate on un-
certain data and are hence not concretizable. This implies
the specialized programs will have a huge static size.
Memoization [18, 22, 1] is a technique that caches func-
tion level output for frequently occurring function inputs so
that redundant computation can be pruned. It works well
when a function is called frequently in a single run and pos-
sible inputs to the function are few. However, our scenario
is diÔ¨Äerent: in one sample run, a program may call some
functions a large number of times, but likely with diÔ¨Äerent
inputs each time; the same function inputs only occur across
runs. To reduce such cross-run redundancy, we may have to
cache for all the possible function inputs in a single run.
We propose a technique called execution coalescing that
packs multiple MC trials into one execution. The approach
allows users to mark program inputs that should receive
samples from a random distribution. Using this informa-
tion, our approach automatically Ô¨Ånds work that is common
across multiple trials and coalesces it so that the common
work is only done once, but the resultsof that work can still
be used independently in each trial. In particular, a variable	
	
	
		


		






 


 
	
	
	
!
"#
	$	
	$
	%
"#
	$	
	$
	%
!
 
	
"
	$ 
	$
	% 
Figure 2: Coalesced execution. Assume three sam-
ples are taken for A[0]and they are {-1.0, 0.0, 1.0 }.
The state shows the left hand side values after each
statement execution.
is associated with a vector if it is directly or indirectly com-
puted from uncertain input. Assume its size is n. Theith
element in the vector corresponds to the value of the vari-
able in the ith sample run. Operations on uncertain values
are carried out on individual elements in the vectors. One
step forward in the packed execution is equivalent to step-
ping forward in the nsample runs simultaneously. Variables
that are certain have only one value. In this case, stepping
forward in the nsample runs is done by executing only one
operationinsteadofexecuting ninstructionssimultaneously,
which allows eliminating redundancy.
Consider the program in Fig. 1 (a). Assume the random
values-1.0,0.0and1.0are chosen for A[0]in three sepa-
rate trials. The coalesced execution of these trials together
is shown in Fig. 2. These diÔ¨Äerent values are shown as a vec-
tor at the Ô¨Årst instance of line 5. Furthermore, the instances
of line 6 also have vector values as they rely on the uncer-
tain input and may yield diÔ¨Äerent results for each trial. In
contrast, because the values generated at other places are
shared across all trials, the state of the variables is repre-
sented by individual values. Note that if the three trials
are executed independently, the shared evaluations must be
repeated three times. For example, line 2 needs to execute
three times without coalescing but only once with it.
Recent advances in hardware and software enable eÔ¨É-
ciency in execution coalescing. Our technique tracks uncer-
tain values that need to be associated with a vector through
program dependence tracking, which is often the dominant
overhead factor. Recent work [23] shows that such analysis
can be implemented eÔ¨Éciently. Moreover, the intel Stream-
ing SIMD Extensions (SSE) instruction set for x86 architec-
tures allowsoperatingon a vectorof valuesin onecycle. Our
technique is mostly transparent to users. They only need to
indicate which input values are uncertain and their distri-
butions. It has no speciÔ¨Åc requirements for the subject
programs and hence can be adapted to other applications
such as combinatorial testing. Our current implementation
supports both C and FORTRAN programs.Our contributions are summarized as follows.
‚Ä¢We present vector based program evaluation rules for
packing multiple MC trials into one execution. It coa-
lesces the common work between trials while produc-
ing unique and correct results for each individual trial.
The semantics handles conditional statements on un-
certain values. If necessary, it executes both branches,
one after the other. It also handles uncertain pointers.
‚Ä¢We conduct formal analysis on the possible savings of
our technique that allows the user to estimate beneÔ¨Åts
in diÔ¨Äerent use cases.
‚Ä¢We devise optimizations to remove unnecessary uncer-
tainty on the Ô¨Çy to further improve cost eÔ¨Äectiveness.
The optimization allows the user to exchange precision
for eÔ¨Éciency. Our evaluation shows that high perfor-
mance can be achieved with little loss of precision.
‚Ä¢We have implemented the approach and evaluated it
to examine the impact and utility of diÔ¨Äerent parame-
ters that characterize eÔ¨Écacy in reducing the runtime
costs of Monte Carlo techniques.
2. COALESCING EXECUTIONS
In this section, we formally deÔ¨Åne execution coalescing
and discuss some important properties.
P‚ààL::=s
s‚ààStmt::=s1;s2|skip|x‚Üêe
|if(x)thens1elses2endif
|while(e)sendwhile
e‚ààExpr::=x|c|x1binopx2|x1binopc|input()|...
x‚ààVar::={x,y,...}
c‚ààConst::={true,false,0,1,2,...}
input() ::=‚ä•‚Üí(c|normal(c,c)|uniform(c,c)|...)
Figure 3: Simple kernel language Lwith Monte
Carlo sampling.
Our system is built on top of gccand hence supports mul-
tiple programminglanguages. For generality, our formal dis-
cussion is facilitated with a simple kernel language presented
in Fig. 3. The kernel language includes no-ops, assignments,
conditional execution via ifstatements, looping with while
statements, binary operators, and both constant and vari-
able values. It explicitly models program input through the
input() function. The function returns a certain value or
a distribution. The simplest form is a uniform distribution
over a range. For the moment, the kernel language does not
model functions, arrays and pointers. We will discuss how
to support these features in Section 4.
Trial ::={1,...,N}
œÉ‚ààStore ::=Var‚ÜíConst
Œì‚ààSampleStore ::=Var‚Üí(Trial‚ÜíConst)
¬µ‚ààSampleMask ::=P(Trial)
Figure 4: DeÔ¨Ånitions for evaluation.
2.1 With Certain Control Flow
We Ô¨Årst introduce how the technique works assuming con-
trol Ô¨Çow is certain. In other words, we assume predicates
do not operate on uncertain variables, and hence diÔ¨Äerenttrials follow the same control Ô¨Çow. For instance, in a ma-
trix multiplication program, if only the matrix elements are
uncertain, execution always follows the same path.
Fig. 4 presents the deÔ¨Ånitions relevant to program state.
SymbolNrepresents the number of sample runs we want to
coalesce. We assign a unique id for each sample run. Trial
represents the set of ids. Symbol œÉrepresents the regular
store, which is a mapping from variables to constants. To
allow coalescing, we introduce an extra store in the sample
space, denoted as Œì. It maps a variable to a mapping from
a sample run id to a value. Intuitively, for each variable, it
stores the values of the variable for each sample run if the
variable is uncertain. We call the mapping the vector value
of the variable, whereas the value in the regular store is the
regular value . Symbol ¬µis used in the presence of uncertain
control Ô¨Çow, which will be discussed in the next section.
The evaluation rules are presented in Table 1. They spec-
ify the actions for evaluating a statement when the condi-
tions are satisÔ¨Åed. The rule names are provided in the last
column. We allow two types of input through the explicit
input() method. In rule Input-Certain , if the statement
reads a certain input value to variable x,xis associated
with the value in the regular store and with ‚ä•in the sample
store, meaning that xis undeÔ¨Åned in the sample store. Note
thatxmight have had an uncertain value before the assign-
ment. Rule Input-Uncertain speciÔ¨Åes that if the input is
uncertain, i.e., the input method returns a distribution, N
samples will be taken and stored to the sample space.
The remaining three rules specify evaluation of the assign-
ment of a binary operation. Rule Binop-Both-Uncertain
applies when both source operands x1andx2have uncertain
values. According to the rule, the left-hand-side variable x
maps to a vector storing the results of the binary operation
on the corresponding elements in the two source vectors. If
x1is uncertain and x2certain, rule Binop-1st-Uncertain
applies and xhas a vector value. Each vector element is
computed from the corresponding vector element in x1and
the value of x2in the regular store. If both source operands
have certain values (rule Binop-Both-Certain ), the result-
ing value is computed from the regular values and updated
to the regular store, and the sample store of xis reset to un-
deÔ¨Åned. Other rules are similarly derived and hence elided.
Fig. 2 shows an example of such evaluation.
2.2 With UncertainControl Flow
In real programs, there are often predicates operating on
uncertain values. In such cases, it is uncertain which branch
will be taken. To handle these cases, upon encountering an
uncertain predicate, we split the coalescing into two sub-
coalescings: one with the sample runs following the true
branch, called the true coalescing , and the other with those
following the false branch, called the false coalescing . We
evaluate them one after the other. At the merge point of
the two branches, the two subcoalescings conjoin to the orig-
inal coalescing. If uncertain predicates nest, a subcoalesc-
ing may further split into smaller subcoalescings1. We have
to make sure the evaluations of the split subcoalescings of
the same predicate are isolated, otherwise a deÔ¨Ånition in the
true branch evaluation may reach a use in the following false
branch evaluation of the same predicate, leading to errors.
Table 2 presents an important subset of evaluation rules.
1Such splittings are bounded because subcoalescings with
only one trial cannot be split.Table 1: Evaluation rules with certain control Ô¨Çow.
statement condition action name
x‚Üêinput()input() returns constant cœÉ(x) =c; Œì(x) =‚ä• Input-Certain
input() returns distribution d‚àÄi‚ààTrial,Œì(x,i) =random(d) Input-Uncertain
x‚Üêx1binopx2Œì(x1)/ne}ationslash=‚ä•Œì(x2)/ne}ationslash=‚ä•‚àÄi‚ààTrial,Œì(x,i) = Œì(x1,i)binopŒì(x2,i)Binop-Both-Uncertain
Œì(x1)/ne}ationslash=‚ä•Œì(x2) =‚ä•‚àÄi‚ààTrial,Œì(x,i) = Œì(x1,i)binopœÉ(x2)Binop-1st-Uncertain
Œì(x1) =‚ä•Œì(x2) =‚ä• œÉ(x) =œÉ(x1)binopœÉ(x2); Œì(x) =‚ä• Binop-Both-Certain
... ... ...
Table 2: Evaluation rules with uncertain control Ô¨Çow. We use /llbrackets/rrbracketto denote the action of evaluating a
statement s. We use Œì(x,¬µ)as the shorthand for ‚àÄi‚àà¬µ,Œì(x,i).
statement condition action name
if(x)thens1elses2Œì(x,¬µ) =true /llbrackets1/rrbracket If-UC-True
‚àÉi‚àà¬µ,Œì(x,i) =true
‚àÉj‚àà¬µ,Œì(x,j) =false¬µT={i‚àà¬µ,Œì(x,i) =true};
¬µF=¬µ‚àí¬µT;push(¬µ);
¬µ=¬µT;/llbrackets1/rrbracket;
¬µ=¬µF;/llbrackets2/rrbracket;
¬µ=pop();If-UC-Both
... ... ...
x‚Üêinput()input() returns constant c
¬µ=TrialœÉ(x) =c; Œì(x) =‚ä• Input-C
input() returns constant c
¬µ‚äÇTrialŒì(x,¬µ) =c Input-C-Mask
input() returns distribution d Œì(x,¬µ) =random(d) Input-UC
x‚Üêx1binopx2Œì(x1) =‚ä•Œì(x2) =‚ä•¬µ=Trial œÉ(x) =œÉ(x1)binopœÉ(x2); Œì(x) =‚ä• Binop-Both-C
Œì(x1,¬µ) =‚ä•Œì(x2,¬µ) =‚ä•
¬µ‚äÇTrialŒì(x,¬µ) =œÉ(x1)binopœÉ(x2) Binop-Both-C-Mask
Œì(x1,¬µ)/ne}ationslash=‚ä•Œì(x2,¬µ)/ne}ationslash=‚ä•‚àÄi‚àà¬µ,Œì(x,i) = Œì(x1,i)binopŒì(x2,i) Binop-Both-UC
... ... ...
x‚Üêx1binopcŒì(x1,¬µ) =‚ä•¬µ‚äÇTrial Œì(x,¬µ) =œÉ(x1)binopc Binop-C-Const-Mask
... ... ...
while(x)sendwhile/llbracketif(x)thens;while(x)sendwhile
else skip /rrbracketWhile
We enhance program state with a sample mask ¬µ, whose
deÔ¨Ånition is in Fig. 4. It identiÔ¨Åes which trials are being
evaluated along the current path. We only need to compute
the values for these trials in the current path. Other trials
require separate computation along diÔ¨Äerent paths.
The Ô¨Årst two rules describe the evaluation of an ifstate-
ment predicating on uncertain values. The Ô¨Årst rule If-UC-
TruespeciÔ¨Åes that even though the predicate operates on
a vector value, if the vector elements are universally true,
we only evaluate the true branch. Rule If-UC-Both speci-
Ô¨Åes that if predicate xhas both trueandfalsevalues, we
divide the current mask ¬µinto two submasks ¬µTand¬µF,
each identifying those trials that follow the true and false
branches, respectively. The true branch statement s1evalu-
ates with ¬µTands2evaluates with ¬µF. Note that although
the stores are not explicitly speciÔ¨Åed in the rule, they up-
date in evaluation order. In other words, the evaluation of
s2operates on stores that have been updated in s1‚Äôs evalu-
ation. The two submasks facilitate isolation, i.e. preventing
the evaluation of s2from seeing values produced in s1.
Thenextthreerulesdescribeinputbehavior. Rule Input-
CspeciÔ¨Åes that we save the certain input to the regular store
only when ¬µis the universal set Trial, meaning the current
coalescing has not been split, i.e. the current path is cer-
tain. Rule Input-C-Mask speciÔ¨Åes that although the input
is certain, the assignment is performed on the sample store
if¬µis a subset. More intuitively, the rule dictates that if
the evaluation is for a split coalescing, we cannot save the
value to the regular store even though it is certain. Because
it is certain only regarding the sample runs indicated by ¬µ,
it might have a diÔ¨Äerent value in other sample runs.The next three rules are for the assignment of a binary op-
eration. Rule Binop-Both-C speciÔ¨Åes that if both operands
are certain and the sample mask is the universal set, we up-
datetheregularstoreof xandresetthesamplestore. Incon-
trast, from Binop-Both-C-Mask , if the sample mask is only a
subset, we update the sample store although both operands
are certain. Note, we test if an operand is certain regarding
the current mask , i.e. Œì( x,¬µ) =‚ä•, instead of Œì( x) =‚ä•.
The reason is that Œì( x,Trial‚àí¬µ) might have been deÔ¨Åned
in the evaluation along the other split branch, which has
no implications on whether xis certain along this branch.
RuleBinop-Both-UC speciÔ¨Åes that if both source operands
are uncertain, we update the sample store, constrained by ¬µ.
RuleBinop-C-Const-Mask is similar to Binop-Both-C-Mask .
Observe that any assignment along a split path ( ¬µ‚äÇTrial)
only updates the sample store but never the regular store,
which ensures that uses of the regular store in rule Binop-
Both-C-Mask in a split branch never see values deÔ¨Åned in
the other branch, but rather those before the split.
RuleWhileevaluates a whilestatement to an ifstate-
ment so that the rules for the ifstatement can be used.
Example. Consider the program in Fig. 5. It computes
a person‚Äôs salary based on the rate and the hours she/he
works. Table 3 presents a sample evaluation. The Ô¨Årst col-
umn shows the control Ô¨Çow, the second and third columns
show the regular store and the sample store, and the last
column shows the rules applied. Here, we coalesce Ô¨Åve sam-
ple runs. At the beginning, the sample mask is the universal
set. The input at line 2 is uncertain, so we take 5 samples.
At line 3, we apply rule If-UC-Both , and the mask is di-
vided. The 1st, 3rd, and 5th sample runs evaluate in the1 r‚Üêinput();
2 h‚Üêinput();
3if(p‚Üêh>40)
4thenr‚Üêr+2
5 s‚Üêr√óh
6 if(q‚Üês>500)
7 thens‚Üê500
8elser‚Üêr‚àí1
9 s‚Üêr√óh
10endif;
11 output(s)
Figure 5: Example for uncertain control Ô¨Çow. The
program computes salary sfrom rate rand work
hoursh. The rate is higher (line 4) if a person works
for more than 40 hours. The salary has a cut-oÔ¨Ä
value 500 (lines 6 and 7). If a person works for less
than 40 hours, the rate is reduced (line 8).
true branch, and the 2nd and 4th runs in the false branch.
At line 4, even though rholds a certain value, the sample
store updates for the 1st, 3rd, and 5th runs. At line 6, the
mask further divides into two submasks. Hence, the assign-
ment at line 7 only updates the value for the 5th run to 500
as highlighted. After the true branch evaluates, the false
branch evaluates. At line 8, rule Binop-C-Const-Mask ap-
plies so that the 2nd and 4th elements of r‚Äôs vector update.
Note that the values are computed from r‚Äôs value in the reg-
ular store œÉ, and hence the deÔ¨Ånition to rat line 4 has no
eÔ¨Äect on line 8. At line 11, the submasks join. Observe, at
the end the values in the sample store are identical to those
acquired in the corresponding independent sample runs.
Safety. In the following, we present the safety claim of our
technique. It is critical to show that the coalesced execution
is equivalent to the Nindependent MC trials. Note that
an independent MC trial is a regular program execution.
Assume random sampling is deterministic for each uncertain
input, i.e., given the same random seed and a distribution,
thesame Nsamplesaregenerated. We furtherdeÔ¨Ånethe ith
independent trial as the execution with each uncertain input
taking the ith sample in the sequence of N. For instance,
the Ô¨Årst MC trial in Table 3 corresponds to the execution
taking the inputs r= 8 and h= 45.
Property 1.Execution coalescing is safe. In particular,
at the end of program evaluation,
(1) if a variable xhasŒì(x)‚â°‚ä•, it must have the same
value across the Nindependent MC trials and the value is
identical to œÉ(x);
(2) ifŒì(x)/ne}ationslash=‚ä•,Œì(x,i)must be identical to the value of x
in theith MC trial.
The property holds for our kernel language. The proof is
elided. For real programs, it holds when we do not consider
exceptions and interrupts. We leave it to our future work to
extend the technique to handle such cases.
3. COSTBENEFITMODEL
The computational beneÔ¨Åts of execution coalescing de-
pend on a combination factors. In this section, we present
a model for estimating the beneÔ¨Åts.
Without coalescing, all MC trials are executed indepen-
dently. Let‚Äôs Ô¨Årst consider the case that all trials execute
along the same control Ô¨Çow. If there are Ntrials, with each
trial executing Iinstructions, the total number of instruc-tionsS(N) is given as follows.
S(N) =NI (1)
In comparison, the cost of execution coalescing, in terms
of executed instructions, is decided by the following factors.
‚Ä¢The number of trials that are coalesced. Let it be N.
‚Ä¢Foreachinstruction,ourtechniquemustcheckwhether
it operates on uncertain variables. Let the slow down
factor for such checking be K.
‚Ä¢We represent the percentage of instructions operating
on uncertain values as T. Hence, in the coalesced exe-
cution,I(1‚àíT) instructions need to execute just once
for thentrials. For each of the remaining ITinstruc-
tions, the operation is performed on vectors, which is
equivalent to executing the instruction Ntimes.
‚Ä¢There is a constant bookkeeping overhead factor M
when executing an instruction on a vector.
Combining all these components yields the expected cost
presented in equation 2.
C(N,K,T,M ) =KI+NMTI = (K+NMT)I(2)
We further compute the beneÔ¨Åt factor Bin equation 3.
B=S(N)
C(N,K,T,M )=NI
(K+NMT)I=1
K/N+MT(3)
Execution coalescing is beneÔ¨Åcial when B >1. A few
observations can be made from equation 3.
‚Ä¢Nneeds to be larger than KandMTneeds to be
smaller than 1. In our implementation, Kis often a
constant in the range 4-10. Hence, we need to coalesce
enough executions to make the technique beneÔ¨Åcial.
‚Ä¢IfMT <1,Bincreases as Nincreases, reÔ¨Çecting that
if more trials are coalesced, each executed instruction
that is shared across trials has a greater reduction in
overall work. It is bounded by1
MT.
‚Ä¢The beneÔ¨Åt increases as the uncertain percentage T
decreases. This reÔ¨Çects that as fewer instructions op-
erate on uncertain values, more of the execution can
be coalesced across trials. It is bounded byN
K.
The above analysis allows us to decide if execution coa-
lescing is appropriate and tune the conÔ¨Åguration of N(and
Tif possible). For instance, assume M= 1.20 andT= 0.90.
SinceMT= 1.08>1, it is guaranteed that coalescing pro-
vides no beneÔ¨Åt. Moreover, assume M= 1.20,T= 0.33,
andK= 4.0. If we want to achieve a speed up B= 2, we
should use N= 40.
When uncertain control Ô¨Çow is considered, the beneÔ¨Åt fac-
tor is computed as follows.
B=1+U
K/N+MT+KMU(4)
Uis the ratio between the instructions that are unique
in a trial over those common in all trials, describing the
percentage of uncertain control Ô¨Çow. We assume Uis a
constant over all trials. The derivation of the inequality is
omitted for space. From the equation, to satisfy B >1, the
following condition must hold.
U <1‚àíK/N‚àíMT
KM‚àí1(5)Table 3: Sample execution for program in Fig. 5. h/mapsto‚Üí{...,525}meanshhas value 52 in the 5th sample run.
control Ô¨Çow œÉŒì ¬µ rule
1 r‚Üêinput () r/mapsto‚Üí8 {1,2,3,4,5} Input-C
2 h‚Üêinput () h/mapsto‚Üí{451,302,483,254,525}{1,2,3,4,5} Input-UC
3if(p‚Üêh>40) p/mapsto‚Üí{T1,F2,T3,F4,T5}{1,2,3,4,5} If-UC-Both
4thenr‚Üêr+2 r/mapsto‚Üí{101,103,105} {1, 3, 5}Binop-C-Const-Mask
5 s‚Üêr√óh s/mapsto‚Üí{4501,4803,5205} {1, 3, 5}Binop-Both-UC
6 if(q‚Üês>500) q/mapsto‚Üí{F1,F3,T5} {1, 3, 5} If-UC-Both
7 thens‚Üê500 s/mapsto‚Üí{4501,4803,5005} {5} Assgn-Const-Mask
8elser‚Üêr‚àí1 r/mapsto‚Üí{101,72,103,74,105}{2, 4}Binop-C-Const-Mask
9 s‚Üêr√óh s/mapsto‚Üí{4501,2102,4803,1754,5005}{2, 4} Binop-Both-UC
11 output(s) {1,2,3,4,5}
Table 5: Example for uncertain array indices. As-
sume the sample mask is {1,2};œÉ={A[0]/mapsto‚Üí3, A[1]/mapsto‚Üí
9};Œì ={i/mapsto‚Üí{01,12}, j/mapsto‚Üí{11,02}}.
instruction wrong correct
1A[i] = 5 œÉ={A[0]/mapsto‚Üí5,Œì ={...,A[0]/mapsto‚Üí{51},
A[1]/mapsto‚Üí5} A[1]/mapsto‚Üí{52}}
2A[j] = 7œÉ={A[0]/mapsto‚Üí7,Œì ={...,A[0]/mapsto‚Üí{51,72},
A[1]/mapsto‚Üí7} A[1]/mapsto‚Üí{71,52}}
1 for (each i‚àà¬µ)
z=libfoo(x,y)2 Œì( z,i)=libfoo(Œì(x,i), Œì(y,i));
Figure 6: Calling a library function.
While we can conÔ¨Ågure Nto reduce the eÔ¨Äect of K/Nin
the condition, the technique must not be beneÔ¨Åcial if U >
1‚àíMT
KM‚àí1. Intuitively, our technique cannot be beneÔ¨Åcial when
theratiobetweenthedivergentcontrolÔ¨Çowandthecommon
control Ô¨Çow is higher than a threshold.
4. HANDLING PRACTICALISSUES
In this section, we discuss how to support more complex
features that are not modeled by our language.
Uncertain Array Indices. Extra eÔ¨Äort is needed when
uncertain values are used as array indices. We cannot sim-
ply perform the array operation on the regular store with
the addresses speciÔ¨Åed by the vector value of the index, even
though that seems straightforward. Instead, the operation
must be performed on the sample store. SpeciÔ¨Åcally, a vec-
torvneeds to be created for each address speciÔ¨Åed by the
uncertain index. Only the ith element of the vector created
for theith address gets updated. The reason is that the ith
address should only be used in the evaluation of the ith trial.
Consider the example in Table 5. Indices iandjare
uncertain. Both can take values 0 and 1 but take diÔ¨Äerent
values within the same trial. The second column shows that
if we follow the na ¬®ƒ±ve approach, after evaluating line 1, A[0]
andA[1] map to 5 in œÉ. After line 2, both map to 7. The
state after line 2, however, is wrong and does not correspond
to the coalescing of the two trials. Proper evaluation should
yield that at line 1, both A[0] andA[1] map to vector values,
and only the 1st trial in A[0] and the 2nd trial in A[1] are
set to 5. Similarly, after line 2, only the 2nd trial in A[0]
and the 1st trial in A[1] are set to 7. The state correctly
represents the coalescing of the two independent trials.
Table 4 presents some of the array access rules. We cur-
rently do not support uncertain base addresses ( Ahas to be
certain). Uncertain pointers are supported in the same way
because they are equivalent to uncertain array indices.
Functions. Handling user deÔ¨Åned functions is direct. For
library functions taking uncertain arguments, since we do
not have their source code, we cannot transform the func-tions to operate on uncertain values. Our solution is to wrap
the function call in a loop that iterates through each trial in
the sample mask. Each iteration calls the function with the
regular values in a trial (extracted from the sample store).
The results are written to the corresponding elements in the
result vector. Fig. 6 shows an example.
5. OPTIMIZATIONBYREMOVINGUNNEC-
ESSARYUNCERTAINTY
We observe that during coalesced execution, there are
variables that are considered uncertain, having vector val-
ues, but the values in the vector are identical or have only
negligible diÔ¨Äerences. We can reduce the vector to a single
value so that the subsequent computation with these vari-
ables can be re-coalesced. Such cases can be caused by:
‚Ä¢An uncertain value going through an operator repre-
senting a many-to-one mapping. Multiple inputs to
the operator can lead to the same output. Sample sce-
narios include: multiply by 0 (e.g. y‚Üêx√ó0), mod-
ulo operation (e.g. y‚Üêx%10), bit operations (e.g.
y‚Üêx& 0xfff), and comparisons (e.g. p‚Üê(x >
10)). Note that our evaluation rules are suboptimal
for these cases because yis uncertain as long as xis
uncertain, disregarding the values in the vector of y.
‚Ä¢Floating point round-oÔ¨Ä, overÔ¨Çow, and underÔ¨Çow can
also lead to identical values in vectors. For instance,
consider the following statement.
y‚Üêx+1.0e9
And assume Œì( x) ={0.0021,0.0052,0.0073}. Since
the Ô¨Çoating point representation can only hold a Ô¨Åxed
number of the most signiÔ¨Åcant digits (suppose it can
hold 7 digits), the contribution from xis then rounded
oÔ¨Ä, leading to Œì( y) ={1.0e91,1.0e92,1.0e93}.
‚Ä¢Recall that when we encounter an uncertain predicate,
weÔ¨Årstevaluateasubsetoftrialsalongthetruebranch
and then the remaining trials along the false branch.
Any assignments inside these branches are performed
on the sample store. It is possible that a variable is
deÔ¨Åned with the same value in the two branches.
There are also cases where Ô¨Çoating point values in a vector
are highly similar although they are not completely identi-
cal. We hence develop a general solution to leverage the
above observations. Given a signiÔ¨Åcance threshold k, if the
diÔ¨Äerences between the Ô¨Årst value in a vector Œì( x) and any
other values in the vector are less than k, we re-coalesce the
vector to a single value. The threshold based recoalescingTable 4: Evaluation rules for uncertain array indices.
statement condition action name
A[x] =cŒì(A) =‚ä•Œì(x,¬µ)/ne}ationslash=‚ä• ‚àÄi‚àà¬µ,Œì(A[Œì(x,i)],i) =c Arr-Const-Wr-UC
y=A[x]Œì(A) =‚ä•Œì(x,¬µ)/ne}ationslash=‚ä• ‚àÄi‚àà¬µ,Œì(A[Œì(x,i)],i)/ne}ationslash=‚ä•‚àÄi‚àà¬µ,Œì(y,i) = Œì(A[Œì(x,i)],i) Arr-Rd-UC
provides a means to trade a conÔ¨Ågurable degree of precision
for increased eÔ¨Éciency by decreasing Tin equation 3 (the
uncertain ratio). Note that when k= 0, we only merge
identical values, no precision is lost.
6. EMPIRICALEVALUATION
Our system is built in gcc, based on the GIMPLE IR.
Since our technique operates on vectors, we leverage SIMD
(Single Instruction, Multiple Data) instructions for better
performance. In particular, we use packed Ô¨Çoating point in-
structions,suchasaddition,subtractionorcomputingsquare
roots, from the SSE2 extension of SIMD instructions. Note
that the beneÔ¨Åt of SSE2 instructions is limited by the width
of the SSE registers, which is 128-bit for our machine. In
other words, we can only pack two double precision Ô¨Çoating
point values at a time.
Our experiments are performed on an Intel quad core
Xeon 1.86GHZ machine with 4GB RAM. We use SPECFP-
2000 as the benchmark set, which includes both C and For-
tran programs. We excluded 2 programs. In particular,
189.lucas is a program that identiÔ¨Åes prime numbers and
hence uncertainty analysis is not applicable. 177.mesa is
omitted because we have not supported direct assignments
of aggregate types such as assignments of struct. We have
total 12 programs (3 C and 9 Fortran).
Our Ô¨Årst experiment evaluates the beneÔ¨Åts of execution
coalescing together with its space overhead but without the
optimization removing unnecessary uncertainty. For each
program, we vary the number of coalesced trials (factor N
in eqn 3 in Section 3) among 1, 10, 20, and 30. We also
vary the percentage of input marked as uncertain, which will
aÔ¨Äect the percentage of instructions operating on uncertain
values (Tin eqn 3), among 1 input, 1%, 5%, 10%, 15%,
and 20% of the input. For each uncertain input, we select
samples from 50%-100% of the original value following a
uniform distribution. The reason that we select samples
smaller than the original is that larger samples may fall out
of the legal range.
Detailed results are presented in Table 6. Details for 30
samples are omitted for space. The nativecolumns show the
originaltime. The T%columnsshowthepercentageofstate-
ments operating on uncertain values. The Kcolumns show
the normalized overhead when only one sample is taken,
which corresponds to running the program on the original
input (w/o uncertainty) with the overhead of checking and
propagating uncertainty (the Kfactor in eqn 3). The B
columns show the beneÔ¨Åt factor (i.e. the speedup) with
the subscripts representing the number of samples. For
172.mgrid and301.apsi , the columns under 1% uncertain
inputpresent data for marking all inputs as uncertain due
to their small number of inputs. We also summarize the
results in Fig. 7, showing the variation in the beneÔ¨Åt over
the number of samples, taking the average over the percent-
age of uncertain input. Note that we put in the data for
30 samples. Fig. 8 further shows the variation in beneÔ¨Åt
over uncertain input, taking the average over the number of
samples. Fig. 9 summarizes the normalized space overhead(details omitted for space). From the table and the Ô¨Ågures,
we make the following observations.
1.11.82.3
0123456
10-sample 20-sample 30-sample
Figure 7: Change in speedup over the number of
samples.
0.01.02.03.04.05.01 uncertain input 1% uncertain input 5% uncertain input
10% uncertain input 15% uncertain input 20% uncertain input
Figure 8: Change in speedup over uncertain input.
0510152025300-sample 10-sample 20-sample 30-sample
Figure 9: The normalized space overhead.
‚Ä¢Coalescingcan speed up MC simulationsbyan average
factor ranging from 0.6 to 5.2 with an average of 2.3
when 30 samples are coalesced. We expect the num-
ber to be high if memory is large enough to Ô¨Åt more
samples. The average overhead of checking and propa-
gating uncertainty ( K) ranges from 2.1 to 17.0 with an
average 7.9. Since the nature of the uncertainty propa-
gation is similar to dynamic information Ô¨Çow tracking
and the state of the art [23] reports an average 2 times
speedup if their aggressive optimizations are applied,
we speculate our overhead can be similarly reduced in
the future. The large Kimplies that we have to coa-
lesce suÔ¨Écient trials to make it beneÔ¨Åcial (see eqn 3).
‚Ä¢The speedup increases with the number of coalesced
samples. It decreases as more inputs are marked un-
certain. The decrease is not that substantial for some
programs. When Tis large, we hardly beneÔ¨Åt with 10
samples.
‚Ä¢The space overhead ties closely with the number of co-
alesced samples. It could be high when coalescing a
large number of samples. We use the standard shadowTable 6: Performance with diÔ¨Äerent conÔ¨Ågurations.
program native# of 1 uncertain input 1% uncertain input 5% uncertain input
input T% KB10B20T% KB10B20T% KB10B20
168.wupwise 4.39 4.6E6 29.27% 4.981.242.0030.31% 5.141.231.9033.02% 5.241.141.80
171.swim 0.40 2.6E5 0.64% 4.162.364.69 3.18% 4.382.003.94 6.72% 4.781.733.20
172.mgrid 34.09 1098.84% 5.800.881.4398.84% 5.800.881.43 N/A
173.applu 0.41 6566.42% 10.10 0.631.0466.42% 10.10 0.631.0467.87% 10.06 0.631.05
178.galgel 1.61 5.2E3 54.01% 8.061.172.2154.05% 8.341.172.1854.08% 7.871.172.22
179.art(C) 27.64 1.0E4 25.42% 7.780.891.4825.42% 6.990.891.4825.43% 6.560.921.57
183.equake(C) 0.48 7.3E3 20.88% 6.261.181.9421.90% 6.351.171.9326.94% 6.861.001.65
187.facerec 9.64 6.6E4 16.76% 2.082.664.1516.95% 2.082.634.0917.01% 2.082.634.07
188.ammp(C) 4.49 9.6E3 9.59% 5.941.382.39 9.59% 5.881.372.38 9.59% 5.911.362.42
191.fma3d 5.08 544 87.82% 13.57 0.470.6787.95% 13.41 0.470.6888.65% 13.52 0.460.67
200.sixtrack 7.19 7.9E4 65.17% 15.12 0.410.5278.40% 16.84 0.350.4478.40% 16.82 0.350.45
301.apsi 4.41 98.35% 4.152.113.67 8.35% 4.162.093.65 N/A
program10% uncertain input 15% uncertain input 20% uncertain input
T% KB10B20T% KB10B20T% KB10B20
168.wupwise 34.65% 5.441.101.7436.84% 5.531.06N/A 37.92% 5.591.04N/A
171.swim 11.34% 5.391.442.5317.38% 5.911.182.0620.38% 6.361.091.85
172.mgrid N/A N/A N/A
173.applu 67.89% 10.20 0.631.0567.91% 10.20 0.631.0667.94% 10.16 0.631.07
178.galgel 58.74% 8.531.112.1558.78% 8.311.122.1358.90% 8.281.122.05
179.art 25.43% 6.930.901.5725.44% 6.770.971.5525.44% 6.770.911.56
183.equake 31.08% 7.350.841.4133.50% 7.710.801.3234.76% 7.880.791.27
187.facerec 17.03% 2.072.614.1117.04% 2.072.554.0917.05% 2.102.614.04
188.ammp 9.59% 5.891.362.39 9.59% 5.861.362.37 9.59% 5.881.382.38
191.fma3d 88.70% 13.51 0.470.6688.82% 13.54 0.470.6788.88% 13.80 0.470.67
200.sixtrack 78.40% 16.95 0.350.4578.40% 16.96 0.350.4578.40% 16.97 0.340.44
301.apsi N/A N/A N/A
memory allocation strategy [20] to allocate shadow
space for a page when any address in that page is used
by the original program. We expect a more sophisti-
cated strategy would reduce the overhead.
ThesecondexperimentshowstheeÔ¨Äectofoptimizingaway
the unnecessary uncertainty. Here, we randomly select one
conÔ¨Åguration: 10 samples and 1 single uncertain input. We
vary the re-coalescing threshold k, i.e. if the variations of
all elements in a vector are smaller than k, we re-coalesce
the vector back to one value. Fig. 10 presents the speedup
results. Table 7 presents the resulting output errors due to
re-coalescing. We do not collect output errors for the pro-
grams that do not beneÔ¨Åt from the optimization. Observe,
the optimization is not always applicable, depending on pro-
gramsemantics(seeSection5). Insuchcases, wehavetopay
the overhead for testing variations, which explains the slight
degradation for 183.equake in Fig. 10. For some programs,
it substantially improves the speedup factor, e.g. from 1.25
to 3.4 for 168.wupwise , with little lost in precision. Note
that a larger kleads to a better speedup. The precision lost
also slightly increases. Our experience with the few other
randomly selected conÔ¨Ågurations also shows similar results.
The third experiment observes the eÔ¨Äectiveness of MC
simulation. Here, we vary the uncertain inputs and observe
the output variations. We randomly select 183.equake . The
program simulates the propagation of elastic waves in large
basins. An unstructured mesh consists of nodes and linear
tetrahedral elements is used to model the earthquake area.
The program computes the displacements for each individ-
ual node. We vary the altitude of a node from 100% to 80%
(of its original value)2and observe the outputs. Fig. 11
presents the outputs with the most substantial displacement
2Weusedtheprovidedtestinputandvariednode977, which
is randomly selected from those in the source of the quake.0.00.51.01.52.02.53.03.5
no merging threshold 1 (1E-8) threshold 2 (1E-7)
threshold 3 (1E-6) threshold 4 (1E-5) threshold 5 (1E-4)
Figure 10: Speedup by re-coalescing with diÔ¨Äerent
thresholds.
values and their variations over the input changes. Each
curve represents one output. We can observe that many of
these outputs change irregularly and substantially. Tradi-
tional uncertainty analysis based on monotonicity, linearity,
and continuity would fail in this case. Furthermore, we ob-
serve that a few percent change of the inputs may lead to
substantial output changes. The variations could be a few
times larger than the origianl values.
Case Study. In the following, we further study the case
and explain the substantial and irregular output variations
by connecting them to uncertain control Ô¨Çow and uncertain
array indices.
For each node i, its position is deÔ¨Åned by a tuple of (lat-
itude, longitude, altitude) stored in ( c[i][0],c[i][1],c[i][2]),
wherec[i][2] is the altitude. The altitude of node 977 is
‚àí11.2. We look at the 10 samples from 95% to 77% of the
original altitude value to explain the impact.
Initially, we have a vector for the altitude of node 977, œÉ=
{c[977][2]/mapsto‚Üí‚àí11.2}; Œì ={c[977][2]/mapsto‚Üí{‚àí10.641,‚àí10.4162,...,
‚àí8.62410}}.Table 7: The average and maximum output error after merging, for 10 samples and 1 uncertain input.
programthreshold 1 (1E-8) threshold 2 (1E-7) threshold 3 (1E-6) threshold 4 (1E-5) threshold 5 (1E-4)
avg/max avg/max avg/max avg/max avg/max
168.wupwise 9.6E-10/1.6E-08 7.7E-09/5.5E-08 8.7E-09/1.6E-07 8.7E-09/9.1E-06 2.1E-08/9.6E-05
171.swim 3.0E-10/1.8E-05 3.0E-10/1.8E-05 3.0E-10/1.8E-05 2.8E-10/1.3E-05 2.8E-10/1.3E-05
179.art 6.4E-16/3.1E-13 6.4E-16/3.1E-13 6.4E-16/3.1E-13 6.4E-16/3.1E-13 6.4E-16/3.1E-13
187.facerec 5.4E-10/1.6E-09 3.3E-09/1.3E-08 3.3E-09/1.3E-08 3.3E-09/1.3E-08 3.3E-09/1.3E-08
188.ammp 1.3E-14/4.3E-12 3.1E-13/3.6E-12 3.1E-13/3.6E-12 3.1E-13/3.6E-12 4.7E-03/9.5E-02
191.fma3d 3.0E-09/2.9E-11 3.0E-09/2.9E-11 3.0E-09/2.9E-11 8.4E-09/2.0E-10 8.4E-09/2.0E-10
200.sixtrack 1.6E-09/4.6E-09 2.2E-09/9.3E-09 1.4E-09/6.2E-09 1.4E-09/6.2E-09 1.4E-09/6.2E-09
301.apsi 8.3E-06/1.0E-02 1.7E-04/7.7E-02 2.0E-03/9.0E-01 3.8E-05/7.9E-03 3.8E-05/8.0E-03
-4E-04-3E-04-2E-04-1E-040E+001E-042E-04
100% 95% 90% 85% 80%
Figure 11: Output variations for 183.equake .
/* Search for the node closest to the point source */
244 bigdist1 = 1000000.0;
247 for (i = 0; i < ARCHnodes; i++) {
248 c0[0] = c[i][0];
249 c0[1] = c[i][1];
250 c0[2] = c[i][2];
251 d1 = distance(c0, Src.xyz);
254 if (d1 < bigdist1) {
255 bigdist1 = d1;
256 Src.sourcenode = i;
257 }
264 }
One step of the computation selects the closest node to
the given earthquake source point in Src.xyz in the above
code. All the nodes are traversed with a loop between
line 247 and 264 (shown above). In the 979th iteration,
at line 250, c0[2] receives the vector {‚àí10.641,‚àí10.4162,...,
‚àí8.624}. Returning from the function distance() ,d1at
line 251 gets the vector of values {0.6751, 0.9982, 1.4223,
1.9464, ..., 6.0729, 7.19810}. The variable bigdist1 holds
the value 4.657. Therefore, we encounter an uncertain pred-
icate (d1 < bigdist1 ) at line 254. The sample mask ¬µ
is divided into two submasks ¬µT={1,2,...,7}and¬µF=
{8,9,10}. According to the evaluation rules in Table 2,
we compute the values of bigdist1 andSrc.sourcenode
in both branches with ¬µTand¬µFrespectively. After line
257, the values in the sample stores are joined together,
i.e.bigdist1/mapsto‚Üí{0.6751, 0.9982, 1.4223, 1.9464, ..., 4.6578,
4.6579, 4.65710},Src.sourcenode/mapsto‚Üí{9771, 9772, 9773, ...,
9738, 9739, 97310}.
After the loop, the node closest to the source point is
selected and stored in variable Src.sourcenode . Now we
haveSrc.sourcenode /mapsto‚Üí{9771, 9772, 9773, ..., 973 8, 9739,
97310}. Thus, for the Ô¨Årst 7 trials, the computed source
node is node 977 while for the last 3 trials it is node 973.
Another code snippet shows that uncertain array indices
add to the irregularity. Variable corholds a certain value
at line 2. When it equals 977 or 973, the coalescing splits
after evaluating the branch at line 293. For instance, whencorequals 973 at line 293, we get a submask of {8,9,10}, so
only the 8th, 9th and 10th elements of vertices[j][k]‚Äôs vector
get updated, according to the evaluation rules in Table 4.
Similarly, the 1st to 7th elements of the vector get updated
whencorequals 977.
289 for (i = 0; i < num_elems; i++) {
293 if (cor == Src.sourcenode) {
...
298 vertices[j][k] = c[cor][k];
306 }
307 }
In later phases, the sub-coalescings are further divided
because of other predicates. This implies that we observe
discontinuity or diÔ¨Äerent trends between outputs belonging
to diÔ¨Äerent sub-coalescings because they go through diÔ¨Äer-
entcomputation. Incontrast, outputsbelongingtothesame
sub-coalescing often have continuity or even monotonicity.
These explain the curves in Fig. 11, where monotonic seg-
ments are observed but irregularity exists across segments.
7. RELATEDWORK
Parallelized Monte Carlo. Parallelization has improved
the eÔ¨Éciency of MC techniques in the context of speciÔ¨Åc ap-
plications[2, 4]. Incontrast,ourapproachisfullyautomated
and works on already developed programs. Additionally, we
don‚Äôt rely on parallelization to improve running time, in-
stead we reduce the common, redundant work across Monte
Carlo trials. Hence, it is orthogonal to parallelization.
Partial Evaluation & Memoization. Partial evaluation
generates a new version of a program where additional as-
sumptionsaboutruntimebehaviorallowmoreaggressiveop-
timization [15]. Memoization caches the results of a function
for given input such that the results can be reused instead
of reevaluating the function. As noted in section 1, these are
useful in program optimization, but they cannot realistically
handle the combining of multiple disparate executions.
DeltaExecution. Deltaexecution[9]eliminatesredundancy
across state explorations in model checking. Each type in
the program is wrapped in a container type that maintains
a vector of the original type. At runtime, operations trans-
late to operations on these container types. The technique
closely ties into model checking. The savings largely come
from places speciÔ¨Åc to model checking, such as state com-
parison to avoid repeated state exploration. They also han-
dle control Ô¨Çow splitting diÔ¨Äerently. Leveraging state man-
agement in the model checker, they allow split branches to
work on isolated stores. Branches only merge at the end
of a method. In contrast, we cannot aÔ¨Äord checkpointing
and restoration; thus our split branches operate on the samestores and achieve isolation through careful evaluation rules.
We also aggressively merge split branches at the join point.
A variant of delta execution also relates [27]. While our
approach enables execution of one program on multiple in-
puts at once, the variant enables execution of multiple pro-
grams on one input at once, eÔ¨Éciently Ô¨Ånding diÔ¨Äerences.
Uncertainty Analysis. Uncertainty and sensitivity anal-
yses compose one client Ô¨Åeld of Monte Carlo techniques.
Other eÔ¨Äorts provide a means of partially automating these
analyses eÔ¨Éciently. These approaches include model check-
ingandtheoremproving[10,6],automateddiÔ¨Äerentiation[3]
and controlled perturbation [12]. They have diÔ¨Éculty han-
dling data structures with heterogeneous data (certain and
uncertain) or multiple uncertain variables, which is noted as
one of the reasons to use sampling based approaches [11].
8. CONCLUSIONS
We propose a technique that can coalesce multiple Monte
Carlo sample executions into one. We leverage the obser-
vation that these sample runs often share a lot of common
executionandhencecoalescingavoidsrepeatingthecommon
execution. Coalescing is achieved by allowing the program
to operate on a single value if it is the same across all the
coalesced runs, and on a vector otherwise. Our technique
executes both the true and false branches of a predicate in
an isolated fashion if its vector values can be both true and
false. Pointers, array indices, and function calls on vector
values are also handled safely such that the coalesced run
produces the same results as independent runs. Our evalu-
ation shows that we can speed up the runtime of 30 sample
runs by an average factor of 2.3 without precision lost or by
up to 3.4 with negligible precision lost.
9. ACKNOWLEDGMENTS
This research is supported, in part, by the National Sci-
ence Foundation (NSF) under grants 0845870 and 0916874.
Any opinions, Ô¨Åndings, conclusions, or recommendations in
this paper are those of the authors and do not necessarily
reÔ¨Çect the views of NSF.
10. REFERENCES
[1] U. Acar, G. Blelloch, and R. Harper. Selective
memoization. In POPL, 2003.
[2] V. Alexandrov, I. Dimov, A. Karaivanova, and C. Tan.
Parallel monte carlo algorithms for information
retrieval. Math. Comput. Simul. , 62(3-6), 2003.
[3] J. Barhen and D. Reister. Uncertainty analysis based
on sensitivities generated using automatic
diÔ¨Äerentiation. In ICCSA, 2003.
[4] I. Beichl, Y. Teng, and J. Blue. Parallel monte carlo
simulation of mbe growth. In IPPS, 1995.
[5] M. Carbin and M. Rinard. Automatically identifying
critical input regions and code in applications. In
ISSTA, 2010.
[6] S. Chaudhuri, S. Gulwani, and R. Lublinerman.
Continuity analysis of programs. In POPL, 2010.
[7] J. Clause, W. Li, and A. Orso. Dytan: a generic
dynamic taint analysis framework. In ISSTA, 2007.
[8] U. Consortium. The universal protein resource
(uniprot) in 2010. Nucleic Acids Res , 38, Jan 2010.
[9] M. d‚ÄôAmorim, S. Lauterburg, and D. Marinov. Delta
execution for eÔ¨Écient state-space exploration ofobject-oriented programs. In ISSTA, 2007.
[10] M. Heimdahl, Y. Choi, and M. Whalen. Deviation
analysis through model checking. In ASE, 2002.
[11] J. Helton, J. Johnson, C. Sallaberry, and C. Storlie.
Survey of sampling-based methods for uncertainty and
sensitivity analysis. Reliability Eng. & Sys. Safety ,
91(10-11), 2006.
[12] Y. Ho, M. Eyler, and T. Chien. A gradient technique
for general buÔ¨Äer storage design in a production line.
Int. Jour. of Prod. Res. , 17(6), 1979.
[13] R. Jampani, F. Xu, M. Wu, L. Perez, C. Jermaine,
and P. Haas. Mcdb: a monte carlo approach to
managing uncertain data. In SIGMOD , 2008.
[14] N. Jones, C. Gomard, and P. Sestoft. Partial
evaluation and automatic program generation .
Prentice-Hall, Inc., 1993.
[15] N. Jones, P. Sestoft, and H. Sondergaard. An
experiment in partial evaluation: the generation of a
compiler generator. In RTA, 1985.
[16] P. Karp. What we do not know about sequence
analysis and sequence databases. Bioinformatics ,
14(9), 1998.
[17] G. Keller, H. ChaÔ¨Äey-Millar, M. Chakravarty,
D. Stewart, and C. Barner-Kowollik. Specialising
simulator generators for high-performance monte-carlo
methods. In PADL, 2008.
[18] Y. Liu and T. Teitelbaum. Caching intermediate
results for program improvement. In PEPM, 1995.
[19] M. Morgan and M. Henrion. Uncertainty: A Guide to
Dealing with Uncertainty in Quantitative Risk and
Policy Analysis . Cambridge University Press, 1992.
[20] N. Nethercote and J. Seward. Valgrind: a framework
for heavyweight dynamic binary instrumentation. In
PLDI, 2007.
[21] J. Newsome and D. Song. Dynamic taint analysis for
automatic detection, analysis, and signature
generation of exploits on commodity software. In
NDSS, 2005.
[22] W. Pugh and T. Teitelbaum. Incremental
computation via function caching. In POPL, 1989.
[23] O. Ruwase, S. Chen, P. Gibbons, and T. Mowry.
Decoupled lifeguards: enabling path optimizations for
dynamic correctness checking tools. In PLDI, 2010.
[24] A. Shankar, S. Sastry, R. Bod¬¥ ƒ±k, and J. Smith.
Runtime specialization with optimistic heap analysis.
InOOPSLA , 2005.
[25] S. Singh, C. MayÔ¨Åeld, R. Shah, S. Prabhakar,
S. Hambrusch, J. Neville, and R. Cheng. Database
support for probabilistic attributes and tuples. In
ICDE, 2008.
[26] E. Tang, E. Barr, X. Li, and Z. Su. Perturbing
numerical calculations for statistical analysis of
Ô¨Çoating-point program (in)stability. In ISSTA, 2010.
[27] J. Tucek, W. Xiong, and Y. Zhou. EÔ¨Écient online
validation with delta execution. In ASPLOS , 2009.
[28] B. Worley. Deterministic uncertainty analysis.
Technical report, Oak Ridge National Lab. TN, 1987.
[29] M. Zhang, X. Zhang, X. Zhang, and S. Prabhakar.
Tracing lineage beyond relational operators. In VLDB,
2007.