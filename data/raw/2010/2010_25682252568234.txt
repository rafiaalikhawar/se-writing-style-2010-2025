Mining Behavior Models from
User-Intensive Web Applications
Carlo Ghezzi
Politecnico di Milano, Italy
DeepSE Group at DEIB
carlo.ghezzi@polimi.itMauro Pezz√®
University of Lugano,
Switzerland
mauro.pezz√®@usi.ch
Michele Sama
Head of cloud and data
Touchtype Ltd, UK
michele@swiftkey.netGiordano Tamburrelli
University of Lugano,
Switzerland
giordano.tamburrelli@usi.ch
ABSTRACT
Many modern user-intensive applications, such as Web ap-
plications, must satisfy the interaction requirements of thou-
sands if not millions of users, which can be hardly fully un-
derstood at design time. Designing applications that meet
user behaviors, by eciently supporting the prevalent nav-
igation patterns, and evolving with them requires new ap-
proaches that go beyond classic software engineering solu-
tions. We present a novel approach that automates the ac-
quisition of user-interaction requirements in an incremental
and reective way. Our solution builds upon inferring a set
of probabilistic Markov models of the users' navigational be-
haviors, dynamically extracted from the interaction history
given in the form of a log le. We annotate and analyze the
inferred models to verify quantitative properties by means
of probabilistic model checking. The paper investigates the
advantages of the approach referring to a Web application
currently in use.
Categories and Subject Descriptors
D.2.4 [ Software Engineering ]: Software/Program Veri-
cation| Model Checking ; H.3.4 [ Information Storage and
Retrieval ]: Systems and Software| User proles
General Terms
Design, Measurement, Verication
Keywords
Web Application, Log Analysis, User Proles, Markov Chains,
Probabilistic Model Checking
This research has been funded by the EU: Programme IDEAS-ERC,
Prj. 227977-SMScom and by a Marie Curie IEF within the 7thEuro-
pean Community Framework Programme: Prj. 302648-RunMore
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proÔ¨Åt or commercial advantage and that copies
bear this notice and the full citation on the Ô¨Årst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciÔ¨Åc
permission and/or a fee.
ICSE ‚Äô14, May 31 - June 7, 2014, Hyderabad, India
Copyright 14 ACM 978-1-4503-2756-5/14/05 ...$15.00.1. INTRODUCTION
A key distinguishing feature of user-intensive software,
and in particular Web applications, is the heavy dependence
on the interactions with many users, who approach the ap-
plications with dierent and evolving needs, attitudes, navi-
gation proles, preferences, and even idiosyncrasies1, which
generate dierent navigation proles. Knowing and predict-
ing the dierent user behaviors are crucial factors that may
directly aect the success of the application. Underestimat-
ing the importance of these factors may lead to technical as
well as non-technical failures that may involve substantial
economic losses. For example, an inadequate or distorted
knowledge of users' navigation preferences may lead to Web
applications characterized by an unsatisfactory user experi-
ence with consequent loss of customers and revenues.
Unfortunately the presence of a huge number of users with
dierent and evolving behaviors make it almost impossible
to accurately predict and model all of them, and to design
applications that can answer all possible needs. Moreover,
the population of users is seldom homogenous and, typically,
several classes of users with distinct user behaviors coexist
at the same time. In addition, no matter how well they are
initially captured, user behaviors change over time. This
leads to the need for learning and rening our understand-
ing of how users interact with the system and to the need for
speculating on the inferred knowledge to drive the progres-
sive system maintenance, adaptation, and customization.
The mainstream approach towards capturing the user be-
haviors consists of monitoring the usage of the system and
subsequently mining possible interaction patterns [35]. Some
existing solutions instrument Web pages to track users' nav-
igation actions { for instance Google Analytics [1] { while
others analyze log les as discussed by Facca and Lanzi [12].
Current solutions suer from several limitations. Some ap-
proaches lack generality, for example, they need to infer
users' navigational proles to support specic actions, such
as run-time link prediction [31] or data caching [39]. Being
tailored to specic tasks, these solutions provide little sup-
port from a general software engineering perspective. On
the other hand, the general frameworks simply return a set
of statistics or patterns that are useful to understand the
preferences of system's users but cannot be directly used to
1We collectively identify these factors under the term user
behavior , or simply behavior when the context is clear.Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proÔ¨Åt or commercial advantage and that copies bear this notice and the full citation
on the Ô¨Årst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speciÔ¨Åc permission and/or a
fee. Request permissions from Permissions@acm.org.
ICSE‚Äô14 , May 31 ‚Äì June 7, 2014, Hyderabad, India
Copyright 2014 ACM 978-1-4503-2756-5/14/05...$15.00
http://dx.doi.org/10.1145/2568225.2568234
277
evaluate software engineering decisions.
This paper describes a framework, called BEAR2, that
overcomes these limitations supporting software engineers
in maintaining and adapting existing user-intensive Web ap-
plications. The proposed approach focuses on REST3archi-
tectures and analyses the log le of the server in which the
application under analysis has been deployed. By analyz-
ing the log le, BEAR infers a set of Markov models that
cluster similar users in classes and capture their behaviors
probabilistically. Developers can decorate the inferred mod-
els with rewards that represent the causal connection among
user navigation actions and technical as well as non-technical
aspects of the application under analysis. An ad-hoc analy-
sis engine mines the models to gather valuable insights about
the users' behaviors and the relation among users and the
entities modeled by the rewards. The analysis engine relies
onProbabilistic model checking [4] to formally verify quan-
titative properties of the behavior of the users captured by
the models. For instance, the analysis engine may compute
the probability that a user, who enters the Web application
from a certain link, navigates across one or more specic
paths and reaches a given target page. The analyses sup-
ported by our approach may focus on the whole population
of users, on a specic class of users in isolation, or may com-
pare dierent classes.
BEAR can be viewed as an approach to facilitate and au-
tomate the acquisition of user-interaction requirements in an
incremental and reective way that supports many aspects
of the application maintenance and evolution. The proposed
solution brings some key advantages with respect to existing
competing alternatives. The inferred models, together with
rewards, represent a general abstraction that can be used to
analyze technical as well as non-technical and domain spe-
cic aspects of the application. Probabilistic model checking
and the incremental inference process allow the progressive
and automatic analysis of large and complex models as soon
as new data become available in the log le.
To the best of our knowledge, this paper contributes to
current research in requirements and design of user-intensive
software in three distinct ways:
1. It is the rst approach that investigates the use of proba-
bilistic model checking to formally verify quantitative prop-
erties of users' behaviors in Web applications.
2. It investigates a novel approach to capture user behaviors
with an inference algorithm and a model-based technique
specically conceived for REST architectures.
3. By introducing rewards, it provides a new way to reason
about the relationships between the interaction of users and
several aspects of the application under analysis.
The remainder of the paper is organized as follows. Sec-
tion 2 overviews the BEAR approach. Section 3 introduces
the running case study we use throughout the paper to ex-
emplify and validate the approach. Section 4 provides a de-
tailed description of the approach, while Section 5 illustrates
some relevant scenarios of our case study that exemplify the
potential applications of BEAR. Section 6 discusses perfor-
mance and scalability of the approach. Section 7 discusses
related work. Section 8 summarizes the main contributions
of the paper, and illustrates the ongoing research work.
2BEhavioral Analysis of REST applications.
3REpresentational State Transfer [13].
BEAR Inference 
Engine 
Engineer 
Analysis 
Results speciÔ¨Åes 
uses 
uses generates 
rewards uses 
generates 
Log File uses 
Propositions 
 User 
Classes 
Properties speciÔ¨Åes 
BEAR Analysis 
Engine Markov Models Figure 1: The BEAR approach
2. THE BEAR APPROACH
In this section we present the BEAR approach. Although
the approach is applicable to dierent kinds of systems and
logs, in this paper, we refer to user interactions with re-
mote services implemented according to a REST architec-
tural style [13, 37]. REST is an increasingly popular archi-
tectural style, in which requests and responses ow through
thestateless transfer of resources via URLs that uniquely
identify the state of the conversation between clients and
servers.
BEAR infers the user behaviors by deriving Markov mod-
els from log les, and quantitatively veries properties of the
interaction patterns by analyzing the inferred models using
probabilistic model checking. The verication provides the
core information to refactor or customize the application
according to emerging user behaviors. BEAR is articulated
in six main steps that we introduce below referring to the
diagram in Figure 1, and discuss in detail in Section 4:
1.Identifying the atomic propositions : The designers give
semantics to the URLs occurring in the log le by means of
a set of atomic propositions that denote the relevant user
actions. This is a necessary setup phase through which the
designer identies the actions relevant for the analysis of the
application. BEAR automatically clusters the entries of the
log that represent URLs into groups univocally identied by
sets of propositions.
2.Identifying user classes : The designers characterize the
population of users by identifying a set of relevant features
to cluster them into distinct classes. For instance, they may
use the feature user-agent to discriminate users depending
on the browser they rely on, or depending on the device they
use (mobile vs. desktop users).
3.Inferring the models : The BEAR inference engine anal-
yses the log le of the application and infers a set of discrete
time Markov chains (DTMCs) [4]. DTMCs are nite state
automata augmented with probabilities: each state is char-
acterized by a discrete probability distribution that regulates
the outgoing transitions. The inference engine generates an
independent DTMC for each user class.2784.Annotating the models with rewards : The designers may
provide information by annotating the states of the models
with numerical values that represent rewards [22]. Rewards
indicate the impact of the state on some metrics of interest.
The annotations are optional and refer to the set of atomic
propositions introduced in the second step.
5.Specifying the properties of the interaction patterns : The
designers formally specify the properties of interest for the
user-intensive system. The properties may predicate on the
probability that users may follow a certain navigational pat-
tern, or may predicate on the rewards.
6.Analyzing the models : The BEAR analysis engine quan-
titatively evaluates the formal properties against the Markov
models, and produces either numerical or boolean results,
depending on the nature of the properties. The obtained
results provide insights on the behaviors of the users and on
the impact of such behaviors on the rewards in the models.
These insights guide designers in refactoring or customizing
the application under analysis.
3. THE REAL-ESTATE EXAMPLE
We introduce the real-world REST application we use
throughout the paper to illustrate and validate the BEAR
approach. The running example has been provided by an IT
consultancy company and is currently in use. We chose it be-
cause it balances generality and simplicity: the application
is general enough to include the main aspects that character-
ize the requirements in the user-intensive REST domain, and
simple enough to illustrate the approach. During our study
we had full access to both the application and the log les,
and we conducted our experiments on the data collected
during the normal operation of the application. For con-
dentiality reasons we anonymize the application selected
as a case-study by using the pseudonym ndyourhouse.com .
Through the ndyourhouse.com website, users may either
browse the housing announcements divided by category, or
search for the available oers with a proprietary search en-
gine accessible from every page of the website. If users nd
an announcement of interest, they can contact a sales agent
for additional details and schedule a visit to the property of
interest. The website is composed of several pages and re-
sources identied by the URLs summarized in Table 1 (the
meaning of the third column will be discussed later and can
be ignored at this stage). The table contains samples of the
relevant URLs and provides a summary of the URLs through
a prex. It ignores URLs that are not relevant for the ex-
periments, for example the URL pointing to the robots.txt
le used by Web spiders. Some of the URLs are simple, for
example /home/ , while others are parametrized, for example
/anncs/sales/?page=<n> or have a parametric structure,
for example /anncs/sales/<id>/ .
Even for a relatively simple REST application like the one
illustrated in this section, information about the behavior of
the nal users is crucial for engineering an application that
meets users' dierent and evolving browsing requirements
and tries to increase the economic value of the service.
4. THE DETAILS OF THE APPROACH
BEAR is grounded on a simple basic assumption, dis-
cussed here before presenting the six steps of the approach in
details. The input to BEAR is a log le structured as a list ofrows that record the interactions between the users and the
Web server of the application under analysis. Each row rep-
resents a request of a Web resource issued by a client. Here-
after we use the terms rowandrequest interchangeably. We
assume that the rows contain the following common data:
the IP address of the user who issued the request to the
server, a timestamp that represents the time of the request,
the user-agent, and the requested URL. These assumptions
correspond to the information provided by the log les com-
pliant to the Common Log Format (CLF) adopted by many
popular Web servers, such as the Apache Web server4.
4.1 Identifying the Atomic Propositions
In the rst step of the approach BEAR associates seman-
tics to the rows of the log le by means of a set of atomic
propositions (AP) that indicate what can be assumed as
valid when a certain entry in the log le is found. For exam-
ple, the proposition homepage is associated to a row in the
log le to indicate that the request corresponding to that
row has led the application to the home page.
The atomic propositions to be associated to the log entries
are encoded as code fragments, called lters . The applica-
tion designers declare the lters as methods decorated with
the annotation @BearFilter . A lter may be parameter-
ized with a regular expressions, to restrict its application
to the URLs that match the expression. The BEAR engine
scans the log le, invokes the lters with matching parame-
ters on each row it analyses, and associates the propositions
returned by the lters to the log le entries.
The third column in Table 1 shows some atomic proposi-
tions associated to URLs of the ndyourhouse.com applica-
tion. Listing 1 shows some examples of lters. The rst lter
associates the proposition homepage to the homepage URL.
The second lter associates the proposition sales anncs to
the URLs that matches the regular expression in its an-
notation, and indicates that the URLs are related to sales
announcements. The third lter associates the propositions
login success and login failto URLs that correspond to lo-
gin attempts, depending on the HTTP status code of the
request. The fourth lter associates the propositions con-
trolpanel to the pages of the control panel. The BEAR in-
ference engine associates propositions only to relevant URLs,
which match some regular expressions in the lters. Rows
that correspond to URLs associated to secondary resources
that are not crucial to capturing relevant aspects of the users
behavior are not labeled with any proposition. For exam-
ple, usually lters do not match URLs that represent CSS or
Javascript resources that are not relevant to user navigation
actions, since they are automatically requested by browsers,
and thus are not labeled with propositions. Filters may
generate propositions dynamically according to information
available in the URL or the application database. For exam-
ple, a lter may use the announcement ID extracted from
the URL to retrieve the location of the sales announcement
from the application database, and may use the location to
build a proposition dynamically.
Filters, propositions and regular expressions are exible
tools that application designers use to characterize the rows
in the log le, exploiting both application and domain spe-
cic knowledge.
4http://httpd.apache.org/docs/2.4/logs.html279Table 1: The relevant URLs of ndyourhouse.com
URL Description Atomic Propositions
/home/ Homepage of ndyourhouse.com homepage
/anncs/sales/ The rst page that shows the sales announcements. sales page,page 1
/anncs/sales/?page=<n> The nthpage that shows sales announcements, <n>is an integer index. sales page,page n
/anncs/sales/<id>/ Detailed view of the sales announcement identied by the string <id>. sales anncs
/anncs/renting/ The rst page that shows the renting announcements. renting page 1
/anncs/renting/?page=<n> The nthpage that shows renting announcements, <n>is an integer index. renting page,page n
/anncs/renting/<id>/ Detailed view of the renting announcement identied by the string <id>. renting anncs
/search/ Page containing the results of a search submitted through the search engine. search
/admin/.../ Website's control panel that allows to publish, edit or delete announcements. control panel
/admin/login/ Login page that allows to access the control panel. login success orlogin fail
/contacts/ URL with the form to contact a sales agent. contacts
/contacts/submit/ URL that indicates that the form used to contact the agency has been submitted. contacts requested
/contacts/tou/ Page that describes the website terms of use. tou
/media/.../ URLs with this prex refer to images, CSS and Javascript resources. -
@BearFilter ( regex="^/home/$ ")
public static Proposition void f i l t e r R e n t i n g ( LogLine l i n e ) f
return new Proposition ( "homepage " ) ;
g
@BearFilter ( regex="^/ anncs / s a l e s /( nw+)/$ ")
public static Proposition void f i l t e r S a l e s ( LogLine l i n e ) f
return new Proposition ( " s a l e s a n n c s " ) ;
g
@BearFilter ( regex="^/admin/ l o g i n /$ ")
public static Proposition void f i l t e r L o g i n ( LogLine l i n e ) f
i f( logLine . getHTTPStatusCode == "302 ")
return new Proposition ( " l o g i n s u c c e s s " ) ;
else
return new Proposition ( " l o g i n f a i l " ) ;
g
@BearFilter ( regex="^/admin/ e d i t /$ ")
public static Proposition void filterAdmin ( LogLine l i n e ) f
return new Proposition ( " c o n t r o l p a n e l " ) ;
g
Listing 1: Some examples of lters
4.2 Identifying the User Classes
In this step of the approach, the designer may dene a set
of user classes relevant for the application under analysis. A
string in the format (name=\value") denes each user class.
The BEAR inference engine uses code fragments called clas-
siers decorated with the annotation @BearClassier to
specify classes of users. The BEAR engine scans the log le,
invokes the classiers on each row, and associates the user
classes returned by the classiers to the log le entries. By
default, BEAR comes with two classiers that extract the
user-agent and the user's location obtained geolocating the
IP address. For instance a row may by associated with the
following user classes:
f(userAgent = \Mozilla= 5:0:::");(location = \Boston ")g
Classiers represent a exible and extensible tool to map
rows in the log into classes. Notice that, as shown in the
example above, each user may belong to multiple classes.
By adding classiers the designers can classify the users
into application or domain specic classes exploiting addi-
tional information that may be stored in customized log les.
For example designers may classify users predicating on their
operating system, the HTTP referrer, the user's time zone,
etc. For the sake of readability in this paper we refer only
to the default classiers, even if all the concepts and exam-
ples we discuss here apply seamlessly to more complex and
application specic classiers.4.3 Inferring the Model
Given the set of atomic propositions APand the user
classes dened through lters and classiers, respectively,
the BEAR inference engine infers a set of discrete time
Markov chains (DTMCs) [4] that represent the users' be-
haviors. The inference process works sequentially and incre-
mentally on the log le as a data stream. Once a log entry is
processed, it may be discarded. Thus the process works ef-
ciently both on-line and o-line, and works both for legacy
applications for which log data have been collected and for
newly deployed applications.
The inference engine generates an independent DTMC for
each user class dened by the classiers. For the sake of
readability we rst dene the inference of a single generic
DTMC model, and then extend the inference algorithm to
multiple DTMCs later in this section. In this paper we de-
rive DTMCs augmented with rewards [3], dened as follows.
A DTMC is a tuple hS;P;L;iwhere:
S: is a non empty nite set of states, and s02Sis the
initial state;
P:SS! [0;1] is a stochastic matrix that represents the
probabilistic edges that connect the states in S. An
elementP(si; sj) represents the probability that the
next state will be sjgiven that the current state is si;
L:S ! 2APis a labeling function that associates each
state with a set of atomic propositions AP.
:S!R0is a state reward function that assigns a
non-negative number to each state. Rewards are non-
negative values that quantify the benet (or disadvan-
tage) of being in a specic state.
As for the DTMCs we infer, the set APconsists of the propo-
sitions identied by lters, as described in Section 4.1. In
addition, no two dierent states can be associated with the
same set of atomic propositions; i.e., the set of atomic propo-
sitions associated with a state univocally identify it. The in-
ference process starts from an initial DTMC, characterized
by:
A set of statesSthat contains the initial state s0and
a sink state ethat models users who leave the system;
A stochastic matrix Pwith only one non-zero element:
P(e;e) = 1;
A labeling function Ldened as follows:280IP TIMESTAMP URL
1 . 1 . 1 . 1 [20/ Dec / 2 0 1 1 : 1 5 : 3 5 : 0 2 ]  /home/
2 . 2 . 2 . 2 [20/ Dec / 2 0 1 1 : 1 5 : 3 5 : 0 7 ]  /admin/ l o g i n /
1 . 1 . 1 . 1 [20/ Dec / 2 0 1 1 : 1 5 : 3 5 : 1 2 ]  / anncs / s a l e s /1756/
2 . 2 . 2 . 2 [20/ Dec / 2 0 1 1 : 1 5 : 3 5 : 1 9 ]  /admin/ e d i t /
Listing 2: Log le excerpt (the listing does not re-
port the user-agents)
L(s) =8
><
>:fstartgifs=s0
fendgifs=e
;otherwise
A reward function that assigns 0 to all the states
inS. Non-null rewards can be added as discussed in
Section 4.4.
This initial DTMC is shown in Figure 2(a). The inference
engine builds the DTMC incrementally by processing the
rows of the log le and inferring transitions between states
inS. The engine processes each row in four steps:
1.Extracting the destination state: The inference engine
examines the propositions associated by the lters with the
current row r. The engine ignores the rows associated with
an empty set of propositions, since they represent transitions
irrelevant in our context. The engine associates rwith a
destination state d2S such thatL(d) =l, wherelis the
set of propositions associated with r. Ifddoes not belong
toSyet, the inference engine adds the new state dtoSand
updates the labeling function Laccordingly.
2.Extracting the user identier: The engine assumes that
distinct IP addresses correspond to dierent users, thus as-
signs a unique identier to each new IP address extracted
from the rows in the log le. In normal operational condi-
tions, IP addresses may not be uniquely associated to users,
and it may happen that dierent users that browse the sys-
tem in dierent occasions may have the same IP address.
To cope with this scenario, the inference engine refers to the
timestamps of the transitions, and assumes that requests is-
sued from the same IP address with signicantly dierent
timestamps are issued by dierent users (i.e., they are given
distinct user identiers). The minimum temporal distance
between timestamps to consider two requests with the same
IP address as issued by distinct users is a parameter, called
userwindow .
3.Extracting the source state: If the previous step assigned
a new user identier to r, the engine assumes that ris the
rst interaction of the user with the system, and associates
the initial state s0as the source state. If the interaction
rcomes from an existing user (a known IP address), the
inference engine retrieves the destination state of the most
recently processed row characterized by the same IP address,
and assigns such state as the source state of r.
4.Computing the probabilities: The engine uses the transi-
tions extracted from the log le to update two sets of coun-
ters that are initially set to zero: a set of counters ci;jfor
each pair of states ( si; sj)2SS , and a set of counters tifor
each statesi2S. The engine increments both the counter
ci;jfor each transition from state sitosjand the counter ti
for each transition whose source state is si, independent of
its destination state. The counter tirepresents the numberof times the users exited state si, while counter ci;jrepre-
sents the number of times the users moved from state sito
statesj. The inference engine updates the counters for each
row in the log le that corresponds to a transition in the
model, and uses these counters to compute the ( i;j) entry
of the stochastic matrix Pthat represents the probability of
traversing the edge from state sito statesj, by computing
the following frequency:
P(si; sj) =ci;j
ti
for all pairs of states siandsj. The probability P(si; sj) is
computed as the ratio between the number of traversals of
the transitions from state sitosjand the total number of
traversals of the transitions exiting state si, and corresponds
to the maximum likelihood estimator forP(si; sj) [11]. The
probabilities can be recomputed incrementally after adding
any number of transitions or states to the DTMC.
Figure 2 illustrates the inference process described so far
referring to the log le in Listing 2 and the lters shown
in Listing 1. The log entries in Listing 2 have been ex-
cerpted from the interactions of two users with the nd-
yourhouse.com application. The BEAR engine starts from
the initial DTMC shown in Figure 2(a), and proceeds in-
crementally through the log le. BEAR associates the rst
row with the proposition homepage . SinceSdoes not con-
tains any state ssuch thatL(s) =fhomepageg, the engine
adds a new state s1toS, and extends the labeling function
withL(s1) =fhomepageg. Being this the rst transition in
the log, the IP address has not been already encountered,
thus the engine considers state s0as the source state for
the inferred transition ( hs0;s1i). The engine increments the
counterst0andc0;1, and consequently sets P(s0; s1) to 1.
Figure 2(b) shows the resulting DTMC.
The second row corresponds to a successful login and
BEAR associates it with the set of propositions flogin success ,
control panelg. The engine associates this row with the
new destination state s2and updates the labeling function:
L(s2)=flogin success ,control panelg. The IP address is new,
and thus the engine creates a new user identier and asso-
ciates the transition with the initial state s0yielding to a
transitionhs0;s2i. The engine increments the counters re-
lated to the new transition: t0= 2 andc0;2= 1, and sets
P(s0; s2) andP(s0; s1) to 0:5. Figure 2(c) shows the result-
ing DTMC.
The engine processes the third and fourth row in a similar
way. It associates the third row with a new destination state
s3such thatL(s3)=fsales anncsg. Since the row is associ-
ated with the same IP address of the rst one, the source
state corresponds to the destination state of the last transi-
tion generated by this user ( s1) and results in the transition
hs1;s3i. The engine increments the counters t1andc1;3, and
consequently sets P(s1; s3) = 1.
When processing the fourth row, BEAR generates a tran-
sition from s2(the destination state of the second transi-
tion that corresponds to the same IP address) to s4with
L(s4) =fcontrol panelg. The engine increments the coun-
terst2andc2;4, and setsP(s2;s4) = 1. Figure 2(d) shows
the resulting DTMC. When the userwindow timeout for a
certain IP address expires, the engine assumes that the user
associated with that address left the system. As a conse-
quence, a later request from the same address is considered
as issued by a new user and the source state is s0. As soon as2811
e
end starts0(a)
1
e
end starts0s1homepage 
1 (b)
0.5 1e
end start s0s1homepage 
0.5 
s2
login_success 
control_panel (c)
1
0.5 e
end start s0s1homepage 
0.5 
s2
login_success 
control_panel s3sales_anncs 
1
1s4
control_panel (d)
0.33 1
0.33 e
end start s0s1homepage 
0.33 
s2
login_success 
control_panel s3sales_anncs 
1
1s4
control_panel 1
1 (e)
Figure 2: DTMC inference process
0.12 
0.63 s1
s6
sales_anncs s3
sales_page 
page_1 s2renting_page 
page_1 
s4renting_anncs 
s7
tou homepage 0.05 
0.15 0.27 0.63 
0.01 0.07 
0.23 
0.12 0.12 
0.05 
0.11 0.39 0.02 
0.3 0.02 0.007 
0.002 0.03 s50.008 
0.67 0.33 0.02 
0.02 starts00.01 
0.02 0.04 
0.13 0.03 0.39 
0.36 0.03 
0.01 0.19 0.06 
contacts 
Figure 3: Partial DTMC of ndyourhouse.com
a the timeout expires, the engine generates a new transition
that leads from the destination state of the last transition
issued by the expired IP address towards the nal state e.
If for example we assume that the userwindow timeout ex-
pires for both the considered users, the engine generates the
transitionshs4;eiandhs3;ei, and updates the correspond-
ing counters. A new interaction occurring after the timeout
that leads to state s3and involves the IP address of one
of previous requests generates a new user and a transition
hs0; s3i. Figure 2(e) shows the resulting DTMC.
By processing the log le of the ndyourhouse.com appli-
cation with a set of lters that match the propositions shown
in the third column of Table 1, the BEAR engine produces
a DTMC with 23 states and 214 transitions. Figure 3 shows
a subset of the generated model. Being the model a subset
of the complete DTMC, the probability of the transitions
exiting a state may not sum to one.
The obtained DTMC captures the behaviors of all users
regardless of the class they belong to. In general, the BEAR
inference engine infers a DTMC for each class of users. For
instance, if a log le contains entries characterized by ve
dierent user-agents and ten dierent locations, the infer-
ence engine infers fteen DTMCs, one for each class of user-
agents and one for each class of user locations. Each log
entry processed by the engine contributes uniquely to the
inference of the DTMCs associated to its user classes. For
instance a row associated with the user classes exemplied
in Section 4.2 contributes to infer a DTMC associated to
the class: ( userAgent = \Mozilla= 5:0:::") and a DTMCassociated to the class: ( location = \Boston "). In this set-
ting, each inferred DTMC captures the behavior of a specic
class of users. The classiers identied by the application
designer directly aect the number of inferred DTMCs and
their level of abstraction.
4.4 Annotating the Models
Rewards are non-negative values the designer can asso-
ciate with propositions to model benets or losses. The
BEAR engine uses the rewards associated with propositions
in the setup phase to automatically annotate the states of
all inferred DTMCs.
In the absence of rewards, BEAR returns a set of mod-
els with default value zero for the reward function of the
states. In the presence of rewards, the BEAR inference en-
gine computes the reward of the states as the sum of the
rewards of the propositions associated with the states. Let
us consider the DTMC in Figure 3, and let us assume that
the designer assigns rewards 2 and 3 to the propositions
salespage andpage 1, respectively, then the BEAR engine
assigns 5 to (s3).
Rewards are a general purpose and abstract tool to aug-
ment the inferred DTMCs with domain specic metrics of
interests. Designers can use rewards to capture both tech-
nical and non-technical metrics of interest. They assign re-
wards to propositions in the setup phase, and do not need to
know the structure and the number of the inferred models,
which are inferred only later while monitoring the applica-
tion behavior.
Here we illustrate the use of rewards with an example
related to a refactoring of the ndyourhouse.com applica-
tion, which aims at increasing the number of announce-
ments displayed to the customers, and thus gives special
recognition to the states that display a large number of an-
nouncements. This can be easily achieved by annotating the
propositions with rewards that depend on the number of an-
nouncements they are related with. They associate reward
6 with the proposition homepage , because homepage dis-
plays six announcements. Likewise, they associate reward
9 with both the propositions page 1and page n, because
they contain nine announcements, and so on. The BEAR
engine computes the reward functions of the states of the
inferred DTMCs accordingly. For instance in the DTMC in
Figure 3,(s1) = 6 since s1is associated only to proposition
homepage ,(s2) = 9 and(s3) = 9.
4.5 Specifying the Properties
In this step, the application designers dene the properties
of interest using Probabilistic Computation Tree Logic (PCTL)
augmented with rewards [17, 22]. PCTL with rewards is282a probabilistic branching-time temporal logic based on the
classic CTL logic [4] that predicates on a state of a Markov
process. Hereafter we provide a formal denition of this logic
through the following recursive grammatical rules:
::=truejaj^j:jP. /p( )jR. /r()
 ::=XjUt
 ::=I=kjCkj3
wherep2[0;1],./2f<;;>;g,t2N[f1g ,r2R0,
andk2Z0, andarepresents an atomic proposition. The
operatorR. /r() supports the specication of properties
that predicate over rewards. Let us rst discuss the seman-
tics of basic PCTL ignoring the reward operator. Formulae
originated by the axiom are called state formulae ; those
originated by  are instead called path formulae . The se-
mantics of a state formula is dened as follows:
sj=true
sj=a ia2L(s)
sj=: is2
sj=1^2isj=1and sj=2
sj=P. /p( ) i Pr(j= j[0] =s)./p
where Pr(j= j[0] =s) is the probability that a path
originating in ssatises . A pathoriginating in ssatises
a path formula  according to the following rules:
j=X i[1]j=
j=1Ut2i90jt([j]j=2^
(80k<j [k]j=1))
Let us now discuss intuitively the reward operator R. /r():
R . /r(I=k) is true in state sif the expected state reward
to be gained in the state entered at step kalong the
paths originating in smeets the bound ./r.
R . /r(Ck) is true in state sif, froms, the expected
reward cumulated afterksteps meets the bound ./r.
R . /r(3) is true in state sif, froms, the expected re-
ward cumulated before reaching a state where holds
meets the bound ./r.
The BEAR analysis engine processes PCTL properties rely-
ing on the PRISM [23] probabilistic model checker5. More
precisely, a BEAR property is composed of a preamble in
curly brackets and a PCTL expression. The preamble de-
nes the scope of the property, while the PCTL expression
species the formula to be veried with the model checker.
The scope of the property is expressed as a string in the
fromname = \regex " that identies the user classes the
PCTL formula refers to. More precisely, the rst part of the
scope (i.e.,name ) is the name of a user class as specied with
the classiers, for instance, a userAgent , while the second
part (i.e.,regex ) corresponds to a regular expression. The
property verication is restricted to the users that belong to
the specied class, and that matches the regular expression
in the scope of the property.
As an example, the designers of ndyourhouse.com who
are interested in the probability that users with a certain
browser, for instance Mozilla, will eventually contact a sales
5We present the PTCL properties in the PRISM syntax.agent can use the following property:
fuserAgent = \(:)Mozilla (:)"gP=?[Fcontactrequested ]
(1)
As another example, the application owners who are inter-
ested in the number of announcements displayed to mobile
users, for example, Android or iOS users, through all the
states up to the nal state can use the following property:
fuserAgent = \(:)(AndroidjiOS)(:)"gR=?[Fend ](2)
that refers to the reward function that associates the number
of displayed announcements to each state as exemplied in
the previous paragraph. The PCTL properties depend only
on the propositions, and designers can specify them without
knowing the structure of the inferred models in terms of
states or transitions. Designers can also express properties in
structured english that can be automatically translated into
PCTL properties [16]. Indeed, designers can be completely
agnostic of the complexity of the formal tools such as the
probabilistic model checking engine and PCTL used by the
BEAR analysis engine.
4.6 Analyzing the Models
The process is completed with the analysis of the proper-
ties specied by the application designer. The BEAR engine
exploits the scope in the preamble of the properties to iden-
tify the set of relevant DTMCs among the inferred models.
As discussed in Section 4.3, the BEAR inference engine gen-
erates a distinct DTMC for each user class.
For each property to be processed, the analysis engine se-
lects the DTMCs associated with the user classes that match
the regular expression indicated in its scope. For example
when analyzing property (1), the analysis engine selects the
DTMCs associated to the Mozilla user-agents. The BEAR
analysis engine selects one or more DTMCs depending on
the user agents contained in the log entries processed dur-
ing the inference process. For instance, it may select the
DTMC associated to the user-agents of Mozilla 5.0 as well
as the DTMCs associated to user-agents of dierent versions
of Mozilla since they all match the regular expression in the
property's scope.
If the scope of the property selects multiple DTMCs, the
BEAR analysis engine merges them and synthesizes a single
DTMC as described hereafter. Let fD1;D2;:::;Dngbe the
set of DTMCs selected according to the scope of a prop-
ertyp, whereDk=hSk;Pk;Lk;ki, for each 1kn,
and letukbe the user class associated to Dk, for each
1kn, the merging procedure produces a new DTMC
T=hST;PT;LT;Tiwhere:
The set of states is the union of the sets of states of
the input DTMCs:
ST=[
1knSk
States of dierent input DTMCs may correspond to
the same state of the merged DTMC since, by con-
struction, two states are equal if and only if they are
characterized by the same propositions, see Section 4.3.283The transition probabilities are computed according to
the law of total probability [11]:
PT(si; sj) =X
1knPk(si; sj)Pi(uk)
wherePi(uk) corresponds to the probability for a user
(associated to the selected user classes) that exited
statesito belong to the specic user class uk.
The labeling function is computed as:
LT(s) =Lk(s) ifs2Sk
The readers should notice that if a state belongs to
more than one DTMC, the labeling function of all the
DTMCs returns the same value by construction.
The reward function is computed as:
T(s) =k(s) ifs2Sk
The readers should notice that if a state belongs to
more than one DTMC, the reward function of all the
DTMCs returns the same value by construction. In ad-
dition the distribution of rewards is independent from
the merging process since, as explained in Section 4.4,
they are dened for propositions.
The synthesized DTMC captures the behaviors of several
user classes. For example, the DTMC synthesized for prop-
erty (1) captures the behavior of all Mozilla users, inde-
pendently of the specic version they use, while each orig-
inal DTMC captures the behavior of users with a specic
version of the Mozilla browser. Similarly, the DTMC syn-
thesized for property (2) captures the probabilistic behav-
ior of all mobile users independently of the specic type of
device. The BEAR analysis engine evaluates the property
for the synthesized DTMC using a model checker, PRISM
in our experiments [23]. The approach does not depend
on the model checker and works with other engines, such
as MRMC [19], as well as with ad-hoc mechanisms for e-
cient runtime analyses, like the approach described by Filieri
et al. [14, 15]. The BEAR analysis engine processed prop-
erty (1) resulting in a probability of 0 :006 that Mozilla users
will contact a sale agent, and the property (2) resulting in
32:36 announcements displayed on average by mobile users.
5. BEAR MODELS AT WORK
BEAR captures information about user interactions with
Web applications. In this section we illustrate how this infor-
mation can be used to enrich or modify the initial require-
ments and to keep them current as user behaviors evolve
over time. BEAR can thus be used to support the evolu-
tion of Web applications. Indeed, many Web applications
are initially designed with little or no knowledge of how its
nal users will behave. Resorting to the experience col-
lected for similar applications, if any, only provides rough
data. In addition, user behaviors may change with their fa-
miliarity with the application and for many other reasons.
The analysis of the DTMCs by means of probabilistic model
checking produces information about navigation anomalies,
emerging behaviours and new attitudes of users that wellcomplements the initial incomplete requirements. We show
how this can be done by illustrating the use of BEAR in dif-
ferent maintenance actions for the ndyourhouse.com case
study introduced in Section 3. The results discussed here re-
fer to a log le composed of 400.000 entries that correspond
to ten months of users' interactions. We ran the BEAR in-
ference engine on the log le relying on the lters listed in
the third column of Table 1, and we obtained 571 DTMCs.
We rst illustrate the use of BEAR for detecting navigation
anomalies and then for inferring emerging behaviours and
new attitudes of users.
5.1 Detecting Navigational Anomalies
A navigational anomaly is a dierence between the actual
and the expected user navigation actions. The expected
navigation is what has been implemented in the application
and is represented by the application's site map . The actual
navigation is instead represented by a path on the DTMCs
inferred by BEAR, which corresponds to actual navigations
performed by users in reality. Navigational anomalies can
be detected by comparing the DTMCs with the site map.
By detecting navigation anomalies, we can nd suggestions
to improve the application. We can, for example, identify
frequent users' workarounds that may witness the lack of
some navigational features. As an example of navigational
anomaly detection, let us compare the model produced by
BEAR with the ndyourhouse.com site map, by running the
BEAR analysis engine with queries of this kind6:
fgP=?[(Xsi)]fsjg
for every state si,sjin the model. This query species the
request for the probability of a user to move from state sj
to statesi. Because of the empty scope in the properties
the analysis engine selected all the 571 inferred DTMCs and
synthesized a unique model that represents the behaviour
of the whole population of users. The synthesized model is
composed of 23 states and 214 transitions and is partially
reported in Figure 3. The BEAR analysis engine identied
several navigational anomalies, despite the fact that the ap-
plication has been in use for more than two years and has
been carefully maintained and corrected. Here we illustrate
one of them that corresponds to the two transitions exiting
states7of Figure 3 that occur with non-negligible proba-
bility in the DTMC, but do not correspond to transitions
in the application site map. Thus, they represent frequent
actions performed by users, which do not correspond to nav-
igation features of the application. State s7corresponds to
theterms of use page (proposition tou) that can be reached
only from the contacts page (state s5). The terms of use
page does not oer a way to go back to the contacts page.
The anomalous transitions that exit state s7correspond to
the users trying to go back to the contacts page (state s5),
likely using the back button of the browser, and ending up
in one of the states proceedings s5(statess4ands6) be-
cause of the AJAX implementation of the application. The
readers should notice that there is no transition from s7to
s1that also precedes s5. This indicates that the log le
does not record any attempt of users to go back from s7to
s5having reached s5directly from s1. This is because s1
correspond to the home page and no users felt the need to
check the terms of use (s7) before consulting renting or sales
6PRISM not only can evauate the truth or falsity of a prop-
erty, but can also compute probability values.284announcements (states s4ands6). This anomaly has been
corrected by adding a back button to the terms of use page.
Although navigational anomaly detection is presently done
by manually inspecting and comparing the analysis results
with a site map, the comparison can be easily automated by
using available XML sitemap descriptions.
5.2 Inferring Behaviours and Attitudes
BEAR can analyze properties that correspond both to
emerging behaviors and behaviors that may derive from new
requirements. Application designers can use properties to
describe the expected behaviors of either all or specic classes
of users, as well as the impact of changes in the navigation
attitude of users. Here we present the results of the analysis
of the ndyourhouse.com log le for properties that repre-
sent the dierent type of analyses. The ndyourhouse.com
owners were interested in improving the access to the appli-
cation in terms of renting versus buying inquires. To do so,
one needed to understand what is the probability of a user
to browse sales and not renting announcements and, vice
versa, the probability of users to browse renting announce-
ments only. The former can be found with the query:
fgP=?[(Fsalesanncs ) & (!(Frentinganncs ))]
The BEAR analysis engine indicated that 48% of users are
interested in sales announcements only, and 20% users are
interested in renting announcements only. The remaining
percentage of users look both for sales and renting announce-
ments. With these data, the designers decided to change
the homepage that initially displayed a random set of an-
nouncements, and now displays renting and sales announce-
ment proportionally to the measured users' behaviors. The
ndyourhouse.com owners were further interested in possi-
ble dierences between mobile versus desktop users. They
simply rened the above query by adding an ad-hoc scope
to restrict the analysis to mobile users (see property (2)
at page 7). The analysis engine selected 32 DTMCs out
of the initial set of inferred DTMCs and synthesized a sin-
gle model that captures the behavior of mobile users only,
which indicated that 35% of the mobile users are interested
in sales announcements only. With this additional informa-
tion, the developers decided to properly customize the mo-
bile home page. As a nal example, the ndyourhouse.com
owners were planning a marketing campaign targeting desk-
top users with a banner published on social networks adver-
tising the website and linked to the sales anncs page. In
this scenario it is crucial to accurately predict the number
of database queries generated by the marketing campaign
to suitably adapt the website infrastructure to prevent a
potential denial of service. Similarly to the example illus-
trated in Section 4.4, developers associated the rewards that
represent the number of data base queries needed to display
each page of the application to the atomic propositions de-
ned by lters to weight each state of the inferred DTMCs
with a reward indicating the impact of that state in terms
of queries to the database, and formulated the query:
f(?!(:)(AndroidjiOS))(:)gR=?[Fendfsalesanncsg]
The query, whose scope excludes Android and iOS users
to just focus on desktop users, asks for the resource usage
(R=?) after hitting the sales anncs page. Since the resource
usage is measured in terms of database queries, the analy-
sis of the property gives the estimate of database queries ofdesktop users after accessing that page. The analysis engine
selected 539 models (out of the initial 571 inferred models)
that capture the behaviors of desktop user and synthesized
an appropriate unique model against which to verify the
property. The product between the obtained result and the
expected click-through rate per hour (estimated from previ-
ous marketing campaigns or with ad-hoc techniques, like the
one dened by Richardson et al. [29]) represents a reasonable
prediction of the database queries generated by the market-
ing campaign. Similarly, we augmented the model with re-
wards that indicate the average amount of storage consumed
by each page request to predict the storage requirements im-
plied by the additional trac of the marketing campaign. In
general, rewards can model every resource that directly de-
pends on users' actions and can be represented numerically
to support capacity planning and forecasting analyses.
6. PERFORMANCE AND SCALABILITY
We evaluated the performance and scalability of both the
BEAR inference and analysis engine on a 2 GHz Intel Core
i7, 8GB RAM with Java ‚Ñ¢1.7.0. Each experiment has been
repeated one hundred of times collecting the average result
and the standard deviation.
The execution time of the BEAR inference engine depends
on two factors: (1) the number of states in the inferred model
and (2) the length of the log le. We generated synthetic log
les composed of 10 ;000 lines generated ad-hoc to produce
DTMCs with an increasing number of states. Figure 4(a)
shows that the BEAR inference engine can produce a DTMC
of 150 states analyzing a log les of 10 ;000 lines in few sec-
onds. Figure 4(b) illustrates instead the execution time of
processing log les of increasing length. The gure reports
the execution time for log les that produce a DTMC of 50
states. BEAR processes a log le of 100 ;000 lines in around
60 seconds. inference engine.
The execution time of the BEAR analysis engine depends
on two factors: (1) the number of synthesised models and
(2) the number of states of synthesised models. In the ex-
periments discussed below we shows the execution of the
synthesis algorithm. We refer to [18] for the performance of
evaluating PCTL properties on DTMCs. We randomly gen-
erated fully connected DTMCs composed by 50 states and
we measured the execution time needed to synthesise a sin-
gle model selecting an increasing number of input DTMCs.
Figure 5(a) shows that the BEAR analysis engine can syn-
thesise a DTMC from 1 ;000 input DTMCs in few seconds.
Figure 5(b) illustrates instead the execution time of syn-
thesising models with an increasing number of states. The
gure reports the execution time to synthesise 500 input
DTMCs. Also in this case BEAR shows a reasonable exe-
cution time. The gure shows that models composed of 120
states are synthesised in around 25 seconds. It is important
to notice that synthesised modes are not necessarily gener-
ated from scratch each time because of an internal caching
mechanism. The log le of the case study is composed of
over 400,000 entries, and we conducted all the experiments
within few minutes, thus conrming the scalability of the ap-
proach. Our implementation has been released as an open
source artifact7that includes an anonymized excerpt of the
ndyourhouse.com log le.
7http://giordano.webfactional.com/?page_id=22285!"""#!$""#%"""#%$""#&"""#&$""#$"""#$$""#'"""#'$""#
("# !"# %"# &"# $"# '"# )"# *"# +"# (""# (("# (!"# (%"# (&"# ($"#
Number of states Execution Time (ms) (a) Increasing number of states.
!"#!!!!"$!!!!"%!!!!"&!!!!"'!!!!"(!!!!")!!!!"
#!!!!" $!!!!" %!!!!" &!!!!" '!!!!" (!!!!" )!!!!" *!!!!" +!! !!" #!!!!!"
Number of log lines Execution Time (ms) (b) Log of increasing length.
Figure 4: Inference engine performance evaluation
!"#!!"$!!!"$#!!"%!!!"%#!!"&!!!"&#!!"'!!!"
$!!" %!!" &!!" '!!" #!!" (!!" )!!" *!!" +!!" $!!!"
Number of DTMCs Execution Time (ms) 
(a) Increasing number of DTMCs.
!"#!!!"$!!!!"$#!!!"%!!!!"%#!!!"&!!!!"
&!" '!" #!" (!" )!" *!" +!" $!!" $$!" $%!"
Number of states Execution Time (ms) (b) Increasing number of states.
Figure 5: Analysis engine performance evaluation
7. RELATED WORK
Many existing works address the problem of model infer-
ence from execution traces tackling a wide range of research
problems that range from mining API patterns [2], infer-
ring specications [25, 10, 30], inferring behavioral models
for web-services and software processes [5, 9], testing Web
applications [28, 34], and detecting faults [27, 26]. Worth
to mention is also Perracotta [38], an approach to mine and
visualize temporal properties of event traces used to study
program evolution and the work by Krka et al. [21] that
consists of an approach to mine invariants and improve pre-
cision of inferred models. Finally, it important to mention
the work by Beschastnikh et al. [7, 33] that illustrates Syn-
optic, a tool that helps developers by inferring from log les
a concise and accurate system model focusing on generat-
ing invariant-constrained models. Beschastnikh et al. also
discuss in [6] an approach to specify inference algorithms
declaratively. Complementary to these approaches there is
the work by Tonella et al. [36] that discusses how to nd the
optimal approximation in the inference process. All these so-
lutions represent fundamental tools for developing complex
and dependable software systems even if, dierently from
BEAR, they do not focus on user behaviours that actually
represent a crucial factor in applications domains such as
user-intensive Web applications.
The problem of capturing and analysing the user behav-
iours in Web applications through the analysis of log les has
been address by many approaches as reported in the survey
of Facca et al. [12]. Many of these approaches focus on min-
ing specic information to perform peculiar tasks. For ex-
ample, Liu and V. Ke selj combine the analysis of Web server
logs with the contents of the requested Web pages to predict
users' future requests [24]. They capture the content of Web
pages by extracting character N-grams that are combinedwith the data extracted from the log les. Schechter et al.
in [32] use instead a tree-based data structure to represent
the collection of paths inferred from the log le to predict the
next page access. Similarly, Sarukkai [31] relies on Markov
chains for link prediction and path analysis. Alternatively,
Yang et al. [39] focus on predicting accesses for ecient data
caching. These approaches, even if extremely helpful for the
specic tasks they have been conceived for, lack of general
applicability. Dierently, the BEAR approach produces a
navigational model that can be used to verify a plethora
of dierent properties that span from simple navigational
probabilities that may be used for link prediction to more
complex properties that may be used for capacity planning
analysis, as exemplied in the paper. Indeed, the exibil-
ity of the proposed approach relies on the expressiveness of
the PCTL formal logic that allows engineers to encode the
properties to be veried including domain specic aspects
of the application. Finally, it is worth to mention the work
by Komuravelli et al [20] that illustrates the inference of a
probabilistic system given nite set of positive and negative
samples and the work by Chierichetti et al. [8] that discusses
the approximation of Web users with Markov models.
8. CONCLUSIONS AND FUTURE WORK
We presented a novel inference mechanism conceived ad-
hoc for probabilistic model checking to elicit requirements
about emerging users' behaviours. The approach extracts
DTMCs that represent the users' behaviours from appli-
cation logs, and analyses them by means of probabilistic
model checking to identify navigation anomalies and emerg-
ing users' behaviours. We are extending the approach with
probabilistic timed automata [4] to capture other user be-
haviors.2869. REFERENCES
[1] Google Analytics.
http://www.google.com/intl/en/analytics/.
[2] M. Acharya, T. Xie, J. Pei, and J. Xu. Mining api
patterns as partial orders from source code: from
usage scenarios to specications. In ESEC/FSE , 2007.
[3] S. Andova, H. Hermanns, and J. Katoen. Discrete-time
rewards model-checked. Formal Modeling and Analysis
of Timed Systems , pages 88|104, 2004.
[4] C. Baier and J.-P. Katoen. Principles of Model
Checking . The MIT Press, 2008.
[5] A. Bertolino, P. Inverardi, P. Pelliccione, and
M. Tivoli. Automatic synthesis of behavior protocols
for composable web-services. In ESEC/FSE , 2009.
[6] I. Beschastnikh, Y. Brun, J. Abrahamson, M. D.
Ernst, and A. Krishnamurthy. Unifying fsm-inference
algorithms through declarative specication. In ICSE ,
pages 252{261. IEEE Press, 2013.
[7] I. Beschastnikh, Y. Brun, S. Schneider, M. Sloan, and
M. Ernst. Leveraging existing instrumentation to
automatically infer invariant-constrained models. In
ESEC/FSE , 2011.
[8] F. Chierichetti, R. Kumar, P. Raghavan, and
T. Sarl os. Are web users really markovian? In WWW ,
pages 609{618. ACM, 2012.
[9] J. E. Cook and A. L. Wolf. Discovering models of
software processes from event-based data. ACM
Transactions on Software Engineering and
Methodology (TOSEM) , 7(3):215{249, 1998.
[10] G. de Caso, V. Braberman, D. Garbervetsky, and
S. Uchitel. Program abstractions for behaviour
validation. In ICSE . IEEE, 2011.
[11] M. DeGroot and M. Schervish. Probability and
Statistics-International Edition . Addison-Wesley.
Publishing. Company., Reading, Massachusetts, 2001.
[12] F. Facca and P. Lanzi. Mining interesting knowledge
from weblogs: a survey. Data & Knowledge
Engineering , 53(3):225{241, 2005.
[13] R. Fielding and R. Taylor. Principled design of the
modern web architecture. ACM Transactions on
Internet Technology (TOIT) , 2(2):115{150, 2002.
[14] A. Filieri, C. Ghezzi, and G. Tamburrelli. Run-time
ecient probabilistic model checking. In ICSE , 2011.
[15] A. Filieri and G. Tamburrelli. Probabilistic verication
at runtime for self-adaptive systems. In Assurances for
Self-Adaptive Systems , pages 30{59. Springer, 2013.
[16] L. Grunske. Specication patterns for probabilistic
quality properties. In ICSE , pages 31{40. IEEE, 2008.
[17] H. Hansson and B. Jonsson. A logic for reasoning
about time and reliability. Formal aspects of
computing , 6(5):512|535, 1994.
[18] D. N. Jansen, J.-P. Katoen, M. Oldenkamp,
M. Stoelinga, and I. Zapreev. How fast and fat is your
probabilistic model checker? an experimental
performance comparison. In Hardware and Software:
Verication and Testing , pages 69{85. Springer, 2008.
[19] J. Katoen, M. Khattri, and I. Zapreevt. A markov
reward model checker. In Quantitative Evaluation of
Systems, 2005. Second International Conference on
the, pages 243{244. IEEE, 2005.
[20] A. Komuravelli, C. S. Pasareanu, and E. M. Clarke.
Learning probabilistic systems from tree samples. InLICS , pages 441{450. IEEE, 2012.
[21] I. Krka, Y. Brun, D. Popescu, J. Garcia, and
N. Medvidovic. Using dynamic execution traces and
program invariants to enhance behavioral model
inference. In ICSE , volume 2. IEEE, 2010.
[22] M. Kwiatkowska, G. Norman, and D. Parker.
Stochastic model checking. In M. Bernardo and
J. Hillston, editors, Formal Methods for Performance
Evaluation , volume 4486 of Lecture Notes in Computer
Science , pages 220|270. Springer, 2007.
[23] M. Kwiatkowska, G. Norman, and D. Parker. PRISM
4.0: Verication of probabilistic real-time systems. In
G. Gopalakrishnan and S. Qadeer, editors, CAV 2011 ,
volume 6806 of LNCS , pages 585{591. Springer, 2011.
[24] H. Liu and V. Ke selj. Combined mining of web server
logs and web contents for classifying user navigation
patterns and predicting users` future requests. Data &
Knowledge Engineering , 61(2):304{330, 2007.
[25] D. Lo and S.-C. Khoo. Smartic: towards building an
accurate, robust and scalable specication miner. In
FSE, pages 265{275. ACM, 2006.
[26] D. Lo, L. Mariani, and M. Pezz e. Automatic steering
of behavioral model inference. In FSE. ACM, 2009.
[27] L. Mariani, F. Pastore, and M. Pezz e. Dynamic
analysis for diagnosing integration faults. Software
Engineering, IEEE Transactions on , 37(4), 2011.
[28] A. Mesbah and A. Van Deursen. Invariant-based
automatic testing of ajax user interfaces. In ICSE .
IEEE, 2009.
[29] M. Richardson, E. Dominowska, and R. Ragno.
Predicting clicks: estimating the click-through rate for
new ads. In WWW . ACM, 2007.
[30] M. Robillard, E. Bodden, D. Kawrykow, M. Mezini,
and T. Ratchford. Automated api property inference
techniques. TSE, 2012.
[31] R. Sarukkai. Link prediction and path analysis using
markov chains. Computer Networks , 33(1), 2000.
[32] S. Schechter, M. Krishnan, and M. Smith. Using path
proles to predict http requests. Computer Networks
and ISDN Systems , 30(1):457{467, 1998.
[33] S. Schneider, I. Beschastnikh, S. Chernyak, M. D.
Ernst, and Y. Brun. Synoptic: summarizing system
logs with renement. Proc. of SLAML , 2010.
[34] M. Schur, A. Roth, and A. Zeller. Mining behavior
models from enterprise web applications. In FSE 2013 .
[35] J. Srivastava, R. Cooley, M. Deshpande, and P.-N.
Tan. Web usage mining: discovery and applications of
usage patterns from web data. SIGKDD Explor.
Newsl. , 1(2):12{23, Jan. 2000.
[36] P. Tonella, A. Marchetto, C. D. Nguyen, Y. Jia,
K. Lakhotia, and M. Harman. Finding the optimal
balance between over and under approximation of
models inferred from execution logs. In ICST , 2012.
[37] E. Wilde and C. Pautasso. REST: From Research to
Practice . Springer, 2011.
[38] J. Yang, D. Evans, D. Bhardwaj, T. Bhat, and
M. Das. Perracotta: mining temporal api rules from
imperfect traces. In ICSE . ACM, 2006.
[39] Q. Yang and H. H. Zhang. Web-log mining for
predictive web caching. Knowledge and Data
Engineering, IEEE Transactions on , 15(4), 2003.287