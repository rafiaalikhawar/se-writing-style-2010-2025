Automatic Detection of Nocuous Coordination  
Ambiguities in Natural Language Requirements 
 
Hui Yang1 Alistair Willis1    Anne De Roeck1 Bashar Nuseibeh1,2 
 
1Department of Computing 
The Open University 
Milton Keynes, MK7 6AA, UK 
{h.yang, a.g.willis, a.deroeck, 
b.nuseibeh}@open.ac.uk2Lero 
University of Limerick 
Limerick, Ireland 
Basher.Nuseibeh@iero.ie 
 
  
ABSTRACT
 
Natural language is prevalent in requirements documents. How-
ever, ambiguity is an intrinsi c phenomenon of natural language, 
and is therefore present in all such documents. Ambiguity occurs 
when a sentence can be interpreted differently by different read-ers. In this paper, we describe an automated approach for charac-terizing and detecting so-called nocuous ambiguities , which carry 
a high risk of misunderstanding am ong different readers. Given a 
natural language requirements docum ent, sentences that contain 
specific types of ambiguity are first extracted automatically from 
the text. A machine learning algorith m is then used to determine 
whether an ambiguous sentence is nocuous or innocuous, based on a set of heuristics that draw  on human judgments, which we 
collected as training data. We im plemented a prototype tool for 
Nocuous Ambiguity Identification (NAI), in order to illustrate and 
evaluate our approach. The tool focuses on coordination ambigu-
ity. We report on the results of a se t of experiments to assess the 
performance and usefulness of the approach.  
Categories and Subject Descriptors  
D.2.1 [ Requirements/Specification ]: Elicitation Methods, Lan-
guage, Methodologies, Tools   
General Terms  
Management, Measurement, Pe rformance, Experimentation 
Keywords  
Natural language requirements, nocuous ambiguity, coordination 
ambiguity, machine lear ning, human judgments 
 1. INTRODUCTION 
Natural language (NL) is still prev alent in the vast majority of 
requirements documents [3]. One im portant reason for this is that 
NL can help various stakehol ders articulate and communicate 
requirements during the entire life cycle of the software develop-
ment. However, NL requirement s also suffer from typical NL 
problems such as ambiguity. Am biguity occurs when a single 
linguistic expression can be interp reted differently by different 
readers. Ambiguous expressions in requirements can be poten-
tially dangerous when they result in poor requirements quality [4]. 
Our research is motivated by the need to reduce the costs of mis-understandings that can occur dur ing requirements engineering, 
when these misunderstandings are due to ambiguities in the NL 
requirements. Our practical goal is to provide a tool to assist writ-
ers of requirements documents by alerting them to potentially 
harmful ambiguities, called nocuous  ambiguities [6]. Unlike in-
nocuous ambiguities, which tend to be interpreted in the same way by all readers, nocuous ambi guities give rise to different 
interpretations by different reader s, thus contributing to misunder-
standings between stakeholders. Su ch a tool needs to highlight 
those linguistic expressions in re quirements that are recognized as 
nocuous ambiguities, and allow the wr iters to return to elicitation, 
or rephrase for the purpose of improving requirements quality, and facilitating effective communication of these requirements among different stakeholders.  
In earlier work [25], we proposed a general methodology for 
automatic identification of nocuous  ambiguity, which we use to 
guide our research on two types of ambiguity, coordination ambi-guity [6, 22] and anaphora ambiguity [24]. In contrast to other 
work, which is intended to resolv e ambiguity [5, 18], our research 
concerns identification of those am biguities that are likely to lead 
to misunderstandings between diffe rent readers, while discounting 
those which tend to be interprete d in the same way by different 
readers despite their surface features. As such, we consider ambi-guity as a property of the relationship between a text and a group of interpreters, rather than a property of a text or expression per 
se. We also add the categorization of nocuous  and innocuous  
ambiguity depending on the likely distribution of interpretations held by a group of readers of that text. We have observed that not all cases of the ambiguity are actually dangerous: in fact, most remain unnoticed and are resolved to the same interpretation by 
 
Permission to make digital or hard copi es of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that 
copies bear this notice and the full citation on the first page. To copy 
otherwise, or republish, to post on servers or to redistribute to lists, re-
quires prior specific permission and/or a fee. ASE’10 , September 20–24, 2010, Antwerp, Belgium. 
Copyright 2010 ACM 978-1-4503-0116-9/10/09…$10.00. 
 
53
all stakeholders. Only nocuous  ambiguity cases that have a high 
risk of misunderstanding between different readers are truly dis-
ruptive and deserving of further attention.  
Our previous work [6, 22] focused on coordination ambiguity, a 
particularly common kind of structural ambiguity, highly preva-lent in requirements documents. We investigated a methodology 
that used a number of heuristics based on corpus-based statistical 
information together with human judgments to predict whether a 
coordination ambiguity may be misunderstood for a given ambi-
guity threshold. However, our fi rst system was semi-automatic: it 
relied on manual selection of c oordination ambiguity instances 
from a number of requirements docum ents. In this paper, we ex-
tend our previous work on coordination ambiguity by introducing 
new functional process modules, su ch as the extraction of am-
biguous coordination instances and the recognition of coordina-
tion constituents contained in c oordination constructions. These 
are necessary for automatic identification of nocuous coordination ambiguity. Moreover, we implem ented a prototype tool, called 
Nocuous Ambiguity Identification (NAI), to illustrate and evalu-ate our approach. Given an NL requirements document, our tool 
can generate a workflow that integrates with several required functional modules to automati cally detect nocuous ambiguity. 
The work in this paper differs from our previous work [6, 22] in the following four ways: first, we bring in information extraction techniques to tackle some probl ems in automatic detection of 
coordination ambiguity, which incl ude pattern-based matching to 
detect ambiguous instances contai ned in the sentences, and named 
entity recognition (NER) techniques for the extraction of coordi-
nation constituents, such as coordinating conjuncts and the at-tached modifiers. Second, we in troduce two more heuristics, col-
location frequency in the local document and semantic similarity. These enrich previous heuristics and explore further aspects of coordination ambiguity that may lead a reader to prefer a particu-lar reading. Third, we employ the LogitBoost algorithm for the building of a so-called “nocuity cl assifier”. We show that this 
machine learning algorithm performs better than the Logistic Regression algorithm that was used in our previous work [22]. Fourth, we implement an automate d tool to detect and highlight 
on screen, nocuous ambiguity in te xt. We are not aware of any 
comparable automated tools.  
The rest of the paper is structur ed as follows. In Section 2, we 
introduce the ambiguity problem  in requirements documents. 
Section 3 provides the detailed de scription of individual func-
tional modules in the framework of the NAI tool used for coordi-
nation ambiguity. The building of a coordination ambiguity data-set and the construction of the nocu ity classifier are described in 
Section 4. Experimental setup a nd results are reported in Section 
5. Section 6 discusses related wo rk, and conclusions and future 
work are presented in Section 7. 
2. The Ambiguity Problem 
Ambiguity is a phenomenon inherent  in natural language. It oc-
curs everywhere in natural langua ge requirements. For example, 
in a total of 11 requirements engi neering documents we collected 
for the study, there were 3404 sentences containing coordination 
ambiguity instances, which make up about 12.68% of all the sen-tences (26829 in total) in this dataset.    Computationally, structural ambiguity can be recognised when a 
parser assigns more than one po ssible parse to a sentence. Coor-
dination ambiguity is one of th e particularly common kinds of 
structural ambiguity. A coordination structure connects two words, phrases, or clauses togeth er via a coordination conjunction 
(e.g., ‘ and’, ‘or’, etc). Consider the following real examples
1:  
E1. They support a typing system for architectural components 
and connectors .  
E2. It is described the size of vector-based input and output . 
In the example (E1), the coordination construction ‘ architectural 
components and connectors ’ can be bracketed as ‘ [architectural 
[components and connectors]] ’ that is interpreted as ‘ architec-
tural components ’ and ‘ architectural connectors ’ ,  o r  a s  ‘ [archi-
tectural components] and [connectors] ’, in which case it is only 
component  which is modified by architectural . However, in the 
example (E2), although the coordination construction ‘ vector-
based input and output ’ can also be interpreted as ‘ [vector-based 
[input and output]] ’ or ‘ [vector-based input] and [output] ’, it 
seems that the former reading is most likely to be preferred, per-
haps because of semantic similarity between input and output. 
When presented to human judges, for example (E1), 7 out of 17 
judges committed to the reading ‘ [architectural [components and 
connectors]] ’ whereas 7 of the remaining judges chose the read-
ing ‘ [architectural components] and [connectors] ’ (information 
about human judgment collection is given in the section 4.1). We 
treat (E1) as an example of nocuous  ambiguity  because the risk of 
diverging interpretation is high. In  contrast, for the example (E2), 
the majority of judges (16 out of 17) agreed with the reading 
‘[vector-based [input and output]] . So we say (E2) exhibits in-
nocuous  ambiguity , because it is interpreted in the same way by 
different readers, and has a lo w risk of being misunderstood. 
Table 1. Construction patterns used in coordination ambigu-ity (n
1 and n2, v1 and v2 are coordinated compounds (i.e. noun or 
verb compound); the underline part-o f-speech tag is the headword 
of the attached modifier; c is a coordination (i.e. ‘ and’ or ‘ or’); p 
is the preposition) 
Type Pattern Example 
adj n1 c n2  manual  input  and output  
vbn n1 c n2 associated  doors  and windows   
nn n1 c n2 project manager  and designer  
[dt|adj] nn  p n1 c n2 the set of plans  and tables   
n1 c n2 nn software  and hardware  product   
 noun 
n
1 c n2 p [dt|adj] nn  book  and paper  on the table   
adv v1 c v2 be manually  rejected  and flagged  
[dt|adj] nn  p v1 c v2 some functions  for receiving  and 
transmitting  
v1 c v2 nn generate  and print  reports  
v1 c v2 adv be inspected  and recorded  automati-
cally     
 
verb 
v
1 c v2 p [dt|adj] nn  be implemented  and executed  on the 
platform  
 
  
                                                                
 
1 Our examples are modified extractions from requirements documents 
available on the web site, h ttp://research.it.uts.edu.au.  
54 
                 Figure 1. The framework for the NAI tool used in coordination ambiguity 
Our automated approach focuses on two main types of coordina-
tion ambiguity: noun compound coordination and verb compound 
coordination, respectively. The c onstruction patterns for these two 
types of coordination ambiguity are depicted in Table 1. The table 
shows that each ambiguity construction pattern generally consists of three basic coordination constituents , two coordinating con-
juncts, near conjunct (NC) and far conjunct (FC), and the attached 
modifier (F). The conjunct is allowed to be a noun or verb com-pound, and the modifier to be an adjective, adverb, or noun. 
For example, the coordination construction ‘ functional model and 
description ’ is the exemplification of the ambiguity pattern ‘ adj n
1 
c n2’, where ‘ model ’ and ‘ description ’ are NC and FC, and the 
adjective ‘ functional ’ is the M, individually. Syntactically, this 
pattern can be interpreted by two distinct bracketing: (1)  ‘[adj  [n 1 
c n2]]’ - ‘[functional [model and description]] ’ and (2) ‘ [adj  n1]c 
[n 2]’ - ‘ [functional model] and [description] ’. We say that (1) 
displays high attachment  of the modifier, where M applies to both 
FC and NC, and (2) displays low attachment , where M applies 
only to NC. 
3. A Nocuous Ambiguity Identification Tool 
We have implemented a Nocuous Ambiguity Identification (NAI) 
tool to automatically detect nocuous ambiguities in text. NAI 
identifies potentially ambiguous patterns in a textual input, and a 
nocuity classifier then classifies the instances into nocuous and 
innocuous cases. 
The conceptual architecture of the NAI tool is shown in Figure 1. 
It consists of four main func tional process modules: Text Pre-
processing Module, Ambiguity In stance Detection Module, Coor-
dination Constituent Extraction Module, and Nocuous Ambiguity 
Classification Module, respectively.  We describe the behavior of each of the modules in turn, and gi ve details of how the system is 
trained in Section 4. 
3.1 Text Pre-processing 
Given a text document, our tool first executes several text pre-
processing steps, including sentence splitting, part-of-speech 
(POS) tagging, and phrase-based shallo w parsing. At first, the text 
is split into a collection of sentence using a sentence boundary 
detector2. Then, for each sentence, the Stanford parser3 is used to 
obtain word lemma and POS tags  (e.g., noun, verb, adjective, 
adverb, etc.) of individual words and associated phrase informa-
tion. Furthermore, some statisti cal information (e.g., word co-
occurrence and word distribution) is calculated based on the posi-
tion of words in the sentence, and saved into the back-end MySQL database. 
3.2 Ambiguous Instance Detection 
This process consists of two main steps: Step I is to detect the 
sentences that contain coordinati on constructions; Step II is to 
extract relevant ambiguity instances from the detected sentences. 
Given a sentence, Step I is to search the POS-tag sequence of the 
sentence and determine whether the sentence contains one of the coordination construction patterns de scribed in Table 1. A search-
ing window is moved from left to  right to examine the POS-tag 
sequence of the sentence. If an ambiguity construction pattern 
falls into the window, then th e corresponding substring is ex-
tracted from the sentence as an ambiguous coordination instance. 
                                                                
 
2 http://text0.mib.man.ac.uk:8080/ scottpiao/sent_detector  
3 http://nlp.stanford.edu/so ftware/lex-parser.shtml  Documents Data Generated 
    Pre-processed 
           Text 
Machine  
Learning   Heuristics 
Machine Judgements Human Judgements     Ambiguity 
       Instances 
Human constituents 
Nocuous 
Ambiguities instances text Sentence 
Splitting POS 
Tagging Shallow 
Parsing 
Word 
Co-occurrence Word 
Distribution Text Pre-processing Processes 
Ambiguous Instance Detection 
Coordination Consti tuent Extraction 
Nocuous Ambiguity Classification Syntactic Rules Syntactic Patterns NLP Tools Techniques 
55If the sentence does contain one or more of the patterns, then step 
II extracts relevant coordination constructions from the sentences. 
This step is fundamental to th e understanding of a sentence, espe-
cially for a sentence which is longer and far more complex [1]. 
For example, 
E3. Use daylight to achieve the desired light setting of room and 
hallway section whenever possible.  
In the sentence (E3), the substring ‘ the desired light setting of 
room and hallway section ’, in practice, can be split into two dis-
tinct ambiguity instances, ‘ the desired light settings of room and 
hallway ’ and ‘ room and hallway section ’. To simplify the ambi-
guity analysis, it is necessary to  treat these two ambiguity in-
stances separately, and determ ine whether either is nocuous. 
Therefore, our ambiguity analysis  is based on a smaller unit level 
- the instance level other than on the sentence level with respect to 
long and complicated sentences.  
3.3 Coordination Constituent Extraction  
The recognition of coordination constitu ents is, in fact, the task of 
named entity recognition (NER). NER plays an important role in 
analyzing the syntactic and sema ntic relations among coordina-
tion constituents discussed later. In order to extract appropriate 
coordination constituents from an ambiguous construction pattern, 
a set of syntactic rules are created based on the position of indi-vidual constituents in the pattern and the property of their corre-sponding POS tags. For each ambiguity instance, two coordinat-
ing conjuncts, near conjunct (NC)  and far conjunct (FC), and the 
attached modifier (M) are separately extracted from the instance by relevant syntactic rule, and sa ved into the database together 
with the corresponding instance id. 
3.4 Heuristics to Predict Nocuity 
Having identified the sentences which contain instances of coor-
dination ambiguity, and recognized the constituents which give rise to the ambiguity, the final stage is to identify whether the 
ambiguity is nocuous or not. To identify cases of nocuous ambi-
guity, we implemented a number of heuristics that identify the 
properties of an ambiguity instance, which in turn may lead an interpreter to favor high attachment or low attachment interpreta-tions. Our tool then classifies the input as nocuous or innocuous 
using the values of the set of heuristics input to a trained classifier using the LogitBoost algorithm. Th e remainder of this subsection 
deals with the particular heuristic s; details of training and using 
the classifier are given in section 4. 
We explored a number of heuristics to apply to ambiguity in-
stances. Each of the individual heuristics attempts to identify 
aspects of the ambiguity instance that may lead an interpreter to 
favor high attachment or low att achment interpretation. Heuristics 
run over the instance and their scores are saved as features in a feature vector that will be used later by a machine learning algo-rithm to classify coordination ambiguity as nocuous or innocuous. 
One of the major approaches we use here is a corpus-based ap-
proach. Some of the heuristics (e.g., coordination matching, dis-
tribution similarity, and collocation frequency) are based on sta-tistical information (e.g., word distribution and collocation) that is obtained from a large English corpus – British National Corpus
4 
(BNC) via the Sketch Engine5 [14]. We here give a brief descrip-
tion of each of the heuristics below. Besides the four heuristics 
used in the previous work [6], we introduce two more heuristics, 
collocation frequency in local document and WordNet-based 
semantic similarity. We present below a brief description of each of the heuristics.  
Coordination Matching.  This heuristics hypothesize that if the 
headwords
6 of the two conjuncts are frequently coordinated in the 
text, then that coordination form s a single syntactic unit, and the 
particular instance should therefore prefer to high attachment  
interpretation. The word sketch facility of the Sketch Engine pro-
vides statistical information about  lists of words that are con-
joined with ‘ and’ or ‘ or’. The higher the ranked score is, the more 
frequently the two conjuncts occu r in the BNC corpus. Consider 
the example below: 
E4. Security and Privacy Requirement  
The Sketch Engine returned a highly-ranking score of 5.8 for the 
coordinated words ‘ security ’ and ‘ privacy ’. In the human judg-
ments we collected, 12 out of 17 judges considered that it should be interpreted as high attachment  interpretation, 4 thought it am-
biguous, and only 1 judge chose lo w attachment interpretation. 
Therefore, this ambiguity tends to  be interpreted as a high attach-
ment reading, i.e. ‘ [[Security and Privacy] Requirement] ’.  
Distributional Similarity.  This heuristic is based on the assump-
tion suggested by Kilgarriff [13], which is that strong distribu-
tional similarity between the headwords of the two conjuncts indicates that the conjuncts form a syntactic unit, thus resulting in 
the preference of the high attachment  interpretation. The distribu-
tional similarity of two words is a measure of how often these two 
words can be found in the same contexts. For example,  
E5. Function for receiving and transmitting  
The words ‘receiving’ and ‘trans mitting’ have strong distribu-
tional similarity despite thei r opposite meanings. Of the 17 
judges, 14 judged this ambiguity to  be high attachment interpreta-
tion, 3 judged it to be an ambiguity, but no one considered it a 
low attachment reading.  
Collocation Frequency.  This heuristic assumes that if the modi-
fier is collocated much more freque ntly with the headword of the 
near conjunct than it is collocated  with the headword of the far 
conjunct; the particular instance should display low attachment  of 
the modifier. The score returned by the heuristics is the ration of 
the collocation frequency with the near headword over the collo-cation frequency with the further headword. Collocation fre-quency had been proven very usef ul in the disambiguation task 
[18, 21]. For instance, 
E6. Project manager and designer  
‘project ’ has a collocation score of 29.55 with ‘ manager ’ in the 
BNC, but it has no collocation with ‘ designer ’. In the collected 
                                                                
 
4 http://www.natcorp.ox.ac.uk/  
5 http://www.sketchengine.co.uk/  
6 A headword is the main word of a phrase,  and the other words in that 
phrase modify it. 
56judgments, 8 judges favored low a ttachment reading while 4 peo-
ple preferred high attachment readi ng. It seems this ambiguity is 
more likely to be interpreted as low a attachment reading.  
Here this heuristic was employed  on two different resources and 
obtained separate heuristics scores:  
a) Local document: the local-based  collocation frequency score is 
calculated based on the collocation frequency information in the 
local document. This score looks mo re useful especially when the 
headwords for coordination constituents are domain-specific words that are too scarce to be found in the BNC. Moreover, the contexts in the local document pr obably provide stronger cues for 
the semantic relationships between  the conjuncts and the attached 
modifier.  
b) BNC Corpus: the corpus-based  collocation frequency score 
that is estimated based on the collocation frequency information 
on the BNC corpus.  
Morphology.  This heuristic is based on syntactic parallelism 
suggested by Okumura and Muraki  [19] in disambiguating coor-
dination. It hypothesizes that if the headwords of the two con-
juncts share a similar morphology, th en they form a syntactic unit, 
hence the instance favoring the high attachment interpretation. The inflectional morphology of a language is the analysis of the changing of words to signify thei r term, number, gender etc: in 
English it consists largely of suffi xes such as –ed, -ing, and -s. 
The derivational morphology of E nglish is more complex, but 
suffixes, such as -ation and -able, are also very common. The 
score returned by this heuristic  is the number of common trailing 
characters of the headwords of the potential conjuncts. 
Semantic Similarity.  This heuristic presen ts a measure of seman-
tic similarity between the headwords of the two conjuncts based 
on the taxonomic structure in WordNet
7. Resnik [20] has pointed 
out that similarity of meanings of  conjoined heads is an important 
cue to coordination ambiguity resolution. For instance, 
E7. vector-based input and output 
E8. manual input and selection 
Clearly ‘ input ’ and ‘ output ’ in (E7) are more similar in semantics 
than ‘ input ’ and ‘ selection ’ in (E8). Therefore, (E7) could be 
more likely to be interpreted as high attachment  of the modifier. 
This assumption conforms to the distribution of human judgment. 
In (E7), 16 out of 17 judges had a favor of high attachment read-ing, while in (E8), only 5 of 17 judges chose high attachment 
interpretation.  
The similarity of meaning could be captured well by semantic 
similarity in the WordNet ta xonomy by measuring the distance 
between the nodes corresponding to the headwords of the two conjuncts. If the headwords of coordinating conjuncts exhibit strong semantic similarity, then the ambiguity instances favour a 
high attachment interpretation. This heuristic is implemented by the NLP tool - Java WordNet Similarity Library
8. 
                                                                 
7 http://wordnet.princeton.edu/  
8 http://nlp.shef.ac.uk/result/software.html  4. Training and Using the Nocuity Classifier 
As discussed in section 3.4, each of the heuristics individually 
indicates a preference for high or low attachment. However, they do not predict whether a given ambiguity instance is nocuous or 
not. In this section, we describe  how the individual heuristics are 
combined to classify a particular ambiguity as nocuous or innocu-
ous at a given ambiguity threshold. 
4.1 Building a Dataset 
In order to build a working classifier, we require a collection of 
human judgments of ambiguous senten ces. This allows us to iden-
tify which sentences display noc uous ambiguity, and use this 
information to identify how the particular heuristics described in 
section 3.4 are combined to replicate the human judgments. 
To train NAI to recognise instances of nocuous ambiguity, we used the dataset collected by Chantree et al. [6]. This dataset de-
scribes 138 coordination instances from the sentences of a set of 
requirement documents. Each of th e instances contains one of the 
ambiguity construction patterns de scribed in Table 1. Among the 
instances, noun compound conjunctions account for a significant number, with 118 instances (85.5%). In noun compound conjunc-tions, nearly half of the cases ar ose as a result of noun modifiers, 
while there are 36 cases with adjective and 18 with preposition 
modifiers. 
Human Judgment Collection. The coordination instances that 
contain potential ambiguity were split into 4 surveys and pre-
sented to a group of computing professionals including academic staff or research students. Each  instance was judged by 17 people. 
For each instance, the judges were asked to select one of the three options: high attachment (HA) of the modifier, low attachment (LA) of the modifier, or ambi guous (A). The latter we call ‘ac-
knowledged ambiguity’ - i.e. the read er realizes an ambiguity is 
present in the text, but does not feel able to judge which interpre-tation was intended by the writer. Table 2 shows the judgment count for two sample instances. In stance (a) was judged mainly to 
have high attachment of the modifier, while instance (b) was 
judged mainly to be ambiguous.  
Table 2. Judgment count for the sample instances (HA=high 
attachment; LA=low attachment; and A=Ambiguous)  
 Judgments 
 HA LA A 
(a) security  and privacy  requirements  12 1 4 
(b) electrical  characteristics  and inter-
face 4 4 9 
 
4.2 Training the Classifier 
A key concept in training the classifier is the ambiguity threshold . 
We require a decision point to determine whether a particular 
instance exhibits nocuous ambiguity or not. We use the concept of 
ambiguity threshold  that represents how much agreement is re-
quired from the judges over a partic ular interpretation. Use of a 
threshold also allows us to adju st tolerance levels: some applica-
tion domains (e.g., safety critical systems) may wish to use to a 
low threshold before consid ering an ambiguity nocuous. 
 
57 
Figure 3. Sample output of the NAI t ool for nocuous coordination ambiguity 
 
Given an instance with multiple possible interpretations and a set 
of judgments, the certainty  of an interpretation is calculated as the 
percentage of the judgments for that interpretation against the 
total number of the judgments fo r the whole instance. For in-
stance, in Table 2, the certainty of HA for Instance (a) is 
12/17=70.58%. 
0102030405060708090100
10 20 30 40 50 60 70 80 90 100
Ambiguity Thresholds (%)Ambiguities (%)Inno
Nocu
 
Figure 2. Proportions of interpretations at different ambiguity 
thresholds  (Nocu=nocuous; Inno=innocuous)  Given a coordination instance, if either of the interpretations, HA 
or LA, has a certainty greater than the ambiguity threshold τ, we 
say this coordination instance displays innocuous  ambiguity. 
The relationship between ambiguity  threshold and the classifica-
tion of nocuous ambiguity in the dataset is illustrated in Figure 2. It shows that, at low thresholds, very few instances exhibit nocu-ous ambiguity, because the certainty constraint is so low that it is easier to be satisfied with a small number of agreements. How-
ever, almost all instances are cla ssified as nocuous since it is very 
hard for judges to reach a consensus on the same interpretation. 
Building the nocuity classifier.  Our nocuity classifier was 
trained based on a set of heuristic scores together with the human 
judgments collected in the training data. More specifically, each 
ambiguity instance is described as a training/test instance, which 
is represented as an attribute-value vector, where the value of each attribute is the score of a particular heuristic described ear-lier. The class label of a traini ng instance, nocuous (Y) or innocu-
ous (N), at a given ambiguity thre shold is determined by the dis-
tribution of multiple human judgments discussed earlier. 
To select an appropriate machine learning (ML) algorithm to 
build our nocuity classifiers, we tested our dataset on a number of 
58ML algorithms available in the WEKA package9 including the 
logistic Regression algorithm that was used in our previous work 
[22]. Finally, we selected the LogitBoost algorithm for building 
the nocuity classifier, because it performed better than other can-
didates including decision trees, J48, Naive Bayes, SVM, and 
Logistic Regression. 
4.3 Applying the Classifier 
To determine whether a test ambiguity instance displays nocuity 
or not, we presented the feature v ector of the instance to the clas-
sifier, and obtained the predicted class label returned by the clas-
sifier. 
Once the process of nocuous ambiguity identification was com-
pleted, we highlighted the ambi guous sentences in the original 
text, each of which contained at least one nocuous ambiguity. In 
addition, we presented the user with the extracted nocuous ambi-
guity instances with the identif ied coordination constituents high-
lighted by different colors. The sample output of our NAI tool for nocuous coordination ambiguity is shown in Figure 3.  
5. Experiments and Results 
To evaluate the performance of our tool, we used a standard 10-
fold cross-validation technique in which, for each iteration, we trained on 90% data and tested on 10% of the remaining data. The 
performance of the system is measured in terms of precision (P), 
recall (R), F-measure (F), and Accuracy: 
 
FN TPTPR+=
                    FP TPTPP+=
  
 
RPPRmeasureF+∗∗+= −2) 1(
ββ    
totalTN TPAccuracy+=  
 
where TF (true positives) is the number of correctly identified 
nocuous  ambiguities, TN (True negative) is the number of cor-
rectly identified innocuous  ambiguities, FN (false negatives) is 
the number of nocuous ambiguity not identified by the system, 
and FP (false positives) is the number of innocuous ambiguities 
which the system incorrectly classified as nocuous. The weight β 
is set with 0.25 in order to favor  the precision. All results are av-
eraged across ten iterations . 
5.1 Performance of the Classifier 
We report in this section the performance (i.e. precision, recall, 
and f-measure) of the ML-based classifier using the LogitBoost algorithm at different ambiguity thresholds. Figure 4 summarizes 
the precision and recall results of the classifier. We compared the 
precision of the classifier with a baseline precision (P_BL) that assumes that all of the instances are potentially nocuous ambigui-ties. Compared with baseline precision (P_BL), the LogitBoost 
classifier performed well with precision of up to 75% on average 
at different threshold levels. It  suggests that the heuristics we 
developed contain distinguished features which provide strong 
                                                                
 
9 http://www.cs.waikato.ac.nz/~ml/index.html  discriminating power in determin ing the nocuity property of an 
ambiguous instance. However, at some low threshold levels, es-
pecially when the thresholds are below 45%, the classifier did not 
work well with respect to recall.  The recall dramatically dropped 
down to 10%. A possible reason for this is the lack of positive 
cases (i.e. nocuous ambiguities) at  the low threshold level (see 
Figure 2), which results in deterioration of performance. 
020406080100120
40 45 50 55 60 65 70 75 80
ambiguity threshold (%)%P
R
P_BL
 
Figure 4. The performance of the classifier at different ambi-
guity thresholds.  (P_BL=Baseline Precision) 
5.2 Impact of the Heuristics 
As described earlier, we introdu ced two more heuristics in our 
NAI tool: one is local-based collocation frequency which ex-
ploited the co-occurrence frequency between coordinating con-
juncts and the attached modifier in the local context; the other is 
semantic similarity that investigated the semantic relationship between the far conjunct and near conjunct using the WordNet taxonomy. To estimate whether th e two newly-added heuristics 
can improve system performance,  we conducted another set of 
experiments with the heuristics only used in our previous work. 
We compared the F-measure performance of two sets of experi-ments, one is for all of the heuris tics (F-All) that we described in 
this paper, and the other is fo r the heuristics except for the two 
newly-added heuristics (F-Part).  
Moreover, to compare the performance of our proposed heuris-
tics-based approach for nocuous ambi guity identification, we used 
a random model as a baseline. In  the random model, we assume 
that each recognized ambiguity instance has the potential to be a 
nocuous  ambiguity, and is counted as a positive match for the 
baseline model. The random model achieves an ‘ideal’ recall R
BL 
of 100%, and the precision and F-measure are calculated as: 
s Ambiguitieof totalJudgmentsby identified NocuousPBL##= 
BL BLBL BL
BLR PRPmeasureF+= −2       
As Figure 5 shows, compared with the baseline F-measure 
(F_BL), the heuristics-based appro ach is an effective method for 
the identification of nocuous ambiguity due to relatively high precision (see Figure 4). Figure 5 s hows that with those selected 
thresholds (0.4-0.55), the heuris tics-based approach exhibits a 
marginal improvement in F-measure compared to the baseline 
model. Nevertheless, the improve ment gradually decreases with 
the increase of the threshold value.  After the threshold is set to 
59above 0.75, the heuristics-based a pproach performs slightly better 
than the baseline model due to th e quite high F-measure value.  
Our results also show that the F-All performs consistently better 
than the F-Part throughout all of the threshold levels. It suggests 
that the performance of our tool did benefit from the newly-added two heuristics, which indicates th at local context information and 
semantic relationships between the coordination constituents pro-
vide the useful clues for the id entification of nocuous ambiguity. 
020406080100120
40 45 50 55 60 65 70 75 80
ambiguity threshold (%)F-measure(%)F_BL
F_Part
F_All
 
Figure 5. The Impact of the heuristics on system performance.  
5.3 System Performance Comparison 
In the implemented NAI tool, we  chose the LogitBoost (LB) algo-
rithm to build our ML-based nocuity classifier, which replaced 
the original Regression (LR) algorithm used in our previous work 
[22]. To evaluate the accuracy of  our proposed ML-based model, 
we used two baseline (BL) mode ls, BL-1 and BL-2 models. The 
BL-1 model assumes that all the instances are innocuous  ambigui-
ties, whereas the BL-2 model suppos es that all the instances are 
nocuous  ambiguities. As discussed previously, at low ambiguity 
thresholds, most of the instan ces are judged as innocuous ambi-
guities, while at high ambiguity thresholds, the majority of the 
instances are nocuous ambiguity (see Figure 2). Therefore, to 
compare fairly with the performance of the ML-based model, a good strategy is to compare it with the BL-1 model at low thresh-olds, and to compare it with the BL-2 model at high thresholds. 
The performance comparison of the LogitBoost (LB) model 
against the two baseline models at  different ambiguity thresholds 
is shown in Figure 6. The LB model performed well with an accu-
racy of above 75% on average at different ambiguity threshold levels. As expected, at very high and very low thresholds, the LB 
model did not outperform the ba seline models due to the high 
accuracy. However, the LB model displayed its advantage when the ambiguity threshold fell in the range between 0.45 and 0.75. The LB model generally performed  better than the baseline mod-
els and the maximum improveme nt was achieved around the 58% 
crossover point where the two base line models intersect. Our tool 
accomplished an approximate 21% increase in accuracy. It sug-gested that the combined heuristics do have some capability of distinguishing nocuous from innocuous  ambiguity at the weakest 
region of the baseline models. 
Figure 6 also shows that, with th e LB model, the tool improved 
the overall accuracy with an increase of approximate 4.4% on average compared with the previous model, the Logistic Regres-
sion (LR) model. The possible explanation is that the LB algo-
rithm is more suitable for the dataset with the numeric-attribute feature vectors.  
0102030405060708090100
40 45 50 55 60 65 70 75 80
Ambiguity Threshold (%)Accuracy (%)BL-1
BL-2
LB
LR
Figure 6. The performance comparison of the ML-based 
model to the two baselines, BL-1 and BL-2.  (BL-1=baseline (all 
innocuous); BL-2=baseline (all nocuous); LB=LogitBoost; 
LR=Logistic Regression)) 
6. Related Work 
NLP tools applied to NL requirements. A number of natural 
language processing (NLP) systems or tools applied to NL re-
quirements had been developed in recent years. Ambriola and 
Gervasi [2] developed a web-base d NLP tool, called Circe, which 
was designed to facilitate the gather ing, elicitation, selection, and 
validation of NL requireme nts. IBM Rational Doors10, a require-
ments management tool , provides relevant functional modules for 
the generation of NL requirements and the traceability among NL requirements. Goldin and Berry [11] implemented a tool called 
Abstfinder to identify the abstrac tions from natural language text 
used for requirements elicitation. Lee and Bryant [16] developed 
an automated system to assist the engineers to build a formal rep-
resentation from informal requi rements like NL requirements. 
Ambiguity in NL requirements. Several studies dealing with 
ambiguity identification have aimed to help improve the quality 
of NL requirements documents. So me tools have been developed 
specifically to detect, measure, or reduce possible structural am-biguities in text. Kamsties et al. [12] describe pattern-driven in-
spection technique to detect am biguities in NL requirements. 
Fuchs and Swhwitter [9] present a restricted NL, called Attempt 
Controlled English (ACE), to translate specifications into sen-tences in first-order logic in or der to reduce the ambiguity in re-
quirement specifications. Mich and Garigliano [17] investigate the 
use of a set of ambiguity indices for the measurement in syntactic and semantic ambiguity, which is implemented using an NLP 
system called LOLITA. Kiyavitska ya et al. [15] proposed a two-
step approach in which a set of lexical and syntactic ambiguity measures are firstly applied to am biguity identification, and then a 
                                                                
 
10 http://www-01.ibm.com/software/awdtools/doors/  
60tool to measure what is potentially ambiguous specific to each 
sentence. 
Finally, some of the tools have been developed to examine the 
quality evaluation of requirements. These include QuARS [7], ARM [23], and the tool by Fantechi et al. [8] for use case re-quirements. These approaches define a quality model composed of a set of quality metrics (e.g., vagueness, subjectivity, optional-ity, weakness, etc.), and devel op analysis techniques based on a 
linguistic approach to detect the defects related to the inherent 
ambiguity in the requirements. 
Coordination Ambiguity. In last decade, a number of approaches 
have been proposed to address ambiguity resolution in coordina-
tion constructions. Previous resear ch efforts have generally fo-
cused on either corpus-based statistical methods (e.g., co-
occurrence frequency between coordinating conjuncts and the 
modifier over a text corpus such as web resource [18] or Wall Street Journal [10]), or linguistic approaches that made use of part-of-speech (POS) tagging and shallow (e.g., phrase) and deep parsing (e.g., parsing tree) inform ation to apply pattern- or rule-
based matching [1, 19, 21]. In add ition, similar to our work, Res-
nik [20] took advantage of semantic similarity of a taxonomy to resolve coordination ambiguity  involving nominal compounds. 
Unlike other related ambiguity work that attempts to resolve am-biguity by applying disambiguation techniques to select the most 
appropriate reading, our studies present readers with nocuous ambiguities which may potentially be misunderstood by different 
readers, and allow these readers to determine the preferred inter-pretations.  
7. Conclusions and Future Work 
Natural language still prevails in a large number of requirements 
documents. We need ways to cope with the ambiguity inherent in 
natural language. It is important to develop scalable automated 
techniques to detect potential noc uous ambiguities in natural lan-
guage requirements, in order to mi nimize their side effects at the 
early stages of the software development lifecycle.  
In this paper, we described an automated approach to characterize 
and identify potentially nocuous ambiguities, which have a high 
risk of misunderstanding among diffe rent readers. Given a natural 
language requirements document, ambiguous instances contained 
in the sentences were first extracted. Then, a machine learning approach was employed to cla ssify ambiguous instances as nocu-
ous or innocuous, subject to a give n ambiguity threshold. A proto-
type tool, which focuses on c oordination ambiguity, was imple-
mented in order to illustrate and evaluate our approach. We re-
ported on a set of experimental re sults to evaluate the perform-
ance and effectiveness of our automated approach. The results show that our tool is capable of accurately detecting potentially dangerous ambiguities in the NL requirements. 
Based on significant technical de velopment and substantive em-
pirical studies, we believe that the application of our approach is 
lightweight and usable in that it allows requirements analysts to 
experiment iteratively to identify potential nocuous ambiguity in requirements, depending on their chosen analysis sensitivity 
threshold. 
However, a number of interesting i ssues remain to be investigated 
in order to improve our tool’s performance and validate its use in 
practice. First, more heuristics n eed to be developed to further explore aspects of ambiguity that enhance the accuracy of our 
tool. Second, our current tool is  specific to coordination ambigu-
ity. It is necessary to extend it to a wider range of ambiguity types, for example, to other types of structural ambiguity like prepositional attachment ambiguity and semantic ambiguities 
such as anaphora. We have begun to explore the identification of nocuous ambiguity in terms of anaphora ambiguity in our ongoing 
work [24]. Third, we need and plan  to make our tool more widely 
accessible to validate its use in practice. This may include migrat-ing the technology to a web-based environment, providing an automated analysis as some kind of web service. Indeed, we en-visage that this automated suppor t for ambiguity analysis should 
fit into a number of requirement s management environments, in 
which requirements authors are able  to invoke such analysis tools 
in the same way as writers invoke spell checkers. We are cur-rently investigating the developm ent of this capability within a 
well-known commercial tool.      
8. ACKNOWLEDGMENTS 
This work was supported by the UK Engineering and Physical 
Sciences Research Council (EPSRC)  as part of the MaTREx pro-
ject (EP/F068859/1), and by the Science Foundation Ireland (SFI 
grant 03/CE2/I303_1). We are grateful to our research partners at 
Lancaster University for their input, and to Ian Alexander for his practical insights and guidance. 
9. REFERENCES 
[1] Agarwal, R., and Boggess, L.  1992. A simple but useful 
approach to conjunct identification. In Proceedings of the 
30th Annual Meeting of the Association for Computational 
Linguistics, 15–21. 
[2] Ambriola, V., and Gervasi, V.  1997. Processing natural lan-
guage requirements. In Proceedings of the 12th international 
conference on Automated software engineering 36-45. 
[3] Berry, D. M., Kamsties, E., a nd Krieger, M. M. 2003. From 
contract drafting to software specification: Linguistic sources 
of ambiguity.  
[4] Boyd, S., Zowghi, D., and Fa rroukh, A. 2005. Measuring the 
expressiveness of a constraine d natural language: An empiri-
cal study. In Proceedings of the 13th IEEE International 
Conference on Requirements Engineering (RE’05), Wash-ington, DC, 339-352. 
[5] Brill, E., and Resnik, P. 1994. A rule-based approach to prepositional phrase attachment disambiguation. In Proceed-ings of COLING, 1198 - 1204. 
[6] Chantree, F., Nuseibeh, B., De Roeck, A., and Willis, A. 
2006. Identifying Nocuous Ambiguities in Natural Language Requirements. In Proceedings of 14th IEEE International Requirements Engineering Conference (RE'06), Minneapo-lis, USA, 59-68. 
[7] Fabbrini, F., Fusani, M., Gnes i, S., and Lami, G. 2001. The 
linguistic approach to the na tural language requirements, 
quality: benefits of the use of an automatic tool. In Proceed-
ings of the twenty sixth annual IEEE computer society-
NASA GSFC software engi neering workshop, 97–105. 
61[8] Fantechi, A., Gnesi, S., La mi, G., and Maccari, A. 2003. 
Applications of Linguistic T echniques for Use Case Analy-
sis. Requirements Engi neering, 8(9), 161-170. 
[9] Fuchs, N. E., and Schwitter, R. 1995. Specifying logic pro-
grams in controlled natural language. In Proceedings of the 
Workshop on Computational L ogic for Natural Language 
Processing, 3–5. 
[10] Goldberg, M. 1999. An unsupervis ed model for statistically 
determining coordinate phrase attachment. In Proceedings of 
ACL, 610-614. 
[11] Goldin, L., and Berry, D. M. 1994. Abstfinder, a prototype abstraction finder for natural language text for use in re-quirements elicitation: design,  methodology, and evaluation. 
In Proceedings of the First International Conference on Re-
quirements Engineering, 18–22. 
[12] Kamsties, E., Berry, D., and Paech, B. 2001. Detecting am-biguities in requirements documents using inspections. In Proceedings of the First Workshop on Inspection in Software Engineering (WISE'01), 68-80. 
[13] Kilgarriff, A. 2003. Thesauruse s for natural language proc-
essing. In Proceedings of NLP-KE, 5–13. 
[14] Kilgarriff, A., Rychly, P., Sm rz, P., and Tugwell, D. 2004. 
The Sketch Engine. In Proceedi ngs of the Eleventh European 
Association for Lexicography (EURALEX), 105–116. 
[15] Kiyavitskaya, N., Zeni, N., Mich, L., and Berry, D. M. 2008. 
Requirements for tools for am biguity identification and 
measurement in natural language requirements specifica-tions. Requirements Engin eering Journal 13, 207–240. 
[16] Lee, B. S., and Bryant, B. R.  2004. Automation of software 
system development using na tural language processing and 
two-level grammar. Radical Innovations of Software and 
Systems Engineering in the Future. Springer, Heidelberg 219–233. 
[17] Mich, L., and Garigliano, R. 2000. Ambiguity measures in 
requirement engineering. In Proceedings of international conference on software—theory and practice (ICS2000), 39–
48. 
[18] Nakov, P., and Hearst, M. 2005.  Using the Web as an Im-
plicit Training Set: Application to Structural Ambiguity 
Resolution. In Proceedings of HLT-NAACL’05, 835–842. 
[19] Okumura, A., and Muraki, K. 1994. Symmetric pattern 
matching analysis for English coordinate structures. In Pro-
ceedings of the 4th Conference on Applied Natural Language Processing, 41–46. 
[20] Resnik, P. 1999. Semantic similarity in a taxonomy: An in-formation-based measure and its  application to problems of 
ambiguity in natural language. Journal of Artificial Intelli-gence Research (JAIR), 11, 95–130. 
[21] Rus, V., Moldovan, D., and Bolohan, O. 2002. Bracketing 
compound nouns for logic form derivation. In Proceedings of the Fifteenth International Florida Artificial Intelligence Re-search Society Conference (FLAIRS), 198 – 202. 
[22] Willis, A., Chantree, F., and De Roeck, A. 2008. Automatic 
Identification of Nocuous Ambiguity. Research on Language & Computation, 6(3-4), 1-23. 
[23] Wilson, W. M., Rosenberg, L. H., and Hyatt, L. E. 1997. Automated analysis of requirement specifications. In Pro-ceedings of the Nineteenth International Conference on Software Engineering (ICSE), 161–171. 
[24] Yang, H., De Roeck, A., Gervas i, Vincenzo, Willis, A., and 
Nuseibeh, B. 2010. Extending Nocuous Ambiguity Analysis 
for Anaphora in Natural Langua ge Requirements. In Pro-
ceedings of the 18th IEEE International Requirements Engi-
neering Conference (RE'10) (In Press). 
[25] Yang, H., De Roeck, A., Willis , A., and Nuseibeh, B. 2010. 
A Methodology for Automatic Identification of Nocuous 
Ambiguity. In Proceedings of the 23rd International Confer-ence on Computational Linguistics (Coling’10) (In Press). 
 
 
62