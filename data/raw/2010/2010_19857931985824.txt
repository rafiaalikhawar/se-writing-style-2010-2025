Coverage Guided Systematic Concurrency Testing
Chao Wang
NEC Laboratories AmericaMahmoud Said
Western Michigan UniversityAarti Gupta
NEC Laboratories America
ABSTRACT
Shared-memory multi-threaded programs are notoriously di f-
ﬁcult to test, and because of the often astronomically large
number of thread schedules, testing all possible interleav -
ings is practically infeasible. In this paper we propose a
coverage-guided systematic testing framework, where we us e
dynamically learned ordering constraints over shared obje ct
accesses to select only high-risk interleavings for test execu-
tion. An interleaving is of high-risk if it has not been cover ed
by the ordering constraints, meaning that it has concurrenc y
scenarios that have not been tested. Our method consists
of two components. First, we utilize dynamic information
collected from good test runs to learn ordering constraints
over the memory-accessing and synchronization statements .
These ordering constraints are treated as likely invariant s
since they are respected by all the tested runs. Second, dur-
ing the process of systematic testing, we use the learned or-
dering constraints to guide the selection of interleavings for
future test execution. Our experiments on public domain
multithreaded C/C++ programs show that, by focusing on
only the high-risk interleavings rather than enumerating a ll
possible interleavings, our method can increase the covera ge
of important concurrency scenarios with a reasonable cost
and detect most of the concurrency bugs in practice.
Categories and Subject Descriptors
D.2.5 [ Software Engineering ]: Testing and Debugging
General Terms
Veriﬁcation, Reliability
Keywords
Concurrency, Coverage, Partial Order Reduction
1. INTRODUCTION
Real-world concurrent programs are notoriously diﬃcult
to test because they often have an astronomically large num-
Permission to make digital or hard copies of all or part of thi s work for
personal or classroom use is granted without fee provided th at copies are
not made or distributed for proﬁt or commercial advantage an d that copies
bear this notice and the full citation on the ﬁrst page. To cop y otherwise, to
republish, to post on servers or to redistribute to lists, re quires prior speciﬁc
permission and/or a fee.
ICSE ’11, May 21–28, 2011, Waikiki, Honolulu, HI, USA
Copyright 2011 ACM 978-1-4503-0445-0/11/05 ...$10.00.ber of thread interleavings. Furthermore, many concurrenc y
related bugs arise only in rare situations, making it diﬃcul t
for programmers to anticipate, and for testers to trigger,
these error-manifesting thread interleavings. In reality , the
common practice of load or stress testing is not eﬀective,
since the outcome is highly dependent on the underlying op-
erating system which controls the thread scheduling. Merel y
running the same test again and again does not guarantee
that the erroneous interleaving would eventually show up.
Typically, in each testing environment, the same interleav -
ings, sometimes with minor variations, tend to be exercised
since the scheduler performs context switches at roughly th e
same program locations.
Systematic concurrency testing techniques [7, 14, 26, 20]
oﬀer a more promising solution to bug detection than stan-
dard load or stress testing. These techniques typically use
astateless model checking framework to systematically ex-
plore all possible thread interleavings with respect to a gi ven
test input. The model checking is stateless in that it directly
searches over the space of feasible thread schedules, and in
doing so, avoids storing the concrete program states (chara c-
terized as combinations of values of the program variables) ;
this is in sharp contrast to classic software model checkers
(e.g. [9, 8, 3]), which search over the concrete state space– a
well known cause of memory blowup.
In systematic concurrency testing, the model checker is of-
ten implemented by using a specialized scheduler process to
monitor, as well as control, the execution order of statemen ts
of the program under test. A program state sis represented
implicitly by the sequence of events that leads the program
from the initial state to s. This is based on the assump-
tion that, in a program where interleaving is the only source
of nondeterminism, executing the same event sequence al-
ways leads to the same state. The state space exploration is
conducted implicitly by running the program in its real ex-
ecution environment again and again, but each time under
a diﬀerent thread schedule. Therefore, systematic concur-
rency testing can handle programs written in full-ﬂedged
programming languages such as C/C++ and Java.
Although systematic concurrency testing has advantages
over the common practice of load or stress testing (where
we are at the mercy of the OS/thread library in triggering
theright interleaving), it is based on a rather brute-force
exhaustive search. Although it has been shown to be very
eﬀective in unit level testing (e.g. [14]), because of the of ten
large number of interleavings, such brute-force exhaustiv e
search is practically infeasible for realistic applicatio ns at a
larger scale. More speciﬁcally, its exhaustive search tend s to
cover all possible interleavings (w.r.t. a given test input ) in aPermission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
ICSE ’11, May 21–28, 2011, Waikiki, Honolulu, HI, USA
Copyright 2011 ACM 978-1-4503-0445-0/11/05 ...$10.00
221
pre-determined order, without favoring one interleaving o ver
another or considering the characteristics of the programs or
properties to be tested.
Although there exist techniques to reduce the cost of ex-
haustive search in stateless model checking, such as dynami c
partial order reduction (DPOR [5]) and preemptive context
bounding (PCB [14]), they are not eﬀective for large pro-
grams. For example, DPOR groups interleavings into equiv-
alence classes and tests one representative from each equiv a-
lence class. It is a sound reduction in that it will not miss an y
bug. However, in practice many equivalence classes them-
selves are redundant since they correspond to essentially t he
same concurrency scenarios. Therefore exhaustively testi ng
them not only is expensive, but also rarely pays oﬀ.
We propose a coverage-guided selective search , where we
continuously learn the ordering constraints over shared ob -
ject accesses in the hope of capturing the already tested
concurrency scenarios; then we use the learned information
to guide the selection of interleavings to cover the unteste d
scenarios. Since in practice, programmers often make, but
sometimes fail to enforce, implicit assumptions regarding
concurrency control, e.g. certain blocks are intended to be
mutually exclusive, certain blocks are intended to be atomi c,
and certain operations are intended to be executed in a spe-
ciﬁc order. Concurrency related program failures are often
the result of such implicit assumptions being broken, e.g.
data races, atomicity violations, order violations, etc. W e
try to infer such assumptions dynamically from the already
tested interleavings, and use them to identify high-risk in-
terleavings, i.e. interleavings that can break some of the
learned assumptions.
Although the programmer’s intent may come from many
sources, e.g. formal design documents and source code an-
notation, they are often diﬃcult to get in practice. For ex-
ample, asking programmers to annotate code or write doc-
uments in a certain manner is often perceived as too much
of a burden. The more viable approach seems to be to infer
them automatically. Fortunately, the very fact that stress
tests are less eﬀective in triggering bug-manifesting inte r-
leavings also implies that it is viable to dynamically learn
the ordering constraints. The reason is that, if no program
failure occurs during stress tests, one can assume that the
tested interleavings are good–they satisfy the programmer ’s
implicit assumptions [12, 27]. In addition, if the program
source code is available, the assumptions may also be mined
from the code (e.g. [25]).
In our coverage-guided selective search framework, we use
a metric called History-aware Predecessor-Set (HaPSet) to
capture the ordering constraints over the frequently occur -
ring (and non-erroneous) interleavings. HaPSets can cap-
ture common characteristics of a relatively large set of in-
terleavings. During systematic testing, we use HaPSets as
guidance to reduce the testing cost. Assuming that it is
not practical to cover all possible interleavings, we choos e
to execute only those interleavings that are not yet covered
by HaPSets. During systematic testing, we also update the
HaPSets by continuously learning from the good interleav-
ings generated in this process, until there are no more inter -
leavings to explore or the desired bug coverage is achieved.
We have implemented the proposed techniques in a sys-
tematic testing tool called Fusion, which is designed for te st-
ing multithreaded C/C++ programs using Linux/POSIX
threads ( PThreads ). Using some public domain concurrent
applications as benchmarks, we show that by using HaPSetsas guidance in systematic concurrency testing, we can sig-
niﬁcantly reduce the testing cost, while still maintaining the
capability of detecting most of the concurrency bugs in prac -
tice. More speciﬁcally, in our preliminary experiments, th e
new selective search algorithm found all the bugs, and at
the same time was often orders-of-magnitude faster than ex-
haustive search .
2. PRELIMINARIES
2.1 Concurrent Programs
We consider a concurrent program with a ﬁnite number of
threads as a state transition system. Threads may access lo-
cal variables in their own stacks, as well as global variable s
in a shared heap. Program statements that read and/or
write global variables are called (shared) memory-accessing
statements. Program statements that access synchroniza-
tion primitives are called synchronization statements. Pro-
gram statements that read and/or write only local variables
are called local statements. For ease of presentation we as-
sume that there is only one statement per source code line.
LetStmt be the set of all statements in the program. Then
each st∈Stmt corresponds to a unique pair of source code
ﬁle name and line number.
A statement stmay be executed multiple times, e.g., when
it is inside a loop or a subroutine, or when stis executed
in more than one thread. Each execution instance of stis
called an event . Let ebe an event and let stmt (e) denote the
statement generating e. An event is represented as a tuple
(tid, type, var ), where tidis the thread index, type is the
event type, and varis a shared variable or synchronization
object. An event is of one of the following forms.
1. (tid, read, var ) is a read from shared variable var;
2. (tid, write, var ) is a write to shared variable var;
3. (tid, fork, var ) creates the child thread var;
4. (tid, join, var ) joins back the child thread var;
5. (tid, lock, var ) acquires the lock variable var;
6. (tid, unlock, var ) releases the lock variable var;
7. (tid, wait, var ) waits on condition variable var;
8. (tid, notify, var ) wakes up an event waiting on var;
9. (tid, notifyall, var ) wakes up all events waiting on var.
In addition, we use the generic event ( tid, access, var ) to
capture all other shared resource accesses that cannot be
classiﬁed as any of the above types, e.g. accesses to a socket .
We do not monitor thread-local statements.
2.2 The State Space
We useSto denote the set of program states. A transition
Se→S advances the program from one state to a successor
state by executing an event e. An event is enabled in state s
if it is allowed to execute according to the program seman-
tics. We use se→s′to denote that event eis enabled in
s, and state s′is the next state. Two events e1, e2may be
co-enabled if there exists a state sin which both of them are
enabled. For programs using PThreads (or Java threads), a
thread may be disabled due to three reasons: (i) executing222lock(var) when varis held by another thread; (ii) execut-
ingwait(var) when varhas not been notiﬁed by another
thread; (iii) executing join(var) when thread varhas not
terminated.
An execution ρ(interleaving) is a sequence s0, . . . , s nof
states such that for all 1 ≤i≤n, there exists a transi-
tion si−1ei→si. During systematic concurrency testing, ρis
stored in a search stack S. We call s∈San abstract state,
because unlike a concrete program state, sdoes not store
the actual valuation of all program variables. (However,
scontains concrete memory addresses in order to identify
events accessing shared memory locations.) Instead, each s
is implicitly represented by the sequence of executed event s
leading the program from the initial state s0tos. This
is based on the assumption that executing the same event
sequence leads to the same state.
Two concurrent transitions are (conﬂict) independent if
and only if the two events can neither disable nor enable
each other, and swapping their order of execution does not
change the combined eﬀect. For example, two events are
(conﬂict) dependent if they access the same the object and
at least one is a write (modiﬁcation); and a lock acquire
is (conﬂict) dependent with another lock acquire over the
same lock variable. In the partial order reduction litera-
ture (e.g. [6, 24, 11]), two interleavings are considered as
equivalent iﬀ they can be transformed into each other by re-
peatedly swapping the adjacent and (conﬂict) independent
transitions.
2.3 Predecessor Sets
An execution ρ=s0. . . s ndeﬁnes a total order over the
set of memory-accessing and synchronization events. The
predecessor set (PSet [27]) was designed to eﬃciently cap-
ture the event ordering constraints common to a potentially
large set of executions. To this end, PSet was deﬁned over
memory-accessing statements, i.e. between read/write op-
erations that may be executed adjacent to each other. Syn-
chronization statements were ignored (this did not suit our
purpose; we extended it in our deﬁnition of HaPSet).
Definition 1 (PSet). Given a set{ρ1, . . . , ρ n}of in-
terleavings and a memory-accessing statement st∈Stmt .
The predecessor set, denoted PSet[st], is a set{st1, . . . , st k}
of statements such that, for all i: 1≤i≤k, an event
produced by statement stis immediately dependent upon an
event produced by statement stiin some interleaving ρj,
where 1≤j≤n.
Event eisimmediately dependent upon eiif and only if ei
is the last event in its thread accessing the same object as
t, and it is also dependent with e. Both stand stiare
statements rather than events, which are distinct executio n
instances of these statements. This is meant to keep PSets
general enough so that the PSets learned from one correct
interleaving remain valid for another correct interleavin g,
possibly under a diﬀerent program input.
PSets are powerful enough to characterize the common
bug patterns such as data races, atomicity violations, and
order violation. Since each PSet[st] has at most|Stmt|el-
ements, the space to store all PSets is O(|Stmt|2) in the
worst case. On average, however, the PSets learned from
real-world programs are usually very small. The study in
[27] showed that 95% of the program statements have an
empty PSet, and for well-designed test suites, typically ittakes less than 100 interleavings for PSet learning to con-
verge, i.e. few new updates are possible afterward.
3. ORDERING CONSTRAINTS
Although PSets are eﬃcient in capturing ordering con-
straints common to a large set of thread interleavings, as a
coverage metric it does not suit our purpose well. This is
because the applications are diﬀerent. In [27], PSets are co l-
lected from good runs during testing and then treated as pro-
gram invariants during production runs. A special-purpose
microprocessor is designed to ensure that the PSets are al-
ways obeyed (with checkpoints and rollbacks upon PSet vi-
olations). The rationale is that, if PSets capture the concu r-
rency scenarios of the tested interleavings, then by allowi ng
only PSet-obeying interleavings in production runs, one ca n
steer away from program failures even if the programs are
still buggy.
In this paper, our goal is not runtime failure avoidance
as in [27] but to improve the coverage during testing. The
main diﬀerence is that, for failure avoidance, it is accept-
able if some already tested interleavings are not captured
by the PSets (as long as they are rare, disallowing them in
production runs will not hurt performance much). However,
for testing, it is crucial to capture what has already been
tested, since the purpose is to prevent the same concurrency
scenario from being tested again.
3.1 History-aware Predecessor Sets
We extend the idea of PSet to deﬁne a new coverage met-
ric called HaPSet1. There are two main diﬀerences between
HaPSets and PSets. First, we consider both synchronization
statements (e.g. lock acquires) as well as memory-accessin g
statements in the deﬁnition of HaPSet. Second, for each
st∈Stmt , in addition to the ﬁelds fileandline, we include
thrandctx, where thris the thread that executes standctx
is the call stack at the time stis executed. The reason is as
follows: With ( file, line ), there remains some degree of am-
biguity regarding the statement which produces an event at
run time. For example, the same statement may be executed
in multiple function/method call contexts, or from multipl e
threads. In many cases, especially in object-oriented pro-
grams, such information is useful and should be included in
order to capture any meaningful ordering constraint.
Since at run time, both the number of threads and the
number of distinct calling contexts can be large, to avoid
memory blowup, ctxonly stores the most recent k (some
small number–5 in our experiments) entries in the call stack ,
andthronly takes two values: 0 means it is the local thread,
and 1 means it is the remote thread. Let eand e′be
two events in an interleaving such that stmt (e) = stand
stmt (e′) = st′, we have st.thr = 0 and st′.thr = 1 when
tid(e)< tid (e′), and st.thr = 1 and st′.thr = 0 when
tid(e)> tid (e′). We do not consider tid(e) =tid(e′), since
it never triggers the HaPSet update. Formally, statement
stis now deﬁned as a tuple ( file, line, thr, ctx ), where file
is the ﬁle name, lineis the line number, thr∈{0,1}is the
thread, and ctxis the truncated calling context.
Definition 2 (HaPSet). Given a set{ρ1, . . . , ρ n}of
interleavings and a shared memory-accessing or synchroniz a-
tion statement st∈Stmt . The History-aware Predecessor
Set, or HaPSet [st], is a set{st1, . . . , st k}of statements such
1pronounced as “Happy Set.”223Thread T1 Thread T2
{//alloc
e1:p:= & a;
}
{//use
e2: if (p/ne}ationslash= 0)
e3:∗(p) := 10
}
{//free
e4:p:= 0;
}e4:W(p)e2:R(p)e1:W(p)
e3:R(p)
e3:W(a)
Figure 1: Serial execution of intended atomic ( e2e3).
that, for all i: 1≤i≤k, an event eproduced by stis imme-
diately dependent upon an event eiproduced by stiin some
interleaving ρj, where 1≤j≤n.
Note that this metric includes both syntactic and seman-
tic elements. Data conﬂicts are at the heart of most concur-
rency errors (data races, atomicity violations, etc.)–the se
are tracked to make this metric relevant for the purpose of
ﬁnding bugs. However, a generalization is achieved by as-
sociating it syntactically with statements, rather than wi th
events. The thread index is again designed to distinguish
between two threads for catching bugs, but abstracts over
speciﬁc thread ids, thereby ensuring that it is scalable ove r
many threads. Finally, by including a bounded functional
context, we provide some measure of context-sensitivity–t his
is especially useful for object-oriented programs.
Example. Consider Figure 1, which has two threads T1, T2
sharing the pointer p. Assume that p= 0 initially. In the
given execution, pis ﬁrst initialized in e1, then used in e2, e3,
and ﬁnally freed in e4. (We assume e1−e4are statements
in the form ( file, line, thr, ctx ).) Since e1is the last state-
ment before e2and they have a data conﬂict, we add e1
toHaPSet [e2]. For e3we do not add any statement into
HaPSet [e3] because e2is the last statement accessing pbut
it is from the same thread (hence no conﬂict). We add e3to
HaPSet [e4] since e3precedes e4in the given execution, and
they have a data conﬂict. To sum up, the HaPSets learned
from this execution are as follows,
HaPSet [e1] ={},HaPSet [e2] ={e1},
HaPSet [e3] ={},HaPSet [e4] ={e3}.
3.2 Why HaPSets are Useful?
We show that the seemingly simple HaPSets are capable
of capture subtle concurrency control patterns.
3.2.1 Atomicity Violation
Consider Figure 1 again, where the block containing e2, e3
is meant to be executed atomically–it ﬁrst conﬁrms that
pointer pis not null and then stores 10 to the pointed mem-
ory location. Therefore, whether e2ande3are two consec-
utive reads of an interleaving is key to deciding whether the
interleaving is buggy. HaPSets can capture this atomicity
constraint: in all good runs where atomicity is not violated ,
HaPSet [e3] is always empty. This is because, although e1, e4
can be executed either before e2or after e3, event e3is al-
ways preceded by e2. Therefore, neither e1nore4can appear
inHaPSet [e3]. Second, e2/ne}ationslash∈HaPSet [e4] because e3(insteadThread T1
a do {
b lock(A)
c tmp = x;
d unlock(A)
e}while(tmp);Thread T2
f lock(A)
gx= 1;
h unlock(A)execution
c1: R(x)
...
c2: R(x)
...
g: W(x)
cn: R(x)
Figure 2: The busy waiting example. Without
HaPSet, there will be excessive backtracking.
ofe2) always precedes e4. Therefore the HaPSets leaned
from all the good runs are as follows,
HaPSet [e1] ={e2},HaPSet [e2] ={e1, e4},
HaPSet [e3] ={},HaPSet [e4] ={e3}.
During testing, it is more fruitful to test interleavings th at
have not been covered by the above HaPSets. One such
interleaving is ρ′=e1e2e4e3, which violates the atomicity
and leads to the dereference of a null pointer. Note that ρ′
corresponds to HaPSet [e3] ={e4}andHaPSet [e4] ={e2}.
3.2.2 Busy Waiting
HaPSets can be used to avoid the excessive testing of cer-
tain interleavings that do not oﬀer any new concurrency
scenario. Consider Figure 2 as an example. There are two
threads T1, T2communicating via variable x. Assume that
x= 0 initially. In the given execution {abcde}kfghabcde ,
the loop in T1is executed ktimes before gin thread T2is
executed.
Without using HaPSets, systematic testing would have to
test a potentially large set of interleavings, each with a di f-
ferent number of loop iterations. This is because, strictly
speaking, none of these interleavings are equivalent to oth -
ers; therefore, based on the theory of partial order reducti on,
one needs to test all of them. However, such tests are of-
ten wasteful since they rarely lead to additional bugs. The
HaPSets computed on these interleavings are
HaPSet [g] ={c},HaPSet [c] ={g},
HaPSet [b] ={f},HaPSet [f] ={b}.
This is because some instances of statement c(orf) are
immediately dependent on instances of g(orb), and vice
versa. (Except for recursive locks, we ignore unlock state-
ments when computing HaPSets.) When using HaPSets as
guidance, we can avoid the aforementioned excessive back-
tracking because none of these interleavings can oﬀer a con-
currency scenario that has not been covered by the HaPSets.
3.3 Learning from Good Runs
For our guided search to be eﬀective, we need to learn
HaPSets from a diversiﬁed set of interleavings. The quality
of the learned HaPSets will be aﬀected by both the test cases
and the thread schedules. Randomized delay can be added
to the scheduler to diversify the thread interleavings. In t his
testing environment, the program is executed under the con-
trol of a scheduler process, which is capable of controlling
the order of operations from diﬀerent threads. These contro l
points are inserted into the program source code automat-
ically via an instrumentation phase, before the source code
is compiled into an executable.
For HaPSet learning, we maintain the following data struc-
tures: a set HaPSet [st] for each statement st∈Stmt ; and224a search stack Sof abstract states s0. . . s n, where s0is the
initial state and snis the ﬁnal state of the interleaving. Re-
call that each s∈Sis an abstract state because sdoes not
store the actual valuations of program variables. Let si.sel
be the event executed at siin the given interleaving in order
to reach si+1.
The pseudo code of our HaPSet learning is presented in
Algorithm 1. The procedure randCTest takes the initial
state s0as input and generates the ﬁrst interleaving with a
randomized thread schedule. Each state s∈Sis associated
with a set s.enabled of events. Recall, for example, that
a lock acquire would be considered as disabled at s, if the
lock is held by another thread. Similarly, a wait would be
considered as disabled at s, if the notiﬁcation has not been
sent. At each execution step, we randomly pick an event
e∈s.enabled , execute it from s, which leads to state s′.
Note that the thread schedules ultimately are still deter-
mined by the underlying operating system. This ensures
that all the generated interleavings are real. If any of them
can trigger a program failure, then it is a real bug. Other-
wise, all of them are assumed to be good runs, in that they
expose the desired program behavior.
Algorithm 1 Learning from good test runs
1: Initially: For all statements st,HaPSet [st] is empty;
2: Sis an empty stack; randCTest (s0)
3:randCTest (s){
4: S.push( s);
5: LearnHaPSets (s); // learning HaPSets
6: while (s.enabled is not empty){
7: Let ebe a randomly chosen item from s.enabled ;
8: //Delay thread tid(e) for a random period;
9: Let s.sel =e;
10: Let s′be the new state after executing se→s′;
11: randCTest (s′);
12:}
13: S.pop( s);
14:}
15:LearnHaPSets (s){
16: if(s/ne}ationslash=s0) ){
17: Let sp∈Sbe the state preceding s;
18: Traverse stack S, for each thread, ﬁnd the last state
sd. where sd.selandsp.selaccess the same object;
19: if(sd.selandsp.selhave a data conﬂict) {
20: Let stp=stmt (sp.sel);
21: Let std=stmt (sd.sel);
22: HaPSet [stp]←HaPSet [stp]∪{std}
23:}
24:}
25:}
During each run, we invoke learnHaPSets at every exe-
cution step. The input to this procedure is the newly reached
state s. Let spbe the state prior to reaching the current
state s, and sp.selbe the event executed between spands.
For each thread, we ﬁnd the last executed event sd.selsuch
that (1) sd.seland sp.selaccess the same object, (2) they
are executed by diﬀerent threads, and (3) there is a data
conﬂict (read-write, write-write, lock-lock, or wait-not ify).
If such an sd.selexists, we add the statement stmt (sd.sel)
into the HaPSet of stmt (sp.sel).4. SYSTEMATIC TESTING
As we mentioned earlier, systematically testing all pos-
sible interleavings can be achieved using stateless model
checking. It can be viewed as a natural extension of rand-
CTest in Algorithm 1. However the scheduler here has total
control in deciding the thread schedule.
4.1 Overall Algorithm
The overall algorithm is illustrated in Algorithm 2 by pro-
cedure sysCTest . It checks all possible thread schedules of
the program for a given test input.
Algorithm 2 Systematic concurrency testing framework
1: Initially: Sis an empty stack; sysCTest (s0)
2:sysCTest (s){
3: S.push( s);
4: UpdateBacktrack (s);
5: let τ∈T idsuch that∃t∈s.enabled :tid(t) =τ;
6: s.backtrack←{τ};
7: s.done←∅;
8: while (∃t:tid(t)∈s.backtrack andt/ne}ationslash∈s.done ){
9: s.done←s.done∪{t};
10: let s.sel =t;
11: let s′be the new state after executing st→s′;
12: sysCTest (s′);
13:}
14: S.pop( );
15:}
16:UpdateBacktrack (s){
17: for each t∈s.enabled{
18: let sd∈Sandsd.selbe the latest event such that
sd.selis dependent and may be co-enabled with t,
19: if(such sdexists){
20: sd.backtrack←sd.backtrack∪BtSet (sd, t)
21:}
22:}
23:}
In addition to s.enabled , each shas an associated sub-
sets.done⊆s.enabled of events, recording the scheduling
choices made at sin some previous test runs. Furthermore,
each shas an associated set s.backtrack consisting of a sub-
set of the enabled threads at s. Each τ∈s.backtrack rep-
resents a future scheduling choice at s, i.e. thread τwill be
executed at sin some future test run.
The procedure sysCTest takes state sas input, where s0
is used for the initial call. At each step, it ﬁrst invokes sub -
routine updateBacktrack to update backtracking points
at some previous state s′∈S. (Backtracking will be ex-
plained in the next paragraph.) Then from s.backtrack it
picks an enabled thread τto execute, leading to a distinct
thread interleaving. The recursive call at Line 11 returns
only after the interleaving ends and we have backtracked
to state s. At this point, s.backtrack must have been up-
dated by some previous call to sysCTest ; it may contain
some threads other than τ, meaning that executing them (as
opposed to τ) from state smay lead to diﬀerent interleav-
ings. The entire procedure terminates when we backtrack
from state s0eventually. Since we do not store the concrete
program states in S, backtracking to a state s′is imple-
mented by re-starting the test run and then applying the225same thread schedule till state s′is reached again.
In the naive approach, at every state s∈S,s.backtrack
consists of all the enabled threads. The set of interleaving s
generated by this naive algorithm is the same as the set
of possible interleavings generated by the actual program
execution. However, the naive approach may end up test-
ing many redundant interleavings. updateBacktrack (s)
is designed to remove some of the redundant interleavings.
It takes the current state as input and iterates through all
the enabled event t∈s.enabled to ﬁnd the latest event sd.sel
that is dependent and may be co-enabled with t. If such an
sdexists, it means that if we ﬂip the execution order from
sd.sel . . . t tot . . . s d.sel, the new interleaving will not be
equivalent to the current one. In practice, the various sys-
tematic concurrency testing tools diﬀer mainly in their way s
of computing the backtrack set.
4.2 Backtracking: Baseline and Variations
The baseline algorithm is only slightly diﬀerent from the
naive algorithm. That is,
BT Set←{tid(q)|q∈sd.enabled}
It is still more eﬃcient than the naive algorithm, since it
adds BTSet only at state sd(as opposed to every state).
For example, consider the case where sddoes not exist in
Line 18. In this case, tis independent with all the previously
executed events ( sd.selfor all sd∈S), and swapping the
execution order of tand sd.selwould not lead to a new
equivalence class. The baseline algorithm would not add
any backtrack point for such cases.
4.2.1 Preemptive Context-Bounding (PCB)
Traditionally, a context switch is deﬁned as the comput-
ing process of storing and restoring the CPU state (context)
when executing a concurrent program, such that multiple
threads can share the same CPU resource. The idea of using
context bounding to reduce complexity of software veriﬁca-
tion was ﬁrst introduced for static analysis [16] and later f or
testing [14]. It has since become an inﬂuential techniques
since in practice many concurrency bugs can be exposed by
interleavings with few context switches. In this setting,
BT Set←{tid(q)|q∈sd.enabled, andcb(sd, q)≤mcb}
where cb(sd, q) is the number of context switches after exe-
cuting qatsd, and mcb is the maximal number of context
switches allowed. From state sd, one can execute qonly if the
number of context switches will not exceed the bound. Al-
though PCB can skip many interleavings, for the ones with
≤mcb context switches, we still need exhaustive search.
For large programs, even with small bound (e.g. 4 or 5), the
number of interleavings is still extremely large.
4.2.2 Dynamic Partial Order Reduction (DPOR)
Partial order reduction is based grouping interleavings
into equivalence classes and then testing only one represen -
tative from each equivalence class. It is a well studied topi c
in model checking. For concurrency testing, the most ad-
vanced technique is the DPOR algorithm by Flanagan and
Godefroid [5]. BTSet is computed by Algorithm 3. First,
we search for an event q∈sd.enabled such that there ex-
ists a happens-before relation between qand the currently
enabled event t. Intuitively, qhappens before tin an in-
terleaving if either (a) we cannot execute tbefore qdue to
program semantics, or (b) swapping the execution order of qandtwould lead to a diﬀerent equivalence class. Obviously
qhappens before tif they are from the same thread. Other
examples include (1) qandtare from diﬀerent threads but
have data conﬂict over a shared object; and (2) there exist
events r, sin the interleaving such that, qhappens before r,
rhappens before s, and shappens before t. The happens-
before relation is transitive (cf. [5]).
Algorithm 3 Computing the backtrack set in DPOR.
1: let q∈sd.enabled such that either tid(q) = tid(t), or
there is a happens-before relation between qandt}
2:if(such qexists)
3: BTSet←{tid(q)};
4:else
5: BTSet←{tid(q)|q∈sd.enabled};
If such qexists, then we have a reduction–we only need
to add tid(q) tosd.backtrack , since executing thread tid(q)
is necessary for the purpose of swapping tand sd.sel. (In
POR theory, this backtrack set is called a persistent set .)
Otherwise, we do not have reduction and have to resort
to the baseline to add all enabled threads to sd.backtrack .
Although partial order reduction is sound in that it never
misses real bugs, in practice, the number of interleavings
after DPOR can still be very large.
5. GUIDING SYSTEMATIC TESTING
Our algorithm builds on Algorithm 2. In contrast to
the exhaustive search in DPOR and PCB, we use HaPSets
learned from the already tested (good) runs to the selection
of the next interleaving. We will explain ﬁrst how to use
HaPSets to select interleavings, and then how to continu-
ously update the HaPSets.
5.1 Guiding Interleaving Selection
We achieve this by modifying the implementation of sub-
routine updateBacktrack . Recall that in Algorithm 2,
Line 18 of updateBacktrack searches through the stack
Sto ﬁnd the last event sd.selthat is dependent and may
be co-enabled with t. If such an sd.selexists, it means that
swapping the execution order from sd.sel . . . t tot . . . s d.sel
would produce a diﬀerent interleaving. In the modiﬁed ver-
sion, we insist that in addition to the condition in Line 18,
the following HaPSet related condition must hold: stmt (t)/ne}ationslash∈
HaPSet [stmt (sd.sel)].
Note that if stmt (t) is not in the HaPSet of stmt (sd.sel),
it means that in all tested runs, the statement that gener-
ates sd.selhas never been immediately dependent upon the
statement that generates t. In this case, the new execution
order t . . . s d.selrepresents a concurrency scenario that has
never been covered by the previous test runs. On the other
hand, if stmt (t) is already in the HaPSet of stmt (sd.sel),
the new interleaving would have a lower risk because this
concurrency scenario has been covered previously.
Algorithm 4 illustrates our new procedure UpdateBack-
track for HaPSet guided selective search. One of the main
advantages of our HaPSet guided search is that, it ﬁts natu-
rally into the existing ﬂow of systematic testing. The addi-
tion of HaPSet guided search requires only small changes to
the software architecture. The guidance from HaPSets aﬀect
only our selection of state sd(Line 4). Once sdis selected,
the backtrack set can be computed independently. This
means we can choose to use the various existing methods to226Algorithm 4 Guiding the systematic testing (with DPOR)
1:UpdateBacktrack (s){
2: for each t∈s.enabled{
3: let sd∈Sandsd.selbe the latest event such that
(1)sd.selis dependent and may be co-enabled with
t,
(2)stmt (t)/ne}ationslash∈HaPSet [stmt (sd.sel)]; // guiding
4: if(such sdexists){
5: sd.backtrack←sd.backtrack∪BtSet (sd, t)
6:}
7:}
8:}
compute BTSet . In practice, we have found that both PCB
and DPOR work well under the guidance of HaPSets, al-
though combining HaPSet with DPOR often performs slightly
better. Note that HaPSet guidance eﬀectively prunes away
large subspaces in the search. Unlike DPOR, this pruning
is not safe, i.e. it may miss errors. This is the basic tradeoﬀ
we make to gain scalability and performance.
At this point, one may contemplate the possibility of us-
ing HaPSets with both DPOR and PCB. We caution that
there is a theoretical diﬃculty in soundly combining PCB
with DPOR (or any persistent-set based POR) in the ﬁrst
place. PCB and DPOR fundamentally are not compatible,
because if you use both, and also set the context bound is
k, some equivalence classes may be missed completely even
if they actually contain some interleavings with CB≤k. In
[13], Musuvathi and Qadeer designed a method to combine
a sleep-set based POR with context bounding (and their
method is quite involved), but to our knowledge there has
been no method for soundly combining persistent-set based
POR (such as DPOR) with context bounding.
5.2 Continuous Learning
In our guided search framework, the quality of HaPSets is
very important. Although we can diversify thread schedules
via randomization, the training runs may still miss many
concurrency scenarios. The interleaving encountered duri ng
the guided search may contain these missing concurrency
scenarios, and therefore are complementary to the initial
learning. Therefore, we update the initial HaPSets during
systematic testing by continuously learning from the tested
(good) interleavings. Continuous learning is made possibl e
by the fact that, unless a bug is detected, the interleaving
checked by systematic testing is always a good run.
Algorithm 5 illustrates the overall selective search algo-
rithm, wherein the call to learnHaPSets at Line 4 allows
for continuous learning of HaPSets. The learning subroutin e
is the same as the one used in Algorithm 1.
The nice thing about continuous learning is that, the good
interleavings produced by systematic testing are freely av ail-
able, since they are byproducts of the search. The more
concurrency scenarios we capture using the HaPSets, the
less number of interleavings would need to be tested in the
future. This ensures progress with respect to the HaPSet
coverage metric. Therefore, on-the-ﬂy updating HaPSets al -
lows the guided search to become a self-improving process,
making the whole process converge much faster.
Example. Consider Figure 2 again. Assume that the ﬁrst
interleaving is ρ1=s0a→s1f→s2g→. . . s 5b→s6c→. . ..Algorithm 5 Continuous learning within systematic testing
1: Initially: Sis an empty stack; guidedCTest (s0)
2:guidedCTest (s){
3: S.push( s);
4: LearnHaPSets (s); // continuous learning
5: UpdateBacktrack (s);
6: let τ∈T idsuch that∃t∈s.enabled :tid(t) =τ;
7: s.backtrack←{τ};
8: s.done←∅;
9: while (∃t:tid(t)∈s.backtrack andt/ne}ationslash∈s.done ){
10: s.done←s.done∪{t};
11: let s′be the new state after executing st→s′;
12: guidedCTest (s′);
13:}
14: S.pop( );
15:}
The HaPSets computed from ρ1via continuous learning
areHaPSet [c] ={g},HaPSet [b] ={f}. Furthermore, the
DPOR backtrack sets will be s1.backtrack ={1,2}and
s2.backtrack ={2}, since thread 1 is disabled at state s2.
According to our guided search algorithm, the next inter-
leaving to be executed is ρ2=s0a→s1b→. . .. The new
HaPSets computed from ρ2areHaPSet [g] ={c},HaPSet [f] =
{b}. After that, however, our guided search algorithm will
allow no other interleavings.
A key point illustrated by the above example is that prun-
ing actually happens at states like s1where locking state-
ments are executed, not when memory-accessing statements
(c, g) are executed. This is why we need to include syn-
chronizations in the deﬁnition of HaPSet. In fact, if we use
only memory-accessing statements (as in the deﬁnition of
PSet [27]), there will be no pruning possible for Figure 2.
6. EXPERIMENTS
We have implemented the proposed method in a tool called
Fusion . The tool is capable of testing multithreaded C/C++
programs in Linux written using the POSIX thread library.
We use source code instrumentation to add the monitoring
and control points to the program, in order to control the
memory-accessing and synchronization statements at run
time. Our implementation is based on the C/C++ front-end
from Edison Design Group. The instrumentation consists of
two steps: (1) before each shared memory access, it also
inserts a request to the scheduler asking for permission to
execute; (2) before each PThreads library routine, it inserts
a request to the scheduler. Since identifying a priori the set
of memory locations that may be shared by more than one
thread is diﬃcult (due to pointers and heap allocated data
structures), we use a light-weight intra-procedural escap e
analysis to conservatively decide whether a statement may
access the shared memory. This is a sound approximation
because treating a local statement as if it is shared poses
no threat to the correctness of our testing tool–it merely
increases the monitoring/control overhead at runtime.
In addition, system calls that may block the calling thread
need to be monitored. This includes, for example, socket
communication routines (e.g. select, send, recv ) and sys-
tem calls that use the real-time information (e.g. usleep(),
pthread_cond_timedwait() ). Such system calls are properly227Table 1: Comparison of HaPSet, DPOR, and PCB with various bou nds on the thrift-lib-cpp example.
Test Program HaPSet DPOR PCB0 PCB1 PCB2 PCB3
name LoC thrds bug type runs time(s) runs time(s) runs time(s) runs time(s) runs time(s) runs time(s)
thrift-lib-w2-5t 18.5k 3 deadlk 14 27.8 23 18.6 512(no) 247.2 26 29.2 215 146.9 871 TO
thrift-lib-w3-5t 18.5k 4 deadlk 18 27.5 733 TO 1301 TO 399 229.7 876 TO 742 TO
thrift-lib-w4-5t 18.5k 5 deadlk 22 33.7 665 TO 1111 TO 980 TO 677 TO 639 TO
thrift-lib-w5-5t 18.5k 6 deadlk 25 38.1 572 TO 899 TO 670 TO 582 TO 573 TO
modeled by the scheduler of out testing tool.
We conducted experiments on some real-world C/C++
applications written for the Linux/PThreads platform. All
benchmarks are from the public domain, accompanied by
test cases to facilitate concrete execution. Our experimen ts
were conducted on a workstation with 2.8 GHz Pentium D
processor and 2GB memory. We have compared the runtime
performance as well as the bug-detecting capability of the
following methods: HaPSet, DPOR, and PCB. Here HaPSet
is our guided search algorithm. DPOR is the original algo-
rithm in [5]. For PCB [14] we set the context bounds to
1,2,3,... For a fair comparison of these methods, we skipped
a priori HaPSet learning sessions, while relying solely on
continuous learning to infer the HaPSets.
6.1 The Thrift C++ Library
Our ﬁrst set of benchmarks come from the Thrift C++
library, part of a software framework used by Facebook for
cross-language services development. The library has 18.5 K
lines of C++ code. We used the version from the current
main development trunk. The test program is also from
the main trunk as part of the make check script. There is
a (known) deadlock error inside the concurrency package,
which itself is a thin C++ layer wrapping up PThreads mu-
tex and condition variable routines to support thread pool
and task management.
The original test program was written for stress tests. It
upfront creates hundreds of worker threads and tens of thou-
sands of tasks to run in parallel. This is a typical way of cre-
ating a heavy workload, hoping to increase the odds of trig-
gering some rare and bug-manifesting interleavings. With
systematic testing, we do not need that many threads/tasks
to expose bugs. Therefore, we set the number of threads
from 2 to 5, and with on average 5 tasks per threads.
We compared the performance of the three methods. The
results are shown in Table 1. The ﬁrst four columns show
the name, the lines of code, the number of threads, and the
bug type. Here thrift-lib-w2-5t , for example, stands for the
test case with 2 worker threads and 5 tasks per worker. The
remaining columns show the performance of each method,
including the number of interleavings tested and the run
time in seconds. We set the time bound to 10 minutes per
method, i.e. TO in the table means timed out in 600 seconds
without ﬁnding a bug.
The results show that HaPSet found all the bugs and was
also fast. Furthermore, it scaled well as we increased the
number of concurrent threads. In comparison, DPOR found
a bug for the 2-worker case, while timed out for the other
cases. For PCB0 (with mcb = 0), it terminated in 247.2
seconds and missed the bug. (Our experience shows that in
general PCB0 is not eﬀective since it frequently misses real
bugs.) With context bound set to 1, PCB found the bugs
for the 2-worker and 3-work cases. However, PCB did not
scale as well when we increased the number of threads orthe context bound.
All three algorithms have signiﬁcant runtime overhead in
comparison to a native test execution. Depending on the
types of target programs, i.e. CPU-bound or communication-
bound, the slowdown ranges from 10X to 100X. This over-
head comes from two sources. First, in order to control
the nondeterminism in executing concurrent programs, the
scheduler insists that at any time, only one thread is al-
lowed to execute. This essentially serializes a concurrent
execution. Second, the monitoring and control of memory-
accessing events often have large overhead.
For thrift-lib-w2-5t , although HaPSet checked 14 runs,
it actually spent more time than what DPOR spent on
checking 23 runs. This is because not all these 14 runs are
included in the 23 runs; and each run may execute a dif-
ferent set of statements and therefore may take a diﬀerent
amount of time. Furthermore, both HaPSet learning and
guiding have some computational overhead.
6.2 The aget/pbzip/pfscan Benchmarks
Our second set of benchmarks are medium-size applica-
tions downloaded from the sourceforge.net . They include
aget-0.4 , a ftp client capability of concurrently download-
ing diﬀerent segments of a large ﬁle, pbzip2-0.9.4 , a parallel
implementation of bzip2 for ﬁle compression and decompres-
sion, and pfscan-1.0 , a concurrent ﬁle scanner that combines
the functionality of find, xargs and fgrep . First we com-
pared the performance of the three methods. The results
are shown in Table 2. For these examples, HaPSet found all
the bugs and was also the fastest, whereas both DPOR and
PCB2 timed out on pbz2-f and pfscan .
Table 2: Comparison of HaPSet, DPOR, and PCB2
on the aget/pbzip2/pfscan examples
Test Program HaPSet DPOR PCB2
name LoC bug thr runs time runs time runs time
aget 1.2k race 6 14 36.9 96 173 94 172
pbzip2 1.9k order 7 2 0.5 2 0.5 2 0.5
pbz2-f 1.9k race 7 8 2.6 608 TO 631 TO
pfscan 960 deadlk 3 28 2.8 1541 TO 2867 TO
Inaget, there is a data race over a variable called bwritten
which is shared by the multiple downloading worker threads
and a separate thread updating the progress bar. In pbzip2 ,
there is an order violation between the main thread and the
consumer threads, sometimes causing a segmentation fault
as a result of null pointer dereferencing. After ﬁxing this b ug
(pbz2-f ), our tool found a previously unreported data race
over variables OutputBuff[i].buf and OutputBuff[i].bufSize
between the consumer threads and the ﬁleWriter thread.
This is a real bug that may cause corrupted ﬁle output.228Inpfscan , there is an injected order violation [27], where a
variable called aworkers , if initialized too late in time, may
cause the main thread to hang.
On aget, we also compared the various settings of PCB
(with mcbfrom 0 to 4), to assess its scalability. The results
in Table 3 show that, PCB0 timed out after 10 minutes with-
out ﬁnding the bug. With all the other settings, PCB found
the bug. Although PCB1 has the best performance for this
example, we caution that in general one needs at least PCB2
since even the simplest atomicity violations need at least t wo
context switches to trigger–with PCB1, no program failure
caused by atomicity violation can be detected.
Table 3: PCB with various context bounds on aget
PCB0 PCB1 PCB2 PCB3 PCB4
runs time runs time runs time runs time runs time
286 TO 12 24.9 94 172 151 277 95 174
On aget, we also assessed the scalability of HaPSet by
gradually increasing the number of worker threads from 2
to 10. The results in Table 4 show that the number of in-
terleavings tested by HaPSet (before the bug is detected)
grows only modestly. This is mainly due to the abstrac-
tion over speciﬁc thread ids that we use in the deﬁnition of
HaPSets. This is in contrast to both DPOR and PCB, where
the number of interleavings typically grow exponentially a s
we increase the number of threads.
Table 4: HaPSet on various threads of aget
2 threads 3 threads 4 threads 5 threads 6 threads
runs time runs time runs time runs time runs time
3 3.1 7 10.0 12 21.4 14 36.9 16 54.2
7 threads 8 threads 9 threads 10 threads
runs time runs time runs time runs time
18 80.9 20 116.4 22 159.9 24 256.9
6.3 Extracted Mozilla/MySQL Bugs
Recall that DPOR is a sound reduction whereas both PCB
and HaPSet are unsound and in theory may miss bugs. Our
results in previous subsections show that the reduction by
HaPSet can be signiﬁcant. Therefore, a natural question is,
would it reduce too much to miss many bugs? extracted bug
samples. Each sample is a small program showcasing a bug
extracted from the real code of Mozilla and MySQL. These
examples were kindly provided by the authors of [27]. For
testing purposes, we inserted some arbitrary shared memory
accesses and locking statements to make them nontrivial.
Since the programs are small, the emphasis here is not on
comparing the runtime performance, since all three compet-
ing methods can ﬁnish quickly. Rather, we would like to
compare their bug-ﬁnding capability.
Table 5 shows the experimental results. The ﬁrst two
columns show the names and the bugs targeted by the test
case. Here atom means an atomicity violation, order means
an order violation, and deadlk means a deadlock. Eventu-
ally, the program failures caused by these atomicity/order
violations are either segmentation faults or corrupted dat a.
The next six columns compare the number of interleavings
and the run time (in seconds). HaPSet not only is fast butTable 5: Comparison on extracted (but real) bugs
Test Program HaPSet DPOR PCB2
name bug runs time runs time runs time
MysqlLog atom 5 0.3 22 1.2 12 0.7
NodeState order 5 0.3 22 1.5 12 0.8
Loadscript atom 5 1.0 79 6.0 27 2.1
SeekToItem atom 3 0.2 127 9.8 22 1.6
UpdateTimer atom 3 0.2 128 11.2 10 1.0
FileTransport deadlk 5 0.2 22 1.2 12 0.9
CreateThread order 5 0.2 22 1.1 12 0.6
ReadWriteProc order 5 1.9 175 12.0 20 2.8
OpenInputStr deadlk 5 0.3 1409 107.7 42 3.1
HttpConnect order 5 3.3 37 28.5 16 12.0
TimerThread deadlk 3 0.2 101 7.3 18 1.2
also ﬁnds all the bugs, despite that it skips most of the in-
terleavings explored by DPOR and PCB2. This provides
strong evidence supporting our claim that, in practice, the
drastic interleaving reduction achieved by HaPSet does not
cause systematic testing to miss many real bugs.
7. RELATED WORK
The notion of predecessor set was ﬁrst introduced by Yu
and Narayanasamy [27]. Their goal was runtime failure
avoidance, for which the PSets learned during testing were
encoded into the program’s executable and designed a specia l-
purpose microprocessor to ensure these PSet constraints du r-
ing the production runs. In contrast, our goal in testing is
to try to trigger previously untested concurrency scenario s.
To this end we have extended their idea to deﬁne the new
metric called HaPSet.
As we have already explained in previous sections, our
work is related to systematic testing techniques [7, 14, 26,
23] based on stateless model checking, but the interleaving
selection in these methods are not guided by any coverage
metric. Among classic model checkers, SPIN [9] and Java
PathFinder [8] are closely related, but they are based on
manipulating concrete program states rather than stateless
model checking.
CTrigger [15] and CalFuzzer [18] are two testing tools
that also use dynamically collected information. CTrigger is
based on the notion of access invariants, i.e. the atomicity of
two consecutive memory-accessing events, and CalFuzzer is
based on detecting potential data races. In comparison, the
HaPSets used in our method are more general since they can
characterize concurrency patterns that subsume data races
and three-access atomicity violations. More recently, Shi et
al.[19] proposed learning def-use invariants over correct exe -
cutions of both sequential and multithreaded programs and
then using such invariants to prune false positives and de-
tect bugs. However, HaPSets are signiﬁcantly diﬀerent from
def-use invariants in that they are deﬁned also for two con-
ﬂicting writes or for one write with a preceding read, and
the two involved accesses must come from diﬀerent threads.
More importantly, our interleaving selection is systemati c
and each test run is guaranteed to exercise a not-yet-tested
interleaving; whereas in the other methods [15, 18, 19], int er-
leaving selection is achieved by inserting sleep() statements
to certain program points, to increase the odds of triggerin g
certain interleavings. Therefore they typically do not hav e
a guarantee of progress.
Our method is also related to the various runtime error229detection algorithms, e.g. [17, 2] and [10, 21, 22]. These
methods focus on analyzing a given interleaving to either de -
tect bugs, or predict bugs in some other related interleavin gs.
These methods are orthogonal to ours, since our method can
systematically generate new interleavings to feed to these
methods. Our method is also diﬀerent from the various
testing techniques based on randomization, e.g. IBM’s Con-
test [4] and [1], although randomization can be used to di-
versify the input to our HaPSet learning.
8. CONCLUSIONS
We have proposed a coverage-guided systematic concur-
rency testing algorithm, where ordering constraints learn ed
from the good test runs are used to guide the selection of
high-risk interleavings for future test execution. We pro-
pose HaPSets to capture these ordering constraints and use
them as a metric to cover important concurrency scenarios.
This selective search strategy, in comparison to exhaustiv ely
testing all possible interleavings, can signiﬁcantly incr ease
the coverage of important concurrency scenarios with a rea-
sonable cost, while maintaining the capability of detectin g
subtle bugs manifested only by rare interleavings.
9. REFERENCES
[1] S. Burckhardt, P. Kothari, M. Musuvathi, and
S. Nagarakatte. A randomized scheduler with
probabilistic guarantees of ﬁnding bugs. In
Architectural Support for Programming Languages and
Operating Systems , pages 167–178, 2010.
[2] F. Chen, T. Serbanuta, and G. Rosu. jPredictor: a
predictive runtime analysis tool for java. In
International Conference on Software Engineering ,
pages 221–230, 2008.
[3] J. C. Corbett, M. B. Dwyer, J. Hatcliﬀ, S. Laubach,
C. S. Pasareanu, Robby, and H. Zheng. Bandera:
Extracting ﬁnite-state models from Java source code.
InInternational Conference on Software Engineering ,
pages 439–448, 2000.
[4] E. Farchi, Y. Nir, and S. Ur. Concurrent bug patterns
and how to test them. In Parallel and Distributed
Processing Symposium , page 286, 2003.
[5] C. Flanagan and P. Godefroid. Dynamic partial-order
reduction for model checking software. In POPL ,
pages 110–121, 2005.
[6] P. Godefroid. Partial-Order Methods for the
Veriﬁcation of Concurrent Systems - An Approach to
the State-Explosion Problem . Springer, 1996.
[7] P. Godefroid. Software model checking: The VeriSoft
approach. Formal Methods in System Design ,
26(2):77–101, 2005.
[8] K. Havelund and T. Pressburger. Model checking Java
programs using Java PathFinder. Software Tools for
Technology Transfer (STTT) , 2(4), 2000.
[9] G. Holzmann, E. Najm, and A. Serhrouchni. SPIN
model checking: An introduction. STTT ,
2(4):321–327, 2000.
[10] V. Kahlon and C. Wang. Universal Causality Graphs:
A precise happens-before model for detecting bugs in
concurrent programs. In CAV , pages 434–449.
Springer, 2010. LNCS 6174.
[11] V. Kahlon, C. Wang, and A. Gupta. Monotonic
partial order reduction: An optimal symbolic partialorder reduction technique. In Computer Aided
Veriﬁcation , pages 398–413, 2009.
[12] S. Lu, J. Tucek, F. Qin, and Y. Zhou. AVIO: detecting
atomicity violations via access interleaving invariants.
InArchitectural Support for Programming Languages
and Operating Systems , pages 37–48, 2006.
[13] M. Musuvathi and S. Qadeer. Partial-order reduction
for context-bounded state exploration. Technical
Report MSR-TR-2007-12, Microsoft Research, Dec.
2007.
[14] M. Musuvathi, S. Qadeer, T. Ball, G. Basler, P. A.
Nainar, and I. Neamtiu. Finding and reproducing
heisenbugs in concurrent programs. In OSDI , pages
267–280, 2008.
[15] S. Park, S. Lu, and Y. Zhou. CTrigger: exposing
atomicity violation bugs from their hiding places. In
Architectural Support for Programming Languages and
Operating Systems , pages 25–36, 2009.
[16] S. Qadeer and D. Wu. KISS: keep it simple and
sequential. In PLDI , pages 14–24, 2004.
[17] S. Savage, M. Burrows, G. Nelson, P. Sobalvarro, and
T. Anderson. Eraser: A dynamic data race detector
for multithreaded programs. ACM Trans. Comput.
Syst. , 15(4):391–411, 1997.
[18] K. Sen. Race directed random testing of concurrent
programs. In PLDI , pages 11–21. ACM, 2008.
[19] Y. Shi, S. Park, Z. Yin, S. Lu, Y. Zhou, W. Chen, and
W. Zheng. Do i use the wrong deﬁnition?: DefUse:
deﬁnition-use invariants for detecting concurrency and
sequential bugs. In OOPSLA , pages 160–174, 2010.
[20] C. Wang, S. Chaudhuri, A. Gupta, and Y. Yang.
Symbolic pruning of concurrent program executions.
InFSE, pages 23–32, 2009.
[21] C. Wang, S. Kundu, M. Ganai, and A. Gupta.
Symbolic predictive analysis for concurrent programs.
InInternational Symposium on Formal Methods , pages
256–272, 2009.
[22] C. Wang, R. Limaye, M. Ganai, and A. Gupta.
Trace-based symbolic analysis for atomicity violations.
InTools and Algorithms for Construction and
Analysis of Systems , 2010.
[23] C. Wang, Y. Yang, A. Gupta, and G. Gopalakrishnan.
Dynamic model checking with property driven
pruning to detect race conditions. In Automated
Technology for Veriﬁcation and Analysis , 2008.
[24] C. Wang, Z. Yang, V. Kahlon, and A. Gupta.
Peephole partial order reduction. In Tools and
Algorithms for Construction and Analysis of Systems ,
pages 382–396, 2008.
[25] M. Xu, R. Bod´ ık, and M. D. Hill. A serializability
violation detector for shared-memory server programs.
InPLDI , pages 1–14, 2005.
[26] Y. Yang, X. Chen, and G. Gopalakrishnan. Inspect: A
Runtime Model Checker for Multithreaded C
Programs. Technical Report UUCS-08-004, University
of Utah, 2008.
[27] J. Yu and S. Narayanasamy. A case for an interleaving
constrained shared-memory multi-processor. In
International Symposium on Computer Architecture ,
pages 325–336, 2009.230