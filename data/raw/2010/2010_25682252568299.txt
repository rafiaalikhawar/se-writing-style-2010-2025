How Do API Documentation and Static Typing Affect API
Usability?
Stefan Endrikat
University of Duisburg-Essen
Germany
stefan.endrikat@stud.uni-
due.deStefan Hanenberg
University of Duisburg-Essen
Germany
stefan.hanenberg@icb.uni-
due.deRomain Robbes
PLEIAD @ DCC
University of Chile
Chile
rrobbes@dcc.uchile.cl
Andreas SteÔ¨Åk
University of Nevada, Las
Vegas
United States
steÔ¨Åka@gmail.com
ABSTRACT
When developers use Application Programming Interfaces (APIs),
they often rely on documentation to assist their tasks. In previ-
ous studies, we reported evidence indicating that static type sys-
tems acted as a form of implicit documentation, beneÔ¨Åting devel-
oper productivity. Such implicit documentation is easier to main-
tain, given it is enforced by the compiler, but previous experiments
tested users without any explicit documentation. In this paper, we
report on a controlled experiment and an exploratory study com-
paring the impact of using documentation and a static or dynamic
type system on a development task. Results of our study both con-
Ô¨Årm previous Ô¨Åndings and show that the beneÔ¨Åts of static typing
are strengthened with explicit documentation, but that this was not
as strongly felt with dynamically typed languages.
Categories and Subject Descriptors
D.2.7 [ Software Engineering ]: Distribution, Maintenance, and
Enhancement
General Terms
Experimentation, Documentation, Human Factors
Keywords
API Usability, Documentation, Static Type Systems
1. INTRODUCTION
The software engineering community has developed a renewed
interest in studies of Application Programming Interfaces (APIs)
and their documentation, although the importance of good APIs has
been known for some time. In his landmark essay, Brooks states
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proÔ¨Åt or commercial advantage and that copies
bear this notice and the full citation on the Ô¨Årst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciÔ¨Åc
permission and/or a fee.
ICSE ‚Äô14, May 31 - June 7, 2014, Hyderabad, India
Copyright 2014 ACM 978-1-4503-2756-5/14/05 ...$15.00.that ‚ÄúThe most radical possible solution for constructing software
is not to construct it at all. [3]‚Äù Brooks describes how APIs and
frameworks are promising attacks on the essential complexities of
software.
While most reasonable scholars would not deny the beneÔ¨Åts of
high quality APIs, the impact of documentation is less clear. For
example, consider several claims on the beneÔ¨Åts of documentation
in the academic literature:
One user mentioned that the equivalent of an empty
API documentation with only the type name, (e.g., Doxy-
gen), could save him hours of source code exploration.[5]
The biggest hurdle when learning an API is the docu-
mentation. If the documentation for an API is good, it
solves 99% of your problems. [25]
While on the surface, many might consider such claims reason-
able if documentation is of truly excellent quality, studies show a
more nuanced state-of-the-practice. For example, not all develop-
ers trust documentation, preferring to investigate the source code
or to talk to colleagues. This is due in part to missing documenta-
tion or the suspicion that documentation may be out of date [26].
Further, an analysis of knowledge exchange sites (e.g., Stack Over-
Ô¨Çow) conducted by Parnin et al. showed that 87% of the classes in
the Android APIs are covered by questions and answers coming on
the Stack OverÔ¨Çow web site [20]. Indeed, the exact, quantiÔ¨Åable,
beneÔ¨Åts of documentation are in need of further investigation.
Proponents of statically typed programming languages further
claim that static typing provides a form of implicit documentation
embedded in the source code. For example, Pierce states that ‚ÄúThe
type declarations in procedure headers and module interfaces con-
stitute a form of documentation, giving useful hints about behav-
ior [22].‚Äù Similarly, Bruce claims that ‚Äú[...] type annotations pro-
vide documentation on constructs that make it easier for program-
mers to determine how the constructs can or should be used [4].‚Äù
We have undertaken an empirical investigation into these and
similar claims, Ô¨Ånding in previous work some empirical evidence
for their arguments. SpeciÔ¨Åcally, controlled experiments compar-
ing static and dynamic type systems have shown that, for some
tasks, type annotations in statically typed code do lead to increases
in productivity [14, 17]. However, while previous work appeared
to support the use of static type systems, all previously reported
experiments included only the implicit documentation provided by
the type system .Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proÔ¨Åt or commercial advantage and that copies bear this notice and the full citation
on the Ô¨Årst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speciÔ¨Åc permission and/or a
fee. Request permissions from Permissions@acm.org.
Copyright is held by the author/owner(s). Publication rights licensed to ACM.
ICSE‚Äô14 , May 31 ‚Äì June 7, 2014, Hyderabad, India
ACM 978-1-4503-2756-5/14/05
http://dx.doi.org/10.1145/2568225.2568299
632
Thus, to better evaluate the hypotheses presented by Pierce and
Bruce, we conducted a randomized controlled trial with a 2x2 ex-
perimental design, and an exploratory study on possible causes,
comparing developers using either static or dynamic typing and
with or without accompanying documentation. Given this setup,
we see at least two possible outcomes. First, it could be the case
that adding documentation is an equalizing factor, affording equal
or better productivity to those with dynamic typing. Second, type
annotations and static checking could beneÔ¨Åt programmers even
further. The results of our experiment show evidence toward the
second explanation‚Äîthe addition of explicit documentation affords
even greater productivity gains to developers using statically typed
languages. Those with dynamic typing did not appear beneÔ¨Åt as
much from the addition of explicit documentation.
While the previous paragraph gives the primary Ô¨Ånding of our
paper, to try and garner possible causes for the behavior we ob-
served, we conducted an exploratory analysis of the interaction logs
of our developers while completing the tasks. Results from this
analysis are not deÔ¨Ånitive, but hint that the number of Ô¨Åle switches
may be a leading indicator on why such differences occur. We also
report on additional indicators from the logs.
Structure of the paper We Ô¨Årst describe related work (Section
2): empirical studies on documentation and type systems. Then, we
describe the design of the experiment in detail (Section 3). Next,
we present the results of our experiments (Section 4). We then per-
form an exploratory study based on the interaction data we recorded
(Section 5). We close the paper with a discussion (Section 6), pos-
sible threats (Section 7), and conclude (Section 8).
2. RELATED WORK
2.1 Studies of APIs and Documentation
Controlled experiments An early experiment by Tenny [29]
compared two types of procedures (inlined or external procedures)
and the occurrence of comments. The experiment showed an effect
of comments on understandability of short programs, measured by
a questionnaire. An experiment by Tryggeseth [31] found that doc-
umentation had a positive impact on the time necessary to com-
plete a maintenance task in a C++ system; the subjects were ap-
proximately 20% faster. The solution was written in pseudocode.
Unfortunately, the experimental design was not fully documented.
An experiment by Prechelt et al. [24] found that documenting
design patterns in source code (in addition to the existing documen-
tation), had a positive effect on development time and correctness.
Entities forming a design pattern were more easily recognized and
processed as a single ‚Äúchunk‚Äù of data, rather than several entities.
On the other hand, Ellis et al. [10] found conÔ¨Çicting evidence on
the usefulness of the Factory design pattern.
Another experiment by Bandi et al. [2] explored the impact of
three complexity metrics on maintenance time: Interaction Level,
Interface Size, and Operation Argument Complexity. All three met-
rics were found useful to predict the maintenance time taken by
several groups of students (93 students in total).
An experiment by Arisholm et al. [1] showed that there is a pos-
itive impact of UML documentation on task correctness, but not
on time. The experiment had several tasks on the same systems
and showed that subjects were more likely to succeed in later tasks
when the documentation was present. A recent pilot experiment by
Leotta et al. [15] seems to, unsurprisingly, conÔ¨Årm that outdated
documentation is less useful than up-to-date documentation. How-
ever, a recent study by Petre [21] showed that usage of UML in the
practice is limited, yielding the necessity to investigate additional
forms of documentation.Dekel and Herbsleb evaluated the effectiveness of eMoose, a
‚Äúknowledge-pushing‚Äù tool, that highlights critical parts of the doc-
umentation and makes them more obvious in the IDE [6]. A con-
trolled experiment was conducted, where participants used eMoose
in a variety of tasks where signiÔ¨Åcantly more likely to solve the
tasks than users from the control group (for four out of 5 tasks). A
follow up video analysis study Ô¨Çeshed out how the documentation
was used in more detail [7].
Exploratory studies Robillard and DeLine conducted a study
on what makes APIs hard to learn [25], based on surveys and inter-
views; they found a variety of obstacles that developers faced, and
documented the implications of them. In particular, developers pre-
fer a continuous presentation of the API, focused on its relation to
the problem domain (how a set of API elements are used together
to solve a given task), as opposed to a hyperlinked documentation
of individual API elements.
Dagenais and Robillard interviewed developers to Ô¨Ånd out what
decisions were taken by developers with respect to documenta-
tion, and the consequence it had on the project [5]. Timely update
to the documentation was found to improve code quality. Three
types of documentation were observed: task-related documenta-
tion aimed at newcomers (getting started type of documentation),
reference documentation documenting program elements, and con-
ceptual documentation.
An exploratory study by Duala-Ekoko and Robillard [9] anal-
ysed video recordings of twenty participants working on a set of
tasks for which they had to discover unknown APIs. The main
information sources where the API documentation, and the web.
They formulated a list of 20 questions the developers asked when
solving the tasks, and made additional observations about the chal-
lenges developers faced, such as discovering types with non-obvious
relationships, and difÔ¨Åculties related to the usage of exceptions.
Roehm et al. conducted a study of program comprehension in
industry, based on an observational study of 28 programmers from
seven companies [26]. It highlighted important facts pertaining to
documentation: source code was more trusted than documentation
due to possible mismatches, or the code not being covered; direct
communication was preferred over the documentation for similar
reasons. On the other hand, understanding the rationale behind the
code (its purpose and how to use it) was found to be very difÔ¨Åcult
(‚Äúexhausting‚Äù) when the code is not documented.
Maalej and Robillard developed a taxonomy of the types and
patterns of knowledge commonly found in APIs [16], based on two
large APIs, the JDK and .NET. The types of knowledge found in
the APIs were: functionality, concepts, directive, purposes, non-
functional requirements, control Ô¨Çow, structure, patterns, examples,
environment issues, and references. They also found a surprisingly
high amount of content (40 to 50%) that brought little value.
Parnin et al. investigated the recent phenomenon of crowd docu-
mentation, as measured by the questions about API elements asked
and answered on the Stack OverÔ¨Çow website [20]. They found
that over 35000 developers contributed in crowd-documenting An-
droid, covering 87% of the classes. Additionally, there was a strong
correlation between usage of API elements and the amount of dis-
cussions on Stack OverÔ¨Çow.
2.2 Type Systems as Documentation
Several empirical studies have been completed on type systems.
We focus our discussion here on those that are related to the impact
of documentation in relation to type systems. Perhaps the earliest
known controlled study on type systems was from Gannon. This
early experiment showed an increase of reliability for subjects us-
ing a statically-typed language [11].633An experiment by Prechelt and Tichy [23] compared two ver-
sions of C, one where the types of arguments of procedure calls are
checked (ANSI C), and another (K&R C), in which the types are
not checked. Both programs came with documentation and type
annotations. One task showed differences between the two treat-
ments, another did not. Thus the experiment gave doubt to the idea
that type annotations are useful as documentation only.
In terms of our own work, several previous studies investigated
the impact of using static and dynamic type systems on the usage
of undocumented APIs. We investigated a variety of maintenance
tasks, including identifying classes in an undocumented API [14].
Through these studies, we observed that as the number of classes
that needed to be identiÔ¨Åed grew, the advantage of the static type
system grew as well. Other tasks related to Ô¨Åxing type errors and
semantic errors were investigated, with an advantage found when
Ô¨Åxing type errors, but none found when Ô¨Åxing semantic errors.
In a second experiment [17], we focused on class identiÔ¨Åcation
tasks in an undocumented API. We also deÔ¨Åned several tasks of
varying complexity, and observed a similar tendency: the larger or
more complex the types to identify were, the greater the advan-
tage was found towards the statically typed language. We com-
plemented the study with an exploratory story of the participants‚Äô
interaction log, which shed some light in our Ô¨Åndings: as tasks
got more complex, developers using the statically typed language
tended to open less Ô¨Åles and switched between them less often.
In a subsequent experiment [12], we investigated the usefulness
of generic types compared to raw types under similar conditions.
The experiment showed that generic types helped for using undoc-
umented APIs, but not for type error Ô¨Åxing tasks or extension tasks.
Recently, we ran an experiment that tested whether the previ-
ously measured differences of static and dynamic type systems could
be reduced to the question whether the beneÔ¨Åt of static type systems
is only a matter of syntax [27]: static, declarative type systems lead
to some additional pieces of syntax in the code (i.e. the type decla-
rations) that already might help developers (without the static type
check). While the Ô¨Åndings were that already the syntax already
helped developers (with correct type names in the code) it turned
out that incorrect type names signiÔ¨Åcantly reduced the usability of
APIs (in comparison to no type names).
Finally, one study by SteÔ¨Åk and Siebert [28], and another by
Denny et al. [8] showed that novices have signiÔ¨Åcant difÔ¨Åculty
with static typing initially. Given these Ô¨Åndings, experimenters
must recognize that results on type systems may not be universal‚Äî
developers at different points in their careers may have different
productivity needs in regards to type systems.
3. EXPERIMENT DESCRIPTION
3.1 Initial Considerations
The goal of the experiment is to compare the effects of docu-
mentation and type system (and their potential interaction). Sev-
eral decisions need to be made on: the research questions, the kind
of documentation, the programming language, the programming
tasks, and the measurements. Similarly to our previous experi-
ments, we measure the effect of documentation and type system
on the time developers take to complete a programming task.
Number and type of subjects The goal was to deÔ¨Åne and exe-
cute an experiment with a relatively small number of participants:
we planned for 20 to 30. Additionally, to make recruitment easier,
we wanted the time commitment for each participant to be rela-
tively small. Unfortunately, programming tasks typically require
several hours to complete. Thus, we settled on a maximum dura-
tion of Ô¨Åve hours.The low number of participants also means that the deviation
among participants needs to be in order to Ô¨Ånd reliable effects.
Otherwise, there is the danger that the measurements are hidden
by confounding factors. These constraints inÔ¨Çuenced the choices
of documentation, programming language, programming environ-
ment and programming task. While it would be desirable to have a
large number of participants in such experiments, the overall mag-
nitude of ours is common in the literature. The experiment of
Pankratius [19] had 13 participants, Dekel‚Äôs [6] had 26, and Wet-
tel‚Äôs [33] had 42. Our previous experiments on type systems all had
between 20 and 30 participants. As in most software engineering
experiments, subjects are students, with all the potential pros and
cons that this entails (see e.g., [30]).
Type of documentation. Obviously the type and quality of doc-
umentation chosen strongly inÔ¨Çuences the results; giving mislead-
ing or outdated documentation would negatively impact the results
[15]. Since the space of possibilities is large, we sought a format of
documentation that would be commonly encountered and useful.
The Ô¨Årst criteria is length . If documentation is exhaustive, de-
velopment time might be spent in reading, with the risk that the
participants would not Ô¨Ånish the task. Participants may also not
read the documentation carefully if it is too long, as seen in Dekel‚Äôs
study [7]. Additionally, participants may search in wrong parts of
the document for a given solution‚Äîa situation that frequently oc-
curs in web searches. Thus, the size of documentation must be
carefully balanced.
Although there are many different documentation formats (such
as JavaDoc, UML diagrams, etc.) we settled on free-form text that
provides a short description of the API and source code examples
that show how to use the API. This matches the needs expressed
in Robillard‚Äôs study [25]: linear documentation instead of small in-
terlinked pieces, and examples showing how various API elements
are used together. This is also similar to the format of questions
and answers in Stack OverÔ¨Çow, strongly suggesting this is the type
of documentation developers favor. We did not investigate UML
documentation as it has already been investigated by Arisholm et
al.[1]. We also did not discarded JavaDoc-type documentation
(hyperlinked documents matching the program structure) as it did
not match developer preferences in Robillard‚Äôs study.
A challenge is to give documentation that is not too close to the
programming task. Otherwise, the task is just to type text. The
examples given were not an exact match with the task to achieve.
Choice of programming language In order to facilitate our pro-
grammers, we needed a language familiar to participants‚Äîotherwise
the time required to teach and learn it would consume most of the
experiment time. At the same time, we needed a language sup-
porting both static and dynamic typing, to reduce the differences
between the experimental treatments.
Since most of the participants come from the University of Duis-
burg-Essen, where Java is taught from the Ô¨Årst semester, we de-
cided to use the programming language Dart. Dart can be used
similarly to Java, but has optional types annotations.
Choice of IDE Similarly to previous experiments, we use a sim-
pliÔ¨Åed IDE that consists of an editor with syntax highlighting, a
treeview for the Ô¨Åles within the software project, and buttons for
running test cases. This reduces the risk that IDE proÔ¨Åciency im-
pacts the results. Furthermore, this reduces the problem that the
IDE is being used by different subjects in different ways. For ex-
ample, subjects could use different IDE features for navigating the
code; where each of these strategies might differ with respect to
developer performance.
Choice of Tasks A Ô¨Årst factor is size: excluding warmup tasks,
experiments in the literature range from one single task [19], to ten634tasks or more [33]. Since documentation takes time to read, short
tasks would give an unfair advantage. Likewise, the effect of static
type systems is more visible on larger tasks [17, 14]. Contrary to
our previous experiments, this experiment uses a single task (plus
warmup), whose expected duration is several hours. This also pre-
vented us from using a within-subject design, as the task would be
too long to be repeated.
The type of task has an important effect on the results. We can-
not expect static type systems or documentation to have the same
effect regardless of the task‚Äîwhich we found in a previous ex-
periment [14]. Hence we need a task that would not favor one or
the other treatments (task or documentation), and would Ô¨Åt in the
API usability scenario. We chose a task where participants use
the API in a way that objects need to be conÔ¨Ågured and passed to
the API. Previous experiments have shown that developers bene-
Ô¨Åt from static type systems in such a situation, but we expect that
developers beneÔ¨Åt from documentation in such a situation as well.
To reduce confounding factors, no other API is necessary, and the
task can be solved without using loops or conditionals (to reduce
confounding factors).
We considered other tasks in [14]: Type error Ô¨Åxing tasks, where
we found an advantage of the type system, and would not expect
an advantage of documentation; and semantic error Ô¨Åxing tasks,
where we found no advantage of the type system, but would expect
an advantage of the documentation, as in Dekel‚Äôs study [6].
3.2 Research Questions
Our independent variables are documentation and type system,
our dependent variable is time, and our research questions are:
RQ1 : When free‚Äìtext form documentation is given, do de-
velopers actually use it?
RQ2 : Is there a difference in development time for a given
programming task between statically and dynamically typed
programming languages, with or without a given free‚Äìtext
form documentation?
One potential confounding factor in measuring development time
is, that potentially developers spend a lot of time only reading doc-
umentation. Hence, we are also interested in the results of RQ2
without taking reading time into account:
RQ3 : Assuming that developers have already read and un-
derstood the documentation, does that inÔ¨Çuence the potential
difference in development time (from RQ2)?
3.3 Programming Task and API
The domain for the programming task was a delivery service:
different kinds of packages from a vendor (with multiple stores)
need to be delivered to a customer. The domain was chosen as it is
easily understandable by the participants.
The programming task is to deliver a product (a kitchen) to a
customer under some constraints: the shortest way should be taken,
i.e. a store that is as close as possible to the target address and a
carrier should be used that is as close as possible to this store. To
solve this task, a method is given to the participants whose header
contains key objects for the customer and the product objects.
The challenge for developers is to locate all the API elements
necessary to properly conÔ¨Ågure a delivery object. Figure 1 shows
an extract from a possible solution (some code is omitted for space
reasons). For the dynamically typed code all type annotations (es-
pecially in the header) would be removed. The Ô¨Ågure also shows
how similar Dart is to Java, and also that neither loops nor condi-
tions are required to solve the task.void t a s k ( C o n n e c t i o n s d a t a l i n k s ,
P e r s o n a l I d e n t i f i c a t i o n customer ,
KitchenKey k i t c h e n ) {
D e l i v e r y d = D e l i v e r y . c r e a t e D e l i v e r y ( ) ;
P r o d u c t prod = d a t a l i n k s . g e t C a t a l o g s ( ) .
getHomeAidsCatalog ( ) .
g e t P r o d u c t B y I d ( k i t c h e n ) ;
d . s e t D e l i v e r y O b j e c t ( prod ) ;
Customer c = d a t a l i n k s . g e t L i b r a r i e s ( ) .
g e t C u s t o m e r L i b r a r y ( ) . g e t C l i e n t ( c u s t o m e r ) ;
C o n t a c t c t = C o n t a c t . c r e a t e C o n t a c t ( ) ;
c t . s e t P e r s o n ( c ) ;
Address ca = / / g e t a d d r e s s v i a c o n n e c t i o n s . . .
d . s e t D e s t i n a t i o n ( c t ) ;
S t o r e s = d a t a l i n k s . getManagers ( ) . . .
g e t N e a r e s t S t o r e F o r P r o d u c t ( prod , ca ) ;
d . s e t S o u r c e ( s ) ;
S h i p p e r s h i p p e r =
. . . g e t N e a r e s t C a r r i e r F o r P r o d u c t ( prod , s ) ;
d . s e t T r a n s p o r t e r ( . . . ) ;
/ / . . . s e t some a d d i t i o n a l p r o p e r t i e s . . .
return d ;
}
Figure 1: Example solution for the task (statically typed)
The API‚Äôs source code was available to the participants. It fol-
lows the design decisions of previous experiments (see [17, 14]).
First, the methods to be used by the developers require parameters
and return objects that are only basic data types or objects from
the API itself (it does not use standard library classes to reduce
confounding factors due to expertise in the standard APIs). Sec-
ond, the API is rather small (about 2000 lines of code), but is large
enough that developers can not completely read the code within the
experiment. For instance, it is larger than the systems used in the
experiment by Arisholm, which had 293 and 338 lines of code et al.
[1]. Finally, the API does not contain additional documentation (no
JavaDoc, no comments, etc.) in order to control the documentation
variable in the experiment more strictly.
Finally, developers were equipped with test cases. When all the
tests pass, the programming task has been solved. The source code
of the tests was not available to the subjects.
3.4 Documentation
As motivated in section 3.1, we decided to put text documents
into the project, i.e. separate ASCII Ô¨Åles, that describe of a certain
part of the API and provide the corresponding code examples. The
names of these ASCII Ô¨Åles give a hint about their content. For ex-
ample, a Ô¨Åle named delivery.txt describes the meaning of delivery
objects in general and provides code snippets creating and instanti-
ating a delivery object and related objects (illustrating how several
API elements are used together, as recommended in [25]). The
documentation also contains parts of the API which were not rel-
evant for the programming task. For instance, the documentation
contains code snippets that show how a certain kind of carrier can
be requested for the delivery, while the programming task requires
to select a carrier depending on the distance from the store.
Figure 2 gives an impression of the documentation delivered to
the participants.1.
1The documentation was written in German‚Äîthis is an abridged
translation of the documentation Ô¨Åle on delivery.635Each document starts with a general description of its content
and then gives a more detailed description of speciÔ¨Åc topics, in-
cluding code snippets. For example, the code in Figure 2 shows
how to create and initialize a contact object ‚Äì which is required for
solving the programming task (see Figure 1). As another example,
the last code snippet in 2 describes the access to a certain object
from the furniture catalog by its name‚Äîcode which is not used in
the solution. The last line of code shows how a product is stored in
the delivery object, which is used in the solution.
We distributed the relevant and the irrelevant information in the
documentation randomly in order to deny participants the ability
to learn where to look into the documentation without reading it.
Altogether, we provided six documentation Ô¨Åles, each with a length
between one and six kilobytes.
D e l i v e r y
A d e l i v e r y o b j e c t c o n t a i n s a l l r e l e v a n t i n f o r m a t i o n
i n o r d e r t o d e l i v e r a p r o d u c t t o a c u s t o m e r . I t
r e q u i r e s f u r t h e r i n f o r m a t i o n a b o u t t h e customer , t h e
p r o d u c t i t s e l f , and t h e s t o r e from which t h e p r o d u c t
i s d e l i v e r e d . [ . . . ]
[ . . . ]
The d e l i v e r y o b j e c t p e r m i t s t o add f u r t h e r r e c e i v e r s
i n o r d e r t o p e r m i t t o d e l i v e r a p r o d u c t a l t h o u g h t h e
r e c e i v e r i s n o t a t home . In c a s e t h i s i s n o t d e s i r e d ,
a c u s t o m e r o b j e c t must be l o a d e d from t h e
P r i v a t e C l i e n t L i b r a r y . A d d i t i o n a l a d d r e s s e s ( such as
n e i g h b o u r s , e t c . ) must be l o a d e d from t h e
L o c a t i o n D i r e c t o r y .
[ . . . ]
The f o l l o w i n g code s n i p p e t e x e m p l i f i e s t h e c r e a t i o n
and i n i t i a l i s a t i o n of a c o n t a c t o b j e c t .
. .
n e i g h b o r c o n t a c t = C o n t a c t . c r e a t e C o n t a c t ( ) ;
n e i g h b o r = Neighbor . c r e a t e N e i g h b o r ( ) ;
n e i g h b o r c o n t a c t . s e t L o c a t i o n ( n e i g h b o r l o c a t i o n ) ;
n e i g h b o r c o n t a c t . s e t P e r s o n ( n e i g h b o r ) ;
p o s t D e l i v e r y . s e t D e s t i n a t i o n ( n e i g h b o r c o n t a c t ) ;
. .
[ . . . ]
For d i f f e r e n t k i n d s of p r o d u c t s t h e r e a r e d i f f e r e n t
c a t a l o g s t h a t can be used t o i d e n t i f y a p r o d u c t ,
f o r example by i t s name . For example , t h e f o l l o w i n g
code s n i p p e t s e l e c t s a p r o d u c t c a l l e d " C h a i r " t o be
d e l i v e r e d .
f u r n i t u r e =
f u r n i t u r e C a t a l o g . getFurnitureByName ( " C h a i r " ) ;
p o s t D e l i v e r y . s e t D e l i v e r y O b j e c t ( f u r n i t u r e ) ;
[ . . . ]
Figure 2: Small extract from the documentation delivery.txt
given to subjects that had access to documentation
3.5 Measurements
The experiment performed a number of different measurements,
where it should be mentioned that development time is the main
measurement for the experiment.
Development time (DT, seconds) : The main measurement
in the experiment is development time (DT), i.e. the time
required to solve the programming tasks. Development time
was measured from the moment when subjects were given
the programming task until the moment when the program-
ming task is fulÔ¨Ålled (i.e. until all test cases passed).Number of Ô¨Åle switches (FS): The number of Ô¨Åle switches
measures how often developers switch between different Ô¨Åles
during development.2
Number of documentation switches (DS): We measured
how often developers (those ones to whom documentation
has been delivered) switched into the documentation. The
number is an indicator for whether documentation has been
used.
Documentation reading time (DRT): We measured the time
that developers (those ones to whom documentation has been
delivered) spent on the documentation, i.e. it is the sum
of time spans between switching from a documentation Ô¨Åle
within the IDE. We give this the name documentation read-
ing time, although this time (probably) additionally includes
the time required to Ô¨Ånd the relevant places in the documen-
tation, to understand the documentation and to think about
the relationship between the given documentation the rela-
tionship to the given programming problem.
Coding time (CT = DT ‚Äì DRT): While the development
time is the complete time required to solve the programming
problem, i.e. including the time required for studying the
documentation, the coding time is the development time mi-
nus the documentation reading time. The intention of this
measurement is to test whether, if we were to assume it was
unnecessary to study the documentation, that coding time is
less than ordinary development time
The measurements described above were taken by the IDE itself
which we prepared in previous experiments to log whenever a test
case is executed, a Ô¨Åle is opened, etc. Additionally, we ran screen
recordings on all machines for all subjects in order to get the chance
to gain additional data that was not directly recorded by the IDE.
This gave us the ability to see what subjects actually did.
3.6 Experimental Design
We designed the experiment as a 2x2 randomized between-subject
factorial design (see [34]) that analyzes the effect of the indepen-
dent variables type system (with the treatments static and dynamic
type system) and documentation (with the treatments of with and
without documentation) on the dependent variable development time
(respectively, coding time). This experimental design consists of
four groups. Each represents a combination of a treatment of both
independent variables. All subjects are randomly assigned to one of
the four groups in a way that Ô¨Ånally each group has approximately
the same number of participants.
This 2x2 design has the potential problem that for a small sample
size, groups may be unbalanced (i.e. good developers are more
frequently in one group than in a different group) and there may be
a high deviations among subjects (i.e. a large difference between
the best and worst developers). These issues may hide the effects
in the experiment‚Äîin such cases the independent variables have a
non-signiÔ¨Åcant effect on the dependent variable. Because of that,
our previous experiments consisted only of 2 groups [14, 17, 12].
However, in contrast to our previous studies, this experiment has a
single, large programming task, which in theory allows us to detect
effects more easily. We conducted a pilot experiment in which we
tested the task on some subjects, which convinced us that the main
2The motivation for this measurement is that previous experiments
that measured differences between static and dynamic type systems
showed that the number of Ô¨Åle switches reveal analogous results to
development time, see section 2.2.636effects are replicable in this setting. Table 1 shows the four different
groups we have in the experiment.
4. EXPERIMENT RESULTS
The experiment has been performed at the University of Duisburg‚Äì
Essen and the University of Koblenz‚ÄìLandau. In total, 25 students
were used as participants in the analysis (two subjects were re-
moved from the data3, one subject dropped out of the experiment).
Each student was in the Ô¨Åfth semester or above and had Ô¨Ånished
basic programming courses at the corresponding university. More
than half of the students were bachelor students, the other ones
were Master students in computer science (or related studies). All
students were males between the ages of 22‚Äì30.
The data was collected in multiple sessions, depending on stu-
dent schedules. Students were randomly assigned to one of the
four groups in order to reduce the potential for accidental bias, the
reasons for which are explained thoughtfully by V ogt (see [32]).
Figure 1 describes the names of the groups and the number of sub-
jects. Almost all groups have the same number of subjects (each
has six subjects) with the exception of group 1 which has seven.
Group 1 has both documentation and static type system, group 2
had no documentation and static typing. The other groups had the
same conÔ¨Åguration of documentation with dynamic typing.
Table 1: Groups: groupNumber (numberOfSubjects)
Documentation
Yes No
Static TS 1(7) 2(6)
Dynamic TS 3(6) 4(6)
4.1 Empirical Data
Table 2 shows the raw anonymized data for the experiment: The
Ô¨Årst row shows each participant number, the second the experi-
mental group. Continuing rows show development time (DT), Ô¨Åle
switches (FS), number of switches to documentation (DS), time
spent reading documentation (DRT) and coding time (CT).
Three participants (8, 20, and 23) were unable to complete the
development task within our time constraints. All of the partici-
pants that were unable to complete the tasks happened to be in the
group without documentation. Two of these (20 and 23) used the
dynamically typed language, while one other (13) was given the
statically typed language.
In order to ease the reading of the raw measurements, Figure 3
illustrates the development times via boxplots, which reveals the
following possible trends, which will be evaluated more carefully
work with standard statistical assessments:
1. The raw median amount of time taken to complete the task
appears to increase from groups 1‚Äì4.
2. The raw difference between static and dynamic typing, with
documentation (groups 1 and 3), appears to be large.
3. The raw difference between static typing without documen-
tation, and dynamic typing with documentation, does not ap-
pear to be large.
3For one subject, the machine crashed which made the experiment
data for this subject unreadable, one subject was a non‚Äìnative ger-
man speaker and said that he did not completely understand the
documentation.4. The raw differences between static typing without documen-
tation and dynamic typing either with or without documen-
tation, appear to be rather small, although this is potentially
mitigated by the fact that 2 of six participants in group 4 did
not complete the task.
Although boxplots are a common way to visualize the results
of the measurements, boxplots might be considered problematic in
this experiment, because the number of subjects for each groups is
relatively small. Hence, Figure 4 illustrates the development times
by showing each individual measurement (and without showing
any descriptive statistics). Again, the raw measurements seem to
suggest that there is the tendency that the absence of static types
and the absence of documentation increased the measured develop-
ment times.
Group4
dynamic 
type, no doc3 
dynamic
 type+doc2
static type, 
no doc1 
static
type+docDevelopment time (in seconds)12000
10000
8000
6000
4000
2000
0
Figure 3: Boxplot for raw development time measurements
Group4
dynamic 
type, no doc3
dynamic 
type + doc2
static 
type, no doc 1
static 
type + docDevelopment time (in seconds)12000
10000
8000
6000
4000
2000
0
Figure 4: Raw development time measurements
4.2 RQ1: Documentation Usage
There are many ways in which an analysis of our Ô¨Årst research
question can be quantiÔ¨Åed, although which is the ‚Äúright‚Äù one is not637Table 2: Measured development times (DT, time in seconds), Ô¨Åle switches (FS), documentation switches (DS), documentation reading
time (DRT), and coding time (CT), * = programming task not Ô¨Ånished in time
1 2 3 4 5 6 7 8 910 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
Group 1 1 1 1 1 1 1 2 2 2 2 2 2 3 3 3 3 3 3 4 4 4 4 4 4
DT
3668
3870
3042
3092
1877
5756
5715
11460*
5056
10439
4618
2374
8363
7468
7196
10947
5574
10046
7931
11460*
6078
9025
11460*
9678
4994
FS
253
189
367
170
210
519
487
1120
359
709
396
320
796
672
730
674
505
734
372
920
419
680
834
616
411
DS
35
47
94
41
43
45
30
0
0
0
0
0
0
175
130
132
93
103
91
0
0
0
0
0
0
DRT
553
741
947
781
435
588
338
0
0
0
0
0
0
1788
1295
1901
1214
1360
1534
0
0
0
0
0
0
CT
3115
3129
2095
2311
1442
5168
5377
11460*
5056
10439
4618
2374
8363
5680
5901
9046
4360
8686
6397
11460*
6078
9025
11460*
9678
4994
Table 3: Descriptive statistics for raw measurements (groups 2 and 4 do not have values for DS and DRT, because they do not have
documentation)
DT FS DS* DRT* CT
Group 1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4
MIN 1877 2374 5574 4994 170 320 372 411 30 - 91 -338 -1214 -1442 2374 4360 4994
MAX 5756 11460 10947 11460 519 1120 734 920 94 -175 -947 -1901 -5377 11460 9046 11460
Avg 3860 7052 8194 8783 314 617 615 647 48 -121 -626 -1515 -3234 7052 6678 8783
Median 3668 6710 7700 9352 253 553 673 648 43 -117 -588 -1447 -3115 6710 6149 9352
Std.Dev. 1430 3589 1973 2715 145 315 145 209 21 - 32 -211 - 278 -1512 3589 1827 2715
obvious. In this experiment, we analyzed the use of documenta-
tion by having participants use a single monitor and tracking both
switches between code and documentation and time spent on each.
From our perspective, there are several other techniques that could
potentially be used, for example, eye tracking technology on sin-
gle or multiple monitors, or even, to give a very different approach,
subjective surveys about the user‚Äôs experience with the documen-
tation, could potentially draw different kinds of insight into RQ1.
While many approaches may have merit, we chose our approach
because it is objective and easily trackable.
Table 4: Results for research question 1
Shapiro‚ÄìWilk T‚ÄìTest 95% ConÔ¨Ådence Interval
DS .12 .001 54 109
DRT .537 .001 724 1345
DRT/DT .99 .001 .15 .23
Given our methodology, we Ô¨Årst analyzed, whether the number
of Ô¨Åle switches to documentation Ô¨Åles (for developers to whom
documentation was delivered) is signiÔ¨Åcantly larger than zero (no
documentation). Table 4 shows the results. In this table, we present
the Shapiro‚ÄìWilk test, which was not signiÔ¨Åcant (p>.12), i.e. a nor-
mal distribution can reasonably be assumed. Thus, we conducted a
traditional parametric 1-sample t-Test (a comparison against zero),
which was found to be signiÔ¨Åcant ( p<.001). The 95% conÔ¨Ådence
interval is between 54 and 109 with the standard alpha of .05. Per-
forming the same study on time reveals that developers spent be-
tween 724 and 1345 seconds (i.e. 12 ‚Äì 22 minutes) on the docu-
mentation and that this difference was also signiÔ¨Åcant (p < .001).
Another way to assess whether documentation was used is to
evaluate how long it was displayed on the screen. We tracked this
using the ratio DRT/DT, effectively the time spent on reading docu-
mentation divided by the total time spent on developing as a whole.
The resulting T-Test reveals that their usage of the API was greater
than zero (p < .001). More speciÔ¨Åcally, the traditional 95% conÔ¨Å-
dence interval indicates developers spent between 15% and 23% ofthe development time on documentation (DRT / DT).
4.3 RQ2: Development Time
The experiment is built as a standard 2x2 between-subject de-
sign with two factors, namely type system and documentation. The
standard analysis technique for this type of design is a two-factor
ANOV A with the aforementioned Ô¨Åxed factors and the dependent
variable development time. While this technique is standard, one
important assumption of the two-factor ANOV A is the homogene-
ity of variances, commonly tested with a Levene Test.
In our case, the Levene Test was signiÔ¨Åcant (p<.034), implying
that our data exhibits heteroscedasticity. In other words, this test
conÔ¨Årms what is visually obvious from our boxplots‚Äîdifferent ex-
perimental groups had different amounts of variance. Given this
observation, we double checked our results using non-parametric
statistics. Results from these tests conÔ¨Årm our Ô¨Åndings and we
present only the parametric results here.
Thus, testing the results as a two-factor ANOV A, we Ô¨Ånd a sig-
niÔ¨Åcant effect for language (p=.007, 2
p=.30), an effect that ap-
proaches signiÔ¨Åcance for documentation (p=.075, 2
p=.14), and a
non-signiÔ¨Åcant effect for the interaction (.211). The overall differ-
ence, with 95% conÔ¨Ådence, between the static and dynamic typ-
ing factors was between 912 and 5353, i.e. between 15 and 89
minutes. In this case, static typing had the advantage. With re-
spect to documentation, given that the effect only approached sig-
niÔ¨Åcance, we urge caution in not overstating validity of the result.
While we report the result for completeness, we make no direct
claims as to whether documentation had a positive impact4. With
that said, overall, the 95% conÔ¨Ådence interval for the difference
between the groups with and without documentation was between
-391 and 4496. Thus, the group with documentation can arguably
4On the other hand, discarding a p-value close to, but exceeding the
traditional signiÔ¨Åcance level (alpha = 0.05), may result in a Type II
error, in this case, assuming that there is no difference when there
actually is. We also note that the alpha level is arbitrary; some
studies use an alpha level of 0.1, others an alpha level of 0.01.638be said to have a small advantage. Table 5 shows the results of run-
ning a T‚ÄìTest (again, to double check the two‚Äìfactor ANOV A with
the signiÔ¨Åcant Levene‚ÄìTest), with comparable results.
Table 5: Results for research question 2 ‚Äì the variable docu-
mentation is only close to signiÔ¨Åcant (p<.1)
Type System Documentation
static dynamic with without
Shapiro‚ÄìWilk (p-values) .08 .45 .63 .17
T-Test (p-values) .008 .098
95% Conf. Interval 912 5353 -391 4496
BeneÔ¨Åt statically typed (with documentation )*
4.4 RQ3: Coding Time
In order to evaluate the amount of time each group spent pro-
gramming, we analyzed the data in a third way, by removing the
amount of time the documentation groups spent analyzing docu-
mentation. Said in a different way, in our third question, we sub-
tracted out the time spent looking at documentation from the total
development time for the documentation group. From this point of
view, the group that never received documentation would theoreti-
cally have the same times.
Group4
dynamic
types, no doc3
dynamic
types + doc2
static
types, no doc1
static
types + docCoding Time (in seconds)12000
10000
8000
6000
4000
2000
0
Figure 5: Boxplot for coding time measurements
We perform this analysis to differentiate between two scenar-
ios. First, developers with documentation might browse it, but ul-
timately spend the same amount of time on programming as the
group without documentation. On the other hand, users with doc-
umentation may spend less time looking at the code, if the docu-
mentation helps them to understand the code more quickly. If the
former was the case, it might imply that documentation was only
marginally helpful, while if the latter was the case, documentation
could be seen as helping developers. Given that our previous re-
sult from RQ2 showed that documentation only approached signif-
icance overall, separating the concerns in this way might help us to
understand what happened in the experiment.
We reran the experimental analysis with this new view of the
data. The Levene-Test is also signiÔ¨Åcant, hence we employ the
same procedure as before. In this case, the type system was signiÔ¨Å-
cant (p=.018, 2
p=.24), documentation was signiÔ¨Åcant (.008, 2
p=.29),and the interaction effect continued to be non-signiÔ¨Åcant (.426).
For typing, the 95% conÔ¨Ådence intervals of the difference between
the groups was between 323 and 5082 seconds (5‚Äì85 minutes). The
static typing group again had the advantage. For the documenta-
tion main effect, the 95% conÔ¨Ådence interval for the difference was
between 810 and 5386 seconds (14‚Äì90 minutes), with more time
needed for the group without documentation. Figure 6 shows the
T-Test that has been applied in order to double‚Äìcheck the two-factor
ANOV A, with comparable results.
For a more intuitive representation of the differences, Figure 5
shows the boxplots for the different coding times for each group.
Again, because of the potential problematic illustration using box-
plots we additionally illustrate the raw measurements in Figure 6.
Group4
dynamic
types, no doc3
dynamic
types + doc2
static
types, no doc1
static
types + docCoding time (in seconds)12000
10000
8000
6000
4000
2000
0
Figure 6: Coding time measurements
Table 6: Results for research question 3
Type System Documentation
static dynamic with without
Shapiro‚ÄìWilk (p-values) .06 .25 .54 .19
T-Test (p-values) .03 .01
95% Conf. Interval 323 5082 810 5386
BeneÔ¨Åt statically typed with documentation
5. EXPLORATORY STUDY
Our purpose in this section is to document additional observa-
tions we had about our data, by analyzing interactions with the de-
velopment environment [17]. Given the time necessary to complete
the tasks, we elected not to require additional time from the subjects
to gather qualitative data; hence we base these observations on the
interaction data only. While we hope they help the reader garner
more information toward possible causes for our observed effects
(e.g., why did documentation help those with static typing?), the
reader should interpret this section as a collection of possible clues,
not deÔ¨Ånitive answers.
First, our previous studies indicated that the number of Ô¨Åle switches
‚Äúmight‚Äù be related to observed differences in productivity and we
wanted to recheck this hypothesis here. Thus, we ran another two-
factor ANOV A with the number of Ô¨Åle switches as the dependent639variable, and type system and documentation as independent vari-
ables (Table 7, top). Both type system (p = .067) and documenta-
tion (p=.064) approach signiÔ¨Åcance, with small effect sizes ( 2
p=.151
and2
p=.155, respectively), with the interaction non-signiÔ¨Åcant (p =
.129). Table 7 also shows results for the number of Ô¨Åle switches, ex-
cluding the number of switches to documentation (code switches).
Table 7: Univariate ANOV A on Ô¨Åle switches (FS) and code
switches (CS)
F p-value partial 2FSType System 3.73 .067 .151
Documentation 3.86 .064 .155
TS * Doc 2.50 .129 .106CSType System 2.316 .143 .099
Documentation 8.344 .007 .298
TS * Doc 1.362 .256 .061
Besides metrics related to switches between Ô¨Åles, which remain
at least suggestive that this metric might be related to the observed
differences with static/dynamic typing with or without documenta-
tion, we also analyzed additional time metrics during the sessions.
For this, we have the following broad observations:
Documentation. Participants with documentation and static
typing (group 1) and dynamic typing (group 3) spent a simi-
lar proportion of time reading documentation (approximately
20%). Both groups used the documentation less in the sec-
ond half of the session. However, participants in the static
group showed a much stronger drop in documentation usage
in the second half of the session. This result could indicate
that the API might be learned more quickly by this group.
Whether this learning occurs is unclear, but could be worthy
of more study.
Source Ô¨Åles. Participants in all groups spent most of their
time in source Ô¨Åles which were relevant to the task. In short,
participants quickly realize whether a Ô¨Åle is not related to the
task. In such a situation the participants navigate again back
to a Ô¨Åle that is related to the programming task.
Test runs. Similar to our previous experiments, we found
that participants using dynamically typed languages conducted
more test runs. Documentation did not appear to have an im-
pact. While the number of test runs might not be relevant for
the present study it might be important in other studies where
the test case execution takes a larger amount of time.
Again, while we offer these more informal observations to try to
give the reader some insight into potential causes, we do so with
an air of caution. Like in all experiments, we had experimental
controls only for the independent variables discussed in this work.
As such, while these results might be suggestive that Ô¨Åle switches,
and potentially learning effects related to static typing, might be
related to our observations, more study is needed to be certain.
6. DISCUSSION
Our experiment comparing the impact of API documentation and
static type systems had the following results:
Subjects who had access to the documentation accessed be-
tween 54 and 109 times the documentation and spent be-
tween 12 and 22 minutes viewing it (respectively between15% and 22% of the development time). From that we con-
clude that the provided documentation was actually used.
The presence of a static type system had a signiÔ¨Åcant posi-
tive effect on development time: Subjects using the statically
typed language required between 15 and 89 minutes less time
for solving the task.
The effect of the presence of documentation approached sig-
niÔ¨Åcance on development time.
Both, the static type system as well as documentation, re-
duced pure coding time signiÔ¨Åcantly. The differences be-
tween the static and the dynamic type system as well as with
and without documentation were comparable (approximately
between 10‚Äì90 minutes).
A possible explanation of the results pro-static type system is
that developers with a static type system (i.e. where the parameter
and return types are explicitly declared in the API) have a beneÔ¨Åt
because they see what types are required and requested. Hence, in-
stead of looking into different code fragments in order to Ô¨Ånd out
what kind of data they need to use in the API (which requires ad-
ditional time), developers are directly guided to the corresponding
types. This effect is so strong that it exists independent of whether
or not documentation is available. For us the important implication
of this is, that previous experiments, which did not take documen-
tation into account, were not falsiÔ¨Åed‚Äîthe large beneÔ¨Åt of static
type systems exists in the presence of documentation.
The interpretation of the pro-documentation effect is slightly harder.
First, it looks as if the documentation effect is not particularly strong
because the effect on development time is only approaches signiÔ¨Å-
cant. However, when reading time is subtracted from the develop-
ment times, the existence of documentation becomes signiÔ¨Åcant ad-
vantage (which is not particularly unexpected). Hence, it looks like
the reading time in the experiment was (still) large and inÔ¨Çuenced
the experiment results in an undesired way. We believe that future
experiments should choose either a larger programming task or a
more precise documentation in order to measure the documentation
effect. One other plausible approach for analyzing the documenta-
tion impact might be conduct multiple sessions with programmers
in a longitudinal sense. This approach, while it could be insightful,
would obviously be time consuming for experimenters.
Additionally, three subjects in the experiment were not able to
complete the programming tasks‚Äînone of these subjects had docu-
mentation available. This could potentially indicate that documen-
tation may increase the correctness of the tasks. In addition, since
these subjects were considered to have Ô¨Ånished just as they ran out
of time, we may have underestimated their completion times.
Our exploratory study allowed us to gain additional insights. As
in previous experiments, a possible explanation for the differences
we see is that both documentation and static typing cause less sys-
tem browsing, which could result in less time spent. We admit read-
ily, however, that this hypothesis appears to be a reasonable inter-
pretation of the data, but that we did not control for Ô¨Åle switches in
the study, and as such, this result is preliminary. We could imagine,
for example, an experiment being conducted with multiple classes
in the same Ô¨Åle (e.g., a series of nested inner classes), where Ô¨Åle
switches would not be necessary. On the other hand, in such an
experiment, we might plausibly see a larger amount of scrolling or
a different kind of navigation in such a case.
The second insight found in our exploratory study is that the
API documentation tends to be browsed less in the second half of
the session. This effect is much more pronounced in static typing
users. This might be interpreted as subjects learning the API over640time. The experiment of Arisholm [1] had a related Ô¨Ånding: their
experiment had several task, and they hypothesize that learning ef-
fects led to increased performance (in terms of correctness) in the
last task.
7. THREATS TO V ALIDITY
Results aside, like any scientiÔ¨Åc experiment, it is not possible
for any one study to fully Ô¨Çesh out all potential confounding vari-
ables in regards to a particular research topic. At best, empirical
work provides a snapshot of carefully controlled variables which,
if done well, can help other researchers Ô¨Ånd answers to deeply dif-
Ô¨Åcult questions over time. Our study is no exception and we focus
here on what we felt were the most likely validity threats.
Type of documentation. First, the quality and shape of soft-
ware documentation can vary considerably. In the study of Roehm
et al. [26], several industrial developers do not trust documenta-
tion and prefer to rely on source code. Dart‚Äôs home page5ac-
knowledges the incompleteness of the documentation and points
to Stack OverÔ¨Çow. Languages like Java and .NET have a much
more extensive coverage, although one study by Maalej and Robil-
lard showed that some API documentation contains bloat and is of
questionable value (e.g., rehashes method names, includes trivial
examples) [16].
In our case, we wrote correct documentation that was equivalent
across groups, was grammatically understandable (in German), and
that covered what we felt were the most important points in re-
gards to learning an API. Our documentation contained examples
and scenarios that we thought would be useful. In other words, we
set out to write what seemed to us to be ‚Äúgood‚Äù documentation. We
would be unsurprised if systems that have misleading, incorrect, or,
more abstractly, poor documentation, might not beneÔ¨Åt program-
mers. For example, a pilot study by Leotta et al. suggests that out
of date, or as they authors say, misaligned, UML documents can
impact programmer productivity [15].
Type of type system. We have chosen Dart‚Äôs type system in the
experiment which has a number of parallels to Java‚Äôs type system:
the types are declared in the code and identiÔ¨Åed by type names. It
might be possible, perhaps likely, that the measured effect for dif-
ferent kinds of type systems is different, but other research groups
must actually prove this using similar experiments to be certain.
Usefulness of documentation. Besides the quality of the docu-
mentation, we must also consider whether the participants found it
useful. We tracked carefully whether participants switched to doc-
umentation and have provided evidence showing as such in RQ1.
However, we can not assess whether the documentation was useful,
correctly used, or comprehended, beyond that.
Single task. Our choice of a single task limits the generalizabil-
ity of our results. Previous results have shown that not all tasks lend
themselves well to static typing, for example [14]. Given this, we
would be unsurprised if different types of tasks impacted program-
mers differently. In this experiment, we chose a single task that
previous studies show lends itself to static typing and evaluated the
impact once documentation was introduced. As the showed, at least
for this task, the effect was strengthened.
Choice of experimental design. In previous studies, we tracked
participants over a set of tasks, conducting what is commonly termed
a repeated measures experiment [14]. In such experiments, partici-
pants complete a set of tasks repeatedly, which likely leads to learn-
ing over time. Such studies have pros and cons, namely in that they
provide a view of how a system can be learned (see e.g., Arisholm
et al. for an example with UML [1]), but may not capture potential
5https://www.dartlang.orgconfounds related to how a system is learned. In contrast, single
tasks provide a snapshot of developers as they use a tool once, but
may not provide much insight into how a system can be used with
continuous usage. Such pros and cons are well known in a variety
of academic disciplines and are discussed by V ogt [32].
The role of expertise. Like previous experiments, we used stu-
dents late in the academic pipeline for our experiment. While one
study suggests that this class of user might be at least representa-
tive of professionals [13], recent research has shown, with some
conÔ¨Ådence, that novices have difÔ¨Åculty with type annotations in
statically typed languages [8, 28]. Likewise, experts in a certain
dynamic languages may not be as strongly affected by the lack of
static typing or documentation, although to our knowledge this has
not been studied formally. As such, the reader should not assume
that our Ô¨Åndings generalize to all levels of expertise.
Choice of environments. The fact that we restricted our par-
ticipants to a simple IDE may not reÔ¨Çect the state of the practice.
Eclipse allows very quick navigation between API elements and
their documentation (including hovering to get documentation [7]),
for JavaDoc-style documentation. However including a state-of-the
art IDE adds a variety of confounding factors related to the exper-
tise of subject with tools. Some subjects may not be aware of the
existence of tools, usage data show that even highly-praised tools
such as Eclipse‚Äôs ‚Äúopen resource‚Äù tool are not used by 88% of de-
velopers [18].
Another issue is that we evaluated participants as they used a sin-
gle monitor, Ô¨Çipping between documentation and computer code.
We ultimately chose to use a single monitor because many individ-
uals do program in this manner (e.g., on a laptop). If the experi-
ment were re-run with dual monitors, participants might have had
documentation on one monitor and code on the other, potentially
minimizing time to Ô¨Çip between them.
8. CONCLUSION
API usability is an active Ô¨Åeld of research. There is evidence
in the literature of two factors that contribute to it: the presence
of documentation, and the presence of type annotations. We con-
ducted a study comparing the impact of documentation and static
type systems on API usability as measured by development time.
The study had two components: a 2x2 randomized controlled trial,
and an exploratory study of the subjects‚Äô interaction logs.
Overall, we found that both static type systems and documen-
tation had a positive effect; however, the effect of static type sys-
tems was stronger, as it was not balanced by the need for additional
reading that documentation exhibits. As a result, the effect of doc-
umentation only approached signiÔ¨Åcant.
Overall our results conÔ¨Årm and expand on previous Ô¨Åndings.
It appears that adding documentation to statically typed systems
strengthens the effects we previously observed [17]. In a sense,
static typing appears to be even more effective when documenta-
tion is added, as both attributes appear to reinforce each other.
9. ACKNOWLEDGEMENTS
We would like to thank the volunteers from the University of
Duisburg‚ÄìEssen and from the University of Koblenz‚ÄìLandau for
participating in the experiment. Last but not least, we especially
thank Ralf L√§mmel from the University of Koblenz‚ÄìLandau for his
kind support.
10. REFERENCES
[1] E. Arisholm, L. C. Briand, S. E. Hove, and Y . Labiche. The
impact of UML documentation on software maintenance: An641experimental evaluation. IEEE Trans. Software Eng. ,
32(6):365‚Äì381, 2006.
[2] R. K. Bandi, V . K. Vaishnavi, and D. E. Turk. Predicting
maintenance performance using object-oriented design
complexity metrics. IEEE Trans. Software Eng. ,
29(1):77‚Äì87, 2003.
[3] F. P. Brooks, Jr. No silver bullet - essence and accidents of
software engineering. IEEE Computer , 20(4):10‚Äì19, 1987.
[4] K. B. Bruce. Foundations of object-oriented languages:
types and semantics . MIT Press, Cambridge, MA, USA,
2002.
[5] B. Dagenais and M. P. Robillard. Creating and evolving
developer documentation: understanding the decisions of
open source contributors. In Proceedings of SIGSOFT FSE
2010 , pages 127‚Äì136, 2010.
[6] U. Dekel and J. D. Herbsleb. Improving API documentation
usability with knowledge pushing. In Proceedings of ICSE
2009 , pages 320‚Äì330, 2009.
[7] U. Dekel and J. D. Herbsleb. Reading the documentation of
invoked API functions in program comprehension. In
Proceedings of ICPC 2009 , pages 168‚Äì177, 2009.
[8] P. Denny, A. Luxton-Reilly, and E. D. Tempero. All syntax
errors are not equal. In Proceedings of ITiCSE 2012 , pages
75‚Äì80, 2012.
[9] E. Duala-Ekoko and M. P. Robillard. Asking and answering
questions about unfamiliar APIs: An exploratory study. In
Proceedings of ICSE 2012 , pages 266‚Äì276, 2012.
[10] B. Ellis, J. Stylos, and B. A. Myers. The factory pattern in
API design: A usability evaluation. In Proceedings of ICSE
2007 , pages 302‚Äì312, 2007.
[11] J. D. Gannon. An experimental evaluation of data type
conventions. Commun. ACM , 20(8):584‚Äì595, 1977.
[12] M. Hoppe and S. Hanenberg. Do developers beneÔ¨Åt from
generic types? an empirical comparison of generic and raw
types in Java. In Proceedings of OOPSLA 2013 , pages
457‚Äì474, 2013.
[13] M. H√∂st, B. Regnell, and C. Wohlin. Using students as
subjects-a comparative study of students and professionals in
lead-time impact assessment. Empirical Software
Engineering , 5(3):201‚Äì214, 2000.
[14] S. Kleinschmager, S. Hanenberg, R. Robbes, √â. Tanter, and
A. SteÔ¨Åk. Do static type systems improve the maintainability
of software systems? an empirical study. In Proceedings of
ICPC 2012 , pages 153‚Äì162, 2012.
[15] M. Leotta, F. Ricca, G. Antoniol, V . Garousi, J. Zhi, and
G. Ruhe. A pilot experiment to quantify the effect of
documentation accuracy on maintenance tasks. In
Proceedings of ICSM 2013 , pages 428‚Äì431, 2013.
[16] W. Maalej and M. P. Robillard. Patterns of knowledge in API
reference documentation. IEEE Trans. Software Eng. ,
39(9):1264‚Äì1282, 2013.
[17] C. Mayer, S. Hanenberg, R. Robbes, √â. Tanter, and A. SteÔ¨Åk.
An empirical study of the inÔ¨Çuence of static type systems on
the usability of undocumented software. In Proceedings ofOOPSLA 2012 , pages 683‚Äì702, 2012.
[18] E. R. Murphy-Hill, R. Jiresal, and G. C. Murphy. Improving
software developers‚Äô Ô¨Çuency by recommending development
environment commands. In Proceedings of SIGSOFT FSE
2012 , page 42, 2012.
[19] V . Pankratius, F. Schmidt, and G. Garreton. Combining
functional and imperative programming for multicore
software: An empirical study evaluating Scala and Java. In
Proceedings of ICSE 2012 , pages 123‚Äì133, 2012.
[20] C. Parnin, C. Treude, L. Grammel, and M.-A. Storey. Crowd
documentation: Exploring the coverage and the dynamics of
API discussions on stack overÔ¨Çow. Technical Report
GIT-CS-12-05, Georgia Institute of Technology, May 2012.
[21] M. Petre. UML in practice. In Proceedings of ICSE 2013 ,
pages 722‚Äì731, 2013.
[22] B. C. Pierce. Types and programming languages . MIT Press,
Cambridge, MA, USA, 2002.
[23] L. Prechelt and W. F. Tichy. A controlled experiment to
assess the beneÔ¨Åts of procedure argument type checking.
IEEE Trans. Softw. Eng. , 24(4):302‚Äì312, 1998.
[24] L. Prechelt, B. Unger-Lamprecht, M. Philippsen, and W. F.
Tichy. Two controlled experiments assessing the usefulness
of design pattern documentation in program maintenance.
IEEE Trans. Software Eng. , 28(6):595‚Äì606, 2002.
[25] M. P. Robillard and R. DeLine. A Ô¨Åeld study of API learning
obstacles. Empirical Software Engineering , 16(6):703‚Äì732,
2011.
[26] T. Roehm, R. Tiarks, R. Koschke, and W. Maalej. How do
professional developers comprehend software? In
Proceedings of ICSE 2012 , pages 255‚Äì265, 2012.
[27] S. Spiza and S. Hanenberg. Type names without static type
checking already improve the usability of APIs (as long as
the type names are correct): An empirical study. 2014. To
appear.
[28] A. SteÔ¨Åk and S. Siebert. An empirical investigation into
programming language syntax. ACM Transactions on
Computing Education , 13(4):19:1‚Äì19:40, Nov. 2013.
[29] T. Tenny. Program readability: Procedures versus comments.
IEEE Trans. Software Eng. , 14(9):1271‚Äì1279, 1988.
[30] W. F. Tichy. Hints for reviewing empirical work in software
engineering. Empirical Software Engineering , 5(4):309‚Äì312,
2000.
[31] E. Tryggeseth. Report from an experiment: Impact of
documentation on maintenance. Empirical Software
Engineering , 2(2):201‚Äì207, 1997.
[32] W. P. V ogt. Quantitative Research Methods for Professionals
in Education and Other Fields . Allyn and Bacon, 2006.
[33] R. Wettel, M. Lanza, and R. Robbes. Software systems as
cities: a controlled experiment. In Proceedings of ICSE
2011 , pages 551‚Äì560, 2011.
[34] C. Wohlin, P. Runeson, M. H√∂st, M. C. Ohlsson, and
B. Regnell. Experimentation in Software Engineering .
Springer, 2012.642