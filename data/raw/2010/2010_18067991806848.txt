AURA: A Hybrid Approach to Identify
Framework Evolution
Wei Wu1, Y ann-Gaël Guéhéneuc1, Giuliano Antoniol2, and Miryung Kim3
1Ptidej Team, DGIGL, École Polytechnique de Montréal, Canada
2SOCCER Lab, DGIGL, École Polytechnique de Montréal, Canada
3ECED, The University of Texas at Austin, USA
E-mail: {wuwei,guehene}@iro.umontreal.ca,
giuliano.antoniol@polymtl.ca, miryung@ece.utexas.edu
ABSTRACT
Software frameworks and libraries are indispensable to to-
day's software systems. As they evolve, it is often time-
consuming for developers to keep their code up-to-date, so
approaches have been proposed to facilitate this. Usually,
these approaches cannot automatically identify change rules
for one-replaced-by-many and many-replaced-by-one meth-
ods, and they trade o® recall for higher precision using one
or more experimentally-evaluated thresholds. We introduce
AURA, a novel hybrid approach that combines call depen-
dency and text similarity analyses to overcome these limita-
tions. We implement it in a Java system and compare it on
¯ve frameworks with three previous approaches by Dagenais
and Robillard, M. Kim et al., and Sch Äaferet al. The compar-
ison shows that, on average, the recall of AURA is 53.07%
higher while its precision is similar, e.g., 0.10% lower.
1. INTRODUCTION
Software frameworks1and libraries are widely used in
software development for cost reduction. They evolve con-
stantly to ¯x bugs and meet new requirements. In theory,
the Application Programming Interface (API) of the new
release of a framework should be backward-compatible with
its previous releases, so that programs linked2to the frame-
work continue to work with the new release. In practice,
the API syntax and semantics change [2, 8, 21]. For ex-
ample, from JHotDraw 5.2 to 5.3, method CH.ifa.draw.
figures.LineConnection.end() was replaced by LineCon-
nection.getEndConnector() ; such a change may have di-
rect consequences on a program using the JHotDraw frame-
1Without loss of generality, we use the term \framework" to
mean both frameworks and libraries.
2We refer readers to [11] for a discussion on the links between
frameworks and programs.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
ICSE ’10, May 2-8 2010, Cape Town, South Africa
Copyright 2010 ACM 978-1-60558-719-6/10/05 ...$10.00.work, such as compile errors, or indirect ones, such as run-
time errors if invoking a deleted method using re°ection.
To prevent backward-compatibility problems, developers
may delay or avoid using a new release. Yet, if they want
to bene¯t from new features or security patches, they must
evolve their programs. This evolution process often requires
a lot of e®ort because developers must dig into the docu-
ments and/or source code of the new and previous releases
to understand their di®erences and to make their programs
compatible with the new release.
Consequently, many approaches have been developed to
ease this evolution process and reduce the developers' e®orts.
Some require that the framework developers do additional
work, such as providing explicit change rules with annota-
tions [3], or that they record API updates to the framework.
[9, 13, 15]. To reduce the framework developers' involve-
ment, some approaches automatically identify change rules
that describe a matching between target methods, i.e., meth-
ods existing in the old release but not in the new one, and
replacement methods in the new release [1, 5, 6, 7, 10, 12,
14, 17, 16, 19, 20, 22, 23].
However, framework developers may not be willing to
build change rules manually or use speci¯c tools. Also, some
previous approaches [5, 20] cannot detect change rules for
target methods not used within the previous releases of the
framework and program. Others, such as [16], cannot iden-
tify replacement methods if the names of the old and new
releases are not similar enough. Still others [5, 12, 17, 16,
20, 23] require context-dependent thresholds which are cho-
sen through experimental evaluations and may not apply in
di®erent contexts.
Furthermore, no existing approaches can automatically
handle one-replaced-by-many (\one-to-many" in the follow-
ing) or many-replaced-by-one (\many-to-one") change rules,
as illustrated in the following section. It is important to
identify these one-to-many and many-to-one change rules,
because they can guide developers towards new functionali-
ties in the new release.
In particular, developers should be provided with as many
relevant change rules as possible to save their e®orts to iden-
tify appropriate rules from a potentially very large code base.
Thus, an approach should have the maximum recall without
decreasing its precision. Indeed, it is easier for a developer
to discard an inappropriate change rule among a couple of
325
h
undred rules than to identify an appropriate change rule
among thousands of possible method pairs.
Consequently, we propose a novel hybrid language- and
context-independent approach, AURA (AUtomatic change
Rule Assistant), that combines the advantages and over-
comes the limitations of previous approaches:
1. It increases recall by combining call dependency and
text similarity analyses in a multi-iteration algorithm;
2. It automatically adapts to di®erent frameworks by not
using any experimentally-evaluated threshold;
3. It reduces developers' e®orts by automatically gener-
ating one-to-many and many-to-one change rules.
Using a detailed evaluation on four medium-size real-world
systems, we show that the percentage of one-to-many and
many-to-one change rules covers 8.08% of the total number
of target methods. Moreover, the results of the evaluation
show that the combination of call dependency and text sim-
ilarity analyses into a multi-iteration algorithm improves,
on average, recall by 53.07% in comparison to previous ap-
proaches with a slight decrease of 0.10% in precision. In our
evaluation, we also apply AURA to Eclipse and compare its
results with those of SemDi® developed by Dagenais and
Robillard [5]. We show that the approximated precision of
AURA is 92.86% while SemDi®'s is up to 100.00%.
In the remainder of this paper, Section 2 presents moti-
vating examples that illustrate the limitations of previous
approaches. Section 3 discusses related work. Section 4
describes our approach while Section 5 evaluates it on ¯ve
real-world systems. Section 6 discusses open issues and Sec-
tion 7 concludes this paper and discusses future work.
2. MOTIV ATING EXAMPLES
We illustrate the advantages of AURA with the following
motivating examples.
Multi-iteration Algorithm.
Let us assume that a developer must adapt her Client
program from using Eclipse JDT 3.1 to its 3.3 release, as
shown in Figure 1. The method Indents.computeIndent-
Length(...) was called in 3.1. However this method on
longer exists in 3.3. Using an automatic approach, the de-
veloper would expect to obtain the following change rules:
(1) Indents.getChangeIndentEdits(...)
ª IndentManipulation.getChangeIndentEdits(...)
(2) Indents.computeIndentLength(...)
ª IndentManipulation.indexOfIndent(...)
where ªmeans \should be replaced with".
Previous approaches, using either text similarity [16] or
call dependency [5, 20] analyses, could provide the devel-
oper with the ¯rst change rule but would not readily suggest
the second one, because the method signatures are not sim-
ilar enough and the callers of the methods changed as well.
AURA would report the two change rules above.
With its multi-iteration algorithm, AURA detects that
Indents.getChangeIndentEdits(...) is replaced by Ind-
entManipulation.getChangeIndentEdits(...) in the ¯rst
iteration. Then, in the following iteration, using the ¯rst
change rule, AURA also reports that Indents.computeInd-// Version 3.1
package org.eclipse.jdt.internal.core.dom.rewrite;
public class SourceModifier implements ISourceModifier {
public ReplaceEdit[] getModifications(String source){
...
return Indents.getChangeIndentEdits(...);
}
}
package org.eclipse.jdt.internal.core.dom.rewrite;
class Indents {
void getChangeIndentEdits(...) {
...
int length= Indents.computeIndentLength(...);
...
}
}
// Version 3.3
package org.eclipse.jdt.internal.core.dom.rewrite;
public class SourceModifier implements ISourceModifier {
public ReplaceEdit[] getModifications(String source){
...
return IndentManipulation.getChangeIndentEdits(...);
}
}
package org.eclipse.jdt.core.formatter;
class IndentManipulation {
void getChangeIndentEdits(...) {
...
int length= this.indexOfIndent(...);
...
}
}
Figure 1: Example of many iterations
// Version 5.2
protected JMenu createEditMenu() {
...
menu.add(new CutCommand("Cut", view()),
new MenuShortcut('x'));
...
}
// Version 5.3
protected JMenu createEditMenu() {
...
menu.add(new UndoableCommand(
new CutCommand("Cut", this)),
new MenuShortcut('x'));
...
}
Figure 2: Example of a one-to-many change rule.
entLength(...) is replaced by IndentManipulation.index-
OfIndent(...) .
One-to-many Change Rules.
Let us assume that a developer must adapt a program
built on top of JHotDraw 5.2 to its 5.3 release. The program
adds some new commands to the framework. For the sake of
simplicity, let us use CutCommand in Figure 2. Syntactically,
this command would not compile with the new release be-
cause the expected signature of commands has changed. Se-
mantically, release 5.3 introduces an undo{redo mechanism
that should be used by the new command if appropriate.
Therefore, the developer would expect to obtain the follow-
ing change rule automatically, which advices the developer
to consider making the command undoable.
(1) CutCommand.CutCommand(DrawingView...)
ª CutCommand.CutCommand(Alignment, DrawingEditor)
and UndoableCommand.UndoableCommand(Command)
Previous approaches, using either text similarity [16] or
326ca
ll dependency [5, 20] analyses, would only report a change
rule from CutCommand.CutCommand(DrawingView...) toCut-
Command.CutCommand(Alignment, DrawingEditor) ,i.e., a
rule ¯xing the syntactic di®erence. They would not help
the developer in spotting the new feature provided by the
framework with its new feature of undoable commands.
AURA reports a one-to-many change rule that suggests re-
placing the target method CutCommand.CutCommand(Drawing-
View...) with calls to the replacement methods CutCommand.
CutCommand(Alignment,DrawingEditor) andUndoableCom-
mand.UndoableCommand(Command) . Figure 2 illustrates the
new implementation where an UndoableCommand now encap-
sulates CutCommand.
Many-to-one Change Rules.
Let us assume that a developer must adapt a program
built on top of JEdit 4.1 to its 4.2 release and the program
called methods DirectoryMenu.DirectoryMenu(...) ,Mar-
kersMenu.MarkersMenu() , and RecentDirectoriesMenu.Re-
centDirectoriesMenu() , which are replaced by EnhancedMe-
nu.EnhancedMenu(...) in the release 4.2.
With previous approaches [16, 20] that generate one-to-
one change rules, the developer could know that Directory-
Menu.DirectoryMenu(...) is replaced by EnhancedMenu.-
EnhancedMenu(...) , but would need to ¯nd the other two
methods manually. Some previous approaches, such as [16],
might produce erroneous change rules for the other two tar-
get methods due to their high textual similarity with other
irrelevant methods.
With AURA, the developer will be informed that the three
methods are replaced by EnhancedMenu.EnhancedMenu(...) ,
which frees her from manually searching for replacements or
relying on incorrect suggestions.
3. RELATED WORK
Several approaches help developers evolve their programs
when the frameworks that they use change. We studied
these approaches and identi¯ed eight features. Table 1 sum-
marizes the di®erent approaches according to their features
and highlight the advantages of AURA. We now further de-
¯ne and discuss the di®erent features and approaches.
Capturing API Updates.
Existing approaches of capturing API-level changes either
require the framework developers' e®orts by manually speci-
fying the change rules or by requiring them to use a particu-
lar IDE to automatically record the refactorings performed.
Chow and Notkin [3] presented a method that requires the
framework developers to provide change rules with the new
releases. CatchUP! [13] and JBuilder [15] record the refac-
toring operations in one release and replay them in another.
MolhadoRef [9] also employs a record-and-replay technique
for handling API-level changes in merging program versions.
These approaches are able to provide accurate change rules
because of the framework developers' involvement, which
might not always be available.
Matching Techniques.
Previous approaches use di®erent code matching techniques
to ¯nd change rules between old and new releases. Dagenais
and Robillard developed SemDi® [5], which suggests adap-
tation to clients by analyzing how a framework adapts toits own changes. Sch Äaferet al. [20] mined framework-usage
change rules from already ported instantiations. These two
previous approaches compute support and con¯dence value
on call dependency analysis. Godfrey and Zou [12] presented
a semi-automatic hybrid approach to perform origin analysis
using text similarity, metrics, and call dependency analyses.
S. Kim et al. [17] automated Godfrey and Zou's approach.
Di®-CatchUp, developed by Xing and Stroulia [24], analy-
ses textual and structural similarities of UML logical design
models to recognise API changes. M. Kim et al. 's [16] ap-
proach leveraged systematic renaming patterns to match old
APIs to new APIs.
Many-to-one and one-to-many.
Godfrey and Zou [12] detected three cases of merging
(Clone Elimination, Service Consolidation, Pipeline Con-
traction) and three cases of splitting (Clone Introduction,
Service Extraction, Pipeline Expansion). We extend merg-
ing/splitting to many-to-one/one-to-many change rules. The
di®erence between merging/splitting and many-to-one/one-
to-many change rules is that the former is limited to cases
de¯ned by Godfrey and Zou [12], while the latter includes
any case, e.g., new functionality. SemDi® [5] and Di®-Catch-
Up [24] are able to report many-to-one and one-to-many
change rules but are semi-automatic, i.e., developers must
manually select correct replacements from a provided candi-
date list. M. Kim et al.'s approach [16] automatically reports
many-to-one rules.
Simply Deleted.
Simply-deleted target methods have no replacement meth-
ods in the new release. Semi-automatic approaches [5, 12,
24] and those that require framework developers' involve-
ment [3, 9, 13] are able to report simply-deleted rules. Au-
tomatic approaches [17, 16, 20] do not report this type of
change rule explicitly in their result.
Automatic and Thresholds.
All automatic approaches [17, 16, 20], except record-and-
replay ones, use thresholds to keep a balance between preci-
sion and recall. Typically, they use experimentally-evaluated
thresholds to ¯lter out candidate replacement methods, thus
potentially increasing precision but decreasing recall.
Types of Changes.
SchÄaferet al. [20] classi¯ed changes between old and new
releases into 12 change patterns. We summarize them into
three categories of change rules: (1) method rules: all the
targets and replacements of a change rule are methods; (2)
¯eld rules: all the targets and replacements of a change rule
can be methods or ¯elds; (3) inheritance rules: the inheri-
tance relation changes. We report the types of changes found
by each approaches and compare the results in Section 5.
Summary.
AURA overcomes the following limitations of existing ap-
proaches:
²Text similarity-based approaches cannot detect replace-
ment methods that do not share similar textual names
with their target methods.
²Call dependency-based approaches cannot detect re-
327Appr oachesF
eatures
Main One-to- Many-Simply-
FDI Matching many to-onedeleted Methods Fields Inheritance Auto- Thres-
T
echnique Rule
s Ru
les Rule
s Rel
ations mati
chold
s
Cho
wet al. [3] Y
es A No No Y
es Y
es No No No No
SemDi®
[5] No CD Y
es Y
es Y
es Y
es No No No Y
es
Go
dfrey et al. [12] No TS,
M, and CD Y
es Y
es Y
es Y
es No No No Y
es
Catc
hUp! [13] Y
es N/A No No No Y
es Y
es Y
es Y
es No
M.
Kim et al. [16] No TS No Y
es Y
es Y
es No No Y
es Y
es
S.
Kim et al. [17] No TS,
M, and CD No No No Y
es No No Y
es Y
es
Sc
hÄafer et al. [20] No CD No No No Y
es Y
es Y
es Y
es Y
es
Di®-C
atchUp [24] No TS
and SS Y
es Y
es Y
es Y
es Y
es Y
es No Y
es
A
URA No CD
and TS Y
es Y
es Y
es Y
es No No Y
es No
T
able 1: Feature comparison. (A = Annotation, CD = Call Dependency, FDI = Framework Developer
Involvement, M = Metrics, N/A = Not Applicable, TS = Text Similarity, SS = Structural Similarity)
placement methods for target methods that are not
used in frameworks and linked programs.
²No approach can automatically detect many-to-one and
one-to-many change rules.
²All automatic approaches except record-and-replay use
thresholds set through experimental evaluations, which
may not apply in any context.
4. OUR APPROACH
Our approach is based on call-dependency and text-simila-
rity analyses and a multi-iteration algorithm. We choose
call dependency and text similarity as the main matching
techniques of our hybrid approach for two reasons: previous
approaches using these analyses have good precision [5, 16,
20]; these techniques are compatible with each other because
they apply directly to source code.
The assumption of our approach is that a target method is
deleted or replaced by one or more replacement methods and
more than one target method can be replaced by the same
replacement method. All replacement methods are taken
from the candidate set of all methods existing in the new
release of a framework or belonging to other frameworks
provided by the same vendor. We do not consider meth-
ods from the frameworks of di®erent vendors. For example,
when we analyze org.eclipse.jdt.core , the methods from
other Eclipse plug-ins, such as org.eclipse.jface , belong
to the candidate replacement method set, but those from
Sun Java Foundation Classes (JFC) do not.
We include the methods from the frameworks provided
by the same vendor only, because framework developers may
move methods between their frameworks. This inclusion is a
good trade-o® between accuracy and performance, because a
large candidate set compromises performance but increases
precision. After analyzing the results of the four medium-
size subject systems in the evaluation, we found that less
than 1% of all methods were replaced by those from the
frameworks of other vendors.
4.1 Preliminary
Call dependency analysis discovers the calls between the
methods of frameworks and the programs using them. These
calls re°ect the behavior of frameworks more accurately than
text similarity, in particular when detecting many-to-one
and one-to-many change rules.
To illustrate the call-dependency analysis used in our ap-
proach, let us de¯ne an anchor as either (1) a pair of meth-ods with the same signature (including return type, declar-
ing module, name, and parameter lists) that exist in both
the old and new releases of the framework or (2) a pair of
methods already identi¯ed as target and replacement meth-
ods. We also de¯ne two predicates for an anchor a:
OLD(a) = the method of ain the old release
NEW(a) = the method of ain the new release
In the following, we note m1!m2 if a method m1 calls a
method m2 (9 if it does not). We compute the con¯dence
value (CV) for a given target method tand its candidate
replacement method cas:
CV(t; c) =A(t; c)
A(t),
with:
A(t) = jjfajais an anchor ^OLD (a)!tgjj
A(t; c) = jjfajais an anchor
^OLD (a)!t^NEW (a)!cgjj
where jjSjjrepresents the cardinality of S. The con¯dence
value represents the call-dependency similarity of a target
method and its candidate replacement methods.
To compute the text similarity of two methods, we tok-
enize each method signature as proposed by Lawrie et al.
[18] by splitting them at upper-case letters and other legal
characters (except lower case letter and numbers), for exam-
ple ` '
and `$' in Java. Based on the tokenized signatures,
our text similarity algorithm computes the similarity of two
methods using ¯rst their signatures (return types, declar-
ing modules, names, and parameter list), then their Lev-
enshtein Distance (LD), and ¯nally, their Longest Common
Subsequence (LCS). When we compare the text similarities
of two candidate replacement methods to a target method,
we ¯rst compare their signature-level similarity. If they are
di®erent, we do not compute their LD and LCS. We apply
the same strategy to LD and LCS.
We combine LD and LCS to compare the text similarity
between two methods, because LD and LCS pertain to two
di®erent aspects of string comparison: LD is concerned with
the di®erence between strings but is not able to tell if they
have something in common, while LCS focuses on their com-
mon part but is not able to tell how di®erent they are. For
example, let us assume that we want to identify the string
most similar to abbetween a,abc, and abcd. Both aand
abchave the same LD and both abcandabcd have the same
LCS. Thus, by combining LD and LCS, we can identify that
abcis most similar to ab.
3284.2Algorithm
Using the previous call dependency and text similarity
analyses, AURA generates change rules from the old to the
new release of a framework in the following steps:
1. Global Data Set Generation By di®erentiating the
sets of method signatures in the old and new releases, we
build the set of target methods, Stm; the set of anchors,
Sa; and, the set of global candidate replacement methods,
Sgcrm, which includes all the methods de¯ned in the new re-
lease. The target methods, whose change rules were already
detected in previous iterations are not included in Stmand
whose replacements are excluded from Sgcrm, were added to
Saafter being detected.
2. Target Methods Classi¯cation. Using call-dependency
analysis, we divide Stmin:
Stmca =ftja2Sa;9OLD (a)!tg
Stmuca =ftja2Sa;@OLD (a)!tg
with: Stm=Stmca[Stmuca :
3. Candidate Replacement Method Set Generation.
Also, using call-dependency analysis, for each target method
tinStmca, we build the set of corresponding candidate re-
placement methods in the new release, using the predicate:
CRMS (t) = fmjm2Sgcrm^a2Sa
^OLD(a) !t
^OLD(a)9m
^NEW(a) !mg:
4. Con¯dence Value Computation. We compute the
con¯dence value of each candidate replacement method cin
CRMS (t), with respect to its corresponding target method
t, with t2Stmca. We then generate change rules for all
target methods in Stmca using the con¯dence values and
jjHCS(t )jj, where HCS(t ) =fcjc2CRMS (t);CV(t; c) =
100% g, as follows:
4a.8tj jjHCS (t)jj= 1. We build the change rule tªc
and add it to Sa(in the form of an anchor). If Sadoes not
change, we stop iterating and go to the next step, or we go
back to Step 1.
4b.8tj jjHCS(t )jj>1. The relation between tand
its candidate replacement methods is one-to-one or one-to-
many. We assign the proper candidate replacement meth-
ods using text similarity analysis and the number N(m; a; t )
of times that tand its candidate replacement methods are
called in their anchors, in two steps:
4b1. Key-replacement Methods Identi¯cation.
We use text-similarity to identify key-replacement methods
for all t. The key-replacement method KR(t) to tis the
only method that is the most similar to tfrom the candidate
replacement methods whose names are equal to t's or from
the methods in HCS(t ).
4b2. Co-replacement Methods Identi¯cation. The
co-replacement methods to tare chosen from CHCS( t) us-
ingN(m; a; t ) and the support S(t; c) de¯ned below. A tar-
get method can have zero or more co-replacement meth-ods regardless of their textual similarity. We de¯ne the
CHCS (t) of co-candidate methods and KAS(t ) of anchors
that call KR(t), and two counters, such as:
CHCS (t) = fcjc2HCS(t )^cis not a key-
replacement method to
any target methods g
KAS(t ) = faja2Sa^NEW (a)!KR(t)g
N(m; a; t ) = jjfNEW(a) !mja2KAS(t )gjj
ALLKR (a) = fkjk is a key-replacement
^NEW(a) !kg
ALLN(t; a ) = jjfNEW(a) !kja2KAS(t )
^k2ALLKR (a)gjj
From an anchor a2KAS(t ), we compute the call count
of the key-replacement of t:m=N(KR(t ); a; t ), the call
count of a candidate method c2CHCS(t ):p=N(c; a; t ),
and the call count of all the key-replacement methods called
ina:q=ALLN( t; a). We compare pwith mandqand
only keep co-candidate methods meeting the two conditions:
²m=p >1:cis called more than one time and exactly
as many as the number of times that KR(t) is called.
In this case, chas a high possibility to collaborate with
KR(t) in the new release.
²q <=p^q >1:cis called as many as (or more than)
the number of times that the key-replacements of all
target methods in the same anchor a, and all the key-
replacements are called more than once. In this case,
cis likely to collaborate with all the key-replacements.
Then, we select the co-candidate methods left in CHCS (t)
with the highest support S(t; c) as the co-replacement meth-
ods, where the support is de¯ned as:
S(t; c) = jjfmj
m2 fall the methods in the new release g
^m!KR(t)^m!cgjj
For a target method whose replacement methods are de-
tected in this step, if its co-replacement methods set is empty,
AURA generates a one-to-one change rule; otherwise it gen-
erates a one-to-many rule.
4c.8tj jjHCS(t )jj= 0. We choose the most similar can-
didate replacement methods to tfrom the methods whose
name is equal to t's inCRMS (t) or from all the methods
inCRMS( t). Then, we choose the candidate methods with
the highest con¯dence value as the replacement methods.
The rules detected by this step could be one-to-many rules
if there is more than one candidate method with same text-
similarity and con¯dence values. In this step, we give text-
similarity analysis priority over con¯dence value, because a
con¯dence value less than 100% indicates a behavior change
in one or more anchors.
5. Text Similarity Only Rule Generation. For each
t2Stmuca , we use text similarity to ¯nd its replacement
methods with the most similar signatures from Sgcrm. If
there is more than one candidate replacement method, we
select one randomly. We could generate one-to-many rule,
if there is more than one candidate method with the same
329Sub
ject Systems Re
leases #
Methods
JF reeChart0.9.11 4,751
0.9.12 5,197
JHot
Draw5.
2 1,4
86
5.
3 2,2
65
JE
dit4.
1 2,7
73
4.
2 3,5
47
St
ruts1.
1 5,9
73
1.
2.4 6,1
11
org.ec
lipse.jdt.core 3.
1 35
,439
or
g.eclipse.jdt.ui 3.
3 47
,237
T
able 2: Subject Systems.
text-similarity to a target. But according to our evaluation,
most relevant cases are one-to-one rules.
6. Simply-deleted Method Rule Identi¯cation Fi-
nally, we examine the target methods in Stmuca . If the
replacement of one of these methods also exists in the old re-
lease, we mark the target method as simply-deleted method,
i.e., a target method with no replacement method in the new
release. We only identify simply-deleted method rules in this
step because target methods in Stmuca have never been used
or their context of use changed between the old and new
releases. Furthermore, their most similar candidate replace-
ment methods are not methods added to the new release.
These target methods are most likely to be simply-deleted.
4.3 Implementation
We implemented our approach as a Java program that
includes two components: Model Builder and Rule Gener-
ator. The former component converts the source code of
the old version and of the new version of a program into the
language-independent AURA Model. The current version of
Model Builder is an Eclipse plugin operating on the abstract
syntax tree generated by the Eclipse Java parser. The latter
component generates change rules using the AURA Model.
It can be used both as an Eclipse plugin or as a standalone
Java program. The executable and source code of AURA
can be found on our Web site5.
5. EV ALUATION
We now evaluate and compare AURA on several systems.
5.1 Design
We evaluated AURA on ¯ve open source systems meeting
the following conditions: (1) di®erent sizes; (2) developed
independently from each other; and, (3) studied in previous
work. The last condition reduces the bias in the selection
of the subject systems and facilitates the comparison with
previous work. Table 2 summarizes the ¯ve subject systems.
We use the four medium size systems (JFreeChart, JHot-
Draw, JEdit, and Struts) to compare AURA with the ap-
proaches of M. Kim et al. [16] and Sch Äaferet al. [20]. We use
the large system ( org.eclipse.jdt.core andorg.eclipse.
jdt.ui) to compare AURA with SemDi® [5].
We reuse the results of the three approaches provided by
their authors because it is impractical to reanalyse all the
target systems and also to avoid experimenter bias.
We include one-to-many change rules by treating them as
one-to-one change rules because the previous approaches do
5w
ww.ptidej.net/downloads/experiments/icse10bnot report such rules. We convert many-to-one change rules
into as many one-to-one change rules as target methods.
5.2 Hypothesis and Performance Indicators
Our hypothesis is that AURA will ¯nd more relevant change
rules than the previous approaches with comparable preci-
sion, i.e., it will have a better recall than and similar preci-
sion to those of the previous approaches.
We cannot use recall and precision [4] directly to compare
the performance of AURA and the previous approaches be-
cause the set relevant rules isa priori unknown in:
Precision =jjfrelevant rules gTfretrieved rules gjj
jjfr
etrieved rules gjj
Recall =jjfrelevant rules gTfretrieved rules gjj
jjfr
elevant rules gjj
Therefore, to eliminate the in°uence of this unknown set,
we de¯ne the set fcorrect rules g, which can be obtained by
manually inspecting the set fretrieved rules gas:
fcorrect rules g=frelevant rules gTfretrieved rules g:
We introduce the di®erences in precision, ¢P, and recall,
¢R, as two functions of the change rules detected by two
di®erent approaches, AandB:
¢P(A; B ) =Precision A¡Precision B
Prec
ision B
=jfcorrect rules gAj £ jf retrieved rules gBj
jfr
etrieved rules gAj £ jf correct rules gBj
¡1
¢R(A; B ) =Recall A¡Recall B
Re
callB
=jfcorrect rules gAj ¡ jf correct rules gBj
jfcor
rect rules gBj
Using ¢P(A; B ) and ¢R (A; B ), we can compare the pre-
cision and recall of two approaches and avoid the in°u-
ence of the unknown set frelevant rules g. We compute
fcorrect rules gfor AURA on four medium-size systems,
JFreeChart, JHotDraw, JEdit, and Struts by manual inspec-
tion. For the previous approaches, we use the data provided
by the corresponding authors.
For the two Eclipse plug-ins, org.eclipse.jdt.core and
org.eclipse.jdt.ui , from 3.1 to 3.3, AURA generates more
than 4,500 change rules. Thus, it is impractical to validate
all these rules manually. We follow Dagenais and Robillard's
evaluation method [5]: choose the same three client pro-
grams of these plug-ins, i.e.,org.eclipse.jdt.debug.ui ,
Mylyn, and JBossIDE; compile them with Eclipse 3.3; use
the change rules found by our approach to solve the compile
errors in scope i.e., compile errors caused by the methods
not existing anymore in release 3.3; and, compute the pre-
cision of the change rules that cover these compile errors.
5.3 Comparison on the Medium-size Systems
In Table 3, we present the ¢P and ¢ Ron each subject
system between AURA and M. Kim et al. 's [16] and Sch Äafer
et al.'s [20] approaches, in column 5 and 6. We then report
the average values for each approach in column 7 and 8. In
the last three rows, we present the total average values of
330Sy
stems Ind
icators A
URA M.
Kim et al. [16] ¢R ¢PAv
erages
¢R ¢P
JHotDraw #Correct rule 97 8119
.49% -
6.69%
53
.21% -5
.66%5.
2-5.3 Pre
cision 92.
38% 99.0
0%
JE
dit #
Correct rule 356 2173
64
.29% -13
.78%4.
1-4.2 Pre
cision 80.
18% 93.0
0%
JF
reeChart #
Correct rule 155 8
875
.86% 3
.50%0.9.
11-0.9.12 Pre
cision 80.
73% 78.0
0%
Sy
stems Ind
icators A
URA Sc
hÄafer et al. [20] ¢R ¢PAv
erages
¢R ¢P
JHot
Draw #
Correct rule 97 8
810
.23% 4
.98%
52
.86% 8.2
4%5.
2-5.3 Pre
cision 92.
38% 88.0
0%
St
ruts #
Correct rule 129 6
695
.49% 11.
50%1.1
-1.2.4 Pre
cision 96.
56% 85.7
0%
T
otal Pre
cision of AURA 88
.25%
Av
erage ¢R 53
.07%
¢P -0.
10%
T
able 3: Comparison of the results on medium-size systems with simply-deleted change rules.
Sy
stems A
URA Se
mDi® [5]
org.e
clipse. #
Errors in Scope 4
jdt.d
ebug.ui #
Found Rules 4 4
3.1
- 3.3 #
Correct Rules 4 4
My
lyn #
Errors in Scope 2
0.5
-2.0 #
Found Rules 2 2
#
Correct Rules 1 2
JB
ossIDE #
Errors in Scope 8
1.
5-2.04#
Found Rules 8 8
#
Correct Rules 8 8
Pr
ecision 92.8
6% ·100:00%
T
able 4: Evaluation of a sample of change rules on
the large system.
AURA compared to the two approaches: ¢R is 53.07% with
a precision of 88.25%, while ¢P is -0.10%.
Comparison with M. Kim et al. ’s [16] approach.
M. Kim et al. [16] present their results in two formats:
¯rst-order relational logic rules, for example \all methods in
class A, replaced by the same name methods in class B, except
methods a()and b()", and matches, for example A.c() ª
B.d(). The latter format corresponds to the change rules of
AURA. Therefore, we use the number of matches from [16]
to compare their results with ours.
On average, ¢ Pis -5.66% while ¢R is 53.21%. We gain
in recall at the small expense of precision.
On JHotDraw from 5.2 to 5.3 and JFreeChart from 0.9.11
to 0.9.12, the ¢ Rs are 19.49% and 75.86% while the ¢Ps are
-6.69% and 3.50%, respectively. These results show that the
combination of call-dependency and text-similarity analy-
ses improves recall with precision comparable to approaches
based on text-similarity analyses. A slight decrease of preci-
sion (-6.69%) is acceptable because the recall increases sat-
isfactorily (19.49%).
On JEdit from 4.1 to 4.2, the ¢R is 64.29% while ¢P
is -13.79%. The ¢P decrease is is twice as much as that
of JHostDraw from 5.2 to 5.3. Two factors cause this de-
crease. First, call-dependency analysis is more sensitive to
structural changes than text similarity analysis. In JEdit
4.2, the API remained quite stable but the implementation
of the methods changed radically. AURA ¯rst uses call de-
pendency analysis that generates irrelevant change rules that
could be avoided if it used text similarity analysis directly.
Second, AURA does not use any experimentally-evaluated
thresholds that would help balancing recall and precision.Comparison with Schäfer et al. ’s [20] approach.
On average, ¢P is 8.24% while ¢ Ris 52.86%. AURA
has positive ¢R and ¢ Pboth on JHotDraw from 5.2 to
5.3 and Struts from 1.1 to 1.2.4 in comparison to Sch Äaferet
al.'s [20]. On JHotDraw from 5.2 to 5.3, the ¢ Rand ¢P
are 10.23% and 5.00%, while they are 95.49% and 11.50%
on Struts from 1.1 to 1.2.4. Text-similarity analysis is the
main contributor to the improvements. In our evaluation,
the change rules of 59.05% target methods (62) of JHotDraw
from 5.2 to 5.3 are detected by call-dependency analysis,
while the number for Struts from 1.1 to 1.2.4 is only 17.04%
(23). Text-similarity analysis generates the change rules for
the other target methods.
In Sch Äaferet al.'s results [20], more change rules were iden-
ti¯ed than by AURA using call-dependency analysis because
they also generate other types of change rules that are not
in the scope of AURA, such as change rules for ¯elds, in-
heritance relations, and methods existing in both the old
and new releases. AURA only generates change rules for
methods that physically disappeared in the new release.
5.4 Comparison on A Large-size System
In Table 4, we present the results of AURA and SemDi®
[5] to solve the compile errors of three Eclipse 3.1 plug-ins
when compiling them against Eclipse 3.3.
In SemDi® [5], correct rules are de¯ned as replacement
methods that can be found in the top three recommenda-
tions provided by SemDi®. It is easy for developers to choose
the right replacement from these three. In our approach,
we provide only one recommendation per target method.
Therefore, to compare the results of AURA with those of
SemDi®, we must account for this discrepancy in the way
correct rules are counted.
If every correct rule was the ¯rst recommendation of the
top three, SemDi® would have a precision of 100.00%, com-
parable to the precision of 92.86% of AURA. However, it is
also possible that the correct rule was the second or third of
the top three. Consequently, for the ¯rst recommendation,
the precision of SemDi® could be actually less than 100%,
thus AURA is competitive with SemDi®.
3A
URA only analyzed the packages org.gjt.sp.* and com-
pared its results with those of M. Kim et al. [16]. These
packages contain the code for JEdit main functions and are
large enough for manual analysis (444 target methods).
4Con¯rmed by Dagenais, it is 1.5-2.0
3315.5Comparison w/o Simply-deleted Methods
Previous approaches, such as [16, 20], do not explicitly re-
port simply-deleted change rules in their results. We remove
the simply-deleted change rules from AURA results and
compare these with the results of the previous approaches
to assess their in°uence on precision and recall.
As shown in Table 5, ¢ Pis stable and remains similar to
that with simply-deleted method rules (0.24% vs -0.10%).
¢Rdecreases from 53.07% to 6.80%. The ¢ Rs of AURA
to the two approaches [16, 20] are di®erent. The ¢ Rto Kim
et al. 's approach [16] decreases to 13.34%, while the ¢R to
SchÄaferet al. 's approach [20] drops to -3.02%.
The sharp decrease of ¢ Rhas two causes. First, large
number of target methods are deleted from the new re-
leases without replacements. Through manual inspection,
we found that, on average, 31.93% of target methods in
the change rules generated by AURA are simply deleted
from the new releases of the four medium-size systems. For
Struts from 1.1 to 1.2.4, this percentage is as high as 57%
(77 methods). Second, AURA and Sch Äaferet al. 's approach
have di®erent scopes, so ¢ Rdecreases dramatically.
Even with this decrease of ¢R on Struts from 1.1 to 1.2.4.
(-15.14%), AURA still improves recall with similar precision
when not considering simply-deleted method rules.
5.6 Performance
Since the analyses of AURA and of the previous approaches
were conducted on di®erent hardware and software plat-
forms, the reported performance data are only descriptive
and we will not compare them.
The analysis of the four medium-size systems takes less
than three minutes on Windows XP SP3 with Intel Core
Duo 1.5GHz and 4GB RAM. M. Kim et al. [16] report
computation times of seven minutes on average while Sch Äafer
et al. [20] report less than 30 minutes, but do not specify
their software and hardware platforms.
Analysing Eclipse JDT core and UI 3.1{3.3 takes seven
hours on CentOS 5.3 with AMD Opteron Dual-Core 2.4GHz
and 16GB RAM. SemDi® [5] took 16 hours on a Pentium D
3.2Ghz with 2GB of RAM running Ubuntu Server 7.04.
5.7 Threats to Validity
We now discuss the threats to validity of our evaluation
following the guidelines provided for case study research [25].
Construct validity threats concern the relation between
theory and observation; in our context, they are mainly due
to errors introduced in the algorithm and the manual vali-
dation. We are aware that we could have introduced a bias
during the manual validation of the change rules produced
by AURA. We did our best to avoid this bias and provide
all data on-line for further independent validation5. AURA
in Step 5 uses a random selection that could also introduce
variation in our results. However, these variations should
occur very rarely.
Threats to internal validity do not a®ect this study, be-
ing a systematic comparison of AURA with the previous
approaches using well-de¯ned measures, ¢P and ¢R .
Conclusion validity threats concern the relation between
the treatment and the outcome. We used un-biased system-
atic measures and the data provided by the authors of the
previous approaches without any changes other than those
discussed in Section 5. Thus, we believe that no threats to
the validity of our conclusion remain.Reliability validity threats concern the possibility of repli-
cating this study. We attempted here to provide all the
necessary details to re-implement AURA and replicate its
evaluation and comparison. Moreover, all studied systems
and data from the previous approaches are publicly available
or available upon request to their authors. The raw data on
which our study is based are available on the Web5.
Threats to external validity concern the possibility to gen-
eralize our ¯ndings. We studied ¯ve systems of di®erent size,
belonging to di®erent domains and evaluated by the previous
approaches. However, we only analyzed Java code; therefore
it is possible that AURA would perform di®erently on other
programming languages, like C ]or C++. Further valida-
tion on a larger set of systems and comparison with other
approaches are desirable.
6. DISCUSSION
We now discuss the strengths and limitations of AURA.
6.1 Strengths
Higher Recall and Comparable Precision.
The evaluation results show that AURA has higher re-
call than and comparable precision to those of previous ap-
proaches. Three factors contribute to the improvement:
1. Combination of call-dependency and text-similari-
ty analyses. Approaches using call-dependency analysis
can only ¯nd change rules whose target and replacement
methods are called by some anchors. For the ¯ve systems
that we analyzed, on average, only 33.85% of the change
rules are found by call-dependency analysis. Sch Äaferet al.
[20] can ¯nd more rules because they also generate other
types of change rules besides target method change rules.
Approaches using text-similarity analysis ¯nd rules for all
target methods but with a higher rate of false positives. In
practice, they trade o® recall for precision using thresholds.
AURA is able to ¯nd change rules for more target methods
than previous approaches, but with a slight loss of precision.
The evaluation results show that the ¢R of AURA is 53.07%
with about -0.10% lower precision, on average.
2. Multi-iteration algorithm. The multi-iteration algo-
rithm improves both recall and precision. It impacts pos-
itively the results in two cases: the ¯rst case is illustrated
in Section 2; the second case occurs by removing the re-
placement methods of other already-detected target meth-
ods from the candidate replacement method set of a tar-
get method. For example, if the candidate set of m()is
fa(),b()g and in a previous iteration AURA detected that
a()is the replacement of x(), then AURA removes a()from
the candidate replacement method set of m()and immedi-
ately identi¯es its replacement as b(). This second case does
not preclude identifying many-to-one change rules in a pre-
vious iteration. On the four medium-size systems, the aver-
age precision decreases by 2.50% if we a use a one-iteration
algorithm, calculated after both call-dependency and text-
similarity analyses.
3. Three{unit text similarity. AURA uses signature-
level similarity, LD and LCS, to compute the text similarity
of two methods. On the four medium-size systems, the av-
erage precision decreases by 3.53%, 2.41%, and 4.51% if we
remove each step, respectively.
332Sy
stems Indi
cators A
URA M.
Kim et al. [16] ¢R ¢PAv
erages
¢R ¢P
JHotDraw #Correct rule 96 8118
.26% -
3.03%
13.3
4% -5
.01%5.2
-5.3 P
recision 96.0
0% 99.0
0%
JEd
it #
Correct rule 247 2173
13
.99% -1
7.00%4.1
-4.2 P
recision 77.1
9% 93.0
0%
JF
reeChart #
Correct rule 9
5 887.7
8% 5.00
%0.9.
11-0.9.12 P
recision 81.9
0% 78.0
0%
Sy
stems Indi
cators A
URA Sc
hÄafer et al. [20] ¢R ¢PAv
erages
¢R ¢P
JHot
Draw #
Correct rule 9
6 889.0
9% 9.09
%
-3
.02% 8.1
1%5.2
-5.3 P
recision 96.0
0% 88.0
0%
St
ruts #
Correct rule 5
6 66-
15.14% 7.12
%1.1
-1.2.4 P
recision 91.0
8% 85.7
0%
Pre
cision of AURA 88
.58%
T
otal ¢R 6.0
8%
Av
erage ¢P 0.2
4%
T
able 5: Comparison of the results on medium-size systems without simply-deleted change rules
Many-to-one, One-to-many, Simply-deleted Rules.
Previous approaches only automatically generate one-to-
one change rules. Some approaches [5, 12] can semi-automa-
tically generate many-to-one and one-to-many rules, but
developers must manually analyze the rules to select the
appropriate replacement methods. AURA applies a call-
dependency analysis ¯rst and then uses a text-similarity
analysis to overcome this limitation of previous approaches.
None of the previous automatic approaches explicitly re-
ports simply-deleted method change rules. We manually
identi¯ed that, in the four medium-size systems, 31.93% of
target methods in the change rules that AURA generated
are simply-deleted. We argue that simply-deleted method
rules are as important as other types of change rules because
they are a part of the total change rules of a program. They
should be identi¯ed, evaluated and counted in the precision
and recall computation.
Threshold.
Existing automatic approaches [17, 16, 20], which do not
require framework developers' involvement, depend on expe-
rimentally-evaluated thresholds. These thresholds cannot
be predicted for a new framework without analyzing it and
evaluating the result. We could use the values of the tuned
thresholds for some frameworks already analyzed, but they
might not be applicable.
AURA completely eliminates thresholds and adapts nat-
urally to di®erent frameworks. It could therefore be used
immediately by developers without any settings.
6.2 Limitations
AURA cannot detect one-to-many and many-to-one change
rules for target methods that are not called by any anchor.
However, it can still ¯nd one-to-one rules using text similar-
ity analysis.
Major changes to the internal implementation of anchors
compromise the precision of AURA. For example, the preci-
sion of AURA for JEdit from 4.1 to 4.2 decreases by 13.78%
wrt. M. Kim et al.'s [16] because, between the two releases,
the API remained quite stable but the implementation of the
methods changed radically, thus confusing the ¯rst steps of
our approach based on call-dependency analysis. This limi-
tation is shared by all call-dependency-based approaches.
AURA only generates change rules for methods. During
the evaluation of AURA, we found that some getters arereplaced by direct ¯eld accesses. Future work includes mod-
ifying the de¯nition of change rules to take into account ¯eld
and type-related changes by analyzing inheritance relations
and polymorphism.
7. CONCLUSION AND FUTURE WORK
We presented AURA, an hybrid approach that combines
call-dependency and text-similarity analyses to provide de-
velopers with change rules when adapting their programs
from one release of a framework to the next.
Our approach o®ers the following contributions:
1. It increases recall by combining call dependency and
text similarity analyses in a multi-iteration algorithm;
2. It automatically adapts to di®erent frameworks by not
using any experimentally-evaluated threshold;
3. It reduces developers' e®orts by automatically gener-
ating one-to-many and many-to-one change rules.
The results of the evaluation of AURA on four medium-
size systems and in comparison to previous work showed
that the combination of call-dependency and text-similarity
analyses into a multi-iteration algorithm improves recall on
average by 53.07% with a slight decrease of 0.10% in preci-
sion. We also applied AURA on Eclipse and compared its
results with those of SemDi® [5] and showed that the ap-
proximated precision of AURA is 92.86% while SemDi®'s is
up to 100.00%.
In future work, we plan to extend our approach in several
directions: analyze target systems in other programming
languages than Java; add heuristics that generate change
rules for types and ¯elds by analyzing inheritance relations
and polymorphism; combine AURA with approaches that
use other matching techniques; present AURA results in
¯rst-order relational logic rules, as introduced by M. Kim
et al. [16]; perform usability studies to determine the e±-
cacy of AURA.
8. ACKNOWLEDGMENTS
We thank Barth¶ el¶ emy Dagenais and Martin P. Robillard
for providing advice adn their data and conducting analy-
sis with the latest version of their approach. We are also
grateful to Thorsten Sch Äafer for his experimental results.
This work has been partly funded by the NSERC Research
333Cha
irs in Software Change and Evolution and in Software
Patterns and Patterns of Software.
9. REFERENCES
[1] G. Antoniol, M. D. Penta, and E. Merlo. An automatic
approach to identify class evolution discontinuities. In
IWPSE '04: Proceedings of the Principles of Software
Evolution, 7th International Workshop , pages 31{40.
IEEE Computer Society, 2004.
[2] I. Balaban, F. Tip, and R. Fuhrer. Refactoring
support for class library migration. In OOPSLA '05:
Proceedings of the 20th annual ACM SIGPLAN
conference on Object-oriented programming, systems,
languages, and applications , pages 265{279, New York,
NY, USA, 2005. ACM.
[3] K. Chow and D. Notkin. Semi-automatic update of
applications in response to library changes. In ICSM
'96: Proceedings of the 1996 International Conference
on Software Maintenance , page 359, Washington, DC,
USA, 1996. IEEE Computer Society.
[4] J. Cohen. Statistical power analysis for the behavioral
sciences . L. Earlbaum Associates, 1988.
[5] B. Dagenais and M. P. Robillard. Recommending
adaptive changes for framework evolution. In ICSE
'08: Proceedings of the 30th international conference
on Software engineering , pages 481{490, New York,
NY, USA, 2008. ACM.
[6] S. Demeyer, S. Ducasse, and O. Nierstrasz. Finding
refactorings via change metrics. In OOPSLA '00:
Proceedings of the 15th ACM SIGPLAN conference on
Object-oriented programming, systems, languages, and
applications, pages 166{177, New York, NY, USA,
2000. ACM.
[7] D. Dig, C. Comertoglu, D. Marinov, and R. Johnson.
Automated detection of refactorings in evolving
components. In ECOOP '06: Proceedings of the 20th
European Conference on Object-Oriented
Programming . Springer Berlin / Heidelberg, July 2006.
[8] D. Dig and R. Johnson. How do apis evolve? a story
of refactoring: Research articles. J. Softw. Maint.
Evol., 18(2):83{107, 2006.
[9] D. Dig, K. Manzoor, R. Johnson, and T. N. Nguyen.
Refactoring-aware con¯guration management for
object-oriented programs. In ICSE '07: Proceedings of
the 29th international conference on Software
Engineering , pages 427{436, Washington, DC, USA,
2007. IEEE Computer Society.
[10] B. Fluri and H. C. Gall. Classifying change types for
qualifying change couplings. In ICPC '06: Proceedings
of the 14th IEEE International Conference on
Program Comprehension , pages 35{45, Washington,
DC, USA, 2006. IEEE Computer Society.
[11] D. M. German and A. E. Hassan. License integration
patterns: Addressing license mismatches in
component-based development. In ICSE '09:
Proceedings of the 2009 IEEE 31st International
Conference on Software Engineering , pages 188{198,
Washington, DC, USA, 2009. IEEE Computer Society.
[12] M. W. Godfrey and L. Zou. Using origin analysis to
detect merging and splitting of source code entities.
IEEE Trans. Softw. Eng. , 31(2):166{181, 2005.[13] J. Henkel and A. Diwan. Catchup!: capturing and
replaying refactorings to support api evolution. In
ICSE '05: Proceedings of the 27th international
conference on Software engineering , pages 274{283,
New York, NY, USA, 2005. ACM.
[14] Y. Kataoka, M. D. Ernst, W. G. Griswold, and
D. Notkin. Automated support for program
refactoring using invariants. In ICSM '01: Proceedings
of the IEEE International Conference on Software
Maintenance (ICSM'01) , page 736, Washington, DC,
USA, 2001. IEEE Computer Society.
[15] C. Kemper and C. Overbeck. What's new with
jbuilder. In JavaOne Sun's 2005 Worldwide Java
Developer Conference , 2005.
[16] M. Kim, D. Notkin, and D. Grossman. Automatic
inference of structural changes for matching across
program versions. In ICSE '07: Proceedings of the
29th international conference on Software
Engineering , pages 333{343, Washington, DC, USA,
Not Available 2007. IEEE Computer Society.
[17] S. Kim, K. Pan, and E. J. Whitehead, Jr. When
functions change their names: Automatic detection of
origin relationships. In WCRE '05: Proceedings of the
12th Working Conference on Reverse Engineering ,
pages 143{152, Washington, DC, USA, 2005. IEEE
Computer Society.
[18] D. Lawrie, H. Feild, and D. Binkley. Syntactic
identi¯er conciseness and consistency. In Sixth IEEE
International Workshop on Source Code Analysis and
Manipulation. , pages 139{148, Sept. 2006.
[19] G. Malpohl, J. J. Hunt, and W. E. Tichy. Renaming
detection. page 73, 2000.
[20] T. Sch Äafer, J. Jonas, and M. Mezini. Mining
framework usage changes from instantiation code. In
ICSE '08: Proceedings of the 30th international
conference on Software engineering , pages 471{480,
New York, NY, USA, May 2008. ACM.
[21] P. Steyaert, C. Lucas, K. Mens, and T. D'Hondt.
Reuse contracts: managing the evolution of reusable
assets. SIGPLAN Not. , 31(10):268{285, 1996.
[22] P. Wei¼gerber and S. Diehl. Identifying refactorings
from source-code changes. In ASE '06: Proceedings of
the 21st IEEE/ACM International Conference on
Automated Software Engineering , pages 231{240,
Washington, DC, USA, 2006. IEEE Computer Society.
[23] Z. Xing and E. Stroulia. Refactoring detection based
on umldi® change-facts queries. In WCRE '06:
Proceedings of the 13th Working Conference on
Reverse Engineering , pages 263{274, Washington, DC,
USA, 2006. IEEE Computer Society.
[24] Z. Xing and E. Stroulia. API-evolution support with
di®-CatchUp. IEEE TRANSACTIONS ON
SOFTWARE ENGINEERING , 33(12):818 { 836,
December 2007.
[25] R. K. Yin. Case Study Research: Design and Methods
- Third Edition . SAGE Publications, 3 edition, 2002.
334