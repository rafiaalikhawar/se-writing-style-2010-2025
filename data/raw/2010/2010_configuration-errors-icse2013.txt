Automated Diagnosis of
Software ConÔ¨Åguration Errors
Sai Zhang Michael D. Ernst
Computer Science & Engineering
University of Washington, USA
fszhang, mernstg@cs.washington.edu
Abstract ‚ÄîThe behavior of a software system often depends
on how that system is conÔ¨Ågured. Small conÔ¨Åguration errors
can lead to hard-to-diagnose undesired behaviors. We present
a technique (and its tool implementation, called ConfDiagnoser)
to identify the root cause of a conÔ¨Åguration error ‚Äî a single
conÔ¨Åguration option that can be changed to produce desired
behavior. Our technique uses static analysis, dynamic proÔ¨Åling,
and statistical analysis to link the undesired behavior to speciÔ¨Åc
conÔ¨Åguration options. It differs from existing approaches in two
key aspects: it does not require users to provide a testing oracle
(to check whether the software functions correctly) and thus is
fully automated; and it can diagnose both crashing and non-
crashing errors.
We evaluated ConfDiagnoser on 5 non-crashing conÔ¨Åguration
errors and 9 crashing conÔ¨Åguration errors from 5 conÔ¨Ågurable
software systems written in Java. On average, the root cause was
ConfDiagnoser‚Äôs Ô¨Åfth-ranked suggestion; in 10 out of 14 errors,
the root cause was one of the top 3 suggestions; and more than
half of the time, the root cause was the Ô¨Årst suggestion.
I. I NTRODUCTION
Many software applications support conÔ¨Åguration options
that allow users to customize their behavior. This Ô¨Çexibility has
a cost: when something goes wrong, diagnosing a conÔ¨Åguration
error can be both time-consuming and frustrating. Technical
support contributes 17% of the total cost of ownership of
today‚Äôs software, and troubleshooting misconÔ¨Ågurations is a
large part of technical support [11].
Software misconÔ¨Ågurations may lead to incorrect output (i.e.,
non-crashing errors) or to unexpected termination (i.e., crashing
errors). Even when an application outputs an error message,
it is often cryptic or misleading [2], [8], [30], [31]. Users may
not even think of conÔ¨Åguration as a cause of their problem.
A. Motivating Example
We next describe a real scenario in which we used Conf-
Diagnoser to solve a conÔ¨Åguration problem. We received a ‚Äúbug
report‚Äù against the Randoop automated test generation tool [16],
from a testing expert who had been using Randoop for quite
a while. The ‚Äúbug report‚Äù indicated that Randoop terminated
normally but failed to generate tests for the NanoXML [15]
program.
Although the reported problem is deterministic and fully
reproducible, it is a silent, non-crashing failure and is challeng-
ing to diagnose. Differing from a crashing error, Randoop did
not exhibit a crashing point, dump a stack trace, output an error
message, or indicate suspicious program variables that maySuspicious configuration option: maxsize
It affects the behavior of predicate:
"newSequence.size() > GenInputsAbstract.maxsize"
(line 312, class: randoop.ForwardGenerator)
This predicate evaluates to true:
3.3% of the time in normal runs (3830 observations)
32.5% of the time in the undesired run (2898 observations)
Fig. 1. The top-ranked conÔ¨Åguration option in ConfDiagnoser‚Äôs error report
for the motivating example in Section I-A.
have incorrect values. Lacking such information makes many
techniques such as dynamic slicing [36], dynamic information
Ô¨Çow tracking [2], and failure trace analysis [18] inapplicable.
In addition, for this scenario, the person who reported the
bug had already minimized the bug report: if any part of the
conÔ¨Åguration or input is removed, Randoop either crashes or no
longer exhibits this error. This further makes search-based fault
isolation techniques such as delta debugging [32] ineffective.
In fact, this bug report does not reveal a real bug in the
Randoop code. Its root cause is that the user failed to set one
conÔ¨Åguration option. Despite the simplicity of the solution,
to the best of our knowledge, no previous conÔ¨Åguration error
diagnosis technique [1], [2], [18], [25], [28], [33], [35] can be
directly applied.
Our technique (and its tool implementation ConfDiagnoser)
can diagnose and correct this problem. We Ô¨Årst reproduced the
error in a ConfDiagnoser-instrumented Randoop version, then
ConfDiagnoser diagnosed the error‚Äôs root cause by analyzing
the recorded execution proÔ¨Åle. ConfDiagnoser produced a
report (Figure 1) in the form of an ordered list of suspicious
conÔ¨Åguration options that should be inspected. The error
report in Figure 1 suggests that a conÔ¨Åguration option named
maxsize is the most likely one. The report also provides
relevant information to explain why: a program predicate
affected by maxsize behaves dramatically differently between
the recorded undesired execution and the correct executions
found in ConfDiagnoser‚Äôs database.
Figure 2 shows the relevant code snippet in Randoop. When
Randoop generates a new test (line 100, in the form of a method-
call sequence), Randoop compares its length with maxsize
(default value: 100). If the generated sequence‚Äôs length exceeds
this pre-deÔ¨Åned limit, Randoop discards it to avoid length ex-
plosion in further test generation. Although maxsize ‚Äôs default
value was carefully chosen by the Randoop developers and
works well for many programs (including those used to testIn class: randoop.main.GenInputsAbstract
//The maxsize configuration option. Default value: 100.
157. public static int maxsize = readFromCommandLine();
In class: randoop.ForwardGenerator
99. public ExecutableSequence step() {
100. ExecutableSequence eSeq = createNewUniqueSequence();
101. AbstractGenerator.currSeq = eSeq.sequence;
102. eSeq.execute(executionVisitor);
103. processSequence(eSeq);
104. if (eSeq.sequence.hasActiveFlags()) {
105. componentManager.addGeneratedSequence(eSeq.sequence);
106. }
107. return eSeq;
108. }
310. private ExecutableSequence createNewUniqueSequence() {
311. Sequence newSequence = ...; //create a sequence
312. if (newSequence.size() > GenInputsAbstract.maxsize) {
313. return null;
314. }
315. if (this.allSequences.contains(newSequence)) {
316. return null;
317. }
318. return new ExecutableSequence(newSequence);
319. }
Fig. 2. SimpliÔ¨Åed code excerpt from Randoop [16] corresponding to the
conÔ¨Åguration problem reported in Figure 1.
Randoop during its development), the generated sequences for
NanoXML are much longer than usual and using maxsize ‚Äôs
default value results in 32.5% of the generated sequences be-
ing discarded (including sequences that the user wishes to
retain). ConfDiagnoser captures such abnormal behavior from
Randoop‚Äôs silent failure, pinpoints the maxsize option, and
suggests the user to change its value. The problem is resolved if
the user changes maxsize to a larger value, for example 1000.
B. Diagnosing ConÔ¨Åguration Errors
Correcting a conÔ¨Åguration error can be divided into two
separate tasks: identifying which speciÔ¨Åc conÔ¨Åguration option
is responsible for the unexpected behavior, and determining a
better value for the conÔ¨Åguration option. This paper addresses
the former task: Ô¨Ånding the root cause of a conÔ¨Åguration error.
Our technique is designed to be used by system administra-
tors and end-users when they encounter an error that they do
not know how to Ô¨Åx. It uses three steps to link the undesired
behavior to speciÔ¨Åc root cause conÔ¨Åguration options:
ConÔ¨Åguration Propagation Analysis . For each conÔ¨Ågura-
tion option, ConfDiagnoser uses a lightweight dependence
analysis, called thin slicing [20], to statically identify the
predicates it affects in the source code.
ConÔ¨Åguration Behavior ProÔ¨Åling . ConfDiagnoser selec-
tively instruments the program-to-diagnose so that it records
the run-time behaviors of affected predicates in an execution
proÔ¨Åle. When the user encounters a suspected conÔ¨Åguration
error, the user reproduces the error using the instrumented
version of the program.
ConÔ¨Åguration Deviation Analysis . ConfDiagnoser selects,
from a pre-built database, correct execution proÔ¨Åles that
are as similar as possible to the undesired one. Then, it
identiÔ¨Åes the predicates whose dynamic behaviors deviate
the most between correct and undesired executions. The
behavioral differences in the recorded predicates provide
evidence for what predicates in a program might be be-having abnormally and why. For each deviated predicate,
ConfDiagnoser further identiÔ¨Åes its affecting conÔ¨Åguration
options as the likely root causes. Finally, it outputs a ranked
list of suspicious conÔ¨Åguration options and explanations.
An important component in ConfDiagnoser is the pre-built
database, which contains proÔ¨Åles from known correct execu-
tions. We envision that the software developers build this
database at release time. The database can be further enriched
by software users as more correct executions are accumulated.
In our experiments (Section IV), we built a database of 6‚Äì16
execution proÔ¨Åles by running examples from software user man-
uals, FAQs, discussion mailing list, forum posts, and published
papers. We found that even such a small database worked
remarkably well for error diagnosis.
Compared to previous approaches [2], [18], [25], [28], [32],
[36], ConfDiagnoser has several notable features:
It is fully automated . ConfDiagnoser does not require a
user to specify when ,why, orhow the program fails. This
is different than many well-known automated debugging
techniques such as delta debugging [32], information Ô¨Çow
analysis [2], and dynamic slicing [36]. Our technique also
provides an explanation of why a conÔ¨Åguration option is
suspicious.
It can diagnose both non-crashing and crashing errors .
Most previous techniques [2], [18], [21], [28] focus exclu-
sively on conÔ¨Åguration errors that cause a crash, an error
message, or a stack trace. By contrast, ConfDiagnoser di-
agnoses conÔ¨Åguration problems that manifest themselves as
either visible or silent failures.
It requires no OS-level support. Our technique requires
no alterations to the JVM or standard library. This dis-
tinguishes our work from competing techniques such as
OS-level conÔ¨Åguration error troubleshooting [21], [28].
C. Evaluation
We evaluated ConfDiagnoser on 14 real conÔ¨Åguration errors
(9 crashing errors and 5 non-crashing errors) from 5 projects.
On average, ConfDiagnoser‚Äôs 5th report was the root cause; in
10 out of 14 cases, the root cause was ConfDiagnoser‚Äôs top
3 reports; and in over half of all cases, the root cause was
ConfDiagnoser‚Äôs Ô¨Årst report. Assuming the database of correct
execution proÔ¨Åles already exists, ConfDiagnoser takes less than
4 minutes on average to diagnose one error. ConfDiagnoser‚Äôs
accuracy and speed make it an attractive alternative to manual
debugging.
We compared ConfDiagnoser to a previous technique, called
ConfAnalyzer [18], which uses dynamic information Ô¨Çow anal-
ysis to reason about the root cause of a conÔ¨Åguration error.
ConfDiagnoser produced better results for non-crashing errors
and similar results for crashing errors.
We also compared ConfDiagnoser to two techniques leverag-
ing existing fault localization techniques [10], [14] to diagnose
conÔ¨Åguration errors. ConfDiagnoser substantially outperformed
both of them.
Finally, we evaluated two internal design choices of Conf-
Diagnoser. First, we show that using thin slicing [20] to  
Configuration Options  
Program  
Propagation  
Analysis  
Instrument  
 Run 
Deviation  
Analysis  
Execution  
Profile  
Affected  
Predicates  
Instrumented  
Program  
Input  
Error  
Report  
Configuration  
(a bad run ) 
Profile  
Database  
 (good runs )   
Profile  
Database  
 (good runs )   Fig. 3. The workÔ¨Çow of our conÔ¨Åguration error diagnosis technique. ‚ÄúPropagation Analysis‚Äù is described in Section II-B. The ‚ÄúInstrument‚Äù and ‚ÄúRun‚Äù
components correspond to the ConÔ¨Åguration Behavior ProÔ¨Åling step in Section II-C. ‚ÄúDeviation Analysis‚Äù is described in Section II-D.
compute the affected predicates yielded more accurate diagnosis
than using full slicing [7]. Second, we show that varying the
execution proÔ¨Åle selection strategy can result in substantially
different results. The similarity-based selection strategy used
in ConfDiagnoser outperformed the other two strategies.
D. Contributions
This paper makes the following contributions:
Technique. We present a technique to diagnose software
conÔ¨Åguration errors. Our technique uses static analysis, dy-
namic proÔ¨Åling, and statistical analysis to link the undesired
behavior to speciÔ¨Åc conÔ¨Åguration options (Section II).
Implementation. We implemented our technique in a tool,
called ConfDiagnoser, for Java software (Section III). It is
available at http://conÔ¨Åg-errors.googlecode.com.
Evaluation. We applied ConfDiagnoser to diagnose 14
conÔ¨Åguration errors in 5 conÔ¨Ågurable Java software projects.
The results show the usefulness of the proposed technique
(Section IV).
II. T ECHNIQUE
We model a conÔ¨Åguration as a set of key-value pairs, where
the keys are strings and the values have arbitrary type. This
abstraction is offered by the POSIX system environment, the
Java Properties API, and the Windows Registry.
A. Overview
Figure 3 sketches the high-level workÔ¨Çow of our technique.
Our technique takes as input a Java program and its conÔ¨Åg-
uration options. It Ô¨Årst performs a propagation analysis to
identify the affected predicates for each conÔ¨Åguration option
(Section II-B). After that, our technique selectively instruments
the program at the affected predicates. To diagnose an error, a
user runs the instrumented program with the error-revealing
input and conÔ¨Åguration to obtain an execution proÔ¨Åle (Sec-
tionII-C). Then, our technique analyzes the obtained execution
proÔ¨Åle to identify the behaviorally-deviated predicates and their
root causes, and reports these to the user (Section II-D).
B. ConÔ¨Åguration Propagation Analysis
For each conÔ¨Åguration option, ConÔ¨Åguration Propagation
Analysis statically determines its affected predicates . In our
context, a predicate is a Boolean expression in a conditional
or loop statement, whose evaluation result determines whether
to execute the following statement or not. A predicate‚Äôs
run-time outcome affects the program control Ô¨Çow. Conf-
Diagnoser focuses on identifying and monitoring conÔ¨Ågurationoption-affected control Ô¨Çow rather than the values, for two
reasons. First, control Ô¨Çow often propagates the majority of
conÔ¨Åguration-related effects and determines a program‚Äôs exe-
cution path, while the value of a speciÔ¨Åc expression may be
largely input-dependent. Second, it simpliÔ¨Åes reporting because
the outcome of a program predicate can only be either true
or false. Nevertheless, a program predicate is not the only ab-
straction our technique can use. Our experiments (Section IV)
empirically demonstrate that choosing other abstractions, such
as monitoring statement-level coverage or method-level invari-
ants, yields less accurate results.
To identify the predicates affected by a conÔ¨Åguration op-
tion, a straightforward way is to use program slicing [7] to
compute a forward slice from the initialization statement of a
conÔ¨Åguration option. Unfortunately, traditional full slicing [7]
is impractical because it includes too much of the program.
This is due to conservatism (for example, in handling pointers)
and to following both data and control dependences. Figure 2
illustrates this problem. Traditional slicing concludes that the
predicates in lines 104, 312, and 315 are affected by the conÔ¨Åg-
uration option maxsize . However, the predicates in lines 104
and 315, though possibly affected by maxsize , are actually
irrelevant to maxsize ‚Äôs value. That is, the value of maxsize
controls the length of a generated sequence rather than deciding
whether a sequence has an active Ô¨Çag (line 104) or a sequence
has been executed before (line 315).
To address this limitation, our technique uses thin slicing
[20], which includes only statements that are directly affected
by a conÔ¨Åguration option. Different from traditional slicing [7],
thin slicing focuses on data Ô¨Çow from the seed (here, a seed is
the initialization statement of a conÔ¨Åguration option), ignoring
control Ô¨Çow dependencies as well as uses of base pointers.
Thin slicing is attractive because it This property separates
pointer computations from the Ô¨Çow of conÔ¨Åguration option
values and naturally connects a conÔ¨Åguration option with its
directly affected statements. For example, in the code excerpt
of Figure 2, a forward thin slice computed for maxsize only
includes the predicate in line 312. Section IV empirically
demonstrates that thin slicing is a better choice than traditional
full slicing for our purposes.
C. ConÔ¨Åguration Behavior ProÔ¨Åling
This step instruments the tested program ofÔ¨Çine by inserting
code to record how often each predicate evaluates to true at
run time.Executing the instrumented program produces an execution
proÔ¨Åle , which consists of a set of predicate proÔ¨Åles . Each
predicate proÔ¨Åle is a 4-tuple consisting of a conÔ¨Åguration
option, one of its affected predicates, the predicate‚Äôs execution
count, and how many times it evaluated to true. For example,
suppose the predicate on line 312 has been executed 100 times,
of which 30 times it evaluated to true. ConfDiagnoser creates
the following predicate proÔ¨Åle:
hmaxsize ,newSequence.size() > maxsize , 100, 30i.
Such predicate proÔ¨Åles are by no means complete in record-
ing the whole execution. However, they capture sufÔ¨Åcient
information to reason about the causal effects of conÔ¨Ågurations
and how a conÔ¨Åguration option relates to software‚Äôs behavior,
as shown by our experiments (Section IV). Collecting these
proÔ¨Åles imposes only moderate performance impact.
D. ConÔ¨Åguration Deviation Analysis
ConfDiagnoser starts error diagnosis after obtaining the exe-
cution proÔ¨Åle from an undesired execution. It selects similar
proÔ¨Åles from known correct executions (Section II-D1), com-
pares each selected proÔ¨Åle with the undesired one to identify
the most behaviorally-deviated predicates (Section II-D2), and
then determines the likely root cause options (Section II-D3).
1) Selecting Similar Execution ProÔ¨Åles for Comparison:
ConfDiagnoser‚Äôs database contains proÔ¨Åles from known correct
executions. These execution proÔ¨Åles can be dramatically dif-
ferent from another. To avoid reporting irrelevant differences
when determining how and why the observed execution proÔ¨Åle
behaves differently from the correct ones, ConfDiagnoser Ô¨Årst
compares the undesired proÔ¨Åle with the correct proÔ¨Åles, then
selects a set of similar ones as the basis of diagnosis.
ConfDiagnoser Ô¨Årst converts each execution proÔ¨Åle einto
an-dimensional vector ve=hre1;re2;:::;r eni, wherenis the
number of predicates affected by conÔ¨Åguration options and
eachreiis a ratio representing how often the i-th predicate
proÔ¨Åle evaluated to true at run time. If a predicate has never
been executed in an execution, ConfDiagnoser uses 0 as its
ratio.
ConfDiagnoser computes the similarity of two execution
proÔ¨Åleseandfby computing the cosine similarity from
information retrieval [29] of veandvf.
Similarity (e;f) =cossim(ve;vf) =Pn
i=1reirfiqPn
i=1r2
eiqPn
i=1r2
fi
This similarity metric compares two execution proÔ¨Åles based
on control Ô¨Çow taken (approximated by how often each pred-
icate evaluated to true). Its value ranges from 0 meaning
completely different predicate behavior, to 1 meaning the same
predicate behavior, and in-between values indicating interme-
diate similarity.
A crashing error sometimes happens soon after the pro-
gram is launched, so the resulting execution proÔ¨Åle is much
smaller than most correct execution proÔ¨Åles. To avoid compar-
ing un-executed predicates, when diagnosing a crashing error,
ConfDiagnoser reduces each correct execution proÔ¨Åle by only
retaining the predicates executed by the crashing proÔ¨Åle, and
then uses the reduced proÔ¨Åle for comparison.Given an undesired execution proÔ¨Åle, ConfDiagnoser selects
all execution proÔ¨Åles (or the reduced proÔ¨Åles for a crashing
error) from the database with a Similarity value above a
threshold (default value: 0.9, as used in our experiments).
2) Identifying Deviated Predicates: Our automated error
diagnosis approach compares an undesired execution proÔ¨Åle
with a set of similar andcorrect execution proÔ¨Åles. The behav-
ioral differences in the recorded predicates provide evidence
for what parts of a program might be incorrect and why.
ConfDiagnoser characterizes the dynamic behavior of a predi-
cate by how often it was evaluated (i.e., the number of observed
executions), and how often it evaluated to true (i.e., the true ra-
tio). The true ratio is more important, but it is less dependable
the fewer times the predicate has been evaluated.
We deÔ¨Åne the following metric, which combines sensitivity
(informally, the need for multiple observations) and speciÔ¨Åcity
(informally, the true ratio) in a standard way by computing
their harmonic mean.
(e;p) =2
1=trueRatio (e;p) + 1=totalExecNum (e;p)
In(e;p),trueRatio (e;p)is the ratio of executions of the
predicatepthat evaluated to true in e, and totalExecNum (e;p)
is the the total number of executions of predicate pine. To
smooth corner cases, if a predicate pis not executed in e,
i.e.,totalExecNum (e;p) = 0 , then(e;p)returns 0; and if
a predicate p‚Äôs true ratio is 0, i.e., trueRatio (e;p) = 0 , then
(e;p)returns 1=totalExecNum (e;p).
The following Deviation metric compares a predicate p
across two execution proÔ¨Åles eandf. A larger Deviation
value indicates that the behavior is more different.
Deviation (p;e;f ) =j(e;p) (f;p)j
Often 1=trueRatio (e;p)1=totalExecNum (e;p), and
then the value of Deviation (p;e;f )depends primarily on p‚Äôs
true ratio difference between execution proÔ¨Åles eandf.
ConfDiagnoser computes the Deviation value for each pred-
icatepappearing in two execution proÔ¨Åles eandf, and ranks
them in decreasing order. The two execution proÔ¨Åles eandf
are for the same program, so they have exactly the same set
of predicates affected by conÔ¨Åguration options.
3) Linking Predicates to Root Causes: ConfDiagnoser
links the behaviorally-deviated predicates to their root cause
conÔ¨Åguration options by using the results of thin slicing
(computed by the ConÔ¨Åguration Propagation Analysis step
in Section II-B). ConfDiagnoser identiÔ¨Åes the affecting
conÔ¨Åguration options for each deviated predicate, and treats
the conÔ¨Åguration option affecting a higher-ranked deviated
predicate as the more likely root cause. If a predicate is
affected by multiple conÔ¨Åguration options, ConfDiagnoser
prefers options whose initialization statements are closer to
the deviated predicate (in terms of breath-Ô¨Årst search distance
in the dependence graph of thin slicing). This heuristic is
based on the intuition that statements closer to the predicate
seem more likely to be relevant to its behavior.
When multiple correct execution proÔ¨Åles are selected for
comparison, ConfDiagnoser Ô¨Årst produces a ranked list of rootcause conÔ¨Åguration options for each comparison pair, and then
outputs a Ô¨Ånal list by using majority voting over all ranking
lists. In the Ô¨Ånal ranking list, one conÔ¨Åguration option ranks
higher than another if it ranks higher in more than half of the
ranking lists. Our implementation breaks possible cycles by
arbitrarily ranking the involved options, but this did not occur
in our experiments.
In the Ô¨Ånal output report (e.g., Figure 1), ConfDiagnoser
generates a brief explanation for each behaviorally-deviated
predicate by showing the difference between the predicate‚Äôs
true ratio during correct executions from the database and
during the undesired execution.
E. Discussion
This paper focuses speciÔ¨Åcally on conÔ¨Åguration errors, as-
suming the application code is correct but the software is
inappropriately conÔ¨Ågured so that it does not behave as desired.
We next discuss some design issues in ConfDiagnoser.
Differences between program inputs and conÔ¨Åguration op-
tions. We took the list of conÔ¨Åguration options for each subject
program from its manual. Typically, the manual calls an input
a conÔ¨Åguration option when it controls a program‚Äôs control
Ô¨Çow rather than producing result data. A conÔ¨Åguration option
is often supplied via a command-line Ô¨Çag or conÔ¨Åguration Ô¨Åle.
Why not use proÔ¨Åles from unit test executions? Conf-
Diagnoser‚Äôs database stores correct proÔ¨Åles from complete
executions that start at the main method. ConfDiagnoser does
not use proÔ¨Åles from unit test executions, which check the
correctness of a single program component and produce an
incomplete execution proÔ¨Åle that is not representative of the
whole program workÔ¨Çow.
Why not store proÔ¨Åles from failing executions in the
database? We envision the proÔ¨Åle database is built by develop-
ers at release time. It is more natural and easier for a developer
to provide correct execution proÔ¨Åles, instead of anticipating
and enumerating the possible errors a user may encounter.
What if a similar execution proÔ¨Åle is not available? Conf-
Diagnoser‚Äôs effectiveness largely depends on the availability
of similar execution proÔ¨Åles from the database. For a given
undesired execution proÔ¨Åle, lacking a similar proÔ¨Åle in Conf-
Diagnoser‚Äôs database may lead ConfDiagnoser to produce less
useful results. It also indicates inadequacy of the tests from
which the database was constructed. Future work should rem-
edy this problem. One possible approach is to synthesize
a new execution, either by generating a new input for the
program [34] or by mutating an existing execution [22].
III. I MPLEMENTATION
We implemented a tool, called ConfDiagnoser, on top of the
WALA framework [24]. Our tool analyzes Java bytecode. It
statically computes the affected predicates for each conÔ¨Ågura-
tion option, assigns a unique ID for each affected predicate, and
then performs ofÔ¨Çine instrumentation. The runtime behavior
of all affected predicates is recorded in a Ô¨Åle.
For a Java program, ConfDiagnoser does not analyze the
standard JDK library and all the dependent libraries. We believeProgram (version) LOC #ConÔ¨Åg Options #ProÔ¨Åles
Randoop (1.3.2) 18587 57 12
Weka Decision Trees (3.6.7) 3810 14 12
JChord (2.1) 23391 79 6
Synoptic (trunk, 04/17/2012) 19153 37 6
Soot (2.5.0) 159273 49 16
Fig. 4. Subject programs. Column ‚ÄúLOC‚Äù is the number of lines of code,
as counted by CLOC [4]. Column ‚Äú#ConÔ¨Åg Options‚Äù is the number of
conÔ¨Åguration options. Column ‚Äú#ProÔ¨Åles‚Äù is the number of execution proÔ¨Åles
in the pre-built database.
this approximation is reasonable, since a conÔ¨Åguration option
set on client software usually does not affect the behaviors of
its dependent libraries.
IV. E VALUATION
Our evaluation answers the following research questions:
How effective is ConfDiagnoser in error diagnosis? Conf-
Diagnoser‚Äôs effectiveness can be reÔ¨Çected by:
‚Äìthe absolute ranking of the actual root cause in Conf-
Diagnoser‚Äôs output (Section IV-C1).
‚Äìthe time cost of error diagnosis (Section IV-C2).
‚Äìcomparison with a previous conÔ¨Åguration error diagnosis
technique (Section IV-C3).
‚Äìcomparison with two fault localization techniques (Sec-
tion IV-C4).
What are the effects of using full slicing [7] rather than
thin slicing [20] to identify the affected predicates? What
are the effects of varying comparison execution proÔ¨Åles?
These are two internal design choices (Section IV-C5).
A. Subject Programs
We evaluated ConfDiagnoser on 5 Java programs shown in
Figure 4. Randoop [16] is an automated test generator for Java
programs. Weka [27] is a toolkit that implements machine
learning algorithms. Our evaluation uses only its decision
tree module. JChord [9] is a program analysis platform that
enables users to design, implement, and evaluate static and
dynamic program analyses for Java. Synoptic [23] mines a
Ô¨Ånite state machine model representation of a system from
logs. Soot [19] is a Java optimization framework for analyzing
and transforming Java bytecode.
1) ConÔ¨Åguration Errors: We collected 14 conÔ¨Åguration er-
rors, listed in Figure 5. This paper evaluates all the conÔ¨Ågura-
tion errors we found; we did not select only errors on which
ConfDiagnoser works well. The misconÔ¨Ågured values include
enumerated types, numerical ranges, regular expressions, and
strings. The 5 non-crashing errors are collected from actual
bug reports, mailing list posts, and our own experience. The
9 crashing errors, taken from [18], were previously used to
evaluate the ConfAnalyzer tool. All 14 conÔ¨Åguration errors are
minimal: if any part of the conÔ¨Åguration or input is removed,
the software either crashes or no longer exhibits the undesired
behavior.
B. Evaluation Procedure
For each subject program, we constructed a proÔ¨Åle database
by running existing (correct) examples from its user manual,
FAQs, discussion mailing list, forum posts, and publishedError ID. Root Cause #Options Program ConfDiagnoser ConfAnalyzer Coverage Invariant ConfDiagnoser
Program ConÔ¨Åguration Option Output Analysis Analysis w/ Full Slicing
Rank #ProÔ¨Åles Rank Rank Rank Rank Rank
Non-crashing errors
1. Randoop maxsize 57 N 10 / 12 1 X 13 N 46
2. Weka m_numFolds 14 N 2 / 12 1 X 4 5 9
3. JChord chord.datarace.exclude.eqth 79 N 2 / 6 3 X 38 2 73
4. Synoptic partitionRegExp 37 N 2 / 6 1 X 1 N 6
5. Soot keep_line_number 49 N 6 / 16 2 X 46 N N
Average 47.2 23.6 3.6 / 10.4 1.6 23.6 20.4 15.7 31.7
Crashing errors
6. JChord chord.main.class 79 1 4 / 6 1 1 1 4 5
7. JChord chord.main.class 79 1 5 / 6 1 1 1 4 5
8. JChord chord.run.analyses 79 1 5 / 6 17 1 17 22 21
9. JChord chord.ctxt.kind 79 1 3 / 6 1 3 25 30 75
10. JChord chord.print.rels 79 1 2 / 6 15 1 20 25 24
11. JChord chord.print.classes 79 1 4 / 6 16 1 13 17 22
12. JChord chord.scope.kind 79 1 5 / 6 1 1 1 N 10
13. JChord chord.reflect.kind 79 1 6 / 6 1 3 5 9 11
14. JChord chord.class.path 79 1 4 / 6 8 N 21 26 6
Average 79 1 4.2 / 6 6.7 5.7 11.5 19.5 19.8
Fig. 6. Experimental results in diagnosing software conÔ¨Åguration errors. Column ‚ÄúRoot Cause ConÔ¨Åguration Option‚Äù shows the actual root cause conÔ¨Åguration
option. Column ‚Äú#Options‚Äù shows the number of conÔ¨Åguration options, taken from Figure 4. Column ‚ÄúProgram Output‚Äù shows the rank of the root cause as
indicated by the program‚Äôs output, such as an error message. Column ‚ÄúConfDiagnoser‚Äù shows the results of using our technique. Column ‚Äú#ProÔ¨Åles‚Äù shows
the number of similar execution proÔ¨Åles selected from the pre-built database for comparison, and the number of executions in the database. For each technique,
Column ‚ÄúRank‚Äù shows the absolute rank of the actual root cause in its output (lower is better). ‚ÄúX‚Äù means the technique is not applicable (i.e., requiring a
crashing point), and ‚ÄúN‚Äù means the technique does not identify the actual root cause. When computing the average rank, each ‚ÄúX‚Äù or ‚ÄúN‚Äù is treated as half of
the number of conÔ¨Åguration options, because a user would need to examine on average half of the options to Ô¨Ånd the root cause. Column ‚ÄúConfAnalyzer‚Äù
shows the results of using a previous technique [18] (Section IV-C 3); the data in this column is taken from [18]. Columns ‚ÄúCoverage Analysis‚Äù and ‚ÄúInvariant
Analysis‚Äù show the results of using two fault localization techniques as described in Section IV-C 4. Column ‚ÄúConfDiagnoser w/ Full Slicing‚Äù shows the
results of using full slicing [7] to compute the affected predicates (Section IV-C5).
Error ID Program Description
Non-crashing errors
1 Randoop No tests generated
2 Weka Low accuracy of the decision tree
3 JChord No datarace reported for a racy program
4 Synoptic Generate an incorrect model
5 Soot Source code line number is missing
Crashing errors
6 JChord No main class is speciÔ¨Åed
7 JChord No main method in the speciÔ¨Åed class
8 JChord Running a nonexistent analysis
9 JChord Invalid context-sensitive analysis name
10 JChord Printing nonexistent relations
11 JChord Disassembling nonexistent classes
12 JChord Invalid scope kind
13 JChord Invalid reÔ¨Çection kind
14 JChord Wrong classpath
Fig. 5. The 14 conÔ¨Åguration errors used in the evaluation.
papers [16], [18]. We spent 3 hours per program, on average,
and obtained 6‚Äì16 execution proÔ¨Åles. The average size of
the proÔ¨Åle database is 35MB, and the largest one (Randoop‚Äôs
database) is 72MB.
We made a simple syntactic change to JChord, which affected
24 lines of code. This change does not modify JChord‚Äôs
semantics; rather, it just encapsulates scattered conÔ¨Åguration
option initialization statements as static class Ô¨Åelds, which
simpliÔ¨Åes specifying the seed statement in performing slicing.
When diagnosing a conÔ¨Åguration error, we Ô¨Årst reproduced
the error on a ConfDiagnoser-instrumented program to obtain
the execution proÔ¨Åle. Then, we ran ConfDiagnoser on theobtained execution proÔ¨Åle to identify its root causes.
Our experiments were run on a 2.67GHz Intel Core PC
with 4GB physical memory (2GB was allocated for the JVM),
running Windows 7.
C. Results
1) Accuracy in Diagnosing ConÔ¨Åguration Errors: As shown
in Figure 6, ConfDiagnoser is highly effective in pinpointing
the root cause of misconÔ¨Ågurations. For all 5 non-crashing
errors and 5 of the 9 crashing errors, it lists the actual root
cause as one of the top 3 options.
a) Non-crashing conÔ¨Åguration errors.: ConfDiagnoser is
particularly effective in diagnosing non-crashing conÔ¨Åguration
errors, which are not supported by most other tools. The
average rank of the root cause in ConfDiagnoser‚Äôs output is
1.6. The primary reason is ConfDiagnoser‚Äôs ability to identify
the behaviorally-deviated predicates through execution proÔ¨Åle
comparison. The top-ranked deviated predicates often provide
useful clues about what parts of a program might be abnormal
and why.
We use the non-crashing error in Weka as an example to
illustrate this point. Weka‚Äôs decision tree implementation is
highly tuned, achieving 70‚Äì90% accuracy on its included exam-
ples. However, its accuracy drops to 62% on a different dataset
we experimented on. We used ConfDiagnoser to diagnose this
problem by Ô¨Årst building a database by running Weka on its
examples, and then obtaining the undesired execution proÔ¨Åle by
running it on our dataset. As a result, ConfDiagnoser outputs
the following report (only the top option is shown):Suspicious configuration option: m_numFolds
It affects the behavior of predicate:
"numFold < numInstances() % numFolds"
(line 1354, class: weka.core.Instances)
This predicate evaluates to true:
20% of the time in normal runs (4 observations)
70% of the time in the undesired run (10 observations)
The above report reveals an important fact about the low
accuracy. The predicate numFold<numInstances() %
numFolds controls the depth of a decision tree. Its true ratio is
substantially higher in the undesired execution than in normal
executions. A higher true ratio leads to a deeper tree that is
more likely to overÔ¨Åt the training data and yield low accu-
racy on the testing data. To resolve this problem, we changed
m_numFolds value from 2 to 3 to reduce the tree depth, and
gained a 5% performance increase.
b) Crashing conÔ¨Åguration errors.: Crashing errors are
often easy for a user to diagnose. This is because a crashing
error often produces a stack trace or error message with valuable
diagnosis clues. In fact, for the crashing errors selected by the
ConfAnalyzer authors, the user is always led to the root cause
by the program output, without the need for further analysis.
For error #6, JChord throws a NoClassDefFoundError when
loading a user-speciÔ¨Åed class. This error reminds the user that
a non-existent class might be provided. For error #7, JChord
outputs an error message of ‚ÄúCould not Ô¨Ånd main class [...] or
main method in that class‚Äù that explicitly informs the user that
the conÔ¨Åguration option to specify a main class might be wrong.
For errors #8‚Äì13, JChord outputs the relevant conÔ¨Åguration
option in its error message. For error #14, JChord throws a
ClassNotFoundException (for the main class) that reminds
the user to check the classpath setting.
ConfDiagnoser is more effective in diagnosing non-crashing
errors (average rank: 1.6) than crashing errors (average rank:
6.7). Most of the crashing errors occur soon after the program
is launched, so ConfDiagnoser lacks enough predicate behavior
observations. Many predicates are only executed once, so their
Deviation scores (Section II-D2) turn out to be the same; and
the BFS-distance-based heuristic to resolve ties (Section II-D3)
only works half the time.
2) Performance of ConfDiagnoser: We measured Conf-
Diagnoser‚Äôs performance in two ways: the time cost in di-
agnosing an error and the overhead introduced in reproducing
an error in a ConfDiagnoser-instrumented program.
As shown in Figure 7, the performance of ConfDiagnoser is
reasonable. On average, it uses less than 4 minutes to diagnose
each conÔ¨Åguration error (including the time to compute thin
slices and the time to recommend suspicious conÔ¨Åguration
options). Computing thin slices for all conÔ¨Åguration options
is expensive. However, this step is one-time effort per pro-
gram and the computed slices can be cached to share across
diagnoses.
The performance overhead to reproduce the buggy behavior
varies among applications. The current tool implementation
imposes a substantial slowdown when reproducing errors 3, 9,
10, and 11 in a ConfDiagnoser-instrumented version. This isError ID. Run-time ConfDiagnoser time (seconds)
Program Slowdown ()Thin Slicing Error Diagnosis
Non-crashing errors
1. Randoop 1.1 50 <1
2. Weka 1.2 43 <1
3. JChord 13.2 147 82
4. Synoptic 3.6 24 <1
5. Soot 3.1 95 21
Mean 2.9 72 21
Crashing errors
6. JChord 2.4 147 79
7. JChord 1.4 147 75
8. JChord 1.5 147 17
9. JChord 28.5 147 30
10. JChord 13.7 147 13
11. JChord 65.1 147 10
12. JChord 1.6 147 83
13. JChord 1.9 147 8
14. JChord 1.4 147 80
Mean 4.3 147 44
Fig. 7. ConfDiagnoser‚Äôs performance. The run-time slowdown column shows
the cost of reproducing the error in an instrumented version of the subject
program, and the mean is the geometric mean. The ConfDiagnoser time has
been divided into two parts ‚Äî computing thin slices and diagnosing an error
‚Äî and the mean is the arithmetic mean.
due to ConfDiagnoser‚Äôs naive, inefÔ¨Åcient instrumentation code,
which we have made no effort to optimize. Even so, an error
can be reproduced in less than 4 minutes on average, with a
worst case of 13 minutes.
3) Comparison with a Previous Technique: We compared
ConfDiagnoser with ConfAnalyzer, a dynamic information
Ô¨Çow-based technique [18]. We chose ConfAnalyzer because
it is the most recent technique and also one of the most pre-
cise conÔ¨Åguration error diagnosis techniques in the literature.
ConfAnalyzer tracks the Ô¨Çow of labeled objects through the
program dynamically, and treats a conÔ¨Åguration option as a
root cause if its value may Ô¨Çow to a crashing point. ConfAna-
lyzer works well for most of the crashing errors (all of which
are from the ConfAnalyzer paper [18]), though as described
above these are easy to diagnose even without tool support.
However, ConfAnalyzer cannot diagnose non-crashing errors.
The experimental results of ConfAnalyzer are shown in
Figure 6 (column ‚ÄúConfAnalyzer‚Äù). For the 9 crashing errors,
ConfDiagnoser produced better results for 3 of them, the same
results for another 3, and worse results for the remaining 3.
ConfAnalyzer performs best on the easiest crashing errors:
those having short execution paths, when an error exhibits al-
most immediately after the software is launched. In such cases,
only a small number of conÔ¨Åguration options are initialized
and few of them can Ô¨Çow to the crashing point. ConfDiagnoser
fails to produce a good diagnosis for these errors, because it can
not identify the statistically-behaviorally-deviated predicates
based on the limited observation of program behaviors.
ConfAnalyzer outputs less accurate or no results for errors
where the root cause option value Ô¨Çows into containers or
system calls (e.g., error #14 in Figure 6). ConfDiagnoser
can reason (to some extent) about the consequence of such a
misconÔ¨Åguration based on the observed predicate behaviors.4) Comparison with Two Fault Localization Techniques:
Another possible way to diagnose a conÔ¨Åguration error is to
leverage existing fault localization techniques, by treating the
undesired execution as a failing run and all correct executions
(in the database) as passing runs. We next compare Conf-
Diagnoser with two state-of-the-art techniques:
Statement-level Coverage Analysis . This technique treats
statements covered by the undesired execution proÔ¨Åle as po-
tentially buggy, and statements covered the correct execution
proÔ¨Åles as correct. Then, it leverages a well-known fault
localization technique, Tarantula [10], to rank the likelihood
of each statement being buggy, and queries the results of
thin slicing to identify its affecting conÔ¨Åguration options as
the root causes. The results are essentially the same for a
variant of coverage analysis: using thin slicing to compute
all affected statements, and only monitoring the coverage
of such affected statements.
Method-level Invariant Analysis . This technique stores
invariants detected by Daikon [5] from correct executions in
the database. It treats a method as having suspicious behav-
ior if its observed invariants from the undesired execution
are different from the invariants stored in the database [14].
This technique ranks a method‚Äôs suspiciousness by the num-
ber of different invariants, and queries the results of thin
slicing to identify its affecting conÔ¨Åguration options as the
root causes.
In Coverage Analysis, the statement-level granularity is too
Ô¨Åne-grained . Many statements have exactly the same coverage
in the failing/passing executions, and thus have the same sus-
piciousness score as computed by Tarantula [10]. Furthermore,
Tarantula only records whether a statement has been executed
or not but does not record how a statement is executed (e.g.,
how often a predicate evaluates to true). The combination of
these two factors causes the low accuracy.
In Invariant Analysis, the method-level granularity is too
coarse-grained . Invariant detection techniques like Daikon [5]
only check program states at method entries and exits to infer
likely pre- and post-conditions, and thus are less sensitive to
control Ô¨Çow details within a method (e.g., a predicate‚Äôs true
ratio). In our study, Invariant Analysis failed to diagnose 3
errors. For Synoptic, it failed to infer invariants. For Soot
and Randoop, it reported the same invariants over undesired
and correct executions, for the method containing behaviorally-
deviated predicates.
This experiment suggests that we cannot treat conÔ¨Åguration
options as just another regular program input, and then directly
apply existing fault localization techniques [10], [14] to Ô¨Ånd the
error causes. The primary reason is that, unlike a program input,
a conÔ¨Åguration option is often used to control a program‚Äôs
control rather than produce result data. Thus, focusing on the
behaviors of relevant predicates as our tool does may be a
good choice.
5) Evaluation of Two Design Choices in ConfDiagnoser:
We investigate the effects of:
using traditional full slicing [7] rather than thin slic-
ing [20] in the ConÔ¨Åguration Propagation Analysis stepError ID. Rank of the Actual Root Cause
Program All ProÔ¨Åles Random Selection Similarity-Based
Non-crashing errors
1. Randoop 1 2 1
2. Weka 7 6 1
3. JChord 16 19 3
4. Synoptic 1 1 1
5. Soot 13 13 2
Average 7.6 8.2 1.6
Crashing errors
6. JChord 1 1 1
7. JChord 1 1 1
8. JChord 17 17 17
9. JChord 1 1 1
10. JChord 15 15 15
11. JChord 16 16 16
12. JChord 25 25 1
13. JChord 1 1 1
14. JChord 9 9 8
Average 9.4 9.4 6.7
Fig. 8. Comparison with different execution proÔ¨Åle selection strategies
(Section IV-C 5). The last column ‚ÄúSimilarity-based‚Äù is the selection strategy
used in ConfDiagnoser, and the data in that column is taken from Figure 6.
(Section II-B) to compute the affected predicates. Figure 6
(Column ‚ÄúFull Slicing‚Äù) shows the results.
varying the comparison execution proÔ¨Åles from the pre-built
database. In particular, we compare the similarity-based
selection strategy used in ConfDiagnoser (Section II-D1)
with two alternatives: selecting all available proÔ¨Åles in
the database, and randomly selecting the same number of
proÔ¨Åles as ConfDiagnoser uses from the database. Figure 8
shows the results. For random selection, we performed the
experiment 10 times and report the average.
As shown in Figure 6 (Column ‚ÄúFull Slicing‚Äù), Conf-
Diagnoser achieves substantially less accurate results when
using full slicing. The primary reason is that full slicing in-
cludes too many irrelevant statements that are only indirectly
affected by a conÔ¨Åguration option value but not pertinent to the
task of error diagnosis. In many cases, monitoring the control
Ô¨Çow of such indirectly-affected predicates and then linking
their behaviors to conÔ¨Åguration options leads to low accuracy.
Furthermore, performing full slicing is much more expensive
than thin slicing; in our experiments, the full slicing algorithm
ran out of memory on Soot.
As shown in Figure 8, varying the selection strategy for
correct traces can affect the results, depending on the applica-
tion being analyzed. Using all available execution proÔ¨Åles or
randomly selecting execution proÔ¨Åles is less effective, because
they make ConfDiagnoser report many irrelevant differences
between an undesired execution and a dramatically different
execution. When diagnosing a crashing error, ConfDiagnoser
is less sensitive to the comparison execution proÔ¨Åles. This
is because crashing proÔ¨Åles are often much smaller, execut-
ing fewer predicates before reaching the crashing points; and
ConfDiagnoser reduces each correct execution proÔ¨Åle before
diagnosis (Section II-D1). Thus, many irrelevant differences
have already been removed.Because the trace selection strategy improved the accuracy
of ConfDiagnoser, we tried applying it to Coverage Analysis
and Invariant Analysis as well. That is, we supplied those
analyses not with the full database of good runs, but with the
runs most similar to the bad run. This approach degraded
the accuracy of the other tools beyond the results shown in
Figure 6, even though it helped ConfDiagnoser. The reason is
that the suspiciousness of a statement or method is inversely
proportional to the number of correct execution proÔ¨Åles that
cover it. When using fewer correct execution proÔ¨Åles, more
statements or methods have the same suspiciousness scores.
D. Experimental Discussion
Limitations. The experiments indicate several limitations of
our technique. First, we only focus on named conÔ¨Åguration
options with a common key-value semantic, and our imple-
mentation and experiments are restricted to Java. Second, we
evaluated ConfDiagnoser on conÔ¨Åguration errors involving just
one mis-conÔ¨Ågured option. Third, our implementation cur-
rently does not support debugging non-deterministic errors.
For non-deterministic errors, ConfDiagnoser could potentially
leverage a deterministic replay system that can capture an unde-
sired non-deterministic execution and faithfully reproduce it for
later analysis. Fourth, ConfDiagnoser‚Äôs effectiveness largely
depends on the availability of a similar but correct execution
proÔ¨Åle. Using an arbitrary execution proÔ¨Åle (as we demon-
strated in Section IV-C 5 by random selection) may signiÔ¨Åcantly
affect the results.
Threats to Validity. There are three major threats to validity
in our evaluation. First, the 5 programs and the conÔ¨Åguration
errors may not be representative. Thus, we can not claim
the results can be generalized to an arbitrary program. For
example, we did not evaluate ConfDiagnoser to diagnose mis-
conÔ¨Ågurations that cause poor performance. Second, in this
paper, we focus speciÔ¨Åcally on conÔ¨Åguration errors, assuming
the application code is correct. Furthermore, in our experi-
ments, all 14 errors have been minimized (as end-users often
do when reporting an error). ConfDiagnoser might produce
different error diagnosis results on buggy application code with
non-minimized inputs. A user who did not know whether a
program was misbehaving due to a bug in the code or an
incorrect conÔ¨Åguration option would need to apply multiple
debugging techniques. We have not yet formulated guidance
regarding when the user should give up on ConfDiagnoser and
assume the error is not related to a conÔ¨Åguration option. Third,
we only compared two dependence analyses (thin slicing and
full slicing), three abstraction granularities (at the predicate
level, statement level [10], and method level [5]), and three
other tools (ConfAnalyzer, Coverage Analysis, and Invariant
Analysis) in our evaluation. Using other dependence analyses,
abstraction levels, or tools might achieve different results.
Experimental Conclusions. We have three chief Ô¨Åndings: (1)
ConfDiagnoser is effective in diagnosing both crashing and
non-crashing conÔ¨Åguration errors with a small proÔ¨Åle database.
(2)ConfDiagnoser produces more accurate diagnosis than ap-
proaches leveraging existing fault localization techniques [10],[14]. (3)Thin slicing and selection of similar comparison traces
permit ConfDiagnoser to produce more accurate diagnosis than
other approaches.
V. R ELATED WORK
This section discusses closely-related work on software
conÔ¨Åguration error diagnosis, automated debugging, and
conÔ¨Åguration-aware software analysis techniques.
Software ConÔ¨Åguration Error Diagnosis. Software conÔ¨Ågu-
ration error diagnosis is recognized as an important research
problem [1], [2], [12], [18], [25], [28]. Chronus [28] relies
on a user-provided testing oracle to check the behavior of the
system, and uses virtual machine checkpoint and binary search
to Ô¨Ånd the point in time where the program behavior switched
from correct to incorrect. AutoBash [21] Ô¨Åxes a misconÔ¨Ågu-
ration by using OS-level speculative execution to try possible
conÔ¨Ågurations, examine their effects, and roll them back when
necessary. PeerPressure [25] uses statistical methods to com-
pare conÔ¨Åguration states in the Windows Registry on different
machines. When a registry entry value on a machine exhibiting
erroneous behavior differs from the value usually chosen by
other machines, PeerPressure Ô¨Çags the value as a potential
error. More recently, ConfAid [2] uses dynamic taint analysis
to diagnose conÔ¨Åguration problems by monitoring causality
within the program binary as it executes. ConfAnalyzer [18]
uses dynamic information Ô¨Çow analysis to precompute possible
conÔ¨Åguration error diagnoses for every possible crashing point
in a program.
Our technique is signiÔ¨Åcantly different from the other ap-
proaches. First, most previous approaches focus exclusively
on conÔ¨Åguration errors that lead to a crash or assertion fail-
ure [1], [2], [18], [28]. By contrast, our technique can diagnose
both crashing andnon-crashing errors. Second, several ap-
proaches [2], [28] assume the existence of a testing oracle that
can check whether the software functions correctly. However,
such oracles are often absent in practice or may not apply to
the speciÔ¨Åc conÔ¨Åguration problem. A typical software user
should not be expected, to invest the substantial time and effort
to create an oracle. By contrast, our technique eliminates this
assumption. Third, approaches like PeerPressure [25] beneÔ¨Åt
from the known schema of the Windows Registry, but cannot
detect conÔ¨Åguration errors that lie outside the registry. Our
technique of analyzing the affected predicate behavior is more
general.
Automated Debugging Techniques. Program slicing [7] and
taint analysis [3] are two well-known techniques to determine
which statements and inputs could affect a particular variable.
Despite their effectiveness in diagnosing a crashing conÔ¨Ågura-
tion error by performing backward reachability analysis from
the crashing point [2], [18], these two techniques cannot be
directly applied to diagnose a non-crashing error.
Delta debugging [32] is a general algorithm to isolate soft-
ware error causes. It reduces differences between a working
state and a broken state to isolate a set of failure-inducing
changes. When using delta debugging, the user must provide
a single nearby working state and a testing oracle to checkwhether an intermediate program state behaves correctly ‚Äî
both of which are difÔ¨Åcult tasks. Furthermore, as a minimiza-
tion technique, delta debugging is not applicable to data (i.e.,
conÔ¨Åguration option values) that are missing nor ones that
are incorrect. By contrast, ConfDiagnoser eliminates these
assumptions by comparing the undesired execution proÔ¨Åle with
correct execution proÔ¨Åles, and then identifying the root cause
conÔ¨Åguration options to account for the behavioral difference.
Statistical algorithms have been applied to the automated
debugging domain. The CBI project [13] analyzes executions
collected from deployed software to isolate software failure
causes. SigniÔ¨Åcantly different than ConfDiagnoser, CBI corre-
lates a predicate‚Äôs evaluation result (either true or false) rather
than its true ratio and execution frequency with the observed be-
haviors. In addition, CBI identiÔ¨Åes likely buggy statements as
its Ô¨Ånal output, while ConfDiagnoser identiÔ¨Åes the behaviorally-
deviated program predicates and links the undesired behavior
to speciÔ¨Åc conÔ¨Åguration options.
ConÔ¨Åguration-Aware Software Analysis and Testing. Empir-
ical studies show that conÔ¨Åguration errors are pervasive, costly,
and time-consuming to diagnose [8], [31]. To alleviate this
problem, researchers have designed various software analysis
techniques to understand and test the behavior of a conÔ¨Ågurable
software system [6], [17]. Compared to ConfDiagnoser, those
techniques can be used to Ô¨Ånd new errors in a conÔ¨Ågurable
software system earlier, but cannot identify the root cause of
a revealed conÔ¨Åguration error. By contrast, our technique is
designed to diagnose an exhibited error.
VI. C ONCLUSION AND FUTURE WORK
This paper presented a practical technique (and its tool imple-
mentation, called ConfDiagnoser) for diagnosing conÔ¨Åguration
errors. Our experimental results show that ConfDiagnoser is
effective in diagnosing both crashing and non-crashing con-
Ô¨Åguration errors, and it does so with a small proÔ¨Åle database.
The source code of ConfDiagnoser is publicly available at
http://conÔ¨Åg-errors.googlecode.com.
Future work should address the following topics:
User study. We plan a user study to evaluate Conf-
Diagnoser‚Äôs usefulness to system administrators and end-users.
A challenge will be Ô¨Ånding study participants who are familiar
with our subject programs.
Fixing conÔ¨Åguration errors. After a conÔ¨Åguration error is
localized, Ô¨Åxing it is often non-trivial. Thus, we plan to apply
automated error patching [30] and software self-adaptation
techniques [26] to Ô¨Åx conÔ¨Åguration errors.
Improving conÔ¨Åguration error reporting. A high-quality
error report allows software developers to understand and cor-
rect the problems they receive. Unfortunately, the quality of
error reports often decreases as software becomes more com-
plex and conÔ¨Ågurable. We plan to develop new error reporting
mechanisms to make conÔ¨Åguration errors more diagnosable.
ACKNOWLEDGEMENTS
This work was supported in part by ABB Corporation and
NSF grant CCF-1016701.REFERENCES
[1]M. Attariyan and J. Flinn. Using causality to diagnose conÔ¨Åguration
bugs. In USENIX ATC , 2008.
[2]M. Attariyan and J. Flinn. Automating conÔ¨Åguration troubleshooting
with dynamic information Ô¨Çow analysis. In OSDI , 2010.
[3]J. Clause, W. Li, and A. Orso. Dytan: A generic dynamic taint analysis
framework. In ISSTA , 2007.
[4] CLOC. http://cloc.sourceforge.net/.
[5]M. D. Ernst, J. Cockrell, W. G. Griswold, and D. Notkin. Dynamically
discovering likely program invariants to support program evolution. In
ICSE , 1999.
[6]B. J. Garvin, M. B. Cohen, and M. B. Dwyer. Using feature locality:
can we leverage history to avoid failures during reconÔ¨Åguration? In
ASAS , 2011.
[7]S. Horwitz, T. Reps, and D. Binkley. Interprocedural slicing using
dependence graphs. In PLDI , 1988.
[8]A. Hubaux, Y . Xiong, and K. Czarnecki. A user survey of conÔ¨Åguration
challenges in Linux and eCos. In VaMoS , 2012.
[9] JChord. http://pag.gatech.edu/chord/.
[10] J. A. Jones, M. J. Harrold, and J. Stasko. Visualization of test information
to assist fault localization. In ICSE , 2002.
[11] A. Kapoor. Web-to-host: Reducing the total cost of ownership. Technical
Report 200503, The Tolly Group , May 2000.
[12] L. Keller, P. Upadhyaya, and G. Candea. ConfErr: A tool for assessing
resilience to human conÔ¨Åguration errors. In DSN , 2008.
[13] B. Liblit, M. Naik, A. X. Zheng, A. Aiken, and M. I. Jordan. Scalable
statistical bug isolation. In PLDI , 2005.
[14] S. McCamant and M. D. Ernst. Predicting problems caused by component
upgrades. In ESEC/FSE , 2003.
[15] NanoXML. http://nanoxml.sourceforge.net//.
[16] C. Pacheco, S. K. Lahiri, M. D. Ernst, and T. Ball. Feedback-directed
random test generation. In ICSE , 2007.
[17] X. Qu, M. B. Cohen, and G. Rothermel. ConÔ¨Åguration-aware regression
testing: an empirical study of sampling and prioritization. In ISSTA ,
2008.
[18] A. Rabkin and R. Katz. Precomputing possible conÔ¨Åguration error
diagnoses. In ASE, 2011.
[19] Soot. http://www.sable.mcgill.ca/soot/.
[20] M. Sridharan, S. J. Fink, and R. Bodik. Thin slicing. In PLDI , 2007.
[21] Y .-Y . Su, M. Attariyan, and J. Flinn. AutoBash: improving conÔ¨Åguration
management with operating system causality analysis. In SOSP , 2007.
[22] W. N. Sumner, T. Bao, and X. Zhang. Selecting peers for execution
comparison. In ISSTA , 2011.
[23] Synoptic. http://code.google.com/p/synoptic/.
[24] WALA. http://sourceforge.net/projects/wala/.
[25] H. J. Wang, J. C. Platt, Y . Chen, R. Zhang, and Y .-M. Wang. Automatic
misconÔ¨Åguration troubleshooting with PeerPressure. In OSDI , 2004.
[26] Y . Wang and J. Mylopoulos. Self-repair through reconÔ¨Åguration: A
requirements engineering approach. In ASE, 2009.
[27] Weka. www.cs.waikato.ac.nz/ml/weka/.
[28] A. Whitaker, R. S. Cox, and S. D. Gribble. ConÔ¨Åguration debugging as
search: Ô¨Ånding the needle in the haystack. In OSDI , 2004.
[29] I. H. Witten, A. Moffat, and T. C. Bell. Managing Gigabytes: Compress-
ing and Indexing Documents and Images . Morgan Kaufmann, 1996.
[30] Y . Xiong, A. Hubaux, S. She, and K. Czarnecki. Generating range Ô¨Åxes
for software conÔ¨Åguration. In ICSE , 2012.
[31] Z. Yin, X. Ma, J. Zheng, Y . Zhou, L. N. Bairavasundaram, and S. Pasu-
pathy. An empirical study on conÔ¨Åguration errors in commercial and
open source systems. In SOSP , 2011.
[32] A. Zeller. Isolating cause-effect chains from computer programs. In
FSE, 2002.
[33] S. Zhang, Y . Lin, Z. Gu, and J. Zhao. Effective identiÔ¨Åcation of failure-
inducing changes: a hybrid approach. In PASTE , 2008.
[34] S. Zhang, D. Saff, Y . Bu, and M. D.Ernst. Combined static and dynamic
automated test generation. In ISSTA , 2011.
[35] S. Zhang, C. Zhang, and M. D. Ernst. Automated documentation inference
to explain failed tests. In ASE, 2011.
[36] X. Zhang, R. Gupta, and Y . Zhang. Precise dynamic slicing algorithms.
InICSE , 2003.