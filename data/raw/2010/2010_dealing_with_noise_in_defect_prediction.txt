Dealing with Noise in Defect Prediction 
Sunghun Kim1, Hongyu Zhang2, Rongxin Wu2 and Liang Gong2 
1 Department of Computer Science and Engineering, Hong Kong University of Science and Technology , Hong Kong  
2 School of Software, Tsinghua Universit y, Beijing, China  
hunkim@cse.ust.hk, hongyu@tsinghua.edu.cn, {se.wu.rongxin,  jacksongl1988}@gmail.com
 
ABSTRACT  
Many software defect prediction models have been built  using 
historical  defect data obtained by mining software repositories 
(MSR).  Recent stu dies have discovered that data so collected  
contain noise s because current defect collection practice s are 
based on optional bug fix keywords or bug report links in change 
logs. Automatically collected defect data based on the change log s 
could  include noi ses.  
This paper proposes approaches to deal with the nois e in defect 
data. First, we measure the impact of noise on defect prediction 
models and provide guidelines f or acceptable noise level. We 
measure noise resistant ability of two well -known defect 
prediction algorithms and find that in general, for large defect 
datasets, adding FP (false positive) or FN (false negative) noise s 
alone does not lead to substantial performance differences. 
However, the prediction performance decreases signific antly 
when th e dataset contains 20% -35% of both FP and FN noises . 
Second, we propose a noise detecti on and eliminati on algorithm  to 
address this problem . Our empirical study shows that our 
algorithm can identify  noisy instances  with reasonable  accuracy . 
In addition, after eliminating the noise s using  our algorithm , 
defect prediction accuracy  is improved . 
Categories and Subject Descriptors 
D.2.7 [ Software Engineering ]: Distribution, Maintenance, and 
Enhancement ‚ÄìRestructuring, reverse engineering, and 
reengineering , D.2.8  [Software Engineering ]: Metrics ‚Äì Product 
metric s, K.6.3 [ Management of Computing and Information 
Systems ]: Software Management  ‚Äì Software maintenance  
General Terms  
Algorithms, Measurement, Experimentation  
Keywords  
Defect prediction, noise resistance, bug gy changes, buggy files, 
data quality.  
1. INTRODUCTION 
Defect prediction is a very active area in software engineering 
research [7, 10, 11, 13, 18, 19, 33, 35] . Many effective new 
metrics and algorithms to predict defect -proneness have  been  proposed. When re searchers evaluate their new algorithms or 
metrics, they often use defect information collected from  the 
change logs in Software Configuration Management (SCM) 
systems and from the bug reports in bug tracking systems.  
Unfortunately, recent studies have found that extracted defect 
information from change logs and bug reports are noisy. For 
example, Aranda  and Venolia et al.  [1] manually inspected ten 
bug reports in Microsoft and interviewed developers related to the 
reports. They found lots of important information missi ng in bug 
reports. Bird et al. [4] also studied the quality of change logs and 
bug reports, and found that many change logs  and bug reports 
were not  linked . They also found that the noisy defect information 
could seriously affect the performan ce of a bug prediction 
algorithm.  
These surprising findings challenge the validity of all existing bug 
prediction algorithms by r aising important questions: How could 
we deal with the noise in the defect data? Are existin g defect 
prediction algorithms stil l useful if their prediction models are 
trained by noisy defect data? How much noise is acceptable for 
bug prediction algorithms? How could we detect and eliminate the 
noise? 
This paper addresses these questions. First, we propo se a method, 
which intention ally adds false positive and negative information 
only in the training data to measure noise resistan ce of a given 
bug prediction algorithm. Using the proposed method, we 
measure noise resistan ce of two well -known bug prediction 
algorithms, change classification and buggy file prediction . We 
found that these two algorithms are relatively nois e resistant. 
When there are enough buggy instances in the dataset s, defect 
prediction  performance (measured in terms of F -measure) does 
not decrease significantly with the increases of false positive  or 
false negative noises. We also find that these algorithms are more 
resistant to false negative noise s. However, the prediction 
performance decreases significantly when the dataset contains 
20%-35% of both FP and FN noises .  
Second, we propose an algorithm to detect and eliminate noises in 
the defect data  to address the noisy data problem . We 
experimental ly evaluate our algorithm  and t he results  show that it 
can identify  noisy instances  with reasonable  accuracy. In addition , 
after eliminating the noise s using  our algorithm , the defect 
prediction accuracy  is improved .  
Overall, this paper makes the following contributions:  
‚Ä¢ Noise resistance  measuring technique : We propose a 
method to measure noise resistan ce of defect prediction  
models .  
‚Ä¢ Empirical study of measuring noise resistan ce: We apply 
the resistan ce measuring method for two well -known 
prediction algorithms, and provide guidelines for  acceptable 
noise level.   
Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies ar e 
not made or distributed for profit or commercial advantage and that 
copies bear this notice and the full ci tation on the first page. To copy 
otherwise, or republish, to post on servers or to redistribute to list s, 
requires prior specific permission and/or a fee.  
ICSE ‚Äô11, May 21 -28, 2011, Waikiki, Honolulu, HI, USA.  
Copyright 2011 ACM 978 -1-4503 -0445 -0/11/05... $10.00.   
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proÔ¨Åt or commercial advantage and that copies
bear this notice and the full citation on the Ô¨Årst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciÔ¨Åc
permission and/or a fee.
ICSE ‚Äô11, May 21‚Äì28, 2011, Waikiki, Honolulu, HI, USA
Copyright 2011 ACM 978-1-4503-0445-0/11/05 ...$10.00
481‚Ä¢ Noise detection technique: We propose an accurate noise 
detection algorithm, which also improves defect prediction 
accuracy.  
In the remainder of the paper, we start by presenting the 
background  on defect  prediction algorithms in Section 2. In 
Section 3 , we discuss the noisy defect data issue. We propose a 
noise resis tance measuring method in Section 4 and apply it for 
change classification and buggy file prediction  in Section 5. We 
present our noise detection algorithm in Section 6. Section 7 
discusses the threats to validity . We r ound off the paper with 
related work in Section 8 and conclusions in Section 9. 
2. BACKGROUND 
2.1 A General Defect Prediction Process 
Before measuring noise resistance of defect  prediction algorithms, 
we describe a common defect prediction process  as shown in 
Figure 1 . Then we introduce two well-known defect prediction 
algorithms used in this paper.  
Instances 
(TRUE)
Training 
instancesMachine
Learnerinstance
Prediction
TRUE or 
FALSEinstances 
(FALSE)features
FeaturesSoftware 
Archives
(1) Labeling
(TRUE or FALSE)(2) Feature
extraction(3) Creating 
a training corpus(4) Building 
a prediction model(5) Prediction &
evaluation  
Figure 1. A general defect  prediction process  
Before  design ing a prediction model , we need to specify  the 
predict ion target. A  prediction model can be used for predicting 
defect -proneness (buggy or clean) of different software entities, 
such as a component [18, 32] , file [16, 19, 33] or a change [9, 11]. 
After deciding the prediction target,  a general defect prediction  
process (Figure 1) can be as follow s: 
Labeling : Defect  data need to be collected for training a 
prediction model . This process typically involves extract ing 
instances  (data items)  from software archives and labeling them as 
TRUE (buggy) or FALSE (clean).  However, s ome recent studies 
have discovered that data collect ed by  mining software 
repositor ies often contain noise. N oisy data threat en the validity of 
prediction models. We will describe this issue  in Section 3.  
Extracting features  and creating  training corpus : This step 
extracts features for predict ing the labels  of instances. Common 
features for defect prediction  are complexity metrics, keywords, 
changes , and structural dependenc ies. By combining labels and 
features of instances, we can create a training corpus  to be used by 
a machine learner to construct a predi ction  model .  
Building prediction models: Using  a training corpus , general 
machine learners such as Support Vector Machines (SVM)  or 
Bayes Net can be used to build a prediction model. The model can 
then take a new instance and predict its label, i.e. TRUE or 
FALSE.  
Evaluation: To evaluate a prediction model, we need a testing 
data set besides a training set. We predict the labels of insta nces in 
the testing set and evaluate the prediction model by comparing th e 
prediction and real labels. To separate the t raining and testing sets, 
10-fold cross -validation is widely used.  
1 2 3 4 5 6 7 8 9 10
testing
set training set
  
Figure 2. 10-fold cross -validation  
In 10 -fold cross -validation, the data is divided equally into 10 
folds as shown in Figure 2. Then the instances in each fold are in 
turn used as a testing set and the remaining nine folds are used to 
train the model. For example, in the first iteration, instances in 
fold2  to fold10  are used as a training set, and fold1  as a testing set.  
There are four possible outcomes from a prediction model:  
classifying  a buggy instance  as buggy (nb‚Üíb), classifying a buggy 
instance  as clean ( nb‚Üíc), classifying  a clean instance  as clean 
(nc‚Üíc), and cla ssifying a clean instance  as buggy ( nc‚Üíb). The 
recall, precision, and F -measures  are widely used to evaluate 
prediction results  [27, 31] . We use these measures  to evaluate 
prediction models as  follows:  
‚Ä¢ Precision (bug gy) = nb‚Üíb
nb‚Üíb+nc‚Üíb 
This is t he number of correct classifications of the type 
(nb‚Üíb) over the total number of predicted buggy instances . 
‚Ä¢ Recall ( buggy ) = nb‚Üíb
nb‚Üíb+nb‚Üíc 
This is t he number of correct classifications of the type 
(nb‚Üíb) over the total number of actual buggy instances . 
‚Ä¢ F-measure  (buggy) = 2*P(b)*R(b)
P(b)+R(b) 
This is a  composite measure of precision and recall.  We use 
the F1 metric that weight s recall and precision equally [27].  
2.2 Software Defect Prediction Algorithms 
In this section, we describe two well -known defect  prediction 
models  used to measure noise resistance .  
2.2.1 Predicting Buggy Change s 
Change Classification (CC) learns buggy change patterns  from 
history, and predicts if a new change introduces bugs or not  
[9,11] . 
File at
Rev 3File at
Rev 1File at
Rev 2File at
Rev 4File at
Rev 
n-1File at
Rev nFile at
Rev n
+1......Change log (Rev n)
"Fixed issue #355"
Bug introducing change Fix change ?  
Figure 3. Change history with buggy and fix changes  
Suppose we have a change history of a file as shown in Figure 3. 
After learning from buggy and clean change patterns f rom 
revision 1 to revisio n n, CC predicts if the change in revision n+1 
introduces bugs.  
To learn from history, CC extracts features from eac h change. To 
label changes as buggy or clean changes, first we need to extract 
changes from project history. Then we identify fix changes  using 
change logs. To extract change history, we use Kenyon  [3], a 
system that extracts source code change histories from SCM 
systems such as CVS and Subversion. Kenyon automatically 
checks out the source code of each revision and extracts change 
482information  such as the change log, author, change date, source 
code, and change deltas.  
Once a commit has been determined to contain a fix, it is possible 
to trace backward in the revision history to determine when the  
fixed erroneous code is introduced in the syst em. We define that 
as a bug -introducing change. The  bug-introducing change 
identification algorithms proposed by ≈öliwerski  et al.  [23] and 
Kim et al. [11] are used.  
A file change involves two source code revisions (an old revision 
and a new revision) and a change delta that records the added 
code (added de lta) and the deleted code (deleted delta) between 
the two revisions. A file change has associated metadata, 
including the change log, author, and commit date. By mining 
change histories, we can derive features such as co -change counts 
to indicate how many files are changed together in a commit, the 
number of authors of a file, and the previous change count of a 
file. Every term in the source code, change delta, an d change log 
texts is used as features. Detailed feature extraction methods o f 
CC can be found in [11] . 
2.2.2 Predicting Buggy Files 
Another common defect prediction model is identifying b uggy 
files in advance. It is widely believed that some inter nal properties 
of software (e.g., metrics) have relationship s with external 
properties (e.g., defects ). In re cent years, many defect prediction 
models based on software metrics  have been proposed ( e.g., [13, 
16, 18, 19, 33]). These prediction models identify code features 
(expressed as measurement data), learn a classification model 
from historical defect data, a nd use the constructed model to 
predict defect -proneness of a new program module . 
Many code features can be extracted from software projects to 
predict defective files. These features include complexity metri cs 
(such as lines of code , cyclomatic complexity, number of classes, 
etc.), process metrics (such as the number of lines of code ch anges, 
the number of file changes, etc.) and resource metrics (suc h as 
developer information, etc). All these metrics, or a combination of  
these metrics, can be used to buil d effective software defect 
prediction models.  
3. NOISE S IN DEFECT DATA 
Both prediction algorithm s described in Section 2 require labels 
(buggy or clean) to build and evaluate models. In this section, we 
discuss typical techniques to identify labels and the noise in the 
labels.  
To label a file/change as buggy or clean, many resea rchers mine 
the bug database and version achieves  for open source systems. 
Two approaches are widely used : searching  for keywords such as 
"Fixed" or "Bug" [14] and searching  for refere nces to bug reports 
like ‚Äú#42233‚Äù [23] . We use both techniques  in our experiments . 
Chen et al. [5] studied open source change log  quality . They 
checked the correctness of each change log and found almost all 
logs were correc t.  
Some open source projects have strong guidelines for  writing their 
change logs. For example, 100% of Columba ‚Äôs change logs used 
in our experiment have a tag such as ‚Äò[bug] ‚Äô, ‚Äò[intern] ‚Äô, ‚Äò[feature] ‚Äô, 
and ‚Äò[ui]‚Äô. Usually, Eclipse developers lea ve relevant bug report 
IDs in their change logs.  
However, s ome recent studies (such as those reported by Bird et al. 
[4]) discovered that data collected via mining software repositories  (MSR)  often contain noise. They found that the 
number of linked bugs (bugs whose change logs and bug reports  
are linked) does not match the number of total fixed bugs (the 
ratio could be even lower than 50%), suggesting a high  percentage 
of false negatives in the defect dataset. This is because developers 
often do not write specific keywords or leave links for fix 
revisions. It is also possible that developers make mi stakes when 
they write keywords or links in the change logs. For this reas on, 
automatically collected defect data based on these keywords or 
links are inevitably  noisy. Recent studies have also found that 
noisy data (in training and testing sets) affect performance of 
prediction models  [4].  
We also performed a replication study of Bird et al. ‚Äôs experiments. 
Our results confirm their findings  about noisy defect da ta. For 
example, for the Eclipse SWT component, there are 32% unlinked 
bugs (bugs that do not reflected in CVS logs) in Eclipse 3.0 and 
21% unlinked bugs in Eclipse 3.1. The existence of the unlinked 
bugs indicates that the defect data collected via MSR is  noisy. We 
also notice d that the noise level decreased in Eclipse 3.4, where 
92.27% SWT bugs are recorded in CVS logs. In this paper, we 
measure the effect of noise on two defect prediction models 
described in Section 2, and propose an algorithm to detect the 
noise in Section 6.  
4. EXPERIMENTAL SETUP 
4.1 Research Questions 
Our experiments are designed to address the following research 
questions:  
RQ1 : How resistant a defect prediction model is to false negative 
(FN) buggy data?  
RQ2 : How resistant a defect predicti on model is to false positive 
(FP) buggy data?  
RQ3 : How resistant a defect prediction model is to both false 
negative (FN) and false positive (FP) buggy data?  
As Bird et al. [4] found out, developers often forget to leave 
explicit messages or links to indicate buggy changes. Since most 
automatic buggy change /file identification s are  based on special 
keywords and links [11, 16, 28, 33, 36]  in the change logs,  this 
will lead to false negatives (missing some buggy chang es) in the 
automatically identified data.  RQ1 measures predictor resistance 
for this case.  
On the other hand, it is possible that developers label a change /file 
as buggy by leaving special keywords and bug report links, 
together with some non bug -fix changes  in one commit . This 
behavior leads to false positives (identifying non -buggy 
changes /files  as buggy). RQ2 measures resistance of defect 
prediction models to false positives in the training data set.  
Finally, RQ3 measures the noise resistan t ability of defect 
prediction models when data has both false positives and false 
negatives.  
4.2 Making Noisy Data 
To address the research questions , we first need  a golden set , 
which contains no FPs and FNs . In addition, we need noisy data 
sets. However, it is very hard to get a golden set.  In our approach, 
we carefully select high quality datasets and assume them  the 
golden set s. Then, to create noise sets, we add FPs and FNs 
intentionally  into the golden sets. To add FPs and FNs, we 
483randomly selects instances in a golden set and artifi cially change 
their label s from buggy to clean or from clean to buggy , inspired 
by experiments in [4]. 
testing set original training setXXXX XXXXXXXXX
X
X: buggy labelled 
instanceBiased training set: false negative instancesX XXXX
1
Adding 
buggy labels2Removing 
buggy labels
Biased training set: false positive instancesXXXX XXXXXXX
XX X
XXXX XXX
 
Figure 4. Creating biased training set  
To make FN data sets (for RQ1), we randomly select n% buggy 
labeled instances and change their labels to clean, as shown in 
Figure 4 (1). Similarly, to make FP data sets (for RQ2) , we select 
n% of clean labeled instances and change their labels to buggy, 
which adds false buggy changes, as shown in Figure 4 (2). For the 
FN and FP data sets (for RQ3), we select random  n% of instances, 
and change their labels. For example, if a clean -labeled instance is 
selected, we change its label to buggy. If a buggy instance is 
selected, we change its label to clean.  
It is very important to note that we add noise only in  the traini ng 
set, not in the testing set. For testing, we use the original golden 
set. In this way, we can measure the accuracy of a defect 
prediction model, which is trained from noisy data sets, to predict 
buggy/clean changes in the golden set.  
In this paper, we use the 10 -fold cross validation  described in 
Section  2. First, we group 9 folds to be used as a training set . 
Then, we add noise only in the training set and leav e the testing 
set unchanged.   
For the machine learner , we use the Bayes Net classifier (the 
Weka implementation [ 26]). Bayesian networks have good 
performance when dealing with a large number of variables with 
much variance in values [27]. We also compare performances of 
other machine learners in Section 5.3.2 . 
4.3 Dummy Predictor 
An effective defect prediction model should outperfor m at least 
random guessing ‚Äì guessing a change/file as buggy or clean 
purely at random. We call a predictor based on random guessing a 
dummy predictor . Since there are only two labels, buggy and 
clean changes, the dummy predictor could also achieve certain 
prediction accuracy. For example, if there are 30% buggy changes 
in a project, by predicting all changes as buggy, the buggy r ecall 
would be 1 and the precision would be 0.3. I t is also possible that 
the dummy predictor randomly predicts a change as buggy o r 
clean with 0.5 probability. In this case, the buggy recall would  be 
0.5, but still the precision is 0.3.  
We use the F -measure of the dummy predictor as a reference line 
when  measuring the noise resistance of defect prediction models . 
We compute the dummy F-measure assuming the dummy predictor randomly predicts 50% as buggy and 50% as clean. For 
example, for a project with 30% buggy changes, the dummy 
buggy F -measure is 0.375 
Ô£∑Ô£∑
Ô£∏Ô£∂
Ô£¨Ô£¨
Ô£≠Ô£´
+√ó√ó30. 0 5 . 030. 0 5 . 02. 
5. NOISE RESISTAN CE 
This section reports our experiments on the  impact of noise on 
two defect prediction algorithms  and discusses the results.  
 
5.1 Noise Resistance of Change Classification 
5.1.1 Subject Programs  
We use Columba , Eclipse JDT.Core  and Scarab as our subjects 
for this experiment  (Table 1), as these projects have high quality 
change logs and links between changes logs and bug reports.  For 
the first two projects, w e adopt  the exact dataset s used in  [11], 
which were also  used by  other r esearchers [2, 21]. We assume 
these datasets  as golden set s and use them to measure noise 
resistance . 
5.1.2 Original Accuracy  
First, we build a CC prediction model using the orig inal training 
set and measure the performance of  the model using a testing set.  
Figure 5 shows the buggy recall, prediction and F -measure. 
Overall, the accuracy results for the first two projects are 
comparable to those reported i n [11]  (the small variations in 
results coming from the use of Bayes  Net instead of SVM and the 
randomness in the 10 -fold cross -validation.) For Columba, the 
buggy precision and recall are around 0.5 to 0.55. For Eclipse, the 
buggy recall is 0.88, and precision is 0.48. We notice that the 
precision for Eclipse reported in  [11] is 0.61 , which is higher than 
our precision, 0.48. However, our recall is 0.88 , which is much 
higher than the recall  0.61 reported in [11] . This happens due to 
the recall -precision tradeoff. To address this issue, we use F -
measure  [26] to measure the noise resistance of CC in this paper.  
                     #VHHZ1SFDJTJPO #VHHZ3FDBMM #VHHZ'NFBTVSF 
4DBSBC 
$PMVNCB 
&DMJQTF 
 
Figure 5. Defect prediction using the original  training set 
Table 1. Analyzed subject programs  for predicting buggy changes  
Project  Revisions  Period  # of clean 
instances  # of buggy 
instanc es % of buggy 
instances  # of features  
Columba  500-1000  05/2003 -09/2003  1,270  530 29.4 17,411  
Eclipse  500-750 10/2001 -11/2001  592 67 10.1 16,192  
Scarab  500-1000  06/2001 -08/2001  724 366 50.6 5,710  
484           
       !"##$%&'()*+",) 
-*.%/,*0101#%+)2%3*4+)%1)#*56)%-&7.%,*2) $PMVNCB 
&DMJQTF 
4DBSBC 
%VNNZGPS&DMJQTF 
%VNNZGPS$PMVNCB 
%VNNZGPS4DBSBC 
           
       !"##$%&'()*+",) 
-./%0,*1212#%+)3%4*5+)%67+189)%-&:/%,*3) $PMVNCB 
&DMJQTF 
4DBSBC 
%VNNZGPS$PMVNCB 
%VNNZGPS&DMJQTF 
%VNNZGPS4DBSBC 
  
           
       !"##$%&'()*+",) 
-./%0,*1212#%+)3%4*5+)%2)#*67)%-&8/%9%4*5+)%:;+167)% -&</%,*3) $PMVNCB 
&DMJQTF 
4DBSBC 
%VNNZGPS$PMVNCB 
%VNNZGPS&DMJQTF 
%VNNZGPS4DBSBC 
   
5.1.3 FN Resistance (RQ1)  
In this section, we meas ure the resistance of CC for false negative 
(FN) training sets. To add  FNs, we randomly select buggy 
instances in the training set  and label them as clean as shown in 
Figure 4 (1). In this way, we increase the rate of FN by changing 
buggy labels to clean. For example, suppose we have 100 buggy 
instances in a training set. Changing labels of 10 buggy instanc es 
to clean will add 10% FN.  
Figure 6 (a) s hows buggy F -measure results for Columba , Eclipse  
and Scarab with  various FN training sets. The x -axis indicates the 
FN rates. The dummy F -measures described in Section 4.3 are 
also shown in the Figures as the reference lines .  
For Columba, the buggy F -measure shows strong resistance 
against the FN training sets. The F -measure values are relatively  
stable. When th e noises reach 60% , the F -measure just drops  0.05.  
The same can be observed for  Scarab , the buggy F -measure is not  
affected by FN noises significantly . After 20% false negatives are 
injected into the training set, the F -measure is not changed. When 
the noi ses reach 60%, the F -measure only drops less than 0.05.  
For Eclipse, the buggy F -measure is just slightly affected by the 
FN training sets  too. After  adding 4 0% FN noises to the training 
set, the F -measure drops from 0 .62 to 0.55. When the noises reach 
60%, the F -measure still remains at 0.50.  
A possible explanation of th ese results is that the features 
characterizing  bugs are often common across the buggy changes. 
Therefore l osing some instances in the training set  does not lead 
to significant performanc e decrease .  
5.1.4 FP Resistance (RQ2)  
We also observe CC F -measures using FP training sets. We add 
FPs in to the training set s as described in Figure 4 (2) and then perform change classifications. The results are shown in Figure 6 
(b).  
For Columba and Scarab, the buggy F -measures are not 
significantly affected by the false positives. For Eclipse, buggy F -
measure s are affected by the FN training sets. After  adding 20% 
FP noises to the training set, the F -measure drops from 0 .6 to 0 .4. 
After having more than 50% FP noises , the F -measure is close to 
that of the dummy predictor.  
A possible explanation of the sensitivity of the Eclipse F -
measure s is the small number of buggy changes  in the dataset . 
There  are only 67 buggy changes as shown in Table 1. After 
adding  many FPs, the features that characterize bugs become less 
obvious  for classifiers  to learn. On the other hand, Columba and 
Scarab all have more than 300 buggy changes to learn  from, the 
features characteriz ing bugs can be still identified and prediction 
performance is still kept.  
5.1.5 FN and FP Resistance (RQ3)  
We also examine the prediction performance when the training 
sets contain both FP and FN noises. As shown in Figure 6 (c), th e 
trend of buggy F -measures for all projects decline  when the noise 
rate increase s.  
For Columba and Scarab, their F -measures only decrease by 0.1 -
0.15 when noise level reaches 60%. Interestingly, the F -measure 
of Eclipse decreases much faster than that of Columba and Scarab . 
Note that, Columba and Scarab have  many buggy/clean instance s 
and switching some labels dose not hurt the prediction too much. 
However, the F -measure of Eclipse significantly drops when the 
level of FP and FN noise increases . After the noises reach 40%, 
the Eclipse ‚Äôs F-measures are almost the same as the dummy F -
measures.  Figure 6. The impact  of noises on predicting 
buggy changes (a). F-measure for FN training 
sets; (b). F-measure for F P training sets; (c). F-
measure for FN&FP training sets . The Bayes 
Net machine learner is used. For Columba and 
Scarab, the F -measures are not affected by the 
noises significantly. For Eclipse, the F -measure 
drops significantly  when  the noise rate 
increases.  
485           
       !"##$%&'()*+",)%
-*.%/,*0101#%+)2%3*4+)%1)#*56)%-&7.%,*2)%%FCVH 
485 
%VNNZGPS%FCVH 
%VNNZGPS485 
           
       !"##$%&'()*+",)%
-./%0,*1212#%+)3%4*5+)%67+189)%-&:/%,*3)%%FCVH 
485 
%VNNZGPS%FCVH 
%VNNZGPS485 
 
           
       !"##$%&'()*+",)%
-./%0,*1212#%+)3%4*5+)%2)#*67)%-&8/%9%4*5+)%:;+167)% -&</%,*3)%%FCVH 
485 
%VNNZGPS%FCVH 
%VNNZGPS485 
  
5.2 Buggy File Prediction 
5.2.1 Subject Programs  
To obtain the ‚Äúgolden set ‚Äù for building prediction models for 
buggy -files, we use the SWT and Debug projects  in Eclipse 3. 4. 
We collected the defect data by mining the Eclipse B ugzilla and 
CVS repos itories.  We find that both projects  have a high 
percentage of linked bugs (bugs whose changes logs and bug 
reports  are linked). F or SWT, 92.27% bugs reported in Bugzilla 
are linked to changes . For Debug, 95.92% bugs are linked. 
Therefore, we use these two datasets as the golden sets .  
Table 2 summarizes the datasets used in this study. The  SWT  
dataset contains  1,485 Java source  files, among which 43.9% files 
are defective. The Debug  dataset  contains 1,065 files, among 
which 24.69% are defective . We have als o collected the following 
metrics  for each file in the projects. These metrics capture 
different aspects  of a file and are used as features for constructing 
our defect prediction model:  
‚Ä¢ Complexity metrics : including LOC (lines of code), average 
cyclomatic complexity  measure , maximum cyclomatic 
complexity measure.  
‚Ä¢ Object -oriented metrics : including the WMC, CBO, NOC, 
DIT, LCOM, RFC metrics that are proposed by Chidamber 
and Kemerer  [6]. 
‚Ä¢ Change metrics : including the number of added and deleted 
lines of code since the last major revision, the number of 
times the file is changed.  
‚Ä¢ Developer metric : the number of developers who changed 
the file.  Following the method described in Section 4.2, we intentionally  
make the dataset noisy by randomly select ing a given pe rcentage 
of instances and changing  their class labels (buggy or  clean ), thus 
artificially  creating false positives and false negatives. We again 
use 10 -fold cross validation  to evaluate  the prediction results. We 
first randomly partition the whole dataset into 10 folds. We use 9  
folds as a training set  and inject noise into them, and then use the 
remaining unchanged 1 fold as the testing set.  The Bayes Net 
classifier is used to construct the prediction model.  
Table 2. The dataset used for predicting buggy f iles 
Project  LOC  #programs  
(src files)  #defects  #defective  
programs  % of linked  
bugs  
SWT  386K  1485  556 653(43.97%)  92.27%  
Debug  77K 1065  294 263(24.69%)  95.92%  
5.2.2 FN Resistance (RQ1)  
Figure 7(a) show s how FN (false negative) training sets affect 
prediction performance. Clearly, the defect prediction model has 
strong resistance against the FN training sets. For SWT, t he buggy 
f-measure using the original dataset is 0.7 9. With the increases of 
noises, the prediction results  are still very stable (with f -meas ures 
around 0.7 8), even when the false negative  rate reaches 60% . 
Simil ar results are found for the Debug project, which exhibits 
stable performance until the FN rate reaches 50% . Although some 
buggy instances are marked as clean, the remaining buggy 
insta nces can capture the program features and can be still used 
for training prediction models effectively.  
5.2.3 FP Resistance (RQ 2) 
Figure 7(b) show s how FP (false positive) training sets affect 
prediction performance. Similarly , the defect prediction model has Figure 7. The impact  of noises on predicting 
buggy files (a). F-measure for FN training set s; 
(b). F-measure for F P training set s; (c). F-
measure for FN&FP training set s. The Bayes 
Net machine learner is used. The F -measures of 
the Debug and SWT projects are not affected by 
the FN or FP noises significantly. However, 
when FN&FP noises reach certain level, the F -
measures drop significantly.  
486resistance against the F P training sets. The prediction results  are 
very stable. For SWT, the F -measure values are all around 0.78 
even the FP rate is 60%. For Debug, the F -measures are about 
0.50 until the FP rate reaches 50%. The data noise introduced by 
false positives does not decrease the prediction accuracy  
significantly.  
5.2.4 FN and FP Resistance (RQ3)  
A training set may contain both false positives and f alse negatives. 
Figure 7(c) show s how FN  and FP noises affect prediction 
accuracy. For SWT, once the FN  and FP noise rate reaches 40%, 
prediction accuracy starts decreasing quickly. For Debug, 
prediction accuracy drops after the FN  and FP noise rate exceeds 
20%. These results show that FN  and FP noises  together  have 
larger impact  on defect prediction.  
5.3 Discu ssions 
5.3.1 Acceptable Noise Rate  
Many bug prediction approaches use software history to build 
prediction models. It is often very difficult to collect perfect 
historical dataset s that have no FPs and FNs. How much noise is 
acceptable for prediction approaches?  
Our experiments show that CC and the buggy file prediction yield 
reasonably stable accuracy at the presence of noises, when the 
Bayes Net learner is used. When the number of buggy instances is 
large enough, increasing FP or FN noises does not affect 
predi ction performance significantly. For datasets with both FP 
and FN noises, the prediction performance decreases when the 
noises increas e. When the number of buggy instances in a dataset 
is small, the prediction performance will be affected by noises 
signifi cantly.  
In defect prediction practices, FNs are more common as some 
defects recorded in bug tracking  systems are not linked to 
CVS/SVN logs [4]. FPs happen when developers leave a message 
saying he fixed a bug, but he actually did not. Chen et al. [5] 
studied the correctness of open source change logs , and they find 
that when developers leave a  message indicating fixing  of bugs, it 
is likely a real fix . Our experimental results show that noises in 
FN or FP alone do not affect prediction performance si gnific antly. 
Also, up to 20% -35% of FP and FN noise s (together) usually do 
not affect the performance significantly either .  
Obviously, our results may not be generalizable to all prediction 
models, but at least th ese can serve as  guideline s for CC and the 
buggy  file prediction users.  We suggest that b efore using th ese 
predictors, users can sample their data and manually inspect them 
to measure FP and FN rates. Based on the rates, they can d ecide if 
their defect data is applicable for these predictors.  
5.3.2 Noise  Resistances of Different Machine 
Learners  
In previous sections, we obtained our results using the  Bayes Net 
machine learner. In this section, we use Na√Øve Bayes, Support 
Vector Machines (SVM) and Bagging learners [26, 27] t o repeat 
the e xperiments and observe  the impact of data noises on 
prediction accuracy.  
Figure 8 show s the noise resistance ability of the four machine 
learners under different False Negative rates. Similar to Bayes  Net, 
the Na√Øve Bayes learner also has strong noise resistance ab ility 
when p redicting buggy files. The F -measures do not change 
significantly when FN rates are increasing. All Bayesian classifiers are based on the Bayer‚Äôs rule. The classifier is 
interested in the most probable hypothesis. Therefore, even if 
there is a certain amou nt of noise in the defect dataset, which 
could affect calculation of probability for some hypothesis, the 
Bayesian classifiers can still make correct classifi cations when the 
most probable hypothesis is preserved.  
The SVM learner performs poorly with noisy  data ‚Äì F-measures 
decrease quickly when FNs increase, until FN rate reaches 50%.  
SVM performs classification by constructing an N -dimensional 
hyperplane that optimally separates the data into two categories. 
The noise in the data could affect the construc tion of the 
hyperplane considerably. Therefore more noise could lead to more 
bias in classification.  
The Bagging (Bootstrap Aggregating) classifier is a machine 
learning algorithm that  ensembles  meta -algorithm s to build 
models. In th is experiment , we use the Multilayer Perceptron  
algorithm as the meta -algorithm  [26]. The Bagging classifier 
separates a training set into several new training sets by random 
sampling, and builds models based on the new training sets. The 
final classification result is obtained by the voting of each model. 
Figure 8 shows that Bagging can improve the original  prediction 
performance , and resist a certain amount of noises. However, 
when the noise level exceeds 40%, the probability of  each model 
making a wrong classification is incre asing, causing the quick 
drop of the performance.  

 !"##$%&'()*+",)%
-,*././#%+)0%1*2+)%/)#*34)%5&67%,*0)%#BZFT/FU 
/BÂôÜWF#BZFT 
47. 
#BHHJOH 
 
Figure 8. SWT defect prediction results of different machine 
learners ( F-measure s for FN training set ). 
6. HANDLING NOISES IN DEFECT DATA 
This section proposes a noise  detection  algorithm and presents its 
evaluation.  
6.1 Identifying Noisy Instances 
We investigate possible method s for identifying  noisy instances in 
defect datasets. If we can detect no ises in advance, it is possible to 
eliminate them and make the data more suitable for predictors.  
We propose a  novel noise  detection  algorithm , called Closest List 
Noise Identification  (CLNI) . The pseudo -code of the algorithm is 
given in Figure 9.  
The CLNI algorithm works as follows. In each iteratio n j, for each 
instance Inst i, its closest instances are listed; we c all it Listi. In 
Listi, the instances are sorted in ascending  order  according to their 
Euclidean Distance to Insti. The percentage  of top N instances that 
have  different class value s from Insti is recorded as Œ∏. If Œ∏ is more 
487than or equal to a given threshold Œ¥, then Insti  is highly probable  
to be a noisy instance  and will be  include d in noise set Aj. The 
above process is repeat ed until the similarity between Aj and Aj-1 is 
over Œµ. Aj will be returned as the identified noise set. Empirical 
study found that when  N is 5, Œ¥ is 0.6 and Œµ is 0.99 , this algorithm 
perform s the best . 
CLNI Algorithm:  
for each iteration j  
for each instance Inst i 
for each instance Inst k 
 if(Inst k ‚àà Aj-1) 
     continue;  
else 
     add EuclideanD istance(Inst i, Inst k) to Listi ; 
 end 
            end 
            calculate  percent age of top N instances in Listi 
 whose  label is different from Inst i as Œ∏; 
            if  Œ∏ ‚â• Œ¥ 
  A j = A j ‚à™ Inst i; 
            end  
        end 
        if |Aj‚à©Aj-1| / Max(| Aj|, |Aj-1|)‚â• Œµ  
            break;  
        end 
end 
return  Aj 
Figure 9. The pseudo -code of the CLNI algorithm  
The high-level idea of CLNI can be illustrated as in  Figure 10. 
The bl ue points represent  clean instances  and the white  points 
represent  buggy instances. When  checking if  an instance I is 
noisy , CLNI first lists all instances that are close to I (the points 
included in the circle ). CLNI then calculates  the ratio of instances 
in the list that have a  class label different from that of I (the 
numb er of white  points over the total number of points in the 
circle ). If the ratio  reaches  a specific threshold Œ¥, we consider 
instance I to have a  high probability  to be a nois y instance.  
 
Figure 10. An illustration of the CLNI algorithm  
6.2 Evaluation 
We evaluate  CLNI  using data from the Eclipse 3.4 SWT and 
Debug projects as described in Se ction 5. 2. These two datasets are 
considered as the golden sets as most of their bugs are linked bugs. 
Following the method described in Section 4.2, we cre ate the 
noisy datasets for these two projects by s elect ing random n% of 
instances and artificially  changing  their labels (from buggy to 
clean and from clean to buggy) . We then apply the CLNI algorithm to detect noisy instances that we have just injected. We 
use Precision, Recall and F -measures to evaluate the performance 
in identifying the noisy instance s. 
Table 3 shows the results when the noise rate is 20% . The 
Precisions are above 0.6, Recall s are above 0.83 and F -measures 
are above 0.71. These  promising  results confirm that the proposed 
CLNI algorithm is capable of identifying noisy insta nces.  
Table 3.The performance of CLNI in identifying noisy 
instances  
 Precision  Recall  F-measure  
Debug  0.681  0.871  0.764  
SWT  0.624  0.830  0.712  
Figure 11 also shows the performance of CLNI under di fferent 
noise levels for the SWT component. When the noise rate is 
below 25%, F -measures increase with the increase of the noise 
rates. When the noise rate is above 35%, CLNI will have bias 
toward  incorrect instances,  causing F -measures to decrease.  
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 
0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 
Noise Rate Precision 
Recall 
F-measure 
 
Figure 11. Performance of CLNI with different noise rates  
After identifying the noises  in the noisy Eclipse 3.4 SWT and 
Debug datasets  using CLNI, we eliminate  these noises by flipping  
their labels. We then evaluate if the noise -removed training set 
improves prediction accuracy.  
The results for the SWT component before and af ter removing FN 
and FP noises are shown in Table 4. In general, after removing the 
noises, the prediction performance  (F-measure) im proves for all 
learners, especially for those that do not have strong noise 
resistance ability. For example, for the SVM learner, wh en 30% 
FN&FP noises were injected into the SWT dataset the F -measure 
was 0.339. After identifying and removing the noises, the F -
measure jumped to 0.706. These results confirm that th e proposed 
CLNI algorithm can improve defect prediction perform ance for 
noisy datasets.  
Table 4.The defect prediction performance (F-measure) 
after identifying and removing noisy instances (SWT)  
Remove  
Noises ?  Noise 
Rate  Bayes 
Net Na√Øve 
Bayes  SVM  Bagging  
No 15% 0.781  0.305  0.594  0.841  
30% 0.777  0.308  0.339  0.781  
45% 0.249  0.374  0.353  0.350  
Yes 
 15% 0.793  0.429  0.797  0.838  
30% 0.802  0.364  0.706  0.803  
45% 0.762  0.418  0.235  0.505    I   
4887. THREATS TO VALIDITY 
We note some  threats  to the validity of this work.  
‚Ä¢ All datasets used in our experiments are collected f rom open 
source projects.  The types of noises introduced by open 
source developers may be different from those introduced by 
employees  in a well -managed software organization. We 
need to  evaluate  if commercial  projects also exhibit similar 
noise resistance behavior in defect prediction. This remains 
as future work.  
‚Ä¢ The golden set used in this paper may not be perfect . For 
example, there are still a few percentages of bugs that are not 
linked to the CVS logs. Even though some files are annotated 
with bug IDs, they may  not be the files that actually contain  
the bugs. It is also possible  that a few bugs may not even be 
recorded in the bug tracking system. Our results may be 
under threat if the golden sets contain a large number of FPs 
and FNs.  
‚Ä¢ The noisy data simulations used in our experiment may not 
reflect the actual noise patterns in practice.  In our 
experiments, instances to be included as FP/FN training sets 
are randomly selected. It is possible that in practice, 
occurrences of some noises actually follow certain pat terns ; 
for example, files developed by a poorly managed  team are 
more likely to contain noisy defect data.  
8. RELATED WORK  
8.1 The Data Quality Problem 
Real-world data are often noisy, which may affect  interpretations 
and models derived  from the data. The  data quality  problem is  
well recognized in the data mining area. Some studies show that 
errors in a large dataset are common and field error rates are 
typically around 5% or more [29, 12] . Many existing learning 
algorithms have integrated various approaches to  handle noises. 
For example, the well -known Decision  Tree algorithm uses tree -
pruning methods to avoid over fitting  problems introduced by 
noises in training data [20] . Zhu and Wu [30] descried a 
quantitative study of the impact of noisy data on classification  
accuracies using the UCI machine learning datasets. They found 
that although  some machine learning algorithms have been 
designed to accommodate noises, noises in class labels can still 
lower classification accuracies. They also suggest preprocessing 
methods (such as eliminating instances containing class noise ) to 
enhance classification accuracy.  
The data quality problem has also been observed by s ome 
software engineering researchers. For example, Mockus [15] 
noted that in many realistic scenarios the da ta quality is low  (e.g., 
some change data could be  missing ), which could affect the 
outcome of an empirical study.  He proposed to use multiple 
imputation methods to mitigate the effects of missing values. 
Myrtveit  et al. [17] and Strike  et al. [22] also no ticed the problem 
of missing and incomplete data in software effort estimation. In 
this paper, we address the problem of noisy data in software  
defect prediction.  
8.2 The Quality of Software Defect Data 
Research on software defect prediction has received much 
attention in recent years, as the ability to predict defect -proneness 
of a software module is important for software quality 
improvement and project management. Many defect prediction  models have been proposed (e.g., [7, 10, 11, 13, 16,  17, 35 ]). 
However, almost all defect prediction models do not take noise in 
the data into consideration.  
As described in Section 3, many current defect predic tion models 
are built based on data collected by mining software repositories 
(MSR). Bird et al. [4] reported that th e data collected in this 
manner could introduce a large amount of noises. Although they 
have noticed the noisy defect data problem, they did not 
empirically measure the impact of different noise levels on defect 
prediction accuracy  or try to eliminate nois e. The noisy data 
problem does not pertain to data collected by MSR only . It may 
occur in industrial  metric projects as well. For example, 
Khoshgoftaar and Seliya  [8] performed an extensive study on 
NASA MDP datasets . They observed low prediction performan ce 
and suggested that ‚Äúinstead of focusing on searching for another 
classification technique for improving prediction accuracy, the 
quality of the software measurement data should be addressed‚Äù.  
They also proposed a noise elimination technique  based on the  k-
means algorithm [25]. They detected outliers in the d ata and 
treated them as noisy instances. The limitation of their method is 
that mislabel ed instances are often not outliers. In this paper, we 
present one of the first empirical studies of the impact of noisy 
data on defect prediction. We also  propose a novel noise detection 
algorithm , which can identify mislabeled instances with good 
accuracy.  
9. CONCLUSIONS 
Defect  data collected based on specific bug fix keywords or bug  
report links in change logs are c ommonly used to build defect  
prediction models and to evaluate the models. Since leaving 
specific  keywords or bug report links in change logs is optional, 
automatically  collected defect data from change logs inevitably 
includes noise.  Recent studies show that noise in defect data  is not  
negligible, and this noise affects prediction performance  [4]. 
However, the issue of dealing with noisy data has not been 
addressed adequately.  
In this paper, we have introduced a method to measure noise 
resistan ce in softwa re defect prediction (for predicting buggy files 
and buggy changes). By applying the method to two well -known 
defect prediction models, we found that  in general, noises in the 
defect data do not affect defect prediction performance in a 
significant manner.  However, the prediction performance 
decreases significantly when the dataset contains 20% -35% of 
both FP s and FN s. 
We have also proposed a new method called CLNI for i dentifying 
noisy instances in defect data. Our experiment  results show that 
CLNI can effectively  identify noises with reasonable  accuracy. 
The noise -eliminated training sets produced by CLNI can improve  
the defect prediction  performance, especially for the machine 
learners that do not have strong noise resistant ability.  
In future, we will further investigate techniques for im proving 
defect prediction accuracy under noisy environment . We will also 
explore if the results ob tained  in this paper are applicable to 
industrial  project s.  
All data used in our experiments are available at :  
http://cod e.google.com/p/hunkim/wiki/HandlingNoise  
ACKNOWLEDGEMENTS 
This research is supported by the Chinese NSF grant  61073006 
and the 2010 Microsoft SEIF Awards.  
489REFERENCES 
[1] J. Aranda  and G. Venolia , The secret life of bugs: Going 
past the errors and omissions in software repositories. Proc . 
ICSE ‚Äô09, Vancouver , Canada,  May 2009, 298 -308. 
[2] L. Aversano, L. Cerulo, and C. Del Grosso, Learning from 
bug-introducing changes to prevent fault prone code. In 
Ninth International Workshop on Principles of Software 
Evolution  (IWPSE'07 ). Dubrovnik, Croatia, Sep 2007.  
[3] J. Bevan,  E. Whitehead Jr. , S Kim  and M. Godfrey ,  
Facilitating Software Evolution with Kenyon, Proc. 
ESEC/FSE ‚Äô05, Lisbon, Portugal, 2005, 177 -186. 
[4] C. Bird, A. Bachmann, E.Aune, J. Duffy, A.  Bernstein, 
V.Filkov, and P. Devanbu, Fair and balanced?: bias in bug -
fix datasets. Proc . ESEC/FSE '09 , August 2009, 121-130. 
[5] K. Chen , S. R. Schach , L. Yu, J. Offutt  and G. Z. Heller , 
Open -Source Change Logs, Empirical Software 
Engineering, vol. 9, September 2004 , 197 ‚Äì 210. 
[6] S.R. C hidamber and C.F. Kemerer, A Metrics Suite for 
Object -Oriented Design, IEEE Trans. Software Eng. , vol. 
20, 476 -493, 1994.  
[7] A. E. Hassan, Predicting Faults Using the Complexity of  
Code Changes , Proc.  ICSE‚Äô 09, Vancouver , Canada, May 
2009.  
[8] T.M. Khoshgoftaar an d N. Seliya, The Necessity of  
Assuring Quality in Software Measurement Data, Proc.10th 
Int‚Äôl Symp. Software Metrics (METRIC S‚Äô04),119-130, 2004 . 
[9] S. Kim , T. Zimmermann , K. Pan and E. Whitehead Jr. , 
Automatic Identification of Bug -Introducing Changes,  Proc . 
ASE‚Äô06, Tokyo, Japan, September  2006 . 
[10] S. Kim, T. Zimmermann, E. Whitehead Jr., A. Zeller, 
Predicting Faults from Cached History, Proc. ICSE‚Äô0 7, 
Minneapolis, USA, 2007  
[11] S. Kim , E. Whitehead Jr.  and Y. Zhang , Classifying 
Software Changes: Clean or Buggy?, IEEE  Trans . of 
Software Engineering vol. 34, 181 -196, March/April 2008 . 
[12] J. Maletic  and A.  Marcus, Data Cleansing: Beyond Integrity 
Analysis. Proc . the Conference on Information Quality 
(IQ2000) , 2000.  
[13] T. Menzies, J. Greenwald and A. Frank,  Data Mining Static 
Code Attributes to Learn Defect Predictors, IEEE Trans. 
Software Eng ineering , 32(11) , 1-12, 2007 .  
[14] A. Mockus and L. G. Votta, Identifying Reasons for 
Software Changes Using Historic Databases, Proc. 16th 
International Conference on Soft ware Maintenance (ICSM 
2000) , San Jose, CA, USA, 2000, 120 -130. 
[15] A. Mockus. Missing Data in Software Engineering. In 
F.Shull et al. (eds.), Guide to Advanced Empirical Software  
Engineering , 185 -200, 2008.  
[16] R. Moser, W. Pedrycz and G. Succi, A Comparative 
Analysis of the Efficiency of Change Metrics and Static 
Code Attributes for Defect Prediction, Proc. ICSE ‚Äô08, 
Leipzig, Germany , May 2008 . 
[17] I. Myrtveit, E. Stensrud, and U. H. Olsson.  Analyzing  Data 
Sets with Missing Data: An Empirical Evaluation of  
Imputation Methods and Likelihood -Based Methods.  IEEE  Trans . on Software Engineering,  27(11), 999 -1013,  2001.  
[18] N. Nagappan, T. Ball,and  A. Zeller, Mining Metrics to 
Predict Component Failures, Proc.  ICSE‚Äô 06, Shanghai, 
China , May 2006.  
[19] T. Ostrand, E. Weyuker and R. Bel l, Predicting the Location 
and Number of Faults in Large Software Systems, IEEE 
Trans. Software Eng ineering , 31 (4), 340 -355, 2005 . 
[20] J.R. Quinlan, Learning from Noisy Data. Proceedings of the 
Second International  Machine Learning Workshop , 
University of Ill inois at Urbana -Champaign , 1983 . 
[21] S. Shivaji, E. Whitehead Jr., R. Akella  and S. Kim, 
Reducing Features to Improve Bug Prediction. Proc . 
ASE‚Äô09, Nov 2009 . 600 -604. 
[22] K. Strike, K. E. Emam, and N. Madhavji. Software Cost  
Estimation with Incomplete Data. IEEE T rans. on Software 
Engineering , 27(10), 890 -908, 2001.  
[23] J. ≈öliwerski , T. Zimmermann  and A. Zeller , When Do 
Changes Induce Fixes?, in Int'l Workshop on Mining 
Software Repositories (MSR 2005) , Saint Louis, Missouri, 
USA, 2005, 24 -28. 
[24] J.Spacco , D. Hovemeyer  and W. Pugh, Tracking Defect 
Warnings Across Versions, in Int'l Workshop on Mining 
Software Repositories (MSR 2006) , Shanghai, China, 2006.  
[25] W. Tang and T.M. Khoshgoftaar, Noise identification w ith 
the k -means algorithm, Proc. 16th  IEEE Int‚Äôl Conference 
on Tools with Artificial Intelligence (ICTAI'04) , Nov. 2004 .  
[26] WEKA: http://www.cs.waikato.ac.nz/ml/weka/  
[27] I.H. Witten and E. Frank, Data Mining: Practical Machine 
Learning Tools and Techniques with Java Implementation , 
second ed. , Morgan Kaufmann, 2005.  
[28] C. C. Williams and J. K. Hollingsworth, Automatic Mining 
of Source Code Repositories to Improve Bug Finding 
Techniques, IEEE Trans. Software Engineering, vol. 31, pp. 
466-480, 2005.  
[29] X. Wu, Knowledge Acquisition from Databases , Ablex 
Publishin g, 1995.  
[30] X. Zhu and X. Wu, Class Noise vs. Attribute Noise: A 
Quantitative Study  of Their Impacts , Artificial Intelligence 
Review  22: 177 ‚Äì210, Kluwer Academic , 2004 . 
[31] H. Zhang and X. Zhang, Comments on "Data Mining Static 
Code Attributes to Learn Defect Predictors", IEEE Trans . 
on Software Eng ineering , 33(9 ), 635 -636, 2007 . 
[32] H. Zhang, X. Zhang and M . Gu, Predicting Defective 
Software Components from Code Complexity Measures, 
Proc. of 13th IEEE Pacific Rim International Symposium on  
Dependable Computing (PRDC 2007) , Dec 2007, Aus tralia . 
[33] H. Zhang, An Investigation of the Relationships between  
Lines of Code and Defects, Proc. ICSM‚Äô0 9, Edmonton, 
Canada, September 2009.  
[34] T. Zimmermann, R. Premraj and A. Zeller, 2007. Predicting 
Defects for Eclipse, Proc. PROMISE ‚Äô07, Minneapolis, 
USA .  
[35] T. Zimmermann and N. Nagappan, Predicting Defects usin g 
Network Analysis on Dependency Graphs, Proc.  ICSE ‚Äô08, 
Leipzig, Germany , May 2008 . 
 
490