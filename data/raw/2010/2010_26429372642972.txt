Localization of Concurrency Bugs
Using Shared Memor
y Access Pairs
Wenwen Wang1,2, Zhenjiang Wang1, Chenggang Wu1∗, Pen-Chung Y ew3,
Xipeng Shen4, Xiang Yuan1,2, Jianjun Li1, Xiaobing Feng1, Y ong Guan5
1SKL Computer Architecture, ICT, CAS,2University of Chinese Academy of Sciences,3University of
Minnesota at Twin-Cities,4College of William and Mary,5Capital Normal University
1,2{wangwenwen|wangzhenjiang|wucg|yuanxiang|lijianjun|fxb}@ict.ac.cn,
3yew@cs.umn.edu,4xshen@cs.wm.edu,5guanyong@mail.cnu.edu.cn
ABSTRACT
We propose an eﬀective approach to automatically local-
ize buggy shared memory accesses that trigger concurrency
bugs. Compared to existing approaches, our approach has
two advantages. First, as long as enough successful runs of
a concurrent program are collected, our approach can local-
ize buggy shared memory accesses even with only one single
failed run captured, as opposed to the requirement of cap-
turing multiple failed runs in existing approaches. This is a
signiﬁcant advantage because it is more diﬃcult to capture
the elusive failed runs than the successful runs in practice.
Second, our approach exhibits more precise bug localization
results because it also captures buggy shared memory ac-
cesses in those failed runs that terminate prematurely, which
are often neglected in existing approaches. Based on this
proposed approach, we also implement a prototype, named
Locon. Evaluation results on 16 common concurrency bugs
show that all buggy shared memory accesses that trigger
these bugs can be precisely localized by Locon with only
one failed run captured.
Categories and Subject Descriptors
D.2.5 [Software Engineering ]: Testing and Debugging
Keywords
ConcurrencyBug; Localization; SharedMemoryAccessPair
1. INTRODUCTION
It is very diﬃcult to debug concurrent programs due to
their congenital non-determinism [36, 30, 23]. First, con-
currency bugs are triggered only under particular thread in-
terleavings. Second, even after a concurrency bug has been
exposed in a failed program run, it still takes a tremendous
amount of time and eﬀort to localize and ﬁx the bug [25].
Previous work [19] shows that it takes nearly 73 days on
∗To whomcorrespondence should be addressed.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from Permissions@acm.org.
ASE’14, September 15-19, 2014, Vasteras, Sweden.
Copyright 2014 ACM 978-1-4503-3013-8/14/09 ...$15.00.
http://dx.doi.org/10.1145/2642937.2642972.average to ﬁx a concurrency bug, and a survey in Microsoft
[9] shows that over half of the respondents suﬀer from con-
currency bugs at least once a month.
Concurrency bugs exhibit most frequently as order viola-
tions(OV) and atomicity violations (AV) (including single-
variable (SAV) and multi-variable (MAV)), accounting for
97% of common concurrency bugs [19]. Some of them are
caused by data races in programs, while others occur even
in data-race-free programs. OV is a violation of the desired
happen-before order [19] between two accesses to the same
shared variable. AV is a violation of a code region’s atomic-
ity(alsoknownas serializability )[19, 28], whichoccurswhen
the code region is unintentionally interleaved with memory
accesses from another thread.
A large amount of work [7, 32, 33, 3, 27, 20, 28] has
been proposed to detect concurrency bugs, e.g. data races
and AV bugs, by leveraging static and/or dynamic program
analysis techniques. Unfortunately, due to the large amount
of false positives, programmers still have to sift through a
large volume of results to localize the bugs. Prior work [26]
ﬁndsthat90%ofthedetecteddataracesareactuallybenign.
Meanwhile, someotherwork[23, 37, 6]triestoexposecon-
currencybugsbytriggeringthemviasystematicorrandomly-
orchestrated thread interleavings. But, even after strenuous
testing, unexposed bugs could still be hidden in programs,
and their manifestation could be catastrophic [15, 8, 31].
During testing concurrent programs, developers have to
manually localize a concurrency bug after it incidentally
shows up in a failed run. However, it is very time and eﬀort
consuming to pinpoint the exact cause or source of a con-
currency bug [19, 30, 12], i.e. identifying the exact memory
accesses that cause the bug, due to their large volume.
To relieve the debugging burden from programmers, some
approaches [30, 29, 12, 5] try to analyze the logged data and
automatically localize the exact causes of the bugs. The
aim of these approaches is not to trigger or detect the bugs,
but tolocalize the causes of the bugs after they show up in
failed runs, a so-called fault localization problem. They col-
lect program predicates at runtime in multiple runs, which
may complete successfully or fail. Such predicates can be
branch outcomes [12], bug patterns [30, 29], or cache coher-
ence events [5], which exhibit diﬀerent behavior in successful
and failed runs. Statistical models such as the Jaccard in-
dex[30, 29], the harmonic mean of the occurrence frequency,
or well-deﬁned scores [12, 5] are then used to calculate the
scores for those predicates. Predicates with higher values
are more likely to be the true causes.
611
The score of each predicate is calculated based on the
relativ
e numbers of the failed and successful runs in which
this predicate shows up. To allow the true causes to have
distinctly higher scores, a suﬃcient number of successful and
failed runs are required. However, it is not easy to capture
a large number of failed runs, e.g. it took about 22 hours
(tens of thousands of successful runs) for a real-world SAV
bug in the Apache Server to manifest in a failed run [28].
More likely, a programmer will capture one failed run after
many more successful runs in reality.
In this paper, we propose a more practical approach to
address the above limitation. Our approach, through a sys-
tematic examination of the common buggy shared memory
accesses, identiﬁes a list of shared memory access pairs that
behave distinctively in failed and successful runs. Three sce-
narios that summarize the manifestation diﬀerences of the
buggy memory access pairs triggering OV or AV (also SAV
or MAV) bugs in successful and failed runs are thus identi-
ﬁed. This inspires us to develop three test procedures, each
of which checks the distinctive memory access pairs in one
of the three scenarios. Experiments on 16 common concur-
rency bugs show that our approach can localize these bugs
even with only one failed run captured and has signiﬁcantly
fewer false positives than the state of art methods.
This paper makes the following contributions:
◮First, we identify an exhaustive list of scenarios that
indicate the manifestation diﬀerences of the buggy access
pairs triggering OV or AV bugs in successful and failed runs.
To the best of our knowledge, this is the ﬁrst attempt to
compile all possible scenarios that cover OV and AV bugs.
◮Second, we propose 3 comprehensive test procedures to
automatically localize buggy memory access pairs by match-
ing them to all identiﬁed scenarios. The test procedures are
designed elaborately to eliminate potential false positives.
◮Third, we implement a prototype, named Locon, based
on the proposed test procedures, and evaluate it on 16 com-
mon concurrency bugs. The results show that Locon is ca-
pable of localizing these bugs precisely with only one failed
runcaptured, i.e. itisnotrequiredtocapturemultiplefailed
runs to pinpoint a bug as in existing approaches.
The rest of this paper is organized as follows. Section
2 describes the main idea of this paper. Section 3 and 4
respectively illustrate scenarios and test procedures. Section
5 and 6 present and evaluate Locon. Section 7 discusses
related work. Section 8 concludes the paper.
2. OVERVIEW
We ﬁrstly deﬁne some notations and then leverage an ex-
ample to motivate the main idea of our approach.
2.1 Notations
A thread is denoted as Ti, where i= 1, 2, ..., indicates
a unique thread id. Ma
xrepresents an access to a shared
variable x, where Mindicates the type of the access, either
R(Read) orW(Write),ais the instruction or statement in
the source text that issues the access.
Ashared memory access pair, or an access pair for short,
represents an immediate dependency (R -W,W-R, orW-W)
between two memory accesses in two threads: Ma
x→Nb
x,
wherexis the shared variable accessed and at leastone ofM
andNis a write. The access pair indicates that Ma
xhappensT1
//session.c
tr_handle*tr_sessionInitFull(...){
...
h=tr_new0(tr_handle,1);
h->peerMgr=tr_peerMgrNew(h);
h->bandwidth=
tr_bandwidthNew(h,NULL);
...
}T2
//bandwidth.c
void allocateBandwidth(...){
...
assert(h->bandwidth);
...
}
correctorder buggyorder
Figure 1: An O V bug in Transmission-1.42.
Program
ExecutionsManualBug
LocalizationProgramBinary
ProgramInputFailedSuccessful
Bug
FixBug
Cause
AccessPair
CollectionTestProcedures
based
onScenariosManualTesting
and
LocalizationProcess
Automatic
Localization
basedonOurApproachAccess
Pairs
Figure 2: Automat ic bug localization based on our approach.
beforeNb
x.Ma
xrefers to the head, and Nb
xrefers to the tail,
of the access pair. Besides, Ma
x→Nb
xandNb
x→Ma
xare
respectively called the reversed access pair of each other.
Asuccessful run is a program execution that produces
the expected output of the programmer. Otherwise, the
executionisconsideredasa failedrun. Inthispaper, weonly
handle with failed runs caused by OV and AV bugs. Also,
failed and successful runs have the sameprogram input.
Typically, the manifestation of concurrency bugs shows
up in the form of wrong values being read from the vio-
lated shared variables. These readsare referred to as ETPs
(ErrorTriggering Points) of the concurrency bugs in this
paper. After the occurrence of an ETP, the program may
continue to run but produce incorrect results, or may be
terminated prematurely due to faults. Therefore, the tail of
the buggy access pair may not show up after the ETP. Such
buggy access pair is often not included in the ﬁnal report of
most existing approaches because its tail is not actually ex-
ecuted. Our approach pays special attention to these buggy
access pairs and results in more precise localization results.
2.2 A Motivating Example
We use the example illustrated in Figure 1 to describe the
main idea of our approach. This is a real-world OV bug
from Transmission-1.42 [34], which is a multi-threaded Bit-
Torrent download client. In this example, h->bandwidth is a
shared variable initialized by the statement h->bandwidth =
trbandwidthNew(h, NULL) insession.c , i.e.Wh->bandwidth
(the statement is omitted for simplicity). The statement
assert(h->bandwidth) inbandwidth.c readsh->bandwidth, i.e.
Rh->bandwidth , which ought to occur after Wh->bandwidth .
Unfortunately, due to the lack of proper synchronization,
Rh->bandwidth is likely to be executed before Wh->bandwidth
andreadsanuninitializedvalue, causinganassertionfailure.
Thus,Rh->bandwidth is theETPof this bug.
Thebugisnotoftentriggeredevenwiththebug-triggering
input. Actually, we cannot come across this bug until tens of
thousands of successful runs have been tested. After studied
the program, we found that the function allocateBandwidth
is called from an event callback function that is often called
about 500 milliseconds after h->peerMgr is created. This is
why the bug occurs rarely in practice. But, if the execution
environment is changed, e.g. the underlying schedule policy,
the bug may manifest and cause the program to crash.
61212 3 4 5 6 78
IDIllegalFailed RunsScenario Successful RunsDescriptionInterleavings Type Run-1 Run-2
OV1a
xWb
xRWa
x→Rb
x IRb
x→Wa
x Unexpected value is read due to the premature write.
2a
xRb
xWRa
x→Wb
x IWb
x→Ra
x Unexpected value is read due to the hysteretic write.
∅ II Buggy access pair is missing due to the ETP.
3a
xWb
xWWa
x→Wb
x IWb
x→Wa
x The ﬁrst write is covered by the second.
SAV4a
xWb
xWc
xRWa
x→Wb
x,Wb
x→Rc
xIRc
x→Wb
xWb
x→Wa
xThe ﬁrst write is covered by the second.
5a
xRb
xWc
xWRa
x→Wb
x,Wb
x→Wc
xIWc
x→Wb
xWb
x→Ra
xThe ﬁrst write is covered by the second.
6a
xRb
xWc
xRRa
x→Wb
x,Wb
x→Rc
xIRc
x→Wb
xWb
x→Ra
xInconsistent values are read due to improper write.
7a
xWb
xRc
xWWa
x→Rb
x,Rb
x→Wc
xIWc
x→Rb
xRb
x→Wa
xUnexpected intermediate value is read between the writes.
Wa
x→Rb
x I Unexpected intermediate value is read after the ﬁrst write.
MAV8a
xRb
xW
d
yWc
yRRa
x→Wb
x,Rc
y→Wd
yIIIRa
x→Wb
xWb
x→Ra
xInconsistent results of multiple variables due to improper accesses.
Wb
x→Ra
x,Wd
y→Rc
yIIIWd
y→Rc
yRc
y→Wd
yInconsistent results of multiple variables due to improper accesses.
Ra
x→Wb
x II Buggy access pairs are partially missing due to the ETP.
9a
xRb
yR
d
yWc
xWRa
x→Wc
x,Rb
y→Wd
yIIIRa
x→Wc
xRb
y→Wd
yInconsistent results of multiple variables due to improper accesses.
Wd
y→Rb
yWc
x→Ra
x
10a
xWb
xR
d
yWc
yRWa
x→Rb
x,Rc
y→Wd
yIIIWa
x→Rb
xRb
x→Wa
xInconsistent values of multiple variables are read.
Rb
x→Wa
x,Wd
y→Rc
yIIIWd
y→Rc
yRc
y→Wd
yInconsistent values of multiple variables are read.
Wa
x→Rb
x II Buggy access pairs are partially missing due to the ETP.
11a
xWb
xW
d
yWc
yRWa
x→Wb
x,Rc
y→Wd
yIIIWa
x→Wb
xWb
x→Wa
xInconsistent results of multiple variables due to improper accesses.
Wb
x→Wa
x,Wd
y→Rc
yIIIWd
y→Rc
yRc
y→Wd
yInconsistent results of multiple variables due to improper writes.
Wa
x→Wb
x II Buggy access pairs are partially missing due to the ETP.
12a
xWb
xW
d
yWc
yWWa
x→Wb
x,Wc
y→Wd
yIIIWa
x→Wb
xWb
x→Wa
xInconsistent results of multiple variables due to improper writes.
Wb
x→Wa
x,Wd
y→Wc
yIIIWd
y→Wc
yWc
y→Wd
yInconsistent results of multiple variables due to improper writes.
13a
xWb
yR
d
yWc
xRWa
x→Rc
x,Rb
y→Wd
yIIIWa
x→Rc
xRb
y→Wd
yInconsistent results of multiple variables due to improper reads.
Wa
x→Rc
x IIWd
y→Rb
yRc
x→Wa
xBuggy access pairs are partially missing due to the ETP.
14a
xWb
yR
d
yWc
xWWa
x→Wc
x,Rb
y→Wd
yIIIWa
x→Wc
xRb
y→Wd
yInconsistent results of multiple variables due to improper accesses.
Wd
y→Rb
yWc
x→Wa
x
15a
xWb
yW
d
yWc
xWWa
x→Wc
x,Wb
y→Wd
yIIIWa
x→Wc
xWb
y→Wd
yInconsistent results of multiple variables due to improper writes.
Wd
y→Wb
yWc
x→Wa
x
Table 1: Diﬀeren t scenarios for OV and AV bugs. ” R”: possible ETPtriggered by a read, ” ∅”: there is no access pair in the failed run.
Figure 2 shows a common process of testing and debug-
ging a concurrent program, which iteratively executes the
program binary with a given input. If the program run is
successful, the testing process iterates again. Otherwise, if
failed, programmers have to manually ﬁnd the bug, which
is often time consuming. Our approach collects access pairs
in program runs, and when a failed run is encountered, test
procedures based on the manifestation scenarios analyze the
collected access pairs and report buggy access pairs that
trigger the bug. For the bug in Figure 1, if the assertion
happens to fail after some successful runs, our approach will
reportRh->bandwidth →Wh->bandwidth as the buggy access
pair after applying Test Procedure II (see Section 4.2).
Note, the program crashes after the execution of the ETP
of the bug, i.e. Rh->bandwidth , andWh->bandwidth is not ac-
tually executed in failed runs. Most existing pattern-based
approaches fail to localize this bug, since they only consider
access patterns gathered in failed runs [21, 30, 29]. For in-
stance, Falcon [30] only ranks access patterns collected in
failed runs and therefore may fail to localize this bug. In-
stead, our approach can successfully report the buggy access
pair as the bug cause. Our advantage comes from the ex-
haustive list of manifestation scenarios presented in Section
3, which is the basis of our approach.3. MANIFESTATION SCENARIOS
We present the scenarios based on two hypotheses [37].
First, most of concurrency bugs can be triggered with a
small number of thread preemptions, i.e. the well-known
small scope hypothesis [10, 23]. We leverage this observa-
tion to bound the number of access pairs in scenarios to
two. Second, a majority of concurrency bugs can be trig-
gered when the buggy access pairs show up in program runs,
regardless of the data value of the shared variables involved
in the access pairs, i.e. the value-independence hypothesis
[37]. Besides, we also assume that properly synchronized
concurrent programs adhere to the data-race-free 0 model
[2]. With this model, the underlying hardware appears to
besequential consistent even though it may be implemented
using a weaker consistency model. Based on these assump-
tions, Table 1 demonstrates a exhaustive list of scenarios for
OV and AV bugs based on two threads T1andT2(omitted
due to space limitation) and at most two shared variables x
andy. We explain each column as follows.
The column 1 shows bug types. The column 2 gives each
type of bug an ID number. For OV bugs, there are only
three possible violations of shared memory access orders:
R-W,W-R, andW-W. Thus, there are three types of OV
bugs (ID-1 ∼ID-3). For SAV bugs, each one involves three
613shared memory accesses [20, 30]. The combinatorial number
ofRandWforthe
threeaccessesis23, butonlyfourofthem
are unserializable, i.e. AV bugs. Thus there are four types
of SAV bugs (ID-4 ∼ID-7). Similarly, we can conclude that
there are eight types of MAV bugs (ID-8 ∼ID-15).
The column 3 lists one possible illegal interleaving for each
type of bug, where the accesses in the left and right sides
are respectively executed by T1andT2, the accesses in the
same thread are executed from top to bottom, and the ar-
row between two accesses denotes their happen-before order,
i.e. the source happens before the target. The bug is trig-
gered under the illegal interleaving, leading to a failed run
in the column 4. For example, ID-1 is an OV bug, where
the intended happen-before order between the two accesses
isRb
xhappens before Wa
x. However, the illegal interleaving
Wa
x-Rb
xviolates this order and causes the failed run in the
column 4, where Wa
x→Rb
xshows up. Some types of bugs
may have more than one possible illegal interleaving. For
example, ID-10 is an MAV bug, where shared variables x
andyshould be written (Wa
xandWd
y) and read (Rb
xand
Rc
y) atomically. There are two possible illegal interleavings,
i.e.Wa
x-Rb
x-Rc
y-Wd
yandRb
x-Wa
x-Wd
y-Rc
y, which respectively
violate the atomicity between (Wa
xandWd
y) and (Rb
xand
Rc
y). We only show the ﬁrst one in the column 3 due to space
limitation, but list all possible buggy access pairs that show
up under each illegal interleaving in the column 4, i.e. the
ﬁrst two cases in the column 4. The third case Wa
x→Rb
xis
listedwithadescriptionofthecauseinthecolumn8, i.e. one
buggy access pair is missing due to the ETPcaused by Rc
y.
Note,Wa
x-Rb
x-Wd
y-Rc
yandRb
x-Wa
x-Rc
y-Wd
yare not illegal in-
terleavings for ID-10, because the execution results of these
two interleavings are respectively equivalent to Wa
x-Wd
y-Rb
x-
Rc
yandRb
x-Rc
y-Wa
x-Wd
y, which means they are serializable.
The illegal interleavings listed in the column 3 represents
all possible anomalous interleavings triggering OV and AV
(includingSAVandMAV)bugs. Theseinterleavingpatterns
are more comprehensive and general than the problematic
access patterns used in prior work for speciﬁc classes of con-
currency bugs [20, 30, 29, 28, 35].
The columns 6 and 7 illustrate access pairs manifested in
possible successful runs. For each OV bug, there is only one
possible case in successful run, where the desired happen-
before order of the involved accesses is followed. For each
SAV or MAV bug, there are two possible cases, and each of
them is a serialized execution of the involved atomic regions
and the atomicity semantics are satisﬁed.
As shown in the column 5, there are three possible man-
ifestation scenarios, denoted as Scenario I, II, and III. The
intuitionbehindthesescenariosistosummarizethemanifes-
tation diﬀerences of the buggy access pairs in the failed and
successful runs. These diﬀerences serve as the basis for our
test procedures for concurrency bug localization. Actually,
developers also pay more attention to abnormal program be-
haviors in failed runs compared with successful runs when
debugging a program. We discuss each scenario as follows.
3.1 Scenario I
In this scenario, buggy access pairs only occur in failed
runs, but notin successful runs. Take ID-4, an SAV bug,
as an example. In the two successful runs (the columns
6 and 7), the atomicity between Wa
xandRc
xis enforced,
and two access pairs respectively show up in each successful
run:Rc
x→Wb
xandWb
x→Wa
x. But, in the failed run(the column 4), the atomicity is violated under the illegal
interleaving: Wa
x-Wb
x-Rc
x. Comparing with the former two
access pairs, we can easily see that buggy access pairs, i.e.
Wa
x→Wb
xandWb
x→Rc
x, only occur in the failed run.
3.2 Scenario II
Buggyaccesspairsmanifestin neithersuccessful norfailed
runs in this scenario. Take ID-2, an OV bug, as an example.
The intended happen-before order between Wb
xandRa
xis
enforced by Wb
x→Ra
xin the successful run, shown in the
column 6. Because Ra
xis anETPthat may terminate pro-
gram execution due to the fault triggered by this violation,
there are two possible cases in the failed runs. One is to have
the buggy access pair Ra
x→Wb
x, which does not present in
the successful runs, but another does not form any access
pair because Wb
xis not executed after Ra
xtriggers an ETP.
We use an ∅to represent this case in the column 4.
AnotherexampleofthisscenarioisID-10, anMAVbug, in
whichRc
yis a possible ETP. There are three possible cases
in failed runs. The ﬁrst two will be discussed in Scenario
III. In the third one, there exists only one buggy access
pairWa
x→Rb
x, and another buggy access pair Rc
y→Wd
yis
missing because Wd
yis not executed after the ETPtriggered
byRc
y. In fact, this bug can only be triggered when the two
buggy access pairs show up together in the same run (i.e.
it is an MAV bug involving two shared variables, xandy).
We call such two buggy access pairs coupled buggy access
pairs. If one of them is missing in a failed run, we call them
partially-missing coupled buggy access pairs.
Note, in the second case of the failed runs in ID-7, the
bug can be triggered by Wa
x→Rb
xalone without Rb
x→Wc
x
as in the ﬁrst case. However, in the third case of the failed
runs in ID-10, the bug cannot be triggered by Wa
x→Rb
x
alone. It needs an illegal interleaving with y. This is why
the former is in Scenario I while the latter in Scenario II.
3.3 Scenario III
Buggy access pairs show up in bothfailed and successful
runs in this scenario. Take ID-10 again as an example. As
mentioned above, there are three possible cases in its failed
runs. In the ﬁrst one (the second one is similar), there is a
coupled buggy access pairs: Wa
x→Rb
xandRc
y→Wd
y, each
of which also shows up in successful runs (the columns 6 and
7). But, they do not show up together in any successful run.
Although Table 1 only lists MAV bugs triggered by only
two access pairs, other unlisted MAV bugs can also fall into
this scenario. Suppose an MAV bug that can be triggered by
”at least” naccess pairs, where n≥2:h1→t1,h2→t2, ...,
hn→tn, which may involve more than two shared variables.
If the happen-before order between hiandti, where 1 ≤
i≤n, is reversed in a program run, the bug will not be
triggered, and the following access pairs will be collected:
h1→t1,h2→t2, ...,ti→hi, ...,hn→tn. Therefore, there
exist at least nsuccessful runs, each of which only reverses
one ofnaccess pairs. Thus, the scenario of the nbuggy
access pairs satisﬁes the conditions of Scenario III: each one
of them shows up in both failed and successful runs, but
they never show up together in any successful run.
Aspreviouslydiscussedinthesmallscopehypothesis, con-
currency bugs triggered by more than two access pairs are
very rare in real practice [19, 37]. Hence, even if it is very
simple, we will not extend our test procedures to localize
such bugs, because it will increase the localization over-
614head signiﬁcantly without much beneﬁt. To the best of our
know
ledge, none of the existing concurrency bug localiza-
tion methods considered such bugs due to their complicated
trigger conditions and rarity in real practice.
4. TEST PROCEDURES
AssumePis a concurrent program and Iis a concurrency-
bug-triggering input of P.PRunandFRunrespectively
denote a set of access pairs that can be collected in a success-
ful and failed run of PunderI.PSetis the set of PRuns.
GivenPSetand anFRun, the fault localization problem
is to identify buggy access pairs triggering the concurrency
bug inFRun. In practice, it is much easier to come across
PRunthanFRundue to the special trigger conditions of
concurrency bugs. Moreover, after a few number of PRuns
encountered, almost all of possible access pairs in PunderI
can be collected. For simplicity of explanation, the following
description ﬁrst assumes that we have collected all possible
access pairs that can appear in PRunsin thePSet. In Sec-
tion 4.4, we will show that the assumption is not required
in practice. Besides, in Section 6.3, we will also study the
sensitivity of localization results on the number of PRuns.
To solve the localization problem, we design 3 test proce-
dures to localize buggy access pairs manifested in scenarios
discussed in Section 3. We describe them as follows.
4.1 Test Procedure I
The ﬁrst test procedure aims to uncover buggy access
pairs manifested with Scenario I, where buggy access pairs
show up in FRun, but not in PRuns. Algorithm 1 shows
the details of this test procedure. In Step1(lines 1 ∼6),
we collect access pairs that show up in FRun, but not in
PRuns, denoted as apSet. However, access pairs in apSet
are not all buggy. This is one major cause of false posi-
tives in other existing approaches. We identify those false
positives in the following steps and ﬁlter them out.
One type of false positives is caused by the predictable
access pairs. An access pair is predictable by another, if
and only if the presence of the former can be inferred by
the presence of the latter, but not vice versa. This means
if the latter presents in a program run, the former will be
also found in this run. Figure 3 shows an OV bug, which is
triggered by W3
y→R2
y(i.e.T1dereferences a null pointer).
In this example, W1
x→R4
xis predictable by R2
y→W3
y,
andW3
y→R2
yis predictable by R4
x→W1
x. If we process
FRun2 using Algorithm 1 (Note: we can process FRun1
andFRun2 in Figure 3 independently as mentioned in Sec-
tion 1), we can obtain W3
y→R2
yandR4
x→W1
xinapSet
afterStep1, whereR4
x→W1
xis a false positive because the
bug is not triggered by it, but by W3
y→R2
y.
We ﬁlter out this type of false positives in Step2of Algo-
rithm 1 (lines 7 ∼10) by identifying predictable access pairs
inapSet. Suppose two access pairs AP1andAP2inapSet,
whereAP1is predictable by AP2. ThenAP2is identiﬁed as
a false positive. The reason is as follows. Assume AP2is
not a false positive but the buggy access pair that triggers
the bug. Then its reversed access pair, denoted as RAP2,
will not be buggy according to OV and SAV bugs shown in
Table 1. Thus, there exists a PRunthat contains both AP1
andRAP2. This violates the precondition that AP1is in
apSet, i.e. AP1does not show up in any PRun.
Another type of false positives is caused by the execution
of a rarely executed code region after an ETPin the failedAlgorithm 1: Test Procedure I
Input: failed run: FR un; successful run set: PSet
Output: buggy access pairs manifested with Scenario I
// Step1: get access pairs that show up only in FRun
1apSet←FRun;
2foraccess pair AP∈apSetdo
3forsuc cessful run PRun∈PSetdo
4 ifAP∈PR unthenapSet←apSet−{AP}; break;
5end
6end
// Step2: filter out predictable access pairs
7fortwo access pairs APi,APj∈apSetdo
8ifAPiis pr edictable by APjthenapSet←apSet−{AP j};
9else if APjis predictable by APithen
apSet←apSet−{AP i};
10end
// Step3: sort access pairs according to occurrence order
11apList←SortAccessPairs(apSet);
12return apList;
T1
1:x=local1;
2:*y=local2;
T2
3:y=NULL;
4:local3=x;intx;
int*y=malloc();T1T2
PRun1
xW
4
xR3
yW2
yRT1T2
FRun11
xW
4
xR3
yW
2
yRT1T2
FRun21
xW4
xR3
yW
2
yR
Figure 3: False positive introduced by predictable access pairs.
run, e.g. an embedded debugging routine in the program.
In this case, all access pairs in this code region will appear
only inFRunbut not in PRuns. They are certainly not
the causes of the concurrency bug. To eliminate such false
positives, access pairs in apSetare sorted according to their
occurrence order in Step3of Algorithm 1 (line 11) as such
false positives always occur afterbuggy access pairs. The
reason is that the formation of these false positive access
pairs results from the triggered concurrency bugs.
4.2 Test Procedure II
Test Procedure II is to localize buggy access pairs man-
ifested with Scenario II, where buggy access pairs show up
in neither PRunsnorFRunbecause they are missing or
partially-missing in FRundue toETPs. Thus, they can-
not be directly localized as those manifested with Scenario I.
We localize these buggy access pairs by observing the mani-
festation of their reversed access pairs in PRunsandFRun.
For missing buggy access pairs, their reversed access pairs
will always show up in PRuns, but not in FRun. For ex-
ample, in the second case of failed runs of ID-2 in Table
1,Ra
x→Wb
xis missing in both successful and failed runs.
But its reversed access pair, i.e. Wb
x→Ra
x, always show up
in successful runs due to the intended happen-before order
between Wb
xandRa
x.
For partially-missing coupled buggy access pairs, the re-
versed access pairs of the missing buggy access pairs show
up inPRunstogether with their counterparts (which are
also buggy access pairs) in the coupled buggy access pairs.
Take the third case of failed runs of ID-10 in Table 1 as an
example. Wa
x→Rb
xandRc
y→Wd
yare partially-missing
coupled buggy access pairs, where Rc
y→Wd
yis missing.
The reversed access pair of Rc
y→Wd
y, i.e.Wd
y→Rc
y, which
is also missing in the failed run, always shows up together
with its counterpart, i.e. Wa
x→Rb
x, in successful runs due
to the required atomicity of accessing to both xandy.
We leverage above observations on reversed access pairs
to localize buggy access pairs manifested with Scenario II.
615Algorithm 2: Test Procedure II
Input: failed run: FR un; successful run set: PSet
Output: buggy access pairs manifested with Scenario II
// Step1.1: get access pairs that show up in all PRuns
1apSet←∅;capSet←∅;
2forsuccessful run PRun∈PSetdo
3ifap Set=∅thenapSet←PRun;
4else
5 foracc ess pair AP∈apSetdo
6 ifAP /∈P RunthenapSet←apSet−{AP};
7 end
8end
9end
// Step1.2: get coupled access pairs that always show up
together in PRuns
10forsuccessful run PRun m∈PSetdo
11 forAPi,APj∈P RunmandAPi,APj/∈apSetand
APi,APjaccess diﬀerent shared variables do
12 fla g←true;
13 forsuccessful run PRun n∈PSetdo
14 if(APi∈P RunnandAPj/∈PRun n)or
(APi/∈PRun nandAPj∈PRun n)thenflag←
false; break;
15 end
16 ifflag= true thencapSet←capSet+{(APi,APj)};
17 end
18end
// Step2.1: check access pairs from Step1.1 in FRun
19rapSet←∅;
20foraccess pair AP∈apSetdo
21 ifAP /∈F Runthen
rapSet←rapSet +{getReversedAP(AP )};
22end
// Step2.2: check coupled access pairs from Step1.2 in
FRun
23forcoupled access pairs (APi,APj)∈capSet do
24 ifAPi∈FR unandAPj/∈FRunthen
25 rap Set←rapSet +{getReversedAP(AP j)}
26 else if APi/∈FRunandAPj∈FRunthen
27 rap Set←rapSet +{getReversedAP(AP i)}
28 end
29end
// Step3: sort access pairs according to occurrence order
30rapList←SortOriginalAccessPairs(rapSet );
31return rapList;
Algorithm 2 shows the details. In Ste p1.1(lines 1 ∼9),
we collect access pairs that show up in all PRuns. In
Step2.1 (lines 19 ∼22), we check each access pair collected
inStep1.1 whether it show up in FRun. If not, its reversed
access pair (returned by getReversedAP ()) is considered as
one of the missing buggy access pairs. Similarly, Step1.2
(lines 10 ∼18) and Step2.2 (lines 23 ∼29) can localize
partially-missing coupled buggy access pairs in FRun.
However, non-buggy access pairs may also be missing in
FRundue toETPs. For example, code regions always exe-
cutedinsuccessfulrunsmaynotbeexecutedinthefailedrun
after an ETPoccurs, so reversed access pairs of the access
pairs in these code regions can also be identiﬁed as buggy
after the above steps. To eliminate such false positives, we
also sort the original access pairs of those obtained by above
two steps according to their occurrence order (Step3 in line
30), as the similar reason to that in Test Procedure I.
Now, apply this test procedure to the example in Fig-
ure 1. After Step1.1, we can ﬁnd that Wh->bandwidth →
Rh->bandwidth shows up in all PRuns, which means the read
access to h->bandwidth should happen after the initializa-
tion. In Step2.1, we can further ﬁnd that this access pair
do not show up in FRun. Thus, its reversed access pair,
Rh->bandwidth →Wh->bandwidth , is reported as the buggy
access pair after sorting in step3.Algorithm 3: Test Procedure III
Input: failed run: FR un; successful run set: PSet
Output: buggy access pairs manifested with Scenario III
// Step1: get access pairs that show up in PRuns and
FRun
1apSet←∅;
2foraccess pair AP∈FRundo
3forsuc cessful run PRun∈PSetdo
4 ifAP∈PR unthenapSet←apSet+{AP}; break;
5end
6end
// Step2: check for coupled buggy access pairs
7capSet←∅;
8forAPi,APj∈apSetandAPi,APjaccess diﬀerent memory
locations andAPi,APjare from two threads do
9flag←true;
10 forsu
ccessful run PRun∈PSetdo
11 if(p candCheckPc(AP i,APj,PRun)) or(tidand
CheckTid(AP i,APj,PRun)) or(loopand
CheckLoop(AP i,APj,PRun)) thenflag←false; break;
12 end
13 ifflag= true thencapSet←capSet+{(APi,APj)};
14end
// Step3: filter out predictable access pairs
15fortwo coupled access pairs (APi,APj),(APm,APn)∈capSet
do
16 ifAPiorAPjis pr edictable by APmorAPnthen
17 cap Set←capSet−{(APm,APn)};
18 else if APmorAPnis predictable by APiorAPjthen
19 cap Set←capSet−{(APi,APj)};
20 end
21end
22return capSet;
4.3 Test Procedure III
This t
est procedure aims to localize buggy access pairs
manifested with Scenario III. Algorithm 3 shows the details
of this test procedure. In Step1(lines 1 ∼6), we collect
access pairs that show up in both PRunsandFRun, de-
noted as apSet. In Step2(lines 7∼14), for any two access
pairs inapSetthat access diﬀerent shared variables and are
from two threads, we check whether they show up together
inPRunsor not. If they do not show up together in any
PRun, they are considered as coupled buggy access pairs,
denoted as capSet. As in Test Procedure I, false positives
caused by predictable access pairs are ﬁltered out in Step3
(lines 15 ∼21).
To ﬁnd out whether coupled access pairs show up together
in anPRun, we need to check whether the heads and the
tails of the two access pairs also form the same coupled ac-
cess pairs in this PRun. The problem is how to represent
theheadsandthetails, whicharedynamicmemoryaccesses.
Generally, they can be represented by the addresses of the
instructions issuing them [30]. This is usually enough for
Test Procedures I and II as they mainly aim to localize OV
and SAV bugs, which only involve a single shared variable.
However, it is insuﬃcient for Test Procedure III.
Figure 4 explains the reason. This is an MAV bug, where
T1writes shared variables xandyin a loop. In successful
runs (PRun 1∼PRun3),T2can read consistent values of
xandyas the atomicity is satisﬁed. If Test Procedure III
processes FRunonly using instruction addresses to identify
dynamic memory accesses, no buggy access pair can be lo-
calized, because any two of the three access pairs in FRun
can be found show up together in PRun2 with the same
instruction addresses of the heads and the tails.
Dynamic instances of the same instruction executed in
diﬀerent threads or calling contexts can also impede our at-
tempt to localize buggy access pairs. But, concurrency bugs
616T2
5:var1=x;
6:var2=y;
7:if(var1!=var2)
8: printerror;T1
1:for(i=0;i<2;i++){
2:x=i+1;
3:y=i+1;
4:}intx=y=0;
T1T2
PRun12
xW6
yR5
xR
3
yW
2
xW
3
yWT1T2
PRun2T1T2
PRun3T1T2
FRun2
xW
3
yW
2
xW
3
yW6
yR5
xR
6
yR5
xR6
yR5
xR2
xW
3
yW
2
xW
3
yW2
xW
3
yW
2
xW
3
yW
Figure 4: An MA V bug example to illustrate the insuﬃciency
of instruction addresses for Test Procedure III.
involving diﬀerent calling contexts are very rare in real ap-
plications[19, 5], soweomitsuchcasesforeﬃciencyreasons.
Besides the instruction address, denoted as pc, we ap-
pend additional information on each shared memory access
to eliminate above confusion. The information include the
tid of the thread executing the access, denoted as tid, and
the loop information of the access, denoted as loop, which
indicates the entry and iteration of the loop if the access is
executed in a loop. With these additional information, the
check in Step2is performed at 3 progressive levels of pre-
ciseness: pc,tid, and loop. Each gives more preciseness, but
also more oﬄine analysis overhead.
We use the example in Figure 4 again to describe how the
check in Step2is performed at each preciseness level. Af-
terStep1in Algorithm 3, apSet={R5
x→W2
x,W3
y→R6
y,
R6
y→W3
y}. InStep2, there are two sets of coupled access
pairs to be checked: (R5
x→W2
x,W3
y→R6
y) and (R5
x→W2
x,
R6
y→W3
y). First, at the pc-level, both of them pass the
check as discussed before and no buggy access pair is lo-
calized. Second, at the tid-level, the two sets of coupled
access pairs respectively have the following forms of tids in
FRun: (T2→T1,T1→T2) and (T 2→T1,T2→T1).
As shown in the column 3 of Table 1, there is no MAV
bug with the second form. Thus, only the ﬁrst coupled
access pairs are checked and the second is discarded. In
PRun2, we can also ﬁnd that these two access pairs have
the same forms of tids, thus no buggy access pair is local-
ized. Third, at the loop-level, which is based on the observa-
tion that programmers generally either make a whole loop
atomic or create atomic regions within the loop body but
rarely across iteration boundaries [24], we can ﬁnd that W2
x
andW3
yin (R5
x→W2
x,W3
y→R6
y) are in the same loop
iteration in FRun, but they are in diﬀerent loop iterations
inPRun2. Thus this coupled access pairs cannot pass the
check in Step2and are reported as buggy access pairs.
4.4 Is the Complete PSet Necessary?
Asstatedabove, eachtestprocedurerequiresthecomplete
PSet, which is the set that contains allfeasible PRuns.
However, it is usually not available in practice. Thus, is
it really necessary to have a truly complete PSetwhen we
apply the test procedures to localize buggy access pairs?
To answer this question, we conduct an experiment on 6
very diverse real concurrent applications shown in Table 2.
In this experiment, each application is executed many times
with the same workload in two environments, Nativeand
Random. Access pairs showing up in each run are collected.
The only diﬀerence between Native and Random environ-Applications KLOCDescription Workload
FFT 1 FFT transformationdefault input
RADIX
1 Integer radix sort default input
Pbzip2 2 File compressor compress a regular ﬁle
Transmission 95 Bittorrent client open a torrent ﬁle
Apache 340 Web server concurrent http requests
MySQL 681 Database server mysql test suite
Table 2: Concu rrent open-source applications for the experi-
ment. ”KLOC”: program size in thousands of lines of code.
Figure 5: Accum ulated access pairs in each program run. ”*-N”:
Native, ”*-R”: Random.
ments is that a random number of microsecond delays are
inserted at the points of shared memory accesses in the Ran-
dom environment to explore more thread interleavings.
Figure5showstheaccumulatednumberofdiﬀerentaccess
pairs in each run. When the number of test runs reaches a
certain amount, the increase in the number of diﬀerent ac-
cess pairs diminishes quickly or remains the same, which
means that the number of diﬀerent access pairs in concur-
rent programs is limited. In fact, the occurrence of a new
access pair depends on the explicit synchronizations used
in a program and the distance1between the head and the
tail of the access pair. Statistically, if these two factors are
kept unchanged, all feasible access pairs can be collected. If
a non-buggy access pair is very diﬃcult to be collected, it
usually means this access pair has a very low probability to
appear in both successful and failed runs. Thus, although
it may introduce false positives if it shows up only in failed
runs, it can be ignored due to the very low probability.
Hence, there is really no need to run a large number of
PRunsto collect a complete PSetbecause there is only a
limited number of access pairs in PRunsfor most programs.
5. IMPLEMENTATION
This section describes Locon, a prototype based on the
proposed test procedures. Locon has two components: 1)
an online proﬁler, which takes a concurrent program binary
and its input as inputs, executes the program binary with
the given input, and collects access pairs during the execu-
tions; and 2) an oﬄine analyzer, which takes collected access
pairs as inputs and applies test procedures to localize buggy
access pairs when a concurrency bug is triggered.
1Thedetailed deﬁnition of the distance can be found in [28].
617The online proﬁler is implemented using Pin [22]. To
record the head and the tail of an access pair, we need to col-
lect the following information of the two memory accesses:
memory address, instruction addresses, thread ids, and loop
information. To obtain the loop information, Locon de-
tects loops via dominators in the control ﬂow graph [4]. The
detection is oﬄine, and introduces no additional overhead.
Loconuses a stack to maintain the loop information for
each thread. When a thread enters a loop, the entry ad-
dress and current iteration number of this loop are pushed
onto the stack. When this thread exits a loop, the corre-
sponding loop information is popped from the stack. For a
nested loop, when the thread exits the outer loop from the
inner loop, information of the two loops needs to be popped
together from the stack.
When a concurrency bug is triggered, the oﬄine analyzer
applies test procedures to localize the bug. Users are free to
apply any of the test procedures via arguments of Locon.
By default, test procedures are applied in the order of I, II,
and then III (in the order of pc, tid, and then loop) until
the bug is localized.
6. EMPIRICAL STUDIES
As shown in Table 3, we use 16 commonly used concur-
rency bug benchmarks in prior work to evaluate Locon [21,
37]. They include 7 extracted bugs, i.e. Group E, and 9
real-world bugs from real applications, i.e. Group R. These
bugs are written in C/C++ and represent diﬀerent failure
symptoms of concurrency bugs. For each bug, we collect
100 successful runs but only 1 failed run. The test proce-
dures and preciseness levels are applied to the failed run in
the default order until the buggy access pairs are localized.
The evaluation is conducted on an Intel Xeon machine with
1.87GHz 48 cores and Debian 6 operating system.
We also compare Locon with a state-of-the-art tool based
on a statistical approach, Falcon [30], which is most rele-
vant and similar to Locon. Due to the unavailability of the
source code, we have to implement Falcon based on our best
knowledge. Falcon uses a ﬁxed-size window to collect access
patterns on each shared memory location. Each pattern is
assigned a suspicious score, and patterns with higher scores
are considered more likely to be bugs. Our implementation
uses the same window size as that mentioned in the Falcon
paper. Other arguments are the same in L ocon.
6.1 Study 1: Effectiveness
As shown in Table 4, 10, 2, and 4 bugs are respectively
localized by Test Procedure I, II, and III. As expected, the
localization ability of Locon is not limited to bugs involv-
ing only one or two shared variables. For example, Bug#3
is an MAV bug involving three shared variables, and it is
successfully localized by Test Procedure III at the tid-level.
An interesting example is Bug#7, which is an MAV bug
but localized by Test Procedure I. Figure 6 shows the source
of this bug. R2
logfilenameandR3
curloginT1,W8
lo gfilename
andW9
curloginT2resp ectively have atomicity semantics.
Even if this bug involves two shared variables logfilename
andcurlog, it can be triggered by W9
curlog→R3
curlog
alone
, whichmeans T1readstherightvalueof logfilename
but the wr
ong value of curlog. Thus it can be localized by
Tes
t Procedure I. This provides a good proof that each test
procedure is not limited to localize concurrency bugs mani-
fested in the corresponding scenario.Concurrent Programs LOCConcurrency Bugs
Type Symptom
E1 scounter 53 SAV assertion failure
2 baccount 107 SAV wrong result
3 clist 168 MAV inconsistent results
4 sbuﬀer 186 SAV assertion failure
5 lprocsweep 99 SAV segmentation fault
6 MySQL-1 e 143 MAV inconsistent results
7 MySQL-2 e 101 MAV abnormal exit
R8 Pb
zip2 2K OV segmentation fault
9 Transmission 95K OV assertion failure
10 FFT 1K OV wrong program output
11 Aget 2.5K MAV inconsistent results
12 Apache-1 290K SAV redundant free crash
13 Apache-2 271K SAV confusion in log ﬁle
14 Apache-3 340K MAV assertion failure
15 MySQL-3 681K SAV miss in log ﬁle
16 MySQL-4 696K SAV segmentation fault
Table 3: Eva luated bugs. ”LOC”: program size in lines of code.
T2
7: intqueue_event(...){
...
8:log_file_name ="SQL";
9:cur_log="\x43";
...
10:}T1
1:log_event*next_event(...){
2: if(log_file_name =="MySQL"){
...
3: if(cur_log !=BINLOG_MAGIC)
4: gotoerr;
5: }
6:}#defineBINLOG_MAGIC "\xfe\x62\x69\x6e"
log_file_name ="MySQL";
cur_log=BINLOG_MAGIC;
Figure 6: Descriptio n of Bug#7.
Compared with Falcon, the localization results of Locon
have much fewer false positives. As shown in Table 4, for
the extracted bugs (i.e. the ﬁrst 7 bugs), Falcon can pre-
cisely identify buggy patterns. However, for the 9 real-world
programs, it is less precise and with more false positives.
Although Falcon can identify all buggy patterns with high
ranks (the column ”rank”), some non-buggy patterns can
also be ranked with high scores (the column ”#rank1”). For
example, in Bug#16, the number of patterns with the high-
estscoreis282. ThereasonisthatFalconrequirestocapture
a suﬃcient number of failed runs to gain higher scores for
buggy patterns. Furthermore, Falcon failed to localize 7 of
16 bugs mainly due to the following three reasons.
First, concurrency bugs are triggered by buggy access
pairsmanifestedinScenarioII,includingBug#9andBug#10.
Due toETPs, tails of those buggy access pairs are not actu-
ally executed in failed runs, i.e. they terminate prematurely.
Hence, Falconfailstocollectthosebuggypatterns. However,
Loconsuccessfully localizes them with Test Procedure II.
Second, concurrency bugs involve multiple shared vari-
ables. This contributes to Bug#3, Bug#6, Bug#11, and
Bug#14. Although Falcon also tries to extract patterns
from bugs involving two shared variables [29], it is very dif-
ﬁcult to extract long patterns from complex bugs involving
more than two shared variables, such as Bug#3. In contrast,
Loconis based on access pairs with only 2 memory accesses
each, rather than long access patterns (involved many mem-
ory accesses in each pattern), hence, has no such limitation.
Third, Falcon sometimes fails to gather a buggy access
pattern due to its ﬁxed window mechanism for eﬃciency.
This impedes the localization of Bug#4, which is an SAV
bug with the buggy pattern of R-W-R, similar to ID-6 in
Table 1. If the last access recorded in a window is a Wand
618Concurrency Bugs #SVLocon F
alcon
#AP #BAP TP I TP IITP IIItime(s) #P rank #rank1 time(s)pc tid loop
1 scounter 1 9 2 /check(2/1) - - - - 0.01 3 1 2 0.01
2 baccount 1 8 2 /check(2/1) - - - - 0.01 2 1 1 0.01
3 clist 3 11 5 0 0 0 /check(6/1) - 0.12 14 N 1 0.01
4 sbuﬀer 1 8 2 /check(1/1) - - - - 0.01 3 N 1 0.01
5 lprocsweep 1 4 1 /check(1/1) - - - - 0.01 4 1 2 0.01
6 MySQL-1 e 2 3 2 0 0 0 /check(1/1) - 0.15 2 N 1 0.01
7 MySQL-2 e 2 32 1 /check(1/1) - - - - 0.01 8 1 1 0.01
8 Pbzip2 1 224 1 /check(1/1) - - - - 0.13 35 1 1 0.01
9 Transmission 1 142 1 0 /check(60/1) - - - 0.42 57 N 5 0.06
10 FFT 2 7187 7 0 /check(7/1) - - - 8.1 50 N 1 0.03
11 Aget 2 7027 2 0 0 0 0 /check(1/1) 33 37 N 2 0.01
12 Apache-1 1 16625 1 /check(1/1) - - - - 27.66 751 9 1 2.33
13 Apache-2 1 1691 2 /check(2/1) - - - - 1.81 658 1 26 1.43
14 Apache-3 2 4899 2 0 79 0 0 /check(1/1) 388.3 563 N 23 1.23
15 MySQL-3 1 780977 2 /check(2/1) - - - - 740.8 1670 1 69 10.66
16 MySQL-4 1 5510 1 /check(1/1) - - - - 13.25 1730 1 282 29.15
Table 4: Local ization results. ”#SV”: number of shared variables involved; ”#AP”: number of access pairs in failed runs; ”#BAP”:
number of buggy access pairs; ”TP”: Test Procedure; ”#P”: number of patterns in failed runs; ”rank”: the rank of the buggy patterns;
”#rank1”: number of patterns with the highest score; ” /check(m/n)”: buggy access pairs are localized by this test procedure, mis the number
of access pairs that pass the check of this test procedure, nis the rank of buggy access pairs; ”N”: failed to localize the bug.
Applications Pin Loco nFalcon
Pbzip2 2.77X 19.96X 17.85X
Transm
ission 3.05X 7.2X 8.17X
FFT 1.36X 3.08X 2.01X
Aget 2.35X 41.34X 49.52X
Apache 1.25X 43.75X 44.25X
MySQL 1.31X 141.81X 131.13X
Table 5: Normalize d runtime overhead of the online proﬁler.
the current access to the same memory location is a Rfrom
the same thread, Falcon discards the R. This causes a loss
of the ﬁrst Rin the collected buggy access pattern.
6.2 Study 2: Efﬁciency
Table5showstheoverheadoftheonlineproﬁlerinLocon.
The overhead of Pin is also included, which is the overhead
of executing an application under Pin without any instru-
mentation. All runtime results are normalized to the native
execution time. Note that the original Falcon was imple-
mented in Java and was compared with the runtime of Java
byte code. Our implementation is in C/C++ and is com-
pared with the runtime of native binaries. In our implemen-
tation, Locon introduces comparable overhead in each run
to that of Falcon on average. We believe the overhead can
be further reduced, which is left as part of our future work.
The two columns ”time(s)”in Table 4 respectively shows
the time consumed by the oﬄine analyzer in Locon and
Falcon. In most cases, Locon is eﬃcient. But, for some
bugs, e.g. Bug#14 and Bug#15, the oﬄine analysis time of
Loconcould be somewhat long. To understand this phe-
nomenon, theprocessofoﬄineanalysisisfurtherpartitioned
into two steps. First, the analyzer reads access pairs from
the available successful runs. Second, the analyzer applies
test procedures to localize the bugs. The time consumed by
each step is shown in Figure 7.
For most bugs, i.e. Bug#10, Bug#12, Bug#13, Bug#15,
and Bug#16, the reading step consumes most of the anal-
ysis time, because the analyzer needs to read all available
successful runs before the localization. Diﬀerent from these
bugs, most of the analysis time for Bug#11 is in the local-
izing step. The reason is that this bug is localized at the
loop-level, which needs more complex checks for the local-0%20%40%6
0%80%100%Percentage
Concurrency BugsReadingLocalizing Others
Figure 7: Over heads of the oﬄine analyzer
ization. However, Bug#14 consumes almost all of the time
in the localizing step. After studying this bug, we ﬁnd that
the loop information recorded in the failed run of this bug
is more complex than that of Bug#11, which made the lo-
calizing step more complex and time-consuming.
6.3 Study 3: Sensitivity
We also study the sensitivity of localization results on the
number of successful runs by observing the change of lo-
calization results using diﬀerent numbers of successful runs.
Figure 8 shows analysis results. For Test Procedures I and
II, the increased number of successful runs makes the local-
ization results more precise. For Test Procedure III, there
are some diﬀerences. For example, in c list, the number of
coupled
access pairs in the localization results increases with
the number of successful runs. After studying the localiza-
tion results, we ﬁnd that the reason is that Test Procedure
III requires access pairs to show up in both successful and
failed runs. If the number of successful runs is very small,
some access pairs may not show up in those successful runs.
From Figure 8, we can conclude that the localization re-
sults of each test procedure are reasonably precise even with
a small number of successful runs collected, which is 10 for
most of the bugs. Even for MySQL-4 and Transmission, the
number of successful runs needed is only 60 and 70, respec-
tively. This demonstrates the practicability of Locon for
bug localization because there is no need to gather a huge
number of successful and failed runs.
6190102030405
060708
0
1 10 20 30 40 50 60 70 80 90 100s_counter b_account
l_proc_sweep MySQL-2_e
Pbzip2 Apache-1
Apache-2 MySQL-3
MySQL-4
(a) Test Procedure I050100150200
1 10 20 30 40 50 60 70 80 90 100Transmission
FFT
(b) Test Procedure II05101520
1 10 20 30 40 50 60 70 80 90 100c_list s_buffer
MySQL-1_e Aget
Apache-3
(c) Test Procedure III
Figure 8: Sensitivit y analysis on the number of successful runs. X-axis is the number of available successful runs, and Y-axis is the
number of access pairs (for Test Procedures I and II) or coupled access pairs (for Test Procedure III) in localization results.
6.4 Threats to Validity
There are several threats to validity of our empirical stud-
ies. Threats to internal validity stem from the empirical
setup. We assume programmers debug and ﬁx the program
by checking shared memory access pairs until the bug is
localized. Although it may be diﬀerent with the real de-
bugging process, this approach has been adopted by many
statistical bug localization research. Threats to external va-
lidity constraint the extent to which our approach can be
used to other types of program bugs, which may introduce
potential false positives or false negatives. Due to the lack
of widely accepted bug benchmark suites, related work also
suﬀer from this threat. To ease this threat, our bug suit has
covered a majority of bugs used in related work.
7. RELATED WORK
Concurrency Bug Localization Statistical analysis is an
eﬀective method for bug localization in sequential programs
[14, 16, 17]. Recently, it is also applied to concurrency bugs.
Falcon [30, 29] localizes concurrency bugs by pinpoint-
ing faulty interleaving patterns. The patterns are extracted
from common types of concurrency bugs that include unseri-
alizableandconﬂictinginterleavingpatterns. Falcongathers
patternsinsuccessfulandfailedrunsviaonlinepatterniden-
tiﬁcation. Each pattern gathered in failed runs is assigned
a suspicious score according to the Jaccard index [1]. The
patterns with higher suspicious scores are more likely to be
concurrency bugs.
CCI [12] applies the statistical debugging technique for
bug isolations. CCI dynamically monitors three types of
interleaving-related predicates on program states and be-
havior to diagnose program failures caused by concurrency
bugs. CCI identiﬁes failure predicates via statistical models.
Based on the statistical model used by CCI, PBI [5] is pro-
posed to diagnose concurrency bugs in software production
runs. The predicates used by PBI, called MESI-predicates,
are cache-coherence events that can be monitored by hard-
ware performance counters.
The commonality of above approaches is that they are
all based on some empirical statistical models, which is the
main diﬀerence between them and Locon. To precisely lo-
calize a concurrency bug, these models require a suﬃcient
number of successful and failed runs. Unfortunately, some
concurrency bugs are not easy to trigger in practice [23, 19,
28]. Therefore, it is quite challenging to gather enough failed
runs for such models. In contrast, our approach has no such
limitation, since it leverages diﬀerent test procedures to lo-
calize buggy access pairs manifested with diﬀerent scenarios.Even with only one single failed run captured, our approach
can still precisely localize the bug. Furthermore, the local-
ization results of our approach have fewer false positives due
to the consideration of shared memory accesses missing in
failed runs.
Concurrency Bug Detection There are many work on
concurrency bug detection, including data races [7, 32, 33,
3, 27] and AV bugs [20, 28, 38]. Diﬀerent with these work,
Loconaims to localize bugs after they are triggered.
Concurrency Bug Testing Some methods use diﬀerent
schedule or preemption policies to eﬀectively expose con-
currency bugs [23, 6, 37]. These methods can complement
Loconin gathering successful runs for the bug localization.
Other Work Some other tools try to automatically ﬁx
concurrency bugs [11, 13, 18]. Locon is also helpful to these
tools since they only ﬁx bugs that have been localized.
8. CONCLUSION
This paper presents an exhaustive list of scenarios that
indicate the manifestation diﬀerences of buggy access pairs
triggering OV or AV bugs in successful and failed runs.
Based on these scenarios, a concurrency bug localization
framework is proposed. The framework consists of 3 test
procedures to localize buggy shared memory access pairs
manifested in those scenarios. Each test procedure is de-
signed with the consideration of minimizing false positives
during the fault localization. A bug localization prototype,
called Locon, is also implemented based on the 3 test pro-
cedures. Empirical results on 16 commonly used bugs show
that Locon is quite precise and practical for concurrency
bug localization. Compared with existing methods, which
require a suﬃcient number of failed runs, Locon needs only
one failed run. Besides, there is also no need to gather a
huge number of successful runs for Locon due to the lim-
ited number of access pairs in concurrent programs.
9. ACKNOWLEDGMENTS
We would like to thank the anonymous reviewers for their
useful feedback. This research is supported by the National
Natural Science Foundation of China (NSFC) under grants
61303052, 61332009, 61303051, and 60925009, the National
High Technology Research and Development Program of
China under grant 2012AA010901, the Innovation Research
Group of NSFC under grant 61221062, the National Ba-
sic Research Program of China under grant 2011CB302504,
the National Science Foundation under grant 1320796, CA-
REER Award and DOE Early Career Award.
62010. REFERENCES
[1]
R. Abreu, P. Zoeteweij, and A. J. C. van Gemund. On
the accuracy of spectrum-based fault localization. In
TAICPART, 2007.
[2] S. V. Adve and M. D. Hill. Weak ordering - a new
deﬁnition. In ISCA, 1990.
[3] S. V. Adve, M. D. Hill, B. P. Miller, and R. H. B.
Netzer. Detecting data races on weak memory
systems. In ISCA, 1991.
[4] A. V. Aho, M. S. Lam, R. Sethi, and J. D. Ullman.
Compilers: Principles, Techniques, and Tools.
Addison-Wesley, 2006.
[5] J. Arulraj, P.-C. Chang, G. Jin, and S. Lu.
Production-run software failure diagnosis via hardware
performance counters. In ASPLOS, 2013.
[6] S. Burckhardt, P. Kothari, M. Musuvathi, and
S. Nagarakatte. A randomized scheduler with
probabilistic guarantees of ﬁnding bugs. In ASPLOS,
2010.
[7] C. Flanagan and S. N. Freund. Type-based race
detection for Java. In PLDI, 2000.
[8] S. Focus. Software bug contributed to blackout.
http://www.securityfocus.com/news/8032.
[9] P. Godefroid and N. Nagappan. Concurrency at
Microsoft – an exploratory survey. In Workshop on
Exploiting Concurrency Eﬃciently and Correctly,
2008.
[10] D. Jackson and C. A. Damon. Elements of style:
Analyzing a software design feature with a
counterexample detector. In IEEE TSE, 1996.
[11] G. Jin, L. Song, W. Zhang, S. Lu, and B. Liblit.
Automated atomicity-violation ﬁxing. In PLDI, 2011.
[12] G. Jin, A. Thakur, B. Liblit, and S. Lu.
Instrumentation and sampling strategies for
cooperative concurrency bug isolation. In OOPSLA,
2010.
[13] G. Jin, W. Zhang, D. Deng, B. Liblit, and S. Lu.
Automated concurrency-bug ﬁxing. In OSDI, 2012.
[14] J. A. Jones and M. J. Harrold. Empirical evaluation of
the Tarantula automatic fault-localization technique.
InASE, 2005.
[15] N. G. Leveson and C. S. Turner. An investigation of
the Therac-25 accidents. In IEEE Computer, 1993.
[16] B. Liblit, A. Aiken, A. X. Zheng, and M. I. Jordan.
Bug isolation via remote program sampling. In PLDI,
2003.
[17] B. Liblit, M. Naik, A. X. Zheng, A. Aiken, and M. I.
Jordan. Scalable statistical bug isolation. In PLDI,
2005.
[18] P. Liu and C. Zhang. Axis: Automatically ﬁxing
atomicity violations through solving control
constraints. In ICSE, 2012.
[19] S. Lu, S. Park, E. Seo, and Y. Zhou. Learning from
mistakes – a comprehensive study on real world
concurrency bug characteristics. In ASPLOS, 2008.
[20] S. Lu, J. Tucek, F. Qin, and Y. Zhou. AVIO:
Detecting atomicity violations via access interleaving
invariants. In ASPLOS, 2006.[21] B. Lucia and L. Ceze. Finding concurrency bugs with
context-aware communication graphs. In MICRO,
2009.
[22] C.-K. Luk, R. Cohn, R. Muth, H. Patil, A. Klauser,
G. Lowney, S. Wallace, V. J. Reddi, and
K. Hazelwood. Pin: Building customized program
analysis tools with dynamic instrumentation. In PLDI,
2005.
[23] M. Musuvathi and S. Qadeer. Iterative context
bouding for systematic testing of multithreaded
programs. In PLDI, 2007.
[24] A. Muzahid, N. Otsuki, and J. Torrellas. Atomtracker:
A comprehensive approach to atomic region inference
and violation detection. In MICRO, 2010.
[25] MySQL. Bug report time to close stats.
http://bugs.mysql.com/bugstats.php , Dec. 2011.
[26] S. Narayanasamy, Z. Wang, J. Tigani, A. Edwards,
and B. Calder. Automatically classifying benign and
harmful data race using replay analysis. In PLDI,
2007.
[27] R. O’Callahan and J.-D. Choi. Hybrid dynamic data
race detection. In PPoPP, 2003.
[28] S. Park, S. Lu, and Y. Zhou. CTrigger: Exposing
atomicity violation bugs from their hiding places. In
ASPLOS, 2009.
[29] S. Park, R. Vuduc, and M. J. Harrold. A uniﬁed
approach for localizing non-deadlock concurrency
bugs. In ICST, 2012.
[30] S. Park, R. W. Vuduc, and M. J. Harrold. Falcon:
Fault localization in concurrent programs. In ICSE,
2010.
[31] PCWorld. Nasdaq’s facebook glitch came from race
conditions.
http://www.pcworld.com/article/255911/nasdaqs_
facebook_glitch_came_from_race_conditions.html.
[32] P. Pratikakis, J. S. Foster, and M. Hicks.
LOCKSMITH: Context-sensitive correlation analysis
for race detection. In PLDI, 2006.
[33] S. Savage, M. Burrows, G. Nelson, P. Sobalvarro, and
T. Anderson. Eraser: A dynamic data race detector
for multithreaded programs. In ACM TOCS, 1997.
[34] Transmission. http://www.transmissionbt.com/ .
[35] M. Vaziri, F. Tip, and J. Dolby. Associating
synchronization constraints with data in an
object-oriented language. In POPL, 2006.
[36] Z. Yin, D. Yuan, Y. Zhou, S. Pasupathy, and
L. Bairavasundaram. How do ﬁxes become bugs? In
FSE, 2011.
[37] J. Yu, S. Narayanasamy, C. Pereira, and G. Pokam.
Maple: A coverage-driven testing tool for
multithreaded programs. In OOPSLA, 2012.
[38] B. Zhou, J. Too, M. Kulkarni, and S. Bagchi.
WuKong: Automatically detecting and localizing bugs
that manifest at large system scales. In HPDC, 2013.
621