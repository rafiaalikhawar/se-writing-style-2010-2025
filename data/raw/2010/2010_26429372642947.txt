Active Code Search: Incorporating User Feedback to
Improve Code Search Relevance
Shaowei Wang, David Lo, and Lingxiao Jiang
School of Information Systems, Singapore Management University
{shaoweiwang.2010,davidlo,lxjiang}@smu.edu.sg
ABSTRACT
Code search techniques return relevant code fragments given
a user query. They typically work in a passive mode: given
a user query, a static list of code fragments sorted by the
relevance scores decided by a code search technique is re-
turned to the user. A user will go through the sorted list of
returned code fragments from top to bottom. As the user
checks each code fragment one by one, he or she will natu-
rally form an opinion about the true relevance of the code
fragment. In an active model, those opinions will be taken
as feedbacks to the search engine for rening result lists.
In this work, we incorporate users' opinion on the results
from a code search engine to rene result lists: as a user
forms an opinion about one result, our technique takes this
opinion as feedback and leverages it to re-order the results to
make truly relevant results appear earlier in the list. The re-
nement results can also be cached to potentially improve fu-
ture code search tasks. We have built our active renement
technique on top of a state-of-the-art code search engine|
Portfolio. Our technique improves Portfolio in terms of Nor-
malized Discounted Cumulative Gain (NDCG) by more than
11.3%, from 0.738 to 0.821.
Categories and Subject Descriptors: D.2.7 [ Software
Engineering ]: Distribution, Maintenance, and Enhance-
ment
General Terms: Algorithms; Experimentation
Keywords: Code Search; User Feedback; Active Learning
1. INTRODUCTION
Millions of open source and industrial software systems
have been developed and deployed. Maintaining these sys-
tems requires constant searching through various code bases
and documents and relate dierent parts of the code to-
gether. Also, the development of new systems can benet
from reusable knowledge hidden in many existing systems
if the developers can search through existing code and nd
relevant code for reuse. A number of studies in the litera-
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proÔ¨Åt or commercial advantage and that copies bear this notice and the full cita-
tion on the Ô¨Årst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciÔ¨Åc permission
and/or a fee. Request permissions from permissions@acm.org.
ASE‚Äô14, September 15-19, 2014, Vasteras, Sweden.
Copyright 2014 ACM 978-1-4503-3013-8/14/09 ...$15.00.
http://dx.doi.org/10.1145/2642937.2642947.ture have proposed various code search techniques that can
return pieces of code considered to be relevant to a user's
query [1,7].
One factor that potentially aects the eectiveness of code
search is the diculty in formulating a precise query that
matches the relevant code fragments needed by users. Often,
users could not exactly specify what they want in the form
of a set of keywords or constraints, but they are often able to
decide whether a given code fragment satises their needs.
Given a list of code fragments returned by a code search
engine, some code fragments might be close to what a user
wants, while others might be very dierent. As a user checks
each code fragment one at a time, he or she will naturally
form an opinion about its relevance. It will be benecial to
utilize user opinions on code fragments that he or she has
checked to improve the results.
The key idea of this paper is to improve code search rele-
vance by integrating users' feedback. Our approach records
user's opinion about each code fragment when they check
through a list of results returned by a search engine and re-
nes the list so that code fragments that are more relevant to
user's need would appear earlier in the list, which save user's
time. It can be built on top of any code search engine that
takes textual descriptions as query inputs (the rst family
of code search techniques mentioned above). The feedback
required by our approach does not impose much additional
eort on users either, as they would need to navigate the
search results anyway to decide whether the results match
their needs; the only dierence is that they need to explicitly
indicate their opinion about a search result when using our
approach.
Specically in this paper, we build our approach on one
of the state-of-the-art code search engines, Portfolio [7], to
incorporate user feedback. Users of our approach rst re-
ceive a list of search results from Portfolio for their query as
usual. Then, they can provide relevance feedback on each
search result when they check through the list one by one.
The feedback is expressed as a label in a 4-point Likert scale,
where 1,2,3, and 4 indicate whether the result is completely
irrelevant, mostly irrelevant, mostly relevant, and highly rel-
evant. Once the relevance feedback for a search result is
recorded, our approach incorporates this feedback to re-sort
the remaining search results. This is performed based on
the semantic and structural similarity between each of the
remaining results and all known relevant or irrelevant re-
sults. The goal is to have search results that are more likely
to be relevant to be re-arranged nearer to the top of the list.
Our approach can also utilize the past feedbacks collected
677
from previous queries to potentially improve the current and
future code search tasks. We refer to this as active code
search |users can more actively aect the output of a code
search engine by providing a series of relevance feedback.
In the information retrieval community, Rocchio is a pop-
ular way to incorporate relevance feedback by rening a
query based on a set of labeled results [6]. Our approach
is a customized reference feedback mechanism tailored for
code search. Dierent from Rocchio, which only considers
textual information, we also consider structural information
extracted from source code.
We have evaluated our approach with 70 queries on a code
base containing 19,414 programs written in C and/or C++,
comprised of about 169 million lines of code. Our evaluation
shows that Portfolio achieves an average NDCG score of
0.738 for the 70 queries on the code base, while our active
code search approach (Portfolioactive) achieves an average
NDCG score of 0.821. We have also compared our approach
against Portfolio merged with Rocchio (Portfoliorocchio) and
nd that Portfoliorocchioachieves a lower average NDCG
score of 0.764.
The structure of this paper is as follows. We present our
active code search approach in Section 2 and zoom in to the
renement engine component in Section 3. Our empirical
evaluation results are presented in Section 4. We discuss
related studies in Section 5. We nally conclude and mention
future work in Section 6.
2. APPROACH OVERVIEW
This section presents the overall framework of our active
code search approach. Section 3 elaborates the details about
the core component of our approach|the renement engine
that reorders the search results from a normal code search
engine based on relevance feedback from users.
Our active code search framework is shown in Figure 1.
It consists of two major processing components (shown as
rectangular blocks in the gure): Code Search Engine and
Renement Engine. The interaction between users and these
two major components would result in a list of results (in the
rounded rectangular block) containing potentially relevant
code fragments. As a user can provide feedback piece by
piece incrementally, the list of search results would be rened
in multiple iterations, as illustrated by the circular arrows.
The sequence of the interactions in our active code search
framework is described in the following paragraphs.
 
 
 
   
 
  Code Search 
Engine  List of  
Results  
Query  Relevance 
Feedback  Refinement 
Engine  Update  Output  
Display  Query  
Figure 1: Active Code Search: Overall Structure
First, a user posts a query to a passive code search engine.
The search engine takes the user query and returns a list of
code fragments (i.e., the \List of Results" block in Figure 1)sorted according to the scores of the code fragments calcu-
lated internally by the search engine. The earlier a code
fragment appears in the list, the more similar the search en-
gine thinks it is to the user query. In this paper, we use
Portfolio [7] in the Code Search Engine block.
Second, as the user navigates through the list of results
one by one, he or she can provide relevance feedback. For
example, after the user investigates the result at the top of
the list, the user forms a judgement if the result is relevant.
The judgement can be expressed as a label for the result in
a 4-point Likert scale, where 1, 2, 3, and 4 indicate whether
the result is completely irrelevant, mostly irrelevant, mostly
relevant, and highly relevant, respectively. Each judgement
for the results investigated by the user is used as input to
the Renement Engine block one at a time.
Third, as the renement engine receives a piece of rele-
vance feedback, it renes the ordering of the results in the
list that have not been investigated by the user, aiming to
improve the overall relevance of the rened list. For exam-
ple, an uninvestigated code fragment that is originally at the
5th position could be shifted to the 48th position, as it is
similar to some investigated code fragments that are given
low relevance scores; on the contrary, an uninvestigated code
fragment at the 48th position could be shifted to the 4th po-
sition, as it is similar to some investigated code fragments
receiving high relevance scores. The process that involves
rening a list of results, displaying a rened list, and pro-
viding relevance feedback repeats until the user decides to
stop or when the list is exhausted. When a user stops, it
could correspond to cases where the user has found what he
or she wants, or where the user decides to accept something
marginally relevant and use the returned code from there, or
where the user gives up and decides to complete his or her
task without reference code, etc. In the renement engine,
the feedbacks of previous queries and their corresponding
rened results are cached. When a new query is posted, if a
similar query is identied in the cache, the rened results of
the similar query are returned. We describe how we identify
the relevant rened results in Section 3.
Our active code search approach does not need to query
the passive code search engine any more during the rene-
ment, and thus does not need changes to the normal code
search engine and can be easily integrated with any engine
that takes user queries in the form of textual descriptions.
3. REFINEMENT ENGINE
This section describes the core component of our approach
in detail. Figure 2 illustrates the structure of our renement
engine. The engine takes in the original user query, a list
of results, the relevance feedback from users expressed in 4-
point Likert scores, and outputs a rened list of results that
shows potentially more relevant results nearer to the top.
Our renement engine has several data blocks (in rounded
rectangles): \Results", \Labeled Results", \Unlabeled Re-
sults", \Rened Query Representation", and \Reordered Re-
sults". Results are the list of results displayed to the user so
far. Labeled results are results that have received relevance
feedback from the user. Unlabeled results are those that
have not received relevance feedback from the user. Rened
Query Representation is used internally in our renement
engine to represent the combined eect of the original query
and the results that have been investigated and labeled by
the user. Reordered results are the rened list outputted by
678Res
ults
 
Labeled Results
 
Unlabeled Results
 
Update 
Query
Rep
resen
tation
Refined
 
Query
Rep
resen
Reorder 
Results
 
Reordered 
Results
 
Relevance 
Feedback
 
 
Parameter
 
Tuning
 
Update
 
 
Original 
Query
 
Feedback
List of 
Results
 
Cache 
Processor
 Figure 2: Renement Engine Component
the renement engine after taking user feedback into con-
sideration and will be presented to the user for additional
feedback.
Our renement engine has also several processing blocks
(in rectangles): \Update Query Representation",\Parameter
Tuning",\Reorder Results", and \Cache Processor". These
processing blocks work together to rene the list of results
for the original user query based on the relevance feedback.
The \Update Query Representation" block produces the re-
ned query representation from labeled results and the orig-
inal query. This rened query representation is used by the
\Reorder Results" block to produce the rened results. The
\Update Query Representation" block accepts parameters
that are to be tuned periodically as more relevance feedback
is received. This is done by the \Parameter Tuning" block.
The \Cache Processor" block stores the original queries that
the renement engine have processed before and their corre-
sponding labeled results, rened query representations, and
reordered results. This block also checks if a new input query
closely matches a past query; if it does, it will bootstrap the
renement engine with the cached data. If the same query
is processed by the renement engine, the cached data will
be updated. We elaborate these processing blocks in Sec-
tions 3.1, 3.2, 3.3, and 3.4.
3.1 Update Query Representation
In this block, we incorporate information from search re-
sults that have received relevance feedback (i.e., Labeled Re-
sults) into the original query. We convert the original query
into its representative vectors. We also convert search re-
sults that have received relevance feedback into their repre-
sentative vectors. We then update the representative vectors
of the original query to a rened query representation. To
elaborate \Update Query Representation", we present some
denitions rst, and then the algorithm.
3.1.1 Query and Result Representations
We rst introduce the representations for a query and a
result that can be used to transform a textual query or a
code fragment into vectors of numerical scores. We consider
two representations: semantic and structural.Definition 1 (Semantic Representation). In the se-
mantic representation, a query and a code fragment are viewed
as a bag of words. We use standard tokenization, stop word
removal, identier splitting, and stemming to convert a query
or a code fragment into a bag of words [6]. The semantic
score of a word is given by the product of its term frequency
andinverse document frequency (tfidf) [6]. The term
frequency (tf) of a word is the number of times the word ap-
pears in the query or code fragment normalized by the total
number of words in the query or code fragment. The in-
verse document frequency ( idf) of a word is the logarithm of
the total number of documents (i.e, the number of methods
in the code base in our code search setting) divided by the
number of documents that contain the word. Given a query
or code fragment q, we denote the vector of semantic scores
representation of qasVScore(q )sem.
Example. Let a query qbelock unlock le , then the term
frequency of words lock, unlock and leare 0.33, 0.33, 0.33
respectively. Also, let the words lock, unlock andleappear
in 270,872, 154,029, and 800,672 methods and there are to-
tally 7,916,458 methods in the code base, so the inverse doc-
ument frequencies for the words calculated by the equation
idf(w;D ) =logjDj
jfd2D:w2dgjare 1.46, 1.71, and 0.99, respec-
tively, where wis a word in the query and dis a method in
the code base D. Finally, the semantic scores are 0.48, 0.54,
and 0.33 for lock, unlock and le, respectively.
Definition 2 (Structural Representation). In the
structural representation, a query and a code fragment are
viewed as a bag of function calls. The structural score of a
function is given by the product of its term frequency and in-
verse document frequency. Given a query or a code fragment
rin the search results, the term frequency of a function in
ris the frequency of the function being called in rnormal-
ized by the total number of function calls in r; the inverse
document frequency of a function is the logarithm of the to-
tal number of code fragments in the search results divided by
the number of code fragments in the search results that call
the function. We denote the vector of structural scores of
rasVScore(r )str. Note that the original query entered by
the user does not contain method calls, thus it is represented
by a vector of zeroes. However, as we incorporate results
that have received relevance feedback from users to the orig-
inal query, the query's vector of structural scores would be
updated.
Example. Consider a code fragment rwhich is a function
ScLockedFile . There is only one function unlock called in
ScLockedFile . So the term frequency of unlock is 1. There
are totally 50 search results returned, and the function un-
lock is called by 2 code fragments in the results; so its in-
verse document frequency value is log50
2=1.40. Finally, the
VScore(r )strforunlock is 1.40.
3.1.2 Vector Operations
We also dene operations that are applied to vectors and
help to make it easier to describe the algorithm in the fol-
lowing subsections of this paper.
Definition 3 (Vector Summation and Division). {
Letv[i]be the score in a vector vcorresponding to a word or
a functioni. In the case that the word or the function idoes
not have a corresponding entry in v, letv[i]returns 0. Let us
679also denote WS(v)to be the set of words and functions that
have corresponding entries in vector v. Given two vectors
v1andv2, the summation of these two vectors would result
in a new vector vr, and8i:vr[i] =v1[i] +v2[i]. Consider a
vectorvand a constant c, the division of vector vbycwould
result in a new vector vr, and8i:vr[i] =v[i]=c.
We also need to dene the similarity metric among vectors
so that we can gauge the renement of queries and search
results. In this paper, we measure vector similarity by using
the well-known cosine similarity [6].
3.1.3 Algorithm
The procedure for\Update Query Representation"is shown
in Algorithm 1. The procedure takes in a set of search re-
sults labeled so far ( LBL), the new feedback ( fback ), the
set of unlabeled results (ULBL), the original user query
origquery , and a set of weights ( 1;2;3;4) that deter-
mine the contributions of labeled results with the Likert
scores 1, 2, 3, and 4 respectively. Weight 1;2;3;4are
set to be -0.3, -0.1, 0.1, and 0.5 initially. The procedure up-
dates the set of labeled results LBL and unlabeled results
ULBL, and creates a rened query representation refquery
which consists of two vectors refquery semandrefquery str
which are the semantic and structural representations.
Algorithm 1 Update Query Algorithm
1:Procedure UpdateQuery
2:Input:
3:LBL : Labeled search results
4:fback : New feedback, i.e., a new Likert score to an unlabeled
search result
5:ULBL: Unlabeled search results
6:origquery : Original user query
7:1;2;3;4: The weights of contributions of labeled results
with the Likert score 1, 2, 3, and 4 respectively
8:Output: UpdatedLBL ,ULBL, and a rened query representa-
tionrefquery
9:Method:
10:Addfback intoLBL , and remove the result labeled by fback
fromULBL
11:LetLBL1,LBL2,LBL3, andLBL4be the sets of results in
LBL with Likert scores 1, 2, 3, and 4 respectively
12:Compute the semantic centers of LBL1,LBL2,LBL3, and
LBL4and denote them as C1sem,C2sem,C3sem, andC4sem13:Compute the structural centers of LBL1,LBL2,LBL3, and
LBL4and denote them as C1str,C2str,C3str, andC4str14:Letrefquery sem=VScore sem(origquery ) +P
i=1:::4iCisem
15:Letrefquery str=VScore str(origquery ) +P
i=1:::4iCistr
16:ReturnLBL ,ULBL, and (refquery sem,refquery str)
To transform the inputs to the outputs, the procedure
works in the following steps. We split the results in LBL
into four sets based on the relevance scores (Line 11). After
this step, we compute the semantic center of each set (Line
12) by the following equation:
Cisem=P
r2LBL iVScore sem(r)
jLBL ij
Similarly, we compute the structural center of each set (Line
13) by the following equation:
Cistr=P
r2LBL iVScore str(r)
jLBL ij
The (semantic or structural) center of each set is a vec-
tor that is the summation of the (semantic or structural)vectors of all results appearing in the set normalized by
the size of the set (i.e., the number of results in the set).
We then compute the semantic rened query representation
(refquery sem) by combining the centers of the correspond-
ing four sets ( Cisem) with the semantic score vector of the
original query ( origquery sem) (Line 15). The structural re-
ned query representation (refquery str) is computed in a
similar way (Line 15). This rened query representation
(refquery sem,refquery str) is then used to help reorder the
search results in Section 3.2.
3.2 Reorder Results Block
In this block, we sort the unlabeled results based on their
similarity with the rened query representation. The pseu-
docode is shown in Algorithm 2. It takes in the rened
query representation refquery generated by Algorithm 1
and a list of unlabeled search results ULBL. It outputs
a reordered ULBL by the following steps. First, it iterates
through the list of unlabeled results to compute the struc-
tural and semantic similarity scores between the score vec-
tors of each result and those of refquery , i.e.,refquery sem
andrefquery str(Lines 8-9). It then takes the average of
these two scores to compute the overall similarity score (Line
10). Second, it reorders the unlabeled results according to
their overall similarity scores in the descending order (Line
12).
Algorithm 2 Reorder Result Algorithm
1:Procedure ReorderResults
2:Input:
3:refquery : Rened query representation
4:ULBL: Unlabeled results
5:Output: ReorderedULBL
6:Method:
7:for allrinULBL do
8: Letsimsem=cos(VScore (r)sem;refquery sem)
9: Letsimstr=cos(VScore (r)str;refquery str)
10: Letsimoverall =simsem +sim str
211:end for
12:Sort all results in ULBL in the descending order according to
their overall similarities to refquery as computed at Line 10
13:ReturnULBL
3.3 Parameter Tuning
Our renement engine takes in 4 weight parameters: 1,
2,3and4. These 4 parameters are initially set to take
the following values: -0.3, -0.1, 0.1, and 0.5, respectively1.
We re-tune these weight parameters whenever we receive a
new relevance feedback rating a result with Likert score 3 or
4 (i.e., the result is marginally or highly relevant). We tune
the algorithm by trying many possible parameter settings
one at a time. For each parameter setting, we consider its
eectiveness on the set of labeled data known so far. We
pick the parameter setting which is the most eective (i.e.,
it achieves the highest NDCG on the set of labeled data
known so far).
The pseudocode of our tuning process is shown in Al-
gorithm 3. Let us dene a notation AP(a;b;s ) to repre-
sent an arithmetic progression (AP) between aandbwith
a steps. We initialize two ranges 1 2and3 4to be
AP( 0:5; 0:4;0:1) andAP(0;0:9;0:1) respectively (Line 6).
The rst is the range of possible parameter values for 1
1These values are determined empirically.
680and2. The second is the range of possible parameter val-
ues for3and4. Then we try to adjust the 4 parameters
by trying dierent combinations of values picked from the
sets1 2and3 4(Line 8).
After a combination of parameters are picked, we evalu-
ate its eectiveness on the set of labeled results known so far
LBL (Line 9). We simply reorder LBL using the parame-
ter combination setting and compute NDCG. The larger the
resultant NDCG score is, we assume the better a parameter
combination is. We repeat the above two steps (i.e., Line 8
and 9) until we nd a local optimum (Line 10). We detect a
locally optimal setting for 1;2;3;4by trying all combi-
nations of values picked from the sets 1 2and3 4in order
and looking for a combination whose resultant NDCG score
is 1 or no smaller than the NDCG scores of its neighbor-
ing congurations in the combinations of parameter values
in the arithmetic progressions. We nally output the local
optimal setting of the 4 parameters (Line 11).
Algorithm 3 Parameter Tuning Procedure
1:Procedure ParameterTuning
2:Input:
3:LBL : Labeled results
4:Output: Parameters 1;2;3;45:Method:
6:Initialize two sets 1 2and3 4to beAP( 0:5; 0:4;0:1) and
AP(0;0:9;0:1) repectively
7:repeat
8: Adjust the value of the 4 parameters
9: Evaluate eectiveness of the adjusted parameters on LBL
10:until a local optimum is reached
11:Output the local optimum
3.4 Cache Processor
Once a user posts a new query, the \Cache Processor"
block checks whether there exists a highly similar query in
the cache. If a highly similar query is identied, its corre-
sponding cached results will be used to bootstrap the re-
nement engine. To identify whether a highly similar query
exists, this block computes the similarity of the new query
with each of the old queries. The similarity of a new query
qnewand an old query qoldis computed by taking the cosine
similarity of their corresponding semantic vectors:
Similarity sem=cos(VScore(q new)sem;VScore(q old)sem)
We rank the old queries based on their similarity scores.
The top one old query whose similarity is larger than a
thresholdtis identied as the highly similar query (ties are
randomly broken). If no old query has similarity above t,
then no highly similar query is identied. In this study, by
default, we set tto 1 which means only the exactly same
query will be identied.
4. EXPERIMENTS
Experimental Settings: The code base we use in this
work is from FreeBSD2. We download 49,889 program ver-
sions from this code base, but use only the latest version for
programs that have multiple versions. Finally, we use 19,414
programs written in C and/or C++ in our study. The total
size of these programs is around 36GB, and they contain
2ftp://ftp.freebsd.org/pub/FreeBSD/distfiles/
0.2
0.3
0.4
0.5
Improvement
-
0.1
0
0.1
0.2
1
4
7
10
13
16
19
22
25
28
31
34
37
40
43
46
49
52
55
58
61
64
67
70
Improvement
Query IdFigure 3: Improvement of Portfolioactiveover
Portfoliooriginalin terms of NDCG
about 169 million lines of code, 8 million functions, and 2
million les. We have used 70 queries created by Portfo-
lio's author [7]. All these queries are formulated as set of
keywords to address some programming tasks reported in
Portfolio's user study.
We involve users for evaluation and use a simple web ap-
plication written in PHP to display search results and collect
user feedback. In the user study, we have 10 participants, 9
of them are PhD students who have at least of two years of
Java and C++ programming experience, and the other is a
professional software engineer who has three years of Java
and C++ programming experience. Each participant is as-
signed a number of queries3and asked to examine the top
fty search results for each query and provide a relevance
score in a 4-point Likert scale for every result.
Evaluation Results: We build our approach on top of
Portfolio and thus we need to compare the two approaches
together. We compare the NDCG of Portfolioactivewith
that of the original Portfolio (Portfoliooriginal). Our evalu-
ation shows that the original Portfolio on average achieves
an NDCG score of 0.738 while Portfolioactiveon average
achieves an NDCG score of 0.821, a 11.3% improvement.
Figure 3 presents a detailed comparison of the NDCG score
for every query. Portfolioactivewins in 67 cases while margin-
ally performing worse for 3 other cases (query number 35,
49, and 52). We also perform a Wilcoxon signed-rank test
on the NDCGs and nd that the improvement achieved by
Portfolioactiveis statistically signicant ( p-value<0.05).
In the default setting, for each code search task, users
give a feedback to each of the rst 50 results and after each
feedback we apply our renement engine. We would like to
test the eectiveness of our active code search approach with
dierent numbers of feedbacks from users (denoted as Kf).
In this experiment, users only give feedback to the rst Kf
results. We vary Kfin the setf1,5,10,20,30,40,50 g. Ta-
ble 1 compares the eectiveness of Portfoliooriginalwith our
Portfolioactivewith dierent number of feedbacks ( Kf) to
rene the results. When only one feedback is given by user
for rening the results, Portfolioactiveachieves a 7.46% im-
provement over Portfoliooriginal. As theKfvalue increases,
Portfolioactiveachieves more and more improvement until
Kfreaches 30. The eectiveness of Portfolioactiveremains
constant when Kfis increased from 30 to 50.
We also want to compare the eectiveness of our approach
against Portfolio with Rocchio (Portfoliorocchio). Dierent
from our proposed approach, standard Rocchio does not con-
sider structural scores and use a static set of weights to in-
corporate labeled results to rene a query. In our experi-
3Each query is assigned to only one participant.
681Table 1: Comparison of Portfoliooriginaland
Portfolioactivein terms of NDCG for dierent Kf
KfPortfoliooriginalPortfolioactiveImprovement
1 0.738 0.794 7.46%
5 0.738 0.809 9.59%
10 0.738 0.816 10.55%
20 0.738 0.820 11.05%
30 0.738 0.821 11.12%
40 0.738 0.821 11.12%
50 0.738 0.821 11.12%
ments, we set the weights of Rocchio to their recommended
values [6]:a= 1,b= 0:75, andc= 0, where a,b, andcare
the weights of the original query, results labeled as relevant,
and results labeled as irrelevant respectively.
We compare Portfolioactivewith Portfoliorocchioin terms
of NDCG. Portfoliorocchioachieves an average NDCG score
of 0.764. Porfolioactiveachieves an average NDCG score of
0.821 which is a 7.5% improvement over Portfoliorocchio's
result. We also perform a Wilcoxon signed-rank test and
nd that the improvement achieved by Portfolioactiveis sta-
tistically signicant (p-value <0:05).
Threats to Validity: Threats to internal validity include
experimenter bias. There might be subjectivity in the rele-
vance scores that a participant assigns to returned code frag-
ments. Threats to external validity relate to the generaliz-
ability of our ndings. We have only investigated 70 queries
and considered a code base consisting of 19,414 projects and
169 million lines of code. We have also only considered the
case where the value of threshold tof the Cache Processor
block is set to 1. Threats to construct validity refer to the
suitability of our evaluation metrics. We use NDCG, a com-
mon metric used to investigate the quality of web search
engines [6].
5. RELATED WORK
There are many code search approaches proposed to help
users nd relevant code. Some of them take textual infor-
mation as input to search code. McMillan et al. propose
Portfolio that takes natural language descriptions as input
and outputs a list of functions or code fragments along with
corresponding call graphs [7,8]. Chan et al. propose an ap-
proach to help developers nd usages of API methods given
simple text phrases [1]. They use an ecient graph search
algorithm to return an optimum connected subgraph that
matches user's query. Haiduc et al. propose a code search
tool named Refoqus that is able to predict the quality of
a textual query for further query reformulation [3]. Dier-
ent from Haiduc et al.'s work, we integrate the structural
information of source code itself with text information to
rene code search results. There are also some code search
engines that allow users to query by specifying structural
constraints [9,10].
Gay et al. [2] use the Rocchio algorithm to incorporate
user feedback to improve the performance of concern local-
ization. They are interested in nding methods that need
to be xed given a defect report. In their approach, each
method and defect report is treated as a simple textual doc-
ument. Dierent from their work, we address a dierent
problem (we recommend code fragments from a short tex-
tual query) and propose a new and specialized query rene-
ment algorithm (instead of using Rocchio) which incorpo-
rates structural information and employs parameter tuning.Several existing studies in software engineering also em-
ploy active learning. Lucia et al. propose an approach that
adds an active learning layer on top of existing clone-based
bug detection tools to increase true positive rate [5]. Hayes
et al. use the Rocchio algorithm to improve the quality
of requirement tracing techniques which infer links between
two textual documents, e.g., high level to low level require-
ments [4].
6. CONCLUSION AND FUTURE WORK
In this paper we propose active code search, where a user
can provide feedback to code search engines and guide the
engines to improve the relevance of search results. We pro-
pose a renement engine that can take into consideration
user relevance feedback: based on a set of results whose rel-
evance feedback have been received, our engine enhances the
user original query and use it to update the search results
by reordering the potentially more relevant search results to
the top of the list for users to see. The renement process
can be repeated for a number of times until the list is ex-
hausted or the user decides to stop searching further down
the list. Our active code search technique imposes little ad-
ditional overhead on users, and can improve the relevance
of search results from any passive code search engine that
takes textual descriptions as user queries. We have evalu-
ated our approach on 70 queries and nd that our active
code search approach on average improves the eectiveness
of code search by 11.3% in terms of NDCG. We have also
compared our approach against Rocchio and nds that we
can improve it by 7.5%.
In this work we only apply our approach to Portfolio. We
plan to apply our approach to other passive code search tools
and show that the active code search paradigm can benet
those code search engines too.
7. REFERENCES
[1] W.-K. Chan, H. Cheng, and D. Lo. Searching connected
api subgraph via text phrases. In SIGSOFT FSE, 2012.
[2] G. Gay, S. Haiduc, A. Marcus, and T. Menzies. On the use
of relevance feedback in IR-based concept location. In
ICSM , 2009.
[3] S. Haiduc, G. Bavota, A. Marcus, R. Oliveto, A. D. Lucia,
and T. Menzies. Automatic query reformulations for text
retrieval in software engineering. In ICSE , 2013.
[4] J. Hayes, A. Dekhtyar, and S. Sundaram. Advanced
candidate link generation for requirements tracing: The
study of methods. In TSE, 2006.
[5] Lucia, D. Lo, L. Jiang, and A. Budi. Active renement of
clone anomaly reports. In ICSE , 2012.
[6] C. D. Manning, P. Raghavan, and H. Sch utze. Introduction
to Information Retrieval . Cambridge, 2008.
[7] C. McMillan, M. Grechanik, D. Poshyvanyk, Q. Xie, and
C. Fu. Portfolio: nding relevant functions and their usage.
InICSE , 2011.
[8] C. McMillan, D. Poshyvanyk, M. Grechanik, Q. Xie, and
C. Fu. Portfolio: Searching for relevant functions and their
usages in millions of lines of code. TOSEM, 22(4), 2013.
[9] S. Wang, D. Lo, and L. Jiang. Code search via
topic-enriched dependence graph matching. In WCRE ,
2011.
[10] X. Wang, D. Lo, J. Cheng, L. Zhang, H. Mei, and J. X. Yu.
Matching dependence-related queries in the system
dependence graph. In ASE, 2010.
682