Scaling Exact Multi-Objective Combinatorial Optimization
by Parallelization
Jianmei Guo, Edward Zulkoski, Rafael Olaechea, Derek Rayside, Krzysztof Czarnecki
University of Waterloo, Canada
Sven Apel
University of Passau, GermanyJoanne M. Atlee
University of Waterloo, Canada
ABSTRACT
Multi-ObjectiveCombinatorialOptimization(MOCO)isfun-
damental to the development and optimization of software
systems. We propose ﬁve novel parallel algorithms for solv-
ing MOCO problems exactly and eﬃciently. Our algorithmsrelyonoﬀ-the-shelfsolverstosearchforexactPareto-optimalsolutions, and they parallelize the search via collaborativecommunication, divide-and-conquer, or both. We demon-strate the feasibility and performance of our algorithms by
experiments on three case studies of software-system de-
signs. A key ﬁnding is that one algorithm, which we callFS-GIA, achieves substantial (even super-linear) speedupsthat scale well up to 64 cores. Furthermore, we analyzethe performance bottlenecks and opportunities of our par-allel algorithms, which facilitates further research on exact,
parallel MOCO.
Categories and Subject Descriptors
D.2.9[ Software Engineering ]: Management— software con-
ﬁguration management ; G.1.6 [Numerical Analysis ]: Op-
timization— constrained optimization
Keywords
Multi-objective combinatorial optimization; parallelization
1. INTRODUCTION
Multi-Objective Combinatorial Optimization (MOCO) ex-
plores a ﬁnite search space of feasible solutions and ﬁnds the
optimal ones that balance multiple (often conﬂicting) objec-
tives simultaneously. MOCO is a fundamental challenge in
many design and development problems in engineering andotherdomains. Forexample, inmobile-phonesystemdesign,one often has to choose between diﬀerent candidate designsthat tradeoﬀ multiplecompeting objectives,s u c ha sl o wc o s t
and high performance. Each candidate design (i.e., a feasi-
ble solution) involves a wide variety of design options, which
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributedfor pro ﬁt or commercial advantage and that copi es bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-publish, to post on servers or to redistribute to lists, requires prior speci ﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
ASE’14, September 15-19, 2014, Vasteras, Sweden.
Copyright 2014 ACM 978-1- 4503-3013-8/14/ 09 ...$15.00.
http://dx.doi.org/10.1145/2642937.2642971.we call features (e.g., enabling Video Calls ) [11], with respect
to a set of constraints (e.g., Video Calls requires Camera)a n d
quality attributes (e.g., the cost of enabling Video Calls ). In
the worst case, the search space of candidate designs grows
exponentially in the number of features. Exploring such a
huge search space is often beyond human capabilities and
makes optimal system design a very challenging task. Inthe same way, many software-engineering problems, such asarchitecture design [1], test data generation [23, 44], andproject planning [20], involve the same form of MOCO.
MOCO problems are mostly NP-hard [10]. To address
them, approximateapproachesthatdependmainly on meta-
heuristics
1have been advocated for years. In most cases,
they solve MOCO problems in an acceptable time, but theyﬁnd only near-optimal solutions, and often suﬀer from pa-rameter sensitivity (i.e., the accuracy of the found solu-
tions varies widely with the parameter settings of these ap-
proaches) [18]. In contrast, exact methods that scan allcandidate solutions one by one often take too long for large-scale problems, but they are accurate in ﬁnding all,exact
optimal solutions, which is desirable for those stakeholderswho never want to miss any optimal opportunity.
Parallel computingcarries outmultiplecalculations simul-
taneously on multiple processors [2]. It divides a large com-putingproblemintomultiplesmaller onesandsolves theminparallel, often with a signiﬁcant performance improvement.In the past, metaheuristics have been parallelized to addressMOCO problems eﬃciently [7, 39]. However, there are only
few parallel algorithms for exact MOCO [39].
We aim at exact, parallel approaches that solve MOCO
problems accurately and eﬃciently. As a baseline, we choose
theGuided Improvement Algorithm (GIA) [33], a general-
purpose, sequential algorithm for solving MOCO problems
exactly. GIA works with most oﬀ-the-shelf SAT (SATisﬁa-
bility), SMT (Satisﬁability Modulo Theories) [12], and CSP(Constraint Satisfaction Problem) [41] solvers. Accordingly,theﬁrstparallel algorithm wepropose, which wecall Parallel
GIA (ParGIA), performs multiple GIAs simultaneously andcollaboratively. In ParGIA, each processor runs a GIA with
a diﬀerent starting point in the search space; once a proces-
sor ﬁnds an optimal solution, it communicates the solutionto other processors so as to reduce duplicate searches.
1A metaheuristic is a strategy for exploring the search space
of aproblemusingavarietyof methodsthattypicallyconsist
of both a diversiﬁcation (i.e., mechanisms to explore thesearch space) and an intensiﬁcation (i.e., mechanisms thatexploit previously found solutions) procedure [6].
409
To further scale MOCO, we propose two parallel divide-
and-conquer algorithms, Objective Split GIA (OS-GIA) and
Feature Split GIA (FS-GIA) . OS-GIA geometrically divides
the search space of a MOCO problem into subspaces, and
then runs in parallel a GIA for each subspace. FS-GIA re-cursively partitions a MOCO problem into subproblems byselecting anddeselecting certainfeatures; thenitperforms in
parallel a GIA for each subproblem. Lastly, we propose two
hybrid parallel algorithms, OS-ParGIA andFS-ParGIA ,i n
which the GIA algorithm in OS-GIA and FS-GIA, respec-tively, is replaced with ParGIA.
We implemented our proposed algorithms and evaluated
them in a series of experiments on three case studies of
software-system design. Our empirical results demonstrate
the feasibility and performance of our parallel algorithms.In particular, FS-GIA shows a desirable scalability with anincreasing number of available processors, and it achieveseven super-linear speedups.
2
In summary, we make the following contributions:
•Algorithms. We propose ﬁve novel parallel MOCO
algorithms that search for exact optimal solutions us-ing oﬀ-the-shelf solvers, and that parallelize the searchvia collaborative communication, divide-and-conquer,or both.
•Implementation. We implement our proposed algo-
rithms and publish the source code and experimentaldata at http://gsd.uwaterloo.ca/epoal .
•Evaluation. We evaluate the performance and scala-
bility of our parallel algorithms and analyze their per-formance bottlenecks.
•Prospects. We open a new direction in scaling exact
MOCO algorithms and demonstrate the potential ofparallelization for exact MOCO.
2. PRELIMINARIES
First, we introduce MOCO by means of the example of
mobile-phonesystem design, including a set of features, con-straints, quality attributes, and objectives. Next, we de-scribe how tospecify features and theirconstraints in propo-sitional formulas and how to use solvers to acquire feasi-ble solutions. Finally, we describe the concepts of Pareto-
optimal solutions and present how GIA (our baseline, se-
quential algorithm) ﬁnds all Pareto-optimal solutions and,this way, solves MOCO problems exactly.
2.1 A Running Example
The design of a software system deployed on a mobile
phone can be characterized by the features supported by
the phone and the constraints deﬁned between the features.
The features represent various design options. For exam-ple, the phone’s software may optionally include support forfeature Video Calls . Figure 1 shows a design scenario of a
mobile-phone system, adapted from Benavides et al. [5].We organize and visualize its features and their constraints
using a feature model [26], a tree structure, in which each
node except the root (i.e., MobilePhone ) has one parent. Re-
lations between a feature and its group of child features are
2Speedup is deﬁned as SP=T1
TP,w h e r eT1is the sequential
execution time of a problem and TPis the parallel execu-
tion time of the same problem using Pprocessors. Linear
speedup is obtained when SP=Pand super-linear speedup
is achieved when SP>P.MobilePhone
OS
Android iOSCalls
Voice Video
cost=5 0
lat.=8 0cost=3 5.5
lat.=2 0cost=8 5.5
lat.=7 0.5cost=9 5
lat.=4 0Connectivity
USB Wiﬁ Camera Bluetoothfeature
quality attribute
mandatory
optional
And-group
Or-group
Alternative-group
Cross-tree constraints :Video requires Camera
Objectives :minimizing cost, minimizing latency
Figure 1: A sample MOCO problem arising in
mobile-phone system design
classiﬁed as And- (no arc), Or- (ﬁlled arc), and Alternative-
groups (arc). The members of an And-group can be eithermandatory (ﬁlled circle) or optional (empty circle). Cross-tree constraints comprise requires and excludes relations be-tween features (e.g., Videorequires Camera).
Stakeholders can customize the mobile-phone system by
selecting features, thereby deriving diﬀerent design variantsthat have diﬀerent quality attributes (e.g., cost and latency).
Each feature may have an inﬂuenceon the quality attributesof a feasible system design that can be implemented andmeasured [17]. For example, selecting (or deselecting) fea-
ture Bluetooth in Figure 1, increases (or reduces) the cost by
50 and the latency by 80 in the ﬁnal mobile-phone system.
The quality attributes of a feasible system design can be cal-culated by aggregating the quality attributes of all selectedfeatures and feature interactions involved [38]. In this pa-per, we assume that we already have the quality attributes
of features and potential feature interactions. For research
on measuring and inferring the quality attributes, we referthe interested reader elsewhere [16, 38, 42].
Among the many feasible design variants, stakeholders of-
ten desire the optimal one that can simultaneously meetmultiple design objectives, such as minimizing cost and min-
imizing latency. This is a typical MOCO problem. However,the objectives are often conﬂicting. For example, a mobile-phone system often achieves a lower latency only by raisingits cost (e.g., using a larger cache). Thus, a MOCO problemusually has a set of optimal solutions, not only a single one.Finding all optimal solutions accurately and eﬃciently is a
major challenge, which we address in this paper.
2.2 Speci ﬁcation and Validation
The solutions to a MOCO problem must be feasible, i.e.,
satisfying the constraints deﬁned between features. To this
end, we specify all features and their constraints in proposi-tional formulas, and then we use oﬀ-the-shelf SAT, SMT, or
CSP solvers to return feasible solutions. Note that combina-
torialoptimization, eithersingle-objectiveormulti-objective,explores a ﬁnitesearch space, and each feature ranges over
a ﬁnite domain [10]. In general, a ﬁnite-domain featurecan be converted to a ﬁnite set of Boolean-domain features[8]. Here, we use only Boolean-domain features to describe
MOCO problems. In a nutshell, we represent a feature as
410Table 1: Feature-selection constraints in propo-
sitional logic ( Prepresents a parent feature and
C1,...,C nare its child features; M⊆{1,...,n}denotes
the mandatory features by their indices in an And-
group;F1andF2denote arbitrary features)
Type Propositional Formulas
Mandatory C1↔P
Optional C1→P
And-group ( P→/logicalandtext
i∈MCi)∧(/logicalortext
1≤i≤nCi→P)
Or-group P↔/logicalortext
1≤i≤nCi
Alternative-group ( P↔/logicalortext
1≤i≤nCi)∧/logicalandtext
i<j(¬Ci∨¬Cj)
Requires F1→F2
Excludes ¬(F1∧F2)
a Boolean decision variable. If a feature is supported by a
system design, then its corresponding variable is assigned
true,a n d falseotherwise. Furthermore, we specify the
constraints between features in propositional formulas. For
example, the constraints deﬁned in feature models can beformulated in propositional logic, as summarized in Table 1[3].
Asolution is a selection of features, i.e., an assignment
of value trueorfalseto the decision variable of each fea-
ture. A feasible solution is a validselection of features that
respects all constraints deﬁned between features. Using oﬀ-the-shelf solvers, we are able to check whether a potentialsolution satisﬁes all constraints, and thus is a feasible solu-tion [3]. For example, by using a SAT solver, we determine
that solution {OS,Android, Calls,Video,Connectivity ,Cam-
era}in Figure 1 is feasible, but solution {OS,Android, Calls,
Video,Connectivity ,Wiﬁ}is not, because the latter violates
the constraint: Videorequires Camera.
2.3 Pareto-Optimal Solutions and GIA
As a baseline, we use GIA (Guided Improvement Algo-
rithm)[33]todeterminewhetherafeasible solutionisPareto-
optimal and where to direct the search next. Figure 2 illus-
trates the search space of the MOCO problem of Figure 1, in
which each point indicates a solution.3Given multiple po-
tentially conﬂicting objectives, a solution is Pareto-optimal
if it is not dominated by any other solution. A solution dom-
inatesanother solution when it is better regarding at least
one objective and not worse regarding all the other objec-tives. According to the deﬁnition of Pareto dominance, a
solution partitions a search space into three areas: inferior,
superior,a n d equilibrium . For example, in Figure 2, the in-
ferior area (top-right) of solution S
5includes all solutions
that are dominated by S 5. The superior area (bottom-left)
contains all solutions that dominate solution S 5.A n y s o -
lution in the equilibrium areas (top-left and bottom-right)does not dominate solution S
5and, at the same time, is
not dominated by S 5. All Pareto-optimal solutions (ﬁlled
points) constitute the Pareto front .
GIA uses a solver to return a solution and then aug-
ments the constraints to search for solutions that domi-nate ones found already. Moreover, GIA incrementally ﬁndsPareto-optimal solutions duringcomputation and thusguar-antees thatall solutions yielded by the algorithm are Pareto-
3Unless otherwise speciﬁed, all solutions mentioned in the
following sections are feasible.S1
S2S3
CostminimizationLatency
minimizationS6
S5
S4S7
Figure 2: Solution S 5partitions the search space
into inferior (top-right), superior (bottom-left), and
equilibrium (top-left and bottom-right) areas
Algorithm 1: Guided Improvement Algorithm (GIA)
input:M,A,O
output:S
1S←∅
2EqC←true
3s←solveOne (M)
4whiles/negationslash=nulldo
5 whiles/negationslash=nulldo
6 s/prime←s
7 SupC←genSupC (s,M,A,O )
8 s←solveOne( M∧EqC∧SupC)
9S←S∪{s/prime}
10EqC←EqC∧genEqC(s/prime,M,A,O )
11 s←solveOne (M∧EqC)
12returnS
S6
S5
S4S7
S3
S2S1
(a)S6
S5
S4S7
S3
S2S1
(b)
Figure 3: (a) GIA returns solution S 4and calculates
the constraints of the superior area of S 4;( b )G I A
ﬁnds a Pareto-optimal solution S 1and calculates the
constraints of the equilibrium area of S 1
optimal, even if one terminates the execution halfway. Thisgains a competitive advantage over other exact methods [15,
29].
Algorithm 1 lists thepseudo-codeof GIA.GIA starts with
am o d e l M, a set of quality attributes A, and multiple ob-
jectivesO. The model Mspeciﬁes all features and their con-
straints as propositional formulas. The output is the Pareto
frontS. As demonstrated in Figure 3a, GIA ﬁrst inputs a
411model into a solver and computes a solution S 4(Line 3 in
Algorithm 1). GIA calculates the constraints of the superior
area of solution S 4(grey area in Figure 3a) and addsthem to
the solver for ﬁnding another solution dominating S 4(Lines
7–8). A Pareto-optimal solution has been found when thereis no solution in its superior area. As shown in Figure 3b,solution S
1is marked as Pareto-optimal (ﬁlled). Next, GIA
calculates the constraints of the equilibrium area of solutionS
1(grey areas in Figure 3b) and adds them to the solver for
ﬁnding other Pareto-optima l solutions (Lines 10–11).
3. PARALLEL ALGORITHMS
In this section, we present ﬁve novel parallel algorithms
for exactly solving MOCO problems. We use the exam-ple of Figures 1 and 2 to explain them. The input of eachalgorithm includes a model Mspecifying features and con-
straints, a set of quality attributes A, multiple objectives O,
and the number of available processors P. The output is the
Pareto front S.
3.1 ParGIA
ParGIA uses Pprocessors to perform PGIAs simultane-
ously and collaboratively over the search space of a given
MOCO problem. Each processor runs a GIA instance to
search for Pareto-optimal solutions and then communicatesthe constraints of the found solutions to all other processors.Algorithm 2 lists the pseudo-code of ParGIA. Figure 4 illus-trates the search process of ParGIA using two processors.Processor P
1inputs a model into a solver and computes
as o l u t i o nS 7;P 1keeps searching until it ﬁnds a Pareto-
optimal solution S 1. Meanwhile, processor P 2starts with
as o l u t i o nS 4and then ﬁnds another Pareto-optimal solu-
tion S 2. If processors P 1and P 2ﬁnd S 1and S 2at exactly
the same time, they communicate to each other the con-straints of the equilibrium areas of S
1and S 2(Lines 11–12
in Algorithm 2). Next, both processors search the com-
binedequilibrium area (grey areas in Figure 4b), looking
for other Pareto-optimal solutions. However, in most cases,the search processes of processors P
1and P 2are not syn-
chronous. For example, processor P 1may ﬁnd solution S 1
before processor P 2reaches solution S 2. In this case, pro-
cessor P 1communicates the constraints of the equilibrium
area of S 1to processor P 2, but it does not receive any con-
straintsfromP 2. Subsequently,processorP 1keepssearching
for other Pareto-optimal solutions in the equilibrium areaof S
1,w h i c hm a yc a u s et h e overlapping search of solution
S2that will be found by processor P 2later. Following the
idea of optimistic parallelism [27], ParGIA does not control
or avoid potentially overlapping search of the same Pareto-optimal solutions. But, after all processors ﬁnish searching,ParGIA checks all found Pareto-optimal solutions and re-moves duplicates (Line 14).
3.2 OS-GIA and OS-ParGIA
OS-GIA geometrically divides the search space of a given
MOCO problem into Psubspaces and then simultaneously
conquersall subspacesusing Pprocessors. Ineach subspace,
a GIA instance is performed by one processor to search forPareto-optimal solutions. Algorithm 3 lists the pseudo-codeof OS-GIA. We adopt the idea of cone separation [7] to di-
vide the search space. First, OS-GIA converts all objectivesto be all minimizations or all maximizations (Line 2 in Algo-
rithm3). Notethatanobjectivetomaximize Xisequivalent
Algorithm 2: ParGIA
input:M,A,O,P
output:S
1S←∅
2foralli←1toPin parallel do
3EqC i←true
4 s←solveOne (M)
5 whiles/negationslash=nulldo
6 whiles/negationslash=nulldo
7 s/prime←s
8 SupC i←genSupC (s,M,A,O )
9 s←solveOne( M∧EqC i∧SupC i)
10 S←S∪{s/prime}
11 forj←1toPdo
12 EqC j←EqC j∧genEqC(s/prime,M,A,O )
13 s←solveOne (M∧EqC i)
14S←postprocess (S)
15returnS
S6
S5
S4S7
S3
S2S1P1
P2
(a)S6
S5
S4S7
S3
S2S1P1
P2
(b)
Figure 4: (a) ParGIA ﬁnds two solutions S 7and S 4
simultaneously using two processors and calculates
the constraints of the superior areas of the two so-lutions; (b) ParGIA ﬁnds two Pareto-optimal solu-tions S
1and S 2and calculates the constraints of the
combined equilibrium areas of the two solutions
to the objective tominimize −X. Withoutloss of generality,
we assume that all objectives involve minimizations, as the
example of Figures 1 and 2. Then, OS-GIA determines areference point (the crossed point in Figure 5a) whose value
regarding each objective is suﬃciently large, such that all
solutions are located in its superior area.
4
Ifthegiven MOCO problemhasmore than two objectives,
OS-GIA projects the search space onto a bi-dimensionalplane formed by two chosen objectives (Line 3). Next, OS-GIA divides theprojective plane into conesby, starting from
the reference point, dividing the 90
◦angle encompassing the
superior area of the reference point into Pequal parts (Lines
4–5). Figure 5a shows the result of dividing the projectiveinto two cones; each cone has a 45
◦angle starting from the
reference point. Finally, OS-GIA applies a GIA to each cone
to search for Pareto-optimal solutions (Line 7).
The overhead of geometric decomposition is very small.
However, choosing the objectives that determine the pro-
jective plane and cones plays a critical role in the eventual
4If all objectives are maximizations, the reference must be
small enough regarding all the objectives.
412P1
P245◦
minimizationminimization
(a)P1,P2
P3,P445◦
minimizationminimization
(b)
Figure 5: After geometrically dividing the search
space of Figure 2 into two cones, (a) OS-GIA per-
forms a GIA instance using one processor in eachcone, and (b) OS-ParGIA performs a ParGIA in-stance using two processors in each cone
Algorithm 3: OS-GIA
input:M,A,O,P
output:S
1S←∅
2O←preprocess (O)
3O2←chooseTwo( O)
4forj←1toPdo
5Mj←objectiveSplit (M,A,O 2,P,j)
6forallj←1toPin parallel do
7Sj=GIA(Mj,A,O)
8S←S∪Sj
9S←postprocess (S)
10returnS
Algorithm 4: OS-ParGIA
input:M,A,O,P
output:S
1S←∅
2Q←P/2
3O←preprocess (O)
4O2←chooseTwo( O)
5forj←1toQdo
6Mj←objectiveSplit (M,A,O 2,Q,j)
7forallj←1toQin parallel do
8Sj=ParGIA(Mj,A,O,2)
9S←S∪Sj
10S←postprocess (S)
11returnS
workload of each processor for each cone. In the worst case,a poor selection of the two objectives may cause all solutionsto fall into one cone of theplane, which results in one proces-sor overloaded and the others idle. To balance the workload
of diﬀerent processors, we follow a straightforward intuition
of choosing the two objectives that have the ﬁrst and sec-ond largest value ranges to form a large projective plane, inwhich all solutions are distributed as evenly as possible.
Note that the Pareto-optimal solutions found by each pro-
cessor are localto that processor’s cone. Moreover, a solu-
tion that is locally Pareto-optimal in a cone may not beglobally Pareto-optimal in the entire search space. There-
fore, after all processors ﬁnish searching, OS-GIA collects all
local Paretofronts foundbyeachprocessor (Line8), removesduplicate solutions, and calculates the globalPareto front in
the search space (Line 9). Such calculation is straightfor-ward and usually takes little time.
OS-ParGIA is a hybrid of OS-GIA and ParGIA. Algo-
rithm 4 lists the pseudo-code of OS-ParGIA. As shown inFigure 5b, the key diﬀerence between OS-ParGIA and OS-GIA is that OS-ParGIA searches each cone using a ParGIAinstance with two processors instead of using a GIA instancewith one processor (Line 8 in Algorithm 4). Moreover, theinputPofOS-ParGIAisdoublethenumberofdividedcones
(Line 2). Of course, one can perform ParGIA using morethan two processors per cone, but the performance gainsare not necessarily higher when using more processors, aswe demonstrate in Section 5.3.
3.3 FS-GIA and FS-ParGIA
FS-GIA divides a given MOCO problem into Psubprob-
lems and then simultaneously conquers all subproblems us-ingPprocessors. For each subproblem, a GIA instance
is performed by one processor to search for Pareto-optimalsolutions. The key idea of FS-GIA is to divide a MOCO
problemintosubproblemsofrelatively equal size ,w h e r ee a c h
subproblem is deﬁned by a partial feature selection that rep-
resents a subset of solutions. If a MOCO problem has N
solutions in total, FS-GIA partitions the set of Nsolutions
intoPsubsets of roughly equal size (ideally,
N
Psolutions).
Subsequently, FS-GIA uses the GIA algorithm to search for
Pareto-optimal solutions among each subset. The approach
has the potential to signiﬁcantly reduce the number of solu-tions that each processor has to explore (by up to a factor ofP), leading to a signiﬁcant performance improvement. Al-
gorithm 5 lists the pseudo-code of FS-GIA.
A major challenge of FS-GIA is to accurately and quickly
calculate the number of all solutions selecting or deselect-ing a certain feature. This reduces to the well-known #SAT
problem [40], where the goal is to count the number of sat-
isfying variable assignments #( x) for a given propositional
formula x. Although this is generally a hard problem, ex-
isting #SAT solvers, such as sharpSAT , which we use for
FS-GIA,
5are capable of quickly calculating the exact num-
ber of solutions for the propositional formulas of our casestudies, as we demonstrate in Section 5.4.
Toacquiresubproblemsasequal-sizedaspossible, FS-GIA
recursively chooses an appropriate feature and partitions
each MOCO subproblemintoasubsubproblemthatincludesthe feature and a subsubproblem that does not (Lines 2–4in Algorithm 5). As shown in Figure 6, FS-GIA starts withthe input model Mof a given MOCO problem. FS-GIA ex-
haustively tests every feature fand computes the numberof
solutions for submodels M∧fandM∧¬f(i.e., the number
of valid assignments when the corresponding variable of fis
trueorfalse). Then, FS-GIAchooses the best splitfeature
(e.g.,f
1in Figure 6) to divide model Minto two submodels
such that:
|#(M∧f)−#(M∧¬f)|is minimal (1)
If multiple features satisfy the above equation, one is ran-domly chosen amongst them. Each submodel is divided re-
cursively by further split features, such as features f
2and
5https://github.com/marcthurley/sharpSAT .
413M
M∧f1Choose a feature fthat minimizes
|#(M∧f)−#(M∧¬f)|
M∧¬f1
M∧f1∧f2M∧f1∧¬f2 M∧¬f1∧f3M∧¬f1∧¬f3f1 ¬f1
f2 ¬f2 f3 ¬f3
Figure 6: FS-GIA recursively chooses features f1,f2,
andf3to divide a given MOCO problem into four
equal-sized subproblems
Algorithm 5: FS-GIA
input:M,A,O,P
output:S
1S←∅
2F←parallelPreprocess (M,P)
3forj←1toPdo
4Mj←generate (M,F,j)
5forallj←1toPin parallel do
6Sj=GIA(Mj,A,O)
7S←S∪Sj
8S←postprocess (S)
9returnS
Algorithm 6: FS-ParGIA
input:M,A,O,P
output:S
1S←∅
2Q←P/2
3F←parallelPreprocess (M,Q)
4forj←1toQdo
5Mj←generate (M,F,j)
6forallj←1toQin parallel do
7Sj=ParGIA(Mj,A,O,2)
8S←S∪Sj
9S←postprocess (S)
10returnS
f3in Figure 6. The above recursive division process forms
a binary decision tree, as shown in Figure 6. Note that the
submodels generated at the same levelo ft h et r e ea r ee q u a l -
sized and non-overlapping. They represent the subproblemswe need for FS-GIA. Hence, the number of generated sub-problems has to be a power of two, which equals to thenumber of assigned processors P, since each subproblem is
assigned one processor.
If we carry out the recursive division process of FS-GIA
sequentially, the dividing overhead increases linearly withP. As shown in Figure 6, FS-GIA has to choose three split
features and divide three times to acquire four subproblemsfor four processors. To reduce the dividing overhead of FS-GIA, we parallelize the preprocessing of FS-GIA (Line 2 inAlgorithm 5) to choose split features simultaneously. For
example, in Figure 6, the preprocessing tasks of choosingfeatures f
2andf3can be run simultaneously. In general,
subproblems with the same number of split-feature clauses
can be further divided in parallel: in step i,2i−1subprob-
lems are split roughly in half, producing 2ismaller subprob-
lems. Thus, the overheadof partitioning themodelincreaseslogarithmically withP.
FS-ParGIA is a hybrid of FS-GIA and ParGIA. Algo-
rithm 6 lists the pseudo-code of FS-ParGIA. The key dif-ference between FS-ParGIA and FS-GIA is that, for eachsubproblem, FS-ParGIA performs a ParGIA using multipleprocessors instead of a GIA using one processor. For exam-ple, Algorithm 6 (Lines 2 and 7) addresses each subproblemusing two processors. Hence, the input Pof FS-ParGIA is
double the Pof FS-GIA, when addressing the same num-
ber of subproblems. As with OS-GIA and OS-ParGIA, thePareto fronts found by FS-GIA and FS-ParGIA are localto the corresponding subproblems. Thus, a post-processingstep(Algorithm5, Line8, andAlgorithm6, Line9)identiﬁeswhich locally Pareto-optimal solutions make up the global
Pareto front with respect to the entire search space. This
step takes proportionately little time.
4. IMPLEMENTATION
We implemented our parallel algorithms using Python
2.7 and its multiprocessing package. The multiprocess-
ingpackage eﬀectively steps aside the issue of Global Inter-
preter Lock by using subprocesses instead of threads.6It
allows programmers to fully leverage multiple processors ona given machine, in which each spawned subprocess is as-
signed a processor.
Following the idea of optimistic parallelism [27] and re-
ducing the communication cost for ParGIA, we avoid using
any synchronization primitives, such as locks, but we usemessage-passing mechanisms, such as queues, which provide
a process-safe communication channel. Thus, each processor
in ParGIA has its own queue to manage the constraints of
the Pareto-optimal points found by itself, and then commu-nicates the constraints to the queues of other processors.
A recent study [35] shows that SMT might be the most
eﬃcient reasoning formalism in checking model properties,
compared to CSP, Alloy [24], and Answer Set Programming
(ASP) [30]. SMT combines standard SAT with richer theo-ries, suchasequalityreasoning, linear arithmetic, bitvectors,andarrays[12]. Therefore, inourimplementation, wereasonabout MOCO problems in SMT solvers. In particular, weimplemented GIA and all ﬁve parallel algorithms with the
eﬃcient SMT solver Z3, developed at Microsoft Research.
7
5. EVALUATION
We conducted a series of experiments to evaluate the par-
allel algorithms we propose. We aim at investigating the
performance and scalability of each algorithm and at iden-tifying potential bottlenecks and opportunities.
5.1 Subjects
Weevaluatedourparallel algorithms onthreeMOCO case
studies from the domain of software-system designs. As
MOCO problems reported by industry [22, 25, 34] are not
6Global Interpreter Lock is the mechanism used by Python
interpreter to assure that only one thread executes Python
bytecode at a time.
7http://z3.codeplex.com .
414Table 2: Overview of subject systems
#Features #Solutions #Objectives
SAS 35 5184 7
Web Portal 44 2120800 4
E-Shop 290 5.02E+49 4
available publicly, we resorted to using three case studies
from existing literature [14, 37] as subjects. The ﬁrst sub-
ject,SAS,optimizesthearchitectureofareal-worldsituation-
awareness system that deploys personnel in emergency re-sponse scenarios [14]. The original authors collected seveninteger and ﬂoat quality attributes for each feature. Ac-cordingly, SAS has seven objectives: minimizing cost, maxi-
mizing reliability, minimizing battery usage, minimizing re-
sponse time, minimizing ramp-up time, minimizing develop-ment time, and minimizi ng deployment time.
Web Portal andE-Shop are two product-line design
models available at the SPLOT website [31], which is a pop-
ular repository of software-system models used by many re-
searchers. Sayyad et al. [37] extended the two models andformulated corresponding MOCO problems by adding inte-ger and ﬂoat quality attributes for each feature and deﬁninga set of objectives. They randomly generated quality at-tributes resembling real-world project characteristics [21].
We use the same data and target the same objectives they
deﬁned for both case studies: minimizing cost, minimizingthe number of defects, maximizing the number of oﬀeredfeatures, and maximizing the number of features that wereused before.
As listed in Table 2, these case studies cover a reasonable
spectrum of MOCO problems with diﬀerent characteristics:diﬀerent number of features (35 to 290), diﬀerent numberof solutions (3 to 49 orders of magnitude), diﬀerent num-ber of objectives (4 to 7), diﬀerent quality attributes (e.g.,cost, response time, battery usage, reliability, and softwaredefects), diﬀerent value types of quality attributes (integer
and ﬂoat), and diﬀerent optimization directions (minimiza-
tion and maximization). Furthermore, ﬁnding the Paretofront of these case studies takes diﬀerent time ranges (fromseconds to minutes and days).
5.2 Experimental Setup
We conducted our experiments on SHARCNET,w h i c hi s
a consortium of Canadian academic institutions that share anetwork of high-performance computers.
8To reduce ﬂuctu-
ations in measurements caused by diﬀerent hardware envi-ronments, we performed all measurements on the same clus-ter comprising 160 AMD Opteron cores each at 2.2GHz
and a total of 640GB RAM. However, we were not able tooccupy all resources (e.g., cores and memory) on the clus-ter at all times, because SHARCNET uses a priority queue
to schedule the jobs submitted to a cluster. The priorityof a job is ranked according to the resources requested bythe job. More resources requested result in lower priorityand more waiting time in the queue. Due to the limitationof resources and time, our experiments use up to 64 coressimultaneously for each job submitted to the cluster.
In our experiments, the independent variables are thesub-
ject systems, the evaluated algorithms, and the number of
8http://www.sharcnet.ca .assigned cores. We measured the execution time of each se-
quential and parallel algorithm ﬁnding the Pareto front of
a certain subject system, and we calculated the speedups ofeach parallel algorithm as the dependent variables.U n f o r -
tunately, we were not able to determine all Pareto-optimalsolutions of E-Shop using any of the algorithms in six days,
which is the maximum execution time allocated to a job
submitted to SHARCNET. Hence, we measured the num-
ber of Pareto-optimal solutions found by each algorithm in
six days as the dependent variable for E-Shop.
To reduce ﬂuctuations in the values of dependent vari-
ables caused by randomness (e.g., the random seeds usedby solvers to return a solution), we evaluated each combina-
tion of the independent variables 10 times. That is, for each
subject system, we executed each of the algorithms with thesame number of assigned cores 10 times, and we measuredthe resulting execution times. We report only the means ofthe execution times for analysis.
As a baseline, we performed sequential GIA on the three
subjects for performance comparison and speedup calcula-tion. On average, GIA takes 70.2 seconds to ﬁnd the Paretofront of SASand 228.8 minutes to ﬁnd the Pareto front of
Web Portal .F o r E-shop, GIA ﬁnds an average of only
one Pareto-optimal solution in six days.
5.3 Performance Comparison
In a ﬁrst set of experiments, we compare the performance
of our parallel algorithms. We aim at determining whichalgorithm has the best performance, is the most scalable,and is the most promising to solve large MOCO problemssuch as E-shop. To obtain results in reasonable time, we
use at most 16 processors simultaneously and apply eachalgorithm only to SASandWeb Portal .
Results. Figure 7 presents the speedups of each parallel
algorithm ﬁnding the Pareto fronts of SASandWeb Por-
talusing up to 16 processors. FS-GIAachieves super-linear
speedups when using two processors for SASand when us-
ing 2 to 16 processors for Web Portal . Furthermore, the
speedup of FS-GIA increases steadily and rapidly when the
number of available processors increases.
FS-ParGIA acquires super-linear speedups when using 8
to 16 processors for Web Portal . Using the same number
of processors, the speedup of FS-ParGIA is less than that
of FS-GIA. Also, the speedup of FS-ParGIA has a stableincreasing trend with the increasing number of processors,
but the speedup increasing rate of FS-ParGIA is lower than
that of FS-GIA.
For ParGIA, OS-GIA, and OS-ParGIA, we did not ob-
serve super-linear or linear speedups in either case study.The speedup of ParGIA increases steadily and slowly as the
number of processors increases, and then reaches a plateau
(e.g., when using 12 or more processors for SAS,o ru s i n g5
or more processors for Web Portal ).
The speedups of OS-GIA and OS-ParGIA ﬂuctuate con-
siderably. In some cases, the speedups of OS-GIA and OS-ParGIA are less than one, which means that the execution
time of OS-GIA and OS-ParGIA is even longer than the
execution time of the sequential algorithm GIA.
Discussion. FS-GIAreliesonasoundandeﬃcient#SAT
solver to control the overhead of dividing a given MOCOproblemintosubproblemsandtoguaranteethatallsubprob-lems are as equal-sized as possible. In the search space of
each subproblem, the number of solutions that FS-GIA ex-
415●●●●
1.52.02.53.03.54.04.55.05.56.06.57.0
2 3 4 5 6 7 8 9 1 01 11 21 31 41 51 6
#ProcessorsSpeedup●FS−GIA
FS−ParGIAOS−GIAOS−ParGIAParGIA
(a)SAS
●●●●
0123456789101112131415161718192021222324252627282930
23456789 1 0 1 1 1 2 1 3 1 4 1 5 1 6
#ProcessorsSpeedup●FS−GIA
FS−ParGIAOS−GIAOS−ParGIAParGIA
(b)Web Portal
Figure 7: Speedups of ﬁve parallel algorithms ﬁnd-
ing the Pareto fronts of (a) SAS and (b) Web Portal,u s i n gu pt o1 6p r o c e s s o r s
plores has been reduced substantially, which results in lower
workload of conqueringeach subproblem. Weareaware that
equal-sized subproblems may not be equally hard[19], that
is, solving each equal-sized subproblems may still need dif-ferentworkload. However, ourempiricalresultsdemonstratethat FS-GIA eﬀectively balances the overhead of the divide-
and-conquer scheme and gains even super-linear speedups.
In both case studies, FS-GIA achieves the best performance.More importantly, FS-GIA presents a desirable scalabilitywhen the number of available processors increases from 2 to16, and the size of case studies augments from SAStoWeb
Portal.
FS-ParGIA uses the same divide-and-conquer approach
as FS-GIA. For each divided subproblem, FS-ParGIA per-forms a ParGIA using two processors, while FS-GIA runs aGIA using one processor. When using the same number ofprocessors, FS-GIA works more eﬃciently than FS-ParGIA.However, when addressing the same number of subprob-
lems, FS-ParGIA usually works more eﬃciently than FS-GIA. For example, working with the same 4 subproblems ofWeb Portal , FS-ParGIA using 8 processors gains a higher
speedupthanFS-GIAusing 4 processors. Therefore, thereis
a trade-oﬀ of using more processors for divide-and-conqueror for collaborative communicationin eachsubproblem. Ourempirical results show that it is worthwhile to spare moreprocessors to perform more equal-sized subproblems instead
of more GIA instances in each subproblem, especially when
the number of available processors is limited.
ParGIA implements collaborative communication using
message passing. However, due to the latency of messagepassing, a processor may already start searching an overlap-ping area before receiving the information about that area
from other processors. The problem of overlapping searches
may get worse when using more processors. According toour empirical results, ParGIA suﬀers from a performancebottleneck when running a large number of GIA instancessimultaneously, and its speedup does not scale well with theincreasing number of processors.
OS-GIA and OS-ParGIA geometrically divide the search
space of a given MOCO problem into cones by projective-plane selection and cone separation. However, it is hard toforesee how solutions are distributed in the global searchspace and in the projective plane. Hence, the algorithmscannot guarantee that the workload of conquering each cone
(i.e., ﬁnding Pareto-optimal solutions in each cone) is eﬀec-
tively balanced among processors, even though the cost ofdividing the original MOCO problem (i.e., obtaining cones)is trivial. Unbalanced loads among processors gives riseto performance ﬂuctuations when using OS-GIA and OS-
ParGIA, as demonstrated by our experiments. In the worst
case, ﬁnding the local Pareto front in a cone may take moretime than ﬁndingthe global Pareto front in the entire searchspace. This can happen when the number of locally Pareto-optimal solutions in a cone is higher than the number ofglobally Pareto-optimal solutions in the entire search space.
5.4 Scalability of FS-GIA
In a second series of experiments, we further explore the
scalability of FS-GIA, as it performed best in the ﬁrst setof experiments. We applied FS-GIA to all three case stud-ies, using up to 64 processors simultaneously. We measuredthe preprocessing time of FS-GIA (i.e., the time of dividinga given MOCO problem into roughly equal-sized subprob-
lems). Then, we analyzed the impact of the preprocessing
time on the scalability of FS-GIA.
Results. Figure 8a shows the speedups of FS-GIA ﬁnd-
ing the Pareto front of SASu s i n gu pt o6 4p r o c e s s o r s .F o r
the normal execution time including the preprocessing (solid
line), we observe that FS-GIA has a stable increasing trend
of speedups when using 2 to 64 processors, but the increas-ing rate signiﬁcantly slows down when using more than 32processors. In contrast, if we ignore the preprocessing time(dashed line), then the speedup of FS-GIA maintains a sta-ble and rapid increasing trend with the increasing number
of processors.
ForWeb Portal , as shown in Figure 8b, the prepro-
cessing time has little impact on the speedup of FS-GIA.
Regardless of whether the preprocessing time is included inthe execution time of FS-GIA, F S-GIA reaches super-linear
speedups that scale well to any number of processors from
2t o6 4 .
416●●●●●●
2468101214161820222426283032343638404244
24 8 1 6 3 2 6 4
#ProcessorsSpeedup●with Preprocessing
without Preprocessing
(a)SAS
●●●●●●
048121620242832364044485256606468
24 8 1 6 3 2 6 4
#ProcessorsSpeedup●with Preprocessing
without Preprocessing
(b)Web Portal
Figure 8: Speedups of FS-GIA (with and without
preprocessing) using up to 64 processors to ﬁnd the
Pareto front for SAS and Web Portal
ForE-Shop, FS-GIA ﬁnds more Pareto-optimal solutions
when using more processors. This is in contrast to the single
Pareto-optimal solution found by the sequential algorithmGIAoverthecourseof six days. Ifwedeterminethespeedupin terms of the number of Pareto-optimal solutions found insix days, as shown in Figure 9, FS-GIA still provides super-
linearspeedupsinallcases(rangingfrom 2to64processors).
Furthermore, the preprocessing time has almost no impacton the speedups of FS-GIA.
Discussion. The preprocessing time of FS-GIA reﬂects
the overhead of dividing a given MOCO problem into sub-problems. As explained in Section 3.3, the preprocessing
time of FS-GIA increases logarithmically with the number
ofgeneratedsubproblems(i.e., equalto PwherePisapower
of two). Our empirical results show that we can eﬀectivelycontrol the dividing overhead of FS-GIA. As listed in Ta-ble 3, the preprocessing of FS-GIA takes 6.6, 6.0, and 34.8seconds to generate 64 equal-sized subproblems for SAS,
Web Portal ,a n d E-Shop.●●●●●●
512234190
24 8 1 6 3 2 6 4
#Processors#Solutions●with Preprocessing
without Preprocessing
Figure 9: Number of Pareto-optimal solutions found
by FS-GIA (with and without preprocessing) usingup to 64 processors in six days for E-Shop
For small MOCO problems, such as SAS,e a c hd i v i d e d
subproblem is small enough that it can be solved in sec-onds. In this case, the impact of the preprocessing time onthe speedup of FS-GIA is non-negligible. As listed in Ta-
ble 3, FS-GIA takes 6.6 seconds to divide the MOCO prob-
lem of SASinto 64 subproblems, whereas all subproblems
can be solved simultaneously in 1.6 seconds. Thus, 80.5% ofthe execution time of FS-GIA is spent on the preprocessingstage. However, for large MOCO problems, such as Web
Portal orE-Shop, the cost of conquering each subproblem
is far more than the cost of dividing these MOCO problemsintosubproblems. Inthiscase, thepreprocessing timeofFS-GIA has little impact on the speedups of FS-GIA, and thespeedup of FS-GIA scales well with the increasing numberof processors.
5.5 Threats to Validity
To increase internal validity, we use either standard or
straightforward techniquestoimplementouralgorithms. Wechose an exact, general-purpose algorithm GIA as a base-line to solve MOCO problems sequentially. We rely on thestandard solver sharpSAT to choose appropriate features
for FS-GIA and FS-ParGIA generating equal-sized subprob-lems. In ParGIA, we implement the collaborative commu-nication between processors using simple message passing.In OS-GIA and OS-ParGIA, we use an intuitive heuristic tochoose two objectives with the ﬁrst and second largest value
ranges to form a large projective plane for cone separation.
However, we cannot guarantee that the Pareto-optimal so-lutions are distributed evenly among the cones, and thustheparallelism in OS-GIA and OS-ParGIA suﬀers from unbal-anced workloads among the processors.
To avoid the misleading eﬀects caused by random ﬂuctua-
tion in measurements, we executed each algorithm 10 timeson each case study, for each hardware conﬁguration from 2to 64 processors, and we used the means of the measuredexecution times in our analyse s of speedups and scalability.
To help ensure external validity, we evaluated our pro-
posed algorithms on three relatively large case studies from
the literature. The three case studies cover a reasonable
417Table 3: Impact of the preprocessing time of FS-GIA ( TFS−GIAprep) on the entire execution time of FS-GIA
(TFS−GIA) and speedups; P– the number of processors; s – seconds; m – minutes
P SAS Web Portal E-Shop
TFS−GIA(s)TFS−GIAprep(s)TFS−GIAprep
TFS−GIASpeedups TFS−GIA(m)TFS−GIAprep(m)TFS−GIAprep
TFS−GIASpeedups TFS−GIAprep(s)
2 32.7 1.1 3.4% 2.1 81.9 0.02 0.02% 2.8 5.8
4 20.1 2.2 10.9% 3.5 30.1 0.03 0.10% 7.6 11.6
8 14.5 3.3 22.8% 4.8 14.1 0.05 0.35% 16.3 17.4
16 10.4 4.4 42.3% 6.8 7.8 0.07 0.90% 29.5 23.2
32 8.7 5.5 63.2% 8.1 5.2 0.08 1.54% 44.1 29.0
64 8.2 6.6 80.5% 8.6 3.4 0.10 2.94% 67.3 34.8
spectrum of MOCO problems with diﬀerent characteristics,
such as diﬀerent sizes of problems, diﬀerent types of qualityattributes, and diﬀerent optimization directions. One sys-
tem has been used in a real-world scenario, and the other
two systems are from a popular repository used by manyresearchers. However, we are aware that the results of ourexperiments may not transfer to other systems. We expectthat especially large systems, such as Linux Kernel with
thousands of features [36], would beneﬁt from a combina-tion of our parallel algorithms and other methods, such asapproximation MOCO approaches.
6. RELATED WORK
In the ﬁeld of Search Based Software Engineering (SBSE),
multi-objective optimization has been identiﬁed as a ma-jor challenge for many software-engineering problems [20].Metaheuristics, such as multi-objective evolutionary algo-rithms, have been used to provide approximate solutions for
MOCO problems [37]. However, it is non-trivial for these
metaheuristics to guarantee the accuracy of approximate so-lutions. On the one hand, metaheuristics depend mainly ona number of heuristically-chosen metrics, such as Hypervol-ume Indicator [46] and Maximum Spread [45], to evaluatethe accuracy of approximate solutions. However, every met-
ric provides some speciﬁc, but incomplete, quantiﬁcations
of accuracy and can only be used eﬀectively under certainconditions [43]. On the other hand, metaheuristics usuallysuﬀer from parameter sensitivity [18]. They often demand aconsiderable time to tune parameters for ﬁnding reasonablyapproximate solutions [32]. These problems motivated us to
explore the feasibility of exact MOCO methods and improve
their performance as far as possible.
As the size and complexity of a software system increases,
not only exact methods but also metaheuristics became tootime-consuming to address large MOCO problems [1, 36].
Many parallel models for metaheuristics havebeen proposed
to solve MOCO problems eﬃciently, and they have beenevaluatedonawiderangeofacademicandreal-worldMOCOproblems in diﬀerent domains [7, 39]. Furthermore, paral-lelization of exact combinatorial-optimization methods, suchas Branch and Bound [28] and Dynamic Programming [4],
has been studied and implemented in multi-core environ-
ments [9]; however, it has been rarely addressed in the con-text of multi-objective optimization [39]. The only work weare aware is from Dhaenens et al. [13], who parallelized theexact solving of MOCO problems by geometrically splittingthe search space into cubes and evaluated their algorithm
on one case study; but their parallelization is not able toscale well up to only 10 processors. Thus, we propose a
diﬀerent geometric decomposition to partition the searchspace into cones. Moreover, we further scale the exact solv-
ing of MOCO problems by collaborative communication and
equal-sized partition using #SAT. Our parallel algorithmstarget all MOCO problems, as long as each solution can beabstracted as a combination of features.
7. CONCLUSION
MOCO has been used to solve many problems in software
engineering (e.g., architecture design [1], test data genera-tion [23, 44], and project planning [20]) and other domains(e.g., hybrid vehicle powertrain design [34], electric vehi-
cle battery design [25], and civil infrastructure repair plan-
ning [22]). We proposed ﬁve novel parallel algorithms forexact and eﬃcient solving of MOCO problems. Our algo-rithms searchfor Pareto-optimal solutions usingoﬀ-the-shelfsolvers and parallelize the search via collaborative commu-
nication and divide-and-conquer. We conducted a series of
experiments on three case studies of software-system design,coveringareasonable spectrumofMOCO problemswith dif-ferent characteristics. Ourempirical results demonstratethefeasibility and performance of our parallel algorithms.
The key ﬁndingfrom our experiments is that FS-GIAout-
performs all other proposed algorithms. FS-GIA partitionsa given MOCO problem into subproblems of relatively equalsize, which eﬀectively balances the workload among the par-allel processes. The result is that FS-GIAcan achievesuper-linear speedups. Moreover, the speedup of FS-GIA scaleswell up to 64 processors, and possibly beyond.
Our work opens a new direction in scaling exact MOCO
methods. Wehopethatourwork encouragesotherresearchersto reconsider the feasibility of exact MOCO methods andto try diﬀerent ways to scale them. Appropriate paralleliza-tion, especially given the increasing availability of multi-coresystems, is deﬁnitely a promising approach.
In future, we plan to further improve our parallel algo-
rithms and evaluate them on larger case studies. We expectthat a combination of our exact, parallel algorithms withexisting approximate approaches would further improve theaccuracy and performance of solving MOCO problems.
8. ACKNOWLEDGMENTS
This work has been partially supported by NECSIS, On-
tarioResearchFund-ResearchExcellenceProjectonModel-Based Development, and the German Research Foundation
(AP 206/4, AP 206/5, AP 206/6, and AP 206/7).
4189. REFERENCES
[1] A. Aleti, B. Buhnova, L. Grunske, A. Koziolek, and
I. Meedeniya. Software Architecture Optimization
Methods: A Systematic Literature Review. IEEE
Transactions on Software Engineering , 39(5):658–683,
2013.
[2] G. Almasi and A. Gottlieb. Highly Parallel
Computing . Benjamin-Cummings, 1989.
[3] D. Batory. Feature Models, Grammars, and
Propositional Formulas. In Proc. SPLC , pages 7–20.
Springer, 2005.
[4] R. Bellman. Dynamic Programming . Dover, 2003.
[5] D. Benavides, S. Segura, and A. Cort´ es. Automated
Analysis of Feature Models 20 Years Later: ALiterature Review. Information Systems ,
35(6):615–636, 2010.
[6] C. Blum and A. Roli. Metaheuristics in Combinatorial
Optimization: Overview and Conceptual Comparison.ACM Computing Surveys, 35(3):268–308, 2003.
[7] J. Branke, H. Schmeck, K. Deb, and M. Reddy.
Parallelizing Multi-Objective EvolutionaryAlgorithms: Cone Separation. In Proc. CEC , pages
1952–1957. IEEE, 2004.
[8] M. Carlsson, G. Ottosson, and B. Carlson. An
Open-Ended Finite Domain Constraint Solver. InProc. PLILP, pages 191–206. Springer, 1997.
[9] I. Chakroun, N. Melab, M.-S. Mezmaz, and
D. Tuyttens. Combining Multi-Core and GPU
Computing for Solving Combinatorial Optimization
Problems. J. Parallel and Distributed Computing ,
73(1):1563–1577, 2013.
[10] C. Coello, C. Dhaenens, and L. Jourdan.
Multi-Objective Combinatorial Optimization:Problematic and Context. In Advances in
Multi-Objective Nature Inspired Computing . Springer,
2010.
[11] K. Czarnecki and U. Eisenecker. Generative
Programming: Methods, Tools, and Applications .
Addison-Wesley, 2000.
[12] L. De Moura and N. Bjørner. Satisﬁability Modulo
Theories: Introduction and Applications. Comm.
ACM, 54(9):69–77, 2011.
[13] C. Dhaenens, J. Lemesre, and E. Talbi. K-PPM: A
New Exact Method to Solve Multi-Objective
Combinatorial Optimization Problems. European
Journal of Operational Research , 200(1):45–53, 2010.
[14] N. Esfahani, S. Malek, and K. Razavi. GuideArch:
Guiding the Exploration of Architectural SolutionSpace under Uncertainty. In Proc. ICSE, pages 43–52.
IEEE, 2013.
[15] M. Gavanelli. An Algorithm for Multi-Criteria
Optimization in CSPs. In Proc. ECAI , pages 136–140.
IOS, 2003.
[16] J. Guo, K. Czarnecki, S. Apel, N. Siegmund, and
A. W
/arrowhookleftasowski. Variability-Aware Performance
Prediction: A Statistical Learning Approach. In Proc.
ASE, pages 301–311. IEEE, 2013.
[17] J. Guo, J. White, G. Wang, J. Li, and Y. Wang. A
Genetic Algorithm for Optimized Feature Selection
with Resource Constraints in Software Product Lines.
J. Systems and Software , 84(12):2208–2221, 2011.[18] D. Hadka and P. Reed. Diagnostic Assessment of
Search Controls and Failure Modes in Many-Objective
Evolutionary Optimization. Evolutionary
Computation , 20(3):423–452, 2012.
[19] Y. Hamadi and C. M. Wintersteiger. Seven Challenges
in Parallel SAT Solving. AI Magazine , 34(2):99–106,
2013.
[20] M. Harman. The Current State and Future of Search
Based Software Engineering. In Proc. FOSE, pages
342–357. IEEE, 2007.
[21] M. Harman, S. Mansouri, and Y. Zhang. Search Based
Software Engineering: A Comprehensive Analysis andReview of Trends Techniques and Applications.Technical report, King’s College London TR-09-03,
2009.
[22] T. Hegazy and A. Elhakeem. Multiple Optimization
and Segmentation Technique (MOST) for Large-Scale
Bilevel Life Cycle Optimization. Canadian Journal of
Civil Engineers , 38:263–271, 2011.
[23] C. Henard, M. Papadakis, G. Perrouin, J. Klein, and
Y. L. Traon. Multi-Objective Test Generation forSoftware Product Lines. In SPLC, pages 62–71. ACM,
2013.
[24] D. Jackson. Software Abstractions: Logic, Language,
and Analysis. MIT, 2006.
[25] A. Jarrett. Multi-Objective Design Optimization of
Electric Vehicle Battery Cooling Plates ConsideringThermal and Pressure Objective Functions .M a s t e r
thesis, Queen University, 2011.
[26] K. Kang, S. Cohen, J. Hess, W. Novak, and
A. Peterson. Feature-Oriented Domain Analysis(FODA) Feasibility Study. Technical report, CMUSEI, SEI-90-TR-21, 1990.
[27] M. Kulkarni, K. Pingali, B. Walter,
G. Ramanarayanan, K. Bala, and L. Chew. OptimisticParallelism Requires Abstractions. In Proc. PLDI ,
pages 211–222. ACM, 2007.
[28] A. Land and A. Doig. An Automatic Method of
Solving Discrete Programming Problems.Econometrica , 28(3):497–520, 1960.
[29] M. Lukasiewycz, M. Glaß, C. Haubelt, and J. Teich.
Solving Multi-Objective Pseudo-Boolean Problems. InProc. SAT , pages 56–69. Springer, 2007.
[30] V. Marek and M. Truszczynski. Stable Models and an
Alternative Logic Programming Paradigm. In The
Logic programming paradigm: A 25-Year Perspective .
Springer, 1999.
[31] M. Mendonca, M. Branco, and D. Cowan. S.P.L.O.T. -
Software Product Lines Online Tools. In Proc.
OOPSLA Companion , pages 761–762. ACM, 2009.
[32] R. Olaechea. Comparison of Exact and Approximate
Multi-Objective Optimization for Software Product
Lines. Master thesis, University of Waterloo, 2013.
[33] D. Rayside, H.-C. Estler, and D. Jackson. A Guided
Improvement Algorithm for Exact, General Purpose,
Many-Objective Combinatorial Optimization.
Technical report, MIT-CSAIL-TR-2009-033, 2009.
[34] J. Ribau, J. Sousa, and C. Silva. Multi-Objective
Optimization of Fuel Cell Hybrid Vehicle Powertrain
Design - Cost and Energy. Technical report, SAE
2013-24-0082, 2013.
419[35] P. Saadatpanah, M. Famelis, J. Gorzny, N. Robinson,
M. Chechik, and R. Salay. Comparing the
Eﬀectiveness of Reasoning Formalisms for PartialModels. In Proc. MoDeVVa , pages 41–46. ACM, 2012.
[36] A. Sayyad, J. Ingram, T. Menzies, and H. Ammar.
Scalable Product Line Conﬁguration: A Straw toBreak the Camel’s Back. In Proc. ASE , pages
465–474. IEEE, 2013.
[37] A. Sayyad, T. Menzies, and H. Ammar. On the Value
of User Preferences in Search-Based SoftwareEngineering: A Case Study in Software Product Lines.InProc. ICSE, pages 492–501. IEEE, 2013.
[38] N. Siegmund, S. Kolesnikov, C. K ¨a s t n e r ,S .A p e l ,
D .B a t o r y ,M .R o s e n m ¨uller, and G. Saake. Predicting
Performance via Automated Feature-Interaction
Detection. In Proc. ICSE, pages 167–177. IEEE, 2012.
[39] E.-G. Talbi, S. Mostaghim, T. Okabe, H. Ishibuchi,
G. Rudolph, and C. Coello. Parallel Approaches for
Multiobjective Optimization. In Multiobjective
Optimization . Springer, 2008.
[40] M. Thurley. sharpSAT - Counting Models with
Advanced Component Caching and Implicit BCP. InProc. SAT , pages 424–429. Springer, 2006.[41] E. Tsang. Foundations of Constraint Satisfaction .
Academic, 1993.
[42] D. Westermann, J. Happe, R. Krebs, and
R. Farahbod. Automated Inference of Goal-Oriented
Performance Prediction Functions. In Proc. ASE ,
pages 190–199. ACM, 2012.
[43] G. Yen and Z. He. Performance Metric Ensemble for
Multiobjective Evolutionary Algorithms. IEEE
Transactions on Evolutionary Computation,
18(1):131–144, 2014.
[44] S. Yoo and M. Harman. Pareto Eﬃcient
Multi-Objective Test Case Selection. In ISSTA, pages
140–150. ACM, 2007.
[45] E. Zitzler, K. Deb, and L. Thiele. Comparison of
Multiobjective Evolutionary Algorithms: Empirical
Results. Evolutionary Computation , 8(2):173–195,
2000.
[46] E. Zitzler and L. Thiele. Multiobjective Evolutionary
Algorithms: A Comparative Case Study and theStrength Pareto Approach. IEEE Transactions on
Evolutionary Computation , 3(4):257–271, 1999.
420