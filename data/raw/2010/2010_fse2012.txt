Mining the Execution History of a Software System
to Infer the Best Time for Its Adaptation
Kyle R. Canavera
Computer Science Dept.
George Mason University
kcanaver@gmu.eduNaeem Esfahani
Computer Science Dept.
George Mason University
nesfaha2@gmu.eduSam Malek
Computer Science Dept.
George Mason University
smalek@gmu.edu
ABSTRACT
An important challenge in dynamic adaptation of a soft-
ware system is to prevent inconsistencies (failures) and dis-ruptions in its operations during and after change. Several
priortechniqueshavesolvedthisproblemwithvarioustrade-
oﬀs. All of them, however, assume the availability of de-tailed component dependency models. This paper presentsa complementary technique that solves this problem in set-tings where such models are either not available, diﬃcult to
build, or outdated due to the evolution of the software. Ourapproach ﬁrst mines the execution history of a software sys-
tem to infer a stochastic component dependency model ,r e p -
resenting the probabilistic sequence of interactions among
the system’s components. We then demonstrate how this
model could be used at runtime to infer the“best time”foradaptationofthesystem’scomponents. Wehavethoroughlyevaluated this research on a multi-user real world softwaresystem and under varying conditions.
Categories and Subject Descriptors
D.2.11 [Software Engineering ]: Software Architectures
General Terms
Algorithms, Design
Keywords
DataMining, DynamicAdaptation, ComponentDependency
1. INTRODUCTION
As engineers have developed new techniques to address
the complexity associated with the construction of modern-
day software systems, an equally pressing need has risen for
mechanisms that automate and simplify the management ofthose systems after they are deployed, i.e., during runtime.This has called for the development of (self-)adaptive soft-ware systems [13, 15]. However, the construction of such
systems has been shown to be signiﬁcantly more challenging
than traditional software systems [3, 16].
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copiesbear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
SIGSOFT’12/FSE-20, November 11–16, 2012, Cary, North Carolina, USA.
Copyright 2012 ACM 978-1-4503-1614-9/12/11 ...$15.00.One important challenge is the management of the run-
time change to avoid inconsistencies during and after the
adaptation. Informally, an inconsistent application state
is one from which the system progresses towards an error
state [14]. In a component-based software system, applica-
tion transactions change the state of the system. An appli-
cation transaction is deﬁned as a set of related interactions
among two or more software components. The importantobservation is that while a transaction is in progress, the in-
ternal state of the participating components may be mutu-
ally inconsistent [14]. To avoid inconsistencies, replacementof components should be delayed until the transaction hasended and the participating components have a stable state.
In their seminal work [14], Kramer and Magee developed
a technique, known as quiescence , that from a static com-
ponent dependency model of the system (e.g., UML Com-
ponent Diagram) calculates the components that have tobe halted (passivated) before a component can be safely
adapted. Relianceonastaticcomponentdependencymodel,
however, makes quiescence rather pessimistic in its analy-sis, which could lead to signiﬁcant delays and disruptions.This is an issue that has been tackled in two recent ap-
proaches, tranquility [22]and version-consistency [17], which
have showed that by leveraging the dynamic component de-
pendency model of the system (e.g., UML Sequence Dia-
gram) it is possible to become more reﬁned in the analysis
and thus reduce the unnecessary overhead.
All of these approaches, however, assume the availability
of an accurate component dependency model of the system.While this may be true in some cases, often such modelsare either not available or provide an inaccurate represen-
tation of the system’s evolving dependencies. For instance,
consider that the majority of existing open-source softwaresystems lack such models, and when not, the models arenot necessarily up-to-date with the system’s implementa-tion. Moreover, in emerging software systems, such as those
comprised of externally provided services, the dependencies
among the system’s components are constantly changing,making it diﬃcult to maintain such models.
Inthispaper, wepresentanovelapproachthatdetermines
the“best time”for adapting a system’s software componentsin settings where an accurate model of their dependenciesis not available. We deﬁne “best time” to be the time atwhich the adaptation of a given component results in nei-ther inconsistency (failure), nor signiﬁcant disruption to the
system. The underlying insight is that by collecting a soft-
ware system’s execution history for a suﬃciently long periodof time, it is possible to mine a stochastic component de-
1pendency model of the system. This model provides a new
kind of probabilisticinformation that has been lacking in the
models used for making adaptation decisions in the prior re-
search [17, 22]. We ﬁrst leverage data mining techniques toinfer a set of probabilistic rules representing the dynamiccomponent dependencies among a system’s software compo-nents. The rules are then used at runtime for determining
the likelihood of a component being in an appropriate state
for adaptation at a given point in time. Finally, by checkingour predictions against the actual behavior of the system,we are able to continuously reﬁne the dependency models to
the system’s evolving interactions.
Ourexperienceswiththoroughevaluationofthisapproach
in the context of a large distributed software system have
been very positive. The results have shown the ability to in-fer precise models that can be used to eﬀectively manage the
interruptions caused by adaptation. We have also developed
and evaluated a novel technique that prevents inconsisten-cies, even when our predictions are oﬀ.
The remainder of this paper is organized as follows. Sec-
tion 2 describes a software system used for illustration ofthe research and its evaluation. Section 3 provides the nec-essary background, while Section 4 motivates the researchin the context of prior work. Section 5 provides an overviewof our approach. Sections 6 to 8 delve into the details. Sec-
tion 9 presents the evaluation. The paper concludes with an
overview of prior research and avenues of future research.
2. ILLUSTRATIVE EXAMPLE
We illustrate the concepts using a software system, called
EmergencyDeploymentSystem(EDS)[18], andintendedforthe deployment and management of personnel in emergency
response scenarios. Figure 1 depicts a subset of EDS’s soft-
ware architecture, and in particular shows the dependencyrelationships among its components.
EDS is used to accomplish four main tasks: (1) track the
resourcesusingResourceMonitor, (2)distributeresourcesto
the rescue teams using Resource Manager, (3) analyze dif-
ferent deployment strategies using Strategy Analyzer, andﬁnally (4) ﬁnd the required steps toward a selected strat-egy using Deployment Advisor. Interested reader may ﬁnd
a more detailed description of EDS in [18]. It suﬃces to
Figure 1: Subset of the Emergency Deployment Sys-
tem’s software architecture.say that EDS is representative of a large component-basedsoftware system, where the components communicate by ex-
changing messages (events). In the largest deployment of
EDS to-date , it was deployed on 105 nodes and used bymore than 100 users [18].
Systems such as EDS are often deployed in highly unpre-
dictable and dynamic settings. Therefore, it is often de-
sirable to be able to adapt such systems at runtime to deal
with changes that may aﬀect the system’s functional or non-functional properties. However, such changes should occurin a manner that do not lead to inconsistency or signiﬁcant
disruption in the services provisioned to the users.
3. RESEARCH BACKGROUND
Kramer and Magee [14] showed that for a component
to remain in a consistent state during/after adaptation, it
should not be changed in the middle of a transaction. Theydeﬁned transaction to be exchange of event between two
componentsbywhichthestateofacomponentisaﬀected. Adependent transaction isinturnatransactionwhosecomple-
tion depends on the completion of consequent transactions.
We ﬁrst formally deﬁne and then illustrate these concepts
using a subset of transactions comprising EDS below. Fig-ure 2 shows the transactions corresponding to the strategy
analysis capability , which is only one of the use cases in EDS.
Anevent eisdeﬁnedasatripletuple e=<s r c ,d s t ,t i m e> ,
where srcanddstare identiﬁers for the source and destina-
tion components, and timeis the timestamp of its occur-
rence. Although an event is also likely to have a payload,it is not relevant to this line of research, and thus not mod-eled. In the EDS example of Figure 2, 12 events ( e
1-e12)a r e
depicted. In this area of research, it is assumed that events,including their source and destination, are observable.
A transaction tis deﬁned as a triple tuple
t=<s t a r t ,e n d ,R> , wherestartandendrespectively rep-
resent the events initiating and terminating the transaction
t,w h i l e Ris a set of transactions that subsequently occur
as a result of t.R/negationslash=∅when tis a dependent transaction
(e.g.,t
1,t3,a n dt4in Figure 2), and R=∅when tis an
independent transaction (e.g., t2,t5,a n dt 6in Figure 2).
Atop-level transaction tis a kind of transaction where
there is no other transaction xin the system such that
t∈x.R. In other words, a transaction is top-level if its oc-
currence is not tied to other transactions in the system. A
Figure 2: Transactions comprising strategy analysis
scenario of EDS.
2top-level transaction corresponds to the system’s use cases
(functional capabilities). For instance, t1in Figure 2 is a
top-level transaction, initiated in response to e1, which rep-
resents the user requesting a service from the system.
Replacing a component in the middle of a transaction
could place the system in an inconsistent state. Considera situation in which Strategy Analyzer component of Fig-
ure 2 is replaced after sending request event e
5, but before
receiving the response event e8. Since the newly installed
component does not have the same state as the old one, itmay not be able to handle response e
8and subsequently
initiate transaction t6via event e9, resulting in an inconsis-
tency and potentially the system’s failure.
Even if the component is stateless, inconsistency problems
may arise. Consider a stateless compression component thatcompresses and decompresses data using two interfaces that
are reverses of one another. Replacing this component with
onethatusesadiﬀerenttypeofcompressionalgorithminthemiddle of a transaction could break the system’s function-ality, since the decompression cannot be performed on data
that was compressed using the old component. By the same
reasoning, state transfer in the case of stateful componentsis not suﬃcient to address inconsistency due to adaptation.
Three general approaches to this problem have been pro-
posed: quiescence ,tranquility,a n d version-consistency.
Quiescence [14] is the established approach for safe adap-
tation of a system. A component is in quiescence and can beadapted if (1) it is not active, meaning it is not participat-
ing in any transaction, and (2) all of the components that
may initiate transactions requiring services of that compo-
nent are passivated. A component is passiveif it continues
to receive and process transactions, but does not initiate anynew ones. At runtime, the decision about which part of thesystem should be passivated is made using a static compo-
nent dependency model , such as that shown in Figure 1. For
instance, to change the Mapcomponent, on top of passivat-
ing itself, Weather Analyzer, Strategy Analysis KB, HQ UI,
Simulation Agent ,a n d Resource Manager components need
to be passivated as well, since those are the components thatmay initiate a transaction on Map.
While quiescence provides consistency guarantees, it is
very pessimistic in its analysis and, therefore, sometimesvery disruptive. Consider that the static dependency model
includes all possible dependencies among the system’s com-
ponents, while at any point in the execution of a softwaresystem only some of those dependencies take eﬀect. To ad-dress this issue, tranquility [22] proposes to use the dynamic
component dependency model of a system in its analysis, an
example of which is shown in Figure 2. Under tranquilitya component can be replaced within a transaction as long asit has not already participated in a transaction that it mayparticipate in again . For instance, under tranquility, Map
c o u l db er e p l a c e de i t h e rb e f o r ei tr e c e i v e se v e n t e
2or after
it sends event e7, but not in between.
A shortcoming of tranquility, as realized in [22], was lack
of support for handling dependent transactions. This issue
was addressed in version-consistency [17], which guarantees
ad e p e n d e n tt r a n s a c t i o ni ss e r v e db ye i t h e rt h eo l dv e r s i o nor new version of a component that is being changed.
4. MOTIV ATION AND OBJECTIVES
Similar to the prior research [17, 22], we believe using
static dependency models for achieving consistency to beoverly disruptive in most cases. However, unlike prior re-search, we do not assume the availability of dynamic com-ponent dependency models (e.g., UML Sequence Diagram)for the following reasons.
•Manually Intensive: Dependency models are not al-
ways available and do not come for free. To developthese models, one has to understand the internal logicof components, which is a manual, cumbersome pro-cess, specially if the developer of those models is notpart of the team that implemented those components.
•Dynamism and Evolution: Determining the dependen-
cies prior to system’s deployment in emerging and in-creasinglydynamicparadigms, suchasservice-oriented
and mobile domain, is diﬃcult. As the system evolves,
the internal logic of its components changes, makingthe manually constructed models inaccurate represen-tations of the system, which if used for making adap-tation decisions may break the system’s consistency.
Therefore, even when dependency models are avail-
able, keeping them up-to-date is a challenge.
•Non-determinism: Finally, and perhaps most im-
portantly, component dependencies are often non-
deterministic, i.e., a component depends on anothercomponent under some circumstances, but not oth-ers. The model depicted in Figure 2 is deterministic,since it assumes the transaction t
1always results in the
same exact sequence of subsequent events and trans-actions. No prior research has developed mechanismsfor ensuring consistency and managing disruption in anon-deterministic setting.
In this research we aim to infer the stochastic component
dependency model of the system. Such a model not only in-
fers the dynamic dependencies among the components (i.e.,information equivalent to that captured in Figure 2), but italso provides a probabilistic measure of the certainty withwhich events and transactions may occur. Thus, our ap-
proach does not compete with the prior research (i.e., tran-
quility and version-consistency), but rather paves the wayfor those techniques to be applicable in settings where dy-namic dependency models are not available.
To keep our approach widely applicable, we make mini-
mal assumptions about the available information from theunderlyingsystem. Theseassumptionsarethesameasthosemade in the prior research:
1.Black-Box Treatment : We assume the software com-
ponents’ implementation is not available. This allowsour approach to be applicable to systems that utilize
services or COTS components, whose source code is
not available. It also enables our approach to natu-rally support the evolution of software components.
2.Observability of Event :W ea s s u m et h a te v e n t sm a r k -
ing the interactions among the system’s componentsare observable. An event could be either a messageexchange or a method call, which could be monitoredvia the middleware facilities that host the components
or instrumentation of the communication links.
3.Observability of Transaction Duration : We assume
eventsstartandend, which as you may recall from
Section 3 indicate beginning and termination of a
transaction, to be observable. This is a reasonableassumption that has also been made by all prior re-search [14, 22, 17]. For instance, in the example of
3Figure 2, the HQ UIcomponent should be able to de-
termine and record the occurrence of dependent trans-
actiont1in terms of request e1, which corresponds to
the user clicking on a button on the GUI, and its ter-
mination via response e12, which corresponds to the
results to be displayed on the GUI.
Ourapproachmakesnofurtherpertinentassumptionsand
requires no additional information from the system. Basedon this minimal information, our objective is to infer thestochastic component dependency model of the system. The
crux of this is the ability to identify the causal relationship
among the transactions. In other words, our objective is todetermine the set Rfor every transaction occurring in the
system (recall the formal deﬁnition of transaction in Sec-tion 3). This is a challenging problem to solve by simply
monitoring the system, given that there may be multiple
concurrently running top-level transactions at any point intime using the same set of components. Moreover, compo-nents in our approach could act non-deterministically, pro-
ducing diﬀerent behaviors under diﬀerent conditions.
5. APPROACH OVERVIEW
We present a novel approach for automatically deriving
the stochastic component dependency model by mining the
execution history of the software system. The result of min-ing is a set of rules expressing the probabilistic relationship
among the occurrences of transactions in the system. This
set of rules represents our stochastic component dependencymodel. Given a set of active transactions in the system,these rules can be used to predict the probability with whicha component canbe changed at a point in timewithout jeop-
ardizing the system’s functionality, while minimizing the in-
terruptions. Additionally, by continuously monitoring thetransactions and the accuracy of predictions, the approachprovides the means to adjust the rules as new patterns of
interaction emerge.
Figure 3 provides an overview of our approach, consist-
ing of two complementary asynchronously running cycles:
Mining Rules andApplying Rules.
The Mining Rules cycle starts by processing the Event
Logof the system to construct a large number of Itemsets.
An itemset indicates the events that occur close in time.Itemsets are then passed through a data mining algorithmto derive Transaction Association Rules (TARs) relating the
relationship between transactions that are occurring in thesystem and those that may happen in the future. Sincemining may generate a large number of rules, some of whichmay be invalid and redundant, we prunethe generated rules
toarriveatasmallnumberofusefulrulesthatcanbeapplied
eﬃciently at runtime.
The Applying Rules cycle starts with the Track Active
Transactions activity that monitors the currently running
transactions in the system. Select Relevant TARs then uses
the information about currently active transactions to pick
a set of candidate TARs from the Rule Base for estimating
the usage probability of components. Update Predictions
uses candidate TARs to update the U s a g eP r e d i c t i o nR e g -
istry, which is a data structure that contains the up-to-date
usage predictions for the components in the system. The
usage prediction for each component is the probability thatthe component will imminently be used as a result of thetransactions running in the system. These predictions can
be calculated either continuously or on an as-needed basis.
Figure 3: Approach overview.
Finally, as indicated by C h e c kP r e d i c t i o nA c c u r a c y ,t h e
predictions are scrutinized at runtime, and if they go abovean unacceptable threshold, a new round of mining based on
the newly collected log of events is initiated. This allowsthe approach to incorporate changes due to how the soft-ware is used or its evolution into the mining process. In thefollowing sections, we describe the details of our approach.
6. MINING RULES
This section describes the Mining Rules cycle (recall Fig-
ure 3). This cycle runs asynchronously, separate from the
system’s execution, and potentially on a diﬀerent platform.
It may repeat throughout the system’s execution to adjustthe model to the evolving behavior of the software system.
6.1 Event Log
Mining operates on an Event Log of the system, which
represents an execution history of the system for a suﬃ-ciently long period of time to be truly representative of howthe system is used. Clearly our approach is not applicableto systems where such a history cannot be collected, or the
system’s past behavior is not indicative of its future, but we
believe most systems do not fall in this category. Since ourobjective is to infer the relationship among the transactions,we would like mining to operate on a representation that isin terms of transactions as opposed to events. As a result,
theEvent Log of the system is automatically processed to
determine all of the transactions that have occurred by pair-
ing thestartand theendevents for each transaction. Recall
from Section 3 that consistent with the prior work [14, 22,
17], weassumethesetypesofeventsareobservableandcould
be used to identify the occurrence of transactions. From thispoint forward, we will mainly focus on transactions, thoughthe reader should be aware of the relationship to the events.
6.2 Constructing Itemsets
The ﬁrst step to mining the relationship among the trans-
actions is to Construct Itemsets (see Figure 3). An itemset,
as in the data mining literature for association rule mining,is a set of items that have occurred together. In the context
ofourresearch, anitemset Iisasetoftransactionsthathave
occurred temporally close to one another at some particular
point during the execution of the system: I={t
1,t2,...,t n}.
The transaction records for the execution history of the
system are transformed into itemsets through a simple pro-
cess. A new itemset is formed for each top-level transaction,
4but not the transactions that those top-level transactions
initiate. A top-level transaction is automatically detected if
its beginning, end, or both do not fall within the beginning
and end of another transaction. All other transactions areplaced in the itemsets for the transactions whose beginningand end times fully surround the beginning and end timesof the present transaction.
In reference to Figure 2, a new itemset would be created
fort
1, as its beginning and end (determined by e1ande12)
do not fall within any other transactions. All the remainingtransactions t
2,t3,t4,t5,a n dt6are added to It1itemset as
follows: It1={t1,t2,t3,t4,t5,t6}.
Using this process, an entire segment of a software sys-
tem’s execution history can be transformed into a set ofitemsetsrepresentingtheoccurrenceoftransactionstogetherin time. Given a suﬃciently large usage history, the ap-
proachcompensatesforconcurrentlyrunningtop-leveltrans-
actions. Consider a version of the scenario depicted in Fig-ure 2 in which a second top-level transaction t
7overlapping
partially in time with t1starts and itself initiates a transac-
tiont8that falls wholly within the beginning and end times
of both t1andt7. The approach will include t8in both
It1andIt7. However, since transactions t1andt7are truly
independent, the false placement of t8inIt1is a random
event that is not likely to occur in a signiﬁcantly large num-
ber of itemsets, and thus safely ignored by the data-mining
algorithm using minimum frequency thresholds.
6.3 Deriving Rules
Several data mining approaches [12] can be used to per-
formlearningonthesetofitemsetsconstructedthisway. Wefound the association rule mining class of algorithms to bethe most suitable for our purposes. The output of an algo-
rithm of this type for our problem is a set of transaction as-
sociation rules (TARs) . TARs are probabilistic rules for pre-
dicting the occurrence of transactions as follows X− →Y:p.
A TAR states that the occurrence of set of transactions X
implies the occurrence of a set of transactions Ywith prob-
abilityp. As shown in Figure 3, TARs derived in this way
are eventually stored in the Rule Base for use during the
system’s adaptation at runtime.
For association rule mining algorithms, an appropriate
value for pis the conﬁdence of the implication X− →Y.
Conﬁdence is deﬁned as:
p=/parenleftBigg/summationdisplay
si/braceleftBigg
1i fX∈si∧Y∈si,
0o t h e r w i s e ./parenrightBigg/slashBigg/parenleftBigg/summationdisplay
si/braceleftBigg
1i fX∈si,
0o t h e r w i s e ./parenrightBigg
Conﬁdence is an appropriate metric for pin TARs be-
cause it provides a measure of the strength of the implica-
tionX− →Y. TARs with strong relations between XandY
have a high conﬁdence value, while TARs with weak rela-tions between XandYhave a low conﬁdence value.
Another metric that is commonly generated by data min-
ing algorithms during the learning phase is support:
s=/parenleftBigg/summationdisplay
si/braceleftBigg
1i fX∈si∧Y∈si,
0o t h e r w i s e ./parenrightBigg/slashBigg
NumberofItemsets
While support is not appropriate for the value of pin TARs,
it is useful in that it provides a measure of the frequencywith which XandYoccur together. As such, we use a
minimum support value during the mining phase in order
to ﬁlter out rare relationships that represent outliers in thegeneral usage of the system. Thus, the errors introduced
in itemsets due to concurrent execution of transactions inthe system (recall Section 6.2) can be ﬁltered out eﬀectively
using a minimum support and conﬁdence threshold.
While the mining algorithm in the Derive Rules activity
produces logically accurate TARs, it typically produces an
excessively large number of TARs, some of which are notuseful. As such, the generated rules must be pruned to make
them suitable for use at runtime. As shown in Figure 3,
theDerive Rules step terminates by passing the raw set of
generated TARs to Prune Rules.
6.4 Pruning the Rule Base
An excessively large number of TARs is produced as a
result of the Derive Rules activity, because we set the min-
imum conﬁdence for a TAR to be very small, i.e., we donot ﬁlter out many TARs based on the conﬁdence level. Wetake this approach contrary to many other applications of
associate rule mining because, while a TAR having a small
pexpresses less conﬁdence in the prediction than does a dif-
ferent TAR having a larger p, both predictions are accurate
and can be used in unison as explained in Section 7.3.
In addition, many of the unnecessary TARs are produced
because the data mining algorithm and its input (i.e., item-sets) do not fully incorporate all of the knowledge that wehave about the system. For instance, itemsets are unorderedand thereby the resulting TARs incorporate no ordering in-
formation. As a result, the mining algorithm produces an
excessively large number of TARs that are not useful.
Since we would like to use the rules at runtime, we need to
prune them to a subset of highly predictive rules that can beapplied eﬃciently at runtime. To that end, and as depicted
in Figure 3, the Derive Rules step terminates by passing
the raw set of generated TARs to Prune Rules . There are
three highly eﬀective heuristics that we have developed for
pruning the TARs.
(1) Redundant TAR Pruning Heuristic : Consider TARs
satisfying this pattern:
TAR
1:X1− →Y1:p1
TAR 2:X2− →Y2:p2
where (X2⊆X1)∧(Y1=Y2)∧(p1=p2)
In this scenario, TAR 1andTAR 2predict the same set of
transactions and at the same level of conﬁdence. However,the conditions for satisfying TAR
2is a subset of those for
TAR 1, i.e.,X2is a subset of X1. As will be explained in
Section 7.2, a TAR’s conditions are considered to be sat-
isﬁed, when the transactions comprising its left hand side
have been observed. Therefore, TAR 1andTAR 2predict
t h es a m ee x a c to u t c o m e ,e x c e p tTAR 2requires fewer con-
ditions to be satisﬁed. We can safely prune TAR 1,s i n c ei t
is redundant.
(2) Less Speciﬁc TAR Pruning Heuristic : Consider TARs
satisfying this pattern:
TAR 1:X1− →Y1:p1
TAR 2:X2− →Y2:p2
TAR 3:X3− →Y3:p3
where (X1=X2=X3)∧(Y1=Y2∪Y3)
In this scenario, TAR 1makes a composite prediction of
TAR 2andTAR 3. All three TARs are satisﬁed with the
observation of the same set of transactions X1=X2=X3.
However, because Y1=Y2∪Y3,TAR 1is a composite pre-
diction of the more speciﬁc predictions made by TAR 2and
TAR 3. Given the deﬁnition of conﬁdence and its use as the
5prediction value p, the prediction value p1forTAR 1will al-
ways be weaker (lower) than the prediction values of p2and
p3forTAR 2andTAR 3, respectively. As a result, TAR 1is
a less speciﬁc rule and can be pruned.
(3) Misordered TAR Pruning Heuristic : Wecanalsoprune
rules by incorporating our knowledge of what constitutes a
valid behavior. We can prune TAR:X− →Y:p, where
∃x∈X∧y∈Y:x.start.src =y.end.dst . In this kind of
TAR one of the predicted transactions in Yhas as its desti-
nation the source of one of the observed transactions in X.
Therefore, the TAR is useless because it predicts the use of
a component that must have already been used. It is impor-
tant to note that, while this type of TAR seems illogical andperhaps presumptively unlikely to be generated, the associ-ation rule mining algorithm and its input (i.e., itemsets) donot recognize any transaction ordering. Furthermore, these
types of TARs can be highly predictive and are very com-
mon. Essentially they predict that the transaction necessaryfor another transaction to occur will in fact occur with thattransaction. Therefore, this pruning step removes many use-
less rules and has the largest impact in our approach.
At the completion of this activity a small subset of gen-
erated rules remains, which is stored in the Rule Base and
used for runtime prediction of component usage.
7. APPLYING RULES
In this section, we describe the activities comprising the
Applying Rules cycle from Figure 3.
7.1 Tracking Active Transactions
Track Active Transactions step processes any observed
eventto.startandto.end, indicating the beginning and ter-
mination of transaction to, respectively. To that end, we use
a data structure, called top-level tracker, and represented as
setTLT, for each top-level transaction active (i.e., currently
running) in the system. The purpose of TLTs is to keep
account of the present transaction activity in the system.
Upon observing to.start, the state of TLTs is updated as
follows. If tois a top-level transaction, a new TLT is created.
But iftois not a top-level transaction, its identiﬁer is added
to all open TLTs, i.e., tois associated with every top-level
transaction that may have caused it. This is done because
there is no way of knowing which top-level transaction has
actually initiated this transaction. Upon observing to.end,
iftois not a top-level transaction, it is ignored. On the
other hand, if tois a top-level transaction, then the TLT
corresponding to tois closed.
Changes to TLTs impact the Usage Prediction Registry.
In the following subsections, we describe the process assum-ingt
o.starthas been observed, but revisit the situation in
whichto.endis observed before concluding.
7.2 Selecting the Relevant Rules
The updated TLTs are used to determine what new pre-
dictions can be made about the probability with which com-ponents will be used. All predictions of the system activitya r em a d eb yu s i n gt h eT A R ss t o r e di nt h eRule Base .W e
must determine what new TARs, if any, are implicated bythe observation of t
o.start.
To that end, we iterate over all TARs in the Rule Base .A
tar∈RuleBase can only be implicated by the observation
ofto.start,i ftoi sam e m b e ro fs e tX of that tar. That is
to say, we cannot make a new prediction based on the giventar, unless tocontributes to the prediction. If this criterion
is met, then we look to see if the taris satisﬁed by any open
top-leveltransactionastrackedbyTLTs. Fora tartobesat-
isﬁed, all transactions in Xmust have been observed during
the processing of at least one TLT. Furthermore, the tar’s
prediction (i.e., Y) should have new transactions other than
the ones that have already occurred during the processing ofthe satisfying TLT. Stated diﬀerently, the taris only consid-
ered to have a useful prediction if (1) all of its prerequisiteshave been seen, and (2) at least some of its predictions areunseen. If both of these conditions are met, then the taris
added to the set CTAR, which is a set of all new TARs that
are candidates for being applied at that given point in time.
The TLT that satisﬁes the conditions for presence of a
tarin CTAR is said to be a basisfor the application of that
tar. This basis information is tracked along with the tarand
used in the next stages.
7.3 Updating the Usage Prediction Registry
The next step is to apply the implicated TARs to up-
date the Usage Prediction Registries , represented as set UP.
Given a component c, there are typically more than a single
TAR predicting its usage probability uc∈UP.W h i l es o m e
may be due to the new observation to.start,o t h e r sm a yb e
due to the prior observations. Therefore, we must combinethe various pvalues from all of the satisﬁed TARS into a
single prediction value u
c.
Before describing how uccan be calculated, we need to de-
ﬁne three sets: (1) CTAR cis a set of candidate TARs that
are supposed to aﬀect a given component ca n dd e ﬁ n e da s
CTAR c={tar|tar∈CTAR∧(∃t∈tar.Y:t.start.dst =c)}.
These are the new TARs based on the observation to.start.
(2)ATAR cis the set of active TARs currently contributing
toucdue to observations made prior to to.start.( 3 ) F i n a l l y ,
PTAR c=CTAR c∪ATAR ci st h ec o m p l e t es e to fT A R s
that determine the new value of uc.
We can now describe how ucis calculated in ﬁve steps:
(1) Removing duplicate TARs: We do not need to recon-
sider atar∈CTAR c, which is already actively predicting
the usage of component c(i.e.,tar∈ATAR c). Therefore,
we remove any such tarfromCTAR c(i.e.,
CTAR c=CTAR c−ATAR c).
(2) Removing superseded TARs: A superseding relation-
ship occurs when we have TARs satisfying this pattern:
TAR 1:X1− →Y1:p1
TAR 2:X2− →Y2:p2
where (Y1=Y2)∧(X1⊆X2)
In this scenario, TAR 2predicts the same set of transactions
asTAR 1,h o w e v e r TAR 2makes use of more information
thanTAR 1and hence makes a more informed prediction.
Therefore, TAR 1is removed from its set (i.e., either CTAR c
orATAR c, depending on which one it came from).
(3) Selecting the best candidate: Even after removing the
redundant rules, we may still have some partially overlap-ping ones. Partially overlapping rules express the variousexecution paths that may eventually result in the use ofsame component. Consider the following two TARs:
TAR
1:{t1,to}−→{t3,tc}:p1
TAR 2:{t2,to}−→{t4,tc}:p2
where (tc.start.dst =c)
SinceTAR 1.Y/negationslash=TAR 2.Y, thesupersedingrelationshipcan-
not be used to remove one of the TARs. However, the obser-vation of a single t
o.startshould at most result in a single
6prediction for the component c. We use a heuristic and
choose the TAR with the highest pv a l u et ob et h eb e s tc a n -
didate. This TAR expresses the greatest risk that cwill be
used. After this step CTAR cmust have a single member.
(4) Trimming PTAR c:Analogous to the logic in the pre-
vious step, it is reasonable to expect each top-level transac-
tion to make a single prediction for a component c. When
there are more than one active top-level transactions, wec a n n o tk n o ww i t hc e r t a i n t yw h i c ht o p - l e v e lt r a n s a c t i o na c -tually initiated t
o.start. However, based on the number of
active TLTs (recall Section 7.1), we know how many top-
level transactions are active in the system when to.startis
observed. Therefore, we approximate by limiting the num-ber of TARs contributing to u
cto the number of top-level
transactions active at that point in time. As with the re-duction of CTAR
cin the previous step, we choose to be
conservative by keeping the TARs with the highest pvalues.
We remove the TARs with the lowest pvalue from PTAR c
until the size of PTAR cis equal to number of active TLTs.
(5) Combining the predictions: At this point, we let the
ATAR cto be equal to PTAR c. We can now recalculate uc
based on the updated ATAR c. Because there are no dupli-
cate, overlapping, or related TARs in ATAR c,w ec a l c u l a t e
ucby combining the prediction values from individual TARs
inATAR cas independent probabilities:
uc=1−probability c is not used =1−/producttext|AT AR c|
i=1(1−pi)
This follows from the fact that according to eachTAR
i∈ATAR c, the probability of cnot being used as
a result of the relationship modeled in TAR iis 1−pi.
So far, we explained how the Usage Prediction Registries
are updated when to.startis observed. However, the ob-
servation of to.endcan also update the Usage Prediction
Registries.I f to.endis a top-level transaction, the tltocor-
responding to to.endis removed. As a result, all the TARs
that have tltoas their only basis are removed from ATAR c.
Since in this case CTAR c=∅, steps 1-4 are skipped, and
step 5 is performed to propagate the impact of these dele-tions on all of the components’ predictions.
TheUsage Prediction Registry is either updated each time
a transaction and its corresponding events are observed, oron an as-needed basis.
8. USING REGISTRY FOR ADAPTATION
The ultimate goal in our research is to use the predictions
for making adaptation decisions. The probabilistic rules in-ferred using our approach collectively represent the stochas-tic dependency model of the system. Such a model couldbe used in the context of both tranquility [22] and version-consistency [17] for adaptation. In our current approach, we
employ a technique similar to that described in tranquility,
where we temporarily buﬀer (store) events intended for acomponent during the time it is being replaced. Alterna-tively, we could have employed a technique similar to that
of version-consistency, where two instances of a component
are leveraged, and incrementally new top-level transactionsare shifted to use the new version. Our approach could beused to both guarantee consistency andminimize disruption
as described in detail below.
8.1 Guarantying Consistency
As speciﬁcally noted in Section 3, inconsistency could re-
sult if a component is adapted at a time in which it has
already participated in a transaction that it participates inagain. That is to say, to maintain consistency ,ac o m p o n e n t
must not be adapted if it has been used in some top-leveldependent transaction until that top-level dependent trans-
action terminates. Our predictions would very nearly ap-
proximate that type of protection, given that a componentthat is used typically ends up with a high usage predictionin its register and that value will not dissipate until the top-level transaction that caused it terminates. However, there
is a slight risk that our approach as described up to this
point would not fully guarantee consistency, because afterall one cannot guarantee the accuracy of mined rules.
In situations where such a risk is unacceptable, we make a
slight modiﬁcation to the approach described in Section 7.3
that allows us to provide consistency guarantees. When we
observe a transaction t
o, whereto.start.dst =c,w el o c kt h e
value of uc= 1 to prevent cfrom being adapted, since we
now know it has participated in a transaction, and changes
to it may result in inconsistencies. However, since we do not
know in which top-level transaction it has participated (i.e.,we do not know the TLT), we keep the prediction locked at1 until all of the TLTs that are the basis of that predictionhave closed, at which point we roll back to the mechanism
described in Section 7.3 for updating its prediction value.
8.2 Minimizing Disruption
Whenuc= 1, we do not adapt c, since the change is likely
to leave the system in an inconsistent state. However, whenu
c<1,chas not yet been in a top-level transaction, but
could still be used at anytime in the future. If we adapt c,
we may disrupt the system, as events sent to that compo-
nent would be buﬀered until the adaptation has ﬁnished. Toeliminate disruption, it is tempting to use u
c= 0 as condi-
tion for adapting c. It may, however, take a long time for
ucto become 0 and this could create a reachability problem,
i.e., a situation in which one has to wait a long time, oreven forever, before the condition for adaptation is met. In
practice, it is often reasonable to accept the potential for a
slight disruption and allow adaptation when u
c</epsilon1.
Figure 4 exempliﬁes how this approach works. Although
the examples are hypothetical, the registries indeed behavesimilarly in practice. A typical registry goes through this
motion many times over the execution of the system: start-
ing at 0 when a top-level transaction is initiated, risingas new observations are made and TARs are applied, andfalling back to 0 once the top-level transaction has termi-
nated. The steps in these functions represent the times at
which the registries are updated. Finally, when the rules areaccurate, we expect the step function to be skewed to theright when the component is eventually used, and skewedto the left otherwise. This is because typically when a com-
ponent is eventually used, additional observations are made
that subsequently satisfy more TARS, which combine to in-crease the component’s usage probability.
As depicted in Figure 4, when a component has a u
c≥/epsilon1
at the time of adaptation decision and the component actu-ally gets used before the end of that transaction ( active), we
say it is a True Positive (TP) result. When a component hasau
c</epsilon1at the time of adaptation decision and the compo-
nent is eventually used (active), we say it is a False Negative
(FN) result. Similarly, False Positive (FP) and True Nega-
tive (TN) results can be deﬁned when a component is noteventually used ( inactive)a sd e p i c t e di nF i g u r e4 b .
The remaining challenge is how to pick a value for /epsilon1that
7timeuc
1
/epsilon1
Component is Used(a)
TP c
FN c timeuc
1
/epsilon1
Component is Not Used(b)
FP c
TN c
Figure 4: Hypothetical behavior of ucasto.startis
observed for some top-level transactions over time:
(a)cis eventually used, and (b) cis not used.
is meaningful. We deﬁne /epsilon1in terms of another parame-
terr, which represents the tolerable rate of all adaptations
that may result in disruption. We believe ris a reasonable
threshold that can be speciﬁed by the user, e.g., the userstating that on average no more than 0.05 (5%) of adapta-
tions should result in a disruption. In essence, ris used to
make a trade-oﬀ between reachability and disruption.
To be able to calculate /epsilon1based on r,w eh a v et or e l a t ea
system-wide threshold deﬁned by rto a component-speciﬁc
threshold deﬁned by /epsilon1. We do this from a probability dis-
tribution of prior predictions, embodied in the recorded u
c
values. Let Uarepresent the set of all recorded predictions
for components that were eventually used (active), and Ui
represent the set of all recorded predictions for componentsthat were eventually not used (inactive). In essence, U
arep-
resents the set of all recorded values corresponding to thestep function of Figure 4a for all components in the system,
whileU
irepresents the same except for Figure 4b. As a re-
sult,Uaindicates the situations in which adaptations could
have possibly disrupted the system in the past.
GivenUaandUi, it is possible to build the corresponding
frequency distributions PUaandPUia ss h o w ni nF i g u r e s5 a
and b, respectively (though /epsilon1is not known when these are
ﬁrst built). Using conventional techniques [2], we can derive
the cumulative distribution function (CDF) FUaandFUi
fromPUaandPUi, as depicted in Figures 5c and d, respec-
tively.FUa(/epsilon1) deﬁnes the fraction of all uasamples where
ua≤/epsilon1. In other words, r=FUa(/epsilon1). Thus, we can calculate
/epsilon1based on the rvalue speciﬁed by the user as the inverse
/epsilon1=F−1
Ua(r). In terms of probability theory, this means that
/epsilon1is ther-quantile of the probability distribution [2]. The
CDF would need to be updated either periodically or asneeded based on the execution history of the system.
It should be apparent from Figures 5a and b that the key
to limiting error in the approach is to skew P
Uatowards high
values of uandPUitowards low values of u. This will result
inFUaremaining at low values and then escalating quickly
as it approaches 1.0, while FUiescalates quickly and then
grows gradually to 1.0. This diﬀerence in FUaandFUican
be seen in Figures 5c and d based on the slight diﬀerence
in skewing shown in PUaandPUiin Figures 5a and b. If
the approach is able to skew the distributions for active and
inactive components diﬀerently, then it eﬀectively achievestherealgoalofthisapproach: itdistinguishesbetweenactiveand inactive components in advance.
9. EV ALUATION
We have developed a prototype of the approach using
Apriori—an association rule-mining algorithm with an im-
plementation provided in WEKA [11]. As explained in Sec-ua
1PUa
0/epsilon1r(a)
ui
1PUi
0/epsilon1(b)
ua
1FUa
0 1/epsilon11
r(c)
ui
1FUi
0 1/epsilon11(d)
Figure 5: (a) Frequency distribution for Ua,( b )F r e -
quency distribution for Ui,( c )C D Ff o r Ua,a n d( d )
CDF for Ui.
tion 6.4, we intentionally use very low conﬁdence and sup-port thresholds: p=0.05 and s=0.045. We performed
experimentation on runtime adaptation of EDS (recall Sec-tion 2). To evaluate the approach, we used several versionsof EDS as shown in Table 1. We used a baseline version
of EDS with a single user. We then repeated the evalu-
ations on higher concurrency systems to evaluate the sus-ceptibility of the approach to concurrency errors. The 80and 137 experiments were simulated by using hyperactive
dummy users, as EDS never naturally reached that level
of concurrency error. Therefore, the values for users aremerely projections, and the precise values for concurrencyerror rate should receive primary focus. Table 1 shows whatpercentage of all recorded transactions were actually erro-
neous duplicates caused by concurrency, as well as the aver-
age number of these erroneously recorded transactions pertop-level transaction. Each experiment had roughly 8 truetransactions per top-level transaction. Finally, to assess the
accuracy and performance of our approach under diﬀerent
conditions, the 80 user experiment was intentionally allowedt oe x e c u t ef o ral o n g e rp e r i o do ft i m e ,w h i c hr e s u l t e di nc o l -lection of signiﬁcantly more top-level transactions.
9.1 Effectiveness of TAR Reducing Heuristics
We ﬁrst show the eﬀectiveness of our rule pruning heuris-
tics (recall Section 6). Signiﬁcant reduction in TAR volumeinthe Prune Rules stagetookplaceinalloftheexperiments.
The reduction number can be seen in Table 1. This reduc-
tion can only be truly appreciated when considered with two
other facts: (1) the reduced rule base does not signiﬁcantlydegrade the accuracy as evaluated next, and (2) becauseof this reduction, the remaining rules can be applied veryeﬃciently at runtime (evaluated in Section 9.4).
Table 1: Experimental systems used in evaluation,
and eﬀects of TAR pruning heuristics.
#o f #o fT L T Concurrency Errors #o fT A R
Users Observed Rate Per Itemset Initial Remain
1 500 0.00% 0.00 38,582 1,683
10 1,628 1.69% 0.13 34,050 2,190
28 2,787 4.51% 0.35 38,248 2,331
40 3,330 10.94% 0.92 38,460 1,758
80 11,920 36.32% 4.19 35,168 3,126
137 3,543 60.77% 11.26 31,442 3,143
8Figure 6: The results from the experiments: (a)
CDF of Uaand (b) CDF of Ui.
9.2 Accuracy of Component Usage Predictions
A crucial evaluation dimension for our approach is the
degree to which it correctly predicts the usage of a compo-nent. As discussed in Section 8.2, the accurate prediction ismanifested through skewing F
Uato a slow growth function
that then escalates quickly at high values of ua, while at the
same time skewing FUito a quickly escalating function that
then grows only gradually over high values of ui. Figures 6a
and b show FUaandFUifor the various experimental sys-
tems that we used. It is clear from comparison of these twocharts that our approach achieved signiﬁcant diﬀerentiation
between active and inactive components.
Using these CDFs, we can quantify the eﬀectiveness of
the approach in terms of FN, TP, TN, and FP. As discussed
in Section 8.2, the approach uses /epsilon1to ﬁx the FN rate at r.
Therefore, the eﬀectiveness of the approach must be mea-sured in its ability to minimize the FP rate based on theﬁxed value of FN. Because our approach achieved signiﬁ-cant diﬀerentiation between F
UaandFUi,f o rr=0.20, we
were able to set /epsilon1at relatively high values and achieve the
very favourable error rates as shown in Table 2. As seen,in all experiments except for that with the highest concur-rency, the unﬁxed error rate of FP was held to below 7%,well below the ﬁxed FN error rate. Beyond demonstrating
accuracy in the prediction of component activity, these ra-
tios also demonstrate that the approach was not noticeablyimpacted by an increase in concurrency in the system untilconcurrency reached extreme levels.
This quality of diﬀerentiation can be viewed with a re-
ceiver operating characteristic curve (ROC curve) [8, 21],
often used to evaluate a binary classiﬁer, as shown in Fig-ure 7. In our case, the ROC curve depicts the change in ther a t i oo fT Pt oF Pa sd i ﬀ e r e n t /epsilon1thresholds are chosen. The
extreme of /epsilon1=1.0 exists at the origin of the ROC plot, while
the extreme of /epsilon1=0.0 exists at the point (1 , 1) of the ROC
plot. Therefore, it can be seen how the TP and FP ratesrespond by moving the /epsilon1threshold to balance between (1)
Table 2: Error and accuracy rates for the experi-
mental systems.
#o f False True True False /epsilon1
Users Negative Positive Negative Positive Value
1 0.212 0.788 0.951 0.049 0.66
10 0.204 0.796 0.947 0.053 0.71
28 0.203 0.797 0.951 0.049 0.74
40 0.203 0.797 0.946 0.054 0.72
80 0.204 0.796 0.937 0.063 0.92
137 0.202 0.798 0.825 0.175 0.95rate of disruption and (2) reachability of adaptation. The
ROC curve shows that the approach does an incredible job
ofachievingtruepositivesdespitechangesinthe /epsilon1threshold.
The comparison of the diﬀerent experiments also shows
the eﬀect of concurrency on the approach. As seen in Ta-
ble 2, higher values for /epsilon1are needed to achieve r=0.20
as concurrency increases. This occurs because, with many
users in the system, there are many more observations that
allow the approach to predict usage of a component c, when
cis actually used. Therefore, as concurrency increases, the
values for uaare more skewed towards 1.0 until, at a con-
currency error rate of roughly 60% for EDS (i.e., case of 137users), active components are constantly at u
c=1.0 un-
til the transactions they participate in subside. While thisis beneﬁcial because it approaches perfect classiﬁcation ofactive components (as can be seen in Figure 7, higher con-
currency systems actually escalate to the (0 , 1) point more
directly), it results in two detriments to the approach.
First, once the concurrency rate forces /epsilon1to be set to 1.0
given some rvalue,/epsilon1has reached its maximum value and
as such cannot compensate for the increasing false positiverate by moving to a higher value. Therefore, once concur-rency forces /epsilon1to be set to 1.0 to achieve r, the approach
can no longer compensate for the higher FP rates caused byeven further increases in concurrency. Second, as concur-
rency increases to greater levels, components remain active
for greater portions of time. But, since at that point allactive components are eﬀectively always at u
a=1.0, prob-
lems of reachability may occur ifthe components never be-
come inactive. An implementation of our approach basedon version-consistency [17] would address this problem, bybringing a new version of the component on line to ser-vice the new top-level transactions, while the old componentgraduallytransitionstoaninactivestate. Thatsaid, wehave
neverbeenabletorecreatesuchanextremescenarioinEDS,
using real user loads or even the highly extreme simulatedcases.
9.3 Accuracy of Desired Disruption Rate
The third point of evaluation is the degree to which the
approach achieves the desired rrate of disruption during
adaptation. The evaluation results presented in the pre-vious section and shown in Table 2 were prospective error
rates due to setting /epsilon1at the speciﬁed level based on historic
prediction values. In this section, then, we look to see howwell the false negative rate rwas tracked once /epsilon1was set. Ta-
Figure 7: ROC Curve for the various experiments.
9Table 3: Tracking of false negative (FN) threshold.
# of Users Mean False Neg. Rate 95% Conf. Interval
1 0.209 [0.204, 0.215]
10 0.200 [0.196, 0.205]
28 0.203 [0.199, 0.207]
40 0.210 [0.207, 0.213]
80 0.206 [0.191, 0.222]
137 0.208 [0.203, 0.213]
ble3showsthemeanfalsenegativeratesand95%conﬁdence
intervals for those false negative rates for the diﬀerent ex-perimental systems. These statistics were calculated basedon 450-sample moving averages that were recalculated at
45 sample intervals. As shown, the system very eﬀectively
tracks the chosen r=0.20 and maintains a fairly tight con-
ﬁdence interval around its mean. Furthermore, it should benoted that the rate of concurrency does not noticeably aﬀectthe tracking of r.
9.4 Performance and Timing
The ﬁnal evaluation criteria are the performance bench-
marks of Mining Rules andApplying Rules cycles. We have
collected these numbers on a MacBook pro laptop with 2.53
GHz Intel Core i5 processor and 4 GB 1067 MHz DDR3
memory. The mining of the event logs to generate the ruleshas been extremely fast. Although we set our support andconﬁdence values very low, resulting in a large number of
rules to be generated, Apriorih a sa l w a y sc o m p l e t e dt h a ti n
less than 2 seconds in all of the experiments described here.
The performance of updating the predictions at runtime
consists of two primary elements: retrieval of relevant TARs
(recallCTARfrom Section 7.2) and update of the Usage
Prediction Registry by applying the rules. For the former,
MySQL database version 5.5.8 is used to store the rule base.However, because retrieval of TARs from MySQL was ob-served to take typically between 1.355 seconds and 0.959
seconds, we implemented a simple caching of the Rule Base .
Based on this caching, the combined time of retrieving rel-
evant TARs and updating the Usage Prediction Registry by
applying the rules takes very little time. The mean process-ing times and 95% conﬁdence intervals for those processing
times are given in Table 4. As seen, the processing times are
quite short, tightly bound in the 95% conﬁdence intervals,and not noticeably eﬀected by the increase in concurrencyexcept for a few millisecond gain in mean processing time
for larger rule bases.
10. RELATED WORK
In Section 3, we described the most related approaches,
namely quiescence [14], version-consistency [17], and tran-
quility [22], including their relationship to this work. Herewe focus on other related literature.
Researchers have used log of event data collected from
a system to construct a model of it for various purposes.
Table 4: Performance of rule application.
#o f Mean Time for Rule 95% Conﬁdence
Users Application (ms) Interval (ms)
1 3.23 [3.087, 3.378]
10 3.80 [3.587, 4.016]
28 2.88 [2.700, 3.056]
40 2.14 [2.093, 2.184]
80 4.90 [4.602, 5.204]
137 5.04 [4.962, 5.126]Cook et al. [4] use the event data generated by a softwareprocess to discover the formal sequential model of that pro-
cess. In a subsequent work [5], they have extended their
work to use the event traces for a concurrent system tobuild a concurrency model of it. Gaaloul et al. [9] discoverthe implicit orchestration protocol behind a set of web ser-vices through structural web service mining of the event logs
and express them explicitly in terms of BPEL. Motahari-
Nezhad et al. [19] present an algorithmic approach for cor-relating individual events, which are scattered across sev-eral systems and data sources, semi-automatically. They
use these correlations to ﬁnd the events that belong to the
same business process execution instance. Wen et al. [23]use the start and end of transactions from the event log tobuild petri-nets corresponding to the processes of the sys-tem. None of these approaches aim to understand the be-
havior of the system for the purpose of adaptation.
Software architecture has been shown to provide an ap-
propriate level of abstraction and generality to deal with the
complexity of dynamically adapting software systems [15,
20]. Gomaa and Hussein [10] developed the notion of recon-
ﬁguration pattern, which is a repeatable sequence of stepsfor placing a software component in the quiescence state. Inrecent work [6, 7], we adopted this concept to provide safeadaptation support on top of a middleware platform.
Finally, mining software repositories (e.g., source control
systems, bug tracking systems, etc.) is a thriving thrust ofresearch (see [1]). Although related, our objective in thispaper is fundamentally diﬀerent from that line of research.
To the best of our knowledge, data mining has not been
applied for determining the time at which changes shouldoccur in a running software system.
11. CONCLUSION
We provided an overview of a mining-based approach that
from the execution history of a software system infers a
stochastic component dependency model of its components.
We have used this model for the purpose of determiningthe time at which a component can be replaced withoutleaving the system in an inconsistent state and creating a
signiﬁcant disruption. Our approach can be applied with
minimal eﬀort. All that is needed is the ability to monitorthe interactions among the components of that system. Ourapproach can be used to learn the emerging component de-pendencies as the software system evolves. The evaluation
of our approach using a real software system and numerous
users have empirically shown the accuracy of the models in-ferred in this way, and the ability to leverage those modelsto make timely and eﬀective adaptation decisions.
In the future, we plan to experiment with other types
of data mining algorithms that leverage frequency of itemoccurrence, as well as temporal and ordering relationshipamong events. With these sorts of approaches, we hope toleverage information that is already available, but not eﬀec-
tively utilized in our current approach to further improve its
precision and performance.
12. ACKNOWLEDGMENTS
This research is supported by award CCF-1217503 from
National Science Foundation and award D11AP00282 from
Defense Advanced Research Projects Agency.
1013. REFERENCES
[1] Mining software repositories. http://msrconf.org/.
[2] Bertsekas, D. P., and Tsitsiklis, J. N. Introduction to
Probability, 2nd Edition . Athena Scientiﬁc, July 2008.
[3] Cheng, B. et al. Software engineering for Self-Adaptive
systems: A research roadmap. In Software Engineering
for Self-Adaptive Systems, LNCS Hot Topics. 2009,
pp. 1–26.
[ 4 ]C o o k ,J .E . ,a n dW o l f ,A .L .D i s c o v e r i n gm o d e l so f
software processes from event-based data. ACM Trans.
Softw. Eng. Methodol. 7, 3 (July 1998), 215–249.
[ 5 ]C o o k ,J .E . ,a n dW o l f ,A .L .E v e n t - b a s e dd e t e c t i o no f
concurrency. In Int’l Symp. on the Foundations of
Software Engineering (Lake Buena Vista, Florida,
Nov. 1998), pp. 35–45.
[6] Esfahani, N., and Malek, S. On the role of
architectural styles in improving the adaptation
support of middleware platforms. In European Conf.
on Software Architecture (Copenhagen, Denmark,
Aug. 2010), pp. 433–440.
[7] Esfahani, N., and Malek, S. Utilizing architectural
styles to enhance the adaptation support ofmiddleware platforms. Journal of Information and
Software Technology 54 , 7 (July 2012), 786–801.
[8] Fawcett, T. An introduction to ROC analysis. Pattern
Recogn. Lett. 27 , 8 (June 2006), 861–874.
[9] Gaaloul, W., Baina, K., and Godart, C. Log-based
mining techniques applied to web service composition
reengineering. Service Oriented Computing and
Applications 2, 2-3 (May 2008), 93–110.
[10] Gomaa, H., and Hussein, M. Software reconﬁguration
patterns for dynamic evolution of software
architectures. In Working IEEE/IFIP Conf. on
Software Architecture (Oslo, Norway, June 2004),
pp. 79–88.
[11] Hall, M. et al. The WEKA data mining software: an
update. SIGKDD Explor. Newsl. 11 , 1 (Nov. 2009),
10–18.
[12] Han, J., and Kamber, M. Data mining: concepts and
techniques . Morgan Kaufmann, 2006.
[13] Kephart, J. O., and Chess, D. M. The vision ofautonomic computing. IEEE Computer 36,1( J a n .
2003), 41–50.
[14] Kramer, J., and Magee, J. The evolving philosophers
problem: Dynamic change management. IEEE Trans.
Softw. Eng. 16 , 11 (Nov. 1990), 1293–1306.
[15] Kramer, J., and Magee, J. Self-Managed systems: an
architectural challenge. In Int’l Conf. on Software
Engineering (Minneapolis, Minnesota, May 2007),
pp. 259–268.
[16] Lemos, R. d. et al. Software engineering for
Self-Adpaptive systems: A second research roadmap.
InSoftware Engineering for Self-Adaptive Systems
(Dagstuhl, Germany, June 2011), R. d. Lemos,
H. Giese, H. Muller, and M. Shaw, Eds.
[17] Ma, X., Baresi, L., Ghezzi, C., Panzica La Manna, V.,
and Lu, J. Version-consistent dynamic reconﬁgurationof component-based distributed systems. In Int’l
S y m p .o nt h eF o u n d a t i o n so fS o f t w a r eE n g i n e e r i n g(Szeged, Hungary, Sept. 2011), ACM, pp. 245–255.
[18] Malek, S., Mikic-Rakic, M., and Medvidovic, N. A
Style-Aware architectural middleware for
Resource-Constrained, distributed systems. IEEE
Trans. Softw. Eng. 31, 3 (Mar. 2005), 256–272.
[19] Motahari-Nezhad, H. R., Saint-Paul, R., Casati, F.,
and Benatallah, B. Event correlation for process
discovery from web service interaction logs. The
VLDB Journal 20 , 3 (June 2011), 417–444.
[20] Oreizy, P., Medvidovic, N., and Taylor, R. N.
Architecture-based runtime software evolution. In Int’l
Conf. on Software Engineering (Kyoto, Japan, Apr.
1998), pp. 177–186.
[21]
Tan, P., Steinbach, M., and Kumar, V. Introduction to
Data Mining , 1 ed. Addison Wesley, May 2005.
[22] Vandewoude, Y., Ebraert, P., Berbers, Y., and
D’Hondt, T. Tranquility: A low disruptive alternativeto quiescence for ensuring safe dynamic updates.IEEE Trans. Softw. Eng. 33 , 12 (Dec. 2007), 856–868.
[23] Wen, L., Wang, J., Aalst, W. M., Huang, B., and Sun,
J. A novel approach for process mining based on eventtypes. J. Intell. Inf. Syst. 32 , 2 (Apr. 2009), 163–190.
11