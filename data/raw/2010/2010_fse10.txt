PENELOPE: Weaving Threads to Expose Atomicity
Violations
Francesco Sorrentino
University of Illinois,
Urbana-Champaign
sorrent1@illinois.eduAzadeh Farzan
University of Toronto
azadeh@cs.toronto.eduP . Madhusudan
University of Illinois,
Urbana-Champaign
madhu@illinois.edu
ABSTRACT
Testing concurrent programs is challenged by the interleaving ex-
plosion problem ‚Äî the problem of exploring the large number of
interleavings a program exhibits, even under a single test input.
Rather than try all interleavings, we propose to test wisely: to exer-
cise only those schedules that lead to interleavings that are typical
error patterns. In particular, in this paper we select schedules that
exercise patterns of interaction that correspond to atomicity viola-
tions. Given an execution of a program under a test harness, our
technique is to algorithmically mine from the execution a small
set of alternate schedules that cause atomicity violations. The pro-
gram is then re-executed under these predicted atomicity-violating
schedules, and veriÔ¨Åed by the test harness. The salient feature of
our tool is the efÔ¨Åcient algorithmic prediction and synthesis of al-
ternate schedules that cover all possible atomicity violations at pro-
gram locations. We implement the tool P ENELOPE that realizes
this testing framework and show that the monitoring, prediction,
and rescheduling (with precise repro) are efÔ¨Åcient and effective in
Ô¨Ånding bugs related to atomicity violations.
Categories and Subject Descriptors
D.2.5 [ Software Engineering ]: Testing and Debugging; D.2.4
[Software Engineering ]: Software/Program VeriÔ¨Åcation
General Terms
Algorithms, Reliability, VeriÔ¨Åcation
Keywords
atomicity violation, concurrency, dynamic analysis, predictive anal-
ysis, schedule selection, testing
This work was funded partly by the Universal Parallel Computing Re-
search Center (UPCRC) at the University of Illinois at Urbana-Champaign,
sponsored by Intel Corp. and Microsoft Corp, by NSF award #0747041,
and by an NSERC Discovery Grant.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proÔ¨Åt or commercial advantage and that copies
bear this notice and the full citation on the Ô¨Årst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciÔ¨Åc
permission and/or a fee.
FSE-18, November 7‚Äì11, 2010, Santa Fe, New Mexico, USA.
Copyright 2010 ACM 978-1-60558-791-2/10/11 ...$10.00.1. INTRODUCTION
Concurrency errors are notoriously hard to Ô¨Ånd, and are char-
acterized by subtle interleaving patterns that tend to manifest in
the Ô¨Åeld, while passing extensive randomized testing in develop-
ment. Furthermore, they are hard to reproduce, record, and repair,
primarily because the programmer faces the difÔ¨Åculty of consider-
ing all the possible interleavings exercised by a nondeterministic
scheduler. Testing all possible interleavings, even under a single
test input, is infeasible, and this interleaving explosion problem is
one of the most important challenges that needs to be overcome
to achieve effective testing of concurrent programs. With the ad-
vent of multicore hardware, concurrent and parallel software fea-
ture prominently in the future, making this an important problem
to solve.
The current technology of testing concurrent programs on a test
input is stress testing ‚Äî to test the program under random schedules
by creating a large number of threads and strewing the code with
sleep commands for random time intervals, and letting the sys-
tem run and re-run for days together. This methodology is highly
unsystematic and does not seem to exploit the fact that most con-
currency errors can be explained using a simple schedule that uses
a very few threads but an intricate interleaving of them.
The focus of research in this area has hence moved to testing sys-
tematically a small subset of schedules. For instance, the C HESS
tool from Microsoft expects programs to use a small number of
threads, and tests all schedules that use a bounded number preemp-
tions (unforced context-switches).
Atomicity-violating schedules.
The thesis of this paper is that one can do interleaving selec-
tion more effectively by only exercising those interleavings that
are symptomatic of common error patterns of interactions among
threads. In this paper, we focus on the common error pattern of
atomicity violations and explore schedule selection algorithms that
systematically choose and schedule those that violate atomicity.
A programmer writing a procedure (say a method of a class)
often desires uninterfered access to certain shared data that will
enable him/her to reason about the procedure locally. The pro-
grammer puts together a concurrency control mechanism to ensure
this atomicity, often achieved by taking locks associated to the data
accessed, or implicitly taking locks, e.g. using synchronized
blocks in Java. This is however extremely error-prone: not acquir-
ing all required locks for the data leads to errors, non-uniform or-
dering of locking can cause deadlocks, and naive ways of granular
locking can inhibit concurrency, which force programmers to in-
vent intricate ways to achieve concurrency and correctness at the
same time. When the procedure interacts with another concur-
rent thread, inconsistent state conÔ¨Ågurations could occur leading
to unexpected behaviors and errors. Such errors due to violationof atomicity are in fact, as far as we know, the most prevalent of
concurrency errors: a recent study of classifying concurrency er-
rors [12] shows that a majority of errors ( 69% ) are atomicity vio-
lations (about 30% more are caused by ordering violations and to-
gether with atomicity capture almost all concurrency errors). This
motivates our choice in selecting executions that violate atomicity
as the criterion for choosing interleavings to execute.
Our philosophy of schedule selection is to exercise only those
schedules that lead the execution to violate atomicity. Atomicity
violations are deÔ¨Åned with respect to specially marked sequential
blocks of code in the program, which we expect are reasoned se-
quentially and locally by the programmer. In our framework, we
choose these blocks automatically to be obvious syntactically de-
lineated blocks of code, such as the methods of a class, bodies of
loop iteration, etc. While our framework does allow this markup to
be reÔ¨Åned by the tester, manual markup is not necessary (and is not
done in our experiments).
The PENELOPE framework.
The P ENELOPE framework works by taking a concurrent pro-
gramPunder a test harness T. The test harness TcallsP, feed-
ing it inputs as and when required, and veriÔ¨Åes that the output (or
reactive behavior) of the program is correct. P ENELOPE Ô¨Årst ex-
ecutes and observes an arbitrary concurrent execution rofPon
T. This execution ris then abstracted to an abstract execution
r0that records only the reads and writes to shared variables, syn-
chronization events such as the acquisition and release of locks,
thread creations, and barriers, and the begin and end of the sequen-
tial blocks of code (local computation, conditional checks, etc. are
suppressed). The abstracted execution r0is then subject to an al-
gorithmic analysis that checks whether there are alternate ways to
schedule the events in r0to obtain atomicity-violating schedules,
where the violation involves two threads T1andT2, where a block
ofT1gets interrupted non-trivially by a statement of T2due to an
interaction involving a single variable. This algorithmic analysis
issound and complete in predicting atomicity violating schedules
(at this level of abstraction). It also generates a set Sof schedules
that cover all possible atomicity violations involving two program
locations in every thread T1that can be interrupted non-trivially by
a program statement in T2.
The predicted schedules, though feasible at the abstract level of
observation (involving locks, barriers, and thread creation), may
not be actually feasible in the program P, and even if feasible, may
not lead to actual errors the test harness attests to. P ENELOPE hence
reschedulesPunder the predicted atomicity-violating schedules in
S, and checks whether the test harness veriÔ¨Åes these executions to
be correct. P ENELOPE hence has no false positives‚Äî it executes
predicted schedules and reports errors only when it Ô¨Ånds one that
violates the test harness.
Algorithms for predicting schedules that violate atomicity.
The algorithms to select atomicity-violating schedules and to
predict with high-accuracy only feasible schedules that respect the
concurrency control mechanism in the program is non-trivial to re-
alize efÔ¨Åciently (in comparison, selecting all schedules with only
kpreemptions, as in C HESS , is algorithmically trivial). In this pa-
per we exhibit carefully crafted algorithmic techniques that can ex-
amine a single (arbitrary) execution of a concurrent program on
a test and predict alternate ways of scheduling the same events
to cause atomicity violations. Our technique generates schedules
that violate atomicity with respect to any two threads and any sin-
gle variable according to the marked boundaries. The restriction
to two threads and one variable is deliberate and pragmatically
motivated‚Äî most atomicity errors occur due to two threads and onevariable and our algorithms scale well in this case. The predicted
schedules, however, may involve the scheduling of all threads, as
these may be necessary to enable the violation in two threads.
The prediction phase of our technique examines the monitored
run and algorithmically checks if they can be rescheduled to vio-
late atomicity. Intuitively, an atomicity violation is characterized
by three events e1,f, ande2, occurring in that order, where e1and
e2are in the same block of one thread, and fis in a different thread,
ande1ande2are both in conÔ¨Çict withf. The Ô¨Årst part of this phase
consists in identifying cut-points , which are pairs of events eandf,
such thatebelongs to the same thread as e1ande2, occurs between
e1ande2, and can be scheduled concurrently with f. Hence any
schedule that executes eandfconcurrently will expose the atom-
icity violation. Finding atomicity violation using cut-points runs
in linear time using a static analysis of the execution by examining
thelocksets andacquisition histories [5, 10] at every point, and is
adapted from our earlier work [5].
After Ô¨Ånding the cut-points, we develop new techniques to gen-
erate alternate schedules that reach these cut-points concurrently,
hence exposing atomicity violations. This is perhaps the most im-
portant technical contribution of this paper. Schedules that reach
the cut-points concurrently are theoretically possible (indeed the
proof of the cut-point generation relies on the existence of such a
schedule [5, 10]). However, our algorithm synthesizes a schedule
that also (heuristically) ensures maximum conformance to the orig-
inal observed execution. Building schedules that adhere as much
as possible to the causal order of the original observed execution
is crucial to building feasible schedules‚Äî the program under test
may after all have many causal orderings such as communication,
barriers, and even creation of threads that need to be respected to
ensure feasibility.
We also propose several heuristics to reduce the number of context-
switches in the predicted schedules, using a combination of escape
analysis and preventing context-switches between a contiguous set
of read-blocks of a thread. This reduction helps controlling the
overhead in the rescheduling phase, as context-switching is expen-
sive to control (our predicted schedules can have hundreds of thou-
sands of context-switches).
Notice that our prediction algorithms are designed on purpose
notto completely adhere to the happens-before causal relation. In
the predictive world, we do not wish to generate schedules that have
the same partial order as the original run (atomicity is a property of
the partial order, not the exact linearization; hence adhering to the
exact causal order will not lead to an atomicity violating schedule).
Implementation and evaluation.
We have implemented the above algorithms and heuristics in a
framework, called P ENELOPE , that tests Java programs. P ENE-
LOPE observes an execution, and weaves the events in them in
different ways to exercise atomicity violating patterns, subjecting
the resulting executions against a given test harness. P ENELOPE
achieves the monitoring and scheduling phases using Java byte-
code transformations that inserts code to monitor and orchestrate
the scheduling of threads. We show that with a standard automatic
marking of atomic blocks, P ENELOPE accurately predicts sched-
ules that violate atomicity, is effective in reducing the number of
alternate schedules to be tested to a small fraction of all possible
interleavings, and is effective in not only Ô¨Ånding bugs but actually
accurately scheduling them (with accurate playback) to expose er-
rors detectable by the test harness.
While there have been several tools recently that aim to Ô¨Ånd er-
rors using atomicity violations, most of them (including V ELO-
DROME [7] and our own earlier framework reported in [4]) only
monitor one observed execution for atomicity violations. In con-trast, our tool observes an execution, predicts a small set of alter-
nate executions that violate atomicity, and runs them against the
test harness to check for actual errors.
Related Work.
There are two main streams of work that use predictive analysis
for concurrent programs that are relevant. In two papers [21, 20],
Wang and Stoller study the prediction of runs that violate serial-
izability from a single run, similar to our prediction algorithm. In
recent work by us [5], we also propose a similar prediction algo-
rithm for detecting serializability violations. The predictive algo-
rithm of [5] is better as it works with little memory overhead, while
that of Wang and Stoller keeps track of a large graph, which doesn‚Äôt
scale as the size of executions increases. The prediction algorithm
in P ENELOPE is adapted from the algorithm described in Farzan
et al [5]. Note that while both these tools predict possible atom-
icity violations, they do not actually synthesize alternate schedules
nor try to reschedule the program to expose these errors, and have
a lot of false-positives. In [19], the prediction algorithm of [5] in
combination with SMT solvers was used to remove false positives.
A recent work related to ours is the tool CT RIGGER [16], that
has similar motivation as ours in Ô¨Ånding and scheduling atomic-
ity violations. CT RIGGER works by examining a few executions,
Ô¨Ånding points where atomicity violations could occur, prunes away
many schedules that are infeasible due to mutually excluded blocks
or ordering constraints, and tries to schedule a selected subset of
the rest by scheduling those that are less likely to manifest. The
algorithms for pruning are however entirely based on heuristics; in
contrast, the algorithms for pruning in P ENELOPE , and in particular
algorithms it uses for synthesizing schedules, are accurate with re-
spect to the nested locking present in the program. An experimental
comparison was not possible, Ô¨Årst because CT RIGGER works for C
while P ENELOPE works for Java, and also because CT RIGGER tool
was not available for comparison.
There has also been recent work on active randomized testing for
atomicity in the tool A TOM FUZZER [15], that uses randomization
and guidance techniques that executes and ‚Äúhold‚Äù threads at strate-
gic points to try to manifest atomicity errors. We found the holding
of threads at strategic points dynamically and randomly are a bit
unpredictable in our experience with the tool This technique also
does not have the capability of handling nested locks, and inter-
rupts threads at wrong positions which lead it to not Ô¨Ånd errors.
There has also been work on Ô¨Ånding atomicity violations by us-
ing a generalized dynamic analysis of an execution. S IDETRACK
is a new tool [23] that Ô¨Ånds atomicity violations by a generalized
analysis of the observed run. Note that this technique does not
examine runs that are causally different (as P ENELOPE does), and
hence does not do any rescheduling. Moreover, this technique only
detects and reports atomicity violations for a programmer to exam-
ine, and cannot produce error traces that violate the test harness A
more liberal notion of generalized dynamic analysis of a single run
have also been studied in a series of papers by Chen et al [3], who
use static analysis of the program to build a causal map to analyze
for atomicity.
The run-time monitoring algorithms and tools for atomicity vi-
olations that check violations in just the observed run are well un-
derstood, and the tool by Farzan and Madhusudan reported in [4]
provides the most space-efÔ¨Åcient algorithm known for this prob-
lem, as the space overhead is bounded when the number of threads
and variables are Ô¨Åxed (the algorithm used in V ELODROME [7] also
can perform sound and complete serializability detection, but the
space-overhead is not bounded). These techniques work for anynumber of threads and variables . Monitoring, of course, is a much
simpler problem than prediction.
A VIO [17] deÔ¨Ånes access interleaving invariants ‚Äî certain pat-
terns of access interactions on variables‚Äî and learns the intended
access interleaving using tests, and monitors runs to Ô¨Ånd errors. A
variant of dynamic two-phase locking algorithm [14] for detection
of serializability violations is used in the atomicity monitoring tool
developed in [22].
Apart from the related work discussed above, atomicity viola-
tions based on serializability have been suggested to be effective in
Ô¨Ånding concurrency bugs in many works [8, 6, 21, 20, 22]. Lip-
ton transactions have been used to Ô¨Ånd atomicity violations in pro-
grams [11, 8, 6, 9].
CHESS [13], from Microsoft, and C ONTESTfrom IBM, are two
mature testing tools. C ONTEST uses random injection of sleep
and wait statements in programs to Ô¨Ånd errors. C HESS system-
atically exercises all executions that involve only a few context-
switches, guided by the intuition that many errors manifest with a
few context-switches/preemptions. However, C HESS is constrained
by the number of executions it must explore, which even with two
context-switches grows quadratically in the length of the execu-
tions, making it infeasible for long tests. Some of the benchmarks
handled by P ENELOPE in this paper will overwhelm C HESS , as
there are too may interleavings it must exercise. On the other hand,
CHESS has one distinct advantage over execution-based analysis
techniques (including P ENELOPE and CT RIGGER ) in that it ex-
plores allexecutions with two context-switches and is not con-
strained by the events that occur in a particular observed schedule.
We believe that P ENELOPE and C HESS are complementary in the
their coverage of the interleaving space.
There is a subtlety in our deÔ¨Ånition of serializability: only ac-
cesses to data (and notsynchronization events like acquisition and
release of locks) are considered in determining if a block in an ex-
ecution is serially executed. For example, if a thread reads the Ô¨Åeld
fof an object twice in an atomically marked section, each time
acquiring an releasing the lock on the object, and another thread
accesses a different Ô¨Åeld gof the same object in between these two
accesses under the same object-lock, we will declare the execution
as a serializable one (this is the deÔ¨Ånition we follow in our earlier
papers as well [5, 4]). However, several papers, including the ones
on S IDETRACK and A TOM FUZZER will declare this execution as
non-serializable, because they take into account the edges caused
by the lock acquisitions and releases in the two threads. Our deÔ¨Å-
nition is in fact the more classical deÔ¨Ånition (followed in database
theory), and we believe is the more accurate and useful deÔ¨Ånition
for checking for errors in concurrent programs.
2. OVERVIEW
Assume that we are given a concurrent program P, a test in-
putI, and a corresponding setof expected (correct) outputs O
(we consider a set of outputs as concurrent programs can be non-
deterministic). The input-output pair (I;O)constitutes a test har-
ness. We run the program Pon the input I, and monitor (observe)
an execution R. In most cases, we will get an expected output
O2O at the end of the execution R. We would like to know
if there is an alternative schedule that leads to an execution R0of
the program which manifest some concurrency bug in P. In other
words, we would like to Ô¨Ånd an alternate execution R0that gener-
ates an output O0that does not belong to the set of expected outputs
O, signalling the existence of a bug. The interleaving explosion
problem prevents us from trying all possible runs (as there are too
many of them) to Ô¨Ånd a bad one. Here, we show how we come upwith likely candidates for such runs, and how we schedule these
candidate runs against the harness to check if it causes a real bug.
public synchronized boolean addAll(Collection c) {
modCount++;
int numNew = c.size();
// ....... possible interference ....
ensureCapacityHelper(elementCount + numNew);
Iterator e = c.iterator();
for (int i=0; i<numNew; i++)
elementData[elementCount++] = e.next();
return numNew != 0;
}
Figure 1: Method addAll of concurrent Java class Vector .
We use the following example to illustrate how P ENELOPE comes
up with alternative runs that violate atomicity, which can Ô¨Ånd con-
currency bugs in a program. Consider the implementation of the
method addAll from the built-in Java library class Vector in
Figure 1. The purpose of this method is to add all elements of the
parameter collection (say another vector) to the end of the current
vector. The method size , which returns the number of elements of
the source, is a synchronized method. The problem in this pro-
gram is that after safely retrieving the number elements that are to
be copied, there can be a concurrent thread that modiÔ¨Åes the source
vector before the method Ô¨Ånishes (for example, a concurrent thread
could remove all the elements from the source vector). Therefore,
when the execution of the addAll method happens, the informa-
tion about the number of the items that are being copied is stale,
and an exception will be raised when it tries to access elements that
are not there anymore.
e1e2fTT/prime........./triangleleft‚äø............
Testing this program is unlikely to reveal these kinds
of bugs in small methods such as addAll , as scenarios
where the second thread gets interleaved in between the
events of the Ô¨Årst thread is unlikely. One has to actively
make it happen, and that is exactly what we do.
This problematic scenario can be generalized using
the following pattern of shared variable accesses:
1. Thread (1) reads the value of a shared variable
(c.size() in the above example).
2. Thread (2) writes (modiÔ¨Åes) the value of that
same shared variable.
3. Thread (1) reads the value of the shared variable
again (in c.iterator() in the above example).
We refer to such a pattern as a RWR pattern (Read-Write-Read).
There are four more patterns that can capture other forms of unde-
sired interference: WRW (Write-Read-Write), RWW (Read-Write-
Write), WWR (Write-Write-Read), and WWW (Write-Write-Write).
PENELOPE focuses on executions that contain one of these patterns
as good candidates for Ô¨Ånding hidden concurrency bugs.
PENELOPE works in three phases.
Phase I: Monitoring. In this phase we execute the pro-
gram on the input from the test harness and observe a run R.
PENELOPE focusses its attention to read and write accesses
to the (potentially) shared variables and acquisitions and re-
leases of locks, thread creation, and barriers, and ignores all
the other events.
Phase II: Prediction. In phase II, P ENELOPE constructs sev-
eral runsR0, based on run R, by carefully reordering the
events in run Rso that each of the runs R0contain at least
one of the suspect access patterns given above.Phase III: Rescheduling. In the last phase, P ENELOPE forces
the program to execute R0by weaving the threads using a
single processor: if this is successful, then the generated out-
putO0is checked to be in the set of expected outputs, O; if
this fails, a bug is reported. If P ENELOPE fails to schedule a
runR0, i.e. ifR0is not a feasible schedule of the program,
PENELOPE discards it and moves to another predicted exe-
cution.
We describe in detail how the main algorithms in Phase II work in
Section 4,
Interleaving selection and comparison with CHESS.
Our approach differs from that of the C HESS [13] tool in the
rationale that is used for selecting candidate runs. C HESS limits
the number of explored runs by bounding the number of context
switches (actually pre-emptions) allowed in a single run. It is as-
sumed that the number of runs with up to a few context switches
is small enough that one can check all of them. This is not quite
true. In a program with nthreads where each thread executes k
steps out of which at most bare potentially blocking, there can be
up to nk
(nb+c)!
executions when the number of context switches
is limited to c[13]. This can amount to a huge number of exe-
cutions for very simple programs. The number of executions ex-
plored by C HESS is in fact higher because of the way they deÔ¨Åne
pre-emptions , where context-switching caused by a thread encoun-
tering a yield statement is not counted as a pre-emption even though
it may be possible to continue executing the thread. The number of
pre-emptions is hence not polynomially bounded in nandk, even
for a Ô¨Åxedc.
Consider, for example, a program that has two threads T1andT2
that manipulate a 1010matrix, where T1updates the Ô¨Årst 5rows
andT2updates the latter 5rows. Each thread sequentially iterates
over the cells in its portion, and for each cell cadds up the value
of all the 8neighboring cells and the cell citself, and writes the re-
sult into the cell c; this example is typical of particle computations
and parallel algorithms in graphics. The computations of the two
threads overlap at the two middle rows. Assume that there is a lock
associated to each cell that is acquired before a thread accesses that
cell. Since there are about 400accesses to lock-protected data by
each thread, C HESS , even with a preemption bound of 2, sets out
to explore approximately 4002of the interleavings, and failed to
Ô¨Ånish even after 4 hours. P ENELOPE , on the other hand, explores a
much smaller set of interleavings (at most one atomicity violating
schedule for each of the 20 variables accessed by both threads), and
Ô¨Ånishes in under 3 minutes.
3. PRELIMINARIES
A concurrent shared memory program, during its computation,
does local computation, may read and write to shared entities (mem-
ory locations), dynamically create threads, and use synchronization
primitives (such as locks). The framework we build will observe
only an abstraction of this computation that ignores local compu-
tation, ignores precise values read or written, but keeps track pre-
cisely of the read and write operations to shared memory locations
as well as synchronization primitive usage. This abstract view of
a computation, which we call an execution , will be the one that is
algorithmically analyzed in order to predict and schedule alternate
executions.
Recall that we assume that the program has been already auto-
matically annotated with atomicity transaction boundaries to indi-
cate rough logical sequential units of computation. This is done
by P ENELOPE automatically by marking obvious units such as thecode for every method, etc. Note that these annotations have no se-
mantic value‚Äì in particular, they are certainly not respected by the
compiler in any way. These transactional boundaries are also part
of the execution that we observe.
Hence, given an execution, each thread executes in it a series of
transactions . A transaction is a sequence of actions; each action
can be a read or a write to a (global) variable, or a synchronization
action.
Let us now deÔ¨Åne the notation to talk about executions (they
must satisfy the property that they contain transactions and that they
respect the semantics of locks).
We assume an inÔ¨Ånite set of thread identiÔ¨Åers T=fT1;T2;:::;g.
We also assume an inÔ¨Ånite set of shared entity names (or just enti-
ties)X=fx1;x2;:::gthat the threads can access. Let us also Ô¨Åx
a set of global locks L.
The set of actions that a thread Tcan perform on a set of entities
XX is deÔ¨Åned as T;X=fT:;T:g[fT:read(x);T:write(x)j
x2Xg[fT:acquire (l);T:release (l)jl2Lg. ActionsT:read(x)
andT:write(x)correspond to thread Treading and writing to en-
tityx, actionsT:andT:correspond to the beginning and the
end of transaction blocks in thread T, and actions T:acquire (l)
andT:release (l)correspond to acquiring and releasing the lock l.
DeÔ¨Åne X=S
T2TT;X(actions on entities Xby all threads),
T=S
XXT;X (actions by thread Ton all entities), and
 =S
XX;T2TT;X(all actions).
For a word?, letjTbe a shorthand notation for jT,
which includes only the actions of thread Tfrom.
A word2?islock-valid if it respects the se-
mantics of the locking mechanism. Formally, let l=
fT:acquire (l);T:release (l)jT2Tg denote the set of locking
actions on a lock l. Thenis lock-valid if for every l2L,jlis
a preÔ¨Åx ofS
T2T(T:acquire (l)T:release (l))?.
LetTran T;X= (T:)fT:read(x);T:write(x)jx2Xg?
(T:). Atransaction trof a threadTis a word in Tran T;X. Let
Tran T= (Tran T;X)?denote the set of all possible sequences
of transactions for a thread T, and let Tran denote the set of all
possible transaction sequences.
DEFINITION 3.1. Anexecution , over a set of threads T, entities
X, and locksL, is a word2(T;X)?such that for each T2T,
jTbelongs to Tran T, andis lock-valid. Let ExecT;Xdenote
the set of all executions over threads Tand entitiesX.
In other words, a execution is a lock-valid sequence of actions
such that its projection to any thread Tis a word divided into a se-
quence of transactions, where each transaction begins with T:, is
followed by a set of reads and writes, and ends with T:.
When we refer to two particular actions [i]and[j]in, we
say they belong to the same transaction if they are actions of the
same thread T, and they are in the same transaction block in jT:
i.e. if there is some Tsuch that[i];[j]2AT, and there is no i0,
i<i0<jsuch that[i0] =T:.
Throughout this paper, we will assume that acquisitions and re-
leases of locks are nested . In other words, locks are released in the
reverse order of how they were acquired. This is true in most of
our examples, and in fact, using synchronized commands in Java
naturally give nested acquisitions and releases for the most part. In
any case, even if lock acquisitions and releases are not nested, our
algorithms and tool will work; it is only the accuracy of predicting
feasible schedules that will get mildly affected.3.1 Access Patterns for Serializability Viola-
tions
There are Ô¨Åveaccess patterns that correspond to simple atomicity
(serializability) violations of two threads andone variable . Each
access pattern corresponds to the existence and relative ordering
of three events in an execution. A pattern consists of two events
e1ande2which belong to a thread T1, and a third event fwhich
belongs to thread T2. These events should appear in an execution
such that=:::e 1:::f:::e 2:::, in other words, foccurs
aftere1, ande2occurs afterfin. Moreover,e1,f, ande2should
be all accesses to the same shared variable x, and should be of one
formats given below. Note that these exhaust all patterns where
e1andfare dependent as well as fande2are dependent (two
events are dependent if they correspond to two accesses to a single
location, where one of them is a write).
T1:T2:read(x)write(x)RWRRWWWWRWWWWRWread(x)......................................................................T1:T2:read(x)write(x)......................................................................write(x)T1:T2:read(x)write(x)......................................................................write(x)T1:T2:read(x)write(x)......................................................................write(x)T1:T2:write(x)......................................................................write(x)write(x)e1=e2=f=e1=e2=f=e1=e2=f=e1=e2=f=e1=e2=f=
3.2 Locksets and acquisition histories
Letbe an execution and let fTgT2Tbe its set of local exe-
cutions. Consider T(for anyT). The lockset held after Tis the
set of all locks Tholds: LockSet (T) =fl2Lj9i:T[i] =
T:acquire (l)and there is no j >i andT[j] =T:release (l)g.
The acquisition history ofTrecords, for each lock lheld by
Tat the end of T, the set of locks that Tacquired (and possibly
released) by Tafter the last acquisition of the lock l. Formally,
the acquisition history of T,AH(T) :LockSet (T)!2L,
whereAH(l)is the set of all locks l02L such that9i:T[i] =
T:acquire (l)and there is no j >i such thatT[j] =T:release (l)
and9k>i: T[k] =T:acquire (l0).
synchronize (l1) {// point Asynchronize (l2) {// point B}synchronize (l3) {// point C}}
Consider the program structure on the
right. We have
LockSet (A) =fl1g;
LockSet (B) =fl1;l2g
LockSet (C) =fl1;l3g:
Also, we have AH(A) =f(l1;fg)g,
AH(B) =f(l1;fl2g);(l2;fg)g, and
AH(C) =f(l1;fl2;l3g);(l3;fg)g.
Two acquisition histories AH1andAH2are said to be compat-
ibleif there are no two locks landl0such thatl02AH1(l)and
l2AH2(l0).
At at the level of abstraction we observe executions (namely ob-
serving reads, writes, and lock acquisitions and releases), it turns
out that we can precisely capture when two sequences of events of
threadsT1andT2can be combined to a lock-valid execution using
just locksets and acquisition histories. More precisely, let be an
execution and let w1be a preÔ¨Åx of jT1andw2be a preÔ¨Åx of jT2.
Then there is an execution 0such that0jT1=w1and0
T2=w2
if and only if the locksets of T1andT2afterw1andw2are disjoint
and the acquisition histories at the end of w1andw2are compatible
(this technical result is due to Kahlon et al [10]). This result will
underpin our algorithms for predicting and scheduling, as it accu-rately captures those schedules that are feasible in a program at our
level of abstraction.
4. PREDICTION OF ATOMICITY-
VIOLATING SCHEDULES
Phase II of our framework, which is the predictive phase, forms
the crux of our algorithms and we describe it in detail below. This
phase is divided into two sub-phases: Phase IIa deals with the prob-
lem of identifying cut-points of the form (e;f)such that a sched-
ule that reaches the two events in each cut-point would trigger an
atomicity violation. This is followed by Phase IIb, the schedule
generation phase, that actually synthesizes schedules based on the
cut-points.
4.1 Phase IIa: Cut-point Generation
The Ô¨Årst part of Phase II of P ENELOPE is one that generates a set
ofcut-points for the observed execution. This algorithm essentially
determines whether it is possible to reschedule the original execu-
tion to one that violates atomicity, and further provides witnesses
in terms of a set of cut-points.
This algorithm is adapted from an algorithm from our earlier
work [5] that determines cut-points accurately and extremely ef-
Ô¨Åciently in linear time in the length of the execution. We deÔ¨Åne the
notion of cut-points and recap the algorithm here as it is involves
concepts that will help in understanding our scheduling algorithm.
e1e2fTT/prime..................œÅ/prime/prime...
Consider a typical execution that we are
looking for, one where e1occurs Ô¨Årst, then
f, and thene2, wheree1ande2occur in
one transaction block while foccurs in an-
other thread, and the three events form one
of the patterns violating atomicity. The ba-
sic argument in our earlier work [5] is that
such an abstract lock-valid execution is pos-
sible if and only if there is an intermediate
eventebetweene1ande2(ecan bee1but
note2) such that there is an execution that reaches exactly up to
executingeandfin the two threads. To see why, notice that we
can roll back fand play it last (as it is an access to a variable), and
hence we can execute e1followed by f, and let the run proceed to
executee2eventually.
e1e2fTT/prime........./triangleleft‚äø.........
...e...AH1AH2LS2LS1}{LS1‚à©LS2=‚àÖAH1compatible withAH2{
It now turns out that checking
whether there is a lock-respecting
run that reaches simultaneously e
andfis easily solvable, simply by
examining the locksets and acqui-
sition histories at these two points,
due to a result by Kahlon et al [10],
which gives a necessary and sufÔ¨Å-
cient condition for it: the locksets
ateandfmust be disjoint and the acquisition histories at eand
fmust be compatible (see Section 3.2 for deÔ¨Ånitions of locksets,
acquisition histories, and compatibility).
More precisely, a cut-point is a pair of events in the execution,
(e;f), such that there is an alternate schedule that can reach exactly
up toeandfsimultaneously, and furthermore any schedule that
does that gives an atomicity violation. The above arguments give a
characterization of all cut-points that lead to atomicity violations.
The algorithm to detect and compute cut-points works as fol-
lows. We iterate over the events of each thread, gathering four sets
R,W,WW , andAA, which are events along with locksets and
acquisition histories. The sets RandWfor a threadTrecord, forevery shared variable x, one witness pair of lockset and acquisition
history among all reads and writes to x, respectively. The sets WW
andAAare more complex. WW records a witness for every pair
of lockset and acquisition-history, an event in between two writes to
x. Similarly,AArecords events between any two accesses (reads
or writes) to x. Finally the sets WW andRare examined together
to check for a compatible pair, and so are the sets AA andW‚Äî
these capture all the Ô¨Åve possible atomicity patterns. For a Ô¨Åxed
set of shared memory locations and locks, the time taken by the al-
gorithm grows linearly in the length of the execution, quadratic in
the number of threads (as every pair of threads must be examined),
and at most quadratic in the size of the program (as all possible
program locations involved in atomicity violations are identiÔ¨Åed).
Furthermore, the algorithm works very well in practice, scaling to
work in minutes on executions of hundreds of thousands of events.
4.2 Phase IIb: Schedule Generation
The scheduling of alternate executions that violate atomicity is
technically and practically the hardest aspect of this paper, and we
will try to explain all the ideas behind it. We Ô¨Årst describe a the-
oretically motivated algorithm that builds a simple execution from
scratch from a cut-point (e;f)that causes an atomicity violation,
using locksets and acquisition histories. This algorithm assumes
that the program is made of nconcurrent threads that do not use
any mode of communication, but just interact using nested locking;
the algorithm is accurate in predicting under this assumption .
Despite its promised accuracy, it cannot itself be used in practice
because the assumption is false‚Äî programs do indeed have causal
ordering of events and communication (one thread could wait for
another for a signal to proceed), and threads do get created and
destroyed, which also gives a causal ordering to events (if T1cre-
atesT2, we can‚Äôt commute events of T2before the event in T1that
created it). Our main idea is to modify the algorithm to make the
predicted run adhere to the causal ordering of events in the origi-
nally observed run as much as possible. The effectiveness of this
adherence is not captured in a theoretical assurance, but its proof is
in the pudding, as we show that this strategy reduces infeasible ex-
ecutions from being predicted to a large degree in our experiments.
Finally, because of the adherence to the original execution, the
predicted executions share the complexity of the original one. In
particular, the predicted schedules that P ENELOPE synthesizes can
have hundreds of thousands of context-switches . The number of
context-switches affects the time required to schedule the predicted
execution, as context-switches take relatively more time than lo-
cal executions. We provide two heuristic but technically sound
transformations of the schedule that reduce the number of context-
switches (the transformations do not break any causal links that
may be there in the program).
We now outline the theoretical algorithm, the adherence algo-
rithm, and the two heuristics to reduce the number of context-switches.
4.2.1 The Theoretical Scheduling Algorithm
Let us, in this subsection, assume that the program consists of a
static set ofnthreads all already active (view them as nsequential
programs interacting with each other), and further assume that the
threads do not communicate with each other explicitly using data.
Let us assume the threads are T1;:::;T nand further that eandf
occur in threads T1andT2, and the locksets at eandfare disjoint
and the acquisition histories at eandfare compatible. Our goal
is to Ô¨Ånd a locking-respecting execution that precisely executes up
untileandfinT1andT2.
First, note that, since we assumed there is no communication,
executing events from T3;:::T ncannever help in the execution ofT1andT2(i.e. never enable events in T1andT2); in fact, they can
only hinder Ô¨Ånding an execution as they could acquire locks that T1
andT2may require. Consequently, we can completely focus only
on scheduling events in threads T1andT2.
The idea is that we are rescheduling up until a cut-point (e;f)in
threadsT1andT2, whereefalls in between e1ande2in a single
thread, and where the sequence e1followed by ffollowed by e2
realizes the atomicity violation. In doing this alternate schedule,
assume that a lock lis held byT2atf, and assume that the same
locklis acquired and released by thread T1, beforee. Then we
must schedule this block in T1before the last acquire of lock lby
T2beforef, asT1will have no chance to acquire it once T2has
made this acquisition.
Computing the above efÔ¨Åciently and in linear time is non-trivial,
and this is where the acquisition history helps, as it exactly captures
the above information. Let x1andx2be the lastevents inT1and
T2, respectively, that are before eandf, respectively, with locksets
empty. Then, we can easily schedule (without violating locking)
Ô¨ÅrstT1up untilx1and thenT2up untilx2. Note that the Ô¨Årst
events afterx1(andx2) either must be e(f) or must be acquisitions
of locks that are never released till e(f) is reached. Hence the crux
of the scheduling is to schedule from x1andx2tilleandf.
The algorithm works by building a graph of causal edges be-
tween events. For every lock lin the lockset of f, ifloccurs in the
acquisition history of ewith respect to some lock l0, then we know
that after the last acquisition of l0byT1, there was an acquisition
(followed by a release) of the lock l. Hence we know that we must
schedule the last release of lock l0inT1(say eventu)before the last
acquisition of linT2(sayv). We capture this by adding a causal
edge fromutov. Symmetrically, we examine the lockset of eand
the acquisition history of fand throw in causal edges. It turns out
that since the acquisition histories are compatible, this graph will
byacyclic , and hence there is a schedule that respects these order-
ings. The algorithm simply takes a linearization of partial order to
obtain a schedule.
The above argument is adapted from the proof that the prediction
algorithm works correctly , which appears in Farzan et al [5], and
in turn depends on a proof of a theorem by Kahlon et al [10]. Our
contribution here is using it in the scheduling algorithm. We have
also augmented the above construction by using vector clocks to
rule out cut-points that are clearly infeasible. We maintain vector-
clocks for events that get updated during thread-creation and at
barriers only, and eliminate cut-points (e;f)whereeandfare not
concurrent with respect to the vector clocks. This greatly reduces
the number of infeasible interleavings P ENELOPE generates.
4.2.2 Algorithms to adhere to original execution
Note that the theoretically synthesized schedule described above
blindly executes T1tillx1andT2tillx2(i.e. to the last point
where thread have empty locksets), and further ignores all other
threads. This actually causes the scheduling algorithm to break
in practice; when we implemented this, most predicted schedules
were not feasible in the program. The reason is that the theoretical
assumption that there is a set of nthreads from the beginning that
do not communicate is not real in practice.
PENELOPE gets around this problem by scheduling a large preÔ¨Åx
of the run accurately according to the original observed schedule.
Adhering to the observed schedule lets us keep the causal condi-
tions that exist in the actual program and yet allows us to get past it
without modeling it or analyzing in our prediction model.
The scheduling algorithm implemented in P ENELOPE works as
follows. Consider a cut-point (e;f)in threadsT1andT2with dis-
joint locksets and compatible acquisition histories, and let x1andx2be the last events in threads T1andT2, beforeeandf, respec-
tively, with empty locksets. Without loss of generality, assume x1
occurs before x2in the original schedule. Now, for every thread
Ti(includingT1andT2), lethibe the last event in Tithat occurs
beforex1in the original schedule with its lockset empty . Note that
h1=x1, buth2need not be equal to x2. The idea now is to sched-
ule all events of each thread Tiup until each hi, and moreover in
the exact order as they occurred in the original schedule.
More precisely, for an observed schedule , we say event j(1
jjj) is in the preÔ¨Åx setPiff[j] =Ti:aand this event occurs
before or is the same as the event hi. The preÔ¨Åx predicted schedule
is then simply the concatenation of all [j], in increasing order of j,
wherej2P. In other words, we run through the original schedule
and pick an event uto occur provided it is an event of thread Ti
(for somei) and occurs before hi.
The rationale of why this preÔ¨Åx has the desired properties goes
as follows.
Region IIIRegion IIRegion IT1T2T3T4h2h3h4x1=h1x2ef‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢
‚Ä¢‚Ä¢
First, notice that if an
eventuof threadTioc-
curs in the preÔ¨Åx sched-
ule, then so does ev-
ery event of Tibefore
u; hence the events we
choose to execute cer-
tainly have their pre-
decessor events in the
same thread scheduled.
Second, we argue that
the schedule is lock-
respecting. Assume that there is an acquisition of a lock lat an
eventuofTiin the preÔ¨Åx. Then the only way this lock is not
feasible to acquire is that another thread Tjacquired it but did not
release it. But since the original observed execution was feasible,
there must have been a release of this lock by Tjin the original
execution, before u. This release must be included in the predicted
preÔ¨Åx execution as well, because we have chosen each hkto be an
event at which the lockset is empty. Consequently the release must
have happened before uin the predicted preÔ¨Åx schedule as well.
Finally, note that since this schedule adheres to the original thread
to a large extent it is likely to be feasible in the real program as
well. This preÔ¨Åx is represented as Region I in the Ô¨Ågure above.
The next middle phase of the predicted execution is to execute
the program from h2tox2inT2(marked as Region II in the Ô¨Ågure).
Since no other thread holds any lock, this is at least lock-feasible.
Finally, we come to the last phase of the predicted execution (Re-
gion III), which schedules events in between x1andeand between
x2andf. This part is done according to the theoretical schedule
synthesized in the previous section, that uses locksets and acqui-
sition histories to go back and forth between the two threads T1
andT2only to reach eandf. Once we reach eandf, we let all
threads go free to execute (this will, hopefully, make T1executee2
eventually which will cause the atomicity violation).
In practice, events in the preÔ¨Åx, Region I, constitute the major-
ity of events compared to Regions II and III. Hence adhering to
the original observed execution accurately in this region greatly
increases the chance for the schedule being feasible, and as we
show in the experiments, leads to most predicted executions being
schedulable.
4.2.3 Heuristics for reducing context-switches
While the above described algorithms focus on generating exe-
cutions that are likely to be feasible , they ignore the overhead com-
plexity of actually scheduling the runs. The executions we observeInstrumenterJava ClassesObserved RunMonitorCut-PointConÔ¨Årmed Bugs
Test HarnessScheduleGeneratorSet of cut-pointsPredicted SchedulesPassed the Test HarnessGeneratorClasses Instrumentedfor SchedulingScheduleInstrumenterof the form (e, f)InstrumentedclassesPhase I: MonitoringPhase II: PredictionPhase III: ReschedulingFigure 2: PENELOPE ARCHITECTURE .
have a large number of context-switches (hundreds of thousands in
some of our examples), and since our predicted executions try to
adhere to the observed runs for feasibility, they also have a large
number of context switches. We hence employ two heuristics to
bring down the number of context-switches, while at the same time
preserving feasibility. Both heuristics rely on the idea that if a se-
quence of actions is provably non-interfering among the threads,
then we can execute the sequence in any order, and in particular,
execute them with the least number of context-switches.
Escape analysis to reduce context-switches:
The Ô¨Årst idea is to observe which of the (non-local) accesses to
memory locations are actually shared in the run; in actual runs,
several shared variables may be touched by only one thread. Let us
call these memory locations unshared in the observed run. Un-
shared variables are not known in advance, and we do observe
them, and they do play a role when we try to reschedule execu-
tions. However, accesses to unshared variables by a thread can-
not affect another thread, and hence we can shufÔ¨Çe these to obtain
fewer context-switches, without affecting feasibility.
Identifying read-blocks to reduce context-switches:
The second idea is that even if there are a large number of con-
tiguous reads to shared variables, these accesses do not cause real
interference between the threads. Hence the reads in this contigu-
ous block can be shufÔ¨Çed in any way without affecting feasibility
of the schedule. Again, we schedule these reads in a way that min-
imizes the number of context-switches, and reduce it to at most the
number of active threads in the block.
5. IMPLEMENTATION
In this section, we explain the key implementation details of
PENELOPE . Figure 2 illustrates the structure of P ENELOPE . For
more information on PENELOPE and the experiments presented
in Section 6, see http://www.cs.uiuc.edu/ sorrent1/penelope .
Phase I: Monitoring: We implemented our monitoring instru-
menter using the Bytecode Engineering Library (BCEL) [2]. Every
class Ô¨Åle in bytecode is (automatically) transformed so that a call
to a global monitor is made after each relevant action is performed.
These relevant actions include Ô¨Åeld and static Ô¨Åeld reads/writes, en-
try/exits to synchronized blocks and methods, array reads/writes,
etc., but excludes actions such as accesses to local variables. The
global monitor communicates with all threads. Each thread informs
the monitor when it is performing an action that needs to be ob-
served.
Phase II(a): Cut-point generation: In this phase, the algorithm
in Section 4.1 is used by the cut-point generator to identiÔ¨Åes all
possible cut-points off-line: i.e. pairs of events (e;f), such that a
schedule that reaches these points concurrently will violate atom-
icity. Note that our algorithm computes only one representative vi-
olation for each pair of threads, each entity, each program location,each pair of events with a compatible set of locksets and acquisition
histories, and each pattern of violation (W-R-W and A-W-A). Since
recurrent locks (multiple acquisitions of the same lock by the same
thread) are typical in Java, the tool is tuned to handle them by sup-
pressing from the analysis the subsequent acquisitions of the same
lock by the same thread. A simple automatic escape analysis unit
(written as a Perl script) excludes from the execution all accesses
to thread-local entities by replacing them with skips (nops), which
enables faster prediction.
Phase II(b): Schedule generation: Theschedule generator syn-
thesizes a schedule for each (predicted) cut-point using the algo-
rithms described in Sections 4.2. The idea is that schedules are
Ô¨Årst synthesized up to the points where locksets are empty using
the original observed schedule, and then the accurate theoretical
scheduling algorithm is used to execute the events of T1andT2
to cause an atomicity pattern violation. Also, we implemented the
heuristics to reduce the number of context-switches by rearranging
events that are not truly shared (using the escape analysis ) as well
as blocks of reads, to reduce the number of context-switches.
Phase III: Rescheduling predicted schedules: Our scheduler is
also implemented using BCEL [2]; we instrument the scheduling
algorithm into the Java classes using bytecode transformations, so
that the same events that were monitored, now interact with a global
scheduler. The scheduler, at each point, looks at the predicted
schedule, and directs the appropriate thread to perform a sequence
ofnsteps. The threads stop at the Ô¨Årst point with a relevant access,
and wait for a signal from the scheduler to proceed, and only then,
they execute the number of (observable) events they were asked to
execute. After this, the threads communicate back to the scheduler,
relinquishing the processor, and await further instructions.
The scheduler uses two vectors SandNto communicate with
program threads. For each thread Ti,N[i]will contain the number
of steps that Tishould take next, and S[i]contains a Boolean value
that allowsTiprogress when true. Only Sis used to communicate
with threads, and is protected by global locks (one for each cell).
ThreadTican safely access N[i]without synchronization (as the
synchronization in Sis used to provide the synchronization for N
as well). The thread T[i]hence does a wait () onS[i](to become
true), and the scheduler wakes up the thread when it is time for
the thread to proceed. The scheduler then waits on S[0]to become
true. When the thread completes its designated number of steps
(fromT[i]), it setsS[0]to true, notiÔ¨Åes the scheduler, and then re-
linquishes control and waits on S[i]to become true again. Once the
execution reaches the point that the eandfevent from the viola-
tion pattern are executed, the scheduler releases all threads to exe-
cute as they please. There is also a timeout mechanism that detects
when the scheduler is trying to schedule an infeasible run. Once the
scheduler reaches eandfand lets all threads go, if a timeout oc-
curs, we signal a deadlock; note that at this point, if all threads are
stuck, then this is a real deadlock in the concurrent program andMonitoring Prediction Scheduling
Application Input Exec- Threads Entities Locks Execution Execution Context Number of Execution Patterns Context Average Total Errors
(LOC) ution Length Time Switches Predicted Time A N FSwitches Time per Time
Time Schedules feasible reschedul-
schedule ing
Vector
(1.3K)VectorTest 0.22s 4 12 2 142 0.29s 5 2 0.01s 0 0 2 5 0.19s 1.10s 1
VectorTest1 0.20s 4 12 2 231 0.20s 5 4 0.01s 0 1 3 5 0.33s 1.61s 1
VectorTest2 0.18s 4 12 2 248 0.20s 5 4 0.01s 0 0 4 5 0.23s 1.57s 1
VectorTest3 0.27s 4 12 2 231 0.27s 5 3 0.01s 0 0 3 5 0.21s 1.27s 1
VectorTest4 0.33s 4 12 2 159 0.35s 3 1 0.01s 0 0 1 3 0.35s 1.27s 1
Stack
(1.4K)StackTest 0.21s 4 12 2 136 0.24s 6 2 0.01s 0 0 2 5 0.32s 1.06s 1
StackTest1 0.48s 4 12 2 209 0.58s 6 1 0.01s 0 0 1 6 0.56s 1.35s 1
StackTest2 0.29s 4 12 2 231 0.28s 5 3 0.01s 0 1 2 5 0.35s 1.77s 1
StackTest3 0.17s 4 12 2 265 0.27s 6 3 0.01s 0 0 3 6 0.18s 1.20s 1
StackTest4 0.18s 4 12 2 127 0.18s 4 2 0.01s 0 0 2 4 0.19s 1.04s 1
elevator
(566)data 16.37s 3 65 8 26K 18.94s 899 167 5.24s 0164 3 595 16.78s 30m42s 0
data2 16.77s 5 113 8 54K 16.26s 1428 63 3.87s 059 4 1262 17.12s 11m44s 0
data3 16.37s 5 457 50 329K 16.93s 50K 699 6m48s 42651 6 39K 17.13s 2h52m 0
tsp
(794)map14 0.19s 2 588 2 32M 3m41s 72K 83 1m36s 29 45 9 60K 15.85s 50m01s 0
map14 0.15s 4 652 2 14M 1m58s 22K 168 29.98s 87 70 11 18K 16.16s 23m41s 0
raytracer
(1.5K)SizeA 3.38s 10 80 10 560 3.57s 54 90 0.01s 0 0 90 48 3.57s 5m52s 1
SizeA 3.94s 20 150 20 1.5K 4.03s 106 380 0.03s 0 0380 93 4.14s 26m20s 1
SizeA 6.52s 30 220 30 2.9K 6.08s 150 870 0.09s 0 0870 99 6.75s 1h38m 1
SizeB 36.50s 10 80 10 560 36.62s 59 90 0.01s 0 0 90 44 38.46s 58m16s 1
colt
(29K)dgemm 0.27s 3 12K 0 286K 0.42s 358 0 1.7s 0 0 0 297 - - 0
50X50
Pool 1.2
(5.8K)PoolTest 0.19s 4 28 1 98 0.19s 11 1 0.01s 0 0 1 4 0.17s 0.18s 1
PoolTest1 0.20s 4 29 1 267 0.24s 25 27 0.01s 0 3 24 19 0.24s 8.20s 1
PoolTest2 0.12s 4 15 1 104 0.17s 5 7 0.01s 0 0 7 5 0.20s 1.48s 1
PoolTest3 0.12s 4 14 1 251 0.18s 20 145 0.01s 50 46 36 49 0.20s 20.76s 1
Pool 1.3
(7K)PoolTest 0.25s 4 30 1 100 0.21s 3 0 0.01s 0 0 0 5 - - 0
PoolTest1 0.18s 4 31 1 265 0.25s 14 19 0.02s 613 0 14 - 5.78s 0
PoolTest2 0.12s 4 15 1 112 0.17s 5 6 0.01s 0 6 0 5 - 1.06s 0
PoolTest3 0.12s 4 18 1 250 0.88s 24 99 0.01s 52 47 0 22 - 11.31s 0
Apache
FtpServer
(22K)lgn_script 1m02s 5 67 4 412 1m02s 9 109 0.02s 583 31 9 1m03s 2h16m 5
Table 1: Experimental Results. A, N and F (of Patterns) indicate number of schedules that are, respectively, ‚ÄúAlready appeared in
observed execution‚Äù, ‚ÄúNot feasible‚Äù and ‚ÄúFeasible‚Äù.
hence corresponds to a real bug (though not an atomicity-related
bug). P ENELOPE detects these kinds of deadlocks as well.
6. EVALUATION
We ran P ENELOPE on a benchmark suite of 9 programs, against
several test harnesses and input parameters, and we evaluated it un-
der several criteria: whether it is successful in predicting alternative
schedules with violation patterns, whether it is able to execute the
predicted schedules, the time it takes to predict and schedule execu-
tions, and whether it is able to discover errors by checking the result
of the execution of schedules with violations on test harnesses.
Benchmarks. The benchmarks used are all concurrent Java pro-
grams that use synchronized blocks and methods as means of
synchronization (using synchronized blocks automatically ensures
nested locking for the most part). They include raytracer from
the Java Grande multithreaded benchmarks [1], elevator and
tsp from [18], Vector andStack from Java libraries, Pool
(two different releases) from Apache Commons, Colt , andApache
FtpServer .
The concurrent program elevator simulates multiple lifts in a
building; tsp is a parallel program that solves the traveling sales-
man problem for a given input map; raytracer renders a frame
of an arrangement of spheres from a given view point; Pool is
an object pooling API in the Apache Commons suite; Colt is
an open source library for high performance computing; Apache
FtpServer is a ftp server and by Apache; and Vector and
Stack are Java libraries that respectively implement the concur-
rent vector and the concurrent stack data structures.Test Suites. In Table 1, we provide all the relevant information
about the conditions under which the tests were run, such as in-
put Ô¨Åles and parameters. For elevator , andtsp, the input Ô¨Åles
were included in the benchmarks, and the table indicates which in-
put Ô¨Åle was used for the results. For elevator , the number of
threads was also speciÔ¨Åed in the input Ô¨Åles, and there were no ad-
ditional parameters to be provided by the user. The test harness
fortsp includes an input Ô¨Åle, a given number of threads, and a
script would compare the minimum tour computed by the program
against the minimum tour computed by a single thread execution.
ForVector (respectively Stack ), we wrote test harnesses with
two threads and two small vectors (respectively stacks), where each
thread executes exactly one method from class Vector (respec-
tively Stack ). All the scheduled runs which manifested real errors
raised exceptions during the execution. The test cases for Pool
were designed similarly, that is for each test case two different
methods were run concurrently on the same object pool. There was
no particular test harness check, and the error in Pool 1.2 man-
ifested as an exception. The same bug was Ô¨Åxed in Pool 1.3 .
Forcolt , thedgemm command was used which invokes a ma-
trix multiplication routine on matrices of size 5050. The test
harness was designed to check the result from a multi-threaded ex-
ecution against the result from a single threaded execution. For
raytracer , all the data is already incorporated in the benchmark
which comes in two sizes A, and B. The only parameter that the
user speciÔ¨Åes is the number of threads. raytracer has a built-in
validation test. In the case of FtpServer , we wrote a test harness
with a client and a server where the client logs in and requests a
connection; the errors manifested as exceptions.Experimental Evaluation. Table 1 demonstrates the result of the
evaluation of Penelope on the benchmarks. The table provides
information about all three phases: monitoring, prediction, and
scheduling. For the monitoring phase, the number of threads, en-
tities (variables), locks, and the length of execution is reported,
as well as how long the execution takes, and how many context
switches exist in the observed run. For the prediction phase, we
report how many violations were discovered (total over all 5 pat-
terns), and how long the prediction phase takes. In the schedul-
ing part, we report the number of violation patters (out of the total
reported in the prediction column) existed in the original run ob-
served (A), the number of violations for which the schedules were
not feasible (N), and the number of violations which appeared in
a successful alternative schedule generated by P ENELOPE (F). We
also report how many context switches (on average) there are in the
alternative feasible schedules, what is the average time per feasi-
ble schedule (a reasonable indication of overhead), and Ô¨Ånally and
most importantly, how many realerrors were found.
PENELOPE Ô¨Ånds several bugs in the benchmarks. The bug in
raytracer is caused by an atomicity violation involving the Ô¨Åeld
JGFRayTracerBench.checksum1 due to wrong synchroniza-
tion. The error in Pool 1.2 (which was Ô¨Åxed in Pool 1.3 )
is caused by an atomicity violation on the variable _factory in
methods borrowObject ,returnObject , ran in parallel with
method close , and in methods addObject andborrowObject
ran in parallel with method setFactory . Note that these are 4
different errors, and they all manifested as exceptions during alter-
native feasible schedules exercised by P ENELOPE .
All bugs in Vector andStack are the result of an atomic-
ity violation that causes the size of a parameter collection to go
stale in the middle of an operation (such as addAll ) that is us-
ing the parameter collection as a source of information, while only
the destination vector/stack is synchronized properly. There were
5 discovered errors for FtpSever (each giving a different ex-
ception), which correspond to variables m_currConnections ,
m_writer ,m_name ,m_currLogins ,m_request all accessed
in method RequestHandler.run of the server while the timer
thread interrupts by closing the connection as a result of a timeout.
PENELOPE predicts several atomicity violating schedules in tsp
andelevator , but they all pass the test harness, and in fact are
not errors (the violation of atomicity was intended and correct).
Observations: Here are some observations we gather from these
experiments on the effectiveness and performance of P ENELOPE :
The number of predicted schedules is small ; in fact, a tiny frac-
tion of all possible executions. This is true even compared to
the number of all runs limited to just two context switches (or
preemptions), as C HESS would do. For instance, in elevator
(data3), there are close to 50,000 points (releases of locks) in 4
threads where preemptions can happen, giving around 15 billion
possible schedules involving just 2 preemptions!
PENELOPE is effective in Ô¨Ånding bugs . We ran the programs
under the test harness several times, and did not Ô¨Ånd (almost)
any of the reported bugs in any of these benchmarks by merely
running tests randomly. It is clear that a more focused approach
is absolutely necessary in Ô¨Ånding errors on these benchmarks.
Despite its small selection of schedules to test, P ENELOPE was
able to identify bugs in these programs.
Reasonable time overhead . The runtime overhead in precisely
scheduling the alternate executions is not prohibitively high, and
is in fact very minimal in most examples. This is despite the
large number of context-switches that are being exercised (284K
context-switches in elevator-data3).PENELOPE Ô¨Ånds bugs under complex scenarios . Note that the
number of context-switches scheduled in the predicted execu-
tions are very high. We believe that this allows P ENELOPE to dig
deep into the search space of the runs. Tools like C HESS execute
all runs with a few context-switches and offer a complementary
search strategy [13].
Zero false positives . If a bug is reported by P ENELOPE , it is a real
bug (i.e. an execution that violates the test harness). A signiÔ¨Å-
cant amount of violations found did not correspond to real bugs,
and are not reported as bugs. This is in contrast to similar tools
based on atomicity checking by Wang and Stoller [20], Farzan et
al [5], and the tools S IDETRACK [23], A TOM FUZZER [15] and
VELODROME [7]. V ELODROME in fact reports atomicity viola-
tions for benchmarks elevator ,tsp, andcolt , though they
donotcorrespond to bugs.
7. REFERENCES
[1]http://http://www.javagrande.org/ .
[2]http://jakarta.apache.org/bcel/ .
[3] F. Chen, T.F. Serbanuta, and G. Rosu. jpredictor: a predictive runtime
analysis tool for java. In ICSE , pages 221‚Äì230, 2008.
[4] A. Farzan and P. Madhusudan. Monitoring atomicity in concurrent
programs. In CAV, pages 52‚Äì65, 2008.
[5] A. Farzan, P. Madhusudan, and F. Sorrentino. Meta-analysis for
atomicity violations under nested locking. In CAV, pp 248‚Äì262, 2009.
[6] C. Flanagan and S. N Freund. Atomizer: a dynamic atomicity
checker for multithreaded programs. In POPL , pages 256‚Äì267, 2004.
[7] C. Flanagan, S. N. Freund, and J. Yi. Velodrome: a sound and
complete dynamic atomicity checker for multithreaded programs. In
PLDI , pages 293‚Äì303, 2008.
[8] C. Flanagan and S. Qadeer. A type and effect system for atomicity. In
PLDI , pages 338‚Äì349, 2003.
[9] J. Hatcliff, Robby, and M. Dwyer. Verifying atomicity speciÔ¨Åcations
for concurrent object-oriented software using model checking. In
VMCAI , pages 175‚Äì190, 2004.
[10] V . Kahlon, F. Ivancic, and A. Gupta. Reasoning about threads
communicating via locks. In CAV, pages 505‚Äì518, 2005.
[11] R. J. Lipton. Reduction: a method of proving properties of parallel
programs. Commun. ACM , 18(12):717‚Äì721, 1975.
[12] S. Lu, S. Park, E. Seo, and Y . Zhou. Learning from mistakes: a
comprehensive study on real world concurrency bug characteristics.
InASPLOS , pages 329‚Äì339, 2008.
[13] M. Musuvathi and S. Qadeer. Iterative context bounding for
systematic testing of multithreaded programs. In PLDI , pages
446‚Äì455. ACM, 2007.
[14] C. Papadimitriou. The theory of database concurrency control .
Computer Science Press., 1986.
[15] C-S Park and K. Sen. Randomized active atomicity violation
detection in concurrent programs. In SIGSOFT ‚Äô08/FSE-16 , pages
135‚Äì145, New York, NY , USA, 2008. ACM.
[16] S. Park, S. Lu, and Y . Zhou. Ctrigger: exposing atomicity violation
bugs from their hiding places. In ASPLOS , pages 25‚Äì36, 2009.
[17] S., J. Tucek, F. Qin, and Y . Zhou. Avio: detecting atomicity violations
via access interleaving invariants. In ASPLOS , pages 37‚Äì48, 2006.
[18] C. von Praun and T. R. Gross. Object race detection. SIGPLAN Not. ,
36(11):70‚Äì82, 2001.
[19] C. Wang, R. Limaye, M. Ganai, and A. Gupta. Trace-based symbolic
analysis for atomicity violations. In TACAS , 2010.
[20] L. Wang and S. D. Stoller. Accurate and efÔ¨Åcient runtime detection
of atomicity errors in concurrent programs. In PPoPP , pages
137‚Äì146, 2006.
[21] L. Wang and S. D. Stoller. Runtime analysis of atomicity for
multi-threaded programs. IEEE Transactions on Software
Engineering , 32:93‚Äì110, 2006.
[22] M. Xu, R. Bod√≠k, and M. D. Hill. A serializability violation detector
for shared-memory server programs. SIGPLAN Not. , 40(6):1‚Äì14,
2005.
[23] J. Yi, C. Sadowski, and C. Flanagan. Sidetrack: generalizing
dynamic atomicity analysis. In PADTAD , pages 1‚Äì10, 2009.