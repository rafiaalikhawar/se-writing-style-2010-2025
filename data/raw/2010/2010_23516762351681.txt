Practical Isolation of Failure-Inducing Changes for
Debugging Regression Faults∗
Kai Yu1,2, Mengxiang Lin1,3, Jin Chen1,4, and Xiangyu Zhang1,2
1State Key Laboratory of Software Development Environment, Beihang University, China
2School of Computer Science and Engineering, Beihang Univer sity, China
3School of Mechanical Engineering and Automation, Beihang U niversity, China
4School of Software, Beihang University, China
{yukai|mxlin|chenjin|zhangxy}@nlsde.buaa.edu.cn
ABSTRACT
During software evolution, new released versions still con -
tain many bugs. One common scenario is that end users
encounter regression faults and submit them to bug track-
ing systems. Diﬀerent from in-house regression testing, ty p-
ically only one test input is available, which passes the old
version and fails the modiﬁed new version. To address the is-
sue, delta debugging has been proposed for failure-inducin g
changes identiﬁcation between two versions. Despite promi s-
ing results, there are two practical factors that thwart the
application of delta debugging: a large number of tests and
misleading false positives. In this work, we present a combi -
nation of coverage analysis and delta debugging that auto-
matically isolates failure-inducing changes. Evaluation s on
twelve real regression faults in GNU software demonstrate
both the speed gain and eﬀectiveness improvements. More-
over, a case study on libPNG and TCPﬂow indicates that
our technique is comparable to peer techniques in debugging
regressions faults.
Categories andSubject Descriptors
D.2.5 [ Software Engineering ]: Testing and Debugging—
Debugging aids, Testing tools, Tracing
General Terms
Algorithms, Experimentation, Reliability, Veriﬁcation
Keywords
Regression fault, delta debugging, coverage analysis, au-
tomated debugging, ﬁeld failure
1. INTRODUCTION
During software evolution, new released versions still con -
tain many bugs [6], especially regression faults (regressi on
∗Mengxiang Lin is the corresponding author.for short). For example, 455 regressions are created dur-
ing 2007 in the Apache Software Foundation bug tracking
database [18]. Many regressions are later found by end users
(which are also known as ﬁeld failures [14]) and submitted
to bug tracking systems. A typical bug report provides the
version number of the buggy program, the failure-revealing
input and the unexpected results. Users sometimes directly
point out that regressions are encountered: such as “testin g
vanilla coreutils-6.7 and coreutils-6.8 on x86/Linux show s
that 6.7 behaves properly but 6.8 does not”, “2.6.3 --includ e
doesn’t work as 2.5.4 does” and so on.
Debugging regressions found in today’s deployed software
is nontrivial. First, software is increasingly complex and re-
gression faults manifest themselves in speciﬁc environmen ts
and conﬁgurations. Second, diﬀerent from in-house regres-
sion testing, usually only one test input tis available, which
passes the old version Pand fails the modiﬁed new version
P′. Moreover, developers, which are always hurried and
overburdened, are unwilling to ﬁnd more failing cases.
To address the issue, recently two techniques (named as
Darwin [23] and Golden [10] respectively) are proposed,
which tries to explain the failure of tinP′. Darwin ﬁrst
computes the path conditions fand f′oftinPandP′.
Then the formula f∧f′is solved, aiming at ﬁnding an al-
ternative input, which follows the same path as tinPbut a
diﬀerent path than tinP′. Comparing control ﬂow behav-
iors of tand its alternative inputs could help to locate fault-
s. However, due to the limitations of the constraint solver,
usually only under-approximation path conditions could be
obtained. As a consequence, a solution to f∧f′may not
satisfy the required properties and thus is an invalid input .
Moreover, if fand f′are found to be logically equivalent,
then no inputs could be generated and Darwin could not be
employed. Golden requires developers to manually provide a
postcondition, and then computes the weakest precondition
(WP) of each version. Then WPs are compared to ﬁnd un-
explained formulae, which are later mapped to source code
as bug reports. Since both Darwin and Golden are built
on top of BitBlaze [24] binary analysis framework, they cir-
cumvent the limitation of library functions. However, as th e
information of external libraries and environment are not
separated, massive computation are spent to process all thi s
information. Speciﬁcally, frequent usage of libraries dur ing
the execution and the ability of constraints solver threats to
scalability and eﬃciency to those approaches.Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
ASE’12, September 3–7, 2012, Essen, Germany
Copyright 2012 ACM 978-1-4503-1204-2/12/09 ...$15.00
20
One way is to focus on the eﬀects of changes rather than
the whole program. Of particular interest for our work is
delta debugging [8][30][32], which systematically search ing
for failure-inducing changes by patching a subset of change s
and observing execution results. Delta debugging was intro -
duced ﬁrst as DDMin [30] for simplifying failure-inducing
changes. Later, DDMin was extended to DD [32]. DD tries
to isolate the minimal diﬀerence between the two versions.
Since DD works on two sets at a time while DDMin performs
simpliﬁcation in one set, DD is more eﬃcient than DDMin
in general [31]. We use the isolation delta debugging algo-
rithm DD in this work. In our previous work, we evaluated
DD in practical settings using real regressions taken from
mid-sized open source systems [28]. Two thirds of isolated
changes of DD in the systems analyzed provide direct or indi-
rect clues in locating regressions. Despite promising resu lts,
there are two practical factors that thwart the application of
DD. First, DD usually requires hundreds or even thousands
of tests1(including compilation and execution) to identify
faulty elements [21]. However, building large application s
takes minutes to hours, which may worsen the eﬃciency of
DD. For example, a full build of Firefox costs about two
hours [1]. Second, as there may be changes which could also
lead to the same behavior as that of the failure, but are not
root causes, DD sometimes return “alternate causes” [31],
or known as “false positives” in the context of bug-ﬁnding.
With respect to developers, after a comparatively long wait ,
they easily wear out their patience if they still have to re-
view a long list of suspected changes with only several being
failure-relevant. Therefore, developers are often frustr ated
and prefer backing to manual debugging.
Our goal is to develop a practical technique for debug-
ging regressions. A practical technique is available, reli able
and eﬃcient in realistic settings. In this work, we intro-
duce a practical and lightweight technique to identify fail ure-
inducing changes. First, statement coverage is collected a nd
analyzed to identify unexecuted changes. The resulting ex-
ecuted changes constitute a dubious set, which is more rele-
vant to the regression and used as the initial searching area .
The beneﬁts are twofold: 1) a smaller searching area often
requires fewer tests, which improves the eﬃciency of faulty
changes identiﬁcation; 2) a more relevant area could lead to
more precise results, which are useful in debugging. Howev-
er, since DD requires the initial set to be a failing set, it is
inappropriate to apply DD to ﬁnd failure-inducing changes
in the dubious set directly. To address the issue, we propose
an algorithm named Augmented Delta Debugging (ADD)
for searching failure-inducing elements in the reﬁned set. In
contrast to DD, which isolates a minimal diﬀerence between
the two versions, our approach minimizes the set of suspi-
cious changes. Contributions of this paper are as follows:
◮We introduce an approach which combines coverage
analysis and delta debugging for failure-inducing changes i-
dentiﬁcation. Conceptually, it improves upon DD by (1) in-
troducing the concept of dubious changes, i.e., changes tha t
are actually covered by a test case, (2) devising a variant of
delta debugging named ADD, which performs minimization
1In the context of delta debugging, a test consists of con-
structing a patched program and determining whether the
failure occurs. Speciﬁcally, delta debugging requires man y
tests to search for failure causes, running the same test cas e.inthe dubious area . As demonstrated in evaluation, ADD
both reduces the number of tests yet is able to return less
false positives.
◮We carried out experiments on twelve real regressions in
GNU software to investigate the feasibility and performanc e
of our approach. Our approach is usually 10 times faster yet
more eﬀective than DD. Moreover, a case study on libPNG
and TCPﬂow demonstrates that our technique is comparable
to peer techniques in debugging regressions.
◮This work aims at helping developers to easily create an
eﬀective lightweight tool for locating regression bugs. Ou r
prototype, a detailed guide and subjects used are publicly
available online: http://code.google.com/p/iregression/
The rest of this paper is organized as follows. Section 2
describes our approach and Section 3 presents experimental
study. Section 4 discusses issues of our method and Section
5 concludes.
2. APPROACH
As described in Section 1, in the scenario of debugging
regressions, the starting point is an old working program P,
a modiﬁed broken program P’, and a test case twhich passes
inPand fails inP’. Given this information, our approach
comprises three steps: execute programs and collect traces
(Section 2.1), analyze traces and obtain executed changes
(Section 2.2), and employ ADD to isolate failure-inducing
changes (Section 2.3).
2.1 Execute Programs
LetS={si}andS′={s′
j}represent the statements in
PandP’ respectively. The set of changes from PtoP’,
termedC={(si, s′
j)|si∈S , s′
j∈S′}, describes a series of
replacements of statement siinPbys′
jinP’. Particularly,
an addition operation of statement s′
jis a replacement ( ◦, s′
j)
while a delete operation of siis a replacement ( si,◦), here◦
stands for the corresponding point. In the ﬁrst step, both P
andP’ are executed with the same test input tand traces
T(P)⊆S ,T(P′)⊆S′are collected.T(P) andT(P′) con-
tain the set of code executed by tinPandP’ respectively.
2.2 AnalyzeTraces
The size of setCis often too large to be eﬀective for search-
ing failure-inducing changes by DD (requiring in practice
hundreds or thousands of test executions, as shown in Sec-
tion 3). Our idea is thus to eliminate those changes that
arenotlikely to induce the regression from Cand provide a
more relevant set to perform isolation.
According to the PIE model [26], a change causes the
diﬀerence of program behaviors if and only if the change ex-
ecutes, infects the state and the infection propagates to th e
output. Therefore, an unexecuted change could not induce
the failure and should be removed from consideration. Let
U={(si, s′
j)|(si, s′
j)∈C, si/\e}atio\slash∈T(P), s′
j/\e}atio\slash∈T(P′)}represent
the set of unexecuted changes, i.e., whose statements are no t
executed in neither PnorP′. After excluding such changes,
the diﬀerence set C\U is regarded as a dubious setD, which
contains failure-inducing changes.21Figure 1: Applying DD to ﬁnd the failure-inducing
changes: c4 and c5. Test results are “PASS” ( /check),
“FAIL” (×) or “UNRESOLVED” ( ?).
2.3 IsolateChanges
In this stage, a novel searching algorithm ADD is devel-
oped to minimize failure-inducing changes in the dubious
change setD. We ﬁrst illustrate both DD and ADD on an
example and then give the algorithm description of ADD.
2.3.1 IllustrativeExample
Figure 1 illustrates DD on a simple example. The failing
setFconsists of eight changes initially. In the context of de-
bugging regressions, a failing set Fmeans patching changes
inFto the original program Presults in the regression fail-
ure (denoted as “FAIL” ( ×) in Figure 1). Initially, Fis
the set of changes between PandP′, i.e., the setCdeﬁned
above. Whether a change is patched to Por not is illustrated
by the black or white box, respectively. In this example, the
failure occurs only if the two changes c4 and c5 are patched
with change c3. Both c4 and c5 are regression causes but a
patch includes them will lead to an unresolved result (e.g.,
a compilation error) in the absence of c3. Any patch that
includes only c4 or c5 results in an unresolved result (de-
noted as “UNRESOLVED” (?) ); any patch that includes c6
but not c3 also leads to an unresolved outcome; otherwise, a
patched program passes the test (denoted as “PASS” ( /check)).
As illustrated later in Section 3.5.2, dependences of chang es
in real scenarios are usually more complicated than this ex-
ample. The columns of c3, c4, c5 and c6 are highlighted.
In Figure 1 (a), DD starts isolating from Fand identiﬁes
failure-inducing changes c4 and c5 (shown in the red part)
after sixteen steps by comparing the diﬀerence between the
smallest failing patch (obtained in Step 12) and largest pas s-
ing patch (obtained in Step 16).
Suppose, in Figure 1, we obtain a dubious set D={c4, c5,
c6, c7}(shown in the yellow part) after coverage analysis.
Further suppose DD is employed here to identify failure-
inducing changes in D. The steps and results are presented
in Figure 1 (b). Since c6 depends on c3, which is not includ-
ed inD, any patch that includes c6 leads to an unresolved
result in the absence of c3. After ten steps, only a single-
element passing set is obtained. and ﬁnally the dubious set
Figure 2: Applying ADD to ﬁnd the failure-inducing
changes: c4 and c5. Test results are “PASS” ( /check) or
“UNRESOLVED” ( ?).
Dis reduced to three elements. This example shows that
DD is inappropriate for isolating an unresolved set.
We propose ADD algorithm to address the issue and il-
lustrate it in Figure 2: the dubious set Dis divided into two
halves ﬁrst. By patching all changes in Fexcept the ﬁrst
half{c4,c5}, a passing patched program is obtained in Step
1 and now the dubious set Dis{c4,c5}. By again splitting
current dubious set into two halves, ADD patches changes in
each subset and their complement set in the rawfailing set
F, and it ﬁnishes in ﬁve steps since the largest granularity
is reached. Finally, changes in the ultimate suspicious set
are regarded as the failure causes, and both c4 and c5 are
identiﬁed.
2.3.2 AugmentedDelta DebuggingAlgorithm
Given a dubious set D, it is infeasible to invoke DD di-
rectly. To handle the situation, Dcan be treated as a fail-
ing set. As illustrated in Figure 1 (b), without necessary
changes (e.g., c3), patching the initial dubious set Dleads
to an unresolved result. Moreover, removing any changes in
Dmay also result in unresolved test outcomes (e.g., the ﬁrst
six tests in Figure 1 (b)) and the regression causes cannot
be identiﬁed precisely.
The occurrence of unresolved results is due to dependences
between changes [28]. Dependences in real programs are
complicated and considerable. For instance, a statement of -
ten has many dependences through control ﬂow and data
ﬂow. Worse, we could not know any dependency in ad-
vance without necessary computation. This prevalent phe-
nomenon greatly aﬀects the performance of DD and presses
for solution.
To alleviate the issue, a natural idea is that the more
changes are patched to P, the greater the likelihood that a
determined test result is produced. Speciﬁcally, every tim e
ADD splits current dubious setDinto subsets, the subset
and its complement in the rawfailing setFshould be tested
(e.g., Step 1, 2 and 4 in Figure 2). Since patching the raw
failing setCresults in the failure, then if we remove only
a small changes of it, the test result of the complement set
may also be determinate. Moreover, if the failure causes
are excluded, then a passing patched program is obtained
and the causes are identiﬁed. By comparison, DD patches
each subset’s complement in current failing set, which is not
suitable for performing isolation in Das illustrated.22Algorithm 1: Add(D,F,n)
The Augmented Delta Debugging Algorithm
Input : A dubious setD/\e}atio\slash=∅, a failing setF, n/greaterorequalslant2
Require :D⊂F ,Test(∅) =PASS ,Test(F) =FAIL
Output : A failure-relevant subset of D
1 if|D|== 1 then
2 returnD
3{di}n
i=1←Partition (D, n)
// SplitDinto nsets,D=d1∪d2∪. . .∪dn
//∀di, dj, di∩dj=∅,|di|≈| dj|≈(|D|/n)
4 for i←1tondo
5 if Test(F\ di) == FAILthen
6 return Add(D\ di,F,max( n−1,2))
7 else if Test(di) == PASS then
8 return Add(D\ di,F,max( n−1,2))
9 else if Test(di) == FAILthen
10 return Add(di,F,2)
11 else if Test(F\ di) == PASS then
12 return Add(di,F,2)
13 end
14 end
// if only unresolved results are produced
15 if n <|D|then
// increase granularity
16 return Add(D,F,min(2 n,|D|))
17 else
// return results
18 returnD
19 end
Algorithm 1 shows our Augmented Delta Debugging Al-
gorithm. The inputs include a dubious set D, a raw failing
setFand the granularity n. The algorithm shares the same
workﬂow of DD in principle: it ﬁrst splits current dubious
set into nsubsets with all subsets pairwise disjoint, then
tries to narrow down the set according to testing results. If
only unresolved results are produced in this round, then the
granularity will be increased and ADD is invoked again. If
the granularity cannot be increased, then the current du-
bious set is treated as the set of failure-inducing changes
and the algorithm terminates. A testing function Test () is
needed to patch current changes to Pand determine test
results. ADD is invoked by Add(D,C,2) initially. Every
time ADD splits current dubious set into subsets (in line
3), then each subset and its complement in the rawfailing
set are tested. According to test results, the dubious set
can be further narrowed down. For example, if patching the
complement set still leads to the failure (in line 5), then th e
corresponding subset is failure-irrelevant; while if patc hing
the complement passes (in line 11), then the failure-releva nt
changes must be included in the subset. If all the tests are
unresolved, then the granularity will be increased (in line 16)
until no more subsets could be generated. A failure-relevan t
set is returned in the end.
3. EVALUATIONS
3.1 Subjects
The ultimate goal of our technique is to guide the de-
veloper to a narrowed area of code, which is suspicious of
being faulty. We want to perform evaluation in a realisticsetting that mimics the scenario of encountering regressio n
faults. Existing research bug benchmarks (e.g., iBugs [15] )
contain few if any real regressions (focusing on explicit bu gs
instead). Subjects in SIR [17] are not used since debugging
regression faults for SIR programs are usually trivial. Thi s
is because the diﬀerence between two SIR program versions
is usually small. In this work, we analyze twelve previously
documented regressions in GNU Software, taking from a re-
cently published benchmark [28]. The benchmark provides
a regression-revealing input together with both the workin g
and the broken versions for each regression. Moreover, all
the regressions have been ﬁxed. Three regression faults in
[28] are excluded due to the limitation of our current pro-
totype (e.g., some regressions lie in yacc ﬁles and we could
not collect coverage information of such ﬁles).
Twelve regressions of ten applications are shown in Table
1. As we can see, these programs are widely used medium-
sized applications. bash is an sh-compatible shell, bcis an
arbitrary precision calculator, diff shows diﬀerences be-
tween two ﬁles, find searches for ﬁles in a directory hier-
archy, gawk is the GNU implementation of awk, grep prints
lines that match a pattern, indent is a program for for-
matting C code, lslists information about ﬁles, make is a
tool to organize a build process and taris an archiver tool.
Both the adjacent working and broken version numbers of
the programsPandP’ along with the description of the
regressions are given. The sites and the Internet addresses
of bug reports are also listed.
3.2 Setup
Some unrelated diﬀerences (such as changes in documents)
betweenPandP’ were not taken into account. Our proto-
type consists of four components: GNU diff utilities, gcov,
ADD module and a Test module. GNU diff is used to
compare the source code of PandP’, and the changes are
regarded asC, i.e., the failing set F. The code coverage
tool gcov is used to ﬁnd out which lines of code are actu-
ally executed2and then the dubious set Dis determined.
ADD is implemented in python. The Test module is used
to build patched programs and determine test outcomes by
ﬁles comparison or searching for regression indicators in o ut-
put. Since patched programs may hang during the iteration,
the module uses a process controller to deal with such sit-
uation and outputs an unresolved result in such case. The
experiments are carried out on an IBM X3650 server with
two Intel Xeon quadcore processors at 2.00 GHz and 4GB
physical memory running CentOS 4.7.
3.3 Peer Techniques
We want to compare our approach with techniques or tool-
s that address the same issue in the same scenario. There
are two recent papers [10][23] about debugging evolving pro -
grams, trying to explain the failure of tinP′through seman-
tic analysis simultaneously of both versions. Both methods
use the BitBlaze [24] binary analysis framework as the un-
derlying platform. Speciﬁcally, concrete execution is car ried
out by the TEMU component and symbolic computation is
performed by the VINE component. We chose a relative s-
2For program indent , the guilty changes are macro deﬁni-
tions. Since gcov cannot tell whether such lines are execut-
ed, these changes are included in Ddirectly.23Table 1: Regression faults used in the study.
Program LOC Ver( P) Ver(P’) Regression Description Site Report URL
bash 97K 3.2.48 4.0 Parsing error [2] bug-bash/2009-03/msg 00018.html
bc 10K 1.05a 1.06 Argument processing error [3] show bug.cgi?id=51525
diﬀ 20K 2.8.1 2.9 Adds additional newline [4] cgi-bin/bugre port.cgi?bug=577832
ﬁnd(a) 24K 4.2.15 4.2.18 Using -L/-H produces wrong output [ 5] bugs/?12181
ﬁnd(b) 40K 4.3.5 4.3.6 Using -mtime produces wrong output [5 ] bugs/?20005
ﬁnd(c) 40K 4.3.5 4.3.6 Using -size produces error message [5 ] bugs/?30180
gawk 37K 3.1.0 3.1.1 Use of strtonum causes abort [4] cgi-bin /bugreport.cgi?bug=159279
grep 6K 2.5.4 2.6 Using --include produces wrong output [5] b ugs/?29876
indent 15K 2.2.9 2.2.11 Adds too many newlines [5] bugs/?270 36
ls 87K 6.7 6.8 Using -x produces wrong output [2] bug-coreuti ls/2007-04/msg00000.html
make 23K 3.80 3.81 Using -r produces wrong output [5] bugs/?2 0006
tar 21K 1.13.25 1.13.90 Wrong uid display [2] bug-tar/2004- 10/msg00034.html
mall subject diff for experimentation. After the concrete
execution, TEMU generated a trace more than 200 MB for
each version, consisted of more than 2.5 million instructio ns
executed by the program. However, after twenty minutes,
VINE failed to convert the instruction traces into VINE in-
termediate representation (IR). If IR traces could not be
obtained, the weakest precondition calculation could not b e
calculated and later semantic analysis in [10][23] could no t
be performed to locate root causes of the bug. It is worth
noting that there are two releases of BitBlaze and we used
the open one since the internal one is not public available.
Diﬀerent releases may lead to diﬀerent results.
If we want to employ spectra-based fault localization [7][2 9]
[33], usually multiple test inputs are required. If only one
failing and one passing executions are available, Chao et al .
propose a spectra-based technique named BayesDebug [19]
to identify faulty predicates. However, it is not designed
for debugging regressions and it does not explicitly consid -
er the semantics of the changes between two versions. A
predicate inPmay be renamed or even changed its condi-
tion inP′, and it is meaningless to compare their evaluation
patterns in that case. To our knowledge, there is a study
[9] addresses the issue of lacking test cases by directed tes t
generation using symbolic execution. It creates new test in -
puts that are similar to the failing execution in order to aid
fault localization. A recent empirical study [25] implemen ts
one prototype on top of the KLEE [11] engine and examines
its eﬀectiveness on twenty real bugs of grep,make and tar.
It is reported that the original failing runs are too complex
for KLEE to handle, causing the system to time out and
resulting the approach performs poorly. Moreover, the im-
plementations of all the techniques discussed above are not
publicly available.
We are aware of two toolkits that could be obtained from
Internet. One is RPrism3, which performs regression cause
analysis and was applied to large Java applications. The
other is DDexpr4, which implements both DD and HDD
(Hierarchical Delta Debugging) [21] and was applied to the
subjects in Table 1. Moreover, the intermediate results cou ld
also be downloaded publicly. Detailed description of the em -
pirical evaluation could be found in [28]. Therefore, we de-
3http://www.kevinjhoffman.com/rprism/
4http://code.google.com/p/ddexpr/cided to compare our method with the techniques evaluated
in [28]. We should point out that “git bisect” [20] is able to
ﬁnd failure-inducing changes by binary search. However, as
surveyed in [28], the development history of most subjects
in Table 1 is not available and thus “git bisect” could not
always be employed.
3.4 Efﬁciency
We now compare our results with both DD and HDD as
summarized in Table 2. The columns |F|and|D|show the
number of changes in the failing and dubious set, respective -
ly. #Test, #Un and #Iso list the number of tests required,
unresolved test results, the number of changes isolated by
DD, HDD or ADD. The columns #FP and #FN also pro-
vide the number of false positives and false negatives in the
isolated changes.
To assess the improvements in eﬃciency, we deﬁne two
measures: #Speedup=#Test(DD)
#Test(ADD)and
#Un↓=#Un(DD)-#Un(ADD)
#Un(DD)×100%. #Speedup is relative
to the number of tests while #Un ↓shows the percentage of
decreased unresolved tests from DD to ADD.
First, we observe that Dis usually signiﬁcantly smaller
thanFand the areas to search for are reduced. The size of D
is less than 10% to that of Ffor two thirds (8/12) programs.
For both find(c) and gawk, there is a dramatic drop in the
number of changes, that is, from 244 to 3 and from 701 to 2.
Second, almost all the numbers of isolated changes by ADD
are no more than that by DD or HDD. For find(b) ,grep
andls, identiﬁed changes are greatly reduced. Third, ADD
achieves speedups of more than 10x vs. DD for two thirds
(8/12) programs. For gawk, ADD is 90X faster than DD.
Last but not least, the number of unresolved tests decreases
signiﬁcantly and the decrease percentage ranges from 72.1%
to even 100.0%. In our evaluation, only executed changes
are regarded as failure-relevant. In fact, if developers ar e
not sure whether a change is faulty or not, they can keep it
in the dubious set Dand invoke ADD for isolation.
3.5 Effectiveness
Since all the bugs have been ﬁxed, we could measure the
usefulness of identiﬁed changes by comparing them with the
patches used in real world. As we can see from Table 2, too
many false positives and false negatives occur in the result s24Table 2: Results of failure-inducing changes identiﬁcatio n.
Changes DD results HDD results ADD results Improvement
Program |F| |D| #Test #Un #Iso #FP #FN #Test #Un #Iso #FP #FN #Test #Un #Iso #FP #FN #SpeedUp #Un ↓
bash 1249 68 440 397 1 0 0 1477 1345 2 1 0 79 73 2 1 0 5.6X 81.6%
bc 420 15 174 163 1 0 0 486 450 1 0 0 13 8 1 0 0 13.4X 95.1%
diﬀ 372 59 330 297 3 2 0 699 624 1 0 0 21 13 1 0 0 15.7X 95.6%
ﬁnd(a) 72 19 48 40 1 0 0 140 113 11 10 0 20 16 1 0 0 2.4X 60.0%
ﬁnd(b) 244 201197 1102 114 112 01178 1114 124 122 0 68 60 12 10 0 17.6X 94.6%
ﬁnd(c) 244 3 85 76 2 1 0 103 92 2 1 0 4 1 1 0 0 21.3X 98.7%
gawk 701 2 271 252 1 0 0 117 94 1 0 0 3 0 1 0 0 90.3X 100.0%
grep 596 532582 2424 114 113 01027 893 51 50 0184 159 21 20 0 14.0X 93.4%
indent 802 47 952 824 2 0 0 745 635 2 0 0 67 60 2 0 0 14.2X 92.7%
ls 73 13 435 406 53 51 0387 361 51 49 0 51 47 11 9 0 8.5X 88.4%
make 1292 269 2052 1893 1 1 14381 4113 171 170 0587 529 64 63 0 3.5X 72.1%
tar 619 51 596 546 1 0 0 530 492 1 0 0 55 48 1 0 0 10.8X 91.2%
of DD and HDD. Therefore, we do not use the common mea-
sures (such as precision, recall, F-measure) here to compar e
the performance of ADD with DD and HDD since it is obvi-
ous ADD outperforms them on the benchmark. Instead, we
classify the results into three categories (false negative s, too
many false positives and true positives) and discuss them
in detail. For space reason, here we just compare results of
DD and ADD, detailed results of HDD can be found in the
website of DDexpr.
3.5.1 FalseNegatives
All the changes isolated are failure-relevant, except the
change isolated by DD for program make (shaded in black).
The real ﬁx (not shown here) removed the check of f->is_target
while the identiﬁed change (shown in Table 3) sheds light
oncheck_lastslash . The diﬀerences between the two ﬁles
have the indicator ‘+’ or ‘-’, which means lines are added
to or removed from the ﬁrst ﬁle, respectively. The isolat-
ed change seems to be unrelated with the failure. To vali-
date our hypothesis that this change could induce the same
behavior as that of the failure but is failure-irrelevant, w e
carried out two experiments. In one experiment, only the
change was applied to the old working program while in an-
other experiment, only this change was reverted in the new
broken version. Both two programs failed the test, which
demonstrated that the change could lead to the same abnor-
mal behavior but was not the root cause (i.e., this change is
a “false negative”). After excluding the change, we reran D-
D. This time DD isolated 182 changes after 4473 executions
and the failure-inducing change was identiﬁed. By compar-
ison, this faulty change is isolated by ADD at the ﬁrst time
among 64 changes identiﬁed. Some articles [28][31] have dis -
cussed why DD occasionally produces false negatives.
3.5.2 TooManyFalsePositives
For the left eleven programs, as we can see from Table 2,
find(b) ,grep and ls(shaded in gray) have many changes
isolated by DD. Close scrutiny reveals almost all identi-
ﬁed changes of these three regressions are failure-irrelev ant
(i.e., false positives). We ﬁrst explain the results of pro-
gram find(b) . After manually examination, we found two
changes (shown as c3 and c4 in Table 4) are failure causes.
According to c3, in the function get_comp_type , the val-
ue of the ﬁrst parameter is modiﬁed and returned. There-
fore, the variable sin the working program and the variableTable 3: The isolated change of make by DD.
--- make-3.80/implicit.c
+++ make-3.81/implicit.c
@@ -235 +354,9 @@
- check_lastslash = lastslash != 0 && strchr (tar-
get, ’/’) == 0;
+ check_lastslash = strchr (target, ’/’)
== 0;
+#ifdef HAVE_DOS_PATHS
+...
+#endif
timearg in the buggy program are reassigned. However,
due to the change c4, the modiﬁed variable is used in the
function get_relative_timestamp , which leads to the re-
gression. Because of dependency, some changes that seem
to be failure-irrelevant are also isolated by DD. For exam-
ple, changes that declare (c1) and initialize (c2) timearg
are identiﬁed. Worse, these changes also depend on other
changes. The regression occurs only if all these changes are
applied together. Any subset of these changes could not in-
duce the regression fault. To produce a failing subset, all
these changes should be included together. Finally, more
than one hundred changes are identiﬁed by DD and it is
not easy to pick out faulty ones among them. In contrast,
by restricting the search space based on coverage analysis,
ADD performs minimization in a smaller and more relevan-
t set (containing only twenty changes) and provides twelve
suspicious changes (including both c3 and c4), which allows
developers to ﬁnd the bug more easily and quickly.
For space reason, here we just brieﬂy interpret results of
program grep and ls. For grep, false positives arise due to
renaming a function. The failure-inducing function exclud-
ed_filename is renamed as excluded_file_name . As a con-
sequence, changes that deﬁne or use the function must be in-
cluded in a failing subset. For ls, The failure-inducing func-
tion print_horizontal used one renamed variables (changed
from cmd_files tocmd_files ), which was widely used in the
program. Therefore, to fail the test of DD, a patch should
contain the renamed change and all the changes that use
the renamed variable. This results in a set ﬁlled with false
positives.25Table 4: Error causes for find(b) .
--- findutils-4.3.5/find/parser.c
+++ findutils-4.3.6/find/parser.c
@@ -3034 +3099 @@ <-change c1
- char *s;
+ const char *timearg;
@@ -3038 +3103 @@ <-change c2
- if ((argv == NULL) || (argv[*arg_ptr] == NULL))
+ if (!collect_arg(argv, arg_ptr, &timearg))
@@ -3043,2 +3108,2 @@ <-change c3
- s = argv[*arg_ptr];
- if (get_comp_type( &s, &comp))
+
+ if (get_comp_type( &timearg , &comp))
@@ -3062 +3127 @@ <-change c4
- if (!get_relative_timestamp( argv[*arg_ptr] , &tval,
origin, DAYSECS, errmsg))
+ if (!get_relative_timestamp( timearg , &tval, ori-
gin, DAYSECS, errmsg))
According to above analysis, in the presence of code refac-
torings that have dependency of faulty code, it is hard to
isolate failure-inducing changes precisely for DD. By usin g
coverage analysis, ADD could search in a more relevant area
and thus avoid many failure-irrelevant changes.
3.5.3 TruePositives
For the left eight regressions, the changes isolated by DD
or ADD both provide valuable direct or indirect clues (i.e.,
true positives) in debugging regressions. We now illustrat e
when the changes isolated by DD could be regarded as direct
clues. Considering the changes isolated by DD for bc(shown
in Table 5), the added change prevents bcfrom properly
reading long-format options. The reason is although a newly
default statement was added, the corresponding statement
that processes long options was not included together, and
thus resulted in the regression. The ﬁx is to add a case
statement before the default statement is reached.
Table 5: Isolated changes of bc.
--- bc-1.05/bc/main.c
+++ bc-1.06/bc/main.c
@@ -112,0 +129,4 @@
+
+default:
+ usage(argv[0]);
+ exit (1);
As an example of indirect clues, consider the changes iso-
lated by DD (shown in Table 6). Although the failure-
inducing change is identiﬁed, the change was introduced to
ﬁx another bug and should not be reverted5. Instead, the
developer repaired the bug by introducing a new condition
to catch and correct the aﬀected variable.
5For detailed information see
http://git.savannah.gnu.org/cgit/diﬀutils.git/commi t/
?id=58d0483b621792959a485876aee05d799b6470deTable 6: Isolated changes of diff.
--- diffutils-2.8.1/src/io.c
+++ diffutils-2.9/src/io.c
@@ -664,2 +650,2 @@
-for (; p0 != beg0; p0--, p1--)
-if (*p0 != *p1)
+while (p0 != beg0)
+if (*--p0 != *--p1)
We should mention that even if isolated changes provides
direct clues, most of them are not easy to understand at
a ﬁrst glance. For example, both DD and ADD precisely
found out the failure-inducing change for bc(shown in Table
5). However, it took us some minutes to ﬁgure out that the
code modiﬁcation changed the control ﬂow and thus caused
the regression. In fact, understanding root causes typical ly
requires the speciﬁc context in which regressions manifest
themselves. Just simply examining failure-inducing chang es
in isolation is always not enough. Fault understanding is
still an open issue and has attracted researchers in recent
years [13][16].
3.6 CaseStudy: libPNGand TCPﬂow
As both Darwin and Golden could not be applied to our
subjects, in this section, we describe our experience with
their subjects for further analysis. Table 7 summarizes the
characters of each subject program and results of each tech-
nique. Speciﬁcally, the number of tests required (if exists ),
the time usage requirements, the size of results and bug re-
ports for each technique are listed. The results of DD and
ADD are isolated changes, while those of Darwin and Golden
are alternative inputs and unexplained constraints respec -
tively. Following the convention of previous studies [7][2 9],
results of Darwin and Golden are obtained from their re-
spective papers. Moreover, to our knowledge, prototypes of
both techniques are not publicly available. Thus, we do not
know the results of applying Golden to debug TCPﬂow.
◮libPNG . First, we describe our experience with libP-
NG, a library for manipulating PNG images. In previous
studies [10][23], researchers used an old version (1.0.7) a s
the broken program P′and a later ﬁxed version (1.2.21)
as the working program P. We applying both DD and
our technique to identify the failure-inducing changes fro m
PtoP′. For DD, after 369 test executions in about 14
minutes, three changes are identiﬁed and shown in Table
8. The second change results in the imaginary regression.
Speciﬁcally, the length check in the second condition (length
> (png_uint_32)png_ptr->num_palette) will be missed if the
ﬁrst condition !(png_ptr->mode & PNG_HAVE_PLTE) is true. Close
scrutiny reveals that else if isifin the working version
of libPNG, meaning the length should always be checked.
For our technique, ADD identiﬁes the single failure-induci ng
change after 157 executions in less than 10 minutes.
Both DD and ADD just provide guilty changes as debug-
ging clues, while the bug reports produced by Darwin and
Golden are more precise. Speciﬁcally, Darwin ﬁrst produces
nine inputs, with only one is valid. The branch instruc-
tion contributed (to the bug report) by the valid input cor-26Table 7: Performance comparison on libPNG and TCPflow .
Changes Darwin Golden DD ADD
Time ResultReport Time Result Report#Tests Time ResultRep ort#Tests Time Result Report
Program LOC |F| |D| (mm:ss)(#input) (#loc)(mm:ss)(#const) (#loc) (mm:ss)(# change) (#loc) (mm:ss)(#change) (#loc)
libPNG 31.2K1751 65 13:34 9 9 112:34 45 17 369 13:54 3 17 157 9:1 4 1 9
TCPﬂow 0.9K 23 5 31:50 22 6 - - - 58 0:47 1 6 12 0:10 3 28
Table 8: Isolated changes of libPNG .
--- libpng-1.2.21/pngrutil.c
+++ libpng-1.0.7/pngrutil.c
@@ -1268 +1168 @@
- if (png_ptr- >color_type == PNG_COLOR_TYPE_GRAY)
+ if (png_ptr- >color_type == PNG_COLOR_TYPE_PALETTE)
@@ -1270,3 +1170,6 @@
- png_byte buf[2];
-
- if (length != 2)
+ if (!(png_ptr- >mode & PNG_HAVE_PLTE))
+ {
+ /* Should be an error, but we can cope with it */
+ png_warning(png_ptr, "Missing PLTE before tRNS");
+ }
+ else if (length >(png_uint_32)png_ptr- >num_palette)
@@ -1277,0 +1181,6 @@
+ if (length == 0)
+ {
+ png_warning(png_ptr, "Zero length tRNS chunk");
+ png_crc_finish(png_ptr, length);
+ return;
+ }
responds to the branch (length > (png_uint_32)png_ptr-
>num_palette) thereby pointing directly to the cause of fail-
ure. Golden takes nearly two hours, due to more than 1,600
of weakest preconditions to be compared. Finally, totally 4 5
unexplained constraints are identiﬁed and careful examina -
tion reveals that one unexplained constraint leads directl y
to the buggy line while the other 44 ones do not help to
identify the root cause of the speciﬁc bug.
◮TCPﬂow . In our second case study, we study TCPflow ,
a tool which captures and displays data sent through TCP
connections. A regression occurs after patching 10_extra-
opts.diff toTCPflow-0.21.ds1-2 . The bug leads to an
oﬀ-by-one error for the patched version in handling of IS-
N (initial sequence number). Table 9 is a simpliﬁed code
fragment for illustration. The regression appears because
the way in which empty packets are handled is changed by
the patch. In the unpatched version of the program, empty
packets are simply ignored (i.e., the variable state will not
be created at all). However in the patched version, empty
packets are not ignored (i.e., the variable state will still be
created for the TCP connection). Given a TCP connection,
state has one critical member ﬁeld named ins, which is used
to store ISN that the program has seen for this connection.
When state is created, insis assigned with the sequence
number of the current packet being handled. Therefore,
the values of state->ins are diﬀerent in the two program
versions. Note that the insﬁeld is later used to calculate
the oﬀset in the output ﬁle when the data is written out.
An incorrect value of state->ins in patched version will
aﬀect the computation of offset , which leads to the oﬀ-
by-one error. The ﬁx in real world is to insert a statement
state->ins++ after the variable state is created.Table 9: Schematic changes of TCPflow .
--- tcpflow-unpatched/src/tcpip.c
+++ tcpflow-patched/src/tcpip.c
handle_tcp (packet_t packet) {
if( this packet has no data) {
+ if ((state = find_flow_state(current_flow)) == NULL)
+ state = create_flow_state(flow, seq);
return;
}
- if ((state = find_flow_state(current_flow)) == NULL)
- state = create_flow_state(flow, seq);
offset = seq - state- >ins;
write data from offset;
There are totally 23 changes in the patch ﬁle 10_extra-
opts.diff and only 5 changes executed in the two versions.
Both DD and ADD isolates changes in less than one minute.
Speciﬁcally, DD identiﬁes the change that corresponds to th e
removed part in Table 9 while the change located by ADD
pointed to the added fragment. The code isolated by ADD
may be more helpful since that is just where the ﬁx is made.
Although TCPflow is not a large application and 10_extra-
opts.diff is a small patch, its path condition size is very
large, partly due to frequent usage of libraries during the e x-
ecution. The authors of Darwin report that there are about
2000 formulae to solve and the estimated time to solve each
one comes to 30 minutes. By leverage several optimization
techniques (e.g., check satisﬁability in a time-bounded ma n-
ner), the total debugging time could be reduced to about
half an hour and the ﬁnal result contains only 6 statements
including the line containing the error cause.
Although comparisons above indicate that our technique
seems to outperform Darwin and Golden in eﬃciency with-
out losing quality, they should not be carelessly interpret -
ed as showing our technique is always a better substitute
for Darwin and Golden. They have their own unique ad-
vantages. First, both Darwin and Golden could leverage
reference programs (also known as golden implementations)
to aid debugging the buggy program. Even if all versions
of a program expose a given error, these methods can stil-
l be employed if a correct program which implements the
same speciﬁcation could be found. Second, the side eﬀects
of both techniques are more informative in debugging. The
alternative inputs generated by Darwin could be used to dis-
cover new errors while the unexplained constraints obtaine d
by Golden is a logical explanation of why the error appears
and makes the bug report more accessible to the program-
mer. Third, as pointed out in previous studies [28], applyin g
delta debugging usually requires manual operations, such a s27constructing the test function, which sometimes could be as
costly as manual debugging.
4. DISCUSSIONS
4.1 Threats to Validity
Like any empirical study, there are a number of threat-
s to the validity of this experiment. The ﬁrst one lies in
the benchmark selection. Since only twelve regressions fro m
small or medium-sized open source applications are used,
absolute performance measures (e.g., our approach is usu-
ally 10 times faster yet more eﬀective than DD) cannot be
generalized to arbitrary programs. Speciﬁcally, biases th at
could potentially aﬀect the result of the experiment may
exist, depending on the nature of regression faults. On the
other hand, because subjects are collected from a wide range
of programs, the initial relative performance comparison i s
credible and promising. To alleviate the issue, we further
compared our results on libPNG and TCPﬂow. Although
conducting more experiments on more subjects of larger
sizes in diﬀerent programming languages are required, we
believe that searching in the dubious set is more eﬀective
than in the raw diﬀerences since the reﬁned set would be
more relevant for the regression.
Another potential threat is whether the measure used to
gauge improvement over existing techniques is reasonable.
As we analyzed in Section 1 and Section 4.2, both mas-
sive executions and misleading false positives thwart the a p-
plication of DD in real-world development. Therefore, the
decrease in the number of executions and failure-irrelevan t
changes indicate eﬀorts reduction in debugging. In terms
of the time consumption, according to our experience, the
number of unresolved tests mainly determines the total ex-
ecution time. Moreover, although our approach requires
ﬁrst running the program with code coverage instrumenta-
tion enabled, usually the runtime overhead (typically seve ral
seconds) could be neglected comparing with the total time
consumed (typically tens of minutes). For example, the time
consumed for find(b) during instrumentation, running DD
and ADD is about 10 seconds, 11 minutes and 7 minutes,
respectively. Timing results of other subjects could be fou nd
in our project website.
Our technique mainly focuses on failure-inducing changes
to provide debugging aids. Although the actual bug may lie
inPand is just manifested by the changes introduced by P′,
such changes provide a good starting point for pinpointing
the root cause. For example, developers could track the con-
trol or data ﬂow around failure-inducing changes manually
or automatically. In short, narrowing down the searching
area of code, which is suspicious of being faulty, could re-
duce eﬀorts in regression debugging. We would like to point
out that both Darwin and Golden are better choices in de-
bugging unmasking regressions , which already existed in P
but are exposed in P′. Therefore, we regard these techniques
and ours are complementary to each other.
As a ﬁnal note, the results of HDD are obtained from [28].
HDD was considered as a useful way [21] to improve the
eﬀectiveness of DD. However, in previous experiments [28],
HDD did not bring deﬁnite improvements in neither eﬃcien-
cy nor accuracy. This is partly because the patch structure
[8] used in [28] may not reﬂect the hierarchy of underlyingprogram changes accurately. HDD may be more eﬀective
if it operates over a syntactical hierarchical representat ion
of programming constructs, such as abstract syntax tree or
control ﬂow graph. As admitted in [21], providing such in-
frastructure is non-trivial and developers may lost their i n-
terest in applying the technique.
4.2 Related Work
◮Delta Debugging . Delta Debugging has long been
considered useful for debugging regressions [30][34], whi ch
searches for failure-inducing changes by rolling back dif-
ferent subsets of changes and observing execution results.
A few optimizations (such as grouping changes according
to the patching date or common usage of identiﬁers) were
proposed [30], and incremental build was also suggested in
Zeller’s book [31]. Two successors of DD are proposed un-
der diﬀerent scenarios. HDD (hierarchical delta debugging )
[21] exploits hierarchical characteristics of program inp uts
to simplify failure-inducing inputs while IDD (iterative d elta
debugging) [8] assumes full history of all revisions are ava il-
able and searches for a correct version iteratively. All the se
strategies mentioned above can be considered complemen-
tary to our work.
We would like to point out that DDMin could also be
adapted to the dubious set D. DDMin does not test the in-
dividual subsets dibut ADD does. Although ADD increases
the number of tests, it also increases the possibilities of p ro-
ducing a passing or failing test outcome. That is partly be-
cause sometimes regression faults are induced by individua l
changes as demonstrated in our evaluations.
◮Spectra-based Fault Localization . Much eﬀort has
been devoted to spectra-based fault localization techniqu es
[7][27][29][33], which demonstrate eﬀectiveness especia lly when
multiple passing and failing test cases are available or cou ld
be generated [9]. A recent evaluation [22] discusses open
challenges of these techniques through user studies. More-
over, in practice we notice that often only one failure-reve aling
test case is available. It is more commonly encountered be-
cause developers often receive regression bug reports with
only one test case. Moreover, according to our observation-
s and experiences, developers are usually unwilling to ﬁnd
more test cases once one is encountered. As discussed in
Section 3.3, Artzi et al. [9] investigate how to generate a
test suite to aid fault localization. However, real softwar e
programs often make use of external code, which prevent
these techniques that rely on symbolic execution from work-
ing. Moreover, the execution status (i.e., pass or fail) of
each generated test case should be determined. Theoreti-
cally, such test generation-based methods require complet e
oracle. As an alternative, BayesDebug [19] is able to work
on a few test inputs. However, it is not applicable to large
evolving program, as it relies on computing proﬁles on all
predicates in each version and does not leverage informa-
tion about changes between the old and new versions as our
approach does.
◮Debugging Evolving Programs . Recent work fo-
cuses on explaining the unexpected behavior of a bug that
hasmanifested itself owing to program modiﬁcations. Dar-
win [23] locates error causes by comparing execution traces
of the failing input and a generated alternative input. How-28ever, Darwin is most suited for explaining regressions that
are exposed by a diﬀerence in control ﬂow. To overcome this
issue, Golden [10] combines slicing and symbolic execution
to explain the unexpected behavior of the buggy program.
In contrast, Hoﬀman et al. [18] introduced a view-based
technique for identifying causes of regressions by compari ng
large execution traces.
Despite promising results, those techniques are expen-
sive to create and even often unavailable. Techniques that
employ test generation [9][10][23] undergo scalability is sues
from exponential number of paths and complexity of the gen-
erated constraints [12] while precise spectrum-based meth -
ods rely on sophisticated dynamic analysis, such as instru-
menting [18][19][33]. Developers may not be familiar with
such techniques and are unlikely to use them. Worse more,
misleading false positives arise in applying above techniq ues.
For example, false positives may be inevitable due to the
limits of constraint solver [10] or approximate analysis [1 8].
This work focuses on helping developers to easily create an
eﬀective lightweight tool for locating regression faults.
5. CONCLUSION
As software evolves, bugs are inevitably introduced. Fix-
ing regression faults reported by end users is urgent but
time-consuming. We have presented a framework for failure-
inducing changes identiﬁcation, which combines coverage
analysis and delta debugging. Our approach is highly ef-
ﬁcient and is applicable to practical settings. Evaluation s
on real regressions shows by searching in a small area, both
eﬃciency and eﬀectiveness can be improved.
To facilitate further research in these and other direction -
s, detailed information on our experiments, is available at
http://code.google.com/p/iregression/
6. ACKNOWLEDGMENTS
We appreciate anonymous reviewers for their supportive
and constructive comments. We would like to thank Dawei
Qi for his help in performing experiments for libPNG and
TCPﬂow. This work was supported by the fund of the S-
tate Key Laboratory of Software Development Environment
SKLSDE-2011ZX-07.
7. REFERENCES
[1]http://developer.mozilla.org/en/Incremental_Build .
[2]http://lists.gnu.org/archive/html/ .
[3]http://bugs.gentoo.org/ .
[4]http://bugs.debian.org/ .
[5]http://savannah.gnu.org/ .
[6] Oﬃcial: Windows 7 has more than 2000 bugs.
http://lxer.com/module/forums/t/28596/ .
[7] R. Abreu, P. Zoeteweij, and A. J. C. v. Gemund. Spectrum-
based multiple fault localization. In ASE, pages 88–99, 2009.
[8] C. Artho. Iterative delta debugging. International Jour-
nal on Software Tools for Technology Transfer , 13:223–246,
2011.
[9] S. Artzi, J. Dolby, F. Tip, and M. Pistoia. Directed test
generation for eﬀective fault localization. In ISSTA , pages
49–60, 2010.
[10] A. Banerjee, A. Roychoudhury, J. A. Harlie, and Z. Liang .
Golden implementation driven software debugging. In FSE,
pages 177–186, 2010.[11] C. Cadar, D. Dunbar, and D. Engler. KLEE: unassisted
and automatic generation of high-coverage tests for comple x
systems programs. In OSDI , pages 209–224, 2008.
[12] C. Cadar, P. Godefroid, S. Khurshid, C. S. P˘ as˘ areanu,
K. Sen, N. Tillmann, and W. Visser. Symbolic execution
for software testing in practice: preliminary assessment. In
ICSE , pages 1066–1071, 2011.
[13] H. Cheng, D. Lo, Y. Zhou, X. Wang, and X. Yan. Identifying
bug signatures using discriminative graph mining. In ISSTA ,
pages 141–152, 2009.
[14] J. Clause and A. Orso. A technique for enabling and sup-
porting debugging of ﬁeld failures. In ICSE , pages 261–270,
2007.
[15] V. Dallmeier and T. Zimmermann. Extraction of bug local -
ization benchmarks from history. In ASE, pages 433–436,
2007.
[16] F. Deng and J. A. Jones. Inferred dependence coverage to
support fault contextualization. In ASE, pages 512–515,
2011.
[17] H. Do, S. Elbaum, and G. Rothermel. Supporting controll ed
experimentation with testing techniques: An infrastructu re
and its potential impact. Empirical Software Engineering ,
10:405–435, 2005.
[18] K. J. Hoﬀman, P. Eugster, and S. Jagannathan. Semantics -
aware trace analysis. In PLDI , pages 453–464, 2009.
[19] C. Liu, Z. Lian, and J. Han. How bayesians debug. In ICDM ,
pages 382–393, 2006.
[20] J. Loeliger. Version control with Git . O’Reilly Media, 2009.
[21] G. Misherghi and Z. Su. HDD: hierarchical delta debuggi ng.
InICSE , pages 142–151, 2006.
[22] C. Parnin and A. Orso. Are automated debugging techniqu es
actually helping programmers? In ISSTA , pages 199–209,
2011.
[23] D. Qi, A. Roychoudhury, Z. Liang, and K. Vaswani. Dar-
win: An approach to debugging evolving programs. ACM
TOSEM , 21(3):19:1–19:29, 2012.
[24] D. Song, D. Brumley, H. Yin, J. Caballero, I. Jager, M. G.
Kang, Z. Liang, J. Newsome, P. Poosankam, and P. Saxena.
BitBlaze: A new approach to computer security via binary
analysis. In ICISS (Keynote Invited paper) , 2008.
[25] W. N. Sumner, T. Bao, and X. Zhang. Selecting peers for
execution comparison. In ISSTA , pages 309–319, 2011.
[26] J. M. Voas. PIE: A dynamic failure-based technique. IEEE
TSE, 18(8):717–727, 1992.
[27] S. Wang, D. Lo, L. Jiang, Lucia, and H. C. Lau. Search-bas ed
fault localization. In ASE, pages 556–559, 2011.
[28] K. Yu, M. Lin, J. Chen, and X. Zhang. Towards automated
debugging in software evolution: evaluating delta debuggi ng
on real regression bugs from developers’ perspectives. Jour-
nal of Systems and Software , 85(10):2305–2317, 2012.
[29] K. Yu, M. Lin, Q. Gao, H. Zhang, and X. Zhang. Locating
faults using multiple spectra-speciﬁc models. In SAC, pages
1404–1410, 2011.
[30] A. Zeller. Yesterday, my program worked. Today, it does
not. Why? In ESEC/FSE , pages 253–267, 1999.
[31] A. Zeller. Why Programs Fail: A Guide to Systematic De-
bugging . Morgan Kaufmann, 2005.
[32] A. Zeller and R. Hildebrandt. Simplifying and isolatin g
failure-inducing input. IEEE TSE , 28(2):183–200, 2002.
[33] L. Zhang, M. Kim, and S. Khurshid. Localizing fault-
inducing program edits based on spectrum information. In
ICSM , pages 23–32, 2011.
[34] S. Zhang, Y. Lin, Z. Gu, and J. Zhao. Eﬀective identiﬁcat ion
of failure-inducing changes: a hybrid approach. In PASTE ,
pages 77–83, 2008.29