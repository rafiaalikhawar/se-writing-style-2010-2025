An Empirical Evaluation and Comparison
of
Manual and Automated Test Selection
Milos Gligoric, Stas Negara, Owolabi Legunsen, and Darko Marinov
University of Illinois at Urbana-Champaign
{gliga,snegara2,legunse2,marinov}@illinois.edu
ABSTRACT
Regression test selection speeds up regression testing by re-
running only the tests that can be aﬀected by the most
recent code changes. Much progress has been made on re-
searchinautomatedtestselectionoverthelastthreedecades,
but it has not translated into practical tools that are widely
adopted. Therefore, developers either re-run all tests after
eachchangeorperform manual test selection . Re-runningall
tests is expensive, while manual test selection is tedious and
error-prone. Despite such a big trade-oﬀ, no study assessed
how developers perform manual test selection and compared
it to automated test selection.
This paper reports on our study of manual test selection
in practice and our comparison of manual and automated
test selection. We are the ﬁrst to conduct a study that
(1) analyzes data from manual test selection, collected in
real time from 14 developers during a three-month study
and (2) compares manual test selection with an automated
state-of-the-research test-selection tool for 450 test sessions.
Almost all developers in our study performed manual test
selection, andtheydidso inmostlyad-hocways. Comparing
manual and automated test selection, we found the two ap-
proaches to select diﬀerent tests in each and every one of the
450 test sessions investigated. Manual selection chose more
tests than automated selection 73% of the time (potentially
wasting time) and chose fewer tests 27% of the time (poten-
tially missing bugs). These results show the need for better
automated test-selection techniques that integrate well with
developers’ programming environments.
1. INTRODUCTION
Regression testing [ 8,42,43] is an important activity in
so
ftware development. It checks that software changes do
not break existing tests. To quickly detect changes that
break tests, it is desirable to run tests frequently. How-
ever, regression testing is also expensive. Because software
projects can have very many tests, re-running all the tests
after every code change is often time-consuming [ 37,38].
Pe
rmission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from Permissions@acm.org.
ASE’14, September 15-19, 2014, Vasteras, Sweden.
Copyright 2014 ACM 978-1-4503-3013-8/14/09 ...$15.00.
http://dx.doi.org/10.1145/2642937.2643019.Regression test selection (RTS) aims to improve the ef-
ﬁciency of regression testing by selecting to re-run only a
subset of the test suite, namely, those tests that can be af-
fected[11,12,43] by the changes. An RTS technique is sa fe
if it guarantees to select all aﬀected tests [ 31,40], i.e., the
be
havior of unselected tests after the code changes remains
the same as before the code changes. One RTS technique is
moreprecisethan another RTS technique if it selects fewer
(non-aﬀected) tests.
While researchers have proposed many automated test-
selection techniques over the last three decades [ 1,3,5,7,14,
15,31,40,42], thereisstillnotoolthatimplementsthesetech-
ni
quesinawaythatispractical enoughfor widespreadadop-
tion. The few successful systems for automated regression
testing, e.g., theTAPsystematGoogle [37 ,38]andtheScout
sy
stem (formerly known as Echelon) at Microsoft [ 16,17,34],
us
e techniques that are either imprecise (e.g., TAP selects,
for each project, either all tests or no tests based on compile-
time project dependencies) or unsafe (e.g., Scout does not
guarantee that the behavior of unselected tests will remain
the same, so Scout only prioritizes [6 ,32] but does not select
te
sts). These systems can work well for very large code-
bases, but most developers do not write code at the scale
of Google or Microsoft. Rather, most developers work on
smaller projects, for which running a coarse-grained tech-
nique like TAP would often be equivalent to re-running all
tests. Such smaller projects would require a ﬁner-grained
technique for more precise RTS (e.g., ﬁnding aﬀected tests
based on control-ﬂow edges or methods [ 2,14,29]).
Th
e lack of practical RTS tools leaves two options for
developers: either automatically re-run all the tests or man-
ually perform test selection. Re-running all the tests is safe
by deﬁnition, but it can be quite imprecise and, therefore,
ineﬃcient. In contrast, manual test selection, which we will
refer to as manual RTS , can be both unsafe and imprecise:
developers can select too few tests and thus miss to run
some tests whose behavior diﬀers due to code changes, or
developers can select too many tests and thus waste time.
Despite the importance of RTS, we are not aware of any
research that studies ifandhowdevelopers perform man-
ual RTS, and how manual and automated RTS compare.
Ouranecdotalexperienceshows thatdevelopersselect torun
only some of their tests, but we do not know how many de-
velopers do so, how many tests they select, why they select
those tests, what automated support they use for manual
RTS in their Integrated Development Environment (IDE),
etc. Also, it is unknown how developers’ manual RTS prac-
tices compare with any automated RTS technique proposed
361
in the literature: how does developers’ reasoning about af-
fe
cted tests compare to the analysis of a safe and precise
automated RTS technique? The need for adoptable auto-
mated RTS tools makes it critical to study current manual
RTS practice and its eﬀects.
This paper presents theresults of theﬁrst studyof manual
RTS and a ﬁrst comparison of manual and automated RTS.
Speciﬁcally, we address the following research questions:
RQ1. How often do developers perform manual RTS?
RQ2. What is the relationship between manual RTS and
size of test suites or amount of code changes?
RQ3. What are some common scenarios in which developers
perform manual RTS?
RQ4. How do developers commonly perform manual RTS?
RQ5. How good is current IDE support in terms of common
scenarios for manual RTS?
RQ6. Howdoes manual RTScompare with automatedRTS,
in terms of precision, safety, and performance?
To address the ﬁrst set of questions about manual RTS
(RQ1-RQ5), we extensively analyzed logs of IDE interac-
tions recorded from a diverse group of 14 developers (work-
ing on 17 projects, i.e., some developers worked on multiple
projects during our study), including several experts from
industry [ 23]. These logs cover a total of 918 hours of de-
ve
lopment, with 5,757 test sessions and a total of 264,562
executed tests. A test session refers to a run of at least
one test between two sets of code changes. We refer to test
sessions withasingletestas single-test sessions , andtestses-
sions with more than one test as multiple-test sessions . To
address RQ6, we compared the safety, precision, and perfor-
mance of manual and automated RTS for 450 test sessions
of one representative project, using the best available auto-
mated RTS research prototype [ 44].
Se
veral of our ﬁndings are surprising. Regardless of the
project properties (small vs. large, few tests vs. many tests,
etc.),almost all developers performed manual RTS. 62% of
all test sessions executed a single test, and of multiple-test
sessions, on average, 59% had some test selection. The per-
vasivenessofmanualRTSestablishes theneedto studyman-
ual RTSin more depthandcompare it with automatedRTS.
Moreover, our comparison of manual and automated RTS
revealed that manual RTS can be imprecise (in 73% of the
test sessions, manualRTSselects more tests thanautomated
RTS) and unsafe (in 27% of the test sessions, manual RTS
selects fewer tests than automated RTS). Finally, our exper-
iments show that current automated RTS may provide little
time savings: the time taken by an automated RTS tool1,
pe
r session, to select tests was 130.94 ±13.77 sec (Mean ±SD)
and the (estimated) time saved (by not executing unselected
tests) was 219.86 ±68.88 sec. These results show a strong
need for better automated RTS tools.
This paper makes the following contributions:
⋆Evidence. We conducted the ﬁrst study to show how
developers perform manual RTS.
1Thismeasuresonlytheanalysistimetoidentifytheaﬀected
tests but notthe time to collect coverage.⋆Examination. We closely examined manual RTS and its
associated factors in practice.
⋆Comparison. Weperformedtheﬁrstcomparisonofman-
ual and automated RTS.
2. EVALUATING MANUALRTS
We present our methodology for analyzing manual RTS
data to answer RQ1-RQ5, and summarize our ﬁndings.
2.1 Methodology
We analyzed the data collected during our previous ﬁeld
study [23], in which we unobtrusively monitored developers’
ID
Es and recorded their programming activities over three
months. Wehadusedthecollected datainourpriorresearch
studies [ 21,23,39] on refactoring and version control; the
wo
rk presented in this paper is the ﬁrst to focus on the
(regression) testing aspects.
To collect data, we asked our study participants to in-
stall our record-and-replay tool, CodingTracker [4], in their
Ec
lipse (Indigo) IDEs. Throughout the study, CodingTracker
recorded detailed code evolution data, ranging from individ-
ual code edits, start of each test, and test outcome (e.g.,
pass/fail) up to high-level events like automated refactoring
invocations and test session executions. CodingTracker up-
loaded the collected data to our centralized repository using
existing infrastructure [ 39].
In
this study, we only consider data from participants who
had more than ten test sessions. Overall, the data encom-
passes 918 hours of code development activities by 14 devel-
opers, of whom ﬁve are professional programmers and nine
are students. The professional programmers worked in dif-
ferent software companies on projects spanning various do-
mains suchas marketing, banking, business process manage-
ment, and database management. The students were Com-
puter Science graduate students and senior undergraduate
interns, whoworkedonavarietyofresearchprojects fromsix
research labs at the University of Illinois. The programming
experience of our study participants varied: one developer
had less than 5 years, eight developers had between 5–10
years, and ﬁve developers had more than 10 years. None
of the study participants knew how we would analyze the
collected data; in fact, we ourselves did not know all the
analyses we would do at the time we collected the data.
In the rest of this section, we discuss the tool used, the
projects analyzed, the challenges faced, and the answers we
found to RQ1-RQ5.
2.1.1 CodingTracker
CodingTracker integrates well with one of the most pop-
ular IDEs, Eclipse [ 19]. Developers do not explicitly inter-
ac
t withCodingTracker during their workﬂow, and thus, the
data recorded by CodingTracker is as close as possible to
what developers normally do. CodingTracker collects infor-
mation about all test sessions. Because test-selection data
is available at every test session, we were able to capture de-
velopers’ manual RTS decisions. Each test session includes a
list of executed tests, their execution time, and their status
on completion (pass or fail). Further, CodingTracker collects
information about code changes between test sessions.
WhileCodingTracker logs provide a treasure trove of data,
they have limitations. First, CodingTracker logs cannot fully
conﬁrm that developers performed manual RTS. In theory,
362ProjectTest Sessions Available Tests Selected Tests Selective
Se
ssions Total Single-Test Debug MinMax Mean MinMax Mean Sum Timemin
P1 41 20 81 7 4.68 1 72.59 106 8928.57%
P2 218 152 68 1886 43.70 1886 9.71 2,116 203 77.27%
P3 41 28 91530 19.46 153015.61 640 238.46%
P4 94 33 22170 182 176.23 1173103.16 9,697 2659.02%
P5 1,231 883 852 1172 83.00 114113.01 16,019 374 99.71%
P6 18 7 5113 6.00 1134.11 74 018.18%
P7 55 54 43 1 8 6.47 1 81.13 62 34 0.00%
P8 612 446 306 159 34.29 1442.56 1,565 8992.77%
P9 443 362 117 1132 85.86 1124 5.66 2,508 246 81.48%
P10 178 108 29 1126 48.54 112414.48 2,577 139 64.29%
P11 129 108 27 119 15.29 1 91.64 211 5395.24%
P12 176 121 74 1121 105.53 112019.39 3,413 153 94.55%
P13 51 36 22 118 12.86 1185.53 282 30.00%
P14 450 146 103 721,012 889.32 11,010 113.40 51,031 242 98.36%
P15 156 78 60 11,663 13.40 11,663 12.98 2,025 928.21%
P16 1,666 855 462 11,606 1,416.10 11,462 103.24 171,990 420 98.40%
P17 198 157 50 1 6 1.83 1 41.24 246 2331.71%/summationtext5,757 3,594 2,258 - - -- - -264,562 2,113 -
Ari Mean 338.65 211.41 132.76 - -174.27 - - -15,562.47 124.31 59.19%
Figure 1: Statistics for projects used in the study; “Selectiv e Sessions” is of multiple-test sessions; we exclude single-test
sessions as they may not be“true”Selective Sessions - developer knows that not all aﬀected tests are selected
developers could have installed some Eclipse plugin that
would perform automated RTS for them. However, we are
not aware of any automated RTS tool that works in Eclipse.
Moreover, we have noticed signiﬁcant time delays between
code changes and the start of test sessions, which likely cor-
respond to developers’ selection times (i.e., time that de-
velopers spend reasoning about which tests to run) and not
automated tool runs. Therefore, we assume that develop-
ers manually selected the tests in each test session. Second,
CodingTracker collects information about code changes but
not entire project states . The original motivation for Cod-
ingTracker was a study of refactorings [ 23], which needed
on
ly code changes, so a design decision was made for Cod-
ingTracker tonotcollect the entire project states (to save
space/time for storing logs on disk and transferring them
to the centralized repository). However, the lack of entire
states creates challenges to exactly reconstruct the project
as the developer had it for each test session (e.g., to pre-
cisely count the number of tests or to compile and run tests
for automated RTS). Sections 2.1.3and3.1.3discuss how
we
address these challenges.
2.1.2 ProjectsUnderAnalysis
As mentioned earlier, we analyzed the data from 14 devel-
opers working on 17 research and industrial projects, e.g., a
Struts web application, a library for natural-language pro-
cessing, a library for object-relational mapping, and a re-
search prototype for refactoring. Note that some developers
worked on several projects in their Eclipse IDE during our
three-month study; CodingTracker recorded separate data
for each project (more precisely, CodingTracker tracks each
Eclipse workspace) that was imported into Eclipse.
Figure1is a summary of test-related data that we col-
le
cted2. For each project, we ﬁrst show the number of test
se
ssions. Our analysis showed that a large number of these
sessions execute only one test. We refer to such test sessions
assingle-test sessions . Further, we found that many of these
single-test sessions execute only one test that had failed in
the immediately preceding session. We refer to such ses-
2Due to the conditions of Institutional Review Board ap-
proval, we cannot disclose the true names of these projects.sions as debug test sessions . Next, we show the number of
available tests , i.e., the total number of tests in the project
at the time of a test session, discussed in more detail in
Section2.1.3. Then, we show the number of se lected tests ,
i.e., a subset of available tests that the developer selected
to execute, including the total number of selected tests that
the developer executed throughout the study and the to-
tal execution time for all test sessions3. Finally, we show
th
e percentage of selective sessions , i.e.,multiple-test ses-
sionswhere the number of selected tests is smaller than the
number of available tests; in other words, the developer per-
formed manual RTS in each such test session by selecting to
execute only a subset of the tests available in that session.
The total test execution time with manual RTS is sub-
stantially lower than it would have been without manual
RTS. The sum of the “Timemin”column in Figure 1shows
th
at, when manual RTS is performed, the total test execu-
tion time for all developers in our study was 2,113 minutes.
In contrast, had the developers always executed all available
tests, we estimate4that it would have resulted in a total test
ex
ecution time of 23,806 minutes. In other words, had the
developers not performed manual RTS, their test executions
would have taken about an order of magnitude more time.
We point out some interesting observations about single-
testsessions. First, theprojectsusedinourstudyspanmany
domains and vary in the number of available and selected
tests, but they all have some single-test sessions and some
multiple-test sessions. Second, single-test sessions include
both debug and non-debug sessions. Non-debug single-test
sessions usually happen when introducing a new class/fea-
ture, because the developer focuses on the new code. By
default, in the rest of the paper, we exclude all single-test
sessions from our analyses and only mention them explicitly
3The reported execution time is extracted from the times-
tamps recorded on developers’ computers. It is likely that
developers used machines with diﬀerent conﬁgurations, but
we do not have such information.
4Note that CodingTracker does/can not record the execution
time for the unselected tests that were not executed; we
estimate the time from the averages of the sessions in which
the tests were executed.
3631// Inputs: Session info extracted from CodingTracker logs
2L
ist/a\}bracketle{tTestSession /a\}bracketri}htsessions;
3Map/a\}bracketle{tTestSession, Set /a\}bracketle{tPair/a\}bracketle{tClassName, MethodName /a\}bracketri}ht/a\}bracketri}ht/a\}bracketri}htexecuted;
4
5// Output: Available tests for each test session
6Map/a\}bracketle{tTestSession, Set /a\}bracketle{tPair/a\}bracketle{tClassName, MethodName /a\}bracketri}ht/a\}bracketri}ht/a\}bracketri}htavailable;
7
8// Compute available tests for each test session
9ComputeAvailable()
10Set/a\}bracketle{tPair/a\}bracketle{tClassName, MethodName /a\}bracketri}ht/a\}bracketri}htT={}// Current available tests
11available = {}
12
13foreachs: sessions
14 Set/a\}bracketle{tPair/a\}bracketle{tClassName, MethodName /a\}bracketri}ht/a\}bracketri}hte= executed(s )
15 if|e|>1
16 T=T\ {(c,m)∈T|∃(c,m′)∈e}
17 T=T∪e
18 available (s) =T
Figure 2: Algorithm for computing a set of available test
methods at each test session
when some of the subsequent plots or other numbers that
we report include single-test sessions.
2.1.3 Challenges
CodingTracker was initially designed to study how code
evolvesovertime[ 23], andthusitrecordedonlycodechanges
an
d various ﬁle activities but not the entire state of the de-
velopers’ projects. As a consequence, we could not easily
extract the number of available tests for each test session:
whileCodingTracker did record the information about tests
that are executed/selected, it had no explicit information
about tests that were notexecuted. Therefore, we devel-
oped an algorithm to estimate the number of available tests
(reported in Figure 1). We designed our algorithm to be
co
nservative and likely under-estimate the number of avail-
able tests. In other words, developers likely performed even
more manual RTS than we report.
Figure2shows the algorithm. The input to the algorithm
is
alistoftestsessions extractedfromthe CodingTracker logs;
each session is mapped to a set of executed tests, and each
test is represented as a pair of a test class and test method
name. The output is a mapping from test sessions to the set
of available tests. Although we extract more information
for each test session, e.g., execution time, that information
is not relevant for this algorithm.
The algorithm keeps track of the current set of available
tests,T, initialized to the empty set (line 10). For each
te
st session, the algorithm adds to Tthe tests executed in
that session (line 17); those tests are deﬁnitely available.
Th
e algorithm also attempts to ﬁnd which tests may have
been removed and are not available any more. For each
multiple-test session, the algorithm removes from Tall the
tests whose class matches one of the tests executed in the
current session s(line16). The assumption is that execut-
in
g one test from some class c(in a session that has more
than one test) likely means that alltests from that class are
executed in the session. Thus, any test from the same class
that was executed previously but not in the current session
was likely removed from the project. This assumption is
supported by the fact that Eclipse provides rather limited
support for selection of multiple tests from the same class
as discussed in Section 2.2. For single-test sessions, the al-
go
rithm only adds the executed test to T; the assumption is
that the same tests remain available as in the previous ses-sion, but the developer decided to run only one of the tests.
Finally,Tbecomes the available set of tests for the current
session (line 18). Note that our algorithm does not account
fo
r removed test classes, but these are very rare in our data
set. For example, we inspected in detail project P14, one of
the largest projects, and no test class was deleted.
2.2 Investigating Manual RTS
In summary, the results showed that almost all developers
in our study performed some manual RTS. They did so re-
gardless of the size of their test suites and projects, showing
that manual RTS is widely practiced. Next, we provide de-
tails of our ﬁndings regarding research questions RQ1-RQ5.
RQ1: How oftendodevelopersperformmanualRTS?
Developers performed manual RTS in 59.19 ±35.16% (mean
±SD) of the test sessions we studied (column “Selective
Sessions” in Figure 1). Note that we ﬁrst compute selec-
ti
ve session ratio for each developer, and then an unweighted
arithmetic mean of those ratios (rather than weighting by
the number of test sessions), because we do not want devel-
opers with the most test sessions to bias the results.
020406080100Selected / Available * 100
Figure 3: Distribution of
test selection ratio with (left)
and without (right) single-
test sessionsAcross all 2,163 multiple-
test sessions in our study,
the average ratio of selected
tests (tests that the devel-
oper executed) to available
tests (tests that could have
been executed), i.e., aver-
agetest selection ratio , was
only 35.07%. Note that this
number is calculated from
all test sessions as if they
were obtained from a single
developer. We show the dis-
tribution of test selection ra-
tios for all test sessions for
all the developers using vi-
olin plots [ 18] in Figure 3.
Av
iolin plot is similar to
a boxplot but additionally
shows probability density of
the data at diﬀerent values.
The left part of Figure 3shows the distribution of test selec-
ti
on ratios when single-test sessions are included, while the
right part shows the distribution when single-test sessions
are excluded. We show only one half of each violin plot due
to space constraint; themissing halves are symmetric. It can
be observed from the violin plots that manual RTS happens
very frequently, and, most of the time, the test selection
ratio is less than 20%.
We note here that our ﬁnding constitutes the ﬁrst em-
pirical evidence concerning manual RTS in practice. More
importantly, we think that this fact should result in a call-
to-arms by the automated RTS community, because poor
manual RTScouldbe hamperingdeveloper productivityand
impacting negatively on software quality.
RQ2: Does manual RTS depend on size of test suites
oramountofcodechanges?
Developers performed manual RTS regardless of the size
of their test suites. We draw this conclusion because al-
most all developers in our study performed manual RTS,
364and they had a wide range of test-suite sizes. The aver-
ag
e test-suite size in all 17 projects we studied was 174.27
tests (column “Available Tests” in Figure 1); the minimum
wa
s 6 tests, and the maximum was 1,663 tests. Considering
that these projects are of small to medium size, and be-
cause they exhibit manual RTS, we expect that developers
of larger projects would perform even more manual RTS.
We also consider the relationship between the size of re-
cent code changes and the number of tests that developers
select in each test session. One may expect that developers
run more tests after large code changes. We correlate the
test selection ratio with the code change ratio for all test
sessions. The code change ratio is calculated as the per-
centage of AST node changes [ 23] since the previous test
se
ssion over the total AST node changes during the entire
studyfor aparticular project. To assess correlation, we mea-
sure the Spearman’s and Pearson’s correlation coeﬃcients5.
Th
e Spearman’s and Pearson’s coeﬃcients are 0.28 (0.25
when single-test sessions are included) and 0.16 (0.16 when
single-test sessions are included), respectively. In all cases,
the p-value was below 0.016, which conﬁrms that some cor-
re
lation exists. However, the low values of coeﬃcients imply
a low correlation between the amount of code changes im-
mediately before a test session and the number of manually
selected tests in that session. This low correlation was a
surprising ﬁnding as we had expected a higher correlation
between code changes and the number of selected tests.
RQ3: Whatarecommonscenariosfor manualRTS?
The most common scenario in which developers performed
manual RTS was while debugging a single test that failed
in the previous session. Recall that we refer to such test
sessions as debug test sessions . As seen in Figure 1(col-
um
n “Single-Test Debug”), debug test sessions account for
2,258 out of the 5,757 total test sessions considered. One
common pattern that we found in the data was that, af-
ter one or more tests fail, developers usually start making
code changes to ﬁx those failing tests and keep re-running
only those failing tests until they pass. After all the fail-
ing tests pass, the developers then run most or all of the
available tests to check for regressions. Another pattern is
when a developer ﬁxes tests one after another, re-running
only a single failing test until it passes. Therefore, even if
the developers had a “perfect” automated RTS tool to run
after each change, such a tool could prove distracting when
running many debug test sessions in sequence. Speciﬁcally,
even if some code changes aﬀect a larger number of tests,
developers may prefer to run only the single test that they
are currently debugging. The existence of other reasons for
RTS, besides eﬃciency improvements, shows a need for a
diﬀerent class of tools and techniques that can meet these
actual developer needs; we discuss this further in Section 4.
It
is also interesting that the sequences of single-test ses-
sions(i.e., single-test sessions without other test sessions in
between)weremuchlongerthanweexpected. Themean ±SD
of the length ofsingle-test session sequences was 6.83± 37.00.
The longest single-test session sequence contains 99 test ses-
5Although the data is not normally distributed, and the
relationship is not linear, we report the Pearson’s coeﬃcient
for completeness.
6A low p-value indicates that Spearman’s or Pearson’s coef-
ﬁcient is unlikely 0.0 100 200 300 400 500 600 700 800
Un
selected execution time (sec)0100200300400500600Selection time (sec)
Figure 4: Relationship between selection time and (esti-
mated) time to execute unselected tests; the plot also shows
the identity line
sions, which may indicate that developers avoid running all
tests when focusing on new features and debugging.
RQ4: How do developers commonly perform manual
RTS?
We found that developers use a number of ad-hoc ways for
manual RTS. These include: (1) commenting out tests that
should not be run, (2) selecting individual nodes of hierar-
chy, by which we refer to the way tests are hierarchically
organized, from test methods to test classes to test pack-
ages to entire projects, and (3) creating test scripts, which
specify runs of several nodes of hierarchy.
Manual RTS by Commenting: One approach used by the
developers was to comment out unit tests they did not want
to run. We observed that developers performed this type of
selection at diﬀerent levels of granularity. Some developers
commented out individual test methods within a test class,
while others commented out entire test classes from JUnit
annotations that specify test suites. In both cases, the time
overhead incurred by the developer in deciding which tests
to run and in commenting out the tests, i.e., selection time ,
is likely to be non-negligible. In other words, selection time
is an estimate of the time spent by developers to manually
“analyze”and select which tests may be aﬀected. Using the
available CodingTracker data, we estimate selection time to
be the time elapsed from the last code change that immedi-
ately preceded a test session and the start of the test session.
We exclude selection time values greater than 10 minutes,
as developers may re-run tests after taking a break from
work. Our experiments with break times of 5 minutes and
20 minutes did not signiﬁcantly change any of the outcomes
of our study. In Figure 4, we show the correlation between
se
lection time and (estimated) time to execute unselected
tests (which is the time saved by not executing unselected
tests). While the overall time savings due to manual RTS is
signiﬁcant, we found that in 31% of the cases (points above
the identity line in Figure 4) developers could have saved
mo
re time by simply running all the tests.
Manual RTS by Selecting Various Nodes of Hierarchy:
Developers also perform test selection by selecting a node
of hierarchy in their IDE, e.g., they could select to run only
a single test or all the tests from a single class or package.
This is a critical RTS limitation in Eclipse—it restricts the
developer to select to run only one node of hierarchy (in the
limit this node represents the entire project such that the
365entire test suite for that project is run). In other words, the
de
veloper is not able to select to run an arbitrary set of tests
or test suites. Related but diﬀerent, in several projects, by
browsing through the changes collected by CodingTracker ,
we noticed that developers were writing scripts (“.launch”
ﬁles in Eclipse) to group tests. Using a script has the same
limitation as manually selecting a node of hierarchy. These
limitations of Eclipse are shared by several popular IDEs as
shown in Figure 5.
RQ
5: How goodis IDE supportformanualRTS?
IDEs provide varying levels of support for performing man-
ual RTS. The IDEs we investigated are: Eclipse7, IntelliJ
ID
EA8, NetBeans9, and VisualStudio 201010.
Su
pport for Arbitrary Manual RTS: Recall from the an-
swer to RQ4 that, in several cases, the developers selected
among tests by commenting out the tests within test classes
or commenting out test classes within test suites. This likely
means that developers would like to arbitrarily select tests
within nodes of hierarchy. Also, ourexperience with running
the automated RTS tool (as discussed in Section 3) shows
th
at all aﬀected tests may not reside in the same node of hi-
erarchy. Thus, it is also important to be able to arbitrarily
select tests across these nodes.
Figure5is a summary of available IDE support for select-
in
g tests at diﬀerent levels of granularity within and across
nodes of hierarchy. All the IDEs allow developers to select
a single test. Moreover, several IDEs oﬀer support for arbi-
trary selection. IntelliJ allows to arbitrarily select tests by
marking (in the GUI) each test to be run subsequently. This
may be tedious for selecting among very many tests and is
only available for arbitrarily selecting test classes across test
packages or test methods within the same class. VisualStu-
dio allows arbitrary selection by specifying regular expres-
sions for test names which may match across multiple nodes
of hierarchy. However, not all developers are familiar with
regular expressions, and knowledge of all test names in the
project is required to write them eﬀectively. Still, based on
our study, having this type of support seems very valuable,
given that it is needed by the developers. More importantly,
Eclipse lacks support for such arbitrary test selection.
Support for RTS across multiple test sessions: We showed
in theanswer toRQ3thatthemost commonpatternofman-
ual RTS occurred during debug test sessions. It is likely that
the changes made between debug test sessions aﬀect more
tests than the test being ﬁxed. Indeed, we found this to be
the case for project P14. It is possible that the developers
do not select other tests aﬀected by the changes due to addi-
tional reasoning required to identify such tests. Thus, their
test selections during debug test sessions are likely to be un-
safe and may lead to extra debug steps at a latter stage.
Although VisualStudio provides some level of RTS automa-
tion, it has some shortcomings that we discuss in Section 4.
On
e observation from our comparison of IDEs is that they
diﬀer in their level of support for the diﬀerent patterns of
7Kepler Service Release 1, build id: 20130919-0819.
8Version 12.1.6, build id: IC-129.1359.
9Version 7.4, build id: 201310111528.
10We selected VisualStudio 2010 rather than the latest ver-
sion because VisualStudio 2010 was the only IDE that has
ever supported automated RTS; interestingly enough, this
automated RTS support has been removed from the IDE in
subsequent releases!RTS Capability
Eclipse
NetBeans
IntelliJ
VS 2010
Select single test + + + +
Run all available tests + + + +
Arbitrary selection in a node of hierarchy - -±+
Arbitrary selection across nodes of hierarchy - -±+
Re-run only previously failing tests + + + +
Select one from many failing tests - -+ +
Arbitrary selection among failing tests - -+ +
Figure 5: RTS capabilities of popular IDEs. (IntelliJ only
pa
rtially supports arbitrary selection)
manual RTS, but even if we combined the best RTS features
from all IDEs investigated, it would still not be suﬃcient for
safe and precise RTS that developers need.
3. MANUAL VS. AUTOMATED RTS
We next discuss the results of our comparison of manual
andautomatedRTS,bywhichweaddressquestionRQ6. We
compare both approaches in terms of safety, precision, and
performance usingone ofthelargest projects from ourstudy.
As noindustry-strengthtool for automated RTSis available,
we used FaultTracer [44], a recently developed state-of-the-
re
search RTS prototype.
3.1 Methodology
We investigated in detail the data collected from one of
our study participants, with the goal of comparing manual
and automated RTS. We chose P14from Figure 1(for rea-
so
ns described in Section 3.1.2). First, we reconstructed the
st
ate ofP14at every test session. Recall that CodingTracker
doesnotcapture the entire state of the project for any test
session. We had to perform a substantial amount of work
to ﬁnd a code version that (likely) matched the point where
the developer used CodingTracker . We acknowledge the help
of theP14developer who helped with this information, es-
pecially that the code moved from an internal repository
to an external repository. It took several email exchanges
to identify the potential version on top of which we could
replay the CodingTracker changes while still being able to
compile the entire project and execute the tests. Second,
for each test session, we ran FaultTracer [44] on the project
an
d compared the tests selected by the tool with the tests
selected by the developer. Because FaultTracer is a research
prototype, it did not support projects (in the general sense
of the term “software projects”) that are distributed across
multipleEclipse projects (inthespeciﬁcterminology ofwhat
Eclipse calls“projects”) even in the same Eclipse workspace.
We worked around this limitation of FaultTracer by auto-
matically merging all Eclipse projects from P14into one
project that FaultTracer could analyze.
Upon replaying the CodingTracker logs and analyzing the
data, we discovered that the developer often ran multiple
test sessions which had no code changes between them. The
developer had organized the tests in separate test suites and
always selected torunthesetestsuites oneata time, thereby
potentially running multiple test sessions in parallel.
To compare manual and automated RTS fairly and con-
sistently, we accounted for the occurrence of multiple test
sessions without intervening changes. This is because Fault-
366Tracerwould only select to run tests after detecting code
c
hanges between consecutive versions of the software. Our
solution was to merge consecutive test sessions which had no
intervening changes. Consider two consecutive test sessions,
XandY, with no intervening changes. Suppose that the
tests and their outcomes for Xare[test1:OK, test4:OK] ,
andforYare[test1:OK, test2:Failure, test3:OK] .Our
merge would produce a union of the tests in XandY, and
if a test happens to have diﬀerent outcome, the merge would
keep the result from X; however, because the test runs hap-
penedwithoutinterveningchanges, itis reasonable toexpect
that if some tests are re-run, their outcomes should be the
same. We checked that, in our entire study, the test runs
are largely deterministic and found a tiny percentage of non-
deterministic tests (0.6%). The eﬀect of non-deterministic
tests on RTS is a worthwhile research topic on its own [ 20].
Fo
r the sessions XandYshown above, the merged ses-
sion would contain the tests [test1:OK, test2:Failure,
test3:OK, test4:OK] .
Having merged the manual test sessions, the number of
test sessions for comparing manual and automated RTS we
obtained was 683. We further limited our comparison to
the ﬁrst 450 of these 683 test sessions, due to diﬃculties in
automating the setup of P14to useFaultTracer to perform
RTS between successive versions. As we studied a very large
project, which evolved very quickly and had dependencies
on environmentandmanythird-partylibraries, we couldnot
easily automate thesetupacross all 683 mergedtestsessions.
The 450 test sessions used constitute the largest consecutive
sequence of test sessions which had the same setup. (We
discuss other challenges in Section 3.1.3.) Across all 450
te
st sessions considered, P14has (on average) 83,980 lines
of code and 889.32 available tests.
3.1.1 FaultTracer
The inputs to FaultTracer are two program versions—old
versionPand new version P′—and the execution coverage
of tests at version P(i.e., a mapping from test to nodes of
extended control-ﬂow graph [ 44] covered by the test). Let
Tbe
the set of tests in P.FaultTracer produces, as output,
a set of tests T′⊆Tthat are aﬀected by the code changes
between PandP′. The unselected tests in T\T′cannot
change their behavior. Note that one also has to run new
tests that are added in P′but do not exist in P.
We chose FaultTracer because it represents the state-of-
the-research in RTS and implements a safe RTS technique.
Also,FaultTracer works at a ﬁne-granularity level (which
improves its precision), because it tracks coverage at the
level of an extended control-ﬂow graph [ 44]. To identify
co
de changes, FaultTracer implements an enhanced change-
impact analysis. In addition, FaultTracer targets projects
written in Java, the same programming language used in
P14, so, there was a natural ﬁt.
However, note that we chose FaultTracer from a very lim-
ited pool. To the best of our knowledge, there exists no
other publicly available tool that performs test selection
at such ﬁne granularity level (e.g., statement, control-ﬂow
edge, basic block, etc.). Systems such as Google’s TAP sys-
tem [37,38] and Microsoft’s Scout system [ 16,17,34] are pro-
pr
ietary. Moreover, TAP implements a coarse-grained anal-
ysis based on dependencies between modules, which would
be overly imprecise for P14that has only few modules.3.1.2 ProjectunderAnalysis
We chose P14for the following major reasons. First, it
was one of the projects with the largest recorded data (in
terms of the number of test sessions) of all 17. Hence there
was a higher chance of observing a greater variety of test
selection patterns. This also means that we had more data
points over which to compare manual and automated RTS
for the same developer. Second, the developer worked on
creating a large and industrially used library, presenting the
opportunity to study test selection in a realistic setting. Fi-
nally, with the help of the original developer of the project,
we were able to gain access to the exact VCS commits of the
projectwhichmatchedtherecordeddata. Atthetimeofthis
writing, developers of other projects have either been unable
to provide us access to their repositories, or we are unable
to reconstruct the revisions of their projects that matched
the exact period in the CodingTracker recording.
3.1.3 Challenges
Because CodingTracker didnotcaptureentireprojectstate,
we had to reconstruct the P14’s developer’s workspace to be
able to build and run tests for our analysis. Using times-
tamps from the CodingTracker logs, we looked for a com-
mit in the developer’s VCS which satisﬁed the following
conditions: (1) the time of the commit matches the VCS
commit timestamp recorded in the CodingTracker logs and
(2) the code compiles after checking it out of the VCS and
adding required dependencies. Finally, this checked-out ver-
sion was imported into Eclipse and used as a basis for re-
playing the CodingTracker logs. By replaying the changes
captured by CodingTracker on top of this initial state, we
obtained the state of the entire project in every succeeding
test session. Note that CodingTracker captures changes to
both the project under test and the testing code, and thus,
the reconstructed developer’s workspace contained all the
tests available at any given test session. We assume that
the ability to replay the CodingTracker logs from the initial
VCS commit till the end of the logs without any error means
that it was a likely valid starting point. Thus, the recon-
structed workspace is as close to the developer’s workspace
as it existed while CodingTracker monitored the developer’s
programming activity.
To mitigate these challenges in future studies focusing on
RTS,CodingTracker would need to be modiﬁed to capture
the complete initial state of the project as well as any de-
pendencies on external libraries.
3.2 Comparing Manual and Automated RTS
The number of selected tests We plot, in Figure 6, the
nu
mber of tests selected by manual RTS against the number
of tests selected by automated RTS (i.e., FaultTracer ) for
each test session. A quick look may reveal that there is a
substantial diﬀerence between manual and automated RTS,
which we further analyze.
Figure7shows the distribution, across test sessions, of the
nu
mber of tests selected by manual and automated RTS. We
show the distribution for two cases: with (“w/”) and with-
out (“w/o”) single-test sessions. It can be seen that the me-
dian is much lower for the automated tool in both cases.
This implies that the developer is imprecise (i.e., selects
more than necessary). Further, if single-test sessions are
included, we can observe that the arithmetic mean (shown
as a star) is lower for manual than automated RTS. How-
3670 100 200 300 400 500 600 700
Ma
nual0100200300400500600700Automated
Figure 6: Relationship of the number of tests selected in
each test session by manual and automated RTS for P14
w/
Ma
nualw/
Automatedw/o
Manualw/o
Automated0100200300400500600700800Tests selected
Figure 7: Distribution of selected tests for P14with (“w/”)
and without (“w/o”) single-test sessions
ever, when single-test sessions are excluded, we can see the
opposite. This indicates, as expected, that developer focuses
on very few tests while debugging and ignores the other af-
fected tests. Finally, when single-test sessions are excluded
from the manually selected tests, we found that many test
sessions contain the number of tests equal to the median.
Our closer inspection shows this to be due to the lack of
support for arbitrary selection in Eclipse, which forced the
developer to run all tests from one class.
Safety and precision One major consideration in com-
paring manual and automated RTS is the safety of these ap-
proaches. In other words, if we assume that the automated
tool always selects all the tests aﬀected by a code change,
does the developer always select a superset of these? If the
answer is in the aﬃrmative, then the developer is practicing
safe RTS. On the contrary, if the set of tests selected by the
developer does not include all the tests selected by the tool,
it means that manual RTS is unsafe (or the tool is impre-
cise). To compare safety between manual and automated
RTS, for every test session, we compare both the number of
tests selected and the relationship between the sets of tests
selected using both approaches.
Figure6shows the relationship between the numbers of
te
sts selected by both approaches. The Spearman’s and
Pearson’s correlationcoeﬃcientsare0.18 (p-valuebelow0.01)
and 0.00 (p-value is 0.98), respectively. These values indi-
cate a rather low, almost non-existent, correlation.
We compared the relation between the sets of tests se-
lected using manual and automated RTS. In 74% of the test
sessions, the developer missed to select at least one of the
tests selected by FaultTracer . Assuming that FaultTracer is0 100 200 300 400 500 600 700
Te
sts selected0.00.20.40.60.81.0Code changes ratioManual
Automated
Figure 8: Relationship of manual and automated RTS with
re
lative size of code changes for P14
safe, we consider these cases to be unsafe. In the remaining
26% of the test sessions, the developer selected a superset
of tests selected by FaultTracer . Moreover, in 73% of the
test sessions, the developer selected more tests than Fault-
Tracer. Assuming that FaultTracer is precise, we consider
these cases to be imprecise. Note that a developer can be
both unsafe and imprecise in the same test session if the de-
veloper selects some non-aﬀected tests and does not select
at least one of the aﬀected tests. Thus, the sum of the per-
centages reported here (74% + 73%) is greater than 100%.
Correlation with code changes InSection 2.2, we found
th
at forallprojects in our study there is low correlation be-
tween code change ratio and manual RTS. We revisit that
correlation in more detail for the P14project. To further
compare manual and automated RTS, we evaluate whether
either of these selection approaches correlates better with
code changes. Eﬀectively, we re-check our intuition that the
developer is more likely to select fewer tests after smaller
code changes. We measured the Pearson’s and Spearman’s
correlation coeﬃcients for bothmanualandautomatedRTS.
The values for Spearman’s coeﬃcients are 0.22 (p-value be-
low 0.01) and 0.01 (p-value is 0.93) for manual and auto-
mated RTS, respectively. The values for Pearson’s coeﬃ-
cients are 0.08 (p-value is 0.10) and -0.02 (p-value is 0.77)
for manual and automated RTS, respectively. While the
correlation is low in all cases, the slightly higher values of
correlation coeﬃcients for manual RTS may indicate that
(compared to automated RTS) the developer indeed selects
fewer tests after smaller changes and more tests after larger
changes, as it becomes harder to reason which tests are af-
fected by larger changes. The plot in Figure 8visualizes
th
e relationship, for each test session, between code change
ratio and the number of selected tests for both manual and
automated RTS. We can observe that manual RTS is less
likely to select many tests for small changes (e.g., fewer red
dots than blue dots are close to the x-axis around the 600
mark). In the end, the size of semantic eﬀect of a change
(as measured by the number of aﬀected tests) is not easy to
predict from the size of the syntactic change (as measured
by the number of AST nodes changed).
Performance Weﬁnallycomparemanualandautomated
RTS based on the time taken to select the tests. Figure 9
shows the distribution of selection time (ﬁrst boxplot), as
de
ﬁned in Section 2.1, and analysis time (second boxplot)
in
curred by FaultTracer . We can observe that the developer
is faster than the automated RTS tool in selecting which
tests to run (the p-value for the Mann-Whitney U test is
368Selection
Ma
nualAnalysis
AutomatedUnselected
AutomatedSelected
Automated050100150200250300Time (sec)
Figure 9: Distribution for P14of estimated manual selection
time (ﬁrst boxplot), automated analysis time (second box-
plot), execution time of the tests unselected by FaultTracer
(third boxplot), and execution time of the selected tests by
FaultTracer (fourth boxplot)
below 0.01). For comparison, we also show the distribution
of estimated execution time for tests that are unselected
byFaultTracer (third boxplot) and actual execution time
for tests selected by FaultTracer (fourth boxplot). We ran
all our experiments on a 3.40 GHz Intel Xeon E3-1240 V2
machine with 16GB of RAM, running Ubuntu Linux 12.04.4
LTS and Oracle Java 64-Bit Server version 1.6.0 45.
On
e can observe that FaultTracer analysis took substan-
tial time. Although the analysis time (130.94 ±13.77 sec-
onds) is, on average, less than the time saved by not running
unselected tests (219.86 ±68.88 seconds), it is important to
note that one may also want to take into accounttime to col-
lect necessary coverage information to enable change impact
analysis; if time taken for analysis plus overhead for collect-
ing coverage plus running selected tests is longer than time
taken for running all the tests, then test selection provides
no beneﬁt. This raises the question whether a ﬁne-grained
technique, such as the one implemented in FaultTracer [44],
ca
n be optimized to bring beneﬁts to smaller projects. One
of our planned future directions is to explore which gran-
ularity level of automated RTS techniques is appropriate
for most projects. Further, we believe that research studies
on automated RTS should provide more information about
their complexity (e.g., time to implement the technique)and
eﬃciency (e.g., analysis time, time to collect coverage, etc.).
Previous research focused mostly on the number of selected
tests (i.e., safety and precision), which is not suﬃcient for
proper comparison anddiscovering a test selection technique
that works in practice.
4. DISCUSSION
We brieﬂy discuss test selection granularity, our experi-
ence with an IDE-integrated automated RTS tool, and pro-
pose a potential improvement to automated RTS in IDEs.
Test Selection Granularity We mentioned earlier that
systems such as TAP at Google [ 37,38] and Scout at Mi-
cr
osoft [16,17,34] are successfully used for test selection/pri-
or
itization. However, these systems are used as part of the
gated check-in [ 9] infrastructure (i.e., all aﬀected regression
te
sts are executed before a commit is accepted into the cen-
tral repository). In other words, they are not used (and are
not applicable) on developers’ machines where developers
commonly work on few modules at a time (and run testslocally). Even developers at either of these companies, let
alone many developers who do not develop code at the scale
of Google or Microsoft, would beneﬁt from an improved ﬁne-
grained test selection. This provides motivation for research
on ﬁnding the best balance between analysis time, imple-
mentation complexity, and beneﬁts obtained from test se-
lection. Improved ﬁne-grained test selection would be more
widely applicable and could be used in addition to coarse-
grained test selection systems.
Experience with IDE-integrated automated RTS We
experimented with Visual Studio 2010, the only tool (to the
best of our knowledge) that integrates automated RTS with
an IDE. We did this to see if such a tool would perform
better than manual RTS in terms of safety and precision.
Speciﬁcally, the Test Impact Analysis (TIA) tool in Visual
Studio 2010 [ 36] was designed to help reduce testing eﬀort
by
focusing on tests that are likely aﬀected by code changes
made since the previous run of the tests. We think this is
an excellent step towards improved RTS in developer en-
vironments and that similar tools should be developed for
other IDEs. We successfully installed TIA and ran it on sev-
eral simple examples we wrote and on an actual open-source
project. However, we found a number of shortcomings with
TIA. Most importantly, the tool is unsafe: any change not
related to a method body is ignored (e.g., ﬁeld values, anno-
tations, etc.). Also, changes like adding a method, removing
a method, or overriding a method remain undetected [ 28].
Fu
rthermore, TIA does not address any of the issues com-
monly faced byselection techniques[ 1,3,5,7,14,15,31,40,42],
su
ch as library updates, reﬂection, external resources, etc.
Our opinion is that a safe but imprecise tool would be more
appreciated by developers.
Potential improvement of IDEs Across all projects,
we observed that developers commonly select tests during
debugging. Thus, one common way by which an IDE might
help is to oﬀer two separate modes of running tests, a reg-
ular mode and atest selection mode . In the regular mode,
the developer may choose to re-run, after a series of code
changes, one or more previously failing tests (while ignoring
other aﬀected tests). Once the test passes, the developer
may run in the test selection mode to check for regressions.
Notice that the test selection runs would be separated by a
series of regular runs. Consider two test selection runs, A
andB(Figure10). InA,some tests were selected to be run
a
nd failed. Developer then performs (regular) runs a1,a2, ...
an, untilthe previouslyfailing test passes. The test selection
runBis then executed to ensure that there are no regres-
sions due to code changes, since A. Note that the analysis
performed before running Bshould consider the diﬀerence
sinceAand not just the diﬀerence between anandB; other-
wise, tests aﬀected by the changes between Aandanwould
not be accounted for. As a simple optimization step, the
tool could exclude the tests aﬀected between AandBthat
were already run after the change that was aﬀecting them.
5. THREATS TO VALIDITY
External: Developers, Projects, and Tools The re-
sults of our study may not generalize to projects outside of
the scope of our study. To mitigate this threat, we used 17
projects that cover various domains and 14 developers with
diﬀerent levels of programming experience. Further, these
projects vary signiﬁcantly in size, number of developers, and
number of tests. Regarding the comparison of manual and
369A ... ...B a a 1 nRegular runs
Selection runs
Figure 10: Anexample of a common patternwhen developer
al
ternates selection and regular runs
automated RTS, we used the largest project for which we
could reconstruct the entire state for many test sessions.
We used FaultTracer , a research prototype, to perform
automated RTS. Other tools [ 1,3,5,7,14,15,31,40,42] that
im
plement diﬀerent test selection techniques could have led
to diﬀerent results. We chose FaultTracer because it imple-
ments a safe and precise test selection technique. To the
best of our knowledge, no other publicly available tool for
test selection exists (except the proprietary tools that work
at coarse-granularity level, which would not be applicable
to any of the projects used in our study). Our experience
with VisualStudio 2010 demonstrated that the implemented
approach is unsafe, thus inappropriate for our study.
Finally, the patterns of test selection could diﬀer in other
languages but Java. We leave the investigation of how man-
ual RTS is performed in other languages for future work.
Internal: Implementation Correctness We extracted
data relevant to manual RTS from the study participants’
recordings. To extract the data, we wrote analyzers on top
of the infrastructure that we used in our prior research stud-
ies on refactorings [ 21,23,39]. Further, new analyzers and
sc
ripts were tested and reviewed by at least two authors.
Construct: IDEs and Metrics Because CodingTracker
is implemented as an Eclipse plugin, all developers in our
study used Eclipse IDE. Therefore, our study results may
not hold for other IDEs. However, because Eclipse is the
most popular IDE for Java [ 19], our results hold for a sig-
ni
ﬁcant portion of Java developers. We leave the replication
of our study using other popular IDEs (both for Java and
other languages) for future work.
6. RELATED WORK
Wecomplement existingwork on automatedRTS[ 1,3,5,7,
14,15,31,40,42] by investigating actual RTS practices of de-
ve
lopers. Over the last three decades, many automated RTS
techniques were proposed, which have mostly focused on im-
proving the precision and safety of test selection [ 24]. One
ke
ydiﬀerenceamongthesetechniquesisthegranularitylevel
at which they detect aﬀected tests (e.g., statement, control-
ﬂow edge, method, etc.). Recent work [ 10,16,34,37,38] has
al
so reported some success with automated RTS in large
software projects. Unfortunately, after decades of research,
nopractical RTStool is available for widespread adoptionby
developers. The need for a practical tool is becoming even
more urgent with the widespread adoption of agile devel-
opment, which heavily relies on (regression) testing [26 ,35].
In
particular, although we did not account for Test Driven
Development (TDD) in our study, the practical impact of
such testing-oriented software development methodologies
on RTS is still largely unknown. Therefore, our study aims
to shed light on common manual RTS practices.
Our study is diﬀerent in scope, emphasis and approach
from that of Greiler et al. [ 13], who recently conducted ast
udy of testing practices among developers. We did not
limit our scope to a speciﬁc class of software (they focus on
testing component-based software). Their emphasis is on
answering important questions about the testing practices
that are (not) adopted by organizations and why. On the
other hand, we focus on howdevelopers perform RTS. Fi-
nally, their approach utilizes interviews and surveys, but we
analyzed data collected from developers in real time.
Concerning the empirical study of RTS techniques, the
closest work to ours is the use of ﬁeld study data by Orso
et al. [25]. They collected usage proﬁle data from us ers
of deployed software for tuning their Gamma approach for
RTS and impact analysis. We study data collected from de-
velopers to gain insight on improving manual RTS. Other
empirical studies have been conducted, mainly to evaluate
proposed automated RTS techniques [ 27,30,33,41]. Our
st
udy is not technique-speciﬁc; rather, we investigate com-
mon manual RTS practices.
Although our work is based on data that has been used
previously, this is the ﬁrst use of the data for studying how
developers perform testing. The data was previously used
to show that VCS commit data is imprecise and incomplete
for studying software evolution [ 23], for comparing manual
an
d automated refactorings [ 21], and for mining ﬁne-grained
co
de change patterns [22 ].
7.
CONCLUSIONSAND FUTUREWORK
In this paper, we provided evidence on the pervasiveness
of manual RTS and compared manual and automated RTS.
The results show that, while developers commonly perform
manual RTS, they need better support to perform manual
RTS while working on their typically-sized projects. In par-
ticular, all but two of the 14 developers in our three-month
user study performed ad-hoc manual RTS in 59.19% of test
sessions. The practice of manual RTS occurred regardless of
project properties like project size, test execution time, and
the size of code changes made before test runs. Further, we
found that manual RTS is most commonly performed dur-
ing debugging. By comparing manual and automated RTS,
we also showed how they diﬀer in terms of safety, precision,
and performance. We found that manual RTS, compared
to an automated RTS tool, selected to run more tests than
necessary in 73% of the 450 test sessions that we evaluated,
and in 74% of the test sessions selected fewer tests than were
aﬀected by the most recent code changes.
Future Work There are three main thrusts in our planned
future work. First, we want to work on ﬁnding a better
balance between automated RTS granularity level and im-
plementation complexity, so that we can deliver a widely
used tool for automated RTS. Second, we plan to create
RTS techniques that incorporate the knowledge of common
debugging scenarios to improve developers’ debugging expe-
rience. Finally, we plan to investigate how developers man-
ually perform other activities related to regression testing,
such as test-suite reduction and test prioritization.
Acknowledgments We thankLamyaa Eloussi, AlexGyori,
Rohan Sharma, and August Shi for helping with the experi-
ments, LingmingZhangfor providing FaultTracer , andAlek-
sandar Milicevic for providing feedback on an early draft
of this paper. This material is based upon work partially
supported by the National Science Foundation under Grant
Nos. CCF-1012759 and CCF-1439957, and by Boeing under
Grant No. ITI RPS #28.
3708. REFERENCES
[1
] T. Ball. On the limit of control ﬂow analysis for
regression test selection. In ISSTA, 1998.
[2] J. Bible, G. Rothermel, and D. S. Rosenblum. A
comparative study of coarse- and ﬁne-grained safe
regression test-selection techniques. TOSEM, 2001.
[3] L. Briand, Y. Labiche, and S. He. Automating
regression test selection based on UML designs. IST,
2009.
[4] CodingTracker. http://codingtracker.web.engr.
il
linois.edu/ .
[5
] R. A. DeMillo, R. J. Lipton, and F. G. Sayward. Hints
on test data selection: Help for the practicing
programmer. Computer , 1978.
[6] S. G. Elbaum, G. Rothermel, S. Kanduri, and A. G.
Malishevsky. Selecting a cost-eﬀective test case
prioritization technique. SQJ, 2004.
[7] E. Engstr ¨om and P. Runeson. A qualitative survey of
regression testing practices. In PROFES , 2010.
[8] K. Fischer, F. Raji, and A. Chruscicki. A methodology
for retesting modiﬁed software. In NTC, 1981.
[9] Use a gated check-in build process to validate changes.
http://msdn.microsoft.com/en-us/library/
dd
787631.aspx .
[1
0] M. Gligoric, R. Majumdar, R. Sharma, L. Eloussi, and
D. Marinov. Regression test selection for distributed
software histories. In CAV, 2014.
[11] J. Goodenough and S. Gerhart. Toward a theory of
test data selection. TOSEM, 1975.
[12] T. L. Graves, M. J. Harrold, J.-M. Kim, A. Porter,
and G. Rothermel. An empirical study of regression
test selection techniques. In ICSE, 1998.
[13] M. Greiler, A. van Deursen, and M. Storey. Test
confessions: A study of testing practices for plug-in
systems. In ICSE, 2012.
[14] M. J. Harrold, J. A. Jones, T. Li, D. Liang, A. Orso,
M. Pennings, S. Sinha, S. A. Spoon, and A. Gujarathi.
Regression test selection for Java software. In
OOPSLA , 2001.
[15] M. J. Harrold and M. L. Soﬀa. An incremental
approach to unit testing during maintenance. In
ICSM, 1988.
[16] J. Hartmann. Applying selective revalidation
techniques at Microsoft. In PNSQC, 2007.
[17] J. Hartmann. 30 years of regression testing: Past,
present and future. In PNSQC, 2012.
[18] J. L. Hintze and R. D. Nelson. Violin plots: A box
plot-density trace synergism. The American
Statistician , 1998.
[19] Java, Java everywhere, 2012. http://sdtimes.com/
co
ntent/article.aspx?ArticleID=36362 .
[2
0] Q. Luo, F. Hariri, L. Eloussi, and D. Marinov. An
empirical analysis of ﬂaky tests. In FSE, 2014. to
appear.
[21] S. Negara, N. Chen, M. Vakilian, R. E. Johnson, and
D. Dig. A comparative study of manual and
automated refactorings. In ECOOP, 2013.
[22] S. Negara, M. Codoban, D. Dig, and R. E. Johnson.
Mining ﬁne-grained code changes to detect unknown
change patterns. In ICSE, 2014.[23] S. Negara, M. Vakilian, N. Chen, R. E. Johnson, and
D. Dig. Is it dangerous to use version control histories
to study source code evolution? In ECOOP, 2012.
[24] A. K. Onoma, W.-T. Tsai, M. Poonawala, and
H. Suganuma. Regression testing in an industrial
environment. Communications , 1998.
[25] A. Orso, T. Apiwattanapong, and M. J. Harrold.
Leveraging ﬁeld data for impact analysis and
regression testing. In FSE, 2003.
[26] D. Parsons, T. Susnjak, and M. Lange. Inﬂuences on
regression testing strategies in agile software
development environments. SQJ, 2013.
[27] X. Qu, M. B. Cohen, and G. Rothermel.
Conﬁguration-aware regression testing: An empirical
study of sampling and prioritization. In ISSTA, 2008.
[28] X. Ren, F. Shah, F. Tip, B. G. Ryder, and O. Chesley.
Chianti: A tool for change impact analysis of Java
programs. In OOPSLA , 2004.
[29] G. Rothermel and M. Harrold. Selecting regression
tests for object-oriented software. In ICSM, 1994.
[30] G. Rothermel and M. Harrold. Empirical studies of a
safe regression test selection technique. TOSEM, 1998.
[31] G. Rothermel and M. J. Harrold. A safe, eﬃcient
regression test selection technique. TOSEM, 1997.
[32] G. Rothermel, R. H. Untch, C. Chu, and M. J.
Harrold. Test case prioritization: An empirical study.
InICSM, 1999.
[33] D. Saﬀ and M. D. Ernst. An experimental evaluation
of continuous testing during development. In ISSTA,
2004.
[34] A. Srivastava and J. Thiagarajan. Eﬀectively
prioritizing tests in development environment. In
ISSTA, 2002.
[35] D. Talby, A. Keren, O. Hazzan, and Y. Dubinsky.
Agile software testing in a large-scale project.
Software , 2006.
[36] Streamline testing process with test impact analysis,
2013.http://msdn.microsoft.com/en-us/library/
ff
576128%28v=vs.100%29.aspx .
[3
7] Testing at the speed and scale of Google, 2011.
http://goo.gl/OKqBk .
[3
8] Tools for continuous integration at Google scale, 2011.
http://www.youtube.com/watch?v=b52aXZ2yi08 .
[3
9] M. Vakilian, N. Chen, S. Negara, B. A. Rajkumar,
B. P. Bailey, and R. E. Johnson. Use, disuse, and
misuse of automated refactorings. In ICSE, 2012.
[40] D. Willmor and S. M. Embury. A safe regression test
selection technique for database-driven applications.
InICSM, 2005.
[41] W. E. Wong, J. R. Horgan, S. London, and
H. Agrawal. A study of eﬀective regression testing in
practice. In ISSRE, 1997.
[42] S. Yau and Z. Kishimoto. A method for revalidating
modiﬁed programs in the maintenance phase. In
COMPSAC , 1987.
[43] S. Yoo and M. Harman. Regression testing
minimization, selection and prioritization: A survey.
STVR, 2012.
[44] L. Zhang, M. Kim, and S. Khurshid. Localizing
failure-inducing program edits based on spectrum
information. In ICSM, 2011.
371