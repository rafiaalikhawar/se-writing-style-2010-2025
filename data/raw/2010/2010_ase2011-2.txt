Generalizing Evolutionary Coupling
with Stochastic Dependencies
Sunny Wong
Siemens Healthcare
Malvern, PA, USA
sunny.wong@siemens.comYuanfang Cai
Dept. of Computer Science
Drexel University
Philadelphia, PA, USA
yfcai@cs.drexel.edu
Abstract—Researchers have leveraged evolutionary coupling
derived from revision history to conduct various software a naly-
ses, such as software change impact analysis (IA). The probl em
is that the validity of historical data depends on the recenc y of
changes and varies withdifferentevolution paths—thus,in ﬂuenc-
ing the accuracy of analysis results. In this paper, we forma lize
evolutionary coupling as a stochastic process using a Marko v
chainmodel.Byvaryingtheparameters of thismodel,we deﬁn ea
family of stochastic dependencies that accounts for different types
of evolution paths. Each member of this family weighs histor ical
data differently according to their recency and frequency. To
assess the utility of this model, we conduct IA on 78 releases of
ﬁve open source systems, using 16 stochastic dependency typ es,
and compare with the results of several existing approaches . The
results show that our stochastic-based IA technique can pro vide
more accurate results than these existing techniques.
Index Terms —impact analysis, stochastic dependency, evolu-
tionary coupling, Markov chain
I. INTRODUCTION
Researchers have leveraged revision history to identify
evolutionary coupling between components by checking how
components historically change together [1]–[3]. The effe c-
tiveness of conducting various software analyses based on
evolutionary coupling has been demonstrated. For example,
Cataldo et al.’s empirical studies [4] suggest that evoluti on-
ary dependencies provide more accurate estimates of coor-
dination requirements than structural dependencies. Soft ware
change impact analysis1(IA) [5] exempliﬁes another analysis
technique where evolutionary dependencies often outperfo rm
structural dependencies. Different from traditional chan ge
impact analysis techniques that are often performed based
on static dependency structure, evolutionary-coupling-b ased
IA captures semantically coupled components that may not
structurally depend on each other.
Although historical data has been shown to be a useful
resource for various analyses, the validity of historical d ata
depends on the recency of changes and varies with different
evolution paths. For example, the amount of history needed
to accurately estimate change impact would be different for a
1Change impact analysis (also called change propagation ana lysis and
change scope analysis) is the method of determining what sof tware elements
(components, ﬁles, etc.) are likely to change (called the ch ange scope or the
impact scope) when, or after, a certain set of elements (call ed the starting
impact set) changes [5].relatively new system that is frequently refactored than fo r a
long established system with a stable architecture.
To improve the accuracy of history-based analysis tech-
niques, we present a formalized generalization of evolutio nary
dependencies based on probability theory, which we call
stochastic dependencies . This formalization deﬁnes a family
of dependency types. To account for different evolution pat hs,
each member of the family weighs historical data differentl y
according to their recency and frequency. Our stochastic
dependencyfamily can be used to augment prevailing history -
based analysis techniquesby selecting a member of the famil y
that ﬁts the project in consideration.
Our stochastic dependency framework is based on the
Markov chain model [6], a probabilistic model often used in
artiﬁcial intelligence for determining the expected resul t of a
randomvariable/event.Ourassumptionisthat each transac tion
(i.e., atomic commit) in revision history can be modeled as a
random variable. To address the temporal issue of changing
dependencies that invalidate historical data, our approac h ﬁrst
limits the length of historical data used for analysis. This
limited sequence of transactions create a sliding window
that resembles a Markov chain (discrete k-th order Markov
process) in which each state of the chain is a transaction fro m
revision history.
FromtheMarkovchain,wecancomputetheprobabilitythat
two components are coupled (i.e., they will both be changed
in the next transaction). This probability value can be used to
reason about whether a componentwill be in the impact scope
of a change. To account for the importance of recent history
over distant history, our model for computing this dependen cy
probability uses a smoothing function to control how much
each transaction contributes to the predicted probability . The
two parameters to our framework (i.e., the length of history to
useandthetypeofsmoothingfunction)allowforthedeﬁniti on
of a family of stochastic dependency types.
To evaluate our approach, we conduct change impact anal-
ysis on 78 releases of ﬁve open source systems by varying
the values of the two stochastic dependency parameters. Our
results show that the stochastic-based change impact anal-
ysis can provide more accurate results than two traditional
structure-based IA approaches and an existing history-bas ed
IA approach for all ﬁve systems.978-1-4577-1639-3/11/$26.00 c2011 IEEE ASE 2011, Lawrence, KS, USA293The rest of this paper is organized as follows: Section II
presentsrelated work. Section III uses an example to overvi ew
the stochastic dependency computation for change impact
analysis. Section IV formally deﬁnes the family of stochast ic
dependencies. Section V presents our evaluation method and
empirical results. Section VI discusses threats to validit y and
future work. Section VII concludes.
II. RELATEDWORK
In this section, we review related work and differentiate ou r
stochastic based technique from existing approaches.
A. Impact Analysis
Numerous IA techniques have been proposed. Below, we
discuss several representative categories of IA approache s.
1) Structure-based Impact Analysis: Various IA techniques
have been developed using software dependency structures.
For example,Briand et al. [7] and Elish and Rine [8] proposed
IA techniques based on the metrics of Chidamber and Ke-
merer[9].Robillard[10]andRajlich[11]presentedalgori thms
based on dependency graph structures. To capture impact tha t
cannot be easily identiﬁed throughsyntactic analysis, som e IA
approaches(e.g.Apiwattanaponget al. [12])use dynamic(i .e.,
instance-based rather than class-based) analysis to measu re
coupling.
2) History-based Impact Analysis: Ying et al. [3] and
Zimmermann et al. [2] leverage revision histories to perfor m
IA based on association rules that identify evolutionary (o r
logical) coupling [1] between software elements. An associ a-
tion rule, of the form a⇒b, states that if ais changed then b
will likely also be changed. These rules are prioritized bas ed
on the heuristics of supportandconﬁdence .Supportis deﬁned
as the number of transactions that aandboccur together in
revision history. Conﬁdence is deﬁned as supportdivided by
the number of times that aoccurs (with or without b). With
minimum support and conﬁdence thresholds, association rul es
are selected to predict the impact scope of a change starting
from a.
Evolutionary dependency approaches often consider the
transactions in the overall revision history to be a multise t and
disregard the temporal sequence (i.e., ordering) of the tra ns-
actions. Two approaches that include temporal information in
IA include the work of Bouktif et al. [13] and Ceccarelli et
al. [14]. Different from our approach of using Markov chains ,
their purpose is to infer cause-effect relationships from t he
temporal data, rather than to ﬁlter out obsolete history dat a.
The Evolution Radar [15] is another approach that con-
siders the temporal sequence of transactions. It allows for
the tracking of evolutionary dependencies over time by di-
viding revision history into time intervals. However, its g oal
is different from our approach in that it is meant for the
interactive visualization of dependency changes to aid the
detection architecture decay.
While these existing history-based techniques are effec-
tive at identifying semantic dependencies between softwar e
elements, refactorings that remove these dependencies maycause the approaches to overestimate the impact scope. Sinc e
the support heuristic never decreases, once support passes
the minimum threshold, a dependency between two elements
continues to exist. Although the conﬁdence heuristic can be
usedinconjunctionwithsupport,asthenumberoftransacti ons
becomes large, conﬁdence only minimally changes with each
transaction. Our stochastic dependency framework address es
this issue by limiting the length of historical information
analyzed and uses a smoothing function to emphasize more
recent history over distant history.
Cataldo [16] explored the question of how many months of
revisionhistoryareenoughtoaccuratelycomputeevolutio nary
dependencies and coordination requirements. He construct ed
task dependency matrices on monthly increments to ﬁnd
when a matrix no longer signiﬁcantly differs from a previous
matrix. Although he found a ﬁx point (19 months) when the
dependencies stabilized, the result is hard to generalize t o
other software systems. Recent architectural refactoring s can
signiﬁcantly alter the structure of dependencies, invalid ating
previous revision history.
3) Probabilistic Impact Analysis: Recently, several impact
analysis techniques have been proposed that are based on
probability theory. For example, Tsantalis et al. [17] deﬁn e
probabilities of impact based on structural relations betw een
software elements in object-oriented designs. Abdi et al. [ 18]
use a Bayesian network (a probabilistic model) with structu ral
coupling metrics to construct the inference model. Mirarab et
al. [19] also use a Bayesian network and combine structural
coupling with historical data in constructing the network.
While our approach also uses a probabilistic model (Markov
chain), stochastic dependencies use the temporal sequence of
transactions in revision history to account for changes to t he
dependency structure over time.
B. Markov Processes
Markov processes (of which Markov chains are a speciﬁc
type) are probabilistic models that are widely used in artiﬁ cial
intelligence (e.g. reinforcement learning [20]) and vario us
other computing ﬁelds (e.g. web search engine algorithm [6] ).
Markov processes have also been applied to software en-
gineering (e.g. generate test inputs [21], classify softwa re
behavior [22], predict component reliability [23]). To the best
ofourknowledge,Markovprocesseshavenotyetbeenapplied
to computing evolutionary coupling.
III. FRAMEWORK OVERVIEW
In this section, we present a high-level overview of our
stochastic dependency framework, using concrete (but hypo -
thetical) examples to demonstrate how to compute stochasti c
dependencies and how to conduct IA using them. The next
section provides more rigorous, formal deﬁnitions and expl a-
nationsfortheconceptsdiscussedhere.Foralltheexample sin
this section, we consider that we have the following sequenc e
of transactions in our revision history: {a, b},{a, c},{d},
{a, b},{a},{a, b, c},{b, d},{a},{a, d},{c},{a, c},{a};
where {a}is the most recent transaction.294Intuitively,a stochastic dependency (β, α)is theprobability
that, if the next transaction includes α, it will include β. To
compute this probability, we look at the previous transacti ons
thatinclude α.Eachoneofthesetransactionsthatalsoincludes
βgives us evidence that a dependency exists (i.e., βdepends
onα). To account for different evolution paths and changing
design structures, we use two parameters in computing the
stochastic dependency value: a limit on history length and a
smoothingfunction.Thehistorylengthparameter kdeﬁnesthe
number of transactions to look at; thereby, ignoring transa c-
tions that occurred in the distant past. The smoothing funct ion
λallows us to give more weight to transactions that occurred
recently over those from the distant past.
We ﬁrst consider an example of limiting to the last ﬁve
transactions ( k= 5) and using a linear λsmoothing function
(i.e.,theusefulnessofhistoricaldatadecayslinearlywi thtime,
as shown in Table I). To compute the stochastic dependencies
onc, we ﬁrst ﬁnd the last ﬁve transactions that involve c:
{a, c},{a, b, c},{c},{a, c}. Since chas only been involved
in four transactions so far, we only use the four available
transactions. Then we compute the stochastic dependencies
as shown below:
{a, c}{c}{a, b, c}{a, c} Pr
aλ1·1λ2·0 λ3·1 λ4·10.67
bλ1·0λ2·0 λ3·1 λ4·00.20
dλ1·0λ2·0 λ3·0 λ4·00
From these stochastic dependency values, we can perform
IA and estimate the impact of changing c. There are various
methods to use these values for IA. The simplest way is to
use a threshold-base rounding scheme. If we use 0.5 as the
minimumthreshold,thenwe wouldpredictthatonly aislikely
to be in the impact scope of c.
As another example, we consider analyzing the impact
of changing a. The last ﬁve transactions that involve aare
{a, b, c},{a},{a, d},{a, c},{a}. Again we use the same
linear smoothing function:
{a}{a, c}{a, d}{a}{a, b, c} Pr
bλ1·0λ2·0λ3·0λ4·0 λ5·10.07
cλ1·0λ2·1λ3·0λ4·0 λ5·10.33
dλ1·0λ2·0λ3·1λ4·0 λ5·00.27
Using the same minimum threshold of 0.5, we would expect
that changing awill not impact any other elements.
IV. FORMALIZATION
In this section, we ﬁrst present the deﬁnitions and mathe-
maticalnotationsto formallydeﬁnethe stochasticdependency .
After that, we describe a method for computing the depen-
dency value.A. Deﬁnitions and Background
LetE={ei}N
i=1be the set of software elements in
the system, where Nis the total number of elements.2Let
T={ti}M
i=1be the sequence of revision history trans-
actions, each involving a subset of software elements (i.e. ,
∀ti∈T:ti⊆E), where Mis the length of T. We
can view Tas a stochastic process, where each tiis a
discrete random variable with domain 2E. For any element
e∈E, letT(e)=/braceleftBig
t(e)
i∈T|e∈t(e)
i/bracerightBig
be the subsequence
ofTinvolving e. Without loss of generality, we use separate
indexing sequences for TandT(e)—in other words, t(e)
1is
not necessarily the ﬁrst element of T,t(e)
2is not necessary the
second element of T, etc. Then let M(e)be the length of T(e).
Giventwo elements a, b∈E, letC(a,b)=/braceleftBig
X(a,b)
i/bracerightBigM(a)
i=1be
astochasticprocessthatmodelswhether bisinthetransactions
that involve a:
X(a,b)
i =/braceleftBigg
1ifb∈t(a)
i
0otherwise
We deﬁne the stochastic dependency (b, a)at time τas
Pr/parenleftBig
X(a,b)
τ = 1/parenrightBig
. The method for computing this probability
varies among the different types of stochastic dependencie s.
Unlike some existing dependency deﬁnitions, in which a
dependencyeitherexistsordoesnot,wedeﬁnethatanelemen t
is stochastically dependent upon another with a continuous
probability. Given a starting impact set a, we compute its
stochastic dependents from all other elements b∈E\ {a}
to determine if bwill be in the impact scope of a. In this
paper, we use the terms change scope and impact scope
interchangeably.
With the probability values of stochastic dependencies,
we can reason about which elements are expected to be in
an impact scope. While we can simply sort the software
elements by decreasing probability values and present the
list to a maintainer, this continuous range also allows for
varioustechniquesto reducethe numberofelementspresent ed
to a maintainer. For example, we can deﬁne a minimum
probability threshold and consider all elements above that
threshold to be in the impact scope. Alternatively, we can
use randomized rounding [24] to select the likely impacted
elements. For simplicity, we use a minimumthresholdstrate gy
for our evaluation in Section V. Next, we formally deﬁne our
method of computing stochastic dependencies.
B. Computing Stochastic Dependencies
Given a sequence of τ−1transactions in revision history
involving an element a, we have deﬁned the probability that
another element bwill be involved in the next transaction
involving aas thestochastic dependency from btoaat time
τ. For each of the τ−1transactions involving a, letxibe the
value of X(a,b)
τ−i(i.e., xiindicates whether bandaoccurred
together in the (τ−i)-th transaction involving a). Then we
2Although software elements may be added and deleted over tim e, we use
Eto refer to the set of elements in the software at the time of in terest.295deﬁnetheprobabilitythat bisstochasticallydependon awhen
theτ-th transaction involving aoccurs as follows:
Pr/parenleftBig
X(a,b)
τ = 1/parenrightBig
≡
Pr/parenleftBig
X(a,b)
τ = 1|X(a,b)
τ−1=x1∧ · · · ∧ X(a,b)
1 =xτ−1/parenrightBig
As a ﬁrst step in accounting for different evolution paths of
software,we consideronlythelatest ktransactionsinvolving a
foranalysis.Weemphasizethatthesearethelast ktransactions
that software element ais involved in, but not necessarily
the latest ktransactions in the revision history. Selecting an
appropriate value for kcan affect the accuracy of impact
analysis. If kis too large then dependencies may have been
removedduringevolution.On the other hand,if kis two small
then semantic dependencies between components may not be
detected. We investigate the effects of various kvalues in our
evaluation reported in Section V.
Only considering the latest ktransactions creates a sliding
window that resembles a discrete k-th order Markov process
(Markov chain), which leads to our method of computing
the stochastic dependency probability. A Markov chain is a
stochastic process with a property that the next state depen ds
onlyonthe currentstate and a ﬁnite numberof previousstate s,
but not the entire history of states. A k-th order Markov chain
depends on the current state and the k−1previous states—or
formally, Pr (Yi|Yi−1, . . . , Y 1)≡Pr (Yi|Yi−1, . . . , Y i−k).
A prevailing model [6] for high order Markov chains
deﬁnes the probability for a next state to be based on a
linear combination from the previous states. Based on this
prevailing model, we derive a method to compute stochastic
dependencies that is also based on a linear combination of
previous states:
Pr/parenleftBig
X(a,b)
τ = 1/parenrightBig
≡k/summationdisplay
i=1λiX(a,b)
τ−i
where λi∈[0,1]
andk/summationdisplay
i=1λi= 1
Based on this model, every time bchanges with ain a
transaction, we gain evidence that bdepends on aand this
evidence contributes to the computed probability. Intuiti vely,
ifboften changes with arecently, then it is more likely that it
will changewith ain the nearfuture.On the contrary,if bonly
changes with ain the distant past, but not recently, it is more
likelythatthisdependencyhasbeenremovedduringevoluti on.
To capturethis temporalphenomenon,we use a monotonically
decreasing smoothing function,3λ, to account for software
evolution and weigh more recent transactions more heavily
than older transactions. Table I shows several functions th at
can be used as the smoothing function λ.
3Technically, λ={λi}k
i=1is a sequence of kvalues but it can be
intuitively understood as a function that maps to the approp riate sequence
value. Hence, we interchangeably refer to it as either a sequ ence or a function.The ﬁrst column shows a constant function in which all
the transactions are weighed the same regardless of how long
ago they occurs, as with the traditional evolutionary depen -
dency deﬁnition. The second column shows a linear function,
indicating that the usefulness of a transaction in terms of
determining stochastic dependency decreases linearly ove r
time.Thethirdcolumnshowsasinusoidalfunctionthatweig hs
the most recent transactions similarly, and older transact ions
lessandless.Thelastcolumnshowsanexponentiallydecayi ng
function, which models a rapid decreasing of the usefulness
of a transaction over time. These last three functions weigh
past transactions less heavily than recent transactions.
By using a limited history kand a decreasing λsequence,
our stochastic dependencies potentially can recover more
quickly from refactorings, which may signiﬁcantly alter th e
dependencystructure of a design, than traditional evoluti onary
dependencies that only use conﬁdence to detect such changes .
Our stochastic dependency deﬁnition is a generalization of
traditional evolutionarydependenciesbecause they repre sent a
speciﬁc kvalue and λsequence for stochastic dependencies.
For example, to use a minimum support value of σand
minimum conﬁdence of χin determining traditionally-deﬁned
evolutionary dependency, we can compute the evolutionary
dependency (b, a)at the time when τ-th transaction involving
aoccurs using our stochastic dependency model by letting
k=τ(considering all the existing transactions involving a)
andλ={1/k}k
i=1(treating these transactions equally). Then
aevolutionarydependency (b, a)issaidtoexist attime τifthe
probabilityexceedsboththeminimumsupportandconﬁdence :
Pr/parenleftBig
X(a,b)
τ = 1/parenrightBig
≥max/braceleftBig
χ,σ
τ/bracerightBig
V. EVALUATION
To evaluate the effectiveness of our stochastic model, we
conductchangeimpactanalysisusingthestochasticdepend en-
cies deﬁned in the previous section. We compare the accuracy
of IA using 16 members of the stochastic dependency family
with different combinations of k(length of history) and λ
(smoothing function). We also compare the results with two
traditional structure-based IA techniques and the prevail ing
evolutionary-coupling-based IA techniques, aiming to ans wer
the following questions:
Q1: Are stochastic-based IA approaches more accurate than
traditional structure-based IA techniques? For each sub-
ject system, we compare the accuracy of each stochastic-
dependency-basedIA with structure-basedIA techniques.
Q2: Are stochastic-based IA approaches more accurate than
prevailinghistory-basedIA techniques,which do nottake
different types of evolution into consideration? As we
mentioned before, the prevailing evolutionary coupling
deﬁnition is a special case of our stochastic dependency
family.
Q3: How does the selection of kaffect the accuracyof impact
analysis? We investigate the hypothesis that the IA will
achieve peak performance with a particular choice of k
and that the length history can impact IA performance.296TABLE I
EXAMPLE LAMBDASEQUENCES
Constant Linear Sinusoidal Exponential
1
k2(k−i+ 1)
k2+kcos/parenleftbig
πk−1(i−1)/parenrightbig
+ 1
k+ 12−i+2
k2k+1
Q4: How does the selection of a λsequence affect the ac-
curacy of impact analysis? We investigate the hypothesis
that, in general, the usefulness of a transaction decreases
over time. We consider that the hypothesis is true if we
observe increasing IA performance with a decreasing λ.
We also hypothesize that the best λfunction for each
subject system will be different because, as independent
projects, their evolution paths should be different.
In this section, we ﬁrst describe the ﬁve software systems to
whichwe applyourapproach.Thenwedescribetheevaluation
procedure and present the results.
A. Subjects
Table II shows some basic information of the following
ﬁve open source systems that we chose as the subjects of
our evaluation. These projects were chosen because they hav e
different sizes and are from different domains.
Log4J:4a popularlogging framework for the Java program-
ming language.
Hadoop Common:5a Java-based distributed computing
framework;HadoopCommonprovidesthe shared components
and functionality used by other Hadoop sub-projects.
JEdit:6a text editor that supports syntax highlighting of
source code and the ability to write macros.
Apache Ant:7an automated software build tool for Java
applications.
JBoss:8oneofthemostwidelyusedJavaapplicationservers
in the world. It providesa platform for runningenterprise J ava
applications based on the Java EE standards.
B. Evaluation Procedure
For each subject system, we extract transaction informatio n
from its Subversion (SVN) repository. For each transaction ,
we randomly select a ﬁle that is involved in the transaction a s
4http://logging.apache.org/log4j/
5http://hadoop.apache.org/common/
6http://www.jedit.org/
7http://ant.apache.org/
8http://www.jboss.org/TABLE II
SUBJECTSYSTEMINFORMATION
Subject # Vers History KSLOC # Trans
Log4J 11 12/00–6/07 15 3550
Hadoop Common 20 2/06–12/09 36 9319
JEdit 12 9/01–2/10 98 17339
Apache Ant 20 1/00–8/10 125 14554
JBoss 15 10/99–8/10 534 107501
the starting impact set, and performed impact analysis on it .
Assuming each transaction is a single change task, an ideal I A
approachwouldbeabletoidentifyallﬁlesinthetransactio nas
being in the impact scope. We perform IA on each transaction
up to ﬁve times (fewer if the transaction has fewer than ﬁve
ﬁles), with a different random starting impact ﬁle each time .
Thesame startingimpact ﬁles foreach transactionare used f or
all the approaches for unbiased comparison. Consistent wit h
theworkofZimmermannet al.[2]we ignoretransactionswith
morethan30ﬁles astheyare unlikelyto be meaningful.These
large blanket operations on the code base are often changes t o
licensing information that need to be updated in the comment
section of all ﬁles; or other trivial housekeeping activiti es on
the code base.
We usethe standardinformationretrievalmeasuresof preci-
sion,recall, and F1for assessing the accuracyof IA. Precision
measures how much of the predicted impact scope is correct,
while recall measures how much of the actual impact scope
was predicted. The F1measure/score combines precision and
recall into a single number for ease of comparison. Next
we introduce the existing IA techniques against which we
compare our stochastic-based IA technique.
precision =#correct
#predicted
recall =#correct
transaction size
F1=2×precision ×recall
precision +recall2971) Structure-based Impact Analysis: To answer the ﬁrst
evaluation question, we compare our stochastic IA accuracy
with that of two traditional structure-basedIA techniques . The
ﬁrst is from the work of Briand et al. [25]. They proposed
a structural coupling measure that combines whether classe s
fromdifferentinheritancehierarchiesinteract(CBO’),w hether
aclass(directlyorindirectly)aggregatesanotherclass( INAG),
and the number of method calls between classes (PIM). To
ease analysis, we normalize their coupling measure into the
range of [0,1]by dividingby the largest measure value. In this
section, we refer to this technique as the staticdependency
approach. The other structure-based approach we compare
against is the work of Robillard [10]. Robillard deﬁnes an
algorithm that, given a starting impact set, assigns a weigh t
in the range [0,1]to other software elements based on ana-
lyzing the structure/topology of the dependency graph. In t his
section, we refer to this technique as the topology dependency
approach.
To perform IA using these measures, we ﬁrst compute the
dependency values of the ﬁles based on the software structur e
in the most recent release. Given a minimum threshold value,
we select all ﬁles whose dependency value is at least the
threshold value to be included in the impact scope. Prior to
performing IA on a given software release, we ﬁrst identify
the minimum threshold value that maximizes the F1measure.
Essentially, we compare against the best accuracy of these
structure-based IA.
2) History-based Impact Analysis: To answer the sec-
ond evaluation question, we compare the IA accuracy using
stochastic dependency against traditional evolutionary d epen-
dency [2], [3]. We perform traditional history-based IA on a
transaction by analyzing all transactions from the beginni ng
of the revision history to the most recent release (in order t o
be fair to the structural dependency experiments). Similar ly to
the structural dependency experiments, we determine the be st
minimum support and conﬁdence values that maximize F1
prior to processing the transactions for each software rele ase.
3) Stochastic-based Impact Analysis: We use the four λ
sequences presented in Table I as the smoothing function
of each stochastic dependency family member. For each λ
sequence, we use the values of 5, 10, 20, and 40 for k
(maximum number of transactions to consider). As a result,
we consider 16 members of the stochastic dependency family
in total. Like with evolutionary dependencies, we consider the
transaction for the most recent software release to be the la test
transaction available for analysis. Although various roun ding
techniques are possible for our stochastic dependency valu e
to determine whether a ﬁle is in the impact scope, we follow
the same strategy as with the structural dependencies—give n
a minimum threshold, a ﬁle is considered in the impact
scope if its stochastic dependency value is at least as large
as the threshold. Also similar to the other two types of IA
approaches, we ﬁnd the best minimum threshold for each
software release.C. Results
Table III shows the results from our evaluation. For each
subject system and IA technique, we show the average F1
score in the column labeled ¯F1. The average F1score is
computed over the IA predictions for all transactions. For
example, with Apache Ant, we ran a total of 4306 impact
analyses on 1313 transactions with each dependency type. We
compute the average F1score for each dependency type, by
adding the F1score for each of the 4306 IA analysis results
and dividing the total by 4306. Due to space constraints, we
onlyshow the F1valuesforthe stochastic dependencieswhere
k= 10.F1values for other kvalues are discussed below.
a) Q1. Comparing with Structure-based IA: Table III
shows that the accuracy, by F1measure, of both structure-
based IA for all the subjects are lower than any member
of the stochastic-based IA approaches, and are also lower
than traditional history-based impact analysis. Take Hado op
Common for example, the topology approach achieved an
average F1score of 0.5163 and the static approach achieved
an average F1score of 0.5174. In contrast, all the stochastic
dependencies had average F1scores of above 0.55.
To conﬁrm our intuition based on the average F1measures,
we apply the Wilcoxon signed rank test [26] to compare each
of the stochastic dependency types against static dependen cy
types.Thenullhypothesis H0foreachstatistical testisthatthe
F1value (per impact analysis on each transaction) achieved
by the existing approach is the same as the F1value achieved
by the stochastic dependency.The alternativehypothesis H1is
that the stochastic dependency achieved greater F1value than
the existing approach. The resulting W-scores and p-values
from performing the statistical tests, with a 95% conﬁdence
interval, are also shown in Table III.
The results of the Wilcoxon signed rank test corroborate
our intuition that the stochastic dependencies outperform ed
the structure-based IA approaches. All the p-values from the
statistic testswere <2.2×10−16(thesmallest p-valuepossible
in our statistics software: R9). These p-values, which are
statistically quite signiﬁcant, indicate that the null hyp othesis
should be rejected and the alternative hypothesisshould be ac-
cepted(i.e.,the stochastic dependenciesprovidemoreacc urate
predictions than the static dependencies). From these resu lts
we can afﬁrmatively answer the ﬁrst evaluation question tha t
stochastic dependencies are more accurate than traditiona l
structural dependencies at IA.
b) Q2. Comparing with Traditional History-based IA:
Lookingat theresultsfromTable III,we see thatforanyofth e
ﬁvesubjectsystem,threeoutofthe fourtypesofthestochas tic
dependencies yield a higher average F1score than traditional
evolutionary dependencies, and that the differences are st a-
tistically signiﬁcant. Take Log4J as an example: we observe
that the average F1for the constant stochastic dependency is
0.3492,which is lower than that of evolutionarydependenci es,
0.3641. The statistical test results ( W-score of 38457.5 and p-
value of 0.9989) conﬁrm that the stochastic dependency does
9http://www.r-project.org/298TABLE III
WILCOXON SIGNEDRANKTESTRESULTS FOR F1SCORES(k= 10)
SubjectStochastic Evolutionary Topology Static
λ ¯F1¯F1 W p ¯F1 W p ¯F1 W p
Log4JConst 0.3492
0.364138457.5 0.9989
0.308266804.5 <2.2e-16
0.299277051 <2.2e-16
Line 0.3913 62979.5 7.133e-6 95164.5 <2.2e-16 131060.5 <2.2e-16
Sine0.3974 82792.5 1.57e-7 119081 <2.2e-16 158798.5 <2.2e-16
Expon 0.3935 94266.5 1.552e-4 90482 <2.2e-16 116364 <2.2e-16
HadoopConst 0.5534
0.5658795625.5 1.0
0.5163809766 <2.2e-16
0.5174880037.5 <2.2e-16
Line 0.5761 1094091 2.654e-5 1488936 <2.2e-16 1584656 <2.2e-16
Sine0.5780 1131795 8.406e-7 1540090 <2.2e-16 1648998 <2.2e-16
Expon 0.6034 1988273 <2.2e-16 2075044 <2.2e-16 2217333 <2.2e-16
JEditConst 0.3950
0.379614149902 5.043e-6
0.35254962412 <2.2e-16
0.35254962412 <2.2e-16
Line 0.3993 14404088 1.054e-6 3555722 <2.2e-16 3555722 <2.2e-16
Sine0.3951 14207902 0.001198 2117458 <2.2e-16 2117458 <2.2e-16
Expon 0.3943 14337290 0.2036 1445818 <2.2e-16 1445818 <2.2e-16
AntConst 0.4203
0.4265295247 0.9938
0.3733625191.5 <2.2e-16
0.3701606184.5 <2.2e-16
Line 0.4690 1209382 <2.2e-16 1988838 <2.2e-16 1988610 <2.2e-16
Sine0.4691 1194846 <2.2e-16 1914936 <2.2e-16 1908747 <2.2e-16
Expon 0.4662 1071560 <2.2e-16 1216914 <2.2e-16 1179197 <2.2e-16
JBossConst 0.6335
0.65001030575 1.0
0.6145928520.5 <2.2e-16
0.6145928520.5 <2.2e-16
Line 0.6574 2183468 4.339e-4 3208790 <2.2e-16 3208790 <2.2e-16
Sine0.6576 2167306 2.895e-4 3045315 <2.2e-16 3045315 <2.2e-16
Expon 0.6751 3027636 <2.2e-16 2444432 <2.2e-16 2444432 <2.2e-16
not outperformthe evolutionary dependenciesin this scena rio.
We highlight the cells of Table III where the statistical
test results indicate that our stochastic dependencies did not
outperform the existing dependency type.
Although the stochastic dependenciesdo not always outper-
form the evolutionary dependencies, we afﬁrmatively answe r
our second evaluation question because, for each subject
system, most stochastic dependency types can provide more
accurate IA results than traditional evolutionary depende ncies.
Due to the differencein the evolutionpathsof subjectsyste ms,
it is not surprising that some stochastic dependency types c an
provide more accurate IA results. Below, we further explore
how the parameters that deﬁne stochastic dependencies( kand
λ) inﬂuence analysis accuracy.
c) Q3. Effects of k:In determining the effects of k
(length of history) on the accuracy of analysis, we again app ly
the Wilcoxon signed rank test. Given a ﬁxed λsmoothing
function, we vary the value of kand compare the F1values.
Table IV shows the average F1scores for Hadoop Common
as a representative example.
TABLE IV
AVERAGE F1SCORES FOR /an}bracketle{tλ, k/an}bracketri}htPAIRS:HADOOPCOMMON
HHHHλk5 10 20 40
Const 0.5679 0.5534 0.5415 0.5289
Line 0.5919 0.5761 0.5597 0.5429
Sine 0.5922 0.5780 0.5608 0.5443
Expon 0.6005 0.6034 0.6032 0.60302From Table IV we see that increasing the value of k
generally decreases the accuracy of analysis (except in the
case of the exponential smoothing function where increasin g
kfrom 5 to 10 improves the accuracy). To corroborate these
observations, we apply the Wilcoxon signed rank test with a
nullhypothesisthattwodifferent kvaluesareequallyaccurate.
The alternative hypothesis is that one of the kvalues is more
accurate than the other. For example, comparing k= 5and
k= 10for the linear smoothing function of Hadoop, we
obtain a W-score of 863620.5 and p-value of 1.749×10−9,
which conﬁrms that k= 5is statistically more accurate than
k= 10. Comparing k= 10andk= 20for the same
smoothing function yields a W-score of 687905 and a p-value
of9.938×10−14, which indicatesthat k= 10ismoreaccurate
thank= 20.
We applied this statistical test to all pairs of kvalues in
each subject system and found consistently that changing
thekvalue does affect the accuracy of analysis. Due to
space restrictions, we do not elaborate on the statistical t est
results for the other subject systems. These results allow u s
to afﬁrmatively answer our third evaluation question—that the
value of kdoes inﬂuence the accuracy of analysis, and that
both too long and too short of a history will negatively impac t
the IA results.
d) Q4. Effects of λ:To evaluate the effects of different
λsequences, we compare the accuracy of applying the four
λsequences for each given subject system. For example,
Table V shows the average F1valuesfor JBoss underdifferent
combination of λandk. For example, when k= 10, the
constant smoothing function yields lower accuracy than the299other functions, and the exponentialsmoothingfunction yi elds
highest accuracy. To conduct the Wilcoxon signed rank test,
we deﬁne null and alternative hypotheses to compare differe nt
λpairs under the same k.
As an example, we deﬁne a null hypothesis as the linear
and constant smoothing functions are equally accurate and t he
alternative hypothesis as the linear function is more accur ate.
For JBoss with k= 10, we obtain a W-score of 2062498
and a p-value of <2.2×10−16. This statistically signiﬁcant
resultconﬁrmsthatthelinear λperformsbetterthanconstant λ
function.Comparingthe exponentialandsinusoidalsmooth ing
functions, we obtain a W-score of 2159677 and a p-value
of1.27×10−12, conﬁrming that the exponential function is
more accurate than the sinusoidal function. On the other han d,
comparing the linear and sinusoidal smoothing functionsyi eld
aW-score of 74374.5 and a p-value of 0.8096, indicating
no signiﬁcant difference in accuracy. These results sugges t
that JBoss follows an evolution path with high refactoring
activity, since the historical data quickly becomes inaccu rate
in performing IA.
TABLE V
AVERAGE F1SCORES FOR /an}bracketle{tλ, k/an}bracketri}htPAIRS:JBOSS
HHHHλk5 10 20 40
Const 0.6508 0.6335 0.6154 0.6107
Line 0.6694 0.6574 0.6367 0.6193
Sine 0.6696 0.6576 0.6366 0.6195
Expon 0.6754 0.6751 0.6752 0.6752
We obtain similar results with other subjects: different
λprovides different IA accuracy in different projects: the
exponential smoothing function is best for Hadoop and JBoss ;
with Log4J, sinusoidal smoothing slightly outperformsoth ers;
forApache Ant, the linearand sinusoidalfunctionsare equa lly
best; for JEdit, the linear and constant functions are equal ly
best. So we can positively answer the last evaluation questi on:
in most cases, a decreasing λsequence increases accuracy and
the best λfor a project differs due to different evolution paths
where the speeds at which historical data becomes irrelevan t
can vary.
D. Assessing Precision and Recall
While the F1score provides a simple, single quantity
measure, the respective importance of precision and recall
measures varies with applications. For the purpose of impac t
analysis, the recall measure is considered more important t han
precision because developers do not want to miss important
ﬁles that need to be changed. In this subsection, we provide
further elaboration on the precision and recall values, usi ng
JBoss, the largest subject systems as a representative exam ple.
In Table VI, we present the quartile values of precision and
recall scores of each type of IA analysis on JBoss.
Table VI shows that the static and topology dependen-
cies have very high precision but low recall compared to
stochastic dependencies. The high precision is not surpris ingbecause these techniques identify components that are high ly
structurally coupled and hence they are most likely to be
impacted by changes. The low recall is also not surprising:
many semantically coupled componentsdo not have structura l
dependencies. By leveraging historical data, the evolutio nary
and stochastic dependencies can detect semantic coupling a nd
improve the recall scores, but at the cost of slightly decrea sed
precision values. The large increase in recall compensates
for the small lost in precision, and improves the overall F1
accuracy.
Similarly, Table VI shows that the exponential smoothing
functionachievesthesameprecisionquartilevaluesasthe evo-
lutionary dependencies, even though the average (arithmet ic
mean) precision is slightly lower (not shown). The table als o
shows that comparing with traditional evolutionary depend en-
cies, exponential stochastic dependency achieves signiﬁc ant
improvement in the recall: in the ﬁrst quartile (Q1), the rec all
increases from 0.25 to 0.5 and the median (Q2) recall increas e
from 0.6 to 1. This means that more than half the impact
analysesconductedbythe exponentialstochasticdependen cies
had perfect recall.
TABLE VI
QUARTILE VALUES OF IA ACCURACY ON JBOSS(k= 10)
Approach Measure Min Q1 Q2 Q3 Max
StaticPrecision 01111
Recall 00.20.511
TopologyPrecision 01111
Recall 00.20.511
EvolutionaryPrecision 01111
Recall 00.250.611
Const StochasticPrecision 01111
Recall 00.250.511
Line StochasticPrecision 00.5111
Recall 00.33111
Sine StochasticPrecision 00.6111
Recall 00.33111
Expon StochasticPrecision 01111
Recall 00.5111
We make similar observations from the other subject sys-
tems: comparing with traditional evolutionary dependenci es,
our stochastic dependencies consistently have slightly lo wer
precision values, but have signiﬁcantly better recall valu es,
and thus signiﬁcantly higher F1values. Table III shows that
theF1values for the other subject systems are lower than
those for JBoss, but this precision/recall tradeoff occurs with
all the subjects.
VI. DISCUSSION
In this section, we discuss the evaluation results, threats to
validity, and possible future work.
Although stochastic dependency is a generalization of evo-
lutionary dependency, not all stochastic dependencies wit h
constant λsequences are evolutionary dependencies due to
thekparameter. In order to identify the same impact scope
as evolutionary dependencies, we need to have a constant300λsequence andkmust be the number of allthe transac-
tions involving the starting impact set. That is why, in our
evaluation, the stochastic dependency types with a constan t
λsequence did not perform the same as traditional history-
basedIA. To achievethesame resultsas existinghistory-ba sed
IA, we would have needed to vary the value of kfor each
transaction.
The low IA accuracy for some of the evaluation subject
systems may seem alarming at ﬁrst (the average F1measure
dips down to1/3for some systems). However, the accuracy in
actual usage may be signiﬁcantly higher. This low accuracy i n
the evaluation is due to two factors: the directional nature of
impact analysis and the selection of a random starting impac t
set. For example, consider a transaction that contains a bas e
classAand its two subclasses BandC, and assume that it is
only possible for changesto Ato affectBandC, but changes to
BorCneveraffect A. Sincewedonotknowthat Ais thecorrect
starting impact set, our evaluationapproachconsiderseac h ﬁle
to be a starting impact set. Hence, even a perfect IA approach
would only achieve a 0.33 average F1score ( F1= 1withAas
starting impact set, F1= 0withBorCas starting impact set).
Identifying the actual starting impact set of a transaction or
modiﬁcationrequestisanactiveareaofresearch(e.g.Anto niol
et al. [27]) that is orthogonal to our stochastic approach an d
can be used to improve IA accuracy in the future.
In our evaluation, we found the optimal minimum threshold
values to compare each IA approach at its best accuracy.
However, in practice, the best value is not known a priori.
One strategy to address this problem is to use the optimal
threshold from the previous release’s transactions. After each
software release, the minimum threshold can be recomputed
and used for estimating during the next release.
e) Threats to Validity: Since we only applied our ap-
proach to ﬁve Java-based, object-oriented systems, we cann ot
conclude that the effectiveness of stochastic dependencie s
generalizes to all software systems; however, we did choose
projects of various sizes and domains to begin addressing th is
issue. Similarly, we only applied the stochastic model to IA ,
but not other analysis techniques where historical data can
be leveraged, such as bug triage prediction. Thus we cannot
claim that the same improvement can be generalized to other
history-based analysis techniques.
As with any technique that derives dependencies from
transactions in revision history, our approach assumes tha t the
transactionsrepresent cohesivework units. In other words , if a
developer only commits to the revision control system a batc h
of ﬁles once in a while, the ﬁles committed together may have
no semantic relationship and are only coincidentally occur ring
in the same transaction. The accuracy of IA produced by
stochastic dependencies and evolutionary dependencies in -
dicate that transactions often are cohesive work units and
generally do indicate semantic coupling of software elemen ts.
Our framework assumes that the transactions of a revision
control history form a stochastic process where the depen-
dencies between software elements control the probability
distributionoftherandomvariablesinthisprocess.Weuse thisassumption as the basis for building the Markov chain model.
Inreality,theprobabilitydistributionoftheserandomva riables
(software elements that occur together in a transaction) ca n
also be inﬂuenced by external factors and our assumption may
not always be true. However, our evaluation shows that mod-
eling the revision history as a stochastic process is a sufﬁc ient
approximation for the actual behavior of development to yie ld
more accurate IA results.
f) Future Work: The parameters of stochastic depen-
dency allow for a large number of speciﬁc dependency types
to be deﬁned. In the study reported in this paper, we only
used several kandλvalues. Identifying additional, successful
λsequences is an ongoing work. For example, a possible
λsequence could be based on the how long ago (i.e., in
terms of days, months, etc.) a transaction was committed. Th e
bestkvalue for different systems may also vary. Exploring
techniques to automatically construct optimal λandkvalues
for a given software system is also a possible future work.
Exploring methods to improve the accuracy of stochastic-
based software analysis in general is an ongoing work. In
particular, we are exploring the use of more complex proba-
bilisticmodels(e.g.hiddenMarkovmodels,dynamicBayesi an
networks [20]) in place of the Markov chain model. Incorpo-
rating structural dependency information into these model s to
improve IA accuracy is also of interest.
VII. C ONCLUSION
Using history-based evolutionary coupling to conduct var-
ious software analyses in software maintenance has gained
popularity recently. However, differences in the speed of
evolution of different software systems affect the validit y of
historical data used for analysis, and thereby the accuracy
of the analysis results. Our stochastic dependency framewo rk
generalizes evolutionary coupling, using a length of histo ryk
and a smoothing function λparameter to account for different
typesofevolutionpaths.By varyingthese two parameters,o ne
can select the combination that best ﬁts the system. Our ex-
perimentshowsthat performingIA using stochastic depende n-
cies consistently outperforms structure-based IA techniq ues.
Comparing with prevailing history-based IA techniques, ou r
stochastic-based approach can provide better IA accuracy— in
particular,signiﬁcant improvementin recall for all ﬁve su bject
systems.
ACKNOWLEDGMENTS
This work was supported in part by the National Science
Foundation under grants CCF-0916891 and DUE-0837665.
REFERENCES
[1] H. Gall, K. Hajek, and M. Jazayeri, “Detection of logical coupling based
on product release history,” in Proc.14th IEEEInternational Conference
on Software Maintenance , Nov. 1998, pp. 190–197.
[2] T.Zimmermann, P.Weißgerber, S. Diehl, and A. Zeller, “M ining version
histories to guide software changes,” IEEE Transactions on Software
Engineering , vol. 31, no. 6, pp. 429–445, Jun. 2005.
[3] A. T. T. Ying, G. C. Murphy, R. Ng, and M. C. Chu-Carroll, “P redicting
source code changes by mining change history,” IEEE Transactions on
Software Engineering , vol. 30, no. 9, pp. 574–586, Sep. 2004.301[4] M. Cataldo, A. Mockus, J. A. Roberts, and J. D. Herbsleb, “ Software
dependencies, work dependencies, and their impact on failu res,”IEEE
Transactions on Software Engineering , vol. 35, no. 6, pp. 864–878, Jul.
2009.
[5] S.A.Bohner andR.S.Arnold, SoftwareChange Impact Analysis . IEEE
Computer Society, 1996.
[6] W.-K. Ching and M. K. Ng, Markov Chains: Models, Algorithms, and
Applications , 2nd ed. Springer, 1996.
[7] L. C. Briand, Y. Labiche, L. O’Sullivan, and M. M. S´ owka, “Automated
impact analysis of UML models,” Journal of Systems and Software ,
vol. 79, no. 3, pp. 339–352, Mar. 2006.
[8] M. O. Elish and D. C. Rine, “Investigation of metrics for o bject-oriented
design logical stability,” in Proc. 7th European Conference on Software
Maintenance and Reengineering , Mar. 2003, pp. 193–200.
[9] S. R. Chidamber and C. F. Kemerer, “A metrics suite for obj ect oriented
design,”IEEE Transactions on Software Engineering , vol. 20, no. 6, pp.
476–493, Jun. 1994.
[10] M. P. Robillard, “Topology analysis of software depend encies,”ACM
Transactions on Software Engineering and Methodology , vol. 17, no. 4,
pp. 18:1–18:36, Aug. 2008.
[11] V. Rajlich, “A model for change propagation based on gra ph rewriting,”
inProc. 13th IEEE International Conference on Software Maint enance,
Oct. 1997, pp. 84–91.
[12] T. Apiwattanapong, A. Orso, and M. J. Harrold, “Efﬁcien t and precise
dynamic impact analysis using execute-after sequences,” i nProc. 27th
International Conference on Software Engineering , May 2005, pp. 432–
441.
[13] S. Bouktif, Y.-G. Gu´ eh´ eneuc, and G. Antoniol, “Extra cting change-
patterns from CVS repositories,” in Proc. 13th Working Conference on
Reverse Engineering , Oct. 2006, pp. 221–230.
[14] M. Ceccarelli, L. Cerulo, G. Canfora, and M. D. Penta, “A n eclectic
approach for change impact analysis,” in Proc. 32nd International
Conference on Software Engineering , May 2010, pp. 163–166.
[15] M. D’Ambros, H. C. Gall, M. Lanza, and M. Pinzger, “Analy zing
software repositories to understand software evolution,” inSoftware
Evolution . Springer, 2008.
[16] M. Cataldo, “Dependencies in geographically distribu ted software de-
velopment: Overcoming the limits of modularity,” Ph.D. dis sertation,
Carnegie Mellon University, 2007.[17] N. Tsantalis, A. Chatzigeorgiou, and G. Stephanide, “P redicting the
probability of change in object-oriented systems,” IEEE Transactions
on Software Engineering , vol. 31, no. 7, pp. 601–614, Jul. 2005.
[18] M. K. Abdi, H. Lounis, and H. A. Sahraoui, “Predicting ma intainabil-
ity expressed as change impact: A machine-learning-based a pproach,”
inProc. 21st International Conference on Software Engineeri ng and
Knowledge Engineering , Jul. 2009, pp. 122–128.
[19] S. Mirarab, A. Hassouna, and L. Tahvildari, “Using baye sian belief
networks to predict change propagation in software systems ,” inProc.
15th IEEE International Conference on Program Comprehensi on, Jun.
2007, pp. 177–188.
[20] S. Russell and P. Norvig, Artiﬁcial Intelligence: A Modern Approach ,
2nd ed. Prentice Hall, 2003.
[21] J. A. Whittaker and M. G. Thomason, “A markov chain model for
statistical software testing,” IEEETransactions onSoftware Engineering ,
vol. 20, no. 10, pp. 812–824, Oct. 1994.
[22] J. F. Bowring, J. M. Rehg, and M. J. Harrold, “Active lear ning for
automatic classiﬁcation of software behavior,” in Proc. ACM SIGSOFT
International Symposium on Software Testing and Analysis , Jul. 2004,
pp. 195–205.
[23] L. Cheung, R. Roshandel, N. Medvidovic, and L. Golubchi k, “Early
prediction of software component reliability,” in Proc. 30th International
Conference on Software Engineering , May 2008, pp. 111–120.
[24] P. Raghavan and C. D. Tompson, “Randomized rounding: A t echnique
for provably good algorithms and algorithmic proofs,” Combinatorica ,
vol. 7, no. 4, pp. 365–374, Dec. 1987.
[25] L. C. Briand, J. W¨ ust, and H. Lounis, “Using coupling me asurement
for impact analysis on object-oriented systems,” in Proc. 15th IEEE
International Conference onSoftwareMaintenance , Aug.1999,pp.475–
482.
[26] F. Wilcoxon, “Individual comparisons by ranking metho ds,”Biometrics
Bulletin, vol. 1, no. 6, pp. 80–83, Dec. 1945.
[27] G. Antoniol, G. Canfora, G. Casazza, and A. D. Lucia, “Id entifying the
starting impact set of a maintenance request,” in Proc. 4th European
Conference on Software Maintenance and Reengineering , Mar. 2000,
pp. 227–230.302