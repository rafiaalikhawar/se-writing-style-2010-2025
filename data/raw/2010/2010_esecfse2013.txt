Code Fragment Summarization
Annie T.T. Ying and Martin P . Robillard
School of Computer Science
McGill University
{annie.ying,martin}@cs.mcgill.ca
ABSTRACT
Currentresearchinsoftwareengineeringhasmostlyfocused
on the retrieval accuracy aspect but little on the presenta-
tion aspect of code examples, e.g., how code examples are
presented in a result page. We investigate the feasibility
ofsummarizingcodeexamplesforbetterpresentingacode
example. Our algorithm based on machine learning could
approximatesummariesinanoraclemanuallygeneratedby
humans with a precision of 0.71. This result is promising
assummarieswiththislevelofprecisionachievedthesame
levelofagreementashumanannotatorswitheachother.
CategoriesandSubjectDescriptors
D.2.8[Software Engineering]: Distribution,Maintenance,
andEnhancement
GeneralTerms
Experimentation
Keywords
MachineLearning,summarization,sourcecodeanalysis
1. INTRODUCTION
The Web is an important resource for a programmer: as
much as 20% of a programmer’s time could be spent on
theWeb [2]. When a programmersearchesforinformation
relatedtoanApplicationProgrammingInterfaces(API),of
thevarioustypesofdocumentationtheprogrammerﬁndson
the Web, code examples are one of the most eﬀective [14],
important [16], and frequently sought-after [17]. Because
searching for code examples is often an indispensable part
of programming, researchers have built search engines and
recommendationsystemsforretrievingrelevantcodeexam-
ples[21,4,1]. Mostofthesesystemsfocusonimprovingthe
accuracyofcodeexampleretrieval.
However, even when a relevant example is returned by a
general or code-speciﬁc search engine, the presentation of
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bearthisnoticeandthefullcitation ontheﬁrst page. Tocopyotherwise, to
republish,topostonserversortoredistributetolists,requirespriorspeciﬁc
permission and/ora fee.
ESEC/FSE’13,August 18-26,2013,Saint Petersburg, Russia
Copyright2013ACM 978-1-4503-2237-9/13/08...$15.00.
Figure 1: A result for “apr ﬁleread” from Google
code examples can hinder the use of the example. Even a
state-of-the-artsearchengineforﬁndingcodeexamplesdoes
not provide adequate cues for a programmer to eﬀectively
evaluate whether the link is worth-while to follow. Often,
the textual snippet accompanying a returned link contains
no code, only a textual summary of the Web page being
linked. When a summary does contain code, the summary
isextractedasifthecodeweretext(Figure1). Theselimita-
tionsoftenfoilaprogrammer’sattempttoevaluatewhether
a search hit is worth pursuing, making it necessary for a
programmertoopenandscanmanyoftheresultpages[20].
Yet, in the context of general search engines, textual snip-
pets form a signiﬁcant part (40% of the time on a search
resultpage[7])ofevaluatingwhetheraparticularreturned
linkisworthnavigatingto.
Weproposetosummarize codeexamples,moreformally,
code fragmentswhich we deﬁne as partial programs that
serve the purpose of demonstrating the usage of an API.
Acode fragment summaryis a shorter code fragment than
the original one, where any line in the summary is more
informative (in the context of a speciﬁc query) than any
otherlinenotinthesummary.
In this paper, we present a feasibility study on one way
of generating code fragment summaries: a supervised ma-
chine learning approach that classiﬁes whether a line in a
codefragmentshouldbeinasummary. Asaninitialinves-
tigation, we exploited two types of features:syntacticfea-
turesofthesourcecode,andwhetheralineisrelatedtothe
givenquery. For training and evaluating our classiﬁer, we
collectedacorpusofcodefragments andconstructedacor-
responding summary oracle using four human annotators.
Thecorpusconsistsof70codefragmentsdirectlyextracted
from code examples illustrating the answers to the Eclipse
oﬃcialFAQ.1Wedeﬁnedthequery,whichisthefocusofthe
summary, as the FAQ question. The summary oracle con-
sistsofsummariesmanuallycreatedbyannotatorsthrough
selectinglinesdeemedimportantforasummary,totalingto
3560judgments. Figure2illustratesonecodefragmentfrom
ourcorpus, fortheFAQ“HowdoIdistinguishbetweenin-
ternalandexternalJARsonthebuildpath?”Thesummary
wegeneratedismarkedasbold.
1http://wiki.eclipse.org/index.php/Eclipse FAQs, accessed
onMay29,20131:IClasspathEntry entry = ...
2:IPath path = entry.getPath();
3:IWorkspace workspace=ResourcesPlugin.getWorkspace();
4:IResourcejarFile=workspace.getRoot().ﬁndMember(path);
5:if (jarFile != null){
6: return jarFile.getLocation();
7:}else{
8: // must be an external JAR (or invalid classpath entry)
9:}
Figure 2: A code fragment summary (in bold)
We found promising evidence in the feasibility of gener-
ating code fragment summaries using a machine learning
approachwithlight-weightfeatures,withaprecisionof0.71
whenweallowedsummariestobeofthesamelengthasthe
oracle. This level of precision were as similar to human-
generated summaries as summaries generated by diﬀerent
humanstoeachother. Oursyntacticandquery-relatedfea-
turesarefasttogenerate(0.09spercodefragment),making
itpossibletodeployinarealsearchengineandothersoft-
wareengineeringsettings. Thissimplealgorithmwillserve
as a baseline, as we develop more sophisticated code frag-
mentsummarizationapproaches.
2. RELATEDWORK
The closest work to ours is a system by Kim et al [11]
thataugmentsAPIdocumentationwithcodeexamplesum-
maries. Thecomponentoftheirsystemresponsibleforsum-
marizationresemblesourtool,alsousingsyntacticandquery
features, though without the use of machine learning. Our
summarizerdiﬀersinthepurpose: theirapproachismeant
forsummarizingmultipleresultsfromacodesearchengine,
suchasKoders,whereasouralgorithmworksonsummariz-
inganysinglecodefragments.
Mica[20]andAssieme[9]areexamplesofthelineofwork
aimingataugmentingsearchengines. Micaaimsathelping
aprogrammerevaluatewhetherasearchhitfromageneral
search engine contains the relevant API elements, and for
Assieme, relevantcodeexamples. Micaworksbyaugment-
ing the search result page with API methods, classes and
ﬁeld names that are contained in Web pages linked by the
searchhits. Assiemeextractsthecodeexamplesfromthere-
turnedWebpagesastheresultanddisplaysvariousstatistics
andlinkstotheAPIelements. Assiemeattemptstobetter
adapt a general search engine to code search by expanding
aprogrammer’squeryonageneralsearchenginewithpro-
gramming language keywords. Our system diﬀers in that
insteadofsimplydisplayingalltheAPIelements(inMica’s
case)orthecodefragments(inAssieme’scase)containedin
a Web page linked by the search hits, wesummarizecode
fragmentscontainedintheWebpagetoimproveontheof-
teninadequatetextualsnippetsprovidedbygeneralsearch
engines.
Researchershaveproposedmanycodeexamplesearchen-
gines (e.g., Strathcona [10], SNIFF [5], Sourcerer [1], and
arecentsystembyBuseandWeimer[4])andcodecomple-
tiontools(e.g.,theIntelligentCodeCompletionsystem[3]).
Mostofthesesystemsattempttosynthesizecodeexamples
orﬁndtherelevantAPIsfromcoderepositoriesthatmatcha
programmer’squery,whetherthequeryisexplicitlyformed
orimplicitlyinferredfromthecontext. However,thesesys-
tems do not help a programmer searching on the Web for
codeexamplesandwishingtousethecontextaccompanyingthedesiredcodefragmentinaWebpage.
Several eﬀorts have investigated reduced representations
ofsoftwareartifacts,includingsummarizingbugreports[15,
12] and producinga succinctset of textual keywords [8] or
textualsummary[18,19]giventhesourcecodeofamethod.
3. CODEFRAGMENTSUMMARIZER
Our classiﬁer uses two types of features: whether a line
contains certain syntactic constructs and whether a line is
related to a query. We set aside 17 code fragments and
summariesforthethedevelopmentoffeatures(development
set)andtherest,53,forevaluation.
3.1 SyntacticFeatures
Weobservedfromthesummariesinthedevelopmentset
that when a line contains a certain type of syntactic con-
structs, the line is more likely to be in a summary. For
example, a line containing an anonymous class declaration
andinstantiationtendedtobeinasummaryintheoracle,
whereasalinecontaininganifconditionaltendednottobe
inasummary.
In total, after experimenting with the development set,
we employed 49 syntactic features. These features include
whether a line in a code fragment contains a certain part
of a type signature (e.g.,typeIsPublic), contains an anony-
mous declaration and instantiation, contains an exception
handling or conditional keyword, is in a block, contains a
variable or ﬁeld declaration, contains a part of a method
signature,containsamethodinvocation,containsamethod
exist statement, contains a comment, contains an assign-
ment, contains a call declared in the code, and contains a
callintheJavaSDK.
Weextractedthesesyntacticfeaturesofacodefragment
from its abstract syntax tree (AST). All syntactic features
are discrete variables, with a binary value depending on
whetheralinecontainsafeature.
3.2 Query-relatedFeatures
We also observed that annotators are more likely to in-
clude in the summary the lines containing the terms from
the query, which is the question in the FAQ. Analogous to
the syntactic features, query related features are discrete
variables, with a binary value dependingon whether a line
contains a feature. We constructed three features that in-
dicate whether an identiﬁer in a code fragment contains a
querytermornot. Weconstructedtwoadditionalfeatures
thatlookedbeyondjustindividuallines:mostTermsistrue
when a line contains the most number of matching terms
amongallthelinesinthesamecodefragment,andmostDi-
verseTermsis analogousexcept it indicates the most num-
berofdistinctterms. Whencomputingthefeatures,wesplit
theidentiﬁersaccordingtothecommoncamelcaseidentiﬁer
namingconvention.
3.3 Classiﬁer,trainingandevaluationdata
WeexperimentedwithtwoseparateclassiﬁersusingNaive
Bayes(NB)andSupportVectorMachine(SVM).
Regardingtrainingdata,thesummariesfromthefouran-
notatorshadaCohenKappa[6]agreementof0.487. Avalue
between 0.4-0.6 is considered a moderate agreement when
the task is well-deﬁned [6]. Since the same code fragment
line could have contradictory markings by the four anno-
tators. Such noise could confuse machine learning models,Average false positive rateAverage true positive rate
0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0               4−line
               3−line
               2−line
               1−line
               0−lineSVM
NB
SVM−query−only
first−N−lines
last−N−linesFigure 3: ROC curves
moresoforSVMthanNaiveBayes. Otherpreviouseﬀortsin
summarizationhaveaddressedthisproblembytrainingand
evaluatingamodelusingagold standard summary[15].
Agoldstandardsummaryforagivencodefragmentconsists
of lines marked as in-summary by at least two annotators,
andlinesmarkedbyoneannotatorornonearenotconsid-
eredtobeinagoldstandardsummary. Onaverage,thesize
of a summary in the gold standard was 33.5% (sd=14.5%)
oftheoriginalcodefragment.
4. FEASIBILITYSTUDY
Our main research question is whether it is feasible to
generatecodefragmentssummariesusingonlysyntacticand
query-related features. On average, our summarizer could
generate features for a code fragment in 0.09 seconds per
codefragment. Ifwecangeneratereasonablecodefragment
summariesusingthesetwosimpletypesoffeatures,itwould
provideuswitha promisingfeasibilityargumentandbase-
linefordevelopingmoresophisticatedtechniques.
Effectiveness Metrics
We usedleave-one-out cross validation, where“one”(also
known as a fold) refers to one code fragment.2For each
fold, we deﬁned the correctness of the generated summary
asthecorrespondinggoldstandardsummary.
Toevaluatehowwellageneratedsummaryresemblesthe
goldstandardsummaryintheoracle,wecomparedthetwo
usingR-precision, an evaluation metric from the ﬁeld of
informationretrieval[13]. R-precisionissimilartoprecision-
at-k,theprecisionforsummariesoflengthk. Precision-at-k
determinesoutofthetopklinespredictedbyourclassiﬁer
(predicted k), howmanyarecorrect(|oracle∩predicted k|).
R-precisiondiﬀersfromprecision-at-kbyallowingsummaries
ofvariablelengths. TheR-precisionofacodefragmenteval-
uatesthetopRlinesreturnedbyourclassiﬁer(predicted R),
whereRisthelengthofthesummaryoracle. Moreformally,
R-precisionisgivenby|oracle∩predicted R|
|predicted R|.
Effectiveness results
As a preliminary evaluation, we assessed the performance
ofoursummarizerbycomparingwhethertheclassiﬁerwas
betterthanthreebaselines: theﬁrst-N-linesclassiﬁerwhich
2To maximize the training data but still evaluate on the
unseenevaluationdata,thecrossvalidationconsistedof53
folds,eachfoldyieldedapredictionforonesummarytrained
on69(i.e.,17+53-1)goldstandardsummaries.A1 A2 A3 A40.40.50.6
3 annotators except ...Kappa agreement4 annotators
3 + KonailaFigure 4: Agreement result
constructs a summary of lengthNby picking the ﬁrstN
lines of a code fragment, thelast-N-linesclassiﬁer which
picks the lastNlines, and theSVM-query-onlyclassiﬁer
that only uses the two query related features described in
Section 3.2. We conducted the performance comparison
throughareceiveroperatorcharacteristic(ROC)curve. An
ROC curve depicts the trade-oﬀ between the true positive
rate and false positive rate as we variedNfrom one line
tofourlines. Thiscurveenablesustounderstandexplicitly
theperformancetrade-oﬀamongdiﬀerentsummarylengths.
The coordinate of a point on the curve is given by the av-
eragetruepositiverate(averageofeachofthetruepositive
ratepercodefragment)andaveragefalsepositiverate(av-
erage of each of the false positive rate per code fragment).
Thetrue positive rateofacodefragmentcisgivenby:
|{lines in gold st. summary ofc} ∩ {lines in gen. summary ofc}|
|{lines in gold st. summary ofc}|
Thefalse positive rateofacodefragmentcisgivenby:
|{lines inc notin gold st. summary} ∩ {lines in gen. sum. ofc}|
|{lines inc notin gold st. summary}|
Averaging the rates per code fragment (rather than for all
lines)alignsbetterwiththeactualtaskofprovidingasum-
maryforacodefragment(ratherthanbeingjustanexercise
ofpredictingsummary-membershipoflines). Thecloserthe
ROCcurveistotheupperleftcorner(withfewerfalseposi-
tivesandmoretruepositives),thebettertheclassiﬁer. The
areaunderthecurvesumsupthisintuition: thebetterthe
classiﬁer,thecloseritsareaunderaROCcurveisto1.
Figure3showsﬁveROCcurves: twoversionsofourclas-
siﬁer(SVMandNB,thetwothickerlines)andthreebase-
lines(SVM-query-only,ﬁrst-N-lines, andlast-N-lines, the
threethinnerlines). Ourtwoclassiﬁershaveareaunderthe
curve of 0.806 forSVMand 0.772 forNB. Both clearly lie
abovetheﬁrst-N-linesbaseline(theareaunderthecurveis
0.493) andlast-N-linesbaseline (the area under the curve
is0.503). Inaddition,thetwoclassiﬁersusingbothsyntac-
tic and query features out-perform the baseline using only
query-related features,SVM-query-only, whose area under
thecurveis0.629.
Comparing generated and annotators’ summaries
Figure4illustratestheKappaagreementofthefouranno-
tators(kappa=0.487)andhowtheagreementchangedwhen
we left out the summaries provided by each one of the an-
notators and replaced the summaries of the left-out anno-
tator with summaries generated by the classiﬁer. In one
case (A3), swapping in the generated summaries evenim-
provedthe agreement, whereas in two cases the agreement
decreasedslightlyandinthethirdcasedecreasedmore. The
averageofthefourkappastatisticsofthethree-annotators-
plus-our-classiﬁersettingswas0.484,almostthesameasthe
agreementofthefourannotators.One could argue that the average R-precision was 0.705
andtheareaundertheROCcurvewerefarfrom1,theper-
fectscore. However,giventheagreementamongthehuman
annotatorsonthesummarieswasconsideredmoderatelylow
(kappa=0.487),thereisalimittotheperformanceonecould
expectfromamachinelearningapproachthatreliedonthis
samesetofannotations. Theseinitialresultsshowpromis-
ingfeasibilityinusinglight-weightfeaturestogeneratecode
fragmentsummaries.
5. CONCLUSIONANDFUTUREDIRECTIONS
We introduced a new idea of generating code fragment
summariesasawaytoprovidesuccinctcuesforWebpages
containing code fragments. These summaries have a wide
rangeofapplicationsinsummarizinganyartifactscontain-
ingcodefragments. Theapproachofusinglight-weightsyn-
tacticandqueryfeaturesachievedaprecisionof0.705when
we allowed summaries to be of the same length as the or-
acle. With this level of precision, our summaries achieved
thesamelevelofagreementashumanannotatorswitheach
other. The features are fast to generate, 0.09 seconds per
codefragment,makingitpossibletodeployinarealsearch
engineandothersoftwareengineeringsettings.
Therearepromisingfuturedirectionstotakeourcurrent
summarizerto. ObservingthesummariesintheEclipseFAQ
oraclerevealedthepromiseinadditionalcode-levelanalysis,
totakeintoaccounttherelationbetweenlineswhengenerat-
ingsummaries. Forexample,simpleintra-methoddata-ﬂow
relatedfeatures—suchaswhetherareturntypeofamethod
call in a code fragment is later used–can be indicative of
whether the line is important for a summary. Analyses on
codefragmentscanposetechnicalchallengesrelatedtothe
factthatcodefragmentsarenotcompleteprograms.
Intheevaluationofoursummaries, deﬁningthecorrect-
nessofasummaryasthesummarylinesmoreagreedupon
bytheannotatorsassumedthatacodefragmenthasasingle
universal summary suitable for everyone. The moderately
lowagreementamongtheannotatorsmotivatedustoinves-
tigate a diﬀerent assumption on the correctness of a sum-
mary: there does not exist one correct summary; rather,
each annotator’s version of the summary is correct. With
thisassumption,anoptimalsummarizerwouldbeonethat
personalizessummaries for each individual. Personalized
summariesisapromisingdirectionforfutureresearch.
6. ACKNOWLEDGMENTS
Thankstothefourannotators,J.Pineau,K.Moﬀatt,P.
Duboue,P.Rigby,Y.Chhetri,F.Ferreira,G.Petrosyan&
C.Treude. ThisworkissupportedbyNSERC&McGill.
7. REFERENCES
[1] S.Bajracharya,J.Ossher,andC.Lopes.Leveraging
usagesimilarityforeﬀectiveretrievalofexamplesin
coderepositories.InProc. of FSE,pages157–166,
2010.
[2] J.Brandt,P.Guo,J.Lewenstein,M.Dontcheva,and
S.Klemmer.Twostudiesofopportunistic
programming: interleavingwebforaging,learning,and
writingcode.InProc. of CHI,pages1589–1598,2009.
[3] M.Bruch,M.Monperrus,andM.Mezini.Learning
fromexamplestoimprovecodecompletionsystems.In
Proc. of ESEC/FSE,pages213–222,2009.[4] R.BuseandW.Weimer.SynthesizingAPIusage
examples.InProc. of ICSE,pages782–792,2012.
[5] S.Chatterjee,S.Juvekar,andK.Sen.SNIFF:A
searchengineforJavausingfree-formqueries.InProc.
of FASE,pages385–400,2009.
[6] J.Cohenetal.Acoeﬃcientofagreementfornominal
scales.Educational and psychological measurement,
20(1):37–46,1960.
[7] E.CutrellandZ.Guan.Whatareyoulookingfor?:
aneye-trackingstudyofinformationusageinweb
search.InProceedings of the Conference on Human
Factors in Computing Systems,pages407–416,2007.
[8] S.Haiduc,J.Aponte,andA.Marcus.Supporting
programcomprehensionwithsourcecode
summarization.InProc. of ICSE-v2,pages223–226,
2010.
[9] R.Hoﬀmann,J.Fogarty,andD.Weld.Assieme:
ﬁndingandleveragingimplicitreferencesinaweb
searchinterfaceforprogrammers.InProc. of UIST,
pages13–22,2007.
[10] R.HolmesandG.Murphy.Usingstructuralcontext
torecommendsourcecodeexamples.InProc. of
ICSE,pages117–125,2005.
[11] J.Kim,S.Lee,S.-W.Hwang,andS.Kim.Enriching
documentswithexamples: Acorpusminingapproach.
Transactions on Information Systems,31,2013.
[12] S.Mani,R.Catherine,V.Sinha,andA.Dubey.
AUSUM:approachforunsupervisedbugreport
summarization.InProc. of FSE,pages1–11,2012.
[13] C.Manning,P.Raghavan,andH.Schutze.
Introduction to information retrieval.Cambridge
UniversityPress,2008.
[14] S.McLellan,A.Roesler,J.Tempest,andC.Spinuzzi.
BuildingmoreusableAPIs.IEEE Software,
15(3):78–86,1998.
[15] S.Rastkar,G.Murphy,andG.Murray.Summarizing
softwareartifacts: acasestudyofbugreports.In
Proc. of ICSE,pages505–514,2010.
[16] M.Robillard.WhatmakesAPIshardtolearn?
Answersfromdevelopers.IEEE Software,26(6):27–34,
2009.
[17] S.Sim,R.Gallardo-Valencia,K.Philip,M.Umarji,
M.Agarwala,C.Lopes,andS.Ratanotayanon.
Softwarereusethroughmethodicalcomponentreuse
andamethodicalsnippetremixing.InProc. of CSCW,
pages1361–1370,2012.
[18] G.Sridhara,E.Hill,D.Muppaneni,L.Pollock,and
K.Vijay-Shanker.Towardsautomaticallygenerating
summarycommentsforJavamethods.InProc. of
ASE,pages43–52,2010.
[19] G.Sridhara,L.Pollock,andK.Vijay-Shanker.
Automaticallydetectinganddescribinghighlevel
actionswithinmethods.InProc. of ICSE,2011.
[20] J.StylosandB.Myers.Mica: Aweb-searchtoolfor
ﬁndingAPIcomponentsandexamples.InProc. of
VL/HCC,pages195–202,2006.
[21] T.XieandJ.Pei.MAPO:miningAPIusagesfrom
opensourcerepositories.InProc. of MSR,pages
54–57,2006.