Generating Obstacle Conditions for Requirements Completeness
Dalal Alrajeh∗, Jeff Kramer∗, Axel van Lamsweerde†, Alessandra Russo∗, and Sebastian Uchitel∗
∗Department of Computing, Imperial College London, UK
{da04,jk,ar3,su2 }@doc.ic.ac.uk
†ICTEAM, Universit ´e catholique de Louvain, Belgium
avl@info.ucl.ac.be
Abstract —Missing requirements are known to be among the
major causes of software failure. They often result from a
natural inclination to conceive over-ideal systems where the
software-to-be and its environment always behave as expected.
Obstacle analysis is a goal-anchored form of risk analysis
whereby exceptional conditions that may obstruct system goals
are identiﬁed, assessed and resolved to produce complete
requirements. Various techniques have been proposed for
identifying obstacle conditions systematically. Among these, the
formal ones have limited applicability or are costly to automate.
This paper describes a tool-supported technique for generating
a set of obstacle conditions guaranteed to be complete and
consistent with respect to the known domain properties. The
approach relies on a novel combination of model checking
and learning technologies. Obstacles are iteratively learned
from counterexample and witness traces produced by model
checking against a goal and converted into positive and negative
examples, respectively. A comparative evaluation is provided
with respect to published results on the manual derivation of
obstacles in a real safety-critical system for which failures have
been reported.
Keywords -Requirements completeness, risk identiﬁcation, ob-
stacle, model synthesis, model checking, inductive learning,
goal-oriented requirements engineering
I. I NTRODUCTION
Completeness is among the most critical and difﬁcult
challenges facing requirements engineers. Missing require-
ments and assumptions are reported as one of the major
causes of software failure [1]. Incompleteness often arises
from the lack of anticipation of exceptional conditions. The
natural inclination is rather to conceive idealised systems;
this prevents adverse events or conditions from being prop-
erly identiﬁed, and as a result, speciﬁcations of suitable
countermeasures in such circumstances are missing.
Risk analysis is therefore at the heart of the requirements
engineering process [1], [2]. A riskis commonly deﬁned as
an uncertain factor whose occurrence may result in some
loss of satisfaction of some corresponding objective. In
goal-oriented system modelling frameworks, obstacles are
introduced as a natural abstraction for risk analysis when
using goal models [3], [4]. An obstacle to a goal is a
precondition for the non-satisfaction of this goal. Depending
on the category of goal being obstructed, obstacles may
correspond to safety hazards, security threats, inaccuracyconditions on software input/output variables with respect
to their environment counterpart, etc.
Obstacle analysis roughly consists of three steps [1]: (a)
identify as many obstacles as possible to every leaf goal in
the systems goal reﬁnement graph, (b) assess the likelihood
and severity of each obstacle; and (c) resolve likely and
severe obstacles by systematic transformations to the goal
model using appropriate countermeasures.
The obstacle identiﬁcation step is obviously crucial. In [4],
a formal technique is described for generating obstacles by
regressing goal negations through available domain proper-
ties. Although quite systematic, this technique appears costly
to implement for goals formalised in a ﬁrst-order real-time
linear temporal logic. No tool support is available.
This paper presents an alternative, tool-supported tech-
nique for obstacle generation. A complete set of obstacles,
relative to what is known about the domain, is computed
by iterating the following cycle: (a) a behaviour model
is synthesised from the available background properties;
(b) this model is veriﬁed against the goal and against a
negated form of it, in order to generate a negative trace
(counterexample) and a positive trace (witness), respectively;
(c) the negative trace is taken as positive example whereas
the positive trace is taken as negative example input for
a learning engine; (d) the learning tool generates a set
of candidate obstacles that cover the positive example and
exclude the negative one; (e) the user can then select from
the generated obstacles those considered likely and severe,
and suggest further domain properties; (f) a new cycle
is applied to the background properties augmented with
such properties and the negated obstacles generated at the
previous cycle. The process terminates when a domain-
complete set of obstacles is generated.
The synergistic use of model checking and learning
technologies was already explored successfully in [5] for
the generation of speciﬁcations of software operations from
goals. Our contributions in this paper include the following.
•A new combination of model checking and learning
for supporting the signiﬁcant, challenging task of risk-
driven elaboration of more complete requirements.
•Automation is increased with respect to the earlier
combined use of model checking and learning (in
[5], positive traces have to be elaborated manually).Moreover, the goal speciﬁcation language is much more
expressive, allowing us to capture the full range of
Achieve andMaintain goals (only Immediate Achieve
goals are considered in [5]).
•Tool support is provided for formal obstacle generation,
which was unavailable to date. Moreover, with respect
to earlier techniques, the one here generates obstacles
that are domain-consistent by construction (domain-
consistency checks have to be performed separately in
[4]). The technique is also guaranteed here to converge
to a domain-complete set of obstacles.
The paper is organised as follows. Section II brieﬂy pro-
vides some background on goal modelling, obstacle analysis,
labelled transition systems for behaviour model analysis and
synthesis, ﬂuent linear temporal logic for goal speciﬁcation
and inductive logic programming for learning obstacles.
Section III introduces a motivating example. The approach
is detailed in Section IV. Section V discusses our experience
in using our technique and tool for generating a variety of
obstacles in a real safety-critical system for which failures
have been reported [6]. Some related work is discussed in
VI. Section VII concludes the paper.
II. B ACKGROUND
A. Goal-Oriented Modelling and Obstacle Analysis
Agoal is a prescriptive statement of intent to be satisﬁed
by the agents forming the system. The latter include the
software to be developed, pre-existing software, devices such
as sensors and actuators, people, etc. The word system thus
refers to the software-to-be together with its environment.
Unlike goals, domain properties are descriptive statements
about the problem world (such as natural laws). A goal
model is an AND/OR graph showing how goals contribute
positively or negatively to each other [1]. Parent goals are
obtained by abstraction whereas child goals are obtained
by reﬁnement. In a goal model, each leaf goal is assigned
to a single system agent as a requirement orassumption
depending on whether it is assigned to the software-to-be or
an environment agent, respectively.
A goal may be behavioural orsoftdepending on whether
it can be satisﬁed in a clear-cut sense or not. This paper
focusses on behavioural goals. A behavioural goal captures
intended behaviour declaratively and implicitly. It can be of
type Achieve orMaintain/Avoid . The speciﬁcation pattern
for an Achieve goal is ifCthen sooner-or-later T, where C
denotes a current condition and Tatarget condition, with
obvious particularisations to Immediate Achieve ,Bounded
Achieve and Unbounded Achieve goals. The pattern for a
Maintain (resp. Avoid ) goal is [ ifCthen ]always G(resp. [ if
Cthen ]never B), where GandBdenote a good andbad
condition, respectively.
An obstacle to a goal is a domain-satisﬁable precondition
for the non-satisfaction of this goal. The following deﬁnition
makes this more precise [1], [4].Deﬁnition 1. Let Dom be a set of known domain properties
and G a goal assertion. An assertion O is said to be an
obstacle to G in Dom iff the following conditions hold:
•{O,Dom} |=¬G (obstruction )
•{O,Dom} /ne}ationslash|=false (domain consistency)
Obstacles can be AND/OR reﬁned into sub-obstacles,
resulting in a goal-anchored form of risk tree. In such tree,
the root obstacle is the negation of the associated leaf goal
in the goal model; an AND-reﬁnement captures a combina-
tion of sub-obstacles entailing the parent obstacle; an OR-
reﬁnement captures alternative ways of entailing the parent
obstacle —and, recursively, of obstructing the corresponding
leaf goal; the leaf sub-obstacles are single, ﬁne-grained
obstacles whose likelihood can be easily estimated.
Deﬁnition 2. Let Dom be a set of known domain properties
and G a goal assertion. A set of obstacles O1,O2,...,O n
toGis said to be domain-complete for Gin Dom if the
following condition holds:
¬O1,¬O2,...,¬On,Dom|=G(domain-completeness )
Note that completeness, while highly desirable, is
bounded by what we know about the domain. Obstacle gen-
eration techniques should therefore support the incremental
elicitation of relevant domain properties as well. Pattern-
based heuristics can be used for more focussed elicitation
[1]. In particular, for the above patterns deﬁning Achieve
and Maintain/Avoid goals, domain properties of the form
ifTthen NorifGthen Nare worth eliciting, where N
denotes a necessary condition for the target condition Tor
good condition G; they result in obstacles of form [ Cand]
never Nor [Cand]sooner-or-later not N, respectively.
B. Model Checking
Model checking is an automated technique for verifying
that a model Msatisﬁes property φ, writtenM|=φ. In this
paper we are interested in declarative models and properties
both expressed in a temporal logic called Fluent Linear
Temporal Logic (FLTL) [7] with labelled transition systems
as the semantic structure.
In FLTL, a ﬂuent is a propositional atom deﬁned by a set
Ifof initiating events, a set Tfof terminating events and an
initial truth value either true orfalse . Given a set of event
labels Act, we write f=/an}bracketle{tIf,Tf,Init/an}bracketri}htas a shorthand for a
ﬂuent deﬁnition, where If⊆Act,Tf⊆ActandIf∩Tf=
∅, and Init∈ {true,false}. FLTL assertions use standard
operators for temporal referencing such as: /circlecopyrt(at the next
time-point), •(at the previous time-point), ✸(some time
in the future), ✷(always in the future), U(always in the
future until), W(always in the future unless) and ⇒(always
implies) [8]. We will also use real-time operators ✷∼x,✸∼x
andU∼xwhere∼∈{<,≤,>,≥}andxdenotes a number
of time-points [9].
The semantics of FLTL assertions is deﬁned in terms
of behaviour models called Labeled Transition Systems
(LTSs). An LTS is an automaton deﬁned by a structure(Q,Act,δ,q0), whereQis a ﬁnite set of states, Actis a set of
event labels, δ⊆ Q× Act×Q is a labeled transition relation,
andq0is an initial state. The global system behaviour is
captured by the parallel composition, denoted as /bardbl, of LTSs
(one for each component) by interleaving their behaviour
but forcing synchronisation on shared events [10]. A trace
in an LTS is a sequence of events from the initial state.
Given a trace trover Actand ﬂuent deﬁnitions D, a ﬂuent
is said to be true (resp. false ) in a trace trat position i,
denoted tr,i|=f,iffeither of the following conditions hold:
(a)the ﬂuent is initially true and no terminating event has
occurred since, or (b)some initiating event has occurred
with no terminating event occurring since then.
An LTS can be synthesised from a safety assertion P,
written in FLTL, using the algorithm in [7]. The resulting
LTS, denoted L(P), is an LTS that captures all inﬁnite traces
over the alphabet Actthat satisfy P. Also, an FLTL property
φcan be veriﬁed against an LTS L(P)using the model
checking algorithm described in [7] and implemented in
the LTSA tool [10]. This procedure involves constructing
an automaton B¨uchi(¬φ) that recognises all inﬁnite traces
over the alphabet Act violating φand checking that the
synchronous product L(P)/bardblB¨uchi(¬φ)is empty. Any trace
leading to the error state in this product is a counterexample
to the property φ, while any trace not leading to the error
state is a witness to the property φ.
C. Inductive Logic Programming
Inductive Logic Programming (ILP) [11] is a machine
learning technique for generating a generalisation Hthat,
together with a given knowledge base B, covers a given set
of positive examples {e+}and excludes a set of negative
examples {e−}, where B,H,e+ande−are expressed as
logic programs [12]. A Mode Declaration (MD) is a form
of language bias that deﬁnes the space of plausible general-
isations (solutions) for a given ILP task, by specifying the
syntactic form of the generalisations that can be learned.
We uses(MD)to denote the set of all plausible gener-
alisations rules that can be constructed from MD. For the
rest of this paper, we deﬁne an inductive task to be a tuple
/an}bracketle{tB,{e+},{e−},MD/an}bracketri}htand an inductive solution as follows.
Deﬁnition 3. Let/an}bracketle{tB,{e+},{e−},MD/an}bracketri}htbe an inductive task.
The logic program H, whereH⊆s(MD), is an inductive
solution iff, ∀e+.B∧H|=e+and∀e−.B∧H/ne}ationslash|=e−,
where|=is entailment under stable model semantics [13].
We consider here logic programs that use the following
predicates from [14]: ﬂuent (f) to assert that fis a ﬂuent,
event (e) for an event e,timepoint (i)for time-point i,trace (s)
for traces,holdsAt(f,i,s)to express that the ﬂuent fholds
at time-point iin traces,happens (e,i,s ) to express that
eventehappens at time-point iin traces, and initiates (e,f)
(resp. terminates (e,f)) to capture ﬂuents that are initiated
(resp. terminated) by event e. The knowledge base alsocomprises a set of frame axioms for determining ﬂuents’
truth values and semantic constraints (e.g., events cannot
happen concurrently), denoted Ax.
III. M OTIVATING EXAMPLE
In this section we give an overview of the approach and
introduce our running example. We use a simpliﬁed version
of the train control system introduced in [1]. The example
is propositional, rather than ﬁrst order, for simplicity; it
comprises a train, a signal and a driver. A train may start or
stop moving on the track. The signal may be set to stop or
go. The view of the signal may be clear orobstructed . The
driver may respond to or ignore the stop signal. We deﬁne
the ﬂuents for the train control system as follows.
TrainStopped =/an}bracketle{tstop train ,start train ,false/an}bracketri}ht
StopSignal =/an}bracketle{tsettostop,settogo,false/an}bracketri}ht
SignalVisible =/an}bracketle{tclear signal ,obstruct signal ,true/an}bracketri}ht
DriverResponsive =/an}bracketle{tdriver responds ,driver ignores ,true/an}bracketri}ht
Consider the goal stating that the train shall stop when the
signal is set to stop:
Goal Achieve [TrainStoppedAtBlockSignalIfStopSignal ]
StopSignal ⇒ /circlecop†rt TrainStopped (1)
Suppose the domain properties include the following neces-
sary conditions for the train to stop:
/circlecop†rtTrainStopped ⇒DriverResponsive (2)
which states that if a train stops, the train driver was
responsive to the stop command, and
/circlecop†rtTrainStopped ⇒SignalVisible (3)
meaning that if a train stops, the stop signal was visible.
Consider a system that satisﬁes the domain properties (2)
and (3). A simple goal violation scenario of such a system
is one in which the signal is set to stop and then although
the stop signal is visible initially, the driver ignores the
signal and the train does not stop. Similarly, another scenario
possible in the domain is when the signal is set to stop and
the signal is visible initially, the signal is then obstructed
(e.g. because of foggy weather) and consequently the train
does not stop. Such scenarios represent situations where the
goal satisfaction is at risk.
The problem is that the goal Achieve [TrainStoppedAt-
BlockSignalIfStopSignal ] is too ideal; it presupposes that
nothing in the environment will prevent the goal from being
achieved. The goal can only be achieved if it is assumed that
the driver is always responsive and the stop signal is always
visible, among others; these two assumptions are likely to
be violated from time to time.
In both scenarios, we can see that some exceptional
circumstance has prevented the goal from being satisﬁed. In
the ﬁrst case, the driver not being responsive is an obstacle
preventing the goal TrainStoppedAtBlockSignalIfStopSignal
from being achieved; this obstacle can be formally expressed
as✸(StopSignal ∧ /circlecopyrt¬ DriverResponsive ). In the second case,the signal being not visible, ✸(StopSignal ∧/circlecopyrt¬ SignalVisible ),
is another obstacle to the goal.
Detecting obstacle conditions under which goals may be
violated is essential for not missing requirements that would
prescribe what the software should do in order to properly
handle such unexpected situations. In this simple example,
detecting obstacles is straightforward; however, for complex
systems, performing such a task manually is likely to be
error-prone and miss important obstacles. The technique in
this paper can automatically detect all obstacles to a goal
for a set of known domain properties.
IV. G ENERATING OBSTACLES TO GOAL SATISFACTION
The approach proposed here considers a set of goals
and background properties as input and iteratively computes
obstacles to the goals. Each iteration can be subdivided into
three main phases (see Figure 1):
Elicit new Dom’BP:=Dom
SelectSynthesise LTS
L(BP)
BP:=BP∪ ¬O∪Dom’Model Check
L(BP)|=C⇒ ¬(ΘT)
Model Check
L(BP)|=C⇒ΘT
Learntr−tr+
e+
e−Dom’{Oi}OAnti-target: C⇒ ¬(ΘT)
Goal:C⇒ΘT(BP)
BP(BP)
Figure 1. Overview of the proposed obstacle generation approach.
•The ﬁrst phase generates domain-consistent counterex-
ample and witness traces for a goal. This is achieved
by synthesising a behaviour model L(BP)from the
background properties BPcurrently available and per-
forming two model checks. To obtain a counterexample
trace tr−to a goal, taking the form C⇒ΘTwhereΘis
a temporal operator, the model is checked against it. To
obtain a witness tr+to the goal, the model is checked
against a perturbation of the property that asserts that
the goal target is violated: C⇒¬ΘT.
•The second phase is concerned with learning obstacles.
A learning system is given the background properties
and the goal in addition to the counterexample and
witness traces generated in the ﬁrst phase. Critically, it
is the counterexample trace to the goal that is provided
to the learning system as a positive example e+, i.e., an
example in which an obstacle is satisﬁed. The witness
trace to the goal is provided as a negative example e−
to the learning system, i.e., an example in which the
obstacle is not satisﬁed. The learning system generates
a set of candidate obstacles {Oi}that cover the positive
example while excluding the negative one.
•The third phase, contrary to the others, requires human
intervention. An engineer makes a selection Ofrom the
set of proposed obstacles based on those consideredlikely and severe. The selected obstacles are negated
and added to the background properties. This is to
allow for further identiﬁcation of obstacles in the next
iteration. In this phase, the engineer can also suggest
further domain properties, based on the learned obsta-
cles and provided they are consistent with the existing
background properties to which they are added, to allow
for identiﬁcation of further obstacles or ﬁner-grained
sub-obstacles.
The process terminates when a domain-complete set of
obstacles is generated (see Deﬁnition 2). In what follows
we structure the presentation according to these phases.
A. Generating Goal Counterexamples and Witnesses
Consider the LTS shown in Figure 2 which is synthesised
from the background properties including (2) and (3) of
Section III. Notice that some of the traces the LTS exhibits
satisfy the system goal Achieve [TrainStoppedAtBlockSignal-
IfStopSignal ] while others do not. Traces which satisfy the
goal are examples in which no obstacles are present (e.g., the
signal is set to stop and the train stops). On the other hand,
traces which violate the goal are examples of obstacles that
are present (e.g., the view of the stop signal is not clear).
To automatically ﬁnd such examples, we perform two model
checks which are explained below.
0 14 2
3
starttrain
settogo
settostop
clearsignal
driverrespondsobstruct signaldriverignore
stop
trainclearsignaldriverresponds
starttrain
settogo
signalstop
obstruct signal
driverignoreobstruct signaldriverignore
starttrain
settogo
settostop
clearsignaldriverrespondsclearsignaldriverignores
starttrain
settogo
settostop
obstruct signaldriverrespondsstart
traindriverresponds
stoptrain
settogo
settostop
clearsignal
Figure 2. The LTS L(BP)for the train control system.
1) Goal Counterexamples: For a given set of background
properties BP, the computation of a counterexample trace to
a goalGis done in a straightforward manner by checking the
LTSL(BP)againstG. If the model checker ﬁnds that L(BP)
contains a trace that violates G, it is produced automatically.
For example, model checking the LTS in Figure 2 against
the goal Achieve [TrainStoppedAtBlockSignalIfStopSignal ]
using the LTSA model checker gives the following violation.
Violation in TrainStoppedAtBlockSignalIfStopSignal:
set_to_stop StopSignal
driver_ignores StopSignal
The left column represents a sequence of events in which
the goal is violated. The column on the right indicates the
ﬂuents that are true immediately after the occurrence of the
event to their left. This trace fragment exempliﬁes a case in
which the train does not stop after the signal is set to stop.
This behaviour thus satisﬁes an obstacle which prevents the
goal from being achieved.If checking the LTS L(BP)againstGdoes not yield a
counterexample, this means that L(BP)satisﬁesGand hence
there are no more obstacles to be generated; the three-phase
cycle then comes to an end.
2) Goals Witnesses: Conceptually, a witness trace to a
goal is obtained by checking the LTS L(BP)against a
property stating that the goal is violated. However, this might
lead to traces that vacuously satisfy the goal. If, for example,
the goal is of the form C⇒ΘTthe property asserting that
the goal is violated is ¬(C⇒ΘT), that is, ✸(C∧ ¬ΘT).
Checking L(BP)against the latter property could generate a
counterexample that never satisﬁes C. Such counterexample
is a witness that vacuously satisﬁes the goal C⇒ΘTas
the antecedent is never true. We are interested in obtaining
witnesses where both CandΘThold. Hence we need to
checkL(BP)against a variant property C⇒¬ΘT. We refer
to this property as the anti-target for the goal C⇒ΘT.
Table I shows the corresponding anti-target assertions
for goals expressed in Achieve andMaintain modes. Anti-
target assertions for goals expressed in Avoid modes can
be constructed by simply removing the negation preceding
the target condition in the anti-target assertion of the corre-
sponding Maintain assertions in Table I.
If checking L(BP)againstG’s anti-target yields no coun-
terexamples, this means that L(BP)satisﬁes the anti-target,
i.e., there are no traces that satisfy the goal non-vacuously.
This is an indication of problem in the formulation of the
goal itself, which needs to be weakened, or in the back-
ground properties which need to be revised. This situation
is beyond the scope of this paper.
Table I
GOALS AND THEIR CORRESPONDING ANTI -TARGET PATTERNS
Mode Goal Pattern Anti-target Pattern
C⇒ /circlecop†rtTC⇒ /circlecop†rt¬T
Achieve C⇒✸≤XTC⇒✷≤X¬T
C⇒✸T C⇒✷¬T
C⇒T C⇒ ¬T
C⇒✷T C⇒✸¬T
Maintain C⇒GWTC⇒ ¬TU¬G
C⇒ •T C⇒ •¬T
C⇒✷≤XTC⇒✸≤X¬T
Returning to the example of the previous section, to
generate a witness to the goal Achieve [TrainStoppedAt-
BlockSignalIfStopSignal ], the LTS in Figure 2 is checked
against the anti-target assertion:
StopSignal ⇒/circlecop†rt¬ TrainStopped (4)
This results in the violation trace below where the signal is
set to stop and the train stops immediately afterwards.
Violation in NotTrainStoppedAtBlockSignalIfStopSignal:
signal_stop StopSignal
stop_train StopSignal && TrainStopped
B. Learning Obstacles from Traces
The next step involves the generation of obstacles from the
produced counterexample and witness traces to the goal. To
perform this computation, ILP is deployed. The applicationof ILP to the obstacle generation task comprises three main
steps: (1) deﬁnition of an ILP task through appropriate
encoding of background properties, goals, counterexample
and witness, (2) computation of an inductive solution, and
(3) decoding of the inductive solutions into obstacles. These
steps are done automatically and are hidden from users who
are only shown the ﬁnal set of possible obstacles.
1) ILP Task for Learning Obstacles: As explained in
Section II-C, ILP computes generalised rules from given
examples and knowledge base, expressed as logic programs.
Hence, to learn obstacles, the background properties, goals,
and counterexample and witness traces have to be mapped
into a logic program; as in any learning task, a suitable
language bias has also to be chosen. The encoding uses, in
addition to the predicates described in Section II, a collection
LOof anti-target predicates, speciﬁc to the anti-target pat-
terns in Table II, and a collection LTof target predicates spe-
ciﬁc to the target patterns in Table III. The correspondence
illustrated in these tables considers the target condition to be
a conjunction or disjunction of ﬂuents and that the predicates
are introduced for each ﬂuent in T. Examples of anti-target
predicates include obstructed next(f,i,s), which means that
a ﬂuentfis prevented from being true at the next time-
point from iin traces, and obstructed always by(f,i,x,s),
which states that a ﬂuent fis obstructed from being true for
xtime-points after time-point iin traces. Examples of target
predicates include holdsAt eventually (f,i,s), meaning that
ﬂuentfis true sometime in the future after time-point i.
Table II
ILP ENCODING OF ANTI -TARGET
Anti-target Pattern Corresponding obstacle predicates
/circlecopyrt¬T obstructed_next(t,I,S).
✷≤x¬T obstructed_always_by(t,I,x,S).
✷¬T obstructed_always(t,I,S).
¬T obstructed(t,I,S).
✸¬T obstructed_eventually(t,I,S).
¬TU¬Gobstructed_always_until(t,I1,I2,S).
obstructed_eventually(g,I1,S).
•¬T obstructed_before(t,I,S).
✸≤x¬T obstructed_eventually_by(t,I,x,S).
Table III
ILP ENCODING OF TARGET
Target Pattern Corresponding goal predicates
/circlecopyrtT holdsAt_next(t,I,S).
✸≤xT holdsAt_eventually_by(t,I,x,S).
✸T holdsAt_eventually(t,I,S).
T holdsAt(t,I,S).
✷T holdsAt_always(t,I,S).
GWTholdsAt_always_until(g,I1,I2,S);
holdsAt_always(g,I1,S).
•T holdsAt_before(t,I,S).
✷≤xT holdsAt_always_by(t,I,x,S).
The knowledge base Bof the ILP task includes an
encoding of the background properties (ﬂuent deﬁnitions
and domain properties), the goals and a set of frame
axioms Axthat specify conditions under which an initi-
ating or terminating changes the truth value of ﬂuents.Fluent deﬁnitions are expressed as initiates andterminates
rules. For example the ﬂuent deﬁnition for TrainStopped is
expressed as initiates(stop_train, trainStopped) and
terminates(start_train, trainStopped) . Domain prop-
erties are encoded as integrity constraints, which force
the learning tool to compute only obstacles that are con-
sistent with the domain. Intuitively, every domain prop-
erty is of the form T⇒N, whereNis either a con-
junction of literals or a weak until formula, deﬁning a
necessary condition for T. In the former case, a domain
property is translated into a set of rules of the form
false :- holdsAt(t,I,S), ...,Lit(n j,I,S) , one for each
conjunct literal njinN, where Litis anholdsAt literal if
njis negated in N, and anot holdsAt literal otherwise.
For example, the domain property /circlecopyrtTrainStopped ⇒Driver-
Responsive in (2) is represented as
false:- holdsAt_next(trainStopped,I,S),
not holdsAt(driverResponsive,I,S).
This means that in any stable model of Bwhere
holdsAt next(trainStopped,I,S) is true, holdsAt(driver-
Responsive, I,S) must also be true. Domain properties of
the form T⇒NWPare encoded into rules of the form
false:- holdsAt(t,I1,S), holdsAt_eventually(p,I1,I2,S) ,
not holdsAt_always_until(n,I1,I2,S).
false:- holdsAt(t,I1,S), not holdsAt_eventually(p,I1,I2,S) ,
not holdsAt_always(n,I1,S).
Goals, on the other hand, are encoded as rules of the
formholdAt(target,I,S):- τ(current,I,S), not O . In
this rule, O is a predicate in LO. The general interpretation
of such a rule is that the goal’s target condition holds if its
current condition holds and no obstacles exist. For instance,
theAchieve goal [ TrainStoppedAtBlockSignalIfStopSignal ],
expressed as (StopSignal ⇒/circlecopyrt TrainStopped )is encoded as
holdsAt_next(trainStopped,I,S):-
holdsAt(stopSignal,I,S),
not obstructed_next(trainStopped,I,S).
This rule informally states that if a signal is set to stop
and no obstacles to stopping the train at the next time-point
are satisﬁed, then the train stops. This encoding, as shown
later, plays a key role in the computation of obstacles. The
encoding is proven to be sound in terms of preservation of
the FLTL model consequences in the encoded logic program.
Theorem 1. Let BP be a set of background properties
expressed in FLTL let L(BP)be the LTS synthesised from
BP and tr a trace in L(BP). Letφbe a property expressed
in FLTL. Let τbe the encoding of BP, φand tr into
a corresponding knowledge base Bunder stable model
semantics. Then the following condition holds:
tr|=φif and only if B|=τ(φ)
2) Inferring Obstacle: Positive and negative examples
need to be provided to the ILP task. In the problem at
hand, the counterexample trace tr−to a goal is taken as a
positive example for learning obstacles to the goal while the
witness trace tr+to the goal is taken as a negative example.Each event occurrence in the counterexample (resp. witness)
trace is represented as a happens fact and added to the
positive (resp. negative) examples; every ﬂuent valuation in
the counterexample (resp. witness) trace is represented as a
holdsAt literal and also added to the positive (resp. negative)
examples. For instance, the goal counterexample trace for
our toy train system shown in Section IV-A can be system-
atically expressed as the following positive examples,
happens(set_to_stop,1,c). holdsAt(stopSignal,1,c).
happens(driver_ignores,2,c). holdsAt(stopSignal,2,c).
and the witness trace as the following negative examples.
happens(set_to_stop,1,w). holdsAt(stopSignal,1,w).
happens(stop_train,2,w). holdsAt(stopSignal,2,w).
holdsAt(trainStopped,2,w).
OnceB,e+ande−are generated, with B=τ(BP)∪
τ(G)∪Ax,e+=τ(tr−)ande−=τ(tr+), they are given to a
non-monotonic ILP system to compute a generalisation H
that covers e+and excludes e−. The encoding of the goal
and traces plays a crucial role in the computation of the
obstacles. Since the consequent of τ(G)is true in the stable
model of Bwhen no obstacles are present whilst the positive
examples indicate the contrary, the ILP task is concerned
with ﬁnding rules that would prevent the derivation of the
goal’s target. To deﬁne the solution space, we deﬁne the
mode declaration to include rules with LOpredicates in
the head, and holdsAt andLTpredicates in the body. In
our running example, the tool generates the following rule
stating that an obstacle to stopping the train at the next time-
point is the driver not being responsive.
obstructed_next(trainStopped, I,S):-
holdsAt(stopSignal, I,S),
not holdsAt_next(driverResponsive, I,S).
The computed explanation is then mapped back into FLTL
and presented to the engineer:
✸(StopSignal ∧/circlecop†rt¬ DriverResponsive ) (5)
In many situations, the learning tool may ﬁnd several
solutions as obstacles to a goal. When this is the case,
these alternatives are presented to the engineer to select the
obstacles considered likely and severe. Theorem 2 below
states that any computed solution is an obstacle to a goal.
Theorem 2. Let BP and Gbe a set of background proper-
ties and a goal, respectively, expressed in FLTL. Let tr−
and tr+be a counterexample and witness traces to G
inL(BP). Let Ax be a set of frame axioms expressed in
the logic programming formalism. Given the inductive task
/an}bracketle{tτ(BP)∪τ(G)∪Ax,τ(tr+),τ(tr−),MD/an}bracketri}ht, a set of rules
H∈s(MD)is an inductive solution under stable model
semantics iff the FLTL expression Ogiven by H=τ(O)is
an obstacle to Gsatisﬁed by tr−and not by tr+.
C. Completing the Obstacles
Once the analyst selects those obstacles, their negation is
added to the background properties. As the LTS synthesisedfrom the extended background properties may contain other
violation traces to the considered goal, the process of model
checking and learning is repeated again.
To generate a domain-complete set of obstacles, in every
new iteration i+1, we look for obstacles different from those
already generated and selected. The negation of each one is
added to the background properties BPand the veriﬁcation
process is repeated again. If a counterexample trace is found,
then the steps in Sections IV-A and IV-B are repeated until
no further counterexamples to the goal are found.
In some cases, the computed obstacles {Oi}may high-
light some domain properties that are missing from the
current background properties, typically, properties of the
formT⇒N(other necessary condition for the goal’s
target) or S⇒O(sufﬁcient condition for the obstacle under
consideration). When this is the case, the engineer may add
such properties to BPbefore the start of the new iteration.
Returning to our running example, the analysis is per-
formed on the LTS synthesised from the background
properties including the domain properties (2) and (3),
ﬂuent deﬁnitions and the negation of the obstacle (5),
i.e.,✷(¬StopSignal ∨/circlecopyrt DriverResponsive ). The counterex-
ample trace (settostop,obstruct signal)is generated. This
trace along with the witness (settostop,stop train)are
given to the learner tool which in turn computes the obstacle
✸(StopSignal ∧/circlecop†rt¬ VisibleSignal ) (6)
Once this is added to the current set of obstacles, a new
cycle results in no further violations to the goal. Hence
the set comprising the obstacles (5) and (6) is the domain-
complete set of obstacles for the goal Achieve [TrainStoppe-
dAtBlockSignalIfStopSignal ]. If sufﬁcient conditions for the
learned obstacles are added to the background properties
as additional domain properties, the process will go on to
generate ﬁner-grained sub-obstacles. Theorem 3 states that
for a given set of background properties BPand goal Gthat
is consistent with BP, the proposed approach will converge
to a domain-complete set of obstacles to goal G.
Theorem 3. Let BP be a set of background properties,
andGa goal expressed in FLTL, consistent with BP. Let
Tr−be the set of shortest ﬁnite counterexample traces
in the composition L(BP)/bardblBuchi(¬G). Let Tr+be a set
of ﬁnite witness traces to the goal Gin the composition
L(BP)/bardblL(G). Let Ax be a set of frame axioms expressed in
the logic programming formalism. Given the inductive task
/an}bracketle{tτ(BP)∪τ(G)∪Ax,τ(Tr+),τ(Tr−),MD/an}bracketri}ht, there exists a set
{Hi}of inductive solutions under stable model semantics,
such that BP ∪ ¬O1∪....∪¬On|=G, whereHi=τ(Oi)
for1< i < n .
V. V ALIDATION
We show here an application of the proposed approach
to the London Ambulance Service (LAS) [15]. We reporton our ﬁndings and illustrate the various steps in our
approach with respect to two goal patterns: Bounded andUn-
bounded Achieve . The application to goals under Maintain
andAvoid modes was similar. For the purpose of validating
our approach, we considered a set of obstacles which were
formally derived manually in [4], [16].
The LAS speciﬁcation, as presented in [4], [16], is for-
malised in a synchronous form of ﬁrst-order Linear Tem-
poral Logic (LTL) where the goals are expected to hold
at ﬁxed time-points but maybe violated in between, and
several events may occur between two consecutive time-
points. LTSs, on the other hand, assume an asynchronous
interpretation of LTL expressions. Therefore, to preserve
these semantics when applying model checking, the avail-
able synchronous speciﬁcation was systematically translated
into asynchronous FLTL using the algorithm described in
[17]. This translation makes use of a special event tick
which is introduced in the alphabet of the LTS to mark the
time-points at which goals are expected to hold. Similarly,
the language of our logic programs is extended with this
notion by deﬁning auxiliary predicates such as holdsAt tick
and holdsAt eventually tick. Fluent deﬁnitions were also
extracted from the speciﬁcation using the technique in [17].
A. Experimental Results
Our experiments generated all the obstacles which were
manually obtained described in [4] as well as other obstacles
that were not found there. The outcomes also conﬁrmed
that our approach provides support for eliciting new domain
properties. For instance, when checking the background
properties against one of the LAS goals reported later in
this section, the model checker produced the counterexample
(tick, a1.assign, a1.inc1.allocate, inc1.resolve, ... )where an
incident is stated to be resolved before the ambulance has
been mobilised and has intervened in the incident. Such
traces help engineers in detecting goals or domain properties
that are too weak, e.g., the domain property ∀inc:Incident
inc.Resolved ⇒ ∃a:Ambulance Intervention(a,inc) .
Furthermore, our approach produced ﬁner sub-obstacles
depending on the granularity of the provided domain prop-
erties. This case is exempliﬁed in Section V-C where the
learning tool produced the obstacle ResourceNotUsedOnPa-
tient which is a reﬁnement of the obstacle PatientNotTreate-
dAtIncidentLocation .
We tested our approach using two ILP systems, XHAIL
[18] and TAL [19]. These mainly differ in directions in
which they explore the generalisation search space and in
the degree of control they provide to users in deﬁning
further syntactic constraints over acceptable generalisations.
In our experience, we observed that the approach may
compute obstacles that appear too general, e.g. an obstacle
toTrainStopped being that the driver is not responsive
even when the signal is not set to stop. However, further
experiments suggest that producing several witnesses to agoal, including traces in which the target of goals is true
(with the condition being false), results in weaker obstacles.
In what follows we illustrate these ﬁndings where applicable.
B. Obstacles to Bounded Achieve Goals
Consider the goal Achieve [MobilisedAmbulanceInter-
vention ] stating that a mobilised ambulance shall intervene
at the incident within 11 minutes.
Goal Achieve [MobilisedAmbulanceIntervention ]:
(•Available ∧a.Mobilised )⇒✸≤11Intervention (a,inc) (7)
The background properties capture the following necessary
conditions for the target condition of the above goal to hold.
∀a:Ambulance ,inc:Incident
(•a.Available ∧a.Mobilised ∧✸(Intervention (a,inc)))
⇒ /circlecop†rt(a.Mobilised W(a.Mobilised ∧Intervention (a,inc)))(8)
∀a:Ambulance ,inc:Incident
(•a.Available ∧a.Mobilised ∧✸(Intervention (a,inc)))
⇒ /circlecop†rt(a.InService W(a.InService ∧Intervention (a,inc)))(9)
These properties state that if an available ambulance is
mobilised and eventually intervenes in an incident, then it
must remain mobilised and in service, respectively, until it
intervenes. The deﬁnitions of the ﬂuents appearing in the
goal and background properties are given below.
Resolved[inc:Incident] =/angbracketleft[inc].resolve, [inc].happens, false /angbracketright
Intervention[a:Ambulance][inc:Incident] =
/angbracketleft[a][inc].intervene,[a][inc].stop intervention, false /angbracketright
Allocated[a:Ambulance].[inc:Incident] =
/angbracketleft[a][inc].allocate,[a][inc].deallocate, false /angbracketright
Mobilised[a:Ambulance] =/angbracketleft[a].mobilise, [a].demobilise, false /angbracketright
Available[a:Ambulance] =/angbracketleft[a].assign, [a].free, true /angbracketright
InService[a:Ambulance] =/angbracketleft[a].onCall, [a].offCall, true /angbracketright
Model checking an LTS synthesised from those back-
ground properties against the goal Achieve [ MobilisedAmbu-
lanceIntervention ] results in the following counterexample.
tick Available.a1
a1.inc1.allocate Available.a1
a1.assign Available.a1
tick Available.a1
a1.mobilise Mobilised.a1
tick Mobilised.a1
a1.offCall
a1.demobilise
tick
a1.inc1.deallocate
...
tick
Cycle in terminal set:
tick
tick
The events appearing in the terminal set are the only ones
that can occur thereafter. In the above violation trace,
the goal antecedent is satisﬁed by the third tick after
which the ambulance does not intervene, i.e. the goal’s
target is not satisﬁed. A witness is generated by check-
ing the synthesised LTS against the anti-target assertion
(•a.Available ∧a.Mobilised )⇒✷≤11¬Intervention (a,inc). In
this case, the model checker produces the following witness.
tick Available.a1
a1.inc1.allocate Available.a1
a1.assigntick
a1.mobilise Mobilised.a1
tick Mobilised.a1
...
a1.inc1.intervene Mobilised.a1 && Intervention.a1.inc1
tick Mobilised.a1 && Intervention.a1.inc1
Cycle in terminal set:
tick
tick
The ambulance a1intervenes by the fourth tick. Please note
that the goal Achieve [MobilisedAmbulanceIntervention ], the
background properties including assertions (8) and (9) and
the counterexample and witness traces are automatically
encoded as a logic program. For instance, the goal Achieve
[MobilisedAmbulanceIntervention ] is encoded as the rule
holdsAt_eventually_by_tick(intervention(A,Inc),I2,11,S):-
holdsAt_tick(available(A),I1,S),
next_tick_at(I1,I2,S),
holdsAt_tick(mobilised(A),I2,S),
not obstructed_always_by_tick(intervention(A,Inc),I2,11,S).
The learning task aims to generate conditions
explaining why holdsAt_eventually_by_tick(interven-
tion(a1,inc1),5,11,c) does not hold in the positive
example but is true in the negative one. The learning
outcome is guided by the mode declaration provided
to the learning system. In this example, the mode
declaration is restricted to learn rules with the predicate
obstructed_always_by_tick in the head. The ILP tool
produces a number of possible obstacles for the goal
Achieve [MobilisedAmbulanceIntervention ] which include:
Obstacle MobilisedAmbulanceStopsService :
∃a:Ambulance ,inc:Incident
✸(•a.Available ∧a.Mobilised ∧✷≤11¬a.InService ) (10)
Obstacle MobilisedAmbulanceDeallocated :
∃a:Ambulance ,inc:Incident
✸(•a.Available ∧a.Mobilised ∧✸≤11¬Allocated (a,inc)) (11)
Obstacle AmbulanceMobilisationRetracted :
∃a:Ambulance ,inc:Incident
✸(•a.Available ∧a.Mobilised ∧/circlecop†rt ¬ a.Mobilised ) (12)
A comparison of the above outcome with the obsta-
cles generated manually in [4] revealed that our method
is capable of computing obstacles produced in [4] by
instantiation of the non-persistence pattern, e.g., the ﬁrst
and last obstacles above cover the obstacles Mobilized-
AmbulanceStopsServiceBeforeIntervention and Ambulance-
MobilizationRetracted , respectively. It also showed that we
were able to generate obstacles not obtained through pattern
instantiations but related to real obstacles reported in the
inquiry report [6] where ambulances did not intervene in
incidents because of incorrect allocation of ambulances.
C. Obstacles to Unbounded Achieve Goals
We now consider the goal Achieve [IncidentResolvedBy-
Intervention ] speciﬁed as follows.
Goal Achieve [IncidentResolvedByIntervention ]
∀a:Ambulance ,inc:Incident
Intervention (a,inc)⇒✸inc.Resolved(13)Among the background properties BP, the following provide
necessary conditions for the target inc.Resolved and for
treating a patient at an incident location.
∀p:Patient,inc:Incident
inc.Resolved ⇒(Injured(p,inc)→TreatedAtLocation (p,inc))(14)
∀p:Patient,inc:Incident
inc.Resolved ⇒(Injured(p,inc)→p.AdmittedToHospital )(15)
∀p:Patients,inc:Incident.TreatedAtLocation (p,inc)⇒
(∀r:Resource CriticallyNeeds (p,r)→UsedOn(r,p))(16)
BPalso includes the ﬂuent deﬁnitions:
AdmittedToHospital[p:Patient] =/angbracketleft[p].admit, [p].discharge, false /angbracketright
Injured[p:Patient][i:Incident] =
/angbracketleft[p][i].isInjured,[p][i].isRecovered, false /angbracketright
TreatedAtLocation[p:Patient][i:Incident] =
/angbracketleft[p][i].isTreated,[p][i].ﬁnishTreatment, false /angbracketright
CriticallyNeeds[p:Patient][r:Resource] =/angbracketleft[p][r].needs,[p][r].use, false /angbracketright
UsedOn[r:Resource][p:Patient] =/angbracketleft[r][p].use,[r][p].not use, false /angbracketright
Model checking L(BP)against the goal results in the
following counterexample trace.
tick
p1.inc1.isInjured
a1.inc1.allocate
a1.assign
tick
a1.mobilise
tick
a1.inc1.intervene Intervention.a1.inc1
tick Intervention.a1.inc1
a1.inc1.stop_intervention
a1.inc1.deallocate
a1.free
tick
Cycle in terminal set:
tick
tick
In this violation trace, an ambulance intervenes but the in-
cident is never resolved.The background properties are then
veriﬁed against the anti-target assertion Intervention (a,inc)⇒
✷¬inc.Resolved which yields the following witness trace.
tick
p1.inc1.isInjured
p1.r1.needs
a1.inc1.allocate
assign.a1
tick
a1.mobilise
tick
a1.inc1.intervene Intervention.a1.inc1
tick Intervention.a1.inc1
r1.p1.use
p1.inc1.isTreated
tick Intervention.a1.inc1
admit.p1
inc1.resolve Resolved.inc1
tick Resolved.inc1
Cycle in terminal set:
tick Resolved.inc1
tick Resolved.inc1
This witness illustrates an incident resolved eventually after
an ambulance’s intervention. The background properties
including domain properties (14), (15) and (16), the goal
Achieve [IncidentResolvedByIntervention ] and the coun-
terexample and witness traces are automatically encoded as
a logic program. For example, the goal [ IncidentResolved-
ByIntervention ] is represented by the following rule.holdsAt_eventually_tick(resolved(Inc),I,S):-
holdsAt_tick(intervention(A,Inc),I,S),
not obstructed_always_tick(resolved(Inc),I,S).
The predicate holdsAt_eventually_tick(f,i,s) means
that ﬂuent fis true at some tick in the future after time-
pointiin tracesunless an obstacle that always prevents it
from being true holds. The ILP tool suggests the following
possible obstacles.
Obstacle IncidentNotResolvedByIntervention
✸∃a:Ambulance ,inc:Incident
(Intervention (a,inc)∧✷¬Resolved(inc)) (17)
Obstacle PatientNotAdmittedToHospital
✸∃a:Ambulance ,inc:Incident
(Intervention (a,inc)∧
∃p:Patient Injured (p,inc)∧✷¬p.AdmittedToHospital )(18)
Obstacle PatientNotTreatedAtIncidentLocation
✸∃a:Ambulance ,inc:Incident
(Intervention (a,inc)∧(∃p:Patient
Injured(p,inc)∧✷¬TreatedAtLocation (p,inc)))(19)
Obstacle CriticalCareNotGivenToPatient
✸∃a:Ambulance ,inc:Incident
(Intervention (a,inc)∧(∃p:Patient,r:Resource
Injured(p,inc)∧✷¬CriticallyNeeds (p,r)))(20)
Obstacle ResourceNotUsedOnPatient
✸∃a:Ambulance ,inc:Incident
(Intervention (a,inc)∧(∃p:Patient,r:Resource
Injured(p,inc)∧✷¬UsedOn(r,p)))(21)
Obstacle AmbulanceNotAvailableAfterIntervention
✸∃a:Ambulance ,inc:Incident
(Intervention (a,inc)∧✸¬a.available ) (22)
By comparing the output of the learning system with those
identiﬁed in [4], we ﬁnd that the ﬁrst obstacle corresponds to
the high-level obstacle IncidentNotResolvedByIntervention
obtained through manual regression in [4]. The obstacles
(18) and (19) are sub-obstacles for IncidentNotResolved-
ByIntervention obtained from the domain properties. The
tool also produced obstacles at a ﬁner level such as Re-
sourceNotUsedOnPatient . Note that this obstacle suggests
that resources may be used on patients who do not need it.
This may trigger the addition of a missing domain property
that states that a necessary condition for using a resource on
a patient is that the patient critically needs it.
VI. D ISCUSSION AND RELATED WORK
Our applications to medium-sized case studies indicate
that the techniques used are scalable. In general, the scal-
ability of the approach is determined by the scalability of
the model checker and learning tool used. The scalability
of model checker is affected by the number of goals and
composed LTSs, one for each domain property, per iteration.
Since the approach is incremental, it takes one goal at a
time and ﬁnds all its obstacles. Hence we do not discuss an
upper bound on the number of goals handled. The number
of domain properties provided per iteration is dependent on
the necessary conditions for the goal’s target condition to besatisﬁed and is assumed to be small. As for the ILP system, it
uses an Answer Set Programming solver [20] which grounds
the logic programs before searching for solutions. It scales
for ﬁnite domains and is comparable to SAT-solvers. The
complexity of translating each assertion to an automata or a
logic program is exponential in the size of the FLTL formula.
As these correspond to individual goals or domain properties
for goals’ target conditions, they remain small enough to be
handled by the tool.
Obstacles were originally introduced as informal goal
obstruction scenarios in [21], used in [3] for requirements
elaboration. Heuristics for informally identifying possible
exceptions and errors in such scenarios are proposed in [22].
Obstacles as goal obstruction preconditions were introduced
in [23] where various techniques for obstacle analysis are
outlined. Lutz and colleagues discuss a convincing appli-
cation of obstacle analysis to identify anomaly-handling
requirements for a NASA unmanned aerial vehicle [24]. The
DDP approach to risk analysis also integrates a lightweight
form of obstacle analysis [2]. The integration of hazard
analysis in the early stages of the RE process is advocated
in [25]. Iterative approaches for defect-driven modiﬁcation
of requirements speciﬁcations in the context of system faults
and human errors are proposed in [26], [27]. Fault trees and
threat trees are discussed as special types of risk trees in
[28] and [29], respectively.
The approach presented here is much related to that
described in [4] where techniques are proposed for iden-
tifying and resolving obstacles to goals. In [4] obstacles
are generated from goal negations and domain properties by
applying a formal regression calculus or by using obstruction
patterns. Though their techniques are sound, the processes
are manual; the pattern-based technique is restricted to
patterns available in the catalogues. Our approach comple-
ments the obstacle identiﬁcation process by (a) providing
automated support for generating obstacles from given goals
and domain properties and (b) computing a wider class of
obstacles that cannot be identiﬁed through pattern-based
techniques. However, our approach does not address the
various obstacle resolution options studied there.
Our approach builds upon the work presented in [5], [30]
where model checking and ILP are used for the elaboration
of speciﬁcations operationalising goals and the derivation
of non-zeno behaviour models, respectively. However the
technique here involves several challenges that were not
addressed in [5], [30]. The witnesses are here generated
without human intervention. The role of counterexamples
and witnesses is inverted as counterexamples are consid-
ered to be positive examples for learning obstacles while
witnesses are considered as negative examples. The goal
language is much more expressive as it covers any Achieve
orMaintain/Avoid goal which were not covered in [5].
The approach presented here is also somewhat related to
the technique described in [31]. The authors in [31] use areasoning technique called abduction by refutation to detect
a complete set of counterexamples for invariants expected
to hold for a given system description. The output of the
approach is a set of transition relations that show where
an invariant does not hold. Such transitions, however, are
not guaranteed to be reachable from the initial state of the
description and hence it is assumed that reachability checks
are performed separately. Also, the invariant structure is
limited to the form (C→T). Our approach has several
advantages: (a) it guarantees that any detected obstacle is
indeed reachable from the initial state, (b) the obstacles are
produced in the goal language and (c) this language is richer
and more expressive than the one applicable in [31].
Our mechanism for encoding goals to learn obstacles is
related to the formalisation of abnormality-relations in logic
programming [32]. In the latter, abnormality-relations are
used to revise an agent’s default assumptions. Similarly,
our use of obstruction predicates in the body of goal rules
provides the means for revising the engineer’s default as-
sumption about when the goal’s target holds in the domain.
VII. C ONCLUSION AND FUTURE WORK
The framework described in this paper is a formal, tool-
supported approach for incrementally generating obstacles
that may prevent goals from being achieved within a given
domain. It uses model checking for generating traces that
violate and satisfy the goals from a given system description,
and an inductive learning tool for computing obstacles from
these traces. This iterative process ends once a domain-
complete set of obstacles is generated. We applied the
proposed approach to a real safety-critical system, the
London Ambulance Service where a domain-complete set
of obstacles was computed. Our application has shown an
improvement over existing methods through automation and
detection of a wider class of obstacles.
We envision a number of extensions to the work presented
here. First, we will investigate enriching the framework with
incremental techniques for resolving the detected obstacles.
For this purpose, we plan to revisit the learning phase of
the approach and apply ILP algorithms for theory revision.
We further plan to extend the framework to handle goals
and obstacles which are associated with probabilities. To do
so, the use of tools such as probabilistic model checking
and probabilistic ILP will be explored. As part of our future
work, we intend to adapt our approach to detect conﬂicts
among goals. One way we envision this being possible is
by learning boundary conditions for goal conﬂict from traces
generated through deadlock checks.
ACKNOWLEDGMENT
We are grateful to Bernard Lambeau and the reviewers for
their careful inspection and feedback on an earlier version of
the paper. This work is ﬁnancially supported by ERC project
PBM - FIMBSE (No. 204853).REFERENCES
[1] A. van Lamsweerde, Requirements Engineering: From System
Goals to UML Models to Software Speciﬁcations . Wiley,
2009.
[2] M. Feather and S. Cornford, “Quantitative risk-based require-
ments reasoning,” J. Requirements Eng. , vol. 8, pp. 248–265,
2003.
[3] A. Ant ´on and C. Potts, “The use of goals to surface re-
quirements for evolving systems,” in Proc. of Intl. Conf. on
Softw. Eng. , 1998, pp. 157–166.
[4] A. van Lamsweerde and E. Letier, “Handling obstacles
in goal-oriented requirement engineering,” IEEE Trans. on
Softw. Eng. , vol. 26, no. 10, pp. 978–1005, 2000.
[5] D. Alrajeh, J. Kramer, A. Russo, and S. Uchitel, “Learning
operational requirements from goal models,” in Proc. of 31st
Intl. Conf. on Softw. Eng. , 2009, pp. 265–275.
[6] 1993, report on the Inquiry Into the London Ambulance
Service. The Communications Directorate, South West
Thames Regional Authority. See also the London
Ambulance System home page, http:// hsn.lond-
amb.sthames.nhs.uk/http.dir/service/organisation/featurs/
info.html.
[7] D. Giannakopoulou and J. Magee, “Fluent model checking
for event-based systems,” in Proc. 11th Symposium on Foun-
dations Softw. Eng. , 2003, pp. 257–266.
[8] Z. Manna and A. Pnueli, The Temporal Logic of Reactive and
Concurrent Systems . Springer, 1992.
[9] R. Koymans, Specifying Message Passing and Time-Critical
Systems with Temporal Logic , ser. (LNCS). Springer, 1992,
vol. 651.
[10] J. Magee and J. Kramer, Concurrency : State Models and
Java Programs . John Wiley and Sons, 1999.
[11] S. Muggleton and L. De Raedt, “Inductive Logic Program-
ming: Theory and Methods,” J. of Log. Program. , vol. 19,20,
pp. 629–679, 1994.
[12] J. Lloyd, Foundations of logic programming . Springer, 1984.
[13] M. Gelfond and V . Lifschitz, “The stable model semantics for
logic programming,” in Proc. of 5th Intl. Conf. on Log. Pro-
gram. , 1988, pp. 1070–1080.
[14] R. Kowalski and M. Sergot, “A logic-based calculus of
events,” New generation computing , vol. 4, no. 1, pp. 67–95,
1986.
[15] A. Finkelstein and J. Dowell, “A comedy of errors: the london
ambulance service case study,” in Proc. of 8th Intl. Workshop
on Software Speciﬁcation and Design , 1996, pp. 2–4.
[16] E. Letier, “Reasoning about agents in goal-oriented require-
ments engineering,” Ph.D. dissertation, Universit ´e Catholique
de Louvain, D ´ept. Ing ´enierie Informatique, Louvain-la-
Neuve, Belgium, 2001.[17] E. Letier, J. Kramer, J. Magee, and S. Uchitel, “Deriving
event-based transitions systems from goal-oriented require-
ments models,” J. Automated Softw. Eng. , vol. 15, pp. 175–
206, 2008.
[18] O. Ray, “Nonmonotonic abductive inductive learning,” J. of
Applied Logic , vol. 7, no. 3, pp. 329–340, 2009.
[19] D. Corapi, A. Russo, and E. Lupu, “Inductive logic pro-
gramming as abductive search,” in Tech. Comm. of 26th
Intl. Conf. on Log. Program. , ser. (LIPIcs), vol. 7, 2010, pp.
54–63.
[20] T. Syrj ¨anen and I. Niemel ¨a, “The smodels system,” in
Log. Prog. and Nonmotonic Reasoning , ser. LNCS, 2001, vol.
2173, pp. 434–438.
[21] C. Potts, “Using schematic scenarios to understand user
needs,” in Proc. of 1st conf. on Designing interactive systems:
processes, practices, methods, & techniques , 1995, pp. 247–
256.
[22] A. Sutcliffe, N. Maiden, S. Minocha, and D. Manuel,
“Supporting scenario-based requirements engineering,” IEEE
Trans. on Softw. Eng. , vol. 24, pp. 1072–1088, December
1998.
[23] A. van Lamsweerde and E. Letier, “Integrating obstacles
in goal-driven requirements engineering,” in Proc. of 20th
Intl. Conf. on Softw. Eng. , 1998, pp. 53 –62.
[24] R. Lutz, A. Patterson-Hine, S. Nelson, C. R. Frost, D. Tal, and
R. Harris, “Using obstacle analysis to identify contingency
requirements on an unpiloted aerial vehicle,” J. Requirements
Eng., vol. 12, pp. 41–54, 2006.
[25] N. Leveson, “An approach to designing safe embedded soft-
ware,” in Proc. 2nd Intl. Conf. on Embedded Software , 2002,
pp. 15–29.
[26] T. Anderson, R. de Lemos, and A. Saeed, “Analysis of safety
requirements for process control systems,” in Predictably
Dependable Computing Systems , 1995, pp. 42–53.
[27] R. de Lemos, B. Fields, and A. Saeed, “Analysis of safety
requirements in the context of system faults and human
errors,” in Proc. of IEEE Intl. Symposium and Workshop
on Systems Engineering of Computer Based Systems , 1995,
pp. 374–381.
[28] N. Leveson, Safeware - System Safety and Computers .
Addison-Wesley, 1995.
[29] B. Schneier, Secrets and Lies: Digital Security in a Networked
World . Wiley, 2000.
[30] D. Alrajeh, J. Kramer, A. Russo, and S. Uchitel, “Deriving
non-zeno behaviour models from goal models using ILP,”
J. Form. Asp. Comput. , vol. 22, pp. 217–241, 2010.
[31] A. Russo, R. Miller, B. Nuseibeh, and J. Kramer, “An
abductive approach for analysing event-based requirements
speciﬁcations,” in Proc. of 18th Intl. Conf. Log. Program. ,
2002, vol. 2401, pp. 69–105.
[32] K. Apt and R. Bol, “Logic programming and negation: A
survey,” J. of Log. Program. , vol. 19/20, pp. 9–71, 1994.