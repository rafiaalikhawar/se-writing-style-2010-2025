SEDGE: Symbolic Example Data Generation
for DataÔ¨Çow Programs
Kaituo Li, Christoph Reichenbachy, Yannis Smaragdakisz, Yanlei Diao, Christoph Csallnerx
Computer Science Department, University of Massachusetts, Amherst, USA
yInstitute of Informatics, Goethe University Frankfurt, Germany
zDepartment of Informatics, University of Athens, Greece
xComputer Science and Engineering, University of Texas at Arlington, USA
Abstract ‚ÄîExhaustive, automatic testing of dataÔ¨Çow (esp. map-
reduce) programs has emerged as an important challenge. Past
work demonstrated effective ways to generate small example data
sets that exercise operators in the Pig platform, used to generate
Hadoop map-reduce programs. Although such prior techniques
attempt to cover all cases of operator use, in practice they often
fail. Our SEDGE system addresses these completeness problems:
for every dataÔ¨Çow operator, we produce data aiming to cover all
cases that arise in the dataÔ¨Çow program (e.g., both passing and
failing a Ô¨Ålter). SEDGE relies on transforming the program into
symbolic constraints, and solving the constraints using a symbolic
reasoning engine (a powerful SMT solver), while using input data
as concrete aids in the solution process. The approach resembles
dynamic-symbolic (a.k.a. ‚Äúconcolic‚Äù) execution in a conventional
programming language, adapted to the unique features of the
dataÔ¨Çow domain.
In third-party benchmarks, SEDGE achieves higher coverage
than past techniques for 5 out of 20 PigMix benchmarks and 7
out of 11 SDSS benchmarks and (with equal coverage for the
rest of the benchmarks). We also show that our targeting of the
high-level dataÔ¨Çow language pays off: for complex programs,
state-of-the-art dynamic-symbolic execution at the level of the
generated map-reduce code (instead of the original dataÔ¨Çow
program) requires many more test cases or achieves much lower
coverage than our approach.
I. I NTRODUCTION
DataÔ¨Çow programming has emerged as an important data
processing paradigm in the area of big data analytics. DataÔ¨Çow
programming consists of specifying a data processing program
as a directed acyclic graph. Internal nodes of the graph
represent operations on the data, for example, using relational
algebra primitives such as Ô¨Ålter, project, and join, or functional
programming primitives such as ‚Äúmap‚Äù applications of user-
deÔ¨Åned local functions and ‚Äúreduce‚Äù operations that collect
values over sets of data. The edges in the graph represent
data tables or Ô¨Åles passed between operators (nodes) in the
graph. Many recently proposed data processing languages and
systems, such as Pig Latin [16], DryadLINQ [11], and Hyrack-
s/Asterix [1] resemble dataÔ¨Çow programming on datasets of
enormous sizes. A user can develop dataÔ¨Çow programs by
either writing the programs directly using the above languages
or compiling queries written in declarative languages such as
SQL and Hive [21].
When a user writes a dataÔ¨Çow programs, he/she will
typically employ example data or test cases to validate it.
Validating with large real data is impractical, both for reasonsof efÔ¨Åciency (running on large data sets takes a long time)
and for reasons of ease-of-validation (it is hard to tell whether
the result is what was expected). One alternative is to sample
the real data available. The sample data need to thoroughly
exercise the program, covering all key behavior of each
dataÔ¨Çow operator. This is very hard to achieve via random
sampling, however. For instance, equi-joining two sample data
tables of small size is likely to produce an empty result, if the
values being joined are distributed arbitrarily.
Another alternative is to synthesize representative data. Such
data synthesis is complicated by the complexity of dataÔ¨Çow
language operators as well as by the presence of user-deÔ¨Åned
functions. Current state-of-the-art in example data generation
for dataÔ¨Çow programs [16] is of limited help. Such tech-
niques can generate high-coverage data for dataÔ¨Çow programs
with simple constraints. However, for dataÔ¨Çow programs with
complex constraints, e.g., with numerous Ô¨Ålters, arithmetic
operations, and user-deÔ¨Åned functions, the generated data are
incomplete due to shortcomings in constraint searching and
solving strategies.
In this paper, we address the problem of efÔ¨Åcient example
data generation for complex dataÔ¨Çow programs by bringing
powerful symbolic reasoning to bear on the process of sample
data generation. We present the Ô¨Årst technique and system
for systematically generating representative example data us-
ing dynamic symbolic execution (DSE) [7], [10], [22] of
dataÔ¨Çow programs. Our concrete setting is the popular Pig
Latin language [17]. Our DSE technique analyzes the program
while executing it using sampled data, determines whether
the sampled input data are complete, and, if not, attempts to
synthesize input tuples that result in the joint sampled and
synthesized data being a complete example data set for the
program.
We have implemented this approach in S EDGE , short for
Symbolic Example Data GEneration . SEDGE is a reimple-
mentation of the example generation part in the Apache
Pig dataÔ¨Çow system, which currently implements the closest
comparable past research, by Olston et al. [16].
Illustration: For a simple demonstration, consider an ap-
plication scenario in computational astrophysics. We surveyed
11 queries in the Sloan Digital Sky Survey1for analyzing star
1http://skyserver.sdss.org/public/en/help/docs/realquery.aspand galaxy observations, and rewrote them using the Pig Latin
language. The most complex query contains 34 Ô¨Ålters and 2
joins. For ease of exposition, we show a simple example query
in Listing 1 by combining features from two actual queries and
will use it as a running example in the paper. (For more details
on the real queries, see the evaluation section.)
1A = LOAD ‚ÄôfileA‚Äô using PigStorage() AS
2(name:chararray, value:int);
3B = LOAD ‚ÄôfileB‚Äô using PigStorage() AS
4(u:double, class:int);
5C = FILTER A BY value < 100 AND value >= 0;
6D = FILTER B BY math.POW(u,2.0) > 0.25;
7E = JOIN C ON value, D ON class;
Listing 1. An example Pig Latin program
The program begins by loading tables A and B from Ô¨Åles
containing measurements. Both kinds of measurements need
to be Ô¨Åltered. The Ô¨Årst Ô¨Ålter keeps only measurements in a
certain value range and the second Ô¨Ålters low uvalues. The
tuples that survive the Ô¨Åltering get joined.
Imagine that we execute the program for a small number of
sampled input tuples from fileA andfileB . If we want to
achieve perfect coverage on random sampling of actual data
alone, we are unlikely to be successful if the sample is small.
The data from the two tables need to pass Ô¨Ålters and (even
more unlikely) have their value andclass Ô¨Åelds coincide.
This is a case where targeted test data generation can help.
Past techniques for example data generation cannot handle
this example well. Olston et al.‚Äôs technique [16] will synthesize
data by considering operators one-by-one in reverse order in
the Pig Latin program. It will attempt to create data to satisfy
the JOIN Ô¨Årst, without concern for the FILTER conditions that
the same data have to satisfy. This will likely fail to satisfy
even the Ô¨Årst FILTER operator: the range 0-99 will have to be
hit purely by chance. The problem for the Olston technique is
thatvalue is not a free variable once the JOIN constraint is
satisÔ¨Åed: it is limited to the values that the system arbitrarily
chose in order to have the JOIN operator produce output.
Even more importantly, the second FILTER operator is hard
to process. It contains a user-deÔ¨Åned function, math.POW .
Although this function is simple, it will still befuddle an au-
tomatic test data generation system. Furthermore, an essential
part of dataÔ¨Çow programming is the ability to use user-deÔ¨Åned
functions freely, however complex these functions may be. The
large volume of work on automatic data generation in other
settings (e.g., SQL databases [3]) does not address user-deÔ¨Åned
functions.
Our approach overcomes such problems by modeling the
entire program in a powerful reasoning engine, handling
complex conditions, and dealing with user-deÔ¨Åned functions
with the aid of concrete values observed over sample data. We
process the program using a domain with (symbolic) variables,
such as value ,class , etc. A symbolic variable ‚Äú columnname ‚Äù
represents the value of one column of an input table for a set
of tuples. We start with a concrete execution of the program
using small samples of real input data. During such concrete
execution we observe, Ô¨Årst, which program cases are covered,and, second, what are the values of user-deÔ¨Åned functions for
real data. E.g., a tuple (3:3;32)of table B will register the
value pair (u: 3:3;math.POW(u,2.0) : 10:89)for the user-
deÔ¨Åned function. This value will later help when trying to
solve symbolic constraints.
After the concrete execution, our approach uses symbolic
reasoning in order to cover program cases that were not
already covered by the concrete execution. The approach
performs a symbolic execution of the program, gathering
constraints along each path to the sources.
We use the Z3 SMT solver [8] (a powerful symbolic
reasoning engine) to solve the constraints. Concrete values
for user-deÔ¨Åned functions are supplied to the solver. That
is, the user-deÔ¨Åned function is treated as a black-box and
the solver is supplied extra constraints of the form u=
3:3)math.POW( u;2:0) = 10 :89. These can aid the solver in
producing satisfying assignments. Essentially, we try to make
an educated guess: Whenever we do not know how to generate
example data for a constraint that depends on a user deÔ¨Åned
function, we can always simplify this constraint by replacing
the symbolic representation of the user-deÔ¨Åned function with
concrete values.
Contributions: In brief, the contributions of our work are
as follows:
We detail a translation of dataÔ¨Çow operators into symbolic
constraints. These constraints are subsequently solved using
a powerful SMT solver.
We adapt the technique of dynamic-symbolic execution
to the domain of dataÔ¨Çow languages. By doing so, we
exploit the unique features of this domain, thus enabling high
coverage. SpeciÔ¨Åcally, we exploit the absence of side-effects
in order to perform a multiple-path analysis: observations on
the values of a user-deÔ¨Åned function on different execution
paths can help solve constraints involving the user-deÔ¨Åned
function.
As a result of the above, we produce an example data
generation technique that achieves higher coverage than past
literature, managing to produce data that exercise all opera-
tors of a dataÔ¨Çow program. We show extensive measurements
to conÔ¨Årm our approach‚Äôs advantage. Our technique achieves
full coverage in all benchmark programs with a boost in
performance for most benchmarks.
II. B ACKGROUND AND CONTEXT
We next discuss some pertinent background on dataÔ¨Çow
programming as well as on concepts and mechanisms intro-
duced in closely related past work.
A. DataÔ¨Çow program
A dataÔ¨Çow program is a directed bipartite graph, separating
computations (i.e., operators) in one partition and compu-
tational intermediate results (i.e., data tables) in the other
partition. In other words, it is a graph in which data tables
Ô¨Çow into operators, and operators Ô¨Çow into data tables. A
data table is a collection of tuples with possible duplicates. A
tuple is typically a sequence of atomic values (integer, long,Ô¨Çoat, chararray, etc.) or complex types (tuple, bag, map). An
operator usually has some input tables and one output table.
We say that a data table is an input table of an operator in
a dataÔ¨Çow program if the data table Ô¨Çows into the operator.
Similarly, we say that a data table is an output table of an
operator in a dataÔ¨Çow program if the operator Ô¨Çows into the
data table. If operator A‚Äôs output table is one of operator B‚Äôs
input tables, A is said to be an upstream neighbor of B and
B is said to be a downstream neighbor of A. An operator
without any upstream neighbor is called a leaf operator, and
an operator without any downstream neighbor is called a root
operator‚Äîthe root operator generates the Ô¨Ånal output.
B. Pig Latin
Pig Latin is a well-known dataÔ¨Çow programming language
and the language front-end of the Apache Pig infrastructure
for analyzing large data sets. The Pig compiler translates Pig
Latin programs into sequences of map-reduce programs for
Hadoop. A Pig Latin program is a sequence of statements
where each statement represents a data transformation. In a
Pig Latin statement, an operator processes a set of input tables
and produces an output table. Following are the core operators
of Pig Latin [17].
1) LOAD: Read the contents of input data Ô¨Åles.
2) FILTER: Discard data that do not satisfy a built-in logic
predicate or a user-deÔ¨Åned boolean function.
3) COGROUP: Divide one or more sets of input tuples into
different groups according to some speciÔ¨Åcation. Each
resultant output tuple consists of a group identiÔ¨Åer and
a nested table containing a set of input tuples satisfying
the speciÔ¨Åcation.
4) GROUP: A special case of COGROUP when only one
set of input tuples is involved.
5) TRANSFORM: Apply a transformation function to input
tuples. Transformation functions include projection, built-
in arithmetic functions (e.g., incrementing a numeric
value), user-deÔ¨Åned functions, and aggregation. An ag-
gregation is implemented by Ô¨Årst invoking COGROUP or
GROUP, and then doing transformation group by group.
For example, Average ()is an aggregation that averages
the values in each group of input tuples.
6) JOIN: Equijoin tuples from two input tables.
7) UNION: Vertically glue together the contents of two input
tables into one output table.
8) FOREACH: Apply some processing to every tuple of
the input data set. FOREACH is often followed by a
GENERATE clause to pick a subset of all available Ô¨Åelds.
9) DISTINCT: Remove duplicate tuples from the input data
set.
10) SPLIT: Split out the input data set into two or more output
data sets. A condition argument determines the partition
that each tuple of the input data goes into.
11) STORE: Write the output data set to a Ô¨Åle.C. Equivalence class model
Our work tries to maximize branch coverage in Pig Latin
programs. An interesting question is what constitutes full
coverage of a Pig Latin operator. In some cases the answer is
clear: the FILTER operator, for instance, is well-covered when
its input contains tuples that satisfy the Ô¨Ålter condition and
tuples that fail the Ô¨Ålter condition. In other cases, the deÔ¨Ånition
of coverage is not as simple. For instance, do we consider a
UNION operator sufÔ¨Åciently covered if all its output tuples
come from a single input table (i.e., if one of its input tables
is empty)? The choice is arbitrary but the more reasonable
option seems to be to require that both inputs of a UNION
operator be non-empty. Furthermore, whether an operator is
covered may be more convenient to discern in some cases
by observing its input and in others by observing its output.
For instance, a JOIN is well-covered when the output is non-
empty, while a UNION is well-covered when its inputs are
both non-empty.
To specify the coverage of operators we inherit the deÔ¨Ånition
ofequivalence classes from Olston et al. [16]‚Äîthe research
work that has formed the basis of the example generation
functionality in Apache Pig. Each Pig Latin operator yields
a set of equivalence classes for either its input or its output
tuples . Equivalence classes partition the actual set of tuples‚Äî
each tuple can belong to at most one equivalence class. To
generate example data with 100% coverage, the input or output
table of each operator (when the program is evaluated with
the example data) must contain at least one tuple belonging
to each of the operator‚Äôs equivalence classes.
We summarize the equivalence class deÔ¨Ånitions for the
operators of Pig Latin below. The deÔ¨Ånitions are from Olston
et al.‚Äôs publication [16] and implementation in Apache Pig.2
LOAD/STORE/FOREACH/TRANSFORM: Every input
tuple is assigned to the same class E1. (I.e., the operator
is always covered, as long as its input is non-empty.)
FILTER: Every input tuple that passes the Ô¨Ålter is assigned
to a class E1; all others are assigned to a class E2. (The
intention is to show at least one record that passes the Ô¨Ålter,
and one that does not pass.)
GROUP/COGROUP: Every output tuple is assigned to the
same class E1. For every output tuple, the nested table for
every group identiÔ¨Åer should contain at least two tuples. (The
purpose of E1is to illustrate a case where multiple input
records are combined into a single output record.)
JOIN: Every output tuple is assigned to the same class
E1. (The intention is to illustrate a case of two input records
being joined.)
UNION: Every input tuple from one input table is assigned
toE1, tuples from the other input table are assigned to E2.
(The aim is to show at least one record from each input table
being placed into the unioned output.)
DISTINCT: Every input tuple is assigned to the same class
E1. For at least one input tuple to DISTINCT, there should be
a duplicate, to show at least one duplicate record is removed.
2http://pig.apache.org/SPLIT: Every input tuple that passes condition iis assigned
to class Ei1; input tuples that do not pass iare assigned to
a class Ei2. The number of equivalence classes of a SPLIT
depends on how many conditions the SPLIT has. If a SPLIT
hasnconditions, it yields 2nequivalence classes. (The aim
is to show, for each split condition, at least one record that
passes the condition, and one that does not pass.)
D. Quantitative Objectives
We use two metrics to describe the quality of example data
and follow earlier terminology [16]:
1) Completeness: The average of per-operator completeness
values. The completeness of an operator is the fraction of
the equivalence classes of the operator for which at least
one example tuple exists. An ideal algorithm should make
example data exist for every equivalence class of every
operator in a Pig Latin program.
2) Conciseness: The average of per-operator conciseness
values. The conciseness of an operator is the ratio of
the number of operator equivalence classes to the total
number of different example tuples for the operator (with
a ceiling of 1). An ideal algorithm should use as few
example tuples as possible to illustrate the semantics of
an operator.
The completeness metric is clearly a metric of cover-
age, as deÔ¨Åned earlier. SpeciÔ¨Åcally, it corresponds to branch
coverage in the program analysis and software engineering
literature. Branch coverage counts the percentage of control-
Ô¨Çow branches that get tested.
III. S EDGE DESIGN
Our system, S EDGE , uses a three-step algorithm to generate
example data in Pig Latin programs.
(1) Downstream Propagation: execute programs using sam-
pled real data, record values of user-deÔ¨Åned functions‚Äî
see Section III-B;
(2) Pruning Pass: eliminate redundant data so that each cov-
ered equivalence class only contains a single member;
(3) Upstream Pass: generate constraints and synthesize data
for equivalence classes that the sampled test data do not
explore by performing DSE.
The last pass (upstream pass) it the key new element of our
approach and is described next.
A. Constraint Generation
The essence of our approach is to represent equivalence
classes symbolically and to produce symbolic constraints that
describe the data tuples that belong in each equivalence class.
Solving the constraints (i.e., producing data that satisfy them)
yields our test inputs. Our constraint generator steps through
the dataÔ¨Çow graph to compute all equivalence classes for each
Pig Latin operation, starting at root (i.e., Ô¨Ånal) operators. We
assume that each root operator is of the form STORE W,
without loss of generality (the analysis enters dummy nodes
of this form when they are implicit). Similarly we assume that
all variable names in our program are unique.We represent the set of constraints (one for each equivalence
class) of a statement V =: : :asC(V). We consider two kinds
of equivalence classes: terminating equivalence classes, which
represent paths of tuples that end at a given operator (e.g.,
Ô¨Åltered out), and binding equivalence classes, which represent
paths through which tuples continue downstream.
For illustration, consider our running example, reproduced
here for ease of reference.
A = LOAD ‚ÄôfileA‚Äô using PigStorage() AS
(name:chararray, value:int);
B = LOAD ‚ÄôfileB‚Äô using PigStorage() AS
(u:double, class:int);
C = FILTER A BY value < 100 AND value >= 0;
D = FILTER B BY math.POW(u,2.0) > 0.25;
E = JOIN C ON value, D ON class;
Here, our root node consumes variable E. Our analysis
considers Eas if it were Ô¨Çowing upstream from a STORE
operation. We invent a symbolic name, P, for the (single)
equivalence class induced by the STORE. Its constraint is
satisÔ¨Åed by all tuples:
C(E)fPg;where8t:P(t)
Note the use of. A dataÔ¨Çow node could receive constraints
from several operators (both its upstream and downstream
neighbors) so our equivalence class inference is using subset
reasoning: we know that C(E)includes at least P, but it could
include other equivalence classes as well. (In this example it
does not.)
C(E)is then propagated to the JOIN statement that con-
structs E. JOINs require tuples to agree on particular Ô¨Åelds
(value andclass , here), so we enforce this property by
encoding it in our constraints:
for all P2C(E) :9Ex:
C(C)fP0
valueg
C(D)fP0
classg
where P0
value (t)P(t)^t:value =Ex
andP0
class (t)P(t)^t:class =Ex
(Note that ‚Äúfor all‚Äù refers to iteration that generates multiple
constraints, where as the 9quantiÔ¨Åer is part of the generated
constraint.) In our example, C(E)contains only one equiv-
alence class, hence C(C)andC(D)also end up with one
equivalence class each.
Continuing the propagation process, we pass the above
constraints on to the FILTER operators of our example. For
instance, consider the statement D = FILTER : : :, which elim-
inates all elements for which math.POW(u,2.0) >0:25does
not hold. This statement Ô¨Årst introduces a binding equivalence
class for each of the equivalence classes Ô¨Çowing upstream
viaC(D). The statement also introduces a single terminating
equivalence class ( P:) to capture the case of tuples that do
not pass the Ô¨Ålter:
C(B)fP:g
where P:(t):(math.POW( t:u ,2.0) >0:25)
and for all P2C(D)
C(B)fP0g
where P0(t)P(t)^math.POW( t:u ,2.0) >0:25Pig Latin code Equivalence class constraints Cardinality constraints
STORE A C(A)fPg;where8t:P(t) #T(P)1
A = FILTER B BY Q C(B)fP:g, where P:(t):[Q]b(t):
for all P2C(A) :C(B)fP0g
where P0(t)P(t)^[Q]b(t)#T(P0
:)1
#T(P0)#T(P)
A = UNION B, C for all P2C(A) :C(B)fPgandC(C)fPg
A = JOIN B BY x, C BY yfor all P2C(A) :9Af:C(B)fP0
xgandC(C)fP0
yg
where P0
x(t)P(t)^t:x=Af
andP0
y(t)P(t)^t:y=Af#T(P0
x)#T(P)
#T(P0
y)#T(P)
A = DISTINCT Bfor all P2C(A) :9At:C(B)fP0g
where P0(t)P(t)^tAt#T(P0)1 + #T(P)
A = GROUP B BY xfor all P2C(A) :9Af:C(B)fP0g
where P0(t)P(t)^t:x=Af#T(P0)1 + #T(P)
Fig. 1. Summary of representative translations from Pig Latin statements into equivalence classes, manifested as constraints. The above constraints are all
binding constraints, except for the terminating P:in FILTER, and for P0in DISTINCT which is both terminating and binding. In the above, []btranslates
boolean Pig expressions into our term language, and T(P)is the set of sample tuples for constraint P. Every rule introduces fresh symbolic names for
equivalence classes, we use fresh variables Afto refer to individual values, and Atto refer to tuples.
In our representation, we have preserved the user-deÔ¨Åned
function math.POW as an example of a function that the
theorem prover cannot handle directly (see Section III-B).
We handle the other FILTER statement similarly and reach
the LOAD statement which completes the analysis. The result-
ingCsets contain symbolic names for all equivalence classes
and our symbolic constraints can be used to deÔ¨Åne members
of these classes.
Table 1 gives the general form of our reasoning for repre-
sentative constructs (also including DISTINCT statements and
cardinality constraints, discussed below). For all operators for
which our Ô¨Årst two analysis passes observed insufÔ¨Åcient cover-
age, we collect constraints using the above scheme to generate
the constraints Pthat represent each insufÔ¨Åciently covered
equivalence class. For each Pwe attempt to add elements to
its corresponding set of samples T(P). We synthesize such
tuples tas follows:
1) Pass Pto the theorem prover and query for witnesses for
the existentially qualiÔ¨Åed Ô¨Åelds. If there are no witnesses,
abort; either the equivalence class is empty/not satisÔ¨Åable
due to conÔ¨Çicting requirements, or the theorem prover
lacks the power to synthesize a representative tuple.
2) Otherwise, extract the witnesses into tuple t0.
3) For any Ô¨Åeld f required by the type constraints over tinP,
extract t2:f from randomly chosen t2from our observed
samples. Combine t0with all the t2:fintot00.
4) For any still-missing Ô¨Åelds (i.e., if no matching t2exists),
Ô¨Åll the Ô¨Åeld with randomly synthesised data, yielding t.
5) If t2T (P)already, repeat the previous two steps as
needed, otherwise insert tintoT(P).
As the last steps (and Table 1) show, there is another
dimension in our sample generation, namely generating the
right amount of sample data. SpeciÔ¨Åcally, recall that our
binding equivalence class for
F = GROUP B BY x
requires at least two tuples. To capture this constraint, we
permit constraints on the cardinality of our sets of witness
tuples, notation #T(P0)2, where predicate P0representsthe binding equivalence class in the above. All such constraints
are greater-than-or-equal constraints, and we always pick the
minimum cardinality that satisÔ¨Åes all constraints.
Another subtlety of our constraint notation comes from the
DISTINCT statement, as in
G = DISTINCT B
This statement eliminates duplicate tuples. Since set semantics
have no notion of duplicates, we extend all of our tuples with
a unique identity Ô¨Åeld that does not occur in the Pig program.
We write t1t2iff the tuples t1andt2have the same Ô¨Åelds,
ignoring the identity Ô¨Åeld. The difference between our t1=t2
andt1t2is analogous to reference comparison ( t1 ==
t2) and value comparison ( t1.equals(t2) ).
To support aggregation operations in sample synthesis,
we further permit reasoning about our sampled tuples. For
example, Pig Latin allows us to write
A = LOAD ...
sum = SUM(A.x)
B = FILTER A BY count == sum
We translate aggregations such as sum = SUM(A.x) into ag-
gregations over our sets of samples. Whenever we synthesize
samples for one of A‚Äôs binding equivalence classes, e.g.
represented by P, we simply set sum =P
t2T(P)t:x. The
translation is analogous for other aggregators (A VG, MAX,
etc.). Aggregators enforce #T(P)2.
B. User-deÔ¨Åned function concretization
In earlier sections, we classiÔ¨Åed our approach as dynamic-
symbolic , following other similar work in different settings
[7], [10], [19], [22]. The important aspect of a dynamic-
symbolic execution approach to test generation is that dynamic
(i.e., concrete) observations are used to help the symbolic
solving process. The foremost aspect where this beneÔ¨Åt is
apparent in our setting is when dealing with user-deÔ¨Åned
functions (UDFs). A user-deÔ¨Åned function is any (side-effect-
free) operator that has a deÔ¨Ånition external to the language.
In the Pig Latin world, this typically means a Java functionpublic class HASH extends EvalFunc<Integer> {
public Integer exec(Tuple input) {
if(input == null || input.size() == 0)
return null ;
Integer y = (Integer) input.get(0);
int hash = y *(y+3);
return hash % 60;
} // ..
}
Fig. 2. The implementation of function HASH.
used to process values, e.g., in a FILTER. What a dynamic-
symbolic execution engine can do is to treat a UDF as a black-
box function. Inside a constraint, a use of a UDF is replaced
by a set of function values from the concrete semantics, under
the assumption that some invocations of UDFs (and return
values thereof) have already been observed.
Consider the example Pig Latin program shown in Listing 2.
Our objective is to generate complete example data with one
tuple passing and one tuple not passing the FILTER. This
program‚Äôs key step is the application of the UDF HASH to
perform Ô¨Åltering, which takes an integer as argument and
returns its hash value.
1A = LOAD ‚ÄôfileA‚Äô using PigStorage() AS
2(x:int, y:int);
3B = FILTER A BY x == HASH(y) AND x > 50;
Listing 2. Example Pig Latin program calling user-deÔ¨Åned function HASH .
A simpliÔ¨Åed implementation of HASH is shown in Figure 2.
In this implementaion, HASH extends the EvalFunc class
(which is required by Pig Latin to construct Java user-deÔ¨Åned
functions3).
Assume that we run the program with two input tu-
ples (33, 42) and (47, 19) that do not pass the FILTER
(since 33 and 47 are not the hash values of 42 and
19, respectively). On these two executions, we obtain
two evaluations of HASH: y = 42, HASH(y) = 30 and
y = 19, HASH(y) = 58. For our technique to have 100%
completeness, we need to generate example data for A(x,y)
such that x == HASH(y) && x >50. Using the two eval-
uations of HASH, we construct two simpliÔ¨Åed versions
of the constraint: x == 30 && y == 42 && x >50 and
x == 58 && y == 19 && x >50. In the simpliÔ¨Åed con-
straints the function call HASH(y) has been concretized to
the observed values (30 and 58, respectively). The second
simpliÔ¨Åed constraint is satisÔ¨Åable while the Ô¨Årst is not. Using
the satisfying assignment, we derive a new example input
(58, 19) for A(x,y).
Thus, our approach records concrete values for UDFs during
the downstream pass, concretizes constraints, and solves them
using automatic constraint solvers, in the upstream pass. We
use uninterpreted functions to encode a concretized constraint.
An uninterpreted function (UF) [5], [6] is a black box with no
3See Pig‚Äôs implementation guide for user-deÔ¨Åned functions at http://pig.
apache.org/docs/r0.9.2/udf.htmlsemantic assumptions other than the obligation that it behave
functionally: equal parameters yield equal function values. To
encode UDFs as uninterpreted functions for our constraint
solver, we supply concrete observations as implications, us-
ing the if-then-else operator ( ite) over boolean formulas and
concrete values.
Consider the example of Listing 2 again, in which we
need to Ô¨Ånd assignments to (x,y) to satisfy the constraint
x == HASH(y) && x >50. We supply the constraint solver
Z3 with concrete observations on the HASH UDF by the
following commands:
1(declare-const x Int)
2(declare-const y Int)
3(define-fun HASH ((x!1 Int)) Int
4 (ite (= x!1 42) 30
5 (ite (= x!1 19) 58
6 0)))
7(assert (not (= (HASH y) 0)))
8(assert (= (HASH y) x))
9(assert (> x 50))
The Ô¨Årst two declare-const commands declare two
integer variables. The define-fun command creates a UF
that takes a parameter representing an integer and returns a
constant value. x1! is the argument of the UF. We have ob-
served two invocations of the function HASH, HASH applied
to y == 42 yields 30, and HASH applied to y == 19 yields
58. To complete the deÔ¨Ånition of the UF, we need to relate
unknown parameter values with a default return value, which
in this case we arbitrarily choose to be zero. Still, we assert that
HASH(y) is not zero to avoid accidental satisfaction. Finally
we provide the constraint x == HASH(y) && x >50 that we
want to solve. Using three assert commands, the system
pushes three formulas into Z3‚Äôs internal constraint stack. We
solve the concretized constraints by asking Z3 to produce a
satisfying assignment for variables in the constraints.
Of course, when the observations of the UDF are not
sufÔ¨Åcient to obtain the desired coverage, Z3 will deem a con-
cretized constraint to be unsatisÔ¨Åable or unknown. To increase
the chance of Ô¨Ånding a satisfying assignment for an abstract
constraint, we also try a second constraint solver, CORAL [4],
when Z3 returns unsatisÔ¨Åable or unknown for a concretized
constraint. The distinction between Z3 and CORAL concerns
the kind of formulas that they can solve: Z3 can derive models
and check satisÔ¨Åability of formulas in decidable theories, while
CORAL can deal with numerical constraints involving unde-
cidable theories. As a consequence of supporting undecidable
theories, CORAL can solve constraints involving UDFs in
the form of common math functions (e.g., power function)
directly without concretization. If neither concretization-and-
employing-Z3 nor calling CORAL can solve a constraint,
SEDGE will be unable to obtain perfect coverage.
Note that our approach to solving UDFs reasons about
all observed values of the UDF in parallel. These UDF
observations may be produced in different paths through the
program, including executions of different test cases. Still, theobservations can be used together (i.e., we can assume that all
of them hold) because of the lack of side-effects in a dataÔ¨Çow
program. In contrast, in dynamic-symbolic execution of an
imperative program, only values (of user-deÔ¨Åned functions)
observed during the current dynamic execution can be lever-
aged at a given constraint solving point.
IV. I MPLEMENTATION
The S EDGE system has required non-trivial implementation
effort, in the support of different data types, the interfacing
with the Z3 constraint solver, and the integration of string
generation capabilities.
A. Symbolic representation of values
SEDGE maintains an intermediate level of abstract syntax
trees for communication between Z3 and Pig Latin constraints.
Each node of the tree denotes a symbolic variable occurring
in the Pig Latin constraints. The high-level idea is that S EDGE
maps an execution path to a conjunction of arithmetic or
string constraints over symbolic variables and constants. Each
symbolic variable has a name and a data type, such as int
and long, mapping to a Ô¨Åeld of a table in a Pig Latin script
with the same name and data type. S EDGE then invokes Z3
to Ô¨Ånd a solution to that constraint system. If the constraint
solver Ô¨Ånds a solution, S EDGE maps it back to input tuples
(tuples from LOAD). S EDGE supports mapping all Pig Latin
data types into a symbolic variable, with support for overÔ¨Çow
and underÔ¨Çow checked arithmetic.
a) int: Integers are represented as 32-bit signed bit-
vectors, since the Z3 constraint solver has better support for
bit-vector arithmetic than for integer arithmetic. Arithmetic
calculations over integers are thus simulated with arithmetic
calculations over 32-bit-vectors. The simulation is accurate
and takes into account the Java (Apache Pig is written in
Java) representation of values of type int as 32-bit-vectors.
Additional constraints are created to check that the bit-wise
computation does not overÔ¨Çow and underÔ¨Çow. Standard library
conversion functions (e.g., java.lang.Long.parseLong(String) )
are used to translate back from 32-bit-vectors into integers.
b) long: Similar to int, long integers are represented
by 64-bit signed bit-vectors, since Z3 does not support long
integer arithmetic.
c) Ô¨Çoat/double: Floating point numbers are represented
by real numbers in the form of fractions of long integers. No
current constraint solvers have good support for Ô¨Çoating-point
arithmetic. Calculations with Ô¨Çoating point numbers are thus
approximated by real-valued calculations. A real number in
the form of fractions of long integers can be translated to a
Ô¨Çoating point number by Ô¨Årst representing the fraction using
BigFraction from Apache Common Math Library ,4and invok-
ingBigFraction.Ô¨ÇoatValue() (orBigFraction.doubleValue() ) to
get the fraction as a Ô¨Çoat (or double, respectively).
d) chararray: A character array is represented by
java.lang.String , which is also the inner representation of a
character array in Pig Latin.
4http://commons.apache.org/math/e) bytearray: We do not support byte arrays directly. We
try to identify the type that the byte array can convert to at
runtime and cast it.
f) boolean: A boolean variable is represented by an
integer with 3values: 1forFALSE ;0forUNDEF ;1for
TRUE .
Arithmetic and string constraints are typically expressed
over Ô¨Åelds of simple types as listed above. Therefore, we
do not deÔ¨Åne complex types (tuple, bag, map) for symbolic
variables.
B. Arithmetic and string constraint solving
As mentioned earlier, S EDGE uses Z3 [8] to solve arithmetic
constraints. Since Z3 provides a C interface and S EDGE
is implemented using Java, to have access to Z3‚Äôs C API
from Java, we employ SWIG.5We wrap Z3‚Äôs C API using
Java proxy classes and generate Java Native Interface [14]
wrapper code automatically. A common problem in wrapping
C programs for Java is that values may be returned in function
parameters in C, but in Java values are typically returned in the
return value of a function. S EDGE uses typemaps in SWIG, a
code generation rule that is attached to a speciÔ¨Åc C data type,
to overcome the problem. Given the data type Dof a value
returned in function parameters, S EDGE constructs a structure
Scontaining a member variable of type D. It also registers a
typemap such that 1) any occurrence of a function parameter
of type Din a function call in Z3 is converted into S, 2) the
return parameter S‚Äôs value can be read after returning from the
function in Java.
For string constraints, the main new element of our imple-
mentation concerns reasoning about string constraints contain-
ing regular expressions. Our approach is based on Xeger6a
Java library for generating a sample string for a regular ex-
pression. Xeger builds a deterministic Ô¨Ånite automaton (DFA)
for a string constraint in the form of a regular expression,
and follows the edges of the DFA probabilistically, until it
arrives at an accepting state of the DFA. Xeger is suboptimal
for two reasons: Ô¨Årst, it may keep visiting the same state until
a ‚Äústack overÔ¨Çow‚Äù error happens; second, it does not support
union, concatenated repetition, intersection, concatenation, or
complement of regular expressions. To avoid the ‚Äùstack over-
Ô¨Çow‚Äù error, our approach keeps a map from state ID to the
number of times a state has been entered and reduces the
probability of re-entering that state proportionally. To support
union, concatenated repetition, intersection, concatenation, and
complement of regular expressions, we add an intermediate
step between building the DFA and following DFA edges
to return a new deterministic automaton for the appropriate
regular expression. For example, when generating a satisfying
assignment that maps a string to a value so that the constraint
$0 matches ‚Äô.*apache.*‚Äô AND $0 matches ‚Äô.*commons.*‚Äô is
satisÔ¨Åed, we intersect the automaton representing ‚Äô.*apache.*‚Äô
with the automaton representing ‚Äô.*commons.*‚Äô .
5http://www.swig.org/
6http://code.google.com/p/xeger/V. E VALUATION
In this section, we evaluate the implementation of S EDGE
by running a wide spectrum of actual Pig Latin programs.
We measured both the completeness of generated example
tuples and the run-time of example generation for S EDGE
and the original Pig example data generator (abbreviated to
‚ÄúOlston‚Äôs system‚Äù in our discussion). Compared to Olston‚Äôs
system, our experiments conÔ¨Årm that S EDGE achieves higher
completeness. In most experiments, S EDGE also incurs a lower
running time.
A. Benchmark programs
To evaluate our system, we applied it to two benchmark
suites:
We use the entirety of the PigMix benchmark suite, con-
sisting of 20 Pig programs designed to model practical Pig
problems.
We use eleven sample SQL queries (the Ô¨Årst ten in the
list and an 11-th selected for being complex) from the
Sloan Digital Sky Survey (SDSS) set7and hand-translated
them directly into Pig code. The complex query contains 34
FILTER operations and 2 JOIN operations.
The PigMix benchmark provides Pig Latin programs for
testing a set of features such as ‚Äúdata with many Ô¨Åelds, but
only a few are used‚Äù and ‚Äúmerge join‚Äù. The SDSS sample
queries typically search for an astronomical object based on
some criteria. For example, Program 5 of the SDSS set is
below:
1A = LOAD Galaxy3 using PigStorage()
2AS (colc_g : float, colc_r : float,
3 cx : float, cy : float);
4B = FILTER A BY
5 (-0.642788 *cx + 0.766044 *cy >=0.0)
6AND (-0.984808 *cx - 0.173648 *cy <0.0);
Listing 3. SDSS program 5
The program Ô¨Ånds galaxies in a given area of the sky, using
a coordinate cut in the unit vector cx, cy, cz. As can be seen,
these benchmark programs are typically short, with only a
handful of them exhibiting interesting complexity.
For each query set, we used the input data that accompany
the relevant queries. PigMix ships with a tuple synthesizer that
generates such data. The SDSS benchmark suite is designed
for the digital sky survey data from the SDSS data release 7.
We selected a random sampling of tuples from the database
of this benchmark, in which the total amount of data is 818
GB, and the total number of rows exceeds 3.4 billion.
B. Methodology and Setup
Since the importance of tuple synthesis varies not only by
benchmark but also by the size of the tuples supplied to the
Ô¨Årst analysis pass (in the ideal case, tuple synthesis is entirely
unnecessary), we ran our benchmarks for sample input tuple
sizes of 10, 30, 100, 300, and 1000 tuples using our system and
Olston‚Äôs system. For each sample input tuple size, we executed
7http://skyserver.sdss.org/public/en/help/docs/realquery.aspeach benchmark program 10 times with different randomly
sampled input tuples. All experiments were performed on a
four core 2.4 GHz machine with 6 GB of RAM.
We conÔ¨Ågured our system to compare directly to Olston‚Äôs
system, which is implemented as the ‚Äúillustrate‚Äù command
in Pig Latin. Unfortunately, the current implementation of
Olston‚Äôs system has some limitations not mentioned in the
published paper. We wanted to evaluate against the approach
and not against the implementation. To that end, we addressed
such limitations or tweaked the benchmark programs so that
the problems do not manifest themselves. The Ô¨Årst issue is that
the downstream pass would discard all sample input containing
null Ô¨Åelds. In the upstream pass, if all input tuples happen to
contain null Ô¨Åelds and thus all of them are discarded, there
would be a NullPointerException . We sidestep the null Ô¨Åeld is-
sue by ensuring that at least one sample input does not contain
null Ô¨Åelds. In addition, the system can only handle 32 FILTER
conditions at most, as it encodes pertinent equivalence classes
for FILTER conditions as individual bits in a (32-bit) integer
index variable. This problem affects one PigMix benchmark
program intended for scalability testing. The benchmark has a
very large set (500+) of FILTER conditions. We sidestepped
the problem by making changes to the PigMix program so
that the resulting program has only 31 FILTER conditions.
Also, when reasoning about a JOIN in the upstream pass, a
NullPointerException is thrown if no data are observed in the
input side of a JOIN (typically because one of its upstream
neighbors is a highly selective operator). We address this issue
by skipping the JOIN if its input has no data and then attempt
to continue upstream propagation. Moreover, by mistakenly
setting a non-tuple Ô¨Åeld to a tuple in a method involved
in upstream propagation, a type casting error arises, which
impedes the ability of Olston‚Äôs system to reason over the JOIN
and FOREACH operations if their downstream neighbor is a
FILTER operator. We disallow assigning the non-tuple Ô¨Åeld to
a tuple in the problematic method.
C. Results
We ran each experiment 10 times and averaged the com-
pleteness of 10 runs (since the completeness may theoretically
vary due to different random choice of initial samples). The
size of the input data has little effect. The results are almost
the same for all sample input sizes.
Figures 3 and Figure 4 show the average completeness
for each Pig Latin program in the PigMix and SDSS sets,
respectively, for a sample input size of 100 tuples. Every bar
corresponds to one program (with the exception of program
L12 in Figure 3, in which there are three subprograms and
example data were generated for three different root operators
corresponding to the three subprograms) for a total of 20
programs.
As can be seen, we improve on completeness for 5 out of 20
PigMix benchmark programs and 7 out of 11 SDSS benchmark
programs. Although the benchmark programs are small and
much of their coverage is achieved with random sampling
of real inputs, they demonstrate clearly the beneÔ¨Åts of ourS1L1L2L3L4L5L6L7L8L9L
10L
11L
12-1L
12-2L
12-3L
13L
14L
15L
16L
170.00.250.50.751.0 Sedge 
Olston/Pig Latin "illustrate"  
completenessFig. 3. Completeness of sample data generation for the PigMix benchmarks
12 3 4 5 6 7 8 9 1 01 100.250.50.751.0completeness  Sedge 
Olston/Pig Latin "illustrate"
Fig. 4. Completeness of sample data generation for the SDSS benchmarks
approach. Practically every program in the two benchmark sets
that has any kind of complexity (either more than one operator
in the same path, or a user-deÔ¨Åned function, or complex Ô¨Ålter
conditions) is not fully covered by Olston‚Äôs approach. For
example, Olston‚Äôs system cannot generate data that fail the
FILTER in the presence of grouping, projecting, udf invocation
in the following program (program S1in Figure 3).
1A = LOAD ‚Äô$widerow‚Äô using PigStorage(‚Äô\u0001‚Äô)
AS (name: chararray, c0: int, c1: int,
..., c31: int);
2B = GROUP A BY name parallel $parrallelfactor;
3C = FOREACH B GENERATE group, SUM(A.c0) as c0,
SUM(A.c1) as c1, ... SUM(A.c31) as c500;
4D = FILTER C BY c0 > 100 AND c1 > 100 AND c2 >
100 ... AND c31 > 100;
Listing 4. PigMix program S1
In fact, S EDGE achieves perfect coverage (i.e., full com-
pleteness) for all benchmark programs. Compared to Olston‚Äôs
approach our improved coverage is due to stronger constraint
solving ability (for programs 4,5,6,7 in Figure 4), to UDF
handling ability (for programs 9,10 in Figure 4) and also to
inter-related constraints and global reasoning (for programs
S1,L5,L12-1,L12-2,L12-3 in Figure 3 and programs 3,11 in
Figure 4).
We also recorded how long it took S EDGE and Olston‚Äôs
system to Ô¨Ånish example generation. We include the infrastruc-
ture bootstrap time on each benchmark program. Both S EDGE
and Olston‚Äôs system need to prepare the Hadoop execution
environment for new executions. S EDGE needs to load its
constraint solver Z3 and CORAL as well.
As can be seen in Figure 5 and Figure 6, S EDGE is fasteron average than Olston‚Äôs system in 18 out of 20 PigMix
benchmark programs and 9 out of 11 SDSS benchmark
programs. For the rest of benchmark programs, S EDGE incurs
a little higher running time than Olston‚Äôs system. From these
numbers we can infer that, although we have to conduct path
exploration and constraint solving, there are even time savings
in most cases due to avoiding the step of pruning redundant
tuples after the upstream pass (because our approach does not
generate redundant data).
S1L1L2L3L4L5L6L7L8L9L
10L
11L
12-1L
12-2L
12-3L
13L
14L
15L
16L
170.00.40.81.21.62.02.42.83.23.64.04.4running time (seconds) Sedge 
Olston/Pig Latin "illustrate" 
Fig. 5. Running time of sample data generation for the PigMix benchmarks
12 3 4 5 6 7 8 9 1 01 10.00.51.01.52.02.53.03.54.04.55.05.56.06.57.0running time (seconds) 
 Sedge 
Olston/Pig Latin "illustrate"
Fig. 6. Running time of sample data generation for the SDSS benchmarks
VI. D ISCUSSION : W HYHIGH-LEVEL DSE
A natural qualitative comparison is between a Dynamic
Symbolic Execution (DSE) engine at the level of the Pig Latin
language and DSE engines for imperative languages, since Pig
Latin code is eventually compiled into imperative code that
uses a map-reduce library. The expected beneÔ¨Åts from our
approach are a) simplicity; b) conciseness of the generated
test cases (i.e., the same coverage with fewer tests); and c)
completeness: an imperative DSE engine may have trouble
solving constraints over the logically more complex generated
code, rather than the original Pig Latin code. Furthermore, an
imperative DSE engine cannot take advantage of the lack of
side-effects in order to better concretize user-deÔ¨Åned functions,
as discussed in Section III-B.
We compared S EDGE with the Pex [22] state-of-the-art DSE
engine in a limit study. Pex accepts C#input, hence we hand-translated Pig Latin programs into C#programs.8The resulting
C#programs are single-threaded without any call to the map-
reduce API, in order to test the applicability of Pex in the ideal
case. (The inclusion of the map-reduce library complicates
the control-Ô¨Çow of the imperative program even more and
can easily cause the DSE engine to miss a targeted branch
of test execution, leading to low coverage of generated test
cases [25].)
For our translation, we inspected the Java code generated
by the Pig compiler and made a best-effort attempt to replicate
it inC#, without map-reduce calls. We translated 13 programs
from our Pig Latin benchmark suites. Since Pex has no
knowledge of the original input (it accepts concrete values
only when passed into the test method as parameters with
primitive types) we enable just the 3rd pass (upstream pass)
of S EDGE , for a fair comparison (i.e., S EDGE also does
not beneÔ¨Åt from sampled real data‚Äîthis also disadvantages
SEDGE as it removes the advantage of better UDF handling).
The results conÔ¨Årm our expectation. The conciseness of
the test suite generated by Pex is low since Pex needs to
examine a lot of irrelevant low-level branches or constraints
that are not necessary for equivalence class coverage of the
high-level Pig Latin control Ô¨Çow. For example, for step A
in the Pig Latin program in Listing 3, Pex generates 30
tuples within 11 tables, of which 3 tuples pass the Ô¨Ålter
in step B, while S EDGE generates 2 tuples within exactly
1 table, of which 1 tuple passes the Ô¨Ålter in step B. The
conciseness of the test suite generated by Pex is 0.05, while
the conciseness of the test suite generated by S EDGE is 0.75.
Furthermore, for speciÔ¨Åc complex constructs we also get much
higher completeness, although quite often Pex also gets perfect
coverage. In our experience, for Pig Latin programs containing
FILTER statements after (CO)GROUP or JOIN statements, the
test suites yielded by Pex lack in completeness. For instance,
in the SDSS program with 34 FILTER operations and 2 JOIN
operations, the Pex completeness is only 0.09.
VII. R ELATED WORK
DataÔ¨Çow languages such as Pig can been seen as a com-
promise between declarative languages such as SQL and im-
perative languages such as C and Java. That is, Pig combines
the declarative feature of straightforward parallel computation
with the imperative feature of explicit intermediate results.
There is little work (discussed in earlier sections) that ad-
dresses test data generation for dataÔ¨Çow languages. Instead, the
related work from various research communities has focused
on the extreme ends of this spectrum, i.e., either on SQL or
on Java-like programming languages.
SpeciÔ¨Åcally, related work in the software engineering com-
munity has focused on traditional procedural and object-
oriented database-centric programs, both using static symbolic
execution [15] and dynamic symbolic execution [9], [13],
[18]. While our work is inspired by such earlier dynamic
8Although there are DSE engines for Java‚Äîe.g., Dsc [12]‚Äîthey do not
match the industrial-strength nature of Pex. Dsc, for instance, does not support
programs with Ô¨Çoating point numbers, which are common in Pig Latin.symbolic execution approaches, we adapted this work to
dataÔ¨Çow programs and their execution semantics. At the other
end, there is work that automatically generates database data
that satisfy external constraints [20] but there is no coverage
or conciceness goal and no application to dataÔ¨Çow languages.
Other work [23] has introduced the idea of code coverage
to SQL queries. For our purposes, we reused the concept of
coverage for Pig Latin as deÔ¨Åned by Olston et al. [16].
In the formal methods community, Qex is generating test
inputs for SQL queries [24]. Similar to our work, Qex maps
a SQL query to SMT and uses the Z3 constraint solver to
infer data tables. However Qex differs from our work in that
Qex does not have a dynamic program analysis component and
therefore cannot observe how a query processes existing exam-
ple data. Earlier work in the software engineering community
on dynamic symbolic execution has shown that dynamic
analysis can make such program analysis more efÔ¨Åcient and
enable it to reason about user-deÔ¨Åned functions, which we
leverage in our work.
In the database community, a common methodology for
testing a database management system or a database appli-
cation is to generate a set of test databases given target query
workloads. Overall our problem differs in that, instead of a
whole database, we aim to generate a small (or minimum if
desired) set of tuples that have perfect path coverage of a
given dataÔ¨Çow program. The recent work on reverse query
processing [2] takes an application query and a result set
as input, and generates a corresponding input database by
exploiting reverse relational algebra . In comparison, our work
focuses on dataÔ¨Çow programs for big data applications, where
many operators are non-relational, e.g., map(), reduce(), and
arbitrary user-deÔ¨Åned functions, and hence a ‚Äúreverse algebra‚Äù
may not exist. The QAGen system [3] further takes into
account a set of constraints, usually cardinality and data
distribution in input and operator output tables, and aims to
generate a database that satisÔ¨Åes these constraints. Analo-
gously to earlier work in the formal methods community, this
work performs a static symbolic analysis and does not obtain
additional information from a dynamic analysis.
VIII. C ONCLUSIONS AND FUTURE WORK
Generating example input data for dataÔ¨Çow programs has
emerged as an important challenge. We presented S EDGE : an
approach and tool for generating example data of dataÔ¨Çow pro-
grams using DSE, in order to achieve high coverage. S EDGE
builds symbolic constraints over the equivalence classes in-
duced by dataÔ¨Çow programming language constructs and can
reason over constraints on user-deÔ¨Åned functions by exploiting
dynamic values as hints. We implemented our technique for
the Pig dataÔ¨Çow system and compared it empirically with the
most closely related prior work. While we currently focus on
the Pig Latin programming language, the principles are quite
general. The same high-level technique can be applied to other
dataÔ¨Çow programming languages such as DryadLINQ [11] and
Hyracks/Asterix [1] and relational algebra such as relational
division and anti-join.REFERENCES
[1] A. Behm, V . R. Borkar, M. J. Carey, R. Grover, C. Li, N. Onose,
R. Vernica, A. Deutsch, Y . Papakonstantinou, and V . J. Tsotras. Asterix:
towards a scalable, semistructured data platform for evolving-world
models. Distributed and Parallel Databases , 29(3):185‚Äì216, 2011.
[2] C. Binnig, D. Kossmann, and E. Lo. Reverse query processing. In Proc.
23rd International Conference on Data Engineering (ICDE) , pages 506‚Äì
515. IEEE, Apr. 2007.
[3] C. Binnig, D. Kossmann, E. Lo, and M. T. ¬®Ozsu. QAGen: Generating
query-aware test databases. In Proc. ACM SIGMOD International
Conference on Management of Data (SIGMOD) , pages 341‚Äì352. ACM,
June 2007.
[4] M. Borges, M. d‚ÄôAmorim, S. Anand, D. Bushnell, and C. S. Pasareanu.
Symbolic execution with interval solving and meta-heuristic search.
InProceedings of the 2012 IEEE Fifth International Conference on
Software Testing, VeriÔ¨Åcation and Validation , ICST ‚Äô12, pages 111‚Äì120,
Washington, DC, USA, 2012. IEEE Computer Society.
[5] R. E. Bryant, S. M. German, and M. N. Velev. Exploiting positive
equality in a logic of equality with uninterpreted functions. In Proc.
11th International Conference on Computer Aided VeriÔ¨Åcation (CAV) ,
pages 470‚Äì482. Springer, 1999.
[6] J. R. Burch and D. L. Dill. Automatic veriÔ¨Åcation of pipelined
microprocessor control. In Proc. 6th International Conference on
Computer Aided VeriÔ¨Åcation (CAV) , pages 68‚Äì80. Springer, 1994.
[7] C. Cadar and D. R. Engler. Execution generated test cases: How to make
systems code crash itself. In Proc. 12th International SPIN Workshop
on Model Checking of Software , pages 2‚Äì23. Springer, Aug. 2005.
[8] L. De Moura and N. Bj√∏rner. SatisÔ¨Åability modulo theories: introduction
and applications. Commun. ACM , 54(9):69‚Äì77, Sept. 2011.
[9] M. Emmi, R. Majumdar, and K. Sen. Dynamic test input generation for
database applications. In Proc. ACM SIGSOFT International Symposium
on Software Testing and Analysis (ISSTA) , pages 151‚Äì162. ACM, July
2007.
[10] P. Godefroid, N. Klarlund, and K. Sen. Dart: Directed automated random
testing. In Proc. ACM SIGPLAN Conference on Programming Language
Design and Implementation (PLDI) , pages 213‚Äì223. ACM, June 2005.
[11] M. Isard, M. Budiu, Y . Yu, A. Birrell, and D. Fetterly. Dryad: Distributed
data-parallel programs from sequential building blocks. In Proc. 2nd
ACM SIGOPS/EuroSys European Conference on Computer Systems
(EuroSys) , pages 59‚Äì72. ACM, 2007.
[12] M. Islam and C. Csallner. Dsc+mock: a test case + mock class generator
in support of coding against interfaces. In Proceedings of the Eighth
International Workshop on Dynamic Analysis , WODA ‚Äô10, pages 26‚Äì31,
New York, NY , USA, 2010. ACM.[13] C. Li and C. Csallner. Dynamic symbolic database application testing.
InProc. 3rd International Workshop on Testing Database Systems
(DBTest) . ACM, June 2010.
[14] S. Liang. Java Native Interface: Programmer‚Äôs Guide and SpeciÔ¨Åcation .
Prentice Hall, June 1999.
[15] M. Marcozzi, W. Vanhoof, and J.-L. Hainaut. Test input generation for
database programs using relational constraints. In Proc. 5th International
Workshop on Testing Database Systems (DBTest) . ACM, May 2012.
[16] C. Olston, S. Chopra, and U. Srivastava. Generating example data
for dataÔ¨Çow programs. In Proc. 2009 ACM SIGMOD International
Conference on Management of Data (SIGMOD) , pages 245‚Äì256. ACM,
2009.
[17] C. Olston, B. Reed, U. Srivastava, R. Kumar, and A. Tomkins. Pig latin:
A not-so-foreign language for data processing. In Proc. ACM SIGMOD
International Conference on Management of Data (SIGMOD) , pages
1099‚Äì1110. ACM, 2008.
[18] K. Pan, X. Wu, and T. Xie. Generating program inputs for database
application testing. In Proc. 26th IEEE/ACM International Conference
on Automated Software Engineering (ASE) , pages 73‚Äì82. IEEE, Nov.
2011.
[19] K. Sen and G. Agha. Cute and jCute: Concolic unit testing and explicit
path model-checking tools. In Proc. 18th International Conference
on Computer Aided VeriÔ¨Åcation (CAV) , pages 419‚Äì423. Springer, Aug.
2006.
[20] Y . Smaragdakis, C. Csallner, and R. Subramanian. Scalable automatic
test data generation from modeling diagrams. In Automated Software
Engineering conference (ASE) , pages 4‚Äì13. ACM Press, Nov. 2007.
[21] A. Thusoo, J. S. Sarma, N. Jain, Z. Shao, P. Chakka, S. Anthony, H. Liu,
P. Wyckoff, and R. Murthy. Hive - a warehousing solution over a map-
reduce framework. PVLDB , 2(2):1626‚Äì1629, 2009.
[22] N. Tillmann and J. De Halleux. Pex: White box test generation for .Net.
InProc. 2nd International Conference on Tests and Proofs (TAP) , pages
134‚Äì153. Springer, 2008.
[23] J. Tuya, M. J. Su ¬¥arez-Cabal, and C. de la Riva. Full predicate coverage
for testing SQL database queries. Software Testing, VeriÔ¨Åcation &
Reliability (STVR) , 20(3):237‚Äì288, Sept. 2010.
[24] M. Veanes, N. Tillmann, and J. de Halleux. Qex: Symbolic SQL
query explorer. In Proc. 16th International Conference on Logic for
Programming, ArtiÔ¨Åcial Intelligence, and Reasoning (LPAR) , pages 425‚Äì
446. Springer, Apr. 2010.
[25] X. Xiao, T. Xie, N. Tillmann, and J. de Halleux. Precise identiÔ¨Åcation
of problems for structural test generation. In Proceedings of the 33rd
International Conference on Software Engineering , ICSE ‚Äô11, pages
611‚Äì620, New York, NY , USA, 2011. ACM.