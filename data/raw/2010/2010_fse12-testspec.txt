Testing Mined Speciï¬cations
Mark Gabel
Department of Computer Science
The University of Texas at Dallas
mark.gabel@utdallas.eduZhendong Su
Department of Computer Science
University of California, Davis
su@ucdavis.edu
ABSTRACT
Speciï¬cations are necessary for nearly every software engineering
task, but they are often missing or incomplete. â€œSpeciï¬cation min-
ingâ€ is a line of research promising to solve this problem through
automated tools that infer speciï¬cations directly from existing pro-
grams. The standard practice is one of inductive learning: mining
tools make observations about software and inductively generalize
them into speciï¬cations. Inductive reasoning is unsound, however,
and existing tools commonly grapple with the problem of inferring
â€œfalseâ€ speciï¬cations, which must be manually checked.
In this work, we introduce a new technique for automatically
validating mined speciï¬cations that lessens this manual burden. Our
technique is not based on heuristics; it rather uses a general, semantic
deï¬nition of a â€œtrueâ€ speciï¬cation. We perform systematic, targeted
program transformations to test a mined speciï¬cationâ€™s necessity
for overall correctness. If a â€œviolatingâ€ program is correct, the
speciï¬cation is false. We have implemented our technique in a
prototype tool that validates temporal properties of Java programs,
and we demonstrate it to be effective through a large-scale case
study on the DaCapo benchmarks.
Categories and Subject Descriptors
D.2.7 [ Software Engineering ]: Distribution, Maintenance, and
Enhancementâ€”Restructuring, reverse engineering, and reengin-
eering; F.3.1 [ Logics and Meaning of Programs ]: Specifying and
Verifying and Reasoning about Programs
Keywords
speciï¬cation inference, reverse engineering, software tools
1. INTRODUCTION
Nearly all software engineering tasks require some form of a
speciï¬cation. Implementation, debugging, and testing, for example,
all involve reconciling a software programâ€™s speciï¬ed and actual
*This research was supported in part by NSF CAREER Grant No. 0546844,
NSF CyberTrust Grant No. 0627749, NSF CCF Grant No. 0702622, NSF
TC Grant No. 0917392, NSF SHF Grant No. 1117603, and the US Air
Force under grant FA9550-07-1-0532. The information presented here does
not necessarily reï¬‚ect the position or the policy of the Government and no
ofï¬cial endorsement should be inferred.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proï¬t or commercial advantage and that copies
bear this notice and the full citation on the ï¬rst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciï¬c
permission and/or a fee.
SIGSOFTâ€™12/FSE-20, November 11â€“16, 2012, Cary, North Carolina, USA.
Copyright 2012 ACM 978-1-4503-1614-9/12/11 ...$15.00.behaviors. Documentation and source code comments are standard
sources of speciï¬cations, but they are often incomplete, incorrect,
or missing entirely. Worse yet, time-saving software toolsâ€”our
research focusâ€”require formal, machine-readable speciï¬cations,
which are even rarer.
Research in speciï¬cation inference [3] aims to solve this prob-
lem through tools that automatically reverse engineer speciï¬cations
directly from programs. Although reasoning soundly about speciï¬-
cations from implementations is generally impossible, the intended
behavior of a stable software project can be somewhat evident.
For example, if calls to the function Lock.lock() often precede
Lock.unlock() , it might be reasonable to suggest that this relation-
ship is necessary andrequired. This example illustrates the essence
of how speciï¬cation â€œminingâ€ tools work: they make observations
about software and generalize them into speciï¬cations, albeit with a
degree of uncertainty.
That â€œdegree of uncertaintyâ€ is a central issue in speciï¬cation
inference. The fundamental cause of imprecision in speciï¬cation
inference is the standard problem of induction: generalizing from ex-
amples is unsound. A mining tool is easily deceived into reporting
frequently-observed coincidental relationships as false speciï¬ca-
tions, e.g.â€œcalls to List.add() must precede List.isEmpty() .â€ The
possibility of these â€œfalse positivesâ€ forces the user of the tool to
validate each inferred speciï¬cation manually.
Manual validation of mined speciï¬cations is an expensive and
error-prone process. Unfortunately, it is also essential: truly sound
reasoning about speciï¬cations requires speciï¬cations, a circular
dilemma. Researchers have nonetheless tried to reduce this burden
with several automated techniques for identifying and ï¬ltering likely-
false speciï¬cations, including leveraging simple usage statistics [ 38],
more advanced statistical models [ 24], and assorted heuristics [ 7].
All of these techniques are alike in that they operate on the belief
that classes of true and/or false speciï¬cations are somehow â€œalikeâ€
and mechanically recognizable.
In our own continuing research, we have found existing speciï¬ca-
tion validation techniques to be insufï¬cient. The current â€œstatistics
and heuristicsâ€ approach does work adequately when applied to the
examples frequently found in the research literature, such as speciï¬-
cations mined from language-level standard libraries. Speciï¬cations
over Javaâ€™s Iterator ( Iterator.hasNext() /next()) and Câ€™s ï¬le de-
scriptors ( open(fd) /close(fd) ) are recognizable examples. Our re-
search focus, however, has been to scale speciï¬cation mining â€œdownâ€
and â€œoutâ€ to project-speciï¬c and highly semantic propertiesâ€”those
perhaps most relevant to the majority of software tasks [ 27]â€”and
existing techniques have left us at a â€œprecision wall.â€ In short, false
positives dominated our early experimental results.
In this paper, we present a novel and effective automated tech-
nique for validating mined speciï¬cations. Our technique is not a
1surface-level statistic or heuristic; it is instead based on a purely
semantic deï¬nition of a â€œtrueâ€ speciï¬cation that is a result of a ï¬rst-
principles reexamination of the problem. We have formulated our
approach around this key observation: a true speciï¬cation is one
that is necessary for correctness.
Our technique takes the following high-level approach. We start
with a program and a (potentially false) mined speciï¬cation, say, that
a call to method foo() should always be followed by a call to bar() .
We then perform a series of automated experiments, each of which
involves 1) transforming the program to violate that speciï¬cation
(in this case, reordering and/or removing calls to foo() andbar() )
and 2) evaluating correctness using testing. If we can generate a
transformed program that both a) violates the mined speciï¬cation
and b) is correct, then we have shown the mined speciï¬cation to be
unnecessary for correctness, and thus false.
We call our technique Deductive Speciï¬cation Inference, or DSI.
The name â€œDeductiveâ€ reï¬‚ects our goal of moving away from heuris-
tics and statistics, but it is of course an idealization: as mentioned
earlier, sound deductive reasoning about speciï¬cations from pro-
grams is impossible. That limitation is manifest here by the require-
ment that we precisely evaluate the correctness of a program, which,
if possible, would require complete speciï¬cationsâ€”obviating the
need to mine them! We have approximated this ideal with testing,
and our implementation of our technique is thus (like any speciï¬-
cation miner must be) imperfect. Nonetheless, DSI is effective in
practice, as we show in a case study of several real Java programs.
This paper includes the following contributions:
1.A novel speciï¬cation validation methodology, DSI, that avoids
the use of statistics or heuristics.
2.An implementation of our method for a well-studied domain:
temporal speciï¬cations over the sequences of method calls in
a Java program.
3.A case study demonstrating our toolâ€™s effectiveness on several
open source Java programs.
4.A detailed discussion of the nuances and limitations of our
technique, including how these limitations speak to funda-
mental limitations of speciï¬cation inference in general.
The next section (Section 2) provides an overview of our approach
through a set of examples. Section 3 then presents in detail both the
DSI methodology and a tool implementing DSI for the domain of
temporal properties of method calls. Our experimental results and
related discussions follow in Section 4. In Section 5 we discuss DSI
in the context of related work, and in Section 6 we conclude with a
discussion of future work.
2. OVERVIEW
In this section, we provide an overview of our general approach
through a series of examples. We begin with an introduction to
our target domain, temporal speciï¬cations, and continue with a
presentation of our general technique as well as our implementation.
2.1 Temporal Properties and Their Inference
The examples in this section are drawn from the domain of tem-
poral speciï¬cations over program elements. Here, â€œtemporalâ€ refers
to the span of runtime execution and â€œprogram elementsâ€ refers to
executable code. A temporal speciï¬cation extends traditional state
assertions (â€œvariable xis always positiveâ€) with the notion of time
(â€œonce xis positive, ywill eventually become positive as wellâ€).
One commonly studied class of temporal properties involves or-
dering restrictions on function calls. Functions are building blocks
of software projects, and the order in which they are composed is
both critical and subtle, especially in imperative and object-orientedsystems with side effects. Common examples include locking disci-
plines, in which a speciï¬cation might state â€œcalls to methods lock
and unlock on each Lock object strictly alternate at runtimeâ€ and
resource usage rules, in which a partial speciï¬cation might state
â€œone should call close on a ï¬le descriptor soon after its ï¬nal use.â€
Temporal properties are often much more domain-speciï¬c and
obscure than these canonical â€œlockingâ€ and â€œresourceâ€ examples,
and they are rarely fully documented. Researchers have recognized
this problem and developed automated software tools capable of
â€œminingâ€ temporal properties directly from programs. The predom-
inant models are forms of inductive learning. Many tools operate
similarly in two high-level steps: 1) observing (at runtime or stati-
cally approximating) the behavior of a program and 2) generalizing
that behavior into a speciï¬cation.
2.2 Validating Speciï¬cations
Figure 1 lists four examples of â€œpotentialâ€ temporal speciï¬cations.
They were synthesized from observations of real software projects,
simpliï¬ed excerpts of which are listed as well. Mining tools may
report speciï¬cations like these for several reasons, including:
The observed property is satisï¬ed (or mostly so) by the ob-
served program. This condition is often trivially true.
The tool observes the property frequently, with examples
occurring frequently at runtime or within the static source
code. This encodes the belief that â€œcommon behavior is likely
to be correctâ€ [12].
Assorted heuristics. For example, the property listed in Fig-
ure 1a involves a method named execute , which may match
a â€œfunction name ï¬lterâ€ that identiï¬es naming patterns that
have often been important in the past.
Ultimately, a speciï¬cation mining tool takes an inductive leap,
essentially â€œliftingâ€ observations into speciï¬cations based on both
observed evidence and prior beliefs.
â€œPotentialâ€ speciï¬cations may not be true, though, which is a
natural consequence of inductive learning. When a programmer
is presented with a mined speciï¬cation, he or she must generally
validate and/or debug it before it becomes useful. Approaches
include:
Code inspection. If the speciï¬cation is not followed, would it
lead to an obvious error?
Reconciling with known requirements. Is the speciï¬cation
clearly (in)consistent with existing speciï¬cations?
Consulting with experts and past software engineering data.
Have the elements of this speciï¬cation been involved in any
prior issues?
Note the lack of a complete, algorithmic solution. This is precisely
what makes speciï¬cation inference difï¬cult in practice and impossi-
ble in the limit. These validation techniques do follow a common
theme, though: they involve using disparate sources of information
to answer the following question as accurately as possible:
Given a potential speciï¬cation j, isjnecessary for
my programâ€™s correct execution?
Our current work can be framed as a method for solving this problem
systematically and automatically.
2.3 Automated Experimental Validation
Returning to the running examples, consider now the contraposi-
tive of the â€œvalidation problemâ€:
Does violating jmake my program incorrect?
21CompilerTest test = ...
2test.reset();
3/* Set up 'prog' variable */
4test.execute(prog, out, err);
a.â€œCall CompilerTest.reset at some point before calling
CompilerTest.execute.â€1ResourceAttributes attr = ...
2/* Other setup */
3attr.setArchive(true);
4attr.setSymbolicLink(false);
b.â€œResourceAttributes.setArchive andsetSymbolicLink
must appear in sequence.â€
1GeneratorAdapter gen = ...
2/* Set up 'type' and 'constr' variables */
3gen.loadThis();
4/* ... other 'gen' invocations */
5gen.invokeConstructor(type, constr);
c.â€œCall GeneratorAdapter.loadThis at some point before
calling GeneratorAdapter.invokeConstructor.â€1SaveManager sm = this;
2/* Other state restoration actions */
3try {
4 sm.restoreMarkers(resource, true, p);
5 sm.restoreSyncInfo(resource, true, p);
6} catch (Exception e) { /* Ignore */ }
d.â€œSaveManager.restoreMarkers andrestoreSyncInfo
must appear in sequence.â€
Figure 1: Four observed temporal properties and a selection of the Java source code that generated them.
1CompilerTest test = ...
2//test.reset();
3/* Set up 'prog' variable */
4test.execute(prog, out, err);
5test.reset();
a.â€œCall CompilerTest.reset at some point before calling
CompilerTest.execute.â€1ResourceAttributes attr = ...
2/* Other setup */
3//attr.setArchive(true);
4attr.setSymbolicLink(false);
5attr.setArchive(true);
b.â€œResourceAttributes.setArchive andsetSymbolicLink
must appear in sequence.â€
1GeneratorAdapter gen = ...
2/* Set up 'type' and 'constr' variables */
3//gen.loadThis();
4/* ... other 'gen' invocations */
5gen.invokeConstructor(type, constr);
6gen.loadThis();
c.â€œCall GeneratorAdapter.loadThis at some point before
calling GeneratorAdapter.invokeConstructor.â€1SaveManager sm = this;
2/* Other state restoration actions */
3try {
4 //sm.restoreMarkers(resource, true, p);
5 sm.restoreSyncInfo(resource, true, p);
6 sm.restoreMarkers(resource, true, p);
7} catch (Exception e) { /* Ignore */ }
d.â€œSaveManager.restoreMarkers andrestoreSyncInfo
must appear in sequence.â€
Figure 2: Transformed programs that should now be â€œwrongâ€ if each speciï¬cation is â€œrealâ€ or â€œnecessary.â€
Phrasing the question this way suggests an experimental solution.
Figure 2 reprises the potential properties listed in Figure 1, but
the code excerpts have now been transformed. For the domain
of temporal properties, we have a strong idea of what it means
to â€œviolateâ€ a speciï¬cation, and in each case the code has been
â€œminimallyâ€ and straightforwardly modiï¬ed to violate each property.
If each of the potential speciï¬cations is true, then each program in
Figure 2 should now be wrong.
The problem now reduces to judging each â€œexperimentâ€ as â€œcor-
rectâ€ or â€œwrong.â€ If we were able to judge any as being correctâ€”
despite being transformedâ€”we could say with some certainty that
the associated speciï¬cation is unnecessary for correct execution and
thus false. Similarly, if one of those programs were now incorrect,
we would obtain evidence (but not proof) that the associated speciï¬-
cation is necessary and true. Note the lack of the word â€œcertaintyâ€
in the latter case: it is rife with subtlety and will be discussed in
more detail throughout this paper.
Judging a program â€œcorrectâ€ or â€œwrongâ€ is generally impossible,
of course, and to do so actually begs the question of a complete spec-
iï¬cation. However, correctness checking can often be approximated
through testing and analysis, giving us the ï¬nal component we need
to automatically (but approximately) validate speciï¬cations. Our
high-level technique is as follows:
1.Start with a proposed speciï¬cation jfrom a program P. For
temporal function-call speciï¬cations, this might be of the formâ€œcalls to function aalways precede calls to function bâ€. The
normal source of proposed speciï¬cation will be a speciï¬cation
mining tool.
2.Create a suite of experimental programs around Pandj, a
sort of â€œdesign spaceâ€ populated with programs similar to
Pbut violating j. We accomplish this through automatic
program transformations. Continuing the earlier example,
this space may consist of the family of programs in which
calls to aandbare reordered.
3.Testthese experimental programs. If jis found to be unnec-
essary for correctness, then jis not a speciï¬cation.
We call this process â€œDeductive Speciï¬cation Inference,â€ or DSI.
This name reï¬‚ects DSIâ€™s logical and experimental nature, but we em-
phasize again that truly sound deductive reasoning in this setting is
impossible. Nonetheless, on our example properties this automated
process is very revealing.
The experiment in Figure 2a crashes early: reset does in fact
set up the precondition for execute to run; the speciï¬cation
is true.
Experiment 2b passes: the order in which these two ï¬elds are
set is irrelevant.
Experiment 2c fails, but notwith an immediate crash: it
ultimately causes operations much later in the test suite to fail.
3GeneratorAdapter is a helper class within a Java bytecode
library. Not following this temporal speciï¬cation will actually
result in the generation of bytecode that violates the Java
Bytecode Speciï¬cation, which is what ultimately causes the
(much) later test failure.
Experiment 2d passes, but perhaps surprisingly so: each oper-
ation contains a substantial amount of overlapping side effects.
From a class-level perspective, though, the tests demonstrate
that the observed ordering is irrelevant.
In theory, one could transform this speciï¬cation validation pro-
cedure into a speciï¬cation inference algorithm, as validation and
inference are fundamentally the same problem. For temporal spec-
iï¬cations, we could simply enumerate every possible ordering of
function calls and systematically validate or invalidate each one.
However, for efï¬ciency and to better leverage advances in speciï¬ca-
tion mining, we bootstrap the process with an inductive speciï¬cation
mining tool.
This section has provided an overview of our automated valida-
tion technique and how we implement it for the domain of temporal
function-call properties. The following section presents our DSI
methodology and implementation in full detail, including a discus-
sion of the strengths and subtle limitations of the technique.
3. TESTING SPECIFICATIONS
We have implemented DSI for the domain of temporal function-
call properties of imperative and object-oriented systems. Our im-
plementation uses automated program transformations to conduct
its experiments, and it uses software testing to approximately evalu-
ate correctness. We introduced the temporal function-call problem
domain earlier (see Section 2.1) and continue here in more depth.
3.1 Temporal Function-Call Properties
We address a common class of speciï¬cation: ordering restrictions
on function calls within a software project. These speciï¬cations
are common and error-prone, as they are not enforced by the type
systems within standard compilers. When they are deï¬ned formally,
however, advanced software tools can check them statically [ 5,15]
or at runtime [8], preventing and eliminating errors.
The formalism we use to represent speciï¬cations is regular lan-
guages. While the most general formalism for expressing these
properties is some form of a temporal logic, many important prop-
erties can be expressed as simple regular languages. The earlier
examples of â€œlockingâ€ and â€œresource disposalâ€ are both regular:
(lock unlock)and(readclose) , respectively. Each speciï¬cation
is quantiï¬ed over a domain of possible â€œusage scenarios,â€ which is a
general way of capturing the notion that the properties only restrict
related function calls, e.g.lock andunlock calls on the â€œsame Lock
objectâ€ or read andclose calls on the â€œsame ï¬le descriptor.â€
Our tool is implemented for programs written in the Java program-
ming language. For simplicity we focus on temporal properties of
function calls on a single object ; that is, our domain of â€œscenariosâ€ is
the set of objects at runtime. Note that we use receiver objects solely
as a convenient and reliable way of relating sets of method calls
through data. No aspect of our implementation is fundamentally
restricted to object-oriented systems.
Remark. A related concept is typestate [35], the notion that indi-
vidual types have a high-level â€œstateâ€ that dictates when certain
operations ( e.g.method calls) are legal, i.e.when they do not violate
aninternal class invariant. Because we focus on single-object prop-
erties, it is tempting to view DSI narrowly as a form of â€œtypestate
inference.â€ Our technique certainly willvalidate typestate proper-
ties (the two examples above fall into this category) but it is far
Dynamic Specification Miner  
(Stripped of heuristics)  
Temporal Function -Call DSI  Experimental Harness  Experiment Selection  Experiment Execution  
 
Result 
Analysis  Experiment  
Validity Tests  Determinism Check  Test Coverage  
Runtime Monitor  Lazy Evaluation  
Framework  delay() 
force() User -Defined  
Tests  assert 
x==0 
Program  
Transformations  Randomized 
Differential  
Tests  Figure 3: Implementation architecture.
more general. By using â€œoverall system correctnessâ€ as an oracle
rather than â€œobeys a preexisting local class invariant,â€ our tool can
(and frequently does) infer interesting domain-speciï¬c properties
like that shown in Figure 1c: the class does not crash when used
incorrectlyâ€”no local class invariant is violatedâ€”but it eventually
causes a violation of a higher-level system speciï¬cation.
3.2 Temporal Function-Call DSI
Our temporal function-call implementation of DSI takes as input:
1. a Java program, and
2. a mined temporal speciï¬cation over a set of function calls.
It runs a series of automated experiments, returning as output:
1. â€œ(likely) true speciï¬cationâ€, or
2. â€œ(likely) false speciï¬cationâ€ (with supplemental details).
The high-level architecture appears in Figure 3. Execution occurs
in two phases: Preamble andExperiment. The ï¬rst involves con-
structing a set of â€œexperimentsâ€ that aim to test the necessity of the
mined speciï¬cation. We implement this process using static program
transformations that use a novel lazy evaluation framework for Java
programs. The second phase is dynamic, running and interpreting
these experiments with respect to a correctness oracle, which we
approximate through testing.
Remark. Testing for â€œnecessity for correctnessâ€ is superï¬cially sim-
ilar to testing for control ï¬‚ow or data ï¬‚ow dependencies, a heavily-
studied subject. Lack of any control or data dependence is sufï¬cient
cause to invalidate a speciï¬cation, but it is far from necessary. This
issue is analogous to the distinction between strong andweak muta-
tion testing [22].
Mining a set speciï¬cations to validate for a given program is straight-
forward. We make use of an existing dynamic inductive speciï¬cation
inference tool [17] that has had its heuristics disabled.
3.3 Phase I: Preamble
Our temporal properties describe ordering relationships between
sets of function calls. We testa mined speciï¬cation by transforming
the input program, in various ways, so that all relevant function calls
are reordered in a speciï¬cation-violating way.
Transforming Java Programs Reordering function calls in real
software projects is problematic. Highly local cases are simple:
if two function calls appear on subsequent lines, their parameters
tend to draw from the same variable scope, simplifying the actual
transformation. In our experience, though, the source code of actual
projects tends to form a complex and rigid â€œscaffoldingâ€ that is
41GeneratorAdapter gen = ...
2/* Set up 'type' and 'constr' variables */
3gen.loadThis();
4/* Other 'gen' invocations */
5/* Possibly crossing procedure boundaries */
6gen.invokeConstructor(type, constr);
a.Original source code.
1Thunk t; // Global, known location
2GeneratorAdapter gen = ...
3/* Set up 'type' and 'constr' variables */
4t = delay({gen.loadThis();});
5/* Other 'gen' invocations */
6/* Possibly crossing procedure boundaries */
7gen.invokeConstructor(type, constr);
8force(t)
b.Transformed source.
Figure 4: The essence of our transformation. Delaying the ï¬rst
invocation until the second has executed violates the mined
speciï¬cation.
difï¬cult to modifyâ€”and the most important and subtle properties
are likely to be those that are not conï¬ned to a pair of sequential
lines of code.
We solve this problem by implementing a robust lazy evaluation
framework for Java programs. Put simply, our framework lets us
capture an arbitrary Java function call andits parameters in an exe-
cutable function object, save it, and execute it later. It brings to Java
the concept of promises in eager functional languages like Scheme.
The entry point is analogous to the delay() primitive in Scheme, but
slightly generalized: given an arbitrary sequence of (straight-line)
Java bytecode, our framework 1) functionally abstracts it and 2)
creates a closure with its (eagerly-bound) parameters, thus convert-
ing it into a thunk. This object may then be executed at any time,
immediately or later, by an analogue of Schemeâ€™s force() primitive.
Lazy evaluation greatly simpliï¬es the task of reordering function
calls. Our transformation occurs at the bytecode level, but it maps
conceptually well on to source code. An example of the essence of
our transformation appears in Figure 4. The higher level operation
in the ï¬gure is â€œdelay the ï¬rst function call until point pâ€. This
operation is the basis of all of our transformations, and the remaining
questions are when andwhere to apply it.
Surveying Behavior Our selection technique relies on proactively-
collected information about the runtime behavior of the program.
Figure 5 provides an overview of the complete DSI process; the left
pane depicts this Preamble phase. Before selecting experiments, we
execute the programâ€™s test cases (the same tests that will be used
during the Experiment phase) twice: once unmodiï¬ed and once
â€œinstrumented,â€ which collects a property-related trace. We then use
this trace to generate our set of transformations. In addition, these
â€œpilot runsâ€ allow us to perform various sanity checks, including:
Is the property actually exercised in these test cases? [if not,
then our â€œbrokenâ€ programs will certainly be judged â€œcorrect,â€
causing a false invalidation.]
Is the property satisï¬ed by the program? [if not, the currently-
passing tests imply this is a trivially false speciï¬cation]
Is the programâ€™s behavior deterministic enough to allow ex-
perimentation? [if not, our experiment selection algorithm
may fail.]
The third check is not as stringent as it appears: we use a ï¬‚exible
form of execution indexing that tolerates a great amount of variation
in program behavior.Selecting Effective Experiments Consider the mined speciï¬ca-
tion â€œmethod fooshould always be called before method bar.â€ There
are many ways to violate this speciï¬cation:
Remove all calls of fooandbarfrom the program entirely.
Randomly delete calls of foo/bar throughout the program.
Reorder the calls by â€œdelayingâ€ all runtime invocations of foo
until the program exits.
Reorder the calls by â€œdelayingâ€ each invocation of fooby the
minimum amount of time necessary to cause a violation.
These actions differ in the amount of change, or â€œdisturbance,â€ they
cause on the target system, and the fourth, least intrusive option
appears most sensible. There is an analogy here to traditional sci-
entiï¬c experimentation and the issue of control. Here, we wish to
answer the question, â€œis the stated relationship between fooandbar
necessary for correctness?â€ A well-designed experimental answer
should vary precisely that relationship and leave unchanged all other
aspects of the programâ€™s execution. In realistic software projects,
full, behavior-isolated â€œcontrolâ€ will be generally impossible: speci-
ï¬cations overlap and program components communicate. Rather, as
the examples demonstrate, it tends to be a matter of degree.
Selection Algorithm The preceding line of reasoning led us to
anexperiment selection algorithm that strives for completeness
and control. We respect completeness by generating test programs
that violate each potential binding of the property at runtime. We
maintain control by doing so in as minimally intrusive manner as
possible. That is, each time the elements of a mined speciï¬cation
are used at runtime, we delay the minimum number of function calls
by the minimum amount of (execution) time necessary to violate
the property.
Our experiment selection algorithm is listed as Algorithm 1.
It takes as input 1) a mined speciï¬cation and 2) the previously-
described trace of the programâ€™s speciï¬cation-related method calls.
It returns an â€œinstructionâ€ for a minimal experiment, (idx;len),
which can be interpreted as, â€œThe minimally-intrusive way to violate
the given speciï¬cation is to delay() theidxthmethod call by len
calls.â€ This algorithm ensures minimality by taking a brute-force
approach: it simply evaluates all possible transformations. In the
worst case, this algorithm runs in time quadratic in the length of
the trace; in practice, it is much closer to linear: the set of â€œtrialsâ€
(line 2) is eagerly pruned (lines 10â€“18) and remains consistently
small. Note that this algorithm is formulated in terms of a single
â€œusage scenario,â€ e.g.the calls surrounding a single ï¬le descriptor,
and it returns a single experiment. In practice, we process multiple
scenarios simultaneously and generate a suite of experiments.
Remark. The simple primitive â€œdelay a call until time pâ€ is powerful:
for example, multiple functions can be simultaneously delayed and
re-emitted in any order. In theory, this â€œdelay primitiveâ€ is notpow-
erful enough to â€œbreakâ€ ( i.e.transform to rejecting) every accepting
string of any arbitrary (non-total) regular language. Fortunately, in
practice, every valid trace of every regular speciï¬cation pattern we
have encountered in our work can be â€œbrokenâ€ by delaying just a
single event, albeit by varying amounts of time. We have performed
a more thorough theoretical investigation of this problem, which we
have omitted for brevity.
Other Implementation Notes One complication to our otherwise
simple process is caused by the presence of explicit return values
from the functions we â€œdelay:â€ if a function is not evaluated, we can-
not know what it will return. We solve this problem by implementing
a model of what a sensible programmer would do in this situation,
inspired by our principle of â€œminimally intrusiveâ€ experiments. We
5Algorithm 1 Algorithm for selecting an experiment for a given
usage scenario. Returns a minimally-disruptive perturbation that
causes a violation of a mined speciï¬cation.
Input: P:(S=fm1; m2;:::g;S;s0;d;F)
Mined regular speciï¬cation over methods fm1; m2;:::g
T:hcall:(idx;method );:::i
An indexed runtime trace of method calls for one â€˜usage
scenario,â€™ where method2fm1;m2;:::g
Output: (idx;len)
A minimal perturbation: delaying the call attrace index
â€˜idxâ€™byâ€˜lenâ€™ number of calls 1) violates Pand 2) minimizes
â€˜lenâ€™ over all such violating transformations.
1:strace s0 .The state of the trace w.r.t. speciï¬cation P
2:E fg .Set of trial experiments: f(idx;len;s)gIf the call at â€˜idx â€™
were to be â€˜delayâ€™ed by â€˜lenâ€™ calls, Pwould be in state s.
3:min NULL .Minimum-amount perturbation
4:
5:for all call:(idx;method )inTdo
6: for all trial :(idx;len;s)inEdo .trial: sis the state of P
had the call at trial:idxbeen delayed
7: trial: s NEXTSTATE(P;trial: s;call:method )
8: trial:len trial:len+1
9: sforced NEXTSTATE(P;trial: s;T[trial: idx])
.sforced is the state of Pif â€˜forcedâ€™ now
10: ifISREJECTING (sforced)andISSINK(sforced)then
11: ifmin=NULL ortrial:len<min:lenthen
12: min (trial:idx;trial:len)
13: end if
14: E Entrial
15: else if min6=NULL andtrial:lenmin:lenthen
16: E Entrial
17: end if .Perf. optimization: track minimum and prune early
18: end for
19: E E[(call:idx;0;strace) .Create a new trial
20: strace NEXTSTATE(P;strace;call:method )
21:end for
22:for all trial :(idx;len;s)inEdo .Tabulate remaining trials
23: ifmin=NULL ortrial:len<min:lenthen
24: min (trial:idx;trial:len)
25: end if
26:end for
27:return min
implement a simple type analysis that allows us to replace the return
value of a delayed call with the value of the â€œnearest-deï¬ned local
variable of the appropriate typeâ€ (or the language-deï¬ned default
value if one is not found). This general deï¬nition automatically
captures many intuitive actions, including reusing the return value
from a previous call (among others).
Multithreaded programs caused complications as well. Avoiding
any single-threaded assumptions handled most issues, but our early
experiments revealed several fundamental challenges. For one, we
may â€œmoveâ€ a function call to a program point at which an impor-
tant lock is no longer held. To solve this issue, our lazy analysis
framework makes note of the locks held during a runtime invocation
ofdelay() and optionally attempts to reacquire them, if necessary,
when the thunk is force()ed . In another case, a delayed call was
indirectly responsible for some event that, if omitted, would cause a
second call to block indeï¬nitely, creating a deadlock. In this case,
we instituted a global â€œinactive timeoutâ€ on our experiments: if the
subject program makes no forward progress after a period of time,
we forcibly terminate the program.The execution of these initial runs, tests, and experiment selection
algorithms form the entirety of the Preamble phase. At its conclu-
sion, we have produced a set of transformed experimental programs
that are ready to be evaluated.
3.4 Phase II: Experiment
The Experiment phase is conceptually simple: we run each ex-
periment in the suite of transformed programs and interpret the
results.
Testing as an Oracle Our approximation of a correctness oracle
istesting. This portion of our tool is pluggable to allow the use of
user-deï¬ned tests and test oracles. In addition, we also provide a
default implementation based on randomized regression testing. We
ï¬rst run the unmodiï¬ed, assumed-correct program on random inputs.
We record the input/output behavior on these inputs as a behavioral
proï¬le, which then becomes our test oracle. This process is similar
to Differential Testing [ 14], which uses automatically generated
tests to test modiï¬ed software for regressions.
Analysis of Results At a high-level, the only important output is
the success or failure of each individual test: failures suggest the
mined speciï¬cation is valid ; successes suggest it is false. However,
exactly how a test executes and fails can be useful knowledge. Re-
gardless of the propertyâ€”the precise relationship it deï¬nes or the
number of functions it referencesâ€”each experiment reduces to de-
laying a single function call until a second call completes (albeit with
other calls possibly executing in between). This simplicity allows
us to analyze a ï¬nite set of cases that may arise during execution;
these cases are depicted in the right pane of Figure 5.
Normal: This is the standard, unmodiï¬ed case for reference.
Function f1executes, followed by f2, which is followed by
normal execution.
Stage 0: f1has been delayed, but execution failed before
reaching f2. The experiment failed at a very fundamental
level: we could not violate this property using our standard
program transformations. Interpretation of this case could fall
in either direction: the tests did fail, but the experiment was
not actually â€œconducted.â€
Stage 1: Execution fails while executing f2. This is indicative
of a real speciï¬cation, but the circumstances also suggest ad-
ditional information: f1, the delayed call, appears to directly
or indirectly establish f2â€™sprecondition.
Stage 2: Execution fails while forcing the execution of f1.
Once again, this is evidence of a real speciï¬cation, but it also
reveals that f2puts the program in a state in which f1cannot
safely execute. An example might include f1involving the
â€œuseâ€ of a resource and f2â€œclosingâ€ it.
Stage 3: The experiment fully completes and execution contin-
ues as in the normal case. If the tests pass, then we have strong
evidence that the speciï¬cation is false: obeying it appears to
beunnecessary for correctness. If the tests fail, then we have
potentially revealed what may be a particularly subtle and
important true speciï¬cationâ€”one that, if violated, silently
puts the system in an undeï¬ned error state.
3.5 Validity and Limitations of Results
In the ideal, we have a fully representative universe of perfectly
controlled experiments executed under fully exhaustive tests, and
DSI always classiï¬es valid and false speciï¬cations correctly. This
ideal is impossible both in theory and in practice, imposing sev-
eral limitations on the techniqueâ€”limitations that also apply to
speciï¬cation inference in general.
6Preamble Phase  Experiment Phase  Sanity and Validity Tests  Property -
related Trace  Baseline 
Output  
Experiment Selection  Experiments: 
Collection of 
â€œDelay by nâ€ 
Transformations  Experiment 
Execution  Spec  Not 
Spec  
Results  
Case Analysis: Each Individual Experiment  Normal  Stage 0: 
Failure  Stage 1:  
f2 Crash  Stage 2:  
f1 Crash  Stage 3:  
Complete  
f1 
f2 f1 f1 
f2 f1 
f2 f1 
f2 
f1 f1 Runtime 
Execution  
Trace  
Test  
Outcome  Baseline 
Run  Instrumented 
Run  Program  Mined 
Spec  Figure 5: The complete DSI process for temporal function-call properties.
False Validations DSI can misjudge false speciï¬cations as true.
Two main issues surround DSIâ€™s false validations: false causal
inference andovergeneralization. Both are ultimately a result of
problems with DSIâ€™s experiments.
False causal inference arises when DSI attempts to violate a
speciï¬cation, but that violation causes unintended, non-speciï¬cation-
related side effects. As we discussed earlier, this issue relates to the
matter of experimental control, and perfectly controlled automated
experimentation within complex, interconnected software systems
is impossible in the limit. We have, however, sought to mitigate this
problem with our notion of minimally-intrusive transformations, and,
to a lesser extent, our choice of domain. Temporal speciï¬cations are
a natural ï¬t for DSI: they specify the ordering of method calls, a
separable concern that can be modiï¬ed fairly independently of the
methods themselves.
Overspeciï¬cation occurs due to a lack of sufï¬cient examples of a
speciï¬cationâ€™s usage in a code base. Consider a hypothetical speciï¬-
cation: â€œ List.size() must be called before calling List.get(int) .â€
It is arguably false: although it reï¬‚ects a sensible bounds check-
ing practice, there are plenty of use cases where it need not be
followed. If those alternate use cases are not reï¬‚ected in the pro-
gram, though, DSI will only generate failing experiments and judge
it valid. This class of false inference is naturally more prevalent
when DSI evaluates more â€œintended-generalâ€ speciï¬cations, such as
those over standard libraries. There is an interesting nuance here,
however: many apparently-overspeciï¬ed false speciï¬cations may
in fact be valid. If every List in one speciï¬c project does always
have the possibility of being empty, then DSI has classiï¬ed a true,
project-speciï¬c reï¬nement of a more general speciï¬cation.
False Invalidations DSI can also misclassify true speciï¬cations as
false. The main issue here is conceptually simpler than the previous
cases: software testing is naturally incomplete, so it is possible
for the â€œspec-violatingâ€ experimental program to be misjudged as
correct. We avoid the more egregious cases of this problem in
practice with the Preamble phaseâ€™s sanity checks that ensure the test
cases exercise the property-related behavior. In other words, we run
all experiments along the paths of known test cases, giving us a form
of â€œpartial completeness.â€ Note, though, that these checks do not
guarantee that the test suite (especially its oracle) is perfect. In any
case, DSI provides a very concrete counterexample that is likely to
be much easier to inspect than a speciï¬cation on its own: a test case
in the form of a similar alternate program that both 1) violates the
mined speciï¬cation and 2) is apparently correct.The Inability to Violate a Speciï¬cation Occasionally, DSI is
unable to violate a given mined speciï¬cation. This anomaly is sur-
prisingly useful in practice. We believe our toolâ€™s ability to â€œbreakâ€
speciï¬cations is comparable in power to a human programmer: it
lacks human ingenuity, but it does have access to the unique and
powerful lazy analysis primitives. If DSI cannot violate a proposed
speciï¬cation in any reasonable way, that speciï¬cation is likely to be
unimportant even if technically true.
An example will clarify this point. Our implementation works by
reordering function calls. Consider the following code snippet:
1public String getResult() {
2 return this.calc.compute();
3}
A simple inductive inference tool may observe that getResult and
compute always execute in sequence at runtime and may present the
relationship as a speciï¬cation. Note, though, that the structure of
the program prevents any sensible violation. A programmer would
quickly dismiss this speciï¬cation as false for the very same reason
that DSI would fail to (even begin to) prove it true: neither DSI nor
a programmer could possibly violate it.
This speciï¬c problem of â€œfalse speciï¬cations caused by one func-
tion calling anotherâ€ has been addressed in the literature through
purpose-built heuristics, such as the â€œcontrol-ï¬‚ow artifact ï¬lterâ€ in
Yang et al. â€™s Perracotta tool [ 38]. DSIâ€™s natural â€œfailure to violateâ€
elegantly generalizes and handles this and many other cases of â€œspu-
rious speciï¬cationsâ€ without the need for heuristics.
Our implemented tool is robust, scalable, and general. In the
following section, we present the results of a case study of our tool
on real, widely-used Java projects.
4. CASE STUDY
This section presents the results of a case study of our DSI tool
for Java programs. We have sought to answer the following research
questions.
1.Is our tool robust? Does it function on complex, real-world
software?
2.What are the characteristics of DSI-validated speciï¬cations
of set of real-world Java programs?
3.What are the practical strengths and limitations of our tool,
and how do they translate to the DSI methodology as a whole?
7We continue with a discussion of our experimental setup, which is
followed by a presentation and an analysis of our results.
4.1 Experimental Setup
Our test subjects are a set of Java programs drawn from the Da-
Capo benchmark suite (version 9:12-Bach ).1DaCapo differs from
traditional â€œmicrobenchmarksâ€ in that it is formed from over a mil-
lion lines of code of real, widely-used applications, creating an
uncommonly realistic workload.
DSI operates on mined speciï¬cations. As we noted earlier, we
modiï¬ed an inductive speciï¬cation learning tool [ 17] to function
without heuristics. We conï¬gured it to ï¬nd all instances of simple
sequences: pairs of method calls that appear to be obeying a se-
quential ordering restriction between them. For brevity, we omit the
implementation and experimental details of this process.
Remark. The â€œsequenceâ€ template is simple, almost to the point
of being simplistic. In practice, it captures a surprisingly broad
amount of speciï¬cation behavior. Our previous work on the Javert
tool [ 18] has demonstrated that almost all temporal function-call
speciï¬cations can be decomposed into fundamental pieces, simple
sequences and small loops. In addition, note that our DSI tool is
not inherently limited to simple patterns, both in theory and in the
current implementation: it can work with any regular speciï¬cation
pattern over any number of distinct functions.
We ran all experiments in parallel on several 64-bit Linux servers,
each conï¬gured with with Intel processors (Xeon and Core 2) and
the 64-bit Oracle Java Virtual Machine, Server Edition, version
1.6.0_25.
4.2 Results
A summary of our results appears in Figure 6. Our tool individ-
ually analyzed 7;848mined speciï¬cations, systematically judging
each as a likely speciï¬cation orlikely non-speciï¬cation. In each
case our tool performed robustly, both validating and invalidating
speciï¬cations within large, complex software projects.
Performance Performance was acceptable: the majority of speciï¬-
cations were analyzed in under two minutes. Our task is embarrass-
ingly parallel as well, a fact we utilized fully in our study by using
several compute servers. Notable performance exceptions were the
Jython and Eclipse benchmarks. Jython and Eclipse contain many
mined speciï¬cations whose violation hinders termination, forcing
our system to often wait until a conservative timeout had expired
(ï¬ve minutes) before proceeding. In practice, this timeout can be
reduced to a value more appropriate to a particular project.
Likely Non-Speciï¬cations Figure 6 lists detailed results of our
toolâ€™s experiments on the 7;848input speciï¬cations. The ï¬rst group
of four columns describes the mined speciï¬cations our tool judged
aslikely non-speciï¬cations.
The ï¬rst column (â€œCould Not Violateâ€) counts speciï¬cations that
could not violate in any way, and as discussed earlier, were judged to
be â€œunimportant.â€ Many of these cases were a result of the â€œcontrol-
ï¬‚ow artifactâ€ speciï¬cations described earlier (one function directly
or indirectly calling another), but there were several other cases as
well that were naturally captured by our toolâ€™s â€œcannot-violate im-
plies non-importanceâ€ principle. For example, one case involved the
static type system preventing us from moving an objectâ€™s constructor
call after its ï¬rst true method call, the more abstract principle here be-
ing â€œordering restrictions involving constructors cannot be violated
1A small minority of the standard benchmarks were omitted solely
due to technical problems with their execution under instrumentation.
We have no reason to believe our results will signiï¬cantly differ once
they are included.by programmers.â€ This eliminated the need to write a â€œconstructor
ordering relationships are falseâ€ heuristic.
The second two columns correspond to â€œStagesâ€ of execution
described earlier and depicted in Figure 5. In the second column
(â€œStage 0â€), every experiment resulted in a program crash soon after
we â€œdelayedâ€ the ï¬rst function call. Stage 0 results generally fall
into two categories:
1.The fact appears to be fully â€œenforcedâ€ at runtime, either
through assertions or other checks, and is thus impossible to
violate on tested code.
2.There is a more speciï¬c and relevant speciï¬cation we should
be analyzing instead. For example, if functions a,b, and c
all execute in sequence at runtime and a crucial relationship
exists between aandb, we will be unable to run a successful
experiment involving aandc. Our heuristic-less speciï¬cation
miner forces us to test every possible mined speciï¬cation, so
we would eventually properly classify the crucial a/bspeciï¬-
cation. The general practical implication is that DSI can more
readily experiment on smaller, more manageable pieces (the
transitive reductions) of larger speciï¬cations.
The third column lists those mined speciï¬cations whose experi-
ments all fully completed, violating the property, but continued
to be judged correct by the relevant tests. The experiments can
be inspected by a programmer and serve as a form of â€œcertiï¬cateâ€
demonstrating that the lack of the speciï¬cationâ€™s necessityâ€”our
primary goal. Note the sheer volume of this category: 55% of
the mined speciï¬cations (over 4;000) were invalidated, every one
of which had the potential to be called a â€œtrue speciï¬cationâ€ by
an inductive learning tool and to waste a programmerâ€™s time. We
note also that the standard frequency heuristic does notappear to
properly classify these â€œshown unnecessaryâ€ speciï¬cations. In our
results, signiï¬cant portions of both rarely- and frequently-executed
speciï¬cations were invalidated.
In response to Research Question 1, we are encouraged by the
fact that this level of interference and experimentation was possible
in large and complex software projects: our initial hypothesis was
that almost anyruntime perturbation of function ordering would
result in a fairly immediate crash.
As noted in the previous section, it is possible for DSI to mis-
takenly invalidate a speciï¬cation. If a transformation successfully
violates a speciï¬cation, it is possible for it to â€œinfectâ€ the state of a
system in such a way that existing tests do not detect. We selected a
limited random sample of 25 of these invalidated speciï¬cations and
manually analyzed their usage in each projectâ€™s source code. None
of them appeared to be improperly classiï¬ed: the ordering exhibited
in the source code is apparently coincidental.
Likely Speciï¬cations To answer RQ2, we analyze the set of likely
speciï¬cations in more depth. The second group of columns in
Figure 6 contains counts of the mined speciï¬cations our tool judged
to be likely true . Each column corresponds to a â€œstageâ€ of execution
described earlier and depicted in Figure 5. The ï¬rst two columns
of this group correspond to fairly standard and local â€œtypestateâ€
speciï¬cations:
1.Speciï¬cations whose experiments all end in Stage 1 exhibit a
precondition relationship: the ï¬rst function call establishes (a
part of) the precondition of the second, and delaying it causes
the appropriate crash.
2.Speciï¬cations whose experiments all end in Stage 2 exhibit a
state transition relationship: the ï¬rst function call was legal
in its original context, but it becomes illegal after the second
8L i k e l y N o n - S p e c i f i c a t i o n s L i k e l y S p e c i f i c a t i o n s
Mean Time/ Could Not Stage 0: Stage 3: Stage 1: Stage 2: Stage 3:
Benchmark Spec. (s) Violate Failure Complete Total f2Crash f1Crash Complete Total
avrora 47.1 16 91 281 388 18 2 52 72
batik 69.4 190 543 1171 1904 27 5 127 159
eclipse 225.9 357 297 627 1281 25 45 75 145
h2 102.8 19 96 325 440 18 2 37 57
jython 936.4 135 129 200 464 10 8 11 29
luindex 35.1 8 125 256 389 10 32 32 74
lusearch 91.4 16 55 73 144 3 6 14 23
pmd 79.1 116 152 593 861 12 1 37 50
sunï¬‚ow 34.1 4 79 64 147 13 0 36 49
xalan 76.0 102 278 713 1093 18 6 55 79
Total 963 1845 4303 7111 154 107 476 737
Figure 6: Detailed results of our case study. Each entry is a count of the number of mined speciï¬cations our tool judged to fall into the
given columnâ€™s category. Times are given in terms of single-CPU time per property, which is independent of our parallel execution
of the larger collection of experiments.
function executes ( e.g.attempting to use a resource after it
has been closed).
Most interesting are the speciï¬cations whose experiments all fully
completed (Stage 3). In these cases, each experiment silently cor-
rupted the program state and caused the tests to fail at a laterâ€”
sometimes much laterâ€”time. A small, randomly sampled collection
of these speciï¬cations follows.
In Lucene, delaying a â€œcommitâ€ operation on an index until
after it is closed corrupts the index but causes no overt failure.
In H2, a connection information object will silently return a
bogus password hash from a connection information if read
before initialization is complete.
Various XML parsers in the projects require handlers to be
set before parsing begins and will not warn the user if none
has been set (our sample included two of these).
In Sunï¬‚ow, the SunflowAPI objectâ€™s initialization is idiomatic.
Moving any part of it until after a call to render causes incor-
rect output but no obvious crash. Two other examples in our
sample followed a similar â€œidiomatic initializationâ€ pattern.
We are continuing to analyze these results in depth, but thus far this
case study has suggested that our DSI tool is general, robust, and
effective.
4.3 Discussion
To further answer Research Questions 2 and 3, we analyze our
results in further detail.
Conï¬dence in Results DSI is systematic and appears to yield
strong, algorithmically-decided results. However, as we discussed
in Section 3.5, our methodology is subject to a number of potential
threats that may lead to incorrect classiï¬cation. We have not yet
manually validated all 8;000classiï¬ed speciï¬cations, but we are
nonetheless conï¬dent in the quality of the majority of our results.
One notable exception is the class of speciï¬cations (weakly)
classiï¬ed as false during â€œStage 0: Failureâ€ (Column 2 of â€œLikely
Non-Speciï¬cations in Figure 6). Recall that these are speciï¬ca-
tions for which we delay the ï¬rst call, but the program crashes
soon after, suggesting that they are difï¬cult to meaningfully violate.
In the previous section, we did classify these â€œfailed experimentsâ€
into two fairly benign general categories, but we believe that withcontinued exploration we may encounter cases for which a human
programmer could have conducted a successful experiment. If this
occurs, however, we intend to use those examples to improve DSIâ€™s
transformations.
On the other hand, we are generally conï¬dent in DSIâ€™s systematic
invalidations: to date, we have observed no false classiï¬cations
caused by insufï¬ciently powerful tests. We are also optimistic about
our continued evaluation of the â€œtrueâ€ speciï¬cations. Our continuing
manual validation of these speciï¬cations has turned up no false
positives thus far.
Performance and Usefulness Obscured by the â€œaverage time per
speciï¬cationâ€ performance statistic is the fact that this case study
consumed two full weeks of CPU time. Our prototype is certainly
computationally expensive: for each mined speciï¬cation, it gener-
ates a family of transformed programs and runs an entire test suite
several times. We note that DSI is currently implemented straight-
forwardly and that we expect many signiï¬cant optimizations to be
possible, including, for example, performing multiple transforma-
tions at once or taking advantage of a capture/replay framework.
We also emphasize that our results form a rich dataset on which
simpler approximations may be trained, similar to Le Goues and
Weimerâ€™s [ 24] lightweight statistical model for classifying mined
speciï¬cations.
5. RELATED WORK
This section discusses our automated speciï¬cation validation tech-
nique in terms of several lines of related work.
Inductive Speciï¬cation Inference Inductive speciï¬cation infer-
ence techniques have been developed for a variety of domains.
Kremenek et al. have presented a framework [ 23] based on proba-
bilistic inference that reï¬‚ects the essence of the inductive process:
leveraging beliefs about software to infer general speciï¬cations from
speciï¬c examples of programs. Several targeted techniques infer
temporal speciï¬cations, the subject of our own implementation of
DSI. Both dynamic [ 3,9,18,25,38] and static [ 34] techniques fol-
low the same general approach: they observe temporal relationships
in programs and inductively elevate them to speciï¬cations. Other
successful application domains of inductive speciï¬cation inference
are assertions over program state [ 13,20], determinism speciï¬ca-
tions for concurrent programs [ 6], and function contracts [ 30]. Less
formal approaches include lightweight â€œprogramming rulesâ€ [ 7,26],
9which have been particularly effective at revealing programming
mistakes.
These techniques all confront one central issue: precision. Gener-
alization is unsound, and the inductive leap from one program to a
speciï¬cation about an entire class of programs is essentially an edu-
cated guess. This issue is not merely theoretical: a recent study [ 29]
has shown that one third of inductively-learned code contracts are
incorrect or irrelevant (and in our experience, the temporal property
domain can be worse). As a result, speciï¬cation inference research
tends to contain an empirical component evaluating precision, and
speciï¬c techniques have also been proposed to help programmers
debug [4] and ï¬lter [24] erroneous mined speciï¬cations.
DSI provides a new way of approaching the speciï¬cation vali-
dation problem. Rather than speculating about speciï¬cations, our
technique allows one to test them experimentally andautomatically.
As a standalone automated validation procedure, it treats classiï¬ca-
tion as a separate problem and could thus be used to improve any of
these existing inductive techniques.
Validating Mined Speciï¬cations with Testing In recent work de-
veloped concurrently with our own, Nguyen and Khoo [ 28] use
mutation testing to validate one class of temporal speciï¬cation: pre-
condition relationships. If a function call foo() appears to be a
precondition for calling bar() , their technique generates a test that
deletes the call to foo() and simply replaces its return value with
arandomly-generated value. Their technique then classiï¬es the
speciï¬cation as â€œsigniï¬cantâ€ if and only if bar() , speciï¬cally, then
crashes with an exception.
DSI is a much more general technique that targets the entire
class of regular temporal properties, of which preconditions are one
special case. It also implements function reordering, a much more
complex and nuanced form of experimentation than their simple
deletion of function calls (which can be done in our tool as well).
Nguyen and Khooâ€™s work also focuses only on â€œlocalâ€ speciï¬cations:
a tested API must crash immediately with an API-related exception
to be judged signiï¬cant. DSI tests signiï¬cance with a more general
notion of system-level correctness, which fully encompasses both
these â€œlocal preconditionsâ€ and a much larger class of subtle, system-
level speciï¬cations. In addition, Nguyen and Khoo do not discuss
the important threats and limitations we describe in Section 3.5.
Fraser and Zeller use an idea similar to DSI in recent work on
generalizing unit tests [ 16]. We aim to invalidate mined speciï¬ca-
tions; they aim to invalidate inferred assertions for unit tests, and
both techniques use mutation as the underlying technique. Apart
from the technical differences arising from the differing problem
domains, DSIâ€™s general execution somewhat differs as well. Put
plainly, we invalidate speciï¬cations when a known-violating pro-
gram does not appear to be broken. They invalidate assertions when
a known-broken program does not appear to violate the assertion.
Non-Inductive Speciï¬cation Inference Some speciï¬cation infer-
ence techniques are non-inductive. Work on extracting component
interfaces [2,21,36] is superï¬cially similar to inductive speciï¬ca-
tion inference techniques, but the processes are more well-deï¬ned
and involve no inductive generalization. These techniques require
low-level speciï¬cations as input (or they are extracted from explicit
assertions in the code). They then solve the formal problem of
extracting a sound, higher-level â€œmodelâ€ of component usage that
avoids violating any of the given low-level speciï¬cations. In essence,
DSI takes this process and lifts it to entire programs. In place of
a model of a speciï¬c component, we deï¬ne a set of speciï¬cation-
violating program transformations, and in place of low-level speciï¬-
cations, we use system tests.Combining Testing with Speciï¬cation Inference Our implemen-
tation of DSI leverages testing to enhance speciï¬cation inference,
the general idea of which was originally proposed by Xie and
Notkin [ 37]. Dallmeier et al. â€™s Tautoko tool [ 10] also uses test-
ing in a similar way. Tautokoâ€™s problem setup is similar to that of
the â€œcomponent interfaceâ€ tools described earlier: generate a compo-
nent â€œmodelâ€ that avoids errors. In Tautokoâ€™s case, the component
is a Java class that is assumed to crash or otherwise raise an error if
used incorrectly. Tautoko starts with an inductively inferred model,
but it then enhances it in a feedback loop by generating targeted,
exploratory tests. There is a parallel here to DSI: we start with an
inductively learned speciï¬cation and essentially â€œgenerate testsâ€ to
validate it.
Testing â€œNecessityâ€ with Experiments Our implementation of
DSI uses experimentation to infer properties of programs. Ruthruff
et al. [33] describe a general conceptual framework, Experimental
Program Analysis, for conducting experiments within programs; our
methodology can be seen as an instance of this framework. An idea
similar to â€œtesting for necessityâ€ has been used by Renieris et al. in
their study of elided conditionals [31]. In their work, the authors
generate experiments that test the whether or not the outcome of a
conditional statement ( i.e.a branch) affects the outcome of a test,
much as we generate experiments that test the necessity of proposed
speciï¬cations.
Automatic parallelization tools make use of Commutativity Anal-
ysis[1,32], which evaluates the necessity of a given ordering of
program statements. This is similar to how we test the â€œnecessityâ€
of a temporal ordering constraint. Commutativity analysis uses a
far stricter criterion than DSI does in practice: that various order-
ings produce semantically identical results. In our implementation
of DSI, the fact that two functions commute is only one of sev-
eral reasons a speciï¬cation might be invalidated, and the fact that
two functions do notcommute does not imply a given ordering is
necessary to correctness.
Our work is also similar to Mutation Testing [ 11,19]. In mutation
testing, modiï¬ed programs (â€œmutantsâ€) are used to experimentally
test the fault-ï¬nding ability of test suites. In our work, we use
transformed programs that are similar to standard mutants to experi-
mentally test the validity of potential speciï¬cations.
6. FUTURE WORK AND CONCLUSION
This paper has presented DSI, a new methodology for automati-
cally validating speciï¬cations mined from programs. DSIâ€™s novelty
lies in its ability to systematically test a speciï¬cation for necessity
for correctness. We have implemented DSI for the domain of tempo-
ral function-call properties of Java programs. Our implementation
creates experiments through fully automated program transforma-
tions, and it evaluates them with traditional software testing. In a
case study, we demonstrated that our tool is effective on real-world
programs.
Our most immediate future work involves implementing DSI for
other domains to further demonstrate the strengths of the method,
and our early results in this area have been promising. We are also
interested in exploring other aspects of the deï¬nition of importance
of a speciï¬cation. We presently deï¬ne â€œimportantâ€ conservatively:
if a programmer can feasibly violate a speciï¬cation it is important.
Collecting more data on what it means for a speciï¬cation to be
â€œimportantâ€ might allow us to synthesize other effective and testable
deï¬nitions like this â€œability to violateâ€ concept introduced here.
Finally, we are exploring the possibility of using our DSI-validated
directly with other software toolsâ€”without intervention by a human
programmer.
10References
[1]F. Aleen and N. Clark. Commutativity Analysis for Software Paral-
lelization: Letting Program Transformations See The Big Picture. In
Proceeding of the 14th International Conference on Architectural Sup-
port for Programming Languages and Operating Systems (ASPLOS),
ASPLOS â€™09, 2009.
[2]R. Alur, P. CernÃ½, P. Madhusudan, and W. Nam. Synthesis of Interface
Speciï¬cations for Java Classes. In POPL â€™05: Proceedings of the 32nd
ACM SIGPLAN-SIGACT Symposium on Principles of Programming
Languages, 2005.
[3]G. Ammons, R. BodÃ­k, and J. R. Larus. Mining Speciï¬cations. In
POPL â€™02: Proceedings of the 29th ACM SIGPLAN-SIGACT Sympo-
sium on Principles of Programming Languages, 2002.
[4]G. Ammons, D. Mandelin, R. BodÃ­k, and J. R. Larus. Debugging Tem-
poral Speciï¬cations with Concept Analysis. In PLDI â€™03: Proceedings
of the ACM SIGPLAN 2003 conference on Programming Language
Design and Implementation, 2003.
[5]T. Ball and S. K. Rajamani. Automatically Validating Temporal
Safety Properties of Interfaces. In SPIN â€™01: Proceedings of the 8th
International SPIN Workshop on Model Checking of Software, 2001.
[6]J. Burnim and K. Sen. DETERMIN: Inferring Likely Deterministic
Speciï¬cations of Multithreaded Programs. In Proceedings of the 32nd
ACM/IEEE International Conference on Software Engineering, ICSE
â€™10, 2010.
[7]R.-Y . Chang, A. Podgurski, and J. Yang. Finding Whatâ€™s Not There:
A New Approach to Revealing Neglected Conditions in Software. In
Proceedings of ISSTA â€™07, 2007.
[8]F. Chen and G. Rosu. MOP: An Efï¬cient and Generic Runtime
Veriï¬cation Framework. In Proceedings of OOPSLA â€™07, 2007.
[9]V . Dallmeier, C. Lindig, A. Wasylkowski, and A. Zeller. Mining
Object Behavior with ADABU. In WODA â€™06: Proceedings of the
2006 International Workshop on Dynamic Systems Analysis, 2006.
[10] V . Dallmeier, N. Knopp, C. Mallon, S. Hack, and A. Zeller. Generating
Test Cases for Speciï¬cation Mining. In Proceedings of the 19th
International Symposium on Software Testing and Analysis, ISSTA â€™10,
2010.
[11] R. A. DeMillo, R. J. Lipton, and F. G. Sayward. Hints on Test Data
Selection: Help for the Practicing Programmer. Computer, 11(4),
1978.
[12] D. Engler, D. Y . Chen, S. Hallem, A. Chou, and B. Chelf. Bugs as
Deviant Behavior: A General Approach to Inferring Errors in Systems
Code. In SOSP â€™01: Proceedings of the Eighteenth ACM Symposium
on Operating Systems Principles, 2001.
[13] M. D. Ernst, A. Czeisler, W. G. Griswold, and D. Notkin. Quickly
Detecting Relevant Program Invariants. In Proceedings of ICSE, 2000.
[14] R. B. Evans and A. Savoia. Differential Testing: A New Approach
to Change Detection. In The 6th Joint Meeting on European software
engineering conference and the ACM SIGSOFT Symposium on the
Foundations of Software Engineering: Companion Papers, 2007.
[15] S. J. Fink, E. Yahav, N. Dor, G. Ramalingam, and E. Geay. Effective
Typestate Veriï¬cation in the Presence of Aliasing. ACM Trans. Softw.
Eng. Methodol., 17(2), 2008.
[16] G. Fraser and A. Zeller. Generating parameterized unit tests. In
Proceedings of the 2011 International Symposium on Software Testing
and Analysis, ISSTA â€™11, 2011.
[17] M. Gabel and Z. Su. Symbolic Mining of Temporal Speciï¬cations. In
Proceedings of ICSE â€™08, 2008.
[18] M. Gabel and Z. Su. Javert: Fully Automatic Mining of General Tem-
poral Properties from Dynamic Traces. In Proceedings of SIGSOFT
â€™08/FSE-16, 2008.
[19] R. G. Hamlet. Testing Programs with the Aid of a Compiler. IEEE
Trans. Softw. Eng., 3(4), 1977.[20] J. Henkel and A. Diwan. Discovering Algebraic Speciï¬cations from
Java Classes. In ECOOP 2003 - 17th European Conference on Object-
Oriented Programming, 2003.
[21] T. A. Henzinger, R. Jhala, and R. Majumdar. Permissive Interfaces. In
ESEC/FSE-13: Proceedings of the 10th European Software Engineer-
ing Conference Held Jointly with 13th ACM SIGSOFT International
Symposium on Foundations of Software Engineering, 2005.
[22] W. E. Howden. Weak mutation testing and completeness of test sets.
IEEE Trans. Softw. Eng., 8(4), July 1982.
[23] T. Kremenek, P. Twohey, G. Back, A. Ng, and D. Engler. From Un-
certainty to Belief: Inferring the Speciï¬cation Within. In OSDIâ€™06:
Proceedings of the 7th conference on USENIX Symposium on Operat-
ing Systems Design and Implementation, 2006.
[24] C. Le Goues and W. Weimer. Speciï¬cation Mining with Few False
Positives. In TACAS â€™09: Proceedings of the 15th International
Conference on Tools and Algorithms for the Construction and Analysis
of Systems, 2009.
[25] C. Lee, F. Chen, and G. RoÂ¸ su. Mining parametric speciï¬cations. In
Proceedings of the 33rd International Conference on Software Engi-
neering, ICSE â€™11, 2011.
[26] Z. Li and Y . Zhou. PR-Miner: Automatically Extracting Implicit
Programming Rules and Detecting Violations in Large Software Code.
InProceedings of ESEC/FSE-13, 2005.
[27] Z. Li, L. Tan, X. Wang, S. Lu, Y . Zhou, and C. Zhai. Have Things
Changed Now?: An Empirical Study of Bug Characteristics in Modern
Open Source Software. In Proceedings of the 1st Workshop on Ar-
chitectural and System Support for Improving Software Dependability,
ASID â€™06, 2006.
[28] A. C. Nguyen and S.-C. Khoo. Extracting signiï¬cant speciï¬cations
from mining through mutation testing. In Proceedings of the 13rd
International Conference on Formal Engineering Methods, ICFEM
â€™11, 2011.
[29] N. Polikarpova, I. Ciupa, and B. Meyer. A Comparative Study of
Programmer-Written and Automatically Inferred Contracts. In Pro-
ceedings of the Eighteenth International Symposium on Software Test-
ing and Analysis, 2009.
[30] M. K. Ramanathan, A. Grama, and S. Jagannathan. Static Speciï¬cation
Inference Using Predicate Mining. In Proceedings of PLDI, 2007.
[31] M. Renieris, S. Chan-Tin, and S. P. Reiss. Elided Conditionals.
InProceedings of the 5th ACM SIGPLAN-SIGSOFT Workshop on
Program Analysis for Software Tools and Engineering, 2004.
[32] M. C. Rinard and P. C. Diniz. Commutativity Analysis: A New
Analysis Technique for Parallelizing Compilers. ACM Trans. Program.
Lang. Syst., 19(6), 1997.
[33] J. R. Ruthruff, S. Elbaum, and G. Rothermel. Experimental program
analysis: a new program analysis paradigm. In Proceedings of the 2006
International Symposium on Software Testing and Analysis, ISSTA â€™06,
2006.
[34] S. Shoham, E. Yahav, S. Fink, and M. Pistoia. Static Speciï¬cation
Mining Using Automata-based Abstractions. In Proceedings of ISSTA,
2007.
[35] R. E. Strom and S. Yemini. Typestate: A Programming Language
Concept for Enhancing Software Reliability. IEEE Trans. Softw. Eng.,
12(1), 1986.
[36] J. Whaley, M. C. Martin, and M. S. Lam. Automatic Extraction of
Object-oriented Component Interfaces. In Proceedings of ISSTA â€™02,
2002.
[37] T. Xie and D. Notkin. Mutually Enhancing Test Generation and
Speciï¬cation Inference. In A. Petrenko and A. Ulrich, editors, Formal
Approaches to Software Testing. 2004.
[38] J. Yang, D. Evans, D. Bhardwaj, T. Bhat, and M. Das. Perracotta:
Mining Temporal API Rules from Imperfect Traces. In Proceedings of
ICSE, 2006.
11