MitigatingtheConfoundingEffectsofProgram
Dependencesf
orEffectiveFaultLocalization
George K. Baah
Georgia Institute of
Technology
Atlanta, GA 30332
baah@cc.gatech.eduAndy Podgurski
Case Western Reserve
University
Cleveland, OH 44106
podgurski@case.eduMary Jean Harrold
Georgia Institute of
Technology
Atlanta, GA 30332
harrold@cc.gatech.edu
ABSTRACT
Dynamic program dependences are recognized as important
factors in software debugging because they contribute to
triggering the eﬀects of faults and propagating the eﬀects
to a program’s output. The eﬀects of dynamic dependences
also produce signiﬁcant confounding bias when statistically
estimating the causal eﬀect of a statement on the occurrence
of program failures, which leads to poor fault-localization re-
sults. This paper presents a novel causal-inference technique
for fault localization that accounts for the eﬀects of dynamic
data and control dependences and thus, signiﬁcantly reduces
confounding bias during fault localization. The technique
employs a new dependence-based causal model together with
matching of test executions based on their dynamic depen-
dences. The paper also presents empirical results indicating
that the new technique performs signiﬁcantly better than
existing statistical fault-localization techniques as well as
our previous fault localization technique based on causal-
inference methodology.
Categories and Subject Descriptors: D.2.5 [Software
Engineering]: Testing and Debugging— Debugging aids, Di-
agnostics
General Terms: Algorithms, Experimentation
Keywords: causal inference, potential outcome model, match-
ing, fault localization, program analysis, debugging
1. INTRODUCTION
The pervasive impact of software systems and applications
on society requires software developers to ensure that their
products are of high quality. However, software development
is a human process, and developers unintentionally intro-
duce faults into software that cause it to fail under certain
conditions. When software failures are observed, developers
engage in debugging to ﬁnd and ﬁx the fault(s). Fault local-
ization is the part of debugging that involves locating faults
in a program. Because of the size and complexity of many
software products, fault localization is often a diﬃcult and
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
notmadeordistributedforprofit orcommercialadvantageandthatcopies
bearthisnoticeandthefullcitationonthefirstpage. Tocopyotherwise,to
republish,topostonserversortoredistributetolists,requirespriorspecific
permissionand/orafee.
ESEC/FSE’11, September5–9,2011,Szeged,Hungary.
Copyright2011ACM978-1-4503-0443-6/11/09...$10.00.time-consuming task, which contributes signiﬁcantly to the
cost of software development and maintenance.
To reduce the cost of fault localization, researchers have
proposed automated statistical fault-localization techniques
(e.g., [1, 4, 13, 15, 16, 17]). These techniques compute
statistical measures of association between the execution
of individual program statements (or other entities) and
the occurrence of program failures, using execution data
from passing and failing executions (e.g., coverage informa-
tion). The measures are called suspiciousness scores . The
techniques rank program statements in decreasing order of
their suspiciousness scores for examination by developers.
The intuition underlying this approach is that the program
statements most strongly associated with failures are the
most likely to be faulty. However, statistical association
does not necessarily indicate causation [12]. Thus, state-
ments with high suspiciousness scores may not be faulty, and
faulty statements may have low suspiciousness scores. Con-
sequently, existing statistical fault-localization techniques
can perform poorly with certain programs and faults.
In recent work [5], we showed that statistical fault-locali-
zation techniques based on measures of association between
the occurrence of certain runtime events (e.g., coverage of
a given statement) and the occurrence of program failures
are subject to confounding bias , because of inﬂuences be-
tween statements in a program. For example, with a number
of coverage-based statistical fault-localization techniques, a
correct statement scand a faulty statement sfthat happen
to be covered by the same subset of test cases will neces-
sarily receive the same suspiciousness score. More generally,
any other statement that inﬂuences scandsfand also inﬂu-
ences the occurrence of failures will confound the suspicious-
ness scores of scandsf. To accurately assess the distinct
eﬀects of covering scandsfon the occurrence of failures, it
is necessary to adjust for such confounding factors.
To address the confounding inﬂuence of some program
statements on the suspiciousness scores of other statements,
we presented a preliminary causal inference model for soft-
ware fault localization [5] (see Section 2). We used this
model to estimate the causal eﬀect of covering a statement
son the occurrence of program failures. To“control”or“ad-
just” for confounding by other statements, the model incor-
porates a coverage-indicator variable for the forward control
dependence predecessor of s(in addition to a coverage indi-
cator for sitself). The model is grounded in the extensive
body of research on making causal inferences from observa-
tional data, which spans such diverse ﬁelds as econometrics,
the social sciences, epidemiology, statistics, and computer
146
science [18, 21]. We contrasted the model analytically with
sever
al existing fault-localization metrics that do not ad-
dress confounding, and presented empirical results indicat-
ing that the model (1) is signiﬁcantly more eﬀective than
some of those techniques and (2) can be incorporated into
other techniques to improve their performance. These re-
sults suggest that existing methodology for making causal
inferences from observational data is very relevant to statis-
tical fault localization.
However, it is easy to see that our previous model does
not address all possible sources of confounding in fault lo-
calization, such as runtime patterns of data dependences
(data ﬂows), the values taken on by inputs and program
variables, and non-determinism because of concurrency [5].
Making the best use of causal-inference methodology in soft-
ware fault localization will require determining the relative
importance of such factors and devising eﬀective and rea-
sonably eﬃcient ways of controlling for them.
In this paper, we present a new technique that controls for
local patterns of both dynamic data dependences and dy-
namic control dependences that can confound the estimated
causal eﬀect of covering a statement son the occurrence
of program failures, which for brevity we call the failure-
causing eﬀect ofs. Static and dynamic data and control
dependences have long been recognized as important factors
in software testing and debugging, because they contribute
both to triggering the eﬀects of faults and to propagating
those eﬀects to a program’s output [22, 24, 26]. Our tech-
nique uses information about dynamic data and control de-
pendences to reduce confounding bias—more than is pos-
sible with our previous model—and to thereby rank state-
ments more eﬀectively for fault localization.
When estimating the failure-causing eﬀect of a statement
s, confounding bias due to dynamic data and control depen-
dences aﬀecting scan be reduced by considering two groups
of executions: those that cover s(referred to as the treat-
ment group ) and those that do not cover s(referred to as the
control group ). The executions that cover sshould have the
same pattern of dynamic data and control dependences as
the executions that do not cover s, in which case the groups
are said to be balanced . In practice, the set of test cases or
operational executions used in statistical fault localization
is usually given, and the subset of them that cover and do
not cover a particular statement scannot be assumed to be
balanced.
To achieve relative balance between the treatment and
control groups and thus reduce confounding bias, our tech-
nique employs a classical causal-inference technique called
matching . For each program statement s, the executions in
the two groups are reorganized so that they are similar with
respect to dynamic data and control dependences that di-
rectly aﬀect s. Each execution ethat covers sis matched
with the execution not covering sthat is most similar to e, as
measured by applying the Mahalanobis-distance metric [18]
to dynamic dependence information collected from the ex-
ecutions. After matching, the failure-causing eﬀect of sis
estimated based on the outcomes of the matched executions.
In this paper, we also present empirical results indicat-
ing that our new technique yields fault-localization rankings
that are more eﬀective than those obtained with our previ-
ous causal model [5] and with several well-known statistical
fault-localization techniques. For example, our new tech-
nique performs better than our old technique on 61.4% of272 faulty program versions, with an average fault-localization
improvement of 8.57%. Our new technique also performs
better than Tarantula [13] on 72.06% of the versions, with
an average fault-localization improvement of 11.44%.
The main beneﬁt of our work is that it provides a practical
way to use both data and control dependence information to
reduce confounding bias and thereby improve the eﬀective-
ness of statistical fault localization. Another beneﬁt is that
basing our technique on accepted causal-inference methodol-
ogy will help to put statistical fault localization on a sounder
footing and stimulate further improvements.
The contributions of the paper are
•A new statistical fault-localization technique that uses
a new causal model, based on dynamic data and con-
trol dependences, together with covariate matching to
more accurately estimate the causal eﬀect of covering
a particular program statement on program failures.
•Empirical results indicating that our new technique is
more eﬀective than our previous causal technique and
other statistical fault-localization techniques.
2. BACKGROUND
In this section, we ﬁrst present background on the potential-
outcome model for causal inference and the dynamic program-
dependence graph. We then describe our previous causal-
inference model for statistical fault localization [5], which
is based on the potential outcome model [18], on Pearl’s
Structural Causal Model [21], and on dynamic program-
dependence analysis [10].
2.1 PotentialOutcomeModel
Thepotential-outcome model [18] provides a theoretical
basis for estimating causal eﬀects using data from either
observational studies or randomized experiments. Suppose
that a researcher wants to estimate the eﬀect of a new or
existing treatment, and suppose further that a set of units
or subjects are available to the researcher. Let Tbe a binary
variable that takes on the value ti= 1 for a unit uiifui
receives the treatment and that takes on the value ti= 0
ifuidoes not receive the treatment. Let Yrepresent the
outcome for an arbitrary unit. The potential outcome model
associates with Ytwopotential-outcome random variables :
Y1andY0. Variable Y1represents the potential outcome
for atreated unit, and variable Y0represents the potential
outcome for an untreated (control) unit. The individual-
level causal eﬀect of treatment for a unit uiis deﬁned as
y1
i−y0
i, where y1
iandy0
iare the realizations of Y1andY0
for unit i. We assume that for any given unit, only one
ofY1andY0can be observed; the other variable’s value
iscounterfactual (counter to the facts) for that unit. This
implies it is impossible to estimate y1
i−y0
idirectly.1
Because individual-level treatment eﬀects cannot be esti-
mated, average treatment eﬀects are estimated instead. For
the scenario above, the average treatment eﬀect τis deﬁned
to be the diﬀerence between the expected outcomes of the
two groups:
τ=E[Y1]−E[Y0] (1)
where E[·] is the expectation operator. Suppose that a ran-
domized experiment is conducted in which a researcher ran-
1Thisproblem has been called the fundamental problem of
causal inference [12].
147domly assigns units to either the treatment group or the
contr
ol group, and suppose an outcome yiis observed for
each unit ui. Then the diﬀerence ¯ y1−¯y0between the sam-
ple means for the treatment group and the control group,
respectively, is an unbiased estimator of τ. The estimate
is unbiased because random treatment assignment tends to
ensure that the treatment group and the control group are
balanced with respect to the distributions of confounding
variables, whether or not the variables are observed. Con-
sequently, the treatment variable Tis independent of the
potential outcomes Y1andY0,
(Y1, Y0)⊥ ⊥T (2)
and so knowing whether a unit is assigned to treatment does
not provide any information about Y1andY0.
It is typically assumed that execution data to be used for
statistical fault localization is generated from a set of ex-
isting test cases or is collected in the ﬁeld from operational
executions. In both cases, statistical fault localization is a
kind of observational study rather than a randomized exper-
iment, because the executions were not randomly assigned
to either the treatment group (e.g., executions covering a
statement s) or the control group (e.g., executions not cov-
ering s). Note that creating test cases speciﬁcally to achieve
coverage of every conditional branch does not ensure that
the test cases that cover a statement sare otherwise similar
to the test cases that do not cover s.
To estimate average treatment eﬀects based on observa-
tional data, statistical techniques that “control for” or “con-
dition on”the observed values of potential confounding vari-
ables [18] are used. An unbiased estimate of the average
treatment eﬀects of treatment variable Ton outcome vari-
ableYcan be obtained by conditioning on a set Xof covari-
ates that render Tconditionally independent of the potential
outcome variables Y1andY0,
(Y1, Y0)⊥ ⊥T|X (3)
In this case, the average treatment eﬀect τis given by the
equation
τ=E[E[Y|T= 1, X ]−E[Y|T= 0, X ]] (4)
where E[·|· ] is the conditional expectation operator. (The
outer, unconditional expectation is evaluated with respect
to the probability distribution of X.)
One way of controlling for observed confounders when es-
timating an average treatment eﬀect is to include them as
predictors in a statistical regression model [18], such as the
linear model
Y=α+τT+βX+ε (5)
In this model, Yis the outcome variable, Tis the treatment
variable, Xis a vector of covariates that includes the ob-
served confounders, and εis a random error term that (ide-
ally) does not depend on the values of TandX. Equation 5
implies that E[Y|T, X] =α+τT+βX. The ﬁtted value ˆ τ
of the coeﬃcient τis an unbiased average-treatment-eﬀect
estimate, assuming the model is correctly speciﬁed.
2.2 DynamicProgramDependences
Informally, in a program P, statement s2iscontrol depen-
dent[10] on a statement s1if, as indicated by the control-ﬂow graph (CFG) for P,2s1determines whether s2is ex-
ecuted. Statement s1dominates statement s2in the CFG
if all paths from P’s entry to s2contains s1. The control
dependence graph (CDG) of Pis a directed graph where
nodes represent statements in Pand edges represent control
dependences. Statement s1is aforward control-dependence
predecessor ofs2in the CDG if s2is control dependent on
s1ands2does not dominate s1in the CFG.
Statement s2is data dependent on statement s1if there is
a deﬁnition of a variable v(i.e., a write of v) ats1, a use of v
(i.e., access with no write) at s2, and there is a least one path
froms1tos2in the CFG on which vis not redeﬁned. The
program-dependence graph (PDG) of program Pis a directed
graph whose nodes represent statements in Pand whose
edges represent control and data dependences. The graph is
annotated with two special nodes, START and EXIT, which
represent entry to and exit from P, respectively.
Informally, the dynamic program dependence graph (Dy-
namic PDG) is a directed graph constructed from a set of
program executions in which nodes represent executed state-
ments and edges represent executed control dependences and
data dependences. Statement s1is adynamic forward con-
trol dependence predecessor ofs2ifs1is a forward control-
dependence predecessor of s2in the Dynamic-PDG. The
Dynamic-PDG, which is a subgraph of the PDG, is similar
to the graph obtained with Agrawal and Horgan’s [2] tech-
nique (Approach 1) for dynamic slicing. However, the latter
is created using one execution whereas the Dynamic-PDG
described here is created using a set of executions. We de-
ﬁne the dynamic forward control dependence subgraph as the
subgraph of the Dynamic-PDG that contains only forward
control-dependence edges.
2.3 PreviousFaultLocalizationModel
To specify a causal model for statistical fault localiza-
tion, it is necessary to indicate the form of the model, the
treatment variable, and the covariates. Our initial causal
model [5] includes a linear regression model for estimating
the failure-causing eﬀect of covering a program statement s:
Y=αs+τsTs+βsCs+εs (6)
This model is ﬁt separately for each statement in a program,
using statement-coverage proﬁles from a set of passing and
failing executions. In the model, Yis a binary variable that
is 1 for a given execution if and only if the program failed;
Tsis a binary treatment variable, which indicates whether s
was covered (executed at least once) during the execution;
Csis a binary covariate, which indicates whether the dy-
namic forward control dependence predecessor dfcdp(s) of s
was covered; αsis a constant intercept; and εsis a random
error term that does not depend on the values of TsandCs.
The average treatment eﬀect of Tsupon Yis estimated by
the ﬁtted value ˆ τsof the coeﬃcient τs, which is used as a
suspiciousness value for s.
A regression model such as Equation 6 describes only sta-
tistical relationships between variables. To obtain a full
causal model, it must be augmented with additional infor-
mation about causal relationships. This information can be
represented by a causal graph [21], which is a directed acyclic
2Inacontrol-ﬂow graph, nodes represent program state-
ments, and edges represent the ﬂow of control between the
statements.
148t1t2t3t4t5(-1,1)
(2,3)
(1,-3)
(0,5)
(3,4)
ˆτ
ˆτmatch
void Proc() {
1 int x=read(); 111110.40 NA
2 int
y=read(); 111110.40 NA
3 if(
x>0 ){ 111110.40 NA
4 if(
y<0) 011010.67 0.67
5 y =
2; 00100-1.00 -1.00
6 print(”
Out:”); 011010.67 0.67
7 print(y+y); 011010.67 1.00
8}
9}
01001
Figure 1: Procedure with test cases, execution data,
and causal-eﬀect
estimates. Error at statement 7;
correct computation should be y ×y.
graph (DAG) whose nodes represent random variables (cor-
responding to causes and eﬀects) and whose edges represent
causal relationships. An edge A→Bin a causal graph indi-
cates that A(potentially) causes B. Equation 6 is implicitly
based on the causal graph obtained from a (structured) pro-
gram’s dynamic forward control dependence subgraph, by
(1) associating with each node a coverage-indicator variable
for the corresponding statement and (2) augmenting the dy-
namic forward control dependence subgraph with edges from
each node to a new node for the failure indicator Y.
The role of the coverage indicator variable Csfordfcdp(s)
in Equation 6 is to control for confounding of the estimated
failure-causing eﬀect of s, due to coverage of other state-
ments. Intuitively, conditioning on Csreduces confounding
because dfcdp(s) is the most immediate cause of sbeing cov-
ered or not being covered on a particular run. Pearl’s well-
known Back-Door Criterion [21] for causal graphs provides
some formal justiﬁcation for Equation 6, because dfcdp(s)
blocks all “back-door paths” from TtoYin the augmented
dynamic forward control dependence subgraph. (Such a
path is actually a semi-path —one in which edges may point
in either direction—whose ﬁrst edge is of the form T←Zfor
some node Z.) Under the very strong assumption that the
augmented dynamic forward control dependence subgraph is
suﬃcient to model failure causation in a program, the Back-
Door Criterion implies that Tis conditionally independent
of the potential outcomes Y1andY0given the value of Cs,
as required for Equation 4 to hold with TsandCsin place
ofTandX, respectively.
3. ADDRESSING BOTH DATA AND CON-
TROLDEPENDENCES
The causal model, given in Equation 6, which was used in
our previous technique for statistical fault localization relies
on the dynamic forward control dependence subgraph as a
model of failure causation. However, the dynamic forward
control dependence subgraph, and therefore Equation 6, does
not adequately reﬂect failure causation in which data depen-
dences play an important role. In this section, we present
our new technique for estimating a statement’s failure caus-
ing eﬀect, which addresses confounding due to both dy-
namic data dependences and dynamic control dependences
by matching executions based on information about both.We begin with an example that illustrates the importance
of incorporating data dependences into our causal model,
and then, present the details of the model.
3.1 MotivatingExample
Consider procedure Procin Figure 1, which has a fault at
line 7.Procshould print the square of the value of yat line 7
but instead prints two times the value of y. The ﬁrst column
showsProcwith line numbers associated with each of its
statements. Columns 2 through 6 represent test cases t1–t5,
respectively. The top entry of each column shows the values
ofxandythat are read at lines 1 and 2, respectively. The
numbers in the column for a test case indicate whether the
corresponding program statement is covered by the test case
(1 for covered, 0 for not covered). The bottom row shows
the outcome of each test-case execution, with 1 indicating a
failing execution and 0 indicating a passing execution.
Suppose we want to compute the failure-causing eﬀect of
statement 7 using Equation 6. For statement 7, as Figure 1
shows, test cases t2,t3, and t5are in the treatment group,
because they cover statement 7, and test cases t1andt4are
in the control group, because they do not cover statement
7. The dynamic forward control dependence predecessor of
statement 7 is statement 3. Variable Csin Equation 6 corre-
sponds to a coverage indicator C3for statement 3, which is
covered by all tests. (Because C3is constant, it or the inter-
ceptαsfrom Equation 6 can be dropped from the model for
statement 7.) The column labeled ˆ τin Figure 1 indicates the
failure-causing eﬀect estimates obtained with Equation 6 for
the statements in procedure Proc. The estimate for state-
ment 7, which is faulty, is 0.67. However, the estimates for
statements 4 and 6 are also 0.67, even though they are not
faulty. Statements 4 and 6 have the same estimate as state-
ment 7 because they are in the same control dependence
region [10] and hence, are covered by the same tests. This
example illustrates that serious confounding may occur even
after conditioning on coverage of each statement’s dynamic
forward control dependence predecessor.
3.2 NewTechnique
Our new technique addresses the inadequacy of our previ-
ous technique for fault localization. The technique consists
of two main components: a new causal model and a match-
ing technique.
3.2.1 Overview
Algorithm LocalizeFault, shown in Figure 2, takes as
input the dynamic program dependence graph (Dynamic-
PDG) of a procedure and the coverage information (Coverage-
Info) produced by executing the program containing the pro-
cedure on a set of test cases. LocalizeFault processes each
statement sin the Dynamic-PDG; it initializes the causal
estimate of each statement s, (ˆτ(s)), to -1.0 (line 2). Local-
izeFault computes the ﬁrst major component of our tech-
nique, the causal model of s(Model(s)) (line 3). At line
4, the algorithm computes the predecessors of s(Pred(s))
from Model(s). After computing Pred(s), LocalizeFault
then computes the second major component of our tech-
nique by matching on the Pred(s) in the Coverage-Info to
produce the matched data, Mdata( s) (line 5). If matching
is successful, the ˆ τ(s) is estimated using Equation 1 (line 7);
if matching is not successful Equation 5 is used to estimate
ˆτ(s) (line 9). The algorithm computes ˆ τ(s) for every state-
149LocalizeFault(Dynamic-PDG, Coverage-Info) {
1 fo
reach s ∈Dynamic-PDG do
2 ˆτ(s) = -1.0
3 Model(s) = Compute causal model of s
4 Pred(s) = Compute predecessors of s from Model(s)
5 Mdata(s) = Match on Pred(s) in Coverage-Info
6 if (Mdata(s) /ne}ationslash=∅){
7 Compute ˆτ(s) from Mdata(s) using Equation 1
8 }else{
9 Compute ˆτ(s) using Equation 5
10 }
11 done
12 sorted_ ˆτ= sort ˆτin descending order
13 return (sorted_ ˆτ)
14}
Figure 2: Algorithm for new technique
ment in the Dynamic-PDG. After the causal estimate for
each statement has been computed, LocalizeFault sorts all
the estimates in descending order at line 12. It then returns
the sorted estimates at line 13 to the developer.
3.2.2 NewCausalModel
Our new causal model extends our previous causal model
(Equation 6) by addressing dynamic data dependences as
well as dynamic control dependences. Dynamic data de-
pendences are important because they carry the values that
are used at a given statement. Whereas a statement’s for-
ward control dependence predecessor determines whether
the statement is covered, the statement’s data dependences
determine the computation it carries out. Conceptually, the
additional causal inﬂuences that we want to account for can
be represented by including dynamic data dependences in
the causal graph for a program, in addition to dynamic
forward control dependences. A complication is that loop-
carried data dependences [10] give rise to directed cycles in
dependence graphs, whereas much of the theory of causal in-
ference developed by Pearl [21] and other authors, including
the Back Door Criterion, is based on causal graphs that are
acyclic. However, to control confounding when estimating
the failure-causing eﬀect of a particular statement si, it suf-
ﬁces to consider acyclic dependence chains terminating at s,
because if there is a causal path from sjtosithat contains
one or more cycles, there must also be a cycle-free path from
sjtosi. For example, consider a program loop of the form
1 while(...)
2 if (...)
3 x = f(y);
4 else
5 y = g(x);
Each edge in the data dependence cycle 3 →5→3 may be
relevant to localizing a distinct fault. However, in seeking
to control confounding while estimating the failure-causing
eﬀect of statement 3, we can ignore the edge (3, 5). Similarly,
when estimating the failure causing eﬀect of statement 5, we
can ignore the edge (5, 3).
The kind of causal graph that is required can be conceptu-
alized in terms of the transitive reduction of a digraph [3]. In
essence, a transitive reduction of a digraph Dis an acyclic,
spanning subdigraph HofDwith no redundant arcs such
that the transitive closures of DandHare isomorphic [6].3
3Klamtand colleagues employ a form of transitive reduction
to model causality in biological networks with cycles [14].
(a) Dynamic-PDG of Proc.
 (b) ICG of state-
ment
7.
Figure 3: (a) Dynamic program dependence graph
(Dynamic-PDG) of procedure Proc. (b) Integrated
causal graph of statement 7.
Aho and colleagues [3] present a transitive reduction algo-
rithm that, for a digraph with cycles, replaces each equiv-
alence class of vertices appearing in the same cycle with a
new vertex. Consider a slight variant of this algorithm, ap-
plied to a graph of dynamic data and control dependences
between program statements. For a given statement swhose
failure-causing eﬀect we want to estimate, this variant pre-
serves all nodes in s’s equivalence class (if there is more than
one) rather than collapsing them to a single node. How-
ever, it breaks any cycles involving sby deleting s’s outgo-
ing edges. Therefore, for each statement sin the dynamic
program dependence graph, an acyclic subgraph Hscan be
constructed that reﬂects all causal inﬂuences on s.Hscan
be transformed into a proper causal graph by augmenting
it as described in Section 2. We call the augmented Hsthe
integrated causal graph forsand denote it ICG(s).4
In the integrated causal graph ICG(s), there will be mul-
tiple back doors to sif it is dependent on multiple state-
ments. Observe, however, that the set of predecessors of s
(Pred( s)) blocks all back-door paths from sto the failure
node Y. Thus, by conditioning on the coverage indicator
variables associated with Pred( s), we may be able to fur-
ther reduce confounding when estimating the failure-causing
eﬀect of s. For example, Figure 3(a) shows the dynamic pro-
gram dependence graph of Proc; dotted edges represent data
dependences and solid edges represent control dependences.
Figure 3(b) shows the ICG of statement 7, where nodes 2, 3,
and 5 of statement 7 are nodes in the dynamic program de-
pendence graph of Proc. As the graph shows, there are three
back-door paths in the graph: 7 ←2→Y, 7←3→Y, and
7←5→Y.
3.2.3 Matching
Matching [18] is a technique that brings some of the ben-
eﬁts of randomized controlled experiments, in terms of re-
duced confounding bias, to observational studies. Matching
involves mapping (if possible) each treatment unit from an
observational study to one or more control units that are
similar to it, in such a way that balance is achieved between
the resulting treatment group and control group with re-
spect to covariate values. Matching may involve removing
4One subtlety is that if a node ninICG(s) represents an
equivalence class Cof nodes from D, a set Zof coverage
indicator variables should be associated with n. For each
statement trepresented in C,Zshould contain a coverage
indicator for t.
150unmatched units or (in eﬀect) duplicating certain units. We
ﬁrst
describe a simple form of matching, called exact match-
ing, before describing the more complex type of matching
used with our new causal model.
Inexact matching , each treatment unit is matched with
one or more control units that have exactly the same covari-
ate values as the treatment unit, if such control units exist.
All matched treatment and control units are retained, and
all unmatched units are discarded. The diﬀerence in the
group means of the resulting treatment group and control
group is an estimate of the treatment eﬀect.
We now illustrate exact matching with reference to the
procedure and data in Figure 1. Let the covariates for each
statement sin procedure Proc be the coverage-indicator
variables associated with the possible dynamic control and
data dependence predecessors of s. For example, for state-
ment 7, there are three predecessors: statement 2, which
deﬁnes the value of yand may be used at statement 7; state-
ment 3, which is statement 7’s forward control dependence
predecessor); and statement 5, which deﬁnes the value of y
and may be used at statement 7. Figure 4 shows the treat-
ment units and the control units with their associated test
cases for statement 7. The treatment units, which are test
cases that cover statement 7 are t2,t3, and t5; the control
units, which are test cases that do not cover statement 7
aret1andt4. The vector of covariate values generated by
the test cases is shown; a vector of the form [1, 1, 0] means
that for that test case, statement 2 was covered, statement
3 was covered, and statement 5 was not covered, respec-
tively. There is an arrow from each treatment unit to a
matching control unit. Note that there is no match for
t3, because it is the only test case to cover each of state-
ment 2, statement 3, and statement 5. Consequently, t3
is not used to estimate the failure-causing eﬀect of state-
ment 7. Using Equation 1, the estimate for statement 7
is (1 + 1)/2−(0 + 0) /2 = 1.0, which is the diﬀerence be-
tween the average outcome values for the treatment group
and the average outcome values for the control group. The
causal-eﬀect estimates for the other statements are shown
in Table 1, in the column labeled ˆ τmatch. (The NA for state-
ment’s 1, 2, and 3 reﬂects the fact that an estimate could not
be computed for the statements because there were no con-
trol units for the statements.) The estimate for statement 5
is 0−1/1 =−1.0 and the estimate for each of statements 4
and 6 is (1+1+0)/3 −(0+0) /2 = 0.67. It can be seen that
the faulty statement, statement 7, has the highest estimate.
Unfortunately, as the number of covariates increases, it
becomes more diﬃcult to ﬁnd exact matches for treatment
units. This diﬃculty results in more discarded units, which
can increase bias5when computing causal estimates. There-
fore, other forms of matching are typically used.
MatchingwithMahalanobisDistance
To obtain more ﬂexibility in matching treatment units with
control units, our technique uses the Mahalanobis distance
metric [18], which is a measure of the similarity dM(a,b)
between two random vectors aandb. Let Sbe the covari-
ance matrix of aandband let the superscript Tdenote
5Thisform of bias is diﬀerent from confounding bias. The
bias is the diﬀerence in an estimator’s expected value and
the true value of the parameter being estimated.
Figure 4: Units in treatment group and control
group with
their covariate values.
matrix/vector transpose. Then
dM(a,b) =p
(a−b)TS−1(a−b) (7)
The
matrix Sis the sample covariance matrix for the treat-
ment and control units. Each treatment unit (test case cov-
ering statement s) with covariate vector ais matched with a
control unit (test case not covering s) with covariate vector
bfor which dM(a,b) is minimal with respect to a thresh-
old. In matching without replacement, the two units are
then removed from further consideration. In matching with
replacement, the control unit is retained to be matched with
another treatment unit. Replacement is normally used if
the number of treatment units exceeds the number of con-
trol units; matching with replacement also reduces bias5[29]
in the causal estimate. A property of Mahalanobis-distance
matching is that it regards all the components of the co-
variate vector as equally important [29]. This property is
important when matching on the predecessors of sbecause
in the absence of any prior information about the relative
importance of the predecessors, our technique treats all pre-
decessors of sas equally important.
4. EMPIRICALEVALUATION
To evaluate the eﬀectiveness of our technique for fault lo-
calization, we implemented it and performed empirical stud-
ies on a set of subjects. In this section, we ﬁrst describe the
set-up and then present the results of the studies.
4.1 EmpiricalStudySetup
For our studies, we used 16 programs with 272 faulty ver-
sions. We used seven programs from the Unix suite (Cal,
Col, Comm, Spline, Tr, and Uniq), all programs in the
Siemens suite (Print-tokens, Print-tokens2, Replace, Sched-
ule, Schedule2, Tcas, and Tot-info), and programs Sed and
Space.6Table 1 shows the characteristics of the subjects.
For each subject, the ﬁrst column gives the program name,
the second column provides the ratio of the number of ver-
sions used to the number of versions available, the third
column gives the number of lines of code for the subject, the
fourth column gives the number of test cases, the ﬁfth col-
umn shows the average number of vertices in the dynamic
program dependence graph, and the last column provides a
description. We omitted 25 faulty versions because either
there were no syntactic diﬀerences between the C ﬁles of the
correct version and the faulty version of the program or none
of the test cases failed when executed on the faulty version
of the program.
6We obtainedthe Unix programs from Eric Wong of Uni-
versity of Texas at Dallas and the Space and Sed programs
from the Software-artifact Infrastructure Repository [9]. We
obtained the Siemens suite from the Aristotle Research Lab.
151Table 1: Subjects used for empirical studies.
Number of Versions Used / Number of Number of Number of
Program Number of Versions Lines of Code Test Cases Nodes Description
Cal 19 / 20 202 162 131.5 calendar printer
Col 29 / 30 102 156 240.0 ﬁlter-line reverser
Comm 10 / 12 167 186 116.1 ﬁle comparer
Look 8 / 14 170 193 140.9 word ﬁnder
Spline 13 / 13 338 700 247.0 curve interpolator
Tr 11 / 11 137 870 150.2 character translator
Uniq 17 / 17 143 431 131.3 duplicate line remover
Print-tokens 5 / 7 472 4130 423.6 lexical analyzer
Print-tokens2 10 / 10 399 4115 340.5 lexical analyzer
Replace 28 / 32 512 395 415.9 pattern replacement
Schedule 9 / 9 292 2710 216.3 priority scheduler
Schedule2 9 / 10 301 2650 225.9 priority scheduler
Tcas 41 / 41 141 1608 136.7 altitude separation
Tot-info 23 / 23 440 1052 249.0 information measure
Sed 10 / 10 14K 363 4151.0 stream editing utility
Space 30 / 38 6K 157 3851.9 ADL interpreter
For our implementation, we used the CIL framework [20],
which
supports the analysis of ANSI C programs, to analyze
the subject programs. We implemented the algorithms to
instrument the programs and to compute dynamic control-
ﬂow graphs in the Object Caml Language (OCaml), because
OCaml is required to interface with CIL. Recall that our
technique uses dynamic control-ﬂow graphs to compute dy-
namic dependences, because the former contain only state-
ments that are actually executed by a set of test cases.
We implemented all fault-localization algorithms in R[23],
which is a statistical computation system with its own lan-
guage and runtime environment. For matching on program
dependences, we used the Rpackage Matching [28]. We used
matching with replacement and a default minimal distance
of 0.00001. We also computed fault matrices that indicate,
for each faulty program, which test cases pass and which test
cases fail. We performed our studies on Mac OS X version
10.5.
4.2 EffectivenessStudies
To study the eﬀectiveness of our technique for fault local-
ization, we use a cost metric that has been used in previous
studies [7, 25]. The metric, which we denote as Cost, mea-
sures the percentage of faulty statements a developer must
examine before encountering the faulty statement. We as-
sume that the statements are examined in a non-decreasing
order of suspiciousness scores. If there are ties, we assume
the developer must examine all the tied statements. For
example, if there are nstatements in a program and all n
statements have the same suspiciousness score, we assume
that the developer must examine all nstatements. Therefore
theCostof ﬁnding the fault is 100%.
To compare the eﬀectiveness of fault-localization tech-
niques A and B with respect to a given program version Pi,
we use B as a baseline, and compute the diﬀerence between
theCostof applying B to Piand the Costof applying A to
Pi. A positive result implies that A performed better than
B onPiand a negative result implies that B performed bet-
ter than A. The diﬀerence between the Costs corresponds to
the magnitude of improvement. For example, if the Costof
A is 20% and the Costof B is 60%, developers will examine
40% fewer statements if they use A for fault localization.
To reduce the uncertainty in the causal eﬀect computed
for each statement because of the matching technique, wecomputed a statement’s causal eﬀect 100 times and took
the average of the 100 causal estimates. We found 100 to be
an acceptable threshold for providing stable causal eﬀects.
Each computed causal eﬀect has an associated standard er-
ror. We computed the average of the standard errors, and
used it to construct a bound (one standard deviation) on
the causal eﬀect for each statement. For example, if the av-
erage of the causal eﬀects of a statement on program failure
is 0.3 and the average standard error is 0.1, then [0.3 ±0.1]
is the range of the causal eﬀect over the 100 samples with
0.2 being the lower bound and 0 .4 being the upper bound.
For brevity, we denote our new fault-localization tech-
nique, which matches executions based on data and con-
trol dependences, as PD; we denote a variant of the tech-
nique that matches only on data dependences as DD, and
we denote our previous causal technique [5], which consid-
ered only control dependences, as CD. We also use PDmin
and PDmax to represent variants of PD computed using
the lower bounds and upper bounds of the causal-eﬀect es-
timates of PD, respectively. For example, if for a statement
the average causal estimate obtained with PD is 0.3 and the
average standard error is 0 .1 then PDmin and PDmax yield
causal estimates of 0 .2 and 0.4, respectively.
Table 2 summarizes the results of comparing fault-local-
ization techniques. The ﬁrst column (Fault Loc. Tech.)
shows the two fault-localization techniques (i.e., A vs B) that
are being compared; the second technique in each pair (i.e.,
B) is the baseline. The second column (Positive (%)) shows
the percentage of faulty versions for which A performed bet-
ter than B, the third column (Negative (%)) shows the per-
centage of faulty versions for which A performed worse than
B, and the fourth column (Neutral (%)) shows the percent-
age of faulty versions for which there was no improvement.
For example, the ﬁrst row, which compares PD to CD, shows
that PD performed better than CD on 61.4% of the faulty
versions, performed worse on 13.97% of the faulty versions,
and performed identically on 24.63% of the faulty versions.
Table 3 shows the minimum (Min), median (Med), max-
imum (Max), and mean (Mean) values of A’s improvement
over B. Half the faulty versions with positive improvement
values have improvements between the minimum and the
median, and the other half have improvements between the
median and the maximum. For example, the ﬁrst row,
which compares PD to CD, shows that half of the 61.4%
1520 20 40 60 80 100 120 140 160 180 200−40−20020406080
VersionImprovement (%)
Figure 5: Comparison of new technique (PD) to old
technique
(CD).
of faulty versions with positive improvement values had im-
provements between 0.05% and 3.84% while the other half
had improvements between 3.84% to 61.71%. The average
positive improvement of PD over CD was 8.57%.
4.2.1 Study1:NewTechniquevsOldTechnique
The goal of this study is to compare the fault-localization
eﬀectiveness of our new technique PD to that of our previous
causal technique CD, which considers control dependences
but not data dependences. To do this, we compared the
Costvalues for the two techniques.
Figure 5 shows the bar graph that summarizes the com-
parison of PD to CD over all program versions. The horizon-
tal axis (baseline) represents the Costof using our previous
technique CD, the vertical axis represents the magnitude of
improvement of PD over CD. The graph contains a verti-
cal bar for each faulty version for which there was positive
or negative improvement on the horizontal axis; the verti-
cal bar shows the diﬀerence in Costs. The bars above the
horizontal axis represent faulty versions for which PD per-
formed better than CD (positive improvement) and the bars
below the horizontal axis represent faulty versions for which
PD performed worse than CD (negative improvement). For
example, for faulty-version 3 on the graph, PD performed
better by about 21.87%. Figure 5 shows that PD performed
better than CD overall. As indicated in Table 2, PD per-
formed better than CD on 61.4% of the faulty versions, worse
on 13.97% of the faulty versions, and showed no improve-
ment on 24.63% of the faulty versions. The ﬁrst row of
Table 3 characterizes the degree of positive improvement of
PD over CD. As the table indicates, half the 61.4% of the
faulty versions with positive improvement values had im-
provements between 0.05% and 3.84%, and the other half
had improvements between 3.84% and 61.71%. The average
positive improvement of PD over CD was 8.57%.
We also compared the Costs of PDmin and PDmax to
PD. As Table 2 shows, PDmin performed better than CD
on 61.4% of the faulty versions, performed worse on 13.97%
of the faulty versions, and performed identically on 24.63%
of the faulty versions. The table also shows that PDmax
performed better than CD on 60.29% of the faulty versions,
performed worse on 14.71% of the faulty versions, and per-
formed identically on 25% of the faulty versions. Table 3
shows that the performance of PD, PDmin, and PDmax areTable 2: Comparison of fault-localization models.
Fault Loc. Tech. Positive (%) Negative (%) Neutral (%)
PD vs CD 61.40 13.97 24.63
PDmin vs CD 61.40 13.97 24.63
PDmax vs CD 60.29 14.71 25.00
PD vs DD 49.26 23.16 27.57
DD vs CD 43.75 25.74 30.51
PD vs Tarantula 72.06 7.72 20.22
PD vs Ochiai 44.12 21.32 34.56
CO vs Ochiai 53.68 10.29 36.03
Table 3: Distribution of positive improvements.
Fault Loc. Tech. Min (%) Med (%) Max (%) Mean (%)
PD vs CD 0.05 3.84 61.71 8.57
PDmin vs CD 0.05 4.03 61.71 8.75
PDmax vs CD 0.05 3.94 62.29 8.60
PD vs DD 0.03 3.43 45.71 6.11
DD vs CD 0.05 5.07 59.43 9.54
PD vs Tarantula 0.05 6.41 70.29 11.44
PD vs Ochiai 0.03 4.79 54.86 7.77
CO vs Ochiai 0.03 4.28 54.86 7.45
similar. The results of PDmin and PDmax provide evidence
of the stabilit
y of the causal estimates computed with PD.5
Overall, the results indicate that PD improved the accuracy
of fault localization over CD by further reducing confound-
ing bias.
Although PD performed better overall, we wanted to see
how much of the improvement resulted from data depen-
dences. To do this, we compared DD to CD with respect
toCost. Table 2 indicates that DD had lower Cost than
CD on 43.75% of the faulty versions and higher cost on only
25.74% of the versions. Thus most of the improvement seen
with PD is due to considering data dependences. Consider-
ing both control and data dependences increases the accu-
racy of our technique by about 18%. Tables 2 and 3 indicate
that causal models based on both data and control depen-
dences are more eﬀective than causal models based on either
type of dependence alone. Overall, the results show that by
blocking back-door paths created by program dependences
in a faulty program, confounding bias can be reduced when
estimating the failure-causing eﬀect of a statement.
4.2.2 Study2:NewTechniquevsOtherTechniques
The goal of this study is to compare PD to two well-known
statistical fault-localization techniques, Tarantula [13] and
Ochiai [1], with respect to Cost. We used the Costs of Taran-
tula and Ochiai as baselines and subtracted the Costof PD.
Figure 6 shows that PD performed better than Tarantula
on most faulty versions. Table 2 indicates that PD per-
formed better than Tarantula on 72.06% of the faulty ver-
sions, performed worse on 7.72% of the faulty versions, and
performed identically on 20.22% of the faulty versions. Ta-
ble 3 also shows that half the 72.06% with positive improve-
ment values had improvements between 0.05% and 6.41%
and that the other half had improvements between 6.41%
and 70.29%. The average positive improvement of PD over
Tarantula was 11.55%.
In our previous work [5], Ochiai performed better than
our older technique (CD). However, Ochiai does not address
confounding bias, so we incorporated CD into Ochiai (calling
it Causal-Ochiai) and showed that Causal-Ochiai performed
better than Ochiai. Here we show that PD performs better
than Ochiai even when the techniques are not composed.
Figure 7 shows that PD performed better than Ochiai
1530 20 40 60 80 100 120 140 160 180 200 220−40−20020406080
VersionImprovement (%)
Figure 6: Comparison of our technique (PD) to
Tarantula.
0 20 40 60 80 100 120 140 160 180−40−30−20−100102030405060
VersionImprovement (%)
Figure 7: Comparison of our technique (PD) to
Ochiai.
on
most of the faulty versions. Table 2 indicates that PD
performed better than Ochiai on 44.12% of the faulty ver-
sions, performed worse on 21.32% of the faulty versions,
and performed identically on 34.56% of the faulty versions.
Table 3 also shows that half the 44.12% of the faulty ver-
sions with positive improvement values had improvements
between 0.03% and 4.79% and that the other half had im-
provements between 4.79% and 54.86%. The average pos-
itive improvement of PD over Ochiai was 7.77%. Tables 2
and 3 show that the performance of Ochiai was improved
when PD was composed with it to produce Causal-Ochiai
(CO). CO performed better on 53.68% of the faulty ver-
sions, performed worse on 10.29% of the faulty versions, and
performed identically on 36.03% of the faulty versions. Also,
half the 53.68% of faulty versions with positive improvement
values had improvements between 0.03% and 4.28% and the
other half had improvements between 4.28% and 54.86%.
The average positive improvement of CO over Ochiai was
7.45%.
Overall, the results indicate that PD performs better than
both Tarantula and Ochiai, and improves Ochiai.
4.2.3 ComputationTime
In this section, we present the average computation time
required to compute all the fault-localization results for oneversion of some of the subjects. We found that the com-
putation time was largely dependent on the size of the test
suite of a program. The Matching package [28] takes con-
siderable time to invert the large covariance matrices. For
Tcas, which had 1608 test cases, it took on average 8 min-
utes. For Schedule, which had 2710 test cases, it took on
average 32.73 minutes. For Printtokens2, which had 4115
test cases, it took on average 2.3 hours.
4.3 ThreatstoValidity
There are three main types of threats to validity that af-
fect our studies: internal, external, and construct. Threats
to internal validity concern factors that might aﬀect depen-
dent variables without the researcher’s knowledge. The im-
plementations of the algorithms we used in our studies could
contain errors. The Matching package we used in our stud-
ies is open source and has been used by other researchers
for experimentation, which provides conﬁdence that the al-
gorithms in the package are stable. To address potential
errors when constructing the dynamic program-dependence
graph, we compared manually generated dynamic program-
dependence graphs of smaller subjects to graphs generated
automatically by our technique.
Threats to external validity occur when the results of a
study cannot be generalized. In this work, such threats are
greatly alleviated because our work is based on established
causal inference theory and methodology. However, more
empirical studies on additional subjects are needed to fully
address this threat.
Threats to construct validity concern the appropriateness
of the metrics used in our evaluation. We used the Costmet-
ric to determine the eﬀectiveness of our technique for fault
localization. The Costmetric is a ranking metric that has
been used to compare techniques in many fault-localization
studies, though under a diﬀerent name (Score ). However,
it is not established whether this metric is well suited for
presenting fault-localization information to developers.
5. RELATEDWORK
There are many techniques that attempt to locate the
fault or faults in a failed program. This section discusses
those most closely related to ours, and compares our new
technique to them.
One category of fault-localization techniques are statisti-
cal fault-localization techniques (e.g., [1, 4, 13, 15, 16, 17,
27, 33]). These techniques compute the association between
program entities and program failure. However, as we dis-
cussed in our previous work and demonstrated with ana-
lytical and empirical studies [5], statistical association does
not imply causation. In contrast to these existing statistical
fault-localization techniques, our technique for ﬁnding the
cause of program failure is grounded in the theory of causal-
inference methodology. Our empirical results in previous
work and those presented in this paper demonstrate the ef-
fectiveness of our approach over the associative techniques.
Another category of fault-localization techniques are slic-
ing techniques (e.g., [30, 32]). Slicing techniques compute
the set of program entities (e.g., statements) that poten-
tially or actually aﬀect the values of variables at a given
program point (e.g., the program output). One limitation
of slicing techniques is that the techniques do not ﬁnd the
cause of the failure but instead compute the set of program
entities associated with the failure. A second limitation is
154that the slicing techniques do not provide guidance as to
how
a developer is to examine the statements in the slice. In
our work, we have demonstrated with analytical and empir-
ical studies that such association does not imply causation.
Our technique, which is grounded in causal theory is more
eﬀective at ﬁnding causes of failures and as such more ef-
fective than slicing techniques. Additionally, our technique
provides guidance to developers on how to systematically
examine program entities to ﬁnd the cause of the fault.
A third category of fault-localization techniques are state-
altering techniques (e.g., [7, 8, 31]). Like our technique,
state-altering techniques are causal techniques. These tech-
niques have a number of limitations that can make them
diﬃcult to apply. First, in performing experiments on pro-
grams, the techniques must ensure the semantic consistency
of memory changes, which can be diﬃcult and expensive.
Second, in performing experiments on programs, the tech-
niques require multiple re-executions of the program, which
can also be expensive. Third, the techniques require the
presence of an oracle (preferably automated) to determine
the outcome (success or failure) of the program after each
such re-execution, which in practice, can be diﬃcult, if not
impossible, to create. The main diﬀerence between our tech-
nique and state-altering techniques is that our causal tech-
nique is observational, whereas they are experimental. Our
technique uses observational data (e.g., coverage informa-
tion) that is often available by executing the program on
test cases. Moreover, our technique does not require an au-
tomated oracle because the test cases have pre-determined
outcomes, and it does not require multiple re-executions of
the program. Finally, our technique avoids the semantic con-
sistency problem encountered by state-altering techniques.
One other technique uses causal analysis and program slic-
ing to generate graphs that explain unexpected behaviors
produced by computational models [11]. This technique is
similar to ours in that both are based on the theory of causal
analysis. However, their technique does not ﬁnd the loca-
tion of faulty statements. Instead, their technique creates a
causal graph that explains an unexpected behavior.
Yet another technique determines whether two versions
of a program are the same, given that one of the versions
has been transformed (e.g., code has been obsfucated) [19].
To do this, their technique performs dynamic matching on
the control-ﬂow graphs of the two versions. Their technique
diﬀers from ours in that we match on program dependences
to reduce confounding bias for eﬀective fault localization,
whereas they match on control ﬂow to determine whether
the programs are the same.
Finally, in previous work [5], we presented a causal model
that is based on only control dependences. In this work, we
have augmented the causal model with data dependences
and have shown through empirical studies the eﬀectiveness
of the new causal model for fault localization.
6. CONCLUSIONANDFUTUREWORK
In this paper we have presented a novel statistically-based
causal-inference technique for software fault localization that
matches executions based on information about dynamic
data and control dependences to obtain more accurate (less
biased) estimates of a given statement’s eﬀect on the occur-
rences of program failures. The use of both covariate match-
ing and data-dependence information in our technique are
innovations. We also presented empirical results indicat-ing that the new technique is more eﬀective overall than
other techniques—including our previous causal-inference
technique, which does not consider data dependences.
Although the presented technique performed well overall,
it performed relatively poorly on some faulty program ver-
sions, which indicates there is room for improvement. We
mention three possible reasons for the technique’s perfor-
mance in some cases, and we suggest how it might be im-
proved.
First, the results obtained with the new technique are sub-
ject to limitations of the test suite used for fault localization.
Even if the test suite has desirable properties overall, it may
be inadequate for accurately estimating the failure-causing
eﬀects of certain statements. This will be the case for any
statement that is covered by very few test cases or for which
it is not possible to extract two reasonably well-matched
comparison groups of test cases that respectively cover and
do not cover the statement. Suspiciousness scores computed
for such statements cannot be trusted because they lack ad-
equate support. To address this issue, we plan to investigate
the automatic generation of test cases with the appropriate
coverage characteristics.
Second, although matching using Mahalanobis distance
was generally eﬀective in our studies, it sometimes failed to
yield balanced comparison groups. In the latter cases, the
rankings produced by our technique are not consistent be-
cause they are based on valid causal estimates mixed with
biased estimates. Using Mahalanobis distance for matching
is also sometimes ineﬃcient, because it is expensive to com-
pute Mahalanobis distance when there are many program
dependences. To address these issues, we plan to explore
the use of other matching techniques, such as those based
on propensity scores [18].
Finally, some faults cannot be eﬀectively localized with-
out considering the variable values carried by data depen-
dences. For example, considering variables values may be
the only way to determine that a check for an unusual con-
dition is missing in a certain program location. Although
the proposed technique considers data dependences it does
not currently consider variable values. We are currently in-
vestigating how this can be done.
Acknowledgements
This research was supported in part by NSF award CCF-
0725202 and an IBM Software Quality Innovation Award
to Georgia Tech, by NSF awards CCF-0820217 and CCF
0702693 and by an award from ABB Corporation to Case
Western Reserve University, and by a FACES (Facilitating
Academic Careers in Engineering and Science) fellowship
award to George Baah. The anonymous reviewers provided
many helpful suggestions that improved the presentation of
the paper.
7. REFERENCES
[1] R. Abreu, P. Zoeteweij, and A. J. van Gemund. On
the Accuracy of Spectrum-Based Fault Localization.
InProceedings of the Testing: Academic and Industrial
Conference Practice and Research Techniques , pages
89–98, September 2007.
[2] H. Agrawal and J. R. Horgan. Dynamic Program
Slicing. In Proceedings of the ACM SIGPLAN 1990
conference on Programming language design and
implementation , PLDI ’90, pages 246–256. ACM, 1990.
155[3] A. V. Aho, M. Garey, and J. D. Ullman. The
transitiv
e reduction of a directed graph. SIAM
Journal on Computing, 1:131–137, 1972.
[4] S. Artzi, J. Dolby, F. Tip, and M. Pistoia. Directed
Test Generation for Eﬀective Fault Localization. In
Proceedings of the International Symposium for
Software Testing and Analysis, July 2010.
[5] G. K. Baah, A. Podgurski, and M. J. Harrold. Causal
Inference for Statistical Fault Localization. In
Proceedings of the International Symposium on
Software Testing and Analysis, July 2010.
[6] J. Bang-Jensen and G. Gutin. Digraphs: Theory,
Algorithms and Applications . Springer-Verlag, 2001.
[7] H. Cleve and A. Zeller. Locating Causes of Program
Failures. In Proceedings of the International
Symposium on the Foundations of Software
Engineering , pages 342–351, May 2005.
[8] R. A. DeMillo, H. Pan, and E. H. Spaﬀord. Critical
Slicing for Software Fault Localization. In ISSTA ’96:
Proceedings of the 1996 ACM SIGSOFT international
symposium on Software testing and analysis , pages
121–134, 1996.
[9] H. Do, S. Elbaum, and G. Rothermel. Supporting
Controlled Experimentation with Testing Techniques:
An Infrastructure and its Potential Impact. Empirical
Software Engineering, 10(4):405–435, 2005.
[10] J. Ferrante, K. J. Ottenstein, and J. D. Warren. The
Program Dependence Graph and Its Use in
Optimization. ACM Transactions on Programming
Languages and Systems , 9(3):319–349, July 1987.
[11] R. Gore and P. F. Reynolds, Jr. Causal program
slicing. In IEEE/SCS Workshop on Principles of
Advanced and Distributed Simulation, 2009.
[12] P. W. Holland. Statistics and Causal Inference.
Journal of American Statistical Association ,
81:945–970, 1986.
[13] J. Jones, M. J. Harrold, and J. Stasko. Visualization
of Test Information to Assist Fault Localization. In
Proceedings of the International Conference on
Software Engineering, pages 467–477, May 2002.
[14] S. Klamt, R. J. Flassig, and K. Sundmacher.
TRANSWESD: inferring cellular networks with
transitive reduction. Bioinformatics , 26:2160–2168,
2010.
[15] B. Liblit, M. Naik, A. X. Zheng, A. Aiken, and M. I.
Jordan. Scalable Statistical Bug Isolation. In
Proceedings of the Conference on Programming
Language Design and Implementation , pages 15–26,
June 2005.
[16] C. Liu, X. Yan, L. Fei, J. Han, and S. P. Midkiﬀ.
SOBER: Statistical Model-based Bug Localization. In
Proceedings of the European Software Engineering
Conference and ACM SIGSOFT Symposium on the
Foundations of Software Engineering , pages 286–295,
September 2005.
[17] Lucia, D. Lo, L. Jiang, and A. Budi. Comprehensive
Evaluation of Association Measures for Fault
Localization. In Proceedings of International
Conference on Software Maintenance (ICSM) , 2010.
[18] S. L. Morgan and C. Winship. Counterfactuals and
Causal Inference: Methods and Principles of Social
Research. Cambridge University Press, 2007.[19] V. Nagarajan, R. Gupta, X. Zhang, M. Madou, and
B. D. Sutter. Matching Control Flow of Program
Versions. In International Conference on Software
Maintenance, 2007.
[20] G. C. Necula, S. McPeak, S. P. Rahul, and
W. Weimer. CIL: Intermediate Language and Tools
for Analysis and Transformation of C Programs. In
Proceedings of the International Conference on
Compiler Construction , pages 213–228, April 2002.
[21] J. Pearl. Causality: Models, Reasoning, and Inference .
Cambridge University Press, San Francisco, CA, USA,
2000.
[22] A. Podgurski and L. A. Clarke. A Formal Model of
Program Dependences and Its Implications for
Software Testing, Debugging, and Maintenance. IEEE
Transactions on Software Engineering , 16(9):965–979,
September 1990.
[23] R Development Core Team. R: A Language and
Environment for Statistical Computing . R Foundation
for Statistical Computing, Vienna, Austria, 2008.
[24] S. Rapps and E. Weyuker. Selecting Software Test
Data Using Data Flow Information. IEEE
Transactions on Software Engineering , SE-11, NO.
4:367–375, 1985.
[25] M. Renieris and S. Reiss. Fault Localization With
Nearest Neighbor Queries. In International Conference
on Automated Software Engineering , pages 30–39,
November 2003.
[26] D. J. Richardson and M. C. Thompson. An Analysis
of Test Data Selection Criteria Using the RELAY
Model of Fault Detection. IEEE Transactions on
Software Engineering, 19(6):533–553, 1993.
[27] R. Santelices, J. A. Jones, Y. Yu, and M. J. Harrold.
Lightweight fault localization using multiple coverage
types. In International Conference on Software
Engineering , 2009.
[28] J. S. Sekhon. Multivariate and Propensity Score
Matching Software with Automated Balance
Optimization: The Matching Package for R.
Forthcoming, Journal of Statistical Software , 2008.
Forthcoming.
[29] E. A. Stuart. Matching Methods for Causal Inference:
A Review and a Look Forward. Statistical Science,
25:1–21, 2010.
[30] M. Weiser. Program Slicing. In Proceedings of the
International Conference on Software Engineering,
pages 439–449, March 1981.
[31] A. Zeller. Isolating Cause-Eﬀect Chains from
Computer Programs. In Proceedings ACM SIGSOFT
10th International Symposium on the Foundations of
Software Engineering, November 2002.
[32] X. Zhang, N. Gupta, and R. Gupta. Pruning Dynamic
Slices With Conﬁdence. In Proceedings of the
Conference on Programming Language Design and
Implementation , pages 169–180, June 2006.
[33] Z. Zhang, W. K. Chan, T. H. Tse, B. Jiang, and
X. Wang. Capturing Propagation of Infected Program
States. In Proceedings of European Software
Engineering Conference and the ACM SIGSOFT
Symposium on the Foundations of Software
Engineering , pages 43–52. ACM, 2009.
156