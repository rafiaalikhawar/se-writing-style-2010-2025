CodeHint: Dynamic and Interactive Synthesis
of Code Snippets
Joel Galenson, Philip Reames, Rastislav Bodik, Bj√∂rn Hartmann, Koushik Sen
University of California, Berkeley, USA
{joel,reames,bodik,bjoern,ksen}@cs.berkeley.edu
ABSTRACT
There are many tools that help programmers nd code frag-
ments, but most are inexpressive and rely on static infor-
mation. We present a new technique for synthesizing code
that is dynamic (giving accurate results and allowing pro-
grammers to reason about concrete executions), easy-to-use
(supporting a wide range of correctness specications), and
interactive (allowing users to rene the candidate code snip-
pets). Our implementation, which we call CodeHint , gener-
ates and evaluates code at runtime and hence can synthe-
size real-world Java code that involves I/O, reection, na-
tive calls, and other advanced language features. We have
evaluated CodeHint in two user studies and show that its
algorithms are ecient and that it improves programmer
productivity by more than a factor of two.
Categories and Subject Descriptors
D.1.2 [ Programming Techniques ]: Automatic Program-
ming; D.2.6 [ Software Engineering ]: Programming Envi-
ronments
General Terms
Experimentation, Languages
Keywords
Program synthesis, IDE
1. INTRODUCTION
Many code fragments are dicult to write, often because
they involve using a new and unfamiliar API. Programmers
have many tools at their disposal, from search and auto-
complete to advanced synthesis techniques, but these often
have only limited applicability. For example, autocomplete
implementations can be helpful at nding a method to call,
but they require some knowledge of the static type of the
receiver and can only generate tiny code fragments.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proÔ¨Åt or commercial advantage and that copies
bear this notice and the full citation on the Ô¨Årst page. To copy otherwise,
to republish, to post on servers or to redistribute to lists, contact the Own-
er/Author.
ICSE ‚Äô14, May 31 - June 7, 2014, Hyderabad, India
Copyright 14 held by Owner/Author.To solve the problem of synthesizing code fragments, we
believe that programmers need to be able to give partial
specications (rather than full correctness conditions) that
may depend on concrete program state. Our approach thus
generates and evaluates code at runtime at a programmer-
chosen location while executing a specic concrete input.
We propose a new approach for synthesizing code that is
dynamic, easy-to-use, and interactive. Running in the dy-
namic context both allows us to nd and lter out candidates
that static techniques could not and allows users to reason
concretely about their desired result. We support a wide
variety of specications so that we can even aid program-
mers with very little knowledge of their desired code. Our
methodology is interactive, letting users incrementally give
more information to rene the candidate code fragments.
Taking advantage of dynamic information allows our al-
gorithms to be more accurate than static techniques by, for
example, dereferencing exactly the expressions that do not
evaluate to null in the current context and downcasting the
result of a method call to its dynamic type to enable sub-
sequent calls. In addition, users can use dynamic values in
their specications and see the results of executing the can-
didates, which can be helpful in choosing the correct result.
One major goal of our work has been to ensure that users
can nd code snippets using whatever partial information
they have about the desired result. Our partial dynamic
specications or pdspecs can be any predicate in the host
language. This exibility allows us to represent the full spec-
trum of specication strength and context sensitivity. For
example, our pdspecs can take the form of constraints on
the desired value (including demonstrating a concrete value),
dynamic type restrictions, or even full functional correctness
specications. We also allow users to write code skeletons to
shape the search space by giving a syntactic outline of the
desired code with holes marking unknown fragments.
Given a set of candidate statements synthesized by our
tool, users can rene this set by continuing to run the pro-
gram or by exercising it on dierent inputs in order to lter
out more candidates that fail the specication. This rene-
ment process can quickly remove many undesirable state-
ments. Users can also sort and lter the candidates and
their results. These features often allow users to nd their
desired code even with simple pdspecs.
Another goal of our work is to enable the synthesis of code
in real-world Java programs. Because it actually executes
candidates in a concrete program state, our implementation
can synthesize code that uses I/O, reection, native calls,
and more. We propose novel new techniques for using stan-Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proÔ¨Åt or commercial advantage and that copies bear this notice and the full citation
on the Ô¨Årst page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the Owner/Author.
Copyright is held by the author/owner(s).
ICSE ‚Äô14, May 31 ‚Äì June 7, 2014, Hyderabad, India
ACM 978-1-4503-2756-5/14/05
http://dx.doi.org/10.1145/2568225.2568250
653
1final JComponent tree = makeTree();
2tree.addMouseListener(new MouseAdapter() {
3public void mousePressed(MouseEvent e) {
4 int x = e.getX(), y = e.getY();
5 Object o = null;
6 // Get the menu bar or the clicked element.
7}
8});
Figure 1: Code that handles clicks on a graphical
tree of elements.
dard features of JVMs such as breakpoints to ensure that
our evaluations have no undesirable side eects, which users
can selectively enable or disable to control the tradeo be-
tween eciency, soundness, and completeness. By analyzing
over ten million lines of code, we have developed a proba-
bilistic model of real-world Java code that helps guide our
search toward more common methods and elds.
To demonstrate the benets of our approach, we con-
ducted two user studies involving 28 subjects solving mul-
tiple programming problems in dierent domains such as
GUIs, string parsing, and Eclipse plugins. The statistically
signicant results show that subjects using our tool com-
plete more tasks in less time and with fewer bugs than those
without it by more than a factor of two. Users gave our tool
positive subjective ratings.
In summary, the main contributions of this work are:
A new method (Section 3.1) for synthesizing code that
is dynamic (giving accurate results and allowing pro-
grammers to reason about concrete executions), easy-
to-use (supporting a wide range of correctness speci-
cations), and interactive (allowing users to rene the
candidate code snippets).
An ecient algorithm (Section 3.2) that exploits the
dynamic context to generate candidate statements that
can include advanced features of the host language
such as I/O, reection, and native calls.
An implementation (Section 4) as a plugin for the
Eclipse IDE called CodeHint that synthesizes Java code,
including Android programs.
Empirical evaluations and user studies (Section 5) that
show that CodeHint is ecient and signicantly im-
proves programmer productivity.
2. OVERVIEW
We now present two example problems and show how
CodeHint can solve them. The rst demonstrates Code-
Hint's algorithms while the second shows the user's per-
spective. Both examples involve the GUI code using the
Java Swing toolkit shown in Figure 1. Readers are also
encouraged to watch a demo video of CodeHint athttp:
//www.cs.berkeley.edu/~joel/codehint/ .
2.1 Algorithm Example
Imagine that a user writes the partial code shown in Fig-
ure 1 and then wants to write code at line 6 to nd the
menu bar for the window that contains the graphical tree
and store it in the variable o. A simple Internet search willreveal that the menu is represented by a JMenuBar object but
will likely provide little information on how such an object
can be acquired.
To use CodeHint to nd this object, the user can set a
breakpoint after line 5 (e.g., near the comment on line 6)
and run the program to that breakpoint, which is the cur-
rent context. Since she knows that she wants to assign a
value of type JMenuBar to the variable o, she can provide
the specication o0instanceof JMenuBar toCodeHint . We
call this specication a pdspec, and the o0denotes the value
of the variable oafter the code to be synthesized is evaluated.
Thus this pdspec encodes the fact that oshould change so
that it contains an object of type JMenuBar .
Given this query, CodeHint will begin a search for expres-
sions that it can assign to oto try to satisfy the pdspec.
This iterative search will start with local variables and gen-
erate larger expressions with operations such as addition and
method calls. CodeHint will evaluate these expressions in
the current context, undoing side eects as they occur, to en-
able it to get precise results that satisfy the user's pdspec. A
probabilistic model will guide the search toward more likely
expressions and CodeHint will group equivalent expressions
together to avoid duplicate work.
Once this search is complete, CodeHint will show the user
approximately ve results. She can then add a new testcase
to the code and run that new input, which will allow Code-
Hint to remove two of the previous results that crash in the
new context.
We now walk through CodeHint 's algorithm in detail.
First iteration.
CodeHint will rst query the debugger for all the variables
in scope, evaluating them so it knows their dynamic types.
These, along with the special values null andthis, will be-
come the initial set of candidate expressions that CodeHint
is considering:
tree, e, x, y, o, this, null
Second iteration.
In its next step, CodeHint will combine these simple ex-
pressions into more complicated ones according to the Java
grammar. To do this, it will rst query the debugger for the
dynamic type of each candidate.
For each object, it will query the debugger for all accessi-
ble methods available on that type (and its supertypes) and
then call each of those methods with all type-safe combina-
tions of the previous set of candidates as arguments (down-
casting when necessary).
As an example, CodeHint will nd that tree is a value of
type JTree (which is a subtype of JComponent , the static
type of the variable). It will then ask the debugger for
the methods of JTree , one of which is getPathForLocation .
This method expects two integer arguments, so CodeHint
will nd all of the integer-valued expressions in its previous
set of candidates, which in this case are xand y.CodeHint
will then call this method with all possible combinations of
these arguments, producing the following four calls:
((JTree)tree).getPathForLocation(x, x)
((JTree)tree).getPathForLocation(x, y)
((JTree)tree).getPathForLocation(y, x)
((JTree)tree).getPathForLocation(y, y)
CodeHint will repeat this process for other methods and for654static methods of classes imported in the current le.
For each pair of primitives, such as integers, CodeHint will
combine them with binary operations. Thus given xand y,
it will generate the following expressions:
x + y, x - y, y - x, x * y, x / y, y / x,
x == y, x != y, x < y, x <= y, x > y, x >= y
It will similarly generate object comparisons, array accesses
and length, eld accesses, integer and boolean negation (e.g.,
-x), addition and subtraction with 1(e.g., x + 1 andx - 1 ),
comparisons with 0(e.g., x > 0 and x < 0 ), and boolean
conjunctions and disjunctions.
CodeHint will evaluate each expression as it is generated
so that it knows its result. However, expressions with side
eects must be handled correctly so they do not aect future
evaluations. For example, tree.add(makeTree()) will mod-
ify the tree, potentially causing future evaluations to return
dierent results in this new context.
To avoid this problem, CodeHint uses novel techniques
based on breakpoints and the Java security manager to undo
in-memory side eects after they occur and block harmful
native calls. Harmless native calls, such as reading from a
le, proceed normally. In practice, we have found that allow-
ing users to relax these restrictions often gives correct results
in less time. In this case, evaluating tree.add(makeTree())
will modify a eld of tree. This will trigger a breakpoint
installed by CodeHint , which will log the change and undo
it after the evaluation nishes.
Once this process is complete, CodeHint will have a new
set of approximately 240 candidate expressions:
tree, e, x, y, o, this, null, x + y, x < y,
tree.getTopLevelAncestor(), Window.getWindows(),
((JTree)tree).getPathForRow(x), ...
The set of candidates can grow quite large, so CodeHint ap-
plies some optimizations that make it signicantly smaller:
To avoid duplicating work, CodeHint builds equiva-
lence classes of expressions based on their results and
only retains one representative of each class in its set of
candidates. For example, tree.getTopLevelAncestor()
is equivalent to SwingUtilities.getRoot(tree) in the
current context, so only one (the former in this case)
will be included in the set of candidates. As we will
see shortly, CodeHint will use the equivalent expres-
sions that are not in the list of candidates to help gen-
erate the results it shows to the user by substituting
equivalent subexpressions.
To avoid spending time searching expressions that are
rarely used in practice, CodeHint uses a probabilistic
model to avoid unlikely method calls and eld accesses.
This model contains information from over ten million
real-world lines of code. As one example, the JTree
class contains a method called getNextMatch that was
not called once in all of the analyzed code, so CodeHint
will not call it in this iteration.
CodeHint would have approximately 370 candidates with-
out either of these optimizations and approximately 560
without both. Since these candidates are used to generate
more expressions in the next iteration, these optimizations
signicantly improve performance.
None of these candidates meet the user's specication, as
none have type JMenuBar , so CodeHint will automaticallycontinue this process of creating larger expressions from its
current candidates.
Third iteration.
The third iteration proceeds exactly as the second: it com-
bines the current candidates to produce larger expressions.
In this iteration, CodeHint will use its probabilistic model
to avoid searching some additional expressions. Libraries
such as Swing often contain many constants that are in-
tended to be used only in certain contexts. For example,
KeyEvent.VK_ENTER helps determine if the user pressed the
Enter key. To avoid using such constants in unrelated con-
texts, CodeHint 's probabilistic model stores exactly how they
are used in practice. This allows it to avoid generating ex-
pressions such as tree.getComponent(KeyEvent.VK_ENTER) ,
as it recognizes that KeyEvent.VK_ENTER was never used as
an argument to getComponent in the analyzed code.
During this iteration, CodeHint will nd that the expres-
sion tree.getTopLevelAncestor() , whose static return type
isContainer , has type JFrame at runtime. CodeHint will
then explore all the methods it can call on a JFrame , includ-
inggetJMenuBar , which returns a JMenuBar . It will thus add
((JFrame)tree.getTopLevelAncestor()).getJMenuBar()
to its next set of candidates.
At this point, CodeHint will also explore some expressions
that it avoided before due to its probabilistic model, such as
calls to JTree.getNextMatch . This allows the algorithm to
prioritize more likely expressions while remaining complete.
Since it calls each method with all possible type-safe com-
binations of the previous set of candidates, CodeHint might
end up making an exorbitant number of calls to a single
method. For example, in this iteration it attempts to call
DefaultTreeCellRenderer.getTreeCellRendererComponent .
This method has seven arguments, including one of type
Object (of which CodeHint has seen over 70 unique values),
one of type int(of which CodeHint has seen over one hun-
dred unique values), and four booleans (each of which can be
true orfalse ), which would result in well over 100,000 calls.
To avoid spending so much time calling a single method,
CodeHint only calls it a small number of times. Specically,
if a method can be called more than a certain number of
times that grows exponentially with the current iteration,
CodeHint calls it at most that many times with arguments
that were candidates from previous iterations.
At the end of this iteration, CodeHint will have generated
and evaluated over 700 expressions.
Once it has nished this process, CodeHint will nd which
of its candidates satisfy the user's pdspec. In this case,
((JFrame)tree.getTopLevelAncestor()).getJMenuBar() ,
when assigned to o, is the only expression that satises the
pdspec. Before showing it to the user, CodeHint will gener-
ate more satisfying expressions by replacing its subexpres-
sions with equivalent expressions.
For example, because SwingUtilities.getRoot(tree) is
equivalent to tree.getTopLevelAncestor() ,CodeHint knows
((JFrame)SwingUtilities.getRoot(tree)).getJMenuBar()
will yield the same result and hence also satisfy the user's
pdspec. CodeHint will thus generate this expression and
three others that are all equivalent to the original expression.
Now that it has expressions to show to the user, CodeHint
will use its probabilistic model to present the user with the
more likely options closer to the top. A call's likelihood is
the number of calls to it that the model contains and an655expression's likelihood is the product of its subexpression's
likelihoods. In this case, the getTopLevelAncestor method
was called 19 times in the model while getRoot was called
15 times, so the former expression is shown rst.
CodeHint will then show the user these expressions that,
when assigned to o, meet the specication (as well as their
values, side eects, and string representations):
((JFrame)SwingUtilities.getWindowAncestor(jtree))
.getJMenuBar()
((JFrame)tree.getTopLevelAncestor()).getJMenuBar()
((JFrame)SwingUtilities.getRoot(tree)).getJMenuBar()
...
IfCodeHint had not found any results that satised the
user's pdspec at this point, it would have notied the user
and asked if it should continue for another iteration.
Note that the results CodeHint found all rely on being able
to discover the dynamic type of an expression and downcast
to it, something that static tools will nd dicult to do.
ReÔ¨Ånement.
The user can now examine these expressions, their results,
and their documentation to try to pick the one she wants to
use. If she is unsure which is correct, she can rene the set
of results by running the program on a dierent input.
Let us assume that the user modies the code so that
the tree is contained within an applet that is itself con-
tained within the top-level window, which might be useful
for running the same code both by itself and within a web
browser. Once she makes this change, she can run the new
program until it reaches the point where she ran the previ-
ous search. CodeHint will then evaluate the previous results
in the new context. In this case, two of them, including
((JFrame)tree.getTopLevelAncestor()).getJMenuBar() ,
will crash. CodeHint will show this smaller set of results to
the user, who may select one of them to use or continue the
process with yet another input.
2.2 User Perspective Example
A common task when writing GUI code is to detect clicks
on a graphical tree of elements. A programmer might be
unsure how to nd the clicked element so she can use it.
Unfortunately, as she does not know the API, she is not even
sure what type this object has; it could be a node object,
the data it represents, or the displayed string. As she does
not even know the type of the expression she desires, it is
dicult to nd. Using CodeHint , she can easily run the code,
click on a node, and give a pdspec that expresses which node
she clicked.
To use CodeHint to nd the code she desires, she can set a
breakpoint after line 5 and then execute her code and click
on the top element. Knowing that in Java most objects
have a toString method that gives a string representation
of their value, she realizes that if she clicks on an element
labeled \Alice", the toString of her desired result should
contain that string. This insight can lead her to enter the
pdspec o0.toString().contains("Alice") , which encodes
the fact that the value of the variable oshould be updated by
the desired statement so that its toString contains \Alice".
CodeHint will then generate eight expressions to assign to o:
((JTree)tree).getPathForLocation(x, y),
((JTree)tree).getSelectionPath(),
((JTree)tree).getLastSelectedPathComponent(), ...To reduce the number of candidates, the user can con-
tinue the execution (or restart it) and click on a dierent
element. Assume she does so and clicks on an element la-
beled \Bob". When the execution suspends at the break-
point, CodeHint will show her all eight of the previous ex-
pressions with their results in the new context. She can then
give a new pdspec to lter out some expressions. Giving
o0.toString().contains("Bob") will remove one expres-
sion. Alternatively, by looking at the results of the eight
expressions she might see that many return an object of
type TreePath that seems to do what she wants, so she can
use the pdspec o0instanceof TreePath to keep only those.
Assume that now the user clicks below all the elements.
She can then see all of the remaining candidate expressions
with their new values and keep only those that evaluate
tonull with the pdspec o0== null .CodeHint will then
eliminate all but one candidate and nd the correct code:
o = ((JTree)tree).getPathForLocation(x, y);
This example shows how our approach can be useful even
when a programmer has very little knowledge about her de-
sired result, perhaps not even its type, by allowing her to
refer to values in the concrete program state. The program-
mer narrows down the set of candidate statements by incre-
mentally providing pdspecs for dierent test scenarios. A
key advantage of our methodology is that programmers can
mix and match value demonstrations, type specications,
and arbitrary pdspecs as desired. In addition, the interac-
tivity of CodeHint allowed the user to nd the correct result
from the initial candidates. This example was inspired by
how one subject in our rst user study solved this problem.
This example also shows how our approach can easily syn-
thesize real code involving complicated libraries. While the
nal synthesized code appears simple, executing it involves
making over 70 method calls that allocate new objects and
use complex objects, generics, and binary-only libraries.
Using skeletons.
Perhaps now the user wants to get the data representing
the object she clicked out of the TreePath object she just
assigned to o. Let us assume that she changes o's type in
the code and discovers from the type's documentation that
it contains a getPathComponent method that she thinks will
help her. However, she is unsure what argument to pass to
this method.
The user can encode this knowledge into a skeleton that
CodeHint will use to guide its search. Specically, she can
enter the skeleton o.getPathComponent(??) where the ??
represents the missing portion of the code. Given this skele-
ton, CodeHint will search for expressions that can be used as
arguments to getPathComponent , which expects an integer.
As before, the user can set a breakpoint where she wants
to insert code, run the program on a test, and click on an
element to hit the breakpoint. If she clicks on \Eve", she can
enter the pdspec _rv0.toString().equals("Eve") to show
that she now wants some object that represents \Eve" (the
_rv0represents the return value of the expression) along with
the skeleton discussed above to guide the search.
CodeHint will now search for integers that, when passed
togetPathComponent , meet the user's specication, which
include the following:
o.getPathComponent(e.getClickCount())
o.getPathComponent(o.getPathCount() - 1)656By rening the set of candidates by giving dierent pdspecs
in dierent states, the user can remove incorrect expressions
such as the rst one above.
This example shows how users can encode their partial
knowledge of the correct code to allow CodeHint to focus its
search on relevant code snippets.
3. PDSPECS AND SYNTHESIS ALGORITHM
We now dene pdspecs and our synthesis algorithm.
3.1 Pdspecs and Our Approach
Without loss of generality, let us assume that we have an
incomplete program in which a statement, say sT, is miss-
ing. The developer wants to synthesize this statement using
CodeHint . She creates a test input so that the execution of
the program on the test input reaches the program location,
say`, that is just before the missing statement. Let be
the program state when the program reaches the program
location`. We use  to denote the set of all feasible pro-
gram states. The goal of CodeHint is to discover the missing
statementsT. Let exec(;s) be the program state obtained
by executing the statement sin state. Let us use 0to
denote exec( ;sT), i.e., the program state reached after the
user executes the missing statement sTin the state . As a
running example, let us assume that isfx7!42gandsT
isx = 2 * x . Then0isfx7!84g.
The user does not know the missing statement sT, but she
might have a good idea about what the program state should
look like after the execution of sT(i.e.,0). With CodeHint ,
the user can indirectly provide a hint about the statement
sTby giving information about the state 0using pdspecs.
A pdspec can give absolute information about 0by describ-
ing the updated program variables and their corresponding
values or it can be a predicate relating the states and0.
For example, a pdspec can specify x0== 84 , where x0rep-
resents the value of xin state0, or it could specify x0> x,
a predicate that relates the states and0. In general, a
pdspec can consist of any expression in the host language.
Note that a pdspec that is specied in the state may
not be a correct pdspec if the program is in a dierent state
at the location `. (A dierent state at location `can be
reached by executing the program on a dierent test input.)
For example, x0> xis not a correct pdspec if the program
state at location `isfx7! 20g, but it is a correct pdspec
if the state isfx7!10g. Similarly, x0== 84 is not a cor-
rect pdspec for any program state with x != 42 . Note that
x0== 2 * x is a correct pdspec for all program states that
reach the location `.
Formally, a pdspec is a logical predicate 2!
ftrue;falsegwhere(;0) checks whether 0is a desired
output state given the input state . As a notational con-
venience, for a pdspec (;0), we refer to variables in the
input state using their names and variables in the output
state0using their primed names. All variables not given in
a pdspec must be equal in the two states and all expressions
in a pdspec must be free of side eects.
The user of CodeHint gives a sequence of pairs of pro-
gram states at `(reached by executing the program on dif-
ferent test inputs) and their corresponding correct pdspecs:
(0;0);(1;1);. LetSbe the set of all syntactically
valid statements that can be written at location `. Based
on the sequence of pairs of states and pdspecs, CodeHint
returns a set of candidate program statements that couldreplace the missing statement. For example, if the user pro-
vides the sequence of pairs ( 0;0);;(i;i), the set of
suitable statements for the location `is reduced to the set
Ci=8
<
:s2S:^
0jij(j;exec(j;s))9
=
;:
The predicate j(j;exec(j;s)) is true if the state jand
the state obtained after executing statement sin the state
jsatisfy the pdspec j. Statement sis in the candidate
set if this predicate is true for all j. If a statement sis in
the candidate set, it implies that at least for the program
states1;;iobserved at the location `, the execution of
the statement swill result in the user's expected program
state. If we could compute the candidate set for all possi-
ble pairs of programs states reachable at `and their corre-
sponding correct pdspecs, the set is guaranteed to contain
the target statement sT. However, in practice it not possi-
ble to enumerate all such possible pairs. Instead, the user
demonstrates such pairs one-by-one and CodeHint computes
a candidate set from the pairs provided by the user. Note
thatCi+1Ci, so the size of the candidate set shrinks as
CodeHint receives more pairs. At any point, if the user no-
tices a statement she could use as a substitue for the missing
statement, she stops the process and uses the statement.
As the set of legal statements Scould be innite, CodeHint
restricts it to a nite set. For example this set could only
include statements whose abstract syntax trees have height
at mostkand only use the variables available in the current
scope. We call kthe depth of the search space.
In practice, users often have some idea of the structure
of the statement they desire. In our running example, the
user might know that she wants to multiply xby something
but might not know exactly what should be multiplied. By
providing a skeleton (discussed more in Section 3.2), the user
can further restrict the search space. Here, this skeleton
might be x = ?? * x , where ??could be replaced by any
valid expression.
In our running example, given the initial state fx7!
42g, the pdspec x0> 42, and the skeleton x = ?? * x , we
present the user with the following set of candidate state-
ments:
C0=fx = 2 * x, x = x * x,  g
Without the skeleton, we would additionally have included
statements like x = 84 ,x = x + 1 , and x = x + x . In ei-
ther case, the number of candidate statements might be large
but will be nite.
Given another initial state fx7!6g, the pdspec x0== 12 ,
and the same skeleton, we present the user with the subset
C1=fx = 2 * xg.
3.2 Synthesis Algorithm
Our algorithm generates statements containing Java ex-
pressions that include variables, array accesses, casts, eld
accesses, method calls, constructor calls, and unary and bi-
nary operators, including calls to static methods and elds of
imported classes.1It begins by collecting all the variables in
scope and then iteratively uses the current set of statements
1There seems to be no technical barrier to extending this
language to cover all Java statements (except for anonymous
class declarations), but we have seen no need to do so yet.657to generate larger ones. When it reaches a xed maximum
size, it removes those that do not satisfy the user's pdspec.
In particular, our algorithm generates statements in in-
creasing order of depth , which is the height of the parse tree.
As examples, xhas depth 1, foo.bar(x,y) has depth 2, and
foo.bar.baz(x+y) has depth 3.
A key insight is that if two statements have the same ef-
fects, we can treat them as equivalent when generating fur-
ther statements. We take advantage of this fact by grouping
statements into equivalence classes based on their eects and
values. As an example, there might be hundreds of pure
boolean-valued expressions at a given depth, but they all
evaluate to either true orfalse . We henceforth describe
our algorithm in this special case of expressions but note
that it works for the more general class of statements.
We dene an equivalence relation on expressions such that
two expressions are equivalent if they have the same side ef-
fects and yield the same value in the current state. We
writee1e2to denote that the expressions e1ande2
have the same side eects and yield the same value in the
state. Following standard Java idioms and practices, two
expressions e1and e2are equivalent if both are primi-
tives and e1== e 2, both are objects of the same type and
e1== null ? e 2== null : e 1.equals(e 2), or both are ar-
rays of the same type with the same number of elements and
all corresponding elements are equivalent.
Our algorithm iteratively builds all expressions up to a
xed depth, evaluating them with a timeout and creating
equivalence classes as it goes. For each equivalence class, we
use only one representative to generate further expressions.
For example, if xand z.f(0) are equivalent, we generate
x+1and foo(x) but not z.f(0)+1 orfoo(z.f(0)) .
We take the current set of side eects into account when
generating new expressions from representatives of these
equivalence classes. For example, when xis42we will no-
tice that xand 42are equivalent. Later, when considering
expressions to add to x++, we will use the equivalence class
for the state where xis43and consider xand42separately.
When the desired depth has been achieved, we test the
equivalence classes for the initial state against the pdspec,
lter out those that do not satisfy it, and recreate the full
set of candidate expressions from the equivalence classes.
Once we discover that an expression satises the pdspec, we
know that all expressions generated from it by replacing its
subexpressions with expressions that are equivalent in the
current state will also satisfy the pdspec. Continuing the
previous example, if p.bar(x) satises the pdspec, so will
p.bar(z.f(0)) .
We apply a number of optimizations to improve the e-
ciency of our algorithm. We use a variety of simple struc-
tural techniques to avoid enumerating obviously equivalent
expressions (such as x+yand y+x). We call the hashCode
method to speed up checking equivalence between objects.
We cache the result of each expression and use that result
instead of the expression itself in future evaluations (and
replay its side eects) to avoid duplicating work.
By actually evaluating the generated expressions, our al-
gorithm can synthesize real-world Java code, including le
I/O, binary libraries, reection, foreign function calls, and
calling the user's own methods.2Thus while the individual
expressions we generate appear somewhat simple, they can
2Some of these features require the user to allow searching
native calls, in which case we cannot undo side eects.in fact be quite complicated. In addition, we were able to
synthesize code for Android without any extra modeling.
Probabilistic model and pruning.
To guide our algorithm toward exploring more likely ex-
pressions, we have added a probabilistic model based on an
oine analysis of over ten million lines of code. This allows
us to compute how often certain types, methods, and elds
are used. Having analyzed how often each method and eld
is accessed, we dene the probability of accessing member m
(calling a method or accessing a eld) of type Tas
P(mjT)P(T) =# accesses of monT
# of accesses on T# of accesses on T
# of accesses:
The probability of an expression is then the product of the
probabilities of its individual accesses. In addition, as Java
classes often contain many constants, we store, for each con-
stant eld, all the places it is used as an argument to a
method. This model is somewhat simplistic, as it assumes
that all method calls are independent, but we have found it
very helpful in practice.
We use this probabilistic model to avoid calling rare meth-
ods and using constants in places they were rarely used as
well as to sort the candidates presented to the user.
The size of the space of expressions that our algorithm
must search at a given depth is exponential due to method
calls. If at one point during a search we have seen 50 dif-
ferent integer values, there will be 503dierent ways to call
a method with three integer arguments. Such growth can
easily overwhelm our algorithm, so we heuristically prune
calls to methods that would otherwise be called with a large
number of dierent arguments, especially ones our proba-
bilistic model denes as uncommon. That is, if a method
can be called more than a certain number of times that is
exponential in the current depth, we articially increase the
depth of calls to it. As we will see in Section 5.1, such calls
occur infrequently in practice. We believe this would be a
promising area in which to integrate symbolic techniques.
Skeletons.
As mentioned earlier, users may provide a skeleton to
shape the search space explored and guide it toward can-
didates they know to be likely. Skeletons consist of normal
Java code with holes for unknown code that should be syn-
thesized. The language contains two types of holes: simple
holes (denoted ??) and list holes (denoted **). Simple holes
can take the value of any expression or name in the language
or be annotated with a set of candidates. List holes are used
for calling functions with an unknown number of arguments,
each of which is a separate expression hole. Users may ad-
ditionally check a box indicating whether or not constructor
calls and inx/prex operators should be searched.
To give some examples, ??.?? represents accessing some
eld of an unknown expression, foo.??{bar,baz}(??) rep-
resents calling either the baror the bazmethod of the foo
object with a single unknown argument, and ??(**) repre-
sents calling an unknown method with any number of un-
known arguments.
Since the given pdspec applies to the whole statement,
we must explore the cross product of all the specied holes.
Given a skeleton, we ll each hole with type-correct val-
ues using the algorithm described above (unless the hole
has been annotated with candidate values, in which case we658simply try those values in lieu of a search). If a skeleton has
many holes, we reduce the depth of our search for each hole.
4. IMPLEMENTATION
We have developed an implementation of our approach,
which we call CodeHint , as a plugin for the Eclipse IDE
for Java.3This allows users to develop normally, using our
approach only when they wish to do so.
To use CodeHint , a programmer must start a debug ses-
sion and navigate to the program location and state in which
she wishes to insert code. She then enters a pdspec and pos-
sibly a skeleton through a dialog.
We then synthesize candidate expressions and show them,
their results and toString s, and their side eects to the user,
who can select which to keep. The user may also view the
Javadocs of methods and elds used in the candidates and
sort or lter them. If she does not nd any expressions she
wants, she can continue the search with an increased depth.
When execution encounters the location of a previous syn-
thesis, we begin our renement process by showing the user
the previously-chosen candidates and their values in the new
context. She may enter a new pdspec to rene the current
set of candidates or lter or sort the results. In addition,
she may abandon the current results and start a new search,
which can be useful if she previously gave an invalid pdspec
or if our search did not nd any correct statements.
Our implementation detects in-memory side eects while
evaluating expressions by installing watchpoints to listen for
eld accesses. This allows us to log all the side eects of an
evaluation and afterwards undo them and show them to the
user. We use Java's security manager to disable external side
eects such as deleting les and we block unknown native
calls (which do not go through the security manager). Users
can selectively enable or disable these features to control the
tradeo between eciency, soundness, and completeness.
5. EVALUATION
We now show, through empirical analysis and two user
studies, that CodeHint is suciently scalable and that it
makes users more productive.
5.1 Empirical Evaluation
Scalability.
To analyze the eciency of our implementation and demon-
strate its scalability with regards to depth (as dened in
Section 3.2), we ran the tasks used in our rst user study
(described in Section 5.2.1) with a typical pdspec (i.e., the
one most frequently used by subjects in the study) and a
skeleton indicating that we should not search constructors
and operators. For each task, we varied the maximum depth
searched, and for each depth, we recorded the number of
unique expressions evaluated and the total time taken.
The experiments were performed on an Intel Core 2 Duo
E6850 with two 3 GHz processors and 3 GB RAM (al-
though our current implementation is single-threaded) run-
ning Linux 3.13.5 and Java 1.7.0 51.
The results in Table 1 show that our current implemen-
tation, when not undoing side eects, can explore a search
space of depths 1, 2, and 3 in one second and depth 4 in
3CodeHint , its source, and video demos are available at
https://github.com/jgalenson/codehint .ve seconds. We do not show depth 5, for which we have
not optimized and all but four benchmarks timeout after
two minutes. When undoing side eects, our unoptimized
implementation takes a couple of seconds to search depth 2.
We believe these results are more than sucient for a useful
tool, and results from the user studies show that users agree.
As part of our methodology, CodeHint nds all expressions
in its search space that satisfy the user's pdspec, not just
one. Since we display each such candidate expression as
soon as we discover it, users can usually see some candidate
expressions well before the search terminates.
To measure the eectiveness of various parts of our algo-
rithm, we searched depth 3 with pruning disabled and with
pruning, equivalence classes, and the probabilistic model dis-
abled. The results, given in the rightmost columns of Ta-
ble 1, show that these improvements signicantly reduce the
search space, enabling CodeHint to nd larger expressions.
Without them, it would not be able to search even depth 3.
To show that CodeHint can synthesize large code frag-
ments, we tracked the size of the expressions explored for
each task. The largest such code snippet contained ten
method calls, showing that our algorithms can indeed scale
to non-trivial code fragments.
Analysis of expressions.
To justify our algorithm, we analyzed ve medium to large
open-source Java projects. The key result is that the vast
majority (99%) of assignment statements are actually quite
simple (with a depth of at most 4).
We analyzed Hadoop, a framework for distributed com-
puting based on MapReduce with a distributed le system,
Tomcat, a web server, FindBugs, a static analysis tool, Hi-
bernate, which maps objects into a database, and JDK, the
implementation of Java.
For each program, we analyzed the right-hand side of as-
signment statements, which is the exact class of statements
we currently generate. Hadoop version 1.0.3 contains 47,248
such expressions, Tomcat 7.0.30 contains 30,740, FindBugs
2.0.1 contains 18,589, Hibernate 4.1.7 contains 54,580, and
JDK 6.25 contains 227,731.
We began by analyzing the depth of the expressions. All
ve programs followed a similar pattern: on average, 27% of
expressions had depth 1, 52% had depth 2, 15% had depth 3,
5% had depth 4, and 1% were more complicated. This shows
that the vast majority (99% in our study) of statements in
real Java code have depth at most 4, which our results above
show that CodeHint can easily search.
To show that these results also hold for code programmers
struggle to write, we repeated the same experiment on code
snippets gathered from questions asked on the popular Stack
Overow website. We examined 43 code samples containing
3333 lines of code and found nearly the same distribution:
25% had depth 1, 59% had depth 2, 13% had depth 3, and
3% had depth 4. While users might want to synthesize mul-
tiple lines of code at a time, we believe these results suggest
that our current algorithms can help real programmers.
We conservatively bounded the number of cases in which
we pruned calls to methods that could be called with many
dierent arguments by analyzing the number of calls with
a given number of arguments. All ve programs followed a
similar pattern: 51% of calls had no arguments (excluding
the receiver), 31% had one argument, 12% had two argu-
ments, 3% had three arguments, and 3% had four or more.659Table 1: An empirical analysis of our algorithm. Each row represents one task from the rst user study. The
rst six result columns show how our algorithm performs at dierent depths, the next shows the performance
when we undo side eects, and the last two show the performance with various parts of the algorithm disabled.
The # columns show the number of expressions searched and times are in seconds.
Normal algorithm Side eects No pruning Brute force
Depth 2 Depth 3 Depth 4 Depth 2 Depth 3 Depth 3
# Time # Time # Time Time # #
P 1 34 0.4 611 1.1 19259 9.1 0.4 54911 2034829
P 2 57 0.5 912 1.9 35232 7.7 0.8 10953 847418
P 3 124 0.4 1156 1.2 124991 17.8 1.0 157052 6200476
P 4 7 0.2 36 0.3 552 0.8 0.3 36 219
P 5 22 0.3 234 0.4 2565 1.1 0.3 4692 155774
S 1 8 0.9 223 2.0 1401 3.6 4.3 3775 39439
S 2 12 0.6 275 1.4 2043 3.6 4.5 4457 51079
S 3 70 0.7 814 2.8 6645 6.2 4.9 41359 1867350
S 4 103 1.1 842 3.7 22138 10.6 8.5 504018 61246626
S 5 32 0.8 595 2.8 179956 15.8 6.7 24409 272025
R 1 22 0.2 98 0.3 846 0.6 0.6 98 33112
R 2 12 0.2 137 0.3 1090 0.5 0.3 137 1004
R 3 8 0.1 13 0.1 51 0.2 0.5 13 19
R 4 7 0.2 19 0.2 80 0.3 0.3 19 33
R 5 24 0.3 229 0.3 1761 1.9 0.4 609 115410
Avg 36.1 0.5 412.9 1.3 26574 5.3 2.3 53769.2 4857654.2
Med 22 0.4 234 1.1 2043 3.6 0.6 4457 115410
Thus 94% of calls in practice contain two or fewer arguments
and hence little combinatorial explosion, and so biasing our
search to avoid the rest seems benecial. We additionally
note that just because a method has multiple arguments
does not mean that there will be many ways to call it, as ar-
guments are often of types that have few values in practice,
such as singletons. Our heuristic will not avoid such calls.
5.2 User Studies
We conducted two user studies to evaluate the usability of
CodeHint and learn how developers use it. The rst study
focused on constrained single line code edits and the second
focused on larger open-ended tasks and used an improved
version of CodeHint . Observations from these studies gener-
ated additional areas for improvement that we implemented
in the meantime, including the skeletons of Section 3.2.
5.2.1 Methodology
Study 1: Line-level tasks.
For the rst study, we created three scenarios with ve
sub-tasks each that mimic code completion tasks program-
mers face in practice. To focus on particular tasks where
CodeHint could be applied, we provided working wrapper
code that participants had to extend. The Parse (or P)
scenario manipulated strings and parsed email headers and
command-line arguments. The Swing (or S) scenario cre-
ated a small GUI. The RandomWriter (or R) scenario cre-
ated a Markov model to generate output that looked similar
to input text. We chose the rst two scenarios to represent
common tasks involving APIs and the third as an exam-
ple application. The rst part of Section 2.2 is a slightly
modied version of one of the tasks.
At the time of the study, CodeHint could solve thirteen of
the fteen tasks; we included the remaining tasks to see how
subjects handled cases it could not solve (both could havebeen found with a slightly higher search depth and can be
found by the current version of the tool).
In our within-subjects study, each user received a random
assignment of control ,experimental , orchoice conditions to
scenarios. In the control condition, users could not use Code-
Hint, in the experimental condition they were required to
use it (although they could write code normally if it failed),
and in the choice condition they could decide whether or
not to use CodeHint for each task. We counterbalanced the
order of the experimental and control groups and assigned
the choice condition last so participants had experience both
using CodeHint and writing code normally.
Study 2: Open-ended tasks.
The tasks in the second study were larger { each task re-
quired writing three to sixteen lines of code that used com-
plex APIs. Two scenarios had three tasks each, again scaf-
folded by wrapper code. The Eclipse scenario implemented
a simple Eclipse proler plugin and the Note scenario in-
volved writing a GUI note-taking application that synchro-
nized data to the cloud. Example tasks included nding all
the objects in the heap of a program running in Eclipse and
adding a menu item that made selected text bold.
Each participant solved one scenario in the control group
and the other in the choice group (as dened above). Sub-
jects in the choice group could solve tasks using a combi-
nation of CodeHint and traditional techniques. Both the
scenarios and the groups were ordered randomly. We addi-
tionally allowed subjects who could not solve Eclipse tasks
in the control group to solve them with CodeHint . The tasks
were designed to be dicult to solve, so we stopped subjects
if they had not completed a task after twenty minutes.
5.2.2 Participants
Nine subjects initially completed the rst study; another
ve were recruited later to complete only the choice con-6600 20 40 60 80 100 120ControlCodeHint
(improved)
Time (s) Study 1 Task Comp letion  Time Figure 2: The task completion time of subjects in
our rst user study. The error bars show the stan-
dard error.
0 10 20 30 40 50 60 70 80CodeHintControl
% completed Study 2 Task C ompletion Rate 
Figure 3: The task completion rate of subjects in
our second user study. The error bars show the
standard error.
dition of the same study with an improved version of the
tool to collect additional data. Twelve were graduate stu-
dents in Computer Science at UC Berkeley and two were
undergraduates.
Another fourteen subjects completed the second study,
which used a further improved version of CodeHint . Ten
were undergraduates and four were graduate students in
Computer Science at UC Berkeley.
In both studies, participants practiced on some training
tasks rst and completed a post-test questionnaire. They
were allowed to use a web browser to search for help. None
of the subjects had ever used CodeHint before, but all were
somewhat familiar with both Java and Eclipse.
5.2.3 Measures
We dened the following measures:
Task completion time : Time taken to either complete or
abandon a task.
Task completion rate : Percentage of tasks users success-
fully completed.
Code quality : Number of bugs in participants' task code.
Tool choice : Fraction of tasks in the choice condition for
which participants opted to use CodeHint .
5.2.4 Results
We rst discuss quantitative results followed by qualita-
tive impressions of how our participants used CodeHint .
Productivity and preference.
Completion time: In all but one case, participants in the
rst study completed all tasks, so we focus our analysis on
task completion times. On average, subjects using the im-
proved version of CodeHint completed tasks in 46 seconds
and control participants took 97 seconds (see Figure 2). Thisdierence is signicant (two-sample t(11) = 2:42;p= 0:033,
two-tailed). This suggests that programmers are more pro-
ductive when using CodeHint .
Completion rate: Participants in the second study did not
complete many tasks, so we focus on task completion rates.
Figure 3 shows the task completion rate for users in our sec-
ond study with and without CodeHint . On average, subjects
using CodeHint completed 69% of sub-tasks while those not
using it completed 27% ( 2(1;N= 14) = 26:06;p< 0:001),
which strongly suggests that programmers complete more
dicult API tasks when using CodeHint .
Code quality: Participants introduced 11 bugs in 122 tasks
solved with CodeHint in our rst study and 24 bugs in
93 tasks in code written without it (two-sample t(12) =
2:81;p= 0:015, two-tailed). This suggests that CodeHint
improves code quality. To focus on the completion rate, we
gave subjects in the second study a comprehensive test suite,
so they wrote almost no bugs.
Tool choice: In the choice condition, each user could elect
whether or not to use CodeHint for each task. On average
over both studies subjects used CodeHint 71% of the time,
suggesting that users found CodeHint valuable.
In the questionnaire, participants in both studies rated
the overall usefulness of CodeHint at an average of 7.7 out
of 10 (with a standard deviation of 1.2). All users reported
that they would use CodeHint for their own development if
it were available for their language and editor and had some
simple aws xed. Six of the subjects asked for the plugin
shortly after completing the user studies and installed it.
Qualitative results.
CodeHint presented only a small number of candidates to
the user after the initial pdspec: in the rst study, the aver-
age number of candidates across all episodes was 13 and the
median was 2, and in the second, the average was 34 and
the median was 3. When rening an existing set of candi-
dates, users in both studies provided pdspecs that reduced
the number of candidates by 31% on average. However,
47 out of the 89 renements did not reduce the size of the
candidate set at all, mostly because all the candidates were
already equivalent on all possible inputs. Ignoring those, the
average reduction was 66%.
Choosing a pdspec requires trading the strength of the
specication for the ease of encoding it and the cost of eval-
uating it. To examine this tradeo, we classied all of the
pdspecs used by subjects while completing the user stud-
ies and found that in the rst study, 53% demonstrated the
desired value, 33% gave the desired type, and 14% were
arbitrary predicates. In the second study, subjects used
only type demonstrations for the Eclipse scenario (as its
diculty lay in nding and using complex types), but for
the Note scenario they gave 51% value demonstrations, 38%
type demonstrations, and 10% arbitrary predicates. This
shows that users can get benets with CodeHint even while
demonstrating simple pdspecs but that the ability to provide
more expressive pdspecs is sometimes valuable.
6. RELATED WORK
Programming by demonstration, also called programming
by example, is a popular area of research [10, 7, 19, 16], much
of which deals with synthesizing macros [17] and scripts [18].
Unfortunately, these techniques have not been widely adopted,
in part because users have diculty understanding and cor-661recting the learned generalizations [15]. We attempted to
avoid these problems by integrating CodeHint into the user's
workow and encoding its state directly in the code.
There has been much work on live programming and how
it can benet programmers [38, 3, 34]. This work inspired
us to design our methodology to support concrete reasoning.
Program synthesis has had numerous successes at synthe-
sizing code in small well-dened domains such as bitvector
logic [13] and data structures [30, 11] as well as somewhat
more general classes of programs [31, 14, 32]. As they are
backed by decision procedures and SMT solvers, these tech-
niques are very ecient in certain domains that have been
fully modeled, while CodeHint is not domain-specic and
works without any modeling (e.g., it can read from the le
system). We thus view these as complementary techniques.
There has been much research on helping programmers ex-
plore new APIs by mining existing code to nd snippets that
are used in practice [20, 12, 27, 35, 9]. Unlike such systems,
by evaluating code at runtime, we can dierentiate between
dierent values of the same type, downcast precisely, and
use more general specications, as in Section 2.
Test generation techniques [37, 39, 28] generate inputs to
explore branches within code (which is equivalent to satisfy-
ing boolean specications) but they do not always generate
the code to construct those inputs. Seeker [36] synthesizes
code fragments with a combination of dynamic and static
analysis. Its reliance upon static analysis means it cannot
generate certain code fragments that CodeHint can but al-
lows it to be more ecient in many cases. We would like to
integrate similar techniques into CodeHint .
Some existing code search tools [25, 4, 21, 1] allow more
general specications such as natural-language queries or
testcases, but they lack the full power of our pdspecs and
our ability to use dynamic information. Our skeletons are
similar to the partial expressions of [24] and the holes of [31].
The Smalltalk method nder [2] allows programmers to
specify concrete arguments and the desired result and then
evaluates all methods of the given receiver with the given
arguments to see which return that result. The algorithm
is thus very simplistic, but it allows giving multiple demon-
strations, similar to our renement methodology.
In summary, the key benets of our approach are the pre-
cision enabled by our dynamic nature, the generality of our
pdspecs, and the fact that we can handle the full Java lan-
guage without any modeling. The main disadvantages are
that we are less ecient and provide fewer correctness guar-
antees than some existing techniques.
The way our algorithm iteratively generates expressions of
larger depth is similar to the approach used by CHESS [22],
as we both search the space in a way that prioritizes elements
that are more likely to occur in practice. Our representation
of the search space is similar to version space algebra [17, 16]
but we can more eciently eliminate redundant elements.
Our equivalence classes are similar to ideas from Daikon [8],
Randoop [23], and model nding [40]. Unlike Randoop, we
enumerate all solutions, guided by our probabilistic model
toward more likely expressions, up to some bound instead
of randomly searching the space. Our techniques are also
more powerful than these, and they handle primitives, ob-
jects, and arrays. We also apply the algorithms in the new
domain of program synthesis, using them to gain the ability
to synthesize real Java code. Most importantly, unlike these
approaches we are sound in the presence of side eects.Our probabilistic model is currently simpler than those
used in other work, e.g., [5]. As we have found our model to
be very useful in practice, we would like to improve it.
Many existing techniques for undoing side eects in Java
focus on transactions [26] or sandboxing [29, 33]. These
approaches can be eective but often require modifying the
client code or the runtime system. In contrast, our watchpoint-
based algorithms are slower but require only a standard
JVM. Research on speeding up watchpoints [41] could im-
prove the eciency of our approach, or we could use a more
ecient technique. Some researchers have used standard
JVM techniques to reset only static state [6]; our approach
in some ways generalizes this work to reset all state.
7. FUTURE WORK
We plan to continue to improve our algorithms. One
promising technique is to integrate a symbolic solver into our
approach, as both have complementary strengths; our algo-
rithm can handle arbitrary Java method calls while symbolic
techniques can quickly explore a very large search space in
certain domains. We would also like to continue to improve
our probabilistic model of what code looks like in practice.
Our approach seems well suited to dynamic languages, as
unlike static tools, we can leverage their runtime informa-
tion. We would thus like to develop an implementation of
CodeHint that works for a language such as JavaScript.4
8. CONCLUSION
We have presented a novel methodology that helps pro-
grammers write dicult statements. This methodology is
dynamic (evaluating code and runtime and letting users in-
spect the results), easy-to-use (accepting a wide range of
specications), and interactive (helping users gain informa-
tion about the missing code). Our algorithms are ecient
and let us synthesize code that uses real-world Java features
such as native calls and reection. We have run two user
studies that shows that our tool CodeHint signicantly im-
proves programmer productivity.
9. ACKNOWLEDGMENTS
We are very grateful to all of the participants in our user
studies for their time and suggestions. We would also like to
thank Leo Meyerovich for his detailed feedback. This work
is supported in part by NSF Grants CCF-1018729, CCF-
1018730, CCF-1139138, CCF-1017810, and CCF-0916351
and gifts from Samsung and Mozilla. The last two authors
are supported in part by Sloan Foundation Fellowships.
10. REFERENCES
[1] S. Bajracharya, T. Ngo, E. Linstead, Y. Dou, P. Rigor,
P. Baldi, and C. Lopes. Sourcerer: A search engine for
open source code supporting structure-based search.
OOPSLA '06, pages 681{682, 2006.
[2] A. Black, S. Ducasse, O. Nierstrasz, D. Pollet,
D. Cassou, M. Denker, et al. Pharo by example . 2009.
[3] M. M. Burnett, J. W. Atwood Jr, and Z. T. Welch.
Implementing level 4 liveness in declarative visual
programming languages. VL '98, pages 126{, 1998.
4A prototype implementation of CodeHint for JavaScript is
available at https://github.com/jgalenson/codehint.js .662[4] S. Chatterjee, S. Juvekar, and K. Sen. Sni: A search
engine for java using free-form queries. In FASE '09,
pages 385{400, 2009.
[5] A. Cozzie and S. T. King. Macho: Writing programs
with natural language and examples. Technical report,
University of Illinois at Urbana-Champaign, 2012.
[6] C. Csallner and Y. Smaragdakis. JCrasher: An
automatic robustness tester for Java.
Software|Practice & Experience , 34(11):1025{1050,
Sept. 2004.
[7] A. Cypher, D. C. Halbert, D. Kurlander,
H. Lieberman, D. Maulsby, B. A. Myers, and
A. Turransky, editors. Watch what I do: programming
by demonstration . MIT Press, 1993.
[8] M. D. Ernst, J. Cockrell, W. G. Griswold, and
D. Notkin. Dynamically discovering likely program
invariants to support program evolution. ICSE '99,
pages 213{224, 1999.
[9] T. Gvero, V. Kuncak, and R. Piskac. Interactive
synthesis of code snippets. In Computer Aided
Verication , pages 418{423. 2011.
[10] D. C. Halbert. Programming by example . PhD thesis,
1984.
[11] P. Hawkins, A. Aiken, K. Fisher, M. Rinard, and
M. Sagiv. Data representation synthesis. PLDI '11,
pages 38{49, 2011.
[12] R. Holmes and G. C. Murphy. Using structural
context to recommend source code examples. In ICSE
'05, pages 117{125, 2005.
[13] S. Jha, S. Gulwani, S. A. Seshia, and A. Tiwari.
Oracle-guided component-based program synthesis. In
ICSE '10, pages 215{224, 2010.
[14] V. Kuncak, M. Mayer, R. Piskac, and P. Suter.
Complete functional synthesis. In PLDI '10, pages
316{329, 2010.
[15] T. Lau. Why pbd systems fail: Lessons learned for
usable ai. In Computer Human Interaction , 2008.
[16] T. Lau, P. Domingos, and D. S. Weld. Learning
programs from traces using version space algebra. In
K-CAP '03, pages 36{43, 2003.
[17] T. A. Lau, P. Domingos, and D. S. Weld. Version
space algebra and its application to programming by
demonstration. In ICML '00, pages 527{534, 2000.
[18] G. Leshed, E. M. Haber, T. Matthews, and T. Lau.
Coscripter: automating & sharing how-to knowledge
in the enterprise. CHI '08, pages 1719{1728, 2008.
[19] H. Lieberman, editor. Your wish is my command:
programming by example . Morgan Kaufmann
Publishers Inc., 2001.
[20] D. Mandelin, L. Xu, R. Bod k, and D. Kimelman.
Jungloid mining: helping to navigate the api jungle. In
PLDI '05, pages 48{61, 2005.
[21] C. McMillan, M. Grechanik, D. Poshyvanyk, Q. Xie,
and C. Fu. Portfolio: Finding relevant functions and
their usage. ICSE '11, pages 111{120, 2011.
[22] M. Musuvathi and S. Qadeer. Iterative context
bounding for systematic testing of multithreaded
programs. PLDI '07, pages 446{455, 2007.
[23] C. Pacheco. Directed random testing . PhD thesis, 2009.[24] D. Perelman, S. Gulwani, T. Ball, and D. Grossman.
Type-directed completion of partial expressions. PLDI
'12, pages 275{286, 2012.
[25] S. P. Reiss. Semantics-based code search. ICSE '09,
pages 243{253, 2009.
[26] A. Rudys and D. S. Wallach. Transactional rollback
for language-based systems. DSN '02, pages 439{448,
2002.
[27] N. Sahavechaphan and K. Claypool. Xsnippet: mining
for sample code. In OOPSLA '06, pages 413{430, 2006.
[28] K. Sen, D. Marinov, and G. Agha. Cute: A concolic
unit testing engine for c. ESEC/FSE-13, pages
263{272, 2005.
[29] J. Siefers, G. Tan, and G. Morrisett. Robusta: taming
the native beast of the jvm. CCS '10, pages 201{211,
2010.
[30] R. Singh and A. Solar-Lezama. Synthesizing data
structure manipulations from storyboards. ESEC/FSE
'11, pages 289{299, 2011.
[31] A. Solar-Lezama, R. Rabbah, R. Bod k, and
K. Ebcio glu. Programming by sketching for
bit-streaming programs. In PLDI '05, pages 281{294,
2005.
[32] S. Srivastava, S. Gulwani, and J. S. Foster. From
program verication to program synthesis. In POPL
'10, pages 313{326, 2010.
[33] M. Sun and G. Tan. Jvm-portable sandboxing of
java's native libraries. In S. Foresti, M. Yung, and
F. Martinelli, editors, Computer Security - ESORICS
2012, volume 7459 of Lecture Notes in Computer
Science , pages 842{858. 2012.
[34] S. L. Tanimoto. Viva: A visual language for image
processing. J. Vis. Lang. Comput. , 1(2):127{139, June
1990.
[35] S. Thummalapenta and T. Xie. Parseweb: a
programmer assistant for reusing open source code on
the web. In ASE '07, pages 204{213, 2007.
[36] S. Thummalapenta, T. Xie, N. Tillmann,
J. de Halleux, and Z. Su. Synthesizing method
sequences for high-coverage testing. OOPSLA '11,
pages 189{206, 2011.
[37] N. Tillmann and J. De Halleux. Pex: White box test
generation for .net. TAP'08, pages 134{153, 2008.
[38] E. M. Wilcox, J. W. Atwood, M. M. Burnett, J. J.
Cadiz, and C. R. Cook. Does continuous visual
feedback aid debugging in direct-manipulation
programming systems? CHI '97, pages 258{265, 1997.
[39] T. Xie, D. Marinov, W. Schulte, and D. Notkin.
Symstra: A framework for generating object-oriented
unit tests using symbolic execution. In N. Halbwachs
and L. Zuck, editors, TACAS , volume 3440 of Lecture
Notes in Computer Science , pages 365{381. 2005.
[40] J. Zhang and H. Zhang. Sem: a system for
enumerating models. IJCAI'95, pages 298{303, 1995.
[41] Q. Zhao, R. Rabbah, S. Amarasinghe, L. Rudolph,
and W.-F. Wong. How to do a million watchpoints:
ecient debugging using dynamic instrumentation.
CC'08/ETAPS'08, pages 147{162, 2008.663