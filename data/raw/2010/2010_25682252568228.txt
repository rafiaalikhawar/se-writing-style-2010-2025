How to Make Best Use of Cross-Company Data in Software
Effort Estimation?
Leandro L. Minku and Xin Y ao
CERCIA, School of Computer Science, The
University of Birmingham
Edgbaston, Birmingham, B15 2TT, UK
{L.Minku,X.Y ao}@cs.bham.ac.uk
ABSTRACT
Previous works using Cross-Company (CC) data for making
Within-Company (WC) Software Eort Estimation (SEE)
try to use CC data or models directly to provide predic-
tions in the WC context. So, these data or models are only
helpful when they match the WC context well. When they
do not, a fair amount of WC training data, which are usu-
ally expensive to acquire, are still necessary to achieve good
performance. We investigate how to make best use of CC
data, so that we can reduce the amount of WC data while
maintaining or improving performance in comparison to WC
SEE models. This is done by proposing a new framework
to learn the relationship between CC and WC projects ex-
plicitly, allowing CC models to be mapped to the WC con-
text. Such mapped models can be useful even when the
CC models themselves do not match the WC context di-
rectly. Our study shows that a new approach instantiating
this framework is able not only to use substantially less WC
data than a corresponding WC model, but also to achieve
similar/better performance. This approach can also be used
to provide insight into the behaviour of a company in com-
parison to others.
Categories and Subject Descriptors
D.2.9 [ Software Engineering ]: Management| Cost esti-
mation ; I.2.6 [ Articial Intelligence ]: Learning| Concept
learning
General Terms
Experimentation, Algorithms, Management
Keywords
Software eort estimation, cross-company learning, transfer
learning, online learning, ensembles of learning machines
Permission to make digital or hard copies of part or all of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proï¬t or commercial advantage and that copies
bear this notice and the full citation on the ï¬rst page. Copyrights for third-
party components of this work must be honored. For all other uses, contact
the Owner/Author.
Copyright is held by the owner/author(s).
ICSE â€™14, May 31 - June 7, 2014, Hyderabad, India
ACM 978-1-4503-2756-5/14/05.
http://dx.doi.org/10.1145/2568225.25682281. INTRODUCTION
Software Eort Estimation (SEE) is the process of esti-
mating the eort required to develop a software project. It
is a task of strategic importance in project management, as
software eort is the major contributing factor for software
cost. Due to the importance of this task, many automated
methods for software cost or eort estimation have been pro-
posed, including several machine learning approaches [28].
Automated methods for creating SEE models rely on a
set of training examples/data which can be used for train-
ing (building) the models. The training examples usually
must contain information on completed projects in the form
of input features and target. Input features can be, for ex-
ample the project's functional size, development type, lan-
guage type, etc. The target is the true required eort of the
project. However, collecting Within-Company (WC) train-
ing examples, i.e., information on projects completed by the
company for which we are interested in providing predic-
tions, takes time. Moreover, even though collecting certain
input features can be relatively cheap and automated by
using some search mechanism in the project database, col-
lecting the true eort can be very expensive and involve a lot
of human eort. As a result, one of the major challenges in
SEE is that WC training sets are typically small, and build-
ing SEE models based solely on such small training sets can
result in poor performance.
In order to overcome this problem, several studies have
attempted to use Cross-Company (CC) training examples
from dierent companies to augment existing WC training
sets or to avoid their need [9, 20, 17]. We will use the term
CC to refer to other companies than the one for which we
are interested in providing estimations. Lately, approaches
that use CC training examples have also been referred to
as transfer learning approaches [11]. All existing SEE CC
approaches so far try to use CC training examples or models
directly to provide predictions in the WC context. As a
result, they are only useful when they reect the WC context
well, i.e, when the eort required by another company to
develop a certain software project would be the same as the
eort required by the company that we are interested in.
When this is not the case, which can happen in practice [20],
these training examples or models perform poorly in the WC
context, and a fair amount of WC training examples would
still be necessary to achieve good performance.
No SEE approach so far has tried to map CC training
examples or models to the WC context, i.e., to learn a func-
tion that transforms the eort of CC training examples or
models to the eort that would be required in the companyPermission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proï¬t or commercial advantage and that copies bear this notice and the full citation
on the ï¬rst page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the Owner/Author.
Copyright is held by the author/owner(s).
ICSE â€™14, May 31 â€“ June 7, 2014, Hyderabad, India
ACM 978-1-4503-2756-5/14/05
http://dx.doi.org/10.1145/2568225.2568228
446
that we are interested in. If such mapping is possible, then
CC training examples or models that would not be helpful
when used directly in the WC context could become helpful
after being mapped, and less WC training examples would
be necessary for building the SEE model. With that in mind,
this paper aims at answering the following question: If we
map CC models to the WC context, can we reduce
the amount of WC training examples used in the
learning while maintaining or improving the perfor-
mance obtained by a WC model trained on more
WC examples?
In order to answer this question, we dene a CC learn-
ing scenario that considers the relationship between CC and
WC training examples explicitly. After that, we propose the
rst framework for learning this relationship and mapping
CC models to the WC context, i.e., a framework designed
to make best use of CC data for SEE. This framework is
then illustrated by a new approach called Dynamic Cross-
company Mapped Model Learning (Dycom). This approach
is evaluated on ve databases and is able to substantially re-
duce the amount of WC projects necessary for the learning
while obtaining similar or better performance than a WC
model trained on more WC examples. This demonstrates
the success of our proposed framework and the importance
of considering the relationship between CC and WC training
examples explicitly.
Dycom is designed and evaluated considering an online
learning scenario where additional WC training examples
may become available with time and the chronology of the
projects is important. The reason why it is important to
consider chronology is that the company may suer changes
with time that can aect the performance of SEE models
[14, 20], i.e., SEE operates in a dynamic as opposed to a
static environment. So, even though CC models may be
mapped to the WC context using a certain function at a
certain moment in time, another function may be necessary
to make the mapping if/when the WC context changes.
2. RELATED WORK
Several SEE works have tried to use CC training examples
or models directly in the WC context as an attempt to deal
with the problem of small WC training sets. Kitchenham
et al. [9] performed a systematic literature review with the
main objective of investigating under what circumstances
individual organizations would be able to rely on CC SEE
models. Even though some trends were observed, no conclu-
sions could be drawn in terms of under what circumstances
CC data would be useful or not. McDonell and Shepperd
[16] also performed a systematic literature review to investi-
gate what evidence is there that CC SEE models are at least
as good as WC estimation models. They point to a lack of
strong evidence to support either CC or WC models. Both
systematic reviews mention that the level of heterogeneity of
the single-company being estimated may vary and inuence
WC models' performance. This has inuenced more recent
works that use local learning approaches to deal with the
heterogeneity of WC and CC data sets [22, 17].
The works retrieved by the systematic reviews [9, 16]
found WC models to be either statistically similar or worse
than CC models. Some of the studies used CC training ex-
amples to augment their existing WC training sets. The re-
sulting WC+CC training set was then used to build a model
to make predictions in the WC context. For example, Leeyand Shepperd [13] tested this strategy using several learn-
ing machines based on the Finish database. In particular,
they ensured that the training examples were projects com-
pleted up to a certain date, and that the test examples were
projects yet to start by this date. Very few other studies
considered chronological splitting [15]. Some other studies
recalibrated stepwise models obtained from WC+CC data
by using only CC training examples [7, 8]. So, the CC model
is not independent of the WC data. For example, Kitchen-
ham and Mendes [8] performed a study on web eort estima-
tion based on the Tukutuku database using this approach.
Other studies used solely CC training examples to build the
CC predictive models [2, 29]. For example, Briand et al. [2]
performed a study using the European Space Agency soft-
ware project database using several dierent learning ma-
chines to create CC and WC models. They compared the
performances resulting from cross-validation using only the
WC data set to the performances of CC models trained only
on a CC data set but tested on the WC data set.
Most of these works assumed the SEE environment to be
static and treated CC training examples as if they were from
the same context of the WC training examples. It is reason-
able that, when the context of the CC and WC examples is
dierent, CC models would perform poorly in the WC con-
text. When the contexts are similar, then CC models may
perform similarly to WC models. More recent works tried
to eliminate projects from predictions when they were likely
to result in poor estimations. For example, Kocaguneli et
al. [12] used a ltering mechanism based on binary trees.
This approach obtained very encouraging results, as mod-
els trained solely with dierent types of projects (or projects
from dierent centres of a company) from the ones we would
like to predict obtained overall similar performance to mod-
els trained on projects of the same type (or from the same
centre). However, this approach assumes a static SEE en-
vironment and does not consider that dierent companies
may behave considerably dierently from each other. In ad-
dition, even though it has been evaluated in the context of
dierent centres of a company, it is unclear how well it would
behave in a strict CC vs WC scenario.
An insightful work [17] in the context of SEE and Soft-
ware Defect Prediction (SDP) clustered WC+CC examples,
and then created prediction rules for each cluster. Given a
certain cluster, its neighbouring cluster with the lowest re-
quired eorts/defects was referred to as the envied cluster.
When making predictions for WC projects from a cluster,
rules created using only the CC examples from the envied
cluster were better than rules created using only the WC
examples from the envied cluster. So, the authors recom-
mend to cluster WC+CC examples, but to learn rules using
solely the CC examples from the envied cluster. However,
even though the paper is written for both SEE and SDP,
this particular conclusion was drawn based solely on SDP
data. The WC models are also likely to have been trained
on very small subsets of the WC data, possibly hindering
their performance. This approach uses the CC examples di-
rectly in the WC context and does not consider the dynamic
aspects of the SEE task.
Another work [20] compares the performance of local mod-
els (regression trees) trained only on WC and only on CC
data throughout time. It conrms that dierent companies
can have dierent contexts and that a certain company may
suer changes over time. These changes can cause it to447behave more/less similarly to other companies, making CC
data benecial or detrimental depending on the moment in
time. An approach called Dynamic Cross-company Learning
(DCL) was designed to identify when CC data is benecial
and then use it to improve performance in comparison to
WC models. DCL was the rst SEE approach able to use
CC models to achieve better performance in comparison to
WC models, showing that it is helpful to check how relevant
CC models are to the current WC context before using them
for WC predictions. However, as the CC models are only
useful when they match the current WC context reasonably
well, DCL still requires a fair amount of WC examples to
achieve good performance during the periods when the CC
models are not useful.
As all existing CC works attempt to use CC training ex-
amples or models directly in the WC context, there is cur-
rently no solution for the problem of small WC training sets
when the CC data do not match the WC context well. As
dierent companies are likely to behave dierently, such sit-
uation is very likely to happen in practice. Our work is the
rst one to consider mapping CC models to the WC context
so that the mapped models can be useful even when the CC
models themselves do not match the WC context. A suc-
cessful mapping would reduce the amount of WC training
examples required for achieving good performance.
3. CC SEE LEARNING SCENARIO
Dierent from previous work, we consider for the rst time
that there is a relationship between the SEE context of a
certain company and other companies. We formalise the
relationship between two companies CAandCBas follows:
fA(x) =gBA(fB(x)) (1)
whereCAis the company in which we are interested, CBis
another company (or a section of this other company), fAis
the true function that provides the required eort to CA,fB
is the true function that provides the required eort to CB,
gBAis a function that maps the eort from the context of CB
to the context of CA, and x= [x1;x2;;xn] are the input
features of a software project. As an illustrative example,
consider the software projects in table 1. In this case, the
true eort of a project in CAis 1:2 times the eort that would
be required in CB, i.e.,fA(x) =gBA(fB(x)) = 1:2fB(x).
Even though the relationship between the eort in CAand
CBis linear in this example, our CC learning scenario does
not restrict gBAto linear functions. Note also that fAand
fBcan be functions of any type. For instance, fA(orfB)
could be composed of sub-functions representing dierent
clusters of the company's data in order to represent the level
of heterogeneity within a company [22, 17]. In practice, it
is also likely that there will be some noise in the eorts.
Given equation 1, the learning task of creating an SEE
model to a certain company CAcan involve the task of learn-
ing the relationship between CAand other companies, i.e.,
learning how to map CC training examples or models from
the context of other companies to the context of CA.
4. CC MODEL MAPPING FRAMEWORK
Based on equation 1, we propose a framework for learn-
ing SEE models for a company CAbased on mapping CC
models toCA's context. It can be used both when WC and
CC training examples arrive continuously (online) and when
they comprise pre-existing sets of xed size (oine).4.1 Mapping One CC SEE Model
This section explains the framework to learn a function
^gBAto map eort estimations given by an SEE model ^fB
from company CBto the context of a company CA. The
learning process is illustrated in gure 1.
WC training data: Assume that training examples from
CAare made available. The number of WC examples can
be very small, and a WC model created based on it may not
be accurate. So, we would like to improve SEE for CAby
using CC training data.
CC training data: Assume that training examples from
CB(or from a section of CB) are made available. The input
features and target of these examples must be compatible
with the WC training examples.
CC SEE model: Training examples from CBare used
for building an SEE model ^fBfor the context of CB. IfCB
does not wish to provide its raw training training examples
toCA, it could provide ^fBdirectly toCA.
Mapping function: Given the WC training examples
fromCAand the CC SEE model ^fB, the learning task is
then to learn a mapping function ^gBAto map SEEs pro-
vided by ^fBto the context of CA. In order to do so, an
appropriate training set needs to be derived from the WC
training examples and ^fB. This is done as follows. For each
training example ( x;y) in the WC training set, use ^fBto
provide an eort estimation ^fB(x). Then, create an exam-
ple ( ^fB(x);y) for training ^ gBA. It is hoped that the task
of learning ^ gBAis not more dicult than the task of learn-
ing a whole WC model ^fAbased solely on the WC training
examples, as this would reduce the amount of WC training
examples required for the learning.
Mapped SEE model: Once the mapping function is
learnt, then an SEE for a project from CAdescribed by the
input features x= [x1;x2;;xn] would be provided by the
following function, named as mapped model :
^fA(x) = ^gBA(^fB(x)): (2)
4.2 Using More than One CC SEE Model
If the SEE model ^fBand the mapping function ^ gBAlearnt
using the framework explained in section 4.1 were perfect,
then the mapped model would be a perfect SEE model to
CA. However, it is very unlikely that an SEE model would
be a perfect, especially considering the limited amount of
training examples and the diculty of the task. In order to
improve the SEEs given to CA, CC training examples from
more than one other company (or sections of a company) can
(and should) be used. The framework explained in section
4.1 can be used to learn the mapping function ^ gBiAfor the
CC model corresponding to each company (or section of a
company)CBi(1iM). Each of the Mmapped models
would provide eort estimations to CAas shown in equation
2. So, for each WC project to be estimated, Mdierent es-
timations can be provided. An approach implementing this
framework would then need to decide which of these estima-
tions should be used, and/or how these estimations should
be combined into a single and more trustful estimation to be
given toCA. Combining estimations of dierent models has
been showing to improve SEE considerably [22, 20]. In ad-
dition to the mapped models, one may also wish to combine
a WC model ^fWAtrained onCA's training examples.448Table 1: Example of relationship between company CAand company CB. In this case, the true eort in
person-hours for a given project in CAis1:2times its true eort in person-hours in CB.
ID Functional Size Development Type Language Type CB's True Eort CA's True Eort
0 100 Enhancement 3GL 500 600
1 300 Re-development 4GL 1300 1560
2 400 New Development 4GL 2000 2400
3 500 New Development 3GL 3000 3600
CB
Training DataTraining
ExamplesLearning ^fB
Function Learnt
^fB
Eort
EstimationsEstimation
Enquiries
CA
Training DataTraining
ExamplesCreate
Mapping
Training
ExamplesTraining
ExamplesMapping
Training DataTraining
ExamplesLearning ^gBAFunction Learnt^gBA
Figure 1: Framework for learning function ^gBAto map estimations given by an SEE model ^fBfrom company
CBinto the context of a company CA.
4.3 Using a Mixed CC Training Set
A certain company CAmay have access to a set of CC
training examples from several dierent companies without
information on which examples belong to which individual
companies among the ones that provided the examples. This
would be the case of a company CAusing data from the
International Software Benchmarking Standards Group (IS-
BSG) [6], for example. Nevertheless, it would still be pos-
sible to use our framework. For instance, the CC training
examples could be split into dierent sets according to their
corresponding productivities, as done in section 7, or accord-
ing to some clustering technique based on their input fea-
tures and targets. In this way, similar CC examples would be
grouped together into dierent CBi, (1iM) sets, where
eachCBiis a \virtual" (imaginary) company that develops
projects with the characteristics of the projects grouped to-
gether. Each virtual company would represent a mix of the
behaviours of the dierent companies that provided the CC
projects. If appropriate functions ^fCBiare learnt, the con-
text of these virtual companies CBican be mapped to the
context ofCA, turning them into useful information to CA.
5. ONLINE LEARNING SCENARIOS
As explained in section 4, the CC model mapping frame-
work can be used both for online and oine learning. In this
paper, we propose an instantiation of the framework where
new WC projects arrive with time and CC training examples
comprise pre-dened sets, similarly to [20]. Such situation
could occur, for example, when a company acquires sets of
CC training examples without a contract to receive updates
of these sets. Another situation would be when a company
adopts a CC training set available in the web, whose size is
not increasing with time.
As companies are evolving entities, changes can happen
that would aect the eort required for them to develop
a software project [20, 14]. Examples of changes are train-ing being provided to employees, new employees being hired,
new programming languages being introduced, the company
starting to take new types of projects, etc. So, it is impor-
tant to consider the chronology (e.g., the date of completion)
of the projects of the company for which we are interested in
providing estimations. Additionally, if this company suers
changes that aect the eort that it would require to de-
velop a software project, the model used for providing such
estimations should be able to adapt to such changes.
We formulate SEE as an online learning problem in which
a new WC project is completed at each time step (point in
time). Whenever a new WC project is completed, we wish
to provide up-to-date estimations of the eort for the next
ten WC projects to be (or being) developed, based on the
latest available SEE model. The number of WC projects
for which up-to-date estimations should be provided at each
time step depends on the needs of the company being esti-
mated. Investigation of values dierent from ten is left as
future work. As an important aim of the current work is to
analyse whether it is possible to reduce the amount of WC
training examples required for building an SEE model, we
consider two dierent scenarios:
1.Each completed WC project contains both information
on its input features and on its true required eort.
2.The WC projects that arrive at every p(p >1) time
steps contain both information on their input features and
true required eort. All remaining WC projects contain only
the information on the input features, whereas the true re-
quired eort is missing. So, even though an eort estimation
is required for all WC projects, only a few of them are used
as training examples once they are completed.
6. DYCOM
In this section, we present Dynamic Cross-company Map-
ped Model Learning (Dycom), an instantiation of the frame-
work presented in section 4. Dycom can be used with CC449training examples from several dierent companies, as de-
scribed in sections 4.2 and 4.3, and operates under the online
learning scenario 2 explained in section 5.
WC training data: Similarly to section 4, we will re-
fer to the company for which we are interested in providing
predictions as CA. WC training examples from CAare in-
coming as described in section 5.
WC SEE model: Our framework allows a WC model
to be trained if desired. In Dycom, whenever a new WC
training example arrives, it is used to train a model ^fWA.
CC training data: The CC training examples are avail-
able beforehand as explained in section 5. They are split into
sections based on some clustering algorithm, or on their pro-
ductivity, or on the size of the projects. Each section CBi
is considered as a separate CC training set. For example,
if there are Ncompanies and the training examples from
each company are split into Ssections, then there will be
M=NSdierent CC training sets. If it is not known
from what company each CC training example comes, then
the whole CC training set is split into dierent sections, as
ifN= 1. The reason for the splitting will be explained in
the paragraph on mapping functions.
CC SEE models: Each of the MCC training sets is
used to create a dierent CC model ^fBi(1iM).
Mapping functions: Whenever a new WC training ex-
ample arrives, each model ^fBiis asked to perform an SEE,
each SEE is then used to create a mapping training example
(^fBi(x);y), and the corresponding ^ gBiAis trained with it,
as described in section 4.1. Dycom assumes that the rela-
tionship formalised in equation 1 can be modelled reason-
ably well by linear functions of the format ^ gBiA(^fBi(x)) =
^fBi(x)biwhen dierent sections containing relatively more
similar CC training examples are considered separately. This
is the reason to split CC training examples into dierent sec-
tions as explained previously. Learning this function means
learning the factor bi, which is done using equation 3:
bi=8
>>>>>>>>><
>>>>>>>>>:1;if no mapping training example
has been received yet;
y
^fBi(x);if (^fBi(x);y) is the rst
mapping training example;
lry
^fBi(x)+ (1 lr)bi;otherwise.(3)
where ( ^fBi(x);y) is the mapping training example being
learnt,lr(0< lr < 1) is a pre-dened smoothing factor
and the factor biin the right side of the equation represents
the latest value of bibefore receiving the current mapping
training example.
The mapping function performs a direct mapping bi= 1
while no mapping training example is received. When the
rst mapping training example is received, biis set to the
valuey=^fBi(x). This gives a perfect mapping for the exam-
ple being learnt, as ^fBi(x)bi=^fBi(x)y=^fBi(x) =y. For
all other mapping training examples received, exponential
smoothing with smoothing factor lris used to set bi. This is
the simple weighted average of the value that would provide
a perfect mapping for the current mapping example and the
previous value of bi, which was calculated based on the pre-
vious mapping examples. Higher smoothing factor lrwill
cause more emphasis on the most recent mapping trainingexamples and higher adaptability to changing environments,
whereas lower lrwill lead to a more stable mapping func-
tion. So, the smoothing function allows learning mapping
functions that provide good mappings based on previous
mapping examples, while allowing adaptability to changes
that may aect a company's required software eorts.
Mapped SEE model: As explained in section 4.2, each
mapped model ^ gBiA(^fBi) and the WC model ^fWAcan pro-
vide an SEE in the WC context when required. The SEE
given by Dycom is the weighted average of these M+ 1 es-
timations:
^fA(x) ="MX
i=1wBi^gBiA(^fBi(x))#
+wWA^fWA(x);(4)
where the weights wBiandwWArepresent how much we
trust each of the models, are positive and sum to one. So,
Dycom uses an ensemble of mapped and WC SEE models.
Weights: The weights are initialised so that they have
the same value for all models being used in the ensemble
and are updated in a similar way to the weights used in [20].
Whenever a new WC training example is made available, the
model which provided the lowest absolute error is considered
to be the winner and the others are the losers . The losers
have their weights multiplied by a pre-dened parameter 
(0< 1), and then all weights are normalised so that
they sum to one.
Algorithm 1 presents Dycom's learning process. Dycom
rst learns the CC models (line 3). The weight associated
to each CC model is initialised to 1 =M, so that each model
has equal weight and all weights sum to one (line 4). The
mapping functions are initialised to use bi= 1 (line 5). Be-
fore any WC training example is made available, the weight
wWAcorresponding to the WC model is initialised to zero
(line 7), because this model has not received any training
yet. In this way, the CC models can be used to make pre-
dictions while there is no WC training example available.
After that, for each new WC training example, the weights
are updated (lines 9{19). If the WC training example is the
rst one, the weight of the WC model needs to be set (line
17). Then, the mapping training examples are created and
used to update the corresponding mapping functions (lines
22 and 23). Finally, the WC model is updated with the WC
training example (line 26).
7. DATABASES
This section presents the databases used in the evaluation
of Dycom. Five dierent databases were used: KitchenMax,
CocNasaCoc81, ISBSG2000, ISBSG2001 and ISBSG. These
include both data sets derived from the PRedictOr Models
In Software Engineering Software (PROMISE) Repository
[18] and the International Software Benchmarking Standards
Group (ISBSG) Repository [6]. Each database contains a
WC data set and three CC data sets derived based on the
projects' productivity.
7.1 KitchenMax
The database KitchenMax is composed of Kitchenham
and Maxwell, which are two SEE data sets available from
the PROMISE Repository. Kitchenham's detailed descrip-
tion can be found in [10]. It comprises 145 maintenance and
development projects undertaken between 1994 and 1998 by
a single software development company. Maxwell's detailed450Algorithm 1 Dycom
Parameters:
DBi(1iM): CC training sets.
: factor for decreasing model weights
1:fLearn CC base models: g
2:foreach CC training set DBido
3: Create CC model ^bBiusingDBi
4:wBi=1
MfInitialise weight.g
5:bi= 1fInitialise mapping function. g
6:end for
7:wWA= 0fInitialise WC model weight. g
8:foreach new WC training example ( x;y)do
9:fUpdate weights:g
10: foreach model ^fBiand ^fWAdo
11: Determine the model's estimation to x.
12: Calculate the absolute error AEBi(orAEWA).
13: end for
14: Determine loser models based on their AE.
15: Multiply loser models' weights by .
16: if(x;y) is the rst WC training example then
17:wWA=1
M+1
18: end if
19: Divide each weight by the sum of all weights.
20:fUpdate mapping functions: g
21: foreach model ^fBido
22: Create mapping example ( ^fBi(x);y).
23: Use ( ^fBi(x);y) to update ^ gBiAbased on Eq. 3.
24: end for
25:fUpdate WC model: g
26: Update WC model ^fWAusing ( x;y).
27:end for
description can be found in [25]. It contains 62 projects from
one of the biggest commercial banks in Finland, covering
the years 1985 to 1993 and both in-house and outsourced
development. In order to make these data sets compati-
ble, a single input feature (functional size) was used. Still,
Maxwell uses functional size, whereas Kitchenham uses ad-
justed functional size. An appropriate mapping function
should be able to overcome this problem. There were no
functional size feature values missing. The target is the ef-
fort in person-hours.
Kitchenham was considered as the WC data, and was
sorted according to the actual start date plus the duration.
This sorting corresponds to the exact completion order of
the projects. Maxwell was considered as the CC data and
was split into three CC sets for use with Dycom according
to their productivity in terms of eort divided by functional
size. The ranges used for the dierent CC sets are shown in
table 2 and were chosen to provide similar size partitions.
This process could be easily automated in practice. WC
projects were not split.
7.2 CocNasaCoc81
The database CocNasaCoc81 is composed of Cocomo Nasa
and Cocomo 81, which are two SEE data sets available from
the PROMISE Repository. Cocomo Nasa contains 60 Nasa
projects from 1980s-1990s and Cocomo 81 consists of the 63
projects analysed by Boehm to develop the software cost es-
timation model COCOMO [1] rst published in 1981. Both
data sets contain 16 input features (15 cost drivers [1] andTable 2: Productivity ranges for CC data sets.
CC Data Productivity Band # Examples
High: (3.85,7.40] 17
Maxwell Medium: (7.40,15.30] 24
Low: (15.30,38.00] 21
High: [0.7,2.85] 19
Cocomo 81 Medium: (2.85,6.60] 20
Low: (6.60,49.00] 24
High: [0.00,5.00] 56
ISBSG2000 Medium: (5.00,13.00] 57
Low: (13.0,155.70] 55
High: [0.00,6.00] 72
ISBSG2001 Medium: (6.00,14.00] 79
Low: (14.00,155.70] 73
High: [0.00,10.00] 291
ISBSG Medium: (10.00,20.00] 250
Low: (20.00,424.90] 285
number of lines of code) and one target (software eort in
person-months). Cocomo 81 contains an additional input
feature (development type) not present in Cocomo Nasa,
which was thus removed. These data sets contain no miss-
ing values.
Cocomo Nasa's projects were considered as the WC data
and Cocomo 81's projects were considered as the CC data.
Cocomo Nasa provides no information on whether the pro-
jects are sorted in chronological order. The original order
of the Cocomo Nasa projects was preserved in order to sim-
ulate the WC projects' chronology. Even though this may
not be the true chronological order, it is still useful to evalu-
ate whether approaches are able to make use of mapped CC
models when/if they are benecial. The three CC data sets
for Dycom were created by separating Cocomo 81's projects
based on the productivity in terms of eort divided by the
number of lines of code using the ranges in table 2. This
database has also been used in [20], where it was shown that
Cocomo 81's projects can sometimes be useful and some-
times detrimental in predicting Cocomo Nasa's projects.
7.3 ISBSG Databases
Three SEE databases were derived from ISBSG Release
10, which contains software project information from several
companies. Information on which projects belong to a single
company for composing a WC data set have been provided
to us upon request. The databases are:
ISBSG2000 { 119 WC projects implemented after the
year 2000 and 168 CC projects implemented up to the
end of year 2000.
ISBSG2001 { 69 WC projects implemented after the
year 2001 and 224 CC projects implemented up to the
end of year 2001.
ISBSG { no date restriction to the 187 WC and 826 CC
projects, meaning that CC projects with implementa-
tion date more recent than WC projects are allowed.
This data set can be used to simulate the case in which
it is known that other companies can be more evolved
than the single company analysed.
These databases have been previously used in [20]. In-
formation on how they were preprocessed can be found in
that paper and is not included here due to space limitations.
Four input features (development type, language type, de-
velopment platform and functional size) and one target (soft-
ware eort in person-hours) were used. The WC projects451were sorted based on the implementation date to compose
a stream of incoming projects. Dycom further uses a sepa-
ration of CC projects into dierent training sets. This was
done by splitting projects according to their normalised level
1 productivity rate in hours per functional size unit, pro-
vided by the repository. The ranges used for creating the
CC sets are shown in table 2. It was shown in [20] that
regression trees (local models [22]) trained on each CC set
were better, similar or worse than regression trees trained
on the WC projects depending on the moment in time.
8. EVALUATION OF DYCOM
This section presents the experiments performed with the
aim of determining whether mapping CC models to the WC
context allows us to reduce the amount of WC training ex-
amples used in the learning while maintaining or improving
performance in comparison to a WC model trained with
more WC examples. These experiments also work as an
evaluation of Dycom, as is used as the mapping learning
approach.
8.1 Experimental Setup
Dycom can be used with any base learner. In this work,
Regression Trees (RTs) were used as the base learners in
the experiments. RTs were chosen because they are local
approaches in which estimations are based on the projects
that are most similar to the project being predicted. This
can help dealing with the heterogeneity within each data
set [22]. RTs have been shown to achieve good performance
for SEE in comparison to several other approaches [22]. We
used the RT implementation REPTree provided by WEKA
[5], where splits are created so as to minimise the variance
of the targets of the training examples in the nodes. Two
dierent approaches were compared: RT and Dycom-RT.
RT: RTs were created to reect WC online learning.
Whenever a new WC training example was provided, the
current RT was discarded and a new RT was trained on all
projects so far (including the one received at the current time
step). This RT was then used to predict the next ten WC
projects. This approach considers that all WC completed
projects contained known true eort, i.e., every time step
received a WC training example (online scenario 1).
Dycom-RT: Dycom was used with RTs as the CC and
WC models under the online scenario 2 with p= 10, i.e.,
a new WC training example was provided only at every 10
time steps. So, Dycom-RT uses only 10% of the WC training
examples used by the RT explained in the paragraph above.
The WC RT used by Dycom was rebuilt from scratch us-
ing all WC training examples so far whenever a new WC
example was made available.
Both Dycom and the WC approach used the same base
learner in the experiments, ensuring that the comparison
is fair. The study of Dycom with other base learning ap-
proaches is left as future work.
At each time step, the performance on the next ten ex-
amples was evaluated based on several dierent measures:
Mean Absolute Error (MAE), Standardised Accuracy (SA),
Root Mean Squared Error (RMSE), correlation coecient
(Corr), Logarithmic Standard Deviation (LSD), Mean Mag-
nitude of the Relative Error (MMRE) and percentage of
predictions within 25% of the actual value (PRED(25)).
The equations to calculate these measures are the follow-
ing, whereTis the number of examples used for evaluatingthe performance, yiis the actual eort for the example i,
and ^yiis the estimated eort for example i:
MAE =1
TPT
i=1j^yi yij;
SA=
1 MAE
MAE rguess
100;
whereMAE is the MAE of the approach being eval-
uated and MAE rguess is the MAE of 1000 runs of
random guess. Random guess is dened here as uni-
formly randomly sampling the true eort over all the
WC projects received up to the current time step. It
is calculated based on the online scenario 1;
RMSE =qPT
i=1(^yi yi)2
T;
Corr =PT
i=1(^yi ^y)(yi y)pPT
i=1(^yi ^y)2pPT
i=1(yi y)2;
where ^yand yare the average predicted and average
actual eorts, respectively;
LSD =r
PT
i=1
ei+s2
22
T 1;wheres2is an estimator of
the variance of the residual eiandei= lnyi ln ^yi;
MMRE =1
TPT
i=1MRE i;whereMRE i=j^yi yij=yi;
PRED (25) =1
TPT
i=1(
1;ifMRE i25
100
0;otherwise.:
These measures have been chosen because they emphasise
a variety of dierent behaviours, evaluating SEE from dif-
ferent angles. MAE has been recommended by [26] for being
unbiased towards under or overestimations. SA is an unbi-
ased measure that allows for interpretability { it is viewed
as the ratio of how much better an approach is than ran-
dom guess [26]. So, it can also be used to give a better idea
of the magnitude of the dierences in performance. RMSE
is a popular measure in the machine learning community
which emphasizes large errors more. Corr is widely used in
sciences as a measure of the strength of linear dependence
between two variables. In the case of SEE, the two variables
are the estimated and the actual eort [3]. LSD uses the
residual in the log-scale, which is independent of size (i.e.,
homoscedastic) [4]. MMRE and PRED(25) are measures
biased towards underestimations and can behave quite dif-
ferently from other measures [4, 23, 21]. They are reported
here to provide insight into the behaviour of the approaches
when analysed together with MAE. In addition to these mea-
sures, the standard deviation (StdDev) of MAE across time
steps has also been used. This is because it is desirable to
have SEE models that are not only accurate, but also whose
errors do not vary much for dierent projects. A more stable
SEE model is more reliable, because it gives a better idea
of the error that is likely to happen when estimating a new
project.
The databases used in the experiments are the ones ex-
plained in section 7. Dycom's parameter was set to the
default value of 0.5, which has been used previously in the
literature for similar weight update mechanisms [20]. Dy-
com's parameter lrwas set to 0.1 after some preliminary
investigation with 0.1 and 0.05. The parameters used with
each RT were the ones more likely to obtain good results
in previous work [22]: minimum total weight of 1 for the
instances in a leaf, and minimum proportion of the variance
on all the data that need to be present at a node in order
for splitting to be performed 0.0001. A single execution was
performed for each data set, as we used deterministic RTs.452Table 3: Overall Average Performance Across Time Steps.
Database Approach MAE StdDev SA RMSE Corr LSD MMRE PRED(25)
RT 2441.0241 2838.2375 30.1782 4850.3387 0.4350 1.2221 102.2562 22.5185
KitchenMax Dycom-RT 2208.6522 2665.4276 36.8249 4287.4476 0.6416 0.8809 114.0218 27.5556
P-value 3.82E-11 6.35E-01 { 1.46E-12 1.62E-16 4.25E-21 3.80E-04 4.12E-05
RT 319.4572 250.2325 33.1366 477.2357 0.6427 0.8623 188.8098 24.4000
CocNasaCoc81 Dycom-RT 161.7917 105.7591 66.1365 243.6504 0.8885 0.6671 91.2384 28.6000
P-value 4.04E-06 1.40E-11 { 5.95E-08 4.12E-07 8.82E-04 2.37E-01 2.78E-02
RT 2753.3726 1257.4586 37.0471 4133.1006 0.3554 1.4592 162.7492 23.7615
ISBSG2000 Dycom-RT 2494.6639 1249.8400 42.9622 3741.8009 0.4515 1.1589 135.5271 19.0826
P-value 4.72E-02 1.01E-01 { 1.83E-01 8.73E-02 1.27E-06 1.90E-01 9.60E-03
RT 3621.9598 1367.9603 11.9270 5149.6267 0.1658 1.8110 395.1080 23.7288
ISBSG2001 Dycom-RT 2543.9495 1165.8591 38.1403 3581.6573 0.5691 1.2447 228.0416 21.0169
P-value 3.21E-06 4.16E-01 { 7.88E-06 2.29E-10 6.24E-08 1.76E-07 2.26E-01
RT 3253.9349 2476.0512 46.2891 4872.9193 0.4412 1.3475 160.7898 21.8966
ISBSG Dycom-RT 3122.6603 2227.9812 48.4560 4473.6527 0.5817 1.0378 162.1517 19.0805
P-value 5.56E-02 3.54E-01 { 4.18E-02 1.90E-09 2.99E-12 2.99E-05 4.59E-02
Cells in lime (light grey) represent better values. P-values of Wilcoxon sign rank tests to compare Dycom-RT against RT for each
database are also shown. For StdDev, the p-values correspond to Levene tests for equality of variances. Cells in orange (dark grey)
indicate statistically signicant dierence when using Holm-Bonferroni corrections at the overall level of signicance of 0.05 considering
the ve databases. P-values for SA are not shown because this measure is an interpretable equivalent of MAE.
8.2 Results
8.2.1 Overall Average Performance
We rst analyse the overall average performances across
time steps (table 3). In order to compare the overall perfor-
mances between RT and Dycom-RT, we performed a Wilco-
xon sign rank test for each database. For StdDev, we per-
formed Levene test for equality of variances. For each perfor-
mance measure, we used Holm-Bonferroni corrections con-
sidering ve comparisons at the overall level of signicance
of 0.05. No tests have been done for SA, because it is an
interpretable equivalent to MAE.
In terms of all performance measures but MMRE and
PRED(25), Dycom-RT obtained always similar or better
overall performance than RT. It is worth noting that even if
Dycom-RT had obtained similar (and never better) overall
performance than RT, this would still have represented a
strong advantage of Dycom-RT. The reason is that Dycom-
RT required much less WC training examples, saving the
cost of collecting the required eort for WC projects. Our
experiments show that Dycom-RT not only managed to use
ten times less WC training examples, but also was able to
provide similar or better overall performance for all data-
bases. In terms of MAE (and SA), RMSE and Correla-
tion, sometimes the dierences were signicant and some-
times not. The measure SA allows us to have a better idea
of the magnitude of the dierences in performance, as it
is interpreted as how much better an approach does than
random guess. For CocNasaCoc81 and ISBSG2001, the dif-
ference in SA was considerably large, being likely to have
large impact in practice. In terms of LSD, there was always
statistically signicant dierence and Dycom-RT was better
for all databases. In terms of StdDev, statistically signicant
dierence was found only for CocNasaCoc81. So, Dycom-
RT and RT behaved mostly similarly in terms of stability of
MAE across time steps.
In terms of MMRE and PRED(25), Dycom-RT was some-
times better, similar and worse than RT. As explained in
section 8.1, these measures are biased towards underestima-
tions. So, combined with the results in terms of MAE, these
results do not imply that Dycom-RT obtained behaviour
sometimes better, similar or worse than RTs, but that Dy-coms sometimes managed to obtain more improvements in
terms of reducing overestimations, sometimes balanced im-
provements, and sometimes more improvements in terms of
reducing underestimations. Sometimes these improvements
pushed the relative errors under the boundary of 25% of the
actual eort and sometimes not.
8.2.2 Performance Throughout Time
In addition to the overall performance across time steps,
when working with online learning, it is also important to
verify the performance at each time step. This allows check-
ing whether a certain approach is better at some time steps,
but worse at others. We make this analysis based on MAE,
which is an unbiased measure recommended in [26]. Figure
2 shows the MAE over time for all databases.
For the databases where Dycom-RT obtained statistically
better overall MAE across time steps than RT (KitchenMax,
CocNasaCoc81 and ISBSG2001), there were very few time
steps when Dycom-RT performed worse than RT. However,
for the databases where Dycom-RT and RT obtained no sta-
tistically signicant dierence in terms of overall MAE (IS-
BSG2000 and ISBSG), Dycom-RT performed considerably
better during some prolonged periods of time and consid-
erably worse during some others. The reason for the worse
performance during these periods of time is likely to be that
the single company suered some change that was not cap-
tured for a while due to the lack of WC training examples.
The best interval pfor the collection of true required eort
of WC projects is likely to depend on how often the com-
pany suers considerable changes. Companies that present
changes more often would need smaller intervals. The in-
tervalpmay also aect the ability of Dycom to deal with
outlier training examples. A very interesting area of future
research is the investigation of manual or algorithmic ap-
proaches to decide when projects should have their required
eorts collected.
9. INSIGHT PROVIDED BY DYCOM
The mapping functions learnt by Dycom explain the rela-
tionship of the eort required by a certain company CAin
comparison to sections CBiof other companies throughout45320 40 60 80 100 120050001000015000
Time StepMAE
  
Dycomâˆ’RT
RT(a) KitchenMax
10 20 30 40 50050010001500
Time StepMAE
  
Dycomâˆ’RT
RT (b) CocNasaCoc81
20 40 60 80 100050001000015000
Time StepMAE
  
Dycomâˆ’RT
RT
(c) ISBSG2000
10 20 30 40 50050001000015000
Time StepMAE
  
Dycomâˆ’RT
RT (d) ISBSG2001
50 100 150050001000015000
Time StepMAE
  
Dycomâˆ’RT
RT
(e) ISBSG
Figure 2: MAE Throughout Time.
time. So, it provides useful insight into the behaviour of
a given company in comparison to other companies. This
relationship can be visualised via plots of the factor bito
show the need for strategic decision making towards the im-
provement of productivity, as well as to monitor the suc-
cess of strategies adopted for such improvement. The map-
ping function considers the eort that dierent sections from
other companies would require for each given project from
CAconsidering their input features. So, plots the factor bi
consider more information than general plots of the eort
or productivity of a company throughout time, which would
provide only trends without considering the individual fea-
tures of the projects being developed.
Figure 3 presents the factor bilearnt for each mapping
function over time. In our experiments, the CC projects
were split into three data sets according to their productiv-
ity. So, the CC sections CBi(1i3) refer to a CC
data set with high productivity, medium productivity and
low productivity. For all databases, CAneeds more eort
than the high productivity CC sections ( bi>1) and less
eort than the low productivity CC sections ( bi<1). How-
ever, each single company CAbeing analysed has a dierent
behaviour with respect to their corresponding CC models,
especially with respect to the high productivity ones.
For instance, KitchenMax's CAinitially behaves quite sim-
ilarly to the medium productivity CC model, and then its
behaviour gradually changes to become more and more sim-
ilar to the high productivity CC model. Initially, a project
inCAwould require around twice the eort that this same
project would require by the high productivity CC model.
In the end of the period observed, this gure improves to
20 40 60 80 100 120012345
Time StepFactor  bi
  
CB1: Low CC Productivity
CB2: Medium CC Productivity
CB3: High CC Productivity(a) KitchenMax
10 20 30 40 50012345
Time StepFactor  bi
  
CB1: Low CC Productivity
CB2: Medium CC Productivity
CB3: High CC Productivity (b) CocNasaCoc81
20 40 60 80 100012345
Time StepFactor  bi
  
CB3: Low CC Productivity
CB2: Medium CC Productivity
CB1: High CC Productivity
(c) ISBSG2000
10 20 30 40 50012345
Time StepFactor  bi
  
CB3: Low CC Productivity
CB2: Medium CC Productivity
CB1: High CC Productivity (d) ISBSG2001
50 100 150012345
Time StepFactor  bi
  
CB3: Low CC Productivity
CB2: Medium CC Productivity
CB1: High CC Productivity
(e) ISBSG
Figure 3: Factor biAssociated to Each CC Data Set
Throughout Time.
1.2 times the eort. This demonstrates the success of the
companyCAin improving its behaviour.
Another example is CocNasaCoc81, whose relationship
with respect to all CC models remains stable. CArequires
around 1.25 times the eort that would be required by the
medium productivity CC model. So, CocNasaCoc81's CA
may wish to adopt some strategy to become more produc-
tive. In order to decide on a strategy, one would have to ask
whyCArequires more eort than the medium productivity
CC model, or why CArequires more eort than the high
productivity CC model. In order to answer these questions,
ifCAhas access to the training examples from these CC
sections, it could analyse them to nd out possible reasons
forCA's lower productivity. For example, CAcould analyse
the medium productivity CC projects that are most simi-
lar (e.g., nearest neighbours) to each given project that CA
wishes to improve. Or it could analyse all training exam-
ples from this CC section if CAwishes to improve its overall
behaviour. If the features of the CC projects from a given
section are completely dierent from the WC projects, then
it may be more dicult to interpret these dierences and
decide on a strategy to improve CA. So, it may be a better
strategy to analyse the examples from the CC set with the
most similar productivity rst, as these are more likely to
be more similar and incur less dramatic changes for a start.
For example, table 4 shows that both the medium pro-
ductivity CC section and CAfrequently used sta with high
language experience. However, the medium productivity CC
section used sta highly experienced in virtual machine more454Table 4: Number of projects with each feature value
for the 20 CC projects from the medium productiv-
ity CC section and the rst 20 WC projects.
Feature / Lexp Vexp
Value CC WC CC WC
Very low 1 0 1 0
Low 1 0 4 4
Nominal 8 8 8 16
High 10 12 7 0
Very high 0 0 0 0
Extremely high 0 0 0 0
`Lexp' refers to the feature language experience and `Vexp'
refers to the feature virtual machine experience.
often thanCA. It may happen that the dierent companies
did not judge whether sta had nominal or high experience
in exactly the same way. However, the fact that the medium
productivity CC section contains more projects with high
virtual machine experience indicates that this feature, rather
than language experience, is more likely to be one of the
reasons for CA's lower productivity. After such analysis,
CAcould then adopt a certain strategy based on the dier-
ences between its project features and the CC project fea-
tures to improve its productivity. For instance, it could hire
sta with more virtual machine experience. The plots of bi
could then be used to analyse how successful the strategies
adopted are being with time.
One might nd intriguing that the same CAprovided by
the ISBSG Repository behave similarly for ISBSG2000 and
ISBSG2001, but dierently for ISBSG. However, this be-
haviour is understandable, because the CC training exam-
ples used for ISBSG were not restricted to past projects.
They can represent, for example, companies that are already
used to certain technologies that are new to CA. When such
new technologies start being used, CAstarts requiring more
eort than these companies.
10. THREATS TO VALIDITY
Internal validity regards to establishing that a certain ob-
servable event was responsible for a change in behaviour.
It is related to the question \Is there something other than
the treatment that could cause the dierence in behaviour?"
[24]. When using machine learning approaches, it is impor-
tant that the approaches being compared use fair param-
eter choices in comparison to each other [19, 27]. In this
paper, both the RTs used as WC learners and within Dy-
com used the same parameters, which were the ones more
likely to obtain good results in the literature [22]. Dycom
contains two extra parameters which were set to the same
value for all databases used in this study, i.e., they were not
ne tuned for each database. So, the results obtained in this
paper do not depend on the user ne tuning Dycom. It is
natural that a software manager with no specialist knowl-
edge on machine learning would run these approaches with
default parameters or parameters that have previously ob-
tained good results in the literature. So, it is reasonable to
perform the analysis in this way in the current paper. Fu-
ture work should investigate whether Dycom's results could
be improved further by ne tuning parameters.
Construct validity regards to accurately naming our mea-
sures and manipulations [24]. We used several dierent per-
formance measures (MAE, StdDev, SA, RMSE, Corr, LSD,
MMRE and PRED(25)). Wilcoxon statistical tests with
Holm-Bonferroni corrections were used to check the statis-tical signicance of the dierences in overall performance,
and SA was also used to give a better idea of the magnitude
of the dierences in performance and the impact that they
are likely to have in practice.
External validity regards to generalizing the study's re-
sults outside the study to other situations [24]. Besides
never using a WC project for training before using it for
testing, we considered ve databases to handle external va-
lidity. Four databases with known WC chronological or-
der were used for evaluating Dycom. Even though the WC
chronological order is not known for the other database, it
can still be used to evaluate whether Dycom is able to suc-
cessfully make use of CC models, contributing to the gen-
eralisation of our results. Obtaining additional databases
for evaluating Dycom is dicult due to our need for non-
proprietary data sets with information on which projects
belong to a single-company among the projects of a cross-
company data set. However, the data sets used in this study
can be made available through PROMISE and ISBSG. So,
researchers and companies willing to use Dycom could use
the same CC data sets used in this study. Further investi-
gation should be done into what would happen with Dycom
when using other base learners than RTs, and when using
other input features for the data sets.
11. CONCLUSIONS
We have introduced a new CC SEE learning scenario that
considers the relationship between the required eort of WC
and CC projects. We propose the rst framework for learn-
ing this relationship and mapping CC models to the WC
context. It is designed to make best use of CC data in SEE.
The benet of such a framework is demonstrated by a new
approach Dycom, which was evaluated on ve databases and
was able to substantially reduce the amount of required WC
training examples while maintaining or improving overall
average performance in comparison to a WC model trained
on more WC examples. The research question raised in sec-
tion 1 was: if we map CC models to the WC context, can
we reduce the amount of WC labelled projects used in the
learning and still obtain similar or better performance than
a WC model? The study performed in this paper gives the
answer \yes" to this question. Dycom is an example of ap-
proach that successfully achieves that. Dycom can also be
used to give insight into monitoring and improving the pro-
ductivity of a company.
Future work includes an investigation of Dycom's sensi-
tivity to parameter values, base learners, input features and
techniques for splitting CC projects into dierent sections.
Our proposed framework could also be used to design other
approaches. For instance, approaches considering dierent
productivity sections of WC data could also be considered.
The online learning scenarios should also be studied when
predicting dierent numbers of WC projects than ten at
each time step, and techniques to decide which projects to
have their eort collected should be investigated. In addi-
tion, the best type of function to describe the relationship
between WC's and CC's eort should be studied.
12. ACKNOWLEDGEMENTS
The authors are most grateful to the reviewers for their
constructive comments. This work was supported by EP-
SRC grant no. EP/J017515/1 and European Commission
FP7 grant no. 270428 . Xin Yao was supported by a Royal
Society Wolfson Research Merit Award.45513. REFERENCES
[1] B. Boehm. Software Engineering Economics .
Prentice-Hall, Englewood Clis, NJ, 1981.
[2] L. Briand, T. Langley, and I. Wieczorek. A replicated
assessment of common software cost estimation
techniques. In International Conference on Software
Engineering (ICSE) , pages 377{386, Limerick, Ireland,
2000.
[3] K. Dejaeger, W. Verbeke, D. Martens, and
B. Baesens. Data mining techniques for software eort
estimation: A comparative study. IEEE Transactions
on Software Engineering (TSE) , 38(2):375{397, 2012.
[4] T. Foss, E. Stensrud, B. Kitchenham, and I. Myrtveit.
A simulation study of the model evaluation criterion
MMRE. IEEE Transactions on Software Engineering
(TSE) , 29(11):985{995, 2003.
[5] M. Hall, E. Frank, G. Holmes, B. Pfahringer,
P. Reutemann, and I. H. Witten. The weka data
mining software: An update. SIGKDD Explorations ,
11(1):10{18, 2009.
[6] ISBSG. The International Software Benchmarking
Standards Group. http://www.isbsg.org , 2011.
[7] R. Jeery, M. Ruhe, and I. Wieczorek. A comparative
study of two software development cost modeling
techniques using multi-organizational and
company-specic data. Information and Software
Technology (IST) , 42(14):1009{1016, 2010.
[8] B. Kitchenham and E. Mendes. A comparison of cross-
company and single-company eort estimation models
for web applications. In Empirical Assessment in
Software Engineering (EASE) , pages 47{55,
Edinburgh, 2004.
[9] B. Kitchenham, E. Mendes, and G. Travassos. Cross
versus within-company cost estimation studies: A
systematic review. IEEE Transactions on Software
Engineering (TSE) , 33(5):316{329, 2007.
[10] B. Kitchenham, S. L. Peeger, B. McColl, and
S. Eagan. An empirical study of maintenance and
development estimation accuracy. Journal of Systems
and Software (JSS) , 64:57{77, 2002.
[11] E. Kocaguneli, B. Cukic, and H. Lu. Predicting more
from less: Synergies of learning. In International NSF
Sponsored Workshop on Realising Articial
Intelligence Synergies in Software Engineering
(RAISE) , pages 42{48, San Francisco, 2013.
[12] E. Kocaguneli, G. Gay, T. Menzies, Y. Yang, and
J. W. Keung. When to use data from other projects
for eort estimation. In IEEE/ACM International
Conference on Automated Software Engineering
(ASE) , pages 321{324, Antwerp, Belgium, 2010.
[13] M. Leey and M. Shepperd. Using genetic
programming to improve software eort estimation
based on general data sets. In Genetic and
Evolutionary Computation Conference (GECCO) ,
pages 2477{2487, Chicago, 2003.
[14] C. Lokan and E. Mendes. Applying moving windows
to software eort estimation. In International
Symposium on Empirical Software Engineering and
Measurement (ESEM) , pages 111{122, Lake Buena
Vista, Florida, USA, 2009.
[15] C. Lokan and E. Mendes. Investigating the use of
chronological split for software eort estimation.IET-Software , 3(5):422{434, 2009.
[16] S. G. McDonell and M. Shepperd. Comparing local
and global software eort estimation models {
reections on a systematic review. In International
Symposium on Empirical Software Engineering and
Measurement (ESEM) , pages 401{409, Madrid, 2007.
[17] T. Menzies, A. Butcher, D. Cok, A. Marcus,
L. Layman, F. Shull, B. Turhan, and T. Zimmerman.
Local vs. global lessons for defect prediction and eort
estimation. IEEE Transactions on Software
Engineering (TSE) , 39(6):822{834, 2013.
[18] T. Menzies, B. Caglayan, Z. He, E. Kocaguneli,
J. Krall, F. Peters, and B. Turhan. The promise
repository of empirical software engineering data.
http://promisedata.googlecode.com, 2012.
[19] T. Menzies and M. Shepperd. Special issue on
repeatable results in software engineering prediction.
Empirical Software Engineering (ESE) , 17:1{17, 2012.
[20] L. Minku and X. Yao. Can cross-company data
improve performance in software eort estimation? In
International Conference on Predictive Models in
Software Engineering (PROMISE) , pages 69{78,
Lund, Sweden, 2012.
[21] L. Minku and X. Yao. An analysis of multi-objective
evolutionary algorithms for training ensemble models
based on dierent performance measures in software
eort estimation. In International Conference on
Predictive Models in Software Engineering
(PROMISE) , Article No. 8, 10 pages, 2013.
[22] L. Minku and X. Yao. Ensembles and locality: Insight
on improving software eort estimation. Information
and Software Technology (IST) , 55(8):1512{1528,
2013.
[23] L. L. Minku and X. Yao. Software eort estimation as
a multi-objective learning problem. ACM Transactions
on Software Engineering and Methodology (TOSEM) ,
22(4):Article No. 35, 32 pages, 2013.
[24] M. L. Mitchell and J. M. Jolley. Research Design
Explained . Cengage Learning, USA, 7th edition, 2010.
[25] P. Sentas, L. Angelis, I. Stamelos, and G. Bleris.
Software productivity and eort prediction with
ordinal regression. Information and Software
Technology (IST) , 47:17{29, 2005.
[26] M. Shepperd and S. McDonell. Evaluating prediction
systems in software project estimation. Information
and Software Technology (IST) , 54(8):820{827, 2012.
[27] L. Song, L. Minku, and X. Yao. The impact of
parameter tuning on software eort estimation using
learning machines. In International Conference on
Predictive Models in Software Engineering
(PROMISE) , Article No. 9, 10 pages, 2013.
[28] J. Wen, S. Li, Z. Lin, Y. Hu, and C. Huang.
Systematic literature review of machine learning based
software development eort estimation models.
Information and Software Technology (IST) , 54:41{59,
2012.
[29] I. Wieczorek and M. Ruhe. How valuable is
company-specic data compared to multi-company
data for software cost estimation? In IEEE
International Software Metrics Symposium
(METRICS) , pages 237{246, Ottawa, 2002.456