Ecological Inference in
Empirical Software Engi neering
Daryl Posnett
Department of Computer Science
University of California, Davis
Davis, CA
dpposnett@ucdavis.eduVladimir Filkov
Department of Computer Science
University of California, Davis
Davis, CA
vﬁlkov@ucdavis.eduPremkumar Devanbu
Department of Computer Science
University of California, Davis
Davis, CA
devanbu@ucdavis.edu
Abstract—Softwaresystemsare decomposedhierarchica lly,for
example, into modules , packages and ﬁles. This hierarchical
decomposition has a p rofound inﬂuence on e volvability, main-
tainability and work as signment. Hierarchical decomposition is
thusclearlyofcentralc oncernforempiricalsof twareengineering
researchers; but it also poses a quandary. At w hat level do we
studyphenomena,such asquality,distribution, collaborationand
productivity? At the le vel of ﬁles? packages? or modules? How
does the level of study affect the truth, meanin g, and relevance
of the ﬁndings? In oth er ﬁelds it has been fo und that choosing
the wrong level might lead to misleading or fallacious results.
Choosingaproperleve l,forstudy,isthusvita llyimportantfor
empirical software eng ineering research; but this issue hasn’t
thusfarbeenexplicitly investigated.Wedescrib etherelatedidea
ofecologicalinference andecologicalfallacyf romsociologyand
epidemiology, and exp lore its relevance to e mpirical software
engineering;wealsopr esentsomecasestudies ,usingdefectand
processdatafrom18o pensourceprojectsto illustratetherisks
of modeling at an agg regation level in the c ontext of defect
prediction,aswellasin hypothesistesting.
I. INTRODUCTION
Large systems are comp osed of multiple modul es. Mod-
ularization [1] is critical to scaling up software e ngineering
projects: without proper modular decomposition, and congru-
ent work assignment [2 ] large system developm ent would
become hopelessly mire d in knowledge bottlene cks and co-
ordination overheads. Ind eed, as systems scale up to millions
of lines of code and b eyond, designers seek hierarchical
modularization; for exam ple, the code in large sof tware prod-
ucts such as Eclipse ex hibit at least 3 hierarch ical levels
of decomposition,viz.,ﬁles, packages and plu gins/modules.
Hierarchical decomposit ion in software produc ts naturally
dovetails (and is often b eneﬁcially congruent [3] , [4]) with
the hierarchical organizat ion of modern software d evelopment
teams. Indeed, quite of ten we ﬁnd software p rocesses are
themselvesalsohierarchi cal,withstepscontained withinsteps.
Thus, we have hierarchic al systems developed by hierarchical
teams using hierarchical processes.
While hierarchical deco mposition is largely an unalloyed
blessing for large softwa re products, teams and p rocesses, we
argue that it poses risks for empirical software e ngineering
research (ESE). ESE is concerned with observab le outcomes
such as quality and prod uctivity; such outcomes are subjectto large-sample studies, so that a) statistical me thods can
be brought to bear for h ypothesis testing, and b) automated
machine learning and m ining methods on past d ata can be
built into tools that sup port programming tasks . Thus for
example, many studies fo cus on software quality, b y choosing
the number of defects in an element as a respons e variable,
and (dependingon the hy pothesis) measuresof suc h factorsas
the complexity of the ele ment, the number of cont ributors, the
developmentorganization ’ssocialandgeographics tructure,as
predictor variables. Like wise, historical defect da ta is mined
and used with machine learning models to aut omatically
predict likely future loci for defects. These ideas animate a
large body of ESE resea rch.
Hierarchical decompositi on becomes important he re. Many
papers1study phenomena at the level of ﬁles [5], [6], wh ile
others study them at high er levels of aggregation [ 7], [8], [9],
[10] (e.g., packages or modules).
But what is the right lev el of study?
The reason why this is an important question, w orthy of
being underlined and ita licized, becomes clear w hen we ask
two derivative questions:
a) If we build a statistic al model to test a hypo thesis at an
aggregated level (e.g., pa ckages), do the ﬁndings i n the model
hold at the dis-aggregate d level (e.g., ﬁles) ?
b) If we build automated prediction models at vary ing aggre-
gated levels (e.g., bug prediction at a m odule-level, or at a
ﬁle-level) does the perfo rmance at these differen t levels give
equally valid indications of the actual cost-effect iveness of
their predictions (e.g., when using these predic tion models for
inspections)?
Ecological inferenceis the conceit that an em pirical ﬁnding
at an aggregated level (e.g.,packages) can apply at t he disag-
gregated level (e.g.ﬁles). When this inferen ce is mistaken, we
have theecological fallacy.
1There are too many pa pers to enumerate, we merely present a few
representatives978-1-4577-1639-3/11/$26.00 c2011 IEEE ASE 2011, Lawrence, KS, USA362
Contributions: In this paper, we make t he following contri-
butions.
1) Wepresentadetailed conceptualoverviewofE cological
inference and ecological fallacy: What are they? Why
are they relevant in softw are engineering? What a re the
speciﬁc risks in software engineering?
2) We present a theore tical discussion of sever al factors
known to give rise to ec ological inference risk:sample
size,zonation, andclass imbalance.
3) We empirically study the incidence of ecologic al infer-
ence in 18 open source p rojects. We ﬁnd:
•Ecological inference risk in defect prediction mod -
els: while it may appear from ROC type measure s
that aggregated, package -level predictions model s
are similar (or slightly b etter than) disaggregated ,
ﬁle-level prediction mod els, in fact, when usin g
cost-effectiveness measu res, ﬁle-level models ar e
decidedly better.
•Ecological inference risk in hypothesis testing: us -
ing multiple regression, we ﬁnd quite a number of
cases where a null hypot hesis is rejected (p≤0.05)
at an aggregated levelcannotbe rejected at the
disaggregated level, and vice versa.
The goal of this paper is to lay out a conceptual fr amework
of ecological inference risk in software engine ering, and
empirically demonstrate the existence of this risk . In future
work, we hope to study t he effects of the factors ( sample size,
zonation, and class imba lance) on this risk.
Relevance to Automated Software Engineering:This paper is
concerned with the cons truction of defect predic tion models,
which provide an autom ated way to focus qua lity-control
efforts.
II. BACKGROUND AND CONCEPTS
The risks of transferring statistical inferences from aggre-
gated groups to smaller constituent groups were noticed as
early as 1950 by Robi nson [11]. He observed that at an
aggregated level (U.S. St ates), immigrant status w as positively
correlated (+0.526) with educational achievement , but at the
individual level, it was negatively correlated (-0 .118). This
discrepancy has been attr ibuted to the tendency of immigrants
to congregate in regions with higher levels of ed ucation. In
this case, the congregat ional tendency of immi grants is a
confounding phenomeno n at the aggregated region al level that
jeopardized the internal validity of the study at that level.
This discrepancy betwee n the ﬁndings at the agg regated and
disaggregated levels illus trates theecological fallacy.
Detecting a phenomenon at an aggregated level, a nd then
inferring it to apply a t a disaggregated leve l is called
ecological inference ;doingsoriskstheecological fallacy .For
brevity, we use the abbre viationEIfor “ecological inferenc e”
andEFfor “ecological fallacy ”. While the above (n ow-
classic)exampleillustrate stheecologicalfallacy,b ynomeans
is it the case that all e cological inferences are subject to
ecological fallacies.This issue has been expl ored in geographical and epidemi-
ological research. The te rm MAUP or Modiﬁable Areal Unit
Problem introduced by O penshaw and Taylor [12 ] addresses
the issues of scale and zonation in geographic data.Scale
refers to the size of the aggregated unit: larger s cale means
biggerandfeweraggrega tedunits.Zonationreferstotheman-
ner in which aggregation is performed. A vividly p athological
example of zonation is gerrymandering, wherein geographic
regions are aggregated in distorted, artiﬁcial w ays to the
deliberateelectoraladvan tageofapoliticalparty.M AUPrefers
totheproblemofchoosin gtheproperscaleandzon ationwhen
studying phenomena that are subject to aggregatio n, and thus
mitigating the risk of eco logical fallacy.
In Empirical software e ngineering (ESE) pheno mena are
often studied at the aggr egated package or modu le levels [9],
[10]. Certainly, assumin g that hypotheses suppo rted at the
package level hold at the ﬁle level is subject toEIrisk. This
risk applies to predictio n models as well. Cons ider the use
of prediction models—th ey are intended to be us ed to focus
quality control efforts suc h as inspection. Inspector s work line
by line, whereas predict ion models are trained o n aggregate
metrics at the level of ﬁles or packages. It’s p ossible, and
indeedithasbeendocum ented,thatpredictionmo delsthatare
designed to work well a t the level of ﬁlesdo notwork well
at the disaggregated leve l of lines, which is argua bly the right
leveltodetermineactual inspectioneffort![13],[1 4].Although
theissueofEIarisesinempiricalsoftw areengineering,toour
knowledge, there has bee n no explicit discussion o f this in the
literature.
A. ESE,EIandEF
In the rest of this sectio n, we qualitatively consi derEIin
the context of ESE, illus trating the issues that can arise using
several examples. The d iscussion below has bee n informed
by similar considerations presented in other ﬁelds [15], [16],
[17] and is in the spirit of the paper by Briandet al.[18]
on the application of me asurement theory in ESE . First, we
discuss the reasons why studies may have to be conducted
at higher levels of aggr egation (thus risking the ecological
fallacy). Next, we discus s construct validity issue s that arise
directly from aggregation . Next, we consider zona tion issues,
which can threaten inte rnal validity, and were at the heart
of Robinson’s original f ormulation of the ecolog ical fallacy.
Finally, we consider the basic, fundamental issue of sample
size that rears its head a t increasing levels of agg regation.
B. Reasons for Aggregat ion
One might reasonably as k, why not avoid ecologi cal fallacies
altogether by always con ducting studies at the low est level of
aggregation (ﬁnest resolu tion)? There are natural reasons for
aggregated studies.
AggregatePhenomena Relevant phenomena app ly only at a
higher aggregated level, or differently at differen t levels. As
a simple example, some object-oriented phenome na, such as
inheritance and class co hesion [19], apply only at the class
level, not at the lower l evel of methods. Fan-in and fan-out363(number of methods call ing, or called-by a meth od) apply at
the method level, not at t he level of a line of code . As a more
subtle example, certain ty pes of phenomena may o nly emerge
at the team level: some teams may be better m anaged or
morecohesivethanother s,andasaresult,team-re latedquality
and productivity effects may apply equally to m embers of a
team, and differentially a cross different teams. If teams (as is
common)areassignedw orkbasedonmodulardec omposition,
thenmoduleswillreﬂect theseteameffects.Insuc hsituations,
an aggregated, module-le vel study may be approp riate; even
so, transferring ﬁndings f rom (or prediction model s trained at)
the aggregated level to th e disaggregated level is r isky.
ObservationalResolutio nData may be observable at only
certain levels of aggrega tion. Field defect data m ay be only
availableatthebinarylev el,sincefailureinformati onavailable
to users and customer support staff might not have ﬁle-
level information. Custo mer satisfaction ratings might only
be available at the leve l of complete applicatio ns. Despite
the unavailability of da ta at lower levels of a ggregation,
EImay be desirable, as it may lead to actionable c oncepts.
For example, if statistica l ﬁndings relating to ﬁel d defects at
the binary level could be transferred to the ﬁle lev el, it may
suggest better ways of do ing work assignment at th e ﬁle level,
inordertoreduceinciden ceofﬁelddefects.Ofcou rse,ifsuch
inferences are fallacious, the results may not be ac tionable.
Next we discuss 3 possib le pitfalls of aggregated s tudies in
empirical software engin eering.
C. Difﬁculties of Interpr eting Aggregated Results
If we model data at an a ggregated level, and obse rve some
statisticallysigniﬁcantres ult,whatisthemeaningo fthatresult
at the disaggregated leve l? Given an aggregated l evel ﬁnding,
what action should we take? A measure’s inter pretation, or
content, may not easily t ransfer from an aggregat ed level to a
disaggregatedlevel;thati s,ameasureofaproperty atacoarser
level of aggregation mig ht be difﬁcult to interpre t and/or act
upon at a more ﬁne-grain ed level. Some epidemio logists (see,
e.g.,Schwartz [17]) have c onsidered this to be a form of
construct validity. In fac t, a ﬁnding at a module level and a
ﬁnding at a ﬁle level, wi th the same measure, mi ght indicate
different remedies. We n ow present twogedankenexamples
of this;
OrganizationalEffects. Organizational effects have been
foundtoaffectquality.Fo rexample,Nagappaneta l[20]report
that metrics such as ow nership and organization al diameter
are excellent predictors of defect occurrence. Th e study was
conducted at the level of binaries in Windows , which are
aggregations of ﬁles. Co nsider the simplest metri c presented,
the number of engineers (NOE) working on a bin ary. If NOE
is strongly correlated w ith defects at the binary level, two
interpretationsarepossib le.Considerabinarywith highNOE,
and also high defects. Fi rst, if the binary has a la rge number
of source ﬁles, each allo cated to a different deve loper (thus
having overall high NOE ), then miscommunicatio n between
developers might be lead ing to errors. In this case , the proper
response might be to mor e carefully deﬁne inter-ﬁ le interfaces(“designrules”[21]).On theotherhand,ifthebina ryhasonly
afewﬁles,buteachisma intainedbymultipledeve lopers(thus
also having high NOE), the difﬁculty might be a rising from
developers tripping over each other’s work, in wh ich case, the
response might be to allo cate fewer developers to each ﬁle, or
to actually split the ﬁles up into smaller work uni ts.
GeographicalIssues. Several studies [22], [2 3], [24] have
found that geographical distribution can affect so ftware qual-
ity. The same argument s presented above apply here. For
example, Ramasubbu & Balan [23] ﬁnd that ge ographical
distribution at the produ ct level negatively impa cts quality
and productivity. Suppo se that products are co mposed of
binaries,andbinariesare composedofﬁles.Whati sthecorrect
response to this ﬁnding? Should we ensure that al l developers
of each product are in o ne location? Or that dev elopers of a
single binary are not geo graphically split? Or that developers
of single ﬁles are not split? Any of these typ es of splits
might have given rise to the quality and productiv ity impact
observed in the study. Int erestingly, the other two studies [22],
[24] were at binary and ﬁle levels, and have yield ed different
results. Spinellis [24] co nsiders the geographical distribution
of committers to ﬁles in FreeBSD; Birdet al.[22] consider
distributionofdevelopers atthebinarylevel.These twostudies
have found no effect of distribution on software quality. The
3 studies were conduct ed in different settings (outsourced
development, open-sourc e, and globalized develo pment, re-
spectively in the order discussed above) and do ubtless, the
results are inﬂuenced by the setting. Still, it is rea sonable to
speculate whether the dis crepancy is a result of th e choice of
aggregation.
D.EIRisk factor: Zonation
Another issue to conside r is the possibility of con founding
factors inzonation, the way in which sma ller units are ag-
gregated into larger units . These can threaten inter nal validity.
Internal validity question s can arise when an obs erved rela-
tionship between indepen dent and dependent varia bles might
be an artifact of confou nding variables. Aggrega tionper se
can lead to confounding, and thus threaten internal validity. In
Robinson’sexamplequot edabove,thetendencyof immigrants
tomovetoareaswherees tablishedresidentsaremo reeducated
is confounding.
E.EIRisk Factor: Sample Siz e
Another issue to conside r in aggregation is samp le size. It
may be possible to cond uct a study at the level of methods,
classes, ﬁles, packages, modules, or even produc ts. Suppose
that in the study, the ph enomena of interest are modeled by
a set of independent var iables and a dependent v ariable, and
furthermore,thatthisset ofvariablescanbereason ablyaggre-
gated at multiple levels f or study. As the level of aggregation
increases, there will nece ssarily be fewer and few er samples.
The lowest levels of agg regation, with the large s ample sizes,
will yield the highest sta tistical power, the higher levels will
have lower power, and g reater risk of over-ﬁt mo dels. On the
other hand, it is possible that some of the variable s are only364�����������������
������������������
������
�������������������������
�������
����������
������������
Fig. 1: Conceptual Fram ework
available at higher levels of aggregation, for reason s discussed
above.
F.EIRisk Factor: Class Imba lance
Defectpredictiondataoft ensuffersfromtheclassi mbalance
problem. Entities are lab eled as defective if they contain at
least one defect and not defective if they contain no defects;
thisclass labelingis seldom balanced in that the majority
of the entities contain n o defects, and yet, it is the few that
do that we wish to ident ify. The resulting class i mbalance is
mediated both by zonatio n and sample size since the number
of defects remains almo st constant across levels2. Even if
only defective entities a ggregate only with othe r defective
entities, the number of ag gregated entities is often sufﬁciently
smaller than the disaggre gated entities that class i mbalance is
reduced. Consequently, i nferences drawn from th e aggregated
datamaynotholdatthe disaggregatedlevelifcla ssimbalance
signiﬁcantly affects mod eling efforts.
G. Summary
Figure 1 summarizes the discussion above, and pre sents our
preliminary conceptual framework of the way to approach
EIin the context of empi rical software engineerin g work.
We consider two end-go als: prediction models, w hich aim to
focus human effort, and h ypothesis testing, which a ims to ﬁnd
statistical evidence in da ta in support or reject cl aims.
The ﬁgure illustrates ho w scaling and zonation arise out
of aggregation. At the s implest level, scaling aff ects sample
size(§II-E),andcanthusaffec tthepowerofstatisticalm odels
built with aggregated dat a. Small sample sizes ca n also make
the performance of pred iction models built with aggregated
data unpredictable. Agg regation naturally also a ffects zona-
tion.Bothsamplesizean dzonationcanaffectclas simbalance.
For example, considering the aforementioned “defe ctive” and
“non-defective” class lab els, class imbalance at th e ﬁle level
mightverywellbereduc edbyaggregationintopa ckages.Fur-
thermore, the manner in which zonation groups n on-defective
ﬁles along with defective ﬁles can also affect imba lance.
All 3 factors, sample siz e, zonation, and class im balance,
affect the quality of the statistical models that a re built to
do defect prediction, or h ypothesis testing. Sample sizes have
well-known effects on t he validity and stability of models.
2In some cases a defect m ay be associated with mo re than one ﬁle within
the same module causing a reduction in the numbe r of defects in the proces s
of aggregation.Zonationcancreateintern al-validitythreatsasares ultoflatent
choices made in the way samples are grouped into aggregates,
as shown by Robinson. F inally, class imbalance a s discussed
above also inﬂuences mo del quality.
III. RELATEDWORK
ESE researchers have inv estigated a wide diversity of phe-
nomena, including variou s aspects of quality and p roductivity.
Ecological inference, and the attendant risks, are li kely to be
relevantinthisbroadcon text.Ourstudyisfocused onresearch
into software quality, sp eciﬁcally into technique s that build
statistical models of def ect occurrence, either fo r hypothesis
testing or defect predictio n; our survey of related w ork is thus
largely conﬁned to statis tical and predictive mode ls of defect
occurrence.
A. Modeling and Predic ting Defects
The core idea in this very popular area of re search is
to consider the number of defects as a respons e variable,
and choose predictors b ased on intuitions about factors that
contribute to defects, su ch as complexity, desig n attributes,
personnel attributes, prog ramming practices, team structures,
and so on. Regression m odels and correlation st udies have
been used to gauge the explanatory power of di fferent vari-
ables, and thus test hyp otheses concerning the etiology of
defects. Another line of research, exempliﬁed by the annual
PROMISE3meetings at ICSE, attem pts to build accurate,
reliablepredictionmodel stopredictwheredefects mightoccur
next,usingpastdatafort raining.Thisresearchiso pportunistic
and eclectic, leveraging a wide set of statistical and machine-
learning techniques to im prove prediction perform ance.
This research area is to o rich and varied to su rvey and
cite in detail. Our intere st here is in aggregated modeling of
defects, for prediction o r hypothesis testing, so we present
some examples of this ap proach.
AggregatedDefectMod elsAggregationiscommoni ndefect
modelingresearch;acom monmotivationisobservationalres-
olution(See§II-B), since ﬁeld defect d ata is often reported at
aggregatedlevels,e.g.,atthelevelofbinaries[2 2],[8],oreven
completeprojects[23].K oruet al.[7]recommendaggregat ing
for a different reason: i mproving model perform ance. They
summethod-levelmeasur esintoclass-levelmeasur es,thenuse
these aggregated measur es along with metrics o f class-level
properties to improve m odel performance. They report that
this approach overcomes problems arising from t he skewed
distribution of defects in the NASA KC2 dataset, and defend
their approach, arguing th at the source dataset is la rge enough
to obtain statistically me aningful results.
Ramleret al.[25] also address how to build a quality
defect model and reiter ate the ﬁndings of Kor uet al.[7].
In a study aimed at pred icting the likelihood (cou nt) of post
release failures at the mo dule level, Nagappanet al.aggregate
per/class and per/functio n metrics by sum and m aximum and
combine with module le vel metrics [26]. Schr ¨oteret al.use
imports gathered at the ﬁle level to predict failu res for both
3Predictor Models in Soft ware Engineering365ﬁlesandpackages.Impor tsareaggregatedtothep ackagelevel
from the subordinate cl asses [9]. The authors a ssert “Intu-
itively, predicting for a co arse granularity is easier while using
ﬁne-grained input feature s yields better results.” Z immerman
et al.[10] report ﬁle and pac kage level correlations b etween
defects and ﬁle level an d aggregate code metric s. In their
study the package level correlations are consiste ntly higher
than the ﬁle level corre lations. Similarly model s built from
package level variables s howed a constantly highe rR2value.
In a later study, Zimmerm anet al.use function, class, and ﬁle
level metrics aggregated to the binary level to pre dict failure
prone binaries [27].
However, aggregation h as not been without co ntroversy.
Ambroset al.[28] criticize the use of aggregate level metrics
fordefectmodelevaluatio n.Theyclaimthat“Predic tionsatthe
package-level are less he lpful since packages are signiﬁcantly
larger”andthat”therevie wofadefect-pronepacka gerequires
more work than a class.” In addition they assert th at “classes
are the building blocks of object-oriented system s, and are
self-contained elements f rom the point of view of design and
implementation” and tha t “package-level informa tion can be
derived from class level i nformation, while the op posite is not
true.”
ClassImbalance As previously discussed , defect data fre-
quently suffers from clas s imbalance. This proble m has been
studied both generally, and within the context of software
defect data. One simple technique to correct for class im-
balance is to bias the sa mpling process. Undersa mpling and
oversampling are opposi te procedures that bias t he sampling
procedure to either favor the minority class (oversa mpling), or
penalize the majority cla ss (undersampling). Drum mond and
Holte showed that when using their cost-sensitive evaluation
technique,viz.the cost of misclassiﬁ cation is taken into
account, that undersamp ling outperforms oversam pling [29].
Other more elaborate te chniques have also been proposed.
Guet al.propose a technique t hat minimizes the impa ct
of problem instances,viz., those instances whose predictors
causeclassoverlap[30]. Menzieset al.applymicro-sampling,
which combines undersa mpling and data reductio n, to defect
data [31].
In conclusion: previous efforts at statistical mod eling of
defect occurrence have g iven some consideration to aggrega-
tionandclassimbalance issues.However,ourspec iﬁcconcern
here is with the validity ofEI: whether models built at the
aggregated level apply a t disaggregated levels. W ith defect
models, the question co ncerns both the hypoth esis testing
and prediction performan ce. The former is addres sed in§V;
before we address the la tter, we ﬁrst discuss how prediction
performance is evaluated .
B. Evaluating Prediction Models
There are several appro aches to defect predicti on model
evaluation: we begin wi th the simplest,precision/recall, and
work up to the more sop histicated,cost-effectiveness curves.
Precision/Recall Defect prediction can be viewed as a binary
classiﬁcation problem. E ntities are classiﬁed as d efect proneornotbasedonpredictor ssuchascomplexity,size ,ownership,
and pre-release defects. A n entity predicted as defe ct-prone is
considered a true positiv e (TP) if it actually cont ains a post-
release defect, and false positive (FP) if it does n ot; entities
predicted as not defect- prone that contain defec ts are false
negatives (FN) and the r est are true negatives (TN ).
Well-known prediction p erformance measures en sue from
these counts.Accuracy, computed asTP+TN
TP+FN+FP+TNyields
the chance that the total number of modules will be predicted
correctly.Precisionis the ratio of correct p redictions to the
total predicted as defec tiveTP
TP+FP.Recallis the ratio of
correct predictions to the actual number of defecti ve entities
TP
TP+FN. A good model should achieve both high precis ion,
andhighrecall,butthere isawell-knowntrade-off betweenthe
two. The F-Measure take s the harmonic mean of t he two, and
hasbeenusedtomeasure overallpredictionperform ance.Most
classiﬁers do not produc e a hard yes/no decision , however.
The classiﬁer output is t ypically a probability tha t must then
be compared to a thresho ld to obtain a classiﬁcatio n decision.
Signiﬁcant work has add ressed how meaningful m odel evalu-
ation criteria are with res pect to defect prediction models. Ma
et al.[32] criticize accuracy a s it ignores the data distr ibution
and cost information. L essmannet al.[33] argue that the
requirement of deﬁning a threshold is reason en ough not to
use such simple static me asures in a defect predict ion context.
ROCAn established method of evaluating classiﬁers inde-
pendently of any particu lar threshold isReceiver Operating
Characteristic(ROC) analysis. An RO C curve represents a
family of precision/reca ll pairs generated from varying the
thresholdvaluebetween0and1andplottingtheFalsePo sitive
RateF P R=FP
FP+TNon thex-axis and the True Posit ive
RateT P R=TP
TP+FNon they-axis. All such curves p ass
throughthepoints(0,0)and(1,1).Thepoint(0,1)represents
perfect classiﬁcation and points on the ROC curv e close to
(0,1)represent high quality classiﬁers. A common w ay to
evaluate the overall qual ity of the classiﬁer is to compute the
area beneath its ROC cu rve, which we denote AUCROC; this
has a value between 0 an d 1.
A limitation of ROC cu rves, and by extension a ll of the
aforementioned measure s, is that they value all entities the
same.Classiﬁcationofan entityasdefectproneisn ottheonly,
oreventheprimary,goal ofdefectprediction.Idea lly,thegoal
ofsucheffortswouldbe toaidinefﬁcientlyguidin gcorrective
maintenance. Thus the cl assiﬁer tells us where to ﬁnd defects,
viz., approximately where t o look to take correctiv e action.
We can view inspection e ffort as roughly proportio nal to lines
of code, and so it make s sense that the value o f inspecting
a class depends on its bug density. Maet al.address this
issue and recommend c areful evaluation of RO C curves at
meaningful performance points [32], Arisholmet al.[34]
address it somewhat dif ferently by deﬁning a m etric, cost
effectiveness, more appr opriate to defect predic tion models
motivated by code inspec tion.
CostEffectiveness Suppose defects were uniformly dis-
tributed through the so urce code. If an inspec tion budget
allows inspection of10%of the source code then we might366TABLE II: Metrics gathe red and their description .
Metric d escription
LOC S ource lines of code
Lines T otal lines in ﬁle/package
# Developers N umber of developers who have
edited this ﬁle/package
# Active Developers N umber of developers on this
ﬁle/pkg in current releas e
Churn N umber of added changed lines
Commits C ount of commits to ﬁle/p kg
Features N umber of new features a s
identiﬁed by issue tracke r
Improvements N umber of improvements as
identiﬁed by issue tracke r
choose10%of the lines at random and might reasonably
expect to ﬁnd about10%of the defects. This sche me requires
minimal work and no ex pertise in data gathering and defect
modeling. Therefore, it is reasonable to expect that any useful
defect prediction method should be able to impro ve on this
result. This is the basis f or the cost effectiveness (CE) metric
deﬁnedbyArisholmet al.[34].TheCEcurveplots percentage
of identiﬁed faults found against the number of lin es of code
accumulatedbyentitiesc onsidered.Touseapredi ctionmodel,
weusethemodeltocom puteapredicteddefectpr obabilityfor
each entity. Entities are ordered by decreasing o rder of fault
probability and increasin g size. A successful mo del is then
one that predicts a greate r percentage of faults fou nd than the
percentage of lines of co de inspected, hence, a cu rve that lies,
at least in part, above t he liney=x. The Arisholmet al.
formulation computes on ly the area above the lin ey=xas
contributingtotheCEme asure.Wesimplytakethe areaunder
the CE curve (denoted AUCCE) as our measure as we a re not
evaluatingpracticalmode ls,rather,wearelooking attherange
of cost effectiveness achi eved by models in differ ent settings.
Arisholmet al.expand on their work o n CE in a systematic
investigation of methods used in building defect prediction
models considering issue s of ROC vs cost effecti veness [13].
Weusethe AUCCEmeasuretoevaluatehow predictionmodels
perform primarily with r espect to code inspection s. We argue
that a independent of pre diction performance, the model that
facilitates the identiﬁcati on of the greatest numbe r of defects
after inspecting the fewe st lines of code is of grea ter practical
use.
IV. EXPERIMENTAL METHODS
Wenowpresentourﬁndi ngsillustratingtheriskso fecolog-
ical inference, in the two settings commonly used in empirical
softwareengineering, def ectpredictionand hypoth esistesting.
These ﬁndings support th e following claims:
1)Prediction models are subject to ecological in ference
risk.Modelsbuiltusingaggre gateddata,evenwhenthe y
show reliable performan ce at the disaggregated level,
can have less reliable p erformance at the aggre gated
level.
2)Hypothesistestingissu bjecttoecologicalinferen cerisk.
Strong, practically signiﬁ cant relationships observ ed atthe aggregated level may weaken, at the disaggreg ated
level
But ﬁrst, some details of our experimental ap proach.
DataGathering We extracted data from the JIRA defect
tracking system and asso ciated git repositories for 87 distinct
versions of 18 different ASF(Apache Software Foun dation)
projects described in Tab le I. For each release we extract the
JIRARSSfeed,anXML reportofJIRAissues.We thencrawl
theassociatedJIRAwebp ageforeachissuefound intheXML
report and extract the co mmits related to that iss ue. We then
link this data to the git l og to determine which c ommits, and
consequently which ﬁles and packages, are assoc iated with
defects. A ﬁle associated with a closed defect in a commit is
considered to be a partia l repair for that defect in that commit
andtheﬁleislabeledasd efectivewithinthatreleas e.Foreach
ﬁleinareleasewegather thesizeinLOC(linesofcode)using
theSLOCcounttoolandaggregatethese countstothepackage
level [35]. The number o f developers associated w ith each ﬁle
and package is identiﬁed from the unique author n ames in the
git log and counted dire ctly for both ﬁles and p ackages. We
distinguish between the number of developers wh o have ever
touched a ﬁle or package and active developers,viz., the count
ofthoseactivelyworking onthepackageinthecur rentrelease.
Similarly,wecountuniqu edefectIDsineachﬁlea ndpackage
to identify the number of defects associated with e ach release.
We countimprovements, the number of code im provements
identiﬁed by the JIRA tra cking system as well asfeatures, the
number of new features.
We would certainly have liked to run our experim ents with
existing data used by ot her researchers,e.g., such as can be
found in the promise rep ository [36]. We note, h owever, the
vast majority of such dat a is measured at a single aggregation
level; it is, therefore, i mpossible topost factoaccurately
perform set unions to ag gregate defect and devel oper counts
to a higher level. A sing le defect, as an example , associated
with each of three class es in a package contrib utes only a
singledefecttothepacka ge.Sincemostavailabled atacontains
only counts and not iss ue identiﬁers, it cannot be properly
aggregated.
ModelingDefects
We use logistic regressi on to classify ﬁles and packages as
defectiveusingthemetric sreferencedinTable??aspredictors
andtheexistence(0or1) ofapostreleasebuginth eentityasa
response. For each predic tor thez-test statisticis computed by
dividing the estimated va lue of the parameter by its standard
error and used to asses t he signiﬁcance of the va riable. This
statistic is a measure of the likelihood that the a ctual value
of the parameter is not zero: the larger the abs olute value
of the statistic, the less likely that the actual va lue of the
parameter could be zero . For each package/ﬁle v ariable pair,
changes in thep-valueassociated with the z-tes t and analpha
value of0.05to decide if a parameter can be judged to have
a signiﬁcant effect.
Given the large number of projects and releases we used
an automated model sel ection technique to iden tify models.
We enumerated all com binations of at most six predictors367TABLE I: Apache projec ts and their description
Project Releases Description # Releases # Files # Packages
Abdera 1.0,1, 1.1 Atom (XML Syndication ) 2 6 72-680 112-113
implementation
Cassandra 0.6.0 - 0.6 .8 Distributed Databa se 9 314-332 31-33
Cayenne 3.0, 3.0.1 Java Object Relational 2 2763-2764 160- 162
Mapping framework
CXF 2.11-2.3.1 Services Framework 17 3086-4097 491 -598
HttpCore 4.0.1,4.1 Http Core Library 1 451-451 29- 29
Ivy 2.0.0 - 2.2 .0 Agile Dependency Manager 3 481-498 65-67
IvyDE 2.0.0, 2.1. 0 Eclipse plugin for Ivy 2 118-95 20-23
James 2.3.0 - 3. 0 Java Apache Mail Enterprise 5 375-477 39-85
Server
Lucene 1.9.1 - 3.0 .3 Text search engine library 7 1010-957 102-85
Mahout 0.4 Machine Learning frame work 1 11 19-1119 147-147
Nutch 1.1, 1.2 Web search software 2 446-453 89- 91
ODE 1.2-1.3.4 Business process executo r 7 10 34-954 122-99
OpenEJB 3.0 - 3.1. 3 Enterprise Java Be ans 9 2191-2949 124-191
Pluto 2.0.0, 2.0. 1 Java Portlet referen ce 2 370-371 44-44
implementation
Shindig 2.0.0, 2.0. 1 OpenSocial applica tion 2 811-812 75-75
Solr 1.3.0 -1.4. 0 Lucene search serv er 2 542-749 33-36
container
Wicket 1.2.7 - 1.3 .7 Web Application F ramework 9 1776-1947 240-249
XercesJ 2.7.1 - 2.11 .0 Java XML parser 5 740-827 67- 71
from those presented in T able??. It has been shown that size
oftenfollowsalog-norm aldistribution[37].Cons equently,we
include the log transform ations of size variables in addition
to their untransformed c ounterparts allowing the model se-
lection process to choos e between them. Log tra nsform data
can increase central tend ency and reduce heteros kedasticity,
however, in some cases u ntransformed data may y ield a better
ﬁtting model [38]. We ra nk the models byAkaike Information
Criterion(AIC) score and select the model with the lowe st
AIC value. Using AIC alone, however, may yi eld models
with high multicollinear ity amongst some of th e variables.
Consequently, we check for this by rejecting m odels with
VIFVariance Inﬂation Facto rhigher than5. Without speciﬁc
reasons for including a high VIF variable in a model, the
value5,isgenerallyconsidered tobethemaximumaccep table
value [38]. By rejecting models with high VIF an d sorting by
how well the models ﬁt the available data, this pr ocess yields
models that are similar t o what a researcher mig ht choose if
she were manually iden tifying models. In addit ion, it does
not constrain our process to choose the same mod el for each
revision and project.
V. FINDINGS
If we build a prediction model that performs we ll on an
aggregated measure, how does it perform on a dis aggregated
measure? To address th is question we build mo dels at two
levels and explore the relationship between the m. For the
aggregatedlevel,weuseﬁ lesandpackages;apredic tionmodel
works well at this level if it accurately predicts which ﬁles
(or packages) have defe cts with as few false p ositives and
false negatives as possib le. As a measure of per formance at
these aggregated levels, we use a ROC chart, a s measured
byAUCROC. For the disaggregated level, we use a cost-
effectiveness lift-chart, a s measured by AUCCE.Webuiltpredictionmode lsattwolevels:atthebas elevelof
ﬁles, and then an aggreg ated level, where ﬁles ar e aggregated
into packages. While m ost ﬁles contain a single Java class,
some ﬁles may contain more than one; each ﬁl e, however,
belongs to at most one p ackage. Of the 87 availa ble versions
of the 18 projects disc ussed in Section IV, w e use only
the 68 versions that me et minimal sample-size and events
per variable requirements considered appropriate fo r use with
maximum likelihood es timation approach used in logistic
regression. [39]. The min imum is required for both number of
packages and number of ﬁles and is computed as follows. Let
pbe the minimum class p ercentage,e.g., for100ﬁles with10
defective,p= 0.1. Letkbe the number of predic tor variables.
Then the minimum numb er of samples isN= 10k
p. We thus
reject datasets where the re are not only insufﬁcie nt numbers
of datapoints, but also i nsufﬁcient numbers of d atapoints in
each class.
A. Aggregation Effects o n Model Quality
In the ﬁrst study, we so ught to evaluate how th e level of
modeling affects the qu ality of the models. We used 55 of
the available revisions fr om 15 projects based on the criteria
that each dataset used n eeded a revision on whi ch to train,
and at least one subseq uent revision on which to evalute
the predictive power of the trained model. Each of the 55
prediction models was e valuated using two mea sures. First
at both levels, we calcu lated the area under the ROC curve
(AUCROC) for each of the models . Thus,e.g., at thepackage
level,AUCROCmeasures how well the package-level model
predicts defectivepackages, and so also respectively the ﬁle-
level model. Second, for both levels, we calculate theAUCCE
measure for each of the models,viz.at the package level, we
plotthecost-effectiveness curve,andcalculatethe AUCCE;this
is repeated for the ﬁle le vel.3680.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0Area Under ROC/CE C urve
File
AUCROCPackage
AUCROCFile
AUCCEPackage
AUCCEFig. 2: Comparing AUCROCandAUCCEfor packages and classes. AUCROCis
not signiﬁcantly differen t across aggregation leve ls by a two-sided Wilcox on
rank sum test withp-value=0.51. AUCCEis signiﬁcantly lower for package
level results by Wilcoxon rank sum test withp-value=2.462e-05.
Even though package and ﬁle level predictor model s are often
indistinguishablebytheir AUCROCmeasures,ﬁlelevelmode ls
show clearly better AUCCEmeasures. This can be ob served in
Figure 2, showing that th eAUCROC’s are sometimes higher a t
thepackagelevel.Thus,i fwebuildamodelatapackagelevel,
andevaluateonlyatthesam elevel,packagelevelmodelsm ay
appearto perform better.
. . .but ﬁle level models are actually better:This phenomenon
can be observed from Fig ure 2, showing that the AUCCE’s are
generally higher at the ﬁ le level. Thus, if we buil d a model at
the level of ﬁles or pack ages, but evaluate at the ﬁne-grained
level (note that AUCCEconsiders lines of code l evel), rather
than at the aggregated l evel, we ﬁnd that ﬁle-le vel models
perform better.
If an unwary investigato r built a prediction mod el at the
package level, and also at the ﬁle level, and com pared them
using same-level AUCROC, he might falsely conc lude that
the package level mode l is giving comparable ( or, in some
cases, better) performanc e, whereas in fact, it is performing
worsein terms of the more de manding and realistic AUCCE
measure. From these ob servations, we draw the following
rather sobering conclusio n.
Aggregated prediction m odels, when evaluated pu rely at
the aggregated level, can look better than they rea lly are.
B. Ecological Inference Risk
Next, we study the risks of Ecological Inference to ESE.
Conceptually, the risk a rises in this setting: on e builds a
statistical model at an a ggregated level, uses it to test hy-
pothesis, andsimply assumes that th e results also hold at
the disaggregated settin g, without testing the s ame model
at the disaggregated set ting. We show that modelin g defectoccurrence at two diffe rent aggregation levels can lead to
substantially different ﬁt s of model parameters to the data,
and consequently to very different statistical infere nces.
We used model selection based on AIC and VIF to select
thebestmultipleregressi onmodelattheaggregate d(package)
level, and used the sam e set of variables from this best
aggregated model to buil d a model at the disaggr egated (ﬁle)
level. This gives a match ed pair of aggregated-dis aggregated
modelsforallprojectrev isions(18projects,68ve rsionstotal).
In order to evaluate the hypothesis that the corre sponding
model variable affects d efects, it is standard pra ctice to use
thep-valueof a coefﬁcient within e ach model. If the p-value
is below a threshold, s ay 0.05, that model va riable has a
signiﬁcant effect on defe cts, if not, the null hypo thesis (that
it does not affect defects ) cannot be rejected. We follow that
practice here in selectin g the signiﬁcant variabl es from our
models. Furthermore, fo r a particular variable, w e check if
the models at both leve ls give the same results (i.e. if it is
signiﬁcant at both levels or insigniﬁcant at both le vels). Thus,
for example, we ﬁnd th at thenumber of active deve lopers
tends to be signiﬁcant at both levels of aggregatio n in many
of the 68 models.
Major changes in a var iable’s signiﬁcance can result in
substantially different c onclusions from the inf erences. To
evaluateEIrisk,westudiedthechan geinthesigniﬁcanceofa
variableinamodelbetwe entheaggregated-disaggr egatedpair.
Speciﬁcally, for a given l evel of aggregation (ﬁle o r package),
and a given project vers ion (18 projects, 68 ver sions total),
we noted two propertie s of each model variab le: its ﬁtted
parameter’s sign (positiv e or negative) and its sig niﬁcance to
the model (signiﬁcant or insigniﬁcant).
We compared the corre sponding parameters be tween the
two levels, and focused on three outcomes ind icative of
EIrisk.Theﬁrstisachang einsign(from+to-orv iceversa)
between the two levels o f modeling; we did not o bserve any
such changes in our data . The second isgain of signiﬁcance,
or GS, where the variable gain s signiﬁcance when aggr egated
from ﬁle to package. Th e last isloss of signiﬁcance, or LS,
where the variable is sig niﬁcant at the package le vel, but not
at the ﬁle level4.
We discuss some observ ed examples of LS and G S below.
For example, thenumber of commitsis an “LS” variable in
the Abdera project: signi ﬁcant at the aggregated p ackage level
for the Abdera project, b ut insigniﬁcant at the ﬁle level. Thus,
if one were to conclude from the package-level model that
codechurnaffecteddefects,andthen unwarilyusedecological
inference to conclude th at ﬁles that were subjec ted to more
commits are more defec tive, that would be fallac ious in this
case; committing additio nal resources to inspect ﬁles would
probably be unwise. Like wise, in the case of Ivy, thenumber
of active developersis a “GS” variable, ins igniﬁcant at the
package level, but gainin g signiﬁcance at the ﬁle level. Thus,
an unwary researcher m ight conclude from a pa ckage-level
4There can be two other possible observations, w hen the coefﬁcients are
eitherboth signiﬁcant, or SSorboth insigniﬁcant, II, but they carry no
inference risk.369TABLE III: Counts of ga ins and loss of signiﬁcan ce in inference models. A s an example (ﬁrst row), the predictorcommitsshowed loss of signiﬁca nce (LS)
in 5 different releases in 2 different projects,abderaandcxf; the variable was actual ly signiﬁcant (p≤0.05) in 26 releases in 12 pr ojects in at least one
level of aggregation.
Type Predictor # Releases # Signiﬁca nt Releases # Projects # Signiﬁcant Projects Projects
LS commits 5 26 2 12 abdera, cxf
LS activedevs 2 32 2 11 abdera, wicket
LS improvements 5 15 2 6 cxf, openejb
LS devs 2 16 2 7 cxf, openejb
LS lines 3 1 2 1 cxf, wicket
LS features 5 9 4 6 cxf, james, nutch, o de
LS added 1 8 1 4 cxf
LS loc 2 1 2 1 cxf, openejb
GS commits 6 26 5 12 cassandra, ivy, nut ch, openejb, wicket
GS activedevs 4 32 4 11 cassandra, cxf, ivy , wicket
GS improvements 4 15 3 6 cxf, openejb, wicke t
GS devs 2 16 2 7 ivy, openejb
GS lines 5 1 4 1 cxf, wicket, abdera, mahout
GS features 2 9 1 6 cxf
GS added 1 8 1 4 wicket
GS loc 4 1 4 1 lucene, cxf, ode, xe rcesj
model that this variable has no inﬂuence at the ﬁle-level,
whereas in fact it does.
In summary, we found t hat out of a total of 108 variables
used in the 68 models, we found 28 instances of GS, and
25 instances of LS. Tabl e III summarizes our ﬁnd ings for the
variables that showed ga in and loss of signiﬁcan ce. Overall,
the table shows a worry ing number of cases of GS and LS.
Certain variables, such a snumber of commitsandnumber of
improvements, show either GS or LS in quite a number of
cases, and illustrate the potential for ecologically fallacious
inferences when hypothe ses are tested at the aggr egated level,
and the results of these tests are inferred to ap ply at the
disaggregated level. Espe cially notable is thenumber of new
featuresvariable. It shows both gain and loss of signiﬁca nce.
For it, we observe GS in 2 revisions, on 1 differen t projects,
and LS in 5 revisions across 4 different proje cts. These
numbers should be con sidered relative to the t otal number
of revisions (and project s) where the variables w ere found to
be signiﬁcant at least at one level. Thus,the number of new
featureswas signiﬁcant in 9 rev isions in 6 different proj ects.
Thisgivesanindicationo fthechancesofrunningi ntoEIrisk.
Inferences drawn from m odels built at aggregated levels
such as models, package s, or binaries, may not t ransfer
to disaggregated compon ents (e.g., ﬁles) used to build the
aggregations.
VI. THREATS TO VALIDITY
Perryet al.[40] identify three form s of validity that must
be addressed in research studies. We now examin e threats to
each form of validity in our study and the metho ds used to
mitigate these threats wh ere possible.
Constructvalidityattemptstoreconcileme asuredproperties
with the concepts they a re believed to represent. Files often
containmorethanonecla ss,hence,theyarethemse lvesaggre-
gations of classes. It cou ld be argued that a better comparison
is classes vs. package. F iles, however, are the low est unit ofaggregation for which we had bug linking data so w e feel that
the aggregation is justiﬁe d in this case.
Internalvalidityistheabilityofastudyto establishacausal
link between independen t and dependent variable s regardless
of what the variables ar e believed to represent. We rely on
linking data to identify d efects, hence, our models cannot cap-
ture defects notlinked by developers. Thisapproac h, however,
is widely employed in th is area. Further, this sam e limitation
applies toimprovementandnew featureannotations of JIRA
issue tickets. We were careful to address mult icollinearity
issues as part of our auto mated model selection p rocess.
External validityrefers to how these resul ts generalize. The
threats here are common to studies of this type. W e use only
open source data and the consider only source wri tten in the
Java language. Our case study is limited to a sing le source of
projects, the Apache Sof tware Foundation, which may affect
towhatextentourresults cangeneralize.However, westudya
largenumberofrevisions ,collectedfrom18differ entprojects.
VII. DISCUSSION & CONCLUSION
We have discussed and illustrated the risks of e cological
inference in software en gineering. However, it i s important
to keep in mind that it is difﬁcult, indeed unwis e to always
attempt to avoid this risk . Software is inherently h ierarchical.
Certainly, products are h ierarchical, (as discussed above with
ﬁles, packages, and plug ins). In addition, teams are hierar-
chical, with hierarchical management structures. D evelopment
processes can also be vie wed hierarchically, with minor steps
within major steps, and iteration loops nested w ithin other
loops.
Becauseofaggregatephenomena,andobservationalresolu-
tion, as discussed above in S ection II-B, it is often ne cessary
to study phenomena and /or gather data at aggreg ated levels
of products, teams, or p rocesses. It is also possi ble that the
resulting ﬁndings are o nly actionable at the d isaggregated
level,viz.,at the level of ﬁles, ind ividual people, or steps of
a process. Therefore, it is unlikely that ecologic al inference,
risks notwithstanding, ca n be completely avoided in empirical370software engineering. W hen making the inferenc e, however,
the risk of ecological f allacy needs to be con sidered and
discussed. As we have argued above, there areconstruct
validityissues; it is not always c lear how to translate a ﬁn ding
relating to an aggregated metric into a concrete act ion that can
be applied to a disaggreg ated product, process, or team. There
arealsointernalvalidityissues,aswediscusseda bove;factors
that inﬂuence aggregation , such as intentional or un intentional
assortativity(inourcase studies)canconfoundthe results,and
threateninternalvalidity whenecologicalinferenc esaremade.
Of course, one must alw ays be mindful while ag gregating of
the loss of statistical pow er due to reduction in sa mple size.
In conclusion, we hope to have convinced the re ader that
a) ecological inference i s often unavoidable in s oftware en-
gineering research, and b) that managing the re sulting risks
of ecological fallacy, is a ripe area for study in our ﬁeld.
We hope others will join us to explore these issue s in other
settings and other datase ts, investigate speciﬁcally the effects
ofsample size,zonationandclass imbalanceonEIrisk, and
also considerEIrisk in their future work .
REFERENCES
[1] D. L. Parnas, “On th e criteria to be used in d ecomposing systems into
modules,”Commun. ACM, vol. 15, no. 12, 72.
[2] M. Cataldo, J. D. H erbsleb, and K. M. Carl ey, “Socio-technical con -
gruence: a framework fo r assessing the impact o f technical and work
dependencies on softwa re development product ivity,” inESEM ’08:
Proceedings of the Sec ond ACM-IEEE interna tional symposium on
Empirical software engi neering and measureme nt. New York, NY,
USA: ACM, 2008, pp. 2 –11.
[3] M. Conway, “How d o committees invent,”Datamation, vol. 14, no. 4,
pp. 28–31, 1968.
[4] J. D. Herbsleb and R. E. Grinter, “Splittin g the Organization and
Integrating the Code: Co nway’s Law Revisited,” inICSE, 1999.
[5] E. J. Weyuker, T. J. Ostrand, and R. M. Bell, “Do too many cooks spo il
the broth? using the num ber of developers to enha nce defect prediction
models,”Empirical Softw. Engg., vol. 13, no. 5, pp. 539– 559, 2008.
[6] A.Meneely,L.Willi ams,W.Snipes,andJ.Os borne,“Predictingfailure s
with developer networks and social network analy sis,” inProceedings
of the 16th ACM SIGSOF T International Symposiu m on Foundations of
software engineering. ACM, 2008.
[7] A. Koru and H. Liu , “Building effective de fect-prediction models i n
practice,”IEEE Software, vol. 22, no. 6, pp. 23–2 9, 2005.
[8] N. Nagappan and T . Ball, “Use of relative code churn measures to
predict system defect den sity,” inProceedings of the 27th international
conference on Software e ngineering. ACM, 2005, p. 292.
[9] A.Schr ¨oter,T.Zimmermann,an dA.Zeller,“Predictingc omponentfail-
uresatdesigntime,”inProceedingsofthe2006 ACM/IEEEinternational
symposium on Empirical software engineering. ACM, 2006, p. 27.
[10] T. Zimmermann, R . Premraj, and A. Zelle r, “Predicting defects fo r
eclipse,” inPROMISE, 2007.
[11] W. Robinson, “Eco logical correlations and t he behavior of individual s,”
Internationaljournalofe pidemiology,vol.15,no.3,pp.351–3 57,1950.
[12] P. Openshaw, S.an d Taylor, “A million or so correlation coefﬁcien ts:
three experiments on th e modiﬁable areal unit problem,”Statistical
Methods in the Spatial S ciences, pp. 127–144, 1979.
[13] E.Arisholm,L.Bri and,andE.Johannessen, “Asystematicandcompr e-
hensive investigation of methods to build and ev aluate fault prediction
models,”Journal of Systems and Software, vol. 83, no. 1, pp. 2– 17,
2010.
[14] T. Menzies, Z. Mi lton, B. Turhan, B. Cuk ic, Y. Jiang, and A. Ben er,
“Defect prediction from static code features: curre nt results, limitations,
new approaches,”Automated Software Eng ineering, pp. 1–33.
[15] D. Freedman, “Eco logical inference and the ecological fallacy,”Inter-
national encyclopedia of the social and behaviora l sciences, pp. 4027–
4030, 2004.[16] S.Piantadosi,D.By ar,andS.Green,“Theec ologicalfallacy,”American
Journal of Epidemiology, vol. 127, no. 5, p. 893, 1988.
[17] S. Schwartz, “The f allacy of the ecological f allacy: the potential misu se
of a concept and the con sequences.”American journal of pub lic health,
vol. 84, no. 5, p. 819, 19 94.
[18] L. Briand, K. Emam , and S. Morasca, “On th e application of measure -
ment theory in software engineering,”Empirical Software Eng ineering,
vol. 1, no. 1, pp. 61–88, 1996.
[19] V. R. Basili, L. C. Briand, and W. L. Melo , “A validation of objec t-
oriented design metrics a s quality indicators,”IEEE Trans. Softw. Eng .,
vol. 22, no. 10, 1996.
[20] N.Nagappan,B.M urphy,andV.Basili,“The inﬂuenceoforganizationa l
structure on software qu ality: an empirical case study,” inICSE 2008,
2008.
[21] C. Baldwin and K. Clark,Design Rules: Vol 1. MIT Press, 2000.
[22] C. Bird, N. Nagap pan, P. Devanbu, H. Ga ll, and B. Murphy, “Do es
distributeddevelopmenta ffectsoftwarequality?:an empiricalcasestudy
of Windows Vista,”CACM, vol. 52, no. 8, pp. 85–9 3, 2009.
[23] N. Ramasubbu and R. Balan, “Globally dist ributed software develop -
mentprojectperformance :anempiricalanalysis,”i nESEC/FSE. ACM,
2007.
[24] D. Spinellis, “Glob al software development in the FreeBSD project, ”
inProceedings of the 2006 international workshop o n Global software
development for the prac titioner. ACM, 2006, p. 79.
[25] R. Ramler, K. Wo lfmaier, E. Stauder, F. K ossak, and T. Natschl ¨ager,
“Key questions in buil ding defect prediction models in practice,”
Product-Focused Softwar e Process Improvement, pp. 14–27, 2009.
[26] N. Nagappan, T. Ball, and A. Zeller, “M ining metrics to predic t
component failures,” inICSE, 2006.
[27] T. Zimmermann, N . Nagappan, and A. Zel ler, “Predicting Bugs fro m
History,”Software Evolution, pp. 69–88, 2008.
[28] M. D’Ambros, M. Lanza, and R. Robbes, “A n extensive comparison o f
bug prediction approache s,” inMSR, 2010.
[29] C. Drummond and R. Holte, “C4. 5, class im balance, and cost sensitiv -
ity:whyunder-sampling beatsover-sampling,”inWorkshoponLearning
from Imbalanced Datase ts II. Citeseer, 2003.
[30] J. Gu, Y. Zhou, a nd X. Zuo, “Making cl ass bias useful: A strate gy
of learning from imbala nced data,”Intelligent Data Engine ering and
Automated Learning-IDE AL 2007, pp. 287–295, 2007.
[31] T. Menzies, Z. M ilton, A. Bener, B. Cuk ic, G. Gay, Y. Jiang, a nd
B.Turhan,“Overcoming CeilingEffectsinDefect Prediction,”Submitted
to IEEE TSE, 2008.
[32] Y.MaandB.Cukic ,“Adequateandprecisee valuationofqualitymode ls
insoftwareengineerings tudies,”inInternationalWorkshop onPredictor
Models in Software Engi neering, 2007. PROMISE ’07: ICSE Workshops
2007, 2007, pp. 1–1.
[33] S. Lessmann, B. B aesens, C. Mues, and S . Pietsch, “Benchmarkin g
classiﬁcation models for software defect predictio n: A proposed frame-
work and novel ﬁndings, ”IEEE Transactions on S oftware Engineering,
vol. 34, no. 4, p. 485, 20 08.
[34] E. Arisholm, L. B riand, and M. Fuglerud , “Data mining techniqu es
for building fault-pronen ess models in telecom ja va software,” inPro-
ceedings of the The 18t h IEEE International Sy mposium on Software
Reliability. IEEE Computer Socie ty, 2007, pp. 215–224.
[35] D. Wheeler, “SLO CCount,”Available fro m http: //www. dwheeler.
com/sloccount.
[36] G. Boetticher, T. M enzies, and T. Ostrand, “PROMISE Repository o f
empirical software engin eering data,”West Virginia University , Depart-
ment of Computer Scienc e, 2007.
[37] H. Zhang and H. B . K. Tan, “An empirical s tudy of class sizes for lar ge
java systems,” inSoftware Engineering Co nference, 2007. APSEC 2 007.
14th Asia-Paciﬁc, dec. 2007, pp. 230 –23 7.
[38] J. Cohen,Applied multiple regres sion/correlation analysis for the be-
havioral sciences. Lawrence Erlbaum, 2 003.
[39] P. Peduzzi, J. Co ncato, E. Kemper, T. H olford, and A. Feinstei n,
“A simulation study of the number of events pe r variable in logistic
regressionanalysis*1,”Journalofclinicalepide miology,vol.49,no.12,
pp. 1373–1379, 1996.
[40] D. Perry, A. Port er, and L. Votta, “Emp irical studies of softwa re
engineering:aroadmap,” inProceedingsoftheconfe renceonThefuture
of Software engineering. ACM New York, NY, USA, 2000, pp. 345–
355.371