Instant Code Clone Search
Mu-Woong Lee Jong-Won Roh Seung-won Hwang Sunghun Kim
Pohang University of Science and Technology (POSTECH) Hong Kong University of
Republic of Korea Science and Technology (HKUST)
{sigliel, nbanoh, swhwang}@postech.edu hunkim@cse.ust.hk
ABSTRACT
In thispaper, wepropose ascalable instantcodeclone search
engine for large-scale software repositories. While there arecommercial code search engines available, they treat soft-ware as text and often fail to ﬁnd semantically related code.
Meanwhile, existing tools for semantic code clone searches
take a “post-mortem” approach involving the detection of
clones“after”the code developmentis completed, and hence,fail toreturn the resultsinstantly. Inclear contrast, we com-bine the strength of these two lines of existing research, by
supporting instant code clone detection. To achieve this
goal, we propose scalable indexing structures on vector ab-stractions of code. Our proposed algorithms allow develop-ers to detect clones of a given code segment among the 1.7million code segments from 492 open source projects in sub-
second response times, without compromising the accuracy
obtained by a state-of-the-art tool.
Categories and Subject Descriptors
D.2.7[Software Engineering ]: Distribution,Maintenance,
and Enhancement— Restructuring, reverse engineering, and
reengineering
General Terms
Design, Management
Keywords
Clone detection, code search
1. INTRODUCTION
Clone detection helps software development and mainte-
nance tasks, as unmanaged code clones make program main-
tenancediﬃcultandmaycause inconsistent clone changes [13,
18]. Therefore, clone detection research has been an active
area for decades, and many practical techniques have beenproposed and widely used [2, 11, 15, 24, 26, 29].
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted w ithout fee provided that copies are
not made or distributed for proﬁt or c ommercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, torepublish, to post on servers or to redist ribute to lists, requires prior speciﬁc
permission and/or a fee.FSE-18, November 7–11, 2010, Santa Fe, New Mexico, USA.
Copyright 2010 ACM 978-1- 60558-791-2/10/ 11 ...$10.00.Existing code clone detection tools usually take a post-
mortem approach by detecting the clones “after” code de-
velopment is completed, as they mainly focus on the code
refactoring scenario. In such scenario, developers, for ex-
ample, may run a code clone detector once per month, and
based on the gathered clone information, they perform any
necessary maintenance work such as refactoring or ﬁxing
inconsistent clone changes. In clear contrast, we focus on
developing a preventive way of ﬁnding clones during devel-
opment process, by enabling an instant and scalable search
for the clones of a given code segment. This type of instant
clone detection encourages to refer to well-tested existing
code, rather than reinvents code clones.
In this paper, we propose an instant clone search engine
that is scalable to the size of code repositories, e.g.,d e t e c t -
ing the clones of a given code segment from 492 open source
projects (54 million LOC, 1.7 million code segments) in a
sub-second response time. Our detector considers structural
similarities between code segments, as in a post-mortemclone detection tool, Deckard [11]. Our key technical con-
tribution is balancing the dual goals of instant response andresult quality, by exploiting a multidimensional indexingstructure, R∗tree[3], and proposing dimensionality reduc-
tion and I/O optimization techniques.
This type of instant detector would enable many interest-
ing applications. For example, when developers work on one
section of a very large project, instant clone search wouldallow them to easily ﬁnd and reference other similar codepieces. Conversely, commercial code search engines, such as
Koders
1or Google code search2, may fail to suggest related
code, because they treat software as text [16, 17]. Using a
post-mortem clone detector is simply overkill for their pur-pose, because post-mortem detectors usually focus on ﬁnd-ing all of the clone pairs, and it involves an unavoidable and
expensive computational cost. In summary, instant clone
search is a helpful approach to support rapid and evolving
software development.
We summarize our key contributions as follows:
⊿We address the problem of how to support instant clonesearch.
⊿Wedevelopcloneindexingtechniquestoachievesub-second
responsetimesfor large-scale real-life software repositories
without compromising clone search accuracy.
⊿For applications where the loss of some accuracy is accept-able, we also propose an approximation scheme, achievinga further speedup by trading oﬀ some accuracy.
1http://koders.com/
2http://www.google.com/codesearch/
167
The restof thepaperis organized as follows. Section 2 dis-
cusses the preliminaries, based on which Section 3 proposes
our algorithms. Section 4 reports our evaluation results.
Section 5 surveys related work, and Section 6 concludes thispaper.
2. PRELIMINARIES
This section discusses the preliminaries in code clone de-
tection (Section 2.1) and multidimensional indexing (Sec-tion 2.2). Building on thesepreliminaries, we formally deﬁneour problem in Section 2.3.
2.1 Code Clone Detection
There have been many code clone detection tools pro-
posed recently, including some tree-based techniques, whichabstract code segments as their corresponding parse trees orabstract syntax tree (AST) [2, 29]. Building on this abstrac-tion, clone detection is essentially tree similarity matching,
which is known to be inherently expensive [32], e.g.,u s i n g
tree edit distance as a similarity notion.
To overcome this inherent complexity, Deckard [11] ap-
proximated such similarity notion by representing an AST
as multi-resolution numerical vectors, known as character-
istic vectors . In other words, each tree node is represented
as a vector representing the frequency of the syntactic ele-ments in the code segment represented by its subtree. Withthis representation, an expensive tree match can be approx-imated as inexpensive vector matches. We adopt such ab-straction as proposed in [11], since its simplicity makes it
possible to develop sophisticated techniquesto improvescal-
ability.
2.2 Multidimensional Indexing
Characteristic vectors representing a code are often high-
dimensional, e.g.,therearemorethantwohundredsdiﬀerent
syntactic element types in JavaASTs. It is thus non-trivial
to ﬁnd matching vectors eﬃciently. To achieve sub-second
response times, we index the code repository using a mul-
tidimensional indexing structure, i.e.,a n R∗tree[3], which
have been widely adopted in database literature.
2.2.1 Naive Adoption
Intuitively,characteristic vectors can be mappedinto mul-
tidimensionalpoints,whichcanthenbeindexedusingamul-
tidimensional index structure such as an R∗tree.Aq u e r y ,
also represented as a vector, corresponds to another point,
such that ﬁnding clones corresponds to ﬁndingthe k-nearest
neighbors ( kNNs) of the query point, which has been ac-
tively studied in the database community [8, 6].
AnR∗treeis a height-balanced tree data structure, where
each node contains a variable number of entries, up to some
pre-deﬁnedmaximum, i.e.,node capacity . Fornon-leafnodes,
each entry contains two pieces of data: a pointer to a childnode, and the minimum bounding rectangle (MBR) of all
entries within this child node. For leaf nodes, each entry
has a pointer to a raw record stored on disks, and the MBR
of this raw record.
To brieﬂy illustrate how this structure can be used to
support a kNN query, Figure 1 shows an example of an
R∗treefor 2-dimensional data points. To ﬁnd kNNs of the
query point q,t h i s R∗treecan be traversed in a best-ﬁrst
search manner, by initially storing the root node’s entries
in the queue and iteratively retrieving the closest entry inxy
R1R2
R3R4R5R6
R7R8R9
R10R11
R12p1p2p3p4
p5p6p7
p8p9p10p11
p12p13p14
q
p15
p16p17p18
p19
p20p21
R1R2R3
R4R5R6 R7R8R9 R10R11R12
p19p20p21 p18 p17 p16 p15 p14 p13 p12 p11 p10p9p8 p7p6 p5p4 p3p2p1
Figure 1: A kNN query on an R∗treefor 2-
dimensional points
the queue (with respect to Euclidean distance) and enqueu-
ing its child node’s entries. This type of search can termi-nate when the top kclosest entries are all pointing raw data
points, e.g.,p
5andp8whenk=2 .
2.2.2 Adoption with Dimensionality Reduction
Though adopting an index discussed above enables to ﬁnd
kNN with a tree traversal and avoid a sequential scan, such
traversal can be more costly than a scan, when the dimen-
sionality is high. This problem is known as the dimensional-
ity curse problem [19]. To avoid this problem, dimensional-
ity reduction techniques are typically used [20, 27], to select
only a few important features and reduce the overall dimen-
sionality.
However, in a dimension-reduced space, a kNN search re-
sult may diﬀer from that in the original space. To illustratethis new challenge with example data in Figure 1, supposewe simply reduce the dimensionality of the dataset to one,by projecting points onto the x-axis. The closest point in
this reduced space is then p
8(i.e., false positive), while the
actual closest point in the original space is p5.
Due to this challenge, in order to get the knearest results
in the original space from the reduced space, we ﬁrst need to
identifyk/primecandidates that are guaranteed to contain all the
correctkresults (k/prime≥k). For instance, in our projection
example, retrieving k= 1 nearest point in the reduced space
(i.e., based on the projection on x-axis) will retrieve false
positive p8, but retrieving k/prime≥2 is guaranteed to include
the correct answer p5.T og u a r a n t e et h a t k/primecandidates do
not exclude any correct result, i.e., no false negatives, the
following lower-bounding property should hold.
Definition 1 (Lower-bounding property). Given a
dimensionality reduction function F()and two data vectors
v1andv2, distance function df()in the reduced space and
distance function do()in the original space should satisfy:
df(F(v1),F(v2))≤do(v1,v2)
If the dimensionality reduction function F() satisﬁes Def-
inition 1, we can obtain the k/primecandidates as follows: First,
we ﬁnd the kNNs ofF(q) on the reduced space. We then
sort these kNNs by their“real”distances to qon the original
space, and choose the kthnearestvector vkfrom these kNNs.
Finally, we identify all points vsatisfying df(F(q),F(v))≤
do(q,v k). This can be done through a simple ε-range search
168on the reduced space, with ε=do(q,v k), and these candi-
dates are guaranteed to have all top- kresults, as formally
proved in [20]. Wethen rank these candidatesby their“real”
distances to ﬁnd the correct knearest results.
2.3 Problem Deﬁnition
This section formally deﬁnes the top- kcode clone search
problem. We deﬁne the code segments and the characteris-
tic vectors in Deﬁnition 2 and 3. Deﬁnition 4 then deﬁnes
the distances between vectors. Based on Deﬁnition 4, Deﬁ-nition 5 deﬁnes top- kcode clones of a query code segment.
Definition 2 (Code segments). Given a code S,i t s
ASTT, and a threshold minT ,i fas u b t r e e T
iofTcontains
at least minT nodes, then Ti’s corresponding part in Sis a
code segment.
Definition 3 (Characteristic vectors). Given a code
segment Siand the AST TiofSi, the characteristic vector
vi=/angbracketleftci(1),ci(2),···,ci(d)/angbracketrightofSiconsists of occurrence coun-
tersci(j)of syntactic elements in Ti.
Definition 4 (Distanc es between vectors). Given
twod-dimensional vectors v1andv2, the distance /bardblv1,v2/bardbl
between v1andv2is theirL2-norm,
/bardblv1,v2/bardbl=/radicalBig
/summationtextdi=1(c1(i)−c2(i))2.
Definition 5 (Top- kcode clones). Given a set Vof
characteristic vectors, a query vector q, and the retrieval size
k, top-kclonesTC k(q)⊂V is a set of vectors TC k(q)=
{v1,v2,···,vm},w h e r e viis theithclosest vector from q,
m≥k,a n d/bardblq,v i/bardbl=/bardblq,v k/bardblfor∀isatisfying k<i≤m.
At o p -kcode clone search query qretrieves a set TC k(q),
andTC kis used as its shorthand. For notational simplicity,
we useTC kto represent both code clones and their corre-
sponding vectors interchangeably.
3. INSTANT CODE CLONE DETECTION
This section proposes indexing structures and algorithms
to solve the top- kcode clone search problem. As a base-
line, Section 3.1 discusses a sequential scan algorithm, Scan.
Section 3.2 then proposes a sub-linear algorithm, FrTCD,u s -
ing an R∗treeindex with dimensionality reduction. Though
FrTCDdemonstrates reasonable scalability in medium-scale
datasets, it still incurs prohibitive I/O costs. We thus study
I/O optimization techniques to further improve the overall
performance and build an enhanced algorithm, InTCD,u p o n
the optimized R∗trees in Section 3.3. Lastly, Section 3.4 dis-
cusses how to further boost performance for scenarios where
compromising some accuracy can be tolerated.
3.1 Baseline: Sequential Scan
One naive solution to ﬁnd the clones of a given query
code segment would be adopting an existing clone detector,
identifying “clusters” of clones as their results. From these
results, we can identify the cluster to which the given query
code belongs and consider other codes in the same cluster
as its clones. However, considering we only need one suchcluster, ﬁnding all clu sters is an overkill.
Alternative solution is to adopt Locality Sensitive Hash-
ing(LSH) [7], as used by Deckard [11] to eﬃciently ﬁnd
nearneighbors(similar vectors)of eachcharacteristic vector.However, LSH is not an exact nearest-neighbor algorithm,
as we will empirically show later in Section 4.4.
We thus adopt a straightforward baseline approach for
the exact computation, using a sequential scan, called Scan,
which simply reads the entire repository sequentially and
updates TC
k, as Algorithm 1 illustrates.
Algorithm 1 :Scan(q,k)
Input :q u e r yv e c t o r q,r e t r i e v a ls i z e k
Output :s e tTC kof vectors of top- kclones
initialize TC k←{ } 1
foreachv∈Vdo 2
UpdateClones (TC k,k,q,v); 3
returnTC k 4
Speciﬁcally, Scansequentiallytestseachcharacteristicvec-
torv∈V.F o re a c h v,Scantests that vis not farther than
a n yv e c t o ri nt h ec u r r e n t l yk n o w nt o p - klistTC k.I fvis
not farther, Scanupdates the list TC k, as Algorithm 2 illus-
trates.
Algorithm 2 :UpdateClones (TC k,k,q,v)
Input :s e tTC k,r e t r i e v a ls i z e k,q u e r yq,v e c t o rv
/*tci∈TC kdenotes the ithnearest vector in
TC k, fromq */
if|T C k|<kthenTC k←TC k∪{v} 1
else if|T C k|≥kand/bardblq,v/bardbl≤/bardblq,tc k/bardblthen 2
TC k←TC k∪{v} 3
remove∀tci∈TC kfarther than tckfromq 4
3.2 Filtering-then-Ranking Clone Detection
This section proposes a ﬁltering-then-ranking top-kcode
clone search algorithm, called FrTCD,u s i n ga n R∗treeindex.
However, naively adopting this type of index structure in-
curs undesirablehigher cost compared to a simple sequential
scan, as discussed in Section 2.2.
To overcome this challenge, we ﬁrst discuss a dimension-
ality reduction technique in Section 3.2.1. We then discuss
how to build an index on this reduced space (Section 3.2.2)a n dt h e ne x e c u t et o p - kclone queries (Section 3.2.3).
3.2.1 Dimensionality Reduction
For a given set VofD-dimensional Ncharacteristic vec-
tors{v1,v2,···,vN}, our goal in dimensionality reduction is
to generate lower-dimensional vectors V/prime={v/prime
1,v/prime
2,···,v/prime
N},
whichsatisfy the lower-bounding property (Deﬁnition1),and
make our algorithms eﬃcient.
As discussed in Section 2, it is important to preserve the
lower-bounding property to ensure that we can retrieve can-
didates including all of the correct kresults, by searching
the reduced space only. Formally, for all viandvj∈V,
and their corresponding reduced vectors v/prime
iandv/prime
j, the dis-
tances measured in the original space and the reduced space
should satisfy /bardblv/prime
i,v/prime
j/bardbl≤/bardblvi,vj/bardbl. We can trivially show
that selecting any D/prime-dimensional subspace of the original
D-dimensional space ensures the lower-bounding property.
However, not all such subspaces are equally eﬀective. A
desirable subspace should reﬂect the original distances be-
tween vectors, or more formally, minimize the sum Δ of
169diﬀerences δi,j,
Δ=/summationdisplay
∀i,∀j,i/negationslash=jδi,j=/summationdisplay
∀i,∀j,i/negationslash=j/bardblvi,vj/bardbl−/bardblv/prime
i,v/prime
j/bardbl,
between two distances measured at the original space and
the subspace respectively. Finding such subspace is knownto be NP-hard [23], which motivates us to develop its ap-proximation schemes.
A straightforward approximation would be a greedy strat-
egy that iteratively picks the dimension that reduces Δ the
most. However, this strategy requires recomputing Δ forall remaining dimensions, at each iteration. To avoid suchrecomputations, we propose to compute the variances of all
dimensions once and select the D
/primedimensions with the high-
est variances.
To demonstrate that this variance-based approach is not
only more eﬃcient, but also as eﬀective as the greedy strat-
egy discussed above, we brieﬂy compare 10-dimensional sub-
spaces selected from 261-dimensional characteristic vectors
obtained from real-life javasource codes (7,195 ﬁles) in Ta-
ble 1. The results in the table indicate that both approaches
produce nearly identical results, while the variance-basedmethod incurs a signiﬁcantly lower cost.
Table 1: Top 10 selected dimensions
Variance-based Greedy strategy
1 identiﬁer identiﬁer
2I D
 TK ID
 TK
3 unary
 expression unary
 expression
4 multiplicative
 expression multiplicative
 expression
5 additive
 expression additive
 expression
6 relational
 expression shift
 expression
7 shift
 expression relational
 expression
8 equality
 expression equality
 expression
9 conditional
 expression conditional
 expression
10 assignment
 expression assignment
 expression
3.2.2 Index Building
Wenowdiscusshowwecanbuildan R∗treeinthedimension-
reduced space. A naive way to create an index is to insert
one vector at a time, which incurs an expensive update on
the index tree per each vector insertion. In contrast, a bulk
loadingapproach amortizes the updatecost, by inserting the
entire dataset at once [5, 14, 21].
Existing bottom-up bulk loading algorithms ﬁrst parti-
tion the entire dataset and build each partition as a leaf
node. Then to build non-leaf nodes, they iteratively applythesamepartitioningprocesstotheresultingnodes, untilwehave only one partition including the entire dataset, which
corresponds to the root node of the R∗tree. Bulk loading
tightly packs the index structure to enable fast lookups, and
it is reported to boost the buildingperformance by hundred-folds [4].
Inparticular, wereviseastate-of-the-artbottom-up R∗tree
bulk loading algorithm, STR [21], to apply to our problem.
STR partitions the given dataset into MBRs, by recursively
subdividing each dimension into the same number of slices.Straightforwardly adoptingthispartitioningpolicyisnotde-sirable for the dimension-reduced characteristic vectors, asthe variance diﬀers signiﬁcantly over dimensions. That is, inone dimension, points are highly clustered in a small range,
while in another, points are well scattered. For such data,partitioning each dimension into the same number of sliceswould render non-square rectangles (with one side signiﬁ-
cantly larger than the other), which incurs higher I/O costthan squared blocks, for the L
2distance function used in
our work.
xy
xy
(a) STR packing (b) Our packing
Figure 2: STR partitions the data into 9 MBRs,
while our packing algorithm partitions the data into8 MBRs. Their utilizations are 83.3% and 93.75%
respectively.
Toillustrate, consider30pointsinatwo-dimensionalspace,
where the node capacity Cof the tree is 4, i.e.,e a c ht r e e
node may hold at most 4 entries, and the xvalues are much
more scattered than yvalues (Figure 2). At thebottom level
of the tree, this dataset should be partitioned into ⌈
30
4⌉=8
or more MBRs. To do this, STR partitions the space by di-
viding each dimension into an equal number of slices, i.e.,3
slices containing the same number of data points, to render
9 MBRs (Figure 2(a)). In contrast, we divide the datasetinto 8 MBRs (2-by-4 as illustrated in Figure 2(b)), to sig-
niﬁcantly enhance the node utilization of STR, i.e., 83.3%
(
30
9×4) into 93.75% (30
8×4).
Formally, fora D-dimensionaldatasetcontaining Npoints,
wesubdividethe ithdimensioninto si=⌈ri/R⌉slices, where
riis the value range, computed as the diﬀerence between
the maximum and minimum values of the ithdimension. In
other words, a dimension with a high riis highly scattered.
Assuming points are uniformly scattered, Rcan be com-
puted as/producttextD
i=1ri
R=N
C. Algorithm 3 formally describes the
data partitioning process.
Algorithm 3 :DataPartitioning (E,C,rrr)
Input :e n t r i e s E={e1,···,en}, node capacity C,
value ranges rrr=/angbracketleftr1,···,rd/angbracketright
Output : partitioned entries E
R←D/radicalBig
C
N·/producttextD
i=1ri;si←⌈ri/R⌉,f o r∀i∈{1,···,D} 1
repeat2
foreachsido 3
si←si−1ifit does not incur an overﬂow 4
until any decrease incurs an overﬂow 5
Slice(E,1,/angbracketlefts1,···,sd/angbracketright) 6
Foragivensetofentries Eanditsvalueranges /angbracketleftr1,···,rd/angbracketright,
Algorithm 3 ﬁrst computes Rand/angbracketlefts1,···,sd/angbracketright(Line 1).
Then Algorithm 3 tries to decrease sivalues (Lines 2-5),
to further enhance the node utilization. In our observation,
assitakes ceiling and thus overestimates the right number
of subdivisions in each dimension, the total number of parti-
tions,/producttextDi=1si, may become too large, i.e., incurring the low
node utilization. To address this problem, Algorithm 3 de-creases each s
iby one, until any decrease incurs an overﬂow,
i.e., with node utilization higher than 100%.
170Algorithm 4 :Slice(E,i,sss)
Input :e n t r i e s E, dimension index i,sss=/angbracketlefts1,···,sd/angbracketright
sortEaccording to the ithdimension 1
subdivide Einto{E/prime
1,···,E/prime
si} 2
ifi<Dthen 3
foreachE/prime
idoSlice(E/prime
i,i+1,/angbracketlefts1,···,sd/angbracketright) 4
Once the “tightest” sivalues are determined, we recur-
sively subdivide each dimension as described in Algorithm 4.
Algorithm 4 ﬁrst sorts the set of entries Ein the ascending
order of the ithdimension, then divides Eintosisubdivi-
sions, namely E1,···,a n dEsi. We make sure the ﬁrst si−1
subdivisions to contain ⌈|E|/si⌉where|E|denotes the num-
ber of entries in E.Esicontains the remaining entries.
To build an R∗tree, we ﬁrst partition the reduced vectors
using Algorithm 3. The resulting partitions become leaf
nodes. Then we perform Algorithm 3 again on the MBRs
of the leaf nodes. In this case, we sort them using their
centers as representative points of the MBRs. We repeatthis partitioning process until we have only one partition,which corresponds to the root of the tree.
3.2.3 Two-phase Query Processing
Thissectionproposestheﬁltering-then-rankingtop- kcode
clone search algorithm, FrTCD, to evaluate top- kcode clone
queries. Basically, FrTCDworks in two phases of ﬁlteringand
ranking as Algorithm 5 formally states.
Algorithm 5 :FrTCD(q,k,T)
Input :q u e r yv e c t o r q, retrieval size k,R∗treeT
Output :s e tTC kof vectors of top- kclones
q/prime←the reduced vector of q 1
N←theknearest neighbors of q/prime; /* from T*/ 2
v/prime
k←thekthnearest vector in N 3
/* by the distances on the original space */
ε←/bardblq,v k/bardbl;C←{v/prime:/bardblq/prime,v/prime/bardbl≤ε}; /* from T*/ 4
initialize TC k←{ } 5
foreachvcorresponding to v/prime∈Cdo 6
UpdateClones (TC k,k,q,v); 7
returnTC k 8
In the ﬁltering phase (Lines 1-4), for a given query vector
q,FrTCDidentiﬁes candidate vectors C, using a combination
ofkNN and range search on the reduced space. More pre-
cisely, FrTCDﬁnds the kNNs,N,o fq/prime, by traversing the
R∗treein a best-ﬁrst search manner (Line 2), and computes
their distances to qin the original space to choose the kth
nearest vector v/prime
kamongN(Line 3). /bardblq,v k/bardbldenotes the
actual distance between the given query qand the charac-
teristic vector vk∈Vcorresponding to v/prime
k.
FrTCDthen performs an ε-range search, where ε=/bardblq,v k/bardbl,
to ﬁnd all the reduced vectors Cwithin the distance /bardblq,v k/bardbl
fromq/prime(Line 4). As /bardblq,v k/bardbl≥/bardblq/prime,v/prime
k/bardbl,Cis guaranteed
to contain correct kresults. We can thus prune out the
remaining vectors V/prime\C, as they are farther away from q,
compared to vk.
In the ranking phase (Lines 5-7), FrTCDcomputes the dis-
tance of each characteristic vector vcorresponding to v/prime∈C,
fromq. Observe that, unlike Scanthat computes the dis-tance for all objects, FrTCDcomputes the distance of a very
small subset Cof raw data records ( N⊂C),i.e.,a sub-linear
algorithm. After FrTCDcomputes the distances of these can-
didates to q, we can ﬁnalize the list of top- kcode clones.
3.3 Interleaved Clone Detection
Thoughweempirically observethat FrTCDalready achieves
an acceptable eﬃciency for medium-scale datasets, as wewill later show with our extensive evaluation results, we can
also observe that FrTCDhas room for further improvements,
as much I/O costs are wasted on random accesses on data
records– According to our experimental results, 80.5% ofthe overall response time of FrTCD(3.4 seconds to ﬁnd the
top-20 code clones in the repository of 1.7 million vectors)corresponds to the random access cost.
Based on this observation, we study two I/O optimization
techniques, (1) reducing the number of random accesses and(2) reducing the cost of each access.
3.3.1 Vector Packing
We ﬁrst study how we reduce the number of random ac-
cesses by “packing” a group of records to be accessed to-
gether, i.e., those with similar values. This type of packing
allows us to reach multiple records with a single random ac-
cess followed by cheaper sorted accesses, which incurs a sig-
niﬁcantly lower cost than performing a random access per
each record. For one-dimensional data, this packing can beimplemented straightforwardly, by storing raw data records
in the same order as the index key. However, for multi-
dimensional data, it is non-trivial to identify an eﬀectiveone-dimensional sorted order to store records.
Figure 3 illustrates a scenario, reading 20 data records
withinεfromqrequires 20 accesses. However, if these points
are packed into two blocks, B
1andB2,w eo n l yn e e dt w o
random accesses to get to the beginnings of the blocks, and
the remaining records can be retrieved by cheaper sequential
accesses. While this strategy may incur the overhead of
retrieving few more false positive records, the overall I/Ocost is greatly reduced, as we also empirically validate inSection 4.
q
B1B2ε
Figure 3: A range query example
For this packing, we apply the same scheme proposed for
bulk loading in Section 3.2.2, and store these blocks in sev-eral ﬁles. After the packing is done, we simply build anR∗treeon the MBRs of the blocks. Each MBR is computed
in thereduced D
/prime-dimensionalsubspace, and thedata blocks
contain the original D-dimensional vectors. As we use the
same scheme in Section 3.2.2 for this packing, data inser-
tion and deletions can be handled in a similar way that the
R∗trees do without the packing.
3.3.2 Single-phase Query Processing
This section proposes InTCD, adopting the vector packing
scheme discussed in the previous section. Speciﬁcally, we
implement InTCDto (1) further reduce the number of ran-
171dom accesses to index nodes by interleaving the kNN search
and the range search in the ﬁltering phase of FrTCD,a n d( 2 )
reduce the cost of random accesses.
Interleaved Index Traversal :Recall that FrTCDperforms
a best-ﬁrst search to ﬁnd the kNNs in the reduced space,
then performs a range search to ﬁnd candidates. After this
candidate selection, FrTCDreads the raw records of the can-
didates to compute the real distances from the query. In
contrast, InTCDinterleaves these two steps, by concurrently
accessing raw records“during”the index traversal.
Basically, InTCDtraverses the index in the reduced space
in the same manner as FrTCD. However, during the traversal,
when a leaf entry is reached, InTCDaccesses the raw data
blockpointedbytheleafentry,withoutwaiting fortheindextraversal to complete, as the cost of reading few extra rawdata is much aﬀordable now with vector packing. Wheneverdata records are accessed from the leaf entry, InTCDupdates
a sorted list, TC
k, of the current known top- kclones, and
we denote the current kth-NN in the list as tck.
Our key observation is that tckcan be used as a pruning
boundary, as we can safely prune out both non-leaf and leaf
entries that are farther than tck. As more data records are
accessed, TC kconverges to the actual top- kresults. In this
interleaved traversal, each index node is accessed at most
once, which enables to outperform FrTCDaccessing some in-
dex nodes twice during the kNN and the range search re-
spectively.
Algorithm 6 formally describes this process of InTCD.
Algorithm 6 :InTCD(q,k,T)
Input :q u e r yv e c t o r q, retrieval size k,R∗treeT
Output :s e tTC kof vectors of top- kclones
/*tci∈TC kdenotes the ithnearest vector in
TC k, fromq */
q/prime←the reduced vector of q 1
TC k←{ };Q←{ };H←{entries within the root of T} 2
whileHis not empty do 3
e←H.pop() 4
if|T C k|<k ormindist (q/prime,e)≤/bardblq,tc k/bardblthen 5
ifeis not a leaf thenH.push(children of e) 6
else 7
Q.push(e) 8
if|Q|>Wthen 9
E←pop block pointers from Q 10
foreachv∈ab l o c ko f Edo 11
UpdateClones (TC k,k,q,v); 12
whileQis not empty do 13
E←pop block pointers from Q 14
foreachv∈ab l o c ko f Edo 15
UpdateClones (TC k,k,q,v); 16
returnTC k 17
Speciﬁcally, toimplementthissingle-scanbest-ﬁrstsearch,
am i nh e a p Hofeis maintained in the ascending order of
mindist (q/prime,e), where q/primedenotes the reduced vector of query
q,eis an entry of the R∗treeindex, and mindist (q/prime,e)d e -
notes the shortest distance between q/primeande(Figure 4).
At the beginning, the entries within the root of Tare
pushed into H(Line 2). Then iteratively, the entry ein
Hwith the minimal mindist (q/prime,e) is processed. If the
mindist (q/prime,e) is no farther than the distances of the cur-eq3d1
d2q1
q2
Figure 4: mindist (qi,e)in a 2-dimensional space.
mindist (q1,e)=d1,mindist (q2,e)=d2,a n d
mindist (q3,e)=0.
renttcktoq, we continue the iterations. Otherwise, we can
safely ignore e(Line 5).
Ifmindist (q/prime,e)≤/bardblq,tc k/bardbl,w et e s ti f eis a leaf entry or
not. Ifeis not a leaf, then the entries within its child node
are pushed into H(Line 6). Otherwise, we process the raw
data block pointed by e.
When processing raw data, a naive approach would be
reading each raw data block right away, which incurs high
random seek costs. Instead, to reduce the cost of random
accesses, we devise an eﬀective scheduling technique, called
delayed loading , by adopting the idea of Circular SCAN disk
scheduling [28].
Delayed Loading :Speciﬁcally, we propose a delayed load-
ing scheme, which delays the reading the block of euntil
we collect multiple blocks to read as the name itself sug-
gests. Figure 5 illustrates how this scheme works in a sce-
nario involving the reading of four blocks B1,B2,B3,a n d
B4. They are stored on the disk in a sorted order, and the
mindistsoftheircorrespondingentries e1,e2,e3,a n de4sat-
isfymindist (q/prime,e2)<mindist (q/prime,e4)<mindist (q/prime,e3)<
mindist (q/prime,e1).
B1 B2 B3 B4Without delayed loading
With delayed loadingSequantial access
Random access
Figure 5: Eﬀectiveness of delayed loading
A naive solution would be reading each block one at a
time, which incurs four expensive seeks, but instead, we can
read these four blocks at once, in the forward direction as inFigure 5, which incurs cheaper seeks followed by sequentialscan.
In Algorithm 6, we maintain a delayed loading queue Q,
which contains block pointers ein the ascending order of
mindist (q
/prime,e). Whenever we have a raw data block to
read, i.e.,w h e nal e a fe n t r y eis reached, we push it into
Q(Line 8). We then delay reading these blocks until we col-
lect a suﬃcient number of entries in Q, determined by the
given threshold W, which we set as 50 in our experiment
(Line 9). When this happens, InTCDthen reads these blocks
in batch (Lines 11-12). Once the traversal terminates, weprocess the remaining blocks in Q(Lines 13-16).
3.4 Approximate Clone Detection
For applications where some accuracy compromise can be
tolerated, approximation is a good strategy to trade accu-
r a c yf o ra ne v e nh i g h e rp e r f o r m a n c e . I nt h i ss e c t i o n ,w e
discuss how we study an approximation scheme, empirically
172achieving a 19 times speedup against InTCD, while compro-
mising no more than 30% of the accuracy.
Speciﬁcally, we propose an approximate top- kcode clone
search algorithm ApTCD, which eﬃciently identiﬁes approxi-
mateanswerswithoutreadingorprocessinghigh-dimensional
data records V. Instead, we read further dimensionality re-
duced records V/primeand issue a dimensionality-reduced query
q/prime.
Intuitively, as dimensionality of V/primeincreases, eﬃciency de-
creases but accuracy increases. To balance this trade-oﬀ,
we need to eﬀectively select the subspace V/prime. This goal is
similar to that of the dimensionality reduction we discussed
for exact algorithms (Section 3.2.1), our approach should
be diﬀerent, as the reduction this time is applied upon al-
ready dimensionality-reduced space. As a result, applyingthe same technology would be redundant and ineﬀective.
From the reduction results reported in Table 1, obtained
by variance-based ranking, we observed that features with
similar variancestendtobecorrelatedfrom oneanother. For
example, features identiﬁer and ID
TKwith the same vari-
ance were also perfectly correlated with each other, which
suggests that these two features can be reduced into one
feature without any loss of accuracy.
There have been many reduction techniques studied for
aggregating correlated features, such as Principal component
analysis (PCA) [12] and Piecewise aggregate approximation
(PAA) [31]. In this research, which aims at scalability, we
consider PAA with higher scalability.
Figure 6 illustrates how PAA reduces the dimensionality.
In the ﬁgure, elements in vare sorted in a descending order
based on the variances of their corresponding dimensions.
71 71 59 52 43 46 30 32 28 17 12 10 sorted vector
142 111 89 62 45 22
201 141 90 39
253 151 67ω=2
ω=3
ω=4low variance high variance
v:
Figure 6: PAA example
To illustrate, consider a scenario of reducing v∈Vinto
a 3-dimensional vector v/prime. Our variance-based dimension
reduction method for FrTCDand InTCDsimply chooses v/prime=
/angbracketleft71,71,59/angbracketright. PAA uses a ﬁxed size disjoint window ωto
dividev, then aggregates each wi ndow. Subsequently,if ω=
2,v/prime=/angbracketleft142,111,89/angbracketright. Similarly, if ω=3 ,v/prime=/angbracketleft201,141,90/angbracketright.
Once we reduce the dimensionality, to evaluate approxi-
mate top- kqueries, we simply build an R∗treeover these re-
ducedvectors V/prime, asshown inSection3.2.2. Wecanthenﬁnd
the approximate results by traversing the tree in a best-ﬁrst
manner (Algorithm 7), similarly to our exact algorithms.
As we later discuss in Section 4, our experimental results
indicate that applying PAA is eﬀective for achieving even
higher performances, e.g., 88 times speedup while compro-
mising 58% of accuracy against InTCDwhenD=1 2a n d
ω=4 .
4. EXPERIMENTAL EV ALUATION
Thissectionempirically evaluatesourproposedalgorithms.
First, we describe how we generate datasets and queries in
Section 4.1. Second, we evaluate the eﬃciency and scalabil-
ity of our algorithms in Section 4.2. Third, we validate the
eﬀectiveness of our approximate query processing scheme
Algorithm 7 :ApTCD(q,k,T)
Input :q u e r yv e c t o r q,r e t r i e v a ls i z e k,R∗treeT
Output :s e t/tildewiderTC kof approximate top- kclones
q/prime←the reduced vector of q 1
/tildewiderTC k←{ };H←{root ofT} 2
whileHis not empty do 3
e←H.pop() 4
ifmindist (q/prime,e)≤/bardblq/prime,/tildewidetck/bardblthen 5
ifeis not a leaf thenH.push(children of e) 6
else UpdateClones (/tildewiderTC k,k,q/prime,e); 7
return /tildewiderTC k 8
in Section 4.3. Lastly, we compare our proposed indexing
structure with Locality Sensitive Hashing (LSH) that is used
by a state-of-the-art post-mortem clone detector, Deckard
(Section4.4). All experimentswere carried outon a machine
with a Pentium IV 3.2GHz processor, with 1GB of memory,running Linux.
4.1 Experimental Setup
We generate characteristic vectors, in the same way de-
scribed in [11] for Deckard , from two real-life javacode
repositories. The ﬁrst repository contains 7,195 javaﬁles
from JDK 1.6.0 Update 13, consisting of 2,075,573 lines of
code total, and the second repository contains 288,846 java
ﬁles (54,709,384 lines) from 492 Java open source projects
hosted on SourceForge, Tigris.org and GoogleCode.
From these repositories, we generated six vector datasets,
two from the JDK code set (denoted as JDK 3,5), and four
fromtheopensourceprojectcodeset(denotedas OSP 3,5,7,9),
by varying the parameter minTofDeckard . The dimen-
sionality of each vector is 261. Table 2 summarizes the sizes
of the resulting vector datasets and the minTsetting. Once
these vector sets were generated, we randomly chose one
hundred vectors from each dataset to use as our queries.
4.2 Efﬁciency & Scalability
To evaluate the eﬃciency and scalability, this section re-
ports index building time and query execution time for vary-
ing retrieval sizes kand dataset sizes |V|.F o r t h i s s e t o f
experiments, we empirically chose 20 as the index dimen-
sionality. We increase kup to 80 to test scalability, though
we may use very small kin practice.
Table 2: Index building time for varying |V|
Dataset minT |V|Building time (s)
FrTCD InTCD
JDK 550 36,658 0.563 0.867
JDK 330 60,582 0.793 1.517
OSP 990 612,926 8.968 34.055
OSP 770 783,933 11.619 46.725
OSP 550 1,072,598 16.939 72.903
OSP 330 1,696,806 27.653 128.118
Table 2 summarizes index building time of InTCDand
FrTCD. This table only shows the building time, excluding
the data processing time for vector extraction and dimen-
sionality reduction. Observe from the table that InTCDtakes
173a relatively longer time than FrTCDto accomplish the vector
packing process, as vectors packed into blocks need to be
stored, causing extra I/Os. However, owing to this packing
scheme, InTCDperforms better than FrTCDin the later ex-
periments. Both the indexes for FrTCDand InTCDcan be
built in minutes, which is acceptable considering that index
creation is a (1) one-time and (2) oﬄine process.
The block sizes for these two datasets were tuned empiri-
cally. For JDKdatasets, the block size was set as 8KB and
each ﬁle contained at most 32 blocks, as the performancewas optimal with such setting. Similarly, for OSPdatasets,
we set the block size and ﬁle size as 384KB and 120 blocks
respectively.
0.037 0.06110−210−1
|V| (millions)Querying time (s)
  
Scan
FrTCD
InTCD
 |V||T C 20|(millions)
0.037 20.85
0.061 24.05
Figure 7: Querying time for varying |V|,JDK
datasets, k=2 0(log-scaled). The table lists the
average number of clones in TC 20.
Figure 7 shows the average querying time for varying |V|
using JDKdatasets, and thetableshows theaverage number
of vectors in TC k. (This number can be larger than kdue
to the ties in the results.) Both FrTCDand InTCDare at
least 7 times faster than Scan. Though in this medium-
scale dataset, the performance gain of InTCDover FrTCDis
less signiﬁcant, this type of performance gap signiﬁcantly
increases as the scale of the data increases, as we later showwith the larger datasets in Figure 8.
0.613 0.784 1.073 1.69710−1100101
|V| (millions)Querying time (s)
  
Scan
FrTCD
InTCD
|V||T C 20|(millions)
0.613 27.26
0.784 40.55
1.073 56.12
1.697 73.23
Figure 8: Querying time for varying |V|,OSP
datasets, k=2 0(log-scaled).
Figure 8 shows the average querying time for varying |V|,
using OSPdatasets. Compared to Figure 7, InTCDachieves
higherspeed-upover FrTCD,i.e., 31.7 times faster than Scan,
which suggests that our proposed I/O optimization tech-
niques are more eﬀective in larger-scale datasets and play a
crucial role in enhancing the overall eﬃciency.
Figure 9 shows the querying time over varying k,u s i n g
OSP 3dataset. As Scanreads the entire data once regardless
ofk, its performance is constant over varying k.I n c l e a r
contrast, InTCDand FrTCDare 36 and 7.3 times faster than
Scanwhenk= 10. Considering that kis typically much
smaller than the data size in general search scenarios, this10 20 40 60 8004812162024
kQuerying time (s)
  
Scan
FrTCD
InTCD
k|T C k|
10 63.60
20 73.23
40 94.92
60 113.7880 135.93
Figure 9: Querying time over varying k,OSP 3
dataset ( |V|=1,697K).
“progressive”behavior of our proposed algorithms, incurring
smaller cost for smaller k, is highly desirable.
4.3 Effectiveness of the Approximation
To validate the proposed approximate query processing
algorithm, called ApTCD, this section reports its performance
and approximation quality for varying approximation set-tings, compared to InTCD.
Figure 10 shows the performance and quality of our ap-
proximation algorithm for varying approximation settings.
We varied the aggregation window size ωf r o m1t o5 ,t h e
dimensionality Dof the index from 12 to 32, and kwas set
to 20.
1 2 3 4 500.020.040.060.08
window sizeQuerying time (s)
  
D=32
D=28
D=24
D=20
D=16
D=12
1 2 3 4 50.20.30.40.50.60.7
window sizeF−score
  
D=32
D=28
D=24
D=20
D=16
D=12
(a) Querying time (b) F1score
Figure 10: Performance and quality of ApTCDfor
varying settings, OSP 3dataset, k=2 0.
In Figure 10(a), it is clear that the lower dimensional
indexes perform better than higher ones in general. Fig-
ure 10(b) reports the approximation accuracy, measured us-ing the balanced F-scores (F
1scores) [25]:
F1score =2·Precision ·Recall
Precision + Recall,
Precision =|/tildewiderTC k∩TC k|
|/tildewiderTC k|, Recall =|/tildewiderTC k∩TC k|
|TC k|,
whereTC kand/tildewiderTC kdenotethequeryresultsetsusing InTCD
and ApTCD, respectively. Note that |T C k|and|/tildewiderTC k|are not
always equal to k, because they may have ties.
Observe from the ﬁgure that our proposed reduction us-
ing PAA enables a high speed-up without compromising theaccuracy much when D=1 2a n d ω= 2 or 3.
We now compare ApTCDwith InTCDusing the following
two settings. First, we chose the setting where ApTCDis
most accurate,i.e.,D=3 2a n d ω= 1. Second, we chose
amoderate setting, where D=2 4a n d ω=2 . B yu s i n g
these two settings, accurate andmoderate , we compared our
approximation scheme with the exact querying algorithm
proposed, InTCD.
1740.613 0.784 1.073 1.69710−210−1100101
|V| (millions)Querying time (s)
  
Scan
InTCD
Accurate
Moderate
10 20 40 60 8010−210−1100101
kQuerying time (s)
  
Scan
InTCD
Accurate
Moderate
(a) Eﬀect of |V|,k=2 0 ( b )E ﬀ e c to f k,OSP 3
Figure 11: Approximation performance using OSP
Figure 11 and Table 3 respectively summarizes the perfor-
mances and accuracies of our approximation scheme using
the above two settings. In Figure 11, our approximation
scheme in both settings signiﬁcantly outperformed InTCD,
and the gap increased as the size of the dataset increases.
For OSP 3dataset, our approximation is 28 and 19 times
faster than InTCDformoderate and accurate respectively.
Meanwhile, the accuracy was not compromised much, asthe precision and recall results in the accurate setting show,
e.g., constantly higher than 0 .7.
Table 3: Approximation quality using OSPdatasets
|V|kAccurate Moderate
(millions) Precision Recall Precision Recall
0.613 20 0.728 0.723 0.586 0.581
0.784 0.746 0.761 0.577 0.575
1.073 0.761 0.764 0.599 0.591
1.697 0.713 0.741 0.609 0.609
1.697 10 0.725 0.713 0.633 0.637
20 0.713 0.741 0.609 0.609
40 0.725 0.730 0.586 0.588
60 0.710 0.730 0.575 0.593
80 0.720 0.733 0.578 0.580
4.4 Comparison with LSH
Lastly, we evaluate the proposed indexing structure used
inInTCD, compared to the LSH implementation used by
Deckard (Figure 12).
Experiment Setting :For the given R-range query, our
R∗treebased index returns the exact answers, while LSH
returns approximate results with probability guarantee P.
More precisely, LSH requires two parameters PandRfor
building the index. For the given query point q,L S Hr e -
turns all points psuch that /bardblp,q/bardbl≤Rwith probability of
Por higher. Due to this nature, technically, LSH structure
needs to be re-built as Rchanges, to ensure the probabilistic
guarantee, unlike our R∗treebased approach building one
single structure and reusing for arbitrary range R.
To accommodate such diﬀerence, we use a favorable set-
ting for LSH, of not considering the rebuilding cost of LSH,
to show our approach outperforms even in such unfair set-ting in Figure 12(a). For queries, we randomly selected 100
vectors from OSP
7,a n dv a r i e d Rfrom 1 to 8.
Querying Time :Observe from Figure 12(a) that, even in
unfavorable settings, our index outperforms LSH in terms
of query execution time. This experiment also shows that,
asRincreases, the performance gap also increases, which
suggests our approach is more scalable for large R.F o r
example, when R= 4 (which means selecting all data points1 2 4 6 810−310−210−1100101
RQuerying time (s)
  
LSH (P=0.99)
LSH (P=0.95)
LSH (P=0.90)
R*tree based
RRecall
P=0.95 P=0.90
1 1.000 1.000
2 1.000 1.000
4 0.993 0.993
6 - 0.986
(a) Performance (b) LSH accuracy
Figure 12: Range query performance (log-scaled),OSP
7dataset ( |V|=784K).
with inL2distance 4 from the query), our index is about
24 times faster than LSH (with P=0.9), and when R=6 ,
ours is 49 times faster than LSH.
Memory Use :Meanwhile, inFigure12(a), somedatapoints
for LSH are missing, which are the cases when LSH could
not return results due to memory shortage.
Alternatively, we can use a disk-based implementation of
LSH, not to be constrained by memory size. However, its
performance will be worse than that of memory-based im-
plementation reported in the ﬁgure, which is already out-
performed by our approach. Deckard goes around this
problem, by dividing a problem into sub-problems and ap-
ply LSH for each sub-problem.
Accuracy :In terms of accuracy, we compare the precision
and recall of our approach and LSH. In Figure 12(b), as the
precision of LSH was perfect in all settings, we only reportits recall when P=0.95 and 0 .90 respectively. Observe
that, as Rincreases, e.g.,R= 4 or 6 in the ﬁgure, LSH
is more likely to miss some vectors within the range R.I n
clear contrast, our indexing scheme guarantees the perfect
precision and recall in all cases.
Summing up, for instant clone search, our R∗treebased
indexing is more suitable than LSH, by ensuring (1) higher
performance, (2) eﬀective memory usage, and (3) perfectaccuracy.
5. RELATED WORK
This section surveys existing research on (1) code clone
detection, (2) code example recommendation, and (3) code
search.
Clone Detection :As already brieﬂy surveyed in Section 2,
there have been many clone detection techniques proposed
that abstract codes as parsing trees and apply hashing [2] orcharacteristic vector comparison [11] for clone detection. Ina similar way, Wahler et al.[29] abstracted codes as XML
trees and adopted the concept of frequent itemsets to ﬁnd
clones that share frequent tree patterns. Recently, a dis-
tributedcodecloneanalysisalgorithm, calledD-CCFinder[24]wasproposed, whichimprovesthescalability ofCCFinder[15]by leveraging multi-cluster machines. However, these tools
are still not scalable enough to achieve online detection in
large-scale repositories.
Code Recommendation :Meanwhile, there have been al-
ternative lines of research taking place, to ﬁnd code exam-
ples that share (1) similar usage patterns or (2) structuralsimilarity.
I nt h eﬁ r s tl i n eo fw o r k ,X i e et al.[30] proposed an ap-
175proach to abstract codes as API call sequences and identify
examples that share similar sequences. Li et al.[22] used the
frequency of operation calls to deﬁne similarity, and then
clustered similar examples into a few representative usagetypes. In the second line of work, Holmes et al.[9, 10] pro-
posed some heuristic structural matching techniques to ﬁnd
relevant example codes. However, the former line of work
[30, 22] cannot be used for structural similarity search and
the latter [9, 10] has limited scalability.
Code Search Engine :There are commercial code search
engines, including Kodersand Google Code Search that ab-
stract codes as text and support simple and regular expres-
sion keyword matches. However, these engines, which treat
codes as simple text, do not support structural matches.
Sourcerer [1] stores some structural information on codes in
relational tables and provides rankedmatching, butdoes notfocus on optimizing search performance.
6. CONCLUSION
In this paper, we introduced scalable and instant code
clone search techniques. These techniques open doors to
many interesting unexplored applications, such as interleav-
ing clone detection with editing sessions during code devel-
opment.
We evaluated the accuracy and eﬃciency of our approach
with large-scale real-life software repositories. In addition to
exact code clone search, we also developed an approximation
algorithm for scenarios wher e some accuracy compromise
can be tolerated, which performs nearly a thousand times
faster than the baseline approach. Both the exact and ap-proximation algorithms achieved sub-second response timesfor large-scale real-life repositories of 1.7 million code seg-
ments.
As future work, we are considering the following:
⊿More features: In addition to the characteristic vectors,
we will consider more features such as the structural rela-tionship between vectors or runtime semantics, to enablemore precise matching.
⊿Industry-scale detection: To build commercial engines,
e.g., to achieve Google’s scalability over billions of doc-
uments, we need to deal with unexplored issues such as
parallelization.
7. ACKNOWLEDGMENTS
Our thanks to Lingxiao Jiang for providing Deckard
source code. This research was supported by National IT
Industry Promotion Agency (NIPA) under the program of
SoftwareEngineeringTechnologiesDevelopmentandtheEn-
gineering Research Center of Excellence Program of KoreaMinistry of Education, Science and Technology (MEST) /National Research Foundati onof Korea (NRF)(Grant 2010-
0001728).
8. REFERENCES
[1] S. K. Bajracharya, T. C. Ngo, E. Linstead, Y. Dou, P. Rigor,
P. Baldi, and C. V. Lopes. Sourcerer: a search engine for open
source code supporting structure-based search. In OOPSLA
Companion , 2006.
[2] I. D. Baxter, A. Yahin, L. Moura, M. Sant’Anna, and L. Bier.
Clone detection using abstract syntax trees. In ICSM ,1998.
[3] N. Beckmann, H. peter Begel, R. Schneider, and B. Seeger. The
r*-tree: an eﬃcient and robust access method for points and
rectangles. In SIGMOD ,1990.[4] S .B e r c h t o l d ,C .B ¨ohm, and H.-P. Kriegel. Improving the query
performance of high-dimensional index structures by bulk-load
operations. In EDBT ,1998.
[5] J. V. d. Bercken and B. Seeger. An evaluation of generic bulk
loading techniques. In VLDB , 200 1.
[6] C. B ¨ohm and F. Krebs. The k-nearest neighbour join: Turbo
charging the kdd process. Knowl. Inf. Syst. , 6(6):72 8–749,
2004.
[7] A. Gionis, P. Indyk, and R. Motwani. Similarity search in high
dimensions via hashing. In VLDB ,1999.
[8] G. R. Hjaltason and H. Samet. Distance browsing in spatial
databases. ACM TODS ,24:265–3 18,1999.
[9] R. Holmes and G. C. Murphy. Using structural context to
recommend source code examples. In ICSE, 2005.
[10 ] R .H o l m e s ,R .J .W a l k e r ,a n dG .C .M u r p h y .A p p r o x i m a t e
structural context matching: An approach to recommend
relevant examples. IEEE Trans. Softw. Eng. , 32( 12):952–970,
2006.
[11] L. Jiang, G. Misherghi, Z. Su, and S. Glondu. Deckard :
Scalable and accurate tree-based detection of code clones. InICSE, 2007.
[12] I. T. Jolliﬀe. Principal Component Analysis . Springer Series in
Statistics. Springer, 2nd edition, 2002.
[
13] E. J ¨urgens, B. Hummel, F. Deissenboeck, and M. Feilkas. Static
bug detection through analysis of inconsistent clones. In
Software Engineering (Workshops) , pages 443–446, 200 8.
[14] I. Kamel and C. Faloutsos. On packing r-trees. In CIKM ,1993.
[15] T. Kamiya, S. Kusumoto, and K. Inoue. CCFinder: a
multilinguistic token-based code clone detection system forlarge scale source code. IEEE Trans. Softw. Eng. ,
28(7):65 4–670, 2002.
[16] J. Kim, S. Lee, S. won Hwang, and S. Kim. Adding examples
into java documents. In ASE, 2009.
[17] J. Kim, S. Lee, S. won Hwang, and S. Kim. Towards an
intelligent code search engine. In AAAI ,2 010.
[18] M .K i m ,V .S a z a w a l ,D .N o t k i n ,a n dG .M u r p h y .A ne m p i r i c a l
study of code clone genealogies. SIGSOFT Softw. Eng. Notes ,
30(5): 187–196, 2005.
[19] F. Korn, B.-U. Pagel, and C. Faloutsos. On the ‘dimensionality
curse’ and the ‘self-similarity blessing’. TKDE ,13(1):96– 111,
2001.
[20] F. Korn, N. Sidiropoulos, C. Faloutsos, E. Siegel, and
Z. Protopapas. Fast nearest neighbor search in medical image
databases. In VLDB ,1996.
[21] S. T. Leutenegger, J. M. Edgington, and M. A. Lopez. STR: A
simple and eﬃcient algorithm for r-tree packing. In ICDE ,
1997.
[22] Y. Li, L. Zhang, G. Li, B. Xie, and J. Sun. Recommending
typical usage examples for component retrieval in reuserepositories. In ICSR, 200 8.
[23] H. Liu and H. Motoda. Feature Selection for Knowledge
Discovery and Data Mining . Kluwer Academic Publishers,
1998
.
[24] S. Livieri, Y. Higo, M. Matushita, and K. Inoue. Very-large
scale code clone analysis and visualization of open sourceprograms using distributed CCFinder: D-CCFinder. In ICSE,
2007.
[25] C. J. V. Rijsbergen. Information Retrieval .
Butterworth-Heinemann, 1979.
[26] C. K. Roy, J. R. Cordy, and R. Koschke. Comparison and
evaluation of code clone detection techniques and tools: Aqualitative approach. Sci. Comput. Program. ,74(7):470–495,
2009.
[27] T. Seidl and H.-P. Kriegel. Optimal multi-step k-nearest
neighbor search. In SIGMOD ,1998.
[28] A. Silberschatz, P. B. Galvin, and G. Gagne. Operating System
Concepts . Wiley Publishing, 8th edition, 200 8.
[29] V. Wahler, D. Seipel, J. W. v. Gudenberg, and G. Fischer.
Clone detection in source code by frequent itemset techniques.
InSCAM , 200 4.
[30] T. Xie, M. Acharya, S. Thummalapenta, and K. Taneja.
Improving software reliability and productivity via mining
program source code. In NSFNGS , 200 8.
[31] B.-K. Yi and C. Faloutsos. Fast time sequence indexing for
arbitrary lp norms. In VLDB , 2000.
[32] K. Zhang and D. Shasha. Simple fast algorithms for the editing
distance between trees and related problems. SIAM J.
Comput. ,18(6):1245–1262, 1989.
176