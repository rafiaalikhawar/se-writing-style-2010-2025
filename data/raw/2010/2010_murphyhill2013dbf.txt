The Design of Bug Fix es 
 
Emerson Murphy -Hill 
Department of Computer Science  
North Carolina State University  
Raleigh, North Carolina, USA  
emerson@csc.ncsu.edu  Thomas Zimmerma nn, Christian Bird, Nachiappan 
Nagappan, and Michael Barnett  
Microsoft Research  
Redmond, Washington, USA  
{tzimmer,cbird,nachin,mbarnett}@microsoft.com
 
 
Abstract —When software engineers  fix bugs , they may have se v-
eral options as to how to fix those bugs . Which fix is chosen has 
many implications, both for practitioners and researchers: What 
is the risk of introducing other bugs during the fix? Is the bug fix 
in the same code tha t caused the bug?  Is the change fixing the 
cause or just covering a symptom?  In this paper, we investigate 
the issue of alternative fixes to bugs and  present an empirical 
study of how engineers make design choices about how to fix 
bugs . Based on qualitat ive interviews with 40 engineer s working 
on a variety of products, 6 bug triage meetings, and a survey 
filled out by 326 engineer s, we found that there are a number of 
factors, many of them non -technical, that influence how bugs  are 
fixed , such as how clos e to release the software is . We also discuss 
several implications for research and practice, including ways to 
make bug prediction and localization more accurate.   
Keywords -component; bugs, faults, empirical study, design  
I.  INTRODUCTION  
As the software sys tems we create and maintain continue to 
grow in capability and complexity, software engineer s must 
ensure that these systems work as intended. When systems do 
not, software engineer s fix the “bugs” that cause this uninten d-
ed behavior.  
Traditionally, resear chers and practitioners have assumed 
that where in the software an engineer  fixes a bug is where an 
error was made  [1]. For example, Endes [2] makes such an 
assumption in a study, but cautions the reader that,  
There is, of course, the initial question of how we can d e-
termine what the error really was. To dispose of this que s-
tion immediately, we will say right away t hat, in the mat e-
rial described here, normally the actual error was equa t-
ed to the correction made. This is not always quite acc u-
rate, because sometimes the real error lies too deep, thus 
the expenditure in time is too great, and the risk of intr o-
ducing new  errors is too high to attempt to solve the real 
error. In these cases the correction made has probably 
only remedied a consequence of the error or circumven t-
ed the problem. To obtain greater accuracy in the anal y-
sis, we really should, instead of consideri ng the corre c-
tions made, make a comparison between the originally i n-
tended implementation and the implementation actually 
carried out. For this, however, we usually have neither 
the means nor the base material.  Although the software engineering community h as 
suspected that this assumption is sometimes false , there exists 
little evidence to help us understand under what circumstances 
it is false. The consequences of this lack of understanding are 
manifold. Let us provide several examples. For researchers 
studying bug prediction  [3] and bug localization [4], models of 
how developers have fixed bugs in the past may not capture the 
true cause of failures, but may instead only capture 
workarounds. For practitioners, when a software engineer is 
evaluated based on how many bugs are fixed, the evaluation 
may not a ccurately affect that engineer’s effect on software 
quality. For educators, without teaching future engineers the 
contextual factors that  go into deciding which fix to apply, as 
engineers they may choose inappropriate fixes.  
However, to our knowledge , there has been no empirical 
research into how bugs fixes are designed. In this paper, we 
seek to understand the design of bug fixes . When we say that 
bug fixes are “designed,” we mean that there are a number of 
potential fixes for a single bug, and choosing b etween those 
fixes is a matter of human judgment. As with any software 
change, an engineer must deal with a number of competing 
forces when choosing exactly what change to make.  The task 
is not always straightforward.   To fill this gap, we seek  to an-
swer two research questions:  
RQ1 : What are the different ways that bugs can be fixed?  
 
 
Figure 1: Characterizing  the design of bug Fixes  
 
 RQ2 : What factors influence which fix an engineer  chooses ? 
This paper ’s primary contribution: T he first systematic 
characterization of the design of bug fixes . It analyzes the  de-
sign space of bug fixes and describes how developers navigate 
that design space , to understand the decisions that go in to 
choosing a bug fix (see Figure 1).  
II. RELATED WORK 
Several researchers have investigated bug fixes. Perhaps the 
most relevant is Leszak, Perry, and Stoll ’s [5] study of the  
causes of defects , where the authors  classified bug reports by 
‘real defect location ’: 
‘Real’ location characterizes the fact that … some defects 
are not fixed by  correcting  the ‘real’ error -causing co m-
ponent, but rather by a … ‘work -around’ somewhere else.   
While the authors co llected real defect locations,  the data was 
not analyzed or reported. Our work explains why one fix would 
be selected over another; or in other words, why an engineer  
would choose a workaround instead of a fix at a “ real location .” 
Ko and Chilana studied 100 contentious open -source bug r e-
ports to investigate how engineer s make decisions about bugs  
[6]. While this paper did investigate “design dimensions” du r-
ing bug fixing, by “design” they meant the design of software 
(for example, whether a fix make s the software mo re usable), 
rather than “design” in the sense we mean it, which is the d e-
sign of the fix itself. Our study also complements this study by 
improving  our understanding of the decision making process 
when fixing bugs, specifically for commercial  software and for 
decisions that get made outside of the bug report itself . 
Breu  and colleagues observed in a study of 600 bug reports 
that 25.3% of discussions in bug reports are spent on the co r-
rection itself, discussions involving suggestions, feedback r e-
quests, and understanding files  [7]. Our study complements this 
work by exploring the design space of bug fixes.  
Several other researchers have investigated the bug fixing . In 
a manual inspection of bug fixes, Lucia and colleagues found 
that some fixes are spread over many lines of code  [4]. Bird 
and colleagues found that bug fixes reported in bug databases 
have different characteristics than fixes not reported in dat a-
bases  [8]. Yin and colleagues investigated why bugs are fixed 
incorrectly, that is, require a later bug fi x to the source code 
changed by the original fix [9]. Aranda and Venolia investiga t-
ed 10 closed bugs and surveyed 110 engineer s about bug coo r-
dination patterns at Mi crosoft  [10]. Spinellis and colleagues 
attempted to correlate code metrics, such as number of bugs 
fixed, to evaluate the quality of open source software  [11]. Sto-
rey and colleagues investigated the interaction of bugs and 
code annotations  [12]. Anvik and colleagues investigated 
which engineer s get assigned to fix bugs  [13]. In contrast to 
these papers, our paper seeks to understand in what way bug 
fixes differ, and why one fix is chosen over another.   III. METHODOLOGY  
To answer our two research questions, we conducted a 
mixed -method study. We used several research methods, 
rather than a single one, both to study our research questions 
in as broad a way as p ossible and to triangulate the answers to 
improve their accuracy  [14]. While we feel that our methods 
are thorough and rigorous, some threats still exist  as we 
discuss in Sect ion V. We now discuss our four research 
methods: opportunistic interviews, firehouse interviews, triage 
meeting observations, and surveying. For each method , we 
discuss the goal of using that method, how we recruited 
participants, the protocol we used, how we analyzed data, and 
a brief  summary of the shape of the data we collected.  
A. Opportunistic Interviews  
With our first method, w e randomly asked engineer s about a 
recent bug they had been involved in fixing.  
Goal . Our goal in performing opportunistic interviews was 
to rapidly obtain qualitative answers to our research questions  
in a way that was minimally obtrusive to interviewees . 
Protocol.  We conducted these inter views by having the first 
author go to a building that housed a particular product group. 
Armed with a list of office numbers for software engineer s, the 
interviewer walked to each engineer ’s office. If the engineer ’s 
door was closed, was wearing headphone s, or was talking to 
someone else, the interviewer  went to the next office. Othe r-
wise, the interviewer introduced himself, said that he was doing 
a study, and asked if the interviewee  had 10 to 15 minutes to 
talk. If the engineer  consented, the interviewer  asked a series of 
semi-structured questions  [14] regarding the last bug that the 
engineer  was involved in fixing.  Although interviewees were 
not offered an incen tive, before the interviewer left, intervie w-
ees were compensated with a $10 gift card for lunch.  
We performed p ilot interviews to identify potential pro b-
lems and rectify them prior to the main study.  In doing so , we 
noticed that pilot interviewees could remember the fix they 
made, but had difficulty recalling the alternative fixes they did 
not make. Some pilot interviewees stated that they fixed the 
bug the only way that it could have been fixed, even though 
there clearly were other fixes, even from  our perspective as 
outsiders. We sought to reduce this ‘hindsight bias’  [15] in our 
interviews using two different techniques. For every odd -
numbered interview (the first, the thir d, and so on), we gave the 
interviewee an example of three bugs and multiple ways of 
fixing each bug. For the other half of the interviewees, we pr e-
sented a small program containing  a simple bug, and then 
asked the interviewee to talk us through how she mi ght fix the 
bug; interviewees typically mentioned several alternative  fixes . 
Comparing the results obtained after starting interviews with 
these two methods, we noticed no qualitative differences in the 
responses received, suggesting that both methods were  about 
equally effective. Comparing p ilot interview results  against real 
interview results , we feel that this technique significantly 
helped interviewees  think broadly about the design space.  
After this introductory exercise , the interviewer asked the in-
terviewee about the most recent bug that they fixed. The inte r-
viewer asked about the software that the bug appeared in, the symptoms, the causes, and whether they considered more than 
one way to fix the bug. If an interviewee did consider multiple 
fixes , we asked her to briefly explain each one, and justify their 
final choice . The full interview guide  can be found online.1 
Participants . To sample a wide variety of engineer s, we re-
cruited interviewees  using a stratified sampling technique, 
sampling acr oss several dimensions of the products that engi-
neers create. We first postulated what factors might influence 
how engineer s design fixes; we list those factors in Table I. 
 
Factor  Values  
Domain  Desktop, web application, ente r-
prise/backend, embedded  
Product Type  Boxed, service  
Bug fix types  Pre-release, post -release  
Number of ve r-
sions shipped  0 to continuous release  
Phase  Planning and milestone quality , main d e-
velopment, stabilization, and maintenance  
 
Table I. Factors  for selecting product groups.  
 
 
Using these factors, we select ed a cross section of Microsoft 
products that spanned those factors. We chose four products 
from which to recruit engineer s, because we estimated that four 
products would balance two competing requirements; that we 
sample enough engineer s from each product team to get a good 
feeling for what bug fixing is like within that team , and to sa m-
ple enough product teams that we could have reasonable gene r-
alizability.  The four product teams that we se lected spanned 
each of the values in the Table.  For example, one team we 
talked to worked on desktop software, one web applications, 
another enterprise/backend, and the last embedded systems.  
Within each product team, we aimed to talk to a total of 8 
softw are engineer s: six were what Microsoft calls “So ftware 
Development Engineers” ( developers  for short) and two were 
“Software D evelopment Engineers in Test” ( testers for short). 
We intervie wed more developers, as  developers  spend more 
time fixing bugs than t esters . Once we reached our quota of 
engineer s in a team , we moved on to the next product team. In 
total, we completed 32 opportunistic interviews  with engineer s. 
Data Analysis . We prepared the interviews for analysis by 
transcribing them. We then coded the transcripts  [16] using the  
ATLAS.ti2 software . Before beginning coding, we defined se v-
eral base codes, including codes to identify symptoms, the fix 
that was applied, alternative fixes, and reasons for discrimina t-
ing between fixes. The first author  did the coding . Additionally, 
our research  group , consisting of 7 full time researchers and 7 
interns, analyzed the coded transcripts again, to determine if 
any other notable themes emerged. Each person in  the group 
analyzed 2  to 4 transcripts  over ½ hour . We regard the first 
                                                           
1 http://people.engr.ncsu.edu/ermurph3/experiments/BugFixDesignInterview.pdf  
2 http://atlasti.com/  author’s coding as methodical and thorough, while the team’s 
analysis was brief and serendipitous . We derived most of the 
results described in this paper from the first author’s coding . 
We use  the codes about fixes to describe the design space (Se c-
tion IV.A ) and codes about discriminating between fixes to 
describe how engineer s navigate that spa ce (Section IV.B ). 
Data Characterization . Overall, we found software engi-
neers very willing to be interviewed. To obtain 32 interviews, 
we visited 152 engineer s’ offices. Most offices were empty or 
the engineer s appeared busy. In only a few cases, engineer s 
explicitly declined to be interviewed, largely because the engi-
neer was too busy.  Interviews lasted between 4 and 30 minutes.  
In this paper, we refer to part icipants as P1 through P32.  
Most participants reported multiple possible fixes for the 
bug that they discussed. In a few cases, participants were un a-
ble to think of alternate solutions; however, the interviewer, 
despite being unfamiliar with the bug, was a ble to suggest an 
alternative fix. In these cases, the engineer  agreed that the fix 
was possible, but never consciously considered the alternative 
fix, due to external project constraints .  
Interestingly, this opportunistic methodology allowed us to 
interview three  engineer s who were in the middle of conside r-
ing multiple fixes for a bug. 
B. Firehouse  Interviews  
Using the firehouse research method  [17], we interviewed 
engineer s immediately after they fixed a bug.   Firehouse r e-
search is so called because of the unpredictable nature of the 
events under study ; if one wants to study social dynamics of 
victims during and immediately after a fire, one has to literally 
live in the fireh ouse, waiting for fires to occur.  Alternatively, 
one can purposefully set fires, although this research metho d-
ology is generally discouraged.   In our case, we do not know 
exactly when a n engineer  is considering a fix, but we can o b-
serve a just -completed fix in a bug tracker and “rush to the sc e-
ne” so that the event is fresh  in the engineer’s mind . 
Goal . Our goal was to obtain qualitative answers to our r e-
search questions in a way that maximized the probability that 
engineer s could accurately recall their bug fix design decisions.  
Protocol . We first picked one product group at Microsoft, 
went into the building where most development for that pro d-
uct takes place, and monitored that group’s bug tracker,  watch-
ing for bugs an engineer  marked as “fixed” within the last ten 
minutes . If the engineer was not located in the building, we 
moved on to the next most recently closed bug. Otherwise , the 
interviewer went immediately to the engineer ’s office.  
When approaching engineer s for this stu dy, we were slightly 
more aggressive than in the opportunistic interviews; if the 
engineer ’s door was closed, we knocked on the door. If the 
engineer  was not in her office by the time we arrived, we wai t-
ed a few minutes.  These interview s were the same as the op-
portunistic interviews, except that the interviewer  insisted that 
he engineer  focused on the bug that they had just closed.  
Participants . Our options for choosing a product group to 
study was fairly limited, because we had to have a personal 
contact within that team that was willing to allow us to have 
live, read-only access to their bug tracker. We chose on e prod-uct, which will remain anonymous; the product group was di f-
ferent from any of those chosen in the opportunistic interviews.  
We aimed to talk  to 8 software engineer s in total for these 
interviews. While we interviewed fewer people than with the 
opportunistic interviews, these firehouse interviews tended to 
take much longer to orchestrate, mostly because we had speci f-
ic people that we wanted to talk to. In retrospect, we did not 
notice any qualitative differences in engineer s’ responses to the 
two interview types, so for the remainder of the paper, we do 
not distinguish between these two groups of participants. Non e-
theless, you may do so if you w ish; participants of the firehouse 
interviews are labeled P33  through P40. 
Data Analysis . We analyzed d ata in the same way as with 
the opportunistic interviews.  
Data Characteristics . We also found engineer s to be rece p-
tive to being interviewed, although th ey were usually surprised 
we asked  about a bug they had just fixed . We reassure d them 
that we are from Microsoft Research, and are there to help.  
In total, we went to 16 offices, and were able to interview  10 
engineers . Two of these we mistakenly interview ed, one b e-
cause his office mate actually  closed the bug, and one because 
the interviewer misread the bug report. We compensated these 
engineer s for their time, but we exclude them from analysis.  
C. Triage Meetings  
We hypothesized that not only do individual engineer s 
make decisions about the design of bug fixes, but perhaps that 
bug fix designs happen during bug triage meetings as well.  
Goal . Our goal was to obtain qualitative answers to our r e-
search questions with respect to how engineer s work together 
to find good bug fix designs.  
Protocol and Participants . We attended six bug triage 
meetings across four product groups. Five of these groups were 
the same groups that we did interviews with. To ensure engi-
neers were comfortable, we did not record these meeting s; ra-
ther, we took notes and observed in silence.  
Data Analysis and Data Characteristics . It became clear 
that there was very little data we could gather in these triage 
meetings, for two reasons. The first is that participants rarely 
discussed how to fix a bug  beyond whether to fix it and when 
to do so. Second, when participants did discuss how to fix 
bugs, the team was so tightly knit that very little explanation 
was needed; this terseness made bug fix decisions basically 
impossible for us to understand w ithout the context that the 
team members had. As a result, we were able to glean few i n-
sights from the meetings. For the few observations that we 
could make, we label these meetings as T1 to T6. Because 
there was little usable data from these meetings, we did not 
perform any data analysis beyond reading through our notes.  
D. Survey  
Goal . Our goal was to quantify our observations made du r-
ing the interviews and triage meetings.  
Protocol . After we performed the interviews and triage 
meetings, we sent a survey to software engineer s at Microsoft.  
As in the interviews, the survey started by giving examples of bugs that could be fixed using different techniques, where the 
examples were drawn from real bugs described by intervie w-
ees. As suggested by Kitchenham and Pfle eger [17], we con-
structed the survey to use formal notations  and limit responses 
to multiple -choice, Likert scale s, and short , free-form answers.  
At the beginning of the survey, we suggested that the r e-
spondent browse bugs that they had recently closed  to ground 
their answers. In Section IV, we discuss these questions, inte r-
mixed with engineer s’ responses. After piloting the survey, w e 
estimate that it took respondents about 15 -20 minutes to fill out  
the survey . The full text of this survey can be found online.3 
Participants . We sen t the survey to 2000 participants by 
selecting employees of Mic rosoft who had “development” in 
their job title, and were not interns or contractors. This fo l-
lowed Kitchenham and Pfleeger ’s advice to understand whet h-
er respondents had enough knowledge to answer the questions 
appropriate ly [17]. We incentivized participation by giving  $50 
Amazon.com gift certificates to two respondents at random .  
Data Analysis . We analyzed our data with descriptive st a-
tistics  (for exam ple, the median) , where appropriate. We did 
not perform inferential statistics (for example, the t -test) be-
cause our research questions do not necessitate them.  When 
reporting survey data, we omit “Not Applicable” question r e-
sponses, so percentages may not  add up to 100%.  
Data Characteristics . 324 engineer s completed the survey, 
a response rate of about 16%, within the range of other sof t-
ware engineering surveys  [18]. Respondents were from all 
eight  divisions of Microsoft. Respondents reported between 
0.08 and 39 years of experience  in the software industry (med i-
an=9.5) , with a median of 5 years of experience at Microsoft. 
65% reported being developers , while 34% reported being tes t-
ers. One respondent reported being a product manager.  
 
IV. RESULTS  
We next characterize  the design options that engineer s have 
when selecting a bug fix (Section IV.A ), and then describe how 
engineer s choose which fix to implement  (Section IV.B ). 
A. Description of the Design Space  
In our interviews, we asked participants to estimate what  
percentage of their bugs  for which there were multiple possible 
solutions. The median was 52%, with a wide range of variance, 
with individual responses ranging from 0% to 100%. This su g-
gests that many bug s can be fixed in multiple ways, although 
this numb er should be interpreted as a rough estimate.  
With respect to the dimensions of the design space, we ob-
tained answers to this research question by ask ing interviewees 
to explain the different fixes that they considered when fixing a 
single bug. In bold below, we present several dimensions on 
which bugs may be fixed , a description of each dimension, and 
an example from our interviews . Note that a single fix can be 
considered a point in this design space ; for example, a fix may 
have low error surfacing  and high refactoring , and simult a-
                                                           
3 http://people.engr.ncsu.edu/ermur ph3/experiments/BugFixDesignSurvey.pdf  neously be placed in the other dimensions. These dimensions 
are not intended to be exhaustive, yet we believe that the nu m-
ber of interviews we performed suggests that the list represents 
a solid foundation on which to build a theory of bug fix design.  
Data Prop agation  Across Components . This dimension 
captures  how far information is allowed to prop agate across  a 
piece of software, where the engineer  has the option of fixing 
the bug by intercepting the data in any of the compone nts. At 
one end of the dimension, data is corrected at its source . 
As an example , P25 worked on software with a layered a r-
chitecture, with at least four layers, the top -most being the user 
interface. The bug was that the user interface was reporting 
disk space sizes far too large , and the engineer  found that the 
problem could be traced back to the lo west-level layer, which 
was reporting values in kilobytes when the user interface was 
expecting values in megabytes. The interviewee had the option 
of fixing the bug by correcting the calculation in the lowest 
layer, or by transforming the data (multiplying by a thousand) 
as it is passed through any of the intermediate layers.  
Error  Surfacing . This dimension describes  how mu ch 
information is revealed to users, whether that information is 
for end users or other engineers . At one end of the dimension, 
the user is made aware of detailed error information; at the 
other, the existence of an error is not revealed . 
P28 described a b ug where the software he was developing 
crashed when the user deleted a file. When fixing the bug, the 
engineer  decided to catch the exception to prevent the crash, 
but also was considering whether or not the user should be n o-
tified that an exceptional sit uation had occurred.  
As another  example, P6 described a bug where she was 
calling an API that returned an empty collection, where she 
expected a non-empty collection. The problem was that she 
passed an incorrect argument to the API, and the empty colle c-
tion signif ied an error. However, an empty collection could 
also signify “no results.” As part of the fix, the engineer  con-
sidered changing the API so that it threw an error when an u n-
expected argument was passed to the API. She anticipated that 
this woul d have helped future engineer s avoid similar bugs.  
Behavioral Alternatives . This dimension relates to whether a 
fix is  perceptible to the user.  At one one end of the dimension, 
the fix  does not require the user to do anything differently; at 
the other end,  she must significantly modify her behavior.  
One exa mple is P11, who described a bug where the back 
button in a mobile application was occasionally not working. As part of the fix, he made the back button work, but had to 
simultaneously disable another feature when the application 
first loads. P11 stated th at having both the back button and the 
other feature working at the same time was simply not poss i-
ble; he had to choose which one should be enabled initially.  
Functionality Removal . This dimension relates to how much 
of a feature is removed during a bug fi x. At one end of the 
dimension, the whole software product is eliminated; at the 
other, no code is removed at all.  
As an example, P18 described a bug in which a crash o c-
curred. Rather than fixing the bug, P18 considered removing 
the feature that the bug wa s in altogether. We were initially 
quite surprised when we heard this story, because the notion 
that an engineer  would remove a feature just to fix a bug 
seems quite extreme. However, removal of features was me n-
tioned repeatedly as a fix for bug s during ou r interviews.  
To quantify functionality removal, w e asked survey r e-
spondents to estimate how often they remove or disab le fea-
tures, rather than alleviating a symptom of a bug . About 75% 
of respondents said they had removed features from their 
software to fix bugs in the past.  
Refactoring . This dimension expresses the degree to which 
code is restructured in the process of fixing a bug, while 
preserving its behavior. A bug may be fixed with a simple 
one-line change, or it may entail significant code restruct uring.  
As an example, P5 considered refactoring to remove some 
copy -and-paste duplication, so “you're not only fixing the bug, 
but you also are kind of improv[ing it]. ” 
In our survey, we asked respondents to report on refacto r-
ing frequency when fixing bugs, as shown in Table II. In the 
table, “Should be refactored” indicates  how often participants 
“notice  code that should be refactored when fixing bugs .” For 
example, 29% of  respondents indicated that they usually n o-
tice code th at should be refactored. The “Is refactored” row 
indicates how often participants “refactor this code that should 
be refactored” . For example, 26% reported rarely refactoring 
code that should be refactored. These results suggest that, al t-
hough engineer s ap pear to regularly encounter code that 
should be refactored, much of this code remains unchanged.  
 
Internal vs External . This dimension relates to how much 
internal code is changed versus external code is changed as 
part of a fix. On one end of this dimensi on, the engineer  makes 
all of her changes to internal code, that is code for which the 
engineer  has a strong sense of ownership. On the other end, 
the bug is fixed by changing only code that is external, that is, 
code for which the engineer  has no ownershi p. 
One example is P33, who maintained a testing framework 
for devices used by several other teams. The bug was that 
many devices were not reporting data in a preferred manner, 
causing undesirable behavior in the P33’s framework. Part of 
the fix was immediate and internal (changing the testing 
framework), but part of it was deferred and external (changing 
each of the other teams’ device code).  
 
Never  
Rarely  
Sometimes  
Usually  
Always  
Should be refactored  1% 7% 56%  29%  5% 
Is refactored  4% 26%  44%  21%  3% 
 
Table I I. Survey respondents’ refactoring behavior  
 
 Accuracy . This dimension captures the degree to which a fix 
utilizes accurate information. One one end of this dimension, 
the engineer  uses highly accurate information, and on the 
other, he uses heuristics or guesses.  
An example is P29, who was working on a bug wher e 
web browser printing was not working well. An accurate fix 
would be one where his print driver retrieves the available 
fonts from the printer, then modifies the browser’s output 
based on the available fonts. A less accurate fix was to use a 
heuristic tha t produces better, but not optimal, print output.  
Hardcoding . This dimension captures to what degree a fix 
hardcodes data. On one end of the dimension, data is specified 
explicitly, and on the other, data is generated dynamically.  
One e xample of fixes on t his dimension is P24, who was 
writing a test harness for a system that received database qu e-
ries. The bug was that some queries that his harness was ge n-
erating were malformed. He considered a completely hardco d-
ed solution to the problem, removing the query  generator and 
using a fixed set of queries instead. A more dynamic solution 
he considered was to modify the generator itself to either filter 
out malformed queries, or not to generate them at all.  B. Navigating the Design Space  
While the previous section described the design space of 
bug fixes, it said nothing about why engineer s implement par-
ticular fixes within that design space. For instance, when would 
an engineer  refactor while fixing a bug, and when would she 
avoid refactor ing? In an ideal world, we would like to think 
that engineer s make decisions based completely on technical 
factors, but realistically, a variety of external factors come into 
play as engineer s navigate this bug fixing design space. In this 
section, we desc ribe those external factors.  
Risk Management by Development Phase . A common way 
that interviewees said that they choose how to design a bug fix 
is by considering the development phase  of the project . Speci f-
ically, participants noted that as software approaches release, 
their changes become more conservative . Conversely, partic i-
pants reported taking more risks in earlier phases , so that if a 
risk materializes , they would have a longer period to compe n-
sate. Two commonly mentioned risk s were the risk that new 
bugs would be introduced  and the risk that spending significant 
time fixing one bug comes at the expense of fixing other bugs.  
P12 provided an example of taking a more conservative 
approach, when he had to fix a bug by either fixing an existing 
implementation of the double checked locking pattern, or r e-
place the pattern with a simpler synchronization mechanism. 
He eventually chose to correct the pattern, even though he 
though t the use of the pattern was ques tionable, because it was 
the “least disruptive” way to fix the bug. He noted that if he 
had fixed the bug at the beginning of the development cycle, he 
would have remov ed the pattern altogether.  
In our survey, we asked engineer s several questions rela t-
ing to risk and development phase, as shown in Table I IIA. 
Here we asked engineer s “How often do the following factors 
influence which fix you choose?” , where each factor is listed at 
left. The table lists the percentage of respondents who choose 
that frequenc y level. Note that the factors are not necessarily  
linked ; for instance, an engineer  could choose to change very 
few lines of code for a reason other than the product is late in 
development. However, our qualitative interviews suggested 
that these factors are typically linked together, and thus we feel 
justified in presenting these four factors as a whole.  These r e-
sults suggest that, for most respondents, risk mitigation usually 
plays an important role in choosing how to fix a bug . 
 
 
Never  
Rarely  
Sometimes  
Usually  
Always  
Optimal fix should 
be reconsidered  1% 17%  38%  29%  14%  
Actually are fixed 
optimally  4% 40%  38%  13%  1% 
 
Table I V. Survey respondents’ optimal fix  
 
 
  
Never  
Rarely  
Sometimes  
Usually  
Always  
(A) Phase of the 
release cycle  2% 6% 17%  35%  37%  
Changes few 
lines of code  3% 10%  32%  38%  17%  
Requires little 
testing effort  3% 12%  31%  37%  16%  
Takes little 
time to i m-
plement  3% 10%  43%  30%  13%  
       
(B) Doesn't 
change inte r-
faces or break 
backwards 
compatibility  0% 2% 8% 36%  53%  
       
(C) Maintains the 
integrity of 
the original 
design  1% 5% 16%  50%  28%  
       (D) Frequency in 
practice  2% 17%  39%  33%  8% 
 
Table I II. Factors that influence engineers’ bug fix design  
 One of the findings that emerged from our interviews is 
that if engineer s are frequently making conservative changes, 
then they may be incurring technical debt. As P15 put it,  
I wish to do it better, but I'm doing it this way because 
blah, blah, blah.  But then I don't know if we ever go back 
and kind of “Oh, okay, we had to do this, now we can 
change it.”  And I feel that code never goes away, right?  
  
We verified this statem ent by asking survey respondents how 
often they think bug s that are initially fixed “suboptimally” 
should  be reconsidered for a more optimal fix in the future.  
We asked how many o f these bugs actually are fixed optimally 
after the initial fix . Table IV displays the results.  These results 
suggest that engineer s often feel that optimal fixes should be 
reconsidered in the future, but that those bugs rarely get fixed 
optimally.  As one respondent noted, “a lthough we talk about 
the correct fix in the next ver sion, it never happens. ”  
 
Interface Breakage . Another factor that participants  said i n-
fluenced their bug fix es is to what degree a fix breaks existing 
interfaces. If a fix breaks an interface that is used by external 
clients, then an engineer  may be less inclined to implement that 
fix because it entails changes in those external clients.  
One example comes from P16, who was working on a bug r e-
lated to playing music and voice over Bluetooth devices. He 
said that a better fix for the problem would be to change the 
Bluetooth standard, but too many clients already depend on it.  
We also asked survey respondents how often the following 
factor influences which fix they choose: “Doesn’t change e x-
ternal interfaces or breaks backwards compatibility.”  72% re-
ported that “ usually ” or “always,” suggesting that changing 
external interfaces is a significant determinant in choosing 
which bug fix to implem ent (Table II IB). 
Consistency . This factor describes to what degree a fix will be 
consistent with the existing software or existing practices. A fix 
that is not consistent with the existing code may compromise 
the design integrity of that code, leading to code rot.  
One example is P10, who fix ed a performance bug in his 
build system. P10 fixed the bug by using the build system in a 
way consistent with how it was being used by other teams. 
However, he felt that a change that was inconsistent with the 
way the  build system currently worked would have produced 
better build performance, at least for his product.  Table IIIC 
lists survey respondents’ attitudes towards the importance of 
maintaining design consistency when fixing bugs.  
User Behavior . This factor desc ribes the effect of how users of 
the software behave on the fix. If users have strong opinions 
about the software, or use a certain part of the software heavily, 
engineer s may choose a fix that suits the user better.  
One example is from T1, where the team discussed bugs in 
a code analysis tool . The team wondered how often a certain 
code pattern was used in practice. They acknowledged that 
their analysis did not work when the pattern was used , but how 
they fixed the bug depended on how often users actually wrote 
code in that pattern . They judged, apparently based either on 
intuition or experience, that several of these bugs were so u n-likely to arise in practice that the effort to  implement a co m-
prehensive fix for the problem  was not justified.   
After hearing  about T1  and some interviewees talk about 
frequency of user behavior, we became interested in how engi-
neers know what users actually do. Thus, we asked two que s-
tions in the survey. In the first we asked how often fixes d e-
pended on usage frequency (Table IIID). These results suggest 
that how frequently a situation occurs in practice sometimes 
influence s how engineer s design fixes.  The second question 
was a multiple -choice question about how engineer s most often 
determine frequency (Table V). In this table,  SQM refers to a 
usage data collector used in a variety of Microsoft products.  
The most common “None of the Above” answer was asking the 
product manager.  In Table V, we were somewhat surprised to 
find that so many engineer s write queries over usage data. 
However, it still appears that many engineer s use ad-hoc met h-
ods for estimating user behavior , including convenience sa m-
pling, estimation, and guessing.  
Cause  Understanding . This factor  describes how thoroughly 
an engineer  understands why a particular bug occurs . In inte r-
views, we were surprised how often engineer s fixed bugs wit h-
out understanding why those bugs occurred . Without thoroug h-
ly understanding a bug, the bug may re -appear at some point in 
the future. On the other hand, complete understanding of w hy a 
bug is occurring can be an extremely time -intensive task.  
P3 provided an example of fixing a bug without a full u n-
derstand ing of the problem. The symptom  of his bug was that 
occasionally an error message appeared to the user whenever 
his software subm itted a particular job. Rather than understan d-
ing why the error was occurring, he fixed the job by simply 
resubmitting the job, which usually completed without error. 
Rather than understanding the problem, as he explained it, “ my 
time is better spent fixin g the other ten bugs that I had .” 
We asked survey respondents why they do not always 
make an optimal fix for a bug ; 18% indicated that they have not 
had “time to figure out why the bug occurred.” This suggests 
that lack of cause understanding is sometimes a problem.  
Guess  4% 
Estimate bas ed on my past experience as a user 
of the software I develop  17% 
Estimate based on my past experience interac t-
ing with users  16% 
Collect data by taking a quick convenience sa m-
ple (e.g., ask devs on my team)  19% 
Collect data by external polling (e.g., ask readers 
of my blog)  2% 
Estimate based on existing usage data that I 
remember seeing in the past (e.g. SQM)  11% 
Write a query over existing usage data (e.g. 
SQM)  18% 
None of the Above  12% 
 
Table V. The most frequent mechanisms used by engineers to 
determine usage frequencies  
 Social Factors . A variety of social factors appear to play a role 
in how bugs are fixed, including mandates from supervisors, 
ability to find knowledgeable people, and code ownership.  
One example of this was P22, who was fixing a bug in a 
datab ase system where records were not sorting in memory, 
causing reduced performance.  The engineer  proposed a fix 
based on “ one week of discussions and bringing new ideas, 
[and] discussing [it with my] manager .” Other interviewees 
discussed their bugs with men tors (P28), peer engineer s (P28) , 
testers (P39), and development leads (P34).  
In the survey we asked how communication with people 
helps inform the bug fixe design  (Table VI ). The results su g-
gest that peer software development engineers (SDEs) and the 
people who originally wrote the code related to where the fix 
might be applied tend to play the most important role in deci d-
ing how a bug gets fixed. We also ask ed survey participants 
about who decides on which bug fix design to implement . Most 
participants said they themselves usually decide, while others 
said it was sometimes a group decision.  Only 6% said their 
manager usually or always decides.  
We also asked  survey respondents  how they communicate 
with others about bug design. Respondents indicated t hat they 
most often communicate  by email (44%), in unplanned mee t-
ings (38%), planned meetings (7%), and in the bug report itself 
(6%).  A few respondents also indicated that they discussed 
design during online code review and with instant messaging.  
However, in a study run in parallel with this one, we i nspected 
200 online code review threads  at Microsoft, but found no su b-
stantial discuss ions of bug fix design [19]. We postulate  that, 
by the time a fix is reviewed, engineer s have already discussed 
and agreed upon the basic design of that fix.  
We asked survey respondents how many people,  including 
themselves, were typically involved in the bug fixing process. 
Table VII shows the results.  These results suggest that while 
finding the cause of a bug and implementing a solution are 
generally 1 - or 2-person activities, choosing a solution tend s 
more often to be a collaborative activity.  One of the more surprising things we heard from some i n-
terviewees was that when they made sub -optimal changes, they 
were sometimes hesitant to file new bug reports so that the 
optimal changes were reconsidered in the future. The rationale 
for not doing so seemed to be at least partly social – respon d-
ents were not sure whether other engine ers would find a more 
optimal fix useful  to them as well. For instance, P2 said the 
optimal fix to his bug would be a change to the way mobile 
applications are built in the build system, but he wasn’t sure 
that he would advocate for this change unless othe r teams 
would find it useful as well. Ideally , this is what “feature e n-
hancement” bug reports with engineer  voting should help with. 
However, P2 didn’t fill out a bug report for this enhancement at 
all, because he judged the time he spent filling out the r eport 
would be wasted if other engineer s didn’t need it. As he put it,  
If I had more data … that other teams did it, … if I could … 
eyeball it quickly…  then I'd [say] , “Hey, you know, other 
teams are doing this.  C learly, it's a [useful] scenario.”  
This led  us to become curious why engineer s avoid filing 
bug reports, so we asked survey respondents to estimate the 
frequency of several possible rationales that we heard about 
during the interviews  (Table VIII ). These results suggest that 
survey respondents rare ly avoid filing bugs for reasons that the 
interviewees discussed. We view these somewhat contradictory 
findings as inconclusive; more study, likely using a different 
study methodology, is necessary to better understand how often 
and why engineer s do not fi le bug reports.  
V. LIMITATIONS  
Although our study provides a unique look at how engi-
neers fix bugs, several limitations of our study must be consi d-
ered when interpreting our results.  
An important limitation is that of generalizability beyond 
the population we studied  (external validity) . While our results 
may represent the practices and attitudes at Microsoft, it seems 
unlikely that they are completely representative of software 
developm ent practices and attitudes in general. However, be-
cause  Microsoft makes a wide variety of software products, 
uses many  of development methods , and employs an intern a-
tional workforce, we believe that our random and stratified 
sampling techniques  improved g eneralizability significantly.  
Giving interviewees ’ and survey respondents ’ example 
bugs and multiple -fix examples may have biased participants 
towards providing answers that aligned with those examples, a 
form of expectancy bias  (internal validity) . Howev er, we 
judged the threat of participants unable to recall implicit or 
 
Never  
Rarely  
Sometimes  
Usually  
Always  
Peer SDEs  1% 4% 27%  47%  17%  
Peer SDETs  4% 15%  37%  30%  8% 
My manager  9% 25%  40%  20%  3% 
My product manager  22%  30%  29%  9% 1% 
The people who 
wrote the code  2% 10%  36%  40%  9% 
Other experts  
(e.g., architects)  9% 30%  32%  10%  3% 
 
Table VI. Who is helpful to communicate with when choosing 
an optimal fix  
 
 
 1 2 3 to 5  6 to 10  11+ 
finding the 
cause of a bug  49%  38%  11%  0% 0% 
choosing a  
solution  24%  43%  31%  1% 0% 
implementing 
the solution  77%  16%  5% 0% 0% 
 
Table VII. How many people are involved in bug fixing activities  
 explicit design decisions outweighed this threat . Future r e-
searchers may be able to confirm or refute our results by using 
a research method that is more robust to expectancy bias.  
Still, some interviewees struggled with remembering the 
design decisions they made, and were generally unable to arti c-
ulate implicit decisions. This type of memory bias is inherent in 
most retrospective research methods. However, we attempted  
to control memory bias by asking  opportunistic interviewees to 
recall their most recently fixed bugs, asking firehouse inte r-
viewees to discuss a bug they just fixed, and asking survey 
respondents to look at bugs they had recently fixed . 
To meet our goal of not significantly interrupting partic i-
pants’ workdays, we kept our interview and survey short, 
which means w e were unable to collect contextual information 
that may have helped us better explain the results. For example, 
in the interviews, we did not ask questions about the gender or 
team structure, which may have some effect on bug fix design s.  
Similarly, a consequence of keeping the survey short is 
that participants may have misunderstood our questions. For 
example, in our survey, we asked engineer s whether they ever 
avoided filing a bug report; this question could be interpreted 
conservatively to mean, “whe n do you not report software fai l-
ures?”, when our intent was for “bug reports” to be interpreted 
broadly to include enhancements. While we tried to minimize 
this threat by piloting our survey , as with all surveys [14], we 
may still have miscommunicated with our respondents .  
VI. IMPLICATIONS  
The findings we present in this paper have several implic a-
tions , a handful of which we discuss in this section . 
Additional Factors in Bug Pred iction  and Localization . 
Previous research has investigated several approaches to pr e-
dicting future bugs based previous bugs  [20] [21], including 
our own  [3]. The intuition behind these approaches appears 
reasonable: how engineer s have fixed bugs  in the past is a good 
predictor of how they should fix bugs in the future. However, 
the empirical results we present in this paper suggest a host of 
factors can cause a bug to be fixed in one way at one point in 
time, but in a completely different way at another. For exa m-
ple, a bug fixed just before release is likely to be fixed diffe r-ently than a bug fixed during the planning phase.  As a result , 
future research in prediction and localization may find it useful 
to incorporate, when possible, these factors into their models.  
Limits  of Bug Prediction  and Localization . Although i n-
corporating some factors, such as development phase, into hi s-
torical bug prediction may improve the accuracy of these mo d-
els, some factors appear practically outside the reach of what  
automated predictors can consider.  For example, when analy z-
ing past bugs, it seems unlikely that an automated predictor can 
know whether or not a past fix was made with an engineer ’s 
full knowledge of why the bug occurred.  
Refactoring while Fixing Bugs . The results of our study 
suggest that engineer s frequently see code that should be refa c-
tored, yet still avoid refactoring. One way that this problem 
could be alleviated is through wider spread use of refactoring 
tools, which should help engineer s refactor  without spending 
excessive time doing so and at minimal risk of introducing new 
bugs. At the same time, such tools remain buggy [22] and diff i-
cult to use [23], so more research in that area is necessary . 
Usage  Analy tics. In our study, it appeared that engineer s 
often made decisions about how to fix bugs without a data -
driven understanding of how real users use their software. 
While a better understanding would clearly be beneficial, gat h-
ering and querying that data appears to be time consuming. 
Microsoft, like many companies, has been gathering software 
usage data for some time, but querying that data requires engi-
neers to be able to find and combine the right data sources, and 
write complex SQL queries. We envision a future where engi-
neers, while deciding the design of a bug fix, can quickly query 
existin g usage data with an easy -to-use tool. To build such a 
tool, research is first needed to discover what kinds of que s-
tions engineer s ask about their usage data, beyond existing 
“questions engineer s ask” studies [24].  
Fix Reconsideration . Engineer s in our study reported 
needing to reconsider bug fixes in the future, but sometimes 
used ad -hoc mechanisms for doing so, such as writing TODOs 
in code. Some of these mechanisms may be  difficult to keep 
track of; for example, which TODOs should be considered 
sooner rather than later . Engineers need a better mechanism to 
reconsider fixes in  the future, as well the time to do so.  
 
 
Never  
Rarely  
Sometimes  
Usually  
Always  
The bug is unlikely to ever be fixed  30%  31%  30%  7% 1% 
Whether or not the bug gets fixed has little impact on the software I’m developing  41%  26%  25%  4% 1% 
I don’t know where to file the bug or who to report it to  52%  27%  13%  6% 0% 
Filing this bug dilutes the urgency of bugs I think are more important to fix  61%  20%  13%  3% 1% 
A bug puts pressure on a colleague to fix the problem; I don’t want to add to his or her 
workload  72%  16%  8% 1% 0% 
Adding another report makes it look like the software is of poor quality or that the team is 
behind  80%  12%  5% 1% 0% 
 
Table VIII . Frequency of reasons for not filing bugs  
 VII. CONCLUSION  
In this paper, we have described a study that combined o p-
portunistic interviews, firehouse interviews , meeting observ a-
tion, and a survey. Our results describe a multi -dimensional 
design space for bug fixes , a space that engineer s navigate by, 
for example, selecting a fix that is least disruptive whenever a 
release looms near. While our study has not investigated a new 
practice, we have taken the critical first step towards unde r-
standing a practice that engi neers have always engaged in, an 
understanding that will enable researchers, practitioners, and 
educators to better understand and improve bug fixes.  
ACKNOWLEDGMENT  
Emerson Murphy -Hill was a Visiting Researcher and M i-
crosoft when this work was carried out.  Thanks to  all partic i-
pants in our study, as well as  Alberto Bacchelli, Andy Begel , 
Nicolas Bettenburg, Rob De Line, Jeff Huang, Ekrem Koc a-
guneli, Tamara Lopez, Pat rick Morrison, Shawn Phillips, Jul i-
ana Saraiva, and Jonathan Silitto . 
VIII. BIBLIOGRAPHY  
[1] Andreas Zeller, "Causes and Effects in Computer 
Programs," in Fifth Intl. Workshop on Automated and 
Algorithmic Debugging , Sept. 2003.  
[2] Albert Endes, "An analysis of errors and their causes in 
system programs," in International Conference on 
Reliable Software , 1975, pp. 327 -336. 
[3] Sunghun Kim, Thomas Zimmermann, Whitehead, James 
Jr., and Andreas Zeller, "Predicting Faults from Cached 
History," in Proceedings of ICSE , 2007, pp. 489 --498. 
[4] Lucia, F. Thung, D. Lo, and Lingxia o Jiang, "Are faults 
localizable?," in Working Conference on Mining Software 
Repositories , june 2012, pp. 74 -77. 
[5] M. Leszak, D.E. Perry, and D. Stoll, "A case study in root 
cause defect analysis," in Proceedings of ICSE , 2000, pp. 
428 -437. 
[6] Andre w J. Ko and Parmit K. Chilana, "Design, 
discussion, and dissent in open bug reports," in 
Proceedings of iConference , 2011, pp. 106 --113. 
[7] Silvia Breu, Rahul Premraj, Jonathan Sillito, and Thomas 
Zimmermann, "Information needs in bug reports: 
improving cooperation between developers and users," in 
Proceedings of the Conference on Computer Supported 
Cooperative Work , 2010, pp. 301 -310. 
[8] Christian Bird et al., "Fair and balanced?: bias in bug -fix 
datasets," in Proceedings of ESEC/FSE , 2009, pp. 121--
130. 
[9] Zuoning Yin, Ding Yuan, Yuanyuan Zhou, Shankar 
Pasupathy, and Lakshmi Bairavasundaram, "How do 
fixes become bugs?," in Proceedings of FSE , 2011, pp. 
26--36. 
[10] Jorge Aranda and Gina Venolia, "The secret life of bugs: 
Going past the errors  and omissions in software 
repositories," in Proceedings of ICSE , 2009, pp. 298 --308. 
[11] Diomidis Spinellis et al., "Evaluating the Quality of Open 
Source Software," Electronic Notes on Theoretical 
Computer Science , vol. 233, pp. 5 --28, Mar. 2009.  
[12] M.A. Storey, J. Ryall, R.I. Bull, D. Myers, and J. Singer, 
"TODO or to bug," in Proceedings of ICSE , may 2008, 
pp. 251 --260. 
[13] John Anvik, Lyndon Hiew, and Gail C. Murphy, "Who 
should fix this bug?," in Proceedings of ICSE , 2006, pp. 
361--370. 
[14] Forrest Shull, Janice Singer, and Dag I.K. Sjoberg, Guide 
to Advanced Empirical Software Engineering .: Springer -
Verlag New York, Inc., 2007.  
[15] Baruch Fischhoff and Ruth Beyth, "'I knew it would 
happen': Remembered probabilities of once -future 
things.," Organizational Behavior & Human 
Performance , vol. 13, pp. 1 --16, Feb. 1975.  
[16] C.B. Seaman, "Qualitative methods in empirical studies 
of softwa re engineering," IEEE Transactions on Software 
Engineering , vol. 25, pp. 557 -572, jul/aug 1999.  
[17] Everett M. Rogers, Diffusion of Innovations, 5th Edition , 
5th ed.: Free Press, aug 2003.  
[18] T. Punter, M. Ciolkowski, B. Freimut, and I. John, 
"Conduc ting on -line surveys in software engineering," in 
Proceedings of Empirical Software Engineering , sept. -1 
oct. 2003, pp. 80 - 88. 
[19] Alberto Bacchelli and Christian Bird, "Expectations, 
Outcomes, and Challenges of Modern Code Review," 
Microsoft Research,  MSR -TR-2012 -85 2012.  
[20] T.J. Ostrand, E.J. Weyuker, and R.M. Bell, "Predicting 
the location and number of faults in large software 
systems," IEEE Transactions on Software Engineering , 
vol. 31, pp. 340 --355, Apr. 2005.  
[21] Ahmed E. Hassan and Richard C. Holt, "The Top Ten 
List: Dynamic Fault Prediction," in Proceedings of the 
International Conference on Software Maintenance , 
2005, pp. 263 --272. 
[22] G. Soares, R. Gheyi, and T. Massoni, "Automated 
Behavioral Testing of Refac toring Engines," IEEE 
Transactions on Software Engineering , 2012.  
[23] Emerson Murphy -Hill, Chris Parnin, and Andrew P. 
Black, "How we refactor, and how we know it," in 
Proceedings of ICSE , 2009, pp. 287 --297. 
[24] Thomas Fritz and Gail C. Murphy, "Using  information 
fragments to answer the questions developers ask," in 
Proceedings of ICSE , 2010, pp. 175 --184. 
 
 
 
 