The Impact of Fault Models on Software Robustness
Evaluations
Stefan Winter
TU Darmstadt
sw@cs.tu-darmstadt.deConstantin S√¢rbu
TU Darmstadt
cs@cs.tu-darmstadt.deNeeraj Suri
TU Darmstadt
suri@cs.tu-darmstadt.de
Brendan Murphy
Microsoft Research
bmurphy@microsoft.com
ABSTRACT
Following the design and in-lab testing of software, the eval-
uation of its resilience to actual operational perturbations in
the eld is a key validation need. Software-implemented fault
injection (SWIFI) is a widely used approach for evaluating
the robustness of software components. Recent research [24,
18] indicates that the selection of the applied fault model has
considerable inuence on the results of SWIFI-based evalua-
tions, thereby raising the question how to select appropriate
fault models (i.e. that provide justied robustness evidence).
This paper proposes several metrics for comparatively eval-
uating fault models's abilities to reveal robustness vulnera-
bilities. It demonstrates their application in the context of
OS device drivers by investigating the inuence (and rela-
tive utility) of four commonly used fault models, i.e. bit ips
(in function parameters and in binaries), data type dependent
parameter corruptions , and parameter fuzzing . We assess the
eciency of these models at detecting robustness vulnerabil-
ities during the SWIFI evaluation of a real embedded operat-
ing system kernel and discuss application guidelines for our
metrics alongside.
Categories and Subject Descriptors
C.4 [Performance of Systems ]: Fault tolerance
General Terms
Measurement, Performance, Reliability
Keywords
Robustness Testing, Fault Injection, Fault Models
1. INTRODUCTION
Under a constant feature-driven market pressure and due
to their ever increasing complexity, many software appli-
cations are often released without being suciently tested.
Even if a software component1is considered to be suciently
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proÔ¨Åt or commercial advantage and that copies
bear this notice and the full citation on the Ô¨Årst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciÔ¨Åc
permission and/or a fee.
ICSE ‚Äô11, May 21‚Äì28, 2011, Honolulu, Hawaii, USA
Copyright 2011 SWMF 978-1-4503-0445-0/11/05 ...$10.00.tested for one application scenario or operational environ-
ment, it may be insuciently tested for reuse in another one.
Especially commercial-o-the-shelf (COTS) commodity soft-
ware components pose a problem in this respect. They are
subject to (re)use in a variety of application scenarios that
may well be unforeseen by their developers. This lack of
knowledge on the intended application scenario makes it dif-
cult for developers to estimate the suciency of their veri-
cation and validation eorts.
In contrast, the users of COTS components are aware of
the application scenarios in which they are planning to use
them. However, their ability to suciently test COTS com-
ponents is frequently also limited, as they usually do not have
access to the components's source code or other information
available to the components's developers. Furthermore, the
same component may be applied in multiple dierent oper-
ational environments within the same application: the same
COTS operating system can for instance be applied for both
the client and the server in a distributed client-server ap-
plication. Since the operational conditions for these dier-
ent applications of the same system dier, the user of this
component needs to test the component for both of them
dierently. Thus, failures frequently result when the oper-
ational environment of the deployed system diers from the
pre-release lab congurations used in the testing phase of the
software development process.
To cope with this deployment variability aspect, SWIFI
is a popular technique for evaluating the robustness of com-
modity commercial software components with respect to un-
expected operational conditions. In order to assess whether
a software component is \suciently" robust for release, it is
exposed to operational perturbations in a controlled manner
while its reactions are closely monitored.
The problem with such approaches is that they are well
suited for demonstrating the presence of vulnerabilities, but
not the absence thereof. In order to show the absence of vul-
nerabilities using SWIFI, it would be necessary to expose the
component under evaluation (CUE) to every possible pertur-
bation. While this is theoretically possible if the component's
input space is nite, it is generally considered impracticable
as the problem is equivalent to that of exhaustive testing [11].
SWIFI-based robustness evaluations therefore require se-
lecting a processable number of perturbations for injection.
This raises the question how to select the injected perturba-
1We adopt Szyperski's notion of software components: units
of composition with contractually specied interfaces and ex-
plicit context dependencies only [30].tions to maximize the robustness evidence derivable from the
experimental evaluation . A common solution to this prob-
lem is provided by application-specic operational proles ,
i.e. probability distributions for stimuli that software is being
exposed to during operation. Their application to the per-
turbation selection problem is based on the argument that
a total absence of vulnerabilities is a too strong condition,
since a vulnerability does not necessarily result in a robust-
ness violation. It does so only if the component is exposed to
a perturbation which exploits this vulnerability. This reduces
the problem to proving the absence of all vulnerabilities that
can be exploited during operation. For this purpose opera-
tional proles of the component are derived from its intended
operational context, including (but not limited to) typical
classes of perturbations, i.e. classes of perturbations that are
expected to be frequently encountered during operation.
However, this approach has two major drawbacks. First,
the robustness evidence derived from such an evaluation is
only valid for the considered operational context of the eval-
uated component; this implies that a component needs to be
re-evaluated for every intended context, thereby impeding
its reusability. Second, a vulnerability, whose exploitation
is highly unlikely in a given application scenario but whose
exploitation consequences are of disastrous impact, would be
ignored by operational proles. However, critical system fail-
ures often result from highly unlikely and hence unexpected
conditions that are by denition not covered by operational
proles [10, 31, 21].
Paper Contributions. Considering these drawbacks, we
investigate an alternate approach based on the idea that the
absence of vulnerabilities can be approximated by maximiz-
ing vulnerability detection and removal. Instead of selecting
typical , application-specic perturbation classes for robust-
ness testing, we give preference to those perturbation classes
that make our evaluations most ecient in terms of detected
vulnerabilities and the required eort for their detection.
Consequently, this paper makes the following contributions
to the current state of the art:
a method to compare the eciency of dierent pertur-
bation classes (termed fault models ) for SWIFI robust-
ness evaluations;
a set of fault model eciency metrics for this purpose;
a pragmatic set of guidelines for the application of these
metrics in SWIFI-based robustness evaluations.
As a case study, we perform SWIFI experimentation on an
embedded operating system (OS) kernel (Windows CE 4.2)
and demonstrate the eectiveness of our approach, compar-
ing four commonly applied fault models.
The paper is organized as follows: Section 2 introduces
basic terminology and related work. Section 3 introduces
the metrics developed for cross-model eciency evaluations,
whose application is demonstrated in an experimental eval-
uation of our approach presented in Section 4 along with a
demonstration and discussion of their utility in Section 5.
2. BACKGROUND AND RELATED WORK
2.1 Robustness Notion and System Model
In this paper we adopt the robustness notion from [12],
according to which robustness is \the degree to which a system
or component can function correctly in the presence of invalid
inputs or stressful environmental conditions."Invalid inputs and stressful environmental conditions are
termed perturbations . If a perturbation causes a software
component to enter an erroneous state, the component pos-
sesses a vulnerability which is activated by the perturbation.
Perturbations are equivalent to what is called external faults
in [3], whereas robustness vulnerabilities are termed internal
faults . The application of (external) fault injection is hence
a sound method for robustness evaluations.
A software CUE is expected to interact with other soft-
ware components via explicit interfaces only. Being part of
a composition, components are expected to provide services
that are relevant for the implementation of the composition's
functionality .Interfaces are sets of services and therefore
constitute the means by which each component's functional-
ity can be accessed.
Figure 1: Contribution of our work in the context of
SWIFI-based robustness evaluations
Figure 1 displays the considered robustness evaluation meth-
odology. We conduct robustness evaluations of a CUE by
injecting perturbations into its runtime environment dur-
ing controlled executions. Perturbations are introduced into
CUE Servers , i.e. components oering services to the CUE
and on whose reliable provision of service the CUE depends.
In order to trigger interactions of the CUE with the CUE
Servers, CUE Clients use services provided by the CUE, i.e.
create a workload for the CUE.
Since we consider direct interactions across the software
components, any perturbation aecting a component must
be mediated by its interfaces. We therefore consider direct
fault injections into data passed to the CUE via the interface
under evaluation (IUE) as well as injections into the binaries
of CUE Server components interacting with the CUE via the
IUE. We do not consider injections into the CUE itself, since
the robustness of a software component is dened by its fault
tolerance with respect to external faults.
A CUE failure detector monitors the CUE, its clients, and
its servers for symptoms of predened CUE failure modes2
of interest. The results of injection experiments are reported
to a component external to the CUE's runtime environment
for oine analysis from which robustness properties of the
CUE are derived according to a set of robustness metrics.
For our discussion of SWIFI-based robustness evaluations,
the following terms are used. An injection run refers to
2Failure modes describe how a component can possibly fail,
e.g. by becoming unresponsive.Table 1: Fault models for robustness evaluations in the literature
Framework/Authors Fault Location Fault Type Fault Latency Injection Trigger
MAFALDA [2]CUE ServerSEUTransient1stoccurrenceIUEMBUPermanentDT
Albinet et al. [1] IUE DT Transient 1stoccurrence
Kalakech et al. [19] IUESEUTransient 1stoccurrenceDT
Xception [6]SEUTransient1stoccurrence
CUE Server MBUIntermittentnthoccurrence
DTPermanentTimer
IUE FZ X-call
G-SWFIT [8] CUE ServerCodingPermanent 1stoccurrencemistakes
Medon ca & Neves [23] CUE ServerCodingPermanent 1stoccurrencemistakes
Johansson [15] IUESEU Transient1stoccurrence
nthoccurrence
DT Intermittent Timer
FZ PermanentX-call
Call Blockcomponent
CUE under
evaluation
interface
IUE under
evaluation
single
SEU event
upset
multiple
MBU bit
upset
data type
DT dependent
corruption
FZ fuzzing
the injection of a specic fault, the execution of the work-
load, and the observation (and logging) of the injection's
eects. An injection campaign is a collection of injection
runs, pertaining to a specic fault model, workload, targeted
CUE Server component, and IUE. A set of injection cam-
paigns targeting the same CUE is called an evaluation of that
CUE, possibly including multiple fault models, workloads,
and IUEs (thus, also multiple CUE Server components).
The approach presented in this paper uses experimental
data of injection campaigns performed for robustness evalua-
tions to evaluate the applied fault model's eciency. This
information can be used to introduce a feedback loop as
highlighted in Figure 1, providing guidance on the selection
of perturbations for subsequent campaigns in an evaluation.
The required information is dened by a set of metrics intro-
duced in Section 3.
2.2 Comparative Fault Model Evaluations
We adopt the fault model notion from [16, 18, 17], where
fault models3for SWIFI-based robustness evaluations of op-
erating systems (OSs) were dened by three basic attributes:
thefault location (where to inject), the fault type (what to in-
ject), and the fault timing (when to inject), where the latter
was further qualied as injection trigger and fault latency .
Table 1 lists a number of existing SWIFI frameworks that
have been applied for software robustness evaluations similar
to those we consider in this paper, along with their reported
fault model support. Fault models are usually discussed only
implicitly in the literature and some SWIFI frameworks pro-
vide mechanisms for exibly extending the set of supported
fault models, e.g. by user dened injection triggers or fault
types. Thus, Table 1 is likely incomplete, but already in-
dicates potentially large numbers of applicable fault models
oered to evaluators, e.g. more than 90 possible attribute
combinations for Xception [6].
The spectrum of fault types in the related literature are
3also termed as error models in the referenced paperssingle bit ips (single-event upset, SEU), multiple simultane-
ous bit ips (multiple bit upset, MBU), data type dependent
corruptions of data elds or parameters (DT), and substitu-
tion by random bit pattern (fuzzing, FZ). The fault latency
addresses the duration of an injection in terms of repeated
fault activation. Transient faults are activated exactly once,
intermittent faults are activated a nite number of times, and
permanent faults are activated every time. Injection trig-
gers determine when a fault is injected, respectively when
it can be activated for the rst time. It can be injected the
rst time its injection location is referenced, after N refer-
ences, after a predened amount of time has passed, after
some predened exception or function call occurs (X-call), or
after a predened sequence of function calls (Call Block).
Although a multitude of fault models for software robust-
ness evaluations are discussed in the literature, we are aware
of only two attempts to compare models. Moraes et al. [24]
compared the eects of fault injections at dierent locations,
i.e. at the IUE and inside of components providing services to
the CUE. They concluded that interface injections, which are
generally less costly in terms of implementation complexity
and execution time, are not a valid substitution for the em-
ulation of programming mistakes within CUE Servers. The
comparison covered only this single aspect, while other dif-
ferences, e.g. in terms of applied fault types, were not taken
into consideration.
Johansson et al. [18] compared the eects of three dierent
fault models for interface injections on robustness evaluations
of Windows CE .NET. However, the applied metrics were
only suited for this specic evaluation and restricted to a
specic CUE failure mode, i.e. system crashes. It is therefore
not applicable when dierent failure modes, e.g. incorrect
computation results, are considered most critical for a given
application. Furthermore, as the applied SWIFI framework
did not support injections into CUE Servers, the comparison
of models with diering fault locations as the one provided
in [24] was impossible.We reuse the SWIFI framework of [15, 18] and extend it
to perform comparisons, as initiated in [24]. In the follow-
ing section we extend the work presented in [18] and develop
generalized and formally accurate denitions the informally
specied metrics so that they can be used for (a) any robust-
ness evaluation that aligns with the generic method described
in Section 2.1 and (b) quantitative comparisons, i.e. we give
quantitative re-denitions providing ratio scale measures for
all considered metrics.
3. FAULT MODEL EFFICIENCY METRICS
We propose four metrics for capturing the eciency of fault
models by applying them to injection campaigns that uti-
lize the respective models. Two of the metrics are benet-
oriented , i.e. the measures are preferably maximized, and
the other two are cost-oriented , i.e. the measures are prefer-
ably minimized.
The proposed metrics are intended to lay the foundation of
objective fault model comparisons. They provide campaign-
granularity measures that can either be used for post-eval-
uation comparisons (in order to guide future evaluations of
similar systems) or in-evaluation comparisons (in order to
design campaigns based on evaluations of previous campaigns
within the same evaluation).
We would like to emphasize that the proposed metrics are
dened in a way that they reect each considered failure
mode of the CUE. Having separate measurements for dier-
ent failure modes enables evaluators to weigh the obtained
results according to the (application-specic) severity associ-
ated with the respective failure mode. Measurements for all
possible failure modes can be performed in one single evalu-
ation. If a dierent operational context is considered for the
same CUE, only a modication of these weights is required
instead of a re-evaluation.
3.1 Metric 1: IUE Coverage
The IUE Coverage of a fault model is the extent to which
it enables the identication of vulnerable services provided
by the CUE. A model's coverage cannot directly be used
for comparisons across dierent CUEs, because no relative
measure can be established due to the unknown total number
of vulnerable services in the given evaluation context. It may,
however, be used for the comparison of dierent fault models
applied in robustness evaluations of the same CUE. Unique
coverage denotes the number of vulnerable services detected
by only one model among all compared fault models.
For the denition of IUE coverage, a precise notion of ser-
vice vulnerability is required. A service of the CUE is termed
asvulnerable with respect to a certain failure mode if there
is a known injection run targeting this service that results in
a respective CUE failure.
Definition 1.For a given injection campaign c, letVfm(c)
denote the set of identied vulnerable services with respect to
failure mode fmand let SIUE
t(c)denote the set of all services
in the IUE that are targeted during campaign cwith IUE(c)
denoting the IUE targeted in c. The IUE coverage covIUE
fm(c)
of this campaign with respect to fmis then dened by the
cardinalities of VfmandSIUE
t(c)as
covIUE
fm(c) =jVfm(c)j
jSIUE
t(c)j(1)
For a set of injection campaigns C=fc1; c2; : : : ; c ngthat
only dier in terms of the applied fault model, the numberof services uniquely covered during some injection campaign
ci2Cis given by
covIUE
fm(ci) =jVfm(ci)nS
cj2C
j6=iVfm(cj)j
jSIUE
t(ci)j(2)
We have dened coverage to reect the targeted IUE as we
have observed signicant dierences depending on the ap-
plied fault model and the IUE. This information is lost when
coverage is dened globally for all services that the CUE uses
for interaction. Consider for instance the case where a CUE
has two interfaces with unequal numbers of services and that
each interface is exclusively targeted by some specic fault
model. Then each model may cover all of the services be-
longing to the interface it targets. Overall, the model target-
ing the \larger" interface would yield a better coverage (and
a better unique coverage) and might consequently be given
preference over the model targeting the \smaller" interface.
However, it would be transparent to the evaluator that this
seemingly preferable model fails to cover any service in the
other (potentially highly critical) interface.
If an injected fault triggers multiple vulnerabilities, the ef-
fect of one fault may be masked by the eect of another fault
and may therefore remain undetected. This can be inter-
preted as a weakness of the applied fault model that fails to
trigger the vulnerabilities independently, which is reected by
the denition of coverage. The advantage of triggering mul-
tiple vulnerabilities in a single run vanishes with the ability
to detect them. As coverage denotes the ability of a fault
model to trigger present robustness vulnerabilities, it is a
benet-oriented metric and thus preferably maximized.
3.2 Metric 2: Injection EfÔ¨Åciency
Injection eciency is dened by the number of failures ob-
served per injection. This metric puts the number of observed
failures (which is considered a measure for the experiments's
eectiveness) in relation to the number of injections neces-
sary to provoke this number of failures.
If several distinct failure modes are considered in a ro-
bustness evaluation, the obtained injection eciencies have
to be weighted according to (application-specic) robustness
requirements of the CUE's intended application. Since the
presented metric is intended to abstract as far as possible
from concrete application requirements, a separate injection
eciency measure is considered for every failure mode. With
respect to possible error masking eects in cases where multi-
ple vulnerabilities are triggered during a single injection run,
the inutility argument from the coverage metric introduction
applies equally and is consistently reected by the metric def-
inition. As injection eciency displays the ability of a fault
model to trigger vulnerabilities leading to failures of a par-
ticular failure mode, it is a benet-oriented metric and thus
preferably maximized.
Definition 2.If for a given injection campaign cthe num-
ber of injection runs is denoted by r(c)and for each failure
mode fmthe number of observed failures is denoted by ffm(c),
then the injection eciency iefm(c)is given by
iefm(c) =ffm(c)
r(c)(3)
3.3 Metric 3: Average Execution Time
The average execution time of an injection run shows how
eciently a robustness evaluation can be performed with aspecic model. Since an absolute measure (for a whole eval-
uation) or even a per-injection measure would ignore the fact
that the outcome of an injection inuences the execution time
dramatically (e.g. system crashes usually require a restart),
the metric must also take the outcome of each injection run
into consideration.
Hence, the execution time for the (in terms of correlated
processing overhead) lowest common failure mode among
evaluations with dierent fault models is a promising indica-
tor. However \System Crash" or \Hang" failure modes should
be excluded, as the timeouts usually applied for their detec-
tion can strongly inuence the results. A potentially consid-
ered \No Failure" mode might as well be excluded because
system failures eventually necessitate further processing of
the results, for instance to provide more detailed (potentially
fault type specic) logging and reporting functionality. How-
ever, as there may be scenarios when an inclusion of this
mode is indispensable for provision of meaningful results4,
its assessment is highly recommended even if its contribution
in a specic comparison may be of minor signicance. The
average execution time per injection run is a cost-oriented
metric and, hence, preferably minimized.
Definition 3.If for a given injection campaign cthe cu-
mulated execution time of all injection runs that resulted in
failure mode fmis denoted by cetfm(c)and the number of ob-
served failures of mode fmis given by ffm(c), the execution
time etfm(c)for each failure mode is dened as
etfm(c) =cetfm(c)
ffm(c)(4)
3.4 Metric 4: Implementation Complexity
The implementation complexity of a fault model indicates
the required eort to set up a fault injection mechanism for
the respective model. This may be measured a posteriori by
the amount of implementation time (e.g. in so-called man
hours of uninterrupted work), source lines of code (SLOC),
or any other software complexity measure as long as it is
measured uniformly for all model implementations.
Any implementation complexity metric suitable for fault
model comparison should mainly rely on comparisons of im-
plementation eorts inherent in the model's requirements or
properties. If, for example, the injection mechanism of one
model requires the modication of one service invocation,
while another requires setting up a table and monitoring a
number of invocations before the actual injection can take
place, the implementation complexity is clearly higher than
in the rst case, since additional logic and data structures
are required. Notably, these attributes are related to prop-
erties of the applied fault model and not a purely syntactical
complexity measure of the actual injector's implementation.
We have considered several approaches for estimating the
implementation eort a priori on the basis of functionality-
related properties, e.g. IFPUG function points [14] and COS-
MIC full function points [13], but none of them seems to be
suciently accurate without experience in terms of either ex-
pert knowledge or statistical data on previous projects.
We therefore decided to use Delivered Source Instructions
(DSI) to measure implementation complexity a posteriori.
Being a SLOC metric, DSI possesses the disadvantages of
4e.g. if there are only two distinct failure modes or in case
of comparing fault models evaluated among experimental se-
tups with dierent CUE failure modes/detectorspurely syntactical metrics discussed above. However, as con-
ceptually preferable function point metrics cannot be ex-
pected to provide more accurate measures without a con-
siderable amount of expertise, DSI is chosen as a more easily
applicable alternative. Restrictions only apply to measure-
ment comparisons among dierent developers, implementa-
tion languages, or IDEs. If these are the same for all com-
pared models, syntactical metrics enable valid cross-model
comparisons. In our targeted scenario, the application of dif-
ferent fault models with one given injection framework main-
tained by one (or only few) evaluators is being considered.
Another a posteriori measure used to compensate the lack
of a direct functional measure is McCabe's cyclomatic com-
plexity [22]. While DSI measures implementation complexity
as the amount of delivered code lines, cyclomatic complexity
takes a more algorithmic and less implementation specic ap-
proach, considering the control ow of the algorithm rather
than the size of its implementation.
Both implementation complexity measures are cumulated
for all source code related to the fault model implementation.
Definition 4.The implementation complexity of a fault
model is given by the DSI sum icDSIaccumulating the DSIs
of all related source code les and the sum of the cyclomatic
complexities of all associated functions iccyc.
In accordance with Boehm's denition in [5], the number
of DSIs equals the number of non-comment source lines.
DSI fullls COCOMO's [4] requirements on code counting
and the obtained measures can thus directly be applied for
COCOMO estimations by any software project manager us-
ing estimation variable values that match the situation of his
or her particular development team best.
Our denition of implementation complexity does not in-
clude eorts for instrumenting software components or for
setting up the CUE's operational environment for the evalu-
ation. The reason is that most modern fault injection frame-
works perform instrumentations for fault injection online to
enable more exible injection triggers. In order to inject a
transient fault into a CUE Server component after a cer-
tain time interval, for instance, the CUE Server needs to be
modied during operation in the course of the evaluation.
Such instrumentations are therefore captured by the execu-
tion time metric. Furthermore, we do not consider eorts
related to installing already implemented fault models, as we
are not aware of any fault injection framework that enables
an extension by existing fault model implementations from
other frameworks. We also do not consider eorts for the ini-
tial setup of the injection framework, because these depend
more on the actual framework than on a specic fault model.
We are aware of the limitations of this combined met-
ric to adequately measure implementation complexity, but
nonetheless suggest its application since all contending met-
rics that we are aware of have similar limitations. Both DSI
and cyclomatic complexity provide ratio scale measures. Im-
plementation complexity is a cost-oriented metric and, thus,
preferably minimized.
4. METRIC APPLICATION
We have validated the applicability of the proposed met-
rics, applying four dierent fault models in a robustness eval-
uation of the Windows CE 4.2 kernel. In the following we
present the experiment setup and the results we obtained
using the previously introduced metrics.4.1 Experiment Setup
Figure 2 provides an overview on the experiment setup.
The CUE is the OS kernel of Windows CE .NET 4.2. The
CUE Server components targeted for injection are device
drivers, used by the CUE to access the system's hardware.
We chose the OS's driver interfaces for evaluation, as drivers
constitute a major cause for OS outages [7, 28, 9].
Operating System
Target driverInjector TrackerExperiment
ManagerTest Applications
- Exp. Setup
- Exp Synch.- Logging- RestartingHost
Computer
Figure 2: Setup for experimental evaluations
The targeted drivers are a serial port driver, an Ether-
net driver, and a CompactFlash card driver. Each of these
drivers provides an interface (the driver's exported interface,
EXP) to the OS kernel and in turn uses a number of in-
terfaces provided by the kernel, i.e. the Device Driver Kit
(DDK) [26], kernel core functionality (CORE) [25], and the
Network Driver Interface Specication (NDIS) [27].
Test applications constitute the CUE Clients intended to
trigger the execution of targeted services. We use test ap-
plications specically designed to trigger executions of the
targeted drivers.
The fault injectors are implemented as software wrappers
located between the CUE and the targeted CUE Servers, in-
tercepting mutual service invocations. Faults are injected
either by corrupting parameters of intercepted service invo-
cations or by modifying drivers's binary images.
We evaluate the performance of four dierent fault models
for a robustness evaluation of the CUE using the SWIFI tool
from [15]. Three models have been implemented for extensive
experimentation by our group [18, 17]. The fourth model was
additionally implemented to enable comparisons of models
with diering fault locations, such as in [24]. The applied
fault types are representative for a large class of fault models
(cf. Table 1 in Section 2) spanning:
Parameter Bit Flips (BF) A parameter of an inter-
cepted service call in the IUE is altered by changing the value
of one or more bits in its binary value. This model is intended
to simulate single event upsets (SEUs) in hardware compo-
nents that propagate through the CUE Servers to the IUE.
Injections of these faults are frequently performed at the IUE
for eciency considerations.
Data Type Dependent Parameter Corruption (DT)
A parameter of an intercepted service call in the IUE is
changed to another value of the same data type. The main
reason for taking the range of a data type into account when
altering a value is the considered type of faults. If a data-
type-related fault results from a programming mistake in a
software component, then at least obvious violations of a pro-
gramming language's type concept can be detected at com-
pile time and removed prior to deployment. Thus, a largefraction of the faults that remain in a deployed component
can be expected to result in erroneous run time values that
are of the same or a similar data type as the correct value.
Parameter Fuzzing (FZ) Parameters of intercepted ser-
vice calls in the IUE are replaced by random values, uni-
formly selected across all possible values for the system ar-
chitecture's word size. As opposed to the BF and DT fault
models, FZ is inspired by random testing rather than sys-
tematic test case derivation.
Single Event Upsets in Binaries (SEU) Faults are in-
jected by randomly ipping single bits in CUE Server bina-
ries. The fault type is derived from SEU eects that physical
perturbations have on computer hardware, i.e. bit ips in
memory and the processing circuitry. The SEU model diers
from the BF model only in terms of the injection location.
We restrict our comparison to fault models with equal fault
timing properties in order to single out the eects of diering
fault types and locations more clearly. We chose to inject
transient faults occurring on the rst call to a targeted service
in order to minimize the runtime of our comparison.
The experiment manager component controls the execu-
tion of experiments. While the interceptors (i.e. tracker and
injector in Figure 2) are monitored for experiment progress,
the CUE, its Servers, and Clients are monitored for fail-
ures during an injection run. Evaluation-relevant events are
logged by sending them to a logging server on the host com-
puter. After the completion of an injection run, the target
manager restarts the system in order to run the next injec-
tion on a clean system image that does not carry any pos-
sibly induced dormant faults or errors. This is necessary to
keep individual injection runs as independent as possible and,
thus, their results individually reproducible.
We consider four dierent CUE failure modes that have
been discussed frequently in the literature (cf. [20]).
No Failure (NF) No eect is detected. An injected fault
may be either not activated or masked by the OS.
Application Error (AE) The injected fault is activated
and the error propagates to the test applications, but no ser-
vice specication is violated. This is for instance the case
when the OS detects an erroneous state and returns an ex-
ception to the test application. Incorrect results returned
by an OS service that nonetheless satisfy its specication in
terms of robustness (i.e. that are wrong but still within the
valid data range for the result) also fall in this category. This
failure mode does not cover incorrect results that violate the
specied data ranges of services.
Application Hang (AH) The injected fault is activated
and the error propagates to the application interface. The
specication of the fault triggering OS service is violated.
The detectable eect is either an incorrect result that vio-
lates the specied data range of the service, abnormal test
application termination, or lacking or wrong error codes.
System Crash (SC) The injected fault is activated and
causes the OS to hang or crash, meaning that it stops to pro-
vide services to any client. Although the system may manage
to reboot autonomously in some cases, external monitoring
and control is generally required for SC failure detection and
the system is usually recovered by a manual reset.
4.2 Experimental Results
As the performed injection campaigns only dier in terms
of the applied fault models and the injection target, these
two properties are used to identify particular campaigns in
the following discussion.Table 2: Coverages for failure mode Application Error (AE) [%]
Campaign covDDK
AE covDDK
AE covCORE
AE covCORE
AE covNDIS
AE covNDIS
AE covEXP
AE covEXP
AE
cBF 71.43 0.0 31.03 0.0 57.14 0.0 30.77 0.0
cDT 71.43 0.0 27.59 1.15 9.92 0.0 0.0 0.0
cFZ 28.57 14.29 40.23 16.09 64.29 7.14 0.0 0.0
cSEU 0.0 0.0 0.0 0.0 0.0 0.0 92.31 46.15
Table 3: Coverages for failure mode Application Hang (AH) [%]
Campaign covDDK
AH covDDK
AH covCORE
AH covCORE
AH covNDIS
AH covNDIS
AH covEXP
AH covEXP
AH
cBF 14.29 0.0 14.94 4.60 57.14 57.14 7.69 0.0
cDT 28.57 0.0 13.79 4.60 0.0 0.0 15.38 0.0
cFZ 57.14 42.86 25.29 13.79 0.0 0.0 30.77 7.69
cSEU 0.0 0.0 0.0 0.0 0.0 0.0 53.38 23.08
Table 4: Coverages for failure mode System Crash (SC) [%]
Campaign covDDK
SC covDDK
SC covCORE
SC covCORE
SC covNDIS
SC covNDIS
SC covEXP
SC covEXP
SC
cBF 14.29 0.0 18.39 5.75 35.71 14.29 0.0 0.0
cDT 14.29 0.0 9.20 0.0 21.43 0.0 0.0 0.0
cFZ 14.29 0.0 10.34 2.30 7.14 0.0 0.0 0.0
cSEU 0.0 0.0 0.0 0.0 0.0 0.0 92.31 92.31DDKdevice
driver
kit
COREkernel core
functions
NDISnetwork
driver
interface
EXPexported
driver
interface
BFparameter
bit ips
DTdata type
dependent
parameter
corruption
FZparameter
fuzzing
SEUsingle-event
upsets in
binaries
Three of the proposed metrics (IUE coverage, injection ef-
ciency, execution time) were directly derived from the same
data that was collected for evaluating the robustness of the
CUE. Additional tools were used to assess the implementa-
tion complexity of the implemented fault models. DSIs were
counted using SLOCCount 2.26 [32]. McCabe's cyclomatic
complexity was measured using SourceMonitor 2.5 [29]. We
now overview the experimental results for the four targeted
metric types, and discuss their implications in Section 5. The
results were obtained by conducting more than 300 injections
per model and driver.
IUE Coverage The obtained coverage values are grouped
by detected failure modes in Tables 2, 3, and 4. For each
IUE (DDK, CORE, NDIS, EXP), the coverage covIUE
fmand
unique coverage covIUE
fmare given in the tables according to
Denition 1. The presented numbers reect the fraction of
all services in the respective IUE that were covered by each
model. The rst column of Table 2, for instance, provides
a comparison of the four applied fault models with respect
to the percentage of services in the DDK interface for which
they have detected vulnerabilities that led to AE failures.
The second column reveals that although BF and DT cover
a larger fraction of DDK services, FZ covers 14.29% of all
DDK services uniquely , i.e. these services are not covered
by BF or DT. A combination of FZ and either DT or BF
would thus yield a coverage of 85.72% of all services provided
by the DDK interface. A coverage of 0.0 means that no
service of the respective IUE was identied as vulnerable.
A unique coverage of 0.0 means that every covered service
was also identied by some other model. The presented data
noticeably contains identical numbers, e.g. 14.29 occurs ve
times. The reason is that every targeted interface comprises
less than 100 services. 14.29% is one out of 7, which is the
number of services used by the drivers in the DDK and the
NDIS interfaces.
Injection Campaign Eciency Figure 3 illustrates the
cumulated injection eciencies for each model and every tar-
geted driver. Regarding the SC failure mode, SEU outper-
BFDTFZSEU BFDTFZSEU BFDTFZSEU0%5%10%15%20%25%30%35%40%45%50%
AE
AH
SC
  Serial Port Driver   Ethernet Driver   Flash Disk DriverInjection efficiencyFigure 3: Comparison of the applied models's injec-
tion eciencies for each targeted driver
forms all other models for each targeted driver. For AE and
AH, no such clear statement can be made; the results also de-
pend on the targeted driver. However, except for the AH e-
ciency with the Ethernet driver as targeted CUE Server, the
highest eciency values are either obtained for the Fuzzing
model or the SEU model.
An injection eciency of 0 was obtained for AH failures
of the Ethernet driver when exposed to FZ faults and for
the ash disk driver when exposed to SEU faults, because no
such failures were detected during the respective campaigns.
The corresponding values are missing in the comparison of
average execution times for the same reason.
Average Execution Time The results obtained for the
execution time metric of injection runs with dierent exper-
iment outcomes are listed in Table 5. The units are minutes
and seconds (before and after the colon).
Experiments resulting in AH or SC failures tend to take
more time than experiments resulting in AE failures or no
failures as they usually imply failure detection by application
or system response timeout. If a faster detection mechanism
can be implemented for these failure modes, their executionTable 5: Average execution times (m:s)
Target Campaign et NF etAE etAH etSC
cBF 00:57 00:53 01:34 03:31
Serial Port cDT 00:33 00:34 01:15 04:39
Driver cFZ 01:06 01:14 01:55 03:37
cSEU 01:19 01:20 02:03 04:39
cBF 00:34 00:29 01:19 04:15
Ethernet cDT 00:42 00:39 00:39 02:24
Driver cFZ 00:51 00:48 { 03:12
cSEU 01:04 01:03 01:44 04:24
cBF 00:43 00:43 01:25 03:41
Flash Disk cDT 00:36 00:37 01:16 03:53
Driver cFZ 01:03 01:02 01:19 03:38
cSEU 02:01 01:57 { 05:09
Table 6: Implementation complexities
Model ic DSI iccyc
BF 133 30
DT 635 222
FZ 272 58
SEU 259 48
time and overall evaluation eciency will greatly improve.
The detection of AE failures seems to add no complexity
in terms of execution time; it is even less than the execution
time for NF experiments in more than half of the cases. Note
that we are considering SWIFI eciency drivers for basic
robustness evaluation needs. If the specic SWIFI objective
is to comprehensively nd robustness vulnerabilities, then
longer execution times are also valid.
If the execution time metric is applied as proposed in Sec-
tion 3, i.e. if models are compared with respect to the execu-
tion times for their lowest common failure modes, BF is the
fastest model for the Ethernet driver and DT is fastest for
the serial port and ash disk drivers. The least performant
models in terms of injection eciency perform best in terms
of execution time and vice versa. This relationship suggests
that the weakness of a model with respect to one metric may
be compensated by its strength with respect to another one.
Implementation Complexity Table 6 provides an over-
view on the assessed implementation complexity of the mod-
els. From both, the DSI count and the McCabe complexity,
DT is by far the most expensive model and BF is the least
expensive model investigated. Being slightly less expensive,
SEU's complexity hardly diers from FZ's.
We see that implementation complexity and execution time
do not correlate in general. The two most expensive models
in terms of execution time (SEU and FZ) are considerably
cheap in terms of implementation complexity and the cheap-
est model in terms of execution time (DT) is the most expen-
sive model in terms of implementation complexity. However,
BF is cheap in terms of both execution time and implemen-
tation complexity. We do add the note that the use (and ob-
tained rankings) of this metric is tied to the specic SWIFI
tool being used. We do not assert completeness or the eort
of setting up and conguring dierent SWIFI environments.
5. METRIC UTILITY
In the following we demonstrate the utility of our proposedmetrics by comparing all applied fault models using the pre-
viously presented measurements. Table 7 gives a simplied
overview of the results, assuming that all CUE failure modes
and IUEs are of equal interest to the evaluator. The rankings
were derived according to how often the models performed
best among all investigated models. The details of these ten-
dencies are highlighted and discussed onwards.
Parameter Bit Flips (BF) BF performs comparatively
well at identifying service vulnerabilities that lead to AE
failures in all targeted interfaces. However, it only detects
vulnerabilities that are also detected by other models. The
model performs best at identifying service vulnerabilities lead-
ing to AH and SC failures in the NDIS and (especially in
the case of SC failures) CORE interfaces, where it also de-
tects vulnerabilities that none of the other investigated mod-
els manages to detect. BF requires the least implementation
eort, but performs worst in terms of injection eciency. In
terms of execution time it is a fairly cheap model to use.
Data Type Dependent Parameter Corruption (DT)
Although it manages to identify a few unique AE vulnerabil-
ities, DT provides average to poor coverage compared to the
other investigated models. DT has the highest implementa-
tion complexity. This result is intuitive, since separate logic
is required for each considered data type used for data ex-
change in the OS/driver (CUE/Server) interface, leading to
a higher number of DSIs and a higher cyclomatic complex-
ity. Overall, DT has a fair injection eciency. It requires
the least amount of time per injection run, meaning that it
allows to perform more experiments than any other model
in a given amount of time. Although the DSI count for the
model is high, only small fractions of its code are actually
executed for a parameter corruption of a specic data type.
This is also indicated by the high cyclomatic complexity.
Parameter Fuzzing (FZ) The FZ model identies a
large number of vulnerabilities for all considered failure modes
and most targeted interfaces. Except for the EXP, CORE
and NDIS interfaces (the latter two only for SC failures), it
outperforms all other models in terms of unique coverage,
i.e. it detects large numbers of service vulnerabilities that
remain undetected by other models. In terms of implemen-
tation complexity, the FZ model comes at almost twice the
cost of the BF model, but is still less than half as costly
as DT. FZ outperforms BF and DT in terms of injection
eciency, but is outperformed by these models in terms of
execution time.
Single Event Upsets in Binaries (SEU) The SEU
model performs very well at identifying vulnerabilities in the
EXP interface but poorly at identifying vulnerabilities in any
other interface. This result is intuitive as, according to the
SEU model, faults are only injected to services belonging to
this interface, i.e. into the the driver binary and thus into
services exported by the driver. The implementation com-
plexity for the SEU model is slightly less than for the FZ
model. However, SWIFI mechanisms for supporting code
mutations with exible injection triggers require a consid-
erable implementation eort. The fraction of code that is
solely related to code mutation mechanisms accounts for a
total of 1050 DSIs with a cumulated cyclomatic complexity
of 262 in the applied SWIFI tool, in addition to about 50
lines of assembly code. However, if code mutations are na-
tively supported by a chosen SWIFI framework, SEU is a
modest model to implement. From an injection eciency
point of view SEU is, similarly to FZ, a very ecient model
with a particular strength in provoking SC failures, but isTable 7: In-practice observation on fault models
Model CoverageImplementation Injection Execution
Complexity Eciency Time
BF F F F ?F F F F F ? ? ? F F F ?
DT F F ? ?F? ? ? F F ? ?F F F F
FZ F F F F F F ? ? F F F ?F F ? ?
SEU F? ? ? F F ? ? F F F ?F? ? ?
Legend: ? ? ? ? Poor,F F F F Good.
(also like FZ) expensive in terms of execution time.
Derived Metrics As already hinted at in the previous
section, the comparative evaluation points out a trade-o be-
tween injection eciency and execution time for three out of
four models (DT, FZ, SEU). In order to quantify this trade-
o more precisely for cross-model comparison, we derive a
combined metric displaying the number of observed failures
per unit of evaluation time. This metric should be of par-
ticular interest to evaluators in the software industry, where
testing ends when resources (time, money) are depleted, since
it enables the selection of a model that maximizes the number
of internal fault activations for a given resource constraint.
The results displayed in Figure 4 show that, when we com-
bine the metrics, the ranking for overall failure stimulation
and also the failure mode distribution from Figure 3 change,
in this case in favor of DT for AE failures. However, since we
have observed weak coverage capabilities for the DT model,
the presented combination of only two metrics alone must not
be considered suciently expressive. The obtained coverage
values suggest to spend at least some evaluation eort us-
ing the FZ and SEU models, since these two models identify
large numbers of vulnerable services that remain undetected
with any other model. If vulnerable services have been iden-
tied using these models, the degree and criticality of these
vulnerabilities can further be quantied using the DT and
BF models as time and available resources permit.
From Figure 4 we also see that a general preference of IUE
injections over CUE Server injections due to eciency con-
siderations is not always justied. Bit ips that are injected
into drivers instead of the OS/driver interface are apparently
more ecient if both injection eciency and execution time
are considered.
Contributions and Limitations The metrics proposed
in this paper are intended to guide fault model selections for
robustness evaluations. The results of our case study demon-
strate the applicability and validity of the presented approach
and the discussion above stresses that each of the proposed
metrics provides valuable insights. In any concrete evalua-
tion, better-than andworse-than relations can be established
for every individual metric by simply applying numerical
greater-than and less-than operators respectively to benet-
and cost-oriented metrics, as we have shown throughout the
presentation and discussion of our results. The individual
measures of each metric are well suited for combinations,
as all metrics provide ratio scale measures and can thus be
quantied precisely. If, for instance, a model performs twice
as good as another one in terms of injection eciency and
execution time, but only half as good in terms of implemen-
tation complexity and coverage, they are equally ecient for
the robustness evaluation if all metrics are weighted equally.
Caveats: The presented approach has some limitations
that evaluators should be aware of for objective usage. Our
approach is restricted to evaluation methodologies that follow
the system model outlined in Section 2.1. We assume explicit
BFDTFZSEU051015202530
AE
AH
SC
Serial Port DriverFailures per hour of evaluationFigure 4: Cumulated failures of the serial port driver
per hour of evaluation time
interactions of the CUE with its environment and thereby
exclude certain classes of robustness evaluation approaches
(such as emulations of hardware-induced software errors in
the CUE) as well as certain classes of CUEs that heavily rely
on implicit interactions with their runtime environments.
We demonstrate the applicability of our approach in a case
study using a specic CUE and specic selections of CUE
Servers and Clients. We cannot claim the universal valid-
ity of inferences drawn from the results of this case study.
For instance, we have seen that the injection eciency met-
ric does not solely depend on the applied fault model, but
also on the targeted CUE Server. For the serial port driver,
the obtained injection eciencies for AH failures are signif-
icantly larger than for the other drivers, independent from
the applied fault model. It is also unclear whether the ob-
tained results are comparable for other CUEs. However, as
mentioned in the metrics introduction in Section 3, our main
concern is to provide a method to comparatively evaluate
contending fault models for a given evaluation attempt, i.e.
a xed CUE.
Up to now, the results of fault model evaluations according
to the proposed approach can only be applied for feedback on
an ongoing evaluation with injection campaign granularity. A
campaign needs to be planned, executed, and evaluated be-
fore any feedback on the eciency of the applied fault model
can be derived and used for planning subsequent campaigns.
6. CONCLUSION AND FUTURE WORK
Considering the importance of robustness testing approach-
es for COTS software components, this paper addresses the
problem of robustness testing suciency in absence of both
a single application scenario and source code access. To this
end it presents an approach to comparatively evaluate the
eciency of dierent fault models applied in SWIFI-basedrobustness evaluations in order to guide their selection.
Our proposed evaluation metrics cover both cost and bene-
t aspects of fault model implementation and usage. Except
for the implementation complexity of fault models, for which
existing assessment tools are referenced, the proposed met-
rics do not require additional measurements beyond those
required for actual robustness evaluations. We have demon-
strated our approach in a robustness evaluation of the Win-
dows CE 4.2 kernel, comparing the eciency of four com-
monly applied fault models as a case study.
We are currently attempting to transfer our fault model
evaluation approach to dierent software platforms (other
OS kernels as well as regular, user-space software) to investi-
gate the proposed quantiers's performance in cross-platform
model comparisons. Currently, we are investigating which
modications of the metric denitions are required to in-
crease the adaptivity of robustness evaluations. For this pur-
pose we consider increasing the granularity of the feedback
provided by our metrics, (i.e. we aim at providing feedback
on a model's eciency per injection run instead of per in-
jection campaign and make this information directly accessi-
ble to the SWIFI framework), so that the robustness evalua-
tion eciency can be optimized on-the-y. We are also con-
sidering introducing further feedback loops into the generic
SWIFI-based robustness evaluation approach that reect the
inuences of other factors, such as the applied workloads and
targeted CUE Servers.
Acknowledgement: Research supported in part by TUD
CASED and Microsoft.
7. REFERENCES
[1] A. Albinet, J. Arlat, and J. C. Fabre. Characterization
of the Impact of Faulty Drivers on the Robustness of
the Linux Kernel. In Proc. DSN , pages 867{876, 2004.
[2] J. Arlat, J. C. Fabre, and M. Rodriguez. Dependability
of COTS Microkernel-based Systems. IEEE Trans.
Comput. , 51(2):138{163, 2002.
[3] A. Avizienis, J. Laprie, B. Randell, and C. Landwehr.
Basic Concepts and Taxonomy of Dependable and
Secure Computing. IEEE Trans. Dependable Secure
Comput. , 1(1):11{33, 2004.
[4] Boehm, Abts, Clark, Horowitz, Brown, Reifer, Chulani,
Madachy, and Steece. Software Cost Estimation with
Cocomo II with CD-ROM . Prentice Hall PTR, 2000.
[5] B. W. Boehm. Software Engineering Economics .
Prentice Hall PTR, 1981.
[6] J. Carreira, H. Madeira, and J. G. Silva. Xception: A
Technique for the Experimental Evaluation of
Dependability in Modern Computers. IEEE Trans.
Softw. Eng. , 24(2):125{136, 1998.
[7] A. Chou, J. Yang, B. Chelf, S. Hallem, and D. Engler.
An Empirical Study of Operating Systems Errors. In
Proc. SOSP , pages 73{88. ACM, 2001.
[8] J. Duraes and H. Madeira. Characterization of
Operating Systems Behavior in the Presence of Faulty
Drivers through Software Fault Emulation. In Proc.
PRDC , pages 201{209, 2002.
[9] A. Ganapathi, V. Ganapathi, and D. Patterson.
Windows XP Kernel Crash Analysis. In Proc. LISA ,
pages 12{22, 2006.
[10] H. Hecht. Rare Conditions { An Important Cause of
Failures. In Proc. COMPASS , pages 81{85, 1993.[11] J. C. Huang. An Approach to Program Testing. ACM
Comput. Surv. , 7(3):113{128, 1975.
[12] IEEE. Standard Glossary of Software Engineering
Terminology. IEEE Std 610.12-1990 , page 1, 1990.
[13] ISO/IEC 19761:2003. Software Engineering {
COSMIC-FFP { A Functional Size Measurement
Method . 2003.
[14] ISO/IEC 20926:2003. Software Engineering { IFPUG
4.1 Unadjusted Functional Size Measurement Method {
Counting Practices Manual . 2003.
[15] A. Johansson. Robustness Evaluation of Operating
Systems . PhD thesis, TU Darmstadt, 2008.
[16] A. Johansson and N. Suri. Error Propagation Proling
of Operating Systems. In Proc. DSN , pages 86{95,
2005.
[17] A. Johansson, N. Suri, and B. Murphy. On the Impact
of Injection Triggers for OS Robustness Evaluation. In
N. Suri, editor, Proc. ISSRE , pages 127{126, 2007.
[18] A. Johansson, N. Suri, and B. Murphy. On the
Selection of Error Model(s) for OS Robustness
Evaluation. In Proc. DSN , pages 502{511, 2007.
[19] A. Kalakech, K. Kanoun, Y. Crouzet, and J. Arlat.
Benchmarking the Dependability of Windows NT4,
2000 and XP. In Proc. DSN , pages 681{686, 2004.
[20] P. Koopman, J. Sung, C. Dingman, D. Siewiorek, and
T. Marz. Comparing Operating Systems using
Robustness Benchmarks. In Proc. SRDS , pages 72{79,
1997.
[21] R. R. Lutz and I. C. Mikulski. Operational Anomalies
as a Cause of Safety-Critical Requirements Evolution.
Journal of Systems and Software , 65(2):155 { 161, 2003.
[22] T. McCabe. A Complexity Measure. IEEE Trans.
Softw. Eng. , SE-2(4):308{320, 1976.
[23] M. Mendon ca and N. Neves. Robustness Testing of the
Windows DDK. In Proc. DSN , pages 554{564, 2007.
[24] R. Moraes, R. Barbosa, J. Duraes, N. Mendes,
E. Martins, and H. Madeira. Injection of Faults at
Component Interfaces and Inside the Component
Code: Are They Equivalent? In R. Barbosa, editor,
Proc. EDCC '06 , pages 53{64, 2006.
[25] MSDN. coredll Module. http://msdn.microsoft.com/
en-us/library/aa448387.aspx .
[26] MSDN. Implementing CEDDK.dll. http://msdn.
microsoft.com/en-us/library/ms898217.aspx .
[27] MSDN. Network Driver Functions. http://msdn.
microsoft.com/en-us/library/ms895631.aspx .
[28] D. Simpson. Windows XP Embedded with Service
Pack 1 Reliability, January 2003.
[29] C. Software. SourceMonitor Version 2.5.
http://www.campwoodsw.com/sourcemonitor.html .
[30] C. Szyperski. Component Software - Beyond
Object-Oriented Programming . Addison-Wesley, 1998.
[31] E. Voas, F. Charron, G. McGraw, K. Miller, and
M. Friedman. Predicting how badly \good" Software
can behave. IEEE Softw. , 14(4):73{83, 1997.
[32] D. A. Wheeler. SLOCCount.
http://www.dwheeler.com/sloccount/ .