MiL Testing of Highly ConÔ¨Ågurable Continuous
Controllers: Scalable Search Using Surrogate Models
Reza Matinnejad, Shiva Nejati, Lionel C. Briand
SnT Centre, University of Luxembourg,Luxembourg
{reza.matinnejad,shiva,nejati,lionel.briand}@uni.luThomas Bruckmann
Delphi Automotive Systems,Luxembourg
thomas.bruckmann@delphi.com
ABSTRACT
Continuous controllers have been widely used in automotive do-
main to monitor and control physical components. These con-
trollers are subject to three rounds of testing: Model-in-the-Loop
(MiL), Software-in-the-Loop and Hardware-in-the-Loop. In our
earlier work, we used meta-heuristic search to automate MiL test-
ing of Ô¨Åxed conÔ¨Ågurations of continuous controllers. In this paper,
we extend our work to support MiL testing of all feasible conÔ¨Ågura-
tions of continuous controllers. SpeciÔ¨Åcally, we use a combination
of dimensionality reduction and surrogate modeling techniques to
scale our earlier MiL testing approach to large, multi-dimensional
input spaces formed by conÔ¨Åguration parameters. We evaluated
our approach by applying it to a complex, industrial continuous
controller. Our experiment shows that our approach identiÔ¨Åes test
cases indicating requirements violations. Further, we demonstrate
that dimensionally reduction helps generate surrogate models with
higher prediction accuracy. Finally, we show that combining our
search algorithm with surrogate modelling improves its efÔ¨Åciency
for two out of three requirements.
Categories and Subject Descriptors [Software Engineering]: Soft-
ware/Program VeriÔ¨Åcation
Keywords: Search-based testing; continuous controllers; automo-
tive software; dimensionality reduction; supervised learning.
1. INTRODUCTION
Embedded software systems are pervasive in the electronics sys-
tem industry, e.g., automotive. Many embedded software systems
are (partly) generated from mathematical models that describe how
devices are monitored, controlled, or regulated [39, 18, 28, 15].
A well-known category of such models is closed loop continuous
controllers [25], which are widely used in industrial control sys-
tems, and are designed using differential equations over continuous
time. These controller models are typically speciÔ¨Åed in mathemat-
ical modeling environments, most notably Simulink [33], that sup-
port automatic transformation of models to source code. The gen-
erated code is then integrated with other necessary software com-
ponents and deployed on embedded devices.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for proÔ¨Åt or commercial advantage and that copies bear
this notice and the full citation on the Ô¨Årst page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior speciÔ¨Åc permission and/or a fee. Request
permissions from permissions@acm.org.
ASE‚Äô14, September 15-19, 2014, Vasteras, Sweden.
Copyright 2014 ACM 978-1-4503-3013-8/14/09 ...$15.00.
http://dx.doi.org/10.1145/2642937.2642978.To identify early design errors of continuous controllers, engi-
neers create a model of the environment, capturing the behavior
of the device that interacts with a controller, and perform testing
and simulations of the controller and the environment models. This
stage of testing is known as Model-in-the-Loop (MiL) [39] testing
and is performed in various embedded system sectors such as the
automotive domain. The subsequent stages of controller develop-
ment are referred to as Software-in-the-Loop (SiL) [39], where the
integrated software system is developed and tested, and Hardware-
in-the-Loop (HiL) [39], where the software deployed on the embed-
ded device is being tested using a real-time simulator. Compared
to SiL and HiL, the development and testing at MiL level are con-
siderably faster as the engineers can quickly modify the controller
model and immediately test the system. In addition, MiL testing is
much less expensive than SiL or HiL testing.
In this paper, we focus on MiL testing of continuous controllers
speciÔ¨Åed as mathematical models in Simulink. It is important to
note that more than half of the controllers used in industry are con-
tinuous controllers [1]. Continuous controllers are used to capture
controllers of low-level devices [18], e.g., controlling the velocity
of a DC motor or the position of a Ô¨Çap. Modeling such controllers
using more conventional software engineering notations such as
state machines results in trivial discrete models where most im-
portant details are abstracted away, and hence, cannot be tested. In
control theory, continuous controllers are speciÔ¨Åed using differen-
tial equations known as proportional-integral-derivative (PID) [25].
These equations include time-continuous variables as well as pa-
rameters that are not time-dependent and are Ô¨Åxed for every con-
troller conÔ¨Åguration. These parameters are referred to as calibra-
tion orconÔ¨Åguration parameters, and optimize the behavior of a
particular controller conÔ¨Åguration for speciÔ¨Åc hardware.
Many existing approaches to testing embedded software systems
focus on analyzing discrete or mixed discrete-continuous systems [15,
30, 14]. These techniques, however, are not amenable to analyz-
ing controllers of low-level devices with a trivial discrete behavior.
Some recent work has concentrated on using search-based algo-
rithms to develop automated and systematic testing techniques for
embedded software systems [16, 11, 20, 21]. Among these, our
earlier work [20, 21] particularly focuses on testing Ô¨Åxed conÔ¨Ågu-
rations of continuous controllers where only time-dependent vari-
ables of controllers are included in the test input data.
In this work, we present an approach for testing continuous con-
trollers while accounting for their feasible conÔ¨Ågurations. Specif-
ically, we develop a search-based technique to generate test cases,
i.e., worst case scenarios, attempting to violate controller require-
ments. Our search strategy traverses individual points in the input
search space of the controller, and is guided by an objective func-
tion deÔ¨Åned based on the output of the controller simulation for
163
each point in the input space. The input space of the controller
in our previous work [21] was made of time-dependent variables
only. In this paper, we extend the search space to include conÔ¨Ågu-
ration parameters as well as the time-dependent variables, allowing
us to test controllers for different hardware conÔ¨Ågurations at the
MiL level. However, the expanded search space becomes so large
that our previous approach can no longer scale to identify worst
case scenarios. That is, to handle multiple controller conÔ¨Ågura-
tions, we cannot merely expand the input search space used in our
earlier work. Instead, we have to build strategies to scale the search
to large multi-dimensional spaces, and to reduce the cost of com-
puting objective functions for individual points in the search space.
In this paper, we extend our previous work [20, 21] to sup-
port MiL testing of all feasible conÔ¨Ågurations of continuous con-
trollers. SpeciÔ¨Åcally, we use a combination of dimensionality re-
duction [6] and surrogate modeling techniques based on super-
vised learning [17, 26, 5, 10] to scale our search to large multi-
dimensional spaces. Given an objective function, we Ô¨Årst use di-
mensionality reduction techniques to identify the input variables
that do not have a signiÔ¨Åcant impact on the output of the objec-
tive functions, i.e., varying the values of those variables does not
cause a signiÔ¨Åcant change in the objective function output. We then
apply an explorative random search [4] and focus the explorative
search only on signiÔ¨Åcant variables. Using the exploration results,
we select some partitions of the input space that are more likely to
include worst case input scenarios. We then apply a single-state
search [19] to the selected partitions to identify worst case scenar-
ios in each partition. Our objective functions require us to simulate
Simulink models and are computationally expensive. Therefore,
for each objective function and for each partition, we use the explo-
ration results to build a surrogate model [17] based on supervised
learning techniques [38]. The surrogate model is faster to compute
than the objective function, and is able to predict its output within
some conÔ¨Ådence interval. Our single-state search uses the surro-
gate model to predict the output of the objective function when the
decision as to which point the search should move to can be made
based on the surrogate model.
We evaluated our approach by applying it to a complex, indus-
trial controller, consisting of 443 Simulink blocks from the auto-
motive domain. Our experiment showed that applying dimension-
ality reduction prior to exploration helps generate more accurate
and predictive surrogate models for two out of three requirements.
In addition, combining single-state search with surrogate modeling
remarkably improves our approach for the same two requirements.
SpeciÔ¨Åcally, for one requirement, the search combined with surro-
gate modeling is eight times faster than the search without surrogate
modeling, and for the other requirement, the search with surrogate
modelling computes higher output values that could not be com-
puted by the search without surrogate modeling. Finally, our ap-
proach identiÔ¨Åed critical violations of the controller requirements
that had been found neither by our earlier work [21] nor by manual
testing based on domain expertise.
2. BACKGROUND AND MOTIV ATION
In this section, we discuss MiL testing of continuous controllers,
and motivate our work based on the needs of the automotive do-
main. Figure 1(a) shows an overview of a controller and a plant
(environment) model in a feedback loop. The system input and
output are respectively shown as desired andactual in Figure 1(a).
The variable desired represents the location we want a robot to
move to, the speed we require an engine to reach, or the position we
need a valve to arrive at. The variable actual represents the actual
state/speed/position of the plant. The actual value is expected to
Plant Model+++‚åÉ+-e(t)actual(t)desired(t)‚åÉKPe(t)KDde(t)dtKIRe(t)dtPIDPlant Model+-‚åÉController Model(SUT)(a)(b)desired actualerroroutputoutput(t)Figure 1: Continuous controllers: (a) A controller model and
its environment (plant) model, and (b) a controller PID formu-
lation [1].
Initial Desired(ID)Desired ValueI (input)Actual Value (output)Final Desired(FD)FsmFrFsttimeT'TT0+‚åß
Figure 2: Continuous controller input and output. Objective
functionsFst,FsmandFrare illustrated on the output signal.
reach the desired value over a certain time limit, making the error,
i.e., the difference between the actual anddesired values, eventu-
ally zero or practically negligible. The task of the controller is to
eliminate the error by manipulating the plant to obtain the desired
effect on the actual status of the plant. The variables desired and
actual are time-dependent and are speciÔ¨Åed using signals over time.
Continuous controllers are designed via mathematical models
known as proportional-integral-derivative (PID) equations [25]. Fig-
ure 1(b) shows the generic (most basic) formulation of a PID equa-
tion. Lete(t) be the difference between desired (t)andactual (t).
A PID equation is a summation of three terms: (1) a proportional
termKPe(t), (2) an integral term KIRt
0e(t)dt, and (3) a deriva-
tive termKDde(t)
dt. The coefÔ¨Åcients KP,KIandKDare conÔ¨Åg-
uration parameters and are Ô¨Åxed for every instance of a controller.
These parameters depend on the physical properties of the plant
(hardware) that the controller eventually interacts with. The num-
ber of conÔ¨Åguration parameters in real-world PID controllers is of-
ten more than three. This is because these PID controllers have
more than three terms, or each term may have a more complex
formulation consisting of more coefÔ¨Åcients. For example, our in-
dustrial system includes six conÔ¨Åguration parameters.
PID formulations are typically speciÔ¨Åed in Simulink, allowing
engineers to simulate controllers, analyze their output, and even-
tually generate code from the controller design. Figure 2 shows
simulations representing the input and output variables of a con-
troller. The input, i.e., variable desired, is shown by a dashed line
and is given as a step signal. SpeciÔ¨Åcally, this input signal Ô¨Årst sets
the controller at an initial desired (ID) value until time T0, and then
requires the controller to move to a Ô¨Ånal desired (FD) value by time
T. The output, i.e., variable actual, shown by a solid line, starts at
zero, and gradually moves to reach and stabilize at the initial de-
sired (ID), and then it moves towards the Ô¨Ånal desired (FD) and
stabilizes there.
To test a controller, engineers simulate the controller using dif-
ferent input step signals by varying IDandFD. For each simulation,
they generate the output signal, i.e., the actual signal in Figure 2,
164and check if the output conforms to the following three main re-
quirements that we identiÔ¨Åed in our previous work [21]:
Stability: The controller shall guarantee that the output will reach
and stabilize at the input after a time limit.
Smoothness: The actual value shall not change abruptly when it is
close to the input.
Responsiveness: The controller shall respond within a time limit.
We deÔ¨Åne three objective functions Fst,FsmandFrover the
output signal to estimate quantitative values for stability, smooth-
ness and responsiveness requirements, respectively. We provided
formal deÔ¨Ånitions of these functions in our earlier work [20, 21].
BrieÔ¨Çy, to evaluate stability, engineers check whether, after time ,
the difference between input and output converges to zero, and in
addition, the output remains stable afterwards. Function Fstmea-
sures the maximum difference between input and output over the
time periods shown by thin dashed arrows in Figure 2. To evaluate
smoothness, engineers check whether the undershoot or overshoot
of the output signal is not too large. As shown in Figure 2, Fsm
measures the maximum undershoot and overshoot of the output sig-
nal. The response time is the time it takes for the controller output
to reach or to become close to its input. As shown in Figure 2, Fr
measures the response time intervals of the output.
To compute Fst,FsmandFr, we provide an input to the con-
troller Simulink model, and use the generated output signal to com-
pute these functions. The input to the controller includes values
for the controller conÔ¨Åguration parameters, and values for IDand
FD, which characterize the input step signal. Therefore, functions
Fst,FsmandFrdepend on the conÔ¨Åguration parameters, and ID
andFDvariables. Having computed these three functions over an
output signal, engineers can then decide, based on their domain
knowledge and thresholds provided in the requirements, whether
the controller under analysis satisÔ¨Åes each of the above require-
ments or not. In general, the higher the objective function value,
the more likely it is that the controller violates the requirement cor-
responding to that objective function.
Currently, in most companies, MiL testing of controllers is lim-
ited to running the controller for a small number of input signals
that are often selected based on the engineers‚Äô domain knowledge
and experience. Existing MiL testing often fails to Ô¨Ånd erroneous
scenarios that the engineers are not aware of a priori. Identifying
such scenarios later during SiL/HiL is much more difÔ¨Åcult and ex-
pensive than during MiL testing.
In our earlier work, we proposed a search-based approach for
testing an individual controller conÔ¨Åguration [20, 21]. That is, we
Ô¨Åxed the values for conÔ¨Åguration parameters based on those used
during HiL testing, and developed search algorithms maximizing
Fst,FsmandFrwithin an input search space with two dimensions:
IDandFD. For example, consider a controller with Ô¨Çoat variables
IDandFDranging from 0 to 1. The input search space of this con-
troller with dimensions IDandFDis shown in Figure 3(a). Our
approach has two steps: exploration and search. For each objec-
tive function, our approach Ô¨Årst computes that function for several
points randomly selected from the input space (exploration step).
We divide the input space into a number of regions, e.g., 100 equal
regions in Figure 3(a), and shade each region based on the objec-
tive function output. The resulting diagram is called a heatmap
diagram. For example, Figure 3(a) is a heatmap diagram generated
based on the stability objective function values for 1000 points.
SpeciÔ¨Åcally, in a heatmap diagram, the points in darker regions
yield a higher, average objective function output than the points
in lighter regions. Hence, darker regions are more likely to include
input values violating the controller requirements. In the search
ID = 0.82
FD = 0.08time(s)1.02.001.00.90.80.70.60.50.40.30.20.10.00.00.10.20.30.40.50.60.70.80.91.0
IDFDDesired Value (input)Actual Value (output)(a)(b)Figure 3: An example representing our MiL testing approach
for a single controller conÔ¨Åguration [21]: (a) A heatmap dia-
gram generated by our approach for the stability requirement,
and (b) the identiÔ¨Åed worst case scenario violating the stability
requirement.
Fst,Fsm,FrRID‚á•RFD‚á•RCal1‚á•RCal2‚á•RCal3‚á•RCal4‚á•RCal5‚á•RCal6(d=8)ObjectiveFunctions[0..1]‚á•[0..1]‚á•[3.5..4]‚á•[2..4]‚á•[0..0.13]‚á•[0.3..0.7]‚á•[0..0.05]‚á•[1..1.2]Input SpaceController
Figure 4: Controller objective functions and the ranges of the
input variables and conÔ¨Åguration parameters for our industrial
controller.
step of our approach, we apply a single-state search algorithm to
dark regions to Ô¨Ånd points that maximize our objective functions,
and hence, are more likely to violate the requirements.
For example, the space shading in Figure 3(a) is produced based
on the output of the stability objective function Fstapplied to a
faulty controller. The controller satisÔ¨Åes the stability requirement
for the input signals generated by the points in the clear-shaded re-
gions. However, applying our single-state search to the dark region
of (ID =[0:8::0: 9]andFD=[0::0:1]) results in Ô¨Ånding the simula-
tion in Figure 3(b) which clearly violates the stability requirement.
Note that since the controller behavior for most of the input space
(more than 95% of the input space) conforms to the stability re-
quirement, it is very unlikely that one can discover the faulty be-
havior by manually selecting and running a few simulations.
In this paper, we extend our MiL testing approach to include not
only IDandFDvariables, but also the controller conÔ¨Åguration pa-
rameters. For example, the industrial controller used as a case study
in this paper has six conÔ¨Åguration parameters referred to as Cal1
toCal6, respectively. The type of variables ID,FD, and Cal1 to
Cal6 is Ô¨Çoat. We denote the range of variables IDandFDbyRID
andRFDrespectively, and the range of each Cali byRCali. Fig-
ure 4 provides value ranges for each of the conÔ¨Åguration variables
andIDandFDvariables in our industrial controller. To compute
highest values of Fst,FsmandFrfor any controller conÔ¨Ågura-
tion, our search algorithm has to handle the search space size of
jRI
DRFDRCal1:::RCal6j.
Due to sheer size of the search
space, we cannot effectively solve our problem by simply apply-
ing existing search algorithms. Instead, in this paper, we com-
bine dimensionality reduction andsurrogate modeling techniques
(based on supervised learning) to perform search in large and multi-
dimensional input spaces and to reduce the cost of computing our
objective functions for individual points in the search space.
3. MIL TESTING USING SEARCH
Figure 5 shows an overview of our search-based approach to MiL
testing of continuous controllers. Similar to our previous work [21],
our approach is composed of an exploration and a search step. The
input to our approach includes a set of objective functions, and a
165+Controller Model (Simulink)
Worst-Case ScenariosList of Critical PartitionsRegressionTree
2.Search withSurrogate ModelingFst,Fsm,FrRegressionTree T1.2Exploration
DomainExpertF:R1‚á•...‚á•Rd!R1.1DimensionalityReduction
F:R‚úì1‚á•...‚á•R‚úìd!R2.1 Build Surrogate Model ÀÜF:R‚úì1‚á•...‚á•R‚úìd!RSet     of                 PairsP2.2 Search  BuildUse+(p, F(p))w=(v1,...,vd)FÀÜF‚úè:R‚úì1‚á•...‚á•R‚úìd‚á•[0..100]!RPoint with the  highest output of Surrogate ÀÜMModel (a) Exploration
(b) Search1.Exploration with Dimensionality ReductionFigure 5: An overview of our automated approach to MiL test-
ing of different conÔ¨Ågurations of continuous controllers: (a)
Exploration step, (b) Search step.
controller Simulink model required to compute the objective func-
tions. SpeciÔ¨Åcally, in our work, objective functions are Fst,Fsm
andFras described in Section 2. The input spaces of these func-
tions are the same and equal to the cross product of the ranges for
IDandFDvariables and the conÔ¨Åguration variables. For exam-
ple, the input space (objective function domains) in our case study
isRIDRFDRCal1:
::RCal6(See
Figure 4). The range of the
objective functions is the set of real numbers R.
In the exploration step (Figure 5(a)), we apply a random (un-
guided) search to the entire input space of the objective functions,
and then based on the results we build a regression tree [38] parti-
tioning the input space such that the variance of the objective func-
tion values within each partition is minimized. Before performing
exploration, for each objective function, we use a dimensionality
reduction strategy to identify dimensions that have the most impact
on that objective function. This allow us to focus the exploration
step only on dimensions with most impact on the objective func-
tions, and hence, increase the scalability of our approach. The re-
gression tree built based on the exploration results enables us to
divide the space into partitions such that, for each partition, the
value of the objective function is predictable within a certain con-
Ô¨Ådence interval. In addition, regression trees allow the engineers
to visualize partitions from a multidimensional space. We then use
regression trees to identify higher risk partitions, i.e., those parti-
tions that contain input values that are likely to violate controller
requirements. Regression trees replace the heatmap diagrams (e.g.,
Figure 3(a)) we used in our earlier work [20], which can no longer
be used for a space that has more than three dimensions. From the
regression trees, we select the partitions with the highest mean for
the objective function, which are considered to be higher risk as
they are more likely to contain critical errors.
In the search step (Figure 5(b)), we focus our search on the se-
lected partitions and employ single-state search algorithms to iden-
tify, within those partitions, the worst case scenarios to test the con-
troller. In this step, we build surrogate models to minimize the
need for running simulations of Simulink controller models so as
to make the search more scalable. Based on our previous expe-
rience, the main cause of computation time of our search is such
simulations. Recall that to compute objective functions, we have
to simulate the input controller model. In this work, for each ob-
jective function and for each input space partition, we create a sur-
rogate model that predicts the objective function values within that
partition. We use the exploration results related to each partition
to build surrogate models. A surrogate model built for an objec-tive function is able to predict the output of that function within a
conÔ¨Ådence interval. Using this model, our single-state search can
determine whether the decision as to which point the search has to
move to can be made without resorting to running simulations or
not. In Sections 3.1 (Exploration) and 3.2 (Search), we describe
the Ô¨Årst and second steps of our approach, respectively.
3.1 Exploration
Figure 5(a) shows the exploration step of our approach. The
input of this step is an objective function F:R1:::Rd!R.
FunctionFcan be any of the objective functions in Figure 4. The
goal of this step is to efÔ¨Åciently select a set of points in the space of
R1:::Rdand compute the output of Ffor each point in this
set. The output of this step is used to identify critical parts of the
R1:::Rdspace (i.e., those partitions for which Fproduces
the most critical (highest) values), and further, to create a surrogate
model for individual critical partitions that can estimate as precisely
as possible the output of Ffor any arbitrary point in that partition.
Given an objective function F, we Ô¨Årst use dimensionality re-
duction techniques to identify search input dimensions that have
the least impact on the output of F. A dimension i(1id)
has a low impact on the output of Fif varying the input vector
(v1;:::;vi;:::;vd)ofFby varyingviwithin the range Ridoes
not yield a signiÔ¨Åcant change in the output of F.
In addition, this step uses adaptive random search [19] to explore
the input space of Fby focusing on the dimensions with most im-
pact onF. Here, we brieÔ¨Çy discuss adaptive random search applied
to the entire input space without considering dimensionality reduc-
tion. In Section 3.1.2, we show how this algorithm is modiÔ¨Åed
to focus on signiÔ¨Åcant search dimensions only. Adaptive random
search is an extension of the naive random search that attempts
to maximize the euclidean distance between the selected points.
Adaptive random search explores the space by iteratively selecting
points in areas of the space where fewer points have already been
selected. Let R1:::Rdbe the input space, and let Pibe the
set of points selected by adaptive random search at iteration i. At
iterationi+ 1, adaptive random search randomly generates a set P
of candidate points in the input space. The search computes dis-
tances between each candidate point pand points already selected
inPi. Formally, for each point p= (v 1;:::;vd)inP, the search
computes a function dist(p;Pi)as follows:
dist(p;
Pi) =MIN (v0
1;:::;v0
d)2PiqPd
j=1(vj v0
j)2
The
search algorithm then adds to Pia pointpinPsuch that
dist(p;Pi)is the largest. The algorithm terminates after generat-
ing a speciÔ¨Åc number of points. Adaptive random search is similar
toquasi-random number generators that are available in some lan-
guages, e.g., MATLAB [32]. Similar to our adaptive random search
algorithm, these number generators attempt to generate points that
are evenly distributed across the entire space. Below, we describe
how we use dimensionality reduction and adaptive random search
to efÔ¨Åciently select a set of points in the input space of Fthat can
be utilized in the search step (Figure 5(b)) for building an effective
surrogate model.
3.1.1 Dimensionality Reduction
Using dimensionality reduction, we identify the dimensions of
the domain of Fthat have the most impact on the output of F. To
do so, we rely on sensitivity analysis which is the study of how the
variations in the outputs of a function are related to the variations
in its inputs [6]. An application of sensitivity analysis is identifying
input variables with the most and the least signiÔ¨Åcant impact on a
166Cal5IDCal3FD
Cal4Cal6Cal1,Cal20.60.40.20.0Sample Standard Deviation (      )-0.6-0.4-0.20.00.2Sample Mean (     ) ‚á§10 2
‚á§10 2S i
 iFigure 6: The elementary effect analysis results for the stability
objective function (F st) with an eight-dimension input space.
given function. Among different sensitivity analysis techniques,
we use the elementary effects method [24]. This method is in-
tuitive, and compared to other techniques such as variance-based
methods [6], requires fewer number of function evaluations, and
hence, is well-suited for functions that are expensive to compute.
The elementary effects method works as follows: Using adap-
tive random search, we generate rpoints in the input space of
R1:::Rd. For each dimension i(1id), and for each
pointj, we varyviin the input vector (v1;:::;vd)ofFby a pa-
rameter and measure the resulting variation ijin the output of
F. We then compute the sample mean i, and the sample standard
deviationSifor each input space dimension ito assess the impact
of that dimension on F. Figure 6 shows an example output of the
elementary effects method for the stability objective function Fst
with an eight-dimension input space. Provided with this diagram,
engineers choose the dimensions with signiÔ¨Åcant impact on Fst.
For example, they may decide that Cal1, Cal2, Cal4, and Cal6
(which have sample means and standard deviations close to zero)
are not signiÔ¨Åcant.
3.1.2 Exploration in the Reduced Dimensional Space
To explore the input space, we use the adaptive random search
algorithm as described at the beginning of Section 3.1. The differ-
ence is that we modify the dist function to ensure that the search
maximizes diversity along the dimensions with signiÔ¨Åcant effect on
F. Otherwise, note that exploration with and without dimension-
ality reduction take about the same time and operate in the same
space. LetDrbe the set of dimensions with signiÔ¨Åcant impact on
F, e.g.,Drin Figure 6 isfID;FD;Cal3; Cal5g. For each candi-
date pointp, we compute distDr(p;Pi)as follows:
distDr(p;
Pi) =MIN (v0
1;:::;v0
d)2PiqP
j2Dr(vj v0
j)2
In
contrast to the dist function presented at the beginning of Sec-
tion 3.1, in distDr, we consider only the values related to the di-
mensions in Dr. That is, we focus on maximizing the diversity of
the selected points along the dimensions in Dr, and the values of
the variables along the dimensions in f1;:::;dgnDrcan either be
Ô¨Åxed or set arbitrarily. This allows us to intensively explore parts
of the space that results in the most variations in the output of F.
Having selected Npoints in the input space of Fand having
computedFfor each point, we build a regression tree based on
these points, e.g., see Figure 7. The regression tree represents a
stepwise partition of the input space aimed at getting increasingly
homogeneous partitions with respect to F. Such a representation
is a convenient and intuitive way to visualize the impact of input
space dimensions on F[38].
Figure 7 shows an example of the regression tree generated from
the exploration results for Fst(N= 1000). Each node in the
tree corresponds to a space partition and is labeled by the num-
ber of the points in that partition as well as the mean and stan-
dard deviation of the values of Fstfor those points. For example,
All PointsFD>=0.43306Count MeanStd DevCount MeanStd DevFD<0.43306Count MeanStd DevID>=0.64679Count MeanStd DevCount MeanStd DevCal5>=0.020847Cal5>0.020847Count MeanStd DevCount MeanStd DevCal5>=0.014827Cal5<0.014827Count MeanStd DevCount MeanStd Dev1000 0.0078220.0049497
ID<0.64679574 0.00595130.0040003426 0.01034250.0049919373 0.00475940.0034346201 0.00816310.0040422182 0.01345550.0052883244 0.00802060.003175170 0.01067950.0052045131 0.00681850.0023515Figure 7: An example of a regression tree generated for Fst.
the highlighted node in Figure 7 corresponds to a partition where
min FDFD<0:43306 and0:020847<Cal5max Cal5,
and it includes 182 points selected during exploration. The mean
and standard deviation of Fstfor these points are 0:0134555 and
0:0052883, respectively.
We select a partition with the highest mean value for the objec-
tive function from the regression tree. Note that such a partition has
to be a leaf node because any non-leaf node has exactly one child
node whose mean value is higher than the mean value of its par-
ent. The partition with the highest mean is more likely to include
errors and critical scenarios. We denote input space partitions by
R, and deÔ¨Åne it asR=R
1:::R
dsuch thatR
iRi
for1id. For example, in Figure 7, we select the highlighted
partition that has the highest mean value, and denote it by
R=R
I
DR
FDR
Cal1R
Cal2R
Cal3R
Cal4R
Cal5R
Cal6
such
thatR
F
D= [min FD::0:43306) ,R
C
al5= (0:020847::max Cal5],
and
R
v=Rv=
[min v::max v]for
every other variable v. After selecting
a partitionR, this partition together with the points generated
during exploration inside this partition are passed to step 2.
3.2 Search
Figure 5(b) shows the search step of our approach. The search
step takes as input a function F, a partitionR, and a setPof
(p;F (p)) pairs computed during the exploration step. SpeciÔ¨Åcally,
the spaceRis a critical input space partition identiÔ¨Åed in the pre-
vious step, and Pincludes pairs of (p;F (p)) wherepis a point in
Rthat was selected by the adaptive random search in step 1.2, and
F(p)is the output of Fapplied top. We refer to Pas an observa-
tion set. In step 2.1in Figure 5(b), we Ô¨Årst build a surrogate model
^Musing supervised learning techniques [38]. The surrogate model
^Mconsists of two functions: A predictive function ^F:R!R
that estimates the output of Ffor any point inR, and an error
function ^F:R[0::100]!R. For each point p2Rand a
given conÔ¨Ådence level cl,^F(p;cl)is the prediction error indicat-
ing that, with a conÔ¨Ådence level of cl, the actual value of F(p)is
within ^F(p)^F(p;cl).
In step 2.2, we search the points in Rto Ô¨Ånd a point that max-
imizesF. Since computing Fis expensive, we expedite the search
by checking whether we can conclusively decide the next point that
the search should move to using ^F. SpeciÔ¨Åcally, the output of ^F
is conclusive, if the prediction error ^Fis less than the difference
between the output of ^Fand the existing highest value found by
the search. Otherwise, we have to compute the actual output of F
by simulating the Simulink controller related to F.
3.2.1 Surrogate Modeling
We use supervised learning techniques to build a surrogate model
of the function F. Given a point p, supervised learning predicts
167F(p)using a setPof observations with known output values [38].
We divide the observation set Pinto a training set and a testset.
The training set is used to infer a predictive function ^F. This is
done by estimating the parameters of ^Fsuch that ^FÔ¨Åts the train-
ing data as well as possible, i.e., for the observations in the training
set, the differences between the output of Fand that of ^Fare min-
imized. The test set is then used to evaluate the accuracy of the
predictions produced by ^Fwhen applied to observations outside
the training set.
Supervised learning techniques are categorized into regression
andclassiÔ¨Åcation techniques where the goal is to predict real-valued
and categorical outputs, respectively. We use regression techniques
becauseFis a real-valued function. SpeciÔ¨Åcally, we use the fol-
lowing regression techniques:
Linear Regression (LR). Linear regression assumes that Fis lin-
ear. Given an observation point p= (v1;:::;vd), linear regression
infers ^Fas a linear function
^F(v1;
:::;vd) =0+1v1+2v2+:::+dvd
Exponential
Regression (ER). Exponential regression assumes that
Fis non-linear but monotonic, and infers ^Fin the following form:
^F(v1;
:::;vd) =0v1
1v2
2:::vd
d
P
olynomial Regression (PR). Polynomial regression assumes that
Fis neither linear nor monotonic. The inferred function ^Ftakes
the form of an nth-degree polynomial:
^F(v1;
:::;vd) =0+dP
i=1i1vi+i2v2
i+:::+invn
i
In
this paper, we consider PR(n = 2;3) because based on our
experiments the predictability of our surrogate models decreases
forn > 3. In the above three regression methods, parameters i
(0id) andij(1idand1jn) have to be esti-
mated using the training data. The goal is to estimate these param-
eters such that the sum squared error of the predicted outputs for
the training points is minimized. That is,Pm
i=1(F(pi) ^F(pi))2,
wheremis the number of observations in the training set, is min-
imized. In addition, we use stepwise regression to build ^Fin the
above three regression methods [38]. Instead of including all vari-
ablesv1tovdat once, stepwise regression aims at selecting a mini-
mal subset of variables that are statistically signiÔ¨Åcant at explaining
the variation in F. The variables may be selected in a forward or
backward way. In the forward selection, variables are iteratively
selected and added as long as the sum squared error over the train-
ing data decreases, i.e., the predictive power of ^Fimproves. At
each iteration, a statistical test (F-test ) is used to determine which
variable best improves the predictive power of ^F[7]. Dually in
the backward elimination, variables are iteratively removed as long
as the predictive power of ^Fover training data does not decrease.
Most implementations of stepwise regression, e.g., stepwiselm in
MATLAB [34], combine the backward and forward methods by it-
eratively switching between them. That is, they add variables using
forward selection for some iterations, and then switch to backward
elimination after a while to remove unnecessary variables. We note
that there are a number of other supervised learning methods such
assupport vector regression [31, 8] and neural networks [38], that
we do not discuss in this paper due to lack of space, and leave them
for future work.
In addition to function ^F, all the above regression methods pro-
vide a function ^F:R[0::100]!R. Function ^F(p;cl)estimates the prediction error for a point pbased on a given con-
Ô¨Ådence level cl, which is a percentage value between 0and100,
usually above 80%. For example the input cl= 95 implies that the
actual value of F(p)lies in the interval of ^F(p)^F(p;cl)with a
conÔ¨Ådence level of 95%.
3.2.2 Single-State Search Using Surrogate Model
As discussed in our earlier work [21] to compute highest val-
ues of our objective functions, among the existing meta-heuristic
search techniques, we opt for single-state algorithms as opposed
topopulation-based ones. This is because population-based search
algorithms compute Ô¨Åtness functions for a set of points (a popu-
lation) at each iteration [19]. Hence, they are less likely to scale
when objective functions are computationally expensive.
We propose a new Hill Climbing (HC) single-state search algo-
rithm [19] extended to use surrogate models. Our algorithm speeds
up the search by avoiding simulations when it is possible to decide
the next move for search, based on the surrogate model predictions.
The algorithm is shown in Figure 8. It takes as input the function
F, the setPof observations in the partition R
1:::R
d, the
surrogate model ^M= (^F;^F), and a conÔ¨Ådence level cl. The
output of the algorithm is a point wwith the highest output of F
found inTsseconds.
The algorithm Ô¨Årst identiÔ¨Åes the observation (p;F (p))2Psuch
thatF(p)is the largest in P, and sets the variable highest toF(p)
(lines 1, 2). At the beginning of each iteration of this algorithm,
highest is the highest output of Fcomputed so far. The algorithm
then iteratively generates a new point newp by tweaking the current
pointp(line 4). The tweak operator is similar to the one used in
our earlier work [21]. That is, we tweak a point p= (v1;:::;vd)
by shifting each viwith a value xrandomly selected from a normal
distribution with mean = 0and variance 2= 0:1jR
ij. In our
previous work, we showed that this tweak operator yields effective
results for two dimension input space functions [21]
For each new point newp , the algorithm computes surrogate
model functions ^F(newp )and^F(newp;cl)(lines 5, 6). At line
7, the algorithm determines whether it needs to compute the ac-
tual value of F(newp ), or it can decide the next move only us-
ing^F(newp ). Figure 9 depicts the conditions under which we
have to compute F(newp ). SpeciÔ¨Åcally, if highest is less than
or equal to ^F(newp ) ^F(newp;cl), or more than or equal to
^F(newp ) +^F(newp;cl), with a conÔ¨Ådence level of cl,highest is
less or greater than F(newp ), respectively. In the case of highest
greater than ^F(newp ) +^F(newp;cl), the search does not move
tonewp , and hence, no need to compute F(newp ). In the case of
highest less than ^F(newp ) ^F(newp;cl), the search may move
tonewp depending on the value of F(newp ). Thus, we compute
F(newp ). If highest is between ^F(newp ) ^F(newp;cl)and
^F(newp )+^F(newp;cl), we cannot conÔ¨Ådently compare highest
with the actual value of F(newp )using ^F(newp ), and hence, have
to compute F(newp ).
Line 7 in Figure 8 summarizes the condition for determining
whether F(newp )has to be computed or not. If yes, the algorithm
computes F(newp ), and reÔ¨Ånes the surrogate model ^Musing the
new observation (newp;F(newp ))(lines 8‚Äì10). Otherwise, at line
11, the algorithm decides the next point that the search should move
to. When it decides to move (lines 12, 13), it updates the current
pointpwithnewp , and highest with the highest value of Fcom-
puted so far and stored in y. In addition, it keeps a copy of newp .
Finally, once the loop at line 3 terminates, the algorithm reports the
pointwwith the highest output of Ffound inTsseconds.
168Algorithm. SI
NGLE STATE SEARCH
Input:F:R
1:::R
d!R.
The setPof(p;F (p)) pairs.
The surrogate model ^M: (^F;^F).
A conÔ¨Ådence level cl.
Output: Point w= (v1;:::;vd)with the highest output of F.
1. Let (p;f)2Ps.t. for all (p0;f0)2P, we haveff0
2.highest =f
3.forTsseconds do:
4.newp =Tweak (p)
5.y=^F(newp)
6.=^F(newp;cl )
7. ifhighest<(y+): /*Ifhighest is less than ^F(newp) + ^F(newp;cl ), we
8. y=F(newp) simulate and compute F(newp), as shown in Figure 9.
9.P=P[(newp;y ) Otherwise, we bypass simulation.*/
10. (^F;^F) =BuildSurrogateModel (P)
11. ify>highest :
12. highest =y
13.p=w=newp
14.return w
Figur
e 8: Single-state Hill Climbing (HC) search algorithm
with surrogate modeling.
 1...ÀÜF(newp) ÀÜF‚úè(newp, cl)ÀÜF(newp) ÀÜF‚úè(newp, cl)...ÀÜF(newp)+ÀÜF‚úè(newp, cl)ÀÜF(newp)+ÀÜF‚úè(newp, cl)...1(]((](Perform SimulationConÔ¨Ådent that ConÔ¨Ådent that Not conÔ¨Ådent aboutthe relation between  highest > F(newp)highest < F(newp)highest F(newp)   andBypass Simulation
Figure 9: Depicting the conditions used by the algorithm in
Figure 8 to perform or to bypass simulations.
Note that the higher the value of cl, the algorithm in Figure 8 is
more likely to compute the actual value of Fby running simula-
tions. Forcl= 100, the interval of ^F(newp )^F(newp;cl)is
equal to ( 1; +1), and hence, the algorithm behaves like a con-
ventional Hill Climbing algorithm and runs a simulation at each
iteration. For cl= 0, we have ^F(newp;cl) = 0, and hence,
the algorithm runs fewer simulation, i.e., only when highest<
^F(newp )(see Figure 9).
4. EXPERIMENT SETUP
In this section, we present the research questions, some infor-
mation about our industrial subject, the metrics used to evaluate
surrogate models, and information about our experiment design.
4.1 Research Questions
RQ1 How do the different surrogate modeling techniques per-
form compared to one another?
RQ2 Does dimensionality reduction improve prediction accu-
racy of the best surrogate modeling technique identiÔ¨Åed in RQ1?
RQ3 How do our single-state search algorithms, with and with-
out surrogate modeling, perform compared to each other?
RQ4 Does our approach help identify testing results that are use-
ful in practice?
InRQ1, for each objective function, we identify, among the
four regression methods discussed in Section 3.2.1, the method
that yields a surrogate model with highest prediction accuracy. For
each objective function, we then use this best surrogate model for
the single-state search. In RQ2, we determine whether focusing
exploration on signiÔ¨Åcant dimensions of each Ô¨Åtness function im-
proves the prediction accuracy of the surrogate models. Recall that
exploration with and without dimensionality reduction take about
the same time and operate in the same space. However, the ex-
ploration results with dimensionality reduction are more diversely
distributed along the dimensions with signiÔ¨Åcant impact on the ob-
jective functions. The question is whether this gives rise to moreaccurate and predictive surrogate models? In RQ3, we compare the
performance and results of our single state search algorithms with
and without surrogate modeling, in order to determine whether our
new approach scales better in large search spaces. Finally, in RQ4,
we compare our best results, i.e., test cases with highest objective
function values, with those obtained in our previous work [21] as
well as the existing test cases used in practice.
4.2 Industrial Subject
Supercharger is an air compressor blowing into a turbo compres-
sor to increase the air pressure supplied to the engine. The air pres-
sure is controlled by a mechanical bypass Ô¨Çap: When the Ô¨Çap is
completely open (resp. closed), the air pressure is minimum (resp.
maximum). Our industrial subject is the Supercharger Bypass Flap
Position Controller (SBPC) which determines the position of the
bypass Ô¨Çap to achieve a desired air pressure. SBPC is one of the
most complex controllers among those dedicated to engine man-
agement. In SBPC, the desired andactual variables (see Figure 1)
represent the desired and actual positions of the Ô¨Çap, respectively.
The Ô¨Çap position is bounded within [0::1] (0for open and 1:0for
closed), i.e.,RID=RFD=[0::1] in our experiments. The SBPC con-
troller and plant models are both implemented in Simulink and in-
clude 443blocks in total. SBPC has six conÔ¨Åguration parameters
Cal1 toCal6 impacting the PID controller terms, and hence, the
controlling behavior of SBPC. Figure 4 shows the ranges for the
SBPC conÔ¨Åguration parameters.
4.3 Surrogate Modeling Evaluation Metrics
To assess the goodness of Ô¨Åt and predictive power of the gener-
ated surrogate models, we use two well-known evaluation metrics
for supervised learning methods: (1) coefÔ¨Åcient of determination or
R2[38] and (2) Mean of Relative Prediction Error or MRPE [27].
SpeciÔ¨Åcally, R2measures the proportion of the total variance of F
explained by ^Ffor the observations in the training set. In other
words,R2is a measure of goodness of Ô¨Åt on the training set. The
value ofR2is always less than 1. The higher the value of R2, the
higher the goodness of Ô¨Åt of ^Fpredictions.
The Mean of Relative Prediction Error (MRPE) [27] measures
the predictive power of ^Fusing the observations from the test set.
Letkbe the number of observations in the test set. We compute the
MRPE as follows:
1
kPk
i=1jF(pi) ^F(pi)
F(pi)j
That is, MRPE measures the average of the relative prediction
errors of ^Ffor the observations in the test set. The lower the value
of MRPE, the higher the predictive power of ^F. Note that a surro-
gate model ^Fwith highR2may not yield low MRPE values due
tooverÔ¨Åtting, i.e., when ^Fis excessively tailored to the training
observations, but does not generalize well to the test observations.
Therefore, to compare different surrogate models, we have to take
into account both R2and MRPE metrics.
In theory, both R2and MRPE can be computed on either the
training set or the test set. When applied to the training set, they
measure the goodness of Ô¨Åt, and when applied to the test set, they
assess the prediction accuracy. However, it is more common to use
R2to measure goodness of Ô¨Åt, to get an idea of how much variance
remains unexplained in the data used to Ô¨Åt the model, and MRPE
to address prediction accuracy as it is more readily interpretable to
assess the applicability of a prediction model. Therefore, we chose
to computeR2on the training set and MRPE on the testset.
1694.4 Experiment Design and Analysis Strategy
We implemented the elementary effect analysis, adaptive ran-
dom search, surrogate modeling techniques including LR, ER, and
PR(n = 2;3), and our single-state search algorithm in MATLAB.
Both adaptive random search and single-state search are required to
call simulations of the SBPC Simulink model. The latter, in addi-
tion, may resort to surrogate modelling by calling and reÔ¨Åning sur-
rogate models. We ran each Simulink simulation for 2sec (T = 2s
in Figure 2) to give SBPC enough time to stabilize. We ran all the
experiments on Amazon micro instance machines which is equal
to two Amazon EC2 Compute Units. Each unit has a CPU capac-
ity of a 1.0-1.2 GHz 2007 Xeon processor. Each 2-sec Simulink
simulation of SBPC takes about 31sec on the Amazon machine.
While calling the surrogate models is negligible (less than 5ms)
and rebuilding them takes about 2sec on average.
To investigate RQ1-RQ4, we designed and performed the fol-
lowing experiments. Below, we use abbreviations DR and SM for
dimensionality reduction and surrogate models, respectively.
EXP-I. To answer RQ1 andRQ2, we computed the output of ex-
ploration once with and once without DR. To compute exploration
results with DR, we applied the elementary effect analysis to our
three objective functions with parameters r= 20 and = 0:556.
Recall from Section 3.1 that ris the number of points, and is the
size of the modiÔ¨Åcation applied to each dimension of each point.
These values are selected based on the guidelines in [6]. To ac-
count for randomness we repeated the elementary effect analysis
10times. The results across different repetitions were consistent,
and out of the eight input space dimensions, four were signiÔ¨Åcant
forFst, and three were signiÔ¨Åcant for FrandFsm. We then ap-
plied our adaptive random search to each of these functions by fo-
cusing the exploration on their signiÔ¨Åcant dimensions only. We let
N= 1000, and executed our adaptive random search to generate
1000 points forFst, and 1000 points forFrandFsm. Note that
sinceFrandFsmhave the same signiÔ¨Åcant dimensions, we gener-
ated the same points for both functions, but kept two output values
for each point.
To compute exploration results without DR, we let N= 2000,
and generated 2000 points across the eight dimensions of the input
space, but computed Fst,Fr, andFsmseparately for each point.
Note that the total number of points generated during exploration
with and without DR was the same and equal to 2000.
For each objective function, we built two regression trees: one
from the exploration results with DR, and one from the exploration
results without DR. Each regression tree node corresponds to an
input space partition. Suppose that andrespectively denote
the standard deviation and mean values related to each partition.
We expand each regression tree until
for every leaf node falls
below 0:1. Expanding the trees further often results in leaf nodes
containing very few observations and corresponding to very small
space partitions. By expanding the tree, the variance of the objec-
tive function values ( ) in leaf node partitions decrease. In each
tree, among all the leaf nodes with
<0:1, we select the one that
has the highest for further search of worst case scenarios.
For the partitions with the highest , based on their observation
points, we created four surrogate models (SMs) using LR, ER, and
PR (n = 2;3) techniques. To effectively apply surrogate modeling
techniques, we want to have at least 200points in the selected parti-
tions. If lower, we generate additional points before building a SM.
We then use these 200points as training data to build SMs. For the
test sets, we generate an additional 50points using a naive random
selection technique. That is, the test points are chosen the same
way irrespective of the use of DR. This allows us to use the sametest sets to facilitate the comparison of the SMs generated with and
without DR.
In total, to obtain the exploration results with and without DR,
we performed 24 experiments (3 objective functions, 4 surrogate
modeling techniques, with/without DR). To account for random-
ness, we repeated each of the 24 experiments 10 times.
EXP-II. To answer RQ3 andRQ4, we performed single state search,
for a given input space partition, once with and once without using
SMs. To compute the search results with SM, we Ô¨Årst built a SM
using the best technique identiÔ¨Åed in RQ1. Then, for each objective
function, we applied our SM-based single state search algorithm in
Figure 8 to a given input space partition. We ran this algorihtm
for three conÔ¨Ådence levels (cl ):80,90, and 95. For each objective
function and each conÔ¨Ådence level, we ran the search for 3000 sec.
To compute the search results without SM, for each objective
function, we applied a Hill Climbing (HC) algorithm similar to that
used in our previous work [21] to a given input space partition.We
let the search run for 3000s, i.e., the total search budget time for the
single state search with and without SM was the same and equal
to 3000s. We refer to our SM-based single state search algorithm
in Figure 8 as HC-SM, and to our single-state search algorithm
without SM as HC-NoSM.
In total, to obtain the search results with and without SM, we per-
formed 9 experiments with SM (3 objective functions, 3 conÔ¨Ådence
levels), and 3 experiment corresponding to the 3 objective functions
without SM. To account for randomness, we repeated each of the
12 experiments 30 times.
5. EXPERIMENT RESULTS
This section provides responses, based on our experiments, for
research questions RQ1 toRQ4 described in Section 4.1.
RQ1. To answer RQ1, we use the SMs generated by the EXP-I
experiments (Section 4.4). Since EXP-I includes 24 experiments,
and each experiment was repeated 10times, we obtain 24 different
groups of SMs where each group consists of 10 different SMs. For
all the SMs, we compute R2and MRPE values. Figure 10(a) shows
the averageR2and average MRPE values for 12 SM groups gener-
ated forFsm,Fst, andFrbased on the exploration results with DR.
As shown in Figure 10(a), the SMs built using PR (n = 3) have the
best goodness of Ô¨Åt (highest R2) and best predictive accuracy (low-
est MRPE). The results for the other 12 SM groups generated from
the exploration results without DR are consistent with those shown
in Figure 10(a). SpeciÔ¨Åcally, our results conÔ¨Årm that, compared to
LR, ER and PR(n = 2), PR (n = 3) generates the most accurate
SMs for our objective functions.
In addition, the results in Figure 10(a) show that while the sur-
rogate models generated by PR (n = 3) forFsmandFrhave high
goodness of Ô¨Åt and predictive power, this is not the case for Fst.
Additionally, though due to space constraints this cannot be shown
here, we observed that applying PR with n >3results in less ac-
curate SMs, i.e., lower R2and higher MRPE.
RQ2. To answer RQ2, we focus on the best SMs in the EXP-I
experiments, i.e., the SMs generated by PR( n= 3). Figure 10(b)
shows the MRPE distributions related to the best SMs generated
from the exploration results both with and without DR, and for each
ofFsm,FrandFst. To statistically compare the MRPE values,
we performed the non-parametric pairwise Wilcoxon Pairs Signed
Ranks test [7], and calculated the effect size using Cohen‚Äôs d[9].
The level of signiÔ¨Åcance () was set to 0:05, and, following stan-
dard practice, dwas labeled small for 0:2d < 0:5, medium
for0:5d <0:8, and high for d0:8[9]. Testing differences
in MRPE distributions shows that for FsmandFr, the SMs built
170(b) Distribution of MRPE for three objective functions with and without DR
0.00.020.040.060.08
0.05
0.010.020.030.04
0.10.20.3
DRNo DRDRNo DRDRNo DR(a) Mean of     /MRPE values for different surrogate modeling techniques R2
Smoothness(       )FsmResponsiveness(     )FrStability(     )FstFstFsmFrPR(n=3)R2/MRPE0.66/0.05260.95/0.02030.78/0.02950.26/0.20430.98/0.01290.85/0.02470.85/0.02450.46/0.17550.54/0.16710.44/0.07910.49/1.22810.22/1.2519LRR2/MRPEERR2/MRPEPR(n=2)R2/MRPEFigure 10: Experiment results for RQ1 and RQ2: (a) Compar-
ing different surrogate modelling techniques, and (b) compar-
ing exploration results with and without dimensionality reduc-
tion (DR).
with DR have signiÔ¨Åcantly better predictive power than those built
without DR. In addition, the effect size is ‚Äúhigh‚Äù for both Fsmand
Fr. ForFst, however, there is no statistically signiÔ¨Åcant difference
between the MRPE values of the SMs generated with and with-
out DR. SpeciÔ¨Åcally, focusing exploration on a reduced-dimension
space signiÔ¨Åcantly improves (with a high effect size) the predictive
power of the SMs for FsmandFr, but does not have a signiÔ¨Åcant
impact on the predictive power of the SMs related to Fst. This
may be due to the SMs for Fstbeing less accurate than those for
FsmandFr. Since based on our results, DR never decreases the
predictive power of the resulting SMs, our results suggest to focus
exploration on the signiÔ¨Åcant dimensions identiÔ¨Åed by DR.
RQ3. To answer RQ3, we use the EXP-II experiments (Section 4.4).
Recall that in EXP-II, each run of each algorithm was executed
for 3000 sec. In each run, we recorded the value of the variable
highest , i.e., the highest found output of the objective function (see
Figure 8), at every 100 sec time interval. Figure 11 compares the
value distributions of highest obtained from HC-SM with cl= 80,
90, and 95, and from HC-NoSM at some selected (representative)
time points for each of the objective functions Fsm,Fst, andFr.
SpeciÔ¨Åcally, Figure 11(a) shows the value distributions of highest
forFsmat 800s, 1500s, 2500s, and 3000s. Figure 11(b) shows
the value distributions of highest forFrat 200s, 300s, and 3000s,
and Figure 11(c) shows the value distributions of highest forFst
at 3000s. Time points, for each Ô¨Åtness function, were selected to
make the overall trends visible for HC-SM and HC-NoSM.
Figures 12(a) and (b) respectively represent the differences be-
tween the mean values found for FsmandFrby each of the HC-
SM algorithms and by HC-NoSM over 3000s of time. For Fsm, as
shown in both Figures 11(a) and 12(a), within 3000s, all the HC-
SM algorithms are able to Ô¨Ånd higher values compared to those
found by HC-NoSM. Among the HC-SM algorithms, cl= 80 Ô¨Ånds
highest values for Fsmat 2500s with a mean of 23% and a me-
dian of 23:4%. HC-SM with cl= 90 andcl= 95 reach slightly
lower values for Fsmat around 3000s. HC-NoSM, however, is not
able to go higher than 22:3%(both mean and median) in the 3000s
allotted time. That is, in 3000s and across 30 different runs, half
of the values computed by HC-SM indicate an over/undershoot of
23:4%or higher, while the highest smoothness violation found by
HC-NoSM is about 22:3%.
The one percent improvement of HC-SM over HC-NoSM is im-
portant in practice. This is because, depending on the hardware
conÔ¨Åguration, engineers specify a maximum over/undershoot (see
Figure 2) that can be tolerated for the smoothness requirement. Ex-
0.215cl=80After 800 secondsAfter 1500 secondsAfter 2500 secondsAfter 3000 seconds
cl=90cl=95NoSM0.2200.2250.2300.235
After 200 seconds
0.1600.1640.168
After 300 secondsAfter 3000 secondsAfter 3000 seconds
0.02200.02220.0224(a) Smoothness (       )Fsm
(b) Responsiveness (     )Fr
(c) Stability (      )Fstcl=80cl=90cl=95NoSMcl=80cl=90cl=95NoSMcl=80cl=90cl=95NoSM
cl=80cl=90cl=95NoSMcl=80cl=90cl=95NoSMcl=80cl=90cl=95NoSMcl=80cl=90cl=95NoSMFigure 11: Boxplots for single-state search output values with
and without surrogate modeling at some selected time points
and applied to: (a) Smoothness (F sm), (b) Responsiveness (F r),
and (c) Liveness (F st).
0.0080.0060.0040.0020.000050010001500200025003000Time (s)
0.0000.0010.0020.0030.004
-0.001050010001500200025003000Time (s)(a) [Mean of HC-SM (cl = 80,90,95)] 
‚ûñ [Mean of HC-NoSM] for  Fsm
FrMean output differencesMean output differencescl=80cl=90cl=95+NoSM
cl=80cl=90cl=95+NoSM(b) [Mean of HC-SM (cl = 80,90,95)] 
‚ûñ [Mean of HC-NoSM] for  
Figure 12: Differences of mean output values of search with
surrogate modeling (HC-SM with cl= 80, 90, 95) and search
without surrogate modelling (HC-NoSM).
ceeding this value, even slightly, is not in general acceptable. In
the particular case of our case study (SBPC), slight deviation for
the smoothness causes the Ô¨Çap to hit other hardware parts, gener-
ating noise and damaging hardware over time. We note that, even
after running HC-NoSM for 5000s, its average and median output
remained at around 22:3%. In general, the overall increase in the
output of HC-NoSM over 5000s was very small.
ForFr, as shown in both Figures 11(b) and 12(b), the HC-SM al-
gorithms Ô¨Ånd their highest values within the Ô¨Årst 300s of time with
cl= 80 being the fastest again. SpeciÔ¨Åcally, at 300s, on average,
HC-SM identiÔ¨Åes a worst response time of 167s, while the average
output of HC-NoSM indicates a response time of 163s. In con-
trast to the results of Fsm, forFr, HC-NoSM is able to match the
HC-SM algorithms in around 2500s of time (see the area shown
by a dashed circle in Figure 12(b)). That is, the HC-SM algo-
rithms are about 8 times faster than HC-NoSM. Finally, for Fst
(Figure 11(c)), we did not observe any noticeable difference be-
tween the values found by the HC-SM algorithms and those found
by HC-NoSM within 3000s of time. That is, within this time, all
the algorithms behaved the same. This is due to the SM for Fst,
which is clearly less accurate than those for FsmandFr. Hence,
171MiL-Testing different conÔ¨ÅgurationsStabilitySmoothnessResponsivenessMiL-Testing Ô¨Åxed conÔ¨ÅgurationsManual MiL-Testing--2.2% deviation 24% over/undershoot 20% over/undershoot 5% over/undershoot 170 ms response time80 ms response time50 ms response timeFigure 13: Comparing our MiL testing results with the results
of MiL testing Ô¨Åxed controller conÔ¨Ågurations [21], and the re-
sults of manual MiL testing.
forFst, the HC-SM algorithms almost run Simulink simulations at
every search iteration, producing the same results as HC-NoSM.
We conclude that for accurate SMs with high predictive power
(i.e., those built for FsmandFr), HC-SM outperforms HC-NoSM.
SpeciÔ¨Åcally, for Fsm, HC-SM computes higher output values that
could not be computed by HC-NoSM, and for Fr, HC-SM is about
eight times faster than HC-NoSM in Ô¨Ånding the same output. This
is because, compared to HC-NoSM, HC-SM runs fewer
Simulink simulations, relying on the SM output in many search
iterations. In addition, HC-SM with cl = 80 is slightly faster than
HC-SM with cl = 90 and 95 because it runs fewer simulations.
RQ4. To demonstrate practical usefulness of our approach, we ar-
gue that MiL testing for different conÔ¨Ågurations of the SBPC con-
troller Ô¨Ånds requirements violations that have neither been identi-
Ô¨Åed via MiL testing of Ô¨Åxed conÔ¨Ågurations, nor by manual testing
based on domain expertise.
Figure 13 compares our results with the results of our previ-
ous work on MiL testing with Ô¨Åxed conÔ¨Åguration parameters [21,
20], and the results of manual expertise-based MiL testing. As
shown in the Ô¨Ågure, by extending our approach to test different
conÔ¨Ågurations within some given ranges, we were able to identify
a critical violation of the stability requirement with a deviation of
2:2%. Note that our previous results [21, 20] as well as the results
from manual testing never indicated any stability error in SBPC.
In addition, for smoothness and responsiveness, our current work
found more critical violations: SpeciÔ¨Åcally, the maximum observed
over/undershoot was 24% compared to the 20% found by our pre-
vious work, and 5%found by manual testing. Finally, we computed
a worst response time of 170ms compared to 80ms found by our
previous work and 50ms identiÔ¨Åed via manual testing.
We conclude our results by noting that, due to limitations of
manual testing, MiL testing in practice mostly focuses on a single
controller conÔ¨Åguration which is typically the one speciÔ¨Åed based
on HiL conÔ¨Åguration parameters. This obviously falls short when
the controller is conÔ¨Ågured and deployed on a hardware with pa-
rameters that differ from those used on HiL. Since existing MiL
testing does not consider different conÔ¨Ågurations, the errors that
could have been found at MiL level go unnoticed until the very late
development stages. Our work attempts to alleviate this shortcom-
ing by enabling MiL testing for various conÔ¨Ågurations obtained by
varying conÔ¨Åguration parameters within their given ranges.
6. RELATED WORK
Several approaches to testing and analysis of Simulink models
rely on formal methods [11], e.g., by analyzing hierarchical Ô¨Ånite
state machines [35], verifying logic control systems [22], and ap-
plying formal model checking [37]. These approaches are more
amenable to veriÔ¨Åcation of logical and state-based behaviors. In
our work, we focused on testing pure continuous controllers which
have not been previously captured by any discrete-event or mixed
discrete-continuous notation [14, 2]. Further, our approach does
not require any additional modelling since we apply our technique
directly to Simulink models already developed as part of the con-
troller development process in industry.Search-based techniques have been applied to Simulink models
to generate test input data with the goal of maximizing some cov-
erage criteria [40, 12]. These criteria, however, are inadequate for
testing continuous behaviours. Further, coverage criteria satisfac-
tion alone is a poor indication of test suite effectiveness [29]. Our
work enables generation of test cases speciÔ¨Åcally for controllers
and based on their high-level requirements.
Continuous controllers have been widely studied in control en-
gineering [25, 36], where the focus has been to optimize controller
behaviors for a speciÔ¨Åc hardware [3]. Such optimization techniques
are largely performed at late stages of development on real hard-
ware and cannot replace our testing approach which is particularly
useful for early design and development of controllers.
Surrogate modeling has been used to scale up computation in
various application domains such as avionics [26], chemical sys-
tems [5], and medical domain [10]. Similar to our work, they use
surrogate models in the context of evolutionary algorithms, and
their goal is to approximate complex mathematical models with
faster-to-compute models, i.e., surrogate models, to speed up highly
time-consuming and costly simulations and experiments. Our work
is the Ô¨Årst to apply surrogate models for testing continuous con-
trollers. Furthermore, our Hill Climbing (HC) single-state search
algorithm represents a new combination of surrogate modelling and
evolutionary algorithms by precisely showing when the search has
to run real simulations and when the surrogate model is sufÔ¨Åcient.
Finally, our algorithm continuously reÔ¨Ånes surrogate models using
the real simulations performed during the search.
In this work, we applied surrogate modeling in conjunction with
dimensionality reduction to test controllers. Dimensionality reduc-
tionin our work differs from input domain reduction used in soft-
ware code testing where the goal is to remove irrelevant variables
prior to test case generation via static analysis [13, 23]. By dimen-
sionality reduction, we mean identifying signiÔ¨Åcant dimensions of
each Ô¨Åtness function to focus exploration on those dimensions.
7. CONCLUSIONS
Testing and veriÔ¨Åcation of the software embedded into cars is
a major challenge in the automotive industry. The problem be-
comes more pressing when engineers have to consider different
hardware conÔ¨Ågurations on which the software will be eventually
deployed. In this paper we proposed an approach to MiL testing
of continuous controllers in large conÔ¨Åguration spaces, based on
meta-heuristic search, with respect to smoothness, responsiveness,
and stability requirements. The main challenge is to scale search
to large multi-dimensional input spaces made up of possibly many
conÔ¨Åguration parameters. To scale search, we combined techniques
for dimensionality reduction and supervised learning to build sur-
rogate models that accurately predict simulation results without re-
sorting to simulation in many cases. Our evaluation shows that our
approach is able to identify critical violations of the controller re-
quirements that had neither been found by our earlier work [20,
21] nor by manual testing. Further, we showed that combining
search with surrogate modeling remarkably improves our approach
for two out of three requirements. SpeciÔ¨Åcally, for one require-
ment, the search combined with surrogate modeling is eight times
faster than the search without surrogate modeling, and for the other
requirement, the search with surrogate modelling computes more
critical requirements violations than what could be detected by the
search without surrogate modeling.
Acknowledgments
Supported by the Fonds National de la Recherche, Luxembourg
(FNR/P10/03 - VeriÔ¨Åcation and Validation Laboratory, and FNR
4878364), and Delphi Automotive Systems, Luxembourg.
1728. REFERENCES
[1] Continuous Controllers. https://controls.engin.umich.
edu/wiki/index.php/PIDTuningClassical, 2007. [Online;
updated 20-May-2013; accessed 21-Mar-2014].
[2] R. Alur. Timed automata. In CAV, pages 8‚Äì22, 1999.
[3] M. Araki. PID control. Control systems, robotics and
automation, 2:1‚Äì23, 2002.
[4] A. Arcuri and L. Briand. Adaptive random testing: An
illusion of effectiveness? In Proceedings of the 2011
International Symposium on Software Testing and Analysis,
pages 265‚Äì275. ACM, 2011.
[5] J. A. Caballero and I. E. Grossmann. An algorithm for the
use of surrogate models in modular Ô¨Çowsheet optimization.
AIChE journal, 54(10):2633‚Äì2650, 2008.
[6] F. Campolongo, J. Cariboni, and A. Saltelli. An effective
screening design for sensitivity analysis of large models.
Environmental modelling & software, 22(10):1509‚Äì1518,
2007.
[7] J. A. Capon. Elementary Statistics for the Social Sciences:
Study Guide. Wadsworth Publishing Company, 1991.
[8] C.-C. Chang and C.-J. Lin. Libsvm: a library for support
vector machines. ACM Transactions on Intelligent Systems
and Technology (TIST), 2(3):27, 2011.
[9] J. Cohen. Statistical power analysis for the behavioral
sciences (rev). Lawrence Erlbaum Associates, Inc, 1977.
[10] D. Douguet. e-LEA3D: a computational-aided drug design
web server. Nucleic acids research, 38(suppl
2):W615‚ÄìW621, 2010.
[11] F. Elberzhager, A. Rosbach, and T. Bauer. Analysis and
testing of MATLAB/Simulink models: A systematic
mapping study. In Proceedings of the 2013 International
Workshop on Joining AcadeMiA and Industry Contributions
to testing Automation (JAMAICA‚Äô13), pages 29‚Äì34. ACM,
2013.
[12] K. Ghani, J. A. Clark, and Y . Zhan. Comparing algorithms
for search-based test data generation of MATLAB/Simulink
models. In IEEE Congress on Evolutionary Computation,
2009. CEC‚Äô09., pages 2940‚Äì2947. IEEE, 2009.
[13] M. Harman, Y . Hassoun, K. Lakhotia, P. McMinn, and
J. Wegener. The impact of input domain reduction on
search-based test data generation. In Proceedings of the the
6th joint meeting of the European software engineering
conference and the ACM SIGSOFT symposium on The
foundations of software engineering, pages 155‚Äì164. ACM,
2007.
[14] T. Henzinger. The theory of hybrid automata. In LICS, pages
278‚Äì292, 1996.
[15] T. Henzinger and J. Sifakis. The embedded systems design
challenge. In FM, pages 1‚Äì15, 2006.
[16] M. Z. Iqbal, A. Arcuri, and L. Briand. Combining
search-based and adaptive random testing strategies for
environment model-based testing of real-time embedded
systems. In SBSE, 2012.
[17] Y . Jin. Surrogate-assisted evolutionary computation: Recent
advances and future challenges. Swarm and Evolutionary
Computation, 1(2):61‚Äì70, 2011.
[18] E. A. Lee and S. A. Seshia. Introduction to embedded
systems: A cyber-physical systems approach. Lee & Seshia,
2011.
[19] S. Luke. Essentials of Metaheuristics. Lulu, 2009.
http://cs.gmu.edu/~sean/book/metaheuristics/.[20] R. Matinnejad, S. Nejati, L. Briand, T. Bruckmann, and
C. Poull. Automated model-in-the-loop testing of continuous
controllers using search. In Search Based Software
Engineering, pages 141‚Äì157. Springer, 2013.
[21] R. Matinnejad, S. Nejati, L. Briand, T. Bruckmann, and
C. Poull. Search-based automated testing of continuous
controllers: Framework, tool support, and case studies. 35
pages. Accepted for publication at IST Journal, 2014.
[22] M. Mazzolini, A. Brusaferri, and E. Carpanzano.
Model-checking based veriÔ¨Åcation approach for advanced
industrial automation solutions. In Emerging Technologies
and Factory Automation (ETFA), 2010 IEEE Conference on,
pages 1‚Äì8. IEEE, 2010.
[23] P. McMinn, M. Harman, K. Lakhotia, Y . Hassoun, and
J. Wegener. Input domain reduction through irrelevant
variable removal and its effect on local, global, and hybrid
search-based structural test data generation. Software
Engineering, IEEE Transactions on, 38(2):453‚Äì477, 2012.
[24] M. D. Morris. Factorial sampling plans for preliminary
computational experiments. Technometrics, 33(2):161‚Äì174,
1991.
[25] N. S. Nise. Control Systems Engineering. John-Wiely Sons,
4th edition, 2004.
[26] Y . S. Ong, P. B. Nair, and A. J. Keane. Evolutionary
optimization of computationally expensive problems via
surrogate modeling. AIAA journal, 41(4):687‚Äì696, 2003.
[27] H. Park and L. Stefanski. Relative-error prediction. Statistics
& probability letters, 40(3):227‚Äì236, 1998.
[28] A. Pretschner, M. Broy, I. Kr√ºger, and T. Stauner. Software
engineering for automotive systems: A roadmap. In FOSE,
pages 55‚Äì71, 2007.
[29] M. Staats, G. Gay, M. Whalen, and M. Heimdahl. On the
danger of coverage directed test case generation. In
Fundamental Approaches to Software Engineering, pages
409‚Äì424. Springer, 2012.
[30] T. Stauner. Properties of hybrid systems-a computer science
perspective. Formal Methods in System Design,
24(3):223‚Äì259, 2004.
[31] I. Steinwart and A. Christmann. Support vector machines.
Springer, 2008.
[32] The MathWorks Inc. MATLAB quasi random numbers.
http://www.mathworks.nl/help/stats/
generating-quasi-random-numbers.html, 2003. [Online;
accessed 17-Mar-2014].
[33] The MathWorks Inc. Simulink.
http://www.mathworks.nl/products/simulink, 2003.
[Online; accessed 25-Nov-2013].
[34] The MathWorks Inc. Stepwise Linear Regression Model.
http://www.mathworks.nl/help/stats/stepwiselm.html,
2003. [Online; accessed 30-Mar-2014].
[35] I. Toyn and A. Galloway. Formal validation of hierarchical
state machines against expectations. In Software Engineering
Conference, 2007. ASWEC 2007. 18th Australian, pages
181‚Äì190. IEEE, 2007.
[36] T. Wescott. PID without a PhD. Embedded Systems
Programming, 13(11):1‚Äì7, 2000.
[37] M. Whalen, D. Cofer, S. Miller, B. H. Krogh, and W. Storm.
Integration of formal analysis into a model-based software
development process. In Formal Methods for Industrial
Critical Systems, pages 68‚Äì84. Springer, 2008.
173[38] I. H. Witten, E. Frank, and M. A. Hall. Data Mining:
Practical Machine Learning Tools and Techniques: Practical
Machine Learning Tools and Techniques. Elsevier, 2011.
[39] J. Zander, I. Schieferdecker, and P. J. Mosterman.
Model-based testing for embedded systems, volume 13. CRC
Press, 2012.[40] Y . Zhan and J. A. Clark. A search-based framework for
automatic testing of MATLAB/Simulink models. Journal of
Systems and Software, 81(2):262‚Äì285, 2008.
174