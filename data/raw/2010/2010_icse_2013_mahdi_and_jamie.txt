See discussions, st ats, and author pr ofiles f or this public ation at : https://www .researchgate.ne t/public ation/261039033
Comparing Multi-Point Stride Coverage and dataÔ¨Çow coverage
Conf erence Paper ¬†¬† in¬†¬†Proceedings - Int ernational Conf erence on Softw are Engineering  ¬∑ May 2013
DOI: 10.1109/IC SE.2013.6606563
CITATIONS
19READS
91
2 author s, including:
Mohammad Mahdi Hassan
Qassim Univ ersity
16 PUBLICA TIONS ¬†¬†¬†92 CITATIONS ¬†¬†¬†
SEE PROFILE
All c ontent f ollo wing this p age was uplo aded b y Mohammad Mahdi Hassan  on 12 Dec ember 2018.
The user has r equest ed enhanc ement of the do wnlo aded file.Comparing Multi-point Stride Coverage and
DataÔ¨Çow Coverage
Mohammad Mahdi Hassan
Computer Science Department
University of Western Ontario
London, Canada
Email: mhassa47@csd.uwo.caJames H. Andrews
Computer Science Department
University of Western Ontario
London, Canada
Email: andrews@csd.uwo.ca
Abstract ‚ÄîWe introduce a family of coverage criteria, called
Multi-Point Stride Coverage (MPSC). MPSC generalizes branch
coverage to coverage of tuples of branches taken from the
execution sequence of a program. We investigate its potential as
a replacement for dataÔ¨Çow coverage, such as def-use coverage.
We Ô¨Ånd that programs can be instrumented for MPSC easily,
that the instrumentation usually incurs less overhead than that
for def-use coverage, and that MPSC is comparable in usefulness
to def-use in predicting test suite effectiveness. We also Ô¨Ånd that
the space required to collect MPSC can be predicted from the
number of branches in the program.
Keywords -Software Testing; Control Flow Coverage; Data Flow
Coverage; Partial Execution Pattern.
I. I NTRODUCTION
Structural coverage measures measure how much of a pro-
gram‚Äôs structure a test suite exercises. Some form of coverage
criteria have been used in software testing for decades to judge
whether test suites are thorough enough [1].
In broad terms, structural coverage criteria can be divided
into two different domains: ‚ÄúControl Flow Coverage‚Äù (CFC)
and ‚ÄúData Flow Coverage‚Äù (DFC). CFC includes measures like
branch coverage, which requires that each direction of each
branch be taken by some test case. DFC includes measures
like def-use coverage, which requires that every def-use path
(path from an assignment of a value to a variable to a use of
that assigned value) is taken by some test case.
In general it is accepted that DFC is superior to CFC
in terms of Ô¨Ånding and locating faults. There is a tradeoff
between them, as DFC is complicated to implement and its
measurement incurs more overhead than CFC, so in practice
some form of CFC is accepted and widely used in the software
industry. In this paper, we bridge the gap between the measures
by introducing a family of CFC criteria which we call ‚ÄúMulti-
Point Stride Coverage‚Äù (MPSC). Informally, instrumentation
for MPSC with gap gandppoints records the coverage of
tuples (b1;:::;b p)of branches taken, where each branch in
the tuple is the one taken gbranches after the previous one.
Our research addresses four main research questions:
RQ1. How many MPSC tuples typically need to be
collected for a program, and how is that number related
to other program metrics?RQ2. What is the relationship between MPSC and DFC
criteria such as def-use?
RQ3. Does MPSC lead to a more accurate assessment of
test suite effectiveness than def-use?
RQ4. What is the performance overhead of collecting
MPSC, compared to the uninstrumented program and to
DFC?
We addressed these questions by experimentation on 15
programs of various sizes in three commonly-used program-
ming languages. Our main Ô¨Åndings on these programs are
summarized below.
Every program is associated with a constant Cwhich
allows us to accurately predict how much storage to
allocate for MPSC data collection, given the number of
branches in the program.
The def-use coverage of test suites is strongly correlated
with their branch coverage, raising questions about how
distinct def-use coverage is from branch coverage in
practical settings.
MPSC allows us to predict the effectiveness of a test suite
with a similar or higher level of accuracy than def-use.
The instrumentation for MPSC is usually more efÔ¨Åcient
that that for def-use coverage.
The simplest member of the MPSC family of criteria is
often the most useful member. However, the family as
a whole provides the user with a wide choice of useful
coverage criteria.
We organize the paper as follows. In Section 2, we give
some basic deÔ¨Ånitions. In Section 3, we discuss related work.
In Section 4, we describe the design and results of our
empirical study of the basic properties of MPSC (RQ1). In
Section 5, we describe the design and results of experiments
to determine the relationships between MPSC and def-use
coverage (RQ2 and RQ3). In Section 6, we describe the design
and results of experiments we did to measure the performance
overhead of MPSC compared to def-use (RQ4). In Section 7,
we present an overall discussion of the results. In Section 8,
we conclude and suggest future work.
II. D EFINITIONS
In this section we give precise deÔ¨Ånitions of the coverage
criteria we consider, in order to facilitate later discussion.Abranch in a program is one direction of an if,for
orwhile decision, or one case of aswitch statement1.
The branch coverage of a test suite (set of test cases) is the
proportion of branches that are executed by at least one test
case [1].
A statement is a deÔ¨Ånition ordefof a variable xif it assigns
xa new value. A statement is a useof a variable xif it contains
a reference to the current value of x. Two statements s1and
s2constitute a def-use pair forxifs1is a def ofx,s2is a
use ofx, and there is a path froms1tos2that does not
pass through any other defs of x[2]. The def-use coverage of
a test suite is the proportion of def-use pair paths that are
executed by at least one test case. We do not discuss in detail
here issues like the treatment of function parameters and the
distinction between P-use and C-use; the reader is referred to
Rapps and Weyuker [2] and Horgan and London [3] for more
detail.
We propose a family of coverage criteria which is a gen-
eralization of branch coverage. If b1andb2are branches, we
say that a test case executes gap-gbranch stride (b1;b2)if
it executesb1, and thengbranches later (interprocedurally) it
executesb2. We deÔ¨Åne the gap-gbranch stride coverage of
a test suite as the proportion of gap- gbranch strides that are
executed by at least one test case.
We generalize this to multi-point stride coverage (MPSC)
by extending the pair (b1;b2)to a sequence of ppoints
(b1;b2;:::;b p), each separated from the next by gbranches
(interprocedurally). We refer to the sequence of ppoints as an
MPSC tuple . In order to handle the beginning and end of the
execution sequence, we assume a sequence of begin pseudo-
branches at the beginning of the sequence, and endpseudo-
branches at the end. For instance, if a small program executes
only the four branches b1;b2;b3;andb4in that order, then
there are six MPSC tuples with gap 1 and 3 points executed by
the program: (begin; begin;b 1),(begin;b 1;b2),(b1;b2;b3),
(b2;b3;b4),(b3;b4;end), and (b4;end;end).
MPSC is thus a family of coverage criteria, one for each
value of gap size gand number of points pin a tuple. If p= 1,
then MPSC is the same as branch coverage; if g= 0, then
MPSC is equivalent to branch coverage, since all the points
in an MPSC tuple with g= 0 are the same. MPSC can be
extended to other kinds of coverage features (e.g., statements
or conditions); the key consideration is that those features must
be collected according to their execution sequence. In this
paper, we concentrate on MPSC based on branch coverage,
since branch coverage is a moderately strong and commonly-
used coverage measure.
We use covset (S;t;g;p )to mean the set of unique MPSC
tuples covered by test case tfor program Swhen using gap g
andppoints. We deÔ¨Åne ncov(S;t;g;p ) =jcovset (S;t;g;p )j.
We extend this to test suites, i.e. sets Tof test cases,
by deÔ¨Åning covset (S;T;g;p )as the union of all the sets
covset (S;t;g;p )for allt2T, and ncov(S;T;g;p ) =
1We restrict our attention to C, C++ and Java. The concepts can be easily
extended to other languages.jcovset (S;T;g;p )j.
We use maxcov (S;g;p )to mean the maximum value that
ncov(S;T;g;p )can be for any test suite T. The precise
value of maxcov (S;g;p )cannot be determined in general,
since whether a given branch in program Scan be taken is
undecidable. An obvious upper bound is Np, whereNis the
number of branches in the program. Static analysis (analysis
of source, executable or intermediate code) could provide
a tighter upper bound, and dynamic analysis (execution of
test cases) can provide a lower bound. In this paper, we
approximate maxcov (S;g;p )by the size of the union of all
the sets covset (S;t;g;p )for all test cases tin a test pool for
the program S.
III. B ACKGROUND AND RELATED WORK
A. Code Coverage Measures
Structural code coverage measures, such as branch cover-
age, have long been studied as a means for evaluating the
thoroughness of a test suite [1]. Tools that automatically
evaluate given code coverage measures on a test suite are
now commonly used in industry [4], [5]. They are now
increasing in importance, since they are used not only in
test case construction by humans, but also in automatic test
input generation [6], [7], [8], test suite minimization and
prioritization [9], [10], and fault localization [11], [12].
Many coverage measures have been proposed; Zhu et al.
[13] give a comprehensive survey. Coverage measures can be
grouped into the two broad classes of controlÔ¨Çow-based and
dataÔ¨Çow-based measures. The former focus on the Ô¨Çow of
control from one statement to another, such as the ‚Äúbranch
coverage‚Äù mentioned above. DataÔ¨Çow-based measures, in con-
trast, focus on the Ô¨Çow of data from ‚ÄúdeÔ¨Ånition‚Äù (‚Äúdef‚Äù)
statements, which assign a value to a variable, to statements
where that value of the variable is used. For instance, the
statement x = y is a def for variable x. If that statement is
on line 15 of a program, and line 25 of the program is ‚Äú if
(x > 100) ‚Äù, and it is possible for control to Ô¨Çow from line
15 to line 25 without executing any other defs of x, then
the statements on line 15 and 25 form a ‚Äúdef-use pair‚Äù, the
dataÔ¨Çow equivalent of a branch of branch coverage. If line 15
and line 25 are in different conditional blocks, then 100% def-
use coverage would require that the def-free path be executed,
while 100% branch coverage might not require this.
We say that coverage measure A subsumes coverage mea-
sure B if every test suite that achieves A must necessarily
achieve B. Frankl and Weyuker [14] proved subsumption and
similar relations among a wide variety of coverage measures,
including the fact that def-use coverage subsumes branch
coverage. Ammann and Offutt extended this to many other
coverage measures [15]. Ball showed that Predicate-Complete
Test Coverage subsumes statement, branch, multiple condition
and predicate coverage [16].
The use of coverage measures is based on the belief that a
test suite that achieves higher coverage, or a stronger coverage
measure, is more effective at exposing faults in the software
under test. Some research [17] and real-world case studies[4] have borne out this belief. However, a large number of
different test coverage measures have been proposed, and
research is needed to study which are most closely connected
with effectiveness.
B. Effectiveness and Coverage Measures
If coverage measure A subsumes coverage measure B, then
a test suite that achieves 100% A coverage also achieves 100%
B coverage. However, achieving the stronger coverage measure
often involves more effort. If, when applied to actual programs,
achieving 100% of A does not result in a test suite that is more
effective (exposes more faults) than achieving 100% of B, then
the effort of achieving 100% of A is not worth it.
Hutchins et al. [17] showed that test suites that achieved
higher coverage were more effective, and that test suites that
achieved high def-use coverage were more effective than those
that achieved high branch coverage. To compare effectiveness,
they hand-seeded faults into seven subject programs (the
‚ÄúSiemens programs‚Äù) and measured the effectiveness of a test
suite as the percentage of faulty versions that the test suite
exposed. These results raised the possibility that the effort of
achieving measures stronger than branch coverage could pay
off.
Hutchins et al.‚Äôs research left open the question of whether
the test suites that achieved higher coverage or subsuming cov-
erage measures were more effective because of some intrinsic
quality of the coverage measure, or simply because the test
suites they required were bigger (contained more test cases).
Siami Namin and Andrews [18] found that both size of a test
suite (number of test cases) and coverage were good predictors
of effectiveness. They also found that a linear combination of
log(size) and coverage yielded a good numerical prediction
of effectiveness. They did not, however, compare coverage
measures against each other.
In this paper, we follow Hutchins et al.‚Äôs general experimen-
tal design, using mutation [19], [20] to generate faulty versions
automatically. We then compare MPSC to def-use coverage in
order to measure how well each predicts the effectiveness of
a test suite, when test suite size is taken into account.
C. Implementation of Def-Use Coverage
Hutchins et al.‚Äôs research suggests that def-use coverage
is useful. Unfortunately, the practical implementation of def-
use coverage poses several difÔ¨Åculties. The Ô¨Årst is that it is
apparently non-trivial to build a tool set for correctly recording
and reporting def-use coverage. We were able to Ô¨Ånd only
one working, compilable tool set that implemented def-use
coverage for C, one for Java, and none for C++. The C tool
(ATAC [3]) did not manage to successfully instrument and
record coverage for two of the large C utility programs that
we used as subjects. Recently Yang et al. reported on the
coverage criteria measured by a wide range of commercial
tools [5]; none of the tools surveyed implemented any form
of def-use coverage. Although the source code analysis needed
for def-use coverage instrumentation does not seem to be very
complex, the above evidence suggests that its complexity isdeceptive, and/or is high enough to discourage tool vendors
from implementing it.
A second difÔ¨Åculty associated with def-use coverage is that
the needed instrumentation slows down the system under test
more than the instrumentation for controlÔ¨Çow-based coverage.
For some systems this may make testing infeasible, and/or
introduce timing bugs that do not appear in the uninstrumented
system. Santelices and Harrold addressed this concern in their
study of more efÔ¨Åcient ways of implementing def-use coverage
[21], but concluded that more work was needed to improve the
efÔ¨Åciency of def-use coverage.
A Ô¨Ånal difÔ¨Åculty concerns arrays. When the program under
test contains arrays, the def-use tool implementor must decide
whether to consider the whole array as one variable, or each
array entry as a separate variable. If the whole array is
considered one variable, then a def-use pair for the variable
can be measured as executed even if one entry is assigned in
the def and a different entry is used in the use. If each array
entry is considered a separate variable, then the number of def-
use pairs is greatly multiplied. In our research, 20 test cases
for one of the subject programs ( jtopas ), when instrumented
for def-use coverage, yielded bytecode Ô¨Åles that were too big
to be executed on the JVM; cutting down the size of an array
in the code cured the problem, but resulted in tests that did
not do the same thing.
D. MPSC
In response to the above issues, researchers seek coverage
measures that are more predictive of test suite quality than
branch coverage, but not as difÔ¨Åcult or expensive to instrument
or collect as def-use coverage. The candidate measures that we
explore in this paper are the ‚ÄúMulti-Point Stride Coverage‚Äù,
or MPSC, measures deÔ¨Åned above.
We had four main motivations for considering MPSC as
a candidate coverage measure. First, it generalizes branch
coverage, as noted above.
Second, because MPSC tracks sequences of branches, it
may capture some of the same information that def-use
coverage captures. Some def-use pairs connect statements in
distant branches, similar to MPSC tuples with g > 1. It is
easy to show that MPSC with a given value of gandpis
incomparable to (neither subsumes nor is subsumed by) def-
use coverage. However, the more practical question is whether,
when applied to actual programs, MPSC is as predictive of
high test suite quality as def-use coverage is. We address this
question through experimentation, which we report on here.
Third, MPSC is relatively easy to instrument for, in widely-
used procedural languages like C, C++ and Java. We built
a simple source code transformation package, called JQXZ,
that searches for if,while ,for andswitch statement
constructs, and instruments the associated decisions and cases.
We believe that similar simple transformations could be done
on other source languages, and on bytecode and native code.
Fourth, we believed that the instrumentation for collecting
MPSC data could be made efÔ¨Åcient, by storing the data in a
hash table of appropriate size and using an appropriate hashfunction. Our performance experiments, reported on in this
paper, bore out this belief.
Why did we not consider other established measures shown
to subsume branch coverage, such as condition/decision,
multi-condition, or ModiÔ¨Åed Condition/Decision Coverage
(MC/DC)? The problem here is the surprising and important
results of Rajan et al. [22], who showed that it is possible
to subvert the intent of such coverage measures by restruc-
turing programs. Rajan et al. showed that programmers can
restructure a program into one which does the same thing, but
whose 100% MC/DC coverage test suites are not as effective
in exposing bugs. The same analysis holds for all three of the
coverage measures mentioned above. We believe that MPSC
will not be as vulnerable to source code transformation as
MC/DC and related measures, because it is an interprocedural
coverage measure deÔ¨Åned in terms of dynamically-executed
sequences of branches rather than the static structure of
decisions. Studying this belief in more detail, however, is a
subject for future work.
E. Antecedents to MPSC
We have not been able to Ô¨Ånd any mentions of measures
equivalent to MPSC with g > 1andp > 1. However, we
do not claim any novelty for MPSC with g= 1. Measures
similar to MPSC with g= 1 andp> 1have appeared in the
literature in the past, although curiously there has not been
much research on them in recent years, and we believe that
we are the Ô¨Årst to investigate such measures empirically.
Pimont and Rault [23] deÔ¨Åned a hierarchy of coverage
measures equivalent to MPSC with g= 1 . Chow [24]
extended their work to coverage of state machine abstractions
of programs by test cases, in particular ‚Äúswitch cover‚Äù, which
is similar to MPSC with g= 1 andp= 2. Woodward et al.
[25] deÔ¨Åned the notion of LCSAJ, or Linear Code Sequence
and Jump, as a ‚Äúbody of code through which the Ô¨Çow of
control may proceed sequentially and which is terminated by
a jump‚Äù. They also deÔ¨Åned an inÔ¨Ånite hierarchy of coverage
criteria TER i, for which TER n+2wheren1is coverage of
distinct subpaths of length nLCSAJs. There is no discussion
of whether the criterion is meant to be inter- or intraprocedural.
Over 20 years later, Woodward and Hennell [26] showed that
a coverage criterion equivalent to all (intraprocedural) LCSAJ
sequences subsumes MC/DC under certain assumptions about
program structure.
‚ÄúPath coverage‚Äù is usually taken to mean coverage of all
possible paths through a program. Even at the intraprocedural
level, it has long been recognized that the number of such
paths can be inÔ¨Ånite [1]. Zhu et al. [13] deÔ¨Åned a simple path
as one that does not repeat edges , while Ammann and Offutt
[15] deÔ¨Åned a simple path as one that does not repeat nodes ,
except that the Ô¨Årst node can be the last (a reÔ¨Ånement of Zhu
et al.‚Äôs notion of ‚Äú elementary path‚Äù). Simple path coverage
under either deÔ¨Ånition does not subsume MPSC, since an
MPSC tuple can contain a given branch an arbitrary number
of times. However, MPSC with g= 1 and a high enough p
does subsume simple path coverage.Finally, Ntafos [27] deÔ¨Åned required k-tuples coverage as
coverage of all possible linked chains of kdef-use pairs. In
such a chain, a decision may be followed by another decision
that is not the next to be executed, as in MPSC tuples with g>
1. The required k-tuples criterion presumably has the same
implementation and efÔ¨Åciency problems as def-use coverage,
which is a special case of it.
IV. B ASIC PROPERTIES OF MPSC
Our Ô¨Årst research question (RQ1 above) deals with the basic
properties of MPSC: ‚ÄúHow many MPSC tuples typically need
to be collected for a program, and how is that number related
to other program metrics?‚Äù
Data structures for collecting MPSC data, for instance hash
tables, can be made more efÔ¨Åcient if we can approximate
how many entries will be needed. The upper bound for this
number is the measure deÔ¨Åned above, maxcov (S;g;p )for
subject program S, gapgand number of points p, since this
is the size of the set of all (g;p)-tuples that can possibly be
collected for S.
Because MPSC with g= 0orp= 1is equivalent to branch
coverage, an upper bound for maxcov (S;0;p)for anypcan
be determined statically, by simply counting the number of
branches in the program. We therefore wanted to see if there is
a relationship between maxcov (S;0;p)and maxcov (S;g;p )
for generalgandp. Because maxcov (S;0;p)is the same for
anyp, andp= 2 was the smallest pfor which we collected
data, we use maxcov (S;0;2)in this paper.
We use maxcovratio (S;g;p )to mean the ratio
maxcov (S;g;p )=maxcov (S;0;2). We can think of this
as the memory multiplication factor needed to collect MPSC
coverage at gap gandppoints.
A. Subject Programs
We addressed our research questions, and in particular RQ1,
by collecting data on 15 subject programs in three languages
(C, C++ and Java). We obtained the subject programs and
corresponding test pools from SIR, the Software-artifact In-
frastructure Repository ( http://sir.unl.edu/ ), except
concordance , which was converted to a subject program
at our university. There are eleven C, three Java and one C++
program used in our research. We chose these programs as
they are used in similar research and accepted as standard.
In Table I we give the subject programs‚Äô names and relevant
information collected from SIR.
We included the small Siemens programs in our research
(programs 6-12) because of the thoroughness and size of their
pools of test cases. In Table I, we also quantify the beneÔ¨Åts
this thoroughness and size give us. The column ‚ÄúBranch Cov‚Äù
gives the source branch coverage achieved by the entire test
pool of each subject program. The Siemens program test pools
are the ones with the highest coverage; in addition, Rothermel
et al. [9] state that the pools cover every feasible branch at
least 30 times.
Larger test pools allow experimenters to select experimental
testsuites that differ more from each other (are more diverse).TABLE I
LIST OF SUBJECT PROGRAMS . BRANCH COV:BRANCH COVERAGE OF WHOLE TEST POOL . TSD: TEST SUITE DIVERSITY MEASURE . PEPC: PARTIAL
EXECUTION PATTERN CONSTANT .
# Test Branch TSD(50) PEPC
Program Type Language SLOC Cases Cov (%) (%) (C)
1 concordance Text processor C++ 1492 372 84.4 86.6 1.41
2 flex Lexer generator C 10459 567 73.3 91.2 1.26
3 gzip Data compression C 5680 211 48.3 76.3 1.46
4 grep String matching C 10068 809 44.6 93.8 1.43
5 sed Stream editor C 14427 370 34.9 86.5 1.13
6 printtokens Lexical Analyzer C 726 4130 91.5 98.8 1.46
7 printtokens2 Lexical Analyzer C 570 4115 95.8 98.8 1.07
8 replace Pattern matcher C 564 5542 97.6 99.1 1.85
9 schedule Data structure C 412 2650 94.6 98.1 2.34
10 schedule2 Data structure C 374 2710 91.8 98.2 2.04
11 tcas Hardware control C 173 1608 86.7 96.9 0.53
12 totinfo Information measure C 565 1051 87.5 95.2 1.50
13 jtopas Tokenizer library Java 5400 207 71.3 75.8 1.21
14 nanoxml XML parser Java 7646 216 69.1 76.9 0.98
15 xml-security Security library Java 16800 84 36.8 40.5 1.16
Larger test suite diversity allows us to take a more represen-
tative sample from the space of possible test suites for the
program. To quantify this test suite diversity, we deÔ¨Åne TSD( x)
as the expected fraction of the test cases of one randomly-
chosen test suite of size xthat would not appear in another
randomly-chosen test suite of size x. (This is equivalent to
1 (x=n), wherenis the size of the test pool.) In Table I we
give the value of TSD(50) for each program, since the largest
random test suites that we generated were of size 50. Again
the Siemens programs show the highest values of test suite
diversity.
Despite the lower coverage and/or lower test suite diversity
of the other subject programs, we retained them in our set of
subject programs because they are larger and represent realistic
situations in which the available test suites achieve lower than
maximal feasible coverage.
B. Procedure
We instrumented the programs using our instrumentation
tool (JQXZ) so that they recorded to a disk Ô¨Åle the full
sequence of branches followed. We then processed the Ô¨Åle
to extract all the MPSC tuples executed by each test case,
forg= 0 to10and forp= 2 to5. We chose this range of
values based on what we expected would be feasible to record.
We computed the value of maxcov (S;g;p )for each subject
programSand each value of gandp. We then visualized the
results using graphs, and investigated the relationships between
g,p, and maxcov using linear regression.
C. Results and Analysis
Figure 1 shows the relationship between g,p, and maxcov ,
for the subject program replace . As expected, maxcov
increases with increasing gandp. For other programs, the
lines are straighter or concave-down rather than concave-up,
but the relationships between the lines are similar.
Using the statistical package R [28], we searched for linear
relationships between g,p, and maxcov . We Ô¨Ånd that the
‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè ‚óè ‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè ‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
0 2 4 6 8 100 50000 100000 150000Subject Program ‚àí‚àí>siemens/replace
 Gap size vs Maxcov line graph for 2,3,4 and 5 Points
gmaxcov2 Points
3 Points
4 Points
5 PointsFig. 1. Graph of maxcov for given values of gandp, for subject program
replace
following linear model can be used to describe MPSC for
a subject program:
log(maxcovratio (S;g;p )) =Clog(g+ 1)log(p)(1)
whereCis a constant for the subject program S, which we
call the ‚Äúpartial execution pattern constant‚Äù2. We Ô¨Ånd that
this model works for all of our subject programs with high
accuracy3.
2For the program tcas , we omit all points with g > 4. We do this because
there are only a small number of unique paths possible in tcas , and each of
the paths is short, producing anomalous results when g > 4.
3The adjusted R2value, a measure of the predictive accuracy of a linear
model, is greater than 0.96 for all programs and greater than 0.99 for most.
The predicted coefÔ¨Åcients have high statistical signiÔ¨Åcance, with p < 0:001
for all programs.‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
0 2 4 6 80 2 4 6 8
log(maxcov_ratio)C*log(g+1)*log(p)Fig. 2. Actual vs. Predicted values of log(maxcovratio )for all programs.
Interestingly, each program has its own partial execution
pattern constant C. The value of Cfor each program is given
in the last column of Table I. This constant does not seem
closely related to any other program metrics. We attempted to
Ô¨Ånd a model for Cthat worked for all of our subject programs,
taking into account SLOC and number of modules. SLOC
and number of modules did not have a statistically signiÔ¨Åcant
relationship to C. For instance, the smallest programs (the
Siemens programs, which also have the highest test pool
coverage) include the program with the highest value of C
and the program with the lowest value of C.
In Figure 2, we show the actual value of log(maxcovratio )
compared to the value predicted by the model, for all values
ofgandpand all subject programs, where each program‚Äôs
predicted value is based on its own partial execution pattern
constantC.
We can simplify equation 1 to predict the value of maxcov
as follows:
maxcov (S;g;p )=maxcov (S;0;2)p(Clog(g+1))(2)
Using the above equations, we can estimate the number of
MPSC tuples that typically need to be collected for a program.
As noted in section II, maxcov (S;0;2)is equivalent to the
number of branches in the program, which can be determined
by static analysis. To estimate C, we can use the highest
value ofCobserved so far for any program, which is 2.34.
A hypothetical MPSC tool could obtain an upper bound for
maxcov (S;1;2)using source code analysis, and then use
equation 1 to Ô¨Ånd an upper bound for C.
V. MPSC AND DEF-USE
Research questions RQ2 and RQ3 have to do with the
relationship between MPSC and def-use coverage. RQ2 askswhether MPSC with some value of gandpis similar to def-
use, in order to answer the direct question of whether MPSC
can be used in place of def-use. RQ3 asks a question which
is more relevant to software engineers considering the use of
coverage criteria: whether MPSC is as predictive as def-use
of the quality of a test suite.
A. Data Collection
We instrumented as many of our subject programs as
possible with tools that measured def-use coverage. We used
ATAC [3], [29] to instrument the C programs; all seven
of the Siemens programs and two of the large Unix utili-
ties (flex andgrep ) could be instrumented successfully.
ATAC could not successfully instrument the C++ program
(concordance ) or the remaining two Unix utilities ( gzip
andsed). We modiÔ¨Åed ATAC slightly in order to print
unique identiÔ¨Åers for each of the def-use pairs covered by
the program. We obtained Santelices‚Äô tool DUAF [21] and
instrumented our three Java subject programs with it.
We then ran each test case on each of the instrumented
programs, and recorded which def-use pairs were covered
by which test cases; this supplemented the information we
collected earlier (see above) on which MPSC tuples were
covered by which test cases. We were not able to run 20 of
the test cases for jtopas , due to the problem with arrays and
DUAF noted above.
For measuring effectiveness, we generated a full set of
mutant descriptions for each of the source programs, using
Andrews‚Äô tool mutgen [20]. We then randomly selected
mutant descriptions, generated the mutant program, compiled
it and ran the full suite of test cases on the mutated program,
until we obtained 100 non-equivalent mutants of each subject
program4. We recorded which test case killed which mutant.
Finally, we generated 20 randomly-chosen test suites for
each size of test suite from 3 to 50 test cases, for a total
of 960 test cases per subject program. Using the information
about which test case killed which mutant, we computed the
effectiveness of each test suite as the percentage of mutants
killed by the test suite. We also computed the cumulative
MPSC (for each collected gandp) and def-use coverage of
each test suite based on the sets of MPSC tuples and def-use
pairs covered by each test case.
B. Relationship Between the Criteria
In Figure 3 we show a scatter plot of def-use vs MPSC
for the subject program xml-security , where gap size
g= 0 and number of points p= 2 (equivalent to branch
coverage)5; each point in the plot represents one test case.
4An equivalent mutant is one which is not killed (detected) by any test
case. Equivalence of mutants is undecidable in theory and often difÔ¨Åcult in
practice. Here, we follow standard practice and approximate by considering
every mutant not killed by any test case in the test pool as equivalent.
5In all scatter plots, the straight line shows the best linear Ô¨Åt, and the curved
line shows the LOWESS (locally-weighted best Ô¨Åt) smoothing line. We use
number of def-use pairs and/or MPSC tuples covered, rather than percentages,
because we are interested in relationships which do not depend on the scales
of the axes.‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè
200 400 600 800 10001000 2000 3000 4000 5000 6000Subject Program ‚àí‚àí>apache‚àíxml‚àísecurity
DU vs MPSC for Gap Size 0 and Number of Points 2
MPSCDUFig. 3. MPSC vs. DU for g= 0 andp= 2 , subject program
Xml-security .
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè ‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè ‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè ‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè ‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè ‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè ‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè ‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè ‚óè‚óè‚óè ‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè ‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè ‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè ‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè ‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè ‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè ‚óè‚óè
‚óè
‚óè‚óè ‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè ‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè
010000 20000 30000 40000 50000 60000 700001000 2000 3000 4000 5000 6000Subject Program ‚àí‚àí>apache‚àíxml‚àísecurity
DU vs MPSC for Gap Size 10 and Number of Points 5
MPSCDU
Fig. 4. MPSC vs. DU for g= 10 andp= 5 , subject program
Xml-security .
Other programs showed the same strongly linear relationship.
We measured the Pearson correlation of branch coverage and
def-use coverage for all programs, Ô¨Ånding that it was greater
than 0.98 for all programs except jtopas , where it was
0.93. This result indicates that simple branch coverage is
very strongly correlated with def-use coverage. We discuss
the implications of this in Section VII.
As we increase the value of gandp, the linear relationship
between MPSC and def-use coverage deteriorates. Figure 4
shows a similar scatter plot to Figure 3, but for g= 10 and
p= 5. We note that MPSC distinguishes between test suites
‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè ‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè ‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè ‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè ‚óè‚óè
‚óè‚óè ‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè ‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè ‚óè ‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè ‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè ‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè ‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè ‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè ‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè ‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè ‚óè ‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè ‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè ‚óè‚óè‚óè‚óè
2000 4000 6000 800020 30 40 50 60 70 80Subject Program ‚àí‚àí>nanoxml
 Number of Mutants detected vs MPSC scatter plot NOP5 GAP10
MPSCNo of Killed MutantsFig. 5. MPSC vs. number of mutants detected for g= 10 andp= 5,
subject program nanoxml .
TABLE II
ACCURACY OF EFFECTIVENESS MODELS . ‚ÄúN/S‚Äù:NOT STATISTICALLY
SIGNIFICANT . ‚ÄúN/A‚Äù:NOT AVAILABLE DUE TO ATAC LIMITATIONS .
Branch Best Best DU
Program AdjR2(g,p) AdjR2AdjR2
1 concordance 0.9135 (0,2) 0.9135 n/a
2 flex 0.8306 (9,2) 0.9246 0.8687
3 gzip n/s (4,3) 0.6836 n/a
4 grep 0.8759 (3,2) 0.8890 0.8853
5 sed 0.9525 (0,2) 0.9525 n/a
6 printtokens 0.9491 (0,2) 0.9491 0.8768
7 printtokens2 0.9041 (0,2) 0.9041 0.9042
8 replace 0.8649 (0,2) 0.8649 0.8641
9 schedule 0.6095 (0,2) 0.6095 0.5750
10 schedule2 0.3208 (1,5) 0.4400 0.3600
11 tcas 0.8557 (5,5) 0.8678 0.8314
12 totinfo 0.3489 (0,2) 0.3489 0.3532
13 jtopas 0.7568 (0,2) 0.7568 0.7515
14 nanoxml 0.9014 (1,5) 0.9066 0.8880
15 xml-security 0.888 (0,2) 0.888 0.8869
(assigns different coverage values) more than def-use does at
these levels.
C. Predicting Effectiveness
We Ô¨Årst visualized the effectiveness of the 960 test suites
that we ran for each program. Figure 5 shows a typical such
visualization: each point is one test suite, the X axis shows
cumulative number of MPSC tuples covered for g= 5 and
p= 10 , and the Y axis shows number of mutants killed.
Siami Namin and Andrews [18] found that both size and
coverage contributed to an accurate prediction of test suite
effectiveness. We therefore reÔ¨Åne RQ3 to the following: ‚ÄúDoes
using the size of a test suite and MPSC lead to a more accurate
model of test suite effectiveness than using the size of the test
suite and def-use coverage?‚ÄùThis formulation of the question accounts for the confound-
ing factor of size in test suite effectiveness. When a test
case is added to a test suite, it cannot decrease the suite‚Äôs
effectiveness; on average, it increases it by a given nonzero
amount. The reÔ¨Åned RQ3 essentially factors out this coverage-
neutral amount of added effectiveness, allowing us to measure
how much additional value we get out of increases in coverage.
To answer the reÔ¨Åned question, we used R to Ô¨Åt models of
the form
AM =c1+c2log(size ) +c3coverage (3)
to our test suite data, where AM (Adequacy on Mutants), is
the percentage of mutants killed by the test suite, size is the
number of test cases, and coverage is the coverage of the test
suite, in MPSC tuples or def-use pairs. We did this for each
value ofgandpand for def-use. We measured the accuracy
of the resulting models by the adjusted R2measure.
The results are summarized in Table II. Column 4 gives the
setting ofgandpthat resulted in the most accurate linear
model for MPSC. Columns 3, 5 and 6 show the adjusted R2
value of the models from branch coverage ( g= 0;p= 2),
from the best setting, and from def-use coverage. The boldface
number in each row is the adjusted R2of the most accurate
model. We consider only cases in which both factors had a
statistically signiÔ¨Åcant effect on effectiveness ( p< 0:05). (For
gzip , in the branch coverage case, log(size )did not have a
statistically signiÔ¨Åcant effect on effectiveness.)
For 7 out of the 15 subjects, the most accurate model was
the one yielded by branch coverage; for 6 others, it was the one
yielded by some other setting of MPSC, and for the remaining
2 it was the one yielded by def-use coverage. However, for
9 out of the 15 subject programs (including the two for
which def-use was the most accurate), the accuracy of the
models resulting from branch coverage, from the best setting
of MPSC, and from DU coverage were all within 0.05 of
each other, indicating that there was little practical difference
among the criteria.
For 11 of the 15 subjects, the accuracy of all models
ranked as ‚Äúhigh‚Äù or ‚Äúvery high‚Äù ( >0:70) on the standard
Guilford scale [30]. For one subject ( totinfo ), the accuracy
of all models ranked as ‚Äúlow‚Äù ( <0:40), and for the other
three subjects, the accuracy of all models ranked as ‚Äúlow‚Äù or
‚Äúmoderate‚Äù (between 0.40 and 0.70). This indicates that for
some subjects, there were confounding factors other than test
suite size and coverage that affected test suite effectiveness.
VI. I MPLEMENTATION AND PERFORMANCE
The JQXZ utility instruments source code to make calls
to a library which collects coverage information. The im-
plementations of the library used for the experiments above
simply recorded each branch executed in a Ô¨Åle. We also
developed two other prototype implementations for Java, a
hash set implementation and a bitset implementation, which
were intended to be closer to implementations that could be
used in production environments.In this section, we describe these implementations and give
the results of performance experiments we ran to measure the
overhead of MPSC collection and compare it to the overhead
of def-use coverage.
A. Prototype Implementations
Both prototype implementations of the Java library de-
pended on a hash function for MPSC tuples. We implemented
several hash functions, but we found in exploratory perfor-
mance experiments that the best used a simple algorithm of
the same form as that of the java.lang.String hash
function.
The implementations maintain a circular buffer representing
the lastg(p 1) + 1 branches executed. The branches are
identiÔ¨Åed by source Ô¨Åle, line and character number, and
(for decisions) whether true or false. Each branch causes an
element to be added to the circular buffer and the newly
completed MPSC tuple to be recorded as covered.
The hash set implementation stores each covered tuple in a
hash set. The initial size of the hash set can be computed based
on the maximum expected number of tuples needed, as out-
lined in Section IV-C. The advantage of this implementation
is that the exact set of tuples is recorded. The disadvantage
(compared to the bitset implementation) is that a larger amount
of storage is needed.
The bitset implementation computes the hash code hof
each covered tuple, and stores it by setting the h-th bit in a
java.util.BitSet to 1. The advantage of this implemen-
tation over the hash set is that much less storage is needed.
The disadvantages are that it is impossible to extract which
tuples have been covered, and that two or more tuples may
hash to the same value, losing information about precisely how
many tuples have been covered. This implementation might
still be useful, for instance for automated test input generation
schemes which only have to compute whether one test case
has covered some line of code not covered earlier.
B. Performance
To measure the performance of our implementations and
compare it to that of def-use, we chose our three Java
subject programs and added two programs used by Santelices
and Harrold [21], tcas (a Java translation of the Siemens
program) and scimark2 (a JVM performance benchmark).
We ran the test suite for each program on the uninstrumented
program, the program instrumented for def-use coverage by
the state-of-the-art tool DUAF developed by Santelices [21],
and the program instrumented for MPSC by our tool. We
were not able to successfully instrument scimark2 for DU
coverage with the current version of DUAF.
For our tool, we ran the program for every setting of gfrom
0 to 10 and pfrom 2 to 5. We ran each program 10 times,
except for scimark2 , which we ran 100 times because it uses
a random number generator to generate some of its test data.
We measured CPU time by the ‚Äúuser‚Äù time reported by the
Unix time facility in bash . We then calculated the average
time to run one test case or (for scimark2 ) the average timeTABLE III
PERFORMANCE DATA .
Uninst DU overhead (%) MPSC overhead (%)
Subject (CPU sec) SH HA Hashset Bitset
jtopas 0.3117 ‚Äì 2078 460.2 594.1
nanoxml 0.1862 ‚Äì 71.60 97.90 70.15
xml-sec 4.487 ‚Äì 86.90 36.69 31.37
tcas 0.1724 17.91 11.91 5.524 5.376
scimark2 29.31 160.38 ‚Äì 0.4389 -2.703
to run the program as a whole. We averaged the results for all
values ofgandpto get a summary value for MPSC.
Table III shows the results. Results are reported for the
uninstrumented program in CPU seconds, and for the other
columns in percentage overhead (extra time needed) for
the instrumentation. The experiments were run on a Sun
UltraSPARC-IIIi with a 1.593GHz processor and 4GB of
memory. For def-use coverage, we give the overhead reported
in [21] where it is available (column SH), and also the
overhead we calculated (column HA), since CPUs and JVMs
can vary.
Table III shows that the overhead for our prototype bitset
implementation was always less than that of DUAF, and that
the overhead for our prototype hash set implementation was
less than that of DUAF for every program except nanoxml .
The overhead for DUAF was greater than both the average
overhead for MPSC across all gandp, and the average
overhead for any individual gandp, except in the case of
nanoxml using the hash set implementation.
In the case of scimark2 , paradoxically, not only did the
MPSC instrumentation have very little effect, on average the
software instrumented with the bitset implementation took
less time than the uninstrumented version. We attribute this
to random noise resulting from choice of random seed by
different runs, and possibly to operating system effects due
to amount of memory allocated for a process.
The average time varied with different values of gandp,
but not in any clear pattern. This was probably due to different
rates of hash collisions.
C. Accuracy of Bitset Implementation
As mentioned above, the bitset implementation may lose
information about how many tuples have been covered, due
to hash collisions. We therefore studied the question of how
much information was lost.
For each subject program, each setting of gandp, and each
test case, we collected the number of MPSC tuples reported as
covered by the hash set and the bitset implementation. For each
subject program, we then calculated the percentage decrease
in number of tuples reported as covered, as an average across
all test cases and all settings of gandp.
We found that the average loss of accuracy ranged from
0.0321% to 0.0754% for all programs, or less than 1 out of
1000 tuples lost due to hash collisions. This result indicates
that the bitset implementation could be useful in situations
where high accuracy is not needed.VII. D ISCUSSION
A. Threats to Validity and their Mitigation
Internal validity: We checked and visualized data and the
results of statistical analyses in various ways. We collected
data forg= 0 and several values of pto conÔ¨Årm that they
were the same, in order to increase our conÔ¨Ådence in our
experimental procedures. The use of many randomly-chosen
test suites was intended to increase internal validity.
Construct validity: Mutation adequacy of randomly con-
structed test suites has been shown to be a good measure
of effectiveness, but other measures are possible. For subject
programs with low-coverage test suites, we may have judged
some mutants to be equivalent which were not; however, as
noted below, our results were consistent across subjects.
External validity: Our results do not extend to other variants
of MPSC, such as intraprocedural variants, or variants based
on statement, block, or condition coverage. We hope to study
such variants in the future. The subjects that we used may not
be representative of real-world software. Our subjects have
complementary strengths and weaknesses: the small Siemens
programs have large, thorough test pools, and the programs
with smaller, less thorough test pools are larger in SLOC. We
are not aware of any subject programs that are larger than the
Unix utilities we used here and also have test pools approach-
ing the size and thoroughness of the Siemens programs. Our
results are consistent across program sizes and test pool sizes,
and across three related but distinct programming languages.
B. Def-use and Branch Coverage
Our most unexpected result was the high correlation of def-
use coverage with branch coverage, and the lack of beneÔ¨Åt
of def-use coverage over branch coverage for predicting test
suite effectiveness. The Hutchins et al. study [17] had led us
to expect that def-use would distinguish between test suites
more than branch coverage, and that def-use would be a better
predictor of test suite effectiveness.
Our results, however, show that when test suite size is
taken into account, def-use coverage performs very similarly
to branch coverage. These results are consistent with those
reported in [18]. Together they may indicate that the im-
proved effectiveness of high def-use coverage test suites in
the Hutchins et al. study is an artefact of the fact that high
def-use coverage test suites need to be bigger than high branch
coverage test suites.
We should note that these results do not extend to other
problems in software testing. Santelices et al., for instance,
showed that DU coverage, and combinations of DU coverage
with other forms of coverage, yielded more accurate fault
localization than branch coverage [31]. More experimentation
would be needed to show whether MPSC coverage can replace
DU coverage for fault localization.
C. MPSC and Branch Coverage
We found that branch coverage was often the most accurate
predictor of test suite effectiveness when combined with test
suite size. However, for some subject programs, MPSC withsome value of g > 0andp > 1was a better predictor. We
also found that our instrumentation for MPSC was efÔ¨Åcient
even withg> 0andp> 1.
These results taken together suggest that users seeking a
coverage criterion stronger than decision coverage can use
MPSC with g= 1 and highp. Since this form of cover-
age subsumes decision coverage, is usually more efÔ¨Åcient to
collect than def-use coverage, and sometimes yields a better
prediction of test suite effectiveness than def-use coverage, it
may present a better option than def-use coverage.
VIII. C ONCLUSION AND FUTURE WORK
In this paper, we deÔ¨Åned and presented the results of empir-
ical studies on Multi-Point Stride Coverage (MPSC), a form
of coverage that subsumes branch coverage. We compared it
to def-use coverage, a well-known dataÔ¨Çow coverage criterion.
We found that the maximum number of MPSC tuples that
need to be collected is highly predictable, given a constant
which is characteristic of a program. We found that def-use
coverage is strongly correlated with branch coverage and often
does not yield a better prediction of test suite effectiveness than
branch coverage, when test suite size is taken into account.
We also found that MPSC with g > 0andp > 1some-
times yielded a better prediction of effectiveness than branch
coverage. Finally, we developed prototype implementations of
MPSC data collection libraries that performed well compared
to a state-of-the-art def-use coverage tool.
Future work includes improving the efÔ¨Åciency of our pro-
totype implementations of MPSC, and exploring variants of
MPSC and applications to other problems in software testing,
such as fault localization and test suite prioritization.
ACKNOWLEDGMENT
Thank you to Gregg Rothermel and the University of
Nebraska-Lincoln for the SIR repository, the source of our
subject programs. Many thanks to Ra ¬¥ul Santelices for sharing
and discussing his Java DU-coverage tools, and to Michael
Ernst for his comments on an earlier draft. This research is
supported by NSERC and by an Ontario government OGS
scholarship to Mohammad Mahdi Hassan.
REFERENCES
[1] G. J. Myers, The Art of Software Testing . New York: Wiley, 1979.
[2] S. Rapps and E. J. Weyuker, ‚ÄúSelecting software test data using data
Ô¨Çow information,‚Äù IEEE Trans. Software Eng. , vol. SE-11, no. 4, pp.
367‚Äì375, April 1985.
[3] J. R. Horgan and S. A. London, ‚ÄúData Ô¨Çow coverage and the C
language,‚Äù in Symp. on Testing, analysis, and veriÔ¨Åcation (TAV) , 1991,
pp. 87‚Äì97.
[4] S. Berner, R. Weber, and R. K. Keller, ‚ÄúEnhancing software testing by
judicious use of code coverage information,‚Äù in Intl. Conf. Software Eng.
(ICSE) , Minneapolis, MN, May 2007, pp. 612‚Äì620.
[5] Q. Yang, J. J. Li, and D. M. Weiss, ‚ÄúA survey of coverage-based testing
tools,‚Äù The Computer Journal , vol. 52, no. 5, 2009.
[6] C. Pacheco, S. K. Lahiri, M. D. Ernst, and T. Ball, ‚ÄúFeedback-
directed random test generation,‚Äù in Intl. Conf. Software Eng. (ICSE) ,
Minneapolis, MN, May 2007, pp. 75‚Äì84.
[7] J. H. Andrews, F. C. H. Li, and T. Menzies, ‚ÄúNighthawk: A two-
level genetic-random unit test data generator,‚Äù in Intl. Conf. Automated
Software Eng. (ASE) , Atlanta, GA, 2007, pp. 144‚Äì153.[8] I. Ciupa, A. Leitner, M. Oriol, and B. Meyer, ‚ÄúArtoo: Adaptive random
testing for object-oriented software,‚Äù in Intl. Conf. Software Eng (ICSE) ,
Leipzig, Germany, May 2008, pp. 71‚Äì80.
[9] G. Rothermel, M. J. Harrold, J. Ostrin, and C. Hong, ‚ÄúAn empirical
study of the effects of minimization on the fault detection capabilities
of test suites,‚Äù in Intl. Conf. Software Maintenance (ICSM) , Washington,
DC, November 1998, pp. 34‚Äì43.
[10] H. Do and G. Rothermel, ‚ÄúOn the use of mutation faults in empirical
assessments of test case prioritization techniques,‚Äù IEEE Trans. Software
Eng., vol. 32, no. 9, pp. 733‚Äì752, 2006.
[11] J. A. Jones, M. J. Harrold, and J. Stasko, ‚ÄúVisualization of test infor-
mation to assist fault localization,‚Äù in Intl. Conf. Software Eng. (ICSE) ,
Orlando, FL, May 2002, pp. 467‚Äì477.
[12] R. Abreu, P. Zoeteweij, and A. J. van Gemund, ‚ÄúOn the accuracy of
spectrum-based fault localization,‚Äù in Testing: Academia and Industry
Conference - Practice And Research Techniques (TAIC PART‚Äô07) , Wind-
sor, United Kingdom, September 2007, pp. 89‚Äì98.
[13] H. Zhu, P. A. V . Hall, and J. H. R. May, ‚ÄúSoftware unit test coverage and
adequacy,‚Äù ACM Computing Surveys , vol. 29, pp. 366‚Äì427, December
1997.
[14] P. Frankl and E. Weyuker, ‚ÄúProvable improvements on branch testing,‚Äù
IEEE Trans. Software Eng. , vol. 19, no. 10, October 1993.
[15] P. Ammann and J. Offutt, Introduction to Software Testing . Cambridge
University Press, 2008.
[16] T. Ball, ‚ÄúA theory of predicate-complete test coverage and generation,‚Äù
inIntl. Symp. on Formal Methods for Components and Objects (FMCO) ,
Leiden, The Netherlands, November 2004, pp. 1‚Äì22.
[17] M. Hutchins, H. Foster, T. Goradia, and T. Ostrand, ‚ÄúExperiments on the
effectiveness of dataÔ¨Çow- and controlÔ¨Çow-based test adequacy criteria,‚Äù
inIntl. Conf. Software Eng. (ICSE) , Sorrento, Italy, May 1994, pp. 191‚Äì
200.
[18] A. Siami Namin and J. Andrews, ‚ÄúThe inÔ¨Çuence of size and coverage
on test suite effectiveness,‚Äù in Intl. Symp. Software Testing and Analysis
(ISSTA) , Chicago, IL, 2009, pp. 57‚Äì68.
[19] A. J. Offutt and R. Untch, ‚ÄúMutation 2000: Uniting the orthogonal,‚Äù in
Mutation 2000: Mutation Testing in the Twentieth and the Twenty First
Centuries , San Jose, CA, October 2000, pp. 45‚Äì55.
[20] J. H. Andrews, L. C. Briand, and Y . Labiche, ‚ÄúIs mutation an appropriate
tool for testing experiments?‚Äù in Intl. Conf. Software Eng (ICSE) , St.
Louis, MO, May 2005, 402-411.
[21] R. Santelices and M. J. Harrold, ‚ÄúEfÔ¨Åciently monitoring data-Ô¨Çow test
coverage,‚Äù in Intl. Conf. Automated Software Eng. (ASE) , November
2007, pp. 343‚Äì352.
[22] A. Rajan, M. W. Whalen, and M. P. Heimdahl, ‚ÄúThe effect of program
and model structure on MC/DC test adequacy coverage,‚Äù in Intl. Conf.
Software Eng. (ICSE) , Leipzig, Germany, 2008, pp. 161‚Äì170.
[23] S. Pimont and J.-C. Rault, ‚ÄúA software reliability assessment based on a
structural and behavioral analysis of programs,‚Äù in Intl. Conf. Software
Eng. (ICSE) , San Francisco, CA, 1976, pp. 486‚Äì491.
[24] T. S. Chow, ‚ÄúTesting software design modeled by Ô¨Ånite-state machines,‚Äù
IEEE Trans. Software Eng. , vol. SE-4(3), pp. 178‚Äì187, 1978.
[25] M. R. Woodward, D. Hedley, and M. A. Hennell, ‚ÄúExperience with path
analysis and testing of programs,‚Äù IEEE Trans. Software Eng. , vol. SE-6,
no. 3, pp. 278‚Äì286, May 1980.
[26] M. R. Woodward and M. A. Hennell, ‚ÄúOn the relationship between two
control-Ô¨Çow coverage criteria: All JJ-paths and MCDC,‚Äù Information
and Software Technology , vol. 48, pp. 433‚Äì440, 2006.
[27] S. C. Ntafos, ‚ÄúOn required element testing,‚Äù IEEE Trans. Software Eng. ,
vol. SE-10, no. 6, pp. 795‚Äì803, November 1984.
[28] W. N. Venables, D. M. Smith, and The R Development Core Team, ‚ÄúAn
introduction to R,‚Äù R Development Core Team, Tech. Rep., June 2006.
[29] J. Horgan and S. London, ‚ÄúATAC: A data Ô¨Çow coverage testing tool
for C,‚Äù in Symp. on Assessment of Quality Software Development Tools ,
May 1992, pp. 2 ‚Äì 10.
[30] J. P. Guilford, Fundamental Statistics in Psychology and Education .
New York: McGraw-Hill, 1956.
[31] R. Santelices, J. A. Jones, Y . Yu, and M. J. Harrold, ‚ÄúLightweight fault-
localization using multiple coverage types,‚Äù in Intl. Conf. Software Eng.
(ICSE) , 2009, pp. 56‚Äì66.
View publication stats