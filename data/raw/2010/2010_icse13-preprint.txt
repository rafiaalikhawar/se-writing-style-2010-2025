Toddler: Detecting Performance Problems via
Similar Memory-Access Patterns
Adrian Nistor1, Linhai Song2, Darko Marinov1, Shan Lu2
1University of Illinois at Urbana-Champaign, USA,2University of Wisconsin‚ÄìMadison, USA
{nistor1, marinov }@illinois.edu, {songlh, shanlu }@cs.wisc.edu
Abstract‚ÄîPerformance bugs are programming errors that cre-
ate signiÔ¨Åcant performance degradation. While developers often
useautomated oracles for detecting functional bugs, detecting per-
formance bugs usually requires time-consuming, manual ana lysis
of execution proÔ¨Åles. The human effort for performance anal ysis
limits the number of performance tests analyzed and enables
performance bugs to easily escape to production. Unfortuna tely,
while proÔ¨Ålers can successfully localize slow executing co de,
proÔ¨Ålers cannot be effectively used as automated oracles.
This paper presents T ODDLER, a novel automated oracle
for performance bugs, which enables testing for performanc e
bugs to use the well established and automated process of
testing for functional bugs. T ODDLER reports code loops whose
computation has repetitive and partially similar memory-a ccess
patterns across loop iterations. Such repetitive work is li kely
unnecessary and can be done faster. We implement T ODDLER
for Java and evaluate it on 9 popular Java codebases. Our
experiments with 11 previously known, real-world performa nce
bugsshow that T ODDLER Ô¨Åndsthese bugswitha higheraccuracy
than the standard Java proÔ¨Åler. Using T ODDLER, we also found
42 new bugs in six Java projects: Ant, Google Core Libraries,
JUnit, Apache Collections, JDK, and JFreeChart. Based on ou r
bug reports, developers so far Ô¨Åxed 10 bugs and conÔ¨Årmed 6
more as real bugs.
I. INTRODUCTION
Software performance is critical for how end-users perceiv e
the quality of the deployed software. Performance bugs1are
programming errors that create signiÔ¨Åcant performance deg ra-
dation[1].Evenwhensoftwareismatureandwrittenbyexper t
programmers, performance bugs have been known to cause
serious and highly publicized incidents [2]‚Äì[5]. The state -of-
the-art techniques for detecting and testing for performan ce
bugs are still preliminary. As a result, performance bugs
easily escape to productionruns,hurt user experience,deg rade
system throughput,and waste computationalresources[6], [7].
BecauseperformancebugsaredifÔ¨ÅculttoÔ¨Ånd,theyaffectev en
well tested software such as Windows 7‚Äôs Windows Explorer,
which had several high-impact performance bugs that escape d
detection for long periods of time, despite their severe eff ects
on user experience [8].
A key reason why performance bugs escape so easily to
production is that testing for performance bugs cannot use
the well established process of testing for functional bugs
1‚ÄúPerformance bug‚Äù is a well accepted term in some communitie s, e.g.,
Mozilla Bugzilla deÔ¨Ånes it as ‚ÄúA bug that affects speed or res ponsiveness‚Äù [1].
However, others prefer ‚Äúperformance problem‚Äù or ‚Äúperforma nce issue‚Äù, be-
cause these problems differ from functional bugs. We take no position on this
and use ‚Äúperformance bug‚Äù and ‚Äúperformance problem‚Äù interc hangeably.withautomated oracles . An automated oracle detects if a test
triggers a (functional or performance) bug, in which case th e
developer needs to inspect the test. To test for functional b ugs,
developers usually follow three steps: (1) write as many and
as diverse tests as allowed by the testing budget, (2) run the se
tests and use automated oracles (e.g., crashes or assertion s) to
Ô¨Ånd which tests fail, and (3) inspect onlythe failing tests. To
test for performance bugs, developers typically write a sma ll
number of tests, use a proÔ¨Åler to localize code regions that
take a lot of time to execute, and then reason whether these
regions can be optimized and if the effort spent for optimizi ng
(time,addedcodecomplexity)isworththepotentialspeedg ain
(whichmay be difÔ¨Åcult to ascertain beforeactuallyperform ing
the optimization). In contrast to functional bugs, the lack of
reliable automated oracles for performance bugs means that
developers cannot easily Ô¨Ånd which tests fail, as in step (2) .
Asaresult,becausedevelopersneedtoinspectalltests/pr oÔ¨Åles
in step (3), they can use only a small number of performance
tests in step (1). In sum, developers follow the current proc ess
of testing for performance bugs not because it has advantage s,
but because developers have no reliable alternatives.
An automated oracle for performance bugs would enable
developers to test for performance bugs using the well estab -
lished process of testing for functional bugs. Unfortunate ly,
proÔ¨Ålers cannot be used as effective oracles for three reaso ns.
First, proÔ¨Ålers give a report for each test , thus running many
tests results in many reports, not just a few failing tests as
for a typical functional oracle. Second, proÔ¨Ålers may miss a
performancebug even when it is executed :if the buggycode is
notslowcomparedtotherest ofthat execution,it isnotrank ed
high by the proÔ¨Åler and is likely ignored by the developer.
Many performance bugs manifest by signiÔ¨Åcantly degrading
performance only for particular input conditions [8]‚Äì[11] , and
the proÔ¨Åled inputs cannot cover all possible conditions. Th ird,
proÔ¨Ålers report what takestime but not what wastestime,
i.e., they do not distinguish truly necessary (albeit expen sive)
work from the likely unnecessarycomputation.In other word s,
proÔ¨Ålersarehighlyusefulwhenthedeveloperwantsto localize
a slow code region but are not effective when the developer
needs todecideif a test likely exposes a performance bug and
thus needs further inspection.
This paper presents T ODDLER, a novel oraclefor perfor-
mance bugs. In brief, T ODDLER reports tests that execute
loops whose computation is repetitive and very similar acro ss
iterations. The intuition is that such loops are likely perf or-978-1-4673-3076-3/13 c2013 IEEE ICSE 2013, San Francisco, CA, USA
Accepted for publication by IEEE. c2013 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/
republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.562mance bugs that waste time: because the work is repetitive
and similar, it could be done faster. We designed T ODDLER
based on two observations about performance bugs. First,
many severe performance bugs (over 50% in the study in
Section II) are contained by nested loops: if an inefÔ¨Åcient
code region is executed outside of a nested loop, then the
inefÔ¨Åciency itself needs to be very severe (e.g., slow I/O) f or
the code region to have a real impact on the overall program
performance.Second,wasted computationis oftenreÔ¨Çected by
repetitive and partially similar memory accesses across lo op
iterations:ifa groupofinstructionsrepeatedlyaccesses similar
memory values, then those instructions probably compute
similar results.
We implemented a full-blown T ODDLER tool for Java and
a simple prototype for C/C++. Our experiments with 11
previously known, real-world performance bugs from 9 Java
projectsshow that T ODDLER is able to Ô¨Ånd all these bugs.Our
C/C++ prototype also Ô¨Ånds 6 previously known bugs in GCC,
Mozilla, and MySQL. Moreover, using T ODDLER helped us
identify42 new real-world bugs in six popular Java projects:
Ant, Google Core Libraries, JUnit, Apache Collections, JDK ,
and JFreeChart. Based on our reports, developers so far have
Ô¨Åxed 10 bugs and conÔ¨Årmed 6 more as real bugs, and the
Apache Collections developers even invited the Ô¨Årst paper
author to become a project committer. Our bug reports are
linked from http://mir.cs.illinois.edu/toddler .
II. STUDY OF PERFORMANCE BUGS
We study over 100 performance bugs from open-source
projects to identify how these bugs depend on loops. We
study both Java and C/C++ projects to obtain more generality
of our Ô¨Åndings. These bugs were collected independently of
TODDLER in a recent study on performancebugs [9], but their
relationship to loops was not analyzed in detail.
Our study shows that about 90% of performance bugs
involveloops,andmorethan50%ofperformancebugsinvolve
at least two levels of loops. The bugs that involve nested loo ps
can be categorized along two dimensions:
‚Ä¢Istheperformanceproblemin the inneror theouterloop?
‚Ä¢Is the performance problem caused by redundant com-
putation or inefÔ¨Åcient computation? We deÔ¨Åne redundant
computation asthesamecomputationbeingunnecessarily
repeated on the same set of data with the same result.
We next describe the four types of real-world performance
bugs categorized along the above two dimensions, and then
discuss how this understanding of real-world bugs can guide
our bug-detectiondesign.For space reasons, we will giveco de
examples only for two categories (but covering both inner an d
outer loops, as well as redundant and inefÔ¨Åcient computatio n).
A. Categories of Severe Performance Bugs
Category 1 (Redundancy in Outer Loops): Redundant
computation is conducted across iterations of an outer loop .
This redundant computation involves an expensive inner loo p,
which makes the performance problem severe. Problems of1// SimpliÔ¨Åed from the XYPlot class in JFreeChart
2public void render(...) {
3for(intitem = 0; item <itemCount; item++) {// Outer Loop
4renderer.drawItem(...item...); // Calls drawVerticalItem
5}
6}
7// SimpliÔ¨Åed from the CandlestickRenderer class in JFreeCh art
8public void drawVerticalItem(...) {
9intmaxVolume = 1;
10for(inti = 0; i<maxCount; i++) {// Inner Loop
11intthisVolume = highLowData.getVolumeValue(series, i).intVa lue();
12if(thisVolume >maxVolume) {
13 maxVolume = thisVolume;
14}
15}
16... = maxVolume;
17}
Fig. 1. A JFreeChart bug with a redundancy in the outer loop
this type are usually difÔ¨Åcult for compilers to optimize, be -
cause they involve nested loops and usually many functions.
They are usually Ô¨Åxed by storing and reusing results from
previous loop iterations.
Figure 1 demonstrates such a bug from JFreeChart, a popu-
lar Java framework for drawing charts. This bug is particula rly
severe, because it causes the chart display to freeze. The ou ter
loop iterates over all the items in a data set (line 3) and for
each item calls the method drawItem , which in turns calls
the method drawVerticalItem . The inner loop (line 10) in
drawVerticalItem computesthe maximumvolume(line 12)
of all the items in the data set. The repeated computation
of maximum is redundant, because the volumes of the items
do not change between calls. Thus, the inner loop can be
performed only once, not in every iteration of the outer loop .
Indeed, to Ô¨Åx this bug, the developer changed the code to
cache and reuse the maximum volume.
Category 2 (Redundancy in Inner Loops): Redundant
computation is conducted across iterations of an inner loop .
This computation waste is ampliÔ¨Åed by outer loops that
dynamically call the inner loop many times. Performance
problems of this type are difÔ¨Åcult for compilers to optimize
when the redundant computation involves function calls. Th ey
are usually Ô¨Åxed by hoisting computation out of the loop.
Category 3 (InefÔ¨Åcient Outer Loops): The program has
an expensivebut necessaryinner loop.Unfortunately,this loop
is inefÔ¨Åciently used by an outer loop, which leads to severe
performance problems. Problems of this type cannot be opti-
mized by compilers, because they require deep understandin g
of code. They are usually Ô¨Åxed by changing outer loops so
that the inner loop will execute less frequently.
Category 4 (InefÔ¨Åcient Inner Loops): The inner loop
conducts an inefÔ¨Åcient, but not redundant, computation. Th is
inefÔ¨Åciency is ampliÔ¨Åed by an outer loop that uses each
iteration to execute the inner loop on a slightly different d ata
set. Again, problems of this type cannot be optimized by
compilers, because they require deep understanding of code .
Their patches need to Ô¨Ånd a more efÔ¨Åcient or incremental
algorithm to replace the inner loop, which often can be
achieved with a more appropriate data structure for the data
set under operation.5631// SetDecorator class in Google Core Libraries contained th is method call
2set.removeAll(arrayList);
3// SimpliÔ¨Åed from the AbstractSet class in the standard Java library
4public boolean removeAll(Collection /an}bracketle{t?/an}bracketri}htc){
5if(size()>c.size()) {
6for(Iterator/an}bracketle{t?/an}bracketri}hti = c.iterator(); i.hasNext(); )
7 remove(i.next());
8}else{
9for(Iterator/an}bracketle{t?/an}bracketri}hti = iterator(); i.hasNext(); ) {// Outer Loop
10 if(c.contains(i.next())) {
11 i.remove();
12 }
13}
14}
15}
16// SimpliÔ¨Åed from the ArrayList class in the standard Java li brary
17public boolean contains(Object o) {
18for(inti = 0; i<size; i++) {// Inner Loop
19if(o.equals(elementData[i]))
20 return true ;
21}
22return false ;
23}
Fig. 2. A Google Core Libraries bug with an inefÔ¨Åcient inner l oop. This
was apreviously unknown bug found by T ODDLER.
Figure 2 demonstrates an example from Google Core Li-
braries (GCL). This is a previously unknown bug found by
TODDLER. After we reported it, GCL developers not only
Ô¨Åxed this bug but also searched through their entire codebas e
for similar code patterns and Ô¨Åxed 8 other classes affected
by similar bugs. (We count these 9 instances as one bugnot
9 bugs.) The GCL code called the removeAll method on a
Setobject, passing it an ArrayList object as a parameter.
TheremoveAll method removes from the set thisall the
elements contained in the speciÔ¨Åed collection c. The method
has a performanceoptimizationthat chooseswhether to iter ate
over the set thisor the collection cbased on their sizes
(line 5), under the assumption that the cost of contains and
removeoperations are similar for the set and the collection
when they have similar sizes. In the elsebranch, the outer
loop iterates over each element of thisand checks if c
contains the element (lines 9‚Äì13).
Whencis anArrayList ,contains performs a linear
search (lines 18‚Äì21), which is inefÔ¨Åcient, so it would have
beenbettertoiterateover candcallremoveonthesetbecause
it has a more efÔ¨Åcient inner loop. Indeed, the GCL developers
changed their code, replacing the call to removeAll by
conceptuallyinliningthebodyof removeAll andkeepingonly
thethenbranch from the body. In general, the solution for
this category is to simplify the inner-loop computation.
B. Implications
Why do developers need automated support for per-
formance testing? The above examples demonstrate that
many performance bugs are difÔ¨Åcult to avoid, because they
involve library functions or APIs whose performance fea-
tures are opaque to developers. In addition, a lot of time-
consuming computation, such as many inner loops in our
examples,is embeddedin codewritten by differentdevelope rs.
As shown in Figure 2, GCL developers did not initially
consider that the performance of the Java library method
AbstractSet.removeAll is sensitive to the data structuresused for parameters, and this information is not even stated in
the documentation for removeAll . Tool support is needed to
help developers detect these hard-to-avoid performance bu gs.
Why do we focus on nested-loop performance bugs?
Bugs that involve nested loops usually have severe perfor-
mance impact. The reason is that the inner loop represents
an expensive computation inside the outer loop, and the oute r
loop ampliÔ¨Åes the performance penalty of the inner loop. For
example, in the JFreeChart bug from Figure 1, the inner loop
is slow, but if executed only once, it cannot have a signiÔ¨Åcan t
effect on performance;however,if executed many times in th e
outer loop, it causes the chart display to freeze.
How can we detect nested-loop performance bugs? A
common feature of above nested-loop performance bugs is
that theyofteninvolve repeatedmemory-accesspatterns .Bugs
from Category 1 conduct redundant computation across outer -
loop iterations. A big chunk of the computation in each outer -
loop iteration repeats the computation from an earlier iter ation
with the same input and the same result. Hence, outer-loop
iterations share long sequences of memory reads that return
the same values. For example, the iterations of the outer
loop in Figure 1 share a long sequence of reads inside the
intValue method (line 11). Bugs from Category 2 conduct
redundant computation during every iteration of an inner lo op,
which results in memory reads that repeatedly return the sam e
value. Bugs from Categories 3 and 4 have less regular pattern s
than bugs from Categories 1 and 2, but the memory-access
similarities are still strong. The outer-loop iterations i n bugs
from Categories 3 and 4 often work on similar data sets. That
is the reason why developers can effectively optimize these
bugs. That is also the reason why there are usually memory
reads that return similar sequences of values across outer-
loop iterations. In sum, looking for repeated memory-acces s
patternsis an effective way to look for performancebugs fro m
all four categories.
III. TODDLER DESIGN AND IMPLEMENTATIONS
Motivated by the study in Section II, we have developed
TODDLER, an automated oracle that Ô¨Ånds likely performance
bugs by looking for loops that read similar sequences of
values across iterations. T ODDLER considers such similar
sequences to be a strong indication of redundant or inefÔ¨Åcie nt
computation and reports such loops as performance bugs.
TODDLER is a dynamic technique. It instruments the code
under test, runs each test from a given test suite, and report s
only the tests that contain loops with similar sequences. We
Ô¨Årst describethe instrumentationthat T ODDLER adds.We then
describe the data structures and algorithms that T ODDLER
uses for storing information about reads and Ô¨Ånding similar ity
among sequences. We Ô¨Ånally discuss our two implementations
of TODDLER in a full-blown tool for Java and a simple
prototype for C/C++.
A. Instrumentation
To monitor loops and read instructions, T ODDLER instru-
mentsthecode,boththeapplicationundertest andthelibra ries5641StartLoop(L1)
2StartIter Read(i 1, v1)
3 StartLoop(L2)
4 StartIter Read(i 2, v2)
5 StartIter Read(i 2, v3) Read(i 3, v4)
6 StartIter Read(i 3, v5)
7 StartIter Read(i 2, v6) Read(i 3, v7)
8 FinishLoop(L2)
9StartIter
10 StartLoop(L2)
11 StartIter Read(i 2, v8)
12 StartIter Read(i 2, v9) Read(i 3, v10)
13 StartIter Read(i 2, v11) Read(i 3, v12)
14 StartIter Read(i 3, v13)
15 FinishLoop(L2)
16 Read(i 4, v14) Read(i 5, v15)
17FinishLoop(L1)
Fig. 3. Example events produced by running instrumented cod e
it depends on, because many performance bugs are caused
by the misuse of libraries. For loops, the instrumentation i s
straightforward:T ODDLERanalyzesthecode,assignsaunique
ID for each static loop, and inserts in the code three types
of method calls that inform the T ODDLER runtime whenever
a loop starts, a loop iteration starts, or a loop Ô¨Ånishes. For
read instructions, the instrumentation itself is also simp le: for
each instruction that reads object Ô¨Åelds or array elements
from the heap (e.g., Java bytecode instructions GETFIELD or
AALOAD), TODDLER inserts a method call that informs the
TODDLER runtime about the value read by the instruction and
the call stack within which the instruction is executed. Not e
that TODDLER identiÔ¨Åes a read instruction by both the static
occurrence of the instruction in the code andthe dynamic
context (i.e., the call stack) in which the instruction exec utes.
We use the term IPCS(instructionpointer+ call stack) to refer
to a static instruction with its dynamic context.
B. Collecting IPCS-Sequences
We use the term IPCS-sequence to refer to the sequence of
values read by all dynamic instances of an IPCS Iduring an
iteration of a loop L. Note that, when Iis inside an inner loop
ofL,theIPCS-sequencefortheouterloop Lislikelytocontain
more than one element. Also note that T ODDLER buildsone
IPCS-sequence per IPCS rather than one IPCS-sequence per
the entire loop iteration, and thus a loop iteration has as ma ny
IPCS-sequences as it has IPCSs.
To illustrate, Figure 3 shows an example stream of events
produced when some instrumented code is executed; iN
represents an IPCS, and vMrepresents a value read. From
these events, T ODDLER createsIPCS-sequencesofvaluesread
by the same IPCS during a loop iteration. For example, for
theouterloopL1, TODDLER would create IPCS-sequences
i1:[v1],i2:[v2,v3,v6], andi3:[v4,v5,v7]for the Ô¨Årst iter-
ation and i2:[v8,v9,v11],i3:[v10,v12,v13],i4:[v14], and
i5:[v15]for the second iteration.
Note that the IPCS-sequences for the innermost loops have
length 1, e.g., for the Ô¨Årst dynamic instance of the innerloop
L2, the IPCS-sequences would be just i2:[v2],i2:[v3], and
i2:[v6]fori2and similar for i3. Also note that an IPCS need
not occur in every iteration of a loop (e.g., i2does not occur
in the third iteration of the Ô¨Årst dynamic instance of L2). In1// Instruction pointer and its dynamic context
2classIPCS{intIP; CallStackHash cs; }
3// Value of a memory location
4classVal{long val; }
5//IPCS-sequence of values read by an IPCSin one iteration
6classSeq{List/an}bracketle{tVal/an}bracketri}htlist;}
7// Dynamic loop record
8classDynLoop {
9intid;// static id of the loop
10CallStackHash cs; // calling context
11intiterations; // number of iterations
12// map each IPCSencountered during loop execution...
13// ...to values read by the IPCSin the iterations
14Map/an}bracketle{tIPCS , List /an}bracketle{tSeq/an}bracketri}ht/an}bracketri}htmap;
15}
Fig. 4. Data structures for storing and processing IPCS-seq uences
that case, T ODDLER still creates an IPCS-sequence (for L1)
of consecutive values read for the same IPCS even if these
values are not read in consecutive loop iterations (of L2).
While this example illustrates T ODDLER only on the loop
nesting depth of two, T ODDLER handles larger nesting depths
in the same manner, by appending IPCS-sequences for the
same IPCS. For example, if one iteration of some loop L0
had the events shown in Figure 3, then for that iteration of L0,
TODDLER would create i1:[v1],i2:[v2,v3,v6,v8,v9,v11],
i3:[v4,v5,v7,v10,v12,v13],i4:[v14], andi5:[v15].
C. Data Structures
Figure 4 shows the data structures that T ODDLER uses to
store information about loops. IPCShas anIPthat statically
determines the instruction (e.g., its class, method, and by te-
code offset within the method in Java) and the call stack
that represents the dynamic context in which the instructio n
executes. (Call stacks can be efÔ¨Åciently computed using has h-
ing [12].) Valrepresents a value read by an instruction, which
is either a primitive value or an object ID (obtained with
System.identityHashCode() in Java). Note that the ID is
oftheobjectbeing returnedbytheread,notoftheobjectbeing
dereferenced .Forexample,in e.next,theIDisof e.nextnot
ofe.SeqisanIPCS-sequenceofvaluesreadbythesame IPCS
in one loop iteration. DynLoop records information about one
dynamic loop instance: the static loop ID (its class, method ,
and bytecode offset within the method in Java), the call stac k
in which the loop executes, the number of loop iterations, an d
the IPCS-sequences across all iterations for each IPCS. For
example, for i2, the two IPCS-sequences of the outer(L1)
loop arei2:[[v2,v3,v6],[v8,v9,v11]].
D. Algorithm for Finding Performance Bugs
Figure 5 shows the pseudo-code of the top-level function.
TODDLER checks for potential performance bugs in each
dynamic loop that had more than a few iterations (by default,
minIter=10; this threshold is a conÔ¨Ågurable parameter of
our algorithm, and Section IV-D discusses the impact of the
parameters).Foreach DynLoop,TODDLER Ô¨ÅndsallIPCSsthat
have similar IPCS-sequences across loop iterations. If the re is
any such IPCS, TODDLER reports a performance bug.
Given a test suite, T ODDLER runs each test, collects
DynLoop objects, and reports a set of static loops that have5651// One parameter for loops
2intminIter;// absolute number of loop iterations
3
4//Input: the record of a dynamic loop
5//Output: whether this loop has performance bugs
6boolean hasPerformanceBug(DynLoop loop) {
7return!(computeSimilarIPCSs(loop).empty());
8}
9
10//Input: the record of a dynamic loop
11//Output:IPCSs that read similar values across iterations
12Set/an}bracketle{tIPCS/an}bracketri}htcomputeSimilarIPCSs(DynLoop loop) {
13Set/an}bracketle{tIPCS/an}bracketri}htsimilarIPCSs = newSet/an}bracketle{tIPCS/an}bracketri}ht();
14// ignore very small loops
15if(loop.iterations <minIter)returnsimilarIPCSs;
16for(curIPCS : loop.map.keyset())
17// compare IPCS-sequences for iterations in which curIPCS occurs
18if(areSimilarIterations(loop.map.get(curIPCS)), loop.itera tions)
19 similarIPCSs.add(curIPCS);
20returnsimilarIPCSs;
21}
Fig. 5. The top-level function for T ODDLER
similar IPCS-sequences for at least one test. For each stati c
loop, T ODDLER generates a set of records that help in under-
standing and debuggingthe problem. Each record contains th e
test that executesthe loop,the call stack for the loop,the s tatic
IP of the instruction that reads similar values, the call sta ck
for that instruction, and statistics about similarity.
Note that T ODDLER can Ô¨Ånd the same loop to be repetitive
for multiple tests. Rather than printing a report for each te st
and each loop, T ODDLER clustersthese reports based on the
staticouter loop. Clustering is commonly used for grouping
failure reports in testing [13], [14].
E. Measuring Similarity
Figure 6 shows the pseudo-code for Ô¨Ånding similar IPCS-
sequences across loop iterations. T ODDLER compares consec-
utive IPCS-sequences for the same IPCS. As mentioned in
Section III-A, an IPCS may not be executed in every iteration
ofaloop.T ODDLERcomputestheratioofthenumberofIPCS-
sequences to the number of loop iterations and ignores IPCSs
that occur in a small ratio of iterations, because even if the
computation at these IPCSs is similar and could be optimized,
they may not be an expensive part of the entire loop. (By
default,minSeqRatio =45%.)
To compare the IPCS-sequences of an IPCS inside a loop
L, TODDLER determines whether these IPCS-sequences are
similarthroughout L based on the relative number of similar
consecutive IPCS-sequences. The IPCS-sequences are consid-
ered similar throughout loop Lif and only if the ratio is larger
than the threshold. (By default, minSimRatio =70%.)
Redundant and inefÔ¨Åcient computation can be reÔ¨Çected not
only by IPCS-sequences that are exactly the same across
iterations, such as the IPCS-sequences from intValue()
in Figure 1, but also by IPCS-sequences that are slightly
different across iterations, such as the IPCS-sequences fo r
elementData[i] inFigure2.Thus,weneedtojudgewhether
two IPCS-sequences are similarenough to represent potential
performance problems.
Figure 7 shows the pseudo code of this algorithm. T OD-
DLERuses thelongest common substring [15] to measure the1// Two parameters for loop iterations
2Ô¨ÇoatminSeqRatio ;// relative number of IPCS-sequences in the loop
3Ô¨ÇoatminSimRatio ;// relative number of similar iterations
4
5//Input:IPCS-sequences for all iterations of a loop
6//Output: whether IPCSreads similar values across iterations
7boolean areSimilarIterations(List /an}bracketle{tSeq/an}bracketri}htseqs,intiterations) {
8// ignore IPCSthat occurs in a small fraction of iterations
9if((seqs.size() / iterations) <minSeqRatio )return false ;
10intsimilar = 0;
11for(inti = 0; i<seqs.size() ‚àí1; i++)
12if(areSimilarSequences(seqs[i], seqs[i+1])) similar++;
13return(similar / (seqs.size() ‚àí1))>=minSimRatio ;
14}
Fig. 6. Checking the similarity throughout a loop
1// Two parameters for IPCS-sequences of values
2intminLCS;// absolute length of the longest common substring
3Ô¨ÇoatminLCSRatio ;// relative length of the longest common substring
4
5//Input: twoIPCS-sequences
6//Output: whether two IPCS-sequences are similar
7boolean areSimilarSequences(Seq S1, SeqS2){
8lcs = longestCommonSubstring( S1,S2).size();
9lcsRatio = lcs / min( S1.size(),S2.size());
10return(lcs>=minLCS) && (lcsRatio >=minLCSRatio );
11}
Fig. 7. Checking the similarity of two IPCS-sequences
similarity between two IPCS-sequences. (Note that substring
refers to the consecutive occurrences of values in the IPCS-
sequences, while subsequence would refer to the potentially
non-consecutive occurrences of values.) The longest common
substring can be computed in O(nm)time where nandmare
the lengths of the two IPCS-sequences [15]. We deÔ¨Åne two
IPCS-sequences to be similar if both the absolute and relati ve
length of their longest common substring are abovethreshol ds.
(By default, minLCS=7 andminLCSRatio =70%.)
F. Filtering Reads
TODDLER can Ô¨Ålter reads that have repetitive values but
are unlikely to indicate performance bugs. First, T ODDLER
ignores IPCS-sequences that repeat only one value. For ex-
ample, an inner loop of the form for (int i = 0; i <
this.size; i++) repeatedly reads the value for this.size
butdoesnotcontainaperformancebug.Notethatthisheuris tic
may cause T ODDLER to lose some Category 2 bugs. For
example, if this.size is returned by a synchronized getter
method, which is slower than just reading this.size , one
may want to pull the getter method call out of the loop.
TODDLER considers all operations to take an equal amount of
time, and therefore does not report the repeated getter meth od
calls as a performance bug. Future implementations can add
timing information to T ODDLER.
Second, T ODDLER for Java ignores reads that happen in
the class initializer methods because these are executed on ly
once per class loading, so even if the code contains a bug,
developers may not want to change it. Third, T ODDLER
allows the users to specify a set of Ô¨Åelds and methods to be
ignored, when the users do not expect them to be indicative of
performance bugs. T ODDLER ignores IPCSs that either read a
speciÔ¨ÅedÔ¨ÅeldorexecuteinacontextwhereaspeciÔ¨Åedmethod566IDApplication Description LoCKnown New
BugsBugs
#1Ant build tool 109,765 18
#2Apache Collections collections library 51,416 120
#3Groovy dynamic language 136,994 10
#4Google Core Libraries collections library 156,004 210
#5JFreeChart chart framework 64,184 11
#6JMeter load testing tool 86,549 10
#7Lucene text search engine 320,899 20
#8PDFBox PDF framework 78,578 10
#9Solr search server 373,138 10
JDK standard library 2
JUnit testing framework 1
SUM 1142
Fig. 8. Applications used in experiments, previously known bugs, and new
bugs found with T ODDLER.
is on the call stack. For example, some Ô¨Åelds are used as
indexesand can appear in an inner loop as for (...) {...
this.cursor++; ... }; if the outer loop resets cursor,the
IPCS-sequence would repeat, but repeatedly reading the ind ex
itself does not indicate inefÔ¨Åcient or redundant computati on.
As another example, appending strings in a loop often leads
to repeated work, and in fact, it is an anti-pattern in Java to
append many Stringobjects. However, to simplify coding,
many times developers do append strings in loops, and may
not want to be bothered with reports of such coding patterns.
By default, T ODDLER ignoresonly three Ô¨Åelds and four
toString /appendmethods from the standard JDK library
java.util classes. Note that specifying these library Ô¨Åelds
and methods is done only once for all applications that use
the library.
G. Implementations
We implemented the T ODDLER technique in a full-blown
tool for Java, which we also call T ODDLER, and a simple
prototype for C/C++. Our Java implementation is based on
static Java-bytecode instrumentation, using Soot 2.4.0 [1 6].
TODDLER uses Soot to instrument every instruction that reads
an object Ô¨Åeld or an array element, the start of each loop,
the start of each loop iteration, and the exit of each loop.
The implementation closely follows the pseudo-code algo-
rithms presented earlier. It performs similarity checks online,
i.e., collects IPCS-sequences of values read in a DynLoop
object and, whenever the program exits a loop, calls the
hasPerformanceBug function from Figure 5 to process the
DynLoop object and decide if there is a performance bug.
Section IV-E discusses our C/C++ prototype.
IV. EXPERIMENTAL RESULTS
OurevaluationfocusesontheJavaversionof T ODDLERand
uses 9 popularJava codebases.Figure 8 lists basic informat ion
about these codebases. We Ô¨Årst evaluated T ODDLER on 11
previously known real-world performance bugs and on over
173,000 existing functional tests from these codebases. We
thensettled on the values for the TODDLER parameters
and evaluated it on newly written performance tests. Ourexperiments found 42 real-world performance bugs in these
codebases (39 in the application code and 3 in the libraries
they use).
The rest of this section Ô¨Årst presents our experiments with
the11previouslyknownbugs.Itthenpresentsourexperimen ts
with performance tests and the new bugs that we found.
It next presents the evaluation with the existing functiona l
tests. It Ô¨Ånally presents a sensitivity analysis of the para meter
values. Unless otherwise speciÔ¨Åed, all the experiments use the
following default values: minIter=10,minSeqRatio =45%,
minSimRatio =70%,minLCS=7,minLCSRatio =70%.
We conduct all experiments where time is measured on an
AMD Athlon machine with 2.1GHz CPU, 3GB memory, and
Oracle/Sun JVM version 1.6.0. We also conduct experiments
where time is not measured on a cluster of machines; while
TODDLER does not need a cluster for regular use, we needed
it for our extensive experiments.
A. Experiments with Previously Known Bugs
To evaluate bug-detection coverage, accuracy, and overhea d
of TODDLER, we Ô¨Årst used 11 known real-world bugs from
the 9 codebases. We searched the respective bug-tracking
databasestocollectthesebugs;theywerereportedbytheus ers
of these applications and the bug description clearly marks
them as performance bugs.
We run T ODDLER on a performance test related to the
bug report for each of the 11 bugs. Because each test is
supposed to reveal a bug, we effectively evaluate if T ODDLER
hasfalse negatives that miss some bugs. We compare the
results of T ODDLER with the results of a traditional proÔ¨Åler
ran on the same tests. As explained in Section I, proÔ¨Ålers
are not designed to detect performance bugs, but are the only
traditional tool that developers could use without T ODDLER.
SpeciÔ¨Åcally, we use HPROF [17], the standard Java proÔ¨Åler.
It outputs a ranked list of methods (more precisely, calling
contexts)that consume the most time. We measure how highly
HPROF ranksthe buggymethod(that containsthe buggycode
region). Additionally, for these 11 tests, we compare the ru n-
time overheads of T ODDLER and HPROF.
1) Bug Detection Results: Figure 9 summarizes the results
for the 11 bugs. T ODDLER Ô¨Ånds all the bugs (no false
negatives) and produces only one false positive. SpeciÔ¨Åcal ly,
for bug #8, T ODDLER produces two reports: one showing the
realbugandonebeingafalsepositive.(SectionIV-Cdiscus ses
false positives.) T ODDLER Ô¨Ånds these 11 bugs because they
involve at least two levels of loops and have similar sequenc es
ofvaluesreadacrossloopiterations.Infact,mostofthese bugs
have so strongly similar sequences that T ODDLER can detect
them under a wide range of threshold settings. (Section IV-D
discusses sensitivity to threshold settings.)
Figure 9 also shows the results for HPROF. We use it with
thecpu=times option as it gives more accurate results than
cpu=samples , though at a higher overhead. However, even
withcpu=times , the results of HPROF for the same code
and same input can vary from one run to another. Therefore,567Known Bug Detected? False P. Rank Slowdown
BugTODD. HPROF TODD. HPROF TODD.HPROF
#1 /check - 0 19.3 13.7 4.2
#2 /check /check 0 1.0 10.0 2.1
#3 /check /check 0 3.7 15.5 3.7
#4.1 /check /check 0 1.8 9.0 3.8
#4.2 /check - 0 5.3 7.5 3.2
#5 /check - 0 53.7 13.4 8.8
#6 /check - 0 10.3 8.5 1.9
#7.1 /check - 0 7.7 6.8 2.5
#7.2 /check /check 0 3.1 25.4 3.1
#8 /check - 1 18.8 51.8 12.1
#9 /check - 0 178.3 114.2 7.1
SUM 11 4 1 15.9X4.0X
Fig. 9. Comparison of T ODDLER and HPROF for bug-triggering tests.
we ran each test under HPROF 10 times and show the mean
ranking of the buggy method.
The developer is unlikely to inspect more than a handful
of methods reported by a proÔ¨Åler. If we consider that HPROF
correctly detects a bug when the buggy method ranks in top
5, then HPROF detects only 4 out of 11 cases that T ODDLER
detects. On the positive, HPROF ranks bug #2 consistently as
number one. On the negative, for 5 out of 11 bugs, HPROF
does not rank the buggy method even in the top ten. For
example, bug #9 comes from a text-search server, Solr. The
method with the performance bug constructs a set of strings
thatrepresentÔ¨Ålterkeywords.Undernormalserversetting ,this
set is small, and the method consumes only about 0.1% of the
total search-querytime. As a consequence, it ranks only abo ut
178th in the proÔ¨Åling results.
A careful reader may wonder if an easier approach would
sufÔ¨Åce to Ô¨Ånd the bugs that T ODDLER Ô¨Ånds: could we simply
report all nested loops as potentially buggy?We addedcode t o
count nested loops during an execution, more precisely stat ic
outer loops that dynamically execute at least one inner loop .
For the 11 tests, the number of such outer loops ranges from 1
to 12, and the total number of such loops is 38. Thus, a na¬® ƒ±ve
technique that reports every nested loop as a performance bu g
would have 27(=38-11) false positives for just these 11 bugs .
In contrast, T ODDLER can identify truly performance-wasting
nested loops by analyzing memory-accesspatterns and repor ts
only one false positive for these 11 cases.
2) Performance Results: The last two columns of Figure 9
show the slowdown that T ODDLER and HPROF have over
an execution with no tool for the 11 bug-triggering tests.
TODDLER causes, on average, a 15.9X slowdown that comes
frommonitoringreadaccessesandcomparingIPCS-sequence s.
Our current implementation of T ODDLER is about 4 times
slower than HPROF. In the future we plan to further reduce
the overhead of T ODDLER through sampling techniques and
static analysis.
B. Experiments with New Bugs and Performance Tests
We further evaluate bug-detection coverage and accuracy of
TODDLER by applying it on performance tests , which is the
intended usage scenario for T ODDLER. To avoid the bias of usastoolauthorsmanuallywritingtests,weusethreesetsoft ests
not written by us: (1) automatically generated tests, (2) te sts
manually written by an undergraduate student familiarwith
performance testing (‚Äúexpert‚Äù), and (3) tests manually wri tten
in a controlled experiment by 8 graduate and undergraduate
studentsunfamiliar with performance testing (‚Äúnovices‚Äù). We
use these different sets to assess how T ODDLER works for
tests with various characteristics.
We focus our efforts on collection classes because they are
widely used and make both automated generation [18] and
manual writing of tests easier than domain-speciÔ¨Åc applica -
tions such as Groovyor Lucene. Ant, Apache Collections, and
Google Core Libraries (GCL) implement collection classes.
The performance tests for collections follow a simple pat-
tern: create some empty collection(s), insert several elem ents
into the collection(s), and Ô¨Ånally call a method under test.
(Note that performance tests need not necessarily check the
functional results of the methods.) The collections for per -
formance tests should not be very small, e.g., when testing
Collection.removeAll(Collection c) , boththisandc
should have a reasonable number of elements, say, over 20
each;iftheyhadaverysmallnumber,say,2each,itisunlike ly
the test would be useful for performance testing.
We wrote a simple library to automate generation of perfor-
mancetestsforcollections.Ourlibrarycangenerateindiv idual
collections of various types, sizes, element types, and ele ment
values, e.g., generate an ArrayList<Integer> with elements
1-50. Moreover, our library can generate multiple collections
with various relationships in terms of types (collections o f
same or different types), sizes (collections of same, small er,
larger sizes), and intersection of elements (collections t hat
are disjoint, equal, or partially intersect), e.g., genera te a set
with elements 1-50and a list with elements 1-75. Our library
supports exhaustive and random selection of combinations o f
these relationships. The design goal for the library was not to
extensively cover all the cases but to provide some reasonab le
tests for T ODDLER.
We collected two types of manually written tests. We asked
the‚Äúexpert‚Äùto writetestsforanymethodsinGCL andApache
Collections. We asked each ‚Äúnovice‚Äù to spend an hour writing
tests for a given set of 10 methods in a class from Apache
Collections; one of these 10 methods contained a known
performance bug, and we wanted to check if the students
would write tests that Ô¨Ånd this bug.
Figure 10 shows the number of tests generated/written for
eachcodebase,the numberofdynamicloopsexecuted,andthe
number of reports that T ODDLER produces. We examined all
these reports to identify if they are real bugs or false posit ives.
We found 35 new, previously unknown performance bugs
in Ant, Apache Collections, GCL, and even in a JDK class
called from these projects; based on our reports, developer sso
far have Ô¨Åxed 8 of these bugs and conÔ¨Årmed 6 more as real
bugs. T ODDLER was highly effective in Ô¨Ånding performance
bugs using both automatically generated and manually writt en
tests. Both types of tests found bugs, and sometimes foundth e
samebugs.(OurstudyusedolderversionsofGCLandApache568WhoApp Tests#Dyn.BugsBugs in FalseSumLoops Test Pos.
#1 69113,748 5 016
Auto #23,375 342,821 18 1221
#41,703 423,406 9 009
Ex-#2 606,761 10 0111
pert#4 606,319 2 002
#2 142,057 1 607
#2 203,043 2 002
#2 51,868 1 001
Nov- #2 183,269 1 001
ice#2 5 606 0 000
#2 284,502 2 002
#2 303,810 1 001
#2 51,996 1 001
Unique Bugs Found: 35 FPs: 4
Fig. 10. Experiments with performance tests. Note that the s ame bug may
be found by different automatically generated and manually written tests.
Collections, without the Ô¨Åxes for the bugs we reported.)
Surprisingly, some ‚Äúnovice‚Äù-written tests found two bugs i n
a class that we expected to have only one bug.
We also found7 performancebugswherethe test codeitself
is unnecessarily slow. For example, the ‚Äúnovice‚Äù-written t ests
hadassertionsthatcheckthemethodresults,andthe assert ions
themselves use rather slow code, e.g., nested loops that sea rch
in lists but could have searched in sets. If such loops appear ed
in the code under test, they would be deÔ¨Ånite bugs that should
be changed.
C. Experiments with Functional Unit Tests
The Ô¨Årst two sets of experiments used tests written for per-
formance, which is the intended usage scenario for T ODDLER.
To further evaluate T ODDLER, we run it on the functional
JUnit tests that come with the 9 codebases used in our
evaluation. Note that this is not the intended usage scenario :
a developer would not use functional tests for performance
testingand thus would not use T ODDLER on the functional
JUnit tests. We perform these experiments onlyto stress-
evaluate T ODDLER.
Our experiments use 173,439 tests shown in Figure 11.
These tests execute 24,810‚Äì3,526,496 dynamic loops (and
1,181,628‚Äì54,054,728 dynamic iterations) per codebase, a
challengefortherun-timemonitoringscalability.Thetes tsalso
cover 21‚Äì919 unique static loops that contain nested loops p er
codebase, a challenge for the bug-detection accuracy.
TODDLER successfully ran for this extensive evaluation and
reported43staticloopsashavingsimilarmemoryaccessesa nd
thuspotentialperformancebugs.Weexaminedalltheserepo rts
and found 7 real bugs. For JFreeChart (#5), one bug is in the
JFreeChartcodeitselfandtheotherinthestandardJDKlibr ary.
For Apache Collections (#2), one bug we reported is already
Ô¨Åxed, and the other three bugs are similar to three bugs we
previously reported and developers resolved by changing th e
Javadoc documentation to clarify the performance problems .
ForAnt(#1),allthreebugshavebeenalreadyÔ¨Åxedinthelate st
release. (Ourexperimentsuse olderversionsof the codebas es.)App# Tests# DynamicBugsBugs in FalseSumLoops Test Pos.
#1 675 877,362 30 3 6
#2 31,105 3,526,496 47 112
#3 464 281,596 00 0 0
#4138,997 2,574,756 00 4 4
#5 332 514,824 20 1 3
#6 164 88,548 00 1 1
#7 6751,488,977 00 4 4
#8 42 24,810 00 0 0
#9 9851,395,494 00 13 13
Unique Bugs Found: 7 FPs: 27
Fig. 11. Experiments on JUnit functional tests. Note that this is not the
intended usage scenario for TODDLER; a developer would not use functional
tests for performance testing .
For Apache Collections (#2) we also found 7 performance
bugs in tests, where the test code is unnecessarily slow and
would need to be Ô¨Åxed had it been in the application code.
The remaining 27 reports are false positives due to three
causes. First, in 10 reports, the test input itself contains a
lot of repetition and similar values, so T ODDLER detects
similarity due to the speciÔ¨Åc input provided, not because th e
computationis repetitive in general. Such false positives could
be eliminated by using less repetitive test inputs. Second,
in 3 reports, the code performs some computation on all
possible pairs of values from two data sets. Such code is
naturally repetitive, but the repetitions are useful compu tation,
not performance bugs. Such false positives may be eliminate d
by analyzing the data Ô¨Çow of computation results, but such an
analysis is beyond the scope of this paper. Third, in 14 repor ts,
the computation is truly repetitive, but removing the repet ition
would be too complex or would not provide clear speedup, so
a developer is unlikely to change the code.
D. Parameter Sensitivity
The false-positive and false-negative rates of T ODDLER
are affected by the values for the Ô¨Åve parameters described
in Section III. All these parameters provide the minimum
threshold that loops/iterations/sequences need to satisf y to
be deemed indicative of performance bugs. Hence, larger
thresholds could lead to fewer false positives but more fals e
negatives, while smaller thresholds could lead to more fals e
positives but fewer false negatives. We experimented with
various threshold values to understand their impact.
Figure 12 shows the results for several conÔ¨Ågurations. For
each conÔ¨Åguration, we change only one threshold value and
keep the other four at the default setting. To evaluate the
impact on false negatives, we apply T ODDLER on the 11
bug-triggering tests for previously known bugs (Section IV -A)
and count the number of bugs found. To evaluate the impact
on false positives, we cannot use T ODDLER in the intended
scenarios from sections IV-A and IV-B, because they have few
false positives. We thus use the functionaltests (Section I V-C).
The default conÔ¨Åguration Ô¨Ånds all 11 known bugs in the
experiments from Section IV-A and reports 27 false positive s
in the experiments from Section IV-C. Figure 12 plots the569          	 
    	 
   	 
	
 
	
	
	 

	
Fig. 12. Parameter-sensitivity experiments. Each conÔ¨Ågur ation changes only
one threshold, with its value shown on the X-axis. The defaul t values are
boxed. Two sets of experiments are conducted for each conÔ¨Ågu ration: the
left/dark bar shows false positives on JUnit tests, and the r ight/light bar
shows bugs found on bug-triggering inputs. The Y-axis shows the numbers
normalized to the results under default setting.
number of bugs found (light/yellow bars) and false positive s
(dark/blue bars) normalized to the values for the default
conÔ¨Åguration. For bugs found, higher is better, and for fals e
positives, lower is better.
Impact on False Negatives: We increased the threshold
value for each parameter, and for only two of them such
increase has caused false negatives. The most sensitive is
minLCS, which measures the absolute length of the longest
common substring between two consecutive IPCS-sequences
for an instruction. When minLCSincreases from the default
7 to 10, the number of bugs found steadily decreases from
11 to 6. The longest common substring is usually shorter
than the total number of inner-loop iterations, which is oft en
determined by the input scale. Therefore,when the input sca le
is small, a high minLCSsetting could miss many bugs.
The other parameter whose increase caused false negatives
isminSeqRatio , which measures the ratio of loop iterations
that haveexecutedthe particularmemory-readinstruction .The
inner loop of bug #7.1 is buried inside an ifstatement that
is executed by about half of the outer-loop iterations. As a
result, this bug is missed once minSeqRatio gets over 50%.
We believe that this type of if/then/else situation is commo n
enough to have the default value under 50%. Note that, except
for this type of bugs, minSeqRatio can be increased to 60%
and beyond without losing any bugs.
Impact on False Positives: For the two parameters that
caused false negatives above, minLCSandminSeqRatio , we
both increased and decreased the threshold values. For the
other three parameters, we only increased the values. We
can see that increasing minIter andminSimRatio over
the default values decreases the number of false positives
by about 12% without loosing any bugs . In practice, one
may want to indeed increase these parameters, but we chose
conservative parameter values. In contrast, minLCSRatio is
the least sensitive: increasing it from 70% to 90% changes
neither false positives nor false negatives.
Choosing the Threshold Values: As seen from the dis-
cussion above, T ODDLER can work well in a large rangeof threshold values. Note that we did not choose the default
threshold values for TODDLER to obtain the best results for
false positives and false negatives . For example, we could
increaseminIter andminSimRatio to get fewer false posi-
tives without missing any bug. Rather, we chose the default
values based on our intuition about the values that could giv e
reasonableresults. Moreover,we settled onthese values before
running T ODDLER on performance tests (Section IV-B).
E.TODDLER for C/C++ Code
Theperformancebugsthat T ODDLER Ô¨Åndsdonotexistonly
in Java code; as already mentioned in Section II, such bugs
alsoexistinC/C++code.Tofurtherevaluateourtechnique, we
implemented a prototype T ODDLER tool for C/C++ code. Our
prototype uses Pin [19] to automatically instrument memory
reads but currently does not automatically instrument loop s;
we manually added loop events for six real-world bugs (three
from GCC, two from Mozilla, and one from MySQL). The
prototype logs values read and loop events, and computes
similarity ofÔ¨Çineby processing these logs using Python. The
results show that this prototype can Ô¨Ånd all these six bugs.
Becausewedonotinstrumentalltheloops,wecannotmeasure
false positives for this prototype.
V. DISCUSSION
Loop Nesting: TODDLER misses bugsthat are notin nested
loops. We intentionally focused on nested loops, because th ey
create more severe performance hits. However, non-nested
loops can also be slow, e.g., loops that contain I/O. T ODDLER
canbeextendedtolookforbugsinsuchloopsby modeling the
native, I/O methods in Java [20] to make their loops explicit .
Other Performance Bugs: TODDLER misses several cate-
gories of performance bugs, including (1) performance bugs
speciÔ¨Åc to multi-threaded code such as lock contention [21] ,
load imbalance [22], or false sharing [23], (2) bugs related to
idle time [24], and (3) object bloat [25]. T ODDLER Ô¨Ånds per-
formance bugs involving loops, which the existing techniqu es
miss, so T ODDLER complements these techniques.
Dynamic Technique: Just like proÔ¨Ålers, T ODDLER requires
a test input. Fortunately,developersalready write some pe rfor-
mance benchmarks but typically measure only real time and
look for regressions. T ODDLER provides an oracle to identify
performance bugs and encourages developers to write perfor -
mance tests. As our evaluation shows, performance tests are
relatively easy to write manually even by developers who are
not familiar with performance testing, and one can sometime s
evenuseautomatedtest-generationtechniquesforperform ance
tests. Future work can focus on developing specialized test -
generation techniques for performance bugs.
Similarity Measures: Because the longest common sub-
string worked quite well for comparing similarity of IPCS-
sequences, we did not evaluate any other approach. Future
work could, for example, use edit distance to compare IPCS-
sequences or, even further, capture the memory accesses not
asIPCS-sequencesofvaluesbut as executiontrees that enco de
loop iterations and then measure tree similarity.570VI. RELATEDWORK
ProÔ¨Åling, Visualization, and Computational Complexity:
ProÔ¨Åling and performance-visualization tools are critica l for
developers to understand the performance features of diffe rent
software components. A lot of recent progress was made
to provide more accurate and efÔ¨Åcient proÔ¨Åling [8], [10],
[26]‚Äì[32]. However, as discussed in Section I, proÔ¨Ålers hav e
fundamental limitations in detecting performance bugs. Se v-
eral tools estimate the worst-time computational complexi ty
of code [33]‚Äì[35], but like proÔ¨Ålers, these tools report tha t
some computation takes time, not if it wastes time. T ODDLER
complements these techniques to Ô¨Ånd performance bugs.
Performance-Bug Detection: Several techniquesdetect the
excessive use of temporary objects, a common performance
problem in object-oriented software [36], [37]. Xu et al. us e a
run-timeanalysistodetectlow-utilitydatastructureswh erethe
effort to construct member Ô¨Åelds outweighs the usage of thes e
Ô¨Åelds [25]. Jin et al. study efÔ¨Åciency rules in performance-
bug patches and detect performance bugs that are similar
with previously patched ones [9]. Other techniques detect
performance problems caused by idle time [24], multi-threa d
false sharing[23],orerrorrecoveryindistributedsystem s[38].
The success of these tools demonstrates the potential of
performance-bug detection, but the existing work only cove rs
a small portion of real-world performance bugs. T ODDLER fo-
cuses on performance bugs caused by inefÔ¨Åcient or redundant
computation across nested loops. Many of these bugs, such as
the real-world example bugs discussed in the paper, cannot b e
detected by the existing performance bug detectors. Theref ore,
TODDLER well complements these techniques.
VII. C ONCLUSIONS
Performance testing would greatly beneÔ¨Åt from automated
oracles for performance bugs. We presented T ODDLER, a
novel oracle that detects performance bugs by identifying
repetitive memory read sequences across loop iterations. T OD-
DLERfound 42 new bugs in 6 popular codebases: Ant,
Google Core Libraries, JUnit, Apache Collections, JDK, and
JFreeChart. So far developers have already Ô¨Åxed 10 of these
bugs and conÔ¨Årmed 6 more as real bugs. We also evaluated
TODDLER with 11 previously known, real-world performance
bugs, and the experiments show T ODDLER can effectively
detect performance bugs with a much higher accuracy than
proÔ¨Åling tools. T ODDLER can help expose more performance
bugs before software release by discovering problems even
before they manifest as slow computation found by proÔ¨Ålers.
While these results are highly promising, T ODDLER is just a
starting point in addressing loop-related performance bug s.
ACKNOWLEDGMENTS
We thank Caius Brindescu, Mihai Codoban, Hassan Es-
lami, Lyle Franklin, Mert Guldur, Alex GyÀù ori, Stas Negara,
FrancescoSorrentino,and Lor¬¥ andSzak¬¥ acsfor helpwith ex per-
iments. This material is based upon work partially supporte dby NSF under Grant Nos. CCF-1054616, CNS-0958199, and
CCF-0746856; and a Clare Boothe Luce faculty fellowship.
REFERENCES
[1] Bugzilla@Mozilla, ‚ÄúBugzilla keyword descriptions,‚Äù
https://bugzilla.mozilla.org/describekeywords.cgi.
[2] P. Kallender, ‚ÄúTrend Micro will pay for PC repair costs,‚Äù 2005,
http://www.pcworld.com/article/120612/article.html.
[3] G. E. Morris, ‚ÄúLessons from the Colorado bene-
Ô¨Åts management system disaster,‚Äù 2004, www.ad-mkt-
review.com/public html/air/ai200411.html.
[4] T. Richardson, ‚Äú1901 census site still down after six mon ths,‚Äù 2002,
http://www.theregister.co.uk/2002/07/03/1901 censussitestilldown/.
[5] D. Mituzas, ‚ÄúEmbarrassment,‚Äù 2009,
http://dom.as/2009/06/26/embarrassment/.
[6] I. Molyneaux, The Art of Application Performance Testing: Help for
Programmers and Quality Assurance . O‚ÄôReilly Media, 2009.
[7] R. E. Bryant and D. R. O‚ÄôHallaron, Computer Systems: A Programmer‚Äôs
Perspective . Addison-Wesley, 2010.
[8] S. Han, Y. Dang, S. Ge, D. Zhang, and T. Xie, ‚ÄúPerformance d ebugging
in the large via mining millions of stack traces,‚Äù in ICSE, 2012.
[9] G. Jin, L. Song, X. Shi, J. Scherpelz, and S. Lu, ‚ÄúUndersta nding and
detecting real-world performance bugs,‚Äù in PLDI, 2012.
[10] M. Jovic, A. Adamoli, and M. Hauswirth, ‚ÄúCatch me if you c an:
Performance bug detection in the wild,‚Äù in OOPSLA, 2011.
[11] S. Zaman, B. Adams, and A. E. Hassan, ‚ÄúA qualitative stud y on
performance bugs,‚Äù in MSR, 2012.
[12] M. D. Bond and K. S. McKinley, ‚ÄúProbabilistic calling co ntext,‚Äù in
OOPSLA, 2007.
[13] A. Podgurski, D. Leon, P. Francis, W. Masri, M. Minch, J. Sun, and
B. Wang, ‚ÄúAutomated support for classifying software failu re reports,‚Äù
inICSE, 2003.
[14] S. Yoo, M. Harman, P. Tonella, and A. Susi, ‚ÄúClustering t est cases
to achieve effective and scalable prioritisation incorpor ating expert
knowledge,‚Äù in ISSTA, 2009.
[15] T. H. Cormen, C. E. Leiserson, and R. L. Rivest, Introduction to
Algorithms . The MIT Press, Cambridge, MA, 1990.
[16] Soot, ‚ÄúSoot: A Java optimization framework,‚Äù
http://www.sable.mcgill.ca/soot/.
[17] Sun Microsystems, ‚ÄúHPROF JVM proÔ¨Åler,‚Äù http://java.s un.com/develo-
per/technicalArticles/Programming/HPROF.html.
[18] R. Sharma, M. Gligoric, A. Arcuri, G. Fraser, and D. Mari nov, ‚ÄúTesting
container classes: Random or systematic?‚Äù in FASE, 2011.
[19] C.-K. Luk, R. S. Cohn, R. Muth, H. Patil, A. Klauser, P. G. Lowney,
S. Wallace, V. J. Reddi, and K. M. Hazelwood, ‚ÄúPin: Building c us-
tomized program analysis tools with dynamic instrumentati on,‚Äù inPLDI,
2005.
[20] W. Visser, K. Havelund, G. Brat, and S. Park, ‚ÄúModel chec king
programs,‚Äù ASE-J, 2003.
[21] N. R. Tallent, J. M. Mellor-Crummey, and A. PorterÔ¨Åeld, ‚ÄúAnalyzing
lock contention in multithreaded applications,‚Äù in PPOPP, 2010.
[22] J. Oh, C. J. Hughes, G. Venkataramani, and M. Prvulovic, ‚ÄúLIME: A
framework for debugging load imbalance in multi-threaded e xecution,‚Äù
inICSE, 2011.
[23] T. Liu and E. D. Berger, ‚ÄúPrecise detection and automati c mitigation of
false sharing,‚Äù in OOPSLA, 2011.
[24] E. Altman, M. Arnold, S. Fink, and N. Mitchell, ‚ÄúPerform ance analysis
of idle programs,‚Äù in OOPSLA, 2010.
[25] G. H. Xu, N. Mitchell, M. Arnold, A. Rountev, E. Schonber g, and
G. Sevitsky, ‚ÄúFinding low-utility data structures,‚Äù in PLDI, 2010.
[26] J. Demme and S. Sethumadhavan, ‚ÄúRapid identiÔ¨Åcation of architectural
bottlenecks via precise event counting,‚Äù in ISCA, 2011.
[27] T. Mytkowicz, A. Diwan, M. Hauswirth, and P. F. Sweeney, ‚ÄúEvaluating
the accuracy of Java proÔ¨Ålers,‚Äù in PLDI, 2010.
[28] M. Hauswirth, A. Diwan, P. F. Sweeney, and M. C. Mozer, ‚ÄúA utomating
vertical proÔ¨Åling,‚Äù in OOPSLA, 2005.
[29] E. Coppa, C. Demetrescu, and I. Finocchi, ‚ÄúInput-sensi tive proÔ¨Åling,‚Äù in
PLDI, 2012.
[30] D. Zaparanuks and M. Hauswirth, ‚ÄúAlgorithmic proÔ¨Åling ,‚Äù inPLDI,
2012.[31] A. Diwan, M. Hauswirth, T. Mytkowicz, and P. F. Sweeney, ‚ÄúTraceAna-
lyzer: A system for processing performance traces,‚Äù Softw. Pract. Exper. ,
March 2011.
[32] D. C. D‚ÄôElia, C. Demetrescu, and I. Finocchi, ‚ÄúMining ho t calling
contexts in small space,‚Äù in PLDI, 2011.
[33] J. Burnim, S. Juvekar, and K. Sen, ‚ÄúWISE: Automated test generation
for worst-case complexity,‚Äù in ICSE, 2009.
[34] S. A. Crosby and D. S. Wallach, ‚ÄúDenial of service via alg orithmic
complexity attacks,‚Äù in USENIX Security Symposium , 2003.
[35] S. Gulwani, K. K. Mehra, and T. M. Chilimbi, ‚ÄúSPEED: Prec ise and
efÔ¨Åcient static estimation of program computational compl exity,‚Äù in
POPL, 2009.
[36] G. H. Xu, M. Arnold, N. Mitchell, A. Rountev, and G. Sevit sky, ‚ÄúGo
with the Ô¨Çow: ProÔ¨Åling copies to Ô¨Ånd runtime bloat,‚Äù in PLDI, 2009.
[37] B. Dufour, B. G. Ryder, and G. Sevitsky, ‚ÄúA scalable tech nique for
characterizing the usage of temporaries in framework-inte nsive Java
applications,‚Äù in FSE, 2008.
[38] C.Killian, K.Nagaraj, S.Pervez, R.Braud, J.W.Anders on, andR.Jhala,
‚ÄúFinding latent performance bugs in systems implementatio ns,‚Äù inFSE,
2010.571