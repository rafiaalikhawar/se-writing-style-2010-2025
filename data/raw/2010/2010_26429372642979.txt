Automated Analysis of Multithreaded Programs for
P
erformance Modeling
Alexander Tarvo
Brown University
Providence, RI, USA
alexta@cs.brown.eduSteven P . Reiss
Brown University
Providence, RI, USA
spr@cs.brown.edu
ABSTRACT
The behavior of multithreaded programs is often diﬃcult
to understand and predict. Synchronization operations and
limited computational resources combine to produce com-
plex non-linear dependencies between a program’s conﬁgu-
ration parameters and its performance. Performance mod-
els are used to understand these dependencies. Such models
are complex, and constructing them requires a solid under-
standing of the program’s behavior. As a result, building
models of complex applications manually is extremely time-
consuming and error-prone. In this paper we demonstrate
that such models can be built automatically.
This paper presents our approach for automatically mod-
eling multithreaded programs. Our framework uses a com-
bination of static and dynamic analyses of a single repre-
sentative run of a system to build a model that can then
be explored under a variety of conﬁgurations. We show how
the models are constructedandshow theyaccurately predict
the performance of various multithreaded programs, includ-
ing complex industrial applications.
Categories and Subject Descriptors
F.3.2 [General ]: Logics and Meaning of Programs— Pro-
gram analysis ; C.4 [Computer Systems Organization ]:
Performance of systems— Modeling techniques
Keywords
Program analysis; performance; modeling
1. INTRODUCTION
Multithreaded programs demonstrate complex non-linear
dependencybetweentheconﬁgurationandperformance. Con-
ﬁgurations may reﬂect variations in the workload, program
options such as the number of threads, and characteristics
of the hardware. To better understand this dependency a
performance prediction model is used. Sucha model predicts
performance of a program in diﬀerent conﬁgurations.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
ASE’14, September 15-19, 2014, Vasteras, Sweden.
Copyright 2014 ACM 978-1-4503-3013-8/14/09 ...$15.00.
http://dx.doi.org/10.1145/2642937.2642979 .Performance models are essential for various applications
[20],[10],[28]. For example, a model may help ﬁnding a good
conﬁguration for deploying the Tomcat web server. For
each combination of conﬁguration parameters, including the
number of CPU cores, the number of Tomcat working thre-
ads, or the rate of incoming connections, the model will pre-
dict response time, throughput, and resource utilization for
Tomcat. A conﬁguration that utilizes resources eﬃciently
and satisﬁes the service agreement can be used for deploy-
ment. Performance models canbe also used todetectperfor-
mance anomalies and discover bottlenecks in the program.
Modern multithreaded applications can be large and com-
plex, and are updated regularly. Building their models man-
ually is extremely time-consuming and error-prone. To be
practical, building such models should be automated.
Building performance models of such applications is hard.
First, it requires discovering queues, threads, and locks in
theprogram; details oftheir behavior; andsemantics oftheir
interaction. Doing this automatically requires complex pro-
gram analysis. Second, it requires measuring demand for
hardware resources such as the CPU, disk, and the network.
This is a complex problem that requires collecting and com-
bining information from multiple sources. Third, the per-
formance of a parallel system is dependent on its contention
for computation resources and locks. Accurate modeling re-
quires simulating these resources and locks in detail.
This paper presents an approach towards automated per-
formance modeling of multithreaded programs. Its main
contribution is a combination of a model that accurately
simulates complex thread interactions in a program and a
methodology to build such models automatically. The paper
makes the following technical contributions:
•A combination of static and dynamic analyses for un-
derstanding the structure and semantics of multithre-
aded programs automatically;
•An approach for collecting parameters of performance
models from user- and kernel-mode traces;
•Veriﬁcation of our approach by constructing models of
various multithreaded programs
While working on the automatic model building we made
importantﬁndings. First, theanalysisofaprogramisgreatly
simpliﬁed if that program relies on well-deﬁned implemen-
tation of high-level locks (semaphores, barriers, blocking
queues etc.). Second, in order to be fast and easy to un-
derstand the resulting model must be simple and compact.
Building compact models requires identifying program con-
structs that do not have signiﬁcant impact on performance,
7
and excluding these constructs from the model. Third, ac-
c
urate prediction requires precise measures of resource de-
mands for the elements of the program. In certain cases
small errors in measuring resource demands can lead to large
prediction errors.
2. SCOPE AND CHALLENGES
Weanalyzeperformanceofmultithreadedapplicationssuch
as servers, multimedia programs, and scientiﬁc computing
applications. Such programs split their workload into sep-
aratetaskssuch as an incoming HTTP request in a web
server, a part of a scene in a 3D renderer, or an object in a
scientiﬁc application. We do not model the performance of
individual tasks or requests; instead we predict the aggregate
performance of the system for a given workload .
Processing tasks is parallelized across thread pools. A
thread pool is a set of threads that have same functional-
ity and can process tasks in parallel. Multiple threads rely
on synchronization to ensure semantic correctness (e.g. the
thread may start executing only after a barrier is lifted) and
to protect shared data. This results in the parallel execution
of some computations and the sequential execution of oth-
ers. Threads also use shared hardware resources, such as the
CPU,disks, andthenetworksimultaneously, whichmaylead
to their saturation. This combination of locking and simul-
taneous resource usage leads to complex non-linear depen-
dencies between conﬁguration parameters of the program
and its performance. As a result, even an expert may not
understand such dependencies on a quantitative level. The
best approach is to build a performance prediction model.
We focus on following aspects of performance modeling:
Automatic generation of performance models. We
minimize the need for human participation in building the
model. Ourprogramanalysis andmodelgeneration aredone
automatically. The analyst need only inspect the gener-
ated model and specify conﬁgurations in which performance
shouldbepredictedandthemetricsthatshouldbecollected.
Generating models from a running a program in
a single conﬁguration. Building the model should not
require running the program many times in many conﬁgu-
rations. Such experimentation is time-consuming and may
not be feasible in a production environment. Instead, we
want to generate the model by running a program in a sin-
glerepresentative conﬁguration , in which the behavior and
resource demandsof the program approach the behaviorand
resource demands of a larger set of conﬁgurations.
Accurate performance prediction for a range of
conﬁgurations. This lets our model to answer “what-if”
questions about the program’s performance, detect perfor-
mance anomalies in the running program, and be used as a
decision-making element of a self-conﬁguring data center.
Wemodelprogramsrunningoncommodityhardware. Pre-
dicting performance of programs runningon cluster and grid
systems would require developing an additional set of hard-
ware models and potentially diﬀerent approach for program
analysis, which is beyond the scope of this paper.
Building performance models of complex, multithreaded
systems is challenging. The primary challenges are:
Discovering the semantics of thread interaction.
Building the performance model requires knowledge of the
queues, buﬀers, and the locks in the program, their seman-
tics (e.g. is this particular lock a semaphore, a mutex, or a
barrier), and interactions (e.g. which thread reads or writesto a particular queue or accesses a particular lock). There
are numerous ways to implement locks and queues, and to
expose their functionality to threads. Discovering this infor-
mation automatically requires complex program analysis.
Discovering parameters of the program’s compo-
nents.Performance of the program depends on parameters
of its locks and queues, and on the resource demands of its
threads. For example, the amount of time the thread has
to wait on a semaphore depends on the number of avail-
able semaphore permits. The amount of time the program
spends on the disk I/O depends on the amount of data it has
to transfer. However, the retrieving parameters of locks and
queues may require further program analysis and obtaining
resource demands may require instrumenting the OS kernel.
3. MODEL DEFINITION
Below we brieﬂy describe the model we build automat-
ically. We use discrete-event simulation models [23] that
consist of three tiers.
The high-leveltier simulates the ﬂowoftasks processed by
the program. It is a queuing network model whose queues
correspond to program’s queues and buﬀers as well as to
some OS queues. The service nodes correspond to the pro-
gram’s threads and thread pools.
The mid-level tier simulates the delaysthat occur in the
program’s threads as they process tasks. Thread models are
probabilistic call graphs (PCGs), where each vertex si∈S
corresponds to a piece of the thread’s code – a code fragment
(CF). Edges representpossible transitions of control ﬂowbe-
tweentheCFsandarelabeledwiththeirprobability. Transi-
tion probabilities are deﬁned by the mapping δ:S→P(S).
We distinguish three major sources of delays in processing
tasks, which correspond to three classes of code fragments :
I/O code fragments (denoted as cio) represent I/O opera-
tions; synchronization( csync)CFsrepresentsynchronization
operations; computation ( ccpu) CFs represent computations
and memory operations. In addition, cinandcoutCFs com-
municate with the high-level queuing model. cinCFs fetch
tasks from the queues of the queuing model, while coutCFs
send tasks to the queuing model.
The lower-tier model simulates the system’s shared re-
sources: the CPU and the OS thread scheduler, the disk
I/O subsystem, and the set L={l1,...,lm}of locks in the
program. These models are part of Q(t) – the state of the
whole simulation at each moment of time t.
As an example, a model of a simple web server is shown in
Figure 1. The accept threadlistens for incoming connections
(the CF s1in its thread model). Once the connection has
been accepted, the accept thread creates a task object ( s2
-s4) and sends ( s5) it into the task queue. Once one of
the working threads becomes available, it fetches ( s6) the
task from the queue and processes it ( s7-s8). The working
thread veriﬁes that the requested page exists, reads if from
the disk, and sends it to the client. Finally, the thread closes
the connection and fetches the next task from the queue.
Execution of each CF results in the delay τ. While the
call graph structure /an}bracketle{tS,δ/an}bracketri}htdoesnotgenerally change between
diﬀerent conﬁgurations, execution times for code fragments
can be aﬀected by resource contention . To accurately simu-
late the time delays τwe rely on the lower-tier models.
For each code fragment we deﬁne a set of parameters Π
(see Table 1). The parameter of a computation CF Π cpu=
/an}bracketle{tτcpu/an}bracketri}htis theCPU time for that fragment. The parame-
8Figure 1: A model for a web server
t
ers of a disk I/O CF is a sequence Π disk=/an}bracketle{tdio1,...,dio k/an}bracketri}ht
of low-level disk I/O operations initiated by that CF. The
number kof I/O requests allows to implicitly simulate the
OS page cache. It was shown [15] that after serving a suﬃ-
cient numberof requests(104to 105in our experiments), the
cache enters a steady state, where the probability of cache
hit converges to a constant. In terms of our model, kfollows
a stationary distribution, where k= 0 indicates a cache hit.
Values of Π cpuand Π diskvary across diﬀerent executions
of CFs, so we representthem as distributions PΠ
cpuandPΠ
disk.
The parameters Π sync=/an}bracketle{tlid,optype,τ out/an}bracketri}htof a synchro-
nization CF are the ID of the lock being called, the type of
synchronization operation (e.g. barrier.await, mutex.enter,
or mutex.exit), and the timeout.
When the thread model needs to compute the τifor the
CFsi, it retrieves the parameters Π iand calls the corre-
sponding low-tier resource model: ccpuCFs call the model
of the CPU and OS scheduler, cioCFs call the model of
disk I/O subsystem, and csyncCFs call the model of a
corresponding lock lj∈L, which simulates that lock se-
mantically. The resource model computes τas a function
τ=f(Π,Q(t)). Once the delay τis over, the resource model
notiﬁes the thread model, which resumes its execution.
Low-level resource models have parameters too. In partic-
ular, the parameter of the CPU is the number of cores. The
parameters /an}bracketle{tlid,ltype,lparam /an}bracketri}htof the lock lj∈Lare the
lock ID, lock type (e.g. semaphore, barrier, or mutex), and
the additional parameters speciﬁc to the type of the lock.
For example, the parameter of the barrier is the barrier ca-
pacity, and the parameter of the semaphore is its count.
Low-level models are implemented as a combination of
queuing and statistical models. Their detailed description
is beyond the scope of this paper. Information on modeling
hardware and locks and can be found in [35],[38],[22].
4. AUTOMATIC MODEL GENERATION
Constructing the performance model requires collecting
the following information about the program automatically:Table 1: Model components and their parameters
Entity Description
S={s1. ..s n} The set of all nodes (code fragments)
in the PCG
δ:S→P(S) Transition probabilities for PCG nodes
τi Delay caused by executing CF si∈S
Πdisk= I/O CF parameters: a sequence of
/an}bracketle{td
io1,...,dio k/an}bracketri}ht low-level I/O operations
Πcpu=/an}bracketle{tτcpu/an}bracketri}ht Computation CF parameters:
the amount of CPU time
Πsync= Synchronization CF parameters: an ID
/an}bracketle{tl
id,optype,τ out/an}bracketri}htof the lock called, operation type, timeout
L={l1. ..l m} The set of all locks in a program
Πlock= Lock parameters: an ID of the lock,
/an}bracketle{tl
id,ltype,lparam /an}bracketri}htlock type, type-speciﬁc parameters
•T
heset ofqueues, threads(correspondtoservice nodes
in the upper-tier model), and knowledge of their inter-
actions (correspond to cin/coutCFs in the middle-tier
model);
•The set of thread pools. The sizes of thread pools are
conﬁguration parameters that impact performance;
•The computations, I/O, and locking operations (corre-
sponding to the set Sof CFs) and the sequence of their
execution (corresponding to transition probabilities δ);
•The parameters of CFs, required to model delays τ;
•The setLof locks, their types, and parameters Π lock.
We collect the required data in four stages (see Figure 2)
using a combination of static and dynamic analysis. Each
stage saves intermediate results into ﬁles that are used as
input to subsequent stages.
First, the program is executed and its call stack is sam-
pled. The stack samples are used to detect thread groups
and libraries in the program. Second, a static analysis of the
programisperformed. Duringthisstagewedetect csync,cin,
cout, andcioCFs. Third, the program is instrumented and
executedagain with the same conﬁguration. The instrumen-
tation log is used to detect program-wide locks and queues,
properties Π of code fragments, and to build the probabilis-
tic call graphs /an}bracketle{tS,δ/an}bracketri}htof the program’s threads. Finally, the
collected information is used to build a performance model.
All these operations are performed automatically.
Below we describe these stages in more details.
4.1 Collecting stack samples
Duringthestacksamplingstageourframework ﬁndsthread
pools, frequently called functions and methods in the pro-
gram, and frequently called library functions. Identifying
libraries is essential for generating correct probabilistic call
graphs (see Section 4.3.1).
As the program is being executed, the framework period-
ically takes“snapshots”of the call stack of the running pro-
gram, which are merged to build a call trie of the program.
In a call trie each leaf node contains the code location being
executed, which includes the name of a function or a method
being executed, and a line number. The non-leaf nodes
provide a call stack for that code location. For each leaf
the framework maintains the list of pairs /an}bracketle{tt1,c1/an}bracketri}ht,.../an}bracketle{ttn,cn/an}bracketri}ht,
where the ciis the number of executions of that code loca-
tion by the thread ti.
Thread groups are detected in two stages. First a map T
is created. Its keysare threadtuplesdiscoveredbysampling,
9Figure 2: Model creation stages and intermediate results
a
nd its values are execution counts. For each leaf in the trie
the framework gets a tuple Ti=/an}bracketle{tt1,...tk/an}bracketri}htof threads that
executed the node along with the total numberof executions
Ci=/summationtext(c1,...,c k). IfTdoes not contains the tuple Ti,
the pair /an}bracketle{tTi,Ci/an}bracketri}htis inserted into T. Otherwise the number
of executions for the existing tuple is increased by Ci.
Second, threadtuplesin Tare merged. Thetuple /an}bracketle{tT1,C1/an}bracketri}ht
is merged with /an}bracketle{tT2,C2/an}bracketri}htif and only if all threads in T2 also
present in T1 andC1≫C2. The resulting tuple is formed
as/an}bracketle{tT1,C1 +C2/an}bracketri}ht. After merging, the tuples T1...Tm∈T
represent the thread pools detected in the program.
Stacksamples are also usedto identifyprogram’s libraries.
For everyfunction ftheframework generatestheset offunc-
tions/an}bracketle{tf1,...,fn/an}bracketri}htthatcalled f. Ifthenumberofcallees n >1,
fis added to the set of library functions . Although the stack
sampling may not detect some rarely executed library func-
tions, this does not aﬀect correctness of our models.
4.2 Static analysis
During static analysis our framework scans the code of
the program and detects csync,cio,cinandcoutCFs. It
also detects the creation points of locks and queues in the
program, as a prerequisite for the dynamic analysis.
The static analyzer represents the program as a depen-
dency graph. The vertices of this graph correspond to func-
tions and methods in the program (both called “function”
herein). The edges are code dependencies (e.g. the func-
tion A calls the function B) and data dependencies (e.g.
the function A refers the class B or creates the instance of
B) between these functions. The transitive closure of all the
vertices in the dependencygraph represents all thecode that
may be executed by the program.
The static analyzer traverses the dependencygraph, start-
ing from the functions discovered during the stack sampling.
It scans the code of the functions, searching for the speciﬁc
constructs that represent CFs. In the process the analyzer
searches for references to other functions and methods, that
are subsequently loaded and analyzed.
There arenumerousways toimplementsynchronizationin
a program. As a result, detecting cin,coutand synchroniza-
tion CFs and determining their operation types optypemay
require complex analysis that is very hard to automate. We
therefore assume the program has used speciﬁc implemen-
tations of locks and queues for thread interactions. Exam-
ples ofsuchimplementations are the java.util.concurrent
package in Java, the System.Threading namespace in C#,
and the boost threading library in C/C++.
The analyzer considers calls to speciﬁc functions that per-
formsynchronizationoperationsandaccessprogram’s queues
ascsync,cin, andcoutCFs appropriately. Typically, these
are the functions that constitute the API of the correspond-
ing thread and locking library. The class of the CF and thetype of synchronization operation optypeare inferred from
the name and the signature of the called function.
The analyzer also tracks low-level synchronization primi-
tives, such as monitors, mutexes, and synchronized regions.
These constructs are modeled explicitly as csyncCFs. How-
ever, when the combination of low-level primitives is used
to implement a high-level lock, the probabilistic call graph
(PCG) may not be able to capture the deterministic behav-
ior of such lock. Consider a custom implementation of a
cyclic barrier that maintains the counter of waiting threads.
When the thread calls the barrier, the program checks the
value of the counter. If the value of the counter is less than
the capacity, the calling thread is suspended; otherwise the
program wakes up all the waiting threads. In the PCG this
behavior will be reﬂected as a fork with the probability of
lifting the barrier equal to 1/(barrier capacity). As a result,
in some cases the model will lift the barrier prematurely, and
in other cases it will not lift the barrier when it is necessary.
The analyzer also tracks calls to the constructors and ini-
tializers of locks and queues. These calls do not directly
correspond to the csyncCFs, but they are used to detect
queues and locks in the program and retrieve their parame-
ters during the dynamic analysis.
To discover the ciocode fragments, the analyzer tracks
API functions that can perform disk I/O. Calls to the func-
tions that may access the ﬁle system metadata are consid-
ered as I/O CFs as are the bodies of low-level functions that
perform ﬁle I/O.
4.3 Dynamic analysis
The purpose of dynamic analysis is to identify ccpuCFs,
the parameters of locks and CFs, and the probabilistic call
graphs/an}bracketle{tS,δ/an}bracketri}htof the program’s threads.
The dynamic analyzer instruments the program and runs
itagaininthesameconﬁgurationastheinitialstack-sampling
run. Each CF detected during the static analysis is instru-
mented with two probes. A start probe is inserted immedi-
ately before the CF, and an end probe is inserted right after
the end of the CF. Each probe is identiﬁed by the unique
numeric identiﬁer (probeID).
Probes reportthetimestamp, theprobeID,andthethread
ID. For CFs corresponding to a function call, the start probe
reports function’s arguments, and the end probe reports the
return value. For method calls probes also report the ref-
erence to the called object, if relevant. This information is
used to obtain parameters of csync,cin, andcoutCFs.
During its execution the instrumented program generates
the sequence of probe hits on a per-thread basis, which con-
stitute a traceof the thread. Two coincident probe hits in
the trace form a pair /an}bracketle{tstart probe ID, end probe ID /an}bracketri}ht. Every
such pair represents an execution of a single code fragment.
10ProbeID Timestamp ObjectID Arguments/
r
eturn value
10 11345231 7683745 0
11 11387461 7683745 4387459
27 11391365 87235467
28 11392132
10205 11396190 1872565
10206 19756012 1872565
6 19873872 87235467
7 19873991
10205 19923752 32748998
10206 25576572 32748998
...
Figure 3: A fragment of the trace for a thread.
The/an}bracketle{tstart probe ID, end probe ID /an}bracketri}htpairsare“overlapping”
in the trace, so the end probe ID of one pair becomes the
startprobeIDofthenextpair. Thusexecutionsof cio,csync,
cin, andcoutCFs in the trace are interleaved with pairs of
probe IDs. These pairs, which represent computations per-
formed between executions of cio,csync,cin, andcoutCFs,
correspond to ccpuCFs.
The Figure 3 depicts an example of such trace. Here the
CF/an}bracketle{t10, 11/an}bracketri}htis acinCF. The object ID=7683745 recorded by
the probe 10 identiﬁes the queue, while the argument value
0 correspond to the timeout of 0 milliseconds. The probe
11 reports the return value 4387459, which is an ID of the
retrieved object. /an}bracketle{t27, 28/an}bracketri}htand/an}bracketle{t6, 7/an}bracketri}htare synchronization CFs
corresponding to the entry and exit from the synchronized
region. The object ID=87235467 identiﬁes the monitor as-
sociated with that region. Two instances of /an}bracketle{t10205, 10206 /an}bracketri}ht
I/O CF correspond to two (unrelated) ﬁle read operations
from the disk. Their object IDs identify the instances of
the corresponding ﬁle objects. Pairs /an}bracketle{t11, 27/an}bracketri}ht,/an}bracketle{t28, 10205 /an}bracketri}ht,
/an}bracketle{t10206, 6/an}bracketri}ht, and/an}bracketle{t7, 10205/an}bracketri}htare the computation CFs.
4.3.1 Construction of probabilistic call graphs
Ana¨ıveapproachtogeneratingtheprobabilisticcallgraph
(PCG) for a thread is to treat the set s1...snof CFs discov-
ered in the trace as the set Sof nodes in the PCG. For each
nodesi∈Sthe subset Snext={sk,...,s m}of succeeding
nodes is retrieved, along with the numbers of occurrences of
the pairs ( si,sk),...,(si,sm). The probability of transition
from the node sitosj,j∈(k...m) is calculated as
p(si,sj) =count(si,sj)/summationtextm
l=kcount(si,sl)(1)
Probabilities of transition for every pair of nodes consti-
tute the mapping δ:S→P(S) in the mid-tier model.
The na¨ıve approach may not represent calls to the pro-
gram’s librariescorrectlyandgeneratesoverlycomplexPCG.
To become practical, this approach must be improved.
Correct representation of library calls. Distinct ex-
ecution paths in the program must be represented as non-
intersecting paths in the PCG, so that the control ﬂow in
the model will not be transferred from one such path to
another. However, if these execution paths call a library
function containing a code fragment, the instrumentation
would emit same probe IDs for both calls, which correspond
to executing the same CF. As a result, distinct execution
paths will be connected by the common node in the PCG,
which is semantically incorrect.
Figure 4: Top: the ground truth PCG from the
t
hread trace. Bottom: the incorrect PCG gener-
ated from the trace that contains a library call.
For example, according to the trace shown on the Figure 3
the program enters the synchronized region, reads data from
a ﬁle, exits the synchronized region, and performs another
unrelated ﬁle read. The “ground truth” call graph has no
loops or branches (see Figure 4, top). However, both I/O
operations will eventually call the same read() I/O API that
contains an /an}bracketle{t10205, 10206 /an}bracketri}htI/O CF. As a result, the gener-
ated PCG will contain a loop in it (see Figure 4, bottom).
While simulating this loop the model may not exit the syn-
chronized region, or may attempt exiting it multiple times.
In both cases the behavior of the model will be incorrect.
To address this problem the dynamic analyzer represents
separate calls to the library CFs as separate PCG nodes
using the node splitting technique described in [33]. For
every CF located within one of the program’s libraries, the
analyzer adds a context information describing the origin of
the call to that library.
This information is obtained by instrumenting calls to the
library functions discovered during the stack sampling (see
Section 4.1). An entry library probe is inserted before every
call to a library function; an exit library probe is inserted
after such call. As the analyzer scans the trace, it maintains
a call stack of library probes. When the entry library probe
is encountered in the trace, its ID is added into the stack.
This ID is removed from the stack when the corresponding
exit probe is detected. When the analyzer detects the CF, it
adds the sequence of library probe IDs present in the stack
as the preﬁx of that CF ID.
For an example, consider that entry/exit library probes
500/501 and 502/503 were inserted into the program, so the
resulting sequence of probe IDs in the trace is 10, 11, 27,
28, 500, 10205, 10206, 501, 6, 7, 502, 10205, 10206, 503.
The corresponding sequence of CF is /an}bracketle{t10, 11/an}bracketri}ht,/an}bracketle{t11, 27/an}bracketri}ht,/an}bracketle{t27,
28/an}bracketri}ht,/an}bracketle{t28, 10205 /an}bracketri}ht,/an}bracketle{t500, 10205, 10206 /an}bracketri}ht,/an}bracketle{t10206, 6/an}bracketri}ht,/an}bracketle{t6, 7/an}bracketri}ht,/an}bracketle{t7,
10205/an}bracketri}ht,/an}bracketle{t502, 10205, 10206 /an}bracketri}ht, which is consistent with the
ground truth PCG.
Reducing the complexity of the model. According
to the na ¨ıve approach, all the computations between cio,
csync,cin, andcoutCFs are represented as ccpuCFs, even if
their impact on performance is negligible. Similarly, every
synchronization region is represented as a pair of CFs, even
if it is very short and never becomes contended in practice.
This leads to an unnecessary complex PCG, consisting of
thousands of CFs (see Table 2). Such complex models have
low performance and are hard to analyze. To simplify the
model we remove all the insigniﬁcant CFs that have negli-
gible impact on the program’s performance.
Model optimization is performed in two steps. First, the
whole timeline of the program’s execution is split into three
phases: the startup phase, when the program doesn’t pro-
11cess tasks yet; the work phase, when the program processes
t
asks; and the shutdown phase, when the program doesn’t
process tasks any more. Finding stages is easy for programs
that handle external requests, such as servers. A timestamp
marking the beginning of the work phase is recorded before
issuing the ﬁrst request, and the end timestamp is recorded
after the last request is complete. If startup or shutdown
stages cannot be easily deﬁned for a program, we assume
these stages are absent in the trace.
The model doesn’t simulate program’s performance dur-
ing the startup and shutdown phases. Among all CFs ex-
ecuted during the startup phase, only the CFs that are re-
quired to build a semantically correct model ( cin,cout, and
csyncCFs that perform complex synchronization operations,
such as awaiting on the barrier) are incorporated into the
model. Remaining CFs are considered as insigniﬁcant. All
the CFs executed during the shutdown phase are considered
as insigniﬁcant.
Second, the insigniﬁcant CFs executed during the work
phase are removed from the model. These are ccpuCFs
whose summary CPU times amounts to less than t% of the
overall CPU time for the thread, and cioCFs whose sum-
mary data transfer amounts to less than t% of data trans-
ferred by the thread. Setting t= 3−5% allows shrinking the
PCG by 50-70% without noticeable impact on the accuracy.
Accounting for determinism in the program be-
havior. Some program behaviors are diﬃcult to represent
accurately using a probabilistic model. First, the execution
ﬂow may take diﬀerent paths depending on the availability
of the task in the queue. To account for this cfetch
inand
cnofetch
in“virtual” nodes are inserted after each cinnode in
the PCG. The cfetch
innode is executed when the cinCF was
able to fetch the task from the queue. cnofetch
inis executed
ifcindid not fetch the task and exited by the timeout.
Second, representing loops as cycles in a PCG may aﬀect
the model’s accuracy. If a loop that performs exactly niter-
ations is representedas a cycle in a PCG, thenthe numberof
iterations Xfor that cycle will not be a constant. It can be
shown that Xwill rather be a random variable that follows a
geometric distribution with mean nand a probability mass
function Pr(X=k) =1
n·(1−1
n)k−1.In most cases this
representation has a minor eﬀect on the prediction accuracy.
However, if the program’s performance ystrictly follows the
function y=f(n), the predicted performance y′will be a
function of a random variable y′=f(X), whose parameters
(mean, standard deviation) may diﬀer noticeably from y.
In our experiments such mispredictions occurred if the
loop performed aninitial populationoftheprogram’s queues
with tasks. To address this issue the dynamic analyzer de-
tects loops in the trace using the algorithm [27]. If the loop
contains the coutnode, the model explicitly simulates it.
Otherwise the loop is represented as a cycle in the PCG.
4.3.2 Retrieving parameters of code fragments
The dynamic analyzer retrieves parameters of the model’s
constructs from the trace.
Locks and task queues. Parametersoflocksandqueues
are obtained from the arguments passed to constructors and
intializers of these locks and queues, and from their return
values. The lock type ltypeis inferred from the signature
of the constructor/intializer. The type-speciﬁc parameters
lparam are retrieved from the values of arguments passed
to that constructor. The lock ID lidis obtained from thereference to thelock returnedbythe constructor; it uniquely
identiﬁes each lock li∈L. Queues and their parameters are
obtained in the same manner.
csync, cin, and c outCFs. Parameters of these CFs are
obtained from the arguments passed to functions and meth-
ods operating on locks and queues, and from their return
values. The ID of the called lock lidis obtained from the
reference to the lock; it is matched to the lidreturned by
the lock constructor/initializer. The type of synchroniza-
tion operation optypeis inferred from the signature of the
called function. The operation timeout τoutis retrieved from
the arguments passed to the function. Parameters of the
cin/coutCFs are obtained in the same manner.
Some low-level synchronization operations, such as an en-
try/exit from a synchronized block, might not call functions
or methods. optypefor such operation is obtained by ana-
lyzing the corresponding instruction in the program. lidis
obtained from the reference to the associated monitor.
ccpuCFs. The parameter of the ccpuCF is the distribu-
tionPτ
cpuof CPU times τcpu.τcpucan be accurately mea-
sured when the execution time of a thread can be deter-
mined. When this is not the case, τcpuis measured as the
diﬀerence between the timestamps of start and end probes
of the CF, substituting clock time for CPU time. However,
in order to use the latter approach we need to avoid conﬁg-
urations where CPU congestion is likely.
cioCFs. The parameters of the cioCF are the num-
berkand properties (the type of I/O operation and the
amount of data transferred) of low-level disk I/O requests
{dio1,...,dio k}initiated bythat cioCF. Thisrequest-speciﬁc
data can be retrieved only from the OS kernel. We used the
blktrace [1] to retrieve the log of all kernel-mode disk I/O
operations initiated by the program.
Generally, the timestamps and thread IDs in the kernel-
mode I/O log might not match the timestamps and thread
IDs in the instrumentation log. To match blktrace log to
the instrumentation log the dynamic analyzer uses cross-
correlation – a technique used in signal processing [36]. The
cross-correlation ( f ⋆g)[t] is a measure of similarity between
signalsfandg, where one of the signals is shifted by the
time lag ∆ t. The result of a cross-correlation is also a signal
whose maximum value is achieved at the point t= ∆t. The
magnitude of that value depends on similarity between f
andg. The more similar are those signals, the higher is the
magnitude of ( f ⋆g)[∆t].
The analyzer represents sequences of I/O operations ob-
tained from the kernel-mode trace and user-mode trace as
signals taking values 0 (no I/O operation at the moment)
and 1 (an ongoing I/O). It generates user I/O signals U=
{u(t)1...u(t)N}for each user-mode thread obtained from the
program trace, and kernel I/O signals B={b(t)1...b(t)M}
for each kernel-mode thread from the blktrace log.
Figure 5 depicts the cross-correlation between signals u(t)
andb(t). The cross-correlation signal ( u(t)⋆b(t))[t] reaches
its maximum value at the point ∆ t= 324, which means that
the user signal u(t) is shifted forwards by ∆ t= 324 ms with
relation to the kernel signal b(t).
The dynamic analyzer matches user to the kernel I/O sig-
nals using a greedy iterative procedure. For each pair of
signals/an}bracketle{tu(t)i∈U,b(t)j∈B/an}bracketri}htthe analyzer computes a
cross-correlation signal xcorrij=b(t)i⋆u(t)jand the value
∆tij= argmax t(xcorrij). The user signal u(t)imatches
the kernel signal b(t)jif the maximum value of the cross-
12Figure 5: Cross-correlation between I/O signals
c
orrelation signal xcorrij[∆tij] is the highest across the sig-
nal pairs.
Next the analyzer aligns user and kernel-mode traces by
subtracting the ∆ tfrom the timestamps of the user-mode
trace. Finally, the kernel-mode I/O operations are asso-
ciated with the user-mode states. Each kernel mode I/O
operation diojis described as a time interval [ tb
start,tb
end]
between its start/end timestamps. Similarly, invocations
of the user mode I/O CFs are described as time intervals
[tu
start,tu
end]. The kernel-mode I/O operation diojis consid-
ered to be caused by the user-mode I/O CF if the amount
of intersection between their corresponding time intervals is
maximal across all the I/O CFs in the trace. Correspond-
ingly, a sequence dioj...dioj+kof low-level I/O operations
associated with the execution of the user-mode CF are con-
sidered to be parameters /an}bracketle{tdio1···diok/an}bracketri}ht ∈PΠ
diskof that CF.
A user-mode I/O CFs that does not intersect any kernel-
mode I/O operation is considered as a cache hit ( k= 0).
4.4 Constructing the performance model
The result of the program analysis is a set of text and xml
ﬁles, which contain all the information required to generate
the model: the list of threads, thread pools, and queues
in the high-level model; the set Sof CFs, their classes and
propertiesΠ; transitionprobabilities δ; thesetoflocks Land
their properties Π lock. This information is used to generate
thethree-tierperformancemodels describedintheSection3.
The models are implemented using the OMNeT simulation
toolset [2] and can reviewed in the OMNeT IDE.
To start using the model the analyst must specify the
model’s conﬁguration parameters (the numbers of threads
in the thread pools, intensity of the workload, sizes of the
queues, the numbers of CPU cores etc). The analyst must
also specify what performance data shouldbe collected. The
modelcanprovideperformancedataforCFs (executiontime
τ), for a group of CFs (e.g. a processing time of the task
by the thread), or for the whole program (e.g. throughput
or a response time). These are the only manual actions
performed during the model construction.
5. MODEL VERIFICATION
We implemented our approach as a tool for automatically
building models of Java programs. The tool uses ASM [3]
framework for bytecode analysis and instrumentation.
We used our tool to automatically build performance mo-
dels of large industrial programs, which demonstrates the
practicality of our approach. We also built models of various
small- to medium-size programs, demonstrating our ability
to model diﬀerent types of multithreaded applications.We estimated the accuracy of our predictions by build-
ing the model of each program from one conﬁguration and
using it to predict performance in a set of other conﬁgura-
tions. Then we measured actual performance of the non-
instrumented program in same conﬁgurations. To get reli-
able measurements we performed three runs of both the ac-
tual program and its model in each conﬁguration. The mean
values of measured and predicted performance metrics were
used to calculated the relative error εof the model:
ε=|measured −predicted |
measured(
2)
We conducted our experiments on a PC equipped with
the 2.4 GHz Intel quad-core CPU, 8GB RAM, and 250 Gb
HDD running Ubuntu Linux. To uncover potential artifacts
in the performance of the test programs our conﬁgurations
cover a variety of program’s behaviors, ranging from under-
utilization of resources (e.g. when the numberof active thre-
ads is less than CPU cores) to their over-utilization. Below
we describe our simulations in detail.
5.1 Modeling large industrial applications
Webuiltperformancemodelsoftwoindustrialopensource
Java programs: Sunﬂow 0.07 3D renderer and Apache Tom-
cat 7.0 web server. We predicted the performance of Tomcat
in two setups: as a standalone web server hosting static web
pages and as a servlet container. Considering diﬀerence in
Tomcat functionality over these setups, corresponding mo-
dels are signiﬁcantly diﬀerent. Table 2 provides information
on programs and their models.
Instrumentationdidnotaltersemanticsoftheseprograms,
but it introduced some overhead. The amount of overhead,
measured as a relative increase in the task processing time
by an instrumented program, constituted 2.5%-7.6%.
The complexity reduction algorithm eliminated 99% to
99.5%ofallCFsasinsigniﬁcantinTomcatandTomcat+iText
models correspondingly. Most of insigniﬁcant CFs were de-
tected during the startup or shutdown stages. No startup
or shutdown stages were detected in the Sunﬂow, and only
80% of its CFs were eliminated as insigniﬁcant.
Our models run 8-1000 times faster than the actual pro-
gram (see Table 2). The actual speedup depends not on the
size of a program, but on a ratio between the times required
to simulate CFs by the model and times required to exe-
cute these CFs by the program. Simulating a CF requires
a (roughly) constant amount of computations, regardless of
its execution time. Thus models that invoke many CFs with
short execution times or simulate intense locking operations
tendto runslower. Asa result, eliminating insigniﬁcant CFs
is essential for high performance of the model.
Using performance models oﬀers two additional sources of
speedup over benchmarking. First, multiple instances of a
model canrunsimultaneously onamulticore computer. Sec-
ond, the model does not require a time-consuming process
of setting up the live system for experimentation.
Sunﬂow 3D renderer. Sunﬂow uses a ray tracing al-
gorithm for image rendering [4]. The main thread splits
the frame into multiple tiles and stores them in the queue.
The pool of working threads reads tile coordinates from the
queue, renders the image tiles, and synthesizes the resulting
image. Given the constant size of the image, the number of
working threads and the number of CPU cores are two main
factors that determine the performance of the Sunﬂow. The
13Table 2: Large programs and their models
Tomcat Tomcat+iText Sunﬂow
(web server) (servlet container)
Program size (LOC) 182810 283143 21987
Number of probes 3178 3926 380
Mean instrumenta-
tion overhead 7.3% 2.4% 5.7%
Number of CFs 11206 9993 209
Total number of
nodes in the model 82 49 42
Simulation
speedup 8-26 37-110 1050
Figure 6: Predicted and measured performance of
S
unﬂow. Good accuracy for conﬁgurations involving
under- and over-utilization of resources
time required to render the image is the main performance
metric.
WepredictedSunﬂowperformancewith1,2,3,4,5,6,8,11,12,
and 16 working threads and with 1,2,3 and 4 active CPU
cores. Figure 6 compares predicted and measured render-
ing times in each of these conﬁgurations. The relative error
varies in ε∈(0.003,0.097) with the average error across all
the conﬁgurations ε= 0.032.
Our experiments demonstrate the ability of our frame-
work to predict performance of a program across diﬀerent
hardware conﬁgurations. This does not yet translate into
an accurate prediction of the program running on a totally
diﬀerent hardware. Diﬀerences in characteristics of CPU,
memory, and cache will result in diﬀerent execution times
for individual CFs. Nevertheless, it opens a path for such a
prediction because CF timing can be estimated analytically
or using microbenchmarks on the target architecture.
Apache Tomcat as a web server. Apache Tomcat is
a widely used web server and Java servlet container. In our
experiments Tomcat relies on a single blocking queueto pass
incoming HTTP requests to a ﬁxed-size thread pool. The
performance of the Tomcat was inﬂuenced by the size of the
thread pool and by the the workload intensity (the num-
ber of requests the server receives in a second, req/s). The
performance metrics are response time Rand throughput T.We used Tomcat to host about 600000 Wikipedia web
pages. Wepredictedperformance of Tomcat with with 1,3,5,
and 8 working threads and workload intensity ranging from
48.3 to 156.2 req/s (measured on the server side).
The prediction results for RandTare depicted at the
Figure 7. The relative prediction error ε(T)∈(0.001,0.087)
with average error ε(T) = 0 .0121. In non-saturated conﬁgu-
rations throughput is roughly equal to the incoming request
rate, thus the relative error for saturated conﬁgurations is a
more informative accuracy metric: ε(Tsat) = 0.027.
The error for Risε(R)∈(0.003,2.452) and ε(R) = 0 .269.
The relatively high error terms are attributedto ﬂuctuations
of the page cache hit rate represented by k. According to
our measurements, mean k= 0.755 with standard devia-
tionσ(k) = 0.046. Overall, precise data collection proved
to be essential for accurate performance prediction. Intro-
ducing an artiﬁcial 15% bias in the value of kresulted in
ε(R)∈(0.015,3.109) with ε(R) = 0 .882. This experiment
demonstrates the importance of the accurate measurement
of CF resource demands.
In a web server setup Tomcat expresses a mixed behavior.
81% of computational resources consumed during processing
the HTTP request is the I/O bandwidth, and 19% is CPU
time. As a result, the single hard drive becomes the bottle-
neck that prevents performance from growing signiﬁcantly
as the number of working thread increases. At the same
time, remaining CPU computations are parallelized across
four CPU cores, resulting in small but noticeable perfor-
mance improvement.
Apache Tomcat as a servlet container. Tomcat is
morefrequentlyusedasaservletcontainer. WeusedTomcat
to host a web application that reads a random passage from
the King James bible, formats it, and converts into the PDF
using the iText [5] library.
The prediction results for Ris depicted at the Figure 7
(Tis not shown due to space limitations). The relative pre-
diction error ε(R)∈(0.000,0.375) with the average error
ε(R) = 0 .122. The error for T ε(T)∈(0.000,0.356) and
ε(T) = 0 .053, with ε(Tsat) = 0.099. The CPU time τCPU
ﬂuctuates less than the demand for I/O bandwidth, which
leads to the lower prediction error.
The model correctly predicts the workload intensity at
which the server saturates. PDF conversion is a CPU-heavy
task, thus performance of the server is bounded by the num-
ber and performance of CPU cores. Since there are four
CPU cores available, the actual saturation point depends
on the number of threads. It ranges from 21.4 req/sec for a
conﬁguration with 1 thread to 85.5 req/sec for 10 threads.
5.2 Modeling small- to medium-size programs
We built models of the following applications: Monte-
carlo (a ﬁnancial application), Moldyn and Galaxy (scien-
tiﬁc computing applications), and Tornado (a Web server).
Although smaller in size, these programs express function-
alities peculiar to a wide range of multithreaded programs.
They implement thread interaction in diﬀerent ways and use
a great variety of synchronization mechanisms to enforce a
correct order ofcomputationsacross multiplethreads. Table
3 present a summary on these programs and their models.
Montecarlo and Moldyn are parts of the Java Grande
benchmark [12] suite. Montecarlo simulates price of market
derivatives by generating a number of time series reﬂecting
14Figure 7: Predicted and measured performance of Tomcat. Left : response time in a web server setup. Small
variation in demand for I/O bandwidth lead to large changes in the response time. Center: throughput in a
web server setup. Conﬁgurations leading to server saturation are detected accurately. Right: response time
in a servlet container setup. Consistent demand for the CPU time leads to an accurate prediction.
Table 3: Small- to medium-size programs and their
models
Montecarlo Moldyn Galaxy Tornado
Size, LOC 3207 1006 2480 1705
Number of probes 18 30 72 40
Number of CFs 17 72 124 88
Number of nodes
in the model 24 46 59 36
Mean error ε 0.062 0.083 0.075 0.262 (R)
0.010 (T)
p
rices of the underlying assets. Time series are generated
independently using a pool of working threads.
Moldyn simulates motion of argon atoms in a cubic vol-
ume. The time is discretized into small steps. During each
step (iteration) working threads compute forces acting on
atoms, andthenthemain threadupdatespositions ofatoms.
Galaxy simulates the gravitational interaction of celestial
bodies using theBarnes-Hut[8] algorithm. During each iter-
ation the main thread rebuilds the octree, the pool of“force
threads” computes forces and updates positions of bodies,
and the pool of“collision threads”detects body collisions.
Tornado is a simple web server, whose structure and be-
havior are described as an example in the Section 3. Unlike
Moldyn, Montecarlo, and Galaxy, which engage the CPU-
intense computations, Tornado workload is dominated by
disk I/O operations.
One conﬁguration parameter common to all these pro-
grams was the size of their thread pools. For Montecarlo,
Moldyn, andGalaxyweexperimentedwith1,2,3,4,8,10,12,16
working threads (Galaxy parameters included the number
of both force threads and collision threads). Parameters of
Tornado were the number of working threads (1,3,5, and 10)
and the workload intensity (ranged from 19.8 to 99.6 req/s).
The relative prediction error for each program is provided
in the Table 3 (detailed results are not shown due to space
constraints).6. DISCUSSION AND LIMITATIONS
Although our framework is capable of building perfor-
mance models automatically, it imposes certain limitations
on the programs we can model. First, our high-level models
represent computations as task processing. This approach
does not cover all possible programs, but covers most pro-
grams of interest for performance purposes.
Second, during data collection we use a single representa-
tive conﬁguration, where the transition probabilities δand
CF parameters Π would be similar to δand Π of a larger set
of conﬁgurations. This requires the usage patterns for the
program, such as the image resolution in Sunﬂow or proba-
bilities of accessing individual web pages in Tomcat, to re-
main similar across the conﬁguration space. Changing usage
patterns may require reconstructing the model. One solu-
tion to this problem would be recollecting δand Π directly
from the running program. Another solution is building a
hybrid of statistical and simulation model, where usage pat-
terns are described using metrics X′, and the dependency
(δ,Π) =f(X′) is approximated statistically.
Third, our current framework requires programs to imple-
ment multithreading using the well-deﬁned synchronization
operations. We donot see it as a major limitation as modern
programming frameworks oﬀer rich libraries of locks which
programmers are encouraged to use [6]. Furthermore, the
semantics of locks implemented using low-level constructs
can be discovered using analysis described in [32].
Third, our models do not explicitly simulate calls made by
the program to other systems, such as Web services or SQL
databases. Timing of these calls can be simulated using sta-
tistical models. Alternatively, these systems can be modeled
using their own performance models, combined into a model
of a distributed system using INET or NS2/3 simulators.
Next, our models simulate memory operations as CPU
computations. This didn’t aﬀect prediction accuracy in our
experiments, but accurate modeling of certain programs or
workloads may require explicit simulation of memory oper-
ations and corresponding OS and hardware components.
15Finally, our framework currently does not include a net-
w
ork model, our static analysis and instrumentation meth-
ods are only implemented for Java applications, and we use
clock time as a substitute for actual CPU time.
7. RELATED WORK
We divide the related work into two categories: (i) perfor-
mance modeling and (ii) automated program analysis and
model construction.
(i) At the high level the performance of the system can be
represented as a function y=f(/vector x), where /vector xis the conﬁgu-
ration and yis the performance of the system.
Analytic models explicitly represent this dependency us-
ing aset ofequations. Ananalyticmodel was usedtopredict
the performance of the DBMS buﬀer pool [28]; the reported
relative errors are ε(T)≤0.1 andε(R)∈(0.33...0.68). Ana-
lytic models were employed to study performance of certain
multithreaded design patterns [37], and as a central element
of the autonomic data center [10].
Building analytic models requires strong mathematical
skills and is hard to automate. Moreover, analytically mod-
eling even a simple multithreaded system is challenging [25].
Statistical models do not explicitly formulate the function
y=f(/vector x). Instead, the system is executed in a number of
conﬁgurations /vector x1,...,/vector xn∈X, where performance measure-
mentsy1,...,yn∈Yare collected. Then some statistical
method is used to approximate the dependency Y=f(X).
Statistical models were used to predict performance of
Hadoop tasks [17], SQL queries [14], and scientiﬁc applica-
tionsrunningonagrid[24] withrelativeerror ε∈(0.01,...,0.25).
CART trees predict performance of the SSD disk with ε∈
(0.17,...,0.25) [19] and the traditional hard drive with ε∈
(0.17,...,0.38)[40]. However, collectingthetrainingset( X,Y)
requires running the system in many diﬀerent conﬁgura-
tions, which is overly time-consuming and costly. The num-
ber of executions can be somewhat reduced by optimizing
the search through the conﬁguration space [41] or by com-
plex program analysis [13]. Still, such experimentation may
not be feasible on a production system.
Queuing networks, Petri nets, and their extensions can
model complex behavior, but their construction requires ex-
tensive information about the system.
The Layered Queuing Network (LQN) represents the sys-
tem as a hierarchy of layers, where each layer includes both
a queueand a service node. LQNs can be solved analytically
and are particularly useful for simulation of distributed sys-
tems, where their accuracy reaches ε≤0.24 [42],[34]. How-
ever, analytic modeling of complex threading behavior with
LQN [16] may be challenging. Palladio Component Models
(PCM) is another approach to simulation where the system
is divided into a number of interconnected components [9].
Colored Petri nets(CPN)extendthetraditional Petrinets
by allowing multiple types of tokens. In [30] CPN predicted
performance of a parallel ﬁle system with ε∈(0.2...0.4),
and in [35] CPN was used to simulating the complex locking
constructs in a program with ε∈(0.0...0.2).
Existing models such as CPN can simulate multithreaded
systems to some extent. But typically they model a single
aspect of the system’s behavior, such as locks, an OS com-
ponent, or a piece of a hardware. This limits applicability of
these models to a speciﬁc scenario. Instead, our models sim-
ulate many factors that inﬂuence performance of a system:
queuing, synchronization operations, and simultaneous us-age of hardware by threads. This allows us to build accurate
models of various multithreaded programs and workloads.
(ii) There is a signiﬁcant amount of work on automated
analysis of parallel an distributed programs. The THOR
tool combines kernel and user-mode instrumentation to un-
derstand and visualize relations between the Java threads
and locks [39]. In [32] the dynamic analysis was used to
understand and visualize locks in a multithreaded program.
The ETE framework uncovers the task ﬂow through the dis-
tributed system using the correlation variables [18]. The
Magpie performs the same task for a parallel program by
tracking invocations of key API functions [7].
Program analysis techniques are also used to automati-
callyconstructperformancemodels. LQNmodelsofmessage-
passing programs were automatically built by recovering
the request ﬂow from the application trace [26]. A PCM
model of the distributed EJB application was constructed
using program instrumentation; it demonstrated accuracy
ε∈(0.1...0.2) for the CPU-bound workload [11]. The PACE
framework [31] uses static analysis to automatically build
performance models of MPI applications with ε <= 0.1
[21]. In [43] authors construct models of distributed sci-
entiﬁc computing applications with ε∈(0.02...0.3).
However, automated construction of performance models
is mostly limited to distributed and message passing pro-
grams. Accuracy of these models can decrease rapidly if the
program performs complex threading operations. For ex-
ample, the accuracy of the model [43] drops to ε= 0.5 for
programs engaged in synchronization operations.
We address this limitation by developing innovative static
and dynamic analyses for building performance models of
multithreaded programs. Our analyses automatically dis-
cover resource demands and thread interactions and trans-
late this information into an accurate model of the system.
The great varietyin the typesof performance models, pro-
grams, and workloads makes it diﬃcult to establish a com-
mon baseline to compare accuracy of performance models.
Thus we compare our approach to a wider group of models,
including analytical and statistical models, and models of
distributed systems. These models have ε∈(0.02,...,0.15)
for CPU-boundworkloads [11],[24],[34],[42],[21],[43], and ε∈
(0.12,...,0.34)forI/O-boundworkloads[40],[14],[19],[28],[29].
Our models have accuracy ε∈(0.032,...,0.122) for CPU-
bound and ε∈(0.262,...,0.269) for disk I/O-bound work-
loads, which is comparable to the state of the art.
8. SUMMARY
In this paper we presented a novel methodology for au-
tomatic modeling of complex multithreaded programs. Our
models accurately simulate synchronization operations and
hardware usage by multiple threads. At the same time, our
framework builds program models automatically and does
not require running the program in many conﬁguration.
We veriﬁed our approach by building models of various
Java applications, including large industrial programs. Our
models predicted performance of these programs across a
range of conﬁgurations with a reasonable degree of accuracy.
Our next steps will be addressing limitations of our mo-
dels discussed in Section 6, which will allow predicting per-
formance for a wider range of applications and workloads.
Acknowledgements. We thank Dr. Eno Thereska for
his insightful comments on the paper. This work is sup-
portedbytheNationalScienceFoundationgrantCCF1130822.
169. REFERENCES
[
1]http://linux.die.net/man/8/btrace .
[2]http://www.omnetpp.org/ .
[3]http://asm.ow2.org/ .
[4]http://sunflow.sourceforge.net/ .
[5]http://itextpdf.com/ .
[6]http://docs.oracle.com/javase/6/docs/
technotes/guides/concurrency/overview.html .
[7] P. Barham, A. Donnelly, R. Isaacs, and R. Mortier.
Using magpie for request extraction and workload
modelling. In Proc. of the Symposium on Opearting
Systems Design & Implementation , pages 18–18,
Berkeley, CA, USA, 2004. USENIX Association.
[8] J. Barnes and P. Hut. A hierarchical o(n log n)
force-calculation algorithm. Nature, 324:446–449, 1986.
[9] S. Becker, H. Koziolek, and R. Reussner. Model-based
performance prediction with the palladio component
model. In Proc. of the 6th international workshop on
Software and performance , WOSP ’07, pages 54–65,
New York, NY, USA, 2007. ACM.
[10] M. Bennani and D. Menasce. Resource allocation for
autonomic data centers using analytic performance
models. In Proc. of International Conference on
Automatic Computing , pages 229–240, Washington,
DC, USA, 2005. IEEE.
[11] F. Brosig, N. Huber, and S. Kounev. Automated
extraction of architecture-level performance models of
distributed component-based systems. In Proc.
International Conference on Automated Software
Engineering , ASE ’11, pages 183–192, Washington,
DC, USA, 2011. IEEE.
[12] J. M. Bull, L. A. Smith, M. D. Westhead, D. S. Henty,
and R. A. Davey. A benchmark suite for high
performance java. In Proc. OF ACM Java Grande
Conference , pages 81–88. ACM, 1999.
[13] B.-G. Chun, L. Huang, S. Lee, P. Maniatis, and
M. Naik. Mantis: Predicting system performance
through program analysis and modeling. CoRR,
abs/1010.0019, 2010.
[14] J. Duggan, U. Cetintemel, O. Papaemmanouil, and
E. Upfal. Performance prediction for concurrent
database workloads. In Proc. of the 2011 ACM
SIGMOD International Conference on Management of
data, SIGMOD ’11, pages 337–348, New York, NY,
USA, 2011. ACM.
[15] W. Feng and Y. Zhang. A birth-death model for web
cache systems: Numerical solutions and simulation. In
Proc. of International Conference on Hybrid Systems
and Applications , pages 272–284, 2008.
[16] G. Franks and M. Woodside. Performance of
multi-level client-server systems with parallel service
operations. In Proc. of the 1st International Workshop
on Software and Performance , WOSP ’98, pages
120–130, New York, NY, USA, 1998. ACM.
[17] A. Ganapathi, Y. Chen, A. Fox, R. Katz, and
D. Patterson. Statistics-driven workload modeling for
the cloud. In Proc. of International Conference on
Data Engineering Workshops , pages 87–92, 2010.
[18] J. L. Hellerstein, M. M. Maccabee, W. N. M. III, and
J. Turek. Ete: A customizable approach to measuring
end-to-end response times and their components indistributed systems. In ICDCS, pages 152–162. IEEE,
1999.
[19] H. Huang, S. Li, A. Szalay, and A. Terzis. Performance
modeling and analysis of ﬂash-based storage devices.
InSymposium on Mass Storage Systems and
Technologies (MSST), , pages 1 –11, may 2011.
[20] T. A. Israr, D. H. Lau, G. Franks, and M. Woodside.
Automatic generation of layered queuing software
performance models from commonly available traces.
InProc. of International Workshop on Software and
Performance , WOSP ’05, pages 147–158, New York,
NY, USA, 2005. ACM.
[21] S. A. Jarvis, B. P. Foley, P. J. Isitt, D. P. Spooner,
D. Rueckert, and G. R. Nudd. Performance prediction
for a code with data-dependent runtimes. Concurr.
Comput. : Pract. Exper. , 20:195–206, March 2008.
[22] T. Kelly, T. Kelly, I. Cohen, I. Cohen, M. Goldszmidt,
M. Goldszmidt, K. Keeton, and K. Keeton. Inducing
models of black-box storage arrays. Technical report,
HP Laboratories Palo Alto, 2004.
[23] A. M. Law and W. D. Kelton. Simulation Modeling
and Analysis . McGraw-Hill Higher Education, 2nd
edition, 1997.
[24] B. C. Lee, D. M. Brooks, B. R. de Supinski,
M. Schulz, K. Singh, and S. A. McKee. Methods of
inference and learning for performance modeling of
parallel applications. In Proc. of SIGPLAN
symposium on Principles and practice of parallel
programming , PPoPP ’07, pages 249–258, New York,
NY, USA, 2007. ACM.
[25] D. A. Menasce and M. N. Bennani. Analytic
performance models for single class and multiple class
multithreaded software servers. In Int. CMG
Conference , 2006.
[26] A. Mizan and G. Franks. An automatic trace based
performance evaluation model building for parallel
distributed systems. SIGSOFT Softw. Eng. Notes ,
36(5):61–72, Sept. 2011.
[27] T. Moseley, D. A. Connors, D. Grunwald, and R. Peri.
Identifying potential parallelism via loop-centric
proﬁling. In Proc. of the 4th international conference
on Computing frontiers , CF ’07, pages 143–152, New
York, NY, USA, 2007. ACM.
[28] D. Narayanan, E. Thereska, and A. Ailamaki.
Continuous resource monitoring for self-predicting
dbms. In Proc. of International Symposium on
Modeling, Analysis, and Simulation of Computer and
Telecommunication Systems , pages 239–248,
Washington, DC, USA, 2005. IEEE.
[29] H. Q. Nguyen and A. Apon. Hierarchical performance
measurement and modeling of the linux ﬁle system. In
Proc. of International Conference on Performance
Engineering , ICPE ’11, pages 73–84, New York, NY,
USA, 2011. ACM.
[30] H. Q. Nguyen and A. Apon. Parallel ﬁle system
measurement and modeling using colored petri nets.
InProc. of International Conference on Performance
Engineering , ICPE ’12, pages 229–240, New York, NY,
USA, 2012. ACM.
[31] G. R. Nudd, D. J. Kerbyson, E. Papaefstathiou, S. C.
Perry, J. S. Harper, and D. V. Wilcox. Pace–a toolset
for the performance prediction of parallel and
17distributed systems. I nt. J. High Perform. Comput.
Appl., 14:228–251, August 2000.
[32] S. Reiss and A. Tarvo. Automatic categorization and
visualization of lock behavior. In Proc. of the ﬁrst
IEEE Working Conference on Software Visualization ,
VISSOFT ’13. IEEE, 2013.
[33] S. P. Reiss and M. Renieris. Encoding program
executions. In Proc. of the 23rd International
Conference on Software Engineering , ICSE ’01, pages
221–230, Washington, DC, USA, 2001. IEEE.
[34] J. Rolia, G. Casale, D. Krishnamurthy, S. Dawson,
and S. Kraft. Predictive modelling of sap erp
applications: Challenges and solutions. In Proc. of the
Fourth International ICST Conference on
Performance Evaluation Methodologies and Tools ,
VALUETOOLS ’09, pages 9:1–9:9, ICST, Brussels,
Belgium, Belgium, 2009. ICST.
[35] N. Roy, A. Dabholkar, N. Hamm, L. W. Dowdy, and
D. C. Schmidt. Modeling software contention using
colored petri nets. In E. L. Miller and C. L.
Williamson, editors, MASCOTS , pages 317–324.
IEEE, 2008.
[36] J. Y. Stein. Digital Signal Processing: A Computer
Science Perspective . John Wiley & Sons, Inc., New
York, NY, USA, 2000.
[37] R. Strebelow, M. Tribastone, and C. Prehofer.
Performance modeling of design patterns for
distributed computation. In International Symposium
on Modeling, Analysis, and Simulation of Computer
and Telecommunication Systems , MASCOTS ’12,
pages 251–258, Washington, DC, USA, 2012. IEEE.[38] A. Tarvo and S. P. Reiss. Using computer simulation
to predict the performance of multithreaded programs.
InProc. of the third joint WOSP/SIPEW
international conference on Performance Engineering ,
ICPE ’12, pages 217–228, New York, NY, USA, 2012.
ACM.
[39] Q. M. Teng, H. C. Wang, Z. Xiao, P. F. Sweeney, and
E. Duesterwald. Thor: A performance analysis tool for
java applications running on multicore systems. IBM
J. Res. Dev. , 54(5):456–472, Sept. 2010.
[40] M. Wang, K. Au, A. Ailamaki, A. Brockwell,
C. Faloutsos, and G. R. Ganger. Storage device
performance prediction with cart models. In Proc. of
International Symposium on Modeling, Analysis, and
Simulation of Computer and Telecommunications
Systems, MASCOTS ’04, pages 588–595, Washington,
DC, USA, 2004. IEEE.
[41] D. Westermann, J. Happe, R. Krebs, and
R. Farahbod. Automated inference of goal-oriented
performance prediction functions. In Proceedings of
the 27th IEEE/ACM International Conference on
Automated Software Engineering , ASE 2012, pages
190–199, New York, NY, USA, 2012. ACM.
[42] J. Xu, A. Ouﬁmtsev, M. Woodside, and L. Murphy.
Performance modeling and prediction of enterprise
javabeans with layered queuing network templates. In
Proc. of Conference on Speciﬁcation and Veriﬁcation
of Component-based Systems , SAVCBS ’05, New York,
NY, USA, 2005. ACM.
[43] Q. Xu and J. Subhlok. Construction and evaluation of
coordinated performance skeletons. In Proc. of
International Conference on High performance
computing , HiPC’08, pages 73–86, Berlin, Heidelberg,
2008. Springer-Verlag.
18