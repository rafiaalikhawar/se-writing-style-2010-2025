AskingandAnsweringQuestionsaboutUnfamiliar APIs:
AnExploratoryStudy
Ekwa Duala-Ekoko and Martin P. Robillard
School of Computer Science
McGill University
Montr´eal, QC, Canada
{ekwa, martin }@cs.mcgill.ca
Abstract—The increasing size of APIs and the increase in
the number of APIs available imply developers must fre-
quently learn how to use unfamiliar APIs. To identify the
types of questions developers want answered when working
with unfamiliar APIs and to understand the difﬁculty they
may encounter answering those questions, we conducted a
study involving twenty programmers working on different
programmingtasks,usingunfamiliarAPIs.Basedonthescre en
captured videos and the verbalization of the participants, we
identiﬁed twenty different types of questions programmers ask
when working with unfamiliar APIs, and provide new insights
to the cause of the difﬁculties programmers encounter when
answering questions about the use of APIs. The questions we
have identiﬁed and the difﬁculties we observed can be used
for evaluating tools aimed at improving API learning, and in
identifyingareas oftheAPIlearningprocesswheretoolsup port
is missing, or could be improved.
I. INTRODUCTION
Modern-day software development is inseparable from the
use of ApplicationProgrammingInterfaces(APIs).Softwar e
developers make use of APIs as interfaces to code libraries
or frameworks to help speed up the process of software
development and to improve the quality of the software.
Before leveraging the beneﬁts of an API, a developer
must discover and understand the behavior and relationship s
between the elements of an API relevant to their task.
Given the increase in the size of APIs and the increase in
the number of APIs developers have to work with, even
experienced developers must frequently learn newer parts
of familiar APIs, or newer APIs when working on new
tasks. Recently, researchers started investigating how de sign
choices common to several APIs affect the API learning
process. For instance, Ellis et al. observed that the Factor y
pattern hinders API learning [1], and a study by Stylos
et al. observed that method placement — for instance,
placing a “send” method on a convenience class such as
EmailTransport.send(EmailMessage) ,instead of hav-
ing it on the main-type such as EmailMessage.send()
— hinders API learning because convenience methods are
difﬁcult to discover when learning to use an API [2].
In this paper, we expand on the body of work on API
learning by investigating the different types of questionsdevelopers ask when working with unfamiliar APIs, in-
vestigating why some questions are difﬁcult to answer,
and researching the cause of the difﬁculty. Our study was
inspired by the work of Sillito et al., who looked at the
differenttypesof questionsdevelopersask when workingon
maintenance tasks [3]. To investigate those questions abou t
the use of APIs that are difﬁcult to answer, we conducted
a study in which twenty participants worked on two pro-
gramming tasks using different real-world APIs. The study
generated over twenty hours of screen captured videos and
the verbalization of the participants spanning 40 differen t
programming sessions. Our analysis of the data involved
generating generic versions of the questions asked by the
participants about the use of the APIs, abstracting each
question from the speciﬁcs of a given API, and identifying
those questions that proved difﬁcult for the participants t o
answer. Based on the results of our analysis, we isolated
twenty different types of questions the programmers asked
when learning to use APIs, and identiﬁed ﬁve of the twenty
questionsasthemostdifﬁcultfortheprogrammerstoanswer
in the context of our study. Drawing from varied sources
of evidence, such as the verbalizations and the navigation
paths of the participants, we explain why they found certain
questions hard to answer, and provide new insights to the
cause of the difﬁculties.
The different types of questions we have identiﬁed and
the difﬁculties we observed can be used for evaluating tools
aimed at improving API learning, and in identifying areas
of the API learning process where tool support is missing,
or could be improved. As an example, we identiﬁed some
areas where support is limited from existing tools includin g
the need for tools that would assist a developer in easily
identifying types that would serve as a good starting point
for searching for code examples, or for exploring the API
for a given programming task.
II. RELATEDWORK
API Usability Studies: Previous studies on API usability
sought to identify factors that hinder the usability of APIs
and to understand the trade-offs between design options.
Ellis et al. conducted a study to compare the usability ofthe Factory pattern in contrast to constructors for object
creation [1]: they observed that the participants experien ced
difﬁculty and required signiﬁcantly more time to construct
objects with a Factory than with a constructor. Stylos
et al. conducted a user study in which the usability of
parameterless constructors was compared to constructors
with parameters: they reported that programmers strongly
preferred, and were more effective with, APIs that provide
parameterless constructors [4]. In another study examinin g
the placement of methods (that is, the class to which a
methodbelongs),Styloset al. reportedthat participantsw ere
signiﬁcantly faster at identifying relevant dependencies and
combining objects when the methods of a starting class
referenced its dependencies [2]. Clarke uses the “Cognitiv e
Dimensions” [5], [6], a framework for describing API us-
ability problems, to identify speciﬁc usability issues wit h
Microsoft APIs, and to help inform the design of more
usable APIs. Other studies have looked at the role of web
resources in learning how to use APIs: in a lab study
involving twenty participants and ﬁve tasks, Brandt et al.
observed that programmersused the Web primarily for just-
in-time learning of new skills, and to clarify or remind
themselves of previously acquired knowledge [7]. Prior
studies have either focused on the usability of different AP I
design choices (e.g., Factory pattern versus constructors ),
or the usability issues of a speciﬁc API, or a learning
resource.Ourstudycomplementspreviouseffortsbylookin g
at the types of questions developers ask when working with
unfamiliar APIs, investigating the cause of the difﬁcultie s
they encounter answering these questions, and providing
suggestions on how tool support for learning how to use
APIs could be improved.
Information Needs of Programmers: Severalcontributions
have been made in the area of the information needs of
programmers in general. Ko et al. conducted a study in
which forty novice programmers were asked to complete
several tasks using Visual Basic .NET [8], and identiﬁed
learningbarriersandinformationneedsthatmustbesatisﬁ ed
for the programmers to compete the tasks. In a different
study, Ko et al. observed and transcribed the activities of
seventeen developers working on different tasks during a
ninety minutes session [9]. Ko et al. analyzed the transcrip ts
for the type of information that developers sought, the
sources they used, and the situations that prevented them
from acquiring information. They identiﬁed twenty one
different information needs of programmers, grouped into
seven categories: writing code, submitting a change, triag ing
bugs, reproducing a failure, understanding execution be-
havior, reasoning about design, and maintaining awareness .
They also observed that the most difﬁcult needs to satisfy
were questions about the rationale for design decisions, an d
that questions about APIs that could not be answered using
the documentationor tools, were answeredby consultingthecoworkers.Sillitoetal.identiﬁedfortyfourdifferentty pesof
questionsasked by programmerswhen maintainingsoftware
code, and investigated the degree to which existing tools
support the questions programmers ask when modifying the
sourcecode[3].Thecontributionsofourstudyare similart o
that of Sillito et al. that lookedat the questionsprogramme rs
asked when maintaining the source code, but ours is in the
context of API learning. In addition, we utilized a more
objectivecriterionfor determininghard-to-answerquest ions,
provide a catalog of qualitative evidence explaining why
developers ﬁnd certain questions hard to answer, and used
more varied sources of evidence (such as navigation paths,
verbalizations, and the time spent on various micro tasks) i n
our analysis than either Sillito et al., or Ko et al.
III. PROGRAMMING STUDY
A. Participants
We recruited participants from the student population of th e
department of Computer Science at McGill University using
on-campus posters and mailing lists, and promised a mon-
etary compensation of $20. Respondents were prescreened
using a questionnaire about their programming experience
and their knowledge of Java and Eclipse.
We selected 20 participants from the respondents for
our study. The participants reported a minimum of 1 year
programming experience with Java, 1 year experience with
the Java API documentation (i.e., Javadoc), and some expe-
rience programming with Eclipse. Our participants reporte d
between1and6yearsofexperienceprogrammingwithJava,
with a median of 3.5 years, and an average of 1.5 years of
paid programming experience. Five of the 20 participants
were female; our participant pool included 4 Ph.D. students ,
11 M.Sc. students, and 5 senior undergraduate students.
Although all of our participants were students, they are
representativeofthepopulationofinterestandtheirexpe rtise
level is comparable to that of recent graduates in software
development positions, which is our target population sinc e
our work aims to support novice programmers.
B. Tasks
We asked each participant to complete two programming
tasks: the ﬁrst task using the JFreeChart1API, and the sec-
ond task using the Java API for XML Processing (JAXP)2.
JFreeChart is a popular open-source API for generating
charts. We used version 1.0.13of the JFreeChart API, which
has 37 packages and 426 non-exception classes. JAXP is an
API for validating and parsing XML documents, developed
by Sun Micosystems. We used version 1.4 of the JAXP API,
which has 23 packages and 207 non-exception classes.
We selected tasks that involved combining multiple ob-
jects because previous work on API usability observed that
1www.jfree.org/jfreechart/
2http://jaxp.java.netdevelopers experienced the most difﬁculty performing such
tasks [2]. We reasoned that tasks requiring the combination
of multiple objects are more likely to reveal a variety of
questions developer want answered and typical challenges
they encounter when learning to use APIs. The participants
weregivenamaximumof35minutesperprogrammingtask.
All the 20 participants were unfamiliar with the APIs used
in the study.
Chart-Task.
We asked the participants to use the JFreeChart API to
construct a pie chart with three slices (45% Undergrads,
35% Master’s, and 20% Ph.D.s), and to save the chart to a
ﬁle in a graphic format. To complete this task, a participant
needed to construct and conﬁgure at least ﬁve API types
(JFreeChart ,PiePlot ,PieDataset ,ChartFactory ,
andChartUtilities ), and had to discover key relation-
ships between the types: for instance, the relationship be-
tweenJFreeChart , the type for representing charts, and
ChartUtilities , the type needed to save the chart.
XML-Task.
We asked the participants to use the JAXP API to verify
whether the structure of an XML ﬁle conforms to a given
XML schema. The participants were provided with both
an XML ﬁle and an XML schema ﬁle, and were asked
to implement a solution that returns true if the XML ﬁle
conforms to the given XML schema, and false otherwise.
This task requiredthe combinationof at least four API types
(Schema,Validator ,SchemaFacotry , andSource) and
was selected because of the unique challenges it presents
to object construction — all the required types are abstract
with no subtypes; the types must be created from factory or
public methods.
C. Study Setting
Participants completed the study using the Eclipse IDE
(version 3.4) and were permitted to use any of the features
of the IDE. Two main information sources were used in
the study: the documentation of the APIs and the Web,
which provides access to example usages of the APIs.
These information sources have been reported to be the
primary learning resources for API users [10], [11]. We
provided the participants with the Firefox browser to acces s
theseinformationsources,anddisabledthebrowser’shist ory
feature to prevent any learning effect between participant s.
The programming studies were conducted individually
in our research lab. The participants began each study
by watching a four-minute video tutorial about the think-
aloud protocol. Participants were then given time to practi ce
thinking aloud while working on a web search task. Soon
after, the participant was given the instructions for the
Chart-Task and was given a maximum of 5 minutes to go
over the task requirements and to ask questions relating to
the requirements. To avoid inﬂuencing the strategy of theparticipants, we did not identify the classes or packages of
the APIsrequiredto completethetasks, as wasthe case with
previous studies [2]. Also, the participants were advised t o
proceed as they would typically do when learning a new
API.
Once the participant was satisﬁed with the task require-
ment, we loaded an Eclipse project which contained a
class with an empty main method and the libraries of the
relevant API. We then showed the participant how to use
the Firefox browser to access the Javadoc pages of the
APIs from the bookmark menu. At this point, the screen
and voice recording software — Camtasia, version 4 —
was started, and the participant was asked to begin the
task. The participant was asked to move to the next task
upon completion of the Chart-Task, or once the 35 minutes
allocated for the task elapsed. The tasks were completed in
the same order by all the 20 participants.
D. Data Collection
We used three data collection techniques in our study: the
think-aloudprotocol,screencapturedvideos,andintervi ews.
In the think-aloud protocol [12], participants are asked to
verbalize their thought process while solving a given task.
Having participants think-aloud was particularly useful i n
our study as it permitted us to obtain an insight into the
participants’ understanding of the structure of the APIs,
to identify the types of questions participants ask when
learning to use APIs, and to understand why a participant
may have difﬁculty answering a given question. We also
conducted semi-structured post-study interviews in which
the participantswere asked to commentabout the challenges
experienced during the programming study. The interviews
lasted 5 minutes.
The screen contents, the verbalizationsof the participant s,
and the interview sessions were captured using Camtasia.
The study produced a total of 40 different programming
sessions and about 20 hours of screen-captured videos and
verbalizations of participants working with unfamiliar re al-
world APIs.
IV. DATAANALYSIS AND RESULTS
Our analysis focused on the questions the participants want
answered about the use of an API. Our goal was to identify
those questions that are difﬁcult to answer, to understand
why these questions proved difﬁcult to answer, and to
recommend programming tools that could facilitate the API
learningprocess.Ourmethodforanalyzingthedatainvolve d
three phases: identifying the different types of questions
asked by the participants, categorizing the questions, and
coding the exploration patterns used by the participants
when searching for answers to these questions. We refer
to a participant by their ID (for instance, P5 for the ﬁfth
participant) and to the tasks as T1 (for the Chart Task) and
T2 (for the XML Task).Table I
DIFFERENT TYPES OF QUESTIONS OBSERVED DURING THE PROGRAMMIN G STUDY
Generic questions, with speciﬁc examples in italics # of occurrences # of participants
Q.1Which packages or namespaces of an API provide types relevan t to my task? “I’m trying to ﬁnd out
which package has classes for creating a pie chart” — P5,T131 16
Q.2Is there an API type that provides a given functionality? “the task says I should create a pie chart;
I’m expecting some sort of a PieChart class to be available” — P18,T111 7
Q.3Does an API type provide a method for performing a given opera tion?“Is there a method on
BufferedImage that helps to save?” — P10,T158 19
Q.4What is the functionality of a given API type? “Let’s look at what the Validator class does”
— P18,T232 17
Q.5Can a method intended to perform operation A be used to perfor m operation B? “I’m hoping that the
draw method can be used to save to a ﬁle, but I’m not too optimis tic about it” — P6,T13 2
Q.6Which keywords best describe a functionality provided by th e API? “I’m going to use the Firefox
search to look if there’s any thing involving[containing th e word] “schema”” — P9,T238 13
Q.7How is the type X related to the type Y? “How isValidator related toSchema ?”— P18, T2 8 7
Q.8How do I get an object of type X from the type Y? “I need to ﬁgure out how to get a
BufferedImage from aPiePlot ”— P6,T12 1
Q.9Which elements of the API are of the type X? “Which classes of the API are Comparable ?”
— P11, T14 4
Q.10Is the object X of the type Y? “let’s see if RenderedImage takesBufferedImage ”—
P1,T12 2
Q.11Does the API provide a helper-type for manipulating objects of a given type? “let’s see if there
are classes related to BufferedImage which can give me the possibility to write the image to a ﬁle”
— P10,T119 13
Q.12How do I create an object of a given type without a public const ructor?“the constructor is protected;
so how do I create a Graphics2D object?” — P11,T157 19
Q.13Which other API elements are necessary to use a given API type ?“I think I need something else that
would save the chart to an image” — P1,T16 5
Q.14Which subtype of an interface or class is the most appropriat e for my task? “I don’t know exactly
which subtype of Source to use for reading an XML ﬁle” — P6,T229 17
Q.15Which types of a given domain (package, namespace) are relev ant to my task? “Which classes of the
“parsers” package could be used for validation?” — P18, T227 17
Q.16Which method from a list of overloaded methods is relevant to my task? “I’m trying to ﬁnd the
appropriate create-piechart method because it seems to be o verloaded” — P16,T14 4
Q.17What role do the arguments of a given method play in its usage? “we have a
newInstance(String) method that takes a String argument and I have no idea what this
String is suppose to be” — P9,T223 17
Q.18What is the valid range of values for a primitive argument, su ch as an integer, of a given method?
“I don’t know if this [double] value should be between 0 and 1” — P10,T13 3
Q.19Is NULL a valid value for a non-primitive argument of a given m ethod?“let’s use NULL for
Comparable and see if the method throws an exception” — P1,T14 4
Q.20How do I determine the outcome of a method call? “[the method]
Validator.validate(Source) returns void; how do I know the results of the validation?” —
P12,T215 13
A. Identiﬁcation of Questions
In this phase, we went through the screen-captured videos
and the verbalizations to produce a list of speciﬁcques-
tions asked by each participants, and to identify segments
of the videos, which we called episodes, corresponding
to each question. Each episode also captured the speciﬁc
approach used by a participant to answer a given ques-
tion. Some questions were explicit: for instance, partic-
ipant P11 asked “How do I create a Graphics2D ob-
ject?”while working on the Chart-tasks. Other questions
were easily inferred from the actions and verbalizations of
the participants: for instance, P1 came across the method
ImageIO.write(RenderedImage, ...) , and said “let’s
see ifRenderedImage takesBufferedImage ”, then
went ahead and used a BufferedImage object where
RenderedImage was expected. The actions and verbaliza-
tion of P1 in this example is phrased into the question:
“IsBufferedImage of the type RenderedImage ?”. Afteridentifying the list of speciﬁc questions for each particip ant,
we then developed genericversions of the questions that
slightly abstract from the speciﬁcs of a given API. For
instance, the question “IsBufferedImage of the type
RenderedImage ?”can be stated more generally as “Is the
object X of the type Y?” . Based on these generic questions,
weidentiﬁedtwentydifferenttypesofquestionsaskedbyth e
participants across both tasks (see Table I). We also provid e
a speciﬁc instance for each generic question as an example,
initalics. Thegenericquestionshighlight,toacertainex tent,
the type and scope of the information developers need
when learning how to use APIs. The number of times each
type of question was observed (# of occurrences) and the
number of participants that asked each type of question (#
of participants) are listed in Table I.
B. Abstraction of Developer Behavior
We needed a high-level abstraction of the actions of the
participants to facilitate the analysis of their behavior a ndthe challenges they experienced when learning to use APIs.
Since our analysis is centered around the questions asked
about the use of the APIs, we transcribed the segments
of the videos corresponding the time frame during which
a participant asked and searched for answers to a given
question. Speciﬁcally, for each participant and for each
episode corresponding to a speciﬁc question, we transcribe d
the video into a series of actions that summarizes the
steps taken by the participant to answer the question. We
considered the following actions:
•Browse: the participant looked through a list of API
elements (packages, types, or methods) either within
Eclipse, or in the documentation, before making a
selection. The Browse action has a targetto denote
the items (either packages, classes, methods, or search
result) the participant was browsing through.
•Select:the participant selected an item from a list of
API elements, or the results of a search query after
browsing. The Select action has a target — the name
of the item selected, and a ﬂag (Yes/No) to indicate if
the selected element was relevant to the question being
answered. The target of the Select action could also be
None, if no item was selected.
•Read:the participant focused on a portion of text or
code. The Read action has a target — the name of
the element being read, and a section (e.g., either the
introduction section, constructor section, or the method
description of an API element) to indicate the location
the participant focused on.
•Navigate: the participant followed a dependency or a
link to another element. The Navigate action has a
target— the name of the item navigated to, a ﬂag
(Yes/No) to indicate if the targetled to information
relevant to answering the question.
•Search: the participant performed a search of the
documentation or the Web. The Search actions has a
target(Documentation/Web), and a ﬂag (Yes/No) to
indicate if the search query contained the name of an
API element.
•Switch:the participant moved from the documentation
to the Web, or IDE, and vice versa.
•Use:the participant attempted to use an API element
or code example found on the Web. This action has a
target— the element or code the participant attempted
to use, and a ﬂag (Yes/No) to indicate if the participant
was successful.
•Backtrack: the participant stepped back to a previous
location of certainty, then decides to explore a different
path. This action has a target— the location the
participant backtracked to.As an example, Table II shows a partial transcript3of
the participant P15 looking for information on how to
save aBufferedImage ; the transcribed actions is shown
under the “Action Sequence” column. P15 started by nav-
igating to the documentation page of BufferedImage ,
browsed through its subtypes, and then selected the sub-
typeWritableImage , not relevant to saving an image.
P15 read the introduction section of WritableImage ,
then backtracked to BufferedImage . P15 then browsed
through the methods of BufferedImage , focusing on
thecreateGraphics method, before switching to the
Web. P15 then searched the Web with a query con-
taining an API element, selected the third results,
read through the code example and discovered the
ImageIO.write(RenderedImage, ...) method. P15
then used ImageIO.write(RenderedImage, ...) suc-
cessful to save the image to a ﬁle.
Table II
TRANSCRIPT EXCERPT FOR PARTICIPANT P15 — C HARTTASK
Time Question Action Sequence
0:18:10 How do I save a
BufferedImage?Nav[BufferedImage]:
Browse[subtypes]:
Select[WritableImage,No]:
Read[WritableImage,Intro]:
Backtrack [BufferedImage]:
Browse[methods]:
Read[createGraphics,description]:
Switch[web]:Search[web,Yes]:
Select[3rd,Yes]:
Read[ImageIO.write,code]:
Use[ImageIO.write,Yes]
What is a Difﬁculty?
As part of our analysis, we intended to identify questions
that proved difﬁcult for the participants to answer and to
understand the cause of the difﬁculty. To accomplish this,
we needed an objective measure as to what constitutes a dif-
ﬁculty in the context of API learning. We decided not to use
the amount of time taken to answer a question as the main
measureofdifﬁcultysincesigniﬁcantperformancevariati ons
have been observed amongst developers with similar level
of experience [13]. At a higher-level, we observed that
some of the actions, or sequence of actions, of a participant
that reﬂected a lack of progress in the search for answers
to a given question would serve as a good measure for
capturing difﬁculty. We used the following action sequence s
asadeﬁnitionofthedifﬁcultyparticipantsencounteredwh en
answering questions about the use of the APIs:
•Use[target, No]: This action sequence captures in-
stances in which a participant attempted to use an
API element but was unsuccessful because the API
does not support the given usage. For instance, the
participants P6 and P8 commented “How can I get an
3The entire transcripts for both tasks, and for all the partic ipants are
available as an online appendix: http://www.cs.mcgill.ca /∼eduala/apistudy/Table III
ASUMMARY OF THE DIFFICULTIES PARTICIPANTS EXPERIENCED
ANSWERING DIFFERENT TYPES OF QUESTIONS ABOUT THE USE OF
APIS.
Question
ID#times #instances #participants#difﬁculty
Q.1 31 1 16 1
Q.2 11 5 7 3
Q.3 58 14 19 13
Q.4 32 3 17 3
Q.5 3 1 2 1
Q.6 38 17 13 7
Q.7 8 5 7 5
Q.8 2 1 1 1
Q.9 4 2 4 2
Q.10 2 0 2 0
Q.11 19 17 13 12
Q.12 57 35 19 17
Q.13 6 1 5 1
Q.14 29 8 17 8
Q.15 27 5 17 4
Q.16 4 0 4 0
Q.17 23 1 17 1
Q.18 3 2 3 2
Q.19 4 1 4 1
Q.20 15 11 13 11
instance of Validator?” after their attempt to instantiate
Validator,anabstractclass,fromthedefaultconstructor
failed. This object instantiation difﬁculty is captured by
theactionsequence Use[Validator.Constructor,No] .We
observed that participants often had expectations about
thedesignofanAPIandexpressedfrustrationwhenthe
structure of an API did not match their expectations.
•Browse[list],Select[target, No], ..., then Back-
track[list], orNavigate[ target, No], ..., then Back-
track[...]: TheSelect[target, No]andNavigate[ target,
No]action sequences capture instances in which a
participant went down an irrelevant information search
path. Once a participant realized the information on a
givenpath was not relevant to answering their question,
they backtracked to previous location of certainty, and
then chose a different path to explore: captured by
theBacktrack action. We consider action sequence
Select[target, No], orNavigate[ target, No], followed
by aBacktrack as an indication of difﬁculty in search-
ing for answers to a given question. The participants
relied on cues in the documentation or code examples
when looking for answer to a given question. At times,
the clues were not available or perceivable. In the
absence of strong cues, participants were left to guess
which search paths to follow, and some participants
inadvertently went down irrelevant search paths.
We summarize the difﬁculties the participants experienced
answering the different types of questions in Table III. For
each question, we provide the number of times the question
was observed (#times), the number of instances with a difﬁ-
culty (#instances), the number of participants who posed th e
question (#participants),and the number of participants w hoexperienceda difﬁculty answering the question (#difﬁcult y).
As a baseline, we considered a question difﬁcult to answer
if all of the following conditions apply:
•At least half of the participants who posed a question
experienced some difﬁculty answering the question.
•At least ﬁve participants experienced some difﬁculty
answering the question.
•A difﬁcultywasobservedinabouthalfthe totalnumber
of the instances in which a question was asked.
For instance, we considered the participants to have
experienced difﬁculty answering question Q.20 since
eleven of the thirteen participants who posed the question
experienced difﬁculty answering it, and since a difﬁculty
was observed in eleven of the ﬁfteen instances in which
the question was asked. We identiﬁed ﬁve questions that
proved difﬁcult for the participants to answer (boldfaced i n
Table III):
Q.6Which keywords best describe a functionality provided
by the API?
Q.7How is the type X related to the type Y?
Q.11Does the API provide a helper-type for manipulating
objects of a given type?
Q.12How do I create an object of a given type without a
public constructor?
Q.20How do I determine the outcome of a method call?
C. Observations
We present our ﬁndings as observations of the challenges a
developermayencounterwhenlearningtouseanAPI, along
with the supporting evidence for each observation. These
observations are supported by the results of our analysis of
the data from the study, and by the verbalizations of the
participants.
Observation 1 (Discovering Relevant Dependencies). Dis-
covering relevant API types not accessible from the type
a developer is working with is a major challenge to API
learners.
Three questions (Q.7, Q.11, and Q.12) of the ﬁve we
identiﬁed as being difﬁcult to answer involved a partic-
ipant either looking for types related to, and relevant to
the use of a type they were working with ( “let’s see if
there are classes related to BufferedImage which can
give me the possibility to write the image to a ﬁle” —
P10, T1), or a participant seeking to discover the rela-
tionship between API types ( “How isValidator related
toSchema?”— P18, T2). Although different, these three
questions illustrate a common problem: the participants
experienced signiﬁcant difﬁculty when relevant API types
were not accessible from the type they were working
with (i.e., these helper-types were not referenced or reach -
able from any of the public members of the type theTable IV
ACOMPARISON OF THE SEARCH QUERIES WITH ,AND WITHOUT ,AN
APIELEMENT .
Search Queries With an API Element
Total Queries: 25
Reformulated Queries: 4
Successful Queries: 21
Search Queries Without an API Element
Total Queries: 13
Reformulated Queries: 12
Successful Queries: 1
developer was working with). For instance, in the Chart
Task, the participants could save the JFreeChart object
usingChartUtilities.saveChart(JFreeChart,...) ,
but most experienced difﬁculty locating ChartUtilities
because it is not accessible from JFreeChart . Twelve of
the 20 participants in our study experienced some difﬁculty
ﬁndingChartUtilities (Q.11, Table III), and three of
the participants were unable to complete the Chart-Task
because they could not locate this relevant dependency.Thi s
observation corroborates the ﬁndings of Stylos et al. [2]
that method placement — the class on which a method
is placed — affects API usability. However, Observation 1
extends beyond method placement: the participants also had
difﬁculty discovering the relationships between types (Q. 7,
Table III), or creating objects for types without a public
constructor (Q.12, Table III) because the relevant helper-
types were not accessible from the type they were working
with. Participant P4 attributed this difﬁculty to the lack o f
a“cross-reference in the API that says get a Validator
instance from a Schema”.
Observation 2 (Query Formulation). In the context of our
study, having an API element as one of the keywords in a
search query was an effective strategy for locating relevan t
code examples.
We analyzed the queries the participants formulated when
searching for code examples on the Web and observed that
queries that contain an API element were more successful
(thatis,ledtoarelevantcodeexample)thanthosewithouta n
API element (see Table IV). There were a total of 25 queries
containing an API element, and of those, only four were
reformulated, and 21 of the 25 queries containing an API
element led to a relevant code example. On the other hand,
there were a total of 13 queries without an API element: 12
of the 13 queries were reformulated, and only one of the
13 queries led to a relevant code example. As an example,
participant P18 started the XML-Task with the search query
“java xml processing tutorials” but found no relevant code
example. He then turned to the documentation where he
identiﬁed the Schema class as relevant to the validation
task. Participant P18 then reformulated the search queryto“java xml validation against schema” from which he
found a relevant code example. Our observationabout query
formulation corroborates the ﬁnding from the analysis of
the Koders’ search engine logs where queries with code
elements lead to the most downloads [14].
A complementary observation about query formulation is
the difﬁculty of guessing keywords that correspond to word
usage in APIs, or their documentation. This difﬁculty was
captured by the question Q.6 ( Which keywords best describe
a functionality provided by the API? ): up to seven of the
thirteen participants who asked this question experienced
some difﬁculty guessing a correct keyword.
void validate( File) throws FileNotValidException
ReturnStatus  validate( File) vs.(A) 
(B) 
Figure 1. Two possibilities for communicating method-leve l execution
failures.
Observation 3 (Exceptions). The use of an exception to
communicate the outcome of a method execution hinders
API comprehension.
There is a long standing debate in the software development
community regarding whether an exception (as in Figure 1
(A)), or a return-status-object4(as in Figure 1 (B)) should
be used to communicate method-level execution failures,
and when each design choice may be appropriate [15]–[17].
When an exception is used to communicate the failure of
thevalidate(File) method, the implication is that the
Fileis considered valid if the method does not throw
an exception. In other words, the exception is used to
communicate the return status ( failureorsuccess) of the
validate(File) method: the validation is said to have
failed if the method validate(File) throws an exception,
and successful otherwise. However, the extent to which this
implicationis apparentto a developerlearningto use an API
remains uncertain. In the XML tasks, the participants had to
usethemethod Validator.validate(Source) ,thatused
an exception to communicate outcome, to validate an XML
ﬁle. We observed that the use of an exception to communi-
cate outcome was problematic to the participants: 11 of the
20 participants experienced signiﬁcant difﬁculty realizi ng
the implicationthat if the method validate(Source) does
not throw an exception, then the Source ﬁle is consid-
ered valid (Q.20, Table III). The 20 participants spent an
average of 4.2 minutes each before becoming aware of
the implication that the XML ﬁle is considered valid if
4We used the word return-status-object to represent either an error code
(a primitive such as a boolean or an integer), or an object tha t contains the
return status of a method execution.the validation method does not throw an exception; the
average time spent to make this discovery increases to 6.7
minutes if we consider just the 11 participants that faced
a difﬁculty. Participants P5, P14, and P20 were unable to
make the discovery within the alloted time for the task,
even after spending 14 minutes, 9 minutes, and 21 minutes,
respectively, on this part of the task.
We looked at the verbalizations of the participants and
the post-study interviews in an attempt to understand why
they could have missed the implication that the XML ﬁle
is considered valid if the validation method does not throw
an exception. We identiﬁed two possible reasons for this
difﬁculty.
The Expectation of the Participants. The partici-
pants expected the API to provide a validation method
with a return-status-object (such as in Figure 1 (B)),
butvalidate(Source) had no return-status-object:
“validate(Source) does not give us something like true
or false; I better look for a method that gives us a boolean ”
— P1. Not ﬁnding the expected method (that is, a vali-
date method with a return-status-object), participants wo uld
initially assume that validate(Source) is not the right
method to use, instead of making the connection that an
exception is used to communicate the success or failure of
the validation.Theywouldthen spendtime lookingforother
methods in the API with a return-status-object that could be
used to validate the XML ﬁle. Not ﬁnding an alternative, the
participants would then return to the validate(Source)
method,re-readits documentation,and realize that the XML
ﬁle is considered valid if the validation method does not
throw an exception.
DisagreementbetweenAPIDesignersandParticipantsas
to what Constitute an “ error condition ”.The second rea-
son expressedbythe participantsas to whytheyexperienced
difﬁculty associating exceptionsto the success, or failur e, of
the validation relates a disagreement as to what constitute
an error condition. According to expert API designers: “ if
a member [method] cannot successfully do what it was
designed to do — what the member name implies — it
should be considered an execution failure, and an exception
should be thrown ” [15, p. 218]. In other words, an execution
failure is said to have occurred if the validate(Source)
method cannot validate the XML ﬁle, and an exception
shouldthereforethrown.Our participants,on the other han d,
seem to associate the throwing of an exception to something
catastrophic:
“if you’re just trying to validate something why would
it throw an exception; It doesn’t make sense; I expected it
[validate(Source) ] to return an object ” — P14.
“in my experience ... I ﬁnd consensus that you throw
exceptions only when you ﬁnd an error. In a Validator you
expect some thing to be valid or invalid. And if its invalidthat should be a common occurrence just as much as it is
valid. So throwing an exception for common occurrence is
not a good idea ” — P11.
This disagreement between API designers and our partic-
ipants reﬂects the debate as to when exceptions should
be used. Some argue that exceptions are for “exceptional
conditions”; others argue that “ exceptions should be used to
report all errors ” [15, p. 212]. The software development
community has yet to agree on what constitutes an excep-
tional condition. Our study indicates that this disagreeme nt
has the potential for inﬂuencing API comprehension.
Observation 4 (Web versus Documentation). The use of
the Web had no effect on the number of tasks successfully
completed or the time taken to complete a task.
In designing the study, we had ten of the participants use
the Web and the API documentation as learning resources
(the Web Group — WG), and the other ten using just
the documentation (the Documentation Group — DG). We
reasoned that partitioning the participants into two group s
would help us identify the challenges programmers faced
whenlookingforanswertothequestionsusingbothlearning
resources. We expected the participants in the Web Group
to be signiﬁcantly more successful since the Web provides
several code examplesfor both tasks. However,we observed
no signiﬁcant advantage, either in terms of the number
of tasks successfully completed or the average time taken
to complete a task, between the participants of the Web
Group over the participants of the Documentation Group.
Six participants from the Documentation Group and seven
participants from the Web Group successfully completed
task T1, and six participants from the DocumentationGroup
and ﬁve participants from the Web Group successfully
completed task T2. We obtained a chi-squared statistic of
0 when we compared the number of tasks successfully
completed between the two groups.
Looking at the task completion times, the participants
of the Documentation Group spent an average of 29 ( ±7)
minutes on task T1 while participants from the Web Group
spent an average of 25 ( ±9) minutes. We observed similar
results for task T2: participantsof the DocumentationGrou p
spent an average of 29 ( ±8) minutes while participants from
the Web Group spent an average of 26 ( ±8) minutes. We
used the Rank test to compare the task completion time
between the two groups and obtained a p-value of 0.45 for
task T1 and a p-value of 0.26 for task T2. But why were the
participants who used the Web not signiﬁcantly better than
those who used the API documentation?
We observed that some participants often underestimate
the time required to ﬁnd code examples on the Web, extract
therelevantcodesnippets,andtocustomizethesnippetsin to
the context of a task. Some participants spent a signiﬁcant
amountoftime extractingandcustomizingrelevantsnippet s.
For example, participant P13 found a code example for taskT2 at the 16 minutes mark, but was unable to complete
the task in the remaining 19 minutes because of difﬁculties
in extracting and customizing relevant code snippets. When
asked about this in the post-study interview, participant P 13
commented that “the example had a different context from
our task, so I had to translate their ideas to ours and that
takessometime” .OtherparticipantsstartedwiththeWebbut
soonrealizedthedifﬁcultyofﬁndingrelevantcodeexample s
with no knowledge of the types in the API. For instance,
P18 started with the Web but soon abandoned the Web
for the API documentation after two unsuccessful searches,
commenting “having some knowledge of the classes in the
API may actually be able to help me understand the infor-
mation provided by the tutorials” . In general, we observed
that both learning resources provide complementary suppor t
to programmers learning to use APIs. Also, the absence of
a signiﬁcant difference between the two groups suggests
that the time required by novice API users to ﬁnd, extract,
and customize code snippets from code examples may be
comparable to the time needed to learn how to use APIs
from the API documentationfor basic tasks such as the ones
in our study.
V. IMPLICATIONS
A. API Design and Documentation
Proponents of the debate on how to communicate method-
level failures typically endorse either the use of an ex-
ception, or the use of a return-status-object, but seldom
both: “exceptions should be used to report all errors for
all code constructs ” — [15, p. 212]. Our results explain
and document why the use of an exception to communicate
the outcome of an operation may be problematic from
an API-comprehension perspective. In such situations, it
seem reasonable for API designers to consider providing
both a return-status-object (to provide status informatio n in
the case of a successful operation) and an exception (to
communicate method execution failure). Steven Clarke of
the user experience group at Microsoft Research, and a
pioneer of the work on API usability, echoed this view in a
book on Framework Design Guidelines: “ although return
codes should not be used to indicate failures, you can
still consider returning status information in the case of a
successful operation ” [15, p. 213].
In general, our study underscores the need to investigate
theimpactofAPIdesignchoicesonAPIlearningandusabil-
ity beforeadoptinga givendesign choice. APIs are provided
to improve programmers’ productivity, but poorly designed ,
orpoorlydocumented,APIsmayproduceacountereffect.In
our work with APIs, we have observedsituationswhere pro-
grammers had to re-inventthe wheel because APIs designed
for their task were difﬁcult to understand [18]. API usabili ty
studies provide a venue for identifying and ﬁxing usability
and comprehension problems before an API is made public.B. Tool Design
The primary motivation for our study was to understand the
natureofAPI learningandhowbest to supportprogrammers
learning to use a new API. We have identiﬁed 20 different
types of questions the programmers asked about the use of
APIs and also ﬁve questions that proved the most difﬁcult
for the programmers to answer. We believe these questions
could help evaluate existing tool support, and identify are as
where support is lacking. As an example, we present three
areas of difﬁculty where support is currently limited.
Discovering Relevant API Elements not Accessible from
the Type a Programmer is Working With. Jadeite [11]
uses a concept known as a “placeholder” to allow a de-
veloper to annotate the documentation of an API type with
other API types or methodsnot accessible, but relevantto it s
use. Given a particular function,Altair [19] and FRIAR [20]
use heuristics and structural relationships to ﬁnd other re -
lated functions. Jadeite is the only tool, to our knowledge,
aimed at helping programmers discover types or methods
not accessible from a main-type. We consider Jadeite a
precursor to an ideal tool for making relevant API elements
not accessible from a type discoverable. Our ideal tool
would automatically generate and recommend placeholders
and would be integrated with the IDE, preferable with the
content assist feature of the IDE.
Discovering the Types of an API Relevant to Imple-
menting a Task. The names of API types and methods
provide a common vocabulary between API users and API
designers; consequently, the use of types and methods for
query formulation proved to be an effective strategy (in the
context of our study) for locating code examples relevant
to implementing a task. Surprisingly, support for helping
programmers discover the types of an API relevant to a
task is limited. In fact, most code recommendation tools are
based on the premise that programmers already know the
types of an API relevant to their tasks [21]–[24], but this
is not often the case. Sourcerer helps programmers locate
relevantAPIelementsbysuggestingwordsfromopensource
systems that share concepts that are related to the terms in
a search query [25]. Jadeite leverages usage statistics fro m
code examples on the Web to display commonly used types
of an API more prominently. Jadeite and Sourcerer have
one drawback: they are unusable in the absence of a corpus
of code examples; consequently, APIs without a corpus of
examples, or less commonly used parts of an API, may not
be supported. There is a need to further explore comple-
mentary approaches (such as the relationships between API
elements used by Prospector [26]) for recommending API
types relevant to a task.
Unmasking the Relationships between API Types. Some
of the difﬁculties we observed occurred when the dependen-cies between related API elements were not obvious, or not
properly documented. For instance, although the Validator
class and Schemaare related (a Validator object is created
from aSchemaobject), this relationship cannot be inferred
from the Validator class. Participant P4 referred to this as
the absence of a “cross-reference in the API documentation
that says get a Validator instance from a Schema” when
commenting about the difﬁculty experienced relating these
types. One potential solution would be to explicitly docu-
mentsuchhiddendependencies.Alternatively,toolscould be
developed to automatically identify and reveal such hidden
relationships between API elements to developers through
the content assist feature of IDEs.
C. Threats to Validity
The results of our study are based on a systematic obser-
vation of programmers working with real-world APIs in a
laboratory environment. Given this setting, there are fact ors
which limit the generalizability of our observations.
The types of questions we observed in the study, the
process of answering the questions, and the challenges the
participants experienced are related to a certain extent to the
tasks and the experience of the participants. Some of the
questions and the difﬁculties we observed in the study have
been observed in previous API usability studies in differen t
settings [1], [2], [4], [8]; However, given that our study wa s
exploratory in nature and intended to probe whydevelopers
experiencedifﬁculties,andalsogiventhelabsettingandp re-
deﬁned tasks, the catalog of questions cannot be considered
complete, but a starting point.
The difﬁculty the participants experienced in associating
the throwing of an exception to the success, or failure,
of the validation could have been a result of their limited
programming experience. This threat was mitigated by our
useofthethink-aloudprotocolwhichshowedthatourpartic -
ipants had no apparentconfusionwith the validation domain
(they could implement a solution to validate the XML ﬁle).
Rather, the difﬁculty they encountered was well isolated to
the use of exceptions to communicate method-level failures :
the implication that an operation is considered successful if
the method does not throw an exception was not apparent
to our participants. Furthermore, Steven Clarke is quoted
as reporting similar observations amongst professional pr o-
grammers in a book on Framework Design Guidelines: “ In
oneAPIusabilitystudywe performed,developershadtocall
anInsertmethod to insert ... records into a database. If
the method did not throw an exception, the implication was
that the records had been inserted successfully. However,
this was’t clear to the participants in the study. They
expected the method to return the number of records that
were successfully inserted ” [15, p. 212]. The results of
our study closely corroborate Clarke’s observation amongs t
professional programmers; the extent and reasons for thedifﬁculty for the population of professional programmers
would have to be determined by another study.
The size of our tasks, the number of tasks, and the
number of participants also limits the generalizability of our
observations. Although our tasks represented real usages o f
real-world APIs, they were limited in size to permit our
participants to complete a task within the 35 minutes time
frame.Withonlytwotasksand20participants,thequestion s
and the challenges observed in our study could be limited.
However, our use of 20 participants is equal or above the
current standard of evidence in user studies of software
engineering tools [7]. Furthermore, given the observation
that “programmersoften approachlargerprogrammingtasks
by focusing on smaller subtasks” [2], we believe that the
different types of questions and the challenges we observed ,
possibly limited, would generalize to other API learning
tasks.
Lastly, our study involved only Java APIs and the Java
API documentation. Some of our observations may be
different for APIs and documentation in other languages.
Also, since our study focused on programmerslearning how
to use unfamiliar Java APIs, our observations may not be
applicable to programmersworking with familiar Java APIs.
Further studies on API usability are required to verify the
generalizability of our observations to these contexts.
VI. CONCLUSION
To understand the difﬁculties programmers encounter when
working with unfamiliar APIs, the cause of the difﬁculties,
and to investigate how best to support API learning, we
conducted a study in which 20 programmers worked on
programming tasks using two real-world APIs. The study
generated over 20 hours of screen captured videos and
the verbalization of the participants spanning 40 differen t
programming sessions. Our analysis of the data involved
generating generic versions of the questions asked by the
participants about the use of the APIs, identifying those
questionstheproveddifﬁculttoanswer,andinvestigating the
cause of the difﬁcultyusing the verbalizationandthe actio ns
of the participants. Based on the results of our analysis, we
identiﬁed 20 different types of questions programmers ask
when learning to use APIs. We also identiﬁed ﬁve of the 20
questions as being the most difﬁcult for the programmers
to answer, and provide observations to explain the potentia l
causes of the difﬁculties. We believe the questions we have
identiﬁed and the difﬁculties we observed can be used
for evaluating tools aimed at improving API learning, and
to identify areas of the API learning process where tool
supportis lacking,or could be improved.As an example,we
identiﬁed some areas where tool support is currently limite d
including the need for tools that would assist a programmer
easilyidentifythetypesofanAPIthatwouldserveasagood
starting point for searching for code examples, or a startin g
point for exploring the API for a given programming task.REFERENCES
[1] B. Ellis, J. Stylos, and B. Myers, “The Factory pattern in API
design: A usability evaluation,” in Proceedings of the 29th
International Conference on Software Engineering , 2007, pp.
302–312.
[2] J.StylosandB.A.Myers,“Theimplicationsof methodpla ce-
ment on API learnability,” in Proc. of the 16th International
Symposium on Foundations of Software Eng. , 2008, pp. 105–
112.
[3] J. Sillito, G. C. Murphy, and K. De Volder, “Asking and
answering questions during a programming change task,”
IEEE Transactions on Software Engineering , vol. 34, no. 4,
pp. 434–451, 2008.
[4] J. Stylos and S. Clarke, “Usability implications of requ ir-
ing parameters in objects’ constructors,” in Proceedings of
the 29th International Conference on Software Engineering ,
2007, pp. 529–539.
[5] S. Clarke, “Measuring API usability,” Dr. Dobbs Journal , pp.
S6 –S9, 2004.
[6] ——,“Evaluating anew programming language,” in Proceed-
ings of the 13th Workshop of the Psychology of Programming
Interest Group , 2001, pp. 275–289.
[7] J. Brandt, P. J. Guo, J. Lewenstein, M. Dontcheva, and
S. R. Klemmer, “Two studies of opportunistic programming:
interleaving web foraging, learning, and writing code,” in
Proceedings of the 27th International Conference on Human
factors in computing systems , 2009, pp. 1589–1598.
[8] A. J. Ko, B. A. Myers, and H. H. Aung, “Six learning
barriers in end-user programming systems,” in Proc. of Visual
Languages and Human Centric Computing , 2004, pp. 199–
206.
[9] A. J. Ko, R. DeLine, and G. Venolia, “Information needs in
collocated software development teams,” in Proceedings of
the 29th International Conference on Software Engineering ,
2007, pp. 344–353.
[10] J. Stylos and B. A. Myers, “Mica: A web-search tool for
ﬁnding API components and examples,” in Proc. of the Visual
Languages and Human-Centric Computing , 2006, pp. 195–
202.
[11] J. Stylos, B. A. Myers, and Z. Yang, “Jadeite: improving
API documentation using usage information,” in Extended
abstracts on Human factors in computing systems , 2009, pp.
4429–4434.
[12] T. Boren and J. Ramey, “Thinking aloud: reconciling the ory
and practice,” IEEE Transactions on Professional Communi-
cation, vol. 43, no. 3, pp. 261–278, 2000.
[13] B. Curtis, “Substantiating programmer variability,” inIEEE,
ser. 7, vol. 69, 1981, pp. 846–846.[14] S. Bajracharya and C. Lopes, “Analyzing and mining a cod e
search engine usage log,” Empirical Software Engineering ,
pp. 1–43, 2010.
[15] K. Cwalina and B. Abrams, Framework design guidelines:
conventions, idioms, and patterns for reusable .Net librar ies,
2nd ed. Addison-Wesley Professional, 2009.
[16] D. Katz, “Error codes or exceptions? Why is reliable
software so hard?” April 2006. [Online]. Available: http:
//damienkatz.net/2006/04/error codevse.html
[17] J. Spolsky, “Exceptions,” October 2003. [Online]. Ava ilable:
http://www.joelonsoftware.com/items/2003/10/13.html
[18] E. Duala-Ekoko and M. P. Robillard, “The information
gathering strategies of API learners,” TR-2010.6, School o f
Computer Science, McGill University, Tech. Rep., 2010.
[19] F.Long,X.Wang,andY.Cai,“Apihyperlinkingviastruc tural
overlap,” in Proceedings of the the 7th joint meeting of the
European software engineering conference and the ACMSIG-
SOFTsymposiumonThefoundations ofsoftwareengineering ,
ser. ESEC/FSE ’09, 2009, pp. 203–212.
[20] Z.M. Saul, V. Filkov, P.Devanbu, andC. Bird,“Recommen d-
ingrandom walks,”in Proceedings ofthethe6thjointmeeting
of the European software engineering conference and the
ACM SIGSOFT symposium on The foundations of software
engineering , ser. ESEC-FSE ’07, 2007, pp. 15–24.
[21] O. Hummel, W. Janjic, and C. Atkinson, “Code conjurer:
Pulling reusable software out of thin air,” IEEE Software ,
vol. 25, pp. 45–52, 2008.
[22] R. Holmes and G. C. Murphy, “Using structural context to
recommend source code examples,” in Proceedings of the
27th International conference onSoftware Engineering , 2005,
pp. 117–125.
[23] S. Thummalapenta and T. Xie, “Parseweb: a programmer
assistant for reusing open source code on the Web,” in Pro-
ceedings of the 22nd International conference on Automated
software Engineering , 2007, pp. 204–213.
[24] T. Xie and J. Pei, “MAPO: mining API usages from open
source repositories,” in Proceedings of the workshop on
Mining software repositories , 2006, pp. 54–57.
[25] S. Bajracharya, J. Ossher, and C. Lopes, “Searching API us-
age examples in code repositories with sourcerer API search ,”
inProceedings of 2010 ICSE Workshop on Search-driven
Development: Users, Infrastructure, Tools and Evaluation ,
2010, pp. 5–8.
[26] D. Mandelin, L. Xu, R. Bod´ ık, and D. Kimelman, “Jungloi d
mining: helping to navigate the API jungle,” in Proceedings
of the International conference on Programming language
design and implementation , 2005, pp. 48–61.