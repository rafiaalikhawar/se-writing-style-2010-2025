A Search Engine For Finding Highly Relevant Applications
Mark Grechanik, Chen
Fu, Qing Xie
Accenture Technology Labs
Chicago, IL 60601
{mark.grechanik,chen.fu,
qing.xie}@accenture.comCollin McMillan, Denys
Poshyvanyk
The College of William and
Mary
Williamsburg, VA 23185
{cmc,denys}@cs.wm.eduChad Cumby
Accenture Technology Labs
Chicago, IL 60601
chad.c.cumby@accenture.com
ABSTRACT
A fundamental problem of ﬁnding applications that are highly rel-
evant to development tasks is the mismatch between the high-level
intent reﬂected in the descriptions of these tasks and low-level
implementation details of applications. To reduce this mismatch
we created an approach called Exemplar (EXEcutable exaMPLes
ARchive) for ﬁnding highly relevant software projects from large
archives of applications. After a programmer enters a natural-
language query that contains high-level concepts (e.g., MIME, data
sets), Exemplar uses information retrieval and program analysis
techniques to retrieve applications that implement these concepts.
Our case study with 39 professional Java programmers shows that
Exemplar is more effective than Sourceforge in helping program-
mers to quickly ﬁnd highly relevant applications.
1. INTRODUCTION
Creating software from existing components is a fundamental
challenge of software reuse. Naturally, when programmers develop
software, they instinctively sense that there are fragments of code
that other programmers wrote and these fragments can be reused.
Reusing fragments of existing applications is beneﬁcial because
complete applications provide programmers with the contexts in
which these fragments exist. Unfortunately, few major challenges
make it difﬁcult to locate existing applications that contain relevant
code fragments.
A fundamental problem of ﬁnding relevant applications is the
mismatch between the high-level intent reﬂected in the descriptions
of these applications and low-level implementation details. This
problem is known as the concept assignment problem [3]. Many
search engines match keywords in queries to words in the descrip-
tions of the applications, comments in their source code, and the
names of program variables and types. If no match is found, then
potentially relevant applications are never retrieved from reposito-
ries. This situation is aggravated by the fact that many application
repositories are polluted with poorly functioning projects [13]; a
match between a keyword from the query with a word in the de-
scription or in the source code of an application does not guarantee
that this application is relevant to the query.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
ICSE ’10, May 2-8 2010, Cape Town, South Africa
Copyright 2010 ACM 978-1-60558-719-6/10/05 ...$10.00.Currently, a prevalent way for programmers to determine if an
application is relevant to their task(s) is to download the applica-
tion, locate and examine fragments of the code that implement fea-
tures of interest, and observe and analyze the runtime behavior to
ensure that the features behave as desired. This process is manual
and laborious; programmers study the source code and executions
proﬁles of the retrieved applications in order to determine whether
they match task descriptions.
Typically, search engines do little to ensure that retrieved appli-
cations contain code fragments that are relevant to requirements
that developers need to implement. Short code snippets that are
returned as results to user queries do not give enough background
or context to help programmers determine how to reuse these snip-
pets, and programmers typically invest a signiﬁcant intellectual ef-
fort (i.e., they need to overcome a high cognitive distance [17]) to
understand how to use these code snippets in larger scopes. On the
other hand, if code fragments are retrieved in the contexts of ap-
plications, it makes it easier for programmers to understand how to
reuse these code fragments.
A majority of code search engines treat code as plain text where
all words have unknown semantics. However, applications con-
tain functional abstractions in a form of API calls whose semantics
are deﬁned precisely. The idea of using API calls to improve code
search was proposed and implemented elsewhere [8][4]; however,
it was not evaluated with statistical signiﬁcance over a large code-
base using a standard information retrieval methodology [22, pages
151-153].
We created an application search system called Exemplar (EX-
Ecutable exaMPLes ARchive) that helps users ﬁnd highly relevant
executable applications for reuse. Exemplar combines information
retrieval and program analysis techniques to reliably link high-level
concepts to the source code of the applications via standard third-
party Application Programming Interface (API) calls that these ap-
plications use. We have built Exemplar as part of our S3archi-
tecture [27] and conducted a case study with 39 professional Java
programmers to evaluate this search engine. The results show with
strong statistical signiﬁcance that users ﬁnd more relevant applica-
tions with higher precision with Exemplar than those with Source-
forge. Exemplar is available for public use1.
2. OUR APPROACH
In this section we describe the key ideas and give intuition about
why and how our approach works.
2.1 The Problem
A straightforward approach for ﬁnding highly relevant applica-
tions is to search through the source code of applications to match
1http://www.xemplar.orgkeywords from queries to the names of program variables and types.
The precision of this approach depends highly on programmers
choosing meaningful names, but their compliance is generally dif-
ﬁcult to enforce [1].
This problem is partially addressed by programmers who create
meaningful descriptions of the applications that they deposit into
software repositories. However, state-of-the-art search engines use
exact matches between the keywords from queries, the words in the
descriptions, and the source code of applications, making it difﬁcult
for users to guess exact keywords to ﬁnd relevant applications. This
is known as the vocabulary problem, which states that “no single
word can be chosen to describe a programming concept in the best
way” [7]. This problem is general to all search engines but some-
what mitigated by the fact that different programmers who partici-
pate in the projects use their vocabularies to write code, comments,
and descriptions of these projects.
Modern search engines do little to ensure that retrieved applica-
tions are highly relevant to tasks or requirements that are described
using high-level queries. To ensure relevancy, a code mining sys-
tem should take high-level queries and return executable applica-
tions whose functionality is described by high-level requirements
thereby solving an instance of the concept assignment problem. We
believe that the rich context provided by whole applications make
it easier for programmers to reuse code fragments from these ap-
plications.
2.2 Key Ideas
Our goal is to automate parts of the human-driven procedure
of searching for relevant applications. Suppose that requirements
specify that a program should encrypt and compress data. When
retrieving sample applications from Sourceforge2using the key-
wordsencrypt andcompress , programmers look at the source
code to check to see if some API calls from third-party packages
are used to encrypt and compress data. Even though the presence
of these API calls does not guarantee that the applications are rele-
vant, it is a good starting point for deciding whether to check these
applications further.
What we seek is to augment standard code search to include API
documentations of widely used libraries, such as standard Java De-
velopment Kit (JDK) . Of course, existing engines allow users to
search for speciﬁc API calls, but knowing in advance what calls to
search for is hard. Our idea is to match keywords from queries to
words in help documentation for API calls in addition to ﬁnding
keyword matches in the descriptions and the source code of appli-
cations. When programmers read these help documents about API
calls that implement certain high-level concepts, they trust these
documents because they come from known and respected vendors,
were written by different people, reviewed multiple times, and have
been used by other programmers who report their experience at
different forums. Help documents are more verbose and accurate,
and consequently trusted more than the descriptions of applications
from repositories [6].
In addition, we observe that relations between concepts entered
in queries are often preserved as dataﬂow links between API calls
that implement these concepts in the program code. This obser-
vation is closely related to the concept of the software reﬂexion
models , formulated by Murphy, Notkin, and Sullivan, where rela-
tions between elements of high-level models (e.g., processing ele-
ments of software architectures) are preserved in their implementa-
tions in source code [24]. For example, if the user enters keywords
secure andsend , and the corresponding API calls encrypt
2http://sourceforge.net/ as of September 6, 2009.andemail are connected via some dataﬂow, then an application
with these connected API calls are more relevant to the query than
ones where these calls are not connected.
Consider, for example, two API calls string encrypt()
andvoid email(string) . After the call encrypt is in-
voked, it returns a string that is stored in some variable. At some
later point a call to the function email is made and the variable is
passed as the input parameter. In this case we say that these func-
tions are connected using a dataﬂow link which reﬂects the implicit
logical connection between keywords in queries, speciﬁcally, the
data should be encrypted and then sent to some destination.
To improve the precision of our approach, our idea is to deter-
mine relations between API calls in retrieved applications. All
things equal, if a dataﬂow link is present between two API calls
in the program code of one application and there is no link between
the same API calls in some other application, then the former ap-
plication should have a higher ranking than the latter. In addition,
knowing how API calls are connected using dataﬂows enables pro-
grammers to better understand the contexts of the code fragments
that contain these API calls. Finally, it is possible to utilize dataﬂow
connections to extract code fragments, which is a subject of our fu-
ture work on our S3architecture [27].
2.3 Our Goal
Our goal is to develop a search engine that is most effective in
the solution domain (i.e., the domain in which engineers use their
ingenuity to solve problems [14, pages 87,109]). In the problem
domain, requirements are expressed using vague objectives or wish
lists. Conversely, in the solution domain engineers go into imple-
mentation details. To realize requirements in the solution domain,
engineers look for reusable abstractions that are often implemented
using third-party API calls. Thus Exemplar should be most effec-
tive when keywords reﬂect the reality of the solution domain.
Consider a situation in which engineers develop a matchmaking
application. Using keywords such as sweet andlove to describe
requirements from the problem domain, it is possible to ﬁnd a va-
riety of applications in Sourceforge with according descriptions.
However, it is unlikely that these keywords are used to describe
API calls in Java documentation, so it is up to engineers to investi-
gate retrieved applications to determine if any code can be reused.
Since Exemplar uses basic word matches in addition to locating
API calls, it performs equally well in this situation when compared
with existing search engines.
Exemplar is more effective than existing code search engines
when keywords come from the solution domain. Consider the fol-
lowing task: ﬁnd an application for sharing, viewing, and explor-
ing large data sets that are encoded using MIME, and the data can
be stored using key value pairs. Using the following keywords
MIME ,share ,view ,data sets ,key value pairs , an
unlikely candidate application called BIOLAP is retrieved using
Exemplar with a high ranking score. The description of this appli-
cation matches only the keywords data sets , and yet this appli-
cation made it to the top ﬁve of the list.
The reason is that BIOLAP uses the class MimeType , specif-
ically its method getParameterMap that deals with MIME-
encoded data. The descriptions of this class and this method con-
tain the desired keywords, and these implementation details are
highly relevant to the solution domain for the given task. It is need-
less to say that BIOLAP, which is contained in the Sourceforge
repository does not show on the top 300 list of retrieved applica-
tions when the search is performed with the Sourceforge search
engine. The same situation happens when users enter other queries
that contain matching keywords in the descriptions of API calls.keywordapp1
appn…descriptions
of apps
(a) Standard search engines.keywordapp1
appn…descriptions
of API callsAPI call1
API call3API call2
(b) Exemplar search engine.
Figure 1: Illustrations of the processes for standard and Exemplar search engines.
The idea behind Exemplar is to enable engineers to retrieve hidden
and highly relevant applications for their solution domains.
2.4 Our Approach
We describe our approach using an illustration of differences be-
tween the process for standard search engines shown in Figure 1(a)
and the Exemplar process shown in Figure 1(b).
Consider the process for standard search engines (e.g., Source-
forge, Google code search) shown in Figure 1(a). A keyword from
the query is matched against words in the descriptions of the appli-
cations in some repository (Sourceforge, Krugle) or words in the
entire corpus of source code (Google Code Search). When a match
is found, applications app 1toapp nare returned.
Consider the process for Exemplar shown in Figure 1(b). A key-
word from the query is matched against the descriptions of different
documents that describe API calls of widely used software pack-
ages. When a match is found, the names of the API calls call 1to
call kare returned. These names are matched against the names
of the functions invoked in these applications. When a match is
found, applications app 1toapp nare returned.
A fundamental difference between these search schemes is that
Exemplar uses help documents to obtain the names of the API calls
in response to user queries. Doing so can be viewed as instances
of the query expansion concept in information retrieval systems [2]
andconcept location [20]. The aim of query expansion is to reduce
this query/document mismatch by expanding the query with con-
cepts that have similar meanings to the set of relevant documents.
Using help documents, the initial query is expanded to include the
names of the API calls whose semantics unequivocally reﬂects spe-
ciﬁc behavior of the matched applications.
In addition to the keyword matching functionality of standard
search engines, Exemplar matches keywords with the descriptions
of the various API calls in help documents. Since a typical appli-
cation invokes API calls from several different libraries, the help
documents associated with these API calls are usually written by
different people who use different vocabularies. The richness of
these vocabularies makes it more likely to ﬁnd matches, and pro-
duce API calls API call 1toAPI call k. If some help docu-
ment does not contain a desired match, some other document may
yield a match. This is how we address the vocabulary problem [7].
As it is shown in Figure 1(b), API calls API call 1,API call 2,
andAPI call 3are invoked in the app 1. It is less probable that
the search engine fails to ﬁnd matches in help documents for all
three API calls, and therefore the application app 1will be retrieved
from the repository.
Searching help documents produces additional beneﬁts. API
calls from help documents are linked to locations in the project
source code where these API calls are used thereby allowing pro-
grammers to navigate directly to these locations and see how high-
level concepts from queries are implemented in the source code.
Doing so solves an instance of the concept assignment problem [3].3. EXEMPLAR ARCHITECTURE
The architecture for Exemplar is shown in Figure 2. The main
elements of the Exemplar architecture are the database holding ap-
plications (i.e., the Apps Archive), the Search and Ranking engines,
and the API call lookup. Applications metadata describes dataﬂow
links between different API calls invoked in the applications. Ex-
emplar is being built on an internal, extensible database of help
documents that come from the JDK API documentation. It is easy
to extend Exemplar by plugging in different help documents for
other widely used third-party libraries.
The inputs to Exemplar are shown in Figure 2 with thick solid
arrows labeled (1) and(4). The output is shown with the thick
dashed arrow labeled (14) .
Exemplar works as follows. The input to the system are help
documents describing various API calls (1). The Help Page Pro-
cessor indexes the description of the API calls in these help doc-
uments and outputs the API Calls Dictionary, which is the set of
tuples <<word 1,...,word n>,API call >linking selected
words from the descriptions of the API calls to the names of these
API calls(2). Our approach for mapping words in queries to API
calls is different from the keyword programming technique [19],
since we derive mappings between words and APIs from external
documentation rather than source code.
When the user enters a query (4), it is passed to the API call
lookup component along with the API Calls Dictionary (3). The
lookup engine searches the dictionary using the words in the query
as keys and outputs the set of the names of the API calls whose
descriptions contain words that match the words from the query
(5). These API calls serve as an input (6) to the Search Engine
along with the Apps Archive (7). The engine searches the Archive
and retrieves applications that contain input API calls (8).
The Analyzer pre-computes the Applications Metadata (10)
that contains dataﬂow links between different API calls from the
applications source code (9). Since this is done ofﬂine, precise pro-
gram analysis can be accommodated in this framework to achieve
better results in dataﬂow ranking. This metadata is supplied to
the Ranking Engine (12) along with the Retrieved Applications
(11) , and the Ranking Engine combines keyword matching score
with API call scores to produce a uniﬁed rank for each retrieved
Help
PagesAPI call 
lookupAPI 
calls
Search
Engine Apps
Archive
Analyzer
Applications
MetadataRetrieved
Applications
Ranking
EngineRelevant
ApplicationsHelp Page
ProcessorAPI calls
Dictionary
12 45 6 7
8
9
10
1211
133
14
Figure 2: Exemplar architecture.application. Finally, the engine sorts applications using their ranks
and it outputs Relevant Applications (13) , which are returned to
the user(14) .
4. RANKING
In this section we discuss our ranking algorithm and its compo-
nents such as dataﬂow computations.
4.1 Components of Ranking
There are three components that compute different scores in
the Exemplar ranking mechanism: a component that computes a
score based on word occurrences (WOS), a component that com-
putes a score based on the number of relevant API calls (RAS),
and a component that computes a score based on dataﬂow con-
nections between these calls (DCS). The total ranking score is the
weighted sum of these three ranking scores. Each component pro-
duces results of different perspectives (i.e., word matches, API
calls, dataﬂow connections). Our goal is to produce a uniﬁed rank-
ing by putting these different rankings together in a single score.
The purpose of WOS is to enable Exemplar to retrieve applica-
tions based on matches between words in queries and words in the
descriptions of applications in repositories. This is a baseline Ex-
emplar search that should be as effective as the one of Sourceforge.
This approach may be useful for a small subset of applications that
do not use any third-party API calls to implement different func-
tions. In this case, Exemplar’s ranking engine should rely on word
matches to return relevant applications.
However, our investigation of available projects in Sourceforge
shows that a majority of applications use third-party API calls, and
some of these calls implement functionality that is typically re-
ferred in keywords from user queries. Simply put, the more relevant
API calls are found in an application, the higher its rank should be.
The component RAS computes the API call-based ranking score.
Finally, the component DCS computes a score based on weighted
dataﬂow links. We detect different ways of passing data between
API calls and assign weights differently to these dataﬂow links. We
discuss these ranking components in depth below.
4.2 WOS Ranking
WOS component uses Okapi BM25 [29], which is a ranking
function that is typically used by search engines to rank match-
ing documents according to their relevance to a given search query.
This function is implemented in the Lucene Java Framework which
is used in Exemplar, and it is distinguished by TREC for its perfor-
mance and considered as state-of-the-art in the IR community [26].
BM25 is a standard bag-of-words retrieval function that ranks a set
of documents based on the relative proximity of query terms (e.g.,
without dependencies) appearing in each document. BM25 score
is computed as Swos=n
∑
i=1IDF(qi)f(qi,D)·(k+1)
f(qi,D)+k·(1−b+b·|D|
µ(|D|)),
where f(qi,D)is the qi’s term frequency in the document Dwith
the length (i.e., the number of words) |D|,µ(|D|)is the average
document length in the text collection from which documents are
drawn, kandbare parameters whose values are usually chosen 1 .2
and 0.75 respectively, and ﬁnally the IDF(qi)is the inverse docu-
ment frequency weight of the query term qi.
4.3 RAS Ranking
We consider each section in the library documentation that de-
scribes different API calls as a separate document. The collection
of API documents is deﬁned as DAPI= (D1
API,D2
API,..., Dk
API).
A corpus is created from DAPIand represented as the term-by-document m×kmatrix M, where mis the number of terms and
kis the number of API documents in the collection. A generic en-
trya[i,j]in this matrix denotes a measure of the weight of the ith
term in the jthAPI document [32].
API calls that are relevant to the user query are obtained by rank-
ing documents, DAPIthat describe these calls as relevant to the
query Q. This relevance is computed as a conceptual similarity, C,
(i.e., the length-normalized inner product) between the user query,
Q, and each API document, DAPI. As a result the set of triples
/angbracketleftA,C,n/angbracketrightis returned, where Ais the API call, nis the number of
occurrences of this API call in the application with the conceptual
similarity, C, of the API call documentation to query terms.
The API call-based ranking score for the application, j, is com-
puted as Sj
ras=p
∑
i=1nj
i·Cj
i
|A|j, where |A|jis the total number of API
calls in the application, j.
4.4 DCS Ranking
To improve the precision of ranking we derive the structure of
connections between API calls and use this structure as an impor-
tant component in computing rankings. The standard syntax for
invoking an API call is t var=o.callname (p1,..., pn). The
structural relations between API calls reﬂect compositional proper-
ties between these calls. Speciﬁcally, it means that API calls access
and manipulate data at the same memory locations.
There are four types of dependencies between API calls: input,
output, true, and anti-dependence [23, page 268]. True depen-
dence occurs when the API call fwrite a memory location that the
API callglater reads (e.g., var=f (...);...;g(var,...);). Anti-
dependence occurs when the API call freads a memory location
that the API call glater writes (e.g., f(var,...),...;var=g (...);).
Output dependence occurs when the API calls fandgwrite the
same memory location. Finally, input dependence occurs when the
API callsfandgread the same memory location.
Consider an all-connected graph (i.e., a clique) where nodes are
API calls and the edges represent dependencies between these calls.
The absence of an edge means that there is no dependency between
two API calls. Let the total number of connections between nrel-
evant API calls be less or equal to n(n−1). Let a connection be-
tween two distinct API calls in the application be deﬁned as Link;
we assign some weight wto this Link based on the strength of the
dataﬂow or control ﬂow dependency type. The ranking is normal-
ized to be between 0 and 1.
The API call connectivity-based ranking score for the applica-
tion, j, is computed as Sj
dcs=n(n−1)
∑
i=1wj
i·Linkj
i
n(n−1), where wiis the
weight to each type of ﬂow dependency for the given link Link i,
such that 1 >wtrue
i>wanti
i>wout put
i>winput
i>0. The intuition
behind using this order is that these dependencies contribute dif-
ferently to ranking heuristics. Speciﬁcally, using the values of the
same variable in two API calls introduces a weaker link as com-
pared to the true dependency where one API call produces a value
that is used in some other API call.
4.5 Combined Ranking
The ﬁnal ranking score is computed as S=λwosSwos+λrasSras+
λdcsSdcs, where λis the interpolation weight for each type of the
score. These weights are determined independently of queries un-
like the scores, which are query-dependent. Adjusting these weights
enables experimentation with how underlying structural and textual
information in application affects resulting ranking scores.5. IMPLEMENTATION
In this section we describe how we implemented Exemplar.
5.1 Crawlers
Exemplar consists of two crawlers: Archiver andWalker .Archiver
populated Exemplar’s repository by retrieving from Sourceforge
more than 30,000 Java projects that contain close to 50,000 submit-
ted archive ﬁles, which comprise the total of 414,357 ﬁles. Walker
traverses Exemplar’s repository, opens each project by extracting
its source code from zipped archive, and applies a dataﬂow compu-
tation utility to the extracted source code. In addition, the Archiver
regularly checks Sourceforge to see if there are new updates and it
downloads these updates into the Exemplar repository.
To extract all occurrences of invocations of JDK API calls in
all available Java projects, we ran 65 threads for over 50 hours on
ﬁve servers and 25 workstations: three of these servers have two
dual core 3.8Ghz EM64T Xeon processors with 8Gb RAM each,
and two have four 3.0Ghz EM64T Xeon CPUs with 32Gb RAM
each. The workstations uniformly had one 2.83Ghz quad-core CPU
and 2Gb RAM. This job resulted in ﬁnding close to twelve million
invocations of these API calls from JDK 1.5 across all projects. The
next item was to compute dataﬂow connections between these calls
in all Java applications in the Exemplar’s repository.
5.2 Dataﬂow Computation
Our approach relies on the tool PMD3for computing approxi-
mate dataﬂow links, which are based on patterns of dataﬂow depen-
dencies. Using these patterns it is possible to recover a large num-
ber of possible dataﬂow links between API calls; however, some
of these recovered links can be false positives. In addition, we
currently recover links among API calls within ﬁles (intraprocedu-
rally), hence it is likely that some intraprocedural links are missed
and no interprocedural analyses are performed.
5.3 Computing Rankings
We use the Lucene search engine4to implement the core retrieval
based on keyword matches. We indexed descriptions and titles of
Java applications, and independently we indexed Java API call doc-
umentation by duplicating descriptions about the classes and pack-
ages in each methods. Thus when users enter keywords, they are
matched separately using the index for titles and descriptions and
the index for API call documents. As a result, two lists are re-
trieved: the list of applications and the list of API calls. Each entry
in these lists are accompanied by a rank (i.e., conceptual similarity,
C, a number between 0 and 1).
The next step is to locate retrieved API calls in the retrieved ap-
plications. To improve the performance we conﬁgure Exemplar to
use the positions of the top two hundred API calls in the retrieved
list. These API calls are crosschecked against API calls invoked in
the retrieved applications, and the combined ranking score is com-
puted for each application. The list of applications is sorted using
the computed ranks, and returned to the user.
6. CASE STUDY DESIGN
Typically, search engines are evaluated using manual relevance
judgments by experts [22, pages 151-153]. To determine how ef-
fective Exemplar is, we conducted a case study with 39 participants
who are Java programmers. We gave a list of tasks described in
English. Our goal is to evaluate how well these participants can
ﬁnd applications that match given tasks using three different search
3http://pmd.sourceforge.net/ as of September 6, 2009.
4http://lucene.apache.org/ as of September 6, 2009.engines: Sourceforge (SF) and Exemplar with (EWD) and with-
out (END) dataﬂow links as part of the ranking mechanism. We
chose to compare Exemplar with Sourceforge because the latter has
a popular search engine with the largest open source Java project
repository, and Exemplar is populated with Java projects from this
repository.
6.1 Methodology
We used a cross validation study design in a cohort of 39 par-
ticipants who were randomly divided into three groups. The study
was sectioned in three experiments in which each group was given
a different search engine (i.e., SF, EWD, or END) to ﬁnd appli-
cations for given tasks. Each group used a different task in each
experiment. Thus each participant used each search engine on dif-
ferent tasks in this case study. Before the study we gave a one-hour
tutorial on using these search engines to ﬁnd applications for tasks.
Each experiment consisted of three steps. First, participants
translated tasks into a sequence of keywords that described key
concepts of applications that they needed to ﬁnd. Then, partic-
ipants entered these keywords as queries into the search engines
(the order of these keywords does not matter) and obtained lists of
applications that were ranked in descending order.
The next step was to examine the returned applications and to de-
termine if they matched the tasks. Each participant accomplished
this step individually, assigning a conﬁdence level, C, to the exam-
ined applications using a four-level Likert scale. We asked partici-
pants to examine only top ten applications that resulted from their
searches.
The guidelines for assigning conﬁdence levels are the following.
1. Completely irrelevant - there is absolutely nothing that the
participant can use from this retrieved project, nothing in it
is related to your keywords.
2. Mostly irrelevant - only few remotely relevant code snippets
or API calls are located in the project.
3. Mostly relevant - a somewhat large number of relevant code
snippets or API calls in the project.
4. Highly relevant - the participant is conﬁdent that code snip-
pets or API calls in the project can be reused.
Twenty-six participants are Accenture employees who work on
consulting engagements as professional Java programmers for dif-
ferent client companies. Remaining 13 participants are graduate
students from the University of Illinois at Chicago who have at least
six months of Java experience. Accenture participants have differ-
ent backgrounds, experience, and belong to different groups of the
total Accenture workforce of approximately 180,000 employees.
Out of 39 participants, 17 had programming experience with Java
ranging from one to three years, and 22 participants reported more
than three years of experience writing programs in Java. Eleven
participants reported prior experience with Sourceforge (which is
used in this case study), 18 participants reported prior experience
with other search engines, and 11 said that they never used code
search engines. Twenty six participants have bachelor degrees and
thirteen have master degrees in different technical disciplines.
6.2 Precision
Two main measures for evaluating the effectiveness of retrieval
are precision and recall [38, page 188-191]. The precision is calcu-
lated as Pr=# of retrieved applications that are relevant
total # of retrieved applications,
i.e., the precision of a ranking method is the fraction of the top r
ranked documents that are relevant to the query, where r=10 inthis case study. Relevant applications are counted only if they are
ranked with the conﬁdence levels 4or3. The precision metrics
reﬂects the accuracy of the search. Since we limit the investigation
of the retrieved applications to top ten, the recall is not measured in
this study.
6.3 Hypotheses
We introduce the following null and alternative hypotheses to
evaluate how close the means are for the Cs and Ps for control and
treatment groups. Unless we specify otherwise, participants of the
treatment group use either END or EWD, and participants of the
control group use SF. We seek to evaluate the following hypotheses
at a 0.05 level of signiﬁcance.
H0The primary null hypothesis is that there is no difference in
the values of conﬁdence level and precision per task between
participants who use SF, EWD, and END.
H1An alternative hypothesis to H0is that there is statistically sig-
niﬁcant difference in the values of conﬁdence level and pre-
cision between participants who use SF, EWD, and END.
Once we test the null hypothesis H0, we are interested in the
directionality of means, µ, of the results of control and treatment
groups. We are interested to compare the effectiveness of EWD
versus the END and SF with respect to the values of conﬁdence
level, C, and precision, P.
H1 (C of EWD versus SF) The effective null hypothesis is that
µEWD
C=µSF
C, while the true null hypothesis is that µEWD
C≤
µSF
C. Conversely, the alternative hypothesis is µEWD
C>µSF
C.
H2(P of EWD versus SF) The effective null hypothesis is that µEWD
P=
µSF
P, while the true null hypothesis is that µEWD
P≤µSF
P. Con-
versely, the alternative hypothesis is µEWD
P>µSF
P.
H3 (C of EWD versus END) The effective null hypothesis is that
µEWD
C=µEND
C, while the true null hypothesis is that µEWD
C≤
µEND
C. Conversely, the alternative is µEWD
C>µEND
C.
H4(P of EWD versus END) The effective null hypothesis is that
µEWD
P=µEND
P, while the true null hypothesis is that µEWD
P≥
µEND
P. Conversely, the alternative is µEWD
P<µEND
P.
H5 (C of END versus SF) The effective null hypothesis is that µEND
C=
µSF
C, while the true null hypothesis is that µEND
C≤µSF
C. Con-
versely, the alternative hypothesis is µEND
C>µSF
C.
H6(P of END versus SF) The effective null hypothesis is that µEND
P=
µSF
P, while the true null hypothesis is that µEND
P≤µSF
P. Con-
versely, the alternative hypothesis is µEND
P>µSF
P.
The rationale behind the alternative hypotheses to H1andH2is
that Exemplar allows users to quickly understand how keywords in
queries are related to implementations using API calls in retrieved
applications. The alternative hypotheses to H3 and H4 are moti-
vated by the fact that if users see dataﬂow connections between
API calls, they can make better decisions about how closely re-
trieved applications match given tasks. Finally, having the alter-
native hypotheses to H5andH6ensures that Exemplar without
dataﬂow links still allows users to quickly understand how key-
words in queries are related to implementations using API calls in
retrieved applications.6.4 Task Design
We designed 26 tasks that participants work on during experi-
ments in a way that these tasks belong to domains that are easy to
understand, and they have similar complexity. A sample task, for
instance, asks a user to design a Java applications for sharing, view-
ing, and exploring large data sets that are encoded using MIME.
Additional criteria for these tasks is that they should represent
real-world programming tasks and should not be biased towards
any of the search engines that are used in this experiment. Descrip-
tions of these tasks should be ﬂexible enough to allow participants
to suggest different keywords for searching. This criteria signiﬁ-
cantly reduces any bias towards evaluated search engines.
6.5 Normalizing Sources of Variations
Sources of variation are all issues that could cause an observa-
tion to have a different value from another observation. We iden-
tify sources of variation as the prior experience of the participants
with speciﬁc applications retrieved by the search engines in this
study, the amount of time they spend on learning how to use search
engines, and different computing environments which they use to
evaluate retrieved applications. The ﬁrst point is sensitive since
some participants who already know how some retrieved applica-
tions behave are likely to be much more effective than other partic-
ipants who know nothing of these applications.
We design this experiment to drastically reduce the effects of
covariates (i.e., nuisance factors) in order to normalize sources of
variations. Using the cross-validation design we normalize varia-
tions to a certain degree since each participant uses all three search
engines on different tasks.
6.6 Tests and The Normality Assumption
We use one-way ANOV A, t-tests for paired two sample for means,
andχ2to evaluate the hypotheses. These tests are based on an as-
sumption that the population is normally distributed. The law of
large numbers states that if the population sample is sufﬁciently
large (between 30 to 50 participants), then the central limit theorem
applies even if the population is not normally distributed [33, pages
244-245]. Since we have 39 participants, the central limit theorem
applies, and the above-mentioned tests have statistical signiﬁcance.
6.7 Threats to Validity
In this section, we discuss threats to the validity of this case study
and how we address these threats.
6.7.1 Internal Validity
Internal validity refers to the degree of validity of statements
about cause-effect inferences. In the context of our experiment,
threats to internal validity come from confounding the effects of
differences among participants, tasks, and time pressure.
Participants. Since evaluating hypotheses is based on the data
collected from participants, we identify two threats to internal va-
lidity: Java proﬁciency and motivation of participants.
Even though we selected participants who have working knowl-
edge of Java as it was documented by human resources, we did not
conduct an independent assessment of how proﬁcient these partici-
pants are in Java. The danger of having poor Java programmers as
participants of our case study is that they can make poor choices of
which retrieved applications better match their queries. This threat
is mitigated by the fact that all participants from Accenture worked
on successful commercial projects as Java programmers.
The other threat to validity is that not all participants could be
motivated sufﬁciently to evaluate retrieved applications. We ad-
dressed this threat by asking participants to explain in a couple of(a) Conﬁdence level, C.
 (b) Precision, P.
Figure 3: Statistical summary of the results of the case study for Cand P.The central box represents the values from the lower to upper
quartile (25 to 75 percentile). The middle line represents the median. The thicker vertical line extends from the minimum to the maximum value.
The ﬁlled-out box represents the values from the minimum to the mean, and the thinner vertical line extends from the quarter below the mean to the
quarter above the mean.
sentences why they chose to assign certain conﬁdence level to ap-
plications, and based on their results we ﬁnancially awarded top
ﬁve performers.
Tasks. Improper tasks pose a big threat to validity. If tasks
are too general or trivial (e.g., open a ﬁle and read its data into
memory), then every application that has ﬁle-related API calls will
be retrieved, thus creating bias towards Exemplar. On the other
hand, if application and domain-speciﬁc keywords describe task
(e.g.,genealogy andGENTECH ), only a few applications will
be retrieved whose descriptions contain these keywords, thus creat-
ing a bias towards Sourceforge. To avoid this threat, we based the
task descriptions on a dozen of speciﬁcations of different software
systems that were written by different people for different compa-
nies.
Time pressure. Each experiment lasted for two hours, and for
some participants it was not enough time to explore all retrieved
applications for each of eight tasks. It is a threat to validity that
some participants could try to accomplish more tasks by shallowly
evaluating retrieved applications. To counter this threat we notiﬁed
participants that their results would be discarded if we did not see
sufﬁcient reported evidence of why they evaluated retrieved appli-
cations with certain conﬁdence levels.
6.7.2 External Validity
To make results of this case study generalizable, we must ad-
dress threats to external validity, which refer to the generalizabil-
ity of a casual relationship beyond the circumstances of our case
study. The fact that supports the validity of the case study design is
that the participants are highly representative of professional Java
programmers. However, a threat to external validity concerns the
usage of search tools in the industrial settings, where requirements
are updated on a regular basis. Programmers use these updated re-
quirements to reﬁne their queries and locate relevant applications
using multiple iterations of working with search engines. We ad-
dressed this threat only partially, by allowing programmers to reﬁnetheir queries multiple times.
In addition, it is sometimes the case when engineers perform
multiple searches using different combinations of keywords, and
they select certain retrieved applications from each of these search
results. We believe that the results produced by asking participants
to decide on keywords and then perform a single search and rank
applications do not deviate signiﬁcantly from the situation where
searches using multiple (reﬁned) queries are performed.
The other threat to external validity comes from different sizes of
software repositories. We populated Exemplar’s repository with all
Java projects from the Sourceforge repository to address this threat
to external validity.
7. RESULTS
In this section, we report the results of the case study and evalu-
ate the null hypotheses.
7.1 Case Study Results
We use one-way ANOV A, t-tests for paired two sample for means,
andχ2to evaluate the hypotheses that we stated in Section 6.3.
7.1.1 Variables
A main independent variable is the search engine (SF, EWD,
END) that participants use to ﬁnd relevant Java applications. The
other independent variable is participants’ Java experience. Depen-
dent variables are the values of conﬁdence level, C, and precision,
P. We report these variables in this section. The effect of other
variables (task description length, prior knowledge) is minimized
by the design of this case study.
7.1.2 Testing the Null Hypothesis
We used ANOV A to evaluate the null hypothesis H0that the
variation in an experiment is no greater than that due to normal
variation of individuals’ characteristics and error in their measure-
ment. The results of ANOV A conﬁrm that there are large differ-H Var Approach Samples Min Max Median µ σ2DF C p T Tcrit
H1 CEWD 1273 1 4 2 2.35 1.191272 -0.02 3.2·10−3412.5 1.96SF 1273 1 4 1 1.82 1.02
H2 PEWD 35 0.12 0.74 0.42 0.41 0.02634 0.34 5.6·10−54.6 2.03SF 35 0.075 0.73 0.48 0.46 0.17
H3 CEWD 1273 1 4 2 2.35 1.191272 0.01 0.004 2.68 1.96END 1273 1 4 3 2.47 1.27
H4 PEWD 35 0.12 0.74 0.42 0.41 0.2634 0.41 0.15 1.5 2.03END 35 0.075 0.73 0.48 0.46 0.17
H5 CEND 1307 1 4 3 2.47 1.131306 -0.02 6.2·10−4614.8 1.96SF 1307 1 4 1 1.84 1.02
H6 PEND 37 0.075 0.73 0.5 0.47 0.0336 0.4 1.1·10−76.6 2SF 37 0 0.71 0.24 0.27 0.16
Table 1: Results of t-tests of hypotheses , H, for paired two sample for means for two-tail distribution, for dependent variable speciﬁed in the
column Var (either CorP) whose measurements are reported in the following columns. Extremal values, Median, Means, µ, variance, σ2, degrees
of freedom, DF, and the pearson correlation coefﬁcient, C, are reported along with the results of the evaluation of the hypotheses, i.e., statistical
signiﬁcance, p, and the Tstatistics.
Java Cper par for relev scores P, average
Expert SF END EWD SF END EWD
Yes 8.3 14 15 0.27 0.42 0.41
No 10.4 17.8 14.8 0.28 0.53 0.41
Summary 18.7 31.7 29.8 0.275 0.475 0.41
Table 2: Contingency table shows relationship between Cs per
participant for relevant scores and Ps for participants with and
without expert Java experience.
ences between the groups for Cwith F=129>Fcrit=3 with
p≈6.4·10−55which is strongly statistically signiﬁcant. The mean
Cfor the SF approach is 1 .83 with the variance 1 .02, which is
smaller than the mean Cfor END, 2 .47 with the variance 1 .27,
and it is smaller than the mean Cfor EWD, 2 .35 with the variance
1.19. Also, the results of ANOV A conﬁrm that there are large dif-
ferences between the groups for Pwith F=14>Fcrit=3.1 with
p≈4·10−6which is strongly statistically signiﬁcant. The mean P
for the SF approach is 0 .27 with the variance 0 .03, which is smaller
than the mean Pfor END, 0 .47 with the variance 0 .03, and it is
smaller than the mean Pfor EWD, 0 .41 with the variance 0 .026.
Based on these results we reject the null hypothesis and we accept
the alternative hypothesis H1.
A statistical summary of the results of the case study for Cand
T(median, quartiles, range and extreme values) are shown as box-
and-whisker plots in Figure 3(a) and Figure 3(b) correspondingly
with 95% conﬁdence interval for the mean.
7.1.3 Comparing Sourceforge with Exemplar
To test the null hypothesis H1, H2, H5, and H6 we applied four
t-tests for paired two sample for means, for CandPfor participants
who used SF and both variants of Exemplar. The results of this test
forCand for Pare shown in Table 1. The column Samples shows
that 37 out of a total of 39 participants participated in all experi-
ments (two participants missed one experiment). Based on these
results we reject the null hypotheses H1, H2, H5 and H6, and we
accept the alternative hypotheses that states that participants who
use Exemplar report higher relevance and precision on ﬁnding
relevant applications than those who use Sourceforge .7.1.4 Comparing EWD with END
To test the null hypotheses H3 and H4, we applied two t-tests
for paired two sample for means, for CandPfor participants who
used the baseline END and EWD. The results of this test for C
and for Pare shown in Table 1. Based on these results we accept
the null hypotheses H3 and H4 that say that participants who use
EWD do not report higher relevance and precision on ﬁnding
relevant applications than those who use END .
There are several explanations for this result. First, given that
our dataﬂow analysis is imperfect, some links are missed and sub-
sequently, the remaining links cannot affect the ranking score sig-
niﬁcantly. Second, it is possible that our dataﬂow connectivity-
based ranking mechanism needs ﬁne-tuning, and it is a subject of
our future work. Finally, after the case study, a few participants
questioned the idea of dataﬂow connections between API calls. As
it turns out, a few participants had vague ideas as to what dataﬂow
connections meant and how to incorporate them into the evaluation
process. At this point it is a subject of our future work to investigate
this phenomenon in more detail.
7.1.5 Experience Relationships
We construct a contingency table to establish a relationship be-
tween CandPfor participants with (3+ years) and without (less
than 3 years) expert Java experience as shown in Table 2. To test
the null hypotheses that the categorical variables CandPare in-
dependent from the categorical variable Java experience, we ap-
ply two χ2-tests, χ2
Candχ2
PforCandPrespectively. We obtain
χ2
C=2.9 for p<0.24 and χ2
P=0.67 for p<0.71. The small
values of χ2allow us to reject these null hypotheses in favor of
the alternative hypotheses suggesting that there is no statistically
strong relationship between expert Java programming experi-
ences of participants and the values of reported Cs and Ps. That
is, participants performed better with Exemplar than with Source-
forge independently of their Java experience.
8. RELATED WORK
Different code mining techniques and tools have been proposed
to retrieve relevant software components from different reposito-
ries as it is shown in Table 3. CodeFinder iteratively reﬁnes code
repositories in order to improve the precision of returned software
components [9]. Like Exemplar, Codeﬁnder reformulates queriesApproach Granularity Corpora Query
Search Input Expansion
CodeFinder [9] M C D Yes
CodeBroker [39] M C D Yes
Mica [34] F C C Yes
Prospector [21] F A C Yes
Hipikat [5] A C D,C Yes
xSnippet [31] F A D Yes
Strathcona [12][11] F C C Yes
AMC [10] F C C No
Google Code F,M,A C,A D,C No
Sourceforge A C D No
SPARS-J [15][16] M C C No
Sourcerer [25] A C C No
CodeGenie [18] A C C No
SpotWeb [37] M C C Yes
ParseWeb [36] F A C Yes
S6[28] F C,A,T C Manual
Krugle F,M,A C,A D,C No
Koders F,M,A C,A D,C No
SNIFF [4] F,M C,A D,C Yes
Exemplar F,M,A C,A D,C Yes
Table 3: Comparison of Exemplar with other related ap-
proaches. ColumnGranularity speciﬁes how search results are re-
turned by each approach (F ragment of code, M odule, or A pplication),
and how users specify queries (C oncept, A PI call, or T est case). The
columnCorpora speciﬁes the scope of search, i.e., C ode or D ocuments,
followed by the column Query Expansion that speciﬁes if an ap-
proach uses this technique to improve the precision of search queries.
in order to expand the search scope. Unlike Exemplar, CodeFinder
heavily depends on the descriptions (often incomplete) of software
components while Exemplar links API help pages whose informa-
tion is of higher quality than ad-hoc descriptions of components.
Codebroker system uses source code and comments written by
programmers to query code repositories to ﬁnd relevant artifacts
[39]. Unlike Exemplar, Codebroker is dependent upon the descrip-
tions of documents and meaningful names of program variables
and types, and this dependency often leads to lower precision of
returned projects.
Even though it returns code snippets rather than applications,
Mica is similar to Exemplar since it uses help pages to ﬁnd relevant
API calls to guide code search [34]. However, Mica uses help doc-
umentation to reﬁne the results of the search while Exemplar uses
help pages as an integral instrument in order to expand the range of
the query. In addition, Exemplar returns executable projects while
Mica returns code snippets as well as non-code artifacts.
SNIFF extends the idea of using documentation for API calls
for query expansion [8][34] in several ways [4]. After retrieving
code fragments, SNIFF then performs intersection of types in these
code chunks to retain the most relevant and common part of the
code chunks. SNIFF also ranks these pruned chunks using the fre-
quency of their occurrence in the indexed code base. In contrast to
SNIFF [4], MICA [34], and our original MSR idea [8], we evalu-
ated Exemplar using a large-scale case study with 39 programmers
to obtain statistically signiﬁcant results, we followed a standard IR
methodology for comparing search engines, and we return fully
executable applications. Exemplar’s internals differ substantiallyfrom previous attempts to use API calls for searching, including
SNIFF: our search results contain multiple levels of granularity, we
conduct a thorough comparison with the state of art search engine
using a large body of Java application code, and we are not tied to
a speciﬁc IDE.
Prospector is a tool that synthesizes fragments of code in re-
sponse to user queries that contain input types and desired output
types [21]. Prospector is an effective tool to assist programmers
in writing complicated code, however, it falls short of providing
support for a full-ﬂedged code search engine.
The Hipikat tool recommends relevant development artifacts (i.e.,
source revisions associated with a past change task) from a project’s
history to a developer [5]. Unlike Exemplar, Hipikat is a pro-
gramming task-oriented tool that does not recommend applications
whose functionalities match high-level requirements.
Strathcona is a tool that heuristically matches the structure of the
code under development to the example code [12][11]. Strathcona
is beneﬁcial when assisting programmers while working with ex-
isting code, however, its utility is not applicable when searching for
relevant projects given a query containing high-level concepts with
no source code.
Robillard proposed an algorithm for calculating program ele-
ments of likely interest to a developer [30]. Exemplar is similar
to this algorithm in that it uses relations between API calls in the
retrieved projects to compute the level of interest (ranking) of the
project, however, it does not ﬁnd relevant applications.
XSnippet is a context-sensitive tool that allows developers to
query a sample repository for code snippets that are relevant to the
programming task at hand [31]. It remains to be seen how XSnip-
pet can be applicable to ﬁnding relevant projects, since its goal is
to return code fragments based on existing code context.
Existing work on ranking mechanisms for retrieving source code
are centered on locating components of source code that match
other components. Quality of match (QOM) ranking measures the
overall goodness of match between two given components [35],
which is different from Exemplar which retrieves applications based
on high-level concepts that users specify in queries. Component
rank model (CRM) is based on analyzing actual usage relations of
the components and propagating the signiﬁcance through the usage
relations [15][16]. Unlike CRM, Exemplar ranking mechanism is
based on a combination of query expansion and relations between
API calls that implement high-level concepts in queries.
S6is a code search engine that uses a set of user-guided program
transformations to map high-level queries into a subset of relevant
code fragments [28], not complete applications. Like Exemplar,
S6uses query expansion, however, it requires additional low-level
details from the user, such as data types of test cases.
9. CONCLUSION
We created an approach called Exemplar for ﬁnding highly rele-
vant software projects from a large archive of executable examples.
In Exemplar, we combined program analysis techniques with in-
formation retrieval to convert high-level user queries to basic func-
tional abstractions that are used automatically in code search en-
gines to retrieve highly relevant applications. We evaluated Exem-
plar with 39 professional Java programmers and found with strong
statistical signiﬁcance that it performed better than Sourceforge in
terms of reporting higher conﬁdence levels and precisions for re-
trieved Java applications. In addition, participants expressed strong
satisfaction with using Exemplar since it enabled them to assess
retrieved applications using well-documented API calls from third-
party trusted Java libraries.Acknowledgments
We warmly thank anonymous reviewers for their comments and
suggestions that helped us to improve the quality of this paper. We
are especially grateful to Dr. Kishore Swaminathan, the Chief Sci-
entist and Director of Research for his invaluable support. This
work is supported by NSF CCF-0916139, CCF-0916260, Accen-
ture, and United States AFOSR grant number FA9550-07-1-0030.
Any opinions, ﬁndings and conclusions expressed herein are the
authors’ and do not necessarily reﬂect those of the sponsors.
10. REFERENCES
[1] N. Anquetil and T. C. Lethbridge. Assessing the relevance of
identiﬁer names in a legacy software system. In CASCON ,
page 4, 1998.
[2] R. A. Baeza-Yates and B. A. Ribeiro-Neto. Modern
Information Retrieval . ACM Press / Addison-Wesley, 1999.
[3] T. J. Biggerstaff, B. G. Mitbander, and D. E. Webster.
Program understanding and the concept assigment problem.
Commun. ACM , 37(5):72–82, 1994.
[4] S. Chatterjee, S. Juvekar, and K. Sen. Sniff: A search engine
for java using free-form queries. In FASE , pages 385–400,
2009.
[5] D. Cubranic, G. C. Murphy, J. Singer, and K. S. Booth.
Hipikat: A project memory for software development. IEEE
Trans. Software Eng. , 31(6):446–465, 2005.
[6] U. Dekel and J. D. Herbsleb. Improving api documentation
usability with knowledge pushing. In ICSE , pages 320–330,
2009.
[7] G. W. Furnas, T. K. Landauer, L. M. Gomez, and S. T.
Dumais. The vocabulary problem in human-system
communication. Commun. ACM , 30(11):964–971, 1987.
[8] M. Grechanik, K. M. Conroy, and K. Probst. Finding
relevant applications for prototyping. In MSR , page 12, 2007.
[9] S. Henninger. Supporting the construction and evolution of
component repositories. In ICSE , pages 279–288, 1996.
[10] R. Hill and J. Rideout. Automatic method completion. In
ASE, pages 228–235, 2004.
[11] R. Holmes and G. C. Murphy. Using structural context to
recommend source code examples. In ICSE , pages 117–125,
2005.
[12] R. Holmes, R. J. Walker, and G. C. Murphy. Strathcona
example recommendation tool. In ESEC/SIGSOFT FSE ,
pages 237–240, 2005.
[13] J. Howison and K. Crowston. The perils and pitfalls of
mining Sourceforge. In MSR , 2004.
[14] E. Hull, K. Jackson, and J. Dick. Requirements Engineering .
SpringerVerlag, 2004.
[15] K. Inoue, R. Yokomori, H. Fujiwara, T. Yamamoto,
M. Matsushita, and S. Kusumoto. Component rank: Relative
signiﬁcance rank for software component search. In ICSE ,
pages 14–24, 2003.
[16] K. Inoue, R. Yokomori, T. Yamamoto, M. Matsushita, and
S. Kusumoto. Ranking signiﬁcance of software components
based on use relations. IEEE Trans. Softw. Eng. ,
31(3):213–225, 2005.
[17] C. W. Krueger. Software reuse. ACM Comput. Surv. ,
24(2):131–183, 1992.
[18] O. A. L. Lemos, S. K. Bajracharya, J. Ossher, R. S. Morla,
P. C. Masiero, P. Baldi, and C. V . Lopes. Codegenie: using
test-cases to search and reuse source code. In ASE ’07 , pages
525–526, New York, NY , USA, 2007. ACM.[19] G. Little and R. C. Miller. Keyword programming in java.
Automated Software Engg. , 16(1):37–71, 2009.
[20] D. Liu, A. Marcus, D. Poshyvanyk, and V . Rajlich. Feature
location via information retrieval based ﬁltering of a single
scenario execution trace. In ASE, pages 234–243, 2007.
[21] D. Mandelin, L. Xu, R. Bodík, and D. Kimelman. Jungloid
mining: helping to navigate the API jungle. In PLDI , pages
48–61, 2005.
[22] C. D. Manning, P. Raghavan, and H. Schtze. Introduction to
Information Retrieval . Cambridge University Press, New
York, NY , USA, 2008.
[23] S. S. Muchnick. Advanced compiler design and
implementation . Morgan Kaufmann Publishers Inc., San
Francisco, CA, USA, 1997.
[24] G. C. Murphy, D. Notkin, and K. J. Sullivan. Software
reﬂexion models: Bridging the gap between source and
high-level models. In SIGSOFT FSE , pages 18–28, 1995.
[25] J. Ossher, S. Bajracharya, E. Linstead, P. Baldi, and
C. Lopes. Sourcererdb: An aggregated repository of
statically analyzed and cross-linked open source java
projects. MSR , 0:183–186, 2009.
[26] J. Pérez-Iglesias, J. R. Pérez-Agüera, V . Fresno, and Y . Z.
Feinstein. Integrating the Probabilistic Models
BM25/BM25F into Lucene. CoRR , abs/0911.5046, 2009.
[27] D. Poshyvanyk and M. Grechanik. Creating and evolving
software by searching, selecting and synthesizing relevant
source code. In ICSE Companion , pages 283–286, 2009.
[28] S. P. Reiss. Semantics-based code search. In ICSE , pages
243–253, 2009.
[29] S. E. Robertson, S. Walker, and M. Hancock-Beaulieu.
Okapi at trec-7: Automatic ad hoc, ﬁltering, vlc and
interactive. In TREC , pages 199–210, 1998.
[30] M. P. Robillard. Automatic generation of suggestions for
program investigation. In ESEC/SIGSOFT FSE , pages
11–20, 2005.
[31] N. Sahavechaphan and K. T. Claypool. XSnippet: mining for
sample code. In OOPSLA , pages 413–430, 2006.
[32] G. Salton. Automatic text processing: the transformation,
analysis, and retrieval of information by computer .
Addison-Wesley, Boston, USA, 1989.
[33] R. M. Sirkin. Statistics for the Social Sciences . Sage
Publications, third edition, August 2005.
[34] J. Stylos and B. A. Myers. A web-search tool for ﬁnding
API components and examples. In IEEE Symposium on VL
and HCC , pages 195–202, 2006.
[35] N. Tansalarak and K. T. Claypool. Finding a needle in the
haystack: A technique for ranking matches between
components. In CBSE , pages 171–186, 2005.
[36] S. Thummalapenta and T. Xie. Parseweb: a programmer
assistant for reusing open source code on the web. In ASE
’07, pages 204–213, New York, NY , USA, 2007. ACM.
[37] S. Thummalapenta and T. Xie. Spotweb: Detecting
framework hotspots and coldspots via mining open source
code on the web. In ASE ’08 , pages 327–336, Washington,
DC, USA, 2008. IEEE Computer Society.
[38] I. H. Witten, A. Moffat, and T. C. Bell. Managing Gigabytes:
Compressing and Indexing Documents and Images, Second
Edition . Morgan Kaufmann, 1999.
[39] Y . Ye and G. Fischer. Supporting reuse by delivering
task-relevant and personalized information. In ICSE , pages
513–523, 2002.