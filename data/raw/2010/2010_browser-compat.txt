Automated Cross-Browser Compatibility Testing
Ali Mesbah
Electrical and Computer Engineering
University of British Columbia
Vancouver, BC, Canada
amesbah@ece.ubc.caMukul R. Prasad
Trusted Systems Innovation Group
Fujitsu Laboratories of America
Sunnyvale, CA, USA
mukul.prasad@us.fujitsu.com
ABSTRACT
With the advent of Web 2.0 applications and new browsers,
the cross-browser compatibility issue is becoming increas-
ingly important. Although the problem is widely recognized
among web developers, no systematic approach to tackle
it exists today. None of the current tools, which provide
screenshots or emulation environments, species any notion
of cross-browser compatibility, much less check it automat-
ically. In this paper, we pose the problem of cross-browser
compatibility testing of modern web applications as a `func-
tional consistency' check of web application behavior across
dierent web browsers and present an automated solution
for it. Our approach consists of (1) automatically analyzing
the given web application under dierent browser environ-
ments and capturing the behavior as a nite-state machine;
(2) formally comparing the generated models for equivalence
on a pairwise-basis and exposing any observed discrepan-
cies. We validate our approach on several open-source and
industrial case studies to demonstrate its eectiveness and
real-world relevance.
Categories and Subject Descriptors
D.2.5 [ Software Engineering ]: Testing and Debugging
General Terms
Reliability, Verication
Keywords
Dynamic analysis, web testing, cross-browser compatibility
1. INTRODUCTION
Web applications pervade all aspects of human activity to-
day. The web browser continues to be the primary gateway
channeling the end-user's interaction with the web applica-
tion. It has been well known for some time that dierent
This author was a visiting researcher at Fujitsu Laborato-
ries of America, when this work was done.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proï¬t or commercial advantage and that copies
bear this notice and the full citation on the ï¬rst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciï¬c
permission and/or a fee.
ICSE â€™11, May 21â€“28, 2011, Waikiki, Honolulu, HI, USA
Copyright 2011 ACM 978-1-4503-0445-0/11/05 ...$10.00.web browsers render web content somewhat dierently [18,
24, 25, 26]. However, the scope and impact of this problem
has been rapidly growing due to two, fairly recent trends.
First, modern, rich-content web applications have a heavy
client-side behavioral footprint, i.e., they are designed to ex-
ecute signicant elements of their behavior exclusively on the
client-side, typically within the web browser. Further, tech-
nologies such as Ajax [12], Flash, and event-handling for
dynamic HTML, which support this thick-client behavior,
are the very aspects in which web browsers dier.
Second, recent years have seen an explosion in the num-
ber of available web browsers. There are nearly 100 dierent
web browsers available today [31]. Coupled with the dier-
ent kinds and versions of operating systems in which these
operate, this yields several hundred dierent client-side en-
vironments in which a web application can be used, each
with a slightly dierent client-side rendering of the applica-
tion [7, 9]. In the context of the above problem, the key
contributions of this paper are:
We dene the cross-browser compatibility problem and
present an in-depth discussion of the issues surround-
ing possible solutions to it.
We present a systematic, fully-automated solution for
cross-browser compatibility testing that can expose a
substantial fraction of the cross-browser issues in mod-
ern dynamic web applications.
We validate our approach on several open-source as
well as industrial case studies to demonstrate its e-
cacy and real-world relevance.
The rest of the paper is organized as follows. In the next
section we discuss the genesis of the cross-browser compati-
bility problem and how it plays out in modern web applica-
tions. We present a formal characterization of the problem
and put the solutions oered by this paper in that context.
Section 3 reviews related work on cross-browser testing. In
Section 4 we present our proposed approach for performing
cross-browser compatibility testing, followed by a descrip-
tion of the salient implementation aspects of that approach,
in Section 5. Section 6 presents an empirical evaluation of
our approach on some open-source as well as industrial case
studies. In Section 7 we discuss the strengths and weak-
nesses of our solution and lessons learnt from our experience.
We conclude the paper in Section 8.
2. CROSS-BROWSER COMPATIBILITY
Problem Genesis. The cross-browser compatibility
(CBC) problem is almost as old as the web browser it-
self. There are several reasons for its genesis and growth
in recent years. The earliest instances of this problem had(a) Rendering in IE 8.0.
(b) Rendering in Firefox 3.5.
Figure 1: CBC issues in an industrial Web 2.0 ap-
plication: missing widgets in Firefox.
its roots in the parallel evolution of dierent browsers and
the lack of standards for web content. Currently, standards
do exist for basic web content (e.g., HTML 4.0) and for
scripting languages such as JavaScript , JScript, and Ac-
tionScript (e.g., ECMA-262). However, in their bid to be
inclusive and render websites that do not adhere to these
standards (e.g., legacy websites) web browsers typically im-
plement their own extensions to the web standards and in
doing so dier in their behavior. The explosive growth in
the number of browsers and client-side environments has
only exacerbated this issue. In addition, modern, dynamic
web applications execute an increasingly bigger fraction of
their functionality in the client-tier i.e., within the end-user's
web browser, thus further amplifying dierences in observed
behavior. Cross-browser compatibility is widely recognized
as an important issue among web developers [26] but hardly
ever addressed directly during the software development pro-
cess. Typically web applications are developed with a single
target client-side conguration in view and manually tested
for a few more, as an after-thought.
Look-and-feel versus Functionality. The common
perception is that the CBC problem is conned to purely
`look-and-feel' dierences on individual screens. While that
was indeed the case for the previous generation of web appli-
cations, the problem plays out in a more systemic, functional
way in modern Web 2.0 applications. Thus, we believe that
the issue should be addressed as a generalized problem of
checking functional consistency of web application behav-
ior across web browsers, rather than purely a `look-and-feel'
issue. This view is also held by several web developers [10].
The following scenario illustrates how the problem plays
out in modern web applications. The human end-user inter-
acts with the web application, through the browser, by view-
ing dynamically served content and responding with user-
events (e.g., clicks ,mouse-overs ,drag-and-drop actions)
and user data (e.g., form data or user authentication data).
The client-side machine in turn interacts with the server-
side tiers of the web application by sending requests to it,
based on the user input and the web application work-ow.
The server side responds with new web content which is
served by the client-machine to the user through the brow-
ser. Dierences in the serving of a particular screen on the
web browser can inuence the set and nature of actionable
elements available on that screen, which can inuence what
actions the user can and does take on the current screen.
This can have a cascading eect by altering requests sent
B:Browserâ€levelÂ Observable Â 
DifferencesA:DOMÂ and/orÂ Traceâ€levelÂ 
Differences
 Differences Differences
D:Â Differences Â detectedÂ 
byÂ Â proposedÂ methodC:IdealÂ setÂ ofÂ 
differences Â toÂ detect yppFigure 2: Browser incompatibility landscape.
from the client-tier to the server tiers, the response received
and subsequent content served on the client, thus inuenc-
ing the overall evolution of the web application on the client
side. Figure 1 shows screen-shots taken from a real en-
terprise (proprietary) Web 2.0 application, illustrating this
scenario. While the application renders correctly in Inter-
net Explorer, the Firefox rendering of it is missing the New
User andDelete User widgets, marked with a red eclipse in
IE. Thus, the human user would be unable to exercise these
features from this screen, thereby completely removing a
very important set of behaviors, namely the application ad-
ministrator being able to add and remove users, from the
evolution of the web application under Firefox.1
Problem Denition. Our working denition of the
CBC problem, for the purposes of this paper, is based on the
Venn diagram of Figure 2. The set Bdepicts those cross-
browser dierences that can be observed by the human user
on the web browser, while the set Arepresents those cross-
browser dierences that are reected as dierences in the
client-side state of one or more screens (e.g., dierences in
the DOM representation or CSS properties of various DOM
elements) or dierences in the set of possible traces, i.e.,
alternating sequences of screens and user actions, on the
client-side. Further, there is the set of dierences B A
which can be visually observed by the user but is not re-
ected as a dierence in the client-side state or in the evolu-
tion of that state. These dierences are typically, though not
necessarily, stylistic and stem from dierent browsers' ren-
dering of identical web content (e.g., CSS les). Although
such dierences are important in their own right, they are
not the focus of this paper. There is also the set A B
of dierences, which exist in the DOM representation but
do not result in any user-observable dierences in behavior.
We argue that these dierences are not relevant since the
nal metric of incompatibility is that which can be observed
by the end user. The set C=A\B, which comprises the
dierences reected in both the user-observable behavior as
well as the underlying client-side state, represents the most
interesting target for our technique. However, as explained
below in Section 4, typically the set of dierences detected
by our technique is a set D, whereCDA.
3. RELATED WORK
Currently there are several tools and services, both open-
source and commercial, that claim to address the problem
of cross browser compatibility checking. They fall under two
broad categories, namely:
Tools capturing screenshots. These web services
can capture and provide screen-shots of how a particular
1We veried that this was an issue of DOM-level dierences, not
simply one of screen-resolution or the size of the browser window.URL, specied by the user, would be rendered, under var-
ious client-side platform combinations of specic browsers,
operating system and screen resolutions. Typical oerings
include IE NetRenderer [15] and ieCapture [14], which focus
on Internet Explorer as well as others like BrowserShots [7],
BrowserCam [3], Browser Photo [6], and Litmus [20] which
include a broader set of browser-OS combinations.
Tools providing emulation environments. These
tools and web services oer a slightly more advanced func-
tionality than the previous category in that they allow the
user to interact with a particular web application as if it
were being served on a particular client-platform (browser-
OS combination). Representatives in this category include
Adobe BrowserLab [5] and CrossBrowserTesting.com, which
span a wide variety of client-side platforms, as well as more
specic ones like BrowserCamp [4] targeted for Mac OS
X browsers, Xenocode Browser Sandbox [32] for windows-
based environments, IETester [16] which is specic to IE and
iPhoney [17] which is an emulator for the iPhone platform.
The oerings in both of the above categories suer, in
varying degrees, from the following fundamental limitations:
1.Compatibility Checking: None of the above tools
species any notion of cross-browser behavioral com-
patibility, much less check it automatically. It is left
to the human user to dene their own notion of com-
patibility and validate it on observed behavior, usually
manually. This makes the whole process very ad hoc
and hence the quality of the check itself, very suspect.
2.Behavioral Coverage: Modern dynamic web appli-
cations display rich, stateful behavior which cannot be
adequately covered with a few screenshots or manual
examination of a few traces. The above solutions do
not provide any automatic method of systematically
and comprehensively exploring web application behav-
ior. Such an exploration is typically needed to expose
bugs embedded deep in the web application work-ow.
3.Scalability: Since the process of actually browsing
the web application under dierent environments and
comparing observed behavior is completely manual, it
is dicult to apply the tools to several browser-OS
platforms in a consistent, repeatable manner.
The Acid Tests [2] developed under the Web Standards
Project present a complementary approach towards achiev-
ing cross-browser compatibility. The Acid Tests are a bat-
tery of 100 tests that check a given browser for enforce-
ment of various W3C and ECMA standards. However, the
fact that almost all common browsers fail some, and some-
times most of these tests2is another indication that cross-
browser compatibility issues will continue to exist in the
foreseeable future. A related eort in this category is the
SputnikTests [30] project which provides a conformance test
suite to check JavaScript engines for ECMAScript 3 com-
pliance. These approaches are complementary to ours since
they check web browsers (or their component engines) for
standards' compliance rather checking the real-time behav-
ior of a specic web application of interest.
Another related body of work is that of GUI testing [21,
33]. Indeed, web applications can be seen as a special class
of general GUI-based applications. However, the specic ar-
chitecture, execution environment (the web browser), imple-
mentation technologies ( JavaScript ,Ajax ) and API stan-
dards (W3C HTML, CSS, DOM) of web applications allow
us to employ a simple nite state machine model and de-
velop very ecient equivalence checking algorithms around
2IE8 scores 20/100 on the Acid3 Test [1].it, whereas [21] use GUI forests, event-ow graphs, and in-
tegration trees.
Eaton and Memon [11] presented one of the rst tech-
niques for automated identication of cross-browser issues
on specic web pages. Their technique identies potentially
problematic HTML tags on a page using a user-supplied,
manually generated classication of good and faulty pages.
Concurrent to our work, Choudhary et al. [8] have proposed
theWebDiff tool, which analyzes the DOM as well as
screen-shots of pairs of screens to automatically locate cross-
browser issues. In both these papers the focus is on iden-
tifying cross-browser dierences in individual screens. In
contrast, our approach also identies more systemic cross-
browser issues that manifest in the overall trace-level be-
havior of the web application. Thus, these techniques are
somewhat complementary to our work.
4. OUR APPROACH
Our overall approach consists of a two-step process. The
rst step is to automatically crawl the given web applica-
tion under dierent browser environments and capture and
store the observed behavior, under each browser, as a nite-
state machine navigation model . The crawling is done in an
identical fashion under each browser to simulate exactly the
same set of user-interaction sequences with the web appli-
cation, under each environment. The second step consists
of formally comparing the generated models for equivalence
on a pairwise-basis and exposing any observed discrepan-
cies. The crawling technology (i.e., model generation pro-
cess), the generated model and the equivalence check are
discussed in more detail in the following sections.
4.1 State Space Exploration
Our approach for automatically exploring the web appli-
cation's state space is based on the Crawljax [22, 23] work.
Crawljax is a crawler capable of exercising client-side code,
detecting and executing doorways (clickables) to various dy-
namic states of modern ( Ajax -based) web applications. By
ring events on the user interface elements and analyzing the
eects on the dynamic DOM tree in a real browser before
and after the event, the crawler incrementally builds a state
machine capturing the states of the user interface and the
possible event-based transitions between them. Crawljax
is fully congurable in terms of the type of elements that
should be examined or ignored during the crawling process.
For more details about the architecture, algorithms or capa-
bilities of Crawljax the interested reader is referred to [22,
23]. We have extended Crawljax in several ways, to apply
it to the CBC problem. This is discussed in Section 5.
4.2 The Navigation Model
The navigation model produced by crawling the web ap-
plication is a nite-state machine. The states represent the
screens observed by the end-user, on the web browser, and
the transitions represent user-actions (e.g. a button click)
that cause the web application to transition from one screen
to another. Each transition is labeled with the user action
that caused it. Each state (screen) is represented by its un-
derlying programmatic representation as viewed by the web
browser. For the purpose of meaningfully and eciently
comparing multiple navigation models we found it useful
to view and analyze the navigational model hierarchically.
The top level is a graph representation of the nite-state
machine with the states (screens) represented merely as un-
named vertices. We refer to this as the state graph . At
the second level is the full programmatic representation ofeach screen/state which we refer to as the screen model of
the state. These are formally dened below. Conceptually,
the state graph captures the set of traces, i.e., alternating se-
quences of user-actions and screen transitions, without refer-
ring to the details of each screen, whereas the screen model
of each screen captures precisely this detail, but without any
knowledge of transitions leading up to or out of the screen.
Denition 1 (State Graph) A State Graph Gis a la-
beled, directed graph, with a special designed start vertex. It
is denoted by a 5 tuple,G(V;E;o;;L), whereVis the set
of vertices, Ethe set of (directed) edges, othe special desig-
nated start vertex, an alphabet of labels and L:E!is
a labelling function that assigns a label from to each edge.
Further,Ghas the following specic characteristics:
1. Each label from can appear on at most one outgo-
ing edge from a given vertex, i.e., no vertex can have
two (or more) outgoing edges with the same label, al-
though the same label can appear on multiple edges in
the overall graph.
2.Gcan have multi-edges, i.e.,9e1;e22E:s(e1) =
s(e2)andd(e1) =d(e2).
3. Each node in Vis reachable from the root r, i.e.,Gis
composed of a single connected component.
4.Gcan be cyclic.
The special node, otypically denotes the start or index
screen (state) of the web application. For an edge e= (u;v)
we refer to its source vertex ubys(e) and destination vertex
vbyd(e). We useOut(v) andIn(v) to denote, respectively,
the set of outgoing and incoming edges of vertex v.
Denition 2 (Screen Model) A Screen Model Tis a ro-
oted, directed, labeled tree. It is denoted by a 5-tuple,T(Q;-
D;r;; ), whereQis the set of vertices, Dis the set of
directed edges, r2Qis the root vertex, is a nite set of
labels and:Q!is a labelling function that assigns a
label fromto each vertex in Q.
In our current implementation, the screen model is essen-
tially an abstracted version of the DOM tree of a given
screen, displayed on the web browser. Note that this model
could easily be generalized to include (and compare) other
aspects of the client-side state such as JavaScript variable
values or CSS properties. Q;D andrhave obvious meanings
in terms of the DOM tree. The label of a particular node is
a combination of the HTML tag-name of the DOM node as
well as a set of name-value pairs representing the attributes
of the DOM node. We use a single label as a convenient
abstraction for these multiple pieces of data, in order to for-
malize the model and the equivalence check performed on
it (Section 4.3). Details of the actual implementation are
discussed in Section 5.
4.3 Equivalence Checking
The equivalence check of the navigation models mirrors
our hierarchical view of them. We rst extract the state
graph models from the respective navigation models and
compare them at a trace-level. This provides a set of trace-
level dierences between the two navigation models, i.e. a
set of traces that exist in one model and not in the other,
and vice versa. It also pairs up each screen in the rst model
with its most likely counter-part in the second model. Next,
candidate matching pairs of screens, produced by the rst
step, are compared in terms of their underlying DOM rep-
resentations. This unearths detailed screen-level dierencesAlgorithm 4.1: ModelEquivCheck (M1; M2)
G1 StateGraph (M1)
G2 StateGraph (M2)
if(TraceEquivCheck (G1; G2) =false )
then OutTraceDiff (G1; G2)
V1 Vertices (G1)
for each v12V1
do8
>>>>>>>><
>>>>>>>>:if(v1:match6=null)
then8
>>>>>><
>>>>>>:v2 v1:match
T1 GetScreen (M1; v1)
T2 GetScreen (M2; v2)
if(ScrnEquivCheck (T1; T2)
=false )
then
OutScrDiff (T1; T2)
whose eects may be conned to the individual screen or
may play into trace-level dierences. Our proposed method
attempts to prune out some of the provably benign dier-
ences (discussed in Section 5). Note that providing a de-
tailed diagnosis for the cause of each observed dierence is
beyond the scope of this work at its current state.
Algorithm 4.1 is the overall algorithm for matching a pair
of navigation models M1andM2, generated through the
crawling. The function StateGraph returns the under-
lying state graph, which is an input to the TraceEquiv-
Check (Algorithm 4.2). TraceEquivCheck checks and
annotates the two graphs. It returns false if G16G2.Out-
TraceDiff extracts trace-level dierences from the anno-
tated graphs. V1is the set of vertices from G1. The function
GetScreen extracts and returns the detailed screen repre-
sentation of a vertex vof a state graph from its correspond-
ing navigation model. This is used for the equivalence check,
ScrnEquivCheck , done by the screen-matching algorithm,
explained later. The function OutScrDiff extracts and
presents these screen-level dierences to the user.
Trace Equivalence Check. The notion of trace equiv-
alence on state graphs and the precise algorithm to perform
such a comparison are discussed in the following.
Denition 3 (Trace Equivalence) Given state graphs
G1(V1;E1;o1;;L1)andG2(V2;E2;o2;;L2),G1andG2
are said to be trace-equivalent, denoted as G1G2, if and
only there exists a bijective mapping function M:V1!V2
such that the following are true:
1.8u;v2V1;(u;v)2E1,(M(u);M(v))2V2
2.8e1(u1;v1)2E1;e2(u2;v2)2E2such thatM(u1) =
u2andM(v1) =v2)L 1(e1) =L2(e1)
3.M(o1) =o2
Algorithm 4.2 implements the trace-level equivalence check
on the state graphs G1andG2as an isomorphism check.
The function Out(v) returns the set of outgoing edges of
vertexv,Label (e) returns the label of edge eand the func-
tionLookup (l,edgeSet ) returns an edge having the label l
from the set of edges edgeSet ornullif none exists. Dest (e)
returns the destination vertex of edge e. It is assumed that
thematch eld of each edge and the visited eld of each ver-
tex is initialized to false and the match eld of each vertex
in is initialized to null(inG1andG2). The above algorithm
is a simple variant of depth-rst search and linear-time in
the sizes of G1;G2i.e.,O(jV1j+jV2j+jE1j+jE2j).
It is important to note here that we have chosen to use
the same alphabet of labels for bothG1andG2, in or-
der to develop and present the theoretical under-pinnings of
our approach. However, in practice, edge labels representing
otherwise identical transitions, also have cross-browser dif-
ferences. Our tool implementation, described in Section 5,Algorithm 4.2: TraceEquivCheck (G1; G2)
procedure Match (u1; u2)
u1:visited true
u2:visited true
u1:match u2
u2:match u1
for each e12Out(u1)
do8
>>>>>>>>>>>>>>>>>>>>>><
>>>>>>>>>>>>>>>>>>>>>>:e2 Lookup (Label (e1);Out(u2))
if(e26=null)
then8
>>>>>>>>>>>>>>>>>>><
>>>>>>>>>>>>>>>>>>>:v1 Dest (e1)
v2 Dest (e2)
if((v1:visited =false )&
(v2:visited =false ))
then8
><
>:e1:match true
e2:match true
edgeCt += 2
Match (v1; v2)
else if ((v1:match =v2)&
(v1:visited =true)&
(v2:visited =true))
then(e1:match true
e2:match true
edgeCt += 2
main
global edgeCt
edgeCt 0
o1 StartVertex (G1)
o2 StartVertex (G2)
Match (o1; o2)
if(edgeCt =jE1j+jE2j)
then return (true)comment: G1G2
else return (false )comment: G16G2
uses a set of abstractions and transformations to reconcile
these dierences and establish edge-label equivalence.
Theorem 1 Algorithm 4.2 returns G1G2, if and only if
they are trace equivalent according to Denition 3.
We have not included the formal proof of the above theo-
rem (and others below) in the paper due to space constraints.
However, the interested reader can nd them at this link.3
In the case where Algorithm 4.2 nds that G16G2, con-
sider the sub-graphs implied by the partial match produced
by the algorithm, i.e. the sub-graph implied by all nodes
and edges that were matched to a counter-part in the other
graph. Specically, consider subgraph G0
1(V0
1;E0
1;o1;;L0
1)
ofG1, whereE0
1=fe2E1:e:match =trueg,V0
1=fv2
V1:v:match6=nullg,L0
1:E0
1!andL0
1(e) =L1(e)8e2
E0
1. The implied sub-graph G0
2ofG2(V0
2;E0
2;o2;;L0
2) can
be similarly dened. As the following theorem states, sub-
graphsG0
1andG0
2are indeed valid state graphs themselves
and trace-equivalent as per Denition 3.
Theorem 2 When Algorithm 4.2 certies G16G2the sub-
graphsG0
1andG0
2induced by the partial match computed
by it, are valid state graphs and trace-equivalent as per the
criteria of Denition 3.
Further, it can be shown that when Algorithm 4.2 returns
G16G2, it not only produces a trace equivalent partial
match but actually a maximal partial match, i.e. there do
not exist a pair of edges e1ande2, wheree12E1bute162E0
1
ande22E2bute262E0
2which can be added to G0
1andG0
2
respectively, along with their source and sink nodes such
that the resulting graphs would also be trace-equivalent.
Theorem 3 IfG16G2, the trace-equivalent partial matches
G0
1andG0
2computed by Algorithm 4.2 are maximal matches.
3URL for proofs of the above theorems: http://www.flacp.
fujitsulabs.com/~mprasad/2b5b385a6d/proofs.pdfNote, that although the algorithm computes maximal
matches, it is easy to show by a simple example that the
match need not be the maximum match possible. It is pos-
sible to have a variant of Algorithm 4.2 that back-tracks on
matching decisions made in order to compute the absolute
maximum match. In our experience, the maximal match
computed by the linear-time algorithm above is sucient
for most practical purposes.
Screen Equivalence Check. Since the screen model is
represented as a rooted, directed, labeled tree (Denition 2),
it is natural to compare two given screen models T1andT2
based on the isomorphism of the respective trees. Thus,
screen models T1(Q1;D1;r1;; 1) andT2(Q2;D2;r2;; 2)
are said to be equivalent, denoted T1T2, if and only if
there exists a bijective mapping N:Q1!Q2such that:
1.N(r1) =r2
2.8q2Q1;1(q) =2(N(q))
3.8u;v2Q1;(u;v)2D1,(N(u);N(v))2D2
Since screen models are rooted, labeled trees, screen match-
ing (the function call ScrnEquivCheck (T1;T2) in Algo-
rithm 4.1) can be performed in linear time by a simple vari-
ant of the tree isomorphism algorithm originally due to Aho,
Hopcroft and Ullman [13]. However, our implementation is
built upon a more contemporary solution to this problem
that respects the special syntax and structure of DOM trees.
Further details of this check are presented in Section 5.
5. TOOL IMPLEMENTATION
We have implemented our CBC testing approach in a tool,
called CrossT , which comprises two components. For the
state exploration component, we have used and extended on
the open source Ajax crawler Crawljax [23].4The input
consists of the URL of the web application under examina-
tion, a list of supported browsers (IE, Firefox, and Chrome),
and a list of user interface elements for inclusion and exclu-
sion during the crawling execution. Using the given settings,
the web application is crawled in each of the browsers and
the inferred navigation model is serialized and saved into the
le system. The implementation of the navigation model it-
self is based on the JGraphT5library. The nodes are state
abstractions in terms of the browser's DOM tree and the
edges are comprised of the DOM elements and event types
that cause state transitions. In the second component, the
saved navigation models are used to automatically conduct
a pair-wise comparison of the web application's behavior in
the given browsers, which itself has two parts, namely, trace-
level and screen-level comparison.
Trace-level Comparison. For detecting trace-level dif-
ferences, we have implemented Algorithm 4.2 in Java, which
produces as output a list of edges and nodes that are mis-
matched in either of the two compared graphs. In order to
compare the labels of two edges, as dened in Denition 3,
our method retrieves the DOM element and the correspond-
ing event type from each edge and tries to reconcile the two
labels. Two edges are considered a match if the edit distance
between the two based on a combination of event types, tag
names, attributes and their values, as well as the XPath po-
sitions of the two elements on the corresponding DOM trees,
is lower than a similarity threshold. The edge comparison
method is fully recongurable: for instance, if persistent ID
attributes are used for elements in a given web application,
the method can be easily congured to use the IDs instead
of the combination sketched above.
4http://crawljax.com
5http://jgrapht.sourceforge.netTable 1: Examples of browser DOM dierences.
Description FF IE CH
uppercase, lowercase style="display: style="DISPLAY: style="display:
semicolon, whitespace style="display: none;" style="DISPLAY: none" style="display: none;"
colgroup <COL width="5%"/> <COLGROUP><COL width="5%"/> <COL width="5%"/>
middle, center valign="middle" valign="center" valign="middle"
attribute value style="visibility:inherit; style="WIDTH:256px; style="visibility:inherit;
order width:256px;" VISIBILITY:inherit;" width:256px;"
attribute addition <INPUT name="reason" value=""/> <INPUT name="reason"/> <INPUT name="reason"/>
element addition <body> <body><iframe id="yui_hist_iframe" <body>
Screen-level Comparison. Each node on the state
graph represents an abstraction of the DOM tree instance
of a particular screen of the web application, as viewed in
the browser. As discussed in Section 2, not all screen-level
dierences should be agged as candidate CBC issues. For
instance, Table 1 shows some of the internal DOM level dif-
ferences (e.g., elements, attributes, their values and order
of appearance) in Firefox (FF), Internet Explorer (IE), and
Chrome (CH), that typically do not cause any observable
functional dierences in the way a web application is pre-
sented to the end-user (the set A Bin Figure 2). Based
on our case studies, we have started to document such dif-
ferences between browsers and develop suitable abstractions
that allows our screen-matching algorithm to ignore them
and present the user with only the true CBC issues.
Our implementation for detecting DOM-level mismatches
is built as an extension of the differencing engine in XM-
LUnit6. To minimize the number of false positives (i.e., irrel-
evant dierences), we have written a DifferenceListener
to ignore case sensitivity, node-level white space, attribute
order, node text values, attribute value order, and white-
space between the values. In addition, our tool takes as
input a list of patterns that can be excluded, such as the
ones shown in Table 1. A custom ElementQualifier is used
to recursively compare nodes and their children to allow for
node permutations in the graphs.
Additionally, as a realization of the OutScrDiff in Al-
gorithm 4.1, we have implemented a visualization plugin
for our tool, which while crawling, makes a snapshot of
each screen change in the browser at hand. Later, when
a mismatch is found between two screens, the correspond-
ing snapshots are used to create a visualization report of the
dierences. More importantly, the report also contains the
transition paths that lead to the mismatched screens, along
with the two corresponding DOM trees having the dier-
ences highlighted (see Figure 5).
6. EMPIRICAL EVALUATION
To assess the ecacy and utility of our approach and the
corresponding implemented tool, we have conducted a num-
ber of case studies following guidelines from [19]. Our eval-
uation addresses the following research questions:
RQ1 What is the eectiveness of our approach in reveal-
ing observable trace-level and screen-level dierences
in dierent browsers?
RQ2 How eective is our DOM-level comparison in mini-
mizing the number of false positives?
RQ3 What is the tool's performance and automation level?
6.1 Subject Systems
The Organizer. Our rst experimental case is an open
source Ajax web application, called the Organizer , de-
veloped by Zammetti in his book `Practical Ajax projects
6http://xmlunit.sourceforge.netTable 2: Experimental data.Exp. Subject
Browser Pair
Detected States
Detected Transitions
Crawled Paths
Avg. DOM Nodes
Crawling (min)
Comparison (sec)
Manual Eort (min)
Organizer FF 13 53 51 112 4 7 2
CH 14 60 54 112
Ind-v1 * FF 100 107 89 650 11 38 3
CH 100 107 89 666
Ind-v2 * FF 200 200 178 230 28 45 3
IE 200 199 178 245
Tmms IE 31 48 23 56 5 7 2
CH 24 34 18 56
Cnn* FF 100 99 65 246 12 32 2
IE 100 99 68 394
with Java technology' [34] and available for download.7It is
a Java EE personal information manager using WebWork,
HSQLDB, Spring JDBC, and the Prototype Ajax library.
IND V1 and V2. Our second experimental sub-
ject consists of two dierent versions ( Ind-v1 andInd-v2 )
of a large industrial enterprise Web 2.0 application. Ind
is a business process manager composed of approximately
214095 LOC in 248 client-side JavaScript les, 90742 LOC
in 353 JSP les, and 58701 LOC in 43 Java les. YUI8
is the Ajax library used in Ind. A complete user interface
redesign forms the main dierence between the two versions.
TM Management System. Tmms is a live, form-
based, enterprise web application for submission, approval,
distribution and archiving of technical memoranda. It is
a very typical example of the many, relatively small but
important, legacy web applications that are common-place
in enterprise settings and usually very poorly compliant with
modern web browsers. It is written in JavaScript and PHP.
Public Domain Sites. We have experimented with
a number of public domain sites, taken from the list of
Mozilla's bug reports on browser incompatibility issues.9
Here we discuss the CNN 2008 US presidential elections page
(referred to as Cnn).10
6.2 Experimental Setup
For each experimental subject, we congured our
tool by providing the URL, elements to be included
(e.g., crawler.click("div") and excluded (e.g.,
crawler.dontClick("div").withAttribute("class",
"logout") ) in the crawling session, as well as the browser
7http://www.apress.com
8http://developer.yahoo.com/yui/
9http://tinyurl.com/ykjc3hl
10Retrieved 10 Mar 2010: http://edition.cnn.com/ELECTION/2008/
results/president/Table 3: Case study results.Exp. Subject
Trace-level Mismatches
Trace-level False Pos.
Screen-level Mismatches
(% of pair of screens)
Screen-level False Pos. (%)
Avg. DOM-level Mismatches
(per trace-level matched
screen pair)
Method
Organizer 7 0 23 33 8 CrossT
- - 100 84 102 XDiff
Ind-v1 0 0 43 18 32 CrossT
- - 100 65 436 XDiff
Ind-v2 7 2 40 37 14 CrossT
- - 100 74 529 XDiff
Tmms 14 0 50 13 36 CrossT
- - 100 50 93 XDiff
Cnn 16 0 41 12 42 CrossT
- - 100 55 737 XDiff
to be used. The browser specications used in our case
studies are as follows: Firefox version 3.5, Chrome version
4.1, and Internet Explorer version 8.0. To constrain the
state space, for this experiment, so that we can manually
assess the correctness of the output, we set the crawl depth
to 3 for all the subjects.
As shown in Table 2, for each subject, we measure the
number of automatically detected states, transitions, and
crawled paths. To give an indication of the size of the
screens, we calculate the average number of DOM nodes
per screen. For three of the subjects (marked with a *) we
specically set a maximum number of detected states (e.g.,
100 or 200) to constrain the experimental data. To address
RQ3, we report the time taken for the crawling and state
exploration phase (in minutes), for the graph equivalence
checking (in seconds), and manual eort required to cong-
ure and use CrossT (in minutes).
In the second phase of the experiment, the saved naviga-
tion models are checked for equivalence at a trace and screen
level. We measure the number of trace-level and screen-level
mismatches (pair of nodes) reported by CrossT , as well as
the number of DOM-level dierences per pair of screens,
output as potential matches by the trace-level comparison.
To form a baseline for comparison, we manually inspect
each subject in the given browsers, to document the observ-
able trace-level and screen-level dierences. In addition, we
use the default settings of XMLUnit's diff method (further
referenced as XDiff ) as a baseline to evaluate the perfor-
mance of our approach for detecting cross-browser DOM-
level dierences.
To measure the eectiveness, we analyze the observed false
positives and where possible false negatives. A false positive
is a reported mismatch that is not observable in the browsers
and as such not relevant for cross-browser testing, i.e., the
setD Cin Figure 2. A false negative is an observable
mismatch that is undetected (set C D).
6.3 Results
Table 3 shows the results of our case studies. The trace-
level mismatches column presents the sum of unmatched
edges in both state graphs, i.e., edges in one state graph
that have no counterpart in the other. The next column
displays the number of false positives for the reported un-matched edges. The screen-level mismatches column shows
the percentage of the screen pairs from the two models that
were reported to be potentially matched at the trace-level
but have dierence at the screen-level, i.e., DOM-level. The
next column reports the percentage of false positives. For
such mismatched screen pairs, the table also shows the av-
erage number of DOM-level dierences that are detected by
our tool and XDiff . Note that the number is represent-
ing all the dierences (e.g., at the node, attribute, children
level) that need to be resolved, before the two DOM trees
(and hence the screen pair) could be seen as a match.
Trace-level Mismatches. The trace-level comparison
produced no false negatives and only 2 false positives (on
Ind-v2 ). The false positives are due to the fact that the
edge labels could not be matched deterministically: no per-
sistent ID attributes were present and the combination of
the attribute names/values and XPath expressions did not
suce. The 5 mismatches correctly reported for Ind-v2 are
caused by a screen-level failure, which is discussed below
under Screen-level Mismatches .
As an example of correctly detected trace-level mismatches,
Figure 3 depicts the state graphs produced for the Orga-
nizer in Chrome and Firefox. As it can be seen, there are
7 state transitions (shown in bold with label `IMGid:logo')
that lead to state 7 (shown in red) that are present in the
Chrome version but are missing from the graph in Firefox.
All these 7 transitions are caused by a click on the logoff
tab in the application. The reason for this dierence, is
that the logoff JavaScript function associated with the
onclick attribute of the corresponding DOM element is not
executed in Firefox. Thus, after clicking on the element,
there is no state change and hence no transition detected.
TheCnn case is interesting as well, since the missing
traces are caused by a bug in the Firefox instance,11which
results in many of the main content panes never nishing
loading. The trace-level mismatches are mainly due to the
transitions originating from content loaded into the pane,
which are present in the IE graph but are missing in the
Firefox one. The mismatches in the Tmms web application
are due to the functionality being totally broken in Chrome,
since many of the clickable elements do not cause any state
changes. In fact, after lling in the form, clicking on the sub-
mit button merely clears all the lled input elds instead of
submitting the form.
Screen-level Mismatches. Screen-level mismatches
are typically more dicult to assess than trace-level dier-
ences. The reason is that a screen-level mismatch could
be caused by a series of DOM-level dierences, and since
the number of DOM-level dierences could be large, despite
our abstraction mechanisms, more eort is required to check
whether a reported mismatch is a relevant observable one.
The rst observation is that our tool outperforms XD-
iff, which represents the current industrial practice, in con-
straining the number of DOM-level dierences to be exam-
ined (reductions of up to 37 times in Ind-v2 ) and reducing
the number of screen-level dierences and false positives (re-
ductions of up to 4.5 times in Cnn).
Figure 5 shows one instance of the detected DOM-level
mismatches in Ind-v1 , taken from our output visualization
plugin. In this case, the class attribute which is on the an-
chor tag in Firefox, has been moved into a newly added tag
element, namely a LABEL in Chrome. This detected DOM-
level dierence is observable in the two browsers as depicted
in Figure 6, i.e., the menu icons are missing in Chrome. This
type of behavior is the root cause of many DOM-level dier-
11https://bugzilla.mozilla.org/show_bug.cgi?id=463639 
	



	


	


	
 		

	
   	

   	


!


	

 	

 	
  			

!
	

	

 	

 	
  		
!
	



	

 	

 	
  		
!
	



	

 	

 	
  		
!
	



	

 	

 " 	
  		 "
 	
#"
 $"
!
	



	

 	

" 	
  			

	
#"$"
 
	



	


	


	

	
 	

   	


!


	

 	

 	
	

!
	

	

 	

 	
 
!
	



	

 	

	

!
	



	

 	

 	

!
	



	

 	

 " 	
  "
 	
#"
 $"
!
	



	

 	

 " 	
	

	
#" $"Figure 3: Generated Organizer graphs in Chrome (top) and Firefox (bottom) with the detected dierences.
% The Organizer 's DOM in Chrome %
<FORM class =" cssmain " id=" contactcreateshow " ... >
<TABLE class =" cssmain ">
<INPUT id=" contactcreateshow_createddt " .../ >
% The Organizer 's DOM in Firefox %
<FORM class =" cssmain " id=" contactcreateshow " ... >
<INPUT id=" contactcreateshow_createddt " .../ >
<TABLE class =" cssmain ">
Figure 4: Example of the Organizer's DOM dier-
ences in Chrome (top) and Firefox (bottom).
ences we witness in Ind-v1 , propagated to all the instances
of the screen-level mismatches.
As far as Ind-v2 is concerned, an interesting instance of
screen-level mismatch is manifested in the way some of the
IFrames' (used as content panes) internal content simply
disappears after a series of clicks in Firefox. This is caused
by the way the style attributes are dynamically added and
removed through JavaScript to the corresponding parent
element ( DIV) on the DOM tree.
A typical example of false positives, and the diculty of
resolving them, is shown in Figure 4, which presents DOM-
level dierences for the same FORM element in Chrome and
Firefox. Here, the INPUT element resides outside the TABLE
in Firefox, and is pulled in the TABLE in Chrome. Although
in this case, there is no clear observable dierence, it is very
dicult to discard such dierences automatically, without
adversely aecting the false negative rate.
7. DISCUSSION
Scope. Cross-browser compatibility spans a wide range
of issues from purely visual `look-and-feel' dierences to lack
of functionality and availability in dierent browsers. Lack
of functionality is believed to be a much more severe problemsince it can easily result in loss of revenue. Ideally, each web
application should behave exactly in the same manner re-
gardless of the rendering browser. Our premise in this work
is that cross-browser dierences, that inuence the function-
ality and usability of a web-application, do exist in practice.
Further, many of these dierences manifest themselves as
dierences in the client-side state of the application under
dierent browsers. These dierences can be captured in an
automated way by using one browser's behavior as an oracle
to test another browser's output (and vice versa) to detect
mismatched transitions and screens. The results of our case
studies validate these claims.
Note that the present implementation only uses the DOM
portion of the client-state for analysis but could easily be
extended to extract dierences based on CSS properties or
even JavaScript variable values. Our current approach
does not target CBC issues caused by dierences that re-
sult from dierent browsers' dierent rendering of identical
web content, i.e. issues that never result in any dierences
in the programmatic client-side state or traces thereof. This
would be an interesting area for future work.
Automation Level. Manual crawling or the record-
and-replay approach embodied by tools such as Selenium12
are other alternatives to the model capture phase of our
approach. However, we believe that Crawljax represents
a more automated, scalable and robust solution to behav-
ior exploration and integrates very well with the subsequent
equivalence checking.
As far as the manual eort is concerned, the main task of
the tester consists of (1) providing the URL of the deployed
web application (2) setting the crawling specications [22,
23] and (3) choosing the desired web browser types for com-
patibility testing, which usually takes a few minutes. The
state exploration and comparison steps are fully automatic.
12http://seleniumhq.orgFigure 5: Detected Ind-v1 DOM-level dierences in Chrome (left) and Firefox (right).
Figure 6: Observable dierences of Ind-v1's Menu
in Chrome (left) and Firefox (right).
Note that it is possible for the tester to tune the screen-level
comparison metrics manually if desired. This optional step
includes for instance specifying which DOM-level dierences
can be ignored to reduce false positives. The time needed
for the actual crawling and generation is fully dependent
on the required state space and the crawling settings. In
our experiments, the longest model generation case was for
Ind-v2 , which took around 28 minutes (for two browsers)
to explore and generate 400 states. The graph comparison
phase is quite fast, e.g., it took less than 45 seconds for Ind-
v2. It requires no manual eort if the default settings are
used. In case custom DOM elements or attributes need to
be ignored by the comparison method, they obviously have
to be provided, which in our case studies was not needed. In
short, the actual model generation and comparison requires
very limited manual eort. Although we do provide some
degree of visualization of the detected mismatches to make
it easier, assessing the correctness of the output is of course
still a manual task.
Completeness. While our proposed technique can ex-
pose several interesting and important cross-browser issues
in a given web application it cannot expose all of them. This
is limited on the one hand by the scope of the algorithm it-
self (discussed above) and on the other hand by the fraction
of application behaviors covered during the crawling. Obvi-
ously, the number of cross-browser issues exposed co-relates
to the number of behaviors covered by the crawling. For
the purposes of this investigation we used simple resource
bounds to limit the crawling. This served us well since typ-
ically the same cross-browser issues appear repeatedly on
dierent states and under dierent application behaviors.
Thus, a complete or comprehensive crawl of the applica-
tion may not even be necessary to expose key cross-browser
issues. In any case, optimizing, guiding or bounding the
crawling to specically target cross-browser issues are inter-esting avenues of future research, in their own right, but are
beyond the scope of this paper.
Correctness. The most conclusive outcome from the
evaluation is that within the automatically explored state
space, our technique is capable of detecting trace-level mis-
matches with a high level of certainty. As per Theorem 1,
Algorithm 4.2 guarantees a zero false positive and false neg-
ative error rate, at the trace-level, provided edge-labels in
the state graphs have been matched (See our formal proof
3). Our label-matching algorithm (Section 5) is quite suc-
cessful in achieving this in practice (no false negatives and
merely 2 false positives). Detected screen-level mismatches
are also reasonably accurate, with the false positive rate
ranging from 12% for Cnn (lowest) to 37% for Ind-v2 (high-
est).
As far as screen-level false negatives are concerned, for the
Organizer andTmms we can claim that there were no false
negatives. For the other subjects, because of the large state
space we are not able to present any concrete numbers, i.e.,
it is dicult to claim that we have detected all the possi-
ble CBC issues, since that requires manually examining and
comparing a couple of hundred states in dierent browsers.
Catalog of Browser DOM Dierences. Ideally, we
would have a comprehensive list of all DOM-level dierences
that are merely bound to dierent browser rendering and
do not have any observable side-eects. The more complete
this list, the more accurate our approach will be in terms
of reducing the number of false positives. Unfortunately no
o-the-shelf catalog exists today. This is a learning process,
which requires examining the web applications at hand and
documenting the noticeable dierences. Our catalog is in-
deed growing with every new case study we conduct.
Non-deterministic and Dynamic Behavior. Com-
paring tree-structured data such as HTML, is known to be
a dicult problem in web-based regression testing [28, 29],
even within the same browser, because of the dynamic and
non-deterministic behavior in modern web applications [27].
Some of the false positives in our results are due to precisely
this reason. In this work, we have presented the extra chal-
lenges involved in dealing with the dierent ways browsers
handle and render DOM changes, from the initial HTML
page through subsequent DOM updates via JavaScript .
Recent solutions proposed, for instance in [27], could help
resolve some of the false positives we currently report.
Browser Sning. Browser sning is a technique used
to determine the web browser of the visitor in order to serve
content in a browser-specic manner. An interesting pattern
we have witnessed in our case studies is when a JavaScript
library treats one browser as the default and uses if-else
statements for other specic browsers to account for CBC
issues. The problem starts when a user is employing a br-
owser not tested for and the default content is generated by
the library. The CBC problem depicted in Figures 5 and 6 isan example of this type of failure. Our technique is capable
of correctly spotting all instances of such dierences.
Threats to Validity. Some of the issues inuencing the
external validity of our results have been addressed in the
above discussion. Although the number of web applications
chosen in our evaluation is limited, we believe they are repre-
sentative of modern web applications present in open-source
and industrial environments. As far as the internal validity
is concerned, since we use external libraries that act as wrap-
pers around native browsers, our evaluation results could be
aected by implementation dierences. We have tried to
mitigate this threat by extensively testing the browsers and
their crawling behavior, for the model generation part.
8. CONCLUDING REMARKS
In this paper we have proposed an automated method for
cross-browser compatibility testing of modern web applica-
tions. The results of our evaluation, on several open-source
and industrial web applications, clearly point to the ubiquity
of the problem. They also demonstrate the ecacy of our
approach in automatically exposing relevant cross-browser
issues at a trace-level as well as at a screen-level.
Our current approach can be enhanced in several ways.
It can immediately benet from a larger catalog of known
DOM-level, cross-browser dierences that are merely bound
to dierent browser rendering and have no observable side
eects. This would directly reduce the screen-level false pos-
itive rate and improve the precision of our approach. An-
other useful direction would be some automated method of
detecting observable cross-browser dierences, not reected
at DOM level. In concert with our tool, such a method could
provide a very potent cross-browser compatibility tester for
the end-user.
9. REFERENCES
[1] Acid 3: Wikipedia. Retrieved 4 Mar 2010.
http://en.wikipedia.org/wiki/Acid3 .
[2] Acid Tests - The Web Standards Project.
http://www.acidtests.org .
[3] BrowserCam. http://www.browsercam.com .
[4] BrowserCamp. http://www.browsrcamp.com .
[5] Adobe BrowserLab. https://browserlab.adobe.com .
[6] Browser Photo. http://www.netmechanic.com/
products/browser-index.shtml .
[7] BrowserShots. http://browsershots.org .
[8] S. R. Choudhary, H. Versee, and A. Orso. Webdi:
Automated identication of cross-browser issues in
web applications. In Proc. of the 26th IEEE Int. Conf.
on Softw. Maintenance (ICSM'10) , pages 1{10, 2010.
[9] CrossBrowserTesting.
http://www.crosbrowsertesting.com .
[10] J. Dolson. What is \Cross-browser compatibility?".
http://www.joedolson.com/articles/2008/03/
what-is-cross-browser-compatibility/ .
[11] C. Eaton and A. M. Memon. An empirical approach to
testing web applications across diverse client platform
congurations. Int. Journal on Web Engineering and
Technology (IJWET) , 3(3):227{253, 2007.
[12] J. J. Garrett. Ajax: A new approach to web
applications. http://www.adaptivepath.com/
publications/essays/archives/000385.php ,
February 2005.
[13] J. E. Hopcroft and J. D. Ullman. Introduction to
Automata Theory, Language and Computation .
Addision-Wesley, 1979.[14] ieCapture. http://www.iecapture.com .
[15] IE NetRenderer. http://ipinfo.info/netrenderer/ .
[16] IETester. http:
//www.my-debugbar.com/wiki/IETester/HomePage .
[17] iPhoney. http:
//sourceforge.net/projects/iphonesimulator/ .
[18] E. Kiciman and B. Livshits. AjaxScope: A platform
for remotely monitoring the client-side behavior of
Web 2.0 applications. In Proc. 21st Symp. on
Operating Sys. Principles (SOSP'07) , pages 17{30,
2007.
[19] B. Kitchenham, L. Pickard, and S. L. Peeger. Case
studies for method and tool evaluation. IEEE Softw. ,
12(4):52{62, 1995.
[20] Litmus. http://litmusapp.com .
[21] A. M. Memon, I. Banerjee, and A. Nagarajan. GUI
ripping: Reverse engineering of graphical user
interfaces for testing. In Proc. of The 10th Working
Conf. on Reverse Engineering , Nov. 2003.
[22] A. Mesbah, E. Bozdag, and A. van Deursen. Crawling
Ajax by inferring user interface state changes. In Proc.
8th Int. Conference on Web Engineering (ICWE'08) ,
pages 122{134. IEEE Computer Society, 2008.
[23] A. Mesbah and A. van Deursen. Invariant-based
automatic testing of Ajax user interfaces. In Proc. 31st
Int. Conference on Software Engineering (ICSE'09) ,
pages 210{220. IEEE Computer Society, 2009.
[24] R. Ramler, E. Weippl, M. Winterer, W. Schwinger,
and J. Altmann. A quality-driven approach to web
testing. In Proceedings of 2nd Conf. on Web
Engineering (ICWE'02) , 2002.
[25] F. Ricca and P. Tonella. Web testing: a roadmap for
the empirical research. In Proc. Int. Symp. on Web
Site Evolution (WSE '05) , pages 63{70, 2005.
[26] J. Rode1 and M. A. P.-Q. Mary Beth Rosson. The
Challenges of Web Engineering and Requirements for
Better Tool Support. Technical Report TR-05-01,
Computer Science Department, Virginia Tech., 2002.
[27] D. Roest, A. Mesbah, and A. van Deursen. Regression
testing Ajax applications: Coping with dynamism. In
Proc. 3rd Int. Conference on Softw. Testing,
Verication and Validation (ICST'10) , pages 128{136.
IEEE Computer Society, 2010.
[28] E. Soechting, K. Dobolyi, and W. Weimer. Syntactic
regression testing for tree-structured output. In Proc.
Int. Symp. on Web Systems Evolution (WSE'09) .
IEEE Computer Society, 2009.
[29] S. Sprenkle, E. Gibson, S. Sampath, and L. Pollock.
Automated replay and failure detection for web
applications. In Proc. 20th Int. Conference on Aut.
Softw. Eng. (ASE'05) , pages 253{262. ACM, 2005.
[30] SputnikTests.
http://code.google.com/p/sputniktests/ .
[31] Wikipedia. List of web browsers. http:
//en.wikipedia.org/wiki/List_of_web_browsers .
[32] Xenocode Browser Sandbox.
http://spoon.net/browsers/ .
[33] Q. Xie and A. M. Memon. Designing and comparing
automated test oracles for GUI-based software
applications. ACM Trans. Softw. Eng. Methodol. ,
16(1):4, 2007.
[34] F. W. Zammetti. Practical Ajax projects with Java
technology . Apress, 2006.