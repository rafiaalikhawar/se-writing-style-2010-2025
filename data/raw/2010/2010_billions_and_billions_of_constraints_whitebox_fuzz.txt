See discussions, st ats, and author pr ofiles f or this public ation at : https://www .researchgate.ne t/public ation/255563498
Billions and Billions of Constraints: Whitebox Fuzz Testing in Production
Article    in  Proceedings - Int ernational Conf erence on Softw are Engineering  · Januar y 2012
DOI: 10.1109/IC SE.2013.6606558
CITATIONS
131READS
513
3 author s, including:
Ella Bounimo va
Micr osoft
8 PUBLICA TIONS    618 CITATIONS    
SEE PROFILE
All c ontent f ollo wing this p age was uplo aded b y Ella Bounimo va on 10 June 2014.
The user has r equest ed enhanc ement of the do wnlo aded file.Billions and Billions of Constraints:
Whitebox Fuzz Testing in Production
Ella Bounimova Patrice Godefroid David Molnar
Abstract
We report experiences with constraint-based whitebox
fuzz testing in production across hundreds of large Win-
dows applications, multiple Microsoft product releases,
and over 400 machine years of computations from 2007
to 2012. Whitebox fuzzing leverages symbolic execution
on binary traces and constraint solving to construct new
inputs to a program. These inputs execute previously un-
covered paths or trigger security vulnerabilities. White-
box fuzzing has found one-third of all ﬁle fuzzing bugs
during the development of Windows 7, saving millions of
dollars in potential security vulnerabilities. We present
two new systems developed to manage our deployments:
SAGAN , which collects data from every single fuzzing
run for further analysis, and JobCenter, which controls
deployment of our whitebox fuzzing infrastructure across
commodity virtual machines. Since June 2010, SAGAN
has recorded over 3.4 billion constraints solved, millions
of symbolic executions, and tens of millions of test cases
generated. Our work represents the largest scale deploy-
ment of whitebox fuzzing, including the largest compu-
tational usage ever for a Satisﬁability Modulo Theories
(SMT) solver, to date. We highlight speciﬁc improve-
ments to whitebox fuzzing driven by our data collection
and open problems that remain.
1 Introduction
Fuzz testing is the process of repeatedly feeding modi-
ﬁed inputs to a program in order to uncover security bugs,
such as buffer overﬂows. First introduced by Miller et
al. [20], traditional blackbox fuzzing is a form of random
testing where an initial seed input is randomly mutated to
generate new fuzzed inputs. While simple, the technique
has been shown to be startingly effective, as thousands of
security vulnerabilities have been found this way. At Mi-
crosoft, fuzz testing is now required by the Security De-
velopment Lifecycle for code that may handle untrusted
input.
Unfortunately, random testing often misses bugs that
depend on speciﬁc trigger values or special structure toan input. More recently, advances in symbolic execution,
constraint generation and solving have enabled whitebox
fuzzing [18], which builds upon recent advances in sys-
tematic dynamic test generation [16, 5], and extends its
scope from unit testing to whole-program security test-
ing. Starting with a well-formed input, whitebox fuzzing
consists of symbolically executing the program under test
dynamically , gathering constraints on inputs from condi-
tional branches encountered along the execution. Each of
those constraints are then negated and solved with a con-
straint solver , whose solutions are mapped to new inputs
that exercise different program execution paths. This pro-
cess is repeated using systematic state-space search tech-
niques , inspired by model checking , that attempt to sweep
through as many as possible feasible execution paths of
the program while checking simultaneously many proper-
ties using a runtime checker (such as Purify, Valgrind or
AppVeriﬁer).
In this paper, we report on the usage of whitebox
fuzzing on a very large scale for the ﬁrst time. Earlier
applications of dynamic test generation focused on unit
testing of small programs [16, 5, 24], typically consist-
ing of a few thousand lines of code, for which these tech-
niques were able to achieve high code coverage and ﬁnd
new bugs, for instance, in Unix utility programs [4] or de-
vice drivers [7]. While promising, this prior work did not
report of any daily use of these techniques and tools.
In contrast, we present here our experience running
whitebox fuzzing on a much larger scale and in produc-
tionmode. Our work follows from one simple but key
observation: the “killer app” for dynamic test generation
iswhitebox fuzzing of ﬁle parsers . Many security vulner-
abilities are due to programming errors in code for pars-
ing ﬁles and packets that are transmitted over the internet.
For instance, the Microsoft Windows operating system in-
cludes parsers for hundreds of ﬁle formats. Any security
vulnerability in any of those parsers may require the de-
ployment of a costly visible security patch to more than a
billion PCs worldwide, i.e., millions of dollars.
Today, our whitebox fuzzer SAGE is now running ev-
ery day on an average of 200machines, and has been
1running for over 400machine years since 2008 . In the
process, it has found many previously-unknown security
vulnerabilities in hundreds of Microsoft applications, in-
cluding image processors, media players, ﬁle decoders,
and document parsers. Notably, SAGE found roughly one
third of all the bugs discovered by ﬁle fuzzing during the
development of Microsoft’s Windows 7. Because SAGE
is typically run last, those bugs were missed by every-
thing else, including static program analysis and blackbox
fuzzing.
Achieving this milestone required facing key chal-
lenges in scalability across several dimensions:
Symbolic execution: how to efﬁciently perform sym-
bolic execution on x86 execution traces with hun-
dreds of millions of instructions and tens of thou-
sands of symbolic variables for applications with
millions of lines of code (like Microsoft Excel).
Constraint generation and solving: how to generate,
solve and manage billions of constraints.
Long-running state-space searches: how to perform
systematic state-space explorations (akin model
checking) effectively for weeks or months at a time.
Diversity: how to easily conﬁgure, check and monitor
whitebox fuzzing so that it is applicable to hundreds
of diverse applications.
Fault tolerance and always-on usage: how to manage
hundreds of machines running whitebox fuzzing
24/7 with as little down-time as possible.
While the ﬁrst challenge was previously discussed in [18],
we describe for the ﬁrst time how to address the other four
challenges.
In addition, we faced challenges from running in pro-
duction , instead of in a research environment. Speciﬁ-
cally, large product groups at Microsoft, such as the Win-
dows and Ofﬁce divisions, have large fuzzing labs of ma-
chines allocated to fuzz testing. Over the last few years,
our whitebox fuzzer was gradually deployed on a larger
and larger scale in those labs which were not under our
control. Indeed, our internal customers were not man-
dated to use this new technology. Instead, we had to pro-
gressively gain their trust and business, by consistently
ﬁnding new security-critical bugs in many applications,
and missed by other competing fuzzers, while growing
our initial research prototype to a full-ﬂedged large scale
operation over several years. For this, we had to gain a
better understanding of how our whitebox fuzzer is being
used and conﬁgured, as well as monitor its progress, while
addressing the scalability challenges listed above and con-
tinually improving our tool and infrastructure.We have developed and deployed two new systems
to help meet these challenges: SAGAN and JobCenter .
SAGAN is a monitoring system which records informa-
tion from every SAGE run and displays it via a Web site,
making it easy to drill down into the progress of a run.
Since June 2010 , SAGAN has recorded over 3.4billion
constraints solved, millions of symbolic executions, and
tens of millions of test cases generated for hundreds of
applications.
JobCenter is a control system which can auto-assign
SAGE jobs to machines as they become available and
monitor progress of each run. JobCenter also helps us
manage the complexity of different conﬁgurations re-
quired for different programs under test.
Our infrastructure enables data-driven improvement ,
where feedback from previous runs helps us focus lim-
ited resources on further research and improve future runs.
In Section 4, we demonstrate this with analyses enabled
by SAGAN data that have led directly to changes in our
whitebox fuzzing practice.
This paper is organized as follows. In Section 2, we
review whitebox fuzzing and SAGE . In Section 3, we
present our two new systems, SAGAN and JobCenter,
and their main features. Then we present in Section 4
several original analyses on the performance of whitebox
fuzz testing for runs of multiple weeks on many different
programs. Each of those analyses were designed to lead
to concrete actionable items that led to further improve-
ments in our tool and infrastructure. We discuss other re-
lated work in Section 5, before concluding in Section 6.
2 Background: Whitebox Fuzzing
2.1 Blackbox Fuzzing
Blackbox fuzzing is a form of blackbox random test-
ing which randomly mutates well-formed program inputs,
and then tests the program with those modiﬁed inputs [13]
with the hope of triggering a bug like a buffer overﬂow.
In some cases, grammars are used to generate the well-
formed inputs, which also allows encoding application-
speciﬁc knowledge and test-generation heuristics.
Blackbox fuzzing is a simple yet effective technique
for ﬁnding security vulnerabilities in software. Thou-
sands of security bugs have been found this way. At Mi-
crosoft, fuzzing is mandatory for every untrusted interface
of every product, as prescribed in the “Security Develop-
ment Lifecycle” [8] which documents recommendations
on how to develop secure software.
Although blackbox fuzzing can be remarkably effec-
tive, its limitations are well-known. For instance, the
then branch of the conditional statement in
2int foo(int x) {// x is an input
int y = x + 3;
if (y == 13) abort(); // error
return 0;
}
has only 1 in 232chances of being exercised if the input
variable xhas a randomly-chosen 32-bit value. This in-
tuitively explains why blackbox fuzzing usually provides
low code coverage and can miss security bugs.
2.2 Whitebox Fuzzing
Whitebox fuzzing [18] is an alternative approach, which
builds upon recent advances in systematic dynamic test
generation [16, 5], and extends its scope from unit testing
to whole-program security testing. Starting with a well-
formed input, whitebox fuzzing consists of symbolically
executing the program under test dynamically , gathering
constraints on inputs from conditional branches encoun-
tered along the execution. The collected constraints are
then systematically negated and solved with a constraint
solver, whose solutions are mapped to new inputs that ex-
ercise different program execution paths. This process is
repeated using search techniques that attempt to sweep
through all (in practice, many) feasible execution paths of
the program while checking simultaneously many proper-
ties using a runtime checker.
For example, symbolic execution of the above program
fragment with an initial value 0for the input variable x
takes the else branch of the conditional statement, and
generates the path constraint x + 3 /negationslash=13. Once this
constraint is negated and solved, it yields x=10, which
gives us a new input that causes the program to follow the
then branch of the conditional statement. This allows
us to exercise and test additional code for security bugs,
even without speciﬁc knowledge of the input format. Fur-
thermore, this approach automatically discovers and tests
“corner cases” where programmers may fail to properly
allocate memory or manipulate buffers, leading to secu-
rity vulnerabilities.
In theory, systematic dynamic test generation can lead
to full program path coverage, i.e., program veriﬁcation .
In practice, however, the search is typically incomplete
both because the number of execution paths in the pro-
gram under test is huge, and because symbolic execution,
constraint generation and constraint solving can be impre-
cise due to complex program statements (pointer manipu-
lations, ﬂoating-point operations, etc.), calls to external
operating-system and library functions, and large num-
bers of constraints which cannot all be solved perfectly# instructions executed 1,455,506,956
# instr. executed after 1st read from ﬁle 928,718,575
# constraints generated (full path constraint) 25,958
# constraints dropped due to cache hits 244,170
# constraints dropped due to limit exceeded 193,953
# constraints satisﬁable (= # new tests) 2,980
# constraints unsatisﬁable 22,978
# constraint solver timeouts ( >5 secs) 0
symbolic execution time (secs) 2,745
constraint solving time (secs) 953
Figure 1: Statistics for a single symbolic execution of a
large Ofﬁce application with a 47 kilobyte input ﬁle.
in a reasonable amount of time. Therefore, we are forced
to explore practical tradeoffs .
2.3 SAGE
The basis of our work is the whitebox fuzzer SAGE [18].
Because we target large applications where a single exe-
cution may contain hundreds of millions of instructions,
symbolic execution is the slowest component. Therefore,
we use a generational search strategy to maximize the
number of new input tests generated from each symbolic
execution: given a path constraint, allthe constraints in
that path are systematically negated one-by-one, placed in
a conjunction with the preﬁx of the path constraint lead-
ing to it, and attempted to be solved by a constraint solver.
This way, a single symbolic execution can generate thou-
sands of new tests. (In contrast, a standard depth-ﬁrst or
breadth-ﬁrst search would negate only the last or ﬁrst con-
straint in each path constraint, and generate at most one
new test per symbolic execution.)
To give the reader an idea of the sizes of path con-
straints for large applications, Figure 1 shows some statis-
tics about a single sample symbolic execution of a large
Ofﬁce application while parsing an input ﬁle of about 47
kilobytes. For ﬁle parser fuzzing, each byte read off the
untrusted input ﬁle corresponds to a symbolic variable.
Our whitebox fuzzer uses several optimizations that are
crucial for dealing with such huge execution traces. These
optimizations are discussed later in Section 4.
Our whitebox fuzzer performs dynamic symbolic exe-
cution at the x86 binary level. It is implemented on top
of the trace replay infrastructure TruScan [23] which con-
sumes trace ﬁles generated by the iDNA framework [1]
and virtually re-executes the recorded runs. TruScan of-
fers several features that substantially simplify symbolic
execution, including instruction decoding, providing an
interface to program symbol information, monitoring var-
ious input/output system calls, keeping track of heap and
3Check for  
Crashes  
(AppVerifier)  Code  
Coverage  
(Nirvana)  Generate  
Constraints  
(TruScan)  Solve  
Constraints  
(Z3) Input0  Coverage  
Data  Constraints  
Input1  
 
 Input2  
… 
 
InputN  
 Figure 2: Architecture of SAGE .
stack frame allocations, and tracking the ﬂow of data
through the program structures. Thanks to off-line trac-
ing, constraint generation in SAGE is completely deter-
ministic because it works with an execution trace that cap-
tures the outcome of all nondeterministic events encoun-
tered during the recorded run. Working at the x86 binary
level allows our tool to be used on any program regardless
of its source language or build process. It also ensures
that“what you fuzz is what you ship” as compilers can
perform source code changes which may impact security.
2.4 SAGE Architecture
The high-level architecture of SAGE is depicted in Fig-
ure 2. Given an initial input Input0, SAGE starts by run-
ning the program under test with AppVeriﬁer to see if this
initial input triggers a bug. If not, SAGE then collects
the list of unique program instructions executed during
this run. Next, SAGE symbolically executes the program
with that input and generates a path constraint, charac-
terizing the current program execution with a conjunc-
tion of input constraints. Then, implementing a gener-
ational search, all the constraints in that path constraint
are negated one-by-one, placed in a conjunction with the
preﬁx of the path constraint leading to it, and attempted
to be solved by a constraint solver (we currently use the
Z3 SMT solver [9]). All satisﬁable constraints are then
mapped to Nnew inputs. These Nnew inputs are then
tested and ranked according to incremental instruction
coverage. For instance, if executing the program with new
Input1 discovers 100 new instructions, Input1 gets a score
of 100, and so on. Next, the new Input with the highest
score is selected to go through the (expensive) symbolic
execution task, and the cycle is repeated, possibly forever.
Note that all the SAGE tasks can be executed in parallelon a multi-core machine or even on a set of machines; we
discuss this in the next section.
3 Infrastructure
3.1 SAGAN
On a single machine, a multi-week whitebox fuzz testing
run can consume hundreds of gigabytes of disk, perform
thousands of symbolic executions and create many test
cases. Each task in the SAGE pipeline offers opportu-
nities for something to go wrong, potentially causing the
fuzz run to stop in its tracks. Moreover, each task also of-
fers room for improvements by observing statistics, start-
ing with the basic time taken in each task. When we ﬁrst
started running whitebox fuzzing at scale, we had no way
to capture this information, short of logging in remotely to
computers that had participated in fuzz runs. Even then,
to avoid running out of disk space, we typically conﬁg-
ured SAGE to delete intermediate results. As a result, we
had limited ability to detect failures and learn from our
test runs.
3.1.1 Principles
We designed a logging service called SAGAN to address
those issues. We developed the following key principles
when designing our logging.
•First, every run of SAGE creates a unique log. Even
if the run fails to start properly, we assign a globally
unique identiﬁer to the run. This allows us to unam-
biguously identify speciﬁc SAGE runs, aiding our
debugging and statistics gathering.
•Second, every log contains enough information to
reproduce the run , including all conﬁguration ﬁles
and command line options. This principle allows us
to quickly reproduce failing runs and search for the
cause of a problem. In the case of failed tasks, we
also send back stdout and stderr ﬁles, which helps us
diagnose previously unseen errors.
•Third, every log has a unique URL that exposes
the log information in a web browser. We have
found this simpliﬁes collaborative troubleshooting
and brainstorming over our next directions, because
we can simply e-mail relevant links back and forth.
We have also created a summary web front end that
shows recent runs and summary statistics, such as
number of crashing test cases found or number of
runs that have a certain percentage of tasks failing.
Using this front end, we can quickly check ongo-
ing lab runs and highlight those with failing tasks
or those which are ﬁnding many crashing test cases.
In cases where users of SAGE need to change their
4conﬁguration, we can then send them the URL for
the log and highlight the problem.
•Fourth, the logging infrastructure should be low im-
pact for the client. This starts with an infrastructure
that takes a small amount of overhead for each client.
Beyond this, it means that even if the logging server
fails, we ensure that the SAGE run can stll continue.
•Finally, the central logs contain enough information
for all analyses. Prior to SAGAN , we would per-
form analyses in support of our research by running
SAGE on a target of interest, then investigating ﬁles
on disk to gather statistics and test hypotheses. While
this could be scripted, it was necessarily time con-
suming because it could be done only on a machine
by machine basis. In addition because this operation
required running SAGE in a way that kept all inter-
mediate data, it necessarily was not the same as the
real test runs. With SAGAN , we can perform analy-
ses, including all the analyses reported in this paper,
without ever touching individual machines.
3.1.2 Architecture
We built the SAGAN logging service to support hundreds
of simultaneously active machines. A central server runs
Microsoft SQL Server and Internet Information Server on
Windows Server 2008 R2. Each of the client machines
makes a direct connection to the SQL Server to insert log
updates. Updates happen at the beginning of every SAGE
run, after every new crashing test case, after every failed
task, and then at random intervals ranging between 30to
45minutes during the run. We randomize the intervals for
updates to avoid synchronization of periodic messages, as
recommended by Floyd and Jacobson [12].
While the overall disk usage of a SAGE run can to-
tal hundreds of gigabytes, we applied our principles to
reduce the amount of information that must be shipped
to our central server. First, we limit the initial informa-
tion sent to the conﬁguration options and command line,
which are a few kilobytes each. Second, each heartbeat
contains counters representing the number of ﬁles created,
number of crashes found, coverage, and a small log ﬁle
from our whitebox fuzzer, which is also typically under
10kilobytes.This means that for a run with no failed tasks,
no constraint solver timeouts, and no crashing test cases
found, our space and bandwidth requirements are mod-
est. For constraint solver timeouts, while we do ship the
entire constraint to SAGAN for later analysis, we limit
the number of such constraints shipped on each run. Fi-
nally, while in the case of failed symbolic execution tasks
we may need to ship instruction traces in the hundreds of
megabytes, we probabilistically decide whether or not to
ship individual traces and ship at most 5such traces perrun.
3.1.3 Data Presentation
We present the data in two main ways. First, we designed
a web front end to expose information from the server.
Every run has its own unique URL that shows conﬁgura-
tion information, health, and number of crashes found at
a glance. By clicking on a link, we can drill down into
statistics from any of the symbolic execution tasks that
have completed during that fuzzing run. For example, we
can see how many constraints were generated and how of-
ten optimizations in constraint generation were invoked.
Second, we run SQL queries against the tables hold-
ing data from SAGE runs. This gives us the ﬂexibility to
answer questions on the ﬂy by looking at the data. For
example, we can create a list of every SAGE run that
has at least one symbolic execution task where more than
10queries to Z3 timed out. We use this capability to
work with our partners and understand if there are spe-
ciﬁc features of programs that might cause long-running
constraint solver queries or other strange behavior. All the
analyses in Section 4 were performed using our infrastruc-
ture.
3.2 JobCenter
The SAGAN system gives us insight, but it is only one
piece of the puzzle. We need an active control system for
the machines running SAGE . In particular, the programs
on which we test SAGE are not static. While SAGE is
running, developers are continually updating the code, ﬁx-
ing bugs and adding new features. Periodically we must
upgrade the programs SAGE tests to the most recent ver-
sion, to ensure that the bugs SAGE ﬁnds are most likely
to reproduce on developers’ machines and have not been
ﬁxed already.
3.2.1 Conﬁguration Management
We run SAGE against multiple conﬁgurations. A conﬁg-
uration consists of a set of initial test cases, or seed ﬁles ,
a target program for execution with its arguments, and a
set of parameters that deﬁne timeout values for each of
the different SAGE tasks. One program of interest may
have many conﬁgurations. For example, a single large
program may have parsers embedded for many different
ﬁle formats. Together with our partners who run SAGE
, we have deﬁned hundreds of distinct SAGE conﬁgura-
tions in use today.
Manually starting SAGE with correct conﬁgurations
on hundreds of machines would be a nightmare. We de-
signed a control service called JobCenter that automates
this work, in conjunction with additional infrastructure
created by different partners of SAGE . Our partners typ-
5ically have infrastructure that can automatically upgrade a
machine to the latest build of the software under test and
reset the disk to a clean state. JobCenter then deploys the
right version of SAGE and the right conﬁguration to start
the fuzz testing run. While we still need manual effort to
determine if a conﬁguration has the right parameters and
options, JobCenter allows us to do this work once and re-
use it across multiple machines automatically.
3.2.2 Control and Recovery
We have made changes to SAGE that integrate it with
JobCenter for runtime control. A JobCenter web service
allows changing conﬁguration values on the ﬂy, which we
use for ﬁne-tuning and experimentation. For example, we
can change how long to wait while tracing the execution
of the test program. We can also pause a SAGE job, al-
lowing us to upgrade the version of SAGE used or per-
form in depth analysis. We have implemented facilities
for controlling multiple jobs at once, as well.
JobCenter can detect when a SAGE run has terminated,
then trigger a request to JobCenter for a new conﬁgura-
tion. This means that even if a particular conﬁguration
is faulty and leads to a paused run, we can try to recover
and continue to use the machine; we developed this after
looking at the utilization data in Section 4. We currently
have at least 90concurrent virtual machines reporting to
JobCenter at any given time.
Finally, our test machines run Windows and typically
need periodic reboots for security patches. Power outages,
servicing, or other events may also cause unexpected re-
boots. We have modiﬁed SAGE to persist run informa-
tion to disk in a way that ensures we can pick up after
such an event when the machine comes back up. The
JobCenter remembers which conﬁguration is associated
with a machine and on machine boot can re-start an inter-
rupted whitebox fuzzing run. Prior to this, we had difﬁ-
culty achieving runs of more than a month in length. Fig-
ure 3 shows the overall architecture, with VMs running
SAGE talking to JobCenter and SAGAN .
3.3 Task Partitioning
As we discussed in Section 2, a SAGE fuzzing run con-
sists of four different types of tasks. In principle, these
tasks could be run on separate machines. For example,
we could perform all tracing on one machine, then send
the resulting instruction traces to a second machine for
symbolic execution, forward the resulting constraints to
another machine. Finally, we could run the newly created
tests and measure coverage on a yet another machine.
In our runs, we typically keep all tasks on the same
machine. We do this for two reasons. First, because the
instruction-level traces we create are often hundreds of
Figure 3: Architecture for JobCenter and SAGAN . Ma-
chines running SAGE communicate to SAGAN for log-
ging and JobCenter for control. Crashing test cases are
placed on a network share, where they are picked up by
the Distributed File Fuzzer test farm. This test farm ap-
plies automatic triage to identify likely bugs, which hu-
mans then review and ﬁle in a bug database.
megabytes in size, which means we would incur delays
in moving them between machines. We do not have the
ability to control the network infrastructure used by our
partners, so we cannot assume fast links between pairs of
machines. Second, as we will see in Section 4, in our ex-
perience most constraints are solved within a tenth of a
second, meaning that it is cheaper to solve them locally
on the machine than to serialize, send over the network,
deserialize, and solve.
In Ofﬁce, however, our partners have developed an
infrastructure for leveraging “nights and weekends” of
machines deployed around the company to test ﬁles for
crashes [14]. To leverage this infrastructure, which can
test hundreds of thousands of ﬁles in hours, we have a
mode where SAGE copies crashing test cases on a net-
work share for later processing. This infrastructure also
performs an additional layer of triage, identifying bugs
that may have already been ﬁled, and prioritizing likely
new bugs in front of users.
4 Data-Driven Whitebox Fuzzing
We now describe several speciﬁc analyses enabled by
SAGAN data collected from runs of SAGE at scale. For
each analysis, we describe improvements made to our
fuzzing practice, or key insights that suggest future di-
rections for whitebox fuzzing. The data we present spans
6several major sessions which each consist of hundreds of
individual executions of the SAGE tool, each SAGE run
itself fuzzing a different application during typically two
to four weeks on a dedicated multi-core machine.
We note that the data analyzed here has been collected
not from controlled experiments, but from production test
runs. We did not choose which applications to test in each
session, the length of the run, or the speciﬁc conﬁguration
settings. This means that sometimes, the data for different
sessions might be more difﬁcult to compare. Because the
data that are being collected are so diverse, however, we
were able to gain valuable feedback to track issues and
evaluate improvements.
4.1 Utilization and Failure Detection
The ﬁrst key analysis is monitoring utilization and de-
tecting failures in SAGE deployments. First, we can
determine failing runs by monitoring heartbeats. Before
SAGAN , we had no way to know how the runs pro-
gressed, meaning that runs could die, perhaps due to a
wrong test setup or running out of disk space, and it would
not be apparent until we examined the results, which
could be weeks later.
Using data from SAGAN , we improved our lab-
machine utilization over time. We show here data for three
successive sessions. In the ﬁrst session with SAGAN , we
were able to detect that many of the machines died unex-
pectedly, as shown on the leftmost chart of Figure 4. We
then used the conﬁguration ﬁles sent back by SAGAN to
diagnose potential failures, then develop a ﬁx. As a result,
we saw improved utilization in the second and third major
sessions, as shown on the middle and right charts of Fig-
ure 4. In another instance (data not shown), after several
hundreds of runs had started (and were expected to run for
at least three weeks), we were able to detect within hours
that all of the symbolic execution tasks were failing due
to a temporary networking problem that happened during
the setup of the session. We then corrected the problem
and re-started the session.
Second, besides obvious symptoms of a failing run, we
check data on SAGAN that indicate how the run is pro-
ceeding. In particular we check whether the run gener-
ated any new tests, whether the symbolic execution task
detected symbolic inputs, and how many bugs have been
found. This in turn helps detect conﬁguration errors. For
example, if the timeout for tracing the application is set
too low, then the trace will end before the application
even reads from an input ﬁle. We then see this show up
in SAGAN as a large percentage of symbolic execution
tasks that fail to detect any symbolic inputs. For runs
with problems, the JobCenter infrastructure allows adjust-
ing conﬁguration parameters of the run. In this example,we can increase the maximum time allowed for creating
the execution trace to solve the problem.
SAGAN data also showed us that in the ﬁrst and sec-
ond sessions, 7% and 3% of all SAGE executions died
due to the machines exhausting disk space. We modiﬁed
SAGE to remove non-critical ﬁles automatically during
low disk space conditions. All subsequent runs have had
0% failures due to low disk space.
We have also used SAGAN to detect which SAGE
tasks have failed most, and why. For example, in one
session, 62% of the about 300SAGE runs had failures
in symbolic execution which did not appear in the previ-
ous session. When we analyzed SAGAN data about these
failures, we found a common problem and traced it to the
changes in the compiler generating the binaries under test.
Working with our partners, we ﬁxed the problem, and in
the next session, we had only 11% of all SAGE runs fail-
ing due to symbolic execution failures, with none of these
remaining failures due to this compiler change.
4.2 Unique Bugs Found by Day
We also investigated when unique bugs are detected over
the course of sessions. Our whitebox fuzzer SAGE uses
AppVeriﬁer conﬁgured to check for heap errors. Each
memory corruption found (such as buffer overﬂow) is
mapped to a crash. Because SAGE can generate many
different test cases that exhibit the same bug, we “bucket”
crashing ﬁles by the stack hash of the crash, which in-
cludes the address of the faulting instruction. It is pos-
sible for the same bug to be reachable by program paths
with different stack hashes for the same root cause. Our
experiments always report the distinct stack hashes.
We collected earliest detection timestamps for each
bucket found during a session. Figure 5 presents this
chart for a session over two hundred programs over three
weeks. We can see the ﬁrst few days were the most pro-
ductive, due to high number of new executions traced cov-
ered by symbolic execution1. The chart also shows two
more ”peaks” of new crash buckets on days 13 and 21.
This shows that new crashes were found throughout the
session.
4.3 Incompleteness and Divergences
Another key analysis we performed was tracking incom-
pleteness in symbolic execution. The x86 instruction set
has over a thousand different instructions. New exten-
sions are added frequently for supporting SIMD opera-
tions, such as Intel’s SSE instruction set. Unfortunately,
SAGE does not understand how to symbolically execute
every such instruction. This is important because failures
1Unfortunately, we cannot reveal absolute numbers of crashes found.
7Figure 4: Utilization graph for three sequential sessions with SAGAN . The x-axis shows time in days, and the y-axis
shows the number of active SAGE runs. By the third session, our utilization has improved due to underlying errors
identiﬁed thanks to SAGAN data and then ﬁxed.
Figure 5: New unique crash buckets per day over 23
days of running SAGE on about 200 programs. The data
suggests that running longer would yield more unique
crashes, although the return becomes typically lower.
to properly symbolically execute can lead to an incom-
plete or wrong path constraint generation, with the effect
of missing bugs or divergences : an input that is expected
to drive the program along a new speciﬁc path actually
follows a different path.
We added instrumentation to SAGE to detect whenever
we found an instruction or an instruction sequence not
properly handled by our symbolic execution engine. We
then conﬁgured SAGE to send back counts of how many
of such cases and of which type were encountered during
every symbolic execution. Moreover, we prioritized the
incompleteness cases into “high” and “low” severity cat-
egories. The high severity case indicates instructions not
handled at all by our symbolic execution. Low severity
means that the handling has known shortcomings but still
creates some kind of approximate constraint.
After a ﬁrst session with this new instrumentation, we
analyzed the new resulting SAGAN data to determine
which instructions and sequences of instructions had thehighest counts. For example, we found that over 90%
of the high severity instructions were shift instructions.
We implemented symbolic instruction handlers for these
instructions. As a result, the data from the next session
showed that in the high severity category, the vast major-
ity of the instructions had now been handled. We repeated
this process across subsequent session, in order to address
remaining holes in symbolic execution and prioritize the
writing of new symbolic instruction handlers.
4.4 Constraint Generating and Solving
Another large amount of data we have collected during
our400machine-years of running SAGE relates to sym-
bolic execution and constraint solving. We now present
such data for a sample set of about 300,000symbolic
executions performed on about 300different applications
running on Windows, and their corresponding constraints.
To be exact, this set consists of 304,190symbolic exe-
cution tasks. The sum of all constraints generated and
solved during those symbolic executions is 129,648,907
constraints, thus an average of 426constraints generated
for each symbolic execution (after all the constraint prun-
ing done by the techniques and heuristics described later
in this section).
4.4.1 Constraint Solving Time
Figure 6 presents the average solver time per constraint
for each symbolic execution. From this chart, one can see
that about 90% of all constraints are solved by Z3 in 0.1
seconds or less, and that about 99% of all constraints are
solved in 1 second or less. Thus, most solver queries are
fast.
4.4.2 Symbolic Execution and Solver Time
Figure 7 compares the total time in seconds spent sym-
bolically executing programs versus solving constraints
for each of the 300,000 symbolic execution tasks consid-
ered. Even though most constraints are solved in less than
8Figure 6: Average time for solving constraint queries for
each symbolic execution task. The blue bars show the
number of tasks with an average solving time in the la-
beled bin. The red line is a cumulative distribution func-
tion over all tasks. 90.18% of all tasks have an average
solving time of 0.1 seconds or less.
1 second, we can see that solving time often dominates
symbolic execution time, because those runs solve many,
many constraints. Moreover, we can also see extreme
cases, or outliers , where symbolic execution time dom-
inates strongly (upper left) or where solving time domi-
nates exclusively (lower right).
A more detailed analysis of the data depicted in Fig-
ure 7 reveals that almost 95% of all the symbolic execu-
tion tasks ﬁt in the lower left corner of the graph, in a box
bounded by 200seconds by 200seconds. Zooming in that
part of graph reveals the picture shown in Figure 8.
Following this observation, we implemented new fea-
tures in SAGE to (1) limit the time in symbolic execu-
tion and (2) limit the number of constraints being gen-
erated. These new features can cut off the outliers ob-
served in Figure 7, which consume a lot of CPU time but
produce few constraints and hence few tests (upper left
area) or spend too much time solving constraints (lower
right area). By zooming in on tasks in the lower right
area, we observed (data not shown here) that most of those
constraints are actually unsatisﬁable, and therefore do not
contribute to new tests. Indeed, intuitively, the more sym-
bolic execution generates constraints, the longer the path
constraint (by deﬁnition), the more constrained the “tail”
of the path constraint is (since every negated constraint is
put in a conjunction with all the constraints before it in
the path constraint), and the more unsatisﬁable the con-
straints in the tail usually are. Therefore, most constraints
in long tails are usually unsatisﬁable. Of course, dropping
Figure 7: Each dot is a symbolic execution task. On the
y-axis, the time to create constraints (secs). On the x-
axis, the time to solve constraints (secs). Notice extreme
outliers in both directions.
unsatisﬁable constraints is harmless for the purpose of test
generation and ﬁnding bugs.
After enforcing such new limits on symbolic execu-
tion time and the number of constraints generated by each
symbolic execution, we saw in the next session increases
in the number of symbolic execution tasks per SAGE run
(5.3 times) as expected, but also an increase in the average
number of queries per symbolic execution task (2.3 times)
and an increase in the total number of queries per SAGE
run (12 times).
4.4.3 Unsatisﬁable Constraints
Figure 9 plots the number of satisﬁable constraints (x axis)
versus the number of unsatisﬁable constraints and time-
outs (y axis) for all symbolic execution tasks in the previ-
ous data set that have a number of SAT solver queries less
than 1,000 and a total number of queries less than 5,000.
This set contains 298,354 tasks, which represents 98.08%
of all tasks in the previous data set (the remaining outliers
are hard to visualize in this form and were therefore omit-
ted). This ﬁgure illustrates that most constraints generated
by most symbolic executions are unsatisﬁable – most dots
are above the red line where y=x.
Why are most constraints generated by SAGE solved
in a fraction of a second? An important optimization we
use is related constraint optimization [18] which removes
the constraints in the path constraint that do not share
symbolic variables with the negated constraint (a simple
syntactic form of “cone-of-inﬂuence” reduction); this op-
timization often eliminates more than 90% of the con-
straints in the path constraint. Another critical (and stan-
dard) optimization is symbolic-expression caching which
9Figure 8: Symbolic execution tasks limited to a maximum
of 200 seconds for constraint generation and 200 seconds
for constraint solving. This set accounts for over 94 per-
cent of all tasks shown in Figure 7.
ensures that structurally equivalent symbolic terms are
mapped to the same physical object, and avoids an expo-
nential blow-up when the same sub-terms appear in differ-
ent sub-expressions. We also use local constraint caching
which skips a constraint if it has already been added to
the path constraint (since its negation cannot possibly be
satisﬁable with the ﬁrst constraint in the path constraint,
i.e.,p∧ ¬pis always unsatisﬁable no matter what pis).
These three optimizations are sound , that is, they cannot
themselves cause to miss bugs.
We also use other optimizations which are necessary
in practice to avoid the generation of too long path con-
straints but arbitrarily prune the search space and are
therefore unsound , i.e., can force the search to miss bugs.
Speciﬁcally, a ﬂip count limit establishes the maximum
number of times a constraint generated from a particular
program branch can be ﬂipped. Moreover, using a cheap
syntactic check, constraint subsumption eliminates con-
straints logically implied by other constraints injected at
the same program branch (mostly likely due to successive
iterations of an input-dependent loop).
Together, these optimizations reduce the size and com-
plexity of the queries sent to the constraint solver. Also,
most of these queries are by construction essentially
conjunctive (i.e., large conjunctions of constraints), and
therefore known to be rather easily solvable most of the
time. (In contrast, large disjunctions are usually harder to
solve.) Our results show that, for whitebox fuzzing, the
art of constraint generation is as important as the art of
constraint solving .
Figure 9: Each dot is a symbolic execution task. On the x-
axis, the number of satisﬁable constraints, i.e., the number
of new test cases generated. On the y-axis, the number of
unsatisﬁable or timedout constraints. Most tasks have a
high proportion of unsatisﬁable constraints.
Figure 10: On the x-axis, we place different branches ob-
served during whitebox fuzzing. On the y-axis we have
the frequency of occurrence. The graph shows only the
100ﬁrst most common branches out of 3,360total.
4.5 Commonality Between Programs
When running a whitebox fuzzer at our scale on hundreds
of applications, we are bound to have common subcom-
ponents being re-fuzzed over and over again, especially in
the Windows operating system where a lot of the core sys-
tem functions are packaged in DLLs (dynamically loaded
libraries) used by many of those applications.
Figure 10 shows statistics about all the program
branches ﬂipped during a whitebox fuzzing session for
about 200 applications running on Windows. Each pro-
gram branch is identiﬁed by a DLL name and an offset in
that DLL, identifying a conditional statement (typically
a jump) at that location, and where a symbolic input-
dependent constraint was generated. The data represent
290,430 program branches ﬂipped. There are 3,360 dis-
10tinct branches, with a maximum frequency of 17,761 (ex-
treme left) and minimum frequency 592 in the extreme
right, which is not shown here – the tail is shown only up
to distinct branch ranked as 100, and there is a very long
and ﬂat tail up to distinct branch 3,360 after this.
As one can clearly see from the ﬁgure, a small percent-
age of unique program branches (to the left) are ﬂipped
over and over again, and represent the part of the search
space where most constraints were generated. Remem-
ber the data shown here was obtained after the pruning
described in the previous subsection preventing the same
branch to be ﬂipped over a speciﬁc limit; without this
pruning, the data would be even more tilted towards these
few instructions. This behavior is typical, even in a sin-
gle application. For instance, when whitebox fuzzing a
structured non-binary parser, most of the input constraints
generated are in the lexer, of course.
Re-fuzzing over and over again the same sub-
components and DLLs is wasteful. In principle, this can
be avoided with compositional testing [15], which cre-
atestest summaries from symbolic execution. These sum-
maries are not only re-usable during a fuzzing session, but
also apply across applications that share common com-
ponents (such as DLLs), and over time from one fuzzing
session to the next [17]. Compositional testing can result
in a search algorithm that is exponentially faster than a
non-compositional one. Every test run in a centralized in-
frastructure can create new test summaries to improve all
future test runs through this component. We are currently
investigating how to best use and deploy such techniques
in production mode, including a centralized repository of
test summaries for large parts of the Windows operating
system.
5 Related Work
Blackbox fuzz testing in clusters at a large scale is not
new. Nagy described a custom-built cluster dedicated
to high volume testing of Microsoft Word that processes
1.7 million test cases per day [22]. The Ofﬁce team at
Microsoft has built a distributed fuzzing framework that
works across “nights and weekends” use of idle desktops,
as well as in clusters [14]. Google’s security team de-
voted 2,000cores over roughly four weeks to fuzz testing
Adobe Flash [11]. We leverage previous work in this area
on the classiﬁcation of crashing test cases, prioritization
of bugs, and automatic reporting of important bugs to de-
velopers.
What is new is the use of whitebox fuzzing at the
scale described in this paper. Whitebox fuzzing combines
and extends program analysis, testing, veriﬁcation, model
checking and automated theorem proving techniques thathave been developed over many years. One of the earliest
proposals for using static analysis as a kind of symbolic
program testing method was proposed by King almost 35
years ago [19]. The idea is to symbolically explore the
tree of all computations the program exhibits when all
possible value assignments to input parameters are con-
sidered. Static test generation consists of analyzing a pro-
gram statically, by using symbolic execution techniques
to attempt to compute inputs to drive the program along
speciﬁc execution paths or branches, without ever execut-
ing the program . Unfortunately, this approach is ineffec-
tive whenever the program contains statements involving
constraints outside the scope of reasoning of the theo-
rem prover, i.e., statements “that cannot be reasoned about
symbolically.” This is frequent in practice due to complex
program statements (pointer manipulations, ﬂoating-point
operations, etc.) and calls to operating-system and library
functions.
A recent breakthrough was the emergence of a second
approach: dynamic test generation [16] [5]. It consists of
executing the program, typically starting with some ran-
dom inputs, while performing symbolic execution dynam-
ically , collecting symbolic constraints on inputs gathered
from predicates in branch statements along the execution,
and then using a constraint solver to infer variants of the
previous inputs in order to steer the next execution of the
program towards an alternative program branch. A key
advantage of dynamic test generation is that imprecision
in symbolic execution can be alleviated using concrete
values and randomization: whenever symbolic execution
does not know how to generate a constraint for a program
statement or library depending on some inputs, one can
always simplify this constraint using the concrete values
of those inputs [16].
Over the last few years, dynamic symbolic execution
and test generation have been implemented in many tools,
such as APOLLO, CUTE, KLEE, PEX and S2E (see [6]
for a recent survey). For instance, the KLEE tool built on
LLVM showed these techniques could yield high cover-
age on all programs in the coreutils andbusybox
suites, outperforming hand-generated tests created over
multiple years. Follow-on tools have applied these tech-
niques to testing drivers [7], and ﬁnding “trigger behav-
ior” in malware [2]. While promising, this prior work did
not report of any daily use of these techniques and tools.
Another recent signiﬁcant milestone is the emergence
ofwhitebox fuzzing [18] as the current main “killer app”
for dynamic test generation, and arguably for automatic
code-driven test generation in general. This in turn has
allowed the developmemt of the next step in this decades-
long journey: the ﬁrst “productization” of large-scale
11symbolic execution and constraint solving. This paper is
the ﬁrst to report on this development.
Our work extends previous dynamic test generation
tools with logging and control mechanisms. These mech-
anisms allow us to run whitebox fuzz testing for weeks on
end with low effort and cost, and they enable data-driven
improvements to our fuzzing platform. The closest related
prior works are Metafuzz [21] and Cloud9 [3]. Metafuzz
also performed logging of whitebox fuzzing on Linux, us-
ing the SmartFuzz plugin for Valgrind [21]. Unlike our
current work, however, Metafuzz has no automatic con-
trol features. Furthermore, the authors do not show how
data from Metafuzz directly inform future research or op-
erations of whitebox fuzzing. Finally, the Metafuzz au-
thors report experiments of 24hours in length, while we
report on multiple multi-week runs of whitebox fuzzing
on many more applications.
Cloud9 [3] is a system for scaling KLEE-style symbolic
execution across a cluster of commodity machines; the
authors use Amazon’s Elastic Compute Cloud in their ex-
periments. Like KLEE, Cloud9 focuses on creating high
coverage test suites for commodity programs. Their tech-
niques for scaling symbolic execution across multiple ma-
chines are complementary to ours and could be used to
inform the task partitioning we described in Section 3.
Again, however, they do not report on multiple multi-
week runs against hundreds of different test programs.
Our work reports on usage and deployments which are or-
ders of magnitude larger in all ﬁve scalability dimensions
identiﬁed in Section 1 than any prior work in this space.
6 Conclusion
We have shown that whitebox fuzzing scales to production
use. Our biggest pain points have been around the hetero-
geneity of different applications with respect to conﬁgura-
tion and in the logistics of provisioning hundreds of ma-
chines. The systems we described here were developed
in direct response to these pain points. The logging from
SAGAN helps us keep track of hundreds of different pro-
grams and seed ﬁles. The JobCenter service then lets us
turn around and deploy new conﬁgurations to machines
running with our infrastructure.
Moving forward, we know from our data that signiﬁ-
cant challenges remain in improving the precision of sym-
bolic execution and combating path explosion. As de-
scribed in Section 2 and 4, we currently use unsound op-
timizations to scale to long execution traces and programs
such as the Ofﬁce suite, but those may also miss important
bugs. Our tool also supports many optional features, such
as reasoning with symbolic pointers [4, 10], which are
sometimes expensive and therefore not always all turnedon in production runs. By adding other monitoring fea-
tures to SAGAN , we hope to drill down and understand
better these cost/precision trade-offs.
We also recognize that whitebox fuzzing is only one
piece of the security puzzle and one niche application
for automatic test generation. The major longer-term win
from all this technology comes in a shared infrastructure
for the entire picture of people, processes, and tools re-
quired to build secure reliable software. Our infrastructure
reduces the cost to “enroll” a program in this infrastruc-
ture, which is the gating factor to applying our techniques
or any others. While in our case the infrastructure is on-
premise, this could also involve the use of machines “in
the cloud.” No matter where it is located, we see central-
ized, data-driven security, testing and software engineer-
ing infrastructure as a key direction for future research.
7 Acklowledgments
This paper would not have been possible without the help
and support of many people. We thank Michael Levin for
starting the development of the SAGE tool and participat-
ing in this project for several years. We also thank Chris
Marsh, Lei Fang, Stuart de Jong, and others in the former
Center for Software Excellence for building the dynamic
program analysis stack SAGE is built upon. We thank
the developers of the Z3 SMT solver, including Niko-
laj Bjorner and Leonardo de Moura. We appreciate the
support of our managers in pursuing this project, includ-
ing Tom Ball, Wolfram Schulte, and Helen Wang. Mehdi
Bouaziz made substantial implementation contributions to
JobCenter while an intern at Microsoft. Additional in-
terns who have worked on SAGE include Dennis Jeffries,
Adam Kiezun, Bassem Elkarablieh, Marius Nita, Cindy
Rubio-Gonzalez, Johannes Kinder, Daniel Luchaup, and
Louis Jachiet.
The work reported in this paper was performed in col-
laboration with our partners in Windows and in Ofﬁce. In
windows, we thank speciﬁcally Nick Bartmon, Eric Dou-
glas, Dustin Duran, David Grant, Elmar Langholz, Vince
Orgovan, Isaac Sheldon, Evan Tice, Dave Weston, and
Russell West, for running SAGE and supporting develop-
ment of the binary analysis tools on which it depends. In
Ofﬁce, we are particularly grateful to Tom Gallagher, Eric
Jarvi and Octavian Timofte, for running SAGE and work-
ing with us to develop JobCenter. From the Microsoft
Security Engineering Center, we thank Matt Miller, Lars
Opstad, Andy Renk, Jason Shirk, and Dazhi Zhang. We
thank our MSR colleagues Rich Draves, Jon Howell, Stu-
art Schecter, and Helen Wang for feedback on drafts of
this paper under short notice. Finally, we thank all the
users of SAGE across Microsoft.
12References
[1] S. Bhansali, W. Chen, S. De Jong, A. Edwards, and
M. Drinic. Framework for instruction-level trac-
ing and analysis of programs. In Second Inter-
national Conference on Virtual Execution Environ-
ments VEE , 2006.
[2] D. Brumley, C. Hartwig, Z. Liang, J. Newsome,
D. Song, and H. Yin. Automatically identifying
trigger-based behavior in malware. In Botnet De-
tection , pages 65–88. Springer, 2008.
[3] S. Bucur, V. Ureche, C. Zamﬁr, and G. Candea. Par-
allel symbolic execution for automated real-world
software testing. In Proceedings of the sixth confer-
ence on Computer systems , EuroSys ’11, New York,
NY, USA, 2011. ACM.
[4] C. Cadar, D. Dunbar, and D. Engler. Klee: Unas-
sisted and automatic generation of high-coverage
tests for complex systems programs. In OSDI , 2008.
[5] C. Cadar, V. Ganesh, P. M. Pawlowski, D. L. Dill,
and D. R. Engler. EXE: Automatically Generating
Inputs of Death. In ACM CCS , 2006.
[6] C. Cadar, P. Godefroid, S. Khurshid, C.S. Pasareanu,
K. Sen, N.Tillmann, and W. Visser. Symbolic Exe-
cution for Software Testing in Practice – Preliminary
Assessment. In ICSE’2011 , Honolulu, May 2011.
[7] V. Chipounov, V. Kuznetsov, and G. Candea. S2E: A
platform for in-vivo multi-path analysis of software
systems. In ASPLOS , 2011.
[8] Microsoft Corporation. Security development life-
cycle, 2012. http://www.microsoft.com/
security/sdl/default.aspx .
[9] L. de Moura and N. Bjorner. Z3: An Efﬁcient SMT
Solver. In Proceedings of TACAS’2008 (14th Inter-
national Conference on Tools and Algorithms for the
Construction and Analysis of Systems) , volume 4963
ofLecture Notes in Computer Science , pages 337–
340, Budapest, April 2008. Springer-Verlag.
[10] B. Elkarablieh, P. Godefroid, and M. Y. Levin. Pre-
cise pointer reasoning for dynamic test generation.
InISSTA , pages 129–140, 2009.
[11] Ch. Evans, M. Moore, and T. Or-
mandy. Fuzzing at scale, 2011. http:
//googleonlinesecurity.blogspot.
com/2011/08/fuzzing-at-scale.html .[12] S. Floyd and V. Jacobson. The synchronization of
periodic routing messages. IEEE/ACM Trans. Netw. ,
2(2):122–136, April 1994.
[13] J. E. Forrester and B. P. Miller. An Empirical Study
of the Robustness of Windows NT Applications Us-
ing Random Testing. In Proceedings of the 4th
USENIX Windows System Symposium , Seattle, Au-
gust 2000.
[14] T. Gallagher and D. Conger. Under the kimono of
ofﬁce security engineering. In CanSecWest , 2010.
[15] P. Godefroid. Compositional Dynamic Test Genera-
tion. In Proceedings of POPL’2007 (34th ACM Sym-
posium on Principles of Programming Languages) ,
pages 47–54, Nice, January 2007.
[16] P. Godefroid, N. Klarlund, and K. Sen. DART:
Directed Automated Random Testing. In Proceed-
ings of PLDI’2005 (ACM SIGPLAN 2005 Confer-
ence on Programming Language Design and Imple-
mentation) , pages 213–223, Chicago, June 2005.
[17] P. Godefroid, S. K. Lahiri, and C. Rubio-Gonzalez.
Statically Validating Must Summaries for Incre-
mental Compositional Dynamic Test Generation.
InProceedings of SAS’2011 (18th International
Static Analysis Symposium) , volume 6887 of Lecture
Notes in Computer Science , pages 112–128, Venice,
September 2011. Springer-Verlag.
[18] P. Godefroid, M.Y. Levin, and D. Molnar. Auto-
mated Whitebox Fuzz Testing. In Proceedings of
NDSS’2008 (Network and Distributed Systems Se-
curity) , pages 151–166, San Diego, February 2008.
[19] J. C. King. Symbolic Execution and Program Test-
ing.Journal of the ACM , 19(7):385–394, 1976.
[20] B.P. Miller, L. Fredriksen, and B. So. An empirical
study of the reliability of UNIX utilities. Communi-
cations of the ACM , 33(12), December 1990.
[21] D. Molnar, X. C. Li, and D. A. Wagner. Dynamic
test generation to ﬁnd integer bugs in x86 binary
linux programs. In USENIX Security Symposium ,
2009.
[22] B. Nagy. Finding microsoft vulnerabilities by
fuzzing binary ﬁles with ruby - a new fuzzing frame-
work. In SyScan , 2009. http://www.youtube.
com/watch?v=u--j4YY 7cg.
13[23] S. Narayanasamy, Z. Wang, J. Tigani, A. Edwards,
and B. Calder. Automatically classifying benign and
harmful data races using replay analysis. In Pro-
gramming Languages Design and Implementation
(PLDI) , 2007.
[24] N. Tillmann and J. de Halleux. Pex - White Box Test
Generation for .NET. In Proceedings of TAP’2008
(2nd International Conference on Tests and Proofs) ,
volume 4966 of Lecture Notes in Computer Science ,
pages 134–153. Springer-Verlag, April 2008.
14
View publication stats