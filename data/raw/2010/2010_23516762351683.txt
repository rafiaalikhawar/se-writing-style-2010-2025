Improving the Effectiveness of Spectra-Based Fault
Localization Using Speciﬁcations
Divya Gopinath, Razieh Nokhbeh Zaeem, and Sarfraz Khurshid
The University of Texas at Austin, Austin, TX, USA
{divyagopinath, nokhbeh}@utexas.edu, khurshid@ece.utexas.edu
ABSTRACT
Fault localization i.e., locating faulty lines of code, is a key step in
removing bugs and often requires substantial manual effort. Recent
years have seen many automated localization techniques, specif-
ically using the program’s passing and failing test runs, i.e., test
spectra . However, the effectiveness of these approaches is sensi-
tive to factors such as the type and number of faults, and the quality
of the test-suite. This paper presents a novel technique that ap-
plies spectra-based localization in synergy with speciﬁcation-based
analysis to more accurately locate faults. Our insight is that unsat-
isﬁability analysis of violated speciﬁcations, enabled by SAT tech-
nology, could be used to (1) compute unsatisﬁable cores that con-
tain likely faulty statements and (2) generate tests that help spectra-
based localization. Our technique is iterative and driven by a feed-
back loop that enables more precise fault localization. Our frame-
work SAT-TAR embodies our technique for Java programs, includ-
ing those with multiple faults. An experimental evaluation using
a suite of widely-studied data structure programs, including the
ANTLR and JTopas parser applications, shows that our technique
localizes faults more accurately than state-of-the-art approaches.
Categories and Subject Descriptors
D.2.5 [ Software Engineering ]: Testing and Debugging— Debug-
ging aids
General Terms
Veriﬁcation
Keywords
Automated Debugging, Fault Localization, Alloy, Kodkod, Taran-
tula, Minimal UNSAT cores
1. INTRODUCTION
Fault Localization – locating faulty lines of code in a buggy pro-
gram – is a fundamental step in removing the bugs. In practice,
fault localization is largely manual and often costly. However, there
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
ASE ’12, September 3-7, 2012, Essen, Germany
Copyright 12 ACM 978-1-4503-1204-2/12/09 ...$15.00.has been much research progress in recent years to automate it [30,
4, 24, 11, 21, 19]. A particularly effective approach is spectrum-
based localization [19, 24], which utilizes the coverage of multiple
program runs – successful and failing – to identify faulty locations.
Most of these techniques produce a suspect list, which ranks pro-
gram entities based on a heuristic metric of suspiciousness, which
measures the likelihood of them being faulty [19, 3].
The effectiveness of these localization techniques, however, is
heavily dependent on the test-suite quality. The number and cover-
age of the tests can adversely impact the localization. For instance,
many failing tests with similar code coverage or passing tests hav-
ing coverage very different from the failing tests can result in too
many statements being marked suspicious of being faulty. The type
and number of faults also affect the accuracy of the results. More-
over, failing tests covering different faults may interfere with each
other leading to a decrease in the suspiciousness values assigned to
the faulty statements. Also, certain faulty statements may get cov-
ered by both failing and passing tests resulting in them being ranked
less suspicious than an actually correct statement being covered by
the failing tests alone. Such faults are more common in programs
dealing with data structures, where some statements may lead to
errors only on certain heap conﬁgurations. These programs are im-
portant as they form the backbone of many applications today such
as XML-parsers, compilers, relational engines, and so on.
A recently proposed approach [5] uses directed test generation
to more effectively localize faults. Given a failing test, it executes
the program using concolic execution [25] to record a path con-
dition comprising of the control-ﬂow predicates along the failure
trace. A set of new path conditions is generated by negating each
of the predicates systematically and solving them using a constraint
solver to generate inputs that would trace new control-ﬂow paths.
The input whose trace covers maximum number of path conditions
having the same truth value as that of the initial failure trace is con-
sidered to be the one most similar to the failing test and is chosen
as the next test case to be added to the suite. The algorithm is
repeated with the new test case to generate additional test inputs.
The rationale behind this selection is that a passing test case most
similar to the failing test yields the maximum beneﬁt in terms of lo-
calizing faults. While the results of test-spectra based localization
approaches can improve using a test-suite thus generated, certain
situations would impede the effectiveness of this approach. For in-
stance, when the failure trace is long including a number of path
conditions, the technique may end up choosing a number of similar
failing runs before hitting upon a passing test case.
A key characteristic of most spectrum-based approaches is that
they depend solely on runtime execution information and are not
guided by reasons for failure. However, advances in enabling the
user to annotate programs with correctness speciﬁcations [10] canPermission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
ASE’12, September 3–7, 2012, Essen, Germany
Copyright 2012 ACM 978-1-4503-1204-2/12/09 ...$15.00
40
be leveraged to guide the search for faulty statements. A recent
work [20] leverages SAT solvers in analyzing correctness constraints
to identify suspicious statements. However complete reliance on
user-deﬁned contracts is also problematic since the completeness
and complexity of the constraints can adversely impact the effec-
tiveness of the localization. Moreover, repetitive static analysis to
improve localization accuracy impacts the scalability of these ap-
proaches [20, 15, 7] to bigger applications.
We present a technique, SAT-TAR , that combines speciﬁcation-
based analysis with dynamic test execution information to produce
effective localization consistently across varying fault and speciﬁ-
cation complexity. Our insight is that unsatisﬁability analysis of vi-
olated speciﬁcations enabled by SAT technology could be used to
augment the ratings produced by test coverage based localization
tools, such as Tarantula [19], which are more scalable and widely
used. Our technique employs a feedback loop to employ SAT-based
analysis and spectrum-based localization in synergy.
SAT-based localization: Given a program annotated with correct-
ness speciﬁcations and the trace of a single run violating these con-
straints, our approach investigates the violated constraints to de-
termine the list of statements responsible for the speciﬁcation vi-
olation. We leverage minimal unsatisﬁable cores (MUC) returned
by SAT solvers for this purpose. However, complex or incomplete
user-deﬁned speciﬁcations can yield long suspect lists. We perform
spectra-based localization to utilize the coverage of different tests
to reﬁne the results of localization.
Spectra-based localization: We employ spectra-based localiza-
tion [19] to obtain suspiciousness ratings for the statements. We
utilize the information obtained from the UNSAT core analysis as
follows; i) we increase the suspiciousness ratings of those state-
ments that appear in the MUC, ii) we generate the test cases for
spectra-based localization using a variant of the directed test gen-
eration approach [5]. Speciﬁcally, after the generation of a set of
test inputs, we select the test input whose trace is most similar to
the failure trace in terms of its coverage of only those statements
that appear in the MUC and hence contribute to the violation. This
enables the generation of test cases that would be most effective
in localizing faults earlier in the process, thereby allowing more
accurate ratings to be generated using fewer test cases.
Instead of using the test-suite as a whole, we enable incremental
processing of each additional test. Heuristic analysis of the ratings
after every round is used to reﬁne the localization. Feedback based
on this analysis is used to improve the SAT-based test generation
strategy on-the-ﬂy. Our technique provides effective localization
even in multiple fault scenarios. We perform root-cause analysis
of speciﬁcation violation and support user-control on addition of
tests to prevent different faults from interfering with each other’s
localization. Test selection strategies face the challenge to balance
between choosing sufﬁcient number and type of tests (to provide
good error detection and localization capabilities), and keeping the
test-suite size manageable under time constraints. SAT-TAR gen-
erates test-suites containing minimal number of test cases required
to produce high quality localization.
This paper makes the following contributions:
•Unsat cores for fault localization. Application of unsatisﬁ-
ability analysis of speciﬁcations, enabled by Alloy and SAT,
to short-list suspicious statements which aids in augmenting
the effectiveness of spectra-based localization.
•Combination of speciﬁcation-based analysis and execu-
tion spectra. Feedback-driven approach combining speciﬁ-
cation based analysis and spectra of tests to produce better lo-
calization than the stand-alone application of each technique
for different types of faults and user-provided contracts.•Minimal test-suite generation. Generation of a minimal
test-suite for effective fault-localization.
•Multiple Faults. Speciﬁcation-based approach to partition
failing tests covering different faults.
•Experimental Evaluation. Our evaluation highlights some
speciﬁc issues with localizing faults using purely spectrum-
based techniques in the implementations of recursive data
structures. We demonstrate that SAT-TAR localizes faults
more accurately than existing state-of-the-art spectra-based
and SAT-based localization techniques.
2. BACKGROUND
TheTarantula [19] tool localizes faults in a program based on
the coverage of a given test suite. It associates with each state-
ment a metric of suspiciousness, which represents the likelihood of
the statement being faulty, and a metric of conﬁdence in the calcu-
lated suspiciousness. In the following deﬁnitions, Failed (s)and
Passed (s)show the number of failing and passing tests covering
statementsrespectively, while TotalFailed andTotalPassed
represent the total number of failing and passing tests.
Susp Tar(s) =Failed (s)
TotalFailed
Failed (s)
TotalFailed+Passed (s)
TotalPassed
Conf Tar(s) =Max (Failed (s)
TotalFailed,Passed (s)
TotalPassed)
Statements with a suspiciousness value of 1.0 are considered to
be the most suspicious. Among the statements with the same sus-
piciousness values, those having a higher value for conﬁdence are
more likely to be faulty since more number of tests lend evidence to
them being suspicious. Each statement is assigned a rank , which
indicates the maximum number of statements that would need to
be examined to reach that statement (inclusive of the statement), if
they were to be arranged in the descending order of suspiciousness
and conﬁdence values.
Alloy [18] is a speciﬁcation language based on ﬁrst-order rela-
tional logic. Its set-based syntax allows modeling the heap of object
oriented programs. Classes are represented as sets, and ﬁelds as re-
lations between the sets. SAT-TAR supports Alloy for writing the
speciﬁcations, including class invariants (also known as repOK ).
TheForge tool set [13] enables veriﬁcation of Java code against
correctness speciﬁcations written in JFSL [13] or JML [10]. It
translates the code and the speciﬁcations to Alloy by encoding the
data and control ﬂow of the procedure in relational logic. The loops
are unrolled a ﬁnite number of times. The ﬂow from one statement
to the next is viewed as a relational implication; at branch state-
ments, the two branch edges are viewed as relational disjunctions.
The conjunction of the pre-condition, the formula representing the
code, and the negation of the post-condition speciﬁcation of the
procedure is fed into the relational back-end engine Kodkod [27]. A
solution represents a counter-example to the speciﬁcation, showing
the presence of a valid input, tracing a valid path through the code,
and producing an output that does not satisfy the post-condition.
Kodkod , the back-end of Forge, is an efﬁcient model ﬁnder for re-
lational logic problems speciﬁed in Alloy. Given a ﬁnite bound on
the scope, it translates the problem into boolean satisﬁability and
uses off-the-shelf SAT solvers to ﬁnd solutions.
Minimal unsatisﬁable cores of declarative speciﬁcations : Con-
siderable work has been done in recent years to ﬁnd minimal un-
satisﬁable cores of unsatisﬁable constraints written as propositional
satisﬁability (SAT) formulas [14, 31]. Given an unsatisﬁable CNF
formula X, a minimal unsatisﬁable sub-formula (MUS) is a subset
of X’s clauses that is both unsatisﬁable and minimal, which means41any subset of an MUS is satisﬁable. There could be many indepen-
dent reasons for a formula’s unsatisﬁability and hence more than
one minimal core. However, extracting all of them is computation-
ally expensive. Torlak et al. [26] propose an efﬁcient algorithm for
the extraction of a single MUC of declarative speciﬁcations based
on the resolution refutation proofs generated by SAT solvers and
theorem provers. The Recycling Core Extractor algorithm (RCE),
returns an unsatisﬁable core of speciﬁcations written in the Alloy
language that is guaranteed to be sound (constraints not included
in the UNSAT core are deﬁnitely irrelevant to the unsatisﬁabil-
ity proof) and irreducible (removal of any constraint from the set
would make the remaining formula satisﬁable). MUC has been
shown to be useful in the identiﬁcation of over-constrained models,
weak theorems, and insufﬁcient scopes while checking models. In
this paper, we use MUC in localizing faults in imperative code.
3. ILLUSTRATIVE OVERVIEW
c l a s s L i n k e d L i s t {
Node h e a d e r ;
i n t s i z e ; / /number ofnodes i nt h i s l i s t
s t a t i c c l a s s Node{
Node n e x t ;
i n t key ; }
boolean d e l e t e ( i n t k ) {
/ /removes t h e node with key v a l u e k
0 : Node prev = n u l l ;
1 : Node l s t = h e a d e r ;
2 : while ( l s t != n u l l ) {
3 : i f( l s t . key == k ) {
4 : i f( prev != n u l l ) {
5 : prev . n e x t = l s t . n e x t ; / /c o r r e c t
prev . n e x t = l s t ; / /removeErr
}e l s e {
6 : h e a d e r = h e a d e r . n e x t ; / /c o r r e c t
h e a d e r = h e a d e r ; / /headErr
}
7 : s i z e−−;/ /c o r r e c t
s i z e = 0 ; / /s i z e E r r
8 : r e t u r n t r u e ; }
9 : prev = l s t ;
10: l s t = l s t . n e x t ; }
11: r e t u r n f a l s e ; } }
Listing 1: LinkedList and its delete method with faults.
We consider the localization of typical faults in a simple data
structure method and illustrate the beneﬁts of our technique by
comparing the results of SAT-TAR with existing state-of-the-art
localization techniques; pure spectra-based localization (TAR)
Tarantula on a test-suite generated using the most recent directed
test generation technique for effective fault-localization, (DT) [5],
andpure SAT-based analysis of speciﬁcations (SAT) .
Example program with faults: Listing 1 shows the delete method
of a singly linked list data structure class. The delete method
takes in an integer argument ( k)and attempts to remove the list
node with the matching key value.
Figure 1 displays a model for the class and its correctness spec-
iﬁcations in Alloy (Section 2). The class invariant ( repOk ) con-
sists of the acyclicity constraint, which checks that the list does not
contain any cycles, the unique-elements constraint, which checks
that each list node has a unique key, and size-invariant , which en-
sures that the size ﬁeld has a value equal to the number of nodes
reachable from the header through the next pointer. We consider
two possible versions for the user-deﬁned post-condition speciﬁ-
cation for the delete method. In deletepostcond1 , the user
includes constraints to speciﬁcally check the functionality of the
method alone; only the node with the matching key value has been
removed from the list ( remove-ok ) and the size of the list has been
decremented on a successful delete ( size-ok ). On the other hand,
indeletepostcond2 , the user ensures that all the class invariantspred repOk(This: LinkedList ){ // class invariant
all n:This.header. *next | n ! inn.^next//acyclicity
# This.header. *next = int This.size//size-invariant
all n, m: This.header. *next |
int m.key = int n.key => n = m//unique-elements}
pred deletepostcond1(This: LinkedList , k: Int){
//remove-ok
This.header. *next.key-k=This.header‘. *next‘.key‘
&&((k in(This.header. *next.key))
=>(This.size-1=This.size‘))//size-ok}
pred deletepostcond2(This: LinkedList , k: Int){
repOk[This]}
Figure 1: LinkedList class invariant, post-conditions for
delete in Alloy.
Table 1: Spectra-based localization using Tarantula.
Stmts 0 1 2 3 4 5 6 7 8 9 10 11
Fault Tech. Tests Rating
sizeErr TarDT2F,2P Susp 0.5 0.5 0.5 0.5 0.6 1.0 0.5 0.60.6 0.5 0.5 0.0
Tar
Conf 1.0 1.0 1.0 1.0 1.0 0.5 0.5 1.01.0 0.5 0.5 0.5
Tar
Rank 8 8 8 8 4 1 8 44 10 10 11
headErr TarDT2F,2P Susp 0.5 0.5 0.5 0.5 0.6 0.0 1.00.6 0.6 0.0 0.0 0.0
Tar
Conf 1.0 1.0 1.0 1.0 1.0 0.5 1.01.0 1.0 1.0 1.0 0.5
Tar
Rank 8 8 8 8 4 12 14 4 10 10 12
are preserved after deletion without including a speciﬁc check to
ensure the removal of the input value. The syntactic sugar of adding
back-tick (‘ ‘’) is used to represent the relations in their post-state
after the execution of a method. We use these two versions to show
how our technique is intended to work with varying levels of com-
plexity and completeness in user-deﬁned speciﬁcations.
Let us consider two faulty versions of the delete method, one
comprising of just sizeErr on statement 7 and the other compris-
ing of just headErr on statement 6. The ﬁrst fault wrongly sets
thesize ﬁeld to 0 instead of decrementing it whenever a match
is found. The second fault incorrectly updates the header ﬁeld.
Applying Tarantula (TAR): We ﬁrst apply Tarantula on a mini-
mal suite, and then consider the use of a suite generated using the
approach for effective fault localization ( TarDT). Consider the use
of just two tests; a failing test case with a two node input list and the
value kmatching the keyof the ﬁrst node, and a passing test case
with an empty input list, to localize sizeErr . The traces of the two
runs are very dissimilar, so most of the statements are assigned the
same suspiciousness and conﬁdence ratings of 1.0. Hence in the
worst case 6 actually correct statements should be examined before
hitting the faulty statement 7, giving it a rank (Section 2) of 7.
The results improve on using a test-suite with 4 tests generated
speciﬁcally for effective fault localization ( TarDTin Table 1, Susp
andConf stand for suspiciousness and conﬁdence, and FandPare
for failing and passing tests respectively). While headErr is well-
localized ranking the faulty statement 6 ﬁrst in the list, the ratings
are not accurate for sizeErr . Statement 7 (rank 4) is rated lower
than the actually correct statement 5. This is because the suite in-
cludes a test that removes from an input list with just one node, for
which setting size to 0 after deletion is valid. Hence statement 7 has
one passing test covering it, whereas all tests covering statement 5
are failing runs.
Applying pure SAT-based analysis of speciﬁcations (SAT): Let
us now consider the application of a technique which solely uses
static analysis of the code and violated speciﬁcations to localize42Table 2: Pure SAT and SAT-TAR localizations for sizeErr .
Stmt SAT Spectra
T arMUC
postcond 1 1F,1F 2F,1P 2F,2P
Susp Rank Susp 1Conf 1Susp 2Conf 2Susp Conf Rank
SAT SAT sattar sattar sattar
0 0.0 12 1.0 1.0 0.5 1.0 0.5 1.0 7
1 1.0 4 2.0 1.0 1.5 1.0 1.5 1.0 4
2 1.0 4 2.0 1.0 1.5 1.0 1.5 1.0 4
3 1.0 4 2.0 1.0 1.5 1.0 1.6 1.0 2
4 0.0 12 1.0 1.0 1.0 1.0 1.0 1.0 6
5 0.0 12 1.0 0.5 1.0 0.5 1.0 0.5 9
6 0.0 12 1.0 0.5 1.0 0.5 1.0 0.5 9
7 1.0 4 2.0 1.0 2.0 1.0 2.0 1.0 1
8 0.0 12 1.0 1.0 1.0 1.0 1.0 1.0 6
9 0.0 12 1.0 0.5 0.3 1.0 0.5 0.5 9
10 0.0 12 1.0 0.5 0.3 1.0 0.5 0.5 9
11 0.0 12 0.0 0.0 0.0 1.0 0.0 1.0 10
Table 3: Pure SAT and SAT-TAR localizations for headErr .
Stmt SAT Spectra
T arMUC
postcond 1 1F,1P 2F,1P 2F,2P
Susp Rank Susp 1Conf 1Susp 2Conf 2Susp Conf Rank
SAT SAT sattar sattar sattar
0 1.0 7 1.5 1.0 1.5 1.0 1.5 1.0 7
1 1.0 7 1.5 1.0 1.5 1.0 1.5 1.0 7
2 1.0 7 1.5 1.0 1.5 1.0 1.5 1.0 7
3 1.0 7 1.5 1.0 1.5 1.0 1.6 1.0 4
4 1.0 7 1.5 1.0 1.6 1.0 1.75 1.0 3
5 0.0 12 0.0 1.0 0.0 0.5 0.0 0.3 12
6 1.0 7 2.0 1.0 2.0 1.0 2.0 1.0 1
7 1.0 7 1.5 1.0 1.6 1.0 1.75 1.0 3
8 0.0 12 0.5 1.0 0.6 1.0 0.75 1.0 8
9 0.0 12 0.0 1.0 0.0 1.0 0.0 0.6 11
10 0.0 12 0.0 1.0 0.0 1.0 0.0 0.6 11
11 0.0 12 0.0 0.0 0.0 0.5 0.0 0.6 11
the same faults. Given a program annotated with correctness spec-
iﬁcations and an input exposing a fault in the code that violates
the speciﬁcations, Kodkod is leveraged to detect a minimal UN-
SAT core (Section 2). The constraints in the core are mapped to
source code statements to determine the list of suspicious state-
ments, each of them being assigned the same suspiciousness value
of 1.0. An input exposing only sizeErr with deletepostcond1 ,
would violate the size-ok constraint. The suspect list produced
includes the faulty statement 7 updating size while eliminating
statements updating other ﬁelds such as statement 5. Faulty state-
ment 7 (SAT in Table 2) is ranked fourth using just a single failing
test. However, complex speciﬁcations, made up of a number of de-
pendent constraints, result in multiple violated constraints involv-
ing many data structure ﬁelds. Such is the case with headErr using
deletepostcond2 (SAT in Table 3). A list with one node whose
key value matches the input kexposes this fault. The violated con-
straint is size-invariant , which includes almost all ﬁelds of the
linked list. Hence the UNSAT core does not help much in ﬁltering
out the correct statements. Faulty statement 6 is ranked seventh.
Applying SAT-TAR: The above results lend motivation to our in-
tuition that instead of dwelling just in the realm of spectra-based
localization or pure SAT-based approaches, it would be helpful to
combine the two techniques in order to achieve effective localiza-
tion consistently across different fault scenarios.
Starting in the SAT domain, our framework takes in a fault-
revealing input. We employ the Forge framework, with Kodkod at
its backend, to determine a Minimal Unsatisﬁable Core. The MUC
consists of the core constraints responsible for the violation and is
mapped back to an initial suspect list of potentially faulty source
code statements. This list is exactly the same as that produced byTable 4: Multi-fault Localization.
Faulty TarDTSAT-TAR(postcond1)
Stmt test suite shows both faults initial trace shows headErr initial trace shows sizeErr
3F,3P 1F,2P 1F,2P
Susp Conf Rank Susp Conf Rank Susp Conf Rank
Tar Tar Tar sattar sattar sattar sattar sattar sattar
6 1.0 0.6 4 2.0 1.0 2 0.5 1.0 11
7 1.0 1.0 3 1.0 1.0 8 2.0 1.0 1
the pure SAT-based technique for the respective faults (SAT in Ta-
bles 2 and 3). Instead of performing further analysis in the SAT-
domain, which may impact efﬁciency, SAT-TAR utilizes spectra-
based localization for further reﬁnement based on the coverage of
other tests. SAT-TAR uses the information gained from MUC anal-
ysis to generate tests which would prove most useful in reﬁning the
precision of localization. Speciﬁcally, we generate an input whose
trace is most similar to the initial run in terms of its coverage of the
statements in the suspect list. This algorithm aids in producing ef-
fective localization using minimal number of test cases. The beneﬁt
of our technique over directed test generation [5], gets highlighted
on bigger and more complex programs (Section 5).
The test inputs along with the initial suspect list are passed onto
the spectra-based analysis domain. We execute the faulty code on
the test inputs using the user-provided post-condition speciﬁcation
as the correctness oracle to determine failing and passing tests. We
use the same suspiciousness and conﬁdence metrics as used by
Tarantula to rate the statements based on the coverage of the tests,
but a value of 1.0 is added to the suspiciousness of statements in the
suspect list built using the MUC analysis (TarMUCin Tables 2 and
3). This is done to factor in the knowledge gained from the analysis
of the speciﬁcations. The tables show the tests incrementally added
to the test-suite and the corresponding ratings. For headErr , the
test case added in the ﬁrst round is a passing run wherein the in-
putkmatches the key of the second node of the list. This test has
its code coverage most similar to the failing run, hence the suspi-
ciousness of all statements in common between the two traces gets
reduced contributing to signiﬁcant reﬁnement in the ratings (Susp 1
and Conf 1in Table 3). The control passes back to the SAT domain
to generate additional test inputs for subsequent rounds. Such a
passing test case is generated after the second round for sizeErr
(Susp 2and Conf 2in Table 2). This process continues until no more
tests with unique coverage of the code can be generated.
Incremental processing of test cases facilitates the availability
of intermediate localization results which is beneﬁcial, as elabo-
rated in Section 4. Particularly, it enables support for user-control,
wherein the user is presented with the results after every round and
he could choose to stop the addition of tests and further processing
when appropriate. In both the fault cases, the best possible local-
ization of the faulty statements is achieved using the ﬁrst few tests.
Hence, ideally the user could stop further processing after the sec-
ond round for sizeErr and the ﬁrst for headErr .
Observe that faulty statements 7 and 6 are both assigned suspi-
ciousness of 2.0 and conﬁdence of 1.0 (Susp sattar and Conf sattar
in Tables 2 and 3) respectively, which are higher than the ratings for
the correct statements. Hence they rank ﬁrst in a descending order-
ing of the statements based on their suspiciousness and conﬁdence.
Compared to the counterpart techniques (TAR, SAT), we provide
high quality localization consistently for both the fault scenarios.
Multiple Faults : In multi-fault cases, tests covering different faults
interfere with each other impacting the effectiveness of pure spectra-
based localization. We applied Tarantula on a code containing
both the faults ( sizeErr andheadErr ) with a suite generated us-
ing DT having failing tests covering both of them. The localiza-
tion (TarDT) is not very effective as the faulty statements 7 and43Fault
Revealing
Input
Program
SpecSAT Spectra
MUC
Analysis
(Forge)
Test Input
GenerationInitial
Suspect
ListTest
Execution
Spectra
Localization
(Tarantula)
Heuristic
AnalysisFinal
RatingsTest Suite
Updated Suspect ListUpdate
Ratings
Test Generation Strategy ChangeFigure 2: SAT-TAR framework
6 rank third and fourth respectively. Our approach localizes the
faults exposed by the initial failing trace and performs violated-
speciﬁcation based partitioning of failing tests covering different
faults. The smallest fault revealing input for the two fault exam-
ple would be a run removing a node from a list with only one
node. This run violates the remove-ok constraint alone caused
due to headErr on statement 6. Failing runs covering sizeErr
and violating size-ok constraint are not included in the test-suite,
thus preventing decrease in the ratings ( headErr in Table 4). The
suspect list built using MUC analysis, also aids in augmenting the
suspiciousness of statement 6, ranking it second in the list. Sim-
ilarly when starting with an input violating size-ok alone, our
approach eliminates the failing runs violating the remove-ok con-
straint, thus localizing statement 7 with high precision with a rank
of 1 ( sizeErr in Table 4).
4. FRAMEWORK DETAILS
In this section we elaborate on the algorithm and implementa-
tion details of the modules of our framework. Figure 2 presents
an overview. The input to the framework is a fault-revealing test
which can be produced by any testing or static veriﬁcation tech-
nique [10, 9]. The two main constituents of our framework are
SAT-based analysis (Section 4.1) and spectra-based analysis (Sec-
tion 4.2). SAT-based analysis short lists an initial suspect list using
MUC analysis and generates suitable test cases, which are passed
onto the spectra domain. The ratings of the spectra-based localiza-
tion using these tests is heuristically analyzed and updated based on
the suspect list. Feedback from the spectra back to the SAT domain
aids in reﬁning the test generation strategy.
4.1 SAT
MUC Analysis : The Forge framework (Section 2) uses symbolic
execution to translate the code of a procedure into a set of formu-
las in relational logic (F code= {f 1,...,fn}). Given a fault-revealing
input (I fr) and the correctness speciﬁcation ( ψ), we attempt to ﬁnd
a valid path through the code for the given input, yielding an out-
put satisfying the post-condition speciﬁcation. Kodkod is invoked
with the following Alloy formula: F code∧ψ. The pre-state re-
lations are bound to the values in the fault revealing input (I fr)
and the scope is ﬁxed to the size of the input. Due to the fault in
the code, the formula is unsatisﬁable and Kodkod returns a proof
of unsatisﬁability (UNSAT). The RCE strategy of Kodkod is used
to produce the MUC (Section 2), a reduced form which consists
of the core constraints responsible for the violation. MUC = {{f-
muc 1,...,f-muc m},{ψ-muc 1,...,ψ-muc m}}, which is some subset of
{{f1,...,fn},ψ}.
Figure 3 shows the main formulas that appear in the MUC ex-
tracted for the sizeErr . The ﬁrst formula represents the violated((k in(This .header. *next.key)) =>(This .size-1= This .size‘))
!(no This .header) =>(This .header.key = val) =>
This .size‘ = size ++(This -> 0)
Figure 3: MUC for sizeErr .
speciﬁcation constraint size-ok . The second formula represents
the constraints that appear on the path that updates the size ﬁeld.
The ﬁrst two constraints represent conditionals on the pre-state re-
lations evaluated by branches 2 and 3 (Listing 1). The last con-
straint represents the faulty statement 7, as an override of the size
relation wherein the list instance, This , is mapped to a value 0.
Note that only those branches that are relevant to the violation
(branches on which the update statement is control-dependent on),
are included in the MUC. For instance, branch statement 4 is not
included in the MUC for sizeErr .
The Forge framework calculates a coverage metric, in the event
that the formula used to invoke Kodkod is unsatisﬁable, which de-
termines the set of source code statements covered during the anal-
ysis. During the translation from imperative code to logic, a for-
mula slice map is maintained, mapping each formula inserted into
the path-constraint (f i), to the set of statements or slice of code
from which the formula was generated [13] ( constraint strategy of
Forge employed for translation). This is utilized to determine the
set of statements mapping to the formulas in the MUC (f-muc i),
which can be considered responsible for the unsatisﬁability. This
forms the initial list of suspicious statements. For the sizeErr ,
statements 1,2,3,7 are short-listed using this analysis from the list
of all statements in the trace 0,1,2,3,4,6,7,8. The suspiciousness
of statement s is, Susp SAT(s) = 1.0 , if statement s is in the initial
suspect list, otherwise it is 0.0.
Kodkod returns only a single MUC for a violation, which is not
sufﬁcient when the trace covers multiple faults violating more than
one constraints. Hence after processing an MUC, we remove the
corresponding violated constraints from the speciﬁcations, and in-
voke the unsatisﬁability analysis again to extract other MUCs. We
thus form a consolidated list of suspicious statements covering all
faults in the code.
Test Input Generation : This module aims to generate test in-
puts that would provide the most beneﬁt in localizing the fault ex-
posed by the initial failure trace. The algorithm involves two main
steps (pseudo-code in Listing 2): i) using the conditionals on the
failure trace to generate a set of additional inputs covering new
portions of the code, and ii) selecting the input which is the most
similar to the failure trace as the next test input ( similarTrace ).
The heuristic behind this technique is that if the input with a trace
most similar to the failure trace happens to be a passing run then it
would provide the most beneﬁt in improving the precision of the
localization, since a considerable number of statements in com-
mon between the two traces could be considered non-faulty. The
ﬁrst step is accomplished by systematically negating each of the
path constraints on the failure trace, {f-pc 1,...,f-pc n}, obtained by
solving for F codebinding the pre-state to I fr. The following new
path constraints get generated; f-pc 1∧f-pc 2∧...∧ ¬f-pcn, f-pc 1∧f-
pc2∧...∧ ¬f-pcn−1so on until ¬f-pc 1. For every new path con-
straint ( newPC ), SAT is employed to look for a valid input whose
trace includes that constraint. Given a failure trace and its unsatis-
ﬁability core, we deﬁne the similarity score of another trace with
respect to the former, as the number of source code statements it
covers that map to the former’s MUC. The trace with the highest
similarity score is considered most similar. The selected trace needs
to cover at least one statement from the suspect list. Our similarity
criterion ensures that when there are more than one traces having
the same number of statements in common with the failure trace,44the one that covers more number of suspicious statements is cho-
sen, thus providing maximum beneﬁt in reﬁning the localization,
if it happens to be a passing run. From the generated inputs, the
one that is most similar to the initial failure trace is chosen as the
test input to be added to the suite, which already contains the initial
fault-revealing input.
Trace t e s t G e n e r a t i o n ( L i s t <Stmnt > s u s p e c t L i s t , Trace ce ,
L i s t <Trace > s e l T r a c e s , T e s t G e n S t r a t e g y s t ) {
C o n s t r a i n t [ ] pathConds = p a t h C o n d C o n s t r a i n t s ( ce ) ;
f o r(i n t i = 0 ; i < pathConds . l e n g t h ; i ++) {
C o n s t r a i n t newPC= c o n j u n c t i o n ( pathConds [ 0 ] , . . . ,
pathConds [ i −1] , n e g a t i o n ( pathConds [ i ] ) ) ;
Trace newTrace = KodKod . s o l v e ( c o n j u n c t i o n ( preCond ,
newPC ) ) ;
i f( newTrace . s a t i s f i a b l e ( ) ) {
i f( s t . e q u a l s (NEW_STRATEGY) )
boolean uniqueCov = uniqueCoverage ( newTrace ,
s e l T r a c e s , s u s p e c t L i s t ) ;
e l s e
uniqueCov= uniqueCoverage ( newTrace , s e l T r a c e s ) ;
i f( uniqueCov )
L i s t <Trace > newTraces . add ( newTrace ) ; } }
i n t max = 0 ;
Trace s i m i l a r T r a c e = n u l l ;
f o r( Trace t : newTraces ) {
L i s t <Stmnt > stmtNums = srcCodeStmts ( t ) ;
f o r( Stmnt s t m t : s u s p e c t L i s t ) {
i f( stmtNums . c o n t a i n s ( s t m t ) ) s c o r e ++;
i f( max < s c o r e )
{max = s c o r e ; s i m i l a r T r a c e = t ; } } }
s e l T r a c e s . add ( s i m i l a r T r a c e ) ;
r e t u r n s i m i l a r T r a c e ; }
Listing 2: Test Input Generation Algorithm.
4.2 Spectra
The tests generated in the SAT domain are executed, followed
by spectra-based localization using the formulas of Tarantula. The
suspiciousness and conﬁdence metrics are calculated as follows,
Susp sattar (s) = SuspMUC
Tar (s) + Susp SAT(s)
Conf sattar (s) = ConfMUC
Tar (s).
The superscriptMUCrepresents MUC based test generation. Tar
indicates Tarantula’s spectra-based localization metrics. SAT repre-
sents the metric obtained from SAT-based analysis.
Violated speciﬁcation based ﬁltering of tests : For failing tests,
the violated constraints are analyzed to conﬁrm that they match
the constraints violated by the initial failing run. Tests that violate
distinctly different constraints are not considered since they pass
through different code faults and would wrongly reduce the ratings
of the faulty statements in the initial failure trace. Note that we
could potentially use SAT to statically look for inputs that would
violate the speciﬁcations (failing) or not (passing) and also perform
the ﬁltering out of distinctly different failing tests. However, we
defer these steps until the actual method execution for efﬁciency
and scalability purposes.
Heuristic Analysis of Ratings : The test inputs are generated
and used for localization one at a time. Such incremental addition
and processing of tests facilitates application of heuristics to reﬁne
the precision of the ratings and to provide feedback to improve the
effectiveness of test generation.
•Reﬁnement of Ratings : Localization using the test added in
every round updates the ratings. If the test added in a round happens
to be a failing run, the statements whose suspiciousness and con-
ﬁdence values decrease from the previous round are not faulty in
most probability. This is under the assumption that the new failing
test also covers the same code faults as the initial run. Hence the
ratings of these statements are not augmented (i.e. Susp sattar (s)
= Susp Tar(s)). When the added test violates a subset of the con-
straints violated by the initial failing run, it probably covers a subsetof the faults exposed by the initial run. In such a case, the above
heuristic is not applied since it may reduce the suspiciousness of the
faulty statements present in the initial failure trace and not covered
by the new test.
•Feedback to customize test generation strategy : Analysis of
the ratings also helps improve the test input generation strategy on-
the-ﬂy. Statements heuristically determined to be non-faulty could
be removed from the suspect list. The feedback of this updated
suspect list to the test input generation module improves the ef-
fectiveness of the similarity criterion. We also asses the perfor-
mance of the tests heuristically to identify redundant tests. Big
fault-revealing inputs, such as long lists, lead to the generation of
a series of failing runs with very similar code coverage, provid-
ing little beneﬁt in reﬁning the ratings. Hence the results are ob-
served periodically to detect such series of tests causing no change
in the ratings of the statements in the suspect list. Subsequently,
the test input generation strategy switches to generating tests which
differ from the previous tests not only in their total code cover-
age, but speciﬁcally in their coverage of the suspicious statements
(NEW_STRATEGY in Listing2).
User control (Optional) : The incremental generation and pro-
cessing of tests is continued until no more test cases with unique
coverage can be generated. The user can optionally be presented
with the output after a speciﬁed number of rounds and if he is sat-
isﬁed with the precision he can choose to stop further reﬁnement.
This helps in cases when additional tests could reduce the localiza-
tion accuracy, such as passing runs covering the faulty statement.
Since we generate test cases that are as close to the initial failing
run as possible, the ﬁrst few tests could be assumed to cover the
same code faults in most probability (in multi-fault scenarios). But
as the rounds increase, the chance of hitting new faults increases
and user-control could aid in stopping the process before the gen-
eration of these tests.
4.3 Discussion of Correctness
The correctness of our approach relies on the accuracy of the
speciﬁcation. However, lack of completeness in the speciﬁcation,
leading to the inclusion of statements in the MUC that may not be
responsible for the violations, can be reﬁned by spectra-based lo-
calization. For single fault cases, the MUC is guaranteed to map
to all statements responsible for the violation. The faulty state-
ments are also localized with lower ranks than the correct ones in
most cases. Multi-fault cases require special handling. When mul-
tiple faults occur simultaneously in a single failure trace, we require
each of the faults to violate a distinct speciﬁcation constraint. Our
iterative processing of MUCs aids in exploring the MUCs covering
different faulty statements. Violated speciﬁcations analysis helps
eliminate failing tests that cover different faults and violate con-
straints other than those violated by the initial run. This prevents
interference which causes decrease in the ratings of the faulty state-
ments. Further, generation of tests similar in coverage to the failing
run and user-input based stopping of reﬁnement aid in preventing
the generation of failing tests covering other faults.
5. EVALUATION
We address the following research questions in the evaluation.
RQ1 : Does SAT-TAR perform better than existing pure spectra-
based localization and pure SAT-based analysis of speciﬁcations,
for different types and number of faults and varying complexity of
speciﬁcations?
RQ2 : How much do the individual components of our localization
algorithm, a) test generation using UNSAT core, and b) augmenta-
tion of ratings using UNSAT core, contribute to its effectiveness?45boolean add(int k){
0 Node y = null;
1 Node x = this.root;
2 while (x != null)
3 { y = x;
4 if (k < x.key)
5 x = x.left;
//BSTtraverseErr
x = x.right;
else
6 { if (k > x.key)
7 x = x.right;
else
8 return false;}}
9 x = new Node();
10 x.key = k;
11 if (y == null)
12 this.root = x;
//BSTrootErr
this.root = null;
else
13 {if (k < y.key)
//BSTbranchErr
if (k < y.key)
14 y.left = x;
else
15 y.right = x;}
//BSTrightErr
y.right = y;}
16 x.parent = y;
//BSTparentErr
x.parent = x;
17 this.size += 1;
//BSTsizeErr1,2
this.size = 1;
18 return true;}void addChild(Tree t){
0 if ( t == null} return;
1 BaseTree childTree =
(BaseTree)t;
2 if (childTree.isNil()){
3 if (this.children !=null
&& this.children ==
childTree.children)
4 throw new
RuntimeException();
5 if (childTree.children
!= null){
6 int n = childTree.
children.size();
7 if (children != null){
8 for(int i=0;i<n;i++)
9 { Tree c = childTree.
children.get(i);
10 this.children.add(c);
11 c.setParent(this);
//ANTchildparErr1,2
c.setParent(c);}}
else{
12 children =
childTree.children
13 int i = 0;
14 while (i < n){
15 Tree c = childTree.
children.get(i);
16 c.setParent(this);
//ANTchildErr
c.setParent(c);
17 i++;
//ANTloopErr
i = i + 2;}}}}
else {
18 if (children == null)
19 children = newList();
20 children.add(t);
21 childTree.
setParent(this);
//ANTparentErr1,2
childTree.
setParent(childTree);}}
(a) (b)
Figure 4: Code Snippets of (a) BST.add (b) ANTLR
BaseTree.addChild with error numbers.
Candidates :BST.add(Integer k) : Binary search tree (BST) is a
data-structure with complex structural properties, commonly used
in applications requiring efﬁcient search. The data structure invari-
ants include uniqueness of each element, acyclicity with respect
toleft ,right andparent pointers, size correctness, and search
constraints. The add(Integer k) method (Figure 4a) inserts a
node at an appropriate position in the tree based on the input value.
BaseTree.addChild(Tree t) :ANother Tool for Language Recog-
nition (ANTLR) [22] is a commonly used open source tool to
build recognizers, interpreters, and compilers from grammars (Da-
Capo benchmarks [8]). It is an excellent candidate because it uses
various kinds of data structures, specially trees, as its backbone.
BaseTree class implements a generic tree structure customized to
parse input grammars. Each instance maintains its children as a list,
each child is in turn a tree and has a pointer to its parent . Every
tree node may also contain a token ﬁeld which represents the pay-
load. The addChild(Tree t) is the main method used to build
all tree structures (Figure 4b). Based on the comments and the pro-
gram logic, we derived the following speciﬁcations: acyclicity of
the children list, accurate parent-child relationships, and addition of
child without any unwarranted modiﬁcations to the tree structure.
Fault scenarios : We seeded different types of faults in both these
methods (Table 5), including incorrect updates to data-structure
ﬁelds, local variables which impact the traversal of the tree (BST-Table 5: Single Fault Results. SAT: pure MUC based local-
ization, TarRand: Tarantula with a randomly generated suite,
TarDT: Tarantula with tests generated by [5], and SAT-TAR
(user - SAT-TAR with user-control ).
Fault SAT TarRandTarDTSAT-TAR SAT-TAR
#Tests=1 #Tests=10 <#Tests,Rank> #Tests
Rank Rank (user)
BST-sizeErr1 6 7.3 <6, 9.4> <6, 2> 2
BST-sizeErr2 9 7.3 <6, 10> <6, 5.4> 5
BST-parentErr 11 6.1 <6, 10.8> <6, 2> 4
BST-rightErr 11 1.2 <6, 1> <6, 1> 3
BST-traverseErr 7 2.4 <6, 4.2> <6, 2> 3
BST-branchErr 11 6.1 <6, 6.3> <6, 1> 4
ANT-parentErr1 3 2 <4, 3.4> <4, 1> 3
ANT-parentErr2 4 2 <4, 3.9> <4, 2.1> 3
ANT-childparErr1 8 6.3 <4, 6.6> <4, 3.6> 2
ANT-childparErr2 9 6.3 <4, 6.4> <4, 4.1> 3
ANT-childErr 7 6 <4, 6.8> <4, 1.8> 2
ANT-loopErr 10 6 <5, 6> <5, 4> 1
Table 6: Results of 2 restricted variants of SAT-TAR.
Fault TarDT+ SAT SAT-TAR TarMUCTarDT
Rank
BST-sizeErr1 2.6 2 9 9.4
BST-sizeErr2 7 5.4 9 10
BST-parentErr 8.2 2 2 10.8
BST-rightErr 1 1 1 1
BST-traverseErr 4 2 2 4.2
BST-branchErr 3.2 1 2 6.3
traverseErr), branch conditions (BST-branchErr) and loop indices
(ANT-loopErr). Some faults lead to the violation of just one con-
straint such as BST-parentErr, which only violates the constraint n
= n. left‘.parent‘ , while others such as BST-rightErr violate
many constraints, involving almost all ﬁelds of the data structure.
Error names ending with labels 1 and 2 represent the same code
fault but with different post-condition speciﬁcations, impacting the
UNSAT core. For instance BST-sizeErr1 violates only size‘ =
size + 1 , while BST-sizeErr2 violates size‘ =# this.root‘.
*(right‘ + left‘) , producing a bigger core and suspect list.
Similarly, for ANT-parentErr2, the post-condition only checks if
for every node the parent pointers have been set correctly for all its
children, while Err1 also checks if the input tree has been added
correctly as a child. We also seeded more than one faults simulta-
neously to simulate multi-fault scenarios (Table 7).
Localization techniques : The spectra-based localization approaches
(TarRand,TarDT) use the same suspiciousness and conﬁdence for-
mulas for localization as Tarantula. For TarRand, we used the state-
of-the-art test input generation technique for data-structures, Ko-
rat [9], to generate tests exhaustively up to a size such that there was
a failing run for every fault. For each fault, we used a code version
containing only that fault and selected 10 tests randomly (ensuring
the inclusion of at least one failing run). For TarDT, the test-suite
was generated using the directed test generation algorithm [5]. We
implemented the algorithm using Forge, with SAT as the back-end
technology, instead of concolic execution as done previously. This
enables an efﬁcient application of the technique on data-structure
programs with complex invariants. We employed a prototype im-
plementation of our algorithm, built on top of the Forge framework,
to obtain results for SAT-TAR . In both SAT-TAR and TarDT, tests
Table 7: Multiple faults localization.
Multi-Fault TarRandSAT-TAR SAT
#Tests=1
Rank #Tests Rank Rank
<#Tests,( r1,...,rn)> loc1,loc2 r1,...,rn r1,...,rn
BSTrootparErr <10,(16.3,7.2)> 2,3 2,3 5,10
BSTrootsizrghtErr <10,(6.6,9.1,16.3)> 5,4 4,5,2 12,12,5
ANTtwoErrs <10,(10.3,2.3)> 2,3 5,1 9,3
SLLsizremErr <4,(6,1)> 4 5,1 9,946are incrementally added to the suite. In order to compare the test-
suite effectiveness, we used the same number of tests for TarDTas
that produced by SAT-TAR to localize a particular fault. SAT is
the pure UNSAT core analysis based technique which assigns the
same suspiciousness value of 1.0 to all statements in a suspect list
obtained by mapping from the MUC.
Metrics : The effectiveness of localization was measured by assign-
ing a rank to the faulty statement (Section 2) based on its position
in the descending order of the suspiciousness and conﬁdence rat-
ings of all the statements. The lower the rank of the faulty state-
ment the better the localization. For instance, a faulty statement
with suspiciousness 0.5, conﬁdence 1.0 is ranked ﬁfth in a list with
two statements having suspiciousness 1.0, conﬁdence 1.0 and two
others with suspiciousness 0.5, conﬁdence 1.0.
Kodkod with MiniSAT as the backend SAT solver was employed
to obtain the minimal unsatisﬁable cores. All the experiments were
run on a system with 2.50GHz Core 2 Duo processor and 4.00GB
RAM running Windows 7. The results are an average of 10 runs.
All the localization approaches, except TarRand, start with a sin-
gle failing run. Hence for every run, we used bounded veriﬁcation
(Forge) to generate a fault-revealing input of random size.
Result Discussion : Table 5 shows the ranks assigned to the faulty
statements for each fault under each technique. The # Tests for
TarDTand SAT-TAR is the minimum number of tests amongst the
10 runs (for the run with the fault-revealing input of smallest size).
RQ1 :SAT-TAR vs Tar : For all types of faults, our approach
consistently produces lower ranks using less number of tests than
TarRand. For most of the faults, the best possible ranking is ob-
tained using the ﬁrst few tests and the algorithm could ideally be
stopped at that point under user-control mode (user in Table 5). The
precision is better than TarDTas well, despite the latter approach
using a test-suite produced speciﬁcally for effective localization.
The use of UNSAT core helps ﬁlter out statements not related
to the speciﬁcation violation. For instance, in BST-sizeErr1, MUC
analysis helps narrow down to 6 statements responsible for the vio-
lation, from a total of 14 statements. Even in faults not directly up-
dating ﬁelds appearing in the speciﬁcations, such as ANT-loopErr
and BST-branchErr, around 30% of statements are eliminated based
on MUC analysis. BST-sizeErr2 is one of the many cases where
our heuristic reﬁnement of ratings proves beneﬁcial. On process-
ing two failing test cases, one adding a right child to the tree and the
other a left child, statements updating the right andleft ﬁelds
are rightfully eliminated from being faulty.
The test-suites for both random selection (TarRand) and directed
test generation (TarDT) include many redundant tests with the same
code coverage. Our approach, on the other hand, speciﬁcally gener-
ates test cases with unique statement coverage which are most simi-
lar to the initial failing test in their coverage of the suspicious state-
ments. ANT-parentErr1 is an instance, wherein a passing test most
similar to the initial failing run is produced in the second round,
resulting in the faulty statement being ranked ﬁrst. The randomly
selected tests (TarRand) and the tests generated by DT (TarDT),
both comprise of many failing tests with similar coverage which
assign the same ratings to almost all statements.
Multiple faults : Table 7 shows the results for multi-fault scenar-
ios (r 1,...,rnrepresent ranks of faulty statements 1 to n, loc 1,loc2
indicate the separate localizations runs for each failure trace). The
test-suite used by TarRandcontains failing tests exposing differ-
ent faults simultaneously which decreases the quality of localiza-
tion due to interference. Our approach works on a single failure
trace and builds a suitable suite to localize the faults exposed by
that trace. For SLLsizeremErr, the initial failure trace covers both
removeErr and sizeErr simultaneously (Listing 1), hence iterativeprocessing of MUCs is performed to obtain the complete list of
possibly suspicious statements.
Consider BSTrootparErr, containing two faults simultaneously
inBST.add ; BSTrootErr and BSTparentErr (Figure 4a). Two dis-
tinct failing tests cover each of these faults and violate two distinct
constraints; one this.root’ andall n :n.left’.parent’
= nrespectively. Our approach processes each of these failing tests
separately and eliminates each of these tests from the other fault’s
localization. The faulty statements are thus assigned lower ranks in
their respective localizations as compared to the ranks assigned to
them by TarRandusing a test-suite containing both the failing tests.
Consider ANTtwoErrs, containing ANTparentErr and ANTchild-
parErr simultaneously in BaseTree.addChild . Failing tests cov-
ering each of these faults violate the same parent-child relationship
constraint. The ﬁrst similar test case is a passing run ranking the
faulty statement 11 at 5, while subsequently the failing test expos-
ing the fault on statement 21 gets included pushing the rank higher.
In the user-control mode, the user could ideally stop reﬁnement in
the ﬁrst round to prevent interference from the other failing run.
SAT-TAR vs SAT : The high ranks of faulty statements (SAT in Ta-
bles 5 and 7), highlight the poor precision of the localization based
on pure SAT-based analysis of speciﬁcations. Speciﬁcally, BST-
sizeErr2, ANT-parentErr2, and ANT-childparErr2 have exactly the
same faults as their counterpart error cases ending with 1. However
MUC analysis returns longer suspect lists for these cases either due
to the increase in the complexity or decrease in the completeness
of the post-condition speciﬁcations. On the other hand, SAT-TAR
employs the coverage of additional tests to reﬁne the ratings in the
initial suspect list and hence the precision does not suffer much.
RQ2 : To address RQ2, we compare SAT-TAR with two restricted
variants, each including one speciﬁc component of the technique
but not the other; i) TarDT+ SAT : This represents a mere ag-
gregation of the ratings obtained from the stand-alone application
of pure spectra-based localization and SAT-based analysis (Susp
=SuspDT
Tar+ Susp SAT), ii) TarMUC: This represents the use of
MUC analysis only for test generation (Susp = SuspMUC
Tar ). SAT-
TAR includes both the components, Susp = SuspMUC
Tar + Susp SAT.
Comparing the results of TarDT+ SAT with SAT-TAR (Ta-
ble 6), shows that in most cases, our technique performs better than
a technique that merely aggregates the ratings from pure spectra-
based and SAT-based localizations. This indicates that our test gen-
eration strategy does provide beneﬁts, as highlighted in the compar-
isonTarMUCvs TarDT. In BST-branchErr, a test case inserting a
node into an empty list is the passing test case that is most effec-
tive in reﬁning the precision of localization. The similarity score
assigned to this test using our metric which is based on the cov-
erage of the suspicious statements (Section 4) is higher than the
score assigned to it using the counterpart technique, which is solely
based on the truth values of the path conditions covered (Section
1). Hence this test case is not included in the suite for TarDT. Two
passing test cases having the same code coverage, passing through
signiﬁcantly different portions of code than the initial failing run,
get used which do not help much.
Even for fault-revealing inputs of fairly small size such as a tree
with 3 nodes, TarDTstarts generating series of similar failing tests
(all the 8 tests for BST-parentErr are failing). These tests have
smaller structures exposing the same fault, which help reduce the
suspiciousness of some statements used in tree traversal, but do not
contribute to the reﬁnement as much as a similar passing test case.
For BSTtraverseErr, the suite for TarDThas 5 failing runs and 1
passing run, while SAT-TAR generates 5 passing runs similar to
the initial failing run. The accuracy of the suspect list used by our
similarity metric also gets updated dynamically based on the feed-47Table 8: Localization Times.
Fault TarDTSAT SAT-TAR
Time in secs
BST-sizeErr1 2.91 8.17 8.30
BST-sizeErr2 2.56 7.14 7.27
BST-parentErr 2.97 7.69 7.75
BST-rightErr 3.98 7.23 7.37
BST-traverseErr 2.27 7.41 7.51
BST-branchErr 4.57 7.40 7.54
ANT-parentErr1 3.98 15.42 15.73
ANT-parentErr2 5.31 15.61 15.96
ANT-childparErr1 19.16 95.42 95.88
ANT-childparErr2 18.13 24.27 24.55
ANT-childErr 20.12 36.4 40.10
ANT-loopErr 43.78 15.54 15.96
Time in mins
JTOPAS-Err1 15.77 1.43 18.12
JTOPAS-Err2 1.80 1.45 4.48
back from the heuristic analysis, which aids generation of effective
tests sooner. The results of SAT-TAR are better than TarMUC,
indicating the beneﬁt of UNSAT core based augmentation of suspi-
ciousness ratings. Speciﬁcally, in cases such as BST-sizeErr, where
the tests produced by our algorithm and DT are almost the same,
MUC analysis based ﬁltering helps lower the ranks signiﬁcantly.
JTopas Case Study :
JTopas [1] is a Java library used for parsing arbitrary text data
such as HTML, XML, programming language source code, and
so on. We used SAT-TAR to localize a fairly complex fault (from
version 0.4 of the application in SIR [2]) and another fault seeded
by us. We tested the Tokenizer class by asserting that the parsed
output satisﬁes the following two properties i) the ’@’ sign is fol-
lowed by a keyword inside a Javadoc comment, ii) the number of
open and closed braces are the same. When the input ﬁle (Input1 in
Listing 3) is parsed, the assertion on the ﬁrst property fails, despite
the fact that the token author following the ’@’ sign is a key-
word. Assuming that the test code was error free, we performed
modular analysis of the nextToken method, which parses the in-
put at the next position into an appropriate token. We annotated the
method with a post-condition speciﬁcation checking that an ’@’
sign is always followed by a token that is a keyword. The code of
this method with the called helper methods inlined, was encoded
in the Forge Intermediate Language [13] with a total LOC of 100.
The ﬁrst set of test inputs generated, comprised of different types
of tokens (separator, space, etc.) substituted before and after ’@au-
thor’, leading to a series of similar failing runs. On detecting that
even after processing 3 failing tests, the suspiciousness of the state-
ments in the suspect list did not change, the test generation strat-
egy switched to producing a run differing from the previous runs
in its coverage of the suspicious statements. A passing test case
was produced shooting up the precision of localization. The faulty
statement in the test4Normal helper method which wrongly sets
the type of the author token as Normal instead of Keyword is as-
signed a rank of 2. TarDT, on the other hand, ends up generating
series of 5 similar failing tests shooting up the rank to 29. Input
2 in Listing 3 violates the property on balanced braces, due to the
seeded fault in test4SpecialSequence method. This is local-
ized by our approach to a rank of 1 using just the ﬁrst few tests.
I n p u t 1 : I n p u t 2 :
/∗∗@author ∗/ /∗{}∗/
Listing 3: Fault-revealing inputs for the two faults in JTopas.
Performance : Table 8 shows the average times taken by TarDT,
pure SAT-based analysis and SAT-TAR for localizing each of the
errors. SAT-TAR consumes, on an average, about twice as much
time as pure Spectra-based localization. More than 97% of this
time is spent in the extraction of the unsatisﬁable core. Hence thedifference in times is more pronounced for applications with large
speciﬁcations such as ANTLR, wherein every node of the tree has
multiple children. The large size of the source code impacts the
performance of all three techniques for the JTOPAS application.
Threats to Validity : Use of our implementations of the SAT tech-
nique and the directed test generation algorithm (DT used in TarDT)
may impact the construct and conclusion validity of our experi-
ments. However, comparing the test-suite generated by SAT-TAR
with the SAT-based implementation of DT, aids in rightly attribut-
ing the differences in the results to purely algorithmic differences,
without being impacted by the back-end technology. The number
of test cases used for techniques TarRandand SAT differ from SAT-
TAR. However, they represent typical applications of these existing
approaches in practise. Representatives of the candidate programs
may impact the external validity of the results.
6. RELATED WORK
Directed Test Generation for Effective Fault Localization : Re-
cent work [5] presents a test-generation strategy speciﬁcally di-
rected at localizing faults. Given an arbitrary failure trace, this
algorithm incrementally adds test cases based on their similarity
score with the initial failing run (Section 1). This technique em-
ploys concolic execution [25] to keep track of path-constraints and
generate input conﬁgurations covering new portions of code. The
scalability of this approach has not been evaluated on data struc-
ture methods, wherein a repOk method would have to be symbol-
ically executed before every method invocation to generate valid
inputs. In our approach, specifying correctness constraints declar-
atively and relational modeling of Java programs aid in efﬁcient
representation of the problem in boolean satisﬁability wherein SAT
technology could be leveraged for efﬁcient generation of inputs.
Our evaluation (Section 5) shows that SAT-TAR produces more ef-
fective tests than DT earlier in the generation process.
Correctness Oracle based augmentation of Tarantula’s ratings :
Artzi et al. present an effective localization approach for web ap-
plications [6], that is related to ours in terms of augmenting the
ratings of Tarantula based on an oracle for correctness. However,
this technique is very speciﬁc to errors in PHP applications gen-
erating HTML pages. Also, only statements for which Tarantula
assigns a rating greater than 0.5 are augmented, which is not very
effective for typical errors in data-structure methods wherein both
passing and failing runs cover the erroneous statements. It is not
explicitly shown if multiple error cases are handled effectively by
the technique. The paper presents a novel condition modeling ap-
proach to catch omitted branch statements. We envisage the use of
MUC based analysis for the same purpose. For instance, if the up-
date to size were omitted in the list example, the violated size-ok
constraint would indicate that the size ﬁeld was possibly wrongly
updated or not updated at all. Analyzing the code statements that
the MUC contains, would likely show the omission error.
Localization using unsatisﬁability analysis of SAT : The recently
developed BugAssist tool [20] models programs and their correct-
ness speciﬁcations in boolean satisﬁability and employs SAT solvers
to localize faulty statements. While we use the MUC returned by
Kodkod, this technique uses the maximum satisﬁability core re-
turned by SAT solvers to deduce minimum unsatisﬁable subsets. It
further makes clauses hard to iteratively determine all possible sus-
picious locations in the program. However, the technique has not
been applied to typical data-structure programs. Our evaluation of
pure SAT based localization (Section 5) on data structure methods
shows that the precision of such techniques is poor when used with
complex speciﬁcation constraints. Our approach combats these is-
sues by combining SAT-based and spectra-based analysis.48Localization based on analysis of code, state and contracts : Pro-
gram Slicing based techniques [16, 30, 12] are the earliest works in
the ﬁeld of employing static analysis to isolate a subset of instruc-
tions as suspicious. However, these suffer from very low precision
and efﬁciency. An extension to these techniques [28] discusses the
idea of generating program slices that satisfy speciﬁc constraints on
the inputs. Griesmayer et al. [15] and Ball et al. [7] explore the use
of model-checking technology for fault localization. The former at-
tempts to identify faulty locations by introducing non-deterministic
variables at program components marked suspicious by the user,
while the latter compares the counter-examples returned by the
model-checker with successful traces to identify faulty transitions.
Others such as Zhang et al. [32] and Huang et al. [17], perform
analysis of program states rather than locations to identify infected
states, leading to more precise localization. Delta-debugging is also
a popular approach that analyzes state differences between failing
and passing test cases to identify suspicious values of variables and
their corresponding locations. The AutoFix-E project [29] uses
contracts to synthesize ﬁx schemas for program repair.
Spectrum-based and Instrumentation-based techniques : Spectra-
based techniques[23, 4] utilize dynamic test information such as
code coverage for localization. Set-Union and Set-Intersection are
popular techniques that compute suspect lists by marking state-
ments exclusively executed by failing runs as suspicious. Nearest
Neighbor Technique [24] is an extension, wherein the difference in
the proﬁle of the passing run most similar to the failing run is con-
sidered instead of including all passing test cases. Instrumentation-
based techniques, e.g., statistical debugging [21] collect and an-
alyze runtime data about the instrumented predicates, to ﬁnd the
location of the bugs. Both spectrum-based and instrumentation-
based techniques typically rank the lines of code based on some
suspiciousness measure. Tarantula [19] marks a statement possibly
faulty if it is primarily executed by failing runs rather than being
exclusively covered by them. Our approach utilizes these metrics
for spectra-based localization, however incremental addition and
processing of test cases along with the input from the UNSAT core
analysis generates results with higher precision.
7. CONCLUSION
SAT-TAR combines unsatisﬁability analysis of correctness spec-
iﬁcations and test-spectra based localization in a novel feedback
driven manner to iteratively reﬁne localization results. The tech-
nique shows beneﬁt in multi-fault cases as well. Case studies on
typical data-structure programs indicate that SAT-TAR can produce
more accurate localization than existing approaches.
8. ACKNOWLEDGMENTS
This work was funded in part by the NSF under Grant Nos. CCF-
0845628 and AFOSR grant FA9550-09-1-0351.
9. REFERENCES
[1]JTopas.
http://jtopas.sourceforge.net/jtopas .
[2]Software-artifact Infrastructure Repository. http:
//sir.unl.edu/content/bios/jtopas.php .
[3]R. Abreu, P. Zoeteweij, and A. van Gemund. An evaluation
of similarity coefﬁcients for software fault localization. In
PRDC , 2006.
[4]H. Agrawal and J. R. Horgan. Dynamic program slicing. In
PLDI , 1990.
[5]S. Artzi, J. Dolby, F. Tip, and M. Pistoia. Directed test
generation for effective fault localization. In ISSTA , 2010.
[6]S. Artzi, J. Dolby, F. Tip, and M. Pistoia. Practical fault
localization for dynamic web applications. In ICSE , 2010.[7]T. Ball, M. Naik, and S. K. Rajamani. From symptom to
cause: localizing errors in counterexample traces. In POPL ,
2003.
[8]S. M. Blackburn and others. The DaCapo Benchmarks: Java
Benchmarking Development and Analysis. In OOPSLA ,
2006.
[9]C. Boyapati, S. Khurshid, and D. Marinov. Korat:
Automated testing based on Java predicates. In ISSTA , 2002.
[10] Y. Cheon and G. T. Leavens. A simple and practical approach
to unit testing: The JML and JUnit way. In ECOOP , 2002.
[11] H. Cleve and A. Zeller. Locating causes of program failures.
InICSE , 2005.
[12] R. A. DeMillo, H. Pan, and E. H. Spafford. Critical slicing
for software fault localization. In ISSTA , 1996.
[13] G. D. Dennis. A Relational Framework for Bounded
Program Veriﬁcation . PhD thesis, Massachusetts Institute of
Technology, 2009.
[14] N. Dershowitz, Z. Hanna, and A. Nadel. A scalable algorithm
for minimal unsatisﬁable core extraction. In SAT, 2006.
[15] A. Griesmayer, S. Staber, and R. Bloem. Automated fault
localization for C programs. ENTCS , 174(4), 2007.
[16] T. Gyimóthy, Á. Beszédes, and I. Forgács. An efﬁcient
relevant slicing method for debugging. In ESEC/FSE , 1999.
[17] T.-Y. Huang, P.-C. Chou, C.-H. Tsai, and H.-A. Chen.
Automated fault localization with statistically suspicious
program states. In LCTES , 2007.
[18] D. Jackson. Software Abstractions: Logic, Language and
Analysis . The MIT Press, 2006.
[19] J. A. Jones. Semi-Automatic Fault Localization . PhD thesis,
Georgia Institute of Technology, 2008.
[20] M. Jose and R. Majumdar. Cause clue clauses: Error
localization using maximum satisﬁability. CoRR ,
abs/1011.1589, 2010.
[21] B. Liblit, M. Naik, A. Zheng, A. Aiken, and M. Jordan.
Scalable statistical bug isolation. In PLDI , 2005.
[22] T. Parr and Others. Another Tool for Language Recognition.
http://www.antlr.org/ .
[23] R. G. R Abreu, P Zoeteweij and A. J. C. van Gemund. A
survey of empirical results on program slicing. Advances in
Computers , 62(11), 2004.
[24] M. Renieris and S. P. Reiss. Fault localization with nearest
neighbor queries. In ASE, 2003.
[25] K. Sen, D. Marinov, and G. Agha. CUTE: a concolic unit
testing engine for C. In FSE, 2005.
[26] E. Torlak, F. S.-H. Chang, and D. Jackson. Finding minimal
unsatisﬁable cores of declarative speciﬁcations. In FM, 2008.
[27] E. Torlak and D. Jackson. Kodkod: A relational model ﬁnder.
InTACAS , 2007.
[28] G. A. Venkatesh. The semantic approach to program slicing.
InPLDI , 1991.
[29] Y. Wei, Y. Pei, C. A. Furia, L. S. Silva, S. Buchholz,
B. Meyer, and A. Zeller. Automated ﬁxing of programs with
contracts. In ISSTA , 2010.
[30] M. Weiser. Program slicing. In ICSE , 1981.
[31] J. Zhang, S. Li, and S. Shen. Extracting minimum
unsatisﬁable cores with a greedy genetic algorithm. In AI,
2006.
[32] Z. Zhang, W. K. Chan, T. H. Tse, B. Jiang, and X. Wang.
Capturing propagation of infected program states. In
ESEC/FSE , 2009.49