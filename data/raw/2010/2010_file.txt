Statically Checking API Protocol Conformance
with Mined Multi-Object SpeciÔ¨Åcations
Michael Pradel
Department of Computer Science
ETH Zurich, SwitzerlandCiera Jaspan Jonathan Aldrich
Institute for Software Research
Carnegie Mellon UniversityThomas R. Gross
Department of Computer Science
ETH Zurich, Switzerland
Abstract ‚ÄîProgrammers using an API often must follow
protocols that specify when it is legal to call particular methods.
Several techniques have been proposed to Ô¨Ånd violations
of such protocols based on mined speciÔ¨Åcations. However,
existing techniques either focus on single-object protocols or
on particular kinds of bugs, such as missing method calls.
There is no practical technique to Ô¨Ånd multi-object protocol
bugs without a priori known speciÔ¨Åcations. In this paper, we
combine a dynamic analysis that infers multi-object protocols
and a static checker of API usage constraints into a fully
automatic protocol conformance checker. The combined system
statically detects illegal uses of an API without human-written
speciÔ¨Åcations. Our approach Ô¨Ånds 41 bugs and code smells in
mature, real-world Java programs with a true positive rate of
51%. Furthermore, we show that the analysis reveals bugs not
found by state of the art approaches.
Keywords -Typestate; Static analysis; SpeciÔ¨Åcation mining
I. I NTRODUCTION
Many application programming interfaces (APIs) impose
constraints on clients, for example by prescribing particular
orders of method calls. Violating such constraints can lead
to unexpected behavior, incorrect results, and even program
crashes. Unfortunately, most API usage constraints are not
formally speciÔ¨Åed, making it difÔ¨Åcult for programmers to
learn the constraints and challenging for program analyses
to Ô¨Ånd violations.
As a motivating example, consider Java source code we
found in a commonly used benchmark program (Figure 1).
The program iterates over a list using an iterator and attempts
to remove elements from the list. This usage of Java‚Äôs
Collection API leads to a runtime exception: Programmers
must not modify a collection while iterating over it. As the
example shows, API usage constraints are often subtle, and
violations of them can be hard to detect.
Automatic bug Ô¨Ånding techniques based on mined speci-
Ô¨Åcations [1]‚Äì[3] are a promising approach to Ô¨Ånd API usage
bugs with little effort. This paper focuses on speciÔ¨Åcations
of legal method call sequences through Ô¨Ånite state machines
(FSMs), called API usage protocols [4]‚Äì[11]. While existing
work focuses on checking programs against mined protocols
for a single object [12], [13], this paper presents an auto-
matic bug Ô¨Ånding technique based on mined multi-object
protocols.Considering multiple interacting objects allows our ap-
proach to specify and check constraints that are not ex-
pressible in single-object protocols. Consider the following
examples from the Java standard library:
While iterating over a Collection with an Iterator ,
one must not modify the Collection . Figure 1 vio-
lates this constraint.
To retrieve the entries of a ZipFile , one must obtain
anEnumeration over all entries, iterate through it, and
Ô¨Ånally close the ZipFile .
Threads can synchronize via Condition s and
ReentrantLock s. Before calling the condition‚Äôs
waiting or signaling methods, one must acquire the
lock. Eventually, the lock must be released.
When wrapping resource-like objects, such as a
FileReader wrapped into a BufferedReader ,
closing the wrapping resource is sufÔ¨Åcient, as it will
implicitly close the wrapped resource.
All examples are taken from the protocols mined in the eval-
uation of this work. Despite this variety, we henceforth focus
on collections and iterators to simplify the presentation.
Checking programs against multi-object protocols can re-
veal more bugs than a single-object approach. For example,
our approach detects the concurrent modiÔ¨Åcation in Figure 1.
This bug cannot be found with a protocol specifying only
Iterator s orCollection s; it is only expressible with a
protocol specifying both together.
In addition to Ô¨Ånding more bugs, a multi-object approach
avoids false positives caused by the limited scope of single-
object protocols. For example, calls to Iterator.next()
should be preceded by Iterator.hasNext() ,
Collection.size() , or Collection.isEmpty()
to avoid requesting a non-existing element. A single-object
Iterator protocol specifying the Ô¨Årst possibility leads to
false positives when the other two possibilities are used. In
contrast, a multi-object protocol can specify all possibilities.
The bug detection technique presented in this paper is the
result of combining two analyses: a dynamic miner of multi-
object protocols [8], [10] and a static checker of API usage
constraints, Fusion [15]. Figure 2 provides an overview of
our approach. The dynamic speciÔ¨Åcation miner runs training
programs to infer speciÔ¨Åcations from program traces. TheseLinkedList pinConnections = ...
Iterator i = pinConnections.iterator ();
while (i.hasNext ()) {
PinLink curr = (PinLink) i.next ();
if(...) {
pinConnections .remove (curr);
}
}
Figure 1. Illegal usage of a collection and
an iterator found in the DaCapo bench-
marks [14].Training
ProgramsTarget
Program
WarningsDynamic
Protocol
MinerTranslationStatic
Checker
(Fusion)PrunerMulti-object
API Usage
ProtocolsRelationship-
based
SpeciÔ¨ÅcationsLegal &
Illegal Calls
Figure 2. Overview. The components to translate mined protocols into relationship-based speciÔ¨Åcations and
to prune warnings are the primary technical contributions of this paper.
API usage protocols are translated into relationship-based
speciÔ¨Åcations suitable for the static checker. The static
checker checks the code of a target program against the
speciÔ¨Åcations and classiÔ¨Åes API method calls as potentially
legal or illegal. Some of the potentially illegal calls may be
due to incomplete protocols, so a pruner uses heuristics to
remove results that are likely false positives. The Ô¨Ånal set of
warnings is then presented to the user for manual inspection.
While most related approaches combine mining and
checking into a single analysis [16]‚Äì[19], we build upon
two special purpose analyses. The main reason is that an
analysis made for mining speciÔ¨Åcations is not necessarily
the best choice for checking programs. In particular,
all-in-one approaches typically do not guarantee that each
reported deviation from a common pattern can actually
occur at runtime. Our work avoids this source of false
positives by using a complete static checker.
The key difÔ¨Åculty in blending the protocol miner with
the static checker is to connect the two formalisms used
for encoding speciÔ¨Åcations. While the miner produces FSMs
labeled with method calls, the checker requires relationship-
based annotations of API methods. We present a translation
between these two formalisms, which is the foundation of
the presented bug Ô¨Ånding technique.
Our approach Ô¨Ånds bugs without any a priori known
speciÔ¨Åcations and without human reÔ¨Ånement or selection of
the mined protocols. The price for this automation is that
the combined analysis is neither sound nor complete: Mined
protocols can be incomplete, introducing false positives, and
incorrect, introducing false negatives. Our results show both
problems to be manageable in practice.
To evaluate our work, we analyze the programs of the
DaCapo benchmark suite [14], a 1.6 MLOC collection of
open-source Java programs. Our system detects 41 issues
(26 bugs and 15 suspicious code smells) with a true positive
rate of 51%. This precision is higher than that of comparable
state of the art approaches (29% in [18], 38% in [16],
38% in [17]), and we show by direct comparison that our
approach reveals bugs that these approaches cannot Ô¨Ånd. To
assess which bugs our approach misses, we randomly seed
bugs into the programs, of which the system Ô¨Ånds 70%. We
attribute the remaining 30% to four types of false negatives.
Furthermore, we report on a sensitivity analysis of the prun-
ing heuristics. Finally, we Ô¨Ånd that multi-object protocols are
relevant in practice: 61% of the mined protocols are multi-object protocols, and 44% of the issues our system detects
are found only with a multi-object protocol.
In summary, this paper contributes the following:
1) A fully automatic technique to Ô¨Ånd multi-object pro-
tocol bugs. It requires neither tests to run the target
program nor formal speciÔ¨Åcations.
2) A translation between FSM-based mined speciÔ¨Åca-
tions and a relationship-based API speciÔ¨Åcation for-
malism, which enables the Ô¨Årst contribution.
3) Pruning techniques to improve precision.
4) Empirical evidence that multi-object protocols are
relevant in practice.
5) Empirical evidence that combining a heuristic, dy-
namic speciÔ¨Åcation miner with a precise, static checker
can reveal serious problems in well-tested programs
with a good true positive rate.
II. B ACKGROUND
Our approach uses a dynamic speciÔ¨Åcation miner and
a static checker to respectively produce and check spec-
iÔ¨Åcations of multi-object protocols. Both analyses divide
a protocol into two parts. The Ô¨Årst part determines the
applicability of a protocol, and the second part describes
the constraints imposed by the protocol.
A. SpeciÔ¨Åcation Mining and API Usage Protocols
We use a dynamic speciÔ¨Åcation miner that extracts API
usage protocols from training programs [8], [10]. Any
existing API client can serve as a training program. A
protocol consists of a deterministic Ô¨Ånite state machine
and a set of typed protocol parameters. States represent
the common state of multiple objects or, for single-object
protocols, the state of a single object. Transitions are labeled
with method signatures that are annotated with protocol
parameters naming the receiver, the method parameters and
the return value.
The protocol miner analyzes execution traces, that is,
sequences of method calls and returns. At Ô¨Årst, the miner
extracts subtraces that contain calls to a particular object o,
to objects passed to oas an argument, and to objects returned
byo. Then, all subtraces for a set of receiver types are
summarized into a protocol in such a way that each subtrace
is accepted by the protocol.
The mined protocols distinguish two kinds of states:
setup states and liable states . The setup states establishwhich objects interact in the protocol by binding objects to
protocol parameters. The liable states describe constraints
on method calls that a programmer ought to respect. The
miner constructs protocols in such a way that the set of
parameters bound at a state is unambiguous. States at which
all parameters are bound are liable states; all other states are
setup states.
Figure 3 shows an API usage protocol describing the inter-
play of a collection cand an iterator i. The protocol speciÔ¨Åes
how to use the iterator (call hasNext() before next() ) and
that updating the collection invalidates the iterator. Calling
the method iterator() onc, which returns i, establishes
the interaction between these two objects. Therefore, the
later states (3, 4, and 5) are all liable states. The table in
Figure 3 gives the set of bound protocol parameters for each
state.
B. Relationship-Based Static Checking
This work uses Fusion [15], a relationship-based, static
analysis, to check API clients against speciÔ¨Åcations over
multiple objects. Fusion encodes usage constraints based
onrelationships between objects. A relationship is a user-
deÔ¨Åned, uninterpreted predicate across objects. For example,
a binary relationship rcontains (List; Element )can express
that an object of type List contains another object of type
Element , such as rcontains (list; element ). If a relation-
ship predicate is true, we say that the objects are in the
relationship. Likewise, to say that the objects are not in a
relationship means that the relationship predicate evaluates
to false.
In Fusion, API methods are speciÔ¨Åed with requirements
and effects. A requirement is a logical proposition over
relationship predicates that describes a precondition on a
method. For example, list.remove(element) may re-
quire that rcontains (list; element )holds. An effect is a
postcondition that describes how the truth of relationship
predicates changes after calling a method. For instance,
list.remove(element) may have the effect to remove
(list; element )from rcontains . Both requirements and ef-
fects are guarded by a trigger , a logical proposition that
describes when the requirement or effect applies. A complete
speciÔ¨Åcation of a protocol in Fusion is a set of relationships,
a set of effects, and a set of requirements on the relevant
methods.
Based on speciÔ¨Åcations of API methods, Fusion performs
an intraprocedural analysis of API clients to check whether
they respect the usage constraints. For each call to a method
with a speciÔ¨Åcation, the analysis checks whether all triggered
requirements are fulÔ¨Ålled and applies all triggered effects.
We use a complete variant of Fusion, which guarantees that
any bug found will actually occur dynamically.1
1Fusion can only make this guarantee to the degree that it is given precise
aliasing information; see Section V-C for how this affects results in practice.1 2 3
45c = new
Collection()
c.update()c.update()
c.update()c.update() i =
c.iterator()
i.hasNext()
i.hasNext()i.next()
State 1 2 3 4 5
Bound protocol parameters fg f cg fc; ig fc; ig fc; ig
Figure 3. Protocol describing how to use a collection and an iterator.
The call c.update() summarizes calls that modify the collection,
for example, c.add() orc.remove() . Liable states have a gray
background.
1Collection c = ...
2Iterator i = c.iterator ();
3if(i.hasNext ())
4 System.out.println(i.next ());
5/*current state unknown */
6c.update (); // legal -- invalidates iterator
7if(i.hasNext ()) // bug: iterator not valid anymore
8 System.out.println(i.next ());
Figure 4. The state in line 5 cannot be determined because different paths
lead to it.
III. F ROM MINED PROTOCOLS TO CHECKABLE
SPECIFICATIONS
This section presents a translation of FSM-based proto-
cols into relationship-based constraints suitable for checking
programs. A more detailed and formal description is given
in [20]. The main goal of the translation is to detect protocol
violations while reporting few false positives. That is, the
generated speciÔ¨Åcations should cause a warning only if the
analyzed source code actually violates a protocol. We use
Figure 3 as a running example throughout this section.
A. Challenges
We must deal with two main challenges, the Ô¨Årst being
common to all static analyses and the second being speciÔ¨Åc
to multi-object checking:
1) Limited Static Knowledge: Static analysis inherently
lacks precise knowledge about which program path is taken
and as a result may miss bugs or report false positives. For
example, Figure 4 shows a piece of source code where static
analysis cannot determine the protocol state in line 5. After
the call to hasNext , the protocol is known to be at state 4.
The branch creates two paths, one path on which next() is
called, leading back to state 3, and another path on which
we stay at state 4. Because static analysis cannot determine
which path is taken, the state in line 5 is unknown. Our
combined analysis should nevertheless Ô¨Ånd the call in line 6
to be legal and detect the protocol violation in line 7.
2) Object Interactions: Checking multi-object protocols
is challenging, because calling a method on one object caninÔ¨Çuence the state of other objects. For example, the call
toupdate() in line 6 does not refer to the iterator i, but
directly affects its state. This dependence is implicit in mined
multi-object protocols, where a state is the common state of
all involved objects.
B. Naive Approach: States as Relationships
An obvious but insufÔ¨Åcient way to map protocols to rela-
tionships is to have a unary relationship per state and to track
each object‚Äôs state by updating these state relationships . For
this naive approach, we create an effect for each transition
leading from state xto state y: This effect removes the
object from rxand adds it to ry. To check whether calling
a method is legal, we require that the receiver object is
in a relationship representing a state where the method is
enabled. For example, calling method i=c.iterator()
requires that r2(c); the call has the effect of removing c
from r2and of adding canditor3.
Unfortunately, the approach does not deal well with the
challenges illustrated in Figure 4. The lack of static knowl-
edge whether line 4 is executed leads to a loss of knowledge
in line 5: Because both r3(i)andr4(i)are possible, but not
both, Fusion concludes to know nothing about i. Thus, the
analysis cannot detect that line 7 is illegal. Furthermore, the
approach is oblivious of the relationship between candi,
so that in line 6, i‚Äôs state is not changed. This limitation is
another reason why the naive approach cannot Ô¨Ånd the bug
in line 7.
C. Revised Approach: Triple Bookkeeping
Given a set of protocols, how can we generate
relationship-based speciÔ¨Åcations in a way that addresses
the above challenges? We extend the approach described in
Section III-B by using relationships to reason about three
aspects of objects with respect to a protocol. We call the
idea underlying this revised approach the triple bookkeeping
principle. First, we keep track of whether calling a method
is allowed with a method relationship . This is a unary
relationship for a method of the protocol. If an object is
in a method relationship, it means that one can legally
call the corresponding method on this object. The protocol
in Figure 3 has four method relationships (the types in
parentheses specify which objects can be in a relationship):
rnext(Iterator )
rhasNext (Iterator )
rupdate (Collection )
riterator (Collection )
There is no method relationship for constructor calls because
a newly created object cannot be in any relationship before
the constructor call.
Second, we keep track of the current state of an object
or a set of objects with state relationship s. The semantics
of a state relationship is that if an object or a set of objectsis in this relationship, we know these objects to be at the
corresponding state. In contrast to the naive approach, the
state relationship here is over all objects that are bound at
that state. The states of the protocol in Figure 3 translate
into four state relationships:
r2(Collection )
r3(Collection; Iterator )
r4(Collection; Iterator )
r5(Collection; Iterator )
There is no state relationship for the initial state because no
variables are bound at this state.
Third, we keep track of which objects interact as part of a
multi-object protocol. For this purpose, we create a protocol
relationship over all protocol parameters. The semantics of
the protocol relationship is that if a set of objects is in
this relationship, we know these objects to interact with
each other in the protocol. For example, we create a binary
relationship rprot(Collection; Iterator )for the two-object
protocol in Figure 3.
The three kinds of relationships serve as data structures for
the static analysis. The analysis uses them to check whether
method calls are allowed and to represent the effects of
method calls.
Checking Requirements for Method Calls: To check
whether calling a method is legal, we require that the
receiver is in the corresponding method relationship. For
example, to call i.next() , we require that rnext(i)is true.
The requirements imposed by a protocol are only valid
if the current state is a liable state. Therefore, we guard
requirements with the protocol relationship, ensuring that
the involved objects interact in the protocol and that the
protocol is in a liable state. For example, the requirement
on calling i.next() is guarded by rprot(c; i). This can be
thought of as a logical precondition of the form:
8c :(rprot(c; i) =)rnext(i))
Making Effects: Calling a method can have effects on
the current state, on the currently enabled and disabled
methods, and on the binding of objects to protocols. We
create speciÔ¨Åcations that reÔ¨Çect these effects by adapting
the corresponding relationships. For example, calling i =
c.iterator() as shown in Figure 3 inÔ¨Çuences the state
maintained by the analysis as shown in Table I.
Knowing that a particular method is called may not be
sufÔ¨Åcient to determine which effects to apply and which
requirements to check because a single method can label
multiple transitions. For example, calling c.update() at
state 2 (leading again to state 2) has different effects than
calling the same method at state 3 (leading to state 5).
We handle this problem by guarding effects with triggers
on state relationships. In the example for c.update() , the
translation creates two constraints in Fusion, one triggered
byr2(c)and the other triggered by r3(c; i).Table I
EXAMPLE EFFECTS FROM CALLING I T E R A T O R ().
Protocol effects of calling Effects on relationships
i = c.iterator()
Move to state 3. Set r3(c; i)to true; set r2(c),
r4(c; i), andr5(c; i)to false.
Enable methods i.hasNext()
andc.update() ; disable methods
i.next() andc.iterator() .Set rhasNext (i) and
rupdate (c)to true; set rnext (i)
andriterator (c)to false.
Establish the interaction between c
andi.Setrprot (c; i)to true.
Our revised approach meets the challenges mentioned in
Section III-A. We deal with limited static knowledge by
maintaining both the current state and the currently enabled
and disabled methods. This approach allows the analysis to
recover knowledge that has been lost when merging paths.
Even when the current state is unknown, the analysis may
still know about currently enabled methods and continue the
analysis based on this information. The translation addresses
the problem of interacting objects by representing states
at which nobjects are bound by n-ary state relationships.
Furthermore, we represent the interaction of objects through
the protocol relationship. By using the protocol relationship
as a trigger, the analysis can apply effects on all involved
objects, even if a method references only a subset of them.
Translating the protocol in Figure 3 gives constraints that
detect the bug in Figure 4. Before checking line 6, the
analysis knows the relationships rupdate (c)andrprot(c; i).
That is, calling c.update() is legal and cis known to
interact with i. The effects of the call lead to the following
relationships after line 6: r5(c),r5(i), and:rhasNext (i).
Thus, the precondition for calling i.hasNext() in line 7
does not hold and Fusion reports a warning.
IV. E NSURING PRECISION
A major concern of a bug Ô¨Ånding system is to report
relevant warnings to a programmer without overwhelming
her with false positives. Our approach must deal with two
sources of false positives. The Ô¨Årst source are the mined
protocols, as these will only describe a subset of all legal
method call sequences. The reason for this incompleteness
is that the speciÔ¨Åcation miner is limited to the call sequences
that it observes during the execution of training programs.
Although one can increase the completeness of protocols by
analyzing more executions [10], it is difÔ¨Åcult to guarantee
that a mined protocol contains all legal sequences.
The second source of false positives is imprecision in the
static analysis. Analyzing object-oriented languages imposes
several challenges, such as aliasing and the need to deal with
all possible execution paths. A static analysis that lists all
possible protocol violations is likely to emit warnings that
cannot happen in a real run of the program.A. Dealing with Incomplete Protocols
We present three automatic pruning techniques to re-
move warnings caused by incomplete protocols. They are
independent of the API, the mining technique, and the
checker, making them generally applicable. The techniques
are based on the assumption that most parts of the training
and target programs use APIs legally. Incomplete protocols
often stand out because they occur rarely during mining or
because the static analysis detects many violations of these
protocols during checking. We use these observations to
identify incomplete protocols and remove the warnings they
cause.
1) Pruning by Number of ConÔ¨Årmations: The speciÔ¨Å-
cation miner counts how often each protocol occurs in
the training programs. We call this value the number of
conÔ¨Årmations of the protocol. Intuitively, a higher number of
conÔ¨Årmations indicates a higher conÔ¨Ådence in the protocol
as there are more concrete examples supporting the protocol.
To avoid false positives due to low conÔ¨Ådence protocols,
we prune all warnings from protocols with a number of
conÔ¨Årmations below a conÔ¨Ågurable threshold.
2) Pruning by Protocol Error Rate: Another useful met-
ric is how many warnings the protocol causes while checking
programs. Incomplete protocols often result in many warn-
ings because previously unseen, legal API usages violate
the mined protocol. We compute the protocol error rate
by dividing the number of method calls that are illegal
according to a protocol by the overall number of method
calls checked for this protocol. To avoid false positives by
incomplete protocols that cause unusually many warnings,
we prune all warnings caused by protocols with a protocol
error rate above a conÔ¨Ågurable threshold.
3) Pruning by Method Error Rate: The third metric is
a variation of the protocol error rate. This metric addresses
protocols with unusually many violations caused by a partic-
ular method speciÔ¨Åed in the protocol. Instead of computing
the error rate for the entire protocol, we compute it for each
method that appears in a protocol. The method error rate
is the number of calls to a method that are found to be
illegal divided by the overall number of checked calls to this
method. If a protocol contains a method with a method error
rate above a conÔ¨Ågurable threshold, we prune all warnings
caused by this protocol.
We perform a sensitivity analysis to determine reasonable
pruning thresholds for these metrics (Section V-G).
B. Dealing with Imprecisions of the Static Checker
To handle imprecisions due to static analysis, we use
the complete variant of Fusion, which guarantees to issue
a warning only when the analysis knows for certain that the
constraint is broken. Fusion is able to do this because it
supports three-valued logic; all predicates evaluate to either
true, false, or unknown. The complete variant only issues
warnings when the trigger predicate is true and the requirespredicate is false. Any loss of precision that leads to an
unknown predicate is ignored by this variant, thus ensuring
that there are no false positives.
The complete variant is guaranteed to give no false
positives only if it has precise aliasing information. As real
alias analyses are imprecise, there are still false positives
from this variant due to incorrect aliasing information. To
reduce these as much as possible, we use a heuristic, Ô¨Çow-
based points-to analysis. This analysis reduces false positives
by optimistically assuming that methods return a fresh label
that is not aliased by anything else and by presuming that
Ô¨Åelds are not written to outside of the local context.
V. E VALUATION
To evaluate the effectiveness of our approach, we use it
to check the API usage of twelve real-world Java programs.
In summary, we have the following Ô¨Åndings:
1)Does the approach detect relevant issues in mature
and well-tested programs? We Ô¨Ånd 41 issues (26 bugs
and 15 code smells) in the programs of the DaCapo
benchmark suite [14]. Detecting relevant problems in
these programs is challenging, since most of them are
in production use since several years and have been
analyzed by various researchers. (Section V-B)
2)How precise are the reported warnings? The true
positive rate of the reported warnings is 51%. (Sec-
tion V-C)
3)How many protocols are multi-object protocols, and
how many warnings come from these protocols in
practice? 61% of the mined protocols are multi-object
protocols, and 44% of the true positives are from
multi-object protocols. (Section V-D)
4)Do we Ô¨Ånd bugs missed by existing approaches? We
show through direct comparison that our approach
Ô¨Ånds bugs missed by existing multi-object bug Ô¨Ånding
techniques [16], [18], which those approaches cannot
Ô¨Ånd due to inherent limitations. (Section V-E)
5)Which bugs do we miss? We randomly seed bugs into
the programs and Ô¨Ånd that our system detects 70% of
them. We describe four categories of defects that the
system currently misses. (Section V-F)
6)What are the best Ô¨Åltering thresholds in practice? We
perform a sensitivity analysis and Ô¨Ånd thresholds for
pruning warnings. (Section V-G)
7)Do the issues found justify the costs of the system?
As the system is fully automatic, the only costs are
Ô¨Ånding the initial training programs and evaluating
the warnings to determine which are false positives.
In practice, we found that most false positives were
localized and fast to detect manually. (Section V-H)
A complete list of all detected issues is available in a
companion report [20].A. Experimental Setup and Warning ClassiÔ¨Åcation
As target programs we use the twelve Java programs of
the DaCapo 9.12 benchmark suite (Table II). In total, these
programs sum up to 1.6 million non-blank, non-comment
lines of Java code.
As training programs for the protocol miner, we use a
set of real-world Java programs used in previous work [10].
The training programs are disjoint from the target programs.
We gather execution traces by running unit tests and bench-
marks of the program, and by manually executing programs
(see [10] for more details). In total, we gather 47 million
runtime events, each being a method call or a method return.
We conÔ¨Ågure the protocol miner to extract protocols for the
entire java. *andjavax. *APIs, giving 223 protocols.
We do not Ô¨Ålter or change these 223 protocols manually
and instead evaluate the combined approach with protocols
that are inferred in a fully automatic way .
We manually inspect reported warnings to classify them
into three categories (similar to [18], [21]):
Bug. An error that can lead to a program crash or
unexpected behavior. We only consider a problem a bug
if a programmer can trigger it via the public interface
of the buggy class and if the Javadoc documentation of
this class does not prevent programmers from triggering
the bug.
Code smell . A program property that indicates that
something may go wrong [22]. We include in this
category code that should be changed to improve main-
tainability or performance. We also consider problems
in test classes as code smells if they fulÔ¨Åll the require-
ments for a bug, but if we do neither Ô¨Ånd nor expect
any use of the test class by other classes.
False positive . All remaining warnings.
We say issue to a warning that is either a bug or a code
smell.
B. Examples of Issues Found
In total, our approach detects 41 issues in the analyzed
programs. Figures 1, 5a, 5b and 5c are four representative
examples of warnings reported by our system (slightly
edited for conciseness). The method calls found to violate a
protocol are highlighted.
The Ô¨Årst example (Figure 1 in Section I) is clearly
a bug. The programmer iterates over a list to Ô¨Ånd and
remove a particular element. Unfortunately, this code is at
risk of throwing a ConcurrentModificationException
because a collection must not be modiÔ¨Åed while iterating
over it. Interestingly, testing may not Ô¨Ånd this bug because
the code only throws an exception if the removed element
is not the last element in the list.
The second example (Figure 5a) is also a clear bug.
The programmer illegally uses an iterator by assuming that
next() returns null after the last element. As iteratorsMap comparators = ...
Iterator i = comparators.values (). iterator ();
for(Comparator c = (Comparator) i.next();
c != null;
c = (Comparator) i.next ()) { ... }
(a) Bug: Illegal call to next() .
BufferedReader in = null;
try{
in = newBufferedReader (...);
...
in.close ();
}finally {
if(in != null) {
try{in.close (); }
catch (IOException e) { ... }
}
}
(b) Code smell: Duplicate call to close() .
LinkedList shingleBuf = ...
if(! shingleBuf.isEmpty ()) {
Token firstShingle = (Token) shingleBuf. get(0);
}
(c) False positive caused by an incomplete protocol.
Figure 5. Examples of warnings found. The method calls that violate a
protocol are highlighted.
instead throw an exception if next() is called after reaching
the last element, this code results in an exception (unless the
map contains null as a value, which is not the case here).
We classify the third example (Figure 5b) as a code smell.
The problem is that close() is called twice because the
finally block is always executed, even when no exception
is thrown. Closing an already closed stream has no effect,
except for the cost of an additional method call. A simple
way to enhance the readability of this source code without
changing its semantics is to remove the Ô¨Årst call to close() .
The fourth example (Figure 5c) is a false positive
caused by an incomplete protocol. The mined protocol
forLinkedList does not allow get() directly after
isEmpty() , which is a legal usage of this class. Hence,
this ‚Äúprotocol violation‚Äù is innocuous and not relevant to
a programmer. As for many of the false positives, one can
easily identify this warning as false, even without knowing
the violated protocol.
While many of the detected issues are related to iterators,
our approach applies to a larger range of problems. In fact,
200 of the 223 protocols are unrelated to iterators. We
Ô¨Ånd many iterator-related issues because many calls that
can violate a protocol relate to iterators: A recent study
of various Java programs [23] shows that at least 51%
of all protocol-related calls to the java. *andjavax. *
APIs are iterator-related. However, even among the iterator-
related issues that our system Ô¨Ånds, there are three distinct
kinds of problems. First, we Ô¨Ånd code that does not re-
spect collection boundaries by calling Iterator.next()
without knowing whether a next element exists (Figure 5a).
Second, we Ô¨Ånd code that fails to prepare an object for a
call by invoking Iterator.remove() without a preced-
ingIterator.next() . Finally, we Ô¨Ånd code that calls aTable II
WARNINGS REPORTED FOR EACH TARGET PROGRAM . MO ARE ISSUES
FOUND ONLY WITH MULTI -OBJECT PROTOCOLS .
Program LOC Warnings
Before After pruning
pruning Total Bugs Code smells True
All MO All MO positives
avrora 69,393 45 13 9 8 0 0 69%
batik 186,460 87 1 0 0 0 0 0%
daytrader 12,325 8 0 0 0 0 0 ‚Äî
eclipse 289,641 188 15 2 2 1 1 20%
fop 102,909 127 13 8 2 1 0 69%
h2 120,821 100 1 0 0 0 0 0%
jython 245,016 70 7 2 1 1 0 43%
lucene 124,105 114 13 3 1 3 1 46%
pmd 60,062 61 15 2 0 8 2 67%
sunÔ¨Çow 21,970 17 0 0 0 0 0 ‚Äî
tomcat 161,131 165 2 0 0 0 0 0%
xalan 172,300 11 1 0 0 1 0 100%
Total 1,566,133 993 81 26 14 15 4 51%
method at a state where the call is not allowed by modifying
a collection during iteration (Figure 1).
C. Precision of Reported Warnings
To make our approach applicable in practice, it is critical
to provide a reasonable percentage of relevant warnings
among all warnings that our system reports. We evaluate
the precision of the reported warnings, that is, how many
true and false positives they contain, by manually inspecting
warnings and by classifying them into bugs, code smells, and
false positives. In total, our system reports 81 warnings for
the twelve programs. This number allows us to manually
inspect all reported warnings and accurately assess how
precise our analysis is for these target programs.
Table II summarizes our results. In total, the system
detects 26 bugs and 15 code smells, giving a combined true
positive rate of 51%. These results show that our analysis is
able to pinpoint a serious number of issues in well-tested,
deployed programs, while reporting only one false positive
for each true positive on average.
To better understand the imprecisions of our approach, we
further analyze the false positives that are reported. The 40
false positives fall into three categories:
30 false positives (37% of all warnings) are caused by
incomplete protocols. Mined protocols are inherently at
risk to be incomplete; further progress in speciÔ¨Åcation
mining may reduce this number in the future.
Two false positives (2%) are from imprecisions in the
static checker, more precisely, because the heuristic
points-to analysis does not detect some intricate alias-
ing patterns. Again, future improvements of our points-
to analysis may reduce this number.
The Ô¨Ånal eight false positives (10%) are because of
special program semantics that change a protocol in away that cannot be captured by our training programs.
For example, this occurs when programs assign special
semantics to widely used interfaces, such as jython,
which contains a custom iterator class that provides
an alternative to calling hasNext() . Since our system
is unaware of the specialized jython semantics but
recognizes the class as an iterator, it produces warnings
when jython uses the alternative to hasNext() .
A complete static checker is crucial to ensure the preci-
sion of our approach. The sound variant of Fusion reports
signiÔ¨Åcantly more (potential) protocol violations and leads
to a higher false positive rate. For example, analyzing avrora
with the sound analysis gives 1,805 warnings, compared to
45 from the complete analysis.
D. Multi-Object Bugs
How useful is it to consider multi-object protocols in
addition to single-object protocols? 135 of the 223 mined
protocols (61%) specify more than one object. The bugs
detected by our approach show a similar picture. Table II
lists how many of the detected bugs and code smells are
found only with multi-object protocols (columns ‚ÄúMO‚Äù). In
total, 18 of 41 issues (44%) only become apparent due to a
multi-object protocol. We conclude from these results that
considering multiple objects is an important advantage over
single-object approaches [12], [13], [24].
E. Comparison to Existing Work
We directly compare our approach to recent work on
anomaly detection [16], [18] and to FindBugs [25], a widely
used static bug checker. While not working with protocols,
these approaches are the closest existing ones: They also an-
alyze multi-object interactions and Ô¨Ånd bugs in the use of the
Java API. To compare to GrouMiner [16] and Tikanga [18],
we run our analysis on their respective target programs and
compare the bugs found. We focus on bugs because [16]
uses a slightly different classiÔ¨Åcation for non-bug warnings.
Our analysis identiÔ¨Åes six bugs that GrouMiner misses.
Likewise, GrouMiner Ô¨Ånds four bugs that our analysis
misses. There are no bugs found by both approaches. Com-
pared to Tikanga, our analysis Ô¨Ånds nine bugs that Tikanga
misses and misses two bugs found by Tikanga. Three bugs
are found by both analyses.
We identify two main reasons for these differences. First,
our approach reveals a class of bugs missed by both existing
techniques. While [16] and [18] only Ô¨Ånd missing calls, our
analysis also reports calls that are illegal at a particular state.
For example, a bug missed by GrouMiner but found by our
analysis is a concurrent modiÔ¨Åcation similar to Figure 1. To
the best of our knowledge, no existing technique based on
mined speciÔ¨Åcations can Ô¨Ånd bugs of this kind. Second, each
technique is limited by the mined speciÔ¨Åcations. Each of
the three approaches misses some bugs because the ‚Äúright‚Äù
speciÔ¨Åcation to reveal the bug is missing. To address thislimitation, one can mine larger sets of programs [10] or
automatically enrich mined speciÔ¨Åcations [13].
To compare our analysis to FindBugs, we run FindBugs
on our target programs. Although FindBugs Ô¨Ånds various
problems, it does not Ô¨Ånd any of the bugs revealed by our
analysis. The reason is that FindBugs is based on a pre-
deÔ¨Åned list of bug patterns, including several API usage
bugs‚Äînone of which covers the bugs our technique Ô¨Ånds.
In summary, the comparison with existing work not only
shows that our approach Ô¨Ånds bugs missed by state of the
art approaches, but also conÔ¨Årms an observation by Bessey
et al.: ‚ÄúThe set of bugs found by tool A is rarely a superset
of another tool B.‚Äù [26].
F . Which Bugs Do We Miss?
Giving an exact answer to this question is nearly impossi-
ble for any real-world program, because the set of all bugs in
such a program is unknown. We try to give an approximate
answer by randomly seeding bugs into the programs from
Table II and by trying to Ô¨Ånd these bugs with our system.
A programmer can violate a protocol by omitting a call
or by calling a method in a situation where the call is not
allowed. We consider these two cases and insert bugs by
adding and removing calls. To seed a single bug, we use the
following procedure. Let Cbe the set of all method calls
that match a transition of at least one protocol.
1) Randomly decide between adding a new call and
removing an existing call.
2) a) To add a call, randomly select a method m
containing a call c2C. Randomly select a call c0
from the union of the alphabets of protocols that
cis checked against. Randomly select a statement
ofmand try to insert c0after this statement.
If the insertion is syntactically impossible (for
example, if there is no receiver for c0), repeat
Step 2a.
b) To remove a call, randomly select a method m
containing a call c2Cand remove cfrom m.
3) If the modiÔ¨Åcation does not result in a bug according
to the deÔ¨Ånition from Section V-A, go back to Step 2.
We applied this procedure 50 times, giving 50 known pro-
tocol violation bugs. Our system detects 35 of them (70%).
This rate gives us some conÔ¨Ådence in the effectiveness of
our approach in Ô¨Ånding bugs. The 15 seeded bugs that the
system misses illustrate some limitations of our work:
The system misses Ô¨Åve bugs because a protocol allows
an illegal sequence of method calls. As we mine pro-
tocols from real-world programs, we cannot guarantee
that all speciÔ¨Åed behavior is legal.
The system misses four bugs because the static checker
conservatively assumes all objects to be in an unknown
state at method entry. That is, if the Ô¨Årst call on an
object is illegal, the static checker might miss it.The system misses two bugs where an illegal call occurs
only during the Ô¨Årst pass through a loop. While the
current implementation of the static checker misses
those problems, this limitation can be addressed by
checking the Ô¨Årst pass through a loop independently
from the loop‚Äôs Ô¨Åxpoint.
The system misses four bugs that go beyond
the expressiveness of the inferred protocols. For
instance, illegally calling Iterator.next() after
Iterator.hasNext() has returned false does not
violate a protocol requiring that next() is preceded
byhasNext() . One can address this limitation by
extending protocol transitions with invariants [27].
The existing static checker can handle these more
expressive protocols.
G. Sensitivity Analysis
As described in Section IV-A, we prune warnings from
protocols with few conÔ¨Årmations or a high error rate. Setting
reasonable thresholds for the pruning metrics is critical to
reduce false positives without removing true positives. To
Ô¨Ånd reasonable thresholds, we perform a sensitivity analysis
by manually classifying a random sample of warnings and
by evaluating how different thresholds affect these warnings.
We inspected 200 warnings, of which 50 were chosen
randomly from all warnings that our system reports without
any pruning. The other 150 were chosen by starting to prune
with strict thresholds that remove many warnings, and by
gradually considering more and more warnings. That is, our
sample includes a fair number of warnings representative
for the entire space of warnings, and a denser sample of
warnings in which we expect to Ô¨Ånd most true positives.
The results of evaluating the warnings that our system
reports with different thresholds are shown in Figure 6. Each
graph shows, for different thresholds, the number of reported
warnings along with the number of issues they contain.
The respective other thresholds are kept at their extreme
values (zero conÔ¨Årmations and 100% maximum error rates)
to measure the effect of each pruning metric separately. All
three thresholds reach a point where no additional issues
are found by including further warnings. We chose these
points as the default conÔ¨Åguration for our system. Although
we cannot guarantee that no relevant warning is pruned
with these thresholds (without inspecting all 993 warnings),
we consider the selected thresholds to be reasonable for
the analyzed programs. Programs with a different level of
maturity may require different thresholds.
H. Costs of the System
Our system has a true positive rate of 51%, which raises
the question: Is this high enough to justify using the system?
To answer this, we consider the three primary costs of using
the system:The cost of Ô¨Ånding and running training programs for
mining protocols. The most costly aspect in terms of
human time is Ô¨Ånding other programs that use the
same API, building them, and producing traces of these
programs. This cost would also be higher for APIs
that are not as common as the Java Standard Library.
However, this cost can be amortized over all future uses
of the system.
The cost of running the static analysis. The two factors
that increase the runtime of the static checker are the
size of the target program and the number of constraints
to check. Checking 1.6 MLOC with 223 protocols
requires a standard PC to run the analysis overnight.
However, as the analysis is intraprocedural, it can be
run modularly at a method-level, and it scales linearly
when the time to analyze a method is bounded.
The cost of investigating warnings. The programmer
must investigate each warning to determine whether
it is a true positive or not. In our experience, true
positive warnings and false positives due to incomplete
protocols are generally fast to check. The warnings that
took us the most time were the false positives due
to special program semantics (10%); these warnings
required some knowledge of the analyzed program.
However, we expect that a programmer more familiar
with the code would classify these warnings much
faster.
VI. R ELATED WORK
This work relates to two strands of research: bug Ô¨Ånding
via anomaly detection and static protocol checking. Our
approach is unique in combining these approaches for multi-
object speciÔ¨Åcations.
A. Bug Finding via Anomaly Detection
There are various approaches to mine common usage pat-
terns and detect anomalies. In contrast to the techniques that
we discuss below, our approach builds upon two independent
program analyses for mining and for checking.
Wasylkowski et al. [18] statically extract temporal patterns
from programs and report calls that miss one or more
preceding calls that establish a certain state. Nguyen et
al. [16] propose a static, graph-based analysis that mines
usage patterns consisting of method calls and Ô¨Åeld accesses.
Their analysis reports an anomaly if a method‚Äôs graph is
similar to a commonly found graph but misses a particular
method call or Ô¨Åeld access. Our approach Ô¨Ånds bugs not
detectable by these approaches (Section V-E).
Several approaches use rules saying that calling a set
of methods implies calling another method [3], [17], [19].
These approaches ignore the order of calls, a limitation
addressed by [28], which however only analyses exception
handling code. Other approaches consider pairs of methods 0 200 400 600 800 1000
 0 20  40  60  80 100 0 5 10 15 20 25 30 35 40 45Warnings reported
Minimum nb. of confirmationsWarnings reported
 (left y-axis)
Issues found
 (right y-axis)
 0 200 400 600 800 1000
 0  2  4  6  8  10 0 5 10 15 20 25 30 35 40 45
Maximum protocol error rate (%) 0 200 400 600 800 1000
 0  2  4  6  8  10 0 5 10 15 20 25 30 35 40 45
Issues found
Maximum method error rate (%)Figure 6. Analysis of the sensitivity to different thresholds for pruning warnings. The vertical lines indicate the thresholds we recommend as default
values and that we use for the evaluation.
that are called consecutively [29], program-dependent in-
variants [2], [30], and properties of method parameters and
return values, such as to be non-null or to convey ownership
of a resource [31], [32]. Our work differs in the kind of
speciÔ¨Åcation used and in the kind of bugs found. Gabel et
al. [24] dynamically infer and check method call ordering
constraints; their approach is limited to single objects. Engler
et al. [1] propose a static analysis that infers programmer
‚Äúbeliefs‚Äù, such as that two methods are always paired with
each other, and Ô¨Ånds violations of them. The approach
requires a custom checker for each kind of belief.
In [33], we present a dynamic bug Ô¨Ånding technique
based on mined protocols. It does not report false positives
and automatically exercises training programs for protocol
mining. In contrast to this work, [33] is limited to bugs that
lead to an exception and may not cover the entire program.
B. Protocol Checking with SpeciÔ¨Åcations
We build upon a complete, static checker for multi-object
speciÔ¨Åcations [15]. The main reason for choosing Fusion for
this work is that related static protocol checkers [34], [35]
are sound but incomplete, that is, reported violations may
be infeasible at runtime. In contrast, Fusion is complete and
avoids this source of false positives.
Other approaches Ô¨Ånd protocol violations with runtime
monitoring [36], [37] and static analysis [38]‚Äì[40]. A pre-
requisite for them is a speciÔ¨Åcation describing legal and
illegal call sequences. Our approach addresses this need by
generating checkable speciÔ¨Åcations from mined protocols.
C. Combining Static and Dynamic Analysis
Whaley et al. [12] statically extract protocols from exist-
ing programs and check them dynamically. Instead, we infer
protocols dynamically and check them statically. This allows
us to leverage runtime information to focus on frequently
occurring API usages and to check programs even when no
test suites to exercise the program are available. Dallmeier
et al. [13] evaluate a technique for reÔ¨Åning mined protocols
by searching seeded bugs with the reÔ¨Åned protocols and a
static typestate checker. Contrary to our work, their approach
detects bugs only when they lead to an exception. Both [12]
and [13] are limited to single-object protocols.Yang et al. [41] present a dynamic speciÔ¨Åcation miner
and validate some of the mined speciÔ¨Åcations by feeding
them into a model checker. Their work focuses on pairs of
methods that must be called in alternating order. Yang et al.
manually select mined speciÔ¨Åcations to check, whereas our
approach is fully automatic.
D. Fault Localization
There are techniques for localizing faults based on multi-
object properties [42], [43]. In contrast to our work, they
require program input that triggers a bug.
VII. C ONCLUSIONS
This work tackles the problem of Ô¨Ånding illegal API us-
ages involving multiple objects in an automatic way and with
satisfactory precision. To address this problem, we combine
a precise, static analysis, which requires speciÔ¨Åcations of
API usage constraints, with a dynamic speciÔ¨Åcation miner,
which infers protocols from existing programs. The result
of this symbiosis is a fully automatic bug detection system
formulti-object protocols . Considering multiple interacting
objects allows our approach to specify and check constraints
that are not expressible with single-object protocols.
Experiments with 1.6 MLOC show the effectiveness of
the approach. Our technique detects 41 issues with a 51%
true positive rate; several of these issues cannot be detected
by existing bug detection techniques that use mined speciÔ¨Å-
cations. These results are particularly compelling since the
analyzed software corpus consists of well-tested and mature
programs that are part of a widely used benchmark suite.
Our results show that multi-object protocols are relevant in
practice: 61% of the mined protocols are multi-object pro-
tocols, and 44% of the true positives are from multi-object
protocols. We conclude from these results that combining
a dynamic protocol miner with a precise, static analysis to
Ô¨Ånd multi-object protocol bugs can reveal serious problems
in well-tested programs.
ACKNOWLEDGMENTS
The work presented in this paper was partially supported
by the Swiss National Science Foundation under grant num-
ber 200021-134453, a fellowship from Los Alamos National
Laboratory, the U.S. Department of Defense, and U.S. NSF
grant CCF-0811592.REFERENCES
[1] D. Engler, D. Y . Chen, S. Hallem, A. Chou, and B. Chelf,
‚ÄúBugs as deviant behavior: A general approach to inferring
errors in systems code,‚Äù in SOSP , 2001, pp. 57‚Äì72.
[2] S. Hangal and M. S. Lam, ‚ÄúTracking down software bugs
using automatic anomaly detection,‚Äù in ICSE , 2002, pp. 291‚Äì
301.
[3] Z. Li and Y . Zhou, ‚ÄúPR-Miner: Automatically extracting
implicit programming rules and detecting violations in large
software code,‚Äù in ESEC/FSE , 2005, pp. 306‚Äì315.
[4] G. Ammons, R. Bod ¬¥ƒ±k, and J. R. Larus, ‚ÄúMining speciÔ¨Åca-
tions,‚Äù in POPL , 2002, pp. 4‚Äì16.
[5] S. Shoham, E. Yahav, S. Fink, and M. Pistoia, ‚ÄúStatic speciÔ¨Å-
cation mining using automata-based abstractions,‚Äù in ISSTA ,
2007, pp. 174‚Äì184.
[6] M. Gabel and Z. Su, ‚ÄúJavert: Fully automatic mining of
general temporal properties from dynamic traces,‚Äù in FSE,
2008, pp. 339‚Äì349.
[7] H. Zhong, L. Zhang, and H. Mei, ‚ÄúInferring speciÔ¨Åcations
of object oriented APIs from API source code,‚Äù in APSEC ,
2008, pp. 221‚Äì228.
[8] M. Pradel and T. R. Gross, ‚ÄúAutomatic generation of object
usage speciÔ¨Åcations from large method traces,‚Äù in ASE, 2009,
pp. 371‚Äì382.
[9] H. Zhong, L. Zhang, T. Xie, and H. Mei, ‚ÄúInferring resource
speciÔ¨Åcations from natural language API documentation,‚Äù in
ASE, 2009, pp. 307‚Äì318.
[10] M. Pradel, P. Bichsel, and T. R. Gross, ‚ÄúA framework for
the evaluation of speciÔ¨Åcation miners based on Ô¨Ånite state
machines,‚Äù in ICSM , 2010, pp. 1‚Äì10.
[11] C. Lee, F. Chen, and G. Rosu, ‚ÄúMining parametric speciÔ¨Åca-
tions,‚Äù in ICSE , 2011.
[12] J. Whaley, M. C. Martin, and M. S. Lam, ‚ÄúAutomatic ex-
traction of object-oriented component interfaces,‚Äù in ISSTA ,
2002, pp. 218‚Äì228.
[13] V . Dallmeier, N. Knopp, C. Mallon, S. Hack, and A. Zeller,
‚ÄúGenerating test cases for speciÔ¨Åcation mining,‚Äù in ISSTA ,
2010, pp. 85‚Äì96.
[14] S. M. Blackburn et al. , ‚ÄúThe DaCapo benchmarks: Java
benchmarking development and analysis,‚Äù in OOPSLA , 2006,
pp. 169‚Äì190.
[15] C. Jaspan and J. Aldrich, ‚ÄúChecking framework interactions
with relationships,‚Äù in ECOOP , 2009, pp. 27‚Äì51.
[16] T. T. Nguyen, H. A. Nguyen, N. H. Pham, J. M. Al-Kofahi,
and T. N. Nguyen, ‚ÄúGraph-based mining of multiple object
usage patterns,‚Äù in ESEC/FSE , 2009, pp. 383‚Äì392.
[17] S. Thummalapenta and T. Xie, ‚ÄúAlattin: Mining alternative
patterns for detecting neglected conditions,‚Äù in ASE, 2009,
pp. 283‚Äì294.
[18] A. Wasylkowski and A. Zeller, ‚ÄúMining temporal speciÔ¨Åca-
tions from object usage,‚Äù in ASE, 2009, pp. 295‚Äì306.
[19] M. Monperrus, M. Bruch, and M. Mezini, ‚ÄúDetecting missing
method calls in object-oriented software,‚Äù in ECOOP , 2010,
pp. 2‚Äì25.
[20] M. Pradel, C. Jaspan, J. Aldrich, and T. R. Gross, ‚ÄúStatically
checking api protocol conformance with mined multi-object
speciÔ¨Åcations, companion report,‚Äù ETH Zurich, Tech. Rep.
752, March 2012.
[21] N. Gruska, A. Wasylkowski, and A. Zeller, ‚ÄúLearning from
6,000 projects: Lightweight cross-project anomaly detection,‚Äù
inISSTA , 2010, pp. 119‚Äì130.[22] M. Fowler, Refactoring: Improving the Design of Existing
Code , 1999.
[23] N. E. Beckman, D. Kim, and J. Aldrich, ‚ÄúAn empirical study
of object protocols in the wild,‚Äù in ECOOP , 2011.
[24] M. Gabel and Z. Su, ‚ÄúOnline inference and enforcement of
temporal properties,‚Äù in ICSE , 2010, pp. 15‚Äì24.
[25] D. Hovemeyer and W. Pugh, ‚ÄúFinding bugs is easy,‚Äù in
OOPSLA Companion , 2004, pp. 132‚Äì136.
[26] A. Bessey, K. Block, B. Chelf, A. Chou, B. Fulton, S. Hallem,
C. Henri-Gros, A. Kamsky, S. McPeak, and D. R. Engler, ‚ÄúA
few billion lines of code later: Using static analysis to Ô¨Ånd
bugs in the real world,‚Äù Commun ACM , vol. 53, no. 2, pp.
66‚Äì75, 2010.
[27] D. Lorenzoli, L. Mariani, and M. Pezz `e, ‚ÄúAutomatic gen-
eration of software behavioral models,‚Äù in ICSE , 2008, pp.
501‚Äì510.
[28] S. Thummalapenta and T. Xie, ‚ÄúMining exception-handling
rules as sequence association rules,‚Äù in ICSE , 2009, pp. 496‚Äì
506.
[29] W. Weimer and G. C. Necula, ‚ÄúMining temporal speciÔ¨Åca-
tions for error detection,‚Äù in TACAS , 2005, pp. 461‚Äì476.
[30] M. D. Ernst, J. Cockrell, W. G. Griswold, and D. Notkin,
‚ÄúDynamically discovering likely program invariants to sup-
port program evolution,‚Äù IEEE T Software Eng , vol. 27, no. 2,
pp. 213‚Äì224, 2001.
[31] B. Hackett, M. Das, D. Wang, and Z. Yang, ‚ÄúModular
checking for buffer overÔ¨Çows in the large,‚Äù in ICSE , 2006,
pp. 232‚Äì241.
[32] T. Kremenek, P. Twohey, G. Back, A. Ng, and D. En-
gler, ‚ÄúFrom uncertainty to belief: Inferring the speciÔ¨Åcation
within,‚Äù in OSDI , 2006.
[33] M. Pradel and T. R. Gross, ‚ÄúLeveraging test generation and
speciÔ¨Åcation mining for automated bug detection without
false positives,‚Äù in ICSE , 2012.
[34] E. Bodden, ‚ÄúEfÔ¨Åcient hybrid typestate analysis by determin-
ing continuation-equivalent states,‚Äù in ICSE , 2010, pp. 5‚Äì14.
[35] N. A. Naeem and O. Lhotak, ‚ÄúTypestate-like analysis of
multiple interacting objects,‚Äù in OOPSLA , 2008, pp. 347‚Äì366.
[36] C. Allan, P. Avgustinov, A. S. Christensen, L. Hendren,
S. Kuzins, O. Lhot ¬¥ak, O. de Moor, D. Sereni, G. Sittampalam,
and J. Tibble, ‚ÄúAdding trace matching with free variables to
AspectJ,‚Äù in OOPSLA , 2005, pp. 345‚Äì364.
[37] F. Chen and G. Rosu, ‚ÄúMOP: An efÔ¨Åcient and generic runtime
veriÔ¨Åcation framework,‚Äù in OOPSLA , 2007, pp. 569‚Äì588.
[38] K. Bierhoff and J. Aldrich, ‚ÄúModular typestate checking of
aliased objects,‚Äù in OOPSLA , 2007, pp. 301‚Äì320.
[39] R. DeLine and M. F ¬®ahndrich, ‚ÄúTypestates for objects,‚Äù in
ECOOP , 2004, pp. 465‚Äì490.
[40] S. J. Fink, E. Yahav, N. Dor, G. Ramalingam, and E. Geay,
‚ÄúEffective typestate veriÔ¨Åcation in the presence of aliasing,‚Äù
ACM T Softw Eng Meth , vol. 17, no. 2, pp. 1‚Äì34, 2008.
[41] J. Yang, D. Evans, D. Bhardwaj, T. Bhat, and M. Das, ‚ÄúPer-
racotta: Mining temporal API rules from imperfect traces,‚Äù in
ICSE , 2006, pp. 282‚Äì291.
[42] V . Dallmeier, C. Lindig, and A. Zeller, ‚ÄúLightweight defect
localization for Java,‚Äù in ECOOP , 2005, pp. 528‚Äì550.
[43] L. Mariani, F. Pastore, and M. Pezz `e, ‚ÄúDynamic analysis for
diagnosing integration faults,‚Äù IEEE Trans Softw Eng , vol. 37,
no. 4, pp. 486‚Äì508, 2011.