Dynodroid: An Input Generation System for Android Apps
Aravind Machiry Rohan Tahiliani Mayur Naik
Georgia Institute of Technology, USA
{amachiry, rohan_tahil, naik}@gatech.edu
ABSTRACT
We present a system Dynodroid for generating relevant in-
puts to unmodied Android apps. Dynodroid views an app
as an event-driven program that interacts with its environ-
ment by means of a sequence of events through the Android
framework. By instrumenting the framework once and for
all, Dynodroid monitors the reaction of an app upon each
event in a lightweight manner, using it to guide the gener-
ation of the next event to the app. Dynodroid also allows
interleaving events from machines, which are better at gen-
erating a large number of simple inputs, with events from
humans, who are better at providing intelligent inputs.
We evaluated Dynodroid on 50 open-source Android apps,
and compared it with two prevalent approaches: users man-
ually exercising apps, and Monkey, a popular fuzzing tool.
Dynodroid, humans, and Monkey covered 55%, 60%, and
53%, respectively, of each app's Java source code on average.
Monkey took 20X more events on average than Dynodroid.
Dynodroid also found 9 bugs in 7 of the 50 apps, and 6 bugs
in 5 of the top 1,000 free apps on Google Play.
Categories and Subject Descriptors
D.2.5 [ Software Engineering ]: Testing and Debugging
General Terms
Reliability, Experimentation
Keywords
GUI testing, testing event-driven programs, Android
1. INTRODUCTION
Mobile apps|programs that run on advanced mobile de-
vices such as smartphones and tablets|are becoming in-
creasingly prevalent. Unlike traditional enterprise software,
mobile apps serve a wide range of users in heterogeneous and
demanding conditions. As a result, mobile app developers,
testers, marketplace auditors, and ultimately end users can
benet greatly from what-if analyses of mobile apps.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proÔ¨Åt or commercial advantage and that copies
bear this notice and the full citation on the Ô¨Årst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciÔ¨Åc
permission and/or a fee.
ESEC/FSE ‚Äô13, August 18-26, 2013, Saint Petersburg, Russia
Copyright 2013 ACM 978-1-4503-2237-9/13/08 ...$15.00.What-if analyses of programs are broadly classied into
static and dynamic. Static analyses are hindered by features
commonly used by mobile apps such as code obfuscation, na-
tive libraries, and a complex SDK framework. As a result,
there is growing interest in dynamic analyses of mobile apps
(e.g., [1, 12, 13, 25]). A key challenge to applying dynamic
analysis ahead-of-time, however, is obtaining program in-
puts that adequately exercise the program's functionality.
We set out to build a system for generating inputs to
mobile apps on Android, the dominant mobile app platform,
and identied ve key criteria that we felt such a system
must satisfy in order to be useful:
Robust : Does the system handle real-world apps?
Black-box : Does the system forgo the need for app
sources and the ability to decompile app binaries?
Versatile : Is the system capable of exercising impor-
tant app functionality?
Automated : Does the system reduce manual eort?
Ecient : Does the system generate concise inputs, i.e.,
avoid generating redundant inputs?
This paper presents a system Dynodroid that satises
the above criteria. Dynodroid views a mobile app as an
event-driven program that interacts with its environment by
means of a sequence of events. The main principle under-
lying Dynodroid is an observe-select-execute cycle, in which
it rst observes which events are relevant to the app in the
current state, then selects one of those events, and nally
executes the selected event to yield a new state in which
it repeats this process. This cycle is relatively straightfor-
ward for UI events |inputs delivered via the program's user
interface (UI) such as a tap or a gesture on the device's
touchscreen. In the observer stage, Dynodroid determines
the layout of widgets on the current screen and what kind
of input each widget expects. In the selector stage, Dyno-
droid uses a novel randomized algorithm to select a widget
in a manner that penalizes frequently selected widgets with-
out starving any widget indenitely. Finally, in the executor
stage, Dynodroid exercises the selected widget.
In practice, human intelligence may be needed for exer-
cising certain app functionality, in terms of generating both
individual events (e.g., inputs to text boxes that expect valid
passwords) and sequences of events (e.g., a strategy for win-
ning a game). For this reason, Dynodroid allows a user to
observe an app reacting to events as it generates them, and
lets the user pause the system's event generation, manually
generate arbitrary events, and resume the system's eventgeneration. Our overall system thereby combines the bene-
ts of both the automated and manual approaches.
We discussed how Dynodroid handles UI events, but sig-
nicant functionality of mobile apps is controlled by non-UI
events we call system events , such as an incoming SMS mes-
sage, a request by another app for the device's audio, or a
notication of the device's battery power running low. Sat-
isfying our ve desirable criteria in the presence of system
events is challenging for two reasons. First, the number of
possible system events is very large, and it is impractical to
generate all possible permutations of those events. For in-
stance, Android supports 108 dierent broadcast receivers,
each of which can send notications called intents to an app.
Second, many system events have structured data that must
be constructed and dispatched correctly to the app along-
side an intent. For example, generating an incoming SMS
message event entails constructing and sending a suitable
object of class android.telephony.SmsMessage .
A distinctive aspect of mobile apps is that all such apps,
regardless of how diverse their functionality, are written
against a common framework that implements a signicant
portion of the app's functionality. Dynodroid exploits this
aspect pervasively in observing, selecting, and executing sys-
tem events, as we describe next.
Dynodroid addresses the problem of handling a very large
number of system events by using the observation that in
practice, a mobile app reacts to only a small fraction of
them we call relevant events. An event is relevant to an app
if the app has registered a listener for the event with the
framework. Finding when an app registers (or unregisters)
such a listener does not require modifying the app: it suf-
ces to instrument the framework once and for all. The ob-
server monitors the app's interaction with the framework via
this instrumentation, and presents the selector only events
that the app has registered to listen. Finally, the executor
constructs any data associated with the selected event and
dispatches it to the app. A salient aspect of Dynodroid con-
cerns how it constructs this data. For instance, the listener
for network connectivity change events expects an object of
class android.net.NetworkInfo describing the new status
of the network interface. Dynodroid cannot arbitrarily cre-
ate such objects; it must obtain them from a pool maintained
by system service android.net.ConnectivityManager . Fi-
nally, whenever an event is executed, any previously relevant
event may become irrelevant and vice versa, for the next
observe-select-execute cycle.
We implemented Dynodroid for the Android platform and
applied it to 50 diverse, real-world, open-source apps. We
compared the performance of Dynodroid in terms of each
app's Java source code coverage to two prevalent approaches:
one involving expert users manually exercising these apps,
and another using Monkey [7], a popular fuzzing tool. Dyno-
droid, humans, and Monkey covered 55%, 60%, and 53%
code, respectively, on average per app. Dynodroid was able
to cover 83% of the code covered by humans per app on av-
erage, showing its ability to automate testing. Also, Dyno-
droid achieved peak coverage faster than Monkey, with Mon-
key requiring 20X more events on average, showing the ef-
fectiveness of our randomized event selection algorithm. Fi-
nally, Dynodroid found 9 bugs in 7 of the 50 apps. In a
separate experiment, we applied Dynodroid to the top 1,000
free apps in Google Play, and it found 6 bugs in 5 of them.
We summarize the main contributions of our work:1. We propose an eective approach for generating in-
puts to mobile apps. It is based on a \observe-select-
execute" principle that eciently generates a sequence
of relevant events. To adequately exercise app func-
tionality, it generates both UI and system events, and
seamlessly combines events from human and machine.
2. We show how to observe, select, and execute system
events for Android in a mobile device emulator with-
out modifying the app. The central insight is to tai-
lor these tasks to the vast common framework against
which all apps are written and from which they pri-
marily derive their functionality.
3. We present extensive empirical evaluation of the sys-
tem, comparing it to the prevalent approaches of man-
ual testing and automated fuzzing, using metrics in-
cluding code coverage and number of events, for di-
verse Android apps including both open-source apps
and top free marketplace apps.
The rest of the paper is organized as follows. Section 2
describes the overall system. The next three sections present
its three main parts: Section 3 the executor, Section 4 the
observer, and Section 5 the selector. Section 6 presents our
experimental results. Section 7 surveys related work and
Section 8 concludes.
2. SYSTEM ARCHITECTURE
This section presents the system architecture of Dyno-
droid. Algorithm 1 shows the overall algorithm of Dyno-
droid. It takes as input the number nof events to generate
to an app under test. It produces as output a list Lofn
events it generates. The rst generated event is to install and
start the app in a mobile device emulator. The remaining
n 1 events are generated one at a time in an observe-select-
execute cycle. Each of these events constitutes either a UI
input or non-UI input to the app, which we call UI event
and system event , respectively. The two kinds of events are
conceptually alike: each event of either kind has a type and
associated data. We distinguish between them because, as
we explain below, Dynodroid uses dierent mechanisms to
handle them. The executor executes the current event, de-
noted e, in the current emulator state, denoted s, to yield a
new emulator state that overwrites the current state. Next,
theobserver computes which events are relevant in the new
state. We denote the set of relevant events E. Finally, the
selector selects one of the events from Eto execute next,
and the process is repeated.
Figure 1 presents a dataow diagram of Dynodroid that
provides more details about the mechanisms it uses to im-
plement the executor , the observer , and the selector
on the Android platform.
Theexecutor triggers a given event using the appropri-
ate mechanism based on the event kind. It uses the Android
Debug Bridge ( adb) to send the event to an Android device
emulator that is running the app under test. For UI events,
the ADB host talks to the ADB daemon ( adbd) on the emu-
lator via the monkeyrunner tool. Note that this tool, which
is used to send events to an emulator via an API, is unre-
lated to the Monkey fuzz testing tool, which runs in an adb
shell directly on the emulator. For system events, the ADB
host talks to the Activity Manager tool ( am) on the emu-
lator, which can send system events as intents to running
apps. Section 3 describes the executor in further detail.Selector(((((((((((((((
Instrumented(SDK(
Hierarchy(Viewer(
Monkey(Runner(
ADB(Daemon(
Ac=vity(Manager(Intercepted(calls(from(apps(to(SDK(
App(under(test(
ADB(Host(
Observer(e(is(UI(event(All(relevant(events(E(=(E1(U(E2%Selected(event(e(from(E%e(is(system(event(Executor(
System(events(E2%UI(events(E1%
Device(Emulator(Ini=al(event(e(((install(and(start(app)%Figure 1 :Dataow diagram of Dynodroid for the Android platform.
Algorithm 1 Overall algorithm of Dynodroid.
INPUT: Number n >0 of events to generate.
OUTPUT: ListLofnevents.
L:= empty list
e:= event to install and start app under test
s:= initial program state
forifrom 1tondo
append etoL
// Execute event ein current state sto yield updated state.
s:=executor (e,s)
// Compute set Eof all events relevant in current state s.
E:=observer (s)
// Select an event e2Eto execute in next iteration.
e:=selector (E)
end for
Theobserver computes the set of events that are rele-
vant to the app under test in the current emulator state. It
consists of two parts to handle the two kinds of events: the
Hierarchy Viewer tool for UI events, and the instrumented
framework (SDK) for system events. Hierarchy Viewer pro-
vides information about the app's GUI elements currently
displayed on the device's screen. The observer computes
the set of relevant UI events E1from this information. The
instrumented SDK provides information about broadcast re-
ceivers and system services for which the app is currently
registered. The observer computes the set of relevant sys-
tem events E2from this information. It provides the set of
all relevant events E=E1[E2to the selector . Section 4
provides more details about the observer .
Theselector selects an event from Eas the next event
to be triggered. Dynodroid implements various selection
strategies that one can choose from upfront, including deter-
ministic vs. randomized, and history dependent vs. history
oblivious strategies. Section 5 describes these strategies and
Section 6 evaluates them on a code coverage client for 50
open-source apps.
3. EXECUTOR
Dynodroid allows both machine and human to generate
events in order to combine the benets of automated and
manual input generation. Figure 2 shows how Dynodroid
allows switching between machine and human. The execu-
torlistens to commands from a console and starts in human
mode , in which it does not trigger any events and instead
start%state%Ô¨Ånal%state%human%mode%machine%mode%RESUME&
PAUSE&
START&console%emulator%
console%commands%state%transi1ons%human%executor%
events%to%app%Figure 2 :State transition diagram of Dynodroid.
allows the human to exercise the app uninterrupted in the
emulator, until a RESUME command is received from the
console. At this point, the executor switches to machine
mode and generates events until the given bound nis reached
or a PAUSE command is received from the console. In the
latter case, the executor switches to human mode again.
Human inputs do not count towards the bound n. Also,
nothing prevents the human from exercising the app in ma-
chine mode as well, along with the machine. Finally, the
STOP command from the console stops Dynodroid.
Theexecutor executes an event, chosen by the selec-
tor, on the emulator via the Android Debug Bridge ( adb).
It uses separate mechanisms for executing UI events and sys-
tem events. UI events are triggered using the monkeyrunner
tool. This tool is a wrapper around adbthat provides an
API to execute UI events. System events are triggered us-
ing the Activity Manager tool ( am). Broadcast events of any
type can be triggered using this tool provided the correct
arguments are supplied.
4. OBSERVER
Theobserver computes the set of relevant events after an
event is executed. We consider an event relevant if triggering
it may result in executing code that is part of the app. The
goal of the observer is to eciently compute as small a
set of relevant events as possible without missing any. This
section describes how the observer computes relevant UI
events (Section 4.1) and relevant system events (Section 4.2).
4.1 UI Events
These are events generated by the SDK in response to
interaction by users with the device's input mechanisms.Table 1 :Mapping registered callback methods and
overridden methods to relevant UI events.
Relevant Event
Tap Tap Drag Text
(short) (long)
If app registers callback:
onClickListener X
onLongClickListener X
onTouchListener X X X
onKeyListener X
onCreateContextMenuListener X
If app overrides method:
onTouchEvent X X X
performLongClick X
performClick X
onKeyDown X
onKeyUp X
Dynodroid supports two input mechanisms: touchscreen and
navigation buttons (specically,\back"and\menu"buttons).
We found these sucient in practice, for three reasons: they
are the most common input mechanisms, the mechanisms
we do not support (keyboard, trackball, etc.) are device-
dependent, and there is often redundancy among dierent
input mechanisms.
Theobserver analyzes the app's current UI state in order
to compute relevant UI events. First, it deems clicking each
navigation button as a relevant UI event, since these buttons
are always enabled. Second, it inspects the view hierarchy ,
which is an Android-specic tree representation of the app's
UI currently displayed on the touchscreen. The SDK pro-
vides two ways by which an app can react to inputs to a UI
element: by overriding a method of the corresponding View
object's class, or by registering a callback with it.
The SDK dispatches each input on the touchscreen to the
root node of the view hierarchy, which in turn depending on
the position of the input dispatches it recursively to one of
its children, until the view to which the input was intended
executes a callback and returns true, denoting that the in-
put was successfully handled. The observer obtains the
view hierarchy from a service called ViewServer that runs
on Android devices having debug support. Once it obtains
the view hierarchy, it considers only the View objects at leaf
nodes of the tree as interesting, as these correspond to visi-
ble UI elements that users can interact with. It extracts two
kinds of data from each such object about the correspond-
ing UI element: (a) the set of callback methods registered
and the set of methods overridden by the app, for listening
to inputs to this UI element, and (b) the location and size
of the UI element on the touchscreen (the position of its
top left corner, its width, height, and scaling factor). The
native Hierarchy Viewer does not provide all of the above
data; we modied the Android SDK source to obtain it. The
observer uses the data in item (a) to compute which UI
events are relevant, as dictated by Table 1. It supports all
common touchscreen inputs: tap inputs, limited kinds of
gestures, and text inputs. Finally, the observer uses the
data in item (b) to compute the parameters of each such
event, as dictated by Table 2.
4.2 System Events
These are events generated by the SDK in response to non-
UI inputs, such as an incoming phone call, a change in geo-location, etc. The SDK provides one of two means by which
an app can receive each type of system event: broadcast
receiver andsystem service . In either case, the app registers
a callback to be called by the SDK when the event occurs.
The app may later unregister it to stop receiving the event.
Theobserver uses the same mechanism for extracting
relevant system events via broadcast receivers and system
services: it instruments the SDK to observe when an app
registers (or unregisters) for each type of system event. This
instrumented SDK is a le system.img that is loaded on the
emulator during bootup. It is produced once and for all
by compiling Java source code of the original SDK that is
manually modied to inject the instrumentation. A system
event becomes relevant when an app registers to receive it
and, conversely, it becomes irrelevant when an app unregis-
ters it. As in the case of UI events, the observer computes
not only which system events are relevant, but also what
data to associate with each. Unlike for UI events, however,
this data can be highly-structured SDK objects (instead of
primitive-typed data). We next outline how the observer
handles specic broadcast receivers (Section 4.2.1) and sys-
tem services (Section 4.2.2).
4.2.1 Broadcast Receiver Events
Android provides two ways for apps to register for system
events via a broadcast receiver, depending on the desired
lifetime: dynamically or statically. In the dynamic case, the
receiver's lifetime is from when Context.registerReceiver() is
called to either until Context.unregisterReceiver() is called
or until the lifetime of the registering app component. In
the static case, the receiver is specied in le AndroidMani-
fest.xml, and has the same lifetime as the app. In either case,
the receiver denes a callback method overriding Broadcas-
tReceiver.onReceive() that the SDK calls when the event
occurs (we provide an example below). The observer sup-
ports both kinds of receivers.
The Gingerbread SDK version we instrumented has 108
dierent kinds of system events as intents for which an app
may register via a broadcast receiver. As a proof of con-
cept, we chose the top 25 receivers used by the top 1,000
free apps in the Google Play market. Supporting additional
intents is straightforward: it involves identifying the type of
data associated with the intent and providing valid values
for the data. The data includes an optional URI, and a Bun-
dle object which is a key-value map that contains any extra
information. Finally, for statically registered receivers, we
also explicitly identify the receiver in the intent, since un-
like UI events which are dispatched to a single View object,
broadcast intents are by default dispatched to all receivers
(possibly from several apps) that register for them.
Table 3 shows the type and values of the data used by the
observer for our 25 chosen broadcast intents. All the data
shown, except those of type URI, are provided in a Bun-
dle object. For instance, an app may register the following
receiver for the SMS RECEIVED intent which is broadcast
upon incoming SMS messages:
public class SmsReceiver extends BroadcastReceiver {
@Override public void onReceive(Context c, Intent e) {
Bundle b = e.getExtras();
Object[] p = (Object[]) b.get("pdus");
SmsMessage[] a = new SmsMessage[p.length];
for (int i = 0; i < pdus.length; i++)
a[i] = SmsMessage.createFromPdu((byte[]) p[i]);
} };Table 2 :User event parameters: l,t,w,hdenote left position, top position, width, and height of the
view, respectively. TL, TR, BL, BR, MB, MT, ML, MR denote top left/right, bottom left/right, and mid
bottom/top/left/right points of the view, respectively.
Event Type Parameters Description
Tap Tap(l+w/2,t+h/2) trigger Tap at center of view
LongTap LongTap( l+w/2,t+h/2) trigger LongTap at center of view
Drag random one of: Drag( l,t,l+w,t+h), Drag( l+w,t+h,l,t), randomly trigger one of gestures:
Drag( l,t+h,l+w,t), Drag( l+w,t,l,t+h), TL to BR, BR to TL, BL to TR,
Drag( l,t+h/2,l+w,t+h/2), Drag( l+w,t+h/2,l,t+h/2), TR to BL, ML to MR, MR to ML,
Drag( l+w/2,t,l+w/2,t+h), Drag( l+w/2,t+h,l+w/2, t) MT to MB, MB to MT
Text arbitrary xed string trigger arbitrary text input
To trigger this event, the executor serializes the appro-
priate intent along with a Bundle object that maps a kay
named \pdus" to an SmsMessage object array representing
arbitrary but well-formed phone numbers and message texts.
4.2.2 System Service Events
System services are a xed set of processes that provide
abstractions of dierent functionality of an Android device.
We distinguish services whose oered functionality depends
on app-provided data from those that are independent of
such data. We call such services internally vs.externally
triggered, as they depend on data internal or external to
the app under test. For instance, the AlarmManager service
is internally triggered, as it depends on the alarm duration
given by an app, but the LocationManager service is exter-
nally triggered, as it depends on the device's geo-location.
Dynodroid only controls externally triggered services as the
app itself controls internally triggered services.
Table 4 shows how Dynodroid handles events of each ex-
ternally triggered service. The \Register/Unregister Mech-
anism" shows how an app registers or unregisters for the
service, which the observer observes via SDK instrumen-
tation. Since services are global components, the observer
uses the ID of the app under test to lter out observing other
apps that may also register or unregister for these services.
The \Callback Mechanism" shows how the app species the
callback to handle events by the service. Lastly, the \Trigger
Mechanism" shows how the executor triggers the callback,
usually via a command sent from the ActivityManager tool
running on the emulator (see Section 3).
The following example showing how an app may use the
LocationManager service:
GpsStatus.Listener l = new GpsStatus.Listener() {
@Override public void onGpsStatusChanged(int event) {...}
};
LocationManager lm = getSystemService(LOCATION_SERVICE);
lm.addGpsStatusListener(l);
...;
lm.removeGpsStatusListener(l);
From the point at which the app registers to listen to GPS
status changes by calling addGpsStatusListener() to the point
at which it unregisters by calling removeGpsStatusListener(),
theobserver regards GPS status change as a relevant sys-
tem event. If the selector described in the next section
selects this event, then the executor triggers it by sending
telnet command \geo x G" to the emulator, where Gis an
arbitrary geo-location (a triple comprising a latitude, longi-
tude, and altitude). This in turn results in invoking callback
onGpsStatusChanged() dened by the app.Algorithm 2 Event selection algorithm BiasedRandom .
1:varG: map from (event, set of events) pairs to int
2:G:= empty map
3:INPUT: SetEof relevant events.
4:OUTPUT: An event in E.
5:for each (einE)do
6:if((e; E) is not in domain of G)then
7: // Initialize score of event ein context E.
8: G(e; E) := init score( e)
9:end if
10:end for
11:varL: map from events to int
12:L:= map from each event in Eto 0
13:while truedo
14: e:= event chosen uniformly at random from E
15: if(L(e) =G(e; E))then
16: // Select ethis time, but decrease its chance of being
17: // selected in context Ein future calls to selector .
18: G(e; E) :=G(e; E) + 1
19: return e
20: else
21: // Increase chance of selecting ein the next iteration.
22: L(e) :=L(e) + 1
23: end if
24:end while
25:procedure initscore( e) :int
26: case (e)of
27: Text event: return -1
28: non-Text UI event: return 1
29: system event: return 2
30: end case
5. SELECTOR
Theselector selects an event for the executor to ex-
ecute from the set of relevant events Ecomputed by the
observer . We implemented three dierent selection strate-
gies in the selector , called Frequency ,UniformRandom , and
BiasedRandom . This section describes these strategies.
The Frequency strategy selects an event from Ethat has
been selected least frequently by it so far. The rationale is
that infrequently selected events have a higher chance of ex-
ercising new app functionality. A drawback of this strategy
is that its deterministic nature leads the app to the same
state in repeated runs. In practice, dierent states might be
reached in dierent runs because of non-determinism inher-
ent in dynamic analysis of Android apps, due to factors such
as concurrency and asynchrony; however, we cannot rely on
them to cover much new app functionality.Table 3 :Broadcast receiver events with associated data as implemented in Dynodroid. These implemented
events have no associated data: BATTERY [CHANGEDjLOWjOKAY] ,ACTION POWER [DIS]CONNECTED ,
ACTION SHUTDOWN ,TIME SET,AUDIO BECOMING NOISY ,DATE CHANGED ,USER PRESENT ,ME-
DIA EJECT , and BOOT COMPLETED .
Action Name Data Type : Description Data Value
APPWIDGET UPDATE int[] : IDs of App Widgets to update random (1-10) sized array of random (0-1000) ints
CONNECTIVITY CHANGEandroid.net.NetworkInfo : status of
the network interfacerandom NetworkInfo object from
android.net.ConnectivityManager
PACKAGE ADDEDint : uid assigned to new package,
bool : true if this follows a `removed'
broadcast for the same packageuid of random installed package,
random bool
PACKAGE REMOVEDint : uid previously given to package,
bool : true if removing entire app,
bool : true if an `added' broadcast
for the same package will followuid of random installed package,
random bool,
random bool
PACKAGE REPLACED int : uid assigned to new package uid of random installed package
MEDIA MOUNTEDURI : path to mount point of media,
bool : true if media is read-only\/mnt/sdcard",
false
TIMEZONE CHANGED TimeZone : time-zone representation America/Los Angeles time-zone
MEDIA BUTTONandroid.view.KeyEvent : key event
that caused this broadcastKeyEvent object with action as ACTION UP and
value as KEYCODE MEDIA PLAY PAUSE
SMS RECEIVEDandroid.telephony.SmsMessage[] :
array of received SMS messagesarray of 1 SmsMessage object with arbitrary
MSISDN and message
MEDIA UNMOUNTED URI : path to mount point of media \/mnt/sdcard"
PHONE STATEint : phone state (idle jringingjohook),
String : incoming phone number1 (ringing),
an arbitrary MSISDN
MEDIA SCANNER FINISHED URI \/mnt/sdcard"
NEW OUTGOING CALL String: outgoing phone number an arbitrary MSISDN
The UniformRandom strategy circumvents the above prob-
lem by selecting an event from Euniformly at random. This
is essentially the strategy used by the Monkey fuzz testing
tool, with three key dierences. First, Monkey can only
generate UI events, preventing it from covering app func-
tionality controlled by system events. Second, Monkey does
not compute relevant events and can send many events that
are no-ops in the current state, hindering eciency and con-
ciseness of the generated event sequence. Third, Monkey
does not compute a model of the app's UI, which has pros
and cons. On one hand, it prevents Monkey from identifying
observationally equivalent UI events (e.g., taps at dierent
points of the same button that have the same eect, of click-
ing the button) and hinders eciency and conciseness; on
the other hand, Dynodroid sends mostly xed inputs (see
Table 2) to a widget, and may fail to adequately exercise
custom widgets (e.g., a game that interprets taps at dier-
ent points of a widget dierently).
A drawback of the UniformRandom strategy is that it does
not take any domain knowledge into account: it does not
distinguish between UI events and system events, nor be-
tween dierent contexts in which an event may occur, nor
between frequent and infrequent events. For instance, an
event that is always relevant (e.g., an incoming phone call
event) stands to be picked much more often than one that
is relevant only in certain contexts (e.g., only on a certain
screen of the app). As another example, navigation buttons
(\back" and \menu") are always relevant, but often have dif-
ferent behavior on dierent screens. These observations mo-
tivate our nal and default selection strategy BiasedRandom .
This strategy is shown in Algorithm 2. Like the Frequency
strategy, it maintains a history of how often each event has
been selected in the past, but it does so in a context-sensitive
manner: the context for an event eat a particular instant is
the set Eof all relevant events at that instant. This history isrecorded in global variable Gthat maps each pair ( e; E) to a
score. The map starts empty and is populated lazily. At any
instant, the score for a pair ( e; E) in the map is either  1,
meaning event eis blacklisted in context E(i.e.,ewill never
be selected in context E), or it is a non-negative integer,
with higher values denoting lesser chance of selecting event
ein context Ein the future. We found it suitable to use the
set of relevant events Eas context because it is ecient to
compute (the observer already computes E) and it strikes
a good balance between factoring too little and too much of
the state into the context.
Each time the selector is called using this strategy in an
observe-select-execute cycle, it runs the algorithm on lines
3-25, taking as input the set of relevant events Efrom the
observer and producing as output the selected event e2E
for the executor to execute. It starts by mapping, for ev-
erye2E, the pair ( e; E) to its initial score in global map
G, unless it already exists in G. We bias the initial score
(lines 26-31) depending on the kind of event. If eis a Text
event, its initial score is  1. In other words, text inputs
are blacklisted in all contexts. Intuitively, the reason is that
text inputs are interesting only if they are followed by a non-
text input, e.g., a button click. Hence, we forbid selecting
text inputs in the selector altogether, and instead require
theexecutor to populate all text boxes in the current UI
before it executes the selected non-Text event. We distin-
guish between two kinds of non-Text events: non-Text UI
events and system events. We choose initial score of 1 for
the former and 2 for the latter, reducing the relative chance
of selecting system events. This bias stems from our obser-
vation that system events tend to be relevant over longer
periods than UI events, with UI events typically being rel-
evant only when a certain screen is displayed. A hallmark
of our algorithm, however, is that it never starves any event
in any context. This is ensured by lines 11-25, which re-Table 4 :Handling of events of externally triggerable system services in Dynodroid.
Service Register/Unregister Mechanism Callback Mechanism Trigger Mechanism
Audio-
ManagerregisterMediaButtonEventReceiver( C) /
unregisterMediaButtonEventReceiver()invoke component denoted by
ComponentName object Csend KeyPress event with keycode of
anyMEDIA BUTTON viamonkeyrunner
requestAudioFocus( L) /
abandonAudioFocus()call onAudioFocusChange() in
AudioManager.OnAudioFocus-
ChangeListener object Lsimulate incoming phone call
from a xed MSIDN Nvia
telnet command \gsm call N"
Location-
ManageraddGpsStatusListener( L) /
removeGpsStatusListener()call onGpsStatusChanged() in
GpsStatus.Listener object Lset geo-location to xed value G
via telnet command \geo x G"
addNmeaListener( L) /
removeNmeaListener()call onNmeaReceived() in
GpsStatus.NmeaListener object Lsend xed NMEA data Svia
telnet command \geo nmea S"
addProximityAlert( G,P) /
removeProximityAlert()trigger PendingIntent Pset geo-location to registered proximal
value Gvia telnet command \geo x G"
requestLocationUpdates() /
removeUpdates()if LocationListener specied, call
on[LocationjStatus]Changed() or
onProvider[Enabled jDisabled]();
else call PendingIntent or post to
message queue of given Looperset geo-location twice, via
commands \geo x G1" and
\geo x G2";G1is random
geo-location, G2is based on
G1and registered criteria.
requestSingleUpdate() /
auto unregister after updatesame as above same as above
Sensor-
ManagerregisterListener( L,S) /
unregisterListener()call on[AccuracyjSensor]Changed()
on SensorEventListener object Lset random values x; y; z for sensor S
via telnet command \sensor set Sx:y:z"
Telephone-
Managerlisten( L,S), with
state Snon-zero / zerocall onDataActivity() or any of
several on*Changed() methods
on PhoneStateListener object Ltrigger state change via telnet
command \gsm C D";Cis random gsm
command and Dis random valid data.
Table 5 :Emulator conguration in our evaluation.
Feature Name Feature Value
Device RAM size 4 GB
Emulator hardware features All except GPU emulation
Sdcard size 1 GB
Files on Sdcard pdf:2, img:2, vcf:11, arr:2, zip:4,
(type:count) 3gp:1, m4v:1, mov:1, mp3:3
peatedly pick an event efrom Euniformly at random, until
one satises condition L(e) =G(e; E), in which case it is
returned as the event selected by the current selector call.
Just before returning, we increment G(e; E) to reduce the
chance of picking ein context Ein a future selector call.
Lis a local map that records the number of times event e
was randomly picked by the above process in the current se-
lector call but passed over for not satisfying the condition.
Thus, fewer the number of times that ehas been chosen in
context Ein past selector calls, or higher the number of
times that ehas been passed over in context Ein the current
selector call, the higher the chance that ewill be selected
to execute in context Ein the current selector call.
6. EMPIRICAL EVALUATION
We evaluated Dynodroid on real-world Android apps, and
compared it to two state-of-the-art approaches for testing
such apps: manual testing and automated fuzzing. All our
experiments were done on Linux machines with 8GB mem-
ory and 3.0GHz processors. We used the Gingerbread ver-
sion of Android, which is the most popular version, installed
on 50% of all devices that recently accessed Google Play [6].
Table 5 shows the emulator conguration that we used in all
experiments. For each run, an app was given a freshly cre-
ated emulator along with only default system applications
and the above conguration. We next describe two studies
we performed: measuring app source code coverage (Section
6.1) and nding bugs in apps (Section 6.2).
Figure 3 :Distribution of open-source apps.
6.1 Study 1: App Source Code Coverage
Our rst study measures the app source code coverage
that dierent input generation approaches are able to achieve.
We randomly chose 50 apps from the Android open-source
apps repository F-Droid [3] for this study. These 50 apps
are suciently diverse as evidenced in Figure 3. The SLOC
of these apps ranges from 16 to 21.9K, with a mean of 2.7K.
We obtained app coverage metrics by using Emma [2],
a popular Java source code coverage tool. Emma generates
detailed line coverage metrics to the granularity of branches,
and provides coverage reports in dierent formats that assist
in analysis and gathering statistics.
Evaluated Approaches. We evaluated the following
ve approaches in this study: Dynodroid using each of the
three selection strategies ( Frequency ,UniformRandom ,Biase-
dRandom ); the Monkey fuzz testing tool provided in the An-
droid platform; and manual testing conducted in a study in-
volving ten users. Table 6 shows the setup we used for each
of these ve approaches on each app. We ran each of the
three variants of Dynodroid for 2,000 events, we ran Monkey
for 10,000 events, and we allowed the users in our study to
manually generate an unlimited number of events.
We used dierent numbers of events for Dynodroid and
Monkey because those are the numbers of events that the
two tools were able to generate in roughly the same duration
in three hours for each of the 50 apps on average. DynodroidTable 6 :Testing approaches used to test each app.
Approach #Events #Runs
Dynodroid Frequency 2,000 1
Dynodroid UniformRandom 2,000 3
Dynodroid BiasedRandom 2,000 3
Monkey 10,000 3
Humans no limit2
Table 7 :Kinds of UI events triggered by Monkey.
Event Type Proportion
Touch 15%
Motion 10%
Trackball 15%
Minor Navigation 25%
Major Navigation 15%
System keys 2%
Apps Switch 2%
Others (keyboard/volume/camera buttons) 16%
runs 5X slower than Monkey primarily due to performance
issues with the version of the o-the-shelf Hierarchy Viewer
tool it calls after each event. On the plus side, as we show
below, Dynodroid achieves peak code coverage much faster
than Monkey, requiring far fewer than even the 2,000 events
we generated.
Monkey triggers a large variety of UI events but no system
events. Table 7 summarizes the kinds of UI events it triggers
in its default conguration that we used. These events are
strictly a superset of the kinds of UI events that Dynodroid
can generate (see Section 4.1).
All ten users that we chose in our study are graduate stu-
dents at Georgia Tech who have experience with not only
using Android apps, but also developing and testing them
using Android developer tools. We provided each of them
with each app's source code, the ability to run the app any
number of times in the Android emulator, and the ability
to inspect app source code coverage reports produced from
those runs. They were allowed to provide any kind of GUI
inputs, including intelligent game inputs and login creden-
tials to websites. They were also allowed to modify the en-
vironment by adding/removing les from the emulator's Sd-
card, to manually trigger system events via a terminal by
studying the apps' source code, etc.
We ensured that each app was assigned to at least two
users. Likewise, we ran each automated approach involv-
ing randomization (Monkey, and the UniformRandom and
BiasedRandom strategies in Dynodroid) three times on each
app. We considered the highest coverage that a user or
run achieved for each app. Perhaps surprisingly, for cer-
tain apps, we found fairly signicant variation in coverage
achieved across the three runs by any of the random ap-
proaches. Upon closer inspection, we found certain events in
these apps that if not selected in a certain state, irreversibly
prevent exploring entire parts of the app's state space. Two
possible xes to this problem are: (i) allowing a relatively
expensive event during testing that removes and re-installs
the app (Dynodroid currently installs the app only once in
a run); and (ii) to simply run the app multiple times and
aggregate their results.
Finally, Android apps may often call other apps such as
a browser, a picture editor, etc. To prevent the automated
approaches in the study from wandering far beyond the appunder test, we prevented both Dynodroid and Monkey from
exercising components not contained in the app under test.
Coverage Results. The results of our code coverage
study for the 50 apps are shown in the three plots in Figure 4.
To enable comparisons for each app across plots, each point
on the X axis of all three plots denotes the same app. We
next elaborate upon the results in each of these plots.
Figure 4a compares the code coverage achieved for each
of the 50 apps by Dynodroid vs. Human, where Human de-
notes the user who achieved the best coverage of all users
in our user study for a given app, and Dynodroid uses the
BiasedRandom strategy. The bar for each app has three
parts: the bottom red part shows the fraction of code that
both Dynodroid and Human were able to cover (i.e., the in-
tersection of code covered by them). Atop this part are two
parts showing the fraction of code that only Dynodroid and
only Human were able to cover (i.e., the code covered by one
but not the other).
Both Dynodroid and Human cover 4-91% of code per app,
for an average of 51%. Dynodroid exclusively covers 0-26%
of code, for an average of 4%, and Human exclusively cov-
ers 0-43% of code, for an average of 7%. In terms of the
total code covered for each app, Human easily outperforms
Dynodroid, achieving higher coverage for 34 of the 50 apps.
This is not surprising, given that the users in our study were
expert Android users, could provide intelligent text inputs
and event sequences, and most importantly, could inspect
Emma's coverage reports and attempt to trigger events to
cover any missed code.
But all the ten users in our study also reported tedious-
ness during testing, how easy it was to miss combinations of
events, and that it was especially mundane to click various
options in the settings of apps one by one. Dynodroid could
be used to automate most of the testing eort of Human, as
measured by what we call the automation degree , measured
as the ratio of coverage achieved by the intersection of Dyno-
droid and Human, to the total coverage achieved by Human.
This ratio varies from 8% to 100% across our 50 apps, with
mean 83% and standard deviation 21%. These observations
justify Dynodroid's vision of synergistically combine human
and machine. It already provides support for intelligent text
inputs, where a user with knowledge of an app can specify
the text that it should use (instead of random text) in the
specic text box widget prior to execution, or can pause
its event generation when it reaches the screen, key in the
input, and let it resume (as described in Section 3).
Figure 4b compares the code coverage achieved for each of
the 50 apps by Dynodroid vs. Monkey. It is analogous to Fig-
ure 4a with Monkey instead of Human. Both Dynodroid and
Monkey cover 4-81% of code per app, for an average of 47%.
Dynodroid exclusively covers 0-46% of code, for an average
of 8%, which is attributed to system events that only Dyno-
droid can trigger. Monkey exclusively covers 0-61% of code,
for an average of 6%, which is attributed to the richer set of
UI events that Monkey can trigger (see Table 2 vs. Table 7).
Another reason is that Dynodroid only generates straight
(Drag) gestures but Monkey combines short sequences of
such gestures to generate more complex (e.g., circular) ges-
tures. Finally, Dynodroid uses xed parameter values for UI
events whereas Monkey uses random values, giving it supe-
rior ability to exercise custom widgets. In terms of the total
code covered for each app, however, Dynodroid outperforms
Monkey, achieving higher coverage for 30 of the 50 apps. 0 20 40 60 80 100% App Code Coveragecommon
only Dynodroid
only Human(a)Code coverage achieved by Dynodroid ( BiasedRandom ) vs. Human.
 0 20 40 60 80 100% App Code Coveragecommon
only Dynodroid
only Monkey
(b)Code coverage achieved by Dynodroid ( BiasedRandom ) vs. Monkey.
 100 1000 10000 Number of EventsBiasedRandom
UniformRandom
Frequency
Monkey
(c)Minimum number of events needed for peak code coverage by various approaches.
Figure 4 :Results of the app source code coverage study. Each point on the X axis in all three plots denotes
the same app from the 50 open-source apps used in the study. Figure 4a shows that Dynodroid can be used
to automate to a signicant degree the tedious testing done by humans. Figure 4b shows that Dynodroid
and Monkey get comparable coverage, but Figure 4c shows that Monkey requires signicantly more events
to do so. Note that the Y axis in Figure 4c uses a logarithmic scale.
Figure 4c compares the minimum number of events that
were needed by each automated approach|Monkey, and
Dynodroid using each of the selection strategies|to achieve
peak code coverage for each of the 50 apps (recall that we ran
Monkey for 10,000 events and Dynodroid for 2,000 events).
To strike a good tradeo between measurement accuracy
and performance, we invoke Emma to aggregate coverage
after every 100 events for each approach on each app, and
hence the minimum number of reported events is 100. It
is evident that all three strategies in Dynodroid require sig-
nicantly fewer events than Monkey; in particular, Monkey
requires 20X more events than BiasedRandom on average.
This is despite Dynodroid considering both system and UI
events at each step. The reason is that Dynodroid only ex-
ercises relevant events at each step and also because it iden-
ties observationally equivalent events. Finally, of the three
selection strategies in Dynodroid, BiasedRandom performsthe best, with each of the other two strategies requiring 2X
more events than it on average.
6.2 Study 2: Bugs Found in Apps
The second study we performed shows that Dynodroid is
an eective bug-nding tool and is also robust. To demon-
strate its robustness, we were able to successfully run Dyno-
droid on the 1,000 most popular free apps from Google Play.
The popularity metric used is a score given to each app by
Google that depends on various factors like number of down-
loads and ratings. These apps are uniformly distributed over
all 31 app categories in Google Play: the minimum, maxi-
mum, mean, and standard deviation of the number of apps
in these categories is 26, 55, 40.3, and 6.3, respectively.
We also found that Dynodroid exposed several bugs in
both the 50 open-source apps we chose from F-Droid and
the 1,000 most popular free apps from Google Play. Table 8Table 8 :Bugs found by Dynodroid in the 50 open-source apps from F-Droid and the 1,000 top free apps from
Google Play. The two classes of apps are separated by the double line, with all the open-source apps listed
above. NULL PTR denotes a \null pointer dereference" exception and ARRAY IDX an \array index out of
bounds" exception.
App Name # Bugs Kind Description
PasswordMakerProForAndroid 1 NULL PTR Improper handling of user data.
com.morphoss.acal 1 NULL PTR Dereferencing null returned by an online service.
hu.vsza.adsdroid 2 NULL PTR Dereferencing null returned by an online service.
cri.sanity 1 NULL PTR Improper handling of user data.
com.zocc.applications.aagtl 2 NULL PTR Dereferencing null returned by an online service.
org.beide.bomber 1 ARRAY IDX Game indexes an array with improper index.
com.addi 1 NULL PTR Improper handling of user data.
com.ibm.events.android.usopen 1 NULL PTR Null pointer check missed in onCreate() of an activity.
com.nullsoft.winamp 2 NULL PTR Improper handling of RSS feeds read from online service.
com.almalence.night 1 NULL PTR Null pointer check missed in onCreate() of an activity.
com.avast.android.mobilesecurity 1 NULL PTR Receiver callback fails to check for null in optional data.
com.aviary.android.feather 1 NULL PTR Receiver callback fails to check for null in optional data.
summarizes these bugs. We mined the Android emulator
logs for any unhandled exceptions that were thrown from
code in packages of the app under test while Dynodroid ex-
ercised the app in the emulator. To be conservative, we
checked for only FATAL EXCEPTION , as this exception is
the most severe and causes the app to be forcefully termi-
nated. We manually ascertained each bug to eliminate any
false positives reported by this method but found that all the
bugs were indeed genuine and did cause the app to crash.
7. RELATED WORK
There are broadly three kinds of approaches for generating
inputs to mobile apps: fuzz testing , which generates random
inputs to the app; systematic testing , which systematically
tests the app by executing it symbolically; and model-based
testing , which tests a model of the app. We elaborate upon
each of these three approaches in this section.
Fuzz Testing. The Android platform includes a fuzz
testing tool Monkey [7] that generates a sequence of random
UI events to Android apps in a mobile device emulator. Re-
cent work has applied Monkey to nd GUI bugs [16] and
security bugs [19]. Fuzz testing is a black-box approach, is
easy to implement robustly, is fully automatic, and can e-
ciently generate a large number of simple inputs. But it is
not suited for generating inputs that require human intel-
ligence (e.g., playing and winning a game) nor is it suited
for generating highly specic inputs that control the app's
functionality, and it may generate highly redundant inputs.
Finally, Monkey only generates UI events, not system events.
It is challenging to randomly generate system events due to
the large space of possible such events and highly structured
data that is associated with them.
Systematic Testing. Several recent eorts [9, 17, 22]
have applied symbolic execution [11, 14, 18] to generate in-
puts to Android apps. Symbolic execution automatically
partitions the domain of inputs such that each partition cor-
responds to a unique program behavior (e.g., execution of a
unique program path). Thus, it avoids generating redundant
inputs and can generate highly specic inputs, but it is dif-
cult to scale due to the notorious path explosion problem.
Moreover, symbolic execution is not black-box and requires
heavily instrumenting the app in addition to the framework.
Model-based Testing. Model-based testing has been
widely studied in testing GUI-based programs [10,20,21,26]using the GUITAR [4] framework. GUITAR has been ap-
plied to Android apps, IPhone apps, and web apps, among
others. In general, model-based testing requires users to
provide a model of the app's GUI [24, 27], though auto-
mated GUI model inference tools tailored to specic GUI
frameworks exist as well [8, 23]. One such tool in the An-
droid platform, that Dynodroid also uses for observing rele-
vant UI events, is Hierarchy Viewer [5]. Model-based testing
harnesses human and framework knowledge to abstract the
input space of a program's GUI, and thus reduce redun-
dancy and improve eciency, but existing approaches pre-
dominantly target UI events as opposed to system events.
The EXSYST tool [15] extends model-based testing using a
genetic algorithm to continually rene models from run-time
feedback, with the goal of maximizing the code coverage of
the given app. It uses guidance based on branch distance
measurement and also aims to produce small test suites.
8. CONCLUSION
We presented a practical system Dynodroid for generat-
ing relevant inputs to mobile apps on the dominant Android
platform. It uses a novel \observe-select-execute" princi-
ple to eciently generate a sequence of such inputs to an
app. It operates on unmodied app binaries, it can generate
both UI inputs and system inputs, and it allows combin-
ing inputs from human and machine. We applied it to a
suite of 50 real-world apps and compared its performance to
two state-of-the-art input generation approaches for Android
apps: manual testing done by expert users and fuzz testing
using the popular Monkey tool. We showed that Dynodroid
can signicantly automate testing tasks that users consider
tedious, and generates signicantly more concise input se-
quences than Monkey. We also showed its robustness by ap-
plying it to the top 1,000 free apps on Google Play. Lastly,
it exposed a few bugs in a handful of the apps to which it
was applied.
ACKNOWLEDGMENTS
We thank the anonymous reviewers for insightful comments.
We thank the ten users in our study for their participation.
This research was supported in part by DARPA contract
#FA8750-12-2-0020, NSF award #1253867, and awards from
Google and Microsoft.9. REFERENCES
[1] DroidBox: Android application sandbox.
http://code.google.com/p/droidbox/ .
[2] EMMA: a free Java code coverage tool.
http://emma.sourceforge.net/ .
[3] Free and Open Source App Repository.
https://f-droid.org/ .
[4] GUITAR: A model-based system for automated GUI
testing. http://guitar.sourceforge.net/ .
[5] Hierarchy Viewer. http://developer.android.com/
tools/help/hierarchy-viewer.html .
[6] Historical distribution of Android versions in use.
http://developer.android.com/about/dashboards/
index.html .
[7] UI/Application Exerciser Monkey. http:
//developer.android.com/tools/help/monkey.html .
[8] D. Amaltano, A. Fasolino, S. Carmine, A. Memon,
and P. Tramontana. Using GUI ripping for automated
testing of Android applications. In Proceedings of 27th
Intl. Conf. on Automated Software Engineering
(ASE) , 2012.
[9] S. Anand, M. Naik, H. Yang, and M. Harrold.
Automated concolic testing of smartphone apps. In
Proceedings of ACM Conf. on Foundations of Software
Engineering (FSE) , 2012.
[10] R. Bryce, S. Sampath, and A. Memon. Developing a
single model and test prioritization strategies for
event-driven software. Trans. on Soft. Engr. , 37(1),
2011.
[11] C. Cadar, D. Dunbar, and D. Engler. KLEE:
Unassisted and automatic generation of high-coverage
tests for complex systems programs. In Proceedings of
8th USENIX Symp. on Operating Systems Design and
Implementation (OSDI) , 2008.
[12] W. Enck, P. Gilbert, B.-G. Chun, L. Cox, J. Jung,
P. McDaniel, and A. Sheth. Taintdroid: An
information-ow tracking system for realtime privacy
monitoring on smartphones. In Proceedings of 9th
USENIX Symp. on Operating Systems Design and
Implementation (OSDI) , 2010.
[13] P. Gilbert, B.-G. Chun, L. Cox, and J. Jung. Vision:
automated security validation of mobile apps at app
markets. In Proceedings of 2nd Intl. Workshop on
Mobile Cloud Computing and Services (MCS) , 2011.
[14] P. Godefroid, N. Klarlund, and K. Sen. DART:
Directed automated random testing. In Proceedings ofACM Conf. on Programming Language Design and
Implementation (PLDI) , 2005.
[15] F. Gross, G. Fraser, and A. Zeller. Search-based
system testing: high coverage, no false alarms. In
Proceedings of the 2012 International Symposium on
Software Testing and Analysis (ISSTA) , 2012.
[16] C. Hu and I. Neamtiu. Automating GUI testing for
Android applications. In Proceedings of 6th
IEEE/ACM Workshop on Automation of Software
Test (AST) , 2011.
[17] J. Jeon, K. Micinski, and J. Foster. Symdroid:
Symbolic execution for dalvik bytecode, 2012. http:
//www.cs.umd.edu/~jfoster/papers/symdroid.pdf .
[18] J. King. Symbolic execution and program testing.
CACM , 19(7):385{394, 1976.
[19] R. Mahmood, N. Esfahani, T. Kacem, N. Mirzaei,
S. Malek, and A. Stavrou. A whitebox approach for
automated security testing of Android applications on
the cloud. In Proceedings of 7th IEEE/ACM Workshop
on Automation of Software Test (AST) , 2012.
[20] A. Memon, M. Pollack, and M. Soa. Automated test
oracles for GUIs. In Proceedings of ACM Conf. on
Foundations of Software Engineering (FSE) , 2000.
[21] A. Memon and M. Soa. Regression testing of GUIs.
InProceedings of ACM Conf. on Foundations of
Software Engineering (FSE) , 2003.
[22] N. Mirzaei, S. Malek, C. Pasareanu, N. Esfahani, and
R. Mahmood. Testing Android apps through symbolic
execution. In Java Pathnder Workshop (JPF) , 2012.
[23] T. Takala, M. Katara, and J. Harty. Experiences of
system-level model-based GUI testing of an Android
app. In Proceedings of 4th Intl. Conf. on Software
Testing, Verication and Validation (ICST) , 2011.
[24] L. White and H. Almezen. Generating test cases for
GUI responsibilities using complete interaction
sequences. In Proceedings of 11th IEEE Intl. Symp. on
Software Reliability Engineering (ISSRE) , 2000.
[25] L. Yan and H. Yin. DroidScope: Seamlessly
reconstructing the OS and Dalvik semantic views for
dynamic Android malware analysis. In Proceedings of
21st USENIX Security Symposium , 2012.
[26] X. Yuan, M. Cohen, and A. Memon. GUI interaction
testing: Incorporating event context. Trans. on Soft.
Engr. , 37(4), 2011.
[27] X. Yuan and A. Memon. Generating event
sequence-based test cases using GUI runtime state
feedback. Trans. on Soft. Engr. , 36(1), 2010.