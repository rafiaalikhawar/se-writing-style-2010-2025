See discussions, st ats, and author pr ofiles f or this public ation at : https://www .researchgate.ne t/public ation/220266033
A Degree-of-Knowledge Model to Captu re Sou rce Code Familiarity
Conf erence Paper    in  Proceedings - Int ernational Conf erence on Softw are Engineering  · May 2010
DOI: 10.1145/1806799.1806856  · Sour ce: DBLP
CITATIONS
130READS
562
4 author s, including:
Thomas F ritz
Univ ersity of Z urich
81 PUBLICA TIONS    3,153  CITATIONS    
SEE PROFILE
Jing wen Ou
Univ ersity of British Columbia
6 PUBLICA TIONS    206 CITATIONS    
SEE PROFILE
Gail Murphy
Univ ersity of British Columbia
197 PUBLICA TIONS    14,800  CITATIONS    
SEE PROFILE
All c ontent f ollo wing this p age was uplo aded b y Gail Murphy  on 29 May 2014.
The user has r equest ed enhanc ement of the do wnlo aded file.A Degree-of-Knowledge Model to Capture Source Code
Familiarity
Thomas Fritz, Jingwen Ou, Gail C. Murphy and Emerson Murphy-Hill
Department of Computer Science
University of British Columbia
Vancouver, BC, Canada
{fritz,jingweno,murphy,emhill}@cs.ubc.ca
ABSTRACT
The size and high rate of change of source code comprising a
software system make it di±cult for software developers to
keep up with who on the team knows about particular parts
of the code. Existing approaches to this problem are based
solely on authorship of code. In this paper, we present data
from two professional software development teams to show
that both authorship and interaction information about how
a developer interacts with the code are important in charac-
terizing a developer's knowledge of code. We introduce the
degree-of-knowledge model that computes automatically a
real value for each source code element based on both au-
thorship and interaction information. We show that the
degree-of-knowledge model can provide better results than
an existing expertise ¯nding approach and also report on
case studies of the use of the model to support knowledge
transfer and to identify changes of interest.
Categories and Subject Descriptors
D.2.6 [ Software Engineering ]: Programming Environments
General Terms
Human Factors
Keywords
expertise, authorship, degree-of-interest, interaction, degree-
of-knowledge, onboarding, recommendation
1. INTRODUCTION
Software developers working with source code face a del-
uge of information daily. The development environments
they use provide fast access to the many (often millions of)
lines of code comprising the systems on which they work.
The hard work of their teammates often results in a high
rate of change in that code. For a professional software
c°ACM, 2010. This is the author’s version of the work. It is
posted here by permission of ACM for your personal use. Not
for redistribution. The deﬁnitive version was published in ICSE’10
http://doi.acm.org/10.1145/nnnnnn.nnnnnn.
ICSE ’10, May 2-8 2010, Cape Town, South Africa
.development team we studied, each developer was, on av-
erage, accepting changes to over one thousand source code
elements per day from other team members into their envi-
ronment (Section 3).
The large °ux in the source can make it di±cult to know
which team member is familiar with which part of the code.
For a developer, lack of this knowledge can complicate many
activities. For instance, the developer may not know who
to ask when questions arise about particular code. For a
team lead, lack of this knowledge can make it di±cult to
know who can bring a new team member up-to-speed in a
particular part of the code.
Existing approaches to determining who knows which code
have sought to determine who has expertise based on au-
thorship of changes to the code alone (e.g., [11]). These
approaches ignore knowledge that is gained by a developer
interacting with the code for such purposes as calling the
code or trying to understand how the code functions. In this
paper, we introduce the degree-of-knowledge (DOK) model
that takes a broader perspective on who knows what code by
considering both authorship and a developer's interactions
with the code. A DOK value for a source code element
is a real value speci¯c to a developer; di®erent developers
may have di®erent DOK values for the same source code
elements. We compute the DOK values for a developer au-
tomatically by combining authorship data from the source
revision system and interaction data from monitoring the
developer's activity in the development environment (Sec-
tion 4).
To determine whether both authorship and interaction
have an e®ect on knowledge, we gathered data from two
professional software development teams. We report on this
data to support two claims. First, the code that developers
work on changes rapidly. Second, code that developers cre-
ate and edit overlaps, but is not the same as, the code with
which developers interact.
Using this data, we conducted experiments with the mem-
bers of two development teams to determine the relative ef-
fect of authorship and interaction towards modelling knowl-
edge (Section 5). We found that whether or not the devel-
oper was the ¯rst author of a code element had the most
e®ect on the element's DOK value. However, we also found
that all aspects of authorship and interaction improve the
quality of the model and help to explain a developer's knowl-
edge of an element.
The availability of DOK models for developers in a team
opens up several possibilities to improve a developer's pro-ductivity and quality of work. We consider three possibil-
ities in this paper through exploratory case studies (Sec-
tion 6). First, we investigate whether DOK values can sup-
port ¯nding who is an expert in particular parts of a code
base. We found that our approach performed better than
existing approaches for this problem that are based on au-
thorship alone. Second, we investigate whether DOK values
can help familiarize (onboard) a new team member onto a
particular part of the development project. From this study,
we learned about kinds of source code for which our current
de¯nition of DOK does not adequately re°ect a developer's
knowledge. Finally, we hypothesized and con¯rmed that we
can accurately identify bug reports that a developer should
likely be aware of. We achieve this identi¯cation by correlat-
ing the developer's DOK values with a bug report's source
code changes, even when those changes were made by other
team members.
This paper makes three contributions:
²it introduces the degree-of-knowledge model that rep-
resents a developer's familiarity with each code ele-
ment;
²it reports on data about professional developers' au-
thorship and interaction with the code, providing em-
pirical evidence about the rate of information °owing
into a developer's environment and the need to con-
sider both authorship and interaction to more accu-
rately re°ect the code elements with which a developer
is familiar; and
²it reports on the use of DOK values in three di®erent
scenarios in professional environments, reporting on
the bene¯ts and limitations of the model and demon-
strating a measurable improvement for one scenario,
¯nding experts, compared to previous approaches.
2. RELATED WORK
Previous automated approaches to determining the famil-
iarity (expertise) of developers with a codebase rely solely
on change information. For instance, the Expertise Rec-
ommender [9] and Expertise Browser [11] each use a form
of the \Line 10 Rule", which is a heuristic that the person
committing changes to a ¯le1has expertise in that ¯le. The
Expertise Recommender uses this heuristic to present the
developer with the most recent expertise for the source ¯le;
the Expertise Browser gathers and ranks developers based
on changes over time. The Emergent Expertise Locator re-
¯nes the approach of the Expertise Browser by considering
the relationship between ¯les that were changed together
when determining expertise [10]. Girba and colleagues con-
sider ¯ner-grained information, equating expertise with the
number of lines of code each developer changes [4]. Hat-
tori and colleagues consider changes that have not yet been
committed [5]. None of these previous approaches consider
the ebb and °ow of a developer's expertise in a particular
part of the system. The Expertise Recommender consid-
ers expertise as a binary function, only one developer at a
time has expertise in a ¯le depending on who last changed
it. The Expertise Browser and Emergent Expertise Locator
1We use the term ¯le but many of these techniques also ap-
ply at a ¯ner-level of granularity, such as methods or func-
tions.represent expertise as a monotonically increasing function;
a developer who completely replaces the implementation of
an existing method has no impact on the expertise of the
developer who originally created the method. Our approach
models the ebb and °ow of multiple developers changing the
same ¯le; a developer's degree-of-knowledge in the ¯le rises
when the developer commits changes to the source reposi-
tory and diminishes when other developers make changes.
The approach we consider in this paper also di®ers from
previous expertise identi¯cation approaches by considering
not just the code a developer authors and changes, but also
code that the developer consults during their work. Schuler
and Zimmermann also noted the need to move beyond au-
thorship for determining expertise, suggesting an approach
that analyzed the changed code for what code was called
(but not changed) [13]. In this way, they were able to cre-
ate expertise pro¯les that included data about what APIs a
developer may be expert in through their use of those APIs.
In this paper, we go a step further, considering how a
developer interacts with the code in a development environ-
ment as they produce changes to the code. We build on ear-
lier work from our research group that introduced degree-of-
interest (DOI) values to represent which program elements a
developer has interacted with signi¯cantly [8]. The more fre-
quently and recently a developer has interacted with a par-
ticular program element, the higher the DOI value; as a de-
veloper moves to work on other program elements, the DOI
value of the initial element decays. Our initial applications
of this concept computed DOIs across all of a developer's
workday [7]. Subsequent work scopes the DOI computation
per task [8]. In this paper, we return to the computation of
DOI across all of a developer's work to capture a developer's
familiarity in the source across tasks.
Others have considered the use of interaction data for sug-
gesting where to navigate next in the code [2], for tracking
the in°uence of copied and pasted code [12] and for under-
standing the di®erences between novice and expert program-
mers [14]. None of these previous e®orts have considered the
use of interaction data for determining expertise in or famil-
iarity with source code.
In a previous study, we considered whether interaction in-
formation alone could indicate for which code a developer
had knowledge [3]. This study involved nineteen industrial
Java programmers. Through this study, we found that DOI
values computed from the interaction information can indi-
cate knowledge about a program's source. This study also
found that other factors, such as authorship of code, should
be used to augment DOI when attempting to gauge a devel-
oper's knowledge of the code. This paper builds on this pre-
vious work, investigating how a combination of interaction
and authorship information indicates a developer's knowl-
edge of code.
3. AUTHORSHIP AND INTERACTION
Existing approaches to representing code familiarity are
based solely on authorship (e.g., [11]). Our previous study
found that a developer's interaction with the code can indi-
cate the developer's knowledge about source code [3]. These
two results suggest a model of code familiarity should be
based on both of these factors. However, if there is a strong
degree of overlap between the code elements authored and
interacted with by a developer, it may be possible to base a
model on only one kind of information, as is currently thet
T1 T2T3T4authorshipinterac/g415on 
3 months7 work days 5 work days(a) An Abstract Timeline
T1 T2 T3 T4
Site1 3/11/2008 22/1/2009 2/2/2009 7/2/2009
Site224/11/2008 12/2/2009 23/2/2009 28/2/2009
(b) Speci¯c Points in Time Used at Each Site
Figure 1: Data Collection Time Periods
case with expertise recommenders. To investigate the role
of both of these factors, we gathered data from two profes-
sional development sites, ¯nding that each presents a unique
and valuable perspective on a developer's code knowledge.
Site1involved seven professional developers (D1 through
D7) building a Java client/server system, using IBM's Ra-
tional Team Concert2system as the source repository. The
professional experience of these developers ranged from one
to twenty-two years, with a mean experience of 11.6 years
(standard deviation of 5.9 years). These developers each
worked on multiple streams (branches) of the code; we chose
to focus our data collection on a developer's major stream.
One developer (D5) could not identify a major stream of
the four on which he worked; as this work pattern makes
authorship di±cult to determine, we have chosen to exclude
his data from the presentation given in this section but have
included his results in the experiment (Section 5) and case
studies (Section 6).
Site2involved two professional developers, who build open
source frameworks for Eclipse as part (but not all) of their
work and who use CVS3as the source repository system.
One developer had three years of professional experience,
the other had ¯ve years.
Figure 1(a) provides an overview of the di®erent periods
of data collection. Authorship information was gathered for
a three month period (T 1to T 3). The interaction data used
to compute DOK values was gathered over seven working
days (T 2to T 3). The data reported on in this section is
from data collected from T 1to T 3. The interaction data
from T 3to T 4was used to update the DOK values as case
studies were conducted during this period. Figure 1(b) maps
abstract time points to particular dates used for each site.
We report in detail on the data from Site 1, providing only
an overview of the data from Site 2due to space limitations.
Unless otherwise indicated, we report the average (mean) of
values with standard deviations (represented as §).
3.1 Authorship Data
We distinguish between three di®erent kinds of authorship
events with respect to a developer D:
2jazz.net , veri¯ed 01/02/10
3www.nongnu.org/cvs , veri¯ed 01/02/10²¯rst authorship, representing whether Dcreated the
¯rst version of the element,
²number of deliveries, representing subsequent changes
after ¯rst authorship made to the element by D,
²acceptances, representing changes to the element not
completed by D.
We found that the authorship of code loaded into a devel-
oper's environment at Site 1changed frequently. At this site,
a ¯rst authorship, delivery or accept event to an element oc-
curred on average every 54 seconds. The developers had
819 (§576) ¯rst authorships, produced 962 ( §755) delivery
events and accepted 153,240 ( §46;572) changes to an ele-
ment over three months. The standard deviations for all of
these values are high, which is not surprising given the di®er-
ent roles of team members (see Section 7). These aggregate
statistics count multiple events happening to the same ele-
ments. Considering unique elements, on average, the devel-
opers ¯rst authored 660 elements4, delivered to 606 unique
elements and accepted changes on 67,437 unique elements.
Thus, each day, a developer authored ten new elements, de-
livered changes to nine elements, and accepted changes to
1068 elements on average over the period T1 to T3.
To provide more insight into this data, we picked a ran-
dom developer and ten random source code elements that
had at least two authorship related events. To give a sense
of the ebb and °ow of the authorship, we estimated weight-
ings for these events, assigning a ¯rst authorship event a
value of 1.0, a delivery event a value of 0.5 (since a delivery
likely changes just part of the element) and an accept event
a negative value of 0.1 (since an accept event corresponds
to someone else changing the element). Figure 2 plots the
resulting values for each of the ten source code elements over
time; each element is represented by a separate line. Only
a few of these elements were the target of several events
over the three months of data we collected; these elements
are indicated by the longer lines in the ¯gure. All elements
except one (on the far right) have an accept event after a
¯rst authorship or delivery, meaning that someone else on
the team has delivered a change to the element; the lines
for these elements have a declining slope. Over the three
months and the six developers, there is a ratio of 86 to 1 for
accept events versus all ¯rst authorship and delivery events.
This large ratio is indicative of the high rate of change occur-
ring to elements in a developer's environment, caused both
by the team members themselves, and by other developers
making changes to the team's codebase.
3.2 Interaction Data
We found at Site 1that developers interacted with many
di®erent elements over a week of work, some of them quite
frequently.
The developers at this site had an average of 8658 ( §3700)
interactions over the seven working days from T 2to T 3,
interacting with 1033 ( §468) distinct elements. As with the
authorship information, the di®erence between individuals is
quite substantial as it depends on the individual's role on the
team and their individual work patterns. Analyzing the data
for the developers separately over the ¯ve day period from
T3to T 4, the number of elements each developer interacted
4The di®erence between the number of ¯rst authorship
events and elements ¯rst authored is caused by developers
merging streams.-250-150-5050150250350Degree-of-Interest (DOI) 
Monday 
02/02/2009 Tuesday 
03/02/2009 Wednesday 
04/02/2009 Thursday 
05/02/2009 Friday 
06/02/2009 0(a) D1
-250-150-5050150250350
Monday 
02/02/2009 Tuesday 
03/02/2009 Wednesday 
04/02/2009 Thursday 
05/02/2009 Friday 
06/02/2009 0 (b) D3
Figure 3: Positive DOI Elements
-0.5 0.0 0.51.0 1.52.0 2.5 3.0 
December 2008 January 2009 February 2009 Degree-of-Authorship (DOA) 
Figure 2: Authorship Events for Ten Elements
with over the prior seven days of interaction is relatively
stable at 8258 ( §1273).
One way to indicate a developer's ongoing interest in a
particular code element is to consider a degree-of-interest
(DOI) real value for the element computed from the inter-
action information. We provide an overview of DOI in the
next section (Section 4.2) and DOI is reported in earlier
work [8, 6]. A positive DOI value suggests that a developer
has been recently and frequently interacting with the ele-
ment; a negative DOI value indicates a developer has been
interacting with other elements substantially since the de-
veloper interacted with this element.
At Site 1, on average, each developer had 45 ( §6) elements
with a positive degree-of-interest per day. We can see this
stability in graphs we produced for two developers. Fig-
ures 3(a) and 3(b) show, for the period of ¯ve working days
(T3to T 4), elements with a positive DOI value on at least
one of the ¯ve days for each of the two developers.5These
graphs show the di®erences in work patterns across the el-
ements for di®erent developers. Some developers, such as
5The DOI values shown in these graphs were based on the
prior seven days of interaction for each day indicated.D1 (Figure 3(a)), continuously interact with a group of ele-
ments, which results in many lines above zero. Other devel-
opers, such as D3 (Figure 3(b)) interact with more elements
less frequently, resulting in more lines below zero due to the
decay of interest in elements. Most of the six developers
also had at least one code element with which he or she was
interacting with a lot more than with the rest of the code.
3.3 Authorship and Interaction
We have argued that authorship and interaction can each
contribute valuable information to representing a developer's
degree-of-knowledge for a source code element. To investi-
gate whether there is a di®erence in the elements a developer
authors versus interacts with, we considered, for each of the
¯ve days between T 3and T 4, the intersection of all code
elements that had a positive DOI with all elements that had
at least one ¯rst authorship or delivery event. On average,
out of 45 elements with a positive DOI, only 12 (27%) also
had at least one ¯rst authorship or delivery event in the pre-
vious three months. Thus, interaction information provides
a di®erent perspective on the code a developer is working
with than solely authorship information.
Our analysis of the data also showed that the number of
elements worked with varies with the day of the week. The
plot in Figure 4 shows the number of elements with at least
one interaction event and the number of elements with a de-
livery or ¯rst authorship event, for each of ¯ve consecutive
days (T 3to T 4). While the number of elements develop-
ers interacted with holds relatively steady, the number of
elements delivered or ¯rst authored increases prominently
on Friday. The data suggests that the developers created
changes throughout the week but delivered most of them on
Friday. This trend suggests that interaction may be a useful
predictor of recent source code familiarity whereas author-
ship helps capture familiarity over a longer period of time.
3.4 Site 2Data
The data collected at Site 2shows a lower rate of change
in authorship information than Site 1, but even at the lower
rate, a substantial amount of the code in a developer's en-
vironment is changing each day. The data from Site 2shows
even less overlap between code interacted with than au-
thored.authorship 
0100 200 300 400 900 1000 1100 number of elements 
Monday 
02/02/2009 Tuesday 
03/02/2009 Wednesday 
04/02/2009 Thursday 
05/02/2009 Friday 
06/02/2009 interac/g415on Figure 4: Authorship and Interaction over Five
Days
On average, developers at Site 2had a ¯rst authorship,
delivery or accept event every 700 seconds (compared to 54
seconds at Site 1). Considering unique elements and com-
paring to Site 1, the developers authored 2.7 times as many
elements (1762 §1835), delivered 2.8 times as many ele-
ments (1697 §1746), and accepted changes to only 1/11 as
many elements (5977 §3454). The developers at Site 2av-
eraged 6195 interactions over the seven working days, inter-
acting with 566 distinct elements and ending, on average,
with 60 elements each day with a positive DOI. The number
of elements with a positive DOI that also had at least one
¯rst authorship or delivery event is four; an overlap of 7%
compared to the 26% overlap at the ¯rst site.
There are several potential reasons for these di®erences.
First, whereas the source repository system in use at Site 1
supported atomic changesets with explicit accept events oc-
curring within the development environment, at Site 2, the
source revision system lacked both of these features. In-
stead, we inferred delivery events based on revision infor-
mation to source code elements; if a developer performed
several commits to the revision system as part of one logical
change, we record this as multiple delivery events. Second,
the lack of an explicit accept event that could be logged
meant that we had to infer at the end of each day that all
outstanding changes were accepted, potentially increasing
the accept events. Finally, the code at Site 2is smaller and
is being worked on by a smaller team, potentially causing a
di®erent event pro¯le.
4. DEGREE-OF-KNOWLEDGE MODEL
Our degree-of-knowledge model for a developer assigns
a real value to each source code element|class, method,
¯eld|for each developer. Our de¯nition of DOK includes
one component indicating a developer's longer-term knowl-
edge of a source code element, represented by a degree-of-
authorship value, and a second component indicating a de-
veloper's shorter-term knowledge, represented by a degree-
of-interest value.
4.1 Degree-of-Authorship
From our study of nineteen industrial developers [3], we
determined that a developer's knowledge in a source code el-
ement depends on whether the developer has authored and
contributed code to the element and how many changes not
authored by the developer have subsequently occurred. We
thus consider the degree-of-authorship (DOA) of a developerin an element to be determined by three factors: ¯rst author-
ship ( FA), the number of deliveries ( DL) and the number
of acceptances ( AC).
4.2 Degree-of-Interest
The degree-of-interest (DOI) represents the amount of
interaction|selections and edits|a developer has had with
a source code element [8]. A selection occurs when a devel-
oper touches a code element; for instance, a selection occurs
when the developer opens a class to edit the class. An edit
occurs when a keystroke is detected in an editor window.
The DOI of an element rises with each interaction the de-
veloper has with the element and decays as the programmer
interacts with other elements. Di®erent kinds of interactions
contribute di®erently to the DOI of an element; for instance,
a selection of an element contributes less to DOI than an
edit of an element. We use weightings for interactions as
de¯ned in the Eclipse Mylyn project, which is successfully
supporting hundreds of thousands of Java programmers in
their daily work. The Eclipse Mylyn project uses DOI as
de¯ned elsewhere [8, 6]. In contrast to Eclipse Mylyn, our
use of DOI considers all interaction a developer has with
the environment and does not consider any task boundaries
indicated by the developer as part of their work.
4.3 Degree-of-Knowledge
We combine the DOA and DOI of a source code element
for a developer to provide an indicator of the developer's fa-
miliarity in that element. The degree-of-knowledge we com-
pute linearly combines the factors contributing to DOA and
the DOI:
DOK =®FA¤FA+®DL¤DL+®AC¤AC+¯DOI¤DOI
5. DETERMINING DOK WEIGHTINGS
Completing our de¯nition of a degree-of-knowledge value
for a source code element requires determining appropri-
ate weightings for the factors contributing to the degree-
of-authorship and for the degree-of-interest. As there is
no speci¯c theory we can use to choose the weightings, we
conducted an experiment to determine appropriate values
empirically. In essence, the experiment involves gathering
data about authorship from the revision history of a project,
about interest by monitoring developers' interactions with
the code as they work on the project and about knowledge
by asking developers to rate their level of knowledge of par-
ticular code elements. Using the developer ratings, we then
apply multiple linear regression to determine appropriate
weightings for the various factors.
We report in this section on an initial determination of
weighting values based on the data collected from Site 1. We
then test these weightings at Site 2. Our intent is to ¯nd
weightings that serve as a basis to support exploratory in-
vestigations of the degree-of-knowledge model. Determining
weightings that might apply across a broader range of devel-
opment situations would require gathering data from many
more projects, which was not warranted at this early stage
of investigation.
5.1 Method
At time T 3in Figure 1, we chose, for each developer, forty
random code elements that the developer had either selected
or edited at least once in the last seven days ( DOI6= 0), orwhich the developer had ¯rst authored ( FA > 0) or deliv-
ered changes to ( DL > 0) in the last three months. We chose
forty as a compromise between gaining data about enough
elements and not encroaching too much on the developer's
working time. Each developer was then asked to assess how
well he or she knew each of those elements on a scale from
one to ¯ve. To help the developers with the rating scale, we
explained that a ¯ve meant that the developer could repro-
duce the code without looking at it, a three meant that the
developer would need to perform some investigations before
reproducing the code, and a one meant that the developer
had no knowledge of the code. Although this process meant
that sometimes we asked the developer about ¯ner-grained
code (a ¯eld) and sometimes coarser-grained code (a class),
subsequent analysis of the data did not show any sensitivity
to granularity.
Through this process, we collected 246 ratings for all seven
developers. This value is less than the 280 possible ratings
because some of the elements we randomly picked were not
Java elements (the authorship and interaction data also in-
cluded XML, Javascript and other types of code) and the de-
velopers stated that they would have di±culty rating them;
we therefore ignored these elements.
For this experiment, we consider results to be statistically
signi¯cant with p <0.05.
5.2 Analysis and Results
For our ¯rst experimental setting, we applied multiple lin-
ear regression to the data collected from the source revision
logs and the interaction logs collected as the developers from
Site2worked. Multiple linear regression analysis tries to ¯nd
a linear equation that best predicts the ratings provided by
developers for the code elements using the four variables:
FA(¯rst authorships), DL(deliveries), AC(accepts) and
DOI (degree-of-interest). Multiple linear regression is suit-
able for our data, even though the user ratings are ordinal,
because we are attempting to ¯nd an approximation, not a
certain class, for the user ratings.
The values of some of the variables, especially DOI and
ACcan be substantially higher than the values of the other
variables. To account for these di®erent scales that could
potentially make the weighting factors di±cult to ascertain,
we applied the analysis both with and without taking the
natural logarithms of the values. With the developer rating
(on a scale of one to ¯ve) as the dependent variable, the
best ¯t of the data was achieved with the values presented
in Table 1, when the natural logarithm of the ACandDOI
values was used. The resulting DOK equation is as follows.
DOK = 3 :293 + 1 :098¤FA+ 0:164¤DL
¡0:321¤ln(1 +AC) + 0:19¤ln(1 +DOI )
Negative values of DOI indicate usage that is not recent. In
this analysis, we considered any negative DOI value to be
zero so as to not unduly penalize DOK. If we had allowed
negative DOI values, then elements that the developer had
never interacted with may have a higher DOK value than
elements interacted with long ago.
The FA,DLandACvariables are signi¯cant in this
model and thus help to explain the user ratings. The DOI
variable is very close to being signi¯cant. An analysis shows
that the DOI is not correlated to any of the other variables,
suggesting that DOI still plays a predictive role, despite notTable 1: Coe±cients for Linear Regression
Weighting Std. Error p-value
Intercept 3.293 0.133 <0.001
FA 1.098 0.179 <0.001
DL 0.164 0.053 0.002
ln(1 +AC) -0.321 0.105 0.002
ln(1 +DOI ) 0.190 0.100 0.059
reaching signi¯cance. We hypothesize that the lack of sig-
ni¯cance is from the lack of elements with a positive DOI
in the set of randomly chosen elements. Only 7% of all data
points have a positive DOI whereas 28% have a positive
FA, 50% have a positive DLand 57% have a positive AC
component.
The F-ratio, a test statistic used for determining the pre-
dictive capability of the model as a whole, is 19.6 with
p<0.000001. This states that the model based on our four
predictor variables has a statistically signi¯cant ability to
predict the user rating. The overall model has an estimated
\goodness of ¯t", R Square, of 0.25 (adjusted R Square is
0.23). R Square represents the fraction of the variation in
our user rating that is accounted for by our variables. The
correlation coe±cient R that represents a measure of the
overall ¯t between our predictor variables and the user rat-
ing is 0.50. The standard error of the estimate is 1.17. The
0.25 R Square value shows that our model does not predict
the user rating completely. However, the p-value of the over-
all model as well as the p-values for the variables indicate
that there is a statistically signi¯cant linear relationship be-
tween our model and the user ratings and that each of the
four variables contributes to the overall explanation of the
user rating.
5.3 External Validity of the Model
To determine if our weightings have any applicability in
a di®erent environment, we conducted a similar experiment
with the two professional Java developers at Site 2. As we
did at Site 1, we again chose forty random code elements
for each developer with the same characteristics as at Site 1
and we asked each developer to rank the presented elements
from one to ¯ve.
We then computed the DOK values for each of the ele-
ments using the weightings determined through the earlier
experiment with the developers at Site 1. To see whether
our previously determined model can describe the relation-
ship between the four variables and the developer ratings
atSite 2, we applied the Spearman rank correlation coef-
¯cient statistic. The Spearman rank correlation is a non-
parametric statistic that is designed to measure correlations
in ordinal data. For the 80 code elements we studied from
the two developers there is a statistically signi¯cant corre-
lation with rs= 0:3847 (p=0.0004). This result suggests
that our model can predict DOK values with reasonable ac-
curacy, even when the environments from which the data
was collected have di®erent pro¯les (Section 3.4). We hy-
pothesize that the correlation coe±cient is low because the
statistic is especially sensitive to individual di®erences when
the sample size is low.6. CASE STUDIES
To determine if degree-of-knowledge (DOK) values can
provide value to software developers, we performed several
exploratory case studies. Two case studies were conducted
with the seven developers at Site 1. A third case study was
performed with three developers at Site 2; these developers
di®ered from those described in Section 3 as data was col-
lected at a di®erent time to support the study. The three
developers were working on a closed-source development ef-
fort; one developer was working part-time. The participat-
ing developers had an average of 2.5 years of professional
experience at the time of the case study.
The ¯rst case study considers the problem of ¯nding ex-
perts who are knowledgeable about particular parts of the
code. The second case study considers a mentoring situation
where an experienced developer might use his DOK values
to help a new team member become familiar (onboard) into
that part of the code base. The third case study considers
whether a developer's DOK values can be used to identify
which changes to the code might be of interest amongst the
many changes occurring during development.
6.1 Finding Experts
The problem of ¯nding experts is to try to identify which
team member knows the most about each part of the code-
base. Our degree-of-knowledge model applies directly to this
problem.
Method.
At Site 1, the code is partitioned into projects, where a
project is a logical group of Java packages. For this case
study, we chose two projects with which most members of
the team had interacted. One project comprised 21 Java
packages; the other comprised 88 packages. For each class
in these packages, and for each of the seven developers par-
ticipating in our study, we calculated the DOK value for
each class-developer pair and then computed DOK values
for each package-developer pair by summing the developer's
DOK values for each class in the package. Using these val-
ues, we produced a diagram, which we call a knowledge map,
that showed for each package in a project, the developer with
the highest DOK for that package. Figure 5 presents a part
of the knowledge map for one project.6In this ¯gure, each
developer is assigned a colour and each package is coloured
(or labelled) according to the developer with the highest
DOK values for that package. For one project, 17 of the 21
packages (80%) were labelled and for the second, 61 out of
88 (69%) were labelled. Thus, 78 packages in total were la-
belled. For the remaining 31 packages, the DOK values for
all developers were not positive, meaning primary expertise
might lie outside the team.
We then conducted individual sessions with each of the
seven team members. In each session, we ¯rst showed the
developer a list of the packages without any DOK values
indicated and asked the developer to write down the name
of the team member whom he thought knows the package
the best. When requested, we showed the developers a list of
the classes within a package. After gathering this data, we
showed the developer the knowledge map and asked if the
map re°ected his view of which developer knows which part
of the code. This approach runs the risk of developers over-
6Please note this ¯gure is best viewed in colour.in°ating their expertise in a package to avoid not appearing
as an expert in anything. We believe this over-in°ation did
not happen because the two projects represent only a small
fraction of the entire system so a developer who did not
indicate expertise in the packages that were part of the case
study might still be expert in some other part of the code.
Results.
We gathered data from six developers (D1-D6); one devel-
oper (D7) did not interact with any of the code in the two
projects and thus was not able to provide meaningful data.
For the 78 of the 109 packages labelled with a single de-
veloper, we gathered 468 (6 developers times 78 packages)
assignments from the developers participating in the study.
In 301 of these cases (64%), the developers in the study as-
signed one developer as being the one that \knows the most"
or \owns" the package. In 166 of these 301 cases (55%), the
result we computed based on DOK values was consistent
with the assignments by the developers.
The 55% accuracy value is a lower bound of our approach's
performance given that the developer assignments were some-
times guesses; after seeing the knowledge map the developers
realized their assignments were likely wrong. All six devel-
opers stated that the knowledge map was reasonable, using
phrases like it is \close" (D4) and it \re°ects [reality] cor-
rectly" (D2).
For the 31 out of the 109 packages for which we did not
¯nd anyone using the DOK values, the six developers as-
signed someone to a package in 104 cases. In 48 of these
cases (46%), the packages had not been touched for a num-
ber of months and were created six months ago. Given that
our DOK values were based on three-months of data, we
were missing the initial authorship data. Developers stated
that in \blank cases" (D4) where our DOK did not deter-
mine anyone, we should adapt the DOK to go back further
in time.
Comparison to Expertise Recommenders.
For this task, it is possible to compare to other approaches,
since earlier work in expertise recommenders has considered
the problem of ¯nding experts. As described in Section 2,
these approaches are based solely on authorship information.
To approximate the results of these earlier approaches, we
computed experts for each package by summing up all ¯rst
authorship and delivery events from the last three months
for a developer for each class in the package. Three months
was the most history available for these elements due to a
major porting of the code at that time. The developer with
the most \experience atoms" [11]|the most events|for a
package is the expert. We applied this expertise approach
to the two projects. In 21 out of the total 109 packages,
the expertise approach labelled a package with a di®erent
expert developer than our DOK-based approach. For these
21 packages, we had 69 assignments from the six develop-
ers. In 34 of these 69 cases (49%), our DOK-based approach
agreed with the developer assignments, whereas the exper-
tise approach agreed in only 17 (24%) of these cases. Thus,
the DOK approach improves the results for the packages
that were labelled di®erently by the two approaches by more
than 100% and the overall result by 11%. This comparison
shows that DOK values can improve on existing approaches
to ¯nding experts.Figure 5: Part of a Knowledge Map
6.2 Onboarding
Becoming productive when joining a new development
project requires learning the basic structure of the code-
base. The process of becoming pro¯cient with a codebase is
known as onboarding [1]. In this case study, we investigated
whether DOK values computed from developers with experi-
ence in a part of a codebase could be used to indicate which
code elements a newcomer should focus on when trying to
learn that part of the code base.
Method.
For this study, we randomly chose three developers (D1,
D3, D5). We asked each developer to describe which code el-
ements from the areas in which he was working would likely
be the most useful for a newcomer trying to come up-to-
speed on the code. We then generated, for each developer,
the twenty elements with the highest DOK and asked the de-
veloper to comment on whether these twenty elements would
likely be helpful for a newcomer.
Results.
Only 2 of the 60 (3%) elements generated across all three
developers were considered by the developers to likely be
helpful for a newcomer. The other 58 (97%) elements were
described by the developers as not being essential for under-
standing the code. The elements generated using the DOK
values were, \only implementations"(D1) or\secondary con-
sumers" (D3). The developers described that a newcomer
only needs to understand basic patterns (D1, D5) and that
while the elements generated using DOK could serve as ex-
amples, it would be necessary to traverse up the inheritance
hierarchy to locate the elements the newcomer should study
(D1). These comments are consistent with the descriptionof the developers that they often recommend newcomers to
look at API elements. The DOK values for the API el-
ements were either very low or zero as they were neither
changing nor were they referred to frequently by the de-
velopers who authored them. Perhaps the developers had
internalized these APIs and did not need to refer to them.
For onboarding, then, the elements with high DOK val-
ues were not considered helpful. However, the developers
comments suggest that the elements with high DOK might
be used as a starting point to ¯nd useful elements for on-
boarding by following call or type hierarchies. We leave the
investigation of this potential use of DOK values to future
research.
6.3 Identifying Changes of Interest
In many development projects, keeping up with how the
work of teammates might e®ect one's work typically re-
quires monitoring the progress of changes. In many projects,
changes to the source code are tied to bug reports either by
listing the bug report in the change or by attaching the
change to a bug report (e.g., by attaching the change itself
or meta-information such as a task context [8]). By mon-
itoring bug reports instead of inspecting individual source
code changes, a developer can be provided more rationale
about a change.
Many bug reports for the project, like source code ele-
ments, also typically change daily. A developer who wishes
to monitor changes of interest must typically perform searches
over the bug repository. Determining which changes the de-
veloper should consider and ensuring searches are returning
interesting changes can be di±cult.
In this case study, we investigated whether a developer's
DOK values can be used to select changes of interest to the
developer because of overlap between the source code change
and the developer's DOK model.
Method.
For this study, we computed a DOK model for each of
three developers from Site 2.
On a particular day, we determined all bugs that had
changed in the previous seven days; we refer to this set as
BC. As these three developers work on multiple projects
and we are analyzing changes of interest for one project, we
used seven days to capture a su±cient amount of change to
the project we were targeting. The same seven days were
used for developers S1 and S2; a di®erent seven days were
used for developer S3. These dates di®ered to accommodate
developers' schedules.
We then determined the subset of bugs that had change in-
formation attached to the bug; we refer to this set as BCI.7
For each bug report with change information in BCI, we
computed the aggregate DOK value for each element in the
change information based on the developer's DOK model.
We formed the set of bugs with an aggregate DOK value
that was positive; we refer to this set as BPOT. We then
removed the bugs in BPOT when the developer was already
mentioned on the bug as an assignee, reporter or had com-
mented on the bug. The resulting set of bugs are those we
7For this project, the change information were task contexts
collected automatically as a developer worked and thus rep-
resented both the elements changed in the revision system
and elements considered by the programmer in making the
change.Table 2: Size of Bug Recommendation Sets
Developer BCBCI BPOT BR
S1 123 26 20 3
S2 123 26 7 2
S3 76 28 5 1
report as bugs of interest to the developer; we refer to this
set as BR. For each bug in BR, we asked the developer
whether they had read the bug or whether they would have
wanted to be aware of the bug.
Results.
Table 2 summarizes the number of bugs in each set for
each developer. Developer S1 was asked about the relevance
of three bugs. He noted that he had read each of these bugs
to make sure no further action was required on his part.
Developer S2 was asked about two bugs. She noted that
for one of these bugs, she was asked in person about the
contents of the bug although she had not read it previously.
The developer was impressed our approach had picked it out
of the many bugs that were undergoing change. Developer
S3 was recommended one bug. He had read the bug but did
not comment explicitly about whether the bug was relevant
to his work.
Overall, our approach provided relevant information to
developers in four out of six cases by recommending non-
obvious bugs based on the developers' DOK values. While
our case study used a very coarse-grained metric to deter-
mine relevance of bugs using DOK values, we were able to
easily recommend more relevant bugs than noise.
7. DISCUSSION
The degree-of-knowledge (DOK) model is in°uenced by
both the software development process and the software sys-
tem being developed. We detail a number of the factors in-
°uencing DOK and how they pose threats to the validity of
the experiments and studies we conducted.
7.1 Amount of Data
Our studies were based on three months of authorship
data and seven working days of interaction data. We chose
this duration for authorship data based on interviews in our
earlier study [3]. In these interviews, developers had sug-
gested three months as a lower bound for the period of time
in which one still has knowledge about code after authoring
it. Also based on our previous study [3], we chose seven
working days of interaction data. Seven working days was
the average number of working days that showed a signi¯-
cant result for the correlation between a developer's knowl-
edge and his interaction.
7.2 Multiple Stream Development
The seven developers we studied at the ¯rst site share
code in streams, which are similar to branches in a source
revision system. The developers deliver their changes to
one or more streams and accept changes from streams into
their workspace. Normally, the developers we studied work
only on a small number of streams. However, during our
data collection and study period, some of the developers
were working on many streams: \it's not a normal situation,right now [it is] very branched out, [and] I almost spend
more time merging than working on it"(D5). When streams
representing di®erent versions of the same code are merged,
additional authorship events are recorded that could skew
the results of our experiment and studies. We tried to mini-
mize the in°uence of these extra events by focusing on only
one major stream for each developer.
7.3 Project Phase
Developers interact di®erently with a codebase depend-
ing on the phase of the project on which they are working.
In the week in which we collected interaction data at Site 1
to determine the DOK weightings, the team was in a test-
ing phase for an upcoming milestone release. Some of the
developers reported that they were only performing minor
adjustments to the code but not really making any bigger
changes to ensure the code did not break. Some develop-
ers stated that a couple of months before they were working
on new features, during which a substantial amount of new
code was created and the focus of individual developers in a
part of the codebase was higher.
The number and size of changesets and the tasks of the de-
velopers (i.e., testing versus feature development) in°uences
the authorship and interaction data. By taking into account
three months of authorship data, seven days of interaction
data and also con¯rming the results of the DOK weighting
experiment at a second site, we have tried to minimize the
impact of project phases on our results. Further longitudi-
nal studies are needed to better understand the impact of
project phases on indicators such as DOK.
7.4 Individual Factors
The ¯rst team of seven developers we studied have a strong
model of code ownership, with code split amongst team
members and certain individuals responsible for certain pack-
ages. Other teams we have spoken with have a model of mu-
tual ownership with the team members often working on the
same code. The style of code ownership within a team in-
°uences the data input to determine DOK values. We have
tried to mitigate the risk of these di®erent styles by consider-
ing whether the weightings determined for one team applied
to another team. However, study of more teams is needed
to determine how robust the DOK values are to team and
individual styles.
A developer's activity also has an in°uence on the data.
For instance, one developer in our study was working on
more than four di®erent streams and was expending e®ort
that week merging streams together. When we applied lin-
ear regression on the data points gained from only this de-
veloper, the result was not signi¯cant. For other developers,
the goodness of ¯t of the model is more than twice as good
as the goodness of ¯t for all developers. Thus, while individ-
ual factors, such as the team's style of code ownership and
activities of individuals in°uence results, by having several
developers, each with a di®erent activity, we have tried to
minimize the risk of individual biases.
7.5 API Elements
In the onboarding case study, API elements a®ected the
outcome: developers suggested API elements as important
that DOK values did not capture. The root of the prob-
lem is that API elements, by necessity, do not change often.
In the three month period we considered for the authorshipcomponent of DOK, there was not a su±cient number of
events on the API elements for their DOK to rise based on
authorship. Furthermore, as API elements often become ba-
sic knowledge, developers do not need to interact with them
frequently so the interaction data also does not cause their
DOK values to rise. The developers who participated in the
case study stated that the elements found using the DOK
are often one or two layers below the API elements. A pos-
sible improvement to the DOK could be to infer familiarity
from subclasses up to the API elements that are the super-
types as it is likely a subclass user knows the API elements
to some extent.
8. SUMMARY
On average, six professional Java developers at a site we
studied, accepted changes to 1068 source code elements per
day and interacted with 923 distinct source code elements
over a seven day period. This data con¯rms what is appar-
ent when one watches a professional developer at work; the
amount of information that °ows into and changes in a devel-
oper's development environment is substantial. By studying
professional developers, we also found evidence that the code
a developer authors and the code with which the developer
interacts are not the same. This high degree of °ux and dif-
ferences in code authored (visible to other team members)
from code considered (not visible to other team members)
makes it di±cult for developers to know who on their team
has familiarity with di®erent parts of the codebase.
To help capture and provide access to information about
who knows what about the code, we have presented the
degree-of-knowledge (DOK) model. By incorporating both
authorship and interaction information gathered for a de-
veloper, a DOK value for each source code element and de-
veloper pair can be produced. We have de¯ned an equation
to compute DOK values and set the weightings for the au-
thorship and interaction factors through an experiment with
seven professional Java software developers. We con¯rmed
these weightings through a second experiment at a second
site with two professional Java developers.
To show that the DOK model can provide value, we re-
port on three exploratory case studies. Through these case
studies, we have shown how DOK values can improve upon
existing approaches to computing expertise that are based
solely on authorship. We have also shown how DOK val-
ues might be used to pick out changes of interest in the
environment to the developer. Directions for future work in
improving DOK were also determined through these studies,
such as ways to better capture familiarity in API elements.
The case studies we have conducted provide initial evi-
dence to suggest that further study of the DOK model is
warranted. In particular, the weightings for the factors con-
tributing to the DOK model and the appropriate amounts of
data to use to compute DOK values require experimentation
with more developers in a greater variety of situations.9. ACKNOWLEDGMENTS
This work was supported in part by IBM and in part by
NSERC. We thank all the developers who participated in
the experiments and case studies.
10. REFERENCES
[1]M. Cherubini, G. Venolia, R. DeLine, and A. J. Ko.
Let's go to the whiteboard: how and why software
developers use drawings. In Proc. of CHI'07 , pages
557{566, 2007.
[2]R. DeLine, A. Khella, M. Czerwinski, and
G. Robertson. Towards understanding programs
through wear-based ¯ltering. In Proc. of SoftVis'05 ,
pages 183{192, 2005.
[3]T. Fritz, G. C. Murphy, and E. Hill. Does a
programmer's activity indicate knowledge of code? In
Proc. of ESEC-FSE'07 , pages 341{350, 2007.
[4]T. Girba, A. Kuhn, M. Seeberger, and S. Ducasse.
How developers drive software evolution. In Proc. of
IWPSE'05 , pages 113{122, 2005.
[5]L. Hattori and M. Lanza. Mining the history of
synchronous changes to re¯ne code ownership. Proc. of
MSR'09 , pages 141{150, 2009.
[6]M. Kersten. Focusing knowledge work with task
context . PhD thesis, University of British Columbia,
2007.
[7]M. Kersten and G. C. Murphy. Mylar: a
degree-of-interest model for IDEs. In Proc. of
AOSD'05 , pages 159{168, 2005.
[8]M. Kersten and G. C. Murphy. Using task context to
improve programmer productivity. In Proc. of FSE'06 ,
pages 1{11, 2006.
[9]D. W. McDonald and M. S. Ackerman. Expertise
recommender: a °exible recommendation system and
architecture. In Proc. of CSCW'00 , pages 231{240,
2000.
[10]S. Minto and G. C. Murphy. Recommending emergent
teams. In Proc. of MSR'07 , page 5, 2007.
[11]A. Mockus and J. D. Herbsleb. Expertise browser: a
quantitative approach to identifying expertise. In
Proc. of ICSE'02 , pages 503{512, 2002.
[12]C. Parnin, C. G Äorg, and S. Rugaber. Enriching
revision history with interactions. In Proc. of MSR'06 ,
pages 155{158, 2006.
[13]D. Schuler and T. Zimmermann. Mining usage
expertise from version archives. In Proc. of MSR'08 ,
pages 121{124, 2008.
[14]L. Zou and M. W. Godfrey. Understanding interaction
di®erences between newcomer and expert
programmers. In Proc. of RSSE'08 , pages 26{29, 2008.View publication stats