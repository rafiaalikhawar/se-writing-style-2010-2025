Detecting Software Modularity Violations
Sunny Wong and
Yuanfang Cai
Drexel University
Philadelphia, PA, USA
{sunny, yfcai}@cs.drexel.eduMiryung Kim
The University of Texas at
Austin
Austin, TX, USA
miryung@ece.utexas.eduMichael Dalton
Drexel University
Philadelphia, PA, USA
mcd45@cs.drexel.edu
ABSTRACT
This paper presents Clio, an approach that detects modular-
ity violations , which can cause software defects, modularity
decay, or expensive refactorings. Clio computes the dis-
crepancies between how components should change together
based on the modular structure, and how components ac-
tually change together as revealed in version history. We
evaluated Clio using 15 releases of Hadoop Common and
10 releases of Eclipse JDT . The results show that hundreds
of violations identiﬁed using Clio were indeed recognized as
design problems or refactored by the developers in later ver-
sions. The identiﬁed violations exhibit multiple symptoms
of poor design, some of which are not easily detectable using
existing approaches.
Categories and Subject Descriptors
D.2.7 [ Software Engineering ]: Maintenance and Enhance-
ment— refactoring, restructuring ; D.2.10 [ Software Engi-
neering ]: Design— modularity violation, refactoring
General Terms
design rule theory, refactoring
Keywords
modularity violation detection, refactoring, bad code smells,
design structure matrix
1. INTRODUCTION
The essence of software modularity is to allow for in-
dependent module evolution and independent task assign-
ment [2,17]. In reality, however, two modules that are sup-
posed to be independent may always change together, due to
unwanted side eﬀects caused by quick and dirty implemen-
tation. For example, inexperienced developers may forget
to remove experimental scaﬀolding code that should not be
kept in the ﬁnal product, and an application programming
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for pro ﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speci ﬁc
permission and/or a fee.
ICSE ’11 May 21–28, 2011, Waikiki, Honolulu, HI, USA
Copyright 2011 ACM 978-1-4503-0445-0/11/05 ...$10.00.interface (API) may be accidentally deﬁned using non-API
classes [14]. Such activities cause modularity decay over
time and may require expensive system-wide refactoring.
Though empirical studies have revealed a strong correlation
between software defects and eroding design structure [6,21],
traditional veriﬁcation and validation techniques do not ﬁnd
modularity violations because these violations do not always
inﬂuence the functionality of software systems directly.
This paper presents Clio, an approach that detects and lo-
cates modularity violations .Clio compares how components
should change together based on the modular structure and
how components actually change together as reﬂected in the
revision history. The rationale is that, if two components
consistently change together to accommodate modiﬁcation
requests,1but they belong to two separate modules that
are supposed to evolve independently, we consider this as a
modularity violation .
Clio has three components. The ﬁrst component calcu-
lates structural coupling —how components should change
together, based on Baldwin and Clark’s design rule theory
and design structure matrix (DSM) [2] modeling. The sec-
ond component extracts change coupling —how components
actually change together [10] through mining the project’s
revision history. The third component identiﬁes modularity
violations by comparing the results of structural coupling
based impact scope analysis with the results of change cou-
pling based impact scope analysis.
We applied Clio to the version histories of two large-scale
open source software systems: 15 releases of Hadoop Com-
mon,2and 10 releases of Eclipse JDT.3Our evaluation strat-
egy was to identify violations for each pair of releases. If
a violation was indeed problematic, it is possible that de-
velopers recognized and ﬁxed it in a later release through
a refactoring. We considered a detected violation as be-
ingconﬁrmed if it was indeed addressed or recognized by
developers later. We used two complementary evaluation
methods. First, we compared the detected violations with
refactorings automatically reconstructed using Kim et al.’s
API matching technique [15]. Second, we manually exam-
ined modiﬁcation requests to see whether those violations
were at least recognized by developers. Because it is possi-
ble that problems in recent versions are not yet recognized
by the developers, we also manually examined the corre-
1Consistent with Ying et al. [30], a modiﬁcation request can
be a bug ﬁx or feature enhancement. The set of ﬁles that
resolve a modiﬁcation request is called its solution .
2http://hadoop.apache.org/common/
3http://www.eclipse.org/jdt/sponding code to determine whether the detected violations
reveal symptoms of poor design.
We identiﬁed 231 modularity violations (47%) from 490
modiﬁcation requests of Hadoop, of which 152 (65%) vi-
olations were conﬁrmed. From 3458 modiﬁcation request
of Eclipse JDT, Clio identiﬁed 399 modularity violations
(12%), which shows that the changes in Eclipse better match
its modular structure. Among these violations, 161 (40%)
were conﬁrmed. The results also show that Clio identiﬁes
modularity violations much earlier than manual identiﬁca-
tion by developers so that designers can be alerted to avoid
accumulating modularity decay. Third, the identiﬁed viola-
tions include symptoms of poor design, some of which cannot
be easily detected using existing approaches.
The rest of this paper is organized as follows. Section 2
presents related work and how Clio diﬀers from existing
approaches. Section 3 describes our modularity violation
detection approach and several background concepts. Sec-
tion 4 details our evaluation method and empirical results.
Section 5 discusses the strengths and limitations of Clio and
Section 6 concludes.
2. RELATED WORK
In this section, we compare and contrast Clio with other
related research topics.
Automatic Detection of Code Smells. Fowler [9]
describes the concept of bad smell as a heuristic for identi-
fying redesign and refactoring opportunities. Example bad
smells include code clone and feature envy. Garcia et al. [11]
proposed several architecture-level bad smells. To automate
the identiﬁcation of bad smells, Moha et al. [16] presented
the Decor tool and domain speciﬁc language (DSL) to auto-
mate the construction of desig n defect detection algorithms.
Several other approaches [23–25] automatically identify bad
smells that indicate needs for refactorings. For example,
Tsantalis and Chatzigeorgiou’s technique [24] identiﬁes ex-
tract method refactoring opportunities using static slicing.
Detection of some speciﬁc bad smells, such as code duplica-
tion, has also been extensively researched. Higo et al. [13]
proposed the Aries tool to identify possible clone refactor-
ing candidates using structural metrics (e.g., the number of
assigned variables, the number of referred variables).
Clio’s modularity violation detection approach is diﬀer-
ent in several aspects. First, it is not conﬁned to particular
types of bad smells. Instead, we hypothesize that multiple
types of bad smells are instances of modularity violations
that can be uniformly detected by Clio. For example, when
code clones change frequently together, Clio will detect this
problem because the co-change pattern deviates from the
designed modular structure. Second, by taking version his-
tories as input, Clio detects the most recently and frequently
occuring violations, instead of bad smells detected in a single
version without regard to the program’s evolution context.
Similar to Clio, Ratzinger et al. [18] also detect bad smells
by examining change coupling. Their approach leaves it to
developers to identify violations from the visualization of
change coupling, while Clio locates violations by comparing
change coupling with structural coupling. The detected vio-
lations thus either reﬂect the problem in the original design
or introduced in the subsequent modiﬁcation requests.
Design Structure Matrix Analysis. Some of the
most widely used design structure matrix (DSM) tools in-clude Lattix,4Struture 101,5and NDepend.6These tools
support automatic derivation of DSMs from source code,
modeling the syntactic dependencies between classes or ﬁles.
Diﬀerent from these tools, the DSMs used in Clio are gen-
erated from augmented constraint networks (ACNs) [4, 5],
which separate the interface and implementation of a class
into two design dimensions, and manifest implicit and indi-
rect dependencies [26] that cannot be revealed by a syntac-
tical DSM [4].
Sangal et al. [20] describe how to use Lattix to identify
undesired dependencies. Using Lattix, a user can specify
which classes should not depend on (i.e., syntactically re-
fer to) which other classes. The tool raises an alert if a
predeﬁned constraint is violated. A key diﬀerence between
Clio’s and Lattix’s detection techniques is that Clio ana-
lyzes version histories to detect violations that occur during
software evolution, many of which are not in the form of
syntactical dependencies and thus will not be detected by
Lattix. Another major diﬀerence is that Clio takes recency
and frequency into consideration when identifying modular-
ity violations.
Dependency Structure and Software Defects. The
relation between software dependency structure and defects
has been widely studied (e.g., Selby and Basili [21]). Var-
ious metrics have been proposed (e.g., Chidamber and Ke-
merer [7]) to measure coupling and failure proneness of com-
ponents. The relation between change coupling [10] and
defects has also been recently studied. Cataldo et al.’s [6]
study reveal a strong correlation between density of change
coupling and failure proneness. Fluri et al.’s [8] study shows
that a large number of change coupling relationships are not
entailed by structural dependencies. While the purpose of
these studies are to statistically account for the relationship
between software defects, change coupling, and syntactic de-
pendencies, Clio’s purpose is to locate modularity violations
that may cause software decay and defects.
3. DETECTION APPROACH
This section presents our modularity violation detection
approach, supported by the Clio framework.7Section 3.1
provides an overview and the following subsections elaborate
the major components and their background knowledge.
3.1 Framework Overview
Suppose that a number of modiﬁcation requests (MRs)
are fulﬁlled when a project evolves from version nton+ 1.
Figure 1 depicts how the project manager can use Clio to
determine whether these changes violate the designed mod-
ular structure so that modularity decay can be detected.
Clio employs a plugin architecture and has three major
components, along with supporting tools. The ﬁrst major
component, dr-predict computes a set of ﬁles that are likely
to be changed together according to the designed modular
structure (Section 3.3). We leverage the design structure
matrix (DSM) model [2], which can be derived from an
augmented constraint network (ACN) [4]—a design model
based on Baldwin and Clark’s design rule theory [2]. An
ACN, in turn, can be derived from source code or design
4http://www.lattix.com/
5http://www.headwaysoftware.com/products/structure101/
6http://www.ndepend.com/
7Clio is the Greek muse of history.models [26,27]. We introduce these background concepts in
Section 3.2. Figure 1 shows that our framework provides
a tool ( Moka [26]) that can reverse-engineer a UML class
diagram from Java binaries, and a tool ( Janus [26]) that
convert the class diagram into an ACN and a DSM to be
used by the dr-predict plugin.
The second major component, logic-predict plugin com-
putes the components that are likely to be changed together
according to change coupling, derived by the extract plu-
gin. The extract plugin of Clio ﬁrst records the set of ﬁles
changed together in transactions8of revision history, and
stores the support and conﬁdence values of each change cou-
pling into a database, following the work of Ying et al. [30]
and Zimmermann et al. [31]. For the solution Sof each mod-
iﬁcation request, the logic-predict plugin selects a subset of
Sthat exhibit the strongest change coupling with other ﬁles
according to the change coupling database. We call this
selected set of ﬁles the starting change set σ.
The logic-predict plugin predicts the change impact of σ
as follows: a ﬁle is predicted to be in the impact scope of σif
the corresponding association rule’s support and conﬁdence
values are above the minimum support thsand conﬁdence
thcthresholds.9The impact scope of σ, computed by logic-
predict , is noted as FileSet B in Figure 1.
The logic-predict plugin shares the σwith dr-predict so
that both plugin components can compute the impact scope
of the same set of ﬁles. The modularity-based impact scope
ofσ, computed by the dr-predict , is noted as FileSet A in Fig-
ure 1. Since σconsists of ﬁles that reveal strongest change
coupling with other ﬁles, the discrepancy between σ’s impact
scopes based on structural couplings and change couplings
is mostly likely to reveal modularity violations.
Finally, given AandB, and a MR solution S, the third
major component of Clio, the detect plug-in, computes a
set of discrepancies, D=(B∩S)\A. By using B∩S, the
detect plugin ﬁlters out ﬁles that were accidentally changed
together. Recurring discrepancies (a subset of ﬁles in D) are
then reported to the users as violations .
Since the logic-predict plugin is a reimplementation of ex-
isting work, in the following subsections we mainly elabo-
rate the modularity-based impact scope analysis approach
embodied by the dr-predict plugin (Section 3.3), the nec-
essary background of it (Section 3.2), as well as the dis-
crepancy calculation method embodied by the detect plugin
(Section 3.4).
3.2 Background
This section introduces key background concepts of our
modularity-based impact scope analysis approach embodied
by the dr-predict plugin. We ﬁrst introduce the augmented
constraint work (ACN) model that is used to derive a design
structure matrix (DSM). By clustering the DSM into a spe-
cial form called the design rule hierarchy (DRH), modules,
8A transaction is deﬁned as an atomic commit in a version
control repository (e.g., Subversion). For repositories that
do not natively support the concept of transactions (e.g.,
CVS), heuristics and techniques (e.g., cvs2svn ) have been
developed to reconstruct transactions.
9Consistent with Zimmermann et al. [31], the frequency of a
set in a set of transactions Tisfrq(T, x)={t|t∈T, x⊆t}|.
The support of a rule, x1⇒x2, by a set of transactions T
issupp (T, x 1⇒x2)=frq(T, x 1∪x2). The conﬁdence of a
rule is conf (T, x 1⇒x2)=frq(T,x1∪x2)
frq(T,x1).Compiled 
Binaries
Revision 
History
Modi cation 
Requests
Solution: SDiscrepancy : DLogical
DepsMoka uml2acndr-predict
Plugin
extract
Plugin
logic-predict
Plugin
detect
PluginFile Set: BFile Set: A
Tool
ArtifactDatabase
Figure 1: Approach Overview: the Clio Framework
deﬁned as independent task assignment, can be automati-
cally identiﬁed and visualized. These modules form the basis
of our change scope analysis approach. Figure 2 depicts a
UML class diagram for a maze game example used in our
prior work [28]. A maze consists of a set of rooms that know
their neighbors, a wall or a door to another room. The base
class, MapSite , captures the commonality of all the maze
components. The diagram shows the abstract factory pat-
tern to support two variations of the game: an enchanted
maze game and a bombed maze game. We use this maze
game as a running example to illustrate these concepts.MapSiteWall Door Room Maze
BombedWall
MazeFactoryNeedingSpellDoor RoomWithA Enchanted
Room
MazeFactory MazeFactoryBombed EnchantedBomb
Figure 2: Maze game UML class diagram [28]
Augmented Constraint Network (ACN). An ACN
consists of a constraint network and a dominance relation.
Figure 3 shows part of an ACN derived from the above UML
class diagram. The constraint network models design deci-sions as variables and model their assumption relations as
logical constraints. In the maze game example, each class is
modeled using two variables (lines 1–6): an interface vari-
able10ending with _interface and an implementation vari-
able ending with _impl . Each variable has a two-value do-
main modeling a current decision and an unknown possibil-
ity. Lines 7 to 9 show several sample assumption relations.
For example, since Room inherits from MapSite , its imple-
mentation makes assumption on both the interface and im-
plementation of MapSite (lines 7, 8).
The dominance relation in an ACN describe asymmet-
ric dependency relationships among design decisions, the
essence of Baldwin and Clark’s concept of design rules [2].
Baldwin and Clark coined the term, design rules , to refer to
stable design decisions that decouple otherwise coupled de-
sign decisions, hiding the details of subordinate components.
We emphasize that Baldwin and Clark’s concept of design
rule is diﬀerent from the concept of rules used in other ar-
eas (e.g., the rules of not creating clones or cyclic dependen-
cies) but rather they are essentially generalized interfaces
between components. Example design rules include abstract
interfaces, application programming interfaces (APIs), or a
shared data format agreed among development teams [22].
Broadly speaking, all non-private parts of a class used by
other classes can be seen as design rules.
For example, line 11 models that Room ’s implementation
decision cannot inﬂuence its interface design, which is a de-
sign rule . One should not arbitrarily change Room ’s interface
to improve its implementation because other components
may depend on it. In our previous work, we deﬁned eight
heuristics to automatically derive dominance relations from
reverse-engineered UML diagrams. Dependencies of a UML
class diagram, such as method calls and object aggregations,
are used to derive constraints in the ACN. The details on
all the heuristics is described in our prior work [26].
1. MapSite
 interface :{orig, other }
2. MapSite
 impl :{orig, other }
3. Room
 interface :{orig, other }
4. Room
 impl :{orig, other }
5. Maze
 interface :{orig, other }
6. Maze
 impl :{orig, other }
7. Room
 impl =orig ⇒MapSite
 interface =orig
8. Room
 impl =orig ⇒MapSite
 impl =orig
9. Maze
 impl =orig ⇒Room
 interface =orig
10. (MapSite
 impl, MapSite
 interface )
11. (Room
 impl, MapSite
 interface )
12. (Maze
 impl, Room
 interface )
Figure 3: Partial Maze game ACN [28]
Design Structure Matrix (DSM). Figure 4 shows a
DSM automatically derived from the maze game ACN. A
DSM is a square matrix whose columns and rows can be
labeled with design variables of an ACN. Each cell marked
with “x” represents a pairwise dependency relation deﬁned
on ACN: if ydepends on x, it means that ymust be changed
10Aninterface variable in an ACN represents the publicly
accessible methods, ﬁelds, etc. of a class. It should not
be confused with the programmatic interface construct pro-
vided by many object-oriented languages.in one of multiple ways to restore the ACN consistency that
is broken by changes to x, and that yis not a design rule
ofx. If so, the cell on row y, column xwill be marked. For
example, cell (r11, c2) indicates that Room_impl depends on
MapSite_interface .
Design Rule Hierarchy (DRH). In order to identify
modules—independent task assignments according to Par-
nas’ deﬁnition [17], our prior work deﬁned a special clus-
tering based on the ACN called the design rule hierarchy
(DRH). Using this clustering, the columns and rows of the
DSM can be reordered into layers , that is, a lower triangle
form in which the top right corner is blank. The ﬁrst layer
in a DSM, l1, is the group of variables clustered at the top
left corner, and does not depend on any other layers. A
layer lnonly depends on layers ln−1tol1. In a DRH, each
layer contains a set of modules that are independent from
each other. In the DSM, the modules are inner groups of
variables along the diagonal, and there are no dependencies
between the modules within the same layer.
Figure 4 shows a DSM clustered into a DRH with four lay-
ers (outer rectangle in bold line along the diagonal) in total:
The ﬁrst layer (r1-2, c1-2) contains the most inﬂuential de-
sign rules that must remain stable. In other words, changing
the top-level design rules, Maze_interface andMapSite_in-
terface , can have drastic eﬀects on the system. The second
layer (r3-6, c3-6) contains decisions that only depend on the
top layer decisions (r1-2, c1-2) . Similarly, the third layer (r7-
13, c7-13) contains decisions that make assumptions about
the decisions within the ﬁrst two layers only.
Within each layer, there are inner rectangles along the
diagonal line such as (r1, c1) or (r7-8, c7-8). They are
modules containing decisions that can be made in paral-
lel because there are no inter-module dependencies within
a layer. For example, MazeFactory_interface (r7) and
MazeFactory_impl (r8) decisions can be made in parallel
with other inner decisions of the same layer, such as Door-
NeedingSpell_interface (r12). The modules in the last
layer (r14-24, c14-24) can be designed, changed, and re-
placed concurrently with each other, not aﬀecting the rest of
the system. For example, the task of designing an enchanted
maze game (r16-17) and the task of designing a bombed
maze game (r20-21) can be independently accomplished.
3.3 Modularity-based Impact Scope Analysis
Given a starting change set σand a DRH-clustered DSM,
thedr-predict plugin calculates the change impact of σas
follows: all the ﬁles that belong to the same module of σare
within its impact scope; if a ﬁle belongs to a module that
depends on the module of σ, then the more dependencies
between the modules, the more likely the dependent module
is within the impact scope of σ; the design rules of σshould
never within its impact scope.
We leverage Robillard’s [19] relevant artifact recommen-
dation algorithm, which identiﬁes a subset of nodes in a
graph, relevant to the initial set of interests based on the
graph’s topology. A DRH-clustered DSM can be represented
as a directed acyclic graph where each vertex ucorresponds
amodule in the DSM, containing a set of decisions, and
each edge ( u→v) deﬁnes that changing a module umay
aﬀect a module v. To demonstrate our approach, we depict
a small subset of the maze game DRH graph in Figure 5 for
the purpose of illustration. In Figure 5, we only show 1 of
the 2 modules in layer 1, 3 modules each from layer 2 andFigure 4: Maze game DSM [28]
3, and 1 module from layer 4. Note that the edges of the
DRH graph are populated based on constraints in the ACN
as introduced in our prior work [28].
Starting from the starting change set (with shaded back-
ground and white text), we assign a weight µ, in the range
[0,1], to each vertex, in a breadth-ﬁrst order. The start-
ing change set vertices are assigned the maximum weight
of 1 and added to a initial set of interests, S. From vertex
Room_interface , we examine its neighbors, the subordinate
decisions that Room_interface inﬂuences, and assign them
a weight. While traversing the graph to assign weights, we
ignore the starting change set’s design rules because they
are supposed to be stable. For example, since the Room class
is the starting change set (row 5 and row 11 in the DSM) in
our example, then its design rules, MapSite ’s interface and
implementation should not be within their impact scope.
Robillard [19] deﬁnes a formula for computing the weight
of a vertex:
µ0=„1+|Sforward ∩S|
|Sforward |·|Sbackward ∩S|
|Sbackward |«α
Using this formula, we assign higher weights to vertices that
share more edges with elements in the set of interest S. This
allows us to identify the components that are likely to be af-
fected by the starting change set due to the strengths of their
design-level dependencies. µis a weight and αis a constant
deﬁned to determine the degree of relevancy propagation.11
To start each iteration of the algorithm, we take all the
vertices that have just been assigned weights, add them to
the set of interest S, and use them as the starting points
for the next round of weight assignment. We repeat this
iterative process until the new weights fall below a certain
threshold. All vertices that were not assigned a weight are
considered to have the minimum weight of 0. Figure 5 shows
11Following the results of Robillard, we use α=0.25 in our
evaluation. However, the value of αdoes not change the
order of suggested elements, so the choice of value is not
important.the weights for each vertex after all weights are propagated.
The vertices whose weights are above the threshold thd(e.g.,
0.75) are then recommended as being in the impact scope
(depicted with a dashed enclosure).
3.4 Discrepancy Analysis
Given the impact scopes of the starting change set σcal-
culated by dr-predict and logic-predict , the detect plugin
ofClio calculates their discrepancies. Because the impact
scope results vary with the thresholds selected, our frame-
work automatically chooses the thresholds with best accu-
racy, measured u sing the standard F1value from information
retrieval. The dr-predict plugin varies the minimum weight
threshold thdfrom 0 to 0.95 in increment of .05 to ﬁnd a
threshold that maximize F1. Similarly, the logic-predict plu-
gin independently varies the support threshold from 2 to 10
and varies the confidence threshold from 0 to 0.95 in incre-
ment of .05 to ﬁnd the maximum F1.
Given the most accurate predictions from dr-predict and
logic-predict , the detect plugin computes their discrepan-
cies and identiﬁes recurring discrepancies over multiple ver-
sions of the software, using a frequent-pattern mining algo-
rithm [12]. The recu rring patterns among these discrepan-
cies are called modularity violations . Consider two MRs with
the same starting change set of {a}. Suppose that the set of
discrepancies is {{a,b,c},{a,b}}. Then, we say that {a,b}is
a modularity violation that occurred twice, and {a,b,c}is a
modularity violation that occurred once.
For example, EnchantedMazeFactory_impl andBombed-
MazeFactory_impl are both located in the last layer of the
DRH, meaning that they should evolve independently from
each other. Clio’sdr-predict plugin would never report that
they are within each other’s impact scope. If the revi-
sion history shows that they consistently change together
(e.g., due to similar changes to cloned code) Clio would
report that there is a modularity violation . Consider an-
other example, since MapSite_interface is the design rule
ofRoom_impl , it is normal that MapSite_interface changesRoom_interface
Room_implMapSite_interface
EnchantedRoom_interface
EnchantedMazeFactory_interface
EnchantedMazeFactory_implDoor_interface
MazeFactory_interface
MazeFactory_implMapSite_implµ = 0
µ = 0 µ = 0
µ = 1
µ = 1 µ = 0.84µ = 0.71
µ = 0.78Layer 1
Layer 2
Layer 3
Layer 4
Figure 5: Maze game design rule hierarchy
and inﬂuences Room_impl along with other dependent com-
ponents. But Clio’sdr-predict plugin would never predict
MapSite_interface to be in the change scope of Room_impl .
If the revision history shows that MapSite always changes
with Room_impl , it is a violation because all other compo-
nents that depend on MapSite may be aﬀected, causing un-
wanted side eﬀects.
4. EVALUATION
To assess the eﬀectiveness of Clio’s modularity violation
detection approach, the evaluation aims to answer the fol-
lowing questions:
Q1. How accurate are the violations identiﬁed by
Clio?That is, do the identiﬁed violations indeed indicate
problems? Given the diﬃculty of ﬁnding the designers of the
subject systems who can most accurately answer this ques-
tion, we evaluate Clio retrospectively and conservatively: we
examine the project’s version history to see how many vio-
lations we identiﬁed in earlier versions are indeed refactored
in later versions or recognized as design problems by the
developers (e.g., through modiﬁcation requests, source code
comments). The precision calculated this way is the most
conservative, lower-bound estimation because it is possible
that some violations we identiﬁed have not been recognized
by the developers yet, and could be refactored in the future.
We do not calculate the recall of our result because it is not
possible to ﬁnd all possible design issues in a system.
Q2. How early can Clio identify problematic vio-
lations? Our purpose is to see if this approach can detect
design problems early in the development process. Although
it may not be necessary to ﬁx a violation as soon as it ap-
pears, making designers aware of violations as soon as pos-
sible can help to avoid accumulating modularity decay. For
each conﬁrmed violation, we compare the version where it
was identiﬁed with where it was actually refactored or rec-
ognized by the developers.Table 1: Characteristics of subject programs
Subjects
 SLOC
 #Transactions
 #Releases
 #MRs
Eclipse JDT
 137K-222K
 27806
 10
 3458
Hadoop
 13K-64K
 3001
 15
 490
Q3. What are the characteristics of violations
identiﬁed by our approach? We examined the detected
violations’ corresponding code to see whether they show any
symptoms of poor design and categorized the violations into
four categories.
4.1 Subjects
We choose two large-scale open source projects, Hadoop
Common and Eclipse Java Development Tools (JDT), as
our evaluation subjects. Hadoop is a Java-based distributed
computing system. We applied our approach to the ﬁrst 15
releases, 0.1.0 to 0.15.0, covering about three years of devel-
opment. Eclipse JDT is a core AST analysis toolkit in the
Eclipse IDE. We studied 10 releases of Eclipse JDT, from re-
lease 2.0 to 3.0.2, also covering about three years of develop-
ment. Our evaluation used both their revision histories and
source code. For Hadoop, we investigated their SVN reposi-
tory to extract transactions. Eclipse JDT used CVS instead
of SVN, so we use the cvs2svn12tool to derive the trans-
actions. In Table 1, we present some basic data regarding
to Hadoop and Eclipse JDT that we studied. We removed
commits with only one ﬁle or more than 30 ﬁles because they
either do not contribute to Clio’s modularity violation detec-
tion or they include noise such as changes to license informa-
tion. For each release pair nandn+1, we computed discrep-
ancies between the results of structural-coupling based im-
pact scope analysis and the results of change-coupling based
impact scope analysis. We then accumulated the discrepan-
cies over the ﬁve most recent re leases to identify recurring
violations. The experiments showed that the results do not
signiﬁcantly diﬀer if we aggregate discrepancies over more
than ﬁve releases.
12http://cvs2svn.tigris.org/Table 2: Modularity violations that occurred at least
twice in the last ﬁve releases
|V|
 |V∩R|
 |V∩M|
 |CV|
 Pr.
Eclipse JDT
 399
 55
 104
 161
 40%
Hadoop
 231
 81
 71
 152
 66%
4.2 Evaluation Procedure
We ran our experiments on a Linux server with two quad-
core 1.6Ghz Intel Xeon processors and 8GB of RAM. We
evaluate the output of Clio (i.e., a set of violations ) by
checking the source code and MR records in later versions
to see if they were indeed refactored or recognized as hav-
ing a design problem. If so, we call such violation as being
conﬁrmed . We use both automated method and manual in-
spection to conﬁrm a violation.
First, we compared the detected violations with refac-
torings that were automatically found by Kim et al.’s API
matching tool [15]. This API matching tool takes two pro-
gram versions as input and detects nine diﬀerent types of
refactorings at a method-header level. It extracts method-
headers from both old and new versions respectively, ﬁnds a
set of seed matches based on name similarity, generates can-
didate high-level transformations based on the seed matches,
and iteratively selects the most likely high-level transforma-
tion to ﬁnd a set of method-header level refactorings. We
chose this technique because it has a 5.01% higher precision
than other similar techniques according our recent compar-
ative study [29].
As these automatically reconstructed refactorings are me-
thod-header level refactorings, we aggregated them up to a
class level to compare with the violations Clio identiﬁed.
We consider a violation as conﬁrmed if it overlaps with any
class-level refactorings. For each violation that is matched
with a reconstructed refactoring, we manually checked the
refactoring to verify that it was indeed a correct refactoring
that ﬁxes design problems since the API-matching tool can
report false positive refactorings.
Second, to complement this automated validation approach,
we also manually inspected modiﬁcation request descriptions
and change logs in the version history to check whether pro-
grammers ﬁxed, or at least plan to ﬁx, these reported vi-
olations through redesign or refactoring activities. For the
rest of the reported violations, we studied the correspond-
ing source code to see whether they include any symptoms
of poor design.
4.3 Results
We analyzed our results by answering the questions pro-
posed at the beginning of the section.
4.3.1 Q1. Accuracy of Identi ﬁed Design Violations
Table 2 shows the total number of violations reported by
Clio (|V|), the total number of violations that match with
automatically reconstructed refactorings ( |V∩R|), the total
number of remaining violations that were conﬁrmed based
on manual inspection ( |V∩M|), the total number of con-
ﬁrmed violations |CV|(which is |V∩R|+|V∩M|), and
theprecision , which is deﬁned as the number of conﬁrmed
violations out of the number of reported violations:|CV|
|V|.Clio reported 231 violations that occur at least twice in a
ﬁve release period in Hadoop, out of which 152 (66%) were
conﬁrmed. 81 of them were automatically conﬁrmed and
71 were manually conﬁrmed. Figure 6 shows the precision
for those violations that occur at least twice and the viola-
tions that occur at least three times. With at least three
occurrences, we obtain a similar precision of 67% but fewer
reported violations. For Eclipse JDT, Clio reported 399 vi-
olations, of which 161 were conservatively conﬁrmed (40%
precision). Requiring violations to occur at least three times
increased the precision to 42%. We only discuss the results
of requiring at least two occurrences for the rest of the paper
because the results of higher occurrence rates are its subsets.
By comparing the results of Hadoop and Eclipse JDT, we
ﬁrst observe that Eclipse is better modularized and more
stable: although Eclipse JDT is about 10 times larger than
Hadoop, less than three times more refactorings were dis-
covered from Eclipse JDT tha n from Hadoop, showing that
it has been less volatile. This is consistent with the fact
that only 12% of all the 3767 Eclipse MRs were detected
to have violations (in Hadoop, the number is 47% out of
the 490 MRs), showing that the changes to Eclipse JDT
matches its modular structure better. Because Eclipse JDT
is much larger and the violations found are much sparser, it
was much harder for us to determine if a violation indicates
a problem, hence leading to a lower precision.
Figure 6: Precision (Hadoop)
In-depth Case Study: Hadoop. Now we present an
in-depth study of Hadoop to demonstrate examples of vio-
lations that are (1) automatically conﬁrmed violations, (2)
manually conﬁrmed violations, (3) false positives (violations
that are not conﬁrmed), and (4) false negatives (refactorings
that are not identiﬁed as violations).
Automatically conﬁrmed violations: In release 0.3.0,
Clio identiﬁed a violation involving FSDirectory andFS-
Namesystem .FSNamesystem depends on FSDirectory.isVa-
lidBlock method, but it often changes with FSNamesystem .
An API-level refactoring was identiﬁed in release 0.13.0,
showing that the isValidBlock method was moved from
FSDirectory toFSNamesystem . Upon further investigation,
we saw that, in the subsequent release, the method was made
private . In this case, Clio identiﬁed this violation 11 releases
prior to the actual refactoring.Manually conﬁrmed violations: Clio reported a vi-
olation in release 0.2.0 involving TaskTracker ,TaskInPro-
gress ,JobTracker ,JobInProgress , and MapOutputFile that
does not match with automatically reconstructed refactor-
ings. We searched Hadoop’s MRs and found an open request
MAPREDUCE-278 , entitled “Proposal for redesign /refac-
toring of the JobTracker andTaskTracker ”. The MR states
that these classes are “ hard to maintain ,brittle , and merits
some rework. ” The MR also mentions that the poor design
of these components have caused various defects.
False positive violations: Violations in this category
cannot be conﬁrmed either automatically or manually. In
most cases, we cannot determine if there is a problem be-
cause we are not domain experts. As an example, in release
0.4.0, Clio reports a violation containing ClientProtocol ,
NameNode ,FSNamesystem , and DataNode .ClientProtocol
contains a public ﬁeld with the protocol version number
and whenever the protocol changes, this number needs to
change. Since NameNode ,DataNode , and FSNamesystem im-
plement the protocol, changes to them induce a change to
ClientProtocol . Although there may actually be a design
problem, we are not able to determine it for sure.
Refactorings that are not violations: Some recon-
structed refactorings are not matched to any violations iden-
tiﬁed by Clio. There are many micro-refactorings that hap-
pen within a class and do not inﬂuence the macro-structure
of the system. Refactorings can also be performed for other
purposes besides addressing modularity violations.
Another reason is that some discrepancies only occur once,
soClio cannot tell if they are accidentally changed together
or there is a problem, but the developers may have realized
and ﬁxed it before it happens again. For example, in version
0.15.1, the INode inner class of FSDirectory was refactored
and extracted into a separate class, and two of its sub-types
INodeFile andINodeDirectory were created so that the
DFSFileInfo andBlocksMap classes can be separated and
use speciﬁc INode subtypes. Clio did not identify a violation
between these classes because they were only involved in a
single MR during the time frame we examined.
4.3.2 Q2. Timing of Violation Detection
In Hadoop and Eclipse JDT, Clio identiﬁes a violation,
on average, 6 and 5 releases respectively, prior to the re-
leases where the classes involved in the violation were actu-
ally refactored. Figure 7 shows the distribution of the con-
ﬁrmed violations over Hadoop releases. Each point in the
plot represents a set of conﬁrmed violations. The horizontal
axis shows the version that the violations were ﬁrst identi-
ﬁed by Clio and the vertical axis shows the version that the
violations were refactored or recognized by the developers.
Points above 20 in the vertical axis signify that the viola-
tions have been recognized by developers but not refactored
yet. Most of the points in Figure 7 are above the line, indi-
cating that Clio can identify design violations early in the
development process so that the designers can be alarmed
to avoid these problems accumulating into severe decay.
4.3.3 Q3. Characteristics of Identi ﬁed Violations
We further analyzed the symptoms of design problems as-
sociated with the detected violations and categorized them
into the following four types: (1) cyclic dependency, (2) code
clone, (3) poor inheritance hierarchy, and (4) unnamed cou-
pling. The ﬁrst three symptoms are both well deﬁned and
Figure 7: Timing of Violation Detection (Hadoop)
Table 3: Characteristics of the Violations
Subjects
 Cyclic
 Clone
 Inheritance
 Coupling
Eclipse JDT
 72
 52
 19
 25
Hadoop
 58
 18
 37
 66
can be detected using existing tools. We call the fourth cat-
egory unnamed because they are not easily detectable using
existing techniques, to the best of our knowledge. Table 3
shows the number of conﬁrmed violations under each cat-
egory in Hadoop and Eclipse JDT. The cyclic dependency,
code clone, and unnamed coupling violations reported in the
table are mutually exclusive from each other. The symptoms
of poor inheritance hierarchy often overlap with cyclic de-
pendency or unnamed coupling. Next we provide examples
from each category.
Cyclic Dependency. Both systems contain consider-
able number of cyclic dependencies. For example, in Eclipse
JDT, we found that the JavaBuilder andAbstractImage-
Builder often change together, and the code shows that
JavaBuilder contains a subclass of AbstractImageBuilder ,
andAbstractImageBuilder contains a JavaBuilder . In a
syntactical DSM, there are no symmetric marks to alert the
designer of this indirect cyclical dependency. Similarly, we
found that all of the following ﬁve ﬁles, or their subsets of-
ten change together: JavaProject ,DeltaProcessor ,Java-
ModelManager ,JavaModel , and JavaCore . It turns out that
these ﬁve classes form a strongly connected components if
represented as a syntactic dependency graph.
Code Clone. Some modularity violations detected by
Clio involve code clones. In Hadoop version 0.12.0, a de-
tected violation involves the classes Task ,MapTask , and Re-
duceTask .Clio reported two violations: one involving Map-
Task and Task , and the other involving ReduceTask and
Task . Various methods and inner classes from ReduceTask
and MapTask were pulled up to the parent Task class in
versions 0.13.0, 0.14.0, and 0.18.0. In Eclipse JDT and
Hadoop, there are 52 and 18 violations, respectively, that
exhibit symptoms of code clones. A traditional clone detec-
tor would likely identify more clones than Clio, but it may
be too costly and unnecessary to refactor all of them. Clio
highlights the ones that happen recently and frequently, and
hence provides more targeted candidates for refactoring.Poor Inheritance Hierarchy. The poor hierarchy vi-
olations we identiﬁed all have the symptoms that the sub-
classes causing the base class and/or other subclasses to
change for diﬀerent reasons. For example, we identiﬁed, in
version 0.2.0 of Hadoop, a violation involving the Distri-
butedFileSystem andFileSystem classes, which was refac-
tored in version 0.12.0: several methods in DistributedFi-
leSystem were pulled up to its parent, FileSystem , making
them available to the other FileSystem subtypes. Another
reason is that the subclasses extensively use some methods
in their parent class and a push-down method refactoring
should have been applied [9]. For example, in Hadoop ver-
sion 0.14.0, the getHints method was pushed down from
theClientProtocol to its subclass, DFSClient , because it
was the only user of this method. They were detected as a
violation in version 0.2.0.
In some cases, the parent classes depend on the subclasses
and form a cyclic dependency. In Hadoop version 0.1, mod-
iﬁcation request #51 describes changing the Distributed-
FileSystem class but its parent class FileSystem and an-
other child of the FileSystem ,LocalFileSystem , are also
part of its solution. There are no syntactic dependencies be-
tween the two sibling classes. By release 0.3, Clio reported
that this modularity violation was observed more than three
times already. The code shows that the parent FileSys-
tem class contains methods to construct both of the two
subclasses. The parent class is thus very unstable because
changes to a child require changes to itself and its other
children. Our intuition that this is a problematic issue was
conﬁrmed when we looked forward through the revision his-
tory and found that by release 0.19, the method to construct
DistributedFileSystem had been deprecated in FileSys-
tem, in favor of a method in a external class. As a similar
example in Eclipse, Scope is the parent of ClassScope and
BlockScope , but it constructs both of it’s children. We cat-
egorized this types of violation as both poor inheritance and
cyclic dependency.
Unnamed Coupling. The ﬁles involved in violations of
this category often change together, but they either do not
explicitly depend on each other (and are not code clones),
or have asymmetric dependencies. For example, In Hadoop,
DatanodeInfo andDataNodeReport were involved in a vio-
lation, and was later refactored. In the modiﬁcation request
comments, the developer says that these classes “seem to be
similar” and needed to be refactored.
The FSDirectory andFSNamesystem we mentioned ear-
lier is also an example of unnamed coupling. Clio detected
this violation because the only allowed change order is from
the interface of FSDirectory toFSNamesystem . But the re-
vision history shows that changes to FSNamesystem often
cause FSDirectory to change. In the corresponding syn-
tactical DSM, these two classes reside in the same package,
andFSNamesystem depends on FSDirectory . Using a Lat-
tix DSM, the user can mark that FSDirectory should not
depend on FSNamesystem so that if FSDirectory explicitly
refers to FSNamesystem , Lattix will raise an alarm. How-
ever, in reality, FSDirectory never explicitly refers to FS-
Namesystem , although it often changes with FSNamesystem .
Table 3 shows that in Hadoop 66 out of 152 of the conﬁrmed
violations fall into this category (In Eclipse, the number is
25 out of 161). We are not aware of existing techniques
that detect these violations that do not ﬁt to pre-deﬁned
symptoms of poor design5. DISCUSSION
The quality of our modularity violation detection approach
depends heavily on the availability of modiﬁcations requests
and their solutions. For small-scale projects or projects
without version control systems, it is hard to apply Clio.
When calculating change coupling, how long a version his-
tory is enough? The answer depends on the speciﬁc project
and how to determine the best threshold is our ongoing work.
In the evaluation, we used all available revision histories to
determine change coupling. Changing the number of ver-
sions used for analysis may alter the results. Our decision of
only considering the ﬁve most recent releases in evaluation
when determining violations is based on the fact that the
results do not signiﬁantly diﬀer when we consider more ver-
sions. Again, this heuristic may vary with diﬀerent projects.
The selection of a starting change set can signiﬁcantly af-
fect the accuracy of violation detection. We use the most
highly coupled elements in a MR solution as the starting
change set. However, other heuristics can be used for select-
ing a starting change set. Identifying such heuristics and
seeing how they aﬀect the accuracy of violation detection is
an ongoing future work. Automatically recovering the origi-
nal starting change set of an MR is an active, but immature,
area of research (e.g., Antoniol et al. [1]). Such techniques
try to reconstruct what developers would have ﬁrst mod-
iﬁed in fulﬁlling the MR. As these techniques mature, we
can evaluate their eﬀectiveness in improving our approach.
Since we only applied Clio to two subject systems, we
cannot conclude that the eﬀectiveness of Clio generalizes
to all software systems; however, we did choose projects of
diﬀerent sizes and domains to begin addressing this issue.
In addition, we cannot guarantee that the modiﬁcation re-
quests used in the evaluation are not biased. As Bird et
al. [3] showed, the MRs that have associated change sets
may not be representative of all the MRs in the system. For
example, although we claim to identify design violations for
actively-developed parts of a system, the collected MRs may
not include the most active parts of the system.
Some violations detected using Clio may not embody any
design problems but reveal valid semantic dependency, as
shown in previous work [30,31]. But our experiments show
that considerable number of violations indeed reﬂect design
problems. The accuracy of Clio also depends on how ac-
curate the ACN model embodies design decisions and their
assumption relations. The ACN model we used in this pa-
per were automatically generated from UML class diagrams
derived from code. Some dependencies can only be reﬂected
in other design models, such as an architectural description.
It is possible that these dependencies are missing from the
ACN model, hence causing false positives. The violation we
discussed in the previous section that contains ClientPro-
tocol ,NameNode ,FSNamesystem , and DataNode is such an
example. A future work is to improve Clio by using high-
level architectural models in addition to reverse-engineered
source models.
6. CONCLUSION
Parnas’s original deﬁnition of a module means an indepen-
dent task assignment, and his information hiding principle
advocates separating internal design decisions using an in-
terface to allow for independent evolution of other modules.
Problems occur if modules that are designed to be inde-pendent always have to change together. This paper pro-
poses a novel approach of identifying eroding design struc-
ture by computing the discrepancies between how compo-
nents should change together and how they actually change
together. We evaluated Clio using the version histories of
Hadoop Common and Eclipse JDT. We conservatively con-
ﬁrmed hundreds of reported violations to be correct. The
result also shows that detected modularity violations exhibit
various symptoms of poor design, showing Clio’s advantages
in contrast to bad-code smell detection techniques that ﬁnd
only pre-deﬁned set of poor design symptoms, without re-
gard to the system’s original design structure nor its evolu-
tion history.
7. ACKNOWLEDGEMENTS
This work was supported in part by National Science
Foundation under grants CCF-0916891, CCF-1043810, and
DUE-0837665.
8. REFERENCES
[1] G. Antoniol, G. Canfora, G. Casazza, and A. D.
Lucia. Identifying the starting impact set of a
maintenance request. In Proc. 4th CSMR , pages
227–230, Mar. 2000.
[2] C. Y. Baldwin and K. B. Clark. Design Rules, Vol. 1:
The Power of Modularity . MIT Press, 2000.
[3] C. Bird, A. Bachmann, E. Aune, J. Duﬀy,
A. Bernstein, V. Filkov, and P. Devanbu. Fair and
balanced? bias in bug-ﬁx datasets. In Proc. 17th FSE ,
pages 121–130, Aug. 2009.
[4] Y. Cai. Modularity in Design: Formal Modeling and
Automated Analysis . PhD thesis, University of
Virginia, Aug. 2006.
[5] Y. Cai and K. J. Sullivan. Modularity analysis of
logical design models. In Proc. 21st ASE , pages
91–102, Sept. 2006.
[6] M. Cataldo, A. Mockus, J. A. Roberts, and J. D.
Herbsleb. Software dependencies, work dependencies,
and their impact on failures. TSE, 35(6):864–878, July
2009.
[7] S. R. Chidamber and C. F. Kemerer. A metrics suite
for object oriented design. TSE, 20(6):476–493, June
1994.
[8] B. Fluri, H. C. Gall, and M. Pinzger. Fine-grained
analysis of change couplings. In Proc. 5th SCAM ,
pages 66–74, Sept. 2005.
[9] M. Fowler. Refactoring: Improving the Design of
Existing Code . Addison-Wesley, July 1999.
[10] H. Gall, K. Hajek, and M. Jazayeri. Detection of
logical coupling based on product release history. In
Proc. 14th ICSM , pages 190–197, Nov. 1998.
[11] J. Garcia, D. Popescu, G. Edwards, and
N. Medvidovic. Identifying architectural bad smells. In
Proc. 13th CSMR , pages 255–258, Mar. 2009.
[12] J. Han, J. Pei, and Y. Yin. Mining frequent patterns
without candidate generation. In Proc. SIGMOD ,
pages 1–12, May 2000.
[13] Y. Higo, T. Kamiya, S. Kusumoto, and K. Inoue.
Refactoring support based on code clone analysis. In
Proc. 5th PROFES , pages 220–233, Apr. 2004.[14] S. Huynh, Y. Cai, Y. Song, and K. Sullivan.
Automatic modularity conformance checking. In Proc.
30th ICSE , pages 411–420, May 2008.
[15] M. Kim, D. Notkin, and D. Grossman. Automatic
inference of structural changes for matching across
program versions. In Proc. 29th ICSE , pages 333–343,
May 2007.
[16] N. Moha, Y.-G. Gu´ eh´eneuc, A.-F. Le Meur, and
L. Duchien. A domain analysis to specify design
defects and generate detection algorithms. In Proc.
11th FASE , pages 276–291, Mar. 2008.
[17] D. L. Parnas. On the criteria to be used in
decomposing systems into modules. CACM ,
15(12):1053–8, Dec. 1972.
[18] J. Ratzinger, M. Fischer, and H. Gall. Improving
evolvability through refactoring. In Proc. 2nd MSR ,
pages 1–5, May 2005.
[19] M. P. Robillard. Topology analysis of software
dependencies. TOSEM , 17(4):18:1–18:36, Aug. 2008.
[20] N. Sangal, E. Jordan, V. Sinha, and D. Jackson. Using
dependency models to manage complex software
architecture. In Proc. 20th OOPSLA , pages 167–176,
Oct. 2005.
[21] R. W. Selby and V. R. Basili. Analyzing error-prone
system structure. TSE, 17(2):141–152, Feb. 1991.
[22] K. J. Sullivan, W. G. Griswold, Y. Cai, and B. Hallen.
The structure and value of modularity in software
design. In Proc. 8th FSE , pages 99–108, Sept. 2001.
[23] N. Tsantalis, T. Chaikalis, and A. Chatzigeorgiou.
JDeodorant: Identiﬁcation and removal of
type-checking bad smells. In Proc. 12th CSMR , pages
329–331, Apr. 2008.
[24] N. Tsantalis and A. Chatzigeorgiou. Identiﬁcation of
extract method refactoring opportunities. In Proc.
13th CSMR , pages 119–128, Mar. 2009.
[25] N. Tsantalis and A. Chatzigeorgiou. Identiﬁcation of
move method refactoring opportunities. TSE,
35(3):347–367, May 2009.
[26] S. Wong. On the Interplay of Architecture and
Collaboration on Software Evolution and Maintenance .
PhD thesis, Drexel University, Dec. 2010.
[27] S. Wong and Y. Cai. Predicting change impact from
logical models. In Proc. 25th ICSM , pages 467–470,
Sept. 2009.
[28] S. Wong, Y. Cai, G. Valetto, G. Simeonov, and
K. Sethi. Design rule hierarchies and parallelism in
software development tasks. In Proc. 24th ASE , pages
197–208, Nov. 2009.
[29] W. Wu, Y.-G. Gu´ eh´eneuc, G. Antoniol, and M. Kim.
AURA: A hybrid approach to identify framework
evolution. In Proc. 32nd ICSE , May 2010.
[30] A. T. T. Ying, G. C. Murphy, R. Ng, and M. C.
Chu-Carroll. Predicting source code changes by mining
change history. TSE, 30(9):574–586, Sept. 2004.
[31] T. Zimmermann, P. Weißgerber, S. Diehl, and
A. Zeller. Mining version histories to guide software
changes. In Proc. 26th ICSE , pages 563–572, May
2004.