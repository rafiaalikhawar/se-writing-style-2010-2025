CarFast: Achieving Higher Statement Coverage Faster
Sangmin Park
Georgia Institute of
Technology
Atlanta, Georgia 30332, USA
sangminp@cc.gatech.eduIshtiaque Hussain,
Christoph Csallner
University of Texas at Arlington
Arlington, TX 76019, USA
ishtiaque.hussain@
mavs.uta.edu,
csallner@uta.eduKunal Taneja
Accenture Technology Labs
and North Carolina State
University
Raleigh, NC 27606, USA
ktaneja@ncsu.edu
B. M. Mainul Hossain
University of Illinois at Chicago
Chicago, IL 60607, USA
bhossa2@uic.eduMark Grechanik
University of Illinois and
Accenture Technology Lab
Chicago, IL 60601, USA
drmark@uic.eduChen Fu, Qing Xie
Accenture Technology Labs
San Jose, CA 95113, USA
{chen.fu, qing.xie}
@accenture.com
ABSTRACT
Testcoverageisanimportantmetricofsoftwarequality,si nceitin-
dicates thoroughness of testing. In industry, test coverag e is often
measured as statement coverage. A fundamental problem of so ft-
ware testing is how to achieve higherstatement coverage faster,
and it is a difﬁcult problem since it requires testers to clev erly ﬁnd
input data that cansteer execution sooner toward sections o f appli-
cation code that contain more statements.
Wecreatedanovelfullyautomaticapproachfor aChievinghigher
stAtementcoveRageFASTer(CarFast) ,whichweimplementedand
evaluated on twelve generated Java applications whose size s range
from 300 LOC to one million LOC. We compared CarFast with
severalpopulartestcasegenerationtechniques,includin gpureran-
dom, adaptive random, and Directed Automated Random Testin g
(DART). Our results indicate with strong statistical signi ﬁcance
that when execution time is measured in terms of the number of
runs of the application on different input test data, CarFas t outper-
forms the evaluated competitive approaches onmost subject appli-
cations.
Categories andSubject Descriptors
D.2.5[SoftwareEngineering ]: TestingandDebugging— Symbolic
execution, Testingtools
Keywords
Testing, StatementCoverage, Experimentation
1. INTRODUCTION
Test coverage is an important metric of software quality [63 ],
since it indicates thoroughness of testing. Statement coverage ,
which measures the percentage of the executed statements to the
Permission to make digital or hard copies of all or part of thi s work for
personal or classroom use is granted without fee provided th at copies are
not made or distributed for proﬁt or commercial advantage an d that copies
bear this notice and thefull citation on the ﬁrstpage. Tocop y otherwise, to
republish, topostonserversortoredistribute tolists,re quires priorspeciﬁc
permission and/or afee.
SIGSOFT’12/FSE-20, November11–16,2012,Cary,NorthCarolina, USA.
Copyright 2012 ACM 978-1-4503-1614-9/12/11 ...$15.00.total number of statements in the application under test [63 ], is
viewed as an important kind of test coverage. Achieving high er
statement coverage is correlated with the probability of de tecting
more defects [51, 49, 38, 10] and increasing reliabilityof s oftware
[44, 11]. Even though it is agreed that statement coverage al one
may not always be a strong indicator of software quality [36, page
181], itisageneral consensus thatachieving higher statem entcov-
erage is desirable for gaining conﬁdence insoftware qualit y, and it
serves as an indicator for test completeness and effectiven ess [51,
11, 59].
Statement coverage is widely used in industry as a common cri -
terion for thoroughness of software testing. Different sta ndards re-
quire achieving high levels of statement coverage: for exam ple,
avionics industry standard, DO-254, demands that close to 100%
statement coverage be achieved, and avionics industry stan dard,
DO-178B and automotive industry standard, IEC 61508 detail
differentrequirements onachievingstatement coverage. M anydif-
ferent organizations use statement coverage as the major cr iterion
for measuring the quality of software testing [18, 39, 52, 58 , 7].
Given the importance of statement coverage, how common it is ,
and how widely it is used to evaluate the thoroughness of test -
ing, it is not surprising that statement coverage is the one c overage
metric that is supported by almost all coverage-based testi ng tools,
whereas other coverage measures (e.g., branch coverage, me thod
coverage, or class coverage) are supported byfewer tools [6 2, 60].
1.1 HigherStatement CoverageFaster
Achieving higher statement coverage means that testers hav e to
select test input data with which they can execute larger por tions
ofapplicationcode. Higherstatementcoverage isalways be tterfor
increasingtheconﬁdenceofstakeholdersinthequalityofs oftware;
however, 100% statement coverage is rarely achieved, espec ially
when testinglarge-scale applications [51, 45, 18]. The fas ter these
testersachievehighercoverage,theloweristhecostoftes ting[35],
sincetesterscanconcentratesooneronotheraspectsoftes tingwith
the selected input data, for example, performance and funct ional
testingwithoracles.
We measure the speed with which a certain level of statement
coverage isachievedbothin thenumber oftestruns oftheapplica-
tion under test (AUT) with different test input data (i.e., i terations
of executing the same application with different input data ) and in
the elapsed time of running the AUT. Whilethe elapsed time givestheabsolutevalueofthetimeittakestoreachacertainleve lofcov-
erage;measuringthenumberofiterations,whichessential lymeans
number of test cases to achieve the coverage goal, is importa nt for
many reasons, e.g., using fewer test cases requires less man ual ef-
fort for creatingtest oracles for these testcases.
Moreover, measuring the number of iterations provides an in -
sight into the potential of a given approach. The elapsed tim e in-
cludes time for generating or selecting input data, and time to run
the AUT using these data. In many systematic test case genera tion
or selection approaches, the time spent on generating or sel ecting
input data is signiﬁcant. Thus, if an approach achieves a hig her
test coverage using fewer iterations, but spends more time o n each
iteration, this time can eventually be reduced by improving the ef-
ﬁciencyof the particular approach.
A big and important challenge is to get higher statement cove r-
age fasterfornontrivialapplications thathaveaverylarg espace of
input parameter values. Many nontrivial applications have com-
plex logic that programmers express by using different cont rol-
ﬂow statements, which are often deeply nested. In addition, these
control-ﬂowstatementshavebranchconditionsthatcontai nexpres-
sions that use different variables whose values are compute d using
some input parameters. In general, it is difﬁcult to choose s peciﬁc
values of input parameters to direct the execution of these a pplica-
tions tocover speciﬁcstatements.
The maximum test coverage is achieved if an application is ru n
with all allowed combinations of values for its inputs. Unfo rtu-
nately, this is often infeasible because of the enormous num ber of
combinations; for example, 20 integer inputs whose values o nly
range from zero to nine already leave us with 1020combinations.
Knowingwhatcombinationsoftestinputdatatoselectfordi fferent
input parameters to drive the AUT towards the statements nes ted
insidevariousbranchesisverydifﬁcult. Thus,afundament alprob-
lemofsoftwaretestingishowtoachievehigherstatementco verage
fasterby selecting speciﬁc values of input parameters that lead th e
executions of applications to cover more statements in a sho rter
time period.
1.2 A Novel Approach
We created a novel approach for aChieving higher stAtement
coveRage FASTer (CarFast) using the intuition that higher state-
ment coverage can be achieved faster if input data are select ed to
drive the execution of the AUT toward branches that contain m ore
statements. That is, if the condition of a control-ﬂow state ment is
evaluatedto true,somecodeisexecutedinthescopeofthisstate-
ment. The statements that are contained in the executed code are
saidtobecontrolledbyorcontainedinthecorrespondingbr anchof
this control-ﬂow statement. In program analysis, these sta tements
aresaidtobe controldependent [48]onthecontrol-ﬂowstatement.
In CarFast, static analysis is used to estimate how many stat e-
mentsarecontainedineachbranch. Onceitisknownwhatbran ches
contain more statements, CarFastuses a constraint-based s election
approach [41] to select input data that will guide the execut ions of
the AUT towards these branches. In CarFast, input data are no t
generated, but they rather come from external databases as w e de-
scribe it in Section 2.5. Comparing to CarFast, many automat ic
testdatagenerationtechniquesusecomputationallyexpen sivecon-
straint solvers [14, 19, 25, 43] to generate test data for achiev-
ing higher test coverage. Using these solvers negatively af fects the
scalabilityofthesetechniques. Byreplacingconstraints olverswith
selectors,notonlydoes CarFastachieve highercoverage fa ster,but
alsoit achieves betterscalability(Section5).
CarFastoffersmultiplebeneﬁts: itisfullyautomaticsinc eitdoes
not require any intervention by testers; it is directed sinc e it ex-plores branches inthe control-ﬂow graph (CFG) oftheAUTrather
than an enormous space of the combinations of the values for t he
inputtestdata;anditisscalableonlarge-scaleapplicati onswithup
to500kLOC as demonstrated by our experiments withtwelve Ja va
applications with up to one million LOC. Even though we built
CarFasttoworkwithJavaprograms, therearenofundamental lim-
itationstogeneralizing ittootherlanguages andplatform s,suchas
C# or C++. This approach may be generalized for other coverag es
that can be (1) statically approximated; and (2) dynamicall y com-
puted. To the best of our knowledge, CarFast is the ﬁrst appro ach
that deﬁnes and uses static program analysis-based coverag e gain
prediction to achieve higher coverage faster, and this work is the
ﬁrstthatevaluated thisidea withstrongstatisticalsigni ﬁcance ona
large number of subject applications of varying sizes.
1.3 OurContributions
Thispaper makes the followingcontributions:
•We developed a novel algorithm for achieving higher state-
ment coverage faster and we implemented it as part of Car-
Fast.
•We applied CarFast to twelve Java applications whose sizes
range from 300 LOC to one million LOC that we generated
using stochastic parse trees [57, 32]. CarFast, subject Jav a
applications, and the stochastic application benchmark ge n-
erator are available for public use1.
•We conducted large-scale experiments using Amazon EC2
to evaluate CarFast and competitive approaches against one
another, speciﬁcally, pure random and adaptive random test -
ing, and Directed Automated Random Testing (DART) [26]
using a rigorous experimental evaluation methodology [2].
The results show that when execution time is measured in
terms of iterations, CarFast outperforms evaluated compet i-
tiveapproaches formostsubjectAUTswithstrongstatistic al
signiﬁcance.
•Finally, we compared pure random testing to adaptive ran-
dom testing to address an open issue in determining which
approach is better [1, 47]. Adaptive random performs statis -
tically as good as pure random testing when lower coverage
istargetedandtimeismeasuredintermsofiterations. When
higher coverage is targeted, pure random beats adaptive ran -
dom testingwithstrongstatisticalsigniﬁcance.
2. THE CARFASTAPPROACH
In this section, we give an illustrative example of how our ap -
proach works, we formulate our hypothesis upon which we de-
signed our approach, and we give analgorithm ofCarFast.
2.1 AnIllustrativeExample
An illustrative example is shown in Figure 1 using Java-like
pseudo-code. Linenumbers tothe right should be thought of a s la-
bels, as much code is omitted for space reasons. This example has
if-else statementsthatcontrol threebranches, wherebranchla-
bels are shown in comments along withthe numbers of statemen ts
that these branches control. These numbers are given purely for an
illustrativepurpose.
Consider executing this code with randomly selected input v al-
uesi1=3 andi2=7, which leads to the elsebranch3in lines
5–8. The number of distinct uncovered statements that are reach -
able from this branch is 100, which is signiﬁcantly less than the
1All tools and subject applications are available for downlo ad at:
http://www.carfast.org1 if( i1 == 10) {
2 ..// branch 1: 300 statements
3 }else if ( i2 == 50 ) {
4 ..// branch 2: 600 statements
5 }else{
6 ..// branch 3: 100 statements
7 if( ..) { if(..) { .. } .. }
8 }
Figure1: Anillustrative example.
numbersofstatementscontainedinthetwootherbranches. W esay
that a previously uncovered statement Sis contained in a branch
B, if there exists a concrete input that triggers an execution that
evaluates the condition of the branch Bandcovers S.
Besides the lower number of statements reachable from branc h
3, the control ﬂow within branch 3 is also more complex than th e
control ﬂow in branch 1 or branch 2 (as denoted by a few example
nestedif-else statements). Clearly, this is one of the worst test
inputs since it covers only 10% of this code at best. To achiev e
higher coverage faster, we would like to learn from this exec ution
to select a test input that satisﬁes i1/ne}ationslash=10∧i2=50 to steer the
next run toward branch 2, since it contains the biggest number of
distinct reachable uncovered statements (i.e., 600), thus increasing
test coverage upto70%.
However, none of the existing approaches can systematicall y
steer the execution towards branch 2, since it is the nature of ran-
domization to select data points independently from one ano ther
fromtheinputspace. DARTdynamicallyanalyzesprogrambeh av-
ior using random testing and generates new test inputs autom ati-
cally to direct the execution systematically along alterna tive pro-
gram paths [26]. A problem is that the original DART algorith m
will keep exploring all nested branches in branch 3using adepth-
ﬁrst search algorithm. That is, DART keeps exploring the branch
for a while even though the gain in coverage will rather be min us-
cule.
2.2 Our Observation, Preliminary Study, and
Hypothesis
Weobservethatmanyapplicationshaveafewbranchesthatco n-
tain large bodies of statements and many more branches that c on-
tain only few statements. This observation is supported by a pre-
liminary study we did on three nontrivial and widely used ope n-
source Apache applications, log4j2,Ant3, andJMeter4. For
each application, we counted the number of statements ineac h ba-
sic block. Our results indicate that the number of statement s per
basic block approximates the power law [42], i.e., approxim ately
80% of the statements are in20% of the basic blocks. Speciﬁca lly,
20%ofthebasicblockscontain73%ofthestatementsin Ant,65%
inJmeter, and 66% in log4j. Assuming that this observation
holdsformanyapplications,ourintuitionisthatwecanexp loitthis
skewed size distribution of the basic blocks in test case sel ection.
I.e., we want to systematically steer AUT execution towards these
large basic blocks, toachieve higher statement coverage fa ster.
Wehypothesizethatwecansigniﬁcantlyacceleratetestsel ection
techniques by guiding the selection to test input data that c overs
those branches that contain more uncovered statements, by e sti-
matingthenumberofdistinctstatements. Thishypothesisi srooted
in the essence of systematic testing, which is a counterpart to ran-
2Version1.2.16, http://logging.apache.org/log4j
3Version1.8.2,http://ant.apache.org
4Version1.0,http://jmeter.apache.orgdom testing where test data inputs are selected without any p rior
knowledge [30]. To test our hypothesis, we combine systemat ic
and random testing approaches in a novel way using the insigh t
that higher coverage can be achieved faster if it is known whi ch
distinct branches that are still uncovered contain more sta tements.
Knowingtheconstraints frombranch conditions mayenable s elec-
tion of test input data that leads to execute the statements w ithin
those branches.
2.3 CarFastby Example
We review how our approach works using our illustrative ex-
ample of Figure 1. Once branch 3of this code is executed us-
ing input values i1=3 andi2=7, constraints C1:i1/ne}ationslash=10 and
C2:i2/ne}ationslash=50 can be learned automatically. Since branch 2con-
tains the biggest number of statements (i.e., the 600 statem ents of
line4), constraint C2can be negated and the resulting constraint
formula will be C1∧¬C2ori1/ne}ationslash=10∧i2=50. In the next step,
test input data is obtained that ﬁts this constraint, that is the value
ofi1/ne}ationslash=10 andi2=50. This process can be repeated as often as
necessary toachieve higher coverage.
Inthe worstcase, this approach may resultintestinput data that
leadsexecutiontowardsalreadyexecutedbranchesorbranc hesthat
have fewer statements. For example, if the ﬁrst input data is se-
lectedi1=10 and branch 1is executed, no additional useful con-
straintexceptfor i1/ne}ationslash=10willbelearnedduringthissteptohelpour
approachtosteerexecutiontowardbranch 2. However,random se-
lection allows testers to select completely different data , which in
turnwillleadtodifferentexecutionproﬁlesandlearningm orecon-
straints [30, 29, 3]. Our hypothesis is that by learning more of
these constraints with each execution of the AUT, it is possi ble to
converge to higher coverage faster. Verifying this hypothe sis is a
goal of this paper.
2.4 Ranking Branches by Statements
Tounderstandwhichbranchesaffectstatementcoveragethe most,
we rank each branch (if, loop, etc.) by the number of statemen ts
it contains (the number of statements that are transitively control-
dependent on that branch). Executing higher ranked branche s en-
ablesachievinghigherstatementcoveragefaster. Torankb ranches,
we construct and traverse a CFG of the AUT, then we count the
(inter-procedural) statements that are control-dependen t on each
branch condition. I.e., if in method c, branchbtransitively con-
trols a call to method m, then the statements of mare treated as if
they were in-lined into the calling method c, and some statements
ofm(and possibly statements of methods called by m) may be in-
cluded inthe count of statements thatare control-dependen t onb.
Speciﬁcally, if the method mis virtual, we perform virtual call
resolution using static class hierarchy analysis, and coun t all the
statements in all the target methods; but use the one with the max-
imum statementstodetermine thenumber ofstatements contr olled
bybranchb.
2.5 Selecting Existing TestInput Data
We assume that the test input data come from existing reposit o-
ries or databases. This is a common practice in industry, we c on-
ﬁrmed it after interviewing professionals at IBM, Accentur e, two
large health insurance companies, a biopharmaceutical com pany,
twolargesupermarket chains,andthreemajorbanks. Forins tance,
theRentersInsuranceProgram designedandbuiltbyamajorinsur-
ance company has a database that contains approximately 78 m il-
lioncustomer proﬁles,whichare used as the testinput data.
As part of CarFast, we translate constraints into SQL querie s
against such existing databases [41]. For example, to ﬁnd va luesfor input variables i1 andi2 that satisfy the constraint i1/ne}ationslash=10∧
i2=50, the following SQL query is executed: SELECT*FROM
InputTbl WHERE i1 != 10 AND i2 = 50 . SomeoftheseSQL
queries include millions of conditions in the WHEREclause, which
caused runtime problems in some commercial strength databa se
management system such as Microsoft SQL server. To scale Car -
Fast, we developed a lightweight and efﬁcient SQL-based con -
straint evaluator that wedescribe inSection3.3.
2.6 The Algorithm
The algorithm CarFast is shown in Algorithm 1. This algo-
rithm takes as its input the set of the input parameter values T, the
AUTP, and the set of accounted AUT branches, B. The total cov-
erage score totalCov is computed and returned in line 31of the
algorithm.
In step2, the algorithm initializes the values for total coverage
totalCov to zero, the set of covered branches Bcovwhose state-
ments are covered, and the set of constraints to the empty set . In
step3, the procedure ComputeBranchFun is called that com-
putes the function BRankthat maps each branch of the AUT, P, to
the approximate number of statements that are reachable fro m this
branch. Next,instep 4,theprocedure Sortsortselementsofinput
setBinthedescendingorderaccordingtothenumberofstatement s
using the function BRank, producing the sorted set Bs. In step5,
the procedure GetRandomTestInput randomly selects a data
object,tfrom the set of the input parameter values, that is, Input
TestData, T,andthisdataobjectisremovedfromthesetinstep 6.
Thisalgorithmrunstheloopbetweensteps 7–30,whichterminates
on thecondition of the reached timelimitor desiredcoverag e (i.e.,
thepredeﬁnedvalue covCeiling ). Instep8,theAUT, Pisexecuted
using the input data t, resulting in the updated value of totalCov,
added branches that were covered during this execution to th e set
of covered branches, Bcov, and added constraints that are learned
during this execution. Then, in the forloop insteps 10–25,each
member branch in the set Bsis examined to check whether it was
coveredinthepreviousrunoftheAUT.Ifsomebranch, bkwascov-
ered, it is removed from the set Bsin line23, otherwise, the ﬁrst
occurrenceofthecorrespondingconstraint, Ckisinverted(ignoring
the following constraints inthe pathcondition) inline 12.
Bytreatingaconstraintasaquerytoobtaininputdatathats atisfy
the conditional WHEREclause, the subsets of test input data ,{tc}
is obtained in line 13that satisfy this clause, that is the ﬂipped
constraint. If{tc}is empty, then no test input data from our input
database can lead to the desired branch, and this message is i ssued
inline20. Otherwise,oneinput, tisrandomlyselectedfromtheset
{tc}inline15andthecontrolisreturnedtoline 25andeventually
to line8, where the AUT is run with this input thereby repeating
the loop.
In some cases, it may not be possible to know the exact con-
straints toreach certain statements, since the set of const raints that
is collected by the concolic engine corresponds to reaching differ-
ent nodes of the CFG,and subsequently different statements . Flip-
pingtheseconstraintsandsolvingtheseﬂippedconstraint s mayre-
sult in input data that will lead the AUT toward other uncover ed
statements, but not necessarily the desired statements. Ho wever,
as more constraints are collected with newly obtained test i nput
data, these constraints eventually enable CarFast to narro w down
the scope of the executed statements tothe ones thatare desi rable.
This works only if the input test data set contains an input th at
can reach such statements. If the application contains a sta tement
thatisnotreachablewiththeinputsfromInputTestData,th enCar-
Fast will never cover that statement. However, as the result s of theAlgorithm 1 The CarFastalgorithm.
1:CarFast( TestInputData T, AUTP,AUTBranches B)
2:totalCov←0,Bcov←{/0},C←/0{Initialize values of the total
statement coverage, the set of covered branches, and the set of
constraints.}
3: ComputeBranchFun( P)/mapsto→BRank:{B}∋b→rank
4: Sort(B,BRank)/mapsto→Bs{Sort elements of the set in the descend-
ingorder bytheir rankusing the function BRank}
5: GetRandomTestInput( T)/mapsto→t∈T
6:T/mapsto→T\t
7:repeat
8: RunAUT( P,t)/mapsto→[(totalCov/mapsto→totalCov +cov′),(Bcov/mapsto→
Bcov∪∆B),(C/mapsto→C∪(C1∧...∧Cn))]
9: foundTest4Branch ←false
10:for allbk∈Bsdo
11: ifbk/∈BcovandCk∈Cthen
12: FlipConstraint( C)/mapsto→(C/mapsto→C1∧...∧¬Ck))
13: GetTestInput( C)/mapsto→{tc}∈T
14: if{tc}/ne}ationslash=/0then
15: GetRandomTestInput( {tc})/mapsto→t
16: T/mapsto→T\t
17: foundTest4Branch ←true
18: break the forloop
19: else
20: printGivenconstraint Ccannot be satisﬁed
21: endif
22: else
23: Bs/mapsto→Bs\bk
24: endif
25:endfor
26:iffoundTest4Branch = false then
27: GetRandomTestInput( T)/mapsto→t
28: T/mapsto→T\t
29:endif
30:untiltime limitisnot reached or totalCov <covCeiling
31:returntotalCov
experimental evaluation show in Section 5, CarFast outperf orms
other competitive approaches under different conditions.
3. IMPLEMENTATIONANDDEPLOYMENT
In this section, we describe main challenges and the salient fea-
turesofourimplementationanddeploymentincludingtheco ncolic
engine.
3.1 MainChallenges
Our implementation goal is to demonstrate that CarFast is vi -
able by applying it to large-scale AUTs. There are two main ch al-
lenges: it is memory-intensive and itcontains CPU-intensi ve com-
ponents. Extracting constraints by executing AUTs takes ti me and
most importantly, signiﬁcant amounts of memory. Typically , con-
colicenginesincurmorethananorderofmagnitudeoverhead from
normal program execution. Memory footprint of concolic eng ines
increases quickly as the engines must keep track of symbolic rep-
resentations of all aspects of the current program executio n (e.g.,
symbolic representations of the static ﬁelds of all loaded c lasses)
as execution traces get longer. In our experiments, one extr acted
constraintfroma50KLOCAUTisoverﬁvemegabytesanditssiz e
growsto50GBforonemillionLOCAUT!Moreover, solvingsuch
constraints can take a long period of time, since it involves per-
forming queries onlarge sets of input test data.3.2 ConcolicExecution Engine
We used Dsc [33], a Java dynamic symbolic execution engine
(i.e., a concolic engine) for Java AUTs in CarFast. Below we d e-
scribe its main features and how we adapted it to scale to larg e
AUTs.
3.2.1 OverviewofDsc
Dsc instruments the bytecode of AUTs automatically by inser t-
ing method calls (i.e.,callbacks) after each instruction i nthe code.
During AUT execution, the callbacks enable Dsc to maintain t he
symbolic state by mirroring the effects of each user program in-
struction, including the effects of reading and writing hea p (i.e.,
array and object ﬁeld) locations, performing integer and ﬂo ating
point arithmetic, following the local and inter-procedura l control-
ﬂows, andhandling exceptions.
Dsc integrates well with the existing Java execution enviro n-
ments; itdoes not require anymodiﬁcations ofthe user appli cation
code, or the virtual machine. Dsc uses the instrumentation f acili-
ties provided by the JVM of Java 5to instrument the user progr am
at load-time [15], using the open source bytecode instrumen tation
framework ASM [8]. By manipulating programs at the bytecode
level, Dsc extends its analysis from the user code into all li braries
called by these programs. In addition, Dsc allows users to se lec-
tivelyexclude classes from instrumentation.
3.2.2 TheDumperModeofDsc
Dsc in its normal mode represents every concrete computatio n
by a corresponding symbolic expression, caches it in memory and
utilizes it later when the same computation is repeated or is used
in a subexpression. However, nontrivial applications cont ain large
number of computation steps and caching symbolic expressio ns
quickly exhaust the available heap memory. Moreover, often com-
putations are done in loops or recursive call chains and when con-
colicengines process theseloops orrecursivecallchains, theypro-
duce long symbolic expressions, which are in turn used in sub se-
quent computations adding quickly to the total length of the re-
sulting symbolic expression. As a result, even for moderate size
programs, concolic executions quickly exhaust all memory.
Toscale tolargeapplications, we introduced a dumper mode f or
Dsc to minimize the memory consumption for the symbolic stat e
representation. Instead of caching symbolic expressions i n heap
memory, this dumper mode introduces local variables (symbo ls)
for each expression and dumpsor writes these expressions to the
disk. Later, a dynamic lookup and replacement technique is u sed
onthedumpﬁletobuildtheconstraintsorpathconditioninvolving
input parameters.
3.3 Constraint-Based Selector
ToimprovethescalabilityofCarFast,wedevelopedaconstr aint-
based selector rather than using off-the-shelf constraint solvers.
Our motivation is twofold: improving the speed of computati on
and better utilizing resources. Speciﬁcally, our concolic engine,
Dsc is a 32-bit tool, meaning that it can only use less than fou r
gigabytes of RAMata time. Addinga constraint solver tothe p ro-
cessspace ofDscwouldsigniﬁcantlyreduceavailablememor y. To
address this problem, we implemented the constraint-based selec-
tor as a separate server process that can serve many Dsc clien ts
simultaneously through socket interfaces.
Fortheimplementationoftheconstraintsolver,atthebegi nning,
we kept all the data in a relational database. Then, by runnin g a
query, wetriedtoselectthe data thatsatisfythe constrain t given as
the WHERE clause of the query. But, the traditional RDBMSsfailed to process a query with such a big constraint part in th e
WHEREclause.
As a different approach, we wrote a set of production rules to
deﬁne a formal grammar so that every possible constraint can be
recognized by that grammar. We used the ANTLR tool5for pro-
cessing the grammar and its languages. With the help of ANTLR ,
we were able toparse and process much larger constraints.
When the server process (constraint solver) receives a quer y
fromaclient,theserverbuildstheabstractsyntaxtreefor thecondi-
tionpart (WHEREclause) ofthe query. Then, itevaluates the tree,
inabottom-upfashion, againstallpossibleinputdatainth ereposi-
tory. Finally,astestinputdata,theserverprocessreturn svaluesfor
the parameters that satisfy all the conditions in the condit ion-part
of the query.
3.4 Miscellaneous
We implemented branch ranking using Java static analysis an d
transformation engine called Soot [54]. All conditional br anch
statements in the AUTs are ranked using the approach describ ed
in Section 2.4. At runtime, we used EMMA6to compute and re-
port statement coverages. Also, we modiﬁed callback functi ons in
Dsctokeeptrackofthecoveredbranches duringthetestexec ution
to reset rankings of the already executed branches to zero to avoid
repeatedly executing already coveredstatements.
4. EXPERIMENTS
To determine how effective CarFast is in achieving higher st ate-
ment coverage faster, we conducted an experiment with compe ti-
tive approaches such as random testing, adaptive random tes ting,
and DART on twelve Java applications (i.e., AUTs) whose size s
rangefrom300LOCtoonemillionLOC.Inthissection,webrie ﬂy
describethesecompetitiveapproaches, providethemethod ologyof
our experimental design, explain our choice of subject AUTs , and
discuss threats tovalidity.
4.1 Variables
Main independent variables are the subject AUTs, the value o f
testcoveragethatshouldbeachievedforAUTsineachexperi ment,
and approaches with which we experiment (i.e., random, adap tive
random testing, DART, and CarFast). A dependent variable is the
execution time that it takes to achieve a given test coverage . We
measure the execution time both in terms of elapsed time, Eand
as a number of iterations of AUT executions with different in put
values,I. The effects of other variables (the structure of AUT and
the types and semantics of input parameters) are minimized b y the
design of thisexperiment.
4.1.1 RandomTesting
Random testing approach, as the name suggests, involves ran -
dom selection of test input data for input parameter values, and
in that it showed remarkably effective and efﬁcient for expl oratory
testing and bug ﬁnding [4, 24]. A seemingly “stupid” idea of r an-
dom testing proved often more effective than systematic sop histi-
catedtestingapproaches[30,29]. Toproveourclaimsinthi spaper,
ourgoalistoshowunderwhatconditionsCarFastoutperform sran-
dom testingwithstrong statisticalsigniﬁcance.
4.1.2 AdaptiveRandomTesting
Adaptive random testing (ART) is a controversial reﬁnement of
the baseline random testing where randomly selected data ar e dis-
5http://www.antlr.org/
6http://emma.sourceforge.nettributed evenly across the input data space [12]. In that, AR T in-
troduces a certain level of control over how input data is sel ected
when compared with the baseline random testing. A recent imp le-
mentationof ARTforobject-oriented languages isARTOO,wh ich
we use as a competitive approach to CarFast in our experiment s
[13]. Prior to our experiment, ARTOO was evaluated on eight
classes from the EiffelBase library, and the sizes of these c lasses
ranged from 779LOC to 2,980LOC. Recently, Arcuri and Briand
presented statistically signiﬁcant results of experiment s that ques-
tion the effectiveness of ARTOO with respect to bug detectio n for
programs withseededfaults[1]. Meyerpointedout inhisres ponse
[47] that the programs with seeded faults behave much differ ently
from programs withrealfaults. Moreover, Arcuri andBriand mea-
sured the time to ﬁnd the ﬁrst fault as a testing metric, which may
not be a rigorous metric [50]. Therefore, we performed an exp er-
iment comparing random testing and ARTOO with a set of small
to large programs with statement coverage as a testing metri c. In
this paper, we also address a research question of how effect ive
ARTOO is in achieving higher coverage faster against compet itive
approaches including random testing.
4.1.3 DART
Directed Automated Random Testing (DART) is an approach
that uses a concolic engine to generate test inputs that expl ore dif-
ferentexecution pathsofaprogram [26]. IntheoriginalDAR Tal-
gorithm,pathexplorationisconductedin Depth-First-Order(DFO)
orBreath-First-Order (BFO) of navigating the CFG of the AUT.
We faithfully re-implemented DART using Dsc, so that we can
evaluate it in an unbiased fashion against CarFast. In the or igi-
nal paper [26], DART was previously evaluated only on three C
applications whose sizes range from a dozen LOC to 30kLOC.
Even though there are many implemented variations of DART,
(e.g., jCUTE, KLEE, Pex ), DART has never been evaluated with
strong statisticalsigniﬁcance on benchmark AUTs.
4.2 Methodology
Our goal is to determine which approach achieves higher stat e-
ment coverage faster. Given the complexity of the subject AU Ts,
it is not clear what is the highest coverage that can be achiev ed for
these AUTs,and given a largespace of input data, itis not fea sible
toruntheAUTsonallinputstoobtainthehigheststatementc over-
age. Theselimitationsdictatethemethodologyofourexper imental
design, speciﬁcally for choosing the threshold for the desi red test
coverage, whichis AUT-speciﬁc and ingeneral less than100% for
a number of reasons, not the least of which is the presence of u n-
reachable code in AUTs. Before conducting experiments, we r un
each benchmark AUT against pairwise test input data, and use the
resulting achieved coverage as the coverage threshold for i t. Each
experiment run ishalted wheneither it hits the coverage thr eshold,
or the execution time limit (24 hours) is reached. The time li mit is
determined experimentally. See Section5.2for details.
We aligned our methodology with the guidelines for statisti cal
tests to assess randomized algorithms in software engineer ing [2].
Our goal is to collect highly representative samples of data when
applying different approaches, perform statistical tests on these
samples, and draw conclusions from these tests. Since our ex -
periments involve random selection of input data, it is impo rtant
to conduct the experiments multiple times to pick the averag e to
avoid skewed results. For each subject application, we ran e ach
experiment 30 times with each approach on the same AUT to con-
sider collected data a good representative sample. It means that
for a total of 12 AUTs we ran 30 experiments for each of the fourapproaches, resulting in a total of 12 ×4×30=1,440 experiment
runs.
To evaluate our hypotheses, we ran statistical tests based o n the
assumption that the population is normally distributed. Th e law
of large numbers states that if the population sample is sufﬁ ciently
large (between 30 to 50 samples), then the central limit theo rem
applies even if the population is not normally distributed [ 56, page
244-245]. Since we have 30 sample runs for each AUT for each
conﬁguration, the central limit theorem applies, and the ab ove-
mentioned tests have statisticalsigniﬁcance.
Experiments are carried out in Amazon EC27virtual machine
largeinstances with the following conﬁguration: 7.5 GB RAM, 4
EC2 Compute Units (2 virtual cores with 2 EC2 Compute Units
each),35GBinstancestorage. Weseta24-hourtimelimitfor each
experiment run. So the estimated total runtime is 1 ,440×24=
34,560 hours. With the cost of USD 0.48 per large instance per
hour as of September, 2011, the estimated cost of this experi ment
was around USD 16,500. However, we underestimated the cost
of building, testing and ﬁxing the experiment environment i tself,
whichresulted ina total cost of around USD30,000.
4.3 Hypotheses
We introduce the following null and alternative hypotheses to
evaluate how close the means are for the Es andIs for control and
treatment groups. Unless we specify otherwise, CarFast is a pplied
to AUTs in the treatment group, and other competitive approa ches
are applied to AUTs in the control group. We seek to evaluate t he
followinghypotheses ata 0 .05 level of signiﬁcance.
H0Theprimarynullhypothesis isthatthereisnodifferencein the
valuesoftestcoveragethatAUTscanachieveinagiventime
interval.
H1Analternative hypothesis to H0is that there is statisticallysig-
niﬁcant difference in the values of test coverage that AUTs
canachieve ina given timeinterval.
Once we test the null hypothesis H0, we are interested in the
directionality of means, µ, of the results of control and treatment
groups, where Sis eitherIorE. In particular, the studies are de-
signed toexamine the following null hypotheses:
H1:CarFast versus Random. The effective null hypothesis is tha t
µCarFast
S=µRand
S, whilethe true null hypothesis is that
µCarFast
S≥µRand
S. Conversely, the alternative hypothesis is
µCarFast
S<µRand
S.
H2:CarFast versus ARTOO. The effective null hypothesis is that
µCarFast
S=µARTOO
S, whilethe true null hypothesis is that
µCarFast
S≥µARTOO
S. Conversely, the alternative hypothesis is
µCarFast
S<µARTOO
S.
H3:CarFast versus DART. The effective null hypothesis is that
µCarFast
S=µDART
S, whilethe true null hypothesis is that
µCarFast
S≥µDART
S. Conversely, the alternative hypothesis is
µCarFast
S<µDART
S.
H4:ARTOOversus Random. Theeffective null hypothesis is that
µARTOO
S=µRand
S, while the truenull hypothesis is that
µARTOO
S≤µRand
S. Conversely, the alternative hypothesis is
µARTOO
S>µRand
S.
7http://aws.amazon.com/ec2/instance-types as of March10 , 2012The rationale behind the alternative hypotheses to H1,H2, and
H3is that Carfast achieves certain test coverage faster than o ther
approaches. The rationale behind the alternative hypothes is toH4
is that the random approach outperforms ARTOO as suggested b y
Arcuri and Briand[1].
4.4 Input Test DataRepository
Recall that instead of generating test data, CarFast select s test
input data from existing repositories. Most nontrivial app lications
haveenormousspacesoftestinputdataobjectsthatarecons tructed
by combining values of different input parameters. Even tho ugh it
is infeasible to create a test data repository that contains the entire
input space, itispossible tocreatecombinations ofvalues that will
result in a smaller space of input data objects using combina torial
design algorithms, which are frequently used by testing pra ctition-
ers [28, 16, 40]. Most prominent are algorithms for t–wise combi-
natorial testing, which requires every possible combinati on of in-
terestingvaluesof tparametersbeincludedinsometestcaseinthe
test suite [28]. Pairwise testing is when t=2, and every unique
pair of values for each pair of input parameters is included i n at
least one test case in the test suite. To construct a test data repos-
itory for evaluating CarFast, we used the ACTS8tool (previously
known asFireEye)togenerate data forourexperiments using pair-
wise testingfromthe range of input data [−50,50]that waschosen
experimentally. Since pairwise selection signiﬁcantly re duces the
numberoftestinputdata,weaddeduptoonemillioncombinat ions
of test input data values usingan unbiased random selection .
4.5 Subject AUTs
Given that we claim signiﬁcant improvements in CarFast when
comparedwithcompetitiveapproaches, itisimportanttose lectap-
plication benchmarks that are not biased, nontrivial, ande nable re-
producibility of results among other things. In general, a b ench-
markisapointofreferencefromwhichmeasurementscanbema de
in order to evaluate and predict the performance of hardware or
software or both [46]. Benchmarks are very important for eva lu-
ating program analysis and testing algorithms and tools [5, 6, 20,
53].
4.5.1 ChallengesWith BenchmarkApplications
Different benchmarks exist to evaluate different aspects s uch
as how scalable program analysis and testing tools are, how f ast
they can reach high test coverage, and how effective these to ols
are in executing applications symbolically or concolicall y. Cur-
rently, a strong preference is towards selecting benchmark s that
have much richer code complexity (e.g., nested if-then-else
statements), class structures, and class hierarchies [5, 6 ]. Unfor-
tunately, complex benchmark applications are very costly t o de-
velop [37, page 3], and it is equally difﬁcult to ﬁnd real-wor ld ap-
plications of wide variety of sizes and software metrics tha t can
serve as unbiased benchmarks for evaluating program analys is and
testingapproaches.
Consider our situation where different test input data sele ction
and generation approaches are evaluated to determine which ap-
proach enables users to achieve higher statement coverage f aster.
On one extreme, “real-world” applications of low complexit y with
very few control-ﬂow statements are poor candidate benchma rks,
since most test input data generation approaches willperfo rm very
well, especially if AUTs take as input parameters only primi tive
types. On the other extreme, it may take signiﬁcant effort to ad-
just these approaches to work with a real-world distributed appli-
cation whose components are written in different languages and
8http://csrc.nist.gov/groups/SNS/acts/index.htmlAkLOC Cl Meth NBD MCC WMC
10.3 4 3 2.5/6 6.3/20 23.5/36
20.6 5 5 2.3/5 10.4/30 33.4/56
31.2 14 19 2.0/5 6.9/26 24.2/44
41.3 18 61 2.2/9 3.8/14 22.4/47
52.1 24 49 2.0/5 4.5/13 24.5/36
65.2 37 184 2.0/8 5.2/23 42.9/86
77.8 38 469 2.2/8 4.3/19 63.4/137
824.2 111 765 2.4/8 4.7/23 66.4/102
946.7 61 428 4.2/12 22.3/56 249.2/347
1098.4 96 1,576 3.4/8 10.7/27 325.3/447
11470.8 311 2,244 4/7 34.1/93 464.3/640
121,157.2 781 17,449 3.8/13 13/47 486/631
Table1: SubjectAUT(A)characteristics. Cl=#classes (NOC ),
Meth = #methods (NOM), NBD = nested block depth, MCC
= McCabe cyclomatic complexity, WMC = weighted methods
per class. The last three columns show average and maximum
values as Avg/Max.
run on different platforms. In addition, current limitatio ns of con-
colic engines (e.g., manipulating arrays, different types ) make it
very difﬁcult to select nontrivial application benchmarks to satisfy
these limitations. Ideally, a large number of different ben chmark
applications are required with different levels of code com plexity
toappropriately evaluate testinput data generation tools .
One way to address the problem is to write benchmark applica-
tions that satisfy the requirements. However, writing benc hmark
application from scratch is laborious, not to mention that a signiﬁ-
cant bias and human error can be introduced [34]. In addition , se-
lecting commercial applications as benchmarks negatively affects
reproducibility of results, which is a cornerstone of the sc ientiﬁc
method [55, 23], since commercial benchmarks cannot be easi ly
shared among organizations and companies for legal reasons and
trade-secret protection. For example, Accenture Human Res ource
Policy item 69 states that source code constitutes conﬁdent ial in-
formation, and other companies have similar policies. Fina lly, In
addition, more than one benchmark is often required to deter mine
thesensitivityofprogramanalysisandtestingapproaches basedon
the variabilityofresults forapplications thathave diffe rent proper-
ties [2],making ita verylaborious exercise.
Ideally, users should be able to easily generate benchmark a p-
plications withdesired properties that are similar to real -world ap-
plications. This idea has been already successfully used in testing
relationaldatabase engines, where complex Structured QueryLan-
guage (SQL) statements are generated using a random SQL state-
ment generator [57]. Suppose that a claim is made that a relat ional
databaseengineperformsbetteratcertainaspectsofSQLop timiza-
tionthan some other engine. The best way toevaluate this cla im is
to create complex SQL statements as benchmarks for this eval ua-
tioninawaythatthesestatementsstresspropertiesthatar especiﬁc
to these aspects of SQL optimization. Since the meaning of SQ L
statementsdoesnotmatterforperformanceevaluation,thi sgenera-
tor creates semantically meaningless but syntactically co rrect SQL
statements thereby enabling users to automatically create low-cost
benchmarkswithreducedbias. Inaddition,syntheticprogr amsand
data have been usedwidelyincomputer visionandimage proce ss-
ing[22, 31].
4.5.2 RandomBenchmarkApplications
We deﬁne a random program by construction. Everyprogram is
an instance of the grammar of the language in which this progr am
is written. We use the grammar to generate branches of a parse
tree for different production rules, where each rule is assi gned theprobabilitywithwhichitisinstantiatedinaprogram. Star tingwith
thetopproductionrulesofthegrammar,eachnonterminal is recur-
sively replaced with its corresponding production rule. Te rminals
are replaced with randomly generated identiﬁers and values , and
they are used in expression with certain probability distri butions,
leadingtoasyntacticallycorrectprogram. Thisapproach i swidely
used in natural language processing, speech recognition, i nforma-
tionretrieval[17],andalsoingeneratingSQLstatementsf ortesting
database engines [57].
4.5.3 SubjectAUTsForExperimentation
We generated twelve subject AUTs whose sizes range from 303
LOC to over one million LOC using our program generator [32].
Tominimizetheeffectofusingdifferentlibrariesanddata typeson
our experimental design, we allowed only integer data types and
standard Java language constructs. Each AUT takes 15 input p a-
rameters, this number is chosen experimentally. Table 1 con tains
characteristicsofthesubjectprograms,withtheﬁrstcolu mnshow-
ing the names followed by other columns with different chara c-
teristics of these AUTs as speciﬁed in the caption. We used th e
generated programs without any tweak, and they are availabl e at
the website given inFootnote 1.
4.6 Threats to Validity
The main threat for our experimental design is the selection of
subject AUTs and their characteristics. Due to limitations of con-
colicengines,whichrequiresﬁnitenumberofvaluesforeac hinput,
we had tosynthesize the subject AUTs, and these AUTs have hig h
cyclomatic complexity, which makes it difﬁcult to choose va lues
for input parameters to achieve high coverage faster. The re sults
may vary for AUTs that have very simple logic or different sou rce
code structures.
The other threat to validity comes from evaluating approach es
basedonstatementcoverage ratherthanusingsome faultdet ection
metric. In general, even though a connection exists between state-
ment coverage and fault detection capability, the latter is a more
robust metricsince itgoes intothe heartof amaingoal oftes ting–
bugdetection. However, existingapproaches for applying f aultde-
tection metric use generated mutants, which are not always e quiv-
alenttoapplications withbugsthatareintroducedbyprogr ammers
[50]. Finally, statement coverage is also an important metr ic for
stakeholders toobtainconﬁdencefromtestingapplication s,andwe
evaluate this important testingmetric.
Finally, a threat to validity is our method for selecting ran ges of
input data. Sinceour AUTsaregenerated, itisunclear whatr anges
of input values should be chosen and how the number of combina -
tions of input values can be minimized effectively. In our ex peri-
mental design we used standard practices used by test engine ers at
different Fortune 500 companies, speciﬁcally to apply comb inato-
rial pairwise testing to create sufﬁciently diverse sets of input test
data.
5. RESULTS
In this section, we provide and explain results of experimen ts
and statisticalteststoaddress our hypotheses.
5.1 Testing theNull Hypothesis
WeusedANOVAtoevaluatethenullhypothesis H0thatthevari-
ation in an experiment is no greater than that due to normal va ria-
tion of individuals’ characteristics and error in their mea surement.
The results of ANOVA conﬁrm that there are large differences be-
tween the approaches for coverage for both measures of execu tion
time. Astheresultshows,allp-valuesarelessthan0.05. He nce,wereject the null hypothesis H0and accept the alternative hypothesis
H1.
StatisticalresultsforexecutiontimesareshowninTable2 . DART
ranoutofmemoryforAUTsA6–A12andCarFastranoutofmem-
oryforAUTA12(over1MilLOC).Basedont-testsforpairedtw o
sample for means for two-tail distribution, we reject hypot heses
H1–H2for time measured as iterations, and we reject hypotheses
H3–H4for both measures of time, i.e.,iterations and elapsed time .
We cansummarize these resultsas following.
•When only iterations are counted, CarFast achieves higher
statement coverage faster when compared with the random
and ARTOO approaches for all AUTs with strong statistical
signiﬁcance. CarFast also outperforms DART for all AUTs
but A5, since both approaches reach the desired coverage in
one iterationfor A5.
•Whenonlyelapsedexecutiontimesarecounted,random and
ARTOOachievehigherstatementcoveragefasterwhencom-
pared with CarFast for all AUTs, with strong statistical sig -
niﬁcance. However, when comparing CarFast with DART,
CarFastoutperformsDARTforallAUTsbutA5forthesame
reason mentioned above.
•Whenonlyiterationsarecounted,therandomapproachachie ves
higher statement coverage faster when compared with the
ARTOOapproach forallAUTswithstrongstatisticalsignif-
icance.
•When only elapsed execution times are counted, the random
approach achieves higher statement coverage faster when
comparedwiththeARTOOapproachforallAUTswithstrong
statisticalsigniﬁcance. Butwhenlowercoverageisspeciﬁ ed,
forsmallprograms(e.g.,A1,A2,A4andA5)therandomap-
proach does not outperform ARTOO.
5.2 InvestigationofCornerCases
We investigated the corner cases. First, we found that in A5
the maximum coverage, 46%, is achieved inone iterationusin g all
approaches. Since the obtained test coverage withDART is si gnif-
icantlylowerwhencomparedwithotherapproaches, werunst atis-
tical tests with results for DART excluded, so that we verify that
the results of testinghypotheses still hold. We chose 78% (i .e.,the
minimum coverage of all approaches but DART) as the coverage
level for which we extracted execution times for these appro aches.
The results are consistent with our previous conclusion. We had
similar treatment for A1–A4, i.e., we excluded the results t hat we
obtainedfromDARTfromdataanalysis,andgotconsistentre sults.
Second,wefoundthattheexecutiontimeofCarFastincrease sat
a much faster rate when compared to that of random and ARTOO
approaches as program size increases. We analyzed the execu tion
timeofCarFastforeachbenchmarkAUT,andwefoundoutthatt he
dumper mode of Dsc takes the most of time for large applicatio ns
when compared with other components of CarFast. For A11, it
takes 22.4 minutes on average per one iteration of an execution,
and it takes 75% of entire CarFast execution time. For A12, Ds c
ran out of memory. That is, the dumper mode of Dsc should be
improved tomake CarFastscalable tolargeprograms.
Finally, we found that the accumulated test coverage for Car -
Fast after running for 24 hours is comparable with that of ran dom
and ARTOOapproaches for A1–A8. As mentioned above, it takes
much longer for CarFast to run on larger applications (A9–A1 2),
which results in much fewer iterations (or out of memory) wit hin
24hours and inachieving less testcoverage.ACovAppImedImeanIminImaxSDIEmedEmeanEminEmaxSDECminCmaxCmed
165%Rand 2729.6 135612.1 8897.2 51174 37.786%88%88%
ART 2828.7 126211.3 8384.1 36185 32.784%88%88%
DART 504973.5 12639111003 14102784.0 33811307 2918 65%81%74%
CF1413.8 8192.7495473.7 183718106.8 84%88%86%
84%Rand11871299.2 2822859 73635393886.0 81786132218.9 86%88%88%
ART17301903.3 57050571205.3 51085847.3 166916216 3844.5 84%88%88%
CF254314.1 133680156.7 46055140.3 315087711623.3 84%88%86%
258%Rand 99.3 4162.9 3534.8 1953 8.687%88%88%
ART 1111.1 5224.6 3232.8 156413.586%88%88%
DART 428481.3 1001578297.3 8861020.4 64085 800.6 58%70%64%
CF 87.8 4101.5405.4 400.5 171606107.5 86%88%87%
86%Rand14281607.8 5083996 73642534815.5 150112364 2270.7 87%88%88%
ART17481963.1 75857971031.7 53086019.5 228018325 3305.5 86%88%88%
CF877906.7 3792017394.32060020980.0 1347735228 4961.3 86%88%87%
345%Rand16.517.1 9325.748.5 52.2 27146 2361%62%61%
ART17.517.8 8306.3 6259.8 29103 18.960%62%61%
DART 737693.5 11041255.7 15591477.0 52571 60145%50%48%
CF 65.9 470.5443571.0 2614247 69855%61%60%
55%Rand 333339.8 21362286.6 976999.3 6231832 25861%62%61%
ART432438.4 2051035148.9 13121324.7 6193174 458.5 60%62%61%
CF124133.3 8220629.810143 12314 87247600312060.5 55%61%60%
450%Rand 22.2 230.43 1111.3 718 3.275%77%76%
ART 22.3 230.46 67.2 610 1.675%77%76%
DART 224222.5 106579111.5 332307.2 61362 333.7 50%53%52%
CF 22.4 240.56 43.5 57.1 39182 28.976%78%77%
75%Rand14981636.7 6373470704.9 45185057.9 218811154 2257.1 75%77%76%
ART22302629.8 80252152629.8 70728511.6 246917730 4564 75%77%76%
CF259266.0 18935646.51075110565.9 672613797 1941.4 76%78%77%
546%Rand 111101111.7 827 3.580%81%80%
ART 11110 33.7 340.279%80%80%
DART 11110 33.4 340.546%51%48%
CF 11110 77.7 517 2.778%79%79%
78%Rand 9861023.2 6101558261.8 30213162.5 18674913 842.4 80%81%80%
ART15561615.6 8552717492.4 49155157.7 265689161646.8 79%80%80%
CF459463.9 31255754.61992620040.9 1332024408 2481.8 78%79%79%
676%Rand 860867.0 6991118113.7 27092721.0 21823562 36378%79%79%
ART12091220.0 9451522152.5 39453940.0 30015038 52478%79%78%
CF405405.7 35945219.12309722793.0 1793625691 1754 76%77%77%
779%Rand 526543.1 45771460.616781736.8 14722329 207.3 82%83%83%
ART671684.1 52191486.321672217.6 16753028 299.4 82%83%82%
CF370380.0 31146441.718187 18829 1556722914 1991 79%81%80%
861%Rand 10199.6 86111 8326.5 330.4 282477 37.761%61%61%
ART105107.1 901228.7353.5 358.1 301411 28.761%61%61%
CF100100.2 951093.378047760.7 69828649 385.5 61%61%61%
964%Rand 325327.3 29736719.411581162.0 10391307 75.171%71%71%
ART406398.9 36244021.614861462.0 13081820 107.3 71%72%71%
CF206210.8 19724011.94368542322.0 3290851949 4860.6 64%65%65%
1061%Rand 372375.4 33842319.116051633.6 14141884 112.6 65%65%65%
ART466464.7 40257533.220992112.1 17812694 190.6 64%65%64%
CF241241.9 2222589.56686666837.7 5522879976 6870.7 61%61%61%
1154%Rand 3232.2 28382.8195197.7 171233 16.774%74%74%
ART 3433.6 2937 2207.5 204.3 177277 12.573%74%74%
CF2626.5 24291.24317341832.4 3075547864 4399.2 54%58%56%
73%Rand16211620.4 1554171837.22553025491.7 2346528092 1074 74%74%74%
ART21062114.5 1968221557.33904538946.2 3500541677 1680.5 73%74%74%
1269%Rand10601067.1 954119855.92943929421.7 2396836642 2549.9 70%70%70%
ART15041488.4 1392159456.54805048896.2 4236758034 4365.1 69%69%69%
Table 2: Results of experiments on subject applications und er test (AUTs A1–A12) with approaches ( App): CarFast (CF), Random
(Rand), ARTOO (ART) and DART. Columns Cmin,Cmax, andCmedgive the minimum, maximum and median values of statement
coverage afterrunningtheAUTsfor24hours. Wedeﬁnethemin imumvalueof Cminasthetargetcoverage ( Cov). Wethendetermine
howlongeachapproachtakestoreachthistarget. Execution timesaremeasuredinthenumberofiterations( I)andelapsedtime( E,
inseconds). For each,we reporttheMedian,Mean,Min,Max. a ndthestandarddeviation( SD). AUTsA1–A5reportmeasurements
with and without including the DART approach, but AUTs A6–A1 2 do not include the DART approach. For details please see
Sections 4.2and 5.2.5.3 Our Interpretation ofResults
We can summarize and interpret the results of our experiment s
as follows.
1. WestronglysuggestthatCarFasthashighpotentialinach iev-
ing higher statement coverage faster and becoming practica l
especially if its execution overhead per iteration can be fu r-
ther reduced. We expect toreduce the overhead inthe future
since we found the bottlenecks of CarFast from our experi-
ments.
2. Whenitcomes tocomparing therandom approach withAR-
TOO, the random approach is still better when higher state-
ment coverage is targeted. ARTOO performs as good as the
randomapproachonlywhenlowerstatementcoverageistar-
geted. We suggest that itis likelythat results depend on cer -
taincharacteristicsoftheAUTs,ﬁndingwhichisasubjecto f
future work.
6. RELATED WORK
Our approach isa testcase prioritizationtechnique: choos ing an
ordering of some existing test suite in order to increase the like-
lihood of revealing faults earlier. Elbaum et. al. [21] surv eyed
several approaches that effectively prioritize test cases in regres-
sion testing. These techniques use greedy algorithm to comp ute
anorderthatachieveshighercoveragesoonerorhigherfaul tdetec-
tionrate. Thus,testcoverageorfaultdetectionrateofeac htestcase
mustbeknown. Inthecontextofregressiontesting,eachtes tcase’s
coverage or fault detection rate on previous versions of the AUT is
used to predict their future performance. Our approach does not
require prior knowledge of test cases and thus it is not restr icted in
the context of regressiontesting.
Dynamic symbolic (or “concolic”) execution engines such as
DART [26] generate test inputs that explore different execu tion
paths of a program. In early work, the exploration strategy i s usu-
allydepth-ﬁrst or breath-ﬁrst, whichis the basis for many t estcase
generation techniques, including ours.
Majumdar et. al. [43] interleave random testing and concoli c
exploration to improve test coverage. Their approach start s with
random testing,changes toconcolicexplorationwhenrando mtest-
ingfailstoincrease coverage, andchanges backtorandom as soon
assomecoverage is gained. In contrast, CarFast uses a systematic
approach based on a static program analysis-based branch co ver-
agegainpredictor,andweevaluatedCarFastwithstatistic alsignif-
icance ona large number of subject applications.
Concolic tools use different search strategies to decide ho w to
pick branches where constrains are negated. Xie et. al. [61] use a
branch’s distance to the target path as the ﬁtness function i n their
work. Here distance means the number of conditional control ﬂow
transfers between a branch and the target path. Branches nea r the
target path are more likely to be picked for negation. Simila rly,
BurnimandSenpickabranchwhenitsdistancetosomeuncover ed
path is small [9]. To increase coverage, Sage [27], by Godefr oid
et al., tries to negate not one, but as many constraints in a pa th
condition as possible.
Our approach differs from search-based approaches in the ﬁt -
ness function—to the best of our knowledge, CarFast is the ﬁr st
tool that deﬁnes and uses a static program analysis based cov erage
gain predictor to guide path exploration. However, genetic -based
approaches are not shown to be scalable, and they usually wor k
on generating test data for expressions with less than 100 bo olean
variables. Oursistheﬁrstapproachthatworksforlarge-sc aleappli-
cations, itis scalable, andit does not require any machine- learningalgorithms, whichare usuallycomputationally intensive. Inthe fu-
ture,once scalablegenetic algorithmsaredeveloped forge nerating
test input data for achieving higher coverage faster, we wil l com-
pare CarFastwiththese algorithms.
7. CONCLUSION ANDFUTURE WORK
Wecreatedanovelfullyautomaticapproachfor aChievinghigher
stAtementcoveRage FASTer(CarFast) , bycombining random test-
ingwithstaticprogramanalysis,concolicexecution,andc onstraint-
based input data selection. We implemented CarFast and appl ied
it to twelve Java applications whose sizes range from 300 LOC to
one million LOC. We compared CarFast, pure random, adaptive
random,andDirectedAutomatedRandomTesting(DART)again st
one another. The results show with strong statistical signi ﬁcance
that when execution time is measured in terms of the number of
runs of the application on different input test data, CarFas t largely
outperforms the evaluated competitive approaches with mos t sub-
ject applications.
Ourexperimental resultsare promising, and thereare sever al ar-
eas that will improve our work. First, we plan to adapt CarFas t
to other test coverage metrics, such as branch coverage and b asic
blockcoverage, tostudyifCarFastcangeneralize toother m etrics.
Next,weplantoinvestigatetherelationshipbetweenhighc overage
and fault-detection abilities with CarFast. Since there is a body of
researchthatshows astrongcorrelation[51,49, 38,10], we expect
that using CarFast increases the probability of ﬁnding faul ts. Fi-
nally, we plan toimprove the implementation of CarFast to re duce
the total elapsed time. We identiﬁed several bottlenecks fr om our
experiments, i.e., in Dsc’s current dumper mode. With more e n-
gineering on the bottlenecks, we expect CarFast to run faste r and
outperform random techniques with respect to both iteratio n and
elapsed time.
8. ACKNOWLEDGMENTS
We warmly thank the anonymous reviewers for their comments
and suggestions that helped us in improving the quality of th is
paper. This material is based upon work supported by the Na-
tional Science Foundation under Grants No. 0916139, 101763 3,
1217928, 1017305, and 1117369. We also acknowledge the sup-
port of Accenture, since it ﬁnanced the cloud computing expe ri-
ment and provided the internship opportunity for the studen t co-
authors toworkonthis project.
9. REFERENCES
[1] A.Arcuri and L.Briand. Adaptive random testing: Anillu sion of
effectiveness? In ISSTA,pages 265–275, 2011.
[2] A.Arcuri and L.C.Briand. A practical guide for using sta tistical
tests to assess randomized algorithms in software engineer ing. In
ICSE, pages 1–10, 2011.
[3] A.Arcuri, M.Z.Iqbal, and L.Briand. Formalanalysis of t he
effectiveness and predictability of random testing. In ISSTA’10 ,
pages 219–230, 2010.
[4] D.L.Bird and C. U.Munoz. Automatic generation of random
self-checking testcases. IBMSyst. J. ,22:229–245, September 1983.
[5] S.M. Blackburn et al. TheDaCapo benchmarks: Java benchm arking
development and analysis. In Proc.21stOOPSLA ,pages 169–190,
Oct. 2006.
[6] S.M. Blackburn et al. Wake up and smell the coffee: Evalua tion
methodology for the 21stcentury. Commun. ACM ,51(8):83–89,
Aug. 2008.
[7] A.Bron, E.Farchi, Y. Magid, Y. Nir, and S. Ur.Applicatio ns of
synchronization coverage. In Proc.10th ACM SIGPLANSymposium
on Principles and Practice ofParallel Programming (PPoPP) ,pages
206–212. ACM, 2005.[8] É.Bruneton, R.Lenglet, and T.Coupaye. ASM:Acode
manipulation tool to implement adaptable systems.In ACM SIGOPS
France (Adaptable and extensible component systems) , Nov. 2002.
[9] J.Burnim and K. Sen.Heuristics for scalable dynamic tes t
generation. In ASE’08, pages 443–446, 2008.
[10] X.Cai and M.R. Lyu.Theeffect of code coverage on fault d etection
under different testing. In A-MOST,pages 1–7, 2005.
[11] M.-H.Chen, M.R.Lyu, and W.E.Wong.An empirical study o f the
correlation between code coverage and reliability estimat ion. In3rd
IEEEInt. Soft. Metrics Sym. ,pages 133–141, 1996.
[12] T.Y. Chen, R. Merkel, G.Eddy,and P.K.Wong.Adaptive ra ndom
testing through dynamic partitioning. In QSIC ’04 , pages 79–86,
2004.
[13] I. Ciupa, A.Leitner, M.Oriol, and B. Meyer. Artoo: Adap tive
random testing for object-oriented software. In ICSE ’08, pages
71–80, 2008.
[14] L.A.Clarke. Asystemto generate test data and symbolic ally execute
programs. IEEETrans.Softw. Eng. ,2(3):215–222, 1976.
[15] G.A. Cohen, J.S.Chase, and D.L.Kaminsky. Automatic pr ogram
transformation with JOIE.In USENIX AnnualTechnical Symposium ,
June 1998.
[16] M. B.Cohen, P.B. Gibbons, W.B. Mugridge, and C. J.Colbo urn.
Constructing test suites for interaction testing. In ICSE,pages 38–48,
2003.
[17] S.Cohen and B. Kimelfeld. Querying parse trees of stoch astic
context-free grammars. In Proc.13th ICDT ,pages 62–75, Mar. 2010.
[18] S.Cornett. Minimum acceptable code coverage. Bullsey e Testing
Technology, http://www.bullseye.com/minimum.html, 201 1.
[19] R. A.DeMillo and A.J.Offutt. Constraint-based automa tic testdata
generation. IEEETrans. Softw. Eng. ,17(9):900–910, 1991.
[20] B. Dufour, K.Driesen, L.Hendren, and C. Verbrugge. Dyn amic
metrics for Java. In Proc.18th OOPSLA ,pages 149–168, Oct. 2003.
[21] S.Elbaum, A.G. Malishevsky, and G.Rothermel. Testcas e
prioritization: A family of empirical studies. IEEETrans.Softw.
Eng.,28(2):159–182, 2002.
[22] M. A.Fischler and R.C. Bolles. Random sampleconsensus : a
paradigm for modelﬁtting with applications to image analys is and
automated cartography. Commun. ACM , 24(6):381–395, 1981.
[23] S.Fomel and J.F.Claerbout. Guest editors’ introducti on:
Reproducible research. Computing in Science and Engineering ,
11(1):5–7, Jan. 2009.
[24] J.E.Forrester and B. P.Miller. An empirical study of th erobustness
of Windows NTapplications using random testing. In USENIX
Windows Systems Symposium - Volume4 ,2000.
[25] P.Godefroid. Compositional dynamic test generation. InPOPL’07 ,
pages 47–54, 2007.
[26] P.Godefroid, N.Klarlund, and K.Sen. Dart: Directed au tomated
random testing. In PLDI’05 , pages 213–223, 2005.
[27] P.Godefroid, M. Y.Levin, and D.A.Molnar. Automated wh itebox
fuzz testing. In Network Distributed Security Symposium (NDSS) .
Internet Society, 2008.
[28] M. Grindal, J.Offutt, and S.F.Andler. Combination tes ting
strategies: A survey. Softw. Test.,Verif. Reliab. , 15(3):167–199, 2005.
[29] D.Hamlet. When only random testing will do. In RT’06, pages 1–9,
2006.
[30] R. Hamlet. Random testing. In Encyclopedia ofSoftware
Engineering , pages 970–978, 1994.
[31] P.Husbands, C. Iancu, and K.Yelick. Aperformance anal ysis of the
Berkeley UPC compiler. In ICS ’03,pages 63–73, 2003.
[32] I. Hussain, C.Csallner, M. Grechanik, C. Fu,Q.Xie, S.P ark,
K.Taneja, and B.M.M. Hossain. Evaluating program analysis and
testing tools with theRUGRAT random benchmark application
generator. In WODA,July 2012.
[33] M. Islam and C. Csallner. Dsc+mock: A testcase +mock cla ss
generator in support of coding against interfaces. In WODA,pages
26–31, July 2010.
[34] A.Joshi, L.Eeckhout, R. H.Bell, Jr.,and L.K.John. Dis tilling theessence ofproprietary workloads into miniature benchmark s.ACM
TACO,5(2):10:1–10:33, Sept. 2008.
[35] C. Kaner. Software negligence & testing coverage. In STAR’96 ,
1996.
[36] C. Kaner, J.Bach, and B.Pettichord. Lessons Learned in Software
Testing. Wiley, Dec. 2001.
[37] K.Kanoun and L.Spainhower. Dependability Benchmarking for
Computer Systems . Wiley, July 2008.
[38] Y. W.Kim.Efﬁcient use of code coverage in large-scale s oftware
development. In CASCON ’03 , pages 145–155, 2003.
[39] K.Koster and D.C. Kao. State coverage: Astructural tes t adequacy
criterion for behavior checking. In Proc. 15th FSE,Companion
Papers, pages 541–544. ACM, Sept. 2007.
[40] R. Kuhn,Y. Lei, and R. Kacker. Practical combinatorial testing:
Beyond pairwise. ITProfessional , 10(3):19–23, 2008.
[41] C. Liand C.Csallner. Dynamic symbolic database applic ation
testing. In Proc. 3rdDBTest ,June 2010.
[42] M.Maierhofer and M.A. Ertl.Local stack allocation. In Proc. 7th
International Conference on Compiler Construction (CC) , pages
189–203. Springer, Apr. 1998.
[43] R. Majumdar and K. Sen.Hybrid concolic testing. In ICSE ’07,
pages 416–426, 2007.
[44] Y. K.Malaiya, M.N.Li,J.M. Bieman, and R. Karcich. Soft ware
reliability growth with test coverage. IEEETrans. on Reliability ,
51:420–426, 2002.
[45] B. Marick. How to misusecode coverage. In Proc.of the16th Intl.
Conf. on Testing Comp.Soft. , pages 16–18, 1999.
[46] G.McDaniel. IBM Dictionary ofComputing . Dec. 1994.
[47] B. Meyer. Testing insights. Bertrand Meyer’s technology blog ,
http://bertrandmeyer.com/2011/07/11/testing-insight s, 2011.
[48] S.S.Muchnick. Advanced compiler design and implementation .
Morgan Kaufmann, 1997.
[49] A.S.Namin and J.H. Andrews. Theinﬂuence of size and cov erage
on test suite effectiveness. In ISSTA’09 , pages 57–68, 2009.
[50] A.S.Namin and S.Kakarla. Theuseof mutation in testing
experiments and its sensitivity to external threats. In ISSTA,pages
342–352, 2011.
[51] P.Piwowarski, M.Ohba, and J.Caruso. Coverage measure ment
experience during function test. In ICSE,pages 287–301, May 1993.
[52] P.Runeson. Asurvey of unit testing practices. IEEESoftw. ,
23:22–29, July 2006.
[53] R. H.Saavedra and A.J.Smith. Analysis of benchmark
characteristics and benchmark performance prediction. ACMTrans.
Comput. Syst. ,14(4):344–384, Nov. 1996.
[54] Sable Reserch Group. Soot: Ajava optimization framewo rk.
http://www.sable.mcgill.ca/soot/ .
[55] M.Schwab, M.Karrenbach, and J.Claerbout. Making scie ntiﬁc
computations reproducible. Computing in Science and Engineering ,
2(6):61–67, Nov. 2000.
[56] R. M.Sirkin. Statistics for the Social Sciences . Sage Publications,
third edition, Aug.2005.
[57] D.R. Slutz. Massive stochastic testing of SQL.In VLDB’98 , pages
618–622, 1998.
[58] R. Torkar and S.Mankefors. Asurvey on testing and reuse . InProc.
IEEEInternational Conference on Software - Science, Techn ology &
Engineering . IEEE,2003.
[59] Y. L.Traon, T.Mouelhi, and B.Baudry. Testing security policies:
Going beyond functional testing. In ISSRE,pages 93–102, 2007.
[60] S.Ur and A. Ziv.Off-the-shelf vs.custom made coverage models,
which is the onefor you? In STAR’98 , May 1998.
[61] T.Xie, N.Tillmann, P.de Halleux, and W.Schulte. Fitne ss-guided
path exploration in dynamic symbolic execution. In DSN, pages
359–368. IEEE,June2009.
[62] Q.Yang, J.J.Li,and D.Weiss.Asurvey of coverage based testing
tools. InProc. International Workshop on Automation ofSoftware
Test(AST) ,pages 99–103. ACM, 2006.
[63] H.Zhu,P.A.V. Hall, and J.H.R. May.Software unit test c overage
and adequacy. ACM Comput. Surv. ,29(4):366–427, 1997.