AR-Miner: Mining Informative Reviews for Developers from
Mobile App Marketplace
Ning Chen, Jialiu Liny, Steven C. H. Hoi, Xiaokui Xiao, Boshen Zhang
Nanyang Technological University, Singapore,yCarnegie Mellon University, USA
{nchen1,chhoi,xkxiao,bszhang}@ntu.edu.sg,yjialiul@cs.cmu.edu
ABSTRACT
With the popularity of smartphones and mobile devices, mo-
bile application (a.k.a. \app") markets have been growing
exponentially in terms of number of users and download-
s. App developers spend considerable eort on collecting
and exploiting user feedback to improve user satisfaction,
but suer from the absence of eective user review ana-
lytics tools. To facilitate mobile app developers discover
the most \informative" user reviews from a large and rapid-
ly increasing pool of user reviews, we present \AR-Miner"
| a novel computational framework for App Review Min-
ing, which performs comprehensive analytics from raw user
reviews by (i) rst extracting informative user reviews by
ltering noisy and irrelevant ones, (ii) then grouping the in-
formative reviews automatically using topic modeling, (iii)
further prioritizing the informative reviews by an eective
review ranking scheme, (iv) and nally presenting the group-
s of most \informative" reviews via an intuitive visualization
approach. We conduct extensive experiments and case s-
tudies on four popular Android apps to evaluate AR-Miner,
from which the encouraging results indicate that AR-Miner
is eective, ecient and promising for app developers.
Categories and Subject Descriptors
D.2 [Software ]: Software Engineering; H.4 [ Information
Systems Applications ]: Miscellaneous
General Terms
Algorithm and Experimentation
Keywords
User feedback, mobile application, user reviews, data mining
1. INTRODUCTION
The proliferation of smartphones attracts more and more
software developers to devote to building mobile applica-
tions (\apps"). As the market competition is becoming more
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proÔ¨Åt or commercial advantage and that copies
bear this notice and the full citation on the Ô¨Årst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciÔ¨Åc
permission and/or a fee.
ICSE ‚Äô14, May 31-June 7, 2014, Hyderabad, India
Copyright 2014 ACM 978-1-4503-2756-5/14/05 ...$15.00.intense, in order to seize the initiative, developers tend to
employ an iterative process to develop, test, and improve
apps [23]. Therefore, timely and constructive feedback from
users becomes extremely crucial for developers to x bugs,
implement new features, and improve user experience ag-
ilely. One key challenge to many app developers is how to
obtain and digest user feedback in an eective and ecient
manner, i.e., the \user feedback extraction" task. One way
to extract user feedback is to adopt typical channels used
in traditional software development, such as (i) bug/change
repositories (e.g., Bugzilla [3]), (ii) crash reporting systems
[19], (iii) online forums (e.g., SwiftKey feedback forum [6]),
and (iv) emails [10].
Unlike the traditional channels, modern app marketplaces,
such as Apple App Store and Google Play, oer a much eas-
ier way (i.e., the web-based market portal and the market
app) for users to rate and post app reviews. These reviews
present user feedback on various aspects of apps (such as
functionality, quality, performance, etc), and provide app
developers a new and critical channel to extract user feed-
back. However, comparing with traditional channels, there
are two outstanding obstacles for app developers to obtain
valuable information from this new channel. First of all, the
proportion of \informative" user reviews is relatively low. In
our study (see Section 5.1), we found that only 35.1% of
app reviews contain information that can directly help de-
velopers improve their apps. Second, for some popular apps,
the volume of user reviews is simply too large to do manu-
al checking on all of them. For example, Facebook app on
Google Play receives more than 2000 user reviews per day,
making it extremely time consuming to do manual checking.
To our best knowledge, very few studies were focused on
extracting valuable information for developers from user re-
views in app marketplace [28, 21, 22]. This paper formal-
ly formulates this as a new research problem. Specically,
to address this challenging problem and tackle the afore-
mentioned two obstacles, we propose a novel computation-
al framework, named \AR-Miner" (App Review Miner), for
extracting valuable information from raw user review da-
ta with minimal human eorts by exploring eective data
mining and ranking techniques. Generally speaking, giv-
en a bunch of user reviews of an app collected during a
certain time interval, AR-Miner rst lters out those \non-
informative" ones by applying a pre-trained classier. The
remaining \informative" reviews are then put into several
groups, and prioritized by our proposed novel ranking mod-
el. Finally, we visualize the ranking results in a concise andPermission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proÔ¨Åt or commercial advantage and that copies bear this notice and the full citation
on the Ô¨Årst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speciÔ¨Åc permission and/or a
fee. Request permissions from Permissions@acm.org.
ICSE‚Äô14 , May 31 ‚Äì June 7, 2014, Hyderabad, India
Copyright 2014 ACM 978-1-4503-2756-5/14/05...$15.00
http://dx.doi.org/10.1145/2568225.2568263
767
intuitive way to help app developers spot the key feedback
users have.
We validate the ecacy of AR-Miner by conducting an ex-
tensive set of experiments and case studies on user reviews of
four Android apps released in Google Play. In particular, we
compare the ranking results generated by AR-Miner against
real app developers' decisions, and also analyze the advan-
tages of AR-Miner over manual inspection and facilities used
in a traditional channel (i.e., online forum). Our empirical
results have validated the eectiveness and eciency of AR-
Miner in helping apps developers. Supplementary materials
(including datasets, additional results, etc.) are publicly
available at1. In short, this paper makes the following main
contributions:
We formulate a new problem that aims to discover the
most \informative" information from raw user reviews
in app markets for developers to improve their apps;
We present AR-Miner as a novel analytic framework to
tackle this new problem, which includes a new, exible
and eective ranking scheme to prioritize the\informa-
tive" reviews;
We evaluate AR-Miner based on user reviews of four
popular Android apps, in which empirical results show
that AR-Miner is eective and promising.
The rest of the paper is organized as follows: Section 2
discusses related work; Section 3 gives the problem state-
ment; Section 4 presents the AR-Miner framework; Section
5 gives empirical results; Section 6 discusses limitations and
threats to validity; nally Section 7 concludes this work.
2. RELATED WORK
We group related work into three major categories, and
survey the literature of each category in detail below.
2.1 App Marketplace Analysis
With the rocketing development of mobile applications,
app marketplace has drawn much more attention among re-
searchers within and outside the software engineering com-
munity. In [24], Harman et al. pointed out that, app mar-
ketplace is a new form of software repository and very dier-
ent from traditional ones. They also analyzed the technical,
customer and business aspects of some apps in BlackBer-
ry World. In [33], Minelli et al. proposed to combine data
extracted from app marketplace with source code to compre-
hend apps in depth. Linares-V asquez et al. [31] empirically
analyzed how the stability and fault-proneness of APIs used
by some free Android apps relate to apps' lack of success.
Chia et al. [14] analyzed the relationship between permis-
sions and community ratings of some apps. Our work is dif-
ferent from the aforementioned studies, mainly because we
explore dierent data in app marketplace, i.e., user reviews,
and formulate a very dierent problem.
Thus far, there has been little work on mining user reviews
in app marketplace. In [36], Pagano and Maalej conducted
an exploratory study to analyze the user reviews crawled
from Apple App Store. They studied (i) the usage and im-
pact of feedback through statistical analysis, and (ii) the
content of feedback via manual content analysis and fre-
quent itemset mining. Chandy et al. [13] presented a latent
1https://sites.google.com/site/appsuserreviews/model to detect \bogus" user reviews in Apple App Store.
Iacob et al. [28] developed a prototype named MARA that
uses a list of linguistic rules to automatically retrieve feature
requests from online user reviews. Although the nature of
data studied in the above three works is similar to ours, the
techniques used or research goals are totally dierent.
To our best knowledge, there are only two previous stud-
ies that are closely related to our work. Galvis Carre~ no et
al. [22] adapted the Aspect and Sentiment Unication Mod-
el (ASUM) proposed in [30] to automatically extract topics
for requirements changes. General speaking, our work dif-
fers from their work in three major aspects. First, our work
aims to discover not only requirement changes, but also oth-
er kinds of valuable information for developers (see Figure
1 in Section 3). Second, we focus on the ranking scheme of
\informative" user reviews, which is not addressed in their
work. Finally, our scheme consisting of an eective ltering
step considerably outperforms the direct application of topic
models in solving our problem (see more details in Section
5.3.1). Fu et al. [21] presented a system named Wiscom to
analyze user reviews in app marketplace at three dierent
levels. The so-called \Meso Level" of Wiscom is to uncover
the reasons of user dissatisfaction and their evolution over
time for an individual app, which is more related to AR-
Miner; however, it suers from two major limitations. First,
it cannot discover app-specic topics by using Latent Dirich-
let Allocation (LDA) [11], since it links all the user reviews
from the same app together as a document. Second, it only
considers negative reviews, thus missing topics with positive
ratings. Unlike Wiscom , AR-Miner can address both limi-
tations. Moreover, we propose a new ranking model to help
app developers prioritize the \informative" user reviews.
2.2 Mining User Reviews on the Web
Our work is related to studies that focus on mining and
analyzing user reviews in other kinds of marketplaces (e.g.,
movies, commodity goods, etc.) and social webs (e.g., news
sites). However, their techniques cannot be directly applied
to our problem in that (1) the objective of our problem is
dierent, i.e., solving a software engineering problem in re-
quirement/bug management; and (2) the characteristics of
user reviews in apps stores are quite dierent, e.g., review
styles [21], volumes, etc. Next, we discuss three most related
classes of work and explain specic dierences.
In the literature, there is a great deal of work that applies
sentiment analysis to user reviews in dierent marketplaces
[37]. In general, these studies aim to determine the semantic
orientation of a given user review at the document [40], sen-
tence [27] or feature level [20], whether the opinion expressed
is positive or negative. There are several studies focusing on
summarizing/visualizing user reviews via identifying prod-
uct features and opinions [42, 7]. Compared with the afore-
mentioned work, our work diers in that (i) we classify each
user review into either \informative" or \non-informative"
from the software engineering perspective, instead of \posi-
tive" or \negative" from the emotional perspective; (ii) the
ultimate goal of our work is dierent, that is visualizing the
ranking results of \informative" information.
Much work has focused on spam review ltering [41, 34,
29]. They aim to protect users and honest vendors via de-
tecting and removing bogus user reviews. Our work diers
from them mainly in two points. First, in our work, the
denition of \non-informative" 6= \spam" (see Section 3).768 Class  Type  (Rule)  Real Example  
Informative Functional flaw that produces incorrect or unexpected  result None of the pictures will load in my news feed. 
Performance flaw that degrades the performance of Apps It lags and doesn't respond to my touch which almost always causes me 
to run into stuff. 
Requests to add/ modify features Amazing app, although I wish there were more themes t o choose from. 
Please make it a little easy to get bananas please and mak e more 
power ups that would be awesome. 
Requests to remove advertisements/notifications So many ads its unplayable! 
Requests to remove permissions This game is adding for too much unexplained permiss ions. 
Non-
informative Pure u ser emotional expression Great fun can't put it down! 
This is a crap app. 
Descriptions of (apps, features, actions, etc.) I have changed my review from 2 star to 1 star. 
Too general/unclear expression of failures and reque sts Bad game this is not working on my phone. 
Questions and inquiries How can I get more points? Figure 1: Dierent Types of Informative and Non-informative Information for App Developers
Second, although the ltering step in AR-Miner can help
remove some types of spam reviews, our major objective is
to rank the \informative" user reviews for app developers.
There also exist several pieces of work on ranking reviews
on the social web. For example, Hsu et al. [26] applied Sup-
port Vector Regression to rank the reviews of a popular news
aggregator Digg. Dalal et al. [18] explored multi-aspects
ranking of reviews of news articles using Hodge decomposi-
tion. Dierent from both works, our work aims to rank the
reviews according to their importance (not quality) to ap-
p developers (not users) from the software engineering per-
spective. Besides, we propose a completely dierent ranking
model in solving our problem.
2.3 Mining Data in Traditional Channels
Our work is also related to studies that apply data mining
(machine learning) techniques on data stored in traditional
channels to support developers with the \user feedback ex-
traction"task. Specically, the rst category of related work
in this eld is to address problems in bug repositories [38,
9, 8, 25]. For example, Sun et al. [38] proposed a discrim-
inative approach to detect duplicate bug reports. Anvik et
al. [9] compared several classication algorithms for solving
the bug assignment problem. Antoniol et al. [8] developed
a machine learning approach to distinguish bugs from non-
bugs. In addition, another category of related work is to
solve problems in other traditional channels (e.g., request
repositories [16, 15], emails [10], crash reporting systems
[19]). For example, Cleland-Huang et al. [15] proposed a
machine learning approach to categorize product-level re-
quirements into pre-dened regulatory codes. Dang et al.
[19] developed an approach based on similarity measures to
cluster crash reports. Bacchelli et al. [10] applied a Naive
Bayes classier to classify email contents at the line-level.
Compared with the previous studies in this area, our work
diers in that we formulate and solve a brand new problem
in a new channel with its distinct features.
3. THE PROBLEM STATEMENT
The \user feedback extraction" task is extremely impor-
tant in bug/requirement engineering. In this paper, we for-
mally formulate it as a new research problem, which aims
to facilitate app developers to nd the most \informative"
information from large and rapidly increasing pool of raw
user reviews in app marketplace.
Consider an individual app, in a time interval T, it re-
ceives a list of user reviews Rwith an attribute set A=fA1;A2;:::;A kg, andri=fri:A1;ri:A2;:::;r i:Akgis the
i-th review instance in R. Without loss of generality, in
this work, we choose A=fText;Rating;Timestamp g, s-
ince these are the common attributes supported in all main-
stream app marketplaces. Table 1 shows an example of R
withtreview instances. In particular, we set the Text at-
tribute ofriat the sentence level. We will explain how to
achieve and why we use this ner granularity in Section 4.2.
Table 1: Example of A List of User Reviews R, R
= Rating, TS = Timestamp
ID Text R TS
r1Nice application, but lacks some important4 Dec 09
features like support to move on SD card.
r2 So, I am not giving ve star rating. 4 Dec 09
r3 Can't change cover picture. 3 Jan 18
r4I can't view some cover pictures even mine. 2 Jan 10
r5 Wish it'd go on my SD card. 5 Dec 15
. . . . . . . . . . . .
rt . . . . . . . . .
In our problem, each riinRis either \ informative "
or \non-informative ". Generally, \informative" implies ri
contains information that app developers are looking to i-
dentify and is potentially useful for improving the quality
or user experience of apps. We summarize dierent types
of \informative" as well as \non-informative" information in
Figure 1 (one or two examples for each type). For example,
r1,r3,r4andr5shown in Table 1 are \informative", since
they report either bugs or feature requests, while r2is \non-
informative", as it is a description of some user action, and
developers cannot get constructive information from it.
Remark. The summarization shown in Figure 1 is not ab-
solutely correct, since the authors are not app developers.
In fact, even for real app developers, no two people would
have the exact same understanding of \informative". This
is an internal threat of validity in our work. To alleviate
this threat, we rst studied some online forums (e.g., [6]) to
identify what kinds of information do real app developers
consider as constructive, and then derived the summariza-
tion shown in Figure 1 based on the ndings.
Generally, given a list of user reviews Rof an app(e.g.,
the one shown in Table 1), the goal of our problem is to
lter out those \non-informative" reviews (e.g., r2), then (i)769group the remaining reviews based on the topics they are
talking about, e.g., fr1;r5gare grouped because they both
talk about feature request related to\SD card"; and (ii) iden-
tify the relative importance of dierent groups and reviews
in the same group (e.g., the relative importance of r1and
r5), and nally present an intuitive visualized summariza-
tion to app developers.
4. OUR FRAMEWORK
In this section, we rst give an overview of our proposed
AR-Miner framework to address the problem stated in Sec-
tion 3, and then present each step of our framework in detail.
4.1 Overview
Figure 2 presents an overview of AR-Miner, which con-
sists of ve major steps. The rst step preprocesses the
raw user review data into well-structured format to facili-
tate subsequent tasks (Section 4.2). The second step ap-
plies a pre-trained classier to lter out \non-informative"
reviews inR(Section 4.3). The third step groups the re-
maining \informative" reviews in such a way that reviews in
the same group are more semantically similar (Section 4.4).
The fourth step (the focus of this paper) sorts (i) groups,
and (ii) reviews in each group according to their level of im-
portance by using our novel ranking model (Section 4.5). In
the last step, we visualize the ranking results and present an
intuitive summary to app developers (Section 4.6).
Preprocess
-ing
Filtering
VisualizationGrouping
RankingUserReviews
Report1 2
4 53
Figure 2: Overview of the AR-Miner framework.
We focus on tackling the \Ranking" Step.
4.2 Preprocessing
The rst step of AR-Miner preprocesses the collected raw
data by (i) converting the raw user reviews into sentence-
level review instances, and then (ii) preprocessing the Text
attribute of the review instances.
The format of raw user reviews varies with dierent app
marketplaces. As mentioned in Section 3, in this work, we
chooseA=fText;Rating;Timestamp g. Figure 3 shows a
real example of a raw user review that contains these three
attributes. The Text attribute of a raw user review often
consists of more than one sentence. In this work, we split
Text into several sentences via a standard sentence splitter
provided by LingPipe [4]. For each sentence, we generate
a review instance riwithRating andTimestamp equal to
the values of the corresponding raw user review. For ex-
ample, the raw user review shown in Figure 3 is converted
into two sentence-level review instances shown in Table 1 ( r1
andr2). We choose the sentence-level granularity because
within a raw user review some sentences can be "informa-
tive"(e.g., sentence 1 shown in Figure 3) and some sentences
are not (e.g., sentence 2). Thus, this ner granularity can
help distinguish \informative" with \non-informative" infor-
mation more accurately.
Dec09,2012
Niceapplication,butlackssomeimportantfeatureslike
supporttomoveonSDcard1.So,Iamnotgivingfive
starrating2.
 
Figure 3: An Example Raw User Review
Further, we preprocess the Text attribute of review in-
stances. We rst tokenize the text, and then remove all
non-alpha-numeric symbols, convert words to lowercase and
eliminate extra whitespace along with stop words/rare word-
s. Next, the remaining words are stemmed to their root for-
m. Finally, we remove review instances that become empty
as a result of the above processing.
4.3 Filtering
The preprocessing step generates a review database R
(e.g., as shown in Table 1). In this step, our goal is to
train some classier that can automatically lter out \non-
informative" reviews from R.
First, we introduce the class label set used in our problem.
As described in Section 3, we have two unique class labels
finformative, non-informative g, where \informative" implies
that the review is constructive/helpful to app developers,
and \non-informative" means that the review contains no
information that is useful for improving apps. We use the
rules (types) summarized in Figure 1 to assign class labels
to review instances. In particular, we solve some ambiguous
cases. For example, we classify \too general/unclear expres-
sion of failures and requests" as \non-informative" (e.g., \It
doesn't work", \It needs more update", and etc.).
To eliminate \non-informative" review instances, we need
to apply a machine learning algorithm to build some classi-
er on the historical training data. In this work, we simply
adopt a well-known and representative semi-supervised al-
gorithm in machine learning, i.e., E xpectation M aximization
for N aive B ayes (EMNB) [35]. The most important reason
we choose EMNB is that it suits our problem well. In our
problem, we can get a mass of unlabeled data almost freely,
but labeling training data is time consuming and labor in-
tensive. Compared with supervised algorithms, EMNB can
use a small amount of labeled data (thus less human eort)
along with plenty of unlabeled data to train a fairly good
classier (see our comparisons in Section 5.5.1). Besides,
NB often outperforms other machine learning algorithms in
text classication [12] and has been widely used in other soft-
ware engineering problems [10, 39]. Finally, NB provides a
nice posterior probability for the predicated class, which is
useful in the ranking step (See Section 4.5.3).
Once the classier is built, it can be applied to lter future
unlabeled user reviews. Table 2 shows a possible good result
(denoted asR,nt) after lteringRshown in Table
1, where \non-informative" review instances are eliminated
(r2), and\informative"ones are preserved ( r1,r3,r4andr5).
The last column \P" of Table 2 indicates the probability of
the review instance belongs to the \informative" class.
4.4 Grouping
This step is to partition the remaining review instances
(R) into several groups such that the Text of review in-
stances in a group is more semantically similar to each other
than theText of review instances in other groups.770Table 2:R, A Possible Good Result after Filtering,
R = Rating, TS = Timestamp, P = Probability
ID Text R TS P
r1Nice application, but lacks
4 Dec 09 0.8 some important features like
support to move on SD card.
r3 Can't change cover picture. 3 Jan 18 0.9
r4I can't view some cover2 Jan 10 0.9
pictures even mine.
r5Wish it'd go on my SD card. 5 Dec 15 0.9
. . . . . . . . . . . .
rn . . . . . . . . .
In general, there are two categories of unsupervised tech-
niques that can be applied to the grouping task. The rst
category is clustering (e.g., K-means [32]), which assumes
that each review instance belongs to exactly one single
cluster (group). However, this assumption may become prob-
lematic for review instances (even at the sentence level) that
exhibit multiple topics (groups) to dierent degrees. As a
result, we adopt another category of techniques: topic mod-
eling which assigns multiple topics to each review instance.
For example, the review \Just add emojis and more themes."
is modeled as a distribution over two topics (50% \emoji",
50% \theme"). We will discuss the comparison of two al-
gorithms in topic modeling , i.e., Latent Dirichlet Allocation
(LDA) [11] and Aspect and Sentiment Unication Model
(ASUM) [30] (adopted in [22]) in our experiments. In the
future, we will explore and compare more topic models.
4.5 Ranking
Given the grouping results, we aim to determine the rel-
ative importance of (i) groups; and (ii) review instances in
each group. To fulll this purpose, we propose a novel rank-
ing model presented as follows.
4.5.1 The Overview of Our Ranking Model
The general form of our ranking model is shown in Algo-
rithm 1. The inputs include (i) a set of groups (topics) G
generated by the grouping step; (ii) two sets of functions fG
(see Section 4.5.2) and fI(see Section 4.5.3) that measure
the importance of various features of groups (e.g., volume)
and review instances (e.g., rating), respectively; and (iii) two
weight vectors wG(wG
i2[0;1],Pm
i=1wG
i= 1) and wI(wI
i2
[0;1],Pn
i=1wI
i= 1) correspond to fGandfI, respective-
ly. Algorithm 1 computes (i) the GroupScore (g)2[0;1] for
each groupg2G(Line 1-3), and (ii) the InstanceScore (r)2
[0;1] for each review instance r2g(Line 4-6). Larger
GroupScore (g) andInstanceScore (r) indicate higher im-
portance. Finally, Algorithm 1 outputs the ranking results.
Our ranking model is exible, since we can obtain ranking
results from dierent angles by adjusting the weight vectors
ofwGandwI(See Section 5.5.2). We also claim that our
ranking model is extensible, because it can easily incorporate
more features (See Section 6 for discussion).
4.5.2 Group Ranking
To measure the importance of dierent groups, we use
fG=ffG
V olume;fG
TimeSeries;fG
AvgRatinggin this work. Next,Algorithm 1: The Ranking Model
Input : A set of groups G, feature function sets
fG=ffG
1;:::;fG
mgandfI=ffI
1;:::;fI
ng,
weight vectors wG= (wG
1;:::;wG
m) and
wI= (wI
1;:::;wI
n)
1 for each group g2Gdo
2 ComputefG
1(g), . . . ,fG
m(g)
3 SetGroupScore (g) =Pm
i=1(wG
ifG
i(g))
4 for each review instance r2gdo
5 ComputefI
1(r), . . . ,fI
n(r)
6 SetInstanceScore (r) =Pn
j=1(wI
jfI
j(r))
7 end
8 end
Output : Groups in decreasing order of GroupScore ;
review instances in each group in decreasing
order ofInstanceScore
Table 3: Per-review Distribution over Groups
(a) Review-Group Matrix
g1gm
r1pr1g1pr1gm
............
rnprng1prngm(b) An Example
g1g2g3
r11.0 0.0 0.0
r21.0 0.0 0.0
r30.5 0.5 0.0
r40.0 0.5 0.5
we describe each feature function in detail.
Volume: Given the remaining nreview instances (after
ltering)R=fr1;:::;r ng, in the grouping phase, we au-
tomatically discover mgroups (topics), denoted as G=
fg1;:::;g mg. As described in Section 4.4, each review in-
stanceri(1in) is modeled as a distribution over G.
The matrix shown in Table 3(a) presents such distributions,
where each entry prigj(1in,1jm) represents the
proportion that review instance riexhibits group gj, and
for eachri,Pm
j=1prigj= 1. For example, in Table 3(b), r4
exhibitsg2with 50% and g3with 50%. The volume of a
groupgis dened as follows,
fG
V olume (g) =nX
i=1prig (1)
For example, in Table 3(b), fG
V olume (g1) = 1+1+0 :5+0 =
2:5. One group with larger volume indicates it is more im-
portant. The reason is that a larger group is more likely
to be a class of common bugs/requests reected by many
users, while a smaller group is more likely to be (i) a kind of
particular bug/request reported by only a few users or (ii)
a few users' wrong/careless operations.
Time Series Pattern: Given the time interval T= [t0;t0+
T] under investigation, we divide TintoK=T=tconsecu-
tive time windows, with each has length of  t. LetTkdenote
thek-th time window, thus Tk= [t0+ (k 1)t;t0+kt],
where 1kK. For eachri2R, we denote ri:TSas the
timestamp when riis posted,t0ri:TSt0+Tfor all
1in. Given a time window Tk, we denote the total
number of review instances posted during it as follows,
v(Tk) =jRTkj=jfri:ri:TS2Tkgj; n =KX
k=1v(Tk)771wherejMjdenotes the cardinality of the set M. For a group
g, we count the volume of review instances posted during
the time window Tk, formally,
v(g;Tk) =X
rj2RTkprjg
Then, we can construct a time series for the group g, repre-
sented by,
Pg(T;t) = [p(g;1);::::::;p (g;K)]g
wherep(g;k) is short for p(g;Tk), andp(g;k) =v(g;Tk)=v(Tk).
Figure 4 shows four typical time series patterns. The pat-
ternP1shown in Figure 4(a) has a rapid rise at a certain
time window (Tk) followed by a small decline then towards
plateau. One group that has this kind of pattern is likely to
be a class of newly introduced bug/request due to some event
happened atTk(e.g., version update, network/server error,
and etc.). In addition, this problem is not solved at the end
ofT.P2shown in Figure 4(b) presents a quick decay at a
certain time window ( Tk). This demonstrates the scenario
where an old bug/request is xed/satised at Tk.P3shown
in Figure 4(c) uctuates slightly within a range over the en-
tireT. This indicates the scenario of an existing bug/request
introduced earlier than t0that is not xed/satised during
T.P4shown in Figure 4(d) implies that the problem is in-
troduced earlier than t0that is relieved (but not addressed)
duringT. Obviously, groups with pattern P1are the most
important (fresher), while groups of pattern P2are the least
important (older). To model the importance of time series
pattern for a group g, we compute fG
TimeSeries (g) by,
fG
TimeSeries (g) =KX
k=1p(g;k)
p(g)l(k) (2)
wherep(g) =PK
k=1p(g;k), andl(k) is a monotonically in-
creasing function of k(the index ofTk), since we aim to set
a higher weight to later Tk. The choice of l(k) depends on
the importance of the freshness, in this work, we simply set
l(k) =k.
Average Rating: We denote ri:Ras the user rating of
ri, in this work, ri:R2 f1;2;3;4;5gfor all 1in.
\Informative" reviews with lower ratings (e.g., 1, 2) tend
to express users' strong dissatisfaction with certain aspects
of apps that need to be addressed immediately (e.g., crit-
ical bugs), thus are more important. On the other hand,
\informative" reviews with higher ratings (e.g., 4, 5) often
describe some kind of users' non-urgent requirements (e.g.,
feature improvement), thus are less important. Therefore,
we measure the rating aspect of a group gby
fG
AvgRating (g) =fG
V olume (g)Pn
i=1prigri:R(3)
which is the inverted average rating of a group g. Larger
fG
AvgRating (g) indicates more importance.
4.5.3 Instance Ranking
Regarding the importance of review instances in a particu-
lar groupg, in this work, we use fI=ffI
Proportion;fI
Duplicates;
fI
Probability;fI
Rating;fI
Timestampg. Next, we describe each fea-
ture function in detail.
(a) Pattern P1
 (b) Pattern P2
(c) Pattern P3
 (d) Pattern P4
Figure 4: Representative Time Series Patterns for
Groups
Proportion: For a group g, eachri2 R (1in)
exhibitgwith a certain proportion prig.prigequals to 1
meansrionly exhibits g, whileprigequals to 0 indicates ri
does not exhibit gat all.riwith larger prigvalue is more
important in g, since it contains more core content of this
group. Formally,
fI
Proportion (r;g) =prg (4)
In this work, we denote Rgas the reviews instances belong
to a groupg.Rgis constructed by eliminating those riwith
prig<inR(we set= 0:01), thusRg=fri:prigg.
For example, in Table 3(b), Rg1=fr1;r2;r3g,r4is ignored
sincepr4g1= 0:0<0:01.
Duplicates: For a group gofRg=fr1;:::;r ngg, we de-
noter:Text as the text of r(represented in the vector s-
pace). It is common that dierent texts of review instances
refer to the same \informative" information. We intend to
remove those duplicates from Rgand form a set of unique re-
view instancesRu
g=fru
1;:::;ru
n0gg, wheren0
gng. Specif-
ically, for each unique review instance ru
i2Ru
g,rj2R g
is considered as a duplicate of ru
iif and only if satisfying
sim(rj:Text;ru
i:Text ), wheresimis a certain similari-
ty metric (e.g., Jaccard similarity used in our work), and is
a predened threshold. We count the number of duplicates
for eachru
i2Ru
g, denoted as duplicates (ru
i;g). The more
duplicatesru
ihas, the more important it is in g. Formally,
fI
Duplicates (r;g) =duplicates (r;g) (5)
whereris a shorthand for ru
i.
Note that, for a group g, we quantify the importance of
every unique review instance r2Ru
g. Forrthat has more
than one duplicate, the rating ofris set as the minimum rat-
ing value of duplicates, and the proportion, probability and
timestamp ofrare set as the maximum values of duplicates.
The features in italics will be introduced below shortly.
Probability: As mentioned in Section 4.3, one of the rea-
sons we choose EMNB as our classier is that it can provide772a posterior probability for each predicated review instance
(denoted as r:P). Intuitively, the larger probability of r
demonstrates that it is more likely to be an \informative"
review, thus more important. Formally,
fI
Probability (r) =r:P (6)
Rating: Similar to the average rating of a group, lower
rating ofrindicates it is more important, thus,
fI
Rating (r) =1
r:R(7)
Timestamp: Similar to the time series pattern of a group,
more fresher of rindicates it is more important, thus,
fI
Timestamp (r) =k (8)
wherekis the index ofTkthat satises r:TS2Tk.
4.6 Visualization
The last step of AR-Miner is to visualize the results gener-
ated by our ranking model. Figure 5 shows a possible visu-
alization of top-10 ranked results (see details in Section 5.4).
The bottom half of Figure 5 is a radar chart which depicts
theGroupScore value of each group (topic) along a separate
axis which starts in the center of the chart and ends on the
outer ring. Each group is labeled by two (or three) top prob-
ability (also descriptive) words within the group, and a data
point near the outer ring indicates a higher GroupScore val-
ue. Intuitively, the group \more theme", which is requesting
for more themes into the app, has the highest GroupScore ,
and theGroupScore s of the remaining groups decrease in
the clockwise direction. To get insight into a group, we
can click its label to view the reviews instances (the de-
tailed information) within the group in decreasing order of
InstanceScore . For example, the top half of Figure 5 shows
the top 2 review instances in the \more theme" group. Due
to space limitation, we present the complete version of Fig-
ure 5 on our website.
5. EMPIRICAL EVALUATION
To evaluate if AR-Miner can really help app developers,
we conduct several experiments and case studies. Speci-
cally, we aim to answer the following questions: (1) What
is the topic discovering performance of our scheme? (2) If
the top-ranked topics generated by AR-Miner represent the
most \informative" user feedback for real app developer-
s? (3) What are the advantages of AR-Miner over purely
manual inspection and facilities used in traditional channels
(e.g., online forum)?
5.1 Dataset
We select 4 Android apps from Google Play, i.e., SwiftKey
Keyboard (smart touchscreen keyboard), Facebook (social
app), Temple Run 2 (parkour game), and Tap Fish (casual
game), as subject apps in our experiments and case studies.
These apps are selected because (i) they cover dierent app
domains; and (ii) they range from large datasets (Facebook
and Temple Run 2) to relatively small datasets (SwiftKey
Keyboard and Tap Fish). We collected the raw user re-
views of these apps from Google Play roughly in the period
from Oct, 2012 to Feb, 2013. Table 4 lists some key fea-
tures of these datasets (after converting to sentence level).
For each dataset, we divide it into two partitions, where re-
views in partition (i) appear before those of partition (ii)

00.2 0.4 0.6 0.8 1more theme [1] 
word type 
restart default 
keyboard [10] 
word predict 
support Chinese 
langauge [6,8] 
predict text [7] like keyboard more option space bar[3] swype feature [2] Review Instance s of topic ‚Äú more theme ‚Äù Score  
1 Also we need more themes! 0.932 
2 Just wish you had more themes or ability to make a 
custom theme. 0.800 
‚Ä¶ ‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶..  ‚Ä¶..  Figure 5: Visualization of Top-10 ranked results
achieved by AR-Miner (SwiftKey). The number in
the square bracket denotes the corresponding rank
in Figure 6.
in terms of their posting time. We adopt some data from
partition (i) for training, and some data from partition (ii)
for test. Specically, for partition (i), we randomly sample
1000 reviews as labeled training pool, and treat the rest as
unlabeled data. For partition (ii), we randomly sample 2000
reviews for labeling and use as test set for evaluation.
Table 4: Statistics of Our Datasets (Train = Train-
ing Pool, Unlab. = Unlabeled Set).
Dataset SwiftKey Facebook TempleRun2 TapFish
Train 1000 1000 1000 1000
Unlab. 3282 104709 57559 3547
Test 2000 2000 2000 2000
% Info. 29.3% 55.4% 31.1% 24.6%
We collected the ground truth labels of the training pool
and test set according to the rules summarized in Figure 1.
Each review is labeled by three dierent evaluators (some
are workers in Amazon Mechanical Turk [1]), and the nal
label is determined by the \majority voting". The row \%
Info." in Table 4 shows the proportion of \informative" re-
views (among data with ground truth). On average, 35.1%
reviews are \informative". Without loss of generality, we
take \informative" as positive class and \non-informative" as
negative class.
5.2 Performance Metrics
In this section, we introduce the performance metrics used
in our evaluation. The rst set of metrics include Precision,
Recall (Hit-rate) and F-measure, which are dened below:
Precision =TP
TP+FP; Recall(Hit rate) =TP
TP+FN773F measure =2PrecisionRecall
Precision + Recall
whereTP;FP;FN represent the numbers of true positives
(hits), false positives, and false negatives (misses), respec-
tively. In addition, we also adopt the well-known N ormalized
Discounted C umulative G ain (NDCG) [17] as a measure for
evaluating the quality of top-k ranking results:
NDCG@k =DCG @k
IDCG @k
where NDCG@k2[0;1], and the higher value implies greater
agreement between the predicted rank order and the ideal
rank order.
5.3 Evaluation of Grouping Performance
We conduct two experiments to evaluate the performance
of our scheme (the rst 3 steps shown in Figure 2) for auto-
matically discovering groups (topics). First, we qualitatively
compare our scheme (which contains a ltering step before
grouping) with the baseline scheme used in [22] (which di-
rectly applies topic models). Second, we explore and com-
pare two dierent settings of our scheme, i.e., (i) EMNB-
LDA (Stanford Topic Modeling Toolbox [5] implementation
for LDA); and (ii) EMNB-ASUM (the original implementa-
tion [2] with default parameters for ASUM), where ASUM
is proposed in [30] and adopted in [22].
We select the EMNB lter for each dataset shown in Ta-
ble 4 as follows. For each experimental trial, we randomly
choose a subset of training pool (128 examples per class) as
training data, and then apply the EMNB algorithm (the
LingPipe implementation [4]) to build a classier on the
combination of training data and unlabeled set, nally mea-
sure the performance on the test set. We repeat the above
experimental trial 50 times and choose the classier with the
best F-measure as the lter. Table 5 shows the F-measure
attained by the four selected lters used in our experiments.
We can see that, their performance is fairly good, especially
theFacebook lter (0.877).
Table 5: The Performance of Selected Filters
Filter SwiftKey Facebook TempleRun2 TapFish
F-measure 0.764 0.877 0.797 0.761
5.3.1 Qualitative Comparison of Both Schemes
The rst experiment qualitatively compares our scheme
(EMNB-LDA) with the baseline scheme (LDA). We apply
both schemes to the test set of each dataset shown in Table
4 after preprocessing. We vary the number of topics (denot-
ed as K) and choose the appropriate K values according to
(i) the perplexity scores [11] on 20% held-out data (should
be small); and (ii) the results themselves (should be reason-
able). Table 6 shows some representative topics found by
EMNB-LDA and LDA from the test set of SwiftKey . For
each topic, we list the top-10 weighted words in the vocab-
ulary distribution. For space reasons, we do not present the
results for other datasets (which are similar).
From the results shown in Table 6, we can draw two obser-
vations. First, most topics found by EMNB-LDA are \infor-
mative", e.g., \theme", \Chinese", \jelly bean", \predict" and
\space"shown in Table 6(a), while LDA presents many\non-
informative" (or redundant) topics, such as \type"(purelyTable 6: Some topics found by EMNB-LDA (K=20)
and LDA (K=36) on \SwiftKey" dataset. The col-
ored words are topic labels.
(a) EMNB-LDA
theme Chinese jelly bean predict space
more languag bean word space
theme chines jelli predict period
wish need galaxi text email
love wait note complet enter
custom user keyboard auto insert
like download samsung like automat
color support screen pen input
star input updat won mark
option except android basic address
keyboard thai swiftkei automat dont
(b) LDA
theme Chinese jelly bean type worth
theme chines predict type worth
more languag text make monei
like faster bean easi denit
color input jelli learn paid
love more time predict penni
wish need issu easier price
custom switch accur speed download
option annoi start accur total
pick time browser perfectli cent
red write samsung time amaz
praise without any advice) and \worth" (emotional expres-
sion) shown in Table 6(b) in red color. The reason is s-
traightforward: LDA does not have a ltering phase. Sec-
ond, although with well-tuned K value, LDA could also
nd \informative" topics discovered by EMNB-LDA, some
of them have lower quality. For example, the topic \jelly
bean" shown in Table 6(b) has (i) lower-ranked key words;
and (ii) lower purity (the word \predict" ranked high).
In sum, we can conclude that our scheme (with ltering)
performs better than the baseline scheme (without ltering)
in solving our problem.
5.3.2 Comparison of Two Topic Models
The second experiment is to compare the performance of
two topic models (LDA and ASUM) in our scheme. For
each dataset shown in Table 4, we manually identify one
appropriate and representative group from \informative" re-
views in the test set as ground truth (prior to running our
scheme), where each review in the group is assigned a pro-
portion score. The \Topic" column of Table 7 shows the
labels of the groups. Following the same setup as the rst
experiment (K=20), we evaluate the performance of EMNB-
LDA and EMNB-ASUM by measuring if they can discover
the pre-identied groups accurately. Table 7 presents the
experimental results in terms of F-measure (averaged over
50 iterations).
Some observations can be drawn from the results shown
in Table 7. First, for all topics, EMNB-LDA performs bet-
ter than EMNB-ASUM in terms of F-measure. One pos-
sible reason is ASUM imposes a constraint that all word-
s in a sentence be generated from one topic [30, page 2].
Thus, sentence-level reviews exhibit several topics are only
assigned to one topic, which results in information lost. Sec-
ond, by looking into the results, the F-measure achieved by774Table 7: Evaluation Results, K = 20
Dataset Topic EMNB- EMNB-
ASUM LDA
SwiftKey \theme" 0.437 0.657
Facebook \status" 0.388 0.583
TempleRun2 \lag" 0.210 0.418
TapFish \easier buck" 0.386 0.477
EMNB-LDA is reasonable but not promising, e.g., 0.657 for
the \theme" topic of SwiftKey . The main reason is the un-
supervised topic modeling is a hard task. Besides, some \in-
formative" reviews are removed wrongly by the lter, while
some \non-informative" ones are not ltered out.
5.4 Evaluation of Ranking Performance
In this section, we report a comprehensive case study to
evaluate the ranking performance of AR-Miner. We aim to
examine whether the top-ranked topics generated by AR-
Miner represent the most \informative" user feedback for
real app developers .
We use SwiftKey Keyboard shown in Table 4 as our sub-
ject app. The developers of this app created a nice SwiftKey
feedback forum to collect user feedback [6]. It provides users
a voting mechanism for every feedback, and feedback with
high-voting is ranked top. Feedback considered to be \infor-
mative" to developers is assigned a Status , which shows the
current progress of this feedback. Therefore, we can obtain a
comparable ranking list of\informative"information for real
developers of SwiftKey Keyboard. Specically, we rst se-
lected all the user feedback in the forums, and then removed
those feedback without Status (indicates \non-informative"
to developers) or assigned Status of \Complete" (indicates
closed) before the time interval T(from Oct 12th, 2012 to
Dec 19th, 2012) we investigated, nally we ranked the re-
maining feedback in the decreasing order of number of votes.
The top-10 ranked results (ground truth) are shown in Fig-
ure 6 (veried around Feb 17th, 2013).
The user reviews of SwiftKey Keyboard collected in T
contains 6463 instances. We use the lter shown in Table
5, and apply both LDA and ASUM algorithms. Finally,
the top-10 ranked results generated by our ranking model
are visualized in Figure 5 (LDA setting, K = 22, = 0:6,
wG= (0:85;0;0:15),wI= (0:2;0:2;0:2;0:2;0:2)). We com-
pare the ranking results attained by AR-Miner with the
ground truth ranking results (Figure 6), and measure the
comparison in terms of Hit-rate and NDCG@10 scores in-
troduced in Section 5.2. Note that, each feedback shown in
Figure 6 is considered to be corresponding to a topic shown
in Figure 5 if and only if (i) the feedback is closely related
to the topic, and (ii) the (semantically similar) feedback can
be found in the top review instances of the topic. Table 8
presents the results.
Remark. We use the ranking list shown in Figure 6 as the
ground truth mainly because it is the choice of real app
developers in real scenarios. We believe it is much more
convincing than a ranking list identied by others (e.g., the
authors). Besides, we assume that the greater agreement
between the ranking results achieved by AR-Miner and the
ground truth, the better performance of AR-Miner. Specif-ically, if both Hit-rate and NDCG@10 equal to 1, we think
that AR-Miner is as eective as SwiftKey feedback forum .
 Rank  Votes  User Feedback  Status  
1 5711  More themes. More themes. More themes.  STARTED  
2 4033  Continuous input - glide your fingers across the 
screen / Flow  STARTED  
3 4025  Option to disable auto -space after punctuation 
and/or prediction  UNDER 
REVIEW  
4 3349 customizable smileys / emoticons  UNDER 
REVIEW 
5 2924 AutoText - Word Substitution / Macros (brb = 'be 
right back') UNDER 
REVIEW 
6 2923  Traditional Chinese  STARTED  
7 2504  An option to use swiftkey  predictions everywhere 
in every app including a web searches  STARTED  
8 2313  Chinese pinyin  STARTED  
9 2095 Thai STARTED 
10  2014  After Jelly Bean update, Swiftkey keeps returning to  
android default keyboard after restart or shutdown  UNDER 
REVIEW  
Figure 6: Top-10 Ranked Results Attained from
SwiftKey Feedback Forum, Highlighted Feedback
Has Corresponding Topic Shown in Figure 5
Table 8: Ranking Results
AR-Miner (LDA) AR-Miner (ASUM)
Hit-rate 0.70 0.50
NDCG@10 0.552 0.437
Some observations can be found from Table 8. First,
for both metrics, LDA performs better than ASUM in our
framework. Second, by looking into the results, AR-Miner
(LDA) achieves 0.70 in terms of Hit-rate, which indicates
that AR-Miner is able to automatically discover the most
\informative" information eectively, and thus can be bene-
cial for app developers, especially those who have not es-
tablished valid channels. Feedback highlighted in Figure 6
has corresponding topic in the radar chart shown in Figure 5.
For example, the ranked 1sttopic discovered by AR-Miner
shown in Figure 5 (\more theme [1]", where the number in
square bracket represents the rank in Figure 6) correspond-
s to the ranked 1stuser feedback (\More themes. More
themes. More themes.") shown in Figure 6.
5.5 Comparison with Manual Inspection and
Traditional Channels
We conduct two case studies to (i) compare AR-Miner
with manual inspection in terms of manpower input; and
(ii) analyze the advantages of AR-Miner over facilities used
in a traditional channel (i.e., online forum).
5.5.1 Manpower Input Analysis
In the rst case study, we apply three schemes: (i) AR-
Miner with EMNB lter (256 training examples); (ii) AR-
Miner with NB lter (500 training examples); and (iii) pure-
ly manual inspection, respectively, to the test set of the Face-
book dataset shown in Table 4 (2000 examples). We record-
ed the approximate manpower input (of the rst author) for
nding the most \informative" information by these three
schemes2. For simplicity, we ignore the performance dier-
ence between AR-Miner and manual inspection. Figure 7(a)
presents the comparison results.
2For purely manual inspection, we recorded the eorts spent
on sampled data, and then estimated the total man-hours.7750.50.9 7.4 
02468
AR-Miner 
(EMNB) AR-Miner 
(NB) Manual 
Inspection
Manpower Input (man-hour) (a) Manpower Comparison
 (b) EMNB vs. NB
Figure 7: Evaluation Results on \Facebook". (a)
manpower comparison with manual inspection, (b)
comparison between EMNB and NB with varied
training data.
Some observations can be found from the results shown
in Figure 7(a). First, we nd that AR-Miner (EMNB lter,
0.5 man-hours) is much more ecient than purely manual
inspection (7.4 man-hours). The reason is AR-Miner only re-
quires humans to label some training data, and can work au-
tomatically after the lter has been built. Second, AR-Miner
(NB) needs more human eorts than AR-Miner (EMNB), s-
ince building a NB lter whose performance is comparable to
a EMNB lter requires manually labeling more training data
(500 and 256 examples for NB and EMNB, respectively, in
this case study). We explain it with the results shown in Fig-
ure 7(b). Following the same setup described in paragraph
2 of Section 5.3, Figure 7(b) shows the average F-measure
of NB and EMNB under varying amounts of training data
(Facebook ). It is obvious that, when the F-measure score is
xed, NB always requires more training data (human eort-
s) than EMNB (the results are similar for other datasets,
check details on our website). Therefore, we choose EMNB
in AR-Miner to reduce human eorts as much as possible.
5.5.2 Comparison with an Online Forum
Following the same setup of SwiftKey Keyboard described
in paragraph 3 of Section 5.4, we conduct a case study to
analyze the advantages of AR-Miner over a traditional chan-
nel, i.e., online forum ( SwiftKey feedback forum ).
First of all, from user reviews, AR-Miner has the abili-
ty to discover fresh \informative" information that does not
exist in the SwiftKey feedback forum . Take the ranked 1st
topic \more theme" shown in Figure 5 as an example. Fig-
ure 8(a) shows more review instances in the top-10 list of
\more theme". The top 1 ranked review shown in Figure
8(a) and the ranked 1st user feedback shown in Figure 6
are semantically the same. Moreover, we observe that the
ranked 10th review (\ ...., or support for third party themes ")
is only discovered by AR-Miner, which oers app developers
new suggestions concerning the topic \more theme". This
kind of new information is benecial to developers, since it
may inspire them to further improve their apps.
Second, AR-Miner can provide app developers deep and
more insights than SwiftKey feedback forum by exibly ad-
justing the weight vectors of wGandwI. For example, as
described in Section 5.4, SwiftKey feedback forum only sup-
port a user voting mechanism (like Volume inwG) to rank
the user feedback, while AR-Miner can achieve it from dier-
ent angles. If setting wG= (0:0;0:0;1:0) (indicates groups
are ranked only according to AvgRating ), the ranking of
\more theme" shown in Figure 5 drops from 1 to 22, which
implies that it's not a kind of critical and urgent problem
Topic: ‚Äúmore theme ‚Äù Score  
1 Also we need more themes! 0.932 
2 Just wish you had more themes or ability 
to make a custom theme. 0.800  
‚Ä¶ ‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶.  ‚Ä¶‚Ä¶.  
10 It would also be nice to have more 
themes or support for third party themes . 0.621  (a) Instances Ranking
00.2 0.4 0.6 
0 1 2 3 4 5 6 7 8 9 10 11 12 13 p(g,k) 
Time Window k 
Score  
1 .932 
2 Just wish you had more themes or ability  
‚Ä¶ ‚Ä¶‚Ä¶.  
10 
support for third party themes .  (b)Pof \theme"
Figure 8: Unique Information Oered by AR-Miner
to users. If setting wG= (0:0;1:0;0:0) (indicates groups
are ranked only according to TimeSeries ), the ranking of
\more theme" drops from 1 to 18. The time series pattern
of \more theme" in this case can be automatically visualized
as shown in Figure 8(b), which helps app developers easily
understand that it's a kind of existing problem.
In sum, this case study implies that even for those app
developers who have already established some traditional
channels, AR-Miner can be a benecial compliment.
6. LIMITATIONS AND THREATS TO VA-
LIDITY
Despite the encouraging results, this work has two poten-
tial threats to validity. First, the authors are not profes-
sional app developers, and thus the dened category rules
of informativeness as summarized in Figure 1 might not be
always true for real app developers. In this paper, we have
attempted to alleviate this threat by (i) studying what kinds
of user feedback are realapp developers concerned with; and
(ii) exploiting realapp developers' decisions as the ground
truth for evaluation. The second threat relates to the gener-
ality of our framework. We validate our framework on user
reviews of four Android apps from Google Play. It is unclear
that if our framework can attain similar good results when
being applied to other kinds of Android apps (e.g., apps in
Amazon Appstore) and apps on other platforms (e.g., iOS).
Future work will conduct a large-scale empirical study to
address the threat. Besides, another limitation of our work
is that we only choose A=fText;Rating;Timestamp gas
mentioned in Section 3, but a real app marketplace may have
more features of user reviews (e.g., Device Name in Google
Play, Amazon Veried Purchase in Amazon Appstore). The
impact of these specic features is unknown, but our frame-
work is rather generic and extensible to incorporating more
features in future work.
7. CONCLUSION
This paper presented AR-Miner, a novel framework for
mobile app review mining to facilitate app developers ex-
tract the most \informative" information from raw user re-
views in app marketplace with minimal manual eort. We
found encouraging results from our extensive experiments
and case studies, which not only validates the ecacy but
also shows the potential application prospect of AR-Miner.
We also discuss some limitations along with threats to va-
lidity in this work, and plan to address them in the future.
8. ACKNOWLEDGMENTS
This work was in part supported by the Singapore MOE
tier-1 research grant (RG33/11). Special thanks to Shaohua
Li for helping us label some of the data. We also thank the
anonymous reviewers for their greatly helpful comments.7769. REFERENCES
[1] Amazon Mechanical Turk. https://www.mturk.com/ .
[2] Aspect and Sentiment Unication Model.
http://uilab.kaist.ac.kr/research/WSDM11/ .
[accessed 21-Jan-2014].
[3] Bugzilla. http://www.bugzilla.org/ .
[4] LingPipe. http://alias-i.com/lingpipe/ .
[5] Stanford Topic Modeling Toolbox.
http://nlp.stanford.edu/software/tmt/tmt-0.4/ .
[6] SwiftKey Feedback Forums.
http://support.swiftkey.net/ . [accessed
21-Jan-2014].
[7] M. Abulaish, Jahiruddin, M. N. Doja, and T. Ahmad.
Feature and opinion mining for customer review
summarization. In Proceedings of the 3rd International
Conference on Pattern Recognition and Machine
Intelligence , pages 219{224, 2009.
[8] G. Antoniol, K. Ayari, M. Di Penta, F. Khomh, and
Y.-G. Gu eh eneuc. Is it a bug or an enhancement?: a
text-based approach to classify change requests. In
Proceedings of the 2008 Conference of the Center for
Advanced Studies on Collaborative Research: Meeting
of Minds , pages 23:304{23:318, 2008.
[9] J. Anvik, L. Hiew, and G. C. Murphy. Who should x
this bug? In Proceedings of the 28th International
Conference on Software Engineering , pages 361{370,
2006.
[10] A. Bacchelli, T. Dal Sasso, M. D'Ambros, and
M. Lanza. Content classication of development
emails. In Proceedings of the 34th International
Conference on Software Engineering , pages 375{385,
2012.
[11] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent
dirichlet allocation. J. Mach. Learn. Res. , 3:993{1022,
Mar. 2003.
[12] R. Caruana and A. Niculescu-Mizil. An empirical
comparison of supervised learning algorithms. In
Proceedings of the 23rd International Conference on
Machine Learning , pages 161{168, 2006.
[13] R. Chandy and H. Gu. Identifying spam in the ios app
store. In Proceedings of the 2nd Joint
WICOW/AIRWeb Workshop on Web Quality , pages
56{59, 2012.
[14] P. H. Chia, Y. Yamamoto, and N. Asokan. Is this app
safe?: A large scale study on application permissions
and risk signals. In Proceedings of the 21st
International Conference on World Wide Web , pages
311{320, 2012.
[15] J. Cleland-Huang, A. Czauderna, M. Gibiec, and
J. Emenecker. A machine learning approach for tracing
regulatory codes to product specic requirements. In
Proceedings of the 32nd International Conference on
Software Engineering , pages 155{164, 2010.
[16] J. Cleland-Huang, R. Settimi, X. Zou, and P. Solc.
The detection and classication of non-functional
requirements with application to early aspects. In
Proceedings of the 14th International Requirements
Engineering Conference , pages 36{45, 2006.
[17] B. Croft, D. Metzler, and T. Strohman. Search
Engines: Information Retrieval in Practice .
Addison-Wesley Publishing Company, USA, 1st
edition, 2009.[18] O. Dalal, S. H. Sengemedu, and S. Sanyal.
Multi-objective ranking of comments on web. In
Proceedings of the 21st International Conference on
World Wide Web , pages 419{428, 2012.
[19] Y. Dang, R. Wu, H. Zhang, D. Zhang, and P. Nobel.
Rebucket: A method for clustering duplicate crash
reports based on call stack similarity. In Proceedings of
the 34th International Conference on Software
Engineering , pages 1084{1093, 2012.
[20] X. Ding, B. Liu, and P. S. Yu. A holistic lexicon-based
approach to opinion mining. In Proceedings of the 1st
International Conference on Web Search and Data
Mining , pages 231{240, 2008.
[21] B. Fu, J. Lin, L. Li, C. Faloutsos, J. Hong, and
N. Sadeh. Why people hate your app: Making sense of
user feedback in a mobile app store. In Proceedings of
the 19th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining , pages
1276{1284, 2013.
[22] L. V. Galvis Carre~ no and K. Winbladh. Analysis of
user comments: An approach for software
requirements evolution. In Proceedings of the 35th
International Conference on Software Engineering ,
pages 582{591, 2013.
[23] H. georg Kemper and E. Wolf. Iterative process
models for mobile application systems: A framework.
InProceedings of the 23rd International Conference on
Information Systems , pages 401{413, 2002.
[24] M. Harman, Y. Jia, and Y. Zhang. App store mining
and analysis: Msr for app stores. In Proceedings of the
9th Working Conference on Mining Software
Repositories , pages 108{111, 2012.
[25] K. Herzig, S. Just, and A. Zeller. It's not a bug, it's a
feature: how misclassication impacts bug prediction.
InProceedings of the 35th International Conference on
Software Engineering , pages 392{401, 2013.
[26] C.-F. Hsu, E. Khabiri, and J. Caverlee. Ranking
comments on the social web. In Proceedings of the
2009 International Conference on Computational
Science and Engineering , pages 90{97, 2009.
[27] M. Hu and B. Liu. Mining and summarizing customer
reviews. In Proceedings of the 10th ACM SIGKDD
International Conference on Knowledge Discovery and
Data Mining , pages 168{177, 2004.
[28] C. Iacob and R. Harrison. Retrieving and analyzing
mobile apps feature requests from online reviews. In
Proceedings of the 10th Working Conference on
Mining Software Repositories , pages 41{44, 2013.
[29] N. Jindal and B. Liu. Opinion spam and analysis. In
Proceedings of the 1st International Conference on
Web Search and Data Mining , pages 219{230, 2008.
[30] Y. Jo and A. H. Oh. Aspect and sentiment unication
model for online review analysis. In Proceedings of the
4th International Conference on Web Search and Data
Mining , pages 815{824, 2011.
[31] M. Linares-V asquez, G. Bavota, C. Bernal-C ardenas,
M. Di Penta, R. Oliveto, and D. Poshyvanyk. Api
change and fault proneness: A threat to the success of
android apps. In Proceedings of the 9th Joint Meeting
on Foundations of Software Engineering , pages
477{487, 2013.777[32] J. B. Macqueen. Some methods of classication and
analysis of multivariate observations. In Proceedings of
the 5th Berkeley Symposium on Mathematical
Statistics and Probability , pages 281{297, 1967.
[33] R. Minelli and M. Lanza. Software analytics for mobile
applications{insights & lessons learned. In Proceedings
of the 17th European Conference on Software
Maintenance and Reengineering , pages 144{153, 2013.
[34] A. Mukherjee, B. Liu, and N. Glance. Spotting fake
reviewer groups in consumer reviews. In Proceedings of
the 21st International Conference on World Wide
Web, pages 191{200, 2012.
[35] K. Nigam, A. K. McCallum, S. Thrun, and
T. Mitchell. Text classication from labeled and
unlabeled documents using em. Mach. Learn. ,
39(2-3):103{134, May 2000.
[36] D. Pagano and W. Maalej. User feedback in the
appstore: An empirical study. In Proceedings of the
21st IEEE International Requirements Engineering
Conference , pages 125{134, 2013.
[37] B. Pang and L. Lee. Opinion mining and sentiment
analysis. Found. Trends Inf. Retr. , 2(1-2):1{135, Jan.
2008.[38] C. Sun, D. Lo, X. Wang, J. Jiang, and S.-C. Khoo. A
discriminative model approach for accurate duplicate
bug report retrieval. In Proceedings of the 32nd
International Conference on Software Engineering ,
pages 45{54, 2010.
[39] B. Turhan and A. B. Bener. Software defect
prediction: Heuristics for weighted na ve bayes. In
Proceedings of the 2nd International Conference on
Software and Data Technologies, Volume SE , pages
244{249, 2007.
[40] P. D. Turney. Thumbs up or thumbs down?: Semantic
orientation applied to unsupervised classication of
reviews. In Proceedings of the 40th Annual Meeting on
Association for Computational Linguistics , pages
417{424, 2002.
[41] S. Xie, G. Wang, S. Lin, and P. S. Yu. Review spam
detection via temporal pattern discovery. In
Proceedings of the 18th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining ,
pages 823{831, 2012.
[42] L. Zhuang, F. Jing, and X.-Y. Zhu. Movie review
mining and summarization. In Proceedings of the 15th
ACM International Conference on Information and
Knowledge Management , pages 43{50, 2006.778