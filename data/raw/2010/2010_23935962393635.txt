Test Input Generation Using Dynamic Programming
Razieh
Nokhbeh Zaeem and Sarfraz Khurshid
The University of Texas at Austin, Austin, TX, USA
nokhbeh@utexas.edu, khurshid@ece.utexas.edu
ABSTRACT
Constraint-based input generation is an effective technique for test-
ing programs, such as compilers and web browsers, which have
complex inputs. However, efﬁcient generation of such inputs re-
mains a challenging problem. We present a novel input genera-
tion technique that takes constraints written as recursive predicates
in the underlying programming language and uses dynamic pro-
gramming to solve the constraints efﬁciently. Our key insight is to
leverage the recursive structure of desired inputs and partition the
problem of generating an input into several sub-problems of gener-
ating smaller inputs that exhibit the same structure, and then to use
dynamic programming – a well-known problem solving methodol-
ogy designed to exploit common sub-problems – to combine them.
A lazy initialization strategy and symbolic execution optimize our
basic technique. Our technique provides not only bounded exhaus-
tive input generation but also enables random input generation. We
show the correctness of our technique. Furthermore, we present an
experimental evaluation, which shows that our technique can pro-
vide over an order of magnitude performance improvement for in-
put generation compared to Korat (an efﬁcient solver for structural
constraints) and Pex (a state-of-the-art tool for symbolic execution).
Finally, we use our technique to effectively ﬁnd bugs in production
versions of Google Chrome and Apple Safari web browsers.
Categories and Subject Descriptors
D.2.5 [ Software Engineering ]: Testing and Debugging—testing
tools
Keywords
Recursive Test Generation, Dynamic Programming, Lazy Initial-
ization, Symbolic Execution
1. INTRODUCTION
Test input generation is one of the most challenging tasks in auto-
mated testing. Generation is especially hard for programs, such as
browsers or compilers, which take complex structures, e.g., HTML
or Java programs, as inputs, because such inputs are hard to gener-
ate automatically or sample at random. Constraint-based testing [9,
21, 24, 29, 40] provides the basis for effective techniques [11, 36,
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
SIGSOFT’12/FSE-20, November 11–16, 2012, Cary, North Carolina, USA.
Copyright 2012 ACM 978-1-4503-1614-9/12/11 ...$15.00.35, 5, 32, 16, 17, 25] for generation of such structurally complex in-
puts: an input constraint deﬁnes the structural properties of a class
of desired inputs, a constraint solver enumerates solutions to the
constraint, and the solutions are reﬁned as test inputs. Advances
in constraint solving technology [4, 13] have enabled solvers to
handle more complex constraints. However, scaling constraint-
based testing remains a challenging problem, particularly when
used for systematic testing, i.e., bounded exhaustive testing, which
tests against all inputs within a bound on the input size.
This paper introduces a novel constraint-based technique for ef-
ﬁcient and scalable generation of structurally complex inputs. To
write constraints, our technique supports recursive predicates that
are written in the same language as the program under test. (Im-
perative predicates that check constraints, especially class invari-
ants, are traditionally called repOK methods [31].) Our key insight
is that not only are recursive predicates more natural to write for
recursive structures (which may also have some non-recursive ele-
ments) – e.g., a binary tree is either null, or a node whose left and
right children are each a binary tree – but also recursive predicates
naturally lend themselves to recursive input generation. Intuitively,
since the predicate deﬁnition is recursive, it sufﬁces to build the
non-recursive elements of a new input, and use the same method
to recursively build the recursive elements. For example, to build a
binary tree which is not null, it sufﬁces to assign one node as the
root and recursively call the same procedure to build binary trees
for the right and left children.
Our technique utilizes the recursive structure of desired inputs to
divide the problem of generating an input into several sub-problems
of generating smaller inputs that exhibit the same structure, and em-
ploys dynamic programming [10] – a well-known problem solving
methodology designed to exploit common sub-problems – to com-
bine them and generate inputs in a bottom-up manner. To illustrate,
when exhaustively generating binary trees as inputs, we build all
binary trees of sizes 0, 1, 2, and 3 and use them to construct binary
trees of size 4. For random test generation, we randomly gener-
ate some binary trees of sizes 0, 1, 2, and 3, and then combine
them to sample binary trees of size 4. Since many recursive test
inputs exhibit the property of overlapping sub-problems, dynamic
programming saves us a lot of computation.
We present three algorithms for input generation: (1) a basic
algorithm that uses dynamic programming directly, (2) a lazy ini-
tialization strategy that optimizes dynamic programming, and (3)
a further optimization using symbolic execution [27, 9]. Our ﬁrst
algorithm ( DP) utilizes dynamic programming to generate test in-
puts up to a given size. To build a new test input, we take formerly
generated test inputs and combine them to build the recursive parts,
explore the state space to assign values to the non-recursive parts,
and pass the resulting candidate object to the repOK predicate for
1
evaluation. If repOK returns true, this object is saved as a valid
input
and is then used to create larger inputs.
Our second algorithm (LazyDP) uses lazy initialization to opti-
mize performance. To cope with the limitation of other resources
beyond computational time, such as memory, we store a recursive
test input concisely as an array of numbers, where each number
recursively represents a formerly generated test. When combining
inputs to build a candidate, we do not expand all the smaller test
inputs in the hierarchy from their concise form to actual objects.
Instead, we expand them lazily whenever repOK accesses them.
Our third algorithm (SymboLazyDP ) further enhances the per-
formance by skipping a systematic search for non-recursive ﬁelds
when possible. For recursive ﬁelds, this search is improved by us-
ing the ﬁrst two algorithms where we restrict the choice of smaller
sub-problems to formerly generated solutions. To avoid the search
for non-recursive ﬁelds, the third algorithm adds symbolic execu-
tion [27, 26, 46], which uses symbolic values for non-recursive
ﬁelds, builds a path condition while executing repOK, and uses an
in-house constraint solver to solve for the symbolic values.
Our technique not only enables efﬁcient generation of inputs
based on textbook data structures, but also a wide-range of string-
based inputs that represent structured data, e.g., strings that belong
to a context-free grammar, SQL queries, data in a database man-
agement system, Java programs, and HTML/CSS pages. We de-
veloped a prototype implementation of our three algorithms and
evaluated it using microbenchmarks and real world applications,
including Google Chrome and Apple Safari web browsers.
This paper makes the following contributions. We introduce
the use of recursive predicates in constraint-based generation
of complex structures (that have recursive and non-recursive el-
ements) to both facilitate predicate formulation and enable faster
input generation. We present a novel technique for input genera-
tion using dynamic programming. To our knowledge, dynamic
programming has not previously been used to generate structurally
complex inputs. We prove the correctness of our technique. We
utilize lazy initialization and symbolic execution to optimize the
performance of dynamic programming for input generation. We
apply our technique based on dynamic programming for both ex-
haustive and random generation of test inputs. We use our proto-
type implementation to evaluate our technique. Experimental re-
sults show our technique improves input generation performance
and scalability for microbenchmarks over state-of-the-art testing
tools Pex [46] and Korat [5], and effectively ﬁnds real bugs in pro-
duction versions of two widely used applications.
2. EXAMPLE
In this section, we describe two examples: a binary tree, which
we use as a running example, and an HTML input that we use
alongside with a CSS input to test a web browser. The recursive
deﬁnition of a binary tree is as follows: a binary tree is either null,
or a node whose left and right children each point to a binary tree.
The tree should not have any loops. Furthermore, in our binary tree,
btSize is equal to the number of nodes in the tree. Listing 1 shows
the recursive implementation of a binary tree. One can easily see
that a recursive repOK is a natural way of describing the properties
of a recursive data structure.
Listing 2 shows the repOK method of an HTML input. The back-
bone of such an input is a generic tree, a recursive data structure.
In addition, other constraints are enforced on HTML tags. In Sec-
tion 4.4, we demonstrate how to leverage this modeling of HTML
ﬁles and list-modeling of CSS ﬁles to automatically generate bug
revealing test inputs for the Chrome and Safari web browsers. Fig-
ure 1 models the nested structure of tags in Listing 7 (Section 4.4).1c l a s s B i n a r y T r e e {
2 B i n a r y T r e e r i g h t , l e f t ;
3 i n t b t S i z e ;
4 boolean repOK ( ) {
5 i f( ! a c y c l i c ( ) ) return f a l s e ;
6 return r e c u r s i v e _ r e p O K ( ) ; }
7 boolean r e c u r s i v e _ r e p O K ( ) {
8 i n t r i g h t B t S i z e , l e f t B t S i z e ;
9 i f( r i g h t == n u l l ) r i g h t B t S i z e = 0 ;
10 e l s e {
11 i f( ! r i g h t . r e c u r s i v e _ r e p O K ( ) )
12 return f a l s e ;
13 r i g h t B t S i z e = r i g h t . b t S i z e ; }
14 i f( l e f t == n u l l ) l e f t B t S i z e = 0 ;
15 e l s e {
16 i f( ! l e f t . r e c u r s i v e _ r e p O K ( ) )
17 return f a l s e ;
18 l e f t B t S i z e = l e f t . b t S i z e ; }
19 i f( b t S i z e == r i g h t B t S i z e + l e f t B t S i z e +1) return t ru e ;
20 e l s e return f a l s e ; } }
Listing 1: A recursive binary tree in Java.
1c l a s s HTML {
2 i n t t a g I n d e x ; HTML[ ] c h i l d ;
3 A t t r i b u t e [ ] a t t r ;
4 S t r i n g [ ] HTMLTags =
5 { " html " , " head " , " l i n k " , " body " , . . . } ;
6 boolean repOK ( ) {
7 i f( ! a c y c l i c ( ) ) return f a l s e ;
8 i f( t a g I n d e x != 0) / / " h tml "
9 return f a l s e ;
10 i f( ! c h i l d [ 0 ] . repOKHead ( ) )
11 return f a l s e ;
12 i f( ! c h i l d [ 1 ] . repOKBody ( ) )
13 return f a l s e ;
14 . . .
15 return t ru e ; }
16 boolean repOKBody ( ) {
17 i f( t a g I n d e x != 3) / / " body "
18 return f a l s e ;
19 i f( ! c h i l d [ 0 ] . r e c u r s i v e _ r e p O K ( ) )
20 return f a l s e ;
21 . . .
22 return t r ue ; }}
Listing 2: HTML repOK method./c63
/c62/c61/c60 /c56/c57/c58/c59html
/d1/d1✄✄✄✄✄
/d29/d29❀❀❀❀❀
/c63/c62/c61/c60 /c56/c57/c58/c59head
/d15 /d15/c63/c62/c61/c60 /c56/c57/c58/c59body
/d15/d15/c63/c62/c61/c60 /c56/c57/c58/c59link /c63 /c62/c61/c60 /c56/c57/c58/c59div
/d15 /d15/c63/c62/c61/c60 /c56/c57/c58/c59h1
/d15/d15/c63/c62/c61/c60 /c56/c57/c58/c59div
/d15 /d15/c63/c62/c61/c60 /c56/c57/c58/c59h1
Figur
e 1: A tree
representation of
an HTML input.
Note that we support repOK methods with both recursive and
non-recursive parts. The btSize of each subtree in Listing 1 and
therepOKHead andrepOKBody checks in Listing 2 are examples
of non-recursive parts of a repOK.
3. FRAMEWORK
In this section, we describe how we use recursive deﬁnitions of
data structures for exhaustive and random test generation. We ex-
plain our three algorithms and prove a theorem on their correctness.
3.1 Recursive repOK Methods
Many test inputs, such as data structures, have an embedded re-
cursive structure. Sets, trees, stacks, queues, arrays, heaps, and
many other data structures have recursive deﬁnitions. Using recur-
sion in repOK to identify correct instances of recursive loop-free
data structures makes repOK easier to write, read, and debug.
Besides identifying correct instances of a data structure, a repOK
method should be able to identify incorrect structures as well, but
incorrect structures are not necessarily loop-free. To comply with
this standard deﬁnition of repOK, we require the template shown
in the repOK method of Listing 1. This repOK ﬁrst checks for cy-
cles, and then enters the recursion phase to avoid an inﬁnite loop.
Throughout this paper we use recursive repOK methods that as-
sume acyclicity.
2We process the source of a recursive repOK through a simple
pattern
matching to ﬁnd all recursive calls. A ﬁeld on which repOK
is recursively called is identiﬁed as a recursive ﬁeld . We could also
identify recursive calls by processing the Abstract Syntax Tree of
method calls.
3.2 Algorithms
Here we describe DP, LazyDP, and SymboLazyDP. The use of
these algorithms is orthogonal to exhaustive or random test genera-
tion. In Section 3.2.2 we describe a variation of the algorithms that
generates random tests.
3.2.1 DP
Generation of test inputs can beneﬁt from the recursive struc-
ture of repOK. Given a recursive repOK, the goal of exhaustive test
generation is to generate all test inputs, in a given scope, for which
repOK returns true. For example, the recursive repOK of a binary
tree (Listing 1) checks whether its right and left children are cor-
rect binary trees and whether btSize is correctly set to the sum of
thebtSize ﬁelds of the children plus one. By observing the ex-
ecution of repOK, we present our recursive method of generation:
a new candidate test is generated by setting its recursive ﬁelds to
formerly generated correct tests, and ﬁnding correct values for the
non-recursive ﬁelds. Then, repOK is invoked on the candidates to
ﬁlter out the incorrect ones. The repOK method directly rejects the
candidates with loops and recursively calls itself to evaluate differ-
ent parts of the loop-free candidates. This check includes assuring
the correctness of recursive substructures. Because we provide pre-
viously generated valid test inputs as substructures, we can bypass
these internal recursive calls and directly return true for them.
Besides breaking the problem into subproblems, our generation
method demonstrates the other property necessary to a dynamic
programming solution: overlapping subproblems. The same sub-
structure is used multiple times to build new candidate test inputs.
For more efﬁcient test generation, the DP algorithm avoids gen-
erating repetitive candidates. To this end, we generate test inputs
in iterations and keep three sets of previously generated inputs:
thisRoundTests contains correct tests generated in the current
iteration,lastRoundTests includes correct tests generated in the
last iteration (which have not yet been used to build other candi-
dates), and pool contains all other correct tests generated so far. At
the beginning of test generation, pool andthisRoundTests are
empty, and lastRoundTests contains only null (line 6 of List-
ing 3). We assume that null is always a valid test input because we
cannot call repOK on null (e.g., it throws a nullPointerException
in a Java program), and for all common data structures null is a
valid instance.
The DP algorithm proceeds as follows (Listing 3). If repOK
makesrrecursive calls, we need r(not necessarily different, but
ordered) tests to build a new test. The outer while loop (lines 10
to 31) executes as long as it makes progress in generating new test
inputs. At each iteration, the test inputs generated in the previ-
ous iterations are combined to form new candidates; then repOK
is invoked to identify correct test inputs. More speciﬁcally, the
recursives array selects a permutation of rrecursive substruc-
tures by calling the nextPermutation method. This method it-
erates over pool andlastRoundTests and upon each invoca-
tion, provides the next permutation of rsubstructures from the set
lastRoundTests∪pool, such that at least one of the rsubstruc-
tures is selected from lastRoundTests. (Section 3.3 shows how
this constraint assures that we would avoid repetitive candidates.)
When all permutations are exhausted, this method returns null .
A challenge for DP is the exponential growth of the number of
candidates at the outer boundaries of the scope. In fact, even cre-1v oid t e s t G e n e r a t i o n ( ) {
2 i n t r = g e t N u m R e c u r s i v e C h i l d r e n ( ) ;
3 i n t s = getNumNonRecursiveFields ( ) ;
4 po ol = i n i t i a l i z e ( ) ;
5 l a s t R o u n d T e s t s = i n i t i a l i z e ( ) ;
6 addTo ( l a s t R o u n d T e s t s , null , 0) ; / / 0 i s t h e s i z e o f n u l l
7 t o t a l E x p l o r e d ++;
8 v a l i d C a s e s G e n e r a t e d ++;
9 boolean p r o g r e s s = tru e ;
10 while ( p r o g r e s s ) { / / t e s t g e n e r a t i o n round lo op
11 p r o g r e s s = f a l s e ;
12 t h i s R o u n d T e s t s = i n i t i a l i z e ( ) ;
13 i n t[ ] r e c u r s i v e s = n e x t P e r m u t a t i o n ( r ) ;
14 while ( r e c u r s i v e s != n u l l ) {
15 i f( s i z e ( r e c u r s i v e s ) >= maxNumRecursives ) {
16 r e c u r s i v e s = n e x t P e r m u t a t i o n P r u n i n g ( r , s i z e (
r e c u r s i v e s ) ) ;
17 c o n t i n u e ; }
18 i n t[ ] f i e l d s = n e x t V a l u a t i o n ( s ) ;
19 while ( f i e l d s != n u l l ) {
20 t o t a l E x p l o r e d ++;
21 O b j e c t t e s t c a s e O b j = b u i l d C a n d i d a t e (
r e c u r s i v e s , f i e l d s ) ;
22 i f( t e s t c a s e O b j . repOK ( ) && ( ! randomIsOn ( ) | |
c o i n T o s s ( s i z e ( r e c u r s i v e s ) ) ) ) {
23 p r o g r e s s = t ru e ;
24 v a l i d C a s e s G e n e r a t e d ++;
25 addTo ( t h i s R o u n d T e s t s , combine ( r e c u r s i v e s ,
f i e l d s ) , s i z e ( r e c u r s i v e s ) ) ; }
26 f i e l d s = n e x t V a l u a t i o n ( s ) ; }
27 r e c u r s i v e s = n e x t P e r m u t a t i o n ( r ) ; }
28 f o r(i n t[ ] t e s t : l a s t R o u n d T e s t s )
29 addTo ( pool , t e s t , s i z e ( t e s t ) ) ;
30 l a s t R o u n d T e s t s = t h i s R o u n d T e s t s ;
31 }}/ / end t e s t g e n e r a t i o n round loo p
Listing 3: Test generation algorithm in Java.
ating
those candidates can considerably affect the performance. To
address this challenge, we keep the tests in each of the three sets
sorted according to their sizes. We use bucket sort [10] (since the
maximum size of a valid test is known from the scope) as we add
new tests. In Listing 3, once recursives is selected, we ﬁrst ex-
amine its size in line 15. If a candidate built with this permuta-
tion would be outside the scope, we throw this permutation away
and also prune all other permutations with the same or bigger sizes
via calling the nextPermutationPruning method. This method
gives the next permutation that is inside the scope. Once we have
such a permutation, we use the rsubstructures to build the recur-
sive ﬁelds of a new candidate. In order to ﬁnd the proper val-
ues for the non-recursive ﬁelds, we perform a systematic search
by calling the nextValuation method, which, upon each invoca-
tion, returns one valuation for the non-recursive ﬁelds. A permu-
tation for recursives together with a valuation of fields gives
us a candidate ( testcaseObj) which we send to repOK. If repOK
returns true (ignoring randomIsOn andcoinToss for now), the
new test gets added to thisRoundTests with its appropriate size,
otherwise it is discarded. At the end of the outer while loop,
when all permutations are exhausted, the tests in lastRoundTests
joinpool (lines 28 and 29), the tests in thisRoundTests replace
lastRoundTests (line 30), and we move on to the next round.
In the DP algorithm, size is deﬁned recursively as follows. If a
candidate is null, its size is 0. Otherwise, the size of a candidate
is the sum of the sizes of its recursive substructures plus one. Note
that in the example of a binary tree, btSize has the same meaning
of size. The size concept built into the DP algorithm, however, does
not necessarily correspond to a ﬁeld of the data structure. The DP
algorithm memoizes and uses the size of a test, whereas it treats
btSize as a non-recursive ﬁeld, and performs a systematic search
to ﬁnd its correct value for any candidate.
Consider the example of ﬁnding binary trees up to size 3 (Fig-
ure 2). In the ﬁrst iteration, the invocation of nextPermutation
3Iteration 1 size = 0 size = 1 size = 2 size = 3
pool
lastRoundTestsnull btSize = 0, #0
thisRoundTests /c31/c30/c29/c28 /c24/c25/c26/c27btSize = 1, #1
/d1/d1✄✄ /d29/d29❀❀
Iteration 2 size = 0 size = 1 size = 2 size = 3
poolbtSize = 0, #0
lastRoundTests /c31/c30/c29/c28 /c24/c25/c26/c27btSize = 1, #1
/d1/d1✄✄ /d29/d29❀❀
thisRoundTests /c31/c30/c29/c28 /c24/c25/c26/c27btSize = 2, #2
/d2/d2✆✆ /d29/d29❁❁
/c31/c30/c29/c28 /c24/c25/c26/c27
/d1/d1✄✄ /d29/d29❀❀/c31/c30/c29/c28 /c24/c25/c26/c27btSize= 2, #3
/d1/d1✂✂ /d28/d28✾✾
/c31/c30/c29/c28 /c24/c25/c26/c27
/d1/d1✄✄ /d29/d29❀❀/c31/c30/c29/c28 /c24/c25/c26/c27btSize= 3, #4
/d121/d121sss/d37/d37❑❑❑
/c31/c30/c29/c28 /c24/c25/c26/c27
/d1/d1✄✄ /d29/d29❀❀/c31/c30/c29/c28 /c24/c25/c26/c27
/d1/d1✄✄ /d29/d29❀❀
Iteration 3 size = 0 size = 1 size = 2 size = 3
poolbtSize = 0, #0 /c31/c30/c29/c28 /c24/c25/c26/c27btSize = 1, #1
/d1/d1✄✄ /d29/d29❀❀
lastRoundTests /c31/c30/c29/c28 /c24/c25/c26/c27btSize = 2, #2
/d2/d2✆✆ /d29/d29❁❁
/c31/c30/c29/c28 /c24/c25/c26/c27
/d1/d1✄✄ /d29/d29❀❀/c31/c30/c29/c28 /c24/c25/c26/c27btSize= 2, #3
/d1/d1✂✂ /d28/d28✾✾
/c31/c30/c29/c28 /c24/c25/c26/c27
/d1/d1✄✄ /d29/d29❀❀/c31/c30/c29/c28 /c24/c25/c26/c27btSize= 3, #4
/d121/d121sss/d37/d37❑❑❑
/c31/c30/c29/c28 /c24/c25/c26/c27
/d1/d1✄✄ /d29/d29❀❀/c31/c30/c29/c28 /c24/c25/c26/c27
/d1/d1✄✄ /d29/d29❀❀
thisRoundTests /c31/c30/c29/c28 /c24/c25/c26/c27btSize = 3, #5
/d1/d1✂✂ /d29/d29❁❁
/c31/c30/c29/c28 /c24/c25/c26/c27
/d1/d1✂✂ /d28/d28✾✾
/c31/c30/c29/c28 /c24/c25/c26/c27
/d1/d1✄✄ /d29/d29❀❀/c31/c30/c29/c28 /c24/c25/c26/c27btSize= 3, #6
/d2/d2✆✆ /d28/d28✾✾
/c31/c30/c29/c28 /c24/c25/c26/c27
/d2/d2✆✆ /d29/d29❁❁
/c31/c30/c29/c28
/c24/c25/c26/c27
/d1/d1✄✄ /d29/d29❀❀/c31/c30/c29/c28 /c24/c25/c26/c27btSize= 3, #7
/d1/d1✂✂ /d29/d29❁❁
/c31/c30/c29/c28 /c24/c25/c26/c27
/d2/d2✆✆ /d29/d29❁❁
/c31/c30/c29/c28 /c24/c25/c26/c27
/d1/d1✄✄ /d29/d29❀❀/c31/c30/c29/c28 /c24/c25/c26/c27btSize= 3, #8
/d2/d2✆✆ /d28/d28✾✾
/c31/c30/c29/c28 /c24/c25/c26/c27
/d1/d1✂✂ /d28/d28✾✾
/c31/c30/c29/c28 /c24/c25/c26/c27
/d1/d1✄✄ /d29/d29❀❀
Figure 2: Finding binary trees up to size 3. Trees are numbered using # in the order they are generated.
returns
different permutations of r(i.e., two) formerly generated bi-
nary trees such that at least one of them is from lastRoundTests.
Here there is only one option: null for both right and left children.
The total size of the binary trees in recursives is0 + 0 = 0 ,
which is still less than 3, so no pruning happens at this point. Fur-
thermore, s= 1 implies that there is one ﬁeld ( btSize) for which
we should systematically search all values in the scope. We build
candidates by assigning null as both children and exploring dif-
ferent values for btSize ranging from 0 to 3 (these numbers come
from the scope). Each of these candidates is sent to repOK, and
the one with btSize correctly set to 1 returns true. Upon receiving
true from repOK, the new test is saved in thisRoundTests and
the algorithm continues until all permutations and valuations are
exhausted. At the end of this round, null moves topool and tree 1
moves tolastRoundTests. The algorithm continues in the same
manner. In the third iteration, ﬁrst, binary trees 2 and 3 are com-
bined with binary tree 0 ( null) to generate four binary trees of size
3. Then, an example of pruning occurs. Once the algorithm selects
tree 2 from lastRoundTests and tree 1 from pool, the check
on the size of recursives indicates that the resulting candidate
would be outside the scope. Therefore, nextPermutationPruning
is called to ﬁnd the next permutation of recursives that is inside
the scope. Because all other permutations are bigger in size, they
are pruned. For brevity, we do not show iteration 4, wherein no
new test is generated. The algorithm terminates at the beginning of
iteration 5, when all valid inputs are in pool.
3.2.2 Random Generation
Our use of dynamic programming is orthogonal to random or
exhaustive test generation and can be applied to both. In order to
generate random tests, we introduce randomization into the process
of saving valid inputs. For random test generation, randomIsOn()
returns true on line 22 of Listing 3. Therefore, saving or dis-
carding a correct test input depends on the value returned from
coinToss(size(recursives)). This method heuristically re-
turns true for all small inputs (inputs with a size less than a thresh-old parameter) in order to save them all, and uses a random num-
ber generator (a coin toss) with a ﬁxed probability of success to
randomly save or discard other correct test inputs. As the algo-
rithm continues to execute, discarded tests do not take part in test
generation. At the end, the algorithm generates one or several ran-
dom tests of a given size. Keeping all small inputs (for example
the single-node binary tree) helps in reducing the chance of hav-
ing repetitive patterns in one random test. In the case of generating
multiple tests, if sharing structures between tests is undesirable, one
could run the algorithm from scratch multiple times. As Section 4
shows, our algorithms are efﬁcient enough to generate big inputs in
a matter of seconds and one can run them several times.
3.2.3 LazyDP
One problem that arises during the test generation is the limita-
tion of other resources beyond computational time, such as mem-
ory. If we keep tests as objects, we run out of the heap memory
space for bigger scopes. As Listing 3 shows, to optimize memory
usage, tests are saved in the compact form of an array of integers.
These integers are either indexes of smaller substructures, which
in turn point to other arrays of integers, or values of non-recursive
ﬁelds. For example, tree 5 is saved as {(right child =) 3, (left child
=) 0, (btSize =) 3} where tree 3 is in turn kept as {(right child
=) 1, (left child =) 0, (btSize =) 2}. The value of a non-recursive
ﬁeld is saved as an index with respect to the scope. For example, if
there is a ﬁeld with primitive type boolean whose values are false
and true in the scope, 0 represents false and 1 represents true.
Whenever we create a bigger candidate test input using smaller
previously generated tests, we need to call repOK to examine the
correctness of the candidate. Consequently, we need to retrieve
the smaller tests, build their corresponding objects, and then utilize
them to build the candidate. We build the smaller tests with the
lazy initialization technique, which means that the substructures
get initialized (i.e., expanded from arrays of integers to objects)
only when repOK accesses a ﬁeld from them.
For binary trees, since repOK of Listing 1 only accesses btSize
4of the direct children of a node, we only expand the direct children
to
objects and keep their children as arrays. For example, the ex-
pansion of binary tree 5, is limited to expanding binary trees 3 and
0, and not binary tree 1, which is a child of binary tree 3.
3.2.4 SymboLazyDP
While both dynamic programming and lazy initialization im-
prove ﬁnding the recursive values of a candidate, in order to ﬁnd
the correct values for its non-recursive ﬁelds (e.g., btSize), we
still need to search all valuations, which diminishes the efﬁciency
and scalability of test generation. To avoid such a search, we ob-
serve that the values of many non-recursive ﬁelds of a test can be
symbolically computed rather than searched for. Following previ-
ous work [27, 26, 46] we use symbolic execution for non-recursive
ﬁelds. Using symbols (instead of concrete values) for ﬁelds, we
build a path condition (a boolean formula over the symbols which
represents the constraints that should be satisﬁed to follow a path)
while executing repOK. At the end of the repOK execution, we use
a constraint solver to solve the path condition and calculate con-
crete values for non-recursive ﬁelds.
In order to enable symbolic execution, we use a source to source
instrumentation on the repOK method [26]. We replace each branch
condition with a boolean variable which takes both true and false
values. When it takes the true value, we add the original branch
condition to the path condition. When it takes the false value, we
add the negation of the original condition. All valuations of such
boolean variables provide all execution paths. Listing 4 shows the
corresponding instrumentation for btSize (replacing the last two
lines of Listing 1). Upon reaching a return statement, we invoke a
constraint solver to solve the path condition and consider each so-
lution as an acceptable valuation for the non-recursive ﬁelds. For
example, btSize of a candidate binary tree is assigned through
solving the condition added on line 20 of Listing 4.
We save the path condition with each valid substructure, but do
not save the solution; i.e., substructures are saved with symbolic
non-recursive ﬁelds and constraints on them. After combining the
substructures, we perform symbolic execution on the entire can-
didate, including the recursive calls, because solving the substruc-
tures separately does not necessarily give compatible results (e.g.,
consider solving the search constraint on integer elements of a bi-
nary search tree; it is possible to save valid integer elements in the
right and left children that violate the search constraint at the root,
making the constraint infeasible).
19i f ( g e t B o o l e a n ( ) ) {
20 addCond ( " b t S i z e " , EQ, " r i g h t B t S i z e + l e f t B t S i z e +1 " ) ;
21 return tr u e ;
22}e l s e {
23 addCond ( " b t S i z e " , NOTEQ, " r i g h t B t S i z e + l e f t B t S i z e +1 " ) ;
24 return f a l s e ; }
Listing 4: Instrumenting BinaryTree for symbolic execution.
3.3
Theorem
In this section, we prove that the DP algorithm generates all valid
tests in the scope, and it does not generated any valid or invalid
candidate more than once.
DeﬁneSRas the value of set S(e.g.,pool) at the end of round
R. Furthermore, deﬁne discarded Ras the set of candidates dis-
carded during
roundR, including those outside the scope.
DEFINITION 1.Visited candidates: visitedCands R=poolR∪
lastRoundTests R∪/uniontextR
i=1discarded i.
DEFI
NITION 2.T(C): For a candidate C, letT(C)be the set
of all test inputs that Cuses as its recursive substructures.LEMMA 1.At the end of each round R(R≥0), and for any
candidate C, the following loop invariant holds:
T(C)⊆poolR→C∈visitedCands R
PROOF BY INDUCTION . The base case is the beginning of round
one, were pool0=∅. The only candidate that does not use any
recursi
ve substructures is null. Yet null∈lastRoundTests 0
and hence null∈vi sitedCands 0, so the invariant holds.
For
the induction step, assume that:
T(C)⊆poolR→C∈visitedCands R (1)
At the
end of round R+1, the tests in lastRoundTests joinpool ,
and then the tests in thisRoundTests replacelastRoundTests.
poolR+1=poolR∪lastRoundTests R (2)
lastRoundTests R+1=thisRoundTests R+1 (3)
Suppose that
the invariant does not hold at the end of round R+1.
∃C′:T(C′)⊆poolR+1∧C′/∈visitedCands R+1 (4)
Now
,T(C′)∩lastRoundTests Ris either (a) =∅or (b)/negationslash=∅ .
For case (a): (2)∧(4)∧(a)→T(C′)⊆poolR (5)
(1
)∧(5)→C′∈visitedCands R (6)
De
finition 1∧(2)∧(6)→C′∈visitedCands R+1 (7)
which contradicts
with (4).
For case (b):
∃t∈T(C′) :t∈lastRoundTests R (8)
Note that
the algorithm generates all permutations of the tests be-
longing to lastRoundTests orpool that have at least one test
fromlastRoundTests, i.e.,
(T(C)⊆poolR∪lastRoundTests R
∧∃t∈T(C) :t∈lastRoundTests R)
↔C∈thisRoundTests R+1∪discarded R+1 (9)
(2
)∧(4)∧(8)∧(9) →C′∈thisRoundTests R+1∪discarded R+1
(10)
De
finition1 ∧(3)∧(10)→C′∈visitedCands R+1 (11)
which ag
ain contradicts with (4).
TH EOREM 1.Part 1: for a given scope, the algorithm gener-
ates all valid tests. Part 2: the algorithm does not generate any
(valid or invalid) candidate more than once.
PROOF OF PART 1. LetC′′be the smallest correct test that the
algorithm f
ails to generate. If the algorithm terminates right after
roundfin:repOK(C′′) =true∧C′′/∈visitedCands f in (12)
∀C:re
pOK(C) =true∧C /∈visitedCands fin
→size(C)≥size(C′′) (13)
∀t∈T(C′′) :size(t)< size(C′′) (14)
∀t∈T(C′′) :repOK( t) =true (15)
(13)∧(14)∧(15)→∀t∈T(C′′):t∈visitedCands f in (16)
Further,
the termination of the algorithm at the end of round fin
means that no progress was made in this round. Accordingly, no
test was added to thisRoundTests fin. By using (3)1:
lastRoundTests fin=thisRoundTests fin=∅
→vi
sitedCands fin=poolfin∪fin/uniondisplay
i=1discarded i (17)
1fin >0since the test generation loop executes at least once.
5Now, let us assume that C′′is inside the scope.
si
ze(C′′)≤scope.size (18)
(
14)∧(18)→ ∀t∈T(C′′) :size(t)≤s cope.size (19)
From the deﬁnition of discarded, we have:
∀C∈discarded R→repOK( C)/negationslash=true∨size(C)>scope.size
(20)
(15)∧(19)∧(20)→ ∀t∈T(C′′) :t /∈f
in/uniondisplay
i=1discarded i(21)
(1
6)∧(17)∧(21)→ ∀t∈T(C′′) :t∈poolfin (22)
Lemma1∧(
22)→C′′∈visitedCands f in (23)
But (23)
contradicts with (12), which proves part 1.
PR OOF OF PART 2BY INDUCTION . Consider R= 0for the in-
duction base. Because only one instance of null is generated be-
fore the ﬁrst round, no repetition happens at R= 0.
For the induction step, assume that all candidates visited up to
the end of round Rare distinct. We use /negationslash≡to show that two can-
didates are different instances, although they might be equal (=).
∄t,t′∈visitedCands R:t/negationslash≡t′∧t=t′(24)
Suppose that
the ﬁrst repetitious candidate, named C, is generated
during round R+1.
C∈thisRoundTests R+1∪discarded R+1 (25)
Cis repetitious,
so another instance of it, named C′, is already
generated at
roundR′2.
C′∈thisRoundTests R′∪discarded R′ (26)
(9
)∧(25)→T(C)⊆poolR∪lastRoundTests R (27)
(9
)∧(26)→T(C′)⊆poolR′−1∪lastRoundTests R′−1(28)
Either (a)R′=R+1or (b) R′< R+1. For case (a), note that
during one round of test generation, methods nextPermutation
andnextPermutationPruning provide distinct permutations. So
in order to have repetitive candidates, at least one of the substruc-
tures should have more than one instance.
∃t∈T(C),t′∈T(C′) :t/negationslash≡t′∧t=t′(29)
(2
7)∧(28)∧(29)∧(a)→∃t,t′∈visitedCands R:t/negationslash≡t′∧t=t′
(30)
which contradicts
with (24).
For case (b), notice that at least one substructure is selected from
lastRoundTests.
(9)∧(25)∧C=C′→∃t∈T(C′):t∈lastRoundTests R(31)
(2
8)∧(31)→∃t∈lastRoundTests R∩visitedCands R′−1(32)
(2
4)∧(32)→R≤R′−1 (33)
which contradicts
with (b).
4. EV ALUATION
In
order to evaluate our test generation methods, we implemented
a prototype of the algorithms and designed some experiments wherein
we address two research questions:
•RQ1 How efﬁcient and scalable are our algorithms, com-
pared to state-of-the-art test generation tools (Pex and Korat)?
2Thealgorithm never generates null again, soR′/negationslash= 0.•RQ2 How
effective are the generated tests in ﬁnding bugs in
real world applications (Chrome and Safari web browsers)?
In the ﬁrst set of experiments, we used six microbenchmarks,
which are complex data structures widely used in programs and as
test inputs. Previous work has extensively used these benchmarks
for evaluation [42, 39, 45, 41, 5, 16, 32]. In order to answer the ﬁrst
question, we need an alternative exploration method of the state
space of possible test inputs. A naive exploration of the state space
will give rather unacceptable results. Therefore, we compare our
methods to Microsoft Pex [46] – a state-of-the-art test generation
tool for .Net – and Korat [5] – a well-known test generation method
and an open source tool [37] for Java programs.
Pex is a white-box test generation tool that performs symbolic
execution. In addition, it uses path-bounded model-checking to
cover different paths in the program. Pex is an appropriate sub-
ject tool; it particularly addresses the effect of symbolic execution.
Generation of test inputs considered in this paper is black-box with
respect to the code under test, yet we allow Pex to explore differ-
ent paths in repOK. We used the same repOK methods for Pex and
Korat, except for minor changes to accommodate syntactic differ-
ences between C# and Java respectively.
The Korat algorithm monitors repOK executions to prune large
portions of the bounded space of candidate structures. Korat is an
appropriate subject tool too; previous work [43] shows that Korat is
among the most efﬁcient solvers for complex structural constraints.
In addition to exhaustively generating test inputs, we compare
the efﬁciency and scalability of our algorithms with Pex and Korat,
when sampling a few large test inputs.
In the second set of experiments, we show how to naturally model
HTML and CSS3 ﬁles as acyclic data structures. Such ﬁles, which
are test inputs to any web browser, are examples of practical and
common, yet user-deﬁned test inputs. By systematically generat-
ing HTML and CSS3 test inputs, our generation methods found
real bugs in the latest versions of two well-known web browsers,
Google Chrome and Apple Safari.
4.1 Experimental Settings
Throughout the evaluation, we ran each experiment ﬁve times
and reported the averages. All experiments used a 2.50GHz Core
2 Duo processor with 4.00GB RAM running Windows 7. We used
Sun’s Java SDK 1.6.0 JVM with our methods and Korat, and Mi-
crosoft Visual C# 2010 version 4.0.30319 RTMRel with Pex ver-
sion 0.94.51006.1. Pex used Z3 theorem prover [13] version 2.0. In
Section 4.4, we used Google Chrome version 13.0.782.220 m for
Windows and Apple Safari 5.1 (7534.50), the latest versions as of
the date of this paper.
For symbolic execution, we used our in-house constraint solver
developed in Java. The source to source instrumentation for sym-
bolic execution is currently manual, but is mechanical and can be
automated [26]. For exhaustive test generation with Pex, we used
the following setting to force Pex to generate all test inputs:
TestEmissionFilter = PexTestEmissionFilter.All
4.2 Microbenchmarks
To address RQ1 for exhaustive test generation, we considered
six microbenchmarks. Table 1 shows the results for the biggest
sizes considered. For all six microbenchmarks and all sizes con-
sidered (including those not shown), DP and LazyDP generate the
same number of tests as Korat. SymboLazyDP and Pex generate
the same number of tests, since they both use symbolic execution
and report one solution for each path condition, instead of explor-
ing all valuations from the state space. In addition to improving
test generation performance and scalability, symbolic execution im-
6Table 1: Exhaustive test generation for the biggest sizes. TO represents a time out of 1000s. Best performance highlighted.
Benchmark
Biggest Valid Tests Candidates Generation Time (s) State
Size Korat/DP/ Symbo- Pex Korat DP/ Symbo- Pex Korat DP LazyDP Symbo- Pex Space
LazyDP
LazyDP LazyDP LazyDP LazyDP
LinkedList 17 131072 18 18 17825963 2228208 34 36 33.188 7.089 3.491 0.025 9.195 1041
BinaryTree 12 290512 290512 TO 15770974 3776644 581023 TO 51.533 17.524 7.947 7.416 TO1026
RedBlackTree 9 6753 271 TO 2207699 610217461 445227 TO 8.728 962.483 TO 4.590 TO1026
FibonacciHeap 6 1125139 197 TO 1364398 2101957 3190 TO 5.654 10.254 16.035 0.078 TO1014
BinaryHeap 8 344571 9 9 24900165 186644641 3593 70 65.129 300.862 283.099 0.075 9.776 1021
HashTable 12 4083 233 TO 11098075 31953529 4176 TO 51.043 75.834 56.740 0.159 TO1038
1.0010.00100.001,000.00
0.000.01
10 11 12 13 14 15 16 17
Size of Generated Tests (SortedSinlyLinkedList)
Korat DP LazyDP SymboLazyDP Pex1.0010.00100.001,000.00
0.000.01
5 6 7 8 9 10 11 12
Size of Generated Tests (BinaryTree)
Korat DP LazyDP SymboLazyDP Pex
1.0010.00100.001,000.00
0.000.01
2 3 4 5 6 7 8 9
Size of Generated Tests (RedBlackTree)
Korat DP LazyDP SymboLazyDP Pex1.0010.00100.001,000.00
0.000.01
1 2 3 4 5 6 7 8
Size of Generated Tests (FibonacciHeap)
Korat DP LazyDP SymboLazyDP Pex
1.0010.00100.001,000.00
0.000.01
1 2 3 4 5 6 7 8
Size of Generated Tests (BinaryHeap)
Korat DP LazyDP SymboLazyDP Pex1.0010.00100.001,000.00
0.000.01
5 6 7 8 9 10 11 12
Size of Generated Tests (HashTable)
Korat DP LazyDP SymboLazyDP Pex
Figure 3: Performance comparison on microbenchmarks.
pro
vestest execution by reporting less, yet representative test in-
puts. SymboLazyDP is the most efﬁcient generation method for all
the microbenchmarks on the biggest size.
Figure 3 shows the performance evaluation results. The ﬁrst data
structure is a sorted singly-linked list of integer elements. Here,
DP and LazyDP constantly outperform Korat. Lazy initialization
is effective here, especially because properties like being sorted are
veriﬁed locally: it sufﬁces to compare each node’s element with
its neighbor’s. Pex outperforms DP and LazyDP on bigger sizes
for two data structures: singly-linked list and binary heap. The
reason is that Pex generates far fewer tests by symbolically exe-
cuting repOK and representing all sorted lists of a given size with
only one test, while Korat, DP, and LazyDP exhaust different valua-
tions. SymboLazyDP, however, shows the best performance among
all the methods. It generates the same number of tests and growswith the same pace as Pex, but it is multiple times faster. Notice
that the vertical axis is logarithmic.
The next benchmark is a binary tree as described in Section 2. As
Figure 3 displays, SymboLazyDP performs the best. Pex can only
enumerate all binary trees up to size 7 before timing out. We also
experimented with red-black trees with size, key and color. While
DP and LazyDP are faster than Korat at ﬁrst, Korat takes over them
at some point because DP and LazyDP have to explore all valu-
ations of key and color, but Korat prunes many of them. Notice
that LazyDP takes slightly more time than DP on this benchmark.
This is because repOK needs to explore down the tree to ﬁnd min-
imum and maximum keys to evaluate the search tree property, and
eventually expands many substructures, which undermines the us-
age of lazy initialization. Pex uses symbolic execution for key and
color, but cannot generate all red-black trees with four nodes or
7Table 2: Random generation of ten tests with 90 ≤size≤100. TO represents a time out of 1000s. Best performance highlighted.
Benchmark Valid Tests Candidates Generation Time (s) State
Korat SymboLazyDP Pex Korat SymboLazyDP Pex Korat SymboLazyDP Pex Space
LinkedList 10 10 10 8110 197 198 0.137 0.136 32210351
BinaryTree 10 10 10 8746 11615 602 0.266 0.174 37510353
RedBlackTree TO 10 TO TO 2350732 TO TO 20.256 TO10529
FibonacciHeap 10 10 TO 4106 54624 TO 0.114 0.567 TO 10527
BinaryHeap 10 10 10 343300 467522 153 7.617 2.823 8310527
HashTable TO 10 TO TO 153508 TO TO 3.947 TO10527
1,00010,000100,0001,000,00010,000,000
Korat
DP , LazyDP
110100
2 3 4 5 6 7 8 9
Size of Generated Tests
Figure 4: Memory usage for RedBlackTree .
more. SymboLazyDP, using dynamic programming and symbolic
execution, closely competes with Korat and eventually takes over.
Figure 4 compares the memory usage of our methods and Korat
for red-black trees. DP and LazyDP generate the same number of
tests, so they always use equal amounts of memory. SymboLazyDP
generates less tests and hence uses less memory. Korat keeps only
one candidate vector while we have to keep all correct tests (albeit
in a compact format). Hence, the memory usage of our methods
grows faster than Korat. However, since the Java heap space is
usually in the order of a few GB’s, 10MB of memory usage should
not be a problem. The memory usage of the other benchmarks
follows a very similar pattern.
The next benchmark is a Fibonacci heap. This is the only case
where the memory usage is a concern for DP and LazyDP. In fact,
for Fibonacci heaps with more than six nodes, DP and LazyDP
run out of the heap space and Korat and Pex run out of time. Yet,
SymboLazyDP does not time out and is the most efﬁcient. The
next benchmark, a binary heap, gives results similar to the sorted
singly-linked list. SymboLazyDP is the most efﬁcient.
In the last benchmark, a hash table implemented using nested
lists, LazyDP and DP are slightly better than Korat. The difference
increases when a time-consuming hash function is used. Since we
use previously generated tests, we avoid many calls to the hash
function. The generation time for Pex starts off at a bigger value
and increases at the same pace as SymboLazyDP. SymboLazyDP
outperforms all the other methods.
Finally, it is worth mentioning that Pex and SymboLazyDP both
require path bounds (e.g., for loop unrolling). Throughout the ex-
periments, we used trial and error to ﬁnd and set the smallest bounds
that provide all tests. Similarly, Korat, DP, and LazyDP need a
bound on primitive (e.g., integer) values. The number of primitives
used usually has a relationship with the test size.
4.3 Random Test Generation
To address RQ1 for random test generation, we considered gen-
erating ten random tests of ninety to a hundred nodes (Table 2).
For Korat and Pex, we took the ﬁrst ten tests generated in the de-
sired size range. For SymboLazyDP, all small tests (up to size
3) were saved and bigger tests were saved or discarded at ran-
dom (Section 3.2.2). Except for the Fibonacci heap benchmark,
SymboLazyDP is the most efﬁcient and scalable.4.4 Google Chrome and Apple Safari
To address RQ2 and showcase the ability of our methods in ﬁnd-
ing bugs in real world, well-tested3, commercial applications, we
tested the
support for rendering CSS3 3D effects by Chrome and
Safari web browsers. CSS (Cascading Style Sheets) is a style sheet
language that separates the presentation of a markup language doc-
ument from its content, and is commonly used to style web pages
written in HTML and XHTML. CSS3, the latest variation of CSS,
enables web developers to add 3D effects to web pages, i.e., posi-
tion and move elements in the three dimensional space.
Apple Safari and Google Chrome web browsers support CSS3
3D effects. As of the date of this paper, Chrome and Safari are
the third and fourth most widely used desktop web browsers with
21.5% and 4.8% worldwide usage share respectively [3]. Safari is
developed in C++ and Objective-C, and precedes Chrome in sup-
porting 3D transforms. Chrome is developed in C++, Assembly,
Python, and JavaScript. Both of these browsers use Webkit layout
engine which introduced 3D transforms in CSS.
We directly tested 11 KLOC of C++ code (74 .cc/.h ﬁles) from
Chrome. Safari is 37 MB compiled. Our test for Safari included
2.7 KLOC (19 .cpp/.h ﬁles) of its open source code plus its closed
source implementation.
4.4.1 Modeling HTML and CSS Test Inputs
An HTML ﬁle is composed of a set of nested HTML elements.
An HTML element includes a start tag (e.g., <h1>) and an end tag
(e.g.,</h1>). The start tag might also have some attributes (e.g.,
class="ClassName"). We modeled an instance of an HTML ﬁle as
a tree. The whole document is contained between <html>start
and end tags, which we consider as the root of the tree. Further,
each tag is represented as a node that has some attributes and an
ordered set of children, which are the tags immediately inside it.
Listing 2 shows some parts of the HTML model. Figure 1 shows
the tree representation of Listing 7.
1s e l e c t o r 1 {
2 p r o p e r t y 1 : v a l u e 1 1 [ v a l u e 1 2 . . . ] ;
3 [ p r o p e r t y 2 : v a l u e 1 2 [ v a l u e 2 2 . . . ] ;
4 . . . ] }
Listing 5: Abstraction of a CSS rule.
A
CSS ﬁle consists of a list of rules. A rule has a selector and a
declaration block. Inside a block, each declaration has a property,
followed by a list of values. Listing 5 shows an abstraction of a CSS
rule. We modeled each CSS rule as a linked list of alphabetically
sorted4properties where each property has a linked list of values.
Asone
could see, our HTML and CSS models are intuitive and
easy to implement as recursive loop-free data structures. Indeed,
we have already implemented both of them as microbenchmarks.
3For example, Chrome is extensively tested before release and
claims to
pass 99% of WebKit’s layout tests [2]. The CSS3 3D
effects are among the WebKit’s layouts.
4Because the order of properties is irrelevant, we keep them sorted
toa
void duplicates.
81. ClassName4 {
2−webkit−t r a n s f o r m : r o t a t e Y ( 180 deg ) ; }
3. ClassName12 {
4−webkit−p e r s p e c t i v e : 80 0;
5−webkit−bac k fa c e −v i s i b i l i t y : h i d d e n ; }
Listing 6: An automatically generated CSS test input (ﬁle.css).
1<h tml >
2 <head >
3 <l i n k r e l =" s t y l e s h e e t " type =" t e x t / c s s " h re f =" f i l e .
c s s ">< / l i n k >< /head >
4 <body >
5 <div c l a s s =" ClassName4 ">
6 <h1> Thi s i s some t e x t
7 <div c l a s s =" ClassName12 ">
8 <h1> T hi s i s some t e x t < / h1>< / div>< /h1>< / div
>< /body >< /html >
Listing 7: An automatically generated HTML test input.
Listings
6 and 7 show bug-revealing examples of HTML and
CSS inputs, automatically generated by our methods.
4.4.2 Experimental Results
Using the above models, we systematically generated all test
inputs with up to eight tags (two <div>tags) inside an HTML
ﬁle and two declarations inside a CSS declaration block. Five
CSS properties were used: perspective, backface-visibility, trans-
form, transform-origin, and transform-style. Also, various values
for these properties were used including perspective, rotate, scale,
skew, and translate for the transform property. Each HTML tag
could have a CSS selector as its class attribute (See Listings 6 and 7
as an example). Consistency constraints between CSS and HTML
ﬁles are maintained by ﬁrst running the CSS input generator and
then feeding the number of classes it generates to the HTML input
generator to exhaustively cover all classes.
Table 3 shows a summary of the results. In total, 3081 test inputs
(each including an HTML ﬁle with the corresponding CSS ﬁle)
were generated. The size of the input space for the chosen bounds is
1010. The size bound in this experiment is too small to get LazyDP
beneﬁts. Ho
wever, SymboLazyDP gives a clear advantage. In our
model, SymboLazyDP is not applicable on CSS inputs for the lack
of non-recursive ﬁelds that can be executed symbolically.
4.4.3 Differential Testing
So far, we have automatically generated the test inputs. But in
order to test Chrome and Safari with these tests, we need an ora-
cle that deﬁnes the correct rendered output for any given test input.
Since no such oracle was available, we use differential testing [34],
where the outputs of two implementations are checked against each
other. Whenever the outputs are not the same, there likely is a bug
in at least one of the implementations. We wrote a test harness
in Java that automatically launches Chrome and Safari with each
test input, and performs a basic image differencing algorithm to
compare the screen shots taken from them. All test inputs were
checked in less than 2 hours. Such time-consuming checks are spe-
ciﬁc to this application. Furthermore, improving the performance
of launching the browsers and image differencing is possible, yet
beyond the scope of this paper.
4.4.4 Bugs Found
Among the 3081 tests generated, 818 tests were rendered differ-
ently by Chrome and Safari. We semi-automatically investigated
these tests. Out of these 818 failures, 148 cases were false posi-
tive due to the inaccuracy of our image differencing algorithm. We
manually classiﬁed the rest of the failing tests (670 tests) based on
the CSS properties used, and found at least three distinct bugs inTable 3: Chrome and Safari test input generation results.
Candidates Gen. Time (s) #Tests
CSS HTML CSS HTML
DP 10,231 3,815,626 0.140 10.851 3081
LazyDP 10,231 3,815,626 0.140 10.850 3081
SymboLazyDP N/A 116,766 N/A 1.628 3081
the production code, stable releases of Chrome and Safari. The ac-
tual
number of faults in the code, which produce these failures, in
fact, may be greater. However, localizing the faults was not possi-
ble due to the proprietary code of Safari. We reported three bugs
in Chrome. One of these bugs was regarding the hidden backface-
visibility of an element. (Listings 6 and 7 reveal this bug.) Another
bug involved the webkit-perspective property. Both of these bugs
in Chrome were already reported and conﬁrmed (issue 76947 in the
Chromium project) and are ﬁxed in the next Canary release. The
last bug was due to a rotation direction inconsistency with the W3C
editor’s draft (21 March 2011) [1] as the standardization in progress
of CSS 3D transforms. This bug is not conﬁrmed by Google Inc.
as of the date of this paper.
Consider Listings 6 and 7. To reveal this bug, we need two nested
classes where the outer one has a 180 degree rotation and the inner
one has a hidden visibility as well as the webkit-perspective prop-
erty. Invoking the hidden visibility by itself or in any other setting
is not enough to show the problem.
4.4.5 Applying Symbolic Execution and Korat
We strove to use symbolic execution on the source code available
from Chrome. The corresponding code, however, includes 74 .cc/.h
ﬁles (11 KLOC of code) that collectively render a CSS 3D effect.
We were unable to apply white box symbolic execution due to the
code size and complexity. Symbolic execution is not feasible for
testing closed source systems (Safari). Korat can, in principle, ﬁnd
the bugs if given enough time. Yet, as we showed, our technique
outperforms Korat in all the cases of exhaustive test generation.
4.5 Threats to Validity
Internal validity. (1) To implement our algorithms for test genera-
tion using dynamic programming, we strictly followed the original
algorithms, used well-known libraries, and validated the number
of inputs generated to match the numbers generated by other inde-
pendently developed tools, namely Korat and Pex. (2) To compare
with Korat, we used its open-source implementation that has been
in the public domain for over ﬁve years, and used the repOK’s that
are distributed with it. (3) To compare with Pex, we used its public
distribution (version 0.94.51006.1) while setting the search depth
bound to the smallest number required to complete the generation
of all inputs within the chosen size in order to minimize the explo-
ration time for Pex. We carefully performed a faithful translation
of Java repOK’s used for Korat into C# to run Pex. We used our
own in-house constraint solver developed in Java for symbolic exe-
cution with dynamic programming, which might give different per-
formance results compared to Z3 [13] used by Pex. Although it is
unlikely that our solver in Java is faster than the state-of-the-art Z3,
to address this threat, we plan to use Z3 as a part of future work.
External validity. The main threat here involves using only two
industrial programs (Chrome and Safari). To address this threat,
we experimented with microbenchmarks that have previously been
used by a number of other authors [42, 39, 45, 41, 5, 16, 32].
Construct validity. We used metrics commonly used in software
testing research to compare test generation tools, and automated
our entire test generation and execution process. Furthermore, we
manually investigated the failures reported for the browser testing.
95. APPLICABILITY
Our
work directly enables systematic (i.e., bounded exhaustive)
testing to scale better for certain applications, e.g., refactoring en-
gines, compilers, model checkers, and browsers, which clearly must
be tested against larger inputs. The inputs to these applications are
programs themselves, which can be modeled and generated at the
Abstract Syntax Tree level – an acyclic structure – using structural
constraints. E.g., Alloy programs were modeled and generated to
ﬁnd bugs in Alloy-alpha [32]; more recently, systematic testing
found bugs in Eclipse, NetBeans, Sun javac, and JPF [16, 12].
More generally, our work can help systematic grammar-based
testing techniques [33, 30]. Such techniques enumerate all strings,
up to a given bound, that belong to a context-free grammar. Context-
free grammars can describe various input types, such as XML schemas
and programs. To illustrate, consider the work of Khalek et al. [25],
which uses constraint-based testing to reveal bugs in Oracle 11g. It
enumerates solutions for a subset of SQL grammar and a schema
to provide queries and populate the database. Our technique has
a direct application in generating strings that belong to the SQL
grammar, and can also improve the generation of tabular test data.
Our use of dynamic programming is not limited to bounded ex-
haustive generation, rather our technique also facilitates random
test generation, which complements systematic testing and has also
been used successfully to ﬁnd bugs [15, 8, 39, 19]. Most recently,
Yang et al. used random test generation to ﬁnd numerous bugs in
mainstream C compilers [49]. As our work shows, dynamic pro-
gramming can be used in synergy with random test generation.
While we describe algorithms for generating recursive structures
without cycles, our approach can be used to generate cyclic struc-
tures as a part of a multi-step generation technique. For example,
we can generate an acyclic backbone in the ﬁrst step using dynamic
programming and populate the remaining ﬁelds using constraint-
based data structure repair [14] in the second step.
6. RELATED WORK
The importance of using speciﬁcations in testing has long been
recognized [20]. Several projects automate test generation from
speciﬁcations in various languages [23, 38]. The speciﬁc use of
logical constraints to represent inputs dates back at least three decades
[9, 21, 24, 29, 40]. But a focus of prior work has been to solve
constraints on primitives, and not on complex structures – which
require very different constraint solving techniques. Korat [5] and
TestEra [32] are among the ﬁrst frameworks to provide systematic
generation of structurally complex tests from constraints. Follow-
ing this spirit of systematic black-box testing, ASTGen [12] and
UDITA [16] are two more recent frameworks, which have been
used successfully to ﬁnd bugs in real applications, including refac-
toring engines. ASTGen requires the user to write imperative test
input generators, whose executions produce input programs for refac-
toring engines. ASTGen bears some similarities to our framework
in composing test generators to build bigger inputs. However, it is
limited to testing refactoring engines and requires the user to ex-
plicitly specify how to generate test inputs. UDITA provides a pro-
gramming language to describe test inputs using a combination of
declarative and imperative styles, where constraint solving is used
in conjunction with partial generators.
Lava [44] and QuickCheck [8] can also provide generation of
complex structures. Lava requires the user to describe inputs us-
ing a production grammar and generates strings in the grammar,
but cannot handle complex constraints, such as those of a red-black
tree. QuickCheck requires the user to write a generator for complex
inputs and generates random inputs for testing functional programsin Haskell using a technique similar to recursion with lazy initial-
ization. Similarly, Gast [28] generates tests for programs writ-
ten in functional languages. However, QuickCheck and Gast use
pure top-down recursion and not dynamic programming. While
dynamic programming has been used for monitor generation from
formal speciﬁcations [22], to our knowledge, there is no previous
work on using dynamic programming for test input generation.
Several tools use method sequences for testing object-oriented
programs, and can generate complex structures using systematic [48]
or randomized exploration [39]. While these tools allow unit test-
ing, they cannot feasibly generate inputs that are parsed from strings
with semantic and syntactic constraints, e.g., XML ﬁles, which
constraint-based test generation handles readily.
The recent advances in constraint solving technology [4, 13]
have led to a rebirth of symbolic execution [27, 9] – a powerful
program analysis technique that was traditionally used for check-
ing small programs with primitive types. Generalized symbolic ex-
ecution [26] implements Korat using the Java PathFinder model
checker [47] and supports structural constraints using symbolic ex-
ecution. Guiding symbolic execution using concrete executions is
rapidly gaining popularity as a means of scaling it up in several re-
cent frameworks, most notably DART [19], CUTE [42], EXE [7],
and Pex [46]. While DART and EXE focus on properties of primi-
tives and arrays to check for security bugs, such as buffer overﬂows,
CUTE and Pex support the use of preconditions in white-box test-
ing. Compositional techniques for symbolic execution, introduced
by PREﬁx and PREfast [6], can handle larger code bases but they
do not currently handle complex structural properties [18]. Our
work provides a novel way to scale symbolic execution by apply-
ing it with dynamic programming in synergy.
7. CONCLUSIONS
We presented a novel technique for exhaustive and random gen-
eration of test inputs for programs that operate on structurally com-
plex tests, e.g., recursive data structures. Our key insight is to lever-
age the recursive structure of desired inputs and partition the prob-
lem of generating an input into several sub-problems of generating
smaller inputs that exhibit the same structure, and to use dynamic
programming to combine them. We used a lazy initialization strat-
egy as well as symbolic execution to optimize the technique. We
formally proved the correctness of our algorithm. Experimental re-
sults show that our technique provides more efﬁcient and scalable
generation of structurally complex tests for a variety of subject pro-
grams, compared to state-of-the-art test generation tools Pex and
Korat. Furthermore, our technique found real bugs in well-tested
commercial applications Google Chrome and Apple Safari.
While in this paper we focus on generation of recursive data
structures, our work paves the way for development of novel tech-
niques for generating more general classes of inputs. To illustrate,
our technique can be used as a sub-routine to populate a recursive
component of a larger input and the other components could be
populated using complementary techniques. Given the increasing
use of constraint solving technology in software veriﬁcation, we
believe the time is ripe for dynamic programming to make a signif-
icant impact on our ability to ﬁnd more bugs faster and to deploy
more reliable software.
8. ACKNOWLEDGMENTS
We thank Mahdi Kefayati for helpful ideas. This work was funded
in part by the NSF under Grant No. CCF-0845628 and AFOSR
grant FA9550-09-1-0351.
109. REFERENCES
[1]
CSS 3D Transforms, Editor’s Draft 21 March 2011. http:
//dev.w3.org/csswg/css3-3d-transforms .
[2] Google Chrome.
http://www.google.com/googlebooks/chrome .
[3] Statcounter. http://statcounter.com .
[4] C. Barrett and S. Berezin. CVC Lite: A new implementation
of the cooperating validity checker. In CAV, pages 19–21,
2004.
[5] C. Boyapati, S. Khurshid, and D. Marinov. Korat:
Automated testing based on Java predicates. In ISSTA , pages
123–133, 2002.
[6] W. R. Bush, J. D. Pincus, and D. J. Sielaff. A static analyzer
for ﬁnding dynamic programming errors. Software—Practice
and Experience , 30(7), 2000.
[7] C. Cadar and D. Engler. Execution generated test cases: How
to make systems code crash itself. In SPIN Workshop, pages
902–902, 2005.
[8] K. Claessen and J. Hughes. Quickcheck: A lightweight tool
for random testing of Haskell programs. In ICFP, pages
268–279, 2000.
[9] L. Clarke. A system to generate test data and symbolically
execute programs. IEEE TSE, (3):215–222, 1976.
[10] T. H. Cormen, C. E. Leiserson, and R. L. Rivest. Introduction
to Algorithms . The MIT Press, 1990.
[11] C. Csallner, Y . Smaragdakis, and T. Xie. DSD-Crasher: A
hybrid analysis tool for bug ﬁnding. TOSEM , 17(2):8, 2008.
[12] B. Daniel, D. Dig, K. Garcia, and D. Marinov. Automated
testing of refactoring engines. In ESEC/FSE, pages 185–194,
2007.
[13] L. de Moura and N. Bjorner. Z3: An efﬁcient SMT solver. In
TACAS, pages 337–340, 2008.
[14] B. Elkarablieh, Y . Zayour, and S. Khurshid. Efﬁciently
generating structurally complex inputs with thousands of
objects. In ECOOP , pages 248–272, 2007.
[15] J. Forrester and B. Miller. An empirical study of the
robustness of Windows NT applications using random
testing. In USENIX, pages 59–68, 2000.
[16] M. Gligoric, T. Gvero, V . Jagannath, S. Khurshid, V . Kuncak,
and D. Marinov. Test generation through programming in
UDITA. In ICSE, pages 225–234, 2010.
[17] M. Gligoric, T. Gvero, S. Lauterburg, D. Marinov, and
S. Khurshid. Optimizing generation of object graphs in Java
PathFinder. In ICST , pages 51–60, 2009.
[18] P. Godefroid. Compositional dynamic test generation. In
POPL , pages 47–54, 2007.
[19] P. Godefroid, N. Klarlund, and K. Sen. DART: directed
automated random testing. In PLDI , pages 213–223, 2005.
[20] J. Goodenough and S. Gerhart. Toward a theory of test data
selection. IEEE TSE , pages 493–510, 1975.
[21] A. Gotlieb, B. Botella, and M. Rueher. Automatic test data
generation using constraint solving techniques. In ISSTA ,
pages 53–62, 1998.
[22] K. Havelund and G. Ro¸ su. Synthesizing monitors for safety
properties. In TACAS, pages 257–268, 2002.
[23] H.-M. Horcher. Improving software tests using Z
speciﬁcations. In ZUM , pages 152–166, 1995.
[24] J. C. Huang. An approach to program testing. ACM
Computing Surveys, 7(3):113–128, 1975.
[25] S. Khalek and S. Khurshid. Systematic testing of database
engines using a relational constraint solver. In ICST, pages50–59, 2011.
[26] S. Khurshid, C. Pasareanu, and W. Visser. Generalized
symbolic execution for model checking and testing. In
TACAS , pages 553–568, 2003.
[27] J. C. King. Symbolic execution and program testing.
Communications of the ACM , 19(7):385–394, 1976.
[28] P. Koopman, A. Alimarine, J. Tretmans, and R. Plasmeijer.
Gast: Generic automated software testing. In IFL, pages
991–991, 2003.
[29] B. Korel. Automated test data generation for programs with
procedures. In ISSTA , pages 209–215, 1996.
[30] R. Lammel and W. Schulte. Controllable combinatorial
coverage in grammar-based testing. In TestCom , pages
19–38, 2006.
[31] B. Liskov and J. Guttag. Program Development in Java:
Abstraction, Speciﬁcation, and Object-Oriented Design .
Addison-Wesley, 2000.
[32] D. Marinov and S. Khurshid. TestEra: A novel framework
for automated testing of Java programs. In ASE, pages
22–31, 2001.
[33] P. Maurer. Generating test data with enhanced context-free
grammars. IEEE Software, 7(4):50–55, 1990.
[34] W. McKeeman. Differential testing for software. Digital
Technical Journal, 10(1):100–107, 1998.
[35] B. Meyer, I. Ciupa, A. Leitner, and L. Liu. Automatic testing
of object-oriented software. In SOFSEM , pages 114–129,
2007.
[36] B. Meyer, A. Fiva, I. Ciupa, A. Leitner, Y . Wei, and E. Stapf.
Programs that test themselves. Computer , 42(9):46–55, 2009.
[37] A. Milicevic, S. Misailovic, D. Marinov, and S. Khurshid.
Korat: A tool for generating structurally complex test inputs.
InICSE, pages 771–774, 2007.
[38] J. Offutt and A. Abdurazik. Generating tests from UML
speciﬁcations. In UML, pages 416–429, 1999.
[39] C. Pacheco, S. Lahiri, M. Ernst, and T. Ball. Feedback-
directed random test generation. In ICSE, pages 75–84, 2007.
[40] C. V . Ramamoorthy, S.-B. F. Ho, and W. T. Chen. On the
automated generation of program test data. IEEE TSE,
2(4):293–300, 1976.
[41] M. Roberson and C. Boyapati. Efﬁcient modular glass box
software model checking. In OOPSLA, pages 4–21, 2010.
[42] K. Sen, D. Marinov, and G. Agha. CUTE: A concolic unit
testing engine for C. In ESEC/FSE , pages 263–272, 2005.
[43] J. Siddiqui and S. Khurshid. An empirical study of structural
constraint solving techniques. In ICFEM, pages 88–106,
2009.
[44] E. G. Sirer and B. N. Bershad. Using production grammars
in software testing. In DSL, pages 1–13, 1999.
[45] M. Staats and C. Pasareanu. Parallel symbolic execution for
structural test generation. In ISSTA , pages 183–194, 2010.
[46] N. Tillmann and J. De Halleux. Pex–white box test
generation for .NET. Tests and Proofs, 4966:134–153, 2008.
[47] W. Visser, K. Havelund, G. Brat, S. Park, and F. Lerda.
Model checking programs. In ASE, pages 203–232, 2003.
[48] T. Xie, D. Marinov, W. Schulte, and D. Notkin. Symstra: A
framework for generating object-oriented unit tests using
symbolic execution. In TACAS, pages 365–381, 2005.
[49] X. Yang, Y . Chen, E. Eide, and J. Regehr. Finding and
understanding bugs in C compilers. In PLDI, pages 283–294,
2011.
11