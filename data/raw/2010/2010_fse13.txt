Con2colic Testing
Azadeh Farzan
University of Toronto, CanadaAndreas Holzer
TU Wien, AustriaNiloofar Razavi
University of Toronto, Canada
Helmut Veith
TU Wien, Austria
ABSTRACT
In this paper, we describe con2colic testing ‚Äî a systematic testing
approach for concurrent software. Based on conc rete and symb olic
executions of a concurrent program, con2colic testing derives in-
puts and schedules such that the execution space of the program
under investigation is systematically explored. We introduce inter-
ference scenarios as key concept in con2colic testing. Interference
scenarios capture the Ô¨Çow of data among different threads and en-
able a uniÔ¨Åed representation of path and interference constraints.
We have implemented a con2colic testing engine and demonstrate
the effectiveness of our approach by experiments.
Categories and Subject Descriptors
D.2.5 [ Software Engineering ]: Testing and Debugging
General Terms
Algorithms, VeriÔ¨Åcation, Security
Keywords
Testing, Concurrency, Concolic, Interference
1. INTRODUCTION
Testing and bug Ô¨Ånding for concurrent programs has been a very
active area of research in recent years. There is an array of tools
that successfully discover errors in concurrent programs by using a
wide range of heuristic techniques that have been developed to alle-
viate the interleaving explosion problem that is inherent in analysis
of concurrent programs. From a classical point of view, testing
(sequential program testing, to be more exact) techniques are of-
ten coupled with a notion of coverage that the technique guaran-
tees. Various coverage criteria have been introduced for sequential
program testing over the years. Existing concurrency testing tech-
niques can be divided into three categories of coverage guarantees
(on program runs, inputs, code, assertions, etc.) that they provide:
(i) No speciÔ¨Åc coverage guarantees: heuristic-based techniques
[25, 24, 22, 12, 4, 2, 17] are based on the philosophy of using
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proÔ¨Åt or commercial advantage and that copies
bear this notice and the full citation on the Ô¨Årst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciÔ¨Åc
permission and/or a fee.
ESEC/FSE ‚Äô13, August 18 ÀùU26, 2013, Saint Petersburg, Russia
Copyright 2013 ACM 978-1-4503-2237-9/13/08 ...$15.00.heuristics to target interleavings that are more likely to contain
bugs, and testing as many of those as possible (under the given
time and space limitations). These techniques have been extremely
successful in Ô¨Ånding bugs, but cannot provide any useful informa-
tion to the tester about what they have or have not missed during
testing, that is, they cannot provide any coverage guarantees.
(ii) Coverage guarantees over the space of interleavings: search
prioritization techniques [11, 5, 10, 14, 3] take a more coverage-
oriented approach than the techniques in category (i) by using ideas
like preemption-bounding [10], context-bounding [14, 11], depth-
bounding [5], and delay-bounding [3] to prioritize the search in
the space of all program interleavings. These prioritizations enable
us to quantify (in a meaningful way) how much of the interleav-
ing space is tested, a property that the heuristic-based search tech-
niques of category (i) do not provide. Search prioritization tech-
niques have been very successful in discovering bugs. For exam-
ple, CHESS [11] is a successful testing tool based on the context-
bounding search prioritization. Nevertheless, these techniques all
work based on a Ô¨Åxed (pre-determined) set of inputs and, therefore,
if a bug cannot be discovered on the given set of inputs, it will be
missed (see Section 3 for an example). Moreover, coverage guar-
antees are only over the space of program interleavings.
(iii) Coverage guarantees over the space of program inputs and
interleavings: There are also sequentialization techniques [8, 23,
15, 13, 16], that, based on context-bounding (or more recently other
types of search prioritization), translate a concurrent program to a
sequential program that has the same behaviour (up to a certain
context bound), and then analyze the sequential program (stati-
cally) for the property of interest. To check if (for example) an
assertion fails, the space of both inputs and interleavings (but up
to the bound) are searched. These techniques are excellent at dis-
covering bugs (they are incomplete, due to the bound, for proving
properties). However, the sequential programs generated are not
appropriate models to be used for testing (cf. Section 9). More
recently, there has been some focus on exploring input and inter-
leaving spaces of the program [19, 21] in a more systematic way.
However, these techniques only explore a subset of the program
behaviours, and therefore, cannot aim for maximum coverage.
We propose a concurrency testing technique that belongs in cat-
egory (iii). Our goal is to systematically test concurrent code such
that meaningful coverage guarantees can be provided. Concolic
(conc rete and symb olic) testing [6, 1] is the gold standard of se-
quential testing with exactly the same point of view for sequential
programs. There, the set of possible inputs (the only parameter that
can change for sequential programs) is systematically explored to
provide standard code coverage guarantees, such as branch cover-
age, for the program and existing program errors are discovered
meanwhile. Our proposed approach can be viewed as the general-Path Exploration
Interference
ScenariosInterference
Exploration
Realizability CheckMulti-threaded
Concolic ExecutionSymb.
Trace
Yes:
realizableSchedule &
Inputs
No: not
realizableSeq. Schedule &
Random Inputs
Figure 1: Overview of Con2colic Testing.
ization of concolic testing to concurrent programs, and hence we
call it concurrent concolic testing , or con2colic testing in short. In
Section 3, we introduce the running example of this paper. Target-
ing code coverage is a reasonable goal for a testing routine, since
it has been shown that most dynamic program errors can be en-
coded as program assertions [7] which can easily be tested/veriÔ¨Åed
through branch coverage. In con2colic testing, we aim to achieve
maximal code coverage for the program (time and space allowing).
It is important to note that almost all existing concurrency testing
techniques today use an under-approximation of the set of concur-
rent executions as a model for testing. Most techniques in category
(i), mentioned before, use one or several program runs as a basis
for predicting buggy runs. The techniques in category (ii), use the
set of program runs prioritized by one of the bounding techniques
discussed above, as the under-approximation of their choice. A
common limitation among most of these techniques is that their
under-approximate model is Ô¨Åxed a priori. Once they start test-
ing, they are limited to performing the tests within their bounded
model1. SpeciÔ¨Åcally, all techniques that we list in category (iii)
(which is our focus) also have this limitation. Con2colic testing
does not have this limitation. Similar to concolic testing, we have
the program at our disposal and we can keep expanding the set of
behaviours that we consider for testing. Naturally, our approach is
also limited by the same constraints that hold concolic testing back,
namely the unavailability of external library functions or undecid-
able logics. This aspect of con2colic testing has a very important
consequence. We can theoretically guarantee completeness in the
limit; by this, we mean that if the testing algorithm runs for long
enough, then we can cover every program branch or declare it un-
reachable (again within the limitations of concolic testing).
We implemented the con2colic testing approach as a tool for
testing multi-threaded C code. We use a set of benchmarks found
in concurrency research literature to demonstrate the practical efÔ¨Å-
ciency of our approach in providing code coverage and Ô¨Ånding bugs
in these benchmarks. In the next section, we provide an extended
overview of con2colic testing.
2. CON2COLIC TESTING
We start by a short overview of concolic testing, in order to high-
light what needs to be done to generalize it to concurrent programs.
Concolic testing has three main components: concolic execution
engine, path exploration, and realizability checker. The concolic
execution engine executes the program on a given input vector con-
colically (i.e. using both concrete and symbolic values for inputs)
and as a result, generates an execution trace that contains a se-
quence of path constraints on symbolic inputs (i.e. branch con-
ditions encountered during execution). The goal is to try to diverge
from the just observed execution by taking a different side of an en-
countered branch. The path exploration component, hence, selects
some of these branch conditions and negates them in order to guide
the execution along a different path. In the realizability checker,
1Category (ii) techniques do not have this limitation theoretically,
but in practice, only very small values for bounds are feasible.SMT solvers are used to generate an input vector (if possible) that
would satisfy the new path constraints, with the understanding that
such an input vector is likely to force the program to execute a
different path. For a deterministic sequential program, each input
vector speciÔ¨Åes a single path in the program.
The behaviour of a concurrent program, however, is not only
inÔ¨Çuenced by the input vector, but also by the interleaving of exe-
cution of threads. Each interleaving of execution of threads deter-
mines a pattern of interferences among the threads. An interference
occurs when a thread reads a value that is generated by another
thread. An interference could substantially change the course of
the local computation of a thread, because the value generated by
another thread may not be producible locally. Therefore, the local
behaviour (e.g. an assertion violation) followed by this interference
may never surface by pure sequential testing.
We propose con2colic testing (see Figure 1) as a concolic testing
approach tailored for concurrent programs. We introduce the con-
cept of interference scenario (formally deÔ¨Åned in Section 5) as a
representation of a set of interferences among the threads. Concep-
tually, interference scenarios are the essential information that de-
Ô¨Åne the important scheduling constraints for a concurrent program
run; in other words, all runs with the same interference scenario
arebehaviourally equivalent under the same input values. The con-
straint system for sequential concolic execution is also modiÔ¨Åed to
model interferences as well. The con2colic testing engine is then
able to produce schedules, in addition to input vectors, for con-
current programs. To accommodate this change, con2colic testing
has one more component (compared to sequential concolic testing)
called interference exploration , that navigates the space of all inter-
ference scenarios in a systematic way.
In con2colic testing, the execution engine is leveraged to execute
a concurrent program based on a given schedule. The important
part of the observed execution is stored in a forest data structure
(formally deÔ¨Åned in Section 5) that keeps track of various interfer-
ence scenarios that have been explored already. The path explo-
ration component then decides what newscenario to try next, aim-
ing at covering previously uncovered parts of the program, based on
the set of interferences that have already been explored. For each
interference scenario, the realizability checker investigates whether
there exist a set of inputs and a feasible schedule such that a pro-
gram execution based on these inputs and this schedule results in
the same set of interferences. If the answer is yes, then the input
and the schedule are used in the next round of concolic execution.
Otherwise, the interference exploration component generates new
candidates by introducing new interferences.
Next, we brieÔ¨Çy explain con2colic testing components and then
we present an exploration strategy that we implemented.
Concolic Execution. There are two input parameters for the con-
colic execution engine in con2colic testing: (1) an input vector and
(2) a schedule. The engine executes the concurrent program with
the given input vector and according to the given schedule. The
program is instrumented such that, during the execution, all im-
portant events are recorded and a symbolic trace is produced as a
result. These important events include synchronization operations,
accesses to shared variables, and path constraints. The symbolic
trace contains all the necessary information for the con2colic en-
gine to make progress. However, it excludes extra information,
such as details of local computations of threads, that can safely be
ignored in our approach to gain scalability and efÔ¨Åciency.
Path Exploration. The role of the path exploration routine is to
explore the input space for a new set of input values that may cover
the yet uncovered parts of the program. This module does not mod-
ify the set of interferences observed in the previous execution, andonly guides the execution down a different program path by negat-
ing the conditions of branches (observed along the last execution).
Therefore, the changes made in this phase are only changes of the
input values, and the essential structure of the schedule (i.e. inter-
ference scenario) remains the same.
Realizability Checker. The role of the realizability checker is to
determine if there is a set of inputs and a feasible schedule that
realize the given interference scenario. Each interference scenario
imposes two constraint systems: (1) constraints on input values that
are satisÔ¨Åed by all executions having the same interference scenario
and (2) constraints that deÔ¨Åne the temporal order of the events of
different threads (e.g., a read event of one thread has to occur af-
ter the corresponding write event of another thread). We formally
deÔ¨Åne the resulting constraint systems in Section 6. If both con-
straint systems have a solution, then an input vector and a schedule
can be inferred which give rise to a real program execution with
exactly the same set of interferences as deÔ¨Åned in the interference
scenario. If at least one of the constraint systems does not have a
solution then the interference scenario has to change. To that end
the current interference scenario is passed to the interference ex-
ploration module (described below), such that a new one will be
derived from it. In Section 7.5, we will discuss how to prune the
exploration space (of interference scenarios) based on the reason
for the unsatisÔ¨Åability of the constraint system.
Interference Exploration. Interference exploration produces new
interference scenarios from previously explored interference sce-
narios, essentially by introducing a new interference. This is done
by picking a read from the given interference scenario that is not in-
terfered by other threads, and an appropriate write from the forest,
and adding an interference from the write to the read to generate
a new interference scenario. Note that the occurrence of the write
event itself may be conditional on the existence of other interfer-
ences. Therefore, to preserve soundness, all of those interferences
should be included in the produced interference scenario as well.
We discuss the function of this module in detail in Section 7.
Exploration Strategy and Completeness. With the above com-
ponents, con2colic testing can exploit different search strategies
and heuristics to explore the interference scenario space. We have
implemented a search strategy that targets branch coverage (Sec-
tion 7). The search strategy then explores interference scenarios
with an increasing number of interferences. That is, all interfer-
ence scenarios with one interference are explored Ô¨Årst, and then
interference scenarios with two interferences are explored, and so
on. A nice feature of this exploration strategy is that it is com-
plete (Theorem 7.3) modulo the exploration bound (and of course
concolic testing limitations).
3. RUNNING EXAMPLE
We use the buggy implementation of function addAll in Fig. 2
as a running example in this paper. The example is a variation of
theaddAll method of the built-in class Vector in Java. The error
in it is a real error. We have rewritten it in C (our tool‚Äôs input
language) and simpliÔ¨Åed it a bit to make it suitable as a running
example. addAll has two input parameters which are pointers to
vector structures. It appends all elements of the second vector to the
end of the Ô¨Årst vector. Each vector has three Ô¨Åelds: data which
is an array holding vector elements, size which represents the size
ofdata , and cnt which keeps track of the number of elements in
data . Function addAll uses a lock lkto synchronize the calls to
this function. It Ô¨Årst checks whether there is enough space to insert
all elements of u->data intov->data , i.e. v->cnt +u->cnt
v->size (cf. line 4). If not, it increases the size of v->datatypedef struct {int cnt, int size, int*data} vector;
pthread_mutex lk;
1void addAll(vector *v, vector *u) {
2 int numElem = v->cnt + u->cnt;
3 pthread_mutex_lock(&lk);
4 if(numElem > v->size) {
5 v->data = realloc (numElem *2);
6 v->size = numElem *2;
7 }
8 assert (v->sizeu->cnt + v->cnt);
9 ... //copy data from u to v
10 v->cnt = v->cnt + u->cnt;
11 pthread_mutex_unlock(&lk);
12}
Figure 2: Function addAll : a buggy concurrent implementation of
vector concatenation.
accordingly. The invariant v->sizeu->cnt +v->cnt is stated
as an assertion in line 8. Finally, it copies the elements and updates
v->cnt . The bug in addAll corresponds to the fact that the value
ofv->cnt is being read (at line 2) outside the lock block and hence
v->cnt can be changed by other threads before the lock block is
executed, leading to an inconsistent state.
For simplicity, we just refer to v->cnt andu->cnt , while in the
real implementation, these accesses are protected by locks. There-
fore, there is no data race on these accesses and the error that we
discuss here is independent of that data race and exists in the origi-
nal race-free code.
Imagine a concurrent program with two threads TandT0each of
them calling addAll with vanduas arguments, where vis shared
between the threads and uis an input of the program. Therefore,
each individual Ô¨Åeld of vis treated as a shared variable and each in-
dividual Ô¨Åeld of uis treated as an input. Also, suppose that initially
v->cnt is 10 and v->size is 20. Consider the situation where
u->cnt =7 and the program is executed as follows:
(i) The Ô¨Årst thread Texecutes line 2, reading 10 from v->cnt , 7
from u->cnt and storing value 17 in numElem .
(ii) The second thread T0is executed completely. It reads val-
ues 10 and 7 from v->cnt andu->cnt , respectively, at line 2
and assigns 17 to numElem . Then, it enters the lock block. Since
v->size is greater than 17 it skips lines 5 and 6 and assigns 17 to
the shared variable v->cnt before exiting the lock block.
(iii) Then, Tcontinues execution: It skips lines 5 and 6 since
(numElem =17)<(v->size =20). However, when Tgets to the
assertion, v->cnt has value 17 written by T0. Therefore, we have
(v->size =20)<(17 + 7), and hence the assertion is violated.
This particular error occurs because the naive locking of indi-
vidual vector s is not the correct way of copying from one vector
(that can change meanwhile) to another. The error is interesting be-
cause it requires a combination of a particular concurrent schedule
combined a with particular (relative) values for the input vectors
to manifest. If the threads are executed sequentially back to back,
nothing goes wrong. On the other hand, if we execute the same
interleaving (as described above), but start with u->cnt having the
value 3 (instead of 7), then nothing goes wrong; the Ô¨Årst thread
assigns 13 to numElem , the second thread skips lines 5 and 6 and
assigns 13 to u->cnt . Then, the Ô¨Årst thread skips lines 5 and 6
since ( numElem =13)<(v->size =20). This means that trigger-
ing this concurrency bug does not solely depend on the concurrent
schedule , nor does it solely depend on the chosen input values; it
depends on Ô¨Ånding the right combination of input values and the
choice of concurrent schedule. Any testing technique that does not
explore the combination space systematically has the potential of
missing on this bug.4. BASIC DEFINITIONS AND NOTATIONS
We will now introduce some notions from concolic testing ad-
justed to our application. Classical sequential concolic testing [6,
1] logs a set of path constraints over input variables during concolic
execution which describes the conditions on the values of the inputs
that have to be true to drive the execution of the program along the
same path. However, doing the same for concolic execution of
multi-threaded programs would result in a set of constraints that
are closely tied to the speciÔ¨Åc schedule performed during program
execution. To solve this problem, we proceed as follows: Instead
of explicitly tracking scheduling decisions, we introduce symbolic
variables which enable us to track the information Ô¨Çow between
threads. More precisely, we introduce an additional symbolic vari-
able each time a shared variable is read and for each shared variable
write, we store the symbolic value (based on symbolic inputs and
symbolic read variables). This will enable us to build constraints
which capture the essence of a concurrent execution path.
A concurrent program consists of a set T=fT1;T2;:::gof
threadsTi, a set of input variables IN, a set of shared variables SV,
a set of local variables LV, and a set of locks L. Let SymbIN be a
set of symbolic input variables fi0;i1;:::gandSymbRV be a set
of symbolic shared read variables fr0;r1;:::g. Let Expr repre-
sent the set of all expressions over SymbIN andSymbRV , and let
Pred(Expr) represent the set of all predicates over Expr . Then, the
set of actions that a thread can perform on a set of shared vari-
ables SVand locksLis deÔ¨Åned as:
=frd(x;r)jx2SV;r2SymbRVg[
fwt(x;val )jx2SV;val2Exprg[f tf(Ti)jTi2Tg[
fac(l);rel(l)jl2Lg[f br( )j 2Pred(Expr)g
Action rd(x;r)corresponds to reading from a shared variable x
and the symbolic value of the shared variable xbecomes the sym-
bolic variable r. Each time we observe a read from a shared vari-
able during concolic execution, we introduce a new symbolic vari-
abler2SymbRV that is uniquely associated with that speciÔ¨Åc
read. Action wt(x;val )corresponds to writing to a shared vari-
ablexa symbolic value which is represented as an expression val.
To couple a read of xwith a write to x, it is enough to connect the
stored expression at the write to the symbolic value of the read, i.e.,
r=val. Action tf(Ti)represents forking thread Ti. Actions ac(l)
andrel(l)represent acquiring and releasing of lock l, respectively.
Finally, action br( )denotes a branch condition which requires
that predicate  is true. We model assertions in a program by two
branches, i.e., one branch for passing the assertion and one branch
for violating the assertion.
We denote the execution of an action by a thread as an event .
Formally, an event is a tuple (Ti;a)2T. LetEV denote the
set of all possible events. During concolic execution, we observe a
sequence of events, a so-called symbolic trace :
DEFINITION 4.1 (S YMBOLIC TRACE ).A symbolic trace is a
Ô¨Ånite string 2EV. By[n], we denote the n-th event of.
Given a symbolic trace ,jTiis the projection of to events in-
volvingTi. A symbolic trace is thread-local, if =jTifor
someTi.
The inputs to our concolic execution engine are an input vector
and a schedule which exactly speciÔ¨Åes the resulting program run:
DEFINITION 4.2 (P ROGRAM RUN).Consider a determinis-
ticconcurrent program P. A (partial) runofP, represented by
R=P(in;), is uniquely described by a valuation inof the in-
put variables IN and a schedule . A scheduleis deÔ¨Åned byInitial thread: T
1 rd(v->cnt;r0)
Context switch: T!T0
2 rd(v->cnt;r0
0)
3 ac(lk)
4 rd(v->size;r0
1)
5 br(r0
0+i0r0
1)
6 rd(v->size;r0
2)
7 rd(v->cnt;r0
3)
8 br(r0
2i0+r0
3)
9 rd(v->cnt;r0
4)
10 wt(v->cnt;r0
4+i0)
11 rel(lk)
Context switch: T0!T
12 ac(lk)
13 rd(v->size;r1)
14 br(r0+i0r1)
15 rd(v->size;r2)
16 rd(v->cnt;r3)
17 br(r2<i0+r3)2 [r0
0= 10 ]
3
4 [r0
1= 20 ]
5 br(r0
0+i0r0
1)
6 [r0
2= 20 ]
7 [r0
3= 10 ]
8 br(r0
2i0+r0
3)
9 [r0
4= 10 ]
10 wt(v->cnt ; r0
4+i0)
111[r0= 10 ]
12
13[r1= 20 ]
14br(r0+i0r1)
15[r2= 20 ]
16[r3=r0
4+i0]
17br(r2< i 0+r3)T0
T
Figure 3: Symbolic trace obtained from the assertion violating
execution of the program in Fig. 2 and its corresponding interfer-
ence scenario IS().i0represents the symbolic value of input
v->cnt .r0,r0
0,r0
3, andr0
4read initial value 10, and r1,r2,r0
1, and
r0
2read initial value 20, and r3readsr0
4+i0
a sequence (Ti1;n1)(Ti2;n2):::(Tim 1;nm 1)(Tim; )where
Tij2T, for1jm, andnj>0, for1j < m , speci-
Ô¨Åes the number of executed actions. A tuple (Tij; )represents the
execution of thread TijuntilTijterminates. A run of program P
isfeasible ifPcan be executed with input vector inand accord-
ing to schedule . Each feasible program run Ryields a symbolic
trace(R).
We assume that the program is instrumented in such a way that
all program actions covered in EV are actually observed by (R).
In Fig. 3, on the left, we show a symbolic trace obtained from the
assertion violating execution of the program in Fig. 2, discussed in
Section 3. Observe that the concolic execution engine only logs
reads from and writes to shared variables, but no reads from or
writes to local variables. Internally, the concolic execution engine
keeps track of the symbolic values of local variables and is there-
fore able to correctly update the symbolic value of a shared variable
when it gets written.
5. INTERFERENCE SCENARIOS
An interference occurs whenever a thread reads a value that is
written by another thread. We introduce interference scenarios to
describe a class of program executions under which certain inter-
ferences happen during concolic execution. Intuitively, an inter-
ference scenario is a set of thread-local traces extended with an
interference relation between write and read events from different
threads. We represent a set of interference scenarios in a data struc-
ture called interference forest. Formally, an interference forest is a
Ô¨Ånite labeled directed acyclic graph whose nodes represent events
and whose edges express relations between events.
DEFINITION 5.1 (I NTERFERENCE FOREST ).Aninterference
forest is a tupleI= (V;E;` )whereVis a set of nodes, `:V!
EV is a labeling function which assigns events to nodes. For v2
Vwhere`(v) = (Ti;a), we also deÔ¨Åne Th (v) =Tiand Ac (v) =awt(y)
...rd(x)
...rd(x)
......ThreadT1
wt(z)
rd(y)
wt(x)
...rd(y)
wt(x)
......ThreadT2
n
Figure 4: Interference scenarios in an interference forest F
to be the thread and the action of the corresponding event, respec-
tively. The set of edges Eis the disjoint union E=EL_[EI
of thread-local edges ELand interference edges EI. A thread-
local edge (or, simply, a local edge) is an edge (s;t)2ELwhere
Th(s) =Th(t). An interference edge (s;t)2EIis an edge where
Th(s)6=Th(t)and Ac (s) = wt(x;val )and Ac (t) = rd(x;r)for
somex,val, andr. We require that EIis an injective relation, i.e.,
each read is connected to at most one write by EI. The thread-local
edges can be naturally partitioned according to their threads, i.e.,
EL=ET1_[ET2:::_[ETn. EachETiinduces a subforest GTi
which consists of all nodes with Th (v) =Tiand edges in ETi. We
require that each GTiis a rooted tree. The number of interference
edgesjEIjis called the degree of the interference forest.
An isomorphism between two interference forests is a graph iso-
morphism which preserves the labeling function. Given an inter-
ference forest J,RI(J)denotes the read nodes involved in the in-
terference edges of J, i.e.,RI(J) =fnrj9nw:(nw;nr)2EIg.
Figure 4 shows an interference forest. The nodes labeled with
read/write and branch actions are represented by squares and di-
amonds, respectively. Local edges are presented by arrows and
interference edges are presented by dotted arrows. The left tree
representsGT1and the right tree represents GT2. The degree of
the interference forest is 2.
DEFINITION 5.2 (I NTERFERENCE SCENARIO ).We deÔ¨Åne an
interference scenario (IS) as an interference forest where each GTi
is a path.
As mentioned at the beginning of this section, an interference
forest is a compact representation for a set of interference scenarios.
DEFINITION 5.3 (C AUSAL INTERFERENCE SCENARIO ).Let
I= (V;E;` )be an interference forest. The transitive closure E
of the edge relation Eis called the causality relation of I. Given a
noden, the causal interference scenario (CIS) ofnis the subforest
ofIinduced by the causal predecessors of n, i.e., by the node set
fvj(v;n)2Eg. We denote it by C=CIS(I;n)and callnthe
sink ofC, i.e., sink(C) =n.
Every causal interference scenario is an interference scenario.
This is also the crucial property why interference forests serve as
compact representations for sets of interference scenarios. In Fig. 4,
the causal interference scenario of node nis the interference sce-
nario enclosed by dashed lines.
Construction of new interference scenarios from existing ones
and merging interference scenarios into an interference forest are
two central operations in our testing approach. But we cannot com-
bine arbitrary interference forests/scenarios; They have to be com-
patible with each other:DEFINITION 5.4 (C OMPATIBLE INTERFERENCE FORESTS ).
Two interference forests I,Jarecompatible if there is an interfer-
ence forestKand interference subforests I0,J0ofKsuch thatI0
andJ0are interference forests themselves and Iis isomorphic to
I0andJis isomorphic to J0.
DeÔ¨Ånition 5.4 applies to compatible interference scenarios as well
since each interference scenario is an interference forest.
REMARK 5.5. Compatible interference forests can be merged
into an interference forest by naturally taking the minimal K, i.e.,
Konly contains nodes and edges corresponding to I0andJ0. If
interference scenarios IandJare not compatible, then there is at
least one thread for which IandJdescribe different computations.
Each symbolic trace deÔ¨Ånes an interference scenario, denoted
byIS(). Intuitively, each event represents a unique node in IS()
which is labeled with that event. For each thread Ti, thread-local
edges are added between the corresponding nodes according to the
order injTi(wherejTi=i;1;i;2;::;i;mdenotes the projec-
tion of events in on threadTi.) An interference edge is added
for each node labeled with a read event if the last write event to
the same shared variable before the read event in is performed by
another thread. More formally, IS() = (V;E;` )is deÔ¨Åned as:
V=S
Tifni;jjni;jis a unique node for event i;jg,
`(ni;j) =i;jfor each node ni;j,
ETi=f(ni;k;ni;k+1)j0km 1g, and
EI=f(ni;k;nj;h)jTh(ni;k)6=Th(nj;h);Ac(ni;k) =
wt(x;val );Ac(nj;h) =rd(x;r)for somex;val;r andi;k
is the last write to xinbeforej;hg.
Figure 3 shows the interference scenario for the symbolic trace ob-
tained from the assertion violating execution of the program in Fig-
ure 2 which is discussed in Section 3.
DEFINITION 5.6 (R EALIZABLE INTERFERENCE SCENARIO ).
An interference scenario Iisrealizable iff there is a feasible partial
program run Rwith=(R)such thatI=IS(). We sayR
realizesIfor such a feasible program run R.
Realizable interference scenarios deÔ¨Åne equivalence classes on
the set of program runs which represent the same Ô¨Çow of data
among the threads. Note that interference scenarios are not mono-
tonic wrt. realizability. Let IandI0be two interference scenarios
whereIis a subgraph of I0. Then, the realizability of Idoes not
imply the realizability of I0and vice versa. We will discuss the
reasons for this behaviour in Section 6.
Interference scenarios specify partial program runs and therefore
unanticipated behaviour can be observed:
DEFINITION 5.7 (U NFORESEEN INTERFERENCES ).LetIbe
a realizable interference scenario and Rbe a partial program run
with=(R)such thatI=IS(). LetR0be a run that ex-
tendsR, i.e.,0=(R0)andis a preÔ¨Åx of 0. Then, IS(0)
is a supergraph of IS(). More speciÔ¨Åcally, IS(0)might contain
some additional interferences. We refer to these interferences as
interferences unforeseen by interference scenario Iin runR0.
6. CONSTRAINT SYSTEMS FOR INTER-
FERENCE SCENARIOS
Each interference scenario implies constraints on both data and
temporal order of the events. In this section, we describe these con-
straints in detail. In Section 7, we present our soundness theoremData Constraints DC(I)
DC(I): DCbranch (V)^DCinterfere (I)^DClocal(I)
DCbranch (V):V
 2BR(V) where
BR(V) =f jv2V;Ac(v) =br( )g
DCinterfere (I):V
(vwt;vrd)2EIDCmatch (vwt;vrd)
DClocal(I):V
(vwt;vrd)2ELDCmatch (vwt;vrd)whereELis the
set of local write-read matches.
DCmatch (vwt;vrd):(val=r)forAc(vwt) = wt(x;val ),Ac(vrd) =
rd(x;r)
Temporal-Consistency Constraints TC(I)
TC(I):V
Ti2TPOTi^FC^LC1^LC2^WRC interfere^
WRC local
POTi:V
ni;j2GTi;ni;jis not a leaf(tni;j<tni;j+1)
FC:V
Ti2T(tntf(Ti)<tni;1)
LC1:V
Ti6=TjV
l2LV
[aq;rl]2LTi;l
[aq0;rl0]2LTj;l 
trl<taq0_trl<taq
LC2:V
Ti6=TjV
l2LV
aq2NoRelTi;l
[aq0;rl0]2LTj;l(trl0<taq)
WRC interfere :V
(u;v)2EICoupled (v;u)
WRC local:V
v62intReadsCoupled (v;LocW (v))
Coupled (v;u):(tu<tv)^V
n2Wxnfug((tn<tu)_(tv<tn))
Figure 5: Constraint systems DC(I)andTC(I)for an interference
scenarioI= (V;E;` )
(Thm. 7.2) that shows how these constraints can be used to check
for the realizability of an interference scenario.
Data Constraints. Each interference scenario I= (V;E;` )de-
Ô¨Ånes a data constraint DC(I)as shown in Fig. 5. Any solution to
DC(I)(if one exists), deÔ¨Ånes an input vector ifor the concurrent
program. The constraint DC(I)consists of three parts: (i) DC branch ,
(ii)DC interfere , and (iii) DC local. The constraint DC branch encodes all
branch conditions occurring in I. The intuition behind this con-
straint is that the program execution should follow the control path
represented by the respective branching conditions. DC interfere re-
lates reads from shared variables, which should be interfered by
writes from other threads, to the symbolic values of the write the
read should interfere with. Finally, DC localrelates each read from a
shared variable, which should not be interfered by any writes from
other threads, to the most recent write to the same shared variable
performed by the same thread. If there is no such write before the
read, we constrain the symbolic value of the shared variable to the
initial value of the variable.
Temporal-Consistency Constraints. An interference scenario I
also deÔ¨Ånes a temporal consistency constraint TC(I). This con-
straint is over symbolic traces and any solution to it deÔ¨Ånes a sched-
ule for the concurrent program. The constraints in TC(I), as de-
Ô¨Åned in Fig. 5, are divided into the following four categories: (i)
thread-local program-order consistency ( POTi), (ii) thread-fork con-
sistency ( FC), (iii) lock consistency ( LC1&LC2), and (iv) write-
read consistency ( WRC 1&WRC 2). For each node ninI, an integer
variable tn(timestamp) is considered to encode the index of the
event of the node in a symbolic trace . In the constraints, let ni;j
represent the jthnode inGTi, and letntf(Ti)represent the node n
whereAc(n) =tf(Ti). The constraints of TC(I)are:
POTi:Ensures that for thread Ti, the thread-local program order
is respected in the schedule.
FC: Ensures that no thread can be scheduled before it is forked.
LC1&LC2:Each lock acquire node aqwithAc(aq) = ac(l)
andTh(aq) =Tiand its corresponding lock release node rlinTi
deÔ¨Åne a lock block, represented by [aq;rl]. LetLTi;lbe the setof lock blocks in thread Tiregarding lock l.LC1ensures that no
two threads can be inside lock blocks of the same lock l, simulta-
neously.LC2ensures that the acquire of lock lby a thread that
never releases it in Imust occur after all releases of lock lin other
threads. In this formula, NoRelTi;lstands for lock acquire nodes in
Tiwith no corresponding lock release nodes.
WRC interfere &WRC local:LetWxrepresent the set of all nodes u
withAc(u) = wt(x;val ),intReads be all nodes vwithAc(n) =
rd(x;r)such thatvis involved in an interference edge in EI, and
LocW be a function that for each node vwithAc(v) = rd(x;r)
andTh(v) =Tireturns a node uwithAc(u) = wt(x;val )and
Th(u) =TiinIsuch thatuis the closest such node to vbefore
vinGTi. For each read node vand write node u, the formula
Coupled (v;u)ensures that the read event of vis coupled with the
write event of uinby forcing all events that write to the corre-
sponding variable to happen either before the event of uor after the
event ofvin.
Non-Monotonicity of Realizability. We can now explain the non-
monotonic behaviour of interference scenarios wrt. realizability
that was mentioned in the discussion following Def. 5.6. Let Iand
I0be two interference scenarios where Iis a subgraph of I0. Then,
according to the data constraints, all constraints in DC branch(I)and
DC interfere (I)appear in DC branch(I0)andDC interfere (I0), respectively.
However, the constraints in DC local(I)andDC local(I0)are incom-
parable. The same phenomenon exists in the temporal-consistency
constraints, i.e., WRC localinIandI0are incomparable. This im-
plies that, by extending an interference scenario, the resulting con-
straint systems do not change in a monotonic way.
7. TESTING ALGORITHM
We now present our con2colic testing algorithm. The algorithm
tries to increase branch coverage in concurrent programs. Recall
that we model each assertion in the program by two branches and,
therefore, implicitly target at Ô¨Ånding assertion violations. Since we
aim for branch coverage, we are speciÔ¨Åcally interested in interfer-
ence scenarios related to nodes labeled with branch actions:
DEFINITION 7.1 (I NTERFERENCE SCENARIO CANDIDATE ).
Letnbe a branch node, i.e., Ac (n) =br( ), for some . A causal
interference scenario C is an interference scenario candidate (ISC)
fornif sink (C) =n.
Note that each ISC Cwith sink(C) =n(if realizable) deÔ¨Ånes a set
of partial program runs where Ac(n)is the last action in the run.
Our algorithm enumerates all ISCs of degree k(starting atk= 0),
checks their realizability and moves on to ISCs of degree (k+ 1) .
To keep the exposition simple, we will make the following sim-
plifying assumptions: (i) There are no unforeseen interferences for
an ISC C, i.e., each program run R0extending a partial run R, with
C=IS(R), results in an interference scenario IS(R0)which has
exactly the same interferences as C. (ii) There are no barriers in a
programP. (iii) There are no locks in a program P. In Section 7.3
we will address assumptions (i) and (ii) and in Section 7.4 we will
address assumption (iii). Note that we state all these assumptions
for ease of presentation and that our approach is not limited to set-
tings where these assumptions are true, especially, all benchmark
programs in Section 8 contain locks.
7.1 Testing Algorithm
Alg. 1 shows our con2colic testing algorithm. Given a concur-
rent program Pand a threshold kmax, the algorithm investigates all
ISCs whose degree is kmax. For each such ISC the algorithm
tries to compute a corresponding test.Algorithm 1: Test(programP, boundkmax)
1IForest forest ;
2ISC-Set W0;:::; Wkmax,UN0;:::; UNkmax
3 fork= 0tokmax do
4 Wk ;
5 UNk ;
6i random inputs
7 foreach threadTjdo
8 ConcolicExecution (P;(i;(Tj; )))
9 W0 W0[ExtractISCs ()
10 fork= 0tokmaxdo
11 while Wk6=;do
12 pick and remove Cfrom Wk
13 (result;i;) RealizabilityCheck (C)
14 ISC-Set iscs ;
15 if result6=realizable then
16 UNk UNk[fCg
17 iscs ExploreISCs (C;write-nodes (forest ))
18 else
19  ConcolicExecution (P;(i;))
20 Wk Wk[ExtractISCs ()
21 Wrts new-write-nodes (forest;)
22 forall the C02UNi;0ik 1do
23 iscs iscs[ExploreISCs (C0;Wrts )
24 foreach C02iscs do
25 k0 Degree (C0)
26 if k0kmaxthen
27 Wk0 Wk0[fC0g
1-5: Data Structures. We have three central data structures: (i) a
global interference forest forest that stores all interference scenarios
explored by concolic execution, (ii) a list of sets W0, . . . , Wkmax,
where each Wk, for0kkmax, serves as a worklist for ISCs
of degreek, and (iii) a list of sets UN0, . . . , UNkmax, where each
UNk, for 0kkmax, stores all processed but unrealizable
ISCs of degree k. All these data structures are initially empty (cf.
lines 1 to 5). During execution of Alg. 1, each generated ISC C
of degreekis initially inserted into Wkand later on, if Cis not
realizable, it is moved to UNkfor further exploration.
6-9: Initial Path Exploration. We initialize W0by executing a
test(i;(Tj; ))for each thread Tj(line 8), where iis a random in-
put vector (we use the same ifor each thread Tj) and the schedule
(Tj; )allows only a sequential execution of thread Tjwithout any
interruption from other threads. When the concolic execution en-
gine reaches the end of thread Tj, the program execution is aborted
without executing any other thread (our approach can be general-
ized to handle barriers, but for simplicity of presentation we ignore
them here). As the result of the concolic execution, a symbolic
traceis returned. Then, at line 9, ExtractISCs takes the symbolic
traceand derives new ISCs with the same degree as IS()for
further exploration of program behaviour, i.e., during initialization
it generates ISCs of degree 0. We describe algorithm ExtractISCs in
the next paragraph. We insert the returned ISCs into worklist W0.
Now, after the initialization phase in lines 3 to 5 in Alg. 1, W0
contains for each thread in Pa set of ISCs for further (initially
thread-local) exploration.Algorithm 2: ExtractISCs (Symbolic Trace ) : ISC-Set
1F addDanglingNodes (IS())
2MergeInterferenceForests (forest;F)
3ISC-Set iscs ;
4 foreach dangling node nnewly added to forest do
5 iscs iscs[fCIS(forest;n)g
6 return iscs
2T0
3 4 5br(r0
0+i0r0
1)
6 7 8br(r0
2i0+r0
3)
9 10 11
(a) Interference scenario IS(T0)for a symbolic trace T0obtained by a
sequential execution of thread T0(cf. Fig. 3).
2T0
3 4 5br(r0
0+i0r0
1)
6 7 8br(r0
2i0+r0
3)
9 10 11
d1
br(r0
0+i0>r0
1)d2
br(r0
2<i0+r0
3)
(b) Interference scenario IS(T0)extended with dangling nodes d1andd2.
2T0
3 4 5br(r0
0+i0r0
1)
6 7 8br(r0
2i0+r0
3)
9 10 11
d1
br(r0
0+i0>r0
1)d2
br(r0
2<i0+r0
3)
(c) Interference scenario candidates CIS (forest;d1)andCIS (forest;d2).
Figure 6: Example showing initialization for thread T0(cf. Fig. 3).
Algorithm ExtractISCs .ExtractISCs , shown in Alg. 2, gets a sym-
bolic traceas input. Each branch event in , has a corresponding
dual branch event where its symbolic constraint is negated. Alg. 2
Ô¨Årst obtains IS(). For example, Fig. 6a shows IS()where
is the symbolic trace returned by the initial sequential execution
of threadT0(introduced in Fig. 3). In line 1 in Alg. 2, IS()is
extended to an interference forest Fby introducing for each dual
branch event a so called dangling node , e.g., nodes d1andd2in
Fig. 6b. Then, Fis merged into forest (cf. line 2) as described
in Remark 5.5. For each dangling node which was not merged
with an existing node, we create an ISC (cf. lines 4 and 5), e.g.,
CIS(forest;d1)andCIS(forest;d2)in Fig. 6c. These ISCs are re-
turned to the main algorithm. Note that all generated ISCs will have
the same degree as IS(). This is due to the fact that the dangling
nodes which are not already present in forest occur after the sink
of the ISC which was used when generating the test for . Since
forest is initially empty, during the initialization phase an ISC is
generated for each dangling node in F.
10-27: Main Loop. The testing algorithm processes worklists
W0;:::; Wkmaxin ascending order (cf. main loop at line 10). While
processing Wk(the loop at line 11), each ISC C2Wkis removed
from Wkand its realizability is checked (see Section 7.2, algorithm
RealizabilityCheck ). Given an ISC C,RealizabilityCheck returns a
triple (result;i;)where result indicates whether Cis realizable or
not. If Cis realizable, then (i;)forms a test that realizes C.
15-17: ISC Exploration. IfCis not realizable, then we store C
intoUNkfor later processing. Since the realizability of ISCs is
not monotonic (as discussed in Section 5), Cstill has a chance to
become realizable if we introduce some more interferences in it.
Therefore, we collect all write nodes stored in forest (cf. line 17)
and then use ExploreISCs (Alg. 3) to extend Cto a set of ISCs2 3 4 5br(r0
0+i0r0
1)
6 7 8br(r0
2i0+r0
3)
9 10 11
1 12 13 14br(r0+i0r1)
15 16 17br(r2<i0+r3)
d3
br(r0+i0>r1)d4
br(r2i0+r3)d1
br(r0
0+i0>r0
1)d2
br(r0
2<i0+r0
3)T0
T
Figure 7: Interference scenario IS()from Fig. 3 extended by dan-
gling nodesd1,d2,d3, andd4. The loosely dashed lines enclose
the interference scenario candidate CIS(forest;d4).
for target branch sink(C)by introducing a new interference from
a write in Wrts to a read in C. Each of the generated ISCs has a
degreei > k and is added to Wiin lines 24 to 27. Since i > k ,
the newly generated ISCs will be processed after Wkis processed
completely. We discuss ExploreISCs at the end of Section 7.1.
19-20: Path Exploration. IfCis realizable, then the program is
concolically executed with input vector iand according to schedule
(cf. line 19). The moment sink(C)is executed, the schedule 
enforces an exclusive execution of thread Th(sink(C))without any
interruption from other threads. As in the sequential case, the mo-
ment the end of thread Th(sink(C))is encountered, the concolic
execution engine aborts program execution. The concolic execu-
tion returns a symbolic trace which is fed to ExtractISCs to derive
ISCs fromsimilar to the k= 0case described earlier (see Fig. 7
for an example with k>0). All generated ISCs are added to Wk.
21-23: ISC Re-Exploration. During concolic execution we might
observe a write event we have not observed so far (this might be
due to the fact that we covered a new code location, or that we ob-
served a write to a shared variable combined with a symbolic value
which was not observed before). Note that, at line 17, we extended
an unrealizable ISC by interferences resulting from writes stored
inforest . When we observe a write event which was not in forest
back then, we have to reconsider all previously unrealizable ISCs
and have to extend them by interferences from this new write event
to reads in these ISCs. This happens in lines 21 and 23. There,
each previously unrealizable ISC C with degree smaller than kis
re-explored with the newly observed writes. Each such write event
must occur after sink(C)was executed and, therefore, has degree k.
Then, the ISCs generated in line 23 have degree greater than kand
are added to the according worklists in lines 24‚Äì27.
Algorithm ExploreISCs .Algorithm ExploreISCs explores ISCs
by extending existing ISCs with new interferences. Let nrbe a
read node in a given ISC C. Letnwbe a write node in forest
and letIwbe the causal interference scenario of nw, i.e.,Iw=
CIS(forest;nw). To create an ISC C00which extends Cby the in-
terference (nw;nr), the algorithm checks the following conditions:
(i)nrandnware in different threads, i.e., Th(nr)6=Th(nw).
(ii)nrreads from the variable to which nwwrites to, i.e., if
Ac(nr) =rd(x;r), for some symbolic variable r, then Ac(nw)
is of the form wt(x;val ), for some expression val.
(iii)nris not involved in any interference in CorIw, i.e.,nr=2
RI(C)andnr=2RI(Iw)(cf. Section 5).
(iv) CandIware compatible (cf. Def. 5.4 and Remark 5.5).Algorithm 3: ExploreISCs (ISC C;write nodes Wrts ): ISC-
Set
1ISC-Set iscs ;
2 foreachnr2read-nodes (C)nRI(C)do
3 letAc(nr) =rd(x;r)for somex
4 foreach nw2Wrts do
5 if Ac(nw) =wt(x;val )for somevalthen
6 if Th(nw)6=Th(nr)then
7 Iw CIS(forest;nw)
8 if nr=2RI(Iw)andcompatible (C;Iw)then
9 C0 merge (C;Iw)
10 C00 extend C0by interference (nw;nr)
11 iscs iscs[fC00g
12 return iscs
If all conditions are satisÔ¨Åed, then, in line 9, CandIware merged as
described in Remark 5.5 and form an ISC C0. Then, in line 10, C0is
extended to ISC C00by introducing the interference edge (nw;nr).
Alg. 3 collects all generated ISCs (cf. line 11) and Ô¨Ånally returns
them in line 12. Note that each generated ISC C00has the same
sink as C, i.e., sink(C00) = sink(C), and has at least one more
interference than C, i.e., Degree (C00)Degree (C)+1. The degree
ofC00might increase by more than one interference, because Iw
might contain interferences which are not present in C, but, due to
the merge, they show up in C0andC00as well.
7.2 Soundness and Completeness
In Section 6, we discussed data constraints DC(I)and tempo-
ral constraints TC(I)corresponding to an interference scenario I.
The following theorem shows how these constraints can be used to
Ô¨Ågure out whether an ISC is realizable or not.
THEOREM 7.2 (S OUNDNESS ).Let C be an ISC in Wkwhere
0kkmax. Then, C is realizable if and only if DC (C)and
TC(C)are satisÔ¨Åable.
Algorithm RealizabilityCheck .Given an ISC Cwith sink(C) =
n, the realizability of Cis checked by determining whether DC(C)
andTC(C)are both satisÔ¨Åable. Assume that Cis realizable and 0
andiare solutions for TC(C)andDC(C), respectively. Then Real-
izabilityCheck algorithm (called in Alg. 1 at line 13) returns a triple
(result;i;)where result determines that Cis realizable and =
0(Th(n); )that forces the sequential execution of thread Th(n)
after0. According to Thm. 7.2, our test generation approach is
sound, i.e., if the program is executed with input vector i, and ac-
cording to schedule 0, then the branch node nwill be covered.
THEOREM 7.3 (C OMPLETENESS ).Given a deterministic pro-
gramPand a bound kmax, Alg. 1 covers all branches of Pthat
require at most kmaxmany interferences to be covered.
Like all completeness results in concolic testing Thm. 7.3 re-
lies on several idealizing assumptions. The theorem states that for
deterministic programs without non-linear arithmetics and calls to
external library functions, our con2colic testing algorithm covers
all branches of Pthat require at most kmaxmany interferences to
be covered. In practice, concolic execution falls back upon con-
crete values observed during execution to handle non-linear com-
putations or calls to external library functions, for which no goodsymbolic representation is available. For that reason, it always un-
derapproximates program behaviours.
Note that if we do not stop Alg. 1 after it performed kmax-many
iterations of the main loop or after full branch coverage is achieved,
then Alg. 1 actually achieves, in the limit, a stronger coverage than
branch coverage. We leave an exact characterization of the cover-
age achievable by Alg. 1 for future work.
7.3 Unforeseen Interferences
In order to drop assumption (i) stated at the beginning of Section
7, we need to make the following changes: (1) The concolic execu-
tion engine stops as soon as an unforeseen interference is observed
and returns a symbolic trace that ends with the read event of the
unforeseen interference. (2) Alg. 2 is extended as follows: When
building forest Fin line 1, we add a distinguished dangling node
which is labeled with the read event of the unforeseen interference.
However, we do not add the unforeseen interference toF. As an
effect, Alg. 2 will then, in line 5, create a causal interference sce-
nario for this special dangling node. Consequently, Alg. 1 will then
try to realize this interference scenario, Ô¨Årst without introducing an
interference. If this is not possible then it will introduce some in-
terferences later. This enables us to explore the interference space
and build the interference forest in a systematic way, i.e., while
processing Wk, the interference forest is updated with interference
scenarios of degree k. Observe that we have to extend the notion of
CIS to allow sink nodes labeled with read events. Since our algo-
rithms never make use of the fact that the sink of a CIS is a branch
node, the overall testing algorithm is not changed. Barriers can be
handled in a similar way; due to space restrictions, we omit details.
7.4 Lock-Protected Writes
Consider an ISC Cwith sink(C) =n. It might be the case
that for a thread Ti6=Th(sink(C)), the last node in GTiis la-
beled with a write event that happened while Tiwas holding some
locks. This may cause the following problems: (i) Cmight never
become realizable, e.g., if nis also protected by the same locks,
thenTidoes not have any chance to release the locks. (ii) Cmight
be realizable but the test generated for Cmay lead to a deadlock,
e.g. thread Th(sink(C))acquires any of these locks later. To solve
these problems, whenever we create a new ISC C, we extend all
thread-local sub-scenarios of Caccording to forest , with the excep-
tion of thread Th(sink(C)), such that for each thread Tithe last
node inGTiis not protected by any lock. As an example consider
the ISC shown in Fig. 7. There, T0holds a lock at node 10. There-
fore, the ISC is extended to include node 11 where T0releases the
lock. Note that this extension might not be unique if the release of a
lock can happen in different code branches. To preserve complete-
ness, we consider all possible extensions, i.e., we actually generate
a set of ISCs. Furthermore, like in the case of re-exploration of
unrealizable ISCs due to newly observed write nodes, we have to
re-explore ISCs whenever we observe new lock-release events. In
our benchmarks, the extensions until lock-free nodes were unique.
7.5 Optimizations for ISC Exploration
Unsat-Core Guidance. In Alg. 1, ISC exploration is performed
by adding new interferences to ISCs. In case an ISC is not real-
izable, it might be the case that no extension of the ISC by inter-
ferences will ever get realizable. From the unsatisfying core of the
constraint systems deÔ¨Åned in Section 6, we can identify such sit-
uations. Let C= (V;E;` )be an ISC. Data constraint DC(C)is
then equal to DC branch(V)^DC interfere (C)^DC local(C). Extending
Cto a new interference scenario C0by adding an interference to C
removes some predicates in DC local(C)from DC(C0)but the pred-icates in DC branch(V)andDC interfere (C)remain as part of DC(C0).
Therefore, if the unsatisfying core of DC(C)does not involve pred-
icates from DC local(C), we can conclude that DC(C0)or any other
extension of Cwill not be realizable as well and, therefore, we can
exclude Cfrom further exploration. Analogously, if TC(C)is not
feasible and no constraints from WRC localare involved in the unsat-
isfying core then, again, we can conclude that Cwill not become
realizable by adding new interferences and we can exclude Cfrom
further exploration. Furthermore, in both cases, the unsat core can
be used to guide the exploration by introducing an interference for
a so far local read whose constraints are involved in the unsat core.
Duplication Freedom. Alg. 1 allows multiple instantiations of the
same ISC. For example, suppose that an ISC Cbecomes realizable
by introducing interferences for two reads. The algorithm can Ô¨Årst
select any of these reads and generate two ISCs in which one of
these reads is interfered. Then, in the future, these two ISCs can be
extended such that the other read is also interfered, generating two
instances of the same ISC. To avoid duplication of ISCs, we use a
caching mechanism. In this way, an ISC will be processed only if
it is not already in the cache.
Prioritized Exploration. While processing each worklist Wk,
we can choose to prioritize the ISCs in Wk. For example, in our
implementation, we assign higher priorities to ISCs which would
cover some yet uncovered part of program code (in case they are
realizable). Based on this exploration strategy, Alg. 1, at line 11
Ô¨Årst processes ISCs with higher priorities.
8. EXPERIMENTS
Our con2colic testing approach is implemented in a tool called
CONCREST as an extension to C REST [1]. We use a standard col-
lection of concurrency benchmarks to evaluate its effectiveness.
We ran our experiments on a dual-core 64-bit Linux machine with
3.2GHz and 16GB RAM.
Benchmarks. bluetooth is a simpliÔ¨Åed version of the Bluetooth
driver from [15]. sor is from Java Grande multi-threaded bench-
marks (which we translated to C). ctrace-a andctrace-b are
two test drivers for the ctrace library. apache-a andapache-b
are test drivers for APACHE FTP server from BugBench [9]. splay
andrbTree are test drivers for a C library implementing several
types of trees. aget is a multi-threaded download accelerator.
pfscan is a multi-threaded Ô¨Åle scanning program. Finally, art
is an example designed by us to evaluate the scalability of our ap-
proach when the number of threads increase. It has the property
that there is a new assertion in it every time we increase the num-
ber of threads by one.
Experimental Results. In our experiments we set kmax= 100
(at most 100 interferences) and a timeout of 2 hours. The results
are presented in Table 1. We learned the following important facts:
(i)CONCREST is effective at Ô¨Ånding bugs. All the known bugs
were discovered. (ii)All bugs discovered by C ONCREST in bench-
marks were the result of a branch which would not be covered se-
quentially. (iii)All bugs were discovered under a relatively small
number of interferences (maximum 4). (iv)On average, a substan-
tial number of branches were not sequentially coverable and were
only covered after interferences were introduced, e.g., for rbTree
which has Ô¨Åxed input, branch coverage increases from 67 to 95
(maximum number of coverable branches). In the lack of a bug
found, reaching this maximum provides guarantees to the tester
that, e.g., no assertions in the code can be violated. (v)We set the
maximum number of interferences to be 100, but the actual bound
explored by C ONCREST is much smaller. This is because in most
cases (with the exception of 2 timeout cases), we either achievedTable 1: Experimental Results
Benchmark ]Threads ]Inputs ]Branches ]Branches Max k reached ]Branches Bug found ]ISC time
(total) k=0/1/2/3/4/... (reason) k=0 !Max k (k) (total) (total)
bluetooth 3 2 24 14/8/2 2 (Full Cov.) 14 !24 yes(2) 282 0.5
sor 3 - 48 37/8/0/0/3 4 (Full Cov.) 37 !48 yes(3) 145 0.6
ctrace-a 3 - 94 54/3 5 (Max Cov.) 54 !57 yes(1) 28 0.7
apache-a 3 3 72 41/0/1 11 (Max Cov.) 41 !42 yes(2) 392 1.0
splay 3 - 112 46/14/4 15 (Max Cov.) 46 !64 no 3501 6.2
apache-b 3 3 48 35/3 11 (Max Cov.) 35 !38 yes(1) 22150 15.4
aget 3 - 88 56/0/1 21 (Max Cov.) 56 !57 yes(2) 23197 170.4
rbTree 3 - 146 67/22/4/2 24 (Max Cov.) 67 !95 no 77037 296.3
pfscan 3 2 130 92/0/0/0/1 4 (Timeout) 92 !93 yes(4) 3012548 7200.0
ctrace-b 3 - 128 75/5 2 (Timeout) 76 !81 yes(1) 315639 7200.1
art2 3 2 8 7/1 1 (Full Cov.) 7 !8 yes(1) 80 0.3
art3 4 3 12 10/1/1 2 (Full Cov.) 10 !12 yes(2) 17942 21.8
art4 5 4 16 13/1/1/1 3 (Full Cov.) 13 !16 yes(3) 2842066 197.1
art5 6 5 20 16/1/1/1/1 4 (Full Cov.) 16 !20 yes(4) 10851573 741.1
]Branches : number of static branches, i.e. number of basic code blocks. k: number of interferences. Full Cov. : all branches are covered. Max Cov. : all
possible interference scenario candidates are explored. ]ISC: number of explored interference scenario candidates. "14/8/2" means 14 branches covered at
k= 0, 8 (new) branches at k= 1, and 2 (new) branches at k= 2.14!24indcates the difference between the number of branches covered sequentially
(14) and the total number of branches covered (24).
full branch coverage or explored all possible ISCs (i.e. no more
branches are coverable). (vi)Our approach scales well as the num-
ber of threads increase; see art.
There are cases where maximum branch coverage is achieved,
but the number does not coincide with the total number of static
branches. These are due to (sanity-check type) assertions in the
code which were never meant to be violated.
Table 2 presents the effect of the optimizations discussed in Sec-
tion 7.5 for pfscan (as an example). In this benchmark, the bug
(i.e. assertion violation) is discovered at k= 4. When there is no
optimization enabled, it runs out of memory with k= 2. The efÔ¨Å-
cacy of unsat-core guidance is clear because without this optimiza-
tionkcannot go higher than 2. In fact, to move to k= 4and catch
the assertion violations, both unsat-core guidance and duplication-
freedom have to be enabled. The effect of prioritized exploration
can be observed by comparing rows 1 and 3: when prioritization is
enabled the assertion violation is found earlier.
We compared our tool with Poirot [13] on some of the bench-
marks. Poirot exploits context-bounding sequentialization of con-
current programs and performs a static analysis to check for safety
properties. A side by side comparison with Poirot (when it does not
aim for coverage in the same sense as C ONCREST is not meaning-
ful). Our experiments showed that Poirot did not scale well for the
programs that we checked. For example, the bug in sorwas found
within a second by our tool, but Poirot was not able to Ô¨Ånd it for
context-bound of 2, 3, and 4 within 1.5hrs (for each bound).
9. RELATED WORK
In the introduction, we surveyed a number of techniques for test-
ing concurrent programs. Here, we focus on a subset of them which
are closer to con2colic testing and discuss how con2colic testing
Table 2: Effects of optimizations on benchmark pfscan .
Row U P D Assertion Coverage t[s] Max k Total t[s]
1 + + + 4554 4 7200 (timeout)
2 - + + - 2 7200 (timeout)
3 + - + 6701 4 7200 (timeout)
4 + + - - 3 7200 (timeout)
5 - - + - 2 7200 (timeout)
6 - + - - 2 out of memory
7 + - - - 3 7200 (timeout)
8 - - - - 2 out of memory
U= unsat core guidance, P= prioritized exploration, D= duplication free-
dom. Symbols + and - represent optimizations being on and off, respec-
tively. Last three columns correspond to the time it took for the assertion to
be covered, the maximum k explored, and the total time for testing.distinguishes from them in more detail.
Extensions of concolic testing to concurrent programs have been
proposed before. In jCute [18], the program is executed concoli-
cally and data-races in the observed execution are identiÔ¨Åed. Then,
either the schedule is Ô¨Åxed and new input values are generated for
the same concurrent schedule, or a new schedule is produced by
keeping the inputs Ô¨Åxed and simply re-ordering the events involved
in a data-race. In contrast to con2colic testing, jCute is incomplete
due to its race-based schedule selection heuristic. Moreover, if a
timeout occurs, it is impossible to quantify the partial work done as
a meaningful coverage measure for the program.
Similar to con2colic testing, a recent related work [17], gener-
ates tests with the aim of increasing code coverage of concurrent
programs. However, it uses an under-approximation of the pro-
gram (i.e. a set of program runs), as opposed to the actual program.
Therefore, it is incomplete. Furthermore, test generation is done by
solving a constraint system that encodes the scheduling constraints
and the data-Ô¨Çow constraints together while considering the whole
computation in the runs. However, con2colic testing generates sep-
arate constraint systems for schedule generation and input gener-
ation which are based on only shared variable accesses and syn-
chronization events; This reduces the complexity of the constraint
systems drastically and increases scalability.
Some other recent work [21, 20] build a framework based on
over and under-approximations of interferences of the programs
to check for safety properties. Like [17], they build a constraint
system which includes local computation as well as global compu-
tation. Therefore, to reduce scalability issues, they focus only on
program slices obtained from program executions.
Finally, several sequentialization techniques [8, 23, 15, 13, 16]
have been proposed to reduce the problem of verifying concurrent
programs to veriÔ¨Åcation of sequential programs. However, one can-
not apply traditional sequential testing techniques on the sequen-
tialized programs obtained by many of them as they are highly
non-deterministic. Furthermore, to execute sequential programs
obtained by [8, 13, 16], we should guess the values of shared vari-
ables at the beginning of each context. However, wrong guesses
might result in reaching invalid program states.
10. ACKNOWLEDGEMENTS
This work was supported by the Canadian NSERC Discovery
Grant, the Vienna Science and Technology Fund (WWTF) grant
PROSEED, and the Austrian National Research Network S11403-
N23 (RiSE) of the Austrian Science Fund (FWF).11. REFERENCES
[1] J. Burnim and K. Sen. Heuristics for Scalable Dynamic Test
Generation. In ASE, pages 443‚Äì446, 2008.
[2] F. Chen, T. Serbanuta, and G. Ro¬∏ su. JPredictor: A Predictive
Runtime Analysis Tool for Java. In ICSE , pages 221‚Äì230,
2008.
[3] M. Emmi, S. Qadeer, and Z. Rakamari ¬¥c. Delay-Bunded
Scheduling. SIGPLAN Not. , 46(1):411‚Äì422, 2011.
[4] A. Farzan, P. Madhusudan, N. Razavi, and F. Sorrentino.
Predicting Null-Pointer Dereferences in Concurrent
Programs. In FSE, pages 47:1‚Äì47:11, 2012.
[5] P. Godefroid. Model Checking for Programming Languages
Using VeriSoft. In POPL , pages 174‚Äì186, New York, NY ,
USA, 1997. ACM.
[6] P. Godefroid, N. Klarlund, and K. Sen. DART: Directed
Automated Random Testing. In PLDI , pages 213‚Äì223, 2005.
[7] P. Godefroid, M. Y . Levin, and D. A. Molnar. Active
Property Checking. In EMSOFT , pages 207‚Äì216, 2008.
[8] A. Lal and T. Reps. Reducing Concurrent Analysis Under a
Context Bound to Sequential Analysis. Form. Methods Syst.
Des., 35:73‚Äì97, 2009.
[9] S. Lu, Z. Li, F. Qin, L. Tan, P. Zhou, and Y . Zhou.
BugBench: Benchmarks for Evaluating Bug Detection Tools.
InWorkshop on the Evaluation of Software Defect Detection
Tools , 2005.
[10] M. Musuvathi and S. Qadeer. Iterative Context Bounding for
Systematic Testing of Multithreaded Programs. SIGPLAN
Not., 42(6):446‚Äì455, 2007.
[11] M. Musuvathi, S. Qadeer, and T. Ball. CHESS: A Systematic
Testing Tool for Concurrent Software, 2007.
[12] S. Park, S. Lu, and Y . Zhou. Ctrigger: Exposing Atomicity
Violation Bugs from Their Hiding Places. In ASPLOS , pages
25‚Äì36, 2009.
[13] S. Qadeer. Poirot: A Concurrency Sleuth. In ICFEM , pages
15‚Äì15, 2011.[14] S. Qadeer and J. Rehof. Context-Bounded Model Checking
of Concurrent Software. In In TACAS , pages 93‚Äì107.
Springer, 2005.
[15] S. Qadeer and D. Wu. KISS: Keep It Simple and Sequential.
SIGPLAN Not. , pages 14‚Äì24, 2004.
[16] Z. Rakamari ¬¥c. STORM: Static Unit Checking of Concurrent
Programs. In ICSE , pages 519‚Äì520, 2010.
[17] N. Razavi, F. Ivancic, V . Kahlon, and A. Gupta. Concurrent
Test Generation Using Concolic Multi-Trace Analysis. In
APLAS , 2012.
[18] K. Sen. Scalable Automated Methods for Dynamic Program
Analysis . PhD thesis, University of Illinois at
Urbana-Champaign, 2006.
[19] K. Sen and G. Agha. Concolic Testing of Multithreaded
Programs and Its Application to Testing Security Protocols.
Technical Report UIUCDCS-R-2006-2676, University of
Illinois at Urbana Champaign, 2006.
[20] N. Sinha and C. Wang. Staged Concurrent Program
Analysis. In FSE, FSE‚Äô10, pages 47‚Äì56, 2010.
[21] N. Sinha and C. Wang. On Interference Abstractions. In
POPL , pages 423‚Äì434, 2011.
[22] F. Sorrentino, A. Farzan, and P. Madhusudan. PENELOPE:
Weaving Threads to Expose Atomicity Violations. In FSE,
pages 37‚Äì46, 2010.
[23] S. Torre, P. Madhusudan, and G. Parlato. Reducing
Context-Bounded Concurrent Reachability to Sequential
Reachability. In CAV, pages 477‚Äì492, 2009.
[24] C. Wang, S. Kundu, M. Ganai, and A. Gupta. Symbolic
Predictive Analysis for Concurrent Programs. In FM, pages
256‚Äì272. Springer, 2009.
[25] W. Zhang, J. Lim, R. Olichandran, J. Scherpelz, G. Jin, S. Lu,
and T. Reps. ConSeq: Detecting Concurrency Bugs through
Sequential Errors. In ASPLOS , pages 251‚Äì264, 2011.