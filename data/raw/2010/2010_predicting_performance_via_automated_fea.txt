Predicting Performance via Automated Feature-Interaction Detection
Norbert Siegmund,∗Sergiy S. Kolesnikov,†Christian K ¨astner,‡Sven Apel,†
Don Batory,§Marko Rosenm ¨uller,∗and Gunter Saake∗
∗University of Magdeburg, Germany
†University of Passau, Germany
‡Philipps University Marburg, Germany
§University of Texas at Austin, USA
Abstract —Customizable programs and program families pro-
vide user-selectable features to allow users to tailor a program
to an application scenario. Knowing in advance which feature
selection yields the best performance is difﬁcult because a direct
measurement of all possible feature combinations is infeasible.
Our work aims at predicting program performance based on
selected features. However, when features interact, accurate
predictions are challenging. An interaction occurs when a
particular feature combination has an unexpected inﬂuence
on performance. We present a method that automatically
detects performance-relevant feature interactions to improve
prediction accuracy. To this end, we propose three heuristics
to reduce the number of measurements required to detect
interactions. Our evaluation consists of six real-world case stud-
ies from varying domains (e.g., databases, encoding libraries,
and web servers) using different conﬁguration techniques (e.g.,
conﬁguration ﬁles and preprocessor ﬂags). Results show an
average prediction accuracy of 95 %.
I. I NTRODUCTION
There are many ways to customize a program. Commonly,
a program uses command-line parameters, conﬁguration ﬁles,
etc. [ 1]. Another way is to derive tailor-made programs at
compile-time using product-line technology. In product-line
engineering, stakeholders derive tailored programs by means
of a program generator to satisfy their requirements [ 2]. The
generation process is based on features , where a feature is a
stakeholder-visible behavior or characteristic of a program [ 2].
By mapping features to implementation units, a generator
produces a program based on a user’s feature selection. In
this paper, we use product-line terminology and call any
customization option that stakeholders can select at compile-
time or load-time a feature of a program.
Stakeholders are also interested in non-functional prop-
erties of a program. For example, a database management
system is usually customized to achieve maximum perfor-
mance when used on a server, but is customized differently
for low energy consumption when deployed on a battery-
supplied system (e.g., on a smartphone or sensor node).
Besides the target platform, other factors inﬂuence non-
functional properties of a program. Database performance
depends on the workload, cache size, page size, disk speed,
reliability and security features, and so forth. Non-functional
properties can be customized by selecting a speciﬁc set offeatures, called a conﬁguration , that yields a valid program.
However, ﬁnding the best conﬁguration efﬁciently is a hard
task. There can be hundreds of features resulting in myriads of
conﬁgurations: 33 optional and independent features yields a
conﬁguration for each human on the planet, and 320 optional
features yields more conﬁgurations than there are estimated
atoms in the universe. To ﬁnd the conﬁguration with the best
performance for a speciﬁc workload requires an intelligent
search; brute-force is infeasible.
We aim at predicting a conﬁguration’s non-functional
properties for a speciﬁc workload based on the user-selected
features [ 3][4]. That is, we aggregate the inﬂuence of each
selected feature on a non-functional property to compute the
properties of a speciﬁc conﬁguration. Here, we concentrate
on performance predictions only. Unfortunately, the accuracy
of performance predictions may be low, because many
factors inﬂuence performance. Usually, a property, such as
performance, is program-wide: it emerges from the presence
and interplay of multiple features. For example, database
performance depends on whether a search index or encryption
is used and how both features operate together. If we
knew how the combined presence of two features inﬂuences
performance, we could predict a conﬁguration’s performance
more accurately. Two features interact if their simultaneous
presence in a conﬁguration leads to an unexpected behavior,
whereas their individual presences do not [5][6].
Today, developers detect feature interactions by analyzing
the program (e.g. source code or control ﬂow) or speciﬁcation
of features [ 7]. These and similar approaches require sub-
stantial domain knowledge, exhaustive analysis capacities, or
availability of source code to achieve the task. Furthermore,
each implementation technique (e.g., conﬁguration options,
#ifdef statements, generators, components, and aspects)
requires a specialized solution. To the best of our knowledge,
there is no generally applicable approach that treats a
customizable program as a black box and detects performance
feature interactions automatically.
We improve the accuracy of predictions in two steps: (i)
we detect which features interact and (ii) we measure to
what extent they interact. In our approach, we aim at ﬁnding
the sweet spot between prediction accuracy, generality in
terms of a black-box approach, and measurement effort.© ACM, 2012. This is the author's version of the work. It is posted here 
by permission of ACM for your personal use. Not for redistribution.The distinguishing property of our approach is that we
neither require domain knowledge, source code, nor complex
program-analysis methods, and we are not restricted to
special implementation techniques, programming languages,
or domains. Overall, we make the following contributions:
•An approach for efﬁcient (in terms of measurement
complexity) automated detection and quantiﬁcation of
performance feature interactions to enable an accurate
prediction of a conﬁguration’s performance.
•An improved tool, called SPL Conqueror [8], to measure
performance, detect feature interactions, and predict
performance in an automated manner.
•A demonstration of practicality and generality of our
approach with six customizable programs and product
lines from different domains, programming languages,
and customization mechanism.
•A 95 percent prediction accuracy when feature interac-
tions are included, which is a 15 percent improvement
over an approach that takes no interactions into account.
In contrast to our previous work [ 3][8], we (1) do not
rely on domain knowledge, (2) reduce the effort for pair-
wise measurement, (3) measure and predict performance
instead of footprint size, (4) incorporate higher-order feature
interactions, and (5) evaluate our approach with additional
industrial product lines.
II. A M ODEL OF FEATURE INTERACTIONS
Our work relies on a recent model of feature composi-
tion [ 9]. If program Pconsists of features a,b, andc,w e
write:P=a·b·cwhere·denotes the associative and
commutative composition of features. Evaluating a·b·c
generates P.1
Features interact: Features that perform one way in
isolation may behave differently when other features are
present; interactions may affect semantics as well as (in our
case) performance of the overall system. A classic example
is a ﬂood-control ( fc) sensor working with a ﬁre-alarm ( fa)
sensor [ 10]. If only one of fcorfais present, the behavior is
unambiguous: Water is turned on when ﬁre is detected and
turned off when a ﬂood is detected. When fcandfaare both
present, there is an interaction fc#fathat turns water off
after the ﬁre sensor turned water on to prevent a ﬁre. In code,
we make this interaction explicit such that we can control
this interaction with an appropriate behavior. Nevertheless,
the interaction is present whether we handle it or not.
More generally, if a program Pcontains features aandb,
it should also include the interaction a#b. Basic mathematics
encodes these ideas. When a stakeholder wants features a
andb, (s)he also wants their interaction a#b(because a#b
says how aandbare to work correctly together, e.g., keeping
water on when ﬁre and ﬂood are detected). The associative
1Henceforth, capital letters denote compositions of one or more terms,
lowercase letters aare terms (features or feature interactions).and commutative operation ×expands a given conﬁguration
to all feature terms and all feature-interaction terms:2
a×b=a#b·a·b (1)
That is, a program does not only contain the behavior of
each individual feature, but also the interaction behaviors
among all features. Many of these feature interactions have
no observable effect; only some of them are relevant. In
this paper, we propose heuristics to detect only the relevant
performance feature interactions.
To relate the above abstract model to performance pre-
diction, we state that performance of a feature composition
Π(a·b)be the sum of their individual performance values:3
Π(a·b)=Π (a)+Π(b) (2)
From (1) and (2), we estimate P’s performance as follows:
Π(P)=Π ( a×b×c)
=Π (a·b·c·a#b·a#c·b#c·a#b#c)
=Π (a)+Π(b)+Π(c)+
Π(a#b)+Π(a#c)+Π(b#c)+Π(a#b#c)
To improve prediction accuracy, we need to determine
the inﬂuence of an interaction on performance. We use a
basic result that follows from (1) and (2). If we can measure
a performance value for Π(a)andΠ(b), we certainly can
measure the value of Π(a×b). We therefore know the value
ofΠ(a#b):
Π(a#b)=Π ( a×b)−Π(a)−Π(b) (3)
Here is the challenge: a product of nfeatures yields
O(2n)terms. We cannot compute a value for each term,
as this is infeasible for anything beyond programs with few
features. Furthermore, (3) assumes that we can measure the
performance inﬂuence of each feature in isolation. This is
not always possible. We avoid both problems by composing
multiple terms that cannot be separately measured as a
single term, called a delta . Given a base conﬁguration C,
we compute the impact of a feature aonC’s performance
as the performance delta induced by feature a:
∆aC=Π (a×C)−Π(C) (4)
From (4) and (1), an equivalent deﬁnition of ∆aCis:
∆aC=Π (a×C)−Π(C)//(4)
=Π (a#C)+Π(a)+Π(C)−Π(C)//(1)
=Π (a#C)+Π(a) (5)
That is,∆aCis the performance contribution of aby itself
plus the performance contributions of a’s interaction with all
2Commutativity and other axioms of sequential, interaction, and product
composition are spelled out in [ 9]; details beyond what is presented here
are non-essential to this paper.
3As a limitation of this approach, we require additivity of performance
measurements.terms in C. (IfCis the empty set, then ∆aC=Π (a)). If
Cis a product of ifeatures, ∆aCis a sum of O(2i)terms.
As we demonstrate in subsequent sections, knowing ∆aC
for some Cis often sufﬁcient to accurately predict the
performance of programs that include a. We do not need to
assign values to each of ∆aC’s terms; we measure only two
variants of (4) instead of 2iterms. Herein lies the key to the
efﬁciency and practicality of our approach.
III. P REDICTING PERFORMANCE
We predict performance (and other non-functional prop-
erties) by measuring the inﬂuence of each feature, its delta,
and summing the deltas for all relevant features. With few
measurements (linear complexity in the number of features),
we can predict performance of all conﬁgurations (exponential
in the number of features). Although the approach is simple,
it yields surprisingly good results.
The general concept of quantifying the inﬂuence of each
feature on performance is as follows: For each feature
a, we ﬁnd a conﬁguration min(a)that is minimal in the
number of features such that min(a)does not contain aand
bothmin(a)anda×min(a)are valid conﬁgurations.4We
determine each feature’s delta as:
∆amin=Π (a×min(a))−Π(min(a))
Consider the feature model in Figure 1, which has ﬁve
features. The minimal conﬁguration for each feature is:5
Feature min()
b {}
i b
t b
e b
db×e
We need only ﬁve measurements to determine the inﬂuence
of each feature (all values in our example are measured in
transactions per second):
∆bmin=Π (b)−0 = 100
∆imin=Π (b×i)−Π(b)= 15
∆tmin=Π (b×t)−Π(b)= −10
∆emin=Π (b×e)−Π(b)= −20
∆dmin=Π (b×e×d)−Π(b×e)= −10
4Features may not be independent, such that we cannot measure arbitrary
conﬁgurations. We explored calculating deltas in the presence of complex
domain dependencies previously [3]. It is outside the scope of this paper.
With constraints between features, in principle, there can be multiple
minimal conﬁgurations (for example, in the presence of mutually exclusive
features). In this case, we use any minimal conﬁguration. Furthermore,
we admit the empty or null program as a minimal conﬁguration when
determining the performance of a root feature.
5A feature model, a standard idea in product-line engineering [ 2],
deﬁnes features and their relationships. Features are decomposed into a
hierarchical structure and are marked as mandatory, optional, or mutually
exclusive. To select a child feature, the parent feature must be selected.
A conﬁguration is valid if its feature selection fulﬁlls all constraints (i.e.,
arbitrary propositional formulas) of the feature model.i t e b
bt
tb
i= Index, t= Transactions, e= Encryption, d= Decryption, b= Base; 
Measured values are transactions per second.ib eb
i#tb
i#ttb×iDBMSmandatory
optionalFeature
be
ibit
bitb)
tbd
i t e bDBMS
dimplies
implies
i t e bDBMS
d
impliesdb×ebed
b be
ibØØ
Figure 1. Measuring deltas for features and interactions.
To predict the performance of a conﬁguration, we simply
add the deltas of all relevant features. For example, for
conﬁguration b×t×i, we predict ∆bmin+∆tmin+∆imin=
100−10+15 = 105 .
Unfortunately, this prediction scheme is inaccurate. As
mentioned earlier, when measuring feature deltas, we might
obtain very different results when using different conﬁg-
urations. Consider Figure 1b, which computes the delta
for feature tfor a different conﬁguration. Our ﬁrst value,
computed above, was ∆tmin=−10, whereas the newly
computed value is ∆tb×i=−5. Consequently, predictions
for the same conﬁguration b×t×iwill differ when using
∆tmin (105) or ∆tb×i(110). The difference is due to
feature interactions. Detecting and quantifying the inﬂuence
of interactions allows us to overcome the differences among
different deltas leading to consistent predictions. The question
is: Which features interact that cause this discrepancy?
If we know that two features interact, we can improve
our prediction by measuring the delta for their interaction.
Suppose conﬁguration Chas both features aandb. The
contribution of the interaction of aandbtoCis:
∆(a#b)C=Π (a#b×C)−Π(C)
=Π (a#b#C)+ Π(a#b)+Π(C)−Π(C)
=Π (a#b)+Π(a#b#C) (6)
Similar to the delta of a feature, the delta of interaction a#b
includes the interaction a#band all interaction terms of a#b
with terms in C.
In Figure 1c, we illustrate such a measurement forinteraction i#t. Knowing the interaction’s delta improves
our predictions: in our example, it patches the value of
∆tmin. If more than two features interact (a.k.a., higher-
order interactions [ 11]), we proceed in a similar way. The
challenge is how to ﬁnd interactions that actually contribute
to performance out of an exponential number of potential
interactions.
IV . A UTOMATED DETECTION OF FEATURE INTERACTIONS
Our goal is to identify feature interactions automatically
using a small number of measurements. Our approach
consists of two steps: (1) identifying features that participate
in some interactions (called interacting features ) and (2)
ﬁnding minimal combinations of features that actually cause
a feature interaction. We use the setting from Figure 1 as
our running example.
A. Detecting Interacting Features
Our ﬁrst step is to identify features that interact. The
rationale is to reduce our search space. For example, suppose
a program has 16 features, in which 4 features interact, the
rest do not. We have to look only at 24=1 6 instead of
216= 65536 conﬁgurations to detect interactions.
In the presence of interacting features, the delta for a
featureadiffers depending on which base conﬁgurations it
was measured with. We say aisnot an interacting feature
if∆aCis the same for all possible base conﬁgurations C
(within some measurement accuracy). Conversely, if ∆aC
changes with different conﬁgurations of C, we know that a
is interacting. We express this as:
ainteracts ⇔∃C, D|C/negationslash=D∧∆aC/negationslash=∆aD
To avoid measuring ∆aCfor a potentially exponential
number of conﬁgurations of C, we use a heuristic. We
determine the deltas of athat are most likely to differ,
because it is affected by the largest number of feature
interactions: We compare ∆amin, the delta for the minimal
conﬁguration, with ∆amax, a delta for a conﬁguration with
most features selected. Let max(a)anda×max(a)be two
valid conﬁgurations, such that max(a)does not contain a
and is a maximal set of features that could be composed
witha. We call max(a)amaximal conﬁguration .6∆amax
is their performance difference:
∆amax=Π (a×max(a))−Π(max(a))
The rationale of determining max(a)is that it maximizes
the number of features that could interact with a. Conse-
quently, if ∆amin and∆amax are similar, then adoes not
interact with the features that are present in max(a)but not
inmin(a). Otherwise, ainteracts with those features (we
do not know yet with which features and to what extent).
Thus, with at most four measurements per feature (two for
6We allow the empty set as a valid conﬁguration. This is necessary to
create a maximal conﬁguration for mandatory features.∆aminusingΠ(a×Min)andΠ(Min), and two for ∆amax
usingΠ(a×Max)andΠ(Max)), we discover interacting
features.7
In our running example, we determine the following max-
imal conﬁgurations and assume the following corresponding
measurements:8
Feature max() Π(max())
ib×t×e×d 60
tb×i×e×d 85
eb×i×t 110
db×i×t×e 90
Notemax(e)does not include d,a sdrequires efor
a valid conﬁguration (Figure 1). With these additional
measurements, we compute the additional deltas as follows
with six measurements:
∆imax=Π (i×max(i))−Π(max(i)) = 20
∆tmax=Π(t×max(t))−Π(max(t)) = −5
∆emax=Π ( e×max(e))−Π(max(e)) = −20
∆dmax=Π (d×max(d))−Π(max(d)) = −10
We conclude that features iandtare interacting:
∆imin/negationslash=∆imaxsince 15/negationslash=2 0
∆tmin/negationslash=∆tmaxsince−10/negationslash=−5
∆emin=∆emaxsince−20 =−20
∆dmin=∆dmaxsince−10 =−10
We know that feature iinteracts with a feature in the set
max(i)\min(i). From these candidate features, we can
exclude features b,e, andd, because their deltas do not
change. Feature tremains the only candidate for interaction.
The same conclusion is reached had we analyzed feature
t(concluding feature iis the only possible interaction
candidate). In this way, we found the feature combination
that causes an interaction. Note that if we ﬁnd more than two
interacting features, we have no information which feature
combination causes an interaction. This is the goal of the
next step.
B. Identifying Feature Combinations Causing Interactions
After detecting all interacting features, we have to ﬁnd the
speciﬁc, valid combinations that actually have an inﬂuence
on performance. Suppose we know that features a,b, andc
are interacting. We have to identify which of the following
interactions have an inﬂuence on performance: a#b,a#c,
b#c,o ra#b#c. Again, we do not want to measure all
combinations (whose number is exponential in the number
of interacting features).
7Of course, there is an obvious situation that we can not detect: when
two interactions cancel each other (e.g., one has inﬂuence +4and another
one−4), we will not detect them. We have no evidence that this situation
is common, but we are aware of its existence.
8Surprisingly, max(b)is an empty conﬁguration, because feature bis
mandatory; the only valid conﬁguration without feature bis the empty set.We use three heuristics. Each makes an assumption under
which it can detect interactions (thus improving performance
prediction) with a few additional measurements. Some
heuristics are based on the experience we gained during
the manual analysis of feature interactions (i.e, searching the
source code for nested #ifdef statements, using domain
knowledge, etc.) for the prediction of a program’s binary
footprint [ 3]. Other heuristics are based on assumptions we
make due to analyses of source-code feature interactions and
on related work (see Section VI). We explore in our evaluation
whether our heuristics actually reduce measurement effort
and improve accuracy of our predictions.
Auxiliary – Implication Graph: In all three heuristics,
we reason about feature chains in an implication graph.
Animplication graph is a graph in which nodes represent
features and directed edges denote implications between
features. Using implications, we conclude that ∆aminalways
includes the inﬂuence of all interactions with features
implied by a(i.e., all features in a’s implication chain).
For example, if feature aalways requires the presence of
featureb, then we have implicitly quantiﬁed the inﬂuence of
interaction a#bwhen computing ∆amin. This mechanism
reduces computation effort in all heuristics, especially, for
hierarchically-deep feature models and for feature models
with many constraints.
Heuristic 1 – Pair-Wise Interactions (PW): We assume
that pair-wise (or ﬁrst-order) interactions are the most
common form of performance feature interactions.
We justify this assumption as follows: Related research
often uses a similar approach: The software-test commu-
nity often uses pair-wise testing to verify the correctness
of programs [ 12][13]. Pair-wise testing was also applied
successfully to test feature interactions in the communication
domain [ 14] and to ﬁnd bugs in product-line conﬁgura-
tions [ 15]. Furthermore, analysis of variability in 40 large-
scale programs showed that structural interactions are mostly
between two features [ 16]; although structural interactions do
not necessarily cause performance feature interactions, we
assume that this distribution also holds for performance,
because the additional code may have some affect on
performance.
Within the set of interacting features, we use this heuristic
to locate pair-wise interactions ﬁrst (as they are the most
common). We search for higher-order interactions with the
remaining heuristics.
Heuristic 2 – Composition of Higher-Order Interactions
(HO): We assume that second-order feature interactions
(i.e., interactions among three features) can be predicted
by analyzing already detected pair-wise interactions.
The rationale is, if three features interact pair-wise in
any combination, they likely also participate in a triple-wise
interaction. That is, if we know that two of these three
interactions {a#b,b#c,a#c}are non-zero, then and only
then will we check whether a#b#chas an inﬂuence onperformance. For example, if both a#bandb#callocate
1 GB RAM, then it is likely that there is an interaction
a#b#cthat results in a lower performance (because 2 GB
RAM was allocated). We experienced this phenomenon in
previous work on measuring and predicting footprint [ 3]. A
different footprint may also indicate a possible impact on
performance, because either functionality is added (increased
footprint) or is removed (decreased footprint). This added or
removed functionality can cause performance deviations.
We do not consider other higher-order interactions to save
a huge number of measurements. Thus, we might miss some
interactions in attempt to balance measurement effort and
accuracy.
Heuristic 3 – Hot-Spot Features (HS): Finally, we
assume the existence of hot-spot features. We experienced
that there are usually a few features that interact with
many features and there are many features that interact
only with few features. High coupling between features or
many dependencies can impact the performance of the whole
system, because both features strongly interact with each
other at the implementation level.
These observations are analogous to previous work
on coupling in feature-oriented and object-oriented soft-
ware [ 17][18], and footprint feature interaction [ 3]. We
anticipate the same distribution for performance feature
interactions, following a power law[18].
Using this heuristic, we perform additional measurements
to locate interactions of hot-spot features with other inter-
acting features. Speciﬁcally, we attempt to locate second-
order interactions for hot-spot features, because they seem to
represent a performance-critical functionality in a program.
We do not identify interactions with an order higher than
three, because this increases measurement effort substantially.
C. Realization
So far, we described a general approach to (1) detect
interacting features and (2) to ﬁnd feature combinations that
cause interactions. Next, we detail how we implemented
these techniques and heuristics in our tool SPL Conqueror :
http://fosd.de/SPLConqueror
As an underlying data structure, we use an implication
graph, as described earlier. We can easily generate this
graph from a feature model using a SAT solver [ 19]. To
locate pair-wise interactions (PW heuristic), we consider
only pair-wise interactions between interacting features of
different implication chains. We do not need to determine
interactions of features belonging to the same implication
chain, because the interaction is already included in ∆amin.
Furthermore, the order of the measurements is crucial. Our
algorithm starts from the top of one implication chain and
determines the inﬂuence of interacting features with the
interacting features of another chain, also starting from
the top. Afterwards, we continue with the next chain. For
example, in Figure 2, the order we use to detect pair-wiseF2
F3
F4F1 F5
F7
F8F6F9
F11
F12F10FBase
Implication chainsinteracts
implies
F1Interacting
Feature
F2Non-interacting
Feature
Figure 2. Implication chains with interacting features.
interactions is F1#F6,F1#F7,F4#F6,F4#F7,F6#F11,
F7#F11,F1#F11,F4#F11.
To identify whether two features aandbinteract, we
compare the measured performance Π(a×b)with the per-
formance prediction of the same conﬁguration that includes
all known feature interactions up to this time. If the result
of∆a#bCexceeds a threshold (e.g., we use the standard
deviation of measurement bias as a threshold), we record it.
Next, we search for second-order interactions among
features that interact in a pair-wise fashion (HO heuristic).
Again, we perform additional measurements and compare
them to the predicted results. For example, if we noticed that
F1interacts with F7andF7interacts with F14, we would
examine whether interaction F1#F7#F14has an inﬂuence
on performance.
Finally, we search for further second-order interactions
involving hot-spot features (HS heuristic). We count the
number of interactions per feature identiﬁed so far. Next,
we compute the average number of interactions per feature.
We classify all features that interact above the arithmetic
mean as hot-spot features (other thresholds are possible, too).
With hot-spot features, we search (with the usual mechanism:
additional measurements, comparing deltas) for interactions
involving (1) a hot-spot feature, (2) a feature that already
interacts with this hot-spot feature, and (3) an interacting
feature that does not interact pair-wise with the hot-spot
feature.
V. E V ALUATION
Our approach to performance prediction is simple. But it
is the simplicity that makes it practical. We demonstrate this
with six real-world case studies.
The goal of our evaluation is to judge prediction accuracy
and the utility of our heuristics. That is, we analyze how we
detect performance feature interactions with additional mea-
surements and how detected interactions improve prediction
accuracy. To that end, we compare predictions with actual
performance measurements.
A. Experimental Setting
We selected six existing real-world programs (i.e., three
customizable programs and three product lines) with differentProject Domain Lang. LOC Features Conﬁgs
Berkeley DB CE Database C 219,811 18 2560
Berkeley DB JE Database Java 42,596 32 400
Apache Web Server C 230,277 9 192
SQLite Database C 312,625 39 3,932,160
LLVM Compiler C++ 47,549 11 1024
x264 Video Enc. C 45,743 16 1152
Table I
OVERVIEW OF SAMPLE PROGRAMS USED IN THE EV ALUATION
characteristics to cover a broad spectrum of scenarios (see
Table I). They are of different sizes (45 thousand to 300
thousand lines of code, 192 to millions of conﬁgurations),
implemented in different languages (C, C++, and Java), and
conﬁgurable with varying mechanisms (such as conditional
compilation, conﬁguration ﬁles, and command-line options).
The programs we selected have usually under 3 000
conﬁgurations. The reason is that, this way, we can actually
measure allconﬁgurations of these programs in a reasonable
time. Hence, even though it required over 60 days of
measurement with multiple computers, we could actually
perform the brute-force approach and determine accuracy of
our prediction over allconﬁgurations.
1) Setup: We measure allconﬁgurations of all programs
that affect performance (i.e., that are invoked by a benchmark).
The exception is SQLite in which we measure only the
conﬁgurations needed to detect interactions and additionally
100 random conﬁgurations to evaluate the accuracy of
predictions. We identiﬁed features in each case study and
created a feature model describing their dependencies. All
feature models and measurement results are available online
at the tool’s website.
We automated the process of generating programs accord-
ing to speciﬁc conﬁgurations and running the benchmark.
Since Berkeley DB C and Java and SQLite use compile-
time conﬁguration, we compiled a new program for each
conﬁguration that includes only the relevant features. For
Apache, LLVM, and x264, we mapped the conﬁguration to
command-line parameters. We used ﬁve standard desktop
computers for the measurements.9
We repeated each measurement between 5 to 20 times
depending on the measurement bias. It is known that
measurement bias can cause false interpretations and are
difﬁcult to control [ 20], especially for performance [ 21]. The
width of the 95 % conﬁdence interval is smaller than 10 %
of the according means. We used a range between 2 to 10 %
to specify the threshold for detecting performance feature
interactions. We use the mean of all measurements of a single
conﬁguration CasΠ(C).
2) Benchmarks: We use standard benchmarks either deliv-
ered by the vendor or used in the community of the respective
9Intel Core 2 Quad CPU 2.66 GHZ, 4GB RAM, Vista 64Bit; AMD
Athlon64 2.2GHz, 2GB RAM, Debian GNU/Linux 7; AMD Athlon64
Dual Core @2.0GHz, 2GB RAM, Debian GNU/Linux 7; Intel Core2
Quad @2.4GHz, 8GB RAM, Debian GNU/Linux 7. Each program was
benchmarked on an individual systems.application. We did not develop our own benchmark to avoid
bias and uncommon performance behavior caused by ﬂaws
in benchmark designs.
Since performance predictions are especially important in
the database domain, we list three database product lines:
Berkeley DB’s Java and C version (which differ signiﬁcantly
in their implementation and provided functionality) and
SQLite. For each program, we use the benchmark delivered
by the vendor. For example, we use Oracle’s standard
benchmark to measure the performance of Berkeley DB. The
workload produced by the benchmarks is a typical sequence
of database operations.
Furthermore, we selected the Apache Web server to
measure its performance in different conﬁgurations. We used
the tools autobench andhttperf to produce the following
workload: For each server conﬁguration, we send 810 requests
per second to a static HTML page (2 KB) provided by the
server. After 60 seconds, we increase the request rate by
30 until 2700 requests per seconds are reached. After this
process, we analyzed at which request rate the Web server
could no longer respond or produced connection errors.
LLVM is a modular compiler infrastructure. For our
benchmarks, we use the opt-tool that provides different
compile-time optimizations. We measure the time LLVM
needs to compile its standard test suite (i.e., with differ-
ent optimizations, such as inline functions and combine
redundant instructions enabled). In this case, the workload
is the program code from the LLVM test suite that has to
be compiled with the enabled optimizations.
x264 is a command line tool to encode video streams into
H.264 and MPEG-4 A VC format. The tool provides several
options, such as parallel encoding on multiple processors. We
measured the time needed to encode the video trailer Sintel
(735 MB). This trailer is used by different video-encoding
projects as a standard benchmark for encoders.
B. Results
We compute a fault rate of our prediction as the rela-
tive difference between predicted and actual performance:
|actual−predicted |
actualand accuracy as 1-fault rate in percent. As
said, we measure each program several times. From these
measurements, we compute the average performance (i.e.,
arithmetic mean) and the standard deviation. We use the
average performance to compute the delta of a feature. We
use the standard deviation to set the threshold at which we
identify a feature interaction, because we consider every
unexpected performance behavior above the measurement
error as an interaction.
1) Accuracy and Effort: In Table II, we show the results
of our six case studies: For each approach, we depict the
required number of measurements, the time needed for these
measurements, and the number of identiﬁed interactions.
Furthermore, we show the distribution of the fault rate of
our predictions with box plots. Finally, we show for eachapproach the mean fault rate of all predictions including the
standard deviation. Note that, when adding a new heuristic,
we keep the previous heuristic working, because they are
successively applied to a program.
The feature-wise (FW) approach does not use a heuristic
and does not account for feature interactions. We achieve
good predictions for programs in which interactions have
no substantial inﬂuence on performance. For example, our
predictions have an average error rate of less than 8 % for
all LLVM conﬁgurations. In contrast, we usually have a
high fault rate (e.g., over 44 % for BerkeleyDB C version)
when no interactions are considerd. The average accuracy of
performance prediction is 79.7 %.
Using the pair-wise heuristic (PW) usually improves predic-
tions signiﬁcantly (91 %, on average), because the majority of
interactions are pair-wise. The beneﬁt of implication chains
compared to the common pair-wise measurement is that
it reduces the number of measurements. For example, we
require 81 measurements to detect ﬁrst-order interactions for
x264 (see Table II), which is 82 less than 163, which would
be needed to measure all pairs of features.
With the higher-order (HO) heuristic, we achieve an aver-
age accuracy of 93.7 % for all case studies. Interestingly, for
LLVM, we could not ﬁnd a feature combination that satisﬁes
our preconditions to search for higher-order interactions. It
is important to note that this heuristic usually doubles the
number of measurements. For Apache the fault rate increases,
because measurement bias over the determined threshold lead
to a false detection of interactions. We detected these false
positives when we search for third-order feature interactions,
as we do with the hot-spot heuristic.
Finally, the hot-spot heuristic (HS) (including the other
two heuristics) improves accuracy again to 95.4 % on
average. Considering that the measurement bias for a single
measurement of the case studies Apache, LLVM, and x264
is 5 %, for SQLite it is 1 %, and for Berkeley C and Java
version it is 2 % our predictions are as accurate as the bias
of a single measurement.
2) Inﬂuence of Heuristics: Since all our heuristics are
consecutively applied, we can visualize the trade-off between
additional measurements and error rate of predictions as
in Figure 3. Dashed lines represent the average error rate
of our predictions and straight lines depict the percentage
of measurements, compared to the maximum number of
measurements. As expected, with an increasing number of
measurements the fault rate decreases. The results show
that the relative number of measurements strongly differ to
achieve the same accuracy for different programs. Further
note that we have to measure approximately 0.1 % of all
variants of SQLite, which demonstrates the scalability of our
approach.
Pair-Wise vs. Higher-Order Interactions: Look at the
Apache case study (which is similar to others): A higher-order
interaction usually improves predictions. We detected 18 ﬁrst-Effort Fault Rate (in %)
Program Appr. Measurements Time (in h) Interactions Distribution Mean±Std
Berkeley CE FW 15 (0.6 %) 3 0 44.1±42.3
PW 139 (5.4 %) 23 14● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ●● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●●●●● ● ●●●● ● ● ● ●●●● ● ● ● ● ● ●●●● ●●●● ● ● ● ● ● ● ● ● ● ● ● ● ●●●● ● ●●●● ● ●●●● ● ● ● ● ●●●● ● ●●●●●●●● ● ●●●● ● ● ●●●●●●● ● ● ● ●●●●●●●●●●●●●● ● ●●●● ● ●●● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ●● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●●●●● ● ●●●● ● ● ● ●●●● ● ● ● ● ● ●●●● ●●●● ● ● ● ● ● ● ● ● ● ● ● ● ●●●● ● ●●●● ● ●●●● ● ● ● ● ●●●● ● ●●●●●●●● ● ●●●● ● ● ●●●●●●● ● ● ● ●●●●●●●●●●●●●● ● ●●●● ● ●●● ●● ● ● ● ● ● ● ● ● ● ●3.9±5.3
HO 160 (6.3 %) 27 22● ●●●● ● ● ●●●●●● ●● ● ● ● ● ● ●●● ●●●●● ● ●●●● ●●●● ●●●● ●●●● ●●●●●●● ● ● ●●●● ●●●● ●●●●●●● ● ● ● ●●●●●●● ● ● ● ●●●●●●●●●●●●●●●●● ● ●●●● ● ● ● ● ●●● ●● ● ●●●● ● ●●●● ● ● ●●●●●●● ● ● ● ● ● ● ● ● ●●●● ●● ●●●● ● ● ●●●●●● ●● ● ● ● ● ● ●●● ●●●●● ● ●●●● ●●●● ●●●● ●●●● ●●●●●●● ● ● ●●●● ●●●● ●●●●●●● ● ● ● ●●●●●●● ● ● ● ●●●●●●●●●●●●●●●●● ● ●●●● ● ● ● ● ●●● ●● ● ●●●● ● ●●●● ● ● ●●●●●●● ● ● ● ● ● ● ● ● ●●●● ●2.8±3.7
HS 164 (6.4 %) 27 22● ●●●● ● ● ●●●●●● ●● ● ● ● ● ● ●●● ●●●●● ● ●●●● ●●●● ●●●● ●●●● ●●●●●●● ● ● ●●●● ●●●● ●●●●●●● ● ● ● ●●●●●●● ● ● ● ●●●●●●●●●●●●●●●●● ● ●●●● ● ● ● ● ●●● ●● ● ●●●● ● ●●●● ● ● ●●●●●●● ● ● ● ● ● ● ● ● ●●●● ●● ●●●● ● ● ●●●●●● ●● ● ● ● ● ● ●●● ●●●●● ● ●●●● ●●●● ●●●● ●●●● ●●●●●●● ● ● ●●●● ●●●● ●●●●●●● ● ● ● ●●●●●●● ● ● ● ●●●●●●●●●●●●●●●●● ● ●●●● ● ● ● ● ●●● ●● ● ●●●● ● ●●●● ● ● ●●●●●●● ● ● ● ● ● ● ● ● ●●●● ●2.8±3.7
BF 2 560 (100 %) 426 - −40 −20 0 20 40−40 −20 0 20 40
Berkeley JE FW 10 (3 %) 8.4 0 17.7±19.6
PW 48 (12 %) 40 24●●●● ● ●●●●● ● ●8.5±9.6
HO 16 (4 %) 97 51●● ● ●●●●●●●● ● ●●●● ●● ● ●●●●●●●● ● ●●●●3.8±5.7
HS 162 (40.5 %) 137 69●●●●●●● ●● ●●●●●●●●●●●●● ●●●●● ● ● ●●●● ● ●●●●●●●● ●● ●●●●●●●●●●●●● ●●●●● ● ● ●●●● ● ●1.7±3.5
BF 400 (100 %) 335 - −40 −20 0 20 40−40 −20 0 20 40
Apache FW 9 (4.7 %) 10 0● ●●●●●●●●● ●●●●●●●●14.9±24.8
PW 29 (15.1 %) 32 18●●● ● ●●●●● ●●●●●●● ● ●●●●● ●●●●7.7±11.2
HO 80 (41.7 %) 89 44●●●●●● ●● ●●●● ●●●●●●●●● ●● ●●●● ●●●11.6±22.7
HS 143 (74.5 %) 159 73●●●●●●●●●●●●●● ●● ●●●●● ● ● ●● ● ● ●● ●● ●● ● ● ●● ●●● ● ● ●● ● ● ●● ●●●●●●● ●●●●●● ●●●●●●● ●● ●●●●●●●●●●●●●●● ●● ●●●●● ● ● ●● ● ● ●● ●● ●● ● ● ●● ●●● ● ● ●● ● ● ●● ●●●●●●● ●●●●●● ●●●●●●● ●● ●5.3±10.8
BF 192 (100 %) 213 - −40 −20 0 20 40−40 −20 0 20 40
SQLite FW 26 (0 %) 2.1 0 7.8±9.2
PW 566 (0 %) 47 2● ● ● ● ●●● ●● ● ● ● ●●● ●9.3±12.5
HO 566 (0 %) 47 3●●7.1±9.1
HS 569 (0 %) 47.4 3●●7±9
BF 3 932 160 (100 %) 327 680 −40 −20 0 20 40−40 −20 0 20 40
LLVM FW 11 (1.1 %) 2 0 7.8±9
PW 62 (6.1 %) 12 27●● ●● ●●●●●●●● ●●●●●●● ● ●●●●●●●● ●●●●●● ● ●●●● ●●●● ● ● ●●●●●●●●●●●● ● ● ● ●●● ●● ● ●●●●●●●●●●● ● ● ● ● ● ● ● ● ● ● ● ● ●● ●● ●●● ●● ● ●●●● ● ●●● ●● ●●●●●●●● ●●●●●●● ● ●●●●●●●● ●●●●●● ● ●●●● ●●●● ● ● ●●●●●●●●●●●● ● ● ● ●●● ●● ● ●●●●●●●●●●● ● ● ● ● ● ● ● ● ● ● ● ● ●● ●● ●●● ●● ● ●●●● ● ●7.4±10.2
HO 62 (6.1 %) 12 27●● ●● ●●●●●●●● ●●●●●●● ● ●●●●●●●● ●●●●●● ● ●●●● ●●●● ● ● ●●●●●●●●●●●● ● ● ● ●●● ●● ● ●●●●●●●●●●● ● ● ● ● ● ● ● ● ● ● ● ● ●● ●● ●●● ●● ● ●●●● ● ●●● ●● ●●●●●●●● ●●●●●●● ● ●●●●●●●● ●●●●●● ● ●●●● ●●●● ● ● ●●●●●●●●●●●● ● ● ● ●●● ●● ● ●●●●●●●●●●● ● ● ● ● ● ● ● ● ● ● ● ● ●● ●● ●●● ●● ● ●●●● ● ●7.4±10.2
HS 88 (8.6 %) 17 38●●● ●●●● ●●●●●●●● ●● ●●● ●●●● ●●●● ●●●●● ●● ●●●● ●●●● ●●● ●●●● ●●●● ●●● ●●●● ● ● ● ●● ●●●●●● ● ●●●●● ● ●●●●● ●●● ● ● ● ●●● ●●●● ●●●●●●●● ●● ●●● ●●●● ●●●● ●●●●● ●● ●●●● ●●●● ●●● ●●●● ●●●● ●●● ●●●● ● ● ● ●● ●●●●●● ● ●●●●● ● ●●●●● ●●● ● ● ●5.7±7
BF 1 024 (100 %) 202 - −40 −20 0 20 40−40 −20 0 20 40
x264 FW 12 (1 %) 2 0 29.6±22
PW 81 (7 %) 16 13 17.9±27.2
HO 89 (7.7 %) 17 17●●●●5.1±15.1
HS 89 (7.7 %) 17 17●●●●5.1±15.1
BF 1 152 (100 %) 224 - −40 −20 0 20 40−40 −20 0 20 40
Table II
EV ALUATION RESULTS FOR SIX CASE STUDIES ;APPROACHES (APPR.):FEATURE -WISE (FW), PAIR -WISE (PW), HIGHER -ORDER (HO), HOT-SPOT (HS),
BRUTE FORCE (BF). M EAN:MEAN FAULT RATE OF PREDICTIONS ,STD:STANDARD DEVIATION OF PREDICTIONS .
order interactions and 55 higher-order interactions. Using
the PW heuristic, we have some features that interact with
many other features (e.g., KeepAlive) and other features that
interact only with one or two features (e.g., ExtendedStatus).
Although without using the hot-spot heuristic, we observe
that some features are more likely to interact. Additionally,
we found two features (Base and InMemory) that do not
interact which substantially decreases the search space.C. Threats to Validity
Internal Validity: Regarding SQLite, we cannot measure
all possible conﬁgurations in reasonable time. Hence, we
sampled only 100 conﬁgurations to compare prediction and
actual performance values. We are aware that this evaluation
leaves room for outliers.
Also, we are aware that measurement bias can cause false
interpretations [ 20]. Since we aim at predicting performance
for a special workload, we do not have to vary benchmarks.
Additionally, we determined the width of a 95 % conﬁdence40,0050,0060,0070,0080,00In percentBerkeley DB C
Berkeley DB Java
Apache
SQLite
LLVM
H264Error rate # of Measurements
0,0010,0020,0030,00
Feature-Wise Pair-Wise Higher-Order Hot-Spot
Applied Heuristics
Figure 3. Comparing percentage of measurements (straight lines) with
average error rates of predictions (dashed lines) for each heuristic.
interval of our measurements smaller than 10 % of the
according means.
External Validity: We aimed at increasing external
validity by choosing programs from different domains with
different conﬁguration mechanisms and implemented with
different programming languages. Furthermore, we used
programs that are deployed and used in the real-world. Nev-
ertheless, we are aware that the results of our evaluations are
not automatically transferable to all conﬁgurable programs.
In addition to our sample program selection, the strong and
exhaustive evaluation (over 60 days of measurement with 5
computers) indicate that our heuristics hold for many practical
application scenarios.
D. Discussion
Although we use a simplistic performance model, we
demonstrated that the approach is feasible. With an average
accuracy of 95 %, we achieve predictions that even stay in the
range of the observed measurement bias for the case studies.
It is important to note that we experienced large differences
in accuracy when we changed the threshold at which a
performance feature interaction is detected. Having a too
small threshold causes many false detections of interactions.
The fault rate increases, because we sum the inﬂuence of
measurement bias instead of the inﬂuence of interactions.
We observed that we need a relatively large number
of measurements when many alternative features exist
compared to independent features, because having many
alternative features limits the number of valid conﬁgura-
tions substantially. For example, we can only generate
400 conﬁgurations in Berkeley DB Java, though it has 32
features. This number is below quadratic. Hence, already
the detection of interacting features require a relatively large
number of measurements. However, having programs with
a small number of valid conﬁgurations make a brute-force
approach feasible, which is not our intended application
scenario.Furthermore, we do not consider performance behavior
of a program independently of the workload. We can
make accurate statements for any conﬁguration given a
speciﬁc workload. That is, we address end-users that have
a certain application scenario in mind but do not know
which conﬁguration performs best. Measurements can be
performed on a live system in a real environment, which
produces more suitable performance predictions than stan-
dard benchmark results in a synthetic environment. For a
new customer or a new workload, we have to repeat the
measurements. We believe that many interactions still exist,
though the values of the interactions will change. This,
however, means that we may save additional measurements
for new customers, since we already know which features
interact.
We believe that our approach is not limited to the detec-
tion of non-functional feature interactions for the property
performance, but also for other quantiﬁable and additive
non-functional properties, such as binary footprint, memory
footprint, and bandwidth.
VI. R ELATED WORK
A. Performance Prediction
There are several approaches that aim at predicting
performance of a customizable program or a product line.
Abdelaziz et al. provide an overview of component-based
prediction approaches [ 22]. Typically, the approaches belong
to one of three categories: model-based, measurement-based
(as we use in this paper), and mixed.
Model-based: Model-based predictions are com-
mon [ 23][24]. For example, linear and multiple regression
explore relationships between input parameters (features)
and measurements. Based on such a regression model,
different estimation methods (e.g., ordinary least squares)
can be used to predict the performance for speciﬁc input
parameters. Bayesian (or belief) networks are used to model
dependencies between variables in the network [ 25]. They
are often used to learn causal relationships and hence may
be applicable to detect feature interactions. Furthermore,
machine-learning approaches can be used to ﬁnd the corre-
lation between a conﬁguration and a measurement (e.g.,
canonical correlation analysis [26]), which uses dataset
pairs to identify those linear combinations of variables
with the best correlation. Principal component analysis [27]
ﬁnds dimensions of maximal variance in a dataset that
can also be used to detect interactions. Ganapathi et al.
provides an analysis for different machine-learning ap-
proaches in the context of performance prediction of database
queries [28].
The feasibility of model-based approaches depends on the
application scenario and program to be analyzed. Our work
differs in that it offers a general way to produce accurate
predictions independent of the application scenario, and it
uses heuristics to signiﬁcantly reduce measurement effort.Krogmann et al. [ 29] combine monitoring data, genetic
programming, and reverse engineering to reduce the number
of measurements to create a platform-independent behavioral
model of components. For a platform-speciﬁc prediction,
they use bytecode-benchmark results of concrete systems
to parameterize the behavior model. We predict the perfor-
mance independently of the used programming language and
availability of bytecode.
Happe et al. present a compositional reasoning approach,
based on the Palladio component model [30]. The idea is
that each component speciﬁes its resource demands and
predicted execution time in a global repository. The approach
is applicable only to component-based programs, whereas
we use a single approach for all customizable programs.
Also in this vein, MDE-based work uses feature models to
customize or synthesize performance models (e.g. [ 31]). This
line of research requires up-front and detailed knowledge
of domain-speciﬁc performance modeling, where tuning
predictions for accuracy can be difﬁcult. Our approach avoids
these problems by directly measuring performance.
Measurement-based: Sincero et al. [ 4] predict a conﬁg-
urations’s non-functional properties based on a knowledge
base consisting of measurements of already produced and
measured conﬁgurations. They aim at ﬁnding a correlation
between feature selection and measurement. This way, they
can provide qualitative information about how a feature
inﬂuences a non-functional property during conﬁguration.
In contrast to our approach, they do not actually predict a
value quantitatively, and they do not provide means to detect
feature interactions.
Chen et al. [ 32] use a combined benchmarking and
proﬁling approach to predict the performance of component-
based applications. Based on a benchmark and a Java proﬁling
tool, a performance prediction model is constructed for
application server components. In contrast, we correlate the
measurements to the conﬁguration, and measure only those
conﬁgurations from which we expect to detect performance
feature interactions.
Abdelaziz et al. argue that most measurement approaches
lack generality [ 22], as they are applicable only to speciﬁc
application scenarios or infrastructures [ 32][33]. Our work
can be used for a broad range of applications of different
domains, implementation techniques, etc.
B. Feature-Interaction Detection
There is a large body of research on automated detection
of feature interactions (e.g., see Nhlabatsi et al. [ 6] and
Calder et al. [ 34] for surveys). Many approaches aim at
detecting feature interactions at the speciﬁcation level. For
example, Calder and Miller use a pair-wise measurement
approach based on linear temporal logic to detect feature
interactions [ 7]. They specify the behavior of a product line in
Promela (a modeling language). Using a model checker, they
generate for each pair-wise combination a model checkingrun to verify whether the deﬁned properties are still valid.
Other approaches use state charts to model and detect feature
interactions [ 35]. For example, in [ 36] feature speciﬁcations
are translated to a reachability graph. The authors use state
transitions to detect whether a certain state is not exclusively
reachable in isolation (i.e. a feature interaction occurs).
There are approaches that provide means to detect semantic
feature interactions, i.e., feature interactions that change the
functional behavior of a program. Some use model checking
techniques to ﬁnd semantic feature interactions [ 37][38].
Apel’s work uses model-checking techniques to verify
whether semantic constraints still hold in a particular feature
combination [ 39][40]. Other approaches aim at investigating
the code base to detect structural feature interactions. For
example, Liu et al. [ 9][41] propose to model feature interac-
tions explicitly using algebraic theory. In contrast to these
approaches, we focus on performance feature interactions in
a black-box fashion.
VII. C ONCLUSION
We presented a method that allows stakeholders to accu-
rately predict the performance of customized and generated
programs. It detects interactions among conﬁguration options
or features and evaluates their inﬂuence on performance. We
detect feature interactions in a step-wise manner. First, we
ﬁnd features that interact. Second, we detect the combinations
of these features that cause a measurable interaction and
quantify their impact on the performance of a conﬁguration.
The common weak spot of such an approach is the exhaustive
number of measurements required to detect interactions. We
solved this problem by means of three heuristics that reduce
the number of measurements without sacriﬁcing precision in
predictions.
Our evaluations demonstrate that an accuracy of 95 %
is possible, on average, when using our heuristics. We
demonstrated generality by using applications of varying
domains, implemented with different programming languages
and techniques, and conﬁgured via conﬁguration ﬁles or
compilation options.
ACKNOWLEDGMENTS
We are grateful to Janet Feigenspan for comments on an earlier
draft of this paper. The work of Siegmund and Saake is supported
by the German ministry of education and science (BMBF), number
01IM10002B. Rosenm ¨uller’s, Apel’s, and Kolesnikov’s work is
supported by the German Research Foundation (SA 465/34-1, AP
206/2, AP 206/4, and LE 912/13). K ¨astner’s work is supported by
ERC grant #203099. Batory’s work is funded by NSF’s Science of
Design Project CCF 0724979.
REFERENCES
[1]A. Rabkin and R. Katz, “Static extraction of program conﬁg-
uration options,” in ICSE . ACM, 2011, pp. 131–140.
[2]K. Czarnecki and U. Eisenecker, Generative Programming:
Methods, Tools, and Applications . Addison-Wesley, 2000.[3]N. Siegmund, M. Rosenm ¨uller, C. K ¨astner, P. Giarrusso,
S. Apel, and S. Kolesnikov, “Scalable prediction of non-
functional properties in software product lines,” in SPLC .
IEEE, 2011, pp. 160–169.
[4]J. Sincero, W. Schroder-Preikschat, and O. Spinczyk, “Ap-
proaching non-functional properties of software product lines:
Learning from products,” in APSEC . IEEE, 2010, pp. 147–
155.
[5]M. Calder, M. Kolberg, E. Magill, and S. Reiff-Marganiec,
“Feature interaction: A critical review and considered forecast,”
Comput. Netw. , vol. 41, no. 1, pp. 115–141, 2003.
[6]A. Nhlabatsi, R. Laney, and B. Nuseibeh, “Feature interaction:
The security threat from within software systems,” Progress
in Informatics , no. 5, pp. 75–89, 2008.
[7]M. Calder and A. Miller, “Feature interaction detection by
pairwise analysis of LTL properties: A case study,” Formal
Methods System Design , vol. 28, no. 3, pp. 213–261, 2006.
[8]N. Siegmund, M. Rosenm ¨uller, M. Kuhlemann, C. K ¨astner,
S. Apel, and G. Saake, “SPL Conqueror: Toward optimiza-
tion of non-functional properties in software product lines,”
Software Quality Journal , 2011, online ﬁrst.
[9]D. Batory, P. H ¨ofner, and J. Kim, “Feature interactions,
products, and composition,” in GPCE . ACM, 2011, pp.
13–22.
[10] J. Lee, K. Kang, and S. Kim, “A feature-based approach to
product line production planning,” in Software Product Lines ,
ser. LNCS. Springer, 2004, vol. 3154, pp. 137–140.
[11] C. Kim, C. K ¨astner, and D. Batory, “On the modularity of
feature interactions,” in GPCE . ACM, 2008, pp. 23–34.
[12] D. Cohen, S. Dalal, J. Parelius, and G. Patton, “The combi-
natorial design approach to automatic test generation,” IEEE
Software , vol. 13, no. 5, pp. 83–88, 1996.
[13] K.-C. Tai and Y . Lei, “A test generation strategy for pairwise
testing,” IEEE TSE , vol. 28, no. 1, pp. 109 –111, 2002.
[14] A. Williams, “Determination of test conﬁgurations for pair-
wise interaction coverage,” in TestComm . Kluwer, 2000, pp.
59–74.
[15] S. Oster, F. Markert, and P. Ritter, “Automated incremental
pairwise testing of software product lines,” in SPLC , 2010,
pp. 196–210.
[16] J. Liebig, S. Apel, C. Lengauer, C. K ¨astner, and M. Schulze,
“An analysis of the variability in forty preprocessor-based
software product lines,” in ICSE . ACM, 2010, pp. 105–114.
[17] S. Apel and D. Beyer, “Feature cohesion in software product
lines: An exploratory study,” in ICSE . ACM, 2011, pp.
421–430.
[18] C. Taube-Schock, R. Walker, and I. Witten, “Can we avoid
high coupling?” in ECOOP . Springer, 2011, pp. 204–228.
[19] S. She, R. Lotufo, T. Berger, A. Wasowski, and K. Czarnecki,
“Reverse engineering feature models,” in ICSE . ACM, 2011,
pp. 461–470.
[20] T. Mytkowicz, A. Diwan, M. Hauswirth, and P. Sweeney,
“Producing wrong data without doing anything obviously
wrong!” in ASPLOS . ACM, 2009, pp. 265–276.
[21] A. Georges, D. Buytaert, and L. Eeckhout, “Statistically
rigorous Java performance evaluation,” in OOPSLA . ACM,
2007, pp. 57–76.
[22] A. Abdelaziz, W. Kadir, and A. Osman, “Comparative analysis
of software performance prediction approaches in context of
component-based system,” IJCA , vol. 23, no. 3, pp. 15–22,
2011.
[23] S. Balsamo, A. Di Marco, P. Inverardi, and M. Simeoni,
“Model-based performance prediction in software development:
A survey,” IEEE TSE , vol. 30, no. 5, pp. 295–310, 2004.[24] I. Witten and E. Frank, Data mining : Practical machine
learning tools and techniques , 2nd ed. Elsevier, Morgan
Kaufman, 2005.
[25] F. Jensen and T. Nielsen, Bayesian Networks and Decision
Graphs , 2nd ed. Springer, 2007.
[26] K. Mardia, J. Kent, and J. Bibby, Multivariate Analysis
(Probability and Mathematical Statistics) , 1st ed. Academic
Press, 1980.
[27] H. Hotelling, “Analysis of a complex of statistical variables
into principal components,” Journal of Educational Psychol-
ogy, vol. 24, no. 6, pp. 417–441, 1933.
[28] A. Ganapathi, H. Kuno, U. Dayal, J. Wiener, A. Fox,
M. Jordan, and D. Patterson, “Predicting multiple metrics
for queries: Better decisions enabled by machine learning,” in
ICDE . IEEE, 2009, pp. 592–603.
[29] K. Krogmann, M. Kuperberg, and R. Reussner, “Using genetic
search for reverse engineering of parametric behavior models
for performance prediction,” IEEE TSE , vol. 36, no. 6, pp.
865–877, 2010.
[30] J. Happe, H. Koziolek, and R. Reussner, “Facilitating perfor-
mance predictions using software components,” IEEE Software ,
vol. 28, no. 3, pp. 27 –33, 2011.
[31] R. Tawhid and D. Petriu, “Automatic derivation of a product
performance model from a software product line model,” in
SPLC . IEEE, 2011, pp. 80–89.
[32] S. Chen, Y . Liu, I. Gorton, and A. Liu, “Performance prediction
of component-based applications,” Journal of System Software ,
vol. 74, no. 1, pp. 35–43, 2005.
[33] S. Yacoub, “Performance analysis of component-based appli-
cations,” in SPLC . Springer, 2002, vol. 2379, pp. 1–5.
[34] M. Calder, M. Kolberg, E. Magill, and S. Reiff-Marganiec,
“Feature interaction: A critical review and considered forecast,”
Comp. Netw. and ISDN Systems , vol. 41, pp. 115–141, 2003.
[35] C. Prehofer, “Plug-and-play composition of features and
feature interactions with statechart diagrams,” Software and
Systems Modeling , vol. 3, no. 3, pp. 221–234, 2004.
[36] K. Pomakis and J. Atlee, “Reachability analysis of feature
interactions: A progress report,” in International Symposium
on Software Testing and Analysis , vol. 21. ACM, 1996, pp.
216–223.
[37] A. Classen, P. Heymans, P.-Y . Schobbens, A. Legay, and J.-F.
Raskin, “Model checking lots of systems: Efﬁcient veriﬁcation
of temporal properties in software product lines,” in ICSE .
ACM, 2010, pp. 335–344.
[38] K. Lauenroth, K. Pohl, and S. Toehning, “Model checking of
domain artifacts in product line engineering,” in ASE. IEEE,
2009, pp. 269–280.
[39] S. Apel, W. Scholz, C. Lengauer, and C. K ¨astner, “Detecting
dependences and interactions in feature-oriented design,” in
ISSRE . IEEE, 2010, pp. 161–170.
[40] S. Apel, H. Speidel, P. Wendler, A. von Rhein, and D. Beyer,
“Detection of feature interactions using feature-aware veriﬁca-
tion,” in ASE. IEEE, 2011, pp. 372–375.
[41] J. Liu, D. Batory, and S. Nedunuri, “Modeling interactions
in feature-oriented designs,” in ICFI . IOS Press, 2005, pp.
178–197.